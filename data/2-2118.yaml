- en: 'To 1 or to 0: Pixel Attacks in Image Classification'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1还是0：图像分类中的像素攻击
- en: 原文：[https://towardsdatascience.com/to-1-or-to-0-pixel-attacks-in-image-classification-ec323555a11a](https://towardsdatascience.com/to-1-or-to-0-pixel-attacks-in-image-classification-ec323555a11a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/to-1-or-to-0-pixel-attacks-in-image-classification-ec323555a11a](https://towardsdatascience.com/to-1-or-to-0-pixel-attacks-in-image-classification-ec323555a11a)
- en: Navigating the Realm of Adversarial Machine Learning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索对抗性机器学习的领域
- en: '[](https://medium.com/@MahamsMultiverse?source=post_page-----ec323555a11a--------------------------------)[![Maham
    Haroon](../Images/5a9ac82369ecbf7719b765ec160a70ef.png)](https://medium.com/@MahamsMultiverse?source=post_page-----ec323555a11a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ec323555a11a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ec323555a11a--------------------------------)
    [Maham Haroon](https://medium.com/@MahamsMultiverse?source=post_page-----ec323555a11a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@MahamsMultiverse?source=post_page-----ec323555a11a--------------------------------)[![Maham
    Haroon](../Images/5a9ac82369ecbf7719b765ec160a70ef.png)](https://medium.com/@MahamsMultiverse?source=post_page-----ec323555a11a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ec323555a11a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ec323555a11a--------------------------------)
    [Maham Haroon](https://medium.com/@MahamsMultiverse?source=post_page-----ec323555a11a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ec323555a11a--------------------------------)
    ·13 min read·Nov 23, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----ec323555a11a--------------------------------)
    ·阅读时间13分钟·2023年11月23日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/5969ea7ddc81cf61597e791f7e4a79c5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5969ea7ddc81cf61597e791f7e4a79c5.png)'
- en: Photo by [Pietro Jeng](https://unsplash.com/@pietrozj?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/green-and-red-light-wallpaper-n6B49lTx7NM?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Pietro Jeng](https://unsplash.com/@pietrozj?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)提供，来源于[Unsplash](https://unsplash.com/photos/green-and-red-light-wallpaper-n6B49lTx7NM?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
- en: Hi there!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 嗨，大家好！
- en: This year, I took part in my first [Capture The Flag (CTF) competition](https://www.kaggle.com/competitions/ai-village-capture-the-flag-defcon31/overview)
    by AI Village @ DEFCON 31, and the experience was intriguing, to say the least.
    The challenges, particularly those involving pixel attacks, caught my attention
    and are the main focus of this post. While I initially intended to share a simple
    version of a pixel attack I performed during the competition, the goal of this
    post is to also delve into strategies for strengthening ML models to better withstand
    pixel attacks like the ones encountered in the competition.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 今年，我参加了我的第一次[Capture The Flag (CTF)比赛](https://www.kaggle.com/competitions/ai-village-capture-the-flag-defcon31/overview)，这次经历可以说非常引人入胜。挑战，特别是那些涉及像素攻击的挑战，引起了我的注意，并成为了这篇文章的主要焦点。虽然我最初打算分享我在比赛中进行的一个简单的像素攻击，但这篇文章的目标也是*深入探讨*如何增强机器学习模型，以更好地抵御像比赛中遇到的像素攻击。
- en: Before we dive into the theory, let’s set the scene with a scenario that’ll
    grab your attention.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入理论之前，让我们通过一个引人入胜的场景来引起你的注意。
- en: 'Picture this: our company, MM Vigilant, is on a mission to develop a cutting-edge
    object detection product. The concept is simple yet revolutionary — customers
    snap a picture of the desired item, and it is delivered at their doorstep a few
    days later. As the brilliant data scientist behind the scenes, you’ve crafted
    the ultimate image-based object classification model. The classification results
    are impeccable, the model evaluation metrics are top-notch, and stakeholders couldn’t
    be happier. The model hits production, and customers are delighted — until a wave
    of complaints rolls in.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下：我们的公司MM Vigilant致力于开发一款前沿的物体检测产品。概念简单而革命性——客户拍下所需物品的照片，几天后它就会送到客户家门口。作为幕后出色的数据科学家，你打造了终极的基于图像的物体分类模型。分类结果无可挑剔，模型评估指标一流，利益相关者也非常满意。模型投入生产，客户也很高兴——直到投诉接踵而至。
- en: Upon investigation, it turns out someone is meddling with the images before
    they reach the classifier. Specifically, every image of a clock is being mischievously
    classified as a mirror. The consequence? Anyone hoping for a clock is receiving
    an unexpected mirror at their door. Quite the unexpected twist, isn’t it?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 经调查，发现有人在图像到达分类器之前对其进行干扰。具体来说，每张钟表的图像都被恶意地分类为镜子。结果如何？任何期待钟表的人都会收到意外的镜子。这真是个意外的转折，不是吗？
- en: Our stakeholders at MM Vigilant are both concerned and intrigued by how this
    mishap occurred and, more importantly, what measures can be taken to prevent it.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在MM Vigilant的利益相关者对这种事故的发生感到既担忧又好奇，更重要的是，如何采取措施来防止它。
- en: The scenario we just explored is a hypothetical situation —though image tempering
    is a very likely scenario, especially if there are vulnerabilities in the model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚探讨的场景是一个假设情境——尽管图像篡改是一个非常可能的情况，特别是当模型存在漏洞时。
- en: So let’s take a closer look on one such manipulation of images…
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们仔细看看这种图像操作的一个例子……
- en: Pixel Attacks in Image Classification
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像分类中的像素攻击
- en: Pixel attacks, specifically in the context of image classification, aim to deceive
    a machine learning (ML) classifier into categorizing an image as something other
    than its actual class. While one can sarcastically argue that a subpar model already
    exhibits such behavior, the goal here is to outsmart state-of-the-art models.
    Needless to say that, there are numerous methods and motivations for these attacks,
    but this post, limited in scope, will narrow its focus to black box, targetted,
    pixel attacks and the precautions involved.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 像素攻击，特别是在图像分类的背景下，旨在欺骗机器学习（ML）分类器，将图像分类为其他类别。虽然可以讽刺地认为，一个不佳的模型已经表现出这种行为，但这里的目标是击败最先进的模型。不用说，这些攻击有很多方法和动机，但这篇文章，限于范围，将重点关注黑箱、针对性像素攻击及其相关的预防措施。
- en: Let’s begin by approaching this concept intuitively. Essentially, any input
    to a neural network undergoes a series of mathematical operations per pixel to
    classify an image as X. Altering a pixel, therefore, leads to a change in the
    result of these mathematical operations altering the prediction score. This can
    be extrapolated to such an extent that if a dominant/”central to classification”
    pixel is manipulated, it’ll exert significant enough influence on the prediction
    score of the class, such that, the outcome is a misclassification, as illustrated
    in the figure below.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从直观上来理解这个概念。实际上，任何输入到神经网络的图像都经过每个像素的一系列数学运算来进行分类。改变一个像素，因此，会导致这些数学运算的结果发生变化，从而改变预测得分。这可以推断到这样一种程度，如果一个主要/“对分类至关重要”的像素被操控，它将对类别的预测得分产生足够大的影响，从而导致误分类，如下图所示。
- en: '![](../Images/c4c97ae5ef8137c6c38478aa35920c65.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4c97ae5ef8137c6c38478aa35920c65.png)'
- en: Image by Author
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于作者
- en: 'In the realm of image classification attacks, there are two well-known approaches,
    depending on the desired outcome of misclassification:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像分类攻击领域，有两种知名的方法，取决于误分类的期望结果：
- en: Targeted attacks
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 针对性攻击
- en: Untargeted attacks
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 未针对性攻击
- en: Targeted attacks
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 针对性攻击
- en: 'Targeted pixel attacks involve a purposeful transformation with the goal of
    having the image classified as a specific class. For instance, imagine a deliberate
    attempt to classify a bear as a boat or an apple as a koala. These attacks have
    dual objectives: minimizing the score for the original class while maximizing
    the score for the target class.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 针对性像素攻击涉及一种有目的的转换，目标是将图像分类为特定类别。例如，想象一个故意尝试将熊分类为船或将苹果分类为考拉的行为。这些攻击有两个目标：最小化原始类别的得分，同时最大化目标类别的得分。
- en: Untargeted attacks
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 未针对性攻击
- en: Conversely, untargeted pixel attacks operate on the premise of avoiding the
    classification of the image as its original class. The task simplifies to minimizing
    the prediction score for the specified class. In other words, the aim is to ensure
    that a bear, for example, is classified as **anything** other than a bear.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，未针对性像素攻击的前提是避免将图像分类为其原始类别。任务简化为最小化指定类别的预测得分。换句话说，目标是确保一只熊，例如，被分类为**除了熊之外的任何东西**。
- en: It’s worth noting that every targeted attack can be considered an untargeted
    attack, but the reverse is not necessarily true.
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 值得注意的是，每个针对性攻击都可以被认为是未针对性攻击，但反之则不一定成立。
- en: In addition to attack types, there are two distinct methodologies for achieving
    these attacks, depending on the availability of the attacked model (A traditional/white
    box approach) or only the resulting scores (A black box methodology).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 除了攻击类型之外，还有两种不同的方法来实现这些攻击，具体取决于被攻击模型的可用性（传统/白盒方法）或仅有的结果分数（黑盒方法）。
- en: Traditional Attacks
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 传统攻击
- en: In traditional or white box attacks, the model is usually available. Gradient
    information can be obtained and employed in attacks like the [Fast Gradient Sign
    Method (FGSM)](https://pytorch.org/tutorials/beginner/fgsm_tutorial.html). This
    method involves perturbing the input data by a small amount in the direction of
    the gradient, causing misclassification without significant alteration of the
    image’s visual appearance.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统或白盒攻击中，模型通常是可用的。可以获取梯度信息并用于如 [快速梯度符号方法（FGSM）](https://pytorch.org/tutorials/beginner/fgsm_tutorial.html)
    这样的攻击。这种方法通过沿梯度方向对输入数据进行小幅扰动，导致误分类，而不会显著改变图像的视觉外观。
- en: A simple github implementation of the approach can be found in the following
    repository.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在以下代码库中找到该方法的简单 GitHub 实现。
- en: '[](https://github.com/ymerkli/fgsm-attack/tree/master?source=post_page-----ec323555a11a--------------------------------)
    [## GitHub - ymerkli/fgsm-attack: Implementation of targeted and untargeted Fast
    Gradient Sign Method'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/ymerkli/fgsm-attack/tree/master?source=post_page-----ec323555a11a--------------------------------)
    [## GitHub - ymerkli/fgsm-attack: 目标和非目标快速梯度符号方法的实现'
- en: 'Implementation of targeted and untargeted Fast Gradient Sign Method - GitHub
    - ymerkli/fgsm-attack: Implementation of…'
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '目标和非目标快速梯度符号方法的实现 - GitHub - ymerkli/fgsm-attack: 实现了…'
- en: github.com](https://github.com/ymerkli/fgsm-attack/tree/master?source=post_page-----ec323555a11a--------------------------------)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/ymerkli/fgsm-attack/tree/master?source=post_page-----ec323555a11a--------------------------------)
- en: Black box Attacks
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 黑盒攻击
- en: Black box attacks, conversely, rely solely on the model predictions. Techniques
    such as [differential evolution](https://machinelearningmastery.com/differential-evolution-from-scratch-in-python/)
    can be employed for executing this type of attack.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 黑盒攻击则完全依赖于模型预测。可以使用如 [差分进化](https://machinelearningmastery.com/differential-evolution-from-scratch-in-python/)
    这样的技术来执行这种类型的攻击。
- en: Differential Evolution is an optimization algorithm that mimics natural selection.
    It works by creating and combining potential solutions in iterations, choosing
    the best-performing ones from a population based on a set criterion. This approach
    is effective for exploring solution spaces and is commonly employed in adversarial
    attacks on machine learning models.
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 差分进化是一种模拟自然选择的优化算法。它通过在迭代中创建和组合潜在解决方案，基于设定的标准从一个种群中选择表现最佳的解决方案。这种方法在探索解决方案空间方面效果显著，并且常用于对机器学习模型的对抗攻击。
- en: Given that our challenge focused on a black box targeted pixel attack, let’s
    jump right into the implementation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们的挑战集中在黑盒目标像素攻击上，让我们直接进入实现部分。
- en: The CTF challenge
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CTF 挑战
- en: For the CTF challenge, the objective was to misclassify an unmistakable image
    of a wolf as a Granny Smith apple — nodding to the Red Riding Hood narrative.
    With a dataset featuring around 1000 classes and a high-resolution image at 768x768
    pixels, surpassing the resolutions of MNIST, CIFAR, and even ImageNet, the difficulty
    lay in deceiving the model by identifying the minimum number of pixels that would
    lead to the target misclassification. It’s worth noting that despite the intricacies
    of high-resolution images, the essence of machine learning classification, as
    mentioned above, lies in the non-intuitive nature of the task, reducing images
    to mere sets of values and a set of mathematical operations that are very dependent
    on these individual values.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 CTF 挑战，其目标是将一张清晰的狼的图像误分类为格兰尼·史密斯苹果——向“小红帽”故事致敬。数据集中包含大约 1000 个类别，图像分辨率为 768x768
    像素，超越了 MNIST、CIFAR 甚至 ImageNet 的分辨率，困难在于通过识别最少的像素数来欺骗模型以达到目标误分类。值得注意的是，尽管高分辨率图像复杂，但机器学习分类的本质，如上所述，在于任务的非直观性，将图像简化为一组值和一系列非常依赖这些个体值的数学运算。
- en: Before we dive into the code, let’s look at the original image of our wolf.
    Doesn’t it seem like it has the potential to pull off an apple disguise? Those
    green eyes, round face, and the green background — all the makings of a fruity
    impostor.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入研究代码之前，让我们先看看我们狼的原始图像。难道它看起来不具有伪装成苹果的潜力吗？那绿色的眼睛、圆圆的脸以及绿色的背景——这些都是一个果味冒名顶替者的所有特征。
- en: '![](../Images/124371fd270a9fb4da747983c8be8b4a.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/124371fd270a9fb4da747983c8be8b4a.png)'
- en: By [AI Village](https://www.kaggle.com/competitions/ai-village-capture-the-flag-defcon31/data)
    under license [Attribution 4.0 International (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由 [AI Village](https://www.kaggle.com/competitions/ai-village-capture-the-flag-defcon31/data)
    提供，许可证 [署名 4.0 国际 (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/)
- en: Embarking on the journey, where the initial score by the black box model for
    class “timber wolf” was about 0.29 and for class “granny smith” it was 0.0005,
    I initially considered the application of [scipy’s differential evolution](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html)
    approach. This method has demonstrated success in pixel attacks involving CIFAR
    and ImageNet datasets. The differential evolution technique involves starting
    with n random samples, representing the population size. At each next step, the
    best offspring are chosen, determined by the model scores, eventually leading
    to our desired outcome. However, given my time constraints and the task involving
    changing the score for only a single image, I opted for a more straightforward
    strategy.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始的旅程中，黑箱模型对“木狼”类别的初始评分约为0.29，而对“青苹果”类别的评分为0.0005。我最初考虑应用 [scipy的差分进化](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html)
    方法。这种方法在涉及CIFAR和ImageNet数据集的像素攻击中已显示成功。差分进化技术涉及从n个随机样本开始，代表种群大小。在每一步，选择最好的后代，通过模型评分来确定，最终导致我们期望的结果。然而，鉴于时间限制和任务涉及仅更改单个图像的评分，我选择了更直接的策略。
- en: The Approach
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法
- en: I started by dividing the original image into progressively smaller blocks,
    starting from 2x2 and reaching up to 16x16\. Focusing on the targeted granny smith
    apples (green), I in turn changed the values in the box to a shade of apple green
    and observed the impact on scores for both, the timber wolf class and the granny
    smith class. I then handpicked 2–3 16x16 blocks applied a version of differential
    evolution within this block. This meant changing one pixel, for a about 50–75
    iterations of randomly selected pixels in the region.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我首先将原始图像划分为逐渐更小的块，从2x2开始，一直到16x16。针对目标青苹果（绿色），我逐一将块中的值更改为苹果绿色，并观察对木狼类别和青苹果类别分数的影响。然后，我手动选择了2-3个16x16的块，在这些块中应用了一种差分进化的方法。这意味着在该区域内随机选择的像素进行了大约50-75次迭代的单像素更改。
- en: While I couldn’t pinpoint the notorious single pixel within the given two days,
    I achieved a highly pixelated attack that altered the classification of the wolf
    to that of a granny smith apple thus getting the flags for two sub problems of
    a 3 part task.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我在给定的两天内无法准确定位到那个臭名昭著的单一像素，但我成功地进行了高度像素化的攻击，将狼的分类改变为青苹果，从而获得了三部分任务的两个子问题的标志。
- en: Now that we have context, let’s jump into a little bit code so you get something
    away from this post.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了背景信息，让我们跳入一些代码中，以便你能从这篇文章中学到一些东西。
- en: Python Code
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python 代码
- en: I treated it as a black box problem and the query when given the image gave
    a list of predictions for all classes. The predictions were sorted by value, so
    the predicted class was the first value in the list.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我将其视为黑箱问题，当给定图像时，查询会提供所有类别的预测列表。预测按值排序，因此预测的类别是列表中的第一个值。
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The get_scores function fed the image to the query in the correct format and
    got the requisite results in a dictionary for the most part.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_scores`函数以正确的格式将图像输入查询，并在大多数情况下以字典形式获得所需的结果。'
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The relevant code
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关代码
- en: The core idea was to pick pixels within the RGB range of an apple’s color and
    test about 50–75 pixels to find the one that maximized the “granny smith” class
    score and minimized the “timber wolf” class score. I gradually increased the size
    of the selected section and modified the optimization process as needed. For instance.
    when I crossed the score for granny smith class crossed the score for timber wolf
    class, I considered all pixels that increased the granny smith class score as
    long as it was greater than the timber wolf score, instead of focussing on alos
    decreasing the score for the Timberwolf class, this obviously sped things up a
    little.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 核心思想是选择在苹果颜色的RGB范围内的像素，并测试约50-75个像素，以找到最大化“**青苹果**”类别分数并最小化“**狼**”类别分数的像素。我逐渐增加了选择区域的大小，并根据需要修改优化过程。例如，当“**青苹果**”类别的分数超过“**狼**”类别的分数时，我考虑所有增加“**青苹果**”类别分数的像素，只要它高于“**狼**”类别的分数，而不是专注于减少“**狼**”类别的分数，这显然加快了一些进程。
- en: Despite not finding the elusive single pixel, I successfully executed a highly
    pixelated attack.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管没有找到那个难以捉摸的单一像素，我成功执行了高度像素化的攻击。
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The resulting outcome looks something like this, it was classified as granny
    smith.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 结果看起来像这样，它被分类为**青苹果**。
- en: '![](../Images/054fe7aeb01f7f2c049f2d3c3b4a3ccb.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/054fe7aeb01f7f2c049f2d3c3b4a3ccb.png)'
- en: Original Image By [AI Village](https://www.kaggle.com/competitions/ai-village-capture-the-flag-defcon31/data)
    under license [Attribution 4.0 International (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/)
    (Edited by Author)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 原始图像由 [AI Village](https://www.kaggle.com/competitions/ai-village-capture-the-flag-defcon31/data)
    提供，许可 [署名 4.0 国际 (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/)（由作者编辑）
- en: The Magnified results
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 放大结果
- en: My wolf is pretty obviously tempered with but this was a high resolution image
    and the attack was a success. I’m sure given a little more time, there’s a possibility
    for a lot less pixels achieving better deception.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我的狼图像显然被篡改了，但这是一个高分辨率图像，攻击成功了。我相信如果再给点时间，可能会用更少的像素达到更好的欺骗效果。
- en: '![](../Images/3f8484e47b07afe9010118614313cde2.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f8484e47b07afe9010118614313cde2.png)'
- en: The pixelated Wolf classifying as a granny smith apple. By [AI Village](https://www.kaggle.com/competitions/ai-village-capture-the-flag-defcon31/data)
    under license [Attribution 4.0 International (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/)
    (Edited by Author)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 像素化的狼被分类为青苹果。由 [AI Village](https://www.kaggle.com/competitions/ai-village-capture-the-flag-defcon31/data)
    提供，许可 [署名 4.0 国际 (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/)（由作者编辑）
- en: A word to the wise…
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提醒一句…
- en: Having witnessed a potential version of a pixel attack that resulted in the
    misclassification of the model based solely on prediction scores and trial and
    error, let’s delve a bit further into how to avoid this.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在目睹了一个可能的像素攻击版本，这个版本仅根据预测分数和试验与错误导致了模型的错误分类之后，让我们进一步探讨如何避免这种情况。
- en: Certainly, the objective here isn’t to encourage performing pixel attacks, unless,
    of course, it’s on your own model as a resilience check. The essence of exploring
    the intricacies of adversarial ML practices is to cultivate awareness of how to
    safeguard your model from succumbing to such approaches.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这里的目标不是鼓励执行像素攻击，除非这是对自己模型的抗性检查。探索对抗性机器学习实践的复杂性本质上是为了培养如何保护模型不受这些方法影响的意识。
- en: So let’s delve into potential fortifications to avoid these scenarios…
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们深入探讨如何避免这些情况…
- en: Possible weakness of Pixel Attacks
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 像素攻击的可能弱点
- en: Pixel attacks, especially in a black box setting, already involve a significant
    amount of trial and error but various strategies can further enhance the robustness
    of models against these attacks.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 像素攻击，特别是在黑箱设置中，已经涉及到大量的试验和错误，但各种策略可以进一步增强模型对这些攻击的鲁棒性。
- en: 1\. Using higher Resolution images
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 使用高分辨率图像
- en: Higher resolution images are harder to attack given they require more resources
    and a higher number of changed features/pixels, thus can be more challenging to
    tamper with subtly.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 高分辨率图像更难被攻击，因为它们需要更多的资源和更高数量的改变特征/像素，因此更难以微妙地篡改。
- en: '![](../Images/9227cf4ea3af5d97f1482ea705e9c58b.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9227cf4ea3af5d97f1482ea705e9c58b.png)'
- en: Image by Author
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由作者提供
- en: 'Clarification: For example, consider a 32x32 image from CIFAR, which has fewer
    pixels, making it more susceptible to tampering. In contrast, higher-resolution
    images, are less prone to pixel attacks due to their increased pixel count. On
    the other hand, these images, while more challenging to tamper with subtly, may
    incur higher computational costs during training. Necessitating a need for striking
    a balance between security and computational efficiency.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 澄清：例如，考虑一个来自CIFAR的32x32图像，它的像素较少，使其更容易受到篡改。相比之下，高分辨率图像由于像素数量更多，因此不易受到像素攻击。另一方面，这些图像虽然在隐蔽篡改时更具挑战，但在训练过程中可能会产生更高的计算成本。因此，需要在安全性和计算效率之间找到平衡。
- en: Increasing prediction score threshold for accepted result
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提高接受结果的预测分数阈值
- en: Given that the attacked images have lower prediction score, a score threshold
    can be utilized to detect potential adversarial attacks.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 由于被攻击的图像具有较低的预测分数，可以利用分数阈值来检测潜在的对抗性攻击。
- en: '![](../Images/d283e1c5f815ebd64c79b3fe41b5ad98.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d283e1c5f815ebd64c79b3fe41b5ad98.png)'
- en: Image by Author
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'Clarification: For instance, setting a threshold below which predictions are
    considered inconclusive provides an added layer of security against adversarial
    attacks.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 澄清：例如，设置一个阈值，低于该阈值的预测被视为不确定，这为防御对抗性攻击提供了额外的安全层。
- en: Again it’s worthwhile to pinpoint that this is a trade-off, higher threshold
    enhances confidence but may limit the classifier’s sensitivity. Finding the right
    balance is crucial to avoid rejecting valid predictions while thwarting adversarial
    attacks.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 再次值得指出的是，这是一个权衡，高阈值提升了信心，但可能会限制分类器的敏感度。找到合适的平衡对于避免拒绝有效预测同时抵御对抗性攻击至关重要。
- en: Considering CNN’s Robustness Against Attacks for Critical Application
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 考虑到CNN在关键应用中的鲁棒性
- en: Turns out while not immune Convolutional Neural Networks (CNNs) are less susceptible
    to such adversarial attacks, given they utilize spacial hierarchies.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，尽管卷积神经网络（CNNs）并非免疫，但由于利用了空间层次结构，它们对这种对抗性攻击的敏感性较低。
- en: '![](../Images/939bb56d8794531c410adeed1260733a.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/939bb56d8794531c410adeed1260733a.png)'
- en: Image by Author
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'Clarification: In simple terms, while an average model treats pixels as individual
    inputs, CNNs consider predefined associations through kernel windows, enhancing
    robustness against adversarial manipulations.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 澄清：简单来说，虽然平均模型将像素视为单独的输入，但卷积神经网络（CNN）通过内核窗口考虑预定义的关联，从而增强了对抗性操控的鲁棒性。
- en: Preprocessing Images before prediction
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测前的图像预处理
- en: It maybe worth applying a robust preprocessing technique to images before feeding
    them into neural networks for prediction thus limiting black box attacks.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在将图像输入神经网络进行预测之前，应用一种稳健的预处理技术可能是值得的，从而限制黑箱攻击。
- en: '![](../Images/18415ca635300fc3186c5e9b8bc41a4e.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18415ca635300fc3186c5e9b8bc41a4e.png)'
- en: Image by Author
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'Clarification: Image compression, for instance, aids in reducing the effects
    of tampering, while computer vision algorithms can identify distortions or anomalies
    in images. Additionally, interpolation techniques can be applied since manipulated
    pixels may not closely match the colors or patterns of the original image.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 澄清：例如，图像压缩有助于减少篡改的影响，而计算机视觉算法可以识别图像中的失真或异常。此外，由于被操控的像素可能与原始图像的颜色或模式不完全匹配，因此可以应用插值技术。
- en: Secure ML Models
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安全的机器学习模型
- en: The above approaches while effective are not one size fits all. Eventually securing
    a certain ML model against adversarial attacks includes rigorous testing and validation
    of models under various conditions, including exposure to potential adversarial
    inputs.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法虽然有效，但并非一刀切。最终，保护某个机器学习模型免受对抗性攻击包括在各种条件下（包括暴露于潜在对抗性输入）对模型进行严格的测试和验证。
- en: Deciding on how much security to add and how often to update the model depends
    on how important it is and the types of threats it might face. But, being aware
    of ethical considerations and understanding possible threats can help reduce the
    risks from attacks.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 决定添加多少安全性以及多频繁更新模型取决于其重要性和可能面临的威胁类型。但是，了解伦理考虑和理解可能的威胁有助于减少攻击风险。
- en: Wrapping Up…
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结……
- en: While it’s true that pixel attacks or any manipulation of images can be a big
    problem for image-based AI systems, there’s also a lot we can do to protect against
    them. Attackers can mess with individual pixels to trick models into making mistakes,
    jeopardizing the reliability of crucial applications like image recognition and
    security systems. This not only leads to security breaches but also undermines
    trust from customers and stakeholders.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然像素攻击或任何图像的操控对基于图像的人工智能系统确实是一个大问题，但我们也可以做很多事情来防范这些攻击。攻击者可以篡改单个像素来欺骗模型，使其犯错误，从而危害图像识别和安全系统等关键应用的可靠性。这不仅会导致安全漏洞，还会破坏客户和利益相关者的信任。
- en: On the flip side, ML practitioners have tools at their disposal to make sure
    models aren’t vulnerable to such attacks.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，机器学习从业者拥有工具来确保模型不容易受到这些攻击的影响。
- en: In this post I attempted exploring pixel attacks, inspired by a CTF challenge,
    and delved into some of the intricacies of deceiving image classification models.
    While the wolf did morph into a Granny Smith apple, it took a lot of computation
    and trial and error and had the model employed some precautions, the attack would’ve
    been unsuccessful.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我尝试探索了像素攻击，受到 CTF 挑战的启发，并深入研究了欺骗图像分类模型的一些复杂性。虽然狼确实变成了一个青苹果，但这需要大量的计算和反复试验，如果模型采取了一些预防措施，攻击将会失败。
- en: I leave a few resources below of similar approaches, and hope you find the topic
    useful in keeping the models safe.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我在下面列出了一些类似的方法资源，希望你能发现这些话题对保持模型安全有用。
- en: Resources
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源
- en: '[](https://github.com/Hyperparticle/one-pixel-attack-keras?source=post_page-----ec323555a11a--------------------------------)
    [## GitHub - Hyperparticle/one-pixel-attack-keras: Keras implementation of "One
    pixel attack for…'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/Hyperparticle/one-pixel-attack-keras?source=post_page-----ec323555a11a--------------------------------)
    [## GitHub - Hyperparticle/one-pixel-attack-keras: Keras 实现的“单像素攻击”…'
- en: Keras implementation of "One pixel attack for fooling deep neural networks"
    using differential evolution on Cifar10 and…
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用差分进化在 Cifar10 上进行“单像素攻击以欺骗深度神经网络”的 Keras 实现和…
- en: 'github.com](https://github.com/Hyperparticle/one-pixel-attack-keras?source=post_page-----ec323555a11a--------------------------------)  [##
    GitHub - max-andr/square-attack: Square Attack: a query-efficient black-box adversarial
    attack via…'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/Hyperparticle/one-pixel-attack-keras?source=post_page-----ec323555a11a--------------------------------)  [##
    GitHub - max-andr/square-attack: Square Attack: 一个查询高效的黑箱对抗攻击，通过…'
- en: 'Square Attack: a query-efficient black-box adversarial attack via random search
    [ECCV 2020] - GitHub …'
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'Square Attack: 一个查询高效的黑箱对抗攻击，通过随机搜索 [ECCV 2020] - GitHub …'
- en: 'github.com](https://github.com/max-andr/square-attack?source=post_page-----ec323555a11a--------------------------------)
    [](https://github.com/kenny-co/procedural-advml?source=post_page-----ec323555a11a--------------------------------)
    [## GitHub - kenny-co/procedural-advml: Task-agnostic universal black-box attacks
    on computer vision…'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/max-andr/square-attack?source=post_page-----ec323555a11a--------------------------------)
    [](https://github.com/kenny-co/procedural-advml?source=post_page-----ec323555a11a--------------------------------)
    [## GitHub - kenny-co/procedural-advml: 无任务通用黑箱攻击计算机视觉…'
- en: Task-agnostic universal black-box attacks on computer vision neural network
    via procedural noise (CCS'19) - GitHub …
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无任务通用黑箱攻击计算机视觉神经网络通过过程噪声（CCS'19） - GitHub …
- en: github.com](https://github.com/kenny-co/procedural-advml?source=post_page-----ec323555a11a--------------------------------)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/kenny-co/procedural-advml?source=post_page-----ec323555a11a--------------------------------)'
