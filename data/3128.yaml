- en: 'Transforming text into vectors: TSDAE’s unsupervised approach to enhanced embeddings'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将文本转化为向量：TSDAE 的无监督方法用于增强嵌入
- en: 原文：[https://towardsdatascience.com/transforming-text-into-vectors-tsdaes-unsupervised-approach-to-enhanced-embeddings-728eb28ea701?source=collection_archive---------1-----------------------#2023-10-16](https://towardsdatascience.com/transforming-text-into-vectors-tsdaes-unsupervised-approach-to-enhanced-embeddings-728eb28ea701?source=collection_archive---------1-----------------------#2023-10-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/transforming-text-into-vectors-tsdaes-unsupervised-approach-to-enhanced-embeddings-728eb28ea701?source=collection_archive---------1-----------------------#2023-10-16](https://towardsdatascience.com/transforming-text-into-vectors-tsdaes-unsupervised-approach-to-enhanced-embeddings-728eb28ea701?source=collection_archive---------1-----------------------#2023-10-16)
- en: '[](https://medium.com/@silviaonofrei?source=post_page-----728eb28ea701--------------------------------)[![Silvia
    Onofrei](../Images/198b04b2063b4269eaff52402dc5f8d5.png)](https://medium.com/@silviaonofrei?source=post_page-----728eb28ea701--------------------------------)[](https://towardsdatascience.com/?source=post_page-----728eb28ea701--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----728eb28ea701--------------------------------)
    [Silvia Onofrei](https://medium.com/@silviaonofrei?source=post_page-----728eb28ea701--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@silviaonofrei?source=post_page-----728eb28ea701--------------------------------)[![Silvia
    Onofrei](../Images/198b04b2063b4269eaff52402dc5f8d5.png)](https://medium.com/@silviaonofrei?source=post_page-----728eb28ea701--------------------------------)[](https://towardsdatascience.com/?source=post_page-----728eb28ea701--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----728eb28ea701--------------------------------)
    [Silvia Onofrei](https://medium.com/@silviaonofrei?source=post_page-----728eb28ea701--------------------------------)'
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fab562e798558&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransforming-text-into-vectors-tsdaes-unsupervised-approach-to-enhanced-embeddings-728eb28ea701&user=Silvia+Onofrei&userId=ab562e798558&source=post_page-ab562e798558----728eb28ea701---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----728eb28ea701--------------------------------)
    ·11 min read·Oct 16, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F728eb28ea701&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransforming-text-into-vectors-tsdaes-unsupervised-approach-to-enhanced-embeddings-728eb28ea701&user=Silvia+Onofrei&userId=ab562e798558&source=-----728eb28ea701---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fab562e798558&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransforming-text-into-vectors-tsdaes-unsupervised-approach-to-enhanced-embeddings-728eb28ea701&user=Silvia+Onofrei&userId=ab562e798558&source=post_page-ab562e798558----728eb28ea701---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----728eb28ea701--------------------------------)
    ·11分钟阅读·2023年10月16日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F728eb28ea701&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransforming-text-into-vectors-tsdaes-unsupervised-approach-to-enhanced-embeddings-728eb28ea701&user=Silvia+Onofrei&userId=ab562e798558&source=-----728eb28ea701---------------------clap_footer-----------)'
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F728eb28ea701&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransforming-text-into-vectors-tsdaes-unsupervised-approach-to-enhanced-embeddings-728eb28ea701&source=-----728eb28ea701---------------------bookmark_footer-----------)![](../Images/ce4e9809888d34f2f0ee66fa4f5a7089.png)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F728eb28ea701&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransforming-text-into-vectors-tsdaes-unsupervised-approach-to-enhanced-embeddings-728eb28ea701&source=-----728eb28ea701---------------------bookmark_footer-----------)![](../Images/ce4e9809888d34f2f0ee66fa4f5a7089.png)'
- en: '[Designed by Freepik](http://www.freepik.com)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[设计来自 Freepik](http://www.freepik.com)'
- en: '*Combine TSDAE pre-training on a target domain with supervised fine-tuning
    on a general-purpose corpus to enhance the quality of the embeddings for a specialized
    domain.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*将 TSDAE 的目标领域预训练与通用语料库上的监督微调相结合，以提高专用领域嵌入的质量。*'
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: Embeddings encode text into high dimensional vector spaces, using dense vectors
    to represent words and to capture their semantic relationships. Recent developments
    in generative AI and LLM, such as context search and RAG rely heavily on the quality
    of their underlying embeddings. While the similarity searches use basic mathematical
    concepts such as cosine similarity, the methods used to build the embedding vectors
    significantly influence the subsequent outcomes.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入将文本编码为高维向量空间，使用密集向量来表示单词并捕捉其语义关系。生成式 AI 和大型语言模型（LLM）中的最新发展，如上下文搜索和 RAG，严重依赖于其底层嵌入的质量。虽然相似性搜索使用了如余弦相似度这样的基本数学概念，但构建嵌入向量的方法对后续结果有显著影响。
- en: In most cases, a pre-trained sentence transformer, will work out of the box
    and will provide reasonable results. There are many choices of BERT based pretrained
    contextual embeddings, some domain specialized, that can be used in these cases,
    and they are available for download from platforms such as HuggingFace.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，预训练的句子变换器可以开箱即用并提供合理的结果。对于这些情况，有许多基于 BERT 的预训练上下文嵌入可供选择，其中一些是领域专用的，可以从如
    HuggingFace 等平台下载。
- en: The issues arise when we are dealing with cases where the corpus contains many
    technical terms specific to a narrow domain or originates from low resource languages.
    In these cases, we need to address the *unknown words* that were not seen during
    pre-training or fine-tuning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理包含许多特定于狭窄领域的技术术语或来源于资源稀缺语言的语料库时，会出现问题。在这些情况下，我们需要处理在预训练或微调期间未见过的*未知词*。
- en: For example a model pre-trained on general text will have hard time to properly
    allocate vectors to titles from a corpus of mathematical research papers.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个在通用文本上预训练的模型将很难将向量准确分配给数学研究论文语料库中的标题。
- en: In these cases, since the model was not exposed to the domain specific words,
    it struggles to determine their meaning and to place them accurately in the vector
    space relative to other words from the corpus. The higher the number of unknown
    words the bigger the impact, and the lower the performance of the model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，由于模型没有接触到领域特定词汇，它难以确定这些词的含义，并将它们准确地放置在相对于语料库中其他词的向量空间中。未知词的数量越多，影响越大，模型的性能越低。
- en: Hence an out of the box pre-trained model will underperform in such scenarios,
    while attempting to pre-train a custom model faces challenges due to the lack
    of labelled data and the need of significant computational resources.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现成的预训练模型在这种情况下表现不佳，而尝试预训练自定义模型由于缺乏标注数据和需要大量计算资源而面临挑战。
- en: Motivation
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动机
- en: This work was incited by a recent research [[aviation_article](https://arxiv.org/abs/2305.09556)]
    that is focusing on the aviation domain whose data has unique characteristics
    such as technical jargon, abbreviations, and unconventional grammar.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作受到了一项近期研究[[aviation_article](https://arxiv.org/abs/2305.09556)]的启发，该研究专注于航空领域，其数据具有独特的特征，如技术术语、缩写和非常规语法。
- en: To address the lack of labelled data, the authors employed one of the most effective
    unsupervised techniques that allow for pre-training embeddings (TSDAE) followed
    by a fine-tuning stage that used labelled data from a general purpose corpus.
    The adapted sentence transformers outperform the general transformers, demonstrating
    the effectiveness of the approach in capturing the characteristics of the aviation
    domain data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决标注数据不足的问题，作者采用了一种最有效的无监督技术之一，即预训练嵌入（TSDAE），随后进行了使用来自通用语料库的标注数据的微调阶段。经过调整的句子变换器优于通用变换器，展示了该方法在捕捉航空领域数据特征方面的有效性。
- en: Outline
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大纲
- en: '[Domain adaptation](https://www.sbert.net/examples/domain_adaptation/README.html)
    is about tailoring text embeddings to a specific domain without the need for labeled
    training data. In this experiment, I am using a two-step approach which, according
    to the [[tsdae_article](https://arxiv.org/abs/2104.06979)], works better than
    just training on the target domain.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[领域适应](https://www.sbert.net/examples/domain_adaptation/README.html)是将文本嵌入调整到特定领域而无需标注训练数据。在本实验中，我使用了一个两步法，根据[[tsdae_article](https://arxiv.org/abs/2104.06979)]，这种方法比仅在目标领域上训练效果更好。'
- en: '![](../Images/6bb26c7aa0c2ff05cfbd30aa19fcf5c0.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6bb26c7aa0c2ff05cfbd30aa19fcf5c0.png)'
- en: Image by the Author
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'Firstly, I start with pre-training focused on the target domain, often termed
    as adaptive pre-training. This phase demands a collection of sentences from our
    dataset. I employ TSDAE for this stage, a method that excels in domain adaptation
    as a pre-training task, significantly surpassing other methods, including the
    masked language model, as emphasized in [[tsdae_article](https://arxiv.org/abs/2104.06979)].
    I am closely following the script: t[rain_tsdae_from_file.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/unsupervised_learning/TSDAE/train_tsdae_from_file.py).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我开始进行专注于目标领域的预训练，通常称为适应性预训练。此阶段需要从我们的数据集中收集句子。我在这个阶段使用 TSDAE，这是一种在作为预训练任务的领域适应方面表现出色的方法，显著超越了包括掩码语言模型在内的其他方法，正如
    [[tsdae_article](https://arxiv.org/abs/2104.06979)] 中所强调的。我正紧密跟随脚本：t[rain_tsdae_from_file.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/unsupervised_learning/TSDAE/train_tsdae_from_file.py)。
- en: Subsequently, I fine-tune the model on the generic labeled AllNLI dataset, employing
    a multiple negative ranking loss strategy. For this stage I am using the script
    from [training_nli_v2.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/nli/training_nli_v2.py).
    As documented in [[tsdae_article](https://arxiv.org/abs/2104.06979)], this additional
    step not only counters over-fitting but also significantly improves the model’s
    performance.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我在通用标记 AllNLI 数据集上对模型进行微调，采用了多重负采样排名损失策略。在此阶段，我使用来自 [training_nli_v2.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/nli/training_nli_v2.py)
    的脚本。正如 [[tsdae_article](https://arxiv.org/abs/2104.06979)] 中所记录的，这一步骤不仅能防止过拟合，还能显著提升模型性能。
- en: TSDAE — Pre-Training on the Target Domain
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TSDAE — 针对目标领域的预训练
- en: TSDAE (*transformer-based sequential denoising auto-encoder)* is an unsupervised
    sentence embedding method, that was first introduced by K. Wang, N. Reimers and
    I. Gurevych in [[tsdae_article](https://arxiv.org/abs/2104.06979)].
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: TSDAE（*基于变换器的序列去噪自编码器*）是一种无监督的句子嵌入方法，首次由 K. Wang、N. Reimers 和 I. Gurevych 在
    [[tsdae_article](https://arxiv.org/abs/2104.06979)] 中提出。
- en: TSDAE uses a modified encoder-decoder transformer design where the key and the
    value of the cross-attention are confined to the sentence embedding. I’ll outline
    the details in the context of the optimal architecture choices highlighted in
    the original paper [[tsdae_article](https://arxiv.org/abs/2104.06979)].
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: TSDAE 使用了修改过的编码器-解码器变换器设计，其中交叉注意力的键和值被限制在句子嵌入中。我将详细说明原文中突出显示的最佳架构选择 [[tsdae_article](https://arxiv.org/abs/2104.06979)]。
- en: '![](../Images/696638076de7ed5792bc391838896cd4.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/696638076de7ed5792bc391838896cd4.png)'
- en: Image by the Author
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: The dataset consists of unlabeled sentences, which during pre-processing are
    corrupted by deleting 60% of their content to introduce input noise.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集由未标记的句子组成，在预处理过程中，通过删除 60% 的内容来引入输入噪声，从而使句子受到破坏。
- en: The Encoder transforms the corrupted sentences into fixed-sized vectors by pooling
    their word embeddings. According to [[tsdae_article](https://arxiv.org/abs/2104.06979)],
    using the CLS pooling method is recommended for extracting the sentence vector.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器通过池化词嵌入将损坏的句子转换为固定大小的向量。根据 [[tsdae_article](https://arxiv.org/abs/2104.06979)]，推荐使用
    CLS 池化方法来提取句子向量。
- en: The Decoder is required to reconstruct the original input sentence from the
    damaged sentence embedding. The authors advise tying the Encoder and Decoder parameters
    during training to reduce the number of parameters in the model, making it easier
    to train and less prone to over-fitting while not affecting the performance.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器需要从损坏的句子嵌入中重构原始输入句子。作者建议在训练过程中将编码器和解码器的参数绑定，以减少模型中的参数数量，从而使训练更加容易，且不易过拟合，同时不会影响性能。
- en: For good reconstruction quality, the sentence embedding from the Encoder must
    optimally capture the semantics. A pretrained transformer such as `bert-base-uncased`
    is used for the Encoder, while the Decoder’s weights are copied from it.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保证良好的重构质量，编码器的句子嵌入必须最佳地捕捉语义。预训练的变换器如 `bert-base-uncased` 被用于编码器，而解码器的权重则从中复制。
- en: The Decoder’s attention mechanism is restricted to the sentence representation
    produced by the Encoder. This modification from the original transformer encoder-decoder
    architecture, limits the information the Decoder retrieves from the Encoder, and
    introduces a bottleneck that forces the Encoder to produce meaningful sentence
    representations.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器的注意力机制仅限于编码器生成的句子表示。这种对原始变换器编码器-解码器架构的修改限制了解码器从编码器中检索的信息，并引入了一个瓶颈，迫使编码器生成有意义的句子表示。
- en: At inference, only the Encoder is used to create sentence embeddings.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在推断时，仅使用编码器来创建句子嵌入。
- en: 'The model is trained to reconstruct the clean sentence from the corrupted sentence
    and this is accomplished by maximizing the objective:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型经过训练，以从损坏的句子中重建干净的句子，这通过最大化目标来完成：
- en: '![](../Images/680c448c34b890f8685787af0730455e.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/680c448c34b890f8685787af0730455e.png)'
- en: Image by the Author
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: AllNLI — Natural Language Inference Dataset
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AllNLI — 自然语言推断数据集
- en: Natural Language Inference (NLI) determines the relationship between two sentences.
    It categorizes the truth of the hypothesis (second sentence) as entailment (true
    based on the premise), contradiction (false based on the premise), or neutral
    (neither guaranteed nor contradicted by the premise). NLI datasets are large labeled
    datasets where pairs of sentences are annotated with their relationship class.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言推断（NLI）确定两个句子之间的关系。它将假设（第二个句子）的真实性分类为蕴含（基于前提的真实）、矛盾（基于前提的虚假）或中立（既不由前提保证也不被前提反驳）。NLI
    数据集是大型标注数据集，其中句子对被标注为它们的关系类别。
- en: 'For this experiment, I use the AllNLI dataset that contains a collection of
    more than 900K records, from combined Stanford Natural Language Inference (SNLI)
    and MultiNLI datasets. This dataset can be downloaded from: [AllNLI download site](http://sbert.net/datasets).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这项实验，我使用了 AllNLI 数据集，其中包含来自合并的斯坦福自然语言推断（SNLI）和 MultiNLI 数据集的超过 900K 条记录。此数据集可以从以下网址下载：[AllNLI
    下载站点](http://sbert.net/datasets)。
- en: Load and Prepare the Pre-training Data
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载和准备预训练数据
- en: To build our domain specific data, we are using the [Kaggle arXiv dataset](https://www.kaggle.com/datasets/Cornell-University/arxiv)
    comprised roughly of about 1.7M scholarly STEM papers sourced from the established
    electronic preprint platform, [arXiv](https://arxiv.org/). Besides title, abstract
    and authors, there is a significant amount of metadata associated with each article.
    However, here we are concerned only with the titles.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建我们的领域特定数据，我们使用了[Kaggle arXiv 数据集](https://www.kaggle.com/datasets/Cornell-University/arxiv)，大约包含
    1.7M 篇学术 STEM 论文，来源于知名电子预印本平台，[arXiv](https://arxiv.org/)。除了标题、摘要和作者之外，每篇文章还附有大量元数据。然而，这里我们只关心标题。
- en: 'After download, I’ll select the mathematics preprints. Given the hefty size
    of the Kaggle file, I’ve added a reduced version of the mathematics papers file
    to Github for easier access. However, if you’re inclined towards a different subject,
    download the dataset, and replace `math` with your desired topic in the code below:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 下载后，我将选择数学预印本。鉴于 Kaggle 文件的庞大，我已将数学论文文件的缩减版本添加到 Github 以便更容易访问。然而，如果你倾向于其他主题，请下载数据集，并在以下代码中将`math`替换为你所需的主题：
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: I’ve loaded our dataset into a Pandas dataframe `df`. A quick inspection shows
    that the reduced dataset contains 55,497 preprints—a more practical size for our
    experiment. While the [[tsdae_article](https://arxiv.org/abs/2104.06979)] suggests
    around 10K entries are adequate, I'll keep the entire reduced dataset. Mathematics
    titles might have LaTeX code, which I'll swap for ISO code to optimize processing.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我已将数据集加载到 Pandas 数据框 `df` 中。快速检查显示，减少后的数据集包含 55,497 篇预印本——这是我们实验的更实际的大小。虽然[[tsdae_article](https://arxiv.org/abs/2104.06979)]建议约
    10K 条目是足够的，我会保留整个减少后的数据集。数学标题可能包含 LaTeX 代码，我将其替换为 ISO 代码以优化处理。
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'I’ll use the `parsed_title` entries for training, so let’s extract them as
    a list:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我将使用`parsed_title`条目进行训练，所以让我们将它们提取为列表：
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Next, let’s form the corrupted sentences by removing approximately 60% of tokens
    from each entry. If you’re interested in exploring further or trying different
    deletion ratios, check out the [denoising script](https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/datasets/DenoisingAutoEncoderDataset.py).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们通过从每个条目中删除大约 60% 的标记来形成损坏的句子。如果你有兴趣进一步探索或尝试不同的删除比例，请查看[去噪脚本](https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/datasets/DenoisingAutoEncoderDataset.py)。
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let’s take a look at what happened to one entry after processing:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看处理后的一个条目发生了什么：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As you notice, `Bethe equations` and `model` were removed from the initial text.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所注意到的，`Bethe equations` 和 `model` 从初始文本中被移除了。
- en: 'The last step in our data processing is to load the dataset in batches:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据处理的最后一步是按批次加载数据集：
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: TSDAE Training
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TSDAE 训练
- en: While I’ll be following the approach from the t[rain_tsdae_from_file.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/unsupervised_learning/TSDAE/train_tsdae_from_file.py),
    I’ll construct it step by step for better understanding.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我将遵循 t[rain_tsdae_from_file.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/unsupervised_learning/TSDAE/train_tsdae_from_file.py)
    中的方法，但我将一步步构建以便更好地理解。
- en: 'Start with selecting a pre-trained transformer checkpoint, and stick with the
    default option:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 从选择一个预训练的转换器检查点开始，并坚持使用默认选项：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Choose `CLS` as the pooling method and specify the dimension of the vectors
    to be constructed:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 选择 `CLS` 作为池化方法，并指定要构建的向量维度：
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next build the sentence transformer by combining the two layers:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，通过组合两层来构建句子转换器：
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Lastly, specify the loss function and tie the encoder-decoder parameters for
    the training phase.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，指定损失函数并在训练阶段绑定编码器-解码器参数。
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now, we’re set to invoke the fit method and train the model. I’ll also store
    it for the subsequent steps. You’re welcome to tweak the hyperparameters to optimize
    your experiment.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备调用 fit 方法并训练模型。我还会将其存储以备后续步骤使用。欢迎您调整超参数以优化实验。
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '*The pre-training stage took about 15 min on a Google Colab Pro instance with
    A100 GPU set on High-RAM.*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*预训练阶段在 Google Colab Pro 实例上使用 A100 GPU 高内存设置，大约花费了 15 分钟。*'
- en: Fine-Tuning on AllNLI Dataset
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 AllNLI 数据集上的微调
- en: 'Let’s start by downloading the AllNLI dataset:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始下载 AllNLI 数据集：
- en: '[PRE11]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, unzip the file and parse the data for training:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，解压文件并解析数据以进行训练：
- en: '[PRE12]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The training dataset has about 563K training samples. Finally, use a special
    loader that loads the data in batches, and avoids duplicates within a batch:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据集大约有 56万3千 个训练样本。最后，使用一个特殊的数据加载器以批量形式加载数据，并避免批量内的重复：
- en: '[PRE13]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The batch size I use here is smaller than the default size of `128` from the
    script. Although a larger batch would give better results, it would require more
    GPU memory, and since I am limited by my computational resources, I choose a smaller
    batch size.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里使用的批量大小比脚本中的默认大小 `128` 要小。虽然更大的批量会给出更好的结果，但它需要更多的 GPU 内存，而由于我的计算资源有限，我选择了较小的批量大小。
- en: Finally, fine-tune the pre-trained model on the AllNLI dataset using MultipleRankingLoss.
    Entailment pairs are positive and the contradiction pairs are hard negatives.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用 MultipleRankingLoss 对 AllNLI 数据集上的预训练模型进行微调。蕴含对是正样本，而矛盾对是困难的负样本。
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '*I fine-tuned the model on the entire 500K dataset, and this took about 40
    min on the Google Colab Pro, for 1 epoch with batch size of 32.*'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*我在整个 50万 数据集上对模型进行了微调，这大约花了 40 分钟，在 Google Colab Pro 上，进行了 1 个周期，批量大小为 32。*'
- en: Evaluate TSDAE Pre-Trained Model and the Fine-Tuned Model
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估 TSDAE 预训练模型和微调模型
- en: I will conduct some preliminary evaluation on the STS (semantic textual similarity)
    dataset from HuggingFace, using the `EmbeddingSimilarityEvaluator`, which returns
    the Spearman rank correlation. However, these evaluations don't employ the specific
    domain I am focusing on, potentially not showcasing the model's true performance.
    For details see Section 4 in [[tsdae_article](https://arxiv.org/abs/2104.06979)].
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我将对 HuggingFace 的 STS（语义文本相似性）数据集进行一些初步评估，使用 `EmbeddingSimilarityEvaluator`，它返回斯皮尔曼等级相关系数。然而，这些评估并没有使用我关注的特定领域，可能未能展示模型的真实表现。详细信息见
    [[tsdae_article](https://arxiv.org/abs/2104.06979)] 第4节。
- en: 'I start with downloading the dataset from HuggingFace and specifying the `validation`
    subset:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我从 HuggingFace 下载数据集，并指定 `validation` 子集：
- en: '[PRE15]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This is a Dataset object of the form:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个形式为的数据集对象：
- en: '[PRE16]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: To better understand it, let’s take a look at one specific entry
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解，让我们看一个具体的条目
- en: '[PRE17]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As we can see from this example, each entry has 4 features, one is the index,
    two sentences and a label (which was created by a human annotator). The label
    can take values between `0` and `5` and measures the similarity level of the two
    sentences (with `5` being most similar). In this example the two sentences are
    on completely different topics.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个例子中我们可以看到，每个条目有4个特征，一个是索引，两个句子和一个标签（由人工标注者创建）。标签的值可以在 `0` 和 `5` 之间，并测量两个句子的相似程度（`5`
    为最相似）。在这个例子中，这两个句子完全在不同的主题上。
- en: To evaluate the model, sentence embeddings for the pairs of sentences are created,
    and the cosine similarity score for each pair is computed. The Spearman rank correlation
    between the labels and the similarity scores is computed as the evaluation score.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估模型，我们为句子对创建句子嵌入，并计算每对的余弦相似度分数。标签与相似度分数之间的斯皮尔曼等级相关性作为评估分数。
- en: 'Since I’ll be using cosine similarity which takes values between 0 and 1, I
    have to normalize the labels:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我将使用取值范围在 0 到 1 之间的余弦相似度，因此需要对标签进行归一化：
- en: '[PRE18]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Wrap the data in the `InputExample` class from HuggingFace:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据封装在 HuggingFace 的 `InputExample` 类中：
- en: '[PRE19]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Create the evaluator based on `EmbeddingSimilarityEvaluator` class in `sentence-transformers`
    library.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个基于 `sentence-transformers` 库中 `EmbeddingSimilarityEvaluator` 类的评估器。
- en: '[PRE20]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We compute the scores for TSDAE model, for the fine-tuned model and for a couple
    of pre-trained sentence transformers:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算了 TSDAE 模型、微调模型和几个预训练句子转换器的得分：
- en: '![](../Images/c4b1c1e0ea3727afee930f65f6b50c6b.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4b1c1e0ea3727afee930f65f6b50c6b.png)'
- en: Image by the Author
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Thus on a general scope dataset, some pre-trained models, such as `all-mpnet-base-v2`
    outperforms the TSDAE fine-tuned model. However, by pre-training, the performance
    of the initial model `bert-base-uncased` more than doubled. It is conceivable
    that better results could be attained by further tweaking the hyperparameters
    for fine-tuning.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在一般范围的数据集上，一些预训练模型，如 `all-mpnet-base-v2`，表现优于 TSDAE 微调模型。然而，通过预训练，初始模型 `bert-base-uncased`
    的性能提高了两倍以上。可以想象，通过进一步调整超参数进行微调，可能会获得更好的结果。
- en: Conclusion
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: For low resources domains, TSDAE in conjunction with fine-tuning is a rather
    efficient strategy for building embeddings. The results obtained here are noteworthy,
    given the amount of data and the computational means. However, for datasets that
    are not particularly unusual or domain specific, taking efficiency and cost into
    account, it might be preferable to choose a pretrained embedding that could provide
    comparable performance.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对于资源有限的领域，将 TSDAE 与微调结合起来是一种相当高效的构建嵌入的策略。考虑到数据量和计算资源，这里的结果相当值得注意。然而，对于那些并不特别异常或领域特定的数据集，考虑到效率和成本，选择一个预训练的嵌入可能会提供可比的性能。
- en: '[**Gihub Link**](https://github.com/SolanaO/Blogs_Content/tree/master/tsdae)
    **to Colab Notebook and sample dataset**.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[**Gihub 链接**](https://github.com/SolanaO/Blogs_Content/tree/master/tsdae)
    **到 Colab 笔记本和示例数据集**。'
- en: '*And so, my friends, we should always embrace the good, the bad, and the messy
    on our learning journey!*'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '*因此，我的朋友们，我们在学习旅程中应该始终拥抱好、坏和混乱的东西！*'
- en: References
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[tsdae_article]. K. Wang, et al., [TSDAE*:* Using Transformer-based Sequential
    Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning](https://arxiv.org/abs/2104.06979)
    (2021) arXiv:2104.06979'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[tsdae_article]. K. Wang 等，[TSDAE：使用基于 Transformer 的序列去噪自编码器进行无监督句子嵌入学习](https://arxiv.org/abs/2104.06979)
    (2021) arXiv:2104.06979'
- en: '[aviation_article]. L. Wang, et al., [Adapting Sentence Transformers for the
    Aviation Domain](https://arxiv.org/pdf/2305.09556.pdf) (2023) arXiv:2305.09556'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[aviation_article]. L. Wang 等，[为航空领域调整句子转换器](https://arxiv.org/pdf/2305.09556.pdf)
    (2023) arXiv:2305.09556'
