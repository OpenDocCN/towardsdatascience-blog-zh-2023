- en: The Transformer Architecture of GPT Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT模型的Transformer架构
- en: 原文：[https://towardsdatascience.com/the-transformer-architecture-of-gpt-models-b8695b48728b?source=collection_archive---------3-----------------------#2023-07-25](https://towardsdatascience.com/the-transformer-architecture-of-gpt-models-b8695b48728b?source=collection_archive---------3-----------------------#2023-07-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-transformer-architecture-of-gpt-models-b8695b48728b?source=collection_archive---------3-----------------------#2023-07-25](https://towardsdatascience.com/the-transformer-architecture-of-gpt-models-b8695b48728b?source=collection_archive---------3-----------------------#2023-07-25)
- en: Learn the details of the Transformer architecture
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解Transformer架构的详细信息
- en: '[](https://medium.com/@bea_684?source=post_page-----b8695b48728b--------------------------------)[![Beatriz
    Stollnitz](../Images/63a2a7daeca6d93e26b3ac0556c42aa1.png)](https://medium.com/@bea_684?source=post_page-----b8695b48728b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b8695b48728b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b8695b48728b--------------------------------)
    [Beatriz Stollnitz](https://medium.com/@bea_684?source=post_page-----b8695b48728b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bea_684?source=post_page-----b8695b48728b--------------------------------)[![Beatriz
    Stollnitz](../Images/63a2a7daeca6d93e26b3ac0556c42aa1.png)](https://medium.com/@bea_684?source=post_page-----b8695b48728b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b8695b48728b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b8695b48728b--------------------------------)
    [Beatriz Stollnitz](https://medium.com/@bea_684?source=post_page-----b8695b48728b--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1c8863892480&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-transformer-architecture-of-gpt-models-b8695b48728b&user=Beatriz+Stollnitz&userId=1c8863892480&source=post_page-1c8863892480----b8695b48728b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b8695b48728b--------------------------------)
    ·22 min read·Jul 25, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb8695b48728b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-transformer-architecture-of-gpt-models-b8695b48728b&user=Beatriz+Stollnitz&userId=1c8863892480&source=-----b8695b48728b---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1c8863892480&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-transformer-architecture-of-gpt-models-b8695b48728b&user=Beatriz+Stollnitz&userId=1c8863892480&source=post_page-1c8863892480----b8695b48728b---------------------post_header-----------)
    发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----b8695b48728b--------------------------------)
    · 22分钟阅读 · 2023年7月25日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb8695b48728b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-transformer-architecture-of-gpt-models-b8695b48728b&user=Beatriz+Stollnitz&userId=1c8863892480&source=-----b8695b48728b---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb8695b48728b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-transformer-architecture-of-gpt-models-b8695b48728b&source=-----b8695b48728b---------------------bookmark_footer-----------)![](../Images/b27fc545e177f9112abf27cfac0ceebb.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb8695b48728b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-transformer-architecture-of-gpt-models-b8695b48728b&source=-----b8695b48728b---------------------bookmark_footer-----------)![](../Images/b27fc545e177f9112abf27cfac0ceebb.png)'
- en: Photo by [fabio](https://unsplash.com/@fabioha?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[fabio](https://unsplash.com/@fabioha?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In 2017, authors from Google published a paper called [Attention is All You
    Need](https://arxiv.org/abs/1706.03762) in which they introduced the Transformer
    architecture. This new architecture achieved unparalleled success in language
    translation tasks, and the paper quickly became essential reading for anyone immersed
    in the area. Like many others, when I read the paper for the first time, I could
    see the value of its innovative ideas, but I didn’t realize just how disruptive
    the paper would be to other areas under the broader umbrella of AI. Within a few
    years, researchers adapted the Transformer architecture to many tasks other than
    language translation, including image classification, image generation, and protein
    folding problems. In particular, the Transformer architecture revolutionized text
    generation and paved the way for GPT models and the exponential growth we’re currently
    experiencing in AI.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 2017 年，谷歌的作者们发布了一篇名为 [Attention is All You Need](https://arxiv.org/abs/1706.03762)
    的论文，在其中他们介绍了 Transformer 架构。这种新架构在语言翻译任务中取得了前所未有的成功，论文很快成为了从事这一领域的人员必读的文献。像许多人一样，当我第一次阅读这篇论文时，我能看出其创新想法的价值，但我没有意识到这篇论文会对
    AI 更广泛领域产生多么深远的影响。在几年的时间里，研究人员将 Transformer 架构应用于除语言翻译之外的许多任务，包括图像分类、图像生成和蛋白质折叠问题。特别是，Transformer
    架构彻底改变了文本生成，并为 GPT 模型和我们当前在 AI 领域经历的指数级增长铺平了道路。
- en: Given how pervasive Transformer models are these days, both in the industry
    and academia, understanding the details of how they work is an important skill
    for every AI practitioner. This article will focus mostly on the architecture
    of GPT models, which are built using a subset of the original Transformer architecture,
    but it will also cover the original Transformer at the end. For the model code…
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于当前 Transformer 模型在工业界和学术界的广泛应用，理解它们的工作细节对每一位 AI 从业者来说都是一项重要技能。本文将主要关注 GPT
    模型的架构，这些模型是基于原始 Transformer 架构的一个子集构建的，但也会在最后涉及原始 Transformer 架构。关于模型代码……
