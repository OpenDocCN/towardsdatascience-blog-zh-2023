- en: How I Leveraged Open Source LLMs to Achieve Massive Savings on a Large Compute
    Project
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何利用开源LLM在大型计算项目中实现巨额节省
- en: 原文：[https://towardsdatascience.com/how-i-leveraged-open-source-llms-to-achieve-massive-savings-on-a-large-compute-project-bd8bb3c7267](https://towardsdatascience.com/how-i-leveraged-open-source-llms-to-achieve-massive-savings-on-a-large-compute-project-bd8bb3c7267)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-i-leveraged-open-source-llms-to-achieve-massive-savings-on-a-large-compute-project-bd8bb3c7267](https://towardsdatascience.com/how-i-leveraged-open-source-llms-to-achieve-massive-savings-on-a-large-compute-project-bd8bb3c7267)
- en: Unlocking Cost-Efficiency in Large Compute Projects with Open Source LLMs and
    GPU Rentals.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用开源LLM和GPU租赁实现大型计算项目的成本效益。
- en: '[](https://medium.com/@ryanshrott?source=post_page-----bd8bb3c7267--------------------------------)[![Ryan
    Shrott](../Images/186524066383b4b02c994692aebb3ea5.png)](https://medium.com/@ryanshrott?source=post_page-----bd8bb3c7267--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bd8bb3c7267--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bd8bb3c7267--------------------------------)
    [Ryan Shrott](https://medium.com/@ryanshrott?source=post_page-----bd8bb3c7267--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ryanshrott?source=post_page-----bd8bb3c7267--------------------------------)[![Ryan
    Shrott](../Images/186524066383b4b02c994692aebb3ea5.png)](https://medium.com/@ryanshrott?source=post_page-----bd8bb3c7267--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bd8bb3c7267--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bd8bb3c7267--------------------------------)
    [Ryan Shrott](https://medium.com/@ryanshrott?source=post_page-----bd8bb3c7267--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bd8bb3c7267--------------------------------)
    ·6 min read·Aug 30, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bd8bb3c7267--------------------------------)
    ·阅读时间6分钟·2023年8月30日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/83382f63b21df0de2b0d47d33ad7212d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/83382f63b21df0de2b0d47d33ad7212d.png)'
- en: Photo by [Alexander Grey](https://unsplash.com/@sharonmccutcheon?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Alexander Grey](https://unsplash.com/@sharonmccutcheon?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: In the world of large language models (LLMs), the cost of computation can be
    a significant barrier, especially for extensive projects. I recently embarked
    on a project that required running 4,000,000 prompts with an average input length
    of 1000 tokens and an average output length of 200 tokens. That’s nearly 5 billion
    tokens! The traditional approach of paying per token, as is common with models
    like GPT-3.5 and GPT-4, would have resulted in a hefty bill. However, I discovered
    that by leveraging open source LLMs, I could shift the pricing model to pay per
    hour of compute time, leading to substantial savings. This article will detail
    the approaches I took and compare and contrast each of them. Please note that
    while I share my experience with pricing, these are subject to change and may
    vary depending on your region and specific circumstances. The key takeaway here
    is the potential cost savings when leveraging open source LLMs and renting a GPU
    per hour, rather than the specific prices quoted.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型语言模型（LLM）的世界中，计算成本可能是一个重大障碍，尤其是在大型项目中。我最近开始了一个需要运行4,000,000个提示的项目，平均输入长度为1000个tokens，平均输出长度为200个tokens。这接近50亿tokens！传统的按token计费的方法（如GPT-3.5和GPT-4所用的）会导致高额账单。然而，我发现通过利用开源LLM，我可以将定价模型转变为按小时计算计算时间，从而节省大量成本。本文将详细说明我采取的方法，并对每种方法进行比较。请注意，虽然我分享了定价经验，但这些费用可能会变化，并可能因您的地区和具体情况而有所不同。关键点是，利用开源LLM和按小时租赁GPU的成本节省潜力，而不是具体的报价。
- en: ChatGPT API
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ChatGPT API
- en: 'I conducted an initial test using GPT-3.5 and GPT-4 on a small subset of my
    prompt input data. Both models demonstrated commendable performance, but GPT-4
    consistently outperformed GPT-3.5 in a majority of the cases. To give you a sense
    of the cost, running all 4 million prompts using the Open AI API would look something
    like this:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我在一个小数据子集上使用了GPT-3.5和GPT-4进行了初步测试。两个模型都表现出色，但GPT-4在大多数情况下始终优于GPT-3.5。为了让你了解成本，通过Open
    AI API运行全部400万个提示的成本大致如下：
- en: '![](../Images/35209c957d0dc0f7f6be806ff60f6cd8.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/35209c957d0dc0f7f6be806ff60f6cd8.png)'
- en: Total cost of running 4mm prompts with input length of 1000 tokens and 200 token
    output length
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 运行400万个提示的总成本，输入长度为1000个tokens，输出长度为200个tokens
- en: While GPT-4 did offer some performance benefits, the cost was disproportionately
    high compared to the incremental performance it added to my outputs. Conversely,
    GPT-3.5 Turbo, although more affordable, fell short in terms of performance, making
    noticeable errors on 2–3% of my prompt inputs. Given these factors, I wasn’t prepared
    to invest $7,600 on a project that was essentially a personal endeavor.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 GPT-4 提供了一些性能上的好处，但相较于其对我的输出所增加的增量性能，成本却过高。相比之下，尽管 GPT-3.5 Turbo 更加实惠，但在性能上有所欠缺，在
    2–3% 的提示输入中出现明显错误。鉴于这些因素，我不愿意在一个本质上是个人项目的项目上投资 $7,600。
- en: Open-Source Models
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开源模型
- en: 'Open-source models to the rescue! With open-source models, the pricing model
    is very different: **you only pay per hour of compute time**. Therefore, your
    goal is to maximize compute iterations per hour. Also, with solutions like Petals.ml,
    you can run your compute for free (with limitations)!'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 开源模型来救援！使用开源模型，定价模式非常不同：**你只需按计算时间的小时收费**。因此，你的目标是最大化每小时的计算迭代次数。此外，像 Petals.ml
    这样的解决方案，你可以免费运行你的计算（有一定限制）！
- en: After trying various models on Hugging Face, I found that a fine-tuned version
    of LLama-2 70B called [Stable Beluga 2](https://huggingface.co/stabilityai/StableBeluga2)
    gave me excellent performance without any fine tuning needed! It performed better
    than GPT-3.5 Turbo, but slightly below GPT-4\. However, this model is still very
    large and requires a very beefy GPU. It’s often best practice to use the smallest
    model possible to complete your task efficiently. Therefore, I tried out the 7B
    version of Stable Beluga 2\. The performance was not good enough!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试了 Hugging Face 上的各种模型后，我发现一个名为 [Stable Beluga 2](https://huggingface.co/stabilityai/StableBeluga2)
    的 LLama-2 70B 微调版本给了我很好的性能，无需任何微调！它的表现优于 GPT-3.5 Turbo，但略低于 GPT-4。然而，这个模型仍然非常庞大，需要非常强大的
    GPU。通常，最佳实践是使用尽可能小的模型来高效完成任务。因此，我尝试了 Stable Beluga 2 的 7B 版本。性能仍然不够好！
- en: Fine-tuning
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调
- en: 'To increase the performance for my task, I used GPT-4 and [Petals.ml](https://github.com/bigscience-workshop/petals)
    (Stable Beluga 2 70B) to generate a fine-tuning dataset. I generated 25K prompt
    completion pairs using Petals.ml and 2K using GPT-4 API. As a reminder Petals.ml
    allows you to run open source LLM models for free using bit-torrent technology.
    The inference time is not great though: it would have taken over a year to do
    4mm iterations.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提升任务性能，我使用了 GPT-4 和 [Petals.ml](https://github.com/bigscience-workshop/petals)（Stable
    Beluga 2 70B）来生成微调数据集。我通过 Petals.ml 生成了 25K 个提示完成对，并通过 GPT-4 API 生成了 2K 个。请注意，Petals.ml
    允许你使用 bit-torrent 技术免费运行开源 LLM 模型。然而，推理时间并不理想：进行 4mm 迭代需要超过一年。
- en: 'Also, Petals.ml hosts this model for free. Therefore, I utilized Petals.ml
    to generate 25,000 prompt completion pairs. I also used the GPT-4 API to generate
    an additional 2K prompt completion pairs. In hindsight, I probably overdid it
    on the training data. I’ve read reports showing that as few as 500 fine tuning
    samples can be enough to improve the performance of an LLM. Anyway, here is the
    total cost of running 2K prompts using Open AI API:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Petals.ml 免费托管了这个模型。因此，我利用 Petals.ml 生成了 25,000 个提示完成对。 我还使用了 GPT-4 API 生成了额外的
    2K 个提示完成对。事后看来，我可能在训练数据上做得过头了。我阅读过报告显示，少至 500 个微调样本就足以提升 LLM 的性能。无论如何，这里是使用 Open
    AI API 运行 2K 提示的总成本：
- en: '![](../Images/80c052ebe7c81d606c55085777984208.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80c052ebe7c81d606c55085777984208.png)'
- en: Total cost of running 2K prompt completion pairs
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 2K 个提示完成对的总成本
- en: 'That’s right: $84\. Can you guess what happened next? Using this newly acquired
    27K prompt completion pairs, I fine-tuned the 7B variant of the smallest LLama
    2 7B, and voila, this new model performed extremely well for my use case. The
    total fine-tuning cost was only $6.6 as I rented an A100 GPU for 6 hours. **In
    the end, my fine-tuned model performed better than GPT-3.5, and slightly worse
    than GPT-4.**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 没错：$84。你能猜到接下来发生了什么吗？使用这 27K 个提示完成对，我对最小的 LLama 2 7B 进行了微调，结果，这个新模型在我的用例中表现极其出色。总微调成本仅为
    $6.6，因为我租用了一个 A100 GPU 6 小时。**最终，我微调后的模型表现优于 GPT-3.5，略逊于 GPT-4。**
- en: Inference
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理
- en: 'Now let’s talk about inference compute costs. My model requires at least 20GB
    VRAM, so we’ll require at least an RTX 3090\. There are four great options currently
    available for GPU rentals: AWS, LambdaLabs, [RunPod](https://runpod.io/?ref=lemrt56t)
    and [Vast.AI](https://cloud.vast.ai/?ref_id=79595). From my experience, Vast.AI
    has the best prices, but the worst reliability. AWS is super reliable but is more
    expensive than the other options. RunPod has great prices and a great UI. I personally
    wanted to keep my costs as low as possible, so I decided to go with Vast.AI for
    this project. Vast.AI is a peer-to-peer GPU rental service, so it’s likely also
    the least secure. After scouring Vast.AI for the best price to performance ratio
    servers available, here are the best options I found:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们谈谈推断计算成本。我的模型至少需要20GB的VRAM，因此我们至少需要RTX 3090。目前，有四个很好的GPU租赁选项：AWS、LambdaLabs、[RunPod](https://runpod.io/?ref=lemrt56t)和[Vast.AI](https://cloud.vast.ai/?ref_id=79595)。根据我的经验，Vast.AI的价格最好，但可靠性最差。AWS非常可靠，但比其他选项更贵。RunPod价格优越，用户界面也很好。我个人想尽量降低成本，所以决定使用Vast.AI进行这个项目。Vast.AI是一个点对点GPU租赁服务，因此它可能也是最不安全的。经过在Vast.AI上寻找最佳性价比的服务器后，以下是我找到的最佳选项：
- en: '![](../Images/387407f56c236b653480400d7dc503f7.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/387407f56c236b653480400d7dc503f7.png)'
- en: Vast.AI total project costs using fune tuned LLama2 7b model. The total cost
    is computed using the number of iterations per second, the price per hour of GPU
    rental, and the total iterations required.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Vast.AI使用微调LLama2 7b模型的总项目成本。总成本是通过每秒的迭代次数、GPU租赁的每小时价格和所需的总迭代次数来计算的。
- en: As shown above, the total cost of inference can be less than $99\. In my case,
    I spun up around 10 servers and computed the number of iterations per second I
    could yield with each. For this particular project, which leveraged a fine-tuned
    variant of Llama2–7B, the RTX A5000 was a great option in terms of price to performance.
    I should also mention that none of this would be possible without the INSANE speeds
    of the open-source library [VLLM](https://github.com/vllm-project/vllm). My compute
    is 100% powered by VLLM under the hood. **Without VLLM, the compute time would
    increase by 20 times!** You may also look at the total runtime and laugh — 638
    hours is 26 days; however, you could easily parallelize the task across multiple
    GPUs/servers.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，推断的总成本可以低于$99。在我的案例中，我启动了大约10台服务器，并计算了每台服务器每秒能产生的迭代次数。对于这个特别的项目，利用了微调的Llama2–7B变体，RTX
    A5000在性价比方面是一个很好的选择。我还应该提到，如果没有开源库*[VLLM](https://github.com/vllm-project/vllm)*的**极快速度**，这一切都不可能实现。我的计算完全依赖于VLLM的支持。**没有VLLM，计算时间将增加20倍！**
    你也可以看看总运行时间并发笑——638小时就是26天；然而，你可以轻松地在多个GPU/服务器之间并行处理任务。
- en: Conclusion
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: In conclusion, the utilization of open-source LLMs and the shift in the pricing
    model from paying per token to paying per hour of compute time resulted in substantial
    cost savings in this large compute project. The total cost, which included prompt
    generation, fine-tuning, and inference, was a mere $189.58\. This is a stark contrast
    to the costs associated with using models like GPT-4 or GPT-3.5 Turbo, which would
    have amounted to $167,810 and $7410.42 respectively.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，利用开源LLM以及将定价模式从按令牌计费改为按计算时间计费，使得在这个大型计算项目中实现了显著的成本节省。总成本，包括提示生成、微调和推断，仅为$189.58。这与使用像GPT-4或GPT-3.5
    Turbo这样的模型的费用形成了鲜明对比，后者的费用分别为$167,810和$7410.42。
- en: The key takeaway from this project is the significant potential for cost reduction
    when leveraging open source LLMs. By renting a GPU per hour, the cost is tied
    to the compute time rather than the number of prompts, which can lead to massive
    savings, especially for extensive projects. This approach not only makes such
    projects more financially feasible but also opens up opportunities for individuals
    and smaller organizations to undertake large compute projects without incurring
    prohibitive costs.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目的主要收获是利用开源LLM可以显著降低成本。通过按小时租用GPU，费用与计算时间挂钩，而不是提示的数量，这可以带来巨大的节省，尤其是对于大型项目。这种方法不仅使这些项目在财务上更具可行性，而且为个人和小型组织提供了承担大型计算项目的机会，而不会产生过高的费用。
- en: The success of this project underscores the value and potential of open-source
    models in the realm of large language models and AI and serves as a testament
    to the power of innovative, cost-effective solutions in tackling complex computational
    tasks.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目的成功突显了开源模型在大型语言模型和AI领域的价值与潜力，证明了创新且具有成本效益的解决方案在解决复杂计算任务中的力量。
- en: Pricing Note
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定价说明
- en: The pricing information provided in this article is based on my personal experience
    and is intended to serve as a general comparison. Prices may vary depending on
    your region and specific circumstances. **The key takeaway is the potential cost
    savings when leveraging open source LLMs and renting a GPU per hour, rather than
    the specific prices quoted.**
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提供的定价信息基于我的个人经验，旨在作为一般比较。价格可能会根据你所在的地区和具体情况有所不同。**关键点是利用开源LLM和按小时租赁GPU的潜在成本节省，而不是具体的报价。**
- en: '📢 Hey there! If you found this article helpful, please consider:'
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 📢 嗨！如果你觉得这篇文章有帮助，请考虑：
- en: 👏 Clapping 50 times (this helps a lot!)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 👏 鼓掌50次（这很有帮助！）
- en: ✏️ Leaving a comment
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ✏️ 留下评论
- en: 🌟 Highlighting parts you found insightful
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 🌟 突出你觉得有洞察力的部分
- en: 👣 Following me
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 👣 关注我
- en: Any questions? 🤔 Don’t hesitate to ask. Supporting me this way is a free and
    easy way to show appreciation for my detailed tutorial articles! 😊
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 有任何问题吗？🤔 请随时提问。以这种方式支持我是一种免费且简单的方式来感谢我的详细教程文章！😊
- en: '**GPU rental recommendations:**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPU租赁推荐：**'
- en: 'Best overall: [RunPod.io GPU Rentals](https://runpod.io/?ref=lemrt56t)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳选择：[RunPod.io GPU 租赁](https://runpod.io/?ref=lemrt56t)
- en: 'Cheapest: [Vast.AI](https://cloud.vast.ai/?ref_id=79595)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最便宜：[Vast.AI](https://cloud.vast.ai/?ref_id=79595)
- en: '**Other Deep Learning Blogs**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**其他深度学习博客**'
- en: '[Efficiently Serving Open Source LLMs](https://medium.com/p/5f0bf5d8fd59)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[高效服务开源LLM](https://medium.com/p/5f0bf5d8fd59)'
- en: '[Computer Vision by Andrew Ng — 11 Lessons Learned](https://medium.com/towards-data-science/efficiently-serving-open-source-llms-5f0bf5d8fd59)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[Andrew Ng的计算机视觉 — 11节课学到的知识](https://medium.com/towards-data-science/efficiently-serving-open-source-llms-5f0bf5d8fd59)'
- en: '[Deep Learning Specialization by Andrew Ng — 21 Lessons Learned](/deep-learning-specialization-by-andrew-ng-21-lessons-learned-15ffaaef627c)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[Andrew Ng的深度学习专修课程 — 21节课学到的知识](/deep-learning-specialization-by-andrew-ng-21-lessons-learned-15ffaaef627c)'
