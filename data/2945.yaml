- en: 'Large Language Models: RoBERTa — A Robustly Optimized BERT Approach'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型：RoBERTa——一种强健优化的BERT方法
- en: 原文：[https://towardsdatascience.com/roberta-1ef07226c8d8?source=collection_archive---------1-----------------------#2023-09-24](https://towardsdatascience.com/roberta-1ef07226c8d8?source=collection_archive---------1-----------------------#2023-09-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/roberta-1ef07226c8d8?source=collection_archive---------1-----------------------#2023-09-24](https://towardsdatascience.com/roberta-1ef07226c8d8?source=collection_archive---------1-----------------------#2023-09-24)
- en: Learn about key techniques used for BERT optimisation
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解用于BERT优化的关键技术
- en: '[](https://medium.com/@slavahead?source=post_page-----1ef07226c8d8--------------------------------)[![Vyacheslav
    Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----1ef07226c8d8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1ef07226c8d8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1ef07226c8d8--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----1ef07226c8d8--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@slavahead?source=post_page-----1ef07226c8d8--------------------------------)[![Vyacheslav
    Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----1ef07226c8d8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1ef07226c8d8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1ef07226c8d8--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----1ef07226c8d8--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Froberta-1ef07226c8d8&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----1ef07226c8d8---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1ef07226c8d8--------------------------------)
    ·5 min read·Sep 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1ef07226c8d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Froberta-1ef07226c8d8&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----1ef07226c8d8---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Froberta-1ef07226c8d8&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----1ef07226c8d8---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1ef07226c8d8--------------------------------)
    ·5分钟阅读·2023年9月24日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1ef07226c8d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Froberta-1ef07226c8d8&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----1ef07226c8d8---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1ef07226c8d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Froberta-1ef07226c8d8&source=-----1ef07226c8d8---------------------bookmark_footer-----------)![](../Images/0fcff885fed32012c5a3ec2976d5f60c.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1ef07226c8d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Froberta-1ef07226c8d8&source=-----1ef07226c8d8---------------------bookmark_footer-----------)![](../Images/0fcff885fed32012c5a3ec2976d5f60c.png)'
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'The appearance of the **BERT** model led to significant progress in NLP. Deriving
    its architecture from **Transformer**, BERT achieves state-of-the-art results
    on various downstream tasks: language modeling, next sentence prediction, question
    answering, NER tagging, etc.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**BERT**模型的出现带来了NLP领域的重大进展。BERT源自**Transformer**架构，在各种下游任务上实现了最先进的结果：语言建模、下一句预测、问答、命名实体识别标记等。'
- en: '[](/bert-3d1bf880386a?source=post_page-----1ef07226c8d8--------------------------------)
    [## Large Language Models: BERT — Bidirectional Encoder Representations from Transformer'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/bert-3d1bf880386a?source=post_page-----1ef07226c8d8--------------------------------)
    [## 大型语言模型：BERT——来自Transformer的双向编码表示'
- en: Understand how BERT constructs state-of-the-art embeddings
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解**BERT**如何构建最先进的嵌入表示
- en: towardsdatascience.com](/bert-3d1bf880386a?source=post_page-----1ef07226c8d8--------------------------------)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/bert-3d1bf880386a?source=post_page-----1ef07226c8d8--------------------------------)
- en: Despite the excellent performance of BERT, researchers still continued experimenting
    with its configuration in hopes of achieving even better metrics. Fortunately,
    they succeeded with it and presented a new model called **RoBERTa** — Robustly
    Optimised BERT Approach.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 BERT 的性能优秀，研究人员仍然继续尝试调整其配置，希望实现更好的指标。幸运的是，他们成功了，并提出了一种新的模型，称为 **RoBERTa**——鲁棒优化
    BERT 方法。
- en: Throughout this article, we will be referring to the official [RoBERTa paper](https://arxiv.org/pdf/1907.11692.pdf)
    which contains in-depth information about the model. In simple words, RoBERTa
    consists of several independent improvements over the original BERT model — all
    of the other principles including the architecture stay the same. All of the advancements
    will be covered and explained in this article.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将引用官方的 [RoBERTa 论文](https://arxiv.org/pdf/1907.11692.pdf)，其中包含有关该模型的深入信息。简单来说，RoBERTa
    包含了对原始 BERT 模型的几个独立改进——所有其他原则包括架构保持不变。所有的进展将会在本文中涵盖和解释。
- en: 1\. Dynamic masking
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 动态掩蔽
- en: From the BERT’s architecture we remember that during pretraining BERT performs
    language modeling by trying to predict a certain percentage of masked tokens.
    The problem with the original implementation is the fact that chosen tokens for
    masking for a given text sequence across different batches are sometimes the same.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 从 BERT 的架构中，我们记得在预训练期间，BERT 通过尝试预测一定百分比的掩蔽标记来执行语言建模。原始实现的问题在于，对于给定的文本序列，在不同的批次中选择掩蔽的标记有时是相同的。
- en: More precisely, the training dataset is duplicated 10 times, thus each sequence
    is masked only in 10 different ways. Keeping in mind that BERT runs 40 training
    epochs, each sequence with the same masking is passed to BERT four times. As researchers
    found, it is slightly better to use dynamic masking meaning that masking is generated
    uniquely every time a sequence is passed to BERT. Overall, this results in less
    duplicated data during the training giving an opportunity for a model to work
    with more various data and masking patterns.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 更准确地说，训练数据集重复了 10 次，因此每个序列仅以 10 种不同的方式进行掩蔽。考虑到 BERT 运行了 40 个训练周期，每个具有相同掩蔽的序列会传递给
    BERT 四次。研究人员发现，使用动态掩蔽稍微更好，即每次将序列传递给 BERT 时掩蔽都是唯一生成的。总的来说，这在训练过程中减少了重复的数据，给模型提供了处理更多不同数据和掩蔽模式的机会。
- en: '![](../Images/5f0d5098a4da926aa2d2e4e5d5b760a7.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5f0d5098a4da926aa2d2e4e5d5b760a7.png)'
- en: Static masking vs Dynamic masking
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 静态掩蔽与动态掩蔽
- en: 2\. Next sentence prediction
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 下一个句子预测
- en: 'The authors of the paper conducted research for finding an optimal way to model
    the next sentence prediction task. As a consequence, they found several valuable
    insights:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的作者进行了研究，以寻找建模下一个句子预测任务的最佳方法。因此，他们发现了几个有价值的见解：
- en: '**Removing the next sentence prediction loss results in a slightly better performance.**'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**去除下一个句子预测损失会导致性能略有提升。**'
- en: '**Passing single natural sentences into BERT input hurts the performance, compared
    to passing sequences consisting of several sentences**. One of the most likely
    hypothesises explaining this phenomenon is the difficulty for a model to learn
    long-range dependencies only relying on single sentences.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将单个自然语言句子传入 BERT 输入会降低性能，相较于传入由多个句子组成的序列**。解释这一现象的一个可能假设是模型仅依靠单个句子难以学习长距离依赖关系。'
- en: '**It more beneficial to construct input sequences by sampling contiguous**
    **sentences from a single document rather than from multiple documents.** Normally,
    sequences are always constructed from contiguous full sentences of a single document
    so that the total length is at most 512 tokens. The problem arises when we reach
    the end of a document. In this aspect, researchers compared whether it was worth
    stopping sampling sentences for such sequences or additionally sampling the first
    several sentences of the next document (and adding a corresponding separator token
    between documents). The results showed that the first option is better.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从单个文档中采样连续的** **句子来构造输入序列比从多个文档中采样更有利。** 通常，序列总是从单个文档的连续完整句子中构造，这样总长度最多为
    512 个标记。问题在于当我们到达文档末尾时。研究人员在这方面比较了是否值得停止采样句子，还是额外采样下一个文档的前几个句子（并在文档之间添加相应的分隔标记）。结果表明，第一个选项更好。'
- en: Ultimately, for the final RoBERTa implementation, the authors chose to keep
    the first two aspects and omit the third one. Despite the observed improvement
    behind the third insight, researchers did not not proceed with it because otherwise,
    it would have made the comparison between previous implementations more problematic.
    It happens due to the fact that reaching the document boundary and stopping there
    means that an input sequence will contain less than 512 tokens. For having a similar
    number of tokens across all batches, the batch size in such cases needs to be
    augmented. This leads to variable batch size and more complex comparisons which
    researchers wanted to avoid.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，对于 RoBERTa 的最终实现，作者选择保留前两个方面而省略第三个方面。尽管第三个见解带来了观察到的改进，但研究人员没有继续采用它，因为这会使之前实现之间的比较变得更加困难。这是因为达到文档边界并在此停止意味着输入序列将包含少于
    512 个标记。为了在所有批次中保持类似的标记数量，在这种情况下需要增加批量大小。这会导致批量大小的变化和更复杂的比较，而研究人员希望避免这种情况。
- en: '![](../Images/49279eaf526bc460b7f3d52923f394a1.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/49279eaf526bc460b7f3d52923f394a1.png)'
- en: 3\. Increasing batch size
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 增加批量大小
- en: Recent advancements in NLP showed that increase of the batch size with the appropriate
    decrease of the learning rate and the number of training steps usually tends to
    improve the model’s performance.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在自然语言处理（NLP）领域的进展表明，增加批量大小并适当减少学习率和训练步数通常会改善模型的性能。
- en: As a reminder, the BERT base model was trained on a batch size of 256 sequences
    for a million steps. The authors tried training BERT on batch sizes of 2K and
    8K and the latter value was chosen for training RoBERTa. The corresponding number
    of training steps and the learning rate value became respectively 31K and 1e-3.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒一下，BERT base 模型在 256 个序列的批量大小下训练了 100 万步。作者尝试了 2K 和 8K 的批量大小，并选择了后者来训练 RoBERTa。相应的训练步数和学习率值分别变为
    31K 和 1e-3。
- en: It is also important to keep in mind that batch size increase results in easier
    parallelization through a special technique called “***gradient accumulation***”.
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 同时，需要注意的是，批量大小的增加通过一种叫做“***梯度累积***”的特殊技术可以更容易地进行并行化。
- en: 4\. Byte text encoding
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 字节文本编码
- en: 'In NLP there exist three main types of text tokenization:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NLP 中，存在三种主要的文本标记化类型：
- en: Character-level tokenization
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符级别的标记化
- en: Subword-level tokenization
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 子词级别的标记化
- en: Word-level tokenization
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词级别的标记化
- en: The original BERT uses a subword-level tokenization with the vocabulary size
    of 30K which is learned after input preprocessing and using several heuristics.
    RoBERTa uses bytes instead of unicode characters as the base for subwords and
    expands the vocabulary size up to 50K without any preprocessing or input tokenization.
    This results in 15M and 20M additional parameters for BERT base and BERT large
    models respectively. The introduced encoding version in RoBERTa demonstrates slightly
    worse results than before.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的 BERT 使用了子词级别的标记化，词汇表大小为 30K，该词汇表在输入预处理后使用多个启发式方法进行学习。RoBERTa 使用字节而非 Unicode
    字符作为子词的基础，并将词汇表大小扩展到 50K，而无需任何预处理或输入标记化。这导致 BERT base 和 BERT large 模型分别增加了 15M
    和 20M 的额外参数。RoBERTa 中引入的编码版本表现稍逊于之前的版本。
- en: Nevertheless, in the vocabulary size growth in RoBERTa allows to encode almost
    any word or subword without using the unknown token, compared to BERT. This gives
    a considerable advantage to RoBERTa as the model can now more fully understand
    complex texts containing rare words.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，RoBERTa 中词汇表大小的增长使其能够编码几乎任何单词或子词，而无需使用未知标记，这相比于 BERT 是一个显著的优势。这使得 RoBERTa
    可以更全面地理解包含稀有词汇的复杂文本。
- en: '![](../Images/cdee0913cccc1b2bf6291958373ee900.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cdee0913cccc1b2bf6291958373ee900.png)'
- en: Pretraining
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预训练
- en: Apart from it, RoBERTa applies all four described aspects above with the same
    architecture parameters as BERT large. The total number of parameters of RoBERTa
    is 355M.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，RoBERTa 采用与 BERT large 相同的架构参数，应用了上述四个方面。RoBERTa 的总参数数量为 355M。
- en: RoBERTa is pretrained on a combination of five massive datasets resulting in
    a total of 160 GB of text data. In comparison, BERT large is pretrained only on
    13 GB of data. Finally, the authors increase the number of training steps from
    100K to 500K.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: RoBERTa 在五个大规模数据集的组合上进行了预训练，总共达到了 160 GB 的文本数据。相比之下，BERT large 只在 13 GB 的数据上进行了预训练。最后，作者将训练步数从
    100K 增加到 500K。
- en: As a result, RoBERTa outperforms BERT large on XLNet large on the most popular
    benchmarks.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，RoBERTa 在最受欢迎的基准测试中超越了 BERT large 和 XLNet large。
- en: RoBERTa versions
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RoBERTa 版本
- en: 'Analogously to BERT, the researchers developed two versions of RoBERTa. Most
    of the hyperparameters in the base and large versions are the same. The figure
    below demonstrates the principle differences:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 BERT，研究人员开发了两个版本的 RoBERTa。基础版和大型版中的大多数超参数是相同的。下图展示了主要的区别：
- en: '![](../Images/a46e3fd0b0d36e07965b858682fb8e91.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a46e3fd0b0d36e07965b858682fb8e91.png)'
- en: The fine-tuning process in RoBERTa is similar to the BERT’s.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: RoBERTa 的微调过程类似于 BERT 的过程。
- en: Conclusion
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'In this article, we have examined an improved version of BERT which modifies
    the original training procedure by introducing the following aspects:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们探讨了 BERT 的改进版本，该版本通过引入以下几个方面修改了原始训练过程：
- en: dynamic masking
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态掩蔽
- en: omitting the next sentence prediction objective
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 省略下一句预测目标
- en: training on longer sentences
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在更长的句子上进行训练
- en: increasing vocabulary size
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加词汇表的大小
- en: training for longer with larger batches over data
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据上使用更大的批量进行更长时间的训练
- en: The resulting RoBERTa model appears to be superior to its ancestors on top benchmarks.
    Despite a more complex configuration, RoBERTa adds only 15M additional parameters
    maintaining comparable inference speed with BERT.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，RoBERTa 模型在顶级基准测试中优于其前身。尽管配置更复杂，但 RoBERTa 仅增加了 1500 万个参数，同时保持了与 BERT 相当的推理速度。
- en: Resources
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源
- en: '[RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692.pdf)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[RoBERTa: 一种稳健优化的 BERT 预训练方法](https://arxiv.org/pdf/1907.11692.pdf)'
- en: '*All images unless otherwise noted are by the author*'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，否则所有图片均为作者提供*'
