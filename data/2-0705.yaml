- en: 'Democratizing AI: MosaicML’s Impact on the Open-Source LLM Movement'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 民主化AI：MosaicML对开源LLM运动的影响
- en: 原文：[https://towardsdatascience.com/democratizing-ai-mosaicmls-impact-on-the-open-source-llm-movement-7972ff12dd92](https://towardsdatascience.com/democratizing-ai-mosaicmls-impact-on-the-open-source-llm-movement-7972ff12dd92)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/democratizing-ai-mosaicmls-impact-on-the-open-source-llm-movement-7972ff12dd92](https://towardsdatascience.com/democratizing-ai-mosaicmls-impact-on-the-open-source-llm-movement-7972ff12dd92)
- en: How high-quality base models unlock new possibilities for an entire industry…
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高质量基础模型如何为整个行业开启新可能性……
- en: '[](https://wolfecameron.medium.com/?source=post_page-----7972ff12dd92--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----7972ff12dd92--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7972ff12dd92--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7972ff12dd92--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----7972ff12dd92--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page-----7972ff12dd92--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----7972ff12dd92--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7972ff12dd92--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7972ff12dd92--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----7972ff12dd92--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7972ff12dd92--------------------------------)
    ·13 min read·Oct 15, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7972ff12dd92--------------------------------)
    ·13 min 阅读·2023年10月15日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4fb50fe15a17b036375d4670a4939ea5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4fb50fe15a17b036375d4670a4939ea5.png)'
- en: (Photo by [Raimond Klavins](https://unsplash.com/@raimondklavins?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/Ql6JhGdbQg0?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: （照片由[Raimond Klavins](https://unsplash.com/@raimondklavins?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    提供，来源于[Unsplash](https://unsplash.com/photos/Ql6JhGdbQg0?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)）
- en: Recently, we have overviewed a lot of current research on the creation of open-source
    large language models (LLMs). Across all of this work, models are created using
    a common framework with a few simple components; see below.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，我们回顾了许多关于创建开源大型语言模型（LLM）的当前研究。在所有这些工作中，模型是通过一个包含几个简单组件的共同框架创建的；见下文。
- en: '![](../Images/56f9033518d72f0d72b0228371d7398c.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56f9033518d72f0d72b0228371d7398c.png)'
- en: Multi-step process for creating and refining an LLM (from [12, 13])
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 创建和优化大型语言模型（来自[12, 13]）的多步骤过程
- en: Although this framework has several steps, the first step is arguably the most
    important. Creating a more powerful base model via extensive, high-quality pre-training
    enables better results when the LLM is refined via supervised fine-tuning (SFT)
    and reinforcement learning from human feedback (RLHF). Then, downstream applications
    are better due to the use of an improved model. The pre-trained (base) model is
    the common starting point for any LLM application.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个框架有几个步骤，但第一步可以说是最重要的。通过广泛的高质量预训练创建一个更强大的基础模型，可以在通过监督微调（SFT）和从人类反馈中进行强化学习（RLHF）时实现更好的结果。然后，下游应用由于使用了改进的模型而表现得更好。预训练（基础）模型是任何LLM应用程序的共同起点。
- en: Until recently, open-source base models either performed poorly compared to
    their proprietary counterparts or could only be used for research. However, this
    changed with the release of MPT-7B and MPT-30B [1, 2] by MosaicML. These open-source
    base models achieve impressive levels of performance, are free for commercial
    use, and come with an entire suite of efficient software for training, fine-tuning,
    and evaluating LLMs. These open-source tools enable a wide variety of specialized
    use cases for LLMs to be explored at a significantly reduced cost, making them
    a powerful resource for practitioners in AI.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 直到最近，开源基础模型要么与其专有对手相比表现不佳，要么只能用于研究。然而，这种情况随着MosaicML发布的MPT-7B和MPT-30B [1, 2]的出现发生了变化。这些开源基础模型达到了令人印象深刻的性能水平，商业使用免费，并且配备了用于训练、微调和评估LLM的完整高效软件套件。这些开源工具使得可以以显著降低的成本探索多种专业应用场景，从而成为AI从业者的强大资源。
- en: Faster LLMs and Longer Context Lengths
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更快的LLM和更长的上下文长度
- en: 'The MPT-7B/30B models are based upon a typical, [decoder-only transformer](/language-models-gpt-and-gpt-2-8bdb9867c50a)
    architecture. However, a few key modifications are made, including:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: MPT-7B/30B 模型基于典型的 [仅解码器变换器](/language-models-gpt-and-gpt-2-8bdb9867c50a) 架构。然而，进行了一些关键修改，包括：
- en: '[ALiBi](https://docs.mosaicml.com/projects/composer/en/stable/method_cards/alibi.html)
    [6] (instead of normal position embeddings)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ALiBi](https://docs.mosaicml.com/projects/composer/en/stable/method_cards/alibi.html)
    [6]（取代了普通的位置嵌入）'
- en: '[Low precision layer norm](https://docs.mosaicml.com/projects/composer/en/latest/method_cards/low_precision_layernorm.html)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[低精度层归一化](https://docs.mosaicml.com/projects/composer/en/latest/method_cards/low_precision_layernorm.html)'
- en: '[Flash Attention](https://github.com/HazyResearch/flash-attention) [7]'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Flash Attention](https://github.com/HazyResearch/flash-attention) [7]'
- en: 'Within this section, we will learn about each of these components, how they
    work, and their impact on LLMs. To fully understand the details of this section,
    it might be useful to review the following concepts:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入了解这些组件，每个组件的工作原理，以及它们对 LLM 的影响。要全面理解本节的细节，可能需要回顾以下概念：
- en: Self-Attention [[link](https://twitter.com/cwolferesearch/status/1641932082283700226?s=20)]
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自注意力 [[link](https://twitter.com/cwolferesearch/status/1641932082283700226?s=20)]
- en: Causal Self-Attention (used by decoder-only LLMs) [[link](https://twitter.com/cwolferesearch/status/1644773244786941952?s=20)]
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因果自注意力（由仅解码器 LLM 使用） [[link](https://twitter.com/cwolferesearch/status/1644773244786941952?s=20)]
- en: ALiBi enables context length extrapolation
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ALiBi 实现了上下文长度的外推
- en: '![](../Images/80ebdbd2cbc254be8fafdafed13cdd8d.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80ebdbd2cbc254be8fafdafed13cdd8d.png)'
- en: Embedding a sequence of tokens within an LLM (created by author)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LLM 中嵌入一个令牌序列（由作者创建）
- en: 'In a vanilla transformer architecture, we create an input sequence of tokens
    by first [tokenizing](https://vaclavkosar.com/ml/Tokenization-in-Machine-Learning-Explained)
    the raw text and looking up the embedding (each token in the tokenizer’s vocabulary
    has a unique embedding) for each token. Then, we add a position embedding to each
    token embedding, thus injecting positional info into the embedding of each token
    in the sequence; see above. This is necessary because the self-attention operation
    is agnostic to the position of each token in the sequence. Although position embeddings
    work well, there’s one big problem: *they struggle to generalize to sequences
    longer than those seen during training*.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个普通的变换器架构中，我们通过首先 [令牌化](https://vaclavkosar.com/ml/Tokenization-in-Machine-Learning-Explained)
    原始文本并查找每个令牌的嵌入（令牌化器词汇表中的每个令牌都有一个唯一的嵌入）来创建一个输入令牌序列。然后，我们将位置嵌入添加到每个令牌嵌入中，从而将位置信息注入到序列中每个令牌的嵌入中；见上文。这是必要的，因为自注意力操作对序列中每个令牌的位置是无感知的。尽管位置嵌入工作良好，但有一个大问题：*它们难以推广到比训练期间见过的更长的序列*。
- en: '![](../Images/2f1ed9718460486ca898dc3ca48bc9dc.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f1ed9718460486ca898dc3ca48bc9dc.png)'
- en: (from [6])
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [6]）
- en: '**The solution.** Attention with Linear Biases **(**ALiBi) [6] solves this
    problem by getting rid of position embeddings altogether. Instead, positional
    information is injected into the transformer as part of the self-attention operation
    by adding an additive penalty to the key-query attention score; see above. We
    should recall that self-attention computes an attention score between each pair
    of tokens within a sequence. ALiBi operates by adding a static, non-learned bias
    (or penalty) to this score that is proportional to the distance between the pair
    of tokens; see below.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案。** 带有线性偏差的注意力**（**ALiBi）[6]通过完全去除位置嵌入来解决这个问题。相反，位置信息通过在自注意力操作中对键-查询注意力分数添加一个加性惩罚注入到变换器中；见上文。我们应当回顾，自注意力计算序列中每对令牌之间的注意力分数。ALiBi
    通过为这个分数添加一个与令牌对之间的距离成比例的静态、非学习的偏差（或惩罚）来操作；见下文。'
- en: '![](../Images/1ba2295d8ea2102c90b334b95463b5c8.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ba2295d8ea2102c90b334b95463b5c8.png)'
- en: Computing the key-query attention score for a particular pair of tokens (created
    by author)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 计算特定令牌对的键-查询注意力分数（由作者创建）
- en: Such an approach is impactful because it depends on the pairwise distance between
    tokens rather than the absolute positions of tokens within a sequence. This quantity
    is less dependent upon the length of the underlying sequence and allows ALiBi
    to generalize much better to sequences that are longer than those seen during
    training; see below. As we will see, MPT models (which use ALiBi) can be trained
    to support larger context lengths compared to most open-source alternatives *and
    can even extrapolate to sequences as long as 84K tokens*!
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法之所以有影响，是因为它依赖于令牌之间的成对距离，而不是序列中令牌的绝对位置。这一量度不那么依赖于基础序列的长度，并允许ALiBi对比训练期间看到的序列更长的序列进行更好的泛化；见下文。正如我们将看到的，使用ALiBi的MPT模型可以训练以支持比大多数开源替代方案更大的上下文长度*甚至可以推断到长度为84K标记的序列*！
- en: '![](../Images/21d505c2e4b85a7d291630146e65b4af.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21d505c2e4b85a7d291630146e65b4af.png)'
- en: (from [6])
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [6])
- en: Faster Inference
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更快的推理
- en: Due to their use of low precision layer norm and FlashAttention [7], the MPT
    models have very fast training and inference speeds (i.e., `1.5-2X` faster than
    similarly-sized LLaMA models [3] using standard [HuggingFace inference pipelines](https://huggingface.co/blog/inference-update)).
    Going further, the weights of these models can be ported to optimized modules
    like [FasterTransformer](https://github.com/NVIDIA/FasterTransformer) or [ONNX](https://github.com/onnx/models)
    to enable even faster inference.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 由于使用低精度层归一化和FlashAttention [7]，MPT模型具有非常快的训练和推理速度（即，比使用标准[HuggingFace推理管道](https://huggingface.co/blog/inference-update)的同等规模的LLaMA模型快`1.5-2X`）。更进一步，这些模型的权重可以迁移到像[FasterTransformer](https://github.com/NVIDIA/FasterTransformer)或[ONNX](https://github.com/onnx/models)这样的优化模块，以实现更快的推理。
- en: '**Low precision layer norm.** Put simply, low precision layer norm performs
    the operations from a [LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)
    module in 16-bit precision. Although such an approach can cause loss spikes in
    some cases, it improves hardware utilization and, in turn, speeds up both training
    and inference. Using low precision layer norm also has minimal impact on the model’s
    final performance.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**低精度层归一化**。简单来说，低精度层归一化以16位精度执行[LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)模块的操作。尽管这种方法在某些情况下可能导致损失峰值，但它改善了硬件利用率，从而加速了训练和推理。使用低精度层归一化对模型的最终性能也几乎没有影响。'
- en: '**Flash attention.** In its canonical form, self-attention is an `O(N^2)` operation,
    where `N` is the length of the input sequence. In order to improve the efficiency
    of this operation, many approximate attention variants have been proposed, such
    as:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**Flash attention**。在其经典形式中，自注意力是一个`O(N^2)`操作，其中`N`是输入序列的长度。为了提高该操作的效率，已经提出了许多近似注意力变体，例如：'
- en: Reformer [[link](https://arxiv.org/abs/2001.04451)]
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reformer [[link](https://arxiv.org/abs/2001.04451)]
- en: SMYRF [[link](https://arxiv.org/abs/2010.05315)]
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SMYRF [[link](https://arxiv.org/abs/2010.05315)]
- en: Performer [[link](https://arxiv.org/abs/2009.14794)]
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Performer [[link](https://arxiv.org/abs/2009.14794)]
- en: The goal of most of these techniques is to derive a “linear” variation of attention
    — a similar/approximate operation with a complexity of `O(N)`. Although these
    variants achieve a theoretical reduction in [FLOPs](https://stackoverflow.com/questions/58498651/what-is-flops-in-field-of-deep-learning),
    *many of them do not achieve any wall-clock speedup in practical scenarios*! Flash
    attention solves this problem by reformulating the attention operation in an IO-aware
    manner; see below.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数这些技术的目标是推导出一种“线性”注意力变体——一种具有`O(N)`复杂度的类似/近似操作。尽管这些变体在理论上减少了[FLOPs](https://stackoverflow.com/questions/58498651/what-is-flops-in-field-of-deep-learning)，*许多在实际场景中并没有实现任何墙钟速度的提升*！Flash
    attention通过以IO感知的方式重新构建注意力操作来解决这个问题；见下文。
- en: '![](../Images/95ca281c05794f507e75428c5cd1c621.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/95ca281c05794f507e75428c5cd1c621.png)'
- en: (from [7])
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [7])
- en: 'The hardware-related details of how FlashAttention is implemented are beyond
    the scope of this post. However, the resulting efficient attention implementation
    has a variety of positive benefits. For example, FlashAttention can:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: FlashAttention的硬件实现细节超出了本文的范围。然而，结果高效的注意力实现带来了各种积极的好处。例如，FlashAttention可以：
- en: Speed up BERT-large [10] training time by 15%
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将BERT-large [10]的训练时间提高15%
- en: Improve training speed by `3X` for GPT-2 [11]
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将GPT-2的训练速度提高`3X` [11]
- en: Enable longer context lengths for LLMs (due to better memory efficiency)
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为LLMs启用更长的上下文长度（由于更好的内存效率）
- en: For more details on FlashAttention, check out the writeup [here](https://shreyansh26.github.io/post/2023-03-26_flash-attention/).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 FlashAttention 的更多细节，请查看 [这里](https://shreyansh26.github.io/post/2023-03-26_flash-attention/)。
- en: 'MPT-7B: A Commercially-Usable LLaMA-7B'
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MPT-7B：一个商业可用的 LLaMA-7B
- en: '![](../Images/fcb2e1d8d8a273e732c0af9d250ef5c7.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fcb2e1d8d8a273e732c0af9d250ef5c7.png)'
- en: The total compute cost of training the MPT-7B model and fine-tuning various
    derivatives of this model (from [1])
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 训练 MPT-7B 模型及其各种衍生模型的总计算成本（来自 [1]）
- en: Proposed in [1], MPT-7B is an open-source, commercially-usable language foundation
    model that broadly matches the performance of similarly-sized, open-source base
    models like LLaMA-7B [3] (which is *not* commercially-usable!). Following the
    lessons of Chinchilla [4], MPT-7B is pre-trained over a large corpus — one trillion
    tokens in total — of diverse, publicly-available text. The code used to train,
    fine-tune, and evaluate MPT-7B is completely open-source, *making this model a
    great resource or starting point for practitioners looking to fine-tune their
    own specialized LLM for solving a variety of different downstream applications*!
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [1] 中提出的 MPT-7B 是一个开源、商业可用的语言基础模型，性能广泛匹配类似规模的开源基础模型，如 LLaMA-7B [3]（它是*不可*商业使用的！）。根据
    Chinchilla [4] 的经验教训，MPT-7B 在一个大的语料库上进行预训练——总计一万亿个标记——这些文本是多样的、公开可用的。用于训练、微调和评估
    MPT-7B 的代码完全开源，*使得这个模型成为实践者们调整自己专门化大语言模型以解决各种不同下游应用的一个很好的资源或起点*。
- en: Creating the Base Model
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建基础模型
- en: Due to its modified architecture, MPT-7B has several desirable properties, such
    as the ability to generalize to much longer context lengths and faster inference
    speeds. Additionally, we see in [1] that this modified architecture leads to the
    elimination of loss spikes during pre-training of MPT-7B, allowing the model to
    be pre-trained without any human intervention (assuming that any hardware failures
    are handled automatically within the LLM’s training code)!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其修改后的架构，MPT-7B 具有几个理想的属性，例如能够泛化到更长的上下文长度和更快的推理速度。此外，我们在 [1] 中看到，这种修改后的架构消除了
    MPT-7B 预训练过程中的损失峰值，使得模型可以在没有任何人工干预的情况下进行预训练（假设任何硬件故障都在大语言模型的训练代码中自动处理）！
- en: '![](../Images/719031a87595e7c842ff731839f2a5bb.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/719031a87595e7c842ff731839f2a5bb.png)'
- en: MPT-7B only experiences hardware failures, which can be automatically resolved,
    during its pre-training process (from [1])
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: MPT-7B 在其预训练过程中仅经历硬件故障，这些故障可以自动解决（来自 [1]）
- en: '**Training process.** Although most LLMs are trained using the [AdamW optimizer](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html),
    MPT adopts the Lion optimizer [8], which improves the stability of the training
    process. The entire training framework is based upon PyTorch’s [Fully Sharded
    Data Parallel (FSDP)](https://github.com/huggingface/blog/blob/main/pytorch-fsdp.md)
    package and uses no pipeline or tensor parallelism. To put it simply, the training
    framework for MPT-7B, which is [completely open-sourced](https://github.com/mosaicml/llm-foundry),
    uses popular/common components, but makes a few useful changes that are found
    to improve the stability of training.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练过程。** 尽管大多数大语言模型使用 [AdamW 优化器](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)
    进行训练，MPT 采用了 Lion 优化器 [8]，这提高了训练过程的稳定性。整个训练框架基于 PyTorch 的 [完全分片数据并行 (FSDP)](https://github.com/huggingface/blog/blob/main/pytorch-fsdp.md)
    包，不使用管道或张量并行。简单来说，MPT-7B 的训练框架是 [完全开源的](https://github.com/mosaicml/llm-foundry)，使用了流行/常见的组件，但进行了几个有用的修改，以提高训练的稳定性。'
- en: '![](../Images/b83525c36783bedc660d813bd8750ac2.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b83525c36783bedc660d813bd8750ac2.png)'
- en: (from [1])
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [1])
- en: '**The data.** The textual corpus used to train MPT-7B is a custom mixture of
    publicly-available datasets (mostly English language); see above. In [1], we see
    that the amount of data used to train MPT-7B is quite large — 1T tokens in total.
    For comparison, open-sources models like [Pythia](https://huggingface.co/EleutherAI/pythia-1.4b)
    and [StableLM](https://github.com/Stability-AI/StableLM) pre-train on 300B and
    800B tokens, respectively. Interestingly, we see that the authors of [1] adopt
    a very particular tokenizer — the [GPT-NeoX-20B](https://huggingface.co/docs/transformers/model_doc/gpt_neox)
    BPE tokenizer— for their model. This tokenizer is desirable because it is trained
    on a large, diverse dataset and handles spaces more consistently than other popular
    tokenizers.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据。** 用于训练 MPT-7B 的文本语料库是由公开数据集（主要是英语数据）定制混合而成；见上文。在 [1] 中，我们看到用于训练 MPT-7B
    的数据量非常大 — 总计 1T 个标记。作为对比，开源模型如 [Pythia](https://huggingface.co/EleutherAI/pythia-1.4b)
    和 [StableLM](https://github.com/Stability-AI/StableLM) 分别在 300B 和 800B 个标记上进行预训练。有趣的是，我们看到
    [1] 的作者采用了一种非常特定的分词器 — [GPT-NeoX-20B](https://huggingface.co/docs/transformers/model_doc/gpt_neox)
    BPE 分词器 — 进行模型训练。这个分词器是受欢迎的，因为它在一个大规模、多样化的数据集上进行训练，并且比其他流行的分词器更一致地处理空格。'
- en: '“This tokenizer has a number of desirable characteristics, most of which are
    relevant for tokenizing code: trained on a diverse mix of data that includes code,
    applies consistent space delimitation (unlike the GPT2 tokenizer which tokenizes
    inconsistently depending on the presence of prefix spaces), and contains tokens
    for repeated space characters.” *— from [1]*'
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “这个分词器具有许多令人满意的特性，其中大多数与代码分词相关：在包括代码的数据混合上训练，应用一致的空格分隔（不像 GPT2 分词器根据前缀空格的存在不一致地进行分词），并且包含了对重复空格字符的处理。”
    *— 来源于 [1]*
- en: As practitioners, we should always be aware of the tokenizer being used by our
    model. This choice — *although typically ignored or overlooked* — can drastically
    impact our results. For example, code-based language models need a tokenizer that
    handles whitespace in a particular manner, while multilingual language models
    have a variety of unique tokenization considerations.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 作为从业者，我们应该始终关注模型所使用的分词器。这个选择 — *尽管通常被忽视或忽略* — 会对我们的结果产生极大影响。例如，基于代码的语言模型需要一个以特定方式处理空格的分词器，而多语言模型则有各种独特的分词考虑因素。
- en: How does it perform?
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它的表现如何？
- en: '![](../Images/4262851d604e3809968c0c19f5bcf82f.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4262851d604e3809968c0c19f5bcf82f.png)'
- en: (from [1])
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: （来源于 [1]）
- en: MPT-7B is compared to a variety of open-source models (e.g., LLaMA, [StableLM](https://github.com/Stability-AI/StableLM),
    [Pythia](https://github.com/EleutherAI/pythia), [GPT-NeoX](https://huggingface.co/docs/transformers/model_doc/gpt_neox),
    [OPT](https://cameronrwolfe.substack.com/p/understanding-the-open-pre-trained-transformers-opt-library-193a29c14a15),
    and [GPT-J](https://huggingface.co/docs/transformers/model_doc/gptj)) on standard
    benchmarks. As shown above, LLaMA-7B achieves drastic improvements over open-source
    alternatives, while MPT-7B matches or exceed LLaMA’s performance. Recent open-source
    LLMs are much better than their predecessors! *LLaMA-7B and MPT-7B are both incredibly
    high-performing base models compared to other open-source models*. However, MPT-7B
    can be used commercially, while LLaMA can only be used for research.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: MPT-7B 在标准基准测试中与各种开源模型进行比较（例如，LLaMA，[StableLM](https://github.com/Stability-AI/StableLM)，[Pythia](https://github.com/EleutherAI/pythia)，[GPT-NeoX](https://huggingface.co/docs/transformers/model_doc/gpt_neox)，[OPT](https://cameronrwolfe.substack.com/p/understanding-the-open-pre-trained-transformers-opt-library-193a29c14a15)
    和 [GPT-J](https://huggingface.co/docs/transformers/model_doc/gptj)）。如上所示，LLaMA-7B
    相较于开源替代方案取得了显著的改进，而 MPT-7B 的表现与 LLaMA 相匹配或超过其表现。近期的开源大型语言模型比其前身要好得多！*LLaMA-7B
    和 MPT-7B 相比其他开源模型都是极其高效的基础模型*。然而，MPT-7B 可以用于商业用途，而 LLaMA 仅能用于研究。
- en: Derivatives of MPT-7B
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MPT-7B 的衍生模型
- en: In addition to releasing the MPT-7B base model, authors in [1] leverage the
    open-source training code for MPT to fine-tune several different derivatives of
    the base model (outlined below). Fine-tuning is very cheap compared to pre-training
    an LLM from scratch (i.e., *10–100X reduction in time and cost, if not more*).
    As such, most of the time and effort in developing MPT-7B went into creating the
    base model, which serves as a starting point for fine-tuning the models below.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 除了发布 MPT-7B 基础模型外，[1] 的作者还利用 MPT 的开源训练代码来微调多个不同的基础模型衍生版本（见下文）。与从头开始预训练一个大型语言模型相比，微调的成本非常低（即，*时间和成本减少
    10–100 倍，甚至更多*）。因此，开发 MPT-7B 的大部分时间和精力都投入到了创建基础模型上，该模型作为微调下述模型的起点。
- en: '![](../Images/08fa6f43cdadaf4d961b5d44c3852f25.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08fa6f43cdadaf4d961b5d44c3852f25.png)'
- en: (from [1])
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: '**MPT-StoryWriter-65K (commercial)** is a version of MPT-7B that has been fine-tuned
    on data with very long context lengths. In particular, authors in [1] leverage
    the [books3 dataset](https://huggingface.co/datasets/the_pile_books3), which contains
    excerpts from fiction books, to create a dataset for fine-tuning (i.e., just using
    the next-token prediction objective) with a 65K token context length. Due to the
    use of ALiBi [6] and FlashAttention [7], MPT-StoryWriter-65K can be feasibly trained
    over such large inputs, used to consume the entirety of The Great Gatsby (68K
    tokens) to write an epilogue (see above), and even generalized to process sequences
    lengths as long as 84K tokens.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**MPT-StoryWriter-65K（商业版）** 是 MPT-7B 的一个版本，经过了在非常长上下文长度的数据上进行微调。特别是，文献中的作者
    [1] 利用了包含虚构书籍摘录的 [books3 dataset](https://huggingface.co/datasets/the_pile_books3)，以创建一个用于微调的数据集（即仅使用下一个令牌预测目标），上下文长度为
    65K 令牌。由于使用了 ALiBi [6] 和 FlashAttention [7]，MPT-StoryWriter-65K 可以有效地在如此大的输入上进行训练，能够处理《了不起的盖茨比》的全部内容（68K
    令牌）以编写后记（见上文），甚至可以推广到处理长度达 84K 令牌的序列。'
- en: “We expect LLMs to treat the input as instructions to follow. Instruction finetuning
    is the process of training LLMs to perform instruction-following in this way.
    By reducing the reliance on clever prompt engineering, instruction finetuning
    makes LLMs more accessible, intuitive, and immediately usable.” *— from [1]*
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我们期望 LLMs 将输入视为需要遵循的指令。指令微调是训练 LLMs 以这种方式执行指令跟随的过程。通过减少对巧妙提示工程的依赖，指令微调使 LLMs
    更加易于访问、直观和立即可用。” *——来自 [1]*
- en: '**MPT-7B-Instruct (commercial)** and **MPT-7B-Chat (non-commercial)** are instruction
    tuned versions of MPT-7B. The instruct variant is fine-tuned over data from [Dolly-15K](https://huggingface.co/datasets/databricks/databricks-dolly-15k)
    and the [Helpful and Harmless dataset](https://huggingface.co/datasets/Anthropic/hh-rlhf),
    while the chat model is trained with data from sources like [ShareGPT](https://sharegpt.com/),
    [HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3), [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca),
    and [Evol-Instruct](https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_70k).
    As outlined by the quote above, instruction tuning takes a pre-trained language
    model and modifies its style or behavior to be more intuitive and accessible,
    usually with an emphasis upon instruction following or problem solving.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**MPT-7B-Instruct（商业版）** 和 **MPT-7B-Chat（非商业版）** 是 MPT-7B 的指令调整版本。指令版是在 [Dolly-15K](https://huggingface.co/datasets/databricks/databricks-dolly-15k)
    和 [Helpful and Harmless dataset](https://huggingface.co/datasets/Anthropic/hh-rlhf)
    数据上进行微调的，而聊天模型则使用了来自 [ShareGPT](https://sharegpt.com/)、[HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3)、[Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca)
    和 [Evol-Instruct](https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_70k)
    等来源的数据进行训练。如上文所述，指令调整是指在预训练语言模型的基础上，对其风格或行为进行修改，使其更加直观和易于访问，通常着重于指令跟随或问题解决。'
- en: 'MPT-30B: An Open-Source GPT-3 Alternative'
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MPT-30B：一个开源的 GPT-3 替代品
- en: '![](../Images/108528023f4e40073f14767ea75f97f7.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/108528023f4e40073f14767ea75f97f7.png)'
- en: MPT-30B improves upon MPT-7B in all performance categories (from [2])
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: MPT-30B 在所有性能类别上都优于 MPT-7B（来自 [2]）
- en: Shortly after its proposal, the MPT-7B model gained significant recognition
    in the AI research community — *it even amassed over 3M downloads on HuggingFace*!
    The success of MPT-7B was no surprise, as it provided a commercially-usable alternative
    to the incredibly popular LLaMA-7B model. Riding this momentum, researchers at
    MosaicML followed MPT-7B with a slightly larger model, called MPT-30B [2], that
    was found to match or exceed the performance of GPT-3 [9]. As such, the proposal
    of MPT-30B continues the trend of making commercially-usable versions of powerful
    base LLMs available to anyone.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在其提出后不久，MPT-7B 模型在 AI 研究界获得了显著认可 —— *它甚至在 HuggingFace 上累计下载量超过了 300 万次*！MPT-7B
    的成功并不令人意外，因为它为极受欢迎的 LLaMA-7B 模型提供了一个商业上可用的替代品。借此势头，MosaicML 的研究人员推出了一个稍大的模型，称为
    MPT-30B [2]，该模型被发现能与 GPT-3 [9] 的表现相匹敌或超过。因此，MPT-30B 的提出延续了为任何人提供强大基础 LLM 的商业可用版本的趋势。
- en: Diving Deeper into MPT-30B
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入了解 MPT-30B
- en: MPT-30B shares the same, modified decoder-only architecture as MPT-7B, which
    uses FlashAttention and low precision layer norm for improved efficiency. Overall,
    the models are quite similar aside from MPT-30B being larger. Interestingly, the
    size of MPT-30B was chosen very specifically. A model of this size is feasible
    to deploy on a single GPU using 8 or 16-bit precision, while alternatives like
    [Falcon-40B](https://falconllm.tii.ae/) are slightly too large to be deployed
    in this manner.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: MPT-30B 共享与 MPT-7B 相同的修改过的解码器架构，使用了 FlashAttention 和低精度层归一化，以提高效率。总体而言，这些模型非常相似，除了
    MPT-30B 更大一些。值得注意的是，MPT-30B 的大小选择得非常具体。这个大小的模型可以在单个 GPU 上使用 8 位或 16 位精度进行部署，而像
    [Falcon-40B](https://falconllm.tii.ae/) 这样的替代品稍微大了一些，无法以这种方式部署。
- en: '![](../Images/5bf95c22273ced86d9ebbdda2e35b4c5.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5bf95c22273ced86d9ebbdda2e35b4c5.png)'
- en: (from [2])
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [2]）
- en: '**What’s different?** MPT-30B is different from MPT-7B in two main ways:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**有什么不同？** MPT-30B 与 MPT-7B 主要有两方面的不同：'
- en: Pre-training data mixture
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练数据混合
- en: Context length
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文长度
- en: The pre-training dataset for MPT-30B is similar to that of MPT-7B, but the mixture
    of data is slightly different; see above. Additionally, MPT-30B is (partially)
    trained using a 8K context length, whereas most other open-source models (e.g.,
    LLaMA, Falcon, and MPT-7B) are trained using a shorter context length of 2K tokens.
    More specifically, we see that MPT-30B uses a training curriculum in which the
    model is first trained with a 2K context length, then switches to an 8K context
    length later in training. During this second phase, the proportion of code in
    the dataset is increased by `2.5X`, leading the resulting model to have improved
    coding abilities compared to other open-source LLMs.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: MPT-30B 的预训练数据集类似于 MPT-7B，但数据的混合略有不同；见上文。此外，MPT-30B 部分使用 8K 上下文长度进行训练，而大多数其他开源模型（例如，LLaMA、Falcon
    和 MPT-7B）使用较短的 2K 令牌上下文长度进行训练。更具体地说，我们看到 MPT-30B 使用一种训练课程，模型首先使用 2K 上下文长度进行训练，然后在训练后期切换到
    8K 上下文长度。在第二阶段，数据集中代码的比例增加了 `2.5X`，使得最终模型在编程能力上比其他开源 LLM 更强。
- en: '**Model variants.** In addition to the MPT-30B base model, authors in [2] release
    chat and instruct variants of MPT-30B. These models follow a similar training
    strategy as MPT-7B-Instruct and MPT-7B-Chat. However, the data used for instruction
    tuning is significantly expanded for both of these models. Interestingly, MPT-30B-Chat
    is found to have impressive programing skills!'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型变体。** 除了 MPT-30B 基础模型外，[2] 中的作者还发布了 MPT-30B 的聊天和指令变体。这些模型遵循类似于 MPT-7B-Instruct
    和 MPT-7B-Chat 的训练策略。然而，这些模型用于指令调优的数据显著增加。有趣的是，发现 MPT-30B-Chat 在编程技能上表现出色！'
- en: Does it perform well?
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它表现得好吗？
- en: '![](../Images/7f9063398a8d60ba7f0ffd8a308921f9.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7f9063398a8d60ba7f0ffd8a308921f9.png)'
- en: (from [2])
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [2]）
- en: In addition to outperforming MPT-7B across a variety of categories, MPT-30B
    achieves comparable performance to top open-source alternatives like LLaMA-30B
    and Falcon-40B; see above. In general, we see that MPT-30B lags behind Falcon
    and LLaMA in solving text-based tasks, but tends to outperform these models on
    programming-related problems (likely due to the higher ratio of code in the pre-training
    dataset!). Notably, we see that MPT-30B outperforms GPT-3 on a variety of in-context
    learning tasks; see below.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在各种类别上优于 MPT-7B 外，MPT-30B 的表现与顶级开源替代品如 LLaMA-30B 和 Falcon-40B 相当；见上文。总体而言，我们发现
    MPT-30B 在解决基于文本的任务时落后于 Falcon 和 LLaMA，但在编程相关问题上通常优于这些模型（可能是因为预训练数据集中代码的比例较高！）。值得注意的是，我们发现
    MPT-30B 在各种上下文学习任务上优于 GPT-3；见下文。
- en: '![](../Images/6d2fb7533c554c9ac72d5323b2c9b96f.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d2fb7533c554c9ac72d5323b2c9b96f.png)'
- en: (from [2])
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [2]）
- en: With this result in mind, it seems that models like MPT-30B could potentially
    lay the foundation for open-source LLM applications that rival the quality of
    proprietary systems. All we need is sufficient refinement and fine-tuning!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 结合这些结果来看，像 MPT-30B 这样的模型可能为开源 LLM 应用奠定了基础，能够与专有系统的质量相媲美。我们所需要的只是足够的精细调整和微调！
- en: Final Remarks
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终备注
- en: “You can train, finetune, and deploy your own private MPT models, either starting
    from one of our checkpoints or training from scratch” *— from [2]*
  id: totrans-94
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “你可以训练、微调和部署你自己的私人 MPT 模型，可以从我们的检查点之一开始，或从头开始训练” *— 来自 [2]*
- en: The foundation models provided by MosaicML are a huge step forward for the open-source
    LLM community, as they provide commercially-usable LLMs that are comparable to
    popular base models like LLaMA and GPT-3\. However, this open-source offering
    goes beyond the MPT models themselves — it includes an [open-source codebase for
    training LLMs](https://github.com/mosaicml/llm-foundry), a variety of [online
    demos](https://huggingface.co/mosaicml), and more.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: MosaicML 提供的基础模型是开源 LLM 社区的一大进步，因为它们提供了与 LLaMA 和 GPT-3 等流行基础模型相当的商用 LLM。然而，这一开源产品不仅仅限于
    MPT 模型本身——它还包括一个 [用于训练 LLM 的开源代码库](https://github.com/mosaicml/llm-foundry)，各种
    [在线演示](https://huggingface.co/mosaicml) 等。
- en: '![](../Images/d1e644e4c8b767e0c765327d15d87d49.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1e644e4c8b767e0c765327d15d87d49.png)'
- en: (from [2])
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [2])
- en: The MPT-7B and 30B models come with an entire ecosystem of open-source tools
    that can be used to create specialized/personalized LLMs. Given that creating
    the base model is the most expensive aspect of any LLM-based system (see above),
    these tools significantly lower the barrier to entry for working with LLMs and
    provide a starting point for solving a variety of downstream applications. Remember,
    [fine-tuning is extremely effective](https://magazine.sebastianraschka.com/i/125373356/finetuning-task-specific-llms-for-your-business-needs)
    (i.e., hard to beat by just prompting a more generic LLM) when we have a particular
    task that we are trying to solve!
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: MPT-7B 和 30B 模型配备了一个完整的开源工具生态系统，可用于创建专业化/个性化的LLMs。鉴于创建基础模型是任何基于LLM系统中最昂贵的部分（见上文），这些工具显著降低了使用LLM的门槛，并提供了一个解决各种下游应用的起点。记住，当我们有一个特定的任务需要解决时，[微调是极其有效的](https://magazine.sebastianraschka.com/i/125373356/finetuning-task-specific-llms-for-your-business-needs)（即，仅通过提示一个更通用的LLM难以超越）！
- en: Connect with me!
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与我联系！
- en: Thanks so much for reading this article. I am [Cameron R. Wolfe](https://cameronrwolfe.me/),
    Director of AI at [Rebuy](https://www.rebuyengine.com/). I study the empirical
    and theoretical foundations of deep learning. If you liked this overview, subscribe
    to my [Deep (Learning) Focus newsletter](https://cameronrwolfe.substack.com/),
    where I help readers understand AI research via overviews of relevant topics from
    the ground up. You can also follow me on [X](https://twitter.com/cwolferesearch)
    and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/), or
    check out my [other writings](https://medium.com/@wolfecameron) on medium!
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢您阅读这篇文章。我是 [Cameron R. Wolfe](https://cameronrwolfe.me/)，[Rebuy](https://www.rebuyengine.com/)
    的 AI 总监。我研究深度学习的经验和理论基础。如果您喜欢这个概述，请订阅我的 [Deep (Learning) Focus 新闻通讯](https://cameronrwolfe.substack.com/)，在其中我通过从基础上概述相关主题帮助读者理解
    AI 研究。您还可以在 [X](https://twitter.com/cwolferesearch) 和 [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)
    上关注我，或者查看我在 medium 上的 [其他文章](https://medium.com/@wolfecameron)！
- en: Bibliography
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] “Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable
    Llms.” *MosaicML*, 5 May 2023, [www.mosaicml.com/blog/mpt-7b.](http://www.mosaicml.com/blog/mpt-7b.)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] “介绍 MPT-7B: 开源商用 LLM 的新标准。” *MosaicML*，2023年5月5日，[www.mosaicml.com/blog/mpt-7b.](http://www.mosaicml.com/blog/mpt-7b.)'
- en: '[2] “MPT-30B: Raising the Bar for Open-Source Foundation Models.” *MosaicML*,
    22 June 2023, [www.mosaicml.com/blog/mpt-30b.](http://www.mosaicml.com/blog/mpt-30b.)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] “MPT-30B: 提升开源基础模型的标准。” *MosaicML*，2023年6月22日，[www.mosaicml.com/blog/mpt-30b.](http://www.mosaicml.com/blog/mpt-30b.)'
- en: '[3] Touvron, Hugo, et al. “Llama: Open and efficient foundation language models.”
    *arXiv preprint arXiv:2302.13971* (2023).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Touvron, Hugo, 等。“Llama: 开放且高效的基础语言模型。” *arXiv 预印本 arXiv:2302.13971* (2023)。'
- en: '[4] Hoffmann, Jordan, et al. “Training compute-optimal large language models.”
    *arXiv preprint arXiv:2203.15556* (2022).'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Hoffmann, Jordan, 等。“训练计算最优的大型语言模型。” *arXiv 预印本 arXiv:2203.15556* (2022)。'
- en: '[5] Zhang, Susan, et al. “OPT: Open Pre-trained Transformer Language Models.”
    *arXiv preprint arXiv:2205.01068* (2022).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Zhang, Susan, 等。“OPT: 开放的预训练变换器语言模型。” *arXiv 预印本 arXiv:2205.01068* (2022)。'
- en: '[6] Press, Ofir, Noah A. Smith, and Mike Lewis. “Train short, test long: Attention
    with linear biases enables input length extrapolation.” *arXiv preprint arXiv:2108.12409*
    (2021).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Press, Ofir, Noah A. Smith, 和 Mike Lewis。“训练短期，测试长期：具有线性偏置的注意力机制实现输入长度外推。”
    *arXiv 预印本 arXiv:2108.12409* (2021)。'
- en: '[7] Dao, Tri, et al. “Flashattention: Fast and memory-efficient exact attention
    with io-awareness.” *Advances in Neural Information Processing Systems* 35 (2022):
    16344–16359.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Dao, Tri, 等。“Flashattention: 快速且内存高效的准确注意力机制，具有IO感知能力。” *神经信息处理系统进展* 35
    (2022): 16344–16359。'
- en: '[8] Chen, Xiangning, et al. “Symbolic discovery of optimization algorithms.”
    *arXiv preprint arXiv:2302.06675* (2023).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] 陈向宁等人。“优化算法的符号发现。” *arXiv 预印本 arXiv:2302.06675* (2023)。'
- en: '[9] Brown, Tom, et al. “Language models are few-shot learners.” *Advances in
    neural information processing systems* 33 (2020): 1877–1901.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] 布朗汤姆等人。“语言模型是少样本学习者。” *神经信息处理系统进展* 33 (2020): 1877–1901。'
- en: '[10] Devlin, Jacob, et al. “Bert: Pre-training of deep bidirectional transformers
    for language understanding.” *arXiv preprint arXiv:1810.04805* (2018).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] 德夫林雅各布等人。“Bert: 深度双向变换器的预训练以实现语言理解。” *arXiv 预印本 arXiv:1810.04805* (2018)。'
- en: '[11] Radford, Alec, et al. “Language Models are Unsupervised Multitask Learners.”'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] 拉德福德亚历克等人。“语言模型是无监督的多任务学习者。”'
- en: '[12] Ouyang, Long, et al. “Training language models to follow instructions
    with human feedback.” *Advances in Neural Information Processing Systems* 35 (2022):
    27730–27744.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] 欧阳龙等人。“通过人类反馈训练语言模型以遵循指令。” *神经信息处理系统进展* 35 (2022): 27730–27744。'
- en: '[13] Glaese, Amelia, et al. “Improving alignment of dialogue agents via targeted
    human judgements.” *arXiv preprint arXiv:2209.14375* (2022).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] 格莱斯艾米莉亚等人。“通过有针对性的人工评判改进对话代理的对齐。” *arXiv 预印本 arXiv:2209.14375* (2022)。'
