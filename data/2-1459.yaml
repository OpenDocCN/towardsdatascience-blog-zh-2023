- en: Machine Learning in a Non-Euclidean Space
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非欧几里得空间中的机器学习
- en: 原文：[https://towardsdatascience.com/machine-learning-in-a-non-euclidean-space-99b0a776e92e](https://towardsdatascience.com/machine-learning-in-a-non-euclidean-space-99b0a776e92e)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/machine-learning-in-a-non-euclidean-space-99b0a776e92e](https://towardsdatascience.com/machine-learning-in-a-non-euclidean-space-99b0a776e92e)
- en: '![](../Images/33e8dc96ecf4a66659e0c47e33bde03e.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/33e8dc96ecf4a66659e0c47e33bde03e.png)'
- en: Photo by [Greg Rosenke](https://unsplash.com/@greg_rosenke?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Greg Rosenke](https://unsplash.com/@greg_rosenke?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Chapter I. Why you should learn about non-Euclidean ML
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一章：为什么你应该学习非欧几里得机器学习
- en: '[](https://medium.com/@mastafa.foufa?source=post_page-----99b0a776e92e--------------------------------)[![Mastafa
    Foufa](../Images/2e0b26ed83f04e943438afa1aab462a8.png)](https://medium.com/@mastafa.foufa?source=post_page-----99b0a776e92e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----99b0a776e92e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----99b0a776e92e--------------------------------)
    [Mastafa Foufa](https://medium.com/@mastafa.foufa?source=post_page-----99b0a776e92e--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mastafa.foufa?source=post_page-----99b0a776e92e--------------------------------)[![Mastafa
    Foufa](../Images/2e0b26ed83f04e943438afa1aab462a8.png)](https://medium.com/@mastafa.foufa?source=post_page-----99b0a776e92e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----99b0a776e92e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----99b0a776e92e--------------------------------)
    [Mastafa Foufa](https://medium.com/@mastafa.foufa?source=post_page-----99b0a776e92e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----99b0a776e92e--------------------------------)
    ·10 min read·Jun 16, 2023
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----99b0a776e92e--------------------------------)
    ·阅读时间10分钟·2023年6月16日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '“Is our comfortable and familiar Euclidean space and its linear structure always
    the right place for machine learning? Recent research argues otherwise: it is
    not always needed and sometimes harmful, as demonstrated by a wave of exciting
    work. Starting with the notion of hyperbolic representations for hierarchical
    data two years ago, a major push has resulted in new ideas for representations
    in non-Euclidean spaces, new algorithms and models with non-Euclidean data and
    operations, and new perspectives on the underlying functionality of non-Euclidean
    ML.” *by* [*Fred Sala*](http://stanford.edu/~fredsala)*,* [*Ines Chami*](http://web.stanford.edu/~chami/)*,*
    [*Adva Wolf*](https://mathematics.stanford.edu/people/adva-wolf)*,* [*Albert Gu*](http://web.stanford.edu/~albertgu/)*,*
    [*Beliz Gunel*](http://web.stanford.edu/~bgunel/) *and* [*Chris Ré*](https://cs.stanford.edu/people/chrismre/),
    [2019](https://dawn.cs.stanford.edu/2019/10/10/noneuclidean/)'
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我们的舒适和熟悉的欧几里得空间及其线性结构是否总是适合机器学习？近期研究对此提出了不同的看法：并非总是必要，有时甚至有害，这一点在一系列令人兴奋的工作中得到了证明。自从两年前提出双曲表示法用于层次数据以来，取得了重大进展，产生了非欧几里得空间表示的新思想、新算法和模型，以及对非欧几里得机器学习基本功能的新视角。”
    *作者* [*Fred Sala*](http://stanford.edu/~fredsala)*、* [*Ines Chami*](http://web.stanford.edu/~chami/)*、*
    [*Adva Wolf*](https://mathematics.stanford.edu/people/adva-wolf)*、* [*Albert Gu*](http://web.stanford.edu/~albertgu/)*、*
    [*Beliz Gunel*](http://web.stanford.edu/~bgunel/) *和* [*Chris Ré*](https://cs.stanford.edu/people/chrismre/)，
    [2019](https://dawn.cs.stanford.edu/2019/10/10/noneuclidean/)
- en: What you will learn in this article.
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你将在本文中学到什么。
- en: Distortion measures how well distance is preserved when representing data in
    another space.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扭曲度量了在将数据表示为另一个空间时距离保留的效果。
- en: For some data, Euclidean space implies high distortion, so non-Euclidean spaces
    like spherical or hyperbolic spaces are used instead.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于某些数据，欧几里得空间会产生较高的扭曲，因此使用像球面或双曲空间这样的非欧几里得空间。
- en: Riemannian geometry tools like manifolds and Riemannian metric are used for
    non-Euclidean Machine Learning.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黎曼几何工具如流形和黎曼度量被用于非欧几里得机器学习。
- en: Manifolds are curved spaces that are locally Euclidean.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流形是局部上欧几里得的曲面。
- en: Exponential and logarithmic maps are used to go from a manifold to its tangent
    space.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指数映射和对数映射用于从流形到其切空间的转换。
- en: Riemannian metric allows to compute shortest distances on the manifold.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黎曼度量允许在流形上计算最短距离。
- en: Before going further in this series about non-Euclidean geometry applied to
    Machine Learning (*ML*), I had to answer an important question. **Is it worth
    learning more about non-Euclidean ML?**
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续阅读本系列关于应用于机器学习（*ML*）的非欧几里得几何之前，我必须回答一个重要的问题。**是否值得深入了解非欧几里得ML？**
- en: To answer such a question, I started by researching non-Euclidean ML. I quickly
    ended up finding a couple of resources. The very first one is from Stanford and
    the citation above is extracted from it. The authors argue that Machine Learning
    was designed with a certain geometry, namely the Euclidean geometry, more by tradition
    or convenience, than by rational thinking.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这个问题，我开始研究非欧几里得ML。我很快找到了一些资源，第一个是来自斯坦福的，以上引用就是摘自其中。作者认为，机器学习的设计采用了某种几何，即欧几里得几何，这更多是传统或便利的选择，而非理性思考的结果。
- en: So far, the choice of the Euclidean geometry doesn’t seem like a major problem.
    But the authors trigger our attention by citing Bronstein et al. in their [seminal
    description of the geometric deep learning](https://arxiv.org/pdf/1611.08097.pdf)
    paradigm.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，选择欧几里得几何似乎并不是一个大问题。但作者通过引用Bronstein等人在其[开创性几何深度学习描述](https://arxiv.org/pdf/1611.08097.pdf)中引起了我们的注意。
- en: “[m]any scientific fields study data with an underlying structure that is a
    non-Euclidean space.” [Bronstein et al.](https://arxiv.org/pdf/1611.08097.pdf)
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “[m]许多科学领域研究具有非欧几里得空间基础结构的数据。” [Bronstein et al.](https://arxiv.org/pdf/1611.08097.pdf)
- en: 'As I continued reading the article, I stumbled upon an aspect that I wasn’t
    familiar with: the notion of space **flatness**.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当我继续阅读文章时，我遇到了一个我不熟悉的方面：空间**平坦性**的概念。
- en: “We have chosen to work with Euclidean space, with all of its inherent properties,
    among the most critical of which is its **flatness**.” [*Fred Sala*](http://stanford.edu/~fredsala)*,*
    [*Ines Chami*](http://web.stanford.edu/~chami/)*,* [*Adva Wolf*](https://mathematics.stanford.edu/people/adva-wolf)*,*
    [*Albert Gu*](http://web.stanford.edu/~albertgu/)*,* [*Beliz Gunel*](http://web.stanford.edu/~bgunel/)
    *and* [*Chris Ré*](https://cs.stanford.edu/people/chrismre/), [2019](https://dawn.cs.stanford.edu/2019/10/10/noneuclidean/)
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我们选择了使用欧几里得空间，其固有的属性中最关键的之一就是**平坦性**。” [*Fred Sala*](http://stanford.edu/~fredsala)*,*
    [*Ines Chami*](http://web.stanford.edu/~chami/)*,* [*Adva Wolf*](https://mathematics.stanford.edu/people/adva-wolf)*,*
    [*Albert Gu*](http://web.stanford.edu/~albertgu/)*,* [*Beliz Gunel*](http://web.stanford.edu/~bgunel/)
    *和* [*Chris Ré*](https://cs.stanford.edu/people/chrismre/)， [2019](https://dawn.cs.stanford.edu/2019/10/10/noneuclidean/)
- en: 'The authors of the Stanford article mention the impact of flatness. Below are
    the three points raised and you should read our series to get more intuition:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 斯坦福文章的作者提到了平坦性的影响。以下是提出的三点，你应该阅读我们的系列文章以获得更多直观的理解：
- en: '**Better representations** — they argue that Euclidean spaces are not fit for
    certain datasets like hierarchical datasets that can be described by trees.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更好的表示** — 他们认为欧几里得空间不适合某些数据集，例如可以用树描述的层次数据集。'
- en: '**Unlocking the full potential of models** — they argue that to push the barriers
    in terms of model performance, we could improve the space the data lie in, by
    moving from Euclidean geometry to non-Euclidean geometry.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**释放模型的全部潜力** — 他们认为，为了推动模型性能的界限，我们可以通过将数据所在的空间从欧几里得几何转变为非欧几里得几何来改进。'
- en: '**More flexible operations** — they argue that operations in non-Euclidean
    spaces are more flexible and require less dimensions. The authors explain that
    later in their article. But we will try to simplify that in our Medium series.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更灵活的操作** — 他们认为非欧几里得空间中的操作更加灵活，所需维度更少。作者在文章中后面解释了这一点。我们将尽量在我们的Medium系列中简化这一点。'
- en: Representing a non-flat entity into a flat space
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将非平坦实体表示到平面空间中
- en: It is important to have an appropriate geometry, conditionally to the input
    data. Below, we show an example of non-Euclidean data that is “forced” to fit
    in a 2-dimensional Euclidean space. This is our **well known spherical planet**
    that is flattened into a plane. However, this is done with **non negligible distortions**.
    By distortion, we mean that distances are not preserved, from the original space
    [*Earth — sphere*] to the space where the data is represented [*Maps — plane*].
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的几何非常重要，依赖于输入数据。下面，我们展示了一个非欧几里得数据的例子，这些数据被“强迫”适应于二维欧几里得空间。这是我们**众所周知的球形地球**被压缩成平面。然而，这种转换伴随着**不可忽视的扭曲**。所谓扭曲是指从原始空间[*地球
    — 球体*]到数据表示的空间[*地图 — 平面*]，距离没有被保留。
- en: For example, below Mexico has ***in reality*** almost the same surface as Greenland
    (*right*) but appears much smaller in the actual projection (*left*).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，下图中的墨西哥在***实际情况中***几乎与格林兰（*右*）具有相同的表面，但在实际投影中（*左*）看起来要小得多。
- en: '![](../Images/a79caf56683cbffe3194b997a903c659.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a79caf56683cbffe3194b997a903c659.png)'
- en: '**Resource:** From the authors. Note that the World map (left) is using the
    Mercator projection of our spherical planet. The Mercator map is defined by the
    formula (x, y) = λ, log tan(π/4 + φ/2). *Adapted from* [*Wikipedia*](https://commons.wikimedia.org/wiki/File:World_map_-_low_resolution_chain_test.svg)*.*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源：** 作者提供。请注意，世界地图（左）使用的是我们球面星球的墨卡托投影。墨卡托地图由公式 (x, y) = λ, log tan(π/4 +
    φ/2) 定义。*改编自* [*维基百科*](https://commons.wikimedia.org/wiki/File:World_map_-_low_resolution_chain_test.svg)*.*'
- en: There are many ways to represent our earth that all involve some degree of distortion.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 表示地球的方式有很多种，这些方式都涉及一定程度的扭曲。
- en: '![](../Images/74da35a7c4dad90e88628d6619c531b5.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/74da35a7c4dad90e88628d6619c531b5.png)'
- en: '**Resource:** Projecting the globe, with distortion. From the authors, adapted
    from [Wikipedia](https://en.wikipedia.org/wiki/Mercator_projection).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源：** 全球投影，带有扭曲。来自作者，改编自[维基百科](https://en.wikipedia.org/wiki/Mercator_projection)。'
- en: For example, distortion is naturally observed in the famous Mercator projection.
    The Greenland problem showcases the loss of information when moving from a spherical
    representation to a plane representation with such projection. This projection
    is not **area preserving,** a core property expected in this case. Indeed, Greenland,
    with an area of about 2.2 million square kilometers, looks larger than South America,
    with an area of about 17.8 million square kilometers. This Mercator projection
    preserves angles but not areas, hence making it non-perfect.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在著名的墨卡托投影中，自然观察到扭曲。格林兰问题展示了从球面表示转到平面表示时信息的丢失。这种投影不是**面积保持的**，这是在这种情况下期望的核心属性。实际上，格林兰的面积约为220万平方公里，比南美洲（面积约1780万平方公里）看起来要大。这种墨卡托投影保持角度但不保持面积，因此使其不完美。
- en: 'Now, other datasets, are also forced to lie in an Euclidean space whereby we
    observe distortions. This is the case of **graphs**: in a Euclidean space, we
    cannot embed large classes of graphs without low distortion or without loss of
    information.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，其他数据集也被迫位于欧几里得空间中，我们观察到扭曲。这是**图形**的情况：在欧几里得空间中，我们不能在低扭曲或信息丢失的情况下嵌入大类图形。
- en: '**Distortion** has several more rigorous mathematical definitions. Essentially,
    we want distortion to measure the quality of the embedding by evaluating how well
    distances are preserved. Here, we define it as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**扭曲**有几个更严格的数学定义。从本质上讲，我们希望扭曲能够通过评估距离保持的好坏来衡量嵌入的质量。这里，我们定义如下：'
- en: Distortion ~ AVG {Graph distance / Embedding distance }
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 扭曲 ~ AVG {图形距离 / 嵌入距离 }
- en: '**Example.**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**例子。**'
- en: In the figure below, we can prove through the Poincare-type inequalities that
    we cannot embed the two cycles (*square*, *circle*) in a Euclidean space without
    distortion. Note that a distortion of 1 is a perfect distortion — graph distances
    are exactly matching the embedding space distances. **Any distortion different
    than 1 means we are not preserving the graph distances.**
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们可以通过庞加莱型不等式证明，我们不能将两个循环（*方形*，*圆形*）嵌入到欧几里得空间中而不扭曲。注意，扭曲为1是完美的扭曲——图形距离完全匹配嵌入空间距离。**任何与1不同的扭曲意味着我们没有保持图形距离。**
- en: '![](../Images/42350b592503cae1e5375925361599e4.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/42350b592503cae1e5375925361599e4.png)'
- en: '**Resource:** From the authors. Optimal embedding of a cycle of length 4 [left],
    and optimal embedding of the 3-star K(1,3) [right]. Adapted from Octavian Ganea’s
    lecture at ETH Zurich.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源：** 作者提供。长度为4的循环的最佳嵌入[左]，以及3-star K(1,3)的最佳嵌入[右]。改编自Octavian Ganea在苏黎世联邦理工学院的讲座。'
- en: On the square above, the two opposite nodes on the diagonal have a distance
    of 2 in terms of **graph distance.** However, the shortest path in the Euclidean
    embedding has a distance of **√2.**
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的方形中，对角线上的两个对立节点在**图形距离**上有2的距离。然而，欧几里得嵌入中的最短路径距离为**√2。**
- en: This concept of distortion is really important as Euclidean geometry does not
    allow having the ideal “projection” of graph data. In particular, for hierachical
    graph data, to minimize distortion, **a solution is to use a hyperbolic space**.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这个扭曲的概念非常重要，因为欧几里得几何不允许对图形数据进行理想的“投影”。特别地，对于层次图形数据，为了最小化扭曲，**一种解决方案是使用双曲空间**。
- en: '**Note**. We will learn more about this example of non-Euclidean space in the
    next chapter.'
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意**。我们将在下一章中深入了解这个非欧几里得空间的例子。'
- en: Representing data in a non-Euclidean space
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在非欧几里得空间中表示数据
- en: It is hard understanding how we can represent data in any other way than with
    vectors in ***Rn***. Plus, how can we move away from the Euclidean distance we
    know so well to compare two vector representations?
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 很难理解我们如何用除了***Rn***以外的方式表示数据。而且，我们如何摆脱我们非常熟悉的欧几里得距离来比较两个向量表示？
- en: A solution is described by manifolds, in Riemannian geometry. Manifolds are
    objects that look like ***Rn but only locally.*** *That means we can locally use
    vectors for representing our data points. But only locally!*
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一种解决方案由黎曼几何中的流形描述。流形是看起来像***Rn 但仅在局部。*** *这意味着我们可以在局部使用向量来表示我们的数据点。但仅在局部！*
- en: '![](../Images/9cba2f31402dd14b9d086d73a44c3a96.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9cba2f31402dd14b9d086d73a44c3a96.png)'
- en: '**Resource**: The tangent space [Light grey, TxM] at a point x from the manifol
    M [Dark grey] and its tangent vector v. The vector x from the Manifold can be
    represented locally in the Euclidean tangent space. From [Wikipedia.](https://en.wikipedia.org/wiki/Tangent_space#)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源**：在流形 M [深灰色] 上的一个点 x 处的切空间 [浅灰色，TxM] 和它的切向量 v。流形中的向量 x 可以在欧几里得切空间中局部表示。来自
    [维基百科](https://en.wikipedia.org/wiki/Tangent_space#)'
- en: The notion of similarity or distances is key in Machine Learning. If we are
    building say an NLP model, we want to preserve the notion of similarity in semantics
    within the embedding space that represents textual input. In other words, we want
    two words that are similar in meaning to also be similar in the Euclidean space,
    i.e. with a low Euclidean distance. Similarly, two words that are dissimilar in
    meaning should be far away in the Euclidean space, i.e. with a high Euclidean
    distance.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 相似性或距离的概念在机器学习中至关重要。如果我们在构建一个 NLP 模型，我们希望在表示文本输入的嵌入空间中保留语义上的相似性。换句话说，我们希望语义相似的两个词在欧几里得空间中也相似，即欧几里得距离低。同样，语义不相似的两个词在欧几里得空间中应当距离较远，即欧几里得距离高。
- en: Hence, there needs to be an equivalent approach when escaping Euclidean geometry.
    This approach is described by a *Riemannian metric.* The **Riemannian metric allows
    us to compare two entities in the non-Euclidean space and preserve this intuitive
    notion of distance**.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当逃避欧几里得几何时，需要有一个等效的方法。这种方法由*黎曼度量*来描述。**黎曼度量使我们能够在非欧几里得空间中比较两个实体，并保留这种直观的距离概念**。
- en: 👀 I remember.
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 👀 我记得了。
- en: Now, we need to remember that in this non-Euclidean framework we can perform
    operations locally on our data representations and we have a metric to measure
    distances. Thus, we are equipped to do ML in non-Euclidean spaces.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要记住，在这个非欧几里得框架中，我们可以对数据表示进行局部操作，并且我们有一个度量来测量距离。因此，我们具备了在非欧几里得空间中进行机器学习的能力。
- en: 🙌🏻 Why should I learn more about ML in a non-Euclidean space?
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 🙌🏻 为什么我要在非欧几里得空间中学习更多关于机器学习的知识？
- en: So far, we know that ML without the genius [Euclid](https://en.wikipedia.org/wiki/Euclid)
    is actually something. There are actual projects that exist and approach our traditional
    machine learning problems with a different geometry framework.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们知道没有天才[欧几里得](https://en.wikipedia.org/wiki/Euclid)的机器学习实际上是有意义的。确实存在一些项目，它们用不同的几何框架来解决我们传统的机器学习问题。
- en: 'Now, a question naturally emerges: **is it worth our time learning more than
    the existence of this field?**'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，一个自然的问题出现了：**值得花时间了解这个领域的存在吗？**
- en: It is a rather scary space that involves non-trivial mathematics. But my friend,
    Aniss Medbouhi, [ML Doctoral Researcher at KTH](https://www.linkedin.com/in/aniss-medbouhi/),
    will help us get past the inherent complexity of this space.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当令人畏惧的空间，涉及复杂的数学。但我的朋友 Aniss Medbouhi，[KTH 的 ML 博士研究员](https://www.linkedin.com/in/aniss-medbouhi/)，将帮助我们克服这个空间固有的复杂性。
- en: The other reason I wasn’t convinced about this space is that I read that it
    was mostly fit for hierarchical data that can be described by trees. At a first
    glance, it doesn’t involve the data I work with on a daily basis.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我之所以对这个空间不太信服的另一个原因是，我读到它主要适用于可以用树描述的层次数据。乍一看，这似乎与我每天处理的数据无关。
- en: 'However, the abstracts below give us an idea of relevant datasets of interest:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，下面的摘要给了我们一些相关数据集的概念：
- en: “However, **recent work has shown that the appropriate isometric space for embedding
    complex networks is not the flat Euclidean space**, **but negatively curved, hyperbolic
    space**. We present a new concept that exploits these recent insights and propose
    learning neural embeddings of graphs in hyperbolic space. We provide experimental
    evidence that embedding graphs in their natural geometry signicantly improves
    performance on downstream tasks for several **real-world public datasets**.” [Chamberlain
    et al.](https://arxiv.org/pdf/1705.10359.pdf)
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “然而，**最近的研究表明，嵌入复杂网络的适当等距空间不是平坦的欧几里得空间，而是负曲率的双曲空间**。我们提出了一个新概念，利用这些最新的见解，建议在双曲空间中学习图的神经嵌入。我们提供了实验证据，表明在其自然几何中嵌入图显著改善了在多个**真实世界公共数据集**上的下游任务表现。”
    [Chamberlain 等](https://arxiv.org/pdf/1705.10359.pdf)
- en: ''
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: “However, while **complex symbolic datasets often exhibit a latent hierarchical
    structure, state-of-the-art methods typically learn embeddings in Euclidean vector
    spaces, which do not account for this property**. For this purpose, we introduce
    a new approach for learning hierarchical representations of symbolic data by **embedding
    them into hyperbolic space** — or more precisely into an n-dimensional Poincaré
    ball.” [Nickel and Kiela](https://arxiv.org/pdf/1705.08039.pdf)
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “然而，虽然**复杂的符号数据集通常表现出潜在的层级结构，最先进的方法通常在欧几里得向量空间中学习嵌入，而这并未考虑到这一特性**。为此，我们引入了一种新的方法，通过**将其嵌入双曲空间**——或更准确地说，嵌入到n维庞加莱球体中——来学习符号数据的层级表示。”
    [Nickel 和 Kiela](https://arxiv.org/pdf/1705.08039.pdf)
- en: 'The datasets mentioned above are listed as follows, by [Chamberlain et al.](https://arxiv.org/pdf/1705.10359.pdf)
    :'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 上述数据集列举如下，由[Chamberlain 等](https://arxiv.org/pdf/1705.10359.pdf)提供：
- en: '(1) Karate: Zachary’s karate club contains 34 vertices divided into two factions.
    [4]'
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '(1) Karate: 扎卡里（Zachary）的空手道俱乐部包含34个顶点，分为两个派系。[4]'
- en: ''
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '(2) Polbooks: A network of books about US politics published around the time
    of the 2004 presidential election and sold by the online bookseller Amazon.com.
    Edges between books represent frequent co-purchasing of books by the same buyers.'
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '(2) Polbooks: 关于美国政治的书籍网络，这些书籍是在2004年总统选举前后出版，并由在线书商Amazon.com出售。书籍之间的边表示相同买家的频繁共同购买。'
- en: ''
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '(3) Football: A network of American football games between Division IA colleges
    during regular season Fall 2000\. [2]'
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '(3) Football: 2000年秋季常规赛期间，Division IA大学之间的美国橄榄球比赛网络。[2]'
- en: ''
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '(4) Adjnoun: Adjacency network of common adjectives and nouns in the novel
    David Coppereld by Charles Dickens. [3]'
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '(4) Adjnoun: 查尔斯·狄更斯（Charles Dickens）小说《大卫·科波菲尔（David Copperfield）》中常见形容词和名词的邻接网络。[3]'
- en: ''
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '(5) Polblogs: A network of hyperlinks between weblogs on US politics, recorded
    in 2005\. [1]'
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '(5) Polblogs: 记录于2005年的美国政治博客之间的超链接网络。[1]'
- en: 'Additionally, in biology, we find this reference dataset:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在生物学中，我们找到以下参考数据集：
- en: '*Biology: Evolutionary data like proteins.* [5]'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*生物学：诸如蛋白质等进化数据。*[5]'
- en: '![](../Images/2af3a2d5ffcf1f54c748242329d868cc.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2af3a2d5ffcf1f54c748242329d868cc.png)'
- en: '**Resource**: A network representation of social relationships among the 34
    individuals in the karate club studied by Zachary. The population is divided into
    two fractions based on an event [4]. Adapted from [Wikipedia](https://en.wikipedia.org/wiki/Zachary%27s_karate_club).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源**：扎卡里的空手道俱乐部中34个人的社会关系网络表示。根据事件[4]，该人群被分为两个部分。改编自[维基百科](https://en.wikipedia.org/wiki/Zachary%27s_karate_club)。'
- en: Finally, NLP data, i.e. textual data, is another type of hierarchical data.
    As a result, a lot of domains may benefit from understanding the advancements
    in non-Euclidean Machine Learning.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，NLP数据，即文本数据，是另一种类型的层级数据。因此，许多领域可能从理解非欧几里得机器学习的进展中受益。
- en: Now that we know how to better represent certain datasets, it is key to tie
    it back to Machine Learning. Any downstream ML tasks require ingesting data first.
    A lot of time is spent in cleaning our underlying data and representing it accurately.
    The quality of the data representation is essential as it directly impacts the
    performance of our models. For example, in NLP, I advise my students to focus
    on architectures that provide good embeddings, e.g. contextual embeddings. There
    has been extensive research in improving embeddings, moving from shallow neural
    networks (*fasttext, word2vec*) to deep neural networks and transformers *(sentence-transformers,
    BERT, RoBERTa, XLM)*. However, it is also worth noting that data representation
    is very much linked to the task at hand, and research shows that certain shallow
    neural networks provide better results than deep neural networks, for certain
    tasks.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何更好地表示某些数据集，将其与机器学习联系起来是关键。任何下游机器学习任务都需要首先摄取数据。大量时间花在清理基础数据和准确表示数据上。数据表示的质量至关重要，因为它直接影响模型的性能。例如，在自然语言处理（NLP）中，我建议我的学生专注于提供良好嵌入的架构，例如上下文嵌入。关于改进嵌入的研究非常广泛，从浅层神经网络（*fasttext,
    word2vec*）到深层神经网络和变换器（*sentence-transformers, BERT, RoBERTa, XLM*）。然而，也值得注意的是，数据表示与手头的任务紧密相关，研究显示对于某些任务，某些浅层神经网络比深层神经网络提供更好的结果。
- en: Conclusion
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we saw that we can leverage non-Euclidean geometry to tackle
    existing problems specific to spherical data and hierarchical datasets like graphs.
    When embedding such datasets into a Euclidean space, the price to pay is a distortion
    that does not permit preserving distances from the original space to the embedding
    space. Such distortion is intuitive in our earth representation where we have
    many ways to represent our globe, some of which do not preserve core properties
    expected such as **area preserving.** Similarly for graphs, core properties need
    to be preserved and distorting the underlying space may result in poorer performance
    for downstream Machine Learning tasks.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们看到可以利用非欧几里得几何来解决特定于球面数据和层次数据集（如图）的现有问题。将这些数据集嵌入到欧几里得空间中，代价是扭曲，这种扭曲无法保留从原始空间到嵌入空间的距离。这种扭曲在我们地球的表示中很直观，我们有多种方式表示我们的地球，其中一些方式无法保留期望的核心属性，例如**面积保持**。对于图来说，核心属性需要被保留，扭曲基础空间可能导致下游机器学习任务的性能较差。
- en: In the next chapter, we will learn more about spherical and hyperbolic geometries.
    We will focus more on the latter and give intuition on how models in such space
    can better embed hierarchical data.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习更多关于球面几何和双曲几何的知识。我们将更多关注后者，并提供如何在这样的空间中更好地嵌入层次数据的直观理解。
- en: Connect with the contributors.
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与贡献者联系。
- en: '![](../Images/3aeef9a3a6d5979f8293050b0b599f88.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3aeef9a3a6d5979f8293050b0b599f88.png)'
- en: ML Doctoral Researcher at KTH Royal Institute of Technology.
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KTH皇家技术学院的机器学习博士研究员。
- en: '*Linkedin*. [https://www.linkedin.com/in/aniss-medbouhi/](https://www.linkedin.com/in/aniss-medbouhi/)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*Linkedin*. [https://www.linkedin.com/in/aniss-medbouhi/](https://www.linkedin.com/in/aniss-medbouhi/)'
- en: '![](../Images/dc4860e542b8410558a3768c6a07eaf3.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc4860e542b8410558a3768c6a07eaf3.png)'
- en: Data Scientist at Microsoft and Teacher at EPITA Paris.
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微软的数据科学家和EPITA巴黎的讲师。
- en: '*Linkedin*. [https://www.linkedin.com/in/mastafa-foufa/](https://www.linkedin.com/in/mastafa-foufa/)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*Linkedin*. [https://www.linkedin.com/in/mastafa-foufa/](https://www.linkedin.com/in/mastafa-foufa/)'
- en: '[1] Lada A. Adamic and Natalie Glance. The political blogosphere and the 2004
    U.S. election. Proceedings of the 3rd international workshop on Link discovery
    — LinkKDD ’05, pages 36–43, 2005.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Lada A. Adamic 和 Natalie Glance. 政治博客圈和2004年美国选举。《第三届国际链接发现研讨会会议录》——LinkKDD
    ’05, 页36–43, 2005。'
- en: '[2] Michelle Girvan and Mark E. J. Newman. Community structure in social and
    biological networks. In Proceedings of the national academy of sciences, 99:7821–7826,
    2002.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Michelle Girvan 和 Mark E. J. Newman. 社会和生物网络中的社区结构。在《国家科学院学报》会议录中，99:7821–7826,
    2002。'
- en: '[3] Mark E. J. Newman. Finding community structure in networks using the eigenvectors
    of matrices. Physical Review E — Statistical, Nonlinear, and Soft Matter Physics,
    74(3):1–19, 2006.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Mark E. J. Newman. 使用矩阵特征向量发现网络中的社区结构。《物理评论E》——统计学、非线性和软物质物理，74(3):1–19,
    2006。'
- en: '[4] Wayne W. Zachary. An information ow model for conict and ssion in small
    groups. Journal of anthropological research, 33:452–473, 1977.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Wayne W. Zachary. 小组中的冲突和分裂的信息流模型。《人类学研究杂志》，33:452–473, 1977。'
- en: '[5] AlQuraishi, Mohammed. “ProteinNet: a standardized data set for machine
    learning of protein structure.” BMC bioinformatics 20.1 (2019): 1–10.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] AlQuraishi, Mohammed. “ProteinNet: 一个标准化的蛋白质结构机器学习数据集。” BMC 生物信息学 20.1
    (2019): 1–10.'
