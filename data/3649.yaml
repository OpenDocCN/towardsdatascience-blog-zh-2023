- en: Large Language Models, MirrorBERT — Transforming Models into Universal Lexical
    and Sentence Encoders
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型，MirrorBERT——将模型转化为通用的词汇和句子编码器
- en: 原文：[https://towardsdatascience.com/large-language-models-mirrorbert-transforming-models-into-universal-lexical-and-sentence-511bd592da48?source=collection_archive---------10-----------------------#2023-12-12](https://towardsdatascience.com/large-language-models-mirrorbert-transforming-models-into-universal-lexical-and-sentence-511bd592da48?source=collection_archive---------10-----------------------#2023-12-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/large-language-models-mirrorbert-transforming-models-into-universal-lexical-and-sentence-511bd592da48?source=collection_archive---------10-----------------------#2023-12-12](https://towardsdatascience.com/large-language-models-mirrorbert-transforming-models-into-universal-lexical-and-sentence-511bd592da48?source=collection_archive---------10-----------------------#2023-12-12)
- en: Discover how mirror augmentation generates data and aces the BERT performance
    on semantic similarity tasks
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解镜像增强如何生成数据，并在语义相似性任务中提升BERT的性能
- en: '[](https://medium.com/@slavahead?source=post_page-----511bd592da48--------------------------------)[![Vyacheslav
    Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----511bd592da48--------------------------------)[](https://towardsdatascience.com/?source=post_page-----511bd592da48--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----511bd592da48--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----511bd592da48--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@slavahead?source=post_page-----511bd592da48--------------------------------)[![Vyacheslav
    Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----511bd592da48--------------------------------)[](https://towardsdatascience.com/?source=post_page-----511bd592da48--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----511bd592da48--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----511bd592da48--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flarge-language-models-mirrorbert-transforming-models-into-universal-lexical-and-sentence-511bd592da48&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----511bd592da48---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----511bd592da48--------------------------------)
    ·7 min read·Dec 12, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F511bd592da48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flarge-language-models-mirrorbert-transforming-models-into-universal-lexical-and-sentence-511bd592da48&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----511bd592da48---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flarge-language-models-mirrorbert-transforming-models-into-universal-lexical-and-sentence-511bd592da48&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----511bd592da48---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----511bd592da48--------------------------------)
    ·7分钟阅读·2023年12月12日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F511bd592da48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flarge-language-models-mirrorbert-transforming-models-into-universal-lexical-and-sentence-511bd592da48&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----511bd592da48---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F511bd592da48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flarge-language-models-mirrorbert-transforming-models-into-universal-lexical-and-sentence-511bd592da48&source=-----511bd592da48---------------------bookmark_footer-----------)![](../Images/786055ae126ec853b631f556033eba06.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F511bd592da48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flarge-language-models-mirrorbert-transforming-models-into-universal-lexical-and-sentence-511bd592da48&source=-----511bd592da48---------------------bookmark_footer-----------)![](../Images/786055ae126ec853b631f556033eba06.png)'
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: It is no secret that BERT-like models play a fundamental role in modern NLP
    applications. Despite their phenomenal performance on downstream tasks, most of
    these models are not that perfect on specific problems without fine-tuning. Embedding
    construction from raw pretrained models usually leads to metrics being far from
    state-of-the-art results. At the same time, fine-tuning is a heavy procedure and
    usually requires at least thousands of annotated data samples to make the model
    better understand the domain data. In some cases, this aspect becomes problematic
    when we cannot simply collect already annotated data or it comes with a high price.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 毫无疑问，类似 BERT 的模型在现代自然语言处理应用中扮演着基础性角色。尽管它们在下游任务上的表现非常出色，但大多数模型在特定问题上并不是那么完美，需要进行微调。从原始预训练模型构建的嵌入通常会导致指标远离最先进的结果。同时，微调是一个繁重的过程，通常需要至少几千个标注数据样本才能使模型更好地理解领域数据。在某些情况下，当我们无法简单地收集已标注的数据或数据价格高昂时，这一问题就会变得很棘手。
- en: '**MirrorBERT** was designed to overcome the aforementioned issue. Instead of
    the standard fine-tuning algorithm, MirrorBERT relies on self-supervision by smartly
    augmenting initial data without any external knowledge. This approach allows MirrorBERT
    to show comparable performance on *semantic similarity problems*. Furthermore,
    by using its innovative contrastive learning technique, MirrorBERT can transform
    pretrained models like [BERT](/bert-3d1bf880386a) or [RoBERTa](https://medium.com/towards-data-science/roberta-1ef07226c8d8)
    into universal lexical encoders in less than a minute!'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**MirrorBERT** 旨在克服上述问题。与标准的微调算法不同，MirrorBERT 通过智能地增强初始数据而不依赖外部知识来进行自我监督。这种方法使
    MirrorBERT 在 *语义相似性问题* 上表现出可比的性能。此外，通过使用其创新的对比学习技术，MirrorBERT 可以在不到一分钟的时间内将像 [BERT](/bert-3d1bf880386a)
    或 [RoBERTa](https://medium.com/towards-data-science/roberta-1ef07226c8d8) 这样的预训练模型转换为通用词汇编码器！'
- en: '[](/roberta-1ef07226c8d8?source=post_page-----511bd592da48--------------------------------)
    [## Large Language Models: RoBERTa — A Robustly Optimized BERT Approach'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 大型语言模型：RoBERTa — 一种鲁棒优化的 BERT 方法](/roberta-1ef07226c8d8?source=post_page-----511bd592da48--------------------------------)'
- en: Learn about key techniques used for BERT optimisation
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解用于 BERT 优化的关键技术
- en: towardsdatascience.com](/roberta-1ef07226c8d8?source=post_page-----511bd592da48--------------------------------)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/roberta-1ef07226c8d8?source=post_page-----511bd592da48--------------------------------)'
- en: With the help of the official [MirrorBERT paper](https://arxiv.org/pdf/2104.08027.pdf),
    we will dive into its crucial details to understand how it works under the hood.
    The obtained knowledge will be universal as the discussed techniques could then
    be used for other NLP models dealing with similarity tasks as well.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 借助官方的 [MirrorBERT 论文](https://arxiv.org/pdf/2104.08027.pdf)，我们将深入了解其关键细节，以理解其内部工作原理。所获得的知识是通用的，因为讨论的技术也可以用于处理相似性任务的其他
    NLP 模型。
- en: Methodology
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法论
- en: To put it simply, MirrorBERT is the same BERT model except for several steps
    introduced in its learning process. Let us discuss each of them.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，MirrorBERT 是与 BERT 模型相同的模型，只不过在其学习过程中引入了几个步骤。让我们逐一讨论这些步骤。
- en: '![](../Images/6246e91c630233298c6072c1d634a84f.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6246e91c630233298c6072c1d634a84f.png)'
- en: MirrorBERT learning process
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: MirrorBERT 学习过程
- en: 1\. Self-duplication
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 自我重复
- en: As the name suggests, MirrorBERT simply duplicates the initial data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名称所示，MirrorBERT 只是简单地重复初始数据。
- en: '![](../Images/2c9a0786ab2891c98633d61f547b057e.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2c9a0786ab2891c98633d61f547b057e.png)'
- en: Self-duplication
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 自我重复
- en: This duplicated data is then used to further construct two different embedding
    representations of the same strings.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这些重复的数据用于进一步构建相同字符串的两种不同嵌入表示。
- en: 2\. Data augmentation
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 数据增强
- en: The authors of the paper propose two intuitive techniques that slightly modify
    dataset texts. According to them, in a vast majority of cases, these text corruptions
    do not change their meaning.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的作者提出了两种直观的技术，这些技术略微修改了数据集文本。根据他们的说法，在绝大多数情况下，这些文本破坏不会改变其含义。
- en: 2.1\. Input augmentation
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1\. 输入增强
- en: Given a pair of strings (xᵢ, x̄ᵢ), the algorithm randomly chooses one of them
    and applies **random span masking** consisting of a random substitution of a substring
    of a fixed length *k* in the text with the [MASK] token.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一对字符串 (xᵢ, x̄ᵢ)，算法随机选择其中一个，并应用 **随机跨度掩码**，即用 [MASK] 令牌随机替换文本中固定长度 *k* 的子字符串。
- en: '![](../Images/63f14a1113cf5967175836dd7075d40d.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63f14a1113cf5967175836dd7075d40d.png)'
- en: Input augmentation through random span masking
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通过随机跨度掩码进行输入增强
- en: 2.2\. Feature augmentation
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2\. 特征增强
- en: Random span masking operates on sentence- / phrase-level. To make a model able
    to work well on word-level tasks as well, another augmentation mechanism is needed
    operating on shorter text fragments. Feature augmentation solves this problem
    by using dropout.
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 随机跨度掩蔽操作在句子/短语级别。为了使模型也能在词级任务中表现良好，还需要另一种机制来处理较短的文本片段。特征增强通过使用 dropout 解决了这个问题。
- en: The **dropout** process refers to turning off a given percentage *p* of neurons
    in a certain network layer. This can be viewed as the equivalent of zeroing corresponding
    neurons in the network.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**Dropout** 过程指的是在某一网络层中关闭一定百分比的 *p* 神经元。这可以视为将网络中对应的神经元置零的等效操作。'
- en: The authors of the paper propose using dropout for data augmentation. When a
    pair of strings (xᵢ, x̄ᵢ) is passed to the network with dropout layers, their
    output representations will be slightly different if, on each forward pass, dropout
    layers always disable different neurons.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的作者建议使用 dropout 进行数据增强。当一对字符串 (xᵢ, x̄ᵢ) 传递到具有 dropout 层的网络中时，如果每次前向传递时 dropout
    层总是禁用不同的神经元，则它们的输出表示会略有不同。
- en: The great aspect of using dropout for feature augmentation is that the dropout
    layers are already included in BERT / RoBERTa architecture meaning no additional
    implementation is needed!
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 dropout 进行特征增强的一个很棒的方面是 dropout 层已经包含在 BERT / RoBERTa 架构中，这意味着无需额外的实现！
- en: While random span masking is applied only to each second object in the dataset,
    the dropout is applied to all of them.
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 虽然随机跨度掩蔽仅应用于数据集中的每第二个对象，但 dropout 是应用于所有对象的。
- en: 3\. Constrastive learning
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 对比学习
- en: '**Contrastive learning** is a machine learning technique consisting of learning
    data representations in a way that similar objects lie close to each other in
    the embedding space while dissimilar are far away from each other.'
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**对比学习** 是一种机器学习技术，旨在学习数据表示，使得相似的对象在嵌入空间中彼此接近，而不相似的对象彼此远离。'
- en: One of the ways of constrastive learning implementation is the usage of a **constrastive
    loss function**. The one chosen for MirrorBERT is ***InfoNCELoss***. Let us understand
    how it works.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对比学习实现的一种方法是使用 **对比损失函数**。MirrorBERT 选择的损失函数是 ***InfoNCELoss***。让我们理解它是如何工作的。
- en: '**InfoNCELoss**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**InfoNCELoss**'
- en: At first sight, the formula for *InfoNCELoss* might look intimidating, so let
    us gradually come to it step by step.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 初看起来，*InfoNCELoss* 的公式可能令人畏惧，所以让我们一步步逐渐理解。
- en: The cosine similarity between two vectors measures how close they align to each
    other taking values in the range from -1 to 1 with greater values indicating higher
    similarity.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 两个向量之间的余弦相似度衡量它们彼此对齐的程度，取值范围从 -1 到 1，值越大表示相似度越高。
- en: '![](../Images/cb4165ec044813fff40e2d051d3d4715.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb4165ec044813fff40e2d051d3d4715.png)'
- en: Cosine similarity between two vectors
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 两个向量之间的余弦相似度
- en: 2\. To better understand the next steps, it is necessary to be aware that *InfoNCELoss*
    uses softmax transformation with the temperature parameter T controlling the smoothness
    of the output softmax distribution. That is why similarities are divided by T.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 为了更好地理解下一步，必须了解 *InfoNCELoss* 使用了 softmax 转换，其中温度参数 T 控制输出 softmax 分布的平滑度。这就是为什么相似度除以
    T。
- en: For more information about softmax temperature, refer to [this article](https://medium.com/towards-data-science/distilbert-11c8810d29fc)
    explaining it in more detail.
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 关于 softmax 温度的更多信息，请参阅 [这篇文章](https://medium.com/towards-data-science/distilbert-11c8810d29fc)
    以了解更详细的解释。
- en: '![](../Images/1041d8194b2bd4dbb761d5e7dd99c559.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1041d8194b2bd4dbb761d5e7dd99c559.png)'
- en: Cosine similarity divided by the temperature
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度除以温度
- en: 3\. As in the standard softmax formula, a prediction (similarity) is then transformed
    to the exponent form.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 与标准 softmax 公式一样，预测（相似度）会被转换为指数形式。
- en: '![](../Images/48220b69b90b9d528b1201a70829643e.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/48220b69b90b9d528b1201a70829643e.png)'
- en: Exponent of cosine similarity
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度的指数
- en: '4\. In the normal softmax formula, the numerator contains an exponent of a
    class probability whereas the denominator is the exponential sum of all distribution
    probabilities. In the case with similarities in *InfoNCELoss*, the formula analogously
    follows the logic:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 在普通的 softmax 公式中，分子包含了类别概率的指数，而分母则是所有分布概率的指数和。在 *InfoNCELoss* 中，类似度的公式也遵循类似的逻辑：
- en: The numerator contains exponential similarity of two slightly modified identical
    strings (xᵢ, x̄ᵢ) which can be thought of as a *positive example*.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分子包含两个稍微修改的相同字符串 (xᵢ, x̄ᵢ) 的指数相似度，可以被视为 *正例*。
- en: The denominator consists of a sum of exponential similarities between xᵢ and
    all other dataset strings xⱼwhich can be seen as a set of *all negative examples*.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分母包括xᵢ与所有其他数据集字符串xⱼ之间的指数相似度之和，这可以看作是*所有负面样本*的集合。
- en: '![](../Images/71f072adfc5040e087c104b86521453c.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/71f072adfc5040e087c104b86521453c.png)'
- en: Softmax formula for cosine similarity. Nᵢ denotes all dataset strings except
    for xᵢ and x̄ᵢ.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度的softmax公式。Nᵢ表示除xᵢ和x̄ᵢ之外的所有数据集字符串。
- en: 5\. In the ideal scenario, we want the similarity between identical strings
    (xᵢ, x̄ᵢ) to be high while the similarity of xᵢ with other strings xⱼ to be low.
    If it is true, then the numerator in the formula above will increase while the
    denominator will decrease making the whole expression larger.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 5. 在理想情况下，我们希望相同字符串（xᵢ，x̄ᵢ）之间的相似度高，而xᵢ与其他字符串xⱼ之间的相似度低。如果这是真的，则上述公式中的分子会增加，而分母会减少，从而使整个表达式增大。
- en: 'Loss functions work inversely: in ideal cases, they take smaller values, and,
    in worse situations, they highly penalise the model. To make the formula above
    compatible with this loss principle, let us add the *negative logarithm* before
    the whole expression.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数的工作方式是相反的：在理想情况下，它们取较小的值，而在较差的情况下，它们会对模型进行严厉惩罚。为了使上述公式与这一损失原则兼容，让我们在整个表达式前添加*负对数*。
- en: '![](../Images/08b63c2c2bb05cd1d427075d0cb09d57.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08b63c2c2bb05cd1d427075d0cb09d57.png)'
- en: Negative log of softmax similarities. This expression can be viewed as a loss
    value for a single string xᵢ.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 负的softmax相似度对数。这个表达式可以看作是单个字符串xᵢ的损失值。
- en: 6\. The expression in the previous step already corresponds to a loss value
    for a single string xᵢ. Since the dataset consists of many strings, we need to
    take all of them into account. For that, let us sum up this expression for all
    the strings.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 6. 上一步的表达式已经对应于单个字符串xᵢ的损失值。由于数据集由多个字符串组成，我们需要考虑所有这些字符串。为此，我们需要对所有字符串求和这个表达式。
- en: '![](../Images/cebef6dbecffc5bbd9148f2d04687326.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cebef6dbecffc5bbd9148f2d04687326.png)'
- en: InfoNCELoss
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: InfoNCELoss
- en: The obtained formula is exactly the *InfoNCELoss*!
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 得到的公式正是*InfoNCELoss*！
- en: InfoNCELoss tries to group similar objects close to each other while pushing
    away the dissimilar ones in the embedding space.
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: InfoNCELoss试图将相似的对象聚集在一起，同时在嵌入空间中推开不相似的对象。
- en: Triplet loss used in [SBERT](https://medium.com/towards-data-science/sbert-deb3d4aef8a4)
    is another example of contrastive learning loss.
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[SBERT](https://medium.com/towards-data-science/sbert-deb3d4aef8a4)中使用的三元组损失是对比学习损失的另一个示例。'
- en: '[](/sbert-deb3d4aef8a4?source=post_page-----511bd592da48--------------------------------)
    [## Large Language Models: SBERT — Sentence-BERT'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/sbert-deb3d4aef8a4?source=post_page-----511bd592da48--------------------------------)
    [## 大型语言模型：SBERT — Sentence-BERT'
- en: Learn how siamese BERT networks accurately transform sentences into embeddings
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解siamese BERT网络如何准确地将句子转换为嵌入
- en: towardsdatascience.com](/sbert-deb3d4aef8a4?source=post_page-----511bd592da48--------------------------------)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/sbert-deb3d4aef8a4?source=post_page-----511bd592da48--------------------------------)
- en: Training resources
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练资源
- en: A surprising fact about MirrorBERT is that it does not require a lot of data
    to be fine-tuned. Furthermore, this data does not have to be external as the whole
    training process is self-supervised.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 关于MirrorBERT的一个令人惊讶的事实是，它不需要大量的数据进行微调。此外，这些数据不需要是外部的，因为整个训练过程是自监督的。
- en: The researchers report that for fine-tuning lexical representations, they use
    only 10k most frequent words in each language. For sentence-level tasks, 10k sentences
    are used.
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 研究人员报告称，为了微调词汇表示，他们仅使用每种语言中最频繁的1万词汇。对于句子级任务，使用1万个句子。
- en: Training details
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练细节
- en: 'The details on the MirrorBERT training are listed below:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: MirrorBERT训练的细节如下：
- en: The temperature is set to *T = 0.04* in sentence-level tasks and to *T = 0.2*
    in word-level tasks.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 温度在句子级任务中设置为*T = 0.04*，在词汇级任务中设置为*T = 0.2*。
- en: In random span masking, *k* is set to 5.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在随机跨度掩蔽中，*k*设置为5。
- en: Dropout is set to *p = 0.1*.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout设置为*p = 0.1*。
- en: AdamW optimizer is used with a learning rate of *2e-5*.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AdamW优化器，学习率为*2e-5*。
- en: The batch size is set to 200 (or 400 with duplicates).
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量大小设置为200（或400个重复样本）。
- en: Lexical models are trained for 2 epochs and sentence-level models are trained
    for a single epoch.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词汇模型训练2个周期，句子级模型训练1个周期。
- en: Instead of the mean pooling of all output token representations, the [CLS] token
    representation is created.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同于对所有输出标记表示进行均值池化，创建了[CLS]标记表示。
- en: Single MirrorBERT training epoch needs only 10–20 seconds.
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 单次MirrorBERT训练周期仅需10–20秒。
- en: Evaluations
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估
- en: 'The authors evaluated metrics on a set of benchmarks by applying mirror fine-tuning.
    The results were reported on three types of tasks: lexical, sentence-level and
    cross-lingual. In each of them, MirrorBERT demonstrated comparable performance
    to other BERT-like fine-tuned models.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 作者通过应用镜像微调在一组基准测试上评估了指标。结果在三种任务类型上进行了报告：词汇级别、句子级别和跨语言。在每种任务中，MirrorBERT 展现了与其他
    BERT 类微调模型相当的性能。
- en: The results also showed that the range between 10k and 20k training examples
    is the most optimal for fine-tuning. The performance of the model gradually decreases
    with more training examples.
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 结果还显示，10k 到 20k 的训练样本范围是微调的最优范围。随着训练样本数量的增加，模型的性能逐渐下降。
- en: Conclusion
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'Mirror fine-tuning literally acts like a magic spell: instead of heavy fine-tuning
    procedures, the mirror framework requires much less time without the use of external
    data being on par with other fine-tuned models like BERT, SBERT or RoBERTa on
    semantic similarity tasks.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 镜像微调实际上就像一个魔法咒语：与繁重的微调程序不同，镜像框架所需的时间要少得多，而且不需要外部数据，其性能与 BERT、SBERT 或 RoBERTa
    等其他微调模型在语义相似性任务上相当。
- en: As a result, MirrorBERT can transform BERT-like pretrained model into universal
    encoders capturing linguistic knowledge with high efficiency.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，MirrorBERT 可以将类似 BERT 的预训练模型转变为通用编码器，以高效捕捉语言知识。
- en: Resources
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源
- en: '[Fast, Effective, and Self-Supervised: Transforming Masked Language Models
    into Universal Lexical and Sentence Encoders](https://arxiv.org/pdf/2104.08027.pdf)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[快速、有效且自监督：将掩蔽语言模型转化为通用词汇和句子编码器](https://arxiv.org/pdf/2104.08027.pdf)'
- en: '*All images unless noted are by the author*'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非特别说明，所有图像均由作者提供*'
