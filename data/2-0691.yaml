- en: Decoupled Frontend — Backend Microservices Architecture for a ChatGPT-Based
    LLM Chatbot
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 面向ChatGPT的LLM聊天机器人解耦前端——后端微服务架构
- en: 原文：[https://towardsdatascience.com/decoupled-frontend-backend-microservices-architecture-for-chatgpt-based-llm-chatbot-61637dc5c7ea](https://towardsdatascience.com/decoupled-frontend-backend-microservices-architecture-for-chatgpt-based-llm-chatbot-61637dc5c7ea)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/decoupled-frontend-backend-microservices-architecture-for-chatgpt-based-llm-chatbot-61637dc5c7ea](https://towardsdatascience.com/decoupled-frontend-backend-microservices-architecture-for-chatgpt-based-llm-chatbot-61637dc5c7ea)
- en: '**A practical guide to building a headless ChatGPT application with Streamlit,
    FastAPI, and the OpenAI API**'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**使用Streamlit、FastAPI和OpenAI API构建无头ChatGPT应用程序的实用指南**'
- en: '[](https://stephen-leo.medium.com/?source=post_page-----61637dc5c7ea--------------------------------)[![Marie
    Stephen Leo](../Images/c5669d884da5ff5c965f98904257d379.png)](https://stephen-leo.medium.com/?source=post_page-----61637dc5c7ea--------------------------------)[](https://towardsdatascience.com/?source=post_page-----61637dc5c7ea--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----61637dc5c7ea--------------------------------)
    [Marie Stephen Leo](https://stephen-leo.medium.com/?source=post_page-----61637dc5c7ea--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://stephen-leo.medium.com/?source=post_page-----61637dc5c7ea--------------------------------)[![Marie
    Stephen Leo](../Images/c5669d884da5ff5c965f98904257d379.png)](https://stephen-leo.medium.com/?source=post_page-----61637dc5c7ea--------------------------------)[](https://towardsdatascience.com/?source=post_page-----61637dc5c7ea--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----61637dc5c7ea--------------------------------)
    [Marie Stephen Leo](https://stephen-leo.medium.com/?source=post_page-----61637dc5c7ea--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----61637dc5c7ea--------------------------------)
    ·8 min read·May 24, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----61637dc5c7ea--------------------------------)
    ·阅读时间8分钟·2023年5月24日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/64f56820c6449617c8f28c080da1275e.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/64f56820c6449617c8f28c080da1275e.png)'
- en: 'Image generated by Author using Midjourney V5.1 using the prompt: “decoupled
    frontend backend software application”'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用Midjourney V5.1生成，提示词：“解耦前端后端软件应用”
- en: In [my previous post](https://medium.com/towards-data-science/anatomy-of-llm-based-chatbot-applications-monolithic-vs-microservice-architectural-patterns-77796216903e),
    I wrote about the differences between monolithic and microservices architecture
    patterns for LLM-based chatbot applications. One of the significant advantages
    of picking the microservices architecture pattern is that it allows the separation
    of frontend code from the data science logic so that a data scientist can focus
    on the data science logic without worrying about the frontend code. In this post,
    I will show you how to build a microservice chatbot application with Streamlit,
    FastAPI, and the OpenAI API. We will decouple the frontend and backend codes from
    each other to easily swap out the frontend for another frontend framework like
    React, Swift, Dash, Gradio, etc.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在[我之前的文章](https://medium.com/towards-data-science/anatomy-of-llm-based-chatbot-applications-monolithic-vs-microservice-architectural-patterns-77796216903e)中，我讨论了基于LLM的聊天机器人应用程序的单体与微服务架构模式之间的差异。选择微服务架构模式的一个显著优势是，它允许前端代码与数据科学逻辑分离，使得数据科学家可以专注于数据科学逻辑，而不必担心前端代码。在这篇文章中，我将向你展示如何使用Streamlit、FastAPI和OpenAI
    API构建微服务聊天机器人应用程序。我们将前端和后端代码解耦，以便可以轻松地将前端替换为其他前端框架，如React、Swift、Dash、Gradio等。
- en: First, create a new conda environment and install the necessary libraries.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，创建一个新的conda环境并安装所需的库。
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Backend: Data Science Logic**'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**后端：数据科学逻辑**'
- en: 'Like my previous blog post, we’ll build the backend using FastAPI. The most
    crucial part of any API is the API contract which defines the input format that
    the API accepts and the output format the API will send back to the client. Defining
    and aligning to a robust API contract allow frontend developers to work independently
    of API developers as long as both parties respect the contract. This is the beauty
    of decoupling the frontend from the backend. FastAPI lets us easily specify and
    validate the API contract using Pydantic models. The API contract for our backend
    is as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 像我之前的博客文章一样，我们将使用FastAPI构建后端。任何API中最关键的部分是API契约，它定义了API接受的输入格式和API将发送回客户端的输出格式。定义并遵循一个健全的API契约可以使前端开发人员独立于API开发人员进行工作，只要双方都尊重契约。这就是将前端与后端解耦的好处。FastAPI允许我们使用Pydantic模型轻松地指定和验证API契约。我们的后端API契约如下：
- en: '![](../Images/9db859fa99931c586aa7bb73965cbf0a.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9db859fa99931c586aa7bb73965cbf0a.png)'
- en: API contract details. Image by Author
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: API 合同细节。图片由作者提供
- en: 'The backend will be responsible for the following tasks:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 后端将负责以下任务：
- en: First, we initialize a new FastAPI app, load the OpenAI API key, and define
    a system prompt that will inform ChatGPT of the role we want it to play. In this
    case, we want ChatGPT to play the role of a comic book assistant, so we prompt
    it as such. Feel free to “engineer” different prompts and see how ChatGPT responds!
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们初始化一个新的 FastAPI 应用，加载 OpenAI API 密钥，并定义一个系统提示，以告知 ChatGPT 我们希望它扮演的角色。在这种情况下，我们希望
    ChatGPT 扮演漫画书助手的角色，因此我们这样提示它。可以随意“设计”不同的提示，并查看 ChatGPT 的回应！
- en: Next, we create two Pydantic models, `Conversation` and `ConversationHistory`,
    to validate the API payload. The `Conversation` model will validate each message
    in the conversation history, while the `ConversationHistory` model is just a list
    of conversations to validate the entire conversation history. The OpenAI ChatGPT
    API can only accept `assistant` or `user` in the `role` parameter, so we specify
    that restriction in the `Conversation` model. If you try sending any other value
    in the `role` parameter, the API will return an error. Validation is one of the
    many benefits of using Pydantic models with FastAPI.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建两个 Pydantic 模型，`Conversation` 和 `ConversationHistory`，用于验证 API 负载。`Conversation`
    模型将验证对话历史记录中的每条消息，而 `ConversationHistory` 模型只是一个对话列表，用于验证整个对话历史记录。OpenAI ChatGPT
    API 只接受 `assistant` 或 `user` 作为 `role` 参数，因此我们在 `Conversation` 模型中指定了这个限制。如果尝试在
    `role` 参数中发送其他值，API 将返回错误。使用 Pydantic 模型与 FastAPI 配合使用的好处之一就是验证。
- en: Next, we reserve the root route for a health check.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们为健康检查保留根路由。
- en: Finally, we define a `/chat` route that accepts a `POST` request. The route
    will receive a `ConversationHistory` payload, which is a list of conversations.
    The route will then convert the payload to a Python dictionary, initialize the
    conversation history with the system prompt and list of messages from the payload,
    generate a response using the OpenAI ChatGPT API, and return the generated response
    and the token usage back to the API caller.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们定义一个 `/chat` 路由，该路由接受一个 `POST` 请求。该路由将接收一个 `ConversationHistory` 负载，这是一系列对话。然后，该路由将负载转换为
    Python 字典，使用系统提示和负载中的消息列表初始化对话历史记录，使用 OpenAI ChatGPT API 生成响应，并将生成的响应和令牌使用情况返回给
    API 调用者。
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: That’s it! We can now run the backend on our local machine using `uvicorn backend:app
    — reload` and test it using the Swagger UI at [http://127.0.0.1:8000/docs.](http://127.0.0.1:8000/docs.)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们现在可以使用 `uvicorn backend:app — reload` 在本地机器上运行后端，并通过 [http://127.0.0.1:8000/docs.](http://127.0.0.1:8000/docs.)
    使用 Swagger UI 进行测试。
- en: '![](../Images/46bbd7111851acebf2ca5fffa50dd1b2.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/46bbd7111851acebf2ca5fffa50dd1b2.png)'
- en: FastAPI docs for the backend. Image by Author
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: FastAPI 后端文档。图片由作者提供
- en: '**Frontend: User Interface**'
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**前端：用户界面**'
- en: We’ll build the frontend headless and completely independent of the backend.
    We only have to respect the API contract used by the backend. Before building
    the frontend user interface, let’s define a few helper functions.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建前端，使其完全独立于后端。我们只需要遵守后端使用的 API 合同。在构建前端用户界面之前，让我们定义一些辅助函数。
- en: '`display_conversation()` will help us display the conversation history using
    the [native streamlit chat elements](https://docs.streamlit.io/library/api-reference/chat).
    We can choose unique avatars for both the user and assistant messages using emojis
    or paths to files.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`display_conversation()` 将帮助我们使用 [原生 streamlit 聊天元素](https://docs.streamlit.io/library/api-reference/chat)
    显示对话历史记录。我们可以使用表情符号或文件路径为用户和助手消息选择独特的头像。'
- en: '`clear_conversation()` will help us clear the conversation history. It will
    also initialize the `conversation_history` session state variable to store the
    conversation history and the `total_cost` session state variable to hold the total
    conversation cost.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`clear_conversation()` 将帮助我们清除对话历史记录。它还将初始化 `conversation_history` 会话状态变量以存储对话历史记录，以及
    `total_cost` 会话状态变量以保存总对话成本。'
- en: '`download_conversation()` will allow us to download the conversation history
    as a CSV file.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`download_conversation()` 将允许我们将对话历史记录下载为 CSV 文件。'
- en: '`calc_cost()`: will help us calculate the cost of the conversation based on
    the number of tokens used. The [OpenAI API charges $0.002 per 1000 output tokens
    and $0.0015 per 1000 input tokens](https://openai.com/pricing#chat), so we’ll
    use that to calculate the conversation cost.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`calc_cost()`: 将帮助我们根据使用的令牌数量计算对话成本。[OpenAI API 对每 1000 个输出令牌收费 $0.002，对每 1000
    个输入令牌收费 $0.0015](https://openai.com/pricing#chat)，所以我们将使用这些费用来计算对话成本。'
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now we have everything we need to build the user interface using Streamlit.
    Let’s create a frontend.py file and import the helper functions we defined above.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们拥有了使用 Streamlit 构建用户界面所需的一切。让我们创建一个 frontend.py 文件并导入我们之前定义的助手函数。
- en: First, we’ll define the URL of our FastAPI backend.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将定义我们 FastAPI 后端的 URL。
- en: '`openai_llm_response()` will append the latest user input to the `conversation_history`
    session state variable using the `user` role. Then, we’ll create the payload in
    the format our backend FastAPI app expects with a `history` field. Finally, we’ll
    send the payload to the backend and append the generated response with the cost
    of the individual API call to the `conversation_history` session state variable.
    We’ll also increment the total cost with the cost of the generated response.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`openai_llm_response()` 将使用 `user` 角色将最新的用户输入附加到 `conversation_history` 会话状态变量中。然后，我们将创建一个符合我们后端
    FastAPI 应用程序期望的格式的有效负载，包含 `history` 字段。最后，我们将有效负载发送到后端，并将生成的响应及单次 API 调用的成本附加到
    `conversation_history` 会话状态变量中。我们还将用生成响应的成本增加总成本。'
- en: '`main()`: is the bulk of the UI design. Below the title, we add buttons for
    clearing and downloading the conversation using the helper functions in utils.py.
    Then we have a chat input field where the user can enter their question. Pressing
    enter will send the text typed in the input field to the backend. Finally, we
    display the cost of the conversation and the conversation history.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`main()`: 是 UI 设计的主要部分。在标题下方，我们使用 utils.py 中的助手函数添加了清除和下载对话的按钮。接着我们有一个聊天输入框，用户可以在其中输入问题。按下回车将把输入框中输入的文本发送到后端。最后，我们展示对话的成本和对话历史。'
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: That’s it! We’ve completed our frontend app. We can now test it using `streamlit
    run frontend.py`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们已完成前端应用程序。现在我们可以使用 `streamlit run frontend.py` 进行测试。
- en: '![](../Images/0c5887d6217d4671781ef04e23b45722.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c5887d6217d4671781ef04e23b45722.png)'
- en: Streamlit App interface. Image by Author
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Streamlit App 界面。图像来源：作者
- en: '**Conclusion**'
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**结论**'
- en: 'Building a chatbot using the OpenAI API following a microservices architecture
    is easy by decoupling the frontend from the backend. Here are some thoughts on
    when to consider a decoupled architecture:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 OpenAI API 构建一个聊天机器人，采用微服务架构通过将前端与后端解耦是很简单的。以下是一些考虑何时采用解耦架构的想法：
- en: Your app is relatively complex or needs to support mid to large-scale traffic.
    The decoupled architecture allows for independent scaling of the frontend and
    backend to handle large-scale traffic.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你的应用相对复杂或需要支持中到大规模的流量。解耦架构允许前端和后端独立扩展，以处理大规模流量。
- en: You have dedicated frontend developer resources to build the UI or need to serve
    external customers requiring a highly polished UI. In this tutorial, we used Streamlit
    to construct a simple user interface, but it can get difficult or even impossible
    to build more complex UIs. It’s best to build customer-facing apps using specialized
    UI frameworks like React, Swift, etc.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你有专门的前端开发资源来构建 UI，或者需要为外部客户提供高度精致的 UI。在本教程中，我们使用了 Streamlit 构建了一个简单的用户界面，但构建更复杂的
    UI 可能会变得困难甚至不可能。最好使用像 React、Swift 等专业 UI 框架来构建面向客户的应用程序。
- en: You want to improve the data science logic independent of the frontend. For
    example, you can update the prompts or add multiple microservices, all orchestrated
    by an API server entry point, without worrying about the frontend code as long
    as you respect the same API contract you’ve aligned with the frontend engineers.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你想独立于前端改进数据科学逻辑。例如，你可以更新提示词或添加多个微服务，所有这些都由 API 服务器入口点进行协调，只要你遵守与前端工程师达成的相同 API
    合同，就无需担心前端代码。
- en: 'However, there may be situations when there are better architectural choices
    than decoupling for your app. Here are some thoughts on when NOT to use decoupled
    architecture:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，可能会有一些情况下，解耦不是你应用的最佳架构选择。以下是一些关于何时不使用解耦架构的想法：
- en: Your app is simple or has low traffic. You can have a monolithic app since scaling
    is not an issue.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你的应用很简单或流量较低。你可以使用单体应用程序，因为扩展不是问题。
- en: You do not have dedicated frontend developer resources to build the UI, or your
    app only serves internal customers who might be more forgiving of a rough UI design.
    This is especially true while building a minimal viable product or prototype.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你没有专门的前端开发资源来构建用户界面，或者你的应用程序仅服务于内部客户，这些客户可能对粗糙的用户界面设计更为宽容。尤其是在构建最小可行产品或原型时，这一点尤为明显。
- en: You’re a unicorn wanting to simultaneously improve the data science logic and
    frontend interface!
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你是一个想要同时提升数据科学逻辑和前端界面的独角兽！
