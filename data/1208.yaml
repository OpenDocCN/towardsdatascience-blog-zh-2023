- en: Boosting Tabular Data Predictions with Large Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用大型语言模型提升表格数据预测
- en: 原文：[https://towardsdatascience.com/boosting-tabular-data-predictions-with-large-language-models-531337f834dc?source=collection_archive---------0-----------------------#2023-04-06](https://towardsdatascience.com/boosting-tabular-data-predictions-with-large-language-models-531337f834dc?source=collection_archive---------0-----------------------#2023-04-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/boosting-tabular-data-predictions-with-large-language-models-531337f834dc?source=collection_archive---------0-----------------------#2023-04-06](https://towardsdatascience.com/boosting-tabular-data-predictions-with-large-language-models-531337f834dc?source=collection_archive---------0-----------------------#2023-04-06)
- en: '![](../Images/a72057e01d27e5e83c1457e4d918103d.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a72057e01d27e5e83c1457e4d918103d.png)'
- en: Image by author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: What happens when you unleash GPT-4 on a tabular Kaggle competition to predict
    home prices?
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 当你将 GPT-4 应用于表格型 Kaggle 竞赛以预测房价时会发生什么？
- en: '[](https://aparnadhinak.medium.com/?source=post_page-----531337f834dc--------------------------------)[![Aparna
    Dhinakaran](../Images/e431ee69563ecb27c86f3428ba53574c.png)](https://aparnadhinak.medium.com/?source=post_page-----531337f834dc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----531337f834dc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----531337f834dc--------------------------------)
    [Aparna Dhinakaran](https://aparnadhinak.medium.com/?source=post_page-----531337f834dc--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://aparnadhinak.medium.com/?source=post_page-----531337f834dc--------------------------------)[![Aparna
    Dhinakaran](../Images/e431ee69563ecb27c86f3428ba53574c.png)](https://aparnadhinak.medium.com/?source=post_page-----531337f834dc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----531337f834dc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----531337f834dc--------------------------------)
    [Aparna Dhinakaran](https://aparnadhinak.medium.com/?source=post_page-----531337f834dc--------------------------------)'
- en: ·
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff32f85889f3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fboosting-tabular-data-predictions-with-large-language-models-531337f834dc&user=Aparna+Dhinakaran&userId=f32f85889f3a&source=post_page-f32f85889f3a----531337f834dc---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----531337f834dc--------------------------------)
    ·9 min read·Apr 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F531337f834dc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fboosting-tabular-data-predictions-with-large-language-models-531337f834dc&user=Aparna+Dhinakaran&userId=f32f85889f3a&source=-----531337f834dc---------------------clap_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff32f85889f3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fboosting-tabular-data-predictions-with-large-language-models-531337f834dc&user=Aparna+Dhinakaran&userId=f32f85889f3a&source=post_page-f32f85889f3a----531337f834dc---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----531337f834dc--------------------------------)
    ·9 min read·2023年4月6日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F531337f834dc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fboosting-tabular-data-predictions-with-large-language-models-531337f834dc&user=Aparna+Dhinakaran&userId=f32f85889f3a&source=-----531337f834dc---------------------clap_footer-----------)'
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F531337f834dc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fboosting-tabular-data-predictions-with-large-language-models-531337f834dc&source=-----531337f834dc---------------------bookmark_footer-----------)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F531337f834dc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fboosting-tabular-data-predictions-with-large-language-models-531337f834dc&source=-----531337f834dc---------------------bookmark_footer-----------)'
- en: '***Follow along with this blog’s*** [***accompanying Colab***](https://colab.research.google.com/gist/PubliusAu/3c0b73ecf5558e4dda2a483693be2b93/arize-wide-llm-kaggle-example-v2-0-1.ipynb)**.**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '***跟随本博客的*** [***配套 Colab***](https://colab.research.google.com/gist/PubliusAu/3c0b73ecf5558e4dda2a483693be2b93/arize-wide-llm-kaggle-example-v2-0-1.ipynb)**.**'
- en: '*This blog is a collaboration with Jason Lopatecki, CEO and Co-Founder of Arize
    AI, and Christopher Brown, CEO and Founder of Decision Patterns*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*本博客与 Arize AI 的 CEO 和联合创始人 Jason Lopatecki 以及 Decision Patterns 的 CEO 和创始人
    Christopher Brown 合作完成*'
- en: Introduction
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: There are two distinct groups in the ML ecosystem. One works with highly organized
    data collected in tables — the tabular-data-focused data scientist. The other
    works on deep learning applications including vision, audio, large language models
    (LLMs), etc. For the purposes of this piece, we call the former the “tabular”
    or “traditional” group and the latter the “LLM” group. Each group uses its own
    techniques and models that have, in large part, developed separately. With the
    recent successes of large language models including OpenAI’s GPT-4 and others,
    we wanted to see if we could use modern LLM results to help make predictions on
    tabular datasets.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习生态系统中，有两个明显不同的群体。一类处理的是高度组织化的表格数据——即以表格数据为重点的数据科学家。另一类则致力于深度学习应用，包括视觉、音频、大型语言模型（LLMs）等。为了本文的目的，我们称前者为“表格”或“传统”群体，后者为“LLM”群体。每个群体都使用自己独特的技术和模型，这些技术和模型在很大程度上是独立发展的。鉴于大型语言模型如OpenAI的GPT-4等最近的成功，我们希望看看是否可以利用现代LLM的结果来帮助对表格数据集进行预测。
- en: In order to demonstrate the efficacy of the approach, we submitted results to
    several blind Kaggle competitions (including the popular “House Prices — Advanced
    Regression Techniques” [competition](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques)).
    The typical Kaggle competition supplies tabular data and is dominated by traditional
    ML approaches. However, we found with little background knowledge, zero data cleaning,
    and zero feature development required by traditional methods, LLMs were able to
    return results with predictive power. LLM predictions were not competitive with
    the leading models produced with lengthy and extensive tabular methods, but were
    strong enough to place well higher than the median score on the leaderboard rankings.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明这种方法的有效性，我们向几个盲测的Kaggle竞赛提交了结果（包括受欢迎的“房价——高级回归技术”[竞赛](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques)）。典型的Kaggle竞赛提供表格数据，并且通常由传统的机器学习方法主导。然而，我们发现，即便在几乎没有背景知识、无需数据清洗和特征开发的情况下，LLMs也能够返回具有预测能力的结果。LLM的预测结果虽然无法与经过长时间和广泛的表格方法得到的领先模型竞争，但足够强大，能够在排行榜上显著高于中位数分数。
- en: We expect this to be the beginning of a number of techniques that use LLMs on
    tabular data and would not be surprised to see their use widen and compete favorably
    to more traditional model development processes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预计这将是多种使用LLM处理表格数据技术的开端，不会感到惊讶如果看到它们的使用范围扩大，并与更传统的模型开发过程进行有利的竞争。
- en: Included in this write up is the first approach we have seen that merges traditional
    tabular datasets and XGBoost models with LLMs using latent structure embeddings,
    allowing the tabular approaches to work off of the numerical “features” produced
    internally by the LLM.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中包括了我们见过的第一个将传统表格数据集和XGBoost模型与LLM使用潜在结构嵌入相结合的方法，使得表格方法能够基于LLM内部生成的数值“特征”进行工作。
- en: To date, we haven’t seen an LLM used this way to date and hope this is the beginning
    of something exciting.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们还没有见过以这种方式使用LLM的案例，希望这将是一个令人兴奋的开端。
- en: Challenges of Applying Deep Learning to Tabular Data
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将深度学习应用于表格数据的挑战
- en: 'The typical machine learning application involves cleaning and training a narrow
    set of data typically collected, held, or acquired by an organization. At a high
    level, the process can be thought of developing a “context” for which only one
    specific type of questions can be asked. When that type of question arises, the
    ML model produces one or more predictions. Further improving models comes from
    three areas: adding more data, improving methods, or acquiring more and different
    features. The last is often the most interesting here, as the data scientist is
    generally always asking herself: “what different data can I get to make my predictions
    better?”'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的机器学习应用涉及清洗和训练通常由组织收集、保存或获取的狭窄数据集。从高层次来看，这个过程可以被认为是开发一个“上下文”，在这个上下文中只能提出特定类型的问题。当这种类型的问题出现时，机器学习模型会生成一个或多个预测。进一步改善模型来自三个方面：增加数据、改进方法或获取更多不同的特征。最后一点通常最为有趣，因为数据科学家通常会问自己：“我能获取哪些不同的数据来改善我的预测？”
- en: Partitioning, boosting, and/or bagging models have been developed for and do
    exceedingly well in this domain. Despite much effort, deep learning has not shown
    to be as effective in this area. Observations show that XGBoost and cousins generalize
    better in production, where deep learning models tend to overfit. A large number
    of teams have tried to improve deep learning on tabular datasets, but these efforts
    have largely lagged behind established, high performing tabular approaches.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 分区、提升和/或袋装模型已经在这个领域得到了开发并表现出色。尽管付出了很多努力，深度学习在这一领域的效果却没有那么显著。观察显示，XGBoost及其类似模型在生产中更具泛化能力，而深度学习模型往往容易过拟合。许多团队尝试改进在表格数据集上的深度学习，但这些努力大多落后于已建立的高性能表格方法。
- en: The Problem of Training with Narrow Data
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用狭窄数据进行训练的问题
- en: A common deep learning approach is to apply a neural network and multilayer
    perceptron (MLP) to a relatively “small” dataset consisting of an organization’s
    data. This approach has been repeatedly shown to require more work (data scientist
    time), resource consumption (training time), and parameter tuning to get similar
    or worse performance than tabular approaches. The failure of deep learning here
    may be a mismatch between the approach and narrow data available to it. Deep Learning
    seems somewhat gated by its ability to learn from narrow data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的深度学习方法是将神经网络和多层感知器（MLP）应用于由组织数据组成的相对“小”数据集。这个方法已多次证明需要更多的工作（数据科学家时间）、资源消耗（训练时间）和参数调优，以获得类似或更差的性能，相比于表格方法。深度学习的失败可能是由于方法与其可用的狭窄数据之间的不匹配。深度学习似乎在从狭窄数据中学习的能力上受到了一定的限制。
- en: '![](../Images/78022d5ae3eff5cd2b3061b3b0dcd1ec.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/78022d5ae3eff5cd2b3061b3b0dcd1ec.png)'
- en: Image by author
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于作者
- en: In the image above, a large parameter neural network model is trained on the
    “small data” of a single company. Training a large model on a relatively small
    dataset causes the model to almost always be over-parameterized. This is because
    the information contained to make decisions is not that large, there are only
    so many “error surfaces” related to the data to optimize performance against.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，一个大型参数神经网络模型在一个单一公司的“小数据”上进行了训练。在相对较小的数据集上训练大型模型会导致模型几乎总是过度参数化。这是因为用于做出决策的信息量不大，与数据相关的“错误面”也有限。
- en: 'Applying a Large Language Model To a Tabular Dataset: Enter Prompt Engineering'
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将大型语言模型应用于表格数据集：进入提示工程
- en: LLMs have come to the fore through two innovations. The first is the transformer
    architecture pioneered by Google and others. The second is the application of
    these architectures to colossal data sets on the order of dozens or hundreds of
    terabytes. A reasonable hypothesis is that these LLMs are able to sidestep the
    “narrow” data problem that beleagures deep learning approaches. By training on
    Internet-scale data, LLMs have built an internal representation for the context
    of many applications. This is a needed step in order to have a model that can
    respond to any number of prompts. A happy and necessary consequence is that the
    LLMs may have developed the context for answering questions related to an organization’s
    prediction problems or those of a Kaggle competition.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）通过两项创新走到了前台。第一项是由谷歌等公司首创的变压器架构。第二项是将这些架构应用于规模达到数十或数百TB的巨大数据集。一个合理的假设是，这些LLMs能够绕过困扰深度学习方法的“狭窄”数据问题。通过在互联网规模的数据上进行训练，LLMs构建了许多应用背景的内部表示。这是为了拥有一个能够回应各种提示的模型所必需的步骤。一个令人愉快且必要的结果是，LLMs可能已经发展出了回答与组织预测问题或Kaggle竞赛相关的问题的背景。
- en: By way of analogy, LLMs have come to understand the context of your problem
    in a similar way that traditional/tabular machine learning has done in its training
    step. Surprisingly, this has been done using a broader source of data and not
    the organization’s specific data. Another way to look at it is that the LLMs has
    trained a model capable of predictions from all the data it acquired elsewhere.
    To the data scientist, this provides access to a diverse dataset and a potential
    treasure trove of information — or they may just provide noise.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 类比而言，LLMs以类似于传统/表格机器学习在其训练步骤中理解问题背景的方式来理解你的问题。令人惊讶的是，这些模型使用的是更广泛的数据来源，而不是组织特定的数据。另一种看法是，LLMs训练了一个能够从其他地方获得的数据进行预测的模型。对于数据科学家来说，这提供了对多样化数据集和潜在信息宝库的访问——或者它们可能只是噪声。
- en: Unlocking the information in LLMs for tabular models encounters two obstacles.
    The first is that LLMs are accessed via prompts and not tabular data (DataFrames).
    The second is that LLMs primarily produce textual output.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 解锁LLM在表格模型中的信息面临两个障碍。第一个是LLM是通过提示而不是表格数据（DataFrames）来访问的。第二个是LLM主要生成文本输出。
- en: 'To overcome the first obstacle, we supply our tabular data through prompts.
    Here the prompt is created on each row of the table. The construction of the prompt
    is pretty simple: a paragraph comprised of sentences, one for each cell in the
    table row, as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服第一个障碍，我们通过提示提供表格数据。这里的提示是在表格的每一行上创建的。提示的构造相当简单：一个由句子组成的段落，每个句子对应表格行中的一个单元格，如下所示：
- en: '![](../Images/edd36272c1e0764e525699b80fc2089c.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/edd36272c1e0764e525699b80fc2089c.png)'
- en: Image by author
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: 'The “row” prompt consists of the following:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: “行”提示由以下内容组成：
- en: '*“The <column name> is <cell value>. The <column name> is <cell value>. …”*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*“<列名> 是 <单元格值>。<列名> 是 <单元格值>。……”*'
- en: 'Two things to note here:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意两点：
- en: It is not necessary to generate prompts for training data, only the data about
    which the prediction needs to be made; and
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不必为训练数据生成提示，只需生成关于需要进行预测的数据的提示；
- en: It is not strictly necessary to ask what prediction will be made of the data.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不必严格要求对数据进行预测。
- en: The second obstacle is that the LLMs produce textual responses. In some instances,
    LLMs can provide predictions. As of this writing, the predictions are not very
    good — likely because the LLMs that are not trained with specific predictions
    in mind. Instead of accessing LLM predictions, we find the flexibility to work
    with the features produced by the LLM preferable. In the parlance of LLMs, the
    features are latent structure embeddings or simply “[embeddings](https://arize.com/blog-course/embeddings-meaning-examples-and-how-to-compute/).”
    These embeddings are accessible through LLM APIs. It is important to note that
    the embedding vectors are typically of values-per-row. Once we extract the embeddings,
    we can run them through a tabular model (XGBoost).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个障碍是LLM生成文本响应。在某些情况下，LLM可以提供预测。目前，预测效果不是很好——可能是因为LLM没有针对特定预测进行训练。与其访问LLM预测，我们更倾向于使用LLM生成的特征。在LLM的术语中，这些特征是潜在结构嵌入或简单的“[嵌入](https://arize.com/blog-course/embeddings-meaning-examples-and-how-to-compute/)”。这些嵌入可以通过LLM
    API访问。需要注意的是，嵌入向量通常是按行值的。一旦我们提取了嵌入，就可以将其通过表格模型（XGBoost）进行处理。
- en: 'The embeddings will be used in two examples: first, to make predictions for
    home prices in a Kaggle data competition ([this blog](https://arize.com/blog-course/applying-large-language-models-to-tabular-data/));
    and second, to measure multivariate drift and anomaly detection using embedding
    drift (an upcoming blog).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入将用于两个示例：首先，是在Kaggle数据竞赛中预测房价（[这个博客](https://arize.com/blog-course/applying-large-language-models-to-tabular-data/)）；其次，是通过嵌入漂移来测量多变量漂移和异常检测（即将发布的博客）。
- en: '![](../Images/3210aeb11f4399874213c75821d6113e.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3210aeb11f4399874213c75821d6113e.png)'
- en: 'Workflow: Table -> Prompt -> LLM -> Embedding -> XGboost -> Prediction (Image
    by author)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流程：表格 -> 提示 -> LLM -> 嵌入 -> XGboost -> 预测（图片来源：作者）
- en: The LLM provides a great simple feature engineering tool available for use on
    any tabular dataset, essentially allowing performance gains with almost no feature
    engineering or parameterization.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: LLM提供了一个极为简单的特征工程工具，可用于任何表格数据集，本质上可以在几乎不进行特征工程或参数化的情况下提升性能。
- en: Towards Harnessing Big Data and Large Language Models
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 朝着利用大数据和大型语言模型的方向前进
- en: In most companies, the data you are training on is small relative to the information
    contained across the internet.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数公司中，你所训练的数据相对于互联网上的信息来说是很小的。
- en: Let’s take an example of home price prediction. Imagine the model you train
    learns from your data that a certain zip-code has higher priced homes. It also
    might learn some interesting relationships with other features, such as that homes
    with pools have higher selling prices. But imagine what you could say about home
    prices in a zip-code by gathering the entire world’s knowledge about that zip
    code and applying it in parallel with your current pricing model?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以房价预测为例。假设你训练的模型从数据中学习到某个邮政编码的房价较高。它还可能学习到一些与其他特征的有趣关系，例如，带游泳池的房子售价更高。但是，试想一下，如果通过收集全球关于该邮政编码的所有知识，并将其与当前定价模型并行应用，你能对该邮政编码的房价说些什么？
- en: '![](../Images/ac6feaec585d82887fb012d76e3c2f11.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac6feaec585d82887fb012d76e3c2f11.png)'
- en: Image by author
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Large language models are trained on an immense amount of data, and through
    that data they learn structure and relationships. Internally, they learn manifolds
    and surfaces in embedding/activation space that relate to concepts and knowledge
    that can be applied to almost anything.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型在大量数据上进行训练，并通过这些数据学习结构和关系。在内部，它们学习嵌入/激活空间中的流形和表面，这些流形和表面与概念和知识相关，可以应用于几乎所有事物。
- en: Results
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: The following approach can be applied to any pandas dataframe with no feature
    engineering. The results in this case was a 0.14 RLMSE, putting us in a respectable
    position in the results with little effort.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 以下方法可以应用于任何 pandas 数据框，无需特征工程。在这种情况下，结果为 0.14 RLMSE，使我们在结果中处于一个值得尊敬的位置，且付出了很少的努力。
- en: How does it work? The data flows the model generating embeddings that represent
    the data in the prompt. The embeddings represent latent structure of the data
    that is flowing through the model. They capture the immense amount of training
    data that is then projected on the specific data we are looking at in the tabular
    data set.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 它是如何工作的？数据流经模型生成嵌入，嵌入表示提示中的数据。这些嵌入表示流经模型的数据的潜在结构。它们捕捉了大量的训练数据，然后将其投影到我们在表格数据集中查看的特定数据上。
- en: '![](../Images/223a557b116d1cf969eeb674658b79d8.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/223a557b116d1cf969eeb674658b79d8.png)'
- en: Image by author
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: The image above shows the embedding space of a Kaggle home price data set. You
    can generate this view with the [Colab](https://colab.research.google.com/gist/PubliusAu/3c0b73ecf5558e4dda2a483693be2b93/arize-wide-llm-kaggle-example-v2-0-1.ipynb)
    that accompanies this blog. Here, the predictions mapped onto the UMAP view of
    the [Kaggle Dataset](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图片展示了 Kaggle 房价数据集的嵌入空间。你可以通过附带的[Colab](https://colab.research.google.com/gist/PubliusAu/3c0b73ecf5558e4dda2a483693be2b93/arize-wide-llm-kaggle-example-v2-0-1.ipynb)
    生成这个视图。在这里，预测被映射到 [Kaggle 数据集](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques)
    的 UMAP 视图上。
- en: '![](../Images/99f33d9aec5a20f54905d72ba8189596.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99f33d9aec5a20f54905d72ba8189596.png)'
- en: Image by author
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: The embeddings of the LLM have immense structure that is represented by the
    UMAP view. One can see some of the information contained by mapping original features
    onto the predictions in the data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 的嵌入具有巨大的结构，这些结构通过 UMAP 视图进行表示。可以通过将原始特征映射到数据中的预测上来查看一些包含的信息。
- en: Final House Price Prediction
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终房价预测
- en: In order to make a prediction, the embedding values flow out of the LLM and
    to the XGBoost model that is trained on the embedding latent structure space.
    The XGBoost model then predicts the price of homes. This is all done with no feature
    engineering.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行预测，嵌入值从 LLM 流出，并传送到在嵌入潜在结构空间上训练的 XGBoost 模型。XGBoost 模型然后预测房价。这一切都无需特征工程。
- en: Why not VAE’s?
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么不使用 VAE？
- en: We started this journey looking at applying Variational Autoencoders (VAEs)
    to tabular data. However, we ultimately found that VAE’s are just trained on too
    little data and are too sensitive to parameters to generate useful value.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始这个过程时考虑将变分自编码器（VAEs）应用于表格数据。然而，我们最终发现 VAE 训练的数据量太少，并且对参数的敏感性太高，无法生成有用的价值。
- en: GPT-4
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT-4
- en: We also tested pasting in the data directly to GPT-4 to see how it would fare
    just making predictions on the dataset directly with no training on the data directly.
    In many cases, the results were impressive. We expect there will be approaches
    to connect tabular data more directly to GPT-4 to help with these predictions
    directly.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还测试了将数据直接粘贴到 GPT-4 中，以查看其在没有直接对数据进行训练的情况下对数据集进行预测的效果。在许多情况下，结果令人印象深刻。我们预计会有方法将表格数据更直接地连接到
    GPT-4，以帮助直接进行这些预测。
- en: '*“The following is a set of row and column data:*'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*“以下是一组行和列数据：*'
- en: '*Id MSSubClass MSZoning …*'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*Id MSSubClass MSZoning …*'
- en: '*1 60 RL …”*'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*1 60 RL …”*'
- en: Once the rows and columns of data are pasted into the context you can ask questions
    about the data.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据的行和列被粘贴到上下文中，你可以对数据进行提问。
- en: '![](../Images/6c328b65cfb81b5b2b70f8abd5faab09.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c328b65cfb81b5b2b70f8abd5faab09.png)'
- en: Example of Lookup (image by author)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 查找示例（作者提供的图片）
- en: It definitely understands the data, the following is a lookup on the exact ID
    in the dataset.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 它确实理解数据，以下是数据集中确切 ID 的查找。
- en: '![](../Images/544e57857c968626290db4e4e52ea4d9.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/544e57857c968626290db4e4e52ea4d9.png)'
- en: Pre-Prompt (image by author)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 预提示（作者提供的图片）
- en: The above example is a pre-prompt prior to pasting in the data to predict.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例是在粘贴数据以进行预测之前的预提示。
- en: '![](../Images/56d0ea5a6d10ccea1ea00d812a6adeac.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56d0ea5a6d10ccea1ea00d812a6adeac.png)'
- en: GPT-4 Prediction (image by author)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4预测（作者提供的图片）
- en: The GPT-4 prediction above is a decent prediction without a model or training
    data. The actual value of the sales price is $130,250\. It’s clear there will
    eventually be methods to connect data with GPT-4 and get fairly decent estimates
    without any training or in combination with training for state of the art (SOTA).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 上述GPT-4预测在没有模型或训练数据的情况下已经相当不错。实际销售价格为130,250美元。显然，未来将会有方法将数据与GPT-4连接，并在没有任何训练或结合最新技术（SOTA）的情况下获得相当不错的估计。
- en: '**A Look Ahead**'
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**展望未来**'
- en: Given how little effort went into optimizing these outcomes, it is reasonable
    to expect [LLMs to be used on tabular data](https://arize.com/blog-course/applying-large-language-models-to-tabular-data/)
    in an increasingly large set of environments. It is also likely that LLMs will
    outperform traditional techniques, on small data sets, at some point in the near
    future. As LLMs and [prompt engineering](https://arize.com/blog-course/prompt-engineering/)
    evolve many areas of data science, tabular data problems are not immune.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于优化这些结果所投入的努力非常少，因此可以合理预期，[大型语言模型（LLMs）将在越来越多的环境中用于表格数据](https://arize.com/blog-course/applying-large-language-models-to-tabular-data/)。LLMs很可能会在某个不久的将来，在小数据集上优于传统技术。随着LLMs和[提示工程](https://arize.com/blog-course/prompt-engineering/)的不断发展，数据科学的许多领域，包括表格数据问题，也不可避免地受到影响。
