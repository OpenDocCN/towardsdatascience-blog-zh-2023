- en: The Curse of Dimensionality, Demystified
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《维度灾难揭秘》
- en: 原文：[https://towardsdatascience.com/the-curse-of-dimensionality-demystified-2fc9b0bb1126](https://towardsdatascience.com/the-curse-of-dimensionality-demystified-2fc9b0bb1126)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-curse-of-dimensionality-demystified-2fc9b0bb1126](https://towardsdatascience.com/the-curse-of-dimensionality-demystified-2fc9b0bb1126)
- en: Understanding the mathematical intuition behind the curse of dimensionality
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解维度灾难背后的数学直觉
- en: '[](https://reza-bagheri79.medium.com/?source=post_page-----2fc9b0bb1126--------------------------------)[![Reza
    Bagheri](../Images/7c5a7dc9e6e31048ce31c8d49055987c.png)](https://reza-bagheri79.medium.com/?source=post_page-----2fc9b0bb1126--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2fc9b0bb1126--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2fc9b0bb1126--------------------------------)
    [Reza Bagheri](https://reza-bagheri79.medium.com/?source=post_page-----2fc9b0bb1126--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://reza-bagheri79.medium.com/?source=post_page-----2fc9b0bb1126--------------------------------)[![Reza
    Bagheri](../Images/7c5a7dc9e6e31048ce31c8d49055987c.png)](https://reza-bagheri79.medium.com/?source=post_page-----2fc9b0bb1126--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2fc9b0bb1126--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2fc9b0bb1126--------------------------------)
    [Reza Bagheri](https://reza-bagheri79.medium.com/?source=post_page-----2fc9b0bb1126--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2fc9b0bb1126--------------------------------)
    ·23 min read·Oct 6, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2fc9b0bb1126--------------------------------)
    ·23分钟阅读·2023年10月6日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/61dacdeae7a371398548e2bad19472fe.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61dacdeae7a371398548e2bad19472fe.png)'
- en: 'Image source: [https://pixabay.com/illustrations/ancient-art-background-cosmos-dark-764930/](https://pixabay.com/illustrations/ancient-art-background-cosmos-dark-764930/)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '图片来源: [https://pixabay.com/illustrations/ancient-art-background-cosmos-dark-764930/](https://pixabay.com/illustrations/ancient-art-background-cosmos-dark-764930/)'
- en: The *curse of dimensionality* refers to the problems that arise when analyzing
    high-dimensional data. The *dimensionality* or *dimension* of a dataset refers
    to the number of linearly independent features in that dataset, so a *high-dimensional*
    dataset is a dataset with a large number of features. This term was first coined
    by Bellman in 1961 when he observed that the number of samples required to estimate
    an arbitrary function with a certain accuracy grows exponentially with respect
    to the number of parameters that the function takes.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*维度灾难* 指的是在分析高维数据时出现的问题。数据集的*维度*或*维数*指的是数据集中线性独立特征的数量，因此*高维*数据集就是特征数量众多的数据集。这个术语首次由贝尔曼在1961年提出，他观察到，为了以一定的准确性估计一个任意函数，所需的样本数量会随着函数所取参数数量的增加而呈指数增长。'
- en: In this article, we take a detailed look at the mathematical problems that arise
    when analyzing a high-dimensional set. Though these problems may look counterintuitive,
    it is possible to erxpalin them intuitively. Instead of a purely theoretical discussion,
    we use Python to create and analyze high-dimensional datasets and see how the
    curse of dimensionality manifests itself in practice. *In this article, all images,
    unless otherwise noted, are by the author.*
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们详细探讨了在分析高维数据集时出现的数学问题。尽管这些问题可能看起来违反直觉，但可以用直观的方式来解释它们。我们不进行纯理论讨论，而是使用
    Python 创建并分析高维数据集，观察维度灾难在实践中的表现。*在这篇文章中，所有图片，除非另有说明，均由作者提供。*
- en: '**Dimension of a dataset**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据集的维度**'
- en: As mentioned before, the dimension of a dataset is defined as the number of
    linearly independent features that it has. A linearly independent feature cannot
    be written as a linear combination of the features in that dataset. Hence, if
    a feature or column in a dataset is a linear combination of some other features,
    it won’t add to the dimension of that dataset. For example, Figure 1 shows two
    datasets. The first one has two linearly independent columns and its dimension
    is 2\. In the second dataset, one column is a multiple of another, hence we only
    have one independent feature. As the plot of this dataset shows, despite having
    two features, all the data points are along a 1-dimensional line. Hence the dimension
    of this dataset is one.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，数据集的维度定义为其具有的线性独立特征的数量。一个线性独立的特征不能表示为数据集中其他特征的线性组合。因此，如果数据集中的一个特征或列是其他特征的线性组合，它不会增加数据集的维度。例如，图1显示了两个数据集。第一个数据集有两个线性独立的列，因此其维度为2。在第二个数据集中，一列是另一列的倍数，因此我们只有一个独立特征。正如该数据集的图示所示，尽管有两个特征，但所有数据点都在一条1维的线上。因此，这个数据集的维度是1。
- en: '![](../Images/5ee523475faa332e048de327c6fe310b.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5ee523475faa332e048de327c6fe310b.png)'
- en: Figure 1
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图1
- en: '**The effect of dimensionality on volume**'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**维度对体积的影响**'
- en: The main reason for the curse of dimensionality is the effect of the dimension
    on volume. Here, we focus on the geometrical interpretation of a dataset. Generally,
    we can assume that a dataset is a random sample drawn from a population. For example,
    assume that our population is the set of points on a square shown in Figure 2\.
    This square is the set of all points (*x*₁, *x*₂) such that 0≤*x*₁≤1 and 0≤*x*₂≤1,
    and the side length of this square is one.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 维度诅咒的主要原因是维度对体积的影响。在这里，我们关注数据集的几何解释。通常，我们可以假设数据集是从一个群体中抽取的随机样本。例如，假设我们的群体是图2所示的正方形上的点集合。这个正方形是所有点(*x*₁,
    *x*₂)的集合，使得0≤*x*₁≤1和0≤*x*₂≤1，且这个正方形的边长为1。
- en: Each population has its own distribution, and here we assume that the points
    on this square are uniformly distributed. It means that if we try to randomly
    select a point from this population, all the points have an equal chance of being
    selected. Now, if we draw a random sample of size *n* from this population (which
    means randomly selecting *n* points from this square), these points form a dataset
    with *n* rows and two features. So, the dimension of this dataset is 2.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 每个群体都有其自己的分布，这里我们假设这个正方形上的点是均匀分布的。这意味着如果我们试图从这个群体中随机选择一个点，所有的点都有相等的被选中机会。现在，如果我们从这个群体中抽取大小为*n*的随机样本（即从这个正方形中随机选择*n*个点），这些点形成了一个具有*n*行和两个特征的数据集。因此，这个数据集的维度是2。
- en: '![](../Images/05563aa0917fa4f78e1b857a610de8ca.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05563aa0917fa4f78e1b857a610de8ca.png)'
- en: Figure 2
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图2
- en: Similarly, we can create a dataset with a dimension of 3 by drawing a random
    sample from a cube of edge length 1 (Figure 2). This cube is the set of all points
    (*x*₁, *x*₂, *x*₃) such that 0≤*x*₁≤1, 0≤*x*₂≤1, and 0≤*x*₃≤1, and the points
    on this cube are uniformly distributed.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们可以通过从一个边长为1的立方体中抽取随机样本来创建一个维度为3的数据集（图2）。这个立方体是所有点(*x*₁, *x*₂, *x*₃)的集合，使得0≤*x*₁≤1,
    0≤*x*₂≤1, 0≤*x*₃≤1，并且这个立方体上的点是均匀分布的。
- en: Finally, we can extend this idea and create a *d*-dimensional dataset by drawing
    a random sample of size *n* from a *d*-dimensional hypercube with an edge length
    of 1\. The hypercube is defined as the set of all points (*x*₁, *x*₂,…, *x*_*d*)
    such that 0≤*xᵢ*≤1 (for *i*=1…*d*). Again, we can assume that the points in this
    hypercube are uniformly distributed. The resulting dataset has *n* examples (observations)
    and *d* features.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们可以扩展这个想法，通过从边长为1的*d*维超立方体中随机抽取大小为*n*的样本来创建一个*d*维数据集。超立方体定义为所有点(*x*₁, *x*₂,…,
    *x*_*d*)的集合，使得0≤*xᵢ*≤1（对于 *i*=1…*d*）。同样，我们可以假设这些点在这个超立方体中是均匀分布的。生成的数据集包含*n*个示例（观测值）和*d*个特征。
- en: The volume of a *d*-dimensional hypercube with an edge length of *L* is *L^d*.
    (please note that if *d*=2, this formula gives the area of a square with a side
    length of *L*. However, here we assume that area is a special case of volume for
    a 2-dimensional object). Hence, if *L*=1, the volume is 1 no matter what the value
    of *d* is. But what is important is not the total volume of the hypercube, but
    how volume is distributed inside the hypercube. Here we use an example to explain
    it.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 边长为*L*的*d*维超立方体的体积是*L^d*。（请注意，如果*d*=2，该公式给出的就是边长为*L*的正方形的面积。然而，这里我们假设面积是二维对象的体积的特例）。因此，如果*L*=1，不管*d*的值是什么，体积都是1。但重要的不是超立方体的总体积，而是体积在超立方体内部的分布。这里我们用一个例子来解释它。
- en: We can divide a unit square into 10 shells as shown in Figure 3\. The first
    shell is indeed a small square at the center of the unit square and its edge length
    is 0.1\. The bottom-right corner of this square is located at *x*₁=0.55\. The
    bottom-right corner of the second shell is at *x*₁=0.6, so its thickness is 0.05\.
    All the remaining shells have the same thickness and the bottom-right corner of
    the outermost shell is at *x*₁=1\. Hence, it covers the sides of the unit square.
    Figure 3 shows one of these shells as an example whose bottom-right corner is
    at *x*₁=0.8.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将单位正方形分成10个壳体，如图3所示。第一个壳体实际上是单位正方形中心的小正方形，边长为0.1。这个正方形的右下角位于*x*₁=0.55。第二个壳体的右下角位于*x*₁=0.6，因此其厚度为0.05。所有其余的壳体都有相同的厚度，最外层壳体的右下角位于*x*₁=1。因此，它覆盖了单位正方形的边缘。图3展示了其中一个壳体作为示例，其右下角位于*x*₁=0.8。
- en: '![](../Images/bf90bcb98ad1d97a3bb80728ec67ab50.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bf90bcb98ad1d97a3bb80728ec67ab50.png)'
- en: Figure 3
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图3
- en: 'We can easily calculate the volume (area) of a shell whose bottom-right corner
    is located at *x*₁=*c*:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以轻松计算出底部右角位于*x*₁=*c*的壳体的体积（面积）：
- en: '![](../Images/2e38b6a7fbd1dc6d9cc9aedbba675e2b.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e38b6a7fbd1dc6d9cc9aedbba675e2b.png)'
- en: Now we can extend this procedure to a *d*-dimensional hypercube. We divide the
    hypercube into 10 hypercubic shells with the same thickness.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将此过程扩展到*d*维超立方体。我们将超立方体分成10个厚度相同的超立方壳体。
- en: 'The volume of a shell with one of its corners at *x*₁=*c* is determined by
    the following formula:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 角落之一位于*x*₁=*c*的壳体的体积由以下公式确定：
- en: '![](../Images/85338605d33150377a6ddacee9ee810a.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/85338605d33150377a6ddacee9ee810a.png)'
- en: Listing 1 calculates the volume of all these shells for different values of
    *d* andcreates a bar plot of the volumes. The result is shown in Figure 4.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 列表1计算了不同*d*值下所有这些壳体的体积，并创建了体积的柱状图。结果如图4所示。
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/3fa0c41a2a21eca5abb97d9c10d914c3.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3fa0c41a2a21eca5abb97d9c10d914c3.png)'
- en: Figure 4
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图4
- en: As you see as the number of dimensions increases, the share of the total volume
    for the inner shells decreases quickly. In fact, for *d≥*50, the outermost shell
    (whose corner is located at *x*₁=1) has more than 99% of the total volume of the
    unit hypercube. Hence, we conclude that in a high-dimensional hypercube, almost
    all of the volume is near the faces of the hypercube not inside it.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，随着维度的增加，内层壳体的总体积份额迅速减少。事实上，对于*d≥*50，最外层壳体（其角落位于*x*₁=1）占据了单位超立方体总量的99%以上。因此，我们得出结论，在高维超立方体中，几乎所有的体积都靠近超立方体的面，而不在其内部。
- en: 'Now let’s calculate the probability of finding a data point in each shell.
    Remember that we assumed that the points in this hypercube are uniformly distributed.
    The probability that a random data point is within a certain shell can be calculated
    by integrating the probability density function (PDF) of the uniform distribution
    over the volume of that shell. The PDF of a continuous uniform distribution over
    a unit hypercube is:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算在每个壳层中找到一个数据点的概率。记住我们假设这些超立方体中的点是均匀分布的。随机数据点在某个壳层内的概率可以通过对该壳层体积上的均匀分布概率密度函数（PDF）进行积分来计算。单位超立方体上的连续均匀分布的PDF是：
- en: '![](../Images/ed3843d964f94569d49e7110338837c3.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed3843d964f94569d49e7110338837c3.png)'
- en: 'So the probability that a random data point lies within one of the shells (denoted
    by *S*) is simply equal to the volume of that shell:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，随机数据点位于其中一个壳体（记作*S*）内的概率等于该壳体的体积：
- en: '![](../Images/a599abf83d92ad3e67cd565356e13064.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a599abf83d92ad3e67cd565356e13064.png)'
- en: Hence, based on the results of Figure 4, if we randomly sample a data point
    from the high-dimensional hypercube, there is a great chance that it will be in
    the outer shells. For *d*≥50, the probability of this random data point being
    in the inner shells is almost zero.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，根据图 4 的结果，如果我们从高维超立方体中随机抽取一个数据点，它很可能会位于外层壳。对于 *d*≥50，这个随机数据点位于内层壳的概率几乎为零。
- en: We can also confirm these results using Python. Listing 2 takes a sample of
    size 5000 from a uniform disitrubtion defined over a *d*-dimensional unit hypercube.
    Then it calculates the number of data points that belong to each shell and creates
    a bar plot of them. It uses the same values of *d* used in Listing 1.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 Python 确认这些结果。列表 2 从定义在 *d* 维单位超立方体上的均匀分布中抽取了 5000 个样本。然后，它计算了每个壳层中的数据点数量，并创建了它们的条形图。它使用了与列表
    1 中相同的 *d* 值。
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/63ceec4bd4438b1b3e971e56f52afcb8.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63ceec4bd4438b1b3e971e56f52afcb8.png)'
- en: Figure 5
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5
- en: The plots are shown in Figure 5, and as you see the shape of the bar plots is
    similar to that of Figure 4\. The number of data points in each shell is proportional
    to the corresponding probability of that shell given in Equation 1, so they are
    proportional to the volume of that shell. In fact, when *d*=100, almost all of
    the data points are contained within the outermost shell, and this is a manifestation
    of the curse of dimensionality.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5 中显示了图形，如你所见，条形图的形状类似于图 4。每个壳层中的数据点数量与方程 1 中给出的壳层的相应概率成比例，因此它们与该壳层的体积成比例。实际上，当
    *d*=100 时，几乎所有的数据点都包含在最外层壳中，这体现了维度的诅咒。
- en: In a high-dimensional hypercube, almost all the observations of a random sample
    are near the faces (Figure 6). If the hypercube is *d*-dimensional, these observations
    can also represent a dataset with *d* features. Such a dataset is clearly not
    representative of the population that it was sampled from. Here the population
    is the set of all the data points in the hypercube, but our sample only contains
    the data points near the faces of the hypercube. In practice, we have no data
    points from the regions which are closer to the center of the hypercube. Suppose
    that we use such a sample as a training data set for a machine-learning model.
    The model never sees a data point that is near the center of the hypercube during
    training, so its prediction for such a data point won’t be reliable. In fact,
    the model cannot generalize its prediction to such data points.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在高维超立方体中，几乎所有随机样本的观察点都接近面（图 6）。如果超立方体是 *d* 维的，这些观察点也可以代表一个具有 *d* 特征的数据集。这样一个数据集显然不能代表其抽样的总体。这里的总体是超立方体中所有数据点的集合，但我们的样本仅包含接近超立方体面上的数据点。在实际中，我们没有来自超立方体中心附近区域的数据点。假设我们使用这样的样本作为机器学习模型的训练数据集。模型在训练过程中从未见过接近超立方体中心的数据点，因此对这样的数据点的预测不会可靠。实际上，模型无法将其预测推广到这样的数据点。
- en: '![](../Images/c00cf2a85f58f40a1a3ab72d50dc1350.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c00cf2a85f58f40a1a3ab72d50dc1350.png)'
- en: Figure 6
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6
- en: 'We can also find a different interpretation for these results. Suppose that
    we have uniform disitrubtion defined on the interval [0,1]. This is like a 1-dimensional
    space. If we take a random sample of size 1 from it and denote it by *X*₁, then
    we have:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以找到这些结果的不同解释。假设我们在区间 [0,1] 上定义了均匀分布。这就像一个 1 维空间。如果我们从中抽取一个大小为 1 的随机样本，并将其记作
    *X*₁，那么我们有：
- en: '![](../Images/6ef35f6c61c1832d7a8023c610c50195.png)![](../Images/f2ec1505d10c4ded47eb121f6c203a51.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6ef35f6c61c1832d7a8023c610c50195.png)![](../Images/f2ec1505d10c4ded47eb121f6c203a51.png)'
- en: 'So, the probability that the *X*₁ is within 0.05 units of the interval’s edges
    is just 10%. Now suppose that we have a sample of size 1 from a 100-dimensional
    uniform distribution and denote it by *X*₁, *X*₂, … *X*₁₀₀. This is an IID (independent
    and identically distributed) sample which means that all *Xᵢ*s are independent
    and each *Xᵢ* has uniform disitrubtion on the interval [0,1]. So, we can write:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，*X*₁ 在区间边缘 0.05 个单位内的概率仅为 10%。现在假设我们有一个来自 100 维均匀分布的样本，记作 *X*₁, *X*₂, … *X*₁₀₀。这是一个
    IID（独立同分布）样本，这意味着所有的 *Xᵢ* 相互独立且每个 *Xᵢ* 在区间 [0,1] 上具有均匀分布。因此，我们可以写出：
- en: '![](../Images/38c52f31c75b947cbb2327b24ba7cec6.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38c52f31c75b947cbb2327b24ba7cec6.png)'
- en: 'And we get:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到：
- en: '![](../Images/e1a7aadbab1765c60d6168d70dd8a02f.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1a7aadbab1765c60d6168d70dd8a02f.png)'
- en: As a result, the probability that at least one *Xᵢ* is within 0.05 units of
    one of the hypercube’s faces is almost 1 (and this means that the observation
    with that *Xᵢ* is within 0.05 units of that face). On average, in a sample of
    size 37594, we can find only one data point that is not in this region (1 / 2.66e-5
    ≈ 37594). This means that we need a very large sample to get just a few data points
    that are not near the faces of the hypercube.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，至少有一个 *Xᵢ* 位于超立方体的某个面 0.05 单位以内的概率几乎为 1（这意味着该 *Xᵢ* 的观察值在该面 0.05 单位以内）。平均而言，在一个大小为
    37594 的样本中，我们只能找到一个不在这个区域的数据点（1 / 2.66e-5 ≈ 37594）。这意味着我们需要一个非常大的样本才能得到仅有的几个不在超立方体面附近的数据点。
- en: As the number of dimensions (or features) in a dataset increases, the amount
    of observations needed to generalize the machine learning model accurately increases
    exponentially. The curse of dimensionality makes the training data set sparse,
    and generalizing the model’s predictions becomes more difficult. Hence, we need
    much more training data (more observations) to generalize the model. In reality,
    preparing such a big training data set may be impractical.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据集中维度（或特征）的增加，为了准确概括机器学习模型所需的观察数量呈指数增长。维度诅咒使得训练数据集变得稀疏，模型预测的概括变得更加困难。因此，我们需要更多的训练数据（更多的观察）来概括模型。实际上，准备如此庞大的训练数据集可能不切实际。
- en: Figure 7 shows an analogy for the effect of the curse dimensionality on random
    sampling. Initially, we have a 2-dimensional shooting target and a novice shooter
    who randomly shoots at it. He has a chance of hitting the innermost ring. At the
    bottom, we have the same target which now has the curse of dimensionality. Almost
    all the targeting area belongs to the outermost ring, so the chance of hitting
    the innermost ring is almost zero.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7 显示了维度诅咒对随机采样影响的类比。最初，我们有一个二维射击目标和一个新手射手，他随机射击。射中最内圈的机会很高。在底部，我们有相同的目标，但现在具有维度诅咒。几乎所有的目标区域都属于最外圈，因此射中最内圈的机会几乎为零。
- en: '![](../Images/df39c780e35649c85a39993feba96b6e.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df39c780e35649c85a39993feba96b6e.png)'
- en: Figure 7
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7
- en: The effect of the curse of dimensionality on random sampling is not limited
    to a uniform distribution. In fact, it happens for any high-dimensional dataset
    regardless of its probability disitrubtion. Let’s see what happens if the features
    in a dataset have a normal disitrubtion. Let’s assume that all the features in
    our dataset are independent and have a standard normal disitrubtion. As a result,
    if we combine these features in a vector, that vector has a standard multivariate
    normal distribution. Figure 8 shows the PDF of a 2-dimensional standard multivariate
    normal distribution.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 维度诅咒对随机采样的影响不仅限于均匀分布。实际上，无论其概率分布如何，这种现象都会发生在任何高维数据集中。让我们看看如果数据集中的特征具有正态分布会发生什么。假设我们数据集中的所有特征都是独立的，并且具有标准正态分布。因此，如果我们将这些特征组合成一个向量，该向量具有标准多元正态分布。图
    8 显示了二维标准多元正态分布的 PDF。
- en: '![](../Images/478ffc7733f0722984516eb7203529ff.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/478ffc7733f0722984516eb7203529ff.png)'
- en: Figure 8
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8
- en: 'The PDF of the standard norma disitrubtion is the product of the PDF of its
    marginal distributions:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 标准正态分布的 PDF 是其边际分布 PDF 的乘积：
- en: '![](../Images/9d48abd9c07a60cae9708477face4697.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d48abd9c07a60cae9708477face4697.png)'
- en: where each *f*(*xᵢ*) is the PDF of a standard normal disitrubtion. Using this
    equation we can plot the PDF of the standard normal disitrubtion at higher dimensions.
    Listing 3 plots the PDF of a *d*-dimensional standard normal disitrubtion for
    3 values of *d* along one of the axes (*xᵢ*). Please note that the PDF has a symmetric
    shape, so the plot is the same for all *xᵢ* and for all lines that pass through
    the origin. Based on these plots, we conclude that the maximum value of the PDF
    is at the origin for any value of *d*, and it drops as we move away from the origin.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 *f*(*xᵢ*) 是标准正态分布的 PDF。使用这个方程，我们可以绘制标准正态分布在更高维度的 PDF。列表 3 绘制了 *d* 维标准正态分布在
    *xᵢ* 轴上的 3 个 *d* 值的 PDF。请注意，PDF 具有对称形状，因此图形对于所有 *xᵢ* 和通过原点的所有线都是相同的。基于这些图，我们得出结论，对于任何
    *d* 值，PDF 的最大值都在原点，且随着远离原点而下降。
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/880a2305cb0d4129894e07de0f5445a9.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/880a2305cb0d4129894e07de0f5445a9.png)'
- en: Figure 9
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9
- en: Listing 4 takes a sample of size 5000 from a *d*-dimensional standard multivariate
    normal distribution. Then it calculates the Euclidian distance of each *d-*dimensional
    data point in this sample from the origin. Finally, it creates a histogram of
    these distances. It uses the same values of *d* used in Listing 1\. The result
    is shown in Figure 10.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4从*d*-维标准多元正态分布中抽取5000个样本。然后，它计算该样本中每个*d*-维数据点到原点的欧几里得距离。最后，它创建这些距离的直方图。它使用了列表1中使用的相同*d*值。结果如图10所示。
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/ad5c3a2ff7fe4da7342959eb0b584888.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad5c3a2ff7fe4da7342959eb0b584888.png)'
- en: Figure 10
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图10
- en: 'Here the number of data points in each bin of the histogram is proportional
    to the probability of finding a data point in that bin. As this figure shows,
    even when *d*=2, the peak of the histogram is not at zero. At the origin, the
    PDF has the maximum value (Figure 8), however, the probability also depends on
    the volume over which we take the integral:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，直方图中每个区间的数据点数量与在该区间找到数据点的概率成正比。正如该图所示，即使在*d*=2时，直方图的峰值也不在零处。在原点，PDF具有最大值（图8），然而，概率也依赖于我们进行积分的体积：
- en: '![](../Images/134ba93e7b7d834cb49e05816a5489f7.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/134ba93e7b7d834cb49e05816a5489f7.png)'
- en: By increasing the dimensionality, the peak of the radial histogram moves further
    from the origin. So the majority of the data points lie in a thin ring, and the
    probability of finding a point anywhere else is almost zero. Please note that
    the maximum value of the PDF is still at the origin at any value of *d* (Figure
    9), however, the probability of obtaining a data point near the origin is almost
    zero. Again the dataset is not representative of the population that it was sampled
    from, and we have no data points from the regions that are very close to the origin
    and have a high PDF value.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 通过增加维度，径向直方图的峰值从原点处移动得更远。因此，大多数数据点都位于一个薄环中，几乎没有其他地方找到数据点的概率。请注意，在任何值的*d*下，PDF的最大值仍然位于原点（图9），然而，在原点附近获得数据点的概率几乎为零。同样，数据集并不代表它所采样的总体，我们没有来自非常接近原点且具有高PDF值的区域的数据点。
- en: It is important to note that the curse of dimensionality has nothing to do with
    the PDF of the probability disitrubtion of the dataset. For example, a uniform
    disitrubtion has the same value over its support no matter how many dimensions
    we have. However, since the probability depends on both the PDF and volume, it
    will be affected by the number of dimensions.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，维度的诅咒与数据集的概率分布的PDF无关。例如，均匀分布在其支持上具有相同的值，无论我们有多少维度。然而，由于概率依赖于PDF和体积，它会受到维度数量的影响。
- en: '**The effect of dimensionality on distance functions**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**维度对距离函数的影响**'
- en: 'The dimensionality also has an important effect on distance. First, let’s see
    what we mean by distance. Let ***x*** be a *d-*dimensionalvector:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 维度对距离也有重要影响。首先，让我们看看“距离”是什么意思。设***x***为*d*维向量：
- en: '![](../Images/a7c0fb88ed925daec18f7013be5d1ac1.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a7c0fb88ed925daec18f7013be5d1ac1.png)'
- en: 'The *p*-norm of ***x*** is defined as:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '***x***的*p*-范数定义为：'
- en: '![](../Images/fa367bf0a342736b0ba642290ef02323.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa367bf0a342736b0ba642290ef02323.png)'
- en: 'We can calculate the distance between the vectors ***x*** and ***y*** using
    *p*-norms:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用*p*-范数计算向量***x***和***y***之间的距离：
- en: '![](../Images/6bc5209a2f63ae3f1414f5ea102ee41a.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6bc5209a2f63ae3f1414f5ea102ee41a.png)'
- en: 'Please note that there are other types of distance functions, but in this article,
    we only focus on this type. When *p*=2, we get the familiar Euclidian distance:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，还有其他类型的距离函数，但在本文中，我们只关注这种类型。当*p*=2时，我们得到熟悉的欧几里得距离：
- en: '![](../Images/0e1ac604e40df86e07c6c39042d5ddc9.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e1ac604e40df86e07c6c39042d5ddc9.png)'
- en: 'This is the length of the vector ***x***-***y*** which is also denoted by ***||x-y||*
    (**we can use a *p*-norm without a subscript when *p*=2**)**. Now let’s see how
    dimensionality affects the distance function. Suppose that the dimensionality
    of our dataset is *d*. We pick one of the points in a data set as a query point
    and denote this point by the vector ***x***_*q*. We find the nearest and farthest
    data points to the query point and denote them by the vectors ***x***_*n* and
    ***x***_*f* respectively (Figure 11). Then we calculate the distance between the
    query point and the nearest and farthest data points:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这是向量***x***-***y***的长度，也可以表示为***||x-y||* (**我们可以在*p*=2时使用一个没有下标的*p*-范数**)**。现在，让我们深入探讨维度对距离函数的影响。假设我们数据集的维度是*d*。我们选择数据集中的一个点作为查询点，并用向量***x***_*q*表示。我们找出离查询点最近和最远的数据点，并分别用向量***x***_*n*和***x***_*f*表示（见图11）。然后我们计算查询点与最近数据点和最远数据点之间的距离：
- en: '![](../Images/5cef2cd988e2af670a18fce72617ef58.png)![](../Images/7b12743f6e4614180b2102cb2b70c900.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5cef2cd988e2af670a18fce72617ef58.png)![](../Images/7b12743f6e4614180b2102cb2b70c900.png)'
- en: 'Now it can be shown that under certain reasonable assumptions on the data distribution,
    we have:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以证明，在对数据分布进行某些合理假设的情况下，我们有：
- en: '![](../Images/df2941e4dc1b0e0787690736e52d940a.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df2941e4dc1b0e0787690736e52d940a.png)'
- en: Hence, as dimensionality increases, the distance of the query point to the farthest
    data point approaches its distance to the nearest data point. Hence, differentiating
    the nearest data point from the other data points becomes impossible.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，随着维度的增加，查询点到最远数据点的距离逐渐接近其到最近数据点的距离。因此，将最近的数据点与其他数据点区分开来变得不可能。
- en: '![](../Images/96da5563bc5d560c4db5aee2e3c1262c.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96da5563bc5d560c4db5aee2e3c1262c.png)'
- en: Figure 11
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图11
- en: 'Listing 5 shows the effect of dimensionality on distance function. It draws
    a sample of size 500 from a uniform disitrubtion defined over a *d*-dimensional
    unit hypercube. Then it calculates the Euclidian distance between each pair of
    data points and plots a histogram of these distances. It uses the same values
    of *d* used in Listing 1\. Figure 12 shows these histograms. Please note that
    in a *d*-dimensional unit hypercube, the minimum possible Euclidian distance between
    two points is zero and the maximum possible distance is:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5显示了维度对距离函数的影响。它从定义在*d*-维单位超立方体上的均匀分布中抽取了500个样本。然后它计算了每对数据点之间的欧几里得距离，并绘制了这些距离的直方图。它使用了列表1中相同的*d*值。图12显示了这些直方图。请注意，在*d*-维单位超立方体中，两点之间的最小欧几里得距离为零，最大可能距离为：
- en: '![](../Images/344e1ea40b8f87c08552ab2067c9932e.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/344e1ea40b8f87c08552ab2067c9932e.png)'
- en: So, for each value of *d*, the limits of the *x*-axis of the histogram are 0
    and √*n*.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于每个*d*值，直方图的*x*轴范围为0到√*n*。
- en: '[PRE4]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/2d1660125479e744b2eedce2a29c07a9.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d1660125479e744b2eedce2a29c07a9.png)'
- en: Figure 12
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图12
- en: As the dimensionality (*d*) increases, the histogram becomes sharper, so all
    pairwise distances lie within a narrow range. This indicates that for each data
    point in the dataset, DMAX (the distance to the farthest data point) approaches
    DMIN (the distance to the nearest data point).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 随着维度（*d*）的增加，直方图变得更加尖锐，因此所有成对距离都在一个狭窄范围内。这表明，对于数据集中的每个数据点，DMAX（到最远数据点的距离）接近DMIN（到最近数据点的距离）。
- en: Let me explain the intuition behind this effect. Figure 13 shows a query point
    (***x***_*q*) and its nearest (***x***_*n*) and farthest data points (***x***_*f).*
    In a 2-dimensional space, each of the vectors
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我解释一下这种效应背后的直觉。图13显示了一个查询点（***x***_*q*）及其最近的（***x***_*n*）和最远的数据点（***x***_*f*）。在二维空间中，每个向量
- en: '![](../Images/608de6d4f3354b5df6375af64f4232ef.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/608de6d4f3354b5df6375af64f4232ef.png)'
- en: and
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '![](../Images/2f460e3bec9694dcd188cb4f8d0e1dde.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f460e3bec9694dcd188cb4f8d0e1dde.png)'
- en: have 2 components. The components of ***x***_*n-****x***_*q* are marked in blue
    and those of ***x***_*f-****x***_*q* are marked inred. Similarly, in a *d*-dimensional
    space, these vectors have *d* components.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个分量。***x***_*n-* ***x***_*q* 的分量用蓝色标记，***x***_*f-* ***x***_*q* 的分量用红色标记。同样，在*d*-维空间中，这些向量有*d*个分量。
- en: '![](../Images/a6efa42d16c9d2802e569fa6181ba6a5.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a6efa42d16c9d2802e569fa6181ba6a5.png)'
- en: Figure 13
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图13
- en: In a 2-dimensional space, to go from one point to another we should pass through
    2 dimensions. For example, in Figure 13, to go from ***x***_*q* to ***x***_*f*,
    we should move *l*₁ units to the right and *l*₂ units up. Please note that *l*₁
    and *l*₂ are the components of the vector ***x***_*f-****x***_*q*, and the distance
    between these points is the square root of *l*₁²+*l*₂². In a similar way, in a
    *d-*dimensional space, to go from ***x***_*q* to ***x***_*f*, we should pass through
    *d* dimensions. Now the vector ***x***_*f-****x***_*q* has *d* components. If
    the *i*-th component is denoted by *lᵢ*, then it means that we should move *lᵢ*
    units along the *i*-th dimension (Figure 13).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在二维空间中，从一个点到另一个点，我们应该通过2个维度。例如，在图13中，从***x***_*q*到***x***_*f*，我们应该向右移动*l*₁单位，向上移动*l*₂单位。请注意，*l*₁和*l*₂是向量***x***_*f-****x***_*q*的组件，这些点之间的距离是*l*₁²+*l*₂²的平方根。同样，在*d-*维空间中，从***x***_*q*到***x***_*f*，我们应该通过*d*个维度。现在，向量***x***_*f-****x***_*q*有*d*个组件。如果第*i*个组件用*lᵢ*表示，则意味着我们应该在第*i*个维度上移动*lᵢ*单位（图13）。
- en: Listing 6 creates a sample of size 500 from a uniform disitrubtion defined over
    a *d*-dimensional unit hypercube. It then randomly picks a query data point (***x***_*q*)
    and finds its nearest (***x***_*n*) and farthest data point (***x***_*f*). Next,
    it calculates the components of the vectors ***x***_*n-****x***_*q* and***x***_*f-****x***_*q.*
    Finally, the absolute values of these components are plotted in two bar plots
    for *d*=2 and *d*=500\. The result is shown in Figure 14.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6创建了一个从定义在*d*-维单位超立方体上的均匀分布中生成的大小为500的样本。然后，它随机选择一个查询数据点（***x***_*q*），并找到其最近的（***x***_*n*）和最远的（***x***_*f*）数据点。接下来，它计算向量***x***_*n-****x***_*q*和***x***_*f-****x***_*q*的组件。最后，这些组件的绝对值在*d*=2和*d*=500的两个条形图中绘制。结果显示在图14中。
- en: '[PRE5]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/84c3b318650e84b71e274ac18c15d587.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84c3b318650e84b71e274ac18c15d587.png)'
- en: Figure 14
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图14
- en: The length of the vector ***x***_*n-****x***_*q* gives the distance between
    ***x***_*q* and ***x***_*n*. Similarly, ||***x***_*f-****x***_*q||* givesthe distance
    between ***x***_*q* and ***x***_*f*. These distances are also shown in separate
    bar plots in Figure 14\. As this figure shows, in a 2-dimensional space, the absolute
    value of the components of ***x***_*n-****x***_*q* is much smaller than that of
    ***x***_*f-****x***_*q.* Hence the difference between ||***x***_*f-****x***_*q||*
    and ||***x***_*n-****x***_*q||* is noticable. In a 500-dimensional space, we have
    500 components. As you see, the absolute value of some of the components of ***x***_*n-****x***_*q*
    is even greater than those of***x***_*f-****x***_*q*. Remember that the length
    of a vector is obtained from the sum of the squares of these components. Hence,
    ||***x***_*f-****x***_*q||* and ||***x***_*n-****x***_*q||* are now much closer
    relatively.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 向量***x***_*n-****x***_*q*的长度给出了***x***_*q*和***x***_*n*之间的距离。同样，||***x***_*f-****x***_*q||*给出了***x***_*q*和***x***_*f*之间的距离。这些距离也在图14中的两个条形图中显示。正如图中所示，在二维空间中，向量***x***_*n-****x***_*q*的组件的绝对值远小于***x***_*f-****x***_*q*的组件。因此，||***x***_*f-****x***_*q||*和||***x***_*n-****x***_*q||*之间的差异是显著的。在500维空间中，我们有500个组件。正如你所见，***x***_*n-****x***_*q*的某些组件的绝对值甚至大于***x***_*f-****x***_*q*。请记住，向量的长度是从这些组件的平方和中得到的。因此，||***x***_*f-****x***_*q||*和||***x***_*n-****x***_*q||*现在相对接近。
- en: We know that all the data points are chosen randomly, so the components of each
    data point are random numbers. When the dimensionality is high, to go from one
    point to another we should move through each dimension by a random distance (Figure
    13), and since we have many dimensions, the total distance between each pair of
    points is roughly the same. Hence the distance from the query point to any other
    points is roughly the same. In fact, as the dimensionality increases, the average
    distance between the points increases, but the relative difference between these
    distances decreases (Figure 14).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道所有数据点都是随机选择的，因此每个数据点的组件都是随机数。当维度很高时，从一个点到另一个点，我们应该通过每个维度移动一个随机距离（图13），由于我们有很多维度，每对点之间的总距离大致相同。因此，从查询点到其他任何点的距离大致相同。事实上，随着维度的增加，点之间的平均距离增加，但这些距离之间的相对差异减少（图14）。
- en: Figure 15 gives an analogy for this effect. Assume that the path between two
    places is made up of several segments. In a 1-dimensional space, we only have
    one segment. In a 2-dimensional space, we have 2 segments between each pair of
    points, and in a *d*-dimensional space, we have *d* segments between them. When
    we only have one segment, the path length gives the actual horizontal distance
    between two points. Hence we can easily distinguish the closer point (*n*) from
    a further point (*f*).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15 给出了这一效果的类比。假设两个地方之间的路径由几个段组成。在一维空间中，我们只有一个段。在二维空间中，每对点之间有 2 个段，而在 *d* 维空间中，我们有
    *d* 个段。当只有一个段时，路径长度给出了两个点之间的实际水平距离。因此，我们可以很容易地区分较近的点 (*n*) 和较远的点 (*f*)。
- en: As the number of segments increases, the path length (which is the sum of the
    length of all segments) increases and diverges from the actual distance between
    the points. Now the horizontal distance between the points doesn’t affect the
    path that much. Instead, the vertical movements along the segments determine the
    path length. As the number of segments goes to infinity, the length of the path
    between *q* and *n* approaches the length of the path between *q* and *f*.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 随着段数的增加，路径长度（即所有段长度的总和）增加，并与点之间的实际距离发生偏离。现在点之间的水平距离对路径的影响不大。相反，沿着段的垂直移动决定了路径长度。随着段数趋向于无穷大，*q*
    和 *n* 之间的路径长度接近于 *q* 和 *f* 之间的路径长度。
- en: '![](../Images/32d121eec4e5f41f2343308b03fb58f7.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/32d121eec4e5f41f2343308b03fb58f7.png)'
- en: Figure 15
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15
- en: A path with *d* segments is similar to the distance between two points in a
    *d*-dimensional space, and each segment can represent one of the components of
    the vector that connects those points in that space. As dimensionality increases,
    we have more segments (components), and the relative difference between the distances
    approaches zero.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有 *d* 个段的路径类似于 *d* 维空间中两个点之间的距离，每个段可以表示连接这些点的向量的一个分量。随着维度的增加，我们有更多的段（分量），距离之间的相对差异接近于零。
- en: '**The effect of dimensionality on learning models**'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**维度对学习模型的影响**'
- en: As mentioned in the previous section, in a high-dimensional space, the concept
    of proximity or distance is not meaningful anymore. Hence, any machine learning
    model that relies on the distance functions can break down in a high-dimensional
    space. In other words, when we have so many features in a training dataset, such
    models won’t give a reliable prediction anymore. One example is the *k-*NN (*k*
    Nearest Neighbors) algorithm. In this section, we will see how the curse of dimensionality
    affects the prediction error of a *k-*NN model (the examples are modified versions
    of those in [1]).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所述，在高维空间中，接近度或距离的概念不再有意义。因此，任何依赖距离函数的机器学习模型在高维空间中可能会崩溃。换句话说，当训练数据集中有很多特征时，这些模型将无法给出可靠的预测。一个例子是
    *k-*NN (*k* 最近邻) 算法。在这一节中，我们将探讨维度诅咒如何影响 *k-*NN 模型的预测误差（这些例子是 [1] 中的修改版本）。
- en: 'Our dataset has *d* features and 1000 examples (observations). These examples
    are drawn from a uniform disitrubtion over a unit *d*-dimensional hypercube. Hence
    each example of this dataset can be written as:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集有 *d* 个特征和 1000 个示例（观察）。这些示例来自单位 *d* 维超立方体上的均匀分布。因此，这个数据集的每个示例可以写作：
- en: '![](../Images/f3dafba6893dcc66f093c48a595868f1.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3dafba6893dcc66f093c48a595868f1.png)'
- en: 'The target of this observation is defined as:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这个观察的目标定义为：
- en: '![](../Images/c13516a3158afc739cdba662a0984641.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c13516a3158afc739cdba662a0984641.png)'
- en: So, the target is simply the square of the first feature of each example. We
    use a *k*-NN model to predict the target for a test data point using this training
    dataset. To predict the target of a test data point, the *k*-NN model first finds
    the *k* nearest neighbors of that data point in the training data set (*k* nearest
    data point to the test point). It finds them using a distance function (Euclidian
    distance in this example). The predicted target of the test point is the average
    of the targets of these *k* nearest neighbors. In this example, we use the 3 nearest
    neighbors (*k*=3).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，目标仅仅是每个示例第一个特征的平方。我们使用一个 *k*-NN 模型，通过这个训练数据集预测测试数据点的目标。为了预测测试数据点的目标，*k*-NN
    模型首先在训练数据集中找到该数据点的 *k* 个最近邻（*k* 个离测试点最近的数据点）。它使用距离函数（在这个例子中是欧几里得距离）来找到它们。测试点的预测目标是这
    *k* 个最近邻的目标值的平均值。在这个例子中，我们使用 3 个最近邻（*k*=3）。
- en: 'The test point is at the center of the hypercube:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 测试点位于超立方体的中心：
- en: '![](../Images/e244802b77e6ca577383293ef85ca940.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e244802b77e6ca577383293ef85ca940.png)'
- en: 'We run 1000 simulations. In each simulation, we sample 1000 data points from
    a *d*-dimensional uniform distribution to create examples of the training dataset
    and calculate their target values. Then we train a *k-*NN model (with *k*=3) on
    this training dataset. Finally, we predict the target of the test data point (denoted
    by *y*^_*t*) using this model and compare it with the actual target which is 0.25\.
    Once we have the predicted target for all the simulations, we can calculate the
    bias, variance, and mean squared error (MSE) of these predictions. Bias is defined
    as:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了 1000 次模拟。在每次模拟中，我们从 *d* 维均匀分布中抽取 1000 个数据点以创建训练数据集的示例，并计算它们的目标值。然后，我们在该训练数据集上训练一个
    *k*-NN 模型（*k*=3）。最后，我们使用该模型预测测试数据点的目标（记作 *y*^_*t*），并与实际目标 0.25 比较。一旦我们获得所有模拟的预测目标，就可以计算这些预测的偏差、方差和均方误差（MSE）。偏差定义为：
- en: '![](../Images/8df1889a84fb998f23ed36446e643686.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8df1889a84fb998f23ed36446e643686.png)'
- en: 'It is the difference between the mean of the predicted target (*y*^_*t*) over
    these simulations and the actual target of the test point (*y*_*t*). The variance
    is the variance of the predicted targets over these simulations:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在这些模拟中预测目标（*y*^_*t*）的均值与测试点的实际目标（*y*_*t*）之间的差异。方差是这些模拟中预测目标的方差：
- en: '![](../Images/77872b54f4e7b703d32cf9d23778c52a.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77872b54f4e7b703d32cf9d23778c52a.png)'
- en: 'The mean squared error (MSE) is defined as:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 均方误差（MSE）定义为：
- en: '![](../Images/91d772e6230c670df79e940c7c1f900d.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91d772e6230c670df79e940c7c1f900d.png)'
- en: 'It can be shown that:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明：
- en: '![](../Images/214a61453af31445fe84a52be054218b.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/214a61453af31445fe84a52be054218b.png)'
- en: Listing 7 calculates the bias, variance, and MSE for this example for different
    values of *d*. The results are plotted in Figure 16.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7 计算了该示例在不同 *d* 值下的偏差、方差和 MSE。结果绘制在图 16 中。
- en: '[PRE6]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/93e6298bdbc2451a585fe973ea8ed8fa.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93e6298bdbc2451a585fe973ea8ed8fa.png)'
- en: Figure 16
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16
- en: The *k*-NN model makes the assumption that close data points should also have
    close targets. However, in a high-dimensional dataset, all the data points are
    roughly at the same distance, so the *k* nearest neighbors can be very far from
    the test point. As a result, The average target of these neighbors is not an accurate
    prediction for the actual target of the test point anymore.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-NN 模型假设相近的数据点也应具有相近的目标。然而，在高维数据集中，所有数据点之间的距离大致相同，因此 *k* 个最近邻可能距离测试点非常远。因此，这些邻居的平均目标不再是测试点实际目标的准确预测。'
- en: Listing 8 shows compares the *k*-NN prediction (*y*^_*t*) of all simulations
    for *d*=2 and *d*=10 (Figure 17). It also shows the actual target of the test
    point (*y_t*) with a black dashed line and the mean of all *k*-NN predictions
    with a blue line. Please note that the distance between the black dashed line
    and the blue line gives the bias.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8 显示了 *d*=2 和 *d*=10 的所有模拟中 *k*-NN 预测（*y*^_*t*）的比较（图 17）。它还用黑色虚线显示了测试点的实际目标（*y_t*），用蓝线显示了所有
    *k*-NN 预测的均值。请注意，黑色虚线和蓝线之间的距离表示偏差。
- en: '[PRE7]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/3a309e8f2eacb4eb3fef91b6e3b90e69.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a309e8f2eacb4eb3fef91b6e3b90e69.png)'
- en: Figure 17
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17
- en: As dimensionality increases, the nearest neighbors will be near the faces of
    the hypercube, and very far from the test point at the center of the hypercube.
    The *x*₁ components of these points are not necessarily close to the *x*₁ of the
    test point and can vary a lot, hence the variance of the predictions quickly increases
    with *d.* However, on average they are close to *x*₁ of the test point, so the
    bias remains relatively small (Figure 16). The mean squared error (MSE) is proportional
    to both bias and variance, so it increases with dimensionality.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 随着维度的增加，最近邻会接近超立方体的面，并且距离超立方体中心的测试点非常远。这些点的 *x*₁ 组件不一定接近测试点的 *x*₁，并且可能变化很大，因此预测的方差随着
    *d* 的增加迅速增加。然而，平均来说，它们接近测试点的 *x*₁，所以偏差保持相对较小（图 16）。均方误差（MSE）与偏差和方差成正比，因此它随维度增加而增加。
- en: 'Listing 9 shows another example. Here the setup is similar to that of Listing
    7, however, the target of the dataset is defined as:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9 显示了另一个示例。这里的设置类似于列表 7，但数据集的目标定义为：
- en: '![](../Images/405529ce5ed95c2bad3cb536eab29810.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/405529ce5ed95c2bad3cb536eab29810.png)'
- en: where
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '![](../Images/1f25252327599653e36fe3137615bd24.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f25252327599653e36fe3137615bd24.png)'
- en: 'The test point is again at the center of the hypercube:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 测试点再次位于超立方体的中心：
- en: '![](../Images/f1e6a9a8ea917375d242269142c7c00f.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1e6a9a8ea917375d242269142c7c00f.png)'
- en: The plots of bias, variance, and MSE for this example are shown in Figure 18\.
    Here the bias increases quickly with dimensionality and variance remains relatively
    small.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子的偏差、方差和MSE的图示如图18所示。在这里，偏差随着维度的增加迅速增加，而方差保持相对较小。
- en: '[PRE8]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/19d32c989bdcc44647c446d0778e0e0a.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/19d32c989bdcc44647c446d0778e0e0a.png)'
- en: Figure 18
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图18
- en: Remember that as dimensionality increases the distance between each pair of
    points increases but the relative difference between these distances approaches
    zero. Hence, for all data points including the the nearest neighbors, ||***x***||
    increases and *y* goes to zero. So the average *y^* of the nearest neighbors diverges
    from the target of the test point. The result is an increase in bias and MSE,
    but the variance remains small.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，随着维度的增加，每对点之间的距离增加，但这些距离之间的相对差异接近于零。因此，包括最近邻在内的所有数据点，||***x***|| 增加，而 *y*
    变为零。因此，最近邻的平均 *y^* 与测试点的目标偏离。结果是偏差和MSE增加，但方差保持较小。
- en: In summary, an increase in dimensionality increases MSE. However, its effect
    on bias or variance depends on how the target of the dataset is defined. These
    examples clearly show that *k*-NN is not a reliable model for high-dimensional
    data.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，维度的增加会导致均方误差（MSE）增加。然而，它对偏差或方差的影响取决于数据集目标的定义。这些例子清楚地表明，*k*-最近邻（k-NN）对于高维数据并不是一个可靠的模型。
- en: In this article, we discussed the curse of dimensionality and its effect on
    volume and distance functions. We saw that in a high-dimensional space, almost
    all of the data points will be far from the origin. Hence, if we use a high-dimensional
    dataset to train a model, the model’s prediction for a data point that is close
    to the origin won’t be reliable. In addition, the concept of distance is not meaningful
    anymore. In a high-dimensional dataset, the pairwise distance of all the data
    points is very close to each other. Hence the machine learning models that rely
    on distance functions won’t be able to give an accurate prediction for a high-dimensional
    dataset.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们讨论了维度诅咒及其对体积和距离函数的影响。我们看到，在高维空间中，几乎所有数据点都将远离原点。因此，如果我们使用高维数据集来训练模型，那么对于接近原点的数据点，模型的预测将不可靠。此外，距离的概念也不再有意义。在高维数据集中，所有数据点之间的成对距离非常接近。因此，依赖于距离函数的机器学习模型将无法为高维数据集提供准确的预测。
- en: '**References**'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: '[1] Hastie, T., et al. The Elements of Statistical Learning. Springer, 2009.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Hastie, T., et al. The Elements of Statistical Learning. Springer, 2009.'
