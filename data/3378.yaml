- en: 'Retrieval-Augmented Generation (RAG): From Theory to LangChain Implementation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）：从理论到LangChain实现
- en: 原文：[https://towardsdatascience.com/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2?source=collection_archive---------0-----------------------#2023-11-14](https://towardsdatascience.com/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2?source=collection_archive---------0-----------------------#2023-11-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2?source=collection_archive---------0-----------------------#2023-11-14](https://towardsdatascience.com/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2?source=collection_archive---------0-----------------------#2023-11-14)
- en: From the theory of the original academic paper to its Python implementation
    with OpenAI, Weaviate, and LangChain
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从原始学术论文的理论到其在OpenAI、Weaviate和LangChain中的Python实现
- en: '[](https://medium.com/@iamleonie?source=post_page-----4e9bd5f6a4f2--------------------------------)[![Leonie
    Monigatti](../Images/4044b1685ada53a30160b03dc78f9626.png)](https://medium.com/@iamleonie?source=post_page-----4e9bd5f6a4f2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4e9bd5f6a4f2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4e9bd5f6a4f2--------------------------------)
    [Leonie Monigatti](https://medium.com/@iamleonie?source=post_page-----4e9bd5f6a4f2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@iamleonie?source=post_page-----4e9bd5f6a4f2--------------------------------)[![Leonie
    Monigatti](../Images/4044b1685ada53a30160b03dc78f9626.png)](https://medium.com/@iamleonie?source=post_page-----4e9bd5f6a4f2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4e9bd5f6a4f2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4e9bd5f6a4f2--------------------------------)
    [Leonie Monigatti](https://medium.com/@iamleonie?source=post_page-----4e9bd5f6a4f2--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3a38da70d8dc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fretrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2&user=Leonie+Monigatti&userId=3a38da70d8dc&source=post_page-3a38da70d8dc----4e9bd5f6a4f2---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4e9bd5f6a4f2--------------------------------)
    ·7 min read·Nov 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4e9bd5f6a4f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fretrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2&user=Leonie+Monigatti&userId=3a38da70d8dc&source=-----4e9bd5f6a4f2---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3a38da70d8dc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fretrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2&user=Leonie+Monigatti&userId=3a38da70d8dc&source=post_page-3a38da70d8dc----4e9bd5f6a4f2---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4e9bd5f6a4f2--------------------------------)
    ·7分钟阅读·Nov 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4e9bd5f6a4f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fretrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2&user=Leonie+Monigatti&userId=3a38da70d8dc&source=-----4e9bd5f6a4f2---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e9bd5f6a4f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fretrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2&source=-----4e9bd5f6a4f2---------------------bookmark_footer-----------)![](../Images/ac6c086f3ecee48e8c43eaf4439e8442.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e9bd5f6a4f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fretrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2&source=-----4e9bd5f6a4f2---------------------bookmark_footer-----------)![](../Images/ac6c086f3ecee48e8c43eaf4439e8442.png)'
- en: Retrieval-Augmented Generation Workflow
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成工作流程
- en: 'Since the realization that you can supercharge large language models (LLMs)
    with your proprietary data, there has been some discussion on how to most effectively
    bridge the gap between the LLM’s general knowledge and your proprietary data.
    There has been a lot of debate around [whether fine-tuning or Retrieval-Augmented
    Generation (RAG) is more suited for this](/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application-94654b1eaba7)
    (spoiler alert: it’s both).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 自意识到可以通过专有数据来强化大型语言模型（LLM）以来，关于如何最有效地弥合LLM的一般知识和您的专有数据之间差距的讨论已经进行了一些讨论。关于[微调与检索增强生成（RAG）哪个更适合此目的的讨论](/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application-94654b1eaba7)已有很多辩论（剧透：两者都是）。
- en: This article first focuses on the concept of RAG and first covers its theory.
    Then, it goes on to showcase how you can implement a simple RAG pipeline using
    [LangChain](https://www.langchain.com/) for orchestration, [OpenAI](https://openai.com/)
    language models, and a [Weaviate](https://weaviate.io/) vector database.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文首先关注RAG的概念，并首先涵盖其理论。接着，展示如何使用[LangChain](https://www.langchain.com/)进行简单的RAG流水线编排，使用[OpenAI](https://openai.com/)语言模型和[Weaviate](https://weaviate.io/)向量数据库。
- en: What is Retrieval-Augmented Generation
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是检索增强生成
- en: Retrieval-Augmented Generation (RAG) is the concept to provide LLMs with additional
    information from an external knowledge source. This allows them to generate more
    accurate and contextual answers while reducing hallucinations.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）是向LLM提供来自外部知识源的额外信息的概念。这使它们能够生成更准确和上下文相关的答案，同时减少幻觉。
- en: Problem
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: 'State-of-the-art LLMs are trained on large amounts of data to achieve a broad
    spectrum of general knowledge stored in the neural network''s weights (parametric
    memory). However, prompting an LLM to generate a completion that requires knowledge
    that was not included in its training data, such as newer, proprietary, or domain-specific
    information, can lead to factual inaccuracies (hallucinations), as illustrated
    in the following screenshot:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最先进的LLM是通过大量数据训练的，以实现存储在神经网络权重（参数化记忆）中的广泛的一般知识。然而，促使LLM生成需要其训练数据中未包含的知识的完成，例如更新的、专有的或特定领域的信息，可能导致事实错误（幻觉），如下截图所示：
- en: '![](../Images/e4a0faeee6e536b3729d70d00f41a34c.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4a0faeee6e536b3729d70d00f41a34c.png)'
- en: ChatGPT’s answer to the question, “What did the president say about Justice
    Breyer?”
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT对问题“总统对贝里尔法官说了什么？”的回答
- en: Thus, it is important to bridge the gap between the LLM’s general knowledge
    and any additional context to help the LLM generate more accurate and contextual
    completions while reducing hallucinations.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，重要的是弥合LLM的一般知识与任何附加背景之间的差距，以帮助LLM生成更准确和上下文相关的完成，同时减少幻觉。
- en: Solution
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Traditionally, neural networks are adapted to domain-specific or proprietary
    information by fine-tuning the model. Although this technique is effective, it
    is also compute-intensive, expensive, and requires technical expertise, making
    it less agile to adapt to evolving information.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，神经网络通过微调模型来适应特定领域或专有信息。虽然这种技术有效，但也计算密集、昂贵，并且需要技术专业知识，使其适应不断变化的信息变得不那么灵活。
- en: In 2020, Lewis et al. proposed a more flexible technique called Retrieval-Augmented
    Generation (RAG) in the paper [Retrieval-Augmented Generation for Knowledge-Intensive
    NLP Tasks](https://arxiv.org/abs/2005.11401) [1]. In this paper, the researchers
    combined a generative model with a retriever module to provide additional information
    from an external knowledge source that can be updated more easily.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 2020年，Lewis等人在论文[知识密集型自然语言处理任务的检索增强生成](https://arxiv.org/abs/2005.11401) [1]中提出了一种更灵活的技术，称为检索增强生成（RAG）。在这篇论文中，研究人员将生成模型与检索模块结合起来，以提供来自外部知识源的额外信息，这些信息可以更容易地更新。
- en: '**In simple terms,** RAG is to LLMs what an open-book exam is to humans. In
    an open-book exam, students are allowed to bring reference materials, such as
    textbooks or notes, which they can use to look up relevant information to answer
    a question. The idea behind an open-book exam is that the test focuses on the
    students’ reasoning skills rather than their ability to memorize specific information.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**简而言之，** RAG对LLM来说就像对人类开放书籍考试一样。在开放书籍考试中，学生被允许携带参考材料，例如教科书或笔记，他们可以用来查找相关信息来回答问题。开放书籍考试背后的思想是，考试侧重于学生的推理能力，而不是他们记忆特定信息的能力。'
- en: 'Similarly, the factual knowledge is separated from the LLM’s reasoning capability
    and stored in an external knowledge source, which can be easily accessed and updated:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，事实知识与LLM的推理能力分离，并存储在可以轻松访问和更新的外部知识源中：
- en: '**Parametric knowledge:** Learned during training that is implicitly stored
    in the neural network''s weights.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数化知识：** 在训练期间学习，隐含存储在神经网络的权重中。'
- en: '**Non-parametric knowledge:** Stored in an external knowledge source, such
    as a vector database.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非参数化知识：** 存储在外部知识源中，例如向量数据库。'
- en: (By the way, I didn’t come up with this genius comparison. As far as I know,
    this comparison was [first mentioned by JJ during the Kaggle — LLM Science Exam
    competition](https://www.kaggle.com/code/jjinho/open-book-llm-science-exam).)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: （顺便说一句，这个天才比较不是我想出来的。据我所知，这个比较是[JJ在Kaggle - LLM科学考试竞赛期间首次提到的](https://www.kaggle.com/code/jjinho/open-book-llm-science-exam)。）
- en: 'The vanilla RAG workflow is illustrated below:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Vanilla RAG 工作流程如下所示：
- en: '![](../Images/ac6c086f3ecee48e8c43eaf4439e8442.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac6c086f3ecee48e8c43eaf4439e8442.png)'
- en: Retrieval-Augmented Generation Workflow
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成工作流程
- en: '**Retrieve:** The user query is used to retrieve relevant context from an external
    knowledge source. For this, the user query is embedded with an embedding model
    into the same vector space as the additional context in the vector database. This
    allows to perform a similarity search, and the top k closest data objects from
    the vector database are returned.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**检索：** 使用用户查询从外部知识源中检索相关内容。为此，用户查询嵌入到与向量数据库中的额外上下文相同的向量空间中。这允许执行相似性搜索，并从向量数据库返回前k个最接近的数据对象。'
- en: '**Augment:** The user query and the retrieved additional context are stuffed
    into a prompt template.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**增强：** 用户查询和检索的额外内容被填充到提示模板中。'
- en: '**Generate:** Finally, the retrieval-augmented prompt is fed to the LLM.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**生成：** 最后，将检索增强提示输入LLM。'
- en: Retrieval-Augmented Generation Implementation using LangChain
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LangChain进行检索增强生成实现
- en: This section implements a RAG pipeline in Python using an [OpenAI](https://openai.com/)
    LLM in combination with a [Weaviate](https://weaviate.io/) vector database and
    an [OpenAI](https://openai.com/) embedding model. [LangChain](https://www.langchain.com/)
    is used for orchestration.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 本节使用OpenAI LLM结合Weaviate向量数据库和OpenAI嵌入模型实现Python中的RAG管道。[LangChain](https://www.langchain.com/)
    用于编排。
- en: 'If you are unfamiliar with LangChain or Weaviate, you might want to check out
    the following two articles:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对LangChain或Weaviate不熟悉，您可能希望查看以下两篇文章：
- en: '[](/getting-started-with-langchain-a-beginners-guide-to-building-llm-powered-applications-95fc8898732c?source=post_page-----4e9bd5f6a4f2--------------------------------)
    [## Getting Started with LangChain: A Beginner’s Guide to Building LLM-Powered
    Applications'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/getting-started-with-langchain-a-beginners-guide-to-building-llm-powered-applications-95fc8898732c?source=post_page-----4e9bd5f6a4f2--------------------------------)
    [## 开始使用LangChain：构建LLM驱动应用程序的初学者指南'
- en: A LangChain tutorial to build anything with large language models in Python
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用Python构建大型语言模型的LangChain教程
- en: 'towardsdatascience.com](/getting-started-with-langchain-a-beginners-guide-to-building-llm-powered-applications-95fc8898732c?source=post_page-----4e9bd5f6a4f2--------------------------------)
    [](/getting-started-with-weaviate-a-beginners-guide-to-search-with-vector-databases-14bbb9285839?source=post_page-----4e9bd5f6a4f2--------------------------------)
    [## Getting Started with Weaviate: A Beginner’s Guide to Search with Vector Databases'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/getting-started-with-langchain-a-beginners-guide-to-building-llm-powered-applications-95fc8898732c?source=post_page-----4e9bd5f6a4f2--------------------------------)
    [](/getting-started-with-weaviate-a-beginners-guide-to-search-with-vector-databases-14bbb9285839?source=post_page-----4e9bd5f6a4f2--------------------------------)
    [## 开始使用Weaviate：使用向量数据库进行搜索的初学者指南
- en: How to use vector databases for semantic search, question answering, and generative
    search in Python with OpenAI and…
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何使用OpenAI和Python中的向量数据库进行语义搜索、问答和生成搜索
- en: towardsdatascience.com](/getting-started-with-weaviate-a-beginners-guide-to-search-with-vector-databases-14bbb9285839?source=post_page-----4e9bd5f6a4f2--------------------------------)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/getting-started-with-weaviate-a-beginners-guide-to-search-with-vector-databases-14bbb9285839?source=post_page-----4e9bd5f6a4f2--------------------------------)
- en: Prerequisites
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 先决条件
- en: 'Make sure you have installed the required Python packages:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您已安装所需的Python包：
- en: '`langchain` for orchestration'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`langchain` 用于编排'
- en: '`openai` for the embedding model and LLM'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`openai` 用于嵌入模型和LLM'
- en: '`weaviate-client` for the vector database'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weaviate-client` 用于向量数据库'
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Additionally, define your relevant environment variables in a .env file in your
    root directory. To obtain an OpenAI API Key, you need an OpenAI account and then
    “Create new secret key” under [API keys](https://platform.openai.com/account/api-keys).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在根目录下的 .env 文件中定义相关的环境变量。要获取 OpenAI API 密钥，你需要一个 OpenAI 账户，然后在[API 密钥](https://platform.openai.com/account/api-keys)下“创建新密钥”。
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Then, run the following command to load the relevant environment variables.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，运行以下命令以加载相关的环境变量。
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Preparation
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'As a preparation step, you need to prepare a vector database as an external
    knowledge source that holds all additional information. This vector database is
    populated by following these steps:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 作为准备步骤，你需要准备一个作为外部知识来源的向量数据库，它包含所有附加信息。通过以下步骤填充这个向量数据库：
- en: Collect and load your data
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集和加载数据
- en: Chunk your documents
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分割文档
- en: Embed and store chunks
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 嵌入和存储片段
- en: The first step is to **collect and load your data** — For this example, you
    will use [President Biden’s State of the Union Address from 2022](https://www.whitehouse.gov/state-of-the-union-2022/)
    as additional context. The raw text document is available in [LangChain’s GitHub
    repository](https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/modules/state_of_the_union.txt).
    To load the data, You can use one of LangChain’s many built-in `[DocumentLoader](https://api.python.langchain.com/en/latest/api_reference.html#module-langchain.document_loaders)`s.
    A `Document` is a dictionary with text and metadata. To load text, you will use
    LangChain’s `TextLoader`.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是**收集和加载你的数据** — 对于这个示例，你将使用[拜登总统 2022 年国情咨文](https://www.whitehouse.gov/state-of-the-union-2022/)作为额外的上下文。原始文本文件可在[LangChain
    的 GitHub 仓库](https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/modules/state_of_the_union.txt)中获得。要加载数据，你可以使用
    LangChain 提供的众多内置的`[DocumentLoader](https://api.python.langchain.com/en/latest/api_reference.html#module-langchain.document_loaders)`之一。`Document`
    是一个包含文本和元数据的字典。要加载文本，你将使用 LangChain 的 `TextLoader`。
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Next, **chunk your documents —** Because the `Document`, in its original state,
    is too long to fit into the LLM’s context window, you need to chunk it into smaller
    pieces. LangChain comes with many built-in [text splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/)
    for this purpose. For this simple example, you can use the `CharacterTextSplitter`
    with a `chunk_size` of about 500 and a `chunk_overlap` of 50 to preserve text
    continuity between the chunks.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，**分割你的文档 —** 由于 `Document` 在其原始状态下过长，无法适配 LLM 的上下文窗口，你需要将其分割成更小的片段。LangChain
    提供了许多内置的[文本分割器](https://python.langchain.com/docs/modules/data_connection/document_transformers/)用于此目的。对于这个简单的示例，你可以使用
    `CharacterTextSplitter`，其 `chunk_size` 大约为 500，`chunk_overlap` 为 50，以保持片段之间的文本连续性。
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Lastly, **embed and store the chunks** — To enable semantic search across the
    text chunks, you need to generate the vector embeddings for each chunk and then
    store them together with their embeddings. To generate the vector embeddings,
    you can use the OpenAI embedding model, and to store them, you can use the Weaviate
    vector database. By calling `.from_documents()` the vector database is automatically
    populated with the chunks.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，**嵌入和存储片段** — 为了在文本片段中启用语义搜索，你需要为每个片段生成向量嵌入，然后将它们与嵌入一起存储。为了生成向量嵌入，你可以使用 OpenAI
    嵌入模型，而为了存储它们，你可以使用 Weaviate 向量数据库。通过调用 `.from_documents()`，向量数据库会自动填充片段。
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Step 1: Retrieve'
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 1：检索
- en: Once the vector database is populated, you can define it as the retriever component,
    which fetches the additional context based on the semantic similarity between
    the user query and the embedded chunks.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦向量数据库被填充，你可以将其定义为检索组件，它根据用户查询和嵌入片段之间的语义相似性来获取额外的上下文。
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Step 2: Augment'
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 2：增强
- en: Next, to augment the prompt with the additional context, you need to prepare
    a prompt template. The prompt can be easily customized from a prompt template,
    as shown below.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，为了增强提示内容，你需要准备一个提示模板。提示可以很容易地从提示模板中定制，如下所示。
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Step 3: Generate'
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 3：生成
- en: Finally, you can build a chain for the RAG pipeline, chaining together the retriever,
    the prompt template and the LLM. Once the RAG chain is defined, you can invoke
    it.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以为 RAG 流程管道构建一个链，将检索器、提示模板和 LLM 链接在一起。一旦定义了 RAG 链，你可以调用它。
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You can see the resulting RAG pipeline for this specific example illustrated
    below:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在下面看到这个特定示例的 RAG 流程管道示意图：
- en: '![](../Images/b939dd150f26bf0a9d0200a3748a64cb.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b939dd150f26bf0a9d0200a3748a64cb.png)'
- en: Retrieval-Augmented Generation Workflow
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成工作流
- en: Summary
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This article covered the concept of RAG, which was presented in the paper [Retrieval-Augmented
    Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)
    [1] from 2020\. After covering some theory behind the concept, including motivation
    and problem solution, this article converted its implementation in Python. This
    article implemented a RAG pipeline using an [OpenAI](https://openai.com/) LLM
    in combination with a [Weaviate](https://weaviate.io/) vector database and an
    [OpenAI](https://openai.com/) embedding model. [LangChain](https://www.langchain.com/)
    was used for orchestration.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了RAG的概念，该概念在2020年的论文 [Retrieval-Augmented Generation for Knowledge-Intensive
    NLP Tasks](https://arxiv.org/abs/2005.11401) [1] 中提出。在介绍概念的理论背景、动机和问题解决方案后，本文使用Python实现了其内容。本文使用了一个
    [OpenAI](https://openai.com/) LLM 结合一个 [Weaviate](https://weaviate.io/) 向量数据库和一个
    [OpenAI](https://openai.com/) 嵌入模型实现了一个RAG流水线。使用 [LangChain](https://www.langchain.com/)
    进行了编排。
- en: Enjoyed This Story?
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 喜欢这个故事吗？
- en: '[*Subscribe for free*](https://medium.com/subscribe/@iamleonie) *to get notified
    when I publish a new story.*'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[*免费订阅*](https://medium.com/subscribe/@iamleonie) *以便在我发布新故事时收到通知。*'
- en: '[](https://medium.com/@iamleonie/subscribe?source=post_page-----4e9bd5f6a4f2--------------------------------)
    [## Get an email whenever Leonie Monigatti publishes.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@iamleonie/subscribe?source=post_page-----4e9bd5f6a4f2--------------------------------)
    [## 每当Leonie Monigatti发布新文章时，收到邮件通知。'
- en: Get an email whenever Leonie Monigatti publishes. By signing up, you will create
    a Medium account if you don't already…
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每当Leonie Monigatti发布新文章时，收到邮件通知。通过注册，如果你还没有Medium账号…
- en: medium.com](https://medium.com/@iamleonie/subscribe?source=post_page-----4e9bd5f6a4f2--------------------------------)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/@iamleonie/subscribe?source=post_page-----4e9bd5f6a4f2--------------------------------)'
- en: '*Find me on* [*LinkedIn*](https://www.linkedin.com/in/804250ab/),[*Twitter*](https://twitter.com/helloiamleonie)*,
    and* [*Kaggle*](https://www.kaggle.com/iamleonie)*!*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*找到我在* [*LinkedIn*](https://www.linkedin.com/in/804250ab/),[*Twitter*](https://twitter.com/helloiamleonie)*，和*
    [*Kaggle*](https://www.kaggle.com/iamleonie)*！*'
- en: Disclaimer
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 免责声明
- en: I am a Developer Advocate at Weaviate at the time of this writing. In addition
    to this article, I have also added the same example to the [Weaviate notebook
    in the LangChain documentation](https://python.langchain.com/docs/integrations/vectorstores/weaviate).
    Alternatively, you can start by following the `[rag-weaviate](https://github.com/langchain-ai/langchain/tree/master/templates/rag-weaviate)`
    [template in LangChain](https://github.com/langchain-ai/langchain/tree/master/templates/rag-weaviate).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在写这篇文章时，我是Weaviate的开发者倡导者。除了本文外，我还在 [LangChain文档中的Weaviate笔记本](https://python.langchain.com/docs/integrations/vectorstores/weaviate)
    中添加了相同的示例。或者，你可以从 [LangChain的rag-weaviate模板](https://github.com/langchain-ai/langchain/tree/master/templates/rag-weaviate)
    开始。
- en: References
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Literature
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文献
- en: '[1] Lewis, P., et al. (2020). Retrieval-augmented generation for knowledge-intensive
    NLP tasks. *Advances in Neural Information Processing Systems*, *33*, 9459–9474.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Lewis, P., et al. (2020). 为知识密集型自然语言处理任务增强检索生成。*Advances in Neural Information
    Processing Systems*, *33*, 9459–9474.'
- en: Images
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图片
- en: If not otherwise stated, all images are created by the author.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图片均由作者创建。
