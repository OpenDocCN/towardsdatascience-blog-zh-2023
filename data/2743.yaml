- en: 'Large Language Models: BERT — Bidirectional Encoder Representations from Transformer'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型：BERT — Transformer 的双向编码器表示
- en: 原文：[https://towardsdatascience.com/bert-3d1bf880386a?source=collection_archive---------0-----------------------#2023-08-30](https://towardsdatascience.com/bert-3d1bf880386a?source=collection_archive---------0-----------------------#2023-08-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/bert-3d1bf880386a?source=collection_archive---------0-----------------------#2023-08-30](https://towardsdatascience.com/bert-3d1bf880386a?source=collection_archive---------0-----------------------#2023-08-30)
- en: Understand how BERT constructs state-of-the-art embeddings
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 BERT 如何构建最先进的嵌入
- en: '[](https://medium.com/@slavahead?source=post_page-----3d1bf880386a--------------------------------)[![Vyacheslav
    Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----3d1bf880386a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3d1bf880386a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3d1bf880386a--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----3d1bf880386a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@slavahead?source=post_page-----3d1bf880386a--------------------------------)[![Vyacheslav
    Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----3d1bf880386a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3d1bf880386a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3d1bf880386a--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----3d1bf880386a--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-3d1bf880386a&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----3d1bf880386a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3d1bf880386a--------------------------------)
    ·11 min read·Aug 30, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3d1bf880386a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-3d1bf880386a&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----3d1bf880386a---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-3d1bf880386a&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----3d1bf880386a---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3d1bf880386a--------------------------------)
    · 11 分钟阅读 · 2023 年 8 月 30 日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3d1bf880386a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-3d1bf880386a&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----3d1bf880386a---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3d1bf880386a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-3d1bf880386a&source=-----3d1bf880386a---------------------bookmark_footer-----------)![](../Images/450d6ac933335232d6ae951d5b7a4e0b.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3d1bf880386a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-3d1bf880386a&source=-----3d1bf880386a---------------------bookmark_footer-----------)![](../Images/450d6ac933335232d6ae951d5b7a4e0b.png)'
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 2017 was a historical year in machine learning when the **Transformer** model
    made its first appearance on the scene. It has been performing amazingly on many
    benchmarks and has become suitable for lots of problems in Data Science. Thanks
    to its efficient architecture, many other Transformer-based models have been developed
    later which specialise more on particular tasks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 2017 年是机器学习的历史性一年，当时**Transformer**模型首次亮相。它在许多基准测试中表现出色，适用于数据科学中的许多问题。由于其高效的架构，后来开发了许多其他基于
    Transformer 的模型，这些模型在特定任务上有了更多的专业化。
- en: One of such models is BERT. It is primarily known for being able to construct
    embeddings which can very accurately represent text information and store semantic
    meanings of long text sequences. As a result, BERT embeddings became widely used
    in machine learning. Understanding how BERT builds text representations is crucial
    because it opens the door for tackling a large range of tasks in NLP.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个这样的模型是 BERT。它主要以能够构建非常准确的文本嵌入而著称，这些嵌入可以表示文本信息并存储长文本序列的语义含义。因此，BERT 嵌入在机器学习中得到了广泛应用。理解
    BERT 如何构建文本表示是至关重要的，因为这为处理自然语言处理中的大量任务打开了大门。
- en: In this article, we will refer to the [original BERT paper](https://arxiv.org/pdf/1810.04805.pdf)
    and have a look at BERT architecture and understand the core mechanisms behind
    it. In the first sections, we will give a high-level overview of BERT. After that,
    we will gradually dive into its internal workflow and how information is passed
    throughout the model. Finally, we will learn how BERT can be fine-tuned for solving
    particular problems in NLP.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将参考 [原始 BERT 论文](https://arxiv.org/pdf/1810.04805.pdf)，查看 BERT 架构并理解其核心机制。在前几部分中，我们将给出
    BERT 的高级概述。之后，我们将逐步深入其内部工作流程及信息在模型中的传递方式。最后，我们将了解如何对 BERT 进行微调，以解决 NLP 中的特定问题。
- en: High level overview
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级概述
- en: '**Transformer**’s architecture consists of two primary parts: encoders and
    decoders. The goal of stacked encoders is to construct a meaningful embedding
    for an input which would preserve its main context. The output of the last encoder
    is passed to inputs of all decoders trying to generate new information.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**Transformer** 的架构由两个主要部分组成：编码器和解码器。堆叠编码器的目标是为输入构建有意义的嵌入，以保持其主要上下文。最后一个编码器的输出传递给所有解码器的输入，试图生成新的信息。'
- en: '**BERT** is a Transformer successor which inherits its stacked bidirectional
    encoders. Most of the architectural principles in BERT are the same as in the
    original Transformer.'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**BERT** 是 Transformer 的继承者，继承了其堆叠的双向编码器。BERT 中的大部分架构原则与原始 Transformer 相同。'
- en: '![](../Images/1de7776be934f0ebf7ede8d9a5864184.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1de7776be934f0ebf7ede8d9a5864184.png)'
- en: Transformer architecture
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 架构
- en: BERT versions
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BERT 版本
- en: 'There exist two main versions of BERT: base and large. Their architecture is
    absolutely identical except for the fact that they use different numbers of parameters.
    Overall, BERT large has 3.09 times more parameters to tune, compared to BERT base.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 存在两个主要版本：base 和 large。它们的架构完全相同，只是参数数量不同。总体而言，BERT large 比 BERT base 多了
    3.09 倍的参数进行调优。
- en: '![](../Images/a1037ce1121042a532a1aff8403e9307.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1037ce1121042a532a1aff8403e9307.png)'
- en: Comparison of BERT base and BERT large
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: BERT base 和 BERT large 的比较
- en: Bidirectional representations
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双向表示
- en: From the letter “B” in the BERT’s name, it is important to remember that BERT
    is a **bidirectional** model meaning that it can better capture word connections
    due to the fact that the information is passed in both directions (left-to-right
    and right-to-left). Obviously, this results in more training resources, compared
    to unidirectional models, but at the same time leads to a better prediction accuracy.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 从 BERT 名称中的字母“B”来看，重要的是要记住 BERT 是一个 **双向** 模型，这意味着它可以更好地捕捉词汇之间的连接，因为信息是双向传递的（从左到右和从右到左）。显然，这与单向模型相比需要更多的训练资源，但同时也导致更好的预测准确性。
- en: For a better understanding, we can visualise BERT architecture in comparison
    with other popular NLP models.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解，我们可以将 BERT 架构与其他流行的 NLP 模型进行比较。
- en: '![](../Images/51e3b04c18a481f9f7a95887afc9a581.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51e3b04c18a481f9f7a95887afc9a581.png)'
- en: Comparison of BERT, OpenAI GPT and ElMo architectures from the [ogirinal paper](https://arxiv.org/pdf/1810.04805.pdf).
    Adopted by the author.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从 [原始论文](https://arxiv.org/pdf/1810.04805.pdf) 中比较 BERT、OpenAI GPT 和 ElMo 架构。由作者采用。
- en: Input tokenisation
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入标记化
- en: Note. In the official paper, authors use the term “**sentence**” to indicate
    text that is passed to the input. To designate the same term, throughout this
    article series we will be using the term “**sequence**”. It is done to avoid confusion
    as “sentence” usually means a single phrase separated by a point and due to the
    fact that in many other NLP research papers the term “sequence” is utilised in
    similar circumstances.
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注：在官方论文中，作者使用“**sentence**”一词来表示传递给输入的文本。为了统一术语，本文系列中我们将使用“**sequence**”一词。这是为了避免混淆，因为“sentence”
    通常指一个由句点分隔的单独短语，而许多其他 NLP 研究论文中“sequence”一词在类似情况下被使用。
- en: 'Before diving into how BERT is trained, it is necessary to understand in what
    format it accepts data. For the input, BERT takes a single sequence or a pair
    of sequences. Each sequence is split into tokens. Additionally, two special tokens
    are passed to the input:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解 BERT 的训练方法之前，有必要了解 BERT 接受数据的格式。对于输入，BERT 接受单个序列或一对序列。每个序列被拆分为标记。此外，两个特殊标记会被传递到输入中：
- en: Note. The official paper uses the term “**sentence**” which designates an input
    **sequence** passed to BERT which can actually consist of several sentences. For
    simplicity, we are going to follow the notation and use the same term throughout
    this article.
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意。官方论文使用了“**句子**”这个术语，它指的是传递给 BERT 的输入**序列**，该序列实际上可以由多个句子组成。为了简化，我们将遵循这个符号，并在本文中使用相同的术语。
- en: '***[CLS]*** — passed before the first sequence indicating its beginning. At
    the same time, *[CLS]* is also used for a classification objective during training
    (discussed in the sections below).'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***[CLS]*** — 在第一个序列之前传递，表示其开始。同时，* [CLS] * 也用于训练中的分类目标（在下面的章节中讨论）。'
- en: '***[SEP]*** — passed between sequences to indicate the end of the first sequence
    and the beginning of the second.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***[SEP]*** — 在序列之间传递，用以表示第一个序列的结束和第二个序列的开始。'
- en: Passing two sequences makes it possible for BERT to handle a large variety of
    tasks where an input contains a pair of sequences (e.g. question and answer, hypothesis
    and premise, etc.).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 传递两个序列使得 BERT 能够处理各种任务，其中输入包含一对序列（例如，问题和答案，假设和前提等）。
- en: Input embedding
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入嵌入
- en: 'After tokenisation, an embedding is built for each token. To make input embeddings
    more representative, BERT constructs three types of embeddings for each token:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在分词之后，为每个标记构建一个嵌入。为了使输入嵌入更具代表性，BERT 为每个标记构建了三种类型的嵌入：
- en: '**Token embeddings** capture the semantic meaning of tokens.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标记嵌入** 捕捉标记的语义意义。'
- en: '**Segment embeddings** have one of two possible values and indicate to which
    sequence a token belongs.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**段嵌入**有两个可能的值，表示标记属于哪个序列。'
- en: '**Position embeddings** contain information about a relative position of a
    token in a sequence.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**位置嵌入** 包含关于标记在序列中相对位置的信息。'
- en: '![](../Images/436c75c0a4550006be88a01948fc4e1c.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/436c75c0a4550006be88a01948fc4e1c.png)'
- en: Input processing
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 输入处理
- en: These embeddings are summed up and the result is passed to the first encoder
    of the BERT model.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这些嵌入被加总，然后结果被传递给 BERT 模型的第一个编码器。
- en: Output
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输出
- en: Each encoder takes *n* embeddings as input and then outputs the same number
    of processed embeddings of the same dimensionality. Ultimately, the whole BERT
    output also contains *n* embeddings each of which corresponds to its initial token.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 每个编码器接受*n*个嵌入作为输入，然后输出相同数量、相同维度的处理后的嵌入。最终，整个 BERT 输出也包含*n*个嵌入，每个嵌入对应其初始的标记。
- en: '![](../Images/349ec9a449ceb70406ce5910691d12fa.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/349ec9a449ceb70406ce5910691d12fa.png)'
- en: Training
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练
- en: 'BERT training consists of two stages:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 训练分为两个阶段：
- en: '**Pre-training**. BERT is trained on unlabeled pair of sequences over two prediction
    tasks: **masked language modeling (MLM)** and **natural language inference (NLI)**.
    For each pair of sequences, the model makes predictions for these two tasks and
    based on the loss values, it performs backpropagation to update weights.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预训练**。BERT 在未标记的序列对上进行训练，涉及两个预测任务：**掩码语言建模（MLM）** 和 **自然语言推理（NLI）**。对于每对序列，模型会对这两个任务进行预测，并根据损失值进行反向传播来更新权重。'
- en: '**Fine-tuning**. BERT is initialised with pre-trained weights which are then
    optimised for a particular problem on labeled data.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**微调**。BERT 使用预训练的权重进行初始化，然后在标记数据上为特定问题进行优化。'
- en: Pre-training
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预训练
- en: Compared to fine-tuning, pre-training usually takes a significant proportion
    of time because the model is trained on a large corpus of data. That is why there
    exist a lot of online repositories of pre-trained models which can be then fine-tined
    relatively fast to solve a particular task.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 与微调相比，预训练通常需要较长的时间，因为模型是在大量数据上训练的。因此，存在许多在线预训练模型的库，这些模型可以被相对快速地微调以解决特定任务。
- en: We are going to look in detail at both problems solved by BERT during pre-training.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将详细查看 BERT 在预训练期间解决的两个问题。
- en: Masked Language Modeling
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 掩码语言建模
- en: 'Authors propose training BERT by masking a certain amount of tokens in the
    initial text and predicting them. *This gives BERT the ability to construct resilient
    embeddings that can use the surrounding context to guess a certain word which
    also leads to building an appropriate embedding for the missed word as well*.
    This process works in the following way:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 作者建议通过掩盖初始文本中的一定数量的标记来训练 BERT 并预测它们。*这使得 BERT 能够构建出具有弹性的嵌入，可以利用周围的上下文来猜测某个词，从而为遗漏的词构建合适的嵌入*。这个过程的工作方式如下：
- en: After tokenization, 15% of tokens are randomly chosen to be masked. The chosen
    tokens will be then predicted at the end of the iteration.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在分词后，15% 的标记被随机选择进行掩盖。选择的标记将在迭代结束时进行预测。
- en: 'The chosen tokens are replaced in one of three ways:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择的标记被以三种方式之一替换：
- en: '*-* 80% of the tokens are replaced by the *[MASK]* token.'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*-* 80% 的标记被替换为 *[MASK]* 标记。'
- en: 'Example*: I bought a book → I bought a [MASK]*'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '示例*: 我买了一本书 → 我买了一个[MASK]*'
- en: '- 10% of the tokens are replaced by a random token.'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 10% 的标记被随机标记替代。'
- en: 'Example: *He is eating a fruit → He is drawing a fruit*'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '示例: *他在吃水果 → 他在画水果*'
- en: '- 10% of the tokens remain unchanged.'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 10% 的标记保持不变。'
- en: 'Example: *A house is near me → A house is near me*'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '示例: *一栋房子在我附近 → 一栋房子在我附近*'
- en: All tokens are passed to the BERT model which outputs an embedding for each
    token it received as input.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有标记被传递给 BERT 模型，模型输出每个接收到的输入标记的嵌入。
- en: 4\. Output embeddings corresponding to the tokens processed at step 2 are independently
    used to predict the masked tokens. The result of each prediction is a probability
    distribution across all the tokens in the vocabulary.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 对应于步骤 2 中处理的标记的输出嵌入被独立用于预测被掩盖的标记。每个预测的结果是词汇表中所有标记的概率分布。
- en: 5\. The cross-entropy loss is calculated by comparing probability distributions
    with the true masked tokens.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 交叉熵损失通过将概率分布与真实掩盖标记进行比较来计算。
- en: 6\. The model weights are updated by using backpropagation.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 6\. 模型权重通过反向传播进行更新。
- en: Natural Language Inference
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然语言推断
- en: For this classification task, BERT tries to predict whether the second sequence
    follows the first. The whole prediction is made by using only the embedding from
    the final hidden state of the *[CLS]* token which is supposed to contain aggregated
    information from both sequences.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个分类任务，BERT 尝试预测第二个序列是否跟随第一个序列。整个预测仅使用来自 *[CLS]* 标记的最终隐藏状态的嵌入，该标记应包含来自两个序列的聚合信息。
- en: Similarly to MLM, a constructed probability distribution (binary in this case)
    is used to calculate the model’s loss and update the weights of the model through
    backpropagation.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 MLM，使用构建的概率分布（二进制的情况下）来计算模型的损失，并通过反向传播更新模型的权重。
- en: For NLI, authors recommend choosing 50% of pairs of sequences which follow each
    other in the corpus (*positive pairs*) and 50% of pairs where sequences are taken
    randomly from the corpus (*negative pairs*).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自然语言推断（NLI），作者建议选择 50% 的序列对，这些序列在语料库中是紧接着的（*正对*），以及 50% 的序列对，其中序列是从语料库中随机选取的（*负对*）。
- en: '![](../Images/49b97c621037edf7eeb72626ce174ad9.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/49b97c621037edf7eeb72626ce174ad9.png)'
- en: BERT pre-training
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 预训练
- en: Training details
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练细节
- en: According to the paper, BERT is pre-trained on BooksCorpus (800M words) and
    English Wikipedia (2,500M words). For extracting longer continuous texts, authors
    took from Wikipedia only reading passages ignoring tables, headers and lists.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 根据论文，BERT 在 BooksCorpus（8 亿单词）和英文维基百科（25 亿单词）上进行预训练。为了提取较长的连续文本，作者仅从维基百科中提取阅读段落，忽略表格、标题和列表。
- en: BERT is trained on a million batches of size equal to 256 sequences which is
    equivalent to 40 epochs on 3.3 billion words. Each sequence contains up to 128
    (90% of the time) or 512 (10% of the time) tokens.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 在大小为 256 的一百万批次上进行训练，这相当于在 33 亿个单词上进行 40 个周期。每个序列包含最多 128（90% 的时间）或 512（10%
    的时间）个标记。
- en: 'According to the original paper, the training parameters are the following:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 根据原始论文，训练参数如下：
- en: 'Optimisator: Adam (learning rate *l* = 1e-4, weight decay L₂ = 0.01, β₁ = 0.9,
    β₂ = 0.999, ε = 1e-6).'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化器：Adam（学习率 *l* = 1e-4，权重衰减 L₂ = 0.01，β₁ = 0.9，β₂ = 0.999，ε = 1e-6）。
- en: Learning rate warmup is performed over the first 10 000 steps and then reduced
    linearly.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率预热在前 10,000 步内进行，然后线性降低。
- en: Dropout (α = 0.1) layer is used on all layers.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在所有层上使用 Dropout（α = 0.1）层。
- en: 'Activation function: GELU.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数：GELU。
- en: Training loss is the sum of mean MLM and mean next sentence prediction likelihoods.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练损失是平均 MLM 和平均下一个句子预测似然的总和。
- en: Fine-tuning
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调
- en: Once pre-training is completed, BERT can literally understand the semantic meanings
    of words and construct embeddings which can almost fully represent their meanings.
    The goal of fine-tuning is to gradually modify BERT weights for solving a particular
    downstream task.
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一旦预训练完成，BERT 可以字面上理解单词的语义，并构建几乎完全表示其意义的嵌入。微调的目标是逐渐调整 BERT 的权重，以解决特定的下游任务。
- en: Data format
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据格式
- en: Thanks to the robustness of the self-attention mechanism, BERT can be easily
    fine-tuned for a particular downstream task. Another advantage of BERT is the
    ability to build *bidirectional* text representations. This gives a higher chance
    of discovering correct relations between two sequences when working with pairs.
    Previous approaches consisted of independently encoding both sequences and then
    applying bidirectional cross-attention to them. BERT unifies these two stages.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 由于自注意力机制的鲁棒性，BERT可以轻松地为特定下游任务进行微调。BERT的另一个优势是能够构建*双向*文本表示。这在处理对时提供了更高的发现两个序列之间正确关系的机会。以前的方法包括独立编码两个序列，然后对它们应用双向交叉注意力。BERT统一了这两个阶段。
- en: 'Depending on a certain problem, BERT accepts several input formats. The framework
    for solving all downstream tasks with BERT is the same: by taking as an input
    a sequence of text, BERT outputs a set of token embeddings which are then fed
    to the model. Most of the time, not all of the output embeddings are used.'
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 根据具体问题，BERT接受几种输入格式。用BERT解决所有下游任务的框架是相同的：输入一个文本序列，BERT输出一组标记嵌入，然后将这些嵌入送入模型。大多数时候，并不是所有的输出嵌入都会被使用。
- en: Let us have a look at common problems and the ways they are solved by fine-tuning
    BERT.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看常见的问题以及通过微调BERT解决这些问题的方法。
- en: '**Sentence pair classification**'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**句子对分类**'
- en: 'The goal of sentence pair classification is to understand the relationship
    between a given pair of sequences. Most of common types of tasks are:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 句子对分类的目标是理解给定序列对之间的关系。常见的任务类型包括：
- en: '*Natural language inference*: determining whether the second sequence follows
    the first.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自然语言推理*：确定第二个序列是否跟随第一个序列。'
- en: '*Similarity analysis*: finding a degree of similarity between sequences.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*相似性分析*：找到序列之间的相似程度。'
- en: '![](../Images/1a1aeb6fad79249756ebcb7b571f565a.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a1aeb6fad79249756ebcb7b571f565a.png)'
- en: Sentence pair classification
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 句子对分类
- en: For fine-tuning, both sequences are passed to BERT. As a rule of thumb, the
    output embedding of the *[CLS]* token is then used for the classification task.
    According to the researchers, the *[CLS]* token is supposed to contain the main
    information about sentence relationships.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于微调，两个序列都传递给BERT。一般来说，* [CLS] *标记的输出嵌入被用来进行分类任务。根据研究人员的说法，* [CLS] *标记应该包含关于句子关系的主要信息。
- en: Of course, other output embeddings can also be used but they are usually omitted
    in practice.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，也可以使用其他输出嵌入，但在实际应用中通常会被省略。
- en: '**Question answering task**'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**问答任务**'
- en: 'The objective of *question answering* is to find an answer in a text paragraph
    corresponding to a particular question. Most of the time, the answer is given
    in the form of two numbers: the start and end token positions of the passage.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*问答*的目标是在文本段落中找到对应于特定问题的答案。大多数时候，答案以两个数字的形式给出：片段的开始和结束标记位置。'
- en: '![](../Images/f78ff21a106291ce5a36445a3a251b6f.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f78ff21a106291ce5a36445a3a251b6f.png)'
- en: Question answering task
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 问答任务
- en: For the input, BERT takes the question and the paragraph and outputs a set of
    embeddings for them. Since the answer is contained within the paragraph, we are
    only interested in output embeddings corresponding to paragraph tokens.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输入，BERT接收问题和段落，并输出一组对应的嵌入。由于答案包含在段落中，我们只对与段落标记对应的输出嵌入感兴趣。
- en: For finding a position of the start answer token in the paragraph, the scalar
    product between every output embedding and a special trainable vector Tₛₜₐᵣₜ is
    calculated. For most cases when the model and the vector Tₛₜₐᵣₜ are trained accordingly,
    the scalar product should be proportional to the likelihood that a corresponding
    token is in reality the start answer token. To normalise scalar products, they
    are then passed to the softmax function and can be thought as probabilities. The
    token embedding corresponding to the highest probability is predicted as the start
    answer token. Based on the true probability distribution, the loss value is calculated
    and the backpropagation is performed. The analogous process is performed with
    the vector Tₑₙ𝒹 for predicting the end token.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到段落中答案起始标记的位置，计算每个输出嵌入与一个特殊的可训练向量Tₛₜₐᵣₓ的标量积。在大多数情况下，当模型和向量Tₛₜₐᵣₓ经过相应训练时，标量积应该与相应标记实际上是起始答案标记的可能性成正比。为了规范化标量积，它们会传递到softmax函数，并可以看作是概率。对应于最高概率的标记嵌入被预测为起始答案标记。根据真实的概率分布，计算损失值并进行反向传播。预测结束标记时会使用向量Tₑₙ𝒹进行类似的过程。
- en: '**Single sentence classification**'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**单句分类**'
- en: 'The difference, compared to previous downstream tasks, is that here only a
    single sentence is passed BERT. Typical problems solved by this configuration
    are the following:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的下游任务相比，区别在于这里只传递单个句子给BERT。此配置解决的典型问题如下：
- en: '*Sentiment analysis*: understanding whether a sentence has a positive or negative
    attitude.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*情感分析*：理解一个句子是否具有积极或消极的态度。'
- en: '*Topic classification*: classifying a sentence into one of several categories
    based on its contents.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*主题分类*：根据句子的内容将句子分类到几个类别之一。'
- en: '![](../Images/114d1219106d34b8c716f7d7e22f50f9.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/114d1219106d34b8c716f7d7e22f50f9.png)'
- en: Single sentence classification
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 单句分类
- en: 'The prediction workflow is the same as for sentence pair classification: the
    output embedding for the *[CLS]* token is used as the input for the classification
    model.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 预测工作流程与句子对分类的工作流程相同：*`[CLS]`*标记的输出嵌入被用作分类模型的输入。
- en: '**Single sentence tagging**'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**单句标注**'
- en: '*Named entity recognition (NER)* is a machine learning problem which aims to
    map every token of a sequence to one of respective entities.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*命名实体识别（NER）*是一个机器学习问题，旨在将序列中的每个标记映射到相应的实体之一。'
- en: '![](../Images/8191bce69a8570d357c72ff10b20896e.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8191bce69a8570d357c72ff10b20896e.png)'
- en: Single sentence tagging
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 单句标注
- en: For this objective, embeddings are computed for tokens of an input sentence,
    as usual. Then every embedding (except for *[CLS]* and *[SEP]*) is passed independently
    to a model which maps each of them to a given NER class (or not, if it cannot).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这个目标，通常会计算输入句子的词嵌入。然后，将每个嵌入（除了*`[CLS]`*和*`[SEP]`*）独立传递给一个模型，该模型将每个嵌入映射到给定的NER类别（如果不能映射，则不进行映射）。
- en: Feature extraction
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征提取
- en: Taking the last BERT layer and using it as embeddings is not the only way to
    extract features from the input text. In fact, the researchers completed several
    experiments of aggregating embeddings in different manners for solving a NER task
    on the CoNLL-2003 dataset. To conduct the experiment, they used the extracted
    embeddings as input for a randomly initialized two-layer 768-dimensional BiLSTM
    before applying the classification layer.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用最后一个BERT层作为嵌入并不是从输入文本中提取特征的唯一方法。实际上，研究人员完成了几种不同方式的嵌入聚合实验，以解决CoNLL-2003数据集上的NER任务。为了进行实验，他们将提取的嵌入作为输入传递给一个随机初始化的两层768维BiLSTM，然后应用分类层。
- en: The ways the embeddings were extracted (from the BERT base) are demonstrated
    in the figure below. As shown, the most performant way was to concatenate the
    four last BERT hidden layers.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入的提取方式（来自BERT基础模型）在下面的图中展示了。如图所示，最有效的方法是连接最后四个BERT隐藏层。
- en: Based on the conducted experiments, it is important to keep in mind that aggregation
    of hidden layers is a potential way of improving embeddings’ representation for
    achieving better results on a variety of NLP tasks.
  id: totrans-115
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 根据已完成的实验，重要的是要记住，隐藏层的聚合是一种可能的改进嵌入表示以在各种NLP任务中取得更好结果的方法。
- en: '![](../Images/b3d9a0b4565828c42c75f85f18c9f5bd.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3d9a0b4565828c42c75f85f18c9f5bd.png)'
- en: The diagram on the left shows the expanded BERT structure with hidden layers.
    The table on the right illustrates the ways the embeddings were constructed and
    the corresponding scores that were achieved by applying respective strategies.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的图表展示了带有隐藏层的扩展BERT结构。右侧的表格则说明了嵌入的构建方式以及通过应用相应策略所取得的得分。
- en: Combining BERT with other features
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将BERT与其他特征结合
- en: 'Sometimes we deal not only with text but with numerical features, for example,
    as well. It is naturally desirable to build embeddings that can incorporate information
    from both text and other non-text features. Here are the recommended strategies
    to apply:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们不仅处理文本，还处理数值特征。例如，自然希望构建能够融合文本和其他非文本特征信息的嵌入。以下是推荐应用的策略：
- en: '**Concatenation of text with non-text features**. For instance, if we work
    with profile descriptions about people in the form of text and there are other
    separate features like their name or age, then a new text description can be obtained
    in the form: “My name is <*name*>. <*profile description*>. I am <*age*> years
    old”. Finally, such a text description can be fed into the BERT model.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将文本与非文本特征进行串联**。例如，如果我们处理的是以文本形式存在的人物简介，并且有其他独立的特征如姓名或年龄，那么可以得到新的文本描述，如：“我的名字是<*name*>。<*profile
    description*>。我<*age*>岁。”最后，这样的文本描述可以输入到BERT模型中。'
- en: '**Concatenation of embeddings with features**. It is possible to build BERT
    embeddings, as discussed above, and then concatenate them with other features.
    The only thing that changes in the configuration is the fact a classification
    model for a downstream task has to accept now input vectors of higher dimensionality.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将嵌入与特征进行串联**。可以如上所述构建BERT嵌入，然后将其与其他特征进行串联。唯一改变的是配置中必须接受更高维度的输入向量用于下游任务的分类模型。'
- en: Conclusion
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we have dived into the processes of BERT training and fine-tuning.
    As a matter of fact, this knowledge is enough to solve the majority of tasks in
    NLP thankfully to the fact that BERT allows to almost fully incorporate text data
    into embeddings.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们深入探讨了BERT的训练和微调过程。实际上，这些知识足以解决大多数自然语言处理任务，感谢BERT几乎完全将文本数据纳入嵌入中的能力。
- en: In recent times, other BERT-like models have appeared (SBERT, RoBERTa, etc.).
    There even exists a special sphere of study called “*BERTology*” which analyses
    BERT capabilities in depth for deriving new high-performant models. These facts
    reinforce the fact that BERT designated a revolution in machine learning and made
    it possible to significantly advance in NLP.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，出现了其他类似BERT的模型（SBERT、RoBERTa等）。甚至存在一个专门研究领域，称为“*BERTology*”，它深入分析BERT的能力，以开发新的高性能模型。这些事实进一步证明了BERT在机器学习领域引发了革命，并使自然语言处理得以显著进步。
- en: Resources
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源
- en: '[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[BERT: 深度双向变换器的预训练用于语言理解](https://arxiv.org/pdf/1810.04805.pdf)'
- en: '*All images unless otherwise noted are by the author*'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，否则所有图像均为作者提供*'
