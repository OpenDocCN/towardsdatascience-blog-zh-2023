- en: 'Unveiling the Dropout Layer: An Essential Tool for Enhancing Neural Networks'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 揭开dropout层的面纱：提升神经网络的必备工具
- en: 原文：[https://towardsdatascience.com/unveiling-the-dropout-layer-an-essential-tool-for-enhancing-neural-networks-e090b726561e](https://towardsdatascience.com/unveiling-the-dropout-layer-an-essential-tool-for-enhancing-neural-networks-e090b726561e)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/unveiling-the-dropout-layer-an-essential-tool-for-enhancing-neural-networks-e090b726561e](https://towardsdatascience.com/unveiling-the-dropout-layer-an-essential-tool-for-enhancing-neural-networks-e090b726561e)
- en: 'Understanding the Dropout Layer: Improving Neural Network Training and Reducing
    Overfitting with Dropout Regularization'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解dropout层：通过dropout正则化提高神经网络训练效果并减少过拟合
- en: '[](https://medium.com/@niklas_lang?source=post_page-----e090b726561e--------------------------------)[![Niklas
    Lang](../Images/5fa71386db00d248438c588c5ae79c67.png)](https://medium.com/@niklas_lang?source=post_page-----e090b726561e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e090b726561e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e090b726561e--------------------------------)
    [Niklas Lang](https://medium.com/@niklas_lang?source=post_page-----e090b726561e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@niklas_lang?source=post_page-----e090b726561e--------------------------------)[![Niklas
    Lang](../Images/5fa71386db00d248438c588c5ae79c67.png)](https://medium.com/@niklas_lang?source=post_page-----e090b726561e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e090b726561e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e090b726561e--------------------------------)
    [Niklas Lang](https://medium.com/@niklas_lang?source=post_page-----e090b726561e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e090b726561e--------------------------------)
    ·7 min read·May 19, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----e090b726561e--------------------------------)
    ·阅读时间7分钟·2023年5月19日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/8cbac42a6864089508adf36e1897f5f7.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8cbac42a6864089508adf36e1897f5f7.png)'
- en: Photo by [Martin Sanchez](https://unsplash.com/@martinsanchez?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[马丁·桑切斯](https://unsplash.com/@martinsanchez?utm_source=medium&utm_medium=referral)拍摄，发布在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: The dropout layer is a layer used in the construction of [neural networks](https://databasecamp.de/en/ml/artificial-neural-networks)
    to prevent [overfitting](https://databasecamp.de/en/ml/overfitting-en). In this
    process, individual nodes are excluded in various training runs using a probability,
    as if they were not part of the network architecture at all.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: dropout层是构建[神经网络](https://databasecamp.de/en/ml/artificial-neural-networks)时用于防止[过拟合](https://databasecamp.de/en/ml/overfitting-en)的一层。在这个过程中，使用一定的概率在不同的训练运行中排除个别节点，仿佛它们根本不属于网络结构。
- en: However, before we can get to the details of this layer, we should first understand
    how a neural network works and why [overfitting](https://databasecamp.de/en/ml/overfitting-en)
    can occur.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，在深入了解这一层的细节之前，我们应首先了解神经网络是如何工作的以及为何会发生[过拟合](https://databasecamp.de/en/ml/overfitting-en)。
- en: 'Quick Recap: How does a Perceptron work?'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速回顾：感知机是如何工作的？
- en: The perceptron is a mathematical model inspired by the structure of the human
    brain. It consists of a single neuron that receives numerical inputs with different
    weights. The inputs are multiplied by their weights and summed up, and the result
    is passed through an activation function. In its simplest form, the perceptron
    produces binary outputs, such as “Yes” or “No,” based on the activation function.
    The sigmoid function is commonly used as an activation function, mapping the weighted
    sum to values between 0 and 1\. If the weighted sum exceeds a certain threshold,
    the output transitions from 0 to 1.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机是一个受到人脑结构启发的数学模型。它由一个接收具有不同权重的数值输入的单一神经元组成。输入值与其权重相乘后加总，结果通过激活函数处理。在最简单的形式中，感知机基于激活函数产生二进制输出，如“是”或“否”。常用的激活函数是sigmoid函数，它将加权和映射到0和1之间的值。如果加权和超过某个阈值，输出将从0转变为1。
- en: '![](../Images/deabfd4e4cf1d6fe680cc9e326079306.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/deabfd4e4cf1d6fe680cc9e326079306.png)'
- en: 'Basic Structure of a Perceptron | Source: Author'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机的基本结构 | 来源：作者
- en: 'For a more detailed look into the concept of perceptrons, feel free to refer
    to this article:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 欲深入了解感知机的概念，欢迎参阅这篇文章：
- en: '[](/what-is-a-perceptron-5ac56720d8cf?source=post_page-----e090b726561e--------------------------------)
    [## Uncovering the Power of Perceptrons: The Building Blocks of Deep Learning'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 揭开感知机的力量：深度学习的构建块](/what-is-a-perceptron-5ac56720d8cf?source=post_page-----e090b726561e--------------------------------)'
- en: Exploring the foundations of artificial neural networks and their real-world
    applications
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索人工神经网络的基础及其实际应用
- en: towardsdatascience.com](/what-is-a-perceptron-5ac56720d8cf?source=post_page-----e090b726561e--------------------------------)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/what-is-a-perceptron-5ac56720d8cf?source=post_page-----e090b726561e--------------------------------)'
- en: 'Quick Recap: What is Overfitting?'
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速回顾：什么是过拟合？
- en: '[Overfitting](https://databasecamp.de/en/ml/overfitting-en) occurs when a predictive
    model becomes too specific to the training data, learning both the patterns and
    noise present in the data. This results in poor generalization and inaccurate
    predictions on new, unseen data. Deep neural networks are particularly susceptible
    to overfitting as they can learn the statistical noise of the training data. However,
    abandoning complex architectures is not desirable, as they enable learning complex
    relationships. The introduction of dropout layers helps address overfitting by
    providing a solution to balance model complexity and generalization.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[过拟合](https://databasecamp.de/en/ml/overfitting-en)发生在预测模型对训练数据过于特定时，学习了数据中的模式和噪声。这会导致对新数据的泛化能力差和预测不准确。深度神经网络特别容易过拟合，因为它们可以学习训练数据的统计噪声。然而，放弃复杂架构并不可取，因为它们能够学习复杂的关系。引入丢弃层有助于解决过拟合问题，通过提供平衡模型复杂性和泛化能力的解决方案。'
- en: '![](../Images/4b85c76203e4d13868674a5d72877acb.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b85c76203e4d13868674a5d72877acb.png)'
- en: 'Difference between Generalisation and Overfitting | Source: Author'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '泛化与过拟合的区别 | 来源: 作者'
- en: 'For a more detailed article on overfitting please refer to our article on the
    topic:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 有关过拟合的更详细文章，请参考我们的相关文章：
- en: '[](https://databasecamp.de/en/ml/overfitting-en?source=post_page-----e090b726561e--------------------------------)
    [## What is Overfitting? | Data Basecamp'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 过拟合是什么？ | 数据库营'
- en: Overfitting is a term from the field of data science and describes the property
    of a model to adapt too strongly to the…
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过拟合是数据科学领域的术语，描述了模型对数据过度适应的特性……
- en: databasecamp.de](https://databasecamp.de/en/ml/overfitting-en?source=post_page-----e090b726561e--------------------------------)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[databasecamp.de](https://databasecamp.de/en/ml/overfitting-en?source=post_page-----e090b726561e--------------------------------)'
- en: How does the Dropout Layer works?
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 丢弃层是如何工作的？
- en: With dropout, certain nodes are set to the value zero in a training run, i.e.
    removed from the network. Thus, they have no influence on the prediction and also
    in the [backpropagation](https://databasecamp.de/en/ml/backpropagation-basics).
    Thus, a new, slightly modified network architecture is built in each run and the
    network learns to produce good predictions without certain inputs.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用丢弃法，某些节点在训练运行中被设置为零，即从网络中移除。因此，它们对预测没有影响，并且在[反向传播](https://databasecamp.de/en/ml/backpropagation-basics)中也是如此。因此，每次运行中都会构建一个新的、略微修改的网络架构，网络学会在没有某些输入的情况下产生良好的预测。
- en: When installing the dropout layer, a so-called dropout probability must also
    be specified. This determines how many of the nodes in the layer will be set equal
    to 0\. If we have an input layer with ten input values, a dropout probability
    of 10% means that one random input will be set equal to zero in each training
    pass. If instead, it is a hidden layer, the same logic is applied to the hidden
    nodes. So a dropout probability of 10% means that 10% of the nodes will not be
    used in each run.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装丢弃层时，还必须指定一个所谓的丢弃概率。这决定了层中多少节点将被设置为0。如果我们有一个包含十个输入值的输入层，10%的丢弃概率意味着在每次训练中，一个随机输入将被设置为零。如果是隐藏层，则对隐藏节点应用相同的逻辑。因此，10%的丢弃概率意味着每次运行中10%的节点将不会被使用。
- en: The optimal probability also depends strongly on the layer type. As various
    papers have found, for the input layer, a dropout probability close to one is
    optimal. For hidden layers, on the other hand, a probability close to 50% leads
    to better results.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳的概率也在很大程度上取决于层的类型。正如各种论文所发现的，对于输入层，接近1的丢弃概率是最佳的。而对于隐藏层，接近50%的概率会产生更好的结果。
- en: Why does the dropout layer prevent overfitting?
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么丢弃层能防止过拟合？
- en: In deep neural networks, overfitting usually occurs because certain neurons
    from different layers influence each other. Simply put, this leads, for example,
    to certain neurons correcting the errors of previous nodes and thus depending
    on each other or simply passing on the good results of the previous layer without
    major changes. This results in comparatively poor generalization.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度神经网络中，过拟合通常发生在不同层的某些神经元互相影响时。简单来说，这导致例如某些神经元纠正前一节点的错误，从而彼此依赖，或者只是传递前一层的好结果而没有重大变化。这会导致相对较差的泛化能力。
- en: By using the dropout layer, on the other hand, neurons can no longer rely on
    the nodes from previous or subsequent layers, since they cannot assume that they
    even exist in that particular training run. This leads to neurons, provably, recognizing
    more fundamental structures in data that do not depend on the existence of individual
    neurons. These dependencies actually occur relatively frequently in regular neural
    networks, as this is an easy way to quickly reduce the loss function and thereby
    quickly get closer to the goal of the model.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，通过使用 dropout 层，神经元不能再依赖于来自前一层或后一层的节点，因为它们无法假设这些节点在特定的训练过程中存在。这导致神经元实际上识别数据中更基本的结构，这些结构不依赖于个别神经元的存在。这些依赖关系在常规神经网络中实际上发生得相对频繁，因为这是快速减少损失函数并迅速接近模型目标的一种简单方法。
- en: Also, as mentioned earlier, the dropout slightly changes the architecture of
    the network. Thus, the trained-out model is then a combination of many, slightly
    different models. We are already familiar with this approach from ensemble learning,
    such as in [Random Forests](https://databasecamp.de/en/ml/random-forests). It
    turns out that the ensemble of many, relatively similar models usually gives better
    results than a single model. This phenomenon is known as the “Wisdom of the Crowds”.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，dropout 也会稍微改变网络的架构。因此，训练出的模型是许多稍微不同模型的组合。我们已经从集成学习中熟悉这种方法，比如在[随机森林](https://databasecamp.de/en/ml/random-forests)中。这表明，许多相对相似的模型的集成通常比单个模型给出更好的结果。这种现象被称为“群体智慧”。
- en: How do you build Dropout into an existing network?
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何将 Dropout 构建到现有网络中？
- en: In practice, the dropout layer is often used after a fully-connected layer,
    since this has comparatively many parameters and the probability of so-called
    “co-adaptation”, i.e. the dependence of neurons on each other, is very high. However,
    theoretically, a dropout layer can also be inserted after any layer, but this
    can then also lead to worse results.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，dropout 层通常在全连接层之后使用，因为全连接层具有相对较多的参数，且所谓的“共适应性”，即神经元之间的依赖性，非常高。然而，从理论上讲，dropout
    层也可以插入在任何层之后，但这可能导致更差的结果。
- en: Practically, the dropout layer is simply inserted after the desired layer and
    then uses the neurons of the previous layer as inputs. Depending on the value
    of the probability, some of these neurons are then set to zero and passed on to
    the subsequent layer.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，dropout 层只是插入在期望的层之后，然后使用前一层的神经元作为输入。根据概率值，这些神经元中的一些会被设置为零，然后传递到后续层。
- en: It is particularly useful to use the dropout layers in larger neural networks.
    This is because an architecture with many layers tends to overfit much more strongly
    than smaller networks. It is also important to increase the number of nodes accordingly
    when a dropout layer is added. As a rule of thumb, the number of nodes before
    the introduction of the dropout is divided by the dropout rate.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在较大的神经网络中使用 dropout 层特别有用。这是因为具有许多层的架构比较小的网络更容易过拟合。当添加 dropout 层时，增加节点的数量也很重要。作为经验法则，节点的数量在引入
    dropout 之前应除以 dropout 率。
- en: What happens to the dropout during inference?
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推理过程中 dropout 会发生什么？
- en: As we have now established, the use of a dropout layer during training is an
    important factor in avoiding overfitting. However, the question remains whether
    this system is also used when the model has been trained and is then used for
    predictions for new data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们现在已经确定的那样，在训练过程中使用 dropout 层是避免过拟合的一个重要因素。然而，问题仍然是这个系统是否也在模型训练完成后用于对新数据进行预测时使用。
- en: In fact, the dropout layers are no longer used for predictions after training.
    This means that all neurons remain for the final prediction. However, the model
    now has more neurons available than it did during training. However, as a result,
    the weights in the output layer are significantly higher than what was learned
    during training. Therefore, the weights are scaled with the amount of the dropout
    rate so that the model still makes good predictions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，训练后 dropout 层将不再用于预测。这意味着所有神经元都将用于最终预测。然而，模型现在拥有比训练期间更多的神经元。因此，输出层的权重显著高于训练期间学到的权重。因此，权重会按照
    dropout 率进行缩放，以确保模型仍能做出良好的预测。
- en: How to use the dropout layers in Python?
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在 Python 中使用 dropout 层？
- en: 'For [Python](https://databasecamp.de/en/python-coding), there are already many
    predefined implementations with which you can use dropout layers. The best-known
    is probably that of Keras or [TensorFlow](https://databasecamp.de/en/python-coding/tensorflow-en).
    You can import these, like other layer types, via “tf.keras.layers”:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 [Python](https://databasecamp.de/en/python-coding)，已经有许多预定义的实现，你可以使用这些实现来应用
    dropout 层。最著名的可能是 Keras 或 [TensorFlow](https://databasecamp.de/en/python-coding/tensorflow-en)。你可以通过“tf.keras.layers”导入这些层类型：
- en: Then you pass the parameters, i.e. on the one hand the size of the input vector
    and the dropout probability, which you should choose depending on the layer type
    and the network structure. The layer can then be used by passing actual values
    in the variable “data”. There is also the parameter “training”, which specifies
    whether the dropout layer is only used in training and not in the prediction of
    new values, the so-called inference.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你需要传递参数，即一方面是输入向量的大小和 dropout 概率，选择这些参数时应考虑层的类型和网络结构。然后，可以通过在变量“data”中传递实际值来使用该层。还有一个参数“training”，它指定
    dropout 层是否仅在训练中使用，而不在新值的预测，即推断中使用。
- en: If the parameter is not explicitly set, the dropout layer will only be active
    for “model.fit()”, i.e. training, and not for “model.predict()”, i.e. predicting
    new values.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果参数未被明确设置，dropout 层将仅在“model.fit()”即训练中处于活动状态，而在“model.predict()”即预测新值时不活动。
- en: This is what you should take with you
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你应该记住的要点
- en: A dropout is a layer in a neural network that sets neurons to zero with a defined
    probability, i.e. ignores them in a training run.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout 是一种神经网络层，它以定义的概率将神经元设置为零，即在训练运行中忽略它们。
- en: In this way, the danger of overfitting can be reduced in deep neural networks,
    since the neurons do not form a so-called adaptation among themselves, but recognize
    deeper structures in the data.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过这种方式，可以减少深度神经网络中的过拟合风险，因为神经元不会在彼此之间形成所谓的适应，而是识别数据中的更深层次结构。
- en: The dropout layer can be used in the input layer as well as in the hidden layers.
    However, it has been shown that different dropout probabilities should be used
    depending on the layer type.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout 层可以用于输入层以及隐藏层。然而，研究表明，不同的 dropout 概率应根据层的类型来使用。
- en: However, once the training has been trained out, the dropout layer is no longer
    used for predictions. However, in order for the model to continue to produce good
    results, the weights are scaled using the dropout rate.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，一旦训练完成，dropout 层将不再用于预测。然而，为了使模型继续产生良好的结果，权重会根据 dropout 率进行缩放。
- en: '*If you like my work, please subscribe* [*here*](https://medium.com/subscribe/@niklas_lang)
    *or check out my website* [*Data Basecamp*](http://www.databasecamp.de/en/homepage)*!
    Also, medium permits you to read* ***3 articles*** *per month for free. If you
    wish to have* ***unlimited*** *access to my articles and thousands of great articles,
    don’t hesitate to get a membership for $****5*** *per month by clicking my referral
    link:* [https://medium.com/@niklas_lang/membership](https://medium.com/@niklas_lang/membership)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你喜欢我的工作，请* [*点击这里*](https://medium.com/subscribe/@niklas_lang) *订阅*，*或者查看我的网站*
    [*Data Basecamp*](http://www.databasecamp.de/en/homepage)*！此外，Medium 允许你每月免费阅读*
    ***3 篇文章*** *。如果你希望* ***无限*** *访问我的文章和数千篇优秀文章，请点击我的推荐链接，会员费用为每月 $****5*** *：*
    [https://medium.com/@niklas_lang/membership](https://medium.com/@niklas_lang/membership)'
- en: '[](/understanding-the-backpropagation-algorithm-c7a99d43088b?source=post_page-----e090b726561e--------------------------------)
    [## Unraveling the Mystery of Back Propagation: A Comprehensive Guide'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/understanding-the-backpropagation-algorithm-c7a99d43088b?source=post_page-----e090b726561e--------------------------------)
    [## 解开反向传播的神秘面纱：全面指南'
- en: Understanding the inner workings of the backpropagation algorithm for training
    neural networks.
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解反向传播算法在训练神经网络中的内部工作机制。
- en: 'towardsdatascience.com](/understanding-the-backpropagation-algorithm-c7a99d43088b?source=post_page-----e090b726561e--------------------------------)
    [](/what-are-tensors-in-machine-learning-5671814646ff?source=post_page-----e090b726561e--------------------------------)
    [## From Vectors to Tensors: Exploring the Mathematics of Tensor Algebra'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/understanding-the-backpropagation-algorithm-c7a99d43088b?source=post_page-----e090b726561e--------------------------------)
    [](/what-are-tensors-in-machine-learning-5671814646ff?source=post_page-----e090b726561e--------------------------------)
    [## 从向量到张量：探索张量代数的数学原理'
- en: How Tensors are Used in Machine Learning and Beyond
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 《张量在机器学习及其应用中的使用》
- en: towardsdatascience.com](/what-are-tensors-in-machine-learning-5671814646ff?source=post_page-----e090b726561e--------------------------------)
    [](/the-importance-of-cross-validation-in-machine-learning-35b728bbce33?source=post_page-----e090b726561e--------------------------------)
    [## The Importance of Cross Validation in Machine Learning
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/what-are-tensors-in-machine-learning-5671814646ff?source=post_page-----e090b726561e--------------------------------)
    [](/the-importance-of-cross-validation-in-machine-learning-35b728bbce33?source=post_page-----e090b726561e--------------------------------)
    [## 机器学习中交叉验证的重要性'
- en: Explaining why Machine Learning needs Cross Validation and how it is done in
    Python
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解释为什么机器学习需要交叉验证以及如何在 Python 中实现。
- en: towardsdatascience.com](/the-importance-of-cross-validation-in-machine-learning-35b728bbce33?source=post_page-----e090b726561e--------------------------------)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/the-importance-of-cross-validation-in-machine-learning-35b728bbce33?source=post_page-----e090b726561e--------------------------------)'
