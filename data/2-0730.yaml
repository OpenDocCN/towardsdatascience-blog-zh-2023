- en: DETR (Transformers for Object Detection)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DETR（用于目标检测的变换器）
- en: 原文：[https://towardsdatascience.com/detr-transformers-for-object-detection-a8b3327b737a](https://towardsdatascience.com/detr-transformers-for-object-detection-a8b3327b737a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/detr-transformers-for-object-detection-a8b3327b737a](https://towardsdatascience.com/detr-transformers-for-object-detection-a8b3327b737a)
- en: Deep Dive and clear explanations on the paper “End to end detection with transformers”
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对论文“基于变换器的端到端检测”的深入剖析和清晰解释
- en: '[](https://medium.com/@francoisporcher?source=post_page-----a8b3327b737a--------------------------------)[![François
    Porcher](../Images/9ddb233f8cadbd69026bd79e2bd62dea.png)](https://medium.com/@francoisporcher?source=post_page-----a8b3327b737a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a8b3327b737a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a8b3327b737a--------------------------------)
    [François Porcher](https://medium.com/@francoisporcher?source=post_page-----a8b3327b737a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@francoisporcher?source=post_page-----a8b3327b737a--------------------------------)[![François
    Porcher](../Images/9ddb233f8cadbd69026bd79e2bd62dea.png)](https://medium.com/@francoisporcher?source=post_page-----a8b3327b737a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a8b3327b737a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a8b3327b737a--------------------------------)
    [François Porcher](https://medium.com/@francoisporcher?source=post_page-----a8b3327b737a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a8b3327b737a--------------------------------)
    ·8 min read·Oct 7, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----a8b3327b737a--------------------------------)
    ·阅读时长8分钟·2023年10月7日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/dae11e25d74f1483e61af5a29e11a35f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dae11e25d74f1483e61af5a29e11a35f.png)'
- en: Photo by [Aditya Vyas](https://unsplash.com/@aditya1702?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[Aditya Vyas](https://unsplash.com/@aditya1702?utm_source=medium&utm_medium=referral)拍摄，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)'
- en: 'Note: This article delves into the intricate world of Computer Vision, specifically
    focusing on Transformers and the Attention Mechanism. It’s recommended to be acquainted
    with the key concepts from the paper [“Attention is All You Need.”](https://arxiv.org/abs/1706.03762)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 注：本文深入探讨了计算机视觉的复杂世界，特别是变换器和注意力机制。建议了解论文[“Attention is All You Need.”](https://arxiv.org/abs/1706.03762)中的关键概念。
- en: A Snapshot of History
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 历史快照
- en: '**DETR**, short for **DE**tection **TR**ansformer, pioneered a novel wave in
    object detection upon its conception by Nicolas Carion and team at **Facebook
    AI Research in 2020.**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**DETR**，即**DE**tection **TR**ansformer，开创了一种新的目标检测方式，由**Nicolas Carion及其团队于2020年在Facebook
    AI Research提出**。'
- en: While not currently holding the SOTA (State Of The Art) status, DETR’s innovative
    reformulation of object detection tasks has significantly influenced subsequent
    models, such as CO-DETR, which is the current State-of-the-Art in **Object Detection**
    and **Instance Segmentation** on [LVIS](https://www.lvisdataset.org).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管目前尚未达到SOTA（最先进技术）水平，DETR对目标检测任务的创新重新定义显著影响了随后的模型，例如CO-DETR，它是[LVIS](https://www.lvisdataset.org)上**目标检测**和**实例分割**的当前最先进技术。
- en: Moving away from the conventional one-to-many problem scenario, where each ground
    truth corresponds to myriad anchor candidates, DETR introduces a fresh perspective
    by viewing object detection as a **set prediction problem**, with a one-to-one
    correspondance between predictions and ground truth, thereby eliminating the need
    for certain post-processing techniques.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的多对一问题场景不同，其中每个真实值对应多个锚点候选，DETR通过将目标检测视为**集合预测问题**，实现了预测与真实值之间的一对一对应，从而消除了对某些后处理技术的需求。
- en: Refresher on Object Detection
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标检测概述
- en: Object detection is a domain of computer vision that focuses on identifying
    and locating objects within images or video frames. Beyond merely classifying
    an object, it provides a bounding box, indicating the object’s location in the
    image, thereby enabling systems to understand the spatial context and positioning
    of various identified objects.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测是计算机视觉的一个领域，专注于识别和定位图像或视频帧中的对象。除了仅仅对对象进行分类外，它还提供一个边界框，指示对象在图像中的位置，从而使系统能够理解各种识别对象的空间上下文和位置。
- en: '![](../Images/882459fab2d3469f3648a695ae2aab34.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/882459fab2d3469f3648a695ae2aab34.png)'
- en: Yolo5 video segmentation, [source](https://commons.wikimedia.org/wiki/File:Yolo5.gif)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Yolo5 视频分割，[来源](https://commons.wikimedia.org/wiki/File:Yolo5.gif)
- en: Object detection is really useful in itself for example in autonomous driving,
    but it is also a preliminary task for **instance segmentation**, where we try
    to seek a more precise contour of the objects, while being able to differentiate
    betwen different instances (unlike semantic segmentation).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 物体检测本身非常有用，例如在自动驾驶中，但它也是**实例分割**的前置任务，在实例分割中我们尝试寻找物体的更精确轮廓，同时能够区分不同实例（与语义分割不同）。
- en: Demystifying Non-Maximum Suppression (and getting rid of it)
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 揭开非极大值抑制的神秘面纱（并摆脱它）
- en: '![](../Images/1b9c9b633277fbe1263194dea79f1675.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b9c9b633277fbe1263194dea79f1675.png)'
- en: Non-Maximum Suppression, Image by author
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 非极大值抑制，作者提供的图像
- en: '**Non-Maximum Suppression (NMS)** has long been the cornerstone in object detection
    algorithms, performing an indispensable role in post-processing to refine the
    prediction outputs. In traditional object detection frameworks, the model proposes
    a plethora of bounding boxes around potential object regions, some of which invariably
    exhibit substantial overlap (as seen on the picture above).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**非极大值抑制 (NMS)** 长期以来一直是物体检测算法中的基石，在后处理过程中发挥了不可或缺的作用，以精细化预测输出。在传统的物体检测框架中，模型提出了大量围绕潜在物体区域的边界框，其中一些不可避免地显示出显著的重叠（如上图所示）。'
- en: NMS addresses this by **preserving the bounding box with the maximum predicted
    objectness scor**e while concurrently suppressing the neighboring boxes that manifest
    a high degree of overlap, quantified by the Intersection over Union (IoU) metric.
    Specifically, given a pre-established IoU threshold, NMS iteratively selects the
    bounding box with the highest confidence score and nullifies those with IoU exceeding
    this threshold, **ensuring a singular, highly confident prediction per object**
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: NMS 通过**保留具有最大预测对象性分数的边界框**，同时抑制那些表现出高重叠度的相邻框来解决这一问题，重叠度通过交并比 (IoU) 量化。具体来说，给定预设的
    IoU 阈值，NMS 迭代选择具有最高置信度分数的边界框，并取消那些 IoU 超过该阈值的边界框，**确保每个对象只有一个高置信度的预测**。
- en: Despite its ubiquity, DETR (DEtection TRansformer) audaciously sidesteps the
    conventional NMS, reinventing object detection by **formulating it as a set prediction
    problem.**
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 DETR（DEtection TRansformer）无处不在，但它大胆地避开了传统的 NMS，通过**将物体检测重新定义为集合预测问题**来重新发明物体检测。
- en: By leveraging transformers, DETR directly predicts a fixed-size set of bounding
    boxes and obviates the necessity for the traditional NMS, remarkably **simplifying
    the object detection pipeline while preserving, if not enhancing, model performance.**
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用变换器，DETR 直接预测固定大小的边界框，从而消除了传统 NMS 的必要性，显著**简化了物体检测管道，同时保持甚至提升了模型性能**。
- en: Deep Dive in DETR architecture
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入探讨 DETR 架构
- en: At the High-Level picture, DETR is
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次图像中，DETR 是
- en: '**An Image Encoder** (it is actually a double Image Encoder because there is
    firstly a **CNN Backbone** followed by a **Transformer** **Encoder** for more
    expressivity)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像编码器**（实际上是一个双重图像编码器，因为首先是一个**CNN骨干网络**，然后是一个**变换器** **编码器**，以增加更多的表达能力）'
- en: A **Transformer Decoder** which produces the bounding boxes from the image encoding.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变换器解码器** 从图像编码中生成边界框。'
- en: '![](../Images/b34f2bb86d4d6e488a9661e3a9571e11.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b34f2bb86d4d6e488a9661e3a9571e11.png)'
- en: DETR architecture, image from [article](https://arxiv.org/abs/2005.12872)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: DETR 架构，图片来自 [文章](https://arxiv.org/abs/2005.12872)
- en: 'Let’s go into more details in each part:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细了解每个部分：
- en: '**Backbone:**'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**骨干网络：**'
- en: 'We start from an initial image with 3 color channels:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个具有 3 个颜色通道的初始图像开始：
- en: And this image is fed into a backbone which is a Convolutional Neural Network
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图像被输入到一个骨干网络中，该骨干网络是一个卷积神经网络
- en: Typical values we use are *C = 2048* and *H = W = H0 =W0 = 32*
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的典型值是 *C = 2048* 和 *H = W = H0 = W0 = 32*
- en: '**2\. Transformer encoder:**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 变换器编码器：**'
- en: The Transformer Encoder is theoretically not mandatory but it adds more expressivity
    to the backbone, and ablation studies show improved performance.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器编码器在理论上并非强制要求，但它为骨干网络增加了更多表达能力，消融研究表明性能有所提升。
- en: First, a 1x1 convolution reduces the channel dimension of the high-level activation
    map *f* from *C* to a smaller dimension *d*.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，1x1 卷积将高层激活图 *f* 的通道维度从 *C* 减少到较小的维度 *d*。
- en: After the Convolution
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积之后
- en: 'But as you know, Transformers map an input sequence of vectors to an output
    sequence of vectors, so we need to collapse the spatial dimension:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 但正如你所知，Transformers将输入向量序列映射到输出向量序列，所以我们需要压缩空间维度：
- en: After collapsing the spatial dimension
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在压缩空间维度后
- en: Now we are ready to feed this to the Transformer Encoder.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备将其输入到Transformer编码器中。
- en: It is important to note that the **Transformer Encoder only uses self-attention
    mechanism.**
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意**Transformer编码器仅使用自注意力机制。**
- en: That’s it for the **Image Encoding part**!
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样**图像编码部分**！
- en: '![](../Images/46dc94deac1f578f41c40d103aaca1e9.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/46dc94deac1f578f41c40d103aaca1e9.png)'
- en: More details on the Decoder, Image from [article](https://arxiv.org/abs/2005.12872)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器的更多细节，图像来自[文章](https://arxiv.org/abs/2005.12872)
- en: '**3\. Transformer decoder:**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. Transformer解码器：**'
- en: This part is the most difficult part to understand, hang on, if you understand
    this, you understand most of the article.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分是最难理解的部分，耐心点，如果你理解了这部分，你就理解了文章的大部分内容。
- en: The Decoder uses a combination of Self-Attention and Cross-Attention mechanisms.
    It is fed *N* object queries, and each query will be transformed into an output
    box and class.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器使用自注意力和交叉注意力机制的组合。它接收*N*个对象查询，每个查询将被转换为一个输出框和类别。
- en: What does a box prediction look like?
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 框预测是什么样的？
- en: It is actually made of 2 components.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 它实际上由两个组件组成。
- en: A bounding box has some coordinates (x1, y1, x2, y2) to identify the bounding
    box.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个边界框具有一些坐标（x1，y1，x2，y2）来识别边界框。
- en: A class (for example seagul, but it can also be empty)
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一类（例如海鸥，但也可以为空）
- en: It is important to note that *N* is fixed. It means that DETR always predict
    exactly *N* bounding boxes. But some of them can be empty. We just have to make
    sure that *N* is big enough to cover enough objects in the images.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意*N*是固定的。这意味着DETR始终预测**N**个边界框。但其中一些可能为空。我们只需确保*N*足够大，以覆盖图像中的足够对象。
- en: Then the Cross-Attention Mechanism can attend the image features produced by
    the Encoding part (Backbone + Transformer Encoder).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然后交叉注意力机制可以关注编码部分（骨干网络 + Transformer编码器）生成的图像特征。
- en: 'If you are unsure about the mechanism, this scheme should clarify things:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对机制不确定，这个方案应该能澄清问题：
- en: '![](../Images/71798b7319d8ae8879d3f90251249028.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/71798b7319d8ae8879d3f90251249028.png)'
- en: Detailed Attention mechanism, image from [article](https://arxiv.org/abs/2005.12872)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 详细的注意力机制，图像来自[文章](https://arxiv.org/abs/2005.12872)
- en: In the original Transformer architecture we produce one token and then we use
    a combination of self-attention and cross-attention to produce the next token
    and repeat. But here we do not need this recurrence formulation, we can just produce
    all the outputs at one so we can exploit parallelism.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始Transformer架构中，我们生成一个token，然后使用自注意力和交叉注意力的组合来生成下一个token并重复。但在这里，我们不需要这种递归的公式，我们可以一次性生成所有输出，从而利用并行性。
- en: 'The main innovation: Bipartite Matching Loss'
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主要创新：二分匹配损失
- en: As mentioned before, DETR produces exactly *N* outputs (bboxes + class). But
    each output corresponds to one ground truth only. If you remember well this is
    the whole point, we do not want to apply any post-processing to filter out overlapping
    boxes.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，DETR产生**N**个输出（框 + 类）。但每个输出只对应一个真实框。如果你记得清楚，这就是关键点，我们不想应用任何后处理来过滤重叠的框。
- en: '![](../Images/96b2fc4578be6806117b213a87aae949.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96b2fc4578be6806117b213a87aae949.png)'
- en: Bipartite Matching, Image by author
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 二分匹配，由作者提供的图像
- en: We want to basically associate each prediction with the closest ground truth.
    So we are in fact looking for a bijection between the prediction set and the ground
    truth set, which minimizes a total loss.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基本上想将每个预测与最近的真实框关联。因此，我们实际上是在寻找预测集与真实框集之间的一个双射，来最小化总损失。
- en: So how do we define this loss?
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们如何定义这个损失呢？
- en: 1\. The matching loss (pairwise loss)
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 匹配损失（成对损失）
- en: 'We first need to define a matching loss, which corresponds to the loss between
    one prediction box and one ground truth box:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要定义一个匹配损失，它对应于一个预测框和一个真实框之间的损失：
- en: 'And this loss is split needs to account for 2 components:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这个损失需要考虑两个组件：
- en: '**The classification loss** (is the class predicted inside the bounding box
    the same as the ground truth)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类损失**（预测框内的类别是否与真实类别相同）'
- en: '**The bounding box loss** (is the bounding box close to the ground truth)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**边界框损失**（边界框是否接近真实框）'
- en: '![](../Images/3624d28403e0a284aa6a0767562ff29d.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3624d28403e0a284aa6a0767562ff29d.png)'
- en: Matching Loss
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 匹配损失
- en: 'And more precisely for the bounding box component there are 2 sub components:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 更准确地说，对于边界框组件，有两个子组件：
- en: Intersection over Union Loss (IOU)
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交并比损失（IOU）
- en: L1 Loss (absolute difference between coordinates)
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L1损失（坐标之间的绝对差异）
- en: '![](../Images/360a54f203774c2e160b37c8428202b6.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/360a54f203774c2e160b37c8428202b6.png)'
- en: 2\. Total Loss of the Bijection
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 双射的总损失
- en: 'To compute the total loss, we just sum over the *N* instances:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 计算总损失时，我们只是对*N*个实例求和：
- en: '![](../Images/b55f7d61d8df711130b3ea3aa7eec395.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b55f7d61d8df711130b3ea3aa7eec395.png)'
- en: 'So basically our problem is to find the bijection that minimizes the total
    loss:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 所以基本上我们的问题是找到最小化总损失的双射：
- en: '![](../Images/c411a6a9b81d88fc882850c11e59d6af.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c411a6a9b81d88fc882850c11e59d6af.png)'
- en: Reformulation of the problem
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 问题的重新表述
- en: Performance Insights
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能洞察
- en: '![](../Images/f0a9bdef234d7ce79dfd7e5147b34a42.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f0a9bdef234d7ce79dfd7e5147b34a42.png)'
- en: Performance of DETR vs Faster RCNN
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: DETR与Faster RCNN的性能对比
- en: '**DETR:** This refers to the original model, which uses a transformer for object
    detection and a **ResNet-50 as a backbone.**'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DETR：** 这指的是原始模型，使用了一个用于目标检测的transformer和**ResNet-50作为骨干网络。**'
- en: '**DETR-R101:** This is a variant of DETR that employs a **ResNet-101 backbone
    instead of ResNet-50**. Here, “R101” refers to “ResNet-101”.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DETR-R101：** 这是DETR的一个变体，采用了**ResNet-101骨干网络而不是ResNet-50**。这里，“R101”指的是“ResNet-101”。'
- en: '**DETR-DC5:** This version of DETR uses the modified, **dilated C5 stage in
    its ResNet-50 backbone**, improving the model’s performance on smaller objects
    due to the increased feature resolution.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DETR-DC5：** 这个版本的DETR使用了修改过的**ResNet-50骨干网络中的扩张C5阶段**，由于特征分辨率的提高，提高了模型在小物体上的表现。'
- en: '**DETR-DC5-R101:** This variant combines **both modifications**. It uses a
    ResNet-101 backbone and includes the dilated C5 stage, benefiting from both the
    deeper network and the increased feature resolution.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DETR-DC5-R101：** 这个变体结合了**这两种修改**。它使用ResNet-101骨干网络，并包括扩张C5阶段，受益于更深的网络和更高的特征分辨率。'
- en: DETR significantly outperforms baselines on large objects, which is very likely
    enabled by the non-local computations allowed by the transformer. But interesting
    enough DETR achieves lower performances on small objects.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: DETR在大型物体上显著超越了基线，这很可能是由于transformer允许的非局部计算。然而，值得注意的是，DETR在小物体上的表现较差。
- en: Why the Attention Mechanism is so Powerful is this case?
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么在这种情况下Attention机制如此强大？
- en: '![](../Images/6148040e2feb2ba4e44ddee1901f4c47.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6148040e2feb2ba4e44ddee1901f4c47.png)'
- en: Attention on Overlapping instances, image from article
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对重叠实例的Attention，图像来自文章
- en: Very interestingly, we can observe that in the case of Overlapping instances,
    Attention mechanism is able to correctly separate individual instances as shown
    on the picture above.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 非常有趣的是，我们可以观察到在重叠实例的情况下，Attention机制能够正确地分离出各个实例，如上图所示。
- en: '![](../Images/54f9274e18939352912de99147a2587b.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/54f9274e18939352912de99147a2587b.png)'
- en: Attention mechanism on extremities
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 极端点的Attention机制
- en: It is also very interesting no note that Attention is focused on the extremities
    of objects to produce the bounding box, which is exactly what we expect.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 同样有趣的是，注意力机制集中在物体的极端点上以生成边界框，这正是我们所期望的。
- en: Wrapping up
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: DETR is not merely a model; it is a paradigm shift, transforming object detection
    from a one-to-many problem into a set prediction problem, effectively utilizing
    Transformer architecture advancements.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: DETR不仅仅是一个模型；它是一个范式转变，将目标检测从一个一对多的问题转变为一个集合预测问题，充分利用了Transformer架构的进展。
- en: Enhancements have unfolded since its inception, with models like DETR++ and
    CO-DETR now steering the ship as State of the Art in Instance Segmentation and
    Object Detection on the [LVIS](https://www.lvisdataset.org) dataset.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 自其诞生以来，已经出现了许多改进，例如DETR++和CO-DETR，现在在[LVIS](https://www.lvisdataset.org)数据集上引领了实例分割和目标检测的最前沿。
- en: 'Thanks for reading! Before you go:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！在你离开之前：
- en: Check my [compilation of AI tutorials](https://github.com/FrancoisPorcher/awesome-ai-tutorials)
    on Github
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看我在Github上的[AI教程合集](https://github.com/FrancoisPorcher/awesome-ai-tutorials)
- en: '[](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----a8b3327b737a--------------------------------)
    [## GitHub — FrancoisPorcher/awesome-ai-tutorials: The best collection of AI tutorials
    to make you a…'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----a8b3327b737a--------------------------------)
    [## GitHub — FrancoisPorcher/awesome-ai-tutorials: 最好的AI教程合集让你成为…]'
- en: The best collection of AI tutorials to make you a boss of Data Science! — GitHub
    …
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最好的AI教程合集，让你成为数据科学的专家！— GitHub …
- en: github.com](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----a8b3327b737a--------------------------------)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----a8b3327b737a--------------------------------)'
- en: Y*ou should get my articles in your inbox.* [***Subscribe here.***](https://medium.com/@francoisporcher/subscribe)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '*你应该将我的文章直接发送到你的邮箱。* [***在这里订阅。***](https://medium.com/@francoisporcher/subscribe)'
- en: '*If you want to have access to premium articles on Medium, you only need a
    membership for $5 a month. If you sign up* [***with my link***](https://medium.com/@francoisporcher/membership)*,
    you support me with a part of your fee without additional costs.*'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你想访问 Medium 上的优质文章，你只需每月支付 $5 会员费。如果你通过* [***我的链接***](https://medium.com/@francoisporcher/membership)*注册，你将用你的一部分费用支持我，而无需额外支付。*'
- en: If you found this article insightful and beneficial, please consider following
    me and leaving a clap for more in-depth content! Your support helps me continue
    producing content that aids our collective understanding.
  id: totrans-108
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你觉得这篇文章有见解且有帮助，请考虑关注我并点赞，以便获得更多深入的内容！你的支持帮助我继续制作对我们集体理解有帮助的内容。
- en: References
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: “End-to-End Object Detection with Transformers” by Nicolas Carion, Francisco
    Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.
    You can read it in full on [arXiv](https://arxiv.org/abs/2005.12872).
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “End-to-End Object Detection with Transformers” 由 Nicolas Carion、Francisco Massa、Gabriel
    Synnaeve、Nicolas Usunier、Alexander Kirillov 和 Sergey Zagoruyko 编写。你可以在 [arXiv](https://arxiv.org/abs/2005.12872)
    上阅读全文。
- en: Zong, Z., Song, G., & Liu, Y. (Year of Publication). DETRs with Collaborative
    Hybrid Assignments Training. [https://arxiv.org/pdf/2211.12860.pdf](https://arxiv.org/pdf/2211.12860.pdf).
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zong, Z., Song, G., & Liu, Y.（出版年份）。DET网上协作混合分配训练。 [https://arxiv.org/pdf/2211.12860.pdf](https://arxiv.org/pdf/2211.12860.pdf)。
- en: '[COCO dataset](https://cocodataset.org/#home)'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[COCO 数据集](https://cocodataset.org/#home)'
