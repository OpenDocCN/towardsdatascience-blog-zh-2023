- en: Getting Started with Multimodality
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多模态入门
- en: 原文：[https://towardsdatascience.com/getting-started-with-multimodality-eab5f6453080](https://towardsdatascience.com/getting-started-with-multimodality-eab5f6453080)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/getting-started-with-multimodality-eab5f6453080](https://towardsdatascience.com/getting-started-with-multimodality-eab5f6453080)
- en: '![](../Images/ae8e7dbea616220acfe1963d16441f60.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae8e7dbea616220acfe1963d16441f60.png)'
- en: Image created with Microsoft Designer
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由Microsoft Designer创建
- en: Understanding vision capabilities of Large Multimodal Models
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解大型多模态模型的视觉能力
- en: '[](https://valentinaalto.medium.com/?source=post_page-----eab5f6453080--------------------------------)[![Valentina
    Alto](../Images/888b8aa17759d8dd5332d8fd4653cf05.png)](https://valentinaalto.medium.com/?source=post_page-----eab5f6453080--------------------------------)[](https://towardsdatascience.com/?source=post_page-----eab5f6453080--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----eab5f6453080--------------------------------)
    [Valentina Alto](https://valentinaalto.medium.com/?source=post_page-----eab5f6453080--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://valentinaalto.medium.com/?source=post_page-----eab5f6453080--------------------------------)[![Valentina
    Alto](../Images/888b8aa17759d8dd5332d8fd4653cf05.png)](https://valentinaalto.medium.com/?source=post_page-----eab5f6453080--------------------------------)[](https://towardsdatascience.com/?source=post_page-----eab5f6453080--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----eab5f6453080--------------------------------)
    [Valentina Alto](https://valentinaalto.medium.com/?source=post_page-----eab5f6453080--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----eab5f6453080--------------------------------)
    ·9 min read·Dec 27, 2023
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----eab5f6453080--------------------------------)
    ·阅读时间9分钟·2023年12月27日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: The recent advances in Generative AI have enabled the development of Large Multimodal
    Models (LMMs) that can process and generate different types of data, such as text,
    images, audio, and video.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 生成AI的最新进展使得大型多模态模型（LMMs）的发展成为可能，这些模型能够处理和生成不同类型的数据，例如文本、图像、音频和视频。
- en: LMMs share with “standard” Large Language Models (LLMs) the capability of generalization
    and adaptation typical of Large Foundation Models. However, LMMs are capable of
    processing data that goes beyond text, including images, audio, and video.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: LMMs与“标准”大型语言模型（LLMs）共享一般化和适应能力，这些能力是大型基础模型所特有的。然而，LMMs能够处理超越文本的数据，包括图像、音频和视频。
- en: One of the most prominent examples of large multimodal models is GPT4V(ision),
    the latest iteration of the Generative Pre-trained Transformer (GPT) family. GPT-4
    can perform various tasks that require both natural language understanding and
    computer vision, such as image captioning, visual question answering, text-to-image
    synthesis, and image-to-text translation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型多模态模型的一个最突出的例子是GPT4V(ision)，它是生成预训练变换器（GPT）系列的最新迭代。GPT-4可以执行各种需要自然语言理解和计算机视觉的任务，例如图像字幕生成、视觉问答、文本到图像合成以及图像到文本翻译。
- en: 'The GPT4V (along with its newer version, the GPT-4-turbo vision), has proved
    extraordinary capabilities, including:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: GPT4V（以及其更新版本GPT-4-turbo vision）展现了非凡的能力，包括：
- en: 'Mathematical reasoning over numerical problems:'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数值问题的数学推理：
- en: '![](../Images/22d6be0bb9790388f39c6d37bdf13ce6.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22d6be0bb9790388f39c6d37bdf13ce6.png)'
- en: Image by the Author
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'Generating code from sketches:'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从草图生成代码：
- en: '![](../Images/6316903035a05f865e00a2bb57a5cebf.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6316903035a05f865e00a2bb57a5cebf.png)'
- en: Image by the Author
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'OCR:'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OCR：
- en: '![](../Images/5a44d2077c76c2466a816925bc5fd34e.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a44d2077c76c2466a816925bc5fd34e.png)'
- en: Image by the Author
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'Description of artistic heritages:'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 艺术遗产的描述：
- en: '![](../Images/bc79aa0a1075216f7e5b904316adf525.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc79aa0a1075216f7e5b904316adf525.png)'
- en: Image by the Author
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: And many others.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 以及许多其他功能。
- en: In this article, we are going to focus on LMMs’ vision capabilities and how
    they differ from the standard Computer Vision algorithms.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将重点关注LMMs的视觉能力，以及它们如何与标准计算机视觉算法不同。
- en: What is Computer Vision
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是计算机视觉
- en: Computer Vision (CV) is a field of artificial intelligence (AI) that enables
    computers and systems to derive meaningful information from digital images, videos,
    and other visual inputs. It uses machine learning and neural networks to teach
    computers to see, observe, and understand. The goal is to mimic the human visual
    system, automating tasks that the human visual system can do. For example, computer
    vision can be used to recognize objects in an image, detect events, estimate 3D
    poses, and restore images.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉（CV）是人工智能（AI）领域的一部分，它使计算机和系统能够从数字图像、视频和其他视觉输入中获取有意义的信息。它使用机器学习和神经网络来教会计算机看、观察和理解。其目标是模仿人类视觉系统，自动化人类视觉系统能够完成的任务。例如，计算机视觉可以用于识别图像中的物体、检测事件、估计
    3D 姿势和修复图像。
- en: Since CV relies on neural networks, that are nothing but mathematical model,
    we need to convert images into numerical input, so that they can be processed
    by our models.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 由于计算机视觉依赖于神经网络，这些网络不过是数学模型，我们需要将图像转换为数值输入，以便它们可以被我们的模型处理。
- en: An image is a multi-dimensional array of pixels. For a grayscale image, this
    is a 2D array where each pixel corresponds to a different shade of gray. For a
    colored image, it’s a 3D array (height, width, and color channels), where each
    color channel (Red, Green, Blue — RGD) has a separate 2D array of pixel intensities
    in that color.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图像是像素的多维数组。对于灰度图像，这是一个 2D 数组，每个像素对应于不同的灰色阴影。对于彩色图像，它是一个 3D 数组（高度、宽度和颜色通道），每个颜色通道（红色、绿色、蓝色
    — RGD）都有一个单独的 2D 像素强度数组。
- en: Each pixel intensity is a numerical value. The most common scenario is that
    this value ranges from 0 (black) to 255 (white). The combination of these pixels
    makes up what we visually perceive as an image.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 每个像素强度是一个数值。最常见的情况是这个值范围从 0（黑色）到 255（白色）。这些像素的组合构成了我们视觉上感知到的图像。
- en: 'The following illustration shows an example of a RGD image (note: pixels’ values
    are meant for examples only and do not represent real values).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 以下插图展示了一个 RGD 图像的例子（注意：像素值仅作为示例，并不代表真实值）。
- en: '![](../Images/28bd1c135856c67ae69a1b0fa8ff4fd4.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/28bd1c135856c67ae69a1b0fa8ff4fd4.png)'
- en: Image by the Author
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Now, the question is how to pre-process these multi-dimensional array in such
    a way that the CV model is able to understand them and preserve as much information
    as possible.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是如何预处理这些多维数组，使得计算机视觉模型能够理解它们并尽可能保留信息。
- en: Before the emergence of multimodal models, computer vision used to rely on specialized
    models that were designed and trained for specific tasks, such as object detection,
    face recognition, scene segmentation, and optical character recognition. One of
    the most popular class of models in this field is that of **Convolutional Neural
    Networks**.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在多模态模型出现之前，计算机视觉通常依赖于为特定任务设计和训练的专用模型，如目标检测、人脸识别、场景分割和光学字符识别。在这个领域中，**卷积神经网络**是最受欢迎的模型之一。
- en: Convolutional Neural Networks
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: '[Convolutional Neural Networks](/deep-learning-for-image-recognition-convolutional-neural-network-with-tensorflow-de6349c31c07)
    are nothing but Neural Networks that exhibit, in at least one layer, the mathematical
    operation of **convolution**.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[卷积神经网络](/deep-learning-for-image-recognition-convolutional-neural-network-with-tensorflow-de6349c31c07)不过是展示了至少一层数学操作**卷积**的神经网络。'
- en: Convolution is an element-wise multiplication between two matrices (representing,
    respectively, a filter specialized in detecting specific features and an equally-sized
    region of the image being processed) with the final summation of the outputs.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积是两个矩阵之间的逐元素乘法（分别代表一个专门用于检测特定特征的滤波器和处理图像的同样大小的区域），并对输出结果进行最终的求和。
- en: '![](../Images/d5f4e3d259ced2dedb5443306808fe1f.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d5f4e3d259ced2dedb5443306808fe1f.png)'
- en: Image by the Author
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Let’s consider the following illustration as an example.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将以下插图作为一个例子。
- en: '![](../Images/b91a0a8886414ba5952e443d5bbdf6ba.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b91a0a8886414ba5952e443d5bbdf6ba.png)'
- en: Image by the Author
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'In the picture, I’ve examined an easy task: we have an alphabet of four letters
    — A, B, C and D — and our algorithm is asked to recognize our input letter (in
    our case, a ‘C’). The input image is passed through a filter specialized, for
    example, in corner detection, then reduced in dimensionality, flattened and processed
    through the fully-connected layers.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，我检查了一个简单的任务：我们有一个由四个字母组成的字母表——A、B、C 和 D——并且我们的算法被要求识别输入字母（在我们的例子中是‘C’）。输入图像通过一个专门的滤波器（例如，用于角点检测）进行处理，然后降维、展平，并通过全连接层处理。
- en: Visual Transformer
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视觉转换器
- en: A first alternative to CNN was introduced with Vision Transformer (original
    paper [here](https://arxiv.org/pdf/2010.11929.pdf)), that share with LLMs the
    core architecture (encoder/decoder).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Vision Transformer（原始论文[这里](https://arxiv.org/pdf/2010.11929.pdf)）作为 CNN 的第一个替代方案被提出，它与
    LLM 共享核心架构（编码器/解码器）。
- en: As per traditional transformers, also in this case the core mechanism is **Attention**
    (original paper [here](https://arxiv.org/abs/1706.03762)), that allows the model
    to selectively focus on different parts of the input sequence when making predictions.
    This concept is applied by teaching the model to focus on certain parts of the
    input data and disregard others to better solve the task at hand.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的转换器一样，在这种情况下核心机制也是**注意力**（原始论文[这里](https://arxiv.org/abs/1706.03762)），它允许模型在进行预测时选择性地关注输入序列的不同部分。这个概念通过教模型关注输入数据的某些部分并忽略其他部分，从而更好地解决当前任务。
- en: The **revolutionary aspect** of attention in Transformers is that it dispenses
    with recurrence (read more about Recurrent Neural Networks (RNNs) [here](https://medium.com/analytics-vidhya/recurrent-neural-networks-97f3b034e70))
    and convolutions, which were extensively relied upon by previous models. The Transformer
    is the first model relying entirely on attention to compute representations of
    its input and output without using sequence-aligned RNNs or convolution. This
    allows the Transformer to capture a wider range of relationships between the words
    in a sentence and learn a more nuanced representation of the input.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**变革性的**注意力机制在转换器中的一个方面是，它摆脱了递归（可以在[这里](https://medium.com/analytics-vidhya/recurrent-neural-networks-97f3b034e70)了解更多关于递归神经网络（RNN）的内容）和卷积，这些都是以前模型广泛依赖的。转换器是第一个完全依赖注意力来计算输入和输出表示的模型，而不使用序列对齐的
    RNN 或卷积。这使得转换器能够捕捉句子中单词之间更广泛的关系，并学习输入的更微妙的表示。'
- en: In Vision Transformers, images are processed differently than in CNNs. In fact,
    an image is divided into so-called patches, typically of the size of 16x16 pixles.
    Then, each patch is flattened into a 1D vector and tokenized, in the same way
    we do with text data in a standard transformer like GPT-3.5.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在视觉转换器中，图像的处理方式与 CNN 不同。实际上，一张图像被划分为所谓的补丁，通常为 16x16 像素大小。然后，每个补丁被展平为 1D 向量并进行标记，就像我们在标准转换器（如
    GPT-3.5）中处理文本数据一样。
- en: '![](../Images/e3477a3a624f8feb7af4b3d6fca0a9a1.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e3477a3a624f8feb7af4b3d6fca0a9a1.png)'
- en: Image by the Author
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: Now, these tokenized patches will be fed into the Vision Tranformer model and
    further converted into lower-dimensional vectors via a **linear projection layer,**
    so that we can work with less memory and computational powerwhile preserving information.
    Plus, as in standard transformer, each token is associated with an indicator about
    its position in the overall context of the image via a **positional embedding
    layer.**
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这些经过标记的补丁将被输入到视觉转换器模型中，并通过**线性投影层**进一步转换为低维向量，这样我们可以在保留信息的同时减少内存和计算能力的需求。此外，与标准转换器一样，每个标记都通过**位置嵌入层**与其在图像整体上下文中的位置指示器相关联。
- en: '![](../Images/7f96677230d6aa881aac2a5fd2742aec.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7f96677230d6aa881aac2a5fd2742aec.png)'
- en: Image by the Author
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: 'Finally, the positional-embedded tokens are passed into the main block of the
    model, that is the transformer encoder. Below you can see an illustration of a
    Vision Transformer (in this case, the scenario is a classification task):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，位置嵌入的标记被传递到模型的主要块，即转换器编码器。下面你可以看到一个视觉转换器的插图（在这个案例中，场景是分类任务）：
- en: '![](../Images/032f0d41f4c82441273a59836659205f.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/032f0d41f4c82441273a59836659205f.png)'
- en: 'Source: [https://arxiv.org/pdf/2010.11929v2.pdf](https://arxiv.org/pdf/2010.11929v2.pdf)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '来源: [https://arxiv.org/pdf/2010.11929v2.pdf](https://arxiv.org/pdf/2010.11929v2.pdf)'
- en: As you can see, the Transformer encoder exhibit the **attention mechanism**
    mentioned above.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，转换器编码器展示了上述**注意力机制**。
- en: Putting all together
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: The idea is that of projecting images’ embeddings in the same latent space as
    language, so that, given a human input made of image + text, the model is able
    to gather the relevant context from an embedding space covering both images and
    texts.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其思想是将图像的嵌入投影到与语言相同的潜在空间中，这样，当给定由图像和文本组成的人类输入时，模型能够从覆盖图像和文本的嵌入空间中获取相关的上下文。
- en: A first example of an image and text model was introduced by OpenAI with CLIP
    (Contrastive Language-Image Pretraining).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI引入的第一个图像和文本模型是CLIP（对比语言-图像预训练）。
- en: '**CLIP**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**CLIP**'
- en: '[CLIP](https://openai.com/research/clip) is a model developed by OpenAI that’s
    designed to understand images and text together. It’s like a bridge that connects
    the world of images and the world of words.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[CLIP](https://openai.com/research/clip)是OpenAI开发的一个模型，旨在同时理解图像和文本。它就像是连接图像世界和文字世界的桥梁。'
- en: Imagine you have a bunch of images and sentences, and you want to match each
    image with the sentence that best describes it. That’s essentially what CLIP does.
    It’s trained to understand which images and sentences are similar to each other.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下你有一堆图像和句子，你想将每张图像与最能描述它的句子匹配。这就是CLIP的核心功能。它被训练以理解哪些图像和句子彼此相似。
- en: The cool thing about CLIP is that it doesn’t need to be specifically trained
    on a certain task to do well at it. For example, if you have a new set of images
    and sentences that CLIP has never seen before (in a so-called zero-shot approach),
    it can still do a pretty good job at matching them up. This is because CLIP has
    learned a general understanding of images and text from a huge amount of data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP的酷炫之处在于它不需要专门针对某个任务进行训练就能表现良好。例如，如果你有一组CLIP从未见过的新图像和句子（即所谓的零样本方法），它仍然可以很好地将它们匹配起来。这是因为CLIP从大量数据中学习了对图像和文本的普遍理解。
- en: '![](../Images/1848d49317a2224a1b0f12faf4deaf8c.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1848d49317a2224a1b0f12faf4deaf8c.png)'
- en: 'Source: [CLIP: Connecting text and images (openai.com)](https://openai.com/research/clip)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '来源：[CLIP: 连接文本和图像 (openai.com)](https://openai.com/research/clip)'
- en: While CLIP was still a predictor model, the state-of-the-art LMMs are meant
    to interact with humans as AI assistant. In other words, they are instruct models.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管CLIP仍然是一个预测模型，但最先进的LMMs旨在作为AI助手与人类互动。换句话说，它们是指令模型。
- en: '**LLaVA**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**LLaVA**'
- en: A great example of an assistant LMM is the open-source model [LLaVA (Large Language
    and Vision Assistant)](https://llava-vl.github.io/), that combines the above mentioned
    CLIP for image encoding and the base LLM [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/),
    for instruction understanding.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的助手LMM的例子是开源模型[LLaVA (Large Language and Vision Assistant)](https://llava-vl.github.io/)，它结合了前面提到的CLIP用于图像编码和基础LMM
    [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/)，用于指令理解。
- en: The idea of LLaVA is that of having a linear layer to connect image features
    produced by CLIP into the word embedding space of the language model Vicuna.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: LLaVA的思想是拥有一个线性层，将CLIP生成的图像特征连接到语言模型Vicuna的词嵌入空间中。
- en: '![](../Images/52bcd962eecc01cc06d1b8d0575f4850.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52bcd962eecc01cc06d1b8d0575f4850.png)'
- en: 'Source: [2304.08485.pdf (arxiv.org)](https://arxiv.org/pdf/2304.08485.pdf)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[2304.08485.pdf (arxiv.org)](https://arxiv.org/pdf/2304.08485.pdf)
- en: To do so, researchers introduced a trainable projection matrix W that converts
    image features into embedding vectors of the same dimensions as word embeddings
    processed by the language model.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，研究人员引入了一个可训练的投影矩阵W，它将图像特征转换为与语言模型处理的词嵌入具有相同维度的嵌入向量。
- en: '**Kosmos-1**'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kosmos-1**'
- en: 'Another example is Kosmos-1, introduced by Microsoft Research in [this paper.](https://arxiv.org/pdf/2302.14045.pdf)
    The approach behind this model is that of having a transformer decoder that perceives
    general modalities in a unified way: inputs are flattened into 1D vectors of tokens
    and tagged with special start and end-of-sequences special tokens (texts as *<s>text</s>*,
    images as *<image>image</image>*). Once tokenized, inputs are encoded via embedding
    modules (specific per data format), then fed into the decoder.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是Kosmos-1，由微软研究院在[这篇论文](https://arxiv.org/pdf/2302.14045.pdf)中介绍。该模型的理念是使用一个变压器解码器以统一的方式感知通用模态：输入被展平为1D令牌向量，并用特殊的开始和结束序列标记（文本如*<s>text</s>*，图像如*<image>image</image>*）标记。一旦标记化，输入通过嵌入模块（特定于数据格式）进行编码，然后输入解码器。
- en: 'In their paper, authors describe the training process of the model, that included
    the following data corpora:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的论文中，作者描述了模型的训练过程，其中包括以下数据语料库：
- en: '**Monomodal, text corpora** →used for **representation learning** (that is,
    the ability to produce meaningful representation of language, keeping the underneath
    semantic structure) and general purpose language tasks, such as instructions-following,
    in-context learning and similar.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单模态文本语料库** → 用于**表示学习**（即，产生有意义的语言表示，保持其潜在语义结构）和一般语言任务，例如遵循指令、上下文学习等。'
- en: '**Image-caption pairs** →needed as the bridge for the model to link images
    with their description'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像-文本对** → 作为模型将图像与其描述关联的桥梁'
- en: '**Interleaved Image-text data** →needed to further align the perception of
    general modalities with language models and improve model’s few-shot ability.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交错的图像-文本数据** → 需要进一步将一般模态的感知与语言模型对齐，并提高模型的少样本能力。'
- en: 'The below picture shows some examples of Kosmos-1 capabilities:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图片展示了一些Kosmos-1的能力示例：
- en: '![](../Images/99296ea927285d89586122a946bf1507.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99296ea927285d89586122a946bf1507.png)'
- en: 'Source: [2302.14045.pdf (arxiv.org)](https://arxiv.org/pdf/2302.14045.pdf)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '来源: [2302.14045.pdf (arxiv.org)](https://arxiv.org/pdf/2302.14045.pdf)'
- en: Overall, the trend of creating a common embedding space for both images and
    words, so that the model can “perceive” both the data formats — texts and images
    — is paving the way to powerful Large Multimodal Models. Plus, pre-trained models
    such as CLIP are being widely used to produce image representations, that can
    be then further converted into the word embedding space.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，创建一个图像和词汇的共同嵌入空间的趋势，使模型能够“感知”这两种数据格式——文本和图像——正在为强大的大型多模态模型铺平道路。此外，像CLIP这样的预训练模型被广泛用于生成图像表示，然后可以进一步转换为词嵌入空间。
- en: Beyond vision
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超越视觉
- en: Generally speaking, the main idea behind multimodal models is to create consistent
    representations of a given concept across different modalities. In the previous
    section, we saw how we can use a Vision Transformer to embed images into lower-dimensional
    vectors in a latent space. Similarly, we could create encoders for each modality
    and using an objective function that encourages the models to produce similar
    embeddings for similar data pairs.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，多模态模型的主要思想是创建不同模态中给定概念的一致表示。在前一部分中，我们看到如何使用视觉变换器将图像嵌入到潜在空间中的低维向量中。同样，我们可以为每种模态创建编码器，并使用目标函数来鼓励模型为相似的数据对生成相似的嵌入。
- en: For example, let’s consider the model [MACAW-LLM](https://github.com/lyuchenyang/Macaw-LLM),
    a multi-modal language model that is able to process images, video, audio, and
    text data, built upon the foundations of CLIP (images and video), [Whisper](https://openai.com/research/whisper)
    (audio), and [LLaMA](https://ai.meta.com/blog/large-language-model-llama-meta-ai/)
    (text).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，我们可以考虑模型 [MACAW-LLM](https://github.com/lyuchenyang/Macaw-LLM)，这是一个多模态语言模型，能够处理图像、视频、音频和文本数据，基于CLIP（图像和视频）、[Whisper](https://openai.com/research/whisper)（音频）和[LLaMA](https://ai.meta.com/blog/large-language-model-llama-meta-ai/)（文本）的基础构建。
- en: '![](../Images/5da82d88f802560230b4eaf262d358c6.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5da82d88f802560230b4eaf262d358c6.png)'
- en: 'Source: [lyuchenyang/Macaw-LLM: Macaw-LLM: Multi-Modal Language Modeling with
    Image, Video, Audio, and Text Integration (github.com)](https://github.com/lyuchenyang/Macaw-LLM)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '来源: [lyuchenyang/Macaw-LLM: Macaw-LLM: 多模态语言建模，集成图像、视频、音频和文本 (github.com)](https://github.com/lyuchenyang/Macaw-LLM)'
- en: As you can see, different embedding modules are used to produce a “shared” embedding
    space, which is aligned with the word embedding space used by the LLM (the Meta
    AI’s LLaMA in this case).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，不同的嵌入模块被用来生成“共享”的嵌入空间，该空间与LLM使用的词嵌入空间（在本例中为Meta AI的LLaMA）对齐。
- en: Conclusions
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: Multi-modality is paving the way towards a new waves of applications and use
    cases. It is also a further milestone towards the concept of [Artificial General
    Intelligence (AGI)](https://openai.com/blog/planning-for-agi-and-beyond), since
    it is making AI systems more and more prone to “perceive” as humans do.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态正在为新一波应用和用例铺平道路。这也是实现[人工通用智能（AGI）](https://openai.com/blog/planning-for-agi-and-beyond)概念的进一步里程碑，因为它使AI系统越来越容易像人类一样“感知”。
- en: 'Needless to say, Large Multimodal Models come with even greater responsabilities
    in terms of ethical considerations: biases, discriminations, privacy violations
    and many others risks are at the core of LMMs’ researches, and now more than even
    [Human Alignment](https://openai.com/blog/introducing-superalignment) is a top
    priority while developing AI systems.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 不用说，大型多模态模型在伦理考量方面带来了更大的责任：偏见、歧视、隐私侵犯以及许多其他风险都是LMM研究的核心，现在比以往任何时候都更加需要关注[人类对齐](https://openai.com/blog/introducing-superalignment)。
- en: References
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[https://arxiv.org/pdf/2010.11929.pdf](https://arxiv.org/pdf/2010.11929.pdf)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/2010.11929.pdf](https://arxiv.org/pdf/2010.11929.pdf)'
- en: '[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
- en: '[https://medium.com/analytics-vidhya/recurrent-neural-networks-97f3b034e70](https://medium.com/analytics-vidhya/recurrent-neural-networks-97f3b034e70)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://medium.com/analytics-vidhya/recurrent-neural-networks-97f3b034e70](https://medium.com/analytics-vidhya/recurrent-neural-networks-97f3b034e70)'
- en: '[lyuchenyang/Macaw-LLM: Macaw-LLM: Multi-Modal Language Modeling with Image,
    Video, Audio, and Text Integration (github.com)](https://github.com/lyuchenyang/Macaw-LLM)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[lyuchenyang/Macaw-LLM：Macaw-LLM：图像、视频、音频和文本整合的多模态语言建模 (github.com)](https://github.com/lyuchenyang/Macaw-LLM)'
- en: '[2302.14045.pdf (arxiv.org)](https://arxiv.org/pdf/2302.14045.pdf)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2302.14045.pdf (arxiv.org)](https://arxiv.org/pdf/2302.14045.pdf)'
- en: '[CLIP: Connecting text and images (openai.com)](https://openai.com/research/clip)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CLIP：连接文本和图像 (openai.com)](https://openai.com/research/clip)'
- en: '[https://ai.meta.com/blog/large-language-model-llama-meta-ai/](https://ai.meta.com/blog/large-language-model-llama-meta-ai/)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://ai.meta.com/blog/large-language-model-llama-meta-ai/](https://ai.meta.com/blog/large-language-model-llama-meta-ai/)'
- en: '[https://openai.com/blog/introducing-superalignment](https://openai.com/blog/introducing-superalignment)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://openai.com/blog/introducing-superalignment](https://openai.com/blog/introducing-superalignment)'
- en: '[https://openai.com/blog/our-approach-to-alignment-research](https://openai.com/blog/our-approach-to-alignment-research)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://openai.com/blog/our-approach-to-alignment-research](https://openai.com/blog/our-approach-to-alignment-research)'
