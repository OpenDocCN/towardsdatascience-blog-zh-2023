- en: Build your own Transformer from scratch using Pytorch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零开始使用Pytorch构建自己的Transformer
- en: 原文：[https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb?source=collection_archive---------0-----------------------#2023-04-26](https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb?source=collection_archive---------0-----------------------#2023-04-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb?source=collection_archive---------0-----------------------#2023-04-26](https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb?source=collection_archive---------0-----------------------#2023-04-26)
- en: Building a Transformer model step by step in Pytorch
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Pytorch中逐步构建一个Transformer模型
- en: '[](https://arjun-sarkar786.medium.com/?source=post_page-----84c850470dcb--------------------------------)[![Arjun
    Sarkar](../Images/de141f1ab68c2b85c9d7a1f31be0d9b5.png)](https://arjun-sarkar786.medium.com/?source=post_page-----84c850470dcb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----84c850470dcb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----84c850470dcb--------------------------------)
    [Arjun Sarkar](https://arjun-sarkar786.medium.com/?source=post_page-----84c850470dcb--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://arjun-sarkar786.medium.com/?source=post_page-----84c850470dcb--------------------------------)[![Arjun
    Sarkar](../Images/de141f1ab68c2b85c9d7a1f31be0d9b5.png)](https://arjun-sarkar786.medium.com/?source=post_page-----84c850470dcb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----84c850470dcb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----84c850470dcb--------------------------------)
    [Arjun Sarkar](https://arjun-sarkar786.medium.com/?source=post_page-----84c850470dcb--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffa31366b2eda&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-your-own-transformer-from-scratch-using-pytorch-84c850470dcb&user=Arjun+Sarkar&userId=fa31366b2eda&source=post_page-fa31366b2eda----84c850470dcb---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----84c850470dcb--------------------------------)
    ·7 min read·Apr 26, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F84c850470dcb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-your-own-transformer-from-scratch-using-pytorch-84c850470dcb&user=Arjun+Sarkar&userId=fa31366b2eda&source=-----84c850470dcb---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffa31366b2eda&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-your-own-transformer-from-scratch-using-pytorch-84c850470dcb&user=Arjun+Sarkar&userId=fa31366b2eda&source=post_page-fa31366b2eda----84c850470dcb---------------------post_header-----------)
    发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----84c850470dcb--------------------------------)
    ·7分钟阅读·2023年4月26日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F84c850470dcb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-your-own-transformer-from-scratch-using-pytorch-84c850470dcb&user=Arjun+Sarkar&userId=fa31366b2eda&source=-----84c850470dcb---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F84c850470dcb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-your-own-transformer-from-scratch-using-pytorch-84c850470dcb&source=-----84c850470dcb---------------------bookmark_footer-----------)![](../Images/3f54c2f78af8d6ca5052e7b2b898034a.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F84c850470dcb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-your-own-transformer-from-scratch-using-pytorch-84c850470dcb&source=-----84c850470dcb---------------------bookmark_footer-----------)![](../Images/3f54c2f78af8d6ca5052e7b2b898034a.png)'
- en: Figure 1\. Photo by [Kevin Ku](https://unsplash.com/@ikukevk?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/deep-learning?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 照片由[Kevin Ku](https://unsplash.com/@ikukevk?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)拍摄，来自[Unsplash](https://unsplash.com/s/photos/deep-learning?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: In this tutorial, we will build a basic Transformer model from scratch using
    PyTorch. The Transformer model, introduced by Vaswani et al. in the paper “Attention
    is All You Need,” is a deep learning architecture designed for sequence-to-sequence
    tasks, such as machine translation and text summarization. It is based on self-attention
    mechanisms and has become the foundation for many state-of-the-art natural language
    processing models, like GPT and BERT.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将使用 PyTorch 从头开始构建一个基本的 Transformer 模型。Transformer 模型由 Vaswani 等人提出，在论文“Attention
    is All You Need”中介绍，是一种针对序列到序列任务（如机器翻译和文本摘要）设计的深度学习架构。它基于自注意力机制，已成为许多最先进自然语言处理模型的基础，如
    GPT 和 BERT。
- en: 'To understand Transformer models in detail kindly visit these two articles:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 要详细了解 Transformer 模型，请访问这两篇文章：
- en: '[1\. All you need to know about ‘Attention’ and ‘Transformers’ — In-depth Understanding
    — Part 1](https://medium.com/towards-data-science/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021)'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[1\. 关于“注意力”和“Transformer”的一切——深入理解——第 1 部分](https://medium.com/towards-data-science/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021)'
- en: '[2\. All you need to know about ‘Attention’ and ‘Transformers’ — In-depth Understanding
    — Part 2](https://medium.com/towards-data-science/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-2-bf2403804ada)'
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[2\. 关于“注意力”和“Transformer”的一切——深入理解——第 2 部分](https://medium.com/towards-data-science/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-2-bf2403804ada)'
- en: 'To build our Transformer model, we’ll follow these steps:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建我们的 Transformer 模型，我们将遵循以下步骤：
- en: Import necessary libraries and modules
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的库和模块
- en: 'Define the basic building blocks: Multi-Head Attention, Position-wise Feed-Forward
    Networks, Positional Encoding'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义基本构建模块：多头注意力、位置逐层前馈网络、位置编码
- en: Build the Encoder and Decoder layers
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建编码器和解码器层
- en: Combine Encoder and Decoder layers to create the complete Transformer model
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结合编码器和解码器层以创建完整的 Transformer 模型
- en: Prepare sample data
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备示例数据
- en: Train the model
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型
- en: Let’s start by importing the necessary libraries and modules.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先导入必要的库和模块。
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now, we’ll define the basic building blocks of the Transformer model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将定义 Transformer 模型的基本构建块。
- en: Multi-Head Attention
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多头注意力
- en: '![](../Images/e8b7aaf2849d3b0b0b0bd006f7dae3aa.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e8b7aaf2849d3b0b0b0bd006f7dae3aa.png)'
- en: 'Figure 2\. Multi-Head Attention (source: image created by author)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 多头注意力（来源：作者创建的图像）
- en: The Multi-Head Attention mechanism computes the attention between each pair
    of positions in a sequence. It consists of multiple “attention heads” that capture
    different aspects of the input sequence.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力机制计算序列中每对位置之间的注意力。它由多个“注意力头”组成，这些头捕捉输入序列的不同方面。
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The MultiHeadAttention code initializes the module with input parameters and
    linear transformation layers. It calculates attention scores, reshapes the input
    tensor into multiple heads, and combines the attention outputs from all heads.
    The forward method computes the multi-head self-attention, allowing the model
    to focus on some different aspects of the input sequence.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: MultiHeadAttention 代码用输入参数和线性变换层初始化模块。它计算注意力得分，将输入张量重塑为多个头，并结合所有头的注意力输出。前向方法计算多头自注意力，使模型能够关注输入序列的不同方面。
- en: Position-wise Feed-Forward Networks
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 位置逐层前馈网络
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The PositionWiseFeedForward class extends PyTorch’s nn.Module and implements
    a position-wise feed-forward network. The class initializes with two linear transformation
    layers and a ReLU activation function. The forward method applies these transformations
    and activation function sequentially to compute the output. This process enables
    the model to consider the position of input elements while making predictions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: PositionWiseFeedForward 类扩展了 PyTorch 的 nn.Module，并实现了位置逐层前馈网络。该类初始化时包含两个线性变换层和一个
    ReLU 激活函数。前向方法依次应用这些变换和激活函数以计算输出。这个过程使模型在做出预测时能够考虑输入元素的位置。
- en: Positional Encoding
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 位置编码
- en: Positional Encoding is used to inject the position information of each token
    in the input sequence. It uses sine and cosine functions of different frequencies
    to generate the positional encoding.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码用于注入输入序列中每个标记的位置信息。它使用不同频率的正弦和余弦函数来生成位置编码。
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The PositionalEncoding class initializes with input parameters d_model and max_seq_length,
    creating a tensor to store positional encoding values. The class calculates sine
    and cosine values for even and odd indices, respectively, based on the scaling
    factor div_term. The forward method computes the positional encoding by adding
    the stored positional encoding values to the input tensor, allowing the model
    to capture the position information of the input sequence.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: PositionalEncoding 类初始化时包含输入参数 d_model 和 max_seq_length，创建一个张量来存储位置编码值。该类根据缩放因子
    div_term 计算偶数和奇数索引的正弦和余弦值。forward 方法通过将存储的位置信息编码值添加到输入张量中来计算位置编码，从而使模型能够捕捉输入序列的位置信息。
- en: Now, we’ll build the Encoder and Decoder layers.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将构建编码器和解码器层。
- en: Encoder Layer
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码器层
- en: '![](../Images/9e8ab3e83decd7c9ea8305b7563b5cb0.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e8ab3e83decd7c9ea8305b7563b5cb0.png)'
- en: 'Figure 3\. The Encoder part of the transformer network (Source: image from
    the original paper)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. Transformer 网络的编码器部分（来源：原始论文中的图像）
- en: An Encoder layer consists of a Multi-Head Attention layer, a Position-wise Feed-Forward
    layer, and two Layer Normalization layers.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器层由一个多头注意力层、一个逐位置前馈层和两个层归一化层组成。
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The EncoderLayer class initializes with input parameters and components, including
    a MultiHeadAttention module, a PositionWiseFeedForward module, two layer normalization
    modules, and a dropout layer. The forward methods computes the encoder layer output
    by applying self-attention, adding the attention output to the input tensor, and
    normalizing the result. Then, it computes the position-wise feed-forward output,
    combines it with the normalized self-attention output, and normalizes the final
    result before returning the processed tensor.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: EncoderLayer 类初始化时包含输入参数和组件，包括一个多头注意力模块、一个逐位置前馈模块、两个层归一化模块和一个丢弃层。forward 方法通过应用自注意力、将注意力输出添加到输入张量并归一化结果来计算编码器层输出。然后，它计算逐位置前馈输出，将其与归一化的自注意力输出结合，并在返回处理后的张量之前归一化最终结果。
- en: Decoder Layer
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解码器层
- en: '![](../Images/28187d88b42ade5225fcc50d80e20c31.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/28187d88b42ade5225fcc50d80e20c31.png)'
- en: 'Figure 4\. The Decoder part of the Transformer network (Souce: Image from the
    original paper)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. Transformer 网络的解码器部分（来源：原始论文中的图像）
- en: A Decoder layer consists of two Multi-Head Attention layers, a Position-wise
    Feed-Forward layer, and three Layer Normalization layers.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器层由两个多头注意力层、一个逐位置前馈层和三个层归一化层组成。
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The DecoderLayer initializes with input parameters and components such as MultiHeadAttention
    modules for masked self-attention and cross-attention, a PositionWiseFeedForward
    module, three layer normalization modules, and a dropout layer.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器层初始化时包含输入参数和组件，如用于掩蔽自注意力和交叉注意力的多头注意力模块、一个逐位置前馈模块、三个层归一化模块和一个丢弃层。
- en: 'The forward method computes the decoder layer output by performing the following
    steps:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: forward 方法通过执行以下步骤计算解码器层输出：
- en: Calculate the masked self-attention output and add it to the input tensor, followed
    by dropout and layer normalization.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算掩蔽自注意力输出并将其添加到输入张量中，然后进行丢弃和层归一化。
- en: Compute the cross-attention output between the decoder and encoder outputs,
    and add it to the normalized masked self-attention output, followed by dropout
    and layer normalization.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算解码器和编码器输出之间的交叉注意力输出，并将其添加到归一化的掩蔽自注意力输出中，然后进行丢弃和层归一化。
- en: Calculate the position-wise feed-forward output and combine it with the normalized
    cross-attention output, followed by dropout and layer normalization.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算逐位置前馈输出并将其与归一化的交叉注意力输出结合，然后进行丢弃和层归一化。
- en: Return the processed tensor.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回处理后的张量。
- en: These operations enable the decoder to generate target sequences based on the
    input and the encoder output.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作使解码器能够根据输入和编码器输出生成目标序列。
- en: Now, let’s combine the Encoder and Decoder layers to create the complete Transformer
    model.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将编码器和解码器层结合起来，创建完整的 Transformer 模型。
- en: Transformer Model
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer 模型
- en: '![](../Images/c141dc670d42086d53956ee42bada0be.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c141dc670d42086d53956ee42bada0be.png)'
- en: 'Figure 5\. The Transformer Network (Source: Image from the original paper)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. Transformer 网络（来源：原始论文中的图像）
- en: 'Merging it all together:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有内容合并在一起：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The Transformer class combines the previously defined modules to create a complete
    Transformer model. During initialization, the Transformer module sets up input
    parameters and initializes various components, including embedding layers for
    source and target sequences, a PositionalEncoding module, EncoderLayer and DecoderLayer
    modules to create stacked layers, a linear layer for projecting decoder output,
    and a dropout layer.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 类结合之前定义的模块，创建一个完整的 Transformer 模型。在初始化期间，Transformer 模块设置输入参数并初始化各种组件，包括源序列和目标序列的嵌入层、位置编码模块、EncoderLayer
    和 DecoderLayer 模块以创建堆叠层、用于投影解码器输出的线性层以及 dropout 层。
- en: 'The generate_mask method creates binary masks for source and target sequences
    to ignore padding tokens and prevent the decoder from attending to future tokens.
    The forward method computes the Transformer model’s output through the following
    steps:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`generate_mask` 方法创建源序列和目标序列的二进制掩码，以忽略填充标记，并防止解码器关注未来的标记。`forward` 方法通过以下步骤计算
    Transformer 模型的输出：'
- en: Generate source and target masks using the generate_mask method.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `generate_mask` 方法生成源序列和目标序列的掩码。
- en: Compute source and target embeddings, and apply positional encoding and dropout.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算源序列和目标序列的嵌入，并应用位置编码和 dropout。
- en: Process the source sequence through encoder layers, updating the enc_output
    tensor.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过编码器层处理源序列，更新 `enc_output` 张量。
- en: Process the target sequence through decoder layers, using enc_output and masks,
    and updating the dec_output tensor.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过解码器层处理目标序列，使用 `enc_output` 和掩码，并更新 `dec_output` 张量。
- en: Apply the linear projection layer to the decoder output, obtaining output logits.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将线性投影层应用于解码器输出，获取输出 logits。
- en: These steps enable the Transformer model to process input sequences and generate
    output sequences based on the combined functionality of its components.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤使 Transformer 模型能够处理输入序列，并基于其组件的组合功能生成输出序列。
- en: Preparing Sample Data
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备示例数据
- en: In this example, we will create a toy dataset for demonstration purposes. In
    practice, you would use a larger dataset, preprocess the text, and create vocabulary
    mappings for source and target languages.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将创建一个玩具数据集用于演示。实际操作中，你会使用更大的数据集，预处理文本，并为源语言和目标语言创建词汇映射。
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Training the Model
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: Now we’ll train the model using the sample data. In practice, you would use
    a larger dataset and split it into training and validation sets.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用示例数据训练模型。在实际操作中，你会使用更大的数据集，并将其拆分为训练集和验证集。
- en: '[PRE8]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We can use this way to build a simple Transformer from scratch in Pytorch. All
    Large Language Models use these Transformer encoder or decoder blocks for training.
    Hence understanding the network that started it all is extremely important. Hope
    this article helps all looking to deep dive into LLM’s.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过这种方式从零开始在 Pytorch 中构建一个简单的 Transformer。所有的大型语言模型都使用这些 Transformer 编码器或解码器模块进行训练。因此，了解最初的网络极为重要。希望这篇文章能帮助所有希望深入了解
    LLM 的人。
- en: References
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Attention is all you need
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《Attention is all you need》
- en: '[A. Vaswani](https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/0),
    [N. Shazeer](https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/1),
    [N. Parmar](https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/2),
    [J. Uszkoreit](https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/3),
    [L. Jones](https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/4),
    [A. Gomez](https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/5),
    [{. Kaiser](https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/6),
    and [I. Polosukhin](https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/7).
    *Advances in Neural Information Processing Systems , page 5998–6008\.* (*2017*)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[A. Vaswani](https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/0)、[N.
    Shazeer](https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/1)、[N.
    Parmar](https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/2)、[J.
    Uszkoreit](https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/3)、[L.
    Jones](https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/4)、[A.
    Gomez](https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/5)、[{.
    Kaiser](https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/6)
    和 [I. Polosukhin](https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/7)。*《神经信息处理系统的进展》,
    第 5998–6008 页\.* (*2017*)'
