- en: Generative Models and the Dance of Noise and Structure
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成模型与噪声和结构的舞蹈
- en: 原文：[https://towardsdatascience.com/generative-models-and-the-dance-of-noise-and-structure-e72fe7494f4f](https://towardsdatascience.com/generative-models-and-the-dance-of-noise-and-structure-e72fe7494f4f)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/generative-models-and-the-dance-of-noise-and-structure-e72fe7494f4f](https://towardsdatascience.com/generative-models-and-the-dance-of-noise-and-structure-e72fe7494f4f)
- en: A Guide To Building Digital Dreamers
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数字梦想者构建指南
- en: '[](https://manuel-brenner.medium.com/?source=post_page-----e72fe7494f4f--------------------------------)[![Manuel
    Brenner](../Images/f62843c79a9b378494cb83caf3ddc792.png)](https://manuel-brenner.medium.com/?source=post_page-----e72fe7494f4f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e72fe7494f4f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e72fe7494f4f--------------------------------)
    [Manuel Brenner](https://manuel-brenner.medium.com/?source=post_page-----e72fe7494f4f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://manuel-brenner.medium.com/?source=post_page-----e72fe7494f4f--------------------------------)[![Manuel
    Brenner](../Images/f62843c79a9b378494cb83caf3ddc792.png)](https://manuel-brenner.medium.com/?source=post_page-----e72fe7494f4f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e72fe7494f4f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e72fe7494f4f--------------------------------)
    [Manuel Brenner](https://manuel-brenner.medium.com/?source=post_page-----e72fe7494f4f--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e72fe7494f4f--------------------------------)
    ·21 min read·Oct 7, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e72fe7494f4f--------------------------------)
    ·阅读时间 21 分钟·2023 年 10 月 7 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: I like to think about what the inhabitants of Renaissance Italy, aflame with
    passion for the possibilities of the human imagination and rationality, would
    have found most astounding about our present-day technology. [Leonardo da Vinci](https://medium.datadriveninvestor.com/there-is-no-clash-between-art-and-science-3028d0420fbe),
    dreaming of flying machines, would have surely been impressed by an Airbus 380
    soaring through the air, with passengers comfortably reclining in their chairs,
    watching movies, and complaining about the Wi-Fi not being fast enough.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢想象一下，文艺复兴时期意大利的居民，充满了对人类想象力和理性可能性的热情，会对我们今天的技术感到最为震惊的是什么。[莱昂纳多·达·芬奇](https://medium.datadriveninvestor.com/there-is-no-clash-between-art-and-science-3028d0420fbe)，曾梦想飞行器，肯定会对一架空中飞翔的空客
    A380 印象深刻，乘客们舒适地坐在座椅上，看电影，抱怨 Wi-Fi 不够快。
- en: 'But out of all the technologies that would have seemed like witchcraft in medieval
    times, the wonders of generative AI might be among the most witchcrafty. What
    would Leonardo, after laboring endless years on the portrait of the Mona Lisa,
    have said if I showed him a device that could paint a portrait of a woman in his
    style in mere seconds? Lo and behold:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有在中世纪看来如同巫术的技术中，生成式 AI 的奇迹可能是最像巫术的。如果我展示给莱昂纳多·达·芬奇一台能够在几秒钟内以他的风格绘制女性肖像的设备，他在劳作了无数年之后对《蒙娜丽莎》肖像的反应会是什么？瞧：
- en: '![](../Images/f16d0b3e7077c2029a697d822ccbdf4e.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f16d0b3e7077c2029a697d822ccbdf4e.png)'
- en: Portrait of a woman in the style of Leonardo da Vinci, painted by DALL-E.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 由 DALL-E 绘制的达·芬奇风格的女性肖像。
- en: 'While admittedly, this woman does not smile quite as seductively and mysteriously
    as the real Mona Lisa (and looks, upon further inspection, somewhat ridiculous),
    many of us have encountered astonishing instances of AI generations: from [ultrarealistic
    images](https://twitter.com/mariswaran/status/1674505829456965633) to eerily convincing
    deep fakes of voices or even entire essays written by AI.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然诚然，这位女性并没有像真正的蒙娜丽莎那样以诱人而神秘的微笑来展现自己（且经细致审视显得有些滑稽），但我们许多人已经遇到了令人惊叹的 AI 生成实例：从
    [超现实的图像](https://twitter.com/mariswaran/status/1674505829456965633)到令人毛骨悚然的深度伪造的声音，甚至是由
    AI 编写的完整文章。
- en: 'Generative AI models are the silicon equivalents of dreamers: they can envision
    something from nothing, **make meaning from noise**. They have learned to dance
    the dance of order and disorder. [They have already changed how we think about
    human creativity](/alphazero-and-the-beauty-of-the-artificial-mind-4ac7f220941a)
    and have opened the door to thousands of new applications, threatening entire
    industries and creating new ones.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 生成性AI模型是梦想家的硅基等效体：它们能够从无到有地构想事物，**从噪声中赋予意义**。它们学会了在秩序与无序之间起舞。[它们已经改变了我们对人类创造力的看法](/alphazero-and-the-beauty-of-the-artificial-mind-4ac7f220941a)，并开启了数千种新应用的门扉，威胁到整个行业并创造了新的行业。
- en: And we are just getting started, with most of these models still in their infancy.
    With the writings of ChatGPT, the images of DALL-E and Midjourney, and, most recently,
    generative models for music like Stability AI’s [StableAudio](https://stability.ai/stable-audio),
    we are looking at an era where an **increasing amount of the sensory signals we
    load into our brains on a daily basis are in some way altered or even fully generated
    by AI**.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚起步，这些模型中的大多数仍处于初期阶段。通过ChatGPT的写作、DALL-E和Midjourney生成的图像，以及最近像Stability AI的[StableAudio](https://stability.ai/stable-audio)这样的音乐生成模型，我们正迎来一个时代，在这个时代，**我们每天输入大脑的感官信号越来越多地被AI以某种方式改变甚至完全生成**。
- en: '![](../Images/42861a27a943782ad8b169d1a2d4d26a.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/42861a27a943782ad8b169d1a2d4d26a.png)'
- en: “A painter at an easel, with splashes of chaotic noise on the left side of the
    canvas, gradually transforming into a structured, beautiful digital city on the
    right. Art style should be semi-realistic with a hint of surrealism. The lighting
    should be soft and diffused, creating a dream-like ambiance”. Prompt by Chat-GPT,
    painting by DALL-E.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: “一个画家在画布的左侧是混乱噪声的溅射，逐渐转变为右侧的结构化、美丽的数字城市。艺术风格应为半现实主义，带有一点超现实主义。光线应柔和而弥漫，营造出梦幻般的氛围。”
    由Chat-GPT提供的提示，由DALL-E绘制的画作。
- en: In this article, I want to lift the lid off this magical black box, diving into
    the fundamental mechanisms of several classes of generative models (Helmholtz
    machines, Variational Autoencoders, Normalizing Flows, Diffusion Models, GANs
    and Transformer-based language models), shedding light on their inner workings,
    and exploring their origins in and connections to neuroscience and cognition.
    The topic is obviously too broad to be covered in one article (even though it
    became much longer than planned), so I tried to balance some technical details
    with a high-level overview, a coherent narrative and sources for further reading,
    so I hope that there is something in there for everybody.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我想揭开这个神奇黑箱的面纱，深入探讨几类生成模型（Helmholtz机、变分自编码器、归一化流、扩散模型、GAN和基于Transformer的语言模型）的基本机制，揭示它们的内部工作原理，并探讨它们在神经科学和认知学中的起源和联系。这个话题显然太广泛，无法在一篇文章中涵盖（尽管文章比预期的要长得多），所以我试图在一些技术细节、高层次概述、连贯的叙述和进一步阅读的来源之间找到平衡，希望每个人都能从中找到一些有用的信息。
- en: “What I cannot create, I do not understand.”
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我无法创造的，我不能理解。”
- en: '***— Richard Feynman***'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***— 理查德·费曼***'
- en: Where do we begin?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从哪里开始？
- en: 'It’s somewhat cliché to start with this frequently cited Feynman quote, but
    the man had a point: understanding is related to the act of creation, and so already
    in the early days of machine learning, building models that could understand became
    related to building models that could create. Turing’s famous test (also known
    as The Imitation Game) can be seen as a variant of this idea: if you convincingly
    manage to fake intelligence, you most likely have discovered something akin to
    the real thing.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以这句常被引用的费曼名言开头有些陈词滥调，但这位大师确实有他的道理：理解与创造行为相关，因此在机器学习的早期，构建能够理解的模型也就与构建能够创造的模型相关。图灵的著名测试（也称为模仿游戏）可以看作是这个观点的一种变体：如果你成功地伪造了智能，那么你很可能发现了类似真实智能的东西。
- en: Among the two most important early generative models are Boltzmann and Helmholtz
    machines.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期生成模型中，两个最重要的模型是Boltzmann机和Helmholtz机。
- en: 'Helmholtz machines are particularly interesting, given their principles are
    tied to the incredibly prescient vision of German physicist Hermann von Helmholtz.
    Helmholtz realized at the end of the 19th century that perception is much better
    described as a process of unconscious inference from sensory data and prior knowledge,
    rather than an objective reflection of an objective reality: cognition is inherently
    probabilistic and influenced by noise, and strongly shaped by our expectations
    and biases. His ideas are increasingly relevant in modern neuroscience, i.e. via
    Karl Friston’s Free Energy Principle (who explicitly cites the Helmholtz machine
    as a source of inspiration) and the [Bayesian Brain Hypothesis](/the-bayesian-brain-hypothesis-35b98847d331).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 赫尔姆霍兹机器特别有趣，因为其原理与德国物理学家赫尔曼·冯·赫尔姆霍兹的极具先见之明的视角紧密相关。赫尔姆霍兹在19世纪末意识到，知觉更好地描述为一种从感官数据和先验知识中进行无意识推理的过程，而不是对客观现实的客观反映：认知本质上是概率性的，并受到噪声的影响，并且受到我们的期望和偏见的强烈塑造。他的观点在现代神经科学中越来越相关，例如通过卡尔·弗里斯顿的自由能原理（他明确引用了赫尔姆霍兹机器作为灵感来源）和[贝叶斯大脑假说](/the-bayesian-brain-hypothesis-35b98847d331)。
- en: '![](../Images/7a38ec01e533c47c6e09f3de82154422.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a38ec01e533c47c6e09f3de82154422.png)'
- en: According to the free energy principle, the brain interacts with the external
    world in an action-perception loop, trying to infer the hidden states of the world
    from its sensation and making its predictions come true through actions. Kfriston,
    CC BY-SA 3.0 <[https://creativecommons.org/licenses/by-sa/3.0](https://creativecommons.org/licenses/by-sa/3.0)>,
    via Wikimedia Commons
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 根据自由能原理，大脑与外部世界在一个行动-感知循环中互动，尝试从其感觉中推断世界的隐藏状态，并通过行动使其预测成真。Kfriston, CC BY-SA
    3.0 <[https://creativecommons.org/licenses/by-sa/3.0](https://creativecommons.org/licenses/by-sa/3.0)>,
    通过维基媒体公用领域
- en: From a Bayesian perspective, the idea is that the brain maintains a generative
    model **p(x,z)** of the world, where **x** are sensory observations and **z**
    are hidden causes/latent explanations of those sensory observations that the brain
    tries to capture, reflecting uncertainty both in the world and in the model of
    the world. As we will see, many, but not all, generative models are formulated
    as probabilistic latent variable models.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从贝叶斯的角度来看，这个想法是大脑维持一个世界的生成模型**p(x,z)**，其中**x**是感官观察结果，**z**是这些感官观察结果的隐藏原因/潜在解释，大脑试图捕捉这些解释，反映了世界和世界模型中的不确定性。正如我们将看到的，许多生成模型，但不是所有，都被表述为概率性潜变量模型。
- en: In the Bayesian language, given such a model, this boils down to the prior distribution
    over latent causes **p(z)** (my prior expectation of observing a lion is smaller
    than observing a dog if I live in NYC), the the overall likelihood of the observations
    **p(x)**, and the relationship between the sensory observations and hidden causes.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯语言中，给定这样的模型，这归结为潜在原因的先验分布**p(z)**（如果我住在纽约市，我观察到狮子的先验期望小于观察到狗的期望），观察结果**p(x)**的整体可能性，以及感官观察和隐藏原因之间的关系。
- en: '**Parsing out the relationship between x and z is at the heart of much of generative
    modeling.**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**解析x和z之间的关系是许多生成建模的核心。**'
- en: 'Their relationship is reflected in two important quantities: the posterior
    **p(z∣x)** and the likelihood **p(x|z)**, which are tied together according to
    Bayes’ famous law:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的关系反映在两个重要量上：后验概率**p(z∣x)**和可能性**p(x|z)**，它们根据贝叶斯著名定律联系在一起：
- en: '![](../Images/f34c917ecf269360329ad5df7505d8d6.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f34c917ecf269360329ad5df7505d8d6.png)'
- en: 'The posterior **p(z∣x)** gives us the probability of a latent cause, given
    observations. We usually don’t have access to this, due to a problem called intractability:
    according to Bayes’ law, we require p(x) to compute it, which necessitates going
    through all possible latent causes and checking how they would explain **x:**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 后验概率**p(z∣x)**给出了在给定观察值的情况下，潜在原因的概率。由于一个称为不可处理性的问题，我们通常无法访问这一点：根据贝叶斯定理，我们需要p(x)来计算它，这要求我们遍历所有可能的潜在原因，并检查它们如何解释**x**：
- en: '![](../Images/e0f622c4a5ca3dac19cd97da7b545b00.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e0f622c4a5ca3dac19cd97da7b545b00.png)'
- en: If the model of the world is complex, these are high-dimensional integrals,
    so this is not efficient or downright impossible.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果世界的模型复杂，这些是高维积分，因此这既不高效也完全不可能。
- en: '**Inferring the posterior constitutes the fundamental challenge of many generative
    models.**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**推断后验概率是许多生成模型的根本挑战。**'
- en: In Helmholtz Machines, the **posterior p(z∣x)** is estimated directly from data
    in a process called recognition, by learning an approximate posterior **q(x|z)**
    and trying to match it as closely to the true **p(z∣x)** as possible.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Helmholtz 机器中，**后验 p(z∣x)** 是通过从数据中直接估计的，这个过程称为识别，通过学习一个近似后验 **q(x|z)** 并尽可能接近真实的
    **p(z∣x)**。
- en: 'The reverse direction of estimating the likelihood **p(x|z)** is usually much
    easier: given a particular latent variable **z**, it just tells us how likely
    the observations **x** is. For this we usually don’t need to integrate anything,
    but can simply run the model forward.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 估计似然 **p(x|z)** 的反向方向通常容易得多：给定一个特定的潜在变量 **z**，它只是告诉我们观察到的 **x** 的可能性。为此，我们通常不需要积分任何东西，只需直接运行模型即可。
- en: 'The likelihood is parameterized by the generative network: how can we produce
    an **x**, given a **z**? If I start with a hidden cause, what would its effect
    on the world look like? If I imagine a person, what would that person’s face look
    or voice sound like?'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 似然性由生成网络参数化：给定一个 **z**，我们如何生成一个 **x**？如果我从一个隐藏原因开始，它对世界的影响会是什么样的？如果我想象一个人，那个人的面貌或声音会是什么样的？
- en: In most generative models, this is the part that is most relevant in practice
    (since it generates the image/text/audio). Once we have an idea of what the mapping
    from **z** to **x** looks like, we can generate samples by sampling a **z** and
    sending it through the generative network.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数生成模型中，这部分在实践中是最相关的（因为它生成图像/文本/音频）。一旦我们了解了 **z** 到 **x** 的映射是什么样的，我们可以通过采样一个
    **z** 并通过生成网络传递它来生成样本。
- en: In Helmholtz machines, both of these directions are parameterized via neural
    networks, which are trained in alternating order by the **Wake-Sleep-algorithm**,
    inspired by similar processes in human cognition, switching between comparing
    generated samples with the real world (wake) using the generative network, and
    mapping its own creation back to its latent states (dream) using the recognition
    network.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Helmholtz 机器中，这两个方向都是通过神经网络参数化的，这些网络通过 **Wake-Sleep 算法** 交替训练，该算法受人类认知中类似过程的启发，在生成网络（觉醒）中比较生成样本与真实世界，并在识别网络（梦境）中将自身创作映射回其潜在状态。
- en: 'Recognition Network z ← x: **q(z|x)**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '识别网络 z ← x: **q(z|x)**'
- en: 'Generative Network z→ x: **p(x|z)**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '生成网络 z→ x: **p(x|z)**'
- en: The structure of the latent space can often aids with the interpretation of
    the learned models. [Disentangling latent representation](https://arxiv.org/abs/2001.04872)
    and aligning them with interpretable features is of interest in many practical
    applications, but also more generally for achieving more interpretable models.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在空间的结构通常有助于解释所学模型。 [解缠潜在表示](https://arxiv.org/abs/2001.04872) 并将其与可解释特征对齐在许多实际应用中都很重要，但也更普遍地用于实现更具解释性的模型。
- en: As a familiar example, say we are building a generative model of images of human
    faces. Following the structure of the Helmholtz machine, we map the images unto
    a latent space. We can then try to discover interesting axes of variation in that
    latent space.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以一个熟悉的例子来说，假设我们正在构建一个人脸图像的生成模型。按照 Helmholtz 机器的结构，我们将图像映射到潜在空间。然后我们可以尝试发现该潜在空间中有趣的变化轴。
- en: Once axis of interesting variation might be related to the age of the persons
    in the images. We can then enforce constraints on the latent space (either in
    a supervised setting by providing data labelled with ages, or in an unsupervised
    setting by identifying age within the learned latent features, assuming it leads
    to significant variation) such that one of its direction, **z_age**, encodes the
    age displayed on the embedded images.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的变化轴可能与图像中人物的年龄有关。然后我们可以在潜在空间中强加约束（无论是在监督设置中通过提供带有年龄标签的数据，还是在无监督设置中通过识别学习到的潜在特征中的年龄，假设它导致显著的变化），以便其一个方向
    **z_age** 编码嵌入图像上显示的年龄。
- en: Knowing this direction can then be used to alter the age of images. If another
    direction **z_beard** encodes beardedness, I can use the model to encode an image
    **x** via the recognition network q(z|x), obtain a z, transform it into a z’=z+a*z_age+b*z_beard,
    and send it back via the generative model p(x|z’) to see a bearded, older version
    of myself.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 知道这个方向后，可以用来改变图像的年龄。如果另一个方向**z_beard**编码了有胡须的特征，我可以通过识别网络 q(z|x) 对图像 **x** 进行编码，得到一个
    z，将其转换为 z’=z+a*z_age+b*z_beard，并通过生成模型 p(x|z’) 发送回去，以查看一个有胡须、年长的自己版本。
- en: Models like OpenAI’s GLOW [let you play around with this on their website](https://openai.com/research/glow),
    but you will likely already be familiar with such applications e.g. in the Faceapp.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 像 OpenAI 的 GLOW [这样的模型让你在他们的网站上玩耍](https://openai.com/research/glow)，但你可能已经熟悉了这种应用，例如在
    Faceapp 中。
- en: All generative modeling boils down to (more or less closely related) variants
    of this, and while it has evolved significantly since the days of Helmholtz machines,
    the fundamental idea of capturing and reproducing the underlying structure of
    data in a probabilistic framework remains. I will now use these concepts to explain
    some of the most common versions of generative models that have come into the
    spotlight of AI research in the past decade.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 所有生成建模归结为（或多或少相关的）这种变体，尽管自 Helmholtz 机器时代以来它已显著发展，但在概率框架中捕获和重现数据的基本结构的理念依然存在。我将使用这些概念来解释过去十年中引起
    AI 研究关注的一些最常见的生成模型版本。
- en: '**Variational Autoencoders (VAEs)**'
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**变分自编码器（VAEs）**'
- en: Now the earth was formless and empty, And God said, ‘Let there be light,’ and
    there was light.
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现在地球是空虚和混沌的，上帝说：“要有光，”就有了光。
- en: '***— Genesis 1:1–5 (NIV)***'
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***— 创世记 1:1–5 (NIV)***'
- en: VAEs were simultaneously formulated in 2013 by [Kingma](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&cd=&ved=2ahUKEwizlfGwp8qBAxW0wQIHHfqNDdMQFnoECBMQAQ&url=https%3A%2F%2Farxiv.org%2Fabs%2F1312.6114&usg=AOvVaw0lcWUpG5O19Y_RMGqNMkr2&opi=89978449)
    and [Rezende](https://proceedings.mlr.press/v32/rezende14.html), and have found
    a wide range of applications (from denoising to compression to time series data).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: VAEs 在 2013 年由 [Kingma](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&cd=&ved=2ahUKEwizlfGwp8qBAxW0wQIHHfqNDdMQFnoECBMQAQ&url=https%3A%2F%2Farxiv.org%2Fabs%2F1312.6114&usg=AOvVaw0lcWUpG5O19Y_RMGqNMkr2&opi=89978449)
    和 [Rezende](https://proceedings.mlr.press/v32/rezende14.html) 同时提出，并且在广泛的应用中找到了用途（从去噪到压缩到时间序列数据）。
- en: 'They are a natural place to start since they are most similar in spirit to
    Helmholtz machines: they use both recognition (encoder) and generative (decoder)
    networks. As already mentioned before, the recognition network approximates the
    **posterior density** **p(z|x) by an approximate density q(z|x).**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法是一个自然的起点，因为它们在精神上最接近 Helmholtz 机器：它们使用识别（编码器）和生成（解码器）网络。正如之前提到的，识别网络通过近似密度
    **q(z|x)** 来近似 **后验密度** **p(z|x)**。
- en: '![](../Images/1b457cf240263a1400026383404a5d7a.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b457cf240263a1400026383404a5d7a.png)'
- en: VAEs are trained to minimize the negative **Evidence Lower Bound (ELBO),** which
    boils down to finding an approximate density **q(z|x)** that is as close to the
    true posterior **p(z|x)** as possible.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: VAEs 的训练目标是最小化负 **证据下界（ELBO）**，这归结为找到一个尽可能接近真实后验 **p(z|x)** 的近似密度 **q(z|x)**。
- en: Here we simply approximate the distribution **q(z|x)** by parameterizing it
    in a way that is trainable via gradient-based methods. There can be many subtleties
    to parameterizing the distribution, but we often just assume it is approximately
    Gaussian, which means it has a mean **μ** and a covariance **σ**, constituting
    the free parameters of the model. These are directly learned by neural networks
    (putting training data **x** into an NN, the output is **μ**).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们通过以一种可以通过基于梯度的方法训练的方式对分布 **q(z|x)** 进行参数化来简单地近似分布。参数化分布可能有很多细微之处，但我们通常假设它近似为高斯分布，这意味着它具有均值
    **μ** 和协方差 **σ**，构成了模型的自由参数。这些参数由神经网络直接学习（将训练数据 **x** 输入到神经网络中，输出为 **μ**）。
- en: In VAEs, the approximate posterior **q(z|x)** is used to draw one or many random
    samples, which are then plugged into the **ELBO,** which is defined as an expectation
    value over samples from **q:**
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在 VAEs 中，近似后验 **q(z|x)** 用于绘制一个或多个随机样本，然后将这些样本代入 **ELBO**，**ELBO** 定义为从 **q**
    中样本的期望值：
- en: '![](../Images/953493d9f445023a5a164ebd32064ea6.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/953493d9f445023a5a164ebd32064ea6.png)'
- en: Since the (negative) **ELBO** constitutes a loss, we can compute gradients,
    which in turn lets gradient descent do its magic.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 由于（负）**ELBO** 构成了损失，我们可以计算梯度，这反过来又让梯度下降发挥其魔力。
- en: 'Given the posterior can be quite complex, it can in practice be hard to compute
    well-behaved gradients from it, since they often have high variance, requiring
    many samples. The reparameterization trick at the heart of VAEs cleverly sidesteps
    this issue by splitting the sampling into two processes:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 由于后验可能相当复杂，在实践中可能难以从中计算出良好行为的梯度，因为它们往往有很高的方差，需要许多样本。VAEs 核心的重参数化技巧巧妙地绕过了这个问题，通过将采样分为两个过程：
- en: First, sample **ϵ** from a standard Gaussian **N(0,1)**.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，从标准高斯分布 **N(0,1)** 中采样 **ϵ**。
- en: Then, transform **ϵ** using the mean **μ** and standard deviation **σ** of **q(z∣x)**
    to get the sample **z=μ+σ×ϵ**
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，使用均值**μ**和标准差**σ**来变换**ϵ**，得到样本**z=μ+σ×ϵ**。
- en: 'What I find particularly elegant about the reparameterization trick is that
    it separates the two central components of each generative process, be it the
    more mundane task of generating handwritten digits, or the metaphorical creation
    of heaven and earth in the Bible quote: a random component, given by the “formless
    and empty” noise of the initial sample **ϵ**, acquires meaning via a complex transformation
    that finally, via the decoder, creates a pattern **x** in the observed world.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现重新参数化技巧特别优雅的一点是，它将每个生成过程的两个核心组件分开，无论是生成手写数字的更平凡任务，还是《圣经》引述中天与地的隐喻创建：一个由初始样本**ϵ**的“无形且空虚”噪声给出的随机组件，通过复杂的变换最终获得意义，通过解码器在观察到的世界中创造出一个模式**x**。
- en: '![](../Images/61dc4ec64fa725cf5a432486d7f2824d.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61dc4ec64fa725cf5a432486d7f2824d.png)'
- en: I trained a simple VAE on the MNIST data set, drew a random initial state, and
    sent it through the decoder to come up with this image that kind of looks like
    a 9\. The decoder implicitly understands the structure of the data, and decodes
    it from a lower-dimensional latent state. The MNIST dataset is available through
    **the Creative Commons Attribution-Share Alike 3.0** license.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我在 MNIST 数据集上训练了一个简单的 VAE，绘制了一个随机初始状态，并通过解码器生成了这个看起来有点像 9 的图像。解码器隐式理解数据的结构，并从一个低维潜在状态中解码它。MNIST
    数据集通过**知识共享署名-相同方式共享 3.0**许可证提供。
- en: Intuitively, the ELBO is composed of a reconstruction term and an entropy term.
    Since entropy, in the world of information theory, measures the unpredictability
    or randomness of information content, the entropy naturally regularizes the training,
    trading off structure and noise during optimization. If the VAE focuses too much
    on reconstruction, it might overfit the data, capturing every tiny detail (including
    noise) from the training data in the latent space. If it, however, gives too much
    weight to the entropy, it might end up with a too simplistic latent space that
    can’t capture the nuances of the data.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，ELBO 由重建项和熵项组成。由于熵在信息理论的世界中衡量信息内容的不可预测性或随机性，因此熵自然会在训练过程中起到正则化作用，在优化过程中权衡结构和噪声。如果
    VAE 过于关注重建，它可能会过拟合数据，在潜在空间中捕捉到训练数据中的每一个细节（包括噪声）。但如果它过分关注熵，它可能会得到一个过于简单的潜在空间，无法捕捉数据的细微差别。
- en: The entropy of the approximate posterior relates to its covariance structure
    **σ**, which gives us a measure of how much of the “formless and empty” noise
    (encoding uncertainty in our explanations) from the initial sample remains. If
    we want to make the whole procedure deterministic, we can simply set **σ** to
    zero, and all uncertainty is removed.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 近似后验的熵与其协方差结构**σ**相关，这为我们提供了一个衡量初始样本中“无形且空虚”噪声（编码解释中的不确定性）剩余量的度量。如果我们想让整个过程变得确定性，我们可以简单地将**σ**设置为零，这样所有的不确定性都会被消除。
- en: 'In a deterministic universe, there is no true noise, only things that our models
    are not powerful enough to capture, or where we simply lack necessary information
    ([I love noise, and have written a whole article about it here](https://manuel-brenner.medium.com/the-importance-of-noise-327fcab7c4fb)).
    As George Box noted: ”all models are wrong, some are useful”, and VAEs learn to
    strike this balance between over- and underconfidence.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个确定性宇宙中，实际上没有真正的噪声，只有我们的模型无法捕捉的东西，或者我们缺乏必要的信息（[我爱噪声，已经在这里写了一整篇文章](https://manuel-brenner.medium.com/the-importance-of-noise-327fcab7c4fb)）。正如乔治·博克所指出的：“所有模型都是错误的，有些是有用的”，而
    VAEs 学会在过度自信和不足自信之间取得平衡。
- en: This organizing principle helps explain why VAEs naturally excel at tasks like
    dimensionality reduction (separating important from unimportant information contained
    in the input data) and [denoising](/denoising-autoencoders-explained-dbb82467fc2).
    As mentioned before, VAEs can also achieve structured representations of the latent
    space, leading to interpretable features.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这个组织原则有助于解释为什么 VAEs 在像降维（将输入数据中的重要信息与不重要信息分离）这样的任务上自然表现优异，以及[去噪](/denoising-autoencoders-explained-dbb82467fc2)。正如前面提到的，VAEs
    还可以实现潜在空间的结构化表示，从而产生可解释的特征。
- en: '![](../Images/23a4c31f32af32b04ea38977b0845cc9.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23a4c31f32af32b04ea38977b0845cc9.png)'
- en: Sampling a grid of the 2D latent space for a VAE trained on MNIST leads to continuous
    transformations of different numbers into each other. Generated by me, [based
    on code for a figure provided here](https://github.com/gonzalorecio/MNIST-latent-representations).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在 MNIST 上训练的 VAE，采样 2D 潜在空间的网格会导致不同数字的连续变换。由我生成的， [基于这里提供的图形代码](https://github.com/gonzalorecio/MNIST-latent-representations)。
- en: '**Normalizing Flows (NF)**'
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**归一化流 (NF)**'
- en: I’ve heard someone call NFs “the reparameterization trick on steroids”, and
    I really like this description. Normalizing flows take over where VAEs leave off
    by formulating recognition as the application of flows.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我听到有人称 NFs 为“强化版的重参数化技巧”，我非常喜欢这个描述。归一化流接管了 VAEs 的空白，通过将识别形式化为流的应用来完成。
- en: NFs operate by iteratively transforming a simple probability distribution, which,
    as in the case of VAEs, is usually a standard Gaussian **N(0,1)**, into more complex,
    intricate ones through a series of invertible transformations.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: NFs 通过一系列可逆变换将简单的概率分布（例如 VAEs 中通常使用的标准高斯分布 **N(0,1)**）迭代地转换为更复杂、更精细的分布。
- en: '![](../Images/a3ca52cceb0f689ea5f9f069ea543099.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a3ca52cceb0f689ea5f9f069ea543099.png)'
- en: Normalizing flows deform samples z0 from a simple distribution into a potentially
    complex one by applying a series of invertible transformations. janosh, MIT <[http://opensource.org/licenses/mit-license.php](http://opensource.org/licenses/mit-license.php)>,
    via Wikimedia Commons
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化流通过应用一系列可逆变换，将样本 z0 从简单分布变形为潜在复杂的分布。 janosh，MIT <[http://opensource.org/licenses/mit-license.php](http://opensource.org/licenses/mit-license.php)>,
    通过 Wikimedia Commons
- en: Where VAEs separate randomness from the structure using a fixed distribution
    and learned transformations (mean and variance), NFs dynamically shape the distribution
    itself. They do this by keeping track of the Jacobian determinant. This gives
    a measure of the volume change of the transformation, e.g. how much it shrinks
    or stretches space, ensuring that the whole latent space morphs in a coherent
    manner.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: VAEs 使用固定的分布和学习到的变换（均值和方差）将随机性与结构分开，而 NFs 则动态地塑造分布本身。它们通过跟踪雅可比行列式来实现这一点。这可以衡量变换的体积变化，例如，它如何收缩或拉伸空间，确保整个潜在空间以一致的方式变化。
- en: As in the case of VAEs, a blob of formlessness is shaped into form.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如同 VAEs 的情况一样，一块无形的东西被塑造成形状。
- en: 'At least two things are cool about NFs: they are invertible, so they allow
    a two-way mapping between both distributions, which can come in handy in many
    situations, e.g. when trying to estimate densities (since once you map back to
    the standard Gaussian **N(0,1)** this can be much easier to compute than for complex
    intractable posteriors), or for [anomaly detection](https://open.spotify.com/episode/1LlqI3frkyTUwuUuqRcVo1?si=3867f2171b974279),
    where the idea is to screen out data that has low likelihood under the learned
    distribution.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 NFs 至少有两件很酷的事：它们是可逆的，因此允许在两个分布之间进行双向映射，这在许多情况下非常有用，例如，当试图估计密度时（因为一旦你映射回标准高斯分布
    **N(0,1)**，这通常比处理复杂的不可处理后验要容易得多），或者用于 [异常检测](https://open.spotify.com/episode/1LlqI3frkyTUwuUuqRcVo1?si=3867f2171b974279)，其思想是筛选出在学习到的分布下概率较低的数据。
- en: '[OpenAI’s GLOW](https://openai.com/research/glow), which I mentioned earlier,
    also makes use of this reversibility to manipulate features like smile, age or
    beardedness in latent space and obtain an altered image almost in real-time.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenAI 的 GLOW](https://openai.com/research/glow)，我之前提到过，也利用这种可逆性来操控潜在空间中的特征，如微笑、年龄或胡须，并几乎实时地获得修改后的图像。'
- en: A second cool thing is their adaptability to different geometries and manifolds.
    A classical example is the application to [spherical symmetries](https://arxiv.org/abs/2002.02428),
    allowing NFs to form latent representations that live on the sphere. Since, despite
    some claims to the opposite, the earth is likely spherical, spherical symmetries
    are very useful e. g. when running simulations of the earth’s weather system.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个很酷的特点是它们对不同几何形状和流形的适应性。一个经典的例子是对 [球面对称性](https://arxiv.org/abs/2002.02428)
    的应用，使得 NFs 能够形成存在于球面上的潜在表示。由于尽管有些观点相反，地球可能是球形的，因此球面对称性非常有用，例如在运行地球天气系统的模拟时。
- en: '**Diffusion Models**'
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**扩散模型**'
- en: Creating noise from data is easy; creating data from noise is generative modeling.
  id: totrans-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 从数据中创建噪声很简单；从噪声中创建数据是生成建模。
- en: '- [***Song et al.***](https://arxiv.org/abs/2011.13456)'
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- [***宋等人***](https://arxiv.org/abs/2011.13456)'
- en: Moving on, diffusion models are one of the most successful generative models
    of the past few years. While they were already proposed in 2015 by [Sohl-Dickstein
    et al.](https://arxiv.org/abs/1503.03585), their success in [image creation](https://arxiv.org/abs/2006.11239)
    has catapulted them to the spotlight, laying the foundation for DALL-E, Midjourney,
    or Stable Diffusion. While their basic architecture is quite different, they still
    relate conceptually to VAEs and Normalizing Flows.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 展开来说，扩散模型是过去几年中最成功的生成模型之一。尽管它们早在2015年就由[Sohl-Dickstein等人](https://arxiv.org/abs/1503.03585)提出，但它们在[图像生成](https://arxiv.org/abs/2006.11239)方面的成功使其成为焦点，为DALL-E、Midjourney或Stable
    Diffusion奠定了基础。虽然它们的基本架构相当不同，但在概念上它们仍与VAEs和正常流相关。
- en: 'Diffusion models split the generative process up into several steps: at every
    step of the way, the training sample is distorted with noise. The aim of the model
    is to learn to remove that noise from the sample. If I haven’t made it clear enough
    before that noise is fascinating, it again takes over a lead role in diffusion
    models.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型将生成过程分解为几个步骤：在每一步中，训练样本都会被噪声扰动。模型的目标是学习如何从样本中去除这些噪声。如果我之前没有足够清楚地说明噪声的迷人之处，那么在扩散模型中，它再次成为了主角。
- en: 'During training, noise is iteratively added to the training data. Using the
    example of images, the model either learns to remove minute levels of noise and
    polish its final details, or to flesh out vague shapes in an otherwise distorted
    image:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，噪声会被反复添加到训练数据中。以图像为例，模型要么学习去除微小的噪声并打磨最终细节，要么在一个被扭曲的图像中完善模糊的形状：
- en: '![](../Images/a8e1982da87ed6005a9c76d0626797b0.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8e1982da87ed6005a9c76d0626797b0.png)'
- en: The generation process in stable diffusion. Benlisquare, CC BY-SA 4.0 <[https://creativecommons.org/licenses/by-sa/4.0](https://creativecommons.org/licenses/by-sa/4.0)>,
    via Wikimedia Commons
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定扩散中的生成过程。Benlisquare，CC BY-SA 4.0 <[https://creativecommons.org/licenses/by-sa/4.0](https://creativecommons.org/licenses/by-sa/4.0)>,
    通过维基媒体公用领域
- en: While the process of recognition is less directly modeled via a recognition
    network, and the training objective changes quite a bit, the process of adding
    noise, and the subsequent monitoring of how the noise should be reduced, can be
    seen as a form of recognition, and the initial noise sample as the initial state
    drawn from p(**z_0**).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管识别过程没有通过识别网络直接建模，且训练目标有很大变化，但添加噪声的过程以及随后的噪声减少监测可以看作是一种识别形式，初始噪声样本则是从p(**z_0**)中提取的初始状态。
- en: 'When generating a completely new sample, the model can just start with pure
    noise and, when trying to make sense of what could be hidden beneath the noise,
    come up with something new:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当生成一个全新的样本时，模型可以从纯噪声开始，并在试图弄清楚噪声下可能隐藏的内容时，创造出一些新的东西：
- en: '![](../Images/b02411a7aecf4dffd6426ed0ecf9e4b8.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b02411a7aecf4dffd6426ed0ecf9e4b8.png)'
- en: When I ask DALL-E for “random white noise”, it returns something that is very
    much not random white noise. It can’t help but reflect some of the structure of
    the data it was trained on.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当我向DALL-E请求“随机白噪声”时，它返回的内容并不是完全随机的白噪声。它不可避免地反映了其训练数据的一些结构。
- en: '**The process of hallucinating something from nothing uncovers the implicitly
    learned distribution of the data it was trained on.**'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**从无中生有的过程揭示了其训练数据隐含学习的分布。**'
- en: Why exactly diffusion models work so well is still open for debate. Among other
    things, they have been likened to [energy-based models of associative memory](https://arxiv.org/pdf/2309.16750.pdf)
    ([made famous through Hopfield networks 40 years ago](https://www.pnas.org/doi/10.1073/pnas.79.8.2554)).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型为何如此有效仍然有待讨论。除此之外，它们被比作[基于能量的联想记忆模型](https://arxiv.org/pdf/2309.16750.pdf)（[40年前通过Hopfield网络而闻名](https://www.pnas.org/doi/10.1073/pnas.79.8.2554)）。
- en: 'Diffusion model also relate to ideas in score-based generative modeling, made
    popular by [Song et al.](https://arxiv.org/abs/2011.13456): unlike traditional
    methods that focus on directly computing data likelihoods, these models center
    on approximating the **score**, which in this context signifies the gradient of
    the data likelihood with respect to the data itself.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型还涉及基于得分的生成建模思想，这一思想由[Song等人](https://arxiv.org/abs/2011.13456)推广：与直接计算数据似然的传统方法不同，这些模型专注于近似**得分**，在这种情况下，得分表示数据似然相对于数据本身的梯度。
- en: Intuitively, the score gives the direction in which a sample should be changed
    to **make it more likely**. By not calculating the likelihood directly, these
    models often sidestep some of the computational challenges we have encountered
    before.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，得分提供了一个样本应如何更改以**提高其可能性**的方向。通过不直接计算可能性，这些模型通常避开了一些我们之前遇到的计算挑战。
- en: The score function can again be modeled by neural networks. One particularly
    intriguing way to represent it is via stochastic differential equations (SDEs),
    which akin to Neural ODEs, represent differential equations through neural nets
    ([something known as implicit layers](http://implicit-layers-tutorial.org/)).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 得分函数可以再次通过神经网络建模。表示它的一种特别有趣的方式是通过随机微分方程（SDEs），类似于神经常微分方程（Neural ODEs），通过神经网络表示微分方程（[一种称为隐式层的东西](http://implicit-layers-tutorial.org/)）。
- en: This is similar in spirit to a continuous-time version of diffusion models.
    Starting from noise, the score function is used to guide it toward a likely sample
    (Stefano Ermon, whose lab develops these techniques, does great talks, [here is
    one explaining all of this in more detail](https://www.youtube.com/watch?v=Uwz7kv3GHEc&t=503s)).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这在精神上类似于扩散模型的连续时间版本。从噪声开始，得分函数用于引导其朝向一个可能的样本（Stefano Ermon，他的实验室开发了这些技术，做了很棒的讲座，[这里是一个更详细的讲解](https://www.youtube.com/watch?v=Uwz7kv3GHEc&t=503s)）。
- en: In diffusion models, the generative process is likewise stochastic, adding a
    stochastic component at every point of the step. Given the process of generation
    is split up into several steps, this allows the introduction of slight variations
    of samples by taking several steps back in the chain and running the process forward
    again.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在扩散模型中，生成过程同样是随机的，在每一步都加入了一个随机成分。由于生成过程被分解成多个步骤，这允许通过向链中回溯几个步骤并重新运行过程，从而引入样本的细微变化。
- en: '![](../Images/bef594ec556b5bb0762ac58f94dd6b4e.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bef594ec556b5bb0762ac58f94dd6b4e.png)'
- en: Slight variations of the painting I used as the Thumbnail for this article,
    made by DALL-E.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我在本文缩略图中使用的画作的细微变化，由DALL-E生成。
- en: In some of the most popular applications of diffusion models like DALL-E or
    Midjourney, the initial state is not necessarily given by a purely random sample
    **z0** from **N(0,1),** but from a joint embedding between vision and language
    **p(z0|x)**, where x can e.g. be a textual input, given by the powerful CLIP (Contrastive
    Language-Image Pre-Training) embedding.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些最受欢迎的扩散模型应用中，如DALL-E或Midjourney，初始状态不一定由纯随机样本**z0**从**N(0,1)**给出，而是由视觉和语言的联合嵌入**p(z0|x)**给出，其中x可以是例如由强大的CLIP（对比语言-图像预训练）嵌入提供的文本输入。
- en: Conditional generation is valuable in all kinds of multi-modal learning setups
    since it combines different sensory modalities into a coherent framework. It will
    probably be an integral part of some of the most exciting developments in AI to
    come.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 条件生成在各种多模态学习设置中具有重要价值，因为它将不同的感官模式结合成一个连贯的框架。这可能会成为未来一些最激动人心的人工智能发展的重要组成部分。
- en: '![](../Images/e9fe04aba429375bee3bcab078a48702.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e9fe04aba429375bee3bcab078a48702.png)'
- en: An impressionist painting of an AI brain imagining a sunset, credit to DALL-E.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一幅印象派画作，描绘了一个AI大脑想象日落，感谢DALL-E。
- en: '**Generative Adversarial Networks (GANs)**'
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**生成对抗网络（GANs）**'
- en: '[GANs](https://arxiv.org/abs/1406.2661) are one of the most popular classes
    of generative models of the recent decade, inspired by a legendary night of drinking
    by Ian Goodfellow and his friends.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[GANs](https://arxiv.org/abs/1406.2661)是最近十年最受欢迎的生成模型类别之一，灵感来源于Ian Goodfellow和他的朋友们的一晚传奇饮酒经历。'
- en: GANs pivot even farther from the dual-network structure of the Helmholtz machine.
    As I mentioned, approximating **p(z|x)** is often the central challenge of generative
    models, so GANs just throw recognition out of the window and try to do without
    it.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: GANs甚至更远离了赫尔姆霍兹机器的双网络结构。如我所提到的，近似**p(z|x)**通常是生成模型的核心挑战，因此GANs干脆把识别抛到窗外，尝试在没有它的情况下进行生成。
- en: GANs also begin by drawing a random noise vector from p**(z0)** (as in diffusion
    models, [this initial vector can also be conditioned on other information](https://arxiv.org/abs/1411.1784),
    such as text), but then train only the generative network (since this is anyway
    what we are most interested in many applications) by including a discriminator,
    and try to match samples from **p(x|z)** from the generative model to examples
    of the training data.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: GANs也开始通过从**p(z0)**中抽取一个随机噪声向量（如在扩散模型中，[这个初始向量也可以依赖于其他信息](https://arxiv.org/abs/1411.1784)，比如文本），但随后仅训练生成网络（因为这是我们在许多应用中最感兴趣的部分），通过包含一个鉴别器，尝试使生成模型中的**p(x|z)**样本与训练数据的示例匹配。
- en: The generative network is trained to produce data that can deceive the discriminator.
    This discriminator, in turn, is trained to distinguish between real and fake samples.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 生成网络的训练目的是产生能够欺骗鉴别器的数据。鉴别器则被训练来区分真实样本和虚假样本。
- en: 'The elegance of GANs lies in this competitive dynamic: the generator improves
    its ability to produce data, guided by feedback from the discriminator, while
    the discriminator itself becomes better at discerning real from fake. It’s the
    perfect zero-sum game, pushing both networks to become better and better (with
    certain risks for deep fake detection [I have written about here](/the-dangers-of-adversarial-learning-874a95cdddd3?sk=10fb6e1ea209a3b28a117d06bded0749)).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: GANs的优雅之处在于这种竞争动态：生成器在鉴别器的反馈指导下提高其生成数据的能力，而鉴别器本身则变得更擅长区分真实与虚假。这是完美的零和游戏，推动两个网络不断变得更好（有一定的深度伪造检测风险，[我在这里写过](
    /the-dangers-of-adversarial-learning-874a95cdddd3?sk=10fb6e1ea209a3b28a117d06bded0749)）.
- en: However, GANs come with their own set of challenges and have arguably lost some
    popularity recently with the success of their competitors, especially diffusion
    models. Training GANs can be notoriously unstable. If the generator produces low-quality
    samples in the beginning, the discriminator’s job becomes too easy, making it
    difficult for the generator to improve. On the other hand, if the discriminator
    becomes too powerful, it can stifle the generator’s growth, leading to mode collapse,
    where the generator ends up producing only a subset of possible outputs.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，GANs也有自身的一系列挑战，并且由于其竞争对手，特别是扩散模型的成功，GANs最近的流行度有所下降。训练GANs可能非常不稳定。如果生成器在开始时产生低质量的样本，鉴别器的工作变得太容易，这使得生成器很难改进。另一方面，如果鉴别器变得过于强大，它可能会抑制生成器的成长，导致模式崩溃，即生成器最终只产生可能输出的一个子集。
- en: '**Transformers and Large Language Models (**LLMs)'
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**Transformers 和大型语言模型（**LLMs）'
- en: Since Transformers have revolutionized the generative modeling landscape for
    text completely, I can’t leave them unmentioned here.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Transformers彻底革新了文本生成建模的格局，我不能在这里不提到它们。
- en: To keep it short, almost all LLMs are based on a variant of the [transformer
    architecture implementing self-attention from the famous 2017 Google paper,](https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)
    details on which would go beyond the scope of this article, [but are explained
    in many other places](/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452).
    This architecture allows LLMs to learn complex relationships between input sequences,
    which works especially well on text.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，几乎所有LLMs都基于[transformer架构的变体，该架构实现了2017年谷歌论文中的自注意力机制](https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)，关于这一点的详细信息超出了本文的范围，[但在许多其他地方有解释](/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452)。这种架构允许LLMs学习输入序列之间的复杂关系，这在文本上特别有效。
- en: Some Transformer variants, like BERT, are trained in a masked language modeling
    setting. They are given sequences with certain tokens masked out and are trained
    to recognize the masked tokens. This is quite similar in spirit to recognition
    in VAEs, where masking words can be interpreted as noising the input. The missing
    word is imputed because the Transformer has learned a probability distribution
    over the input data **p(x),** and can understand the most likely word given the
    context.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一些Transformer变体，如BERT，是在掩码语言建模设置下训练的。它们接收有些标记被掩盖的序列，并被训练去识别这些掩盖的标记。这在精神上与VAEs中的识别非常相似，其中掩盖单词可以被解释为对输入进行噪声处理。缺失的单词被填补，因为Transformer已经学习了输入数据的概率分布**p(x)**，并能够理解给定上下文的最可能单词。
- en: From a generative perspective, transformer-based LLMs model the probability
    of each potential word or phrase following the previous one(s), frequently conditioned
    on an input prompt. This again expresses a variant of the ever-present probability
    distribution **p(x|z)**.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 从生成的角度看，基于变压器的LLM模型会根据输入提示，建模每个潜在单词或短语在前一个单词之后的概率。这再次表达了始终存在的概率分布**p(x|z)**的一种变体。
- en: However, there are usually no explicit hidden variables **z** in Transformers,
    because the prompt and context are themselves words. Instead, the self-attention
    mechanism extracts the probability of token p(**x_i**|(**x1​,x2​,…,xt​)**) from
    all the observed words (**x1​,x2​,…,xt**​), and of course from the implicit distribution
    over all words and contexts it has seen in the billions of lines of training data.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，变压器中通常没有显式的隐藏变量**z**，因为提示和上下文本身就是单词。相反，自注意机制从所有观察到的单词（**x1​,x2​,…,xt**）中提取标记p(**x_i**|(**x1​,x2​,…,xt​)**)的概率，当然还包括它在数十亿行训练数据中看到的所有单词和上下文的隐含分布。
- en: 'While noise is not directly a part of training Transformers, LLMs naturally
    include a probabilistic component. This makes a lot of sense, since language is
    not uniquely determined (hence Markov Models were originally developed on the
    foundation of Russian poetry, [as I discuss at length here](/understanding-markov-chains-cbc186d30649)),
    and the same paragraph can be expressed in many different ways: when generating
    responses or samples, there are usually several words that fit the given context
    well, so there is a distribution over possible continuations p(**x_i**|(**x1​,x2​,…,xt​)**),'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然噪音并不是训练变压器的直接组成部分，但LLM自然地包含了一个概率成分。这很有意义，因为语言不是唯一确定的（因此，马尔可夫模型最初是在俄罗斯诗歌的基础上开发的，[我在这里详细讨论](/understanding-markov-chains-cbc186d30649)），同一段文字可以用多种不同的方式表达：在生成响应或样本时，通常有几个单词适合给定的上下文，因此存在可能延续的分布
    p(**x_i**|(**x1​,x2​,…,xt​)**),
- en: The probabilities for selecting different words can be scaled with the so-called
    temperature hyperparameter. Depending on whether you are looking for a creative
    or deterministic response, the level of “noisiness” can effectively be controlled
    by this parameter. LLMs like Chat-GPT allow you to ask for a specific temperature
    during a response.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 选择不同单词的概率可以通过所谓的温度超参数进行缩放。根据你是寻找创意还是确定性响应，这个参数可以有效地控制“噪声”水平。像Chat-GPT这样的LLM允许你在响应时请求特定的温度。
- en: 'Chat-GPT rephrased this paragraph for me with a high-temperature setting: *“In
    the swirling galaxy of Transformers, noise isn''t the main star, but LLMs groove
    to the uncertain beats of language. Crafting answers isn''t about nailing that
    one epic word but jamming with a cocktail of potential wordy melodies p(x_i|(x1​,x2​,…,xt​)).”*'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Chat-GPT用高温设置重新表述了这段文字：*“在变压器的旋转银河中，噪音不是主角，但LLM在语言的不确定节拍中舞动。构建答案不是为了找到那个单一的绝妙词汇，而是与潜在单词旋律的鸡尾酒一同狂欢
    p(x_i|(x1​,x2​,…,xt​))。”*
- en: 'While a high temperature makes ChatGPT sound like it is literally high, a “high
    temperature” is used here in analogy to Boltzmann’s statistical formulation of
    thermodynamics, which postulates that the states of a system follow an exponential
    distribution, depending on its temperature and the energy of the state:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然高温度使得ChatGPT听起来好像真的很兴奋，但这里的“高温度”类比于玻尔兹曼的热力学统计公式，该公式假设系统状态遵循指数分布，取决于系统的温度和状态的能量：
- en: '![](../Images/a350ea73a4bf379c7d00dad6eaf67104.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a350ea73a4bf379c7d00dad6eaf67104.png)'
- en: The Boltzmann Distribution for different lambdas, which are inversely related
    to the temperature. Newystats, CC BY-SA 4.0 <[https://creativecommons.org/licenses/by-sa/4.0](https://creativecommons.org/licenses/by-sa/4.0)>,
    via Wikimedia Commons
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的lambda下的玻尔兹曼分布与温度呈反比关系。Newystats，CC BY-SA 4.0 <[https://creativecommons.org/licenses/by-sa/4.0](https://creativecommons.org/licenses/by-sa/4.0)>,
    通过维基共享资源
- en: 'The analogy to Transformers is no coincidence: The softmax function is used
    in the self-attention mechanism when mapping the obtained [scaled dot-product
    scores between keys and queries to probabilities](/the-power-of-the-dot-product-in-artificial-intelligence-c002331e1829).
    The softmax has the exact same functional form as the Boltzmann distribution and
    is used in both cases to map unnormalized scores (or energies in the case of the
    Boltzmann distribution) to a normalized probability distribution.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 与变换器的类比并非巧合：softmax函数在自注意力机制中用于将[键和值之间的缩放点积分数映射为概率](/the-power-of-the-dot-product-in-artificial-intelligence-c002331e1829)。softmax与玻尔兹曼分布具有完全相同的函数形式，在这两种情况下都用于将未归一化的分数（在玻尔兹曼分布中是能量）映射到归一化的概率分布。
- en: As in thermodynamics, temperature closely relates to entropy and thus, in turn,
    to uncertainty/noise. In the Boltzmann distribution, as the temperature increases,
    the probabilities for different energy states become more uniform. Maximum uniformity
    leads to maximum entropy since all states are equally likely. In LLMs, this means
    that all words that are vaguely possible are predicted at the next stage with
    equal probability. It, however, doesn’t mean that generated text is completely
    random even for high temperatures, as we saw in the examples above. The selection
    of most probable tokens, even when scaled by higher temperatures, still mostly
    represents coherent language.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在热力学中，温度与熵密切相关，从而也与不确定性/噪声相关。在玻尔兹曼分布中，随着温度的升高，不同能量状态的概率变得更加均匀。最大均匀性导致最大熵，因为所有状态的可能性相等。在LLMs中，这意味着所有可能的词汇在下一阶段都有相等的概率被预测。然而，这并不意味着生成的文本完全是随机的，即使在高温下，正如我们在上面的例子中看到的。即使在较高温度下，最可能的标记的选择仍然大多代表了连贯的语言。
- en: If I managed to communicate one idea with this article, it should be that noise
    plays a crucial role in all generative models. Generative modeling is the art
    of taking formless noise and breathing structure into it.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我在这篇文章中传达了一个观点，那就是噪声在所有生成模型中发挥着至关重要的作用。生成建模是一种将无形噪声赋予结构的艺术。
- en: As the past years have shown, many ways lead to Rome, and different models can
    make sense depending on the goals, data modalities, and on pragmatic considerations
    about what works best when scaled up to enormous model sizes (such as Transformers)
    and when training with gradient-based methods.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来的经验表明，许多途径通向罗马，不同的模型可以根据目标、数据模式以及在扩展到巨大模型规模（如变换器）和使用基于梯度的方法进行训练时的实际考虑来取得有效结果。
- en: “What I can create, I still don’t understand.”
  id: totrans-127
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我能创造的东西，我仍然不理解。”
- en: '***— What Richard Feynman should have mentioned***'
  id: totrans-128
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***— 理查德·费曼应该提到的***'
- en: The enormity of state-of-the-art generative models and the complexity of the
    training data has contributed to challenges with interpreting them. Diffusion
    models and Transformers are not formulated as latent variable models, and can
    feel like giant black-boxes, with interpretations lagging behind substantially,
    especially given increasing worries about their impact on the real world.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 先进的生成模型的巨大规模和训练数据的复杂性带来了诠释这些模型的挑战。扩散模型和变换器并没有被构建为潜变量模型，因此可能像巨大的黑箱一样，解释滞后，特别是在对其对现实世界影响的担忧日益增加的情况下。
- en: However, we might still learn to uncover some structures within, [such as in
    this new paper by Max Tegmark et al.](https://paperswithcode.com/paper/language-models-represent-space-and-time),
    in which they decscribe discovering intermediate representations of space and
    time in LLMs, and liken to the emergence of an interpretable world model. Others
    [creatively apply the tools of cognitive psychology](https://arxiv.org/abs/2206.14576)
    to understand the behavior of LLMs, much as we try to understand the complexities
    of human behavior.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们仍然可能学会揭示其中的一些结构，[例如Max Tegmark等人的这篇新论文](https://paperswithcode.com/paper/language-models-represent-space-and-time)，其中描述了在LLMs中发现空间和时间的中间表征，并将其比作可解释世界模型的出现。其他人则[创造性地应用认知心理学的工具](https://arxiv.org/abs/2206.14576)来理解LLMs的行为，就像我们试图理解人类行为的复杂性一样。
- en: '[In a recent podcast episode](https://www.youtube.com/watch?v=-hxeDjAxvJ8),
    Marc Andreessen called the question of whether generative models can be **meaningfully
    trained and improved on synthetic data from generative models** a trillion-dollar
    question. Training on this essentially free data would open up many possibilities,
    providing a form of self-play (which has already been successfully used by DeepMind
    with AlphaGo and AlphaFold) to continue tuning generative models without relying
    on expensively curated training data. Andreessen related this question to the
    information-theoretic perspectives on the relationship between signal and noise,
    going back to Shannon: simply asked, how can there be more information in a model
    than what we fed into it?'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在[最近的一期播客节目](https://www.youtube.com/watch?v=-hxeDjAxvJ8)中，马克·安德森称，生成模型是否能**在合成数据上进行有意义的训练和改进**是一个价值万亿美元的问题。利用这种基本免费的数据进行训练将打开许多可能性，提供一种自我游戏（这种方法已经被DeepMind成功地用于AlphaGo和AlphaFold）来继续调整生成模型，而无需依赖昂贵的精心策划的训练数据。安德森将这个问题与关于信号与噪声关系的信息论观点联系在一起，追溯到香农：简单地说，如何能在模型中包含比我们输入的更多的信息？
- en: If they don’t like it, they call it hallucination, if they like it, they call
    it creativity.
  id: totrans-132
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果他们不喜欢，就称之为幻觉；如果他们喜欢，就称之为创造力。
- en: '***— Marc Andreessen***'
  id: totrans-133
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***— 马克·安德森***'
- en: Is it true that generative models only imitate what they see in the training
    data? In what sense does the noisiness of the training process and the noisiness
    of the model formulation itself lead to a kind of generalization beyond the training
    data (I’ve considered related questions in my recent article on the [Free Lunch
    Theorem](/why-there-kind-of-is-free-lunch-56f3d3c4279f))? After all, noise is
    widely used in [machine learning models to boost generalization](/why-more-is-more-in-deep-learning-b28d7cedc9f5).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型是否只是模仿它们在训练数据中看到的内容？训练过程中的噪声和模型本身的噪声在多大程度上导致了一种超越训练数据的泛化（我在最近的一篇关于[免费午餐定理](/why-there-kind-of-is-free-lunch-56f3d3c4279f)的文章中考虑了相关问题）？毕竟，噪声在[机器学习模型中被广泛用于提升泛化能力](/why-more-is-more-in-deep-learning-b28d7cedc9f5)。
- en: 'Noise can both lead to hallucination and creativity, to the hallucination of
    alternative facts and the creation of alternative perspectives that weren’t there
    before. With generative models, it can also be argued that the “information” lies
    not just in the raw data, but in the combinatorial possibilities of that data.
    Generative models provide us with a novel and enticing way to explore this combinatorial
    space. In the words of Mark Twain:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声既能导致幻觉和创造力，也能引发替代事实的幻觉和之前不存在的替代视角的创造。通过生成模型，还可以认为“信息”不仅存在于原始数据中，还存在于这些数据的组合可能性中。生成模型为我们提供了一种新颖而诱人的方式来探索这种组合空间。用马克·吐温的话来说：
- en: “There is no such thing as a new idea. It is impossible. We simply take a lot
    of old ideas and put them into a sort of mental kaleidoscope. We give them a turn
    and they make new and curious combinations.”
  id: totrans-136
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “没有真正的新想法。这个想法是不可能的。我们只是将许多旧想法放入某种心理万花筒中。我们给它转动，它们就会产生新的和有趣的组合。”
- en: '***— Mark Twain***'
  id: totrans-137
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***— 马克·吐温***'
- en: 'And in the spirit of the quote itself not being a new idea, we can yet again
    go back to the bible (which I didn’t expect to quote twice in an AI article):'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，既然这句话本身并不是新想法，我们可以再次回到圣经（我没想到在一篇AI文章中会引用两次）：
- en: What has been will be again, what has been done will be done again; there is
    nothing new under the sun.
  id: totrans-139
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 过去的将会重现，已做过的将会再做；在太阳底下没有新事物。
- en: '**Ecclesiastes 1:9 (Bible)**'
  id: totrans-140
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**传道书 1:9 (圣经)**'
- en: It can be argued that a similar interplay between noise and structure can also
    be observed to play out in human creativity. In my recent article on g[enius and
    mental visualization](https://manuel-brenner.medium.com/genius-and-the-power-of-mental-visualization-836bf3763071),
    I’ve explored how in the brain, free, unstructured, mind-wandering activity by
    the default mode network ( what Scott Barry Kaufman calls the “imagination network”),
    can often provide an impulse that is then shaped by more deliberate, focused practice
    and skill into some of the most astounding works of art and genius.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 可以说，人类创造力中也能观察到噪声与结构之间的类似相互作用。在我最近的一篇关于[天才与心理视觉化](https://manuel-brenner.medium.com/genius-and-the-power-of-mental-visualization-836bf3763071)的文章中，我探讨了在大脑中，默认模式网络（斯科特·巴里·考夫曼称之为“想象网络”）的自由、无结构的思维漫游活动，往往能提供一种冲动，这种冲动随后被更为刻意、集中的练习和技能塑造为一些最令人惊叹的艺术作品和天才创作。
- en: '![](../Images/90dac23093f1440094e9c68c4f0d91d1.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90dac23093f1440094e9c68c4f0d91d1.png)'
- en: DALL-E painting in the style of Jackson Pollock, capturing the structured randomness
    of the original random style of painting.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: DALL-E 以杰克逊·波洛克的风格绘画，捕捉原始随机风格绘画的结构化随机性。
- en: 'Even the most novel works of art and science have to be understood in a language
    that is already partially familiar to us. As Wittgenstein noted: there is no private
    language. Generative models are learning to speak our language, are learning to
    approximate the **p(x)** of all the things we most care about, and, by trading
    off noise and structure, reveal endless new patterns within this distribution
    and slightly outside of it. Their creativity can in turn be used to inspire our
    own creativity.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是最创新的艺术和科学作品，也必须用我们已经部分熟悉的语言来理解。正如维特根斯坦所指出的：没有私人语言。生成模型正在学习说我们的语言，学习逼近我们最关心的事物的**p(x)**，并通过在噪声和结构之间权衡，揭示这种分布内外的无尽新模式。它们的创造力反过来可以用来激发我们的创造力。
- en: “Those who do not want to imitate anything, produce nothing.”
  id: totrans-145
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “那些不愿意模仿任何东西的人，什么也不会产生。”
- en: — ***Salvador Dalí***
  id: totrans-146
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: — ***萨尔瓦多·达利***
- en: It is in part daunting but also exhilarating to think about how generative models
    are already beginning to shape our sensory input, questioning and pushing the
    boundary of how we perceive the world and our minds, and how we think of creativity
    and genius.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 想到生成模型已经开始塑造我们的感官输入，质疑并推动我们对世界和心智的感知边界，以及我们对创造力和天才的理解，这一方面让人感到畏惧但也充满兴奋。
- en: 'So I think there is no better way to end this article than with the words of
    Chat-GPT, dreaming itself to be Leonardo da Vinci:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我认为用Chat-GPT的话结束这篇文章再好不过了，它梦想成为列奥纳多·达芬奇：
- en: “True genius lies not in mere imitation, but in the alchemy of blending the
    known with the unknown.”
  id: totrans-149
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “真正的天才不仅仅在于模仿，而在于将已知与未知融合的炼金术。”
- en: '***— Leonardo da Vinci/Chat-GPT***'
  id: totrans-150
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***— 列奥纳多·达芬奇/Chat-GPT***'
- en: Thanks for reading!
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: If you like my writing, please [subscribe to get my stories via mail](https://manuel-brenner.medium.com/subscribe).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢我的写作，请[订阅以通过邮件获取我的故事](https://manuel-brenner.medium.com/subscribe)。
