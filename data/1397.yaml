- en: A Gentle Intro to Chaining LLMs, Agents, and utils via LangChain
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€Šæ¸©å’Œä»‹ç»ï¼šé€šè¿‡ LangChain é“¾æ¥ LLMsã€ä»£ç†å’Œå·¥å…·ã€‹
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/a-gentle-intro-to-chaining-llms-agents-and-utils-via-langchain-16cd385fca81?source=collection_archive---------0-----------------------#2023-04-21](https://towardsdatascience.com/a-gentle-intro-to-chaining-llms-agents-and-utils-via-langchain-16cd385fca81?source=collection_archive---------0-----------------------#2023-04-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/a-gentle-intro-to-chaining-llms-agents-and-utils-via-langchain-16cd385fca81?source=collection_archive---------0-----------------------#2023-04-21](https://towardsdatascience.com/a-gentle-intro-to-chaining-llms-agents-and-utils-via-langchain-16cd385fca81?source=collection_archive---------0-----------------------#2023-04-21)
- en: '*#LLM for beginners*'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*#åˆå­¦è€…çš„ LLM*'
- en: Understand the basics of agents, tools, and prompts and some learnings along
    the way
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç†è§£ä»£ç†ã€å·¥å…·å’Œæç¤ºçš„åŸºç¡€çŸ¥è¯†ä»¥åŠä¸€äº›å­¦ä¹ ç»éªŒ
- en: '[](https://varshitasher.medium.com/?source=post_page-----16cd385fca81--------------------------------)[![Dr.
    Varshita Sher](../Images/a3f2e9bf1dc1d8cbe018e54f9341f608.png)](https://varshitasher.medium.com/?source=post_page-----16cd385fca81--------------------------------)[](https://towardsdatascience.com/?source=post_page-----16cd385fca81--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----16cd385fca81--------------------------------)
    [Dr. Varshita Sher](https://varshitasher.medium.com/?source=post_page-----16cd385fca81--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://varshitasher.medium.com/?source=post_page-----16cd385fca81--------------------------------)[![Varshita
    Sher åšå£«](../Images/a3f2e9bf1dc1d8cbe018e54f9341f608.png)](https://varshitasher.medium.com/?source=post_page-----16cd385fca81--------------------------------)[](https://towardsdatascience.com/?source=post_page-----16cd385fca81--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----16cd385fca81--------------------------------)
    [Varshita Sher åšå£«](https://varshitasher.medium.com/?source=post_page-----16cd385fca81--------------------------------)'
- en: Â·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff8ca36def59&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-intro-to-chaining-llms-agents-and-utils-via-langchain-16cd385fca81&user=Dr.+Varshita+Sher&userId=f8ca36def59&source=post_page-f8ca36def59----16cd385fca81---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----16cd385fca81--------------------------------)
    Â·20 min readÂ·Apr 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F16cd385fca81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-intro-to-chaining-llms-agents-and-utils-via-langchain-16cd385fca81&user=Dr.+Varshita+Sher&userId=f8ca36def59&source=-----16cd385fca81---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff8ca36def59&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-intro-to-chaining-llms-agents-and-utils-via-langchain-16cd385fca81&user=Dr.+Varshita+Sher&userId=f8ca36def59&source=post_page-f8ca36def59----16cd385fca81---------------------post_header-----------)
    å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----16cd385fca81--------------------------------)
    Â·20åˆ†é’Ÿé˜…è¯»Â·2023å¹´4æœˆ21æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F16cd385fca81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-intro-to-chaining-llms-agents-and-utils-via-langchain-16cd385fca81&user=Dr.+Varshita+Sher&userId=f8ca36def59&source=-----16cd385fca81---------------------clap_footer-----------)'
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F16cd385fca81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-intro-to-chaining-llms-agents-and-utils-via-langchain-16cd385fca81&source=-----16cd385fca81---------------------bookmark_footer-----------)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F16cd385fca81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-intro-to-chaining-llms-agents-and-utils-via-langchain-16cd385fca81&source=-----16cd385fca81---------------------bookmark_footer-----------)'
- en: 'Audience: For those feeling overwhelmed with the giant (yet brilliant) libraryâ€¦'
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å—ä¼—ï¼šå¯¹äºé‚£äº›è¢«åºå¤§ï¼ˆä½†å“è¶Šï¼‰åº“æ„Ÿåˆ°ä¸çŸ¥æ‰€æªçš„äººâ€¦
- en: '![](../Images/4c6806a457e762ae481cc1f29a02d4bd.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c6806a457e762ae481cc1f29a02d4bd.png)'
- en: Image generated by Author using [DALL.E 2](https://openai.com/product/dall-e-2)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…ä½¿ç”¨[DALL.E 2](https://openai.com/product/dall-e-2)ç”Ÿæˆçš„å›¾åƒ
- en: Introduction
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: Iâ€™d be lying if I said I have got the entire LangChain library covered â€” in
    fact, I am far from it. But the buzz surrounding it was enough to shake me out
    of my writing hiatus and give it a go ğŸš€.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘è¯´æˆ‘æŒæ¡äº†æ•´ä¸ª LangChain åº“ï¼Œé‚£æˆ‘å°±æ˜¯åœ¨æ’’è°â€”â€”å®é™…ä¸Šï¼Œæˆ‘è¿œè¿œæ²¡æœ‰åšåˆ°ã€‚ä½†æ˜¯ï¼Œå›´ç»•å®ƒçš„çƒ­è®®è¶³ä»¥è®©æˆ‘æ‘†è„±å†™ä½œ hiatusï¼Œå»å°è¯•ä¸€ä¸‹ ğŸš€ã€‚
- en: The initial motivation was to see what was it that LangChain was adding (on
    a practical level) that set it apart from the chatbot I built last month using
    the `ChatCompletion.create()` function from the `openai` package. Whilst doing
    so, I realized I needed to understand the building blocks for LangChain first
    before moving on to the more complex parts.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åˆçš„åŠ¨æœºæ˜¯çœ‹çœ‹LangChainåœ¨å®è·µä¸­æ·»åŠ äº†ä»€ä¹ˆï¼ˆåœ¨å®é™…æ°´å¹³ä¸Šï¼‰ï¼Œè¿™ä½¿å®ƒä¸åŒäºä¸Šä¸ªæœˆæˆ‘ç”¨`openai`åŒ…ä¸­çš„`ChatCompletion.create()`å‡½æ•°æ„å»ºçš„èŠå¤©æœºå™¨äººã€‚åœ¨è¿™æ ·åšçš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘æ„è¯†åˆ°éœ€è¦å…ˆç†è§£LangChainçš„åŸºç¡€æ„å»ºå—ï¼Œç„¶åå†è½¬å‘æ›´å¤æ‚çš„éƒ¨åˆ†ã€‚
- en: This is what this article does. Heads-up though, there will be more parts coming
    as I am truly fascinated by the library and will continue to explore to see what
    all can be built through it.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯æœ¬æ–‡æ‰€åšçš„äº‹æƒ…ã€‚è¯·æ³¨æ„ï¼Œéšç€æˆ‘å¯¹è¿™ä¸ªåº“çš„ç€è¿·å’ŒæŒç»­æ¢ç´¢ï¼Œå°†ä¼šæœ‰æ›´å¤šçš„éƒ¨åˆ†å‡ºç°ã€‚
- en: Letâ€™s begin by understanding the fundamental building blocks of LangChain â€”
    i.e. Chains. If youâ€™d like to follow along, hereâ€™s the [GitHub repo](https://github.com/V-Sher/LangChain-Tutorial).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»ç†è§£LangChainçš„åŸºæœ¬æ„å»ºå— â€”â€” å³é“¾æ¡å¼€å§‹ã€‚å¦‚æœä½ æƒ³è·Ÿè¿›ï¼Œè¯·æŸ¥çœ‹è¿™ä¸ª[GitHubä»“åº“](https://github.com/V-Sher/LangChain-Tutorial)ã€‚
- en: What are chains in LangChain?
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LangChainä¸­çš„é“¾æ¡æ˜¯ä»€ä¹ˆï¼Ÿ
- en: Chains are what you get by connecting one or more large language models (LLMs)
    in a logical way. (Chains can be built of entities other than LLMs but for now,
    letâ€™s stick with this definition for simplicity).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: é“¾æ¡æ˜¯é€šè¿‡ä»¥é€»è¾‘æ–¹å¼è¿æ¥ä¸€ä¸ªæˆ–å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è€Œå¾—åˆ°çš„ã€‚ ï¼ˆè™½ç„¶é“¾æ¡å¯ä»¥ç”±é™¤LLMsä»¥å¤–çš„å®ä½“æ„å»ºï¼Œä½†ç°åœ¨è®©æˆ‘ä»¬æš‚æ—¶ä½¿ç”¨è¿™ä¸ªå®šä¹‰ä»¥ç®€åŒ–é—®é¢˜ï¼‰ã€‚
- en: OpenAI is a type of LLM (provider) that you can use but there are others like
    Cohere, Bloom, Huggingface, etc.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAIæ˜¯ä¸€ç§LLMï¼ˆæä¾›è€…ï¼‰ï¼Œä½ å¯ä»¥ä½¿ç”¨å®ƒï¼Œä½†è¿˜æœ‰å…¶ä»–åƒCohereã€Bloomã€Huggingfaceç­‰ã€‚
- en: '*Note: Pretty much most of these LLM providers will need you to request an
    API key in order to use them. So make sure you do that before proceeding with
    the remainder of this blog. For example:*'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ³¨æ„ï¼šå‡ ä¹æ‰€æœ‰è¿™äº›LLMæä¾›è€…éƒ½éœ€è¦æ‚¨ç”³è¯·APIå¯†é’¥æ‰èƒ½ä½¿ç”¨å®ƒä»¬ã€‚æ‰€ä»¥è¯·ç¡®ä¿åœ¨ç»§ç»­é˜…è¯»æœ¬åšå®¢çš„å…¶ä½™éƒ¨åˆ†ä¹‹å‰ï¼Œæ‚¨å·²ç»è¿™æ ·åšäº†ã€‚ä¾‹å¦‚ï¼š*'
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*P.S. I am going to use OpenAI for this tutorial because I have a key with
    credits that expire in a monthâ€™s time, but feel free to replace it with any other
    LLM. The concepts covered here will be useful regardless.*'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*P.S. æˆ‘å°†åœ¨æœ¬æ•™ç¨‹ä¸­ä½¿ç”¨OpenAIï¼Œå› ä¸ºæˆ‘æœ‰ä¸€ä¸ªä¸€ä¸ªæœˆåè¿‡æœŸçš„ç§¯åˆ†å¯†é’¥ï¼Œä½†è¯·éšæ„æ›¿æ¢ä¸ºä»»ä½•å…¶ä»–LLMã€‚æ— è®ºå¦‚ä½•ï¼Œè¿™é‡Œæ¶µç›–çš„æ¦‚å¿µéƒ½å°†æ˜¯æœ‰ç”¨çš„ã€‚*'
- en: Chains can be simple (i.e. Generic) or specialized (i.e. Utility).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: é“¾æ¡å¯ä»¥ç®€å•ï¼ˆä¾‹å¦‚é€šç”¨ï¼‰æˆ–ä¸“ä¸šåŒ–ï¼ˆä¾‹å¦‚å®ç”¨ï¼‰ã€‚
- en: 'Generic â€” A single LLM is the simplest chain. It takes an input prompt and
    the name of the LLM and then uses the LLM for text generation (i.e. output for
    the prompt). Hereâ€™s an example:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€šç”¨ â€” å•ä¸ªLLMæ˜¯æœ€ç®€å•çš„é“¾æ¡ã€‚å®ƒæ¥å—ä¸€ä¸ªè¾“å…¥æç¤ºå’ŒLLMçš„åç§°ï¼Œç„¶åä½¿ç”¨LLMè¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼ˆå³è¾“å‡ºæç¤ºçš„ç»“æœï¼‰ã€‚è¿™é‡Œæ˜¯ä¸€ä¸ªä¾‹å­ï¼š
- en: Letâ€™s build a basic chain â€” create a prompt and get a prediction
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªåŸºæœ¬çš„é“¾æ¡ â€”â€” åˆ›å»ºä¸€ä¸ªæç¤ºå¹¶è·å–é¢„æµ‹ç»“æœ
- en: Prompt creation (using `PromptTemplate`) is a bit fancy in Lanchain but this
    is probably because there are quite a few different ways prompts can be created
    depending on the use case (we will cover `AIMessagePromptTemplate`,
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Lanchainä¸­ï¼Œä½¿ç”¨`PromptTemplate`åˆ›å»ºæç¤ºï¼ˆPromptï¼‰æœ‰ç‚¹èŠ±å“¨ï¼Œä½†è¿™å¯èƒ½æ˜¯å› ä¸ºæ ¹æ®ç”¨ä¾‹çš„ä¸åŒï¼Œå¯ä»¥æœ‰å¤šç§ä¸åŒçš„æ–¹å¼æ¥åˆ›å»ºæç¤ºï¼ˆæˆ‘ä»¬å°†æ¶µç›–`AIMessagePromptTemplate`ç­‰ç­‰ï¼‰ã€‚
- en: '`HumanMessagePromptTemplate` etc. in the next blog post). Hereâ€™s a simple one
    for now:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`HumanMessagePromptTemplate`ç­‰ç­‰å°†åœ¨ä¸‹ä¸€ç¯‡åšå®¢æ–‡ç« ä¸­æ¶µç›–ã€‚ç°åœ¨å…ˆçœ‹ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼š'
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Note: If you require multiple* `*input_variables*`*, for instance:* `*input_variables=["product",
    "audience"]*` *for a template such as* `*â€œWhat is a good name for a company that
    makes {product} for {audience}â€*`*, you need to do* `print(prompt.format(product="podcast
    player", audience="childrenâ€)` to get the updated prompt.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ³¨æ„ï¼šå¦‚æœæ‚¨éœ€è¦å¤šä¸ª* `*input_variables*`ï¼Œä¾‹å¦‚ï¼š* `*input_variables=["product", "audience"]*`
    *ç”¨äºæ¨¡æ¿ï¼Œä¾‹å¦‚* `*â€œä¸€ä¸ªå…¬å¸çš„å¥½åå­—ï¼Œä¸º{product}åˆ¶ä½œ{audience}â€*`ï¼Œåˆ™éœ€è¦æ‰§è¡Œ* `print(prompt.format(product="podcast
    player", audience="childrenâ€)` *ä»¥è·å–æ›´æ–°åçš„æç¤ºã€‚'
- en: Once you have built a prompt, we can call the desired LLM with it. To do so,
    we create an `LLMChain` instance (in our case, we use `OpenAI`'s large language
    model `text-davinci-003`). To get the prediction (i.e. AI-generated text), we
    use `run` function with the name of the `product`.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ‚¨å»ºç«‹äº†ä¸€ä¸ªæç¤ºï¼Œæˆ‘ä»¬å°±å¯ä»¥è°ƒç”¨æ‰€éœ€çš„LLMã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ª`LLMChain`å®ä¾‹ï¼ˆåœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨`OpenAI`çš„å¤§å‹è¯­è¨€æ¨¡å‹`text-davinci-003`ï¼‰ã€‚è¦è·å–é¢„æµ‹ç»“æœï¼ˆå³AIç”Ÿæˆçš„æ–‡æœ¬ï¼‰ï¼Œæˆ‘ä»¬ä½¿ç”¨`run`å‡½æ•°å’Œ`product`çš„åç§°ã€‚
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If you had more than one input_variables, then you wonâ€™t be able to use `run`.
    Instead, youâ€™ll have to pass all the variables as a `dict`. For example, `llmchain({â€œproductâ€:
    â€œpodcast playerâ€, â€œaudienceâ€: â€œchildrenâ€})`.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¦‚æœä½ æœ‰å¤šä¸ªè¾“å…¥å˜é‡ï¼Œé‚£ä¹ˆå°±ä¸èƒ½ä½¿ç”¨`run`ã€‚ç›¸åï¼Œä½ éœ€è¦å°†æ‰€æœ‰å˜é‡ä½œä¸º`dict`ä¼ é€’ã€‚ä¾‹å¦‚ï¼Œ`llmchain({"product": "podcast
    player", "audience": "children"})`ã€‚'
- en: '*Note 1: According to* [*OpenAI*](https://openai.com/blog/introducing-chatgpt-and-whisper-apis),`*davinci*`
    *text-generation models are 10x more expensive than their chat counterparts i.e*
    `*gpt-3.5-turbo*`*, so I tried to switch from a text model to a chat model (i.e.
    from* `*OpenAI*` *to* `*ChatOpenAI*`*) and the results are pretty much the same.*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ³¨æ„ 1ï¼šæ ¹æ®* [*OpenAI*](https://openai.com/blog/introducing-chatgpt-and-whisper-apis)ï¼Œ`*davinci*`
    *æ–‡æœ¬ç”Ÿæˆæ¨¡å‹çš„è´¹ç”¨æ˜¯å…¶èŠå¤©å¯¹åº”æ¨¡å‹çš„ 10 å€ï¼Œå³* `*gpt-3.5-turbo*`*ï¼Œå› æ­¤æˆ‘å°è¯•ä»æ–‡æœ¬æ¨¡å‹åˆ‡æ¢åˆ°èŠå¤©æ¨¡å‹ï¼ˆå³ä»* `*OpenAI*`
    *åˆ°* `*ChatOpenAI*`*ï¼‰ï¼Œç»“æœå·®åˆ«ä¸å¤§ã€‚*'
- en: '*Note 2: You might see some tutorials using* `*OpenAIChat*`*instead of* `*ChatOpenAI*`*.
    The former is* [*deprecated*](https://github.com/hwchase17/langchain/issues/1556#issuecomment-1463224442)
    *and will no longer be supported and we are supposed to use* `*ChatOpenAI*`*.*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ³¨æ„ 2ï¼šä½ å¯èƒ½ä¼šçœ‹åˆ°ä¸€äº›æ•™ç¨‹ä½¿ç”¨* `*OpenAIChat*`*è€Œä¸æ˜¯* `*ChatOpenAI*`*ã€‚å‰è€…å·²ç»* [*å¼ƒç”¨*](https://github.com/hwchase17/langchain/issues/1556#issuecomment-1463224442)
    *å¹¶ä¸”å°†ä¸å†å—æ”¯æŒï¼Œæˆ‘ä»¬åº”ä½¿ç”¨* `*ChatOpenAI*`*ã€‚*'
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This concludes our section on simple chains. It is important to note that we
    rarely use generic chains as standalone chains. More often they are used as building
    blocks for Utility chains (as we will see next).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™éƒ¨åˆ†å…³äºç®€å•é“¾çš„ä»‹ç»åˆ°æ­¤ä¸ºæ­¢ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å¾ˆå°‘å°†é€šç”¨é“¾ä½œä¸ºç‹¬ç«‹é“¾ä½¿ç”¨ã€‚æ›´å¸¸è§çš„æ˜¯å®ƒä»¬ä½œä¸ºå®ç”¨é“¾çš„æ„å»ºå—ï¼ˆæ­£å¦‚æˆ‘ä»¬æ¥ä¸‹æ¥ä¼šçœ‹åˆ°çš„ï¼‰ã€‚
- en: 2\. Utility â€” These are specialized chains, comprised of many LLMs to help solve
    a specific task. For example, LangChain supports some end-to-end chains (such
    as `[AnalyzeDocumentChain](https://python.langchain.com/docs/use_cases/question_answering/how_to/analyze_document)`
    for summarization, QnA, etc) and some specific ones (such as `[GraphQnAChain](https://python.langchain.com/en/latest/modules/chains/index_examples/graph_qa.html#querying-the-graph)`
    for creating, querying, and saving graphs). We will look at one specific chain
    called `PalChain` in this tutorial for digging deeper.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. å®ç”¨å·¥å…· â€” è¿™äº›æ˜¯ä¸“é—¨çš„é“¾ï¼Œç”±è®¸å¤š LLM ç»„æˆï¼Œä»¥å¸®åŠ©è§£å†³ç‰¹å®šä»»åŠ¡ã€‚ä¾‹å¦‚ï¼ŒLangChain æ”¯æŒä¸€äº›ç«¯åˆ°ç«¯çš„é“¾ï¼ˆå¦‚`[AnalyzeDocumentChain](https://python.langchain.com/docs/use_cases/question_answering/how_to/analyze_document)`
    ç”¨äºæ€»ç»“ã€é—®ç­”ç­‰ï¼‰å’Œä¸€äº›ç‰¹å®šçš„é“¾ï¼ˆå¦‚`[GraphQnAChain](https://python.langchain.com/en/latest/modules/chains/index_examples/graph_qa.html#querying-the-graph)`
    ç”¨äºåˆ›å»ºã€æŸ¥è¯¢å’Œä¿å­˜å›¾å½¢ï¼‰ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨ä¸€ä¸ªåä¸º `PalChain` çš„ç‰¹å®šé“¾ã€‚
- en: PAL stands for [Programme Aided Language Model](https://arxiv.org/pdf/2211.10435.pdf).
    `PALChain` reads complex math problems (described in natural language) and generates
    programs (for solving the math problem) as the intermediate reasoning steps, but
    offloads the solution step to a runtime such as a Python interpreter.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: PAL ä»£è¡¨ [ç¨‹åºè¾…åŠ©è¯­è¨€æ¨¡å‹](https://arxiv.org/pdf/2211.10435.pdf)ã€‚`PALChain` è¯»å–å¤æ‚çš„æ•°å­¦é—®é¢˜ï¼ˆç”¨è‡ªç„¶è¯­è¨€æè¿°ï¼‰å¹¶ç”Ÿæˆç¨‹åºï¼ˆç”¨äºè§£å†³æ•°å­¦é—®é¢˜ï¼‰ä½œä¸ºä¸­é—´æ¨ç†æ­¥éª¤ï¼Œä½†å°†è§£å†³æ­¥éª¤å§”æ‰˜ç»™å¦‚
    Python è§£é‡Šå™¨ç­‰è¿è¡Œæ—¶ã€‚
- en: 'To confirm this is in fact true, we can inspect the `_call()` in the base code
    [here](https://github.com/hwchase17/langchain/blob/master/langchain/chains/pal/base.py).
    Under the hood, we can see this chain:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç¡®è®¤è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥æ£€æŸ¥åŸºç¡€ä»£ç ä¸­çš„ `_call()` [è¿™é‡Œ](https://github.com/hwchase17/langchain/blob/master/langchain/chains/pal/base.py)ã€‚åœ¨åº•å±‚ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™ä¸ªé“¾ï¼š
- en: '[first uses a generic](https://github.com/hwchase17/langchain/blob/master/langchain/chains/pal/base.py#L58)
    `[LLMChain](https://github.com/hwchase17/langchain/blob/master/langchain/chains/pal/base.py#L58)`
    [to understand the query](https://github.com/hwchase17/langchain/blob/master/langchain/chains/pal/base.py#L58)
    we pass to it and get a prediction. Thus, this chain requires passing an LLM at
    the time of initializing (we are going to use the same OpenAI LLM as before).'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[é¦–å…ˆä½¿ç”¨ä¸€ä¸ªé€šç”¨é“¾](https://github.com/hwchase17/langchain/blob/master/langchain/chains/pal/base.py#L58)
    `[LLMChain](https://github.com/hwchase17/langchain/blob/master/langchain/chains/pal/base.py#L58)`
    [æ¥ç†è§£æˆ‘ä»¬ä¼ é€’çš„æŸ¥è¯¢](https://github.com/hwchase17/langchain/blob/master/langchain/chains/pal/base.py#L58)
    å¹¶è·å–é¢„æµ‹ã€‚å› æ­¤ï¼Œè¿™ä¸ªé“¾åœ¨åˆå§‹åŒ–æ—¶éœ€è¦ä¼ é€’ä¸€ä¸ª LLMï¼ˆæˆ‘ä»¬å°†ä½¿ç”¨ä¹‹å‰çš„ OpenAI LLMï¼‰ã€‚'
- en: s[econd, it uses Python REPL](https://github.com/hwchase17/langchain/blob/master/langchain/chains/pal/base.py#L63-L64)
    to solve the function/program outputted by the LLM.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ç¬¬äºŒï¼Œä½¿ç”¨ Python REPL](https://github.com/hwchase17/langchain/blob/master/langchain/chains/pal/base.py#L63-L64)
    æ¥è§£å†³ LLM è¾“å‡ºçš„å‡½æ•°/ç¨‹åºã€‚'
- en: '*P.S. It is a good practice to inspect* `*_call()*` *in* `*base.py*` *for any
    of the chains in LangChain to see how things are working under the hood.*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*é™„æ³¨ï¼šæ£€æŸ¥* `*_call()*` *åœ¨* `*base.py*` *ä¸­æ˜¯ä¸€ä¸ªå¥½ä¹ æƒ¯ï¼Œå¯ä»¥æŸ¥çœ‹ LangChain ä¸­çš„ä»»ä½•é“¾å¦‚ä½•åœ¨åº•å±‚å·¥ä½œã€‚*'
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*Note1:* `*verbose*` *can be set to* `*False*` *if you do not need to see the
    intermediate step.*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ³¨æ„1ï¼šå¦‚æœä½ ä¸éœ€è¦çœ‹åˆ°ä¸­é—´æ­¥éª¤ï¼Œ`*verbose*` *å¯ä»¥è®¾ç½®ä¸º* `*False*`*ã€‚*'
- en: Now some of you may be wondering â€” *but what about the prompt? We certainly
    didnâ€™t pass one as we did for the generic* `*llmchain*` *we built.* The fact is,
    it is automatically loaded when using `.from_math_prompt()`. You can check the
    default prompt using `palchain.prompt.template` or you can directly inspect the
    prompt file [here](https://github.com/hwchase17/langchain/blob/master/langchain/chains/pal/math_prompt.py).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæœ‰äº›äººå¯èƒ½ä¼šæƒ³ â€” *ä½†æç¤ºå‘¢ï¼Ÿæˆ‘ä»¬è‚¯å®šæ²¡æœ‰åƒæˆ‘ä»¬å»ºç«‹çš„é€šç”¨* `*llmchain*` *é‚£æ ·ä¼ é€’å®ƒã€‚* å®é™…ä¸Šï¼Œå½“ä½¿ç”¨`.from_math_prompt()`æ—¶ï¼Œå®ƒä¼šè‡ªåŠ¨åŠ è½½ã€‚æ‚¨å¯ä»¥ä½¿ç”¨`palchain.prompt.template`æ£€æŸ¥é»˜è®¤æç¤ºï¼Œæˆ–è€…ç›´æ¥æŸ¥çœ‹æç¤ºæ–‡ä»¶[è¿™é‡Œ](https://github.com/hwchase17/langchain/blob/master/langchain/chains/pal/math_prompt.py)ã€‚
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*Note: Most of the utility chains will have their prompts pre-defined as part
    of the library (check them out* [*here*](https://github.com/hwchase17/langchain/tree/master/langchain/chains)*).
    They are, at times, quite detailed (read: lots of tokens) so there is definitely
    a trade-off between cost and the quality of response from the LLM.*'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ³¨æ„ï¼šå¤§å¤šæ•°å®ç”¨é“¾æ¡çš„æç¤ºä½œä¸ºåº“çš„ä¸€éƒ¨åˆ†æ˜¯é¢„å®šä¹‰çš„ï¼ˆåœ¨è¿™é‡ŒæŸ¥çœ‹* [*è¿™é‡Œ*](https://github.com/hwchase17/langchain/tree/master/langchain/chains)*ï¼‰ã€‚å®ƒä»¬æœ‰æ—¶éå¸¸è¯¦ç»†ï¼ˆå³ï¼šæœ‰å¾ˆå¤šä»¤ç‰Œï¼‰ï¼Œå› æ­¤åœ¨æˆæœ¬å’ŒLLMå“åº”è´¨é‡ä¹‹é—´è‚¯å®šå­˜åœ¨æƒè¡¡ã€‚*'
- en: Are there any Chains that donâ€™t need LLMs and prompts?
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ˜¯å¦å­˜åœ¨ä¸éœ€è¦LLMå’Œæç¤ºçš„é“¾æ¡ï¼Ÿ
- en: '*Even though PalChain requires an LLM (and a corresponding prompt) to parse
    the userâ€™s question written in natural language, there are some chains in LangChain
    that donâ€™t need one. These are mainly transformation chains that preprocess the
    prompt, such as removing extra spaces, before inputting it into the LLM. You can
    see another example* [*here*](https://python.langchain.com/en/latest/modules/chains/generic/transformation.html)*.*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*å°½ç®¡PalChainéœ€è¦ä¸€ä¸ªLLMï¼ˆä»¥åŠç›¸åº”çš„æç¤ºï¼‰æ¥è§£æç”¨æˆ·ç”¨è‡ªç„¶è¯­è¨€ç¼–å†™çš„é—®é¢˜ï¼Œä½†åœ¨LangChainä¸­æœ‰ä¸€äº›é“¾æ¡ä¸éœ€è¦ã€‚è¿™äº›ä¸»è¦æ˜¯é¢„å¤„ç†æç¤ºçš„è½¬æ¢é“¾æ¡ï¼Œä¾‹å¦‚åˆ é™¤é¢å¤–çš„ç©ºæ ¼ï¼Œç„¶åå°†å…¶è¾“å…¥LLMã€‚ä½ å¯ä»¥åœ¨å¦ä¸€ä¸ªä¾‹å­ä¸­çœ‹åˆ°*
    [*è¿™é‡Œ*](https://python.langchain.com/en/latest/modules/chains/generic/transformation.html)*ã€‚*'
- en: Can we get to the good part and start creating chains?
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬èƒ½è¿›å…¥ç²¾å½©éƒ¨åˆ†å¹¶å¼€å§‹åˆ›å»ºé“¾æ¡å—ï¼Ÿ
- en: Of course, we can! We have all the basic building blocks we need to start chaining
    together LLMs logically such that input from one can be fed to the next. To do
    so, we will use `SimpleSequentialChain`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶å¯ä»¥ï¼æˆ‘ä»¬å·²ç»æœ‰äº†å¼€å§‹é€»è¾‘åœ°å°†LLMè¿æ¥åœ¨ä¸€èµ·çš„åŸºæœ¬æ„å»ºå—ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`SimpleSequentialChain`ã€‚
- en: The documentation has some great examples on this, for example, you can see
    [here](https://python.langchain.com/en/latest/modules/chains/generic/transformation.html)
    how to have two chains combined where chain#1 is used to clean the prompt (remove
    extra whitespaces, shorten prompt, etc) and chain#2 is used to call an LLM with
    this clean prompt. Hereâ€™s [another one](https://js.langchain.com/docs/modules/chains/foundational/sequential_chains/#simplesequentialchain)
    where chain#1 is used to generate a synopsis for a play and chain#2 is used to
    write a review based on this synopsis.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡æ¡£ä¸­æœ‰ä¸€äº›å¾ˆå¥½çš„ä¾‹å­ï¼Œä¾‹å¦‚ï¼Œä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://python.langchain.com/en/latest/modules/chains/generic/transformation.html)çœ‹åˆ°å¦‚ä½•ç»„åˆä¸¤ä¸ªé“¾æ¡ï¼Œå…¶ä¸­é“¾æ¡#1ç”¨äºæ¸…ç†æç¤ºï¼ˆåˆ é™¤é¢å¤–ç©ºæ ¼ï¼Œç¼©çŸ­æç¤ºç­‰ï¼‰ï¼Œè€Œé“¾æ¡#2ç”¨äºä½¿ç”¨è¿™ä¸ªå¹²å‡€çš„æç¤ºè°ƒç”¨LLMã€‚è¿™é‡Œè¿˜æœ‰[å¦ä¸€ä¸ªä¾‹å­](https://js.langchain.com/docs/modules/chains/foundational/sequential_chains/#simplesequentialchain)ï¼Œå…¶ä¸­é“¾æ¡#1ç”¨äºä¸ºä¸€éƒ¨æˆå‰§ç”Ÿæˆç®€ä»‹ï¼Œè€Œé“¾æ¡#2åˆ™ç”¨äºåŸºäºæ­¤ç®€ä»‹æ’°å†™è¯„è®ºã€‚
- en: While these are excellent examples, I want to focus on something else. If you
    remember before, I mentioned that chains can be composed of entities other than
    LLMs. More specifically, I am interested in chaining agents and LLMs together.
    *But first, what are agents?*
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶è¿™äº›éƒ½æ˜¯å¾ˆå¥½çš„ä¾‹å­ï¼Œä½†æˆ‘æƒ³ä¸“æ³¨äºå…¶ä»–äº‹æƒ…ã€‚å¦‚æœä½ è¿˜è®°å¾—ï¼Œæˆ‘æåˆ°é“¾æ¡å¯ä»¥ç”±é™¤äº†LLMä»¥å¤–çš„å®ä½“ç»„æˆã€‚æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘å¯¹å°†ä»£ç†å’ŒLLMç»„åˆåœ¨ä¸€èµ·å¾ˆæ„Ÿå…´è¶£ã€‚*ä½†é¦–å…ˆï¼Œä»€ä¹ˆæ˜¯ä»£ç†ï¼Ÿ*
- en: Using agents for dynamically calling LLMs
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä»£ç†åŠ¨æ€è°ƒç”¨LLM
- en: It will be much easier to explain what an agent does vs. what it is.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè§£é‡Šä»£ç†çš„ä½œç”¨ä¸å…¶æ˜¯ä»€ä¹ˆï¼Œå°†ä¼šæ›´åŠ å®¹æ˜“ã€‚
- en: Say, we want to know the weather forecast for tomorrow. If were to use the simple
    ChatGPT API and give it a prompt `Show me the weather for tomorrow in London`,
    it wonâ€™t know the answer because it does not have access to real-time data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬æƒ³çŸ¥é“æ˜å¤©çš„å¤©æ°”é¢„æŠ¥ã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨ç®€å•çš„ChatGPT APIå¹¶ç»™å®ƒä¸€ä¸ªæç¤º`Show me the weather for tomorrow
    in London`ï¼Œå®ƒä¸ä¼šçŸ¥é“ç­”æ¡ˆï¼Œå› ä¸ºå®ƒæ— æ³•è®¿é—®å®æ—¶æ•°æ®ã€‚
- en: '![](../Images/a379c0d1313089f343dc25f6510660a3.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a379c0d1313089f343dc25f6510660a3.png)'
- en: Wouldnâ€™t it be useful if we had an arrangement where we could utilize an LLM
    for understanding our query (i.e prompt) in natural language and then call the
    weather API on our behalf to fetch the data needed? This is exactly what an agent
    does (amongst other things, of course).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬èƒ½æœ‰ä¸€ä¸ªå®‰æ’ï¼Œåˆ©ç”¨ LLM ç†è§£æˆ‘ä»¬çš„æŸ¥è¯¢ï¼ˆå³æç¤ºï¼‰ï¼Œç„¶åä»£è¡¨æˆ‘ä»¬è°ƒç”¨å¤©æ°” API æ¥è·å–æ‰€éœ€æ•°æ®ï¼Œé‚£ä¸æ˜¯å¾ˆæœ‰ç”¨å—ï¼Ÿè¿™æ­£æ˜¯ä»£ç†æ‰€åšçš„ï¼ˆå½“ç„¶è¿˜æœ‰å…¶ä»–äº‹æƒ…ï¼‰ã€‚
- en: An agent has access to an LLM and a suite of tools for example Google Search,
    Python REPL, math calculator, weather APIs, etc.
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä»£ç†å¯ä»¥è®¿é—® LLM å’Œä¸€å¥—å·¥å…·ï¼Œä¾‹å¦‚ Google æœç´¢ã€Python REPLã€æ•°å­¦è®¡ç®—å™¨ã€å¤©æ°” API ç­‰ã€‚
- en: There are quite a few agents that LangChain supports â€” see [here](https://python.langchain.com/docs/modules/agents/agent_types/)
    for the complete list, but quite frankly the most common one I came across in
    tutorials and YT videos was `zero-shot-react-description`. This agent uses [ReAct](https://arxiv.org/abs/2210.03629)
    (Reason + Act) framework to pick the most usable tool (from a list of tools),
    based on what the input query is.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain æ”¯æŒå¾ˆå¤šä»£ç†â€”â€”å®Œæ•´åˆ—è¡¨è¯·è§[è¿™é‡Œ](https://python.langchain.com/docs/modules/agents/agent_types/)ï¼Œä½†å¦ç‡è¯´ï¼Œæˆ‘åœ¨æ•™ç¨‹å’Œ
    YouTube è§†é¢‘ä¸­æœ€å¸¸è§çš„ä»£ç†æ˜¯ `zero-shot-react-description`ã€‚è¿™ä¸ªä»£ç†ä½¿ç”¨äº†[ReAct](https://arxiv.org/abs/2210.03629)ï¼ˆReason
    + Actï¼‰æ¡†æ¶ï¼Œæ ¹æ®è¾“å…¥æŸ¥è¯¢ä»å·¥å…·åˆ—è¡¨ä¸­é€‰æ‹©æœ€åˆé€‚çš„å·¥å…·ã€‚
- en: '*P.S.:* [*Hereâ€™s*](https://tsmatz.wordpress.com/2023/03/07/react-with-openai-gpt-and-langchain/)
    *a nice article that goes in-depth into the ReAct framework.*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*P.S.:* [*è¿™é‡Œ*](https://tsmatz.wordpress.com/2023/03/07/react-with-openai-gpt-and-langchain/)
    *æœ‰ä¸€ç¯‡æ·±å…¥æ¢è®¨ ReAct æ¡†æ¶çš„å¥½æ–‡ç« ã€‚*'
- en: Letâ€™s initialize an agent using `initialize_agent` and pass it the `tools` and
    `LLM` it needs. Thereâ€™s a long list of tools available [here](https://python.langchain.com/docs/integrations/tools/)
    that an agent can use to interact with the outside world. For our example, we
    are using the same math-solving tool as above, called `pal-math`. This one requires
    an LLM at the time of initialization, so we pass to it the same OpenAI LLM instance
    as before.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä½¿ç”¨ `initialize_agent` åˆå§‹åŒ–ä¸€ä¸ªä»£ç†ï¼Œå¹¶ä¼ é€’å®ƒæ‰€éœ€çš„ `tools` å’Œ `LLM`ã€‚ä»£ç†å¯ä»¥ä½¿ç”¨çš„å·¥å…·æ¸…å•å¯ä»¥åœ¨[è¿™é‡Œ](https://python.langchain.com/docs/integrations/tools/)æ‰¾åˆ°ã€‚å¯¹äºæˆ‘ä»¬çš„ä¾‹å­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸Šé¢æåˆ°çš„åŒä¸€ä¸ªæ•°å­¦è§£å†³å·¥å…·ï¼Œå«åš
    `pal-math`ã€‚è¿™ä¸ªå·¥å…·åœ¨åˆå§‹åŒ–æ—¶éœ€è¦ä¸€ä¸ª LLMï¼Œå› æ­¤æˆ‘ä»¬ä¼ é€’ç»™å®ƒä¹‹å‰ç›¸åŒçš„ OpenAI LLM å®ä¾‹ã€‚
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Letâ€™s test it out on the same example as above:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åœ¨ä¸Šè¿°ç›¸åŒçš„ä¾‹å­ä¸Šæµ‹è¯•ä¸€ä¸‹ï¼š
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '*Note 1: At each step, youâ€™ll notice that an agent does one of three things
    â€” it either has an* `*observation*`*, a* `*thought*`*, or it takes an* `*action*`*.
    This is mainly due to the ReAct framework and the associated prompt that the agent
    is using:*'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ³¨æ„1ï¼šåœ¨æ¯ä¸€æ­¥ï¼Œä½ ä¼šæ³¨æ„åˆ°ä»£ç†åšäº†ä¸‰ä»¶äº‹ä¹‹ä¸€â€”â€”å®ƒè¦ä¹ˆæœ‰ä¸€ä¸ª* `*observation*`*ï¼Œè¦ä¹ˆæœ‰ä¸€ä¸ª* `*thought*`*ï¼Œè¦ä¹ˆé‡‡å–ä¸€ä¸ª*
    `*action*`*ã€‚è¿™ä¸»è¦æ˜¯ç”±äº ReAct æ¡†æ¶å’Œä»£ç†ä½¿ç”¨çš„ç›¸å…³æç¤ºï¼š*'
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '*Note2: You might be wondering whatâ€™s the point of getting an agent to do the
    same thing that an LLM can do. Some applications will require not just a predetermined
    chain of calls to LLMs/other tools, but potentially an unknown chain that depends
    on the userâ€™s input [*[*Source*](https://python.langchain.com/en/latest/modules/agents.html#agents)*].
    In these types of chains, there is an â€œagentâ€ which has access to a suite of tools.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ³¨æ„2ï¼šä½ å¯èƒ½ä¼šæƒ³ï¼Œä¸ºä»€ä¹ˆè¦è®©ä»£ç†åš LLM å¯ä»¥åšçš„äº‹æƒ…ã€‚ä¸€äº›åº”ç”¨ä¸ä»…éœ€è¦ä¸€ä¸ªé¢„å®šçš„ LLM/å…¶ä»–å·¥å…·è°ƒç”¨é“¾ï¼Œå¯èƒ½è¿˜éœ€è¦ä¸€ä¸ªå–å†³äºç”¨æˆ·è¾“å…¥çš„æœªçŸ¥é“¾
    [*[*æ¥æº*](https://python.langchain.com/en/latest/modules/agents.html#agents)*]ã€‚åœ¨è¿™äº›ç±»å‹çš„é“¾ä¸­ï¼Œæœ‰ä¸€ä¸ªâ€œä»£ç†â€ï¼Œå¯ä»¥è®¿é—®ä¸€å¥—å·¥å…·ã€‚*'
- en: For instance,* [*hereâ€™s*](https://python.langchain.com/en/latest/modules/agents/agent_executors/examples/agent_vectorstore.html#create-the-agent)
    *an example of an agent that can fetch the correct documents (from the vectorstores)
    for* `*RetrievalQAChain*` *depending on whether the question refers to document
    A or document B.*
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œ* [*è¿™æ˜¯*](https://python.langchain.com/en/latest/modules/agents/agent_executors/examples/agent_vectorstore.html#create-the-agent)
    *ä¸€ä¸ªä»£ç†çš„ç¤ºä¾‹ï¼Œå®ƒå¯ä»¥æ ¹æ®é—®é¢˜æ˜¯æŒ‡æ–‡æ¡£ A è¿˜æ˜¯æ–‡æ¡£ Bï¼Œè·å–æ­£ç¡®çš„æ–‡æ¡£ï¼ˆä»å‘é‡å­˜å‚¨ä¸­ï¼‰ã€‚*
- en: For fun, I tried making the input question more complex (using Demi Mooreâ€™s
    age as a placeholder for Dadâ€™s actual age).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æœ‰è¶£ï¼Œæˆ‘å°è¯•ä½¿è¾“å…¥é—®é¢˜æ›´å¤æ‚ï¼ˆç”¨ Demi Moore çš„å¹´é¾„ä½œä¸º Dad å®é™…å¹´é¾„çš„å ä½ç¬¦ï¼‰ã€‚
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Unfortunately, the answer was slightly off as the agent was not using the latest
    age for Demi Moore (since Open AI models were trained on data until 2020). This
    can be easily fixed by including another tool â€”
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å¹¸çš„æ˜¯ï¼Œç­”æ¡ˆæœ‰äº›åå·®ï¼Œå› ä¸ºä»£ç†æ²¡æœ‰ä½¿ç”¨æœ€æ–°çš„ Demi Moore å¹´é¾„ï¼ˆç”±äº OpenAI æ¨¡å‹çš„è®­ç»ƒæ•°æ®æˆªè‡³åˆ° 2020 å¹´ï¼‰ã€‚è¿™å¯ä»¥é€šè¿‡åŒ…å«å¦ä¸€ä¸ªå·¥å…·è½»æ¾ä¿®å¤â€”â€”
- en: '`tools = load_tools([â€œpal-mathâ€, **"serpapi"**], llm=llm)`. `serpapi` is useful
    for answering questions about current events.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`tools = load_tools([â€œpal-mathâ€, **"serpapi"**], llm=llm)`ã€‚`serpapi` å¯¹äºå›ç­”å½“å‰äº‹ä»¶çš„é—®é¢˜éå¸¸æœ‰ç”¨ã€‚'
- en: '*Note: It is important to add as many tools as you think may be relevant to
    the user query. The problem with using a single tool is that the agent keeps trying
    to use the same tool even if itâ€™s not the most relevant for a particular observation/action
    step.*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ³¨æ„: æ·»åŠ å°½å¯èƒ½å¤šçš„ç›¸å…³å·¥å…·å¯¹ç”¨æˆ·æŸ¥è¯¢æ˜¯å¾ˆé‡è¦çš„ã€‚ä½¿ç”¨å•ä¸€å·¥å…·çš„é—®é¢˜åœ¨äºï¼Œå³ä½¿å®ƒä¸é€‚ç”¨äºç‰¹å®šçš„è§‚å¯Ÿ/è¡ŒåŠ¨æ­¥éª¤ï¼Œä»£ç†ä¹Ÿä¼šç»§ç»­å°è¯•ä½¿ç”¨ç›¸åŒçš„å·¥å…·ã€‚*'
- en: Hereâ€™s another example of a tool you can use â€” `podcast-api`. You need to [get
    your own API key](https://www.listennotes.com/api/pricing/) and plug it into the
    code below.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å¦ä¸€ä¸ªä½ å¯ä»¥ä½¿ç”¨çš„å·¥å…·ç¤ºä¾‹â€”â€”`podcast-api`ã€‚ä½ éœ€è¦[è·å–ä½ è‡ªå·±çš„ API å¯†é’¥](https://www.listennotes.com/api/pricing/)å¹¶å°†å…¶æ’å…¥ä¸‹é¢çš„ä»£ç ä¸­ã€‚
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '*Note1: There is a* [*known error*](https://github.com/hwchase17/langchain/pull/1833)
    *with using this API where you might see,* `*openai.error.InvalidRequestError:
    This modelâ€™s maximum context length is 4097 tokens, however you requested XXX
    tokens (XX in your prompt; XX for the completion). Please reduce your prompt;
    or completion length.*` *This happens when the response returned by the API might
    be too big. To work around this, the documentation suggests returning fewer search
    results, for example, by updating the question to* `"Show me episodes for money
    saving tips, return only 1 result"`.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ³¨æ„1: æœ‰ä¸€ä¸ª* [*å·²çŸ¥é”™è¯¯*](https://github.com/hwchase17/langchain/pull/1833) *ï¼Œåœ¨ä½¿ç”¨è¿™ä¸ª
    API æ—¶ä½ å¯èƒ½ä¼šçœ‹åˆ°ï¼Œ`*openai.error.InvalidRequestError: This modelâ€™s maximum context length
    is 4097 tokens, however you requested XXX tokens (XX in your prompt; XX for the
    completion). Please reduce your prompt; or completion length.*` *å½“ API è¿”å›çš„å“åº”å¯èƒ½è¿‡å¤§æ—¶ä¼šå‘ç”Ÿè¿™ç§æƒ…å†µã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ–‡æ¡£å»ºè®®è¿”å›æ›´å°‘çš„æœç´¢ç»“æœï¼Œä¾‹å¦‚ï¼Œé€šè¿‡å°†é—®é¢˜æ›´æ–°ä¸º*
    â€œShow me episodes for money saving tips, return only 1 resultâ€ã€‚'
- en: '*Note2: While tinkering around with this tool, I noticed some inconsistencies.
    The responses arenâ€™t always complete the first time around, for instance here
    are the input and responses from two consecutive runs:*'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ³¨æ„2: åœ¨ä½¿ç”¨è¿™ä¸ªå·¥å…·æ—¶ï¼Œæˆ‘æ³¨æ„åˆ°äº†ä¸€äº›ä¸ä¸€è‡´çš„åœ°æ–¹ã€‚å“åº”ç¬¬ä¸€æ¬¡ç”Ÿæˆæ—¶å¹¶ä¸æ€»æ˜¯å®Œæ•´çš„ï¼Œä¾‹å¦‚ï¼Œä»¥ä¸‹æ˜¯ä¸¤æ¬¡è¿ç»­è¿è¡Œçš„è¾“å…¥å’Œå“åº”ï¼š*'
- en: '*Input: â€œPodcasts for getting better at Frenchâ€*'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¾“å…¥: â€œæé«˜æ³•è¯­æ°´å¹³çš„æ’­å®¢â€*'
- en: '*Response 1: â€œThe best podcast for learning French is the one with the highest
    review score.â€'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›åº” 1: â€œå­¦ä¹ æ³•è¯­çš„æœ€ä½³æ’­å®¢æ˜¯è¯„ä»·åˆ†æ•°æœ€é«˜çš„é‚£ä¸ªã€‚â€*'
- en: 'Response 2: â€˜The best podcast for learning French is â€œFrenchPod101â€.*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›åº” 2: â€œå­¦ä¹ æ³•è¯­çš„æœ€ä½³æ’­å®¢æ˜¯â€˜FrenchPod101â€™ã€‚â€*'
- en: Under the hood, the tool is first using an LLMChain for [building the API URL](https://github.com/hwchase17/langchain/blob/master/langchain/chains/api/base.py#L115)
    based on our input instructions (something along the lines of `[https://listen-api.listennotes.com/api/v2/search?q=french&type=podcast&page_size=3](https://listen-api.listennotes.com/api/v2/search?q=french&type=podcast&page_size=3%29)`[)](https://listen-api.listennotes.com/api/v2/search?q=french&type=podcast&page_size=3%29)
    and [making the API call](https://github.com/hwchase17/langchain/blob/master/langchain/chains/api/base.py#L116).
    Upon receiving the response, it uses another LLMChain that [summarizes the response](https://github.com/hwchase17/langchain/blob/master/langchain/chains/api/base.py#L117)
    to get the answer to our original question. You can check out the prompts [here](https://github.com/hwchase17/langchain/blob/master/langchain/chains/api/prompt.py)
    for both LLMchains which describe the process in more detail.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åº•å±‚ï¼Œè¿™ä¸ªå·¥å…·é¦–å…ˆä½¿ç”¨ LLMChain æ¥[æ„å»º API URL](https://github.com/hwchase17/langchain/blob/master/langchain/chains/api/base.py#L115)ï¼Œæ ¹æ®æˆ‘ä»¬çš„è¾“å…¥æŒ‡ä»¤ï¼ˆç±»ä¼¼äº
    `[https://listen-api.listennotes.com/api/v2/search?q=french&type=podcast&page_size=3](https://listen-api.listennotes.com/api/v2/search?q=french&type=podcast&page_size=3%29)`ï¼‰å’Œ[è¿›è¡Œ
    API è°ƒç”¨](https://github.com/hwchase17/langchain/blob/master/langchain/chains/api/base.py#L116)ã€‚æ¥æ”¶åˆ°å“åº”åï¼Œå®ƒä½¿ç”¨å¦ä¸€ä¸ª
    LLMChain æ¥[æ€»ç»“å“åº”](https://github.com/hwchase17/langchain/blob/master/langchain/chains/api/base.py#L117)ï¼Œä»¥è·å¾—å¯¹æˆ‘ä»¬åŸå§‹é—®é¢˜çš„å›ç­”ã€‚ä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/hwchase17/langchain/blob/master/langchain/chains/api/prompt.py)æŸ¥çœ‹ä¸¤ä¸ª
    LLMchains çš„æç¤ºï¼Œå®ƒä»¬è¯¦ç»†æè¿°äº†è¿™ä¸ªè¿‡ç¨‹ã€‚
- en: I am inclined to guess the inconsistent results seen above are resulting from
    the summarization step because I have separately debugged and tested the API URL
    (created by LLMChain#1) via Postman and received the right response. To further
    confirm my doubts, I also stress-tested the summarization chain as a standalone
    chain with an empty API URL hoping it would throw an error but got the response
    *â€œInvestingâ€™ podcasts were found, containing 3 results in total.â€* ğŸ¤·â€â™€ Iâ€™d be
    curious to see if others had better luck than me with this tool!
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å€¾å‘äºçŒœæµ‹ä¸Šè¿°ä¸ä¸€è‡´çš„ç»“æœæ˜¯ç”±äºæ€»ç»“æ­¥éª¤é€ æˆçš„ï¼Œå› ä¸ºæˆ‘å·²ç»é€šè¿‡Postmanå•ç‹¬è°ƒè¯•å¹¶æµ‹è¯•äº†ç”±LLMChain#1åˆ›å»ºçš„API URLï¼Œå¹¶ä¸”å¾—åˆ°äº†æ­£ç¡®çš„å“åº”ã€‚ä¸ºäº†è¿›ä¸€æ­¥ç¡®è®¤æˆ‘çš„ç–‘è™‘ï¼Œæˆ‘è¿˜å¯¹æ€»ç»“é“¾è¿›è¡Œäº†å‹åŠ›æµ‹è¯•ï¼Œä½œä¸ºä¸€ä¸ªç‹¬ç«‹é“¾ä½¿ç”¨äº†ä¸€ä¸ªç©ºçš„API
    URLï¼Œå¸Œæœ›å®ƒèƒ½æŠ›å‡ºä¸€ä¸ªé”™è¯¯ï¼Œä½†å¾—åˆ°äº†*â€œå‘ç°äº†â€˜æŠ•èµ„â€™æ’­å®¢ï¼Œæ€»å…±æœ‰3ä¸ªç»“æœã€‚â€* ğŸ¤·â€â™€ æˆ‘å¾ˆå¥½å¥‡å…¶ä»–äººæ˜¯å¦åœ¨ä½¿ç”¨è¿™ä¸ªå·¥å…·æ—¶æ¯”æˆ‘æ›´å¹¸è¿ï¼
- en: 'Use Case 2: Combine chains to create an age-appropriate gift generator'
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç”¨ä¾‹ 2ï¼šç»“åˆé“¾åˆ›å»ºä¸€ä¸ªé€‚åˆå¹´é¾„çš„ç¤¼ç‰©ç”Ÿæˆå™¨
- en: 'Letâ€™s put our knowledge of agents and sequential chaining to good use and create
    our own sequential chain. We will combine:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å……åˆ†åˆ©ç”¨ä»£ç†å’Œé¡ºåºé“¾çš„çŸ¥è¯†ï¼Œåˆ›å»ºæˆ‘ä»¬è‡ªå·±çš„é¡ºåºé“¾ã€‚æˆ‘ä»¬å°†ç»“åˆï¼š
- en: 'Chain #1 â€” The `agent` we just created that can solve [age problems](https://www.cliffsnotes.com/study-guides/algebra/algebra-i/word-problems/age-problems)
    in math.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'é“¾ #1 â€” æˆ‘ä»¬åˆšåˆ›å»ºçš„`agent`ï¼Œèƒ½å¤Ÿè§£å†³æ•°å­¦ä¸­çš„[å¹´é¾„é—®é¢˜](https://www.cliffsnotes.com/study-guides/algebra/algebra-i/word-problems/age-problems)ã€‚'
- en: 'Chain #2 â€” An LLM that takes the age of a person and suggests an appropriate
    gift for them.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'é“¾ #2 â€” ä¸€ä¸ªLLMï¼Œå®ƒæ¥å—ä¸€ä¸ªäººçš„å¹´é¾„å¹¶å»ºè®®ä¸€ä¸ªé€‚åˆä»–ä»¬çš„ç¤¼ç‰©ã€‚'
- en: '[PRE11]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now that we have both chains ready we can combine them using `SimpleSequentialChain`.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½äº†ä¸¤ä¸ªé“¾ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`SimpleSequentialChain`å°†å®ƒä»¬ç»“åˆèµ·æ¥ã€‚
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'A couple of things to note:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: éœ€è¦æ³¨æ„å‡ ç‚¹ï¼š
- en: We need not explicitly pass `input_variables` and `output_variables` for `SimpleSequentialChain`
    as the underlying assumption is that the output from chain 1 is passed as input
    to chain 2.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸éœ€è¦ä¸º`SimpleSequentialChain`æ˜ç¡®ä¼ é€’`input_variables`å’Œ`output_variables`ï¼Œå› ä¸ºå…¶åŸºæœ¬å‡è®¾æ˜¯é“¾1çš„è¾“å‡ºä½œä¸ºé“¾2çš„è¾“å…¥ã€‚
- en: 'Finally, we can run it with the same math problem as before:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¹‹å‰çš„æ•°å­¦é—®é¢˜æ¥è¿è¡Œå®ƒï¼š
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: There might be times when you need to pass along some additional context to
    the second chain, in addition to what it is receiving from the first chain. For
    instance, I want to set a budget for the gift, depending on the age of the person
    that is returned by the first chain. We can do so using `SimpleMemory`.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰æ—¶ä½ å¯èƒ½éœ€è¦å°†ä¸€äº›é¢å¤–çš„ä¸Šä¸‹æ–‡ä¼ é€’ç»™ç¬¬äºŒä¸ªé“¾ï¼Œè€Œä¸ä»…ä»…æ˜¯ä»ç¬¬ä¸€ä¸ªé“¾æ¥æ”¶çš„å†…å®¹ã€‚ä¾‹å¦‚ï¼Œæˆ‘æƒ³æ ¹æ®ç¬¬ä¸€ä¸ªé“¾è¿”å›çš„å¹´é¾„ä¸ºç¤¼ç‰©è®¾å®šé¢„ç®—ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨`SimpleMemory`æ¥å®ç°ã€‚
- en: First, letâ€™s update the prompt for `chain_two` and pass to it a second variable
    called `budget` inside `input_variables`.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè®©æˆ‘ä»¬æ›´æ–°`chain_two`çš„æç¤ºï¼Œå¹¶åœ¨`input_variables`ä¸­ä¼ é€’ä¸€ä¸ªåä¸º`budget`çš„ç¬¬äºŒä¸ªå˜é‡ã€‚
- en: '[PRE15]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'If you compare the `template` we had for `SimpleSequentialChain` with the one
    above, youâ€™ll notice that I have also updated the first inputâ€™s variable name
    from `age` â†’ `output`. This is a crucial step, failing which an error would be
    raised at the time of [chain validation](https://github.com/hwchase17/langchain/blob/master/langchain/chains/sequential.py#L41)
    â€” `*Missing required input keys: {age}, only had {input, output, budget}*`.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æ¯”è¾ƒæˆ‘ä»¬ä¸º`SimpleSequentialChain`å‡†å¤‡çš„`template`ä¸ä¸Šè¿°çš„æ¨¡æ¿ï¼Œä½ ä¼šæ³¨æ„åˆ°æˆ‘è¿˜å°†ç¬¬ä¸€ä¸ªè¾“å…¥çš„å˜é‡åä»`age`æ›´æ–°ä¸º`output`ã€‚è¿™æ˜¯ä¸€ä¸ªå…³é”®æ­¥éª¤ï¼Œå¦‚æœå¤±è´¥ï¼Œå°†åœ¨[é“¾éªŒè¯](https://github.com/hwchase17/langchain/blob/master/langchain/chains/sequential.py#L41)æ—¶å¼•å‘é”™è¯¯
    â€” `*ç¼ºå°‘å¿…éœ€çš„è¾“å…¥é”®ï¼š{age}ï¼Œåªæœ‰ {input, output, budget}*`ã€‚
- en: This is because the output from the first entity in the chain (i.e. `agent`)
    will be the input for the second entity in the chain (i.e. `chain_two`) and therefore
    the variable names must match**.** Upon inspecting `agent`â€™s output keys, we see
    that the output variable is called `output`, hence the update.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å› ä¸ºé“¾ä¸­çš„ç¬¬ä¸€ä¸ªå®ä½“ï¼ˆå³`agent`ï¼‰çš„è¾“å‡ºå°†ä½œä¸ºç¬¬äºŒä¸ªå®ä½“ï¼ˆå³`chain_two`ï¼‰çš„è¾“å…¥ï¼Œå› æ­¤å˜é‡åå¿…é¡»åŒ¹é…**ã€‚** æ£€æŸ¥`agent`çš„è¾“å‡ºé”®æ—¶ï¼Œæˆ‘ä»¬å‘ç°è¾“å‡ºå˜é‡å«åš`output`ï¼Œå› æ­¤è¿›è¡Œäº†æ›´æ–°ã€‚
- en: '[PRE16]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Next, letâ€™s update the kind of chain we are making. We can no longer work with
    `SimpleSequentialChain` because it only works in cases where this is a single
    input and single output. Since `chain_two` is now taking two `input_variables`,
    we need to use `SequentialChain` which is tailored to handle multiple inputs and
    outputs.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬æ›´æ–°æˆ‘ä»¬æ­£åœ¨åˆ¶ä½œçš„é“¾çš„ç±»å‹ã€‚æˆ‘ä»¬ä¸èƒ½å†ä½¿ç”¨`SimpleSequentialChain`ï¼Œå› ä¸ºå®ƒä»…é€‚ç”¨äºå•è¾“å…¥å•è¾“å‡ºçš„æƒ…å†µã€‚ç”±äº`chain_two`ç°åœ¨éœ€è¦ä¸¤ä¸ª`input_variables`ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨`SequentialChain`ï¼Œå®ƒä¸“é—¨å¤„ç†å¤šä¸ªè¾“å…¥å’Œè¾“å‡ºã€‚
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'A couple of things to note:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: éœ€è¦æ³¨æ„å‡ ç‚¹ï¼š
- en: Unlike `SimpleSequentialChain`, passing `input_variables` parameter is mandatory
    for `SequentialChain`. It is a list containing the name of the input variables
    that the first entity in the chain (i.e. `agent` in our case) expects.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸`SimpleSequentialChain`ä¸åŒï¼Œå¯¹äº`SequentialChain`ï¼Œä¼ é€’`input_variables`å‚æ•°æ˜¯å¼ºåˆ¶æ€§çš„ã€‚å®ƒæ˜¯ä¸€ä¸ªåŒ…å«é“¾ä¸­ç¬¬ä¸€ä¸ªå®ä½“ï¼ˆå³æˆ‘ä»¬æ¡ˆä¾‹ä¸­çš„`agent`ï¼‰æœŸæœ›çš„è¾“å…¥å˜é‡åç§°çš„åˆ—è¡¨ã€‚
- en: Now some of you may be wondering how to know the exact name used in the input
    prompt that the `agent` is going to use. We certainly did not write the prompt
    for this agent (as we did for `chain_two`)! It's actually pretty straightforward
    to find it out by inspecting the prompt template of the `llm_chain` that the agent
    is made up of.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œä½ ä»¬ä¸­çš„ä¸€äº›äººå¯èƒ½æƒ³çŸ¥é“å¦‚ä½•çŸ¥é“`agent`å°†è¦ä½¿ç”¨çš„è¾“å…¥æç¤ºä¸­ä½¿ç”¨çš„ç¡®åˆ‡åç§°ã€‚æˆ‘ä»¬ç¡®å®æ²¡æœ‰ä¸ºè¿™ä¸ª`agent`ï¼ˆå¦‚æˆ‘ä»¬ä¸º`chain_two`æ‰€åšçš„é‚£æ ·ï¼‰ç¼–å†™è¿‡æç¤ºï¼äº‹å®ä¸Šï¼Œé€šè¿‡æ£€æŸ¥`llm_chain`çš„æç¤ºæ¨¡æ¿ï¼Œæ‰¾å‡ºå®ƒå…¶å®éå¸¸ç®€å•ã€‚
- en: '[PRE18]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As you can see toward the end of the prompt, the questions being asked by the
    end-user is stored in an input variable by the name `input`. If for some reason
    you had to manipulate this name in the prompt, make sure you are also updating
    the `input_variables` at the time of the creation of `SequentialChain`.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æ‚¨å¯ä»¥åœ¨æç¤ºçš„æœ€åçœ‹åˆ°çš„é‚£æ ·ï¼Œæœ€ç»ˆç”¨æˆ·æå‡ºçš„é—®é¢˜å­˜å‚¨åœ¨ä¸€ä¸ªåä¸º`input`çš„è¾“å…¥å˜é‡ä¸­ã€‚å¦‚æœå› æŸç§åŸå› æ‚¨å¿…é¡»åœ¨æç¤ºä¸­æ“çºµè¿™ä¸ªåç§°ï¼Œè¯·ç¡®ä¿åœ¨åˆ›å»º`SequentialChain`æ—¶åŒæ—¶æ›´æ–°`input_variables`ã€‚
- en: 'Finally, you could have found out the same information without going through
    the whole prompt:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæ‚¨å¯ä»¥åœ¨ä¸æŸ¥çœ‹æ•´ä¸ªæç¤ºçš„æƒ…å†µä¸‹æ‰¾åˆ°ç›¸åŒçš„ä¿¡æ¯ï¼š
- en: '[PRE19]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`[SimpleMemory](https://github.com/hwchase17/langchain/blob/master/langchain/memory/simple.py#L6)`
    is an easy way to store context or other bits of information that shouldnâ€™t ever
    change between prompts. It requires one parameter at the time of initialization
    â€” `memories`. You can pass elements to it in `dict` form. For instance, `SimpleMemory(memories={â€œbudgetâ€:
    â€œ100 GBPâ€})`.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[SimpleMemory](https://github.com/hwchase17/langchain/blob/master/langchain/memory/simple.py#L6)`
    æ˜¯ä¸€ç§å­˜å‚¨ä¸Šä¸‹æ–‡æˆ–å…¶ä»–ä¿¡æ¯ç‰‡æ®µçš„ç®€ä¾¿æ–¹æ³•ï¼Œè¿™äº›ä¿¡æ¯åœ¨æç¤ºä¹‹é—´ä¸åº”æ›´æ”¹ã€‚å®ƒåœ¨åˆå§‹åŒ–æ—¶éœ€è¦ä¸€ä¸ªå‚æ•° â€” `memories`ã€‚æ‚¨å¯ä»¥ä»¥`dict`å½¢å¼ä¼ é€’å…ƒç´ ç»™å®ƒã€‚ä¾‹å¦‚ï¼Œ`SimpleMemory(memories={â€œbudgetâ€:
    â€œ100 GBPâ€})`ã€‚'
- en: Finally, letâ€™s run the new chain with the same prompt as before. You will notice,
    the final output has some luxury gift recommendations such as weekend getaways
    in accordance with the higher budget in our updated prompt.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œè®©æˆ‘ä»¬ç”¨ä¸ä¹‹å‰ç›¸åŒçš„æç¤ºè¿è¡Œæ–°é“¾ã€‚æ‚¨ä¼šæ³¨æ„åˆ°ï¼Œæœ€ç»ˆè¾“å‡ºåŒ…æ‹¬ä¸€äº›å¥¢ä¾ˆç¤¼å“æ¨èï¼Œä¾‹å¦‚å‘¨æœ«åº¦å‡ï¼Œä¸æˆ‘ä»¬æ›´æ–°çš„æç¤ºä¸­çš„æ›´é«˜é¢„ç®—ç›¸åŒ¹é…ã€‚
- en: '[PRE20]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Conclusion
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: Hopefully, the learnings I have shared through this post have made you more
    comfortable in taking a deep dive into the library. This article just scratched
    the surface, there is so much more to cover. For instance, how to build a QnA
    chatbot over your own datasets, and how to optimize memory for these chatbots
    so that you can cherry-pick/summarize conversations to send in the prompt rather
    than sending all previous chat history as part of the prompt.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: å¸Œæœ›é€šè¿‡æœ¬æ–‡åˆ†äº«çš„å­¦ä¹ å†…å®¹èƒ½è®©æ‚¨æ›´è½»æ¾åœ°æ·±å…¥äº†è§£è¿™ä¸ªåº“ã€‚æœ¬æ–‡åªæ˜¯çš®æ¯›ï¼Œè¿˜æœ‰å¾ˆå¤šå†…å®¹å¯ä»¥æ¢è®¨ã€‚ä¾‹å¦‚ï¼Œå¦‚ä½•åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šæ„å»ºé—®ç­”èŠå¤©æœºå™¨äººï¼Œä»¥åŠå¦‚ä½•ä¼˜åŒ–è¿™äº›èŠå¤©æœºå™¨äººçš„è®°å¿†ï¼Œä»¥ä¾¿æ‚¨å¯ä»¥é€‰æ‹©æ€§åœ°/æ€»ç»“æ€§åœ°å‘é€å¯¹è¯ï¼Œè€Œä¸æ˜¯å°†æ‰€æœ‰ä»¥å‰çš„èŠå¤©å†å²ä½œä¸ºæç¤ºçš„ä¸€éƒ¨åˆ†å‘é€å‡ºå»ã€‚
- en: As always if thereâ€™s an easier way to do/explain some of the things mentioned
    in this article, do let me know. In general, refrain from unsolicited destructive/trash/hostile
    comments!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å¾€å¸¸ä¸€æ ·ï¼Œå¦‚æœæœ‰æ›´ç®€å•çš„æ–¹æ³•æ¥æ‰§è¡Œ/è§£é‡Šæœ¬æ–‡ä¸­æåˆ°çš„ä¸€äº›å†…å®¹ï¼Œè¯·å‘Šè¯‰æˆ‘ã€‚æ€»çš„æ¥è¯´ï¼Œé¿å…æœªç»è¯·æ±‚çš„ç ´åæ€§/åƒåœ¾/æ•Œå¯¹çš„è¯„è®ºï¼
- en: Until next time âœ¨
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´åˆ°ä¸‹æ¬¡è§ âœ¨
- en: '*I enjoy writing step-by-step beginnerâ€™s guides, how-to tutorials, decoding
    terminology used in ML/AI, etc. If you want full access to all my articles (and
    others on Medium), then you can sign up using* [***my link***](https://varshitasher.medium.com/membership)*here****.***'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '*æˆ‘å–œæ¬¢æ’°å†™é€æ­¥åˆå­¦è€…æŒ‡å—ã€å¦‚ä½•æ•™ç¨‹ã€è§£ç ML/AIæœ¯è¯­ç­‰ã€‚å¦‚æœæ‚¨å¸Œæœ›å…¨é¢è®¿é—®æˆ‘çš„æ‰€æœ‰æ–‡ç« ï¼ˆä»¥åŠMediumä¸Šçš„å…¶ä»–æ–‡ç« ï¼‰ï¼Œå¯ä»¥ä½¿ç”¨* [***æˆ‘çš„é“¾æ¥***](https://varshitasher.medium.com/membership)*åœ¨è¿™é‡Œæ³¨å†Œ*ã€‚'
- en: '[](/step-by-step-guide-to-explaining-your-ml-project-during-a-data-science-interview-81dfaaa408bf?source=post_page-----16cd385fca81--------------------------------)
    [## Step by step guide to explaining your ML project during a data science interview.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/step-by-step-guide-to-explaining-your-ml-project-during-a-data-science-interview-81dfaaa408bf?source=post_page-----16cd385fca81--------------------------------)
    [## é€æ­¥æŒ‡å—ï¼šåœ¨æ•°æ®ç§‘å­¦é¢è¯•ä¸­è§£é‡Šæ‚¨çš„MLé¡¹ç›®ã€‚'
- en: With a bonus sample script at the end that lets you show off your tech skills
    discreetly!
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¹¶é™„å¸¦ä¸€ä¸ªç¤ºä¾‹è„šæœ¬ï¼Œè®©æ‚¨å¯ä»¥æ‚„æ‚„å±•ç¤ºæ‚¨çš„æŠ€æœ¯æŠ€èƒ½ï¼
- en: towardsdatascience.com](/step-by-step-guide-to-explaining-your-ml-project-during-a-data-science-interview-81dfaaa408bf?source=post_page-----16cd385fca81--------------------------------)
    [](/time-series-modeling-using-scikit-pandas-and-numpy-682e3b8db8d1?source=post_page-----16cd385fca81--------------------------------)
    [## Time Series Modeling using Scikit, Pandas, and Numpy
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[æ—¶é—´åºåˆ—å»ºæ¨¡ä½¿ç”¨ Scikitã€Pandas å’Œ Numpy](https://towardsdatascience.com/step-by-step-guide-to-explaining-your-ml-project-during-a-data-science-interview-81dfaaa408bf?source=post_page-----16cd385fca81--------------------------------)
    [](/time-series-modeling-using-scikit-pandas-and-numpy-682e3b8db8d1?source=post_page-----16cd385fca81--------------------------------)
    [## æ—¶é—´åºåˆ—å»ºæ¨¡ä½¿ç”¨ Scikitã€Pandas å’Œ Numpy'
- en: Intuitive use of seasonality to improve model accuracy.
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç›´è§‚åœ°ä½¿ç”¨å­£èŠ‚æ€§æ¥æé«˜æ¨¡å‹å‡†ç¡®æ€§ã€‚
- en: towardsdatascience.com](/time-series-modeling-using-scikit-pandas-and-numpy-682e3b8db8d1?source=post_page-----16cd385fca81--------------------------------)
    [](/hands-on-introduction-to-github-actions-for-data-scientists-f422631c9ea7?source=post_page-----16cd385fca81--------------------------------)
    [## Hands-On Introduction to Github Actions for Data Scientists
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[æ•°æ®ç§‘å­¦å®¶å®ç”¨çš„ GitHub Actions ä»‹ç»](https://towardsdatascience.com/time-series-modeling-using-scikit-pandas-and-numpy-682e3b8db8d1?source=post_page-----16cd385fca81--------------------------------)
    [](/hands-on-introduction-to-github-actions-for-data-scientists-f422631c9ea7?source=post_page-----16cd385fca81--------------------------------)
    [## ä½¿ç”¨ GitHub Actions è¿›è¡ŒåŠ¨æ‰‹å®è·µçš„ä»‹ç»'
- en: Learn how to automate experiment tracking with Weights & Biases, unit testing,
    artifact creation, and lots moreâ€¦
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å­¦ä¹ å¦‚ä½•ä½¿ç”¨ Weights & Biases è‡ªåŠ¨åŒ–å®éªŒè·Ÿè¸ªã€å•å…ƒæµ‹è¯•ã€å·¥ä»¶åˆ›å»ºä»¥åŠæ›´å¤šå†…å®¹â€¦
- en: 'towardsdatascience.com](/hands-on-introduction-to-github-actions-for-data-scientists-f422631c9ea7?source=post_page-----16cd385fca81--------------------------------)
    [](/deploying-an-end-to-end-deep-learning-project-with-few-clicks-part-2-89009cff6f16?source=post_page-----16cd385fca81--------------------------------)
    [## Deploying an End to End Deep Learning Project with few clicks: Part 2'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[æ•°æ®ç§‘å­¦å®¶å®ç”¨çš„ GitHub Actions ä»‹ç»](https://towardsdatascience.com/hands-on-introduction-to-github-actions-for-data-scientists-f422631c9ea7?source=post_page-----16cd385fca81--------------------------------)
    [](/deploying-an-end-to-end-deep-learning-project-with-few-clicks-part-2-89009cff6f16?source=post_page-----16cd385fca81--------------------------------)
    [## ä½¿ç”¨å°‘é‡ç‚¹å‡»éƒ¨ç½²ç«¯åˆ°ç«¯æ·±åº¦å­¦ä¹ é¡¹ç›®ï¼šç¬¬2éƒ¨åˆ†'
- en: Taking model from Jupyter notebook to Flask app, testing API endpoint using
    Postman, and Heroku deployment
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å°†æ¨¡å‹ä» Jupyter notebook è½¬ç§»åˆ° Flask åº”ç”¨ç¨‹åºï¼Œä½¿ç”¨ Postman æµ‹è¯• API ç«¯ç‚¹ï¼Œå¹¶è¿›è¡Œ Heroku éƒ¨ç½²
- en: towardsdatascience.com](/deploying-an-end-to-end-deep-learning-project-with-few-clicks-part-2-89009cff6f16?source=post_page-----16cd385fca81--------------------------------)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[å°†ç«¯åˆ°ç«¯æ·±åº¦å­¦ä¹ é¡¹ç›®éƒ¨ç½²åˆ° Heroku](https://towardsdatascience.com/deploying-an-end-to-end-deep-learning-project-with-few-clicks-part-2-89009cff6f16?source=post_page-----16cd385fca81--------------------------------)'
