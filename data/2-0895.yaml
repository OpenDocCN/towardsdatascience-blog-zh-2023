- en: Fine-tune a Large Language Model with Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç”¨ Python å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/fine-tune-a-large-language-model-with-python-b1c09dbc58b2](https://towardsdatascience.com/fine-tune-a-large-language-model-with-python-b1c09dbc58b2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/fine-tune-a-large-language-model-with-python-b1c09dbc58b2](https://towardsdatascience.com/fine-tune-a-large-language-model-with-python-b1c09dbc58b2)
- en: '![](../Images/1d8727d8fba25a5fd2ed429d267bf0c1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d8727d8fba25a5fd2ed429d267bf0c1.png)'
- en: Photo by [Manouchehr Hejazi](https://unsplash.com/@patrol?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[Manouchehr Hejazi](https://unsplash.com/@patrol?utm_source=medium&utm_medium=referral)
    çš„ç…§ç‰‡ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)'
- en: Learn how to fine-tune a BERT from scratch on a custom dataset
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å­¦ä¹ å¦‚ä½•åœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šä»å¤´å¼€å§‹å¾®è°ƒ BERT
- en: '[](https://medium.com/@marcellopoliti?source=post_page-----b1c09dbc58b2--------------------------------)[![Marcello
    Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----b1c09dbc58b2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b1c09dbc58b2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b1c09dbc58b2--------------------------------)
    [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----b1c09dbc58b2--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@marcellopoliti?source=post_page-----b1c09dbc58b2--------------------------------)[![Marcello
    Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----b1c09dbc58b2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b1c09dbc58b2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b1c09dbc58b2--------------------------------)
    [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----b1c09dbc58b2--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b1c09dbc58b2--------------------------------)
    Â·4 min readÂ·Apr 18, 2023
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒåœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b1c09dbc58b2--------------------------------)
    Â·4 åˆ†é’Ÿé˜…è¯»Â·2023 å¹´ 4 æœˆ 18 æ—¥
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: In this article, we will deal with the **fine-tuning of BERT for sentiment classification**
    using PyTorch. BERT is a large language model that offers a good balance between
    popularity and model size, which can be fine-tuned **using a simple GPU**. We
    can download a **pre-trained BERT from Hugging Face (HF)**, so there is no need
    to train it from scratch. In particular, we will use the distilled (smaller) version
    of BERT, called **Distil-BERT.**
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†å¤„ç†ä½¿ç”¨ PyTorch çš„**BERT æƒ…æ„Ÿåˆ†ç±»å¾®è°ƒ**ã€‚BERT æ˜¯ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæä¾›äº†å—æ¬¢è¿ç¨‹åº¦ä¸æ¨¡å‹å¤§å°ä¹‹é—´çš„è‰¯å¥½å¹³è¡¡ï¼Œå¯ä»¥**ä½¿ç”¨ç®€å•çš„
    GPU**è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬å¯ä»¥ä» Hugging Face (HF) ä¸‹è½½**é¢„è®­ç»ƒçš„ BERT**ï¼Œå› æ­¤æ— éœ€ä»å¤´å¼€å§‹è®­ç»ƒã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨åä¸º**Distil-BERT**çš„ç²¾ç®€ï¼ˆæ›´å°ï¼‰ç‰ˆæœ¬ã€‚
- en: '[Distil-BERT](https://huggingface.co/docs/transformers/model_doc/distilbert#:~:text=DistilBERT%20is%20a%20small%2C%20fast,the%20GLUE%20language%20understanding%20benchmark.)
    is widely used in production since it has **40% fewer parameters** than BERT uncased.
    It runs **60% faster and retains 95% performance** in the [GLUE](https://arxiv.org/abs/1804.07461)
    language comprehension benchmark.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[Distil-BERT](https://huggingface.co/docs/transformers/model_doc/distilbert#:~:text=DistilBERT%20is%20a%20small%2C%20fast,the%20GLUE%20language%20understanding%20benchmark.)
    åœ¨ç”Ÿäº§ä¸­å¹¿æ³›ä½¿ç”¨ï¼Œå› ä¸ºå®ƒæ¯” BERT uncased**å°‘äº† 40% çš„å‚æ•°**ã€‚å®ƒè¿è¡Œ**å¿« 60% å¹¶ä¿æŒ 95% çš„æ€§èƒ½**åœ¨ [GLUE](https://arxiv.org/abs/1804.07461)
    è¯­è¨€ç†è§£åŸºå‡†ä¸­ã€‚'
- en: We start by installing all the necessary libraries. The first line is to capture
    the output of the installation and keep your notebook clean.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆå®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ã€‚ç¬¬ä¸€è¡Œæ˜¯æ•è·å®‰è£…è¾“å‡ºï¼Œå¹¶ä¿æŒä½ çš„ç¬”è®°æœ¬æ•´æ´ã€‚
- en: I will use Deepnote to run the code in this article but you also use Google
    Colab if you prefer.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†ä½¿ç”¨ Deepnote æ¥è¿è¡Œæœ¬æ–‡ä¸­çš„ä»£ç ï¼Œä½†å¦‚æœä½ æ›´å–œæ¬¢ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ Google Colabã€‚
- en: You can also check the version of the libraries you are using with the following
    line of code.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ è¿˜å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç è¡Œæ£€æŸ¥ä½ æ­£åœ¨ä½¿ç”¨çš„åº“çš„ç‰ˆæœ¬ã€‚
- en: Now you need to specify some general setups, including the number of epochs
    and device hardware. We set a fixed random seed which helps for the reproducibility
    of the experiment.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ éœ€è¦æŒ‡å®šä¸€äº›å¸¸è§„è®¾ç½®ï¼ŒåŒ…æ‹¬è®­ç»ƒè½®æ¬¡å’Œè®¾å¤‡ç¡¬ä»¶ã€‚æˆ‘ä»¬è®¾ç½®äº†ä¸€ä¸ªå›ºå®šçš„éšæœºç§å­ï¼Œä»¥å¸®åŠ©å®éªŒçš„å¯é‡å¤æ€§ã€‚
- en: Loading the IMDb movie review dataset
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ è½½ IMDb ç”µå½±è¯„è®ºæ•°æ®é›†
- en: Letâ€™s see how to prepare and tokenize the IMDb movie review dataset and fine-tune
    Distilled BERT. Fetch the compressed data and unzip it.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹å¦‚ä½•å‡†å¤‡å’Œæ ‡è®° IMDb ç”µå½±è¯„è®ºæ•°æ®é›†ï¼Œå¹¶å¾®è°ƒ Distilled BERTã€‚è·å–å‹ç¼©æ•°æ®å¹¶è§£å‹ç¼©å®ƒã€‚
- en: As usual, we need to split the data into training, validation, and test sets.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å’Œå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬éœ€è¦å°†æ•°æ®æ‹†åˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚
- en: Tokenize the dataset
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ ‡è®°æ•°æ®é›†
- en: Letâ€™s **tokenize the texts into individual word tokens** using the tokenizer
    implementation inherited from the pre-trained model class.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä½¿ç”¨ä»é¢„è®­ç»ƒæ¨¡å‹ç±»ç»§æ‰¿çš„åˆ†è¯å™¨å®ç°**å°†æ–‡æœ¬åˆ†è¯ä¸ºå•ä¸ªå•è¯æ ‡è®°**ã€‚
- en: Tokenization is used in natural language processing to split paragraphs and
    sentences into smaller units that can be more easily assigned meaning.
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åˆ†è¯åŒ–ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ï¼Œå°†æ®µè½å’Œå¥å­åˆ†å‰²æˆæ›´å°çš„å•ä½ï¼Œä»¥ä¾¿æ›´å®¹æ˜“èµ‹äºˆå…¶æ„ä¹‰ã€‚
- en: '**With Hugging Face you will always find a tokenizer associated with each model**.
    If you are not doing research or experiments on tokenizers itâ€™s always preferable
    to use the standard tokenizers.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**é€šè¿‡ Hugging Faceï¼Œä½ å°†æ€»æ˜¯èƒ½æ‰¾åˆ°ä¸æ¯ä¸ªæ¨¡å‹ç›¸å…³è”çš„åˆ†è¯å™¨**ã€‚å¦‚æœä½ ä¸æ˜¯åœ¨ç ”ç©¶æˆ–å®éªŒåˆ†è¯å™¨ï¼Œé€šå¸¸æ›´æ¨èä½¿ç”¨æ ‡å‡†åˆ†è¯å™¨ã€‚'
- en: '**Padding** is a strategy for ensuring tensors are rectangular by adding a
    special padding token to shorter sentences. On the other, sometimes a sequence
    may be too long for a model to handle. In this case, youâ€™ll need to truncate the
    sequence to a shorter length.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¡«å……**æ˜¯ä¸€ç§é€šè¿‡å‘è¾ƒçŸ­çš„å¥å­æ·»åŠ ç‰¹æ®Šå¡«å……æ ‡è®°æ¥ç¡®ä¿å¼ é‡æ˜¯çŸ©å½¢çš„ç­–ç•¥ã€‚å¦ä¸€æ–¹é¢ï¼Œæœ‰æ—¶åºåˆ—å¯èƒ½å¤ªé•¿ï¼Œæ¨¡å‹æ— æ³•å¤„ç†ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ éœ€è¦å°†åºåˆ—æˆªæ–­ä¸ºè¾ƒçŸ­çš„é•¿åº¦ã€‚'
- en: Letâ€™s pack everything into a Python class that we are going to name IMDbDataset.
    We are also going to use this custom dataset to create the corresponding dataloaders.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å°†æ‰€æœ‰å†…å®¹æ‰“åŒ…åˆ°ä¸€ä¸ªåä¸º IMDbDataset çš„ Python ç±»ä¸­ã€‚æˆ‘ä»¬è¿˜å°†ä½¿ç”¨è¿™ä¸ªè‡ªå®šä¹‰æ•°æ®é›†æ¥åˆ›å»ºç›¸åº”çš„æ•°æ®åŠ è½½å™¨ã€‚
- en: 'The encodings variable stores a lot of information about the tokenized text.
    We can extract only the most relevant information via dictionary comprehension.
    The dictionary contains:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: encodings å˜é‡å­˜å‚¨äº†å¤§é‡å…³äºåˆ†è¯æ–‡æœ¬çš„ä¿¡æ¯ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡å­—å…¸æ¨å¯¼æå–å‡ºæœ€ç›¸å…³çš„ä¿¡æ¯ã€‚å­—å…¸åŒ…å«ï¼š
- en: '**input_ids**: are the indices corresponding to each token in the sentence.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**input_ids**ï¼šæ˜¯å¥å­ä¸­æ¯ä¸ªæ ‡è®°å¯¹åº”çš„ç´¢å¼•ã€‚'
- en: '**labels**: classes labels'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ ‡ç­¾**ï¼šç±»åˆ«æ ‡ç­¾'
- en: '**attention_mask**: indicates whether a token should be attended to or not
    (for example padding tokens will not be attended).'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**attention_mask**ï¼šæŒ‡ç¤ºä¸€ä¸ªæ ‡è®°æ˜¯å¦åº”è¯¥è¢«å…³æ³¨ï¼ˆä¾‹å¦‚å¡«å……æ ‡è®°å°†ä¸ä¼šè¢«å…³æ³¨ï¼‰ã€‚'
- en: Letâ€™s construct datasets and corresponding dataloaders.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ„å»ºæ•°æ®é›†åŠç›¸åº”çš„æ•°æ®åŠ è½½å™¨ã€‚
- en: Loading and fine-tuning BERT
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ è½½å’Œå¾®è°ƒ BERT
- en: Finally, we are done with the data preprocessing, and we can start fine-tuning
    our model. Letâ€™s define a model and an optimization algorithm, Adam in this case.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å®Œæˆäº†æ•°æ®é¢„å¤„ç†ï¼Œå¯ä»¥å¼€å§‹å¾®è°ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚æˆ‘ä»¬æ¥å®šä¹‰ä¸€ä¸ªæ¨¡å‹å’Œä¸€ä¸ªä¼˜åŒ–ç®—æ³•ï¼Œè¿™é‡Œä½¿ç”¨çš„æ˜¯ Adamã€‚
- en: '*DistilbertForSequenceCLassification* specifies the downstream task we want
    to fine-tune the model on, which is sequence classification in this case. Note
    that â€œuncasedâ€ means that the model does not distinguish between upper and lower
    case letters.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*DistilbertForSequenceCLassification* æŒ‡å®šäº†æˆ‘ä»¬å¸Œæœ›å¾®è°ƒæ¨¡å‹çš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œè¿™åœ¨è¿™é‡Œæ˜¯åºåˆ—åˆ†ç±»ã€‚æ³¨æ„ï¼Œâ€œuncasedâ€æ„å‘³ç€æ¨¡å‹ä¸åŒºåˆ†å¤§å°å†™å­—æ¯ã€‚'
- en: Before training the model, we need to define some metrics to compare the model
    improvements. In this simple case, we can use conventional accuracy for classification.
    Notice that this function is quite long because we are loading the dataset batch
    by batch to work around ram and GPU limitations. Usually, these resources are
    never enough when fine-tuning huge datasets.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€äº›æŒ‡æ ‡æ¥æ¯”è¾ƒæ¨¡å‹çš„æ”¹è¿›ã€‚åœ¨è¿™ä¸ªç®€å•çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¼ ç»Ÿçš„åˆ†ç±»å‡†ç¡®ç‡ã€‚è¯·æ³¨æ„ï¼Œè¿™ä¸ªå‡½æ•°ç›¸å½“é•¿ï¼Œå› ä¸ºæˆ‘ä»¬æŒ‰æ‰¹æ¬¡åŠ è½½æ•°æ®é›†ä»¥ç»•è¿‡
    RAM å’Œ GPU é™åˆ¶ã€‚é€šå¸¸ï¼Œåœ¨å¾®è°ƒå¤§å‹æ•°æ®é›†æ—¶ï¼Œè¿™äº›èµ„æºæ€»æ˜¯ä¸å¤Ÿçš„ã€‚
- en: In the compute_accuracy function, we load a given batch and then take the predicted
    labels from the outputs. While doing this, we keep track of the total number of
    examples via the variable num_examples. In the same way, we keep track of the
    number of correct predictions via the correct_pred variable. After we have iterated
    over the complete dataloader, we can compute the accuracy by the last division.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ compute_accuracy å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬åŠ è½½ä¸€ä¸ªç»™å®šçš„æ‰¹æ¬¡ï¼Œç„¶åä»è¾“å‡ºä¸­è·å–é¢„æµ‹æ ‡ç­¾ã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å˜é‡ num_examples è·Ÿè¸ªç¤ºä¾‹çš„æ€»æ•°ã€‚åŒæ ·ï¼Œæˆ‘ä»¬é€šè¿‡
    correct_pred å˜é‡è·Ÿè¸ªæ­£ç¡®é¢„æµ‹çš„æ•°é‡ã€‚åœ¨è¿­ä»£å®Œæ•´ä¸ªæ•°æ®åŠ è½½å™¨åï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æœ€åçš„é™¤æ³•è®¡ç®—å‡†ç¡®ç‡ã€‚
- en: You can also notice how to use the model in the compute_accuracy function. We
    feed the model with input_ids along with the attention_mask information that denotes
    whether a token is an actual text token or padding. The model returns a SequenceClassificatierOutput
    object from which we get the logits and convert them into a class using the argmax
    function.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ è¿˜å¯ä»¥æ³¨æ„åˆ°å¦‚ä½•åœ¨ compute_accuracy å‡½æ•°ä¸­ä½¿ç”¨æ¨¡å‹ã€‚æˆ‘ä»¬å°† input_ids ä»¥åŠæŒ‡ç¤ºæ ‡è®°æ˜¯å¦ä¸ºå®é™…æ–‡æœ¬æ ‡è®°æˆ–å¡«å……çš„ attention_mask
    ä¿¡æ¯ä¼ é€’ç»™æ¨¡å‹ã€‚æ¨¡å‹è¿”å›ä¸€ä¸ª SequenceClassificatierOutput å¯¹è±¡ï¼Œæˆ‘ä»¬ä»ä¸­è·å– logits å¹¶é€šè¿‡ argmax å‡½æ•°å°†å…¶è½¬æ¢ä¸ºç±»åˆ«ã€‚
- en: Training (fine-tuning) loop
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒï¼ˆå¾®è°ƒï¼‰å¾ªç¯
- en: If you know how to code a training loop in PyTorch you won't have any issues
    understanding this fine-tuning loop. As in any neural network, we give inputs
    to the network, calculate the output, compute the loss, and do parameter updates
    based on this loss.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ çŸ¥é“å¦‚ä½•åœ¨PyTorchä¸­ç¼–å†™è®­ç»ƒå¾ªç¯ï¼Œä½ å°±ä¸ä¼šæœ‰ä»»ä½•é—®é¢˜ç†è§£è¿™ä¸ªå¾®è°ƒå¾ªç¯ã€‚åƒä»»ä½•ç¥ç»ç½‘ç»œä¸€æ ·ï¼Œæˆ‘ä»¬ç»™ç½‘ç»œè¾“å…¥æ•°æ®ï¼Œè®¡ç®—è¾“å‡ºï¼Œè®¡ç®—æŸå¤±ï¼Œå¹¶æ ¹æ®è¿™ä¸ªæŸå¤±è¿›è¡Œå‚æ•°æ›´æ–°ã€‚
- en: Every few epochs we print the training progress to get feedback.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ç»è¿‡å‡ ä¸ªå‘¨æœŸï¼Œæˆ‘ä»¬å°±æ‰“å°è®­ç»ƒè¿›åº¦ä»¥è·å¾—åé¦ˆã€‚
- en: Final Thoughts
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœ€ç»ˆæ€è€ƒ
- en: In this article, we have seen how to perform fine-tuning of a Large Language
    Model such as BERT by using PyTorch exclusively. Actually, there is a much faster
    and even smarter way to do this using the Transformers library from Hugging Face.
    This library allows us to create a Trainer object for fine-tuning where we can
    specify parameters such as the number of epochs and more in just a few lines of
    code. Follow me if you are curious to see how to do it in the next article! [ğŸ˜‰](https://emojipedia.org/it/apple/ios-15.4/faccina-che-fa-l-occhiolino/)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å·²ç»å­¦ä¹ äº†å¦‚ä½•ä»…ä½¿ç”¨PyTorchå¯¹åƒBERTè¿™æ ·çš„é¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚å®é™…ä¸Šï¼Œæœ‰ä¸€ç§æ›´å¿«ã€æ›´æ™ºèƒ½çš„æ–¹æ³•å¯ä»¥é€šè¿‡Hugging
    Faceçš„Transformersåº“æ¥å®ç°ã€‚è¿™ä¸ªåº“å…è®¸æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç”¨äºå¾®è°ƒçš„Trainerå¯¹è±¡ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å‡ è¡Œä»£ç ä¸­æŒ‡å®šå¦‚å‘¨æœŸæ•°ç­‰å‚æ•°ã€‚å¦‚æœä½ å¯¹ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­çš„å®ç°æ„Ÿåˆ°å¥½å¥‡ï¼Œè·Ÿéšæˆ‘å§ï¼[ğŸ˜‰](https://emojipedia.org/it/apple/ios-15.4/faccina-che-fa-l-occhiolino/)
- en: The End
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“æŸ
- en: '*Marcello Politi*'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*Marcello Politi*'
- en: '[Linkedin](https://www.linkedin.com/in/marcello-politi/), [Twitter](https://twitter.com/_March08_),
    [Website](https://marcello-politi.super.site/)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[é¢†è‹±](https://www.linkedin.com/in/marcello-politi/)ï¼Œ[æ¨ç‰¹](https://twitter.com/_March08_)ï¼Œ
    [ç½‘ç«™](https://marcello-politi.super.site/)'
