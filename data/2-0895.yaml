- en: Fine-tune a Large Language Model with Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用 Python 微调大型语言模型
- en: 原文：[https://towardsdatascience.com/fine-tune-a-large-language-model-with-python-b1c09dbc58b2](https://towardsdatascience.com/fine-tune-a-large-language-model-with-python-b1c09dbc58b2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/fine-tune-a-large-language-model-with-python-b1c09dbc58b2](https://towardsdatascience.com/fine-tune-a-large-language-model-with-python-b1c09dbc58b2)
- en: '![](../Images/1d8727d8fba25a5fd2ed429d267bf0c1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d8727d8fba25a5fd2ed429d267bf0c1.png)'
- en: Photo by [Manouchehr Hejazi](https://unsplash.com/@patrol?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[Manouchehr Hejazi](https://unsplash.com/@patrol?utm_source=medium&utm_medium=referral)
    的照片，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)'
- en: Learn how to fine-tune a BERT from scratch on a custom dataset
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习如何在自定义数据集上从头开始微调 BERT
- en: '[](https://medium.com/@marcellopoliti?source=post_page-----b1c09dbc58b2--------------------------------)[![Marcello
    Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----b1c09dbc58b2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b1c09dbc58b2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b1c09dbc58b2--------------------------------)
    [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----b1c09dbc58b2--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@marcellopoliti?source=post_page-----b1c09dbc58b2--------------------------------)[![Marcello
    Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----b1c09dbc58b2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b1c09dbc58b2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b1c09dbc58b2--------------------------------)
    [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----b1c09dbc58b2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b1c09dbc58b2--------------------------------)
    ·4 min read·Apr 18, 2023
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b1c09dbc58b2--------------------------------)
    ·4 分钟阅读·2023 年 4 月 18 日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: In this article, we will deal with the **fine-tuning of BERT for sentiment classification**
    using PyTorch. BERT is a large language model that offers a good balance between
    popularity and model size, which can be fine-tuned **using a simple GPU**. We
    can download a **pre-trained BERT from Hugging Face (HF)**, so there is no need
    to train it from scratch. In particular, we will use the distilled (smaller) version
    of BERT, called **Distil-BERT.**
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将处理使用 PyTorch 的**BERT 情感分类微调**。BERT 是一个大型语言模型，提供了受欢迎程度与模型大小之间的良好平衡，可以**使用简单的
    GPU**进行微调。我们可以从 Hugging Face (HF) 下载**预训练的 BERT**，因此无需从头开始训练。特别是，我们将使用名为**Distil-BERT**的精简（更小）版本。
- en: '[Distil-BERT](https://huggingface.co/docs/transformers/model_doc/distilbert#:~:text=DistilBERT%20is%20a%20small%2C%20fast,the%20GLUE%20language%20understanding%20benchmark.)
    is widely used in production since it has **40% fewer parameters** than BERT uncased.
    It runs **60% faster and retains 95% performance** in the [GLUE](https://arxiv.org/abs/1804.07461)
    language comprehension benchmark.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[Distil-BERT](https://huggingface.co/docs/transformers/model_doc/distilbert#:~:text=DistilBERT%20is%20a%20small%2C%20fast,the%20GLUE%20language%20understanding%20benchmark.)
    在生产中广泛使用，因为它比 BERT uncased**少了 40% 的参数**。它运行**快 60% 并保持 95% 的性能**在 [GLUE](https://arxiv.org/abs/1804.07461)
    语言理解基准中。'
- en: We start by installing all the necessary libraries. The first line is to capture
    the output of the installation and keep your notebook clean.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先安装所有必要的库。第一行是捕获安装输出，并保持你的笔记本整洁。
- en: I will use Deepnote to run the code in this article but you also use Google
    Colab if you prefer.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我将使用 Deepnote 来运行本文中的代码，但如果你更喜欢，也可以使用 Google Colab。
- en: You can also check the version of the libraries you are using with the following
    line of code.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用以下代码行检查你正在使用的库的版本。
- en: Now you need to specify some general setups, including the number of epochs
    and device hardware. We set a fixed random seed which helps for the reproducibility
    of the experiment.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你需要指定一些常规设置，包括训练轮次和设备硬件。我们设置了一个固定的随机种子，以帮助实验的可重复性。
- en: Loading the IMDb movie review dataset
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载 IMDb 电影评论数据集
- en: Let’s see how to prepare and tokenize the IMDb movie review dataset and fine-tune
    Distilled BERT. Fetch the compressed data and unzip it.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下如何准备和标记 IMDb 电影评论数据集，并微调 Distilled BERT。获取压缩数据并解压缩它。
- en: As usual, we need to split the data into training, validation, and test sets.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们需要将数据拆分为训练集、验证集和测试集。
- en: Tokenize the dataset
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标记数据集
- en: Let’s **tokenize the texts into individual word tokens** using the tokenizer
    implementation inherited from the pre-trained model class.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用从预训练模型类继承的分词器实现**将文本分词为单个单词标记**。
- en: Tokenization is used in natural language processing to split paragraphs and
    sentences into smaller units that can be more easily assigned meaning.
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 分词化用于自然语言处理，将段落和句子分割成更小的单位，以便更容易赋予其意义。
- en: '**With Hugging Face you will always find a tokenizer associated with each model**.
    If you are not doing research or experiments on tokenizers it’s always preferable
    to use the standard tokenizers.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**通过 Hugging Face，你将总是能找到与每个模型相关联的分词器**。如果你不是在研究或实验分词器，通常更推荐使用标准分词器。'
- en: '**Padding** is a strategy for ensuring tensors are rectangular by adding a
    special padding token to shorter sentences. On the other, sometimes a sequence
    may be too long for a model to handle. In this case, you’ll need to truncate the
    sequence to a shorter length.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**填充**是一种通过向较短的句子添加特殊填充标记来确保张量是矩形的策略。另一方面，有时序列可能太长，模型无法处理。在这种情况下，你需要将序列截断为较短的长度。'
- en: Let’s pack everything into a Python class that we are going to name IMDbDataset.
    We are also going to use this custom dataset to create the corresponding dataloaders.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将所有内容打包到一个名为 IMDbDataset 的 Python 类中。我们还将使用这个自定义数据集来创建相应的数据加载器。
- en: 'The encodings variable stores a lot of information about the tokenized text.
    We can extract only the most relevant information via dictionary comprehension.
    The dictionary contains:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: encodings 变量存储了大量关于分词文本的信息。我们可以通过字典推导提取出最相关的信息。字典包含：
- en: '**input_ids**: are the indices corresponding to each token in the sentence.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**input_ids**：是句子中每个标记对应的索引。'
- en: '**labels**: classes labels'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签**：类别标签'
- en: '**attention_mask**: indicates whether a token should be attended to or not
    (for example padding tokens will not be attended).'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**attention_mask**：指示一个标记是否应该被关注（例如填充标记将不会被关注）。'
- en: Let’s construct datasets and corresponding dataloaders.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建数据集及相应的数据加载器。
- en: Loading and fine-tuning BERT
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载和微调 BERT
- en: Finally, we are done with the data preprocessing, and we can start fine-tuning
    our model. Let’s define a model and an optimization algorithm, Adam in this case.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们完成了数据预处理，可以开始微调我们的模型。我们来定义一个模型和一个优化算法，这里使用的是 Adam。
- en: '*DistilbertForSequenceCLassification* specifies the downstream task we want
    to fine-tune the model on, which is sequence classification in this case. Note
    that “uncased” means that the model does not distinguish between upper and lower
    case letters.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*DistilbertForSequenceCLassification* 指定了我们希望微调模型的下游任务，这在这里是序列分类。注意，“uncased”意味着模型不区分大小写字母。'
- en: Before training the model, we need to define some metrics to compare the model
    improvements. In this simple case, we can use conventional accuracy for classification.
    Notice that this function is quite long because we are loading the dataset batch
    by batch to work around ram and GPU limitations. Usually, these resources are
    never enough when fine-tuning huge datasets.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型之前，我们需要定义一些指标来比较模型的改进。在这个简单的例子中，我们可以使用传统的分类准确率。请注意，这个函数相当长，因为我们按批次加载数据集以绕过
    RAM 和 GPU 限制。通常，在微调大型数据集时，这些资源总是不够的。
- en: In the compute_accuracy function, we load a given batch and then take the predicted
    labels from the outputs. While doing this, we keep track of the total number of
    examples via the variable num_examples. In the same way, we keep track of the
    number of correct predictions via the correct_pred variable. After we have iterated
    over the complete dataloader, we can compute the accuracy by the last division.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在 compute_accuracy 函数中，我们加载一个给定的批次，然后从输出中获取预测标签。在此过程中，我们通过变量 num_examples 跟踪示例的总数。同样，我们通过
    correct_pred 变量跟踪正确预测的数量。在迭代完整个数据加载器后，我们可以通过最后的除法计算准确率。
- en: You can also notice how to use the model in the compute_accuracy function. We
    feed the model with input_ids along with the attention_mask information that denotes
    whether a token is an actual text token or padding. The model returns a SequenceClassificatierOutput
    object from which we get the logits and convert them into a class using the argmax
    function.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以注意到如何在 compute_accuracy 函数中使用模型。我们将 input_ids 以及指示标记是否为实际文本标记或填充的 attention_mask
    信息传递给模型。模型返回一个 SequenceClassificatierOutput 对象，我们从中获取 logits 并通过 argmax 函数将其转换为类别。
- en: Training (fine-tuning) loop
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练（微调）循环
- en: If you know how to code a training loop in PyTorch you won't have any issues
    understanding this fine-tuning loop. As in any neural network, we give inputs
    to the network, calculate the output, compute the loss, and do parameter updates
    based on this loss.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你知道如何在PyTorch中编写训练循环，你就不会有任何问题理解这个微调循环。像任何神经网络一样，我们给网络输入数据，计算输出，计算损失，并根据这个损失进行参数更新。
- en: Every few epochs we print the training progress to get feedback.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 每经过几个周期，我们就打印训练进度以获得反馈。
- en: Final Thoughts
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终思考
- en: In this article, we have seen how to perform fine-tuning of a Large Language
    Model such as BERT by using PyTorch exclusively. Actually, there is a much faster
    and even smarter way to do this using the Transformers library from Hugging Face.
    This library allows us to create a Trainer object for fine-tuning where we can
    specify parameters such as the number of epochs and more in just a few lines of
    code. Follow me if you are curious to see how to do it in the next article! [😉](https://emojipedia.org/it/apple/ios-15.4/faccina-che-fa-l-occhiolino/)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们已经学习了如何仅使用PyTorch对像BERT这样的预训练大语言模型进行微调。实际上，有一种更快、更智能的方法可以通过Hugging
    Face的Transformers库来实现。这个库允许我们创建一个用于微调的Trainer对象，我们可以在几行代码中指定如周期数等参数。如果你对下一篇文章中的实现感到好奇，跟随我吧！[😉](https://emojipedia.org/it/apple/ios-15.4/faccina-che-fa-l-occhiolino/)
- en: The End
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结束
- en: '*Marcello Politi*'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*Marcello Politi*'
- en: '[Linkedin](https://www.linkedin.com/in/marcello-politi/), [Twitter](https://twitter.com/_March08_),
    [Website](https://marcello-politi.super.site/)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[领英](https://www.linkedin.com/in/marcello-politi/)，[推特](https://twitter.com/_March08_)，
    [网站](https://marcello-politi.super.site/)'
