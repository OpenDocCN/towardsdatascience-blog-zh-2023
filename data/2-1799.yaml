- en: Running Llama 2 on CPU Inference Locally for Document Q&A
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在本地运行 Llama 2 进行文档问答的 CPU 推理
- en: 原文：[https://towardsdatascience.com/running-llama-2-on-cpu-inference-for-document-q-a-3d636037a3d8](https://towardsdatascience.com/running-llama-2-on-cpu-inference-for-document-q-a-3d636037a3d8)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/running-llama-2-on-cpu-inference-for-document-q-a-3d636037a3d8](https://towardsdatascience.com/running-llama-2-on-cpu-inference-for-document-q-a-3d636037a3d8)
- en: Clearly explained guide for running quantized open-source LLM applications on
    CPUs using Llama 2, C Transformers, GGML, and LangChain
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 清晰解释了如何使用 Llama 2、C Transformers、GGML 和 LangChain 在 CPU 上运行量化开源 LLM 应用程序的指南
- en: '[](https://kennethleungty.medium.com/?source=post_page-----3d636037a3d8--------------------------------)[![Kenneth
    Leung](../Images/2514dffb34529d6d757c0c4ec5f98334.png)](https://kennethleungty.medium.com/?source=post_page-----3d636037a3d8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3d636037a3d8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3d636037a3d8--------------------------------)
    [Kenneth Leung](https://kennethleungty.medium.com/?source=post_page-----3d636037a3d8--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://kennethleungty.medium.com/?source=post_page-----3d636037a3d8--------------------------------)[![Kenneth
    Leung](../Images/2514dffb34529d6d757c0c4ec5f98334.png)](https://kennethleungty.medium.com/?source=post_page-----3d636037a3d8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3d636037a3d8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3d636037a3d8--------------------------------)
    [Kenneth Leung](https://kennethleungty.medium.com/?source=post_page-----3d636037a3d8--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3d636037a3d8--------------------------------)
    ·11 min read·Jul 18, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3d636037a3d8--------------------------------)
    ·阅读时间 11 分钟·2023 年 7 月 18 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/887641ba170cdfdd4548d8d2553f96b1.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/887641ba170cdfdd4548d8d2553f96b1.png)'
- en: Photo by [NOAA](https://unsplash.com/@noaa?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/computing-cloud?orientation=landscape&license=free&utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [NOAA](https://unsplash.com/@noaa?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，照片来自 [Unsplash](https://unsplash.com/s/photos/computing-cloud?orientation=landscape&license=free&utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: Third-party commercial large language model (LLM) providers like OpenAI’s GPT4
    have democratized LLM use via simple API calls. However, teams may still require
    self-managed or private deployment for model inference within enterprise perimeters
    due to various reasons around data privacy and compliance.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 像 OpenAI 的 GPT4 这样的第三方商业大型语言模型 (LLM) 提供商通过简单的 API 调用使 LLM 的使用实现了民主化。然而，由于数据隐私和合规性的各种原因，团队可能仍需在企业边界内进行自我管理或私有部署以进行模型推理。
- en: The proliferation of open-source LLMs has fortunately opened up a vast range
    of options for us, thus reducing our reliance on these third-party providers.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 开源 LLM 的普及幸运地为我们提供了广泛的选择，从而减少了对这些第三方供应商的依赖。
- en: When we host open-source models locally on-premise or in the cloud, the dedicated
    compute capacity becomes a key consideration. While GPU instances may seem the
    most convenient choice, the costs can easily spiral out of control.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在本地或云端托管开源模型时，专用计算能力成为关键考虑因素。尽管 GPU 实例可能看起来是最方便的选择，但成本可能会迅速失控。
- en: In this easy-to-follow guide, we will discover how to run quantized versions
    of open-source LLMs on local CPU inference for retrieval-augmented generation
    (aka document Q&A) in Python. In particular, we will leverage the latest, highly-performant
    [Llama 2](https://ai.meta.com/llama/) chat model in this project.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这份易于跟随的指南中，我们将探讨如何在 Python 中为检索增强生成（即文档问答）运行开源 LLM 的量化版本。在这个项目中，我们特别利用了最新的高性能
    [Llama 2](https://ai.meta.com/llama/) 聊天模型。
- en: Contents
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内容
- en: '***(1)*** [*Quick Primer on Quantization*](#afd1)***(2)*** [*Tools and Data*](#3527)***(3)***
    [*Open-Source LLM Selection*](#b5a9)***(4)*** [*Step-by-Step Guide*](#5fa3)***(5)***
    [*Next Steps*](#be5c)'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***(1)*** [*量化简明指南*](#afd1)***(2)*** [*工具和数据*](#3527)***(3)*** [*开源 LLM 选择*](#b5a9)***(4)***
    [*逐步指南*](#5fa3)***(5)*** [*后续步骤*](#be5c)'
- en: The accompanying GitHub repo for this article can be found [**here**](https://github.com/kennethleungty/Llama-2-Open-Source-LLM-CPU-Inference).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的 GitHub 仓库可以在 [**这里**](https://github.com/kennethleungty/Llama-2-Open-Source-LLM-CPU-Inference)
    找到。
- en: (1) Quick *Primer on Quantization*
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: (1) 快速 *量化简明指南*
- en: LLMs have demonstrated excellent capabilities but are known to be compute- and
    memory-intensive. To manage their downside, we can use quantization to compress
    these models to reduce the memory footprint and accelerate computational inference
    while maintaining model performance.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）展示了出色的能力，但它们的计算和内存需求却很高。为了管理这些缺点，我们可以使用量化来压缩这些模型，从而减少内存占用，加速计算推理，同时保持模型性能。
- en: Quantization is the technique of reducing the number of bits used to represent
    a number or value. In the context of LLMs, it involves reducing the precision
    of the model’s parameters by storing the weights in lower-precision data types.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是减少表示一个数字或值所用位数的技术。在大型语言模型的背景下，它涉及通过将权重存储在低精度数据类型中来减少模型参数的精度。
- en: Since it reduces model size, quantization is beneficial for deploying models
    on resource-constrained devices like CPUs or embedded systems.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由于量化减少了模型的大小，因此它有利于在资源受限的设备上部署模型，如 CPU 或嵌入式系统。
- en: A common method is to quantize model weights from their original 16-bit floating-point
    values to lower precision ones like 8-bit integer values.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的方法是将模型权重从其原始的16位浮点值量化为较低精度的8位整数值。
- en: '![](../Images/2a385153273118660c8eb7265f870e49.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2a385153273118660c8eb7265f870e49.png)'
- en: Weight quantization from FP16 to INT8 | Image by author
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 从 FP16 到 INT8 的权重量化 | 图片作者
- en: (2) Tools and Data
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: (2) 工具和数据
- en: The following diagram illustrates the architecture of the document knowledge
    Q&A application we will build in this project.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了我们将在本项目中构建的文档知识问答应用程序的架构。
- en: '![](../Images/b0261b53ed8cc4fa5ac7e31b5d210072.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b0261b53ed8cc4fa5ac7e31b5d210072.png)'
- en: Document Q&A Architecture | Image by author
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 文档问答架构 | 图片作者
- en: The file we will run the document Q&A on is the public [177-page 2022 annual
    report](https://ir.manutd.com/financial-information/annual-reports/2022.aspx)
    of Manchester United Football Club.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要运行文档问答的文件是曼彻斯特联足球俱乐部的公共[177页2022年年度报告](https://ir.manutd.com/financial-information/annual-reports/2022.aspx)。
- en: '*Data Source:* Manchester United Plc (2022). 2022 Annual Report 2022 on Form
    20-F. [https://ir.manutd.com/~/media/Files/M/Manutd-IR/documents/manu-20f-2022-09-24.pdf](https://ir.manutd.com/~/media/Files/M/Manutd-IR/documents/manu-20f-2022-09-24.pdf)
    (CC0: Public Domain, as [SEC content](https://www.sec.gov/os/webmaster-faq#:~:text=All%20Government%2Dcreated%20content%20on,free%20to%20access%20and%20reuse.)
    is public domain and free to use)'
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*数据来源：* 曼彻斯特联队有限公司（2022）。2022年20-F表格年度报告。[https://ir.manutd.com/~/media/Files/M/Manutd-IR/documents/manu-20f-2022-09-24.pdf](https://ir.manutd.com/~/media/Files/M/Manutd-IR/documents/manu-20f-2022-09-24.pdf)（CC0：公共领域，因为[SEC内容](https://www.sec.gov/os/webmaster-faq#:~:text=All%20Government%2Dcreated%20content%20on,free%20to%20access%20and%20reuse.)
    是公共领域且可以自由使用）'
- en: The local machine for this project has an **AMD Ryzen 5 5600X 6-Core Processor**
    coupled with **16GB RAM** (DDR4 3600). While it also has an RTX 3060TI GPU (8GB
    VRAM), it will not be used in this project since we will focus on CPU usage.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目的本地机器配备了**AMD Ryzen 5 5600X 6核处理器**和**16GB RAM**（DDR4 3600）。虽然它还配备了 RTX 3060TI
    GPU（8GB VRAM），但在本项目中将不使用该 GPU，因为我们将专注于 CPU 的使用。
- en: 'Let us now explore the software tools we will leverage in building this backend
    application:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们探讨一下在构建这个后台应用程序时将使用的软件工具：
- en: (i) LangChain
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: (i) LangChain
- en: '[LangChain](https://python.langchain.com/docs/get_started) is a popular framework
    for developing applications powered by language models. It provides an extensive
    set of integrations and data connectors, allowing us to chain and orchestrate
    different modules to create advanced use cases like chatbots, data analysis, and
    document Q&A.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[LangChain](https://python.langchain.com/docs/get_started) 是一个流行的框架，用于开发由语言模型驱动的应用程序。它提供了一整套集成和数据连接器，允许我们将不同的模块链式连接和编排，创建诸如聊天机器人、数据分析和文档问答等高级用例。'
- en: (ii) C Transformers
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: (ii) C Transformers
- en: '[C Transformers](https://github.com/marella/ctransformers) is the Python library
    that provides bindings for transformer models implemented in C/C++ using the [GGML](https://github.com/ggerganov/ggml)
    library. At this point, let us first understand what GGML is about.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[C Transformers](https://github.com/marella/ctransformers) 是一个 Python 库，提供了对使用[GGML](https://github.com/ggerganov/ggml)库在
    C/C++ 中实现的变换模型的绑定。首先，让我们了解一下 GGML 的内容。'
- en: Built by the team at [ggml.ai](https://ggml.ai/), the GGML library is a tensor
    library designed for machine learning, where it enables large models to be run
    on consumer hardware with high performance. This is achieved through integer quantization
    support and built-in optimization algorithms.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 由 [ggml.ai](https://ggml.ai/) 团队构建的 GGML 库是一个用于机器学习的张量库，它使大型模型能够在消费级硬件上高效运行。这是通过整数量化支持和内置优化算法实现的。
- en: As a result, GGML versions of LLMs (quantized models in binary formats) can
    be run performantly on CPUs. Given that we are working with Python in this project,
    we will use the C Transformers library, which essentially offers the Python bindings
    for the GGML models.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，LLM 的 GGML 版本（量化模型的二进制格式）可以在 CPU 上高效运行。鉴于我们在这个项目中使用 Python，我们将使用 C Transformers
    库，它本质上提供了 GGML 模型的 Python 绑定。
- en: C Transformers supports a selected set of open-source models, including popular
    ones like Llama, GPT4All-J, MPT, and Falcon.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: C Transformers 支持一组选定的开源模型，包括像 Llama、GPT4All-J、MPT 和 Falcon 等流行模型。
- en: '![](../Images/ddb81375f29847ad669c11c9adde31ce.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ddb81375f29847ad669c11c9adde31ce.png)'
- en: LLMs (and corresponding model type name) supported on C Transformers | Image
    by author
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 支持 C Transformers 的 LLM（及相应的模型类型名称） | 图片由作者提供
- en: (iii) Sentence-Transformers Embeddings Model
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: (iii) Sentence-Transformers 嵌入模型
- en: '[sentence-transformers](https://github.com/UKPLab/sentence-transformers) is
    a Python library that provides easy methods to compute embeddings (dense vector
    representations) for sentences, text, and images.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[sentence-transformers](https://github.com/UKPLab/sentence-transformers) 是一个
    Python 库，提供了计算句子、文本和图像的嵌入（稠密向量表示）的方法。'
- en: It enables users to compute embeddings for more than 100 languages, which can
    then be compared to find sentences with similar meanings.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 它使用户能够计算超过 100 种语言的嵌入，然后可以进行比较以找到具有相似含义的句子。
- en: We will use the open-source [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)
    model for this project because it offers optimal speed and excellent general-purpose
    embedding quality.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用开源的 [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)
    模型，因为它提供了最佳的速度和出色的通用嵌入质量。
- en: (iv) FAISS
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: (iv) FAISS
- en: '[Facebook AI Similarity Search (FAISS)](https://github.com/facebookresearch/faiss)
    is a library designed for efficient similarity search and clustering of dense
    vectors.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[Facebook AI 相似性搜索（FAISS）](https://github.com/facebookresearch/faiss) 是一个设计用于高效相似性搜索和密集向量聚类的库。'
- en: Given a set of embeddings, we can use FAISS to index them and then leverage
    its powerful semantic search algorithms to search for the most similar vectors
    within the index.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一组嵌入，我们可以使用 FAISS 对它们进行索引，然后利用其强大的语义搜索算法在索引中搜索最相似的向量。
- en: Although it is not a full-fledged vector store in the traditional sense (like
    a database management system), it handles the storage of vectors in a way optimized
    for efficient nearest-neighbor searches.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然它不是传统意义上的完整向量存储（如数据库管理系统），但它以一种优化的方式处理向量存储，以便高效地进行最近邻搜索。
- en: (v) Poetry
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: (v) Poetry
- en: '[Poetry](https://python-poetry.org/) is used for setting up the virtual environment
    and handling Python package management in this project because of its ease of
    use and consistency.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[Poetry](https://python-poetry.org/) 被用于设置虚拟环境和处理 Python 包管理，因为它易于使用且一致性高。'
- en: Having previously worked with [venv](https://docs.python.org/3/library/venv.html),
    I highly recommend switching to Poetry as it makes dependency management more
    efficient and seamless.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 之前使用过 [venv](https://docs.python.org/3/library/venv.html) 的我强烈推荐切换到 Poetry，因为它使依赖管理更高效、更无缝。
- en: '*Check out* [*this video*](https://www.youtube.com/watch?v=0f3moPe_bhk) *to
    get started with Poetry.*'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*查看* [*这个视频*](https://www.youtube.com/watch?v=0f3moPe_bhk) *以开始使用 Poetry。*'
- en: '[](https://kennethleungty.medium.com/membership?source=post_page-----3d636037a3d8--------------------------------)
    [## Join Medium with Kenneth’s referral link'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://kennethleungty.medium.com/membership?source=post_page-----3d636037a3d8--------------------------------)
    [## 通过 Kenneth 的推荐链接加入 Medium'
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 作为 Medium 会员，你的一部分会员费将支付给你阅读的作者，并且你可以全面访问每一个故事。
- en: kennethleungty.medium.com](https://kennethleungty.medium.com/membership?source=post_page-----3d636037a3d8--------------------------------)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: kennethleungty.medium.com](https://kennethleungty.medium.com/membership?source=post_page-----3d636037a3d8--------------------------------)
- en: (3) Open-Source LLM Selection
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: (3) 开源 LLM 选择
- en: There has been tremendous progress in the open-source LLM space, and the many
    LLMs can be found on [HuggingFace’s Open LLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在开源 LLM 领域已经取得了巨大的进展，许多 LLM 可以在[HuggingFace 的 Open LLM 排行榜](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)上找到。
- en: 'I chose the latest open-source [**Llama-2–7B-Chat model**](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)
    ([GGML 8-bit](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML)) for this
    project based on the following considerations:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我为这个项目选择了最新的开源[**Llama-2–7B-Chat 模型**](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)
    ([GGML 8-bit](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML))，这是基于以下考虑：
- en: Model Type (Llama 2)
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型类型（Llama 2）
- en: It is an open-source model supported in the C Transformers library.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一个在 C Transformers 库中支持的开源模型。
- en: Currently the top performer across multiple metrics based on its Open LLM leaderboard
    rankings (as of July 2023).
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据 2023 年 7 月的 Open LLM 排行榜，它在多个指标上表现最佳。
- en: Demonstrates a huge improvement on the previous benchmark set by the original
    Llama model.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在原始 Llama 模型设定的基准上表现出巨大的改进。
- en: It is widely mentioned and downloaded in the community.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在社区中被广泛提及和下载。
- en: Model Size (7B)
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型大小（7B）
- en: Given that we are performing document Q&A, the LLM will primarily be used for
    the relatively simple task of summarizing document chunks. Therefore, the 7B model
    size fits our needs as we technically do not require an overly large model (e.g.,
    65B and above) for this task.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鉴于我们正在进行文档问答，LLM 将主要用于相对简单的任务——总结文档块。因此，7B 模型的大小符合我们的需求，因为我们在技术上并不需要过于庞大的模型（例如
    65B 及以上）。
- en: Fine-tuned Version (Llama-2-7B-Chat)
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调版本（Llama-2-7B-Chat）
- en: The Llama-2-7B base model is built for text completion, so it lacks the fine-tuning
    required for optimal performance in document Q&A use cases.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Llama-2-7B 基础模型是为文本补全构建的，因此缺乏在文档问答用例中获得最佳性能所需的微调。
- en: The Llama-2–7B-Chat model is the ideal candidate for our use case since it is
    designed for conversation and Q&A.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Llama-2–7B-Chat 模型是我们用例的理想选择，因为它是专为对话和问答设计的。
- en: The model is licensed (partially) for commercial use. It is because the fine-tuned
    model Llama-2-Chat model leverages publicly available instruction datasets and
    over 1 million human annotations.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型（部分）授权用于商业用途。这是因为经过微调的 Llama-2-Chat 模型利用了公开的指令数据集和超过 100 万条人工标注。
- en: Quantized Format (8-bit)
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 量化格式（8位）
- en: Given that the RAM is constrained to 16GB, the 8-bit GGML version is suitable
    as it only requires a memory size of 9.6GB.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鉴于 RAM 限制为 16GB，8 位 GGML 版本适合，因为它只需要 9.6GB 的内存。
- en: The 8-bit format also offers a comparable response quality to 16-bit.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 8 位格式也提供了与 16 位相当的响应质量。
- en: The original unquantized 16-bit model requires a memory of ~15 GB, which is
    too close to the 16GB RAM limit.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始的未量化 16 位模型需要约 15 GB 的内存，这距离 16GB RAM 的限制非常接近。
- en: Other smaller quantized formats (i.e., 4-bit and 5–bit) are available, but they
    come at the expense of accuracy and response quality.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还有其他更小的量化格式（即 4 位和 5 位），但它们在准确性和响应质量上有所折衷。
- en: (4) Step-by-Step Guide
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: (4) 步骤指南
- en: Now that we know the various components, let us go through the step-by-step
    guide on how to build the document Q&A application.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了各种组件，让我们逐步了解如何构建文档问答应用程序。
- en: '*The accompanying codes for this guide can be found in* [***this GitHub repo***](https://github.com/kennethleungty/Llama-2-Open-Source-LLM-CPU-Inference)*,
    and all the dependencies can be found in the* [*requirements.txt*](https://github.com/kennethleungty/Llama-2-Open-Source-LLM-CPU-Inference/blob/main/requirements.txt)
    *file.*'
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*本指南的配套代码可以在* [***这个 GitHub 仓库***](https://github.com/kennethleungty/Llama-2-Open-Source-LLM-CPU-Inference)*中找到，所有依赖项可以在*
    [*requirements.txt*](https://github.com/kennethleungty/Llama-2-Open-Source-LLM-CPU-Inference/blob/main/requirements.txt)
    *文件中找到。*'
- en: '***Note****: Since many tutorials are already out there, we will* ***not***
    *be deep diving into the intricacies and details of the general document Q&A components
    (e.g., text chunking, vector store setup). We will instead focus on the open-source
    LLM and CPU inference aspects in this article.*'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '***注意***：由于已有许多教程，我们* ***不会*** *深入探讨一般文档问答组件的复杂性和细节（例如，文本分块、向量存储设置）。我们将重点关注开源
    LLM 和 CPU 推断方面的内容。*'
- en: Step 1 — Process data and build vector store
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 1 — 处理数据并构建向量存储
- en: 'In this step, three sub-tasks will be performed:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，将执行三个子任务：
- en: Data ingestion and splitting text into chunks
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据摄取和将文本拆分成块
- en: Load embeddings model (sentence-transformers)
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载嵌入模型（sentence-transformers）
- en: Index chunks and store in FAISS vector store
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 索引块并存储在 FAISS 向量存储中
- en: After running the Python script above, the vector store will be generated and
    saved in the local directory named `'vectorstore/db_faiss'`, and is ready for
    semantic search and retrieval.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述 Python 脚本后，向量存储将被生成并保存在名为 `'vectorstore/db_faiss'` 的本地目录中，并且准备好进行语义搜索和检索。
- en: Step 2 — Set up prompt template
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 2 — 设置提示模板
- en: Given that we use the Llama-2–7B-Chat model, we must be mindful of the prompt
    templates utilized here.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们使用的是 Llama-2–7B-Chat 模型，我们必须注意这里使用的提示模板。
- en: For example, OpenAI’s GPT models are designed to be conversation-in and message-out.
    It means input templates are expected to be in a chat-like [transcript format](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/chatgpt?pivots=programming-language-chat-ml)
    (e.g., separate system and user messages).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，OpenAI 的 GPT 模型设计为对话输入和消息输出。这意味着输入模板应该是类似聊天的 [记录格式](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/chatgpt?pivots=programming-language-chat-ml)（例如，分开的系统和用户消息）。
- en: However, those templates would not work here because our Llama 2 model is not
    specifically optimized for that kind of conversational interface. Instead, a classic
    prompt template like the one below would be preferred.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，这些模板在这里无法使用，因为我们的 Llama 2 模型并未特别优化用于那种对话界面。相反，更经典的提示模板，如下所示，将更为合适。
- en: '***Note:*** The relatively smaller LLMs, like the 7B model, appear particularly
    sensitive to formatting. For instance, I got slightly different outputs when I
    altered the whitespaces and indentation of the prompt template.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '***注意：*** 较小的 LLM，如 7B 模型，似乎对格式特别敏感。例如，当我改变提示模板的空格和缩进时，输出略有不同。'
- en: Step 3 — Download the Llama-2–7B-Chat GGML binary file
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 3 — 下载 Llama-2–7B-Chat GGML 二进制文件
- en: Since we will be running the LLM locally, we need to download the binary file
    of the quantized Llama-2–7B-Chat model.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将在本地运行 LLM，因此需要下载量化的 Llama-2–7B-Chat 模型的二进制文件。
- en: We can do so by visiting [TheBloke’s Llama-2–7B-Chat GGML page hosted on Hugging
    Face](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML) and then downloading
    the GGML 8-bit quantized file named `llama-2–7b-chat.ggmlv3.q8_0.bin`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过访问 [TheBloke 的 Llama-2–7B-Chat GGML 页面，托管在 Hugging Face 上](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML)，然后下载名为
    `llama-2–7b-chat.ggmlv3.q8_0.bin` 的 GGML 8 位量化文件。
- en: '![](../Images/c4b9e92e2546826aae6a0a006154a48c.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4b9e92e2546826aae6a0a006154a48c.png)'
- en: Files and versions page of Llama-2–7B-Chat-GGML page on HuggingFace | Image
    by author
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: HuggingFace 上 Llama-2–7B-Chat-GGML 页面中的文件和版本页面 | 作者提供的图片
- en: The downloaded `.bin` file for the 8-bit quantized model can be saved in a suitable
    project subfolder like `/models`.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 下载的 `.bin` 文件用于 8 位量化模型，可以保存在类似 `/models` 的适当项目子文件夹中。
- en: 'The [model card page](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML)
    also displays more information and details for each quantized format:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[模型卡页面](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML)还展示了每种量化格式的更多信息和细节：'
- en: '![](../Images/9357c083bdf782b74d96b526c7989a08.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9357c083bdf782b74d96b526c7989a08.png)'
- en: Different quantized formats with details | Image by author
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 不同量化格式的详细信息 | 作者提供的图片
- en: '***Note:*** *To download other GGML quantized models supported by C Transformers,
    visit the main* [*TheBloke page on HuggingFace*](https://huggingface.co/TheBloke)
    *to search for your desired model and look for the links with names that end with
    ‘-GGML’.*'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '***注意：*** *要下载 C Transformers 支持的其他 GGML 量化模型，请访问主* [*TheBloke 在 HuggingFace
    的页面*](https://huggingface.co/TheBloke) *以搜索您所需的模型，并查找名称以 ''-GGML'' 结尾的链接。*'
- en: Step 4— Setup LLM
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 4 — 设置 LLM
- en: To utilize the GGML model we downloaded, we will leverage the [integration](https://python.langchain.com/docs/ecosystem/integrations/ctransformers)
    between C Transformers and LangChain. Specifically, we will use the CTransformers
    LLM wrapper in LangChain, which provides a unified interface for the GGML models.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用我们下载的 GGML 模型，我们将使用 [C Transformers 和 LangChain 的集成](https://python.langchain.com/docs/ecosystem/integrations/ctransformers)。具体来说，我们将使用
    LangChain 中的 CTransformers LLM 包装器，它为 GGML 模型提供了统一的接口。
- en: We can define a host of [configuration settings](https://github.com/marella/ctransformers#config)
    for the LLM, such as maximum new tokens, top k value, temperature, and repetition
    penalty.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以为 LLM 定义一系列 [配置设置](https://github.com/marella/ctransformers#config)，例如最大新令牌数、top
    k 值、温度和重复惩罚。
- en: '***Note****:* I set the temperature as 0.01 instead of 0 because I got odd
    responses (e.g., a long repeated string of the letter E) when the temperature
    was exactly zero.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '***注意：*** *我将温度设置为 0.01，而不是 0，因为当温度恰好为零时，我得到了奇怪的响应（例如，一长串重复的字母 E）。*'
- en: Step 5 — Build and initialize RetrievalQA
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第5步 — 构建和初始化 RetrievalQA
- en: With the prompt template and C Transformers LLM ready, we write three functions
    to build the LangChain RetrievalQA object that enables us to perform document
    Q&A.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好提示模板和 C Transformers LLM 后，我们编写了三个函数来构建 LangChain RetrievalQA 对象，使我们能够执行文档问答。
- en: Step 6 — Combining into the main script
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第6步 — 整合到主脚本中
- en: The next step is to combine the previous components into the `main.py` script.
    The `argparse` module is used because we will pass our user query into the application
    from the command line.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将之前的组件整合到 `main.py` 脚本中。由于我们将通过命令行将用户查询传递到应用程序，因此使用了 `argparse` 模块。
- en: Given that we will return source documents, additional code is appended to process
    the document chunks for a better visual display.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们将返回源文档，附加代码用于处理文档片段，以获得更好的视觉显示效果。
- en: To evaluate the speed of CPU inference, the `timeit` module is also utilized.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估 CPU 推理的速度，还使用了 `timeit` 模块。
- en: Step 7 — Running a sample query
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第7步 — 运行示例查询
- en: It is now time to put our application to the test. Upon loading the virtual
    environment from the project directory, we can run a command in the command line
    interface (CLI) that comprises our user query.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候对我们的应用程序进行测试了。在从项目目录加载虚拟环境后，我们可以在命令行界面（CLI）中运行一个包含用户查询的命令。
- en: 'For example, we can ask about the value of the minimum guarantee payable by
    Adidas (Manchester United’s global technical sponsor) with the following command:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以使用以下命令询问 Adidas（曼联的全球技术赞助商）最低保障金额的值：
- en: '[PRE0]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '***Note:*** If we are not using Poetry, we can omit the prepended `poetry run`.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '***注意：*** 如果我们不使用 Poetry，我们可以省略前面的 `poetry run`。'
- en: Results
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: '![](../Images/f240bd0d09d6d249e0eca0ad1a743aad.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f240bd0d09d6d249e0eca0ad1a743aad.png)'
- en: Output from user query passed into document Q&A application | Image by author
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 从用户查询传递到文档问答应用程序的输出 | 图片作者
- en: The output shows that we successfully obtained the correct response for our
    user query (i.e., £750 million), along with the relevant document chunks that
    are semantically similar to the query.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示我们成功获取了用户查询的正确响应（即 7.5 亿英镑），以及与查询语义相似的相关文档片段。
- en: The total time of 31 seconds for launching the application and generating a
    response is pretty good, given that we are running it locally on an AMD Ryzen
    5600X (which is a good CPU but by no means the best in the market currently).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 启动应用程序并生成响应的总时间为 31 秒，考虑到我们在本地使用的是 AMD Ryzen 5600X（虽然是一款不错的 CPU，但绝非市场上最好的），这个时间还是相当不错的。
- en: The result is even more impressive given that running LLM inference on GPUs
    (e.g., directly on HuggingFace) can also take double-digit seconds.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 结果更为令人印象深刻，因为在 GPU 上运行 LLM 推理（例如，直接在 HuggingFace 上）也可能需要两位数的秒数。
- en: Your Mileage May Vary
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你的使用情况可能会有所不同
- en: Depending on your CPU, the time taken to obtain a response may vary. For example,
    when I test it out on my laptop, it could go into the range of several minutes.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的 CPU，获取响应的时间可能会有所不同。例如，当我在我的笔记本电脑上测试时，可能会在几分钟的范围内。
- en: The thing to note is that getting LLMs to fit into consumer hardware is still
    in the early stages, so we cannot expect speeds that are on par with OpenAI APIs
    (which are driven by loads of computing power).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，将 LLM 适配到消费级硬件仍处于早期阶段，因此我们不能期望达到与 OpenAI API（由大量计算能力驱动）相当的速度。
- en: For now, one can certainly consider running this on a more powerful CPU instance,
    or switching to using GPU instances (such as free ones on Google Colab).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，确实可以考虑在更强大的 CPU 实例上运行此程序，或者切换到使用 GPU 实例（例如 Google Colab 上的免费实例）。
- en: (5) Next Steps
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: (5) 下一步
- en: Now that we have built a document Q&A backend LLM application that runs on CPU
    inference, there are many exciting steps we can take to bring this project forward.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建了一个运行在 CPU 推理上的文档问答后端 LLM 应用程序，我们可以采取许多令人兴奋的步骤来推进这个项目。
- en: 'Build a frontend chat interface with Streamlit, especially since it has made
    two major announcements recently: [Integration of Streamlit with LangChain](https://blog.streamlit.io/langchain-streamlit/),
    and the [launch of Streamlit ChatUI](https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps)
    to build powerful chatbot interfaces easily.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Streamlit 构建前端聊天界面，特别是考虑到它最近做出了两个重要公告：[Streamlit 与 LangChain 的集成](https://blog.streamlit.io/langchain-streamlit/)，以及
    [Streamlit ChatUI 的发布](https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps)，以便轻松构建强大的聊天机器人界面。
- en: Dockerize and deploy the application on a cloud CPU instance. While we have
    explored local inference, the application can easily be ported to the cloud. We
    can also leverage more powerful CPU instances on the cloud to speed up inference
    (e.g., [compute-optimized](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/compute-optimized-instances.html)
    AWS EC2 instances like c5.4xlarge)
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将应用程序容器化并部署到云CPU实例上。虽然我们已经探索了本地推理，但应用程序可以轻松移植到云端。我们还可以利用云端更强大的CPU实例来加速推理（例如，[计算优化](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/compute-optimized-instances.html)的AWS
    EC2实例，如c5.4xlarge）
- en: Experiment with slightly larger LLMs like the [Llama 13B Chat](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML)
    model. Since we have worked with 7B models, assessing the performance of slightly
    larger ones is a good idea since they should theoretically be more accurate and
    still fit within memory.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试稍大的LLMs，如[Llama 13B Chat](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML)模型。由于我们已经处理了7B模型，评估稍大一些的模型的性能是个好主意，因为它们在理论上应该更准确，并且仍能适应内存。
- en: Experiment with smaller quantized formats like the 4-bit and 5-bit (including
    those with the new k-quant method) to objectively evaluate the differences in
    inference speed and response quality.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试使用较小的量化格式，如4-bit和5-bit（包括使用新k-quant方法的格式），以客观评估推理速度和响应质量的差异。
- en: Leverage local GPU to speed up inference. If we want to test the use of [GPUs
    on the C Transformers models](https://github.com/marella/ctransformers#gpu), we
    can do so by running some of the model layers on the GPU. It is useful because
    Llama is the only model type that has GPU support currently.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用本地GPU加速推理。如果我们想测试在[C Transformers模型上使用GPU](https://github.com/marella/ctransformers#gpu)，可以通过在GPU上运行一些模型层来实现。这是有用的，因为目前只有Llama模型类型支持GPU。
- en: Evaluate the use of [vLLM](https://vllm.readthedocs.io/en/latest/), a high-throughput
    and memory-efficient inference and serving engine for LLMs. However, utilizing
    vLLM requires the use of GPUs.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估使用[vLLM](https://vllm.readthedocs.io/en/latest/)，这是一个高吞吐量且内存高效的LLMs推理和服务引擎。然而，使用vLLM需要使用GPU。
- en: I will work on articles and projects addressing the above ideas in the upcoming
    weeks, so stay tuned for more insightful generative AI content!
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在接下来的几周内处理有关上述想法的文章和项目，敬请关注更多有见地的生成式AI内容！
- en: Before you go
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 离开前
- en: I welcome you to **join me on a journey of data science discovery!** Follow
    this [Medium](https://kennethleungty.medium.com/) page and visit my [GitHub](https://github.com/kennethleungty)
    to stay updated with more engaging and practical content. Meanwhile, have fun
    running open-source LLMs on CPU inference!
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我欢迎你**加入我数据科学发现之旅！** 关注这个[Medium](https://kennethleungty.medium.com/)页面，并访问我的[GitHub](https://github.com/kennethleungty)以获取更多有趣且实用的内容。同时，享受在CPU推理上运行开源LLMs的乐趣吧！
- en: '[](/arxiv-keyword-extraction-and-analysis-pipeline-with-keybert-and-taipy-2972e81d9fa4?source=post_page-----3d636037a3d8--------------------------------)
    [## arXiv Keyword Extraction and Analysis Pipeline with KeyBERT and Taipy'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/arxiv-keyword-extraction-and-analysis-pipeline-with-keybert-and-taipy-2972e81d9fa4?source=post_page-----3d636037a3d8--------------------------------)
    [## arXiv关键词提取与分析管道，使用KeyBERT和Taipy]'
- en: Build a keyword analysis application in Python comprising a frontend user interface
    and backend pipeline
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Python中构建一个包含前端用户界面和后台管道的关键词分析应用程序
- en: towardsdatascience.com](/arxiv-keyword-extraction-and-analysis-pipeline-with-keybert-and-taipy-2972e81d9fa4?source=post_page-----3d636037a3d8--------------------------------)
    [](/how-to-dockerize-machine-learning-applications-built-with-h2o-mlflow-fastapi-and-streamlit-a56221035eb5?source=post_page-----3d636037a3d8--------------------------------)
    [## How to Dockerize Machine Learning Applications Built with H2O, MLflow, FastAPI,
    and Streamlit
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/arxiv-keyword-extraction-and-analysis-pipeline-with-keybert-and-taipy-2972e81d9fa4?source=post_page-----3d636037a3d8--------------------------------)
    [](/how-to-dockerize-machine-learning-applications-built-with-h2o-mlflow-fastapi-and-streamlit-a56221035eb5?source=post_page-----3d636037a3d8--------------------------------)
    [## 如何Docker化构建于H2O、MLflow、FastAPI和Streamlit的机器学习应用程序]
- en: An easy-to-follow guide to containerizing multi-service ML applications with
    Docker
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个简单易懂的Docker多服务ML应用程序容器化指南
- en: towardsdatascience.com](/how-to-dockerize-machine-learning-applications-built-with-h2o-mlflow-fastapi-and-streamlit-a56221035eb5?source=post_page-----3d636037a3d8--------------------------------)
    [](/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f?source=post_page-----3d636037a3d8--------------------------------)
    [## Micro, Macro & Weighted Averages of F1 Score, Clearly Explained
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/how-to-dockerize-machine-learning-applications-built-with-h2o-mlflow-fastapi-and-streamlit-a56221035eb5?source=post_page-----3d636037a3d8--------------------------------)
    [](/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f?source=post_page-----3d636037a3d8--------------------------------)
    [## F1分数的微平均、宏平均与加权平均，详细解释'
- en: Understanding the concepts behind the micro average, macro average and weighted
    average of F1 score in multi-class classification.
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解多类分类中F1分数的微平均、宏平均和加权平均的概念。
- en: towardsdatascience.com](/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f?source=post_page-----3d636037a3d8--------------------------------)
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f?source=post_page-----3d636037a3d8--------------------------------)'
