- en: 'Prompt Engineering 101: Zero, One, and Few-Shot Prompting'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示工程101：零次、一次和少次提示
- en: 原文：[https://towardsdatascience.com/prompt-engineering-101-zero-one-and-few-shot-prompting-1e8ced03d434?source=collection_archive---------1-----------------------#2023-09-19](https://towardsdatascience.com/prompt-engineering-101-zero-one-and-few-shot-prompting-1e8ced03d434?source=collection_archive---------1-----------------------#2023-09-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/prompt-engineering-101-zero-one-and-few-shot-prompting-1e8ced03d434?source=collection_archive---------1-----------------------#2023-09-19](https://towardsdatascience.com/prompt-engineering-101-zero-one-and-few-shot-prompting-1e8ced03d434?source=collection_archive---------1-----------------------#2023-09-19)
- en: An introduction to a basic prompt engineering strategy
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本提示工程策略简介
- en: '[](https://medium.com/@aashishnair?source=post_page-----1e8ced03d434--------------------------------)[![Aashish
    Nair](../Images/23f4b3839e464419332b690a4098d824.png)](https://medium.com/@aashishnair?source=post_page-----1e8ced03d434--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1e8ced03d434--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1e8ced03d434--------------------------------)
    [Aashish Nair](https://medium.com/@aashishnair?source=post_page-----1e8ced03d434--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@aashishnair?source=post_page-----1e8ced03d434--------------------------------)[![Aashish
    Nair](../Images/23f4b3839e464419332b690a4098d824.png)](https://medium.com/@aashishnair?source=post_page-----1e8ced03d434--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1e8ced03d434--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1e8ced03d434--------------------------------)
    [Aashish Nair](https://medium.com/@aashishnair?source=post_page-----1e8ced03d434--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3087ba81e065&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprompt-engineering-101-zero-one-and-few-shot-prompting-1e8ced03d434&user=Aashish+Nair&userId=3087ba81e065&source=post_page-3087ba81e065----1e8ced03d434---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1e8ced03d434--------------------------------)
    ·5 min read·Sep 19, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1e8ced03d434&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprompt-engineering-101-zero-one-and-few-shot-prompting-1e8ced03d434&user=Aashish+Nair&userId=3087ba81e065&source=-----1e8ced03d434---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3087ba81e065&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprompt-engineering-101-zero-one-and-few-shot-prompting-1e8ced03d434&user=Aashish+Nair&userId=3087ba81e065&source=post_page-3087ba81e065----1e8ced03d434---------------------post_header-----------)
    发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----1e8ced03d434--------------------------------)
    ·5分钟阅读·2023年9月19日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1e8ced03d434&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprompt-engineering-101-zero-one-and-few-shot-prompting-1e8ced03d434&user=Aashish+Nair&userId=3087ba81e065&source=-----1e8ced03d434---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1e8ced03d434&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprompt-engineering-101-zero-one-and-few-shot-prompting-1e8ced03d434&source=-----1e8ced03d434---------------------bookmark_footer-----------)![](../Images/dc425a1abba413deb8fc810c761d1831.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1e8ced03d434&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprompt-engineering-101-zero-one-and-few-shot-prompting-1e8ced03d434&source=-----1e8ced03d434---------------------bookmark_footer-----------)![](../Images/dc425a1abba413deb8fc810c761d1831.png)'
- en: Image by [Alexandra_Koch](https://pixabay.com/users/alexandra_koch-621802/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=7720802)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=7720802)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Alexandra_Koch](https://pixabay.com/users/alexandra_koch-621802/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=7720802)提供，来自[Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=7720802)
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: Despite their seemingly supernatural capabilities, LLMs are ultimately predictive
    models that simply predict the next word in the sequence of words based on the
    provided context.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它们看似具有超自然的能力，但LLM（大型语言模型）最终仍然是预测模型，仅根据提供的上下文预测序列中的下一个词。
- en: As such, their performances don’t just hinge on the vast volumes of data they
    are trained with; they also depend heavily on the context provided through the
    users’ inputs.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，它们的表现不仅仅依赖于它们接受过的大量数据；它们还严重依赖于用户输入中提供的上下文。
- en: Frequent users of LLM-powered chatbots are aware of the importance of context.
    Without sufficient context, chatbots, whether they are publicly available services
    (e.g., ChatGPT) or custom-made LLM products, will struggle to carry out the more
    complex instructions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 频繁使用LLM驱动聊天机器人的用户知道上下文的重要性。没有足够的上下文，无论是公开服务（例如ChatGPT）还是定制的LLM产品，聊天机器人都难以执行更复杂的指令。
- en: 'Here, we delve into one of the most basic strategies used to guide LLMs to
    answer prompts correctly: providing context within user prompts. This is often
    carried out with 3 different methods: zero-shot prompting, one-shot prompting,
    and few-shot prompting.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将深入探讨引导大型语言模型（LLMs）正确回答提示的最基本策略之一：在用户提示中提供上下文。这通常通过三种不同的方法进行：零-shot 提示、一-shot
    提示和少-shot 提示。
- en: Zero-Shot Prompting
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 零-Shot 提示
- en: If you’ve interacted with an LLM-powered chatbot before, you’ve likely already
    used zero-shot prompting unwittingly. Zero-shot prompting entails relying solely
    on an LLM’s pre-trained information…
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前与一个由LLM驱动的聊天机器人互动过，你很可能已经不自觉地使用了零-shot 提示。零-shot 提示涉及仅依靠LLM的预训练信息……
