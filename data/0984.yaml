- en: A Pathway Towards Responsible AI Generated Content
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迈向负责任的AI生成内容之路
- en: 原文：[https://towardsdatascience.com/a-pathway-towards-responsible-ai-generated-content-6c915e8155f9?source=collection_archive---------6-----------------------#2023-03-16](https://towardsdatascience.com/a-pathway-towards-responsible-ai-generated-content-6c915e8155f9?source=collection_archive---------6-----------------------#2023-03-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-pathway-towards-responsible-ai-generated-content-6c915e8155f9?source=collection_archive---------6-----------------------#2023-03-16](https://towardsdatascience.com/a-pathway-towards-responsible-ai-generated-content-6c915e8155f9?source=collection_archive---------6-----------------------#2023-03-16)
- en: Warnings from privacy, bias, toxicity, misinformation and IP issues
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于隐私、偏见、毒性、错误信息和知识产权问题的警告
- en: '[](https://lingjuanlyu.medium.com/?source=post_page-----6c915e8155f9--------------------------------)[![Lingjuan
    Lyu](../Images/13d419f9c2245588585e9f08a71be829.png)](https://lingjuanlyu.medium.com/?source=post_page-----6c915e8155f9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6c915e8155f9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6c915e8155f9--------------------------------)
    [Lingjuan Lyu](https://lingjuanlyu.medium.com/?source=post_page-----6c915e8155f9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://lingjuanlyu.medium.com/?source=post_page-----6c915e8155f9--------------------------------)[![Lingjuan
    Lyu](../Images/13d419f9c2245588585e9f08a71be829.png)](https://lingjuanlyu.medium.com/?source=post_page-----6c915e8155f9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6c915e8155f9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6c915e8155f9--------------------------------)
    [Lingjuan Lyu](https://lingjuanlyu.medium.com/?source=post_page-----6c915e8155f9--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fca2f89d83dfb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pathway-towards-responsible-ai-generated-content-6c915e8155f9&user=Lingjuan+Lyu&userId=ca2f89d83dfb&source=post_page-ca2f89d83dfb----6c915e8155f9---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6c915e8155f9--------------------------------)
    ·11 min read·Mar 16, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6c915e8155f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pathway-towards-responsible-ai-generated-content-6c915e8155f9&user=Lingjuan+Lyu&userId=ca2f89d83dfb&source=-----6c915e8155f9---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fca2f89d83dfb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pathway-towards-responsible-ai-generated-content-6c915e8155f9&user=Lingjuan+Lyu&userId=ca2f89d83dfb&source=post_page-ca2f89d83dfb----6c915e8155f9---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6c915e8155f9--------------------------------)
    · 11分钟阅读 · 2023年3月16日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6c915e8155f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pathway-towards-responsible-ai-generated-content-6c915e8155f9&user=Lingjuan+Lyu&userId=ca2f89d83dfb&source=-----6c915e8155f9---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c915e8155f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pathway-towards-responsible-ai-generated-content-6c915e8155f9&source=-----6c915e8155f9---------------------bookmark_footer-----------)![](../Images/ec1f0af59fbc8d5a38c4ad691d14fe3b.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c915e8155f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pathway-towards-responsible-ai-generated-content-6c915e8155f9&source=-----6c915e8155f9---------------------bookmark_footer-----------)![](../Images/ec1f0af59fbc8d5a38c4ad691d14fe3b.png)'
- en: 'Figure 1: The scope of responsible AIGC.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：负责任的AI生成内容的范围。
- en: '**Introduction**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**介绍**'
- en: 'AI Generated Content (AIGC) has received tremendous attention within the past
    few years, with content ranging from image, text, to audio, video, etc. Meanwhile,
    AIGC has become a double-edged sword and recently received much criticism regarding
    its responsible usage. In this vision paper, we focus on three main risks that
    may hinder the healthy development and deployment of AIGC in practice, including
    risks from: (1) privacy; (2) bias, toxicity, misinformation; and (3) intellectual
    property (IP), as highlighted in Figure 1\. By documenting known and potential
    risks, as well as any possible misuse scenarios of AIGC, the aim is to draw attention
    to potential risks and misuse, help society to eliminate obstacles, and promote
    the more ethical and secure deployment of AIGC. Additionally, we provide insights
    into the promising directions for tackling these risks while constructing generative
    models, enabling AIGC to be used responsibly to benefit society.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: AI 生成内容（AIGC）在过去几年内引起了极大的关注，内容涵盖了图像、文本、音频、视频等。同时，AIGC 成为了一把双刃剑，最近也因其负责任的使用问题受到了许多批评。在这篇愿景论文中，我们关注可能阻碍
    AIGC 健康发展和实际部署的三大主要风险，包括：（1）隐私；（2）偏见、毒性、错误信息；和（3）知识产权（IP），如图 1 所示。通过记录已知和潜在的风险，以及
    AIGC 可能的误用场景，旨在引起对潜在风险和误用的关注，帮助社会消除障碍，推动 AIGC 更加伦理和安全的部署。此外，我们还提供了在构建生成模型时解决这些风险的有前景的方向，以便
    AIGC 能够负责任地使用，以造福社会。
- en: '**1\. Privacy**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 隐私**'
- en: '**Privacy leakage**. Large foundation models are known to be vulnerable to
    privacy risks, and it is possible that AIGC models that build upon these models
    could also be subject to privacy leakage. For instance, Stable Diffusion memorized
    duplicate images in the training data [Rombach *et al.*, 2022c]. [Somepalli *et
    al.*, 2022] demonstrated that Stable Diffusion blatantly copies images from its
    training data, and the generated images are simple combinations of the foreground
    and background objects of the training dataset. Moreover, the system occasionally
    displays the ability to reconstruct memories, producing objects that are semantically
    equivalent to the original without being identical in pixel form. The existence
    of such images raises concerns about data memorization and the ownership of diffusion
    images.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**隐私泄露**。大型基础模型已知存在隐私风险，建立在这些模型基础上的 AIGC 模型也可能面临隐私泄露。例如，Stable Diffusion 在训练数据中记住了重复的图像
    [Rombach *等*, 2022c]。 [Somepalli *等*, 2022] 证明 Stable Diffusion 明目张胆地从训练数据中复制图像，生成的图像仅仅是训练数据集中前景和背景对象的简单组合。此外，系统有时展示了重建记忆的能力，生成的对象在语义上等同于原始对象，但像素形式上并不完全相同。这些图像的存在引发了对数据记忆和扩散图像所有权的担忧。'
- en: Similarly, recent research has shown that Stable Diffusion and Google’s Imagen
    can leak photos of real people and copyrighted images [Heikkila ̈, 2023]. In Matthew
    Butterick’s recent litigation [Butterick, 2023], he pointed out that because all
    visual information in the system is derived from copyrighted training images,
    the images produced are necessarily works derived from those training images,
    regardless of their outward appearance. DALL·E 2 has also encountered similar
    problems. It can sometimes reproduce images from its training data rather than
    creating new ones. OpenAI found that this image regurgitation occurs due to images
    being replicated many times in the dataset. Similarly, when I asked ChatGPT “What
    is the privacy risk of ChatGPT”, it responded with 4 potential risks to privacy,
    as illustrated in Figure 2.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，近期研究显示 Stable Diffusion 和 Google 的 Imagen 可能泄露真实人物照片和受版权保护的图像 [Heikkila
    ̈, 2023]。在 Matthew Butterick 最近的诉讼中 [Butterick, 2023]，他指出由于系统中的所有视觉信息都来源于受版权保护的训练图像，因此生成的图像必然是这些训练图像的衍生作品，不论其外观如何。DALL·E
    2 也遇到了类似的问题。它有时会再现训练数据中的图像，而不是生成新的图像。OpenAI 发现这种图像复述现象发生的原因是数据集中图像被多次复制。类似地，当我问
    ChatGPT “ChatGPT 的隐私风险是什么”时，它回应了 4 种潜在的隐私风险，如图 2 所示。
- en: '![](../Images/204ee327dd53b1c1728d81f30b0be6a6.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/204ee327dd53b1c1728d81f30b0be6a6.png)'
- en: 'Figure 2: An answer to “What is the privacy risk of ChatGPT” by ChatGPT (Jan.
    30, 2023 version).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：ChatGPT 对“ChatGPT 的隐私风险是什么”的回答（2023年1月30日版本）。
- en: '**Privacy actions**. For privacy actions, at the industry level, Stability
    AI has recognized the limitations of Stable Diffusion, such as the potential for
    memorization of replicated images in the training data. To address this, they
    provide a [website](https://rom1504.github.io/clip-retrieval/) to support the
    identification of such memorized images. In addition, art company Spawning AI
    has created a website called “[Have I Been Trained](https://haveibeentrained.com)”
    to assist users in determining whether their photos or works have been used as
    AI training materials. OpenAI has taken steps to address privacy concerns by reducing
    data duplication through deduplication. Furthermore, companies such as Microsoft
    and Amazon have implemented measures to prevent employee breaches of confidentiality
    by banning the sharing of sensitive data with ChatGPT, given that this information
    could be utilized for training data for future versions of ChatGPT.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**隐私措施**。在隐私措施方面，业内，Stability AI已认识到Stable Diffusion的局限性，例如训练数据中可能记忆重复图像的潜在问题。为了解决这一问题，他们提供了一个
    [网站](https://rom1504.github.io/clip-retrieval/) 来支持识别这些记忆图像。此外，艺术公司Spawning AI创建了一个名为“[Have
    I Been Trained](https://haveibeentrained.com)”的网站，以帮助用户确定他们的照片或作品是否被用作AI训练材料。OpenAI采取了措施通过去重减少数据重复，以应对隐私问题。此外，微软和亚马逊等公司实施了措施，防止员工泄露机密，通过禁止与ChatGPT共享敏感数据，因为这些信息可能被用于ChatGPT未来版本的训练数据。'
- en: '**2.** **Bias, toxicity, misinformation**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**2.** **偏见、毒性、虚假信息**'
- en: '**Problematic datasets and AIGC models.** Since the training data used in AI
    models are collected in the real world, they can unintentionally reinforce harmful
    stereotypes, exclude or marginalize certain groups, and contain toxic data sources,
    which can incite hate or violence and offend individuals [Weidinger et al., 2021].
    For example, the LAION dataset, which is used to train diffusion models, has been
    criticized for containing problematic content related to social stereotyping,
    pornography, racist slurs, and violence. Although some AIGC models like Imagen
    try to filter out undesirable data, such as pornographic imagery and toxic language,
    the filtered data can still contain sexually explicit or violent content.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题数据集和AIGC模型**。由于AI模型使用的训练数据来自现实世界，它们可能无意中加剧有害的刻板印象，排除或边缘化某些群体，并包含有毒的数据源，这些数据可能煽动仇恨或暴力并冒犯个人
    [Weidinger et al., 2021]。例如，用于训练扩散模型的LAION数据集因包含与社会刻板印象、色情、种族侮辱和暴力相关的问题内容而受到批评。尽管一些AIGC模型如Imagen尝试过滤掉不良数据，例如色情图像和有毒语言，但过滤后的数据仍可能包含性暗示或暴力内容。'
- en: Models trained, learned, or fine-tuned on the aforementioned problematic datasets
    without mitigation strategies can inherit harmful stereotypes, social biases,
    and toxicity, leading to unfair discrimination and harm to certain social groups
    [Weidinger *et al.*, 2021]. For example, Stable Diffusion v1 was trained primarily
    on the LAION- 2B data set, which only contains images with English descriptions.
    As a result, the model was biased towards white, Western cultures, and prompts
    in other languages may not be adequately represented. Follow-up versions of the
    Stable Diffusion model were fine-tuned on filtered versions of the LAION dataset,
    but the bias issue still occurs. Similarly, DALLA·E and DALLA·E 2 have been found
    to exhibit negative stereotypes against minoritized groups. Google’s Imagen also
    encodes several social biases and stereotypes, such as generating images of people
    with lighter skin tones and aligning with Western gender stereotypes. Due to these
    issues, most companies decided not to make their AIGC models available to the
    public.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有缓解策略的情况下，使用上述有问题的数据集进行训练、学习或微调的模型可能继承有害的刻板印象、社会偏见和毒性，从而导致对某些社会群体的不公平歧视和伤害
    [Weidinger *et al.*, 2021]。例如，Stable Diffusion v1主要在LAION-2B数据集上进行训练，该数据集仅包含英文描述的图像。因此，该模型偏向于白人、西方文化，其他语言的提示可能没有得到充分代表。Stable
    Diffusion模型的后续版本在过滤后的LAION数据集上进行了微调，但偏见问题仍然存在。类似地，DALLA·E和DALLA·E 2被发现对少数群体表现出负面刻板印象。Google的Imagen也编码了几个社会偏见和刻板印象，例如生成肤色较浅的人物图像并符合西方性别刻板印象。由于这些问题，大多数公司决定不将其AIGC模型公开。
- en: To illustrate the inherent bias in AIGC models, we tested a toy example on Stable
    Diffusion v2.1\. As shown in Figure 3, images generated with the prompt “Three
    engineers running on the grassland” were all male and none of them belong to the
    racial minorities, indicating a lack of diversity in the generated images.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明AIGC模型中的固有偏差，我们在Stable Diffusion v2.1上测试了一个玩具示例。如图3所示，使用提示“在草原上奔跑的三名工程师”生成的图像全部是男性，而且没有任何一个属于少数族裔，显示出生成图像的多样性不足。
- en: '![](../Images/e2ae1f193ddcd9a34c94a44e7e123d44.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2ae1f193ddcd9a34c94a44e7e123d44.png)'
- en: 'Figure 3: Images generated with the text “Three engineers running on the grassland”
    by Stable Diffusion v2.1\. There are 28 people in the 9 images, all of them are
    male. Moreover, none of them belong to the racial minorities. This shows a huge
    bias of Stable Diffusion.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：使用Stable Diffusion v2.1生成的文本“在草原上奔跑的三名工程师”的图像。9张图像中共有28人，全部为男性。而且，没有任何一个人属于少数族裔。这显示了Stable
    Diffusion的巨大偏差。
- en: There is also a risk of misinformation when models provide inaccurate or false
    answers. The content generated by GPT and its derivatives may appear to be accurate
    and authoritative, but it could be completely inaccurate. Therefore, it can be
    used for misleading purposes in schools, laws, medical domains, weather forecasting,
    or anywhere else. For example, the answer on medical dosages that ChatGPT provides
    could be inaccurate or incomplete, potentially leading to the user taking dangerous
    or even life-threatening actions. Prompted misinformation on traffic laws could
    cause accidents and even death if drivers follow the false traffic rules.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型提供不准确或虚假的答案时，也存在信息错误的风险。GPT及其衍生模型生成的内容可能看起来准确且权威，但实际上可能完全不准确。因此，这可能被用于学校、法律、医疗领域、天气预报或其他任何地方的误导目的。例如，ChatGPT提供的医疗剂量答案可能不准确或不完整，可能导致用户采取危险甚至危及生命的行动。关于交通法规的误导性信息如果被驾驶员采纳，可能导致事故甚至死亡。
- en: '**Bias, toxicity, misinformation mitigation**. OpenAI took extra measures to
    ensure that any violent or sexual content was removed from the training data for
    DALLA·E 2 by carefully filtering the original training dataset. However, filtering
    can introduce biases into the training data that can then be propagated to the
    downstream models. To address this issue, OpenAI developed pre-training techniques
    to mitigate the consequent filter-induced biases.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**偏见、有毒信息、虚假信息的缓解**。OpenAI采取了额外措施，通过仔细筛选原始训练数据集，确保DALLA·E 2的训练数据中移除了任何暴力或性内容。然而，筛选可能会引入偏见，这些偏见可能会传播到下游模型。为了解决这个问题，OpenAI开发了预训练技术，以缓解由筛选引起的偏见。'
- en: To ensure that AI-driven models reflect the current state of society, it is
    essential to regularly update the training corpora used in AIGC models with the
    most recent information. This will help prevent information lag and ensure that
    the models remain updated, relevant, and beneficial to collect new training data
    and update the model regularly. One noticeable point is that while biases and
    stereotypes can be reduced in the source datasets, they can still be propagated
    or even exacerbated during the training and development of AIGC models. Therefore,
    it is crucial to evaluate the existence of bias, toxicity, and misinformation
    throughout the entire lifecycle of model training and development, rather than
    staying solely at the data source level.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为确保AI驱动的模型反映当前社会状态，必须定期用最新信息更新AIGC模型所用的训练语料。这有助于防止信息滞后，并确保模型保持更新、相关且有益，因此需要定期收集新的训练数据并更新模型。一个明显的点是，虽然可以减少源数据集中的偏见和刻板印象，但这些偏见仍可能在AIGC模型的训练和开发过程中被传播甚至加剧。因此，评估模型训练和开发全过程中的偏见、有毒信息和虚假信息的存在是至关重要的，而不仅仅停留在数据源级别。
- en: '**3.** **IP Protection**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.** **知识产权保护**'
- en: '**IP infringement**. The ownership and protection of generated content have
    raised a significant amount of concern and debate. There is a risk of copyright
    infringement with the generated content if it copies existing works, whether intentionally
    or not, raising legal questions about IP infringement. In November 2022, Matthew
    Butterick filed a class action lawsuit against Microsoft’s subsidiary GitHub,
    accusing that their product Copilot violated copyright law [Butterick, 2022].
    The lawsuit centers around Copilot’s illegal use of licensed code sections from
    the internet without attribution. Texas A&M professor Tim Davis also provided
    examples of his code being copied verbatim by Copilot. Although Microsoft and
    OpenAI have acknowledged that Copilot is trained on open-source software in public
    GitHub repositories, Microsoft claims that the output of Copilot is merely a series
    of code “suggestions” and does not claim any rights in these suggestions. Microsoft
    also does not make any guarantees regarding the correctness, security, or copyright
    of the generated code.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**知识产权侵权**。生成内容的所有权和保护引发了大量关注和辩论。如果生成的内容无论是有意还是无意地复制了现有作品，就存在版权侵权的风险，这引发了关于知识产权侵权的法律问题。2022年11月，Matthew
    Butterick 对微软的子公司 GitHub 提起了集体诉讼，指控其产品 Copilot 侵犯了版权法 [Butterick, 2022]。该诉讼的核心在于
    Copilot 非法使用了来自互联网的有版权代码段而未给予署名。德州农工大学的教授 Tim Davis 也提供了他的代码被 Copilot 逐字复制的例子。尽管微软和
    OpenAI 已确认 Copilot 是在公共 GitHub 仓库中的开源软件上进行训练的，但微软声称 Copilot 的输出仅仅是一系列代码“建议”，并不主张对这些建议的任何权利。微软还不对生成代码的正确性、安全性或版权做出任何保证。'
- en: For text-to-image models, several generative models have faced accusations of
    infringing on the creative work of artists. [Somepalli *et al.*, 2022] presented
    evidence suggesting that art-generating AI systems, such as Stable Diffusion,
    may copy from the data on which they were trained. While Stable Diffusion disclaims
    any ownership of generated images and allows users to use them freely as long
    as the image content is legal and non-harmful, this freedom raises questions about
    ownership ethics. Generative models like Stable Diffusion are trained on billions
    of images from the Internet without the approval of the IP holders, which some
    argue is a violation of their rights.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于文本生成图像模型，一些生成模型面临了侵犯艺术创作的指控。[Somepalli *et al.*, 2022] 提出了证据，表明像 Stable Diffusion
    这样的艺术生成 AI 系统可能会从其训练数据中复制内容。虽然 Stable Diffusion 声明不拥有生成图像的版权，并允许用户在图像内容合法且无害的情况下自由使用这些图像，但这种自由引发了关于所有权伦理的问题。像
    Stable Diffusion 这样的生成模型在未经知识产权持有者批准的情况下，训练于来自互联网的数十亿张图像，这被一些人认为是对其权利的侵犯。
- en: '**IP problem mitigation**. To mitigate IP concerns, many companies have started
    implementing measures to accommodate content creators. Midjourney, for instance,
    has added a DMCA takedown policy to its terms of service, allowing artists to
    request the removal of their work from the dataset if they suspect copyright infringement.
    Similarly, Stability AI plans to offer artists the option of excluding themselves
    from future versions of Stable Diffusion.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**知识产权问题缓解**。为了缓解知识产权问题，许多公司已开始采取措施以适应内容创作者。例如，Midjourney 在其服务条款中添加了 DMCA 删除政策，允许艺术家在怀疑版权侵权时请求将他们的作品从数据集中删除。类似地，Stability
    AI 计划为艺术家提供一个选项，允许他们从未来版本的 Stable Diffusion 中排除自己。'
- en: Furthermore, text watermarks, which have previously been used to protect the
    IP of language generation APIs [He *et al.*, 2022a; He *et al.*, 2022b], can also
    be used to identify if these AIGC tools have utilized samples from other sources
    without permission. This is evident in Stable Diffusion, which has generated images
    with the Getty Images’ watermark on them [Vincent, 2023]. In light of the growing
    popularity of AIGC, the need for watermarking is becoming increasingly pressing.
    OpenAI is developing a watermark to identify text generated by its GPT model.
    It could be a valuable tool for educators and professors to detect plagiarism
    in assignments generated with such tools. Google has already applied a Parti watermark
    to all images it releases.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，文本水印，这种水印以前用于保护语言生成 API 的知识产权 [He *et al.*, 2022a; He *et al.*, 2022b]，也可以用来识别这些
    AIGC 工具是否未经许可使用了来自其他来源的样本。这在 Stable Diffusion 中表现得很明显，该模型生成的图像上带有 Getty Images
    的水印 [Vincent, 2023]。随着 AIGC 的日益流行，水印的需求变得越来越迫切。OpenAI 正在开发一种水印，用于识别其 GPT 模型生成的文本。这将成为教育工作者和教授检测使用此类工具生成的作业中的剽窃行为的有价值工具。谷歌已对其发布的所有图像应用了
    Parti 水印。
- en: In addition to watermarking, OpenAI has released a classifier that can distinguish
    between text generated by AI and that written by humans. However, it should not
    be relied exclusively on for critical decisions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 除了水印技术，OpenAI还发布了一种分类器，可以区分由AI生成的文本和由人类编写的文本。然而，这不应仅仅依赖于此来做出关键决策。
- en: '**Discussion**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**讨论**'
- en: Beyond above issues in responsible AIGC, there are more components that need
    attention, including but not limited to below points.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在负责任的AIGC中提到的问题外，还有更多需要关注的方面，包括但不限于以下几点。
- en: 'Concerns on misuse: The foundation models that power AIGC have made it easier
    and cheaper to create deepfakes that are close to the original, posing additional
    risks and concerns. The misuse of these technologies could lead to the spread
    of fake news, hoaxes, harassment and misinformation, harm the reputations of individuals,
    or even break the law.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 滥用的担忧：为AIGC提供动力的基础模型使得创建接近真实的深度伪造变得更容易且更便宜，这带来了额外的风险和担忧。这些技术的滥用可能导致虚假新闻、骗局、骚扰和虚假信息的传播，损害个人声誉，甚至违法。
- en: 'Vulnerability to poisoning attack: It would be a disaster if a foundation model
    is compromised. For example, a diffusion model with a hidden “backdoor” could
    carry out malicious actions when it encounters a specific trigger pattern during
    data generation [Chou *et al.*, 2022]. This Trojan effect could cause catastrophic
    damage to downstream applications that depend on the compromised diffusion model.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对投毒攻击的脆弱性：如果基础模型被攻破，将是一场灾难。例如，一个具有隐藏“后门”的扩散模型在数据生成过程中遇到特定触发模式时可能会执行恶意行为[Chou
    *et al.*, 2022]。这种木马效应可能对依赖于受损扩散模型的下游应用造成灾难性损害。
- en: 'Debate on whether AIGC will replace humans: The use of AIGC has faced criticism
    from those who fear that it will replace human jobs. Insider has listed [several
    jobs](https://www.businessinsider.com/chatgpt-jobs-at-risk-replacement-artificial-intelligence-ai-labor-trends-2023-02)
    that could potentially be replaced by ChatGPT, including coders, data analysts,
    legal assistants, traders, accountants, etc. Some artists worry that the wide
    use of image generation tools such as Stable Diffusion could eventually make human
    artists, photographers, models, cinematographers, and actors commercially uncompetitive
    [Heikkila ̈, 2022b].'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 关于AIGC是否会取代人类的辩论：AIGC的使用面临着那些担心它会取代人类工作的批评。Insider列出了[几种可能被ChatGPT取代的工作](https://www.businessinsider.com/chatgpt-jobs-at-risk-replacement-artificial-intelligence-ai-labor-trends-2023-02)，包括编码员、数据分析师、法律助理、交易员、会计师等。一些艺术家担心，诸如Stable
    Diffusion这样的图像生成工具的广泛使用，最终可能使人类艺术家、摄影师、模特、摄影师和演员在商业上失去竞争力[Heikkila ̈, 2022b]。
- en: 'Explainable AIGC: The black-box nature of foundation models can lead to unsatisfactory
    results. For example, it is frequently challenging to determine the information
    used to generate a model’s output, which makes biases occur within datasets. An
    explanation is a critical element in comprehending how and why AIGC creates these
    problems.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释的AIGC：基础模型的“黑箱”特性可能导致不令人满意的结果。例如，常常很难确定生成模型输出所使用的信息，这会导致数据集中出现偏差。解释是理解AIGC为何以及如何产生这些问题的关键因素。
- en: 'Responsible open-sourcing: As the code and models behind AIGC are not transparent
    to the public, and their downstream applications are diverse and may have complex
    societal impacts, it is challenging to determine the potential harms they may
    cause. Therefore, the need for responsible open-sourcing becomes critical in determining
    whether the benefits of AIGC outweigh its potential risks in specific use cases.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的开源：由于AIGC背后的代码和模型对公众不透明，并且它们的下游应用多样，可能具有复杂的社会影响，因此很难确定它们可能造成的潜在危害。因此，负责任的开源变得至关重要，以确定在特定使用案例中AIGC的利益是否超过其潜在风险。
- en: 'User feedback: Gathering user feedback is also an essential element of responsible
    AIGC. Companies such as OpenAI actively seek feedback from users to identify harmful
    outputs that could arise in real-world scenarios, as well as to uncover and mitigate
    novel risks. By involving users in the feedback loop, AIGC developers can better
    understand the potential consequences of their models and take corrective actions
    to minimize any negative impacts.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 用户反馈：收集用户反馈也是负责任AIGC的一个关键因素。像OpenAI这样的公司积极寻求用户反馈，以识别在实际场景中可能出现的有害输出，并发现和缓解新的风险。通过将用户纳入反馈循环，AIGC开发者可以更好地理解模型的潜在后果，并采取纠正措施以最小化任何负面影响。
- en: 'Consent, credit, and compensation to data owners or contributors: Many AIGC
    models are trained on datasets without obtaining consent or providing credit or
    compensation to the original data contributors. To avoid negative impacts, AIGC
    companies should obtain consent from data contributors and take proactive measures
    before training their models. Failure to do so could result in lawsuits against
    AIGC.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对数据拥有者或贡献者的同意、信用和补偿：许多 AIGC 模型是在未获得同意或未向原始数据贡献者提供信用或补偿的情况下训练的。为了避免负面影响，AIGC
    公司在训练模型之前应获得数据贡献者的同意，并采取积极措施。如果未能做到这一点，可能会导致针对 AIGC 的诉讼。
- en: 'Environment impact caused by training AIGC models: The massive size of AIGC
    models, which can have billions or trillions of parameters, results in high environmental
    costs for both model training and operation. For example, GPT-3 has 175 billion
    parameters and requires significant computing resources to train. GPT-4 might
    have even more parameters than its predecessor and is expected to leave a more
    significant carbon emission. Failing to take appropriate steps to mitigate the
    substantial energy costs of AIGC could lead to irreparable damage to our planet.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 训练 AIGC 模型造成的环境影响：AIGC 模型的巨大规模，可能拥有数十亿或数万亿个参数，导致了模型训练和运行的高环境成本。例如，GPT-3 拥有 1750
    亿个参数，需要大量的计算资源来进行训练。GPT-4 可能比前身有更多的参数，并预计会产生更显著的碳排放。如果不采取适当措施来减轻 AIGC 的高能源成本，可能会对我们的星球造成无法修复的损害。
- en: '**Conclusion**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**结论**'
- en: Although AIGC is still in its infancy, it is rapidly expanding and will remain
    active for the foreseeable future. Current AIGC technologies only scratch the
    surface of what AI can create in the field of art. While AIGC offers many opportunities,
    it also carries significant risks. In this work, we provide a synopsis of both
    current and potential threats in recent AIGC models, so that both the users and
    companies can be well aware of these risks and make the appropriate actions to
    mitigate them. It is important for companies to incorporate responsible AI practices
    throughout all AIGC-related projects.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 AIGC 仍处于起步阶段，但其发展迅速，并将在可预见的未来保持活跃。目前的 AIGC 技术只是 AI 在艺术领域创作潜力的冰山一角。虽然 AIGC
    提供了许多机会，但也带来了重大风险。在这项工作中，我们提供了对当前和潜在威胁的概要，以便用户和公司能够充分认识这些风险，并采取适当措施加以减轻。公司在所有
    AIGC 相关项目中融入负责任的 AI 实践至关重要。
- en: All images unless otherwise noted are by the author.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图片均为作者所摄。
- en: '**Paper link**:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**论文链接**：'
- en: '[## [2303.01325] A Pathway Towards Responsible AI Generated Content'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[## [2303.01325] 走向负责任的 AI 生成内容之路'
- en: 'Abstract: AI Generated Content (AIGC) has received tremendous attention within
    the past few years, with content ranging…'
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要：AI 生成内容（AIGC）在过去几年受到了极大的关注，内容范围涵盖……
- en: 128.84.21.203](http://128.84.21.203/abs/2303.01325?source=post_page-----6c915e8155f9--------------------------------)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[128.84.21.203](http://128.84.21.203/abs/2303.01325?source=post_page-----6c915e8155f9--------------------------------)'
- en: '**Reference**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: '*[Rombach et al., 2022c] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
    Patrick Esser, and Bjo ̈rn Ommer. Stable diffusion v1 model card.* [*https://github.com/CompVis/stable-diffusion/blob/main/*](https://github.com/CompVis/stable-diffusion/blob/main/)
    *Stable Diffusion v1 Model Card.md, 2022.*'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*[Rombach et al., 2022c] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
    Patrick Esser 和 Bjo ̈rn Ommer. 稳定扩散 v1 模型卡。* [*https://github.com/CompVis/stable-diffusion/blob/main/*](https://github.com/CompVis/stable-diffusion/blob/main/)
    *Stable Diffusion v1 Model Card.md, 2022。*'
- en: '*[Heikkila ̈, 2023] Melissa Heikkila ̈. Ai models spit out photos of real people
    and copyrighted images.* [*https://www.technologyreview.com/2023/02/*](https://www.technologyreview.com/2023/02/)
    *03/1067786/ai-models-spit-out-photos-of-real-people- and-copyrighted-images/,
    2023.*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*[Heikkila ̈, 2023] Melissa Heikkila ̈. AI 模型吐出的真实人物照片和受版权保护的图像。* [*https://www.technologyreview.com/2023/02/*](https://www.technologyreview.com/2023/02/)
    *03/1067786/ai-models-spit-out-photos-of-real-people-and-copyrighted-images/,
    2023。*'
- en: '*[Butterick, 2022] Matthew Butterick. Github copilot investigation.* [*https://githubcopilotinvestigation.com/,*](https://githubcopilotinvestigation.com/,)
    *2022.*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*[Butterick, 2022] Matthew Butterick. Github copilot 调查。* [*https://githubcopilotinvestigation.com/,*](https://githubcopilotinvestigation.com/,)
    *2022。*'
- en: '*[Butterick, 2023] Matthew Butterick. Stable diffusion litigation.* [*https://stablediffusionlitigation.com,*](https://stablediffusionlitigation.com,)
    *2023.*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*[Butterick, 2023] Matthew Butterick. 稳定扩散诉讼。* [*https://stablediffusionlitigation.com,*](https://stablediffusionlitigation.com,)
    *2023。*'
- en: '*[Somepalli et al., 2022] Gowthami Somepalli, Vasu Singla, Micah Goldblum,
    Jonas Geiping, and Tom Goldstein. Diffusion art or digital forgery? investigating
    data replication in diffusion models. arXiv preprint arXiv:2212.03860, 2022.*'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*[索梅帕利等, 2022] 高瓦米·索梅帕利, 瓦苏·辛格拉, 米卡·戈德布鲁姆, 乔纳斯·盖平, 和 汤姆·戈德斯坦。扩散艺术还是数字伪造？调查扩散模型中的数据复制。arXiv
    预印本 arXiv:2212.03860, 2022。*'
- en: '*[He et al., 2022a] Xuanli He, Qiongkai Xu, Lingjuan Lyu, Fangzhao Wu, and
    Chenguang Wang. Protecting intellectual property of language generation apis with
    lexical watermark. AAAI, 2022.*'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*[何等, 2022a] 何玄力, 徐琼凯, 吕灵娟, 吴方钊, 和 王成光。通过词汇水印保护语言生成 API 的知识产权。AAAI, 2022。*'
- en: '*[He et al., 2022b] Xuanli He, Qiongkai Xu, Yi Zeng, Lingjuan Lyu, Fangzhao
    Wu, Jiwei Li, and Ruoxi Jia. Cater: Intellectual property protection on text generation
    apis via conditional watermarks. Advances in Neural Information Processing Systems,
    2022.*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*[何等, 2022b] 何玄力, 徐琼凯, 曾义, 吕灵娟, 吴方钊, 李季伟, 和 贾若溪。Cater：通过条件水印保护文本生成 API 的知识产权。神经信息处理系统进展,
    2022。*'
- en: '*[Vincent, 2023] James Vincent. Getty images is suing the creators of ai art
    tool stable diffusion for scraping its content.* [*https://www.theverge.com/2023/1/17/23558516/*](https://www.theverge.com/2023/1/17/23558516/)
    *ai-art-copyright-stable-diffusion-getty-images-lawsuit, 2023.*'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*[文森特, 2023] 詹姆斯·文森特。Getty 图片公司起诉 AI 艺术工具 Stable Diffusion 的创作者，因其抓取了 Getty
    的内容。* [*https://www.theverge.com/2023/1/17/23558516/*](https://www.theverge.com/2023/1/17/23558516/)
    *ai-art-copyright-stable-diffusion-getty-images-lawsuit, 2023。*'
- en: '*[Weidinger et al., 2021] Laura Weidinger, John Mellor, Maribeth Rauh, Conor
    Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa
    Kasirzadeh, et al. Ethical and social risks of harm from language models. arXiv
    preprint arXiv:2112.04359, 2021.*'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*[魏丁格等, 2021] 劳拉·魏丁格, 约翰·梅洛, 玛丽贝丝·劳, 康纳·格里芬, 乔纳森·尤萨托, 黄博森, 玛伊拉·程, 米亚·格莱斯, 博尔哈·巴列,
    阿图萨·卡西尔扎德, 等。语言模型的伦理和社会风险。arXiv 预印本 arXiv:2112.04359, 2021。*'
- en: '*[Heikkila ̈, 2022b] Melissa Heikkila ̈. This artist is dominating ai-generated
    art. and he’s not happy about it.* [*https://www.technologyreview.com/2022/09/16/*](https://www.technologyreview.com/2022/09/16/)
    *1059598/this-artist-is-dominating-ai-generated-art-and- hes-not-happy-about-it/,
    2022.*'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*[海基拉, 2022b] 梅利莎·海基拉。这个艺术家主宰了 AI 生成的艺术，并且对此感到不满。* [*https://www.technologyreview.com/2022/09/16/*](https://www.technologyreview.com/2022/09/16/)
    *1059598/this-artist-is-dominating-ai-generated-art-and-hes-not-happy-about-it/,
    2022。*'
- en: '*[Chou et al., 2022] Sheng-Yen Chou, Pin-Yu Chen, and Tsung-Yi Ho. How to backdoor
    diffusion models? arXiv preprint arXiv:2212.05400, 2022.*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*[周等, 2022] 周声彦, 陈品渝, 和 何宗义。如何在扩散模型中植入后门？arXiv 预印本 arXiv:2212.05400, 2022。*'
