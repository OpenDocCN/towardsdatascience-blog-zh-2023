- en: Unlocking the Power of Text Data with LLMs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用LLMs解锁文本数据的力量
- en: 原文：[https://towardsdatascience.com/unlocking-the-power-of-text-data-with-llms-3ddcd063274a](https://towardsdatascience.com/unlocking-the-power-of-text-data-with-llms-3ddcd063274a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/unlocking-the-power-of-text-data-with-llms-3ddcd063274a](https://towardsdatascience.com/unlocking-the-power-of-text-data-with-llms-3ddcd063274a)
- en: DATA SCIENCE LAB
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据科学实验室
- en: 'Learn how to handle text data with LLMs: a step-by-step guide for newbies'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习如何使用LLMs处理文本数据：新手逐步指南
- en: '[](https://medium.com/@sofia-rosa?source=post_page-----3ddcd063274a--------------------------------)[![Sofia
    Rosa](../Images/fe74364da94c392f0eb99f7d528dba66.png)](https://medium.com/@sofia-rosa?source=post_page-----3ddcd063274a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3ddcd063274a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3ddcd063274a--------------------------------)
    [Sofia Rosa](https://medium.com/@sofia-rosa?source=post_page-----3ddcd063274a--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@sofia-rosa?source=post_page-----3ddcd063274a--------------------------------)[![Sofia
    Rosa](../Images/fe74364da94c392f0eb99f7d528dba66.png)](https://medium.com/@sofia-rosa?source=post_page-----3ddcd063274a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3ddcd063274a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3ddcd063274a--------------------------------)
    [Sofia Rosa](https://medium.com/@sofia-rosa?source=post_page-----3ddcd063274a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3ddcd063274a--------------------------------)
    ·11 min read·Oct 23, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3ddcd063274a--------------------------------)
    ·11分钟阅读·2023年10月23日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/68220afffe74dde11a7fa11e5b938296.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/68220afffe74dde11a7fa11e5b938296.png)'
- en: Image generated by the author using Midjourney
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用Midjourney生成的图像
- en: Customer reviews, employee surveys, and social media posts can be incredibly
    powerful in **revealing people’s attitudes** toward a specific product or service.
    However, most data analysts do very little with this type of data. *Why, you ask?*
    Generating insights from text data is **no easy task** and can leave even the
    most experienced data analysts scratching their heads for days.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 客户评论、员工调查和社交媒体帖子在**揭示人们对特定产品或服务的态度**方面可能非常有力。然而，大多数数据分析师对这种类型的数据几乎不做任何处理。*你问为什么？*
    从文本数据中生成见解**并非易事**，即使是最有经验的数据分析师也可能为此苦恼数天。
- en: This is where Large Language Models (LLMs) come to the rescue. They can help
    carry out tasks such as translation, summarization, sentiment analysis, and much
    more. **But what is an LLM, exactly?** To simplify things, you can think of an
    LLM as a *parrot*. Just like a parrot repeats what it hears at home, an LLM imitates
    human language. A key difference is that LLMs have been trained on a huge volume
    of data — far beyond what a parrot would learn in its cage! This is why LLMs have
    the ability to generate coherent and contextually relevant text without the occasional
    nonsense of a parrot. 🦜
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是大型语言模型（LLMs）派上用场的时候。它们可以帮助执行翻译、总结、情感分析等任务。**但究竟什么是LLM？** 简而言之，你可以把LLM想象成一只*鹦鹉*。就像鹦鹉重复它在家听到的东西一样，LLM模仿人类语言。一个关键的区别是，LLMs已经在大量数据上进行过训练——远远超过了鹦鹉在笼子里能学到的东西！这就是LLMs能够生成连贯且与上下文相关的文本，而不是像鹦鹉那样偶尔说些无稽之谈的原因。🦜
- en: In this article, we’ll explore how LLMs work and how they make it **easier than
    ever** for data analysts to extract insights from text data. There are multiple
    LLMs now available via APIs, each with different capabilities and price points.
    We’ll be using GPT-3 via OpenAI API. At the time of writing, OpenAI charges for
    API usage based on the number of requests made and the number of tokens generated.
    The total cost for this tutorial amounted to $0.2.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将探讨LLMs如何工作，以及它们如何使数据分析师**比以往更容易**从文本数据中提取见解。目前有多种LLMs通过API提供，每种LLM具有不同的功能和价格。我们将使用OpenAI
    API中的GPT-3。在撰写时，OpenAI根据请求次数和生成的令牌数量收费。此次教程的总费用为$0.2。
- en: Time to dig in!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在开始深入探索吧！
- en: Table of Contents
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目录
- en: '▹ [Step 1: Downloading the Data](#9da9)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ▹ [第1步：下载数据](#9da9)
- en: '▹ [Step 2: Reading the Data](#4faf)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ▹ [第2步：读取数据](#4faf)
- en: '▹ [Step 3: Data Pre-Processing](#6da6)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ▹ [第3步：数据预处理](#6da6)
- en: '▹ [Step 3a: Dealing with NaN Values](#185d)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ▹ [第3a步：处理NaN值](#185d)
- en: '▹ [Step 3b: Transforming Text for GPT-3](#3fc1)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ▹ [第3b步：为GPT-3转换文本](#3fc1)
- en: '▹ [Step 3c: Counting Tokens](#dcd3)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ▹ [第3c步：计算令牌](#dcd3)
- en: '▹ [Step 4: Setting Up an OpenAI Account](#078a)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ▹ [第4步：设置OpenAI账户](#078a)
- en: '▹ [Step 5: Working with GPT-3](#d5e3)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ▹ [第 5 步：使用 GPT-3](#d5e3)
- en: '▹ [Step 6: Summarizing the Results](#7746)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ▹ [第 6 步：总结结果](#7746)
- en: Prerequisites
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 先决条件
- en: 'To follow along in this tutorial, you will need to have the following:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟随本教程，你需要具备以下内容：
- en: Working knowledge of Python
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 的基础知识
- en: Python 3 environment
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 3 环境
- en: OpenAI API key (*see step 4*)
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI API 密钥（*见第 4 步*）
- en: 'Step 1: Downloading the Data'
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一步：下载数据
- en: The dataset we’ll use is an industry-wide survey conducted by [Kaggle](https://www.kaggle.com/datasets/kaggle/kaggle-survey-2017)
    in 2017 aimed at uncovering new trends in machine learning and data science. For
    this tutorial, we’ll only be using the **freeformResponses** csv file, which contains
    open-ended answers to Kaggle’s questions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的数据集是 [Kaggle](https://www.kaggle.com/datasets/kaggle/kaggle-survey-2017)
    在 2017 年进行的一项行业调查，旨在揭示机器学习和数据科学的新趋势。在本教程中，我们将仅使用 **freeformResponses** csv 文件，该文件包含对
    Kaggle 问题的开放式回答。
- en: '![](../Images/41572fcfc0635ecc0dcf149d13da712e.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41572fcfc0635ecc0dcf149d13da712e.png)'
- en: Snippet of the freeformResponses csv file
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: freeformResponses csv 文件的片段
- en: 'Step 2: Reading the Data'
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二步：读取数据
- en: Next, we’ll read the csv file into a dataframe and focus on the column “**PersonalProjectsChallengeFreeForm**”.
    This column contains challenges people face when using public datasets for their
    personal projects. Kaggle, as a platform for data science and machine learning,
    can use these insights to improve its services (e.g., by developing relevant content,
    tutorials, and resources that specifically address these challenges).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把 csv 文件读取到数据框中，并重点关注“**PersonalProjectsChallengeFreeForm**”列。该列包含人们在使用公共数据集进行个人项目时面临的挑战。作为数据科学和机器学习平台，Kaggle
    可以利用这些见解来改进其服务（例如，通过开发相关内容、教程和专门解决这些挑战的资源）。
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/67c1d6d1a364e3c77d43d5429f108e74.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67c1d6d1a364e3c77d43d5429f108e74.png)'
- en: Output
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 输出
- en: 'Step 3: Data Pre-Processing'
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三步：数据预处理
- en: Data pre-processing involves a series of steps to clean and prepare the data
    for analysis. GPT-3 can handle relatively clean and structured text data without
    the need for extensive pre-processing. However, for complex or non-standard data,
    some extra pre-processing may be necessary to ensure the best results when leveraging
    GPT-3\. This is something to keep in mind if your text contains multiple languages,
    spelling errors, or domain-specific terms.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理涉及一系列步骤，以清理和准备数据以进行分析。GPT-3 可以处理相对干净和结构化的文本数据，而无需 extensive 预处理。然而，对于复杂或非标准的数据，可能需要一些额外的预处理，以确保在利用
    GPT-3 时获得最佳结果。如果你的文本包含多种语言、拼写错误或领域特定术语，需要特别注意这一点。
- en: '**Step 3a: Dealing with NaN Values**'
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**第 3a 步：处理 NaN 值**'
- en: We’ll start by dealing with NaN (Not A Number) values. NaN values represent
    missing or undefined values with very distinct properties, making it important
    to detect them early on using the `isna()` function. Once identified, we can take
    appropriate measures to handle them effectively.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先处理 NaN（非数字）值。NaN 值表示缺失或未定义的值，具有非常独特的属性，因此在早期使用 `isna()` 函数检测它们是很重要的。一旦识别出这些值，我们可以采取适当的措施有效地处理它们。
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/875549df74f51394a4397baefe276963.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/875549df74f51394a4397baefe276963.png)'
- en: Output
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 输出
- en: There are 13,214 NaN values (80% of all responses!), meaning that these people
    did not provide an answer to the question. The simplest approach is to remove
    all the entries that contain NaN values using the `dropna()` function. However,
    depending on your specific use case, you might prefer to handle NaN values differently,
    such as by replacing them with specific values.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 有 13,214 个 NaN 值（占所有响应的 80%！），这意味着这些人没有回答问题。最简单的方法是使用 `dropna()` 函数删除所有包含 NaN
    值的条目。然而，根据你的具体使用情况，你可能更愿意以其他方式处理 NaN 值，例如通过用特定值替换它们。
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/0a874a59394c34dcfad442f6f3ce3cc6.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a874a59394c34dcfad442f6f3ce3cc6.png)'
- en: Output
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 输出
- en: For demo purposes, we’ll work with only the first 500 (non-null) responses from
    the survey.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为演示目的，我们将仅使用调查中的前 500 条（非空）响应。
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Step 3b: Transforming Text for GPT-3**'
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**第 3b 步：为 GPT-3 转换文本**'
- en: Next, we’ll transform the text data into a format suitable for GPT-3\. We’ll
    extract all the values from the “**PersonalProjectsChallengeFreeForm**” column
    and store them in the “**challenges**” list. This transformation begins with the
    use of the `squeeze()` function, which converts the dataframe into a pandas series.
    Subsequently, the `tolist()` function converts this series into a list.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把文本数据转换为适合GPT-3的格式。我们将从“**PersonalProjectsChallengeFreeForm**”列中提取所有值，并将它们存储在“**challenges**”列表中。这一转换从使用`
    squeeze()`函数开始，该函数将数据框转换为pandas系列。随后，`tolist()`函数将这个系列转换为列表。
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/a171aaf6a16d20293a9be13953e7a3d2.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a171aaf6a16d20293a9be13953e7a3d2.png)'
- en: Output
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 输出
- en: In this example, “**challenges**” is a list where each element represents a
    response from the original survey. We’ll provide this text as input to GPT-3.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，“**challenges**”是一个列表，其中每个元素代表原始调查的一个回应。我们将把这个文本作为输入提供给GPT-3。
- en: 'Step 3c: Counting Tokens'
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3c步：计算tokens
- en: 'Our text is almost ready for GPT-3\. Before we proceed, it’s important that
    we understand how GPT-3 works with text. Initially, it performs **tokenization**,
    which involves splitting the text into smaller units known as *tokens*. Tokens
    are units of text, such as sentences, words, numbers, or even punctuation marks.
    For example, the phrase “**hello friend!**” can be split into three tokens: “**hello**”,
    “ **friend**” and “**!**”.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的文本几乎准备好用于GPT-3了。在我们继续之前，了解GPT-3如何处理文本是很重要的。最初，它执行**分词**，这涉及将文本拆分成称为*tokens*的更小单元。Tokens是文本单元，例如句子、单词、数字，甚至标点符号。例如，短语“**hello
    friend!**”可以拆分为三个tokens：“**hello**”、“**friend**”和“**!**”。
- en: '![](../Images/5d87ea48d166eb2b3e14498d29a7bc3c.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d87ea48d166eb2b3e14498d29a7bc3c.png)'
- en: Example of tokenization
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 分词示例
- en: 'After tokenization, GPT-3 proceeds to **encoding**, which means it converts
    these tokens into token numbers. In our example, the three tokens “hello”, “ friend”
    and “!” can be converted into three token numbers: “**15339**”, “ **4333**” and
    “**0**”.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在分词之后，GPT-3继续进行**编码**，这意味着它将这些tokens转换为token数字。在我们的示例中，三个tokens“hello”、“ friend”和“!”可以转换为三个token数字：“**15339**”、“**4333**”和“**0**”。
- en: '![](../Images/41e738312bfeae289cab399a8f7ded85.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41e738312bfeae289cab399a8f7ded85.png)'
- en: Example of encoding
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 编码示例
- en: By determining the number of tokens in our text, we’ll know whether the text
    is too long for the model to process as well as how much an OpenAI API call will
    cost (as API calls are billed based on the number of tokens sent in your input
    plus the number of tokens that GPT returns in the output).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 通过确定文本中的tokens数量，我们将知道文本是否过长而无法被模型处理，以及OpenAI API调用的费用（API调用的费用是根据输入中发送的tokens数量以及GPT返回的tokens数量来计费的）。
- en: To do this, we’ll install a library called `tiktoken` and import the necessary
    module `encoding_for_model`. Since different LLMs use different methods for encoding
    text, we’ll need to specify the model we’ll be using, which is “**gpt-3.5-turbo-16k**”.
    For each sentence, we’ll then tokenize and encode the text.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将安装一个名为`tiktoken`的库，并导入必要的模块`encoding_for_model`。由于不同的LLM使用不同的方法对文本进行编码，我们需要指定我们将使用的模型，即“**gpt-3.5-turbo-16k**”。然后，我们将对每个句子进行分词和编码。
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/2b234c7df476c123b2c699fe72675fa6.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b234c7df476c123b2c699fe72675fa6.png)'
- en: Output
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 输出
- en: The last step is to count the tokens, which can be accomplished by determining
    the length of the list “**num_tokens**”.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是计算tokens，这可以通过确定列表“**num_tokens**”的长度来完成。
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/33a8968b910758874a33ed159bbed6ea.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/33a8968b910758874a33ed159bbed6ea.png)'
- en: Output
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 输出
- en: To estimate the total cost based on our input, we can refer to [the pricing
    documentation](https://openai.com/pricing). In our case, 4629 tokens would translate
    to a cost of $0.01.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 要根据我们的输入估算总费用，我们可以参考[定价文档](https://openai.com/pricing)。在我们的情况下，4629个tokens将转换为0.01美元的费用。
- en: 'Step 4: Setting Up an OpenAI Account'
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4步：设置OpenAI账户
- en: Our text is finally ready for GPT-3 (we’re getting closer to the good stuff!).
    To work with GPT-3, we’ll be using the OpenAI API. Make sure that you have an
    OpenAI account set up to access the OpenAI API. If you don’t already have an account,
    follow the steps below to create one.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的文本终于准备好用于GPT-3了（我们离好东西越来越近了！）。要使用GPT-3，我们将使用OpenAI API。确保你已经设置了OpenAI账户以访问OpenAI
    API。如果你还没有账户，请按照下面的步骤创建一个。
- en: To kick things off, head to the [OpenAI](http://platform.openai.com) website
    and click on the “**Sign Up**” button in the top right corner of the page. Fill
    in the form with your email address, create a password, and provide any other
    necessary info. Then, hit the “**Create Account**” button. Keep an eye on your
    inbox as you’ll receive a confirmation email. Click the link in the email to verify
    your account. Once that’s done, you’re all set to log in.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，访问 [OpenAI](http://platform.openai.com) 网站，并点击页面右上角的“**注册**”按钮。填写表单，输入你的电子邮件地址，创建一个密码，并提供其他必要的信息。然后，点击“**创建账户**”按钮。请留意你的收件箱，你会收到一封确认邮件。点击邮件中的链接来验证你的账户。完成这些步骤后，你就可以登录了。
- en: With your account created, the next step is funding it. Remember, as you use
    the API, you’ll be billed for your usage. Simply go to “**Manage Account**” and
    find the “**Billing**” tab. There, you can add your payment card details and specify
    the initial amount you want to put in your account.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 账户创建后，下一步是为其充值。请记住，在使用 API 时，你会为使用量付费。只需前往“**管理账户**”并找到“**账单**”标签。在那里，你可以添加你的支付卡详细信息，并指定你希望在账户中存入的初始金额。
- en: The final important step is to generate your API Key, which serves as a private
    access key to the API. You can create it in the “**API Keys**” tab. Keep this
    key safe because it can’t be recovered if lost. However, if it slips through the
    cracks, you do have the option to create a new one.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的重要步骤是生成你的 API 密钥，它作为对 API 的私密访问密钥。你可以在“**API 密钥**”标签中创建它。请妥善保管此密钥，因为如果丢失无法恢复。不过，如果不幸丢失，你可以选择创建一个新的密钥。
- en: 'Step 5: Working with GPT-3'
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 5 步：使用 GPT-3
- en: Now that we have access to GPT-3 through the OpenAI API, we can send a request
    containing the input and API key. In return, we’ll get a response containing the
    GPT-3 output.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经通过 OpenAI API 访问了 GPT-3，我们可以发送包含输入和 API 密钥的请求。作为回报，我们将获得包含 GPT-3 输出的响应。
- en: '![](../Images/82f40db52b0c5e1a5f1156a2861255ee.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82f40db52b0c5e1a5f1156a2861255ee.png)'
- en: Using GPT-3 via the OpenAI API
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 GPT-3 通过 OpenAI API
- en: First, we’ll install a library called `openai`. Then, we’ll set up the API key
    to authenticate our requests.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将安装一个名为 `openai` 的库。然后，我们将设置 API 密钥以验证我们的请求。
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We’ll send our text to GPT-3 and ask it to summarise the main topics, which
    are then stored in the “**response**” variable.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将文本发送给 GPT-3 并要求其总结主要话题，这些话题随后存储在“**response**”变量中。
- en: '💡 **Note**: *This code is a simplified example, and you can adapt it for various
    tasks by adjusting the user message and system message according to your specific
    needs.*'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 💡 **注意**：*这段代码是一个简化的示例，你可以根据具体需求调整用户消息和系统消息，以适应各种任务。*
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s go through the code step by step:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步步分析代码：
- en: '`response = ai.ChatCompletion.create(`: This line initiates a request to GPT-3
    and assigns the response to the variable “**response**”.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`response = ai.ChatCompletion.create(`：这一行发起一个请求到 GPT-3，并将响应赋值给变量“**response**”。'
- en: '`model = ''gpt-3.5-turbo-16k''`: This parameter specifies which GPT-3 model
    to use.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model = ''gpt-3.5-turbo-16k''`：这个参数指定使用哪个 GPT-3 模型。'
- en: '`messages = [ ... ]`: This section defines a list of messages for which GPT-3
    will create a response. Each message has a role (e.g., system or user) and content.
    The system message helps set the *behavior* of GPT-3\. For example, we can say:
    “You’re a helpful assistant. Your task is to analyze a set of reviews”. The user
    message, on the other hand, provides *instructions* for the task. For example,
    we can say: “Below is a set of reviews. Please, identify the main topics mentioned
    in these comments”.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`messages = [ ... ]`：这一部分定义了一系列消息，GPT-3 将为这些消息生成响应。每条消息都有一个角色（例如系统或用户）和内容。系统消息有助于设置
    GPT-3 的*行为*。例如，我们可以说：“你是一个有用的助手。你的任务是分析一组评论”。而用户消息则提供*指示*以完成任务。例如，我们可以说：“下面是一组评论。请识别这些评论中提到的主要话题”。'
- en: '`temperature = 0`: This parameter influences the randomness of the responses.
    You can think of it as a way to control how creative and unpredictable the responses
    are. Setting it to 0 means that you’ll get the same output every time you ask,
    almost like a broken record. On the other hand, setting it to a higher value (e.g.,
    0.8) means that you’ll get a fresh output.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`temperature = 0`：这个参数影响响应的随机性。你可以将其视为控制响应的创意性和不可预测性的方式。将其设置为 0 意味着你每次提问都会得到相同的输出，几乎像是坏掉的唱片。另一方面，将其设置为较高的值（例如
    0.8）则意味着你将获得新的输出。'
- en: '`max_tokens = 6000`: This parameter specifies the maximum number of tokens
    the response can contain. Setting it to 6000 ensures that the response doesn''t
    exceed this length. If the response exceeds this limit, it will be truncated.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_tokens = 6000`：此参数指定了响应可以包含的最大令牌数。将其设置为6000可确保响应不会超过此长度。如果响应超出此限制，它将被截断。'
- en: After receiving a response from GPT-3, we’ll return the content (excluding any
    additional meta-information).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在收到GPT-3的响应后，我们将返回内容（不包括任何额外的元信息）。
- en: '[PRE9]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'GPT-3 returned five topics:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3返回了五个话题：
- en: '“**1\. Data cleaning and preparation**: Many reviews mention the challenge
    of cleaning and preparing the data for analysis. This includes dealing with missing
    values, formatting issues, unstructured data, and the need for data wrangling.'
  id: totrans-96
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1\. 数据清理和准备**：许多评论提到清理和准备数据进行分析的挑战。这包括处理缺失值、格式问题、非结构化数据和数据整理的需求。'
- en: ''
  id: totrans-97
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**2\. Data quality and documentation**: Several reviews highlight the poor
    quality of the data, including lack of documentation, incorrect documentation,
    and unreliable data. Issues with data completeness, accuracy, and reliability
    are also mentioned.'
  id: totrans-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2\. 数据质量和文档**：一些评论强调了数据质量差的问题，包括缺乏文档、不正确的文档和不可靠的数据。还提到了数据完整性、准确性和可靠性的问题。'
- en: ''
  id: totrans-99
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**3\. Finding and accessing relevant datasets**: Many reviewers express difficulties
    in finding the right datasets for their projects. This includes challenges in
    finding datasets that match specific requirements, lack of availability, limited
    size or relevance of public datasets, and the need to collect personal data.'
  id: totrans-100
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**3\. 寻找和访问相关数据集**：许多评论者表达了在为他们的项目寻找合适数据集方面的困难。这包括找到符合特定要求的数据集的挑战、可用性不足、公共数据集的规模或相关性有限以及收集个人数据的需求。'
- en: ''
  id: totrans-101
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**4\. Connectivity and data fusion**: Some reviews mention challenges related
    to data connectivity and fusion, such as integrating data from different sources,
    dealing with inconsistent formats, and merging datasets.'
  id: totrans-102
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**4\. 连接性和数据融合**：一些评论提到与数据连接性和融合相关的挑战，例如整合来自不同来源的数据、处理不一致的格式和合并数据集。'
- en: ''
  id: totrans-103
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**5\. Computing power and scalability**: A few reviews mention challenges related
    to computing power and scalability, particularly when working with large datasets
    or when processing data on a single machine.’,'
  id: totrans-104
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**5\. 计算能力和可扩展性**：一些评论提到与计算能力和可扩展性相关的挑战，特别是在处理大型数据集或在单台机器上处理数据时。'
- en: ‘These topics reflect common challenges faced by individuals when working with
    data, including issues related to data quality, data preparation, dataset availability,
    and technical limitations.”
  id: totrans-105
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这些话题反映了个人在处理数据时面临的常见挑战，包括数据质量、数据准备、数据集可用性和技术限制的问题。
- en: '💡 **Note**: *While GPT-3 is powerful as it is, you can often achieve better
    results by fine-tuning the model with your training data.*'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 💡 **注意**：*虽然GPT-3本身很强大，但通过用你的训练数据对模型进行微调，通常可以获得更好的结果。*
- en: 'Step 6: Summarizing the Results'
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6步：总结结果
- en: These topics reflect common challenges faced by individuals when working with
    data, including issues related to data preparation, data quality, reliability,
    and scalability. A company like Kaggle can leverage these insights to develop
    educational material that specifically addresses these challenges, thereby providing
    valuable support for their community.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这些话题反映了个人在处理数据时面临的常见挑战，包括数据准备、数据质量、可靠性和可扩展性相关的问题。像Kaggle这样的公司可以利用这些见解来开发专门解决这些挑战的教育材料，从而为他们的社区提供宝贵的支持。
- en: Conclusion
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we’ve explored the significant potential of LLMs in extracting
    insights from text data. We’ve discussed how LLMs work and how they can be a game-changer
    for data analysts dealing with text data. You now have the knowledge to apply
    these concepts to your own text analysis tasks.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们探讨了大型语言模型（LLMs）在从文本数据中提取洞察力方面的巨大潜力。我们讨论了LLMs是如何工作的，以及它们如何成为处理文本数据的数据分析师的游戏规则改变者。现在你有了将这些概念应用于你自己文本分析任务的知识。
- en: I hope you found this article helpful. If you have any questions or thoughts,
    I’ll be happy to read them in the comments!
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你发现这篇文章对你有帮助。如果你有任何问题或想法，我很乐意在评论中阅读它们！
