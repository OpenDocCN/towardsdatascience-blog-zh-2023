- en: The Multi-Task Optimization Controversy
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多任务优化争议
- en: 原文：[https://towardsdatascience.com/the-multi-task-optimization-controversy-793cbb431d98?source=collection_archive---------12-----------------------#2023-09-29](https://towardsdatascience.com/the-multi-task-optimization-controversy-793cbb431d98?source=collection_archive---------12-----------------------#2023-09-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-multi-task-optimization-controversy-793cbb431d98?source=collection_archive---------12-----------------------#2023-09-29](https://towardsdatascience.com/the-multi-task-optimization-controversy-793cbb431d98?source=collection_archive---------12-----------------------#2023-09-29)
- en: Do we need special algorithms to train models on multiple tasks at the same
    time?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们是否需要特别的算法来同时训练多个任务的模型？
- en: '[](https://medium.com/@samuel.flender?source=post_page-----793cbb431d98--------------------------------)[![Samuel
    Flender](../Images/390d82a673de8a8bb11cef66978269b5.png)](https://medium.com/@samuel.flender?source=post_page-----793cbb431d98--------------------------------)[](https://towardsdatascience.com/?source=post_page-----793cbb431d98--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----793cbb431d98--------------------------------)
    [Samuel Flender](https://medium.com/@samuel.flender?source=post_page-----793cbb431d98--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@samuel.flender?source=post_page-----793cbb431d98--------------------------------)[![Samuel
    Flender](../Images/390d82a673de8a8bb11cef66978269b5.png)](https://medium.com/@samuel.flender?source=post_page-----793cbb431d98--------------------------------)[](https://towardsdatascience.com/?source=post_page-----793cbb431d98--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----793cbb431d98--------------------------------)
    [Samuel Flender](https://medium.com/@samuel.flender?source=post_page-----793cbb431d98--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fce56d9dcd568&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-multi-task-optimization-controversy-793cbb431d98&user=Samuel+Flender&userId=ce56d9dcd568&source=post_page-ce56d9dcd568----793cbb431d98---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----793cbb431d98--------------------------------)
    ·6 min read·Sep 29, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F793cbb431d98&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-multi-task-optimization-controversy-793cbb431d98&user=Samuel+Flender&userId=ce56d9dcd568&source=-----793cbb431d98---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fce56d9dcd568&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-multi-task-optimization-controversy-793cbb431d98&user=Samuel+Flender&userId=ce56d9dcd568&source=post_page-ce56d9dcd568----793cbb431d98---------------------post_header-----------)
    发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----793cbb431d98--------------------------------)
    · 6分钟阅读 · 2023年9月29日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F793cbb431d98&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-multi-task-optimization-controversy-793cbb431d98&user=Samuel+Flender&userId=ce56d9dcd568&source=-----793cbb431d98---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F793cbb431d98&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-multi-task-optimization-controversy-793cbb431d98&source=-----793cbb431d98---------------------bookmark_footer-----------)![](../Images/d49f4ff793f5ebba7bb708389b741416.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F793cbb431d98&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-multi-task-optimization-controversy-793cbb431d98&source=-----793cbb431d98---------------------bookmark_footer-----------)![](../Images/d49f4ff793f5ebba7bb708389b741416.png)'
- en: Photo by [Javier Allegue Barros](https://unsplash.com/@soymeraki?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Javier Allegue Barros](https://unsplash.com/@soymeraki?utm_source=medium&utm_medium=referral)拍摄，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: The [multi-task learning paradigm](/multi-task-learning-in-recommender-systems-a-primer-508e661a2029#:~:text=Multi%2Dtask%20learning%20matters%20because,each%20other%20%E2%80%94%20creating%20negative%20transfer.)
    — that is, the ability to train models on multiple tasks at the same time — has
    been a blessing as much as a curse.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[多任务学习范式](/multi-task-learning-in-recommender-systems-a-primer-508e661a2029#:~:text=Multi%2Dtask%20learning%20matters%20because,each%20other%20%E2%80%94%20creating%20negative%20transfer.)——即同时训练多个任务的能力——既是福音也是诅咒。'
- en: 'A blessing because it allows us to build a single model where previously we
    would have needed multiple. That makes live simpler: fewer models that need to
    be maintained, re-trained, tuned, and monitored.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个福音，因为它允许我们构建一个单一模型，而之前我们需要多个模型。这使得生活变得更简单：需要维护、重新训练、调整和监控的模型更少。
- en: 'A curse because it opens up an entirely new pandora’s box of questions: which
    tasks should be learned together? Which tasks do we really need? What happens
    if tasks are competing with each other? How can we make make the model prioritize
    certain tasks over others? How can we avoid ‘task rot’, that is, the accumulation
    of task heads over time that eventually lead to degradation of model performance?'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个诅咒，因为它打开了一整套全新的问题：哪些任务应该一起学习？我们真正需要哪些任务？如果任务之间相互竞争会发生什么？我们如何让模型优先处理某些任务而非其他任务？我们如何避免“任务衰退”，即任务头的积累最终导致模型性能下降？
- en: It is questions like these that spawned a new subdomain of Machine Learning
    known as *multi-task optimization*, that is, the science of how to optimize a
    model on multiple, sometimes competing, tasks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 正是这些问题催生了一个新的机器学习子领域，称为*多任务优化*，即如何在多个有时相互竞争的任务上优化模型的科学。
- en: Scalarization
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标量化
- en: Scalarization is Mathematic’s answer to the multi-task optimization problem.
    In a multi-task model we are trying to learn K tasks, such as predicting…
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 标量化是数学对多任务优化问题的回答。在一个多任务模型中，我们试图学习K个任务，例如预测…
