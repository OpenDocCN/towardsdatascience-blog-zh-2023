- en: Siamese Neural Networks with Triplet Loss and Cosine Distance
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Siamese神经网络与三重损失和余弦距离
- en: 原文：[https://towardsdatascience.com/siamese-neural-networks-with-tensorflow-functional-api-6aef1002c4e](https://towardsdatascience.com/siamese-neural-networks-with-tensorflow-functional-api-6aef1002c4e)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/siamese-neural-networks-with-tensorflow-functional-api-6aef1002c4e](https://towardsdatascience.com/siamese-neural-networks-with-tensorflow-functional-api-6aef1002c4e)
- en: 'Theory & Code-along: Triplet loss with cosine distance for Siamese Networks
    on CIFAR-10 dataset'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理论与代码实践：使用三重损失和余弦距离进行Siamese网络在CIFAR-10数据集上的训练
- en: '[](https://tanpengshi.medium.com/?source=post_page-----6aef1002c4e--------------------------------)[![Tan
    Pengshi Alvin](../Images/449d736eed966b01715f87c6269c88a5.png)](https://tanpengshi.medium.com/?source=post_page-----6aef1002c4e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6aef1002c4e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6aef1002c4e--------------------------------)
    [Tan Pengshi Alvin](https://tanpengshi.medium.com/?source=post_page-----6aef1002c4e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://tanpengshi.medium.com/?source=post_page-----6aef1002c4e--------------------------------)[![Tan
    Pengshi Alvin](../Images/449d736eed966b01715f87c6269c88a5.png)](https://tanpengshi.medium.com/?source=post_page-----6aef1002c4e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6aef1002c4e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6aef1002c4e--------------------------------)
    [Tan Pengshi Alvin](https://tanpengshi.medium.com/?source=post_page-----6aef1002c4e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6aef1002c4e--------------------------------)
    ·11 min read·May 12, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6aef1002c4e--------------------------------)
    ·阅读时长11分钟·2023年5月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f7edeabf5d107ac8fe08ae568bf787fb.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f7edeabf5d107ac8fe08ae568bf787fb.png)'
- en: Image by [Alex Meier](https://unsplash.com/@alexmeier19) on [Unsplash](https://unsplash.com/)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Alex Meier](https://unsplash.com/@alexmeier19) 提供，来源于 [Unsplash](https://unsplash.com/)
- en: What if we could encode every object image (human faces, etc) into a template
    — a vector of numbers? Thereafter, we could objectively determine the similarity
    between objects by numerically comparing — finding the distances — between their
    templates. In deep learning, this is exactly what the Siamese Neural Networks
    hope to achieve.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们可以将每个对象图像（如人脸等）编码成一个模板——一个数字向量呢？之后，我们可以通过对比它们的模板——计算距离——来客观地确定对象之间的相似性。在深度学习中，这正是Siamese神经网络希望实现的目标。
- en: The Siamese Neural Networks are basically models that, upon training, generate
    distinct feature vectors (templates) for each input object. Although these models
    are typically adopted in templating object images (Computer Vision) generally
    speaking, they may also be employed for text and sound data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Siamese神经网络基本上是经过训练后为每个输入对象生成独特特征向量（模板）的模型。尽管这些模型通常用于对象图像的模板（计算机视觉），但它们也可以用于文本和声音数据。
- en: Apart from security authentication, such as facial recognition and signature
    comparisons, Siamese Neural Networks are also commonly used in E-Commerce platforms
    to measure product similarity. For instance, some E-Commerce platforms allow you
    can search for similar products by uploading an image of an object you are looking
    for. On Kaggle, there is even a [Product Matching competition](https://www.kaggle.com/competitions/shopee-product-matching)
    by Shopee, a leading E-Commerce company in South-East Asia.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 除了安全认证，如人脸识别和签名比对，Siamese神经网络还常用于电子商务平台中测量产品相似性。例如，一些电子商务平台允许你通过上传你想寻找的对象的图像来搜索类似产品。在Kaggle上，甚至有一个由东南亚领先电子商务公司Shopee举办的
    [产品匹配竞赛](https://www.kaggle.com/competitions/shopee-product-matching)。
- en: In this article, we will explore a common data set in Tensorflow — the CIFAR-10
    — that bears some semblance to the problem of product similarity search, except
    that the objects of interest are automobiles — cars, planes, trucks, ships, etc
    — and animals (or pets if you like!) — cats, dogs, horses, birds, deers, etc.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将探索一个在Tensorflow中常见的数据集——CIFAR-10——该数据集与产品相似性搜索问题有些相似，只不过兴趣对象是汽车——如汽车、飞机、卡车、船只等——以及动物（或者说宠物也行！）——如猫、狗、马、鸟、鹿等。
- en: Before we begin, we have to first understand the theory behind Siamese Neural
    Network. Thereafter we will explore the codes for training and evaluating a simple
    Siamese Neural Network on the CIFAR-10 data set.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，我们首先需要理解Siamese神经网络背后的理论。之后，我们将探索在CIFAR-10数据集上训练和评估简单Siamese神经网络的代码。
- en: Ready? Let’s start!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好了吗？开始吧！
- en: 1\. Theory of Siamese Networks
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1. 孪生网络理论
- en: I have to admit the cover image in this article is a little misleading — the
    word ‘Siamese’ in fact does not come from the ‘Siamese Cat’. Rather it comes from
    the ‘Siamese Twins’, or conjoined twins — twins who are conjoined in one part
    of the body.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我不得不承认本文中的封面图片有点误导——‘Siamese’一词实际上并不是源于‘暹罗猫’。而是来源于‘暹罗双胞胎’，即身体某部分连在一起的双胞胎。
- en: Hence, the Siamese Neural Networks basically mean twin neural networks, that
    are typically conjoined at the end — the Lambda layer, as we will see — before
    feeding the model output to the loss function. During the training of these twin
    networks, their weights are exactly the same between them during the initialization,
    forward propagation and backpropagation process.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，孪生神经网络基本上指的是双胞神经网络，这些网络通常在最后——Lambda层，如我们将看到的——连接在一起，然后将模型输出输入损失函数。在训练这些双胞神经网络的过程中，它们的权重在初始化、前向传播和反向传播过程中完全相同。
- en: 'Because we are generally working with images, each body of twin neural networks
    is typically a Convolutional Neural Network (CNN). If you are unfamiliar with
    CNN or need to refresh your idea, I have an excellent article about it here:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们通常处理的是图像，每对孪生神经网络通常是卷积神经网络（CNN）。如果你对CNN不熟悉或需要刷新记忆，我这里有一篇关于CNN的优秀文章：
- en: '[](https://medium.com/mlearning-ai/transfer-learning-and-convolutional-neural-networks-cnn-e68db4c48cca?source=post_page-----6aef1002c4e--------------------------------)
    [## Transfer Learning and Convolutional Neural Networks (CNN)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/mlearning-ai/transfer-learning-and-convolutional-neural-networks-cnn-e68db4c48cca?source=post_page-----6aef1002c4e--------------------------------)
    [## 迁移学习与卷积神经网络（CNN）'
- en: A complete guide from CNN to Transfer Learning for Kaggle’s Cat versus Dog dataset
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从CNN到迁移学习的完整指南，适用于Kaggle的猫狗数据集
- en: medium.com](https://medium.com/mlearning-ai/transfer-learning-and-convolutional-neural-networks-cnn-e68db4c48cca?source=post_page-----6aef1002c4e--------------------------------)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/mlearning-ai/transfer-learning-and-convolutional-neural-networks-cnn-e68db4c48cca?source=post_page-----6aef1002c4e--------------------------------)
- en: 'With this idea in mind, we will introduce 2 common types of Siamese Neural
    Networks:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 牢记这一点，我们将介绍两种常见的孪生神经网络：
- en: 1.1 Contrastive Loss Siamese Networks
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 对比损失孪生网络
- en: The first type is the Siamese Neural Networks based on calculating the Euclidean/Cosine
    distance between the embedding layers — the feature vectors — of twin CNNs, before
    comparing with the ground truths (1:Match, 0:Non-Match) to determine the Contrastive
    Loss.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种类型是基于计算双胞CNN的嵌入层（特征向量）之间的欧几里得/余弦距离，然后与真实值（1:匹配，0:不匹配）比较来确定对比损失的孪生神经网络。
- en: 'Below is an illustration of such a model:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是这种模型的示意图：
- en: '![](../Images/6e001a974af5e4778941f59fd76f5f89.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6e001a974af5e4778941f59fd76f5f89.png)'
- en: Example of Contrastive Loss Siamese Neural Networks. Image adapted from the
    [SigNet paper](https://arxiv.org/abs/1707.02131).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对比损失的孪生神经网络示例。图片改编自[SigNet论文](https://arxiv.org/abs/1707.02131)。
- en: '![](../Images/015d8860b253b9a015f19bee792f0457.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/015d8860b253b9a015f19bee792f0457.png)'
- en: Contrastive Loss formula with Euclidean Distance, where Y is the ground truth.
    Image by Author.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对比损失公式与欧几里得距离，其中Y为真实值。图片作者提供。
- en: 1.2 Triplet Loss Siamese Networks
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 三重损失孪生网络
- en: The second type of Siamese Neural Networks is based on calculating the 2 Euclidean/Cosine
    distances among the embedding layers (feature vectors) — between the Anchor and
    Positive Image, and between the Anchor and Negative Image — of triplet CNNs, and
    then computing the Triplet Loss completely in the Lambda layer, without comparing
    with any ground truths.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种类型的孪生神经网络基于计算三重CNN的嵌入层（特征向量）之间的两个欧几里得/余弦距离——即锚点和正样本之间，锚点和负样本之间——然后在Lambda层中完全计算三重损失，而不与任何真实值进行比较。
- en: Because research has shown that this Triplet Loss model is generally more robust
    than the Contrastive Loss model, we will focus on this type of Siamese Network
    in this article.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因为研究表明这种三重损失模型通常比对比损失模型更鲁棒，所以我们将在本文中重点讨论这种类型的孪生网络。
- en: 'Below is an illustration of such a model:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是这种模型的示意图：
- en: '![](../Images/bde0ccaf7d7f9105aa30b78ea33aa7d1.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bde0ccaf7d7f9105aa30b78ea33aa7d1.png)'
- en: Example of Triplet Loss Siamese Neural Networks. Image adapted from the [SigNet
    paper](https://arxiv.org/abs/1707.02131).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 三重损失孪生神经网络的示例。图片改编自[SigNet论文](https://arxiv.org/abs/1707.02131)。
- en: '![](../Images/aba68b6d5df33a9376c706b60e1f0f2f.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aba68b6d5df33a9376c706b60e1f0f2f.png)'
- en: Triplet Loss formula with Euclidean Distance, where A is the Anchor image input,
    P is the Positive image input and N is the Negative image input. Image by Author.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 使用欧几里得距离的三元组损失公式，其中A是锚点图像输入，P是正样本图像输入，N是负样本图像输入。图片由作者提供。
- en: 1.3 The Goal of Siamese Networks
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 孪生网络的目标
- en: 'Now, we have seen the approximate architectures of the Siamese Neural Networks.
    But upon the training of the networks, what are we intending to achieve here?
    Let’s take a look at the illustration below:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经看到孪生神经网络的大致架构。但是在训练网络后，我们打算达到什么目标？让我们看看下面的插图：
- en: '![](../Images/3bbad849a2fd6a9ff5e1ff3a8b828c29.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3bbad849a2fd6a9ff5e1ff3a8b828c29.png)'
- en: Training of Siamese Networks reduces distances between similar images while
    increasing distance between dissimilar images. Image by the [FaceNet paper](https://arxiv.org/abs/1503.03832).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 孪生网络的训练减少了相似图像之间的距离，同时增加了不相似图像之间的距离。图片来源于[FaceNet论文](https://arxiv.org/abs/1503.03832)。
- en: We see that the Siamese Networks are learning to recreate similar feature vectors
    between images of the same class. As such, after training, the distances between
    similar images’ templates will decrease, while that between dissimilar images’
    templates will increase.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到孪生网络正在学习在同一类别图像之间重建相似的特征向量。因此，训练后，相似图像模板之间的距离将减少，而不相似图像模板之间的距离将增加。
- en: With that being said, it is important to cover as many classes of images as
    possible during training, such that the model may also generalize to unseen classes
    (signatures, faces, etc).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，在训练过程中覆盖尽可能多的图像类别是很重要的，以便模型也能推广到未见过的类别（签名、面孔等）。
- en: Finally, during model evaluation, we are chiefly concerned with generating templates
    of input image data. Hence, only a single CNN network or body of the twins/triplet
    networks is extracted, without the Lambda layer, when running template inference.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在模型评估期间，我们主要关注生成输入图像数据的模板。因此，在进行模板推理时，仅提取单个CNN网络或双胞胎/三胞胎网络的主体，而不包括Lambda层。
- en: 1.4 Euclidean Distance versus Cosine Distance
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4 欧几里得距离与余弦距离
- en: 'Before we move to coding, let us first differentiate between 2 common vector
    distance metric — the Euclidean distance and the Cosine Distance. So far in the
    above illustration, we have shown the Euclidean distance, because it is more intuitive
    to understand, but not necessarily more superior to the Cosine distance in building
    a better model. Let’s illustrate below:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始编码之前，让我们首先区分两个常见的向量距离度量——欧几里得距离和余弦距离。到目前为止，在上述插图中，我们展示了欧几里得距离，因为它更直观易懂，但在构建更好的模型时并不一定优于余弦距离。下面我们来说明一下：
- en: '![](../Images/9d78ba9342a646db79e9681c87655878.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d78ba9342a646db79e9681c87655878.png)'
- en: The illustration of the Euclidean Distance and Cosine Distance between two vectors
    in a 2-D space. Image by Author.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 2D空间中两个向量的欧几里得距离和余弦距离的插图。图片由作者提供。
- en: 'From the above, it is clear that the Euclidean distance is simply the ‘coordinate
    distance’ between 2 feature vectors, while the Cosine distance is one measure
    of the ‘angular distance’ between them. Hence, when 2 feature vectors are oriented
    far apart, we can see both Euclidean distance and Cosine distance are similarly
    huge. But there is a subtle difference and let’s see below:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述内容来看，欧几里得距离只是两个特征向量之间的“坐标距离”，而余弦距离是它们之间“角度距离”的一种度量。因此，当两个特征向量远离时，我们可以看到欧几里得距离和余弦距离都很大。但它们之间存在微妙的差别，下面我们来看一下：
- en: '![](../Images/1fe1655927f5b4b0bc6fff6bde06f9f6.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1fe1655927f5b4b0bc6fff6bde06f9f6.png)'
- en: A comparison between Euclidean Distance and Cosine distance at small angles
    but with difference in vector length. Image by Author.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离和余弦距离在小角度下但向量长度不同的比较。图片由作者提供。
- en: While the Cosine distance measures only the angular difference between feature
    vectors, the Euclidean distance measures a second dimension — length difference.
    As such, while more intuitive, the Euclidean distance is inherently a more complex
    metric than the Cosine distance.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然余弦距离仅测量特征向量之间的角度差异，但欧几里得距离测量第二维度——长度差异。因此，虽然更直观，但欧几里得距离本质上比余弦距离更复杂。
- en: Generally speaking, both the Euclidean and Cosine distance are widely used,
    and the choice largely depends on empirical exploration. Nonetheless, for a smaller
    data set and dedicated number of classes, adopting the Cosine distance in the
    loss function could be a better choice, and that is what we would be doing for
    our CIFAR-10 data set.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，欧几里得距离和余弦距离都被广泛使用，选择取决于经验探索。然而，对于较小的数据集和特定数量的类别，采用余弦距离作为损失函数可能是一个更好的选择，这也是我们为CIFAR-10数据集所做的。
- en: 2\. Siamese Networks Code-Along
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2. 孪生网络代码练习
- en: Next, let’s get our hands dirty with coding. We will adopt engineering the Triplet
    Loss Siamese Networks on the TensorFlow CIFAR-10 data set. We will base our Triplet
    Loss on the Cosine Distance, and then during the evaluation of test set, compare
    test images using Angular Similarity.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们开始编码吧。我们将基于TensorFlow CIFAR-10数据集构建三元组损失孪生网络。我们将基于余弦距离来构建三元组损失，然后在测试集评估时，通过角度相似度来比较测试图像。
- en: '***Note****: Angular Similarity is used, as it is based on Cosine Distance,
    except that the values are scaled between 0% to 100%.*'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '***注意****：使用的是角度相似度，因为它基于余弦距离，但值范围缩放在0%到100%之间。*'
- en: '![](../Images/9e537dbfa4d172e7258b836ed56ba068.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e537dbfa4d172e7258b836ed56ba068.png)'
- en: The formula for Angular Similarity. Image by Author.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 角度相似度的公式。图片由作者提供。
- en: Also note that in the model initialization process, we would be adopting the
    TensorFlow Functional API (contrast with the Sequential API we used in the Transfer
    Learning and CNN article introduced earlier), together custom Lambda layer and
    a custom loss function.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要注意的是，在模型初始化过程中，我们将采用TensorFlow的功能性API（对比之前在迁移学习和CNN文章中使用的顺序API），以及自定义Lambda层和自定义损失函数。
- en: Without further hesitation, let’s dive into coding!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不犹豫地，让我们开始编码吧！
- en: 2.1 Exploring the CIFAR-10 Data Set
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 探索CIFAR-10数据集
- en: '[PRE0]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/16cfa2b5ba4012be50c9d9f780b74180.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/16cfa2b5ba4012be50c9d9f780b74180.png)'
- en: '[PRE1]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/61f9aced5df9e6b84789b1650cb2fb91.png)![](../Images/f22b68b08fe4799a91f041d0a31d8e70.png)![](../Images/6fea6756284b4263f056c09e83ed2cf4.png)![](../Images/e51b0c6f3c4b4efa4de8607be1874f2a.png)![](../Images/7157ca6350aeea8a0754b141e91e8237.png)![](../Images/2b39b23b9cd20ebaadcb917ce4d71c46.png)![](../Images/9b7925eed02a4a44dd085d1e0974cefe.png)![](../Images/a23fa14d32c333e2d635cbcf3b39bce4.png)![](../Images/62b8c3a47e98cedffdc78457504ad164.png)![](../Images/cb05639963bc4428bf686a7b120ee594.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61f9aced5df9e6b84789b1650cb2fb91.png)![](../Images/f22b68b08fe4799a91f041d0a31d8e70.png)![](../Images/6fea6756284b4263f056c09e83ed2cf4.png)![](../Images/e51b0c6f3c4b4efa4de8607be1874f2a.png)![](../Images/7157ca6350aeea8a0754b141e91e8237.png)![](../Images/2b39b23b9cd20ebaadcb917ce4d71c46.png)![](../Images/9b7925eed02a4a44dd085d1e0974cefe.png)![](../Images/a23fa14d32c333e2d635cbcf3b39bce4.png)![](../Images/62b8c3a47e98cedffdc78457504ad164.png)![](../Images/cb05639963bc4428bf686a7b120ee594.png)'
- en: 2.2 Generating Triplets
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 生成三元组
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/ecf346bb3cdf61eabc733d4ca1a25593.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ecf346bb3cdf61eabc733d4ca1a25593.png)'
- en: 2.3 Preparing for Model Training/Evaluation
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 准备模型训练/评估
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 2.4 Model Training
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 模型训练
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/f8b519f71d1c4b015ad3f10971c9dcc6.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8b519f71d1c4b015ad3f10971c9dcc6.png)'
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/1ba077aa358af2dca12e061686fede13.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ba077aa358af2dca12e061686fede13.png)'
- en: 2.5 Model Evaluation
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 模型评估
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/086464d89072204c0ac1bed23d6e7143.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/086464d89072204c0ac1bed23d6e7143.png)'
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](../Images/20d182345f5be7ff10a3d3df96a5f514.png)![](../Images/f760b286cc30a620c19bfa8e28d8c02f.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20d182345f5be7ff10a3d3df96a5f514.png)![](../Images/f760b286cc30a620c19bfa8e28d8c02f.png)'
- en: 3\. Conclusion
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3. 结论
- en: Congratulations on completing the theory and code-along! I hope this tutorial
    has provided an all-around useful introduction to Siamese Networks and their application
    to object similarity.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你完成理论和代码练习！希望这个教程为你提供了关于**孪生网络**及其在对象相似度应用方面的全面介绍。
- en: Before we end, I should also add that how the object similarity scores are processed
    depends on the problem statement.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束之前，我还要补充的是，如何处理对象相似度分数取决于问题陈述。
- en: If we are doing a 1:1 object comparison during production (whether 2 objects
    are similar or different), then usually a similarity threshold must be set based
    on the level of False Match Rate (FMR) at test time. On the other hand, if we
    are doing 1:N object matching, then usually the objects with the highest similarity
    scores are returned and ranked.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在生产中进行1:1对象比较（即两个对象是否相似或不同），通常需要基于测试时的**假匹配率**（FMR）设置一个相似度阈值。另一方面，如果进行1:N对象匹配，通常会返回相似度得分最高的对象，并进行排序。
- en: '*Note: For the complete codes, check out my* [*GitHub*](https://github.com/tanpengshi/Siamese_Networks_Triplet_Loss_Cosine_Distance)*.*'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*注：有关完整的代码，请查看我的* [*GitHub*](https://github.com/tanpengshi/Siamese_Networks_Triplet_Loss_Cosine_Distance)*。*'
- en: 'With that, I thank you for your time and hope you enjoyed this tutorial. I
    would also like to end off by introducing you to an extremely important topic
    of [data-centric machine learning](https://landing.ai/data-centric-ai/) that is
    elaborated on in this article:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢您的时间，希望您喜欢本教程。我还想介绍一个在这篇文章中详细阐述的极其重要的话题——[以数据为中心的机器学习](https://landing.ai/data-centric-ai/)。
- en: '[](https://pub.towardsai.net/data-collection-and-augmentation-strategy-for-artificial-intelligence-37eacc49129f?source=post_page-----6aef1002c4e--------------------------------)
    [## Data-Centric AI — Data Collection and Augmentation Strategy'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pub.towardsai.net/data-collection-and-augmentation-strategy-for-artificial-intelligence-37eacc49129f?source=post_page-----6aef1002c4e--------------------------------)
    [## 以数据为中心的 AI — 数据收集和增强策略]'
- en: A comprehensive guide to data generation strategy for data-centric Machine Learning
    projects
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于以数据为中心的机器学习项目的数据生成策略的综合指南
- en: pub.towardsai.net](https://pub.towardsai.net/data-collection-and-augmentation-strategy-for-artificial-intelligence-37eacc49129f?source=post_page-----6aef1002c4e--------------------------------)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[pub.towardsai.net](https://pub.towardsai.net/data-collection-and-augmentation-strategy-for-artificial-intelligence-37eacc49129f?source=post_page-----6aef1002c4e--------------------------------)'
- en: Thanks for reading! If you have enjoyed the content, pop by my other articles
    on [Medium](https://tanpengshi.medium.com/) and follow me on [LinkedIn](https://www.linkedin.com/in/tanpengshi/).
  id: totrans-97
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 感谢阅读！如果您喜欢我的内容，可以浏览我在 [Medium](https://tanpengshi.medium.com/) 上的其他文章，并在 [LinkedIn](https://www.linkedin.com/in/tanpengshi/)
    上关注我。
- en: ''
  id: totrans-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '***Support me!*** — If you are *not* subscribed to Medium, and like my content,
    do consider supporting me by joining Medium via my [referral link](https://tanpengshi.medium.com/membership).'
  id: totrans-99
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***支持我！*** — 如果您*没有*订阅 Medium，并且喜欢我的内容，请考虑通过我的[推荐链接](https://tanpengshi.medium.com/membership)来支持我。'
- en: '[](https://tanpengshi.medium.com/membership?source=post_page-----6aef1002c4e--------------------------------)
    [## Join Medium with my referral link - Tan Pengshi Alvin'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://tanpengshi.medium.com/membership?source=post_page-----6aef1002c4e--------------------------------)
    [## 通过我的推荐链接加入 Medium - Tan Pengshi Alvin]'
- en: Read every story from Tan Pengshi Alvin (and thousands of other writers on Medium).
    Your membership fee directly…
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阅读 Tan Pengshi Alvin 的每一个故事（以及 Medium 上成千上万的其他作家）。您的会员费用直接…
- en: tanpengshi.medium.com](https://tanpengshi.medium.com/membership?source=post_page-----6aef1002c4e--------------------------------)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[tanpengshi.medium.com](https://tanpengshi.medium.com/membership?source=post_page-----6aef1002c4e--------------------------------)'
