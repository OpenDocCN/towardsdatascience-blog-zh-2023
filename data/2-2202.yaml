- en: Understanding the Importance of Diversity in Ensemble Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解集成学习中多样性的重要性
- en: 原文：[https://towardsdatascience.com/understanding-the-importance-of-diversity-in-ensemble-learning-34fb58fd2ed0](https://towardsdatascience.com/understanding-the-importance-of-diversity-in-ensemble-learning-34fb58fd2ed0)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/understanding-the-importance-of-diversity-in-ensemble-learning-34fb58fd2ed0](https://towardsdatascience.com/understanding-the-importance-of-diversity-in-ensemble-learning-34fb58fd2ed0)
- en: The role of diversity in improving ensemble performance
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多样性在提升集成性能中的作用
- en: '[](https://donatoriccio.medium.com/?source=post_page-----34fb58fd2ed0--------------------------------)[![Donato
    Riccio](../Images/0af2a026e72a023db4635522cbca50eb.png)](https://donatoriccio.medium.com/?source=post_page-----34fb58fd2ed0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----34fb58fd2ed0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----34fb58fd2ed0--------------------------------)
    [Donato Riccio](https://donatoriccio.medium.com/?source=post_page-----34fb58fd2ed0--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://donatoriccio.medium.com/?source=post_page-----34fb58fd2ed0--------------------------------)[![Donato
    Riccio](../Images/0af2a026e72a023db4635522cbca50eb.png)](https://donatoriccio.medium.com/?source=post_page-----34fb58fd2ed0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----34fb58fd2ed0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----34fb58fd2ed0--------------------------------)
    [Donato Riccio](https://donatoriccio.medium.com/?source=post_page-----34fb58fd2ed0--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----34fb58fd2ed0--------------------------------)
    ·9 min read·Jan 2, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----34fb58fd2ed0--------------------------------)
    ·阅读时长9分钟·2023年1月2日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/dab0c86b9beea259e2d15c4ae808d51d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dab0c86b9beea259e2d15c4ae808d51d.png)'
- en: Photo by [Mulyadi](https://unsplash.com/@mullyadii?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Mulyadi](https://unsplash.com/@mullyadii?utm_source=medium&utm_medium=referral)
    提供，发布在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
    上
- en: Ensemble learning is a machine learning technique that combines multiple models
    to achieve better results. It is only in recent years, with the increasing computing
    speed, that ensemble learning became one of the most effective techniques for
    tackling difficult problems in the machine learning domain. This approach is used
    in most of the winning solutions for machine learning competitions, with prizes
    up to 100,000 dollars.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习是一种将多个模型结合以获得更好结果的机器学习技术。只有近年来，随着计算速度的提高，集成学习才成为应对机器学习领域困难问题的最有效技术之一。这种方法在大多数机器学习竞赛的获胜解决方案中使用，奖金高达10万美元。
- en: How do we select the best ensemble members, to combine them in a single, more
    powerful model? In this article, we’ll explore how to choose models to ensemble
    and how to aggregate them.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何选择最佳的集成成员，将它们组合成一个更强大的模型？在本文中，我们将探讨如何选择模型进行集成以及如何对它们进行聚合。
- en: '**What is diversity?**'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**什么是多样性？**'
- en: Ensemble learning is based on the concept of combining multiple models, called
    *weak learners*. The name comes from the idea that individual ensemble members
    don’t need to be very accurate. As long as they are a little bit better than a
    random model, combining them is beneficial. Diversity is an important concept
    in ensemble learning, and refers to the idea that the individual models prediction
    in an ensemble should be **as different from each other as possible.** This is
    because different models are likely to make different types of errors. By combining
    the predictions of a diverse set of models, we can reduce the overall error of
    the ensemble.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习基于结合多个模型的概念，这些模型被称为*弱学习者*。这个名字来源于这样一个想法：个体集成成员不需要非常准确。只要它们比随机模型稍微好一点，结合它们就会有益。多样性是集成学习中的一个重要概念，指的是集成中的个体模型的预测应该**尽可能地彼此不同**。这是因为不同的模型可能会犯不同类型的错误。通过结合多样化模型的预测，我们可以减少整体的集成误差。
- en: '**How can we increase diversity?**'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**我们如何增加多样性？**'
- en: 'Over the years, several ensemble learning algorithms were developed. Each one
    has a unique way to achieve diversity. These approaches include:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，开发了几种集成学习算法。每种算法都有独特的方式来实现多样性。这些方法包括：
- en: Training each model on a **different subset of the training data.** When samples
    are drown with replacement, it’s known as **bagging**. When sampling is performed
    without replacement, it’s called **pasting**. It can also help to reduce the variance
    of the ensemble’s predictions, which can improve its generalization. The idea
    of diversity in bagging has been further developed in **Random Forest** and **Extremely
    Randomized Trees**. The first that achieves diversity by choosing a **random number
    of features** available to the trees at each split, while the latter employs a
    **random split** in order to lower the correlation between trees in the ensemble.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在**不同的训练数据子集**上训练每个模型。当样本是有放回地抽取时，称为**自助法**。当抽样是无放回地进行时，称为**粘贴法**。这也有助于减少集成预测的方差，从而改善其泛化能力。自助法中的多样性理念在**随机森林**和**极端随机树**中得到了进一步的发展。前者通过选择每次分裂时树可用的**随机特征数量**来实现多样性，而后者通过进行**随机分裂**来降低树之间的相关性。
- en: Training each model using a **different set of features.** An ensemble can be
    trained using different combinations of the available features or different transformations
    of the original features. This can help to capture different aspects of the data,
    leading to improved performance.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**不同特征集**训练每个模型。可以使用可用特征的不同组合或原始特征的不同转换来训练集成。这有助于捕捉数据的不同方面，从而提高性能。
- en: Training **each model using a different type of algorithm.** This is the approach
    used in **voting** and **stacking** meta-models. By using different algorithms,
    the individual models in the ensemble can capture different patterns in the data
    and make different types of errors.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**不同类型的算法**训练**每个模型**。这是**投票**和**堆叠**元模型中使用的方法。通过使用不同的算法，集成中的个别模型可以捕捉数据中的不同模式并产生不同类型的错误。
- en: '![](../Images/7e568ea704605381e5de71ae88ddb2f0.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e568ea704605381e5de71ae88ddb2f0.png)'
- en: Voting ensembles. It’s important to include diverse ensemble members. [1]
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 投票集成。包括多样化的集成成员是很重要的。[1]
- en: However, diverse predictions are not always better.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，多样化的预测并不总是更好。
- en: '**Good and Bad diversity**'
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**良好的和不良的多样性**'
- en: In a majority vote ensemble, the concept of *wasted* votes can be used to understand
    the diversity of the ensemble. If the ensemble is already correct, low disagreement
    among the classifiers indicates that several votes have been wasted, as the same
    correct decision would have been made regardless of the individual classifiers
    votes. This is known as **good diversity**, as it measures disagreement where
    the ensemble is already correct. More disagreement in this situation means fewer
    wasted votes. In contrast, **bad diversity** measures disagreement where the ensemble
    is incorrect. In this case, any disagreement represents a wasted vote, as the
    individual classifier did not contribute to the correct decision made by the ensemble.
    To maximize the efficiency of the ensemble, it is important to increase good diversity
    and decrease bad diversity, which can be achieved by reducing the number of wasted
    votes.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在多数投票集成中，*浪费* 投票的概念可以用来理解集成的多样性。如果集成已经是正确的，分类器之间的低分歧表明已经有一些投票被浪费了，因为无论个别分类器的投票结果如何，都会做出相同的正确决策。这被称为**良好的多样性**，它测量了集成已经正确时的分歧。在这种情况下，更多的分歧意味着更少的浪费投票。相反，**不良的多样性**测量集成不正确时的分歧。在这种情况下，任何分歧都代表着浪费的投票，因为个别分类器没有为集成做出的正确决策做出贡献。为了最大化集成的效率，重要的是增加良好的多样性并减少不良的多样性，这可以通过减少浪费投票的数量来实现。
- en: Another way to think about this is from the accuracy perspective. For example,
    suppose that the decision tree model tends to be very good at identifying dogs,
    but has trouble with cats. The logistic regression model, on the other hand, is
    better at identifying cats but has trouble with dogs. Each of these algorithms
    has its own strengths and weaknesses, and will make different types of errors
    on the data. By combining the predictions of these two models, we can create an
    ensemble that is more accurate overall than either individual model. This is an
    example of **good diversity**. We could add a third model, bad at both classifying
    cats and dogs. The third model would increase the ensemble’s diversity without
    bringing any benefits. This is considered **bad diversity.** [2]
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 从准确性角度来看，另一种思考方式是这样的。例如，假设决策树模型在识别狗方面表现很好，但在识别猫时有困难。另一方面，逻辑回归模型在识别猫方面表现更好，但在识别狗时有困难。这些算法各有优缺点，会对数据产生不同类型的错误。通过结合这两个模型的预测，我们可以创建一个总体准确性高于任何单个模型的集成。这就是**良好多样性**的一个例子。我们可以再添加一个第三个模型，该模型在分类猫和狗方面都很差。第三个模型将增加集成的多样性，但不会带来任何好处。这被认为是**差的多样性**。[2]
- en: '**How can we measure diversity?**'
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**我们如何测量多样性？**'
- en: Let’s call ***f₁, f₂, … fₙ*** the predictions of different models in an ensemble.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们称***f₁, f₂, … fₙ***为集成中不同模型的预测。
- en: 'There are two types of diversity measures: pairwise and global. Pairwise metrics
    need to be computed for every pair of ***fᵢ, fⱼ*.** In the end, you’ll get a ***n*x*n***
    matrix. Global ones are computed on the whole matrix of predictions, and they
    are represented by a single value. The following list is by no means exhaustive,
    you can find more measures in the paper by Kuncheva, in the references.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 多样性度量有两种类型：成对和全局。成对度量需要对每一对***fᵢ, fⱼ***进行计算。最终，你会得到一个***n*x*n***矩阵。全局度量是在整个预测矩阵上计算的，它们由单一值表示。以下列表绝非详尽，你可以在Kuncheva的论文中找到更多度量，参考文献中有列出。
- en: In the formulas, 0 means that the prediction is wrong, and 1 that is correct.
    For example, ***N⁰¹*** means the number of times where the first classifier is
    correct, and the second is wrong.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在公式中，0表示预测错误，1表示预测正确。例如，***N⁰¹***表示第一个分类器正确而第二个错误的次数。
- en: '**Pearson correlation coefficient** The simplest way to compute similarity
    in an ensemble is to use Pearson correlation coefficient. If the predictions from
    two models are very correlated, it means that they are very similar. The maximum
    diversity is obtained when **ρᵢⱼ** = 0, while two classifiers that produce the
    exact same outputs will have **ρᵢⱼ** = 1\. It’s possible to compute the correlation
    coefficient when the predictions are probabilities, like in soft voting ensembles.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**皮尔逊相关系数** 计算集成中相似性的最简单方法是使用皮尔逊相关系数。如果两个模型的预测高度相关，这意味着它们非常相似。当**ρᵢⱼ** = 0时获得最大多样性，而两个产生完全相同输出的分类器将有**ρᵢⱼ**
    = 1。即使在软投票集成中，当预测是概率时，也可以计算相关系数。'
- en: The following metrics are used on binary class predictions.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下指标用于二元类别预测。
- en: '**Disagreement** As the name suggests, it’s how much disagreement there is
    between the predictions. It is calculated by dividing the number of times the
    predictions differ by the total number of predictions made. Disagreement takes
    values between 0 (there are no prediction that differ) and 1 (every prediction
    differs).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**分歧** 正如名字所示，它表示预测之间的分歧程度。通过将预测不同的次数除以总的预测次数来计算分歧。分歧值介于0（没有预测不同）和1（每个预测都不同）之间。'
- en: '![](../Images/6dda48c6898fea53e2ce9afd9032c975.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6dda48c6898fea53e2ce9afd9032c975.png)'
- en: Disagreement measure. [2]
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 分歧度量。[2]
- en: '**Yule’s Q** Yule’s Q has values between -1 and 1\. This metric adds an important
    information: positive values indicate that models are classifying the *same* objects
    correctly, while those who are wrong on different objects will have negative Q
    values. The value of 0 suggests that the predictions are independent.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**Yule的Q** Yule的Q值介于-1和1之间。这个度量提供了重要的信息：正值表示模型在正确分类*相同*对象，而在不同对象上的错误将导致负Q值。0的值表明预测是独立的。'
- en: '![](../Images/998accb7bc085fe11babb8fa26d2d4ff.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/998accb7bc085fe11babb8fa26d2d4ff.png)'
- en: Yule’s Q. [2]
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Yule的Q。[2]
- en: '**Entropy** The following measure is a global metric. It’s computed on the
    whole matrix of predictions. Entropy is based on the idea that when disagreement
    is the maximum, half of the predictions are zero, while the other half are one.
    In the following formula, L is the total number of ensemble members, and *l* is
    the total number of classifiers that correctly classify an instance z*ⱼ.*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**熵** 以下度量是一个全局指标。它在整个预测矩阵上计算。熵基于这样一个思想：当不一致性达到最大时，一半的预测是零，而另一半是1。在以下公式中，L是集成模型的总成员数，*l*是正确分类实例z*ⱼ*的分类器总数。'
- en: '![](../Images/fa6d2be519d578a8225102e55dac5a6b.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa6d2be519d578a8225102e55dac5a6b.png)'
- en: Entropy for binary predictions. [2]
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 二元预测的熵。[2]
- en: '**Python implementation**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python实现**'
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Building a voting classifier
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建投票分类器
- en: Let’s now compare two ensembles to evaluate how diverse their members are. For
    the following examples I used [pycaML](https://pypi.org/project/pycaML/), which
    allows to train and compare models and ensembles with few lines of code.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们比较两个集成模型，以评估其成员的多样性。在以下示例中，我使用了[pycaML](https://pypi.org/project/pycaML/)，它允许用几行代码训练和比较模型及集成模型。
- en: I choose 9 models for the first ensemble, and 10 models for the second one.
    Every models is trained with default parameters. The [dataset](https://archive.ics.uci.edu/ml/datasets/diabetes)
    is available in the UCI repository.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我为第一个集成模型选择了9个模型，为第二个集成模型选择了10个模型。每个模型都使用默认参数进行训练。数据集可在[UCI存储库](https://archive.ics.uci.edu/ml/datasets/diabetes)中获得。
- en: '**Model comparison** For having a baseline to compare the ensemble with, the
    following figure shows the 10 best performing models. We’ll focus on the F1 score.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型比较** 为了有一个基准来比较集成模型，下面的图展示了表现最好的10个模型。我们将重点关注F1得分。'
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/66223cd711a87a7f65f2ed4bfa4deba9.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66223cd711a87a7f65f2ed4bfa4deba9.png)'
- en: 10 best performing models on the diabetes dataset, generated with pycaML. Image
    by author.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在糖尿病数据集上表现最好的10个模型，由pycaML生成。图像由作者提供。
- en: The prediction matrix can be computed as follows. In the end, we’ll have a 154x19
    matrix, where each row is an instance of the test set, and every column is a model.
    After obtaining the matrix, we can split it in two ensembles to compare them.
    The first one contains the odd indexed models, while the second one the even indexed
    models.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 预测矩阵可以如下计算。最终，我们将得到一个154x19的矩阵，每一行是测试集中的一个实例，每一列是一个模型。得到矩阵后，我们可以将其拆分为两个集成模型进行比较。第一个包含奇数索引的模型，而第二个包含偶数索引的模型。
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Looking at the disagreement matrix for ensemble 1, it seems that Perceptron
    is the model that disagrees the most with the others. On the other hand, there
    is little disagreement between boosting models.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 查看集成模型1的不一致矩阵，似乎Perceptron是与其他模型不一致最多的模型。另一方面，提升模型之间的不一致较少。
- en: '![](../Images/82eb081f10c8ba4a579ef01fc1e1a943.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82eb081f10c8ba4a579ef01fc1e1a943.png)'
- en: Disagreement matrix for ensemble 1\. Image by author.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 集成模型1的不一致矩阵。图像由作者提供。
- en: Ensemble 2 contains different models. Overall, it seems that disagreement is
    pretty low. The member with the highest disagreement is Passive Aggressive Classifier.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 集成模型2包含不同的模型。总体来看，不一致性相当低。与其他模型不一致最多的是Passive Aggressive Classifier。
- en: '![](../Images/09a8430841010c5bb47963d02f7be570.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09a8430841010c5bb47963d02f7be570.png)'
- en: Disagreement matrix for ensemble 2\. Image by author.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 集成模型2的不一致矩阵。图像由作者提供。
- en: Since Ensemble 1 has higher entropy, it means that the diversity is higher.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 由于集成模型1具有更高的熵，这意味着其多样性更高。
- en: '![](../Images/281a275728aa29d0b855d8e0d89905ed.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/281a275728aa29d0b855d8e0d89905ed.png)'
- en: Entropy scores. Image by author.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 熵得分。图像由作者提供。
- en: Let’s now build the final model by taking the majority vote (mode). This meta-model
    is called hard voting. The two ensembles are evaluated using the same metric as
    before, F1 score.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过投票来建立最终模型，这种元模型被称为硬投票。两个集成模型使用与之前相同的指标进行评估，即F1得分。
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/519b7ab3e40e2308a67e1a29b53243a1.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/519b7ab3e40e2308a67e1a29b53243a1.png)'
- en: Results after evaluating the models. Image by author.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 模型评估后的结果。图像由作者提供。
- en: Finally, we show that higher diversity (ensemble 1) leads to a better ensemble
    model. The best model found in the comparison is Extra Trees, with a F1 score
    of 0.71\. The final model performs better than the best model trained on the dataset
    by a considerable margin, reaching a score of 0.764.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们展示了更高的多样性（集成模型1）导致了更好的集成模型。比较中找到的最佳模型是Extra Trees，其F1得分为0.71。最终模型的表现比在数据集上训练的最佳模型高出不少，达到了0.764的得分。
- en: Conclusions
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: Ensemble learning is a powerful machine learning technique that involves training
    multiple models and combining their predictions to achieve improved performance.
    We found that as the diversity of the models in the ensemble increased, the performance
    of the ensemble also increased.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习是一种强大的机器学习技术，它涉及训练多个模型并结合它们的预测以实现更好的性能。我们发现，当集成中的模型多样性增加时，集成的性能也会提高。
- en: This finding highlights the value of diversity in ensemble learning. When building
    an ensemble, it’s important to consider diversity among the models in order to
    achieve the best possible performance.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这一发现突显了集成学习中多样性的价值。在构建集成模型时，考虑模型之间的多样性对于实现最佳性能至关重要。
- en: So, the next time you’re working on a machine learning project and considering
    using an ensemble, remember to prioritize diversity among the models. It could
    make all the difference in the final performance of your model.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，下次你在进行机器学习项目并考虑使用集成学习时，记得优先考虑模型之间的多样性。这可能会对模型的最终性能产生重要影响。
- en: '*Enjoyed this article? Get weekly data science interview questions delivered
    to your inbox by subscribing to my newsletter,* [*The Data Interview*](https://thedatainterview.substack.com/)*.*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*喜欢这篇文章吗？通过订阅我的通讯，* [*数据面试*](https://thedatainterview.substack.com/)*，每周获取数据科学面试问题。*'
- en: '*Also, you can find me on* [*LinkedIn*](https://www.linkedin.com/in/driccio/)*.*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*此外，你还可以在* [*LinkedIn*](https://www.linkedin.com/in/driccio/)*上找到我。*'
- en: References
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Géron, A. (2019). Hands-on machine learning with Scikit-Learn, Keras and
    TensorFlow: concepts, tools, and techniques to build intelligent systems (2nd
    ed.). O’Reilly.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Géron, A. (2019). 实用机器学习与 Scikit-Learn、Keras 和 TensorFlow: 构建智能系统的概念、工具和技术（第2版）。O’Reilly.'
- en: '[2] Kuncheva, L.I., Whitaker, C.J. Measures of Diversity in Classifier Ensembles
    and Their Relationship with the Ensemble Accuracy. *Machine Learning* **51**,
    181–207 (2003). [https://doi.org/10.1023/A:1022859003006](https://doi.org/10.1023/A:1022859003006)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Kuncheva, L.I., Whitaker, C.J. 分类器集成中的多样性测量及其与集成准确性的关系。 *机器学习* **51**,
    181–207 (2003). [https://doi.org/10.1023/A:1022859003006](https://doi.org/10.1023/A:1022859003006)'
