- en: Map, Filter, and CombinePerKey Transforms in Writing Apache Beam Pipelines with
    Examples
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在编写 Apache Beam 管道时使用示例进行 Map、Filter 和 CombinePerKey 转换
- en: 原文：[https://towardsdatascience.com/map-filter-and-combineperkey-transforms-in-writing-apache-beam-pipelines-with-examples-e06926124a02](https://towardsdatascience.com/map-filter-and-combineperkey-transforms-in-writing-apache-beam-pipelines-with-examples-e06926124a02)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/map-filter-and-combineperkey-transforms-in-writing-apache-beam-pipelines-with-examples-e06926124a02](https://towardsdatascience.com/map-filter-and-combineperkey-transforms-in-writing-apache-beam-pipelines-with-examples-e06926124a02)
- en: '![](../Images/3c0bc71612c5632ec1b351c55f68aa03.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c0bc71612c5632ec1b351c55f68aa03.png)'
- en: Photo by [JJ Ying](https://unsplash.com/@jjying?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [JJ Ying](https://unsplash.com/@jjying?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Let’s Practice with Some Real Data
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们用一些真实数据进行练习
- en: '[](https://rashida00.medium.com/?source=post_page-----e06926124a02--------------------------------)[![Rashida
    Nasrin Sucky](../Images/42bd057e8eca255907c43c29a498f2ca.png)](https://rashida00.medium.com/?source=post_page-----e06926124a02--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e06926124a02--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e06926124a02--------------------------------)
    [Rashida Nasrin Sucky](https://rashida00.medium.com/?source=post_page-----e06926124a02--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://rashida00.medium.com/?source=post_page-----e06926124a02--------------------------------)[![Rashida
    Nasrin Sucky](../Images/42bd057e8eca255907c43c29a498f2ca.png)](https://rashida00.medium.com/?source=post_page-----e06926124a02--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e06926124a02--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e06926124a02--------------------------------)
    [Rashida Nasrin Sucky](https://rashida00.medium.com/?source=post_page-----e06926124a02--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e06926124a02--------------------------------)
    ·8 min read·Jul 12, 2023
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e06926124a02--------------------------------)
    ·8分钟阅读·2023年7月12日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'Apache Beam is getting popularity as the unified programming model for efficient
    and portable big data processing pipelines. It can deal with both batch and streaming
    data. That’s how the name comes from. Beam is combination of the words Batch and
    Stream:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Beam 作为统一的编程模型，在高效和可移植的大数据处理管道中越来越受欢迎。它可以处理批量数据和流数据。这也是名字的由来。Beam 是 Batch
    和 Stream 两个词的组合：
- en: B(from **B**atch) + eam(from str**eam**)= Beam
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: B（来自**B**atch）+ eam（来自 str**eam**）= Beam
- en: The portability also is a great feature. You just need to focus on running the
    pipeline and it can be run from anywhere such as Spark, Flink, Apex, or Cloud
    Dataflow. You don’t need to change the logic or syntax for that.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 便携性也是一个很棒的特性。你只需专注于运行管道，它可以在任何地方运行，例如 Spark、Flink、Apex 或 Cloud Dataflow。你不需要更改逻辑或语法。
- en: In this article, we will focus on learning to write some ETL Pipelines using
    examples. We will try some transform operations using a good dataset and hopefully
    you will find all this transform operations useful in your work as well.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将专注于学习如何通过示例编写一些ETL管道。我们将尝试使用一个好的数据集进行一些转换操作，希望你会发现这些转换操作在工作中也非常有用。
- en: 'Please feel free to download this [public dataset](https://creativecommons.org/publicdomain/zero/1.0/)
    and follow along:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 请随意下载这个[公共数据集](https://creativecommons.org/publicdomain/zero/1.0/)并跟随练习：
- en: '[Sample Sales Data | Kaggle](https://www.kaggle.com/datasets/kyanyoga/sample-sales-data)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例销售数据 | Kaggle](https://www.kaggle.com/datasets/kyanyoga/sample-sales-data)'
- en: 'A Google Colab notebook is used for this exercise. So, installation is very
    easy. Just use this line of code:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习使用了 Google Colab notebook。因此，安装非常简单。只需使用这一行代码：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After installation is done, I made a directory for this exercise named ‘data’:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，我为这个练习创建了一个名为‘data’的目录：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Let’s dive into today’s topic that is the transform operations. To start with
    we will work on a simplest pipeline that is just read the CSV file and Write it
    to a text file.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨今天的话题——转换操作。首先，我们将处理一个最简单的管道，即读取CSV文件并将其写入文本文件。
- en: This is not as simple as Padas read_csv() method. It requires a coder() opeartion.
    First a CustomCoder() class was defined here that first encode the objects into
    byte strings, then decode the bytes to its corresponding objects and finally specifies
    if this coder is guaranteed to encode values deterministically. Please check [the
    documentation here.](https://beam.apache.org/releases/pydoc/2.2.0/apache_beam.coders.coders.html)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这不像 Padas 的 read_csv() 方法那么简单。它需要一个 coder() 操作。首先，在这里定义了一个 CustomCoder() 类，该类首先将对象编码为字节字符串，然后将字节解码为其对应的对象，并最终指定这个
    coder 是否保证对值进行确定性编码。请查看 [文档这里。](https://beam.apache.org/releases/pydoc/2.2.0/apache_beam.coders.coders.html)
- en: If this is your first Pipeline, please notice the syntax for a pipeline. After
    the CustomCoder() class there is the simplest pipeline. We initiated the empty
    pipeline as ‘p1’ first. Then we wrote the ‘sales’ Pipeline where first read the
    CSV file from the data folder that we created earlier. In Apache beam each transform
    operation in the pipeline starts with this | sign. After reading the data from
    the CSV file we just write it to a text file. At the end, with a run () method
    we ran the pipeline. This is the standard and usual pipeline syntax in Apache
    beam.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是你的第一个管道，请注意管道的语法。在 CustomCoder() 类之后是最简单的管道。我们首先将空管道初始化为‘p1’。然后我们编写了‘sales’管道，其中首先从我们之前创建的数据文件夹中读取
    CSV 文件。在 Apache beam 中，管道中的每个转换操作都以 | 符号开始。读取 CSV 文件中的数据后，我们只是将其写入文本文件。最后，通过 run()
    方法我们运行了管道。这是 Apache beam 中标准和常用的管道语法。
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If you check your ‘data’ folder now you will see a ‘output-00000-of-00001’
    file there. Printing the first 5 rows from this file to check the data:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你现在检查你的‘data’文件夹，你会看到一个‘output-00000-of-00001’文件。从这个文件中打印前 5 行以检查数据：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Output:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Map
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Map
- en: Let’s see how to use a Map transform in the above pipeline. This is the most
    common transform operation. The transformation you specify in the Map will apply
    to every single element in the PCollection.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下如何在上述管道中使用 Map 转换。这是最常见的转换操作。你在 Map 中指定的转换将应用于 PCollection 中的每一个元素。
- en: For example, I would like to add a split method to create lists out of every
    element in the PCollection. Here we will use lambda for Map transform. If you
    are not familiar with lambda, observe this code with lambda here. After lanbda
    we mentioned ‘row’, any other name of the variable is fine too. Whatever function
    or method we would apply to the ‘row’ that will be applied to every element in
    the PCollection.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我想添加一个 split 方法以从 PCollection 中的每个元素创建列表。在这里，我们将使用 lambda 进行 Map 转换。如果你不熟悉
    lambda，请查看这里的 lambda 代码。lambda 后我们提到‘row’，任何其他变量名也可以。我们对‘row’应用的任何函数或方法将应用于 PCollection
    中的每个元素。
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Look, it is the exact same syntax. Just I put one extra line of code in between
    read and write operation. Again printing the first 5 rows of the output to check:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 看，它是完全相同的语法。只是我在读取和写入操作之间多了一行代码。再次打印输出的前 5 行进行检查：
- en: '[PRE6]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Output:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE7]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Look, each element has become a list.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 看，每个元素都变成了一个列表。
- en: Filter
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Filter
- en: Next, I will add Filter transform to the above code block as well. Lambda can
    be used here again for filter as well. We will filter out all the data and keep
    only the data for ‘Classic Cars’ from Produc line. The 11th column of the dataset
    is the product line. As you know Python is zero indexed. So, counting of column
    number also starts from zero.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将把 Filter 转换也添加到上述代码块中。这里的 lambda 也可以用于过滤。我们将过滤掉所有数据，只保留 Produc line 中的‘经典汽车’数据。数据集的第
    11 列是产品线。正如你所知，Python 是零索引的。所以，列号的计数也从零开始。
- en: '[PRE8]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As before, printing the first 5 rows for checking:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如之前所示，打印前 5 行进行检查：
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Output:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE10]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Look at the 11th element of each list in the output above. It’s ‘Classic Cars’.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 查看上面输出中每个列表的第 11 个元素。它是‘经典汽车’。
- en: Answering Some Questions
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回答一些问题
- en: '**How much quantity was ordered for each type of automobile?**'
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**每种类型的汽车订购了多少数量？**'
- en: To find this out, we will first create tuples where the first element or the
    key will come from the 11th element of the dataset and the second element that
    means the value will be the second element of the dataset that is ‘QUANTITY ORDERED’.
    In the next step, we will use CombinePerKey () method. As the name suggest, it
    will combine the values with an aggregate function for each key.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找出这一点，我们首先将创建元组，其中第一个元素或键将来自数据集的第 11 个元素，第二个元素即值将是数据集的第二个元素，即‘订购数量’。在下一步中，我们将使用
    CombinePerKey() 方法。正如名字所示，它将为每个键结合具有聚合函数的值。
- en: It will be clearer when you will see the code. Here is the code.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当你看到代码时会更清楚。这里是代码。
- en: '[PRE11]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As you can see, we used Map function twice here. First to split and make lists
    as before and then from each row of data we took the product line that is 10th
    column, and the Quantity that is second column only.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们在这里使用了两次 Map 函数。第一次是分割并像之前一样生成列表，然后从每行数据中提取第 10 列的产品线和第二列的数量。
- en: 'Here is the output:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE12]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Just printed out the first 10 rows of the output. As you can see, we have the
    quantity ordered for each row of the data here. The next and the final step to
    answer the question above is to combine all the values for each item. There is
    CombinePerKey method available in apache beam for that. As the name suggest, it
    will combine the values with an aggregate function for each key. In this case
    we need the ‘sum’.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 只是打印了输出的前 10 行。如你所见，这里每行数据的订单数量都列出了。回答上述问题的下一步也是最后一步是将每项的所有值结合起来。Apache Beam
    中有 CombinePerKey 方法可以实现。顾名思义，它会为每个键使用聚合函数来结合值。在这种情况下，我们需要的是“总和”。
- en: '[PRE13]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Output:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE14]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: So, we have the total quantity ordered for each product.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们得到了每个产品的总订单数量。
- en: '**From which states more than 2000 orders were placed?**'
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**哪些州的订单数量超过了 2000 个？**'
- en: This is an interesting question where we need every transform, we have done
    earlier plus another filter transform. We need to calculate the total number of
    orders for each state the way we calculated the total number of orders for each
    product in the previous example. And then the quantity that are more than 2000
    should be filtered in.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个有趣的问题，我们需要每一个之前做过的变换加上另一个过滤变换。我们需要计算每个州的总订单数量，就像在前面的例子中计算每个产品的总订单数量一样。然后，将数量超过
    2000 的订单进行过滤。
- en: In all the previous examples, lambda function were used in Map and Filter transforms.
    Here we will see how we can define a function and use that in the Map or Filter
    function. A function quantity_filter() is defined here that returns the items
    with value count more than 2000.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有之前的例子中，lambda 函数被用于 Map 和 Filter 变换。这里我们将看到如何定义一个函数并在 Map 或 Filter 函数中使用它。这里定义了一个函数
    quantity_filter()，它返回值数量大于 2000 的项。
- en: '[PRE15]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Output:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE16]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This is the output where if the quantity is not bigger than 2000, it returned
    ‘None’. I don’t like to see all this ‘None’ values. I will add another filter
    transform to filter out these ‘None’ values.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出，其中如果数量不超过 2000，则返回‘None’。我不喜欢看到所有这些‘None’值。我将添加另一个过滤变换来过滤掉这些‘None’值。
- en: '[PRE17]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Output:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE18]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: So, we have total 5 values returned where the order count is bigger than 2000.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们总共有 5 个返回的值，其中订单数量大于 2000。
- en: Conclusion
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: In this tutorial I wanted to show how to use Map, Filter, CombinePerKey transforms
    in Apache beam in writing ETL pipelines. Hopefully they are clear enough to use
    in your own projects. I will explain how to use ParDo in my next article.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我想展示如何在 Apache Beam 中使用 Map、Filter 和 CombinePerKey 变换来编写 ETL 管道。希望它们足够清晰，能够在你的项目中使用。我将在下一篇文章中解释如何使用
    ParDo。
- en: Feel free to follow me on [Twitter](https://twitter.com/rashida048) and like
    my [Facebook](https://www.facebook.com/rashida.smith.161) page.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 随意关注我 [Twitter](https://twitter.com/rashida048) 并点赞我的 [Facebook](https://www.facebook.com/rashida.smith.161)
    页面。
- en: More Reading
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关阅读
- en: '[A Detailed Tutorial on Polynomial Regression in Python, Overview, Implementation,
    and Overfitting | by Rashida Nasrin Sucky | Jun, 2023 | Towards AI](https://pub.towardsai.net/a-detailed-tutorial-on-polynomial-regression-in-python-overview-implementation-and-overfitting-e319fc7e5b8f)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[关于 Python 中多项式回归的详细教程、概述、实现和过拟合 | 作者：Rashida Nasrin Sucky | 2023年6月 | Towards
    AI](https://pub.towardsai.net/a-detailed-tutorial-on-polynomial-regression-in-python-overview-implementation-and-overfitting-e319fc7e5b8f)'
- en: '[Complete Implementation of a Mini VGG Network for Image Recognition | by Rashida
    Nasrin Sucky | Towards Data Science](/complete-implementation-of-a-mini-vgg-network-for-image-recognition-849299480356)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[迷你 VGG 网络图像识别的完整实现 | 作者：Rashida Nasrin Sucky | Towards Data Science](/complete-implementation-of-a-mini-vgg-network-for-image-recognition-849299480356)'
- en: '[How to Define Custom Layer, Activation Function, and Loss Function in TensorFlow
    | by Rashida Nasrin Sucky | Towards Data Science](/how-to-define-custom-layer-activation-function-and-loss-function-in-tensorflow-bdd7e78eb67)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[如何在 TensorFlow 中定义自定义层、激活函数和损失函数 | 作者：Rashida Nasrin Sucky | Towards Data
    Science](/how-to-define-custom-layer-activation-function-and-loss-function-in-tensorflow-bdd7e78eb67)'
- en: '[A Step-by-Step Tutorial to Develop a Multi-Output Model in TensorFlow | by
    Rashida Nasrin Sucky | Towards Data Science](/a-step-by-step-tutorial-to-develop-a-multi-output-model-in-tensorflow-ec9f13e5979c)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[在 TensorFlow 中开发多输出模型的逐步教程 | 作者：拉希达·纳斯林·苏基 | 数据科学前沿](/a-step-by-step-tutorial-to-develop-a-multi-output-model-in-tensorflow-ec9f13e5979c)'
- en: '[Easy Method of Edge Detection in OpenCV Python | by Rashida Nasrin Sucky |
    Towards Data Science](/easy-method-of-edge-detection-in-opencv-python-db26972deb2d)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenCV Python 中的简单边缘检测方法 | 作者：拉希达·纳斯林·苏基 | 数据科学前沿](/easy-method-of-edge-detection-in-opencv-python-db26972deb2d)'
