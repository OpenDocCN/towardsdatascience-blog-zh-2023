- en: 'Teaching is Hard: How to Train Small Models and Outperforming Large Counterparts'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 教学很难：如何训练小模型并超越大型对手
- en: 原文：[https://towardsdatascience.com/teaching-is-hard-how-to-train-small-models-and-outperforming-large-counterparts-f131f9d463e1](https://towardsdatascience.com/teaching-is-hard-how-to-train-small-models-and-outperforming-large-counterparts-f131f9d463e1)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/teaching-is-hard-how-to-train-small-models-and-outperforming-large-counterparts-f131f9d463e1](https://towardsdatascience.com/teaching-is-hard-how-to-train-small-models-and-outperforming-large-counterparts-f131f9d463e1)
- en: '|MODEL DISTILLATION|AI|LARGE LANGUAGE MODELS|'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '|模型蒸馏|人工智能|大型语言模型|'
- en: Distilling the knowledge of a large model is complex but a new method shows
    incredible performances
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蒸馏大型模型的知识是复杂的，但一种新方法显示出惊人的性能
- en: '[](https://salvatore-raieli.medium.com/?source=post_page-----f131f9d463e1--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----f131f9d463e1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f131f9d463e1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f131f9d463e1--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----f131f9d463e1--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://salvatore-raieli.medium.com/?source=post_page-----f131f9d463e1--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----f131f9d463e1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f131f9d463e1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f131f9d463e1--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----f131f9d463e1--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f131f9d463e1--------------------------------)
    ·12 min read·Nov 11, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f131f9d463e1--------------------------------)
    ·阅读时间 12 分钟·2023年11月11日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a16ec7ef78aa98b058f1e94098af933b.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a16ec7ef78aa98b058f1e94098af933b.png)'
- en: Photo by [JESHOOTS.COM](https://unsplash.com/@jeshoots?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [JESHOOTS.COM](https://unsplash.com/@jeshoots?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '[Large language models](https://en.wikipedia.org/wiki/Large_language_model)
    (LLMs) and few-shot learning have shown we can use these models for unseen tasks.
    However, these skills have a cost: a huge number of parameters. This means you
    need also a specialized infrastructure and restrict state-of-the-art LLMs to only
    a few companies and research teams.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[大型语言模型](https://en.wikipedia.org/wiki/Large_language_model)（LLMs）和少样本学习已经证明我们可以将这些模型用于未见过的任务。然而，这些技能是有代价的：大量的参数。这意味着你还需要一个专业化的基础设施，并且将最先进的LLMs限制在只有少数几家公司和研究团队中。'
- en: Do we really need a unique model for each task?
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们真的需要为每个任务设计一个独特的模型吗？
- en: Would it be possible to create specialized models that could replace them for
    specific applications?
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有可能创建专门的模型来替代它们用于特定的应用？
- en: How can we have a small model that competes with giant LLMs for specific applications?
    Do we necessarily need a lot of data?
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何才能拥有一个在特定应用中与大型LLMs竞争的小模型？我们是否确实需要大量的数据？
- en: In this article, I give an answer to these questions.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我对这些问题给出了答案。
- en: “Education is the key to success in life, and teachers make a lasting impact
    in the lives of their students.” –Solomon Ortiz
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “教育是人生成功的关键，教师在学生的生活中留下了深远的影响。” ——所罗门·奥尔蒂斯
- en: Match the champion!
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 匹配冠军！
- en: '![](../Images/b6319bfeb892fa586fd258fd3e5904bd.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b6319bfeb892fa586fd258fd3e5904bd.png)'
- en: Photo by [Fauzan Saari](https://unsplash.com/@fznsr_?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Fauzan Saari](https://unsplash.com/@fznsr_?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: The art of teaching is the art of assisting discovery. — Mark Van Doren
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 教学的艺术是协助发现的艺术。——马克·范·多伦
- en: '[Large language models (LLMs)](https://en.wikipedia.org/wiki/Large_language_model)
    have shown revolutionary capabilities. For example, researchers have been surprised
    by elusive behavior such as [in-context learning](https://ai.stanford.edu/blog/understanding-incontext/).
    This has led to an increase in the scale of models, with larger and larger models
    [searching for new capabilities](https://openai.com/research/scaling-laws-for-neural-language-models)
    that appear beyond a number of parameters.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[大型语言模型（LLMs）](https://en.wikipedia.org/wiki/Large_language_model)展现了革命性的能力。例如，研究人员对像[上下文学习](https://ai.stanford.edu/blog/understanding-incontext/)这样的难以捉摸的行为感到惊讶。这导致模型规模的增加，越来越大的模型[寻找新能力](https://openai.com/research/scaling-laws-for-neural-language-models)，这些能力超出了参数的数量。'
- en: '[](/all-you-need-to-know-about-in-context-learning-55bde1180610?source=post_page-----f131f9d463e1--------------------------------)
    [## All You Need to Know about In-Context Learning'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/all-you-need-to-know-about-in-context-learning-55bde1180610?source=post_page-----f131f9d463e1--------------------------------)
    [## 关于上下文学习的一切'
- en: What is and how does it work what makes Large Language Models so powerful
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是大型语言模型，它是如何工作的，以及是什么使大型语言模型如此强大
- en: 'towardsdatascience.com](/all-you-need-to-know-about-in-context-learning-55bde1180610?source=post_page-----f131f9d463e1--------------------------------)
    [](/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9?source=post_page-----f131f9d463e1--------------------------------)
    [## Emergent Abilities in AI: Are We Chasing a Myth?'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/all-you-need-to-know-about-in-context-learning-55bde1180610?source=post_page-----f131f9d463e1--------------------------------)
    [](/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9?source=post_page-----f131f9d463e1--------------------------------)
    [## 人工智能中的涌现能力：我们是否在追逐一个神话？'
- en: Changing Perspective on Large Language Models emerging properties
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对大型语言模型出现特性的观点变化
- en: towardsdatascience.com](/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9?source=post_page-----f131f9d463e1--------------------------------)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9?source=post_page-----f131f9d463e1--------------------------------)'
- en: This comes at a cost, however; models such as [GPT-3](https://en.wikipedia.org/wiki/GPT-3)
    (more than 175 trillion parameters) require at least 350 GB GPU for running. This
    means you need specialized infrastructure not only to train but also to use it
    in [inference](https://cloud.google.com/bigquery/docs/inference-overview). Deploying
    such a model to make it publicly accessible requires significant challenges and
    costs (especially if you want to reduce latency). **Thus, only a few companies
    can afford to deploy models of a certain size for real-world applications.**
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 但这会有代价；例如，[GPT-3](https://en.wikipedia.org/wiki/GPT-3)（超过175万亿个参数）至少需要350 GB的GPU来运行。这意味着你需要专门的基础设施来训练和使用它进行[推理](https://cloud.google.com/bigquery/docs/inference-overview)。将这样的模型部署以使其公开访问需要克服重大挑战和成本（尤其是如果你想减少延迟）。**因此，只有少数公司能够负担得起为实际应用部署一定规模的模型。**
- en: Models that have more than 100 B parameters have large modeling capabilities
    that however are spread over many skills. In contrast, models with less than 10
    B have reduced modeling ability but one can concentrate this ability on a single
    task. For example, reasoning is one of the abilities shown by models over 100
    B parameters but is absent in small models. [The authors of this study](https://arxiv.org/abs/2301.12726)
    show that reasoning is only one of many capabilities in a large LLM. Therefore,
    focusing the training of a small model on reasoning can yield appreciable results
    even with a model smaller than 100 B.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有超过100 B参数的模型具有大型建模能力，但这些能力分散在许多技能上。相比之下，少于10 B的模型建模能力较弱，但可以将这种能力集中于单一任务。例如，推理是超过100
    B参数模型展示的一种能力，但在小型模型中缺失。[这项研究的作者](https://arxiv.org/abs/2301.12726)表明，推理只是大型LLM中的众多能力之一。因此，将小型模型的训练重点放在推理上，即使模型小于100
    B，也可以获得显著的结果。
- en: 'Of course, specializing in a small model comes at a price: performance on other
    tasks. But often you are interested only in one task, so you can use a small model.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，专注于小型模型会有代价：对其他任务的表现。但通常你只对一个任务感兴趣，因此可以使用小型模型。
- en: '![](../Images/2511991c35ab392135d6736ea8a5e38a.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2511991c35ab392135d6736ea8a5e38a.png)'
- en: 'previous works suggested that reasoning abruptly appears with the scale (left
    plot). The authors in this study show that by focusing a model on a reasoning
    task (specialization) you can achieve good results in reasoning. image source:
    [here](https://arxiv.org/abs/2301.12726)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以前的研究表明，推理能力随着规模的增加而突然出现（左侧图）。这项研究的作者表明，通过专注于推理任务（专业化），你可以在推理方面取得良好的结果。图片来源：[这里](https://arxiv.org/abs/2301.12726)
- en: Therefore, several companies have focused on small models that show acceptable
    performance only for particular tasks. In addition, the use of [**fine-tuning**](https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning))
    has made it possible to create small, specialized models for a specific application.
    For some tasks such as classification, fine-tuning requires an annotated dataset
    of elements. Collecting these annotated datasets is expensive, so another technique
    used is [distillation](https://en.wikipedia.org/wiki/Knowledge_distillation).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，几家公司专注于仅对特定任务表现良好的小模型。此外，[**微调**](https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning))的使用使得为特定应用创建小型专业模型成为可能。对于一些任务，如分类，微调需要一个带注释的数据集。收集这些带注释的数据集是昂贵的，因此使用的另一种技术是[蒸馏](https://en.wikipedia.org/wiki/Knowledge_distillation)。
- en: '**Distillation** is a technique in which you train a small model using labels
    that are generated from a larger model. Collecting these unlabeled datasets can
    be equally expensive (for example, in the medical domain). The higher the performance
    must be, the higher these costs are. So achieving the same performance as an LLM
    with fine-tuning or distillation can be computationally expensive.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**蒸馏**是一种技术，通过它你可以利用从更大模型生成的标签来训练一个小模型。收集这些未标记的数据集可能同样昂贵（例如，在医疗领域）。性能要求越高，成本也就越高。因此，使用微调或蒸馏来实现与大型语言模型（LLM）相同的性能可能在计算上是昂贵的。'
- en: Thus, how can we make a small model capable of learning from an LLM in a data
    and time efficient manner?
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 因此，我们如何才能使小模型以数据和时间高效的方式从LLM中学习呢？
- en: How to make an LLM an efficient teacher
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何让LLM成为高效的教师
- en: '![](../Images/45bb89ea853b0b0f6619552f333d2df3.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/45bb89ea853b0b0f6619552f333d2df3.png)'
- en: Photo by [ThisisEngineering RAEng](https://unsplash.com/@thisisengineering?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [ThisisEngineering RAEng](https://unsplash.com/@thisisengineering?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: I cannot teach anybody anything; I can only make them think. – Socrates
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我不能教任何人任何东西；我只能让他们思考。——苏格拉底
- en: When we want to train a small model, LLMs are used either to generate labels
    for unlabeled text or for [data augmentation](https://en.wikipedia.org/wiki/Data_augmentation)
    (taking a dataset of examples generated by an LLM). Intuitively, this may not
    be enough to make model learning efficient.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想训练一个小模型时，LLM 要么用来为未标记的文本生成标签，要么用于[数据增强](https://en.wikipedia.org/wiki/Data_augmentation)（从LLM生成的示例数据集中提取）。直观上，这可能不足以使模型学习高效。
- en: For example, if I want my small model to learn how to rank tweets (positively,
    negatively, or neutrally) I can download a large number of tweets, generate the
    labels with an LLM, and train the small model with the provided labels.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我想让我的小模型学习如何对推文进行排序（积极、消极或中立），我可以下载大量推文，通过LLM生成标签，然后用这些标签训练小模型。
- en: '![](../Images/9b2f5ce4a0dbf71ce5fdbfc0b6dc6ec6.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b2f5ce4a0dbf71ce5fdbfc0b6dc6ec6.png)'
- en: schematic representation of distillation. image by the author
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 蒸馏的示意图。图片由作者提供
- en: '**However, while this can work for a simple task such as tweet classification
    it is not enough for more complex tasks.** We may download riddles from the internet
    and ask an LLM to solve them, but the solution itself does not provide us with
    any information on the solving process. A small model trained with the solutions
    would not learn how to solve a riddle.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**然而，虽然这对于像推文分类这样的简单任务有效，但对于更复杂的任务来说是不够的。** 我们可能会从互联网上下载谜题并让LLM解决它们，但解决方案本身并未提供关于解决过程的任何信息。一个通过解决方案训练的小模型不会学会如何解谜。'
- en: Indeed, to learn how to solve difficult tasks (such as solving a riddle) you
    need more information than just the solution.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，要学会解决困难的任务（例如解谜），你需要比仅仅解决方案更多的信息。
- en: Actually, this is also true for LLMs, for reasoning tasks (arithmetic, commonsense,
    and symbolic reasoning) providing context with [chain-of-thought](https://www.promptingguide.ai/techniques/cot)
    helps the model arrive at the solution without hallucinating.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这对于LLM也是如此，对于推理任务（算术、常识和符号推理），提供[链式思维](https://www.promptingguide.ai/techniques/cot)的上下文有助于模型得出解决方案而不会产生幻觉。
- en: '![](../Images/f304565b4553c321fc000579a7519ba6.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f304565b4553c321fc000579a7519ba6.png)'
- en: 'image source: [here](https://arxiv.org/abs/2201.11903)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2201.11903)
- en: Building on this intention, some [Google researchers](https://blog.research.google/2023/09/distilling-step-by-step-outperforming.html)
    have gone so far as to train small models with capabilities exceeding LLMs in
    specific tasks (770M parameters with 540B [PaLM](https://ai.google/discover/palm2/)).
    They then described this approach in a recently published paper.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这一意图，一些[谷歌研究人员](https://blog.research.google/2023/09/distilling-step-by-step-outperforming.html)甚至训练了在特定任务上超越LLM的小模型（770M参数与540B[PaLM](https://ai.google/discover/palm2/)）。他们随后在最近发表的一篇论文中描述了这种方法。
- en: '[](https://arxiv.org/abs/2305.02301?source=post_page-----f131f9d463e1--------------------------------)
    [## Distilling Step-by-Step! Outperforming Larger Language Models with Less Training
    Data and Smaller…'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://arxiv.org/abs/2305.02301?source=post_page-----f131f9d463e1--------------------------------)
    [## 逐步提炼！用更少的训练数据和更小的模型超越大型语言模型…](https://arxiv.org/abs/2305.02301?source=post_page-----f131f9d463e1--------------------------------)'
- en: Deploying large language models (LLMs) is challenging because they are memory
    inefficient and compute-intensive for…
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署大型语言模型（LLMs）具有挑战性，因为它们在内存使用上效率低下，并且计算密集型……
- en: arxiv.org](https://arxiv.org/abs/2305.02301?source=post_page-----f131f9d463e1--------------------------------)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[arxiv.org](https://arxiv.org/abs/2305.02301?source=post_page-----f131f9d463e1--------------------------------)'
- en: In short, the authors exploited the ability of an LLM to reason (beyond simply
    generating labels). **Taking an unlabeled dataset, they asked the LLM to generate
    the correct labels and rationale** (natural language explanations of why that
    is the most appropriate label for the question). After that, they used both the
    label and the rationale to train small models.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，作者利用了LLM进行推理的能力（超越单纯生成标签）。**通过使用一个未标记的数据集，他们要求LLM生成正确的标签和推理**（为什么这是最合适的标签的自然语言解释）。之后，他们使用标签和推理来训练小模型。
- en: '![](../Images/260f62b7e824bd0a1843e11651594841.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/260f62b7e824bd0a1843e11651594841.png)'
- en: schematic representation of the approach. image by the author
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法的示意图。图像由作者提供
- en: '**In this way, they provided the small model with not only the problem solution
    but also how the teacher (the LLMs) arrived at the solution.** Moreover, the rationale
    contains not only an explanation but also useful elements for understanding the
    task (elements that are not easy to infer from simple input, especially for a
    model with a limited number of parameters).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**通过这种方式，他们不仅向小模型提供了问题的解决方案，还提供了老师（LLMs）如何得出该解决方案的过程。** 此外，推理不仅包含解释，还包含理解任务的有用元素（这些元素从简单的输入中不易推断出，特别是对于参数有限的模型）。'
- en: '![](../Images/1705884062947f3167cb0b1628621e0e.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1705884062947f3167cb0b1628621e0e.png)'
- en: 'image source: [here](https://arxiv.org/abs/2305.02301)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2305.02301)
- en: Distilling step-by-step
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逐步提炼
- en: Going into more detail, the authors used the same prompts that had been used
    for [Chain-of-Thought (CoT)](https://arxiv.org/abs/2201.11903). A prompt that
    consists of a question, the context or rationale, and the answer to the question.
    After that, the rationale is appended to the question and the model must give
    the answer.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 更详细地说，作者使用了与[链式思维（CoT）](https://arxiv.org/abs/2201.11903)相同的提示。一个提示包括一个问题、背景或推理，以及问题的答案。之后，将推理附加到问题上，模型必须给出答案。
- en: '![](../Images/8d47874aced51dc7f61a3787faf8ecd2.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8d47874aced51dc7f61a3787faf8ecd2.png)'
- en: 'image source: [here](https://arxiv.org/abs/2305.02301)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2305.02301)
- en: 'The small model is trained with a **simple multitask approach**: it must predict
    the correct label and also generate the corresponding rationale. The [loss function](https://en.wikipedia.org/wiki/Loss_function)
    also takes into account whether there is an error in generating the rationale.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 小模型通过**简单的多任务方法**进行训练：它不仅需要预测正确的标签，还需要生成相应的推理。[损失函数](https://en.wikipedia.org/wiki/Loss_function)也会考虑生成推理时是否出现错误。
- en: '![](../Images/22a15086a1ef2fee84b316f0fe4f05d0.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22a15086a1ef2fee84b316f0fe4f05d0.png)'
- en: In this way, the authors force the model to generate intermediate reasoning
    steps as well, guiding the model to the correct answer.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，作者迫使模型生成中间推理步骤，从而引导模型找到正确的答案。
- en: Metaphorically, it is like a teacher forcing the student to write down all the
    reasoning steps instead of providing the answer directly. The advantage of this
    approach is that at test time the model will no longer need the teacher model
    (LLM) but should have learned to reason.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 从比喻的角度来看，这就像是一个老师强迫学生写下所有的推理步骤，而不是直接给出答案。这种方法的优点是，在测试时，模型将不再需要老师模型（LLM），而应该学会进行推理。
- en: Can we teach reasoning to a student?
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们能否教会学生推理？
- en: '![](../Images/d9501579775396f5c29ba346354f5818.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d9501579775396f5c29ba346354f5818.png)'
- en: Photo by [Element5 Digital](https://unsplash.com/@element5digital?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Element5 Digital](https://unsplash.com/@element5digital?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Tell me and I forget. Teach me and I remember. Involve me and I learn. – Benjamin
    Franklin
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你告诉我，我会忘记。你教我，我会记住。你让我参与，我会学习。 – 本杰明·富兰克林
- en: The authors used PaLM (540 B parameters) as the LLM to generate the rationales.
    They chose to use [T5](https://arxiv.org/abs/1910.10683) as a small model, using
    available pre-trained weights checkpoints. Interestingly, the authors use a very
    small model that has already been trained. **In this way, they use a model that
    already has a general knowledge of the language but can be adapted to a specific
    task.**
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用PaLM (540 B参数)作为LLM生成理由。他们选择使用[T5](https://arxiv.org/abs/1910.10683)作为小模型，使用现有的预训练权重检查点。有趣的是，作者使用了一个已经训练过的非常小的模型。**通过这种方式，他们使用一个已经具备一般语言知识的模型，但可以适应特定任务。**
- en: '![](../Images/2bb515c5763c2f132286a68a6666a01c.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2bb515c5763c2f132286a68a6666a01c.png)'
- en: Model comparison to better understand the difference in size (circles are proportional).
    Image by the author, script to generate the image can be found [here](https://github.com/SalvatoreRa/tutorial/blob/main/other/Code_for_Teaching_is%C2%A0Hard.ipynb)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 模型比较，以更好地理解大小差异（圆圈按比例）。图片由作者提供，生成图片的脚本可以在[这里](https://github.com/SalvatoreRa/tutorial/blob/main/other/Code_for_Teaching_is%C2%A0Hard.ipynb)找到
- en: 'They chose three particular natural language processing tasks:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 他们选择了三个特定的自然语言处理任务：
- en: '[**Natural language inference**](https://nlpprogress.com/english/natural_language_inference.html)**.**
    they used two different datasets: e-SNLI and ANLI.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**自然语言推理**](https://nlpprogress.com/english/natural_language_inference.html)**。**
    他们使用了两个不同的数据集：e-SNLI 和 ANLI。'
- en: '[**Commonsense question answering**](https://arxiv.org/abs/1811.00937) **(CQA).**'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**常识问答**](https://arxiv.org/abs/1811.00937) **(CQA)。**'
- en: '**Arithmetic math word problems (SVAMP).**'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**算术数学应用题 (SVAMP)。**'
- en: As can be seen, these are tasks and datasets that require the model to show
    reasoning capabilities
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如所示，这些任务和数据集要求模型展示推理能力。
- en: 'In the article, the approach is compared with the two classical approaches:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在文章中，该方法与两种经典方法进行了比较：
- en: '[**fine-tuning**](https://d2l.ai/chapter_computer-vision/fine-tuning.html)**.**
    where the pre-trained model is trained on annotated examples with the correct
    labels.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**微调**](https://d2l.ai/chapter_computer-vision/fine-tuning.html)**。** 其中预训练模型在带有正确标签的注释示例上进行训练。'
- en: '[**Distillation**](https://neptune.ai/blog/knowledge-distillation)**.** where
    the LLM is used to generate the ground-truth labels'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**蒸馏**](https://neptune.ai/blog/knowledge-distillation)**。** 在该方法中，LLM用于生成真实标签。'
- en: The results show that the new approach (*Distilling step-by-step*) outperforms
    standard finetuning in all benchmark datasets and tasks analyzed but also requires
    far fewer examples to achieve better performance. Thus, the approach performs
    better but it is also cheaper (with only 12.5 percent of examples outperforming
    classical finetuning).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，新的方法（*逐步提炼*）在所有基准数据集和任务中都优于标准微调，同时所需示例也远少于达到更好表现的标准。因此，这种方法性能更佳，同时成本更低（仅有12.5%的示例表现超过传统微调）。
- en: '![](../Images/89f5d43ffd244f5e602ddf7126606701.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89f5d43ffd244f5e602ddf7126606701.png)'
- en: 'image source: [here](https://arxiv.org/abs/2305.02301)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2305.02301)
- en: And the same is true for standard distillation, the new approach is both more
    performant and requires many fewer examples.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对于标准蒸馏而言，同样的新方法在性能上更优，并且所需的示例数量也少得多。
- en: '![](../Images/93ab3c85614ed66d327f10649bce775b.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93ab3c85614ed66d327f10649bce775b.png)'
- en: 'image source: [here](https://arxiv.org/abs/2305.02301)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2305.02301)
- en: The authors then used the same approach with different versions of the model
    (220M, 770M, 11B) and compared it with the LLM baseline (PaLM). The result shows
    that the new approach improves performance according to scale (larger models perform
    better). In addition, step-by-step distilling for some tasks seems to outperform
    even the LLM baseline. In other words, a 770 M model manages in ANLI to outperform
    a model 700 times larger. Even more impressive is that in e-SNLI a 220M model
    outperforms a 2000 times larger model.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们随后使用不同版本的模型（220M、770M、11B）采用相同的方法，并与 LLM 基线（PaLM）进行比较。结果表明，新方法根据规模提高了性能（更大的模型表现更好）。此外，逐步蒸馏在某些任务上似乎甚至超越了
    LLM 基线。换句话说，770M 模型在 ANLI 中超越了一个大 700 倍的模型。更令人印象深刻的是，在 e-SNLI 中，一个 220M 的模型超越了一个大
    2000 倍的模型。
- en: '![](../Images/c910902ea5c7d937496b0624ed16a6cb.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c910902ea5c7d937496b0624ed16a6cb.png)'
- en: 'image source: [here](https://arxiv.org/abs/2305.02301)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '图片来源: [这里](https://arxiv.org/abs/2305.02301)'
- en: In standard fine-tuning, we use human label annotated, while in distillation
    we use an unlabeled setting. Again the results are similar, showing that the model
    can learn even from data that are annotated by an LLM.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准微调中，我们使用人工标注的数据，而在蒸馏中，我们使用未标注的数据。结果类似，显示模型即使从 LLM 标注的数据中也能学习。
- en: '![](../Images/543183a4dd5a3cf5b84665b2b0b95845.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/543183a4dd5a3cf5b84665b2b0b95845.png)'
- en: 'image source: [here](https://arxiv.org/abs/2305.02301)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '图片来源: [这里](https://arxiv.org/abs/2305.02301)'
- en: '**These results in themselves are impressive, but it is incredible that you
    do not need the entire dataset.** Even with only 0.1 % of the dataset, the approach
    is effective. This is not the case for standard fine-tuning and task distillation
    where to see appreciable performance you need many more examples. In ANLI, for
    T5-770M 80% of the examples are enough to outperform PaLM 540B. Even with the
    full dataset, standard fine-tuning fails to reach the LLM baseline'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**这些结果本身已经很令人印象深刻，但令人难以置信的是你不需要整个数据集。** 即使仅用 0.1% 的数据集，该方法仍然有效。对于标准的微调和任务蒸馏，您需要更多的示例才能看到显著的性能。在
    ANLI 中，对于 T5-770M，80% 的示例足以超越 PaLM 540B。即使使用完整的数据集，标准微调也无法达到 LLM 基线。'
- en: '![](../Images/160a8530e0ac01d2159ae2238c7b235b.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/160a8530e0ac01d2159ae2238c7b235b.png)'
- en: 'image source: [here](https://arxiv.org/abs/2305.02301)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '图片来源: [这里](https://arxiv.org/abs/2305.02301)'
- en: '![](../Images/a51fd497ee7c1462d2a33983a1345dfc.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a51fd497ee7c1462d2a33983a1345dfc.png)'
- en: 'image source: [here](https://arxiv.org/abs/2305.02301)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '图片来源: [这里](https://arxiv.org/abs/2305.02301)'
- en: As the authors note, although the approach works with other models (such as
    the 20B GPT-NeoX model) the results are inferior. This is because a PaLM provides
    higher quality and more detailed rationales.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 正如作者所提到的，尽管这种方法也适用于其他模型（如 20B GPT-NeoX 模型），但结果不如预期。这是因为 PaLM 提供了更高质量和更详细的推理。
- en: '![](../Images/e5999ff2b5c293d03d099e888ff1f58a.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e5999ff2b5c293d03d099e888ff1f58a.png)'
- en: 'image source: [here](https://arxiv.org/abs/2305.02301)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '图片来源: [这里](https://arxiv.org/abs/2305.02301)'
- en: In an ablation study, they noted that multi-task training works better. In other
    words, asking the model to generate the rationale helps its learning.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个消融研究中，他们注意到多任务训练效果更好。换句话说，让模型生成推理有助于它的学习。
- en: '![](../Images/4c69bf30d74f4a09174cecd2ecdc626e.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c69bf30d74f4a09174cecd2ecdc626e.png)'
- en: 'image source: [here](https://arxiv.org/abs/2305.02301)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '图片来源: [这里](https://arxiv.org/abs/2305.02301)'
- en: 'The authors released also the code to be tested by the community:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们也发布了供社区测试的代码：
- en: '[](https://github.com/google-research/distilling-step-by-step?source=post_page-----f131f9d463e1--------------------------------)
    [## GitHub - google-research/distilling-step-by-step'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[## GitHub - google-research/distilling-step-by-step](https://github.com/google-research/distilling-step-by-step?source=post_page-----f131f9d463e1--------------------------------)'
- en: Contribute to google-research/distilling-step-by-step development by creating
    an account on GitHub.
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过在 GitHub 上创建帐户，您可以为 google-research/distilling-step-by-step 的开发做出贡献。
- en: github.com](https://github.com/google-research/distilling-step-by-step?source=post_page-----f131f9d463e1--------------------------------)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[GitHub - google-research/distilling-step-by-step](https://github.com/google-research/distilling-step-by-step?source=post_page-----f131f9d463e1--------------------------------)'
- en: Parting thoughts
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结束语
- en: '![](../Images/21e76ab1b41f50cfdf35f35c010f0b68.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21e76ab1b41f50cfdf35f35c010f0b68.png)'
- en: Photo by [Saif71.com](https://unsplash.com/@saif71?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Saif71.com](https://unsplash.com/@saif71?utm_source=medium&utm_medium=referral)
    提供，来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Teaching is the one profession that creates all other professions. – Unknown
  id: totrans-109
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 教育是创造所有其他职业的唯一职业。 – 无名氏
- en: '**This article shows how an LLM can be used to teach smaller models how to
    solve specific tasks.** Beyond the results, this article shows how even for smaller
    models providing context allows them to arrive at the solution. Thus, the approach
    allows a user to distill a small model with fewer data and outperform large LLMs:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**本文展示了如何利用LLM教导较小的模型解决特定任务。** 超越结果，本文还展示了即使是较小的模型，通过提供上下文也能得出解决方案。因此，这种方法使用户能够用更少的数据提炼出一个小模型，并超越大型LLM：'
- en: '![](../Images/6f31a123753708a494011954a7a8ef64.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f31a123753708a494011954a7a8ef64.png)'
- en: 'schematic representation of the article. Image source: [here](https://arxiv.org/pdf/2305.02301.pdf)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 文章的示意图。图片来源：[这里](https://arxiv.org/pdf/2305.02301.pdf)
- en: The authors show in this paper that models up to 2000 times smaller than an
    LLM, can learn and outperform the model teacher on complex tasks such as reasoning
    tasks. Moreover, compared to classical step-by-step distilling approaches, it
    requires much less data.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在本文中展示了比LLM小2000倍的模型能够学习并在复杂任务（如推理任务）上超越教师模型。此外，与经典的逐步提炼方法相比，它需要的数据要少得多。
- en: In general, there has been a paradigm shift in recent times in model learning
    research, in which attempts are being made to separate memorization and actual
    learning.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，近年来模型学习研究发生了范式转变，试图将记忆与实际学习分开。
- en: '[](https://levelup.gitconnected.com/grokking-learning-is-generalization-and-not-memorization-52c43c9025e4?source=post_page-----f131f9d463e1--------------------------------)
    [## Grokking: Learning Is Generalization and Not Memorization'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://levelup.gitconnected.com/grokking-learning-is-generalization-and-not-memorization-52c43c9025e4?source=post_page-----f131f9d463e1--------------------------------)
    [## 理解：学习是泛化而非记忆'
- en: Understanding how a neural network learns helps us to avoid that the model from
    forgetting what it learns
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解神经网络如何学习可以帮助我们避免模型忘记所学内容。
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/grokking-learning-is-generalization-and-not-memorization-52c43c9025e4?source=post_page-----f131f9d463e1--------------------------------)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[levelup.gitconnected.com](https://levelup.gitconnected.com/grokking-learning-is-generalization-and-not-memorization-52c43c9025e4?source=post_page-----f131f9d463e1--------------------------------)'
- en: Indeed, this article shows that to perform a specific task you do not need a
    large capacity (memorization). You can teach a small model to learn a task providing
    information on how to solve the problem (generalization).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，本文表明，要执行特定任务，你并不需要大容量（记忆）。你可以教导一个小模型通过提供解决问题的信息来学习任务（泛化）。
- en: This work is important because, with little data, a much smaller model can be
    trained to excel on a task. These models can then be deployed much more easily
    with very little cost. In addition, the approach works with any model, so a user
    can use either an open source model (such as LLaMA) or the API to a proprietary
    model (GPT-4 or PaLM) use step-by-step distilling, and create their own specialized
    model.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作很重要，因为用少量数据，可以训练出一个更小的模型在任务上表现出色。这些模型可以以更低的成本更容易地部署。此外，这种方法适用于任何模型，因此用户可以使用开源模型（如LLaMA）或专有模型（GPT-4或PaLM）的API进行逐步提炼，创建自己的专业模型。
- en: This work opens up as many exciting possibilities as inexpensively developing
    specialized models for many applications and with superior performance to giant
    models. These models then can be deployed not only online but also on desktop
    computers or in cell phone applications. Thus, having a small but proprietary
    dataset you can develop and deploy expert models with limited resources.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作开辟了许多令人兴奋的可能性，如以低成本开发适用于多个应用的专业模型，并且其性能优于巨型模型。这些模型不仅可以在线部署，还可以在桌面计算机或手机应用中使用。因此，拥有一个小而专有的数据集，你可以用有限的资源开发和部署专家模型。
- en: For example, you can imagine a user developing a small model specialized in
    solving riddles. You just need to create the rationale with the LLM, use *distill
    step-by-step* to train your expert model, and then you could even deploy it on
    a phone app.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可以设想一个用户开发一个专门解决谜题的小模型。你只需与LLM创建推理，使用*逐步提炼*来训练你的专家模型，然后甚至可以将其部署到手机应用上。
- en: TL;DR
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TL;DR
- en: Google unveils a new simple approach for distilling knowledge from a large model.
    Using both rationales and answers you can teach a small model (even 2000 times
    smaller) to outperform LLMs in reasoning tasks.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google公布了一种新的简单方法来从大型模型中提取知识。通过使用推理和答案，你可以教导一个小模型（甚至小2000倍）在推理任务中超越LLM。
- en: The approach outperforms previous state-of-the-art
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种方法超越了之前的最新技术。
- en: This approach requires a small training set and a small model size
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种方法只需要一个小的训练集和较小的模型尺寸
- en: This approach enables the deployment of independent language models for specialized
    tasks. The model size is now compatible with web apps, and inference on device,
    and you do need complex infrastructure.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种方法使得可以为专业任务部署独立的语言模型。现在模型尺寸与网页应用兼容，并且可以在设备上进行推理，无需复杂的基础设施。
- en: What do you think? Let me know in the comments
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你怎么看？在评论中告诉我
- en: 'If you have found this interesting:'
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如果你觉得这很有趣：
- en: '*You can look for my other articles, you can also* [***subscribe***](https://salvatore-raieli.medium.com/subscribe)
    *to get notified when I publish articles, and you can also connect or reach me
    on*[***LinkedIn***](https://www.linkedin.com/in/salvatore-raieli/)***.***'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '*你可以查看我的其他文章，你也可以* [***订阅***](https://salvatore-raieli.medium.com/subscribe)
    *以便在我发布文章时收到通知，也可以在*[***LinkedIn***](https://www.linkedin.com/in/salvatore-raieli/)***上联系或找到我。***'
- en: '*Here is the link to my GitHub repository, where I am planning to collect code
    and many resources related to machine learning, artificial intelligence, and more.*'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '*这是我GitHub仓库的链接，我计划在这里收集与机器学习、人工智能等相关的代码和资源。*'
- en: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----f131f9d463e1--------------------------------)
    [## GitHub - SalvatoreRa/tutorial: Tutorials on machine learning, artificial intelligence,
    data science…'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----f131f9d463e1--------------------------------)
    [## GitHub - SalvatoreRa/tutorial: 机器学习、人工智能、数据科学的教程…'
- en: Tutorials on machine learning, artificial intelligence, data science with math
    explanation and reusable code (in python…
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于机器学习、人工智能、数据科学的教程，包括数学解释和可重复使用的代码（用Python编写…
- en: github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----f131f9d463e1--------------------------------)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----f131f9d463e1--------------------------------)
- en: '*or you may be interested in one of my recent articles:*'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '*或者你可能对我的一篇近期文章感兴趣：*'
- en: '[](/reshaping-the-models-memory-without-the-need-for-retraining-9ade69f56296?source=post_page-----f131f9d463e1--------------------------------)
    [## Reshaping the Model’s Memory without the Need for Retraining'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/reshaping-the-models-memory-without-the-need-for-retraining-9ade69f56296?source=post_page-----f131f9d463e1--------------------------------)
    [## 无需重新训练即可重塑模型的记忆'
- en: Erasing any echo of problematic content a large language model has learned
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 擦除大型语言模型所学到的有问题内容的任何回响
- en: 'towardsdatascience.com](/reshaping-the-models-memory-without-the-need-for-retraining-9ade69f56296?source=post_page-----f131f9d463e1--------------------------------)
    [](https://levelup.gitconnected.com/beyond-words-unraveling-speech-from-brain-waves-with-ai-7ff81862dfff?source=post_page-----f131f9d463e1--------------------------------)
    [## Beyond Words: Unraveling Speech from Brain Waves with AI'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/reshaping-the-models-memory-without-the-need-for-retraining-9ade69f56296?source=post_page-----f131f9d463e1--------------------------------)
    [](https://levelup.gitconnected.com/beyond-words-unraveling-speech-from-brain-waves-with-ai-7ff81862dfff?source=post_page-----f131f9d463e1--------------------------------)
    [## 超越语言：用AI解码脑波中的言语
- en: AI is capable of decoding speech from non-invasive brain recordings
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AI能够从非侵入性脑记录中解码语言
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/beyond-words-unraveling-speech-from-brain-waves-with-ai-7ff81862dfff?source=post_page-----f131f9d463e1--------------------------------)
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: levelup.gitconnected.com](https://levelup.gitconnected.com/beyond-words-unraveling-speech-from-brain-waves-with-ai-7ff81862dfff?source=post_page-----f131f9d463e1--------------------------------)
- en: Reference
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Here is the list of the principal references I consulted to write this article
    (only the first author name of an article is cited).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我在撰写本文时参考的主要文献列表（只引用了每篇文章的第一作者姓名）。
- en: Fu, 2023, Specializing Smaller Language Models towards Multi-Step Reasoning,
    [link](https://arxiv.org/abs/2301.12726)
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 傅, 2023, 《将较小的语言模型专门化为多步骤推理》, [链接](https://arxiv.org/abs/2301.12726)
- en: Hinton, 2015, Distilling the Knowledge in a Neural Network, [link](https://arxiv.org/abs/1503.02531)
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 辛顿, 2015, 《提炼神经网络中的知识》, [链接](https://arxiv.org/abs/1503.02531)
- en: Howard, 2018, Universal Language Model Fine-tuning for Text Classification,
    [link](https://arxiv.org/abs/1801.06146)
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 霍华德, 2018, 《通用语言模型微调用于文本分类》, [链接](https://arxiv.org/abs/1801.06146)
- en: Kaplan, 2020, Scaling Laws for Neural Language Models, [link](https://arxiv.org/abs/2001.08361)
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 卡普兰, 2020, 《神经语言模型的规模定律》, [链接](https://arxiv.org/abs/2001.08361)
- en: Wei, 2022, Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,
    [link](https://arxiv.org/abs/2201.11903)
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 韦, 2022, 《链式思维提示在大型语言模型中引发推理》, [链接](https://arxiv.org/abs/2201.11903)
- en: Hsieh, 2023, Distilling Step-by-Step! Outperforming Larger Language Models with
    Less Training Data and Smaller Model Sizes, [link](https://arxiv.org/abs/2305.02301)
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hsieh, 2023, 逐步提炼！以更少的训练数据和更小的模型尺寸超越更大的语言模型，[链接](https://arxiv.org/abs/2305.02301)
- en: 'Chowdhery, 2022, PaLM: Scaling Language Modeling with Pathways, [link](https://arxiv.org/abs/2204.02311)'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Chowdhery, 2022, PaLM: 通过路径扩展语言建模，[链接](https://arxiv.org/abs/2204.02311)'
- en: Raffel, 2019, Exploring the Limits of Transfer Learning with a Unified Text-to-Text
    Transformer, [link](https://arxiv.org/abs/1910.10683)
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Raffel, 2019, 使用统一的文本到文本转换器探索迁移学习的极限，[链接](https://arxiv.org/abs/1910.10683)
