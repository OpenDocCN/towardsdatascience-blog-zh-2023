- en: Mastering Logistic Regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精通逻辑回归
- en: 原文：[https://towardsdatascience.com/mastering-logistic-regression-3e502686f0ae](https://towardsdatascience.com/mastering-logistic-regression-3e502686f0ae)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/mastering-logistic-regression-3e502686f0ae](https://towardsdatascience.com/mastering-logistic-regression-3e502686f0ae)
- en: From theory to implementation in Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从理论到 Python 实现
- en: '[](https://medium.com/@roiyeho?source=post_page-----3e502686f0ae--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----3e502686f0ae--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3e502686f0ae--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3e502686f0ae--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----3e502686f0ae--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@roiyeho?source=post_page-----3e502686f0ae--------------------------------)[![Roi
    Yehoshua 博士](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----3e502686f0ae--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3e502686f0ae--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3e502686f0ae--------------------------------)
    [Roi Yehoshua 博士](https://medium.com/@roiyeho?source=post_page-----3e502686f0ae--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3e502686f0ae--------------------------------)
    ·17 min read·May 20, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3e502686f0ae--------------------------------)
    ·17 分钟阅读·2023年5月20日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/054e0f77d6a401cd71794ce2aa09c550.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/054e0f77d6a401cd71794ce2aa09c550.png)'
- en: Image by [Gerd Altmann](https://pixabay.com/users/geralt-9301/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1044090)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1044090)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [Gerd Altmann](https://pixabay.com/users/geralt-9301/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1044090)
    的 [Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1044090)
- en: Logistic regression is one of the most common machine learning algorithms. It
    can be used to predict the probability of an event occurring, such as whether
    an incoming email is spam or not, or whether a tumor is malignant or not, based
    on a given labeled data set.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是最常见的机器学习算法之一。它可以用来预测事件发生的概率，例如预测来邮件是否为垃圾邮件，或者肿瘤是否为恶性肿瘤，基于给定的标记数据集。
- en: Due to its simplicity, logistic regression is often used as a baseline against
    which other, more complex models are evaluated.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其简单性，逻辑回归通常被用作评估其他更复杂模型的基准。
- en: The model has the word “logistic” in its name, since it uses the **logistic
    function** (sigmoid) to convert a linear combination of the input features into
    probabilities.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的名称中包含“逻辑”一词，因为它使用**逻辑函数**（Sigmoid）将输入特征的线性组合转换为概率。
- en: It also has the word “regression” in its name, since its output is a continuous
    value between 0 and 1, although it is typically used as a **binary classifier**
    by choosing a threshold value (usually 0.5) and classifying inputs with probability
    greater than the threshold as the positive class, and those below the threshold
    as the negative class.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 它的名称中也包含“回归”一词，因为它的输出是一个介于 0 和 1 之间的连续值，尽管它通常作为**二分类器**使用，通过选择一个阈值（通常为 0.5），并将概率大于阈值的输入分类为正类，而将低于阈值的输入分类为负类。
- en: In this article we will discuss the logistic regression model in depth, implement
    it from scratch in Python, and then show its implementation in Scikit-Learn.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将深入讨论逻辑回归模型，从头开始在 Python 中实现它，然后展示其在 Scikit-Learn 中的实现。
- en: 'Background: Binary Classification Problems'
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景：二分类问题
- en: 'Recall that in [supervised machine learning](https://medium.com/@roiyeho/introduction-to-supervised-machine-learning-313730eb5aa2)
    problems, we are given a training set of *n* labeled samples: *D* = {(**x**₁,
    *y*₁), (**x**₂, *y*₂), … , (**x***ₙ, yₙ*)}, where **x***ᵢ* is a *m*-dimensional
    vector that contains the **features** of sample *i*, and *yᵢ* represents the **label**
    of that sample. Our goal is to build a model whose predictions are as close as
    possible to the true labels.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下在 [监督机器学习](https://medium.com/@roiyeho/introduction-to-supervised-machine-learning-313730eb5aa2)
    问题中，我们会得到一个包含 *n* 个标记样本的训练集：*D* = {(**x**₁, *y*₁), (**x**₂, *y*₂), … , (**x***ₙ,
    yₙ*)}，其中 **x***ᵢ* 是一个 *m* 维向量，包含样本 *i* 的**特征**，*yᵢ* 代表该样本的**标签**。我们的目标是构建一个预测尽可能接近真实标签的模型。
- en: 'In **classification problems**, the label *yᵢ* can take one of *k* values,
    representing the *k* classes to which the samples belong. More specifically, in
    **binary classification problems**, the label *yᵢ* can assume only two values:
    0 (representing the negative class) and 1 (representing the positive class).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在**分类问题**中，标签 *yᵢ* 可以取 *k* 个值之一，表示样本所属的 *k* 个类别。更具体地说，在**二分类问题**中，标签 *yᵢ* 只能取两个值：0（表示负类）和
    1（表示正类）。
- en: 'In addition, we distinguish between two types of classifiers:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们区分两种类型的分类器：
- en: '**Deterministic classifiers** output a **hard label** for each sample, without
    providing probability estimates for the classes. Examples for such classifiers
    include [perceptrons](https://medium.com/towards-data-science/perceptrons-the-first-neural-network-model-8b3ee4513757),
    [K-nearest neighbors](https://medium.com/@roiyeho/k-nearest-neighbors-knn-a-comprehensive-guide-7add717806ad),
    and SVMs.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**确定性分类器**为每个样本输出一个**硬标签**，而不提供类别的概率估计。这类分类器的例子包括[感知机](https://medium.com/towards-data-science/perceptrons-the-first-neural-network-model-8b3ee4513757)、[K-近邻](https://medium.com/@roiyeho/k-nearest-neighbors-knn-a-comprehensive-guide-7add717806ad)和支持向量机（SVM）。'
- en: '**Probabilistic classifiers** output probability estimates for the classes,
    and then assign a label to the given sample based on these probabilities (typically
    the label of the class with the highest probability). Examples for such classifiers
    include logistic regression, naïve Bayes classifiers, and [neural networks](https://medium.com/towards-data-science/multi-layer-perceptrons-8d76972afa2b)
    that use sigmoid or softmax in the output layer.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**概率分类器**输出类别的概率估计，然后根据这些概率为给定样本分配标签（通常是具有最高概率的类别标签）。这类分类器的例子包括逻辑回归、朴素贝叶斯分类器和使用
    sigmoid 或 softmax 作为输出层的[神经网络](https://medium.com/towards-data-science/multi-layer-perceptrons-8d76972afa2b)。'
- en: The Logistic Regression Model
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归模型
- en: 'Logistic regression is a probabilistic classifier that handles binary classification
    problems. Given a sample (**x**, *y*), it outputs a probability *p* that the sample
    belongs to the positive class:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是一种处理二分类问题的概率分类器。给定一个样本 (**x**, *y*)，它输出样本属于正类的概率 *p*：
- en: '![](../Images/d45b6f548b6b589d1f8c9f1dfa22a0cf.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d45b6f548b6b589d1f8c9f1dfa22a0cf.png)'
- en: If this probability is higher than some threshold value (typically chosen as
    0.5), then the sample is classified as 1, otherwise it is classified as 0.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个概率高于某个阈值（通常选择为 0.5），则样本被分类为 1，否则被分类为 0。
- en: How does the model estimate the probability *p*?
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 模型如何估计概率*p*？
- en: The basic assumption in logistic regression is that the **log-odds of the event
    that the sample belongs to the positive class is a linear combination of its features**.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归的基本假设是**样本属于正类的事件的对数赔率是其特征的线性组合**。
- en: '**Log-odds** (also called **logit**) is the logarithm of the **odds ratio**,
    which is the ratio between the probability that the sample belongs to the positive
    class and the probability that it belongs to the negative class:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**对数赔率**（也称为**logit**）是**赔率比**的对数，赔率比是样本属于正类的概率与样本属于负类的概率之间的比率：'
- en: '![](../Images/4b2d5596a24be64140c99eb6ac1765e9.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b2d5596a24be64140c99eb6ac1765e9.png)'
- en: The log-odds (logit) function
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对数赔率（logit）函数
- en: We assume here that the base of the logarithm is *e* (i.e., natural logarithm),
    although other bases can be used as well.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里假设对数的底数是 *e*（即自然对数），尽管也可以使用其他底数。
- en: 'The graph of the logit function is shown below:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: logit 函数的图像如下所示：
- en: '![](../Images/ff6827dbd135cac0ec6872fce8bcb001.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff6827dbd135cac0ec6872fce8bcb001.png)'
- en: The logit function
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: logit 函数
- en: As can be seen, the logit function maps probability values in (0, 1) into real
    numbers in (-∞, +∞).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，logit 函数将 (0, 1) 区间的概率值映射到 (-∞, +∞) 区间的实数值。
- en: In logistic regression, we assume that the log odds is a linear combination
    of the features, i.e.,
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在逻辑回归中，我们假设对数赔率是特征的线性组合，即：
- en: '![](../Images/6e62fbedc12d1be120fa4777b42f1994.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6e62fbedc12d1be120fa4777b42f1994.png)'
- en: where **w** = (*w*₀, …, *wₘ*) are the **parameters** (or weights) of the model.
    The parameter *w*₀ is often called the **intercept** (or bias).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 **w** = (*w*₀, …, *wₘ*) 是模型的**参数**（或权重）。参数 *w*₀ 通常被称为**截距**（或偏置）。
- en: 'The points for which *p* = 0.5 (i.e., the log odds is equal to 0) define the
    separating hyperplane between the two classes, whose equation is:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *p* = 0.5（即对数赔率等于 0）的点定义了两个类别之间的分隔超平面，其方程为：
- en: '![](../Images/74c8e107254ec62915b2af1181a5a591.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/74c8e107254ec62915b2af1181a5a591.png)'
- en: The equation of the separating hyperplane
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 分隔超平面的方程
- en: 'The weight vector **w** is orthogonal to this hyperplane. Every example above
    the hyperplane (**w***ᵗ***x** > 0**)** is classified as a positive example, whereas
    every example below the hyperplane (**w***ᵗ***x** < 0**)** is classified as a
    negative example:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 权重向量 **w** 与此超平面正交。超平面上方的每个样本 (**w***ᵗ***x** > 0**)** 被分类为正样本，而超平面下方的每个样本 (**w***ᵗ***x**
    < 0**)** 被分类为负样本：
- en: '![](../Images/e991e170dd4d8e1a5f2f0cf77f34ed67.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e991e170dd4d8e1a5f2f0cf77f34ed67.png)'
- en: This makes logistic regression a **linear classifier**, since it assumes that
    the boundary between the classes is a linear surface. Other linear classifiers
    include [perceptrons](https://medium.com/towards-data-science/perceptrons-the-first-neural-network-model-8b3ee4513757)
    and SVMs.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得逻辑回归成为一种**线性分类器**，因为它假设类别之间的边界是一个线性表面。其他线性分类器包括[感知器](https://medium.com/towards-data-science/perceptrons-the-first-neural-network-model-8b3ee4513757)和支持向量机（SVM）。
- en: 'We can find a direct correlation between *p* and the parameters **w**, by exponentiating
    both sides of the log-odds equation:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过对对数几率方程两边取指数，找到 *p* 和参数 **w** 之间的直接关联：
- en: '![](../Images/8abd4f8da7b78d14e45f5650c5af6124.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8abd4f8da7b78d14e45f5650c5af6124.png)'
- en: 'where *σ* is the **sigmoid function** (also known as the **logistic function**):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *σ* 是**Sigmoid 函数**（也称为**逻辑函数**）：
- en: '![](../Images/89eae5c5e6e3b27204838eb5b3481c13.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89eae5c5e6e3b27204838eb5b3481c13.png)'
- en: 'The sigmoid function is used to convert the log-odds (**w***ᵗ***x**) into probabilities.
    It has a characteristic “S” or shaped curve:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 函数用于将对数几率 (**w***ᵗ***x**) 转换为概率。它具有一个特征“ S”形曲线：
- en: '![](../Images/dd1a3ef14df25d51c82d947e9a36b49d.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd1a3ef14df25d51c82d947e9a36b49d.png)'
- en: The sigmoid function
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 函数
- en: As can be seen, the function maps real numbers in (-∞, +∞) into probability
    values in (0, 1).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如可以看出，该函数将实数范围 (-∞, +∞) 映射为概率值范围 (0, 1)。
- en: 'The sigmoid function has some nice mathematical properties that will be useful
    later:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 函数具有一些优良的数学性质，这些性质在后面会很有用：
- en: '![](../Images/15b985b012b1fa23f822043d0c1c5d57.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15b985b012b1fa23f822043d0c1c5d57.png)'
- en: 'The following diagram summarizes the computational process of logistic regression
    starting from the inputs until the final prediction:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 下图总结了从输入到最终预测的逻辑回归计算过程：
- en: '![](../Images/72f9ffae8042d4954afdd5fed718c560.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/72f9ffae8042d4954afdd5fed718c560.png)'
- en: The logistic regression model
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归模型
- en: Log Loss
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对数损失
- en: Our goal is to find the parameters **w** that will make the model’s predictions
    *p* = *σ*(**w***ᵗ***x**) as close as possible to the true labels *y*. To that
    end, we need to define a **loss function** that will measure how far our model’s
    predictions are from the true labels. This function needs to be differentiable,
    so it can be optimized using techniques such as gradient descent (for more information
    on loss functions in machine learning see [this article](https://medium.com/towards-data-science/loss-functions-in-machine-learning-9977e810ac02)).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是找到参数**w**，使模型的预测 *p* = *σ*(**w***ᵗ***x**) 尽可能接近真实标签 *y*。为此，我们需要定义一个**损失函数**，用来衡量模型预测与真实标签之间的差距。这个函数需要是可微分的，以便可以使用如梯度下降等技术进行优化（有关机器学习中的损失函数的更多信息，请参见[这篇文章](https://medium.com/towards-data-science/loss-functions-in-machine-learning-9977e810ac02)）。
- en: 'The loss function used by logistic regression is called **log loss** (or **logistic
    loss**). It is defined as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归使用的损失函数称为**对数损失**（或**逻辑损失**）。其定义如下：
- en: '![](../Images/204fff1b75019188c1b25ebf1249e8e7.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/204fff1b75019188c1b25ebf1249e8e7.png)'
- en: The log loss function
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对数损失函数
- en: How did we get to this function? The derivation of this function is based on
    the [maximum likelihood principle](https://medium.com/@roiyeho/maximum-likelihood-855b6df92c43).
    More specifically, we can show that log loss is the negative log likelihood under
    the assumption that the labels have a [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution)
    (i.e., a probability distribution of a binary random variable that takes 1 with
    probability *p* and 0 with probability 1 − *p*).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是如何得到这个函数的？这个函数的推导基于[最大似然原理](https://medium.com/@roiyeho/maximum-likelihood-855b6df92c43)。更具体地说，我们可以证明对数损失是在标签具有[伯努利分布](https://en.wikipedia.org/wiki/Bernoulli_distribution)（即一种二元随机变量的概率分布，其中
    1 的概率为 *p*，0 的概率为 1 − *p*）假设下的负对数似然。
- en: 'Mathematically, we will show that:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，我们将证明：
- en: '![](../Images/615eb356f5860b57e640f5c6bb4f2830.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/615eb356f5860b57e640f5c6bb4f2830.png)'
- en: where *P*(*y*|*p*) is the probability of getting the label *y* given the model’s
    prediction *p* (i.e., the likelihood of the data given our model).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *P*(*y*|*p*) 是在模型预测 *p* 给定的情况下获得标签 *y* 的概率（即，数据在我们的模型下的似然）。
- en: '**Proof:**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**证明**：'
- en: Given a model of the data (the labels) as a Bernoulli distribution with parameter
    *p*, the probability that a sample belongs to the positive class is simply *p*,
    i.e.,
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个具有参数 *p* 的贝努利分布模型（标签），样本属于正类的概率就是 *p*，即，
- en: '![](../Images/0844535fa1311bfd6b66aeb64a22be3e.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0844535fa1311bfd6b66aeb64a22be3e.png)'
- en: 'Similarly, the probability that the sample belongs to the negative class is:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，样本属于负类的概率是：
- en: '![](../Images/23c8ef1dc747aca60c605bbf04954e39.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23c8ef1dc747aca60c605bbf04954e39.png)'
- en: 'We can write these two equations more compactly as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这两个方程更紧凑地写成如下形式：
- en: '![](../Images/c658cf69f1834acc67a0262ac647d9d5.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c658cf69f1834acc67a0262ac647d9d5.png)'
- en: '*Explanation*: when *y* = 1, *pʸ* = *p* and (1 − *p*)¹⁻*ʸ* = 1, therefore *P*(*y*|*p*)
    = *p*. Similarly, when *y* = 0, *pʸ* = 1 and (1 − *p*)¹⁻*ʸ* = 1 − *p*, therefore
    *P*(*y*|*p*) = 1 − *p*.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '*解释*：当 *y* = 1 时，*pʸ* = *p* 和 (1 − *p*)¹⁻*ʸ* = 1，因此 *P*(*y*|*p*) = *p*。类似地，当
    *y* = 0 时，*pʸ* = 1 和 (1 − *p*)¹⁻*ʸ* = 1 − *p*，因此 *P*(*y*|*p*) = 1 − *p*。'
- en: 'Therefore the log likelihood of the data given the model is:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，给定模型的数据的对数似然是：
- en: '![](../Images/cf2cbf2a69b101936b74249690535fb9.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf2cbf2a69b101936b74249690535fb9.png)'
- en: The log loss is exactly the negative of this function. Therefore, maximizing
    the log likelihood is equivalent to minimizing the log loss.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对数损失恰好是该函数的负值。因此，最大化对数似然等同于最小化对数损失。
- en: 'The following plot shows the log loss when *y* = 1:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了当 *y* = 1 时的对数损失：
- en: '![](../Images/8be4e4573f31964eee4dae08c1f55a63.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8be4e4573f31964eee4dae08c1f55a63.png)'
- en: The log loss equals to 0 only in case of a perfect prediction (*p* = 1 and *y*
    = 1, or *p* = 0 and *y* = 0), and approaches infinity as the prediction gets worse
    (i.e., when *y* = 1 and *p* → 0 or *y* = 0 and *p* → 1).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对数损失仅在预测完全准确时（*p* = 1 且 *y* = 1，或 *p* = 0 且 *y* = 0）为0，并且当预测变差时（即，当 *y* = 1
    且 *p* → 0 或 *y* = 0 且 *p* → 1）接近无穷大。
- en: 'The **cost function** calculates the average loss over the whole data set:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**成本函数** 计算整个数据集上的平均损失：'
- en: '![](../Images/51b377c12bd97944e1ff47659d7bf32b.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51b377c12bd97944e1ff47659d7bf32b.png)'
- en: 'This function can be written in a vectorized form as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数可以用向量化的形式表示如下：
- en: '![](../Images/ec54b26bd9532f213ddddc0f5e561e14.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ec54b26bd9532f213ddddc0f5e561e14.png)'
- en: where **y** = (*y*₁, …, *yₙ*) is a vector that contains the labels of all the
    training samples, and **p** = (*p*₁, …, *pₙ*) is a vector that contains the predicted
    probabilities of the model for all the training samples.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 **y** = (*y*₁, …, *yₙ*) 是一个包含所有训练样本标签的向量，而 **p** = (*p*₁, …, *pₙ*) 是一个包含模型对所有训练样本的预测概率的向量。
- en: This cost function is convex, i.e., it has a single global minimum. However,
    there is no closed-form solution for finding the optimal **w*** (due to the non-linearities
    introduced by the log function). Therefore, we need to use iterative optimization
    methods such as gradient descent in order to find the minimum.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这个成本函数是凸的，即，它有一个全局最小值。然而，由于对数函数引入的非线性，没有封闭形式的解决方案来找到最佳 **w***。因此，我们需要使用迭代优化方法如梯度下降来找到最小值。
- en: Gradient Descent
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降
- en: 'Gradient descent is an iterative approach for finding a minimum of a function,
    where we take small steps in the opposite direction of the gradient in order to
    get closer to the minimum:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是一种迭代方法，用于寻找函数的最小值，其中我们沿着梯度的相反方向采取小步，以接近最小值：
- en: '![](../Images/f419ed93df7d803b49270ddeb7f6070b.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f419ed93df7d803b49270ddeb7f6070b.png)'
- en: Gradient descent
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降
- en: 'In order to use gradient descent to find the minimum of the cost *J*(**w**),
    we need to compute its partial derivatives with respect to each one of the weights.
    The partial derivative of *J*(**w**) with respect to a given weight *wⱼ* is:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用梯度下降法找到成本 *J*(**w**)* 的最小值，我们需要计算其相对于每一个权重的偏导数。*J*(**w**)* 对于给定权重 *wⱼ* 的偏导数为：
- en: '![](../Images/551631cc09e61f7ca4198ddf6e64e665.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/551631cc09e61f7ca4198ddf6e64e665.png)'
- en: '**Proof**:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**证明**：'
- en: '![](../Images/21277b342cb8e559ec2d9f9bc2be4ac1.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21277b342cb8e559ec2d9f9bc2be4ac1.png)'
- en: 'Thus, the gradient vector can be written in a vectorized form as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，梯度向量可以用向量化的形式表示如下：
- en: '![](../Images/6ebd8e83e32c3a1a60e778f6a507f6e2.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6ebd8e83e32c3a1a60e778f6a507f6e2.png)'
- en: 'And the gradient descent update rule is:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降更新规则为：
- en: '![](../Images/de25b46c2bab6095aa6077f02fa573a3.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de25b46c2bab6095aa6077f02fa573a3.png)'
- en: where *α* is a learning rate that controls the step size (0 < *α* < 1).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*α*是一个学习率，控制步长（0 < *α* < 1）。
- en: Note that whenever you use gradient descent, you must make sure that your data
    set is **normalized** (otherwise gradient descent may take steps of different
    sizes in different directions, which will make it unstable).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，每当你使用梯度下降时，你必须确保数据集已经**标准化**（否则梯度下降可能会在不同方向上采取不同大小的步伐，这会导致不稳定）。
- en: Implementation in Python
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python 实现
- en: We will now implement the logistic regression model in Python from scratch,
    including the cost function and gradient computation, optimizing the model using
    gradient descent, evaluation of the model, and plotting the final decision boundary.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将从头实现逻辑回归模型，包括成本函数和梯度计算，使用梯度下降优化模型，模型评估以及绘制最终的决策边界。
- en: For the demonstration we will use the [Iris data set](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html)
    (BSD license). The original data set contains 150 samples of Iris flowers that
    belong to one of three species (setosa, versicolor and virginica). We will change
    it into a binary classification problem by using only the first two types of flowers
    (setosa and versicolor). In addition, we will use only the first two features
    of each flower (sepal width and sepal length).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示，我们将使用[Iris 数据集](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html)（BSD
    许可证）。原始数据集包含150个属于三种花卉之一的鸢尾花样本（山鸢尾、变色鸢尾和维吉尼亚鸢尾）。我们将其转化为一个二分类问题，只使用前两种花卉（山鸢尾和变色鸢尾）。此外，我们只使用每朵花的前两个特征（花萼宽度和花萼长度）。
- en: Loading the Data Set
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载数据集
- en: 'Let’s first import the required libraries and fix the random seed in order
    to get reproducible results:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入所需的库并固定随机种子，以获得可重复的结果：
- en: '[PRE0]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we load the data set:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们加载数据集：
- en: '[PRE1]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s plot the data:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制数据：
- en: '[PRE2]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/2d9937474c68acf2acaafacf7bfedfa7.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d9937474c68acf2acaafacf7bfedfa7.png)'
- en: The Iris data set
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 鸢尾花数据集
- en: As can be seen, the data set is linearly separable, therefore logistic regression
    should be able to find the boundary between the two classes.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如可以看到，数据集是线性可分的，因此逻辑回归应该能够找到两个类别之间的边界。
- en: 'Next, we need to add a column of ones to the features matrix *X* in order to
    represent the bias (*w*₀):'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要向特征矩阵*X*中添加一列1，以表示偏置项（*w*₀）：
- en: '[PRE4]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We now split the data set into training and test sets:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将数据集分成训练集和测试集：
- en: '[PRE5]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Model Implementation
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型实现
- en: 'We are now ready to implement the logistic regression model. We start by defining
    a helper function to compute the sigmoid function:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备实现逻辑回归模型。我们从定义一个辅助函数来计算 sigmoid 函数开始：
- en: '[PRE6]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Next, we implement the cost function that returns the cost of a logistic regression
    model with parameters **w** on a given data set (*X*, **y**), and also its gradient
    with respect to **w**.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们实现成本函数，该函数返回给定数据集（*X*，**y**）上具有参数**w**的逻辑回归模型的成本，以及相对于**w**的梯度。
- en: '[PRE7]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note that we are using the vectorized forms of the cost and the gradient functions
    that have been shown previously.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们正在使用之前展示的成本函数和梯度函数的向量化形式。
- en: 'To sanity check this function, let’s compute the cost and gradient of the model
    on some random weight vector:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对这个函数进行合理性检查，我们来计算模型在某个随机权重向量上的成本和梯度：
- en: '[PRE8]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output we get is:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的输出是：
- en: '[PRE9]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Gradient Descent Implementation
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度下降实现
- en: We will now implement gradient descent in order to find the optimal **w*** that
    minimizes the cost function on a given training set. The algorithm will run at
    most *max_iter* passes over the training set (defaults to 5000), unless the cost
    has not decreased by at least *tol* since the previous iteration (defaults to
    0.0001), in which case the training stops immediately.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将实现梯度下降，以找到最优的**w**，使成本函数在给定训练集上最小化。该算法最多会对训练集进行*max_iter*次迭代（默认为5000），除非成本在上一次迭代后没有至少减少*tol*（默认为0.0001），在这种情况下训练将立即停止。
- en: '[PRE10]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Normally at this point you would have to normalize your data set, since gradient
    descent does not work well with features that have different scales. In our specific
    data set normalization is not necessary since the ranges of the two features are
    similar.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 通常在这一点上你需要对数据集进行标准化，因为梯度下降对于具有不同尺度的特征效果不好。在我们的特定数据集中，由于两个特征的范围相似，因此标准化不是必需的。
- en: 'Let’s now call this function to optimize our model:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们调用这个函数来优化我们的模型：
- en: '[PRE11]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The algorithm converges after 1,413 iterations and the optimal **w*** we get
    is:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 算法在1,413次迭代后收敛，我们得到的**w***是：
- en: '[PRE12]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: There are other solvers you can use for the optimization which are often faster
    than gradient descent, such as conjugate gradient (CG) and truncated Newton (TNC).
    See [scipy.optimize.minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html)
    for more details on how to use these optimizers.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他优化器可以使用，这些优化器通常比梯度下降更快，例如共轭梯度（CG）和截断牛顿（TNC）。有关如何使用这些优化器的更多细节，请参见[scipy.optimize.minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html)。
- en: Using the Model for Predictions
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用模型进行预测
- en: Now that we have found the optimal parameters of the model, we can use it for
    predictions.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经找到了模型的最佳参数，可以使用它进行预测。
- en: 'First, let’s write a function that gets a matrix of new samples *X* and returns
    their probabilities of belonging to the positive class:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们编写一个函数，它接受新样本的矩阵*X*并返回它们属于正类的概率：
- en: '[PRE13]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The function computes the predictions of the model by simply taking the sigmoid
    of *Xᵗ***w** (which computes *σ*(**w***ᵗ***x**) for every row **x** in the matrix).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数通过简单地计算*Xᵗ***w**的sigmoid值来计算模型的预测（即对矩阵中每一行**x**计算*σ*(**w***ᵗ***x**)）。
- en: 'For example, let’s find the probability that a sample located at (6, 2) belongs
    to the versicolor class:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们找出位于(6, 2)的样本属于versicolor类的概率：
- en: '[PRE14]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This sample has 89.52% chance of being a versicolor flower. This makes sense
    since this sample is located well within the area of the versicolor flowers far
    from the border between the classes.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这个样本有89.52%的概率属于versicolor花。这是合理的，因为这个样本位于versicolor花的区域内，远离类别之间的边界。
- en: 'On the other hand, the probability that a sample located at (5.5, 3) belongs
    to the versicolor class is:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，位于(5.5, 3)的样本属于versicolor类的概率是：
- en: '[PRE16]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This time the probability is much lower (only 56.44%), since this sample is
    close to the border between the classes.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这次概率要低得多（仅56.44%），因为这个样本接近类别之间的边界。
- en: 'Let’s write another function that returns the predicted class labels instead
    of probabilities:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写另一个函数，它返回预测的类别标签而不是概率：
- en: '[PRE18]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The function simply predicts 1 whenever the probability of belonging to the
    positive class is at least 0.5, and 0 otherwise.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数简单地在正类的概率至少为0.5时预测1，否则预测0。
- en: 'Let’s test this function with the samples from above:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用上面的样本测试这个函数：
- en: '[PRE19]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As expected, both of the samples are classified as 1.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，这两个样本都被分类为1。
- en: Evaluating the Model
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'Next, let’s write a function to compute the accuracy of the model on a given
    data set:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们编写一个函数来计算模型在给定数据集上的准确性：
- en: '[PRE21]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The function first finds the predicted labels of the model on the given data
    set *X,* and compares them to the true labels **y**. The accuracy is then measured
    as the mean number of correct classifications:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数首先找到模型在给定数据集*X*上的预测标签，并将其与真实标签**y**进行比较。然后计算准确性，作为正确分类的平均数量：
- en: '![](../Images/d624d81c65fa15a038fde0246258dfba.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d624d81c65fa15a038fde0246258dfba.png)'
- en: 'Let’s use this function to find the accuracy of our model on the training and
    the test sets:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用这个函数来找出模型在训练集和测试集上的准确性：
- en: '[PRE22]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As expected, the scores are very high since the data set is linearly separable.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，由于数据集是线性可分的，得分非常高。
- en: In addition to accuracy, there are other important metrics that are used to
    evaluate classification models such as precision, recall and F1 score. These metrics
    will be discussed in a future post.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 除了准确性，还有其他重要指标用于评估分类模型，如精确度、召回率和F1分数。这些指标将在未来的文章中讨论。
- en: Plotting the Decision Boundary
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 绘制决策边界
- en: Finally, since our data set is two-dimensional, we can plot the boundary line
    between the classes that was found by our model. To that end, we first need to
    find the equation of this line.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，由于我们的数据集是二维的，我们可以绘制模型找到的类别之间的边界线。为此，我们首先需要找到这条线的方程。
- en: The boundary line is defined by the points for which the prediction of our model
    is exactly 0.5, i.e.,
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 边界线由模型预测值恰好为0.5的点定义，即：
- en: '![](../Images/27dadcb4ae17a4836c653fc811826e38.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/27dadcb4ae17a4836c653fc811826e38.png)'
- en: 'The sigmoid function is equal to 0.5 when its input is equal to 0, therefore
    we can write:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 当sigmoid函数的输入等于0时，其值为0.5，因此我们可以写成：
- en: '![](../Images/2bbc7ec897ddce1dd5749879782b3261.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2bbc7ec897ddce1dd5749879782b3261.png)'
- en: 'After rearranging the terms we get:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 重新排列项后我们得到：
- en: '![](../Images/99d2bf0c245062e5b1f58b661b4ec241.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99d2bf0c245062e5b1f58b661b4ec241.png)'
- en: 'That is, the slope of the boundary line is -*w*₁/*w*₂ and its intercept is
    -*w*₀/*w*₂. We can now write a function that plots this line:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 即，边界线的斜率为 -*w*₁/*w*₂，截距为 -*w*₀/*w*₂。我们现在可以编写一个绘制这条线的函数：
- en: '[PRE26]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![](../Images/5514bd18eafdf2330842ddd847eb5671.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5514bd18eafdf2330842ddd847eb5671.png)'
- en: The decision boundary between the classes
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 类别之间的决策边界
- en: We can see that only one sample was misclassified by our model. Training the
    model for more iterations (around 200,000) would have found a line that perfectly
    separates the two classes. With a fixed step size, the optimal convergence rate
    of gradient descent is very slow. This can be improved by using an adaptive learning
    rate (e.g., using more aggressive step sizes to compensate for the rapidly vanishing
    gradients).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，只有一个样本被模型误分类。训练模型更多的迭代（约 200,000 次）会找到一个完美分离两类的分界线。使用固定步长时，梯度下降的最优收敛速度非常慢。可以通过使用自适应学习率（例如，使用更激进的步长来补偿快速消失的梯度）来改进这一点。
- en: The LogisticRegression Class in Scikit-Learn
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scikit-Learn 中的 LogisticRegression 类
- en: Although implementing logistic regression from scratch had its own educational
    merits, a more practical choice would be to use the ready-made [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)
    class from Scikit-Learn. This class uses more efficient solvers than the plain
    vanilla gradient descent, and it also provides additional options such as regularization
    and early stopping.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管从头实现逻辑回归有其自身的教育意义，但更实际的选择是使用 Scikit-Learn 提供的现成的 [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)
    类。该类使用比普通梯度下降更高效的解算器，并且还提供了额外的选项，如正则化和提前停止。
- en: 'The important hyperparameters of this class are:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 该类的重要超参数有：
- en: '*penalty* — specifies the type of regularization to apply. Can be one of the
    options: None, ‘l2’ (the default), ‘l1’ and ‘elasticnet’. See [this article](https://medium.com/@roiyeho/regularization-19b1879415a1)
    for more information on regularization.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*penalty* — 指定要应用的正则化类型。可以是以下选项之一：None, ‘l2’（默认值）, ‘l1’ 和 ‘elasticnet’。有关正则化的更多信息，请参见
    [这篇文章](https://medium.com/@roiyeho/regularization-19b1879415a1)。'
- en: '*tol* — the tolerance for the stopping criterion (defaults to 0.0001).'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*tol* — 停止准则的容忍度（默认为 0.0001）。'
- en: '*C* — the inverse of the regularization coefficient (defaults to 1.0). Smaller
    values specify stronger regularization.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*C* — 正则化系数的倒数（默认为 1.0）。较小的值表示更强的正则化。'
- en: '*solver* — the algorithm to use for the optimization. Can take one of the options:
    ‘lbfgs’ (the default), ‘liblinear’, ‘newton-cg’, ‘newton-cholesky’, ‘sag’, ‘saga’.
    Read the documentation for more information on these optimizers.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*solver* — 用于优化的算法。可以选择以下选项之一：‘lbfgs’（默认值），‘liblinear’，‘newton-cg’，‘newton-cholesky’，‘sag’，‘saga’。有关这些优化器的更多信息，请阅读文档。'
- en: '*max_iter* — the maximum number of iterations for the solvers to converge (defaults
    to 100)'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*max_iter* — 解算器收敛的最大迭代次数（默认为 100）'
- en: '*multi_class* — how to handle multi-class classification problems. Can take
    one of the options: ‘ovr’ (One Vs. Rest, i.e., a binary classifier is built for
    each class against all the other ones), ‘multinomial’ (uses multinomial logistic
    regression) or ‘auto’ (the default).'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*multi_class* — 处理多分类问题的方法。可以选择以下选项之一：‘ovr’（一对其余，即为每个类别构建一个二分类器与其他类别对抗）、‘multinomial’（使用多项式逻辑回归）或
    ‘auto’（默认）。'
- en: 'When using the LogisticRegression class, you do not need to manually add a
    column of ones to the design matrix *X*, since this is done automatically by Scikit-Learn.
    Therefore, before building the model, we will split the original data (without
    the extra column of ones) into training and test sets:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LogisticRegression 类时，您无需手动将一列全是 1 的列添加到设计矩阵 *X* 中，因为这会由 Scikit-Learn 自动完成。因此，在构建模型之前，我们将原始数据（没有额外的全是
    1 的列）分割成训练集和测试集：
- en: '[PRE28]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Let’s create an instance of LogisticRegression with its default settings, and
    fit it to the training set:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用默认设置创建一个 LogisticRegression 实例，并将其拟合到训练集上：
- en: '[PRE29]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, we evaluate the model on the training and the test sets:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在训练集和测试集上评估模型：
- en: '[PRE30]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This time we get perfect scores both on the training and the test sets. We
    can also check how many iterations were needed for convergence by querying the
    *n_iter_* attribute:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这次我们在训练集和测试集上都获得了完美的分数。我们还可以通过查询 *n_iter_* 属性来检查收敛所需的迭代次数：
- en: '[PRE32]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Only 15 iterations were needed for convergence! Clearly, the solver used by
    LogisticRegression ([L-BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS)
    by default) is more efficient than our implementation of gradient descent.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 仅需15次迭代即可收敛！显然，LogisticRegression使用的求解器（默认使用[L-BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS)）比我们实现的梯度下降更高效。
- en: 'We can plot the decision boundary found by the model as we did previously.
    However, this time the optimal coefficients are stored in two different attributes
    of the model:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像之前一样绘制模型找到的决策边界。然而，这次最佳系数存储在模型的两个不同属性中：
- en: '*coef_* is an array that contains all the weights except for the intercept'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*coef_* 是一个数组，包含所有权重，除了截距项'
- en: '*intercept_* is the intercept term (*w*₀)'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*intercept_* 是截距项 (*w*₀)'
- en: 'Therefore, we need to concatenate these two attributes into one array before
    calling the *plot_decision_boundary*() function:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要在调用*plot_decision_boundary*()函数之前，将这两个属性连接成一个数组：
- en: '[PRE34]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![](../Images/b18f099a77da55cd7e524d6175341179.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b18f099a77da55cd7e524d6175341179.png)'
- en: The decision boundary found by LogisticRegression
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: LogisticRegression找到的决策边界
- en: As expected, the line found by LogisticRegression perfectly separates the two
    classes.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，LogisticRegression找到的线完美地分隔了两个类别。
- en: Summary
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Let’s summarize the pros and cons of logistic regression as compared to other
    classification models.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下逻辑回归与其他分类模型的优缺点。
- en: '**Pros**:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**：'
- en: When the data is linearly separable, the algorithm is guaranteed to find a separating
    hyperplane between the classes.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当数据是线性可分的时，算法保证能找到一个类间的分离超平面。
- en: Provides class probability estimates
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供类概率估计
- en: Does not suffer from overfitting (but usually underfits the data)
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不容易过拟合（但通常对数据存在欠拟合）
- en: Highly interpretable (the weight associated with each feature represents its
    importance)
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高度可解释（与每个特征相关的权重表示其重要性）
- en: Highly scalable (requires a number of parameters linear in the number of features)
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高度可扩展（需要的参数数量与特征数量线性相关）
- en: Can handle redundant features (by assigning them weights close to 0)
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以处理冗余特征（通过赋予它们接近0的权重）
- en: Has a small number of hyperparameters
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数数量较少
- en: '**Cons**:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**：'
- en: Can find only linear decision boundaries between classes
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只能找到类别之间的线性决策边界
- en: Usually outperformed by more complex models
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常被更复杂的模型超越
- en: Supports only binary classification, but can be extended to multi-class. The
    extension of logistic regression to multi-class problems (called **multinomial
    logistic regression** or **softmax regression**) is covered in [this article](/deep-dive-into-softmax-regression-62deea103cb8).
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅支持二分类，但可以扩展到多分类。将逻辑回归扩展到多分类问题（称为**多项式逻辑回归**或**softmax回归**）在[这篇文章](/deep-dive-into-softmax-regression-62deea103cb8)中有介绍。
- en: Cannot deal with missing values
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法处理缺失值
- en: Final Notes
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最终备注
- en: All images unless otherwise noted are by the author.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图像均由作者提供。
- en: 'The code examples of this article can be found on my github: [https://github.com/roiyeho/medium/tree/main/logistic_regression](https://github.com/roiyeho/medium/tree/main/logistic_regression)'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的代码示例可以在我的github上找到：[https://github.com/roiyeho/medium/tree/main/logistic_regression](https://github.com/roiyeho/medium/tree/main/logistic_regression)
- en: Thanks for reading!
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
