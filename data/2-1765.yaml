- en: Random Numbers in Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中的随机数
- en: 原文：[https://towardsdatascience.com/random-numbers-in-machine-learning-9cb5d83d078e](https://towardsdatascience.com/random-numbers-in-machine-learning-9cb5d83d078e)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/random-numbers-in-machine-learning-9cb5d83d078e](https://towardsdatascience.com/random-numbers-in-machine-learning-9cb5d83d078e)
- en: All about pseudo-random numbers, seeding, and reproducibility
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于伪随机数、种子设置和可重复性的一切
- en: '[](https://medium.com/@caroline.arnold_63207?source=post_page-----9cb5d83d078e--------------------------------)[![Caroline
    Arnold](../Images/2560e106ba9deda7889c7d253792d814.png)](https://medium.com/@caroline.arnold_63207?source=post_page-----9cb5d83d078e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9cb5d83d078e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9cb5d83d078e--------------------------------)
    [Caroline Arnold](https://medium.com/@caroline.arnold_63207?source=post_page-----9cb5d83d078e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@caroline.arnold_63207?source=post_page-----9cb5d83d078e--------------------------------)[![Caroline
    Arnold](../Images/2560e106ba9deda7889c7d253792d814.png)](https://medium.com/@caroline.arnold_63207?source=post_page-----9cb5d83d078e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9cb5d83d078e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9cb5d83d078e--------------------------------)
    [Caroline Arnold](https://medium.com/@caroline.arnold_63207?source=post_page-----9cb5d83d078e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9cb5d83d078e--------------------------------)
    ·8 min read·Oct 20, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9cb5d83d078e--------------------------------)
    ·8分钟阅读·2023年10月20日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/cede17b2b0c90191d192b48d7de7c688.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cede17b2b0c90191d192b48d7de7c688.png)'
- en: Photo by [Riho Kroll](https://unsplash.com/@rihok?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Riho Kroll](https://unsplash.com/@rihok?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Machine learning relies on statistics, and random numbers are important to the
    performance of many steps in the data processing and model training pipeline.
    Modern machine learning frameworks provide abstractions and functions that implement
    randomness under the hood, and for us as data scientists and machine learning
    engineers, the details of random number generation often remain obscure.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习依赖于统计学，随机数对数据处理和模型训练管道中许多步骤的性能至关重要。现代机器学习框架提供了在后台实现随机性的抽象和函数，而对于我们这些数据科学家和机器学习工程师来说，随机数生成的细节往往仍然模糊不清。
- en: 'In this article, I want to shed some light on random numbers in machine learning.
    You will read about:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我想探讨机器学习中的随机数。你将阅读到：
- en: 3 examples of the use of random numbers in machine learning
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习中随机数使用的3个示例
- en: Generating (pseudo-)random numbers
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成（伪）随机数
- en: Fixing random numbers by seeding
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过设定随机种子来固定随机数
- en: 'Reproducible machine learning: necessary lines of code for scikit-learn, tensorflow,
    and pytorch.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可重复的机器学习：scikit-learn、tensorflow 和 pytorch 的必要代码行
- en: By the end of this article, you will know what happens when you use random numbers
    in your machine learning pipeline, and you will learn the necessary lines of code
    to ensure reproducibility of your machine learning algorithms.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文结束时，你将了解在机器学习管道中使用随机数时发生了什么，并学习确保机器学习算法可重复性的必要代码行。
- en: 3 Examples for the use of random numbers in machine learning
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习中随机数的3个使用示例
- en: To illustrate the importance of random numbers, we discuss three examples where
    they are relevant along the machine learning pipeline.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明随机数的重要性，我们讨论了机器学习管道中与随机数相关的三个示例。
- en: Creating train/test splits of a dataset
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建数据集的训练/测试划分
- en: Weight initialization in a neural network
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经网络中的权重初始化
- en: Choosing minibatches during training
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练过程中的小批量选择
- en: '**Train/test split** Splitting your dataset into training and test data is
    one of the most important steps in evaluating the performance of a machine learning
    algorithm. We are interested in creating models that generalize well to data not
    used during training. To this end, a collection of data samples is divided into
    at least two disjoint sets.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练/测试划分** 将数据集分为训练数据和测试数据是评估机器学习算法性能的最重要步骤之一。我们关注的是创建能很好泛化到训练过程中未使用的数据的模型。为此，一组数据样本被划分为至少两个不相交的集合。'
- en: The training data is used to train the algorithm, i.e. to iteratively fix the
    model parameters. The test data is used to validate the algorithm by applying
    a trained model to test data and reporting appropriate metrics.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据用于训练算法，即迭代地调整模型参数。测试数据用于通过将训练好的模型应用于测试数据并报告适当的指标来验证算法。
- en: '![](../Images/9a5a0010d6e5ee72993ca18b135788a1.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9a5a0010d6e5ee72993ca18b135788a1.png)'
- en: 'Random split of a dataset into train and test data. Image: Author.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的随机划分为训练数据和测试数据。图片：作者。
- en: The popular scikit-learn function `sklearn.model_selection.train_test_split`
    uses random numbers. Under the hood, it creates an array of indices corresponding
    to the length of the data set. The indices are then randomly assigned to the train
    and test data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 流行的scikit-learn函数`sklearn.model_selection.train_test_split`使用随机数。在底层，它创建一个与数据集长度对应的索引数组。这些索引随后被随机分配到训练数据和测试数据中。
- en: '**Weight initialization**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**权重初始化**'
- en: 'The layers of a neural network contain parameters that are adjusted during
    training. For linear layers, these parameters are weights and biases. They are
    initially assigned random values. Consider the example of a linear layer with
    4 neurons connected to an output layer with 3 potential classes: This layer corresponds
    to a matrix *W* of 4 x 3 weights.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的层包含在训练过程中调整的参数。对于线性层，这些参数是权重和偏置。它们最初被赋予随机值。考虑一个4个神经元连接到具有3个潜在类别的输出层的线性层的示例：该层对应于一个4
    x 3权重矩阵*W*。
- en: '![](../Images/59fffcc5ca648fe4d7673e49fed816f5.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/59fffcc5ca648fe4d7673e49fed816f5.png)'
- en: 'Three different random initializations for the 4 x 3 matrix W. Image: Author.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 4 x 3矩阵W的三种不同随机初始化。图片：作者。
- en: The initialization of the weights is crucial for the convergence of the model
    training. If all weights had the same value, the backpropagation algorithm would
    see no reason to treat each neuron differently when updating the weights.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 权重的初始化对模型训练的收敛至关重要。如果所有权重都具有相同的值，反向传播算法在更新权重时将没有理由对每个神经元进行不同的处理。
- en: Therefore, when the model is first instantiated, the weights are chosen randomly.
    Typically, the random numbers have a reasonable statistical distribution behind
    them. Xavier Glorot found in 2010 that training a feed-forward neural network
    of layer dimension n x m is improved when the network weights are initialized
    from the uniform distribution [1].
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当模型首次实例化时，权重是随机选择的。通常，这些随机数背后有合理的统计分布。Xavier Glorot在2010年发现，当网络权重从均匀分布中初始化时，训练一个层级维度为n
    x m的前馈神经网络会得到改进[1]。
- en: '**Choosing training batches** In general, it is neither computationally feasible
    nor advisable to use the entire training dataset to update the model parameters
    in one go. Therefore, the training dataset is divided into minibatches of fixed
    size. The dataloader creates these minibatches and can randomly shuffle the data.
    This is to prevent data from entering training in a biased way, e.g. in a temporal
    order because of the way it was collected.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**选择训练批次** 通常，使用整个训练数据集一次性更新模型参数既不可行也不建议。因此，训练数据集被分成固定大小的小批量。数据加载器创建这些小批量并可以随机打乱数据。这是为了防止数据以有偏的方式进入训练，例如由于收集方式而按照时间顺序。'
- en: '![](../Images/9245d04a92db4b56d02c9df60e9965fd.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9245d04a92db4b56d02c9df60e9965fd.png)'
- en: 'Minibatches of size 4 formed from a dataset with 12 samples. Top: Dataset is
    not shuffled. Bottom: Dataset is shuffled randomly. Image: Author.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 从包含12个样本的数据集中形成的大小为4的小批量。顶部：数据集没有打乱。底部：数据集被随机打乱。图片：作者。
- en: Stochastic gradient descent relies on the randomness of these minibatches. By
    presenting random subsets of the training data to the algorithm, each backpropagation
    step emphasizes a slightly different aspect of the training data. This avoids
    getting stuck in local minima during training.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降依赖于这些小批量的随机性。通过向算法呈现训练数据的随机子集，每一步反向传播都会强调训练数据的一个略微不同的方面。这避免了在训练过程中陷入局部最小值。
- en: Pseudo-random numbers
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伪随机数
- en: Modern programming languages and machine learning frameworks provide tools for
    the developer to generate random numbers without worrying about the underlying
    algorithm. To generate a sequence of 100 uniform random numbers between 0 and
    1 in Python, simply type
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现代编程语言和机器学习框架提供了生成随机数的工具，使开发者无需担心底层算法。要在Python中生成100个0到1之间的均匀随机数，只需输入
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: But what actually happens under the hood when you execute this line of code?
    Enter the Random Number Generators (RNGs).
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当你执行这一行代码时，实际上会发生什么？这时就引入了随机数生成器（RNGs）。
- en: A standard computer algorithm produces predictable results, which is exactly
    the opposite of what we want— a random sequence of numbers. What we are looking
    for is an algorithm that produces sequences of numbers that are *hard to predict*.
    Note that *hard* does not mean *impossible*! So we need a source of entropy and
    a cryptographic algorithm. For a true random number generator, the source of entropy
    could be informed by the environment, e.g. by taking a sensor temperature or the
    decay of a radioactive particle as input [3].
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 标准计算机算法产生可预测的结果，这恰恰与我们想要的相反——一个随机的数字序列。我们所寻找的是一个能生成*难以预测*的数字序列的算法。需要注意的是，*难*并不意味着*不可能*！因此，我们需要一个熵源和一个加密算法。对于真正的随机数生成器，熵源可以由环境提供，例如通过测量传感器温度或放射性粒子的衰变来获取输入
    [3]。
- en: For machine learning, we do not need the same high-end randomness that would
    be required for a cryptographic application. Python — specifically the numpy library
    — implements an RNG to generate pseudo-random numbers.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习，我们不需要像加密应用那样高端的随机性。Python — 特别是 numpy 库 — 实现了一个 RNG 来生成伪随机数。
- en: In everyday language, a pseudo-random number sequence is close enough to a truly
    random sequence that the lack of complete randomness does not affect the purpose
    of the algorithm. The correlation length of the sequence must be sufficient for
    the application.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在日常语言中，伪随机数序列足够接近真正的随机序列，以至于不完全的随机性不会影响算法的目的。序列的相关长度必须足够满足应用的需求。
- en: From an initial seed, a function *f* is applied to generate a state. Another
    function *g* is applied to generate a random number. This process is repeated
    as many times as necessary. The functions *f, g* should not be invertible. Also,
    the different states must provide enough variability to really generate a lot
    of different random numbers, the so-called sequence length.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 从初始种子开始，应用一个函数 *f* 生成一个状态。另一个函数 *g* 被用来生成一个随机数。这一过程会重复进行多次。函数 *f, g* 应该是不可逆的。此外，不同的状态必须提供足够的变异性，以真正生成大量不同的随机数，这被称为序列长度。
- en: '![](../Images/58d59cfb5bde0f3915d838c17a967198.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/58d59cfb5bde0f3915d838c17a967198.png)'
- en: 'Pseudo-random number generator following [4]. Image: Author.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循[4]的伪随机数生成器。图像：作者。
- en: The underlying algorithm used in Python is called Mersenne Twister [4]. It provides
    sequences of random numbers for each state and is computationally more efficient
    than the simple implementation shown above. It is not cryptographically secure,
    but it is fast and sufficiently powerful.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Python 中使用的底层算法叫做 Mersenne Twister [4]。它为每个状态提供随机数序列，并且比上面展示的简单实现计算效率更高。虽然它在加密上不安全，但它快速且足够强大。
- en: Random number distributions
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机数分布
- en: Often we are interested in drawing random numbers that follow a certain statistical
    distribution. For example, the uniform distribution assigns equal probability
    to all numbers in a given interval. The normal distribution follows a Gaussian
    curve with a fixed mean and variance.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常感兴趣的是生成符合某种统计分布的随机数。例如，均匀分布在给定区间内为所有数字分配相等的概率。正态分布遵循具有固定均值和方差的高斯曲线。
- en: '![](../Images/7055ffbb5bf73a5f41c0b228d5808f2a.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7055ffbb5bf73a5f41c0b228d5808f2a.png)'
- en: 'Normal distribution (left) and uniform distribution (right) of 1000 random
    numbers. Image: Author.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 1000个随机数的正态分布（左）和均匀分布（右）。图像：作者。
- en: Distributions of random numbers can be transformed into each other by their
    cumulative distribution function (CDF). Thus, it is sufficient to be able to draw
    uniform numbers, and any other distribution can be generated from them. The details
    are beyond the scope of this article, for a start we recommend [5].
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 随机数的分布可以通过其累积分布函数（CDF）相互转换。因此，只需要能够生成均匀分布的数字，其他任何分布都可以从中生成。具体细节超出了本文的范围，初步了解我们推荐[5]。
- en: Fixing random numbers by seeding
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过设定种子来固定随机数
- en: As explained above, the seed is an integer that serves as input for the pseudo-random
    number generator.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，种子是一个整数，用作伪随机数生成器的输入。
- en: For a fixed seed, the sequence of pseudo-random numbers generated by the presented
    algorithm is always the same.
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对于固定的种子，由该算法生成的伪随机数序列总是相同的。
- en: We can use this insight to achieve two different objectives.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用这个洞察来实现两个不同的目标。
- en: '**Choose a new random seed** When you choose a new random seed, all aspects
    of the machine learning pipeline that rely on random numbers are initialized with
    different values. While the underlying distributions and dataset sizes remain
    unchanged, the training process can be affected.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**选择新的随机种子** 当你选择新的随机种子时，所有依赖于随机数的机器学习流程方面都将使用不同的值进行初始化。虽然底层分布和数据集大小保持不变，但训练过程可能会受到影响。'
- en: Ideally, your training procedure should not be too dependent on the choice of
    random seed. You want to make sure that the training converges regardless of the
    initialization, and not rely on golden runs that magically produce great results
    without ever being reproducible.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，你的训练过程不应过于依赖随机种子的选择。你希望确保训练能够收敛，而不是依赖于那些神奇地产生良好结果的黄金运行，并且这些结果无法重复。
- en: '**Fix the random seed** Fixing the random seed is particularly popular for
    teaching purposes. When I create a simple model for a small dataset, the probability
    that my results depend on the random seed is quite high. Whenever I prepare teaching
    materials, such as a Jupyter notebook, for students, I fix the random seed so
    that they get the same results. This reduces confusion and allows students to
    focus on the new concepts rather than the less important digits of the metrics.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**修复随机种子** 修复随机种子在教学中尤为流行。当我为小数据集创建一个简单模型时，结果依赖于随机种子的概率相当高。每当我为学生准备教学材料，如Jupyter笔记本时，我都会修复随机种子，以便他们获得相同的结果。这减少了混淆，让学生能够专注于新概念，而不是度量标准中不那么重要的数字。'
- en: '![](../Images/6c12caaf7bcc716e1731fce5a3656db6.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c12caaf7bcc716e1731fce5a3656db6.png)'
- en: Clones look exactly the same. Photo by [Phil Shaw](https://unsplash.com/@phillshaw?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 克隆看起来完全一样。照片由[Phil Shaw](https://unsplash.com/@phillshaw?utm_source=medium&utm_medium=referral)拍摄，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Reproducibility in machine learning
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习中的可重复性
- en: We have now learned that random numbers are critical to the performance of the
    machine learning process. However, if we want exact reproducibility of our algorithm,
    we need to fix the random seeds.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经了解到，随机数对机器学习过程的性能至关重要。然而，如果我们希望算法的精确可重复性，就需要修复随机种子。
- en: It is important to realize that the random seed must be fixed separately for
    each library that uses random numbers. For example, even if we fix the `numpy`
    random seed by `np.random.seed(789123)` , the `torch` random seed will not be
    affected by that and training will not be reproducible.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要意识到，随机种子必须为每个使用随机数的库单独修复。例如，即使我们通过`np.random.seed(789123)`修复了`numpy`的随机种子，`torch`的随机种子也不会受到影响，因此训练将无法重复。
- en: In the following, we summarize the necessary calls to fix random seeds for different
    popular machine learning frameworks.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面，我们总结了修复不同流行机器学习框架随机种子的必要调用。
- en: '**Scikit-Learn** uses numpy’s random number generator throughout. To fix the
    random seed for this framework, it is sufficent to set'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scikit-Learn**在整个过程中使用numpy的随机数生成器。要修复该框架的随机种子，只需设置'
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The popular framework offers another function to access random number generators,
    [*check_random_state*](https://scikit-learn.org/stable/modules/generated/sklearn.utils.check_random_state.html#sklearn.utils.check_random_state).
    To generate 1000 random numbers quickly:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 流行的框架提供了另一种访问随机数生成器的函数，[*check_random_state*](https://scikit-learn.org/stable/modules/generated/sklearn.utils.check_random_state.html#sklearn.utils.check_random_state)。要快速生成1000个随机数：
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Tensorflow 2** provides its own random number generator module in `tf.random`
    . According to the [documentation](https://www.tensorflow.org/guide/random_numbers),
    fixing random seeds there ensures reproducibility. However, it is not guaranteed
    across different versions of tensorflow. To generate the same pseudo-random numbers,
    use the `stateless_XXX` functions from the `tf.random` module, e.g., for a series
    of 1000 random numbers following a normal distribution:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**Tensorflow 2**提供了其自己的随机数生成模块`tf.random`。根据[文档](https://www.tensorflow.org/guide/random_numbers)，在那里修复随机种子可以确保可重复性。然而，这在不同版本的tensorflow中并不能保证。为了生成相同的伪随机数，可以使用`tf.random`模块中的`stateless_XXX`函数，例如，对于遵循正态分布的1000个随机数序列：'
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Pytorch** [controls the random number generation](https://pytorch.org/docs/stable/notes/randomness.html)
    in a way that is very similar to the numpy statements. The following will set
    the seed for all procedures that rely on this random number generator:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**Pytorch** [以类似于 numpy 语句的方式控制随机数生成](https://pytorch.org/docs/stable/notes/randomness.html)。以下将设置所有依赖于该随机数生成器的过程的种子：'
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: However, they point out that some functions may depend on the numpy random number
    generator, so it is advisable to fix that seed as well.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，他们指出一些函数可能依赖于 numpy 随机数生成器，因此建议也固定该种子。
- en: The Pytorch documentation points out a problem with full reproducibility. Not
    only is it impossible to reproduce the random numbers exactly between versions,
    but the numbers may also depend on the hardware. Using different GPUs with different
    CUDA toolkits or CPUs can lead to different random number sequences.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch 文档指出了一个关于完全可重复性的问题。不仅在不同版本之间不可能精确重现随机数，而且这些数字可能还会依赖于硬件。使用不同的 GPU 配合不同的
    CUDA 工具包或 CPU 可能会导致不同的随机数序列。
- en: Summary
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: Random numbers are critical to the performance of machine learning algorithms.
    They ensure that datasets are partitioned in an unbiased manner, improve the generalizability
    of the algorithm, and increase the convergence of training procedures.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 随机数对机器学习算法的性能至关重要。它们确保数据集以无偏差的方式进行划分，提高算法的泛化能力，并增加训练过程的收敛性。
- en: Under the hood, sequences of random numbers are generated by so-called RNGs,
    established algorithms that are able to provide random numbers following different
    distributions.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，随机数序列由所谓的 RNGs 生成，这些算法能够提供遵循不同分布的随机数。
- en: The random seed can be used to fix the sequence of random numbers. This makes
    the machine learning algorithm reproducible, which is especially useful for teaching
    purposes and for academic papers. Finally, remember that if you are using a mix
    of machine learning frameworks in your project, you may need to set the random
    seed for all of them!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 随机种子可以用来固定随机数序列。这使得机器学习算法具有可重复性，这对于教学目的和学术论文特别有用。最后，请记住，如果你在项目中使用了多种机器学习框架，你可能需要为所有框架设置随机种子！
- en: References
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考资料
- en: Xavier Glorot and Yoshua Bengio, “Understanding the difficulty of training deep
    feedforward neural networks”, *Proceedings of the Thirteenth International Conference
    on Artificial Intelligence and Statistics*, PMLR 9:249–256, 2010\. [[paper](http://proceedings.mlr.press/v9/glorot10a.html)]
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Xavier Glorot 和 Yoshua Bengio, “Understanding the difficulty of training deep
    feedforward neural networks”, *Proceedings of the Thirteenth International Conference
    on Artificial Intelligence and Statistics*, PMLR 9:249–256, 2010\. [[paper](http://proceedings.mlr.press/v9/glorot10a.html)]
- en: '[https://numpy.org/doc/stable/reference/random/index.html](https://numpy.org/doc/stable/reference/random/index.html)'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://numpy.org/doc/stable/reference/random/index.html](https://numpy.org/doc/stable/reference/random/index.html)'
- en: '[https://www.redhat.com/en/blog/understanding-random-number-generators-and-their-limitations-linux](https://www.redhat.com/en/blog/understanding-random-number-generators-and-their-limitations-linux)'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://www.redhat.com/en/blog/understanding-random-number-generators-and-their-limitations-linux](https://www.redhat.com/en/blog/understanding-random-number-generators-and-their-limitations-linux)'
- en: 'Mersenne Twister in detail: [https://www.cryptologie.net/article/331/how-does-the-mersennes-twister-work/](https://www.cryptologie.net/article/331/how-does-the-mersennes-twister-work/)'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '详细介绍梅森旋转算法: [https://www.cryptologie.net/article/331/how-does-the-mersennes-twister-work/](https://www.cryptologie.net/article/331/how-does-the-mersennes-twister-work/)'
- en: '[https://www.wikiwand.com/en/Inverse_transform_sampling](https://www.wikiwand.com/en/Inverse_transform_sampling)'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://www.wikiwand.com/en/Inverse_transform_sampling](https://www.wikiwand.com/en/Inverse_transform_sampling)'
- en: 'Tensorflow: [https://www.tensorflow.org/guide/random_numbers](https://www.tensorflow.org/guide/random_numbers)'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Tensorflow: [https://www.tensorflow.org/guide/random_numbers](https://www.tensorflow.org/guide/random_numbers)'
- en: 'Pytorch: [https://pytorch.org/docs/stable/notes/randomness.html](https://pytorch.org/docs/stable/notes/randomness.html)'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Pytorch: [https://pytorch.org/docs/stable/notes/randomness.html](https://pytorch.org/docs/stable/notes/randomness.html)'
