- en: Intermediate Deep Learning with Transfer Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 中级深度学习与迁移学习
- en: 原文：[https://towardsdatascience.com/intermediate-deep-learning-with-transfer-learning-f1aba5a814f](https://towardsdatascience.com/intermediate-deep-learning-with-transfer-learning-f1aba5a814f)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/intermediate-deep-learning-with-transfer-learning-f1aba5a814f](https://towardsdatascience.com/intermediate-deep-learning-with-transfer-learning-f1aba5a814f)
- en: A practical guide for fine-tuning Deep Learning models for computer vision and
    natural language processing
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实用指南，用于调整深度学习模型以进行计算机视觉和自然语言处理
- en: '[](https://medium.com/@iamleonie?source=post_page-----f1aba5a814f--------------------------------)[![Leonie
    Monigatti](../Images/4044b1685ada53a30160b03dc78f9626.png)](https://medium.com/@iamleonie?source=post_page-----f1aba5a814f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f1aba5a814f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f1aba5a814f--------------------------------)
    [Leonie Monigatti](https://medium.com/@iamleonie?source=post_page-----f1aba5a814f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@iamleonie?source=post_page-----f1aba5a814f--------------------------------)[![Leonie
    Monigatti](../Images/4044b1685ada53a30160b03dc78f9626.png)](https://medium.com/@iamleonie?source=post_page-----f1aba5a814f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f1aba5a814f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f1aba5a814f--------------------------------)
    [Leonie Monigatti](https://medium.com/@iamleonie?source=post_page-----f1aba5a814f--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f1aba5a814f--------------------------------)
    ·11 min read·Feb 22, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f1aba5a814f--------------------------------)
    ·阅读时间 11 分钟·2023年2月22日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/77c0a9277a5199f8f3fbdf7450e3992a.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77c0a9277a5199f8f3fbdf7450e3992a.png)'
- en: Image by the author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: Getting started with Deep Learning is easy. You can have a neural network setup
    and training within just a few lines of code. But it can become overwhelming when
    you go from a beginner to an intermediate level. You are confronted with many
    new terms like “EfficientNet” or “DeBERTa”. What are all these terms? What do
    they have to do with neural networks?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 入门深度学习很简单。你只需几行代码就可以设置并训练一个神经网络。然而，当你从初学者进阶到中级水平时，可能会感到不知所措。你会遇到许多新术语，比如“EfficientNet”或“DeBERTa”。这些术语是什么意思？它们与神经网络有什么关系？
- en: The beginner tutorials don’t tell you that only a few people train neural networks
    from scratch in practice. This is because neural networks take a lot of time and
    computational resources to train on a sufficiently large dataset to perform well.
    Hence, re-using a pre-trained model as a starting point is a common practice.
    This practice is called Transfer Learning [2].
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 初学者教程没有告诉你，实际上只有少数人从头开始训练神经网络。这是因为神经网络在足够大的数据集上进行训练需要大量的时间和计算资源。因此，使用预训练模型作为起点是一种常见的做法。这种做法称为迁移学习
    [2]。
- en: The beginner tutorials don’t tell you that only a few people train neural networks
    from scratch in practice.
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 初学者教程没有告诉你，实际上只有少数人从头开始训练神经网络。
- en: Prerequisites
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前提条件
- en: This article will guide you from a beginner to an intermediate level in the
    field of Deep Learning. Thus, it assumes you already have some basic understanding
    of Deep Learning concepts.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将指导你从初学者成长为中级深度学习领域的专家。因此，本文假设你已经对深度学习概念有一些基本了解。
- en: Disclaimer
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 免责声明
- en: This article is **inspired by the following four resources** on best practices
    in Deep Learning I have recently come across and summarizes their key points into
    an article format. None of the original ideas in this article are mine — instead,
    view this article as study notes.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本文**灵感来源于我最近接触的四个关于深度学习最佳实践的资源**，并将其关键点总结成一篇文章。本文中的原始观点并非我所创 — 可以将这篇文章视为学习笔记。
- en: 'V. Godbole, G. E. Dahl, J. Gilmer, C. J. Shallue and Z. Nado: [Deep Learning
    Tuning Playbook](https://github.com/google-research/tuning_playbook) [4] (Format:
    Git repository)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: V. Godbole, G. E. Dahl, J. Gilmer, C. J. Shallue 和 Z. Nado：[深度学习调整手册](https://github.com/google-research/tuning_playbook)
    [4]（格式：Git 仓库）
- en: 'S. Bhutani with H20.ai: [Best Practises for Training ML Models](https://www.youtube.com/watch?v=_mzrfMA8Qx4)
    [1] (Video format)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S. Bhutani 和 H20.ai：[机器学习模型训练最佳实践](https://www.youtube.com/watch?v=_mzrfMA8Qx4)
    [1]（视频格式）
- en: 'P. Singer and Y. Babakhin: [Practical Tips for Deep Transfer Learning](https://drive.google.com/drive/folders/1VtJF-zPbXc-V-UDl2bDgWJp05DnKZpQH)
    presented at Kaggle Days Paris in November 2022\. [11] (Presentation format)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P. Singer 和 Y. Babakhin：[深度迁移学习的实用技巧](https://drive.google.com/drive/folders/1VtJF-zPbXc-V-UDl2bDgWJp05DnKZpQH)
    于2022年11月在Kaggle Days巴黎展示 [11]（演示文稿格式）
- en: 'D. Kłeczek with Munich NLP: [Ten Proven Techniques to Improve Your NLP Model
    Training](https://www.youtube.com/watch?v=UbL1QMwDpec) [9] (Video format)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: D. Kłeczek 与慕尼黑NLP：[提高你的NLP模型训练的十种有效技巧](https://www.youtube.com/watch?v=UbL1QMwDpec)
    [9]（视频格式）
- en: A Brief Introduction to Transfer Learning
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移学习简要介绍
- en: Transfer Learning describes the practice of re-using a pre-trained neural network
    instead of training one from scratch to save time and computational resources.
    Thus, were are transferring the pre-learned weights and knowledge from one task
    to another. The pre-trained models are also called **backbones**.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习描述了重新使用预训练神经网络而不是从头训练网络以节省时间和计算资源的实践。因此，我们将预先学习的权重和知识从一个任务转移到另一个任务。预训练模型也被称为**骨干网络**。
- en: '![](../Images/a27087d9ae9dee6d870e82b63acb2ebf.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a27087d9ae9dee6d870e82b63acb2ebf.png)'
- en: Transfer Learning (Image by the author)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习（图片由作者提供）
- en: Transfer learning is practical when your dataset is small or similar to the
    one your backbone was trained on [2]. But it never hurts to use transfer learning
    [8], even if your dataset is sufficiently large or your task is different from
    the one your backbone was trained on, because usually, the first few layers contain
    generic information.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的数据集较小或与骨干网络训练的数据集相似时，迁移学习是实用的 [2]。但即使你的数据集足够大或你的任务不同于骨干网络训练的任务，使用迁移学习也无妨
    [8]，因为通常，前几层包含通用信息。
- en: While you can keep the transferred model weights and only retrain the classifier,
    it is common to **fine-tune** the model weights of the whole model.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管你可以保持转移模型的权重并仅重新训练分类器，但通常会**微调**整个模型的权重。
- en: 'Some popular backbones for computer vision (CV) problems currently are:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当前计算机视觉（CV）问题的一些流行骨干网络包括：
- en: ResNet (Residual Network) [6]
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNet（残差网络） [6]
- en: DenseNet [7]
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DenseNet [7]
- en: EfficientNet [12]
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EfficientNet [12]
- en: 'Respectively, some popular backbones for natural language processing (NLP)
    problems are:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 分别，针对自然语言处理（NLP）问题的一些流行骨干网络有：
- en: BERT (Bidirectional Encoder Representations from Transformers) [3]
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT（双向编码器表示从变换器） [3]
- en: RoBERTa (Robustly Optimized BERT Pretraining Approach) [10]
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RoBERTa（强健优化BERT预训练方法） [10]
- en: DeBERTa(Decoding-enhanced BERT with disentangled attention) [5]
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeBERTa（解码增强BERT与解耦注意力） [5]
- en: Of course, Transfer Learning is only possible due to researchers sharing their
    model checkpoints for the benefit of others [2].
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，迁移学习的可能性仅仅因为研究人员共享他们的模型检查点以造福他人 [2]。
- en: 'The main steps of approaching a CV or NLP problem with Deep Learning are:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 处理CV或NLP问题的主要步骤包括：
- en: '[Building a baseline](#4e16)'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[建立基线](#4e16)'
- en: '[Increasing complexity and improving the performance in small increments](#d76d)'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[逐步增加复杂性和提高性能](#d76d)'
- en: '[Squeezing the last bits of performance](#bd54)'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[挤压最后一点性能](#bd54)'
- en: 'Step 1: Building a Baseline'
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一步：建立基线
- en: As a first step, you should set up a baseline to which you will compare any
    experiments you run in the second step. You should not spend too much time on
    this step. Just make sure you have a good enough working baseline.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，你应该建立一个基线，用于比较你在第二步中进行的任何实验。你不应该在这一步花费太多时间。只需确保你有一个足够好的工作基线。
- en: Backbone
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 骨干网络
- en: The backbone describes the pre-trained model you are using as your starting
    point. Don’t spend too much time choosing the perfect backbone for your baseline.
    You should switch out the backbone in the second step and experiment with others
    anyways (see [Model complexity](#d117)) — so just pick one. **How about EfficientNet
    for CV and DeBERTa for (English) NLP problems** [1, 9, 11]?
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 骨干网络描述了你作为起点使用的预训练模型。不要花太多时间选择完美的骨干网络。你应该在第二步中更换骨干网络并尝试其他网络（见[模型复杂性](#d117)）——所以只需选择一个即可。**如何选择EfficientNet用于CV问题和DeBERTa用于（英语）NLP问题**
    [1, 9, 11]？
- en: Batch size
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量大小
- en: Decide on a batch size at this stage of the model development. Don’t change
    the batch size unless necessary because it requires starting the tuning process
    all over again [4, 11].
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型开发的这一阶段决定一个批量大小。除非必要，否则不要改变批量大小，因为这需要重新开始调优过程 [4, 11]。
- en: Usually, increasing the batch size will increase the training speed. Thus, it
    is common practice to use the biggest batch size in powers of two (e.g., 16, 32,
    64, etc.) that fits in the available memory [4].
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，增加批量大小会提高训练速度。因此，常见做法是使用内存中最大可容纳的2的幂次的批量大小（例如16, 32, 64等）[4]。
- en: Number of training steps (epochs)
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练步骤数量（轮次）
- en: '**Number of epochs** — You don’t require many additional training steps for
    fine-tuning a pre-trained model. Usually, **5 epochs is a good starting point**
    for the initial baseline [11].'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练轮次** — 微调预训练模型时不需要很多额外的训练步骤。通常，**5个轮次是一个好的起点**作为初始基线[11]。'
- en: '**Early stopping** — While early stopping has been popular in the past [5],
    it is not so much anymore [1, 4, 11]. Early stopping is a technique that stops
    training when the validation metric does not longer improve to avoid overfitting
    to the training data. However, this technique can cause you to overfit to and
    leak information from the validation set [1, 11].'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**提前停止** — 尽管提前停止在过去很受欢迎[5]，但现在不再如此[1, 4, 11]。提前停止是一种技术，当验证指标不再改善时停止训练，以避免对训练数据的过拟合。然而，这种技术可能导致你对验证集进行过拟合并泄漏信息[1,
    11]。'
- en: Additionally, early stopping always requires a validation set. Thus, you won’t
    be able to retrain the final model on the whole dataset for an extra performance
    boost [1] (see [Retraining on the full dataset](#f816)). Thus, **early stopping
    is not recommended**.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，提前停止总是需要一个验证集。因此，你将无法在整个数据集上重新训练最终模型以获得额外的性能提升[1]（见[在完整数据集上重新训练](#f816)）。因此，**不推荐使用提前停止**。
- en: '**Best checkpoint picking** — There is no common understanding of the best
    practices regarding best checkpoint picking. Best checkpoint picking is the practice
    of training the neural network for a fixed number of runs (without early stopping)
    and then selecting the run with the best validation metric. While the [Deep Learning
    Tuning Playbook](https://github.com/google-research/tuning_playbook) [4] recommends
    using best checkpoint picking, Kaggle Grandmasters don’t recommend it because
    it requires the use of a validation set [1, 11] (see [Retraining on the full dataset](#f816))'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**最佳检查点选择** — 关于最佳检查点选择的最佳实践没有共同的理解。最佳检查点选择是指训练神经网络固定次数（不使用提前停止）后，选择具有最佳验证指标的运行。尽管[深度学习调优手册](https://github.com/google-research/tuning_playbook)
    [4] 推荐使用最佳检查点选择，但Kaggle大师们不推荐，因为它需要使用验证集[1, 11]（见[在完整数据集上重新训练](#f816)）。'
- en: Optimizer and its learning rate
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化器及其学习率
- en: '**Optimizer** — The [Deep Learning Tuning Playbook](https://github.com/google-research/tuning_playbook)
    [4] recommends starting with either…'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**优化器** — [深度学习调优手册](https://github.com/google-research/tuning_playbook) [4]
    推荐从以下之一开始…'
- en: Stochastic gradient descent (SGD) or
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机梯度下降（SGD）或
- en: Adam.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adam。
- en: However, Kaggle Grandmasters shared that SGD requires more tuning efforts to
    achieve similar performance as with Adam and thus recommend Adam [1]. They specifically
    recommend **AdamW as an optimizer** [1].
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Kaggle大师们表示，SGD需要更多的调优工作才能达到与Adam相似的性能，因此推荐Adam[1]。他们特别推荐**AdamW作为优化器**[1]。
- en: '**Learning rate** — While you would need to start with a larger learning rate
    when training a neural network from scratch, for fine-tuning, we can set a **smaller
    initial learning rate of, e.g., 1e-3** [11]. This is because we assume that the
    weights of the pre-trained model are already good and should not be changed too
    much too quickly [2].'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习率** — 虽然在从头训练神经网络时你需要使用较大的学习率，但在微调时，我们可以设置**较小的初始学习率，例如1e-3**[11]。这是因为我们假设预训练模型的权重已经很好，不应过快地进行大幅调整[2]。'
- en: If you are unsure what learning rate to choose, you could also check the paper
    of the model architecture you are using to find out what learning rate they used
    [9].
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不确定选择什么学习率，也可以查看你使用的模型架构的论文，以了解他们使用了什么学习率[9]。
- en: '[**Learning rate scheduler**](/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863)
    — While it is recommended to start with a constant learning rate for the initial
    baseline when training a neural network from scratch [4], you should use a [learning
    rate scheduler](/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863)
    for your initial baseline when fine-tuning [1, 4, 11].'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[**学习率调度器**](/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863)
    — 虽然在从头训练神经网络时建议使用恒定学习率作为初始基线[4]，但在微调时应该使用[学习率调度器](/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863)作为初始基线[1,
    4, 11]。'
- en: Similarly to the optimizer, people like to argue about the best [learning rate
    scheduler](/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863).
    While it was popular to reduce the learning rate when the validation metric reached
    a plateau, this approach is dependent on a validation set [1]. Kaggle Grandmasters
    **recommend the cosine annealing learning rate scheduler** [1, 11], which is shown
    below.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于优化器，人们喜欢争论最佳的[学习率调度器](/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863)。虽然当验证指标达到平台期时，减少学习率的方法曾经很流行，但这种方法依赖于验证集[1]。Kaggle
    大师**推荐余弦退火学习率调度器**[1, 11]，如下所示。
- en: '![](../Images/2bf822f9e00d4b52db4707986c46da7b.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2bf822f9e00d4b52db4707986c46da7b.png)'
- en: Cosine Decay/Annealing Learning Rate Scheduler (Image by the author via [“A
    Visual Guide to Learning Rate Schedulers in PyTorch”](https://medium.com/towards-data-science/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863))
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦衰减/退火学习率调度器（图片由作者提供，通过[《PyTorch 中学习率调度器的视觉指南》](https://medium.com/towards-data-science/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863)）
- en: '**For NLP,** you could keep your learning rate constant when you are not using
    many epochs to fine-tune, and your initial learning rate is already small [1].'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**对于 NLP，**当你使用的训练轮次不多时，你可以保持学习率不变，并且你的初始学习率已经很小[1]。'
- en: Bells and whistles
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 花里胡哨的技术
- en: Every month, there is a fancy new technique you can apply to your training pipeline.
    But for the sake of the baseline, it is recommended to keep it as simple as possible
    and add fancy features later [4].
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 每个月都有一种新奇的技术可以应用到你的训练流程中。但为了基线模型的效果，建议尽量保持简单，并在后续添加新特性[4]。
- en: Regularizing vs. overfitting to the validation set
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化与对验证集的过拟合
- en: A few years ago, early stopping and reducing the learning rate on a plateau
    were considered regularization techniques to avoid overfitting of the model to
    the training set [8]. Today, these techniques are considered to be leaky in terms
    of **overfitting to the validation set** [1].
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 几年前，早期停止和在平台期降低学习率被视为避免模型对训练集过拟合的正则化技术[8]。今天，这些技术被认为在**对验证集的过拟合**方面存在泄漏[1]。
- en: Additionally, any technique dependent on a validation set prevents us from re-training
    the model on our final configuration on the entire dataset, which can help increase
    the model’s performance.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，任何依赖于验证集的技术都会阻止我们在最终配置下对整个数据集重新训练模型，这有助于提高模型性能。
- en: 'Step 2: Increasing Complexity and Improving the Performance in Small Increments'
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2步：逐步增加复杂性并小幅提升性能
- en: You should spend most of your time in this second step. In this step, you will
    run many experiments with adjustments made to your baseline model. You can also
    add all the bells, whistles, and whatever technique is hot that month to your
    model. Just add them in small increments to be able to evaluate their impact.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该将大部分时间花在第二步。在这一阶段，你将进行许多实验，并对基线模型进行调整。你还可以将所有的花里胡哨的技术添加到你的模型中。只需逐步添加，以便能够评估其影响。
- en: For this step, it is recommended to have some sort of [experiment tracking system](https://medium.com/@iamleonie/intro-to-mlops-experiment-tracking-for-machine-learning-858e432bd133)
    in place, whether with pen and paper or an experiment tracking tool.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这一步，建议使用某种[实验跟踪系统](https://medium.com/@iamleonie/intro-to-mlops-experiment-tracking-for-machine-learning-858e432bd133)，无论是纸笔记录还是实验跟踪工具。
- en: Hyperparameter tuning
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数调优
- en: Theoretically, you could maximize your model’s performance by running an automated
    hyperparameter optimization algorithm over the entire search space of possible
    hyperparameters. But this is not practical. Thus, you should first define a search
    space by running a few initial experiments [4].
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，你可以通过在所有可能的超参数搜索空间中运行自动化超参数优化算法来最大化模型性能。但这并不现实。因此，你应该先通过进行一些初步实验来定义搜索空间[4]。
- en: 'The most important hyperparameters to tune are the learning rate and the number
    of epochs. Kaggle Grandmasters recommend the following ranges as starting points
    [11]:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的超参数是学习率和训练轮次。Kaggle 大师推荐以下范围作为起始点[11]：
- en: 'Learning rate: 1e-4 to 1e-3'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率：1e-4 到 1e-3
- en: 'Epochs: 2 to 10'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练轮次：2 到 10
- en: '**For NLP,** you will need only a few epochs because the models are better
    pre-trained than in CV.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**对于 NLP，**你只需少量轮次，因为模型的预训练效果优于计算机视觉领域。'
- en: Because we already have reduced ranges for the hyperparameter search spaces,
    we can use an algorithm to [automate hyperparameter tuning](https://medium.com/@iamleonie/intro-to-mlops-hyperparameter-tuning-9a938d21f894)
    [4, 8]. It is recommended to prefer random search or bayesian optimization over
    grid search [4, 8].
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们已经缩小了超参数搜索空间，所以我们可以使用算法来[自动化超参数调整](https://medium.com/@iamleonie/intro-to-mlops-hyperparameter-tuning-9a938d21f894)[4,
    8]。建议优先使用随机搜索或贝叶斯优化，而不是网格搜索[4, 8]。
- en: Data augmentations
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据增强
- en: A Deep Learning model’s performance heavily relies on the amount of data. Thus,
    increasing the amount of data with augmented data can help improve your model’s
    performance [1, 8, 11].
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型的性能严重依赖于数据的数量。因此，通过增加增强数据的数量，可以帮助提高模型的性能[1, 8, 11]。
- en: 'Common data augmentation in CV are:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉中的常见数据增强包括：
- en: Horizontal or vertical flipping
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 水平或垂直翻转
- en: Rotating
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 旋转
- en: Resizing
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整大小
- en: Random cropping
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机裁剪
- en: Shifting
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移位
- en: Mixup
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mixup
- en: Cutout
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cutout
- en: Cutmix
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cutmix
- en: '![](../Images/a82ff7e165b1304c857801c948f3acfe.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a82ff7e165b1304c857801c948f3acfe.png)'
- en: 'Data Augmentation Techniques: Mixup, Cutout, Cutmix (Image by the author)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强技术：Mixup、Cutout、Cutmix（图片由作者提供）
- en: 'You can find the implementations of cutout, mixup, and cutmix in PyTorch in
    this article:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这篇文章中找到 PyTorch 实现的 cutout、mixup 和 cutmix：
- en: '[](/cutout-mixup-and-cutmix-implementing-modern-image-augmentations-in-pytorch-a9d7db3074ad?source=post_page-----f1aba5a814f--------------------------------)
    [## Cutout, Mixup, and Cutmix: Implementing Modern Image Augmentations in PyTorch'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/cutout-mixup-and-cutmix-implementing-modern-image-augmentations-in-pytorch-a9d7db3074ad?source=post_page-----f1aba5a814f--------------------------------)
    [## Cutout、Mixup 和 Cutmix：在 PyTorch 中实现现代图像增强'
- en: Data augmentation techniques for Computer Vision implemented in Python
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用 Python 实现的计算机视觉数据增强技术
- en: towardsdatascience.com](/cutout-mixup-and-cutmix-implementing-modern-image-augmentations-in-pytorch-a9d7db3074ad?source=post_page-----f1aba5a814f--------------------------------)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/cutout-mixup-and-cutmix-implementing-modern-image-augmentations-in-pytorch-a9d7db3074ad?source=post_page-----f1aba5a814f--------------------------------)'
- en: '**For NLP,** data augmentations that work on CV seem to work on text data as
    well, like random cropping, resizing, or cutout [9]. But not all CV data augmentation
    techniques can be translated to text data directly and thus require their own
    augmentation techniques:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**对于 NLP，** 在计算机视觉中有效的数据增强技术似乎也适用于文本数据，如随机裁剪、调整大小或 cutout [9]。但并非所有计算机视觉数据增强技术都能直接转化为文本数据，因此需要其自身的数据增强技术。'
- en: 'Back translation: Translating text to another language and then back to the
    original language'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向翻译：将文本翻译成另一种语言，然后再翻译回原始语言
- en: 'Masked Entity Language Modeling (MELM) [13]: Randomly masking a percentage
    of tokens in a sentence (similar to cutout in CV [9])'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 掩蔽实体语言建模（MELM）[13]：随机掩蔽句子中一定比例的标记（类似于计算机视觉中的 cutout [9]）
- en: 'Replacing: Replacing words in a sentence with synonyms.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 替换：用同义词替换句子中的词汇。
- en: Model complexity
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型复杂性
- en: The best suited backbone will be different for every problem. Thus, you should
    try a few different backbones [1, 11].
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 最适合的骨干网会因问题而异。因此，你应该尝试几种不同的骨干网[1, 11]。
- en: Garbage in, garbage out
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 垃圾进，垃圾出
- en: Any ML model stands or falls with the quality of data you feed it. Thus, it
    is essential to experiment with different configurations for your model’s training
    pipeline and review the training data. A good approach is to **review** which
    samples your model is predicting well and which it is performing poorly on.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 任何机器学习模型的好坏取决于你输入的数据质量。因此，实验不同的模型训练管道配置并审查训练数据是至关重要的。一个好的方法是**审查**模型预测效果好的样本和表现不佳的样本。
- en: If you **visualize** these samples, you will make your life much easier [9].
    This will give you a hint about possible issues in the data or training pipeline.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你**可视化**这些样本，你会使自己的工作更轻松[9]。这将给你一个关于数据或训练流程可能存在的问题的提示。
- en: 'Step 3: Squeezing the Last Bits of Performance'
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 3：挤出最后的性能
- en: Once you reach the end of experimentation (e.g., deadline or satisfactory performance),
    you can add some finishing touches and squeeze out the last bits of performance
    of your model.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你完成实验（例如，截止日期或满意的性能），你可以进行一些最后的调整，挤出模型的最后性能。
- en: Retraining on the full dataset
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在完整数据集上重新训练
- en: Deep Learning models are data-hungry. That’s why you can boost your model’s
    performance if you retrain your model with the final training configuration on
    the full dataset [1, 11].
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型对数据需求量大。这就是为什么如果你用最终的训练配置在完整数据集上重新训练模型，可以提升模型的性能[1, 11]。
- en: Ensembles
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成方法
- en: 'While a few years ago, big ensembles were fashionable, today, smaller ensembles
    of up to three models are en vogue [1]. When ensembling models, you could try
    the following strategies:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 几年前，大型集成模型曾经很流行，而今天，最多三个模型的小型集成更为时尚[1]。在集成模型时，你可以尝试以下策略：
- en: Combine models with the same training configuration but with different seeds
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合具有相同训练配置但不同种子的模型
- en: Combine different models with a lot of diversity (e.g., different backbones)
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合具有多样性的不同模型（例如，不同的骨干网络）
- en: Adding more bells and whistles
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加更多花里胡哨的东西
- en: If you are still unsatisfied with your results, try pseudo-labeling [9] or test
    time augmentations [11] and see if you can squeeze that little bit of performance.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对结果仍不满意，尝试伪标签[9]或测试时增强[11]，看看能否挤出一点点性能提升。
- en: Conclusion
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: If you have just finished your “Introduction to Deep Learning” course and want
    to level up your skills, here is the high-level recipe to approach any Deep Learning
    problem.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你刚刚完成了“深度学习入门”课程，并希望提升技能，这里有一种高层次的方法来处理任何深度学习问题。
- en: 'Start with a simple baseline:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个简单的基线开始：
- en: '**Backbone:** Just one to start with — you should experiment with others anyways:
    EfficientNet for CV and DeBERTa for NLP'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础模型：** 先从一个开始——无论如何你都应该尝试其他模型：用于计算机视觉的EfficientNet和用于自然语言处理的DeBERTa'
- en: '**Batch size (fixed):** Check what is the biggest batch size in powers of two
    (e.g., 16, 32, 64, etc.) to fit in the available memory, and then don’t change
    it unless really necessary.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量大小（固定）：** 检查在可用内存中适应的最大批量大小（例如，16、32、64等），然后除非非常必要，否则不要改变它。'
- en: '**Number of training steps:** 5 epochs without early stopping'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练步数：** 5个epoch，无早停'
- en: '**Optimizer (fixed):** AdamW'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化器（固定）：** AdamW'
- en: '**Learning rate:** 1e-3'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率：** 1e-3'
- en: '**Learning rate scheduler (fixed):** Cosine Annealing'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率调度器（固定）：** 余弦退火'
- en: 'Set up an [experiment tracking system](https://medium.com/@iamleonie/intro-to-mlops-experiment-tracking-for-machine-learning-858e432bd133)
    in place and get cracking:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 建立一个[实验跟踪系统](https://medium.com/@iamleonie/intro-to-mlops-experiment-tracking-for-machine-learning-858e432bd133)并开始行动：
- en: '**Hyperparameter tuning:** Start with tuning of the learning rate (0.0001–0.001)
    and epochs (2–10). Use random search or bayesian optimization for [automated hyperparameter
    tuning](https://medium.com/@iamleonie/intro-to-mlops-hyperparameter-tuning-9a938d21f894).'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**超参数调整：** 从学习率（0.0001–0.001）和epoch（2–10）的调整开始。使用随机搜索或贝叶斯优化进行[自动超参数调整](https://medium.com/@iamleonie/intro-to-mlops-hyperparameter-tuning-9a938d21f894)。'
- en: '**Data augmentation:** The more, the merrier. Mixup, cutout, cutmix are especially
    popular because of their effectiveness.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据增强：** 增多为妙。Mixup、cutout、cutmix因其有效性而特别流行。'
- en: '**Backbone:** Try different backbones and model complexities'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础模型：** 尝试不同的骨干网络和模型复杂性'
- en: '**Bells and whistles:** In this step, you can go wild with trying every hot
    new trend in Deep Learning and checking if it will improve your model''s performance'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**花里胡哨：** 在这一步，你可以大胆尝试每一个新兴的深度学习趋势，看看它是否会提升你的模型表现。'
- en: To squeeze out the last bit of performance, you should select up to three of
    your best training configurations, retrain a model on the entire dataset, and
    ensemble them.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了挤出最后一丝性能，你应该选择最多三个最佳训练配置，重新在整个数据集上训练模型，并进行集成。
- en: Enjoyed This Story?
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 喜欢这个故事吗？
- en: '[*Subscribe for free*](https://medium.com/subscribe/@iamleonie) *to get notified
    when I publish a new story.*'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[*免费订阅*](https://medium.com/subscribe/@iamleonie) *以便在我发布新故事时收到通知。*'
- en: '[](https://medium.com/@iamleonie/subscribe?source=post_page-----f1aba5a814f--------------------------------)
    [## Get an email whenever Leonie Monigatti publishes.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@iamleonie/subscribe?source=post_page-----f1aba5a814f--------------------------------)
    [## 当Leonie Monigatti发布时获取电子邮件。'
- en: Get an email whenever Leonie Monigatti publishes. By signing up, you will create
    a Medium account if you don’t already…
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 当Leonie Monigatti发布时获取电子邮件。如果你还没有账户，通过注册你将创建一个Medium账户……
- en: medium.com](https://medium.com/@iamleonie/subscribe?source=post_page-----f1aba5a814f--------------------------------)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/@iamleonie/subscribe?source=post_page-----f1aba5a814f--------------------------------)'
- en: '*Find me on* [*LinkedIn*](https://www.linkedin.com/in/804250ab/),[*Twitter*](https://twitter.com/helloiamleonie)*,
    and* [*Kaggle*](https://www.kaggle.com/iamleonie)*!*'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '*在* [*LinkedIn*](https://www.linkedin.com/in/804250ab/)、[*Twitter*](https://twitter.com/helloiamleonie)*和*
    [*Kaggle*](https://www.kaggle.com/iamleonie)*上找到我！*'
- en: References
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] S. Bhutani with H20.ai (2023). [Best Practises for Training ML Models |
    @ChaiTimeDataScience #160](https://www.youtube.com/watch?v=_mzrfMA8Qx4) presented
    on YouTube in January 2023.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] S. Bhutani 与 H20.ai (2023). [训练机器学习模型的最佳实践 | @ChaiTimeDataScience #160](https://www.youtube.com/watch?v=_mzrfMA8Qx4)
    于2023年1月在YouTube上发布。'
- en: '[2] CS231n Convolutional Neural Networks for Visual Recognition (2023). [Transfer
    Learning](https://cs231n.github.io/transfer-learning/) (accessed February 3rd,
    2023)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] CS231n 视觉识别的卷积神经网络 (2023)。[迁移学习](https://cs231n.github.io/transfer-learning/)（访问日期：2023年2月3日）'
- en: '[3] J. Devlin, M. M. Chang,K. Lee & K. Toutanova (2018). Bert: Pre-training
    of deep bidirectional transformers for language understanding. *arXiv preprint
    arXiv:1810.04805*.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] J. Devlin, M. M. Chang, K. Lee & K. Toutanova (2018). BERT：用于语言理解的深度双向转换器的预训练。*arXiv预印本
    arXiv:1810.04805*。'
- en: '[4,] V. Godbole, G. E. Dahl, J. Gilmer, C. J. Shallue and Z. Nado (2023). [Deep
    Learning Tuning Playbook](https://github.com/google-research/tuning_playbook)
    (Version 1.0) (accessed February 3rd, 2023)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[4,] V. Godbole, G. E. Dahl, J. Gilmer, C. J. Shallue 和 Z. Nado (2023). [深度学习调优手册](https://github.com/google-research/tuning_playbook)（第1.0版）（访问日期：2023年2月3日）'
- en: '[5] P. He, X. Liu, J. Gao, & W. Chen (2020). Deberta: Decoding-enhanced bert
    with disentangled attention. *arXiv preprint arXiv:2006.03654*.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] P. He, X. Liu, J. Gao, & W. Chen (2020). Deberta：通过解耦注意力的解码增强BERT。*arXiv预印本
    arXiv:2006.03654*。'
- en: '[6] K. He, X. Zhang, S. Ren, & J. Sun (2016). Deep residual learning for image
    recognition. In *Proceedings of the IEEE conference on computer vision and pattern
    recognition* (pp. 770–778).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] K. He, X. Zhang, S. Ren, & J. Sun (2016). 深度残差学习用于图像识别。载于 *IEEE计算机视觉与模式识别会议论文集*（第770–778页）。'
- en: '[7] G. Huang, Z. Liu, L. Van Der Maaten & K. Q. Weinberger (2017). Densely
    connected convolutional networks. In *Proceedings of the IEEE conference on computer
    vision and pattern recognition* (pp. 4700–4708).'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] G. Huang, Z. Liu, L. Van Der Maaten & K. Q. Weinberger (2017). 密集连接卷积网络。载于
    *IEEE计算机视觉与模式识别会议论文集*（第4700–4708页）。'
- en: '[8] A. Karpathy (2019). [A Recipe for Training Neural Networks](http://karpathy.github.io/2019/04/25/recipe/)
    (accessed February 3rd, 2023)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] A. Karpathy (2019). [训练神经网络的配方](http://karpathy.github.io/2019/04/25/recipe/)（访问日期：2023年2月3日）'
- en: '[9] D. Kłeczek with Munich NLP (2023). [Ten Proven Techniques to Improve Your
    NLP Model Training](https://www.youtube.com/watch?v=UbL1QMwDpec) (accessed February
    9th, 2023)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] D. Kłeczek 与慕尼黑NLP (2023)。[提升NLP模型训练效果的十种有效技巧](https://www.youtube.com/watch?v=UbL1QMwDpec)（访问日期：2023年2月9日）'
- en: '[10] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen & V. Stoyanov (2019).
    Roberta: A robustly optimized bert pretraining approach. *arXiv preprint arXiv:1907.11692*.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen & V. Stoyanov (2019).
    RoBERTa：一种稳健优化的BERT预训练方法。*arXiv预印本 arXiv:1907.11692*。'
- en: '[11] P. Singer and Y. Babakhin (2022). [Practical Tips for Deep Transfer Learning](https://drive.google.com/drive/folders/1VtJF-zPbXc-V-UDl2bDgWJp05DnKZpQH)
    presented at Kaggle Days Paris in November 2022.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] P. Singer 和 Y. Babakhin (2022). [深度迁移学习的实用技巧](https://drive.google.com/drive/folders/1VtJF-zPbXc-V-UDl2bDgWJp05DnKZpQH)
    于2022年11月在Kaggle Days Paris上展示。'
- en: '[12] M. Tan, & Q. Le (2019). Efficientnet: Rethinking model scaling for convolutional
    neural networks. In *International conference on machine learning* (pp. 6105–6114).
    PMLR'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] M. Tan, & Q. Le (2019). EfficientNet：重新思考卷积神经网络的模型缩放。载于 *国际机器学习会议*（第6105–6114页）。PMLR'
- en: '[13] R. Zhou, X. Li, R. He, L. Bing, E. Cambria, L. Si, & C. Miao (2021). MELM:
    Data augmentation with masked entity language modeling for low-resource NER. *arXiv
    preprint arXiv:2108.13655*.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] R. Zhou, X. Li, R. He, L. Bing, E. Cambria, L. Si, & C. Miao (2021). MELM：通过掩蔽实体语言建模进行低资源NER的数据增强。*arXiv预印本
    arXiv:2108.13655*。'
