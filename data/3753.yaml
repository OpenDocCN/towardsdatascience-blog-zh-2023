- en: LLM+RAG-Based Question Answering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŸºäºLLM+RAGçš„é—®é¢˜å›ç­”
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/llm-rag-based-question-answering-6a405c8ad38a?source=collection_archive---------0-----------------------#2023-12-25](https://towardsdatascience.com/llm-rag-based-question-answering-6a405c8ad38a?source=collection_archive---------0-----------------------#2023-12-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/llm-rag-based-question-answering-6a405c8ad38a?source=collection_archive---------0-----------------------#2023-12-25](https://towardsdatascience.com/llm-rag-based-question-answering-6a405c8ad38a?source=collection_archive---------0-----------------------#2023-12-25)
- en: How to do poorly on Kaggle, and learn about RAG+LLM from it
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¦‚ä½•åœ¨Kaggleä¸Šè¡¨ç°ä¸ä½³ï¼Œå¹¶ä»ä¸­å­¦ä¹ RAG+LLM
- en: '[](https://teemukanstren.medium.com/?source=post_page-----6a405c8ad38a--------------------------------)[![Teemu
    KanstrÃ©n](../Images/8ad278d60d1fa3f794fccb4c61d607ce.png)](https://teemukanstren.medium.com/?source=post_page-----6a405c8ad38a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6a405c8ad38a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6a405c8ad38a--------------------------------)
    [Teemu KanstrÃ©n](https://teemukanstren.medium.com/?source=post_page-----6a405c8ad38a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://teemukanstren.medium.com/?source=post_page-----6a405c8ad38a--------------------------------)[![Teemu
    KanstrÃ©n](../Images/8ad278d60d1fa3f794fccb4c61d607ce.png)](https://teemukanstren.medium.com/?source=post_page-----6a405c8ad38a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6a405c8ad38a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6a405c8ad38a--------------------------------)
    [Teemu KanstrÃ©n](https://teemukanstren.medium.com/?source=post_page-----6a405c8ad38a--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9fc0679190dc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-rag-based-question-answering-6a405c8ad38a&user=Teemu+Kanstr%C3%A9n&userId=9fc0679190dc&source=post_page-9fc0679190dc----6a405c8ad38a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6a405c8ad38a--------------------------------)
    Â·23 min readÂ·Dec 25, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6a405c8ad38a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-rag-based-question-answering-6a405c8ad38a&user=Teemu+Kanstr%C3%A9n&userId=9fc0679190dc&source=-----6a405c8ad38a---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9fc0679190dc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-rag-based-question-answering-6a405c8ad38a&user=Teemu+Kanstr%C3%A9n&userId=9fc0679190dc&source=post_page-9fc0679190dc----6a405c8ad38a---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6a405c8ad38a--------------------------------)
    Â·23åˆ†é’Ÿé˜…è¯»Â·2023å¹´12æœˆ25æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6a405c8ad38a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-rag-based-question-answering-6a405c8ad38a&user=Teemu+Kanstr%C3%A9n&userId=9fc0679190dc&source=-----6a405c8ad38a---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6a405c8ad38a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-rag-based-question-answering-6a405c8ad38a&source=-----6a405c8ad38a---------------------bookmark_footer-----------)![](../Images/f497014da435709dae04a493366a7919.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6a405c8ad38a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-rag-based-question-answering-6a405c8ad38a&source=-----6a405c8ad38a---------------------bookmark_footer-----------)![](../Images/f497014da435709dae04a493366a7919.png)'
- en: Image generated with ChatGPT+/DALL-E3, asking for an illustrative image for
    an article about RAG.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒç”±ChatGPT+/DALL-E3ç”Ÿæˆï¼Œå±•ç¤ºäº†å…³äºRAGçš„æ–‡ç« çš„æ’å›¾ã€‚
- en: Retrieval Augmented Generation (RAG) seems to be quite popular these days. Along
    the wave of Large Language Models (LLMâ€™s), it is one of the popular techniques
    to get LLMâ€™s to perform better on specific tasks such as question answering on
    in-house documents. Some time ago, I played on a [Kaggle competition](https://www.kaggle.com/competitions/kaggle-llm-science-exam)
    that allowed me to try it out and learn a bit better than random experiments on
    my own. Here are a few learnings from that and the following experiments while
    writing this article.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¼¼ä¹ç°åœ¨ç›¸å½“å—æ¬¢è¿ã€‚éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…´èµ·ï¼Œå®ƒæˆä¸ºäº†ä½¿LLMåœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½çš„çƒ­é—¨æŠ€æœ¯ä¹‹ä¸€ï¼Œæ¯”å¦‚å¯¹å†…éƒ¨æ–‡æ¡£è¿›è¡Œé—®ç­”ã€‚å‰æ®µæ—¶é—´ï¼Œæˆ‘å‚åŠ äº†ä¸€ä¸ª
    [Kaggleæ¯”èµ›](https://www.kaggle.com/competitions/kaggle-llm-science-exam)ï¼Œè¿™è®©æˆ‘èƒ½å¤Ÿå°è¯•å®ƒï¼Œå¹¶æ¯”è‡ªå·±éšæ„å®éªŒå­¦åˆ°æ›´å¤šä¸€äº›ã€‚ä»¥ä¸‹æ˜¯ä»è¿™äº›å®éªŒä¸­è·å¾—çš„ä¸€äº›ç»éªŒæ•™è®­ã€‚
- en: All images, unless otherwise noted, are by the author. Generated with the help
    of ChatGPT+/DALL-E3 (where noted), or taken from my personal Jupyter notebooks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤éå¦æœ‰è¯´æ˜ï¼Œå¦åˆ™æ‰€æœ‰å›¾ç‰‡å‡ç”±ä½œè€…æä¾›ã€‚ç”Ÿæˆå·¥å…·ä¸º ChatGPT+/DALL-E3ï¼ˆå¦‚æœ‰æ³¨æ˜ï¼‰ï¼Œæˆ–å–è‡ªæˆ‘ä¸ªäººçš„ Jupyter ç¬”è®°æœ¬ã€‚
- en: RAG Overview
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAG æ¦‚è¿°
- en: RAG has two main parts, retrieval and generation. In the first part, retrieval
    is used to fetch (chunks of) documents related to the query of interest. Generation
    uses those fetched chunks as added input, called *context*, to the answer generation
    model in the second part. This added context is intended to give the generator
    more up-to-date, hopefully better, information to base its generated answer on
    than just its base training data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: RAG æœ‰ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šæ£€ç´¢å’Œç”Ÿæˆã€‚åœ¨ç¬¬ä¸€éƒ¨åˆ†ä¸­ï¼Œæ£€ç´¢ç”¨äºè·å–ä¸æŸ¥è¯¢ç›¸å…³çš„ï¼ˆå—ï¼‰æ–‡æ¡£ã€‚ç”Ÿæˆåˆ™ä½¿ç”¨è¿™äº›æ£€ç´¢åˆ°çš„å—ä½œä¸ºé¢å¤–è¾“å…¥ï¼Œå³ *context*ï¼Œä¼ é€’ç»™ç¬¬äºŒéƒ¨åˆ†çš„ç­”æ¡ˆç”Ÿæˆæ¨¡å‹ã€‚è¿™ä¸ªé™„åŠ çš„ä¸Šä¸‹æ–‡æ—¨åœ¨ä¸ºç”Ÿæˆå™¨æä¾›æ¯”åŸºæœ¬è®­ç»ƒæ•°æ®æ›´åŠæ—¶ã€æ›´å¥½çš„ä¿¡æ¯ï¼Œä»¥ç”Ÿæˆç­”æ¡ˆã€‚
- en: Building the RAG Input, or Chunking Text
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ„å»º RAG è¾“å…¥ï¼Œæˆ–æ–‡æœ¬åˆ†å—
- en: LLMâ€™s have a maximum context or sequence window length they can handle, and
    the generated input context for RAG needs to be short enough to fit into this
    sequence window. We want to fit as much relevant information into this context
    as possible, so getting the best â€œchunksâ€ of text from the potential input documents
    is important. These chunks should optimally be the most relevant ones for generating
    the correct answer to the question posed to the RAG system.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: LLM çš„æœ€å¤§ä¸Šä¸‹æ–‡æˆ–åºåˆ—çª—å£é•¿åº¦æ˜¯å®ƒä»¬å¯ä»¥å¤„ç†çš„èŒƒå›´ï¼ŒRAG ç”Ÿæˆçš„è¾“å…¥ä¸Šä¸‹æ–‡éœ€è¦è¶³å¤ŸçŸ­ä»¥é€‚åº”è¿™ä¸ªåºåˆ—çª—å£ã€‚æˆ‘ä»¬å¸Œæœ›å°†å°½å¯èƒ½å¤šçš„ç›¸å…³ä¿¡æ¯çº³å…¥è¿™ä¸ªä¸Šä¸‹æ–‡ï¼Œå› æ­¤ä»æ½œåœ¨çš„è¾“å…¥æ–‡æ¡£ä¸­è·å–æœ€ä½³çš„â€œå—â€æ–‡æœ¬éå¸¸é‡è¦ã€‚è¿™äº›å—åº”å½“æ˜¯ç”Ÿæˆæ­£ç¡®ç­”æ¡ˆæ‰€éœ€çš„æœ€ç›¸å…³çš„å†…å®¹ã€‚
- en: As a first step, the input text is typically chunked into smaller pieces. A
    basic pre-processing step in RAG is converting these chunks into embeddings using
    a specific embedding model. A typical sequence window for an embedding model is
    512 tokens, which also makes a practical target for chunk size. Once the documents
    are chunked and encoded into embeddings, a similarity search using the embeddings
    can be performed to build the context for generating the answer.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºç¬¬ä¸€æ­¥ï¼Œè¾“å…¥æ–‡æœ¬é€šå¸¸ä¼šè¢«åˆ†å—æˆæ›´å°çš„ç‰‡æ®µã€‚RAG çš„ä¸€ä¸ªåŸºæœ¬é¢„å¤„ç†æ­¥éª¤æ˜¯ä½¿ç”¨ç‰¹å®šçš„åµŒå…¥æ¨¡å‹å°†è¿™äº›å—è½¬æ¢ä¸ºåµŒå…¥ã€‚ä¸€ä¸ªå…¸å‹çš„åµŒå…¥æ¨¡å‹çš„åºåˆ—çª—å£ä¸º 512
    ä¸ª tokensï¼Œè¿™ä¹Ÿä½¿å…¶æˆä¸ºå®é™…çš„åˆ†å—ç›®æ ‡ã€‚ä¸€æ—¦æ–‡æ¡£è¢«åˆ†å—å¹¶ç¼–ç ä¸ºåµŒå…¥ï¼Œå°±å¯ä»¥ä½¿ç”¨è¿™äº›åµŒå…¥è¿›è¡Œç›¸ä¼¼æ€§æœç´¢ï¼Œä»¥æ„å»ºç”Ÿæˆç­”æ¡ˆçš„ä¸Šä¸‹æ–‡ã€‚
- en: 'I have found [Langchain](https://github.com/langchain-ai/langchain) to provide
    useful tools for input loading and chunking. For example, chunking a document
    with Langchain (in this case, using tokenizer for [Flan-T5-Large](https://huggingface.co/google/flan-t5-large)
    model) is as simple as:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å‘ç° [Langchain](https://github.com/langchain-ai/langchain) æä¾›äº†æœ‰ç”¨çš„å·¥å…·ç”¨äºè¾“å…¥åŠ è½½å’Œæ–‡æœ¬åˆ†å—ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨
    Langchain å¯¹æ–‡æ¡£è¿›è¡Œåˆ†å—ï¼ˆåœ¨æ­¤æƒ…å†µä¸‹ï¼Œä½¿ç”¨ [Flan-T5-Large](https://huggingface.co/google/flan-t5-large)
    æ¨¡å‹çš„åˆ†è¯å™¨ï¼‰æ˜¯éå¸¸ç®€å•çš„ï¼š
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This produces the following two chunks:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†ç”Ÿæˆä»¥ä¸‹ä¸¤ä¸ªå—ï¼š
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In the above code, *chunk_size* 12 tells LangChain to aim for a maximum of
    12 tokens per chunk. Depending on the text structure, [this may not always be
    100% exact](https://stackoverflow.com/questions/76633836/what-does-langchain-charactertextsplitters-chunk-size-param-even-do).
    However, in my experience it works generally well. Something to keep in mind is
    the difference between tokens vs words. Here is an example of tokenizing the above
    *section_text*:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œ*chunk_size* 12 å‘Šè¯‰ LangChain æ—¨åœ¨æ¯ä¸ªå—æœ€å¤šåŒ…å« 12 ä¸ª tokenã€‚æ ¹æ®æ–‡æœ¬ç»“æ„ï¼Œ[è¿™å¯èƒ½å¹¶ä¸æ€»æ˜¯ 100%
    ç²¾ç¡®](https://stackoverflow.com/questions/76633836/what-does-langchain-charactertextsplitters-chunk-size-param-even-do)ã€‚ç„¶è€Œï¼Œæ ¹æ®æˆ‘çš„ç»éªŒï¼Œè¿™é€šå¸¸æ•ˆæœå¾ˆå¥½ã€‚éœ€è¦è®°ä½çš„æ˜¯
    tokens å’Œå•è¯ä¹‹é—´çš„åŒºåˆ«ã€‚ä¸‹é¢æ˜¯å¯¹ä¸Šè¿° *section_text* è¿›è¡Œåˆ†è¯çš„ä¸€ä¸ªç¤ºä¾‹ï¼š
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Resulting output tokens:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆçš„è¾“å‡º tokensï¼š
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Most words in the *section_text* form a token on their own, as they are [common
    words in texts](https://huggingface.co/docs/transformers/tokenizer_summary). However,
    for special forms of words, or domain words this can be a bit more complicated.
    For example, here the word â€œuncharacteristicâ€ becomes three tokens [â€œ *un*â€, â€œ
    *character*â€, â€œ *istic*â€]. This is because the model tokenizer knows those 3 partial
    sub-words but not the entire word (â€œ *uncharacteristic* â€œ). Each model comes with
    its own tokenizer to match these rules in input and model training.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•° *section_text* ä¸­çš„å•è¯æœ¬èº«å½¢æˆä¸€ä¸ª tokenï¼Œå› ä¸ºå®ƒä»¬æ˜¯[æ–‡æœ¬ä¸­çš„å¸¸è§å•è¯](https://huggingface.co/docs/transformers/tokenizer_summary)ã€‚ç„¶è€Œï¼Œå¯¹äºç‰¹æ®Šå½¢å¼çš„å•è¯æˆ–é¢†åŸŸè¯æ±‡ï¼Œè¿™å¯èƒ½ä¼šæ›´å¤æ‚ã€‚ä¾‹å¦‚ï¼Œåœ¨è¿™é‡Œï¼Œâ€œuncharacteristicâ€
    è¿™ä¸ªè¯å˜æˆäº†ä¸‰ä¸ª tokens [â€œ *un*â€ï¼Œ â€œ *character*â€ï¼Œ â€œ *istic*â€]ã€‚è¿™æ˜¯å› ä¸ºæ¨¡å‹çš„åˆ†è¯å™¨çŸ¥é“è¿™ä¸‰ä¸ªéƒ¨åˆ†è¯æ±‡ï¼Œä½†ä¸çŸ¥é“æ•´ä¸ªå•è¯ï¼ˆâ€œ
    *uncharacteristic*â€ï¼‰ã€‚æ¯ä¸ªæ¨¡å‹éƒ½æœ‰è‡ªå·±çš„åˆ†è¯å™¨æ¥åŒ¹é…è¾“å…¥å’Œæ¨¡å‹è®­ç»ƒä¸­çš„è¿™äº›è§„åˆ™ã€‚
- en: In chunking, the [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter)
    from Langchain used in above code counts these tokens, and looks for given separators
    to split the text into chunks as requested. Trials with different chunk sizes
    may be useful. In my Kaggle experiment I started with the maximum size for the
    embedding model, which was 512 tokens. Then proceeded to try chunk sizes of 256,
    128, and 64 tokens.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åˆ†å—ä¸­ï¼Œæ¥è‡ªLangchainçš„[RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter)ç”¨äºä¸Šè¿°ä»£ç ä¸­ï¼Œè®¡ç®—è¿™äº›ä»¤ç‰Œï¼Œå¹¶å¯»æ‰¾ç»™å®šçš„åˆ†éš”ç¬¦å°†æ–‡æœ¬æ‹†åˆ†æˆè¯·æ±‚çš„å—ã€‚ä¸åŒå—å¤§å°çš„è¯•éªŒå¯èƒ½ä¼šæœ‰ç”¨ã€‚åœ¨æˆ‘çš„Kaggleå®éªŒä¸­ï¼Œæˆ‘ä»åµŒå…¥æ¨¡å‹çš„æœ€å¤§å¤§å°å¼€å§‹ï¼Œå³512ä¸ªä»¤ç‰Œã€‚ç„¶åå°è¯•äº†256ã€128å’Œ64ä¸ªä»¤ç‰Œçš„å—å¤§å°ã€‚
- en: Example RAG Query
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹RAGæŸ¥è¯¢
- en: 'The [Kaggle competition](https://www.kaggle.com/competitions/kaggle-llm-science-exam)
    I mentioned was about multiple-choice question answering based on Wikipedia data.
    The task was to select the correct answer option from the multiple options for
    each question. The obvious approach was to use RAG to find required information
    from a Wikipedia dump, and use it to generate the correct. Here is the first question
    from competition data, and its answer options to illustrate:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æåˆ°çš„[Kaggleæ¯”èµ›](https://www.kaggle.com/competitions/kaggle-llm-science-exam)æ˜¯åŸºäºç»´åŸºç™¾ç§‘æ•°æ®çš„å¤šé¡¹é€‰æ‹©é¢˜å›ç­”ã€‚ä»»åŠ¡æ˜¯ä»å¤šä¸ªé€‰é¡¹ä¸­é€‰æ‹©æ¯ä¸ªé—®é¢˜çš„æ­£ç¡®ç­”æ¡ˆã€‚æ˜¾è€Œæ˜“è§çš„æ–¹æ³•æ˜¯ä½¿ç”¨RAGä»ç»´åŸºç™¾ç§‘æ•°æ®ä¸­æ‰¾åˆ°æ‰€éœ€çš„ä¿¡æ¯ï¼Œå¹¶ç”¨å®ƒæ¥ç”Ÿæˆæ­£ç¡®ç­”æ¡ˆã€‚ä»¥ä¸‹æ˜¯æ¯”èµ›æ•°æ®ä¸­çš„ç¬¬ä¸€ä¸ªé—®é¢˜åŠå…¶ç­”æ¡ˆé€‰é¡¹ï¼Œç”¨äºè¯´æ˜ï¼š
- en: '![](../Images/35ff5c28169adc2755e25364da4b542b.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/35ff5c28169adc2755e25364da4b542b.png)'
- en: Example question and answer options A-E.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹é—®é¢˜å’Œç­”æ¡ˆé€‰é¡¹A-Eã€‚
- en: The multiple-choice questions were an interesting topic to try out RAG. But
    the most common RAG use case is, I believe, answering questions based on source
    documents. Kind of like a chatbot, but typically question answering over domain
    specific or (company) internal documents. I use this basic question answering
    use case to demonstrate RAG in this article.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šé¡¹é€‰æ‹©é¢˜æ˜¯å°è¯•RAGçš„ä¸€ä¸ªæœ‰è¶£è¯é¢˜ã€‚ä½†æˆ‘ç›¸ä¿¡ï¼Œæœ€å¸¸è§çš„RAGç”¨ä¾‹æ˜¯æ ¹æ®æºæ–‡æ¡£å›ç­”é—®é¢˜ã€‚æœ‰ç‚¹åƒèŠå¤©æœºå™¨äººï¼Œä½†é€šå¸¸æ˜¯é’ˆå¯¹ç‰¹å®šé¢†åŸŸæˆ–ï¼ˆå…¬å¸ï¼‰å†…éƒ¨æ–‡æ¡£çš„é—®ç­”ã€‚æˆ‘åœ¨æœ¬æ–‡ä¸­ä½¿ç”¨è¿™ä¸ªåŸºæœ¬çš„é—®ç­”ç”¨ä¾‹æ¥å±•ç¤ºRAGã€‚
- en: As an example RAG question for this article, I needed something the LLM would
    not know the answer to directly based on its training data alone. I used Wikipedia
    data, and since it is likely used as part of training data for LLMâ€™s, I needed
    a question related to something after the model was trained. The model I used
    for this article was [Zephyr 7B beta](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta),
    trained in early 2023\. Finally, I settled on asking about the [Google Bard AI
    chatbot](https://bard.google.com/). It has had many developments over the past
    year, after the Zephyr training date. I also have a decent knowledge of Bard to
    evaluate the LLMâ€™s answers. Thus I used â€œ*what is google bard?* â€œ as an example
    question for this article.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºæœ¬æ–‡çš„RAGç¤ºä¾‹é—®é¢˜ï¼Œæˆ‘éœ€è¦ä¸€ä¸ªLLMæ— æ³•ä»…å‡­å…¶è®­ç»ƒæ•°æ®ç›´æ¥å›ç­”çš„é—®é¢˜ã€‚æˆ‘ä½¿ç”¨äº†ç»´åŸºç™¾ç§‘æ•°æ®ï¼Œå› ä¸ºå®ƒå¯èƒ½æ˜¯LLMè®­ç»ƒæ•°æ®çš„ä¸€éƒ¨åˆ†ï¼Œæ‰€ä»¥æˆ‘éœ€è¦ä¸€ä¸ªä¸æ¨¡å‹è®­ç»ƒåç›¸å…³çš„é—®é¢˜ã€‚æˆ‘ä¸ºæœ¬æ–‡ä½¿ç”¨çš„æ¨¡å‹æ˜¯[Zephyr
    7B beta](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)ï¼Œäº2023å¹´åˆè®­ç»ƒå®Œæˆã€‚æœ€åï¼Œæˆ‘å†³å®šè¯¢é—®[Google
    Bard AIèŠå¤©æœºå™¨äºº](https://bard.google.com/)ã€‚å®ƒåœ¨Zephyrè®­ç»ƒæ—¥æœŸä¹‹åçš„ä¸€å¹´é‡Œæœ‰å¾ˆå¤šå‘å±•ã€‚æˆ‘å¯¹Bardä¹Ÿæœ‰ä¸€å®šäº†è§£ï¼Œä»¥è¯„ä¼°LLMçš„ç­”æ¡ˆã€‚å› æ­¤ï¼Œæˆ‘ä½¿ç”¨â€œ*what
    is google bard?*â€ä½œä¸ºæœ¬æ–‡çš„ç¤ºä¾‹é—®é¢˜ã€‚
- en: Embedding Vectors
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åµŒå…¥å‘é‡
- en: 'The first phase of retrieval in RAG is based on the embedding vectors, which
    are really just points in a multidimensional space. They look something like this
    (only the first 10 values here):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: RAGçš„ç¬¬ä¸€é˜¶æ®µæ£€ç´¢åŸºäºåµŒå…¥å‘é‡ï¼Œè¿™äº›å‘é‡å®é™…ä¸Šåªæ˜¯å¤šç»´ç©ºé—´ä¸­çš„ç‚¹ã€‚å®ƒä»¬çœ‹èµ·æ¥åƒè¿™æ ·ï¼ˆè¿™é‡Œåªåˆ—å‡ºäº†å‰10ä¸ªå€¼ï¼‰ï¼š
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'These embedding vectors can be used to compare the words/sentences, and their
    relations, against each other. These vectors can be built using embedding models.
    A nice set of those models with various stats per model can be found on the [MTEB
    leaderboard](https://huggingface.co/spaces/mteb/leaderboard). Using one of those
    models is as simple as this:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›åµŒå…¥å‘é‡å¯ä»¥ç”¨æ¥æ¯”è¾ƒå•è¯/å¥å­åŠå…¶ç›¸äº’å…³ç³»ã€‚è¿™äº›å‘é‡å¯ä»¥é€šè¿‡åµŒå…¥æ¨¡å‹æ„å»ºã€‚å¯ä»¥åœ¨[MTEBæ’è¡Œæ¦œ](https://huggingface.co/spaces/mteb/leaderboard)æ‰¾åˆ°å„ç§ç»Ÿè®¡æ•°æ®çš„æ¨¡å‹é›†ã€‚ä½¿ç”¨è¿™äº›æ¨¡å‹ä¹‹ä¸€å°±åƒè¿™æ ·ç®€å•ï¼š
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The model page on HuggingFace typically shows the example code. The above loads
    the model â€œ [bge-small-en](https://huggingface.co/BAAI/bge-small-en-v1.5) â€œ from
    local disk. To create the embeddings using this model is just:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: HuggingFaceä¸Šçš„æ¨¡å‹é¡µé¢é€šå¸¸æ˜¾ç¤ºç¤ºä¾‹ä»£ç ã€‚ä¸Šè¿°ä»£ç ä»æœ¬åœ°ç£ç›˜åŠ è½½æ¨¡å‹â€œ [bge-small-en](https://huggingface.co/BAAI/bge-small-en-v1.5)
    â€ã€‚ä½¿ç”¨æ­¤æ¨¡å‹åˆ›å»ºåµŒå…¥åªæ˜¯ï¼š
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In this case, the embedding model is used to encode the given question into
    an embedding vector. The vector is the same as the example above:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒåµŒå…¥æ¨¡å‹ç”¨äºå°†ç»™å®šçš„é—®é¢˜ç¼–ç ä¸ºåµŒå…¥å‘é‡ã€‚è¯¥å‘é‡ä¸ä¸Šé¢çš„ç¤ºä¾‹ç›¸åŒï¼š
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The shape (, 384) tells me *q_embeddings* is a single vector (as opposed to
    embedding a list of multiple texts at once) of length 384 floats. The slice above
    shows the first 10 values out of those 384\. Some models use longer vectors for
    more accurate relations, others, like this one, shorter (here 384). Again, [MTEB
    leaderboard](https://huggingface.co/spaces/mteb/leaderboard) has good examples.
    The small ones require less space and computation, larger ones give some improvements
    in representing the relations between chunks, and sometimes sequence length.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å½¢çŠ¶ (, 384) è¡¨ç¤º *q_embeddings* æ˜¯ä¸€ä¸ªé•¿åº¦ä¸º 384 ä¸ªæµ®ç‚¹æ•°çš„å•ä¸€å‘é‡ï¼ˆè€Œä¸æ˜¯ä¸€æ¬¡åµŒå…¥å¤šä¸ªæ–‡æœ¬çš„åˆ—è¡¨ï¼‰ã€‚ä¸Šé¢çš„åˆ‡ç‰‡æ˜¾ç¤ºäº†è¿™
    384 ä¸ªå€¼ä¸­çš„å‰ 10 ä¸ªã€‚æŸäº›æ¨¡å‹ä½¿ç”¨æ›´é•¿çš„å‘é‡ä»¥è·å¾—æ›´å‡†ç¡®çš„å…³ç³»ï¼Œè€Œå…¶ä»–æ¨¡å‹ï¼Œå¦‚æœ¬ä¾‹æ‰€ç¤ºï¼Œåˆ™ä½¿ç”¨è¾ƒçŸ­çš„å‘é‡ï¼ˆæ­¤å¤„ä¸º 384ï¼‰ã€‚å†æ¬¡ï¼Œ[MTEB æ’è¡Œæ¦œ](https://huggingface.co/spaces/mteb/leaderboard)
    æä¾›äº†å¾ˆå¥½çš„ç¤ºä¾‹ã€‚è¾ƒå°çš„å‘é‡éœ€è¦æ›´å°‘çš„ç©ºé—´å’Œè®¡ç®—ï¼Œè€Œè¾ƒå¤§çš„å‘é‡åœ¨è¡¨ç¤ºå—ä¹‹é—´çš„å…³ç³»ä»¥åŠæœ‰æ—¶çš„åºåˆ—é•¿åº¦æ–¹é¢æä¾›äº†ä¸€äº›æ”¹è¿›ã€‚
- en: 'For my RAG similarity search, I first needed embeddings for the question. This
    is the *q_embeddings* above. This needed to be compared against embedding vectors
    of all the searched articles (or their chunks). In this case all the chunked Wikipedia
    articles. To build embedding for all of those:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæˆ‘çš„ RAG ç›¸ä¼¼æ€§æœç´¢ï¼Œæˆ‘é¦–å…ˆéœ€è¦é—®é¢˜çš„åµŒå…¥ã€‚è¿™å°±æ˜¯ä¸Šé¢çš„ *q_embeddings*ã€‚éœ€è¦å°†å…¶ä¸æ‰€æœ‰è¢«æœç´¢æ–‡ç« ï¼ˆæˆ–å…¶å—ï¼‰çš„åµŒå…¥å‘é‡è¿›è¡Œæ¯”è¾ƒã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‰€æœ‰è¢«åˆ†å—çš„ç»´åŸºç™¾ç§‘æ–‡ç« ã€‚è¦ä¸ºæ‰€æœ‰è¿™äº›æ„å»ºåµŒå…¥ï¼š
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Here *article_chunks* is a list of all chunks for all articles from the English
    Wikipedia dump. This way they can be batch-encoded.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œ *article_chunks* æ˜¯æ¥è‡ªè‹±æ–‡ç»´åŸºç™¾ç§‘æ•°æ®è½¬å‚¨çš„æ‰€æœ‰æ–‡ç« çš„æ‰€æœ‰å—çš„åˆ—è¡¨ã€‚è¿™æ ·å®ƒä»¬å¯ä»¥æ‰¹é‡ç¼–ç ã€‚
- en: Vector Databases
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‘é‡æ•°æ®åº“
- en: Implementing similarity search over a large set of documents / document chunks
    is not too complicated at a basic level. A common way is to calculate [cosine
    similarity](https://www.geeksforgeeks.org/how-to-calculate-cosine-similarity-in-python/)
    between the query and document vectors, and sort accordingly. However, at large
    scale, this sometimes gets a bit complicated to manage. Vector databases are tools
    that make this management and search easier / more efficient at scale.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤§è§„æ¨¡æ–‡æ¡£/æ–‡æ¡£å—ä¸Šå®ç°ç›¸ä¼¼æ€§æœç´¢ï¼Œåœ¨åŸºæœ¬å±‚é¢ä¸Šå¹¶ä¸å¤æ‚ã€‚ä¸€ä¸ªå¸¸è§çš„æ–¹æ³•æ˜¯è®¡ç®—æŸ¥è¯¢å’Œæ–‡æ¡£å‘é‡ä¹‹é—´çš„ [ä½™å¼¦ç›¸ä¼¼æ€§](https://www.geeksforgeeks.org/how-to-calculate-cosine-similarity-in-python/)ï¼Œå¹¶è¿›è¡Œæ’åºã€‚ç„¶è€Œï¼Œåœ¨å¤§è§„æ¨¡æ—¶ï¼Œè¿™æœ‰æ—¶ä¼šå˜å¾—æœ‰äº›å¤æ‚ã€‚å‘é‡æ•°æ®åº“æ˜¯ä½¿è¿™ç§ç®¡ç†å’Œæœç´¢åœ¨è§„æ¨¡ä¸Šå˜å¾—æ›´ç®€å•/æ›´é«˜æ•ˆçš„å·¥å…·ã€‚
- en: For example, [Weaviate](https://weaviate.io/) is a vector database that was
    used in [StackOverflowâ€™s AI-based search](https://resources.stackoverflow.co/topic/thought-leadership/stack-overflows-ai-journey-webinar).
    In its latest versions, it can also be used in an [embedded mode](https://weaviate.io/developers/weaviate/installation/embedded),
    which should have made it usable even in a Kaggle notebook. It is also used in
    some [Deeplearning.AI LLM short courses](https://learn.deeplearning.ai/vector-databases-embeddings-applications/lesson/1/introduction),
    so at least seems somewhat popular. Of course, there are many others and it is
    good to make comparisons, this field also evolves fast.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œ[Weaviate](https://weaviate.io/) æ˜¯ä¸€ä¸ªå‘é‡æ•°æ®åº“ï¼Œæ›¾ç”¨äº [StackOverflow çš„åŸºäº AI çš„æœç´¢](https://resources.stackoverflow.co/topic/thought-leadership/stack-overflows-ai-journey-webinar)ã€‚åœ¨å…¶æœ€æ–°ç‰ˆæœ¬ä¸­ï¼Œå®ƒä¹Ÿå¯ä»¥ä»¥
    [åµŒå…¥æ¨¡å¼](https://weaviate.io/developers/weaviate/installation/embedded) ä½¿ç”¨ï¼Œè¿™ä½¿å¾—å®ƒç”šè‡³å¯ä»¥åœ¨
    Kaggle ç¬”è®°æœ¬ä¸­ä½¿ç”¨ã€‚å®ƒä¹Ÿè¢«ç”¨äºä¸€äº› [Deeplearning.AI LLM çŸ­æœŸè¯¾ç¨‹](https://learn.deeplearning.ai/vector-databases-embeddings-applications/lesson/1/introduction)ï¼Œæ‰€ä»¥è‡³å°‘ä¼¼ä¹æœ‰äº›å—æ¬¢è¿ã€‚å½“ç„¶ï¼Œè¿˜æœ‰è®¸å¤šå…¶ä»–å·¥å…·ï¼Œè¿›è¡Œæ¯”è¾ƒæ˜¯å¾ˆå¥½çš„ï¼Œè¿™ä¸ªé¢†åŸŸä¹Ÿåœ¨å¿«é€Ÿå‘å±•ã€‚
- en: In my trials, I used [FAISS](https://github.com/facebookresearch/faiss) from
    Facebook/Meta research as the vector database. FAISS is more of a library than
    a client-server database, and was thus simple to use in a Kaggle notebook. And
    it worked quite nicely.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘çš„è¯•éªŒä¸­ï¼Œæˆ‘ä½¿ç”¨äº†æ¥è‡ª Facebook/Meta ç ”ç©¶çš„ [FAISS](https://github.com/facebookresearch/faiss)
    ä½œä¸ºå‘é‡æ•°æ®åº“ã€‚FAISS æ›´åƒæ˜¯ä¸€ä¸ªåº“ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå®¢æˆ·ç«¯-æœåŠ¡å™¨æ•°æ®åº“ï¼Œå› æ­¤åœ¨ Kaggle ç¬”è®°æœ¬ä¸­ä½¿ç”¨éå¸¸ç®€å•ã€‚å®ƒçš„è¡¨ç°ä¹Ÿå¾ˆä¸é”™ã€‚
- en: Chunked Data and Embeddings
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆ†å—æ•°æ®å’ŒåµŒå…¥
- en: 'Once the chunking and embedding of all the articles was all done, I built a
    Pandas DataFrame with all the relevant information. Here is an example with the
    first 5 chunks of the Wikipedia dump I used, for a document titled *Anarchism*:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ‰€æœ‰æ–‡ç« çš„åˆ†å—å’ŒåµŒå…¥å®Œæˆåï¼Œæˆ‘æ„å»ºäº†ä¸€ä¸ªåŒ…å«æ‰€æœ‰ç›¸å…³ä¿¡æ¯çš„ Pandas DataFrameã€‚ä»¥ä¸‹æ˜¯æˆ‘ä½¿ç”¨çš„ç»´åŸºç™¾ç§‘æ•°æ®è½¬å‚¨å‰ 5 ä¸ªå—çš„ç¤ºä¾‹ï¼Œæ–‡æ¡£æ ‡é¢˜ä¸º
    *æ— æ”¿åºœä¸»ä¹‰*ï¼š
- en: '![](../Images/b0cad38abdd821b3e885baf39e3bcafb.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b0cad38abdd821b3e885baf39e3bcafb.png)'
- en: First 5 chunks from the first article in the Wikipedia dump I used.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä½¿ç”¨çš„ç»´åŸºç™¾ç§‘æ•°æ®è½¬å‚¨ä¸­çš„ç¬¬ä¸€ç¯‡æ–‡ç« çš„å‰ 5 ä¸ªå—ã€‚
- en: 'Each row in this table (a Pandas DataFrame) contains data for a single chunk
    after the chunking process. It has 5 columns:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¡¨æ ¼ä¸­çš„æ¯ä¸€è¡Œï¼ˆä¸€ä¸ª Pandas DataFrameï¼‰åŒ…å«å—åŒ–è¿‡ç¨‹åå•ä¸ªå—çš„æ•°æ®ã€‚å®ƒæœ‰ 5 åˆ—ï¼š
- en: '*chunk_id*: allows me to map chunk embeddings to the chunk text later.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*chunk_id*ï¼šå…è®¸æˆ‘ç¨åå°†å—åµŒå…¥æ˜ å°„åˆ°å—æ–‡æœ¬ã€‚'
- en: '*doc_id*: allows mapping the chunks back to their document.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*doc_id*ï¼šå…è®¸å°†å—æ˜ å°„å›å…¶æ–‡æ¡£ã€‚'
- en: '*doc_title*: for trialing approaches such as adding the doc title to each chunk.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*doc_title*ï¼šç”¨äºå°è¯•ä¸€äº›æ–¹æ³•ï¼Œä¾‹å¦‚å°†æ–‡æ¡£æ ‡é¢˜æ·»åŠ åˆ°æ¯ä¸ªå—ä¸­ã€‚'
- en: '*chunk_title*: article subsection title for the chunk, same purpose as doc_title'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*chunk_title*ï¼šå—çš„æ–‡ç« å­éƒ¨åˆ†æ ‡é¢˜ï¼Œä¸ *doc_title* ç›®çš„ç›¸åŒã€‚'
- en: '*chunk*: the actual chunk text'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*chunk*ï¼šå®é™…çš„å—æ–‡æœ¬'
- en: 'Here are the embeddings for the first five Anarchism chunks, same order as
    the DataFrame above:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯å‰äº”ä¸ªæ— æ”¿åºœä¸»ä¹‰å—çš„åµŒå…¥ï¼Œé¡ºåºä¸ä¸Šé¢çš„ DataFrame ç›¸åŒï¼š
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Each row is partially only shown here, but illustrates the idea.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸€è¡Œåœ¨è¿™é‡Œéƒ¨åˆ†å±•ç¤ºï¼Œä½†è¯´æ˜äº†æ¦‚å¿µã€‚
- en: Seach for Similar Query Embeddings vs Chunk Embeddings
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœç´¢ç›¸ä¼¼çš„æŸ¥è¯¢åµŒå…¥ä¸å—åµŒå…¥
- en: 'Earlier I encoded the query vector for query â€œ *what is google bard?* â€œâ€˜, followed
    by encoding all the article chunks. With these two sets of embeddings, the first
    part of RAG search is simple: finding the documents â€œsemanticallyâ€ closest to
    the query. In practice just calculating a measure such as cosine similarity between
    the query embedding vector and all the chunk vectors, and sorting by the similarity
    score.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æ—©äº›æ—¶å€™ï¼Œæˆ‘å¯¹æŸ¥è¯¢â€œ *what is google bard?* â€è¿›è¡Œäº†ç¼–ç ï¼Œç„¶åç¼–ç äº†æ‰€æœ‰æ–‡ç« å—ã€‚é€šè¿‡è¿™ä¸¤ç»„åµŒå…¥ï¼ŒRAG æœç´¢çš„ç¬¬ä¸€éƒ¨åˆ†å¾ˆç®€å•ï¼šæ‰¾åˆ°â€œè¯­ä¹‰ä¸Šâ€æœ€æ¥è¿‘æŸ¥è¯¢çš„æ–‡æ¡£ã€‚å®é™…ä¸Šï¼Œåªéœ€è®¡ç®—æŸ¥è¯¢åµŒå…¥å‘é‡ä¸æ‰€æœ‰å—å‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå¹¶æŒ‰ç›¸ä¼¼åº¦å¾—åˆ†æ’åºã€‚
- en: 'Here are the top 10 â€œsemanticallyâ€ closest chunks to the *q_embeddings*:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä¸ *q_embeddings* è¯­ä¹‰ä¸Šæœ€æ¥è¿‘çš„å‰ 10 ä¸ªå—ï¼š
- en: '![](../Images/fa786f4b953c673500fa0eeb4ed0bf31.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa786f4b953c673500fa0eeb4ed0bf31.png)'
- en: Top 10 chunks sorted by their cosine similarity with the question.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æŒ‰ç…§ä¸é—®é¢˜çš„ä½™å¼¦ç›¸ä¼¼åº¦æ’åºçš„å‰ 10 ä¸ªå—ã€‚
- en: Each row in this table (DataFrame) represents a chunk. The *sim_score* here
    is the calculated cosine similarity score, and the rows are sorted from highest
    cosine similarity to lowest. The table shows the top 10 highest *sim_score* rows.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¡¨æ ¼ä¸­çš„æ¯ä¸€è¡Œï¼ˆDataFrameï¼‰è¡¨ç¤ºä¸€ä¸ªå—ã€‚è¿™é‡Œçš„ *sim_score* æ˜¯è®¡ç®—çš„ä½™å¼¦ç›¸ä¼¼åº¦å¾—åˆ†ï¼Œè¡Œä»æœ€é«˜ä½™å¼¦ç›¸ä¼¼åº¦åˆ°æœ€ä½æ’åºã€‚è¡¨æ ¼æ˜¾ç¤ºäº†å‰
    10 ä¸ªæœ€é«˜çš„ *sim_score* è¡Œã€‚
- en: Re-ranking
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Re-ranking
- en: A pure embeddings based similarity search is very fast and low-cost in terms
    of computation. However, it is not quite as accurate as some other approaches.
    *Re-ranking* is a term used to describe the process of using another model to
    more accurately sort this initial list of top documents, with a more computationally
    expensive model. This model is usually too expensive to run against all documents
    and chunks, but running it on the set of top chunks after the initial similarity
    search is much more feasible. Re-ranking helps to get a better list of final chunks
    to build the input context for the generation part of RAG.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: çº¯ç²¹åŸºäºåµŒå…¥çš„ç›¸ä¼¼æ€§æœç´¢åœ¨è®¡ç®—ä¸Šéå¸¸å¿«é€Ÿä¸”ä½æˆæœ¬ã€‚ç„¶è€Œï¼Œå®ƒä¸å¦‚å…¶ä»–ä¸€äº›æ–¹æ³•å‡†ç¡®ã€‚*Re-ranking* æ˜¯ä¸€ä¸ªæœ¯è¯­ï¼Œç”¨äºæè¿°ä½¿ç”¨å¦ä¸€ä¸ªæ¨¡å‹æ›´å‡†ç¡®åœ°æ’åºè¿™äº›åˆå§‹æ–‡æ¡£åˆ—è¡¨çš„è¿‡ç¨‹ï¼Œè¿™ç§æ¨¡å‹é€šå¸¸è®¡ç®—æˆæœ¬æ›´é«˜ã€‚è¿™ä¸ªæ¨¡å‹é€šå¸¸åœ¨æ‰€æœ‰æ–‡æ¡£å’Œå—ä¸Šè¿è¡Œçš„æˆæœ¬å¤ªé«˜ï¼Œä½†åœ¨åˆå§‹ç›¸ä¼¼æ€§æœç´¢åçš„å‰å‡ ä¸ªå—ä¸Šè¿è¡Œå°±å¯è¡Œå¾—å¤šã€‚Re-ranking
    å¸®åŠ©è·å¾—æ›´å¥½çš„æœ€ç»ˆå—åˆ—è¡¨ï¼Œä»¥ä¾¿ä¸º RAG çš„ç”Ÿæˆéƒ¨åˆ†å»ºç«‹è¾“å…¥ä¸Šä¸‹æ–‡ã€‚
- en: 'The same [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard)
    that hosts metrics for the embedding models also has re-ranking scores for many
    models. In this case I used the [bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)
    model for re-ranking:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ ·çš„ [MTEB æ’è¡Œæ¦œ](https://huggingface.co/spaces/mteb/leaderboard) ä¹Ÿæ‰˜ç®¡äº†è®¸å¤šæ¨¡å‹çš„ re-ranking
    å¾—åˆ†ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä½¿ç”¨äº† [bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)
    æ¨¡å‹è¿›è¡Œ re-rankingï¼š
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'After adding *rerank_score* to the chunk DataFrame, and sorting with it:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å°† *rerank_score* æ·»åŠ åˆ°å— DataFrame å¹¶ç”¨å®ƒè¿›è¡Œæ’åºåï¼š
- en: '![](../Images/4fe80155ef3b8e0c33a065708d942c8d.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4fe80155ef3b8e0c33a065708d942c8d.png)'
- en: Top 10 chunks sorted by their re-rank score with the question.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æŒ‰ç…§ä¸é—®é¢˜çš„é‡æ–°æ’åºå¾—åˆ†æ’åºçš„å‰ 10 ä¸ªå—ã€‚
- en: Comparing the two tables above (first sorted by *sim_score* vs now by *rerank_score*),
    there are some clear differences. Sorting by the plain similarity score ( *sim_score*)
    from embeddings, the [Tenor page](https://en.wikipedia.org/wiki/Tenor_(website))
    is the 5th most similar chunk. Since Tenor appears to be a GIF search engine hosted
    by Google, I guess it makes some sense to see its embeddings close to the question
    â€œ *what is google bard?* â€œ. But it has nothing really to do with Bard itself,
    except that Tenor is a Google product in a similar domain.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯”è¾ƒä¸Šé¢çš„ä¸¤ä¸ªè¡¨æ ¼ï¼ˆç¬¬ä¸€ä¸ªæŒ‰*sim_score*æ’åºï¼Œç°æŒ‰*rerank_score*æ’åºï¼‰ï¼Œå¯ä»¥çœ‹åˆ°ä¸€äº›æ˜æ˜¾çš„å·®å¼‚ã€‚æŒ‰åµŒå…¥ç”Ÿæˆçš„æ™®é€šç›¸ä¼¼æ€§å¾—åˆ†ï¼ˆ*sim_score*ï¼‰æ’åºï¼Œ
    [Tenoré¡µé¢](https://en.wikipedia.org/wiki/Tenor_(website)) æ˜¯ç¬¬5ä¸ªæœ€ç›¸ä¼¼çš„ç‰‡æ®µã€‚ç”±äºTenorä¼¼ä¹æ˜¯ä¸€ä¸ªç”±Googleæ‰˜ç®¡çš„GIFæœç´¢å¼•æ“ï¼Œæˆ‘æƒ³çœ‹åˆ°å®ƒçš„åµŒå…¥ä¸é—®é¢˜â€œ*what
    is google bard?*â€æ¥è¿‘æ˜¯æœ‰é“ç†çš„ã€‚ä½†å®ƒå®é™…ä¸Šä¸Bardæœ¬èº«æ²¡æœ‰ä»€ä¹ˆå…³ç³»ï¼Œåªæ˜¯Tenoræ˜¯ä¸€ä¸ªåœ¨ç±»ä¼¼é¢†åŸŸçš„Googleäº§å“ã€‚
- en: However, after sorting by the *rerank_score*, the results make much more sense.
    Tenor is gone from the top 10, and only the last two chunks from the top 10 list
    appear to be unrelated. These are about the names â€œBardâ€ and â€œBÃ¥rdâ€. Possibly
    because the best source of information on Google Bard appears to be the [page
    on Google Bard](https://en.wikipedia.org/wiki/Bard_(chatbot)), which in the above
    tables is document with id 6026776\. After that I guess RAG runs out of good article
    matches and goes a bit off-road (BÃ¥rd). Which is also seen in the negative re-rank
    scores for those two last rows/chunks of the table.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œåœ¨æŒ‰*rerank_score*æ’åºåï¼Œç»“æœæ›´æœ‰æ„ä¹‰ã€‚Tenorä»å‰10åä¸­æ¶ˆå¤±äº†ï¼Œå‰10ååˆ—è¡¨ä¸­çš„æœ€åä¸¤ä¸ªç‰‡æ®µä¼¼ä¹ä¸ç›¸å…³ã€‚è¿™äº›ç‰‡æ®µå…³äºâ€œBardâ€å’Œâ€œBÃ¥rdâ€çš„åå­—ã€‚å¯èƒ½æ˜¯å› ä¸ºæœ‰å…³Google
    Bardçš„æœ€ä½³ä¿¡æ¯æ¥æºä¼¼ä¹æ˜¯ [Google Bardé¡µé¢](https://en.wikipedia.org/wiki/Bard_(chatbot))ï¼Œåœ¨ä¸Šè¿°è¡¨æ ¼ä¸­è¿™æ˜¯idä¸º6026776çš„æ–‡æ¡£ã€‚ä¹‹åï¼Œæˆ‘çŒœRAGç”¨å®Œäº†å¥½çš„æ–‡ç« åŒ¹é…ï¼Œå¹¶æœ‰äº›åç¦»äº†æ­£è½¨ï¼ˆBÃ¥rdï¼‰ã€‚è¿™ä¹Ÿå¯ä»¥ä»è¡¨æ ¼æœ€åä¸¤è¡Œ/ç‰‡æ®µçš„è´Ÿé¢é‡æ–°æ’åºå¾—åˆ†ä¸­çœ‹åˆ°ã€‚
- en: Typically there would likely be many relevant documents and chunks across those
    documents, not just the 1 document and 8 chunks as above. But in this case this
    limitation helps illustrate the difference in basic embeddings-based similarity
    search and re-ranking, and how re-ranking can positively affect the end result.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ä¼šæœ‰è®¸å¤šç›¸å…³æ–‡æ¡£å’Œæ–‡æ¡£ä¸­çš„ç‰‡æ®µï¼Œè€Œä¸ä»…ä»…æ˜¯ä¸Šé¢æåˆ°çš„1ä»½æ–‡æ¡£å’Œ8ä¸ªç‰‡æ®µã€‚ä½†æ˜¯åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¿™ç§é™åˆ¶æœ‰åŠ©äºè¯´æ˜åŸºäºåŸºæœ¬åµŒå…¥çš„ç›¸ä¼¼æ€§æœç´¢å’Œé‡æ–°æ’åºä¹‹é—´çš„åŒºåˆ«ï¼Œä»¥åŠé‡æ–°æ’åºå¦‚ä½•ç§¯æåœ°å½±å“æœ€ç»ˆç»“æœã€‚
- en: Building the Context
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ„å»ºä¸Šä¸‹æ–‡
- en: What do we do once we have collected the top chunks for RAG input? We need to
    build the context for the generator model from these chunks. At its simplest,
    this is just a concatenation of the selected top chunks into a long text sequence.
    The maximum length of this sequence in constrained by the used model. As I used
    the [Zephyr 7B model](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta), I
    used 4096 tokens as the maximum length. The [Zephyr page](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)
    gives this as a flexible sequence limit (with sliding attention window). Longer
    context seems better, but it appears [this is not always the case](https://www-cs.stanford.edu/~nfliu/papers/lost-in-the-middle.arxiv2023.pdf).
    Better try it.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬æ”¶é›†äº†RAGè¾“å…¥çš„é¡¶çº§ç‰‡æ®µåï¼Œæˆ‘ä»¬è¯¥æ€ä¹ˆåšï¼Ÿæˆ‘ä»¬éœ€è¦ä»è¿™äº›ç‰‡æ®µä¸­ä¸ºç”Ÿæˆæ¨¡å‹æ„å»ºä¸Šä¸‹æ–‡ã€‚æœ€ç®€å•çš„æ–¹æ³•å°±æ˜¯å°†é€‰æ‹©çš„é¡¶çº§ç‰‡æ®µè¿æ¥æˆä¸€ä¸ªé•¿æ–‡æœ¬åºåˆ—ã€‚è¯¥åºåˆ—çš„æœ€å¤§é•¿åº¦å—æ‰€ç”¨æ¨¡å‹çš„é™åˆ¶ã€‚ç”±äºæˆ‘ä½¿ç”¨äº†
    [Zephyr 7Bæ¨¡å‹](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)ï¼Œæ‰€ä»¥æˆ‘å°†4096ä¸ªæ ‡è®°ä½œä¸ºæœ€å¤§é•¿åº¦ã€‚
    [Zephyré¡µé¢](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)å°†æ­¤ä½œä¸ºä¸€ä¸ªçµæ´»çš„åºåˆ—é™åˆ¶ï¼ˆå¸¦æœ‰æ»‘åŠ¨æ³¨æ„çª—å£ï¼‰ã€‚æ›´é•¿çš„ä¸Šä¸‹æ–‡ä¼¼ä¹æ›´å¥½ï¼Œä½†
    [è¿™å¹¶ä¸æ€»æ˜¯å¦‚æ­¤](https://www-cs.stanford.edu/~nfliu/papers/lost-in-the-middle.arxiv2023.pdf)ã€‚æœ€å¥½å°è¯•ä¸€ä¸‹ã€‚
- en: 'Here is the base code I used to generate the answer with this context:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯æˆ‘ç”¨æ¥ç”Ÿæˆå…·æœ‰æ­¤ä¸Šä¸‹æ–‡çš„ç­”æ¡ˆçš„åŸºæœ¬ä»£ç ï¼š
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As noted, in this case the context was just a concatenation of the top ranked
    chunks.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸Šä¸‹æ–‡åªæ˜¯å°†æ’åæœ€é«˜çš„ç‰‡æ®µè¿æ¥èµ·æ¥ã€‚
- en: Generating the Answer
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç”Ÿæˆç­”æ¡ˆ
- en: 'For comparison, first lets try what the model answers without any added context,
    i.e. based on its training data alone:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: è¿›è¡Œæ¯”è¾ƒï¼Œé¦–å…ˆè®©æˆ‘ä»¬å°è¯•æ¨¡å‹åœ¨æ²¡æœ‰ä»»ä½•é¢å¤–ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹çš„å›ç­”ï¼Œå³ä»…åŸºäºå…¶è®­ç»ƒæ•°æ®ï¼š
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This gives (one of many runs, slight variations but generally similar):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç»™å‡ºäº†ï¼ˆå¤šæ¬¡è¿è¡Œä¹‹ä¸€ï¼Œç•¥æœ‰å˜åŒ–ä½†é€šå¸¸ç›¸ä¼¼ï¼‰ï¼š
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Generally accurate, but missing much of the latest developments. In comparison,
    lets try with providing the generated context to the question:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸æ¥è¯´ï¼Œè™½ç„¶å‡†ç¡®ï¼Œä½†ç¼ºä¹æœ€æ–°çš„å‘å±•ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•å°†ç”Ÿæˆçš„ä¸Šä¸‹æ–‡æä¾›ç»™é—®é¢˜ï¼š
- en: '[PRE14]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following is an example answer with the top *sim_score* sorted chunks as
    context (includes the Tenor and BÃ¥rd page chunks):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ç­”æ¡ˆï¼Œä½¿ç”¨äº†æŒ‰*sim_score*æ’åºçš„ç‰‡æ®µä½œä¸ºä¸Šä¸‹æ–‡ï¼ˆåŒ…æ‹¬Tenorå’ŒBÃ¥rdé¡µé¢ç‰‡æ®µï¼‰ï¼š
- en: '[PRE15]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This is not a very good answer since it starts talking about completely non-related
    topics here, *Tenor* and *BÃ¥rd*. Partly because in this case the Tenor chunk is
    included in the context, and chunk order also generally less optimal as it is
    not re-ranked.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ç­”æ¡ˆï¼Œå› ä¸ºå®ƒå¼€å§‹è°ˆè®ºå®Œå…¨æ— å…³çš„è¯é¢˜ï¼Œ*Tenor*å’Œ*BÃ¥rd*ã€‚éƒ¨åˆ†åŸå› æ˜¯å› ä¸ºåœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒTenorå—è¢«åŒ…å«åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼Œå—çš„é¡ºåºä¹Ÿé€šå¸¸è¾ƒå·®ï¼Œå› ä¸ºæ²¡æœ‰é‡æ–°æ’åºã€‚
- en: 'In comparison, with *rerank_score* sorted context chunks (better chunk ordering
    and Tenor gone):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸æ¯”ä¹‹ä¸‹ï¼Œä½¿ç”¨*rerank_score*æ’åºçš„ä¸Šä¸‹æ–‡å—ï¼ˆæ›´å¥½çš„å—æ’åºå’ŒTenoræ¶ˆå¤±ï¼‰ï¼š
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now the unrelated topics are gone and the answer in general is better and more
    to the point.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä¸ç›¸å…³çš„ä¸»é¢˜å·²ç»æ¶ˆå¤±ï¼Œç­”æ¡ˆæ€»ä½“ä¸Šæ›´å¥½ï¼Œæ›´åˆ‡é¢˜ã€‚
- en: This highlights that it is not only important to find proper context to give
    to the model, but also to trim out the unrelated context. At least in this case,
    the Zephyr model was not able to directly identify which part of the context was
    relevant, but rather seems to have summarized the it all. Cannot really fault
    the model, as I gave it that context and asked to use it.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™çªå‡ºäº†ä¸ä»…è¦æ‰¾åˆ°é€‚å½“çš„ä¸Šä¸‹æ–‡ä»¥æä¾›ç»™æ¨¡å‹ï¼Œè€Œä¸”è¿˜è¦å»é™¤æ— å…³çš„ä¸Šä¸‹æ–‡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒZephyræ¨¡å‹ä¼¼ä¹æ— æ³•ç›´æ¥è¯†åˆ«å“ªä¸ªéƒ¨åˆ†çš„ä¸Šä¸‹æ–‡æ˜¯ç›¸å…³çš„ï¼Œè€Œæ˜¯ä¼¼ä¹å¯¹æ‰€æœ‰å†…å®¹è¿›è¡Œäº†æ€»ç»“ã€‚ä¸èƒ½çœŸæ­£è´£æ€ªæ¨¡å‹ï¼Œå› ä¸ºæˆ‘æä¾›äº†è¿™äº›ä¸Šä¸‹æ–‡å¹¶è¦æ±‚å®ƒä½¿ç”¨è¿™äº›å†…å®¹ã€‚
- en: Looking at the re-rank scores for the chunks, a general filtering approach based
    on metrics such as negative re-rank scores would have solved this issue also in
    the above case, as the â€œbadâ€ chunks in this case have a negative re-rank score.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹å—çš„é‡æ–°æ’åºåˆ†æ•°ï¼ŒåŸºäºè¯¸å¦‚è´Ÿé¢é‡æ–°æ’åºåˆ†æ•°ç­‰æŒ‡æ ‡çš„ä¸€èˆ¬è¿‡æ»¤æ–¹æ³•ä¹Ÿå¯ä»¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œå› ä¸ºåœ¨è¿™ç§æƒ…å†µä¸‹â€œåâ€å—å…·æœ‰è´Ÿé¢é‡æ–°æ’åºåˆ†æ•°ã€‚
- en: Something to note is that Google released a new and much improved *Gemini* family
    of models for Bard, around the time I was writing this article. It is not mentioned
    in the generated answers here since the Wikipedia dumps are generated with a slight
    delay. So as one might imagine, it is important to try to have up-to-date information
    in the context, and to keep it relevant and focused.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒGoogleåœ¨æˆ‘å†™è¿™ç¯‡æ–‡ç« æ—¶å‘å¸ƒäº†ä¸€ä¸ªå…¨æ–°çš„ã€æ˜¾è‘—æ”¹è¿›çš„*Gemini*æ¨¡å‹ç³»åˆ—ã€‚ç”±äºç»´åŸºç™¾ç§‘çš„å†…å®¹ç”Ÿæˆæœ‰ä¸€äº›å»¶è¿Ÿï¼Œå› æ­¤è¿™é‡Œç”Ÿæˆçš„ç­”æ¡ˆæ²¡æœ‰æåˆ°è¿™ä¸ªæ¨¡å‹ã€‚å› æ­¤ï¼Œå¦‚äººä»¬æ‰€æƒ³ï¼Œå°è¯•ä¿æŒä¸Šä¸‹æ–‡çš„ä¿¡æ¯æ˜¯æœ€æ–°çš„ï¼Œå¹¶ä¿æŒå…¶ç›¸å…³æ€§å’Œé‡ç‚¹æ˜¯å¾ˆé‡è¦çš„ã€‚
- en: Visual Embedding Check
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¯è§†åŒ–åµŒå…¥æ£€æŸ¥
- en: Embeddings are a great tool, but sometimes it is a bit difficult to really grasp
    how they are working, and what is happening with the similarity search. A basic
    approach is to plot the embeddings against each other to get some insight into
    their relations.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: åµŒå…¥æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å·¥å…·ï¼Œä½†æœ‰æ—¶ç¡®å®å¾ˆéš¾çœŸæ­£ç†è§£å®ƒä»¬æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œä»¥åŠç›¸ä¼¼åº¦æœç´¢å‘ç”Ÿäº†ä»€ä¹ˆã€‚ä¸€ä¸ªåŸºæœ¬çš„æ–¹æ³•æ˜¯å°†åµŒå…¥å½¼æ­¤ç»˜åˆ¶ï¼Œä»¥è·å¾—ä¸€äº›å…³äºå®ƒä»¬å…³ç³»çš„è§è§£ã€‚
- en: 'Building such a visualization is quite simple with [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis)
    and visualization libraries. It involves mapping the embedding vectors to 2 or
    3 dimensions, and plotting the results. Here I map from those 384 dimensions to
    2, and plot the result:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºè¿™æ ·çš„å¯è§†åŒ–ä½¿ç”¨[PCA](https://en.wikipedia.org/wiki/Principal_component_analysis)å’Œå¯è§†åŒ–åº“æ˜¯ç›¸å½“ç®€å•çš„ã€‚å®ƒæ¶‰åŠå°†åµŒå…¥å‘é‡æ˜ å°„åˆ°2ç»´æˆ–3ç»´ï¼Œå¹¶ç»˜åˆ¶ç»“æœã€‚åœ¨è¿™é‡Œï¼Œæˆ‘å°†è¿™384ç»´æ˜ å°„åˆ°2ç»´ï¼Œå¹¶ç»˜åˆ¶äº†ç»“æœï¼š
- en: '[PRE17]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'For the top 10 articles in the â€œ *what is google bard?* â€œ question, this gives
    the following visualization:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºâ€œ*what is google bard?*â€é—®é¢˜çš„å‰10ç¯‡æ–‡ç« ï¼Œè¿™é‡Œç»™å‡ºäº†ä»¥ä¸‹å¯è§†åŒ–ï¼š
- en: '![](../Images/67b91c3adabe96243a9d5e60010483b7.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67b91c3adabe96243a9d5e60010483b7.png)'
- en: PCA-based 2D plot of question embeddings vs article 1st chunk embeddings.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºPCAçš„2Dç»˜å›¾ï¼Œæ¯”è¾ƒé—®é¢˜åµŒå…¥ä¸æ–‡ç« ç¬¬ä¸€ä¸ªå—åµŒå…¥ã€‚
- en: In this plot, the red dot is the embedding for the question â€œ *what is google
    bard?*â€. The blue dots are the closest Wikipedia article matches, according to
    *sim_score*.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªå›¾ä¸­ï¼Œçº¢ç‚¹æ˜¯é—®é¢˜â€œ*what is google bard?*â€çš„åµŒå…¥ã€‚è“ç‚¹æ˜¯æ ¹æ®*sim_score*æ‰¾åˆ°çš„æœ€æ¥è¿‘çš„ç»´åŸºç™¾ç§‘æ–‡ç« åŒ¹é…é¡¹ã€‚
- en: The [Bard article](https://en.wikipedia.org/wiki/Bard_(chatbot)) is obviously
    the closest one to the question, while the rest are a bit further off. The [Tenor
    article](https://en.wikipedia.org/wiki/Tenor_(website)) seems to be about second
    closest, while the [BÃ¥rd one](https://en.wikipedia.org/wiki/B%C3%A5rd) is a bit
    further away, possibly due to the loss of information in mapping from 384 dimensions
    to 2\. Due to this, the visualization is not perfectly accurate but helpful for
    quick human overview.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[Bardæ–‡ç« ](https://en.wikipedia.org/wiki/Bard_(chatbot))æ˜¾ç„¶æ˜¯ä¸é—®é¢˜æœ€æ¥è¿‘çš„ï¼Œè€Œå…¶ä»–çš„åˆ™ç¨è¿œä¸€äº›ã€‚[Tenoræ–‡ç« ](https://en.wikipedia.org/wiki/Tenor_(website))ä¼¼ä¹æ˜¯ç¬¬äºŒæ¥è¿‘çš„ï¼Œè€Œ[BÃ¥rdæ–‡ç« ](https://en.wikipedia.org/wiki/B%C3%A5rd)åˆ™ç¨è¿œä¸€äº›ï¼Œå¯èƒ½æ˜¯å› ä¸ºä»384ç»´æ˜ å°„åˆ°2ç»´æ—¶ä¿¡æ¯çš„ä¸¢å¤±ã€‚ç”±äºè¿™ä¸€ç‚¹ï¼Œå¯è§†åŒ–å¹¶ä¸æ˜¯å®Œå…¨å‡†ç¡®çš„ï¼Œä½†å¯¹å¿«é€Ÿäººå·¥æ¦‚è§ˆæ˜¯æœ‰å¸®åŠ©çš„ã€‚'
- en: 'The following figure illustrates an actual error finding from my Kaggle code
    using a similar PCA plot. Looking for a bit of insights, I tried a simple question
    about the first article in the Wikipedia dump (â€œ *Anarchism*â€). With the question
    â€œ *what is the definition of anarchism?* â€œ . The following is what the PCA visualization
    looked like for the closest articles, the marked outliers are perhaps the most
    interesting part:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹å›¾å±•ç¤ºäº†æˆ‘åœ¨Kaggleä»£ç ä¸­å‘ç°çš„å®é™…é”™è¯¯ï¼Œä½¿ç”¨äº†ç±»ä¼¼çš„PCAå›¾ã€‚ä¸ºäº†è·å–ä¸€äº›è§è§£ï¼Œæˆ‘å¯¹ç»´åŸºç™¾ç§‘è½¬å‚¨ä¸­çš„ç¬¬ä¸€ç¯‡æ–‡ç« ï¼ˆâ€œ *æ— æ”¿åºœä¸»ä¹‰*â€ï¼‰æå‡ºäº†ä¸€ä¸ªç®€å•çš„é—®é¢˜ï¼šâ€œ
    *æ— æ”¿åºœä¸»ä¹‰çš„å®šä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ*â€ã€‚ä¸‹é¢æ˜¯PCAå¯è§†åŒ–çš„ç»“æœï¼Œæ ‡è®°çš„ç¦»ç¾¤ç‚¹å¯èƒ½æ˜¯æœ€æœ‰è¶£çš„éƒ¨åˆ†ï¼š
- en: '![](../Images/200d00bcdabbf0f98d58bdd7cf0f5802.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/200d00bcdabbf0f98d58bdd7cf0f5802.png)'
- en: My fail shown in PCA-based 2D plot of Kaggle embeddings for selected top documents.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨PCAåŸºäº2Då›¾çš„KaggleåµŒå…¥ä¸­æ˜¾ç¤ºçš„å¤±è´¥ï¼Œé’ˆå¯¹æ‰€é€‰çš„é¡¶çº§æ–‡æ¡£ã€‚
- en: The red dot in the bottom left corner is again the question. The cluster of
    blue dots next to it are all related articles about anarchism. And then there
    are the two outlier dots on the top right. I removed the titles from the plot
    to keep it readable. The two outlier articles seemed to have nothing to do with
    the question when looking.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: å·¦ä¸‹è§’çš„çº¢ç‚¹å†æ¬¡è¡¨ç¤ºé—®é¢˜ã€‚æ—è¾¹çš„è“ç‚¹ç°‡æ˜¯æ‰€æœ‰ä¸æ— æ”¿åºœä¸»ä¹‰ç›¸å…³çš„æ–‡ç« ã€‚ç„¶åå³ä¸Šè§’æœ‰ä¸¤ä¸ªç¦»ç¾¤ç‚¹ã€‚æˆ‘åˆ é™¤äº†å›¾è¡¨ä¸­çš„æ ‡é¢˜ä»¥ä¿æŒå…¶å¯è¯»æ€§ã€‚æŸ¥çœ‹æ—¶ï¼Œè¿™ä¸¤ä¸ªç¦»ç¾¤æ–‡ç« ä¼¼ä¹ä¸é—®é¢˜æ— å…³ã€‚
- en: Why is this? As I indexed the articles with various chunk sizes of 512, 256,
    128, and 64, I had some issues in processing all the articles for 256 chunk size,
    and restarted the chunking in the middle. This resulted in some differences in
    indices of some of those embeddings vs the chunk texts I had stored. After noticing
    these strange looking results, I re-calculated the embeddings with the 256 token
    chunk size, and compared the results vs size 512, noted this difference. Too bad
    the competition was done at that time ğŸ™‚
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆä¼šè¿™æ ·ï¼Ÿç”±äºæˆ‘ä½¿ç”¨äº†512ã€256ã€128å’Œ64çš„å„ç§å—å¤§å°æ¥ç´¢å¼•æ–‡ç« ï¼Œåœ¨å¤„ç†256å—å¤§å°çš„æ‰€æœ‰æ–‡ç« æ—¶é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼Œå¹¶åœ¨ä¸­é€”é‡æ–°å¯åŠ¨äº†åˆ†å—ã€‚è¿™å¯¼è‡´æŸäº›åµŒå…¥ä¸æˆ‘å­˜å‚¨çš„å—æ–‡æœ¬çš„ç´¢å¼•æœ‰æ‰€ä¸åŒã€‚åœ¨æ³¨æ„åˆ°è¿™äº›å¥‡æ€ªçš„ç»“æœåï¼Œæˆ‘é‡æ–°è®¡ç®—äº†256ä¸ªä»¤ç‰Œå—å¤§å°çš„åµŒå…¥ï¼Œå¹¶å°†ç»“æœä¸512å¤§å°è¿›è¡Œæ¯”è¾ƒï¼Œæ³¨æ„åˆ°è¿™ä¸ªå·®å¼‚ã€‚å¯æƒœé‚£æ—¶æ¯”èµ›å·²ç»ç»“æŸğŸ™‚
- en: More Advanced Context Selection
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ›´é«˜çº§çš„ä¸Šä¸‹æ–‡é€‰æ‹©
- en: In the above I discussed chunking the documents and using similarity search
    + re-ranking as a method to find relevant chunks and build a context for the question
    answering. I found sometimes it is also useful to consider how the initial documents
    to chunk are selected vs just the chunks themselves.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°å†…å®¹è®¨è®ºäº†å°†æ–‡æ¡£åˆ†å—å¹¶ä½¿ç”¨ç›¸ä¼¼åº¦æœç´¢+é‡æ–°æ’åºä½œä¸ºæ‰¾åˆ°ç›¸å…³å—å’Œæ„å»ºé—®é¢˜å›ç­”ä¸Šä¸‹æ–‡çš„æ–¹æ³•ã€‚æˆ‘å‘ç°æœ‰æ—¶ä¹Ÿæœ‰å¿…è¦è€ƒè™‘åˆå§‹æ–‡æ¡£çš„é€‰æ‹©æ–¹å¼ï¼Œè€Œä¸ä»…ä»…æ˜¯å—æœ¬èº«ã€‚
- en: 'As example methods, the [advanced RAG](https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/)
    course on [DeepLearning.AI](https://www.deeplearning.ai/) , presents two approaches:
    sentence windowing, and hierarchical chunk merging. In summary this looks at nearby-chunks
    and if multiple are ranked high by their scores, takes them as a single large
    chunk. The â€œhierarchyâ€ coming from considering larger and larger chunk combinations
    for joint relevance. Aiming for more cohesive context vs random ordered small
    chunks, giving the generator LLM better input to work with.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºç¤ºä¾‹æ–¹æ³•ï¼Œ[é«˜çº§RAG](https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/)è¯¾ç¨‹åœ¨[DeepLearning.AI](https://www.deeplearning.ai/)ä¸Šä»‹ç»äº†ä¸¤ç§æ–¹æ³•ï¼šå¥å­çª—å£åŒ–å’Œå±‚æ¬¡å—åˆå¹¶ã€‚æ€»ç»“æ¥è¯´ï¼Œè¿™ç§æ–¹æ³•æŸ¥çœ‹é™„è¿‘çš„å—ï¼Œå¦‚æœå¤šä¸ªå—çš„åˆ†æ•°å¾ˆé«˜ï¼Œåˆ™å°†å®ƒä»¬ä½œä¸ºä¸€ä¸ªå¤§çš„å—ã€‚æ‰€è°“â€œå±‚æ¬¡ç»“æ„â€æ˜¯é€šè¿‡è€ƒè™‘è¶Šæ¥è¶Šå¤§çš„å—ç»„åˆæ¥å…±åŒç›¸å…³ã€‚æ—¨åœ¨æä¾›æ›´è¿è´¯çš„ä¸Šä¸‹æ–‡ï¼Œè€Œä¸æ˜¯éšæœºæ’åºçš„å°å—ï¼Œç»™ç”ŸæˆLLMæ›´å¥½çš„è¾“å…¥ã€‚
- en: 'As a simple example of this, here is the re-ranked set of top chunks for my
    above Bard example:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºä¸€ä¸ªç®€å•çš„ç¤ºä¾‹ï¼Œè¿™æ˜¯æˆ‘ä¸Šé¢Bardç¤ºä¾‹çš„é‡æ–°æ’åºçš„å‰å‡ ä¸ªå—ï¼š
- en: '![](../Images/fa17b950e30b9f3b12c8cb936393317c.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa17b950e30b9f3b12c8cb936393317c.png)'
- en: Top 10 chunks for my Bard example, sorted by rerank_score.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨Bardç¤ºä¾‹ä¸­çš„å‰10ä¸ªå—ï¼ŒæŒ‰é‡æ–°æ’åºåˆ†æ•°æ’åºã€‚
- en: The leftmost column here is the index of the chunk. In my generation, I just
    took the top chunks in this sorted order as in the table. If we wanted to make
    the context a bit more coherent, we could sort the final selected chunks by their
    order within a document. If there is a small piece missing between highly ranked
    chunks, adding the missing one (e.g., here chunk id 7) could help in missing gaps,
    similar to the hierarchical merging. This could be something to try as a final
    step for final gains.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ€å·¦ä¾§çš„åˆ—æ˜¯å—çš„ç´¢å¼•ã€‚åœ¨æˆ‘çš„ç”Ÿæˆä¸­ï¼Œæˆ‘åªæ˜¯æŒ‰è¡¨ä¸­çš„æ’åºé¡ºåºå–äº†é¡¶çº§å—ã€‚å¦‚æœæˆ‘ä»¬æƒ³ä½¿ä¸Šä¸‹æ–‡æ›´è¿è´¯ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰æ–‡æ¡£ä¸­çš„é¡ºåºå¯¹æœ€ç»ˆé€‰æ‹©çš„å—è¿›è¡Œæ’åºã€‚å¦‚æœåœ¨é«˜åº¦æ’åçš„å—ä¹‹é—´æœ‰å°ç‰‡æ®µç¼ºå¤±ï¼Œæ·»åŠ ç¼ºå¤±çš„éƒ¨åˆ†ï¼ˆä¾‹å¦‚è¿™é‡Œçš„å—
    ID 7ï¼‰å¯èƒ½æœ‰åŠ©äºå¡«è¡¥ç©ºç™½ï¼Œç±»ä¼¼äºå±‚æ¬¡åˆå¹¶ã€‚è¿™å¯èƒ½æ˜¯ä½œä¸ºæœ€ç»ˆæ­¥éª¤è¿›è¡Œå°è¯•çš„å†…å®¹ï¼Œä»¥è·å¾—æœ€ç»ˆçš„æ”¹è¿›ã€‚
- en: In my Kaggle experiments, I performed initial document selection based on the
    first chunk only. In part due to Kaggleâ€™s resource limits, but it appeared to
    have some other advantages as well. Typically, an articleâ€™s beginning acts as
    a summary (introduction or abstract). Initial chunk selection from such ranked
    articles may help select chunks with more relevant overall context.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘çš„ Kaggle å®éªŒä¸­ï¼Œæˆ‘ä»…åŸºäºç¬¬ä¸€ä¸ªå—è¿›è¡Œåˆæ­¥æ–‡æ¡£é€‰æ‹©ã€‚è¿™éƒ¨åˆ†æ˜¯ç”±äº Kaggle çš„èµ„æºé™åˆ¶ï¼Œä½†å®ƒä¼¼ä¹ä¹Ÿæœ‰ä¸€äº›å…¶ä»–çš„ä¼˜åŠ¿ã€‚é€šå¸¸ï¼Œä¸€ç¯‡æ–‡ç« çš„å¼€å¤´éƒ¨åˆ†å……å½“äº†æ€»ç»“ï¼ˆå¼•è¨€æˆ–æ‘˜è¦ï¼‰ã€‚ä»è¿™äº›æ’åæ–‡ç« ä¸­è¿›è¡Œåˆæ­¥å—é€‰æ‹©å¯èƒ½æœ‰åŠ©äºé€‰æ‹©å…·æœ‰æ›´ç›¸å…³æ•´ä½“ä¸Šä¸‹æ–‡çš„å—ã€‚
- en: This is visible in my Bard example above, where both the *rerank_score* and
    *sim_score* are highest for the first chunk of the best article. To try to improve
    this, I also tried using a larger chunk size for this initial document selection,
    to include more of the introduction for better relevance. Then chunked the top
    selected documents with smaller chunk sizes for experimenting on how good the
    context is with each size.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä¸Šé¢çš„ Bard ç¤ºä¾‹ä¸­å¯ä»¥çœ‹åˆ°ï¼Œæ— è®ºæ˜¯ *rerank_score* è¿˜æ˜¯ *sim_score*ï¼Œå¯¹äºæœ€ä½³æ–‡ç« çš„ç¬¬ä¸€ä¸ªå—éƒ½æ˜¯æœ€é«˜çš„ã€‚ä¸ºäº†æ”¹è¿›è¿™ä¸€ç‚¹ï¼Œæˆ‘è¿˜å°è¯•ä½¿ç”¨æ›´å¤§çš„å—å¤§å°è¿›è¡Œåˆå§‹æ–‡æ¡£é€‰æ‹©ï¼Œä»¥åŒ…æ‹¬æ›´å¤šçš„å¼•è¨€ä»¥æé«˜ç›¸å…³æ€§ã€‚ç„¶åå°†é¡¶çº§é€‰æ‹©çš„æ–‡æ¡£æŒ‰è¾ƒå°çš„å—å¤§å°è¿›è¡Œåˆ‡åˆ†ï¼Œä»¥å®éªŒæ¯ç§å¤§å°çš„ä¸Šä¸‹æ–‡æ•ˆæœã€‚
- en: While I could not run the initial search on all chunks of all documents on Kaggle
    due to resource limitations, I tried it outside of Kaggle. In these trials, I
    noticed that sometimes single chunks of unrelated articles get ranked high, while
    in reality misleading for the answer generation. For example, actor biography
    in a related movie. Initial document relevance selection may help avoid this.
    Unfortunately, I did not have time to study this further with different configurations,
    and good re-ranking may already help.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºèµ„æºé™åˆ¶ï¼Œæˆ‘æ— æ³•åœ¨ Kaggle ä¸Šå¯¹æ‰€æœ‰æ–‡æ¡£çš„æ‰€æœ‰å—è¿›è¡Œåˆå§‹æœç´¢ï¼Œä½†æˆ‘åœ¨ Kaggle å¤–éƒ¨å°è¯•äº†ä¸€ä¸‹ã€‚åœ¨è¿™äº›è¯•éªŒä¸­ï¼Œæˆ‘å‘ç°æœ‰æ—¶å•ä¸ªä¸ç›¸å…³çš„æ–‡ç« å—ä¼šè¢«æ’åè¾ƒé«˜ï¼Œè€Œå®é™…ä¸Šå¯¹ç­”æ¡ˆç”Ÿæˆå­˜åœ¨è¯¯å¯¼ã€‚ä¾‹å¦‚ï¼Œä¸ç›¸å…³ç”µå½±çš„æ¼”å‘˜ä¼ è®°ã€‚åˆæ­¥çš„æ–‡æ¡£ç›¸å…³æ€§é€‰æ‹©å¯èƒ½æœ‰åŠ©äºé¿å…è¿™ç§æƒ…å†µã€‚ä¸å¹¸çš„æ˜¯ï¼Œæˆ‘æ²¡æœ‰æ—¶é—´ç”¨ä¸åŒçš„é…ç½®è¿›ä¸€æ­¥ç ”ç©¶è¿™ä¸ªé—®é¢˜ï¼Œå¥½çš„é‡æ’åå¯èƒ½å·²ç»æœ‰å¸®åŠ©ã€‚
- en: Finally, repeating the same information in multiple chunks in the context is
    not very useful. Top ranking of the chunks does not guarantee that they best complement
    each other, or best chunk diversity. For example, LangChain has a special chunk
    selector for [Maximum Marginal Relevance](https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/mmr).
    It does this by penalizing new chunks by how close they are to the already added
    chunks.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œåœ¨ä¸Šä¸‹æ–‡ä¸­é‡å¤ç›¸åŒçš„ä¿¡æ¯åœ¨å¤šä¸ªå—ä¸­ä¸æ˜¯å¾ˆæœ‰ç”¨ã€‚å—çš„æœ€é«˜æ’åå¹¶ä¸ä¿è¯å®ƒä»¬å½¼æ­¤æœ€å¥½åœ°è¡¥å……ï¼Œæˆ–æœ€ä½³å—å¤šæ ·æ€§ã€‚ä¾‹å¦‚ï¼ŒLangChain ä¸º [æœ€å¤§è¾¹é™…ç›¸å…³æ€§](https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/mmr)
    æä¾›äº†ä¸€ä¸ªç‰¹æ®Šçš„å—é€‰æ‹©å™¨ã€‚å®ƒé€šè¿‡å¯¹æ–°å—è¿›è¡Œæƒ©ç½šï¼Œæƒ©ç½šä¾æ®æ˜¯å®ƒä»¬ä¸å·²æ·»åŠ å—çš„ç›¸ä¼¼åº¦æ¥å®ç°è¿™ä¸€ç‚¹ã€‚
- en: Extending the RAG Query
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ‰©å±• RAG æŸ¥è¯¢
- en: 'I used a very simple question / query for my RAG example here (â€œ *what is google
    bard?*â€), and simple is good to illustrate the basic RAG concept. This is a pretty
    short query input considering that the embedding model I used had a 512 token
    maximum sequence length. If I encode this question into tokens using the tokenizer
    for the embedding model ( *bge-small-en*), I get the following tokens:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨è¿™é‡Œä½¿ç”¨äº†ä¸€ä¸ªéå¸¸ç®€å•çš„é—®é¢˜/æŸ¥è¯¢ä½œä¸ºæˆ‘çš„ RAG ç¤ºä¾‹ï¼ˆâ€œ *what is google bard?*â€ï¼‰ï¼Œç®€å•çš„æŸ¥è¯¢æœ‰åŠ©äºè¯´æ˜åŸºæœ¬çš„ RAG
    æ¦‚å¿µã€‚è€ƒè™‘åˆ°æˆ‘ä½¿ç”¨çš„åµŒå…¥æ¨¡å‹å…·æœ‰ 512 ä»¤ç‰Œçš„æœ€å¤§åºåˆ—é•¿åº¦ï¼Œè¿™ä¸ªæŸ¥è¯¢è¾“å…¥ç›¸å½“ç®€çŸ­ã€‚å¦‚æœæˆ‘ä½¿ç”¨åµŒå…¥æ¨¡å‹çš„åˆ†è¯å™¨ï¼ˆ*bge-small-en*ï¼‰å°†è¿™ä¸ªé—®é¢˜ç¼–ç æˆä»¤ç‰Œï¼Œæˆ‘ä¼šå¾—åˆ°ä»¥ä¸‹ä»¤ç‰Œï¼š
- en: '[PRE18]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Which amounts to a total of 7 tokens. With a maximum sequence length of 512,
    this leaves plenty of room if I want to use a longer query sentence. Sometimes
    this can be useful, especially if the information we want to retrieve is not such
    a simple query, or if the domain is more complex. For a very small query, the
    semantic search may not work best, as noted also in the [Stack Overflows AI Journey
    posting](https://stackoverflow.blog/2023/07/31/ask-like-a-human-implementing-semantic-search-on-stack-overflow/).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ€»å…±æ˜¯ 7 ä¸ªæ ‡è®°ã€‚æœ€å¤§åºåˆ—é•¿åº¦ä¸º 512ï¼Œè¿™ä¸ºæˆ‘ä½¿ç”¨æ›´é•¿çš„æŸ¥è¯¢å¥å­ç•™å‡ºäº†è¶³å¤Ÿçš„ç©ºé—´ã€‚æœ‰æ—¶è¿™å¾ˆæœ‰ç”¨ï¼Œç‰¹åˆ«æ˜¯å½“æˆ‘ä»¬æƒ³è¦æ£€ç´¢çš„ä¿¡æ¯ä¸æ˜¯ç®€å•çš„æŸ¥è¯¢ï¼Œæˆ–è€…é¢†åŸŸè¾ƒå¤æ‚æ—¶ã€‚å¯¹äºéå¸¸ç®€å•çš„æŸ¥è¯¢ï¼Œè¯­ä¹‰æœç´¢å¯èƒ½æ•ˆæœä¸å¥½ï¼Œæ­£å¦‚åœ¨[Stack
    Overflows AI Journey æ–‡ç« ](https://stackoverflow.blog/2023/07/31/ask-like-a-human-implementing-semantic-search-on-stack-overflow/)ä¸­æåˆ°çš„é‚£æ ·ã€‚
- en: For example, the Kaggle competition had a set of questions, each with 5 answer
    options to pick from. I initially tried RAG with just the question as the input
    for the embedding model. The search results were not too great, so I tried again
    with the question + all the answer options as the query. This produced much better
    results.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼ŒKaggle æ¯”èµ›æœ‰ä¸€ç»„é—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜æœ‰ 5 ä¸ªç­”æ¡ˆé€‰é¡¹å¯ä»¥é€‰æ‹©ã€‚æˆ‘æœ€åˆå°è¯•äº†ä»…å°†é—®é¢˜ä½œä¸ºåµŒå…¥æ¨¡å‹çš„è¾“å…¥çš„ RAGã€‚æœç´¢ç»“æœå¹¶ä¸ç†æƒ³ï¼Œå› æ­¤æˆ‘å†æ¬¡å°è¯•äº†å°†é—®é¢˜
    + æ‰€æœ‰ç­”æ¡ˆé€‰é¡¹ä½œä¸ºæŸ¥è¯¢ã€‚è¿™äº§ç”Ÿäº†æ›´å¥½çš„ç»“æœã€‚
- en: 'As an example, the first question in the training dataset of the competition:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºä¸€ä¸ªä¾‹å­ï¼Œæ¯”èµ›è®­ç»ƒæ•°æ®é›†ä¸­çš„ç¬¬ä¸€ä¸ªé—®é¢˜ï¼š
- en: '[PRE19]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This is 32 tokens for the *bge-small-en* model. So about 480 still left to fit
    into the maximum 512 token sequence length.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº *bge-small-en* æ¨¡å‹ï¼Œè¿™éœ€è¦ 32 ä¸ªæ ‡è®°ã€‚å› æ­¤ï¼Œæœ€å¤§ 512 ä¸ªæ ‡è®°çš„åºåˆ—é•¿åº¦è¿˜å‰©å¤§çº¦ 480 ä¸ªæ ‡è®°å¯ä»¥å¡«å……ã€‚
- en: 'Here is the first question along with the 5 answer options given for it:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ç¬¬ä¸€ä¸ªé—®é¢˜åŠå…¶ç»™å‡ºçš„ 5 ä¸ªç­”æ¡ˆé€‰é¡¹ï¼š
- en: '![](../Images/52216d4403a7ac4827efa1264b76eb88.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52216d4403a7ac4827efa1264b76eb88.png)'
- en: Example question and answer options A-E. Concatenating all these texts formed
    the query.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹é—®é¢˜å’Œç­”æ¡ˆé€‰é¡¹ A-Eã€‚å°†æ‰€æœ‰è¿™äº›æ–‡æœ¬åˆå¹¶å½¢æˆäº†æŸ¥è¯¢ã€‚
- en: Concatenating the question and the given options into one RAG query gives this
    a length 235 tokens, with still more than 50% of embedding model sequence length
    left. In my case, this approach produced much better results. Both from manual
    inspection, and for the competition score. Thus, experimenting with different
    ways to make the RAG query itself more expressive is worth a try.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: å°†é—®é¢˜å’Œç»™å®šçš„é€‰é¡¹åˆå¹¶æˆä¸€ä¸ª RAG æŸ¥è¯¢ï¼Œå¾—åˆ°çš„é•¿åº¦ä¸º 235 ä¸ªæ ‡è®°ï¼Œå¹¶ä¸”ä»ç„¶æœ‰è¶…è¿‡ 50% çš„åµŒå…¥æ¨¡å‹åºåˆ—é•¿åº¦å‰©ä½™ã€‚å°±æˆ‘è€Œè¨€ï¼Œè¿™ç§æ–¹æ³•äº§ç”Ÿäº†æ›´å¥½çš„ç»“æœï¼Œæ— è®ºæ˜¯é€šè¿‡äººå·¥æ£€æŸ¥ï¼Œè¿˜æ˜¯æ¯”èµ›åˆ†æ•°ã€‚å› æ­¤ï¼Œå°è¯•ä¸åŒçš„æ–¹å¼ä½¿
    RAG æŸ¥è¯¢æœ¬èº«æ›´å…·è¡¨ç°åŠ›æ˜¯å€¼å¾—çš„ã€‚
- en: Hallucinations
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¹»è§‰
- en: Finally, there is the topic of [hallucinations](https://machinelearningmastery.com/a-gentle-introduction-to-hallucinations-in-large-language-models/),
    where the model produces text that is incorrect or fabricated. The Tenor example
    from my *sim_score* sorting is one kind of an example, even if the generator did
    base it on the actual given context. So better keep the context good I guess :).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åæ˜¯å…³äº[å¹»è§‰](https://machinelearningmastery.com/a-gentle-introduction-to-hallucinations-in-large-language-models/)çš„è¯é¢˜ï¼Œå³æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ä¸æ­£ç¡®æˆ–è™šæ„ã€‚æˆ‘çš„
    *sim_score* æ’åºä¸­çš„ Tenor ç¤ºä¾‹å°±æ˜¯ä¸€ä¸ªä¾‹å­ï¼Œå³ä½¿ç”Ÿæˆå™¨ç¡®å®åŸºäºå®é™…çš„ç»™å®šä¸Šä¸‹æ–‡ã€‚å› æ­¤ï¼Œæˆ‘æƒ³æœ€å¥½è¿˜æ˜¯ä¿æŒä¸Šä¸‹æ–‡è‰¯å¥½ :).
- en: To address hallucinations, the chatbots from the big AI companies ( [Google
    Bard](https://bard.google.com/chat), [ChatGPT](https://chat.openai.com/), [Bing
    Chat](https://www.bing.com/chat)) all provide means to link parts of their generated
    answers to verifiable sources. [Bard](https://bard.google.com/chat) has a specific
    â€œGâ€ button that performs a Google search and highlights parts of the generated
    answer that match the search results. Too bad we do not always have a world-class
    search-engine for our data to help.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è§£å†³å¹»è§‰é—®é¢˜ï¼Œå¤§å‹ AI å…¬å¸ï¼ˆ[Google Bard](https://bard.google.com/chat)ï¼Œ[ChatGPT](https://chat.openai.com/)ï¼Œ[Bing
    Chat](https://www.bing.com/chat)ï¼‰çš„èŠå¤©æœºå™¨äººéƒ½æä¾›äº†å°†å…¶ç”Ÿæˆçš„å›ç­”éƒ¨åˆ†é“¾æ¥åˆ°å¯éªŒè¯æ¥æºçš„æ–¹æ³•ã€‚[Bard](https://bard.google.com/chat)
    æœ‰ä¸€ä¸ªç‰¹å®šçš„ â€œGâ€ æŒ‰é’®ï¼Œå¯ä»¥æ‰§è¡Œ Google æœç´¢ï¼Œå¹¶çªå‡ºæ˜¾ç¤ºä¸æœç´¢ç»“æœåŒ¹é…çš„ç”Ÿæˆå›ç­”éƒ¨åˆ†ã€‚å¯æƒœæˆ‘ä»¬å¹¶ä¸æ€»æ˜¯æ‹¥æœ‰ä¸–ç•Œçº§çš„æœç´¢å¼•æ“æ¥å¸®åŠ©å¤„ç†æˆ‘ä»¬çš„æ•°æ®ã€‚
- en: '[Bing Chat](https://www.bing.com/chat) has a similar approach, highlighting
    parts of the answer and adding a reference to the source websites. [ChatGPT](https://chat.openai.com/)
    has a slightly different approach; I had to explicitly ask it to verify its answer
    and update with latest developments, telling it to use its browser tool. After
    this, it did an internet search and linked to specific websites as sources. The
    source quality seemed to vary quite a bit as in any internet search. Of course,
    for internal documents this type of web search is not possible. However, linking
    to the source should always be possible even internally.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[Bing Chat](https://www.bing.com/chat)é‡‡ç”¨äº†ç±»ä¼¼çš„æ–¹æ³•ï¼Œçªå‡ºæ˜¾ç¤ºç­”æ¡ˆçš„éƒ¨åˆ†å¹¶æ·»åŠ å¯¹æºç½‘ç«™çš„å¼•ç”¨ã€‚[ChatGPT](https://chat.openai.com/)çš„æ–¹æ³•ç•¥æœ‰ä¸åŒï¼›æˆ‘å¿…é¡»æ˜ç¡®è¦æ±‚å®ƒéªŒè¯å…¶ç­”æ¡ˆå¹¶æ›´æ–°æœ€æ–°è¿›å±•ï¼Œå‘Šè¯‰å®ƒä½¿ç”¨å…¶æµè§ˆå™¨å·¥å…·ã€‚ä¹‹åï¼Œå®ƒè¿›è¡Œäº†äº’è”ç½‘æœç´¢å¹¶é“¾æ¥åˆ°ç‰¹å®šç½‘ç«™ä½œä¸ºæ¥æºã€‚æºçš„è´¨é‡ä¼¼ä¹æœ‰å¾ˆå¤§çš„å˜åŒ–ï¼Œå°±åƒä»»ä½•äº’è”ç½‘æœç´¢ä¸€æ ·ã€‚å½“ç„¶ï¼Œå¯¹äºå†…éƒ¨æ–‡æ¡£ï¼Œè¿™ç§ç±»å‹çš„ç½‘ç»œæœç´¢æ˜¯ä¸å¯èƒ½çš„ã€‚ç„¶è€Œï¼Œå³ä½¿åœ¨å†…éƒ¨ï¼Œä¹Ÿåº”è¯¥å§‹ç»ˆå¯ä»¥é“¾æ¥åˆ°æ¥æºã€‚'
- en: I also asked Bard, ChatGPT+, and Bing for ideas on detecting hallucinations.
    The results included an LLM hallucination [ranking index](https://www.rungalileo.io/blog/hallucination-index),
    including [RAG hallucination](https://www.rungalileo.io/hallucinationindex). When
    tuning LLMâ€™s, it might also help to set the [temperature parameter to zero](https://txt.cohere.com/llm-parameters-best-outputs-language-ai/)
    for the LLM to generate deterministic, most probable output tokens.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è¿˜è¯¢é—®äº†Bardã€ChatGPT+å’ŒBingå…³äºæ£€æµ‹å¹»è§‰çš„æƒ³æ³•ã€‚ç»“æœåŒ…æ‹¬ä¸€ä¸ªLLMå¹»è§‰[æ’åæŒ‡æ•°](https://www.rungalileo.io/blog/hallucination-index)ï¼Œä»¥åŠ[RAGå¹»è§‰](https://www.rungalileo.io/hallucinationindex)ã€‚åœ¨è°ƒä¼˜LLMæ—¶ï¼Œå°†[æ¸©åº¦å‚æ•°è®¾ä¸ºé›¶](https://txt.cohere.com/llm-parameters-best-outputs-language-ai/)å¯èƒ½æœ‰åŠ©äºLLMç”Ÿæˆç¡®å®šæ€§çš„ã€æœ€å¯èƒ½çš„è¾“å‡ºä»¤ç‰Œã€‚
- en: Finally, as this is a very common problem, there seem to be various approaches
    being built to address this challenge a bit better. For example, specific [LLMâ€™s
    to help detect halluciations](https://huggingface.co/blog/dhuynh95/automatic-hallucination-detection)
    seem to be a promising area. I did not have time to try them, but certainly relevant
    in bigger projects.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œç”±äºè¿™æ˜¯ä¸€ä¸ªéå¸¸å¸¸è§çš„é—®é¢˜ï¼Œä¼¼ä¹æœ‰å„ç§æ–¹æ³•æ­£åœ¨è¢«æ„å»ºä»¥æ›´å¥½åœ°è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚ä¾‹å¦‚ï¼Œç‰¹å®šçš„[LLMæ¥å¸®åŠ©æ£€æµ‹å¹»è§‰](https://huggingface.co/blog/dhuynh95/automatic-hallucination-detection)ä¼¼ä¹æ˜¯ä¸€ä¸ªæœ‰å‰é€”çš„é¢†åŸŸã€‚æˆ‘æ²¡æœ‰æ—¶é—´å°è¯•å®ƒä»¬ï¼Œä½†åœ¨æ›´å¤§çš„é¡¹ç›®ä¸­è‚¯å®šæ˜¯ç›¸å…³çš„ã€‚
- en: Evaluating Results
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¯„ä¼°ç»“æœ
- en: Besides implementing a working RAG solution, it is also nice to be able to tell
    something about how well it works. In the Kaggle competition this was quite simple.
    I just ran the solution to try to answer the given questions in the training dataset,
    comparing to the correct answers given in the training data. Or submitted the
    model for scoring on the Kaggle competition test set. The better the answer score,
    the better one could call the RAG solution, even if there was more to the score.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†å®ç°ä¸€ä¸ªæœ‰æ•ˆçš„RAGè§£å†³æ–¹æ¡ˆä¹‹å¤–ï¼Œèƒ½å¤Ÿè¯„ä¼°å®ƒçš„æ•ˆæœä¹Ÿæ˜¯å¾ˆæœ‰ä»·å€¼çš„ã€‚åœ¨Kaggleæ¯”èµ›ä¸­ï¼Œè¿™ç›¸å½“ç®€å•ã€‚æˆ‘åªæ˜¯è¿è¡Œè§£å†³æ–¹æ¡ˆä»¥å°è¯•å›ç­”è®­ç»ƒæ•°æ®é›†ä¸­çš„ç»™å®šé—®é¢˜ï¼Œå¹¶å°†å…¶ä¸è®­ç»ƒæ•°æ®ä¸­æä¾›çš„æ­£ç¡®ç­”æ¡ˆè¿›è¡Œæ¯”è¾ƒã€‚æˆ–è€…å°†æ¨¡å‹æäº¤åˆ°Kaggleæ¯”èµ›æµ‹è¯•é›†è¿›è¡Œè¯„åˆ†ã€‚ç­”æ¡ˆåˆ†æ•°è¶Šé«˜ï¼ŒRAGè§£å†³æ–¹æ¡ˆå°±è¶Šå¥½ï¼Œå³ä½¿åˆ†æ•°èƒŒåè¿˜æœ‰æ›´å¤šå†…å®¹ã€‚
- en: In many cases, a suitable evaluation dataset for domain specific RAG may not
    be available. For this scenario, one might want to start with some generic NLP
    evaluation datasets, such as [this list](https://paperswithcode.com/task/question-answering#:~:text=Popular%20benchmark%20datasets%20for%20evaluation%20question%20answering%20systems%20include%20SQuAD,models%20are%20T5%20and%20XLNet.).
    Tools such as LangChain also come with [support for auto-generating questions
    and answers](https://blog.langchain.dev/auto-eval-of-question-answering-tasks/),
    and evaluating them. In this case, an LLM is used to create example questions
    and answers for a given set of documents, and another LLM is used to evaluate
    whether the RAG can provide the correct answer to these questions. This is perhaps
    better explained in this [tutorial on RAG evaluation with LangChain](https://learn.deeplearning.ai/langchain/lesson/6/evaluation).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œå¯èƒ½æ²¡æœ‰é€‚ç”¨äºé¢†åŸŸç‰¹å®šRAGçš„åˆé€‚è¯„ä¼°æ•°æ®é›†ã€‚å¯¹äºè¿™ç§æƒ…å†µï¼Œå¯ä»¥è€ƒè™‘ä»ä¸€äº›é€šç”¨NLPè¯„ä¼°æ•°æ®é›†å¼€å§‹ï¼Œä¾‹å¦‚[è¿™ä¸ªåˆ—è¡¨](https://paperswithcode.com/task/question-answering#:~:text=Popular%20benchmark%20datasets%20for%20evaluation%20question%20answering%20systems%20include%20SQuAD,models%20are%20T5%20and%20XLNet.)ã€‚åƒLangChainè¿™æ ·çš„å·¥å…·è¿˜æä¾›äº†[è‡ªåŠ¨ç”Ÿæˆé—®é¢˜å’Œç­”æ¡ˆ](https://blog.langchain.dev/auto-eval-of-question-answering-tasks/)å¹¶è¿›è¡Œè¯„ä¼°çš„æ”¯æŒã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½¿ç”¨ä¸€ä¸ªLLMä¸ºç»™å®šæ–‡æ¡£é›†åˆ›å»ºç¤ºä¾‹é—®é¢˜å’Œç­”æ¡ˆï¼Œå¦ä¸€ä¸ªLLMç”¨äºè¯„ä¼°RAGæ˜¯å¦èƒ½å¤Ÿæä¾›è¿™äº›é—®é¢˜çš„æ­£ç¡®ç­”æ¡ˆã€‚ä¹Ÿè®¸å¯ä»¥åœ¨è¿™ä¸ª[LangChainçš„RAGè¯„ä¼°æ•™ç¨‹](https://learn.deeplearning.ai/langchain/lesson/6/evaluation)ä¸­æ›´å¥½åœ°è§£é‡Šã€‚
- en: While the generic solutions are likely good to start with, in a real project
    I would try to collect a real dataset of questions and answers from the domain
    experts and the intended users of the RAG solution. As the LLM is typically expected
    to generate a natural language response, this can vary a lot while still being
    correct. For this reason, evaluating if the answer was correct or not is not as
    straightforward as a regular expression or similar pattern matching. Here, I find
    the idea of using another LLM to evaluate whether the given response matches a
    reference response a very useful tool. These models can deal with the text variation
    much better.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶é€šç”¨è§£å†³æ–¹æ¡ˆåœ¨å¼€å§‹æ—¶å¯èƒ½å¾ˆå¥½ï¼Œä½†åœ¨å®é™…é¡¹ç›®ä¸­ï¼Œæˆ‘ä¼šå°è¯•æ”¶é›†æ¥è‡ªé¢†åŸŸä¸“å®¶å’Œç›®æ ‡ç”¨æˆ·çš„çœŸå®é—®é¢˜å’Œç­”æ¡ˆæ•°æ®é›†ã€‚ç”±äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸è¢«æœŸæœ›ç”Ÿæˆè‡ªç„¶è¯­è¨€å“åº”ï¼Œè¿™äº›å“åº”å¯èƒ½åœ¨æ­£ç¡®çš„å‰æä¸‹å˜åŒ–å¾ˆå¤§ã€‚å› æ­¤ï¼Œè¯„ä¼°ç­”æ¡ˆæ˜¯å¦æ­£ç¡®ä¸åƒæ­£åˆ™è¡¨è¾¾å¼æˆ–ç±»ä¼¼çš„æ¨¡å¼åŒ¹é…é‚£ä¹ˆç›´æ¥ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘å‘ç°ä½¿ç”¨å¦ä¸€ç§LLMæ¥è¯„ä¼°ç»™å®šçš„å“åº”æ˜¯å¦åŒ¹é…å‚è€ƒå“åº”æ˜¯ä¸€ä¸ªéå¸¸æœ‰ç”¨çš„å·¥å…·ã€‚è¿™äº›æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†æ–‡æœ¬å˜å¼‚ã€‚
- en: Conclusions
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: RAG is a very nice tool, and is quite a popular topic these days with the high
    interest in LLMâ€™s in general. While RAG and embeddings have been around for a
    good while, the latest powerful LLMâ€™s and their fast evolution have perhaps made
    them more interesting for many advanced use cases. I expect the field to keep
    evolving at a good pace, and it is sometimes a bit difficult to keep up to date
    on everything. For this, summaries such as reviews on [RAG developments](https://arxiv.org/pdf/2312.10997.pdf)
    can give points to at least keep the main developments in sight.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: RAG æ˜¯ä¸€ä¸ªéå¸¸ä¸é”™çš„å·¥å…·ï¼Œéšç€å¯¹LLMçš„é«˜åº¦å…³æ³¨ï¼Œå®ƒç°åœ¨ä¹Ÿæ˜¯ä¸€ä¸ªç›¸å½“çƒ­é—¨çš„è¯é¢˜ã€‚è™½ç„¶RAGå’ŒåµŒå…¥æŠ€æœ¯å·²ç»å­˜åœ¨äº†å¾ˆé•¿æ—¶é—´ï¼Œä½†æœ€æ–°çš„å¼ºå¤§LLMåŠå…¶å¿«é€Ÿæ¼”å˜å¯èƒ½ä½¿å®ƒä»¬åœ¨è®¸å¤šé«˜çº§åº”ç”¨åœºæ™¯ä¸­æ›´å…·å¸å¼•åŠ›ã€‚æˆ‘é¢„è®¡è¿™ä¸€é¢†åŸŸå°†æŒç»­ä»¥è‰¯å¥½çš„é€Ÿåº¦å‘å±•ï¼Œæœ‰æ—¶å¾ˆéš¾è·Ÿä¸Šæ‰€æœ‰æœ€æ–°åŠ¨æ€ã€‚ä¸ºæ­¤ï¼Œåƒ[RAG
    å‘å±•ç»¼è¿°](https://arxiv.org/pdf/2312.10997.pdf)è¿™æ ·çš„æ€»ç»“å¯ä»¥æä¾›è‡³å°‘ä¿æŒä¸»è¦å‘å±•æ–¹å‘çš„å‚è€ƒã€‚
- en: 'The RAG approach in general is quite simple: find a set of chunks of text similar
    to the given query, concatenate them into a context, and ask the LLM for an answer.
    However, as I tried to show here, there can be various issues to consider in how
    to make this work well and efficiently for different needs. From good context
    retrieval, to ranking and selecting the best results, and finally being able to
    link the results back to actual source documents. And evaluating the resulting
    query contexts and answers. And as [Stack Overflow people noted](https://stackoverflow.blog/2023/07/31/ask-like-a-human-implementing-semantic-search-on-stack-overflow/),
    sometimes the more traditional lexical or hybrid search is very useful as well,
    even if semantic search is cool.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€èˆ¬æ¥è¯´ï¼ŒRAG æ–¹æ³•ç›¸å½“ç®€å•ï¼šæ‰¾åˆ°ä¸ç»™å®šæŸ¥è¯¢ç±»ä¼¼çš„ä¸€ç»„æ–‡æœ¬å—ï¼Œå°†å®ƒä»¬æ‹¼æ¥æˆä¸Šä¸‹æ–‡ï¼Œç„¶åå‘LLMè¯·æ±‚ç­”æ¡ˆã€‚ç„¶è€Œï¼Œæ­£å¦‚æˆ‘åœ¨è¿™é‡Œå°è¯•å±•ç¤ºçš„é‚£æ ·ï¼Œåœ¨å¦‚ä½•ä½¿è¿™ä¸€è¿‡ç¨‹å¯¹ä¸åŒéœ€æ±‚æœ‰æ•ˆä¸”é«˜æ•ˆæ–¹é¢ï¼Œå¯èƒ½ä¼šæœ‰å„ç§é—®é¢˜éœ€è¦è€ƒè™‘ã€‚ä»è‰¯å¥½çš„ä¸Šä¸‹æ–‡æ£€ç´¢ï¼Œåˆ°æ’åå’Œé€‰æ‹©æœ€ä½³ç»“æœï¼Œæœ€åèƒ½å¤Ÿå°†ç»“æœé“¾æ¥å›å®é™…çš„æºæ–‡æ¡£ã€‚è¿˜è¦è¯„ä¼°ç”Ÿæˆçš„æŸ¥è¯¢ä¸Šä¸‹æ–‡å’Œç­”æ¡ˆã€‚æ­£å¦‚[Stack
    Overflow çš„äººä»¬æŒ‡å‡ºçš„](https://stackoverflow.blog/2023/07/31/ask-like-a-human-implementing-semantic-search-on-stack-overflow/)ï¼Œæœ‰æ—¶æ›´ä¼ ç»Ÿçš„è¯æ±‡æœç´¢æˆ–æ··åˆæœç´¢ä¹Ÿéå¸¸æœ‰ç”¨ï¼Œå³ä½¿è¯­ä¹‰æœç´¢ä¹Ÿå¾ˆé…·ã€‚
- en: Thatâ€™s all for today. RAG onâ€¦
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ä»Šå¤©å°±åˆ°è¿™é‡Œã€‚RAGç»§ç»­...
- en: '![](../Images/852104944e2f6138ecb58522d8e91819.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/852104944e2f6138ecb58522d8e91819.png)'
- en: ChatGPT+/DALL-E3 vision of what it means to RAG on..
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT+/DALL-E3 å¯¹ RAG çš„ç†è§£..
- en: '*Originally published at* [*http://teemukanstren.com*](https://teemukanstren.com/2023/12/25/llmrag-based-question-answering/)
    *on December 25, 2023.*'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '*æœ€åˆå‘å¸ƒäº* [*http://teemukanstren.com*](https://teemukanstren.com/2023/12/25/llmrag-based-question-answering/)
    *äº2023å¹´12æœˆ25æ—¥ã€‚*'
