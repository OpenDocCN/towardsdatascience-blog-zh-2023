- en: How t-SNE Outperforms PCA in Dimensionality Reduction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何 t-SNE 超越 PCA 实现维度减少
- en: 原文：[https://towardsdatascience.com/how-t-sne-outperforms-pca-in-dimensionality-reduction-7a3975e8cbdb?source=collection_archive---------5-----------------------#2023-05-23](https://towardsdatascience.com/how-t-sne-outperforms-pca-in-dimensionality-reduction-7a3975e8cbdb?source=collection_archive---------5-----------------------#2023-05-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-t-sne-outperforms-pca-in-dimensionality-reduction-7a3975e8cbdb?source=collection_archive---------5-----------------------#2023-05-23](https://towardsdatascience.com/how-t-sne-outperforms-pca-in-dimensionality-reduction-7a3975e8cbdb?source=collection_archive---------5-----------------------#2023-05-23)
- en: PCA vs t-SNE for visualizing high-dimensional data in a lower-dimensional space
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PCA 与 t-SNE 在低维空间中可视化高维数据的对比
- en: '[](https://rukshanpramoditha.medium.com/?source=post_page-----7a3975e8cbdb--------------------------------)[![Rukshan
    Pramoditha](../Images/b80426aff64ff186cb915795644590b1.png)](https://rukshanpramoditha.medium.com/?source=post_page-----7a3975e8cbdb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7a3975e8cbdb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7a3975e8cbdb--------------------------------)
    [Rukshan Pramoditha](https://rukshanpramoditha.medium.com/?source=post_page-----7a3975e8cbdb--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://rukshanpramoditha.medium.com/?source=post_page-----7a3975e8cbdb--------------------------------)[![Rukshan
    Pramoditha](../Images/b80426aff64ff186cb915795644590b1.png)](https://rukshanpramoditha.medium.com/?source=post_page-----7a3975e8cbdb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7a3975e8cbdb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7a3975e8cbdb--------------------------------)
    [Rukshan Pramoditha](https://rukshanpramoditha.medium.com/?source=post_page-----7a3975e8cbdb--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff90a3bb1d400&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-t-sne-outperforms-pca-in-dimensionality-reduction-7a3975e8cbdb&user=Rukshan+Pramoditha&userId=f90a3bb1d400&source=post_page-f90a3bb1d400----7a3975e8cbdb---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7a3975e8cbdb--------------------------------)
    ·15 min read·May 23, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7a3975e8cbdb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-t-sne-outperforms-pca-in-dimensionality-reduction-7a3975e8cbdb&user=Rukshan+Pramoditha&userId=f90a3bb1d400&source=-----7a3975e8cbdb---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff90a3bb1d400&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-t-sne-outperforms-pca-in-dimensionality-reduction-7a3975e8cbdb&user=Rukshan+Pramoditha&userId=f90a3bb1d400&source=post_page-f90a3bb1d400----7a3975e8cbdb---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7a3975e8cbdb--------------------------------)
    ·15 min 阅读·2023年5月23日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7a3975e8cbdb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-t-sne-outperforms-pca-in-dimensionality-reduction-7a3975e8cbdb&user=Rukshan+Pramoditha&userId=f90a3bb1d400&source=-----7a3975e8cbdb---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a3975e8cbdb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-t-sne-outperforms-pca-in-dimensionality-reduction-7a3975e8cbdb&source=-----7a3975e8cbdb---------------------bookmark_footer-----------)![](../Images/3b7e20bb0c5cf927b95d2c4314e06247.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a3975e8cbdb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-t-sne-outperforms-pca-in-dimensionality-reduction-7a3975e8cbdb&source=-----7a3975e8cbdb---------------------bookmark_footer-----------)![](../Images/3b7e20bb0c5cf927b95d2c4314e06247.png)'
- en: Photo by [Pat Whelen](https://unsplash.com/@patwhelen?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/Rxt252RzQlY?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Pat Whelen](https://unsplash.com/@patwhelen?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来源于 [Unsplash](https://unsplash.com/photos/Rxt252RzQlY?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: In machine learning, **dimensionality reduction** refers to reducing the number
    of input variables in the dataset. The number of input variables refers to the
    **dimensionality** of the dataset.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，**维度减少**指的是减少数据集中的输入变量数量。输入变量的数量即为数据集的**维度**。
- en: 'Dimensionality reduction techniques are mainly divided into two main categories:
    *Linear* and *Non-linear (Manifold)*.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 维度减少技术主要分为两大类：*线性* 和 *非线性（流形）*。
- en: Under linear methods, we have discussed [*Principal Component Analysis (PCA)*](https://medium.com/data-science-365/3-easy-steps-to-perform-dimensionality-reduction-using-principal-component-analysis-pca-79121998b991),
    [*Factor Analysis (FA)*](/factor-analysis-on-women-track-records-data-with-r-and-python-6731a73cd2e0),
    [*Linear Discriminant Analysis (LDA)*](/lda-is-highly-effective-than-pca-for-dimensionality-reduction-in-classification-datasets-4489eade632)
    and [*Non-Negative Matrix Factorization (NMF)*](/non-negative-matrix-factorization-nmf-for-dimensionality-reduction-in-image-data-8450f4cae8fa).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性方法下，我们讨论了[*主成分分析（PCA）*](https://medium.com/data-science-365/3-easy-steps-to-perform-dimensionality-reduction-using-principal-component-analysis-pca-79121998b991)、[*因子分析（FA）*](/factor-analysis-on-women-track-records-data-with-r-and-python-6731a73cd2e0)、[*线性判别分析（LDA）*](/lda-is-highly-effective-than-pca-for-dimensionality-reduction-in-classification-datasets-4489eade632)和[*非负矩阵分解（NMF）*](/non-negative-matrix-factorization-nmf-for-dimensionality-reduction-in-image-data-8450f4cae8fa)。
- en: Under non-linear methods, we have discussed [*Autoencoders (AEs)*](/how-autoencoders-outperform-pca-in-dimensionality-reduction-1ae44c68b42f)
    and [*Kernel PCA*](/dimensionality-reduction-for-linearly-inseparable-data-5030f0dc0f5e).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在非线性方法下，我们讨论了[*自编码器（AEs）*](/how-autoencoders-outperform-pca-in-dimensionality-reduction-1ae44c68b42f)和[*核主成分分析（Kernel
    PCA）*](/dimensionality-reduction-for-linearly-inseparable-data-5030f0dc0f5e)。
- en: '**t-Distributed Stochastic Neighbor Embedding (t-SNE)** is also a *non-linear*
    dimensionality reduction method used for visualizing high-dimensional data in
    a lower-dimensional space to find important clusters or groups in the data.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**t-分布随机邻居嵌入（t-SNE）**也是一种*非线性*降维方法，用于在低维空间中可视化高维数据，以便找到数据中的重要簇或群体。'
- en: All dimensionality reduction techniques fall under the category of unsupervised
    machine learning in which we can reveal hidden patterns and important relationships
    in the data without requiring labels.
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 所有降维技术都属于无监督机器学习的范畴，在这种方法中，我们可以揭示数据中的隐藏模式和重要关系，而无需标签。
- en: ''
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So, dimensionality reduction algorithms deal with unlabeled data. When training
    such algorithms, the…
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 因此，降维算法处理的是未标记的数据。在训练这些算法时，…
