- en: Managing the Cloud Storage Costs of Big-Data Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理大数据应用程序的云存储成本
- en: 原文：[https://towardsdatascience.com/managing-the-cloud-storage-costs-of-big-data-applications-e3cbd92cf51c?source=collection_archive---------15-----------------------#2023-06-26](https://towardsdatascience.com/managing-the-cloud-storage-costs-of-big-data-applications-e3cbd92cf51c?source=collection_archive---------15-----------------------#2023-06-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/managing-the-cloud-storage-costs-of-big-data-applications-e3cbd92cf51c?source=collection_archive---------15-----------------------#2023-06-26](https://towardsdatascience.com/managing-the-cloud-storage-costs-of-big-data-applications-e3cbd92cf51c?source=collection_archive---------15-----------------------#2023-06-26)
- en: Tips for Reducing the Expense of Using Cloud-Based Storage
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 降低使用基于云的存储成本的技巧
- en: '[](https://chaimrand.medium.com/?source=post_page-----e3cbd92cf51c--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page-----e3cbd92cf51c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e3cbd92cf51c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e3cbd92cf51c--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page-----e3cbd92cf51c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://chaimrand.medium.com/?source=post_page-----e3cbd92cf51c--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page-----e3cbd92cf51c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e3cbd92cf51c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e3cbd92cf51c--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page-----e3cbd92cf51c--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9440b37e27fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanaging-the-cloud-storage-costs-of-big-data-applications-e3cbd92cf51c&user=Chaim+Rand&userId=9440b37e27fe&source=post_page-9440b37e27fe----e3cbd92cf51c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e3cbd92cf51c--------------------------------)
    ·11 min read·Jun 26, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe3cbd92cf51c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanaging-the-cloud-storage-costs-of-big-data-applications-e3cbd92cf51c&user=Chaim+Rand&userId=9440b37e27fe&source=-----e3cbd92cf51c---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9440b37e27fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanaging-the-cloud-storage-costs-of-big-data-applications-e3cbd92cf51c&user=Chaim+Rand&userId=9440b37e27fe&source=post_page-9440b37e27fe----e3cbd92cf51c---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e3cbd92cf51c--------------------------------)
    · 11分钟阅读 · 2023年6月26日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe3cbd92cf51c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanaging-the-cloud-storage-costs-of-big-data-applications-e3cbd92cf51c&user=Chaim+Rand&userId=9440b37e27fe&source=-----e3cbd92cf51c---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe3cbd92cf51c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanaging-the-cloud-storage-costs-of-big-data-applications-e3cbd92cf51c&source=-----e3cbd92cf51c---------------------bookmark_footer-----------)![](../Images/c6ef4be9850fa0fcf28ebe2f3ea7e0c7.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe3cbd92cf51c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanaging-the-cloud-storage-costs-of-big-data-applications-e3cbd92cf51c&source=-----e3cbd92cf51c---------------------bookmark_footer-----------)![](../Images/c6ef4be9850fa0fcf28ebe2f3ea7e0c7.png)'
- en: Photo by [JOSHUA COLEMAN](https://unsplash.com/@joshstyle?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/storage?orientation=landscape&license=free&utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [JOSHUA COLEMAN](https://unsplash.com/@joshstyle?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来源于 [Unsplash](https://unsplash.com/s/photos/storage?orientation=landscape&license=free&utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: With the growing reliance on ever-increasing amounts of data, modern day companies
    are more dependent than ever on high-capacity and highly scalable data-storage
    solutions. For many companies this solution comes in the form of cloud-based storage
    service, such as [Amazon S3](https://aws.amazon.com/s3/), [Google Cloud Storage](https://cloud.google.com/storage),
    and [Azure Blob Storage](https://azure.microsoft.com/en-us/products/category/storage/),
    each of which come with a rich set of APIs and features (e.g., [multi-tier storage](https://aws.amazon.com/s3/storage-classes/intelligent-tiering/))
    supporting a wide variety of data storage designs. Of course, cloud storage services
    also have an associated cost. This cost is usually comprised of a number of components
    including the overall size of the storage space you use, as well as activities
    such as transferring data into, out of, or within cloud storage. The price of
    Amazon S3, for example, includes (as of the time of this writing) [six cost components](https://aws.amazon.com/s3/pricing/?p=pm&c=s3&z=4),
    each of which need to be taken into consideration. It’s easy to see how managing
    the cost of cloud storage can get complicated, and designated calculators (e.g.,
    [here](https://calculator.aws/#/)) have been developed to assist with this.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对不断增长的数据量的依赖，现代公司比以往任何时候都更加依赖高容量和高度可扩展的数据存储解决方案。对于许多公司来说，这种解决方案通常表现为云存储服务，如[Amazon
    S3](https://aws.amazon.com/s3/)、[Google Cloud Storage](https://cloud.google.com/storage)和[Azure
    Blob Storage](https://azure.microsoft.com/en-us/products/category/storage/)，这些服务都提供了丰富的API和功能（例如，[多层存储](https://aws.amazon.com/s3/storage-classes/intelligent-tiering/)），支持各种数据存储设计。当然，云存储服务也有相关的成本。这些成本通常包括多个组成部分，包括你使用的存储空间的总体大小，以及如将数据传入、传出或在云存储中传输等活动。例如，Amazon
    S3的价格（截至本文撰写时）包括[六个成本组成部分](https://aws.amazon.com/s3/pricing/?p=pm&c=s3&z=4)，每个部分都需要考虑。可以看出，管理云存储成本可能变得复杂，因此开发了指定的计算器（例如，[这里](https://calculator.aws/#/)）来协助处理这一问题。
- en: In a [recent post](/on-the-importance-of-compressing-big-data-edd4cc7441d2),
    we expanded on the importance of designing your data and your data usage so as
    to reduce the costs associated with data storage. Our focus there was on using
    data compression as a way to reduce the overall size of your data. In this post
    we focus on a sometimes overlooked cost-component of cloud storage — the [cost
    of API requests made against your cloud storage buckets and data objects](https://aws.amazon.com/s3/pricing/?nc=sn&loc=4).
    We will demonstrate, by example, why this component is often underestimated and
    how it can become a significant portion of the cost of your big data application,
    if not managed properly. We will then discuss a couple of simple ways to keep
    this cost under control.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在一篇[近期文章](/on-the-importance-of-compressing-big-data-edd4cc7441d2)中，我们扩展了设计数据和数据使用的重要性，以降低与数据存储相关的成本。我们在那里关注的是使用数据压缩作为减少数据总体大小的一种方法。在这篇文章中，我们关注的是一个有时被忽视的云存储成本组成部分
    —— [对你的云存储桶和数据对象进行的API请求成本](https://aws.amazon.com/s3/pricing/?nc=sn&loc=4)。我们将通过示例展示为什么这个组成部分常常被低估，以及如果管理不当，它如何可能成为你大数据应用成本的重要组成部分。然后我们将讨论几种简单的方法来控制这一成本。
- en: Disclaimers
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 免责声明
- en: Although our demonstrations will use Amazon S3, the contents of this post are
    just as applicable to any other cloud storage service. Please do not interpret
    our choice of Amazon S3 or any other tool, service, or library we should mention,
    as an endorsement for their use. The best option for you will depend on the unique
    details of your own project. Furthermore, please keep in mind that any design
    choice regarding how you store and use your data will have its pros and cons that
    should be weighed heavily based on the details of your own project.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的演示将使用Amazon S3，但本文的内容同样适用于任何其他云存储服务。请不要将我们选择Amazon S3或我们提到的任何其他工具、服务或库解读为对其使用的支持。最适合你的选项将取决于你自己项目的具体细节。此外，请记住，关于如何存储和使用数据的任何设计选择都会有其利弊，这些利弊应根据你自己项目的细节权衡。
- en: This post will include a number of experiments that were run on an [Amazon EC2
    c5.4xlarge](https://aws.amazon.com/ec2/instance-types/c5/) instance (with 16 vCPUs
    and [“up to 10 Gbps” of network bandwidth](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-network-bandwidth.html)).
    We will share their outputs as examples of the comparative results you might see.
    Keep in mind that the outputs may vary greatly based on the environment in which
    the experiments are run. Please do not rely on the results presented here for
    your own design decisions. We strongly encourage you to run these as well as additional
    experiments before deciding what is best for your own projects.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将包括在 [Amazon EC2 c5.4xlarge](https://aws.amazon.com/ec2/instance-types/c5/)
    实例（具有 16 个 vCPUs 和 [“高达 10 Gbps” 的网络带宽](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-network-bandwidth.html)）上运行的一系列实验。我们将分享这些实验的输出，作为你可能看到的比较结果的示例。请注意，输出结果可能会因实验运行的环境而有很大不同。请不要将这里呈现的结果作为你自己设计决策的依据。我们强烈建议你在决定自己项目的最佳方案之前，运行这些以及其他额外的实验。
- en: A Simple Thought Experiment
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个简单的思考实验
- en: Suppose you have a data transformation application that acts on 1 MB data samples
    from S3 and produces 1 MB data outputs that are uploaded to S3\. Suppose that
    you are tasked with transforming 1 billion data samples by running your application
    on an appropriate [Amazon EC2 instance](https://aws.amazon.com/ec2/) (in the same
    region as your S3 bucket in order to avoid data transfer costs). Now let’s assume
    that [Amazon S3 charges](https://aws.amazon.com/s3/pricing/) $0.0004 for every
    1000 GET operations and $0.005 for every 1000 PUT operations (as at the time of
    this writing). At first glance, these costs might seem so low that they would
    be negligible compared to the other costs related to the data transformation.
    However, a simple calculation shows that our Amazon S3 API calls alone will tally
    a bill of **$5,400**!! This can easily be the most dominant cost factor of your
    project, even more than the cost of the compute instance. We will return to this
    thought experiment at the end of the post.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个数据转换应用程序，处理来自 S3 的 1 MB 数据样本，并生成 1 MB 数据输出，然后将其上传到 S3。假设你的任务是通过在合适的 [Amazon
    EC2 实例](https://aws.amazon.com/ec2/) 上运行你的应用程序来转换 10 亿个数据样本（与 S3 存储桶位于同一地区，以避免数据传输成本）。现在假设
    [Amazon S3 收费](https://aws.amazon.com/s3/pricing/) 每 1000 次 GET 操作收费 $0.0004，每
    1000 次 PUT 操作收费 $0.005（截至本文撰写时）。乍一看，这些成本似乎如此低，以至于与数据转换相关的其他成本相比，几乎可以忽略不计。然而，简单的计算显示，单是我们的
    Amazon S3 API 调用就会产生**$5,400**的费用!! 这可能是你项目中最主要的成本因素，甚至比计算实例的成本还要高。我们将在本文最后回到这个思考实验。
- en: Batch Data into Large Files
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据批量归为大文件
- en: The obvious way to reduce the costs of the API calls is to group samples together
    into files of a larger size and run the transformation on batches of samples.
    Denoting our batch size by *N*, this strategy could potentially reduce our cost
    by a factor of *N* (assuming that multi-part file transfer is not used — see below).
    This technique would save money not just on the PUT and GET calls but on **all**
    of the cost components of Amazon S3 that are dependent on the number of object
    files rather than the overall size of the data (e.g., [lifecycle transition requests](https://aws.amazon.com/s3/pricing/?trk=c8974be7-bc21-436d-8108-722e8ab912e1&sc_channel=ps&ef_id=Cj0KCQjw7aqkBhDPARIsAKGa0oJZYkx6SjrewVcE--kFsSRWT1-nzsS07TMivfP1P1SVqRSO4mKx_MIaAkCBEALw_wcB%3AG%3As&s_kwcid=AL%214422%213%21645125274431%21e%21%21g%21%21s3+pricing%2119574556914%21145779857032)).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 降低 API 调用成本的显而易见方法是将样本归为大文件，并在样本批次上进行转换。将我们的批量大小记作*N*，这种策略可能将我们的成本降低*N*倍（假设没有使用多部分文件传输——见下文）。这种技术不仅在
    PUT 和 GET 调用上节省了费用，还能在**所有**与对象文件数量相关的 Amazon S3 成本组成部分上节省开支，而不是数据的整体大小（例如，[生命周期转换请求](https://aws.amazon.com/s3/pricing/?trk=c8974be7-bc21-436d-8108-722e8ab912e1&sc_channel=ps&ef_id=Cj0KCQjw7aqkBhDPARIsAKGa0oJZYkx6SjrewVcE--kFsSRWT1-nzsS07TMivfP1P1SVqRSO4mKx_MIaAkCBEALw_wcB%3AG%3As&s_kwcid=AL%214422%213%21645125274431%21e%21%21g%21%21s3+pricing%2119574556914%21145779857032))。
- en: There are a number of disadvantages to grouping samples together. For example,
    when you store samples individually, you can freely access any one of them at
    will. This becomes more challenging when samples are grouped together. (See [this
    post](/training-from-cloud-storage-with-s5cmd-5c8fb5c06056) for more on the pros
    and cons of batching samples into large files.) If you do opt for grouping samples
    together, the big question is how to choose the size *N*. A larger *N* could reduce
    storage costs but might introduce latency, increase the compute time, and, by
    extension, increase the compute costs. Finding the optimal number may require
    some experimentation that takes into account these and additional considerations.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 将样本组合在一起有一些缺点。例如，当你单独存储样本时，你可以随时访问其中的任何一个。当样本组合在一起时，这就变得更加具有挑战性。（有关将样本批处理成大文件的优缺点，请参见[这篇文章](/training-from-cloud-storage-with-s5cmd-5c8fb5c06056)）。如果你选择将样本组合在一起，那么最大的疑问是如何选择大小
    *N*。较大的 *N* 可能会降低存储成本，但可能会引入延迟，增加计算时间，并由此增加计算成本。找到最佳数量可能需要一些实验，考虑到这些以及其他额外的因素。
- en: But let’s not kid ourselves. Making this kind of change will not be easy. Your
    data may have many consumers (both human and artificial) each with their own particular
    set of demands and constraints. Storing your samples in separate files can make
    it easier to keep everyone happy. Finding a batching strategy that satisfies everyone
    will be difficult.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们不要自欺欺人。进行这种更改并不容易。你的数据可能有很多消费者（包括人类和人工智能），每个人都有自己特定的需求和限制。将样本存储在单独的文件中可以更容易地让每个人满意。找到一个能够满足所有人需求的批处理策略将会很困难。
- en: 'Possible Compromise: Batched Puts, Individual Gets'
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可能的折衷方案：批量 PUT，单独 GET
- en: A compromise you might consider is to upload large files with grouped samples
    while enabling access to individual samples. One way to do this is to maintain
    an index file with the locations of each sample (the file in which it is grouped,
    the start-offset, and the end-offset) and expose a thin API layer to each consumer
    that would enable them to freely download individual samples. The API would be
    implemented using the index file and an S3 API that enables extracting specific
    ranges from object files (e.g., Boto3’s [get_object](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/get_object.html)
    function). While this kind of solution would not save any money on GET calls (since
    we are still pulling the same number of individual samples), the more expensive
    PUT calls would be reduced since we would be uploading a lower number of larger
    files. Note that this kind of solution poses some limitations on the library we
    use to interact with S3 as it depends on an API that allows for extracting partial
    chunks of the large file objects. In previous posts (e.g., [here](https://medium.com/towards-data-science/streaming-big-data-files-from-cloud-storage-634e54818e75))
    we have discussed the different ways of interfacing with S3, many of which do
    **not** support this feature.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以考虑的一种折衷方案是上传包含分组样本的大文件，同时允许访问单个样本。实现这一点的一种方法是维护一个索引文件，其中包含每个样本的位置（其所在的文件、起始偏移量和结束偏移量），并向每个消费者提供一个轻量的
    API 层，以便他们可以自由下载单个样本。该 API 将使用索引文件和一个允许从对象文件中提取特定范围的 S3 API（例如，Boto3 的 [get_object](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/get_object.html)
    函数）来实现。虽然这种解决方案不会节省 GET 调用的费用（因为我们仍然要提取相同数量的单个样本），但由于我们上传的是较少的较大文件，因此更昂贵的 PUT
    调用将会减少。请注意，这种解决方案对我们与 S3 交互所使用的库有一些限制，因为它依赖于一个允许从大型文件对象中提取部分块的 API。在之前的帖子中（例如，[这里](https://medium.com/towards-data-science/streaming-big-data-files-from-cloud-storage-634e54818e75)），我们讨论了与
    S3 接口的不同方式，其中许多方式**不**支持此功能。
- en: The code block below demonstrates how to implement a simple [PyTorch dataset](https://pytorch.org/docs/stable/data.html)
    (with PyTorch version 1.13) that uses the [Boto3 get_object](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.get_object)
    API to extract individual 1 MB samples from large files of grouped samples. We
    compare the speed of iterating the data in this manner to iterating the samples
    that are stored in individual files.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码块演示了如何实现一个简单的 [PyTorch 数据集](https://pytorch.org/docs/stable/data.html)（使用
    PyTorch 版本 1.13），该数据集使用 [Boto3 get_object](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.get_object)
    API 从包含分组样本的大文件中提取单个 1 MB 样本。我们将这种方式迭代数据的速度与迭代存储在单独文件中的样本的速度进行比较。
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The table below summarizes the speed of data traversal for different choices
    of the sample grouping size, *N*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 下表总结了不同样本分组大小 *N* 的数据遍历速度。
- en: '![](../Images/e0bd7fc2dd13510cfdc9762f48d73fe2.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e0bd7fc2dd13510cfdc9762f48d73fe2.png)'
- en: Impact of Different Grouping Strategies on Data Traversal Time (by Author)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 不同分组策略对数据遍历时间的影响（作者）
- en: Note, that although these results strongly imply that grouping samples into
    large files has a relatively small impact on the performance of extracting them
    individually, we have found that the comparative results vary based on the sample
    size, file size, the values of the file offsets, the number of concurrent reads
    from the same file, etc. Although we are not privy to the internal workings of
    the Amazon S3 service, it is not surprising that considerations such as memory
    size, memory alignment, and throttling would impact performance. Finding the optimal
    configuration for your data will likely require a bit of experimentation.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，尽管这些结果强烈暗示将样本分组到大文件中对单独提取它们的性能影响相对较小，但我们发现比较结果会根据样本大小、文件大小、文件偏移量的值、对同一文件的并发读取次数等有所变化。虽然我们无法了解
    Amazon S3 服务的内部工作原理，但考虑到内存大小、内存对齐和限制等因素对性能的影响也不足为奇。找到适合你数据的最佳配置可能需要一些实验。
- en: One significant factor that could interfere with the money-saving grouping strategy
    we have described here is the use of multi-part downloading and uploading, which
    we will discuss in the next section.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可能干扰我们在这里描述的节省成本的分组策略的重要因素是多部分下载和上传的使用，我们将在下一节中讨论。
- en: Use Tools that Enable Control Over Multi-part Data Transfer
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用可以控制多部分数据传输的工具
- en: 'Many cloud storage service providers support the option of multi-part uploading
    and downloading of object files. In multi-part data transfer, files that are larger
    than a certain threshold are divided into multiple parts that are transferred
    concurrently. This is a critical feature if you want to speed up the data transfer
    of large files. AWS [recommends using multi-part upload for files larger than
    100 MB](https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html).
    In the following simple example, we compare the download time of a 2 GB file with
    the multi-part *threshold* and *chunk-size* set to different values:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 许多云存储服务提供商支持对象文件的多部分上传和下载选项。在多部分数据传输中，大于某个阈值的文件被划分为多个部分并同时传输。如果你想加速大文件的数据传输，这是一项关键功能。AWS
    [建议对大于 100 MB 的文件使用多部分上传](https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html)。在以下简单示例中，我们比较了将
    2 GB 文件的多部分 *阈值* 和 *分块大小* 设置为不同值时的下载时间：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The results of this experiment are summarized in the table below:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 此实验的结果总结在下面的表格中：
- en: '![](../Images/15610cf9ad7376004c75cee5df9c88cf.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15610cf9ad7376004c75cee5df9c88cf.png)'
- en: Impact of Multi-part chunk size on Download Time (by Author)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 多部分分块大小对下载时间的影响（作者）
- en: Note that the relative comparison will greatly depend on the test environment
    and specifically on the speed and bandwidth of communication between the instance
    and the S3 bucket. Our experiment was run on an instance that was in the same
    region as the bucket. However, as the distance increases, so will the impact of
    using multi-part downloading.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，相对比较将大大依赖于测试环境，特别是实例与 S3 存储桶之间的通信速度和带宽。我们的实验是在与存储桶位于同一区域的实例上运行的。然而，随着距离的增加，使用多部分下载的影响也会增加。
- en: With regards to the topic of our discussion, it is important to note the cost
    implications of multi-part data transfer. Specifically, when you use multi-part
    data transfer, [you are charged for the API operation of each one of the file
    parts](https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html#mpuploadpricing).
    Consequently, using multi-part uploading/downloading will limit the cost savings
    potential of batching data samples into large files.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 关于我们讨论的话题，重要的是要注意多部分数据传输的成本影响。具体来说，当你使用多部分数据传输时，[你会为每个文件部分的 API 操作收费](https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html#mpuploadpricing)。因此，使用多部分上传/下载将限制将数据样本批量处理成大文件的成本节省潜力。
- en: Many APIs use multi-part downloading **by default**. This is great if your primary
    interest is reducing the latency of your interaction with S3\. But if your concern
    is limiting cost, this default behavior does not work in your favor. Boto3, for
    example, is a popular Python API for uploading and downloading files from S3\.
    If not specified, the boto3 S3 APIs such as [upload_file](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/upload_file.html)
    and [download_file](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/download_file.html)
    will use a default [TransferConfig](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/customizations/s3.html#boto3.s3.transfer.TransferConfig),
    which applies multi-part uploading/downloading with a chunk-size of 8 MB to any
    file larger than 8 MB. If you are responsible for controlling the cloud costs
    in your organization, you might be unhappily surprised to learn that these APIs
    are being widely used with their default settings. In many cases, you might find
    these settings to be unjustified and that increasing the multi-part *threshold*
    and *chunk-size* values, or disabling multi-part data transfer altogether, will
    have little impact on the performance of your application.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 许多API**默认**使用多部分下载。如果你的主要关注点是减少与S3的交互延迟，这种做法是很好的。但是，如果你的关心是限制成本，那么这种默认行为对你来说并不有利。例如，Boto3是一个流行的Python
    API，用于从S3上传和下载文件。如果未指定，boto3的S3 API如 [upload_file](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/upload_file.html)
    和 [download_file](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/download_file.html)
    将使用默认的 [TransferConfig](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/customizations/s3.html#boto3.s3.transfer.TransferConfig)，该配置将对任何大于8
    MB的文件应用8 MB的块大小的多部分上传/下载。如果你负责控制组织中的云成本，你可能会惊讶地发现这些API被广泛使用其默认设置。在许多情况下，你可能会发现这些设置是不必要的，增加多部分*阈值*和*块大小*值，或完全禁用多部分数据传输，对应用程序的性能影响很小。
- en: Example — Impact of Multi-part File Transfer Size on Speed and Cost
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例 — 多部分文件传输大小对速度和成本的影响
- en: 'In the code block below we create a simple multi-process transform function
    and measure the impact of the multi-part chunk size on its performance and cost:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码块中，我们创建一个简单的多进程转换函数，并测量多部分块大小对其性能和成本的影响：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In this example we have fixed the file size to 500 MB and applied the same multi-part
    settings to both the download and upload. A more complete analysis would vary
    the size of the data files and the multi-part settings.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将文件大小固定为500 MB，并对下载和上传应用了相同的多部分设置。一个更完整的分析将会变化数据文件的大小和多部分设置。
- en: In the table below we summarize the results of the experiment.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在下表中，我们总结了实验的结果。
- en: '![](../Images/a4fc335db1fe163928b43ba1615afbaf.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4fc335db1fe163928b43ba1615afbaf.png)'
- en: Impact of Multi-part Chunk Size on Data Transformation Speed and Cost (by Author)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 多部分块大小对数据转换速度和成本的影响（作者）
- en: The results indicate that up to a multi-part chunk size of 500 MB (the size
    of our files), the impact on the time of the data transformation is minimal. On
    the other hand, the potential savings to the cloud storage API costs is significant,
    up to 98.4% when compared with using Boto3’s default chunk size (8MB). Not only
    does this example demonstrate the cost benefit of grouping samples together, but
    it also implies an additional opportunity for savings through appropriate configuration
    of the multi-part data transfer settings.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，至多达到500 MB的多部分块大小（我们文件的大小），数据转换时间的影响是最小的。另一方面，与使用Boto3的默认块大小（8MB）相比，云存储API成本的潜在节省是显著的，最多可达98.4%。这个例子不仅展示了将样本分组的成本效益，而且还暗示了通过适当配置多部分数据传输设置获得额外节省的机会。
- en: Conclusion
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Let’s apply the results of our last example to the thought experiment we introduced
    at the top of this post. We showed that applying a simple transformation to 1
    billion data samples would cost $5,400 if the samples were stored in individual
    files. If we were to group the samples into 2 million files, each with 500 samples,
    and apply the transformation without multi-part data transfer (as in the last
    trial of the example above), the cost of the API calls would be reduced to $10.8!!
    At the same time, assuming the same test environment, the impact we would expect
    (based on our experiments) on the overall runtime would be relatively low. I would
    call that a pretty good deal. Wouldn’t you?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将上一个示例的结果应用到我们在本文开头介绍的思维实验中。我们展示了如果数据样本存储在单独的文件中，对1亿个数据样本应用简单转换的费用将是$5,400。如果我们将样本分组为200万个文件，每个文件有500个样本，并且在不使用多部分数据传输的情况下（如上述示例的最后一次试验），API调用的成本将减少到$10.8！！同时，假设测试环境相同，我们预期的整体运行时间的影响（基于我们的实验）将相对较低。我认为这是一个相当不错的交易。你觉得呢？
- en: Summary
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'When developing cloud-based big-data applications it is vital that we be fully
    familiar with **all** of the details of the costs of our activities. In this post
    we focused on the “Requests & data retrievals” component of the Amazon S3 pricing
    strategy. We demonstrated how this component can become a major part of the overall
    cost of a big-data application. We discussed two of the factors that can affect
    this cost: the manner in which data samples are grouped together and the way in
    which multi-part data transfer is used.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发基于云的大数据应用时，了解我们活动的**所有**成本细节至关重要。在这篇文章中，我们专注于Amazon S3定价策略中的“请求与数据检索”组件。我们演示了这个组件如何成为大数据应用总体成本的重要组成部分。我们讨论了可能影响这一成本的两个因素：数据样本的分组方式和多部分数据传输的使用方式。
- en: Naturally, optimizing just one cost component is likely to increase other components
    in a way that will raise the overall cost. An appropriate design for your data
    storage will need to take into account **all** potential cost factors **and will
    greatly depend on your specific data needs and usage patterns**.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 自然，仅优化一个成本组件可能会以某种方式增加其他组件，从而提高整体成本。数据存储的适当设计需要考虑**所有**潜在成本因素**并且将大大依赖于你的具体数据需求和使用模式**。
- en: As usual, please feel free to reach out with comments and corrections.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，请随时提出评论和修正。
