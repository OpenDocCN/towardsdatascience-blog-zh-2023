- en: What is Bayes Error?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是贝叶斯误差？
- en: 原文：[https://towardsdatascience.com/what-is-bayes-error-4bfadcc9c0ad?source=collection_archive---------5-----------------------#2023-06-06](https://towardsdatascience.com/what-is-bayes-error-4bfadcc9c0ad?source=collection_archive---------5-----------------------#2023-06-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/what-is-bayes-error-4bfadcc9c0ad?source=collection_archive---------5-----------------------#2023-06-06](https://towardsdatascience.com/what-is-bayes-error-4bfadcc9c0ad?source=collection_archive---------5-----------------------#2023-06-06)
- en: A simple introduction to a fundamental concept of machine learning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对机器学习基本概念的简单介绍
- en: '[](https://medium.com/@wanshunwong?source=post_page-----4bfadcc9c0ad--------------------------------)[![Wanshun
    Wong](../Images/42d967999b28ba8ab207f1858e6a4e6b.png)](https://medium.com/@wanshunwong?source=post_page-----4bfadcc9c0ad--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4bfadcc9c0ad--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4bfadcc9c0ad--------------------------------)
    [Wanshun Wong](https://medium.com/@wanshunwong?source=post_page-----4bfadcc9c0ad--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@wanshunwong?source=post_page-----4bfadcc9c0ad--------------------------------)[![Wanshun
    Wong](../Images/42d967999b28ba8ab207f1858e6a4e6b.png)](https://medium.com/@wanshunwong?source=post_page-----4bfadcc9c0ad--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4bfadcc9c0ad--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4bfadcc9c0ad--------------------------------)
    [Wanshun Wong](https://medium.com/@wanshunwong?source=post_page-----4bfadcc9c0ad--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb145fb04b8bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-bayes-error-4bfadcc9c0ad&user=Wanshun+Wong&userId=b145fb04b8bd&source=post_page-b145fb04b8bd----4bfadcc9c0ad---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4bfadcc9c0ad--------------------------------)
    ·6 min read·Jun 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4bfadcc9c0ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-bayes-error-4bfadcc9c0ad&user=Wanshun+Wong&userId=b145fb04b8bd&source=-----4bfadcc9c0ad---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb145fb04b8bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-bayes-error-4bfadcc9c0ad&user=Wanshun+Wong&userId=b145fb04b8bd&source=post_page-b145fb04b8bd----4bfadcc9c0ad---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4bfadcc9c0ad--------------------------------)
    ·6分钟阅读·2023年6月6日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4bfadcc9c0ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-bayes-error-4bfadcc9c0ad&user=Wanshun+Wong&userId=b145fb04b8bd&source=-----4bfadcc9c0ad---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4bfadcc9c0ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-bayes-error-4bfadcc9c0ad&source=-----4bfadcc9c0ad---------------------bookmark_footer-----------)![](../Images/7227c8d6f3145f701b9b4f4de11a018e.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4bfadcc9c0ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-bayes-error-4bfadcc9c0ad&source=-----4bfadcc9c0ad---------------------bookmark_footer-----------)![](../Images/7227c8d6f3145f701b9b4f4de11a018e.png)'
- en: Image by the author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: When making different decisions and estimations regarding a machine learning
    project, such as
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在做出不同的决定和估计时，例如
- en: deciding whether to start / keep working on the project or not,
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决定是否开始或继续进行项目，
- en: estimating the business impact of the project,
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估计项目的业务影响，
- en: choosing the main strategy for improving the model performance,
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择提高模型性能的主要策略，
- en: one of the most important considerations is how much room for model performance
    improvement is there. For example, suppose we have a binary classification model
    and its accuracy is 85%. We might then think that there is still plenty of room
    for improvement, and promise our boss at least a 5% increase in accuracy in a
    couple of weeks. However, this thought process of going from “85% accuracy” to
    “plenty of room for improvement” implicitly assumes the best possible model performance
    is 100% accuracy. Unfortunately, such assumptions are often not true, resulting
    in us having a misunderstanding of our project and making bad decisions.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的考虑因素之一是模型性能改善的空间有多大。例如，假设我们有一个二分类模型，其准确率为 85%。我们可能会认为仍然有很多改进的空间，并承诺在几周内提高至少
    5% 的准确率。然而，这种从“85% 准确率”到“改进空间很大”的思维过程隐含假设了最佳模型性能是 100% 准确率。不幸的是，这种假设通常是不真实的，导致我们对项目有误解并做出错误决策。
- en: In this article we will focus on the binary classification setting, and will
    use error rate (which is 1 - accuracy) as our model performance metric. Then,
    in order to have a good estimation of the room for reducing the model error rate,
    we will make use of a concept known as the Bayes Error (also known as the Bayes
    Error Rate).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将重点关注二分类设置，并使用误差率（即 1 - 准确率）作为我们的模型性能指标。然后，为了对减少模型误差率的空间进行良好的估计，我们将使用一个被称为贝叶斯误差（也称为贝叶斯误差率）的概念。
- en: Definition of Bayes Error
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯误差的定义
- en: The Bayes Error of a dataset is the lowest possible error rate that any model
    can achieve. In particular, if the Bayes Error is non-zero, then the two classes
    have some overlaps, and even the best model will make some wrong predictions.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的贝叶斯误差是任何模型能够达到的最低可能误差率。特别是，如果贝叶斯误差非零，那么两个类别之间有一些重叠，即使是最好的模型也会有一些错误的预测。
- en: 'There are many possible reasons for a dataset to have a non-zero Bayes Error.
    For example:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集存在非零贝叶斯误差可能有很多原因。例如：
- en: '**Poor data quality**: Some images in a computer vision dataset are very blurry.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据质量差**：计算机视觉数据集中某些图像非常模糊。'
- en: '**Mislabelled data**'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标记错误的数据**'
- en: '**The labelling process is inconsistent**: When deciding whether a job applicant
    should proceed to the next round of interview, different interviewers might have
    different opinions.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标记过程不一致**：在决定是否将求职者进入下一轮面试时，不同的面试官可能有不同的意见。'
- en: '**The data generating process is inherently stochastic**: Predicting heads
    or tails from coin flipping.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据生成过程本质上是随机的**：例如，从掷硬币中预测正面或反面。'
- en: '**Information missing from the feature vectors**: When predicting whether a
    baby has certain genetic traits or not, the feature vector contains information
    about the father but not the information about the mother.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征向量中缺少信息**：在预测婴儿是否具有某些遗传特征时，特征向量包含父亲的信息而没有母亲的信息。'
- en: In general, it is impossible to compute the exact value of the Bayes Error.
    However, there exist several estimation methods. The method that we are going
    to introduce is the simplest one, and it is based on soft labels.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，无法计算贝叶斯误差的确切值。然而，存在几种估计方法。我们将介绍的方法是最简单的一种，它基于软标签。
- en: Soft Labels
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 软标签
- en: First, let us denote the two classes of our dataset by 0 and 1\. The class label
    of every instance in our dataset is in the set {0, 1}, and there is no middle
    ground. In literature, this is known as hard labels (to contrast with soft labels).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们用 0 和 1 表示数据集中的两个类别。我们数据集中每个实例的类别标签都在 {0, 1} 集合中，并且没有中间状态。在文献中，这被称为硬标签（以与软标签对比）。
- en: 'Soft labels generalize hard labels by allowing middle ground and by incorporating
    our confidence (and uncertainty) about the class labels. It is defined as the
    probability of an instance belonging to class 1:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 软标签通过允许中间状态并结合我们对类别标签的信心（以及不确定性）来泛化硬标签。它定义为实例属于类别 1 的概率：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In particular, *s_i* takes value in the interval [0, 1]. Here are some examples:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，*s_i* 取值范围在 [0, 1] 内。以下是一些示例：
- en: '*s_i* = 1 means we are 100% confident that the instance belongs to class 1.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*s_i* = 1 意味着我们 100% 确信该实例属于类别 1。'
- en: '*s_i* = 0 means we are 100% confident that the instance belongs to class 0,
    because the probability of it belonging to class 1 is 0%.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*s_i* = 0 意味着我们 100% 确信该实例属于类别 0，因为它属于类别 1 的概率为 0%。'
- en: '*s_i* = 0.6 means we think it is more likely for the instance to be in class
    1, but we are not very sure.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*s_i* = 0.6意味着我们认为实例更可能属于类别1，但我们不是很确定。'
- en: Notice that we can always convert soft labels to hard labels by checking *s_i*
    > 0.5 or not.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们总是可以通过检查*s_i* > 0.5来将软标签转换为硬标签。
- en: How to obtain soft labels
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何获得软标签
- en: 'There are several common ways of obtaining soft labels:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 获取软标签的几种常见方法：
- en: The most obvious way is to ask our dataset annotator to provide both the class
    label and his/her confidence level about the label.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最明显的方法是要求我们的数据集标注者提供类别标签及其对标签的置信水平。
- en: If we have multiple annotators, we can ask them to provide hard labels for each
    instance. Then we can use the proportions of the hard labels as soft labels. E.g.
    If we have 5 annotators, 4 of them think *x_i* belongs to class 1, and the remaining
    one thinks *x_i* belongs to class 0, then *s_i* = 0.8
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们有多个标注者，我们可以要求他们为每个实例提供硬标签。然后我们可以将硬标签的比例作为软标签。例如，如果我们有5个标注者，其中4个认为*x_i*属于类别1，剩下的一个认为*x_i*属于类别0，那么*s_i*
    = 0.8
- en: If the class labels are derived from some data sources, then we can use the
    same data sources to compute the soft labels. E.g. We want to predict whether
    a student can pass an exam or not. Suppose the total score of the exam is 100,
    and a passing score is 50 or greater. Hence, the hard labels are obtained simply
    by checking if *score* ≥ 50\. To compute the soft labels, we can apply a calibration
    method such as Platt scaling to *score*.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果类别标签来源于某些数据源，那么我们可以使用相同的数据源来计算软标签。例如，我们想预测一个学生是否能通过考试。假设考试总分为100，及格分数为50或更高。因此，硬标签通过检查*score*
    ≥ 50获得。为了计算软标签，我们可以应用如Platt缩放之类的校准方法到*score*。
- en: Estimating Bayes Error
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 估计贝叶斯误差
- en: 'Intuitively speaking, it is not hard to believe that Bayes Error and soft labels
    are correlated. After all, if there is uncertainty about the class labels, then
    it makes sense that even the best model will make some wrong predictions. The
    formula for estimating the Bayes Error using soft labels is very straightforward:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 从直观上讲，不难相信贝叶斯误差与软标签是相关的。毕竟，如果对类别标签存在不确定性，那么即使是最好的模型也会产生一些错误预测。使用软标签估计贝叶斯误差的公式非常简单：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: which is the average of min(*s_i*, 1 - *s_i*). The simplicity of this formula
    makes it easy to use and applicable to many datasets.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这是min(*s_i*, 1 - *s_i*)的平均值。这个公式的简便性使其易于使用，并适用于许多数据集。
- en: Concrete Examples
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 具体示例
- en: First let us consider the extreme case where the soft labels are either 0 or
    1\. This means we are 100% certain about the class labels. The term min(*s_i*,
    1 - *s_i*) is always 0, hence *β* is also 0\. This agrees with our intuition that
    the best model will be able to avoid making wrong predictions for this dataset.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，让我们考虑极端情况，其中软标签为0或1。这意味着我们对类别标签100%确定。术语min(*s_i*, 1 - *s_i*)始终为0，因此*β*也为0。这与我们的直觉一致，即最好的模型能够避免对该数据集产生错误预测。
- en: Consider a more interesting case where we have 10 instances, and the soft labels
    are 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1\. Then
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑一个更有趣的情况，我们有10个实例，软标签为0.1、0.2、0.3、0.4、0.5、0.6、0.7、0.8、0.9、1。然后
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Making Use of Bayes Error
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用贝叶斯误差
- en: 'Having a good estimation of the Bayes Error not only allows us to understand
    our dataset more, but also helps us in the following ways:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对贝叶斯误差有一个良好的估计不仅可以让我们更好地理解数据集，还可以在以下方面帮助我们：
- en: Understand the room for model performance improvement
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解模型性能改进的空间
- en: Let us revisit the example given in the introduction of this article. Our model
    has accuracy 85%, which means the error rate is 15%. Suppose the Bayes Error is
    estimated to be 13%. In this case the room for improvement is actually only 2%.
    Most importantly, we should not promise our boss a 5% improvement in model performance.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下本文开头给出的例子。我们的模型准确率为85%，这意味着错误率为15%。假设贝叶斯误差估计为13%。在这种情况下，改进的空间实际上只有2%。最重要的是，我们不应该向老板承诺模型性能提高5%。
- en: '**Determine if we need a new dataset**'
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**确定我们是否需要新的数据集**'
- en: Very often we have some minimum model performance requirements for our machine
    learning projects. For example, our model error rate is required to be ≤ 10%,
    so that the customer support team won’t be overloaded. If the Bayes Error of our
    dataset is estimated to be 13%, then instead of working on our model we should
    look for a new dataset. Maybe we need better cameras and sensors to collect data,
    or maybe we need new data sources to add more independent variables to our feature
    vectors.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的机器学习项目通常有最低模型性能要求。例如，我们的模型误差率要求 ≤ 10%，以免客服团队过于繁忙。如果我们数据集的贝叶斯误差估计为13%，那么我们应该寻找新的数据集，而不是改进模型。也许我们需要更好的相机和传感器来收集数据，或者需要新的数据源来增加特征向量中的独立变量。
- en: Understand the bias-variance tradeoff
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解偏差-方差权衡
- en: Suppose our model has training error 8% and test error 10%. If we know the Bayes
    Error is close to 0%, then we can conclude that both the training error and the
    test error are large. Therefore, we should try to reduce the bias of our model.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的模型训练误差为8%，测试误差为10%。如果我们知道贝叶斯误差接近0%，则可以得出结论，训练误差和测试误差都很大。因此，我们应该尝试减少模型的偏差。
- en: On the other hand, if the Bayes Error is 7%, then
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果贝叶斯误差为7%，则
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: and we should work on the variance part instead.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应当改进方差部分。
- en: Further Reading
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: The Bayes Error estimation formula above is introduced in [2]. We refer to that
    paper for various theoretic properties of the formula such as the rate of convergence.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述贝叶斯误差估计公式在[2]中介绍。我们参考那篇论文了解公式的各种理论属性，如收敛速率。
- en: The lecture [Nuts and Bolts of Applying Deep Learning](https://www.youtube.com/watch?v=F1ka6a13S9I&t=3040s)
    by Andrew Ng talks about using human level performance as a proxy for the Bayes
    Error.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Andrew Ng 的讲座 [应用深度学习的基本原理](https://www.youtube.com/watch?v=F1ka6a13S9I&t=3040s)
    讨论了将人类水平的表现作为贝叶斯误差的代理。
- en: The Bayes Error quantifies the irreducible error of a given task. The decomposition
    of model error into bias, variance, and irreducible error for zero-one loss function
    (and other loss functions) is studied in [1].
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 贝叶斯误差量化了给定任务的不可减少误差。模型误差的偏差、方差和不可减少误差的分解（包括零一损失函数和其他损失函数）在[1]中进行了研究。
- en: '[3] shows that classifiers trained on soft labels generalize better to out-of-sample
    datasets, and are more resistant to adversarial attacks.'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3] 证明了训练于软标签上的分类器对样本外数据集的泛化能力更强，且对对抗攻击的抵抗力更强。'
- en: References
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: P. Domingos. [A Unified Bias-Variance Decomposition and its Applications](https://homes.cs.washington.edu/~pedrod/papers/mlc00a.pdf)
    (2000), ICML 2000.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: P. Domingos. [统一的偏差-方差分解及其应用](https://homes.cs.washington.edu/~pedrod/papers/mlc00a.pdf)
    (2000)，ICML 2000。
- en: T. Ishida, I. Yamane, N. Charoenphakdee, G. Niu, and M. Sugiyama. [Is the Performance
    of My Deep Network Too Good to Be True? A Direct Approach to Estimating the Bayes
    Error in Binary Classification](https://openreview.net/pdf?id=FZdJQgy05rz) (2023),
    ICLR 2023.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: T. Ishida、I. Yamane、N. Charoenphakdee、G. Niu 和 M. Sugiyama. [我的深度网络表现是否过于完美？一种直接估计二分类贝叶斯误差的方法](https://openreview.net/pdf?id=FZdJQgy05rz)
    (2023)，ICLR 2023。
- en: J.C. Peterson, R.M. Battleday, T.L. Griffiths, and O. Russakovsky. [Human uncertainty
    makes classification more robust](https://openaccess.thecvf.com/content_ICCV_2019/papers/Peterson_Human_Uncertainty_Makes_Classification_More_Robust_ICCV_2019_paper.pdf)
    (2019), ICCV 2019.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: J.C. Peterson、R.M. Battleday、T.L. Griffiths 和 O. Russakovsky. [人类不确定性使分类更加稳健](https://openaccess.thecvf.com/content_ICCV_2019/papers/Peterson_Human_Uncertainty_Makes_Classification_More_Robust_ICCV_2019_paper.pdf)
    (2019)，ICCV 2019。
