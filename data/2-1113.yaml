- en: How to Become a Data Engineer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•æˆä¸ºæ•°æ®å·¥ç¨‹å¸ˆ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/how-to-become-a-data-engineer-c0319cb226c2](https://towardsdatascience.com/how-to-become-a-data-engineer-c0319cb226c2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/how-to-become-a-data-engineer-c0319cb226c2](https://towardsdatascience.com/how-to-become-a-data-engineer-c0319cb226c2)
- en: A shortcut for beginners in 2024
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2024å¹´åˆå­¦è€…çš„æ·å¾„
- en: '[](https://mshakhomirov.medium.com/?source=post_page-----c0319cb226c2--------------------------------)[![ğŸ’¡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----c0319cb226c2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c0319cb226c2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c0319cb226c2--------------------------------)
    [ğŸ’¡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----c0319cb226c2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mshakhomirov.medium.com/?source=post_page-----c0319cb226c2--------------------------------)[![ğŸ’¡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----c0319cb226c2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c0319cb226c2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c0319cb226c2--------------------------------)
    [ğŸ’¡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----c0319cb226c2--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c0319cb226c2--------------------------------)
    Â·17 min readÂ·Oct 7, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒåœ¨[æ•°æ®ç§‘å­¦å‰æ²¿](https://towardsdatascience.com/?source=post_page-----c0319cb226c2--------------------------------)
    Â·17åˆ†é’Ÿé˜…è¯»Â·2023å¹´10æœˆ7æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/b49c1deff780a74750c4dfda9056b149.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b49c1deff780a74750c4dfda9056b149.png)'
- en: Photo by [Gabriel Vasiliu](https://unsplash.com/@gabimedia?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±[Gabriel Vasiliu](https://unsplash.com/@gabimedia?utm_source=medium&utm_medium=referral)æä¾›ï¼Œ[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)ä¸Šå‘å¸ƒ
- en: This story explains an accelerated way of getting into the data engineering
    role by learning the required skills and familiarising yourself with data engineering
    tools and techniques. It will be useful for beginner-level IT practitioners and
    intermediate software engineers who would like to make a career change. Through
    my years as a Head of Data Engineering for one of the most successful start-ups
    in the UK and mid-east, I learned a lot from my career and I would like to share
    this knowledge and experience with you. This is a reflection of my personal experience
    in the data engineering field I acquired over the last 12 years. I hope it will
    be useful for you.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ•…äº‹è§£é‡Šäº†ä¸€ç§åŠ é€Ÿè¿›å…¥æ•°æ®å·¥ç¨‹è§’è‰²çš„æ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ æ‰€éœ€çš„æŠ€èƒ½å¹¶ç†Ÿæ‚‰æ•°æ®å·¥ç¨‹å·¥å…·å’ŒæŠ€æœ¯ã€‚è¿™å¯¹åˆçº§ITä»ä¸šè€…å’Œå¸Œæœ›è½¬è¡Œçš„ä¸­çº§è½¯ä»¶å·¥ç¨‹å¸ˆå°†éå¸¸æœ‰ç”¨ã€‚é€šè¿‡æˆ‘ä½œä¸ºè‹±å›½å’Œä¸­ä¸œåœ°åŒºæœ€æˆåŠŸåˆåˆ›å…¬å¸çš„æ•°æ®å·¥ç¨‹ä¸»ç®¡çš„å¤šå¹´ç»éªŒï¼Œæˆ‘ä»èŒä¸šç”Ÿæ¶¯ä¸­å­¦åˆ°äº†å¾ˆå¤šï¼Œæˆ‘å¸Œæœ›ä¸æ‚¨åˆ†äº«è¿™äº›çŸ¥è¯†å’Œç»éªŒã€‚è¿™æ˜¯æˆ‘åœ¨æ•°æ®å·¥ç¨‹é¢†åŸŸè·å¾—çš„ä¸ªäººç»éªŒçš„åæ˜ ã€‚æˆ‘å¸Œæœ›è¿™å¯¹æ‚¨æœ‰ç”¨ã€‚
- en: Data engineer â€” the role
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°æ®å·¥ç¨‹å¸ˆâ€”â€”è§’è‰²
- en: First and foremost, why data engineer?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œä¸ºä»€ä¹ˆé€‰æ‹©æ•°æ®å·¥ç¨‹å¸ˆï¼Ÿ
- en: Data engineering is an exciting and very rewarding field. Itâ€™s a fascinating
    job where we have a chance to work with everything that touches data â€” APIs, data
    connectors, data platforms, business intelligence and dozens of data tools available
    in the market. Data engineering is closely connected with Machine learning (ML).
    You will create and deploy all sorts of data and ML pipelines.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®å·¥ç¨‹æ˜¯ä¸€ä¸ªä»¤äººå…´å¥‹ä¸”éå¸¸æœ‰å›æŠ¥çš„é¢†åŸŸã€‚è¿™æ˜¯ä¸€ä»½è¿·äººçš„å·¥ä½œï¼Œæˆ‘ä»¬æœ‰æœºä¼šå¤„ç†æ‰€æœ‰ä¸æ•°æ®ç›¸å…³çš„äº‹ç‰©â€”â€”APIã€æ•°æ®è¿æ¥å™¨ã€æ•°æ®å¹³å°ã€å•†ä¸šæ™ºèƒ½ä»¥åŠå¸‚åœºä¸Šæ•°åç§æ•°æ®å·¥å…·ã€‚æ•°æ®å·¥ç¨‹ä¸æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ç´§å¯†ç›¸å…³ã€‚ä½ å°†åˆ›å»ºå’Œéƒ¨ç½²å„ç§æ•°æ®å’ŒMLç®¡é“ã€‚
- en: It definitely wonâ€™t be boring and it pays well.
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è¿™ä»½å·¥ä½œç»å¯¹ä¸ä¼šæ— èŠï¼Œå¹¶ä¸”è–ªèµ„ä¼˜åšã€‚
- en: It pays well because itâ€™s not easy to build a good data platform. It starts
    with requirements gathering and design and requires considerable experience. Itâ€™s
    not an easy task and requires some really good programming skills as well. The
    job itself is secure because as long businesses generate data this job will be
    in high demand.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä»½å·¥ä½œå›æŠ¥ä¸°åšï¼Œå› ä¸ºå»ºç«‹ä¸€ä¸ªè‰¯å¥½çš„æ•°æ®å¹³å°å¹¶ä¸å®¹æ˜“ã€‚å®ƒä»éœ€æ±‚æ”¶é›†å’Œè®¾è®¡å¼€å§‹ï¼Œéœ€è¦ç›¸å½“çš„ç»éªŒã€‚è¿™ä¸æ˜¯ä¸€é¡¹ç®€å•çš„ä»»åŠ¡ï¼Œä¹Ÿéœ€è¦ä¸€äº›çœŸæ­£å‡ºè‰²çš„ç¼–ç¨‹æŠ€èƒ½ã€‚å·¥ä½œæœ¬èº«æ˜¯å®‰å…¨çš„ï¼Œå› ä¸ºåªè¦ä¼ä¸šäº§ç”Ÿæ•°æ®ï¼Œè¿™ä»½å·¥ä½œå°±ä¼šæœ‰å¾ˆé«˜çš„éœ€æ±‚ã€‚
- en: The companies will always hire someone who knows how to process (ETL) data efficiently.
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å…¬å¸æ€»æ˜¯ä¼šè˜ç”¨é‚£äº›çŸ¥é“å¦‚ä½•é«˜æ•ˆå¤„ç†ï¼ˆETLï¼‰æ•°æ®çš„äººã€‚
- en: Data engineering has been one of the fastest-growing careers in the UK over
    the last five years, ranking 13 on LinkedInâ€™s list of the most in-demand jobs
    in 2023 [1]. The other reason to join is the scarcity. In IT space it is incredibly
    difficult to find a good data engineer these days.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®å·¥ç¨‹åœ¨è¿‡å»äº”å¹´é‡Œæˆä¸ºäº†è‹±å›½å¢é•¿æœ€å¿«çš„èŒä¸šä¹‹ä¸€ï¼Œåœ¨2023å¹´LinkedInçš„æœ€å—æ¬¢è¿èŒä¸šæ¦œå•ä¸­æ’åç¬¬13 [1]ã€‚å¦ä¸€ä¸ªåŠ å…¥çš„ç†ç”±æ˜¯ç¨€ç¼ºæ€§ã€‚åœ¨ITé¢†åŸŸï¼Œå¦‚ä»Šæ‰¾åˆ°ä¸€ä¸ªä¼˜ç§€çš„æ•°æ®å·¥ç¨‹å¸ˆæ˜¯éå¸¸å›°éš¾çš„ã€‚
- en: As a â€œHead of Data Engineeringâ€ I receive 4 job interview invites on LinkedIn
    each week. On average. Entry level data engineering roles are in higher demand.
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä½œä¸ºâ€œæ•°æ®å·¥ç¨‹è´Ÿè´£äººâ€ï¼Œæˆ‘æ¯å‘¨åœ¨LinkedInä¸Šæ”¶åˆ°4ä¸ªèŒä½é¢è¯•é‚€è¯·ã€‚å¹³å‡è€Œè¨€ï¼Œåˆçº§æ•°æ®å·¥ç¨‹è§’è‰²çš„éœ€æ±‚æ›´é«˜ã€‚
- en: 'According to tech job research conducted by DICE Data Engineer is the fastest
    growing tech occupation:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®DICEçš„æŠ€æœ¯èŒä½ç ”ç©¶ï¼Œæ•°æ®å·¥ç¨‹å¸ˆæ˜¯å¢é•¿æœ€å¿«çš„æŠ€æœ¯èŒä¸šï¼š
- en: '![](../Images/ee08b504be177749be9112bfa0f2dd1c.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee08b504be177749be9112bfa0f2dd1c.png)'
- en: 'Source: [DICE](http://marketing.dice.com/pdf/2020/Dice_2020_Tech_Job_Report.pdf)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[DICE](http://marketing.dice.com/pdf/2020/Dice_2020_Tech_Job_Report.pdf)
- en: Modern data stack
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç°ä»£æ•°æ®æ ˆ
- en: Modern data stack refers to a collection of data processing tools and data platform
    types.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç°ä»£æ•°æ®æ ˆæŒ‡çš„æ˜¯ä¸€ç³»åˆ—æ•°æ®å¤„ç†å·¥å…·å’Œæ•°æ®å¹³å°ç±»å‹ã€‚
- en: Are you in the space?
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä½ åœ¨è¿™ä¸ªé¢†åŸŸå—ï¼Ÿ
- en: â€œAre you in the space?â€ â€” This is the question I was asked during one of my
    job interviews. You would want to be able to answer this one, and be aware of
    the news, IPOs, recent developments, breakthroughs, tools and techniques.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: â€œä½ åœ¨è¿™ä¸ªé¢†åŸŸå—ï¼Ÿâ€â€”â€”è¿™æ˜¯æˆ‘åœ¨ä¸€æ¬¡é¢è¯•ä¸­è¢«é—®åˆ°çš„é—®é¢˜ã€‚ä½ ä¼šå¸Œæœ›èƒ½å¤Ÿå›ç­”è¿™ä¸ªé—®é¢˜ï¼Œå¹¶äº†è§£ç›¸å…³æ–°é—»ã€é¦–æ¬¡å…¬å¼€å‹Ÿè‚¡ã€æœ€è¿‘çš„å‘å±•ã€çªç ´ã€å·¥å…·å’ŒæŠ€æœ¯ã€‚
- en: 'Familiarise yourself with common data platform architecture types, i.e. data
    lake, lakehouse, data warehouse and be ready to answer which tools they use. Check
    this article for some examples:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ç†Ÿæ‚‰å¸¸è§çš„æ•°æ®å¹³å°æ¶æ„ç±»å‹ï¼Œå³æ•°æ®æ¹–ã€æ¹–ä»“ã€æ•°æ®ä»“åº“ï¼Œå¹¶å‡†å¤‡å¥½å›ç­”å®ƒä»¬ä½¿ç”¨å“ªäº›å·¥å…·ã€‚æŸ¥çœ‹è¿™ç¯‡æ–‡ç« äº†è§£ä¸€äº›ç¤ºä¾‹ï¼š
- en: '[](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----c0319cb226c2--------------------------------)
    [## Data Platform Architecture Types'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[## æ•°æ®å¹³å°æ¶æ„ç±»å‹](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----c0319cb226c2--------------------------------)'
- en: How well does it answer your business needs? Dilemma of a choice.
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å®ƒåœ¨å¤šå¤§ç¨‹åº¦ä¸Šæ»¡è¶³äº†ä½ çš„ä¸šåŠ¡éœ€æ±‚ï¼Ÿé€‰æ‹©çš„å›°å¢ƒã€‚
- en: towardsdatascience.com](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----c0319cb226c2--------------------------------)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[## æ•°æ®å¹³å°æ¶æ„ç±»å‹](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----c0319cb226c2--------------------------------)'
- en: Data pipelines
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°æ®ç®¡é“
- en: As a data engineer, you will be tasked with data pipeline design almost every
    day. You would want to familiarise yourself with data pipeline design patterns
    and be able to explain when to use them. It is crucial to apply this knowledge
    in practice as it defines which tools to use. The right set of data transformation
    tools can make the data pipeline extremely efficient.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºä¸€åæ•°æ®å·¥ç¨‹å¸ˆï¼Œä½ å‡ ä¹æ¯å¤©éƒ½ä¼šè¢«è¦æ±‚è¿›è¡Œæ•°æ®ç®¡é“è®¾è®¡ã€‚ä½ ä¼šå¸Œæœ›ç†Ÿæ‚‰æ•°æ®ç®¡é“è®¾è®¡æ¨¡å¼ï¼Œå¹¶èƒ½å¤Ÿè§£é‡Šä½•æ—¶ä½¿ç”¨å®ƒä»¬ã€‚å°†è¿™äº›çŸ¥è¯†åº”ç”¨äºå®è·µæ˜¯è‡³å…³é‡è¦çš„ï¼Œå› ä¸ºå®ƒå†³å®šäº†ä½¿ç”¨å“ªäº›å·¥å…·ã€‚æ­£ç¡®çš„æ•°æ®è½¬æ¢å·¥å…·ç»„åˆå¯ä»¥ä½¿æ•°æ®ç®¡é“æå…¶é«˜æ•ˆã€‚
- en: 'So, we need to know exactly when to apply streaming data processing and when
    to apply batch. One can be very expensive and the other one can save thousands.
    However, business requirements might be different in each case. This article has
    a comprehensive list of data pipeline design patterns:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å‡†ç¡®çŸ¥é“ä½•æ—¶åº”ç”¨æµæ•°æ®å¤„ç†ï¼Œä½•æ—¶åº”ç”¨æ‰¹å¤„ç†ã€‚ä¸€ç§å¯èƒ½éå¸¸æ˜‚è´µï¼Œè€Œå¦ä¸€ç§åˆ™å¯èƒ½èŠ‚çœæ•°åƒã€‚ç„¶è€Œï¼Œä¸šåŠ¡éœ€æ±‚å¯èƒ½åœ¨æ¯ç§æƒ…å†µä¸‹éƒ½ä¸åŒã€‚æœ¬æ–‡æä¾›äº†å…¨é¢çš„æ•°æ®ç®¡é“è®¾è®¡æ¨¡å¼åˆ—è¡¨ï¼š
- en: '[](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----c0319cb226c2--------------------------------)
    [## Data pipeline design patterns'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[## æ•°æ®ç®¡é“è®¾è®¡æ¨¡å¼](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----c0319cb226c2--------------------------------)'
- en: Choosing the right architecture with examples
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é€‰æ‹©åˆé€‚çš„æ¶æ„åŠç¤ºä¾‹
- en: towardsdatascience.com](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----c0319cb226c2--------------------------------)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[## æ•°æ®ç®¡é“è®¾è®¡æ¨¡å¼](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----c0319cb226c2--------------------------------)'
- en: Data modelling
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°æ®å»ºæ¨¡
- en: I would say data modelling is an essential part of data engineering. Many data
    platforms are designed in a way that data is being loaded into the data warehouse
    solution â€œas isâ€. This is called the ELT approach. Data engineers are tasked to
    create data transformation pipelines using standard SQL dialect very often. Good
    SQL skills are a must. Indeed, SQL is natural for analytics querying and pretty
    much is a standard this day. It helps to query data efficiently and enables all
    business stakeholders with the power of analytics easily.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è®¤ä¸ºæ•°æ®å»ºæ¨¡æ˜¯æ•°æ®å·¥ç¨‹çš„ä¸€ä¸ªå…³é”®éƒ¨åˆ†ã€‚è®¸å¤šæ•°æ®å¹³å°çš„è®¾è®¡æ–¹å¼æ˜¯å°†æ•°æ®â€œåŸæ ·â€åŠ è½½åˆ°æ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆä¸­ã€‚è¿™è¢«ç§°ä¸º ELT æ–¹æ³•ã€‚æ•°æ®å·¥ç¨‹å¸ˆç»å¸¸éœ€è¦ä½¿ç”¨æ ‡å‡†
    SQL æ–¹è¨€åˆ›å»ºæ•°æ®è½¬æ¢ç®¡é“ã€‚è‰¯å¥½çš„ SQL æŠ€èƒ½æ˜¯å¿…ä¸å¯å°‘çš„ã€‚ç¡®å®ï¼ŒSQL å¯¹äºåˆ†ææŸ¥è¯¢æ˜¯è‡ªç„¶çš„ï¼Œç°å¦‚ä»Šå‡ ä¹å·²ç»æˆä¸ºæ ‡å‡†ã€‚å®ƒæœ‰åŠ©äºé«˜æ•ˆæŸ¥è¯¢æ•°æ®ï¼Œå¹¶è®©æ‰€æœ‰ä¸šåŠ¡åˆ©ç›Šç›¸å…³è€…è½»æ¾ä½¿ç”¨åˆ†æåŠŸèƒ½ã€‚
- en: 'Data engineers must know how to **cleanse, enrich and update datasets.** For
    example, using MERGE to perform incremental updates. Run this SQL in your workbench
    or data warehouse (DWH). It explains how it worls:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®å·¥ç¨‹å¸ˆå¿…é¡»çŸ¥é“å¦‚ä½•**æ¸…æ´—ã€ä¸°å¯Œå’Œæ›´æ–°æ•°æ®é›†**ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨ MERGE æ‰§è¡Œå¢é‡æ›´æ–°ã€‚åœ¨ä½ çš„å·¥ä½œå°æˆ–æ•°æ®ä»“åº“ï¼ˆDWHï¼‰ä¸­è¿è¡Œæ­¤ SQLã€‚å®ƒè§£é‡Šäº†å®ƒçš„å·¥ä½œåŸç†ï¼š
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Some advanced SQL hints and tricks can be found here:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€äº›é«˜çº§ SQL æç¤ºå’ŒæŠ€å·§å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ï¼š
- en: '[](/advanced-sql-techniques-for-beginners-211851a28488?source=post_page-----c0319cb226c2--------------------------------)
    [## Advanced SQL techniques for beginners'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[## é¢å‘åˆå­¦è€…çš„é«˜çº§ SQL æŠ€å·§](https://towardsdatascience.com/advanced-sql-techniques-for-beginners-211851a28488?source=post_page-----c0319cb226c2--------------------------------)'
- en: On a scale from 1 to 10 how good are your data warehousing skills?
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åœ¨ 1 åˆ° 10 çš„å°ºåº¦ä¸Šï¼Œä½ çš„æ•°æ®ä»“åº“æŠ€èƒ½æœ‰å¤šå¥½ï¼Ÿ
- en: towardsdatascience.com](/advanced-sql-techniques-for-beginners-211851a28488?source=post_page-----c0319cb226c2--------------------------------)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/advanced-sql-techniques-for-beginners-211851a28488?source=post_page-----c0319cb226c2--------------------------------)'
- en: Coding
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¼–ç 
- en: This is really important as data engineering is not only about data modelling
    and SQL. Consider data engineers as software engineers instead. They must have
    good knowledge of ETL/ELT techniques and also must be able to code at least in
    Python. Yes, it is obvious that Python is no doubt, the most convenient programming
    language for data engineering but everything you can do with Python can be easily
    done with any other language, i.e. Java Script or Java. Donâ€™t limit yourself,
    youâ€™ll have time to learn any language your company chose as the main one for
    their stack.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™éå¸¸é‡è¦ï¼Œå› ä¸ºæ•°æ®å·¥ç¨‹ä¸ä»…ä»…æ¶‰åŠæ•°æ®å»ºæ¨¡å’Œ SQLã€‚å¯ä»¥å°†æ•°æ®å·¥ç¨‹å¸ˆè§†ä¸ºè½¯ä»¶å·¥ç¨‹å¸ˆã€‚ä»–ä»¬å¿…é¡»å…·å¤‡ ETL/ELT æŠ€æœ¯çš„è‰¯å¥½çŸ¥è¯†ï¼Œå¹¶ä¸”è‡³å°‘èƒ½å¤Ÿä½¿ç”¨ Python
    ç¼–ç ã€‚æ˜¯çš„ï¼Œæ˜¾ç„¶ Python æ— ç–‘æ˜¯æ•°æ®å·¥ç¨‹æœ€æ–¹ä¾¿çš„ç¼–ç¨‹è¯­è¨€ï¼Œä½†ä½ ç”¨ Python å¯ä»¥åšçš„ä»»ä½•äº‹æƒ…ï¼Œéƒ½å¯ä»¥ç”¨å…¶ä»–è¯­è¨€è½»æ¾å®Œæˆï¼Œæ¯”å¦‚ JavaScript
    æˆ– Javaã€‚ä¸è¦é™åˆ¶è‡ªå·±ï¼Œä½ ä¼šæœ‰æ—¶é—´å­¦ä¹ å…¬å¸é€‰æ‹©ä½œä¸ºå…¶æŠ€æœ¯æ ˆä¸»è¦è¯­è¨€çš„ä»»ä½•è¯­è¨€ã€‚
- en: I would recommend to start with **data APIs and requests**. Combining this knowledge
    with Cloud services gives us a very good foundation for any ETL processes we might
    need in the future.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å»ºè®®ä»**æ•°æ® API å’Œè¯·æ±‚**å¼€å§‹ã€‚å°†è¿™äº›çŸ¥è¯†ä¸äº‘æœåŠ¡ç»“åˆï¼Œä¸ºæœªæ¥å¯èƒ½éœ€è¦çš„ä»»ä½• ETL è¿‡ç¨‹æä¾›äº†ä¸€ä¸ªå¾ˆå¥½çš„åŸºç¡€ã€‚
- en: We canâ€™t know everything and itâ€™s not required to be a coding guru but we must
    know how to process data.
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸èƒ½çŸ¥é“ä¸€åˆ‡ï¼Œä¹Ÿä¸è¦æ±‚æˆä¸ºç¼–ç ä¸“å®¶ï¼Œä½†æˆ‘ä»¬å¿…é¡»çŸ¥é“å¦‚ä½•å¤„ç†æ•°æ®ã€‚
- en: 'Consider this example of loading data into BigQuery data warehouse. It will
    use BigQuery client libraries [5] to insert rows into a table:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥å°†æ•°æ®åŠ è½½åˆ° BigQuery æ•°æ®ä»“åº“ä¸ºä¾‹ã€‚å®ƒå°†ä½¿ç”¨ BigQuery å®¢æˆ·ç«¯åº“ [5] å°†è¡Œæ’å…¥åˆ°è¡¨ä¸­ï¼š
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We canâ€™t know everything and itâ€™s not required to be a coding guru but we must
    know how to process data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸èƒ½çŸ¥é“ä¸€åˆ‡ï¼Œä¹Ÿä¸è¦æ±‚æˆä¸ºç¼–ç ä¸“å®¶ï¼Œä½†æˆ‘ä»¬å¿…é¡»çŸ¥é“å¦‚ä½•å¤„ç†æ•°æ®ã€‚
- en: We can run it locally or deploy it in the cloud as a serverless application.
    It can be triggered by any other service we choose. For example, deploying an
    AWS Lambda or GCP Cloud Function can be very efficient. It will process our data
    pipeline events with ease and almost at zero cost. There is plenty of articles
    in my blog explaining how easy and flexible it can be.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥åœ¨æœ¬åœ°è¿è¡Œå®ƒï¼Œä¹Ÿå¯ä»¥å°†å…¶éƒ¨ç½²åˆ°äº‘ç«¯ä½œä¸ºæ— æœåŠ¡å™¨åº”ç”¨ç¨‹åºã€‚å®ƒå¯ä»¥ç”±æˆ‘ä»¬é€‰æ‹©çš„ä»»ä½•å…¶ä»–æœåŠ¡è§¦å‘ã€‚ä¾‹å¦‚ï¼Œéƒ¨ç½² AWS Lambda æˆ– GCP Cloud
    Function ä¼šéå¸¸é«˜æ•ˆã€‚å®ƒå°†è½»æ¾å¤„ç†æˆ‘ä»¬çš„æ•°æ®ç®¡é“äº‹ä»¶ï¼Œå‡ ä¹ä¸äº§ç”Ÿæˆæœ¬ã€‚æˆ‘çš„åšå®¢ä¸­æœ‰å¾ˆå¤šæ–‡ç« è§£é‡Šäº†å®ƒçš„ç®€ä¾¿æ€§å’Œçµæ´»æ€§ã€‚
- en: Airflow, Airbyte, Luidgi, Hudiâ€¦
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Airflowã€Airbyteã€Luigiã€Hudiâ€¦â€¦
- en: Play with 3rd party frameworks and libraries that help to manage data platforms
    and orchestrate data pipelines. Many of them are open-source such as Apache Hudi
    [6] and help to understand what is data platform management from different angles.
    Many of them are really good at managing batch and streaming workloads. I learned
    a lot simply by using them. Apache Airflow, for example, offers a lot of ready
    data connectors. We can use them to run our ETL tasks with ease with any cloud
    vendor (AWS, GCP, Azure).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å¸®åŠ©ç®¡ç†æ•°æ®å¹³å°å’Œç¼–æ’æ•°æ®ç®¡é“çš„ç¬¬ä¸‰æ–¹æ¡†æ¶å’Œåº“è¿›è¡Œå®éªŒã€‚è®¸å¤šæ¡†æ¶æ˜¯å¼€æºçš„ï¼Œå¦‚Apache Hudi [6]ï¼Œå¹¶ä¸”ä»ä¸åŒè§’åº¦å¸®åŠ©ç†è§£æ•°æ®å¹³å°ç®¡ç†ã€‚å®ƒä»¬ä¸­çš„è®¸å¤šåœ¨ç®¡ç†æ‰¹å¤„ç†å’Œæµå¤„ç†å·¥ä½œè´Ÿè½½æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æˆ‘é€šè¿‡ä½¿ç”¨å®ƒä»¬å­¦åˆ°äº†å¾ˆå¤šä¸œè¥¿ã€‚ä¾‹å¦‚ï¼ŒApache
    Airflow æä¾›äº†å¾ˆå¤šç°æˆçš„æ•°æ®è¿æ¥å™¨ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒä»¬è½»æ¾åœ°è¿è¡Œä»»ä½•äº‘ä¾›åº”å•†ï¼ˆAWSã€GCPã€Azureï¼‰çš„ETLä»»åŠ¡ã€‚
- en: Itâ€™s very easy to create batch data processing jobs using these frameworks.
    If we take a look under the hood it definitely makes things much clearer in terms
    of what the actual ETL is.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¿™äº›æ¡†æ¶åˆ›å»ºæ‰¹å¤„ç†æ•°æ®å¤„ç†ä½œä¸šéå¸¸å®¹æ˜“ã€‚å¦‚æœæˆ‘ä»¬æ·±å…¥äº†è§£ï¼Œå®ƒç¡®å®å¯ä»¥ä½¿ETLçš„å®é™…æ“ä½œæ›´åŠ æ¸…æ™°ã€‚
- en: 'Consider this example of ML pipeline I built using airflow connectors to train
    the recommendation engine:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œæˆ‘ä½¿ç”¨ airflow è¿æ¥å™¨æ„å»ºäº†ä¸€ä¸ªæœºå™¨å­¦ä¹ ç®¡é“æ¥è®­ç»ƒæ¨èå¼•æ“ï¼š
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: It will create a simple data pipeline graph to export data into cloud storage
    bucket and then train the ML model using MLEngineTrainingOperator.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒå°†åˆ›å»ºä¸€ä¸ªç®€å•çš„æ•°æ®ç®¡é“å›¾ï¼Œå°†æ•°æ®å¯¼å‡ºåˆ°äº‘å­˜å‚¨æ¡¶ä¸­ï¼Œç„¶åä½¿ç”¨MLEngineTrainingOperatorè®­ç»ƒMLæ¨¡å‹ã€‚
- en: '![](../Images/e316a43ba542db4191a0a78ccfa00721.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e316a43ba542db4191a0a78ccfa00721.png)'
- en: ML model training using Airflow. Image by author.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Airflowè¿›è¡ŒMLæ¨¡å‹è®­ç»ƒã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: Orchestrate data pipelines
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¼–æ’æ•°æ®ç®¡é“
- en: Frameworks are great but data engineers must know how to create their own frameworks
    to orchestrate data pipelines. This brings us back to raw vanilla coding and working
    with client libraries and APIs. A good advice here will be to familiarise yourself
    with data tools and their API endpoints. Very often it is much more intuitive
    and easier to create and deploy our own microservice to perform ETL/ELT jobs.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æ¡†æ¶éå¸¸é‡è¦ï¼Œä½†æ•°æ®å·¥ç¨‹å¸ˆå¿…é¡»çŸ¥é“å¦‚ä½•åˆ›å»ºè‡ªå·±çš„æ¡†æ¶æ¥ç¼–æ’æ•°æ®ç®¡é“ã€‚è¿™å°†æˆ‘ä»¬å¸¦å›åˆ°åŸå§‹çš„åŸºç¡€ç¼–ç ä»¥åŠä½¿ç”¨å®¢æˆ·ç«¯åº“å’ŒAPIã€‚åœ¨è¿™é‡Œï¼Œä¸€ä¸ªå¥½çš„å»ºè®®æ˜¯ç†Ÿæ‚‰æ•°æ®å·¥å…·åŠå…¶APIç«¯ç‚¹ã€‚é€šå¸¸ï¼Œåˆ›å»ºå¹¶éƒ¨ç½²æˆ‘ä»¬è‡ªå·±çš„å¾®æœåŠ¡æ¥æ‰§è¡ŒETL/ELTä»»åŠ¡æ›´åŠ ç›´è§‚å’Œå®¹æ˜“ã€‚
- en: Orchestrate data pipelines with your own tools
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä½ è‡ªå·±çš„å·¥å…·ç¼–æ’æ•°æ®ç®¡é“
- en: For example, we can create a simple serverless application that will consume
    data from the message broker, such as SNS. Then it can process these events and
    orchestrate other microservices we create to perform ETL tasks. Another example
    is a simple AWS Lambda that is being triggered by new files created in the data
    lake, then based on the information it reads from the pipeline configuration file
    it can decide which service to invoke or which table to load data into.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªç®€å•çš„æ— æœåŠ¡å™¨åº”ç”¨ç¨‹åºï¼Œå®ƒå°†ä»æ¶ˆæ¯ä»£ç†ï¼ˆå¦‚SNSï¼‰ä¸­è·å–æ•°æ®ã€‚ç„¶åï¼Œå®ƒå¯ä»¥å¤„ç†è¿™äº›äº‹ä»¶å¹¶ç¼–æ’æˆ‘ä»¬åˆ›å»ºçš„å…¶ä»–å¾®æœåŠ¡æ¥æ‰§è¡ŒETLä»»åŠ¡ã€‚å¦ä¸€ä¸ªä¾‹å­æ˜¯ä¸€ä¸ªç®€å•çš„AWS
    Lambdaï¼Œå®ƒç”±æ•°æ®æ¹–ä¸­åˆ›å»ºçš„æ–°æ–‡ä»¶è§¦å‘ï¼Œç„¶åæ ¹æ®å®ƒä»ç®¡é“é…ç½®æ–‡ä»¶ä¸­è¯»å–çš„ä¿¡æ¯ï¼Œå®ƒå¯ä»¥å†³å®šè°ƒç”¨å“ªä¸ªæœåŠ¡æˆ–å°†æ•°æ®åŠ è½½åˆ°å“ªä¸ªè¡¨ä¸­ã€‚
- en: Consider this application below. Itâ€™s a very simple AWS Lambda that can be run
    locally or when itâ€™s deployed in the cloud.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·è€ƒè™‘ä¸‹é¢çš„è¿™ä¸ªåº”ç”¨ç¨‹åºã€‚å®ƒæ˜¯ä¸€ä¸ªéå¸¸ç®€å•çš„AWS Lambdaï¼Œå¯ä»¥åœ¨æœ¬åœ°è¿è¡Œæˆ–åœ¨äº‘ä¸­éƒ¨ç½²æ—¶è¿è¡Œã€‚
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`app.py` can be anything with ETL, we just need to add some logic like we did
    with data loading into BigQuery in the previous example using a couple of client
    libraries:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`app.py` å¯ä»¥æ˜¯ä»»ä½•ETLä»»åŠ¡ï¼Œæˆ‘ä»¬åªéœ€æ·»åŠ ä¸€äº›é€»è¾‘ï¼Œå°±åƒåœ¨å‰é¢çš„ç¤ºä¾‹ä¸­ä½¿ç”¨å‡ ä¸ªå®¢æˆ·ç«¯åº“å°†æ•°æ®åŠ è½½åˆ°BigQueryä¸­ä¸€æ ·ï¼š'
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'No we can run it locally or deploy using infrastructure as code. This command
    line script will run this service locally:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥åœ¨æœ¬åœ°è¿è¡Œå®ƒæˆ–ä½¿ç”¨åŸºç¡€è®¾æ–½å³ä»£ç è¿›è¡Œéƒ¨ç½²ã€‚è¿™ä¸ªå‘½ä»¤è¡Œè„šæœ¬å°†åœ¨æœ¬åœ°è¿è¡Œæ­¤æœåŠ¡ï¼š
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Alternatively it can be deployed in the cloud and we can invoke it from there.
    This brings us into the Cloud.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å¦å¤–ï¼Œå®ƒä¹Ÿå¯ä»¥éƒ¨ç½²åˆ°äº‘ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä»é‚£é‡Œè°ƒç”¨å®ƒã€‚è¿™å°†æˆ‘ä»¬å¸¦å…¥äº†äº‘ç¯å¢ƒã€‚
- en: Cloud services providers
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: äº‘æœåŠ¡æä¾›å•†
- en: Everything is managed in the cloud these days. Thatâ€™s why learning at least
    one vendor is crucial. It can be AWS, GCP or Azure. They are the leaders and we
    would want to focus on one of them. It will be ideal to get a Cloud certification,
    such as â€œ[Google Cloud Professional Data Engineer](https://cloud.google.com/learn/certification/data-engineer)â€
    [7] or similar. These exams are difficult but it is worth getting one as it gives
    a good overview of data processing tools and makes us look very credible. I did
    one, have written down my experience in an article and you can find it in my stories.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä¸€åˆ‡éƒ½åœ¨äº‘ç«¯ç®¡ç†ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆè‡³å°‘å­¦ä¹ ä¸€ç§ä¾›åº”å•†çš„æœåŠ¡è‡³å…³é‡è¦ã€‚å¯ä»¥æ˜¯AWSã€GCPæˆ–Azureã€‚å®ƒä»¬æ˜¯é¢†å¯¼è€…ï¼Œæˆ‘ä»¬å¸Œæœ›ä¸“æ³¨äºå…¶ä¸­ä¹‹ä¸€ã€‚è·å¾—äº‘è®¤è¯æ˜¯ç†æƒ³çš„ï¼Œæ¯”å¦‚â€œ[Google
    Cloud Professional Data Engineer](https://cloud.google.com/learn/certification/data-engineer)â€
    [7] æˆ–ç±»ä¼¼çš„è®¤è¯ã€‚è¿™äº›è€ƒè¯•å¾ˆéš¾ï¼Œä½†å€¼å¾—è·å¾—ï¼Œå› ä¸ºå®ƒæä¾›äº†æ•°æ®å¤„ç†å·¥å…·çš„è‰¯å¥½æ¦‚è¿°ï¼Œå¹¶ä½¿æˆ‘ä»¬çœ‹èµ·æ¥éå¸¸å¯ä¿¡ã€‚æˆ‘è€ƒè¿‡ä¸€æ¬¡ï¼Œå·²åœ¨æ–‡ç« ä¸­è®°å½•äº†æˆ‘çš„ç»éªŒï¼Œä½ å¯ä»¥åœ¨æˆ‘çš„æ•…äº‹ä¸­æ‰¾åˆ°ã€‚
- en: 'Everything data engineers create with cloud functions and/or docker can be
    deployed in the cloud. Consider this AWS CloudFormation stack template below.
    We can use it to deploy our simple ETL microservice:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®å·¥ç¨‹å¸ˆä½¿ç”¨äº‘åŠŸèƒ½å’Œ/æˆ–dockeråˆ›å»ºçš„æ‰€æœ‰å†…å®¹éƒ½å¯ä»¥éƒ¨ç½²åœ¨äº‘ä¸­ã€‚è€ƒè™‘ä»¥ä¸‹AWS CloudFormationå †æ ˆæ¨¡æ¿ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒæ¥éƒ¨ç½²æˆ‘ä»¬çš„ç®€å•ETLå¾®æœåŠ¡ï¼š
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If we run this shell script in our command line it will deploy our service
    and all required resources such as IAM policies, in the cloud:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬åœ¨å‘½ä»¤è¡Œä¸­è¿è¡Œè¿™ä¸ªshellè„šæœ¬ï¼Œå®ƒå°†éƒ¨ç½²æˆ‘ä»¬çš„æœåŠ¡å’Œæ‰€æœ‰æ‰€éœ€çš„èµ„æºï¼Œå¦‚IAMç­–ç•¥åˆ°äº‘ä¸­ï¼š
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'After that we can invoke our service using SDK by running this CLI command
    from our command line tool:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨SDKé€šè¿‡åœ¨å‘½ä»¤è¡Œå·¥å…·ä¸­è¿è¡Œæ­¤CLIå‘½ä»¤æ¥è°ƒç”¨æˆ‘ä»¬çš„æœåŠ¡ï¼š
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Master command line tools
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç²¾é€šå‘½ä»¤è¡Œå·¥å…·
- en: Command line tools from Cloud vendors are very useful and help to create scripts
    to test our ETL services when they are deployed in the cloud. Data engineers use
    it a lot. Working with data lakes we would want to master CLI commands that help
    us manage Cloud storage, i.e. upload, download and copy files and objects. Why
    do we do this? Very often files in cloud storage trigger various ETL services.
    Batch processing is a very common data transformation pattern and in order to
    investigate bugs and errors we might need to download or copy files between buckets.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: äº‘ä¾›åº”å•†çš„å‘½ä»¤è¡Œå·¥å…·éå¸¸æœ‰ç”¨ï¼Œæœ‰åŠ©äºåˆ›å»ºæµ‹è¯•æˆ‘ä»¬åœ¨äº‘ä¸­éƒ¨ç½²çš„ETLæœåŠ¡çš„è„šæœ¬ã€‚æ•°æ®å·¥ç¨‹å¸ˆç»å¸¸ä½¿ç”¨å®ƒã€‚ä¸æ•°æ®æ¹–ä¸€èµ·å·¥ä½œæ—¶ï¼Œæˆ‘ä»¬å¸Œæœ›æŒæ¡å¸®åŠ©æˆ‘ä»¬ç®¡ç†äº‘å­˜å‚¨çš„CLIå‘½ä»¤ï¼Œå³ä¸Šä¼ ã€ä¸‹è½½å’Œå¤åˆ¶æ–‡ä»¶å’Œå¯¹è±¡ã€‚ä¸ºä»€ä¹ˆæˆ‘ä»¬è¦è¿™æ ·åšï¼Ÿå› ä¸ºäº‘å­˜å‚¨ä¸­çš„æ–‡ä»¶ç»å¸¸è§¦å‘å„ç§ETLæœåŠ¡ã€‚æ‰¹å¤„ç†æ˜¯ä¸€ä¸ªéå¸¸å¸¸è§çš„æ•°æ®è½¬æ¢æ¨¡å¼ï¼Œä¸ºäº†è°ƒæŸ¥é”™è¯¯å’Œé—®é¢˜ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦åœ¨æ¡¶ä¹‹é—´ä¸‹è½½æˆ–å¤åˆ¶æ–‡ä»¶ã€‚
- en: '![](../Images/c59684bded8474031b90c752aed36ea9.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c59684bded8474031b90c752aed36ea9.png)'
- en: ETL on Data lake objects with AWS Lambda. Image by author.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ•°æ®æ¹–å¯¹è±¡ä¸Šä½¿ç”¨AWS Lambdaè¿›è¡ŒETLã€‚ä½œè€…æä¾›çš„å›¾åƒã€‚
- en: In this example, we can see that the service outputs data into Kinesis and then
    it is stored in the data lake. When file objects are being created in S3 they
    trigger the ETL process handled by AWS Lambda. The result is being saved in the
    S3 bucket to consume by AWS Athena in order to generate a BI report with AWS Quicksight.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æœåŠ¡å°†æ•°æ®è¾“å‡ºåˆ°Kinesisï¼Œç„¶åå­˜å‚¨åœ¨æ•°æ®æ¹–ä¸­ã€‚å½“S3ä¸­åˆ›å»ºæ–‡ä»¶å¯¹è±¡æ—¶ï¼Œå®ƒä»¬ä¼šè§¦å‘ç”±AWS Lambdaå¤„ç†çš„ETLè¿‡ç¨‹ã€‚ç»“æœè¢«ä¿å­˜åˆ°S3æ¡¶ä¸­ï¼Œä»¥ä¾›AWS
    Athenaä½¿ç”¨ï¼Œä»è€Œç”Ÿæˆä¸€ä¸ªä½¿ç”¨AWS Quicksightçš„BIæŠ¥å‘Šã€‚
- en: 'Here is the set of AWS CLI commands we might want to use at some point:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘ä»¬å¯èƒ½åœ¨æŸä¸ªæ—¶åˆ»æƒ³è¦ä½¿ç”¨çš„ä¸€ç»„AWS CLIå‘½ä»¤ï¼š
- en: Copy and upload file
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¤åˆ¶å’Œä¸Šä¼ æ–‡ä»¶
- en: '[PRE9]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Recursively copy/upload/download all files in the folder
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é€’å½’å¤åˆ¶/ä¸Šä¼ /ä¸‹è½½æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶
- en: '[PRE10]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Recursively delete all contents
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é€’å½’åˆ é™¤æ‰€æœ‰å†…å®¹
- en: '[PRE11]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Delete a bucket
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ é™¤ä¸€ä¸ªæ¡¶
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: There are more advanced examples but I think the idea is clear.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰æ›´å¤šé«˜çº§ç¤ºä¾‹ï¼Œä½†æˆ‘è®¤ä¸ºè¿™ä¸ªæ¦‚å¿µå·²ç»å¾ˆæ¸…æ¥šäº†ã€‚
- en: We would want to manage cloud storage efficiently with scripts.
  id: totrans-92
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¸Œæœ›ä½¿ç”¨è„šæœ¬æœ‰æ•ˆåœ°ç®¡ç†äº‘å­˜å‚¨ã€‚
- en: We can chain these commands into shell scripts which makes CLI a very powerful
    tool.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å°†è¿™äº›å‘½ä»¤é“¾æ¥æˆshellè„šæœ¬ï¼Œè¿™ä½¿å¾—CLIæˆä¸ºä¸€ä¸ªéå¸¸å¼ºå¤§çš„å·¥å…·ã€‚
- en: 'Consider this shell script for example. It will check if storage bucket for
    lambda package exists, upload and deploy our ETL service as a Lambda Function:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œè€ƒè™‘è¿™ä¸ªshellè„šæœ¬ã€‚å®ƒå°†æ£€æŸ¥æ˜¯å¦å­˜åœ¨lambdaåŒ…çš„å­˜å‚¨æ¡¶ï¼Œä¸Šä¼ å¹¶éƒ¨ç½²æˆ‘ä»¬çš„ETLæœåŠ¡ä½œä¸ºLambdaå‡½æ•°ï¼š
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: More advanced examples can be found in my previous stories.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´é«˜çº§çš„ç¤ºä¾‹å¯ä»¥åœ¨æˆ‘ä¹‹å‰çš„æ•…äº‹ä¸­æ‰¾åˆ°ã€‚
- en: Data quality
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°æ®è´¨é‡
- en: 'Now when we know how to deploy ETL services, perform requests and pull data
    from external APIs we need to learn how to observe the data we have in our data
    platform. Ideally, we would want to check data quality in real-time as data flows
    into our data platform. It can be done both ways using the ETL or ELT approach.
    Streaming applications built with Kafka or Kinesis have libraries to analyze data
    quality as data flows in the data pipeline. ELT approach is preferable when data
    engineers delegate data observability and data quality management to other stakeholders
    working with the data warehouse. Personally, I like the latter approach as it
    saves time. Consider data warehouse solutions as a single source of truth for
    everyone in the company. Finance, marketing and customer services teams can access
    data and check for **any potential issues.** Among these we typically see the
    following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œå½“æˆ‘ä»¬çŸ¥é“å¦‚ä½•éƒ¨ç½²ETLæœåŠ¡ã€æ‰§è¡Œè¯·æ±‚å¹¶ä»å¤–éƒ¨APIæ‹‰å–æ•°æ®æ—¶ï¼Œæˆ‘ä»¬éœ€è¦å­¦ä¹ å¦‚ä½•è§‚å¯Ÿæˆ‘ä»¬åœ¨æ•°æ®å¹³å°ä¸Šæ‹¥æœ‰çš„æ•°æ®ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨æ•°æ®æµå…¥æ•°æ®å¹³å°æ—¶å®æ—¶æ£€æŸ¥æ•°æ®è´¨é‡ã€‚å¯ä»¥é€šè¿‡ETLæˆ–ELTæ–¹æ³•æ¥å®ç°ã€‚ä½¿ç”¨Kafkaæˆ–Kinesisæ„å»ºçš„æµåº”ç”¨ç¨‹åºå…·æœ‰åˆ†ææ•°æ®è´¨é‡çš„åº“ï¼Œæ•°æ®åœ¨æ•°æ®ç®¡é“ä¸­æµåŠ¨æ—¶å¯ä»¥è¿›è¡Œåˆ†æã€‚å½“æ•°æ®å·¥ç¨‹å¸ˆå°†æ•°æ®å¯è§‚å¯Ÿæ€§å’Œæ•°æ®è´¨é‡ç®¡ç†å§”æ´¾ç»™å…¶ä»–åœ¨æ•°æ®ä»“åº“ä¸­å·¥ä½œçš„åˆ©ç›Šç›¸å…³è€…æ—¶ï¼ŒELTæ–¹æ³•æ›´ä¸ºå¯å–ã€‚å°±ä¸ªäººè€Œè¨€ï¼Œæˆ‘å–œæ¬¢åè€…ï¼Œå› ä¸ºå®ƒèŠ‚çœäº†æ—¶é—´ã€‚å°†æ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆè§†ä¸ºå…¬å¸ä¸­æ¯ä¸ªäººçš„å•ä¸€çœŸå®æ¥æºã€‚è´¢åŠ¡ã€å¸‚åœºè¥é”€å’Œå®¢æˆ·æœåŠ¡å›¢é˜Ÿå¯ä»¥è®¿é—®æ•°æ®å¹¶æ£€æŸ¥**ä»»ä½•æ½œåœ¨é—®é¢˜**ã€‚è¿™äº›ä¸­æˆ‘ä»¬é€šå¸¸ä¼šçœ‹åˆ°ä»¥ä¸‹æƒ…å†µï¼š
- en: missing data
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¼ºå¤±æ•°æ®
- en: data source outages
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ•°æ®æºä¸­æ–­
- en: data source changes when schema fields are updated
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ•°æ®æºåœ¨æ¨¡å¼å­—æ®µæ›´æ–°æ—¶å‘ç”Ÿå˜åŒ–
- en: various data anomalies such as outliers or unusual application/user behaviour.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å„ç§æ•°æ®å¼‚å¸¸ï¼Œå¦‚å¼‚å¸¸å€¼æˆ–ä¸å¯»å¸¸çš„åº”ç”¨ç¨‹åº/ç”¨æˆ·è¡Œä¸ºã€‚
- en: Data engineers create alarms and schedule notifications for any potential data
    issues so data users stay always informed.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®å·¥ç¨‹å¸ˆåˆ›å»ºè­¦æŠ¥å¹¶å®‰æ’é€šçŸ¥ï¼Œä»¥ä¾¿å¯¹æ½œåœ¨çš„æ•°æ®é—®é¢˜ä¿æŒå…³æ³¨ã€‚
- en: 'Consider this example when a daily email is sent to stakeholders informing
    them about data outage:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘è¿™ä¸ªä¾‹å­ï¼Œå½“æ¯å¤©å‘é€é‚®ä»¶é€šçŸ¥åˆ©ç›Šç›¸å…³è€…æœ‰å…³æ•°æ®ä¸­æ–­çš„æƒ…å†µï¼š
- en: '![](../Images/c9b2a060d7071e8e6c95c403b0901690.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c9b2a060d7071e8e6c95c403b0901690.png)'
- en: Email alarm. Image by author.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: é‚®ä»¶è­¦æŠ¥ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: In my stories you can find an article explaining how to schedule such data monitoring
    workflow using SQL.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘çš„æ•…äº‹ä¸­ï¼Œä½ å¯ä»¥æ‰¾åˆ°ä¸€ç¯‡æ–‡ç« ï¼Œè§£é‡Šå¦‚ä½•ä½¿ç”¨SQLå®‰æ’è¿™æ ·çš„æ•°æ®ç›‘æ§å·¥ä½œæµã€‚
- en: '[](/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0?source=post_page-----c0319cb226c2--------------------------------)
    [## Automated emails and data quality checks for your data'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0?source=post_page-----c0319cb226c2--------------------------------)
    [## è‡ªåŠ¨åŒ–é‚®ä»¶å’Œæ•°æ®è´¨é‡æ£€æŸ¥'
- en: Data warehouse guide for better and cleaner data with scheduled emails
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ•°æ®ä»“åº“æŒ‡å—ä»¥ä¾¿æ›´å¥½ã€æ›´å¹²å‡€çš„æ•°æ®å’Œå®šæœŸé‚®ä»¶
- en: towardsdatascience.com](/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0?source=post_page-----c0319cb226c2--------------------------------)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0?source=post_page-----c0319cb226c2--------------------------------)
- en: 'Consider this snippet below. It ewill check yeterday data ffor any missing
    field using **row conditions** and send a notification alarm if any were found:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘ä»¥ä¸‹ä»£ç ç‰‡æ®µã€‚å®ƒå°†æ£€æŸ¥æ˜¨å¤©çš„æ•°æ®æ˜¯å¦æœ‰ä»»ä½•ç¼ºå¤±å­—æ®µï¼Œå¹¶åœ¨å‘ç°æ—¶å‘é€é€šçŸ¥è­¦æŠ¥ï¼š
- en: '[PRE14]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Data environments
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°æ®ç¯å¢ƒ
- en: Data engineers test data pipelines. There are various ways of doing this. In
    general, it requires a data environment split between production and staging pipelines.
    Often we might need an extra sandbox for testing purposes or to run data transformation
    unit tests when our ETL services trigger CI/CD workflows.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®å·¥ç¨‹å¸ˆæµ‹è¯•æ•°æ®ç®¡é“ã€‚æœ‰å¤šç§æ–¹æ³•æ¥å®ç°è¿™ä¸€ç‚¹ã€‚é€šå¸¸ï¼Œå®ƒéœ€è¦å°†æ•°æ®ç¯å¢ƒåˆ†ä¸ºç”Ÿäº§å’Œé¢„å‘å¸ƒç®¡é“ã€‚æˆ‘ä»¬é€šå¸¸å¯èƒ½éœ€è¦ä¸€ä¸ªé¢å¤–çš„æ²™ç›’ç”¨äºæµ‹è¯•ç›®çš„ï¼Œæˆ–è€…åœ¨æˆ‘ä»¬çš„ETLæœåŠ¡è§¦å‘CI/CDå·¥ä½œæµæ—¶è¿è¡Œæ•°æ®è½¬æ¢å•å…ƒæµ‹è¯•ã€‚
- en: This is a common practice and job interviewers might ask a couple of questions
    regarding this. It might seem a little tricky in the beginning but this diagram
    below exlpains how it works.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ç§å¸¸è§åšæ³•ï¼Œé¢è¯•å®˜å¯èƒ½ä¼šé—®ä¸€äº›ç›¸å…³é—®é¢˜ã€‚åˆšå¼€å§‹å¯èƒ½æœ‰ç‚¹æ£˜æ‰‹ï¼Œä½†ä¸‹é¢çš„å›¾è§£è§£é‡Šäº†å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚
- en: '![](../Images/081619c52103b0f55e7e9c6dca8bd0f8.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/081619c52103b0f55e7e9c6dca8bd0f8.png)'
- en: CI/CD workflow example for ETL services. Image by author.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: CI/CDå·¥ä½œæµç¤ºä¾‹ï¼Œç”¨äºETLæœåŠ¡ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: For example, we can use infrastructure as code and GitHub Actions to deploy
    and test staging resources on any pull request from the development branch. When
    all tests are passed and we are happy with our ETL service we can promote it to
    production by merging into the master branch.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨åŸºç¡€è®¾æ–½å³ä»£ç å’ŒGitHub Actionsæ¥éƒ¨ç½²å’Œæµ‹è¯•ä»»ä½•æ¥è‡ªå¼€å‘åˆ†æ”¯çš„æ‹‰å–è¯·æ±‚ä¸­çš„é¢„å‘å¸ƒèµ„æºã€‚å½“æ‰€æœ‰æµ‹è¯•é€šè¿‡ä¸”æˆ‘ä»¬å¯¹ETLæœåŠ¡æ»¡æ„æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡åˆå¹¶åˆ°ä¸»åˆ†æ”¯å°†å…¶æå‡åˆ°ç”Ÿäº§ç¯å¢ƒã€‚
- en: Consider this GitHub action workflow below. It will deploy our ETL service on
    staging and do the testing. Suc approach helps reduce errors and deliver data
    piplelines faster.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·çœ‹ä¸‹é¢è¿™ä¸ª GitHub Action å·¥ä½œæµã€‚å®ƒå°†ä¼šåœ¨é¢„ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²æˆ‘ä»¬çš„ ETL æœåŠ¡å¹¶è¿›è¡Œæµ‹è¯•ã€‚è¿™æ ·çš„åšæ³•æœ‰åŠ©äºå‡å°‘é”™è¯¯å¹¶æ›´å¿«åœ°äº¤ä»˜æ•°æ®ç®¡é“ã€‚
- en: '[PRE15]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: There is a full solution example in one of my stories.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘çš„ä¸€ä¸ªæ•…äº‹ä¸­æœ‰ä¸€ä¸ªå®Œæ•´çš„è§£å†³æ–¹æ¡ˆç¤ºä¾‹ã€‚
- en: Machine learning
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æœºå™¨å­¦ä¹ 
- en: Adding a machine learning component will make us a machine learning engineer.
    Data engineering and ML are very close as data engineers create data pipelines
    that are consumed by ML services often.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: æ·»åŠ ä¸€ä¸ªæœºå™¨å­¦ä¹ ç»„ä»¶å°†ä½¿æˆ‘ä»¬æˆä¸ºæœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆã€‚æ•°æ®å·¥ç¨‹ä¸æœºå™¨å­¦ä¹ éå¸¸æ¥è¿‘ï¼Œå› ä¸ºæ•°æ®å·¥ç¨‹å¸ˆåˆ›å»ºçš„æ•°æ®ç®¡é“é€šå¸¸è¢«æœºå™¨å­¦ä¹ æœåŠ¡ä½¿ç”¨ã€‚
- en: We donâ€™t need to know every machine-learning model
  id: totrans-124
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸éœ€è¦äº†è§£æ¯ä¸€ç§æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚
- en: We canâ€™t compete with cloud service providers such as Amazon ang Google in machine
    learning and data science but we need to know how to use it.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ— æ³•ä¸åƒäºšé©¬é€Šå’Œè°·æ­Œè¿™æ ·çš„äº‘æœåŠ¡æä¾›å•†åœ¨æœºå™¨å­¦ä¹ å’Œæ•°æ®ç§‘å­¦é¢†åŸŸç«äº‰ï¼Œä½†æˆ‘ä»¬éœ€è¦çŸ¥é“å¦‚ä½•ä½¿ç”¨å®ƒã€‚
- en: There are numerous managed ML services provided by cloud vendors and we would
    want to familiarize ourselves with them. Data engineers prepare datasets for these
    services and it will definitely be useful to do a couple of tutorials on this.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: äº‘ä¾›åº”å•†æä¾›äº†è®¸å¤šæ‰˜ç®¡çš„æœºå™¨å­¦ä¹ æœåŠ¡ï¼Œæˆ‘ä»¬å¸Œæœ›ç†Ÿæ‚‰è¿™äº›æœåŠ¡ã€‚æ•°æ®å·¥ç¨‹å¸ˆä¸ºè¿™äº›æœåŠ¡å‡†å¤‡æ•°æ®é›†ï¼Œåšå‡ ä¸ªç›¸å…³çš„æ•™ç¨‹è‚¯å®šä¼šå¾ˆæœ‰ç”¨ã€‚
- en: 'For example, consider a churn prediction project to understand user churn and
    how to use managed ML services to generate predictions for users. This can easily
    done with BigQuery ML [9] by creating a simple logistic regression model like
    so:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œè€ƒè™‘ä¸€ä¸ªç”¨æˆ·æµå¤±é¢„æµ‹é¡¹ç›®ï¼Œä»¥äº†è§£ç”¨æˆ·æµå¤±æƒ…å†µä»¥åŠå¦‚ä½•ä½¿ç”¨æ‰˜ç®¡çš„æœºå™¨å­¦ä¹ æœåŠ¡ä¸ºç”¨æˆ·ç”Ÿæˆé¢„æµ‹ã€‚è¿™å¯ä»¥é€šè¿‡ BigQuery ML [9] è½»æ¾å®Œæˆï¼Œåªéœ€åˆ›å»ºä¸€ä¸ªç®€å•çš„é€»è¾‘å›å½’æ¨¡å‹ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE16]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'And then we can generate predictions using SQL:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ SQL ç”Ÿæˆé¢„æµ‹ï¼š
- en: '[PRE17]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Conclusion
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: Iâ€™ve tried to summarise a set of data engineering skills and techniques that
    are typically required for entry-level data engineering roles. Based on my experience
    these skills can be acquired within two to three months of active learning. I
    would recommend starting with cloud service providers and Python to build a simple
    ETL service with a CI/CD pipeline for staging and production split. It doesnâ€™t
    cost anything and we can learn fast by running them both locally and when they
    are deployed in the cloud. Data engineers are in high demand in the market right
    now. I hope this article will help you to learn a couple of new things and prepare
    for job interviews.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è¯•å›¾æ€»ç»“ä¸€ç»„æ•°æ®å·¥ç¨‹æŠ€èƒ½å’ŒæŠ€æœ¯ï¼Œè¿™äº›æŠ€èƒ½é€šå¸¸æ˜¯å…¥é—¨çº§æ•°æ®å·¥ç¨‹å¸ˆè§’è‰²æ‰€éœ€çš„ã€‚æ ¹æ®æˆ‘çš„ç»éªŒï¼Œè¿™äº›æŠ€èƒ½å¯ä»¥åœ¨ä¸¤åˆ°ä¸‰ä¸ªæœˆçš„ç§¯æå­¦ä¹ ä¸­æŒæ¡ã€‚æˆ‘å»ºè®®ä»äº‘æœåŠ¡æä¾›å•†å’Œ
    Python å¼€å§‹ï¼Œå»ºç«‹ä¸€ä¸ªç®€å•çš„ ETL æœåŠ¡ï¼Œå¹¶ä¸ºé¢„ç”Ÿäº§å’Œç”Ÿäº§ç¯å¢ƒè®¾ç½® CI/CD ç®¡é“ã€‚è¿™ä¸éœ€è¦èŠ±è´¹ä»»ä½•è´¹ç”¨ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡åœ¨æœ¬åœ°å’Œäº‘ä¸­è¿è¡Œè¿™äº›æœåŠ¡æ¥å¿«é€Ÿå­¦ä¹ ã€‚å½“å‰å¸‚åœºä¸Šå¯¹æ•°æ®å·¥ç¨‹å¸ˆçš„éœ€æ±‚å¾ˆé«˜ã€‚æˆ‘å¸Œæœ›è¿™ç¯‡æ–‡ç« èƒ½å¸®åŠ©ä½ å­¦ä¹ ä¸€äº›æ–°çŸ¥è¯†ï¼Œå¹¶ä¸ºå·¥ä½œé¢è¯•åšå‡†å¤‡ã€‚
- en: Recommedned read
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨èé˜…è¯»
- en: '[1] [https://www.linkedin.com/pulse/linkedin-jobs-rise-2023-25-uk-roles-growing-demand-linkedin-news-uk/](https://www.linkedin.com/pulse/linkedin-jobs-rise-2023-25-uk-roles-growing-demand-linkedin-news-uk/)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [https://www.linkedin.com/pulse/linkedin-jobs-rise-2023-25-uk-roles-growing-demand-linkedin-news-uk/](https://www.linkedin.com/pulse/linkedin-jobs-rise-2023-25-uk-roles-growing-demand-linkedin-news-uk/)'
- en: '[2] [https://towardsdatascience.com/data-platform-architecture-types-f255ac6e0b7](/data-platform-architecture-types-f255ac6e0b7)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [https://towardsdatascience.com/data-platform-architecture-types-f255ac6e0b7](/data-platform-architecture-types-f255ac6e0b7)'
- en: '[3] [https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3](/data-pipeline-design-patterns-100afa4b93e3)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3](/data-pipeline-design-patterns-100afa4b93e3)'
- en: '[4] [https://medium.com/towards-data-science/advanced-sql-techniques-for-beginners-211851a28488](https://medium.com/towards-data-science/advanced-sql-techniques-for-beginners-211851a28488)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [https://medium.com/towards-data-science/advanced-sql-techniques-for-beginners-211851a28488](https://medium.com/towards-data-science/advanced-sql-techniques-for-beginners-211851a28488)'
- en: '[5] [https://cloud.google.com/python/docs/reference/bigquery/latest](https://cloud.google.com/python/docs/reference/bigquery/latest)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [https://cloud.google.com/python/docs/reference/bigquery/latest](https://cloud.google.com/python/docs/reference/bigquery/latest)'
- en: '[6] [https://hudi.apache.org/docs/overview/](https://hudi.apache.org/docs/overview/)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] [https://hudi.apache.org/docs/overview/](https://hudi.apache.org/docs/overview/)'
- en: '[7] [https://cloud.google.com/learn/certification/data-engineer](https://cloud.google.com/learn/certification/data-engineer)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] [https://cloud.google.com/learn/certification/data-engineer](https://cloud.google.com/learn/certification/data-engineer)'
- en: '[8] [https://towardsdatascience.com/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0](/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] [https://towardsdatascience.com/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0](/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0)'
- en: '[9] [https://cloud.google.com/bigquery/docs/bqml-introduction](https://cloud.google.com/bigquery/docs/bqml-introduction)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] [https://cloud.google.com/bigquery/docs/bqml-introduction](https://cloud.google.com/bigquery/docs/bqml-introduction)'
