- en: How to Implement Random Forest Regression in PySpark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在 PySpark 中实现随机森林回归
- en: 原文：[https://towardsdatascience.com/how-to-implement-random-forest-regression-in-pyspark-9582f4964285](https://towardsdatascience.com/how-to-implement-random-forest-regression-in-pyspark-9582f4964285)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-implement-random-forest-regression-in-pyspark-9582f4964285](https://towardsdatascience.com/how-to-implement-random-forest-regression-in-pyspark-9582f4964285)
- en: A PySpark tutorial on regression modeling with Random Forest
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个关于随机森林回归的 PySpark 教程
- en: '[](https://medium.com/@yazihejazi?source=post_page-----9582f4964285--------------------------------)[![Yasmine
    Hejazi](../Images/1c280c78e49f62345b3cd0c30b185482.png)](https://medium.com/@yazihejazi?source=post_page-----9582f4964285--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9582f4964285--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9582f4964285--------------------------------)
    [Yasmine Hejazi](https://medium.com/@yazihejazi?source=post_page-----9582f4964285--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@yazihejazi?source=post_page-----9582f4964285--------------------------------)[![Yasmine
    Hejazi](../Images/1c280c78e49f62345b3cd0c30b185482.png)](https://medium.com/@yazihejazi?source=post_page-----9582f4964285--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9582f4964285--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9582f4964285--------------------------------)
    [Yasmine Hejazi](https://medium.com/@yazihejazi?source=post_page-----9582f4964285--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9582f4964285--------------------------------)
    ·6 min read·Sep 25, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9582f4964285--------------------------------)
    ·6 分钟阅读·2023年9月25日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/68a991e06a39b6a7830a528d8b9a5fb6.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/68a991e06a39b6a7830a528d8b9a5fb6.png)'
- en: Photo by [Jachan DeVol](https://unsplash.com/@jachan_devol?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Jachan DeVol](https://unsplash.com/@jachan_devol?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: PySpark is a powerful data processing engine built on top of Apache Spark and
    designed for large-scale data processing. It provides scalability, speed, versatility,
    integration with other tools, ease of use, built-in machine learning libraries,
    and real-time processing capabilities. It is an ideal choice for handling large-scale
    data processing tasks efficiently and effectively, and its user-friendly interface
    allows for easy code writing in Python.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 是一个强大的数据处理引擎，建立在 Apache Spark 之上，旨在进行大规模数据处理。它提供了可扩展性、速度、多功能性、与其他工具的集成、易用性、内置的机器学习库以及实时处理能力。它是高效有效地处理大规模数据处理任务的理想选择，其用户友好的界面允许用
    Python 轻松编写代码。
- en: Using the [Diamonds data](https://ggplot2.tidyverse.org/reference/diamonds.html)
    found on ggplot2 ([source](https://plotly.com/ggplot2/), [license](https://ggplot2.tidyverse.org/LICENSE.html)),
    we will walk through how to implement a random forest regression model and analyze
    the results with PySpark. If you’d like to see how linear regression is applied
    to the same dataset in PySpark, you can [check it out here](/beginners-guide-to-linear-regression-with-pyspark-bfc39b45a9e9)!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 使用在 ggplot2 上找到的 [Diamonds 数据](https://ggplot2.tidyverse.org/reference/diamonds.html)（[来源](https://plotly.com/ggplot2/)，[许可证](https://ggplot2.tidyverse.org/LICENSE.html)），我们将演示如何在
    PySpark 中实现随机森林回归模型并分析结果。如果你想查看线性回归如何应用于同一数据集，请 [点击这里](/beginners-guide-to-linear-regression-with-pyspark-bfc39b45a9e9)!
- en: 'This tutorial will cover the following steps:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程将涵盖以下步骤：
- en: Load and prepare the data into a vectorized input
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据加载并准备为向量化输入
- en: Train the model using RandomForestRegressor from MLlib
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 MLlib 中的 RandomForestRegressor 训练模型
- en: Evaluate model performance using RegressionEvaluator from MLlib
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 MLlib 中的 RegressionEvaluator 评估模型性能
- en: Plot and analyze feature importance for model transparency
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制并分析特征重要性以提高模型透明度
- en: '![](../Images/4d00e29513e00e4411be59bec39d5745.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d00e29513e00e4411be59bec39d5745.png)'
- en: Photo by [Martin de Arriba](https://unsplash.com/@martindearriba?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Martin de Arriba](https://unsplash.com/@martindearriba?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Prepare the Data
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据
- en: The `diamonds` dataset contains features such as `carat`, `color`, `cut`, `clarity`,
    and more, all listed in the [dataset documentation](https://ggplot2.tidyverse.org/reference/diamonds.html).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`diamonds`数据集包含如`carat`、`color`、`cut`、`clarity`等特征，所有特征都列在[数据集文档](https://ggplot2.tidyverse.org/reference/diamonds.html)中。'
- en: The target variable that we are trying to predict for is `price`.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们试图预测的目标变量是`price`。
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/b51ffc95c7f7ec22a5d4386007d4501e.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b51ffc95c7f7ec22a5d4386007d4501e.png)'
- en: Just like the [linear regression tutorial](/beginners-guide-to-linear-regression-with-pyspark-bfc39b45a9e9),
    we need to preprocess our data so that we have a resulting vector of numerical
    features to use as our model input. We need to encode our categorical variables
    into numerical features and then combine them with our numerical variables to
    make one final vector.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 就像[线性回归教程](/beginners-guide-to-linear-regression-with-pyspark-bfc39b45a9e9)一样，我们需要预处理数据，以便我们得到一个数值特征向量作为模型输入。我们需要将分类变量编码为数值特征，然后将它们与数值变量结合起来，形成一个最终向量。
- en: 'Here are the steps to achieve this result:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 达到此结果的步骤如下：
- en: Separate numerical and categorical features.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数值特征和分类特征分开。
- en: 'Preprocess categorical features with [StringIndexer](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.StringIndexer.html):
    This is essentially assigning a numeric value to each category (i.e.; Fair: 0,
    Ideal: 1, Good: 2, Very Good: 3, Premium: 4).'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '使用[StringIndexer](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.StringIndexer.html)预处理分类特征：这本质上是为每个类别分配一个数值（即：Fair:
    0, Ideal: 1, Good: 2, Very Good: 3, Premium: 4）。'
- en: 'Preprocess categorical features with [OneHotEncoder](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.ml.feature.OneHotEncoder.html):
    This converts categories into binary vectors. The result is a SparseVector that
    indicates which index from StringIndexer has the one-hot value of 1.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用[OneHotEncoder](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.ml.feature.OneHotEncoder.html)预处理分类特征：这将类别转换为二进制向量。结果是一个稀疏向量，表示StringIndexer中的哪个索引具有独热值1。
- en: Combine your numerical features with your new categorical encoded features.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将你的数值特征与新的分类编码特征组合起来。
- en: Assemble feature vector with [VectorAssembler](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html).
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用[VectorAssembler](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html)组装特征向量。
- en: You can use the following code to index and one-hot encode your categorical
    features. This will complete steps 1–3.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下代码对分类特征进行索引和独热编码。这将完成步骤1-3。
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can use the following code to assemble your final feature vector. This will
    complete steps 4–5\. Then, we can run the stages as a pipeline. This runs the
    data through all the feature transformations we’ve defined so far.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下代码来组装你的最终特征向量。这将完成步骤4-5。然后，我们可以将这些阶段作为管道运行。这会将数据经过我们到目前为止定义的所有特征转换。
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Train the Model
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: First, we need to split our dataset into train and test sets.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要将数据集拆分为训练集和测试集。
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, let’s import the Random Forest Regressor ([pyspark.ml.regression.**RandomForestRegressor**](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.RandomForestRegressor.html))
    model from MLlib. Some of the default of some parameters to pay attention to are:
    `maxDepth=5`, `numTrees=20`. You can adjust these during cross-validation or manually
    in order to get the best set of parameters for your problem. Then call `fit()`
    to fit the model to the training data. After fitting the model to our train data,
    we can start predicting. To get predictions on the data, call `transform()`.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们导入MLlib中的随机森林回归模型（[pyspark.ml.regression.**RandomForestRegressor**](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.RandomForestRegressor.html)）。需要注意的一些默认参数是：`maxDepth=5`，`numTrees=20`。你可以在交叉验证期间或手动调整这些参数，以获得适合你问题的最佳参数集。然后调用`fit()`来将模型拟合到训练数据上。在将模型拟合到训练数据后，我们可以开始预测。要对数据进行预测，请调用`transform()`。
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, you can use MLlib’s [RegressionEvaluator](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.RegressionEvaluator.html)
    to evaluate the model. There are multiple evaluation metrics to choose from for
    your use case:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以使用MLlib的[RegressionEvaluator](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.RegressionEvaluator.html)来评估模型。根据你的使用案例，有多种评估指标可以选择：
- en: RMSE — Root mean squared error (Default)
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RMSE — 均方根误差（默认）
- en: MSE — Mean squared error
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MSE — 均方误差
- en: R2 — R-squared
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R2 — 决定系数
- en: MAE — Mean absolute error
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MAE — 平均绝对误差
- en: Var — Explained variance
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Var — 解释的方差
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Train R2: 0.9092 | Test R2: 0.9069'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '训练 R2: 0.9092 | 测试 R2: 0.9069'
- en: Analyze Feature Importance
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析特征重要性
- en: While building the Random Forest, the algorithm tries to minimize **entropy**,
    which is a measure of uncertainty. Entropy is maximized in uniform distribution
    (i.e.; we don’t know if a coin will flip heads or tails), and more uncertainty
    requires more information. The Random Forest algorithm uses Information Gain (IG)
    which is equal to the entropy before minus the entropy after, weighted by the
    number of examples. This helps the algorithm decide which feature should be used
    to split the data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建随机森林时，算法尝试最小化**熵**，这是衡量不确定性的指标。熵在均匀分布中最大化（即我们不知道硬币是正面还是反面），而更多的不确定性需要更多的信息。随机森林算法使用信息增益（IG），其等于分裂前的熵减去分裂后的熵，加权实例数量。这帮助算法决定应该使用哪个特征来分裂数据。
- en: One thing you’ll want to look into is which features are most relevant in your
    model. The Random Forest algorithm has built-in feature importance which can be
    [calculated in different ways](https://mljar.com/blog/feature-importance-in-random-forest/).
    PySpark Random Forest follows the scikit-learn implementation that uses **Gini
    importance** (or mean decrease impurity). Scikit-learn also provides an implementation
    of permutation-based feature importance, but this is not built into PySpark.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能需要关注的是哪些特征在你的模型中最相关。随机森林算法内置特征重要性，可以[以不同方式计算](https://mljar.com/blog/feature-importance-in-random-forest/)。PySpark
    随机森林遵循使用**基尼重要性**（或平均减少的不纯度）的 scikit-learn 实现。Scikit-learn 还提供了基于置换的特征重要性实现，但这在
    PySpark 中没有内置。
- en: 'As described in the [documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.RandomForestRegressionModel.html#pyspark.ml.regression.RandomForestRegressionModel.featureImportances),
    the feature importance is calculated by:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如[文档](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.RandomForestRegressionModel.html#pyspark.ml.regression.RandomForestRegressionModel.featureImportances)中所述，特征重要性是通过以下方式计算的：
- en: importance(feature j) = sum (over nodes which split on feature j) of the gain,
    where the gain is scaled by the number of instances passing through the node
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重要性（特征 j）= 对于在特征 j 上进行分裂的节点，增益的总和，其中增益由通过该节点的实例数量进行缩放
- en: Normalize feature importances to sum to 1.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将特征重要性归一化，使其总和为 1。
- en: We can extract the feature importance from a fitted Random Forest model using
    `rf_model.featureImportances`. Then, use this feature importance and match it
    to the extracted feature names to make it available to view or plot. This view
    is especially of high interest to key stakeholders who want to understand the
    key drivers of the model.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`rf_model.featureImportances`从拟合的随机森林模型中提取特征重要性。然后，使用这些特征重要性并将其与提取的特征名称匹配，以便进行查看或绘图。这种视图对希望了解模型关键驱动因素的主要利益相关者特别重要。
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/e542d71810a8bbc5e317b0a3698839f9.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e542d71810a8bbc5e317b0a3698839f9.png)'
- en: Photo by Author
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 作者拍摄的照片
- en: Summary
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this tutorial, we covered the following steps for implementing Random Forest
    with PySpark.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们介绍了使用 PySpark 实现随机森林的以下步骤。
- en: One-hot encode categorical features using StringIndexer and OneHotEncoder
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 StringIndexer 和 OneHotEncoder 对类别特征进行独热编码
- en: Create input feature vector column using VectorAssembler
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 VectorAssembler 创建输入特征向量列
- en: Split data into train and test
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据拆分为训练集和测试集
- en: Initialize and fit the RandomForestRegressor model on train data
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练数据上初始化并拟合 RandomForestRegressor 模型
- en: Transform model on test data to make predictions
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试数据上转换模型以进行预测
- en: Evaluate the model with RegressionEvaluator
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 RegressionEvaluator 评估模型
- en: Analyze feature importance to understand and improve the model
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分析特征重要性以理解和改进模型
- en: These steps allow us to prepare the data into a vectorized input so that we
    can implement a random forest model in PySpark with Apache Spark’s scalable machine
    learning library, MLlib. We can evaluate the model performance and plot and analyze
    the random forest feature importance to understand our model better. This allows
    us to take advantage of the scalability, speed, and versatility that PySpark provides.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤允许我们将数据准备成向量化输入，从而可以在 PySpark 中使用 Apache Spark 的可扩展机器学习库 MLlib 实现随机森林模型。我们可以评估模型性能，并绘制和分析随机森林特征重要性，以更好地理解我们的模型。这使我们能够利用
    PySpark 提供的可扩展性、速度和多功能性。
