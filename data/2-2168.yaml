- en: UMAP Variance Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: UMAP变异解释
- en: 原文：[https://towardsdatascience.com/umap-variance-explained-b0eacb5b0801](https://towardsdatascience.com/umap-variance-explained-b0eacb5b0801)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/umap-variance-explained-b0eacb5b0801](https://towardsdatascience.com/umap-variance-explained-b0eacb5b0801)
- en: '[MATHEMATICAL STATISTICS AND MACHINE LEARNING FOR LIFE SCIENCES](https://towardsdatascience.com/tagged/stats-ml-life-sciences)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[数学统计与生命科学机器学习](https://towardsdatascience.com/tagged/stats-ml-life-sciences)'
- en: A simple way to interpret UMAP components
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释UMAP成分的简单方法
- en: '[](https://nikolay-oskolkov.medium.com/?source=post_page-----b0eacb5b0801--------------------------------)[![Nikolay
    Oskolkov](../Images/23ec4d70ea0d237eb26782c0c98ed00a.png)](https://nikolay-oskolkov.medium.com/?source=post_page-----b0eacb5b0801--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b0eacb5b0801--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b0eacb5b0801--------------------------------)
    [Nikolay Oskolkov](https://nikolay-oskolkov.medium.com/?source=post_page-----b0eacb5b0801--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://nikolay-oskolkov.medium.com/?source=post_page-----b0eacb5b0801--------------------------------)[![Nikolay
    Oskolkov](../Images/23ec4d70ea0d237eb26782c0c98ed00a.png)](https://nikolay-oskolkov.medium.com/?source=post_page-----b0eacb5b0801--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b0eacb5b0801--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b0eacb5b0801--------------------------------)
    [Nikolay Oskolkov](https://nikolay-oskolkov.medium.com/?source=post_page-----b0eacb5b0801--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b0eacb5b0801--------------------------------)
    ·19 min read·Mar 27, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----b0eacb5b0801--------------------------------)
    ·阅读时间19分钟·2023年3月27日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4a5c3be44e4808f1ca5e658c9ba47eaf.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4a5c3be44e4808f1ca5e658c9ba47eaf.png)'
- en: UMAP computed on [MNIST black and white images of handwritten digits](https://en.wikipedia.org/wiki/MNIST_database).
    Image by author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在[MNIST手写数字黑白图像](https://en.wikipedia.org/wiki/MNIST_database)上计算的UMAP。作者提供的图像
- en: This is the **twenty fifth** post from my column [**Mathematical Statistics
    and Machine Learning for Life Sciences**](https://towardsdatascience.com/tagged/stats-ml-life-sciences),
    where I use plain language to discuss analytical methods from Computational Biology
    and Life Sciences. [**UMAP**](https://arxiv.org/abs/1802.03426)is a [**dimensionality
    reduction**](https://en.wikipedia.org/wiki/Dimensionality_reduction) technique
    that together with [**tSNE**](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)
    became increasingly popular and *de-facto* a standard tool for analysis of [**single
    cell genomics**](https://en.wikipedia.org/wiki/Single-cell_sequencing) data, where
    traditional methods such as [**PCA**](https://en.wikipedia.org/wiki/Principal_component_analysis)
    **have limitations**. However, one disadvantage of UMAP and tSNE in comparison
    with PCA is their **non-interpretable** components that are not straightforward
    to link to the variation in the original data. In this article, I suggest a simple
    method for **estimating amount of variation explained** **by leading UMAP and
    tSNE components**. Using the classic [MNIST](https://en.wikipedia.org/wiki/MNIST_database)
    dataset of black and white images of handwritten digits as a benchmark, I demonstrate
    that **leading UMAP and tSNE components are inferior to PCA components in terms
    of explaining total data variation**, however, surprisingly, demonstrate a better
    linkage to the labels of data points, i.e. they explain more **biological rather
    than total variation in the data**.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我专栏[**数学统计与生命科学机器学习**](https://towardsdatascience.com/tagged/stats-ml-life-sciences)的第**二十五**篇文章，我用简单的语言讨论计算生物学和生命科学中的分析方法。[**UMAP**](https://arxiv.org/abs/1802.03426)是一种[**降维**](https://en.wikipedia.org/wiki/Dimensionality_reduction)技术，与[**tSNE**](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)一起变得越来越流行，并且*实际上*成为了分析[**单细胞基因组学**](https://en.wikipedia.org/wiki/Single-cell_sequencing)数据的标准工具，而传统方法如[**PCA**](https://en.wikipedia.org/wiki/Principal_component_analysis)
    **存在局限性**。然而，与PCA相比，UMAP和tSNE的一个缺点是它们的**不可解释**成分，难以直接与原始数据的变异性联系起来。在这篇文章中，我建议了一种简单的方法来**估计由主要UMAP和tSNE成分解释的变异量**。以经典的[MNIST](https://en.wikipedia.org/wiki/MNIST_database)手写数字黑白图像数据集为基准，我展示了**主要UMAP和tSNE成分在解释数据的总体变异性方面劣于PCA成分**，然而，令人惊讶的是，它们在数据点标签的关联性方面表现更好，即它们解释了**数据中更多的生物学而非总变异性**。
- en: Preparing MNIST Data for Analysis
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为分析准备MNIST数据
- en: As a benchmark dataset, we will be using the [**MNIST**](https://en.wikipedia.org/wiki/MNIST_database)which
    includes 70 000 black and white images of handwritten digits of **28 x 28 pixels**
    **resolution,** i.e. **784 pixels** per image. First, we will download the MNIST
    dataset, check its dimensions and visualize a few random images.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 作为基准数据集，我们将使用[**MNIST**](https://en.wikipedia.org/wiki/MNIST_database)，该数据集包含70,000张手写数字的黑白图像，**28
    x 28像素**的**分辨率**，即每张图像**784像素**。首先，我们将下载MNIST数据集，检查其维度并可视化一些随机图像。
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/bbb2ee20e2181038303ea0ac141c5ad8.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bbb2ee20e2181038303ea0ac141c5ad8.png)'
- en: A few random MNIST images of handwritten digits. Image by author
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一些随机的MNIST手写数字图像。图片由作者提供
- en: The values of MNIST dataset represent **pixel intensities** varying from 0 to
    255, where 0 corresponds to the black background. Therefore, MNIST is a **“zero-inflated”**
    data set which is very **similar to a** **typical single cell** gene expression
    dataset. It is usually recommended to normalize the black and white pixel intensities
    by the largest 255 value. However, here, by analogy with single cell gene expression,
    we will use a **log-transform** as another mild normalization strategy. Also,
    for the sake of time, we will not use all the 70 000 images but **randomly pick
    10 000 images**, later we will draw 10 000 images a few times to make sure that
    our conclusions are robust.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST数据集的值表示**像素强度**，范围从0到255，其中0对应黑色背景。因此，MNIST是一个**“零膨胀”**数据集，这与**典型的单细胞**基因表达数据集非常**相似**。通常建议通过最大值255来归一化黑白像素强度。然而，在这里，类比单细胞基因表达，我们将使用**对数变换**作为另一种温和的归一化策略。同时，为了节省时间，我们不会使用所有70,000张图像，而是**随机挑选10,000张图像**，稍后我们会多次抽取这10,000张图像，以确保我们的结论是可靠的。
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As a first exploratory step, in order to **understand the variation** **in MNIST**,
    we will perform a PCA analysis on the MNIST itself and a **shuffled** version
    of MNIST. This will help us to estimate the number of meaningful, i.e. non-redundant,
    dimensions out of the initial 784 dimensions, i.e. we will figure our the number
    of **informative** dimensions to keep for all the future tested dimension reduction
    and clustering techniques. More details are [here](/how-to-tune-hyperparameters-of-tsne-7c0596a18868).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步探索，为了**理解MNIST中的变异**，我们将对MNIST及其**打乱**版本进行PCA分析。这将帮助我们估计初始784个维度中有多少个有意义的，即非冗余的维度，即我们将确定要保留的**信息性**维度数量，以供将来所有测试的降维和聚类技术使用。更多详情请见[这里](/how-to-tune-hyperparameters-of-tsne-7c0596a18868)。
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/94bf20752b11ee9fbc7a5fe0fcc8ec1e.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94bf20752b11ee9fbc7a5fe0fcc8ec1e.png)'
- en: PCA 2D map, scree plot, and diagnostic figures addressing the number of informative
    principal components in the MNIST images of handwritten digits data set. Image
    by author
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 2D地图、碎石图和诊断图，说明MNIST手写数字数据集中信息性主要成分的数量。图片由作者提供
- en: An important outcome of the PCA analysis on MNIST and shuffled MNIST is that
    the dataset seems to have **62 informative** principal componentsthat all together
    **capture 86% of variation in the data**. Therefore, if we replace the original
    dataset including 784 features by 62 PCs, we will keep most of the variation in
    the data, but will **reduce the dimensionality of the data by more than 10 times**.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对MNIST和打乱后的MNIST进行PCA分析的重要结果是，数据集似乎有**62个信息性**主要成分，这些成分总共**捕捉了86%的数据变异**。因此，如果我们用62个PC替代原始包含784个特征的数据集，我们将保留数据中的大部分变异，但**数据的维度将减少超过10倍**。
- en: Now, let us visualize the MNIST dataset as a 2D map using UMAP and tSNE dimension
    reduction techniques. In both cases, we will use 2 top principal components, i.e.
    2 PCs, for initialization. The perplexity hyperparameterfor tSNE and the number
    of nearest neighbors for UMAP will be computed as a square root of the number
    of data points (images). See [here](/how-to-tune-hyperparameters-of-tsne-7c0596a18868)
    for details.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用UMAP和tSNE降维技术将MNIST数据集可视化为2D地图。在这两种情况下，我们将使用2个主要成分，即2个PC进行初始化。tSNE的困惑度超参数和UMAP的最近邻数量将计算为数据点（图像）数量的平方根。详情请见[这里](/how-to-tune-hyperparameters-of-tsne-7c0596a18868)。
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/5e6cf27df107b6aa10b7c06c5dd2594e.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e6cf27df107b6aa10b7c06c5dd2594e.png)'
- en: tSNE and UMAP two-dimensional maps of MNIST handwritten digits data. Image by
    author
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: tSNE和UMAP的MNIST手写数字数据二维地图。图片由作者提供
- en: One obvious conclusion that can be drawn from the comparison of 2D PCA, tSNE
    and UMAP pictures is that the **classes** **of handwritten digits** are much more
    resolved by the non-linear **neighbor-graph** dimensionality reduction methods
    such as tSNE / UMAP compared to the **linear** **matrix factorization** techniques
    such as PCA. Therefore if we assume the classes of handwritten digits (aka biological
    phenotype, e.g. cell type in scRNAseq experiments) to comprise **most of the variation**
    in the MNIST data, it probably makes sense to hypothesize that **two tSNE/UMAP
    components are capabale of capturing more biological variation than two PCA components
    (at least in the MNIST dataset)**. In the next sections, we are going to try to
    **quantify and prove this hypothesis**.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从2D PCA、tSNE和UMAP图像的比较中可以得出一个明显的结论，即**手写数字的类别**在非线性**邻域图**降维方法（如tSNE / UMAP）中比在**线性**
    **矩阵分解**技术（如PCA）中得到的分辨率要高得多。因此，如果我们假设手写数字的类别（即生物表型，例如scRNAseq实验中的细胞类型）占据了MNIST数据中**大部分的变异**，那么可以合理地假设**两个tSNE/UMAP组件能够捕捉到比两个PCA组件更多的生物变异（至少在MNIST数据集中）**。在接下来的章节中，我们将尝试**量化和证明这一假设**。
- en: MNIST variation explained by PCA components
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PCA组件解释的MNIST变异
- en: As non-linearneighbor-graph based techniques, **UMAP / tSNE do not seem to have
    the concept of variance explained by its components** **in contrast to matrix
    factorization linear dimension reduction techniques such as PCA**, see for example
    the [answer](https://github.com/lmcinnes/umap/issues/122) of Leland McInnes, the
    author of UMAP. Here, nevertheless, we will try to quantify the amount of MNIST
    variation in pixel intensities explained by UMAP / tSNE components and compare
    it with the amount of variation explained by PCA components using the [**Partial
    Least Square (PLS) regression**](https://en.wikipedia.org/wiki/Partial_least_squares_regression)
    and the [**R-squared statistic**](https://en.wikipedia.org/wiki/Coefficient_of_determination)
    generalized for matrix operations.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 作为非线性邻域图技术，**UMAP / tSNE似乎没有其组件所解释的变异概念**，**与矩阵分解线性降维技术（如PCA）形成对比**，例如，请参见UMAP作者Leland
    McInnes的[回答](https://github.com/lmcinnes/umap/issues/122)。不过，在这里，我们将尝试量化UMAP /
    tSNE组件解释的MNIST像素强度变异量，并与PCA组件解释的变异量进行比较，使用[**部分最小二乘（PLS）回归**](https://en.wikipedia.org/wiki/Partial_least_squares_regression)和[**R方统计量**](https://en.wikipedia.org/wiki/Coefficient_of_determination)来推广矩阵运算。
- en: Let us check the percentage of MNIST variation explained by first principal
    component (PC1), we can easily compute and extract it from the PCA as the normalized
    **first eigen value** of MNIST. Below, we can see that **PC1 explains ~11% of
    MNIST variation**.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查MNIST中由第一个主成分（PC1）解释的变异百分比，我们可以很容易地通过PCA计算并提取它作为MNIST的标准化**第一个特征值**。如下所示，**PC1解释了约11%的MNIST变异**。
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, **we will** **reproduce this number from the** **Partial Least Squares
    (PLS)** **regression**. We are going to use the following reasoning. Suppose we
    want to **approximate a matrix X by another matrix PCA_matrix**, which for now
    includes only one column, that is PC1, but generally can include up to 784 columns
    for MNIST dataset. Then, we can fit a PLS linear regression model of **X = B *
    PCA_matrix** and compute a [**R-squared statistic**](https://en.wikipedia.org/wiki/Coefficient_of_determination)
    which will reflect the amount of variation in **X** explained by **PCA_matrix**.
    In the matrix form, the [R-squared statistic](https://en.wikipedia.org/wiki/Coefficient_of_determination)
    will be given by the following equation:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，**我们将** **通过** **部分最小二乘（PLS）** **回归重新生成这个数字**。我们将使用以下推理。假设我们想要**用另一个矩阵PCA_matrix来近似矩阵X**，目前只包括一个列，即PC1，但通常可以包括最多784列的MNIST数据集。然后，我们可以拟合一个PLS线性回归模型**X
    = B * PCA_matrix**并计算一个[**R方统计量**](https://en.wikipedia.org/wiki/Coefficient_of_determination)，这将反映**X**中由**PCA_matrix**解释的变异量。在矩阵形式中，[R方统计量](https://en.wikipedia.org/wiki/Coefficient_of_determination)将由以下方程给出：
- en: '![](../Images/144e59fde8f7fd2f478158cf04c9c208.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/144e59fde8f7fd2f478158cf04c9c208.png)'
- en: 'where **B*PCA_matrix** represents a prediction of **X** from its approximation
    **PCA_matrix** and will be found from the PLS regression (first equation). To
    technically implement this procedure in **scikit-learn Python module**, we will
    first fit a PLS model with **X** as a response variable, and **PCA_matrix** as
    an explanatory variable, we compute a prediction **y_pred = B*PCA_matrix**, and,
    finally, we can either use the **r2_score** function in scikit-learn, or the second
    equation above for computing the [R-squared statistic](https://en.wikipedia.org/wiki/Coefficient_of_determination).
    Let us check that both will give identical answers:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其中**B*PCA_matrix**表示从其近似**PCA_matrix**对**X**的预测，并将通过PLS回归（第一方程）得出。为了在**scikit-learn
    Python模块**中技术性地实现这一过程，我们首先拟合一个以**X**为响应变量、**PCA_matrix**为解释变量的PLS模型，我们计算预测值**y_pred
    = B*PCA_matrix**，最后，我们可以使用scikit-learn中的**r2_score**函数，或使用上述第二方程来计算[R-squared统计量](https://en.wikipedia.org/wiki/Coefficient_of_determination)。让我们检查一下两者是否会给出相同的答案：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Thus, by utilizing PLS regression, we computed the fraction of total MNIST **variation
    explained by the first principal component** **outside of the PCA algorithm**.
    Compare the fraction calculated by the PLS with the respective variance explained
    by the PCA algorithm pca.explained_variance_ratio_[0] which is the ratio of first
    eigen value divided by the sum of all eigen values. They are nearly identical.
    Now let **PCA_matrix** consist of a number of PCs. Below, we demonstrate that
    the PLS computation of cumulative fraction of variance explained by principal
    components will result in **nearly identical** to PCA cumulative variance explained.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过利用PLS回归，我们计算了**第一主成分解释的MNIST总变异** **在PCA算法之外**的比例。将PLS计算的比例与PCA算法中pca.explained_variance_ratio_[0]（即第一特征值与所有特征值之和的比率）解释的方差进行比较。它们几乎是相同的。现在让**PCA_matrix**包含若干个主成分。下面，我们演示PLS计算主成分解释的累计方差将**与PCA累计解释的方差几乎相同**。
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/18c9c122c04f8e0aa06f9b2bbf5b9755.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18c9c122c04f8e0aa06f9b2bbf5b9755.png)'
- en: Comparison of cumulative variances in MNIST image pixel intensities data explained
    by PCA algorithm and PLS-based approach. Image by author
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PCA算法和基于PLS的方法对MNIST图像像素强度数据解释的累计方差进行比较。图片由作者提供
- en: For clarity, we plot PLS-computed MNIST variance explained for PC1-PC20 on the
    top of the native PCA eigen value based variance explained which is done for PC1
    — PC100\. We can see, that **the** **curves of cumulative variance explained computed
    by PLS and PCA nicely coincide**. This is no surprise since PLS computation essentially
    mimics what is going under the hood in the PCA algorithm. However, very importantly,
    this gives us an **instrument** of computing variance of **X** explained by **any
    approximation**, i.e. not only the **PCA_matrix** but also any other matrix. In
    the next session, we will use **UMAP_matrix** as an approximation of the original
    **X** matrix of MNIST pixel intensities and **compare the cumulative variance
    of X explained by UMAP and PCA components**.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清晰起见，我们将PLS计算的MNIST方差（PC1-PC20）绘制在原生PCA特征值基础上解释的方差（PC1-PC100）之上。我们可以看到，**PLS和PCA计算的累计方差曲线很好地重合**。这并不令人惊讶，因为PLS计算本质上模拟了PCA算法的内部过程。然而，非常重要的是，这为我们提供了计算**X**通过**任何近似**（不仅是**PCA_matrix**，还包括其他任何矩阵）解释的方差的**工具**。在下一节中，我们将使用**UMAP_matrix**作为MNIST像素强度的原始**X**矩阵的近似，并**比较UMAP和PCA组件解释的X累计方差**。
- en: MNIST variation explained by UMAP components
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MNIST由UMAP组件解释的变异
- en: 'In this section we will use the methodology of PLS estimation of variance explained
    developed in the previous section. Again, we will use the same reasoning: since
    UMAP gives some sort of **approximation** **of the original data** **X**, and
    people even run **clustering on 2D UMAP** for discovering cell types in scRNAseq
    field, we can use the **UMAP_matrix** (and we can do the same for tSNE) as an
    approximation of **X**, then we fit the PLS model **X=B * UMAP_matrix**, and estimate
    the fraction of MNIST variance explained by UMAP components via R-squared statistic
    as'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用前一节中开发的PLS估计解释方差的方法。同样，我们将使用相同的推理：由于UMAP提供了对原始数据**X**的某种**近似**，并且人们甚至在2D
    UMAP上运行**聚类**以发现scRNAseq领域中的细胞类型，我们可以将**UMAP_matrix**（对于tSNE也是如此）作为**X**的近似，然后拟合PLS模型**X=B
    * UMAP_matrix**，并通过R-squared统计量估计UMAP组件解释的MNIST方差的比例。
- en: '![](../Images/bff56ab4bae4e757d2606b5d6b0ae95c.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bff56ab4bae4e757d2606b5d6b0ae95c.png)'
- en: Now, we will use the PLS methodology developed in the previous section, and
    compute the fraction of MNIST variance explained by the first UMAP component.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用在前一部分开发的PLS方法，计算第一个UMAP组件解释的MNIST方差比例。
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We conclude that **first components of both UMAP and tSNE explain about 7% of
    MNIST variation which is lower than the 11% explained by the first PCA component**.
    This is not surprising. Intuitively, it is hard to expect that there can be another
    latent variable alternative to PC1 that explains more than 11% of MNIST variation
    without redefining the concept of “explained variance”. The current definition
    comes from the PCA (**normalized eigen values**), and linear regression, that
    is R-squared, analyses. Both are linear frameworks. **In addition,** **PC1 by
    definition corresponds to a direction of maximal data variation**. Therefore,
    if another latent variable comes from a nonlinear analysis such as UMAP, that
    does not *per se* attempts to maximize variation in the data, **it would be hard
    to expect that e.g. UMAP1 explains more variation in MNIST** **within linear definition
    of “variance explained”**. Now we will calculate the variance explained by a few
    top tSNE and UMAP components. To make our analysis more robust towards sampling
    of the 10 000 images, we will independently draw them **N_iter** times. Also,
    for each iteration, we will slightly **change /** **perturb** **the hyperparameters
    of UMAP and tSNE, as well as the number of data points for PCA**. This will allow
    us to build **confidence intervals** and address the sensitivity of our analysis.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得出结论，**UMAP和tSNE的前几个组件解释了约7%的MNIST变异，这低于第一个PCA组件解释的11%**。这并不令人惊讶。直观上，很难期待有另一个潜在变量替代PC1，解释超过11%的MNIST变异而不重新定义“解释方差”的概念。当前的定义来自PCA（**标准化特征值**）和线性回归，即R平方分析。两者都是线性框架。**此外，**
    **PC1根据定义对应于数据变异的最大方向**。因此，如果另一个潜在变量来自非线性分析如UMAP，这并不*本身*旨在最大化数据的变异，**那么很难期待例如UMAP1在MNIST中解释更多变异**
    **在“解释方差”的线性定义下**。现在我们将计算几个顶级tSNE和UMAP组件解释的方差。为了使我们的分析对10,000张图像的采样更具稳健性，我们将独立抽取它们**N_iter**次。此外，对于每次迭代，我们将稍微**改变/**
    **扰动** **UMAP和tSNE的超参数，以及PCA的数据点数量**。这将使我们能够建立**置信区间**并解决我们分析的敏感性。
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/c0e87730240753177159acb8cd7c75f6.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c0e87730240753177159acb8cd7c75f6.png)'
- en: Cumulative variance in MNIST image pixel intensities data explained by PCA,
    tSNE and UMAP components. Image by author
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST图像像素强度数据的累积方差由PCA、tSNE和UMAP组件解释。图片由作者提供
- en: Here, we conclude that leading PCA components explain consistently more variation
    in MNIST dataset than leading tSNE and UMAP components. This result was expected
    since UMAP and tSNE do not aim at building directions of maximal variation in
    contrast to PCA. Further, **the amounts of variation in the MNIST data explained
    by leading tSNE and UMAP components are comparable, and both lower than the ones
    explained by PCA components**. Therefore, we seem to observe a systematic difference
    in MNIST explained variance **between matrix factorization and neghbor-graph dimensionality
    reduction techniques**.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们得出结论，主成分PCA在MNIST数据集中的解释变异量始终大于主成分tSNE和UMAP。这个结果是预期的，因为UMAP和tSNE并不旨在建立最大变异方向，而PCA则有此目标。此外，**主成分tSNE和UMAP在MNIST数据中解释的变异量是相当的，并且都低于PCA组件解释的变异量**。因此，我们似乎观察到MNIST解释方差**在矩阵分解和邻接图降维技术之间**的系统性差异。
- en: Biological variation explained by UMAP components
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生物学变异由UMAP组件解释
- en: In the previous section, we showed that leading components of UMAP and tSNE
    explain significantly less variation in MNIST dataset compared to the leading
    PCA components, which is a sort of **expected result** since PCA by definition
    attempts to find components of maximal variation in the data in contrast to UMAP
    and tSNE.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一部分中，我们展示了UMAP和tSNE的主成分在MNIST数据集中解释的变异量明显少于主成分PCA，这是一种**预期的结果**，因为PCA根据定义旨在寻找数据中的最大变异成分，而UMAP和tSNE则不是。
- en: 'This is however an interesting paradox: looking at the 2D UMAP and tSNE figures
    of MNIST data we can clearly see more **distinct** clusters of handwritten digits
    compared to the ones in the corresponding 2D PCA figure. **Despite 2D UMAP and
    tSNE explain less variation in** MNIST **than PCA**.'
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 然而，这确实是一个有趣的悖论：通过查看MNIST数据的2D UMAP和tSNE图，我们可以明显看到更多的**明显**的手写数字簇，相比于对应的2D PCA图。**尽管2D
    UMAP和tSNE解释的MNIST方差** **比PCA少**。
- en: Here, we are going to hypothesize that UMAP / tSNE components are linked to
    a **phenotype of interest**, that is MNIST **labels** in our case, or cell types
    for scRNAseq, **rather than total variation in the data**. To check this hypothesis,
    let us first explore how leading PCA, UMAP and tSNE components correlate with
    the MNIST labels.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将假设 UMAP / tSNE 组件与**感兴趣的表型**有关，即 MNIST **标签**，或者 scRNAseq 的细胞类型，**而不是数据的总变异**。为了检验这一假设，让我们首先探索主导
    PCA、UMAP 和 tSNE 组件如何与 MNIST 标签相关。
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/50fe3a391b1723753fbf729be66a2fbc.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/50fe3a391b1723753fbf729be66a2fbc.png)'
- en: Pairwise Spearman correlations between MNIST labels and PCA / tSNE / UMAP components.
    Image by author
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 标签与 PCA / tSNE / UMAP 组件之间的配对斯皮尔曼相关性。图像由作者提供
- en: We can observe, that **both tSNE1 and UMAP1 have a stronger correlation with
    the MNIST labels compared to PC1**. Further, the second components of PCA, tSNE
    and UMAP have comparable strength of correlation with the MNIST labels, **whereas
    the** **third UMAP component has a much stronger correlation with MNIST labels
    compared to the third components of PCA and tSNE**, that is PC3 and tSNE3, respectively,
    which **appear to be almost uncorrelated with the MNIST labels**. In the complete
    notebook, available on my [github](https://github.com/NikolayOskolkov/UMAP_VarianceExplained),
    I show that the **obtained heatmap is robust** **with respect to sampling and
    perturbations of PCA / tSNE / UMAP hyperparameters**.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到，**tSNE1 和 UMAP1 与 MNIST 标签的相关性比 PC1 更强**。此外，PCA、tSNE 和 UMAP 的第二个组件与
    MNIST 标签的相关性强度相当，**而第三个 UMAP 组件与 MNIST 标签的相关性远远强于 PCA 和 tSNE 的第三个组件**，即 PC3 和
    tSNE3，这两个组件**与 MNIST 标签几乎没有相关性**。在我的 [github](https://github.com/NikolayOskolkov/UMAP_VarianceExplained)
    上提供的完整笔记本中，我展示了**得到的热图在采样和 PCA / tSNE / UMAP 超参数扰动方面是稳健的**。
- en: '**Alternatively**, we can also quantify the linkage between the PCA, tSNE and
    UMAP components on one side, and the MNIST labels (classes of digits, aka cell
    types in scRNAseq data) on the other side, through the **PLS regression** methodology
    developed in the previous sections. By analogy, if we consider **UMAP_matrix**
    (or **PCA_matrix** or **tSNE_matrix**) as an approximation of the MNIST vector
    of **labels**, we can fit the PLS model **labels= B * UMAP_matrix**, and estimate
    the fraction of **variation in the MNIST** **labels** **explained by the UMAP
    components** as'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**另外**，我们还可以通过前几节开发的 **PLS 回归** 方法量化 PCA、tSNE 和 UMAP 组件与 MNIST 标签（数字类，即 scRNAseq
    数据中的细胞类型）之间的联系。通过类比，如果我们将 **UMAP_matrix**（或 **PCA_matrix** 或 **tSNE_matrix**）视为
    MNIST 标签向量的近似值，我们可以拟合 PLS 模型 **labels= B * UMAP_matrix**，并估算**UMAP 组件解释的 MNIST
    标签变异** 的比例。'
- en: '![](../Images/38885f1e36fef283e485d7379df5d97a.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38885f1e36fef283e485d7379df5d97a.png)'
- en: 'Let us test how much variance in MNIST **labels** is explained by PC1, tSNE1
    and UMAP1:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们测试 PC1、tSNE1 和 UMAP1 解释了多少 MNIST **标签**的方差：
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We can clearly see that tSNE1, and especially UMAP1, explain much more variation
    in MNIST labels compared to PC1, which confirms what we have previously observed
    with Spearman correlation in the heatmap above. We can also reproduce the **R-squared**
    values above from the linear regression model in **statsmodels** package.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以清楚地看到，tSNE1，特别是 UMAP1，比 PC1 解释了更多的 MNIST 标签变异，这证实了我们之前在上面的热图中通过斯皮尔曼相关性观察到的结果。我们还可以从
    **statsmodels** 包中的线性回归模型中复现上述的**R-squared** 值。
- en: '[PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/747e74fd087029c335546e63c0d2ca7d.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/747e74fd087029c335546e63c0d2ca7d.png)'
- en: Now, **once we have made sure that** **we can correctly compute** the variance
    in MNIST labels explained by PC1, tSNE1 and UMAP1 by using the PLS and R — squared
    methodology, we can extend this procedure for other leading PCA /tSNE/ UMAP components,
    and visualize how thecumulative variance explained changesas more and more components
    are added.As per usual, we will perturb the PCA, tSNE and UMAP with respect to
    the sub-sampling iteration, number of images and hyperparameters of the methods.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，**一旦我们确保** **可以正确计算** 由 PC1、tSNE1 和 UMAP1 解释的 MNIST 标签方差，通过使用 PLS 和 R-squared
    方法，我们可以将这一过程扩展到其他主要的 PCA / tSNE / UMAP 组件，并可视化累积方差解释如何随着更多组件的加入而变化。像往常一样，我们将对
    PCA、tSNE 和 UMAP 进行子采样迭代、图像数量和方法超参数的扰动。
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/8370fcea93acd07e61f9786869c53039.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8370fcea93acd07e61f9786869c53039.png)'
- en: Cumulative variance in MNIST labels explained by PCA, tSNE and UMAP components.
    Image by author
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: PCA、tSNE 和 UMAP 组件解释的 MNIST 标签的累积方差。图像由作者提供
- en: '**Here,** **surprisingly, we observe that** **leading UMAP and tSNE components
    seem to explain much more of variation in the MNIST labels compared to leading
    PCA components.** This is somehow counter-intuitive as we saw in the previous
    section that leading tSNE and UMAP components explain less variation in MNIST
    image pixel intensities. Nevertheless, this cumulative explained variance plot
    above basically confirms the correlation heatmap between the MNIST labels, and
    PCA / tSNE / UMAP components obtained previously.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**在这里，** **令人惊讶的是，我们观察到** **领先的UMAP和tSNE组件似乎比领先的PCA组件解释了MNIST标签中更多的变化**。这在某种程度上是违反直觉的，因为我们在前一节中看到领先的tSNE和UMAP组件解释了较少的MNIST图像像素强度变化。然而，上面的累计解释方差图基本上确认了MNIST标签与PCA
    / tSNE / UMAP组件之间的相关性热图。'
- en: Here, we have an interesting effect. We saw in the previous section that the
    leading tSNE and UMAP components can not capture more variation in the MNIST than
    leading PCA components. However, surpisingly, they are **able to capture more
    variation in MNIST labels, that is classes of handwritten digits, despite the
    classes were not provided to tSNE / UMAP at all, when performing the dimension
    reduction.** In other words, all three dimension reduction techniques are completely
    unsupervised, i.e. they know nothing about the classes of handwritten digits.
    Nevertheless, **tSNE and especially UMAP components seem to be, surprisingly,
    linked to the classes of digits** without being able to capture substantial proportion
    of variation in MNIST pixel intensities across the images. I do not fully understand
    this effect but believe it is an interesting observation, which I would be curious
    to further explore. **Let me know in the comments, if you have more insights into
    the nature of this effect.**
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们观察到一个有趣的现象。我们在前一节中看到，领先的tSNE和UMAP组件无法捕捉比领先的PCA组件更多的MNIST变化。然而，令人惊讶的是，尽管在进行降维时这些类没有提供给tSNE
    / UMAP，它们却**能够捕捉MNIST标签，即手写数字的类别中的更多变化**。换句话说，这三种降维技术都是完全无监督的，即它们对手写数字的类别一无所知。尽管如此，**tSNE，尤其是UMAP组件，似乎与数字类别惊人地相关**，而无法捕捉MNIST图像中像素强度的实质性变化。我不完全理解这个现象，但认为这是一个有趣的观察，我很想进一步探索。**如果你对这种现象的本质有更多见解，请在评论中告诉我。**
- en: Summary
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this article, we have learnt that a **Partial Least Squares (PLS)** **approach**
    can be used for building **intuition about data variation explained by tSNE and
    UMAP components**. Using this approach, we demonstrated that **tSNE and UMAP components
    explain (unsurprisingly) less variation in MNIST image pixel intensities** **data
    compared to PCA components**, however, are surprisingly strongly **linked to the
    labels of MNIST images**. The nature of this effect is unclear taken into account
    the unsupervised design of all the three dimension reduction techniques.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们了解到**偏最小二乘（PLS）** **方法**可以用来建立**对tSNE和UMAP组件解释的数据变化的直觉**。使用这种方法，我们展示了**tSNE和UMAP组件解释（并不出奇）比PCA组件少的MNIST图像像素强度变化**，然而，令人惊讶的是，它们与**MNIST图像的标签****有很强的关联**。考虑到这三种降维技术的无监督设计，这种效果的本质尚不清楚。
- en: As per usual, let me know in the comments below what analytical methods from
    Life Sciencesand Computational Biology seem **especially mysterious** to you and
    I will try to address them in this column. Check the files used for the post on
    my [github](https://github.com/NikolayOskolkov/UMAP_VarianceExplained). Follow
    me at **Medium** [**Nikolay Oskolkov**](https://medium.com/u/8570b484f56c?source=post_page-----b0eacb5b0801--------------------------------),
    in **Twitter** @NikolayOskolkov, in **Mastodon** @oskolkov@mastodon.social and
    connect via [**Linkedin**](http://linkedin.com/in/nikolay-oskolkov-abb321186).
    In the next post, I will discuss **how to cluster in UMAP space**, stay tuned.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，请在下面的评论中告诉我在生命科学和计算生物学中哪些分析方法对你来说**特别神秘**，我会尽量在这一栏中讨论它们。查看我在[github](https://github.com/NikolayOskolkov/UMAP_VarianceExplained)上的帖子所用的文件。在**Medium**上关注我，[**Nikolay
    Oskolkov**](https://medium.com/u/8570b484f56c?source=post_page-----b0eacb5b0801--------------------------------)，在**Twitter**上@NikolayOskolkov，在**Mastodon**上@oskolkov@mastodon.social，通过[**Linkedin**](http://linkedin.com/in/nikolay-oskolkov-abb321186)与我联系。在下一篇文章中，我将讨论**如何在UMAP空间中进行聚类**，敬请关注。
