- en: 'The Return of the Fallen: Transformers for Forecasting'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 失落的回归：用于预测的变换器
- en: 原文：[https://towardsdatascience.com/the-return-of-the-fallen-transformers-for-forecasting-24f6fec5bc30?source=collection_archive---------11-----------------------#2023-05-24](https://towardsdatascience.com/the-return-of-the-fallen-transformers-for-forecasting-24f6fec5bc30?source=collection_archive---------11-----------------------#2023-05-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-return-of-the-fallen-transformers-for-forecasting-24f6fec5bc30?source=collection_archive---------11-----------------------#2023-05-24](https://towardsdatascience.com/the-return-of-the-fallen-transformers-for-forecasting-24f6fec5bc30?source=collection_archive---------11-----------------------#2023-05-24)
- en: '![](../Images/f96a42e66749f5f9311f8123b4545868.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f96a42e66749f5f9311f8123b4545868.png)'
- en: Photo by [Aditya Vyas](https://unsplash.com/@aditya1702?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Aditya Vyas](https://unsplash.com/@aditya1702?utm_source=medium&utm_medium=referral)
    提供，发布在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'Introducing a new transformer model: PatchTST'
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍一种新的变换器模型：PatchTST
- en: '[](https://medium.com/@upadhyan?source=post_page-----24f6fec5bc30--------------------------------)[![Nakul
    Upadhya](../Images/336cb21272e9b1f098177adbde50e92e.png)](https://medium.com/@upadhyan?source=post_page-----24f6fec5bc30--------------------------------)[](https://towardsdatascience.com/?source=post_page-----24f6fec5bc30--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----24f6fec5bc30--------------------------------)
    [Nakul Upadhya](https://medium.com/@upadhyan?source=post_page-----24f6fec5bc30--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@upadhyan?source=post_page-----24f6fec5bc30--------------------------------)[![Nakul
    Upadhya](../Images/336cb21272e9b1f098177adbde50e92e.png)](https://medium.com/@upadhyan?source=post_page-----24f6fec5bc30--------------------------------)[](https://towardsdatascience.com/?source=post_page-----24f6fec5bc30--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----24f6fec5bc30--------------------------------)
    [Nakul Upadhya](https://medium.com/@upadhyan?source=post_page-----24f6fec5bc30--------------------------------)'
- en: ·
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d9dddc62a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-return-of-the-fallen-transformers-for-forecasting-24f6fec5bc30&user=Nakul+Upadhya&userId=4d9dddc62a80&source=post_page-4d9dddc62a80----24f6fec5bc30---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----24f6fec5bc30--------------------------------)
    ·5 min read·May 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F24f6fec5bc30&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-return-of-the-fallen-transformers-for-forecasting-24f6fec5bc30&user=Nakul+Upadhya&userId=4d9dddc62a80&source=-----24f6fec5bc30---------------------clap_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d9dddc62a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-return-of-the-fallen-transformers-for-forecasting-24f6fec5bc30&user=Nakul+Upadhya&userId=4d9dddc62a80&source=post_page-4d9dddc62a80----24f6fec5bc30---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----24f6fec5bc30--------------------------------)
    · 5 min read · 2023年5月24日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F24f6fec5bc30&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-return-of-the-fallen-transformers-for-forecasting-24f6fec5bc30&user=Nakul+Upadhya&userId=4d9dddc62a80&source=-----24f6fec5bc30---------------------clap_footer-----------)'
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F24f6fec5bc30&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-return-of-the-fallen-transformers-for-forecasting-24f6fec5bc30&source=-----24f6fec5bc30---------------------bookmark_footer-----------)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F24f6fec5bc30&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-return-of-the-fallen-transformers-for-forecasting-24f6fec5bc30&source=-----24f6fec5bc30---------------------bookmark_footer-----------)'
- en: Lately, there has been a significant surge in the adoption of Transformer-based
    approaches. The remarkable achievements of models like BERT and ChatGPT have encouraged
    researchers to explore the application of this architecture in various areas,
    including time series forecasting. However, recent work by researchers at the
    Chinese University of Hong Kong and the International Digital Economy Activity
    showed that the implementations developed for this task were less than optimal
    and could be beaten by a simple linear model on various benchmarks [1].
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，基于变换器的方法被广泛采用。像BERT和ChatGPT这样的模型取得的显著成就鼓励了研究人员探索这种架构在各种领域的应用，包括时间序列预测。然而，香港中文大学和国际数字经济活动的研究人员最近的工作表明，为这一任务开发的实现效果不佳，并且在各种基准测试中可能被简单的线性模型击败[1]。
- en: 'In response, researchers at Princeton and IBM proposed PatchTST (Patched Time
    Series Transformer) in their paper [A Time Series is Worth 64 Words](https://arxiv.org/abs/2211.14730)
    [2]. In this paper, Nie et. al introduces 2 key mechanisms that bring transformers
    back to the forecasting arena:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，普林斯顿大学和IBM的研究人员在他们的论文[A Time Series is Worth 64 Words](https://arxiv.org/abs/2211.14730)
    [2]中提出了PatchTST（修补时间序列变换器）。在这篇论文中，Nie等人介绍了两种关键机制，将变换器带回预测领域：
- en: 'Patched attention: Their attention takes in large parts of the time series
    as tokens instead of a point-wise attention'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修补注意力：他们的注意力机制将时间序列的大部分作为标记进行处理，而不是逐点注意力
- en: 'Channel Independence: different target series in a time series are processed
    independently of each other with different attention weights.'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 渠道独立性：时间序列中的不同目标序列被独立处理，使用不同的注意力权重。
- en: In this post, I aim to summarize how these two mechanisms work and discuss the
    implications of the results discovered by Nie et. al [2].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我旨在总结这两种机制的工作原理，并讨论Nie等人[2]发现的结果的意义。
- en: 'Background: Are Transformers Effective'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景：变换器是否有效
- en: 'Before we dive into PatchTST, we need to first understand the problems that
    Zeng et. al discovered with self-attention in the forecasting space. For those
    interested in a detailed summary, I highly encourage reading the original paper
    or the summary I have written on their work:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨PatchTST之前，我们需要首先了解Zeng等人发现的自注意力在预测领域中的问题。对于那些对详细总结感兴趣的人，我强烈建议阅读原始论文或我对他们工作的总结：
- en: '[](https://arxiv.org/abs/2205.13504?source=post_page-----24f6fec5bc30--------------------------------)
    [## Are Transformers Effective for Time Series Forecasting?'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://arxiv.org/abs/2205.13504?source=post_page-----24f6fec5bc30--------------------------------)
    [## 变换器在时间序列预测中有效吗？'
- en: Recently, there has been a surge of Transformer-based solutions for the long-term
    time series forecasting (LTSF) task…
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最近，长期时间序列预测（LTSF）任务中出现了许多基于变换器的解决方案……
- en: arxiv.org](https://arxiv.org/abs/2205.13504?source=post_page-----24f6fec5bc30--------------------------------)
    [](/transformers-lose-to-linear-models-902164ca5974?source=post_page-----24f6fec5bc30--------------------------------)
    [## Do Transformers Lose to Linear Models?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: arxiv.org](https://arxiv.org/abs/2205.13504?source=post_page-----24f6fec5bc30--------------------------------)
    [](/transformers-lose-to-linear-models-902164ca5974?source=post_page-----24f6fec5bc30--------------------------------)
    [## 变换器是否输给了线性模型？
- en: Long-Term Forecasting using Transformers may not be the way to go
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用变换器进行长期预测可能不是最佳选择
- en: towardsdatascience.com](/transformers-lose-to-linear-models-902164ca5974?source=post_page-----24f6fec5bc30--------------------------------)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/transformers-lose-to-linear-models-902164ca5974?source=post_page-----24f6fec5bc30--------------------------------)
- en: In summary, self-attention has a few key problems when applied to the forecasting
    space. More specifically, prior time-series transformers had point-wise self-attention
    mechanisms where each individual time-stamp was treated as a token. However, this
    has two main issues. For one, this causes the attention to be permutation-invariant
    and the same attention values would be observed if you were to flip points around.
    Additionally, a single timestamp doesn’t have a lot of information contained by
    itself and gets its importance from the timestamps around it. A parallel in language
    processing is if we focused on individual characters instead of words.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 总而言之，自注意力在应用于预测领域时存在一些关键问题。具体而言，以前的时间序列变换器使用逐点自注意力机制，其中每个时间戳被视为一个标记。然而，这存在两个主要问题。首先，这导致注意力机制对点的排列不变，如果你将点的位置颠倒，则观察到的注意力值相同。此外，单个时间戳本身包含的信息有限，其重要性来自于周围的时间戳。与语言处理中的情况类似，如果我们关注单个字符而不是单词。
- en: 'These problems led to a few interesting results when testing forecasting transformers:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题在测试预测变换器时导致了一些有趣的结果：
- en: Transformers seemed incredibly prone to overfitting on random patterns since
    adding noise to the data did not decrease the performance of the transformers
    significantly.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变换器似乎非常容易对随机模式过拟合，因为向数据中添加噪声并没有显著降低变换器的性能。
- en: Longer lookback periods did not help with the accuracy, indicating that the
    transformers were unable to pick up on significant temporal patterns.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更长的回溯期并没有帮助提高准确性，表明变换器未能捕捉到显著的时间模式。
- en: Here Comes PatchTST
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这就是 PatchTST
- en: 'In an attempt to address the issues that transformers have in this space, Nie
    et. al [2] introduced two main mechanisms that differentiate PatchTST from prior
    models: Channel Independence and Patching.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对变换器在这一领域的问题，Nie 等 [2] 引入了两个主要机制，使 PatchTST 区别于以往模型：通道独立性和分块。
- en: In previous works, all target time series would be concatenated together into
    a matrix where each row of the matrix is a single series and the columns are the
    input tokens (one for each timestamp). These input tokens would then be projected
    into the embedding space, and these embeddings were passed into a single attention
    layer
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在以往的工作中，所有目标时间序列会被连接到一个矩阵中，其中每一行是一个单独的序列，列是输入标记（每个时间戳一个）。这些输入标记随后会被投射到嵌入空间，这些嵌入会传递到一个单一的注意力层。
- en: 'PatchTST instead opts for Channel Independence where each series is passed
    independently into the transformer backbone (Figure 1.b). This means that every
    series has its own set of attention weights, allowing the model to specialize
    better. This approach is commonly used in convolution networks and has been shown
    to significantly improve the accuracy of networks [2]. Channel independence also
    enables the use of the second mechanism: patching.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: PatchTST 选择了通道独立性，每个系列独立传入变换器骨干网（图 1.b）。这意味着每个系列都有自己的一组注意力权重，使得模型可以更好地专注。该方法通常用于卷积网络，并且已被证明能显著提高网络的准确性
    [2]。通道独立性还支持第二种机制：分块。
- en: '![](../Images/ed8f20b8e8f0138091c00826051e66f3.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed8f20b8e8f0138091c00826051e66f3.png)'
- en: 'Figure 1: PatchTST Model Overview (Figure from Nie et. al 2023 [2])'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：PatchTST 模型概览（图源自 Nie 等 2023 [2]）
- en: As mentioned before, attending on single time-steps is like attending on a single
    character. You end up losing a sense of order (“dog ”and “god” would have the
    same attention values with character-wise attention) and also increase the memory
    usage of the model at hand. So what’s the solution? We need to attend to words
    obviously!
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，关注单个时间步就像关注单个字符。你最终会失去顺序感（“dog”和“god”在字符级注意力下会有相同的注意力值），并且还会增加模型的内存使用。那么解决方案是什么？显然我们需要关注单词！
- en: Or more accurately, the authors propose to split each input time series up into
    fixed-length patches [2]. These patches are then passed through dedicated channels
    as the input tokens to the main model (the length of the patch is the token size)
    [2]. The model then adds a positional encoding to each patch and runs it through
    a vanilla transformer [3] encoder.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 更准确地说，作者提议将每个输入时间序列拆分成固定长度的块 [2]。这些块随后作为输入标记传入主模型（块的长度即为标记大小） [2]。模型接着为每个块添加位置编码，并通过普通变换器
    [3] 编码器处理。
- en: This approach naturally allows PatchTST to take local semantic information into
    account that is lost with point-wise tokens. Additionally segmenting the time
    series significantly reduces the number of input tokens needed, allowing the model
    to capture information from longer sequences and dramatically reducing the amount
    of memory needed to train and predict [2]. Additionally, the patching mechanism
    also makes it viable to do representational learning on time series, making PatchTST
    an even more flexible model [2].
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法自然允许 PatchTST 考虑到点级标记丢失的局部语义信息。此外，时间序列的分块显著减少了所需的输入标记数量，使得模型能够捕捉来自更长序列的信息，并显著减少了训练和预测所需的内存
    [2]。此外，分块机制还使得对时间序列进行表示学习成为可能，使 PatchTST 成为一个更灵活的模型 [2]。
- en: Forecasting Experimental Results
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测实验结果
- en: '![](../Images/79695be48ec9ef14bec75cd5add57850.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/79695be48ec9ef14bec75cd5add57850.png)'
- en: 'Figure 2: MSE & MAE results on benchmarks. First place is bolded, second place
    is underlined. (Figure from Nie et. al 2023)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：基准上的 MSE 和 MAE 结果。第一名加粗，第二名带下划线。（图源自 Nie 等 2023）
- en: 'In their paper, the authors test two different variations of PatchTST: one
    with 64 patches inputted into the model (hence the title) and one with 42 patches.
    The 42 patch variant has the same lookback window as the other models, therefore
    it can be thought of as a fair comparison to the other models. For both variants,
    a patch length of 16 and a stride of 8 were used to construct the input tokens
    [2]. As seen in Figure 2, the PatchTST variants dominate the results, with DLinear
    [1] winning in a very small number of cases. On average, PatchTST/64 achieved
    a 21% reduction in MSE and a 16.7% reduction in MAE. PatchTST/42 achieved a 20.2%
    reduction in MSE and a 16.4% reduction in MAE.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的论文中，作者测试了 PatchTST 的两种不同变体：一种是输入模型的 64 个 patch（因此得名），另一种是 42 个 patch。42
    个 patch 的变体与其他模型具有相同的回溯窗口，因此可以视为对其他模型的公平比较。对于这两种变体，使用了长度为 16 的 patch 和步幅为 8 来构建输入令牌
    [2]。如图 2 所示，PatchTST 变体在结果中占据主导地位，DLinear [1] 在极少数情况下获胜。平均而言，PatchTST/64 实现了 MSE
    下降 21% 和 MAE 下降 16.7%。PatchTST/42 实现了 MSE 下降 20.2% 和 MAE 下降 16.4%。
- en: Conclusion
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: PatchTST represents a promising future for Transformer architectures in the
    time-series forecasting task, especially as patching is a simple and effective
    operator that can easily be implemented in future models. Additionally, the accuracy
    success of PatchTST indicates that time-series forecasting is indeed a complex
    task that could benefit from capturing complex non-linear interactions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: PatchTST 代表了 Transformer 架构在时间序列预测任务中的一个有前景的未来，特别是由于 patching 是一个简单有效的操作符，可以在未来模型中轻松实现。此外，PatchTST
    的准确性成功表明，时间序列预测确实是一个复杂的任务，可能从捕捉复杂的非线性交互中获益。
- en: Resources and References
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源和参考文献
- en: 'PatchTST Github repository: [https://github.com/yuqinie98/PatchTST](https://github.com/yuqinie98/PatchTST)'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PatchTST Github 仓库：[https://github.com/yuqinie98/PatchTST](https://github.com/yuqinie98/PatchTST)
- en: 'An implementation of PatchTST can be found in NeuralForecast: [https://nixtla.github.io/neuralforecast/](https://nixtla.github.io/neuralforecast/)'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PatchTST 的实现可以在 NeuralForecast 中找到：[https://nixtla.github.io/neuralforecast/](https://nixtla.github.io/neuralforecast/)
- en: 'If you are interested in neural forecasting architectures that aren’t transformers,
    consider reading my previous article on Neural Basis Analysis Networks: [https://towardsdatascience.com/xai-for-forecasting-basis-expansion-17a16655b6e4](/xai-for-forecasting-basis-expansion-17a16655b6e4)'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你对非 Transformer 的神经预测架构感兴趣，可以考虑阅读我之前的文章《神经基础分析网络》：[https://towardsdatascience.com/xai-for-forecasting-basis-expansion-17a16655b6e4](/xai-for-forecasting-basis-expansion-17a16655b6e4)
- en: If you are interested in Forecasting, Deep Learning, and Explainable AI, consider
    supporting my writing by giving me a follow!
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对预测、深度学习和可解释人工智能感兴趣，可以通过关注我来支持我的写作！
- en: References
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] A. Zeng, M. Chen, L. Zhang, Q. Xu. [Are Transformers Effective for Time
    Series Forecasting?](https://arxiv.org/abs/2205.13504) (2022). Thirty-Seventh
    AAAI Conference on Artificial Intelligence.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] A. Zeng, M. Chen, L. Zhang, Q. Xu. [Transformers 对时间序列预测是否有效？](https://arxiv.org/abs/2205.13504)（2022）。第三十七届
    AAAI 人工智能会议。'
- en: '[2] Y. Nie, N. H. Nguyen, P. Sinthong, and J. Kalagnanam. A Time Series is
    Worth 64 Words: Long-term Forecasting with Transformers (2023). International
    Conference on Learning Representations, 2023.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Y. Nie, N. H. Nguyen, P. Sinthong 和 J. Kalagnanam. 《一个时间序列值64个单词：使用 Transformers
    的长期预测》（2023）。国际学习表征会议，2023。'
- en: '[3]A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez, L.
    Kaiser, I. Polosukhin. [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
    (2017). 31st Conference on Neural Information Processing Systems.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    L. Kaiser, I. Polosukhin. [注意力即你所需](https://arxiv.org/abs/1706.03762)（2017）。第
    31 届神经信息处理系统会议。'
