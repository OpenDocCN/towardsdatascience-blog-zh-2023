- en: Beyond Transformers with PyNeuraLogic
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越变压器：PyNeuraLogic的未来
- en: 原文：[https://towardsdatascience.com/beyond-transformers-with-pyneuralogic-10b70cdc5e45?source=collection_archive---------6-----------------------#2023-02-07](https://towardsdatascience.com/beyond-transformers-with-pyneuralogic-10b70cdc5e45?source=collection_archive---------6-----------------------#2023-02-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/beyond-transformers-with-pyneuralogic-10b70cdc5e45?source=collection_archive---------6-----------------------#2023-02-07](https://towardsdatascience.com/beyond-transformers-with-pyneuralogic-10b70cdc5e45?source=collection_archive---------6-----------------------#2023-02-07)
- en: TOWARDS [DEEP RELATIONAL LEARNING](https://medium.com/tag/deep-relational-learning)
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度关系学习的前行 [DEEP RELATIONAL LEARNING](https://medium.com/tag/deep-relational-learning)
- en: Demonstrating the power of neuro-symbolic programming
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 展示神经符号编程的力量
- en: '[](https://medium.com/@lukaszahradnik?source=post_page-----10b70cdc5e45--------------------------------)[![Lukáš
    Zahradník](../Images/fe12d5cebab663f1df4372203636a6e4.png)](https://medium.com/@lukaszahradnik?source=post_page-----10b70cdc5e45--------------------------------)[](https://towardsdatascience.com/?source=post_page-----10b70cdc5e45--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----10b70cdc5e45--------------------------------)
    [Lukáš Zahradník](https://medium.com/@lukaszahradnik?source=post_page-----10b70cdc5e45--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@lukaszahradnik?source=post_page-----10b70cdc5e45--------------------------------)[![Lukáš
    Zahradník](../Images/fe12d5cebab663f1df4372203636a6e4.png)](https://medium.com/@lukaszahradnik?source=post_page-----10b70cdc5e45--------------------------------)[](https://towardsdatascience.com/?source=post_page-----10b70cdc5e45--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----10b70cdc5e45--------------------------------)
    [Lukáš Zahradník](https://medium.com/@lukaszahradnik?source=post_page-----10b70cdc5e45--------------------------------)'
- en: ·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffcee2c9fb760&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-transformers-with-pyneuralogic-10b70cdc5e45&user=Luk%C3%A1%C5%A1+Zahradn%C3%ADk&userId=fcee2c9fb760&source=post_page-fcee2c9fb760----10b70cdc5e45---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----10b70cdc5e45--------------------------------)
    ·8 min read·Feb 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F10b70cdc5e45&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-transformers-with-pyneuralogic-10b70cdc5e45&user=Luk%C3%A1%C5%A1+Zahradn%C3%ADk&userId=fcee2c9fb760&source=-----10b70cdc5e45---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffcee2c9fb760&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-transformers-with-pyneuralogic-10b70cdc5e45&user=Luk%C3%A1%C5%A1+Zahradn%C3%ADk&userId=fcee2c9fb760&source=post_page-fcee2c9fb760----10b70cdc5e45---------------------post_header-----------)
    刊登于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----10b70cdc5e45--------------------------------)
    ·8 分钟阅读·2023 年 2 月 7 日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F10b70cdc5e45&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-transformers-with-pyneuralogic-10b70cdc5e45&user=Luk%C3%A1%C5%A1+Zahradn%C3%ADk&userId=fcee2c9fb760&source=-----10b70cdc5e45---------------------clap_footer-----------)'
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F10b70cdc5e45&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-transformers-with-pyneuralogic-10b70cdc5e45&source=-----10b70cdc5e45---------------------bookmark_footer-----------)![](../Images/10b9f75778c05a2122e8e1b51d3285a0.png)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F10b70cdc5e45&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-transformers-with-pyneuralogic-10b70cdc5e45&source=-----10b70cdc5e45---------------------bookmark_footer-----------)![](../Images/10b9f75778c05a2122e8e1b51d3285a0.png)'
- en: Visualization of the attention computation graph from the perspective of one
    token, with visible relationships between tokens. Image by the author.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个标记的角度可视化注意力计算图，展示了标记之间的可见关系。图片由作者提供。
- en: In the last few years, we have seen a rise of Transformer¹ based models with
    successful applications in many fields, such as Natural Language Processing or
    Computer Vision. In this article, we will explore a concise, explainable, and
    extendable way to express deep learning models, specifically transformers, as
    a hybrid architecture, i.e., via marrying deep learning with symbolic artificial
    intelligence. To do so, we will implement models in a Python neuro-symbolic framework
    called [PyNeuraLogic](https://github.com/LukasZahradnik/PyNeuraLogic/) *(the author
    is a co-author of the framework)*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，我们看到基于Transformer¹的模型在许多领域取得了成功应用，如自然语言处理或计算机视觉。本文将探讨一种简洁、可解释且可扩展的方式来表达深度学习模型，特别是transformers，作为一种混合架构，即将深度学习与符号人工智能结合。为此，我们将在一个名为[PyNeuraLogic](https://github.com/LukasZahradnik/PyNeuraLogic/)的Python神经符号框架中实现这些模型*（作者是该框架的共同作者）*。
- en: “We cannot construct rich cognitive models in an adequate, automated way without
    the triumvirate of hybrid architecture, rich prior knowledge, and sophisticated
    techniques for reasoning.”
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我们不能在没有混合架构、丰富的先验知识和复杂的推理技术三者结合的情况下，以充分自动化的方式构建丰富的认知模型。”
- en: ''
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Gary Marcus²
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: — Gary Marcus²
- en: Combining symbolic representation with deep learning fills the gaps in the current
    deep learning models, such as out-of-the-box explainability or missing techniques
    for reasoning. Maybe, raising the number of parameters is not the soundest approach
    to achieving these desired results, just like increasing the number of camera
    megapixels does not necessarily yield better photos.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 将符号表示与深度学习结合起来，填补了当前深度学习模型中的空白，例如开箱即用的可解释性或缺乏推理技术。也许，提高参数数量并不是实现这些期望结果的最佳方法，就像增加相机的像素数量并不一定能拍出更好的照片一样。
- en: '![](../Images/680dc9c97005d1c4701ef2d0c54e5636.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/680dc9c97005d1c4701ef2d0c54e5636.png)'
- en: High-level visualization of the neuro-symbolic concept Lifted Relational Neural
    Networks³ (LRNN), which (Py)NeuraLogic implements. Here we show a simple template
    (logic program) with one linear layer followed by a sum aggregation. For each
    (input) sample a unique neural network is constructed. Image by the author.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 由PyNeuraLogic实现的神经符号概念Lifted Relational Neural Networks³ (LRNN)的高级可视化。在这里，我们展示了一个简单的模板（逻辑程序），其中包括一个线性层，后跟一个求和聚合。对于每个（输入）样本，都会构建一个独特的神经网络。图片由作者提供。
- en: The PyNeuraLogic framework is based on logic programming with a twist — logic
    programs hold differentiable parameters. The framework is well-suited for smaller
    structured data, such as molecules, and complex models, such as Transformers and
    Graph Neural Networks. On the other hand, PyNeuraLogic is not the best choice
    for non-relational and large tensor data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: PyNeuraLogic框架基于逻辑编程，但有所不同——逻辑程序包含可微分的参数。该框架非常适合较小的结构化数据，如分子，以及复杂模型，如Transformers和图神经网络。另一方面，PyNeuraLogic并不是处理非关系型和大张量数据的最佳选择。
- en: The key component of the framework is a differentiable logic program that we
    refer to as a template. A template consists of logic rules that define the structure
    of neural networks in an abstract way — we can think of a template as a blueprint
    of the model’s architecture. The template is then applied to each input data instance
    to produce (via grounding and neuralization) a neural network unique to the input
    sample. This process is entirely different from other frameworks with predefined
    architectures that cannot adjust themselves to different input samples. For a
    bit closer introduction to the framework, you can see, e.g., a [previous article](/beyond-graph-neural-networks-with-pyneuralogic-c1e6502c46f7)
    on PyNeuralogic from the perspective of Graph Neural Networks.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 该框架的关键组件是一个可微分的逻辑程序，我们称之为模板。模板由定义神经网络结构的逻辑规则组成——我们可以将模板视为模型架构的蓝图。然后将模板应用于每个输入数据实例，通过生成和神经化过程产生一个对输入样本独特的神经网络。这一过程与其他具有预定义架构的框架完全不同，后者无法根据不同的输入样本进行调整。有关框架的更详细介绍，可以参考，例如，从图神经网络的角度出发的[上一篇文章](/beyond-graph-neural-networks-with-pyneuralogic-c1e6502c46f7)。
- en: Symbolic Transformers
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 符号Transformers
- en: '![](../Images/bd0bd53131df7c150f1d66c176f9ed6d.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd0bd53131df7c150f1d66c176f9ed6d.png)'
- en: The Transformer architecture consists of two blocks — encoder (left) and decoder
    (right). Both blocks share similarities — the decoder is an extended encoder;
    therefore, we will focus only on the encoder, as the decoder implementation is
    analogous. Image by the author, inspired by [1].
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器架构由两个模块组成——编码器（左）和解码器（右）。这两个模块有相似之处——解码器是扩展的编码器，因此我们只关注编码器，因为解码器的实现类似。图像由作者提供，灵感来自于
    [1]。
- en: We generally tend to implement deep learning models as tensor operations over
    input tokens batched into one large tensor. This makes sense because deep learning
    frameworks and hardware (e.g., GPUs) are typically optimized for processing larger
    tensors instead of multiple ones of diverse shapes and sizes. Transformers are
    no exception, and it is common to batch individual token vector representations
    into one large matrix and represent the model as operations over such matrices.
    Nevertheless, such implementations hide how individual input tokens relate to
    each other, as can be demonstrated in Transformer’s attention mechanism.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常倾向于将深度学习模型实现为对批处理成一个大张量的输入标记进行的张量操作。这是有道理的，因为深度学习框架和硬件（如 GPUs）通常优化为处理较大的张量，而不是多个形状和大小各异的小张量。变换器（Transformers）也不例外，将单个标记向量表示批处理成一个大矩阵，并将模型表示为对这些矩阵的操作。然而，这种实现方式隐藏了单个输入标记之间的关系，这可以通过变换器的注意力机制来演示。
- en: The Attention Mechanism
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意力机制
- en: The attention mechanism forms the very core of all the Transformer models. Specifically,
    its classic version makes use of a so-called multi-head scaled dot-product attention.
    Let us decompose the scaled dot-product attention with one head (for clarity)
    into a simple logic program.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制构成了所有变换器模型的核心。具体来说，它的经典版本使用了所谓的多头缩放点积注意力。让我们将一个头的缩放点积注意力分解成一个简单的逻辑程序（为了清晰起见）。
- en: '![](../Images/75505fc4973eb9f9d900b57bcf55a17d.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75505fc4973eb9f9d900b57bcf55a17d.png)'
- en: The scaled dot product attention equation
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放点积注意力方程
- en: The purpose of the attention is to decide what parts of the input the network
    should focus on. The attention does that by computing a weighted sum of the values
    *V*, where the weights represent the compatibility of the input keys *K* and queries
    *Q*. In this specific version, the weights are computed by the softmax function
    of the dot product of queries *Q* and keys *K*, divided by the square root of
    the input feature vector dimensionality *d_k*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制的目的是决定网络应该关注输入的哪些部分。注意力机制通过计算值 *V* 的加权和来实现这一点，其中权重表示输入键 *K* 和查询 *Q* 的兼容性。在这个特定版本中，权重是通过查询
    *Q* 和键 *K* 的点积的 softmax 函数计算的，并除以输入特征向量维度 *d_k* 的平方根。
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In PyNeuraLogic, we can fully capture the attention mechanism with the above
    logical rules. The first rule expresses the computation of the weights — it calculates
    the product of the inverse square root of dimensionality with a transposed *j*-th
    key vector and *i*-th query vector. Then we aggregate all the results for a given
    *i* and all possible *j*’s with softmax.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyNeuraLogic 中，我们可以通过上述逻辑规则完全捕捉注意力机制。第一条规则表示权重的计算——它计算维度的逆平方根与转置的 *j* 第 *j*
    个键向量和 *i* 第 *i* 个查询向量的乘积。然后我们对给定的 *i* 和所有可能的 *j* 使用 softmax 聚合所有结果。
- en: The second rule then calculates a product between this weight vector and the
    corresponding *j*-th value vector and sums up the results across different *j*’s
    for each respective *i*-th token.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 第二条规则计算了这个权重向量与相应的 *j* 第 *j* 个值向量之间的乘积，并对每个各自的 *i* 第 *i* 个标记的不同 *j* 的结果进行求和。
- en: Attention Masking
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注意力掩码
- en: During the training and evaluation, we usually limit what input tokens can attend
    to. For example, we want to restrict tokens from looking ahead and attending to
    upcoming words. Popular frameworks, such as PyTorch, implement this via masking,
    that is, by setting a subset of elements of the scaled dot-product result to some
    very low negative number. Those numbers enforce the softmax function to assign
    zero as the weight for the corresponding token pair.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练和评估过程中，我们通常限制输入标记可以关注的内容。例如，我们想限制标记向前看并关注即将出现的单词。流行框架（如 PyTorch）通过掩码实现这一点，即通过将缩放点积结果的某些子集设置为非常低的负数来实现。这些数字强制
    softmax 函数为相应的标记对分配零作为权重。
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: With our symbolic representation, we can implement this by simply adding one
    body relation serving as a constraint. When calculating the weights, we restrict
    the *j* index to be less than or equal to the *i* index. In contrast to the masking,
    we compute only the needed scaled dot products.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的符号表示，我们可以通过简单地添加一个作为约束的主体关系来实现这一点。在计算权重时，我们限制 *j* 索引小于或等于 *i* 索引。与掩码不同，我们仅计算所需的缩放点积。
- en: '![](../Images/2fc97073fac58ff837b52cea92fc9e62.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2fc97073fac58ff837b52cea92fc9e62.png)'
- en: Regular deep learning frameworks constrain the attention via masking (on the
    left). First, the whole QK^T matrix is calculated, then the values are masked
    by overriding with low values (white crossed cells) to simulate attending only
    to the relevant tokens (blue cells). In PyNeuraLogic, we compute only needed scalar
    values by applying a symbolic constraint (on the right) — hence there are no redundant
    calculations. This benefit is even more significant in the following attention
    versions. Image by the author.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 常规深度学习框架通过掩码来限制注意力（左侧）。首先，计算整个 QK^T 矩阵，然后通过用低值（白色交叉单元）覆盖值来掩盖，从而模拟仅关注相关的标记（蓝色单元）。在
    PyNeuraLogic 中，我们通过应用符号约束（右侧）仅计算所需的标量值——因此没有冗余计算。这个好处在随后的注意力版本中更加显著。图片由作者提供。
- en: Beyond standard Attention aggregation
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超越标准注意力聚合
- en: 'Of course, the symbolic “masking” can be completely arbitrary. Most of us heard
    of the *GPT-3⁴* (or its applications, such as *ChatGPT*), based on Sparse Transformers.⁵
    The Sparse Transformer’s attention (the strided version) has two types of attention
    heads:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，符号“掩码”可以是完全任意的。我们中的大多数人听说过 *GPT-3⁴*（或其应用，如 *ChatGPT*），基于稀疏 Transformer。稀疏
    Transformer 的注意力（分段版本）有两种类型的注意力头：
- en: One that attends only to previous *n* tokens *(0* ≤ *i* − *j* ≤ *n)*
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种只关注前 *n* 个标记 *(0* ≤ *i* − *j* ≤ *n*)
- en: One that attends only to every *n*-th previous token (*(i* − *j) % n = 0*)
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种只关注每 *n* 个前一个标记（*(i* − *j) % n = 0*）
- en: The implementations of both types of heads require again only minor changes
    (e.g., for *n = 5*).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 两种类型的头的实现再次只需少量更改（例如，对于 *n = 5*）。
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/eafd5be4a52a8c3619b31c01893e4720.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eafd5be4a52a8c3619b31c01893e4720.png)'
- en: The Relational Attention equations
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 关系注意力方程
- en: We can go even further and generalize the attention for graph-like (relational)
    inputs, just like in Relational Attention.⁶ This type of attention operates on
    graphs, where nodes attend only to their neighbors (nodes connected by an edge).
    Queries *Q*, keys *K*, and values *V* are then edge embeddings summed with node
    vector embeddings.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进一步推广图形（关系）输入的注意力机制，就像在关系注意力中一样。这种类型的注意力机制在图上操作，其中节点仅关注其邻居（由边连接的节点）。查询 *Q*、键
    *K* 和值 *V* 然后是边的嵌入，与节点向量嵌入相加。
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This type of attention is, in our case, again almost the same as the previously
    shown scaled dot-product attention. The only difference is the addition of extra
    terms to capture the edges. Feeding a graph as input into the attention mechanism
    seems quite natural, which is not entirely surprising, considering that the Transformer
    is a type of [Graph Neural Network](https://medium.com/towards-data-science/beyond-graph-neural-networks-with-pyneuralogic-c1e6502c46f7),
    acting on fully-connected graphs (when no masking is applied). In the traditional
    tensor representation, this is not that obvious.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，这种类型的注意力几乎与之前展示的缩放点积注意力相同。唯一的区别是增加了额外的项来捕捉边。将图作为输入馈送到注意力机制中似乎相当自然，这也并不令人惊讶，考虑到
    Transformer 是一种[图神经网络](https://medium.com/towards-data-science/beyond-graph-neural-networks-with-pyneuralogic-c1e6502c46f7)，作用于全连接图（当没有应用掩码时）。在传统的张量表示中，这并不那么明显。
- en: The Transformer Encoder
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer 编码器
- en: Now, when we showcased the implementation of the Attention mechanism, the missing
    pieces to construct an entire transformer encoder block are relatively straightforward.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们展示注意力机制的实现时，构建整个 transformer 编码器块的缺失部分相对简单。
- en: Embeddings
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入
- en: We have already seen in the Relational Attention how one can implement embeddings.
    For the traditional Transformer, the embeddings will be pretty similar. We project
    the input vector into three embedding vectors — keys, queries, and values.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在关系注意力中看到如何实现嵌入。对于传统的 Transformer，嵌入会非常相似。我们将输入向量投影到三个嵌入向量——键、查询和值。
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Skip connections, Normalization, and Feed-forward Network
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跳跃连接、归一化和前馈网络
- en: Query embeddings are summed with the attention’s output via a skip connection.
    The resulting vector is then normalized and passed into a multilayer perceptron
    (MLP).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 查询嵌入通过跳跃连接与注意力的输出相加。得到的向量随后被归一化并传递到多层感知机（MLP）。
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: For the MLP, we will implement a fully connected neural network with two hidden
    layers, which can be elegantly expressed as one logic rule.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 MLP，我们将实现一个具有两个隐藏层的全连接神经网络，这可以优雅地表示为一个逻辑规则。
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The last skip connection with normalization is then identical to the previous
    one.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一层跳跃连接的归一化与前一层相同。
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Putting it all together
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将一切结合起来
- en: We have built all the necessary parts to construct a Transformer encoder. The
    decoder utilizes the same components; therefore, its implementation would be analogous.
    Let us combine all the blocks into one differentiable logic program that can be
    embedded into a Python script and compiled into Neural Networks with PyNeuraLogic.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经构建了所有必要的部分来构造一个变换器编码器。解码器利用相同的组件，因此，其实现也会类似。让我们将所有模块组合成一个可微分的逻辑程序，可以嵌入到
    Python 脚本中，并使用 PyNeuraLogic 编译为神经网络。
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Conclusion
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we analysed the Transformer architecture and demonstrated its
    implementation in a neuro-symbolic framework called [PyNeuraLogic](https://github.com/LukasZahradnik/PyNeuraLogic/).
    Via this approach, we were able to implement various types of Transformers with
    only minor changes in the code, illustrating how everyone can quickly pivot and
    develop novel Transformer architectures. It also points out the unmistakable resemblance
    of various versions of Transformers, and of Transformers with GNNs.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们分析了变换器架构，并展示了其在一个称为 [PyNeuraLogic](https://github.com/LukasZahradnik/PyNeuraLogic/)
    的神经符号框架中的实现。通过这种方法，我们能够仅通过对代码进行微小的更改来实现各种类型的变换器，展示了每个人都可以快速调整并开发新颖的变换器架构。它还指出了各种版本的变换器和变换器与图神经网络之间的明显相似性。
- en: '[1]: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,
    A., Kaiser, L., & Polosukhin, I.. (2017). Attention Is All You Need.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,
    A., Kaiser, L., & Polosukhin, I.. (2017). 注意力机制即你所需的一切。'
- en: '[2]: Marcus, G.. (2020). The Next Decade in AI: Four Steps Towards Robust Artificial
    Intelligence.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[2]: Marcus, G.. (2020). 人工智能的下一个十年：迈向强健人工智能的四个步骤。'
- en: '[3]: Gustav Šourek, Filip Železný, & Ondřej Kuželka (2021). Beyond graph neural
    networks with lifted relational neural networks*. Machine Learning, 110(7), 1695–1738.*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[3]: Gustav Šourek, Filip Železný, & Ondřej Kuželka (2021). 超越图神经网络：提升关系神经网络*。机器学习,
    110(7), 1695–1738*。'
- en: '[4]: Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
    Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss,
    A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter,
    C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J.,
    Berner, C., McCandlish, S., Radford, A., Sutskever, I., & Amodei, D.. (2020).
    Language Models are Few-Shot Learners.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[4]: Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
    Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss,
    A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter,
    C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J.,
    Berner, C., McCandlish, S., Radford, A., Sutskever, I., & Amodei, D.. (2020).
    语言模型是少样本学习者。'
- en: '[5]: Child, R., Gray, S., Radford, A., & Sutskever, I.. (2019). Generating
    Long Sequences with Sparse Transformers.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[5]: Child, R., Gray, S., Radford, A., & Sutskever, I.. (2019). 生成稀疏变换器的长序列。'
- en: '[6]: Diao, C., & Loynd, R.. (2022). Relational Attention: Generalizing Transformers
    for Graph-Structured Tasks.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[6]: Diao, C., & Loynd, R.. (2022). 关系注意力：为图结构任务推广变换器。'
- en: '*The author would like to thank* [*Gustav Šír*](https://medium.com/@sir.gustav)
    *for proofreading this article and giving valuable feedback. If you want to learn
    more about combining logic with deep learning, head to* [*Gustav’s article series*](https://medium.com/tag/deep-relational-learning)*.*'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*作者感谢* [*Gustav Šír*](https://medium.com/@sir.gustav) *校对了本文并提供了宝贵的反馈。如果你想了解更多关于逻辑与深度学习结合的内容，请查看*
    [*Gustav 的文章系列*](https://medium.com/tag/deep-relational-learning)*。*'
