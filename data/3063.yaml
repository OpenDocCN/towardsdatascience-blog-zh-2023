- en: How Does PPO With Clipping Work?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PPO带裁剪的工作原理是什么？
- en: 原文：[https://towardsdatascience.com/how-does-ppo-with-clipping-work-eff71a7a974a?source=collection_archive---------8-----------------------#2023-10-07](https://towardsdatascience.com/how-does-ppo-with-clipping-work-eff71a7a974a?source=collection_archive---------8-----------------------#2023-10-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-does-ppo-with-clipping-work-eff71a7a974a?source=collection_archive---------8-----------------------#2023-10-07](https://towardsdatascience.com/how-does-ppo-with-clipping-work-eff71a7a974a?source=collection_archive---------8-----------------------#2023-10-07)
- en: Intuition + math + code, for practitioners
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 直觉 + 数学 + 代码，为从业者
- en: '[](https://medium.com/@byjameskoh?source=post_page-----eff71a7a974a--------------------------------)[![James
    Koh, PhD](../Images/8e7af8b567cdcf24805754801683b426.png)](https://medium.com/@byjameskoh?source=post_page-----eff71a7a974a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----eff71a7a974a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----eff71a7a974a--------------------------------)
    [James Koh, PhD](https://medium.com/@byjameskoh?source=post_page-----eff71a7a974a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@byjameskoh?source=post_page-----eff71a7a974a--------------------------------)[![James
    Koh, PhD](../Images/8e7af8b567cdcf24805754801683b426.png)](https://medium.com/@byjameskoh?source=post_page-----eff71a7a974a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----eff71a7a974a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----eff71a7a974a--------------------------------)
    [James Koh, PhD](https://medium.com/@byjameskoh?source=post_page-----eff71a7a974a--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F780706b02d58&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-does-ppo-with-clipping-work-eff71a7a974a&user=James+Koh%2C+PhD&userId=780706b02d58&source=post_page-780706b02d58----eff71a7a974a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----eff71a7a974a--------------------------------)
    ·9 min read·Oct 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Feff71a7a974a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-does-ppo-with-clipping-work-eff71a7a974a&user=James+Koh%2C+PhD&userId=780706b02d58&source=-----eff71a7a974a---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F780706b02d58&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-does-ppo-with-clipping-work-eff71a7a974a&user=James+Koh%2C+PhD&userId=780706b02d58&source=post_page-780706b02d58----eff71a7a974a---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----eff71a7a974a--------------------------------)
    · 9分钟阅读 · 2023年10月7日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Feff71a7a974a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-does-ppo-with-clipping-work-eff71a7a974a&user=James+Koh%2C+PhD&userId=780706b02d58&source=-----eff71a7a974a---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feff71a7a974a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-does-ppo-with-clipping-work-eff71a7a974a&source=-----eff71a7a974a---------------------bookmark_footer-----------)![](../Images/847c27062f8d23472ca40f8a61a45a03.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feff71a7a974a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-does-ppo-with-clipping-work-eff71a7a974a&source=-----eff71a7a974a---------------------bookmark_footer-----------)![](../Images/847c27062f8d23472ca40f8a61a45a03.png)'
- en: Photo by [Tamanna Rumee](https://unsplash.com/@tamanna_rumee?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Tamanna Rumee](https://unsplash.com/@tamanna_rumee?utm_source=medium&utm_medium=referral)
    提供，发布于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In Reinforcement Learning, Proximal Policy Optimization (PPO) is often cited
    as the example for a policy approach, as compared to DQN (a value-based approach)
    and the big family of actor-critic methods which includes TD3 and SAC.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，**近端策略优化（PPO）**常被作为策略方法的例子，与DQN（基于价值的方法）和包括TD3和SAC在内的演员-评论家方法大家族相比。
- en: I recalled some time ago, when I was learning it for the first time, I was left
    unconvinced. Many teachers adopt a sort of hand-wavy approach. I don’t buy that,
    and neither should you.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我回忆起不久前，当我第一次学习它时，我并不信服。许多教师采用了一种敷衍的方式。我不认同这种方法，你也不应如此。
- en: In this article, I will attempt to explain how PPO works, supporting the math
    with both intuition and code. You can try different scenarios, and see for yourself
    that it works not just in principle but also in practice, and that there is no
    cherry picking.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将尝试解释PPO的工作原理，结合直观和代码来支持数学原理。你可以尝试不同的场景，亲自验证它不仅在理论上有效，也在实践中有效，并且没有选择性偏差。
- en: Why Bother?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么要费心？
- en: PPO and the other SOTA models can be implemented in minutes using stable-baselines3
    (sb3). Anyone following the documentation can get it running, without knowledge
    of the underlying model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: PPO和其他SOTA模型可以使用stable-baselines3（sb3）在几分钟内实现。任何遵循文档的人都可以运行它，而无需了解底层模型。
- en: However, whether you are a practitioner or a theorist, fundamentals do matter.
    If you simply treat PPO (or any model for that matter) as a black box, how do
    you expect your users to have faith in what you deliver?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，无论你是从业者还是理论家，基础知识都是重要的。如果你只是把PPO（或任何模型）当作一个黑箱，你怎么能期望你的用户对你提供的结果有信心呢？
- en: I will do a detailed code walkthrough later this month, writing a wrapper such
    that any…
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在本月稍晚时进行详细的代码演示，编写一个包装器，以便任何人……
