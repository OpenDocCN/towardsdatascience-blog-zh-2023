- en: What Is Partial Information Decomposition and How Features Interact
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是部分信息分解及特征如何交互
- en: 原文：[https://towardsdatascience.com/what-is-partial-information-decomposition-and-how-features-interact-a713456a1029?source=collection_archive---------3-----------------------#2023-12-08](https://towardsdatascience.com/what-is-partial-information-decomposition-and-how-features-interact-a713456a1029?source=collection_archive---------3-----------------------#2023-12-08)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/what-is-partial-information-decomposition-and-how-features-interact-a713456a1029?source=collection_archive---------3-----------------------#2023-12-08](https://towardsdatascience.com/what-is-partial-information-decomposition-and-how-features-interact-a713456a1029?source=collection_archive---------3-----------------------#2023-12-08)
- en: How information about a target variable is distributed across its multiple features
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于目标变量的信息如何在其多个特征中分布
- en: '[](https://medium.com/@rodrigopesilva?source=post_page-----a713456a1029--------------------------------)[![Rodrigo
    Silva](../Images/d260f05ed9887c5072e0590db1481be2.png)](https://medium.com/@rodrigopesilva?source=post_page-----a713456a1029--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a713456a1029--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a713456a1029--------------------------------)
    [Rodrigo Silva](https://medium.com/@rodrigopesilva?source=post_page-----a713456a1029--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](https://medium.com/@rodrigopesilva?source=post_page-----a713456a1029--------------------------------)](https://medium.com/@rodrigopesilva?source=post_page-----a713456a1029--------------------------------)
    [![Rodrigo Silva](../Images/d260f05ed9887c5072e0590db1481be2.png)](https://medium.com/@rodrigopesilva?source=post_page-----a713456a1029--------------------------------)
    [![](https://towardsdatascience.com/?source=post_page-----a713456a1029--------------------------------)](https://towardsdatascience.com/?source=post_page-----a713456a1029--------------------------------)
    [![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a713456a1029--------------------------------)
    [Rodrigo Silva](https://medium.com/@rodrigopesilva?source=post_page-----a713456a1029--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F222e82adf972&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-partial-information-decomposition-and-how-features-interact-a713456a1029&user=Rodrigo+Silva&userId=222e82adf972&source=post_page-222e82adf972----a713456a1029---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a713456a1029--------------------------------)
    ·10 min read·Dec 8, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa713456a1029&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-partial-information-decomposition-and-how-features-interact-a713456a1029&user=Rodrigo+Silva&userId=222e82adf972&source=-----a713456a1029---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F222e82adf972&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-partial-information-decomposition-and-how-features-interact-a713456a1029&user=Rodrigo+Silva&userId=222e82adf972&source=post_page-222e82adf972----a713456a1029---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a713456a1029--------------------------------)
    · 10 分钟阅读 · 2023年12月8日 [![](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa713456a1029&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-partial-information-decomposition-and-how-features-interact-a713456a1029&user=Rodrigo+Silva&userId=222e82adf972&source=-----a713456a1029---------------------clap_footer-----------)]'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa713456a1029&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-partial-information-decomposition-and-how-features-interact-a713456a1029&source=-----a713456a1029---------------------bookmark_footer-----------)![](../Images/1b6c41b4f07a4b381d9da086ab84a80c.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa713456a1029&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-partial-information-decomposition-and-how-features-interact-a713456a1029&source=-----a713456a1029---------------------bookmark_footer-----------)
    ![](../Images/1b6c41b4f07a4b381d9da086ab84a80c.png)'
- en: Photo by Alina Grubnyak, via [Unsplash](https://unsplash.com/photos/low-angle-photography-of-metal-structure-ZiQkhI7417A).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：Alina Grubnyak，来自 [Unsplash](https://unsplash.com/photos/low-angle-photography-of-metal-structure-ZiQkhI7417A)。
- en: When a target variable is influenced by multiple sources of information, it
    is crucial (and yet not trivial) to understand how each source contributes to
    the overall information provided.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当目标变量受到多个信息来源的影响时，理解每个来源对整体信息的贡献是至关重要的（虽然这并不简单）。
- en: In this article I'll start with the basic concept of surprise, then I'll proceed
    to explain how entropy consists of the average amount of surprise distributed
    over a random variable, and this gives us the conditions to define mutual information.
    After this, I talk about partial information decomposition for cases where we
    have multiple sources of information.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将从惊讶的基本概念开始，然后解释熵如何由分布在随机变量上的平均惊讶量组成，这为我们定义互信息提供了条件。接下来，我将讨论在信息来源多个的情况下的部分信息分解。
- en: Surprise and Entropy
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 惊讶与熵
- en: 'Maybe one of the most intuitive ways of defining Entropy from an Information
    standpoint is to first talk about *surprise*. The measure of surprise works just
    as how we expect: less probable events are more surprising, while more probable
    events are less surprising. The mathematical definition that encompasses these
    properties is the one shown below:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 也许从信息的角度定义熵最直观的方法是首先谈谈*惊讶*。惊讶的度量正如我们所期望的那样：不太可能发生的事件更令人惊讶，而更可能发生的事件则不那么令人惊讶。涵盖这些属性的数学定义如下：
- en: '![](../Images/504cc18591fa38a92ef3cbae196f492a.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/504cc18591fa38a92ef3cbae196f492a.png)'
- en: We can see by the graph in Figure 1 that this definition is pretty related to
    the properties we talked about. When some event has a high chance of happening
    (p closer to 1), then the surprise is close to zero. On the other hand, if an
    event has a very low probability of happening, its surprise gets arbitrarily large.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 从图1中的图表可以看出，这一定义与我们讨论的属性非常相关。当某些事件发生的概率很高（p 接近 1）时，惊讶度接近零。另一方面，如果事件发生的概率非常低，则其惊讶度会变得任意大。
- en: '![](../Images/30eaeac06aa5aba94f96381493b2d47c.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30eaeac06aa5aba94f96381493b2d47c.png)'
- en: 'Figure 1: The graph of surprise. Image by author.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：惊讶的图示。作者提供的图像。
- en: 'Now, what does entropy have to do with surprise? Well, entropy is the average
    surprise over all the values of a random variable. Therefore, if we have some
    random variable X, and the set of all possible outcomes of X is called A_X (we
    call it the "alphabet of X"), then entropy H is defined as:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，熵与惊讶有什么关系呢？熵是随机变量所有值的平均惊讶。因此，如果我们有一个随机变量 X，且 X 的所有可能结果的集合称为 A_X（我们称之为 X 的“字母表”），那么熵
    H 定义为：
- en: '![](../Images/fe145a377dca1c49c7254829cdcbf2cd.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fe145a377dca1c49c7254829cdcbf2cd.png)'
- en: 'Great. Now we tied up entropy with surprise, we can understand one useful interpretation
    of entropy:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 很好。现在我们将熵与惊讶联系起来，可以理解熵的一个有用解释：
- en: Entropy is a measure of ignorance.
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 熵是无知的度量。
- en: 'How can this be? I will explain it with a silly example. Imagine that you have
    to take a final physics exam. Within the language we have developed so far, we
    can consider the test as a random variable with some alphabet of possible questions.
    Suppose now two scenarios:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这怎么可能呢？我将通过一个简单的例子来解释。假设你需要参加一个期末物理考试。在我们目前所发展的语言中，我们可以将考试视为具有某些可能问题字母表的随机变量。现在假设两个场景：
- en: You studied hard for this exam and you know what kind of questions will be in
    the exam, so *on average*, you will not be so surprised by your exam.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你为这次考试努力学习，你知道考试中会出现什么样的问题，所以*平均来说*，你不会对考试感到太惊讶。
- en: You didn't really study and you don't know which type of question will be in
    the exam, so your level of surprise will be pretty high throughout the exam.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你没有认真学习，也不知道考试中会出现什么类型的问题，所以你在考试中的惊讶度会相当高。
- en: So when your average surprise is bigger coincides perfectly with the scenario
    where you don't have as much information.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，当你的平均惊讶度更大时，正好与信息较少的情况相吻合。
- en: Speaking from a technical standpoint, more peaked distributions (e.g. distributions
    where certain values are more likely to happen than others) have a lower entropy
    than more dispersed ones, where every event has about the same chance of happening.
    That is why we say that the distribution with the highest entropy is the uniform
    distribution, where any value can happen with the same chance.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术角度来看，更尖峰的分布（例如某些值比其他值更可能发生的分布）具有比更分散的分布更低的熵，而后者中的每个事件发生的概率大致相同。这就是为什么我们说熵最高的分布是均匀分布，在这种分布中，任何值发生的机会都相同。
- en: Entropy and (Mutual) Information
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 熵和（互）信息
- en: Now that we have established a measure of average surprise on a system described
    by a random variable (this is the entropy), we can create the link of entropy
    with information.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经建立了一个由随机变量描述的系统的平均惊讶度量（即熵），我们可以将熵与信息建立联系。
- en: 'Since entropy is a measure of ignorance over some system, the lack of it represents…
    *information*. In this sense, it is pretty natural to create a measure called
    mutual information: it measures the information you gain once you know some information
    about the system:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 由于熵是对某个系统的无知的度量，它的缺乏代表着… *信息*。从这个意义上说，创建一个叫做互信息的度量是很自然的：它衡量的是当你知道系统的一些信息后，你获得的信息量。
- en: '![](../Images/c1a89b1c6cd1d70018c7e024903efd3f.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c1a89b1c6cd1d70018c7e024903efd3f.png)'
- en: 'This definition says: take the average surprise of a random variable X, then
    take the average surprise of the random variable X, but now consider that we know
    the outcome of another random variable Y. Subtract the former by the latter, and
    you know how much *ignorance* you removed from your system X by knowing about
    Y.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定义是：取随机变量 X 的平均惊讶度，然后取随机变量 X 的平均惊讶度，但现在考虑我们知道另一个随机变量 Y 的结果。将前者减去后者，你就知道通过了解
    Y，你从系统 X 中移除了多少 *无知*。
- en: 'Let''s come back to our silly example: suppose you don''t know what questions
    will be asked within your test, and this is X. Now suppose that a friend of yours
    has made a test from the same teacher, about the same subject, one week before
    your test. He tells you everything that *his* test covered (which happens to be
    another random variable Y). The most plausible to say is that your ignorance from
    your test has dropped, which means your test X and your friend''s test Y share
    information.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们愚蠢的例子：假设你不知道考试中会问哪些问题，这就是 X。现在假设你的一个朋友在你考试前一周做了一个来自同一老师、关于同一科目的测试。他告诉你他的测试涵盖了哪些内容（这恰好是另一个随机变量
    Y）。最可能的情况是，你对自己考试的无知减少了，这意味着你的测试 X 和你朋友的测试 Y 共享信息。
- en: In Figure 2 there is a nice, comprehensible Venn Diagram showing the relation
    between the entropies and the information shared between a pair of variables X
    and Y.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2 中有一个清晰易懂的维恩图，展示了两个变量 X 和 Y 之间的熵和共享信息的关系。
- en: '![](../Images/0014751e4795774787d549300583f9c7.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0014751e4795774787d549300583f9c7.png)'
- en: 'Figure 2: Mutual Information scheme. Image by author, heavily inspired by many
    others.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：互信息方案。图像由作者提供，受到许多其他图像的启发。
- en: But what if we have multiple sources of information?
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 但如果我们有多个信息来源呢？
- en: So far we have only talked about cases where we have one feature X and one target
    variable Y, but it is quite obvious that this does not generalize well. Hence,
    now imagine we have a random variable Y (say, a target variable from a classification
    model) and we want to know the amount of information provided by each one of the
    n features of the model X_1, X_2, …, X_n. One could say that it suffices to calculate
    the mutual information shared by X_1 and Y, then by X_2 and Y, and so on. Well,
    in the real world, our features can interact among them and create non-trivial
    relations, and if we want to have a consistent framework we have to take these
    interactions into account.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只讨论了一个特征 X 和一个目标变量 Y 的情况，但显然这并不具有良好的普遍性。因此，现在设想我们有一个随机变量 Y（比如一个分类模型中的目标变量），我们想知道模型中每个特征
    X_1, X_2, …, X_n 提供的信息量。可以说，只需计算 X_1 和 Y 之间的互信息，然后是 X_2 和 Y 之间的互信息，依此类推。然而，在现实世界中，我们的特征可能会相互作用并创建非平凡的关系，如果我们想要一个一致的框架，就必须考虑这些相互作用。
- en: 'Let''s take the case where we have two input signals X_1 and X_2, and we want
    to quantify the mutual information between these two features and a target feature
    Y. That is, we want to calculate I(Y; {X_1, X_2}). The Partial Information Decomposition
    framework states that this information can be divided into four non-negative components:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个有两个输入信号 X_1 和 X_2 的情况，我们想要量化这两个特征和一个目标特征 Y 之间的互信息。也就是说，我们想要计算 I(Y; {X_1,
    X_2})。部分信息分解框架指出，这些信息可以分为四个非负成分：
- en: '**Syn**(Y; {X_1, X_2}): the Synergy of the two features. This is an amount
    of information about Y provided solely by the two features together.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Syn**(Y; {X_1, X_2})：两个特征的协同作用。这是两个特征共同提供的关于 Y 的信息量。'
- en: '**Rdn**(Y; {X_1, X_2}): the Redundancy of the two features. This quantity accounts
    for the information about Y that can be explained either by X_1 or X_2 alone.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Rdn**(Y; {X_1, X_2})：两个特征的冗余度。这个量表示关于 Y 的信息，这些信息可以通过 X_1 或 X_2 单独解释。'
- en: '**Unq**(Y; X_1) and **Unq**(Y; X_2): the Unique Information, which measures
    the information about Y that only X_1 can explain for Unq(Y; X_1) or that only
    X_2 can explain for Unq(Y; X_2).'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Unq**(Y; X_1) 和 **Unq**(Y; X_2)：唯一信息，衡量的是只有 X_1 可以解释的关于 Y 的信息（对于 Unq(Y; X_1)）或只有
    X_2 可以解释的关于 Y 的信息（对于 Unq(Y; X_2)）。'
- en: 'Notice that only **Unq**(Y; X_1) and **Unq**(Y; X_2) correspond to a scenario
    of no interaction between features. Hence, the mutual information **I**(Y; {X_1,
    X_2}) can be decomposed into its four components:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，只有**Unq**(Y; X_1)和**Unq**(Y; X_2)对应于特征之间没有交互的情景。因此，互信息**I**(Y; {X_1, X_2})可以分解为其四个组件：
- en: '**I**(Y; {X_1, X_2}) = **Syn**(Y; {X_1, X_2}) + **Rdn**(Y; {X_1, X_2}) + **Unq**(Y;
    X_1) + **Unq**(Y; X_2)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**I**(Y; {X_1, X_2}) = **Syn**(Y; {X_1, X_2}) + **Rdn**(Y; {X_1, X_2}) + **Unq**(Y;
    X_1) + **Unq**(Y; X_2)'
- en: Just as before, we can draw a nice Venn diagram that summarizes the dependency
    of these quantities.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 像之前一样，我们可以绘制一个漂亮的维恩图来总结这些量的依赖关系。
- en: '![](../Images/290f0c460177727a0f6657892086ec8c.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/290f0c460177727a0f6657892086ec8c.png)'
- en: 'Figure 3: Venn diagram for partial information decomposition. Image by author,
    heavily inspired by [1].'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：部分信息分解的维恩图。图像由作者提供，受[1]的强烈启发。
- en: Each of these terms is called an *atom of information*. Any non-atomic information
    can be decomposed into atomic parts, that cannot be decomposed.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 每个这些术语被称为*信息原子*。任何非原子信息都可以分解为原子部分，而原子部分不能再分解。
- en: 'It was Williams and Beer [1] who first proposed this framework (and came up
    with a way of calculating partial information). It turns out that calculating
    these quantities is not trivial and deserves an article by itself. There is more
    than one measure of partial information decomposition, and all of them follow
    the same process: they imagine a measure that satisfies a series of nice-to-have
    characteristics and that is consistent with what we expect to happen with some
    quantity called "information". All these measurements have strong and weak spots,
    and they are nicely implemented in `dit` library, which will be briefly introduced
    and used to give some examples in the following section.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 是威廉姆斯和比尔[1]首次提出了这一框架（并提出了一种计算部分信息的方法）。事实证明，计算这些量并不简单，值得专门写一篇文章。部分信息分解有不止一种测量方法，所有这些方法都遵循相同的过程：他们设想一种满足一系列理想特性并与我们期望的某个量称为“信息”的一致的测量方式。所有这些测量方法都有强项和弱项，它们在`dit`库中得到了很好的实现，并将在接下来的部分中简要介绍并用来给出一些例子。
- en: Partial Information Decomposition examples and the dit library
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部分信息分解示例和dit库
- en: To tie these concepts together, let’s see some examples. The `dit` library is
    a great tool for these experiments when it comes to information theory concepts.
    It is a lib that consists of creating customized probability distributions, and
    then performing measurements over them. There are several features within this
    library, and they can be found on their [Github](https://github.com/dit/dit) or
    at the official [documentation page](https://dit.readthedocs.io/en/latest/generalinfo.html).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将这些概念结合起来，让我们看一些例子。`dit`库是进行信息理论概念实验的绝佳工具。它是一个创建自定义概率分布的库，然后对其进行测量。这个库有几个功能，可以在他们的[Github](https://github.com/dit/dit)或官方[文档页面](https://dit.readthedocs.io/en/latest/generalinfo.html)中找到。
- en: For all examples to come, we can think of two features X_1 and X_2, both of
    them binary, and the target variable Y is some boolean operation with the features.
    All forms of measuring partial information will be due to Williams and Beer [1],
    but other forms proposed by other authors are also implemented in `dit` .
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于接下来的所有例子，我们可以考虑两个特征X_1和X_2，都是二进制的，目标变量Y是特征的一些布尔操作。所有测量部分信息的形式将基于威廉姆斯和比尔[1]，但其他作者提出的形式也在`dit`中实现。
- en: Unique information
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 独特信息
- en: For this example, imagine that the target variable Y is given by Fig. 4\. Notice
    that the output is always equal to the feature X_1, which makes the feature X_2
    completely irrelevant.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，假设目标变量Y如图4所示。注意输出总是等于特征X_1，这使得特征X_2完全无关紧要。
- en: '![](../Images/62395ed9352922437eda396dc65460ea.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62395ed9352922437eda396dc65460ea.png)'
- en: 'Figure 4: Diagram of AND gate and a unique source of information.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：与唯一信息源的AND门示意图。
- en: For this reason, the information that X_1 and X_2 provide about Y is fully concentrated
    in X_1\. In the formalism we have developed so far, we can say that the information
    about Y is *unique* to X_1.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，X_1和X_2对Y提供的信息完全集中在X_1中。在到目前为止我们所开发的形式中，我们可以说关于Y的信息对X_1是*唯一*的。
- en: 'In `dit` library, we can create this as:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在`dit`库中，我们可以这样创建：
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `dit` library encodes the type of information as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`dit`库对信息的编码如下：'
- en: '{0:1}: the synergistic information between X_1 and X_2'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '{0:1}: X_1和X_2之间的协同信息'
- en: '{0}: unique information provided by X_1'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '{0}: X_1 提供的独特信息'
- en: '{1}: unique information provided by X_2'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '{1}: X_2提供的唯一信息'
- en: '{0}{1}: redundant information provided by X_1 and X_2'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '{0}{1}: X_1和X_2提供的冗余信息'
- en: We can see by the output that the only partial information (the "pi" column)
    provided is from X_1.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中我们可以看到，唯一的部分信息（“pi”列）来自X_1。
- en: Redundant Information
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 冗余信息
- en: The next example serves to show the redundant information. Here, both X_1, X_2,
    and Y are equal as shown in Fig. 5, so the redundant information about Y provided
    by X_1 and X_2 is maximal.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个例子展示了冗余信息。在这里，X_1、X_2和Y的值相等，如图5所示，因此X_1和X_2提供的关于Y的冗余信息是最大的。
- en: '![](../Images/8fbdf8d7dbbd179f741c0705abdb5843.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8fbdf8d7dbbd179f741c0705abdb5843.png)'
- en: 'Figure 5: Fully redundant information.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：完全冗余的信息。
- en: 'Using `dit` the code goes as:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`dit`代码如下：
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As we can see, the only information about Y provided by X_1 and X_2 is redundant,
    in other words, provided by both of them.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，X_1和X_2提供的关于Y的唯一信息是冗余的，换句话说，由它们共同提供。
- en: Synergistic Information
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 协同信息
- en: A classic example of synergistic information is the XOR gate. The diagram for
    the XOR gate is shown in Fig. 6.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 协同信息的经典例子是XOR门。XOR门的图示见图6。
- en: '![](../Images/76e8db6da9b520b9098e9af1875d24ed.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/76e8db6da9b520b9098e9af1875d24ed.png)'
- en: 'Figure 6: The XOR gate with fully synergistic information'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：具有完全协同信息的XOR门
- en: 'Notice by this distribution that we can only know the target variable Y once
    we know both X_1 and X_2\. It is not possible to know Y without X_1 and X_2, simply
    because for each value of X_1 we have both values for Y; and the same goes for
    X_2\. The code in`dit` goes:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 从这种分布中可以看出，只有在知道X_1和X_2的情况下，我们才能知道目标变量Y。因为对于每个X_1的值，我们都有Y的两个值；同样的情况也适用于X_2。`dit`中的代码如下：
- en: '[PRE2]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As expected, the only information about Y that X_1 and X_2 convey is {0:1},
    which is the synergistic information.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，X_1和X_2传递的关于Y的唯一信息是{0:1}，这是协同信息。
- en: Final comments and takeaways
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终评论和要点
- en: Finally, we can see that the interaction between variables can pose a difficult
    challenge when we have at our disposal only mutual information. There needs to
    be some tool to measure information coming from multiple sources (and possibly
    the interaction between these sources of information). This is a perfect ground
    for the Partial Information Decomposition (PID) framework.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们可以看到，当我们仅依赖互信息时，变量之间的互动可能会带来挑战。需要一些工具来测量来自多个源的信息（以及这些信息源之间的互动）。这是部分信息分解（PID）框架的理想场景。
- en: 'Usually, the measurements in this field are convoluted and involve some formal
    logic: this can be left for another thorough article about this topic, but now
    it suffices to say that these tools are not only important, but their need arises
    naturally from the information approach.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这一领域的测量方法较为复杂，涉及一些形式逻辑：这些内容可以留待另篇详细文章，但现在只需说明，这些工具不仅重要，而且它们的需求自然来源于信息方法。
- en: References
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] P. L. Williams and R. D. Beer, [Nonnegative decomposition of multivariate
    information](https://arxiv.org/pdf/1004.2515.pdf), *arXiv preprint arXiv:1004.2515*,
    2010'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] P. L. Williams 和 R. D. Beer, [多变量信息的非负分解](https://arxiv.org/pdf/1004.2515.pdf)，*arXiv预印本arXiv:1004.2515*，2010年'
- en: '[2] Shujian Yu, et al., [Understanding Convolutional Neural Networks with Information
    Theory: An Initial Exploration](https://arxiv.org/pdf/1804.06537.pdf), *arXiv
    preprint arXiv:*1804.06537v5, 2020'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Shujian Yu等，[通过信息理论理解卷积神经网络：初步探索](https://arxiv.org/pdf/1804.06537.pdf)，*arXiv预印本arXiv:*1804.06537v5，2020年'
- en: '[3] A. J. Gutknecht, M. Wibral and A. Makkeh, [Bits and pieces: understanding
    information decomposition from part-whole relationships and formal logic](https://arxiv.org/pdf/2008.09535.pdf),
    *arXiv preprint arXiv:*2008.09535v2, 2022'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] A. J. Gutknecht, M. Wibral 和 A. Makkeh, [Bits and pieces: 理解从部分-整体关系和形式逻辑中的信息分解](https://arxiv.org/pdf/2008.09535.pdf)，*arXiv预印本arXiv:*2008.09535v2，2022年'
- en: '[4] James, R. G., Ellison, C. J. and Crutchfield, J. P., [dit: a Python package
    for discrete information theory](https://joss.theoj.org/papers/10.21105/joss.00738.pdf),
    The Journal of Open Source Software, 2018'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] James, R. G., Ellison, C. J. 和 Crutchfield, J. P., [dit: 用于离散信息理论的Python包](https://joss.theoj.org/papers/10.21105/joss.00738.pdf)，《开源软件期刊》，2018年'
