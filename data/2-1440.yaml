- en: Local Light Field Fusion
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 局部光场融合
- en: 原文：[https://towardsdatascience.com/local-light-field-fusion-14c07ed36117](https://towardsdatascience.com/local-light-field-fusion-14c07ed36117)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/local-light-field-fusion-14c07ed36117](https://towardsdatascience.com/local-light-field-fusion-14c07ed36117)
- en: How to render 3D scenes on a smart phone
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何在智能手机上渲染3D场景
- en: '[](https://wolfecameron.medium.com/?source=post_page-----14c07ed36117--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----14c07ed36117--------------------------------)[](https://towardsdatascience.com/?source=post_page-----14c07ed36117--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----14c07ed36117--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----14c07ed36117--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page-----14c07ed36117--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----14c07ed36117--------------------------------)[](https://towardsdatascience.com/?source=post_page-----14c07ed36117--------------------------------)[![数据科学前沿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----14c07ed36117--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----14c07ed36117--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----14c07ed36117--------------------------------)
    ·12 min read·Apr 11, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布在[数据科学前沿](https://towardsdatascience.com/?source=post_page-----14c07ed36117--------------------------------)
    ·12分钟阅读·2023年4月11日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d1c22270367a41b360ab04e39462e1a2.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1c22270367a41b360ab04e39462e1a2.png)'
- en: (Photo by [Clyde He](https://unsplash.com/@clyde_he?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/backgrounds/colors/light?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: （照片由[Clyde He](https://unsplash.com/@clyde_he?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)拍摄，来源于[Unsplash](https://unsplash.com/backgrounds/colors/light?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)）
- en: By now, we should know that deep learning is a great way to represent 3D scenes
    and generate new renderings of these scenes from arbitrary viewpoints. The problem
    with the approaches we have seen so far (e.g., [ONets](https://cameronrwolfe.substack.com/p/shape-reconstruction-with-onets)
    and [SRNs](https://cameronrwolfe.substack.com/p/scene-representation-networks)
    [2, 3], however, is that they require many images of the underlying scene to be
    available to train the model. With this in mind, we might wonder whether it’s
    possible to obtain a deep learning-based scene representation with fewer samples
    of the underlying scene. *How many images do we actually need to train a high-resolution
    scene representation?*
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们应该知道深度学习是表示3D场景并从任意视角生成这些场景的新渲染的一个极佳方法。然而，我们迄今为止看到的方法（例如，[ONets](https://cameronrwolfe.substack.com/p/shape-reconstruction-with-onets)
    和 [SRNs](https://cameronrwolfe.substack.com/p/scene-representation-networks) [2,
    3]）的问题是它们需要大量的场景图像来训练模型。考虑到这一点，我们可能会想知道是否可以用更少的样本获得基于深度学习的场景表示。*我们实际需要多少图像来训练一个高分辨率的场景表示？*
- en: 'This question was addressed and answered by the Local Light Field Fusion (LLFF)
    [1] approach for synthesizing scenes in 3D. An extension of [light field](http://lightfield-forum.com/what-is-the-lightfield/)
    rendering [4], LLFFs generate scene viewpoints by expanding several sets of existing
    views into multi-plane image (MPI) representations, then rendering a new viewpoint
    by blending these representations together. The resulting method:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题通过局部光场融合（LLFF）[1]方法来解决，该方法用于合成3D场景。LLFF是[光场](http://lightfield-forum.com/what-is-the-lightfield/)渲染[4]的一个扩展，通过将几个现有视图扩展为多平面图像（MPI）表示，然后通过混合这些表示来渲染新的视角。该方法的结果是：
- en: Accurately models complex scenes and effects like reflections.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 精确地模拟复杂场景和效果，例如反射。
- en: Is theoretically shown to reduce the number samples/images required to produce
    an accurate scene representation.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理论上显示减少了生成准确场景表示所需的样本/图像数量。
- en: Plus, *LLFFs are prescriptive*, meaning that the framework can be used to tell
    users how many and what type of images are needed to produce an accurate scene
    representations. Thus, LLFFs are an accurate, deep learning-based methodology
    for generative modeling of 3D scenes that provide useful, prescriptive insight.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，*LLFF是规范性的*，这意味着该框架可以用于告知用户需要多少和什么类型的图像来生成准确的场景表示。因此，LLFF是一种准确的、基于深度学习的方法，用于生成3D场景的建模，并提供有用的、规范性的见解。
- en: '![](../Images/a914dbd330427b92b20e4211f3707cb5.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a914dbd330427b92b20e4211f3707cb5.png)'
- en: (from [1])
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [1])
- en: Background
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: To understand LLFFs, we need to understand a few concepts related to both computer
    vision and deep learning in general. We will first talk about the concept of light
    fields, then go over a few deep learning concepts that are used by LLFF.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解LLFF，我们需要了解与计算机视觉和深度学习相关的一些概念。我们将首先讨论光场的概念，然后介绍LLFF所使用的一些深度学习概念。
- en: '**light fields.** A [light field](http://lightfield-forum.com/what-is-the-lightfield/)
    represents a 3D scene as rays of light that are directionally flowing through
    space. Traditionally, we can use light fields to render views of scenes by just
    *(i)* sampling a scene’s light field (i.e., capturing images with depth and [calibration](https://cameronrwolfe.substack.com/i/97472888/background)
    information) at a bunch of different points and *(ii)* interpolating between these
    fields.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**光场。** 一个[光场](http://lightfield-forum.com/what-is-the-lightfield/)将一个 3D 场景表示为在空间中方向性流动的光线。传统上，我们可以使用光场通过仅仅*(i)*
    采样场景的光场（即，捕获具有深度和[校准](https://cameronrwolfe.substack.com/i/97472888/background)信息的图像）在不同的点和*(ii)*
    在这些光场之间进行插值来渲染场景的视图。'
- en: '![](../Images/04b8e3808022961648344eeb32326860.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04b8e3808022961648344eeb32326860.png)'
- en: (from [1])
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [1])
- en: For such a system, we know from research in [signal processing](https://en.wikipedia.org/wiki/Signal_processing)
    how many samples we would have to take to accurately render new views of a scene.
    The minimum number of samples needed to accurately represent a scene is referred
    to as the [Nyquist rate](https://www.tutorialspoint.com/what-is-nyquist-rate-and-nyquist-interval);
    see above. Practically, the number of samples required by the Nyquist rate is
    prohibitive, but research in plenoptic sampling [7] aims to improve sample efficiency
    and reduce the number of required samples significantly below that of the Nyquist
    rate.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这样的系统，我们从[信号处理](https://en.wikipedia.org/wiki/Signal_processing)的研究中了解到，为了准确渲染场景的新视角，我们需要采集多少样本。准确表示场景所需的最小样本数量称为[奈奎斯特率](https://www.tutorialspoint.com/what-is-nyquist-rate-and-nyquist-interval)；见上文。实际上，奈奎斯特率所需的样本数量是
    prohibitive 的，但全景采样的研究 [7] 旨在提高样本效率，并显著减少所需样本数量，低于奈奎斯特率。
- en: The inner workings of plenoptic sampling are not important for the purposes
    of this overview. The main idea that we should take away from this discussion
    is that authors in [1] extend the idea of plenoptic sampling to enable accurate
    scene renderings when fewer (and potentially occluded) samples are available;
    see below.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 全景采样的内部工作原理对于本概述的目的并不重要。我们应该从这次讨论中得到的主要思想是，[1]中的作者将全景采样的概念扩展到在样本较少（且可能被遮挡）的情况下实现准确的场景渲染；见下文。
- en: '![](../Images/b3a41b77d7580bc781cb4c1778a2b52a.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3a41b77d7580bc781cb4c1778a2b52a.png)'
- en: (from [1])
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [1])
- en: Beyond its sample efficiency, plenoptic sampling is a theoretical framework
    that enables prescriptive analysis. Instead of just taking images of a scene and
    hoping this is enough, *we can specifically identify the number and type of images
    that should be included for training an LLFF by drawing upon this analysis*!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 除了样本效率之外，**全景采样**还是一种理论框架，它能够进行规范性分析。*我们可以通过这种分析具体确定用于训练LLFF的图像数量和类型，而不仅仅是拍摄场景图像并希望这就足够了*。
- en: '**convolutions in 3D.** Most of us are probably familiar with 2D convolutions,
    such as those used within image-based CNNs. However, LLFF actually utilizes 3D
    convolutions. *Why?* We will learn more later on, but the basic reason is that
    the input to our neural network is not just an image or group of images, it has
    an extra depth dimension. So, we need to perform convolutions in a manner that
    considers this extra dimension.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**3D 卷积。** 我们大多数人可能对 2D 卷积比较熟悉，比如图像基础的CNN中使用的那些。然而，LLFF 实际上利用了 3D 卷积。*为什么？*
    我们稍后会深入了解，但基本原因是我们神经网络的输入不仅仅是一张图像或一组图像，它有一个额外的深度维度。因此，我们需要以考虑这一额外维度的方式进行卷积。'
- en: '![](../Images/22f14677324b269b12940598020a7cfc.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22f14677324b269b12940598020a7cfc.png)'
- en: 2D convolution on an image versus a 3D convolution on three video frames (created
    by author)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图像上的 2D 卷积与三帧视频上的 3D 卷积（由作者创建）
- en: 3D convolutions accomplish this goal exactly. Namely, instead of just convolving
    over all spatial dimensions within an input, we convolve over both spatial and
    depth dimensions. Practically, this adds an extra dimension to our convolutional
    kernel, and the convolution operation traverses the input both spatially and depth-wise.
    This process is illustrated in the figure above, where we first spatially convolve
    over a group of frames, then move on to the next group of frames to perform another
    spatial convolution.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 卷积正好实现了这个目标。也就是说，我们不仅在输入的所有空间维度上进行卷积，还在空间和深度维度上进行卷积。实际上，这为我们的卷积核添加了一个额外的维度，卷积操作在空间和深度上遍历输入。这个过程在上面的图中得到了说明，我们首先在一组帧上进行空间卷积，然后移动到下一组帧进行另一轮空间卷积。
- en: 3D convolutions are commonly used in video deep learning applications. For anyone
    who is interested in learning more about this topic or the inner workings of 3D
    convolutions, feel free to check out my overview of deep learning for video at
    [this link](https://cameronrwolfe.substack.com/p/deep-learning-on-video-part-one-the-early-days-8a3632ed47d4)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 卷积通常用于视频深度学习应用。对于那些对深入了解这个主题或 3D 卷积的内部工作感兴趣的人，可以查看我关于视频深度学习的概述，[点击此处](https://cameronrwolfe.substack.com/p/deep-learning-on-video-part-one-the-early-days-8a3632ed47d4)。
- en: '**perceptual loss.** The goal of LLFFs is to produce images that accurately
    resemble actual, ground truth viewpoints of a scene. To train a system to accomplish
    this goal, we need an *image reconstruction loss* that tells us how closely a
    generated image matches the actual image we are trying to replicate. One option
    is to compute the [L1](https://mathworld.wolfram.com/L1-Norm.html)/[L2](https://mathworld.wolfram.com/L2-Norm.html)-norm
    of the difference between the two images — basically just a [mean-squared error](https://en.wikipedia.org/wiki/Mean_squared_error)
    loss directly on image pixels.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**感知损失**。LLFFs 的目标是生成准确地类似于场景实际地面真值视角的图像。为了训练一个系统实现这个目标，我们需要一个 *图像重建损失*，它告诉我们生成的图像与我们试图复制的实际图像有多接近。一个选项是计算两张图像之间差异的
    [L1](https://mathworld.wolfram.com/L1-Norm.html)/[L2](https://mathworld.wolfram.com/L2-Norm.html)
    范数——基本上只是对图像像素的 [均方误差](https://en.wikipedia.org/wiki/Mean_squared_error) 损失。'
- en: 'However, simply measuring pixel differences isn’t the best metric for image
    similarity; e.g., *what if the generated image is just translated one pixel to
    the right compared to the target?* A better approach can be accomplished with
    a bit of deep learning. In particular we can:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，单纯测量像素差异并不是图像相似性的最佳指标；例如，*如果生成的图像相比目标图像仅仅右移了一像素呢？* 一个更好的方法可以通过一点深度学习实现。特别是我们可以：
- en: Take a pre-trained deep neural network.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用一个预训练的深度神经网络。
- en: Use this model to embed both images into a feature vector (i.e., the final layer
    of activations before classification).
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这个模型将图像嵌入到特征向量中（即，分类前的最终激活层）。
- en: Compute the difference between these vectors (e.g., using an L1 or L2-norm)
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算这些向量之间的差异（例如，使用 L1 或 L2 范数）
- en: This approach, called the perceptual loss [5], is a powerful image similarity
    metric that is used heavily in deep learning research (especially for generative
    models); see Section 3.3 in [6].
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法称为感知损失 [5]，是一种强大的图像相似性度量，广泛用于深度学习研究（特别是生成模型）；见 [6] 的第 3.3 节。
- en: How do LLFFs represent scenes?
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLFFs 如何表示场景？
- en: '![](../Images/3e38d1175ca9f5acc45c3e6e2b4c05e7.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e38d1175ca9f5acc45c3e6e2b4c05e7.png)'
- en: (from [1])
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: （引自 [1]）
- en: '*“The overall strategy of our method is to use a deep learning pipeline to
    promote each sampled view to a layered scene representation with D depth layers,
    and render novel views by blending between renderings from neighboring scene representations.”*
    — from [1]'
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“我们方法的总体策略是使用深度学习管道将每个采样视图提升为具有 D 个深度层的分层场景表示，并通过在相邻场景表示的渲染之间进行混合来渲染新的视图。”*
    — 引自 [1]'
- en: 'Starting with some images and [camera viewpoint information](https://cameronrwolfe.substack.com/i/97472888/background),
    LLFFs render novel scene viewpoints using two, distinct steps:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 从一些图像和 [相机视角信息](https://cameronrwolfe.substack.com/i/97472888/background) 开始，LLFFs
    通过两个不同的步骤渲染新的场景视角：
- en: Convert the image into an MPI representation.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像转换为 MPI 表示。
- en: Generate a view by blending renderings from nearby MPIs.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过混合附近 MPI 的渲染生成视图。
- en: '**what are MPIs?** MPIs are a camera-centric representation of 3D space. This
    means that we consider a specific camera viewpoint, then decompose 3D space from
    the perspective of this particular viewpoint. In particular, 3D space is decomposed
    based on three coordinates: `x`, `y`, and depth. Then, associated with each of
    these coordinates is an RGB color and an opaqueness value, denoted as α. See the
    link [here](https://single-view-mpi.github.io/) for more details.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**什么是 MPIs？** MPIs 是一种以相机为中心的 3D 空间表示。这意味着我们考虑一个特定的相机视角，然后从该特定视角分解 3D 空间。特别是，3D
    空间基于三个坐标进行分解：`x`、`y` 和深度。然后，与这些坐标相关联的是 RGB 颜色和不透明度值，记作 α。有关更多详细信息，请参见 [这里](https://single-view-mpi.github.io/)。'
- en: '**generating MPIs.** To generate an MPI in LLFF, we need a set of five images,
    including a reference image and four nearest neighbors in 3D space. Using camera
    viewpoint information, we can re-project these images into [plane sweep](https://www.coursera.org/lecture/geometric-algorithms/plane-sweep-concept-Jch6p)
    volumes (PSVs) with depth `D`. Here, each depth dimension corresponds to different
    ranges of depths within a scene from a particular viewpoint.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成 MPIs。** 要在 LLFF 中生成 MPI，我们需要一组五张图像，包括一张参考图像和四个在 3D 空间中最近的邻居。通过相机视角信息，我们可以将这些图像重新投影到[平面扫掠](https://www.coursera.org/lecture/geometric-algorithms/plane-sweep-concept-Jch6p)体积（PSVs）中，深度为
    `D`。在这里，每个深度维度对应于特定视角下场景中的不同深度范围。'
- en: '![](../Images/45c69113510af6d9362eb827341a78df.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/45c69113510af6d9362eb827341a78df.png)'
- en: (from [1])
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: （来源于 [1]）
- en: From here, we can concatenate all of these volumes and pass them through a series
    of 3D convolutional layers (i.e., a [3D CNN](https://cameronrwolfe.substack.com/p/deep-learning-on-video-part-three-diving-deeper-into-3d-cnns-cb3c0daa471e)).
    For each MPI coordinate (i.e., consists of an `[x, y]` spatial location and a
    depth), this 3D CNN will output an RGB color and an opacity value α, forming an
    MPI scene representation; see above. In [1], this is referred to as a “layered
    scene representation”, due to the different depths represented within the MPI.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，我们可以将所有这些体积连接起来，并通过一系列 3D 卷积层传递（即，[3D CNN](https://cameronrwolfe.substack.com/p/deep-learning-on-video-part-three-diving-deeper-into-3d-cnns-cb3c0daa471e)）。对于每个
    MPI 坐标（即，由 `[x, y]` 空间位置和深度组成），这个 3D CNN 将输出一个 RGB 颜色和一个不透明度值 α，形成一个 MPI 场景表示；见上文。在
    [1] 中，这被称为“分层场景表示”，因为 MPI 中表示了不同的深度。
- en: '**reconstructing a view.** Once an MPI is generated for a scene, we still need
    to take this information and use it to synthesize a novel scene viewpoint. In
    [1], this is done by rendering multiple MPIs and taking a weighted combination
    of their results.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**重建视图。** 一旦为一个场景生成了 MPI，我们仍然需要利用这些信息来合成一个新颖的场景视角。在 [1] 中，通过渲染多个 MPIs 并对其结果进行加权组合来完成这项工作。'
- en: '![](../Images/383c912b1d218cecdd1427f965698f41.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/383c912b1d218cecdd1427f965698f41.png)'
- en: (from [1])
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: （来源于 [1]）
- en: In particular, we generate MPIs (using the 3D CNN described above) from multiple
    sets of images that are close to the desired viewpoint, then use [homography warping](https://medium.com/swlh/image-processing-with-python-image-warping-using-homography-matrix-22096734f09a)
    (i.e., this “warps” each MPI to the desired viewpoint) and [alpha compositing](https://ciechanow.ski/alpha-compositing/)
    (i.e., this combines the different warped MPIs into a single view) to produce
    an RGB image of the desired viewpoint.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，我们从多个接近所需视角的图像集中生成 MPIs（使用上述描述的 3D CNN），然后使用[单应性变换](https://medium.com/swlh/image-processing-with-python-image-warping-using-homography-matrix-22096734f09a)（即，将每个
    MPI “扭曲”到所需视角）和[α 合成](https://ciechanow.ski/alpha-compositing/)（即，将不同扭曲的 MPIs
    组合成一个单一视图）来生成所需视角的 RGB 图像。
- en: '**why do we need multiple MPIs?** The approach in [1] typically produces two
    MPIs using different sets of images, then blends these representations into a
    single scene rendering. This is necessary to remove artifacts within the rendering
    process and because a single MPI is unlikely to include all the information that’s
    needed for the new camera pose. For example, *what if a portion of the image is
    occluded in the original viewpoints?* Blending multiple MPIs lets us avoid these
    artifacts and deal with issues like occlusion and limited field of view; see below.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么我们需要多个 MPIs？** [1] 中的方法通常使用不同的图像集生成两个 MPIs，然后将这些表示混合成一个单一的场景渲染。这是为了去除渲染过程中出现的伪影，因为单个
    MPI 不太可能包含新相机姿态所需的所有信息。例如，*如果原始视角中的图像部分被遮挡怎么办？* 混合多个 MPIs 可以避免这些伪影并处理诸如遮挡和视场限制等问题；见下文。'
- en: '![](../Images/c87e4af05e5a6d308016d8a6b6804dd1.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c87e4af05e5a6d308016d8a6b6804dd1.png)'
- en: (from [1])
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: '**training the LLFF framework.** To train the LLFF framework, we use a combination
    of real and synthetic (e.g., renderings from [SUNCG](https://sscnet.cs.princeton.edu/)
    and [UnrealCV](https://unrealcv.org/)) data. During each training iteration, we
    sample two sets of five images (used to create two MPIs) and a single, hold-out
    viewpoint. We generate an estimate of this held-out viewpoint by following the
    approach described above, then apply a perceptual loss function [5] that captures
    how different the outputted viewpoint is from the ground truth.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练LLFF框架。** 为了训练LLFF框架，我们使用真实和合成（例如，[SUNCG](https://sscnet.cs.princeton.edu/)
    和 [UnrealCV](https://unrealcv.org/) 渲染）数据的组合。在每次训练迭代中，我们采样两组五张图像（用于创建两个MPI）和一个单独的保留视点。我们通过遵循上述方法生成此保留视点的估计值，然后应用感知损失函数
    [5]，以捕捉输出视点与真实情况的差异。'
- en: 'We can train LLFF end-to-end because all of its components are differentiable.
    To perform a training iteration, we just need to:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对LLFF进行端到端训练，因为它的所有组件都是可微分的。要执行训练迭代，我们只需要：
- en: Sample some images.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 采样一些图像。
- en: Generate a predicted viewpoint.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个预测视点。
- en: Compute the perceptual loss.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算感知损失。
- en: Perform a [(stochastic) gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)
    update.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行 [(随机) 梯度下降](https://en.wikipedia.org/wiki/Gradient_descent) 更新。
- en: '**theoretical reduction in required samples.** Sampling according to the Nyquist
    rate is intractable for scene representations because the number of samples required
    is too high. Luckily, the deep learning-based LLFF approach in [1] is shown theoretically
    to reduce the number of required samples for an accurate scene representation
    significantly. In fact, the number of required views for an accurate LLFF reconstruction
    is shown to be 4,000`X` below the Nyquist rate empirically; see below.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**理论上减少所需样本的数量。** 根据奈奎斯特率进行采样在场景表示中是不可行的，因为所需样本的数量太高。幸运的是，[1]中的基于深度学习的LLFF方法在理论上已显示出显著减少准确场景表示所需的样本数量。实际上，准确的LLFF重建所需的视图数量在实证中被显示为低于奈奎斯特率的4,000`X`；见下文。'
- en: '![](../Images/781fa3f9439ad3ee13bb841f5cb983e0.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/781fa3f9439ad3ee13bb841f5cb983e0.png)'
- en: (from [1])
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: Using LLFF in practice
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践中的LLFF
- en: '![](../Images/4fe4320322141b2964eb448b0bfbdefb.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4fe4320322141b2964eb448b0bfbdefb.png)'
- en: (from [1])
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: LLFFs are evaluated based on their ability to render novel scene viewpoints
    with limited sampling capability (i.e., far below the Nyquist rate). Within the
    experimental analysis, one of the first major findings is that blending renderings
    from multiple MPIs — as opposed to just rendering a view from a single MPI — is
    quite beneficial. As shown above, this approach improves accuracy and enables
    non-Lambertian effects (e.g., reflections) to be captured.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: LLFF的评估基于其在有限采样能力（即远低于奈奎斯特率）的情况下渲染新场景视点的能力。在实验分析中，第一个主要发现之一是，将多个MPI的渲染结果混合——而不是仅从单个MPI渲染视图——是相当有益的。如上所示，这种方法提高了准确性，并且能够捕捉到非兰伯特效应（例如，反射）。
- en: LLFF is more capable of modeling complex scenes, both quantitatively and qualitatively,
    compared to baselines. In particular, LLFF seems to yield much more consistent
    results when fewer samples of the underlying scene are available, whereas baselines
    experience a deterioration in performance; see below.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 与基线相比，LLFF在建模复杂场景方面无论是定量还是定性上都更具能力。特别是，当可用的基础场景样本较少时，LLFF似乎能够产生更加一致的结果，而基线方法则会出现性能下降；见下文。
- en: '![](../Images/06b2e4ad23acbdc0b313a7c04adf7206.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06b2e4ad23acbdc0b313a7c04adf7206.png)'
- en: (from [1])
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: LLFF’s sample efficiency emphasizes the utility of deep learning. Namely, the
    model can learn implicit prior information from the training data that allows
    it to better handle ambiguity! To make this point more concrete, let’s consider
    a case where we have some input views, but this data doesn’t give us all the information
    we need to produce an accurate, novel view (e.g., maybe some relevant part of
    the scene is occluded). Because we are using deep learning, our neural network
    has learned prior patterns from data that allow it to infer a reasonable output
    in these cases!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: LLFF的采样效率突出了深度学习的实用性。即，模型可以从训练数据中学习隐含的先验信息，使其能够更好地处理模糊性！为了更具体地说明这一点，我们考虑一个有输入视图的情况，但这些数据并没有提供我们生成准确新视图所需的所有信息（例如，场景中的某些相关部分可能被遮挡）。由于我们使用了深度学习，我们的神经网络已经从数据中学到了先验模式，使其能够在这些情况下推断出合理的输出！
- en: '![](../Images/9e4204613a6788375ec5c97d8b732305.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e4204613a6788375ec5c97d8b732305.png)'
- en: (from [1])
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: To better understand how LLFF compares to baselines, it’s really useful to look
    at qualitative examples of output. Several examples are provided in the figure
    above, but these outputs are best viewed as a video so that the smoothness in
    interpolation between different viewpoints is easily visible. For examples of
    this, check out the project page for LLFF [here](https://bmild.github.io/llff/)!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解LLFF与基线方法的比较，查看输出的定性示例非常有用。上图提供了几个示例，但这些输出最好以视频形式查看，以便能清晰地看到不同视角之间的插值平滑度。有关示例，请查看LLFF的项目页面
    [这里](https://bmild.github.io/llff/)！
- en: '**LLFF on a smart phone.** As a practical demonstration of the LLFF framework,
    authors create a smart phone app for high-quality interpolation between views
    of a scene. Given a fixed resolution, LLFF can efficiently produce novel scene
    viewpoints using a reasonable number of scene images. This app instructs the user
    to capture specific samples of an underlying scene, then renders views from predicted
    MPIs in real-time using LLFF.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**LLFF在智能手机上的应用。** 作为LLFF框架的实际演示，作者创建了一个用于在场景视角之间进行高质量插值的智能手机应用程序。给定固定分辨率，LLFF可以使用合理数量的场景图像高效地生成新的场景视角。该应用程序指示用户捕捉特定的场景样本，然后使用LLFF实时渲染从预测的MPI中生成的视角。'
- en: '![](../Images/0ded5ffe7d9d8c4ca4a0fbe943e4e5ef.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ded5ffe7d9d8c4ca4a0fbe943e4e5ef.png)'
- en: (from [1])
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: （来源于 [1]）
- en: Beyond the quality of LLFF renderings, recall that it is a prescriptive framework,
    meaning that the authors in [1] provide theory for the number and type of image
    samples needed to accurately represent a scene. Along these lines, *the LLFF app
    actually guides users to take specific images of the underlying scene*. This feature
    leverages the proposed sampling analysis in [1] to determine needed samples and
    uses VR overlays to instruct users to capture specific scene viewpoints; see above.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 除了LLFF渲染的质量，还要记住它是一个规范性框架，这意味着在 [1] 中的作者提供了准确表示场景所需的图像样本数量和类型的理论。沿着这些思路，*LLFF应用实际上会引导用户拍摄特定的场景图像*。此功能利用
    [1] 中提出的采样分析来确定所需样本，并使用VR覆盖来指导用户捕捉特定场景视角；见上文。
- en: Takeaways
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收获
- en: The LLFF framework is quite a bit different from other methods of representing
    scenes that we have seen so far. It uses a 3D CNN instead of [feed-forward networks](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks),
    comes with theoretical guarantees, and is more related to signal processing than
    deep learning. Nonetheless, the framework is incredibly interesting, and hopefully
    the context provided in this overview will make it a bit easier to understand.
    The major takeaways are as follows.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: LLFF框架与我们迄今为止看到的其他场景表示方法有很大不同。它使用3D CNN而不是 [前馈网络](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks)，具备理论保证，并且与信号处理的关系更大，而非深度学习。尽管如此，这个框架非常有趣，希望本概述提供的背景能让理解它变得稍微容易些。主要收获如下。
- en: '**plenoptic sampling + deep learning.** As mentioned throughout this overview,
    the number of samples required to produce accurate scene representations with
    LLFF is quite low (especially when compared to the Nyquist rate). Such sample
    efficiency is partially due to the plenoptic sampling analysis upon which LLFF
    is based. However, using deep learning allows patterns from training data to be
    learned and generalized, which has a positive effect on the efficiency and accuracy
    of resulting scene renderings.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**全光谱采样 + 深度学习。** 正如在本概述中提到的那样，使用LLFF生成准确场景表示所需的样本数量非常少（尤其是与Nyquist率相比）。这种样本效率部分得益于LLFF所基于的全光谱采样分析。然而，使用深度学习允许从训练数据中学习并泛化模式，这对结果场景渲染的效率和准确性产生了积极影响。'
- en: '**real-time representations.** Beyond the quality of viewpoints rendered by
    LLFF, the method was implemented in a smart phone app that can run in real-time!
    This practically demonstrates the efficiency of LLFF and shows that it is definitely
    usable in real world applications. However, performing the necessary preprocessing
    to render viewpoints with LLFF takes ~10 minutes.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**实时表示。** 除了LLFF渲染的视角质量，方法还被实现为可以实时运行的智能手机应用程序！这实际上展示了LLFF的效率，并表明它在现实世界应用中确实可用。然而，使用LLFF进行视角渲染所需的预处理大约需要10分钟。'
- en: '**multiple viewpoints.** To create the final LLFF result, we generate two MPIs,
    which are blended together. We could render a scene with a single MPI, but using
    multiple MPIs is found to create more accurate renderings (i.e., fewer artifacts
    and missing details). In general, this finding shows us that redundancy is useful
    for scene representations — useful data that is missing from one viewpoint might
    be present in another!'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**多个视角。** 为了生成最终的LLFF结果，我们生成两个MPI，并将它们混合在一起。我们可以用单个MPI来渲染一个场景，但使用多个MPI被发现可以创建更准确的渲染（即，较少的伪影和遗漏的细节）。一般来说，这一发现向我们表明冗余对于场景表示是有用的——从一个视角缺失的有用数据可能在另一个视角中存在！'
- en: '**limitations.** Obviously, the quality of scene representations can always
    be improved — LLFFs are not perfect. Beyond this simple observation, one potential
    limitation of LLFF is that, to produce an output, we need to provide several images
    as input (e.g., experiments in [1] require ten input images for each output).
    Comparatively, models like SRNs [3] are trained over images of an underlying scene,
    but they do not necessarily require that these images be present at inference
    time!'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**局限性。** 显然，场景表示的质量总是可以提高的——LLFF并不完美。除了这个简单的观察之外，LLFF的一个潜在局限是，为了生成输出，我们需要提供几张图像作为输入（例如，[1]中的实验需要每个输出十张输入图像）。相比之下，像SRNs
    [3]这样的模型是在一个基础场景的图像上进行训练的，但它们不一定需要这些图像在推理时存在！'
- en: Closing remarks
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结束语
- en: Thanks so much for reading this article. I am [Cameron R. Wolfe](https://cameronrwolfe.me/),
    Director of AI at [Rebuy](https://www.rebuyengine.com/) and PhD student at Rice
    University. I study the empirical and theoretical foundations of deep learning.
    You can also check out my [other writings](https://medium.com/@wolfecameron) on
    medium! If you liked it, please follow me on [twitter](https://twitter.com/cwolferesearch)
    or subscribe to my [Deep (Learning) Focus newsletter](https://cameronrwolfe.substack.com/),
    where I help readers build a deeper understanding of topics in deep learning research
    via understandable overviews of popular papers on that topic.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢阅读这篇文章。我是[Cameron R. Wolfe](https://cameronrwolfe.me/)，[Rebuy](https://www.rebuyengine.com/)的人工智能总监和莱斯大学的博士生。我研究深度学习的实证和理论基础。你还可以查看我在medium上的[其他文章](https://medium.com/@wolfecameron)！如果你喜欢这篇文章，请关注我的[twitter](https://twitter.com/cwolferesearch)或订阅我的[Deep
    (Learning) Focus newsletter](https://cameronrwolfe.substack.com/)，在这里我通过对相关热门论文的易懂概述，帮助读者深入理解深度学习研究中的话题。
- en: Bibliography
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Mildenhall, Ben, et al. “Local light field fusion: Practical view synthesis
    with prescriptive sampling guidelines.” *ACM Transactions on Graphics (TOG)* 38.4
    (2019): 1–14.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 米尔登霍尔，本等。“局部光场融合：具有指导性采样准则的实用视图合成。” *ACM图形学交易（TOG）* 38.4 (2019)：1–14。'
- en: '[2] Mescheder, Lars, et al. “Occupancy networks: Learning 3d reconstruction
    in function space.” *Proceedings of the IEEE/CVF conference on computer vision
    and pattern recognition*. 2019.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] 梅施德，拉斯等。“占用网络：在函数空间中学习3D重建。” *IEEE/CVF计算机视觉与模式识别会议论文集*。2019年。'
- en: '[3] Sitzmann, Vincent, Michael Zollhöfer, and Gordon Wetzstein. “Scene representation
    networks: Continuous 3d-structure-aware neural scene representations.” *Advances
    in Neural Information Processing Systems* 32 (2019).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] 西茨曼，文森特，迈克尔·佐尔赫弗，和戈登·维茨斯坦。“场景表示网络：连续的3D结构感知神经场景表示。” *神经信息处理系统进展* 32 (2019)。'
- en: '[4] Levoy, Marc, and Pat Hanrahan. “Light field rendering.” *Proceedings of
    the 23rd annual conference on Computer graphics and interactive techniques*. 199.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] 莱沃伊，马克，和帕特·汉拉汉。“光场渲染。” *第23届计算机图形与交互技术年会论文集*。199。'
- en: '[5] Dosovitskiy, Alexey, and Thomas Brox. “Generating images with perceptual
    similarity metrics based on deep networks.” *Advances in neural information processing
    systems* 29 (2016).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] 多索维茨基，阿列克谢，和托马斯·布罗克斯。“利用基于深度网络的感知相似度度量生成图像。” *神经信息处理系统进展* 29 (2016)。'
- en: '[6] Chen, Qifeng, and Vladlen Koltun. “Photographic image synthesis with cascaded
    refinement networks.” *Proceedings of the IEEE international conference on computer
    vision*. 2017.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] 陈启峰，弗拉德伦·科尔顿。“基于级联优化网络的摄影图像合成。” *IEEE国际计算机视觉会议论文集*。2017年。'
- en: '[7] Chai, Jin-Xiang, et al. “Plenoptic sampling.” *Proceedings of the 27th
    annual conference on Computer graphics and interactive techniques*. 2000.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] 柴进祥等。“全景采样。” *第27届计算机图形与交互技术年会论文集*。2000年。'
- en: '[8] Mildenhall, Ben, et al. “Nerf: Representing scenes as neural radiance fields
    for view synthesis.” *Communications of the ACM* 65.1 (2021): 99–106.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Mildenhall, Ben 等人。“Nerf: 将场景表示为神经辐射场以进行视图合成。” *ACM通讯* 65.1 (2021): 99–106。'
