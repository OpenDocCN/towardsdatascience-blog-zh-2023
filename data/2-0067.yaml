- en: 3D Generative Modeling with DeepSDF
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用DeepSDF进行3D生成建模
- en: 原文：[https://towardsdatascience.com/3d-generative-modeling-with-deepsdf-2cd06f1ec9b3](https://towardsdatascience.com/3d-generative-modeling-with-deepsdf-2cd06f1ec9b3)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/3d-generative-modeling-with-deepsdf-2cd06f1ec9b3](https://towardsdatascience.com/3d-generative-modeling-with-deepsdf-2cd06f1ec9b3)
- en: Simple neural networks can capture complex 3D geometries
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单的神经网络可以捕捉复杂的3D几何形状
- en: '[](https://wolfecameron.medium.com/?source=post_page-----2cd06f1ec9b3--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----2cd06f1ec9b3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2cd06f1ec9b3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2cd06f1ec9b3--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----2cd06f1ec9b3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page-----2cd06f1ec9b3--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----2cd06f1ec9b3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2cd06f1ec9b3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2cd06f1ec9b3--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----2cd06f1ec9b3--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2cd06f1ec9b3--------------------------------)
    ·10 min read·Jan 30, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2cd06f1ec9b3--------------------------------)
    ·阅读时间10分钟·2023年1月30日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/bf966799f188a362f2dfb88b1ea03ac4.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bf966799f188a362f2dfb88b1ea03ac4.png)'
- en: (Photo by [Milad Fakurian](https://unsplash.com/@fakurian?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/3D-shape?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: （照片由 [Milad Fakurian](https://unsplash.com/@fakurian?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来源于 [Unsplash](https://unsplash.com/s/photos/3D-shape?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)）
- en: 'Prior research in computer graphics and 3D computer vision has proposed numerous
    approaches for representing 3D shapes. Such methods are useful for:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机图形学和3D计算机视觉领域的先前研究提出了多种表示3D形状的方法。这些方法对以下方面有用：
- en: storing memory-efficient representations of known shapes
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 存储内存高效的已知形状表示
- en: generating new shapes
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成新形状
- en: fixing/reconstructing shapes based on limited or noisy data
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于有限或噪声数据修复/重建形状
- en: Beyond classical approaches, deep learning — or, more specifically, generative
    neural networks — can be used to represent 3D shapes. To do this, we can train
    a neural network to output a representation of a 3D shape, allowing representations
    for a variety of shapes to be indirectly stored within the weights of the neural
    network. Then, we can query this neural network to produce new shapes.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 超越传统方法，深度学习——更具体地说，生成性神经网络——可以用来表示3D形状。为此，我们可以训练一个神经网络来输出3D形状的表示，从而将多种形状的表示间接存储在神经网络的权重中。然后，我们可以查询这个神经网络来生成新形状。
- en: 'Within this post, we will study one of such methods, called DeepSDF [1], that
    uses a simple, feed-forward neural network to learn signed distance function (SDF)
    representations for a variety of 3D shapes. The basic idea is simple: instead
    of directly encoding a geometry (e.g., via a mesh), we train a generative neural
    network to output this geometry. Then, we can perform inference to *(i)* obtain
    the direct encoding of a (potentially new) 3D shape or *(ii)* fix/reconstruct
    a 3D shape from noisy data.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将深入研究其中一种方法，称为DeepSDF [1]，它使用一个简单的前馈神经网络来学习多种3D形状的符号距离函数（SDF）表示。基本思想很简单：我们不是直接编码几何体（例如，通过网格），而是训练一个生成性神经网络来输出这些几何体。然后，我们可以进行推理以*(i)*
    获取（潜在的新）3D形状的直接编码，或*(ii)* 从噪声数据中修复/重建一个3D形状。
- en: '![](../Images/a58e91d767aa16f117af0e79fa99b2ad.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a58e91d767aa16f117af0e79fa99b2ad.png)'
- en: (from [1])
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: Background
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: Before diving into how DeepSDF works, there are a few background concepts that
    we will need to understand. First, we’ll talk a bit about how 3D shapes are usually
    represented, as well as how a signed distance function (SDF) can be used to represent
    a 3D shape. Then, we’ll talk about feed-forward neural networks, an incredibly
    simple deep learning architecture that is used heavily by research in 3D modeling
    of shapes.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解DeepSDF的工作原理之前，我们需要理解一些背景概念。首先，我们将讨论一下3D形状通常是如何表示的，以及签名距离函数（SDF）如何用于表示3D形状。然后，我们将讨论前馈神经网络，这是一种非常简单的深度学习架构，在3D形状建模的研究中被广泛使用。
- en: Representing 3D shapes
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 表示3D形状
- en: 'When considering how to store a 3D shape in a computer, we have three options:
    a point cloud, mesh, or voxels. Each of these representations have different benefits
    and limitations, but they are all valid methods of directly representing a 3D
    shape. Let’s get a basic idea of how they work.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑如何在计算机中存储3D形状时，我们有三个选项：点云、网格或体素。这些表示方法各有不同的优缺点，但都是直接表示3D形状的有效方法。让我们简单了解它们的工作原理。
- en: '**Point cloud.** Point clouds are pretty easy to understand. As we might infer
    from the name, they just store a group of points with `[x, y, z]` coordinates
    in space, and these points are used to represent an underlying geometry. Point
    clouds are useful because they closely match the type of data we would get from
    sensors like LiDAR or depth-sensing cameras. But, point clouds do not provide
    a [watertight](https://davidstutz.de/a-formal-definition-of-watertight-meshes/)
    surface (i.e., a shape with one, closed surface).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**点云。** 点云相对容易理解。顾名思义，它们只是存储空间中一组具有 `[x, y, z]` 坐标的点，这些点用于表示一个潜在的几何形状。点云非常有用，因为它们与我们从LiDAR或深度传感器相机等传感器中获得的数据非常接近。但点云无法提供[完全封闭](https://davidstutz.de/a-formal-definition-of-watertight-meshes/)的表面（即具有一个封闭表面的形状）。'
- en: '**Mesh.** One 3D representation that can provide a watertight surface is a
    mesh. Meshes are 3D shape representations based upon collections of vertices,
    edges, and faces that describe an underlying shape. Put simply, a mesh is just
    a list of polygons (e.g., triangles) that, when stitched together, form a 3D geometry.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**网格。** 一种可以提供封闭表面的3D表示方法是网格。网格是基于顶点、边和面集合的3D形状表示，用于描述一个潜在的形状。简单来说，网格就是一系列多边形（例如三角形），这些多边形拼接在一起形成一个3D几何形状。'
- en: '**Voxel-based representation.** Voxels are just pixels with volume. Instead
    of a pixel in a 2D image, we have a voxel (i.e., a cube) in 3D space. To represent
    a 3D shape with voxels, we can:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于体素的表示。** 体素只是具有体积的像素。在2D图像中的像素在3D空间中被称为体素（即立方体）。要使用体素表示3D形状，我们可以：'
- en: Divide a section of 3D space into discrete voxels
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将3D空间的一个部分划分为离散体素
- en: Identify whether each voxel is filled or not
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定每个体素是否被填充
- en: Using this simple technique, we can construct a voxel-based 3D object. To get
    a more accurate representation, we can just increase the number of voxels that
    we use, forming a finer discretization of 3D space. See below for an illustration
    of the difference between point clouds, meshes, and voxels.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种简单的技术，我们可以构建一个基于体素的3D对象。为了获得更准确的表示，我们可以简单地增加使用的体素数量，从而形成更细致的3D空间离散化。请参见下文了解点云、网格和体素之间的差异插图。
- en: '![](../Images/6b32481e536dce4eebfaf04aebb7df3f.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b32481e536dce4eebfaf04aebb7df3f.png)'
- en: (from [3])
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: （来自[3]）
- en: Signed distance functions
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 签名距离函数
- en: Directly storing a 3D shape using a point cloud, mesh, or voxels requires a
    lot of memory. Instead, we will usually want to store an indirect representation
    of the shape that’s more efficient. One approach for this would be to use a signed
    distance function (SDF).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 直接使用点云、网格或体素存储3D形状需要大量内存。相反，我们通常希望存储一种更高效的间接表示方法。一种方法是使用签名距离函数（SDF）。
- en: Given a spatial `[x, y, z]` point as input, SDFs will output the distance from
    that point to the nearest surface of the underlying object being represented.
    The sign of the SDF’s output indicates whether that spatial point is inside (negative)
    or outside (positive) of the object’s surface. See the equation below.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个空间 `[x, y, z]` 点作为输入，SDF将输出该点到所表示对象的最近表面的距离。SDF输出的符号表示该空间点在对象表面内（负值）还是外（正值）。请参见下面的方程。
- en: '![](../Images/5b5e3604c4a47b01bcbe0f35908fcf33.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b5e3604c4a47b01bcbe0f35908fcf33.png)'
- en: (created by author)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: （由作者创建）
- en: We can identify the surface of a 3D object by finding the locations at which
    the SDF is equal to zero, indicating that a given point is at the boundary of
    the object. After finding this surface using the SDF, we can generate a mesh by
    using algorithms like [Marching Cubes](https://graphics.stanford.edu/~mdfisher/MarchingCubes.html#:~:text=Marching%20cubes%20is%20a%20simple,a%20region%20of%20the%20function.).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过找到 SDF 等于零的位置来识别 3D 物体的表面，这表明某个点位于物体的边界。通过使用 SDF 找到这个表面后，我们可以通过使用类似 [Marching
    Cubes](https://graphics.stanford.edu/~mdfisher/MarchingCubes.html#:~:text=Marching%20cubes%20is%20a%20simple,a%20region%20of%20the%20function.)
    的算法生成网格。
- en: '**Why is this useful?** At a high level, SDFs allow us to store a function
    instead of a direct representation of the 3D shape. This function is likely more
    efficient to store, and we can use is to recover a mesh representation anyways!'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**这有什么用？** 从高层次看，SDF 允许我们存储一个函数，而不是 3D 形状的直接表示。这种函数可能更高效地存储，并且我们仍然可以用它来恢复网格表示！'
- en: Feed-forward neural networks
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前馈神经网络
- en: 'Many highly-accurate methods for modeling 3D shapes are based upon feed-forward
    network architectures. Such an architecture takes a vector as input and applies
    the same two transformations within each of the network’s layers:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 许多高精度的 3D 形状建模方法基于前馈网络架构。这样的架构将一个向量作为输入，并在网络的每一层内应用相同的两个变换：
- en: Linear transformation
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 线性变换
- en: Non-linear activation function
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 非线性激活函数
- en: 'Though the dimension of our input is fixed, two aspects of the network architecture
    are free for us to choose: the hidden dimension and the number of layers. Variables
    like this that we, as practitioners, are expected to set are called *hyperparameters*.
    The correct setting of these hyperparameters depends upon the problem and/or application
    we are trying to solve.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们输入的维度是固定的，但网络架构的两个方面是我们可以选择的：隐藏维度和层数。像这样的变量，我们作为实践者需要设置，称为 *超参数*。这些超参数的正确设置取决于我们试图解决的问题和/或应用。
- en: '**The code.** There is not much complexity to feed-forward networks. We can
    implement them easily in PyTorch as shown below.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码。** 前馈网络的复杂性不大。我们可以像下面展示的那样在 PyTorch 中轻松实现它们。'
- en: '[DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation](https://arxiv.org/abs/1901.05103)
    [1]'
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[DeepSDF: 学习连续有符号距离函数以进行形状表示](https://arxiv.org/abs/1901.05103) [1]'
- en: '![](../Images/0e5feb52af5461139cd125a5957fc1b5.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e5feb52af5461139cd125a5957fc1b5.png)'
- en: (from [1])
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: Prior research in computer graphics and 3D computer vision has proposed numerous
    classical approaches for representing 3D shapes and geometries. In [1], authors
    propose a deep learning-based approach, called DeepSDF, that uses a neural network
    to learn a continuous SDF for a broad class of shapes. Put simply, this means
    that we can encode a SDF-based representation of multiple different types of 3D
    shapes using a single, feed-forward neural network, allowing such shapes to be
    represented, interpolated or even completed from partial data; see above.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机图形学和 3D 计算机视觉领域，先前的研究提出了许多经典的方法来表示 3D 形状和几何体。在 [1] 中，作者提出了一种基于深度学习的方法，称为
    DeepSDF，该方法使用神经网络来学习广泛类别形状的连续 SDF。简单来说，这意味着我们可以通过一个单一的前馈神经网络来编码基于 SDF 的多种不同类型的
    3D 形状，从而使这些形状能够被表示、插值甚至从部分数据中完成；见上文。
- en: 'The idea behind DeepSDF is simple: we want to use a neural network to perform
    [regression](/regression-explained-in-simple-terms-dccbcad96f61) directly on the
    values of an SDF. To do this, we train this model over point samples from the
    SDF (i.e., individual `[x, y, z]` points with an associated SDF value). If we
    train a network in this way, then we can easily predict the SDF values of query
    positions, as well as recover a shape’s surface by finding the points at which
    the SDF is equal to zero.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: DeepSDF 的核心思想很简单：我们希望使用神经网络直接对 SDF 的值进行 [回归](/regression-explained-in-simple-terms-dccbcad96f61)。为此，我们通过
    SDF 的点样本（即具有相关 SDF 值的单个 `[x, y, z]` 点）来训练该模型。如果我们以这种方式训练网络，我们就可以轻松预测查询位置的 SDF
    值，并通过找到 SDF 等于零的点来恢复形状的表面。
- en: '**How do we represent the shape?** More specifically, consider a single shape,
    from which we sample a fixed number of 3D point samples with SDF values. We should
    note here that taking more point samples would allow the shape to be represented
    with higher-precision, but this comes at the cost of increased compute costs.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们如何表示形状？** 更具体地说，考虑一个单一形状，从中我们采样固定数量的 3D 点样本及其 SDF 值。我们应该注意，采样更多的点将使形状的表示更加精确，但这会增加计算成本。'
- en: '![](../Images/c63a942a3eea71aab66db0185441de8d.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c63a942a3eea71aab66db0185441de8d.png)'
- en: (created by author)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: （由作者创作）
- en: In the equation above, `x` is a vector containing `[x, y, z]` coordinates, while
    `s` is the SDF value associated with these coordinates for a given shape.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的方程中，`x` 是一个包含 `[x, y, z]` 坐标的向量，而 `s` 是与这些坐标相关联的给定形状的 SDF 值。
- en: '**Training the neural network.** From here, we can directly train a feed-forward
    neural network to produce the SDF value `s` given `x` as input by training over
    these sample pairs using an [L1 regression loss](https://amitshekhar.me/blog/l1-and-l2-loss-functions).
    Then, the resulting model can output accurate SDF values to represent the underlying
    shape; see the left subfigure below.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练神经网络。** 从这里，我们可以直接训练一个前馈神经网络，以 `x` 作为输入，利用这些样本对进行训练，从而生成 SDF 值 `s`，使用 [L1
    回归损失](https://amitshekhar.me/blog/l1-and-l2-loss-functions)。然后，结果模型可以输出准确的 SDF
    值来表示底层形状；见下图左侧子图。'
- en: '![](../Images/c7ed73dccc5f8ffdc416ec8d50c8ae21.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7ed73dccc5f8ffdc416ec8d50c8ae21.png)'
- en: (from [1])
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: The limitation of such a model is that it only represents a single shape. Ideally,
    we would want to model a variety of shapes with a single neural network. To accomplish
    this, we can associate a latent vector (i.e., “Code” in the figure above) with
    each shape. This is a low-dimensional vector that is unique to each shape that
    is stored within our neural network. This latent vector can be added as an input
    to the neural network to inform the network that it is producing output for a
    particular shape. This simple trick allows us to represent multiple shapes within
    a single model (this saves a lot of memory!); see the right subfigure above.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模型的局限性在于它仅表示单一形状。理想情况下，我们希望用一个神经网络建模多种形状。为此，我们可以将一个潜在向量（即上图中的“Code”）与每个形状关联。这是一个对每个形状唯一的低维向量，存储在我们的神经网络中。可以将这个潜在向量作为输入添加到神经网络中，以告知网络它正在为特定形状生成输出。这一简单技巧允许我们在一个模型中表示多个形状（这节省了大量内存！）；见上图右侧子图。
- en: 'The final question we might be asking is: *how do we obtain this latent vector
    for each shape?* In [1], the authors do this by proposing an auto-decoder architecture
    that *(i)* adds the latent vector to the model’s input and *(ii)* learns the best
    latent vector for each shape via gradient descent during training; see below.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会问的最终问题是：*我们如何为每个形状获取这个潜在向量？* 在 [1] 中，作者通过提出一个自解码器架构来实现这一点，该架构 *(i)* 将潜在向量添加到模型的输入中，并
    *(ii)* 在训练过程中通过梯度下降学习每个形状的最佳潜在向量；见下文。
- en: '![](../Images/172277acb3e98f3e0d6fef31677e0969.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/172277acb3e98f3e0d6fef31677e0969.png)'
- en: (from [1])
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: Typically, latent vectors are learned via an [autoencoder architecture](https://www.jeremyjordan.me/autoencoders/),
    but this requires the addition of an extra encoder module that incurs extra computational
    expense. The authors in [1] propose the auto-decoder approach to avoid this extra
    compute. The difference between these approaches is shown below.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，潜在向量是通过 [自编码器架构](https://www.jeremyjordan.me/autoencoders/) 学习的，但这需要额外的编码器模块，增加了额外的计算开销。作者在
    [1] 中提出了自解码器方法以避免这种额外计算。这些方法之间的区别如下所示。
- en: '**Producing a shape.** To perform inference with DeepSDF, we must:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成形状。** 要使用 DeepSDF 进行推理，我们必须：'
- en: Start with a sparse/incomplete set of SDF value samples
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从稀疏/不完整的 SDF 值样本开始
- en: Determine the best possible latent vector from these samples
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这些样本中确定最佳的潜在向量
- en: Perform inference with our trained neural network over a bunch of different
    points in 3D space to determine SDF values
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们训练好的神经网络对 3D 空间中的一系列不同点进行推理，以确定 SDF 值
- en: From here, we can visualize the shape represented by DeepSDF with algorithms
    like [Marching Cubes](https://graphics.stanford.edu/~mdfisher/MarchingCubes.html#:~:text=Marching%20cubes%20is%20a%20simple,a%20region%20of%20the%20function.)
    that discretize 3D space and extract an actual 3D geometry based on these SDF
    values.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，我们可以使用像 [Marching Cubes](https://graphics.stanford.edu/~mdfisher/MarchingCubes.html#:~:text=Marching%20cubes%20is%20a%20simple,a%20region%20of%20the%20function.)
    这样的算法来可视化由 DeepSDF 表示的形状，这些算法对 3D 空间进行离散化，并基于这些 SDF 值提取实际的 3D 几何形状。
- en: '**The data.** DeepSDF is trained and evaluated using the synthetic [ShapeNet](https://shapenet.org/)
    dataset. In particular, its performance is measured across four tasks.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据。** DeepSDF 是使用合成的 [ShapeNet](https://shapenet.org/) 数据集进行训练和评估的。特别地，它的性能在四个任务中得到了衡量。'
- en: Representing shapes in the training set
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练集中的形状表示
- en: Reconstructing unseen (test) shapes
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重建未见（测试）形状
- en: Completing partial shapes
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成部分形状
- en: Sampling new shapes from the latent space
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从潜在空间采样新形状
- en: For the first three tasks, we see that DeepSDF tends to outperform baseline
    methodologies consistently, revealing that it can represent complex shapes with
    high accuracy and even recover shapes from incomplete samples quite well. This
    is quite remarkable given that we are storing numerous 3D shapes within a single,
    memory-efficient neural network; see below.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前三个任务，我们发现 DeepSDF 一直优于基线方法，这表明它能够以高精度表示复杂形状，甚至能够从不完整的样本中较好地恢复形状。考虑到我们在一个单一且节省内存的神经网络中存储了大量的
    3D 形状，这一点尤为显著；见下图。
- en: '![](../Images/0666b62f1f67ddd8099f8ca004ac4727.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0666b62f1f67ddd8099f8ca004ac4727.png)'
- en: (from [1])
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: We can also interpolate the embedding space of a DeepSDF model to produce coherent
    results. This allows us to do things like find the average shape between a truck
    and car; see below.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以对 DeepSDF 模型的嵌入空间进行插值，以生成连贯的结果。这使我们能够做一些事情，比如找到卡车和汽车之间的平均形状；见下图。
- en: '![](../Images/e586b80d8563ced0ec97ee57faf7b65e.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e586b80d8563ced0ec97ee57faf7b65e.png)'
- en: (from [1])
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: From these results, we can see that interpolation between latent vectors yields
    a smooth transition between shapes, revealing that the continuous SDFs embedded
    by DeepSDF are meaningful! Common features of shapes — such as truck beds or arms
    of chairs — are captured within the representation leveraged by DeepSDF. This
    is quite remarkable for such a simple, feed-forward network.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些结果中，我们可以看到潜在向量之间的插值产生了形状之间的平滑过渡，这表明 DeepSDF 嵌入的连续 SDF 是有意义的！形状的常见特征 —— 如卡车车厢或椅子扶手
    —— 都被 DeepSDF 利用的表示所捕捉。这对于这样一个简单的前馈网络来说，实在是非常了不起。
- en: Takeaways
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收获
- en: DeepSDF is a feed-forward, generative neural network that we can use to represent
    and manipulate 3D shapes. Using this model, we can easily perform tasks like generate
    the mesh representation of a shape, recover an underlying shape from incomplete
    or noisy data, and even generate a new shape that is an interpolation of known
    geometries. The benefits and limitations of DeepSDF are outlined below.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: DeepSDF 是一个前馈生成神经网络，我们可以用来表示和操作 3D 形状。使用这个模型，我们可以轻松地执行诸如生成形状的网格表示、从不完整或噪声数据中恢复基本形状，甚至生成一个新的形状，这个新形状是已知几何形状的插值。以下列出了
    DeepSDF 的优点和局限性。
- en: '**Lots of compression.** To store 3D geometries in a computer, we can use mesh
    or voxel representations. To avoid the memory overhead of directly storing shapes
    like this, we can use a generative models like DeepSDF. With such an approach,
    we no longer need the direct mesh encoding of a geometry. Instead, we can use
    DeepSDF — a small neural network that is easy to store — to accurately generate
    meshes for a variety of shapes.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**大量压缩。** 要在计算机中存储 3D 几何形状，我们可以使用网格或体素表示。为了避免直接存储这种形状的内存开销，我们可以使用像 DeepSDF
    这样的生成模型。通过这种方法，我们不再需要几何形状的直接网格编码。相反，我们可以使用 DeepSDF —— 一个易于存储的小型神经网络 —— 来准确生成各种形状的网格。'
- en: '**Fixing a broken geometry.** Given partial or noisy representation of an underlying
    shape, DeepSDF can be used to recover an accurate mesh; see below. In comparison,
    most prior methods cannot perform such a task — they require access to a full
    3D shape representation that matches the type of data used to train the model.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**修复破损几何形状。** 给定基本形状的部分或噪声表示，DeepSDF 可以用来恢复准确的网格；见下图。相比之下，大多数之前的方法无法执行这种任务
    —— 它们需要访问与用于训练模型的数据类型匹配的完整 3D 形状表示。'
- en: '![](../Images/5627bfd09e800a63e867ecb8013928b7.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5627bfd09e800a63e867ecb8013928b7.png)'
- en: (from [1])
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: '**Interpolating the latent space.** Deep SDF can represent a lot of different
    shapes and embed their properties into a low-dimensional latent space. Plus, experiments
    show that this latent space is meaningful and has good coverage. Practically,
    this means that we can take latent vectors (i.e., vector representations of different
    objects), [linearly interpolate](https://en.wikipedia.org/wiki/Linear_interpolation)
    between them, and produce a valid, novel shape. We can easily use this to generate
    new shapes that have a variety of interesting properties.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**插值潜在空间。** Deep SDF 能够表示许多不同的形状，并将它们的属性嵌入到低维潜在空间中。此外，实验表明，这个潜在空间是有意义的，并且具有良好的覆盖范围。实际上，这意味着我们可以在潜在向量（即，不同对象的向量表示）之间进行
    [线性插值](https://en.wikipedia.org/wiki/Linear_interpolation)，并生成有效的新形状。我们可以轻松地利用这一点生成具有各种有趣属性的新形状。'
- en: '**Limitations.** DeepSDF is great, but it always requires access to a (possibly
    noisy or incomplete) 3D geometry to run inference. Plus, searching for the best
    possible latent vector (i.e., this must always be done before performing inference
    due to the auto-decoder approach) is computationally expensive. In this way, the
    inference abilities of DeepSDF are somewhat limited. To summarize, the approach
    is slow and cannot generate new shapes from scratch, which leaves room for improvement
    in future work.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**局限性。** DeepSDF 非常出色，但它总是需要访问（可能是噪声或不完整的）3D 几何体来进行推断。此外，寻找最佳潜在向量（即，由于自动解码器方法，这必须始终在进行推断之前完成）计算成本很高。因此，DeepSDF
    的推断能力在某种程度上是有限的。总结来说，该方法较慢，无法从头生成新形状，这为未来的改进留下了空间。'
- en: Closing remarks
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结束语
- en: Thanks so much for reading this article. I am [Cameron R. Wolfe](https://cameronrwolfe.me/),
    a research scientist at [Alegion](https://www.alegion.com/) and PhD student at
    Rice University studying the empirical and theoretical foundations of deep learning.
    You can also check out my [other writings](https://medium.com/@wolfecameron) on
    medium! If you liked it, please follow me on [twitter](https://twitter.com/cwolferesearch)
    or subscribe to my [Deep (Learning) Focus newsletter](https://cameronrwolfe.substack.com/),
    where I write series of understandable overviews on important deep learning topics.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢您阅读本文。我是 [Cameron R. Wolfe](https://cameronrwolfe.me/)，一名在 [Alegion](https://www.alegion.com/)
    工作的研究科学家，同时也是赖斯大学的博士生，研究深度学习的经验和理论基础。您还可以查看我在 medium 上的 [其他文章](https://medium.com/@wolfecameron)！如果您喜欢，请在
    [twitter](https://twitter.com/cwolferesearch) 上关注我，或者订阅我的 [Deep (Learning) Focus
    新闻通讯](https://cameronrwolfe.substack.com/)，我在其中撰写关于重要深度学习主题的易于理解的概述系列。
- en: Bibliography
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Park, Jeong Joon, et al. “Deepsdf: Learning continuous signed distance
    functions for shape representation.” *Proceedings of the IEEE/CVF conference on
    computer vision and pattern recognition*. 2019.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Park, Jeong Joon 等. “Deepsdf: 学习用于形状表示的连续符号距离函数。” *IEEE/CVF计算机视觉与模式识别会议论文集*.
    2019.'
- en: '[2] Mildenhall, Ben, et al. “Nerf: Representing scenes as neural radiance fields
    for view synthesis.” *Communications of the ACM* 65.1 (2021): 99–106.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Mildenhall, Ben 等. “Nerf: 将场景表示为神经辐射场以进行视图合成。” *ACM通讯* 65.1 (2021): 99–106.'
- en: '[3] Hoang, Long, et al. “A deep learning method for 3D object classification
    using the wave kernel signature and a center point of the 3D-triangle mesh.” *Electronics*
    8.10 (2019): 1196.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Hoang, Long 等. “一种使用波形核签名和3D三角网中心点的深度学习方法进行3D对象分类。” *电子学* 8.10 (2019):
    1196.'
