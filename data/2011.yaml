- en: Fine-tune MPT-7B on Amazon SageMaker
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Amazon SageMaker 上微调 MPT-7B
- en: 原文：[https://towardsdatascience.com/fine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa?source=collection_archive---------5-----------------------#2023-06-20](https://towardsdatascience.com/fine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa?source=collection_archive---------5-----------------------#2023-06-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/fine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa?source=collection_archive---------5-----------------------#2023-06-20](https://towardsdatascience.com/fine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa?source=collection_archive---------5-----------------------#2023-06-20)
- en: '![](../Images/2f5691f2830e17d1af6c907a144f4567.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f5691f2830e17d1af6c907a144f4567.png)'
- en: Photo by [Jeffery Ho](https://unsplash.com/@jefferyho?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Jeffery Ho](https://unsplash.com/@jefferyho?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)。
- en: Learn how to prepare a dataset and create a training job to fine-tune MPT-7B
    on Amazon SageMaker
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解如何准备数据集并创建训练作业，以在 Amazon SageMaker 上微调 MPT-7B
- en: '[](https://medium.com/@joao.pereira.abt?source=post_page-----1e68e71051fa--------------------------------)[![João
    Pereira](../Images/2946b185eb134ddfaa71cf5af5e3cda6.png)](https://medium.com/@joao.pereira.abt?source=post_page-----1e68e71051fa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1e68e71051fa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1e68e71051fa--------------------------------)
    [João Pereira](https://medium.com/@joao.pereira.abt?source=post_page-----1e68e71051fa--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@joao.pereira.abt?source=post_page-----1e68e71051fa--------------------------------)[![João
    Pereira](../Images/2946b185eb134ddfaa71cf5af5e3cda6.png)](https://medium.com/@joao.pereira.abt?source=post_page-----1e68e71051fa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1e68e71051fa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1e68e71051fa--------------------------------)
    [João Pereira](https://medium.com/@joao.pereira.abt?source=post_page-----1e68e71051fa--------------------------------)'
- en: ·
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6743ea128017&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa&user=Jo%C3%A3o+Pereira&userId=6743ea128017&source=post_page-6743ea128017----1e68e71051fa---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1e68e71051fa--------------------------------)
    ·9 min read·Jun 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1e68e71051fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa&user=Jo%C3%A3o+Pereira&userId=6743ea128017&source=-----1e68e71051fa---------------------clap_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6743ea128017&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa&user=Jo%C3%A3o+Pereira&userId=6743ea128017&source=post_page-6743ea128017----1e68e71051fa---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1e68e71051fa--------------------------------)
    ·9分钟阅读·2023年6月20日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1e68e71051fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa&user=Jo%C3%A3o+Pereira&userId=6743ea128017&source=-----1e68e71051fa---------------------clap_footer-----------)'
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1e68e71051fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa&source=-----1e68e71051fa---------------------bookmark_footer-----------)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1e68e71051fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa&source=-----1e68e71051fa---------------------bookmark_footer-----------)'
- en: New large language models (LLMs) are being announced every week, each trying
    to beat its predecessor and take over the evaluation leaderboards. One of the
    latest models out there is [MPT-7B](https://www.mosaicml.com/blog/mpt-7b) that
    was released by MosaicML. Unlike other models of its kind, this 7-billion-parameter
    model is open-source and licensed for commercial use ([Apache 2.0 license](https://github.com/mosaicml/llm-foundry/blob/main/LICENSE))
    🚀.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 每周都有新的大型语言模型（LLMs）发布，每个模型都试图超越其前任并登上评估排行榜。最新发布的模型之一是 [MPT-7B](https://www.mosaicml.com/blog/mpt-7b)，由
    MosaicML 发布。与其他同类模型不同的是，这款拥有70亿参数的模型是开源的，并且具有商业使用许可（[Apache 2.0 许可证](https://github.com/mosaicml/llm-foundry/blob/main/LICENSE)）🚀。
- en: Foundation models like MPT-7B are pre-trained on datasets with trillions of
    tokens (100 tokens ~ 75 words) crawled from the web and, when prompted well, they
    can produce impressive outputs. However, to truly unlock the value of large language
    models in real-world applications, smart prompt-engineering might not be enough
    to make them work for your use case and, therefore, fine-tuning a foundation model
    on a domain-specific dataset is required.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 类似 MPT-7B 的基础模型是在从网络爬取的拥有万亿个标记（100 tokens ~ 75 words）的数据集上进行预训练的，当提示得当时，它们能够生成令人印象深刻的输出。然而，要真正发掘大语言模型在实际应用中的价值，仅仅依靠智能提示工程可能不足以使其适应你的用例，因此，需要在特定领域的数据集上对基础模型进行微调。
- en: LLMs have billions of parameters and, consequently, fine-tuning such large models
    is challenging. Good news is that fine-tuning is much cheaper and faster as compared
    to pre-training the foundation model given that 1) the domain-specific datasets
    are "small" and 2) fine-tuning requires only a few passes over the training data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型（LLMs）具有数十亿个参数，因此，微调如此大型的模型是具有挑战性的。好消息是，相较于对基础模型进行预训练，微调的成本和时间都要便宜得多，因为
    1) 特定领域的数据集是“较小”的，2) 微调只需对训练数据进行少量遍历。
