- en: Learn to Unlearn Machines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学会“遗忘”机器
- en: 原文：[https://towardsdatascience.com/learn-to-unlearn-machines-6b0843dfc40f?source=collection_archive---------5-----------------------#2023-11-23](https://towardsdatascience.com/learn-to-unlearn-machines-6b0843dfc40f?source=collection_archive---------5-----------------------#2023-11-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/learn-to-unlearn-machines-6b0843dfc40f?source=collection_archive---------5-----------------------#2023-11-23](https://towardsdatascience.com/learn-to-unlearn-machines-6b0843dfc40f?source=collection_archive---------5-----------------------#2023-11-23)
- en: A data-driven approach to machine unlearning of generative language models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一种基于数据的方法来进行生成语言模型的机器遗忘
- en: '[](https://medium.com/@suxodolskaya?source=post_page-----6b0843dfc40f--------------------------------)[![Evgeniya
    Sukhodolskaya](../Images/2d7cd9aa6b106fefa2ae598a4255ec10.png)](https://medium.com/@suxodolskaya?source=post_page-----6b0843dfc40f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6b0843dfc40f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6b0843dfc40f--------------------------------)
    [Evgeniya Sukhodolskaya](https://medium.com/@suxodolskaya?source=post_page-----6b0843dfc40f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@suxodolskaya?source=post_page-----6b0843dfc40f--------------------------------)[![Evgeniya
    Sukhodolskaya](../Images/2d7cd9aa6b106fefa2ae598a4255ec10.png)](https://medium.com/@suxodolskaya?source=post_page-----6b0843dfc40f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6b0843dfc40f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6b0843dfc40f--------------------------------)
    [Evgeniya Sukhodolskaya](https://medium.com/@suxodolskaya?source=post_page-----6b0843dfc40f--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fab8927d88a52&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-to-unlearn-machines-6b0843dfc40f&user=Evgeniya+Sukhodolskaya&userId=ab8927d88a52&source=post_page-ab8927d88a52----6b0843dfc40f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6b0843dfc40f--------------------------------)
    ·7 min read·Nov 23, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6b0843dfc40f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-to-unlearn-machines-6b0843dfc40f&user=Evgeniya+Sukhodolskaya&userId=ab8927d88a52&source=-----6b0843dfc40f---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fab8927d88a52&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-to-unlearn-machines-6b0843dfc40f&user=Evgeniya+Sukhodolskaya&userId=ab8927d88a52&source=post_page-ab8927d88a52----6b0843dfc40f---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6b0843dfc40f--------------------------------)
    ·7分钟阅读·2023年11月23日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6b0843dfc40f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-to-unlearn-machines-6b0843dfc40f&user=Evgeniya+Sukhodolskaya&userId=ab8927d88a52&source=-----6b0843dfc40f---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6b0843dfc40f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-to-unlearn-machines-6b0843dfc40f&source=-----6b0843dfc40f---------------------bookmark_footer-----------)![](../Images/54886bda691ab0a30577746d91efa2f7.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6b0843dfc40f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-to-unlearn-machines-6b0843dfc40f&source=-----6b0843dfc40f---------------------bookmark_footer-----------)![](../Images/54886bda691ab0a30577746d91efa2f7.png)'
- en: Image generated with [DALLE 3](https://chat.openai.com/)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图像生成自 [DALLE 3](https://chat.openai.com/)
- en: In today’s tech landscape, you’d be hard pressed to find someone who hasn’t
    heard of machine learning. Over the last decade the research field has been so
    trendy that even those outside the industry are now familiar with terms such as
    Artificial Intelligence (AI), Neural Networks (NNs), and Machine Learning (ML).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今的科技领域中，几乎很难找到一个没有听说过机器学习的人。在过去的十年里，研究领域如此流行，以至于即使是行业之外的人也熟悉诸如人工智能（AI）、神经网络（NNs）和机器学习（ML）等术语。
- en: However, when it comes to *machine unlearning*, it seems the legal industry
    has heard more about it than the tech community. The recent boom in Large Language
    Models (LLMs), which in the fast-paced world of IT feels like a decade even if
    it’s only been 1–2 years, has unearthed hundreds of unresolved ethical and legal
    issues related to AI development. Novelists [are suing](https://www.hollywoodreporter.com/business/business-news/authors-sue-meta-openai-class-action-1235588711/)
    OpenAI for using their texts to train GPT models without consent. Twitter is [abuzz
    with critical comments from](https://twitter.com/mattdeitke/status/1638608472525897728)
    artists who believe their works were used in violation of copyright laws. [Complying
    with “the right to be forgotten” has become extremely challenging.](https://arxiv.org/pdf/2307.03941.pdf)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，谈到*机器遗忘*时，似乎法律行业比科技社区听到的更多。最近大型语言模型（LLMs）的迅猛发展，在快速变化的IT世界中感觉像是十年，尽管实际上只有1-2年，揭示了数百个与AI发展相关的未解决的伦理和法律问题。小说家[起诉](https://www.hollywoodreporter.com/business/business-news/authors-sue-meta-openai-class-action-1235588711/)
    OpenAI未经同意使用他们的文本来训练GPT模型。Twitter上[充满了](https://twitter.com/mattdeitke/status/1638608472525897728)艺术家的批评评论，他们认为他们的作品被用于侵犯版权法。[遵守“被遗忘权”变得极其困难。](https://arxiv.org/pdf/2307.03941.pdf)
- en: Much like [AI alignment](/what-is-ai-alignment-2bbbe4633c7f), machine unlearning
    appears to be an overlooked field, given the limited open-sourced solutions available.
    I believe that machine unlearning exploration should be encouraged and popularized,
    especially considering that the current laws and ethical norms surrounding AI
    usage are underdeveloped and severely lack mechanisms for data protection. In
    this article, I would like to suggest some practical improvements to [one of the
    first applied unlearning techniques for generative language models.](https://browse.arxiv.org/pdf/2310.02238.pdf)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 与[AI对齐](/what-is-ai-alignment-2bbbe4633c7f)类似，机器遗忘似乎是一个被忽视的领域，因为现有的开源解决方案有限。我相信应该鼓励和推广机器遗忘的探索，特别是考虑到目前有关AI使用的法律和伦理规范尚不完善，数据保护机制严重缺失。在本文中，我想提出一些对[生成语言模型早期应用遗忘技术](https://browse.arxiv.org/pdf/2310.02238.pdf)的实际改进建议。
- en: Machine Unlearning
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器遗忘
- en: 'The term “machine unlearning” or “machine forgetting” means exactly what it
    sounds like: it includes techniques designed to erase requested information from
    a machine learning model’s “knowledge storage”. However, it’s far from intuitive
    when you need to consider actual methods to achieve this efficiently in terms
    of time, computational resources, and model performance on the “not unlearned”
    data. An obvious solution is to retrain models from scratch using the initial
    dataset while excluding the “forget set” — but this would be an extremely impractical
    approach to deep neural network unlearning.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: “机器遗忘”或“机器遗忘”一词的意思正如其字面意思所示：它包括旨在从机器学习模型的“知识存储”中删除请求信息的技术。然而，当你需要考虑实际方法以在时间、计算资源和模型在“未遗忘”数据上的性能方面高效实现这一点时，这远非直观。一个明显的解决方案是使用初始数据集从头开始重新训练模型，同时排除“遗忘集”——但对于深度神经网络的遗忘来说，这将是一个极其不切实际的方法。
- en: '![](../Images/7f0bb7fabe14e41fa2c8ccdd811a2f6c.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7f0bb7fabe14e41fa2c8ccdd811a2f6c.png)'
- en: “Machine Unlearning Framework” from “[Survey of Machine Learning](https://arxiv.org/pdf/2209.02299.pdf)”
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: “机器遗忘框架”来自于“[机器学习调查](https://arxiv.org/pdf/2209.02299.pdf)”
- en: 'The core research findings in the field of machine unlearning are concisely
    compiled in “[A Survey of Machine Unlearning](https://arxiv.org/pdf/2209.02299.pdf)”.
    Another article that covers the basics with accessible explanations is “[Machine
    unlearning: The duty of forgetting](/machine-unlearning-the-duty-of-forgetting-3666e5b9f6e5)”.
    While I personally recommend these resources, you can find a multitude of other
    quality research materials on the subject. Yet in terms of practical applications,
    there remains much to be done.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 机器遗忘领域的核心研究成果被简洁地汇编在“[机器遗忘调查](https://arxiv.org/pdf/2209.02299.pdf)”中。另一篇涵盖基础知识并提供易于理解解释的文章是“[机器遗忘：遗忘的责任](/machine-unlearning-the-duty-of-forgetting-3666e5b9f6e5)”。虽然我个人推荐这些资源，但你可以找到大量其他优质的研究材料。然而，在实际应用方面，仍有许多工作要做。
- en: A promising initiative that might shift this field from theoretical exploration
    to practical application is the [NeurIPS 2023 Machine Unlearning challenge](https://www.kaggle.com/competitions/neurips-2023-machine-unlearning/overview).
    Here, participants compete to create an unlearning algorithm for the ResNet18
    Convolutional Neural Network.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有前景的举措，可能将该领域从理论探索转向实际应用，是[NeurIPS 2023机器遗忘挑战赛](https://www.kaggle.com/competitions/neurips-2023-machine-unlearning/overview)。在这里，参与者竞争以创建一个用于ResNet18卷积神经网络的遗忘算法。
- en: Machine Unlearning of Generative Language Models
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成语言模型的机器遗忘
- en: Considering the widespread accessibility and promotion of generative language
    models to the vast majority of internet users, there’s a critical need for unlearning
    mechanisms. One of the first successful techniques was not so long ago published
    in an open source; you can find the details in “[Who’s Harry Potter? Approximate
    Unlearning in LLMs](https://browse.arxiv.org/pdf/2310.02238.pdf)” by Ronen Eldan
    and Mark Russinovich.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到生成语言模型的广泛可访问性和对大多数互联网用户的推广，迫切需要遗忘机制。最早成功的技术之一不久前在开源中发布；你可以在**罗宁·埃尔丹**和**马克·鲁西诺维奇**的“[谁是哈利·波特？LLM中的近似遗忘](https://browse.arxiv.org/pdf/2310.02238.pdf)”中找到详细信息。
- en: '![](../Images/2540c12c081ec5d6d2265013c3160122.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2540c12c081ec5d6d2265013c3160122.png)'
- en: Image generated with [StableDiffusion](https://stablediffusionweb.com/#demo)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[StableDiffusion](https://stablediffusionweb.com/#demo)生成的图像
- en: The authors use a data augmentation approach for machine unlearning on the [Llama
    2 7b chat model](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) released
    this summer by Meta. The chosen unlearning target, also known as the “forget set”,
    is the Harry Potter saga (ingenious, these muggles!), which is a perfect example
    of machine unlearning due to the possible violation of copyright law. They show
    that with just one GPU hour of fine-tuning, the resulting model is unable to recall
    most of the Harry Potter-related content, while its performance on common benchmarks
    remains almost unaffected.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们对Meta在今年夏天发布的[Llama 2 7b聊天模型](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)采用了一种数据增强方法进行机器遗忘。选择的遗忘目标，也称为“遗忘集”，是哈利·波特系列（真是聪明，这些麻瓜！），这是机器遗忘的一个完美示例，因为它可能涉及版权法的违反。他们展示了只需一小时的GPU微调，得到的模型就无法回忆起大多数与哈利·波特相关的内容，同时其在常见基准上的表现几乎没有受到影响。
- en: Approach overview
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法概述
- en: The main goal of the approach is to make Llama 2 7b forget the linkage between
    entities from a defined forget set (*“*Harry” <is friends with> *“Hermione”*)
    by giving the model plausible generic alternatives (“Harry” <is friends with>
    *“Sally”*). To provide these alternatives as target labels in a **fine-tuning
    dataset**, idiosyncratic terms from the “domain to be forgotten” should be highly
    penalized during the generation of targets. Such penalization could be achieved
    by combining in equation (1) logits generated by a **reinforced model** on the
    original input — Harry Potter books — and by a **baseline model** on a **generic
    translation** of the original input.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法的主要目标是让Llama 2 7b忘记从定义的遗忘集中的实体之间的联系（*“哈利”<是朋友>“赫敏”*），通过给模型提供合理的通用替代（“哈利”<是朋友>
    *“莎莉”*）。为了将这些替代作为**微调数据集**中的目标标签，在生成目标时，“待遗忘领域”中的特殊术语应受到高度惩罚。这种惩罚可以通过将方程（1）中由**强化模型**在原始输入——哈利·波特书籍——上生成的logits与**基线模型**在原始输入的**通用翻译**上生成的logits相结合来实现。
- en: '![](../Images/fafe018e676d86fdbf62bed5da44b3ac.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fafe018e676d86fdbf62bed5da44b3ac.png)'
- en: Equation (1) from “[Who’s Harry Potter? Approximate Unlearning in LLMs](https://browse.arxiv.org/pdf/2310.02238.pdf)”
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: “[谁是哈利·波特？LLM中的近似遗忘](https://browse.arxiv.org/pdf/2310.02238.pdf)”中的方程（1）
- en: The **reinforced model** is Llama 2 7b fine-tuned additionally on Harry Potter
    novels. The **baseline model** is untuned Llama 2 7b. To shift the **baseline
    model**’s output distribution away from the Harry Potter theme, the authors replace
    idiosyncratic terms in the original input with generic ones so the model generates
    a next word based on a context unrelated to the Harry Potter saga. To automate
    such replacements, the authors introduce a dictionary of **anchor terms** — terms
    specific to “Harry Potter” — mapped onto **generic translations***.* The dictionary
    is fully gathered by [GPT-4](https://openai.com/research/gpt-4).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**增强模型**是进一步微调于哈利·波特小说的Llama 2 7b。**基线模型**是未经微调的Llama 2 7b。为了使**基线模型**的输出分布偏离哈利·波特主题，作者将原始输入中的特有术语替换为通用术语，以便模型根据与哈利·波特系列无关的上下文生成下一个词。为了自动化这种替换，作者引入了一个**锚定术语**字典——特定于“哈利·波特”的术语——映射到**通用翻译**上。该字典由[GPT-4](https://openai.com/research/gpt-4)完全收集。'
- en: '![](../Images/3a8e75eb642c69556705cbd281f2de30.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a8e75eb642c69556705cbd281f2de30.png)'
- en: '{‘Anchor Terms’: ‘Generic translations’} from “[Who’s Harry Potter? Approximate
    Unlearning in LLMs](https://browse.arxiv.org/pdf/2310.02238.pdf)”'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '{‘Anchor Terms’: ‘通用翻译’}来自“[谁是哈利·波特？LLMs中的近似遗忘](https://browse.arxiv.org/pdf/2310.02238.pdf)”'
- en: The resulting **fine-tuning dataset** consists of tokenized blocks of text from
    Harry Potter books in a one-to-one mapping to target labels , which are tokens
    corresponding to the maximal entries of the *v_generic* from the equation (1).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的**微调数据集**由来自哈利·波特书籍的标记化文本块组成，与目标标签一一对应，这些目标标签是对应于方程（1）中*v_generic*的最大条目的标记。
- en: '![](../Images/0a7c818dcbbe86c7e511a14891c32c9c.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a7c818dcbbe86c7e511a14891c32c9c.png)'
- en: A piece of the finetuning dataset from “[Who’s Harry Potter? Approximate Unlearning
    in LLMs](https://browse.arxiv.org/pdf/2310.02238.pdf)”
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 来自“[谁是哈利·波特？LLMs中的近似遗忘](https://browse.arxiv.org/pdf/2310.02238.pdf)”的微调数据集的一部分
- en: 'To summarize, the authors describe four steps in the unlearning process:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，作者描述了遗忘过程中的四个步骤：
- en: '![](../Images/7eaa5a59eda23eecd3516e5f77e8031f.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7eaa5a59eda23eecd3516e5f77e8031f.png)'
- en: Machine Unlearning Algorithm from “[Who’s Harry Potter? Approximate Unlearning
    in LLMs](https://browse.arxiv.org/pdf/2310.02238.pdf)”
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 来自“[谁是哈利·波特？LLMs中的近似遗忘](https://browse.arxiv.org/pdf/2310.02238.pdf)”的机器遗忘算法
- en: 'Leveraging the Approach: Key Challenges'
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用该方法：关键挑战
- en: The results of the data augmentation approach are promising, encouraging further
    application in similar tasks. Yet, the authors left some room for improvement
    in several application stages.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强方法的结果很有前景，鼓励在类似任务中进一步应用。然而，作者在几个应用阶段留有改进的空间。
- en: '**Dependency on GPT-4’s existing knowledge:** The algorithm to some extent
    depends on GPT-4’s prior understanding of the Harry Potter series to generate
    generic translations. While the model is expected to have extensive knowledge
    of the Harry Potter realm, a reassessment by fans of the series could provide
    invaluable insights.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**对GPT-4现有知识的依赖：** 该算法在一定程度上依赖GPT-4对哈利·波特系列的先前理解来生成通用翻译。虽然模型预计对哈利·波特领域有广泛的知识，但系列粉丝的重新评估可能提供宝贵的见解。'
- en: '**Challenges with idiosyncratic terms:** Penalizing all unique terms related
    to the series poses an issue. For instance, replacing every instance of ‘Harry’
    with a common name like ‘John’ disrupts the model’s grasp of natural language,
    leading to sentences like, “Harry went up to him and said, ‘Hi, my name is *John*’.”
    To address this, the authors employ the following strategy:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**特有术语的挑战：** 惩罚与系列相关的所有独特术语是一个问题。例如，将每个‘Harry’替换为像‘John’这样的常见名字，会扰乱模型对自然语言的理解，导致句子变成，“Harry走向他，说，‘嗨，我的名字是*John*’。”为解决这个问题，作者采用了以下策略：'
- en: Excluding repeated instances of anchored terms from contributing to the loss
    function beyond their initial occurrence.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排除重复的锚定词实例对损失函数的贡献，超出其初始出现的影响。
- en: Lowering the likelihood of logits connected to translations of terms that have
    appeared before.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低与之前出现过的术语翻译相关的logits的可能性。
- en: However, this strategy also affects the model’s general language comprehension.
    A plausible alternative useful for the fine-tuning dataset would be, for example,
    “Harry went up to him and said, ‘Hi, my name is *Harold*’.”
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种策略也会影响模型的通用语言理解。例如，一个适用于微调数据集的合理替代方案是，“Harry走向他，说，‘嗨，我的名字是*Harold*’。”
- en: '**Evaluation techniques:** The team utilized GPT-4 for an initial evaluation,
    comprising 300 Harry Potter prompt completions, and further analysis of completions.
    Nonetheless, they acknowledged its limitations in accuracy, opting for manual
    inspections of the results for more thorough verification in their final training.
    The authors have not provided insights on how to set up such a manual inspection.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**评估技术：** 团队利用GPT-4进行了初步评估，评估内容包括300个《哈利·波特》提示的完成情况，以及对完成内容的进一步分析。然而，他们承认其准确性存在局限性，因此选择了对结果进行人工检查，以便在最终训练中进行更彻底的验证。作者没有提供如何进行这种人工检查的详细信息。'
- en: Overcoming the Challenges
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 克服挑战
- en: A more effective way to address the key challenges would be a hybrid approach
    that combines human insight and Large Language Models (LLMs).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 解决关键挑战的更有效方法是结合人类洞察力和大型语言模型（LLMs）的混合方法。
- en: In order to harness the collective strengths of human intuition and large language
    models, I have designed three crowdsourcing project interfaces that facilitate
    collaborative labeling using LLMs and the crowd. Each interface designed for human
    labeling is tailored to a challenge listed above.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用人类直觉和大型语言模型的集体优势，我设计了三个众包项目接口，方便使用LLM和人群进行协作标注。每个为人工标注设计的接口都针对上面列出的挑战量身定制。
- en: '**Dependency on GPT-4’s existing knowledge:**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**对GPT-4现有知识的依赖：**'
- en: '![](../Images/8802ee2afad515c63f253e1687081cd9.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8802ee2afad515c63f253e1687081cd9.png)'
- en: Image by author
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Use the *Named Entity Recognition (NER)* to correct GPT-4 NER choices for a
    dictionary of anchor terms. As input, provide the text and GPT-4’s selection of
    terms (you can ask the model to return positions in the text directly), and instruct
    the crowd to correct and complement the selected entities.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*命名实体识别（NER）*来纠正GPT-4对锚定术语词典的NER选择。作为输入，提供文本和GPT-4的术语选择（你可以要求模型直接返回文本中的位置），并指示人群纠正和补充选定的实体。
- en: '**Challenges with Idiosyncratic Terms:**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**处理特殊术语的挑战：**'
- en: '![](../Images/4bee77ece5822db0885119a02587d7dc.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4bee77ece5822db0885119a02587d7dc.png)'
- en: Image by author
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: With the help of a baseline model, check on linguistic correctness prompts with
    completions done by the baseline model on a generic translation of the original
    input. All examples where the baseline model is unsure of an answer (the probability
    of output tokens is below a certain threshold, chosen by you empirically) should
    be sent to a crowdsourcing project with the interface shown on the image.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 借助基线模型，检查语言正确性提示，通过基线模型对原始输入的通用翻译进行完成。所有基线模型对答案不确定的示例（输出标记的概率低于你选择的经验阈值）应发送到显示在图片中的众包项目。
- en: '**Evaluation Techniques:**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**评估技术：**'
- en: '![](../Images/788dbb4ef5f519fe8453843c4ae946e4.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/788dbb4ef5f519fe8453843c4ae946e4.png)'
- en: Image by author
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Manual inspection of the evaluation done by GPT-4 can be designed like in the
    image above.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 可以按照上述图片所示的方式设计GPT-4的评估的人工检查。
- en: Conclusion
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: The authors highlight that, unlike the fictional world of Harry Potter, non-fiction
    areas may not have the same abundance of unique terms, which might make the data
    augmentation approach based on achor terms not applicable. However, if the data
    augmentation techniques outlined in this article fit your project, consider integrating
    the suggested improvements and introducing further your own tweaks. Together,
    we can advance the field of machine unlearning!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 作者指出，与虚构的《哈利·波特》世界不同，非虚构领域可能没有如此丰富的独特术语，这可能使基于锚定术语的数据增强方法不适用。然而，如果本文中概述的数据增强技术适用于你的项目，考虑整合建议的改进并引入你自己的调整。让我们共同推进机器去学习领域的发展！
