- en: CLIP Model and The Importance of Multimodal Embeddings
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CLIP模型及其多模态嵌入的重要性
- en: 原文：[https://towardsdatascience.com/clip-model-and-the-importance-of-multimodal-embeddings-1c8f6b13bf72?source=collection_archive---------1-----------------------#2023-12-11](https://towardsdatascience.com/clip-model-and-the-importance-of-multimodal-embeddings-1c8f6b13bf72?source=collection_archive---------1-----------------------#2023-12-11)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/clip-model-and-the-importance-of-multimodal-embeddings-1c8f6b13bf72?source=collection_archive---------1-----------------------#2023-12-11](https://towardsdatascience.com/clip-model-and-the-importance-of-multimodal-embeddings-1c8f6b13bf72?source=collection_archive---------1-----------------------#2023-12-11)
- en: '[](https://medium.com/@faheemrustamy?source=post_page-----1c8f6b13bf72--------------------------------)[![Fahim
    Rustamy, PhD](../Images/949c8654bd91d03124d0ba95182d8558.png)](https://medium.com/@faheemrustamy?source=post_page-----1c8f6b13bf72--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1c8f6b13bf72--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1c8f6b13bf72--------------------------------)
    [Fahim Rustamy, PhD](https://medium.com/@faheemrustamy?source=post_page-----1c8f6b13bf72--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@faheemrustamy?source=post_page-----1c8f6b13bf72--------------------------------)[![Fahim
    Rustamy, PhD](../Images/949c8654bd91d03124d0ba95182d8558.png)](https://medium.com/@faheemrustamy?source=post_page-----1c8f6b13bf72--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1c8f6b13bf72--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1c8f6b13bf72--------------------------------)
    [Fahim Rustamy, PhD](https://medium.com/@faheemrustamy?source=post_page-----1c8f6b13bf72--------------------------------)'
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F931fc8afcda1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclip-model-and-the-importance-of-multimodal-embeddings-1c8f6b13bf72&user=Fahim+Rustamy%2C+PhD&userId=931fc8afcda1&source=post_page-931fc8afcda1----1c8f6b13bf72---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1c8f6b13bf72--------------------------------)
    ·10 min read·Dec 11, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1c8f6b13bf72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclip-model-and-the-importance-of-multimodal-embeddings-1c8f6b13bf72&user=Fahim+Rustamy%2C+PhD&userId=931fc8afcda1&source=-----1c8f6b13bf72---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F931fc8afcda1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclip-model-and-the-importance-of-multimodal-embeddings-1c8f6b13bf72&user=Fahim+Rustamy%2C+PhD&userId=931fc8afcda1&source=post_page-931fc8afcda1----1c8f6b13bf72---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1c8f6b13bf72--------------------------------)
    ·10分钟阅读·2023年12月11日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1c8f6b13bf72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclip-model-and-the-importance-of-multimodal-embeddings-1c8f6b13bf72&user=Fahim+Rustamy%2C+PhD&userId=931fc8afcda1&source=-----1c8f6b13bf72---------------------clap_footer-----------)'
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1c8f6b13bf72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclip-model-and-the-importance-of-multimodal-embeddings-1c8f6b13bf72&source=-----1c8f6b13bf72---------------------bookmark_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1c8f6b13bf72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclip-model-and-the-importance-of-multimodal-embeddings-1c8f6b13bf72&source=-----1c8f6b13bf72---------------------bookmark_footer-----------)'
- en: CLIP, which stands for Contrastive Language-Image Pretraining, is a deep learning
    model developed by OpenAI in 2021\. CLIP’s embeddings for images and text share
    the same space, enabling direct comparisons between the two modalities. This is
    accomplished by training the model to bring related images and texts closer together
    while pushing unrelated ones apart. This article will explain how CLIP works and
    guide you through an example to train CLIP model using the flikker and COCO datasets.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP，即对比语言-图像预训练，是OpenAI在2021年开发的深度学习模型。CLIP的图像和文本嵌入共享同一空间，使得两种模态之间可以直接进行比较。这是通过训练模型使相关的图像和文本更接近，同时将不相关的图像和文本推远来实现的。本文将解释CLIP的工作原理，并指导你如何使用flikker和COCO数据集训练CLIP模型。
- en: 'You can find the code in this GitHub repo:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这个GitHub仓库中找到代码：
- en: '[https://github.com/RustamyF/clip-multimodal-ml](https://github.com/RustamyF/clip-multimodal-ml)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/RustamyF/clip-multimodal-ml](https://github.com/RustamyF/clip-multimodal-ml)'
- en: '**CLIP’s Applications**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**CLIP的应用**'
- en: 'Some applications of CLIP include:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP的一些应用包括：
- en: 'Image Classification and Retrieval: CLIP can be used for image classification
    tasks by associating images with natural language descriptions. It allows for
    more versatile and flexible image retrieval systems where users can search for
    images using textual queries.'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图像分类和检索：CLIP可以用于图像分类任务，通过将图像与自然语言描述关联起来。它允许更为多样和灵活的图像检索系统，用户可以通过文本查询来搜索图像。
- en: 'Content Moderation: CLIP can be used to moderate content on online platforms
    by analyzing images and accompanying text to identify and filter out inappropriate
    or harmful content.'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 内容审查：CLIP可以通过分析图像及其附带的文本来识别和过滤不适当或有害的内容，从而用于在线平台上的内容审查。
- en: The original CLIP model aimed to unite image and text modalities within a shared
    embedding space. This concept, along with its techniques, extends beyond images
    and text to embrace other modalities. Netflix, in [this blog post](https://netflixtechblog.com/building-in-video-search-936766f0017c),
    trained a model by combining video and text modalities in the common embedding
    space to enhance search within video applications. [Contrastive Language-Audio
    Pretraining (CLAP)](https://arxiv.org/abs/2206.04769) is another model that integrates
    text and audio modalities within the same embedding space, making it valuable
    for improving search functionalities within audio applications.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 原始CLIP模型旨在将图像和文本模态统一到一个共享的嵌入空间中。这个概念及其技术不仅限于图像和文本，还扩展到其他模态。Netflix在[这篇博客文章](https://netflixtechblog.com/building-in-video-search-936766f0017c)中通过在共享嵌入空间中结合视频和文本模态来训练模型，以增强视频应用中的搜索功能。[对比语言-音频预训练
    (CLAP)](https://arxiv.org/abs/2206.04769)是另一种将文本和音频模态集成在同一嵌入空间中的模型，有助于改善音频应用中的搜索功能。
- en: The underlying technology for CLIP is extremely simple but very powerful, opening
    the door for many multi-model machine learning techniques. Meta AI recently released
    [**ImageBind**](https://imagebind.metademolab.com/), which learns a joint embedding
    across six modalities — images, text, audio, depth, thermal, and IMU data. CLIP,
    the first large-scale AI model that accepts two modalities, is a prerequisite
    to understanding ImageBind and other multi-modality AI systems.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP的基础技术非常简单但却非常强大，为许多多模态机器学习技术打开了大门。Meta AI 最近发布了[**ImageBind**](https://imagebind.metademolab.com/)，该技术在六种模态——图像、文本、音频、深度、热成像和IMU数据之间学习联合嵌入。CLIP是第一个接受两种模态的大规模AI模型，它是理解ImageBind和其他多模态AI系统的前提。
- en: '![](../Images/1a5691a4c507c3342a3efbd4537d7ee5.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a5691a4c507c3342a3efbd4537d7ee5.png)'
- en: Imagebind from META AI accepts six different modalities as input (Taken from
    [ImageBind's official GitHub page](https://github.com/facebookresearch/ImageBind)).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: META AI的Imagebind接受六种不同的模态作为输入（取自[ImageBind的官方GitHub页面](https://github.com/facebookresearch/ImageBind)）。
- en: '**What is CLIP**'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**什么是CLIP**'
- en: CLIP is designed to predict which N × N potential (image, text) pairings within
    the batch are actual matches. To achieve this, CLIP establishes a multi-modal
    embedding space through the joint training of an image encoder and text encoder.
    **The CLIP loss aims to maximize the cosine similarity between the image and text
    embeddings for the N genuine pairs in the batch while minimizing the cosine similarity
    for the N² − N incorrect pairings.** The optimization process involves using a
    symmetric cross-entropy loss function that operates on these similarity scores.
    The following presents pseudocode (taken from the original paper) outlining the
    core implementation of CLIP.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP旨在预测批次中的N × N潜在（图像、文本）配对中哪些是真实匹配的。为此，CLIP通过图像编码器和文本编码器的联合训练建立了一个多模态嵌入空间。**CLIP损失的目标是最大化批次中N个真实配对的图像和文本嵌入之间的余弦相似度，同时最小化N²
    − N个错误配对的余弦相似度。** 优化过程涉及使用对称交叉熵损失函数，该函数作用于这些相似度得分。以下展示了伪代码（取自原始论文），概述了CLIP的核心实现。
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here’s a step-by-step description of each line in the pseudo code and its implementation
    using PyTorch:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是每一行伪代码的逐步描述及其在PyTorch中的实现：
- en: '**Model Architecture:**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型架构：**'
- en: 'ClIP uses two separate architectures as the backbone for encoding vision and
    text datasets:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP使用两种独立的架构作为视觉和文本数据集编码的骨干：
- en: '`image_encoder`: Represents the neural network architecture (e.g., ResNet or
    Vision Transformer) responsible for encoding images.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_encoder`：表示负责编码图像的神经网络架构（例如，ResNet或Vision Transformer）。'
- en: '`text_encoder`: Represents the neural network architecture (e.g., CBOW, BERT,
    or Text Transformer) responsible for encoding textual information.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder`：表示神经网络架构（如 CBOW、BERT 或 Text Transformer），负责对文本信息进行编码。'
- en: The original CLIP model was trained from scratch without initializing the image
    encoder and the text encoder with pre-trained weights due to the large volume
    of the dataset (400 million image-text pairs) that they used to train their CLIP
    model. In the example in this blog post, we’ll do things a bit differently. We’ll
    start with pre-trained weights from resnet (for images) and distilbert (for text)
    models to initialize these parts.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 原始 CLIP 模型从头开始训练，没有用预训练权重初始化图像编码器和文本编码器，因为他们用于训练 CLIP 模型的数据集体积巨大（4 亿对图像-文本）。在本博客文章中的示例中，我们会有所不同。我们将从
    resnet（用于图像）和 distilbert（用于文本）模型的预训练权重开始，以初始化这些部分。
- en: '![](../Images/5c97c5a7d2daac2d6759947f8927d4e1.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5c97c5a7d2daac2d6759947f8927d4e1.png)'
- en: Architecture of CLIP model (taken from the original paper)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP 模型的架构（取自原始论文）
- en: '**Input Data:**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入数据：**'
- en: 'The model takes a batch of n pairs of images and texts as input where:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 模型接收一个包含 n 对图像和文本的小批量作为输入，其中：
- en: '`I[n, h, w, c]`: Represents a minibatch of aligned images, where `n` is the
    batch size, `h` is the image height, `w` is the image width, and `c` is the number
    of channels.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`I[n, h, w, c]`：表示对齐图像的小批量，其中 `n` 是批量大小，`h` 是图像高度，`w` 是图像宽度，`c` 是通道数。'
- en: '`T[n, l]`: Represents a minibatch of aligned texts, where `n` is the batch
    size, and `l` is the length of the textual sequence.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`T[n, l]`：表示对齐文本的小批量，其中 `n` 是批量大小，`l` 是文本序列的长度。'
- en: '![](../Images/565a0701a6c33df60c9fb469afa20d20.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/565a0701a6c33df60c9fb469afa20d20.png)'
- en: One batch of image and caption pairs for a batch size of 128
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一批图像和标题对，批量大小为 128
- en: '**Feature Extraction:**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征提取：**'
- en: '`I_f = image_encoder(I)`: Extracts feature representations (`I_f`) from the
    image encoder. The shape of `I_f` is `[n, d_i]`, where `d_i` is the dimensionality
    of the image features.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`I_f = image_encoder(I)`：从图像编码器中提取特征表示（`I_f`）。`I_f` 的形状为 `[n, d_i]`，其中 `d_i`
    是图像特征的维度。'
- en: '`T_f = text_encoder(T)`: Extracts feature representations (`T_f`) from the
    text encoder. The shape of `T_f` is `[n, d_t]`, where `d_t` is the dimensionality
    of the text features.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`T_f = text_encoder(T)`：从文本编码器中提取特征表示（`T_f`）。`T_f` 的形状为 `[n, d_t]`，其中 `d_t`
    是文本特征的维度。'
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Learned Projections:**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习到的投影：**'
- en: '`W_i[d_i, d_e]`: Represents the learned projection matrix for mapping image
    features (`I_f`) to an embedding space (`I_e`). The shape of `W_i` is `[d_i, d_e]`,
    where `d_e` is the desired dimensionality of the joint embedding space.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`W_i[d_i, d_e]`：表示用于将图像特征（`I_f`）映射到嵌入空间（`I_e`）的学习投影矩阵。`W_i` 的形状为 `[d_i, d_e]`，其中
    `d_e` 是联合嵌入空间的期望维度。'
- en: '`W_t[d_t, d_e]`: Represents the learned projection matrix for mapping text
    features (`T_f`) to the same embedding space (`T_e`). The shape of `W_t` is `[d_t,
    d_e]`.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`W_t[d_t, d_e]`：表示用于将文本特征（`T_f`）映射到相同嵌入空间（`T_e`）的学习投影矩阵。`W_t` 的形状为 `[d_t, d_e]`。'
- en: The projection operation can be coded using a neural network with two linear
    layers, whose weights are the **learned projection matrix**. In most cases, the
    projection weights are the only weights with active gradients that can be trained
    on new datasets. Additionally, the projection layer plays a crucial role in aligning
    the dimensions of image and text embeddings, ensuring that they have the same
    size.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 投影操作可以通过一个包含两个线性层的神经网络来编码，这些层的权重就是**学习到的投影矩阵**。在大多数情况下，投影权重是唯一具有活跃梯度的权重，可以在新数据集上进行训练。此外，投影层在对齐图像和文本嵌入的维度方面起着至关重要的作用，确保它们具有相同的大小。
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Embedding and Normalization:**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**嵌入和归一化：**'
- en: '`I_e = l2_normalize(np.dot(I_f, W_i), axis=1)`: Embeds and normalizes image
    features in the joint embedding space (`I_e`).'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`I_e = l2_normalize(np.dot(I_f, W_i), axis=1)`：在联合嵌入空间中嵌入并归一化图像特征（`I_e`）。'
- en: '`T_e = l2_normalize(np.dot(T_f, W_t), axis=1)`: Embeds and normalizes text
    features in the joint embedding space (`T_e`).'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`T_e = l2_normalize(np.dot(T_f, W_t), axis=1)`：在联合嵌入空间中嵌入并归一化文本特征（`T_e`）。'
- en: The code below illustrates the sequential processing of image and text data.
    Initially, the data undergoes processing through the base encoder, followed by
    the projection layer. finally, normalized embeddings are generated for both modalities
    and returned.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码展示了图像和文本数据的顺序处理过程。最初，数据经过基本编码器处理，然后通过投影层，最后生成归一化的嵌入并返回。
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Cosine Similarities:**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**余弦相似度：**'
- en: '`logits = np.dot(I_e, T_e.T) * np.exp(t)`: Computes pairwise cosine similarities
    between image and text embeddings, scaled by a learned temperature parameter `t`.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits = np.dot(I_e, T_e.T) * np.exp(t)`：计算图像和文本嵌入之间的成对余弦相似度，按学习到的温度参数`t`进行缩放。'
- en: In this example, we interchangeably use similarity with logits in the same way
    that was used in the original paper. We will not include the temperature parameter
    `t` in this blog post.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们以与原始论文中相同的方式交替使用相似度和logits。我们将在本博客中不包含温度参数`t`。
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Symmetric Loss Function:**'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**对称损失函数：**'
- en: CLIP uses contrastive loss (first introduced in [Representation Learning with
    Contrastive Predictive Coding](https://arxiv.org/pdf/1807.03748.pdf)) to bring
    related images and texts closer together while pushing unrelated ones apart.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP 使用对比损失（首次在[对比预测编码的表示学习](https://arxiv.org/pdf/1807.03748.pdf)中引入）来将相关的图像和文本拉近，同时将不相关的图像和文本分开。
- en: '`labels = np.arange(n)`: Generates labels representing the indices of the batch.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels = np.arange(n)`：生成表示批次索引的标签。'
- en: '`loss_i = cross_entropy_loss(logits, labels, axis=0)`: Computes the cross-entropy
    loss along the image axis.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss_i = cross_entropy_loss(logits, labels, axis=0)`：计算图像轴上的交叉熵损失。'
- en: '`loss_t = cross_entropy_loss(logits, labels, axis=1)`: Computes the cross-entropy
    loss along the text axis.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss_t = cross_entropy_loss(logits, labels, axis=1)`：计算文本轴上的交叉熵损失。'
- en: '`loss = (loss_i + loss_t)/2`: Computes the symmetric average of the image and
    text losses.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss = (loss_i + loss_t)/2`：计算图像和文本损失的对称平均。'
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Final Custom CLIP Model**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**最终自定义 CLIP 模型**'
- en: 'Combing all the different pieces together, the final custom CLIP model looks
    like the following:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有不同的部分组合在一起，最终的自定义 CLIP 模型如下所示：
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Example**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例**'
- en: This example demonstrates the process of creating image caption datasets and
    training a custom CLIP model. The aim is to train a vision encoder and a text
    encoder jointly to project the representation of images and their captions into
    the same embedding space, such that the caption embeddings are located near the
    embeddings of the images they describe. The code for this project is in my [GitHub
    repository](https://github.com/RustamyF/clip-multimodal-ml).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例演示了创建图像标题数据集和训练自定义 CLIP 模型的过程。目标是联合训练视觉编码器和文本编码器，将图像及其标题的表示投影到相同的嵌入空间，使标题嵌入位于它们描述的图像的嵌入附近。此项目的代码在我的[GitHub
    存储库](https://github.com/RustamyF/clip-multimodal-ml)中。
- en: '**Dataset and Dataloader**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据集和数据加载器**'
- en: Our custom CLIP model will be trained using the [flickr30k dataset](https://huggingface.co/datasets/nlphuji/flickr30k).
    This dataset comprises more than 31,000 images, each with a minimum of 5 independent
    human-generated captions. We will use two captions for each image in this example
    to have a total of 62,000 image and text pairs for training. Although traditionally
    employed for image captioning tasks, we intend to adapt the image-caption pairs
    to train our dual encoder model specifically for image search purposes. The [GitHub
    repository](https://github.com/RustamyF/clip-multimodal-ml) also includes the
    code to train the model on the MS-COCO dataset with 164,000 image and text pairs.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的自定义 CLIP 模型将使用[flickr30k 数据集](https://huggingface.co/datasets/nlphuji/flickr30k)进行训练。该数据集包含超过31,000张图像，每张图像至少有5个独立的人类生成的标题。我们将在本示例中使用每张图像的两个标题，共有62,000对图像和文本用于训练。尽管传统上用于图像标题任务，但我们打算将图像-标题对适配到我们的双编码器模型，专门用于图像搜索。
    [GitHub 存储库](https://github.com/RustamyF/clip-multimodal-ml)中还包括了用于在MS-COCO数据集上训练模型的代码，其中包含164,000对图像和文本。
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Key model constants include`embed_dim` for learned representations, `transformer_embed_dim`
    for transformer layer features, and `max_len` for text input length. The chosen
    `text_model` is “distilbert-base-multilingual-cased.” Training spans 3`epochs`
    with a`batch_size` of 128, which are the constants that will feed into the model
    building and training.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 关键模型常量包括`embed_dim`用于学习的表示，`transformer_embed_dim`用于变换器层特征，以及`max_len`用于文本输入长度。选择的`text_model`是“distilbert-base-multilingual-cased”。训练跨度为3`epochs`，`batch_size`为128，这些常量将输入到模型构建和训练中。
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The DataLoader is set up for efficient iteration during training, providing
    organized access to image-caption pairs.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: DataLoader 被设置为在训练期间高效迭代，提供对图像-标题对的有组织访问。
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here is an example of an image caption pair in one of the batches in the dataset.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这是数据集中一个批次中图像标题对的示例。
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/4069befa5671d85cbb02e7f5ed477411.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4069befa5671d85cbb02e7f5ed477411.png)'
- en: Here, we initiate our CustomModel and send it to the device (CPU or GPU). Additionally,
    we specify the parameters to be optimized throughout the training process. Given
    that we have fixed the base layer for both text and image encoders, only the parameters
    associated with the projection layer will undergo training on the new dataset.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们初始化我们的 CustomModel 并将其发送到设备（CPU 或 GPU）。此外，我们指定了在训练过程中需要优化的参数。由于我们已经固定了文本和图像编码器的基础层，因此只有与投影层相关的参数将在新数据集上进行训练。
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Model training**'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型训练**'
- en: The training was performed with a Tesla T4 (g4dn-xlarge) GPU machine for 3 training
    epochs. The [Jupyter Notebook](https://github.com/RustamyF/clip-multimodal-ml/blob/main/notebooks/flicker30kclip_model.ipynb)
    is available in the project’s [GitHub repository](https://github.com/RustamyF/clip-multimodal-ml)
    and contains the code for the training loop.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 训练是在一台 Tesla T4 (g4dn-xlarge) GPU 机器上进行了 3 个训练 epoch。该 [Jupyter Notebook](https://github.com/RustamyF/clip-multimodal-ml/blob/main/notebooks/flicker30kclip_model.ipynb)
    可在项目的 [GitHub 仓库](https://github.com/RustamyF/clip-multimodal-ml) 中找到，并包含训练循环的代码。
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The following are the results of training loops for each epoch using the flicker30k
    dataset. For additional details, please refer to [this notebook](https://github.com/RustamyF/clip-multimodal-ml/blob/main/notebooks/flicker30kclip_model.ipynb).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用 flicker30k 数据集进行每个 epoch 的训练循环结果。有关更多详细信息，请参阅[此笔记本](https://github.com/RustamyF/clip-multimodal-ml/blob/main/notebooks/flicker30kclip_model.ipynb)。
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Here are the results from the training loops for each epoch using the COCO2017
    dataset. The model exhibits faster convergence on the COCO dataset, attributed
    to the availability of over 160,000 image-text pairs, in contrast to the 62,000
    image pairs in the flickr30k dataset. For additional details, please refer to
    [this notebook](https://github.com/RustamyF/clip-multimodal-ml/blob/main/notebooks/coco2017clip_model.ipynb).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用 COCO2017 数据集进行每个 epoch 的训练循环结果。与 flickr30k 数据集中的 62,000 张图像对相比，COCO 数据集提供了超过
    160,000 张图像-文本对，使模型在 COCO 数据集上的收敛速度更快。有关更多详细信息，请参阅[此笔记本](https://github.com/RustamyF/clip-multimodal-ml/blob/main/notebooks/coco2017clip_model.ipynb)。
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Conclusion**'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**结论**'
- en: In conclusion, this blog post has explored the CLIP model, uncovering its potential
    for wide-ranging applications. As we understand the applications of CLIP, it becomes
    evident that its impact spans far beyond initial expectations, paving the way
    for innovative solutions across diverse fields. CLIP was the first successful
    model that bridged the gap between different modalities and opened avenues for
    cross-disciplinary innovations.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本博客文章探讨了 CLIP 模型，揭示了其广泛应用的潜力。随着我们对 CLIP 应用的理解变得更加深入，其影响显然超出了最初的预期，为各个领域的创新解决方案铺平了道路。CLIP
    是第一个成功地弥合了不同模态之间差距的模型，并开启了跨学科创新的途径。
