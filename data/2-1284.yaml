- en: Hyperparameter Optimization — Intro and Implementation of Grid Search, Random
    Search and Bayesian Optimization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数优化——网格搜索、随机搜索和贝叶斯优化的简介与实现
- en: 原文：[https://towardsdatascience.com/hyperparameter-optimization-intro-and-implementation-of-grid-search-random-search-and-bayesian-b2f16c00578a](https://towardsdatascience.com/hyperparameter-optimization-intro-and-implementation-of-grid-search-random-search-and-bayesian-b2f16c00578a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/hyperparameter-optimization-intro-and-implementation-of-grid-search-random-search-and-bayesian-b2f16c00578a](https://towardsdatascience.com/hyperparameter-optimization-intro-and-implementation-of-grid-search-random-search-and-bayesian-b2f16c00578a)
- en: Most common hyperparameter optimization methodologies to boost machine learning
    outcomes
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升机器学习结果的最常见超参数优化方法
- en: '[](https://medium.com/@fmnobar?source=post_page-----b2f16c00578a--------------------------------)[![Farzad
    Mahmoodinobar](../Images/2d75209693b712300e6f0796bd2487d0.png)](https://medium.com/@fmnobar?source=post_page-----b2f16c00578a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b2f16c00578a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b2f16c00578a--------------------------------)
    [Farzad Mahmoodinobar](https://medium.com/@fmnobar?source=post_page-----b2f16c00578a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@fmnobar?source=post_page-----b2f16c00578a--------------------------------)[![Farzad
    Mahmoodinobar](../Images/2d75209693b712300e6f0796bd2487d0.png)](https://medium.com/@fmnobar?source=post_page-----b2f16c00578a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b2f16c00578a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b2f16c00578a--------------------------------)
    [Farzad Mahmoodinobar](https://medium.com/@fmnobar?source=post_page-----b2f16c00578a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b2f16c00578a--------------------------------)
    ·10 min read·Mar 13, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[数据科学前沿](https://towardsdatascience.com/?source=post_page-----b2f16c00578a--------------------------------)·阅读时间10分钟·2023年3月13日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/13056882b4fc4ae3b84019b4de503886.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13056882b4fc4ae3b84019b4de503886.png)'
- en: Photo by [Jonas Jaeken](https://unsplash.com/@jonasjaekenmedia) on [Unsplash](https://unsplash.com/photos/Gg2ttawakqE)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[乔纳斯·贾肯](https://unsplash.com/@jonasjaekenmedia)提供，来自[Unsplash](https://unsplash.com/photos/Gg2ttawakqE)
- en: Usually the first solution that comes to mind when trying to improve a machine
    learning model is to just add more training data. Additional data usually helps
    (barring certain situations) but generating high-quality data can be quite expensive.
    Hyperparameter optimization can save us time and resources by getting the best
    model performance using the existing data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，尝试提高机器学习模型性能时，第一个想到的解决方案是增加更多的训练数据。额外的数据通常有帮助（在某些情况下除外），但生成高质量数据可能非常昂贵。超参数优化可以通过利用现有数据来节省时间和资源，以获得最佳模型性能。
- en: Hyperparameter optimization, as the name suggests, is the process of identifying
    the best combination of hyperparameters for a machine learning model to satisfy
    an optimization function (i.e. maximize the performance of the model, given the
    data set in study). In other words, each model comes with multiple knobs and levers
    that we can change, until we get to the optimized combination. A few examples
    of parameters that we can change during hyperparameter optimization can be learning
    rate, architecture of a neural network (e.g. number of hidden layers), regularization,
    etc.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数优化，顾名思义，是识别机器学习模型最佳超参数组合的过程，以满足优化函数（即在给定的数据集上最大化模型性能）。换句话说，每个模型都有多个我们可以调整的旋钮和杠杆，直到我们找到优化的组合。在超参数优化过程中，我们可以调整的一些参数示例包括学习率、神经网络的架构（例如，隐藏层的数量）、正则化等。
- en: In this post, we are going to conceptually walk through the three most common
    hyperparameter optimization methodologies, namely Grid Search, Random Search and
    Bayesian Optimization, and then implement them.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将概念性地介绍三种最常见的超参数优化方法，即网格搜索、随机搜索和贝叶斯优化，并进行实现。
- en: I am going to include a high-level comparison table here for future reference
    and then each one will be further explored, explained and implemented in the remainder
    of the post.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在这里包含一个高层次的对比表，以便将来参考，然后在文章的其余部分将进一步探讨、解释和实现每一种方法。
- en: '![](../Images/b18f9d1f656e46b1e954d189de413ca5.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b18f9d1f656e46b1e954d189de413ca5.png)'
- en: Table 1 — Hyperparameter Optimization Methodology Comparison
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 表1——超参数优化方法比较
- en: Let’s get started!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: '*(All images, unless otherwise noted, are by the author.)*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*(除非另有说明，所有图片均由作者提供。)*'
- en: '[](https://medium.com/@fmnobar/membership?source=post_page-----b2f16c00578a--------------------------------)
    [## Join Medium with my referral link - Farzad Mahmoodinobar'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 通过我的推荐链接加入 Medium - Farzad Mahmoodinobar](https://medium.com/@fmnobar/membership?source=post_page-----b2f16c00578a--------------------------------)'
- en: Read every story from Farzad (and other writers on Medium). Your membership
    fee directly supports Farzad and other…
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阅读 Farzad（以及 Medium 上其他作者）的每个故事。你的会员费用直接支持 Farzad 和其他人…
- en: medium.com](https://medium.com/@fmnobar/membership?source=post_page-----b2f16c00578a--------------------------------)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/@fmnobar/membership?source=post_page-----b2f16c00578a--------------------------------)'
- en: 1\. Grid Search
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 网格搜索
- en: Grid search is probably the simplest and most intuitive method of hyperparameter
    optimization, which involves exhaustively searching for the best combination of
    hyperparameters in the defined search space. “Search Space” in this context is
    the entire hyperparameters and the values for such hyperparameters that are bing
    considered in the optimization process. Let’s walk through an example to better
    understand Grid Search.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索可能是最简单且最直观的超参数优化方法，它涉及在定义的搜索空间中穷尽地寻找最佳超参数组合。在这个上下文中，“搜索空间”是指在优化过程中考虑的所有超参数及其值。让我们通过一个例子更好地理解网格搜索。
- en: 'Let’s assume we have a machine learning model that has only three parameters,
    each of which can take the values provided in the lists below:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个机器学习模型，它只有三个参数，每个参数可以取下面列表中的值：
- en: parameter_1 = [1, 2, 3]
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: parameter_1 = [1, 2, 3]
- en: parameter_2 = [a, b, c]
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: parameter_2 = [a, b, c]
- en: parameter_3 = [x, y, z]
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: parameter_3 = [x, y, z]
- en: We do not know which combination of these parameters will optimize our model’s
    optimization function (i.e. provide the best output for our machine learning model).
    In Grid Search we simply try every single combination of these parameters, measure
    the performance of the model for each and then simply pick the combination generating
    the best performance! In this example, parameter_1 can take 3 values (i.e. 1,
    2 or 3), parameter_2 can take 3 values (i.e. a, b and c) and parameter_3 can take
    3 values (i.e. x, y and z). In other words, there are 3*3*3=27 combinations in
    total. Grid Search in this example will involve 27 rounds of evaluating the performance
    of the ML model to find the best performing combination.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不知道这些参数的哪种组合能够优化我们模型的优化函数（即，为我们的机器学习模型提供最佳输出）。在网格搜索中，我们简单地尝试这些参数的每一个组合，测量每个组合模型的性能，然后选择产生最佳性能的组合！在这个例子中，parameter_1
    可以取 3 个值（即 1、2 或 3），parameter_2 可以取 3 个值（即 a、b 和 c），parameter_3 可以取 3 个值（即 x、y
    和 z）。换句话说，总共有 3*3*3=27 种组合。在这个例子中，网格搜索将涉及 27 轮评估 ML 模型性能，以找到表现最佳的组合。
- en: 'As you can see, this approach is very simple (akin to a trial-and-error task)
    but it also has some limitations. Let’s summarize advantages and disadvantages
    together:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，这种方法非常简单（类似于试错任务），但它也有一些局限性。让我们一起总结优缺点：
- en: 'Advantages:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 优点：
- en: Easy to understand and implement
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 易于理解和实现
- en: Easy to parallelize
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 易于并行化
- en: Works well for discrete and continuous spaces
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适用于离散和连续空间
- en: 'Disadvantages:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: Expensive in large and/or complex models with larger number of hyperparameters
    (since all combinations will have to be tried and evaluated)
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于具有较多超参数的大型和/或复杂模型来说成本较高（因为需要尝试和评估所有组合）
- en: Memoryless — Does not learn from past observations
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无记忆性 — 不从过去的观察中学习
- en: May not find the optimal combination if the search space is too large
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果搜索空间过大，可能无法找到最佳组合
- en: My recommendation is that if you have a simple model with a small search space,
    use the Grid Search. Otherwise, keep reading to find solutions better suited for
    larger search spaces.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我的建议是，如果你有一个简单的模型，且搜索空间较小，请使用网格搜索。否则，继续阅读，寻找更适合较大搜索空间的解决方案。
- en: Let’s implement Grid Search with a real example.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个实际的例子来实现网格搜索。
- en: 1.1\. Grid Search — Implementation
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1\. 网格搜索 — 实现
- en: To implement Grid Search, we will create a random forest classification model
    using the [Iris data set from scikit-learn](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html).
    This data set includes 3 different irises petal and sepal length, which will be
    used in this classification exercise. For the purposes of this post, model development
    is of the secondary importance, since the objective is to compare performance
    of various hyperparameter optimization strategies . I encourage you to focus on
    the model evaluation results and the time it takes for each of the hyperparameter
    optimization methodologies to reach their selected set of hyperparameters. I will
    describe the results and then will provide a summary comparison table for the
    three methodologies used in this post.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现网格搜索，我们将使用来自 [scikit-learn 的鸢尾花数据集](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html)
    创建一个随机森林分类模型。该数据集包含三种不同的鸢尾花的花瓣和萼片长度，将用于本次分类练习。对于本帖的目的，模型开发的优先级较低，因为目标是比较各种超参数优化策略的性能。我鼓励你关注模型评估结果以及每种超参数优化方法达到其选定超参数集所需的时间。我将描述结果，并提供一个本帖中使用的三种方法的总结比较表。
- en: 'The search space, which includes all the values of the hyperparameters is defined
    as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索空间，即包括所有超参数值的空间，定义如下：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Above search space consists of 4*5*3*3=180 total combinations of hyperparameters.
    We will use Grid Search to find the combination which optimizes the objective
    function as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 上述搜索空间由 4*5*3*3=180 种超参数组合组成。我们将使用网格搜索找到优化目标函数的组合，如下所示：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Results:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：
- en: '![](../Images/61c82cd7f6518083c1393774739732a1.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61c82cd7f6518083c1393774739732a1.png)'
- en: Grid Search Results
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索结果
- en: Here we can see the hyperparameter values selected using Grid Search. `best_score`
    describes the evaluation results using the selected set of hyperparameters and
    `elapsed_time` describes the time it took my local laptop to execute this hyperparameter
    optimization strategy. Keep in mind the evaluation results and the elapsed time
    for the sake of comparison when we get to the next methods. For now, let’s move
    on to Random Search.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到使用网格搜索选择的超参数值。`best_score` 描述了使用选定超参数集的评估结果，`elapsed_time` 描述了我的本地笔记本计算执行这个超参数优化策略所需的时间。请记住评估结果和耗时，以便在我们讨论下一些方法时进行比较。现在，让我们继续讨论随机搜索。
- en: 2\. Random Search
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 随机搜索
- en: Random Search, as the name suggests, is the process of randomly sampling hyperparameters
    from a defined search space. As opposed to Grid Search which exhaustively goes
    through every single combination of hyperparameters’ values, Random Search only
    selects a random subset of hyperparameter values for a pre-defined number of iterations
    (depending on the available resources such as time, budget, objective, etc.),
    calculates the machine learning model’s performance for each and then selects
    the best one.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索，顾名思义，是从定义的搜索空间中随机抽取超参数。与网格搜索逐一遍历每种超参数值组合不同，随机搜索仅选择一组预定义次数（取决于可用资源，如时间、预算、目标等）的超参数值的随机子集，为每组计算机器学习模型的性能，然后选择最佳的那一组。
- en: 'As you can imagine based on the approached described above, Random Search is
    less expensive compared to a full Grid Search but still has its own advantages
    and disadvantages, as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述描述，你可以想象，随机搜索比完全的网格搜索成本更低，但仍有其自身的优缺点，如下所示：
- en: 'Advantages:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 优点：
- en: Easy to understand and implement
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 易于理解和实现
- en: Easy to parallelize
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 易于并行化
- en: Works for discrete and continuous spaces
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适用于离散空间和连续空间
- en: Less expensive than Grid Search
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比网格搜索便宜
- en: More likely to converge to the optimum compared to Grid Search with the same
    number of attempts
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与相同尝试次数的网格搜索相比，更有可能收敛到最优解
- en: 'Disadvantages:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: Memoryless — Does not learn from past observations
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无记忆 — 不从过去的观察中学习
- en: May miss important hyperparameter values, given the random selection
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于随机选择，可能会遗漏重要的超参数值
- en: In the next method, we will tackle the “memoryless” disadvantage of Grid and
    Random Search, through Bayesian Optimization. But before getting there, let’s
    implement Random Search.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一种方法中，我们将通过贝叶斯优化解决网格搜索和随机搜索的“无记忆”缺点。但在此之前，让我们先实现随机搜索。
- en: 2.1\. Random Search — Implementation
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1\. 随机搜索 — 实现
- en: Using the code snippet below, we will implement a Random Search hyperparameter
    optimization for the same problem described in the Grid Search implementation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用下面的代码片段，我们将为网格搜索实现中描述的相同问题实施随机搜索超参数优化。
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Results:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：
- en: '![](../Images/f04c7a1231b6b6527e305b5be18eb388.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f04c7a1231b6b6527e305b5be18eb388.png)'
- en: Random Search Results
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索结果
- en: These results are quite interesting, when compared to the results of Grid Search.
    The `best_score` remains unchanged but the `elapsed_time` was decreased from 352.0
    to 75.5 seconds! That’s quite impressive! In other words, Random Search managed
    to find a set of hyperparameters that yielded the same level of performance as
    Grid Search, in around 21% of the time required for a Grid Search! That’s much
    more efficient.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 与网格搜索结果相比，这些结果相当有趣。`best_score`保持不变，但`elapsed_time`从352.0秒减少到75.5秒！这真是令人印象深刻！换句话说，随机搜索成功找到了一组超参数，其性能与网格搜索相同，但所需时间仅为网格搜索的约21%！这效率高得多。
- en: Next, let’s move on to our next approach, called Bayesian Optimization, which
    learns from each attempt during optimization.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们继续讨论下一个方法，称为贝叶斯优化，它从每次优化尝试中学习。
- en: 3\. Bayesian Optimization
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3. 贝叶斯优化
- en: Bayesian Optimization is a hyperparameter optimization methodology that uses
    probabilistic models to “learn” from previous attempts and to guide the search
    towards the optimum combination of hyperparameters in the search space that optimizes
    the machine learning model’s objective function.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯优化是一种超参数优化方法论，它使用概率模型从之前的尝试中“学习”，并指导搜索以找到优化机器学习模型目标函数的超参数的最佳组合。
- en: Bayesian Optimization approach can be broken down into 4 steps, which I will
    describe below. I encourage you to read through the steps for a better understanding
    of the process but knowledge is not a requirement for using this method.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯优化方法可以分解为4个步骤，我将在下面进行描述。我鼓励你通读这些步骤以更好地理解该过程，但使用此方法并不要求掌握这些知识。
- en: Define a “Prior”, which is a probabilistic model about our belief at a point
    in time about the most likely combination of hyperparameters to optimize the objective
    function
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个“先验”，这是一个关于我们在某个时间点关于最有可能优化目标函数的超参数组合的信念的概率模型
- en: Evaluate the model for a sample of hyperparameters
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对一组超参数评估模型
- en: Using the knowledge gained in Step 2, update the probabilistic model from Step
    1 (i.e. what we called “Prior”) about where we believe the most likely combination
    of hyperparameters to optimize the objective function is. Our updated belief is
    called the “Posterior”. In other words, knowledge gained in Step 2 helped us have
    a better understanding about the search space and took us from Prior to Posterior,
    making Posterior our “latest” knowledge about the search space and the objective
    function, informed by Step 2
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 利用第2步获得的知识，更新第1步中的概率模型（即我们称之为“先验”）关于我们认为最有可能优化目标函数的超参数组合。我们更新后的信念称为“后验”。换句话说，第2步获得的知识帮助我们更好地理解搜索空间，将我们从先验转变为后验，使后验成为我们关于搜索空间和目标函数的“最新”知识，由第2步提供信息。
- en: Repeat Steps 2 and 3, until the performance of the model converges, resources
    are exhausted or some other pre-defined metric is met
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤2和3，直到模型性能收敛、资源耗尽或满足其他预定义的指标
- en: 'If you are interested in learning more about the details of Bayesian Optimization,
    you can look at the following post:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对贝叶斯优化的详细信息感兴趣，可以查看以下文章：
- en: '[](https://medium.com/@fmnobar/conceptual-overview-of-bayesian-optimization-for-parameter-tuning-in-machine-learning-a3b1b4b9339f?source=post_page-----b2f16c00578a--------------------------------)
    [## Bayesian Optimization in Machine Learning'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@fmnobar/conceptual-overview-of-bayesian-optimization-for-parameter-tuning-in-machine-learning-a3b1b4b9339f?source=post_page-----b2f16c00578a--------------------------------)
    [## 机器学习中的贝叶斯优化'
- en: This post is about hyperparameter optimization through Bayesian Optimization.
    The task intended to help choose a set of…
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 这篇文章讲述了通过贝叶斯优化进行超参数优化的内容。这个任务旨在帮助选择一组...
- en: medium.com](https://medium.com/@fmnobar/conceptual-overview-of-bayesian-optimization-for-parameter-tuning-in-machine-learning-a3b1b4b9339f?source=post_page-----b2f16c00578a--------------------------------)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@fmnobar/conceptual-overview-of-bayesian-optimization-for-parameter-tuning-in-machine-learning-a3b1b4b9339f?source=post_page-----b2f16c00578a--------------------------------)
- en: Now that we understand how Bayesian Optimization works, let’s take a look at
    its advantages and disadvantages.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了贝叶斯优化的工作原理，让我们来看看它的优缺点。
- en: 'Advantages:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 优势：
- en: Learns from past observations and is hence more efficient — in other words,
    it is expected to find a better set of hyperparameters in fewer iterations, compared
    to memoryless methodologies
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从过去的观察中学习，因此更高效——换句话说，预计在更少的迭代中找到更好的超参数集，相比于没有记忆的方法论
- en: Converges to an optimum, given certain assumptions
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在特定假设下会收敛到最优解
- en: 'Disadvantages:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: Difficult to parallelize
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 难以并行化
- en: Computationally heavier than Grid and Random Search per iteration
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次迭代的计算量比网格搜索和随机搜索要大
- en: Choice of the initial probabilistic distribution of the Prior and functions
    used in Bayesian Optimization (e.g. acquisition function, etc.) can significantly
    impact the performance and its learning curve
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯优化中先验的初始概率分布和所用的函数（例如获取函数等）的选择会显著影响性能和学习曲线
- en: With the details out of the way, let’s implement Bayesian Optimization and look
    at the results.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 详细信息已经说明，让我们实现贝叶斯优化并查看结果。
- en: 3.1\. Bayesian Optimization — Implementation
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1\. 贝叶斯优化 — 实施
- en: Similar to the previous section, we will use the code snippet below to implement
    a Bayesian Optimization hyperparameter optimization for the same problem described
    in the Grid Search implementation.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 与上一节类似，我们将使用下面的代码片段来实现贝叶斯优化超参数优化，针对与网格搜索实现中描述的相同问题。
- en: '[PRE3]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Results:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：
- en: '![](../Images/ee556232c9fa4edfb53602442440ce2a.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee556232c9fa4edfb53602442440ce2a.png)'
- en: Bayesian Optimization Results
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯优化结果
- en: Another interesting set of results! `best_score` remained the same as what we
    were able to achieve through Grid and Random Search but the results were achieved
    in only 23.1 seconds, compared to 75.5 seconds for Random Search and 352.0 seconds
    for Grid Search! In other words, the time required using Bayesian Optimization
    was around 93% less than what was required in a Grid Search. That is a large productivity
    gain, which becomes much more meaningful in larger and more complex models and
    search spaces.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的结果集！`best_score`与我们通过网格搜索和随机搜索所能达到的相同，但这些结果仅在23.1秒内完成，相比之下，随机搜索耗时75.5秒，网格搜索耗时352.0秒！换句话说，使用贝叶斯优化所需的时间比网格搜索少约93%。这是一个巨大的生产力提升，对于更大更复杂的模型和搜索空间来说意义更为重大。
- en: Note that Bayesian Optimization achieved these results using only 10 iterations,
    because it can learn from previous iterations (as opposed to Random and Grid Search).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，贝叶斯优化仅用10次迭代就达到了这些结果，因为它可以从之前的迭代中学习（与随机搜索和网格搜索不同）。
- en: Results Comparison
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果比较
- en: Below table is a comparison of the results of the three approaches discussed.
    “Methodology” column describes the hyperparameter optimization approach used.
    It is followed by the hyperparameters selected using each methodology. “Best Score”
    is the score achieved using that specific approach, followed by “Elapsed Time”,
    which indicates the time it took for the optimization strategy to run on my local
    laptop. Last column, “Gained Efficiency”, assumes Grid Search as the baseline
    and then calculates the efficiency gained (using elapsed time) for each of the
    other two approaches, compared to Grid Search. For example, since Random Search
    took 75.5 seconds, while Grid Search took 352.0 seconds, the gained efficiency
    for Random Search relative to the baseline of Grid Search is calculated as 1–75.5/352.0=78.5%.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格对三种方法的结果进行了比较。“方法论”列描述了所使用的超参数优化方法。接下来是每种方法所选的超参数。“最佳得分”是使用特定方法获得的分数，接下来是“经过时间”，这表示优化策略在我的本地笔记本电脑上运行所需的时间。最后一列“提高效率”假设网格搜索为基准，然后计算相对于网格搜索的其他两种方法的效率提升（使用经过时间）。例如，由于随机搜索耗时75.5秒，而网格搜索耗时352.0秒，因此相对于网格搜索的基准，随机搜索的效率提升计算为1–75.5/352.0=78.5%。
- en: '![](../Images/8d70ae1e3b7131f0d71d705dbee86243.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8d70ae1e3b7131f0d71d705dbee86243.png)'
- en: Table 2 — Methodology Performance Comparison Table
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 表2 — 方法性能比较表
- en: 'Two main takeaways from the comparison table above:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的比较表中可以得出两个主要结论：
- en: '**Efficiency:** We can see how learned approaches such as Bayesian Optimization
    can find an optimized set of hyperparameters in a shorter period of time.'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**效率：** 我们可以看到，像贝叶斯优化这样的学习方法可以在更短的时间内找到优化的超参数集。'
- en: '**Parameter Selection:** There can be more than one correct answer. For example,
    the selected parameters for Bayesian Optimization are different than those for
    Grid and Random Search, although the evaluation metric (i.e. `best_score`) remain
    the same. This can be even more important in larger and more complex settings.'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**参数选择：** 可能有不止一个正确答案。例如，贝叶斯优化所选的参数与网格搜索和随机搜索的参数不同，尽管评估指标（即`best_score`）保持不变。在更大和更复杂的设置中，这一点可能更加重要。'
- en: Conclusion
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this post, we talked about what hyperparameter optimization is and introduced
    three of the most common methodologies used for this optimization exercise. Then
    we walked through each of these three methodologies in detail and implemented
    them in a classification exercise. Lastly, we compared the results of implementing
    these three methodologies. We found out how approaches such as Bayesian Optimization
    that can learn from previous attempts can be significantly more efficient, which
    can be an important factor in large and complex models (e.g. deep neural networks),
    where efficiency can be a deciding factor.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们讨论了超参数优化是什么，并介绍了三种用于此优化任务的最常见方法。然后，我们详细讲解了这三种方法，并在分类任务中实现了它们。最后，我们比较了这三种方法的实施结果。我们发现，像贝叶斯优化这样的能够从之前尝试中学习的方法，可能会显著更高效，这在大型和复杂模型（例如深度神经网络）中尤为重要，因为效率可能成为决定性因素。
- en: Thanks for Reading!
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: If you found this post helpful, please [follow me on Medium](https://medium.com/@fmnobar)
    and subscribe to receive my latest posts!
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你觉得这篇文章有帮助，请[在 Medium 上关注我](https://medium.com/@fmnobar)并订阅以接收我最新的文章！
