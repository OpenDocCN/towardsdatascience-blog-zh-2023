- en: Attention from Alignment, Practically Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从对齐中学到的注意力机制，实用解读
- en: 原文：[https://towardsdatascience.com/attention-from-alignment-practically-explained-548ef6588aa4?source=collection_archive---------2-----------------------#2023-07-19](https://towardsdatascience.com/attention-from-alignment-practically-explained-548ef6588aa4?source=collection_archive---------2-----------------------#2023-07-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/attention-from-alignment-practically-explained-548ef6588aa4?source=collection_archive---------2-----------------------#2023-07-19](https://towardsdatascience.com/attention-from-alignment-practically-explained-548ef6588aa4?source=collection_archive---------2-----------------------#2023-07-19)
- en: Learn from what matters, Ignore what doesn’t.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习重要的内容，忽略不相关的部分。
- en: '[](https://medium.com/@danielwarfield1?source=post_page-----548ef6588aa4--------------------------------)[![Daniel
    Warfield](../Images/c1c8b4dd514f6813e08e401401324bca.png)](https://medium.com/@danielwarfield1?source=post_page-----548ef6588aa4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----548ef6588aa4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----548ef6588aa4--------------------------------)
    [Daniel Warfield](https://medium.com/@danielwarfield1?source=post_page-----548ef6588aa4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@danielwarfield1?source=post_page-----548ef6588aa4--------------------------------)[![Daniel
    Warfield](../Images/c1c8b4dd514f6813e08e401401324bca.png)](https://medium.com/@danielwarfield1?source=post_page-----548ef6588aa4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----548ef6588aa4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----548ef6588aa4--------------------------------)
    [Daniel Warfield](https://medium.com/@danielwarfield1?source=post_page-----548ef6588aa4--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbdc4072cbfdc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-from-alignment-practically-explained-548ef6588aa4&user=Daniel+Warfield&userId=bdc4072cbfdc&source=post_page-bdc4072cbfdc----548ef6588aa4---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----548ef6588aa4--------------------------------)
    ·11 min read·Jul 19, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F548ef6588aa4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-from-alignment-practically-explained-548ef6588aa4&user=Daniel+Warfield&userId=bdc4072cbfdc&source=-----548ef6588aa4---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbdc4072cbfdc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-from-alignment-practically-explained-548ef6588aa4&user=Daniel+Warfield&userId=bdc4072cbfdc&source=post_page-bdc4072cbfdc----548ef6588aa4---------------------post_header-----------)
    发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----548ef6588aa4--------------------------------)
    ·11分钟阅读·2023年7月19日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F548ef6588aa4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-from-alignment-practically-explained-548ef6588aa4&user=Daniel+Warfield&userId=bdc4072cbfdc&source=-----548ef6588aa4---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F548ef6588aa4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-from-alignment-practically-explained-548ef6588aa4&source=-----548ef6588aa4---------------------bookmark_footer-----------)![](../Images/fddcbc3132d55c17e2ede623de6dbc4b.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F548ef6588aa4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-from-alignment-practically-explained-548ef6588aa4&source=-----548ef6588aa4---------------------bookmark_footer-----------)![](../Images/fddcbc3132d55c17e2ede623de6dbc4b.png)'
- en: Photo by [Armand Khoury](https://unsplash.com/@armand_khoury?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Armand Khoury](https://unsplash.com/@armand_khoury?utm_source=medium&utm_medium=referral)
    提供，发布在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Attention, as popularized by the landmark paper [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
    (2017), is arguably the most important architectural trend in machine learning
    right now. Originally intended for sequence to sequence modeling, attention has
    exploded into virtually every sub-discipline of machine learning.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如2017年的开创性论文[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)所推广的，注意力机制可以说是当前机器学习领域最重要的架构趋势。最初用于序列到序列建模的注意力机制，现已扩展到几乎所有机器学习的子领域。
- en: This post will describe a particular flavor of attention which proceeded the
    transforner style of attention. We’ll discuss how it works, and why it’s useful.
    We’ll also go over some literature and a tutorial implementing this form of attention
    in PyTorch. By reading this post, you will have a more thorough understanding
    of attention as a general concept, which is useful in exploring more cutting edge
    applications.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将描述一种特定的注意力机制，这种机制在变压器风格的注意力机制之前就已存在。我们将讨论它的工作原理以及它的实用性。我们还会回顾一些文献和一个在PyTorch中实现这种注意力机制的教程。通过阅读本文，你将对注意力机制作为一种通用概念有更深入的了解，这对探索更前沿的应用非常有用。
- en: The Reason For Attention
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意力的理由
- en: The attention mechanism was originally popularized in [Neural Machine Translation
    by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473v7.pdf)(2014),
    which is the guiding reference for this particular post. This paper employs an
    encoder-decoder architecture for english-to-french translation.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制最初是在[通过共同学习对齐和翻译的神经机器翻译](https://arxiv.org/pdf/1409.0473v7.pdf)(2014)中推广的，这也是本文的指导参考。该论文采用了一个用于英法翻译的编码器-解码器架构。
