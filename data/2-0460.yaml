- en: BYOL —The Alternative to Contrastive Self-Supervised Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BYOL —对比自监督学习的替代方法
- en: 原文：[https://towardsdatascience.com/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c](https://towardsdatascience.com/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c](https://towardsdatascience.com/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c)
- en: '[🚀Sascha’s Paper Club](https://towardsdatascience.com/tagged/saschas-paper-club)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[🚀Sascha的论文俱乐部](https://towardsdatascience.com/tagged/saschas-paper-club)'
- en: 'Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning by J.
    Grill et. al.'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning 由 J.
    Grill 等人'
- en: '[](https://medium.com/@SaschaKirch?source=post_page-----5d0a26983d7c--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----5d0a26983d7c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5d0a26983d7c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5d0a26983d7c--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----5d0a26983d7c--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@SaschaKirch?source=post_page-----5d0a26983d7c--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----5d0a26983d7c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5d0a26983d7c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5d0a26983d7c--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----5d0a26983d7c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5d0a26983d7c--------------------------------)
    ·10 min read·Sep 7, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5d0a26983d7c--------------------------------)
    ·10分钟阅读·2023年9月7日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: In today’s paper analysis we will have a close look into the paper behind BYOL
    (**B**ootstrap **Y**our **O**wn **L**atent). It provides an alternative to contrastive
    self-supervised learning techniques for representation learning removing the need
    for a large corpus of negative samples and gigantic batch sizes. Furthermore it
    is a landmark paper on the path of understanding today’s state-of-the-art foundation
    models such as the [DINO](https://arxiv.org/abs/2104.14294) family, including
    [DINOv2](https://arxiv.org/abs/2304.07193).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在今天的论文分析中，我们将深入探讨关于 BYOL (**B**ootstrap **Y**our **O**wn **L**atent) 的论文。它提供了一个对比自监督学习技术的替代方案，能够去除对大量负样本和庞大批量大小的需求。此外，它在理解今天最先进的基础模型（如
    [DINO](https://arxiv.org/abs/2104.14294) 系列，包括 [DINOv2](https://arxiv.org/abs/2304.07193)）的道路上具有里程碑式的意义。
- en: While contrastive self-supervised learning frameworks still feel kind of intuitive,
    BYOL can be confusing and intimidating at first. Therefore, it’s a great paper
    to analyze together. So let’s dive into it and strip it down to uncover its core
    ideas!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管对比自监督学习框架仍然有些直观，但 BYOL 起初可能会让人感到困惑和不安。因此，这是一个很好的论文来一起分析。让我们深入了解它，揭示其核心思想吧！
- en: '![](../Images/63946f35ab17c2db935b29bf3105ec08.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63946f35ab17c2db935b29bf3105ec08.png)'
- en: Image created from [publication](https://arxiv.org/abs/2006.07733) by [Sascha
    Kirch](https://medium.com/@SaschaKirch)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Sascha Kirch](https://medium.com/@SaschaKirch) 创作，来源于 [出版物](https://arxiv.org/abs/2006.07733)
- en: '**Paper:** [Bootstrap your own latent: A new approach to self-supervised Learning](https://arxiv.org/abs/2006.07733)
    by Jean-Bastien Grill et. al., 13 Jun. 2020'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**论文：** [Bootstrap your own latent: A new approach to self-supervised Learning](https://arxiv.org/abs/2006.07733)
    由 Jean-Bastien Grill 等人，2020年6月13日'
- en: ''
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Resources:** [GitHub](https://github.com/deepmind/deepmind-research/tree/master/byol)'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**资源：** [GitHub](https://github.com/deepmind/deepmind-research/tree/master/byol)'
- en: ''
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Category:** similarity learning, representation learning, computer vision,
    foundation models'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**类别：** 相似性学习、表征学习、计算机视觉、基础模型'
- en: ''
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[**Other Walkthroughs**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**:**'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[**其他讲解**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**:**'
- en: '[[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)]
    — [[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)]
    — [[Segment Anything](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    — [[Depth Anything](https://medium.com/towards-data-science/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?sk=fc6197edd68e6137c3396c83e50f65cb)]
    — [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    — [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)]
    — [[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)]
    — [[Segment Anything](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    — [[Depth Anything](https://medium.com/towards-data-science/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?sk=fc6197edd68e6137c3396c83e50f65cb)]
    — [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    — [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
- en: Outline
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大纲
- en: Context & Background
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 背景与概述
- en: Claimed Contributions
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 声称的贡献
- en: Method
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 方法
- en: Experiments
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实验
- en: Conclusion
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结论
- en: Further Readings & Resources
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进一步阅读与资源
- en: Context & Background
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景与概述
- en: BYOL falls into the category of self-supervised representation learning via
    similarity learning. Self-supervised means no explicit ground truth labels are
    provided, but a supervision signal might be constructed from unlabeled data. Representation
    learning means the model learns to encode its input into a lower dimensional and
    semantically rich representation space. And finally in similarity learning features
    that are similar are mapped close to each other in the latent representation space,
    while non-similar features are mapped further apart. These representations are
    crucial in many deep learning tasks that built upon these representations to for
    example generate new data, perform classification, segmentation or monocular depth
    estimation.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: BYOL 属于通过相似性学习进行的自监督表示学习。自监督意味着没有提供明确的真实标签，但可以从未标记的数据中构建监督信号。表示学习意味着模型学习将输入编码到一个维度较低且语义丰富的表示空间中。最后，在相似性学习中，相似的特征在潜在表示空间中被映射得相互接近，而不相似的特征则被映射得更远。这些表示在许多深度学习任务中至关重要，这些任务利用这些表示来生成新数据、执行分类、分割或单目深度估计等。
- en: Many successful methods, such as [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500),
    [GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05),
    [MoCo](https://arxiv.org/abs/1911.05722) or [SimCLR](https://arxiv.org/abs/2002.05709)
    use a contrastive learning approach. In contrastive learning, a score for matching
    data pairs is maximized, while a score for non-matching data is minimized. This
    process does heavily depend on the batch size and the number of negative samples
    provided during training. This dependency makes data collection and training more
    challenging.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 许多成功的方法，如[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)、[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)、[MoCo](https://arxiv.org/abs/1911.05722)或[SimCLR](https://arxiv.org/abs/2002.05709)使用了对比学习的方法。在对比学习中，最大化匹配数据对的得分，同时最小化不匹配数据的得分。这一过程严重依赖于训练期间提供的批量大小和负样本数量。这种依赖性使得数据收集和训练变得更加具有挑战性。
- en: 'BYOL aims to:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: BYOL的目标是：
- en: Get rid for the need of negative samples and large batch size as required for
    contrastive learning.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 摆脱对对比学习所需的负样本和大批量大小的需求。
- en: Decrease the dependency to domain specific augmentations to be applicable to
    other domains as language or images.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 减少对特定领域数据增强的依赖，使其适用于其他领域，如语言或图像。
- en: Among many references made in the paper, BYOL highlights its similarities to
    [mean teacher](https://arxiv.org/abs/1703.01780), the [momentum encoder](https://arxiv.org/abs/1911.05722)
    and [predictions of bootstrapped latents (PBL)](https://arxiv.org/abs/2004.14646).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在论文中提到的许多参考文献中，BYOL 突出了它与[平均教师](https://arxiv.org/abs/1703.01780)、[动量编码器](https://arxiv.org/abs/1911.05722)和[引导潜在预测
    (PBL)](https://arxiv.org/abs/2004.14646)的相似性。
- en: Claimed Contributions (According to Authors)
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 声称的贡献（根据作者的说法）
- en: Introduction of BYOL (Bootstrap your own latent), a self-supervised representation
    learning method that does not require negative pairs (as in contrastive learning)
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BYOL（Bootstrap your own latent）的介绍，这是一种自监督表示学习方法，不需要负对（如对比学习中的）。
- en: BYOL representations are shown to outperform the state-of-the-art (at the time
    of the paper’s release)
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BYOL 表示法被证明优于当时的最新技术水平（论文发布时）。
- en: BYOL is shown to be more resilient to batch size and used image augmentations
    compared to its contrastive counterparts
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与对比学习法相比，BYOL 显示出对批量大小和使用的图像增强方法更具弹性。
- en: '![Sascha Kirch](../Images/3edf0b4a499cde306202656453c7fe0a.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![Sascha Kirch](../Images/3edf0b4a499cde306202656453c7fe0a.png)'
- en: '[Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----5d0a26983d7c--------------------------------)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----5d0a26983d7c--------------------------------)'
- en: Paper Walkthroughs by Sascha Kirch
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Sascha Kirch 的论文解析
- en: '[View list](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2?source=post_page-----5d0a26983d7c--------------------------------)7
    stories![“DDPM — Denoising Diffusion Probabilistic Models “ paper illustration
    by Sascha Kirch](../Images/6e785c0a911386676abebe0fa646f483.png)![“Depth Anything”
    paper illustration by Sascha Kirch](../Images/bd8cd71a02e42cf64d0afd39f41f48e0.png)![](../Images/8708d91a4a1902cef889ced95d46fc39.png)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2?source=post_page-----5d0a26983d7c--------------------------------)7
    个故事！[“DDPM — 去噪扩散概率模型” 论文插图，由 Sascha Kirch 绘制](../Images/6e785c0a911386676abebe0fa646f483.png)![“Depth
    Anything” 论文插图，由 Sascha Kirch 绘制](../Images/bd8cd71a02e42cf64d0afd39f41f48e0.png)![](../Images/8708d91a4a1902cef889ced95d46fc39.png)'
- en: Method
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法
- en: Now that we have seen what BYOL claims to solve let’s try to understand how
    this is achieved. First let’s observe the architecture presented in Fig.1
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了 BYOL 声称要解决的问题，让我们尝试理解如何实现这一目标。首先，让我们观察图 1 中呈现的架构。
- en: '![](../Images/153409ea2efbcbf75ad2378f1063bab6.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/153409ea2efbcbf75ad2378f1063bab6.png)'
- en: 'Fig. 1: Framework architecture. [Image Source](https://arxiv.org/abs/2006.07733)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：框架架构。 [图像来源](https://arxiv.org/abs/2006.07733) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    的注释。
- en: 'BYOL consists of two networks: the online network and the target network. The
    online network consists of three submodules, namely the encoder, projector and
    predictor. The target network consists of two submodules, namely the encoder and
    projector. The encoder and predictor of both networks share the exact same architecture,
    they only differ in their model weights. While the online network is optimized
    during training, the target network updates its weights by an exponentially moving
    average of itself and the online network.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: BYOL 由两个网络组成：在线网络和目标网络。在线网络由三个子模块组成，即编码器、投影器和预测器。目标网络由两个子模块组成，即编码器和投影器。两个网络的编码器和预测器具有完全相同的架构，它们仅在模型权重上有所不同。在线网络在训练过程中进行优化，而目标网络则通过在线网络和自身的指数移动平均来更新其权重。
- en: '**Encoder** — The encoder consists of a ResNet convolutional neuronal network.
    It translates the input image into a latent representation.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**编码器** —— 编码器由一个 ResNet 卷积神经网络组成。它将输入图像转换为潜在表示。'
- en: '**Projector** — Projects the latent space from a 4096-dimensional space into
    a 256-dimensional space via a multi-layer perceptron network (MLP). I guess the
    projector is not critical for the framework to work, but 256 is simply a convenient
    output dimension often used in the field of representation learning.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**投影器** —— 通过多层感知器网络（MLP）将潜在空间从 4096 维空间投影到 256 维空间。我猜测投影器对框架的工作并不关键，但 256
    只是表示学习领域中常用的方便输出维度。'
- en: '**Predictor** — Aims to predict the projected latent space of the target network
    from the projected latent space of the online network. Crucial to avoid representation
    collapse.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测器** —— 旨在从在线网络的投影潜在空间中预测目标网络的投影潜在空间。避免表示崩溃至关重要。'
- en: 'During training, two different and randomly selected augmentations are applied
    on an input image to construct two different views of that image. One view is
    fed into the online model and another view is fed into the target model. These
    augmentations include among others: resizing, flipping, cropping, color distortion,
    grayscale conversion, Gaussian blur and saturation. The training objective is
    to minimize the squared L2-distance between both networks output. After training,
    **only the encoder of the online network is kept as the final model!**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，对输入图像应用两个不同且随机选择的增强来构建该图像的两个不同视图。一个视图被输入到在线模型中，另一个视图被输入到目标模型中。这些增强包括但不限于：调整大小、翻转、裁剪、颜色扭曲、灰度转换、高斯模糊和饱和度。训练目标是最小化两个网络输出之间的平方L2距离。训练后，**最终只保留在线网络的编码器作为最终模型！**
- en: 'That’s all. Easy, right? 😜 Well, after reading the paper my face was more like
    this: 😵 While it is relatively straight forward to understand the processing of
    the framework if you break it down to its key components, gaining a intuition
    did cost me quite some time.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。简单，对吧？😜 好吧，看完论文后我的表情更像这样：😵 虽然将框架的处理过程分解为其关键组件相对直接，但获得直觉确实花费了我不少时间。
- en: Before we try to gain some intuition of why BYOL actually works, let’s first
    strip down the presented equations and demystify them.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们尝试理解为什么BYOL实际有效之前，让我们首先简化呈现的方程并揭示它们的奥秘。
- en: Math Demystification
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数学揭秘
- en: Having a rough overview of BYOL’s architecture and how it is trained, let’s
    have a closer look at the equations. I have to say, the math part presented in
    the paper is way more complicated than it needs to be. While in some cases it
    is presented way to complex, in other cases it lags on clarity and leaves room
    for interpretation causing confusion.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在大致了解了BYOL的架构及其训练方式后，让我们仔细看看这些方程。我不得不说，论文中呈现的数学部分比实际需要的要复杂得多。在某些情况下，它的展示方式过于复杂，而在其他情况下，它在清晰度上滞后，留下了解释的空间，造成混淆。
- en: I’ll focus on those equations from which I think are important to understand
    what is happening. Let’s start by analyzing them in the exact reversed order,
    because why not? 😜
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我会专注于那些我认为理解发生了什么的方程。我们从精确倒序分析这些方程开始，为什么不呢？😜
- en: 'First, let’s talk about the update of the models’ parameters during training.
    Recall that we have two models: the online model and the target model. The online
    model is updated by optimizing a loss function using a LARS optimizer.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们谈谈训练期间模型参数的更新。回顾一下，我们有两个模型：在线模型和目标模型。在线模型通过使用LARS优化器优化损失函数来更新。
- en: '![](../Images/b3fdffacdfd667decdc3fe64eaba9260.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3fdffacdfd667decdc3fe64eaba9260.png)'
- en: 'Equation 1: Weight update of online network. [Source](https://arxiv.org/abs/2006.07733)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 1：在线网络的权重更新。[来源](https://arxiv.org/abs/2006.07733) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    的注释
- en: 'The equation above simply says: “update the model’s parameters theta by calling
    an optimizer function upon the current parameters, the gradients of these parameters
    with respect to a loss function and a learning rate eta”.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的方程简单地说：“通过调用优化器函数对当前参数、这些参数相对于损失函数的梯度以及学习率eta进行更新，来更新模型的参数theta”。
- en: 'The target model on the other hand is not updated via optimization but by copying
    the weights from the online model and applying an exponential moving average on
    the copied updated weights and the current weights of the target network:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，目标模型不是通过优化来更新，而是通过从在线模型中复制权重，并对复制的更新权重和目标网络的当前权重应用指数移动平均：
- en: '![](../Images/9325b82871ba916524d9e0601e02b0ab.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9325b82871ba916524d9e0601e02b0ab.png)'
- en: 'Equation 2: Weight update of target network. [Source](https://arxiv.org/abs/2006.07733)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2：目标网络的权重更新。[来源](https://arxiv.org/abs/2006.07733) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    的注释
- en: 'The equation above simply says: “update the model’s parameter xi by calculating
    an exponential moving average with the decay rate tau of the current weights xi
    and the updated weights of the online model”. Tau follows a cosine schedule to
    decrease the contribution of the online model throughout the training.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的方程简单地说：“通过计算当前权重xi和在线模型的更新权重的指数移动平均来更新模型的参数xi”。Tau遵循余弦调度，以减少整个训练过程中在线模型的贡献。
- en: Now let’s have a look on the loss function used to update the online model.
    It is defined as the sum of two other loss functions. These losses share the same
    equation as we will see later but are calculated on two different inputs of the
    network. Recall from Fig. 1\. that two different views (i.e. v and v’) are generated
    from an image x by applying different augmentations. One view is input into the
    online model and the other one into the target model. During training, two forward
    passes are performed before calculating the loss, where the input for the networks
    are swapped. The image input to the online model is input into the target model
    and vice versa.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看用于更新在线模型的损失函数。它被定义为两个其他损失函数的和。这些损失函数具有相同的方程，稍后我们将看到，但在网络的两个不同输入上计算。回忆一下图
    1\.，从图像 x 生成了两个不同的视角（即 v 和 v'），通过应用不同的增强。一个视角输入到在线模型中，另一个输入到目标模型中。在训练期间，计算损失之前执行两个前向传递，其中网络的输入会交换。输入到在线模型的图像会输入到目标模型中，反之亦然。
- en: '![](../Images/045f6f999186a4c829428be4c00eca0e.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/045f6f999186a4c829428be4c00eca0e.png)'
- en: 'Equation 3: BYOL’s loss function. [Source](https://arxiv.org/abs/2006.07733)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 3：BYOL 的损失函数。 [来源](https://arxiv.org/abs/2006.07733) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    的注释
- en: 'The loss for the individual forward passes is a squared L2 distance of the
    L2-normalized outputs of the online model and the target model. Let’s break down
    the corresponding equation from the paper:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 个体前向传递的损失是在线模型和目标模型的 L2 归一化输出的平方 L2 距离。让我们分解论文中的相应方程：
- en: '![](../Images/2e456b27a6689941529b2fb798f4c768.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e456b27a6689941529b2fb798f4c768.png)'
- en: 'Equation 4: Individual loss function. [Source](https://arxiv.org/abs/2006.07733)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 4：个体损失函数。 [来源](https://arxiv.org/abs/2006.07733) + [Sascha Kirch](https://medium.com/@SaschaKirch)
    的注释
- en: 'Note: The paper says that this is a mean squared error, which is actually not
    correct. The L2-distance does not divide by its number of elements. I guess they
    confused it with calculating the mean over all batches.'
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注：论文中提到这是均方误差，实际上并不正确。L2 距离没有除以其元素数量。我猜他们将其与计算所有批次的均值混淆了。
- en: Intuition of BYOL
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BYOL 的直觉
- en: Now as we are equipped with an understanding of the framework and the key message
    of the equations, let us try to gain some intuition. I’ll present you what the
    authors think and then I’ll try to add some intuition of my own, well knowing
    it might not be accurate 🤡.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经理解了框架和方程的核心信息，让我们尝试获得一些直觉。我会呈现作者的观点，然后我会尝试加入一些我自己的直觉，虽然知道这可能不准确 🤡。
- en: '***How does BYOL learn its representations?*** — The model is encouraged to
    generate the same latent representation of its two inputs, which represent two
    different views of the same object/scene. A cat is still a cat regardless of the
    image being blurred, in grayscale or flipped. In fact, I think the heavy augmentations
    are crucial here. It basically tells the model “Look, these are different variations
    of the same thing, so ignore these variations and consider them equal when extracting
    representations of the object/scene!”.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '***BYOL 如何学习其表示？*** — 模型被鼓励生成其两个输入的相同潜在表示，这两个输入代表了同一对象/场景的不同视角。无论图像是模糊的、黑白的还是翻转的，猫仍然是猫。事实上，我认为强大的数据增强在这里至关重要。它基本上告诉模型：“看，这些是同一事物的不同变体，所以忽略这些变体，当提取对象/场景的表示时，将它们视为相等！”。'
- en: '***Why are the representations not collapsing?*** — Recall that earlier we
    said, BYOL falls into the category of similarity learning. Wouldn’t it be the
    easiest way for the network, to just map everything into the same point in the
    latent space to achieve the highest similarity? In fact, this is one of the mayor
    difficulties in similarity learning and is called “collapsing solutions”. Contrastive
    learning approaches solve this issue by providing many negative samples for a
    given match to map similar features close to each other in the latent space while
    mapping dissimilar features farther apart. BYOL solves this issue by introducing
    an asymmetry between the online and the target network with their predictor submodule
    and by employing an update rule for the target network parameters based on the
    exponentially moving average to ensure near optimality of the predictor throughout
    training.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '***为什么表示没有崩溃？*** — 回顾之前我们提到，BYOL属于相似性学习的范畴。网络不会最简单地将所有内容映射到潜在空间的同一点以实现最高相似度吗？实际上，这是相似性学习中的一个主要困难，称为“崩溃解决方案”。对比学习方法通过为每个匹配提供许多负样本来解决这个问题，以将相似特征映射到潜在空间中的彼此接近，同时将不相似的特征映射到更远的位置。BYOL通过在在线网络和目标网络之间引入不对称性及其预测子模块，并通过基于指数移动平均的更新规则来解决这个问题，以确保在整个训练过程中预测器的近似最优。'
- en: '[](https://medium.com/@SaschaKirch/subscribe?source=post_page-----5d0a26983d7c--------------------------------)
    [## Get an email whenever Sascha Kirch publishes 🚀'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@SaschaKirch/subscribe?source=post_page-----5d0a26983d7c--------------------------------)
    [## 获取Sascha Kirch发布的新邮件通知 🚀'
- en: Get an email whenever Sascha Kirch publishes 🚀 Looking to learn more about deep
    learning or simply stay up to date…
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 获取Sascha Kirch发布的新邮件通知 🚀 想要了解更多深度学习知识或保持最新动态…
- en: medium.com](https://medium.com/@SaschaKirch/subscribe?source=post_page-----5d0a26983d7c--------------------------------)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@SaschaKirch/subscribe?source=post_page-----5d0a26983d7c--------------------------------)
- en: Experiments
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验
- en: The authors of BYOL presented experiments and ablations to demonstrate the effectiveness
    of their method.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: BYOL的作者展示了实验和消融研究，以证明他们方法的有效性。
- en: '*Ablation on Batch Size*'
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*批量大小的消融研究*'
- en: From contrastive representation learning methods (e.g. [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    and [GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05))
    we know that there is a large dependence on the batch size during training. [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    for example was trained on a batch size of 32,768, which is crazy considering
    it is a multi-modal language-image model.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 从对比表示学习方法（例如，[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)和[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)）我们知道，在训练过程中，批量大小有很大的依赖性。例如，[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)是在批量大小为32,768的情况下训练的，这在考虑到它是一个多模态语言-图像模型时显得非常疯狂。
- en: The authors claim, since BYOL does not require negative samples, it is not as
    sensitive to lower batch sizes which they backup with the following experiment
    shown in Fig.2.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 作者声称，由于BYOL不需要负样本，它对较低批量大小的敏感度较低，他们通过图2所示的实验进行了验证。
- en: '![](../Images/4bc8059a6403de2b0bca72cd8eaf9d4b.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4bc8059a6403de2b0bca72cd8eaf9d4b.png)'
- en: 'Fig. 2: Impact of batch size. [Image Source](https://arxiv.org/abs/2006.07733)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：批量大小的影响。[图片来源](https://arxiv.org/abs/2006.07733) + [Sascha Kirch](https://medium.com/@SaschaKirch)的注释。
- en: Sadly, this might still be too large for my private laptop 😅
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 可惜，这对于我的私人笔记本电脑来说可能仍然太大了 😅
- en: Ablation on Robustness of Image Augmentations
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像增强的鲁棒性消融研究
- en: The [SimCLR](https://arxiv.org/abs/2002.05709) paper has shown that contrastive
    vision methods are sensitive to their choice on image augmentations, especially
    those affecting the color histogram. While crops of the same image share a similar
    color histogram, crops of negative pairs don’t. The model can take a shortcut
    during training and focus on differences in color histograms rather than the semantic
    features.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[SimCLR](https://arxiv.org/abs/2002.05709)论文表明，对比视觉方法对图像增强的选择非常敏感，特别是那些影响颜色直方图的增强。虽然相同图像的裁剪部分共享类似的颜色直方图，但负样本对的裁剪部分则不然。模型在训练过程中可能会走捷径，专注于颜色直方图的差异，而不是语义特征。'
- en: The authors claim that BYOL is more robust towards their choice of image augmentations,
    because of the way the online and target networks are updated. While this hypothesis
    is backed up by an experiment, there is still a strong dependency and hence a
    drop in performance.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们声称，BYOL对图像增强选择的鲁棒性更强，因为在线和目标网络的更新方式。虽然这一假设得到了实验的支持，但仍然存在较强的依赖性，因此性能有所下降。
- en: '![](../Images/29ea63b28df3e850c03a04e04176a409.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29ea63b28df3e850c03a04e04176a409.png)'
- en: 'Fig. 3: Robustness towards image augmentations. [Image Source](https://arxiv.org/abs/2006.07733)
    + annotations by [Sascha Kirch](https://medium.com/@SaschaKirch).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：对图像增强的鲁棒性。[图片来源](https://arxiv.org/abs/2006.07733) + [Sascha Kirch](https://medium.com/@SaschaKirch)的注释。
- en: Linear Evaluation on ImageNet
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在ImageNet上的线性评估
- en: In the field of representation learning, an important characteristic is the
    model’s ability project semantically rich features into a latent space, to cluster
    similar features and to separate dissimilar features. A common test is to freeze
    the model (in case of BYOL only the encoder of the online model) and to train
    a linear classifier on top of the representations.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在表示学习领域，一个重要的特征是模型将语义丰富的特征投射到潜在空间的能力，以便对相似特征进行聚类，并将不同的特征分开。一个常见的测试是冻结模型（对于BYOL，只冻结在线模型的编码器），并在表示的基础上训练线性分类器。
- en: Linear evaluation of BYOL has been performed on ImageNet and has been compared
    to many other models and outperforms the previous state-of-the-art of that time.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: BYOL的线性评估已在ImageNet上进行，并与许多其他模型进行了比较，超越了当时的最新技术。
- en: You’ll find in many papers the differentiation between ResNet-50 encoder and
    other variations of ResNet. It’s just that the ResNet-50 has been emerged to be
    the standard network to evaluate performance on.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 你会在许多论文中发现ResNet-50编码器与其他ResNet变体的区别。只是ResNet-50已经成为评估性能的标准网络。
- en: '![](../Images/16b12891474954baee3a1cb16206edaf.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/16b12891474954baee3a1cb16206edaf.png)'
- en: 'Table 1: Linear evaluation on ImageNet. [Source](https://arxiv.org/abs/2006.07733)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：在ImageNet上的线性评估。[来源](https://arxiv.org/abs/2006.07733)
- en: Semi-Supervised Fine-Tuning for classification
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 半监督微调用于分类
- en: Another very typical experiment setup in representation learning is the model’s
    performance when fine-tuned to a specific downstream task and dataset.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 表示学习中的另一个典型实验设置是模型在特定下游任务和数据集上进行微调时的表现。
- en: Table 2 depicts the metrics when finetuning BYOL on a classification task using
    either 1% or 10% of the entire ImageNet training set.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 表2展示了在分类任务上对BYOL进行微调时使用1%或10%的整个ImageNet训练集的指标。
- en: '![](../Images/770661dfb170a1d00aa5b21ca752decc.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/770661dfb170a1d00aa5b21ca752decc.png)'
- en: 'Table 2: Semi-supervised training on ImageNet. [Source](https://arxiv.org/abs/2006.07733)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：在ImageNet上的半监督训练。[来源](https://arxiv.org/abs/2006.07733)
- en: Transfer to Other Vision Tasks
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迁移到其他视觉任务
- en: The authors also present experiments where they transfer-learn BYOL on a semantic
    segmentation task and a monocular depth estimation task, two other important fields
    of computer vision.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们还展示了他们在语义分割任务和单目深度估计任务（计算机视觉的两个重要领域）中迁移学习BYOL的实验。
- en: The differences to previous approaches are marginal, but I guess the key message
    here is, “We have a different approach that works just as good”
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的方法相比，差异微乎其微，但我想这里的关键信息是，“我们有一种不同的方法，效果同样出色。”
- en: '![](../Images/a569a55cb7fd46bed3e371110e8cf386.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a569a55cb7fd46bed3e371110e8cf386.png)'
- en: 'Table 3: Transfer to other vision tasks. [Source](https://arxiv.org/abs/2006.07733)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：迁移到其他视觉任务。[来源](https://arxiv.org/abs/2006.07733)
- en: Conclusion
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: BYOL presented an alternative approach for self-supervised representation learning.
    By implementing two networks that perform similarity learning, BYOL can be trained
    without the need of negative training samples like those needed for contrastive
    learning approaches. To avoid collapsing solutions the target network is updated
    via EMA from the online network and an extra prediction sub-module is built on
    top of the online network.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: BYOL提出了一种自监督表示学习的替代方法。通过实现两个进行相似性学习的网络，BYOL可以在没有对比学习方法所需的负样本的情况下进行训练。为了避免崩溃解，目标网络通过EMA从在线网络中更新，并在在线网络之上构建了一个额外的预测子模块。
- en: Further Readings & Resources
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读与资源
- en: 'If you have made it so far: congratulations🎉 and thank you😉! Since it seems
    that you are quite interested in the topic, here are some further resources:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经读到这里：恭喜🎉并感谢😉！既然你似乎对这个话题很感兴趣，这里有一些进一步的资源：
- en: 'Following a list of papers that built upon BYOL:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是基于BYOL的论文列表：
- en: '[DINO: Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294)'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[DINO: 自监督视觉变换器中的新兴特性](https://arxiv.org/abs/2104.14294)'
- en: '[DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193)'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[DINOv2: 无监督学习强健的视觉特征](https://arxiv.org/abs/2304.07193)'
- en: 'Here are two of my articles about the contrastive learning methods [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    and [GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)
    for self-supervised representation learning:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我关于对比学习方法的两篇文章：[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)和[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)，用于自监督表示学习：
- en: '[](/the-clip-foundation-model-7770858b487d?source=post_page-----5d0a26983d7c--------------------------------)
    [## The CLIP Foundation Model'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/the-clip-foundation-model-7770858b487d?source=post_page-----5d0a26983d7c--------------------------------)
    [## CLIP基础模型'
- en: Paper Summary— Learning Transferable Visual Models From Natural Language Supervision
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 论文总结——从自然语言监督中学习可迁移的视觉模型
- en: 'towardsdatascience.com](/the-clip-foundation-model-7770858b487d?source=post_page-----5d0a26983d7c--------------------------------)
    [](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=post_page-----5d0a26983d7c--------------------------------)
    [## GLIP: Introducing Language-Image Pre-Training to Object Detection'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 'towardsdatascience.com](/the-clip-foundation-model-7770858b487d?source=post_page-----5d0a26983d7c--------------------------------)
    [](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=post_page-----5d0a26983d7c--------------------------------)
    [## GLIP: 将语言-图像预训练引入目标检测'
- en: 'Paper Summary: Grounded Language-Image Pre-training'
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 论文总结：基础语言-图像预训练
- en: towardsdatascience.com](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=post_page-----5d0a26983d7c--------------------------------)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=post_page-----5d0a26983d7c--------------------------------)
