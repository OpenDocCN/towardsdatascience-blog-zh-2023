- en: Decision Trees for Classificationâ€Šâ€”â€ŠComplete Example
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆ†ç±»å†³ç­–æ ‘â€”â€”å®Œæ•´ç¤ºä¾‹
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/decision-trees-for-classification-complete-example-d0bc17fcf1c2?source=collection_archive---------1-----------------------#2023-01-01](https://towardsdatascience.com/decision-trees-for-classification-complete-example-d0bc17fcf1c2?source=collection_archive---------1-----------------------#2023-01-01)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/decision-trees-for-classification-complete-example-d0bc17fcf1c2?source=collection_archive---------1-----------------------#2023-01-01](https://towardsdatascience.com/decision-trees-for-classification-complete-example-d0bc17fcf1c2?source=collection_archive---------1-----------------------#2023-01-01)
- en: A detailed example how to construct a Decision Tree for classification
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…³äºå¦‚ä½•æ„å»ºåˆ†ç±»å†³ç­–æ ‘çš„è¯¦ç»†ç¤ºä¾‹
- en: '[](https://medium.com/@pumaline?source=post_page-----d0bc17fcf1c2--------------------------------)[![Datamapu](../Images/63b0c7f9a3d160c5bb039bbebd791f7e.png)](https://medium.com/@pumaline?source=post_page-----d0bc17fcf1c2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d0bc17fcf1c2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d0bc17fcf1c2--------------------------------)
    [Datamapu](https://medium.com/@pumaline?source=post_page-----d0bc17fcf1c2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@pumaline?source=post_page-----d0bc17fcf1c2--------------------------------)[![Datamapu](../Images/63b0c7f9a3d160c5bb039bbebd791f7e.png)](https://medium.com/@pumaline?source=post_page-----d0bc17fcf1c2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d0bc17fcf1c2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d0bc17fcf1c2--------------------------------)
    [Datamapu](https://medium.com/@pumaline?source=post_page-----d0bc17fcf1c2--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffcd72d75ae6e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-for-classification-complete-example-d0bc17fcf1c2&user=Datamapu&userId=fcd72d75ae6e&source=post_page-fcd72d75ae6e----d0bc17fcf1c2---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d0bc17fcf1c2--------------------------------)
    Â·8 min readÂ·Jan 1, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd0bc17fcf1c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-for-classification-complete-example-d0bc17fcf1c2&user=Datamapu&userId=fcd72d75ae6e&source=-----d0bc17fcf1c2---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffcd72d75ae6e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-for-classification-complete-example-d0bc17fcf1c2&user=Datamapu&userId=fcd72d75ae6e&source=post_page-fcd72d75ae6e----d0bc17fcf1c2---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d0bc17fcf1c2--------------------------------)
    Â·8åˆ†é’Ÿé˜…è¯»Â·2023å¹´1æœˆ1æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd0bc17fcf1c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-for-classification-complete-example-d0bc17fcf1c2&user=Datamapu&userId=fcd72d75ae6e&source=-----d0bc17fcf1c2---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd0bc17fcf1c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-for-classification-complete-example-d0bc17fcf1c2&source=-----d0bc17fcf1c2---------------------bookmark_footer-----------)![](../Images/a45b59b3ef0c4737f791d8f710bbf118.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd0bc17fcf1c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-for-classification-complete-example-d0bc17fcf1c2&source=-----d0bc17fcf1c2---------------------bookmark_footer-----------)![](../Images/a45b59b3ef0c4737f791d8f710bbf118.png)'
- en: Photo by Fabrice Villard on Unsplash
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±Fabrice Villardæä¾›ï¼Œæ¥è‡ªUnsplash
- en: This article explains how we can use decision trees for classification problems.
    After explaining important terms, we will develop a decision tree for a simple
    example dataset.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡è§£é‡Šäº†æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨å†³ç­–æ ‘æ¥è§£å†³åˆ†ç±»é—®é¢˜ã€‚åœ¨è§£é‡Šé‡è¦æœ¯è¯­åï¼Œæˆ‘ä»¬å°†ä¸ºä¸€ä¸ªç®€å•çš„ç¤ºä¾‹æ•°æ®é›†æ„å»ºä¸€ä¸ªå†³ç­–æ ‘ã€‚
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: '[A decision tree is a decision support tool that uses a tree-like model of
    decisions and their possible consequences, including chance event outcomes, resource
    costs, and utility. It is one way to display an algorithm that only contains conditional
    control statements.](https://en.wikipedia.org/wiki/Decision_tree)'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[å†³ç­–æ ‘æ˜¯ä¸€ç§å†³ç­–æ”¯æŒå·¥å…·ï¼Œå®ƒä½¿ç”¨æ ‘çŠ¶æ¨¡å‹å±•ç¤ºå†³ç­–åŠå…¶å¯èƒ½çš„åæœï¼ŒåŒ…æ‹¬éšæœºäº‹ä»¶ç»“æœã€èµ„æºæˆæœ¬å’Œæ•ˆç”¨ã€‚è¿™æ˜¯ä¸€ç§ä»…åŒ…å«æ¡ä»¶æ§åˆ¶è¯­å¥çš„ç®—æ³•å±•ç¤ºæ–¹å¼ã€‚](https://en.wikipedia.org/wiki/Decision_tree)'
- en: 'Traditionally decision trees are drawn manually, but they can be learned using
    Machine Learning. They can be used for both regression and classification problems.
    In this article we will focus on classification problems. Letâ€™s consider the following
    example data:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼ ç»Ÿä¸Šï¼Œå†³ç­–æ ‘æ˜¯æ‰‹åŠ¨ç»˜åˆ¶çš„ï¼Œä½†å¯ä»¥é€šè¿‡æœºå™¨å­¦ä¹ è¿›è¡Œå­¦ä¹ ã€‚å®ƒä»¬å¯ç”¨äºå›å½’å’Œåˆ†ç±»é—®é¢˜ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†é‡ç‚¹å…³æ³¨åˆ†ç±»é—®é¢˜ã€‚è®©æˆ‘ä»¬è€ƒè™‘ä»¥ä¸‹ç¤ºä¾‹æ•°æ®ï¼š
- en: '![](../Images/0ff178c454666a8b72804aec2f7de623.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ff178c454666a8b72804aec2f7de623.png)'
- en: Example data (constructed by the author)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹æ•°æ®ï¼ˆç”±ä½œè€…æ„å»ºï¼‰
- en: Using this simplified example we will predict whether a person is going to be
    an astronaut, depending on their age, whether they like dogs, and whether they
    like gravity. Before discussing how to construct a decision tree, letâ€™s have a
    look at the resulting decision treeÂ forÂ ourÂ exampleÂ data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¿™ä¸ªç®€åŒ–çš„ä¾‹å­ï¼Œæˆ‘ä»¬å°†é¢„æµ‹ä¸€ä¸ªäººæ˜¯å¦ä¼šæˆä¸ºå®‡èˆªå‘˜ï¼Œå–å†³äºä»–ä»¬çš„å¹´é¾„ã€æ˜¯å¦å–œæ¬¢ç‹—ä»¥åŠæ˜¯å¦å–œæ¬¢é‡åŠ›ã€‚åœ¨è®¨è®ºå¦‚ä½•æ„å»ºå†³ç­–æ ‘ä¹‹å‰ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹æˆ‘ä»¬ç¤ºä¾‹æ•°æ®çš„æœ€ç»ˆå†³ç­–æ ‘ã€‚
- en: '![](../Images/36dbbe9e24de192c40ec14269cf0efa3.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36dbbe9e24de192c40ec14269cf0efa3.png)'
- en: Final decision tree for example data
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹æ•°æ®çš„æœ€ç»ˆå†³ç­–æ ‘
- en: We can follow the paths to come to a decision. For example, we can see that
    a person who doesnâ€™t like gravity is not going to be an astronaut, independent
    of the other features. On the other side, we can also see, that a person who likes
    gravity and likes dogs is going to be an astronaut independent of the age.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è·Ÿè¸ªè·¯å¾„æ¥åšå‡ºå†³å®šã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œä¸å–œæ¬¢é‡åŠ›çš„äººä¸ä¼šæˆä¸ºå®‡èˆªå‘˜ï¼Œä¸å…¶ä»–ç‰¹å¾æ— å…³ã€‚å¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥çœ‹åˆ°ï¼Œå–œæ¬¢é‡åŠ›å’Œå–œæ¬¢ç‹—çš„äººå°†ä¼šæˆä¸ºå®‡èˆªå‘˜ï¼Œä¸å¹´é¾„æ— å…³ã€‚
- en: Before going into detail how this tree is constructed, letâ€™s define some important
    terms.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¯¦ç»†è®¨è®ºå¦‚ä½•æ„å»ºè¿™æ£µæ ‘ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å®šä¹‰ä¸€äº›é‡è¦çš„æœ¯è¯­ã€‚
- en: Terms
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœ¯è¯­
- en: Root Node
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ ¹èŠ‚ç‚¹
- en: The top-level node. The first decision that is taken. In our example the root
    node is â€˜likes gravityâ€™.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: é¡¶çº§èŠ‚ç‚¹ã€‚åšå‡ºçš„ç¬¬ä¸€ä¸ªå†³ç­–ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæ ¹èŠ‚ç‚¹æ˜¯â€œå–œæ¬¢é‡åŠ›â€ã€‚
- en: Branches
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ†æ”¯
- en: Branches represent sub-trees. Our example has two branches. One branch is, e.g.
    the sub-tree from â€˜likes dogsâ€™ and the second one from â€˜age < 40.5â€™ on.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†æ”¯ä»£è¡¨å­æ ‘ã€‚æˆ‘ä»¬çš„ä¾‹å­æœ‰ä¸¤ä¸ªåˆ†æ”¯ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªåˆ†æ”¯æ˜¯ä»â€œå–œæ¬¢ç‹—â€å¼€å§‹çš„å­æ ‘ï¼Œå¦ä¸€ä¸ªæ˜¯ä»â€œå¹´é¾„ < 40.5â€å¼€å§‹çš„å­æ ‘ã€‚
- en: Node
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: èŠ‚ç‚¹
- en: A node represents a split into further (child) nodes. In our example the nodes
    are â€˜likes gravityâ€™, â€˜likes dogsâ€™ and â€˜age < 40.5â€™.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªèŠ‚ç‚¹ä»£è¡¨è¿›ä¸€æ­¥ï¼ˆå­ï¼‰èŠ‚ç‚¹çš„åˆ‡åˆ†ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼ŒèŠ‚ç‚¹æœ‰â€œå–œæ¬¢é‡åŠ›â€ã€â€œå–œæ¬¢ç‹—â€å’Œâ€œå¹´é¾„ < 40.5â€ã€‚
- en: Leaf
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¶å­èŠ‚ç‚¹
- en: Leafs are at the end of the branches, i.e. they donâ€™t split any more. They represent
    possible outcomes for each action. In our example the leafs are represented by
    â€˜yesâ€™ and â€˜noâ€™.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å¶å­èŠ‚ç‚¹ä½äºåˆ†æ”¯çš„æœ«ç«¯ï¼Œå³ä¸å†è¿›è¡Œåˆ‡åˆ†ã€‚å®ƒä»¬ä»£è¡¨æ¯ä¸ªåŠ¨ä½œçš„å¯èƒ½ç»“æœã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œå¶å­èŠ‚ç‚¹ç”±â€œæ˜¯â€å’Œâ€œå¦â€è¡¨ç¤ºã€‚
- en: Parent Node
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: çˆ¶èŠ‚ç‚¹
- en: A node which precedes a (child) node is called a parent node. In our example
    â€˜likes gravityâ€™ is a parent node of â€˜likes dogsâ€™ and â€˜likes dogsâ€™ is a parent
    node of â€˜age < 40.5â€™.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªåœ¨ï¼ˆå­ï¼‰èŠ‚ç‚¹ä¹‹å‰çš„èŠ‚ç‚¹ç§°ä¸ºçˆ¶èŠ‚ç‚¹ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œâ€œå–œæ¬¢é‡åŠ›â€æ˜¯â€œå–œæ¬¢ç‹—â€çš„çˆ¶èŠ‚ç‚¹ï¼Œè€Œâ€œå–œæ¬¢ç‹—â€æ˜¯â€œå¹´é¾„ < 40.5â€çš„çˆ¶èŠ‚ç‚¹ã€‚
- en: Child Node
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å­èŠ‚ç‚¹
- en: A node under another node is a child node. In our example â€˜likes dogsâ€™ is a
    child node of â€˜likes gravityâ€™ and â€˜age < 40.5â€™ is a child node of â€˜likes dogsâ€™.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªèŠ‚ç‚¹åœ¨å¦ä¸€ä¸ªèŠ‚ç‚¹ä¸‹æ–¹ç§°ä¸ºå­èŠ‚ç‚¹ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œâ€œå–œæ¬¢ç‹—â€æ˜¯â€œå–œæ¬¢é‡åŠ›â€çš„å­èŠ‚ç‚¹ï¼Œè€Œâ€œå¹´é¾„ < 40.5â€æ˜¯â€œå–œæ¬¢ç‹—â€çš„å­èŠ‚ç‚¹ã€‚
- en: Splitting
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ‡åˆ†
- en: The process of dividing a node into two (child) nodes.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å°†ä¸€ä¸ªèŠ‚ç‚¹åˆ’åˆ†ä¸ºä¸¤ä¸ªï¼ˆå­ï¼‰èŠ‚ç‚¹çš„è¿‡ç¨‹ã€‚
- en: Pruning
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‰ªæ
- en: Removing the (child) nodes of a parent node is called pruning. A tree is grown
    through splitting and shrunk through pruning. In our example, if we would remove
    the node â€˜age < 40.5â€™ we would prune the tree.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ç§»é™¤çˆ¶èŠ‚ç‚¹çš„ï¼ˆå­ï¼‰èŠ‚ç‚¹ç§°ä¸ºå‰ªæã€‚æ ‘é€šè¿‡åˆ‡åˆ†ç”Ÿé•¿ï¼Œé€šè¿‡å‰ªæç¼©å°ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œå¦‚æœæˆ‘ä»¬ç§»é™¤èŠ‚ç‚¹â€œå¹´é¾„ < 40.5â€ï¼Œæˆ‘ä»¬å°†å¯¹æ ‘è¿›è¡Œå‰ªæã€‚
- en: '![](../Images/1556654976019b99810dc7f15c6695ca.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1556654976019b99810dc7f15c6695ca.png)'
- en: Decision tree illustration
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å†³ç­–æ ‘æ’å›¾
- en: We can also observe, that a decision tree allows us to mix data types. We can
    use numerical data (â€˜ageâ€™) and categorical data (â€˜likes dogsâ€™, â€˜likes gravityâ€™)
    in the same tree.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œå†³ç­–æ ‘å…è®¸æˆ‘ä»¬æ··åˆæ•°æ®ç±»å‹ã€‚æˆ‘ä»¬å¯ä»¥åœ¨åŒä¸€æ£µæ ‘ä¸­ä½¿ç”¨æ•°å€¼æ•°æ®ï¼ˆâ€œå¹´é¾„â€ï¼‰å’Œåˆ†ç±»æ•°æ®ï¼ˆâ€œå–œæ¬¢ç‹—â€ã€â€œå–œæ¬¢é‡åŠ›â€ï¼‰ã€‚
- en: Create a Decision Tree
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆ›å»ºå†³ç­–æ ‘
- en: 'The most important step in creating a decision tree, is the *splitting* of
    the data. We need to find a way to split the data set (*D*) into two data sets
    (*D_1*) and (*D_2*). There are different criteria that can be used in order to
    find the next split, for an overview see e.g. [here](https://www.analyticsvidhya.com/blog/2020/06/4-ways-split-decision-tree/#:~:text=Steps%20to%20split%20a%20decision%20tree%20using%20Information%20Gain%3A,entropy%20or%20highest%20information%20gain).
    We will concentrate on one of them: the [*Gini Impurity*](https://www.learndatasci.com/glossary/gini-impurity/#:~:text=a%20simple%20dataset-,What%20is%20Gini%20Impurity%3F,nodes%20to%20form%20the%20tree.),
    which is a criterion for categorical target variables and also the criterion used
    by the Python library [scikit-learn](https://scikit-learn.org/stable/modules/tree.html#classification).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ›å»ºå†³ç­–æ ‘ä¸­æœ€é‡è¦çš„æ­¥éª¤æ˜¯å¯¹æ•°æ®çš„ *åˆ†è£‚*ã€‚æˆ‘ä»¬éœ€è¦æ‰¾åˆ°ä¸€ç§æ–¹æ³•å°†æ•°æ®é›† (*D*) åˆ†è£‚ä¸ºä¸¤ä¸ªæ•°æ®é›† (*D_1*) å’Œ (*D_2*)ã€‚å¯ä»¥ä½¿ç”¨ä¸åŒçš„æ ‡å‡†æ¥å¯»æ‰¾ä¸‹ä¸€ä¸ªåˆ†è£‚ï¼Œæ¦‚è§ˆè§ä¾‹å¦‚
    [è¿™é‡Œ](https://www.analyticsvidhya.com/blog/2020/06/4-ways-split-decision-tree/#:~:text=Steps%20to%20split%20a%20decision%20tree%20using%20Information%20Gain%3A,entropy%20or%20highest%20information%20gain)ã€‚æˆ‘ä»¬å°†é›†ä¸­äºå…¶ä¸­ä¸€ä¸ªæ ‡å‡†ï¼š[åŸºå°¼ä¸çº¯åº¦](https://www.learndatasci.com/glossary/gini-impurity/#:~:text=a%20simple%20dataset-,What%20is%20Gini%20Impurity%3F,nodes%20to%20form%20the%20tree.)ï¼Œå®ƒæ˜¯ä¸€ä¸ªç”¨äºåˆ†ç±»ç›®æ ‡å˜é‡çš„æ ‡å‡†ï¼Œä¹Ÿæ˜¯
    Python åº“ [scikit-learn](https://scikit-learn.org/stable/modules/tree.html#classification)
    ä½¿ç”¨çš„æ ‡å‡†ã€‚
- en: Gini Impurity
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŸºå°¼ä¸çº¯åº¦
- en: 'The Gini Impurity for a data set *D* is calculated as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›† *D* çš„åŸºå°¼ä¸çº¯åº¦è®¡ç®—å¦‚ä¸‹ï¼š
- en: '![](../Images/dcd4ca062f20d515c6c01822250af8fa.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dcd4ca062f20d515c6c01822250af8fa.png)'
- en: with n = n_1 + n_2 the size of the data set (D) and
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ n = n_1 + n_2 è¡¨ç¤ºæ•°æ®é›† (D) çš„å¤§å°ï¼Œå¹¶ä¸”
- en: '![](../Images/55268b5346592788b91d1ebd51acdd75.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55268b5346592788b91d1ebd51acdd75.png)'
- en: with *D_1* and *D_2* subsets of *D*, ğ‘_ğ‘— the probability of samples belonging
    to class ğ‘— at a given node, and ğ‘ the number of classes. The lower the Gini Impurity,
    the higher is the homogeneity of the node. The Gini Impurity of a pure node is
    zero. To split a decision tree using Gini Impurity, the following steps need to
    be performed.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*D_1* å’Œ *D_2* æ˜¯ *D* çš„å­é›†ï¼Œğ‘_ğ‘— æ˜¯åœ¨ç»™å®šèŠ‚ç‚¹ä¸Šæ ·æœ¬å±äºç±»åˆ« ğ‘— çš„æ¦‚ç‡ï¼Œğ‘ æ˜¯ç±»åˆ«çš„æ•°é‡ã€‚åŸºå°¼ä¸çº¯åº¦è¶Šä½ï¼ŒèŠ‚ç‚¹çš„åŒè´¨æ€§è¶Šé«˜ã€‚çº¯èŠ‚ç‚¹çš„åŸºå°¼ä¸çº¯åº¦ä¸ºé›¶ã€‚ä¸ºäº†ä½¿ç”¨åŸºå°¼ä¸çº¯åº¦æ¥åˆ†è£‚å†³ç­–æ ‘ï¼Œéœ€è¦æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ã€‚'
- en: For each possible split, calculate the Gini Impurity of each child node
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªå¯èƒ½çš„åˆ†è£‚ï¼Œè®¡ç®—æ¯ä¸ªå­èŠ‚ç‚¹çš„åŸºå°¼ä¸çº¯åº¦
- en: Calculate the Gini Impurity of each split as the weighted average Gini Impurity
    of child nodes
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®¡ç®—æ¯ä¸ªåˆ†è£‚çš„åŸºå°¼ä¸çº¯åº¦ï¼Œä½œä¸ºå­èŠ‚ç‚¹åŸºå°¼ä¸çº¯åº¦çš„åŠ æƒå¹³å‡å€¼
- en: Select the split with the lowest value of Gini Impurity
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€‰æ‹©åŸºå°¼ä¸çº¯åº¦å€¼æœ€ä½çš„åˆ†è£‚
- en: Repeat steps 1â€“3 until no further split is possible.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: é‡å¤æ­¥éª¤ 1â€“3 ç›´åˆ°ä¸èƒ½å†åˆ†è£‚ä¸ºæ­¢ã€‚
- en: To understand this better, letâ€™s have a look at an example.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´å¥½åœ°ç†è§£è¿™ä¸€ç‚¹ï¼Œè®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªä¾‹å­ã€‚
- en: 'First Example: Decision Tree with two binary features'
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªç¤ºä¾‹ï¼šå…·æœ‰ä¸¤ä¸ªäºŒå…ƒç‰¹å¾çš„å†³ç­–æ ‘
- en: 'Before creating the decision tree for our entire dataset, we will first consider
    a subset, that only considers two features: â€˜likes gravityâ€™ and â€˜likes dogsâ€™.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸ºæ•´ä¸ªæ•°æ®é›†åˆ›å»ºå†³ç­–æ ‘ä¹‹å‰ï¼Œæˆ‘ä»¬å°†é¦–å…ˆè€ƒè™‘ä¸€ä¸ªå­é›†ï¼Œè¯¥å­é›†ä»…è€ƒè™‘ä¸¤ä¸ªç‰¹å¾ï¼šâ€œå–œæ¬¢é‡åŠ›â€å’Œâ€œå–œæ¬¢ç‹—â€ã€‚
- en: 'The first thing we have to decide is, which feature is going to be the *root
    node*. We do that by predicting the target with only one of the features and then
    use the feature, that has the lowest Gini Impurity as the root node. That is,
    in our case we build two shallow trees, with just the root node and two leafs.
    In the first case we use â€˜likes gravityâ€™ as a root node and in the second case
    â€˜likes dogsâ€™. We then calculate the Gini Impurity for both. The trees look like
    this:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆéœ€è¦å†³å®šå“ªä¸ªç‰¹å¾å°†ä½œä¸º *æ ¹èŠ‚ç‚¹*ã€‚æˆ‘ä»¬é€šè¿‡ä»…ç”¨ä¸€ä¸ªç‰¹å¾æ¥é¢„æµ‹ç›®æ ‡ï¼Œç„¶åé€‰æ‹©åŸºå°¼ä¸çº¯åº¦æœ€ä½çš„ç‰¹å¾ä½œä¸ºæ ¹èŠ‚ç‚¹ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸¤ä¸ªæµ…å±‚æ ‘ï¼Œåªæœ‰æ ¹èŠ‚ç‚¹å’Œä¸¤ä¸ªå¶å­ã€‚åœ¨ç¬¬ä¸€ä¸ªæ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨â€œå–œæ¬¢é‡åŠ›â€ä½œä¸ºæ ¹èŠ‚ç‚¹ï¼Œåœ¨ç¬¬äºŒä¸ªæ¡ˆä¾‹ä¸­ä½¿ç”¨â€œå–œæ¬¢ç‹—â€ã€‚ç„¶åæˆ‘ä»¬è®¡ç®—ä¸¤ä¸ªæ ‘çš„åŸºå°¼ä¸çº¯åº¦ã€‚è¿™äº›æ ‘çš„æ ·å­å¦‚ä¸‹ï¼š
- en: '![](../Images/a7cbf81c91434e4974dd0fa52e21864a.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a7cbf81c91434e4974dd0fa52e21864a.png)'
- en: Image by the author
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: 'The Gini Impurity for these trees are calculated as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ ‘çš„åŸºå°¼ä¸çº¯åº¦è®¡ç®—å¦‚ä¸‹ï¼š
- en: '**Case 1:**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ¡ˆä¾‹ 1ï¼š**'
- en: 'Dataset 1:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›† 1ï¼š
- en: '![](../Images/079a8cfb48bd78791b73edbf569b2bd3.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/079a8cfb48bd78791b73edbf569b2bd3.png)'
- en: 'Dataset 2:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›† 2ï¼š
- en: '![](../Images/39cfa1633b7fbf37959ff0cbc29f8383.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/39cfa1633b7fbf37959ff0cbc29f8383.png)'
- en: 'The Gini Impurity is the weighted mean of both:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºå°¼ä¸çº¯åº¦æ˜¯ä¸¤è€…çš„åŠ æƒå‡å€¼ï¼š
- en: '![](../Images/bc97ce3e7eae3f23478251857c2e9b13.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc97ce3e7eae3f23478251857c2e9b13.png)'
- en: '**Case 2:**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ¡ˆä¾‹ 2ï¼š**'
- en: 'Dataset 1:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›† 1ï¼š
- en: '![](../Images/bee46fd6beab0ec0c42e071c3899739c.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bee46fd6beab0ec0c42e071c3899739c.png)'
- en: 'Dataset 2:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›† 2ï¼š
- en: '![](../Images/220e31774ee921f5ec8ecd3b0865c8a8.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/220e31774ee921f5ec8ecd3b0865c8a8.png)'
- en: 'The Gini Impurity is the weighted mean of both:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºå°¼ä¸çº¯åº¦æ˜¯ä¸¤è€…çš„åŠ æƒå‡å€¼ï¼š
- en: '![](../Images/7b00d2fb5b4c651548228a07a25bdcd0.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b00d2fb5b4c651548228a07a25bdcd0.png)'
- en: That is, the first case has lower Gini Impurity and is the chosen split. In
    this simple example, only one feature remains, and we can build the final decision
    tree.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å³ï¼Œç¬¬ä¸€ä¸ªæ¡ˆä¾‹çš„åŸºå°¼ä¸çº¯åº¦è¾ƒä½ï¼Œæ˜¯é€‰æ‹©çš„æ‹†åˆ†ã€‚åœ¨è¿™ä¸ªç®€å•çš„ç¤ºä¾‹ä¸­ï¼Œåªå‰©ä¸‹ä¸€ä¸ªç‰¹å¾ï¼Œæˆ‘ä»¬å¯ä»¥æ„å»ºæœ€ç»ˆçš„å†³ç­–æ ‘ã€‚
- en: '![](../Images/74ab2bd34dbd66e8504ea62a5a88a2b2.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/74ab2bd34dbd66e8504ea62a5a88a2b2.png)'
- en: Final Decision Tree considering only the features â€˜likes gravityâ€™ and â€˜likes
    dogsâ€™
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: åªè€ƒè™‘ç‰¹å¾â€˜å–œæ¬¢é‡åŠ›â€™å’Œâ€˜å–œæ¬¢ç‹—â€™çš„æœ€ç»ˆå†³ç­–æ ‘
- en: 'Second Example: Add a numerical Variable'
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªç¤ºä¾‹ï¼šæ·»åŠ ä¸€ä¸ªæ•°å€¼å˜é‡
- en: Until now, we considered only a subset of our dataÂ set - the categorical variables.
    Now we will add the numerical variable â€˜ageâ€™. The criterion for splitting is the
    same. We already know the Gini Impurities for â€˜likes gravityâ€™ and â€˜likes dogsâ€™.
    The calculation for the Gini Impurity of a numerical variable is similar, however
    the decision takes more calculations. The following steps need to be done
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç°åœ¨ä¸ºæ­¢ï¼Œæˆ‘ä»¬åªè€ƒè™‘äº†æ•°æ®é›†çš„ä¸€ä¸ªå­é›†â€”â€”åˆ†ç±»å˜é‡ã€‚ç°åœ¨æˆ‘ä»¬å°†æ·»åŠ æ•°å€¼å˜é‡â€˜å¹´é¾„â€™ã€‚æ‹†åˆ†çš„æ ‡å‡†ç›¸åŒã€‚æˆ‘ä»¬å·²ç»çŸ¥é“â€˜å–œæ¬¢é‡åŠ›â€™å’Œâ€˜å–œæ¬¢ç‹—â€™çš„åŸºå°¼ä¸çº¯åº¦ã€‚æ•°å€¼å˜é‡çš„åŸºå°¼ä¸çº¯åº¦è®¡ç®—ç±»ä¼¼ï¼Œä½†å†³ç­–éœ€è¦æ›´å¤šè®¡ç®—ã€‚éœ€è¦æ‰§è¡Œä»¥ä¸‹æ­¥éª¤
- en: Sort the data frame by the numerical variable (â€˜ageâ€™)
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æŒ‰æ•°å€¼å˜é‡ï¼ˆâ€˜å¹´é¾„â€™ï¼‰å¯¹æ•°æ®æ¡†è¿›è¡Œæ’åº
- en: Calculate the mean of neighbouring values
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®¡ç®—é‚»è¿‘å€¼çš„å‡å€¼
- en: Calculate the Gini Impurity for all splits for each of these means
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®¡ç®—è¿™äº›å‡å€¼çš„æ‰€æœ‰æ‹†åˆ†çš„åŸºå°¼ä¸çº¯åº¦
- en: This is again our data, sorted by age, and the mean of neighbouring values is
    given on the left-hand side.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åˆæ˜¯æˆ‘ä»¬çš„æ•°æ®ï¼ŒæŒ‰å¹´é¾„æ’åºï¼Œå·¦ä¾§ç»™å‡ºäº†é‚»è¿‘å€¼çš„å‡å€¼ã€‚
- en: '![](../Images/d825cb583abd383d6979f1e7996dddaf.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d825cb583abd383d6979f1e7996dddaf.png)'
- en: The data set sorted by age. The left hand side shows the mean of neighbouring
    values for age.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: æŒ‰å¹´é¾„æ’åºçš„æ•°æ®é›†ã€‚å·¦ä¾§æ˜¾ç¤ºäº†å¹´é¾„çš„é‚»è¿‘å€¼çš„å‡å€¼ã€‚
- en: We then have the following possible splits.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¾—åˆ°ä»¥ä¸‹å¯èƒ½çš„æ‹†åˆ†ã€‚
- en: '![](../Images/260539e5a4dd3b1394dc0e5015ae71d8.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/260539e5a4dd3b1394dc0e5015ae71d8.png)'
- en: Possible splits for age and their Gini Imputity.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: å¹´é¾„çš„å¯èƒ½æ‹†åˆ†åŠå…¶åŸºå°¼ä¸çº¯åº¦ã€‚
- en: We can see that the Gini Impurity of all possible â€˜ageâ€™ splits is higher than
    the one for â€˜likes gravityâ€™ and â€˜likes dogsâ€™. The lowest Gini Impurity is, when
    using â€˜likes gravityâ€™, i.e. this is our *root node* and the first split.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæ‰€æœ‰å¯èƒ½çš„â€˜å¹´é¾„â€™æ‹†åˆ†çš„åŸºå°¼ä¸çº¯åº¦éƒ½é«˜äºâ€˜å–œæ¬¢é‡åŠ›â€™å’Œâ€˜å–œæ¬¢ç‹—â€™çš„åŸºå°¼ä¸çº¯åº¦ã€‚å½“ä½¿ç”¨â€˜å–œæ¬¢é‡åŠ›â€™æ—¶ï¼ŒåŸºå°¼ä¸çº¯åº¦æœ€ä½ï¼Œå³è¿™æ˜¯æˆ‘ä»¬çš„*æ ¹èŠ‚ç‚¹*å’Œç¬¬ä¸€æ¬¡æ‹†åˆ†ã€‚
- en: '![](../Images/cd3c81e701f1cdd6bf1fe61bddf6ce7d.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cd3c81e701f1cdd6bf1fe61bddf6ce7d.png)'
- en: The first split of the tree. â€˜likes gravityâ€™ is the root node.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‘çš„ç¬¬ä¸€æ¬¡æ‹†åˆ†ã€‚â€˜å–œæ¬¢é‡åŠ›â€™æ˜¯æ ¹èŠ‚ç‚¹ã€‚
- en: 'The subset Dataset 2 is already pure, that is, this node is a *leaf* and no
    further splitting is necessary. The *branch* on the left-hand side, Dataset 1
    is not pure and can be split further. We do this in the same way as before: We
    calculate the Gini Impurity for each feature: â€˜likes dogsâ€™ and â€˜ageâ€™.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å­é›†æ•°æ®é›†2å·²ç»æ˜¯çº¯å‡€çš„ï¼Œå³è¿™ä¸ªèŠ‚ç‚¹æ˜¯ä¸€ä¸ª*å¶å­èŠ‚ç‚¹*ï¼Œæ— éœ€è¿›ä¸€æ­¥æ‹†åˆ†ã€‚å·¦ä¾§çš„*åˆ†æ”¯*ï¼Œæ•°æ®é›†1ä¸æ˜¯çº¯å‡€çš„ï¼Œå¯ä»¥è¿›ä¸€æ­¥æ‹†åˆ†ã€‚æˆ‘ä»¬åƒä¹‹å‰ä¸€æ ·è®¡ç®—æ¯ä¸ªç‰¹å¾çš„åŸºå°¼ä¸çº¯åº¦ï¼šâ€˜å–œæ¬¢ç‹—â€™å’Œâ€˜å¹´é¾„â€™ã€‚
- en: '![](../Images/70d756f4890d3d097c10e1e251c8af18.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/70d756f4890d3d097c10e1e251c8af18.png)'
- en: Possible splits for Dataset 2.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›†2çš„å¯èƒ½æ‹†åˆ†ã€‚
- en: We see that the lowest Gini Impurity is given by the split â€œlikes dogsâ€. We
    now can build our final tree.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çœ‹åˆ°æœ€ä½çš„åŸºå°¼ä¸çº¯åº¦æ˜¯ç”±â€œå–œæ¬¢ç‹—â€çš„æ‹†åˆ†ç»™å‡ºçš„ã€‚æˆ‘ä»¬ç°åœ¨å¯ä»¥æ„å»ºæˆ‘ä»¬çš„æœ€ç»ˆå†³ç­–æ ‘ã€‚
- en: '![](../Images/36dbbe9e24de192c40ec14269cf0efa3.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36dbbe9e24de192c40ec14269cf0efa3.png)'
- en: Final Decision Tree.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆå†³ç­–æ ‘ã€‚
- en: Using Python
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Python
- en: 'In Python, we can use the scikit-learn method [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/tree.html#classification)
    for building a Decision Tree for classification. Note, that scikit-learn also
    provides [DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor),
    a method for using Decision Trees for Regression. Assume that our data is stored
    in a data frame â€˜dfâ€™, we then can train it using the â€˜fitâ€™ method:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Pythonä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨scikit-learnæ–¹æ³•[DecisionTreeClassifier](https://scikit-learn.org/stable/modules/tree.html#classification)æ¥æ„å»ºåˆ†ç±»å†³ç­–æ ‘ã€‚è¯·æ³¨æ„ï¼Œscikit-learnè¿˜æä¾›äº†[DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor)ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå›å½’çš„å†³ç­–æ ‘æ–¹æ³•ã€‚å‡è®¾æˆ‘ä»¬çš„æ•°æ®å­˜å‚¨åœ¨æ•°æ®æ¡†â€˜dfâ€™ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨â€˜fitâ€™æ–¹æ³•è¿›è¡Œè®­ç»ƒï¼š
- en: '[PRE0]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We can visualize the resulting tree using the â€˜*plot_tree*â€™ method. It is the
    same as we built, only the splitting criteria is named with â€˜<=â€™ instead of â€˜<â€™,
    and the â€˜trueâ€™ and â€˜falseâ€™ paths go to the other direction. That is, there are
    some differences in the appearance.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨â€˜*plot_tree*â€™æ–¹æ³•å¯è§†åŒ–ç”Ÿæˆçš„æ ‘ã€‚è¿™ä¸æˆ‘ä»¬æ„å»ºçš„æ ‘ç›¸åŒï¼Œåªæ˜¯åˆ†å‰²æ ‡å‡†ç”¨â€˜<=â€™ä»£æ›¿äº†â€˜<â€™ï¼Œè€Œâ€˜trueâ€™å’Œâ€˜falseâ€™è·¯å¾„çš„æ–¹å‘ç›¸åã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¤–è§‚ä¸Šå­˜åœ¨ä¸€äº›å·®å¼‚ã€‚
- en: '[PRE1]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/4acd653ec0c964848a0e2b58da27225f.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4acd653ec0c964848a0e2b58da27225f.png)'
- en: Resulting Decision Tree using scikit-learn.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨scikit-learnç”Ÿæˆçš„å†³ç­–æ ‘ã€‚
- en: Advantages and Disadvantages of Decision Trees
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å†³ç­–æ ‘çš„ä¼˜ç¼ºç‚¹
- en: When working with decision trees, it is important to know their advantages and
    disadvantages. Below you can find a list of pros and cons. This list, however,
    is by no means complete.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½¿ç”¨å†³ç­–æ ‘æ—¶ï¼Œäº†è§£å…¶ä¼˜ç¼ºç‚¹å¾ˆé‡è¦ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›ä¼˜ç¼ºç‚¹çš„åˆ—è¡¨ï¼Œä½†è¿™ä¸ªåˆ—è¡¨å¹¶ä¸å®Œå…¨ã€‚
- en: Pros
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¼˜åŠ¿
- en: Decision trees are intuitive, easy to understand and interpret.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å†³ç­–æ ‘ç›´è§‚ã€æ˜“äºç†è§£å’Œè§£é‡Šã€‚
- en: Decision trees are not effected by outliers and missing values.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å†³ç­–æ ‘ä¸å—å¼‚å¸¸å€¼å’Œç¼ºå¤±å€¼çš„å½±å“ã€‚
- en: The data doesnâ€™t need to be scaled.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ•°æ®ä¸éœ€è¦è¿›è¡Œç¼©æ”¾ã€‚
- en: Numerical and categorical data can be combined.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ•°å€¼æ•°æ®å’Œåˆ†ç±»æ•°æ®å¯ä»¥ç»“åˆä½¿ç”¨ã€‚
- en: Decision trees are non-parametric algorithms.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å†³ç­–æ ‘æ˜¯éå‚æ•°ç®—æ³•ã€‚
- en: Cons
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¼ºç‚¹
- en: Overfitting is a common problem. Pruning may help to overcome this.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿‡æ‹Ÿåˆæ˜¯ä¸€ä¸ªå¸¸è§é—®é¢˜ã€‚å‰ªæå¯èƒ½æœ‰åŠ©äºå…‹æœè¿™ä¸ªé—®é¢˜ã€‚
- en: Although decision trees can be used for regression problems, they cannot really
    predict continuous variables as the predictions must be separated in categories.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è™½ç„¶å†³ç­–æ ‘å¯ä»¥ç”¨äºå›å½’é—®é¢˜ï¼Œä½†å®ƒä»¬ä¸èƒ½çœŸæ­£é¢„æµ‹è¿ç»­å˜é‡ï¼Œå› ä¸ºé¢„æµ‹å¿…é¡»ä»¥ç±»åˆ«å½¢å¼åˆ†éš”ã€‚
- en: Training a decision tree is relatively expensive.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®­ç»ƒå†³ç­–æ ‘ç›¸å¯¹æ˜‚è´µã€‚
- en: Conclusion
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: In this article, we discussed a simple but detailed example of how to construct
    a decision tree for a classification problem and how it can be used to make predictions.
    A crucial step in creating a decision tree is to find the best split of the data
    into two subsets. A common way to do this is the Gini Impurity. This is also used
    in the scikit-learn library from Python, which is often used in practice to build
    a Decision Tree. Itâ€™s important to keep in mind the limitations of decision trees,
    of which the most prominent one is the tendency to overfit.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†ä¸€ä¸ªç®€å•ä½†è¯¦ç»†çš„ç¤ºä¾‹ï¼Œè¯´æ˜äº†å¦‚ä½•ä¸ºåˆ†ç±»é—®é¢˜æ„å»ºå†³ç­–æ ‘ï¼Œä»¥åŠå¦‚ä½•åˆ©ç”¨å®ƒè¿›è¡Œé¢„æµ‹ã€‚åˆ›å»ºå†³ç­–æ ‘çš„å…³é”®æ­¥éª¤æ˜¯æ‰¾åˆ°å°†æ•°æ®åˆ†æˆä¸¤ä¸ªå­é›†çš„æœ€ä½³åˆ†å‰²æ–¹å¼ã€‚å¸¸ç”¨çš„æ–¹æ³•æ˜¯åŸºå°¼ä¸çº¯åº¦ã€‚è¿™ä¹Ÿè¢«Pythonä¸­çš„scikit-learnåº“æ‰€ä½¿ç”¨ï¼Œè¯¥åº“åœ¨å®é™…ä¸­å¸¸ç”¨äºæ„å»ºå†³ç­–æ ‘ã€‚é‡è¦çš„æ˜¯è¦è®°ä½å†³ç­–æ ‘çš„å±€é™æ€§ï¼Œå…¶ä¸­æœ€çªå‡ºçš„å°±æ˜¯è¿‡æ‹Ÿåˆçš„å€¾å‘ã€‚
- en: References
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: Chris Nicholson, Decision Trees (2020), pathmindâ€Šâ€”â€ŠA.I. Wiki, A Beginnerâ€™s Guide
    to Important Topics in AI, Machine Learning, and Deep https://wiki.pathmind.com/decision-tree.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…‹é‡Œæ–¯Â·å°¼ç§‘å°”æ£®ï¼Œã€Šå†³ç­–æ ‘ã€‹ï¼ˆ2020ï¼‰ï¼Œpathmindâ€Šâ€”â€ŠA.I. Wikiï¼Œã€ŠAIã€æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ä¸­çš„é‡è¦ä¸»é¢˜åˆå­¦è€…æŒ‡å—ã€‹https://wiki.pathmind.com/decision-treeã€‚
- en: Abhishek Sharma, [4 Simple Ways to Split a Decision Tree in Machine LearningOverview
    over splitting methods](https://www.analyticsvidhya.com/blog/2020/06/4-ways-split-decision-tree/#:~:text=Steps%20to%20split%20a%20decision%20tree%20using%20Information%20Gain%3A,entropy%20or%20highest%20information%20gain)Â (2020),
    analyticsvidhya
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é˜¿æ¯”è°¢å…‹Â·å¤å°”é©¬ï¼Œ[4ç§ç®€å•çš„æ–¹å¼æ¥æ‹†åˆ†å†³ç­–æ ‘](https://www.analyticsvidhya.com/blog/2020/06/4-ways-split-decision-tree/#:~:text=Steps%20to%20split%20a%20decision%20tree%20using%20Information%20Gain%3A,entropy%20or%20highest%20information%20gain)Â ï¼ˆ2020ï¼‰ï¼Œanalyticsvidhya
- en: All images unless otherwise noted are by the author.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤éå¦æœ‰è¯´æ˜ï¼Œæ‰€æœ‰å›¾ç‰‡å‡ä¸ºä½œè€…æ‰€ç”¨ã€‚
- en: 'Find more Data Science and Machine Learning posts here:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œæ‰¾åˆ°æ›´å¤šæ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ çš„æ–‡ç« ï¼š
- en: '[## More'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[## æ›´å¤š'
- en: Data Science and Machine Learning Blog
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ åšå®¢
- en: datamapu.com](https://datamapu.com/?source=post_page-----d0bc17fcf1c2--------------------------------)
    [](https://medium.com/@pumaline/subscribe?source=post_page-----d0bc17fcf1c2--------------------------------)
    [## Get an email whenever Pumaline publishes.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: datamapu.com](https://datamapu.com/?source=post_page-----d0bc17fcf1c2--------------------------------)
    [](https://medium.com/@pumaline/subscribe?source=post_page-----d0bc17fcf1c2--------------------------------)
    [## è®¢é˜…Pumalineå‘å¸ƒçš„å†…å®¹æ—¶ä¼šæ”¶åˆ°ç”µå­é‚®ä»¶ã€‚
- en: Get an email whenever Pumaline publishes. By signing up, you will create a Medium
    account if you don't already haveâ€¦
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®¢é˜…Pumalineå‘å¸ƒçš„å†…å®¹æ—¶ä¼šæ”¶åˆ°ç”µå­é‚®ä»¶ã€‚é€šè¿‡æ³¨å†Œï¼Œå¦‚æœä½ è¿˜æ²¡æœ‰Mediumè´¦å·ï¼Œå°†ä¼šåˆ›å»ºä¸€ä¸ªâ€¦
- en: medium.com](https://medium.com/@pumaline/subscribe?source=post_page-----d0bc17fcf1c2--------------------------------)
    [](https://www.buymeacoffee.com/pumaline?source=post_page-----d0bc17fcf1c2--------------------------------)
    [## Pumaline
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/@pumaline/subscribe?source=post_page-----d0bc17fcf1c2--------------------------------)
    [](https://www.buymeacoffee.com/pumaline?source=post_page-----d0bc17fcf1c2--------------------------------)
    [## Pumaline'
- en: Hey, I like to learn and share knowledge about Data Science and Machine Learning.
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å—¨ï¼Œæˆ‘å–œæ¬¢å­¦ä¹ å’Œåˆ†äº«å…³äºæ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ çš„çŸ¥è¯†ã€‚
- en: www.buymeacoffee.com](https://www.buymeacoffee.com/pumaline?source=post_page-----d0bc17fcf1c2--------------------------------)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[www.buymeacoffee.com](https://www.buymeacoffee.com/pumaline?source=post_page-----d0bc17fcf1c2--------------------------------)'
