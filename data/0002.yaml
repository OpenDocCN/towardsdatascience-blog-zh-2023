- en: Decision Trees for Classification — Complete Example
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类决策树——完整示例
- en: 原文：[https://towardsdatascience.com/decision-trees-for-classification-complete-example-d0bc17fcf1c2?source=collection_archive---------1-----------------------#2023-01-01](https://towardsdatascience.com/decision-trees-for-classification-complete-example-d0bc17fcf1c2?source=collection_archive---------1-----------------------#2023-01-01)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/decision-trees-for-classification-complete-example-d0bc17fcf1c2?source=collection_archive---------1-----------------------#2023-01-01](https://towardsdatascience.com/decision-trees-for-classification-complete-example-d0bc17fcf1c2?source=collection_archive---------1-----------------------#2023-01-01)
- en: A detailed example how to construct a Decision Tree for classification
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于如何构建分类决策树的详细示例
- en: '[](https://medium.com/@pumaline?source=post_page-----d0bc17fcf1c2--------------------------------)[![Datamapu](../Images/63b0c7f9a3d160c5bb039bbebd791f7e.png)](https://medium.com/@pumaline?source=post_page-----d0bc17fcf1c2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d0bc17fcf1c2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d0bc17fcf1c2--------------------------------)
    [Datamapu](https://medium.com/@pumaline?source=post_page-----d0bc17fcf1c2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@pumaline?source=post_page-----d0bc17fcf1c2--------------------------------)[![Datamapu](../Images/63b0c7f9a3d160c5bb039bbebd791f7e.png)](https://medium.com/@pumaline?source=post_page-----d0bc17fcf1c2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d0bc17fcf1c2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d0bc17fcf1c2--------------------------------)
    [Datamapu](https://medium.com/@pumaline?source=post_page-----d0bc17fcf1c2--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffcd72d75ae6e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-for-classification-complete-example-d0bc17fcf1c2&user=Datamapu&userId=fcd72d75ae6e&source=post_page-fcd72d75ae6e----d0bc17fcf1c2---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d0bc17fcf1c2--------------------------------)
    ·8 min read·Jan 1, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd0bc17fcf1c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-for-classification-complete-example-d0bc17fcf1c2&user=Datamapu&userId=fcd72d75ae6e&source=-----d0bc17fcf1c2---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffcd72d75ae6e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-for-classification-complete-example-d0bc17fcf1c2&user=Datamapu&userId=fcd72d75ae6e&source=post_page-fcd72d75ae6e----d0bc17fcf1c2---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d0bc17fcf1c2--------------------------------)
    ·8分钟阅读·2023年1月1日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd0bc17fcf1c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-for-classification-complete-example-d0bc17fcf1c2&user=Datamapu&userId=fcd72d75ae6e&source=-----d0bc17fcf1c2---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd0bc17fcf1c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-for-classification-complete-example-d0bc17fcf1c2&source=-----d0bc17fcf1c2---------------------bookmark_footer-----------)![](../Images/a45b59b3ef0c4737f791d8f710bbf118.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd0bc17fcf1c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-for-classification-complete-example-d0bc17fcf1c2&source=-----d0bc17fcf1c2---------------------bookmark_footer-----------)![](../Images/a45b59b3ef0c4737f791d8f710bbf118.png)'
- en: Photo by Fabrice Villard on Unsplash
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由Fabrice Villard提供，来自Unsplash
- en: This article explains how we can use decision trees for classification problems.
    After explaining important terms, we will develop a decision tree for a simple
    example dataset.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本文解释了我们如何使用决策树来解决分类问题。在解释重要术语后，我们将为一个简单的示例数据集构建一个决策树。
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: '[A decision tree is a decision support tool that uses a tree-like model of
    decisions and their possible consequences, including chance event outcomes, resource
    costs, and utility. It is one way to display an algorithm that only contains conditional
    control statements.](https://en.wikipedia.org/wiki/Decision_tree)'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[决策树是一种决策支持工具，它使用树状模型展示决策及其可能的后果，包括随机事件结果、资源成本和效用。这是一种仅包含条件控制语句的算法展示方式。](https://en.wikipedia.org/wiki/Decision_tree)'
- en: 'Traditionally decision trees are drawn manually, but they can be learned using
    Machine Learning. They can be used for both regression and classification problems.
    In this article we will focus on classification problems. Let’s consider the following
    example data:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，决策树是手动绘制的，但可以通过机器学习进行学习。它们可用于回归和分类问题。在这篇文章中，我们将重点关注分类问题。让我们考虑以下示例数据：
- en: '![](../Images/0ff178c454666a8b72804aec2f7de623.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ff178c454666a8b72804aec2f7de623.png)'
- en: Example data (constructed by the author)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 示例数据（由作者构建）
- en: Using this simplified example we will predict whether a person is going to be
    an astronaut, depending on their age, whether they like dogs, and whether they
    like gravity. Before discussing how to construct a decision tree, let’s have a
    look at the resulting decision tree for our example data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个简化的例子，我们将预测一个人是否会成为宇航员，取决于他们的年龄、是否喜欢狗以及是否喜欢重力。在讨论如何构建决策树之前，让我们看一下我们示例数据的最终决策树。
- en: '![](../Images/36dbbe9e24de192c40ec14269cf0efa3.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36dbbe9e24de192c40ec14269cf0efa3.png)'
- en: Final decision tree for example data
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 示例数据的最终决策树
- en: We can follow the paths to come to a decision. For example, we can see that
    a person who doesn’t like gravity is not going to be an astronaut, independent
    of the other features. On the other side, we can also see, that a person who likes
    gravity and likes dogs is going to be an astronaut independent of the age.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以跟踪路径来做出决定。例如，我们可以看到，不喜欢重力的人不会成为宇航员，与其他特征无关。另一方面，我们也可以看到，喜欢重力和喜欢狗的人将会成为宇航员，与年龄无关。
- en: Before going into detail how this tree is constructed, let’s define some important
    terms.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在详细讨论如何构建这棵树之前，让我们定义一些重要的术语。
- en: Terms
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 术语
- en: Root Node
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 根节点
- en: The top-level node. The first decision that is taken. In our example the root
    node is ‘likes gravity’.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 顶级节点。做出的第一个决策。在我们的例子中，根节点是“喜欢重力”。
- en: Branches
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分支
- en: Branches represent sub-trees. Our example has two branches. One branch is, e.g.
    the sub-tree from ‘likes dogs’ and the second one from ‘age < 40.5’ on.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 分支代表子树。我们的例子有两个分支。例如，一个分支是从“喜欢狗”开始的子树，另一个是从“年龄 < 40.5”开始的子树。
- en: Node
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 节点
- en: A node represents a split into further (child) nodes. In our example the nodes
    are ‘likes gravity’, ‘likes dogs’ and ‘age < 40.5’.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一个节点代表进一步（子）节点的切分。在我们的例子中，节点有“喜欢重力”、“喜欢狗”和“年龄 < 40.5”。
- en: Leaf
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 叶子节点
- en: Leafs are at the end of the branches, i.e. they don’t split any more. They represent
    possible outcomes for each action. In our example the leafs are represented by
    ‘yes’ and ‘no’.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 叶子节点位于分支的末端，即不再进行切分。它们代表每个动作的可能结果。在我们的例子中，叶子节点由“是”和“否”表示。
- en: Parent Node
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 父节点
- en: A node which precedes a (child) node is called a parent node. In our example
    ‘likes gravity’ is a parent node of ‘likes dogs’ and ‘likes dogs’ is a parent
    node of ‘age < 40.5’.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一个在（子）节点之前的节点称为父节点。在我们的例子中，“喜欢重力”是“喜欢狗”的父节点，而“喜欢狗”是“年龄 < 40.5”的父节点。
- en: Child Node
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 子节点
- en: A node under another node is a child node. In our example ‘likes dogs’ is a
    child node of ‘likes gravity’ and ‘age < 40.5’ is a child node of ‘likes dogs’.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一个节点在另一个节点下方称为子节点。在我们的例子中，“喜欢狗”是“喜欢重力”的子节点，而“年龄 < 40.5”是“喜欢狗”的子节点。
- en: Splitting
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 切分
- en: The process of dividing a node into two (child) nodes.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 将一个节点划分为两个（子）节点的过程。
- en: Pruning
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 剪枝
- en: Removing the (child) nodes of a parent node is called pruning. A tree is grown
    through splitting and shrunk through pruning. In our example, if we would remove
    the node ‘age < 40.5’ we would prune the tree.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 移除父节点的（子）节点称为剪枝。树通过切分生长，通过剪枝缩小。在我们的例子中，如果我们移除节点“年龄 < 40.5”，我们将对树进行剪枝。
- en: '![](../Images/1556654976019b99810dc7f15c6695ca.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1556654976019b99810dc7f15c6695ca.png)'
- en: Decision tree illustration
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树插图
- en: We can also observe, that a decision tree allows us to mix data types. We can
    use numerical data (‘age’) and categorical data (‘likes dogs’, ‘likes gravity’)
    in the same tree.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以观察到，决策树允许我们混合数据类型。我们可以在同一棵树中使用数值数据（“年龄”）和分类数据（“喜欢狗”、“喜欢重力”）。
- en: Create a Decision Tree
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建决策树
- en: 'The most important step in creating a decision tree, is the *splitting* of
    the data. We need to find a way to split the data set (*D*) into two data sets
    (*D_1*) and (*D_2*). There are different criteria that can be used in order to
    find the next split, for an overview see e.g. [here](https://www.analyticsvidhya.com/blog/2020/06/4-ways-split-decision-tree/#:~:text=Steps%20to%20split%20a%20decision%20tree%20using%20Information%20Gain%3A,entropy%20or%20highest%20information%20gain).
    We will concentrate on one of them: the [*Gini Impurity*](https://www.learndatasci.com/glossary/gini-impurity/#:~:text=a%20simple%20dataset-,What%20is%20Gini%20Impurity%3F,nodes%20to%20form%20the%20tree.),
    which is a criterion for categorical target variables and also the criterion used
    by the Python library [scikit-learn](https://scikit-learn.org/stable/modules/tree.html#classification).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 创建决策树中最重要的步骤是对数据的 *分裂*。我们需要找到一种方法将数据集 (*D*) 分裂为两个数据集 (*D_1*) 和 (*D_2*)。可以使用不同的标准来寻找下一个分裂，概览见例如
    [这里](https://www.analyticsvidhya.com/blog/2020/06/4-ways-split-decision-tree/#:~:text=Steps%20to%20split%20a%20decision%20tree%20using%20Information%20Gain%3A,entropy%20or%20highest%20information%20gain)。我们将集中于其中一个标准：[基尼不纯度](https://www.learndatasci.com/glossary/gini-impurity/#:~:text=a%20simple%20dataset-,What%20is%20Gini%20Impurity%3F,nodes%20to%20form%20the%20tree.)，它是一个用于分类目标变量的标准，也是
    Python 库 [scikit-learn](https://scikit-learn.org/stable/modules/tree.html#classification)
    使用的标准。
- en: Gini Impurity
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基尼不纯度
- en: 'The Gini Impurity for a data set *D* is calculated as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 *D* 的基尼不纯度计算如下：
- en: '![](../Images/dcd4ca062f20d515c6c01822250af8fa.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dcd4ca062f20d515c6c01822250af8fa.png)'
- en: with n = n_1 + n_2 the size of the data set (D) and
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 n = n_1 + n_2 表示数据集 (D) 的大小，并且
- en: '![](../Images/55268b5346592788b91d1ebd51acdd75.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55268b5346592788b91d1ebd51acdd75.png)'
- en: with *D_1* and *D_2* subsets of *D*, 𝑝_𝑗 the probability of samples belonging
    to class 𝑗 at a given node, and 𝑐 the number of classes. The lower the Gini Impurity,
    the higher is the homogeneity of the node. The Gini Impurity of a pure node is
    zero. To split a decision tree using Gini Impurity, the following steps need to
    be performed.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*D_1* 和 *D_2* 是 *D* 的子集，𝑝_𝑗 是在给定节点上样本属于类别 𝑗 的概率，𝑐 是类别的数量。基尼不纯度越低，节点的同质性越高。纯节点的基尼不纯度为零。为了使用基尼不纯度来分裂决策树，需要执行以下步骤。'
- en: For each possible split, calculate the Gini Impurity of each child node
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个可能的分裂，计算每个子节点的基尼不纯度
- en: Calculate the Gini Impurity of each split as the weighted average Gini Impurity
    of child nodes
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个分裂的基尼不纯度，作为子节点基尼不纯度的加权平均值
- en: Select the split with the lowest value of Gini Impurity
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择基尼不纯度值最低的分裂
- en: Repeat steps 1–3 until no further split is possible.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 重复步骤 1–3 直到不能再分裂为止。
- en: To understand this better, let’s have a look at an example.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这一点，让我们来看一个例子。
- en: 'First Example: Decision Tree with two binary features'
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一个示例：具有两个二元特征的决策树
- en: 'Before creating the decision tree for our entire dataset, we will first consider
    a subset, that only considers two features: ‘likes gravity’ and ‘likes dogs’.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在为整个数据集创建决策树之前，我们将首先考虑一个子集，该子集仅考虑两个特征：“喜欢重力”和“喜欢狗”。
- en: 'The first thing we have to decide is, which feature is going to be the *root
    node*. We do that by predicting the target with only one of the features and then
    use the feature, that has the lowest Gini Impurity as the root node. That is,
    in our case we build two shallow trees, with just the root node and two leafs.
    In the first case we use ‘likes gravity’ as a root node and in the second case
    ‘likes dogs’. We then calculate the Gini Impurity for both. The trees look like
    this:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要决定哪个特征将作为 *根节点*。我们通过仅用一个特征来预测目标，然后选择基尼不纯度最低的特征作为根节点。也就是说，在我们的案例中，我们构建了两个浅层树，只有根节点和两个叶子。在第一个案例中，我们使用“喜欢重力”作为根节点，在第二个案例中使用“喜欢狗”。然后我们计算两个树的基尼不纯度。这些树的样子如下：
- en: '![](../Images/a7cbf81c91434e4974dd0fa52e21864a.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a7cbf81c91434e4974dd0fa52e21864a.png)'
- en: Image by the author
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'The Gini Impurity for these trees are calculated as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这些树的基尼不纯度计算如下：
- en: '**Case 1:**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**案例 1：**'
- en: 'Dataset 1:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 1：
- en: '![](../Images/079a8cfb48bd78791b73edbf569b2bd3.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/079a8cfb48bd78791b73edbf569b2bd3.png)'
- en: 'Dataset 2:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 2：
- en: '![](../Images/39cfa1633b7fbf37959ff0cbc29f8383.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/39cfa1633b7fbf37959ff0cbc29f8383.png)'
- en: 'The Gini Impurity is the weighted mean of both:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 基尼不纯度是两者的加权均值：
- en: '![](../Images/bc97ce3e7eae3f23478251857c2e9b13.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc97ce3e7eae3f23478251857c2e9b13.png)'
- en: '**Case 2:**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**案例 2：**'
- en: 'Dataset 1:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 1：
- en: '![](../Images/bee46fd6beab0ec0c42e071c3899739c.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bee46fd6beab0ec0c42e071c3899739c.png)'
- en: 'Dataset 2:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 2：
- en: '![](../Images/220e31774ee921f5ec8ecd3b0865c8a8.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/220e31774ee921f5ec8ecd3b0865c8a8.png)'
- en: 'The Gini Impurity is the weighted mean of both:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 基尼不纯度是两者的加权均值：
- en: '![](../Images/7b00d2fb5b4c651548228a07a25bdcd0.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b00d2fb5b4c651548228a07a25bdcd0.png)'
- en: That is, the first case has lower Gini Impurity and is the chosen split. In
    this simple example, only one feature remains, and we can build the final decision
    tree.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 即，第一个案例的基尼不纯度较低，是选择的拆分。在这个简单的示例中，只剩下一个特征，我们可以构建最终的决策树。
- en: '![](../Images/74ab2bd34dbd66e8504ea62a5a88a2b2.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/74ab2bd34dbd66e8504ea62a5a88a2b2.png)'
- en: Final Decision Tree considering only the features ‘likes gravity’ and ‘likes
    dogs’
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 只考虑特征‘喜欢重力’和‘喜欢狗’的最终决策树
- en: 'Second Example: Add a numerical Variable'
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二个示例：添加一个数值变量
- en: Until now, we considered only a subset of our data set - the categorical variables.
    Now we will add the numerical variable ‘age’. The criterion for splitting is the
    same. We already know the Gini Impurities for ‘likes gravity’ and ‘likes dogs’.
    The calculation for the Gini Impurity of a numerical variable is similar, however
    the decision takes more calculations. The following steps need to be done
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，我们只考虑了数据集的一个子集——分类变量。现在我们将添加数值变量‘年龄’。拆分的标准相同。我们已经知道‘喜欢重力’和‘喜欢狗’的基尼不纯度。数值变量的基尼不纯度计算类似，但决策需要更多计算。需要执行以下步骤
- en: Sort the data frame by the numerical variable (‘age’)
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按数值变量（‘年龄’）对数据框进行排序
- en: Calculate the mean of neighbouring values
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算邻近值的均值
- en: Calculate the Gini Impurity for all splits for each of these means
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算这些均值的所有拆分的基尼不纯度
- en: This is again our data, sorted by age, and the mean of neighbouring values is
    given on the left-hand side.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这又是我们的数据，按年龄排序，左侧给出了邻近值的均值。
- en: '![](../Images/d825cb583abd383d6979f1e7996dddaf.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d825cb583abd383d6979f1e7996dddaf.png)'
- en: The data set sorted by age. The left hand side shows the mean of neighbouring
    values for age.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 按年龄排序的数据集。左侧显示了年龄的邻近值的均值。
- en: We then have the following possible splits.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下可能的拆分。
- en: '![](../Images/260539e5a4dd3b1394dc0e5015ae71d8.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/260539e5a4dd3b1394dc0e5015ae71d8.png)'
- en: Possible splits for age and their Gini Imputity.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 年龄的可能拆分及其基尼不纯度。
- en: We can see that the Gini Impurity of all possible ‘age’ splits is higher than
    the one for ‘likes gravity’ and ‘likes dogs’. The lowest Gini Impurity is, when
    using ‘likes gravity’, i.e. this is our *root node* and the first split.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，所有可能的‘年龄’拆分的基尼不纯度都高于‘喜欢重力’和‘喜欢狗’的基尼不纯度。当使用‘喜欢重力’时，基尼不纯度最低，即这是我们的*根节点*和第一次拆分。
- en: '![](../Images/cd3c81e701f1cdd6bf1fe61bddf6ce7d.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cd3c81e701f1cdd6bf1fe61bddf6ce7d.png)'
- en: The first split of the tree. ‘likes gravity’ is the root node.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 树的第一次拆分。‘喜欢重力’是根节点。
- en: 'The subset Dataset 2 is already pure, that is, this node is a *leaf* and no
    further splitting is necessary. The *branch* on the left-hand side, Dataset 1
    is not pure and can be split further. We do this in the same way as before: We
    calculate the Gini Impurity for each feature: ‘likes dogs’ and ‘age’.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 子集数据集2已经是纯净的，即这个节点是一个*叶子节点*，无需进一步拆分。左侧的*分支*，数据集1不是纯净的，可以进一步拆分。我们像之前一样计算每个特征的基尼不纯度：‘喜欢狗’和‘年龄’。
- en: '![](../Images/70d756f4890d3d097c10e1e251c8af18.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/70d756f4890d3d097c10e1e251c8af18.png)'
- en: Possible splits for Dataset 2.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集2的可能拆分。
- en: We see that the lowest Gini Impurity is given by the split “likes dogs”. We
    now can build our final tree.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到最低的基尼不纯度是由“喜欢狗”的拆分给出的。我们现在可以构建我们的最终决策树。
- en: '![](../Images/36dbbe9e24de192c40ec14269cf0efa3.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36dbbe9e24de192c40ec14269cf0efa3.png)'
- en: Final Decision Tree.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最终决策树。
- en: Using Python
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Python
- en: 'In Python, we can use the scikit-learn method [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/tree.html#classification)
    for building a Decision Tree for classification. Note, that scikit-learn also
    provides [DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor),
    a method for using Decision Trees for Regression. Assume that our data is stored
    in a data frame ‘df’, we then can train it using the ‘fit’ method:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，我们可以使用scikit-learn方法[DecisionTreeClassifier](https://scikit-learn.org/stable/modules/tree.html#classification)来构建分类决策树。请注意，scikit-learn还提供了[DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor)，这是一个用于回归的决策树方法。假设我们的数据存储在数据框‘df’中，我们可以使用‘fit’方法进行训练：
- en: '[PRE0]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We can visualize the resulting tree using the ‘*plot_tree*’ method. It is the
    same as we built, only the splitting criteria is named with ‘<=’ instead of ‘<’,
    and the ‘true’ and ‘false’ paths go to the other direction. That is, there are
    some differences in the appearance.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用‘*plot_tree*’方法可视化生成的树。这与我们构建的树相同，只是分割标准用‘<=’代替了‘<’，而‘true’和‘false’路径的方向相反。也就是说，外观上存在一些差异。
- en: '[PRE1]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/4acd653ec0c964848a0e2b58da27225f.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4acd653ec0c964848a0e2b58da27225f.png)'
- en: Resulting Decision Tree using scikit-learn.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 使用scikit-learn生成的决策树。
- en: Advantages and Disadvantages of Decision Trees
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树的优缺点
- en: When working with decision trees, it is important to know their advantages and
    disadvantages. Below you can find a list of pros and cons. This list, however,
    is by no means complete.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用决策树时，了解其优缺点很重要。以下是一些优缺点的列表，但这个列表并不完全。
- en: Pros
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优势
- en: Decision trees are intuitive, easy to understand and interpret.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树直观、易于理解和解释。
- en: Decision trees are not effected by outliers and missing values.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树不受异常值和缺失值的影响。
- en: The data doesn’t need to be scaled.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据不需要进行缩放。
- en: Numerical and categorical data can be combined.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数值数据和分类数据可以结合使用。
- en: Decision trees are non-parametric algorithms.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树是非参数算法。
- en: Cons
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺点
- en: Overfitting is a common problem. Pruning may help to overcome this.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过拟合是一个常见问题。剪枝可能有助于克服这个问题。
- en: Although decision trees can be used for regression problems, they cannot really
    predict continuous variables as the predictions must be separated in categories.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然决策树可以用于回归问题，但它们不能真正预测连续变量，因为预测必须以类别形式分隔。
- en: Training a decision tree is relatively expensive.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练决策树相对昂贵。
- en: Conclusion
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we discussed a simple but detailed example of how to construct
    a decision tree for a classification problem and how it can be used to make predictions.
    A crucial step in creating a decision tree is to find the best split of the data
    into two subsets. A common way to do this is the Gini Impurity. This is also used
    in the scikit-learn library from Python, which is often used in practice to build
    a Decision Tree. It’s important to keep in mind the limitations of decision trees,
    of which the most prominent one is the tendency to overfit.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们讨论了一个简单但详细的示例，说明了如何为分类问题构建决策树，以及如何利用它进行预测。创建决策树的关键步骤是找到将数据分成两个子集的最佳分割方式。常用的方法是基尼不纯度。这也被Python中的scikit-learn库所使用，该库在实际中常用于构建决策树。重要的是要记住决策树的局限性，其中最突出的就是过拟合的倾向。
- en: References
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Chris Nicholson, Decision Trees (2020), pathmind — A.I. Wiki, A Beginner’s Guide
    to Important Topics in AI, Machine Learning, and Deep https://wiki.pathmind.com/decision-tree.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 克里斯·尼科尔森，《决策树》（2020），pathmind — A.I. Wiki，《AI、机器学习和深度学习中的重要主题初学者指南》https://wiki.pathmind.com/decision-tree。
- en: Abhishek Sharma, [4 Simple Ways to Split a Decision Tree in Machine LearningOverview
    over splitting methods](https://www.analyticsvidhya.com/blog/2020/06/4-ways-split-decision-tree/#:~:text=Steps%20to%20split%20a%20decision%20tree%20using%20Information%20Gain%3A,entropy%20or%20highest%20information%20gain) (2020),
    analyticsvidhya
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阿比谢克·夏尔马，[4种简单的方式来拆分决策树](https://www.analyticsvidhya.com/blog/2020/06/4-ways-split-decision-tree/#:~:text=Steps%20to%20split%20a%20decision%20tree%20using%20Information%20Gain%3A,entropy%20or%20highest%20information%20gain) （2020），analyticsvidhya
- en: All images unless otherwise noted are by the author.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图片均为作者所用。
- en: 'Find more Data Science and Machine Learning posts here:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里找到更多数据科学和机器学习的文章：
- en: '[## More'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 更多'
- en: Data Science and Machine Learning Blog
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据科学和机器学习博客
- en: datamapu.com](https://datamapu.com/?source=post_page-----d0bc17fcf1c2--------------------------------)
    [](https://medium.com/@pumaline/subscribe?source=post_page-----d0bc17fcf1c2--------------------------------)
    [## Get an email whenever Pumaline publishes.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: datamapu.com](https://datamapu.com/?source=post_page-----d0bc17fcf1c2--------------------------------)
    [](https://medium.com/@pumaline/subscribe?source=post_page-----d0bc17fcf1c2--------------------------------)
    [## 订阅Pumaline发布的内容时会收到电子邮件。
- en: Get an email whenever Pumaline publishes. By signing up, you will create a Medium
    account if you don't already have…
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 订阅Pumaline发布的内容时会收到电子邮件。通过注册，如果你还没有Medium账号，将会创建一个…
- en: medium.com](https://medium.com/@pumaline/subscribe?source=post_page-----d0bc17fcf1c2--------------------------------)
    [](https://www.buymeacoffee.com/pumaline?source=post_page-----d0bc17fcf1c2--------------------------------)
    [## Pumaline
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/@pumaline/subscribe?source=post_page-----d0bc17fcf1c2--------------------------------)
    [](https://www.buymeacoffee.com/pumaline?source=post_page-----d0bc17fcf1c2--------------------------------)
    [## Pumaline'
- en: Hey, I like to learn and share knowledge about Data Science and Machine Learning.
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 嗨，我喜欢学习和分享关于数据科学和机器学习的知识。
- en: www.buymeacoffee.com](https://www.buymeacoffee.com/pumaline?source=post_page-----d0bc17fcf1c2--------------------------------)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[www.buymeacoffee.com](https://www.buymeacoffee.com/pumaline?source=post_page-----d0bc17fcf1c2--------------------------------)'
