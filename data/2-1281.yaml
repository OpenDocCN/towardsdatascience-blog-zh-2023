- en: 🤗Hugging Face Transformers Agent
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🤗Hugging Face Transformers Agent
- en: 原文：[https://towardsdatascience.com/hugging-face-transformers-agent-3a01cf3669ac](https://towardsdatascience.com/hugging-face-transformers-agent-3a01cf3669ac)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/hugging-face-transformers-agent-3a01cf3669ac](https://towardsdatascience.com/hugging-face-transformers-agent-3a01cf3669ac)
- en: Comparisons with 🦜🔗LangChain Agent
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与🦜🔗LangChain Agent的比较
- en: '[](https://sophiamyang.medium.com/?source=post_page-----3a01cf3669ac--------------------------------)[![Sophia
    Yang, Ph.D.](../Images/c133f918245ea4857dc46df3a07fc2b1.png)](https://sophiamyang.medium.com/?source=post_page-----3a01cf3669ac--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3a01cf3669ac--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3a01cf3669ac--------------------------------)
    [Sophia Yang, Ph.D.](https://sophiamyang.medium.com/?source=post_page-----3a01cf3669ac--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://sophiamyang.medium.com/?source=post_page-----3a01cf3669ac--------------------------------)[![Sophia
    Yang, Ph.D.](../Images/c133f918245ea4857dc46df3a07fc2b1.png)](https://sophiamyang.medium.com/?source=post_page-----3a01cf3669ac--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3a01cf3669ac--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3a01cf3669ac--------------------------------)
    [Sophia Yang, Ph.D.](https://sophiamyang.medium.com/?source=post_page-----3a01cf3669ac--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3a01cf3669ac--------------------------------)
    ·5 min read·May 14, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----3a01cf3669ac--------------------------------)
    ·阅读时间5分钟·2023年5月14日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Just two days ago, 🤗Hugging Face released Transformers Agent — an agent that
    leverages natural language to choose a tool from a curated collection of tools
    and accomplish various tasks. Does it sound familiar? Yes, it does because it’s
    a lot like 🦜🔗LangChain Tools and Agents. In this blog post, I will cover what
    Transformers Agent is and its comparisons with 🦜🔗LangChain Agent.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 就在两天前，🤗Hugging Face发布了Transformers Agent——一个利用自然语言从经过筛选的工具集合中选择工具并完成各种任务的代理。听起来熟悉吗？是的，因为它很像🦜🔗LangChain
    Tools and Agents。在这篇博客中，我将介绍Transformers Agent是什么，以及它与🦜🔗LangChain Agent的比较。
- en: 'Try out the Code:'
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 尝试代码：
- en: You can try out the code in [this colab](https://colab.research.google.com/drive/1c7MHD-T1forUPGcC_jlwsIptOzpG3hSj)
    (provided by Hugging Face).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[这个colab](https://colab.research.google.com/drive/1c7MHD-T1forUPGcC_jlwsIptOzpG3hSj)中尝试代码（由Hugging
    Face提供）。
- en: '**What are Transformers Agents?**'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**什么是Transformers Agents？**'
- en: 'In short, it provides a natural language API on top of transformers: we define
    a set of curated tools and design an agent to interpret natural language and to
    use these tools.'
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 简而言之，它在transformers之上提供了一个自然语言API：我们定义了一组经过筛选的工具，并设计了一个代理来解释自然语言并使用这些工具。
- en: 'I can imagine engineers at HuggingFace be like: We have so many amazing models
    hosted on HuggingFace. Can we integrate those with LLMs? Can we use LLMs to decide
    which model to use, write code, run code, and generate results? Essentially, nobody
    needs to learn all the complicated task-specific models anymore. Just give it
    a task, LLMs (agents) will do everything for us.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以想象HuggingFace的工程师们会这样说：我们在HuggingFace上托管了这么多令人惊叹的模型。我们能否将这些与LLM整合起来？我们能否利用LLM决定使用哪个模型，编写代码，运行代码并生成结果？本质上，没有人再需要学习所有复杂的任务特定模型了。只需给LLM（代理）一个任务，它们将为我们做所有事情。
- en: 'Here are the steps:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是步骤：
- en: '![](../Images/6a222be0579d543b90b678228080848d.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a222be0579d543b90b678228080848d.png)'
- en: 'Source: [https://huggingface.co/docs/transformers/transformers_agents](https://huggingface.co/docs/transformers/transformers_agents)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '来源: [https://huggingface.co/docs/transformers/transformers_agents](https://huggingface.co/docs/transformers/transformers_agents)'
- en: 'Instruction: the prompt users provide'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Instruction: 用户提供的提示'
- en: 'Prompt: a prompt template with the specific instruction added, where it lists
    multiple tools to use.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Prompt: 一个包含具体指令的提示模板，其中列出了多个可用的工具。'
- en: 'Tools: a curated list of transformers models, e.g., Flan-T5 for question answering,'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tools: 一组经过筛选的transformers模型，例如用于问答的Flan-T5'
- en: 'Agent: an LLM that interprets the question, decides which tools to use, and
    generates code to perform the task with the tools.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Agent: 是一个大语言模型（LLM），它解释问题，决定使用哪些工具，并生成代码来利用这些工具完成任务。'
- en: 'Restricted Python interpreter: execute Python code.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 限制Python解释器：执行Python代码。
- en: How does it work?
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: 'Step 1: Instantiate an agent.'
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一步：实例化一个代理。
- en: Step 1 is to instantiate an agent. An agent is just an LLM, which can be an
    OpenAI model, a StarCoder model, or an OpenAssistant model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是实例化一个代理。一个代理就是一个LLM，可以是OpenAI模型、StarCoder模型或OpenAssistant模型。
- en: The OpenAI model needs the OpenAI API key and the usage is not free. We load
    the StarCoder model and the OpenAssistant model from the HuggingFace Hub, which
    requires HuggingFace Hub API key and it is free to use.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI模型需要OpenAI API密钥，并且使用不是免费的。我们从HuggingFace Hub加载StarCoder模型和OpenAssistant模型，这需要HuggingFace
    Hub API密钥，并且可以免费使用。
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Step 2: Run the agent.'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2步：运行代理。
- en: '`agent.run` is a single execution method and selects the tool for the task
    automatically, e.g., select the image generator tool to create an image.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`agent.run`是一个单一执行方法，会自动选择任务所需的工具，例如，选择图像生成工具来创建图像。'
- en: '![](../Images/067c6d283ea9db4702a5337699127b6e.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/067c6d283ea9db4702a5337699127b6e.png)'
- en: '`agent.chat` keeps the chat history. For example, here it knows we generated
    a picture earlier and it can transform an image.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`agent.chat`保持聊天记录。例如，它知道我们之前生成了一个图片，并且它可以转换图像。'
- en: '![](../Images/4eaf141fd1387e09e7e726179d35505b.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4eaf141fd1387e09e7e726179d35505b.png)'
- en: How is it different from 🦜🔗LangChain Agent?
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它与🦜🔗LangChain Agent有什么不同？
- en: 'Transformers Agent is still experimental. It’s a lot smaller scope and less
    flexible. The main focus of Transformers Agent right now is for using Transformer
    models and executing Python code, whereas LangChain Agent does “almost” everything.
    Let be break it down to compare different components between Transformers and
    LangChain Agents:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers Agent仍处于实验阶段。它的范围较小，灵活性较差。目前Transformers Agent的主要关注点是使用Transformer模型和执行Python代码，而LangChain
    Agent几乎能做“一切”。让我们逐一比较Transformers和LangChain Agents之间的不同组件：
- en: Tools
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工具
- en: '🤗Hugging Face Transfomers Agent has an amazing list of tools, each powered
    by transformer models. These tools offer three significant advantages: 1) Even
    though Transformers Agent can only interact with few tools currently, it has the
    potential to communicate with over 100,000 Hugging Face model. It possesses full
    multimodal capabilities, encompassing text, images, video, audio, and documents.;
    2) Since these models are purpose-built for specific tasks, utilizing them can
    be more straightforward and yield more accurate results compared to relying solely
    on LLMs. For example, instead of designing the prompts for the LLM to perform
    text classification, we can simply deploy BART that’s designed for text classification;
    3) These tools unlocked capabilities that LLMs alone can’t accomplish. Take BLIP,
    for example, which enables us to generate captivating image captions — a task
    beyond the scope of LLMs.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 🤗Hugging Face Transformers Agent拥有一个惊人的工具列表，每个工具都由变换器模型驱动。这些工具提供了三大显著优势：1) 尽管Transformers
    Agent目前只能与少数工具交互，但它有可能与超过100,000个Hugging Face模型进行通信。它具有完整的多模态能力，包括文本、图像、视频、音频和文档。2)
    由于这些模型是为特定任务量身定制的，使用它们可以比单独依赖LLMs更直接、更准确。例如，我们可以直接部署专为文本分类设计的BART，而不是为LLM设计提示。3)
    这些工具解锁了LLMs无法完成的功能。例如，BLIP使我们能够生成引人注目的图像描述——这是LLMs无法实现的任务。
- en: 🦜🔗LangChain tools are all external APIs, such as Google Search, Python REPL.
    In fact, LangChain supports HuggingFace Tools via the `load_huggingface_tool`
    function. LangChain can potentially do a lot of things Transformers Agent can
    do already. On the other hand, Transformers Agents can potentially incorporate
    all the LangChain tools as well.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 🦜🔗LangChain工具都是外部API，例如Google搜索、Python REPL。实际上，LangChain通过`load_huggingface_tool`函数支持HuggingFace工具。LangChain有可能做很多Transformers
    Agent已经能够做的事情。另一方面，Transformers Agents也有可能整合所有LangChain工具。
- en: In both cases, each tool is just a Python file. You can find the files of 🤗Hugging
    Face Transformers Agent tools [here](https://github.com/huggingface/transformers/tree/main/src/transformers/tools)
    and 🦜🔗LangChain tools [here](https://github.com/hwchase17/langchain/tree/master/langchain/utilities).
    As you can see, each Python file contains one class indicating one tool.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这两种情况下，每个工具都是一个Python文件。你可以在[这里](https://github.com/huggingface/transformers/tree/main/src/transformers/tools)找到🤗Hugging
    Face Transformers Agent工具的文件，也可以在[这里](https://github.com/hwchase17/langchain/tree/master/langchain/utilities)找到🦜🔗LangChain工具的文件。正如你所见，每个Python文件包含一个类，表示一个工具。
- en: Agent
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代理
- en: 🤗Hugging Face Transformers Agent uses [this prompt template](https://github.com/huggingface/transformers/blob/main/src/transformers/tools/prompts.py#L19-L93)
    to determine which tool to use based on the tool’s description. It asks the LLM
    to provide an explanations and it provides some few-shots learning examples in
    the prompt.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 🤗Hugging Face Transformers Agent使用[这个提示模板](https://github.com/huggingface/transformers/blob/main/src/transformers/tools/prompts.py#L19-L93)根据工具的描述确定使用哪个工具。它要求LLM提供解释，并在提示中提供一些少量示例。
- en: 🦜🔗LangChain by default uses the ReAct framework to determine which tool to use
    based on the tool’s description. The ReAct framework is described in this [paper](https://arxiv.org/pdf/2210.03629.pdf).
    It does not only act on a decision but also provides *thoughts* and *reasoning,*
    which is similar to the *explanations* Transformers Agent uses. In addition, 🦜🔗LangChain
    has four [agent types](https://python.langchain.com/en/latest/modules/agents/agents/agent_types.html).
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 🦜🔗LangChain默认使用ReAct框架，根据工具的描述来确定使用哪个工具。ReAct框架在这篇[论文](https://arxiv.org/pdf/2210.03629.pdf)中有描述。它不仅进行决策，还提供*思考*和*推理*，这类似于Transformers
    Agent使用的*解释*。此外，🦜🔗LangChain还有四种[代理类型](https://python.langchain.com/en/latest/modules/agents/agents/agent_types.html)。
- en: '**Custom Agent**'
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**自定义代理**'
- en: 'Creating a custom agent is not too difficult in both cases:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 创建自定义代理在这两种情况下都不是很困难：
- en: See the HuggingFace Transformer Agent example towards the end of [this colab](https://colab.research.google.com/drive/1c7MHD-T1forUPGcC_jlwsIptOzpG3hSj).
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看HuggingFace Transformer Agent示例，请参见[这个colab](https://colab.research.google.com/drive/1c7MHD-T1forUPGcC_jlwsIptOzpG3hSj)的末尾。
- en: See the LangChain [example](https://python.langchain.com/en/latest/modules/agents/agents/custom_agent.html)
    here.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看LangChain的[示例](https://python.langchain.com/en/latest/modules/agents/agents/custom_agent.html)。
- en: “Code-execution”
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: “代码执行”
- en: 🤗Hugging Face Transformers Agent includes “code-execution” as one of the steps
    after the LLM selects the tools and generates the code. This restricts the Transformers
    Agent’s goal to execute Python code.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 🤗Hugging Face Transformers Agent在LLM选择工具并生成代码之后将“代码执行”作为步骤之一。这将Transformers
    Agent的目标限制为执行Python代码。
- en: '🦜🔗LangChain includes “code-execution” as one of its tools, which means that
    executing code is not the last step of the whole process. This provides a lot
    more flexibility on what the task goal is: it could be executing Python code,
    or it could also be something else like doing a Google Search and returning search
    results.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 🦜🔗LangChain将“代码执行”作为其工具之一，这意味着执行代码不是整个过程的最后一步。这提供了更多灵活性：任务目标可以是执行Python代码，也可以是其他任务，例如进行Google搜索并返回搜索结果。
- en: Conclusion
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this blog post, we explored the functionality of 🤗Hugging Face Transformers
    Agents and compared it to 🦜🔗LangChain Agents. I look forward to witnessing further
    developments and advancements in Transformers Agent.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我们探讨了🤗Hugging Face Transformers Agents的功能，并将其与🦜🔗LangChain Agents进行了比较。我期待着看到Transformers
    Agent的进一步发展和进步。
- en: . . .
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: . . .
- en: By [Sophia Yang](https://www.linkedin.com/in/sophiamyang/) on May 12, 2023
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 作者：[Sophia Yang](https://www.linkedin.com/in/sophiamyang/)，发表于2023年5月12日
- en: Sophia Yang is a Senior Data Scientist. Connect with me on [LinkedIn](https://www.linkedin.com/in/sophiamyang/),
    [Twitter](https://twitter.com/sophiamyang), and [YouTube](https://www.youtube.com/SophiaYangDS)
    and join the DS/ML [Book Club](https://dsbookclub.github.io/) ❤️
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Sophia Yang是一位高级数据科学家。可以在[LinkedIn](https://www.linkedin.com/in/sophiamyang/)、[Twitter](https://twitter.com/sophiamyang)和[YouTube](https://www.youtube.com/SophiaYangDS)上与我联系，并加入DS/ML
    [书友会](https://dsbookclub.github.io/) ❤️
