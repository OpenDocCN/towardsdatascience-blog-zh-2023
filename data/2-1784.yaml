- en: Retrieval Augmented Generation — Intuitively and Exhaustively Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检索增强生成——直观而全面的解释
- en: 原文：[https://towardsdatascience.com/retrieval-augmented-generation-intuitively-and-exhaustively-explain-6a39d6fe6fc9](https://towardsdatascience.com/retrieval-augmented-generation-intuitively-and-exhaustively-explain-6a39d6fe6fc9)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/retrieval-augmented-generation-intuitively-and-exhaustively-explain-6a39d6fe6fc9](https://towardsdatascience.com/retrieval-augmented-generation-intuitively-and-exhaustively-explain-6a39d6fe6fc9)
- en: Making language models that can look stuff up
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 制作能够检索信息的语言模型
- en: '[](https://medium.com/@danielwarfield1?source=post_page-----6a39d6fe6fc9--------------------------------)[![Daniel
    Warfield](../Images/c1c8b4dd514f6813e08e401401324bca.png)](https://medium.com/@danielwarfield1?source=post_page-----6a39d6fe6fc9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6a39d6fe6fc9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6a39d6fe6fc9--------------------------------)
    [Daniel Warfield](https://medium.com/@danielwarfield1?source=post_page-----6a39d6fe6fc9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@danielwarfield1?source=post_page-----6a39d6fe6fc9--------------------------------)[![Daniel
    Warfield](../Images/c1c8b4dd514f6813e08e401401324bca.png)](https://medium.com/@danielwarfield1?source=post_page-----6a39d6fe6fc9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6a39d6fe6fc9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6a39d6fe6fc9--------------------------------)
    [Daniel Warfield](https://medium.com/@danielwarfield1?source=post_page-----6a39d6fe6fc9--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6a39d6fe6fc9--------------------------------)
    ·12 min read·Oct 12, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6a39d6fe6fc9--------------------------------)
    ·阅读时长 12 分钟·2023年10月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/8a64b639851984b11ef2f5e26a773463.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a64b639851984b11ef2f5e26a773463.png)'
- en: “Data Retriever” By Daniel Warfield using MidJourney. All images by the author
    unless otherwise specified.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: “数据检索器”由 Daniel Warfield 使用 MidJourney 制作。所有图片由作者提供，除非另有说明。
- en: In this post we’ll explore “retrieval augmented generation” (RAG), a strategy
    which allows us to expose up to date and relevant information to a large language
    model. We’ll go over the theory, then imagine ourselves as resterauntours; we’ll
    implement a system allowing our customers to talk with AI about our menu, seasonal
    events, and general information.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将深入探讨“检索增强生成”（RAG）这一策略，该策略使我们能够向大型语言模型提供最新和相关的信息。我们将讨论理论，然后假设自己是餐厅老板；我们将实现一个系统，使我们的客户能够与AI讨论我们的菜单、季节性活动和一般信息。
- en: '![](../Images/275875b7b031a6af1c337568b3d25062.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/275875b7b031a6af1c337568b3d25062.png)'
- en: The final result of the practical example, a chat bot which can serve specific
    information about our restaurant.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 实践示例的最终结果是一个聊天机器人，能够提供有关我们餐厅的具体信息。
- en: '**Who is this useful for?** Anyone interested in natural language processing
    (NLP).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**谁对这个有用？** 对自然语言处理（NLP）感兴趣的任何人。'
- en: '**How advanced is this post?** This is a very powerful, but very simple concept;
    great for beginners and experts alike.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**这篇文章的难度如何？** 这是一个非常强大但又非常简单的概念；对初学者和专家都很合适。'
- en: '**Pre-requisites:** Some cursory knowledge of large language models (LLMs)
    would be helpful, but is not required.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**前提条件：** 一些大型语言模型（LLMs）的基础知识会有所帮助，但不是必须的。'
- en: The Core of the Issue
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题的核心
- en: LLMs are expensive to train; chat GPT-3 famously cost a cool $3.2M on compute
    resources alone. If we opened up a new restaurant, and wanted to use an LLM to
    answer questions about a menu, it’d be cool if we didn’t have to dish out millions
    of dollars every time we introduced a new seasonal salad. We could do a smaller
    training step (called fine tuning) to try to get the model to learn a small amount
    of highly specific information, but this process can still be hundreds to thousands
    of dollars.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 训练LLMs成本高昂；例如，GPT-3 的计算资源成本高达320万美元。如果我们开了一家新餐厅，想用LLM回答有关菜单的问题，如果我们不必每次推出新的季节性沙拉时都花费数百万美元，那将非常棒。我们可以进行较小的训练步骤（称为微调），以尝试让模型学习少量高度特定的信息，但这个过程仍然可能花费数百到数千美元。
- en: Another problem with LLMs is their confidence; sometimes they say stuff that’s
    flat out wrong with abject certainty (commonly referred to as haluscinating).
    As a result it can be difficult to discern where an LLM is getting its information
    from, and if that information is accurate. If a customer with allergies asks if
    a dish contains tree-nuts, it’d be cool if we could ensure our LLM uses accurate
    information so our patrons don’t go into anaphylactic shock.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个LLM的问题是它们的自信心；有时它们以绝对确定的方式说出完全错误的内容（通常称为幻觉）。因此，很难辨别LLM获取信息的来源，以及这些信息是否准确。如果一个过敏的客户询问一道菜是否含有坚果，如果我们能确保我们的LLM使用准确的信息，那将非常棒，以免我们的顾客进入过敏性休克。
- en: Attorney Steven A. Schwartz first landed himself in hot water through his use
    of ChatGPT, which resulted in six fake cases being cited in a legal brief. — A
    famous example of hallucination in action. [source](https://www.legaldive.com/news/chatgpt-lawyer-fake-cases-lawyer-uses-chatgpt-sanctions-generative-ai/653925/#:~:text=Attorney%20Steven%20A.,cited%20in%20a%20legal%20brief.)
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 律师 Steven A. Schwartz 首次因使用 ChatGPT 而陷入麻烦，导致法律简报中引用了六个虚假的案例。— 幻觉的一个著名例子。[来源](https://www.legaldive.com/news/chatgpt-lawyer-fake-cases-lawyer-uses-chatgpt-sanctions-generative-ai/653925/#:~:text=Attorney%20Steven%20A.,cited%20in%20a%20legal%20brief.)
- en: Both the issue of updating information and using proper sources can be mitigated
    with RAG.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 更新信息和使用适当来源的问题可以通过RAG来缓解。
- en: Retrieval Augmented Generation, In a Nutshell
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检索增强生成，总结
- en: In-context learning is the ability of an LLM to learn information not through
    training, but by receiving new information in a carefully formatted prompt.For
    example, say you wanted to ask an LLM for the punchline, and only the punchline,
    of a joke. Jokes come in a setup-punchline combo extremely often, and because
    LLMs are statistical models it can be difficult for them to break that prior knowledge.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文学习是大型语言模型（LLM）通过接收精心格式化的提示中的新信息来学习知识的能力，而不是通过训练。例如，假设你想询问LLM一个笑话的结尾部分，仅仅是结尾部分。笑话通常由设置和结尾组成，而由于LLM是统计模型，它们可能难以打破这种先前知识。
- en: '![](../Images/2870eb9dda995bc2985728811c173a04.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2870eb9dda995bc2985728811c173a04.png)'
- en: An example of ChatGPT failing a task because of lack of context
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一个因缺乏上下文而导致 ChatGPT 任务失败的例子
- en: One way we can solve this is by giving the model “context”; we can give it a
    sample in a cleverly formatted prompt such that the LLM gives us the right information.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过给模型“上下文”来解决这个问题；我们可以在巧妙格式化的提示中给它一个样本，从而使LLM给出正确的信息。
- en: '![](../Images/eff4dff862fb4034d6c4fd4d9fd7edac.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eff4dff862fb4034d6c4fd4d9fd7edac.png)'
- en: An example of ChatGPT succeeding at the same task when more context is provided
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当提供更多上下文时，ChatGPT 成功完成相同任务的一个例子
- en: This trait of LLMs has all sorts of cool uses. I’ve written an article on how
    [this ability can be used to talk with an LLM about images](https://medium.com/towards-data-science/visual-question-answering-with-frozen-large-language-models-353d42791054),
    and [how it can be used to extract information from conversations](https://medium.com/towards-data-science/conversations-as-directed-graphs-with-lang-chain-46d70e1a846c).
    In this article we’ll be leveraging this ability to inject information into the
    model via a carefully constructed prompt, based on what the user asked about,
    to provide the model in-context information.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的这一特性有各种酷炫的应用。我曾写过一篇文章讲述[这一能力如何用于与LLM讨论图像](https://medium.com/towards-data-science/visual-question-answering-with-frozen-large-language-models-353d42791054)，以及[如何利用它从对话中提取信息](https://medium.com/towards-data-science/conversations-as-directed-graphs-with-lang-chain-46d70e1a846c)。在本文中，我们将利用这种能力，通过精心构建的提示，将信息注入模型，根据用户的询问，提供上下文信息。
- en: '![](../Images/4d15714e4c78867b57b7a7402113d62c.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d15714e4c78867b57b7a7402113d62c.png)'
- en: A conceptual diagram of RAG. The prompt is used to retrieve information in a
    knowledge base, which is in turn used to augment the prompt. This augmented prompt
    is then fed into the model for generation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: RAG的概念图。提示用于从知识库中检索信息，这些信息又用于增强提示。增强后的提示然后被输入模型进行生成。
- en: 'the RAG process comes in three key parts:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: RAG过程包括三个关键部分：
- en: '**Retrieval:** Based on the prompt, retrieve relevant knowledge from a knowledge
    base.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**检索：** 根据提示，从知识库中检索相关知识。'
- en: '**Augmentation:** Combine the retrieved information with the initial prompt.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**增强：** 将检索到的信息与初始提示结合起来。'
- en: '**Generate:** pass the augmented prompt to a large language model, generating
    the final output.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**生成：** 将增强的提示传递给大型语言模型，生成最终输出。'
- en: '![](../Images/adc7ca9c29445672832edb3e7016fce7.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/adc7ca9c29445672832edb3e7016fce7.png)'
- en: An example of what a RAG prompt might look like.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: RAG提示的一个示例。
- en: Retrieval
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检索
- en: 'The only really conceptually challenging part of RAG is retrieval: How do we
    know which documents are relevant to a given prompt?'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: RAG的唯一真正概念性挑战在于检索：我们如何知道哪些文档与给定的提示相关？
- en: There’s a lot of ways this could be done. Naively, you could iterate through
    all your documents and ask an LLM “is this document relevant to the question”.
    You could pass both the document and the prompt to the LLM, ask the LLM if the
    document is relevant to the prompt, and use some query parser (I talk about those
    [here](https://medium.com/me/stats/post/46d70e1a846c)) to get the LLM to give
    you a yes or no answer.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过很多方式完成。简单地说，你可以遍历所有文档并询问LLM“这个文档是否与问题相关”。你可以将文档和提示一起传递给LLM，询问LLM该文档是否与提示相关，并使用某种查询解析器（我在[这里](https://medium.com/me/stats/post/46d70e1a846c)谈到了这些）来获取LLM的“是”或“否”回答。
- en: Alternatively, for an application as simple as ours, we could just provide all
    the data. We’ll probably only have a few documents we’ll want to refer to; our
    restaurant’s menu, events, maybe a document about the restaurants history. we
    could inject all that data into every prompt, combined with the query from a user.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，对于像我们这样简单的应用，我们可以直接提供所有数据。我们可能只需要参考几个文档；例如餐厅的菜单、活动，或许还有关于餐厅历史的文档。我们可以将所有这些数据注入到每个提示中，结合用户的查询。
- en: 'However, say we don’t just have a restaurant, but a restaurant chain. We’d
    have a vast amount of information our customers could ask about: dietary restrictions,
    when the company was founded, where the stores are located, famous people who’ve
    dined with us. We’d have an entire franchise’s worth of documents; too much data
    to just put it all in every query, and too much data to ask an LLM to iterate
    through all documents and tell us which ones are relevant.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，假设我们不仅仅有一家餐厅，而是一个餐饮连锁。我们会有大量客户可能会询问的信息：饮食限制，公司成立时间，店铺位置，与我们共进过餐的名人。我们将拥有整个连锁的文档；数据量太大，无法将所有数据放入每个查询中，也太多以至于不能要求LLM遍历所有文档并告诉我们哪些是相关的。
- en: We can use **word vector embeddings** to deal with this problem. With word vector
    embeddings we can quickly calculate the similarity of different documents and
    prompts. The next section will go over word vector embeddings in a nutshell, and
    the following section will detail how they can be used for retrieval within RAG.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用**词向量嵌入**来处理这个问题。通过词向量嵌入，我们可以快速计算不同文档和提示之间的相似度。下一节将简要介绍词向量嵌入，随后将详细说明如何在RAG中用于检索。
- en: Word Vector Embeddings in a Nutshell
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词向量嵌入概述
- en: 'This section is an excerpt from my article on transformers:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本节摘自我关于变压器的文章：
- en: '[](/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb?source=post_page-----6a39d6fe6fc9--------------------------------)
    [## Transformers — Intuitively and Exhaustively Explained'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb?source=post_page-----6a39d6fe6fc9--------------------------------)
    [## Transformers — 直观且详尽的解释]'
- en: 'Exploring the modern wave of machine learning: taking apart the transformer
    step by step'
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索现代机器学习的浪潮：一步步拆解变压器
- en: towardsdatascience.com](/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb?source=post_page-----6a39d6fe6fc9--------------------------------)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb?source=post_page-----6a39d6fe6fc9--------------------------------)
- en: In essence, a word vector embedding takes individual words and translates them
    into a vector which somehow represents its meaning.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，词向量嵌入将单个词翻译成一个向量，从而表示其含义。
- en: '![](../Images/8fb247d28028bef3a19edeb00049211e.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8fb247d28028bef3a19edeb00049211e.png)'
- en: 'The job of a word to vector embedder: turn words into numbers which somehow
    capture their general meaning.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 词到向量嵌入器的工作：将词转换为某种方式捕捉其一般含义的数字。
- en: The details can vary from implementation to implementation, but the end result
    can be thought of as a “space of words”, where the space obeys certain convenient
    relationships. **Words are hard to do math on, but vectors which contain information
    about a word, and how they relate to other words, are significantly easier to
    do math on.** This task of converting words to vectors is often referred to as
    an “embedding”.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 细节可能因实现而异，但最终结果可以被认为是一个“词空间”，这个空间遵循某些方便的关系。**单词很难进行数学运算，但包含有关单词及其与其他单词关系的信息的向量则显著更容易进行数学运算。**
    将单词转换为向量的任务通常被称为“嵌入”。
- en: Word2Vect, a landmark paper in the natural language processing space, sought
    to create an embedding which obeyed certain useful characteristics. Essentially,
    they wanted to be able to do algebra with words, and created an embedding to facilitate
    that. With Word2Vect, you could embed the word “king”, subtract the embedding
    for “man”, add the embedding for “woman”, and you would get a vector who’s nearest
    neighbor was the embedding for “queen”.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vect，作为自然语言处理领域的一个里程碑论文，试图创建一个遵循某些有用特性的嵌入。基本上，他们希望能够对单词进行代数运算，并创建了一个嵌入来促进这一点。使用Word2Vect，你可以对“king”进行嵌入，减去“man”的嵌入，添加“woman”的嵌入，你会得到一个其最近邻是“queen”嵌入的向量。
- en: '![](../Images/bc588726075ed5b6b704a11e2d222172.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc588726075ed5b6b704a11e2d222172.png)'
- en: A conceptual demonstration of doing algebra on word embeddings. If you think
    of each of the points as a vector from the origin, if you subtracted the vector
    for “man” from the vector for “king”, and added the vector for “woman”, the resultant
    vector would be near the word queen. In actuality these embedding spaces are of
    much higher dimensions, and the measurement for “closeness” can be a bit less
    intuitive (like cosine similarity), but the intuition remains the same.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对单词嵌入进行代数运算的概念演示。如果你把每一个点看作是从原点的向量，如果你从“king”的向量中减去“man”的向量，并添加“woman”的向量，那么结果向量会接近于“queen”。实际上，这些嵌入空间的维度要高得多，“接近度”的测量可能不太直观（比如余弦相似度），但直觉仍然是一样的。
- en: I’ll cover word embeddings more exhaustively in a future post, but for the purposes
    of this article they can be conceptualized as a machine learning model which has
    learned to group words as vectors in a meaningful way. With a word embedding you
    can start thinking of words in terms of distance. For instance, the distance between
    a prompt and a document. This idea of distance is what we’ll use to retrieve relevant
    documents.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在未来的文章中更全面地讲解词嵌入，但就本文而言，它们可以被概念化为一种机器学习模型，该模型已经学会以有意义的方式将单词分组为向量。通过词嵌入，你可以开始以距离的方式思考单词。例如，提示和文档之间的距离。这种距离的概念就是我们将用来检索相关文档的东西。
- en: Using Word Embeddings For Retrieval
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用词嵌入进行检索
- en: We know how to turn words into a point in some high dimensional space. How can
    we use those to know which documents are relevant to a given prompt? There’s a
    lot of ways this can be done, it’s still an active point of research, but we’ll
    consider a simple yet powerful approach; the manhattan distance of the mean vector
    embedding.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道如何将单词转换为某个高维空间中的点。我们如何利用这些点来知道哪些文档与给定提示相关？这可以通过很多方法来完成，这仍然是一个活跃的研究领域，但我们将考虑一种简单而有效的方法：均值向量嵌入的曼哈顿距离。
- en: The Mean Vector Embedding
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 均值向量嵌入
- en: We have a prompt which can be thought of as a list of words, and we have documents
    which can also be thought of as lists of words. We can summarize these lists of
    words by first embedding each word with Word2Vect, then we can calculate the average
    of all of the embeddings.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个可以被视为单词列表的提示，也有一些可以被视为单词列表的文档。我们可以通过首先使用Word2Vect对每个单词进行嵌入，然后计算所有嵌入的平均值来总结这些单词列表。
- en: '![](../Images/46b0abb65fc285f74b009544251598fd.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/46b0abb65fc285f74b009544251598fd.png)'
- en: A conceptual diagram of calculating the mean vector of all embeddings in a sequence
    of words. Each index in the resultant vector is simply the average of all the
    corresponding index in every word.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一个计算序列中所有嵌入的均值向量的概念图。结果向量中的每个索引只是每个单词中对应索引的平均值。
- en: Conceptually, because the word vector encodes the meaning of a word, the mean
    vector embedding calculates the average meaning of the entire phrase.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，因为单词向量编码了单词的含义，均值向量嵌入计算了整个短语的平均含义。
- en: '![](../Images/46f67becf61f62d925524abf0f2a075f.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/46f67becf61f62d925524abf0f2a075f.png)'
- en: A conceptual diagram of calculating the mean vector of a prompt. The patron
    to our restaurant asks “When does the restaurant have live bands?” Each of these
    words is passed through an embedder (like word2vect), and then the mean of each
    of these vectors is calculated. This is done by calculating the average of each
    index. Conceptually, this can be thought of as calculating the average meaning
    of the entire phrase.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 计算提示词均值向量的概念图。我们的餐馆的顾客问：“餐馆什么时候有现场音乐？”每一个单词都通过嵌入器（如word2vect），然后计算这些向量的均值。通过计算每个索引的平均值来完成。这可以被视为计算整个短语的平均含义。
- en: Manhattan Distance
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 曼哈顿距离
- en: Now that we’ve created a system which can summarize the meaning of a sequence
    of words down to a single vector, we can use this vector to compare how similar
    two sequences of words are. In this example, we’ll use the manhattan distance,
    though many other distance measurements can be used.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了一个系统，可以将一系列单词的含义总结为一个向量，我们可以使用这个向量比较两个单词序列的相似性。在这个例子中，我们将使用曼哈顿距离，尽管也可以使用许多其他距离测量方法。
- en: '![](../Images/80d24dab52d8ac62d1b681baaed873a4.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80d24dab52d8ac62d1b681baaed873a4.png)'
- en: 'A conceptual diagram of the manhattan distance. On the left we find it’s namesake:
    instead of a traditional distance measurement between the two points, the manhattan
    distance is the sum of the distance along the two axis; the y axis and the x axis.
    On the right you can see this concept illustrated in terms of comparing vectors.
    We find the distance between the vectors on an element by element basis, then
    sum those distances together to get the manhattan distance. Conceptually, this
    method of distance calculation is best when different axis might represent fundamentally
    different things, which is a common intuition in vector embeddings.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 曼哈顿距离的概念图。左侧是其名字的由来：与传统的两点距离测量不同，曼哈顿距离是沿两个轴——y轴和x轴的距离之和。右侧可以看到这个概念在比较向量时的表现。我们在元素逐一比较向量之间的距离，然后将这些距离相加得到曼哈顿距离。从概念上讲，这种距离计算方法在不同轴可能代表根本不同的事物时效果最佳，这在向量嵌入中是常见的直观感受。
- en: Combining these two concepts together, we can find the mean vector embedding
    of the prompt, and all documents, and use the manhattan distance to sort the documents
    in terms of distance, a proxy for relatedness.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 将这两个概念结合起来，我们可以找到提示词和所有文档的均值向量嵌入，并使用曼哈顿距离根据距离对文档进行排序，这是相关性的代理。
- en: '![](../Images/90ee7b074a6e1fd0a15a7ef6ee3f3924.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90ee7b074a6e1fd0a15a7ef6ee3f3924.png)'
- en: How the most relevant documents are found. The mean vector embedding is calculated
    for the prompt, as well as all documents. A distance is calculated between the
    prompt and all documents, allowing the retrieval system to prioritize which documents
    to include in augmentation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如何找到最相关的文档。计算提示词及所有文档的均值向量嵌入。计算提示词与所有文档之间的距离，从而使检索系统能够优先考虑哪些文档应包含在增强中。
- en: And that’s the essence of retrieval; you calculate a word vector embedding for
    all words in all pieces of text, then compute an mean vector which represents
    each piece of text. We can then use the manhattan distance as a proxy for similarity.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是检索的本质；你为所有文本中的所有单词计算词向量嵌入，然后计算一个均值向量来表示每个文本。我们可以将曼哈顿距离作为相似性的代理。
- en: In terms of actually deciding which documents to use, there’s a lot of options.
    You could set a maximum distance threshold, in which any larger distance would
    count as irrelevant, or you could always include the document with the minimum
    distance. The exact details depend on the needs of the application. To keep things
    simple we’ll always retrieve the document with the lowest distance to the prompt.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际决定使用哪些文档时，有很多选择。你可以设定一个最大距离阈值，大于这个距离的文档将被视为不相关，或者你可以始终包含距离最小的文档。具体细节取决于应用的需求。为了简单起见，我们将始终检索距离提示词最近的文档。
- en: A Note on Vector Databases
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于向量数据库的说明
- en: Before I move onto augmentation and generation, a note.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在我继续讲解增强和生成之前，有一点说明。
- en: 'In this article I wanted to focus on the concepts of RAG without going through
    the specifics of vector data bases. They’re a fascinating and incredibly powerful
    technology which I’ll be building from scratch in a future post. If you’re implementing
    RAG in a project, you’ll probably want to use a vector database to achieve better
    query performance when calculating the distance between a prompt and large number
    of documents. Here’s a few options you might be interested in:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我想专注于RAG的概念，而不深入探讨向量数据库的具体细节。它们是令人着迷且极其强大的技术，我将在未来的帖子中从零开始构建。如果你在项目中实现RAG，你可能希望使用向量数据库来提高在计算提示与大量文档之间的距离时的查询性能。以下是一些你可能感兴趣的选项：
- en: '[Chroma](https://www.trychroma.com/)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Chroma](https://www.trychroma.com/)'
- en: '[Weaviate](https://weaviate.io/)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Weaviate](https://weaviate.io/)'
- en: '[Faiss](https://github.com/facebookresearch/faiss)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Faiss](https://github.com/facebookresearch/faiss)'
- en: '[Pinecone](https://www.pinecone.io/)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Pinecone](https://www.pinecone.io/)'
- en: typically RAG is achieved by hooking up one of these databases with LangChain,
    a workflow I’m planning on tackling in another future post.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，RAG是通过将这些数据库之一与LangChain连接来实现的，这是我计划在另一个未来帖子中解决的工作流程。
- en: Augmentation and Generation
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 增强和生成
- en: 'Cool, so we’re able to retrieve which documents are relevant to a users prompt.
    How do we actually use them? This can be done with a prompt formatted to the specific
    application. For instance, we can declare the following format:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，我们能够检索哪些文档与用户的提示相关。那么我们如何实际使用这些文档呢？这可以通过格式化为特定应用程序的提示来完成。例如，我们可以声明以下格式：
- en: '[PRE0]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This format can then be used, along with whichever document was deemed useful,
    to augment the prompt. This augmented prompt can then be passed directly to the
    LLM to generate the final output.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以使用此格式以及被认为有用的文档来增强提示。这个增强的提示可以直接传递给LLM，以生成最终输出。
- en: RAG From Scratch
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零开始的RAG
- en: We’ve covered the theory; retrieval, augmentation, and generation. In order
    to further our understanding, we’ll implement RAG more or less from scratch. We’ll
    use a pre-trained word vector embedder and LLM, but we’ll do distance calculation
    and augmentation ourselves.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了理论，包括检索、增强和生成。为了进一步理解，我们将从零开始实现RAG。我们将使用预训练的词向量嵌入器和LLM，但我们会自己进行距离计算和增强。
- en: 'you can find the full code here:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到完整的代码：
- en: '[## MLWritingAndResearch/RAGFromScratch.ipynb at main · DanielWarfield1/MLWritingAndResearch'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[## MLWritingAndResearch/RAGFromScratch.ipynb at main · DanielWarfield1/MLWritingAndResearch'
- en: Notebook Examples used in machine learning writing and research - MLWritingAndResearch/RAGFromScratch.ipynb
    at main ·…
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用于机器学习写作和研究的笔记本示例 - MLWritingAndResearch/RAGFromScratch.ipynb at main ·…
- en: github.com](https://github.com/DanielWarfield1/MLWritingAndResearch/blob/main/RAGFromScratch.ipynb?source=post_page-----6a39d6fe6fc9--------------------------------)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/DanielWarfield1/MLWritingAndResearch/blob/main/RAGFromScratch.ipynb?source=post_page-----6a39d6fe6fc9--------------------------------)
- en: Downloading the Word to Vector Encoder
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载词到向量编码器
- en: First we need to download a pre-trained encoder, which has learned the relationships
    between words and, as a result, knows which words belong in certain regions of
    space.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要下载一个预训练的编码器，它已经学习了词与词之间的关系，因此知道哪些词属于某些空间区域。
- en: '[PRE1]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/eb6e21c8c6765f7e87c9e9a22c617390.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb6e21c8c6765f7e87c9e9a22c617390.png)'
- en: The embedding for the word “apple”
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 词“apple”的嵌入
- en: Embedding a Document or Prompt
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入文档或提示
- en: Now that we have an encoder, we can calculate the mean of all embeddings in
    a given word to embed an entire sequence of text, like a prompt or document.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了编码器，可以计算给定单词的所有嵌入的均值，以嵌入整个文本序列，如提示或文档。
- en: '[PRE2]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/dab78967d1a5f8ea18eb41f723ef2c4d.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dab78967d1a5f8ea18eb41f723ef2c4d.png)'
- en: The mean vector of all embeddings in “it’s a sunny day today”
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: “it’s a sunny day today”中所有嵌入的均值向量
- en: Calculating Distance
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算距离
- en: We can use scipy’s cdist function to calculate the manhattan distance, which
    is used as a proxy for similarity.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用scipy的cdist函数计算曼哈顿距离，该距离被用作相似度的代理。
- en: '[PRE3]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/ee8613767e8cc362c91ad112ad9a523b.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee8613767e8cc362c91ad112ad9a523b.png)'
- en: The distance between similar and different phrases. Notice how the similar phrases
    don’t actually have any of the same words, but have similar general meaning. Also,
    the last quote is from a book I’m reading called “Beyond Good and Evil”. I’m not
    trying to be edgy. There’s a part where Nietzsche talks about, perhaps, reality
    is painful in nature, and the strength of one's will is the capacity to observe
    it undiluted.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 相似和不同短语之间的距离。注意，相似的短语实际上没有任何相同的单词，但具有类似的一般意义。另外，最后的引用来自我正在阅读的一本书，名为《善恶的彼岸》。我并不是要显得尖锐。尼采提到，现实可能本质上是痛苦的，而一个人意志的力量就是观察它不加掩饰的能力。
- en: Defining Retrieval and Augmentation
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义检索和增强
- en: Now that we’re calculating relevance, it might be useful to define a few documents.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们在计算相关性时，定义一些文档可能会很有用。
- en: '[PRE4]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now we can define a function which uses our previous distance calculation to
    define which documents are relevant to a given prompt
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以定义一个函数，利用我们之前的距离计算来定义哪些文档与给定提示相关。
- en: '[PRE5]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/fe665c0058e8b27c3c9c0c9f337426ac.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fe665c0058e8b27c3c9c0c9f337426ac.png)'
- en: Note, this is just a proof of concept. One of the issue I faced was when the
    term “guys” showed up in the prompt, i.e. “what pasta dishes do you guys have”.
    The info states that the restaurant was founded by “two brothers”, and the info
    would come up instead of the menu. These types of quirks are the reality of the
    art.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这只是一个概念验证。我遇到的一个问题是，当提示中出现“guys”这个词时，即“你们有什么意大利面菜肴”。信息显示餐馆由“两兄弟”创办，信息会显示在菜单之前。这些类型的怪癖是艺术的现实。
- en: Augmenting and Generating
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增强和生成
- en: Now we can put it all together. Get a query from a user, retrieve relevant documents,
    augment the prompt, and pass it to an LLM.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将所有内容整合在一起。从用户那里获取查询，检索相关文档，增强提示，并将其传递给 LLM。
- en: 'Augmentation might look something like this:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 增强可能看起来像这样：
- en: '[PRE6]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/6afd0f4f0276951f0c5e9521d877480c.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6afd0f4f0276951f0c5e9521d877480c.png)'
- en: 'And generation might look something like this:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 生成可能看起来像这样：
- en: '[PRE7]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/cb97520f4847996013dded8755e7748b.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb97520f4847996013dded8755e7748b.png)'
- en: our custom RAG enabled chat bot in action.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定制的 RAG 启用聊天机器人在运行中。
- en: Conclusion
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: And that’s it! In this post we went over how word vector embeddings play a key
    part in RAG and how embeddings can be manipulated to summarize a sequence of words.
    We went over using distance to get relevant information, then tied it all together
    with augmentation to query an LLM. In the end we created a chatbot that can leverage
    up-to-date information.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些了！在这篇文章中，我们讨论了词向量嵌入如何在 RAG 中发挥关键作用，以及如何操作嵌入来总结一系列单词。我们讨论了如何使用距离获取相关信息，然后通过增强将所有内容结合起来以查询
    LLM。最后，我们创建了一个可以利用最新信息的聊天机器人。
- en: Follow For More!
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关注以获取更多信息！
- en: I describe papers and concepts in the ML space, with an emphasis on practical
    and intuitive explanations. I plan on creating more posts on best practice RAG
    implementation techniques, and implementing a vector database from scratch. Stay
    tuned!
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我描述了 ML 领域的论文和概念，强调实际和直观的解释。我计划创建更多关于最佳实践 RAG 实施技术的帖子，并从头开始实现一个向量数据库。敬请关注！
- en: '[](https://medium.com/@danielwarfield1/subscribe?source=post_page-----6a39d6fe6fc9--------------------------------)
    [## Get an email whenever Daniel Warfield publishes'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@danielwarfield1/subscribe?source=post_page-----6a39d6fe6fc9--------------------------------)
    [## 每当 Daniel Warfield 发布新文章时获取电子邮件'
- en: High quality data science articles straight to your inbox. Get an email whenever
    Daniel Warfield publishes. By signing up, you…
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高质量的数据科学文章直接送到你的邮箱。每当 Daniel Warfield 发布新文章时，你会收到邮件。通过注册，你…
- en: medium.com](https://medium.com/@danielwarfield1/subscribe?source=post_page-----6a39d6fe6fc9--------------------------------)
    [![](../Images/1f6f4c8a07d69cf53e055e0130a85b03.png)](https://www.buymeacoffee.com/danielwarfield)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/@danielwarfield1/subscribe?source=post_page-----6a39d6fe6fc9--------------------------------)
    [![](../Images/1f6f4c8a07d69cf53e055e0130a85b03.png)](https://www.buymeacoffee.com/danielwarfield)'
- en: Never expected, always appreciated. By donating you allow me to allocate more
    time and resources towards more frequent and higher quality articles. [Link](https://www.buymeacoffee.com/danielwarfield)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 意外的感激，总是令人感激。通过捐赠，你可以让我分配更多的时间和资源来撰写更频繁、更高质量的文章。[链接](https://www.buymeacoffee.com/danielwarfield)
- en: '**Attribution:** All of the images in this document were created by Daniel
    Warfield, unless a source is otherwise provided. You can use any images in this
    post for your own non-commercial purposes, so long as you reference this article,
    [https://danielwarfield.dev](https://danielwarfield.dev/), or both.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**归属:** 本文档中的所有图像均由**丹尼尔·沃菲尔德**创作，除非另有来源说明。您可以将本帖中的任何图像用于自己的非商业目的，只要您引用本文，
    [https://danielwarfield.dev](https://danielwarfield.dev/)，或两者兼顾。'
