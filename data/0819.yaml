- en: Combating Overfitting with Dropout Regularization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与**Dropout正则化**对抗过拟合
- en: 原文：[https://towardsdatascience.com/combating-overfitting-with-dropout-regularization-f721e8712fbe?source=collection_archive---------2-----------------------#2023-03-03](https://towardsdatascience.com/combating-overfitting-with-dropout-regularization-f721e8712fbe?source=collection_archive---------2-----------------------#2023-03-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/combating-overfitting-with-dropout-regularization-f721e8712fbe?source=collection_archive---------2-----------------------#2023-03-03](https://towardsdatascience.com/combating-overfitting-with-dropout-regularization-f721e8712fbe?source=collection_archive---------2-----------------------#2023-03-03)
- en: Discover the Process of Implementing Dropout in Your Own Machine Learning Models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索在自己的机器学习模型中实现Dropout的过程
- en: '[](https://medium.com/@rohankvij?source=post_page-----f721e8712fbe--------------------------------)[![Rohan
    Vij](../Images/6ef53fffb4749e1665360555bf18275f.png)](https://medium.com/@rohankvij?source=post_page-----f721e8712fbe--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f721e8712fbe--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f721e8712fbe--------------------------------)
    [Rohan Vij](https://medium.com/@rohankvij?source=post_page-----f721e8712fbe--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@rohankvij?source=post_page-----f721e8712fbe--------------------------------)[![罗汉·维吉](../Images/6ef53fffb4749e1665360555bf18275f.png)](https://medium.com/@rohankvij?source=post_page-----f721e8712fbe--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f721e8712fbe--------------------------------)[![数据科学前沿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f721e8712fbe--------------------------------)
    [罗汉·维吉](https://medium.com/@rohankvij?source=post_page-----f721e8712fbe--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe44b36765084&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombating-overfitting-with-dropout-regularization-f721e8712fbe&user=Rohan+Vij&userId=e44b36765084&source=post_page-e44b36765084----f721e8712fbe---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f721e8712fbe--------------------------------)
    ·7 min read·Mar 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff721e8712fbe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombating-overfitting-with-dropout-regularization-f721e8712fbe&user=Rohan+Vij&userId=e44b36765084&source=-----f721e8712fbe---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe44b36765084&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombating-overfitting-with-dropout-regularization-f721e8712fbe&user=Rohan+Vij&userId=e44b36765084&source=post_page-e44b36765084----f721e8712fbe---------------------post_header-----------)
    发表在[数据科学前沿](https://towardsdatascience.com/?source=post_page-----f721e8712fbe--------------------------------)
    · 7分钟阅读 · 2023年3月3日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff721e8712fbe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombating-overfitting-with-dropout-regularization-f721e8712fbe&user=Rohan+Vij&userId=e44b36765084&source=-----f721e8712fbe---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff721e8712fbe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombating-overfitting-with-dropout-regularization-f721e8712fbe&source=-----f721e8712fbe---------------------bookmark_footer-----------)![](../Images/2cb3dea34c130db7298a48db8ba2f7d2.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff721e8712fbe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombating-overfitting-with-dropout-regularization-f721e8712fbe&source=-----f721e8712fbe---------------------bookmark_footer-----------)![](../Images/2cb3dea34c130db7298a48db8ba2f7d2.png)'
- en: Photo by [Pierre Bamin](https://unsplash.com/@bamin?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[皮埃尔·巴敏](https://unsplash.com/@bamin?utm_source=medium&utm_medium=referral)拍摄，发布于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Overfitting is a common challenge that most of us have incurred or will eventually
    incur when training and utilizing a machine learning model. Ever since the dawn
    of machine learning, researchers have been trying to combat overfitting. One such
    technique they came up with was dropout regularization, in which neurons in the
    model are removed at random. In this article, we will explore how dropout regularization
    works, how you can implement it in your own model, as well as its benefits and
    disadvantages when compared to other methods.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合是大多数人在训练和使用机器学习模型时都会遇到的常见挑战。自机器学习诞生以来，研究人员一直在尝试解决过拟合问题。其中一种方法是 dropout 正则化，即随机删除模型中的神经元。在本文中，我们将探讨
    dropout 正则化的工作原理，如何在自己的模型中实现它，以及与其他方法相比的优缺点。
- en: I. Introduction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: I. 介绍
- en: What is Overfitting?
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是过拟合？
- en: 'Overfitting is when a model is overtrained on its training data, leading it
    to perform poorly on new data. Essentially, in the model’s strive to be as accurate
    as possible, it focuses too much on fine details and noise within its training
    dataset. These attributes are often not present in real-world data, so the model
    tends to not perform well. Overfitting can occur when a model has too many parameters
    relative to the amount of data. This can lead the model to hyper-focus on smaller
    details that are not relevant to the general patterns the model must develop.
    For example, suppose a complex model (many parameters) is trained to identify
    whether a horse is present in a picture or not. In that case, it might start focusing
    on details about the sky or environment rather than the horse itself. This can
    happen when:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合是指模型在训练数据上过度训练，导致其在新数据上的表现不佳。实质上，模型在追求尽可能准确的过程中，过分关注训练数据中的细节和噪声。这些特征在现实世界的数据中往往不存在，因此模型的表现往往不佳。当模型的参数数量相对于数据量过多时，就容易发生过拟合。这会导致模型过度关注与模型需要开发的一般模式无关的小细节。例如，假设一个复杂的模型（参数较多）被训练来识别图片中是否存在马，那么它可能会开始关注天空或环境的细节，而不是马本身。这种情况可能发生在：
- en: The model is too complex (has too many parameters) for its own good.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型过于复杂（参数过多），对自己不利。
- en: The model is trained for too long.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型训练时间过长。
- en: The dataset the model was trained on is too small.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型训练所用的数据集过小。
- en: The model is trained and tested on the same data.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型在相同的数据上进行训练和测试。
- en: The dataset the model is trained on has repetitive features that make it prone
    to overfitting.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型训练的数据集具有重复的特征，容易导致过拟合。
- en: Why is Overfitting Important?
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么过拟合很重要？
- en: Overfitting is more than a simple annoyance — it can destroy entire models.
    It gives the illusion that a model is performing well, even though it will have
    failed to make proper generalizations about the data provided.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合不仅仅是一个简单的烦恼——它可能摧毁整个模型。它给人一种模型表现良好的假象，尽管它可能没有对提供的数据做出正确的概括。
- en: Overfitting can have extremely serious consequences, especially in fields such
    as healthcare, where AI is becoming more and more proliferated. An AI that was
    not properly trained nor tested due to overfitting can lead to incorrect diagnoses.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合可能会带来极其严重的后果，尤其是在医疗保健等领域，人工智能越来越普及。由于过拟合而未能经过适当训练或测试的人工智能可能会导致错误的诊断。
- en: II. What is Dropout?
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: II. 什么是 Dropout？
- en: Dropout as a Regularization Technique
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dropout 作为一种正则化技术
- en: Ideally, the best way to combat overfitting would be to train a plethora of
    models of different architecture all on the same dataset and then average their
    outputs. The problem with this approach is that it is incredibly resource and
    time intensive. While it might be affordable with relatively small models, large
    models that might take large amounts of time to train could easily overwhelm anyone’s
    resources.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，对抗过拟合的最佳方法是训练多种不同架构的模型，并对它们的输出进行平均。然而，这种方法的问题在于，它极其耗费资源和时间。虽然相对较小的模型可能还算负担得起，但训练大型模型可能需要大量时间，这很容易超出任何人的资源承受能力。
- en: Dropout works by essentially “dropping” a neuron from the input or hidden layers.
    Multiple neurons are removed from the network, meaning they practically do not
    exist — their incoming and outcoming connections are also destroyed. This artificially
    creates a multitude of smaller, less complex networks. This forces the model to
    not become solely dependent on one neuron, meaning it has to diversify its approach
    and develop a multitude of methods to achieve the same result. For instance, going
    back to the horse example, if one neuron is primarily responsible for the tree
    part of the horse, its being dropped will force the model to focus more on other
    features of the image. Dropout can also be applied directly to the input neurons,
    meaning that entire features go missing from the model.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout 的工作原理是通过“丢弃”输入层或隐藏层中的神经元来实现的。网络中会移除多个神经元，这意味着它们实际上不存在——它们的输入和输出连接也会被破坏。这会人为地创建出多个较小的、更简单的网络。这迫使模型不依赖于一个单独的神经元，也就是说，它必须多样化其方法，并开发出多种方法来实现相同的结果。例如，回到马的例子，如果一个神经元主要负责马的树部分，那么它被丢弃会迫使模型更多地关注图像的其他特征。Dropout
    也可以直接应用于输入神经元，这意味着整个特征会从模型中消失。
- en: Applying Dropout to a Neural Network
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 Dropout 应用于神经网络
- en: Dropout is applied to a neural network by randomly dropping neurons in every
    layer (including the input layer). A pre-defined dropout rate determines the chance
    of each neuron being dropped. For example, a dropout rate of 0.25 means that there
    is a 25% chance of a neuron being dropped. Dropout is applied during every epoch
    during the training of the model.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout 是通过在每一层（包括输入层）随机丢弃神经元来应用于神经网络的。预定义的 dropout 率决定了每个神经元被丢弃的概率。例如，dropout
    率为 0.25 意味着神经元被丢弃的概率是 25%。Dropout 在模型训练的每个 epoch 中应用。
- en: Keep in mind that there is no ideal dropout value — it heavily depends on the
    hyperparameters and end goal of the model.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，没有理想的 dropout 值——这在很大程度上依赖于模型的超参数和最终目标。
- en: Dropout and Sexual Reproduction
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dropout 和有性繁殖
- en: Think back to your freshman biology class — you probably covered meiosis, or
    sexual reproduction. During the process of meiosis, random genes mutation occur.
    This means that the resulting offspring might have traits that both parents do
    not have present in their genes. This randomness, over time, allows populations
    of organisms to become more suited to their environment. This process is called
    evolution, and without it, we would not exist today.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下你的大一生物课——你可能学过减数分裂或有性繁殖。在减数分裂的过程中，基因会发生随机突变。这意味着结果后代可能具有父母双方基因中都没有的特征。这种随机性随着时间的推移，使生物群体更适应其环境。这个过程被称为进化，没有它，我们今天也不会存在。
- en: Both dropout and sexual reproduction seek to increase diversity and stop a system
    from becoming reliant on one set of parameters, with no room for improvement.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout 和有性繁殖都旨在增加多样性，防止系统依赖于一组参数，而没有改进的空间。
- en: III. Apply Dropout to Your Model
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: III. 将 Dropout 应用于您的模型
- en: Dataset
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: 'Let’s start with a dataset that might be prone to overfitting:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个可能容易过拟合的数据集开始：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This data ties back to our example of the horse and its environment. We have
    abstracted the qualities of the image into a simple format it is easy to interpret.
    As can be clearly seen, the data is not ideal as images with horses in them also
    happen to contain trees, green grass, or a blue sky — they might be in the same
    picture, but one does not influence the other.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据回到了我们关于马和它的环境的例子。我们已将图像的特性抽象成一个易于解释的简单格式。可以清楚地看到，数据并不理想，因为包含马的图像也可能包含树、绿色草地或蓝色天空——它们可能在同一张图片中，但一个不会影响另一个。
- en: The MLP Model
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MLP 模型
- en: 'Let’s quickly create a simple MLP using Keras:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用 Keras 快速创建一个简单的 MLP：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: I highly recommend using Python notebooks such as Jupyter Notebook to organize
    your code so you can quickly rerun cells without having to retrain the model.
    Split the code along each comment.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈推荐使用像 Jupyter Notebook 这样的 Python 笔记本来组织你的代码，以便你可以快速重新运行单元，而无需重新训练模型。沿着每个注释拆分代码。
- en: 'Let’s further analyze the data we are testing the model with:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进一步分析一下我们正在测试模型的数据：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Essentially, we have an image with all the attributes of a horse, but without
    any of the environmental factors we included in the data (green grass, blue sky,
    tree, etc). The model outputs:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，我们有一张包含所有马的属性的图像，但没有我们在数据中包含的任何环境因素（绿色草地、蓝色天空、树木等）。模型输出：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Ouch! Even though the model has a face and a tail — what we are using to identify
    the horse — it is only 2.7% sure that the image is a horse image.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀！即使模型有脸和尾巴——我们用来识别马的特征——它对图像是马的判断信心只有 2.7%。
- en: Implementing Dropout in an MLP
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 MLP 中实现 Dropout
- en: 'Keras makes implementing dropout, among other methods to prevent overfitting,
    shockingly simple. We just have to go back to the list containing the layers of
    the model:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 使得实现 dropout（以及其他防止过拟合的方法）变得极其简单。我们只需返回到包含模型层的列表：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: And add some dropout layers!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 并添加一些 dropout 层！
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now the model outputs:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型输出：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: It is 99% sure that the horse image, even though it does not contain the environmental
    variables, is a horse!
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管马图像中不包含环境变量，它的判断信心为 99% 这是一张马的图片！
- en: The `Dropout(0.5)` line indicates that any of the neurons in the layer above
    have a 50% chance of being “dropped,” or removed from existence, in reference
    tod the following layers. By implementing dropout, we have essentially trained
    the MLP on hundreds of models in a resource-efficient manner.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dropout(0.5)` 这一行表示，上层中的任何神经元都有 50% 的机会被“丢弃”或从存在中移除，以便对后续层进行参考。通过实现 dropout，我们实际上以资源高效的方式在数百个模型上训练了
    MLP。'
- en: Choosing a Dropout Rate
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择一个 Dropout 率
- en: The best way to find the ideal dropout rate for your model is through trial
    and error — there is no one-size-fits-all. Start with a low dropout rate, around
    0.1 or 0.2, and slowly increase it until you reach your desired accuracy. Using
    our horse MLP, a dropout of 0.05 results in the model being 16.5% confident the
    image is that of a horse. On the other hand, a dropout of 0.95 simply drops out
    too many neurons for the model to function — but still, a confidence of 54.1%
    is achieved. These values are not appropriate for this model, but that does mean
    they might be the right fit for others.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 找到适合你模型的理想 dropout 率的最佳方法是通过反复试验——没有一种放之四海而皆准的方法。可以从较低的 dropout 率开始，大约 0.1 或
    0.2，然后逐渐增加，直到达到你期望的准确性。以我们的马匹 MLP 为例，dropout 率为 0.05 会导致模型对图像是马的判断信心为 16.5%。另一方面，dropout
    率为 0.95 会导致模型丧失过多的神经元以至于无法正常工作——但仍然能够达到 54.1% 的信心。这些值对该模型并不合适，但可能对其他模型适用。
- en: IV. Conclusion
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: IV. 结论
- en: Let’s recap — dropout is a powerful technique used in machine learning to prevent
    overfitting and overall improve model performance. It does this by randomly “dropping”
    neurons from the model in the input and hidden layers. This allows the classifier
    to train on hundreds to thousands of unique models in one training session, preventing
    it from hyper-focusing on certain features.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下——dropout 是一种在机器学习中用于防止过拟合并整体提升模型性能的强大技术。它通过随机“丢弃”输入层和隐藏层中的神经元来实现这一点。这使得分类器在一次训练过程中能够训练出数百到数千个独特的模型，从而避免过度关注某些特征。
- en: In the coming articles, we will discover new techniques used in the field of
    machine learning as an alternative or addition to dropout. Stay tuned for more!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的文章中，我们将探索在机器学习领域中用于替代或补充 dropout 的新技术。敬请期待更多内容！
