- en: RAG vs Finetuning — Which Is the Best Tool to Boost Your LLM Application?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAG与微调——哪种是提升你的LLM应用的最佳工具？
- en: 原文：[https://towardsdatascience.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application-94654b1eaba7](https://towardsdatascience.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application-94654b1eaba7)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application-94654b1eaba7](https://towardsdatascience.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application-94654b1eaba7)
- en: The definitive guide for choosing the right method for your use case
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为你的用例选择正确方法的权威指南
- en: '[](https://heiko-hotz.medium.com/?source=post_page-----94654b1eaba7--------------------------------)[![Heiko
    Hotz](../Images/d08394d46d41d5cd9e76557a463be95e.png)](https://heiko-hotz.medium.com/?source=post_page-----94654b1eaba7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----94654b1eaba7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----94654b1eaba7--------------------------------)
    [Heiko Hotz](https://heiko-hotz.medium.com/?source=post_page-----94654b1eaba7--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://heiko-hotz.medium.com/?source=post_page-----94654b1eaba7--------------------------------)[![Heiko
    Hotz](../Images/d08394d46d41d5cd9e76557a463be95e.png)](https://heiko-hotz.medium.com/?source=post_page-----94654b1eaba7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----94654b1eaba7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----94654b1eaba7--------------------------------)
    [Heiko Hotz](https://heiko-hotz.medium.com/?source=post_page-----94654b1eaba7--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----94654b1eaba7--------------------------------)
    ·19 min read·Aug 24, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----94654b1eaba7--------------------------------)
    ·19分钟阅读·2023年8月24日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f87aad7c078e00a5f9db9a00ad4debad.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f87aad7c078e00a5f9db9a00ad4debad.png)'
- en: Image by author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Prologue
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: 'As the wave of interest in Large Language Models (LLMs) surges, many developers
    and organisations are busy building applications harnessing their power. However,
    when the pre-trained LLMs out of the box don’t perform as expected or hoped, the
    question on how to improve the performance of the LLM application. And eventually
    we get to the point of where we ask ourselves: Should we use [Retrieval-Augmented
    Generation](https://arxiv.org/abs/2005.11401) (RAG) or model finetuning to improve
    the results?'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对大型语言模型（LLMs）兴趣的浪潮涌动，许多开发者和组织忙于构建利用其强大功能的应用。然而，当现成的预训练LLMs未能如预期般表现时，如何提升LLM应用性能的问题就会浮现。最终，我们会问自己：我们应该使用[检索增强生成](https://arxiv.org/abs/2005.11401)（RAG）还是模型微调来改善结果？
- en: 'Before diving deeper, let’s demystify these two methods:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨之前，让我们揭开这两种方法的神秘面纱：
- en: '**RAG**: This approach integrates the power of retrieval (or searching) into
    LLM text generation. It combines a retriever system, which fetches relevant document
    snippets from a large corpus, and an LLM, which produces answers using the information
    from those snippets. In essence, RAG helps the model to “look up” external information
    to improve its responses.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**RAG**：这种方法将检索（或搜索）的力量集成到LLM文本生成中。它结合了一个检索系统，该系统从大规模语料库中获取相关文档片段，以及一个LLM，该LLM使用这些片段中的信息生成答案。实质上，RAG帮助模型“查找”外部信息以改进其响应。'
- en: '![](../Images/37f3ba773a6c401ffdd3eead8bdf76b7.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/37f3ba773a6c401ffdd3eead8bdf76b7.png)'
- en: Image by author
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: '**Finetuning**: This is the process of taking a pre-trained LLM and further
    training it on a smaller, specific dataset to adapt it for a particular task or
    to improve its performance. By finetuning, we are adjusting the model’s weights
    based on our data, making it more tailored to our application’s unique needs.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**微调**：这是将预训练LLM进一步在一个较小的特定数据集上进行训练的过程，以便将其适应于特定任务或提升其性能。通过微调，我们根据我们的数据调整模型的权重，使其更加符合我们应用的独特需求。'
- en: '![](../Images/f8686a451839dd8611ecc0de7f592cb4.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8686a451839dd8611ecc0de7f592cb4.png)'
- en: Image by author
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Both RAG and finetuning serve as powerful tools in enhancing the performance
    of LLM-based applications, but they address different aspects of the optimisation
    process, and this is crucial when it comes to choosing one over the other.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: RAG和微调都是提升基于LLM的应用性能的强大工具，但它们针对优化过程的不同方面，这在选择其中一个时至关重要。
- en: 'Previously, I would often suggest to organisations that they experiment with
    RAG before diving into finetuning. This was based on my perception that both approaches
    achieved similar results but varied in terms of complexity, cost, and quality.
    I even used to illustrate this point with diagrams such as this one:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我经常建议组织在深入微调之前先尝试 RAG。这是基于我对这两种方法虽然实现了类似结果但在复杂性、成本和质量上有所不同的看法。我甚至曾用类似的图表来说明这一点：
- en: '![](../Images/96b1f7fd587d026b3ca2d92f3424c3a5.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96b1f7fd587d026b3ca2d92f3424c3a5.png)'
- en: Image by author
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'In this diagram, various factors like complexity, cost, and quality are represented
    along a single dimension. The takeaway? RAG is simpler and less expensive, but
    its quality might not match up. My advice usually was: start with RAG, gauge its
    performance, and if found lacking, shift to finetuning.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图表中，各种因素如复杂性、成本和质量沿着单一维度表示。要点是什么？RAG 更简单且成本更低，但其质量可能无法匹配。我的建议通常是：从 RAG 开始，评估其性能，如果发现不足，则转向微调。
- en: However, my perspective has since evolved. I believe it’s an oversimplification
    to view RAG and finetuning as two techniques that achieve the same result, just
    where one is just cheaper and less complex than the other. They are fundamentally
    distinct — instead of *co-linear* they are actually *orthogonal* — and serve different
    requirements of an LLM application.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我的观点已经有所变化。我认为将 RAG 和微调视为实现相同结果的两种技术，这种看法过于简化，因为一种技术更便宜、复杂度更低而已。它们本质上是不同的——而不是*共线*，而是*正交*——且满足
    LLM 应用的不同需求。
- en: 'To make this clearer, consider a simple real-world analogy: When posed with
    the question, “Should I use a knife or a spoon to eat my meal?”, the most logical
    counter-question is: “Well, what are you eating?” I asked friends and family this
    question and everyone instinctively replied with that counter-question, indicating
    that they don’t view the knife and spoon as interchangeable, or one as an inferior
    variant of the other.'
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为了更清楚地说明这一点，可以考虑一个简单的现实世界类比：当被问到“我应该用刀子还是勺子来吃饭？”时，最合逻辑的反问是：“你在吃什么？”我问过朋友和家人，每个人本能地回答了这个反问，这表明他们并不认为刀子和勺子是可以互换的，或者其中一种是另一种的低级变体。
- en: What is this about?
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这是什么内容？
- en: In this blog post, we’ll dive deep into the nuances that differentiate RAG and
    finetuning across various dimensions that are, in my opinion, crucial in determining
    the optimal technique for a specific task. Moreover, we’ll be looking at some
    of the most popular use cases for LLM applications and use the dimensions established
    in the first part to identify which technique might be best suited for which use
    case. In the last part of this blog post we will identify additional aspects that
    should be considered when building LLM applications. Each one of those might warrant
    its own blog post and therefore we can only touch briefly on them in the scope
    of this post.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客中，我们将深入探讨区分 RAG 和微调的各种维度，这些维度在我看来对于确定特定任务的最佳技术至关重要。此外，我们将查看一些最受欢迎的 LLM
    应用场景，并使用第一部分确定的维度来识别哪种技术最适合哪些用例。在博客的最后部分，我们将识别在构建 LLM 应用时应考虑的其他方面。每一个方面可能都值得单独撰写博客，因此在本篇博客中我们只能简要提及。
- en: Why should you care?
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你为什么要关心这个？
- en: 'Choosing the right technique for adapting large language models can have a
    major impact on the success of your NLP applications. Selecting the wrong approach
    can lead to:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 选择适合调整大型语言模型的技术对 NLP 应用的成功有重大影响。选择错误的方法可能导致：
- en: Poor model performance on your specific task, resulting in inaccurate outputs.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的具体任务上，模型表现不佳，导致输出不准确。
- en: Increased compute costs for model training and inference if the technique is
    not optimized for your use case.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果技术未针对你的使用案例进行优化，模型训练和推理的计算成本会增加。
- en: Additional development and iteration time if you need to pivot to a different
    technique later on.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果需要以后转向不同的技术，则会增加额外的开发和迭代时间。
- en: Delays in deploying your application and getting it in front of users.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署应用程序并让其面向用户的延迟。
- en: A lack of model interpretability if you choose an overly complex adaptation
    approach.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果选择了过于复杂的适配方法，则可能缺乏模型解释性。
- en: Difficulty deploying the model to production due to size or computational constraints.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于大小或计算约束，难以将模型部署到生产环境中。
- en: The nuances between RAG and finetuning span model architecture, data requirements,
    computational complexity, and more. **Overlooking these details can derail your
    project timeline and budget.**
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 和微调之间的细微差别涵盖了模型架构、数据需求、计算复杂性等多个方面。**忽视这些细节可能会破坏你的项目时间表和预算。**
- en: This blog post aims to prevent wasted effort by clearly laying out when each
    technique is advantageous. With these insights, you can hit the ground running
    with the right adaptation approach from day one. The detailed comparison will
    equip you to make the optimal technology choice to achieve your business and AI
    goals. **This guide to selecting the right tool for the job will set your project
    up for success.**
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本博客旨在通过清晰地阐述何时使用每种技术来防止浪费努力。通过这些见解，你可以从第一天起就以正确的适应方法迅速开展工作。详细的比较将使你能够做出最佳的技术选择，以实现你的商业和
    AI 目标。**本指南将帮助你选择合适的工具，为你的项目成功奠定基础。**
- en: So let’s dive in!
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们深入探讨吧！
- en: '**Key considerations for boosting performance**'
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**提升性能的关键考虑因素**'
- en: Before we choose RAG vs Fintuning, we should assess the requirements of our
    LLM project along some dimensions and ask ourselves a few questions.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择 RAG 还是微调之前，我们应该评估 LLM 项目的需求，并在一些维度上提出几个问题。
- en: Does our use case require access to external data sources?
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的用例是否需要访问外部数据源？
- en: When choosing between finetuning an LLM or using RAG, one key consideration
    is whether the application requires access to external data sources. If the answer
    is yes, RAG is likely the better option.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择对 LLM 进行微调还是使用 RAG 时，一个关键的考虑因素是应用程序是否需要访问外部数据源。如果答案是肯定的，RAG 可能是更好的选择。
- en: RAG systems are, by definition, designed to augment an LLM’s capabilities by
    retrieving relevant information from knowledge sources before generating a response.
    This makes this technique well-suited for applications that need to query databases,
    documents, or other structured/unstructured data repositories. The retriever and
    generator components can be optimised to leverage these external sources.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 系统的定义是通过在生成回应之前从知识来源检索相关信息，以增强 LLM 的能力。这使得这种技术非常适合需要查询数据库、文档或其他结构化/非结构化数据存储的应用程序。检索器和生成器组件可以被优化以利用这些外部来源。
- en: In contrast, while it is possible to finetune an LLM to learn some external
    knowledge, doing so requires a large labelled dataset of question-answer pairs
    from the target domain. This dataset must be updated as the underlying data changes,
    making it impractical for frequently changing data sources. The finetuning process
    also does not explicitly model the retrieval and reasoning steps involved in querying
    external knowledge.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，虽然可以对 LLM 进行微调以学习一些外部知识，但这需要来自目标领域的大量标注数据集。随着基础数据的变化，这些数据集必须不断更新，使得它不适用于频繁变化的数据源。微调过程也没有明确建模在查询外部知识时涉及的检索和推理步骤。
- en: So in summary, if our application needs to leverage external data sources, using
    a RAG system will likely be more effective and scalable than attempting to “bake
    in” the required knowledge through finetuning alone.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，如果我们的应用程序需要利用外部数据源，使用 RAG 系统可能会比单纯依靠微调“内置”所需知识更有效和可扩展。
- en: Do we need to modify the model’s behaviour, writing style, or domain-specific
    knowledge?
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们是否需要修改模型的行为、写作风格或领域特定知识？
- en: Another very important aspect to consider is how much we need the model to adjust
    its behaviour, its writing style, or tailor its responses for domain-specific
    applications.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个非常重要的方面是我们需要模型在多大程度上调整其行为、写作风格或为领域特定应用量身定制其回应。
- en: Finetuning excels in its ability to adapt an LLM’s behaviour to specific nuances,
    tones, or terminologies. If we want the model to sound more like a medical professional,
    write in a poetic style, or use the jargon of a specific industry, finetuning
    on domain-specific data allows us to achieve these customisations. This ability
    to influence the model’s behaviour is essential for applications where alignment
    with a particular style or domain expertise is vital.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 微调在将 LLM 的行为适应于特定的细微差别、语气或术语方面表现出色。如果我们希望模型听起来更像医学专业人员、用诗意的风格写作，或使用特定行业的术语，在领域特定数据上进行微调可以实现这些定制。这种影响模型行为的能力对于需要与特定风格或领域专长一致的应用程序至关重要。
- en: RAG, while powerful in incorporating external knowledge, primarily focuses on
    information retrieval and doesn’t inherently adapt its linguistic style or domain-specificity
    based on the retrieved information. It will pull relevant content from the external
    data sources but might not exhibit the tailored nuances or domain expertise that
    a finetuned model can offer.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: RAG虽然在整合外部知识方面强大，但主要关注信息检索，并不会根据检索到的信息自我调整语言风格或领域特定性。它将从外部数据源中提取相关内容，但可能不会展现出微调模型所能提供的定制化细微差别或领域专业知识。
- en: So, if our application demands specialised writing styles or deep alignment
    with domain-specific vernacular and conventions, finetuning presents a more direct
    route to achieving that alignment. It offers the depth and customisation necessary
    to genuinely resonate with a specific audience or expertise area, ensuring the
    generated content feels authentic and well-informed.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们的应用程序需要特定的写作风格或深度对齐于特定领域的术语和惯例，微调提供了实现这种对齐的更直接途径。它提供了深入的定制，使得内容真正与特定受众或专业领域产生共鸣，确保生成的内容感觉真实且信息丰富。
- en: Quick recap
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 快速回顾
- en: These two aspects are by far the most important ones to consider when deciding
    which method to use to boost LLM application performance. Interestingly, they
    are, in my opinion, orthogonal and can be used independently (and also be combined).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个方面是决定使用哪种方法来提升LLM应用性能时最重要的方面。有趣的是，我认为它们是正交的，可以独立使用（也可以组合使用）。
- en: '![](../Images/0223fdf1d85e8e9c749b8759307f0c1e.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0223fdf1d85e8e9c749b8759307f0c1e.png)'
- en: Image by author
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'However, before diving into the use cases, there are a few more key aspects
    we should consider before choosing a method:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 但在深入使用案例之前，我们还应该考虑几个关键方面来选择方法：
- en: How crucial is it to suppress hallucinations?
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 抑制幻觉有多重要？
- en: One downside of LLMs is their tendency to hallucinate — making up facts or details
    that have no basis in reality. This can be highly problematic in applications
    where accuracy and truthfulness are critical.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的一个缺点是它们有产生幻觉的倾向——编造没有现实依据的事实或细节。这在准确性和真实性至关重要的应用中可能会非常有问题。
- en: Finetuning can help reduce hallucinations to some extent by grounding the model
    in a specific domain’s training data. However, the model may still fabricate responses
    when faced with unfamiliar inputs. Retraining on new data is required to continuously
    minimise false fabrications.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 微调可以在一定程度上帮助减少幻觉，通过将模型基于特定领域的训练数据。然而，当面对不熟悉的输入时，模型仍可能会编造响应。需要对新数据进行再训练以持续最小化虚假伪造。
- en: In contrast, RAG systems are inherently less prone to hallucination because
    they ground each response in retrieved evidence. The retriever identifies relevant
    facts from the external knowledge source before the generator constructs the answer.
    This retrieval step acts as a fact-checking mechanism, reducing the model’s ability
    to confabulate. The generator is constrained to synthesise a response supported
    by the retrieved context.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，RAG系统本质上较不容易产生幻觉，因为它们将每个响应基于检索到的证据。检索器在生成器构造答案之前会从外部知识源中识别相关事实。这一步骤充当了一个事实检查机制，减少了模型虚构的能力。生成器被限制在基于检索到的上下文合成响应。
- en: So in applications where suppressing falsehoods and imaginative fabrications
    is vital, RAG systems provide in-built mechanisms to minimise hallucinations.
    The retrieval of supporting evidence prior to response generation gives RAG an
    advantage in ensuring factually accurate and truthful outputs.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在需要抑制虚假信息和想象性伪造至关重要的应用中，RAG系统提供了内建机制来最小化幻觉。生成响应之前的证据检索使RAG在确保输出真实准确方面具有优势。
- en: How much labelled training data is available?
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有多少标记好的训练数据可用？
- en: When deciding between RAG and finetuning, a crucial factor to consider is the
    volume of domain- or task-specific, labelled training data at our disposal.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定RAG和微调之间时，一个关键因素是我们手头上是否有大量的领域或任务特定的标记训练数据。
- en: Finetuning an LLM to adapt to specific tasks or domains is heavily dependent
    on the quality and quantity of the labelled data available. A rich dataset can
    help the model deeply understand the nuances, intricacies, and unique patterns
    of a particular domain, allowing it to generate more accurate and contextually
    relevant responses. However, if we’re working with a limited dataset, the improvements
    from finetuning might be marginal. In some cases, a scant dataset might even lead
    to overfitting, where the model performs well on the training data but struggles
    with unseen or real-world inputs.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对 LLM 进行微调以适应特定任务或领域严重依赖于可用标注数据的质量和数量。丰富的数据集可以帮助模型深入理解特定领域的细微差别、复杂性和独特模式，从而生成更准确和上下文相关的回应。然而，如果我们使用的是有限的数据集，微调的改善可能会很小。在某些情况下，稀少的数据集甚至可能导致过拟合，模型在训练数据上表现良好，但在未见或现实世界输入中表现不佳。
- en: On the contrary, RAG systems are independent from training data because they
    leverage external knowledge sources to retrieve relevant information. Even if
    we don’t have an extensive labelled dataset, a RAG system can still perform competently
    by accessing and incorporating insights from its external data sources. The combination
    of retrieval and generation ensures that the system remains informed, even when
    domain-specific training data is sparse.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，RAG 系统独立于训练数据，因为它们利用外部知识源来检索相关信息。即使我们没有广泛的标注数据集，RAG 系统仍然可以通过访问和整合外部数据源的见解来有效地工作。检索和生成的结合确保了系统保持知情，即使在领域特定的训练数据稀缺时。
- en: In essence, if we have a wealth of labelled data that captures the domain’s
    intricacies, finetuning can offer a more tailored and refined model behaviour.
    But in scenarios where such data is limited, a RAG system provides a robust alternative,
    ensuring the application remains data-informed and contextually aware through
    its retrieval capabilities.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上，如果我们拥有大量标注数据，能够捕捉领域的复杂性，微调可以提供更加定制化和精细化的模型行为。但在数据有限的情况下，RAG 系统提供了一个稳健的替代方案，通过其检索能力确保应用保持数据驱动和上下文相关。
- en: How static/dynamic is the data?
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据的静态/动态特性如何？
- en: Another fundamental aspect to consider when choosing between RAG and finetuning
    is the dynamic nature of our data. How frequently is the data updated, and how
    imperative is it for the model to stay current?
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择 RAG 和微调时，另一个需要考虑的基本方面是我们数据的动态特性。数据更新的频率如何？对于模型保持最新状态的重要性如何？
- en: Finetuning an LLM on a specific dataset means the model’s knowledge becomes
    a static snapshot of that data at the time of training. If the data undergoes
    frequent updates, changes, or expansions, this can quickly render the model outdated.
    To keep the LLM current in such dynamic environments, we’d have to retrain it
    frequently, a process that can be both time-consuming and resource-intensive.
    Additionally, each iteration requires careful monitoring to ensure that the updated
    model still performs well across different scenarios and hasn’t developed new
    biases or gaps in understanding.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对特定数据集进行 LLM 微调意味着模型的知识成为训练时数据的静态快照。如果数据经常更新、变化或扩展，这会迅速使模型过时。为了保持 LLM 在这种动态环境中的时效性，我们必须频繁地重新训练模型，这个过程既耗时又资源密集。此外，每次迭代都需要仔细监控，以确保更新后的模型在不同场景中仍表现良好，并且没有出现新的偏见或理解上的漏洞。
- en: In contrast, RAG systems inherently possess an advantage in environments with
    dynamic data. Their retrieval mechanism constantly queries external sources, ensuring
    that the information they pull in for generating responses is up-to-date. As the
    external knowledge bases or databases update, the RAG system seamlessly integrates
    these changes, maintaining its relevance without the need for frequent model retraining.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 相对而言，RAG 系统在动态数据环境中天生具有优势。它们的检索机制不断查询外部源，确保用于生成回应的信息是最新的。随着外部知识库或数据库的更新，RAG
    系统无缝地整合这些变化，保持其相关性而无需频繁的模型重新训练。
- en: In summary, if we’re grappling with a rapidly evolving data landscape, RAG offers
    an agility that’s hard to match with traditional finetuning. By always staying
    connected to the most recent data, RAG ensures that the responses generated are
    in tune with the current state of information, making it an ideal choice for dynamic
    data scenarios.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，如果我们在应对快速发展的数据环境，RAG 提供了一种与传统微调难以匹敌的灵活性。通过始终连接到最新的数据，RAG 确保生成的回应与当前的信息状态保持一致，使其成为动态数据场景的理想选择。
- en: How transparent/interpretable does our LLM app need to be?
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的LLM应用需要多透明/可解释？
- en: The last aspect to consider is the degree to which we need insights into the
    model’s decision-making process.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个要考虑的方面是我们需要对模型决策过程的洞察程度。
- en: Finetuning an LLM, while incredibly powerful, operates like a black box, making
    the reasoning behind its responses more opaque. As the model internalises the
    information from the dataset, it becomes challenging to discern the exact source
    or reasoning behind each response. This can make it difficult for developers or
    users to trust the model’s outputs, especially in critical applications where
    understanding the “why” behind an answer is vital.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 微调LLM虽然极具威力，但像一个黑箱一样运行，使得其响应背后的推理更为不透明。随着模型从数据集中内化信息，识别每个响应背后的确切来源或推理变得困难。这可能会使开发者或用户难以信任模型的输出，特别是在需要理解答案背后“为什么”的关键应用中。
- en: RAG systems, on the other hand, offer a level of transparency that’s not typically
    found in solely finetuned models. Given the two-step nature of RAG — retrieval
    and then generation — users can peek into the process. The retrieval component
    allows for the inspection of which external documents or data points are selected
    as relevant. This provides a tangible trail of evidence or reference that can
    be evaluated to understand the foundation upon which a response is built. The
    ability to trace back a model’s answer to specific data sources can be invaluable
    in applications that demand a high degree of accountability or when there’s a
    need to validate the accuracy of the generated content.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: RAG系统提供了一种在纯粹微调模型中通常找不到的透明度。鉴于RAG的两步性质——检索和生成——用户可以窥见其过程。检索组件允许检查哪些外部文档或数据点被选择为相关。这提供了可以评估的具体证据或参考，以理解响应的基础。追溯模型答案到特定数据源的能力在需要高度责任感或需要验证生成内容准确性的应用中非常宝贵。
- en: In essence, if transparency and the ability to interpret the underpinnings of
    a model’s responses are priorities, RAG offers a clear advantage. By breaking
    down the response generation into distinct stages and allowing insight into its
    data retrieval, RAG fosters greater trust and understanding in its outputs.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，如果透明性和解释模型响应基础的能力是优先事项，RAG提供了明确的优势。通过将响应生成分解为不同阶段并允许洞察其数据检索，RAG促进了对其输出的更大信任和理解。
- en: Summary
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: Choosing between RAG and finetuning becomes more intuitive when considering
    these dimensions. If we need lean towards accessing external knowledge and valuing
    transparency, RAG is our go-to. On the other hand, if we’re working with stable
    labelled data and aim to adapt the model more closely to specific needs, finetuning
    is the better choice.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑这些维度时，选择RAG和微调变得更为直观。如果我们倾向于访问外部知识并重视透明性，RAG是我们的首选。另一方面，如果我们处理的是稳定的标注数据，并且目标是更紧密地调整模型以满足特定需求，微调则是更好的选择。
- en: '![](../Images/d7d7d86c38b86d9425b094c82fab1746.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d7d7d86c38b86d9425b094c82fab1746.png)'
- en: Image by author
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: In the following section, we’ll see how we can assess popular LLM use cases
    based on these criteria.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将看到如何根据这些标准评估流行的LLM使用案例。
- en: Use cases
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用案例
- en: 'Let’s look at some popular use cases and how the above framework can be used
    to choose the right method:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些流行的使用案例，以及上述框架如何用于选择正确的方法：
- en: '**Summarisation (in a specialised domain and/or a specific style)**'
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**总结（在专业领域和/或特定风格中）**'
- en: '**1\. External knowledge required?** For the task of summarizing in the style
    of previous summaries, the primary data source would be the previous summaries
    themselves. If these summaries are contained within a static dataset, there’s
    little need for continuous external data retrieval. However, if there’s a dynamic
    database of summaries that frequently updates and the goal is to continually align
    the style with the newest entries, RAG might be useful here.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 是否需要外部知识？** 对于以之前的总结风格进行总结的任务，主要数据来源将是之前的总结本身。如果这些总结包含在静态数据集中，则无需持续的外部数据检索。然而，如果存在一个动态的总结数据库，经常更新且目标是不断地将风格与最新条目对齐，RAG可能会在这里发挥作用。'
- en: '**2\. Model adaptation required?** The core of this use case revolves around
    adapting to a specialised domain or a and/or a specific writing style. Finetuning
    is particularly adept at capturing stylistic nuances, tonal variations, and specific
    domain vocabularies, making it an optimal choice for this dimension.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 是否需要模型适应？** 这个用例的核心在于适应专业领域和/或特定写作风格。微调特别擅长捕捉风格细微差别、语调变化和特定领域的词汇，使其成为这一维度的最佳选择。'
- en: '**3\. Crucial to minimise hallucinations?** Hallucinations are problematic
    in most LLM applications, including summarisation. However, in this use case,
    the text to be summarised is typically provided as context. This makes hallucinations
    less of a concern compared to other use cases. The source text constrains the
    model, reducing imaginative fabrications. So while factual accuracy is always
    desirable, suppressing hallucinations is a lower priority for summarisation given
    the contextual grounding.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. 是否至关重要的是最小化虚构？** 在大多数大型语言模型应用中，包括总结，虚构都是一个问题。然而，在这个用例中，被总结的文本通常作为上下文提供。这使得虚构问题相较于其他用例的关注度较低。源文本限制了模型，减少了虚构的可能性。因此，尽管事实准确性总是值得追求，但在总结中压制虚构的优先级较低，因为有上下文作为基础。'
- en: '**4\. Training data available?** If there’s a substantial collection of previous
    summaries that are labelled or structured in a way that the model can learn from
    them, finetuning becomes a very attractive option. On the other hand, if the dataset
    is limited, and we’re leaning on external databases for stylistic alignment, RAG
    could play a role, although its primary strength isn’t style adaptation.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**4\. 是否有训练数据？** 如果有大量标记或结构化的先前总结，模型可以从中学习，那么微调将是一个非常有吸引力的选择。另一方面，如果数据集有限，而我们依赖外部数据库来进行风格对齐，RAG
    可能会发挥作用，尽管它的主要强项不是风格适应。'
- en: '**5\. How dynamic is the data?** If the database of previous summaries is static
    or updates infrequently, the finetuned model’s knowledge will likely remain relevant
    for a longer time. However, if the summaries update frequently and there’s a need
    for the model to align with the newest stylistic changes continuously, RAG might
    have an edge due to its dynamic data retrieval capabilities.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**5\. 数据的动态性如何？** 如果先前总结的数据库是静态的或更新不频繁，那么微调模型的知识可能会在较长时间内保持相关。然而，如果总结更新频繁，并且需要模型不断地与最新的风格变化保持一致，RAG
    可能由于其动态数据检索能力而具有优势。'
- en: '**6\. Transparency/Interpretability required?** The primary goal here is stylistic
    alignment, so the “why” behind a particular summarisation style might be less
    critical than in other use cases. That said, if there’s a need to trace back and
    understand which previous summaries influenced a particular output, RAG offers
    a bit more transparency. Still, this might be a secondary concern for this use
    case.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**6\. 是否需要透明性/可解释性？** 这里的主要目标是风格上的一致性，因此某种特定总结风格背后的“原因”可能不像其他用例那样关键。不过，如果需要追溯并了解哪些先前的总结影响了特定输出，RAG提供了更多的透明度。尽管如此，这对该用例来说可能是次要问题。'
- en: '**Recommendation:** For this use case ***finetuning*** appears to be the more
    fitting choice. The primary objective is stylistic alignment, a dimension where
    finetuning shines. Assuming there’s a decent amount of previous summaries available
    for training, finetuning an LLM would allow for deep adaptation to the desired
    style, capturing the nuances and intricacies of the domain. However, if the summaries
    database is extremely dynamic and there’s value in tracing back influences, considering
    a hybrid approach or leaning towards RAG could be explored.'
  id: totrans-88
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**推荐：** 对于这个用例，***微调*** 似乎是更合适的选择。主要目标是风格一致性，这是微调擅长的一个维度。如果有足够多的先前总结用于训练，微调一个大型语言模型将允许对所需风格进行深度适应，捕捉领域的细微差别和复杂性。然而，如果总结数据库极其动态，并且追溯影响具有价值，可以探索混合方法或倾向于
    RAG。'
- en: Question/answering system on organisational knowledge (i.e. external data)
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于组织知识的问答系统（即外部数据）
- en: '**1\. External knowledge required?** A question/answering system relying on
    organisational knowledge bases inherently requires access to external data, in
    this case, the org’s internal databases and document stores. The system’s effectiveness
    hinges on its ability to tap into and retrieve relevant information from these
    sources to answer queries. Given this, RAG stands out as the more suitable choice
    for this dimension, as it’s designed to augment LLM capabilities by retrieving
    pertinent data from knowledge sources.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 是否需要外部知识？** 依赖于组织知识库的问答系统本质上需要访问外部数据，在这种情况下，即组织的内部数据库和文档存储。系统的有效性取决于其从这些来源中提取和检索相关信息的能力。鉴于此，RAG在这一维度上更为合适，因为它设计用来通过从知识来源中检索相关数据来增强LLM的能力。'
- en: '**2\. Model adaptation required?** Depending on the organization and its field,
    there might be a requirement for the model to align with specific terminologies,
    tones, or conventions. While RAG focuses primarily on information retrieval, finetuning
    can help the LLM adjust its responses to the company’s internal vernacular or
    the nuances of its domain. Thus, for this dimension, depending on the specific
    requirements finetuning might play a role.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 是否需要模型调整？** 根据组织和其领域的不同，模型可能需要与特定术语、语调或惯例保持一致。虽然RAG主要关注信息检索，但微调可以帮助LLM调整其回应，以适应公司的内部用语或其领域的细微差别。因此，在这一维度上，根据具体需求，微调可能会发挥作用。'
- en: '**3\. Crucial to minimise hallucinations?** Hallucinations are a major concern
    in this use case, due to the knowledge-cutoff of LLMs. If the model is unable
    to answer a question based on the data it has been trained on, it will almost
    certainly revert to (partially or entirely) making up a plausible but incorrect
    answer.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. 是否至关重要以减少幻觉？** 幻觉在这种用例中是一个主要关注点，因为LLM的知识截止点。如果模型无法根据其训练数据回答问题，它几乎肯定会（部分或完全）编造一个合理但不正确的答案。'
- en: '**4\. Training data available?** If the organization has a structured and labeled
    dataset of previously answered questions, this can bolster the finetuning approach.
    However, not all internal databases are labeled or structured for training purposes.
    In scenarios where the data isn’t neatly labeled or where the primary focus is
    on retrieving accurate and relevant answers, RAG’s ability to tap into external
    data sources without needing a vast labeled dataset makes it a compelling option.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**4\. 是否有可用的训练数据？** 如果组织有结构化和标记化的历史问答数据集，这可以增强微调方法。然而，并非所有内部数据库都已标记或结构化用于训练目的。在数据未被整齐标记或主要关注于检索准确相关答案的情况下，RAG能够访问外部数据源而无需大量标记数据集，使其成为一个令人信服的选择。'
- en: '**5\. How dynamic is the data?** Internal databases and document stores in
    organisations can be highly dynamic, with frequent updates, changes, or additions.
    If this dynamism is characteristic of the organisation’s knowledge base, RAG offers
    a distinct advantage. It continually queries the external sources, ensuring its
    answers are based on the latest available data. Finetuning would require regular
    retraining to keep up with such changes, which might be impractical.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**5\. 数据的动态性如何？** 组织内部的数据库和文档存储可以是高度动态的，经常更新、更改或添加。如果这种动态性是组织知识库的特征，RAG则具有明显的优势。它不断查询外部来源，确保其回答基于最新的数据。微调则需要定期重新训练以跟上这些变化，这可能不切实际。'
- en: '**6\. Transparency/Interpretability required?** For internal applications,
    especially in sectors like finance, healthcare, or legal, understanding the reasoning
    or source behind an answer can be paramount. Since RAG provides a two-step process
    of retrieval and then generation, it inherently offers a clearer insight into
    which documents or data points influenced a particular answer. This traceability
    can be invaluable for internal stakeholders who might need to validate or further
    investigate the sources of certain answers.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**6\. 是否需要透明度/可解释性？** 对于内部应用，特别是在金融、医疗或法律等行业，理解答案背后的推理或来源可能至关重要。由于RAG提供了检索和生成的两步过程，它本质上提供了更清晰的洞察力，显示了哪些文档或数据点影响了特定的答案。这种可追溯性对内部利益相关者来说非常宝贵，他们可能需要验证或进一步调查某些答案的来源。'
- en: '**Recommendation:** For this use case ***a RAG system*** seems to be the more
    fitting choice. Given the need for dynamic access to the organisation’s evolving
    internal databases and the potential requirement for transparency in the answering
    process, RAG offers capabilities that align well with these needs. However, if
    there’s a significant emphasis on tailoring the model’s linguistic style or adapting
    to domain-specific nuances, incorporating elements of finetuning could be considered.'
  id: totrans-96
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**建议：** 对于这种使用场景，***RAG 系统***似乎是更合适的选择。鉴于需要动态访问组织不断发展的内部数据库以及可能需要回答过程中的透明度，RAG
    提供的功能与这些需求相契合。然而，如果对模型的语言风格或领域特定细节有重大关注，则可以考虑结合微调的元素。'
- en: Customer Support Automation (i.e. automated chatbots or help desk solutions
    providing instant responses to customer inquiries)
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 客户支持自动化（即提供即时响应的自动聊天机器人或帮助台解决方案）
- en: '**1\. External knowledge required?** Customer support often necessitates access
    to external data, especially when dealing with product details, account-specific
    information, or troubleshooting databases. While many queries can be addressed
    with general knowledge, some might require pulling data from company databases
    or product FAQs. Here, RAG’s capability to retrieve pertinent information from
    external sources would be beneficial. However, it’s worth noting that a lot of
    customer support interactions are also based on predefined scripts or knowledge,
    which can be effectively addressed with a finetuned model.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 是否需要外部知识？** 客户支持通常需要访问外部数据，特别是处理产品详细信息、账户特定信息或故障排除数据库时。虽然许多查询可以通过一般知识来解决，但有些可能需要从公司数据库或产品
    FAQ 中提取数据。在这里，RAG 从外部来源检索相关信息的能力将非常有益。然而，值得注意的是，许多客户支持互动也基于预定义的脚本或知识，这些可以通过微调的模型有效解决。'
- en: '**2\. Model adaptation required?** Customer interactions demand a certain tone,
    politeness, and clarity, and might also require company-specific terminologies.
    Finetuning is especially useful for ensuring the LLM adapts to the company’s voice,
    branding, and specific terminologies, ensuring a consistent and brand-aligned
    customer experience.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 是否需要模型适配？** 客户互动需要特定的语调、礼貌和清晰度，并且可能还需要公司特有的术语。微调特别有助于确保语言模型适应公司的声音、品牌和特定术语，从而确保一致且符合品牌的客户体验。'
- en: '**3\. Crucial to minimise hallucinations?** For customer support chatbots,
    avoiding false information is essential to maintain user trust. Finetuning alone
    leaves models prone to hallucinations when faced with unfamiliar queries. In contrast,
    RAG systems suppress fabrications by grounding responses in retrieved evidence.
    This reliance on sourced facts allows RAG chatbots to minimise harmful falsehoods
    and provide users with reliable information where accuracy is vital.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. 是否必须尽量减少幻觉？** 对于客户支持聊天机器人来说，避免虚假信息对于维持用户信任至关重要。仅靠微调会使模型在面对不熟悉的查询时容易出现幻觉。相比之下，RAG
    系统通过基于检索到的证据来抑制虚构。这种对获取事实的依赖使 RAG 聊天机器人能够减少有害的虚假信息，并在准确性至关重要的情况下为用户提供可靠的信息。'
- en: '**4\. Training data available?** If a company has a history of customer interactions,
    this data can be invaluable for finetuning. A rich dataset of previous customer
    queries and their resolutions can be used to train the model to handle similar
    interactions in the future. If such data is limited, RAG can provide a fallback
    by retrieving answers from external sources like product documentation.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**4\. 是否有训练数据可用？** 如果公司有客户互动的历史记录，这些数据对于微调非常宝贵。以前客户查询及其解决方案的丰富数据集可以用来训练模型，以便未来处理类似的互动。如果这样的数据有限，RAG
    可以通过从外部来源（如产品文档）检索答案来提供备用方案。'
- en: '**5\. How dynamic is the data?** Customer support might need to address queries
    about new products, updated policies, or changing service terms. In scenarios
    where the product line up, software versions, or company policies are frequently
    updated, RAG’s ability to dynamically pull from the latest documents or databases
    is advantageous. On the other hand, for more static knowledge domains, finetuning
    can suffice.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**5\. 数据的动态性如何？** 客户支持可能需要处理有关新产品、更新的政策或变化的服务条款的查询。在产品阵容、软件版本或公司政策频繁更新的情况下，RAG
    动态从最新文档或数据库中提取信息的能力具有优势。另一方面，对于更静态的知识领域，微调可能就足够了。'
- en: '**6\. Transparency/Interpretability required?** While transparency is essential
    in some sectors, in customer support, the primary focus is on accurate, fast,
    and courteous responses. However, for internal monitoring, quality assurance,
    or addressing customer disputes, having traceability regarding the source of an
    answer could be beneficial. In such cases, RAG’s retrieval mechanism offers an
    added layer of transparency.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**6\. 是否需要透明度/可解释性？** 虽然在某些领域透明度很重要，但在客户支持中，主要关注的是准确、快速和礼貌的响应。然而，对于内部监控、质量保证或处理客户争议，了解答案来源的可追溯性可能是有益的。在这种情况下，RAG的检索机制提供了额外的透明度。'
- en: '**Recommendation:** For customer support automation a ***hybrid approach***
    might be optimal. Finetuning can ensure that the chatbot aligns with the company’s
    branding, tone, and general knowledge, handling the majority of typical customer
    queries. RAG can then serve as a complementary system, stepping in for more dynamic
    or specific inquiries, ensuring the chatbot can pull from the latest company documents
    or databases and thereby minimising hallucinations. By integrating both approaches,
    companies can provide a comprehensive, timely, and brand-consistent customer support
    experience.'
  id: totrans-104
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**推荐：** 对于客户支持自动化，***混合方法***可能是最佳选择。调整可以确保聊天机器人符合公司的品牌、语气和一般知识，处理大部分典型的客户查询。RAG可以作为补充系统，处理更动态或特定的查询，确保聊天机器人可以从最新的公司文档或数据库中提取信息，从而减少虚假信息的生成。通过整合这两种方法，公司可以提供全面、及时和品牌一致的客户支持体验。'
- en: '![](../Images/e04eeb1b4c15cc73fbee2c7dee229e7c.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e04eeb1b4c15cc73fbee2c7dee229e7c.png)'
- en: Image by author
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 作者图片
- en: Additional aspects to consider
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 需要考虑的额外方面
- en: 'As mentioned above, there are other factors that should be considered when
    deciding between RAG and finetuning (or both). We can’t possibly dive deep into
    them, as all of them are multi-faceted and don’t have clear answers like some
    of the aspects above (for example, if there is no training data the finetuning
    is just simply not possible). But that doesn’t mean we should disregard them:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，决定RAG和调整（或两者结合）时还有其他因素需要考虑。我们无法深入探讨这些因素，因为它们都具有多面性，并没有像上述某些方面那样明确的答案（例如，如果没有训练数据，调整根本不可能）。但这并不意味着我们应忽视它们：
- en: Scalability
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可扩展性
- en: As an organisation grows and its needs evolve, how scalable are the methods
    in question? RAG systems, given their modular nature, might offer more straightforward
    scalability, especially if the knowledge base grows. On the other hand, frequently
    finetuning a model to cater to expanding datasets can be computationally demanding.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 随着组织的成长及需求的演变，所用方法的可扩展性如何？由于RAG系统具有模块化特性，可能提供更直接的可扩展性，尤其是当知识库增长时。另一方面，频繁调整模型以适应扩展的数据集可能计算量巨大。
- en: Latency and Real-time Requirements
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 延迟和实时要求
- en: If the application requires real-time or near-real-time responses, consider
    the latency introduced by each method. RAG systems, which involve retrieving data
    before generating a response, might introduce more latency compared to a finetuned
    LLM that generates responses based on internalised knowledge.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果应用程序需要实时或近实时的响应，请考虑每种方法引入的延迟。RAG系统涉及在生成响应前检索数据，相比于基于内在知识生成响应的调整后的LLM，可能会引入更多的延迟。
- en: Maintenance and Support
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 维护和支持
- en: Think about the long-term. Which system aligns better with the organisation's
    ability to provide consistent maintenance and support? RAG might require upkeep
    of the database and the retrieval mechanism, while finetuning would necessitate
    consistent retraining efforts, especially if the data or requirements change.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 从长远角度考虑。哪个系统更符合组织提供一致维护和支持的能力？RAG可能需要维护数据库和检索机制，而调整则需要持续的重新训练，特别是数据或需求发生变化时。
- en: Robustness and Reliability
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 鲁棒性和可靠性
- en: How robust is each method to different types of inputs? While RAG systems can
    pull from external knowledge sources and might handle a broad array of questions,
    a well finetuned model might offer more consistency in certain domains.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 每种方法对不同类型输入的鲁棒性如何？尽管RAG系统可以从外部知识源中提取信息，可能处理各种问题，而经过良好调整的模型在某些领域可能提供更多的一致性。
- en: Ethical and Privacy Concerns
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理和隐私问题
- en: Storing and retrieving from external databases might raise privacy concerns,
    especially if the data is sensitive. On the other hand, a finetuned model, while
    not querying live databases, might still produce outputs based on its training
    data, which could have its own ethical implications.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 从外部数据库存储和检索数据可能引发隐私问题，特别是当数据敏感时。另一方面，尽管微调模型不查询实时数据库，但它仍可能基于其训练数据产生输出，这可能有其自身的伦理影响。
- en: Integration with Existing Systems
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与现有系统的集成
- en: Organisations might already have certain infrastructure in place. The compatibility
    of RAG or finetuning with existing systems — be it databases, cloud infrastructures,
    or user interfaces — can influence the choice.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 组织可能已经有某些基础设施到位。RAG 或微调与现有系统（无论是数据库、云基础设施还是用户界面）的兼容性可以影响选择。
- en: User Experience
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用户体验
- en: Consider the end-users and their needs. If they require detailed, reference-backed
    answers, RAG could be preferable. If they value speed and domain-specific expertise,
    a finetuned model might be more suitable.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑最终用户及其需求。如果他们需要详细的、基于参考的答案，RAG 可能更为合适。如果他们重视速度和领域特定的专业知识，微调模型可能更适合。
- en: Cost
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成本
- en: Finetuning can get expensive, especially for really large models. But in the
    past few months costs have gone down significantly thanks to parameter efficient
    techniques like [QLoRA](https://github.com/artidoro/qlora). Setting up RAG can
    be a large initial investment — covering the integration, database access, maybe
    even licensing fees — but then there’s also the regular maintenance of that external
    knowledge base to think about.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 微调可能会变得非常昂贵，特别是对于非常大的模型。然而，在过去几个月中，由于像[QLoRA](https://github.com/artidoro/qlora)这样的参数高效技术，成本已大幅下降。设置
    RAG 可能需要大额的初始投资——包括集成、数据库访问，甚至可能还有许可费用——但还需要考虑对外部知识库的定期维护。
- en: Complexity
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 复杂性
- en: Finetuning can get complex quickly. While many providers now offer one-click
    finetuning where we just need to provide the training data, keeping track of model
    versions and ensuring that the new models still perform well across the board
    is challenging. RAG, on the other hand, can also get complex quickly. There’s
    the setup of multiple components, making sure the database stays fresh, and ensuring
    the pieces — like retrieval and generation — fit together just right.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 微调可能会迅速变得复杂。虽然许多提供商现在提供一键微调，只需提供训练数据，但跟踪模型版本并确保新模型在各方面仍表现良好是具有挑战性的。另一方面，RAG
    也可能迅速变得复杂。涉及多个组件的设置，确保数据库保持最新，并确保各个部分——如检索和生成——恰到好处地配合在一起。
- en: Conclusion
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: As we’ve explored, choosing between RAG and finetuning requires a nuanced evaluation
    of an LLM application’s unique needs and priorities. There is no one-size-fits-all
    solution; success lies in aligning the optimisation method with the specific requirements
    of the task. By assessing key criteria — the need for external data, adapting
    model behaviour, training data availability, data dynamics, result transparency,
    and more — organisations can make an informed decision on the best path forward.
    In certain cases, a hybrid approach leveraging both RAG and finetuning may be
    optimal.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所探讨的，选择 RAG 还是微调需要对 LLM 应用的独特需求和优先级进行细致的评估。没有一种放之四海而皆准的解决方案；成功在于将优化方法与任务的具体要求对齐。通过评估关键标准——对外部数据的需求、模型行为的调整、训练数据的可用性、数据动态、结果透明度等——组织可以做出明智的决策，确定最佳前进路径。在某些情况下，利用
    RAG 和微调的混合方法可能是最优的。
- en: The key is avoiding assumptions that one method is universally superior. Like
    any tool, their suitability depends on the job at hand. Misalignment of approach
    and objectives can hinder progress, while the right method accelerates it. As
    an organisation evaluates options for boosting LLM applications, it must resist
    oversimplification and not view RAG and finetuning as interchangeable and choose
    the tool that empowers the model to fulfil its capabilities aligned to the needs
    of the use case. The possibilities these methods unlock are astounding but possibility
    alone isn’t enough — execution is everything. The tools are here — now let’s put
    them to work.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于避免假设某种方法在所有情况下都是优越的。像任何工具一样，它们的适用性取决于具体的任务。方法和目标的不匹配可能会阻碍进展，而正确的方法则会加速进展。在组织评估提升
    LLM 应用的选项时，必须抵制过度简化的倾向，不应将 RAG 和微调视为可以互换的工具，而是要选择能够使模型充分发挥其能力并与用例需求对齐的工具。这些方法所解锁的可能性令人惊叹，但仅有可能性是不够的——执行才是关键。工具已经在这里——现在让我们开始使用它们。
- en: Heiko Hotz
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 海科·霍茨
- en: 👋 Follow me on [Medium](https://heiko-hotz.medium.com/) and [LinkedIn](https://www.linkedin.com/in/heikohotz/)
    to read more about Generative AI, Machine Learning, and Natural Language Processing.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 👋 关注我在[Medium](https://heiko-hotz.medium.com/)和[LinkedIn](https://www.linkedin.com/in/heikohotz/)上的动态，阅读更多关于生成式AI、机器学习和自然语言处理的内容。
- en: 👥 If you’re based in London join one of our [NLP London Meetups](https://www.meetup.com/nlp_london/).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 👥 如果你在伦敦，可以加入我们的[NLP London Meetups](https://www.meetup.com/nlp_london/)。
- en: 📔 My thoughts on AI News on [😇 Naughty Neural](https://naughtyneural.net/).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 📔 我对AI新闻的想法见[😇 Naughty Neural](https://naughtyneural.net/)。
- en: '![](../Images/af8595174a18a825043bfa43302463d2.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af8595174a18a825043bfa43302463d2.png)'
- en: Image by author
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
