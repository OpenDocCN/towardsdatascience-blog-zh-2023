- en: 'GenAI for Better NLP Systems I: A Tool for Generating Synthetic Data'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GenAI æå‡ NLP ç³»ç»Ÿ Iï¼šç”Ÿæˆåˆæˆæ•°æ®çš„å·¥å…·
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/genai-for-better-nlp-systems-i-a-tool-for-generating-synthetic-data-4b862ef3f88a](https://towardsdatascience.com/genai-for-better-nlp-systems-i-a-tool-for-generating-synthetic-data-4b862ef3f88a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/genai-for-better-nlp-systems-i-a-tool-for-generating-synthetic-data-4b862ef3f88a](https://towardsdatascience.com/genai-for-better-nlp-systems-i-a-tool-for-generating-synthetic-data-4b862ef3f88a)
- en: An experiment on using GenAI for generating and augmenting synthetic data using
    Python for Prompt Engineering
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ GenAI ç”Ÿæˆå’Œæ‰©å……åˆæˆæ•°æ®çš„å®éªŒï¼ŒåŸºäº Python çš„æç¤ºå·¥ç¨‹
- en: '[](https://nroy0110.medium.com/?source=post_page-----4b862ef3f88a--------------------------------)[![Nabanita
    Roy](../Images/83ab7766a28c79371ebf9517e1f273d2.png)](https://nroy0110.medium.com/?source=post_page-----4b862ef3f88a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4b862ef3f88a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4b862ef3f88a--------------------------------)
    [Nabanita Roy](https://nroy0110.medium.com/?source=post_page-----4b862ef3f88a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://nroy0110.medium.com/?source=post_page-----4b862ef3f88a--------------------------------)[![Nabanita
    Roy](../Images/83ab7766a28c79371ebf9517e1f273d2.png)](https://nroy0110.medium.com/?source=post_page-----4b862ef3f88a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4b862ef3f88a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4b862ef3f88a--------------------------------)
    [Nabanita Roy](https://nroy0110.medium.com/?source=post_page-----4b862ef3f88a--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4b862ef3f88a--------------------------------)
    Â·7 min readÂ·Sep 29, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4b862ef3f88a--------------------------------)
    Â·é˜…è¯»æ—¶é—´7åˆ†é’ŸÂ·2023å¹´9æœˆ29æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/2c632c9d6ff85d169081e0ad66f249ac.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2c632c9d6ff85d169081e0ad66f249ac.png)'
- en: Photo by [SR](https://unsplash.com/@lemonmelon?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼š[SR](https://unsplash.com/@lemonmelon?utm_source=medium&utm_medium=referral)
    äº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: One of the key challenges of Machine Learning(ML) is unbalanced data and the
    biases they introduce in ML models. With the advent of powerful Generative AI
    (GenAI) models, we can augment imbalanced training data with synthetic data easily,
    especially for Natural Language Processing(NLP) tasks. As a result, we can train
    models using classic ML algorithms for better performances in scenarios where
    Deep Learning models or directly using LLMs are not an option for reasons like
    the cost of computation, memory, availability of infrastructure or model explainability.
    Besides, despite the great efficacy shown by LLMs, we still donâ€™t fully trust
    them. However, we can use LLMs to aid our work as data professionals and overcome
    roadblocks in building NLP systems.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰çš„ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯æ•°æ®ä¸å¹³è¡¡åŠå…¶å¼•å…¥çš„æ¨¡å‹åå·®ã€‚éšç€å¼ºå¤§ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰æ¨¡å‹çš„å‡ºç°ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾åœ°ç”¨åˆæˆæ•°æ®æ¥æ‰©å……ä¸å¹³è¡¡çš„è®­ç»ƒæ•°æ®ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç»å…¸çš„
    ML ç®—æ³•æ¥è®­ç»ƒæ¨¡å‹ï¼Œä»¥åœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹æˆ–ç›´æ¥ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸å¯è¡Œçš„æƒ…å†µä¸‹è·å¾—æ›´å¥½çš„æ€§èƒ½ï¼Œä¾‹å¦‚è®¡ç®—æˆæœ¬ã€å†…å­˜ã€åŸºç¡€è®¾æ–½çš„å¯ç”¨æ€§æˆ–æ¨¡å‹å¯è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼Œå°½ç®¡
    LLMs å±•ç°å‡ºäº†æå¤§çš„æ•ˆèƒ½ï¼Œä½†æˆ‘ä»¬ä»ç„¶æ²¡æœ‰å®Œå…¨ä¿¡ä»»å®ƒä»¬ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨ LLMs æ¥è¾…åŠ©æˆ‘ä»¬çš„æ•°æ®ä¸“ä¸šå·¥ä½œï¼Œå…‹æœåœ¨æ„å»º NLP ç³»ç»Ÿæ—¶é‡åˆ°çš„éšœç¢ã€‚
- en: In this article, I have demonstrated how we can improve model performance for
    minority classes in imbalanced datasets using GenAI and Python to generate synthetic
    data and how we can iteratively engineer prompts to generate the desired outcome.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨ GenAI å’Œ Python ç”Ÿæˆåˆæˆæ•°æ®æ¥æ”¹å–„ä¸å¹³è¡¡æ•°æ®é›†ä¸­å°‘æ•°ç±»çš„æ¨¡å‹æ€§èƒ½ï¼Œä»¥åŠå¦‚ä½•é€šè¿‡è¿­ä»£ä¼˜åŒ–æç¤ºè¯æ¥ç”ŸæˆæœŸæœ›çš„ç»“æœã€‚
- en: The Balancing Act
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¹³è¡¡ä¹‹é“
- en: '![](../Images/086715743b0108b8c9033b978701d277.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/086715743b0108b8c9033b978701d277.png)'
- en: Photo by [Tingey Injury Law Firm](https://unsplash.com/@tingeyinjurylawfirm?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼š[Tingey Injury Law Firm](https://unsplash.com/@tingeyinjurylawfirm?utm_source=medium&utm_medium=referral)
    äº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Long story short, ML models need enough examples to learn patterns and predict
    accurately. If the data contains fewer examples, then the model does not generalise
    and consequently, perform well. In such scenarios, the model can overfit for classes
    that have more examples and underfit for classes with fewer examples. To tackle
    unbalanced data we traditionally use statistical sampling methods like over or
    under-sampling, typically using [SMOTE](https://www.geeksforgeeks.org/ml-handling-imbalanced-data-with-smote-and-near-miss-algorithm-in-python/).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: é•¿è¯çŸ­è¯´ï¼Œæœºå™¨å­¦ä¹ æ¨¡å‹éœ€è¦è¶³å¤Ÿçš„ç¤ºä¾‹æ¥å­¦ä¹ æ¨¡å¼å’Œå‡†ç¡®é¢„æµ‹ã€‚å¦‚æœæ•°æ®åŒ…å«è¾ƒå°‘çš„ç¤ºä¾‹ï¼Œæ¨¡å‹åˆ™æ— æ³•æ³›åŒ–ï¼Œè¡¨ç°ä¹Ÿä¼šè¾ƒå·®ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¨¡å‹å¯èƒ½ä¼šå¯¹æ ·æœ¬è¾ƒå¤šçš„ç±»åˆ«è¿‡æ‹Ÿåˆï¼Œè€Œå¯¹æ ·æœ¬è¾ƒå°‘çš„ç±»åˆ«æ¬ æ‹Ÿåˆã€‚ä¸ºäº†åº”å¯¹ä¸å¹³è¡¡çš„æ•°æ®ï¼Œæˆ‘ä»¬ä¼ ç»Ÿä¸Šä½¿ç”¨ç»Ÿè®¡é‡‡æ ·æ–¹æ³•ï¼Œå¦‚è¿‡é‡‡æ ·æˆ–æ¬ é‡‡æ ·ï¼Œé€šå¸¸ä½¿ç”¨[SMOTE](https://www.geeksforgeeks.org/ml-handling-imbalanced-data-with-smote-and-near-miss-algorithm-in-python/)ã€‚
- en: Why balancing data in NLP is hard?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆå¹³è¡¡ NLP ä¸­çš„æ•°æ®å¾ˆå›°éš¾ï¼Ÿ
- en: There are several examples of classification tasks in NLP where data is imbalanced
    and we have to resort to under-sampling to overcome this challenge. Information
    loss is the fundamental problem with under-sampling. While SMOTE implements efficient
    over-sampling strategies for numerical datasets, it is not suitable for texts
    or any sort of vectorized representations or embeddings for texts. This is because
    numeric datasets can easily be replicated with random sampling strategies, drawing
    from a combination of featuresâ€™ probability distributions. For texts, it is not
    as simple to generate the embedded patterns since it is difficult to capture syntactical
    and semantic diversity using these methods.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ NLP ä¸­ï¼Œæœ‰å‡ ä¸ªåˆ†ç±»ä»»åŠ¡çš„æ•°æ®æ˜¯ä¸å¹³è¡¡çš„ï¼Œæˆ‘ä»¬ä¸å¾—ä¸é‡‡ç”¨æ¬ é‡‡æ ·æ¥å…‹æœè¿™ä¸€æŒ‘æˆ˜ã€‚æ¬ é‡‡æ ·çš„æ ¹æœ¬é—®é¢˜æ˜¯ä¿¡æ¯ä¸¢å¤±ã€‚å°½ç®¡ SMOTE ä¸ºæ•°å€¼æ•°æ®é›†å®æ–½äº†é«˜æ•ˆçš„è¿‡é‡‡æ ·ç­–ç•¥ï¼Œä½†å®ƒä¸é€‚ç”¨äºæ–‡æœ¬æˆ–ä»»ä½•å½¢å¼çš„æ–‡æœ¬å‘é‡åŒ–è¡¨ç¤ºæˆ–åµŒå…¥ã€‚è¿™æ˜¯å› ä¸ºæ•°å€¼æ•°æ®é›†å¯ä»¥é€šè¿‡éšæœºé‡‡æ ·ç­–ç•¥è½»æ¾å¤åˆ¶ï¼Œç»“åˆç‰¹å¾çš„æ¦‚ç‡åˆ†å¸ƒã€‚å¯¹äºæ–‡æœ¬è€Œè¨€ï¼Œç”±äºéš¾ä»¥ä½¿ç”¨è¿™äº›æ–¹æ³•æ•æ‰å¥æ³•å’Œè¯­ä¹‰çš„å¤šæ ·æ€§ï¼Œå› æ­¤ç”ŸæˆåµŒå…¥æ¨¡å¼å¹¶ä¸ç®€å•ã€‚
- en: 'Generative AI: A Modern Tool for Synthetic Data for NLP'
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå¼ AIï¼šç”¨äº NLP çš„ç°ä»£åˆæˆæ•°æ®å·¥å…·
- en: This year, the power of GenAI is unleashed and prompt engineering is revolutionizing
    the way we work, and AI leaders are rethinking their operational strategies. Here
    is a demonstration of how GenAI can fuel NLP systems with synthetic data using
    prompt engineering.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä»Šå¹´ï¼ŒGenAI çš„åŠ›é‡è¢«é‡Šæ”¾ï¼Œæç¤ºå·¥ç¨‹æ­£åœ¨é©æ–°æˆ‘ä»¬çš„å·¥ä½œæ–¹å¼ï¼ŒAI é¢†å¯¼è€…æ­£åœ¨é‡æ–°æ€è€ƒä»–ä»¬çš„æ“ä½œç­–ç•¥ã€‚ä¸‹é¢æ˜¯ GenAI å¦‚ä½•é€šè¿‡æç¤ºå·¥ç¨‹ä¸º NLP
    ç³»ç»Ÿæä¾›åˆæˆæ•°æ®çš„æ¼”ç¤ºã€‚
- en: 'Use-Case: Enriching Emotions Dataset with Synthetic Data'
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ¡ˆä¾‹ï¼šç”¨åˆæˆæ•°æ®ä¸°å¯Œæƒ…æ„Ÿæ•°æ®é›†
- en: '**Background**'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**èƒŒæ™¯**'
- en: 'In my previous blogs on comparing [Multiclass Text Classifications with Keras
    Embedding Layer vs Word2vec embeddings](/multiclass-text-classification-using-keras-to-predict-emotions-a-comparison-with-and-without-word-5ef0a5eaa1a0)
    and [Fine-tuning ANNs using KerasTuner](/how-i-improved-the-performance-of-a-multiclass-text-classifier-using-kerastune-and-other-basic-data-161a22625009),
    I presented models that performed well on the emotions â€” â€˜joyâ€™ and â€˜sadnessâ€™,
    as target classes. This is because they had a sizable number of samples among
    all the other emotions in the dataset. Below is the distribution of the train
    data where clearly the number of examples for â€˜joyâ€™ and â€˜sadnessâ€™ (majority classes)
    are higher than â€˜angerâ€™, â€˜loveâ€™, â€˜surpriseâ€™, and â€˜fearâ€™ (minority classes):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä¹‹å‰å…³äºæ¯”è¾ƒ[ä½¿ç”¨ Keras åµŒå…¥å±‚ä¸ Word2vec åµŒå…¥çš„å¤šåˆ†ç±»æ–‡æœ¬åˆ†ç±»](/multiclass-text-classification-using-keras-to-predict-emotions-a-comparison-with-and-without-word-5ef0a5eaa1a0)å’Œ[ä½¿ç”¨
    KerasTuner å¾®è°ƒ ANNs](/how-i-improved-the-performance-of-a-multiclass-text-classifier-using-kerastune-and-other-basic-data-161a22625009)çš„åšå®¢ä¸­ï¼Œæˆ‘å±•ç¤ºäº†åœ¨æƒ…æ„Ÿâ€”â€˜å–œæ‚¦â€™å’Œâ€˜æ‚²ä¼¤â€™ä½œä¸ºç›®æ ‡ç±»åˆ«â€”ä¸Šè¡¨ç°è‰¯å¥½çš„æ¨¡å‹ã€‚è¿™æ˜¯å› ä¸ºå®ƒä»¬åœ¨æ•°æ®é›†ä¸­æ ·æœ¬æ•°é‡ç›¸å¯¹è¾ƒå¤šã€‚ä»¥ä¸‹æ˜¯è®­ç»ƒæ•°æ®çš„åˆ†å¸ƒï¼Œå…¶ä¸­â€˜å–œæ‚¦â€™å’Œâ€˜æ‚²ä¼¤â€™ï¼ˆå¤šæ•°ç±»åˆ«ï¼‰çš„ç¤ºä¾‹æ•°é‡æ˜æ˜¾é«˜äºâ€˜æ„¤æ€’â€™ã€â€˜çˆ±â€™ã€â€˜æƒŠè®¶â€™å’Œâ€˜ææƒ§â€™ï¼ˆå°‘æ•°ç±»åˆ«ï¼‰ï¼š
- en: From the above figure, we can conclude that this data is highly imbalanced for
    emotions apart from â€˜joyâ€™ and â€˜sadnessâ€™. â€˜angerâ€™ and â€˜fearâ€™ have a similar number
    of samples and the performance of the trained models on this data was slightly
    poorer than â€˜joyâ€™ and â€˜sadnessâ€™. The poorest performance was observed for â€˜loveâ€™
    and â€˜surpriseâ€™ with 78% and 71% f1-scores for the best-chosen model from the exercise.
    Below, is the results on the test dataset for reference.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä¸Šå›¾å¯ä»¥å¾—å‡ºç»“è®ºï¼Œè¿™äº›æ•°æ®åœ¨â€˜å–œæ‚¦â€™å’Œâ€˜æ‚²ä¼¤â€™ä¹‹å¤–çš„æƒ…æ„Ÿä¸Šé«˜åº¦ä¸å¹³è¡¡ã€‚â€˜æ„¤æ€’â€™å’Œâ€˜ææƒ§â€™çš„æ ·æœ¬æ•°é‡ç›¸ä¼¼ï¼Œè€Œåœ¨è¿™äº›æ•°æ®ä¸Šè®­ç»ƒå‡ºçš„æ¨¡å‹è¡¨ç°ç•¥é€Šäºâ€˜å–œæ‚¦â€™å’Œâ€˜æ‚²ä¼¤â€™ã€‚è¡¨ç°æœ€å·®çš„æ˜¯â€˜çˆ±â€™å’Œâ€˜æƒŠè®¶â€™ï¼Œåœ¨æœ€ä½³é€‰æ‹©æ¨¡å‹ä¸­ï¼Œå…¶
    f1 åˆ†æ•°åˆ†åˆ«ä¸º 78% å’Œ 71%ã€‚ä»¥ä¸‹æ˜¯æµ‹è¯•æ•°æ®é›†çš„ç»“æœä¾›å‚è€ƒã€‚
- en: '![](../Images/2a83bd0a20bcd5a2e7e2949e2a61fe7d.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2a83bd0a20bcd5a2e7e2949e2a61fe7d.png)'
- en: 'Confusion Matrix for Model Performance without Augmented Synthetic Data | Image
    source: Author'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ€§èƒ½çš„æ··æ·†çŸ©é˜µï¼ˆæ²¡æœ‰å¢å¼ºåˆæˆæ•°æ®ï¼‰| å›¾ç‰‡æ¥æºï¼šä½œè€…
- en: The best-performing model (notebook [here](https://github.com/royn5618/Medium_Blog_Codes/blob/master/Emotion%20Detection/EmotionClassifier_KerasTuner_2.ipynb))
    was a fine-tuned model *without using word2vec* and directly passing the token
    representation to the Keras embedding layer. Evidently, the precision for â€˜surpriseâ€™
    (class 5) is 84%, which is good but recall is 62%, which is quite low. Higher
    precision means that when the classifier predicts an emotion, it is more likely
    to be correct. In other words, it minimizes false positives. On the other hand,
    higher recall means that the classifier is better at capturing all instances of
    the target emotion, minimizing false negatives. A 62% recall means that 62% of
    all the texts with the emotion â€˜surpriseâ€™ were correctly identified. In real-world
    applications, achieving a balance between precision and recall is recommended,
    prioritizing one over another depending on business objectives. Here, the scores
    for precision and recall is unbalanced.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ç°æœ€å¥½çš„æ¨¡å‹ï¼ˆç¬”è®°æœ¬ [è¿™é‡Œ](https://github.com/royn5618/Medium_Blog_Codes/blob/master/Emotion%20Detection/EmotionClassifier_KerasTuner_2.ipynb)ï¼‰æ˜¯ä¸€ä¸ªç»è¿‡å¾®è°ƒçš„æ¨¡å‹ï¼Œ*æ²¡æœ‰ä½¿ç”¨
    word2vec*ï¼Œè€Œæ˜¯ç›´æ¥å°†ä»¤ç‰Œè¡¨ç¤ºä¼ é€’ç»™ Keras åµŒå…¥å±‚ã€‚æ˜¾ç„¶ï¼Œâ€˜æƒŠè®¶â€™ï¼ˆç±»åˆ« 5ï¼‰çš„ç²¾ç¡®åº¦ä¸º 84%ï¼Œè¿™æ˜¯ä¸é”™çš„ï¼Œä½†å¬å›ç‡ä¸º 62%ï¼Œè¿™ç›¸å½“ä½ã€‚è¾ƒé«˜çš„ç²¾ç¡®åº¦æ„å‘³ç€åˆ†ç±»å™¨é¢„æµ‹æƒ…æ„Ÿæ—¶æ›´æœ‰å¯èƒ½æ˜¯æ­£ç¡®çš„ã€‚æ¢å¥è¯è¯´ï¼Œå®ƒæœ€å°åŒ–äº†å‡é˜³æ€§ã€‚å¦ä¸€æ–¹é¢ï¼Œè¾ƒé«˜çš„å¬å›ç‡æ„å‘³ç€åˆ†ç±»å™¨æ›´æ“…é•¿æ•æ‰ç›®æ ‡æƒ…æ„Ÿçš„æ‰€æœ‰å®ä¾‹ï¼Œæœ€å°åŒ–å‡é˜´æ€§ã€‚62%çš„å¬å›ç‡æ„å‘³ç€62%çš„æ‰€æœ‰å¸¦æœ‰â€˜æƒŠè®¶â€™æƒ…æ„Ÿçš„æ–‡æœ¬è¢«æ­£ç¡®è¯†åˆ«ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå»ºè®®åœ¨ç²¾ç¡®åº¦å’Œå¬å›ç‡ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œæ ¹æ®ä¸šåŠ¡ç›®æ ‡ä¼˜å…ˆè€ƒè™‘å…¶ä¸­ä¸€ä¸ªã€‚åœ¨è¿™é‡Œï¼Œç²¾ç¡®åº¦å’Œå¬å›ç‡çš„åˆ†æ•°ä¸å¹³è¡¡ã€‚
- en: '**Objective: Increase recall for the emotion â€˜surpriseâ€™, which is a minor class
    in this imbalanced dataset, using synthetic data generated by GPT.**'
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**ç›®æ ‡ï¼šæé«˜å¯¹æƒ…æ„Ÿâ€˜æƒŠè®¶â€™çš„å¬å›ç‡ï¼Œè¯¥æƒ…æ„Ÿåœ¨è¿™ä¸ªä¸å¹³è¡¡çš„æ•°æ®é›†ä¸­å±äºå°ç±»ï¼Œä½¿ç”¨ç”± GPT ç”Ÿæˆçš„åˆæˆæ•°æ®ã€‚**'
- en: 2\. Prompt Engineering and Synthetic Data Generation
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. æç¤ºå·¥ç¨‹å’Œåˆæˆæ•°æ®ç”Ÿæˆ
- en: In this experiment, I will be re-writing the 572 samples for â€˜surpriseâ€™ in the
    training data and augment them to increase the total number of samples to 1200
    for that emotion.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªå®éªŒä¸­ï¼Œæˆ‘å°†é‡æ–°ç¼–å†™ 572 ä¸ªâ€˜æƒŠè®¶â€™æ ·æœ¬ï¼Œå¹¶å¯¹å…¶è¿›è¡Œæ‰©å……ï¼Œå°†è¯¥æƒ…æ„Ÿçš„æ ·æœ¬æ€»æ•°å¢åŠ åˆ° 1200 ä¸ªã€‚
- en: '**Prompt Engineering:**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**æç¤ºå·¥ç¨‹ï¼š**'
- en: Prompt engineering is the art of crafting instructions for an AI model that
    allows it to produce the most accurate response. In this use case, I will next
    be crafting a prompt that enables me to generate synthetic data. Not just that,
    I will provide examples and an output structure as well so that I can easily parse
    the synthetic data into a Pandas data frame.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æç¤ºå·¥ç¨‹æ˜¯ä¸º AI æ¨¡å‹åˆ¶ä½œæŒ‡ä»¤çš„è‰ºæœ¯ï¼Œä½¿å…¶èƒ½å¤Ÿäº§ç”Ÿæœ€å‡†ç¡®çš„å›åº”ã€‚åœ¨è¿™ä¸ªç”¨ä¾‹ä¸­ï¼Œæˆ‘å°†æ¥ä¸‹æ¥åˆ¶ä½œä¸€ä¸ªæç¤ºï¼Œä½¿æˆ‘èƒ½å¤Ÿç”Ÿæˆåˆæˆæ•°æ®ã€‚ä¸ä»…å¦‚æ­¤ï¼Œæˆ‘è¿˜å°†æä¾›ç¤ºä¾‹å’Œè¾“å‡ºç»“æ„ï¼Œä»¥ä¾¿æˆ‘å¯ä»¥è½»æ¾åœ°å°†åˆæˆæ•°æ®è§£æåˆ°
    Pandas æ•°æ®æ¡†ä¸­ã€‚
- en: 'This experiment uses the following prompt:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå®éªŒä½¿ç”¨äº†ä»¥ä¸‹æç¤ºï¼š
- en: 'Here are a few lessons learnt about the process of prompting for this experiment:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å…³äºè¿™ä¸ªå®éªŒæç¤ºè¿‡ç¨‹çš„ä¸€äº›ç»éªŒæ•™è®­ï¼š
- en: This is the 15th-ish version of the prompt structure.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ç¬¬åäº”ç‰ˆæœ¬å·¦å³çš„æç¤ºç»“æ„ã€‚
- en: I asked to â€œgenerate X samplesâ€ instead of re-writing. That did not work very
    well since it started being repetitive in the syntactic style of the texts, especially
    after generating the first 15-20 of the synthetic samples, the model stopped generating
    a variety of sentence structures.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘è¦æ±‚â€œç”Ÿæˆ X ä¸ªæ ·æœ¬â€è€Œä¸æ˜¯é‡å†™ã€‚ç”±äºåœ¨ç”Ÿæˆå‰ 15-20 ä¸ªåˆæˆæ ·æœ¬åï¼Œæ¨¡å‹å¼€å§‹é‡å¤ä½¿ç”¨ç›¸åŒçš„å¥æ³•é£æ ¼ï¼Œå®ƒæ²¡æœ‰æ•ˆæœç‰¹åˆ«å¥½ï¼Œæ¨¡å‹åœæ­¢ç”Ÿæˆå„ç§å¥å­ç»“æ„ã€‚
- en: Note how I instructed GPT-4 to remove punctuations and lowercase as well. This
    is useful for text cleaning tasks.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„æˆ‘å¦‚ä½•æŒ‡ç¤º GPT-4 ç§»é™¤æ ‡ç‚¹ç¬¦å·å¹¶è½¬ä¸ºå°å†™ã€‚è¿™å¯¹äºæ–‡æœ¬æ¸…ç†ä»»åŠ¡éå¸¸æœ‰ç”¨ã€‚
- en: I tried saying â€œGenerate text samples with emotion â€˜fearâ€™â€ and GPT-4 conveniently
    added the word â€˜fearâ€™ in all samples. I had to use â€˜underlyingâ€™, or â€˜embeddedâ€™
    emotion in the prompt while I was trying to generate the samples instead of rewriting.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘å°è¯•è¯´â€œç”Ÿæˆå¸¦æœ‰æƒ…æ„Ÿâ€˜ææƒ§â€™çš„æ–‡æœ¬æ ·æœ¬â€ï¼ŒGPT-4 æ–¹ä¾¿åœ°åœ¨æ‰€æœ‰æ ·æœ¬ä¸­æ·»åŠ äº†â€˜ææƒ§â€™ä¸€è¯ã€‚æˆ‘åœ¨å°è¯•ç”Ÿæˆæ ·æœ¬æ—¶ä¸å¾—ä¸ä½¿ç”¨â€˜æ½œåœ¨çš„â€™æˆ–â€˜åµŒå…¥çš„â€™æƒ…æ„Ÿï¼Œè€Œä¸æ˜¯é‡å†™ã€‚
- en: I tried adding â€˜using synonymsâ€™ in the instructions already but in some texts,
    the synonyms did not match well. So I removed that and added the clause on preserving
    the emotion because, in some texts, the emotion in content appeared vague to me.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘å·²ç»å°è¯•åœ¨æŒ‡ä»¤ä¸­æ·»åŠ äº†â€˜ä½¿ç”¨åŒä¹‰è¯â€™ï¼Œä½†åœ¨æŸäº›æ–‡æœ¬ä¸­ï¼ŒåŒä¹‰è¯çš„åŒ¹é…æ•ˆæœä¸å¥½ã€‚æ‰€ä»¥æˆ‘å»æ‰äº†è¿™ä¸ªè¦æ±‚ï¼Œå¹¶æ·»åŠ äº†ä¿æŒæƒ…æ„Ÿçš„æ¡æ¬¾ï¼Œå› ä¸ºåœ¨ä¸€äº›æ–‡æœ¬ä¸­ï¼Œå†…å®¹ä¸­çš„æƒ…æ„Ÿå¯¹æˆ‘æ¥è¯´æ˜¾å¾—æ¨¡ç³Šã€‚
- en: '**Addressing Token Limits and Full Code**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**è§£å†³ä»¤ç‰Œé™åˆ¶å’Œå®Œæ•´ä»£ç **'
- en: 'Like everyone, I have the same token limits for prompting GPT-4\. Hereâ€™s what
    I have done. I *chunk*-ified the dataset and ran through all the data points.
    Here is the full code:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: åƒæ¯ä¸ªäººä¸€æ ·ï¼Œæˆ‘ä¹Ÿæœ‰ç›¸åŒçš„ä»¤ç‰Œé™åˆ¶æ¥æç¤ºGPT-4ã€‚ æˆ‘åšäº†ä»€ä¹ˆå‘¢ï¼Ÿæˆ‘å°†æ•°æ®é›†è¿›è¡Œäº†*åˆ†å—*å¤„ç†ï¼Œå¹¶è¿è¡Œäº†æ‰€æœ‰æ•°æ®ç‚¹ã€‚ä»¥ä¸‹æ˜¯å®Œæ•´çš„ä»£ç ï¼š
- en: 'Here are a few lessons learnt about the process of prompting for this part
    of the experiment:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºè¿™éƒ¨åˆ†å®éªŒçš„æç¤ºè¿‡ç¨‹ï¼Œæˆ‘å­¦åˆ°äº†å‡ ä¸ªç»éªŒæ•™è®­ï¼š
- en: I ran into â€œ503 â€” The engine is currently overloaded, please try again laterâ€
    error without giving the code some resting time using time.sleep()
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘é‡åˆ°äº†â€œ503 â€” å¼•æ“å½“å‰è¿‡è½½ï¼Œè¯·ç¨åå†è¯•â€çš„é”™è¯¯ï¼Œæœªç»™ä»£ç ä¸€äº›ä¼‘æ¯æ—¶é—´ï¼ˆä½¿ç”¨time.sleep()ï¼‰
- en: I instructed the model to produce â€˜;â€™ separated samples so that I can parse
    them easily into a data frame.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘æŒ‡ç¤ºæ¨¡å‹ç”Ÿæˆâ€˜;â€™åˆ†éš”çš„æ ·æœ¬ï¼Œä»¥ä¾¿æˆ‘å¯ä»¥è½»æ¾åœ°å°†å®ƒä»¬è§£æåˆ°æ•°æ®æ¡†ä¸­ã€‚
- en: I placed the above instruction in the first line of the prompt. â€œDo not use
    any punctuationâ€ instruction was placed where it is. This resulted into generating
    the samples without any separation at all. (Rookie mistake I know in ğŸ™„)
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘å°†ä¸Šè¿°è¯´æ˜æ”¾åœ¨äº†æç¤ºçš„ç¬¬ä¸€è¡Œã€‚ â€œä¸è¦ä½¿ç”¨ä»»ä½•æ ‡ç‚¹ç¬¦å·â€çš„è¯´æ˜è¢«æ”¾åœ¨äº†å®ƒæ‰€åœ¨çš„ä½ç½®ã€‚è¿™å¯¼è‡´ç”Ÿæˆçš„æ ·æœ¬å®Œå…¨æ²¡æœ‰åˆ†éš”ã€‚ï¼ˆæ–°æ‰‹é”™è¯¯ï¼Œæˆ‘çŸ¥é“ğŸ™„ï¼‰
- en: 'Now that I straightened the synthetic data generation and augmentation, here
    is the updated result of the tuned classifier (trained [here](https://github.com/royn5618/Medium_Blog_Codes/blob/master/GenAI_4_NLP_Systems/EmotionClassifier_KerasTuner_2.ipynb))
    now:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘å·²ç†é¡ºäº†åˆæˆæ•°æ®ç”Ÿæˆå’Œå¢å¼ºï¼Œè¿™é‡Œæ˜¯è°ƒæ•´åçš„åˆ†ç±»å™¨çš„æ›´æ–°ç»“æœï¼ˆ[è®­ç»ƒäºæ­¤](https://github.com/royn5618/Medium_Blog_Codes/blob/master/GenAI_4_NLP_Systems/EmotionClassifier_KerasTuner_2.ipynb)ï¼‰ï¼š
- en: '![](../Images/bea951e54bd2fe9008dda7e22f5f0147.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bea951e54bd2fe9008dda7e22f5f0147.png)'
- en: 'Confusion Matrix for Model Performance with Augmented Synthetic Data | Image
    source: Author'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¦æœ‰å¢å¼ºåˆæˆæ•°æ®çš„æ¨¡å‹æ€§èƒ½æ··æ·†çŸ©é˜µ | å›¾ç‰‡æ¥æºï¼šä½œè€…
- en: Clearly, the recall has increased from 62% to 79% for the emotion â€˜surpriseâ€™
    (Class 5), but the precision decreased from 84% to 72%. However, having a balance
    in the precision and recall scores is important, which is definitely better than
    the model without the synthetic data. In fact, the f1-score has improved from
    71% to 75% using this strategy. The overall model performance remains the same
    but is likely to be boosted on augmenting other classes as well â€” thatâ€™s for a
    future experimnetation.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¾ç„¶ï¼Œå¯¹äºæƒ…æ„Ÿâ€˜æƒŠè®¶â€™ï¼ˆç±»åˆ«5ï¼‰ï¼Œå¬å›ç‡ä»62%æé«˜åˆ°79%ï¼Œä½†ç²¾ç¡®ç‡ä»84%ä¸‹é™åˆ°72%ã€‚ç„¶è€Œï¼Œå¹³è¡¡ç²¾ç¡®ç‡å’Œå¬å›ç‡æ˜¯é‡è¦çš„ï¼Œè¿™æ˜¾ç„¶æ¯”æ²¡æœ‰åˆæˆæ•°æ®çš„æ¨¡å‹è¦å¥½ã€‚å®é™…ä¸Šï¼Œä½¿ç”¨è¿™ç§ç­–ç•¥ï¼Œf1åˆ†æ•°ä»71%æé«˜åˆ°75%ã€‚æ•´ä½“æ¨¡å‹æ€§èƒ½ä¿æŒä¸å˜ï¼Œä½†åœ¨å¢å¼ºå…¶ä»–ç±»åˆ«æ—¶å¯èƒ½ä¼šå¾—åˆ°æå‡â€”â€”è¿™æ˜¯æœªæ¥çš„å®éªŒæ–¹å‘ã€‚
- en: Conclusion
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: This was a brief demonstration of how we can use GenAI to generate synthetic
    data for NLP-based use cases which otherwise could be a more complex task.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åªæ˜¯ä¸€ä¸ªç®€è¦æ¼”ç¤ºï¼Œè¯´æ˜æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨ç”Ÿæˆå¼AIä¸ºåŸºäºNLPçš„ç”¨ä¾‹ç”Ÿæˆåˆæˆæ•°æ®ï¼Œè¿™åœ¨å…¶ä»–æƒ…å†µä¸‹å¯èƒ½æ˜¯ä¸€ä¸ªæ›´å¤æ‚çš„ä»»åŠ¡ã€‚
- en: There is a crucial challenge here, however. GPT-4 produces data that might induce
    patterns or biases in the texts, leading models to overfit on the patterns in
    synthetic data and consequently impede the classifierâ€™s performance. Therefore,
    quality testing of the generated data and the overall performance is essential.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªå…³é”®çš„æŒ‘æˆ˜ã€‚GPT-4ç”Ÿæˆçš„æ•°æ®å¯èƒ½ä¼šåœ¨æ–‡æœ¬ä¸­å¼•å…¥æ¨¡å¼æˆ–åå·®ï¼Œå¯¼è‡´æ¨¡å‹åœ¨åˆæˆæ•°æ®çš„æ¨¡å¼ä¸Šè¿‡æ‹Ÿåˆï¼Œä»è€Œé˜»ç¢åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚å› æ­¤ï¼Œå¯¹ç”Ÿæˆæ•°æ®å’Œæ•´ä½“æ€§èƒ½çš„è´¨é‡æµ‹è¯•æ˜¯è‡³å…³é‡è¦çš„ã€‚
- en: Despite this limitation, Generative AI is incredibly helpful in expediting the
    generation of synthetic data for balancing classes in imbalanced datasets.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æœ‰è¿™ä¸ªé™åˆ¶ï¼Œç”Ÿæˆå¼AIåœ¨åŠ é€Ÿç”Ÿæˆåˆæˆæ•°æ®ä»¥å¹³è¡¡ä¸å¹³è¡¡æ•°æ®é›†ä¸­çš„ç±»åˆ«æ–¹é¢éå¸¸æœ‰å¸®åŠ©ã€‚
- en: Hope you enjoyed this blog ğŸ™‚. Here are the [Google Colab Notebooks](https://github.com/royn5618/Medium_Blog_Codes/tree/master/GenAI_4_NLP_Systems)
    on my GitHub.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å¸Œæœ›ä½ å–œæ¬¢è¿™ä¸ªåšå®¢ğŸ™‚ã€‚ä»¥ä¸‹æ˜¯æˆ‘çš„GitHubä¸Šçš„[Google Colabç¬”è®°æœ¬](https://github.com/royn5618/Medium_Blog_Codes/tree/master/GenAI_4_NLP_Systems)ã€‚
- en: '*Thanks for visiting!*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ„Ÿè°¢è®¿é—®ï¼*'
- en: '**My Links:** [Medium](https://medium.com/@nroy0110) | [LinkedIn](https://www.linkedin.com/in/nabanita-roy/)
    | [GitHub](https://github.com/royn5618)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**æˆ‘çš„é“¾æ¥ï¼š** [Medium](https://medium.com/@nroy0110) | [LinkedIn](https://www.linkedin.com/in/nabanita-roy/)
    | [GitHub](https://github.com/royn5618)'
