- en: 'Simplifying Transformers: State of the Art NLP Using Words You Understand —
    part 3— Attention'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简化变换器：使用你理解的词汇进行的前沿 NLP — 第三部分 — 注意力机制
- en: 原文：[https://towardsdatascience.com/transformers-part-3-attention-7b95881714df?source=collection_archive---------3-----------------------#2023-08-07](https://towardsdatascience.com/transformers-part-3-attention-7b95881714df?source=collection_archive---------3-----------------------#2023-08-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/transformers-part-3-attention-7b95881714df?source=collection_archive---------3-----------------------#2023-08-07](https://towardsdatascience.com/transformers-part-3-attention-7b95881714df?source=collection_archive---------3-----------------------#2023-08-07)
- en: Deep dive into the core technique of LLMs — attention
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入探讨大型语言模型的核心技术 — 注意力机制
- en: '[](https://medium.com/@chenmargalit?source=post_page-----7b95881714df--------------------------------)[![Chen
    Margalit](../Images/fb37720654b3d1068b448d4d9ad624d5.png)](https://medium.com/@chenmargalit?source=post_page-----7b95881714df--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7b95881714df--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7b95881714df--------------------------------)
    [Chen Margalit](https://medium.com/@chenmargalit?source=post_page-----7b95881714df--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@chenmargalit?source=post_page-----7b95881714df--------------------------------)[![Chen
    Margalit](../Images/fb37720654b3d1068b448d4d9ad624d5.png)](https://medium.com/@chenmargalit?source=post_page-----7b95881714df--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7b95881714df--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7b95881714df--------------------------------)
    [Chen Margalit](https://medium.com/@chenmargalit?source=post_page-----7b95881714df--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff8e6113b0479&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-part-3-attention-7b95881714df&user=Chen+Margalit&userId=f8e6113b0479&source=post_page-f8e6113b0479----7b95881714df---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7b95881714df--------------------------------)
    ·14 min read·Aug 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7b95881714df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-part-3-attention-7b95881714df&user=Chen+Margalit&userId=f8e6113b0479&source=-----7b95881714df---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff8e6113b0479&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-part-3-attention-7b95881714df&user=Chen+Margalit&userId=f8e6113b0479&source=post_page-f8e6113b0479----7b95881714df---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7b95881714df--------------------------------)
    · 14 min read · 2023年8月7日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7b95881714df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-part-3-attention-7b95881714df&user=Chen+Margalit&userId=f8e6113b0479&source=-----7b95881714df---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7b95881714df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-part-3-attention-7b95881714df&source=-----7b95881714df---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7b95881714df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-part-3-attention-7b95881714df&source=-----7b95881714df---------------------bookmark_footer-----------)'
- en: Tansformers have made a serious impact in the field of AI, perhaps in the entire
    world. This architecture is comprised of several components, but as the original
    paper is named “Attention is All You Need”, it’s somewhat evident that the attention
    mechanism holds particular significance. Part 3 of this series will primarily
    concentrate on attention and the functionalities around it that make sure the
    Transformer philharmonic plays well together.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器在人工智能领域，甚至在整个世界中产生了深远的影响。这一架构由多个组件组成，但正如原始论文所称“Attention is All You Need”（注意力机制是你所需要的一切），显然注意力机制具有特别的重要性。本系列的第三部分将主要集中在注意力机制及其周边功能上，以确保变换器的各个部分能够协调运行。
- en: '![](../Images/6d53a5dc36eff54f49b0c3290c3348c4.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d53a5dc36eff54f49b0c3290c3348c4.png)'
- en: Image from the [original paper](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
    by Vaswani, A. et al.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来自 [原始论文](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)，作者：Vaswani,
    A. 等。
- en: Attention
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注意力机制
- en: In the context of Transformers, attention refers to a mechanism that enables
    the model to focus on relevant parts of the input during processing. Image a flashlight
    that shines on specific parts of a sentence, allowing the model to give more to
    less significance depending on context. I believe examples are more effective
    than definitions because they are some kind of brain teaser, providing the brain
    with the possibility to bridge the gaps and comprehend the concept on its own.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Transformers 的背景下，attention 指的是一种机制，使模型在处理过程中能够专注于输入的相关部分。想象一下一个手电筒，它照射到句子的特定部分，允许模型根据上下文给予不同的重视程度。我相信例子比定义更有效，因为它们是一种脑力挑战，提供给大脑弥合空白并自行理解概念的可能性。
- en: 'When presented with the sentence, “The man took the chair and disappeared,”
    you naturally assign varying degrees of importance (e.g. attention) to different
    parts of the sentence. Somewhat surprisingly, if we remove specific words, the
    meaning remains mostly intact: “man took chair disappeared.” Although this version
    is broken English, compared to the original sentence you can still understand
    the essence of the message. Interestingly, three words (“The,” “the,” and “and”)
    account for 43% of the words in the sentence but do not contribute significantly
    to the overall meaning. This observation was probably clear to every Berliner
    who came across my amazing German while living there (one can either learn German
    or be happy, it''s a decision you have to make) but it''s much less apparent to
    ML models.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当面对句子“那个人拿着椅子消失了”时，你自然会对句子的不同部分赋予不同程度的重要性（例如，attention）。有些许令人惊讶的是，如果我们去掉一些特定的词，意思基本上仍然保持不变：“man
    took chair disappeared。”虽然这个版本是断裂的英语，但相比于原句，你仍然可以理解信息的精髓。有趣的是，三个词（“The”，“the”，和“and”）占了句子中43%的词汇，但对整体意义贡献不大。这一观察对每个遇到我惊人德语的柏林人来说可能是显而易见的（你可以选择学习德语或保持快乐，这取决于你自己的决定），但对
    ML 模型来说却不那么明显。
- en: 'In the past, previous architectures like RNNs, (Recurrent Neural Networks)
    faced a significant challenge: they struggled to “remember” words that appeared
    far back in the input sequence, typically beyond 20 words. As you already know,
    these models essentially rely on mathematical operations to process data. Unfortunately,
    the mathematical operations used in earlier architectures were not efficient enough
    to carry word representations adequately into the distant future of the sequence.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 过去，以前的架构如 RNN（递归神经网络）面临一个重大挑战：它们在“记住”出现在输入序列远端的词汇时存在困难，通常是超过 20 个词。如你所知，这些模型本质上依赖数学运算来处理数据。不幸的是，早期架构中使用的数学运算效率不足，无法将词汇表示充分地传递到序列的遥远未来。
- en: This limitation in long-term dependency hindered the ability of RNNs to maintain
    contextual information over extended periods, impacting tasks such as language
    translation or sentiment analysis where understanding the entire input sequence
    is crucial. However, Transformers, with their attention mechanism and self-attention
    mechanisms, address this issue more effectively. They can efficiently capture
    dependencies across long distances in the input, enabling the model to retain
    context and associations even for words that appear much earlier in the sequence.
    As a result, Transformers have become a groundbreaking solution to overcome the
    limitations of previous architectures and have significantly improved the performance
    of various natural language processing tasks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这种长期依赖的限制阻碍了 RNN 在较长时间内保持上下文信息的能力，影响了语言翻译或情感分析等任务，其中理解整个输入序列至关重要。然而，Transformers
    通过其 attention 机制和自注意力机制，更有效地解决了这个问题。它们可以有效地捕捉输入中长距离的依赖关系，使模型能够保留上下文和关联，即使是对于在序列中出现较早的词汇。因此，Transformers
    成为克服以前架构限制的突破性解决方案，并显著提高了各种自然语言处理任务的性能。
- en: To create exceptional products like the advanced chatbots we encounter today,
    it is essential to equip the model with the ability to distinguish between high
    and low-value words and also retain contextual information over long distances
    in the input. The mechanism introduced in the Transformers architecture to address
    these challenges is known as **attention**.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建像我们今天遇到的先进聊天机器人这样的卓越产品，至关重要的是使模型具备区分高价值词汇和低价值词汇的能力，并在输入的长距离中保留上下文信息。Transformers
    架构中引入的机制来应对这些挑战被称为 **attention**。
- en: '*Humans have been developing techniques to discriminate between humans for
    a very long time, but as inspiring as they are, we won’t be using those here.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*人类长期以来一直在开发区分人的技术，尽管这些技术很有启发性，但我们在这里不会使用它们。*'
- en: Dot Product
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 点积
- en: How can a model even theoretically discern the importance of different words?
    When analyzing a sentence, we aim to identify the words that have stronger relationships
    with one another. As words are represented as vectors (of numbers), we need a
    measurement for the similarity between numbers. The mathematical term for measuring
    vector similarity is “Dot Product.” It involves multiplying elements of two vectors
    and producing a scalar value (e.g., 2, 16, -4.43), which serves as a representation
    of their similarity. Machine Learning is grounded in various mathematical operations,
    and among them, the Dot Product holds particular importance. Hence, I’ll take
    the time to elaborate on this concept.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 模型如何从理论上辨别不同单词的重要性？在分析句子时，我们的目标是识别彼此关系更强的单词。由于单词被表示为向量（数字），我们需要一个测量数字相似性的标准。测量向量相似性的数学术语是“点积”。它涉及两个向量的元素相乘，产生一个标量值（例如2、16、-4.43），作为它们相似性的表示。机器学习建立在各种数学操作之上，其中点积具有特别重要的意义。因此，我将花时间详细阐述这个概念。
- en: '***Intuition*** Image we have real representations (embeddings) for 5 words:
    “florida”, “california”, “texas”, “politics” and “truth”. As embeddings are just
    numbers, we can potentially plot them on a graph. However, due to their high dimensionality
    (the number of numbers used to represent the word), which can easily range from
    100 to 1000, we can’t really plot them as they are. We can’t plot a 100-dimensional
    vector on a 2D computer/phone screen. Moreover, The human brain finds it difficult
    to understand something above 3 dimensions. What does a 4-dimensional vector look
    like? I don’t know.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '***直观*** 想象一下我们有5个单词的真实表示（嵌入）：“florida”、“california”、“texas”、“politics”和“truth”。由于嵌入只是数字，我们可以将它们绘制在图表上。然而，由于它们的高维度（用于表示单词的数字数量），可能从100到1000不等，我们不能像现在这样绘制它们。我们无法在二维计算机/手机屏幕上绘制一个100维的向量。此外，人脑很难理解超过3维的事物。四维向量是什么样的？我不知道。'
- en: To overcome this issue, we utilize Principal Component Analysis ([PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)),
    a technique that reduces the number of dimensions. By applying PCA, we can project
    the embeddings onto a 2-dimensional space (x,y coordinates). This reduction in
    dimensions helps us visualize the data on a graph. Although we lose some information
    due to the reduction, hopefully, these reduced vectors will still preserve enough
    similarities to the original embeddings, enabling us to gain insights and comprehend
    relationships among the words.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这个问题，我们使用主成分分析（[PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)），这是一种减少维度的技术。通过应用
    PCA，我们可以将嵌入投影到二维空间（x，y 坐标）。这种维度减少帮助我们在图表上可视化数据。尽管由于减少维度我们会丢失一些信息，但希望这些减少的向量仍能保留足够的相似性，使我们能够洞察并理解单词之间的关系。
- en: These numbers are based on the [GloVe](https://nlp.stanford.edu/projects/glove/)
    Embeddings.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字基于 [GloVe](https://nlp.stanford.edu/projects/glove/) 嵌入。
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You can perhaps notice there is some pattern in the numbers, but we’ll plot
    the numbers to make life easier.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到这些数字中有一些模式，但我们将绘制这些数字以简化处理。
- en: '![](../Images/3789805a1e17369e25f6782a1447c82b.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3789805a1e17369e25f6782a1447c82b.png)'
- en: 5 2D vectors
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 5 个二维向量
- en: In this visualization, we see five 2D vectors (x,y coordinates), representing
    5 different words. As you can see, the plot suggests some words are much more
    related to others.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个可视化中，我们看到五个二维向量（x，y 坐标），代表5个不同的单词。正如你所见，图示表明一些单词彼此之间的关系要比其他单词更加紧密。
- en: '***math*** The mathematical counterpart of visualizing vectors can be expressed
    through a straightforward equation. If you aren’t particularly fond of mathematics
    and recall the authors’ description of the Transformers architecture as a “simple
    network architecture,” you probably think this is what happens to ML people, they
    get weird. It''s probably true, but not in this case, this **is** simple. I’ll
    explain:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '***数学*** 视觉化向量的数学对应物可以通过一个简单的方程来表达。如果你不特别喜欢数学，并且记得作者对 Transformers 架构的描述是一个“简单的网络架构”，你可能会觉得这就是机器学习者的状态，他们变得奇怪。这可能是真的，但在这种情况下，这**确实**很简单。我会解释：'
- en: '![](../Images/9c30ce12b81f66f6150a48bcf1295d33.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c30ce12b81f66f6150a48bcf1295d33.png)'
- en: Dot Product Formula
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 点积公式
- en: 'The symbol ||a|| denotes the magnitude of vector “a,” which represents the
    distance between the origin (point 0,0) and the tip of the vector. The calculation
    for the magnitude is as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 符号 ||a|| 表示向量“a”的大小，代表了原点（点 0,0）和向量尖端之间的距离。计算大小的公式如下：
- en: '![](../Images/99ba69b48b9cdc757d3704272b7abe9b.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99ba69b48b9cdc757d3704272b7abe9b.png)'
- en: Vector magnitude formula
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 向量大小公式
- en: The outcome of this calculation is a number, such as 4, or 12.4.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这个计算的结果是一个数字，例如 4 或 12.4。
- en: Theta (θ), refers to the angle between the vectors (look at the visualization).
    The cosine of theta, denoted as cos(θ), is simply the result of applying the cosine
    function to that angle.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Theta (θ) 指的是向量之间的角度（参见可视化）。θ的余弦，表示为 cos(θ)，是将余弦函数应用于该角度的结果。
- en: '***code*** Using the [GloVe](https://nlp.stanford.edu/projects/glove/) algorithm,
    researchers from Stanford University have generated embeddings for actual words,
    as we discussed earlier. Although they have their specific technique for creating
    these embeddings, the underlying concept remains the same as we talked about [in
    the previous part of the series](https://medium.com/@chenmargalit/transformers-part-2-input-2a8c3a141c7d).
    As an example, I took 4 words, reduced their dimensionality to 2, and then plotted
    their vectors as straightforward x and y coordinates.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '***代码*** 使用[GloVe](https://nlp.stanford.edu/projects/glove/)算法，斯坦福大学的研究人员生成了实际单词的嵌入，正如我们之前讨论的那样。尽管他们有自己创建这些嵌入的具体技术，但基本概念与我们在[系列的上一部分](https://medium.com/@chenmargalit/transformers-part-2-input-2a8c3a141c7d)中讨论的相同。作为一个例子，我选择了4个单词，将它们的维度减少到2，然后将它们的向量绘制为简单的
    x 和 y 坐标。'
- en: To make this process function correctly, downloading the [GloVe](https://nlp.stanford.edu/projects/glove/)
    embeddings is a necessary prerequisite.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个过程正确运行，下载[GloVe](https://nlp.stanford.edu/projects/glove/)词嵌入是一个必要的前提。
- en: '*Part of the code, especially the first box is inspired by some code I’ve seen,
    but I can’t seem to find the source.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*部分代码，特别是第一个框，受到我见过的某些代码的启发，但我找不到来源。'
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We now possess a genuine representation of all 5 words. Our next step is to
    conduct the dot product calculations.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在拥有了5个单词的真实表示。我们的下一步是进行点积计算。
- en: 'Vector magnitude:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 向量的大小：
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Dot Product between two **similar** vectors.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 两个**相似**向量之间的点积。
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Dot Product between two **dissimilar** vectors.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 两个**不相似**向量之间的点积。
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As evident from the dot product calculation, it appears to capture and reflect
    an understanding of similarities between different concepts.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 从点积计算可以看出，它似乎捕捉并反映了不同概念之间的相似性。
- en: Scaled Dot-Product attention
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缩放点积注意力
- en: '***intuition***Now that we have a grasp of Dot Product, we can delve back into
    attention. Particularly, the self-attention mechanism. Using self-attention provides
    the model with the ability to determine the importance of each word, regardless
    of its “physical” proximity to the word. This enables the model to make informed
    decisions based on the contextual relevance of each word, leading to better understanding.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '***直觉*** 现在我们已经掌握了点积，我们可以重新深入关注。特别是自注意力机制。使用自注意力使模型能够确定每个单词的重要性，无论它与当前单词的“物理”接近程度如何。这使得模型能够根据每个单词的上下文相关性做出明智的决策，从而达到更好的理解。'
- en: 'To achieve this ambitious goal, we create 3 matrics composed out of learnable
    (!) parameters, known as Query, Key and Value (Q, K, V). The query matrix can
    be envisioned as a query matrix containing the words the user inquires or asks
    for (e.g. when you ask chatGPT if: “god is available today at 5 p.m.?” that is
    the query). The Key matrix encompasses all other words in the sequence. By computing
    the dot product between these matrices, we get the degree of relatedness between
    each word and the word we are currently examining (e.g., translating, or producing
    the answer to the query).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这个宏大的目标，我们创建了3个由可学习的 (!) 参数组成的矩阵，称为查询矩阵、键矩阵和值矩阵（Q, K, V）。查询矩阵可以被看作是包含用户询问或请求的单词的矩阵（例如，当你问
    chatGPT：“god is available today at 5 p.m.?” 这就是查询）。键矩阵包含了序列中的所有其他单词。通过计算这些矩阵之间的点积，我们可以得到每个单词与当前正在检查的单词（例如，翻译或生成查询答案）之间的相关度。
- en: The Value Matrix provides the “clean” representation for every word in the sequence.
    Why do I refer to it as clean where the other two matrices are formed in a similar
    manner? because the value matrix remains in its original form, we don’t use it
    after multiplication by another matrix or normalize it by some value. This distinction
    sets the Value matrix apart, ensuring that it preserves the original embeddings,
    free from additional computations or transformations.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 值矩阵提供了序列中每个词的“干净”表示。为什么我称其为干净，而其他两个矩阵以类似方式形成？因为值矩阵保持原始形式，我们不在另一个矩阵乘法后使用它，也不通过某些值对其进行归一化。这一区别使值矩阵独特，确保它保持原始嵌入，免于额外的计算或转换。*
- en: All 3 matrices are constructed with a size of word_embedding (512). However,
    they are divided into “heads”. In the paper the authors used 8 heads, resulting
    in each matrix having a size of sequence_length by 64\. You might wonder why the
    same operation is performed 8 times with 1/8 of the data and not once with all
    the data. The rationale behind this approach is that by conducting the same operation
    8 times with 8 different sets of weights (which are as mentioned, learnable),
    we can exploit the inherent diversity in the data. Each head can focus on a specific
    aspect within the input and in aggregate, this can lead to better performance.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 所有3个矩阵的大小都为word_embedding（512）。然而，它们被划分为“头”。在论文中，作者使用了8个头，导致每个矩阵的大小为sequence_length乘以64。你可能会好奇，为什么同样的操作要对1/8的数据进行8次，而不是对所有数据进行一次。这种方法的理论依据是，通过用8个不同的权重集（如前所述，可学习的）进行8次相同的操作，我们可以利用数据中的固有多样性。每个头可以关注输入中的特定方面，总体上，这可以带来更好的性能。*
- en: '*In most implementations we don''t really divide the main matrix to 8\. The
    division is achieved through indexing, allowing parallel processing for each part.
    However, these are just implementation details. Theoretically, we could’ve done
    pretty much the same using 8 matrices.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*在大多数实现中，我们实际上并不会将主矩阵划分为8个部分。分割是通过索引实现的，从而允许每个部分进行并行处理。然而，这些只是实现细节。从理论上讲，我们也可以使用8个矩阵来实现相同的功能。*'
- en: The Q and K are multiplied (dot product) and then normalized by the square root
    of the number of dimensions. We pass the result through a [Softmax](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.softmax.html)
    function and the result is then multiplied by the matrix V.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Q和K被相乘（点积），然后通过维度数量的平方根进行归一化。我们将结果通过[Softmax](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.softmax.html)函数，然后将结果乘以矩阵V。*
- en: The reason for normalizing the results is that Q and K are matrices that are
    generated somewhat randomly. Their dimensions might be completely unrelated (independent)
    and multiplications between independent matrices might result in very big numbers
    which can harm the learning as I’ll explain later in this part.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正常化结果的原因是Q和K是随机生成的矩阵。它们的维度可能完全不相关（独立），独立矩阵之间的乘法可能会产生非常大的数字，这可能会对学习产生不利影响，稍后我会在这一部分中解释。*
- en: We then use a non-linear transformation named [Softmax](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.softmax.html),
    to make all numbers range between 0 and 1, and sum to 1\. The result is similar
    to a probability distribution (as there are numbers from 0 to 1 that add up to
    1). These numbers exemplify the relevance of every word to every other word in
    the sequence.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用名为[Softmax](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.softmax.html)的非线性变换，将所有数字的范围调整到0到1之间，并使其总和为1。结果类似于概率分布（因为数字从0到1相加为1）。这些数字体现了序列中每个词与其他词的相关性。*
- en: Finally, we multiply the result by matrix V, and lo and behold, we’ve got the
    self-attention score.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将结果乘以矩阵V，结果就是自注意力得分。*
- en: '*The encoder is actually built out of N (in the paper, N=6) identical layers,
    each such layer gets its input from the previous layer and does the same. The
    final layer passes the data both to the Decoder (which we will talk about in a
    later part of this series) and to the upper layers of the Encoder.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*编码器实际上由N（在论文中，N=6）个相同的层构建而成，每一层都从上一层接收输入并进行相同的操作。最终一层将数据传递给解码器（我们将在本系列的后面部分讨论）以及编码器的上层。*'
- en: Here is a visualization of self-attention. It's like groups of friends in a
    classroom. Some people are more connected to some people. Some people aren’t very
    well connected to anyone.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这是自注意力的可视化。它就像课堂上的朋友小组。有些人与某些人联系更紧密，而有些人则与任何人都没有很好地联系。*
- en: '![](../Images/d2c11c6680c5e7272614738a17ac8af6.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2c11c6680c5e7272614738a17ac8af6.png)'
- en: Image from the [original paper](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
    by Vaswani, A. et al.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自Vaswani等人的[原始论文](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
- en: '***math***Thq Q, K and V matrices are derived through a linear transformation
    of the embedding matrix. Linear transformations are important in machine learning,
    and if you have an interest in becoming an ML practitioner, I recommend exploring
    them further. I won''t delve deep, but I will say that linear transformation is
    a mathematical operation that moves a vector (or a matrix) from one space to another
    space. It sounds more complex than it is. Imagine an arrow pointing in one direction,
    and then moving to point 30 degrees to the right. This illustrates a linear transformation.
    There are a few conditions for such an operation to be considered linear but it''s
    not really important for now. The key takeaway is that it retains many of the
    original vector properties.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '***数学*** Q、K和V矩阵是通过对嵌入矩阵进行线性变换得到的。线性变换在机器学习中非常重要，如果你有兴趣成为一名ML从业者，我建议你进一步探索这些内容。我不会深入探讨，但我会说线性变换是一种将向量（或矩阵）从一个空间移动到另一个空间的数学操作。这听起来比实际情况要复杂。想象一下一个箭头指向一个方向，然后转到右边30度。这就是线性变换的示例。这样的操作有几个条件才被认为是线性的，但目前这些不太重要。关键是它保留了许多原始向量的属性。'
- en: 'The entire calculation of the self-attention layers is performed by applying
    the following formula:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力层的整个计算是通过应用以下公式来执行的：
- en: '![](../Images/0fa457644a0388968acb432206b0fe11.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0fa457644a0388968acb432206b0fe11.png)'
- en: Scaled Dot-Product Attention —Image from the [original paper](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
    by Vaswani, A. et al.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放点积注意力 — 图片来自Vaswani等人的[原始论文](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
- en: 'The calculation process unfolds as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 计算过程如下：
- en: 1\. We multiply Q by K transposed (flipped).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 我们将Q乘以转置的K（翻转后的）。
- en: 2\. We divide the result by the square root of the dimensionality of matrix
    K.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 我们将结果除以矩阵K维度的平方根。
- en: '3\. We now have the “attention matrix scores” that describe how similar every
    word is to every other word. We pass every row to a [Softmax](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.softmax.html)
    (a non-linear) transformation. [Softmax](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.softmax.html)
    does three interesting relevant things:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 我们现在有了描述每个单词与其他每个单词相似度的“注意力矩阵分数”。我们将每一行传递给[Softmax](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.softmax.html)（一种非线性）变换。[Softmax](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.softmax.html)
    做了三件有趣的相关事情：
- en: a. It scales all the numbers so they are between 0 and 1.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: a. 它将所有数字缩放到0和1之间。
- en: b. It makes all the numbers sum to 1.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: b. 它使所有数字的总和为1。
- en: c. It accentuates the gaps, making the slightly more important, much more important.
    As a result, we can now easily distinguish the varying degrees to which the model
    perceives the connection between words x1 and x2, x3, x4, and so on.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: c. 它强调了差距，使得稍微重要的变得更加重要。因此，我们现在可以轻松区分模型感知单词x1与x2、x3、x4等之间的连接程度。
- en: 4\. We multiply the score by the V matrix. This is the final result of the self-attention
    operation.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 我们将分数乘以V矩阵。这是自注意力操作的最终结果。
- en: Masking
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 掩码
- en: In the [previous chapter in this series](https://medium.com/@chenmargalit/transformers-part-2-input-2a8c3a141c7d),
    I’ve explained that we employ dummy tokens to treat special occurrences in the
    sentence such as the first word in the sentence, the last word, etc. One of these
    tokens, denoted as <PADDING>, indicates that there is no actual data, and yet
    we need to maintain consistent matrix sizes throughout the entire process. To
    ensure the model comprehends these are dummy tokens and should therefore not be
    considered during the self-attention calculation, we represent these tokens as
    minus infinity (e.g. a very large negative number, e.g. -153513871339). The masking
    values are added to the result of the multiplication between Q by K. [Softmax](https://medium.com/@chenmargalit/transformers-part-2-input-2a8c3a141c7d)
    then turns these numbers into 0\. This allows us to effectively ignore the dummy
    tokens during the attention mechanism while preserving the integrity of the calculations.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在[本系列的前一章](https://medium.com/@chenmargalit/transformers-part-2-input-2a8c3a141c7d)中，我解释了我们如何使用虚拟标记来处理句子中的特殊情况，例如句子的第一个词、最后一个词等。其中一个标记，表示为<PADDING>，指示没有实际数据，但我们需要在整个过程中保持矩阵大小的一致性。为了确保模型理解这些是虚拟标记，因此在自注意力计算过程中不应考虑这些标记，我们将这些标记表示为负无穷（例如一个非常大的负数，例如-153513871339）。掩码值会被加到Q和K的乘法结果上。[Softmax](https://medium.com/@chenmargalit/transformers-part-2-input-2a8c3a141c7d)然后将这些数字转换为0。这使我们能够在注意力机制中有效地忽略虚拟标记，同时保持计算的完整性。
- en: Dropout
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dropout
- en: Following the self-attention layer, a dropout operation is applied. Dropout
    is a regularization technique widely used in Machine Learning. The purpose of
    regularization is to impose constraints on the model during training, making it
    more difficult for the model to rely heavily on specific input details. As a result,
    the model learns more robustly and improves its ability to generalize. The actual
    implementation involves choosing some of the activations (the numbers coming out
    of different layers) randomly, and zeroing them out. In every pass of the same
    layer, different activations will be zeroed out preventing the model from finding
    solutions that are specific to the data it gets. In essence, dropout helps in
    enhancing the model’s ability to handle diverse inputs and making it more difficult
    for the model to be tailored to specific patterns in the data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在自注意力层之后，应用了dropout操作。Dropout是一种在机器学习中广泛使用的正则化技术。正则化的目的是在训练过程中对模型施加约束，使其更难过度依赖特定的输入细节。因此，模型学习得更为稳健，并提高了其泛化能力。实际的实现涉及随机选择一些激活（来自不同层的数字），并将它们置为零。在同一层的每次传递中，不同的激活将被置为零，防止模型找到特定于数据的解决方案。从本质上讲，dropout有助于增强模型处理多样化输入的能力，并使模型更难以适应数据中的特定模式。
- en: Skip connection
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Skip connection
- en: Another important operation done in the Transformer architecture is called Skip
    Connection.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 另一项在Transformer架构中执行的重要操作称为Skip Connection。
- en: '![](../Images/69d5ee94ca9dbdc0b378791faf98e236.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69d5ee94ca9dbdc0b378791faf98e236.png)'
- en: Image from the [original paper](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
    by Vaswani, A. et al.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源于[Vaswani等人的原始论文](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)。
- en: Skip Connection is a way to pass input without subjecting it to any transformation.
    To illustrate, imagine that I report to my manager who reports it to his manager.
    Even with very pure intentions of making the report more useful, the input now
    goes through some modifications when processed by another human (or ML layer).
    In this analogy, the Skip-Connection would be me, reporting straight to my manager’s
    manager. Consequently, the upper manager receives input both through my manager
    (processed data) **and** straight from me (unprocessed). The senior manager can
    then hopefully make a better decision. The rationale behind employing skip connections
    is to address potential issues such as vanishing gradients which I will explain
    in the following section.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Skip Connection是一种传递输入而不进行任何变换的方式。举例来说，想象一下我向我的经理汇报，经理再向他的上司汇报。即使抱着使报告更有用的纯粹意图，当输入经过另一位人（或ML层）处理时，输入也会经历一些修改。在这个类比中，Skip-Connection就是我直接向我的经理的上司汇报。因此，上级经理既接收到经过我的经理处理的数据（处理过的数据）**又**直接来自我（未经处理的数据）。高级经理可以因此做出更好的决策。使用跳跃连接的理由是解决潜在问题，例如消失的梯度，我将在下一节中解释。
- en: Add & Norm Layer
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Add & Norm Layer
- en: '***Intuition***'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '***直觉***'
- en: The “Add & Norm” layer performs addition and normalization. I’ll start with
    addition as it's simpler. Basically, we add the output from the self-attention
    layer to the original input (received from the skip connection). This addition
    is done element-wise (every number to its same positioned number). The result
    is then normalized.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: “Add & Norm”层执行加法和标准化。我将从加法开始，因为它较为简单。基本上，我们将自注意力层的输出加到原始输入上（通过跳跃连接接收到的）。这个加法是逐元素的（每个数值加到其对应位置的数值）。结果然后进行标准化。
- en: The reason we normalize, again, is that each layer performs numerous calculations.
    Multiplying numbers many times can lead to unintended scenarios. For instance,
    if I take a fraction, like 0.3, and I multiply it with another fraction, like
    0.9, I get 0.27 which is smaller than where it started. if I do this many times,
    I might end up with something very close to 0\. This could lead to a problem in
    deep learning called vanishing gradients.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行标准化的原因是，每层进行大量计算。多次相乘的数值可能导致意外情况。例如，如果我取一个分数，如0.3，然后将其与另一个分数，如0.9相乘，我得到0.27，比开始时的数值要小。如果我多次这样做，最终可能会得到非常接近0的数值。这可能导致深度学习中的一个问题，称为梯度消失。
- en: I won’t go too deep right now so this article doesn’t take ages to read, but
    the idea is that if numbers become very close to 0, the model won't be able to
    learn. The basis of modern ML is calculating gradients and adjusting the weights
    using those gradients (and a few other ingredients). If those gradients are close
    to 0, it will be very difficult for the model to learn effectively.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我现在不会深入探讨，以免这篇文章阅读起来很耗时间，但基本思想是，如果数值接近0，模型将无法有效学习。现代机器学习的基础是计算梯度并使用这些梯度（以及其他一些要素）来调整权重。如果这些梯度接近0，模型将很难有效学习。
- en: On the contrary, the opposite phenomenon, called exploding gradients, can occur
    when numbers that are not fractions get multiplied by non-fractions, causing the
    values to become excessively large. As a result, the model faces difficulties
    in learning due to the enormous changes in weights and activations, which can
    lead to instability and divergence during the training process.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，另一种现象叫做梯度爆炸，当非分数的数值被非分数的数值相乘时，会导致值变得极其大。结果，模型在学习过程中由于权重和激活值的巨大变化面临困难，这可能导致训练过程中的不稳定和发散。
- en: ML models are somewhat like a small child, they need protection. One of the
    ways to protect these models from numbers getting too big or too small is normalization.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型有点像小孩子，需要保护。保护这些模型免受数值过大或过小影响的一种方法是标准化。
- en: '***Math***'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '***数学***'
- en: The layer normalization operation looks frightening (as always) but it’s actually
    relatively simple.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 层标准化操作看起来令人害怕（像往常一样），但实际上相对简单。
- en: '![](../Images/439edb2efdd1eca95054cc81e4fa924d.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/439edb2efdd1eca95054cc81e4fa924d.png)'
- en: Image by [Pytorch](https://pytorch.org/), taken from [here](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Pytorch](https://pytorch.org/) 提供，来自 [这里](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)
- en: 'In the layer normalization operation, we follow these simple steps for each
    input:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在层标准化操作中，我们对每个输入执行以下简单步骤：
- en: Subtract its mean from the input.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从输入中减去其均值。
- en: Divide by the square root of the variance and add an epsilon (some tiny number),
    used to avoid division by zero.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除以方差的平方根，并加上一个 epsilon（一个很小的数值），用于避免除以零。
- en: Multiply the resulting score by a learnable parameter called gamma (γ).
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将结果得分乘以一个可学习的参数，称为 gamma (γ)。
- en: Add another learnable parameter called beta (β).
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加另一个可学习的参数，称为 beta (β)。
- en: These steps ensure the mean will be close to 0 and the standard deviation close
    to 1\. The normalization process enhances the training's stability, speed, and
    overall performance.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤确保均值接近0，标准差接近1。标准化过程增强了训练的稳定性、速度和整体性能。
- en: '***Code***'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '***代码***'
- en: '[PRE8]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Summary:'
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结：
- en: At this point, we have a solid understanding of the main inner workings of the
    Encoder. Additionally, we have explored Skip Connections, a purely technical (and
    important) technique in ML that improves the model’s ability to learn.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们对编码器的主要内部工作有了坚实的理解。此外，我们还探讨了跳跃连接，这是一种纯技术（且重要）的机器学习技术，可以提高模型的学习能力。
- en: Although this section is a bit complicated, you have already acquired a substantial
    understanding of the Transformers architecture as a whole. As we progress further
    in the series, this understanding will serve you in understanding the remaining
    parts.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这一部分有些复杂，但你已经对Transformers架构有了相当深入的理解。随着系列的深入，这种理解将帮助你掌握剩下的部分。
- en: Remember, this is the State of the Art in a complicated field. This isn’t easy
    stuff. Even if you still don’t understand everything 100%, well done for making
    this great progress!
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，这在复杂领域中代表了最前沿技术。这不是简单的内容。即使你还不能完全理解所有内容，也为取得这段伟大进步而感到自豪！
- en: The next part will be about a foundational (and simpler) concept in Machine
    Learning, the Feed Forward Neural Network.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分将讨论机器学习中的一个基础（且更简单的）概念——前馈神经网络。
- en: '![](../Images/5c161490b13a6b3464b2ffd894325e1b.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5c161490b13a6b3464b2ffd894325e1b.png)'
- en: Image from the [original paper](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
    by Vaswani, A. et al.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于Vaswani, A.等人的[原始论文](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)。
