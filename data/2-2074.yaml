- en: 'The Ultimate Guide to Training BERT from Scratch: Introduction'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零开始训练 BERT 的终极指南：介绍
- en: 原文：[https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f](https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f](https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f)
- en: 'Demystifying BERT: The definition and various applications of the model that
    changed the NLP landscape.'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 揭开 BERT 的神秘面纱：定义以及改变 NLP 领域的模型的各种应用。
- en: '[](https://dpoulopoulos.medium.com/?source=post_page-----b048682c795f--------------------------------)[![Dimitris
    Poulopoulos](../Images/ce535a1679779f5a2ec8b024e6691e50.png)](https://dpoulopoulos.medium.com/?source=post_page-----b048682c795f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b048682c795f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b048682c795f--------------------------------)
    [Dimitris Poulopoulos](https://dpoulopoulos.medium.com/?source=post_page-----b048682c795f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://dpoulopoulos.medium.com/?source=post_page-----b048682c795f--------------------------------)[![Dimitris
    Poulopoulos](../Images/ce535a1679779f5a2ec8b024e6691e50.png)](https://dpoulopoulos.medium.com/?source=post_page-----b048682c795f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b048682c795f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b048682c795f--------------------------------)
    [Dimitris Poulopoulos](https://dpoulopoulos.medium.com/?source=post_page-----b048682c795f--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b048682c795f--------------------------------)
    ·10 min read·Sep 2, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b048682c795f--------------------------------)
    ·10 分钟阅读·2023年9月2日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f9d976bd1bc99fc0737ffec178ea1be4.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f9d976bd1bc99fc0737ffec178ea1be4.png)'
- en: Photo by [Ryan Wallace](https://unsplash.com/@accrualbowtie?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Ryan Wallace](https://unsplash.com/@accrualbowtie?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '[Part II](/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822)
    and [Part III](/the-ultimate-guide-to-training-bert-from-scratch-prepare-the-dataset-beaae6febfd5)
    of this story are now live.'
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[第二部分](/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822)和[第三部分](/the-ultimate-guide-to-training-bert-from-scratch-prepare-the-dataset-beaae6febfd5)现已上线。'
- en: 'A few weeks ago, I trained and deployed my very own question-answering system
    using Retrieval Augmented Generation (RAG). The goal was to introduce such a system
    over my study notes and create an agent to help me connect the dots. LangChain
    truly shines in these specific types of applications:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 几周前，我使用检索增强生成（RAG）训练并部署了我自己的问答系统。目标是将这种系统应用于我的学习笔记，并创建一个代理来帮助我连接点滴。LangChain
    在这些特定类型的应用中真正表现出色：
- en: 'As the system''s quality blew me away, I couldn’t help but dig deeper to understand
    the wizardry under the hood. One of the features of the RAG pipeline is its ability
    to sift through mountains of information and find the context most relevant to
    a user’s query. It sounds complex but starts with a simple yet powerful process:
    encoding sentences into information-dense vectors.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 由于系统的质量令我惊叹不已，我迫不及待地深入了解其背后的魔法。RAG 流水线的一个特点是能够筛选大量信息，并找到与用户查询最相关的上下文。听起来很复杂，但从一个简单而强大的过程开始：将句子编码成信息密集的向量。
- en: 'The most popular way to create these sentence embeddings for free is none other
    than SBERT, a [sentence transformer](https://www.sbert.net/) built upon the legendary
    BERT encoder. And finally, that brings us to the main object of this series: understanding
    the fascinating world of BERT. What is it? What can you do with it? And the million-dollar
    question: How can you train your very own BERT model from scratch?'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 创建这些句子嵌入的最流行免费方法就是 SBERT，这是一种建立在传奇 BERT 编码器之上的[句子变换器](https://www.sbert.net/)。最后，这也带我们进入了本系列的主要主题：理解
    BERT 的迷人世界。它是什么？你可以用它做什么？还有一个百万美元的问题：如何从零开始训练你自己的 BERT 模型？
- en: We’ll kick things off by demystifying what BERT actually is, delve into its
    objectives and wide-ranging applications, and then move on to the nitty-gritty
    — like preparing datasets, mastering tokenization, understanding key metrics,
    and, finally, the ins and outs of training and evaluating your model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从揭示 BERT 实际上是什么开始，深入探讨其目标和广泛应用，然后转到具体细节——如准备数据集、掌握分词、理解关键指标，最后是训练和评估模型的方方面面。
- en: This series will be highly detailed and technical, featuring code snippets as
    well as links to GitHub repositories. By the end, I’m confident you’ll gain a
    deeper understanding of why BERT is regarded as a legendary model in the field
    of NLP. So, if you share my excitement, grab a colab Notebook, and let’s dive
    in!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本系列将非常详细且技术性强，包含代码片段以及 GitHub 仓库的链接。到最后，我相信你将更深入地理解为什么 BERT 被视为 NLP 领域的传奇模型。因此，如果你和我一样兴奋，拿起一个
    colab Notebook，和我们一起深入了解吧！
- en: '[Learning Rate](https://www.dimpo.me/newsletter?utm_source=medium&utm_medium=article&utm_campaign=bert-intro)
    is a newsletter for those who are curious about the world of ML and MLOps. If
    you want to learn more about topics like this subscribe [here](https://www.dimpo.me/newsletter?utm_source=medium&utm_medium=article&utm_campaign=bert-intro).
    You’ll hear from me on the last Sunday of every month with updates and thoughts
    on the latest MLOps news and articles!'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[学习率](https://www.dimpo.me/newsletter?utm_source=medium&utm_medium=article&utm_campaign=bert-intro)
    是一份针对对机器学习和 MLOps 世界感到好奇的人的新闻通讯。如果你想了解更多这样的主题，可以 [点击这里](https://www.dimpo.me/newsletter?utm_source=medium&utm_medium=article&utm_campaign=bert-intro)
    订阅。你会在每个月的最后一个星期天收到我的更新和对最新 MLOps 新闻和文章的想法！'
- en: The Definition
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义
- en: BERT, which stands for Bidirectional Encoder Representations from Transformers,
    is a revolutionary Natural Language Processing (NLP) model developed by Google
    in 2018 (Michael Rupe, [How the Google BERT Update Changed Keyword Research](https://www.t3seo.com/how-the-google-bert-update-changed-keyword-research/)).
    Its introduction marked a significant advancement in the field, setting new state-of-the-art
    benchmarks across various NLP tasks. For many, this is regarded as the ImageNet
    moment for the field.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: BERT，即双向编码器表示模型，是 Google 于 2018 年开发的一种革命性自然语言处理（NLP）模型（Michael Rupe，[Google
    BERT 更新如何改变关键词研究](https://www.t3seo.com/how-the-google-bert-update-changed-keyword-research/)）。它的引入标志着该领域的重大进步，为各种
    NLP 任务设定了新的最先进基准。对许多人来说，这被视为该领域的 ImageNet 时刻。
- en: 'BERT is pre-trained on a massive amount of data, with one goal: to understand
    what language is and what’s the meaning of context in a document. As a result,
    this pre-trained model can be fine-tuned for specific tasks such as question-answering
    or sentiment analysis.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 在大量数据上进行预训练，目标是理解语言是什么以及文档中的上下文意义是什么。因此，这个预训练模型可以针对特定任务如问答或情感分析进行微调。
- en: So far, so good, but too much theory won’t get us anywhere. Thus, let’s briefly
    go through the architecture of BERT over the next section and then take a closer
    look at how you pre-train such a model and, more importantly, how you can put
    it to good use, using the question-answering use case as a concrete example.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切顺利，但理论太多也无济于事。因此，让我们在下一节中简要回顾一下 BERT 的架构，然后更详细地看看如何预训练这样的模型，更重要的是，如何将其用于实际应用，以问答用例为具体例子。
- en: The Scaffold
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 框架
- en: 'BERT’s architecture is based on the Transformer model, which has been particularly
    influential in the realm of deep learning for NLP tasks. The Transformer model
    itself is made of an encoder-decoder stack, but BERT only uses the encoder:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 的架构基于 Transformer 模型，这在深度学习的 NLP 任务中具有特别的影响力。Transformer 模型本身由编码器-解码器堆栈组成，但
    BERT 只使用编码器：
- en: '![](../Images/6befc9916c917599b5e2f58ceefb7500.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6befc9916c917599b5e2f58ceefb7500.png)'
- en: Bert Architecture — Image by Author
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Bert 架构 — 图片由作者提供
- en: 'Let’s see what each colored block in the architecture signifies:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看架构中每个彩色块代表的含义：
- en: '**Input Embeddings:** Input tokens (words or subwords) are transformed into
    embeddings, which are then fed into the model. BERT combines both token and positional
    embeddings as input.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入嵌入：** 输入标记（单词或子词）被转换为嵌入，然后输入到模型中。BERT 将标记嵌入和位置嵌入结合作为输入。'
- en: '**Positional Encodings:** Since BERT and the underlying Transformer architecture
    do not have any built-in sense of word order (as recurrent models like LSTMs do),
    they incorporate positional encodings to give the model information about the
    position of words in a sequence. In the original Transformer paper, the positional
    encodings were fixed beforehand; however, nowadays, it’s much more common to learn
    these encodings during training.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**位置编码：** 由于 BERT 和基础 Transformer 架构没有内建的词序感（如 LSTM 等递归模型），它们引入位置编码来提供有关词在序列中位置的信息。在原始的
    Transformer 论文中，位置编码是预先固定的；然而，如今，更常见的是在训练过程中学习这些编码。'
- en: '**Attention Mechanism:** One of the primary innovations in the Transformer
    architecture is the “self-attention mechanism”, which allows the model to weigh
    the importance of different words in a sentence relative to a given word, thereby
    capturing context. This is crucial for BERT’s bidirectionality.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**注意力机制：** Transformer 架构中的主要创新之一是“自注意力机制”，它使模型能够根据给定词的上下文来加权句子中不同词的相对重要性，从而捕捉上下文。这对
    BERT 的双向性至关重要。'
- en: '**Feed-Forward Neural Network:** Each Transformer block contains a feed-forward
    neural network that operates independently on each position.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前馈神经网络：** 每个 Transformer 块包含一个前馈神经网络，它在每个位置上独立操作。'
- en: '**Layer Normalization & Residual Connections:** Each sub-layer (like self-attention
    or feed-forward neural network) in the model includes a residual connection around
    it followed by layer normalization. This helps in training deep networks by mitigating
    the vanishing gradient problem.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层归一化与残差连接：** 模型中的每个子层（如自注意力或前馈神经网络）包括一个残差连接，然后是层归一化。这有助于通过减轻梯度消失问题来训练深层网络。'
- en: '**Multiple Stacks:** BERT’s depth is one of its defining characteristics. BERT''s
    “base” version uses 12 stacked Transformer encoders, while the “large” version
    uses 24.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多个堆叠：** BERT 的深度是其定义特征之一。BERT 的“基础”版本使用 12 层堆叠的 Transformer 编码器，而“大型”版本使用
    24 层。'
- en: Now that we have a mental image in our cache let’s proceed to examine more closely
    how we can train such a model.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们在缓存中有了心理图像，让我们进一步探讨如何训练这样的模型。
- en: Going to School
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 上学
- en: 'We usually split the training of BERT into two phases: in the first phase —
    called pre-training — the goal is to teach the model what language is and how
    context changes the meaning of words. In the second phase — called fine-tuning
    — we make it do something actually useful.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常将 BERT 的训练分为两个阶段：第一阶段——称为预训练——目标是教会模型语言是什么以及上下文如何改变词语的意义。在第二阶段——称为微调——我们使模型执行一些实际有用的任务。
- en: Let’s examine the two phases separately, using concrete examples and visuals.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分别探讨这两个阶段，使用具体的示例和视觉效果。
- en: Pre-training
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预训练
- en: 'In pre-training, BERT tries to solve two tasks simultaneously: i) masked language
    model (MLM or “cloze” test) and ii) next-sentence prediction (NSP).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练中，BERT 试图同时解决两个任务：i) 掩码语言模型（MLM 或“完形填空”测试）和 ii) 下一句预测（NSP）。
- en: The word [cloze](https://en.wiktionary.org/wiki/cloze) is derived from closure
    in [Gestalt theory](https://en.wikipedia.org/wiki/Gestalt_psychology) ( “Gestalt
    psychology”).
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 词汇 [cloze](https://en.wiktionary.org/wiki/cloze) 源自 [格式塔理论](https://en.wikipedia.org/wiki/Gestalt_psychology)
    中的闭合（“格式塔心理学”）。
- en: In the first paradigm, random words in a sentence are replaced with a `[MASK]`
    token, and BERT tries to predict the original word from the context. This differs
    from traditional language models, which predict words in a sequence. This is crucial
    for BERT since it is trying to encode every word in a sequence using information
    from both directions, left and right, hence “Bidirectional”.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个范式中，句子中的随机词被替换为 `[MASK]` 标记，BERT 尝试从上下文中预测原始词。这不同于传统语言模型，它们预测序列中的词。这对 BERT
    至关重要，因为它试图利用左右两个方向的信息对序列中的每个词进行编码，因此称为“双向”。
- en: For the second task, BERT takes in two sentences, determining if the second
    sentence follows the first. This helps BERT understand context across sentences.
    Also, this is where segment embeddings become critical, as they allow the model
    to differentiate between the two sentences. A pair of sentences fed into BERT
    for NSP will be assigned different segment embeddings to indicate to the model
    which sentence each token belongs to.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二个任务，BERT 接受两句话，判断第二句话是否跟在第一句话之后。这帮助 BERT 理解跨句子的上下文。此外，这里是段嵌入变得关键的地方，因为它们使模型能够区分这两句话。一对输入到
    BERT 的句子将被分配不同的段嵌入，以指示模型每个词元属于哪个句子。
- en: 'How does BERT learn these two tasks simultaneously? The following figure will
    clarify everything:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: BERT如何同时学习这两个任务？以下图示将会澄清所有问题：
- en: '![](../Images/d1942614f4740405f9162a1f4b02f43d.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1942614f4740405f9162a1f4b02f43d.png)'
- en: BERT pre-training — Image by Author
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: BERT预训练 — 作者提供的图像
- en: 'First, we add two special tokens to our sequence: the `[CLS]` token — for classification
    — and the `[SEP]` token to separate the two sentences. Next, we pass the sequence
    through BERT and obtain one contextualized representation (i.e., embedding) for
    each token. If you’re paying attention, you’ll see two new embeddings: `E_A` and
    `E_B`. These are two new embeddings we learn during training to inform BERT that
    there are two separate sequences in the example. We’ll see the role of these embeddings
    later in the fine-tuning phase.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们在序列中添加两个特殊标记：`[CLS]`标记——用于分类——和`[SEP]`标记，用于分隔两个句子。接下来，我们将序列通过BERT，并为每个标记获得一个上下文化的表示（即嵌入）。如果你仔细观察，你会看到两个新的嵌入：`E_A`和`E_B`。这两个新的嵌入是在训练过程中学习的，用于通知BERT示例中存在两个独立的序列。我们将在微调阶段进一步了解这些嵌入的作用。
- en: 'Here comes the final part: First, we take the embedding of the `[CLS]` token
    and pass it through a new linear layer with two output units to classify it either
    as a positive label (the sentences are related) or a negative one. Take a look
    at the HuggingFace `transformers` library source code on [GitHub](https://github.com/huggingface/transformers/blob/07998ef39926b76d3f6667025535d0859eed61c3/src/transformers/models/bert/modeling_bert.py#L716):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是最终部分：首先，我们获取`[CLS]`标记的嵌入，将其通过一个新的线性层，输出两个单元，用于将其分类为正标签（句子相关）或负标签。请查看HuggingFace
    `transformers`库的源代码：[GitHub](https://github.com/huggingface/transformers/blob/07998ef39926b76d3f6667025535d0859eed61c3/src/transformers/models/bert/modeling_bert.py#L716)。
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s first examine the model’s prediction shape for the Masked ML task. After
    some transformations, the predictions for each token provide a score for every
    word in the vocabulary. So, the first output will be of shape `[1, 50,000]` if
    we assume that we have 50,000 words in our vocabulary. Let’s see that in the [code](https://github.com/huggingface/transformers/blob/07998ef39926b76d3f6667025535d0859eed61c3/src/transformers/models/bert/modeling_bert.py#L706):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们检查模型在Masked ML任务中的预测形状。经过一些变换后，每个标记的预测为词汇表中的每个单词提供一个分数。因此，如果我们假设词汇表中有50,000个单词，则第一个输出的形状将是`[1,
    50,000]`。我们可以在[代码](https://github.com/huggingface/transformers/blob/07998ef39926b76d3f6667025535d0859eed61c3/src/transformers/models/bert/modeling_bert.py#L706)中查看。
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Finally, to compute the total loss, we use cross entropy to compute the loss
    of each task separately, and then we add them [together](https://github.com/huggingface/transformers/blob/07998ef39926b76d3f6667025535d0859eed61c3/src/transformers/models/bert/modeling_bert.py#L1142):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了计算总损失，我们使用交叉熵分别计算每个任务的损失，然后将它们[加在一起](https://github.com/huggingface/transformers/blob/07998ef39926b76d3f6667025535d0859eed61c3/src/transformers/models/bert/modeling_bert.py#L1142)。
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the final section, let’s see how we can fine-tune BERT to produce a question-answering
    model.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一部分，我们来看一下如何微调BERT以生成问答模型。
- en: Graduation
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 毕业
- en: 'Once pre-trained, we can specialize BERT on a specific task using a smaller
    labeled dataset. For example, let’s take a closer look at the Q&A task:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练完成后，我们可以使用较小的标注数据集对BERT进行特定任务的专业化。例如，我们详细查看Q&A任务：
- en: Fine-tuning BERT for question-answering (Q&A) tasks, such as the [Stanford Question
    Answering Dataset (SQuAD)](https://rajpurkar.github.io/SQuAD-explorer/explore/1.1/dev/Super_Bowl_50.html),
    involves adjusting the model to predict the start and end positions of the answer
    in a given passage for a provided question.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 微调BERT以处理问答（Q&A）任务，如[斯坦福问答数据集（SQuAD）](https://rajpurkar.github.io/SQuAD-explorer/explore/1.1/dev/Super_Bowl_50.html)，涉及调整模型以预测给定问题在段落中的答案的开始和结束位置。
- en: Let’s go through the steps to fine-tune BERT for such tasks.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步了解如何对BERT进行微调以处理这些任务。
- en: Dataset preparation
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集准备
- en: Each item in the dataset will typically have a question, a passage (or reference),
    and the start and end positions of the answer within the passage as the label.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中每个项目通常会包含一个问题、一个段落（或参考），以及答案在段落中的开始和结束位置作为标签。
- en: We tokenize the question and the passage into subwords using BERT’s tokenizer,
    separate the question from the passage using the `[SEP]` special token, and start
    the input sequence using the `[CLS]` special token.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用BERT的分词器将问题和段落分词，将问题与段落分隔开，使用`[SEP]`特殊标记，并使用`[CLS]`特殊标记开始输入序列。
- en: Finally, we create a new array, marking the question as segment `A` and the
    reference as segment `B`. We will use this information to add the learned segment
    embeddings later on.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们创建了一个新数组，将问题标记为段`A`，参考标记为段`B`。我们将利用这些信息在稍后添加学到的段嵌入。
- en: '![](../Images/0084e2efd13b31a45c05215b77d84f0f.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0084e2efd13b31a45c05215b77d84f0f.png)'
- en: BERT QnA Dataset Preparation — Image by Author
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: BERT问答数据集准备 — 作者图片
- en: Model modification
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型修改
- en: Although the pre-trained BERT model can output contextualized embeddings for
    each token in a sequence, for Q&A tasks, you’ll need to derive start and end position
    predictions from these embeddings.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管预训练的BERT模型可以输出序列中每个token的上下文嵌入，但对于问答任务，你需要从这些嵌入中推导出开始和结束位置的预测。
- en: 'To this end, we add a dense (fully connected) layer on top of BERT, with two
    output nodes: one for predicting the start position and one for predicting the
    end position of the answer in the [passage](https://github.com/huggingface/transformers/blob/c385de24414e4ec6125ee14c46c128bfe70ecb66/src/transformers/models/bert/modeling_bert.py#L1803).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们在BERT顶部添加了一个全连接层，具有两个输出节点：一个用于预测答案的开始位置，另一个用于预测答案在[passage](https://github.com/huggingface/transformers/blob/c385de24414e4ec6125ee14c46c128bfe70ecb66/src/transformers/models/bert/modeling_bert.py#L1803)中的结束位置。
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Training
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: For each token in the passage, the model will output a score indicating how
    likely that token is the starting point of the answer and another score for how
    likely it is the ending point of the answer.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对于文章中的每个token，模型将输出一个分数，指示该token成为答案起点的可能性，另一个分数指示其成为答案终点的可能性。
- en: '![](../Images/5347c5000f5e9812dc29493591b7d994.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5347c5000f5e9812dc29493591b7d994.png)'
- en: BERT QnA Prediction — Image by Author
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: BERT问答预测 — 作者图片
- en: In the figure above, it seems that the model predicts that the answer starts
    with token `15`, because the model assigned the highest probability to its output
    and ends with token `18`, for the same reason.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，模型似乎预测答案以token `15`开始，因为模型将最高概率分配给该输出，并以token `18`结束，原因相同。
- en: We use a SoftMax function over the entire sequence to get a probability distribution
    for the start and end positions, and the [loss](https://github.com/huggingface/transformers/blob/c385de24414e4ec6125ee14c46c128bfe70ecb66/src/transformers/models/bert/modeling_bert.py#L1875)
    is calculated using the cross entropy between the predictions and the correct
    start and end positions.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在整个序列上使用SoftMax函数，以获得开始和结束位置的概率分布，[损失](https://github.com/huggingface/transformers/blob/c385de24414e4ec6125ee14c46c128bfe70ecb66/src/transformers/models/bert/modeling_bert.py#L1875)通过预测和正确的开始及结束位置之间的交叉熵来计算。
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Finally, we start with the pre-trained BERT weights and use a smaller learning
    rate (e.g., `2e-5` or `3e-5`) since BERT is already pre-trained. Too large a learning
    rate might cause the model to diverge. We fine-tune the model on the Q&A dataset
    for several epochs until the validation performance plateaus or starts decreasing.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们从预训练的BERT权重开始，使用较小的学习率（例如`2e-5`或`3e-5`），因为BERT已经是预训练的。过大的学习率可能导致模型发散。我们在问答数据集上微调模型，进行多个周期，直到验证性能稳定或开始下降。
- en: Conclusion
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: We’ve covered a lot of ground in this comprehensive guide on training BERT from
    scratch. Starting from understanding what BERT is, we delved into its architectural
    intricacies, the logic behind its pre-training and fine-tuning, and even explored
    how to adapt it for a question-answering task. Along the way, we touched on the
    key metrics, the importance of tokenization, and the code snippets essential for
    anyone looking to get their hands dirty in the field of Natural Language Processing.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本全面的指南中，我们涵盖了从零开始训练BERT的许多内容。从了解BERT是什么开始，我们*深入探讨*了其架构复杂性、预训练和微调的逻辑，甚至探索了如何将其调整用于问答任务。在这个过程中，我们触及了关键指标、分词的重要性，以及对于任何想要动手实践自然语言处理领域的人来说，至关重要的代码片段。
- en: The story of BERT, however, doesn’t end here. In the next story, we’ll take
    things from the start and dive deeper into the BERT tokenizer and how it learns
    to split words into subwords. Stay tuned!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，BERT的故事并未就此结束。在下一个故事中，我们将从头开始，更深入地*探讨*BERT的分词器以及它如何学习将单词拆分为子词。敬请关注！
- en: About the Author
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于作者
- en: My name is [Dimitris Poulopoulos](https://www.dimpo.me/?utm_source=medium&utm_medium=article&utm_campaign=bert-intro),
    and I’m a machine learning engineer working for [HPE](https://www.hpe.com/us/en/home.html).
    I have designed and implemented AI and software solutions for major clients such
    as the European Commission, IMF, the European Central Bank, IKEA, Roblox and others.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我的名字是[Dimitris Poulopoulos](https://www.dimpo.me/?utm_source=medium&utm_medium=article&utm_campaign=bert-intro)，我是一名为[HPE](https://www.hpe.com/us/en/home.html)工作的机器学习工程师。我为欧盟委员会、国际货币基金组织、欧洲中央银行、宜家、Roblox等主要客户设计并实施了AI和软件解决方案。
- en: If you are interested in reading more posts about Machine Learning, Deep Learning,
    Data Science, and DataOps, follow me on [Medium](https://towardsdatascience.com/medium.com/@dpoulopoulos/follow),
    [LinkedIn](https://www.linkedin.com/in/dpoulopoulos/), or [@james2pl](https://twitter.com/james2pl)
    on Twitter.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对阅读更多关于机器学习、深度学习、数据科学和数据操作的文章感兴趣，可以在[Medium](https://towardsdatascience.com/medium.com/@dpoulopoulos/follow)、[LinkedIn](https://www.linkedin.com/in/dpoulopoulos/)上关注我，或在Twitter上关注[@james2pl](https://twitter.com/james2pl)。
- en: Opinions expressed are solely my own and do not express the views or opinions
    of my employer.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 所表达的观点完全是我个人的，并不代表我的雇主的观点或意见。
