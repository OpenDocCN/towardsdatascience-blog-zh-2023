- en: The 5 Pillars of Trustworthy LLM Testing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可信赖的LLM测试的5个支柱
- en: 原文：[https://towardsdatascience.com/the-5-pillars-of-trustworthy-llm-testing-846c0b305059?source=collection_archive---------7-----------------------#2023-11-20](https://towardsdatascience.com/the-5-pillars-of-trustworthy-llm-testing-846c0b305059?source=collection_archive---------7-----------------------#2023-11-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-5-pillars-of-trustworthy-llm-testing-846c0b305059?source=collection_archive---------7-----------------------#2023-11-20](https://towardsdatascience.com/the-5-pillars-of-trustworthy-llm-testing-846c0b305059?source=collection_archive---------7-----------------------#2023-11-20)
- en: Part 4 of a study on generative AI usage and testing
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成AI使用与测试研究的第4部分
- en: '[](https://markopolocheno.medium.com/?source=post_page-----846c0b305059--------------------------------)[![Mark
    Chen](../Images/2d51d4e7ab451b55733a018a3d10a0a7.png)](https://markopolocheno.medium.com/?source=post_page-----846c0b305059--------------------------------)[](https://towardsdatascience.com/?source=post_page-----846c0b305059--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----846c0b305059--------------------------------)
    [Mark Chen](https://markopolocheno.medium.com/?source=post_page-----846c0b305059--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://markopolocheno.medium.com/?source=post_page-----846c0b305059--------------------------------)[![Mark
    Chen](../Images/2d51d4e7ab451b55733a018a3d10a0a7.png)](https://markopolocheno.medium.com/?source=post_page-----846c0b305059--------------------------------)[](https://towardsdatascience.com/?source=post_page-----846c0b305059--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----846c0b305059--------------------------------)
    [Mark Chen](https://markopolocheno.medium.com/?source=post_page-----846c0b305059--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F377682c0f342&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-5-pillars-of-trustworthy-llm-testing-846c0b305059&user=Mark+Chen&userId=377682c0f342&source=post_page-377682c0f342----846c0b305059---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----846c0b305059--------------------------------)
    ·9 min read·Nov 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F846c0b305059&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-5-pillars-of-trustworthy-llm-testing-846c0b305059&user=Mark+Chen&userId=377682c0f342&source=-----846c0b305059---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[阅读](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F377682c0f342&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-5-pillars-of-trustworthy-llm-testing-846c0b305059&user=Mark+Chen&userId=377682c0f342&source=post_page-377682c0f342----846c0b305059---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----846c0b305059--------------------------------)
    ·9分钟阅读·2023年11月20日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F846c0b305059&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-5-pillars-of-trustworthy-llm-testing-846c0b305059&user=Mark+Chen&userId=377682c0f342&source=-----846c0b305059---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F846c0b305059&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-5-pillars-of-trustworthy-llm-testing-846c0b305059&source=-----846c0b305059---------------------bookmark_footer-----------)![](../Images/52af4ad1af2e1693c59ec5b6b97b01cf.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F846c0b305059&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-5-pillars-of-trustworthy-llm-testing-846c0b305059&source=-----846c0b305059---------------------bookmark_footer-----------)![](../Images/52af4ad1af2e1693c59ec5b6b97b01cf.png)'
- en: Photo by [Yasmine Duchesne](https://unsplash.com/@y_duchesne?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源 [Yasmine Duchesne](https://unsplash.com/@y_duchesne?utm_source=medium&utm_medium=referral)
    于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Large language models (LLMs) have made their way into the hands of workers in
    various industries, the minds of tech company leaders, and the hearts of students
    in every learning environment. But with great power, and an even greater number
    of model parameters, comes great responsibility. Now, the growing concern among
    companies, professionals, and even students is knowing when and where an LLM fails.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已经进入了各行各业的工作者手中，科技公司领导的思维中，以及每个学习环境中的学生心中。但有了强大的能力和更多的模型参数，就肩负了更大的责任。现在，公司、专业人士甚至学生日益关注的是如何判断LLM的失败时机和场景。
- en: From the perspective of a machine learning engineer at [Kolena](https://www.kolena.io/),
    this article is a continuation in a series of discussions highlighting a testing
    paradigm for LLMs, identifying the failure modes of LLMs at a high level.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 从[Kolena](https://www.kolena.io/)的机器学习工程师的角度来看，本文是系列讨论的延续，突出LLM的测试范式，识别LLM的高层次失败模式。
- en: What makes an LLM untrustworthy?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么使得LLM不可信？
- en: Suppose an LLM receives a doctor’s long and detailed notes about a patient to
    help recommend a suitable medication. However, if the LLM fails to recognize the
    patient’s allergies, an incorrect output from the LLM could lead to life-threatening
    consequences.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个LLM收到了一位医生关于病人的长篇详细笔记以帮助推荐合适的药物。然而，如果LLM未能识别病人的过敏反应，LLM输出的错误信息可能会导致生命危险的后果。
- en: When an LLM makes a mistake or even produces a lie, widely called a hallucination,
    the repercussions can be significant. In one dramatic case featuring Google’s
    LLM Bard, [hallucinations cost the company more than $100 billion](https://www.reuters.com/technology/google-ai-chatbot-bard-offers-inaccurate-information-company-ad-2023-02-08/)!
    Whether the cost is a person’s health or a company’s financials, discovering the
    hallucinations an LLM can produce is crucially important.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当LLM犯错或甚至产生虚假的信息时，通常被称为幻觉，其影响可能非常严重。在一个涉及谷歌LLM Bard的戏剧性案例中，[幻觉让公司损失超过1000亿美元](https://www.reuters.com/technology/google-ai-chatbot-bard-offers-inaccurate-information-company-ad-2023-02-08/)!
    无论成本是一个人的健康还是公司的财务，发现LLM可能产生的幻觉至关重要。
- en: So what is a trustworthy LLM? Under different usage scenarios, users define
    trustworthiness differently–for example, judges would require minimized bias,
    writers want maximized generation quality. The diagrams below depict the significance
    in understanding how LLMs behave. For certain user groups and general applications,
    baseline GPT models could be a great choice, but for more advanced applications,
    basic GPT models may no longer perform effectively. Perhaps it makes more sense
    to create or fine-tune LLMs for specific purposes, and then test them to verify
    that they are trustworthy.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，什么是可信赖的LLM？在不同的使用场景下，用户对可信赖性的定义不同——例如，法官要求最小化偏见，作家则希望最大化生成质量。下面的图示展示了理解LLM行为的重要性。对于某些用户群体和通用应用，基础的GPT模型可能是一个很好的选择，但对于更高级的应用，基础GPT模型可能不再有效。也许更有意义的是为特定目的创建或微调LLM，然后测试它们以验证其可信赖性。
- en: '![](../Images/7b0059d124ce537f54ead3c97edb4b19.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b0059d124ce537f54ead3c97edb4b19.png)'
- en: COMPARISON ACROSS THE 5 PILLARS OF TRUSTWORTHY LLM TESTING
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 可信赖LLM测试的五大支柱对比
- en: 'There are multiple factors used in evaluating overall LLM performance, which
    is not just limited to the hot topic of hallucinations. LLMs can be very flexible,
    so they naturally have multiple failure modes. At a high level, we should consider
    these 5 pillars at a minimum:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 评估整体LLM表现的因素有很多，不仅仅限于幻觉这一热门话题。LLM可以非常灵活，因此它们自然有多种失败模式。从高层次来看，我们至少应考虑这五大支柱：
- en: Hallucination
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 幻觉
- en: Bias
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 偏见
- en: Reasoning
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 推理
- en: Generation Quality
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成质量
- en: Model Mechanics
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型机制
- en: If people carry out ML or LLM testing using software unit testing principles,
    testing against these pillars is easy. For example, developers can identify the
    existence of hallucinations using datasets similar to [TruthfulQA](https://paperswithcode.com/dataset/truthfulqa).
    Generation quality is quantifiable with sentiment analysis or readability metrics.
    Ultimately, the way to gauge LLM performance changes for different purposes, so
    there is a need to set up custom evaluation processes for each pillar.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果人们使用软件单元测试原则进行ML或LLM测试，测试这些支柱是简单的。例如，开发者可以使用类似于[TruthfulQA](https://paperswithcode.com/dataset/truthfulqa)的数据集来识别幻觉的存在。生成质量可以通过情感分析或可读性指标来量化。*最终*，评估LLM表现的方式因目的不同而有所变化，因此需要为每个支柱设置自定义评估流程。
- en: Hallucination
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 幻觉
- en: For certain users, an LLM’s validity and accuracy are of utmost importance.
    Thus, the first pillar of LLM testing is hallucination. A hallucination refers
    to an LLM’s output that does not correlate with real-world facts. It happens because
    LLMs are generative agents that aim to produce impressive and pleasing results.
    If they cannot recall the correct information as context to produce answers, they
    cannot be absolutely truthful, and would augment information to fill in the blanks.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些用户来说，LLM的有效性和准确性至关重要。因此，LLM测试的第一支柱是幻觉。幻觉是指LLM的输出与现实世界事实不符。这种情况发生是因为LLM是生成型代理，旨在产生令人印象深刻和愉悦的结果。如果它们无法回忆正确的信息作为背景来生成答案，它们就不能绝对真实，并会增加信息以填补空白。
- en: Hallucinations are the first pillar of LLM testing because users might not notice
    the untruthfulness of LLMs while performing common NLP tasks or when asking questions
    pertaining to the STEM (science, technology, engineering, and math) fields at
    scale. For niche applications, such as advanced use cases pertaining to the legal
    or medical fields, hallucinations might be more common because pre-trained LLMs
    might struggle to understand jargon and not have sufficient knowledge to perform
    specific tasks.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 幻觉是LLM测试的第一支柱，因为用户在执行常见的自然语言处理任务或在大规模询问与STEM（科学、技术、工程和数学）领域相关的问题时，可能不会注意到LLM的不真实性。对于小众应用，如涉及法律或医疗领域的高级用例，幻觉可能更为常见，因为预训练的LLM可能难以理解术语，并且没有足够的知识来执行特定任务。
- en: '**Examples:**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：**'
- en: '**Hallucination in General Question Answering:** User: Why did Jack and Jill
    go up the hill?'
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**一般问答中的幻觉：** 用户：杰克和吉尔为什么上山？'
- en: 'Model: Jack and Jill went up the hill to drink water.'
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 模型：杰克和吉尔上山是为了喝水。
- en: 'Issue: From the children’s nursery rhyme, Jack and Jill went to fetch water.
    The model claimed that they are going to drink water.'
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 问题：根据儿童童谣，杰克和吉尔去取水。模型声称他们要喝水。
- en: ''
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Hallucination in Math:**'
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**数学中的幻觉：**'
- en: 'User: Is 17077 prime?'
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 用户：17077 是质数吗？
- en: 'Model: No, 17077 is not prime.'
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 模型：不，17077 不是质数。
- en: 'Issue: 17077 is prime.'
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 问题：17077 是质数。
- en: ''
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Hallucination in Scientific Writing:** “Out of the 178 references analyzed
    … 28 references neither turned up on Google search nor had an existing DOI” —
    [PubMed](https://pubmed.ncbi.nlm.nih.gov/37182055/)'
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**科学写作中的幻觉：**“在分析的178个参考文献中……28个参考文献既没有出现在Google搜索中，也没有现有的DOI” — [PubMed](https://pubmed.ncbi.nlm.nih.gov/37182055/)'
- en: 'Issue: ChatGPT makes up non-existent references.'
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 问题：ChatGPT 编造了不存在的参考文献。
- en: Bias
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏差
- en: An enduring challenge that engineers face, and end users alike, is machine learning
    bias. This is a challenge that is evolving alongside the growth of the AI industry.
    A synonym of bias is unfairness, which differs in definition by use case and user.
    Different groups and stakeholders would define what it means to be fair differently.
    For example, should a model distinguish between equity and equality?
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 工程师和最终用户面临的一个持久挑战是机器学习偏差。这是一个随着AI行业的发展而不断演变的挑战。偏差的同义词是不公平，这在不同的使用案例和用户中定义有所不同。不同的群体和利益相关者会对公平的含义有不同的定义。例如，模型是否应该区分公平与平等？
- en: Addressing bias is a significant and self-evident issue in every ML testing
    process, and it’s particularly true for LLMs as they are trained on various internet
    sources. Imagine how many new headlines we would see if GPT-4 suddenly became
    overtly racist and sexist? The ethical issues behind LLMs generating racial, religious,
    gender, political, or social bias points to the need for ongoing research and
    advancement in LLM testing.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 解决偏差是每个机器学习测试过程中一个重要且不言而喻的问题，特别是对于LLM，因为它们是基于各种互联网来源进行训练的。想象一下，如果GPT-4突然变得极端种族主义和性别歧视，我们会看到多少新的头条新闻？LLM生成种族、宗教、性别、政治或社会偏见的伦理问题突显了在LLM测试中需要不断研究和进步。
- en: '**Example:**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：**'
- en: '![](../Images/29749c548c5057c52760fffc200e693b.png)![](../Images/474d8ad5473814d22ad06a2633723078.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29749c548c5057c52760fffc200e693b.png)![](../Images/474d8ad5473814d22ad06a2633723078.png)'
- en: Religious Bias in ChatGPT-3.5
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT-3.5中的宗教偏见
- en: From the above, we see how OpenAI mitigates bias by ensuring that no output
    reflects Christianophobia or Islamophobia, which does exist in the world and may
    exist in its training data. However, we also see that the model output for the
    Christian and Muslim differs, where after the Muslim prompt, ChatGPT essentially
    says let’s not be racist. Since both ChatGPT discussions occurred independently,
    this single example sufficiently shows that perfectly mitigating bias is hard.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述内容中，我们可以看到OpenAI如何通过确保没有输出反映基督教恐惧症或伊斯兰恐惧症来减轻偏见，这些偏见在世界上确实存在，并可能存在于其训练数据中。然而，我们也看到基督教和穆斯林的模型输出有所不同，在穆斯林提示后，ChatGPT基本上表示我们不要表现出种族歧视。由于这两个ChatGPT讨论独立进行，这一个例子足以表明，完全减轻偏见是困难的。
- en: Reasoning
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推理
- en: LLMs can struggle with tasks that require deep understanding of context, which
    is where human experts excel. When LLMs have the ability to reason and logically
    process information, they have a better chance of understanding complex use cases
    and generating accurate and coherent responses. Furthermore, the potential to
    reason can increase usability in various professional domains such as law, health,
    and research.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型在需要深入理解上下文的任务中可能会遇到困难，这正是人类专家擅长的地方。当大型语言模型具备推理和逻辑处理信息的能力时，它们更有可能理解复杂的用例，并生成准确而连贯的回应。此外，推理的潜力可以增加在法律、健康和研究等各种专业领域中的可用性。
- en: It is hard to claim that an LLM’s output is credible or reliable in the absence
    of transparent justification. For instance, in common NLP tasks, an LLM’s reasoning
    capabilities can be the difference between providing an insightful response and
    merely regurgitating retained information. By continuously evaluating and improving
    the reasoning abilities of LLMs, we can ensure they provide improved accuracy
    and reliability for more valuable outputs.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在缺乏透明的解释的情况下，很难声称大型语言模型的输出是可信或可靠的。例如，在常见的自然语言处理任务中，大型语言模型的推理能力可能是提供有洞察力的回应与仅仅重复保留信息之间的差异。通过不断评估和改进大型语言模型的推理能力，我们可以确保它们提供更准确和可靠的输出。
- en: '**Examples:**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：**'
- en: '**Reasoning in Literature:** User: In the nursery rhyme Jack and Jill, why
    do you think they fell down? Explain your reasoning.'
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**文学中的推理：** 用户：在儿歌《杰克与吉尔》中，你认为他们为什么会摔倒？解释你的推理。'
- en: 'Model: […] Here are a few possible interpretations: 1\. Accident: The most
    straightforward interpretation is that Jack and Jill’s fall was purely accidental.
    Like any children, they may have tripped or stumbled while climbing the hill,
    which can happen to anyone, young or old. 2\. Moral Lesson: Some have suggested
    that the rhyme might have been created to teach children a lesson about the consequences
    of disobedience or carelessness. […]'
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 模型：[…] 这里有几种可能的解释：1\. 意外：最直接的解释是杰克和吉尔的摔倒纯属意外。像任何孩子一样，他们在爬坡时可能绊倒或跌倒，这对任何人来说都可能发生，不论年轻或年老。2\.
    道德教训：有人建议，这首儿歌可能是为了教孩子们关于不服从或粗心的后果。 […]
- en: ''
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Reasoning in Math:** User: Is the sum of natural numbers from 1 to n equal
    to n*(n+1)/2? If so, prove it step by step.'
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**数学中的推理：** 用户：1到n的自然数和是否等于n*(n+1)/2？如果是，请逐步证明。'
- en: 'Model: Yes, the sum of natural numbers from 1 to n is indeed equal to n*(n+1)/2\.
    This is a well-known result and can be proven using mathematical induction.'
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 模型：是的，1到n的自然数和确实等于n*(n+1)/2。这是一个众所周知的结果，可以通过数学归纳法证明。
- en: 'Mathematical induction is a proof technique that involves two steps: 1\. Base
    Case: Prove that the statement holds for the smallest possible value of n. 2\.
    Inductive Step: Assume that the statement is true for some arbitrary positive
    integer k and then prove that it must also be true for k+1.'
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数学归纳法是一种证明技巧，包含两个步骤：1\. 基础情况：证明该声明在最小的n值下成立。2\. 归纳步骤：假设该声明对某个任意正整数k成立，然后证明它对k+1也成立。
- en: 'Let’s prove this statement step by step: Base Case (n = 1): When n is 1, we
    need to show that 1*(1+1)/2 = 1\. This is indeed true: 1*(1+1)/2 = 1*2/2 = 2/2
    = 1\. So, the base case is true.'
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 让我们逐步证明这一声明：基础情况（n = 1）：当n为1时，我们需要证明1*(1+1)/2 = 1。这确实是正确的：1*(1+1)/2 = 1*2/2
    = 2/2 = 1。因此，基础情况是成立的。
- en: 'Inductive Step: Assume that the sum of natural numbers from 1 to k is k*(k+1)/2
    for some positive integer k. This is our induction hypothesis (IH). […]'
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 归纳步骤：假设1到k的自然数和是k*(k+1)/2，对于某个正整数k。这是我们的归纳假设（IH）。 […]
- en: Generation Quality
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成质量
- en: 'LLMs must consider generation quality for several reasons:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型必须考虑生成质量，原因有几个：
- en: 1\. *Ethical responsibility*. LLMs are able to generate vast amounts of text,
    and must generate content that meets ethical, moral, and societal standards. Improving
    generation quality helps address concerns such as the presence of toxic, hateful,
    harmful, propagandic, or inappropriate content so that it doesn’t spread unethical
    ideologies or variations of online harassment. For example, an LLM should never
    encourage illegal activity.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. *伦理责任*。LLM 能生成大量文本，并且必须生成符合伦理、道德和社会标准的内容。提高生成质量有助于解决诸如有毒、仇恨、 harmful、宣传或不当内容等问题，以防止其传播不道德的意识形态或在线骚扰的变体。例如，LLM
    不应鼓励非法活动。
- en: 2\. *Privacy and safety*. Training LLMs on vast amounts of data could expose
    them to personal information. Additionally, if the cloud logs usage history, personal
    information can easily be stored directly from prompts if users aren’t careful.
    It’s important for developers to establish compliance with government regulations
    by making sure LLMs have an appropriate level of privacy for individuals while
    helping them keep their personal information safe. For example, an LLM should
    never mistakenly reveal personal information that endangers someone.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. *隐私和安全*。在大量数据上训练 LLM 可能会使其接触到个人信息。此外，如果云端记录使用历史，个人信息可能会直接从提示中存储，如果用户不小心的话。开发者需要确保
    LLM 在保护个人信息安全的同时，遵守政府法规，为个人提供适当的隐私保护。例如，LLM 不应错误地泄露可能危害他人的个人信息。
- en: 3\. *Coherence*. The quality of generated content directly impacts user experience.
    If LLMs even occasionally produce low-quality or incoherent responses, users may
    lose interest in the system. By improving generation quality, LLMs can offer more
    readable, coherent, and useful outputs, which improves their utility for various
    applications such as writing assistance, language translation, or content summarization.
    For example, an LLM should avoid grammatical errors and use a vocabulary appropriate
    to its intended audience.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. *连贯性*。生成内容的质量直接影响用户体验。如果 LLM 即使偶尔生成低质量或不连贯的响应，用户可能会失去对系统的兴趣。通过提高生成质量，LLM
    可以提供更易读、连贯和有用的输出，从而提高其在写作辅助、语言翻译或内容总结等各种应用中的实用性。例如，LLM 应避免语法错误，并使用适合其目标受众的词汇。
- en: In summary, when focusing on improving generation quality for LLMs, it is important
    to uphold ethical standards beyond bias, to maintain privacy and safety, and to
    provide coherent outputs for the best possible user experience.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在专注于提高 LLM 的生成质量时，重要的是要超越偏见，维护隐私和安全，并提供连贯的输出，以获得最佳的用户体验。
- en: Model Mechanics
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型机制
- en: 'The importance of testing an LLM’s mechanics lies in making sure it is adaptable,
    versatile, and broadly applicable. Whether it’s answering questions, translating
    languages, or even coding, the model should seamlessly transition between different
    applications (unless it serves one specific application). Some examples of these
    mechanics are: cost-effectiveness, consistency, and personalization.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 测试 LLM 机制的重要性在于确保其适应性、通用性和广泛适用性。无论是回答问题、翻译语言还是编码，模型都应在不同应用之间无缝过渡（除非它服务于一个特定的应用）。这些机制的一些例子包括：性价比、一致性和个性化。
- en: The most fundamental attribute that users consider is an LLM’s cost-effectiveness.
    LLMs might cost too much money or too much time to produce one outcome, which
    will hinder a model’s approachability especially for tasks at scale. It is worth
    mentioning the cost of training LLMs too, which costs millions of dollars, but
    leaves an enormous environmental footprint. In OpenAI’s case, training GPT-3 generated
    [502 metric tons of carbon](https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf),
    and keeping it accessible to users only grows that footprint over time.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 用户考虑的最基本属性是 LLM 的性价比。LLM 可能花费过多的金钱或时间来产生一个结果，这会影响模型的可用性，尤其是在大规模任务中。值得一提的是训练
    LLM 的成本，这需要数百万美元，同时还会留下巨大的环境足迹。以 OpenAI 为例，训练 GPT-3 产生了 [502 公吨碳](https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf)，并且让其对用户保持可用性只会随着时间的推移增加这个足迹。
- en: In some cases, uniform and reliable responses are essential, which means that
    LLMs should possess consistency. Other times, people treasure creativity. LLM
    creativity is commonly tunable via a parameter called temperature.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，统一和可靠的响应是至关重要的，这意味着大语言模型（LLM）应该具备一致性。而有时候，人们珍视创造力。LLM 的创造力通常通过一个称为温度的参数进行调整。
- en: Personalization is another important mechanic, allowing users to easily steer
    an LLM in the direction it should go for the best final output. On one hand, this
    means that LLMs should be receptive to any query a user makes, properly reflecting
    the user’s demands in various prompts for style, tone, or special instructions.
    People call this ability prompt engineering. On the other hand, going beyond simple
    interactions with a chatbot or interface, LLMs should be easy to update, retrain,
    or fine-tune for any specific application.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 个性化是另一个重要机制，使用户能够轻松地引导LLM朝着最佳最终输出的方向前进。一方面，这意味着LLM应该对用户提出的任何查询都保持敏感，正确反映用户在各种风格、语调或特殊指令上的需求。人们称这种能力为提示工程。另一方面，LLM应当不仅仅限于简单的聊天机器人或界面的互动，还应当易于更新、再培训或微调以适应任何特定应用。
- en: '**Examples:**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：**'
- en: '**Haiku prompt:** User: Make me a haiku about university.'
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**俳句提示：** 用户：给我写一首关于大学的俳句。'
- en: 'Model: Books and lectures hum,'
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 模型：书籍和讲座在低语，
- en: Campus whispers knowledge’s breath,
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 校园低语知识的气息，
- en: Mind blooms in each class.
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 每堂课上心灵绽放。
- en: ''
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Pig Latin prompt:** User: Please summarize the plot of Finding Nemo in one
    sentence in pig latin.'
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**猪拉丁语提示：** 用户：请用猪拉丁语总结《海底总动员》的情节。'
- en: 'Model: Inway ethay oceanway, entlegray ishfay atherfay earchsay orfay ishay
    onlay, emonay, ohway aswhay ostlay omfray ishay addledbray onay, etsgay ishay
    elphay findway imhay.'
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 模型：在海洋中，灰色鱼父寻求他的唯一，寂寞，谁被从他那迷糊的睡眠中遗失，找回他的帮助。
- en: Different applications will have different mechanics of significance. For some,
    interactions via an API of very large bandwidth would be absolutely necessary
    24/7\. Others might need the LLM to retain contextual information from multiple
    messages over time or be reasonably transparent and explainable. With so many
    applications of LLMs, it is important for all users, engineers, and stakeholders,
    to outline the model mechanics that are important to them, ensuring maximum utility
    and effectiveness.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的应用程序具有不同的重要机制。对于某些应用，全天候需要通过具有非常大带宽的API进行交互是绝对必要的。其他应用可能需要LLM从多个消息中保留上下文信息，或在某种程度上透明和可解释。鉴于LLM的众多应用，对所有用户、工程师和利益相关者来说，明确对他们重要的模型机制至关重要，以确保最大效用和效果。
- en: Conclusion
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Responsible AI extends beyond these 5 pillars, but at a high level, all LLMs
    should adhere to some performance standard for hallucinations, bias, reasoning,
    generation quality, and mechanics of usage.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的AI超越了这五大支柱，但从高层次来看，所有LLM都应遵守某些性能标准，包括幻觉、偏见、推理、生成质量和使用机制。
- en: However, everyone defines the significance of certain failure modes differently.
    For example, an LLM designed to operate in the healthcare industry would want
    to emphasize on testing for hallucinations, whereas an LLM assisting a judge in
    court would want to exhibit fairness, or at least exhibit less bias than the human
    judge. Engineers might only care about speed and cost on top of some fundamental
    level of performance.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，每个人对某些失败模式的重要性定义不同。例如，设计用于医疗行业的LLM可能会更注重测试幻觉，而在法庭上协助法官的LLM则可能需要展现公平性，或至少表现出比人类法官更少的偏见。工程师可能只关心速度和成本，同时保持一定的基础性能水平。
- en: It’s an ongoing challenge to make a one-model-for-all LLM that upholds all 5
    pillars of trustworthy LLM testing. Making progress in mechanics might lead to
    a decline in generation quality, but making progress in improving output quality
    could, in turn, slowly reveal hallucinations or biases!
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让一个能够满足所有五大支柱的LLM仍然是一个持续的挑战。提升机制的进展可能导致生成质量下降，但改善输出质量的进展可能会慢慢揭示幻觉或偏见！
- en: It’s up to individual organizations, professionals, end-users, and you, to decide
    what makes an LLM trustworthy! What would you do if you needed to not produce
    hallucinations at the cost of a worsened overall generation quality? Which pillars
    does your LLM lack, and how will you find out?
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 由各个组织、专业人士、终端用户以及你自己来决定什么使LLM值得信赖！如果你需要以降低总体生成质量为代价来避免产生幻觉，你会怎么做？你的LLM缺乏哪些支柱，你将如何发现？
- en: Thanks for reading! Be sure to check out my applied LLM testing findings [here](https://medium.com/@markopolocheno)
    to learn more about [GPT-4 performance](/quantifying-gpt-4s-hidden-regressions-over-time-368d3a16dca),
    [prompt engineering](/how-well-do-gpt-models-follow-prompts-cfc0e0f36e71), and
    [common NLP tasks](/how-to-validate-openai-gpt-model-performance-with-text-summarization-298978fea764).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！请务必查看我在[这里](https://medium.com/@markopolocheno)发布的应用LLM测试发现，以了解更多关于[GPT-4表现](/quantifying-gpt-4s-hidden-regressions-over-time-368d3a16dca)、[提示工程](/how-well-do-gpt-models-follow-prompts-cfc0e0f36e71)和[常见NLP任务](/how-to-validate-openai-gpt-model-performance-with-text-summarization-298978fea764)的信息。
- en: All images of plots are screenshots taken from [Kolena](https://www.kolena.io/)
    unless otherwise indicated. Note that similar plots can be manually generated
    in common frameworks such as mathplotlib.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 所有图像均为从[Kolena](https://www.kolena.io/)截取的屏幕截图，除非另有说明。请注意，类似的图表可以在常见框架中手动生成，如mathplotlib。
