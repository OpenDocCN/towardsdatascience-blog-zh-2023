- en: A step-by-step guide to robust ML classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逐步指南：稳健的机器学习分类
- en: 原文：[https://towardsdatascience.com/a-step-by-step-guide-to-robust-ml-classification-5ce83592eb1d](https://towardsdatascience.com/a-step-by-step-guide-to-robust-ml-classification-5ce83592eb1d)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-step-by-step-guide-to-robust-ml-classification-5ce83592eb1d](https://towardsdatascience.com/a-step-by-step-guide-to-robust-ml-classification-5ce83592eb1d)
- en: How to avoid common pitfalls and dig deeper into our models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何避免常见的陷阱并深入探讨我们的模型
- en: '[](https://ryancburke8.medium.com/?source=post_page-----5ce83592eb1d--------------------------------)[![Ryan
    Burke](../Images/4a6c1ac506da2456406afe46bae8e732.png)](https://ryancburke8.medium.com/?source=post_page-----5ce83592eb1d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5ce83592eb1d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5ce83592eb1d--------------------------------)
    [Ryan Burke](https://ryancburke8.medium.com/?source=post_page-----5ce83592eb1d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ryancburke8.medium.com/?source=post_page-----5ce83592eb1d--------------------------------)[![Ryan
    Burke](../Images/4a6c1ac506da2456406afe46bae8e732.png)](https://ryancburke8.medium.com/?source=post_page-----5ce83592eb1d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5ce83592eb1d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5ce83592eb1d--------------------------------)
    [Ryan Burke](https://ryancburke8.medium.com/?source=post_page-----5ce83592eb1d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5ce83592eb1d--------------------------------)
    ·17 min read·Mar 3, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5ce83592eb1d--------------------------------)
    ·阅读时间17分钟·2023年3月3日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f1856f0dfbd79cecda67d8bfcb71783b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1856f0dfbd79cecda67d8bfcb71783b.png)'
- en: Photo by [Luca Bravo](https://unsplash.com/@lucabravo?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/wallpapers/nature/forest?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Luca Bravo](https://unsplash.com/@lucabravo?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来源于 [Unsplash](https://unsplash.com/wallpapers/nature/forest?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: In previous articles, I focused mainly on presenting individual algorithms that
    I found interesting. Here, I walk through a complete ML classification project.
    The goal is to touch on some of the common pitfalls in ML projects and describe
    to the readers how to avoid them. I will also demonstrate how we can go further
    by analysing our model errors to gain important insights that normally go unseen.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的文章中，我主要介绍了我认为有趣的单一算法。在这里，我将介绍一个完整的机器学习分类项目。目标是讨论一些机器学习项目中的常见陷阱，并向读者描述如何避免这些陷阱。我还将演示如何通过分析模型错误进一步挖掘，以获得通常未被发现的重要见解。
- en: If you would like to see the whole notebook, please check it out → [here](https://github.com/ryancburke/forest_cover)
    ←
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想查看完整的笔记本，请点击这里 → [这里](https://github.com/ryancburke/forest_cover) ←
- en: Libraries
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 库
- en: Below, you will find a list of the libraries I used for today’s analyses. They
    consist of the standard data science toolkit along with the necessary sklearn
    libraries.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我今天分析中使用的库的列表。它们包括标准的数据科学工具包以及所需的sklearn库。
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Data
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据
- en: Today’s dataset includes the forest cover data that is ready-to-employ with
    sklearn. Here’s a description from sklearn’s site.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的数据集包括可以直接用在sklearn中的森林覆盖数据。以下是来自sklearn网站的描述。
- en: '**Data Set Characteristics:**'
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**数据集特点：**'
- en: The samples in this dataset correspond to 30×30m patches of forest in the US,
    collected for the task of predicting each patch’s cover type, i.e. the dominant
    species of tree. There are seven cover types, making this a multi-class classification
    problem. Each sample has 54 features, described on the dataset’s homepage. Some
    of the features are boolean indicators, while others are discrete or continuous
    measurements.
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 该数据集中的样本对应于美国30×30米的森林区域，收集用于预测每个区域的覆盖类型，即主要树种。共有七种覆盖类型，这使得这是一个多类别分类问题。每个样本有54个特征，特征在数据集主页上有描述。部分特征为布尔指标，其他则为离散或连续测量值。
- en: 'Number of Instances: 581 012'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 实例数量：581 012
- en: Feature information (Name / Data Type / Measurement / Description**)**
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征信息（名称 / 数据类型 / 测量 / 描述**）
- en: Elevation / quantitative /meters / Elevation in meters
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高度 / 定量 / 米 / 以米为单位的高度
- en: Aspect / quantitative / azimuth / Aspect in degrees azimuth
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方面 / 定量 / 方位角 / 以度为单位的方位角
- en: Slope / quantitative / degrees / Slope in degrees
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 坡度 / 定量 / 度 / 以度为单位的坡度
- en: Horizontal_Distance_To_Hydrology / quantitative / meters / Horz Dist to nearest
    surface water features
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Horizontal_Distance_To_Hydrology / 定量 / 米 / 到最近水体的水平距离
- en: Vertical_Distance_To_Hydrology / quantitative / meters / Vert Dist to nearest
    surface water features
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vertical_Distance_To_Hydrology / 定量 / 米 / 到最近水体的垂直距离
- en: Horizontal_Distance_To_Roadways / quantitative / meters / Horz Dist to nearest
    roadway
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Horizontal_Distance_To_Roadways / 定量 / 米 / 到最近道路的水平距离
- en: Hillshade_9am / quantitative / 0 to 255 index / Hillshade index at 9am, summer
    solstice
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hillshade_9am / 定量 / 0 到 255 指数 / 早上9点的阴影指数，夏至
- en: Hillshade_Noon / quantitative / 0 to 255 index / Hillshade index at noon, summer
    soltice
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hillshade_Noon / 定量 / 0 到 255 指数 / 中午的阴影指数，夏至
- en: Hillshade_3pm / quantitative / 0 to 255 index / Hillshade index at 3pm, summer
    solstice
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hillshade_3pm / 定量 / 0 到 255 指数 / 下午3点的阴影指数，夏至
- en: Horizontal_Distance_To_Fire_Points / quantitative / meters / Horz Dist to nearest
    wildfire ignition points
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Horizontal_Distance_To_Fire_Points / 定量 / 米 / 到最近火灾点的水平距离
- en: Wilderness_Area (4 binary columns) / qualitative / 0 (absence) or 1 (presence)
    / Wilderness area designation
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wilderness_Area (4 个二元列) / 定性 / 0（缺失）或 1（存在） / 荒野区域指定
- en: Soil_Type (40 binary columns) / qualitative / 0 (absence) or 1 (presence) /
    Soil Type designation
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Soil_Type (40 个二元列) / 定性 / 0（缺失）或 1（存在） / 土壤类型指定
- en: '**Number of classes:**'
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**类别数：**'
- en: Cover_Type (7 types) / integer / 1 to 7 / Forest Cover Type designation
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cover_Type (7 种类型) / 整数 / 1 到 7 / 森林覆盖类型指定
- en: Load dataset
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据集
- en: Here’s a simple function to load this data into your notebook as a dataframe.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个简单的函数可以将这些数据加载到你的笔记本中作为数据框。
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Using df.info() and df.describe() to get to know our data better, we see that
    there are no missing data and it consists of quantitative variables. The dataset
    is also rather large (> 580 000 rows). I originally tried to run this on the entire
    dataset, but it took FOREVER, so I recommend using a fraction of the data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 df.info() 和 df.describe() 来更好地了解我们的数据，我们发现没有缺失数据，并且它由定量变量组成。数据集也相当大（> 580,000
    行）。我最初尝试在整个数据集上运行这个，但花费了很长时间，所以我建议使用数据的一个部分。
- en: 'Regarding the target variable, which is the forest cover class, using df.target.value_counts(),
    we see the following distribution (in descending order):'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 关于目标变量，即森林覆盖类别，使用 df.target.value_counts()，我们看到以下分布（按降序排列）：
- en: '**Class 2 = 283,301'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**类别 2 = 283,301'
- en: Class 1 = 211,840
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 类别 1 = 211,840
- en: Class 3 = 35,754
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 类别 3 = 35,754
- en: Class 7 = 20,510
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 类别 7 = 20,510
- en: Class 6 = 17,367
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 类别 6 = 17,367
- en: Class 5 = 9,493
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 类别 5 = 9,493
- en: Class 4 = 2,747**
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 类别 4 = 2,747
- en: It is important to note that our classes are imbalanced and we will need to
    keep this in mind when selecting a metric to evaluate our models.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意我们的类别是不平衡的，我们在选择评估模型的指标时需要牢记这一点。
- en: Prepare your data
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备你的数据
- en: One of the most common misunderstandings when running ML models is processing
    our data prior to splitting. Why is this a problem?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 运行机器学习模型时最常见的误解之一是对数据进行处理而不是拆分。为什么这是一个问题？
- en: Let’s say we plan on scaling our data using the whole dataset. The equations
    below are taken from their respective links.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们计划使用整个数据集来缩放我们的数据。以下方程来自它们各自的链接。
- en: '**Ex1** [**StandardScaler()**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**Ex1** [**StandardScaler()**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler)'
- en: z = (x — u) / s
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: z = (x — u) / s
- en: '**Ex2** [**MinMaxScaler()**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**Ex2** [**MinMaxScaler()**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler)'
- en: X_std = (X - X.min()) / (X.max() - X.min())
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: X_std = (X - X.min()) / (X.max() - X.min())
- en: X_scaled = X_std * (max - min) + min
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: X_scaled = X_std * (max - min) + min
- en: The most important thing we should notice is they include information such as
    mean, standard deviation, min, max. If we perform these functions prior to splitting,
    the features in our train set will be computed based on information included in
    the test set. This is an example of [data leakage](https://machinelearningmastery.com/data-leakage-machine-learning/).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该注意到的最重要的一点是，它们包括了均值、标准差、最小值和最大值等信息。如果我们在拆分之前执行这些函数，训练集中的特征将基于测试集中的信息进行计算。这是一个[数据泄露](https://machinelearningmastery.com/data-leakage-machine-learning/)的例子。
- en: Data leakage is when information from outside the training dataset is used to
    create the model. This additional information can allow the model to learn or
    know something that it otherwise would not know and in turn invalidate the estimated
    performance of the mode being constructed.
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数据泄漏是指使用来自训练数据集外的信息来创建模型。这些额外的信息可能使模型学习或知道一些它本来不会知道的东西，从而使正在构建的模型的估计性能无效。
- en: Therefore, the first step after getting to know our dataset is to split it and
    keep your test set ***unseen*** until the very end. In the code below, we split
    the data into 80% (training set) and 20% (test set). You will also note that I
    have only kept 50,000 total samples to reduce the time it takes to train & evaluate
    our models. Trust me, you will thank me later!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在了解数据集后，第一步是将其拆分，并保持测试集 ***未见*** 直到最后。在下面的代码中，我们将数据拆分为 80%（训练集）和 20%（测试集）。你还会注意到，我只保留了
    50,000 个样本，以减少训练和评估模型所需的时间。相信我，你会感谢我的！
- en: It is also worth noting that we are stratifying on the target variable. This
    is good practice for imbalanced datasets as it maintains the distribution of classes
    in the train and test set. If we don’t do this, there’s a chance that some of
    the underrepresented classes aren’t even present in our train or test sets.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 还值得注意的是，我们在目标变量上进行了分层。这对不平衡的数据集是一种良好的实践，因为它保持了训练集和测试集中类别的分布。如果我们不这样做，可能会有一些不足代表的类别在训练集或测试集中根本不存在。
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Feature engineering
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程
- en: With our train and test sets ready, we can now work on the fun stuff. The first
    step in this project is to generate some features that could add useful information
    to train our models.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的训练集和测试集准备好之后，我们可以开始处理有趣的内容。这个项目的第一步是生成一些可能为训练我们的模型提供有用信息的特征。
- en: This step can be a little tricky. In the real world, this requires domain-specific
    knowledge on the particular subject you are working. To be completely transparent
    with you, despite being a lover of nature and everything outdoors, I am no expert
    in why certain trees grow in specific areas.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步可能有些棘手。在现实世界中，这需要对你所处理的特定领域有专业的知识。完全透明地告诉你，尽管我热爱自然和户外活动，但我对某些树木为什么在特定地区生长并不精通。
- en: For this reason, I have consulted [[1](https://www.kaggle.com/code/skillsmuggler/forest-cover-feature-engineering-and-reduction/notebook#Feature-reduction)]
    [[2](https://www.kaggle.com/code/codename007/forest-cover-type-eda-baseline-model/notebook)]
    [[3](https://www.kaggle.com/code/kwabenantim/forest-cover-feature-engineering#Adding-new-features)]
    who have a better understanding of this domain than myself. I have amalgamated
    the knowledge from these references to create the features you will find below.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 出于这个原因，我咨询了 [[1](https://www.kaggle.com/code/skillsmuggler/forest-cover-feature-engineering-and-reduction/notebook#Feature-reduction)]
    [[2](https://www.kaggle.com/code/codename007/forest-cover-type-eda-baseline-model/notebook)]
    [[3](https://www.kaggle.com/code/kwabenantim/forest-cover-feature-engineering#Adding-new-features)]，他们对这个领域的理解比我更深入。我结合了这些参考资料中的知识，创建了下面的特征。
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: On a side note, when you are working with large datasets, pandas can be somewhat
    slow. Using [swifter](https://pypi.org/project/swifter/), as you can see in the
    last two lines above, you can significantly speed up the time it takes to apply
    a function to your dataframe. The article → [here](/speed-up-your-pandas-processing-with-swifter-6aa314600a13)
    compares several methods used to speed this process up.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，当你处理大数据集时，pandas 可能会有些慢。使用 [swifter](https://pypi.org/project/swifter/)，正如上面最后两行所示，你可以显著加快对数据框应用函数的时间。文章
    → [这里](/speed-up-your-pandas-processing-with-swifter-6aa314600a13) 比较了几种加速此过程的方法。
- en: Feature selection
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征选择
- en: At this point we have more than 70 features. If the goal is end up with the
    best performing model, then you could try to use all of these as inputs. With
    that said, often in business there is a trade-off between performance and complexity
    that needs to be considered.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 目前我们有超过 70 个特征。如果目标是获得表现最佳的模型，那么你可以尝试使用所有这些特征作为输入。话虽如此，商业中常常需要考虑性能和复杂性之间的权衡。
- en: As an example, suppose we have 94% accuracy in our model using all of these
    features. Then, imagine we have 89% accuracy with only 4 features. What is the
    cost we are willing to pay for a more interpretable model. Always weigh performance
    and complexity.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，假设我们使用所有这些特征时模型的准确率为 94%。然后，假设只有 4 个特征时准确率为 89%。我们愿意为一个更具可解释性的模型支付什么样的代价。始终权衡性能和复杂性。
- en: Keeping that in mind, I will perform feature selection to try and reduce the
    complexity right away. [Sklearn](https://scikit-learn.org/stable/modules/feature_selection.html)
    provides many options worth considering. In this example, I will use [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest)
    which will select a pre-specified number of features that provide the best performance.
    Below, I have requested (and listed) the best performing 15 features. These are
    the features that I will use to train the models in the following section.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 记住这一点，我将立即执行特征选择，以尽量减少复杂性。[Sklearn](https://scikit-learn.org/stable/modules/feature_selection.html)提供了许多值得考虑的选项。在这个例子中，我将使用[SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest)，它将选择预先指定数量的特征，这些特征提供最佳性能。下面，我请求（并列出了）表现最佳的15个特征。这些特征将用于在下一部分中训练模型。
- en: '[PRE4]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Baseline models
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基线模型
- en: 'In this section I will compare three different classifiers:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我将比较三种不同的分类器：
- en: '[KNeighboursClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KNeighboursClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)'
- en: '[RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)'
- en: '[ExtraTreesClassifer](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ExtraTreesClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)'
- en: I have provided links for those who wish to investigate each model further.
    They will also be helpful in the section on hyperparameter tuning, where you can
    find all modifiable parameters when trying to improve your models. Below you will
    find two functions to define and evaluate the baseline models.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我提供了链接以便那些希望进一步研究每个模型的人。这些链接在超参数调优部分也会很有帮助，你可以在其中找到所有可以调整的参数，以改善你的模型。下面你会发现两个函数，用于定义和评估基线模型。
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: There are some key elements in the second function that are worth discussing
    further. The first of which is [StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html).
    Recall, we split the original dataset into 80% training and 20% test. The test
    set will be reserved for the final evaluation of our top performing model.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个函数中有一些关键元素值得进一步讨论。其中第一个是[StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html)。请回忆一下，我们将原始数据集拆分为80%的训练集和20%的测试集。测试集将保留用于最终评估我们表现最好的模型。
- en: Using cross-validation will provide us with a better evaluation of our models.
    Specifically, I have set up a 10-fold cross-validation. For those not familiar,
    the model is trained on k — 1 folds and is validated on the remaining fold at
    each step. At the end you will have access to an average and variation of the
    k models, providing you with better insight than a simple train-test evaluation.
    Stratified K fold, as I eluded to earlier, is used to ensure that each fold has
    an approximately equal representation of the target classes.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用交叉验证将为我们提供更好的模型评估。具体来说，我设置了一个10折交叉验证。对于那些不熟悉的人，该模型在每一步中都在k — 1个折上进行训练，并在剩下的折上进行验证。最终，你将能够访问到k个模型的平均值和变异性，这比简单的训练-测试评估提供了更好的见解。正如我之前提到的，分层K折用于确保每个折中目标类的表示大致相等。
- en: The second point worth discussing is the scoring metric. There are many [metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)
    available to evaluate the performance of your models, and often there are several
    that could suit your project. It’s important to keep in mind what you are trying
    to demonstrate with the results. If you work in a business setting, often the
    metric that is most easily explained to those without a data background is preferred.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 第二点值得讨论的是评分指标。有许多[指标](https://scikit-learn.org/stable/modules/model_evaluation.html)可用于评估模型的性能，通常有几个适合你的项目。重要的是要记住你试图通过结果展示什么。如果你在商业环境中工作，通常选择最容易向没有数据背景的人解释的指标。
- en: On the other hand, there are metrics that are unsuitable to your analyses. For
    this project, we have imbalanced classes. If you go to the link provided above,
    you will find options for this case. I opted to use the weighted F1 score. Let’s
    briefly discuss why I chose this metric.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，有些指标不适合你的分析。对于这个项目，我们有不平衡的类别。如果你访问上面提供的链接，你会发现适用于这种情况的选项。我选择使用加权 F1 分数。让我们简要讨论一下为什么我选择了这个指标。
- en: A very common classification metric is accuracy, which is the percent of correct
    classifications. While this may seem like an excellent option, suppose we have
    a binary classification where the target classes are uneven (i.e. group 1 = 90,
    group 2 = 10). It is possible to have 90% accuracy, which is great, but if we
    explore further, we have correctly classified all of group 1 and failed to classify
    any of the group 2\. In this case our model is not terribly informative.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常常见的分类指标是准确率，它是正确分类的百分比。虽然这可能看起来是一个很好的选项，但假设我们有一个目标类不均衡的二分类问题（即组 1 = 90，组
    2 = 10）。我们可能会得到 90% 的准确率，这听起来很棒，但如果我们进一步探讨，我们发现正确分类了所有组 1 的样本，却未能分类组 2 的任何样本。在这种情况下，我们的模型信息量并不大。
- en: If we would have used the weighted F1 score we would have a result of 42.6%.
    If you’re interested in reading more on the F1 score → [here](/the-f1-score-bec2bbc38aa6)
    is an article explaining how it is calculated.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用加权 F1 分数，我们会得到 42.6% 的结果。如果你有兴趣了解更多关于 F1 分数的信息 → [这里](/the-f1-score-bec2bbc38aa6)
    有一篇文章解释了它是如何计算的。
- en: After training the baseline models, I have plotted the results from each below.
    The baseline models all performed relatively well. Remember, at this point I have
    done nothing to the data (i.e. transform, remove outliers). The Extra trees classifier
    had the highest weighted F1 score at 86.9%.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练基准模型后，我已绘制了每个模型的结果。所有基准模型的表现都相对良好。请记住，此时我对数据没有做任何处理（即未进行转换或去除异常值）。额外树分类器的加权
    F1 分数最高，为 86.9%。
- en: '![](../Images/f5d1439a88b1bb065aadfcaa8202ffdb.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f5d1439a88b1bb065aadfcaa8202ffdb.png)'
- en: Results from the 10-fold CV. KNN had the lowest at 78.8%, the RF was second
    with 85.9%, and the ET had the highest weighted F1 score at 86.9%. *Image provided
    by author*
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 10 倍交叉验证的结果如下。KNN 的表现最低，为 78.8%，RF 排在第二位，为 85.9%，而 ET 的加权 F1 分数最高，为 86.9%。*图片由作者提供*
- en: Transforming the data
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据转换
- en: The next step in this project will look at the effects of data transformation
    on model performance. Whereas many decision tree-based algorithms are not sensitive
    to the magnitude of the data, it is reasonable to expect that models measuring
    distance between samples , such as the KNN perform differently when scaled [[4](https://ai.stackexchange.com/questions/22307/why-are-decision-trees-and-random-forests-scale-invariant#:~:text=Decision%20Tree%20and%20Random%20Forest,work%20fine%20without%20feature%20scaling.)]
    [[5](https://stats.stackexchange.com/questions/244507/what-algorithms-need-feature-scaling-beside-from-svm)].
    In this section, we will scale our data using StandardScaler and MinMaxScaler
    as described above. Below you will find a function that describes a pipeline that
    will apply the scaler and then train the model using scaled data.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目的下一步将研究数据转换对模型性能的影响。虽然许多基于决策树的算法对数据的量纲不敏感，但可以合理预期，测量样本之间距离的模型，例如 KNN，当数据缩放时性能会有所不同
    [[4](https://ai.stackexchange.com/questions/22307/why-are-decision-trees-and-random-forests-scale-invariant#:~:text=Decision%20Tree%20and%20Random%20Forest,work%20fine%20without%20feature%20scaling.)]
    [[5](https://stats.stackexchange.com/questions/244507/what-algorithms-need-feature-scaling-beside-from-svm)]。在本节中，我们将使用上述描述的
    StandardScaler 和 MinMaxScaler 对数据进行缩放。下面是一个函数，它描述了一个管道，该管道将应用缩放器，然后使用缩放后的数据训练模型。
- en: '[PRE7]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The results using the StandardScaler are presented below. We see that our hypothesis
    regarding scaling the data appears to hold. Both the random forest and extra trees
    classifiers both performed nearly identically, whereas the KNN improved in performance
    by roughly 4%. Despite this increase, the two tree-based classifiers still outperform
    the scaled KNN.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 StandardScaler 的结果如下。我们看到有关数据缩放的假设似乎成立。随机森林和额外树分类器的表现几乎相同，而 KNN 的性能提高了大约
    4%。尽管有所提升，但这两个基于树的分类器仍然优于缩放后的 KNN。
- en: '![](../Images/f929c80f19245ca85081e472bacfb1db.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f929c80f19245ca85081e472bacfb1db.png)'
- en: Results from the 10-fold CV using the StandardScaler to transform our data.
    KNN still had the lowest although the performance increased to 83.8%. The RF was
    second with 85.8%, and the ET once again had the highest weighted F1 score at
    86.8%. *Image provided by author*
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 使用StandardScaler对数据进行10折交叉验证的结果。尽管表现有所提升至83.8%，KNN的表现仍然最低。RF排名第二，为85.8%，ET再次以86.8%的加权F1分数名列最高。*图像由作者提供*
- en: Similar results can be seen when the MinMaxScaler is used. The results from
    all models are almost identical to those presented using the StandardScaler.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MinMaxScaler时可以看到类似的结果。所有模型的结果几乎与使用StandardScaler时所呈现的结果完全相同。
- en: '![](../Images/75dba5daf7061c01f44fa4fd86846478.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75dba5daf7061c01f44fa4fd86846478.png)'
- en: Results from the 10-fold CV using the MinMaxScaler to transform our data. Each
    performed almost identically to those using StandardScaler. KNN still had the
    lowest at 83.9%. The RF was second with 86.0%, and the ET once again had the highest
    weighted F1 score at 87.0%. *Image provided by author*
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MinMaxScaler对数据进行10折交叉验证的结果。每个模型的表现几乎与使用StandardScaler时相同。KNN的表现仍然最低，为83.9%。RF排名第二，为86.0%，ET再次以87.0%的加权F1分数名列最高。*图像由作者提供*
- en: It is worth noting at this point that I also checked the effect of removing
    outliers. For this, I removed values that were beyond +/- 3 SD for each feature.
    I am not presenting the results here because there were no values outside this
    range. If you are interested in seeing how this was performed, please feel free
    to check out the notebook found at the link provided at the beginning of this
    article.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，我还检查了去除异常值的效果。为此，我移除了每个特征值超出±3个标准差的值。我在这里没有展示结果，因为没有值超出这个范围。如果你有兴趣了解如何执行此操作，请随时查看本文开头提供的笔记本链接。
- en: Hyperparameter tuning using GridSearchCV
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用GridSearchCV进行超参数调整
- en: The next step is to try and improve our models by tuning the hyperparameters.
    We will do so on the scaled data because it had the best average performance when
    considering our three models. Sklearn discusses this in more detail → [here](https://scikit-learn.org/stable/modules/grid_search.html).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是通过调整超参数来尝试改进我们的模型。我们将在缩放后的数据上进行，因为它在考虑我们的三种模型时表现最佳。Sklearn对此进行了更详细的讨论 →
    [这里](https://scikit-learn.org/stable/modules/grid_search.html)。
- en: I chose to use [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)
    (CV for cross validated). Below you will find a function that performs a 10-fold
    cross validation on the models we have been using. The one additional detail here
    is that we need to provide the list of hyperparameters we want to be evaluated.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我选择使用[GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)（CV表示交叉验证）。下面你将找到一个对我们一直使用的模型进行10折交叉验证的函数。这里唯一的额外细节是我们需要提供希望评估的超参数列表。
- en: Up to this point, we have not even looked at our test set. Before commencing
    the grid search, we will scale our train and test data using the StandardScaler.
    We are doing this here because we are going to find the best hyperparameters for
    each model and use those as inputs into a VotingClassifier (as we will discuss
    in the next section).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们甚至没有查看我们的测试集。在开始网格搜索之前，我们将使用StandardScaler对训练数据和测试数据进行缩放。我们在这里这样做是因为我们要找到每个模型的最佳超参数，并将这些超参数作为输入到VotingClassifier中（我们将在下一节讨论）。
- en: To properly scale our full dataset we have to follow the procedure below. You
    will see that the scaler is only fit on the training data. Both the training and
    test set are transformed based on the scaling parameters found with the training
    set, thus eliminating any chance of data leakage.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确缩放我们的完整数据集，我们必须按照以下程序进行。你将看到缩放器仅在训练数据上进行拟合。训练集和测试集都基于训练集找到的缩放参数进行转换，从而消除任何数据泄漏的可能性。
- en: '[PRE8]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Next, I have provided the grid search parameters that were tested for each of
    the models.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我提供了对每个模型测试的网格搜索参数。
- en: '[PRE10]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Voting ensemble classifier
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 投票集成分类器
- en: We have determined the best combination of parameters to optimise our models.
    These parameters will be used as the inputs into a [VotingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html),
    which is an ensemble estimator that trains several models and then aggregates
    the findings for a more robust prediction. I found this → [article](/use-voting-classifier-to-improve-the-performance-of-your-ml-model-805345f9de0e)
    which provides a detailed overview of the voting classifier and the different
    ways to use it.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经确定了优化我们模型的最佳参数组合。这些参数将作为输入用于一个[投票分类器](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)，这是一个集成估计器，它训练几个模型，然后汇总结果以获得更稳健的预测。我找到了一篇→
    [文章](/use-voting-classifier-to-improve-the-performance-of-your-ml-model-805345f9de0e)，提供了关于投票分类器及其使用方式的详细概述。
- en: The best parameters for each model are listed below. The output from the voting
    classifer shows that we achieved a weighted F1 score of 87.5% on the training
    set and 88.4% on the test set. Not bad!
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型的最佳参数列在下面。投票分类器的输出显示，我们在训练集上达到了 87.5% 的加权 F1 分数，在测试集上达到了 88.4%。还不错！
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Analysing the errors
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 错误分析
- en: The performance of our model is pretty good. With that said, it can be very
    insightful to investigate where the model failed. Below, you will find the code
    to generate a confusion matrix. Let’s see if we can learn something.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的表现相当不错。尽管如此，调查模型失败的地方可能非常有见地。下面，你将找到生成混淆矩阵的代码。让我们看看是否能学到一些东西。
- en: '[PRE12]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/d5369074906634124bff8df76e7c9c89.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d5369074906634124bff8df76e7c9c89.png)'
- en: Confusion matrix on the test set. Image by author
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集上的混淆矩阵。图片由作者提供
- en: Right away, it becomes quite evident that the underrepresented classes are not
    learned very well. This is so important because despite using a metric that is
    appropriate to evaluate imbalanced classes, you can’t make a model learn something
    that isn’t there.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 立刻可以看出，代表性不足的类别学习得不是很好。这非常重要，因为尽管使用了适合评估不平衡类别的指标，你还是无法让模型学习那些不存在的东西。
- en: To analyse our errors, we could create visualisations; however, with 15 features
    and 7 classes this can start to feel like one of those trippy stereogram images
    that you stare at until an image forms. An alternative approach is the following.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了分析我们的错误，我们可以创建可视化；然而，考虑到有 15 个特征和 7 个类别，这可能会开始感觉像那种你盯着看直到图像形成的迷幻立体图像。另一种方法如下。
- en: Machine learning classification of errors
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习错误分类
- en: In this section I am going to compare the predicted values to the ground truth
    in our test set and create a new variable, ‘*error’*. Below, I am setting up a
    dataset to be used in a binary classification analysis, where the target is error
    vs. no error using the same features as above.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将比较预测值与测试集中的实际值，并创建一个新变量，‘*error*’。下面，我正在设置一个数据集，用于二分类分析，目标是错误与非错误，使用与上述相同的特征。
- en: Since we already know that the underrepresented classes were not well learned,
    the goal here is to see which features were most associated with errors independent
    of class.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经知道代表性不足的类别没有很好地学习，那么这里的目标是查看哪些特征与错误的关联最为明显，而不考虑类别。
- en: '[PRE13]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: With our new dataset, the next step is to build a classification model. This
    time we are going to add a step using [SHAP](https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html).
    This will allow us to understand how each feature impacts the model, which in
    our case is error.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的新数据集，下一步是构建一个分类模型。这一次我们将添加一个步骤，使用[SHAP](https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html)。这将使我们能够理解每个特征如何影响模型，而在我们的案例中，这个模型是错误。
- en: Below, we have used a Random Forest to fit the data. Once again we are using
    K-fold cross-validation to give us a better estimate of the contribution of each
    feature. At the bottom, I have generated a dataframe with the average, standard
    deviation, and maximum shap values.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在下文中，我们使用了随机森林来拟合数据。我们再次使用 K 折交叉验证来更好地估计每个特征的贡献。最后，我生成了一个数据框，其中包含平均值、标准差和最大
    SHAP 值。
- en: '[PRE14]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: For a better visual experience, below is a shap summary plot. On the left hand
    side we have the feature names. The plot demonstrates the impact of each feature
    on the model for different values of that feature. Whereas the dispersion (how
    far to the right or left) describes the overall impact of a feature on the model,
    the colouring provides us with a little extra information.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好的视觉体验，下面是一个 SHAP 总结图。左侧是特征名称。该图展示了每个特征对模型的影响，针对不同特征值的影响。虽然分散度（向右或向左的距离）描述了特征对模型的整体影响，着色则为我们提供了额外的信息。
- en: '![](../Images/ca2d8b4f786111e9c1a89c5ec21d74f0.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ca2d8b4f786111e9c1a89c5ec21d74f0.png)'
- en: SHAP summary plot for the error classification model. Image by author
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 错误分类模型的SHAP总结图。图像由作者提供
- en: The first thing we notice is that the features with the greatest impact on the
    model relate more to distance features (i.e. to water, road, or fire ignition
    points) than to the type of forest (wilderness area) or soil type.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先注意到，对模型影响最大的特征更多地与距离特征（即水源、道路或火源点）相关，而不是森林类型（荒野区域）或土壤类型。
- en: Next, when we look at the colour distribution, we see a more clear differentiation
    of high vs low values for the first feature Hydro_Road_Fire_mean than the rest.
    The same might be said for Road_Fire_mean, albeit to a lesser degree.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，当我们查看颜色分布时，我们可以看到第一个特征 Hydro_Road_Fire_mean 在高值与低值之间的差异比其他特征更明显。对于 Road_Fire_mean
    也是如此，尽管程度较轻。
- en: 'To interpret what this means, we can formulate a statement like the following
    : *When the average distance to water, fire ignition points and road is low, there
    is a more likely chance of making an error.*'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释这意味着什么，我们可以提出这样的说法：*当到水源、火源点和道路的平均距离较低时，更容易出现错误。*
- en: Once again, I must insist that my forestry ‘*expertise’* is limited to a couple
    of weeks. I did do some research to help me interpret what this could mean and
    came across a couple of articles [[6](https://www.researchgate.net/publication/352180034_Application_of_remote_sensing_and_machine_learning_algorithms_for_forest_fire_mapping_in_a_Mediterranean_area/figures?lo=1)]
    [[7](https://www.researchgate.net/publication/233978116_GIS-grid-based_and_multi-criteria_analysis_for_identifying_and_mapping_peat_swamp_forest_fire_hazard_in_Pahang_Malaysia/figures?lo=1)]
    that suggest the distance to the road is a significant factor in the risk of forest
    fires.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我的林业‘*专业知识*’仅限于几周。我确实做了一些研究来帮助我解读这可能意味着什么，并发现了一些文章 [[6](https://www.researchgate.net/publication/352180034_Application_of_remote_sensing_and_machine_learning_algorithms_for_forest_fire_mapping_in_a_Mediterranean_area/figures?lo=1)]
    [[7](https://www.researchgate.net/publication/233978116_GIS-grid-based_and_multi-criteria_analysis_for_identifying_and_mapping_peat_swamp_forest_fire_hazard_in_Pahang_Malaysia/figures?lo=1)]，这些文章表明，道路的距离是森林火灾风险的一个重要因素。
- en: This leads me to hypothesise that forest fire may be a significant factor influencing
    the errors made on our dataset. It seems logical to me that areas impacted by
    fire would have a very different representation of forest diversity to those unaffected
    by fire. I’m sure someone with more experience could let me know if that makes
    sense :)
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这让我推测森林火灾可能是影响我们数据集上的错误的重要因素。对我来说，被火灾影响的区域与未受影响的区域在森林多样性上的表现差异很大是合乎逻辑的。我相信有更多经验的人可以告诉我这是否有道理
    :)
- en: Summary
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Today, we went through a step-by-step ML multi-classification problem. We touched
    on some important considerations when conducting these analyses, namely the importance
    of splitting the dataset before we start to manipulate it. This is one of the
    most common pitfalls in ML projects that can lead to serious issues limiting our
    ability to generalise our findings.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，我们详细讲解了一个逐步的机器学习多分类问题。我们讨论了进行这些分析时的一些重要考虑因素，即在开始处理数据集之前分割数据集的重要性。这是机器学习项目中最常见的陷阱之一，可能导致严重问题，限制了我们推广发现的能力。
- en: We also touched on the importance of selecting an appropriate metric to evaluate
    our models. Here, we used the weighted F1 score, which was appropriate for imbalanced
    classes. Despite this, we still saw that the underrepresented classes were not
    well learned.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了选择适当的评估指标以评估我们模型的重要性。在这里，我们使用了加权 F1 分数，这适用于不平衡的类别。尽管如此，我们仍然看到被低估的类别没有得到很好的学习。
- en: In my notebook, I also included a section on oversampling to create balanced
    classes using [ADASYN](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.ADASYN.html),
    which is a variation of [SMOTE](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html).
    To save you the suspense, upsampling significantly improved the results on the
    training set (but not the test set).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的笔记本中，我还包含了一个关于过采样的部分，使用了[ADASYN](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.ADASYN.html)来创建平衡的类别，它是[SMOTE](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html)的一种变体。为了让你们不再悬念，过采样显著改善了训练集的结果（但没有改善测试集的结果）。
- en: This leads us to the error analysis, which is an important part of any ML project.
    A binary error classification was performed and may suggest that forest fires
    were implicated in many of the model errors. This could also explain, to a certain
    extent, why upsampling didn’t improve our final model.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了错误分析，这是任何机器学习项目中的重要部分。进行了二分类错误分析，结果可能表明森林火灾在许多模型错误中起到了作用。这也在一定程度上解释了为什么过采样没有改善我们的最终模型。
- en: Finally, I want to thank you all for taking the time to read this article! I
    hope some of you found it to be helpful :)
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，感谢大家抽时间阅读这篇文章！我希望你们中的一些人觉得它有帮助 :)
