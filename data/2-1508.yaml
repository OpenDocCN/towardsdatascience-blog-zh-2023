- en: Meta AI Introduces Revolutionary Image Segmentation Model Trained on 1 Billion
    Masks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Meta AI推出了突破性的图像分割模型，该模型基于10亿个掩码进行训练。
- en: 原文：[https://towardsdatascience.com/meta-ai-introduces-revolutionary-image-segmentation-model-trained-on-1-billion-masks-8f13c86a13a2](https://towardsdatascience.com/meta-ai-introduces-revolutionary-image-segmentation-model-trained-on-1-billion-masks-8f13c86a13a2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/meta-ai-introduces-revolutionary-image-segmentation-model-trained-on-1-billion-masks-8f13c86a13a2](https://towardsdatascience.com/meta-ai-introduces-revolutionary-image-segmentation-model-trained-on-1-billion-masks-8f13c86a13a2)
- en: Segment Anything - Best DL Model for Image Segmentation.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Segment Anything - 最佳深度学习图像分割模型。
- en: '[](https://medium.com/@gkeretchashvili?source=post_page-----8f13c86a13a2--------------------------------)[![Gurami
    Keretchashvili](../Images/4da78f113a0046c2deb8224e09dd9e3d.png)](https://medium.com/@gkeretchashvili?source=post_page-----8f13c86a13a2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8f13c86a13a2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8f13c86a13a2--------------------------------)
    [Gurami Keretchashvili](https://medium.com/@gkeretchashvili?source=post_page-----8f13c86a13a2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@gkeretchashvili?source=post_page-----8f13c86a13a2--------------------------------)[![古拉米·克雷特查什维利](../Images/4da78f113a0046c2deb8224e09dd9e3d.png)](https://medium.com/@gkeretchashvili?source=post_page-----8f13c86a13a2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8f13c86a13a2--------------------------------)[![数据科学](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8f13c86a13a2--------------------------------)
    [古拉米·克雷特查什维利](https://medium.com/@gkeretchashvili?source=post_page-----8f13c86a13a2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8f13c86a13a2--------------------------------)
    ·7 min read·Apr 6, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8f13c86a13a2--------------------------------)
    ·7分钟阅读·2023年4月6日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '**Introduction**'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**介绍**'
- en: After revolutionary step made by OpenAI’s ChatGPT in NLP, AI progression continues
    and Meta AI introduces astonishing progress in computer vision. Meta AI research
    team introduced the model called Segment Anything Model (SAM) and a dataset of
    1 Billion masks on 11 Million images. Segmentation of an image is identifying
    which image pixels belong to an object.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenAI的ChatGPT在自然语言处理领域取得革命性进展之后，人工智能的进步继续进行，Meta AI也在计算机视觉领域取得了令人惊叹的进展。Meta
    AI研究团队推出了名为Segment Anything Model（SAM）的模型，并在1100万张图像上使用10亿个掩码的数据集。图像分割是识别哪些图像像素属于一个对象。
- en: '![](../Images/971677e742403a8ddc0322de29fe513c.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/971677e742403a8ddc0322de29fe513c.png)'
- en: Demo of image segmentation by ai.facebook.com
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 由 ai.facebook.com 提供的图像分割演示
- en: 'The proposed project mainly includes three pillars: **Task**, **Model** and
    **Data**.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目主要包括三个支柱：**任务**、**模型**和**数据**。
- en: '**1\. Segment Anything Task**'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**1. Segment Anything 任务**'
- en: The main goal for Meta AI team was to create a promptable image segmentation
    model that would work with user input prompt as it is working with ChatGPT. Therefore,
    they came up with the solution to integrate user input with the image to produce
    segmentation masks. Segmentation prompt can be any information indicating what
    to segment in an image. For example, set of foreground or background point, a
    box, free-form text etc. So the model’s output is a valid segmentation mask given
    any user defined prompt.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Meta AI团队的主要目标是创建一个可以根据用户输入提示工作的图像分割模型，就像ChatGPT一样。因此，他们提出了将用户输入与图像集成以生成分割掩码的解决方案。分割提示可以是任何指示图像中要分割内容的信息。例如，前景或背景点的集合、框、自由形式的文本等。因此，模型的输出是给定任何用户定义提示的有效分割掩码。
- en: 2\. Segment Anything Model
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2. Segment Anything 模型
- en: The promotable Segment Anything Model (SAM) has three components shown in the
    figure bellow.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 可推广的Segment Anything Model（SAM）有三个组件，如下图所示。
- en: '![](../Images/8352e7a206b80c841e9ccd992a47ca50.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8352e7a206b80c841e9ccd992a47ca50.png)'
- en: Segment anything model workflow by ai.facebook.com
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything 模型工作流程由 ai.facebook.com 提供
- en: A high level of model architecture consists of *an image encoder*, *prompt encoder*,
    and *mask decoder*. For***the image encoder*** they have used MAE [1] pre-trained
    model that has Vision Transformer(ViT) [2] architecture. ViT models are state-of-the-art
    models in image classification and segmentation tasks. As for the prompts, they
    divided them into two types — one type of prompts is *sparse* such as points,
    boxes, and text and another type is *dense* such as masks. **The prompt encoder**
    step creates embeddings for each type of prompt. As for **the mask decoder**,
    it just maps image embeddings, prompt embeddings, and output tokens to a mask.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 高级模型架构由*图像编码器*、*提示编码器*和*掩码解码器*组成。对于***图像编码器***，他们使用了具有 Vision Transformer (ViT)
    [2] 架构的 MAE [1] 预训练模型。ViT 模型在图像分类和分割任务中是最先进的模型。至于提示，他们将其分为两种类型——一种是*稀疏*的提示，如点、框和文本，另一种是*密集*的提示，如掩码。**提示编码器**步骤为每种类型的提示创建嵌入。至于**掩码解码器**，它仅将图像嵌入、提示嵌入和输出令牌映射到掩码上。
- en: 3\. Segment Anything Data
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3.2 Segment Anything 数据集
- en: '**3.1 Segment Anything Data Engine**'
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3.1 Segment Anything 数据引擎**'
- en: '![](../Images/53fda8ddeccfd49e5b6bf76d1ea3daf9.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/53fda8ddeccfd49e5b6bf76d1ea3daf9.png)'
- en: Garbage in garbage out (image by the author)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 垃圾进垃圾出（作者图像）
- en: The principle — garbage in garbage out — applies to the AI domain as well. If
    the input data is poor quality, a model-generated result will not be good as well.
    That is why, the Meta team tried to select high-quality images to train their
    model. The team has created a data engine to filter the raw image dataset. Creating
    a data engine is divided into three stages.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 原则——垃圾进垃圾出——同样适用于 AI 领域。如果输入数据质量较差，则模型生成的结果也不会好。这就是为什么 Meta 团队试图选择高质量图像来训练他们的模型。该团队创建了一个数据引擎来筛选原始图像数据集。创建数据引擎分为三个阶段。
- en: '*Manual stage*: Human professional annotators were involved to label masks
    on the image manually.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*人工阶段*：专业的人工标注员参与手动标注图像上的掩码。'
- en: '*Semi-automatic stage*: They trained the model on annotated images and made
    an inference on the rest of the images. Then, human annotators were asked to label
    additional unlabeled objects that were not detected by the model or correct segments
    with low confidence scores.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*半自动阶段*：他们在标注的图像上训练了模型，并对其余图像进行了推断。然后，要求人工标注员标注模型未检测到的附加未标记对象，或纠正置信度低的分段。'
- en: '*Fully automatic stage*: This stage includes automatic mask generation and
    automatic filtering stage which tries to leave non-ambiguous masks and keep the
    masks based on confidence, stability, and size.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*完全自动阶段*：这一阶段包括自动掩码生成和自动过滤阶段，试图保留非歧义掩码，并根据置信度、稳定性和大小保留掩码。'
- en: '**3.2 Segment Anything Dataset**'
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3.2 Segment Anything 数据集**'
- en: The Segment Anything Data Engine created a 1 Billion masks dataset (SA-1B) on
    11 Million diverse, high resolution (3300x4900 pixels on average) and licensed
    images. It is worth mentioning that 99.1% of masks were generated automatically,
    however the quality is so high because they are carefully selected.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything 数据引擎创建了一个包含 10 亿个掩码的数据集（SA-1B），这些掩码基于 1100 万张多样化、高分辨率（平均 3300x4900
    像素）和授权的图像。值得一提的是，99.1% 的掩码是自动生成的，但由于它们经过精心选择，质量非常高。
- en: Conclusion — Why is the model revolutionary
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论——为什么该模型具有革命性
- en: 'Meta AI team together with other Huge company teams are doing great progress
    in development of AI. The Segment Anything Model (SAM) has capabilities to power
    applications in numerous domains that require finding and segmenting any object
    in any image. For example:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Meta AI 团队与其他大型公司团队一起在 AI 发展方面取得了重大进展。Segment Anything 模型（SAM）具有为许多领域中的应用提供支持的能力，这些领域需要在任何图像中查找和分割任何对象。例如：
- en: SAM could be a component of a large multimodal model that integrated images,
    text, audio etc.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SAM 可能是一个大型多模态模型的组成部分，该模型集成了图像、文本、音频等。
- en: SAM could enable selecting an object AR/VR domain based on a user’s gaze and
    then “lifting” it into 3D
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SAM 可以基于用户的视线在 AR/VR 领域选择对象，然后将其“提取”到 3D 中。
- en: SAM can improve creative applications such as extracting image regions for video
    editing.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SAM 可以提升创意应用，例如提取图像区域以进行视频编辑。
- en: and many more.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还有更多。
- en: Image Segmentation Demo
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像分割演示
- en: In this part, I will try to use [official GitHub code](https://github.com/facebookresearch/segment-anything)
    to play with the algorithm using [Google Colab](https://colab.research.google.com/)
    and perform two types of segmentation on the image. First, I will do segmentation
    with user-defined prompt and second I will do fully automatic segmentation.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我将尝试使用[官方 GitHub 代码](https://github.com/facebookresearch/segment-anything)来玩转算法，并使用[Google
    Colab](https://colab.research.google.com/)对图像进行两种类型的分割。首先，我将进行用户定义提示的分割，其次，我将进行完全自动的分割。
- en: 'Part 1: Image segmentation using user-defined prompt'
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1部分：使用用户定义提示进行图像分割
- en: Set up (import libraries and installations)
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置（导入库和安装）
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 2\. Helper functions to plot masks, point and boxes on the image.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 辅助函数用于在图像上绘制掩码、点和框。
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 3\. Input image (initial image to segment). Lets try to select the mask of a
    first bag of a groceries.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 输入图像（初始图像进行分割）。让我们尝试选择第一袋食品的掩码。
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/c70fb14c5c02fd0df72e0e28e673e0fc.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c70fb14c5c02fd0df72e0e28e673e0fc.png)'
- en: Input image (image from Facebook research)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 输入图像（图像来自Facebook研究）
- en: 4\. Load the pretrained model called [*sam_vit_h_4b8939.pth*](https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth)which
    is a default model. There are another lighter version of models such as[*sam_vit_l_0b3195.pth*](https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth)
    and [*sam_vit_b_01ec64.pth*](https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 加载名为[*sam_vit_h_4b8939.pth*](https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth)的预训练模型，该模型为默认模型。还有其他更轻量的模型版本，如[*sam_vit_l_0b3195.pth*](https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth)和[*sam_vit_b_01ec64.pth*](https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth)
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 5\. Visualize the point on the image(user prompt) which will help to identify
    our target object — first glossary bag.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 可视化图像上的点（用户提示），这将有助于识别我们的目标对象 — 第一个食品袋。
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/1d8e8b196a4c7245e10b40bd73ddc4cf.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d8e8b196a4c7245e10b40bd73ddc4cf.png)'
- en: Input image with user prompt (image from Facebook research)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 带用户提示的输入图像（图像来自Facebook研究）
- en: 6\. Make a prediction to generate a mask of the object.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 6\. 进行预测以生成目标对象的掩码。
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 7\. Show top 3 generated mask. When *multimask_output=True*, the algorithm returns
    three mask. Later we can select the one with the highest score.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 7\. 显示前3个生成的掩码。当*multimask_output=True*时，算法返回三个掩码。稍后我们可以选择分数最高的一个。
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/11c8884ed59aad93a3241f52717e0bc3.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11c8884ed59aad93a3241f52717e0bc3.png)'
- en: Prediction results (image by the author)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 预测结果（图像由作者提供）
- en: 'The highlighted objects are the masks predicted by the model. As the result
    shows, the model generated three output masks with following prediction scores:
    mask1 — 0.990, Mask2 — 0.875 and Mask3 — 0.827\. We select mask1 which has the
    highest score. Voila!!!! Model’s prediction mask is out target object that we
    wanted to segment initially. The result is amazing, the model works quite well!'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 突出的对象是模型预测的掩码。结果显示，模型生成了三个输出掩码，预测分数分别为：掩码1 — 0.990，掩码2 — 0.875 和掩码3 — 0.827。我们选择了分数最高的掩码1。瞧！！！模型预测的掩码就是我们最初想要分割的目标对象。结果令人惊讶，模型表现得相当好！
- en: 'Part 2: Fully Automatic Image segmentation — Cont.'
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2部分：完全自动的图像分割 — 续
- en: Plotting function of segments
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分段的绘图函数
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 2\. Generate masks automatedly
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 自动生成掩码
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 3\. Show the result
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 显示结果
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/1f467ce0e91dabed111231e051319569.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f467ce0e91dabed111231e051319569.png)'
- en: Automatic segmentation result by SAM (image by the author)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: SAM的自动分割结果（图像由作者提供）
- en: The algorithm identified 137 different objects (masks) using default parameters.
    Each mask contains information about segment area, bounding box coordinates, prediction
    score and stability score that could be used to filter out unwonted segments.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 算法使用默认参数识别了137个不同的对象（掩码）。每个掩码包含关于分段区域、边界框坐标、预测分数和稳定性分数的信息，这些信息可以用来过滤掉不需要的分段。
- en: '*I hope you enjoyed it and now can start creating beautiful apps yourself.
    If you have any questions or would like to share your thoughts about this article,
    feel free to comment, I will be happy to answer.*'
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*希望你喜欢这篇文章，并且现在可以开始自己创建美丽的应用程序了。如果你有任何问题或想分享对这篇文章的看法，请随时评论，我会很高兴回答。*'
- en: ''
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*If you want to support my work directly and also get unlimited access on Medium
    articles, become a Medium member using my* [*referral link*](https://medium.com/@gkeretchashvili/membership)
    *here. Thank you a million times and have a nice day!*'
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*如果你想直接支持我的工作并且获得Medium文章的无限访问权限，请通过我的* [*推荐链接*](https://medium.com/@gkeretchashvili/membership)
    *成为Medium会员。在此，万分感谢，祝你有美好的一天！*'
- en: '[](https://medium.com/@gkeretchashvili/membership?source=post_page-----8f13c86a13a2--------------------------------)
    [## Join Medium with my referral link - Gurami Keretchashvili'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@gkeretchashvili/membership?source=post_page-----8f13c86a13a2--------------------------------)
    [## 通过我的推荐链接加入Medium - Gurami Keretchashvili'
- en: Read every story from Gurami Keretchashvili (and thousands of other writers
    on Medium). Your membership fee directly…
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阅读Gurami Keretchashvili（以及Medium上成千上万其他作者）的每一个故事。你的会员费直接...
- en: medium.com](https://medium.com/@gkeretchashvili/membership?source=post_page-----8f13c86a13a2--------------------------------)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@gkeretchashvili/membership?source=post_page-----8f13c86a13a2--------------------------------)
- en: References
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross
    Girshick. Masked autoencoders are scalable vision learners. CVPR, 2022.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, 和 Ross
    Girshick. 掩蔽自编码器是可扩展的视觉学习者. CVPR, 2022.'
- en: '[2] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
    Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,
    Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words:
    Transformers for image recognition at scale. ICLR, 2021.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
    Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,
    Sylvain Gelly, Jakob Uszkoreit, 和 Neil Houlsby. 一张图胜过16x16个词：大规模图像识别的变换器. ICLR,
    2021.'
- en: '[3] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland,
    Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo,
    Piotr Dollar, Ross Girshick. Segment Anything, 2023'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland,
    Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo,
    Piotr Dollar, Ross Girshick. Segment Anything, 2023'
- en: My Previous articles about ML deployment
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我之前关于机器学习部署的文章
- en: '[](/how-to-deploy-machine-learning-models-end-to-end-dog-breed-identification-project-5689457d8973?source=post_page-----8f13c86a13a2--------------------------------)
    [## How to Deploy Machine Learning models? End-to-End Dog Breed Identification
    Project!'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/how-to-deploy-machine-learning-models-end-to-end-dog-breed-identification-project-5689457d8973?source=post_page-----8f13c86a13a2--------------------------------)
    [## 如何部署机器学习模型？端到端狗品种识别项目！'
- en: Simplest way to deploy your ML model on the web.
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在网络上部署你的机器学习模型的最简单方法。
- en: towardsdatascience.com](/how-to-deploy-machine-learning-models-end-to-end-dog-breed-identification-project-5689457d8973?source=post_page-----8f13c86a13a2--------------------------------)
    [](/how-to-deploy-machine-learning-models-601f8c13ff45?source=post_page-----8f13c86a13a2--------------------------------)
    [## How to Deploy Machine Learning Models
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/how-to-deploy-machine-learning-models-end-to-end-dog-breed-identification-project-5689457d8973?source=post_page-----8f13c86a13a2--------------------------------)
    [](/how-to-deploy-machine-learning-models-601f8c13ff45?source=post_page-----8f13c86a13a2--------------------------------)
    [## 如何部署机器学习模型
- en: The easiest way to deploy machine learning models on the web
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在网络上部署机器学习模型的最简单方法
- en: towardsdatascience.com](/how-to-deploy-machine-learning-models-601f8c13ff45?source=post_page-----8f13c86a13a2--------------------------------)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/how-to-deploy-machine-learning-models-601f8c13ff45?source=post_page-----8f13c86a13a2--------------------------------)
