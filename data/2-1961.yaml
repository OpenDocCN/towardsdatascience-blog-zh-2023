- en: Teaching AI to Play Board Games
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 教人工智能玩棋盘游戏
- en: 原文：[https://towardsdatascience.com/teaching-ai-to-play-board-games-77e5d1749dd9](https://towardsdatascience.com/teaching-ai-to-play-board-games-77e5d1749dd9)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/teaching-ai-to-play-board-games-77e5d1749dd9](https://towardsdatascience.com/teaching-ai-to-play-board-games-77e5d1749dd9)
- en: Using reinforcement learning from scratch to teach a computer to play Tic-Tac-Toe
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用从零开始的强化学习教计算机玩井字棋
- en: '[](https://heiko-hotz.medium.com/?source=post_page-----77e5d1749dd9--------------------------------)[![Heiko
    Hotz](../Images/d08394d46d41d5cd9e76557a463be95e.png)](https://heiko-hotz.medium.com/?source=post_page-----77e5d1749dd9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----77e5d1749dd9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----77e5d1749dd9--------------------------------)
    [Heiko Hotz](https://heiko-hotz.medium.com/?source=post_page-----77e5d1749dd9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://heiko-hotz.medium.com/?source=post_page-----77e5d1749dd9--------------------------------)[![Heiko
    Hotz](../Images/d08394d46d41d5cd9e76557a463be95e.png)](https://heiko-hotz.medium.com/?source=post_page-----77e5d1749dd9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----77e5d1749dd9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----77e5d1749dd9--------------------------------)
    [Heiko Hotz](https://heiko-hotz.medium.com/?source=post_page-----77e5d1749dd9--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----77e5d1749dd9--------------------------------)
    ·18 min read·Dec 12, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----77e5d1749dd9--------------------------------)
    ·18 分钟阅读·2023年12月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/60b0afcb68a3b3d4c14f8bdf7964d2ef.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/60b0afcb68a3b3d4c14f8bdf7964d2ef.png)'
- en: Image by author (created with ChatGPT)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供（由 ChatGPT 创建）
- en: What is this about?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这是什么内容？
- en: It appears that everyone in the AI sector is currently honing their Reinforcement
    Learning (RL) skills, especially in Q-learning, following the recent rumours about
    OpenAI’s new AI model, [*Q**](https://en.wikipedia.org/wiki/OpenAI#Q*)and I’m
    joining in too. However, rather than speculating about *Q** or revisiting old
    papers and examples for Q-learning, I’ve decided to use my enthusiasm for board
    games to give an introduction to Q-learning 🤓
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，人工智能领域似乎每个人都在提升他们的强化学习（RL）技能，特别是在 Q-learning 方面，跟随关于 OpenAI 新 AI 模型 [*Q**](https://en.wikipedia.org/wiki/OpenAI#Q*)
    的最新传闻，我也参与其中。然而，我决定用我对棋盘游戏的热情来介绍 Q-learning 🤓，而不是对 *Q** 进行猜测或重温 Q-learning 的旧论文和示例。
- en: In this blog post, I will create a simple programme from scratch to teach a
    model how to play Tic-Tac-Toe (TTT). I will refrain from using any RL libraries
    like [*Gym*](https://github.com/openai/gym) or [*Stable Baselines*](https://github.com/DLR-RM/stable-baselines3);
    everything is hand-coded in native Python, and the script is merely 100 lines
    long. If you’re curious about how to instruct an AI to play games, keep reading.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我将从头开始创建一个简单的程序，教一个模型如何玩井字棋（TTT）。我将避免使用任何强化学习库，比如 [*Gym*](https://github.com/openai/gym)
    或 [*Stable Baselines*](https://github.com/DLR-RM/stable-baselines3)；所有内容都是用原生
    Python 手动编写的，脚本只有 100 行。如果你对如何指导人工智能玩游戏感到好奇，请继续阅读。
- en: You can find all the code on GitHub at [https://github.com/marshmellow77/tictactoe-q](https://github.com/marshmellow77/tictactoe-q).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 GitHub 上找到所有代码，链接为 [https://github.com/marshmellow77/tictactoe-q](https://github.com/marshmellow77/tictactoe-q)。
- en: Why is it important?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么这很重要？
- en: Teaching an AI to play Tic-Tac-Toe (TTT) might not seem all that important.
    However, it does provide a (hopefully) clear and understandable introduction to
    Q-learning and RL, which might be important in the field of Generative AI (GenAI)
    since there has been speculation that stand-alone GenAI models, such as GPT-4,
    are insufficient for significant advancements. They are limited by the fact that
    they can only ever predict the next token and not being able to reason at all.
    RL is believed to be able to address this issue and potentially enhance the responses
    from GenAI models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 教人工智能玩井字棋（TTT）可能看起来并不那么重要。然而，它确实提供了一个（希望）清晰且易于理解的 Q-learning 和 RL 的介绍，这在生成式人工智能（GenAI）领域可能是重要的，因为有人猜测像
    GPT-4 这样的独立 GenAI 模型对于显著的进步是不够的。它们的局限性在于只能预测下一个标记，而无法进行任何推理。RL 被认为能够解决这个问题，并可能增强
    GenAI 模型的响应能力。
- en: But whether you’re aiming to brush up on your RL skills in anticipation of these
    advancements, or you’re simply seeking an engaging introduction to Q-learning,
    this tutorial is designed for both scenarios 🤗
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你是为了迎接这些进展而提升你的RL技能，还是仅仅寻求一个有趣的Q学习入门教程，这个教程都适合这两种情况🤗
- en: '**Understanding Q-Learning**'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**理解Q学习**'
- en: At its core, Q-learning is an algorithm that learns the value of an action in
    a particular state, and then uses this information to find the best action. Let’s
    consider the example of the *Frozen Lake* game, a popular single-player game used
    to demonstrate Q-learning.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，Q学习是一种算法，它学习特定状态下一个动作的价值，然后利用这些信息找到最佳动作。让我们考虑*Frozen Lake*游戏的例子，这是一款用于演示Q学习的流行单人游戏。
- en: 'In Frozen Lake, the player (starting at cell 0) navigates across a grid of
    ice and water cells, aiming to reach a goal (cell 15) without falling into the
    water. Each cell represents a state, and the player can move in four directions:
    up, down, left, or right.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在Frozen Lake中，玩家（从单元格0开始）在冰和水的网格上移动，目标是到达目标（单元格15）而不掉入水中。每个单元格代表一个状态，玩家可以向四个方向移动：上、下、左或右。
- en: '![](../Images/445f990a7fb1ddd8e01d917fc85bd76b.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/445f990a7fb1ddd8e01d917fc85bd76b.png)'
- en: Image by author (created with Stable Diffusion)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片（使用Stable Diffusion创建）
- en: At the beginning of the game the ***agent*** (that’s what an AI player is usually
    called) has no information and will just try out some moves randomly. In the context
    of Q-learning, this exploration phase is crucial. The agent learns by receiving
    rewards or penalties based on its actions. In Frozen Lake, reaching the goal earns
    a high reward, while falling into water results in a penalty. This system of rewards
    and penalties guides the agent in learning the most effective path to the goal.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在游戏开始时，***代理***（这就是AI玩家通常的称呼）没有任何信息，只会随机尝试一些动作。在Q学习的背景下，这个探索阶段至关重要。代理通过根据其动作获得奖励或惩罚来学习。在Frozen
    Lake中，达到目标会获得高奖励，而掉入水中则会受到惩罚。这种奖励和惩罚的系统引导代理学习最有效的到达目标的路径。
- en: Q-learning uses a table, known as a Q-table, to record the value of each action
    in each state. This table is updated continuously as the agent explores the environment.
    The Q-table entries, known as Q-values, represent the expected utility of taking
    a certain action in a given state, and they are updated using the [Bellman Equation](https://en.wikipedia.org/wiki/Bellman_equation).
    This equation considers the immediate reward from an action and the highest possible
    future rewards (more about this later).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习使用一个表格，称为Q-表，用于记录每个状态下每个动作的价值。随着代理探索环境，这个表格会不断更新。Q-表条目，称为Q值，表示在给定状态下采取某个动作的预期效用，它们通过[贝尔曼方程](https://en.wikipedia.org/wiki/Bellman_equation)进行更新。这个方程考虑了动作的即时奖励和可能的最高未来奖励（稍后会详细讲解）。
- en: 'Basically, the Q-table is a cheatsheet or lookup table for the agent: Depending
    in which state the game is, the agent will look up that state, determine which
    action has the most utility (i.e. which is the best action to take) and then executes
    that action. Below an illustration of how the Q-table could look like:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，Q-表是代理的备忘单或查找表：根据游戏的状态，代理会查找该状态，确定哪个动作具有最高效用（即哪个是最佳动作），然后执行该动作。以下是Q-表可能的示例：
- en: '![](../Images/1b92d5b9413528dfdb03ec6cdcf27250.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b92d5b9413528dfdb03ec6cdcf27250.png)'
- en: Image by author
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: In this example the player would choose the action *Right* if it was in state
    1 (i.e. on cell 1) because it is the action with the highest vale.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，如果玩家处于状态1（即在单元格1），他会选择动作*右*，因为这是具有最高价值的动作。
- en: Over time, as the agent explores the environment and updates the Q-table, it
    becomes more adept at navigating the Frozen Lake, eventually learning an optimal
    or near-optimal policy to reach the goal reliably. The beauty of Q-learning in
    this scenario is its model-free nature, meaning it doesn’t require a model of
    the environment and can learn solely from interaction, making it broadly applicable
    to various problems in RL.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，代理探索环境并更新Q-表，它在导航Frozen Lake时变得更加熟练，最终学会了一种最佳或接近最佳的策略，以可靠地到达目标。Q学习在这种情况下的美妙之处在于它的无模型性质，这意味着它不需要环境模型，可以仅通过交互学习，使其广泛适用于各种RL问题。
- en: There exist many tutorials that demonstrate how to leverage and implement Q-learning
    for the Frozen Lake game, for example [https://towardsdatascience.com/q-learning-for-beginners-2837b777741](/q-learning-for-beginners-2837b777741).
    However, as previously mentioned, my interest as a board game enthusiast lies
    in adapting this method for two-player games, and potentially even for games with
    more players.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 存在许多教程演示了如何利用和实现 Q-learning 来解决 Frozen Lake 游戏，例如 [https://towardsdatascience.com/q-learning-for-beginners-2837b777741](/q-learning-for-beginners-2837b777741)。然而，正如前面提到的，作为一个棋盘游戏爱好者，我对将这种方法适用于双人游戏，甚至更多玩家的游戏更感兴趣。
- en: '**Challenges in Two-Player Games**'
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**双人游戏中的挑战**'
- en: 'Applying Q-learning to a two-player game such as TTT necessitates a minor modification.
    In the Frozen Lake game, the next state is determined solely by the agent’s action.
    However, in TTT, although a player may take a turn, the subsequent state also
    depends on the opponent’s move. For instance, if I place an ‘X’ in the top-left
    corner, the next state is uncertain because my opponent has several potential
    moves:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 将 Q-learning 应用于双人游戏，如井字棋，需要进行一些小的修改。在 Frozen Lake 游戏中，下一状态仅由代理的行动决定。然而，在井字棋中，尽管玩家可能采取一个回合，但随后的状态还依赖于对手的行动。例如，如果我在左上角放置一个‘X’，那么下一状态是不确定的，因为我的对手有几个潜在的移动：
- en: '![](../Images/db3bd40a35d9275cb4e3928f83be3e65.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db3bd40a35d9275cb4e3928f83be3e65.png)'
- en: Image by author
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Several methods can be employed to tackle this issue. One approach involves
    simulating all possible actions of the opponent and their respective outcomes.
    This requires generating a probability distribution for all potential subsequent
    states and updating the Q-values according to the anticipated results of these
    states. However, this method can be computationally demanding. In this tutorial
    we will take a much simpler approach and just take a random action for the opponent
    and update the Q-table based on the actual outcome of this action. This reflects
    the unpredictable nature of the opponent quite well as we will see later on. With
    this approach, Q-learning can be effectively adapted to two-player games, allowing
    AI to not only learn optimal moves but also (eventually) adapt to the strategies
    of human opponents.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 可以采用几种方法来解决这个问题。一种方法是模拟对手所有可能的行动及其相应结果。这需要生成所有潜在后续状态的概率分布，并根据这些状态的预期结果更新 Q 值。然而，这种方法可能计算量较大。在本教程中，我们将采用一种更简单的方法，为对手随机采取一个动作，并根据这个动作的实际结果更新
    Q 表。这很好地反映了对手的不可预测性，正如我们后面将看到的那样。通过这种方法，Q-learning 可以有效地适应双人游戏，使 AI 不仅能够学习最佳移动，还能（最终）适应人类对手的策略。
- en: This methodology, in principle, mirrors the approach used to train [AlphaGo
    Zero](https://en.wikipedia.org/wiki/AlphaGo_Zero). This AI program played 4.9
    million games of Go against itself in rapid succession. During this process, it
    continuously improved its skill, learning and adapting strategies autonomously.
    This self-learning method, bypassing the need to simulate every possible move
    of the opponent, presents an efficient and effective way for AI systems to learn
    and adapt to complex tasks.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法原则上与训练 [AlphaGo Zero](https://en.wikipedia.org/wiki/AlphaGo_Zero) 的方法类似。该
    AI 程序在快速连续的对弈中自我对弈了 490 万局围棋。在这个过程中，它不断提高自己的技能，自主学习和调整策略。这种自学习方法，绕过了模拟对手每一个可能的移动的需求，为
    AI 系统提供了一种高效且有效的学习和适应复杂任务的方法。
- en: '![](../Images/bf55b0ed4cb304a37d134c2305ff771f.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bf55b0ed4cb304a37d134c2305ff771f.png)'
- en: 'Game 2 between Lee Sedol and AlphaGo with famous move 37 by AlphaGo. Image
    credit: [https://commons.wikimedia.org/wiki/File:Lee_Sedol_(W)_vs_AlphaGo_(B)_-_Game_2.svg](https://commons.wikimedia.org/wiki/File:Lee_Sedol_(W)_vs_AlphaGo_(B)_-_Game_2.svg)
    (Licence CC BY-SA 4.0)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 李世石与 AlphaGo 的第2局比赛，AlphaGo 的著名第37步。图片来源：[https://commons.wikimedia.org/wiki/File:Lee_Sedol_(W)_vs_AlphaGo_(B)_-_Game_2.svg](https://commons.wikimedia.org/wiki/File:Lee_Sedol_(W)_vs_AlphaGo_(B)_-_Game_2.svg)（许可
    CC BY-SA 4.0）
- en: In the next sections, we will delve into how these principles are applied specifically
    in the case of Tic-Tac-Toe, illustrating the implementation of Q-learning in an
    environment with two players.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将深入探讨这些原则如何在井字棋的具体情况下应用，展示在双人环境中 Q-learning 的实现。
- en: '**Q-Learning for Tic-Tac-Toe**'
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**井字棋的 Q-Learning**'
- en: As we venture into applying Q-learning to Tic-Tac-Toe, it’s important to understand
    the setup of our program and the environment in which our AI agent will be operating.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始将 Q-learning 应用于井字棋时，了解我们程序的设置以及 AI 代理将要操作的环境非常重要。
- en: Overview
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: The code is designed to train an AI (which we will call *Player 1* or *agent*)
    to play a Tic-Tac-Toe-like game using Q-learning, a form of reinforcement learning.
    It begins by setting up the learning parameters and initializing a Q-table to
    store the value of different actions in different states. The script defines several
    functions to manage the game’s mechanics, such as determining possible moves,
    checking for a winner, updating the game state, and calculating the next state
    and reward after a move.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码旨在训练一个 AI（我们称之为*玩家 1*或*智能体*），通过 Q 学习（一种强化学习形式）来玩类似井字棋的游戏。它首先设置学习参数并初始化一个
    Q 表来存储不同状态下不同动作的值。脚本定义了几个函数来管理游戏机制，如确定可能的移动、检查胜者、更新游戏状态，以及在移动后计算下一个状态和奖励。
- en: In the main part of the script, a Q-learning algorithm is implemented. It runs
    through numerous episodes, simulating games between theagent and its opponent
    (which we will call *player 2*). In each episode, the AI either explores a random
    move or exploits knowledge from the Q-table to make a move, learning from the
    outcomes to update the Q-table values. This process involves adjusting the exploration
    rate over time, shifting from random exploration to more strategic moves as it
    learns.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在脚本的主要部分中，实现了 Q 学习算法。它运行多个回合，模拟智能体与其对手（我们称之为*玩家 2*）之间的游戏。在每一回合中，AI 要么探索一个随机动作，要么利用
    Q 表中的知识来进行决策，从结果中学习以更新 Q 表的值。这个过程涉及随着时间的推移调整探索率，从随机探索转向更具策略性的动作。
- en: A key aspect of our setup is the AI’s opponent. Unlike more complex scenarios
    where the opponent might have a sophisticated strategy, our AI will be playing
    against an opponent that makes random moves. This choice simplifies the learning
    environment and allows us to focus on the AI’s learning process rather than the
    complexity of the opponent’s strategy.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设置的一个关键方面是 AI 的对手。与对手可能拥有复杂策略的更复杂场景不同，我们的 AI 将与一个随机移动的对手进行游戏。这一选择简化了学习环境，使我们可以专注于
    AI 的学习过程，而不是对手策略的复杂性。
- en: The Q-Learning Setup
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Q 学习设置
- en: 'Our Q-learning setup involves key parameters that will influence how the AI
    learns:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 Q 学习设置涉及一些关键参数，这些参数将影响 AI 的学习方式：
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Learning Rate (**`**learning_rate**`**):** This determines how much new information
    affects existing knowledge. A higher rate accelerates learning but can lead to
    instability. A rate of 0.2 strikes a balance between learning new strategies and
    retaining previous learning.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率 (**`**learning_rate**`**):** 这决定了新信息对现有知识的影响程度。较高的学习率加速了学习过程，但可能导致不稳定。学习率为
    0.2 在学习新策略和保留之前的学习之间取得了平衡。'
- en: '**Discount Factor (**`**discount_factor**`**):** This reflects the importance
    of future rewards, influencing how far-sighted the AI''s strategy will be. With
    a value of 0.9, it places significant emphasis on future rewards, encouraging
    the AI to think ahead rather than just focus on immediate gains.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**折扣因子 (**`**discount_factor**`**):** 这反映了未来奖励的重要性，影响 AI 策略的远见程度。折扣因子为 0.9
    时，AI 会特别重视未来奖励，鼓励 AI 前瞻性思考，而不仅仅关注即时收益。'
- en: '**Number of Episodes (**`**num_episodes**`**):** This is the number of games
    the AI will play to learn, providing ample opportunity for the AI to experience
    various game scenarios. Setting this to 10 million (`1e7`) allows extensive training,
    giving the AI ample opportunity to learn from a wide range of game scenarios.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回合数 (**`**num_episodes**`**):** 这是 AI 学习的游戏数量，为 AI 提供了充足的机会来体验各种游戏场景。将其设置为
    1000 万 (`1e7`) 允许广泛的训练，为 AI 提供了从各种游戏场景中学习的充足机会。'
- en: '**Exploration Rate (**`**epsilon**`**):** The exploration rate (epsilon) is
    initially set high to allow the AI to explore various actions, rather than solely
    exploiting known strategies. Initially, the AI will explore more (due to `epsilon`
    being 1.0). Over time, as `epsilon` decays towards `epsilon_min`, the AI will
    start exploiting its learned strategies more.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**探索率 (**`**epsilon**`**):** 探索率（epsilon）最初设置较高，以允许 AI 探索各种动作，而不是仅仅利用已知策略。最初，AI
    会更多地进行探索（由于 `epsilon` 为 1.0）。随着时间的推移，`epsilon` 逐渐减小到 `epsilon_min`，AI 将开始更多地利用其学习到的策略。'
- en: '**Side note on the exploration rate**'
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**关于探索率的附注**'
- en: ''
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The exploration rate in Q Learning, often represented by the symbol ε (epsilon),
    is a crucial parameter that dictates the balance between exploration (trying new
    actions) and exploitation (using the best-known actions). Initially, the agent
    doesn’t have much information about the environment, so it’s beneficial for it
    to explore widely by trying out different actions. The exploration rate, typically
    set to a high value at the start (e.g., 1 or close to it), determines the probability
    of the agent taking a random action instead of the best-known action according
    to the Q-table.
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在 Q 学习中，探索率通常用符号 ε（epsilon）表示，这是一个关键参数，决定了探索（尝试新动作）和利用（使用已知最佳动作）之间的平衡。最初，智能体对环境了解不多，因此它需要广泛探索，通过尝试不同的动作。探索率通常在开始时设定为较高的值（例如
    1 或接近 1），决定了智能体选择随机动作而不是根据 Q 表选择最佳已知动作的概率。
- en: ''
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: However, as the agent learns more about the environment and the Q-table becomes
    more reliable, it becomes less necessary to explore, and more beneficial to exploit
    the knowledge gained. This is where the exploration rate decay comes into play.
    The exploration rate decay is a factor by which the exploration rate is reduced
    over time. It ensures that as the agent learns and gathers more information, it
    gradually shifts from exploring the environment to exploiting the learned values
    in the Q-table.
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 然而，随着智能体对环境的了解越来越多，Q 表变得更加可靠，探索的必要性减少，利用已获得的知识变得更加有益。这时，探索率衰减就发挥作用了。探索率衰减是一个随着时间推移而减少探索率的因素。它确保智能体在学习和收集更多信息的过程中，逐渐从探索环境转向利用
    Q 表中学到的值。
- en: ''
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The reason why this balance is important in Q Learning is to avoid two main
    issues:'
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这种平衡在 Q 学习中很重要，因为它可以避免两个主要问题：
- en: ''
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Getting Stuck in Local Optima:** If the agent only exploits known information
    (low exploration), it might get stuck in local optima. This means it repeatedly
    chooses actions that seem best based on limited information but might miss out
    on discovering actions that lead to better long-term rewards.'
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**陷入局部最优：** 如果智能体只利用已知信息（低探索），可能会陷入局部最优。这意味着它会根据有限的信息反复选择看似最佳的动作，但可能错过发现能带来更好长期奖励的动作。'
- en: ''
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Inefficient Learning:** On the other hand, if the agent explores too much
    (high exploration) and for too long, it can lead to inefficient learning. The
    agent might keep trying suboptimal actions without sufficiently leveraging the
    knowledge it has already acquired, leading to slower convergence to the optimal
    policy.'
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**低效学习：** 另一方面，如果智能体过度探索（高探索）且时间过长，可能导致低效学习。智能体可能会不断尝试次优动作而没有充分利用已经获得的知识，从而导致收敛到最优策略的速度变慢。'
- en: ''
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: By appropriately setting the exploration rate and its decay, Q-learning algorithms
    can effectively balance these two aspects, allowing the agent to explore the environment
    initially and then gradually focus more on exploiting the best strategies it has
    learned. This balance is key to the efficiency and effectiveness of learning in
    complex environments.
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 通过适当设置探索率及其衰减，Q-learning 算法可以有效地平衡这两个方面，使智能体能够最初探索环境，然后逐渐更多地专注于利用它所学到的最佳策略。这种平衡对于在复杂环境中学习的效率和有效性至关重要。
- en: In the next sections, we will dive into the code to see how the AI uses Q-learning
    to make decisions, update its strategy, and ultimately aim to master Tic-Tac-Toe.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将深入代码，看看 AI 如何使用 Q-learning 来做决策、更新策略，并最终掌握 Tic-Tac-Toe。
- en: Code Deep Dive
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码深度解析
- en: Training Script
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练脚本
- en: This is a walkthrough of the [train.py](https://github.com/marshmellow77/tictactoe-q/blob/master/train.py)
    file in the GH repo.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 [train.py](https://github.com/marshmellow77/tictactoe-q/blob/master/train.py)
    文件的详细解读。
- en: 'The training starts with the for loop (roughly in the middle of the script)
    where we will play a certain number of episodes:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 训练从 for 循环开始（大致在脚本的中间），我们将在其中进行一定数量的回合：
- en: '[PRE1]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Following that, we determine the start player randomly. An even easier approach
    would have been to just make our agent always the start player. But implementing
    a random start player is not much more effort and generalises the Q-table mode,
    i.e. our agent will learn how to play as starting player but also as non-starting
    player.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们随机确定起始玩家。一个更简单的方法是让我们的智能体总是作为起始玩家。然而，实现一个随机起始玩家并不比直接总是让智能体作为起始玩家多花费多少精力，并且这种方法使
    Q 表模式更加通用，即我们的智能体将学习如何作为起始玩家以及非起始玩家进行游戏。
- en: 'If player 2 starts the game, then we start with making a random move for player
    2:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果玩家 2 开始游戏，那么我们将为玩家 2 进行随机移动：
- en: '[PRE2]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now we enter the actual training loop within a game of TTT which will only
    stop if the game has finished. A key mechanic is the exploitation vs exploration
    mechanic, which we discussed earlier. This is implemented like so:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们进入实际的 TTT 游戏训练循环，只有在游戏结束时才会停止。一个关键机制是之前讨论的开发 vs 探索机制。它的实现如下：
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The lower the epsilon value, the less the agent will explore by playing random
    moves and the more it will leverage the Q-table.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: epsilon 值越低，智能体通过随机移动进行的探索越少，它将更多地利用 Q 表。
- en: 'Once the agent’s action has been chosen, we will execute it and determine the
    next state (and any rewards if applicable):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦选择了智能体的动作，我们将执行它并确定下一状态（以及适用的奖励）：
- en: '[PRE4]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The function that does all this warrants a closer look:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 处理所有这些操作的函数值得更仔细地查看：
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this function we first update the state of the board and check if our agent
    has won the game. If that is not the case, then we play a random move for the
    opponent and check again if the opponent has won the game. Depending on the outcome
    we return a reward of 0 (game still ongoing), 0.1 (draw), +1 (agent won), or -1
    (opponent won). The reason we choose 0.1 as a reward for a draw is so that the
    agent is incentivised to end a game quickly.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数中，我们首先更新棋盘的状态并检查我们的智能体是否赢得了游戏。如果没有，我们为对手进行随机移动，并再次检查对手是否赢得了游戏。根据结果，我们返回
    0（游戏仍在进行中）、0.1（平局）、+1（智能体获胜）或 -1（对手获胜）。我们选择 0.1 作为平局的奖励是为了激励智能体尽快结束游戏。
- en: 'Now that we have determined the reward come the most crucial part of the entire
    program: Updating the Q-table via the Bellman equation:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确定了奖励，接下来是整个程序中最关键的部分：通过 Bellman 方程更新 Q 表：
- en: '[PRE6]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This Bellman equation is much better explained in other blog posts (again,
    refer back to [https://towardsdatascience.com/q-learning-for-beginners-2837b777741](/q-learning-for-beginners-2837b777741)).
    But for a very brief explanation:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 Bellman 方程在其他博客文章中解释得更好（再次参考 [https://towardsdatascience.com/q-learning-for-beginners-2837b777741](/q-learning-for-beginners-2837b777741)）。但简要解释如下：
- en: 'As discussed earlier, the Q-table is essentially a big cheatsheet: It keeps
    track of all the possible states in the game and the value of each possible move
    from that state. It tells the agent how good each move is in a given situation,
    based on what it has learned so far.'
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如前所述，Q 表本质上是一个大的备忘单：它跟踪游戏中的所有可能状态以及从该状态开始的每个可能移动的价值。它告诉智能体在给定情况下每个移动的好坏，基于它迄今为止学到的知识。
- en: ''
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Bellman equation updates this Q-table. It does this by looking at the *immediate*
    rewards the agent receives (winning, losing, drawing the game) and the quality
    of future states (i.e. *future* rewards) it can move to. So, after each game,
    the agent uses the Bellman equation to revise its Q-table, learning which moves
    are likely to lead to a win, a loss, or a draw.
  id: totrans-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Bellman 方程更新这个 Q 表。它通过查看智能体收到的*即时*奖励（赢、输、平局）和它可以移动到的未来状态（即*未来*奖励）的质量来实现。因此，在每局游戏后，智能体使用
    Bellman 方程来修订其 Q 表，学习哪些移动可能导致胜利、失败或平局。
- en: Lastly, we adjust the exploration rate, so that in future plays the agent uses
    the Q table more and explores less.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们调整探索率，以便在未来的游戏中，智能体更多地使用 Q 表而较少进行探索。
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Running the Training
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行训练
- en: Once we have the training script ready, we can execute it. Fortunately, this
    process is not computationally demanding and completes very quickly, requiring
    no special computing power. I executed this on a MacBook M1 Air, for example,
    and it concluded within 5 minutes for 10 million games. Once the training is complete,
    we will save the Q table (which is not particularly large) so that we can use
    it to test the agent, play games against the AI, and potentially continue training
    at a later stage to further enhance the table. Let’s have a look at it 🧐
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练脚本准备好，我们就可以执行它。幸运的是，这个过程计算需求不高，完成得非常快，不需要特别的计算能力。例如，我在 MacBook M1 Air 上执行了这个过程，它在
    1000 万局游戏中不到 5 分钟就完成了。训练完成后，我们将保存 Q 表（它不是特别大），以便我们可以用它来测试智能体，与 AI 对战，并可能在稍后的阶段继续训练，以进一步增强表格。我们来看看吧
    🧐
- en: Manual Inspection of Q table
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q 表的人工检查
- en: 'The table is relatively easy to understand: Each row represents the board state
    and the actions that can be taken and their quality. Let’s have a look at some
    interesting states. Note that your table will probably have different (but hopefully
    similar) values:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表格相对容易理解：每一行代表了棋盘状态、可采取的行动及其质量。让我们来看看一些有趣的状态。请注意，你的表格可能会有不同（但希望是相似）的值：
- en: '![](../Images/f47b55da1649ba7c93d541cd6511960e.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f47b55da1649ba7c93d541cd6511960e.png)'
- en: Image by author
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: The board state shows where each player has placed already (the first 3 numbers
    represent the first row, the next 3 the second row, and the last 3 the last row.
    The actions correspond to the positions on the board, and the number for each
    action indicates the quality of that action. In this example we see a state where
    it looks like only one move (action 7) is considered good, all other moves seem
    losing.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 棋盘状态显示了每个玩家已经放置的位置（前3个数字代表第一行，接下来的3个代表第二行，最后3个代表最后一行。动作对应棋盘上的位置，每个动作的数字表示该动作的质量。在这个例子中，我们看到一个状态，似乎只有一个动作（动作7）被认为是好的，其他所有动作都显得较差。
- en: 'NB: The indices for the board positions are as follows:'
  id: totrans-95
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：棋盘位置的索引如下：
- en: '![](../Images/3d7945604a7896490040cbf64047977d.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d7945604a7896490040cbf64047977d.png)'
- en: Image by author
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'So, let’s visualise the board state for this particular entry in the Q-table:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们来可视化Q表中的这个特定条目的棋盘状态：
- en: '![](../Images/5fd8c1971270157298573d23b371f20c.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5fd8c1971270157298573d23b371f20c.png)'
- en: Image by author
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Indeed, the only good move for our agent (player 1) in this position is to choose
    position 7\. All the other moves might potentially lead to losing the game (remember
    that player 2 will play a random move on its next turn, so a loss is not guaranteed).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 的确，在这个位置，代理（玩家1）唯一的好选择是选择位置7。所有其他移动可能会导致输掉比赛（请记住，玩家2将在下一轮随机移动，因此输掉比赛并非必然）。
- en: 'Let’s look at one more example:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再看一个例子：
- en: '![](../Images/1c365fcb2f84711d6b1ca0d27a1629ce.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c365fcb2f84711d6b1ca0d27a1629ce.png)'
- en: Image by author
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: '![](../Images/854666655addde48dd4dfeb51047bb9f.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/854666655addde48dd4dfeb51047bb9f.png)'
- en: Image by author
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: In this example the best move is obviously to play in position 8 (bottom right)
    and win the game. If the agent were to play any other move, it is likely that
    it will lose the game. Therefore the Q table will inform our agent to take action
    8.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，显然最佳移动是选择位置8（右下角）并赢得比赛。如果代理选择其他位置，它很可能会输掉比赛。因此，Q表将指示我们的代理采取动作8。
- en: Testing the New Agent
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试新代理
- en: Now that we’ve trained the model, we can test it with a the script [test.py](https://github.com/marshmellow77/tictactoe-q/blob/master/test.py)
    in the GH repo. In it we will let the agent a play a number of games against an
    opponent that plays random moves and see how well it performs. We start by initialising
    our agent and load the Q-table to use it for decision-making in a game environment.
    The `play_game` function simulates a game, using the loaded Q-table to guide the
    agent's decisions. The game environment here is a simple 3x3 board where each
    state represents a different configuration of the board.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练了模型，我们可以用GH仓库中的脚本[test.py](https://github.com/marshmellow77/tictactoe-q/blob/master/test.py)来测试它。在脚本中，我们将让代理与一个随机移动的对手进行若干局比赛，看看它的表现如何。我们首先初始化我们的代理并加载Q表以便在游戏环境中用于决策。`play_game`函数模拟了一场比赛，使用加载的Q表来指导代理的决策。这里的游戏环境是一个简单的3x3棋盘，每个状态代表棋盘的不同配置。
- en: The agent, which plays as Player 1, makes decisions based on the Q-table — choosing
    the action with the highest value in the current state. If a state is not found
    in the Q-table, the agent makes a random move. This combination of learned behaviour
    and randomness helps to evaluate the robustness of the training. Player 2’s moves
    are entirely random, providing a varied set of scenarios for the agent to navigate.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 代理以玩家1的身份，根据Q表做出决策——选择当前状态下值最高的行动。如果在Q表中找不到状态，代理将做出随机移动。这种学习行为和随机性的结合有助于评估训练的鲁棒性。玩家2的移动完全随机，为代理提供了多样化的场景。
- en: The outcomes of these games are then tracked, quantifying the number of wins,
    losses, and draws. This helps in assessing the effectiveness of the trained model.
    If the `log_lost_games` flag is set, detailed logs of the games where the agent
    lost are saved, which can be invaluable for further analysis and improvement of
    the model. This testing process, playing a substantial number of games, gives
    a comprehensive view of the agent's capabilities post-training.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这些游戏的结果会被跟踪，量化胜利、失败和平局的数量。这有助于评估训练模型的效果。如果设置了`log_lost_games`标志，将保存详细的失败游戏日志，这对于进一步分析和改进模型是非常宝贵的。这一测试过程，通过进行大量游戏，提供了对训练后代理能力的全面了解。
- en: '![](../Images/e88dbab492830f205ffd07e030e1ebd6.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e88dbab492830f205ffd07e030e1ebd6.png)'
- en: Image by author
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Playing a Game Against the AI
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与AI对战
- en: It looks like the test against a random bot went well. Our AI managed to win
    more than 95% of the games. Now, we want to play against the AI ourselves. We
    can use [play.py](https://github.com/marshmellow77/tictactoe-q/blob/master/play.py)
    in the GH repo to do that.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来对随机机器人进行的测试很成功。我们的AI赢得了超过95%的比赛。现在，我们想亲自与AI对战。我们可以使用[play.py](https://github.com/marshmellow77/tictactoe-q/blob/master/play.py)来实现这一点。
- en: In this program, we interact with the AI through a simple console interface.
    The game board is represented as a 3x3 grid, with each position numbered from
    0 to 8\. When it’s our turn, we’ll be prompted to enter the number corresponding
    to the position where we want to make our move.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个程序中，我们通过一个简单的控制台界面与AI互动。游戏板表示为一个3x3的网格，每个位置从0到8编号。当轮到我们时，我们会被提示输入一个数字，以选择我们想要移动的位置。
- en: The AI uses the Q-table loaded from a CSV file to make its decisions. This Q-table,
    derived from the previous training process, guides the AI to choose the best possible
    move based on the current state of the game board. If the AI encounters a state
    not present in the Q-table, it defaults to making a random move.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: AI使用从CSV文件加载的Q表来做出决策。这个Q表来源于之前的训练过程，引导AI根据当前的游戏板状态选择最佳可能的移动。如果AI遇到Q表中没有的状态，它将默认进行随机移动。
- en: The game alternates between our turn and the AI’s turn. After each move, the
    updated board is displayed, and the program checks for a winner. If a player wins
    or the game results in a draw, the game ends, and the outcome is announced — whether
    it’s a win for us, a win for the AI, or a draw.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏在我们的回合和AI的回合之间交替进行。每次移动后，更新后的棋盘会被显示，程序会检查是否有赢家。如果玩家获胜或游戏结果为平局，游戏结束，结果将被宣布——无论是我们获胜、AI获胜还是平局。
- en: 'This interactive game provides a great opportunity to test the AI’s capabilities
    in real-time. Let’s get started:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这个互动游戏提供了一个很好的机会来实时测试AI的能力。让我们开始吧：
- en: '![](../Images/eac09908ab709e6708a660a156074384.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eac09908ab709e6708a660a156074384.png)'
- en: Image by author
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: In this game, if we don’t choose action 0 (top left corner) the AI will have
    a chance to win the game. Will it realise that?
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个游戏中，如果我们不选择动作0（左上角），AI将有机会赢得比赛。它会意识到这一点吗？
- en: '![](../Images/3bf5a11009097edc79b18d607d9ef811.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3bf5a11009097edc79b18d607d9ef811.png)'
- en: Image by author
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: It did! Nice 😊
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 确实做到了！很好😊
- en: '**Conclusion**'
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**结论**'
- en: In this post we trained our AI agent against a player that plays random moves.
    This was already good enough to achieve a win rate of more than 95% against an
    opponent that plays random moves. But there are ways to improve the training process
    and hopefully also the performance of the AI.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们训练了我们的AI代理对抗一个进行随机移动的玩家。这已经足够好，能够在对抗进行随机移动的对手时达到超过95%的胜率。但是，有方法可以改进训练过程，希望也能提高AI的表现。
- en: The Impact of Parameter Tuning
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参数调整的影响
- en: 'The journey of applying Q-learning to Tic-Tac-Toe reveals a crucial aspect
    of RL: the art of fine-tuning parameters. Getting these parameters right, such
    as the balance between exploitation and exploration, the learning rate, and the
    discount factor, is key to the success of an RL agent.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 将Q学习应用于井字游戏揭示了强化学习的一个关键方面：调整参数的艺术。正确设置这些参数，如开发与探索之间的平衡、学习率和折扣因子，是RL代理成功的关键。
- en: '**Exploration vs. Exploitation:** Controlled by the `epsilon` value, this balance
    dictates how often the agent tries new strategies versus relying on known strategies.
    A high exploration rate encourages the agent to try new things, potentially leading
    to innovative strategies, while a high exploitation rate makes the agent rely
    on its existing knowledge, which can be efficient but may miss out on better strategies.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**探索与利用：** 由`epsilon`值控制，这一平衡决定了智能体尝试新策略的频率与依赖已知策略的比例。高探索率鼓励智能体尝试新事物，可能导致创新策略，而高利用率使智能体依赖现有知识，虽然可能更高效，但可能会错过更好的策略。'
- en: '**Learning Rate:** A high learning rate means the agent quickly adopts new
    information, which can be beneficial in dynamic environments but may lead to instability
    if the agent overwrites useful learnings too rapidly. Conversely, a low learning
    rate means the agent relies more on past knowledge, leading to stable but potentially
    slower learning.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率：** 高学习率意味着智能体迅速采纳新信息，这在动态环境中可能有利，但如果智能体过快地覆盖有用的学习，可能导致不稳定。相反，低学习率意味着智能体更多依赖过去的知识，导致稳定但可能较慢的学习。'
- en: '**Discount Factor:** This parameter influences how much the agent values future
    rewards. A high discount factor makes the agent forward-thinking, considering
    the long-term consequences of its actions. A low discount factor, on the other
    hand, makes the agent short-sighted, focusing on immediate rewards.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**折扣因子：** 这个参数影响智能体对未来奖励的重视程度。高折扣因子使智能体更具前瞻性，考虑其行动的长期后果。相反，低折扣因子则使智能体目光短浅，专注于即时奖励。'
- en: Changes in these parameters can significantly alter the behaviour of the RL
    agent. For instance, an agent with a low discount factor might play Tic-Tac-Toe
    aggressively, focusing on immediate wins rather than setting up future strategies.
    In contrast, an agent with a high discount factor might play more strategically,
    considering the implications of each move on the future state of the game.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数的变化可以显著改变RL智能体的行为。例如，折扣因子低的智能体可能会以攻击性方式玩井字棋，专注于即时胜利，而不是制定未来的策略。相反，折扣因子高的智能体可能会更具策略性，考虑每一步对游戏未来状态的影响。
- en: Similarly, an agent with a high learning rate might rapidly adapt to new strategies,
    constantly evolving its gameplay, while one with a low learning rate might stick
    to tried-and-tested strategies, showing less variation in its game.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，高学习率的智能体可能迅速适应新策略，不断发展其游戏玩法，而低学习率的智能体可能坚持经过验证的策略，游戏中的变化较小。
- en: Your Turn to Experiment
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 轮到你来实验了
- en: This is where the true excitement lies in reinforcement learning. Each parameter
    can be tweaked to observe how it affects the learning and performance of the AI
    agent. I invite you to dive into this world of experimentation. Adjust the learning
    rate, exploration rate, and discount factor. Observe how these changes impact
    the AI’s strategy in playing Tic-Tac-Toe.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是强化学习真正的激动所在。每个参数都可以进行微调，以观察它如何影响AI智能体的学习和表现。我邀请你深入这个实验的世界。调整学习率、探索率和折扣因子，观察这些变化如何影响AI在井字棋游戏中的策略。
- en: More Advanced Techniques
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更高级的技术
- en: To further improve the model’s performance, implementing a self-play mechanism,
    where the AI plays against versions of itself from different stages of training
    (rather than playing agains an opponent that makes random moves), could be an
    effective strategy. This approach has been successfully used in systems like AlphaGo
    and could lead to a more robust and adaptable AI player.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步提高模型的表现，实施自我对弈机制，即AI与来自不同训练阶段的自身版本对弈（而不是与进行随机移动的对手对弈），可能是一种有效的策略。这种方法在AlphaGo等系统中成功应用过，并可能导致更强大和适应性更强的AI玩家。
- en: For more complex games, like Chess and Go, maintaining a Q-table will not be
    feasible any more as it becomes too big. In these games, incorporating techniques
    like Deep Q-learning could significantly enhance the AI’s learning capability.
    By using a neural network to approximate the Q-table, the AI can handle more complex
    states beyond the simple 3x3 Tic-Tac-Toe grid, making it scalable for more complicated
    games.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更复杂的游戏，如国际象棋和围棋，维持一个Q表将不再可行，因为它变得过于庞大。在这些游戏中，采用像深度Q学习这样的技术可以显著增强AI的学习能力。通过使用神经网络来逼近Q表，AI可以处理超出简单3x3井字棋网格的更复杂状态，使其在更复杂的游戏中具备可扩展性。
- en: In conclusion, the current setup has already shown promising results. These
    suggested improvements, however, could elevate the AI’s performance further, transforming
    it from a competent Tic-Tac-Toe player into a sophisticated AI capable of tackling
    more complex strategic games.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，目前的设置已经展示了有希望的结果。然而，这些建议的改进可能会进一步提升AI的表现，将其从一个合格的井字棋玩家转变为一个能够应对更复杂战略游戏的高级AI。
- en: Further Material on the Topic
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步的相关资料
- en: 'If you are interested in learning more about how RL is being used for board
    games, check out the two videos below. The first one is quite short and dives
    into how modern chess AI bots are playing the game:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对学习更多关于强化学习如何应用于棋盘游戏感兴趣，可以查看下面的两个视频。第一个视频非常简短，*深入探讨了现代象棋 AI 机器人如何进行游戏*：
- en: 'The second video is the movie *AlphaGo* (which is free on Youtube), and tells
    story of how AlphaGo model was developed and how it beat the world champion at
    the time:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个视频是电影*AlphaGo*（在YouTube上免费观看），讲述了AlphaGo模型的开发过程以及它如何击败当时的世界冠军：
- en: Heiko Hotz
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Heiko Hotz
- en: 👋 Follow me on [Medium](https://heiko-hotz.medium.com/) and [LinkedIn](https://www.linkedin.com/in/heikohotz/)
    to read more about Generative AI, Machine Learning, and Natural Language Processing.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 👋 在[Medium](https://heiko-hotz.medium.com/)和[LinkedIn](https://www.linkedin.com/in/heikohotz/)关注我，阅读更多关于生成
    AI、机器学习和自然语言处理的内容。
- en: 👥 If you’re based in London join one of our [NLP London Meetups](https://www.meetup.com/nlp_london/).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 👥 如果你在伦敦，可以参加我们的[NLP London Meetups](https://www.meetup.com/nlp_london/)。
- en: '![](../Images/adb4c021b5bd4b11847ab1787aefa7aa.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/adb4c021b5bd4b11847ab1787aefa7aa.png)'
