- en: Teaching AI to Play Board Games
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ•™äººå·¥æ™ºèƒ½ç©æ£‹ç›˜æ¸¸æˆ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/teaching-ai-to-play-board-games-77e5d1749dd9](https://towardsdatascience.com/teaching-ai-to-play-board-games-77e5d1749dd9)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/teaching-ai-to-play-board-games-77e5d1749dd9](https://towardsdatascience.com/teaching-ai-to-play-board-games-77e5d1749dd9)
- en: Using reinforcement learning from scratch to teach a computer to play Tic-Tac-Toe
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä»é›¶å¼€å§‹çš„å¼ºåŒ–å­¦ä¹ æ•™è®¡ç®—æœºç©äº•å­—æ£‹
- en: '[](https://heiko-hotz.medium.com/?source=post_page-----77e5d1749dd9--------------------------------)[![Heiko
    Hotz](../Images/d08394d46d41d5cd9e76557a463be95e.png)](https://heiko-hotz.medium.com/?source=post_page-----77e5d1749dd9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----77e5d1749dd9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----77e5d1749dd9--------------------------------)
    [Heiko Hotz](https://heiko-hotz.medium.com/?source=post_page-----77e5d1749dd9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://heiko-hotz.medium.com/?source=post_page-----77e5d1749dd9--------------------------------)[![Heiko
    Hotz](../Images/d08394d46d41d5cd9e76557a463be95e.png)](https://heiko-hotz.medium.com/?source=post_page-----77e5d1749dd9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----77e5d1749dd9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----77e5d1749dd9--------------------------------)
    [Heiko Hotz](https://heiko-hotz.medium.com/?source=post_page-----77e5d1749dd9--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----77e5d1749dd9--------------------------------)
    Â·18 min readÂ·Dec 12, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----77e5d1749dd9--------------------------------)
    Â·18 åˆ†é’Ÿé˜…è¯»Â·2023å¹´12æœˆ12æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/60b0afcb68a3b3d4c14f8bdf7964d2ef.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/60b0afcb68a3b3d4c14f8bdf7964d2ef.png)'
- en: Image by author (created with ChatGPT)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›ï¼ˆç”± ChatGPT åˆ›å»ºï¼‰
- en: What is this about?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä»€ä¹ˆå†…å®¹ï¼Ÿ
- en: It appears that everyone in the AI sector is currently honing their Reinforcement
    Learning (RL) skills, especially in Q-learning, following the recent rumours about
    OpenAIâ€™s new AI model, [*Q**](https://en.wikipedia.org/wiki/OpenAI#Q*)and Iâ€™m
    joining in too. However, rather than speculating about *Q** or revisiting old
    papers and examples for Q-learning, Iâ€™ve decided to use my enthusiasm for board
    games to give an introduction to Q-learning ğŸ¤“
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰ï¼Œäººå·¥æ™ºèƒ½é¢†åŸŸä¼¼ä¹æ¯ä¸ªäººéƒ½åœ¨æå‡ä»–ä»¬çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æŠ€èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ Q-learning æ–¹é¢ï¼Œè·Ÿéšå…³äº OpenAI æ–° AI æ¨¡å‹ [*Q**](https://en.wikipedia.org/wiki/OpenAI#Q*)
    çš„æœ€æ–°ä¼ é—»ï¼Œæˆ‘ä¹Ÿå‚ä¸å…¶ä¸­ã€‚ç„¶è€Œï¼Œæˆ‘å†³å®šç”¨æˆ‘å¯¹æ£‹ç›˜æ¸¸æˆçš„çƒ­æƒ…æ¥ä»‹ç» Q-learning ğŸ¤“ï¼Œè€Œä¸æ˜¯å¯¹ *Q** è¿›è¡ŒçŒœæµ‹æˆ–é‡æ¸© Q-learning çš„æ—§è®ºæ–‡å’Œç¤ºä¾‹ã€‚
- en: In this blog post, I will create a simple programme from scratch to teach a
    model how to play Tic-Tac-Toe (TTT). I will refrain from using any RL libraries
    like [*Gym*](https://github.com/openai/gym) or [*Stable Baselines*](https://github.com/DLR-RM/stable-baselines3);
    everything is hand-coded in native Python, and the script is merely 100 lines
    long. If youâ€™re curious about how to instruct an AI to play games, keep reading.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡åšå®¢æ–‡ç« ä¸­ï¼Œæˆ‘å°†ä»å¤´å¼€å§‹åˆ›å»ºä¸€ä¸ªç®€å•çš„ç¨‹åºï¼Œæ•™ä¸€ä¸ªæ¨¡å‹å¦‚ä½•ç©äº•å­—æ£‹ï¼ˆTTTï¼‰ã€‚æˆ‘å°†é¿å…ä½¿ç”¨ä»»ä½•å¼ºåŒ–å­¦ä¹ åº“ï¼Œæ¯”å¦‚ [*Gym*](https://github.com/openai/gym)
    æˆ– [*Stable Baselines*](https://github.com/DLR-RM/stable-baselines3)ï¼›æ‰€æœ‰å†…å®¹éƒ½æ˜¯ç”¨åŸç”Ÿ
    Python æ‰‹åŠ¨ç¼–å†™çš„ï¼Œè„šæœ¬åªæœ‰ 100 è¡Œã€‚å¦‚æœä½ å¯¹å¦‚ä½•æŒ‡å¯¼äººå·¥æ™ºèƒ½ç©æ¸¸æˆæ„Ÿåˆ°å¥½å¥‡ï¼Œè¯·ç»§ç»­é˜…è¯»ã€‚
- en: You can find all the code on GitHub at [https://github.com/marshmellow77/tictactoe-q](https://github.com/marshmellow77/tictactoe-q).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨ GitHub ä¸Šæ‰¾åˆ°æ‰€æœ‰ä»£ç ï¼Œé“¾æ¥ä¸º [https://github.com/marshmellow77/tictactoe-q](https://github.com/marshmellow77/tictactoe-q)ã€‚
- en: Why is it important?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦ï¼Ÿ
- en: Teaching an AI to play Tic-Tac-Toe (TTT) might not seem all that important.
    However, it does provide a (hopefully) clear and understandable introduction to
    Q-learning and RL, which might be important in the field of Generative AI (GenAI)
    since there has been speculation that stand-alone GenAI models, such as GPT-4,
    are insufficient for significant advancements. They are limited by the fact that
    they can only ever predict the next token and not being able to reason at all.
    RL is believed to be able to address this issue and potentially enhance the responses
    from GenAI models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æ•™äººå·¥æ™ºèƒ½ç©äº•å­—æ£‹ï¼ˆTTTï¼‰å¯èƒ½çœ‹èµ·æ¥å¹¶ä¸é‚£ä¹ˆé‡è¦ã€‚ç„¶è€Œï¼Œå®ƒç¡®å®æä¾›äº†ä¸€ä¸ªï¼ˆå¸Œæœ›ï¼‰æ¸…æ™°ä¸”æ˜“äºç†è§£çš„ Q-learning å’Œ RL çš„ä»‹ç»ï¼Œè¿™åœ¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰é¢†åŸŸå¯èƒ½æ˜¯é‡è¦çš„ï¼Œå› ä¸ºæœ‰äººçŒœæµ‹åƒ
    GPT-4 è¿™æ ·çš„ç‹¬ç«‹ GenAI æ¨¡å‹å¯¹äºæ˜¾è‘—çš„è¿›æ­¥æ˜¯ä¸å¤Ÿçš„ã€‚å®ƒä»¬çš„å±€é™æ€§åœ¨äºåªèƒ½é¢„æµ‹ä¸‹ä¸€ä¸ªæ ‡è®°ï¼Œè€Œæ— æ³•è¿›è¡Œä»»ä½•æ¨ç†ã€‚RL è¢«è®¤ä¸ºèƒ½å¤Ÿè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¹¶å¯èƒ½å¢å¼º
    GenAI æ¨¡å‹çš„å“åº”èƒ½åŠ›ã€‚
- en: But whether youâ€™re aiming to brush up on your RL skills in anticipation of these
    advancements, or youâ€™re simply seeking an engaging introduction to Q-learning,
    this tutorial is designed for both scenarios ğŸ¤—
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æ— è®ºä½ æ˜¯ä¸ºäº†è¿æ¥è¿™äº›è¿›å±•è€Œæå‡ä½ çš„RLæŠ€èƒ½ï¼Œè¿˜æ˜¯ä»…ä»…å¯»æ±‚ä¸€ä¸ªæœ‰è¶£çš„Qå­¦ä¹ å…¥é—¨æ•™ç¨‹ï¼Œè¿™ä¸ªæ•™ç¨‹éƒ½é€‚åˆè¿™ä¸¤ç§æƒ…å†µğŸ¤—
- en: '**Understanding Q-Learning**'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ç†è§£Qå­¦ä¹ **'
- en: At its core, Q-learning is an algorithm that learns the value of an action in
    a particular state, and then uses this information to find the best action. Letâ€™s
    consider the example of the *Frozen Lake* game, a popular single-player game used
    to demonstrate Q-learning.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æœ¬è´¨ä¸Šè®²ï¼ŒQå­¦ä¹ æ˜¯ä¸€ç§ç®—æ³•ï¼Œå®ƒå­¦ä¹ ç‰¹å®šçŠ¶æ€ä¸‹ä¸€ä¸ªåŠ¨ä½œçš„ä»·å€¼ï¼Œç„¶ååˆ©ç”¨è¿™äº›ä¿¡æ¯æ‰¾åˆ°æœ€ä½³åŠ¨ä½œã€‚è®©æˆ‘ä»¬è€ƒè™‘*Frozen Lake*æ¸¸æˆçš„ä¾‹å­ï¼Œè¿™æ˜¯ä¸€æ¬¾ç”¨äºæ¼”ç¤ºQå­¦ä¹ çš„æµè¡Œå•äººæ¸¸æˆã€‚
- en: 'In Frozen Lake, the player (starting at cell 0) navigates across a grid of
    ice and water cells, aiming to reach a goal (cell 15) without falling into the
    water. Each cell represents a state, and the player can move in four directions:
    up, down, left, or right.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Frozen Lakeä¸­ï¼Œç©å®¶ï¼ˆä»å•å…ƒæ ¼0å¼€å§‹ï¼‰åœ¨å†°å’Œæ°´çš„ç½‘æ ¼ä¸Šç§»åŠ¨ï¼Œç›®æ ‡æ˜¯åˆ°è¾¾ç›®æ ‡ï¼ˆå•å…ƒæ ¼15ï¼‰è€Œä¸æ‰å…¥æ°´ä¸­ã€‚æ¯ä¸ªå•å…ƒæ ¼ä»£è¡¨ä¸€ä¸ªçŠ¶æ€ï¼Œç©å®¶å¯ä»¥å‘å››ä¸ªæ–¹å‘ç§»åŠ¨ï¼šä¸Šã€ä¸‹ã€å·¦æˆ–å³ã€‚
- en: '![](../Images/445f990a7fb1ddd8e01d917fc85bd76b.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/445f990a7fb1ddd8e01d917fc85bd76b.png)'
- en: Image by author (created with Stable Diffusion)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡ï¼ˆä½¿ç”¨Stable Diffusionåˆ›å»ºï¼‰
- en: At the beginning of the game the ***agent*** (thatâ€™s what an AI player is usually
    called) has no information and will just try out some moves randomly. In the context
    of Q-learning, this exploration phase is crucial. The agent learns by receiving
    rewards or penalties based on its actions. In Frozen Lake, reaching the goal earns
    a high reward, while falling into water results in a penalty. This system of rewards
    and penalties guides the agent in learning the most effective path to the goal.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¸¸æˆå¼€å§‹æ—¶ï¼Œ***ä»£ç†***ï¼ˆè¿™å°±æ˜¯AIç©å®¶é€šå¸¸çš„ç§°å‘¼ï¼‰æ²¡æœ‰ä»»ä½•ä¿¡æ¯ï¼Œåªä¼šéšæœºå°è¯•ä¸€äº›åŠ¨ä½œã€‚åœ¨Qå­¦ä¹ çš„èƒŒæ™¯ä¸‹ï¼Œè¿™ä¸ªæ¢ç´¢é˜¶æ®µè‡³å…³é‡è¦ã€‚ä»£ç†é€šè¿‡æ ¹æ®å…¶åŠ¨ä½œè·å¾—å¥–åŠ±æˆ–æƒ©ç½šæ¥å­¦ä¹ ã€‚åœ¨Frozen
    Lakeä¸­ï¼Œè¾¾åˆ°ç›®æ ‡ä¼šè·å¾—é«˜å¥–åŠ±ï¼Œè€Œæ‰å…¥æ°´ä¸­åˆ™ä¼šå—åˆ°æƒ©ç½šã€‚è¿™ç§å¥–åŠ±å’Œæƒ©ç½šçš„ç³»ç»Ÿå¼•å¯¼ä»£ç†å­¦ä¹ æœ€æœ‰æ•ˆçš„åˆ°è¾¾ç›®æ ‡çš„è·¯å¾„ã€‚
- en: Q-learning uses a table, known as a Q-table, to record the value of each action
    in each state. This table is updated continuously as the agent explores the environment.
    The Q-table entries, known as Q-values, represent the expected utility of taking
    a certain action in a given state, and they are updated using the [Bellman Equation](https://en.wikipedia.org/wiki/Bellman_equation).
    This equation considers the immediate reward from an action and the highest possible
    future rewards (more about this later).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Qå­¦ä¹ ä½¿ç”¨ä¸€ä¸ªè¡¨æ ¼ï¼Œç§°ä¸ºQ-è¡¨ï¼Œç”¨äºè®°å½•æ¯ä¸ªçŠ¶æ€ä¸‹æ¯ä¸ªåŠ¨ä½œçš„ä»·å€¼ã€‚éšç€ä»£ç†æ¢ç´¢ç¯å¢ƒï¼Œè¿™ä¸ªè¡¨æ ¼ä¼šä¸æ–­æ›´æ–°ã€‚Q-è¡¨æ¡ç›®ï¼Œç§°ä¸ºQå€¼ï¼Œè¡¨ç¤ºåœ¨ç»™å®šçŠ¶æ€ä¸‹é‡‡å–æŸä¸ªåŠ¨ä½œçš„é¢„æœŸæ•ˆç”¨ï¼Œå®ƒä»¬é€šè¿‡[è´å°”æ›¼æ–¹ç¨‹](https://en.wikipedia.org/wiki/Bellman_equation)è¿›è¡Œæ›´æ–°ã€‚è¿™ä¸ªæ–¹ç¨‹è€ƒè™‘äº†åŠ¨ä½œçš„å³æ—¶å¥–åŠ±å’Œå¯èƒ½çš„æœ€é«˜æœªæ¥å¥–åŠ±ï¼ˆç¨åä¼šè¯¦ç»†è®²è§£ï¼‰ã€‚
- en: 'Basically, the Q-table is a cheatsheet or lookup table for the agent: Depending
    in which state the game is, the agent will look up that state, determine which
    action has the most utility (i.e. which is the best action to take) and then executes
    that action. Below an illustration of how the Q-table could look like:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºæœ¬ä¸Šï¼ŒQ-è¡¨æ˜¯ä»£ç†çš„å¤‡å¿˜å•æˆ–æŸ¥æ‰¾è¡¨ï¼šæ ¹æ®æ¸¸æˆçš„çŠ¶æ€ï¼Œä»£ç†ä¼šæŸ¥æ‰¾è¯¥çŠ¶æ€ï¼Œç¡®å®šå“ªä¸ªåŠ¨ä½œå…·æœ‰æœ€é«˜æ•ˆç”¨ï¼ˆå³å“ªä¸ªæ˜¯æœ€ä½³åŠ¨ä½œï¼‰ï¼Œç„¶åæ‰§è¡Œè¯¥åŠ¨ä½œã€‚ä»¥ä¸‹æ˜¯Q-è¡¨å¯èƒ½çš„ç¤ºä¾‹ï¼š
- en: '![](../Images/1b92d5b9413528dfdb03ec6cdcf27250.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b92d5b9413528dfdb03ec6cdcf27250.png)'
- en: Image by author
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: In this example the player would choose the action *Right* if it was in state
    1 (i.e. on cell 1) because it is the action with the highest vale.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œå¦‚æœç©å®¶å¤„äºçŠ¶æ€1ï¼ˆå³åœ¨å•å…ƒæ ¼1ï¼‰ï¼Œä»–ä¼šé€‰æ‹©åŠ¨ä½œ*å³*ï¼Œå› ä¸ºè¿™æ˜¯å…·æœ‰æœ€é«˜ä»·å€¼çš„åŠ¨ä½œã€‚
- en: Over time, as the agent explores the environment and updates the Q-table, it
    becomes more adept at navigating the Frozen Lake, eventually learning an optimal
    or near-optimal policy to reach the goal reliably. The beauty of Q-learning in
    this scenario is its model-free nature, meaning it doesnâ€™t require a model of
    the environment and can learn solely from interaction, making it broadly applicable
    to various problems in RL.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€æ—¶é—´çš„æ¨ç§»ï¼Œä»£ç†æ¢ç´¢ç¯å¢ƒå¹¶æ›´æ–°Q-è¡¨ï¼Œå®ƒåœ¨å¯¼èˆªFrozen Lakeæ—¶å˜å¾—æ›´åŠ ç†Ÿç»ƒï¼Œæœ€ç»ˆå­¦ä¼šäº†ä¸€ç§æœ€ä½³æˆ–æ¥è¿‘æœ€ä½³çš„ç­–ç•¥ï¼Œä»¥å¯é åœ°åˆ°è¾¾ç›®æ ‡ã€‚Qå­¦ä¹ åœ¨è¿™ç§æƒ…å†µä¸‹çš„ç¾å¦™ä¹‹å¤„åœ¨äºå®ƒçš„æ— æ¨¡å‹æ€§è´¨ï¼Œè¿™æ„å‘³ç€å®ƒä¸éœ€è¦ç¯å¢ƒæ¨¡å‹ï¼Œå¯ä»¥ä»…é€šè¿‡äº¤äº’å­¦ä¹ ï¼Œä½¿å…¶å¹¿æ³›é€‚ç”¨äºå„ç§RLé—®é¢˜ã€‚
- en: There exist many tutorials that demonstrate how to leverage and implement Q-learning
    for the Frozen Lake game, for example [https://towardsdatascience.com/q-learning-for-beginners-2837b777741](/q-learning-for-beginners-2837b777741).
    However, as previously mentioned, my interest as a board game enthusiast lies
    in adapting this method for two-player games, and potentially even for games with
    more players.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å­˜åœ¨è®¸å¤šæ•™ç¨‹æ¼”ç¤ºäº†å¦‚ä½•åˆ©ç”¨å’Œå®ç° Q-learning æ¥è§£å†³ Frozen Lake æ¸¸æˆï¼Œä¾‹å¦‚ [https://towardsdatascience.com/q-learning-for-beginners-2837b777741](/q-learning-for-beginners-2837b777741)ã€‚ç„¶è€Œï¼Œæ­£å¦‚å‰é¢æåˆ°çš„ï¼Œä½œä¸ºä¸€ä¸ªæ£‹ç›˜æ¸¸æˆçˆ±å¥½è€…ï¼Œæˆ‘å¯¹å°†è¿™ç§æ–¹æ³•é€‚ç”¨äºåŒäººæ¸¸æˆï¼Œç”šè‡³æ›´å¤šç©å®¶çš„æ¸¸æˆæ›´æ„Ÿå…´è¶£ã€‚
- en: '**Challenges in Two-Player Games**'
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**åŒäººæ¸¸æˆä¸­çš„æŒ‘æˆ˜**'
- en: 'Applying Q-learning to a two-player game such as TTT necessitates a minor modification.
    In the Frozen Lake game, the next state is determined solely by the agentâ€™s action.
    However, in TTT, although a player may take a turn, the subsequent state also
    depends on the opponentâ€™s move. For instance, if I place an â€˜Xâ€™ in the top-left
    corner, the next state is uncertain because my opponent has several potential
    moves:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å°† Q-learning åº”ç”¨äºåŒäººæ¸¸æˆï¼Œå¦‚äº•å­—æ£‹ï¼Œéœ€è¦è¿›è¡Œä¸€äº›å°çš„ä¿®æ”¹ã€‚åœ¨ Frozen Lake æ¸¸æˆä¸­ï¼Œä¸‹ä¸€çŠ¶æ€ä»…ç”±ä»£ç†çš„è¡ŒåŠ¨å†³å®šã€‚ç„¶è€Œï¼Œåœ¨äº•å­—æ£‹ä¸­ï¼Œå°½ç®¡ç©å®¶å¯èƒ½é‡‡å–ä¸€ä¸ªå›åˆï¼Œä½†éšåçš„çŠ¶æ€è¿˜ä¾èµ–äºå¯¹æ‰‹çš„è¡ŒåŠ¨ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘åœ¨å·¦ä¸Šè§’æ”¾ç½®ä¸€ä¸ªâ€˜Xâ€™ï¼Œé‚£ä¹ˆä¸‹ä¸€çŠ¶æ€æ˜¯ä¸ç¡®å®šçš„ï¼Œå› ä¸ºæˆ‘çš„å¯¹æ‰‹æœ‰å‡ ä¸ªæ½œåœ¨çš„ç§»åŠ¨ï¼š
- en: '![](../Images/db3bd40a35d9275cb4e3928f83be3e65.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db3bd40a35d9275cb4e3928f83be3e65.png)'
- en: Image by author
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: Several methods can be employed to tackle this issue. One approach involves
    simulating all possible actions of the opponent and their respective outcomes.
    This requires generating a probability distribution for all potential subsequent
    states and updating the Q-values according to the anticipated results of these
    states. However, this method can be computationally demanding. In this tutorial
    we will take a much simpler approach and just take a random action for the opponent
    and update the Q-table based on the actual outcome of this action. This reflects
    the unpredictable nature of the opponent quite well as we will see later on. With
    this approach, Q-learning can be effectively adapted to two-player games, allowing
    AI to not only learn optimal moves but also (eventually) adapt to the strategies
    of human opponents.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥é‡‡ç”¨å‡ ç§æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ä¸€ç§æ–¹æ³•æ˜¯æ¨¡æ‹Ÿå¯¹æ‰‹æ‰€æœ‰å¯èƒ½çš„è¡ŒåŠ¨åŠå…¶ç›¸åº”ç»“æœã€‚è¿™éœ€è¦ç”Ÿæˆæ‰€æœ‰æ½œåœ¨åç»­çŠ¶æ€çš„æ¦‚ç‡åˆ†å¸ƒï¼Œå¹¶æ ¹æ®è¿™äº›çŠ¶æ€çš„é¢„æœŸç»“æœæ›´æ–° Q å€¼ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å¯èƒ½è®¡ç®—é‡è¾ƒå¤§ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†é‡‡ç”¨ä¸€ç§æ›´ç®€å•çš„æ–¹æ³•ï¼Œä¸ºå¯¹æ‰‹éšæœºé‡‡å–ä¸€ä¸ªåŠ¨ä½œï¼Œå¹¶æ ¹æ®è¿™ä¸ªåŠ¨ä½œçš„å®é™…ç»“æœæ›´æ–°
    Q è¡¨ã€‚è¿™å¾ˆå¥½åœ°åæ˜ äº†å¯¹æ‰‹çš„ä¸å¯é¢„æµ‹æ€§ï¼Œæ­£å¦‚æˆ‘ä»¬åé¢å°†çœ‹åˆ°çš„é‚£æ ·ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼ŒQ-learning å¯ä»¥æœ‰æ•ˆåœ°é€‚åº”åŒäººæ¸¸æˆï¼Œä½¿ AI ä¸ä»…èƒ½å¤Ÿå­¦ä¹ æœ€ä½³ç§»åŠ¨ï¼Œè¿˜èƒ½ï¼ˆæœ€ç»ˆï¼‰é€‚åº”äººç±»å¯¹æ‰‹çš„ç­–ç•¥ã€‚
- en: This methodology, in principle, mirrors the approach used to train [AlphaGo
    Zero](https://en.wikipedia.org/wiki/AlphaGo_Zero). This AI program played 4.9
    million games of Go against itself in rapid succession. During this process, it
    continuously improved its skill, learning and adapting strategies autonomously.
    This self-learning method, bypassing the need to simulate every possible move
    of the opponent, presents an efficient and effective way for AI systems to learn
    and adapt to complex tasks.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•åŸåˆ™ä¸Šä¸è®­ç»ƒ [AlphaGo Zero](https://en.wikipedia.org/wiki/AlphaGo_Zero) çš„æ–¹æ³•ç±»ä¼¼ã€‚è¯¥
    AI ç¨‹åºåœ¨å¿«é€Ÿè¿ç»­çš„å¯¹å¼ˆä¸­è‡ªæˆ‘å¯¹å¼ˆäº† 490 ä¸‡å±€å›´æ£‹ã€‚åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œå®ƒä¸æ–­æé«˜è‡ªå·±çš„æŠ€èƒ½ï¼Œè‡ªä¸»å­¦ä¹ å’Œè°ƒæ•´ç­–ç•¥ã€‚è¿™ç§è‡ªå­¦ä¹ æ–¹æ³•ï¼Œç»•è¿‡äº†æ¨¡æ‹Ÿå¯¹æ‰‹æ¯ä¸€ä¸ªå¯èƒ½çš„ç§»åŠ¨çš„éœ€æ±‚ï¼Œä¸º
    AI ç³»ç»Ÿæä¾›äº†ä¸€ç§é«˜æ•ˆä¸”æœ‰æ•ˆçš„å­¦ä¹ å’Œé€‚åº”å¤æ‚ä»»åŠ¡çš„æ–¹æ³•ã€‚
- en: '![](../Images/bf55b0ed4cb304a37d134c2305ff771f.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bf55b0ed4cb304a37d134c2305ff771f.png)'
- en: 'Game 2 between Lee Sedol and AlphaGo with famous move 37 by AlphaGo. Image
    credit: [https://commons.wikimedia.org/wiki/File:Lee_Sedol_(W)_vs_AlphaGo_(B)_-_Game_2.svg](https://commons.wikimedia.org/wiki/File:Lee_Sedol_(W)_vs_AlphaGo_(B)_-_Game_2.svg)
    (Licence CC BY-SA 4.0)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æä¸–çŸ³ä¸ AlphaGo çš„ç¬¬2å±€æ¯”èµ›ï¼ŒAlphaGo çš„è‘—åç¬¬37æ­¥ã€‚å›¾ç‰‡æ¥æºï¼š[https://commons.wikimedia.org/wiki/File:Lee_Sedol_(W)_vs_AlphaGo_(B)_-_Game_2.svg](https://commons.wikimedia.org/wiki/File:Lee_Sedol_(W)_vs_AlphaGo_(B)_-_Game_2.svg)ï¼ˆè®¸å¯
    CC BY-SA 4.0ï¼‰
- en: In the next sections, we will delve into how these principles are applied specifically
    in the case of Tic-Tac-Toe, illustrating the implementation of Q-learning in an
    environment with two players.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨è¿™äº›åŸåˆ™å¦‚ä½•åœ¨äº•å­—æ£‹çš„å…·ä½“æƒ…å†µä¸‹åº”ç”¨ï¼Œå±•ç¤ºåœ¨åŒäººç¯å¢ƒä¸­ Q-learning çš„å®ç°ã€‚
- en: '**Q-Learning for Tic-Tac-Toe**'
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**äº•å­—æ£‹çš„ Q-Learning**'
- en: As we venture into applying Q-learning to Tic-Tac-Toe, itâ€™s important to understand
    the setup of our program and the environment in which our AI agent will be operating.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬å¼€å§‹å°† Q-learning åº”ç”¨äºäº•å­—æ£‹æ—¶ï¼Œäº†è§£æˆ‘ä»¬ç¨‹åºçš„è®¾ç½®ä»¥åŠ AI ä»£ç†å°†è¦æ“ä½œçš„ç¯å¢ƒéå¸¸é‡è¦ã€‚
- en: Overview
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¦‚è¿°
- en: The code is designed to train an AI (which we will call *Player 1* or *agent*)
    to play a Tic-Tac-Toe-like game using Q-learning, a form of reinforcement learning.
    It begins by setting up the learning parameters and initializing a Q-table to
    store the value of different actions in different states. The script defines several
    functions to manage the gameâ€™s mechanics, such as determining possible moves,
    checking for a winner, updating the game state, and calculating the next state
    and reward after a move.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ®µä»£ç æ—¨åœ¨è®­ç»ƒä¸€ä¸ª AIï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸º*ç©å®¶ 1*æˆ–*æ™ºèƒ½ä½“*ï¼‰ï¼Œé€šè¿‡ Q å­¦ä¹ ï¼ˆä¸€ç§å¼ºåŒ–å­¦ä¹ å½¢å¼ï¼‰æ¥ç©ç±»ä¼¼äº•å­—æ£‹çš„æ¸¸æˆã€‚å®ƒé¦–å…ˆè®¾ç½®å­¦ä¹ å‚æ•°å¹¶åˆå§‹åŒ–ä¸€ä¸ª
    Q è¡¨æ¥å­˜å‚¨ä¸åŒçŠ¶æ€ä¸‹ä¸åŒåŠ¨ä½œçš„å€¼ã€‚è„šæœ¬å®šä¹‰äº†å‡ ä¸ªå‡½æ•°æ¥ç®¡ç†æ¸¸æˆæœºåˆ¶ï¼Œå¦‚ç¡®å®šå¯èƒ½çš„ç§»åŠ¨ã€æ£€æŸ¥èƒœè€…ã€æ›´æ–°æ¸¸æˆçŠ¶æ€ï¼Œä»¥åŠåœ¨ç§»åŠ¨åè®¡ç®—ä¸‹ä¸€ä¸ªçŠ¶æ€å’Œå¥–åŠ±ã€‚
- en: In the main part of the script, a Q-learning algorithm is implemented. It runs
    through numerous episodes, simulating games between theagent and its opponent
    (which we will call *player 2*). In each episode, the AI either explores a random
    move or exploits knowledge from the Q-table to make a move, learning from the
    outcomes to update the Q-table values. This process involves adjusting the exploration
    rate over time, shifting from random exploration to more strategic moves as it
    learns.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è„šæœ¬çš„ä¸»è¦éƒ¨åˆ†ä¸­ï¼Œå®ç°äº† Q å­¦ä¹ ç®—æ³•ã€‚å®ƒè¿è¡Œå¤šä¸ªå›åˆï¼Œæ¨¡æ‹Ÿæ™ºèƒ½ä½“ä¸å…¶å¯¹æ‰‹ï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸º*ç©å®¶ 2*ï¼‰ä¹‹é—´çš„æ¸¸æˆã€‚åœ¨æ¯ä¸€å›åˆä¸­ï¼ŒAI è¦ä¹ˆæ¢ç´¢ä¸€ä¸ªéšæœºåŠ¨ä½œï¼Œè¦ä¹ˆåˆ©ç”¨
    Q è¡¨ä¸­çš„çŸ¥è¯†æ¥è¿›è¡Œå†³ç­–ï¼Œä»ç»“æœä¸­å­¦ä¹ ä»¥æ›´æ–° Q è¡¨çš„å€¼ã€‚è¿™ä¸ªè¿‡ç¨‹æ¶‰åŠéšç€æ—¶é—´çš„æ¨ç§»è°ƒæ•´æ¢ç´¢ç‡ï¼Œä»éšæœºæ¢ç´¢è½¬å‘æ›´å…·ç­–ç•¥æ€§çš„åŠ¨ä½œã€‚
- en: A key aspect of our setup is the AIâ€™s opponent. Unlike more complex scenarios
    where the opponent might have a sophisticated strategy, our AI will be playing
    against an opponent that makes random moves. This choice simplifies the learning
    environment and allows us to focus on the AIâ€™s learning process rather than the
    complexity of the opponentâ€™s strategy.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¾ç½®çš„ä¸€ä¸ªå…³é”®æ–¹é¢æ˜¯ AI çš„å¯¹æ‰‹ã€‚ä¸å¯¹æ‰‹å¯èƒ½æ‹¥æœ‰å¤æ‚ç­–ç•¥çš„æ›´å¤æ‚åœºæ™¯ä¸åŒï¼Œæˆ‘ä»¬çš„ AI å°†ä¸ä¸€ä¸ªéšæœºç§»åŠ¨çš„å¯¹æ‰‹è¿›è¡Œæ¸¸æˆã€‚è¿™ä¸€é€‰æ‹©ç®€åŒ–äº†å­¦ä¹ ç¯å¢ƒï¼Œä½¿æˆ‘ä»¬å¯ä»¥ä¸“æ³¨äº
    AI çš„å­¦ä¹ è¿‡ç¨‹ï¼Œè€Œä¸æ˜¯å¯¹æ‰‹ç­–ç•¥çš„å¤æ‚æ€§ã€‚
- en: The Q-Learning Setup
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Q å­¦ä¹ è®¾ç½®
- en: 'Our Q-learning setup involves key parameters that will influence how the AI
    learns:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ Q å­¦ä¹ è®¾ç½®æ¶‰åŠä¸€äº›å…³é”®å‚æ•°ï¼Œè¿™äº›å‚æ•°å°†å½±å“ AI çš„å­¦ä¹ æ–¹å¼ï¼š
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Learning Rate (**`**learning_rate**`**):** This determines how much new information
    affects existing knowledge. A higher rate accelerates learning but can lead to
    instability. A rate of 0.2 strikes a balance between learning new strategies and
    retaining previous learning.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å­¦ä¹ ç‡ (**`**learning_rate**`**):** è¿™å†³å®šäº†æ–°ä¿¡æ¯å¯¹ç°æœ‰çŸ¥è¯†çš„å½±å“ç¨‹åº¦ã€‚è¾ƒé«˜çš„å­¦ä¹ ç‡åŠ é€Ÿäº†å­¦ä¹ è¿‡ç¨‹ï¼Œä½†å¯èƒ½å¯¼è‡´ä¸ç¨³å®šã€‚å­¦ä¹ ç‡ä¸º
    0.2 åœ¨å­¦ä¹ æ–°ç­–ç•¥å’Œä¿ç•™ä¹‹å‰çš„å­¦ä¹ ä¹‹é—´å–å¾—äº†å¹³è¡¡ã€‚'
- en: '**Discount Factor (**`**discount_factor**`**):** This reflects the importance
    of future rewards, influencing how far-sighted the AI''s strategy will be. With
    a value of 0.9, it places significant emphasis on future rewards, encouraging
    the AI to think ahead rather than just focus on immediate gains.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æŠ˜æ‰£å› å­ (**`**discount_factor**`**):** è¿™åæ˜ äº†æœªæ¥å¥–åŠ±çš„é‡è¦æ€§ï¼Œå½±å“ AI ç­–ç•¥çš„è¿œè§ç¨‹åº¦ã€‚æŠ˜æ‰£å› å­ä¸º 0.9
    æ—¶ï¼ŒAI ä¼šç‰¹åˆ«é‡è§†æœªæ¥å¥–åŠ±ï¼Œé¼“åŠ± AI å‰ç»æ€§æ€è€ƒï¼Œè€Œä¸ä»…ä»…å…³æ³¨å³æ—¶æ”¶ç›Šã€‚'
- en: '**Number of Episodes (**`**num_episodes**`**):** This is the number of games
    the AI will play to learn, providing ample opportunity for the AI to experience
    various game scenarios. Setting this to 10 million (`1e7`) allows extensive training,
    giving the AI ample opportunity to learn from a wide range of game scenarios.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å›åˆæ•° (**`**num_episodes**`**):** è¿™æ˜¯ AI å­¦ä¹ çš„æ¸¸æˆæ•°é‡ï¼Œä¸º AI æä¾›äº†å……è¶³çš„æœºä¼šæ¥ä½“éªŒå„ç§æ¸¸æˆåœºæ™¯ã€‚å°†å…¶è®¾ç½®ä¸º
    1000 ä¸‡ (`1e7`) å…è®¸å¹¿æ³›çš„è®­ç»ƒï¼Œä¸º AI æä¾›äº†ä»å„ç§æ¸¸æˆåœºæ™¯ä¸­å­¦ä¹ çš„å……è¶³æœºä¼šã€‚'
- en: '**Exploration Rate (**`**epsilon**`**):** The exploration rate (epsilon) is
    initially set high to allow the AI to explore various actions, rather than solely
    exploiting known strategies. Initially, the AI will explore more (due to `epsilon`
    being 1.0). Over time, as `epsilon` decays towards `epsilon_min`, the AI will
    start exploiting its learned strategies more.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¢ç´¢ç‡ (**`**epsilon**`**):** æ¢ç´¢ç‡ï¼ˆepsilonï¼‰æœ€åˆè®¾ç½®è¾ƒé«˜ï¼Œä»¥å…è®¸ AI æ¢ç´¢å„ç§åŠ¨ä½œï¼Œè€Œä¸æ˜¯ä»…ä»…åˆ©ç”¨å·²çŸ¥ç­–ç•¥ã€‚æœ€åˆï¼ŒAI
    ä¼šæ›´å¤šåœ°è¿›è¡Œæ¢ç´¢ï¼ˆç”±äº `epsilon` ä¸º 1.0ï¼‰ã€‚éšç€æ—¶é—´çš„æ¨ç§»ï¼Œ`epsilon` é€æ¸å‡å°åˆ° `epsilon_min`ï¼ŒAI å°†å¼€å§‹æ›´å¤šåœ°åˆ©ç”¨å…¶å­¦ä¹ åˆ°çš„ç­–ç•¥ã€‚'
- en: '**Side note on the exploration rate**'
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**å…³äºæ¢ç´¢ç‡çš„é™„æ³¨**'
- en: ''
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The exploration rate in Q Learning, often represented by the symbol Îµ (epsilon),
    is a crucial parameter that dictates the balance between exploration (trying new
    actions) and exploitation (using the best-known actions). Initially, the agent
    doesnâ€™t have much information about the environment, so itâ€™s beneficial for it
    to explore widely by trying out different actions. The exploration rate, typically
    set to a high value at the start (e.g., 1 or close to it), determines the probability
    of the agent taking a random action instead of the best-known action according
    to the Q-table.
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åœ¨ Q å­¦ä¹ ä¸­ï¼Œæ¢ç´¢ç‡é€šå¸¸ç”¨ç¬¦å· Îµï¼ˆepsilonï¼‰è¡¨ç¤ºï¼Œè¿™æ˜¯ä¸€ä¸ªå…³é”®å‚æ•°ï¼Œå†³å®šäº†æ¢ç´¢ï¼ˆå°è¯•æ–°åŠ¨ä½œï¼‰å’Œåˆ©ç”¨ï¼ˆä½¿ç”¨å·²çŸ¥æœ€ä½³åŠ¨ä½œï¼‰ä¹‹é—´çš„å¹³è¡¡ã€‚æœ€åˆï¼Œæ™ºèƒ½ä½“å¯¹ç¯å¢ƒäº†è§£ä¸å¤šï¼Œå› æ­¤å®ƒéœ€è¦å¹¿æ³›æ¢ç´¢ï¼Œé€šè¿‡å°è¯•ä¸åŒçš„åŠ¨ä½œã€‚æ¢ç´¢ç‡é€šå¸¸åœ¨å¼€å§‹æ—¶è®¾å®šä¸ºè¾ƒé«˜çš„å€¼ï¼ˆä¾‹å¦‚
    1 æˆ–æ¥è¿‘ 1ï¼‰ï¼Œå†³å®šäº†æ™ºèƒ½ä½“é€‰æ‹©éšæœºåŠ¨ä½œè€Œä¸æ˜¯æ ¹æ® Q è¡¨é€‰æ‹©æœ€ä½³å·²çŸ¥åŠ¨ä½œçš„æ¦‚ç‡ã€‚
- en: ''
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: However, as the agent learns more about the environment and the Q-table becomes
    more reliable, it becomes less necessary to explore, and more beneficial to exploit
    the knowledge gained. This is where the exploration rate decay comes into play.
    The exploration rate decay is a factor by which the exploration rate is reduced
    over time. It ensures that as the agent learns and gathers more information, it
    gradually shifts from exploring the environment to exploiting the learned values
    in the Q-table.
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œéšç€æ™ºèƒ½ä½“å¯¹ç¯å¢ƒçš„äº†è§£è¶Šæ¥è¶Šå¤šï¼ŒQ è¡¨å˜å¾—æ›´åŠ å¯é ï¼Œæ¢ç´¢çš„å¿…è¦æ€§å‡å°‘ï¼Œåˆ©ç”¨å·²è·å¾—çš„çŸ¥è¯†å˜å¾—æ›´åŠ æœ‰ç›Šã€‚è¿™æ—¶ï¼Œæ¢ç´¢ç‡è¡°å‡å°±å‘æŒ¥ä½œç”¨äº†ã€‚æ¢ç´¢ç‡è¡°å‡æ˜¯ä¸€ä¸ªéšç€æ—¶é—´æ¨ç§»è€Œå‡å°‘æ¢ç´¢ç‡çš„å› ç´ ã€‚å®ƒç¡®ä¿æ™ºèƒ½ä½“åœ¨å­¦ä¹ å’Œæ”¶é›†æ›´å¤šä¿¡æ¯çš„è¿‡ç¨‹ä¸­ï¼Œé€æ¸ä»æ¢ç´¢ç¯å¢ƒè½¬å‘åˆ©ç”¨
    Q è¡¨ä¸­å­¦åˆ°çš„å€¼ã€‚
- en: ''
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The reason why this balance is important in Q Learning is to avoid two main
    issues:'
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è¿™ç§å¹³è¡¡åœ¨ Q å­¦ä¹ ä¸­å¾ˆé‡è¦ï¼Œå› ä¸ºå®ƒå¯ä»¥é¿å…ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼š
- en: ''
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Getting Stuck in Local Optima:** If the agent only exploits known information
    (low exploration), it might get stuck in local optima. This means it repeatedly
    chooses actions that seem best based on limited information but might miss out
    on discovering actions that lead to better long-term rewards.'
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼š** å¦‚æœæ™ºèƒ½ä½“åªåˆ©ç”¨å·²çŸ¥ä¿¡æ¯ï¼ˆä½æ¢ç´¢ï¼‰ï¼Œå¯èƒ½ä¼šé™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚è¿™æ„å‘³ç€å®ƒä¼šæ ¹æ®æœ‰é™çš„ä¿¡æ¯åå¤é€‰æ‹©çœ‹ä¼¼æœ€ä½³çš„åŠ¨ä½œï¼Œä½†å¯èƒ½é”™è¿‡å‘ç°èƒ½å¸¦æ¥æ›´å¥½é•¿æœŸå¥–åŠ±çš„åŠ¨ä½œã€‚'
- en: ''
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Inefficient Learning:** On the other hand, if the agent explores too much
    (high exploration) and for too long, it can lead to inefficient learning. The
    agent might keep trying suboptimal actions without sufficiently leveraging the
    knowledge it has already acquired, leading to slower convergence to the optimal
    policy.'
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**ä½æ•ˆå­¦ä¹ ï¼š** å¦ä¸€æ–¹é¢ï¼Œå¦‚æœæ™ºèƒ½ä½“è¿‡åº¦æ¢ç´¢ï¼ˆé«˜æ¢ç´¢ï¼‰ä¸”æ—¶é—´è¿‡é•¿ï¼Œå¯èƒ½å¯¼è‡´ä½æ•ˆå­¦ä¹ ã€‚æ™ºèƒ½ä½“å¯èƒ½ä¼šä¸æ–­å°è¯•æ¬¡ä¼˜åŠ¨ä½œè€Œæ²¡æœ‰å……åˆ†åˆ©ç”¨å·²ç»è·å¾—çš„çŸ¥è¯†ï¼Œä»è€Œå¯¼è‡´æ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥çš„é€Ÿåº¦å˜æ…¢ã€‚'
- en: ''
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: By appropriately setting the exploration rate and its decay, Q-learning algorithms
    can effectively balance these two aspects, allowing the agent to explore the environment
    initially and then gradually focus more on exploiting the best strategies it has
    learned. This balance is key to the efficiency and effectiveness of learning in
    complex environments.
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: é€šè¿‡é€‚å½“è®¾ç½®æ¢ç´¢ç‡åŠå…¶è¡°å‡ï¼ŒQ-learning ç®—æ³•å¯ä»¥æœ‰æ•ˆåœ°å¹³è¡¡è¿™ä¸¤ä¸ªæ–¹é¢ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿæœ€åˆæ¢ç´¢ç¯å¢ƒï¼Œç„¶åé€æ¸æ›´å¤šåœ°ä¸“æ³¨äºåˆ©ç”¨å®ƒæ‰€å­¦åˆ°çš„æœ€ä½³ç­–ç•¥ã€‚è¿™ç§å¹³è¡¡å¯¹äºåœ¨å¤æ‚ç¯å¢ƒä¸­å­¦ä¹ çš„æ•ˆç‡å’Œæœ‰æ•ˆæ€§è‡³å…³é‡è¦ã€‚
- en: In the next sections, we will dive into the code to see how the AI uses Q-learning
    to make decisions, update its strategy, and ultimately aim to master Tic-Tac-Toe.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥ä»£ç ï¼Œçœ‹çœ‹ AI å¦‚ä½•ä½¿ç”¨ Q-learning æ¥åšå†³ç­–ã€æ›´æ–°ç­–ç•¥ï¼Œå¹¶æœ€ç»ˆæŒæ¡ Tic-Tac-Toeã€‚
- en: Code Deep Dive
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»£ç æ·±åº¦è§£æ
- en: Training Script
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒè„šæœ¬
- en: This is a walkthrough of the [train.py](https://github.com/marshmellow77/tictactoe-q/blob/master/train.py)
    file in the GH repo.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ [train.py](https://github.com/marshmellow77/tictactoe-q/blob/master/train.py)
    æ–‡ä»¶çš„è¯¦ç»†è§£è¯»ã€‚
- en: 'The training starts with the for loop (roughly in the middle of the script)
    where we will play a certain number of episodes:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒä» for å¾ªç¯å¼€å§‹ï¼ˆå¤§è‡´åœ¨è„šæœ¬çš„ä¸­é—´ï¼‰ï¼Œæˆ‘ä»¬å°†åœ¨å…¶ä¸­è¿›è¡Œä¸€å®šæ•°é‡çš„å›åˆï¼š
- en: '[PRE1]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Following that, we determine the start player randomly. An even easier approach
    would have been to just make our agent always the start player. But implementing
    a random start player is not much more effort and generalises the Q-table mode,
    i.e. our agent will learn how to play as starting player but also as non-starting
    player.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ç€ï¼Œæˆ‘ä»¬éšæœºç¡®å®šèµ·å§‹ç©å®¶ã€‚ä¸€ä¸ªæ›´ç®€å•çš„æ–¹æ³•æ˜¯è®©æˆ‘ä»¬çš„æ™ºèƒ½ä½“æ€»æ˜¯ä½œä¸ºèµ·å§‹ç©å®¶ã€‚ç„¶è€Œï¼Œå®ç°ä¸€ä¸ªéšæœºèµ·å§‹ç©å®¶å¹¶ä¸æ¯”ç›´æ¥æ€»æ˜¯è®©æ™ºèƒ½ä½“ä½œä¸ºèµ·å§‹ç©å®¶å¤šèŠ±è´¹å¤šå°‘ç²¾åŠ›ï¼Œå¹¶ä¸”è¿™ç§æ–¹æ³•ä½¿
    Q è¡¨æ¨¡å¼æ›´åŠ é€šç”¨ï¼Œå³æˆ‘ä»¬çš„æ™ºèƒ½ä½“å°†å­¦ä¹ å¦‚ä½•ä½œä¸ºèµ·å§‹ç©å®¶ä»¥åŠéèµ·å§‹ç©å®¶è¿›è¡Œæ¸¸æˆã€‚
- en: 'If player 2 starts the game, then we start with making a random move for player
    2:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœç©å®¶ 2 å¼€å§‹æ¸¸æˆï¼Œé‚£ä¹ˆæˆ‘ä»¬å°†ä¸ºç©å®¶ 2 è¿›è¡Œéšæœºç§»åŠ¨ï¼š
- en: '[PRE2]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now we enter the actual training loop within a game of TTT which will only
    stop if the game has finished. A key mechanic is the exploitation vs exploration
    mechanic, which we discussed earlier. This is implemented like so:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬è¿›å…¥å®é™…çš„ TTT æ¸¸æˆè®­ç»ƒå¾ªç¯ï¼Œåªæœ‰åœ¨æ¸¸æˆç»“æŸæ—¶æ‰ä¼šåœæ­¢ã€‚ä¸€ä¸ªå…³é”®æœºåˆ¶æ˜¯ä¹‹å‰è®¨è®ºçš„å¼€å‘ vs æ¢ç´¢æœºåˆ¶ã€‚å®ƒçš„å®ç°å¦‚ä¸‹ï¼š
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The lower the epsilon value, the less the agent will explore by playing random
    moves and the more it will leverage the Q-table.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: epsilon å€¼è¶Šä½ï¼Œæ™ºèƒ½ä½“é€šè¿‡éšæœºç§»åŠ¨è¿›è¡Œçš„æ¢ç´¢è¶Šå°‘ï¼Œå®ƒå°†æ›´å¤šåœ°åˆ©ç”¨ Q è¡¨ã€‚
- en: 'Once the agentâ€™s action has been chosen, we will execute it and determine the
    next state (and any rewards if applicable):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦é€‰æ‹©äº†æ™ºèƒ½ä½“çš„åŠ¨ä½œï¼Œæˆ‘ä»¬å°†æ‰§è¡Œå®ƒå¹¶ç¡®å®šä¸‹ä¸€çŠ¶æ€ï¼ˆä»¥åŠé€‚ç”¨çš„å¥–åŠ±ï¼‰ï¼š
- en: '[PRE4]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The function that does all this warrants a closer look:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å¤„ç†æ‰€æœ‰è¿™äº›æ“ä½œçš„å‡½æ•°å€¼å¾—æ›´ä»”ç»†åœ°æŸ¥çœ‹ï¼š
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this function we first update the state of the board and check if our agent
    has won the game. If that is not the case, then we play a random move for the
    opponent and check again if the opponent has won the game. Depending on the outcome
    we return a reward of 0 (game still ongoing), 0.1 (draw), +1 (agent won), or -1
    (opponent won). The reason we choose 0.1 as a reward for a draw is so that the
    agent is incentivised to end a game quickly.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªå‡½æ•°ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆæ›´æ–°æ£‹ç›˜çš„çŠ¶æ€å¹¶æ£€æŸ¥æˆ‘ä»¬çš„æ™ºèƒ½ä½“æ˜¯å¦èµ¢å¾—äº†æ¸¸æˆã€‚å¦‚æœæ²¡æœ‰ï¼Œæˆ‘ä»¬ä¸ºå¯¹æ‰‹è¿›è¡Œéšæœºç§»åŠ¨ï¼Œå¹¶å†æ¬¡æ£€æŸ¥å¯¹æ‰‹æ˜¯å¦èµ¢å¾—äº†æ¸¸æˆã€‚æ ¹æ®ç»“æœï¼Œæˆ‘ä»¬è¿”å›
    0ï¼ˆæ¸¸æˆä»åœ¨è¿›è¡Œä¸­ï¼‰ã€0.1ï¼ˆå¹³å±€ï¼‰ã€+1ï¼ˆæ™ºèƒ½ä½“è·èƒœï¼‰æˆ– -1ï¼ˆå¯¹æ‰‹è·èƒœï¼‰ã€‚æˆ‘ä»¬é€‰æ‹© 0.1 ä½œä¸ºå¹³å±€çš„å¥–åŠ±æ˜¯ä¸ºäº†æ¿€åŠ±æ™ºèƒ½ä½“å°½å¿«ç»“æŸæ¸¸æˆã€‚
- en: 'Now that we have determined the reward come the most crucial part of the entire
    program: Updating the Q-table via the Bellman equation:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»ç¡®å®šäº†å¥–åŠ±ï¼Œæ¥ä¸‹æ¥æ˜¯æ•´ä¸ªç¨‹åºä¸­æœ€å…³é”®çš„éƒ¨åˆ†ï¼šé€šè¿‡ Bellman æ–¹ç¨‹æ›´æ–° Q è¡¨ï¼š
- en: '[PRE6]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This Bellman equation is much better explained in other blog posts (again,
    refer back to [https://towardsdatascience.com/q-learning-for-beginners-2837b777741](/q-learning-for-beginners-2837b777741)).
    But for a very brief explanation:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ª Bellman æ–¹ç¨‹åœ¨å…¶ä»–åšå®¢æ–‡ç« ä¸­è§£é‡Šå¾—æ›´å¥½ï¼ˆå†æ¬¡å‚è€ƒ [https://towardsdatascience.com/q-learning-for-beginners-2837b777741](/q-learning-for-beginners-2837b777741)ï¼‰ã€‚ä½†ç®€è¦è§£é‡Šå¦‚ä¸‹ï¼š
- en: 'As discussed earlier, the Q-table is essentially a big cheatsheet: It keeps
    track of all the possible states in the game and the value of each possible move
    from that state. It tells the agent how good each move is in a given situation,
    based on what it has learned so far.'
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼ŒQ è¡¨æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªå¤§çš„å¤‡å¿˜å•ï¼šå®ƒè·Ÿè¸ªæ¸¸æˆä¸­çš„æ‰€æœ‰å¯èƒ½çŠ¶æ€ä»¥åŠä»è¯¥çŠ¶æ€å¼€å§‹çš„æ¯ä¸ªå¯èƒ½ç§»åŠ¨çš„ä»·å€¼ã€‚å®ƒå‘Šè¯‰æ™ºèƒ½ä½“åœ¨ç»™å®šæƒ…å†µä¸‹æ¯ä¸ªç§»åŠ¨çš„å¥½åï¼ŒåŸºäºå®ƒè¿„ä»Šä¸ºæ­¢å­¦åˆ°çš„çŸ¥è¯†ã€‚
- en: ''
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Bellman equation updates this Q-table. It does this by looking at the *immediate*
    rewards the agent receives (winning, losing, drawing the game) and the quality
    of future states (i.e. *future* rewards) it can move to. So, after each game,
    the agent uses the Bellman equation to revise its Q-table, learning which moves
    are likely to lead to a win, a loss, or a draw.
  id: totrans-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Bellman æ–¹ç¨‹æ›´æ–°è¿™ä¸ª Q è¡¨ã€‚å®ƒé€šè¿‡æŸ¥çœ‹æ™ºèƒ½ä½“æ”¶åˆ°çš„*å³æ—¶*å¥–åŠ±ï¼ˆèµ¢ã€è¾“ã€å¹³å±€ï¼‰å’Œå®ƒå¯ä»¥ç§»åŠ¨åˆ°çš„æœªæ¥çŠ¶æ€ï¼ˆå³*æœªæ¥*å¥–åŠ±ï¼‰çš„è´¨é‡æ¥å®ç°ã€‚å› æ­¤ï¼Œåœ¨æ¯å±€æ¸¸æˆåï¼Œæ™ºèƒ½ä½“ä½¿ç”¨
    Bellman æ–¹ç¨‹æ¥ä¿®è®¢å…¶ Q è¡¨ï¼Œå­¦ä¹ å“ªäº›ç§»åŠ¨å¯èƒ½å¯¼è‡´èƒœåˆ©ã€å¤±è´¥æˆ–å¹³å±€ã€‚
- en: Lastly, we adjust the exploration rate, so that in future plays the agent uses
    the Q table more and explores less.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬è°ƒæ•´æ¢ç´¢ç‡ï¼Œä»¥ä¾¿åœ¨æœªæ¥çš„æ¸¸æˆä¸­ï¼Œæ™ºèƒ½ä½“æ›´å¤šåœ°ä½¿ç”¨ Q è¡¨è€Œè¾ƒå°‘è¿›è¡Œæ¢ç´¢ã€‚
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Running the Training
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¿è¡Œè®­ç»ƒ
- en: Once we have the training script ready, we can execute it. Fortunately, this
    process is not computationally demanding and completes very quickly, requiring
    no special computing power. I executed this on a MacBook M1 Air, for example,
    and it concluded within 5 minutes for 10 million games. Once the training is complete,
    we will save the Q table (which is not particularly large) so that we can use
    it to test the agent, play games against the AI, and potentially continue training
    at a later stage to further enhance the table. Letâ€™s have a look at it ğŸ§
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦è®­ç»ƒè„šæœ¬å‡†å¤‡å¥½ï¼Œæˆ‘ä»¬å°±å¯ä»¥æ‰§è¡Œå®ƒã€‚å¹¸è¿çš„æ˜¯ï¼Œè¿™ä¸ªè¿‡ç¨‹è®¡ç®—éœ€æ±‚ä¸é«˜ï¼Œå®Œæˆå¾—éå¸¸å¿«ï¼Œä¸éœ€è¦ç‰¹åˆ«çš„è®¡ç®—èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œæˆ‘åœ¨ MacBook M1 Air ä¸Šæ‰§è¡Œäº†è¿™ä¸ªè¿‡ç¨‹ï¼Œå®ƒåœ¨
    1000 ä¸‡å±€æ¸¸æˆä¸­ä¸åˆ° 5 åˆ†é’Ÿå°±å®Œæˆäº†ã€‚è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬å°†ä¿å­˜ Q è¡¨ï¼ˆå®ƒä¸æ˜¯ç‰¹åˆ«å¤§ï¼‰ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥ç”¨å®ƒæ¥æµ‹è¯•æ™ºèƒ½ä½“ï¼Œä¸ AI å¯¹æˆ˜ï¼Œå¹¶å¯èƒ½åœ¨ç¨åçš„é˜¶æ®µç»§ç»­è®­ç»ƒï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºè¡¨æ ¼ã€‚æˆ‘ä»¬æ¥çœ‹çœ‹å§
    ğŸ§
- en: Manual Inspection of Q table
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q è¡¨çš„äººå·¥æ£€æŸ¥
- en: 'The table is relatively easy to understand: Each row represents the board state
    and the actions that can be taken and their quality. Letâ€™s have a look at some
    interesting states. Note that your table will probably have different (but hopefully
    similar) values:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¡¨æ ¼ç›¸å¯¹å®¹æ˜“ç†è§£ï¼šæ¯ä¸€è¡Œä»£è¡¨äº†æ£‹ç›˜çŠ¶æ€ã€å¯é‡‡å–çš„è¡ŒåŠ¨åŠå…¶è´¨é‡ã€‚è®©æˆ‘ä»¬æ¥çœ‹çœ‹ä¸€äº›æœ‰è¶£çš„çŠ¶æ€ã€‚è¯·æ³¨æ„ï¼Œä½ çš„è¡¨æ ¼å¯èƒ½ä¼šæœ‰ä¸åŒï¼ˆä½†å¸Œæœ›æ˜¯ç›¸ä¼¼ï¼‰çš„å€¼ï¼š
- en: '![](../Images/f47b55da1649ba7c93d541cd6511960e.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f47b55da1649ba7c93d541cd6511960e.png)'
- en: Image by author
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: The board state shows where each player has placed already (the first 3 numbers
    represent the first row, the next 3 the second row, and the last 3 the last row.
    The actions correspond to the positions on the board, and the number for each
    action indicates the quality of that action. In this example we see a state where
    it looks like only one move (action 7) is considered good, all other moves seem
    losing.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: æ£‹ç›˜çŠ¶æ€æ˜¾ç¤ºäº†æ¯ä¸ªç©å®¶å·²ç»æ”¾ç½®çš„ä½ç½®ï¼ˆå‰3ä¸ªæ•°å­—ä»£è¡¨ç¬¬ä¸€è¡Œï¼Œæ¥ä¸‹æ¥çš„3ä¸ªä»£è¡¨ç¬¬äºŒè¡Œï¼Œæœ€å3ä¸ªä»£è¡¨æœ€åä¸€è¡Œã€‚åŠ¨ä½œå¯¹åº”æ£‹ç›˜ä¸Šçš„ä½ç½®ï¼Œæ¯ä¸ªåŠ¨ä½œçš„æ•°å­—è¡¨ç¤ºè¯¥åŠ¨ä½œçš„è´¨é‡ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°ä¸€ä¸ªçŠ¶æ€ï¼Œä¼¼ä¹åªæœ‰ä¸€ä¸ªåŠ¨ä½œï¼ˆåŠ¨ä½œ7ï¼‰è¢«è®¤ä¸ºæ˜¯å¥½çš„ï¼Œå…¶ä»–æ‰€æœ‰åŠ¨ä½œéƒ½æ˜¾å¾—è¾ƒå·®ã€‚
- en: 'NB: The indices for the board positions are as follows:'
  id: totrans-95
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šæ£‹ç›˜ä½ç½®çš„ç´¢å¼•å¦‚ä¸‹ï¼š
- en: '![](../Images/3d7945604a7896490040cbf64047977d.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d7945604a7896490040cbf64047977d.png)'
- en: Image by author
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: 'So, letâ€™s visualise the board state for this particular entry in the Q-table:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œè®©æˆ‘ä»¬æ¥å¯è§†åŒ–Qè¡¨ä¸­çš„è¿™ä¸ªç‰¹å®šæ¡ç›®çš„æ£‹ç›˜çŠ¶æ€ï¼š
- en: '![](../Images/5fd8c1971270157298573d23b371f20c.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5fd8c1971270157298573d23b371f20c.png)'
- en: Image by author
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: Indeed, the only good move for our agent (player 1) in this position is to choose
    position 7\. All the other moves might potentially lead to losing the game (remember
    that player 2 will play a random move on its next turn, so a loss is not guaranteed).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: çš„ç¡®ï¼Œåœ¨è¿™ä¸ªä½ç½®ï¼Œä»£ç†ï¼ˆç©å®¶1ï¼‰å”¯ä¸€çš„å¥½é€‰æ‹©æ˜¯é€‰æ‹©ä½ç½®7ã€‚æ‰€æœ‰å…¶ä»–ç§»åŠ¨å¯èƒ½ä¼šå¯¼è‡´è¾“æ‰æ¯”èµ›ï¼ˆè¯·è®°ä½ï¼Œç©å®¶2å°†åœ¨ä¸‹ä¸€è½®éšæœºç§»åŠ¨ï¼Œå› æ­¤è¾“æ‰æ¯”èµ›å¹¶éå¿…ç„¶ï¼‰ã€‚
- en: 'Letâ€™s look at one more example:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å†çœ‹ä¸€ä¸ªä¾‹å­ï¼š
- en: '![](../Images/1c365fcb2f84711d6b1ca0d27a1629ce.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c365fcb2f84711d6b1ca0d27a1629ce.png)'
- en: Image by author
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: '![](../Images/854666655addde48dd4dfeb51047bb9f.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/854666655addde48dd4dfeb51047bb9f.png)'
- en: Image by author
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: In this example the best move is obviously to play in position 8 (bottom right)
    and win the game. If the agent were to play any other move, it is likely that
    it will lose the game. Therefore the Q table will inform our agent to take action
    8.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæ˜¾ç„¶æœ€ä½³ç§»åŠ¨æ˜¯é€‰æ‹©ä½ç½®8ï¼ˆå³ä¸‹è§’ï¼‰å¹¶èµ¢å¾—æ¯”èµ›ã€‚å¦‚æœä»£ç†é€‰æ‹©å…¶ä»–ä½ç½®ï¼Œå®ƒå¾ˆå¯èƒ½ä¼šè¾“æ‰æ¯”èµ›ã€‚å› æ­¤ï¼ŒQè¡¨å°†æŒ‡ç¤ºæˆ‘ä»¬çš„ä»£ç†é‡‡å–åŠ¨ä½œ8ã€‚
- en: Testing the New Agent
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æµ‹è¯•æ–°ä»£ç†
- en: Now that weâ€™ve trained the model, we can test it with a the script [test.py](https://github.com/marshmellow77/tictactoe-q/blob/master/test.py)
    in the GH repo. In it we will let the agent a play a number of games against an
    opponent that plays random moves and see how well it performs. We start by initialising
    our agent and load the Q-table to use it for decision-making in a game environment.
    The `play_game` function simulates a game, using the loaded Q-table to guide the
    agent's decisions. The game environment here is a simple 3x3 board where each
    state represents a different configuration of the board.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»è®­ç»ƒäº†æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨GHä»“åº“ä¸­çš„è„šæœ¬[test.py](https://github.com/marshmellow77/tictactoe-q/blob/master/test.py)æ¥æµ‹è¯•å®ƒã€‚åœ¨è„šæœ¬ä¸­ï¼Œæˆ‘ä»¬å°†è®©ä»£ç†ä¸ä¸€ä¸ªéšæœºç§»åŠ¨çš„å¯¹æ‰‹è¿›è¡Œè‹¥å¹²å±€æ¯”èµ›ï¼Œçœ‹çœ‹å®ƒçš„è¡¨ç°å¦‚ä½•ã€‚æˆ‘ä»¬é¦–å…ˆåˆå§‹åŒ–æˆ‘ä»¬çš„ä»£ç†å¹¶åŠ è½½Qè¡¨ä»¥ä¾¿åœ¨æ¸¸æˆç¯å¢ƒä¸­ç”¨äºå†³ç­–ã€‚`play_game`å‡½æ•°æ¨¡æ‹Ÿäº†ä¸€åœºæ¯”èµ›ï¼Œä½¿ç”¨åŠ è½½çš„Qè¡¨æ¥æŒ‡å¯¼ä»£ç†çš„å†³ç­–ã€‚è¿™é‡Œçš„æ¸¸æˆç¯å¢ƒæ˜¯ä¸€ä¸ªç®€å•çš„3x3æ£‹ç›˜ï¼Œæ¯ä¸ªçŠ¶æ€ä»£è¡¨æ£‹ç›˜çš„ä¸åŒé…ç½®ã€‚
- en: The agent, which plays as Player 1, makes decisions based on the Q-table â€” choosing
    the action with the highest value in the current state. If a state is not found
    in the Q-table, the agent makes a random move. This combination of learned behaviour
    and randomness helps to evaluate the robustness of the training. Player 2â€™s moves
    are entirely random, providing a varied set of scenarios for the agent to navigate.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£ç†ä»¥ç©å®¶1çš„èº«ä»½ï¼Œæ ¹æ®Qè¡¨åšå‡ºå†³ç­–â€”â€”é€‰æ‹©å½“å‰çŠ¶æ€ä¸‹å€¼æœ€é«˜çš„è¡ŒåŠ¨ã€‚å¦‚æœåœ¨Qè¡¨ä¸­æ‰¾ä¸åˆ°çŠ¶æ€ï¼Œä»£ç†å°†åšå‡ºéšæœºç§»åŠ¨ã€‚è¿™ç§å­¦ä¹ è¡Œä¸ºå’Œéšæœºæ€§çš„ç»“åˆæœ‰åŠ©äºè¯„ä¼°è®­ç»ƒçš„é²æ£’æ€§ã€‚ç©å®¶2çš„ç§»åŠ¨å®Œå…¨éšæœºï¼Œä¸ºä»£ç†æä¾›äº†å¤šæ ·åŒ–çš„åœºæ™¯ã€‚
- en: The outcomes of these games are then tracked, quantifying the number of wins,
    losses, and draws. This helps in assessing the effectiveness of the trained model.
    If the `log_lost_games` flag is set, detailed logs of the games where the agent
    lost are saved, which can be invaluable for further analysis and improvement of
    the model. This testing process, playing a substantial number of games, gives
    a comprehensive view of the agent's capabilities post-training.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ¸¸æˆçš„ç»“æœä¼šè¢«è·Ÿè¸ªï¼Œé‡åŒ–èƒœåˆ©ã€å¤±è´¥å’Œå¹³å±€çš„æ•°é‡ã€‚è¿™æœ‰åŠ©äºè¯„ä¼°è®­ç»ƒæ¨¡å‹çš„æ•ˆæœã€‚å¦‚æœè®¾ç½®äº†`log_lost_games`æ ‡å¿—ï¼Œå°†ä¿å­˜è¯¦ç»†çš„å¤±è´¥æ¸¸æˆæ—¥å¿—ï¼Œè¿™å¯¹äºè¿›ä¸€æ­¥åˆ†æå’Œæ”¹è¿›æ¨¡å‹æ˜¯éå¸¸å®è´µçš„ã€‚è¿™ä¸€æµ‹è¯•è¿‡ç¨‹ï¼Œé€šè¿‡è¿›è¡Œå¤§é‡æ¸¸æˆï¼Œæä¾›äº†å¯¹è®­ç»ƒåä»£ç†èƒ½åŠ›çš„å…¨é¢äº†è§£ã€‚
- en: '![](../Images/e88dbab492830f205ffd07e030e1ebd6.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e88dbab492830f205ffd07e030e1ebd6.png)'
- en: Image by author
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: Playing a Game Against the AI
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸AIå¯¹æˆ˜
- en: It looks like the test against a random bot went well. Our AI managed to win
    more than 95% of the games. Now, we want to play against the AI ourselves. We
    can use [play.py](https://github.com/marshmellow77/tictactoe-q/blob/master/play.py)
    in the GH repo to do that.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹èµ·æ¥å¯¹éšæœºæœºå™¨äººè¿›è¡Œçš„æµ‹è¯•å¾ˆæˆåŠŸã€‚æˆ‘ä»¬çš„AIèµ¢å¾—äº†è¶…è¿‡95%çš„æ¯”èµ›ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬æƒ³äº²è‡ªä¸AIå¯¹æˆ˜ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨[play.py](https://github.com/marshmellow77/tictactoe-q/blob/master/play.py)æ¥å®ç°è¿™ä¸€ç‚¹ã€‚
- en: In this program, we interact with the AI through a simple console interface.
    The game board is represented as a 3x3 grid, with each position numbered from
    0 to 8\. When itâ€™s our turn, weâ€™ll be prompted to enter the number corresponding
    to the position where we want to make our move.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªç¨‹åºä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç®€å•çš„æ§åˆ¶å°ç•Œé¢ä¸AIäº’åŠ¨ã€‚æ¸¸æˆæ¿è¡¨ç¤ºä¸ºä¸€ä¸ª3x3çš„ç½‘æ ¼ï¼Œæ¯ä¸ªä½ç½®ä»0åˆ°8ç¼–å·ã€‚å½“è½®åˆ°æˆ‘ä»¬æ—¶ï¼Œæˆ‘ä»¬ä¼šè¢«æç¤ºè¾“å…¥ä¸€ä¸ªæ•°å­—ï¼Œä»¥é€‰æ‹©æˆ‘ä»¬æƒ³è¦ç§»åŠ¨çš„ä½ç½®ã€‚
- en: The AI uses the Q-table loaded from a CSV file to make its decisions. This Q-table,
    derived from the previous training process, guides the AI to choose the best possible
    move based on the current state of the game board. If the AI encounters a state
    not present in the Q-table, it defaults to making a random move.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: AIä½¿ç”¨ä»CSVæ–‡ä»¶åŠ è½½çš„Qè¡¨æ¥åšå‡ºå†³ç­–ã€‚è¿™ä¸ªQè¡¨æ¥æºäºä¹‹å‰çš„è®­ç»ƒè¿‡ç¨‹ï¼Œå¼•å¯¼AIæ ¹æ®å½“å‰çš„æ¸¸æˆæ¿çŠ¶æ€é€‰æ‹©æœ€ä½³å¯èƒ½çš„ç§»åŠ¨ã€‚å¦‚æœAIé‡åˆ°Qè¡¨ä¸­æ²¡æœ‰çš„çŠ¶æ€ï¼Œå®ƒå°†é»˜è®¤è¿›è¡Œéšæœºç§»åŠ¨ã€‚
- en: The game alternates between our turn and the AIâ€™s turn. After each move, the
    updated board is displayed, and the program checks for a winner. If a player wins
    or the game results in a draw, the game ends, and the outcome is announced â€” whether
    itâ€™s a win for us, a win for the AI, or a draw.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: æ¸¸æˆåœ¨æˆ‘ä»¬çš„å›åˆå’ŒAIçš„å›åˆä¹‹é—´äº¤æ›¿è¿›è¡Œã€‚æ¯æ¬¡ç§»åŠ¨åï¼Œæ›´æ–°åçš„æ£‹ç›˜ä¼šè¢«æ˜¾ç¤ºï¼Œç¨‹åºä¼šæ£€æŸ¥æ˜¯å¦æœ‰èµ¢å®¶ã€‚å¦‚æœç©å®¶è·èƒœæˆ–æ¸¸æˆç»“æœä¸ºå¹³å±€ï¼Œæ¸¸æˆç»“æŸï¼Œç»“æœå°†è¢«å®£å¸ƒâ€”â€”æ— è®ºæ˜¯æˆ‘ä»¬è·èƒœã€AIè·èƒœè¿˜æ˜¯å¹³å±€ã€‚
- en: 'This interactive game provides a great opportunity to test the AIâ€™s capabilities
    in real-time. Letâ€™s get started:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªäº’åŠ¨æ¸¸æˆæä¾›äº†ä¸€ä¸ªå¾ˆå¥½çš„æœºä¼šæ¥å®æ—¶æµ‹è¯•AIçš„èƒ½åŠ›ã€‚è®©æˆ‘ä»¬å¼€å§‹å§ï¼š
- en: '![](../Images/eac09908ab709e6708a660a156074384.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eac09908ab709e6708a660a156074384.png)'
- en: Image by author
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: In this game, if we donâ€™t choose action 0 (top left corner) the AI will have
    a chance to win the game. Will it realise that?
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªæ¸¸æˆä¸­ï¼Œå¦‚æœæˆ‘ä»¬ä¸é€‰æ‹©åŠ¨ä½œ0ï¼ˆå·¦ä¸Šè§’ï¼‰ï¼ŒAIå°†æœ‰æœºä¼šèµ¢å¾—æ¯”èµ›ã€‚å®ƒä¼šæ„è¯†åˆ°è¿™ä¸€ç‚¹å—ï¼Ÿ
- en: '![](../Images/3bf5a11009097edc79b18d607d9ef811.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3bf5a11009097edc79b18d607d9ef811.png)'
- en: Image by author
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: It did! Nice ğŸ˜Š
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®å®åšåˆ°äº†ï¼å¾ˆå¥½ğŸ˜Š
- en: '**Conclusion**'
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ç»“è®º**'
- en: In this post we trained our AI agent against a player that plays random moves.
    This was already good enough to achieve a win rate of more than 95% against an
    opponent that plays random moves. But there are ways to improve the training process
    and hopefully also the performance of the AI.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬è®­ç»ƒäº†æˆ‘ä»¬çš„AIä»£ç†å¯¹æŠ—ä¸€ä¸ªè¿›è¡Œéšæœºç§»åŠ¨çš„ç©å®¶ã€‚è¿™å·²ç»è¶³å¤Ÿå¥½ï¼Œèƒ½å¤Ÿåœ¨å¯¹æŠ—è¿›è¡Œéšæœºç§»åŠ¨çš„å¯¹æ‰‹æ—¶è¾¾åˆ°è¶…è¿‡95%çš„èƒœç‡ã€‚ä½†æ˜¯ï¼Œæœ‰æ–¹æ³•å¯ä»¥æ”¹è¿›è®­ç»ƒè¿‡ç¨‹ï¼Œå¸Œæœ›ä¹Ÿèƒ½æé«˜AIçš„è¡¨ç°ã€‚
- en: The Impact of Parameter Tuning
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚æ•°è°ƒæ•´çš„å½±å“
- en: 'The journey of applying Q-learning to Tic-Tac-Toe reveals a crucial aspect
    of RL: the art of fine-tuning parameters. Getting these parameters right, such
    as the balance between exploitation and exploration, the learning rate, and the
    discount factor, is key to the success of an RL agent.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: å°†Qå­¦ä¹ åº”ç”¨äºäº•å­—æ¸¸æˆæ­ç¤ºäº†å¼ºåŒ–å­¦ä¹ çš„ä¸€ä¸ªå…³é”®æ–¹é¢ï¼šè°ƒæ•´å‚æ•°çš„è‰ºæœ¯ã€‚æ­£ç¡®è®¾ç½®è¿™äº›å‚æ•°ï¼Œå¦‚å¼€å‘ä¸æ¢ç´¢ä¹‹é—´çš„å¹³è¡¡ã€å­¦ä¹ ç‡å’ŒæŠ˜æ‰£å› å­ï¼Œæ˜¯RLä»£ç†æˆåŠŸçš„å…³é”®ã€‚
- en: '**Exploration vs. Exploitation:** Controlled by the `epsilon` value, this balance
    dictates how often the agent tries new strategies versus relying on known strategies.
    A high exploration rate encourages the agent to try new things, potentially leading
    to innovative strategies, while a high exploitation rate makes the agent rely
    on its existing knowledge, which can be efficient but may miss out on better strategies.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¢ç´¢ä¸åˆ©ç”¨ï¼š** ç”±`epsilon`å€¼æ§åˆ¶ï¼Œè¿™ä¸€å¹³è¡¡å†³å®šäº†æ™ºèƒ½ä½“å°è¯•æ–°ç­–ç•¥çš„é¢‘ç‡ä¸ä¾èµ–å·²çŸ¥ç­–ç•¥çš„æ¯”ä¾‹ã€‚é«˜æ¢ç´¢ç‡é¼“åŠ±æ™ºèƒ½ä½“å°è¯•æ–°äº‹ç‰©ï¼Œå¯èƒ½å¯¼è‡´åˆ›æ–°ç­–ç•¥ï¼Œè€Œé«˜åˆ©ç”¨ç‡ä½¿æ™ºèƒ½ä½“ä¾èµ–ç°æœ‰çŸ¥è¯†ï¼Œè™½ç„¶å¯èƒ½æ›´é«˜æ•ˆï¼Œä½†å¯èƒ½ä¼šé”™è¿‡æ›´å¥½çš„ç­–ç•¥ã€‚'
- en: '**Learning Rate:** A high learning rate means the agent quickly adopts new
    information, which can be beneficial in dynamic environments but may lead to instability
    if the agent overwrites useful learnings too rapidly. Conversely, a low learning
    rate means the agent relies more on past knowledge, leading to stable but potentially
    slower learning.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å­¦ä¹ ç‡ï¼š** é«˜å­¦ä¹ ç‡æ„å‘³ç€æ™ºèƒ½ä½“è¿…é€Ÿé‡‡çº³æ–°ä¿¡æ¯ï¼Œè¿™åœ¨åŠ¨æ€ç¯å¢ƒä¸­å¯èƒ½æœ‰åˆ©ï¼Œä½†å¦‚æœæ™ºèƒ½ä½“è¿‡å¿«åœ°è¦†ç›–æœ‰ç”¨çš„å­¦ä¹ ï¼Œå¯èƒ½å¯¼è‡´ä¸ç¨³å®šã€‚ç›¸åï¼Œä½å­¦ä¹ ç‡æ„å‘³ç€æ™ºèƒ½ä½“æ›´å¤šä¾èµ–è¿‡å»çš„çŸ¥è¯†ï¼Œå¯¼è‡´ç¨³å®šä½†å¯èƒ½è¾ƒæ…¢çš„å­¦ä¹ ã€‚'
- en: '**Discount Factor:** This parameter influences how much the agent values future
    rewards. A high discount factor makes the agent forward-thinking, considering
    the long-term consequences of its actions. A low discount factor, on the other
    hand, makes the agent short-sighted, focusing on immediate rewards.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æŠ˜æ‰£å› å­ï¼š** è¿™ä¸ªå‚æ•°å½±å“æ™ºèƒ½ä½“å¯¹æœªæ¥å¥–åŠ±çš„é‡è§†ç¨‹åº¦ã€‚é«˜æŠ˜æ‰£å› å­ä½¿æ™ºèƒ½ä½“æ›´å…·å‰ç»æ€§ï¼Œè€ƒè™‘å…¶è¡ŒåŠ¨çš„é•¿æœŸåæœã€‚ç›¸åï¼Œä½æŠ˜æ‰£å› å­åˆ™ä½¿æ™ºèƒ½ä½“ç›®å…‰çŸ­æµ…ï¼Œä¸“æ³¨äºå³æ—¶å¥–åŠ±ã€‚'
- en: Changes in these parameters can significantly alter the behaviour of the RL
    agent. For instance, an agent with a low discount factor might play Tic-Tac-Toe
    aggressively, focusing on immediate wins rather than setting up future strategies.
    In contrast, an agent with a high discount factor might play more strategically,
    considering the implications of each move on the future state of the game.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å‚æ•°çš„å˜åŒ–å¯ä»¥æ˜¾è‘—æ”¹å˜RLæ™ºèƒ½ä½“çš„è¡Œä¸ºã€‚ä¾‹å¦‚ï¼ŒæŠ˜æ‰£å› å­ä½çš„æ™ºèƒ½ä½“å¯èƒ½ä¼šä»¥æ”»å‡»æ€§æ–¹å¼ç©äº•å­—æ£‹ï¼Œä¸“æ³¨äºå³æ—¶èƒœåˆ©ï¼Œè€Œä¸æ˜¯åˆ¶å®šæœªæ¥çš„ç­–ç•¥ã€‚ç›¸åï¼ŒæŠ˜æ‰£å› å­é«˜çš„æ™ºèƒ½ä½“å¯èƒ½ä¼šæ›´å…·ç­–ç•¥æ€§ï¼Œè€ƒè™‘æ¯ä¸€æ­¥å¯¹æ¸¸æˆæœªæ¥çŠ¶æ€çš„å½±å“ã€‚
- en: Similarly, an agent with a high learning rate might rapidly adapt to new strategies,
    constantly evolving its gameplay, while one with a low learning rate might stick
    to tried-and-tested strategies, showing less variation in its game.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ ·ï¼Œé«˜å­¦ä¹ ç‡çš„æ™ºèƒ½ä½“å¯èƒ½è¿…é€Ÿé€‚åº”æ–°ç­–ç•¥ï¼Œä¸æ–­å‘å±•å…¶æ¸¸æˆç©æ³•ï¼Œè€Œä½å­¦ä¹ ç‡çš„æ™ºèƒ½ä½“å¯èƒ½åšæŒç»è¿‡éªŒè¯çš„ç­–ç•¥ï¼Œæ¸¸æˆä¸­çš„å˜åŒ–è¾ƒå°ã€‚
- en: Your Turn to Experiment
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è½®åˆ°ä½ æ¥å®éªŒäº†
- en: This is where the true excitement lies in reinforcement learning. Each parameter
    can be tweaked to observe how it affects the learning and performance of the AI
    agent. I invite you to dive into this world of experimentation. Adjust the learning
    rate, exploration rate, and discount factor. Observe how these changes impact
    the AIâ€™s strategy in playing Tic-Tac-Toe.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯å¼ºåŒ–å­¦ä¹ çœŸæ­£çš„æ¿€åŠ¨æ‰€åœ¨ã€‚æ¯ä¸ªå‚æ•°éƒ½å¯ä»¥è¿›è¡Œå¾®è°ƒï¼Œä»¥è§‚å¯Ÿå®ƒå¦‚ä½•å½±å“AIæ™ºèƒ½ä½“çš„å­¦ä¹ å’Œè¡¨ç°ã€‚æˆ‘é‚€è¯·ä½ æ·±å…¥è¿™ä¸ªå®éªŒçš„ä¸–ç•Œã€‚è°ƒæ•´å­¦ä¹ ç‡ã€æ¢ç´¢ç‡å’ŒæŠ˜æ‰£å› å­ï¼Œè§‚å¯Ÿè¿™äº›å˜åŒ–å¦‚ä½•å½±å“AIåœ¨äº•å­—æ£‹æ¸¸æˆä¸­çš„ç­–ç•¥ã€‚
- en: More Advanced Techniques
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ›´é«˜çº§çš„æŠ€æœ¯
- en: To further improve the modelâ€™s performance, implementing a self-play mechanism,
    where the AI plays against versions of itself from different stages of training
    (rather than playing agains an opponent that makes random moves), could be an
    effective strategy. This approach has been successfully used in systems like AlphaGo
    and could lead to a more robust and adaptable AI player.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„è¡¨ç°ï¼Œå®æ–½è‡ªæˆ‘å¯¹å¼ˆæœºåˆ¶ï¼Œå³AIä¸æ¥è‡ªä¸åŒè®­ç»ƒé˜¶æ®µçš„è‡ªèº«ç‰ˆæœ¬å¯¹å¼ˆï¼ˆè€Œä¸æ˜¯ä¸è¿›è¡Œéšæœºç§»åŠ¨çš„å¯¹æ‰‹å¯¹å¼ˆï¼‰ï¼Œå¯èƒ½æ˜¯ä¸€ç§æœ‰æ•ˆçš„ç­–ç•¥ã€‚è¿™ç§æ–¹æ³•åœ¨AlphaGoç­‰ç³»ç»Ÿä¸­æˆåŠŸåº”ç”¨è¿‡ï¼Œå¹¶å¯èƒ½å¯¼è‡´æ›´å¼ºå¤§å’Œé€‚åº”æ€§æ›´å¼ºçš„AIç©å®¶ã€‚
- en: For more complex games, like Chess and Go, maintaining a Q-table will not be
    feasible any more as it becomes too big. In these games, incorporating techniques
    like Deep Q-learning could significantly enhance the AIâ€™s learning capability.
    By using a neural network to approximate the Q-table, the AI can handle more complex
    states beyond the simple 3x3 Tic-Tac-Toe grid, making it scalable for more complicated
    games.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ›´å¤æ‚çš„æ¸¸æˆï¼Œå¦‚å›½é™…è±¡æ£‹å’Œå›´æ£‹ï¼Œç»´æŒä¸€ä¸ªQè¡¨å°†ä¸å†å¯è¡Œï¼Œå› ä¸ºå®ƒå˜å¾—è¿‡äºåºå¤§ã€‚åœ¨è¿™äº›æ¸¸æˆä¸­ï¼Œé‡‡ç”¨åƒæ·±åº¦Qå­¦ä¹ è¿™æ ·çš„æŠ€æœ¯å¯ä»¥æ˜¾è‘—å¢å¼ºAIçš„å­¦ä¹ èƒ½åŠ›ã€‚é€šè¿‡ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥é€¼è¿‘Qè¡¨ï¼ŒAIå¯ä»¥å¤„ç†è¶…å‡ºç®€å•3x3äº•å­—æ£‹ç½‘æ ¼çš„æ›´å¤æ‚çŠ¶æ€ï¼Œä½¿å…¶åœ¨æ›´å¤æ‚çš„æ¸¸æˆä¸­å…·å¤‡å¯æ‰©å±•æ€§ã€‚
- en: In conclusion, the current setup has already shown promising results. These
    suggested improvements, however, could elevate the AIâ€™s performance further, transforming
    it from a competent Tic-Tac-Toe player into a sophisticated AI capable of tackling
    more complex strategic games.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä¹‹ï¼Œç›®å‰çš„è®¾ç½®å·²ç»å±•ç¤ºäº†æœ‰å¸Œæœ›çš„ç»“æœã€‚ç„¶è€Œï¼Œè¿™äº›å»ºè®®çš„æ”¹è¿›å¯èƒ½ä¼šè¿›ä¸€æ­¥æå‡AIçš„è¡¨ç°ï¼Œå°†å…¶ä»ä¸€ä¸ªåˆæ ¼çš„äº•å­—æ£‹ç©å®¶è½¬å˜ä¸ºä¸€ä¸ªèƒ½å¤Ÿåº”å¯¹æ›´å¤æ‚æˆ˜ç•¥æ¸¸æˆçš„é«˜çº§AIã€‚
- en: Further Material on the Topic
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¿›ä¸€æ­¥çš„ç›¸å…³èµ„æ–™
- en: 'If you are interested in learning more about how RL is being used for board
    games, check out the two videos below. The first one is quite short and dives
    into how modern chess AI bots are playing the game:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å¯¹å­¦ä¹ æ›´å¤šå…³äºå¼ºåŒ–å­¦ä¹ å¦‚ä½•åº”ç”¨äºæ£‹ç›˜æ¸¸æˆæ„Ÿå…´è¶£ï¼Œå¯ä»¥æŸ¥çœ‹ä¸‹é¢çš„ä¸¤ä¸ªè§†é¢‘ã€‚ç¬¬ä¸€ä¸ªè§†é¢‘éå¸¸ç®€çŸ­ï¼Œ*æ·±å…¥æ¢è®¨äº†ç°ä»£è±¡æ£‹ AI æœºå™¨äººå¦‚ä½•è¿›è¡Œæ¸¸æˆ*ï¼š
- en: 'The second video is the movie *AlphaGo* (which is free on Youtube), and tells
    story of how AlphaGo model was developed and how it beat the world champion at
    the time:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªè§†é¢‘æ˜¯ç”µå½±*AlphaGo*ï¼ˆåœ¨YouTubeä¸Šå…è´¹è§‚çœ‹ï¼‰ï¼Œè®²è¿°äº†AlphaGoæ¨¡å‹çš„å¼€å‘è¿‡ç¨‹ä»¥åŠå®ƒå¦‚ä½•å‡»è´¥å½“æ—¶çš„ä¸–ç•Œå† å†›ï¼š
- en: Heiko Hotz
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Heiko Hotz
- en: ğŸ‘‹ Follow me on [Medium](https://heiko-hotz.medium.com/) and [LinkedIn](https://www.linkedin.com/in/heikohotz/)
    to read more about Generative AI, Machine Learning, and Natural Language Processing.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ‘‹ åœ¨[Medium](https://heiko-hotz.medium.com/)å’Œ[LinkedIn](https://www.linkedin.com/in/heikohotz/)å…³æ³¨æˆ‘ï¼Œé˜…è¯»æ›´å¤šå…³äºç”Ÿæˆ
    AIã€æœºå™¨å­¦ä¹ å’Œè‡ªç„¶è¯­è¨€å¤„ç†çš„å†…å®¹ã€‚
- en: ğŸ‘¥ If youâ€™re based in London join one of our [NLP London Meetups](https://www.meetup.com/nlp_london/).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ‘¥ å¦‚æœä½ åœ¨ä¼¦æ•¦ï¼Œå¯ä»¥å‚åŠ æˆ‘ä»¬çš„[NLP London Meetups](https://www.meetup.com/nlp_london/)ã€‚
- en: '![](../Images/adb4c021b5bd4b11847ab1787aefa7aa.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/adb4c021b5bd4b11847ab1787aefa7aa.png)'
