- en: Building a Tree-Structured Parzen Estimator from Scratch (Kind Of)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头开始构建树状 Parzen 估计器（有点像）
- en: 原文：[https://towardsdatascience.com/building-a-tree-structured-parzen-estimator-from-scratch-kind-of-20ed31770478?source=collection_archive---------4-----------------------#2023-04-04](https://towardsdatascience.com/building-a-tree-structured-parzen-estimator-from-scratch-kind-of-20ed31770478?source=collection_archive---------4-----------------------#2023-04-04)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/building-a-tree-structured-parzen-estimator-from-scratch-kind-of-20ed31770478?source=collection_archive---------4-----------------------#2023-04-04](https://towardsdatascience.com/building-a-tree-structured-parzen-estimator-from-scratch-kind-of-20ed31770478?source=collection_archive---------4-----------------------#2023-04-04)
- en: An alternative to traditional hyperparameter tuning methods
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 传统超参数调整方法的替代方案
- en: '[](https://medium.com/@coljhor?source=post_page-----20ed31770478--------------------------------)[![Colin
    Horgan](../Images/1327a2996a36c0d5384e3316fd0ede2d.png)](https://medium.com/@coljhor?source=post_page-----20ed31770478--------------------------------)[](https://towardsdatascience.com/?source=post_page-----20ed31770478--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----20ed31770478--------------------------------)
    [Colin Horgan](https://medium.com/@coljhor?source=post_page-----20ed31770478--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@coljhor?source=post_page-----20ed31770478--------------------------------)[![Colin
    Horgan](../Images/1327a2996a36c0d5384e3316fd0ede2d.png)](https://medium.com/@coljhor?source=post_page-----20ed31770478--------------------------------)[](https://towardsdatascience.com/?source=post_page-----20ed31770478--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----20ed31770478--------------------------------)
    [Colin Horgan](https://medium.com/@coljhor?source=post_page-----20ed31770478--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8d3875046cb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-tree-structured-parzen-estimator-from-scratch-kind-of-20ed31770478&user=Colin+Horgan&userId=8d3875046cb&source=post_page-8d3875046cb----20ed31770478---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----20ed31770478--------------------------------)
    ·9 min read·Apr 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F20ed31770478&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-tree-structured-parzen-estimator-from-scratch-kind-of-20ed31770478&user=Colin+Horgan&userId=8d3875046cb&source=-----20ed31770478---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8d3875046cb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-tree-structured-parzen-estimator-from-scratch-kind-of-20ed31770478&user=Colin+Horgan&userId=8d3875046cb&source=post_page-8d3875046cb----20ed31770478---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----20ed31770478--------------------------------)
    · 9 分钟阅读 · 2023年4月4日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F20ed31770478&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-tree-structured-parzen-estimator-from-scratch-kind-of-20ed31770478&user=Colin+Horgan&userId=8d3875046cb&source=-----20ed31770478---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F20ed31770478&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-tree-structured-parzen-estimator-from-scratch-kind-of-20ed31770478&source=-----20ed31770478---------------------bookmark_footer-----------)![](../Images/2ab4359012ab270c86d4e9cb71a89fb3.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F20ed31770478&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-tree-structured-parzen-estimator-from-scratch-kind-of-20ed31770478&source=-----20ed31770478---------------------bookmark_footer-----------)![](../Images/2ab4359012ab270c86d4e9cb71a89fb3.png)'
- en: Visualization of TPE for 2-dimensional hyperparameter tuning. Image by [Alexander
    Elvers via Wikipedia Commons](https://commons.wikimedia.org/wiki/File:Hyperparameter_Optimization_using_Tree-Structured_Parzen_Estimators.svg).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 2维超参数调整的 TPE 可视化。图片由 [Alexander Elvers via Wikipedia Commons](https://commons.wikimedia.org/wiki/File:Hyperparameter_Optimization_using_Tree-Structured_Parzen_Estimators.svg)
    提供。
- en: The way a machine learning model fits itself to data is governed by a set of
    initial conditions called hyperparameters. Hyperparameters help to restrict the
    learning behavior of a model so that it will (hopefully) be able to fit the data
    well and within a reasonable amount of time. Finding the best set of hyperparameters
    (often called “tuning”) is one of the most important and time consuming parts
    of the modeling task. Historical approaches to hyperparameter tuning involve either
    a brute force or random search over a grid of hyperparameter combinations called
    Grid Search and Random Search, respectively. Although popular, Grid and Random
    Search methods lack any way of converging to a decent set of hyperparameters —
    that is, they are purely trial and error. In this article we will tackle a relatively
    new approach for hyperparameter tuning — the Tree-Structured Parzen Estimator
    (TPE) — and understand its function programmatically through a step-by-step implementation
    in Python.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型如何适应数据由一组称为超参数的初始条件决定。超参数有助于限制模型的学习行为，以便它（希望）能够很好地拟合数据并在合理的时间内完成。找到最佳的超参数集（通常称为“调优”）是建模任务中最重要且耗时的部分之一。历史上，超参数调优的方法涉及对超参数组合网格进行暴力搜索或随机搜索，分别称为网格搜索和随机搜索。尽管受欢迎，网格搜索和随机搜索方法缺乏收敛到一个合理的超参数集的方法——也就是说，它们完全是试错法。本文将探讨一种相对较新的超参数调优方法——树结构帕尔岑估计器（TPE）——并通过逐步实现的Python代码深入了解其功能。
- en: '![](../Images/8b0b65b9623213ff0e406dd23dee0e14.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b0b65b9623213ff0e406dd23dee0e14.png)'
- en: 'Figure 1: Animation of Grid Search and Random Search techniques.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：网格搜索和随机搜索技术的动画。
- en: TPE is a Bayesian optimization algorithm. This means it allows us to start with
    some initial beliefs about what our best model hyperparameters are, and update
    these beliefs in a principled way as we learn how different hyperparameters impact
    model performance. This is already a significant improvement over Grid Search
    and Random Search! Instead of determining the best hyperparameter set through
    trial and error, over time we can try more combinations of hyperparameters which
    lead to good models and fewer that do not.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: TPE 是一种贝叶斯优化算法。这意味着它允许我们从一些关于最佳模型超参数的初始信念开始，并在学习不同超参数如何影响模型性能时，以有原则的方式更新这些信念。这已经比网格搜索和随机搜索有了显著的改进！我们可以通过尝试更多导致良好模型的超参数组合，而不是通过试错法来确定最佳超参数集。
- en: 'TPE gets its name from two main ideas: 1\. using Parzen Estimation to model
    our beliefs about the best hyperparameters (more on this later) and 2\. using
    a tree-like data structure called a posterior-inference graph to optimize the
    algorithm runtime. In this example we will ignore the “tree-structured” part as
    it has nothing to do with hyperparameter tuning per se. Additionally, we will
    not go into the blow by blow of Bayesian statistics, expected improvement, etc.
    The goal here is to develop a high-level conceptual understanding of TPE and how
    it works. For a more thorough treatment of these topics see the original paper
    on TPE by J. Bergstra and colleagues [[1]](https://proceedings.neurips.cc/paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: TPE 得名于两个主要思想：1\. 使用帕尔岑估计来建模我们对最佳超参数的信念（稍后会详细介绍）和 2\. 使用一种称为后验推断图的树状数据结构来优化算法运行时间。在这个例子中，我们将忽略“树结构”部分，因为它与超参数调优本身无关。此外，我们不会深入讨论贝叶斯统计、期望改进等内容。这里的目标是对
    TPE 及其工作原理有一个高层次的概念理解。有关这些主题的更深入讨论，请参阅 J. Bergstra 和同事关于 TPE 的原始论文 [[1]](https://proceedings.neurips.cc/paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html)。
- en: Setting Up The Example
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置示例
- en: To build our implementation of TPE, we will need a toy example to work with.
    Let’s imagine we want to find the line of best-fit through some randomly generated
    data. In this example, we have two hyperparameters to tune — the line slope *m*
    and intercept *b*.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建我们的 TPE 实现，我们需要一个玩具示例来操作。假设我们想通过一些随机生成的数据找到最佳拟合线。在这个例子中，我们有两个超参数需要调优——线的斜率
    *m* 和截距 *b*。
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/067200e0643dfd2e28a1904dbdf0e9f0.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/067200e0643dfd2e28a1904dbdf0e9f0.png)'
- en: 'Figure 2: Randomly generated linear data to be used for TPE.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：用于 TPE 的随机生成线性数据。
- en: 'Since TPE is an optimization algorithm we also need some metric to optimize
    over. We will use root mean-squared error (RMSE). Let’s define the function `rmse`
    to calculate this metric as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 TPE 是一种优化算法，我们还需要一些指标来优化。我们将使用均方根误差（RMSE）。让我们定义函数 `rmse` 来计算这个指标，如下所示：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The last thing we will need is some initial beliefs about what our best hyperparameters
    are. Let’s say we believe that the slope of the line of best-fit is a uniform
    random variable on the interval (10,100) and intercept is also a uniform random
    variable on the interval (-6000, -3000). These distributions are called priors.
    That they are uniform random variables is equivalent to saying we believe the
    true values of the best hyperparameters are equally likely to lie anywhere within
    their respective intervals. We will implement a class to encapsulate these variables,
    and use them to define our initial search space.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的最后一件事是一些关于最佳超参数的初步信念。假设我们认为最佳拟合线的斜率在 (10,100) 区间上是一个均匀随机变量，而截距也在 (-6000,
    -3000) 区间上是一个均匀随机变量。这些分布称为先验分布。它们是均匀随机变量等同于说我们认为最佳超参数的真实值在各自的区间内有相同的可能性。我们将实现一个类来封装这些变量，并使用它们来定义我们的初始搜索空间。
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: With all this set up complete, we can move on to coding the algorithm itself.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 完成所有这些设置后，我们可以继续编写算法本身。
- en: '**Step 1: Random Exploration**'
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**步骤 1：随机探索**'
- en: The first step of TPE is to randomly sample sets of hyperparameters from our
    priors. This process gives us a first approximation of where the areas of our
    search space are that produce good models. The function `sample_priors` consumes
    our initial search space and a number of random samples to draw from it. It then
    evaluates the resulting models using our objective function `rmse` and returns
    a Pandas DataFrame containing the slope, intercept, and RMSE of each trial.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: TPE 的第一步是从我们的先验分布中随机采样超参数集合。这个过程给了我们一个关于哪些搜索空间区域能够生成良好模型的初步估计。函数 `sample_priors`
    消耗我们的初始搜索空间和要从中抽取的随机样本数。然后它使用我们的目标函数 `rmse` 评估生成的模型，并返回一个包含每次试验的斜率、截距和 RMSE 的
    Pandas 数据框。
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/8cc42a2b57351e22227deb27808292e8.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8cc42a2b57351e22227deb27808292e8.png)'
- en: 'Figure 3: Results of random sampling hyperparameters for 30 iterations. Each
    ‘x’ denotes a hyperparameter sample.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：30 次迭代的随机采样超参数结果。每个‘x’表示一个超参数样本。
- en: 'Step 2: Partitioning the Search Space and Parzen Estimation'
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 2：划分搜索空间和 Parzen 估计
- en: After generating some initial samples from our priors, we now split our hyperparameter
    search space in two using a quantile threshold γ, where γ is between 0 and 1\.
    Let’s arbitrarily choose γ=0.2\. Hyperparameter combinations which result in a
    model that performs in the top 20% of all models we have created thus far get
    grouped into a “good” distribution l(x). All other hyperparameter combinations
    belong to a “bad” distribution g(x).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在从我们的先验分布生成一些初始样本后，我们现在使用分位数阈值 γ 将超参数搜索空间分成两个部分，其中 γ 在 0 和 1 之间。我们可以任意选择 γ=0.2。那些产生模型表现位于我们目前创建的所有模型的前
    20% 的超参数组合被归入“良好”分布 l(x)。所有其他超参数组合则归入“差”分布 g(x)。
- en: '![](../Images/fdd6cfdf07d5465a134c3cb4b4f8445b.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fdd6cfdf07d5465a134c3cb4b4f8445b.png)'
- en: 'Figure 4: KDE of l(x) and g(x) distributions after 30 rounds of randomly selecting
    hyperparameters. Darker colored areas indicate higher density.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：经过 30 轮随机选择超参数后的 l(x) 和 g(x) 分布的 KDE。颜色较深的区域表示密度较高。
- en: It turns out that the best next combination of our hyperparameters to test is
    given by the maximum of g(x)/l(x) (if you would like to see the derivation of
    this see [[1]](https://proceedings.neurips.cc/paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html)).
    This intuitively makes sense. We want hyperparameters which are highly likely
    under our “good” distribution l(x) and not very likely under our “bad” distribution
    g(x). We can model each of g(x) and l(x) using Parzen Estimators which is where
    the “PE” in “TPE” comes from. The rough idea of Parzen Estimation a.k.a. Kernel
    Density Estimation (or KDE) is that we are going to average across a series of
    normal distributions each centered on an observation belonging to g(x) or l(x)
    (respectively). The resulting distributions have high density over regions of
    our search space where samples are close together and low density over regions
    where samples are far apart. To perform Parzen Estimation we will use the `KernelDensity`
    object from the SKLearn library. The function `segment_distributions` consumes
    our trials DataFrame and our threshold γ and returns a Parzen Estimator for l(x)
    and g(x) each. The resulting distributions are visualized in Figure 4.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，下一组要测试的最佳超参数组合由 g(x)/l(x) 的最大值给出（如果您想查看推导过程，请参见 [[1]](https://proceedings.neurips.cc/paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html)）。这在直观上是合理的。我们希望超参数在我们的“好”分布
    l(x) 下很可能，而在我们的“坏”分布 g(x) 下不太可能。我们可以使用 Parzen 估计器建模 g(x) 和 l(x)，这就是“TPE”中的“PE”来自哪里。Parzen
    估计的粗略思路即核密度估计（或 KDE）是我们将对一系列正态分布取平均，每个分布以属于 g(x) 或 l(x) 的观察值为中心。结果分布在我们的搜索空间中样本接近的区域具有高密度，而在样本远离的区域具有低密度。为了执行
    Parzen 估计，我们将使用 SKLearn 库中的 `KernelDensity` 对象。函数 `segment_distributions` 消耗我们的试验数据框和我们的阈值
    γ，并返回 l(x) 和 g(x) 的 Parzen 估计器。结果分布在图4中可视化。
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Step 3: Determining the Next “Best” Hyperparameters to Test'
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3步：确定要测试的下一个“最佳”超参数
- en: As mentioned in Step 2, the next best set of hyperparameters to test maximize
    g(x)/l(x). We can determine what this set of hyperparameters are in the following
    way. First we draw N random samples from l(x). Then for each of those samples
    we evaluate their log-likelihood with respect to l(x) and g(x), selecting the
    sample which maximizes g(x)/l(x) as the next hyperparameter combination to test.
    The SKLearn `KernelDensity` implementation we decided to use makes this computation
    very easy.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如第2步所述，下一组要测试的最佳超参数集最大化 g(x)/l(x)。我们可以通过以下方式确定这一组超参数。首先，我们从 l(x) 中抽取 N 个随机样本。然后，对于每一个样本，我们评估其相对于
    l(x) 和 g(x) 的对数似然，选择最大化 g(x)/l(x) 的样本作为下一组要测试的超参数组合。我们决定使用的 SKLearn `KernelDensity`
    实现使得这一计算非常简单。
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: All Together Now
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现在汇总
- en: All that is left for us to do is string all of the previously discussed components
    together into a single algorithm and we have our implementation of TPE! The decisions
    we have to make here are how many rounds of random exploration we want to perform,
    how many total iterations of the algorithm we want to complete, and what our cutoff
    threshold γ will be (yes, even TPE has hyperparameters). Here are a few things
    to consider when choosing these quantities.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的就是将之前讨论的所有组件串联起来，得到 TPE 的实现！我们需要做出的决定是我们想要进行多少轮随机探索，算法要完成多少次迭代，以及我们的截止阈值
    γ 将是多少（是的，即使是 TPE 也有超参数）。在选择这些数量时，有几个方面需要考虑。
- en: If the “best” set of hyperparameters are not captured by your priors before
    you start TPE, the algorithm may have a difficult time converging.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果在开始 TPE 之前，您的先验没有捕捉到“最佳”超参数集，那么算法可能会很难收敛。
- en: The more rounds of random exploration you perform, the better your initial approximations
    of g(x) and l(x) will be, which may improve the results of `tpe`.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你进行的随机探索轮次越多，对 g(x) 和 l(x) 的初步近似就会越准确，这可能会改善`tpe`的结果。
- en: The higher the value of γ, the fewer samples will end up in l(x). Having only
    a few samples to use when estimating l(x) may lead to poor hyperparameter selection
    by `tpe`.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: γ 的值越高，最终落在 l(x) 中的样本就会越少。仅有少量样本用于估计 l(x) 可能会导致 `tpe` 选择不佳的超参数。
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Results
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: 'To perform TPE over our synthetic data we created previously we run the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要对我们之前创建的合成数据执行 TPE，我们运行如下：
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: That’s it! We can now analyze our results. For the sake of brevity, the code
    used to generate the following visualizations will not be shown. However, the
    source code can be found in the GitHub repo for this project.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些！我们现在可以分析我们的结果。为了简洁起见，生成以下可视化所用的代码将不予展示。然而，源代码可以在此项目的GitHub仓库中找到。
- en: First let’s compare our best set of hyperparameters from TPE with the actual
    best slope and intercept from a regression solver.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们将TPE的最佳超参数设置与回归求解器得到的实际最佳斜率和截距进行比较。
- en: '![](../Images/5b18d8ac0f54935ab6ca2605165ec7fa.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b18d8ac0f54935ab6ca2605165ec7fa.png)'
- en: 'Figure 5: Line of best-fit using hyperparameters obtained by TPE and Linear
    Regression.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：使用TPE和线性回归获得的超参数的最佳拟合线。
- en: As we can see from Figure 5, with TPE we are able to closely approximate the
    best set of hyperparameters for our line. We wouldn’t expect TPE to out-perform
    the regression solver because linear regression has a closed-form solution. However,
    over an infinite number of trials we would expect it to converge to a similar
    solution.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从图5中可以看出，使用TPE我们能够密切接近我们线性回归模型的最佳超参数设置。我们不期望TPE能超越回归求解器，因为线性回归有一个封闭形式的解。然而，在无限次的试验中，我们期望它会收敛到类似的解。
- en: TPE is an optimization algorithm, so we don’t just care that we were able to
    find a decent set of hyperparameters. We also need to check that over the 200
    iterations our objective function decreases. Figure 6 demonstrates that after
    our initial 30 random samples our TPE implementation proceeds to minimize our
    objective function with a clear(ish) trend.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: TPE是一种优化算法，因此我们不仅关心是否能够找到一组不错的超参数。我们还需要检查在200次迭代中我们的目标函数是否降低。图6展示了在初始30个随机样本之后，我们的TPE实现有一个明确的（或大致上）趋势来最小化我们的目标函数。
- en: '![](../Images/85c6dbd49209c25f64eb03f3a4221644.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/85c6dbd49209c25f64eb03f3a4221644.png)'
- en: 'Figure 6: RMSE across all TPE trials. Note the difference in y-axis scale between
    the left and right plots. Red dot indicates trial with lowest RMSE.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：所有TPE试验中的RMSE。注意左侧和右侧图中y轴刻度的差异。红点表示具有最低RMSE的试验。
- en: So far everything looks great! The last thing we want to check is how our beliefs
    about the “best” distribution of our hyperparameters changed across our 200 iterations.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止一切看起来都很棒！我们最后要检查的是我们关于超参数“最佳”分布的信念在200次迭代中的变化。
- en: '![](../Images/c4ec859125be962d6c4b9f78e3481345.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4ec859125be962d6c4b9f78e3481345.png)'
- en: 'Figure 7: Parzen Estimation of l(x) across 200 iterations of our TPE algorithm.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：我们TPE算法在200次迭代中的l(x)的Parzen估计。
- en: As we can see in Figure 7, we start with a very broad distribution of l(x) which
    rapidly collapses down to something approximating our end result. Figures 6 and
    7 clearly illustrate how each of TPE’s three simple steps combine to give us an
    algorithm capable of exploring a search space in a complex yet intuitive way.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如图7所示，我们从一个非常宽泛的l(x)分布开始，迅速收敛到接近最终结果的分布。图6和图7清楚地说明了TPE的三个简单步骤如何组合成一个能够以复杂但直观的方式探索搜索空间的算法。
- en: '**Conclusions**'
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**结论**'
- en: Hyperparameter tuning is a critical part of the modeling process. While Grid
    Search and Random Search approaches are easy to implement, TPE as an alternative
    provides a more principled way of tuning hyperparameters and is pretty simple
    from a conceptual perspective. Several Python libraries exist with very good implementations
    of TPE including [Hyperopt](https://github.com/hyperopt/hyperopt) (which was created
    and is maintained by the authors of [1]) and [Optuna](https://optuna.org/). Whether
    for something as simple as our toy example or as complex as hyperparameter tuning
    for a neural network, TPE is a versatile, effective, and simple technique that
    has been gaining popularity in Data Science and Machine Learning over the last
    few years. Next time you find yourself tuning your model hyperparameters — maybe
    skip the Grid Search.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调优是建模过程中的关键部分。尽管网格搜索和随机搜索方法易于实现，但TPE作为替代方案提供了一个更有原则的超参数调优方法，从概念上也比较简单。许多Python库都很好的实现了TPE，包括[Hyperopt](https://github.com/hyperopt/hyperopt)（由[1]的作者创建并维护）和[Optuna](https://optuna.org/)。无论是像我们的玩具示例一样简单，还是像神经网络超参数调优一样复杂，TPE都是一个多功能、有效且简单的技术，近年来在数据科学和机器学习领域越来越受欢迎。下次当你在调整模型超参数时，或许可以跳过网格搜索。
- en: References
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Bergstra, J., Bardenet, R., Bengio, Y., & Kégl, B., Algorithms for hyper-parameter
    optimization (2011), *Advances in neural information processing systems*, *24*.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bergstra, J., Bardenet, R., Bengio, Y., & Kégl, B., 超参数优化算法 (2011)，*神经信息处理系统进展*，*24*。
- en: '*All images unless otherwise noted are by the author.*'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，否则所有图像均由作者提供。*'
