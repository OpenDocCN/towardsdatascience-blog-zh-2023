- en: Use GPT Models to Generate Text Data for Training Machine Learning Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 GPT 模型生成用于训练机器学习模型的文本数据
- en: 原文：[https://towardsdatascience.com/use-gpt-models-to-generate-text-data-for-training-machine-learning-models-3ff169ce5580](https://towardsdatascience.com/use-gpt-models-to-generate-text-data-for-training-machine-learning-models-3ff169ce5580)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/use-gpt-models-to-generate-text-data-for-training-machine-learning-models-3ff169ce5580](https://towardsdatascience.com/use-gpt-models-to-generate-text-data-for-training-machine-learning-models-3ff169ce5580)
- en: A step-by-step guide in Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python中的逐步指南
- en: '[](https://jin-cui.medium.com/?source=post_page-----3ff169ce5580--------------------------------)[![Jin
    Cui](../Images/e5ddcbaa6d7da38f960d2c5fea71b538.png)](https://jin-cui.medium.com/?source=post_page-----3ff169ce5580--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3ff169ce5580--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3ff169ce5580--------------------------------)
    [Jin Cui](https://jin-cui.medium.com/?source=post_page-----3ff169ce5580--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://jin-cui.medium.com/?source=post_page-----3ff169ce5580--------------------------------)[![Jin
    Cui](../Images/e5ddcbaa6d7da38f960d2c5fea71b538.png)](https://jin-cui.medium.com/?source=post_page-----3ff169ce5580--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3ff169ce5580--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3ff169ce5580--------------------------------)
    [Jin Cui](https://jin-cui.medium.com/?source=post_page-----3ff169ce5580--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3ff169ce5580--------------------------------)
    ·9 min read·Jul 12, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3ff169ce5580--------------------------------)
    ·9 min read·2023年7月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/8915005799ca4a06d7b79a06fea29dd9.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8915005799ca4a06d7b79a06fea29dd9.png)'
- en: Photo by [Claudio Schwarz](https://unsplash.com/@purzlbaum?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Claudio Schwarz](https://unsplash.com/@purzlbaum?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Motivation
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动机
- en: 'Data are fundamental to building Machine Learning models, yet text data for
    training Machine Learning models are difficult to collect for the following reasons:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 数据对于构建机器学习模型至关重要，但文本数据的收集却面临以下困难：
- en: Open-source text datasets are limited. Privacy rules and commercial confidentiality
    often restrict distribution of privileged data. In addition, publicly available
    datasets may not be licensed for commercial use, or more critically may not be
    context relevant. For example, IMDB movie reviews are unlikely to be meaningful
    for analysing customer sentiments towards Banking products.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开源文本数据集是有限的。隐私规则和商业机密通常限制了特权数据的分发。此外，公开的数据集可能不具备商业使用的许可证，或者更重要的是，可能不具有相关的上下文。例如，IMDB
    的电影评论可能对分析客户对银行产品的情感没有意义。
- en: Machine Learning models typically need a large amount of training data to perform.
    It may take a company, particularly a start-up, considerable time to collect a
    credible line of text data. In addition, these data may not have been labelled
    with a response variable for a specific Machine Learning task. For example, a
    company may have been collecting customer complaints verbatim, but may not necessarily
    have a granular understanding of the topics or sentiments of these complaints.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习模型通常需要大量的训练数据才能有效运作。公司，特别是初创公司，可能需要大量时间来收集可靠的文本数据。此外，这些数据可能没有针对特定机器学习任务标注响应变量。例如，一家公司可能收集了客户的投诉原文，但可能没有详细了解这些投诉的主题或情感。
- en: How can we overcome the above constraints and generate fit-for-purpose text
    data in a scalable and cost-effective way? Given the recent advances in Large
    Language Models and Generative AI, this article* provides a tutorial on generating
    synthetic text data by calling [OpenAI’s GPT model suites](https://platform.openai.com/docs/models)
    in Python.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何克服上述限制，以可扩展且具有成本效益的方式生成适合目的的文本数据？鉴于近期在大型语言模型和生成型人工智能方面的进展，本文*提供了一个关于通过调用
    [OpenAI 的 GPT 模型套件](https://platform.openai.com/docs/models) 来生成合成文本数据的教程。
- en: To demonstrate, let’s explore a use case of generating customer complaints data
    for an insurance company. With enriched text data for training language models,
    the use case is that the company could potentially achieve better customer outcomes
    by performing better in Natural Language Understanding tasks such as categorising
    complaints into topics or scoring complainant sentiments.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以演示为例，我们来探讨一个为保险公司生成客户投诉数据的用例。通过丰富的文本数据来训练语言模型，该用例是公司可能通过在自然语言理解任务中表现更好，例如将投诉分类到主题中或评分投诉者情感，从而实现更好的客户结果。
- en: '**This article is 100% ChatGPT-free.*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**这篇文章 100% 无 ChatGPT。*'
- en: 'Prerequisite: Setting up an OpenAI API key'
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前提条件：设置 OpenAI API 密钥
- en: To be able to call the GPT models, simply register an account with OpenAI and
    access the API key under [User Settings](https://platform.openai.com/account/api-keys).
    Make sure to keep this key private.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调用 GPT 模型，只需注册 OpenAI 账户并在 [用户设置](https://platform.openai.com/account/api-keys)
    下访问 API 密钥。请务必保持此密钥的私密。
- en: Note that depending on usage, accessing GPT models comes with a [cost](https://openai.com/pricing),
    although this hasn’t been material for myself (<$0.08 USD for preparing this tutorial).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，根据使用情况，访问 GPT 模型会有一定的 [费用](https://openai.com/pricing)，尽管对我来说这并不重要（准备此教程的费用不到
    $0.08 美元）。
- en: 'Step 1: Load the required Python packages'
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一步：加载所需的 Python 包
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Step 2: Generate a single customer complaint'
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二步：生成单个客户投诉
- en: Let’s start by generating one customer complaint data point using the *text-davinci-003*
    model under the GPT-3.5 model suites.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用 *text-davinci-003* 模型在 GPT-3.5 模型套件下生成一个客户投诉数据点。
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Note that with respect to the code above:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，关于上述代码：
- en: You need to insert your private API key string in the first line of the code.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要在代码的第一行插入你的私有 API 密钥字符串。
- en: The *prompt_text* helps the GPT model understand its role and thereby generate
    a customer complaint by responding to the question prompted.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*prompt_text* 帮助 GPT 模型理解其角色，从而通过回应提示问题生成客户投诉。'
- en: Other parameters referenced in the code (*temperature, top_n, max_tokens, frequency_penalty
    and presence_penalty*) are explained in more detail in [this section](https://medium.com/towards-data-science/beginners-guide-to-the-gpt-3-model-2daad7fc335a#2b15)
    of my article.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码中引用的其他参数（*temperature, top_n, max_tokens, frequency_penalty 和 presence_penalty*）在
    [这一部分](https://medium.com/towards-data-science/beginners-guide-to-the-gpt-3-model-2daad7fc335a#2b15)
    的文章中有更详细的解释。
- en: 'The customer complaints generated by the above code are:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成的客户投诉是：
- en: '*I am writing to express my extreme dissatisfaction with the life insurance
    company I am a customer of. The time it has taken to assess my life insurance
    claim has been unacceptable and has had a horrible impact on my customer experience.
    I find it unacceptable that I have been waiting for so long to receive a response
    to my claim. I urge the company to take immediate action to address this issue
    and to ensure that all customers receive prompt and satisfactory responses in
    the future.*'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*我写信是为了表达对我所购买的寿险公司极度的不满。评估我的寿险索赔所花费的时间不可接受，对我的客户体验造成了严重影响。我觉得我等待如此长时间才能收到对我索赔的回复是不可接受的。我敦促公司立即采取行动解决这一问题，并确保所有客户未来都能收到及时且令人满意的回应。*'
- en: This seems reasonable and coherent at first glance.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 初看起来这似乎是合理且连贯的。
- en: 'Step 3: Generate customer complaints at scale'
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三步：大规模生成客户投诉
- en: You may argue that you can simply replicate Step 2 by entering the *prompt_text*
    in ChatGPT. You are definitely right if you simply wish to generate limited number
    of data points. However, it’s not at all feasible to be repeating the exercise
    manually on the ChatGPT front-end for large scale text data generation. How do
    we then automate this task (ultimately scaling up the operation of generating
    customer complaints)? The answer lies in just a slight tweak to the code in Step
    2.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会争辩说，你可以通过在 ChatGPT 中输入 *prompt_text* 来简单地复制第二步。如果你只是想生成有限数量的数据点，你确实是对的。然而，对于大规模文本数据生成，手动在
    ChatGPT 前端重复此操作完全不可行。那么，我们如何自动化这个任务（最终扩展生成客户投诉的操作）呢？答案在于对第二步代码的一个小调整。
- en: By design of the GPT models as well as the nature of the *temperature* parameter
    which determines the creativity and diversity of the generated texts, each run
    of the code in Step 2 generates a different customer complaint. Given this, we
    just need to set up a loop for running the code in Step 2 *n* times and storing
    the output of each of these runs.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 GPT 模型的设计以及 *temperature* 参数的性质决定了生成文本的创造力和多样性，因此第 2 步中每次运行代码都会生成不同的客户投诉。鉴于此，我们只需要设置一个循环来运行第
    2 步中的代码 *n* 次，并存储每次运行的输出结果。
- en: 'To demonstrate, the code below creates a loop for generating *n = 3* complaints
    and storing the output in a dataframe:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示，下面的代码创建了一个循环，用于生成 *n = 3* 个投诉，并将输出存储在数据框中：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The snippet below shows the 3 customer complaints generated. Needless to say,
    the parameter *n* can be set to a number of your choice.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了生成的 3 条客户投诉。不用说，参数 *n* 可以设置为你选择的任意数字。
- en: '![](../Images/3ba2a79103e12bb256fa325f95cbe5f8.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ba2a79103e12bb256fa325f95cbe5f8.png)'
- en: 'Image 1: Generated customer complaints. Image by author.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图像 1：生成的客户投诉。图片作者提供。
- en: More Advanced Use Cases
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更高级的用例
- en: Zero-shot vs. Few-shot training for GPT models
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 零样本与少样本训练在 GPT 模型中的比较
- en: Providing standalone text prompts to GPT models per the use case above is considered
    **Zero-shot** training. Text data generated via Zero-shot training can be slightly
    generic for a specific task.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述用例向 GPT 模型提供独立的文本提示被视为**零样本**训练。通过零样本训练生成的文本数据可能会对特定任务略显通用。
- en: In a scenario where we already have limited but meaningful training data and
    would like to generate additional training data which provides some resemblance
    to the existing data, we can point the *prompt_text* input to existing data. This
    provides the GPT model **Few-shot** training.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们已经拥有有限但有意义的训练数据，并希望生成额外的训练数据以与现有数据相似的情况下，我们可以将 *prompt_text* 输入指向现有数据。这提供了
    GPT 模型**少样本**训练。
- en: 'For example, pretend we would like to Few-shot train a GPT model to generate
    some texts resembling the data from the IMDB movie review dataset (which is already
    stored by you in the *df_imdb* variable, hypothetically):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们希望对 GPT 模型进行少样本训练，以生成一些类似于 IMDB 电影评论数据集的文本（假设你已将其存储在 *df_imdb* 变量中）：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that this code also implicitly creates a label for the generated text data.
    In this instance, we are creating ‘positive’ reviews. Text data with a ‘negative’
    or ‘neutral’ label can be generated in a similar manner. This gives us a labelled
    dataset without any manual effort!
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这段代码还隐式地为生成的文本数据创建了标签。在这个实例中，我们正在生成‘正面’评论。具有‘负面’或‘中立’标签的文本数据也可以以类似的方式生成。这使我们能够获得带标签的数据集，而无需任何人工努力！
- en: Moreover, this code demonstrates that GPT models are able to generate data by
    recognising the pattern in the text prompt, without needing to be promoted with
    a question. In this instance, it’s been trained to give a movie review after being
    given the “REVIEW:” prompt.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这段代码展示了 GPT 模型能够通过识别文本提示中的模式生成数据，而无需通过问题进行引导。在这个实例中，它被训练以在给出“REVIEW:”提示后生成电影评论。
- en: 'To improve resemblance to existing data, we can even Few-shot train the GPT
    models with more than one data point from the existing data. This can be easily
    done by updating the *prompt_text* input to:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高与现有数据的相似性，我们甚至可以用现有数据中的多个数据点进行少样本训练。可以通过更新*prompt_text*输入来轻松完成：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Other GPT models
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他 GPT 模型
- en: By far the *text-davinci-003* GPT model has been used to demonstrate the use
    cases in this article. Other [GPT models](https://platform.openai.com/docs/models/gpt-3-5)
    can also be called via the *engine* parameter as needed. For instance, some GPT
    models such as *gpt-3.5-turbo* are more powerful than others, but may need to
    be called differently in Python as they take a ‘dialogue’ as input as opposed
    to a text string.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，*text-davinci-003* GPT 模型已被用来演示本文中的用例。其他 [GPT 模型](https://platform.openai.com/docs/models/gpt-3-5)
    也可以通过 *engine* 参数按需调用。例如，某些 GPT 模型如 *gpt-3.5-turbo* 比其他模型更强大，但在 Python 中可能需要不同的调用方式，因为它们接受的是‘对话’作为输入，而不是文本字符串。
- en: The code below shows a call to the *gpt-3.5-turbo* model for generating complaints
    data.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了调用 *gpt-3.5-turbo* 模型生成投诉数据。
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'And the output:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果：
- en: '![](../Images/ae8d3b96c02e9093130985f58ee30139.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae8d3b96c02e9093130985f58ee30139.png)'
- en: 'Image 2: Generated customer complaints. Image by author.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图像 2：生成的客户投诉。图片作者提供。
- en: Risks
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 风险
- en: 'Whilst the use cases above demonstrate the ready-set-go implementation of the
    GPT models (or more broadly, Large Language Models), given that these models are
    relatively new, users may need to be cautious about the known and unknown inherent
    risks associated with integrating these models to real-life data and the way we
    work. Specific to generating text data for training Machine Learning models, putting
    my Risk and Governance hat on, below are the key (known) risks in my view when
    implementing the technology in practice:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然上述用例展示了 GPT 模型（或更广义的大型语言模型）的即刻实施，但鉴于这些模型相对较新，用户可能需要对将这些模型与实际数据和工作方式整合所固有的已知和未知风险保持谨慎。特别是在生成用于训练机器学习模型的文本数据时，戴上我的风险和治理帽子，以下是我认为在实际实施技术时的关键（已知）风险：
- en: There may be **Privacy** concerns with respect to providing certain type of
    input to GPT models via OpenAI’s API endpoints. This is relevant when companies
    are leveraging proprietary data to generate augmenting text data (as discussed
    in the **Advanced Use Cases** above), particularly when it comes to providing
    input such as customer complaints which contain personal information. One potential
    mitigation against this is to de-identify personal information in a pre-processing
    step to ensure complaints data are used on an anonymous basis. In addition, companies
    using this kind of technology need to develop guardrails and policies governing
    the use of private and sensitive information.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于通过 OpenAI 的 API 端点提供某些类型的输入给 GPT 模型，可能存在**隐私**问题。这在公司利用专有数据生成增强文本数据时尤其相关（如上文的**高级用例**讨论），特别是当输入内容如包含个人信息的客户投诉时。一种潜在的缓解措施是，在预处理步骤中去标识个人信息，以确保投诉数据以匿名方式使用。此外，使用这种技术的公司需要制定监管措施和政策，管理私人和敏感信息的使用。
- en: For certain type of use cases, the text data generated by the GPT models may
    introduce **Biases**. For example, as GPT models were pre-trained based on large
    corpus of publicly available texts on the internet, for the task of generating
    both positive and negative customer feedback, it may inherently have bias towards
    the latter (i.e. customer complaints) under the assumption that these are more
    prominent on the internet. In addition, the texts generated may not be ‘fine-tuned’
    enough to cater for a specific product feature offered by a company as related
    texts may not be available on the internet. This ultimately presents a trade-off
    between efficient text data generation and usability of such data.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于某些类型的用例，GPT 模型生成的文本数据可能会引入**偏见**。例如，由于 GPT 模型是基于大量互联网上公开文本的语料库进行预训练的，因此在生成正面和负面客户反馈的任务中，它可能天然倾向于后者（即客户投诉），假设这些内容在互联网上更为突出。此外，生成的文本可能没有‘细致调整’到适应公司提供的特定产品特性，因为相关文本可能在互联网上不可用。这最终表现为在高效文本数据生成与数据可用性之间的权衡。
- en: With respect to **Recency**, although it’s expected that the GPT models will
    continue to receive updates, at time of writing most models were trained based
    on internet data up to September 2021 (i.e. almost 2 years ago).
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于**时效性**，尽管预计 GPT 模型将继续获得更新，但在撰写本文时，大多数模型的训练数据基于截至 2021 年 9 月的互联网数据（即将近 2 年前）。
- en: Other known risks associated with Large Language Models such as Hallucinations
    are less relevant in this context.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 与大语言模型相关的其他已知风险，如**幻觉**，在此背景下较不相关。
- en: Concluding Thoughts
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: This article offers a practical (and generative) way of some practical constraints
    to accessing textual data for training Machine Learning models.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提供了一种实际的（和生成的）方法来解决获取用于训练机器学习模型的文本数据的一些实际约束。
- en: Taking a step back, with proper attention and care around risks, this article
    is an example of how users can access OpenAI’s GPT models from the ‘back-end’.
    This allows users to unlock commercial opportunities for using Large Language
    Models backing ChatGPT which was originally designed for individual ad-hoc use
    cases.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 退一步说，正确关注和处理风险的情况下，本文展示了用户如何从‘后端’访问 OpenAI 的 GPT 模型。这使用户能够解锁利用大型语言模型支持 ChatGPT
    的商业机会，ChatGPT 最初是为个别临时用例设计的。
- en: '*Are you a fan of these practical tutorials on Machine Learning related topics?
    As I ride the AI/ML wave, I enjoy writing and sharing step-by-step guides and
    how-to tutorials in a comprehensive language with ready-to-run codes. If you would
    like to access all my articles (and articles from other practitioners/writers
    on Medium), you can sign up using* [*the link*](https://medium.com/@jin-cui/membership)
    *here!*'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*你是这些有关机器学习主题的实用教程的粉丝吗？随着我乘着人工智能/机器学习的浪潮，我喜欢用全面的语言编写和分享逐步指南和操作教程，并附有可直接运行的代码。如果你想访问我所有的文章（以及Medium上其他从业者/作者的文章），你可以通过*
    [*这个链接*](https://medium.com/@jin-cui/membership) *进行注册！*'
