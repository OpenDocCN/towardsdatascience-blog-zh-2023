- en: Serve Large Language Models from Your Computer with Text Generation Inference
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从你的电脑上提供大语言模型服务，通过文本生成推理
- en: 原文：[https://towardsdatascience.com/serve-large-language-models-from-your-computer-with-text-generation-inference-54f4dd8783a7](https://towardsdatascience.com/serve-large-language-models-from-your-computer-with-text-generation-inference-54f4dd8783a7)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/serve-large-language-models-from-your-computer-with-text-generation-inference-54f4dd8783a7](https://towardsdatascience.com/serve-large-language-models-from-your-computer-with-text-generation-inference-54f4dd8783a7)
- en: Examples with the instruct version of Falcon-7B
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Falcon-7B的指令版本的示例
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----54f4dd8783a7--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----54f4dd8783a7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----54f4dd8783a7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----54f4dd8783a7--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----54f4dd8783a7--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie?source=post_page-----54f4dd8783a7--------------------------------)[![本杰明·玛丽](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----54f4dd8783a7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----54f4dd8783a7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----54f4dd8783a7--------------------------------)
    [本杰明·玛丽](https://medium.com/@bnjmn_marie?source=post_page-----54f4dd8783a7--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----54f4dd8783a7--------------------------------)
    ·6 min read·Jul 13, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----54f4dd8783a7--------------------------------)
    ·阅读时间6分钟·2023年7月13日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/41e6b63543c42e19cc0b1e4ee23f95bf.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41e6b63543c42e19cc0b1e4ee23f95bf.png)'
- en: Photo by [Nana Dua](https://unsplash.com/@nanadua11?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Nana Dua](https://unsplash.com/@nanadua11?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Running very large language models (LLM) locally, on consumer hardware, is now
    possible thanks to quantization methods such as [QLoRa](/qlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b)
    and [GPTQ](https://github.com/IST-DASLab/gptq).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 通过量化方法如[QLoRa](/qlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b)和[GPTQ](https://github.com/IST-DASLab/gptq)，现在可以在消费者硬件上本地运行非常大的语言模型（LLM）。
- en: Considering how long it takes to load an LLM, we may also want to keep the LLM
    in memory to query it and have the results instantly. If you use LLMs with a standard
    inference pipeline, you must reload the model each time. If the model is very
    large, you may have to wait several minutes for the model to generate an output.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到加载大语言模型的时间，我们可能还希望将LLM保持在内存中以便即时查询并获取结果。如果你使用标准的推理管道，你必须每次重新加载模型。如果模型非常大，你可能需要等待几分钟才能生成输出。
- en: There are various frameworks that can host LLMs on a server (locally or remotely).
    On my blog, I have already presented the [Triton Inference Server which is a very
    optimized framework](https://medium.com/towards-data-science/deploy-your-local-gpt-server-with-triton-a825d528aa5d),
    developed by NVIDIA, to serve multiple LLMs and balance the load across GPUs.
    But if you have only one GPU and if you want to host your model on your computer,
    using a Triton inference may seem unsuitable.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 有各种框架可以在服务器（本地或远程）上托管大语言模型（LLMs）。在我的博客中，我已经介绍了[NVIDIA开发的非常优化的Triton推理服务器框架](https://medium.com/towards-data-science/deploy-your-local-gpt-server-with-triton-a825d528aa5d)，它用于服务多个LLMs，并在GPU之间平衡负载。但是，如果你只有一个GPU，并且希望在你的电脑上托管模型，使用Triton推理可能会显得不太合适。
- en: In this article, I present an alternative called Text Generation Inference.
    A more straightforward framework that implements all the minimum features to run
    and serve LLMs on consumer hardware.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我介绍了一种替代方案，称为文本生成推理。这是一个更简单的框架，实现了运行和服务LLMs所需的所有基本功能，适用于消费者硬件。
- en: After reading this article, you will have on your computer a chat model/LLM
    deployed locally and waiting for queries.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完本文后，你将在你的电脑上拥有一个本地部署并等待查询的聊天模型/LLM。
- en: Text Generation Inference
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本生成推理
- en: '[Text Generation Inference](https://github.com/huggingface/text-generation-inference)
    (TGI) is a framework written in Rust and Python for deploying and serving LLMs.
    It is developed by Hugging Face and distributed with an [Apache 2.0 license](https://github.com/huggingface/text-generation-inference/blob/main/LICENSE).
    Hugging Face uses it in production to power their inference widgets.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[文本生成推理](https://github.com/huggingface/text-generation-inference)（TGI）是一个用
    Rust 和 Python 编写的框架，用于部署和服务 LLM。它由 Hugging Face 开发，并以 [Apache 2.0 许可证](https://github.com/huggingface/text-generation-inference/blob/main/LICENSE)
    进行分发。Hugging Face 在生产中使用它来驱动他们的推理小部件。'
- en: Even though TGI has been optimized for A100 GPUs, I found TGI very suitable
    for self-hosted LLMs, on consumer hardware such as RTX GPUs, thanks to the support
    for quantization and [paged attention](https://medium.com/towards-data-science/vllm-pagedattention-for-24x-faster-llm-inference-fdfb1b80f83).
    However, it requires a particular installation to support RTX GPUs, which I will
    detail later in this article.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 TGI 已针对 A100 GPU 进行了优化，但由于对量化和 [分页注意力](https://medium.com/towards-data-science/vllm-pagedattention-for-24x-faster-llm-inference-fdfb1b80f83)的支持，我发现
    TGI 非常适合自托管的 LLM，在如 RTX GPU 这样的消费级硬件上表现出色。然而，它需要特定的安装来支持 RTX GPU，这一点我将在本文后续部分详细说明。
- en: Recently, I also found out that Hugging Face is optimizing some LLM architectures
    so that they run faster with TGI.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，我还发现 Hugging Face 正在优化一些 LLM 架构，以便它们在 TGI 下运行更快。
- en: This is notably the case for the Falcon models which are relatively slow when
    run with a standard inference pipeline but much faster when run with TGI. [One
    of the authors behind the Falcon models told me on Twitter](https://twitter.com/slippylolo/status/1673721277025009664?s=20)
    that this is because they rushed the implementation of multi-query attention while
    Hugging Face optimized it to work with TGI.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这尤其适用于 Falcon 模型，这些模型在使用标准推理管道时运行较慢，但在使用 TGI 时运行更快。[一位 Falcon 模型的作者在 Twitter
    上告诉我](https://twitter.com/slippylolo/status/1673721277025009664?s=20)，这是因为他们在多查询注意力的实现上匆忙，而
    Hugging Face 则优化了它以便与 TGI 配合使用。
- en: 'Several LLM architectures are optimized this way to run faster with TGI: BLOOM,
    OPT, GPT-NeoX, etc. The full list is available and regularly updated on TGI’s
    GitHub.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种 LLM 架构以这种方式进行了优化，以便在 TGI 下运行得更快：BLOOM、OPT、GPT-NeoX 等。完整列表可以在 TGI 的 GitHub
    上找到，并定期更新。
- en: Set up Text Generation Inference
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置文本生成推理
- en: Hardware and Software Requirements
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 硬件和软件要求
- en: I tested with an RTX 3060 12 GB. It should work with all the RTX 30x and 40x
    but note that TGI is specially optimized for A100 GPUs.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我在 RTX 3060 12 GB 上进行了测试。它应该适用于所有 RTX 30x 和 40x，但请注意 TGI 特别优化了 A100 GPU。
- en: To run the commands, you will need a UNIX OS. I used Ubuntu 20.04 through Windows
    WSL2.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行这些命令，你需要一个 UNIX 操作系统。我使用了通过 Windows WSL2 的 Ubuntu 20.04。
- en: It should also work without modifications on Mac OS.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 它在 Mac OS 上也应该可以正常工作。
- en: TGI requires Python ≥ 3.9.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: TGI 需要 Python ≥ 3.9。
- en: I will first present how to install TGI from scratch which I think is not straightforward.
    If you run into issues during the installation, you may need to run a Docker image
    instead. I’ll address both scenarios.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我将首先介绍如何从零开始安装 TGI，我认为这并不简单。如果你在安装过程中遇到问题，可能需要改用 Docker 镜像。我将讨论这两种情况。
- en: Set up
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置
- en: 'TGI is written in Rust. You need it installed. If you don’t have it, run the
    following command in your terminal:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: TGI 是用 Rust 编写的。你需要先安装它。如果你没有安装，运行以下命令：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: It should take less than 2 minutes. I recommend restarting your shell, e.g.,
    opening a new terminal, to make sure that all your environment variables are correctly
    updated.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该花费不到 2 分钟。我建议重启你的 shell，例如，打开一个新的终端，以确保所有环境变量正确更新。
- en: Then, we create a dedicated conda environment. This step is optional but I prefer
    to have a separate environment for each one of my projects.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建一个专用的 conda 环境。此步骤是可选的，但我更喜欢为每个项目创建一个独立的环境。
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We have also to install Protoc. Hugging Face currently recommends version 21.12\.
    You will need sudo privileges for this.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要安装 Protoc。Hugging Face 目前推荐版本 21.12。你需要具有 sudo 权限。
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We have installed all the requirements. Now, we can install TGI.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经安装了所有要求。现在，我们可以安装 TGI。
- en: 'First, clone the GitHub repository:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，克隆 GitHub 仓库：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'And then install TGI:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然后安装 TGI：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*Note: I set BUILD_EXTENSIONS to False to deactivate the custom CUDA kernels
    since I don’t have A100 GPUs.*'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：我将 BUILD_EXTENSIONS 设置为 False 以停用自定义 CUDA 内核，因为我没有 A100 GPU。*'
- en: It should install smoothly… On my computer, it didn’t. I had to run all the
    commands in the file server/Makefile by hand. I suspect a problem with my environment
    variables that were not loaded properly due to “make” switching to a different
    shell for some reason. You may have to do the same.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 应该可以顺利安装……但在我的计算机上却没有。我不得不手动运行 server/Makefile 文件中的所有命令。我怀疑是由于“make”由于某种原因切换到不同的
    shell，导致我的环境变量没有正确加载。你可能也需要这样做。
- en: '*Note: If you fail to install it, don’t worry! Hugging Face has created a Docker
    image that you can launch to start the server, as we will see in the next section.*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：如果安装失败，不用担心！Hugging Face 创建了一个 Docker 镜像，你可以启动它以启动服务器，我们将在下一部分中看到。*'
- en: Launching a model with TGI
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TGI 启动模型
- en: 'For the following examples, I use the instruct version of the [Falcon-7B model](https://huggingface.co/tiiuae/falcon-7b-instruct)
    which is distributed under an Apache 2.0 license. If you want to know more about
    the Falcon models, I presented them in a previous article:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于以下示例，我使用的是 [Falcon-7B 模型](https://huggingface.co/tiiuae/falcon-7b-instruct)
    的 instruct 版本，它在 Apache 2.0 许可证下分发。如果你想了解更多关于 Falcon 模型的信息，我在上一篇文章中做了介绍：
- en: '[](/introduction-to-the-open-llm-falcon-40b-performance-training-data-and-architecture-98388fa40226?source=post_page-----54f4dd8783a7--------------------------------)
    [## Introduction to the Open LLM Falcon-40B: Performance, Training Data, and Architecture'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[## Open LLM Falcon-40B 介绍：性能、训练数据和架构](https://example.org/introduction-to-the-open-llm-falcon-40b-performance-training-data-and-architecture-98388fa40226?source=post_page-----54f4dd8783a7--------------------------------)'
- en: Get started using Falcon-7B, Falcon-40B, and their instruct versions
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开始使用 Falcon-7B、Falcon-40B 及其 instruct 版本
- en: towardsdatascience.com](/introduction-to-the-open-llm-falcon-40b-performance-training-data-and-architecture-98388fa40226?source=post_page-----54f4dd8783a7--------------------------------)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](https://example.org/introduction-to-the-open-llm-falcon-40b-performance-training-data-and-architecture-98388fa40226?source=post_page-----54f4dd8783a7--------------------------------)'
- en: Without Docker
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不使用 Docker
- en: The installation created a new command “text-generation-launcher” that will
    start the TGI server.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 安装创建了一个新的命令“text-generation-launcher”，它将启动 TGI 服务器。
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'model-id: The model name on the [Hugging Face Hub](https://huggingface.co/models).'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: model-id：模型名称在 [Hugging Face Hub](https://huggingface.co/models)。
- en: 'num-shard: Set it to the number of GPUs you have and that you would like to
    exploit.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: num-shard：设置为你拥有的 GPU 数量，以及你希望利用的数量。
- en: 'port: The port on which you want the server to listen.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: port：你希望服务器监听的端口。
- en: 'quantize: If you are using a GPU with less than 24 GB of VRAM, you will need
    to quantize the model to avoid running out of memory. Here I choose “bitsandbytes”
    for on-the-fly quantization. GPTQ (“gptq”) is also available but I’m less familiar
    with this algorithm.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: quantize：如果你使用的 GPU 内存少于 24 GB，你需要对模型进行量化，以避免内存不足。我选择了“bitsandbytes”进行即时量化。GPTQ（“gptq”）也可用，但我对这个算法不太熟悉。
- en: With Docker (if the manual installation failed)
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Docker（如果手动安装失败）
- en: '*Note: if the Docker daemon is not running, and if you run Ubuntu through WSL,
    start the daemon in another terminal with “sudo dockerd”.*'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：如果 Docker 守护进程未运行，并且你通过 WSL 运行 Ubuntu，请在另一个终端中使用“sudo dockerd”启动守护进程。*'
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The parameters are almost the same as with text-generation-launcher. You can
    replace “all” with “0” if you have only one GPU.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 参数几乎与 text-generation-launcher 一样。如果你只有一个 GPU，你可以将“all”替换为“0”。
- en: Keep this Docker image running as long as you want to use the server.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 保持这个 Docker 镜像运行，只要你想使用服务器。
- en: Querying a model with TGI
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TGI 查询模型
- en: 'To query the model served by TGI with a Python script, you will have to install
    the following library:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 要用 Python 脚本查询 TGI 提供的模型，你需要安装以下库：
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then in a Python script, write something like this:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在 Python 脚本中，写类似这样的代码：
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'It should print:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 它应该打印：
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Which is a translation of poor quality. This is expected from a 7-billion-parameter
    model. It is slightly better at coding tasks:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种较差质量的翻译。这是对一个 70 亿参数模型的预期。它在编码任务上略好一些：
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'It generates:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 它生成：
- en: '[PRE11]javascript'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE11]javascript'
- en: function removeSpaces(str) {
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: function removeSpaces(str) {
- en: return str.replace(/\s+/g, '');
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: return str.replace(/\s+/g, '');
- en: '}'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: console.log(removeSpaces('Hello World'));
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: console.log(removeSpaces('Hello World'));
- en: console.log(removeSpaces('Hello World'));
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: console.log(removeSpaces('Hello World'));
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You can also query with curl instead of a Python script:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以用 curl 进行查询，而不是 Python 脚本：
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: TGI is indeed fast. Generating an output with Falcon-7B and a maximum number
    of tokens set to 500 takes only a few seconds with my RTX 3060 GPU.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: TGI 确实很快。使用 Falcon-7B 并将最大 token 数设置为 500，仅需几秒钟，我的 RTX 3060 GPU 即可完成。
- en: With the standard inference pipeline, it takes nearly 40 seconds, without even
    counting the time it takes to load the model.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用标准推理管道，几乎需要40秒，还不包括加载模型所需的时间。
- en: Conclusion
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Self-hosting a chat model (i.e., instruct LLM) has many advantages. The main
    one is that you don’t send your data on the Internet. Another one is that you
    completely control the operating cost, which is only reflected in your electricity
    bill.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 自托管聊天模型（即，指导LLM）有很多优势。主要的是你不会将数据发送到互联网。另一个是你完全控制操作成本，这只反映在你的电费账单上。
- en: However, if you use a consumer GPU, you won’t be able to run state-of-the-art
    LLMs. Even for smaller LLMs, we have to quantize them to run only GPUs equipped
    with less than 24 GB of VRAM. Quantization also reduces LLMs’ accuracy.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你使用消费级GPU，你将无法运行最先进的LLM。即使是较小的LLM，我们也必须将其量化，以便在配备少于24 GB VRAM的GPU上运行。量化还会降低LLM的准确性。
- en: 'Nonetheless, even small quantized LLMs can still be good for simple tasks:
    simple coding problems, binary classification, …'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，即使是小型量化LLM也仍然适合简单任务：简单的编码问题、二元分类……
- en: You can now do all these tasks on your computer, just by querying your self-hosted
    LLM.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以通过查询你的自托管LLM在计算机上完成所有这些任务。
- en: '*If you like this article and would be interested to read the next ones, the
    best way to support my work is to become a Medium member using this link:*'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你喜欢这篇文章并对阅读接下来的文章感兴趣，支持我工作的最佳方式是通过这个链接成为Medium会员：*'
- en: '[](https://medium.com/@bnjmn_marie/membership?source=post_page-----54f4dd8783a7--------------------------------)
    [## Join Medium with my referral link - Benjamin Marie'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie/membership?source=post_page-----54f4dd8783a7--------------------------------)
    [## 通过我的推荐链接加入Medium - Benjamin Marie]'
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story…
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 作为Medium会员，你的一部分会员费会给你阅读的作者，并且你可以全面访问每一个故事……
- en: medium.com](https://medium.com/@bnjmn_marie/membership?source=post_page-----54f4dd8783a7--------------------------------)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@bnjmn_marie/membership?source=post_page-----54f4dd8783a7--------------------------------)
- en: '*If you are already a member and want to support this work,* [*just follow
    me on Medium*](https://medium.com/@bnjmn_marie)*.*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你已经是会员并且想支持这项工作，* [*只需关注我在Medium上的账户*](https://medium.com/@bnjmn_marie)*。*'
