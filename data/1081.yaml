- en: Transformers in depth – Part 1\. Introduction to Transformer models in 5 minutes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入了解Transformers – 第1部分。5分钟介绍Transformer模型
- en: 原文：[https://towardsdatascience.com/transformers-in-depth-part-1-introduction-to-transformer-models-in-5-minutes-ad25da6d3cca?source=collection_archive---------0-----------------------#2023-03-27](https://towardsdatascience.com/transformers-in-depth-part-1-introduction-to-transformer-models-in-5-minutes-ad25da6d3cca?source=collection_archive---------0-----------------------#2023-03-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/transformers-in-depth-part-1-introduction-to-transformer-models-in-5-minutes-ad25da6d3cca?source=collection_archive---------0-----------------------#2023-03-27](https://towardsdatascience.com/transformers-in-depth-part-1-introduction-to-transformer-models-in-5-minutes-ad25da6d3cca?source=collection_archive---------0-----------------------#2023-03-27)
- en: Understanding Transformer architecture and its key insights in 5 minutes
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在5分钟内理解Transformer架构及其关键见解
- en: '[](https://gabrielfurnieles.medium.com/?source=post_page-----ad25da6d3cca--------------------------------)[![Gabriel
    Furnieles](../Images/710c939d8114ea8a4db16fd9f2c71f8a.png)](https://gabrielfurnieles.medium.com/?source=post_page-----ad25da6d3cca--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ad25da6d3cca--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ad25da6d3cca--------------------------------)
    [Gabriel Furnieles](https://gabrielfurnieles.medium.com/?source=post_page-----ad25da6d3cca--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://gabrielfurnieles.medium.com/?source=post_page-----ad25da6d3cca--------------------------------)[![Gabriel
    Furnieles](../Images/710c939d8114ea8a4db16fd9f2c71f8a.png)](https://gabrielfurnieles.medium.com/?source=post_page-----ad25da6d3cca--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ad25da6d3cca--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ad25da6d3cca--------------------------------)
    [Gabriel Furnieles](https://gabrielfurnieles.medium.com/?source=post_page-----ad25da6d3cca--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe77c10fd9715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-in-depth-part-1-introduction-to-transformer-models-in-5-minutes-ad25da6d3cca&user=Gabriel+Furnieles&userId=e77c10fd9715&source=post_page-e77c10fd9715----ad25da6d3cca---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ad25da6d3cca--------------------------------)
    ·9 min read·Mar 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fad25da6d3cca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-in-depth-part-1-introduction-to-transformer-models-in-5-minutes-ad25da6d3cca&user=Gabriel+Furnieles&userId=e77c10fd9715&source=-----ad25da6d3cca---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe77c10fd9715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-in-depth-part-1-introduction-to-transformer-models-in-5-minutes-ad25da6d3cca&user=Gabriel+Furnieles&userId=e77c10fd9715&source=post_page-e77c10fd9715----ad25da6d3cca---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ad25da6d3cca--------------------------------)
    ·9分钟阅读·2023年3月27日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fad25da6d3cca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-in-depth-part-1-introduction-to-transformer-models-in-5-minutes-ad25da6d3cca&user=Gabriel+Furnieles&userId=e77c10fd9715&source=-----ad25da6d3cca---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fad25da6d3cca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-in-depth-part-1-introduction-to-transformer-models-in-5-minutes-ad25da6d3cca&source=-----ad25da6d3cca---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fad25da6d3cca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-in-depth-part-1-introduction-to-transformer-models-in-5-minutes-ad25da6d3cca&source=-----ad25da6d3cca---------------------bookmark_footer-----------)'
- en: This is the first part of the article’s extended version, soon you will find
    its continuation [here](https://medium.com/@gabrielfurnieles).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这是文章扩展版的第一部分，不久你将能在[这里](https://medium.com/@gabrielfurnieles)找到其后续内容。
- en: '**Author’s note.** For this very first part, I’ve decided to introduce the
    notions and concepts necessary to get a better understanding of Transformer models
    and to make it easier to follow the next chapter. If you are already familiar
    with Transformers, you can check the last section to get a summary of this article
    and feel free to jump to the second part, where more mathematics and complex notions
    are presented. Nevertheless, I hope you find some value in the explanations of
    this text too. Thanks for reading!'
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**作者注释。** 在这一部分，我决定介绍理解 Transformer 模型所需的概念和知识，以便更好地理解接下来的章节。如果你已经熟悉 Transformers，可以查看最后一节以获取本文的总结，并随意跳到第二部分，其中介绍了更多数学和复杂的概念。尽管如此，我希望你也能从这部分的解释中获得一些价值。感谢阅读！'
- en: Since the release of the latest Large Language Models (LLaM), like the GPT series
    of [OpenAI](https://openai.com/blog/chatgpt), the open source model [Bloom](https://bigscience.huggingface.co/blog/bloom)
    or Google’s announcements about [LaMDA](https://blog.google/technology/ai/lamda/)
    among others, Transformers have demonstrated their huge potential and have become
    the cutting-edge architecture for Deep Learning models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 自从最新的大型语言模型（LLaM）发布以来，例如 [OpenAI](https://openai.com/blog/chatgpt) 的 GPT 系列，[Bloom](https://bigscience.huggingface.co/blog/bloom)
    开源模型或 Google 关于 [LaMDA](https://blog.google/technology/ai/lamda/) 的公告等，Transformers
    展示了其巨大潜力，成为深度学习模型的前沿架构。
- en: Although several articles have been written on transformers and the mathematics
    under their hood [[2]](https://arxiv.org/pdf/2207.09238.pdf) [[3]](https://jalammar.github.io/illustrated-transformer/)
    [[4]](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/),
    in this series of articles I’d like to present a complete overview combining what
    I’ve considered the best approaches, with my own point of view and personal experience
    working with Transformer models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管已有几篇关于 Transformer 及其背后数学的文章 [[2]](https://arxiv.org/pdf/2207.09238.pdf) [[3]](https://jalammar.github.io/illustrated-transformer/)
    [[4]](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)，在这一系列文章中，我希望提供一个完整的概述，结合我认为最好的方法、我个人的观点以及与
    Transformer 模型工作的经验。
- en: This article attempts to provide a deep mathematical overview of Transformer
    models, showing the source of their powerand explaining the reason behind each
    of its modules.
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本文试图提供 Transformer 模型的深度数学概述，展示其强大的来源并解释其每个模块背后的原因。
- en: '**Note.** The article follows the original transformer model from the paper
    [Vaswani, Ashish, et al. 2017.](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注。** 文章遵循了原始 Transformer 模型，参考了论文 [Vaswani, Ashish, et al. 2017.](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)'
- en: Setting the environment. A brief introduction to Natural Language Processing
    (NLP)
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 环境设置。自然语言处理（NLP）简要介绍
- en: Before getting started with the Transformer model, it is necessary to understand
    the task for which they have been created, to process text.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始使用 Transformer 模型之前，有必要了解它们创建的任务，即处理文本。
- en: Since neural networks work with numbers, in order to feed text to a neural network
    we must first transform it into a numerical representation. The act of transforming
    text (or any other object) into a numerical form is called **embedding**. Ideally,
    the embedded representation is able to reproduce characteristics of the text such
    as the relationships between words or the sentiments of the text.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 由于神经网络处理的是数字，为了将文本输入神经网络，我们必须首先将其转换为数字表示。将文本（或其他对象）转换为数字形式的过程称为 **嵌入**。理想情况下，嵌入表示能够再现文本的特征，例如单词之间的关系或文本的情感。
- en: There are several ways to perform the embedding and it is not the purpose of
    this article to explain them (more information can be found in NLP Deep Learning),
    but rather we should understand their general mechanisms and the outputs they
    produce. If you are not familiar with embeddings, just think of them as another
    layer in the model’s architecture that transforms text into numbers.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入有多种实现方式，本文并不打算解释它们（更多信息可以在 NLP 深度学习中找到），而是要理解它们的一般机制和产生的输出。如果你不熟悉嵌入，只需将其视为模型架构中的另一层，将文本转换为数字。
- en: The most commonly used embeddings work on the words of the text, transforming
    each word into a vector of a really high dimension (the elements into which the
    text is divided to apply the embedding are called **tokens**). In the original
    paper [1] the **embedding dimension** for each token/word was *512*. It is important
    to note that the vector modulus is also **normalized** so the neural network is
    able to learn correctly and avoid exploding gradients.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的嵌入方法作用于文本中的单词，将每个单词转换为一个高维向量（文本中分割的元素称为**标记**）。在原始论文 [1] 中，每个标记/单词的**嵌入维度**为*512*。值得注意的是，向量的模也**被归一化**，以便神经网络能够正确学习并避免梯度爆炸。
- en: An important element of embedding is the **vocabulary**. This corresponds to
    the set of all tokens (words) that can be used to feed the Transformer model.
    The vocabulary isn’t necessarily to be just the words used in the sentences, but
    rather any other word related to its topic. For instance, if the Transformer will
    be used to analyze legal documents, every word related to the bureaucratic jargon
    must be included in the vocabulary. Note that the larger the vocabulary (if it
    is related to the Transformer task), the better the embedding will be able to
    find relationships between tokens.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入的一个重要元素是**词汇表**。这对应于可以用来输入到 Transformer 模型中的所有标记（单词）集合。词汇表不一定仅限于句子中使用的单词，而是与其主题相关的任何其他单词。例如，如果
    Transformer 将用于分析法律文件，那么与官僚术语相关的每个单词都必须包含在词汇表中。请注意，词汇表越大（如果与 Transformer 任务相关），嵌入就越能找到标记之间的关系。
- en: Apart from the words, there are some other **special tokens** added to the vocabulary
    and the text sequence. These tokens mark special parts of the text like the beginning
    <START>, the end <END>, or padding <PAD> (the padding is added so that **all sequences
    have the same length**). The special tokens are also embedded as vectors.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 除了单词外，还有一些其他**特殊标记**被添加到词汇表和文本序列中。这些标记标记文本中的特殊部分，如开始<START>、结束<END>或填充<PAD>（填充是为了使**所有序列具有相同长度**）。这些特殊标记也被嵌入为向量。
- en: In mathematics, the embedding space constitutes a **normalized vector space**
    in which each vector corresponds to a specific token. The basis of the vector
    space is determined by the relationships the embedding layer has been able to
    find among the tokens. For example, one dimension might correspond to the verbs
    that end in *-ing*, another one could be the adjectives with positive meaning,
    etc. Moreover, the angle between vectors determines the similarity between tokens,
    forming clusters of tokens that have a semantic relationship.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学中，嵌入空间构成了一个**标准化向量空间**，其中每个向量对应于一个特定的标记。向量空间的基由嵌入层能够找到的标记之间的关系决定。例如，一个维度可能对应于以*-ing*
    结尾的动词，另一个可能是具有积极含义的形容词，等等。此外，向量之间的角度决定了标记之间的相似性，形成具有语义关系的标记簇。
- en: '![](../Images/378e6c974c5288ccdef023e5404cce13.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/378e6c974c5288ccdef023e5404cce13.png)'
- en: Text embedding illustration. On the left, words/tokens are embedded as vectors
    where d_model represents the maximum embedding dimensions. On the right, the first
    3 dimensions have been represented. Although the example is exaggerated, note
    how similar words form groups (clusters) representing the similarity between them.
    Image by the author.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 文本嵌入示意图。左侧，单词/标记被嵌入为向量，其中 d_model 代表最大嵌入维度。右侧，表示了前 3 个维度。尽管示例有所夸张，但请注意相似的单词如何形成组（簇），代表它们之间的相似性。图片由作者提供。
- en: '**Note 1.** Although only the task of text processing has been mentioned, Transformers
    are in fact designed to process any type of sequential data.'
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注 1.** 尽管只提到了文本处理任务，但实际上 Transformer 是为了处理任何类型的序列数据而设计的。'
- en: Transformer’s workflow
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer 的工作流程
- en: '![](../Images/fd2c0d057ce9723313f403dbb091328a.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd2c0d057ce9723313f403dbb091328a.png)'
- en: Transformer diagram. Image from the paper [Vaswani, Ashish, et al. 2017.](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 图示。图片来源于论文 [Vaswani, Ashish, et al. 2017.](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
- en: Above, is one of the most replicated diagrams in the last years of Deep Learning
    research. It summarizes the complete workflow of Transformers, representing each
    of the parts/modules involved in the process.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 上面是近年来深度学习研究中最常见的图示之一。它总结了 Transformers 的完整工作流程，表示了过程中的每个部分/模块。
- en: A higher perspective view divides Transformers into **Encoder** (left blue block
    in the diagram)and **Decoder** (right blue block).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从更高的视角来看，Transformer 被分为 **编码器**（图中的左侧蓝色块）和 **解码器**（右侧蓝色块）。
- en: '**To illustrate how Transformers work I will use the example task of text translation
    from English to Spanish.**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**为了说明 Transformer 的工作原理，我将使用从英语到西班牙语的文本翻译任务作为例子。**'
- en: Encoder’s objective is to find the relationships between the tokens of the input
    sequence i.e. the sentence to translate into Spanish. It gets the English sentence
    as input (after applying the embedding) and outputs the same sequence weighted
    by the attention mechanism. Mathematically, [the Encoder performs a transformation
    in the embedding space of English tokens, weighting the vectors according to their
    importance in the sentence meaning](http://a).
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器的目标是找出输入序列中各个标记之间的关系，即需要翻译成西班牙语的句子。它将英语句子作为输入（应用嵌入后），并输出加权后的相同序列，权重由注意力机制决定。从数学上讲，[编码器在英语标记的嵌入空间中执行变换，根据句子的意义对向量进行加权](http://a)。
- en: '**Note 2.** I haven’t defined yet what attention is, but essentially think
    of it as a function that returns some coefficients that define the importance
    of each word in the sentence with respect to the others.'
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意 2.** 我还没有定义注意力是什么，但本质上可以将其视为一个函数，该函数返回一些系数，用于定义句子中每个词相对于其他词的重要性。'
- en: On the other hand, Decoder takes as first input the translated sentence in Spanish
    (*Outputs* in the original diagram), applies attention, and then combines the
    result with the encoder’s output in another attention mechanism. Intuitively,
    [the Decoder learns to relate the target embedding space (Spanish) with respect
    to the input embedding space (English) so that it finds a basis transformation
    between both vector spaces.](http://a)
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，解码器以翻译后的西班牙语句子（原始图中的*Outputs*）作为第一个输入，应用注意力机制，然后将结果与编码器的输出通过另一种注意力机制进行结合。直观地说，[解码器学会了将目标嵌入空间（西班牙语）与输入嵌入空间（英语）相关联，从而在这两个向量空间之间找到基础变换。](http://a)
- en: '![](../Images/8ed0d9b4caf9c66100ef7ac4123b1737.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8ed0d9b4caf9c66100ef7ac4123b1737.png)'
- en: Transformer’s high perspective overview. The Encoder and Decoder layers have
    been grouped into single blocks for simplicity, and the outputs of each block
    have been annotated. Image by the author.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 的高层视角概述。编码器和解码器层已被简化为单个块，并且每个块的输出已被标注。图片由作者提供。
- en: '**Note 3.** For clarity, I will use the notation source input to refer to the
    Encoder’s input (sentence in English) and target input to the expected output
    introduced in the Decoder (sentence in Spanish). This notation will remain consistent
    for the rest of the article.'
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意 3.** 为了清晰起见，我将使用源输入来指代编码器的输入（英语句子），并使用目标输入来指代解码器中的期望输出（西班牙语句子）。这种标记将在文章的其余部分保持一致。'
- en: 'Now let’s have a closer look at the Inputs (source and target) and the Output
    of the Transformer:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们更详细地查看 Transformer 的输入（源输入和目标输入）以及输出：
- en: As we’ve seen, Transformer input text is embedded into a high dimensional vector
    space, so instead of a sentence, a sequence of vectors is entered. However, there
    exists a better mathematical structure to represent sequences of vectors, the
    matrices! And even further, when training a neural network, we don’t train it
    sample by sample, we rather use batches in which several samples are packed. **The
    resulting input is a tensor of shape *[N, L, E]* where *N* is the batch size,
    *L* the sequence length, and *E* the embedding dimension**.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，Transformer 的输入文本被嵌入到一个高维向量空间中，因此输入的是一系列向量而不是一个句子。然而，还有一种更好的数学结构来表示向量序列，那就是矩阵！更进一步地，在训练神经网络时，我们不是逐样本训练，而是使用多个样本打包成的批次进行训练。**最终的输入是形状为
    *[N, L, E]* 的张量，其中 *N* 是批次大小，*L* 是序列长度，*E* 是嵌入维度**。
- en: '![](../Images/843e1b3194c7413d1bdf9442785e84b0.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/843e1b3194c7413d1bdf9442785e84b0.png)'
- en: Illustration of the **Source input tensor**. On the left, the 2D tensor represents
    the first **embedded sequence** of the batch. On the right, the full batch tensor
    containing all the sequences of a single batch that is fed into the Transformer
    Encoder. Similarly, the Target tensor has the same shape but contains the true
    output. Image by the author.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**源输入张量**的示意图。在左侧，二维张量表示批次的第一个 **嵌入序列**。在右侧，是包含单个批次所有序列的完整批次张量，输入到 Transformer
    编码器中。同样，目标张量具有相同的形状，但包含真实的输出。图片由作者提供。'
- en: As for the output of the Transformer, a Linear + Softmax layer is applied which
    produces some Output Probabilities (recall that the Softmax layer outputs a probability
    distribution over the defined classes). The output of the Transformer isn’t the
    translated sentence but a probability distribution over the target vocabulary
    that determines the words with the highest probability. Note that for each position
    in the sequence length, a probability distribution is generated to select the
    next token with a higher probability. **Since during training the Transformer
    processes all the sentences at once, we get as output a 3D tensor that represents
    the probability distributions over the vocabulary tokens with shape *[N, L, V]*
    where N is the batch size, L the sequence length, and V the vocabulary length**.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Transformer的输出，应用了一个线性 + Softmax 层，产生一些输出概率（回忆一下，Softmax 层输出的是定义类别的概率分布）。Transformer的输出不是翻译后的句子，而是一个目标词汇的概率分布，决定了具有最高概率的单词。注意，对于序列长度中的每个位置，生成一个概率分布来选择下一个具有更高概率的令牌。**由于在训练过程中Transformer一次处理所有句子，我们得到一个3D张量，表示词汇令牌的概率分布，其形状为*[N,
    L, V]*，其中N是批量大小，L是序列长度，V是词汇长度**。
- en: '![](../Images/ed71c7ef0668468c4c66409316b6f6b4.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed71c7ef0668468c4c66409316b6f6b4.png)'
- en: Illustration of the **Output Probabilities tensor**. On the left, the probability
    distribution prediction over the vocabulary for a single sequence. Each column
    represents a word in the target vocabulary space, and the rows correspond to the
    sequence tokens. On the right, the full predicted tensor for a whole batch. Image
    by the author.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出概率张量**的示意图。左侧是单个序列的词汇预测概率分布。每列表示目标词汇空间中的一个单词，每行对应序列中的令牌。右侧是整个批次的完整预测张量。图片由作者提供。'
- en: Finally, **the predicted tokens are the ones with the highest probability**.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，**预测的令牌是概率最高的**。
- en: '**Note 4.** As explained in the Introduction to NLP section, all the sequences
    after the embedding have the same length, which corresponds to the longest possible
    sequence that can be introduced/produced in/by the Transformer.'
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意 4.** 如在NLP引言部分所述，嵌入后的所有序列都具有相同的长度，这对应于Transformer可以引入/生成的最长序列。'
- en: Training vs Predicting
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练与预测
- en: For the final section of the article’s Part 1, I’d like to make a point about
    the Training vs Predicting phases of Transformers.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在文章第一部分的最后一节中，我想谈谈Transformer的训练阶段与预测阶段的区别。
- en: As explained in the previous section, Transformers take two inputs (source and
    target). During Training, the Transformer is able to process all the inputs at
    once, meaning the input tensors are only passed one time through the model. The
    output is effectively the 3-dimensional probability tensor presented in the previous
    figure.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如上一节所述，Transformer接受两个输入（源和目标）。在训练过程中，Transformer能够一次处理所有输入，这意味着输入张量仅通过模型传递一次。输出实际上是前面图中呈现的三维概率张量。
- en: '![](../Images/43e0b7ffc23fc3941cfb4bd39846e0cf.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/43e0b7ffc23fc3941cfb4bd39846e0cf.png)'
- en: Transformer training phase. First, the source input (English sentence) is embedded
    and attention is applied in the Encoder. Then, the target input (Spanish) is entered
    into the Decoder, where source attention and target attention are combined in
    the Decider’s 2nd layer. Finally, the probability distribution over the vocabulary
    tokens is generated and the highest tokens are selected to form the translated
    output sentence. **Note that dimensions (L,E) do not change between layers**.
    Image by the author.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer训练阶段。首先，源输入（英文句子）被嵌入并在编码器中应用注意力。然后，目标输入（西班牙语）被输入到解码器中，在解码器的第二层中，源注意力和目标注意力被结合。最后，生成词汇令牌的概率分布，并选择概率最高的令牌以形成翻译后的输出句子。**注意，在层之间（L,E）的维度不变**。图片由作者提供。
- en: On the contrary, in the Prediction phase, there is no target input sequence
    to feed the Transformer (we wouldn’t need a Deep Learning model for text translation
    if we already know the translated sentence). So, what do we enter as target input?
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，在预测阶段，没有目标输入序列供 Transformer 输入（如果我们已经知道翻译后的句子，就不需要深度学习模型进行文本翻译了）。那么，我们输入什么作为目标输入呢？
- en: It is at this point that the **auto-regressive behavior** of Transformers comes
    to light. The Transformer can process the source input sequence at once in the
    Encoder, but for the Decoder’s module, it enters a loop where at each iteration
    it just produces the next token in the sequence (a row probability vector over
    the vocabulary tokens). The selected tokens with higher probability are then entered
    as the target input again so the Transformer always predicts the next token based
    on its previous predictions (hence the auto-regressive meaning). But what should
    be the first token entered at the very first iteration?
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 就在这一点上，**Transformer的自回归行为**显现出来。Transformer可以在编码器中一次处理源输入序列，但对于解码器模块，它进入一个循环，每次迭代只生成序列中的下一个标记（一个关于词汇标记的行概率向量）。具有较高概率的选择标记随后作为目标输入再次输入，因此Transformer总是根据其之前的预测来预测下一个标记（因此具有自回归含义）。但是，在第一次迭代时应该输入哪个标记呢？
- en: Remember the special tokens from the Introduction to NLP section? The first
    element introduced as target input is the beginning token <START> that marks the
    opening of the sentence.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 记得在NLP介绍部分提到的特殊标记吗？作为目标输入引入的第一个元素是起始标记<START>，它标志着句子的开始。
- en: '![](../Images/45ed7f6e7fa569b5cce698efbf7b81a7.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/45ed7f6e7fa569b5cce698efbf7b81a7.png)'
- en: Transformer predicting phase. The Encoder part remains unchanged, whereas Decoder’s
    tensors are row vectors. The input text for the first (0) iteration is empty since
    there is only the <START> token. **Note the arrow connecting the predicted token
    back to the target input representing the auto-regressive behavior**. In the next
    iterations, the target input will grow as the transformer predicts new tokens
    from the sequence. However, only the last row in the output probability tensor
    is taken into account, which means that the past predicted tokens cannot change.
    Image by the author.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer预测阶段。编码器部分保持不变，而解码器的张量是行向量。第一次（0次）迭代的输入文本为空，因为只有<START>标记。**注意连接预测标记回到目标输入的箭头，表示自回归行为**。在接下来的迭代中，目标输入会随着Transformer从序列中预测新的标记而增加。然而，只考虑输出概率张量中的最后一行，这意味着过去预测的标记不能改变。图片由作者提供。
- en: '![](../Images/06790501544f720c7279d40653f906fd.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06790501544f720c7279d40653f906fd.png)'
- en: Table comparing Decoder dimensions during predicting phase (iteration 0 is depicted
    in the above figure). Note the increase in dimensions from 1 to L-1\. Once the
    Transformer has predicted the second last token in the sequence, it adds the <END>
    special token finishing the prediction. Image by the author.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 比较解码器在预测阶段的维度的表（上图展示了迭代0）。注意维度从1增加到L-1。一旦Transformer预测了序列中的倒数第二个标记，它会添加<END>特殊标记来完成预测。图片由作者提供。
- en: Summary
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This Part has been an Introduction to the first notions and concepts necessary
    to get a better understanding of Transformers models. In the next Part, I will
    delve into each of the modules of the Transformers architecture where most of
    the mathematics resides.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分介绍了了解Transformer模型所需的初步概念和思想。在下一部分，我将深入探讨Transformer架构的每个模块，其中大部分数学内容都在其中。
- en: 'The main ideas and concepts behind this article are:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章的主要思想和概念是：
- en: Transformers work in a **normalized vector space** defined by the embedding
    system and where each dimension represents a characteristic between tokens.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformers在**规范化向量空间**中工作，该空间由嵌入系统定义，每个维度代表标记之间的一个特征。
- en: Transformers **inputs are tensors of shape [N, L, E]** where N denotes the batch
    size, L is the sequence length (constant for every sequence thanks to the padding)
    and E represents the embedding dimension.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformers **输入是形状为[N, L, E]的张量**，其中N表示批量大小，L是序列长度（由于填充对每个序列都是恒定的），E代表嵌入维度。
- en: While the **Encoder** finds relationships between tokens in the source embedding
    space, the **Decoder’s** task is to learn the projection from the source space
    into the target space.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当**编码器**在源嵌入空间中发现标记之间的关系时，**解码器**的任务是学习从源空间到目标空间的投影。
- en: Transformer’s output is a line vector whose length is equal to the vocabulary’s
    size and where **each coefficient represents the probability** of the corresponding
    indexed token being placed next in the sequence.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer的输出是一个行向量，其长度等于词汇表的大小，**每个系数代表相应索引标记在序列中下一个位置的概率**。
- en: During training, Transformer processes all its inputs at once, outputting a
    [N, L, V]tensor (V is the vocabulary length). But during predicting, **Transformers
    are auto-regressive**, predicting token by token always based on their previous
    predictions.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练期间，Transformer会一次处理所有输入，输出一个[N, L, V]张量（V为词汇表长度）。但在预测过程中，**Transformers是自回归的**，总是基于之前的预测逐个预测标记。
- en: Soon the next article’s part will be available [**here**](https://medium.com/@gabrielfurnieles)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 下一篇文章的部分内容将很快在[**这里**](https://medium.com/@gabrielfurnieles)发布。
- en: Thanks for reading! If you’ve found this article helpful or inspiring, please
    consider following me for more content on AI and Deep Learning.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！如果你发现这篇文章有帮助或启发，请考虑关注我，以获取更多关于AI和深度学习的内容。
- en: '[](https://medium.com/@gabrielfurnieles?source=post_page-----ad25da6d3cca--------------------------------)
    [## Gabriel Furnieles - Medium'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[加布里埃尔·弗尼耶莱斯 - Medium](https://medium.com/@gabrielfurnieles?source=post_page-----ad25da6d3cca--------------------------------)'
- en: Read writing from Gabriel Furnieles on Medium. Mathematical engineering student
    specializing in AI and ML. I write…
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Medium上阅读加布里埃尔·弗尼耶莱斯的文章。他是数学工程专业的学生，专注于AI和ML。我写…
- en: medium.com](https://medium.com/@gabrielfurnieles?source=post_page-----ad25da6d3cca--------------------------------)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[加布里埃尔·弗尼耶莱斯 - Medium](https://medium.com/@gabrielfurnieles?source=post_page-----ad25da6d3cca--------------------------------)'
- en: 'You might also be interested in:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还会对以下内容感兴趣：
- en: '[](/the-5-most-promising-ai-models-for-image-translation-de2677e526e6?source=post_page-----ad25da6d3cca--------------------------------)
    [## The 5 most promising AI models for Image translation'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[5种最有前途的图像翻译AI模型](https://towardsdatascience.com/the-5-most-promising-ai-models-for-image-translation-de2677e526e6?source=post_page-----ad25da6d3cca--------------------------------)'
- en: State of the Art of the latest models for generating images from other images
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最新图像生成模型的最前沿技术
- en: towardsdatascience.com](/the-5-most-promising-ai-models-for-image-translation-de2677e526e6?source=post_page-----ad25da6d3cca--------------------------------)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[链接](https://towardsdatascience.com/the-5-most-promising-ai-models-for-image-translation-de2677e526e6?source=post_page-----ad25da6d3cca--------------------------------)'
