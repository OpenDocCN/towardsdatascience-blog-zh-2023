- en: Natural Language Processing For Absolute Beginners
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理初学者指南
- en: 原文：[https://towardsdatascience.com/natural-language-processing-for-absolute-beginners-a195549a3164](https://towardsdatascience.com/natural-language-processing-for-absolute-beginners-a195549a3164)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/natural-language-processing-for-absolute-beginners-a195549a3164](https://towardsdatascience.com/natural-language-processing-for-absolute-beginners-a195549a3164)
- en: Solving complex NLP tasks in 10 lines of Python code
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用10行Python代码解决复杂的NLP任务
- en: '[](https://dmitryelj.medium.com/?source=post_page-----a195549a3164--------------------------------)[![Dmitrii
    Eliuseev](../Images/7c48f0c016930ead59ddb785eaf3e0e6.png)](https://dmitryelj.medium.com/?source=post_page-----a195549a3164--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a195549a3164--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a195549a3164--------------------------------)
    [Dmitrii Eliuseev](https://dmitryelj.medium.com/?source=post_page-----a195549a3164--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://dmitryelj.medium.com/?source=post_page-----a195549a3164--------------------------------)[![Dmitrii
    Eliuseev](../Images/7c48f0c016930ead59ddb785eaf3e0e6.png)](https://dmitryelj.medium.com/?source=post_page-----a195549a3164--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a195549a3164--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a195549a3164--------------------------------)
    [Dmitrii Eliuseev](https://dmitryelj.medium.com/?source=post_page-----a195549a3164--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a195549a3164--------------------------------)
    ·9 min read·Sep 2, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a195549a3164--------------------------------)
    ·阅读时间9分钟·2023年9月2日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/bc90b1586db7d9db3689cfb34615b58a.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc90b1586db7d9db3689cfb34615b58a.png)'
- en: Image by author (Generated using Craiyon)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片（使用Craiyon生成）
- en: It is mostly true that NLP (Natural Language Processing) is a complex area of
    computer science. Frameworks like SpaCy or NLTK are large and often require some
    learning. But with the help of open-source large language models (LLMs) and modern
    Python libraries, many tasks can be solved much more easily. And even more, results,
    which only several years ago were available only in science papers, can now be
    achieved with only 10 lines of Python code.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: NLP（自然语言处理）通常被认为是计算机科学中的复杂领域。像SpaCy或NLTK这样的框架体积庞大，通常需要一些学习。但借助开源的大型语言模型（LLMs）和现代Python库，许多任务可以更容易地解决。而且，甚至在几年前只有科学论文中才能找到的结果，现在也可以通过仅仅10行Python代码实现。
- en: Without further ado, let’s get into it.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 话不多说，我们开始吧。
- en: 1\. Language Translation
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 语言翻译
- en: 'Have you ever wondered how Google Translate works? Google [is using](https://blog.research.google/2020/06/recent-advances-in-google-translate.html)
    a deep learning model trained on a vast amount of text. Now, with the help of
    the [Transformers library](https://huggingface.co/docs/transformers/index), it
    can be done not only in Google Labs but on an ordinary PC. In this example, I
    will be using a pre-trained [T5-base](https://huggingface.co/t5-base) (Text-to-Text
    Transfer Transformer) model. This model was first trained on raw text data, then
    fine-tuned on source-target pairs like (“translate English to German: the house
    is wonderful”, “Das Haus ist Wunderbar”). Here “translate English to German” is
    a prefix that “tells” the model what to do, and the phrases are the actual context
    that the model should learn.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否曾想过Google翻译是如何工作的？Google [使用](https://blog.research.google/2020/06/recent-advances-in-google-translate.html)一个在大量文本上训练的深度学习模型。现在，借助
    [Transformers库](https://huggingface.co/docs/transformers/index)，不仅在Google实验室，还能在普通PC上完成这项工作。在此示例中，我将使用一个预训练的
    [T5-base](https://huggingface.co/t5-base)（文本到文本转换器）模型。该模型最初在原始文本数据上训练，然后在诸如（“将英语翻译成德语：这所房子很棒”，“Das
    Haus ist Wunderbar”）的源-目标对上微调。在这里，“将英语翻译成德语”是一个前缀，告诉模型该做什么，而短语是模型应该学习的实际上下文。
- en: I**mportant warning**. Large language models are literally pretty large. The
    *T5ForConditionalGeneration* class, used in this example, will automatically download
    the “t5-base” model, which is about 900 MB in size. Before running the code, be
    sure that there is enough disk space and that your traffic is not limited.
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**重要警告**。大型语言模型的体积确实很大。此示例中使用的 *T5ForConditionalGeneration* 类将自动下载约900 MB大小的“t5-base”模型。在运行代码之前，请确保有足够的磁盘空间，并且流量没有限制。'
- en: 'A pre-trained T5 model can be used in Python:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在Python中使用预训练的T5模型：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here, a *T5Tokenizer* class converts a source string to a digital form; this
    process is called *tokenization*. In our example, a “*translate English to German:
    the weather is good*” text will be converted to the *[13959, 1566, 12, 2968, 10,
    8, 1969, 19, 207, 1]* array. A “*generate”* method is doing the actual job, and
    finally, the tokenizer is making a backward conversion. As an output, we will
    get the result “Das Wetter ist gut”.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，一个 *T5Tokenizer* 类将源字符串转换为数字形式；这个过程叫做 *分词*。在我们的例子中，一个“*将英语翻译成德语：天气很好*”的文本将被转换为
    *[13959, 1566, 12, 2968, 10, 8, 1969, 19, 207, 1]* 数组。“*生成*”方法实际执行任务，最后，分词器进行反向转换。作为输出，我们将得到结果“Das
    Wetter ist gut”。
- en: 'Can we make this code even shorter? Actually, we can. With the help of the
    [Transformer’s *pipeline*](https://huggingface.co/docs/transformers/main_classes/pipelines)
    class, we can create an abstract pipeline that allows us to do this task in only
    2 lines of Python code:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否使这段代码更简洁？实际上，我们可以。借助 [Transformer 的 *pipeline*](https://huggingface.co/docs/transformers/main_classes/pipelines)
    类，我们可以创建一个抽象管道，只需 2 行 Python 代码即可完成这个任务：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For self-education purposes, I generally prefer the first approach because it
    is easier to understand what is going on “under the hood”. But for “production”
    purposes, the second way is much more flexible, and it also allows the use of
    different models without changing the code.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 出于自学的目的，我通常更喜欢第一种方法，因为它更容易理解“幕后”的情况。但对于“生产”目的，第二种方法更具灵活性，它还允许在不更改代码的情况下使用不同的模型。
- en: 2\. Summarization
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 摘要生成
- en: 'The goal of text summarization is to transform the document into a shortened
    version, which obviously will take time if done manually. And surprisingly, a
    T5 model can do it as well; the only change we need is to change the prefix:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 文本摘要的目标是将文档转换为简短版本，这显然如果手动完成会花费时间。令人惊讶的是，T5 模型也可以做到这一点；我们唯一需要更改的是前缀：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As we can see, the result is pretty accurate.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，结果非常准确。
- en: 'In the same way, as with the first example, using a pipeline can produce shorter
    code for the same task:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 与第一个例子一样，使用管道可以为相同的任务生成更简短的代码：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Readers can be curious about other tasks that are possible with a “t5-base”
    model. We can easily print them all:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 读者可能对使用 “t5-base” 模型可以完成的其他任务感兴趣。我们可以轻松地将它们全部打印出来：
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 3\. Question Answering
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 问答
- en: 'Another interesting functionality that large language models can provide is
    answering questions in a given context. I will be using the same piece of text
    as in the previous example:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型还可以提供另一种有趣的功能，即在给定的上下文中回答问题。我将使用与之前示例相同的文本：
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this case, I was using a [distilbert-base-cased-distilled-squad](https://huggingface.co/distilbert-base-cased-distilled-squad)
    model, which is 261 MB in size. As we can see, the model is not only providing
    an answer but also retrieving the position from the original text, which is good
    for verification of the results.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我使用了一个 [distilbert-base-cased-distilled-squad](https://huggingface.co/distilbert-base-cased-distilled-squad)
    模型，模型大小为 261 MB。正如我们所见，该模型不仅提供了答案，还从原始文本中检索了位置，这对验证结果很有帮助。
- en: 4\. Language Generation
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 语言生成
- en: Another fun process is language generation. For this example, I will be using
    a [GPT-2 model](https://huggingface.co/gpt2). This is obviously not the latest
    GPT model we have today, but GPT-2 is freely available, and it is small enough
    (the file size is 548 MB) to run on an ordinary PC.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的过程是语言生成。对于这个例子，我将使用一个 [GPT-2 模型](https://huggingface.co/gpt2)。这显然不是我们今天拥有的最新
    GPT 模型，但 GPT-2 是免费提供的，并且其文件大小为 548 MB，足够在普通 PC 上运行。
- en: 'Let’s see how it works:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它是如何工作的：
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the same way, a much shorter code can be used with a pipeline:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，可以使用更简短的代码通过管道完成：
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Practically speaking, I cannot see a lot of sense in these texts, but from
    a grammar perspective, they are good enough, and for some sort of automation or
    unit testing, they can be useful. Interestingly, the GPT-2 model was released
    in 2019\. Just for fun, I asked the same question, “Please continue the phrase
    I am going to say,” to GPT-3.5 (released in 2022), and got this answer:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我在这些文本中看不到太多意义，但从语法角度来看，它们足够好，对于某种自动化或单元测试，它们可能是有用的。有趣的是，GPT-2 模型于 2019
    年发布。为了好玩，我向 GPT-3.5（发布于 2022 年）提出了相同的问题，“请继续我将要说的短语”，并得到了以下答案：
- en: '[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This result is much better; great progress was achieved in these three years.
    But obviously, it would not be possible to run a GPT-3.5 model on a regular PC
    today, even if it were released in the public domain.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果要好得多；这三年取得了巨大的进步。但显然，即使GPT-3.5模型在今天发布于公共领域，也不可能在普通电脑上运行。
- en: 5\. Sentiment Analysis
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 情感分析
- en: A previous example was mostly made for fun, but sentiment analysis is much more
    important for business. Sentiment analysis is the process of analyzing the sentiment
    in a given text and finding a subjective opinion. It may be particularly important
    for web shops, streaming platforms, and any other services where many users can
    publish reviews.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的示例主要是为了娱乐，但情感分析对于商业来说更为重要。情感分析是分析给定文本中的情感并找出主观意见的过程。对于网络商店、流媒体平台和其他许多用户可以发布评论的服务，这可能特别重要。
- en: 'For this test, I will be using a [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)
    model:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个测试，我将使用一个[distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)模型：
- en: '[PRE9]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: I deliberately tried to use phrases that were not so easy to analyze, but the
    model gave correct answers. Obviously, natural language is very flexible, and
    it is still possible to make a text that will give a false result. For example,
    this model gave the incorrect answer to the phrase “I expected it to be terrible,
    but I was mistaken”. At the same time, the (many times larger) GPT-3.5 model was
    able to parse it correctly. On the other side, considering that the DistilBERT
    model size is only 268 MB and it can be used for free (a model has an Apache 2.0
    license), the result is pretty good. Readers can also try [other open-source models](https://huggingface.co/models?pipeline_tag=text-classification)
    and choose the best one for their needs.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我故意尝试使用不易分析的短语，但模型给出了正确的答案。显然，自然语言是非常灵活的，仍然可能出现导致错误结果的文本。例如，这个模型对短语“我期待它很糟糕，但我错了”给出了错误的回答。同时，（体积大得多的）GPT-3.5模型能够正确解析。另一方面，考虑到DistilBERT模型的大小仅为268
    MB且可以免费使用（模型拥有Apache 2.0许可证），结果还是相当不错的。读者也可以尝试[其他开源模型](https://huggingface.co/models?pipeline_tag=text-classification)，并选择最适合他们需求的模型。
- en: 6\. Named Entity Recognition (NER)
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 命名实体识别（NER）
- en: Another interesting part of natural language processing is “named entity recognition”.
    It is the process of extracting the entities, like names, locations, dates, etc.,
    from the unstructured text. For this test, I will be using a [bert-base-NER model](https://huggingface.co/dslim/bert-base-NER)
    (file size is 433 MB).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理的另一个有趣部分是“命名实体识别”。这是从非结构化文本中提取实体，如名称、地点、日期等的过程。对于这个测试，我将使用一个[bert-base-NER模型](https://huggingface.co/dslim/bert-base-NER)（文件大小为433
    MB）。
- en: 'Let’s consider an example:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个例子：
- en: '[PRE10]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As we can see, the model was able to correctly determine the main entities mentioned
    in the text, such as name, location, and company.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，该模型能够正确确定文本中提到的主要实体，如名称、地点和公司。
- en: 7\. Keyword Extraction
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 关键词提取
- en: 'In the last example, we tested NER, but not all parameters were extracted from
    a text. A separate keyword extraction algorithm can also be useful for the same
    task. For this example, I will be using [KeyBERT](https://maartengr.github.io/KeyBERT/index.html):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一个例子中，我们测试了NER，但不是所有的参数都从文本中提取出来了。一个单独的关键词提取算法对于同一任务也可能很有用。对于这个例子，我将使用[KeyBERT](https://maartengr.github.io/KeyBERT/index.html)：
- en: '[PRE11]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As we can see, keyword extraction can also be useful as an addition to NER,
    and it can find some extra data from the same phrase.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，关键词提取作为NER的补充也很有用，它可以从相同的短语中提取一些额外的数据。
- en: Conclusion
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, I tested different algorithms for Natural Language Processing
    (NLP). As I promised at the beginning of the article, with the help of modern
    libraries, pretty complex tasks can be solved in 10 lines of Python code. It is
    also important to mention that all this code can run locally without any API subscriptions
    or keys. And last but not least, I hope readers can see that NLP can also be fun.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我测试了不同的自然语言处理（NLP）算法。正如我在文章开头承诺的那样，通过现代库的帮助，复杂的任务可以在10行Python代码中解决。还要提到的是，这些代码都可以在本地运行，无需任何API订阅或密钥。最后但同样重要的是，我希望读者能看到NLP也可以很有趣。
- en: Thanks for reading. If you enjoyed this story, feel free [to subscribe](https://medium.com/@dmitryelj/membership)
    to Medium, and you will get notifications when my new articles will be published,
    as well as full access to thousands of stories from other authors.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读。如果你喜欢这个故事，可以随时[订阅](https://medium.com/@dmitryelj/membership)Medium，你将会在我的新文章发布时收到通知，并且可以全面访问其他作者的成千上万篇故事。
