- en: 'Decision Trees: Introduction & Intuition'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å†³ç­–æ ‘ï¼šä»‹ç»ä¸ç›´è§‚ç†è§£
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/decision-trees-introduction-intuition-dac9592f4b7f](https://towardsdatascience.com/decision-trees-introduction-intuition-dac9592f4b7f)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/decision-trees-introduction-intuition-dac9592f4b7f](https://towardsdatascience.com/decision-trees-introduction-intuition-dac9592f4b7f)
- en: Making data-informed decisions with Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Python åšæ•°æ®é©±åŠ¨å†³ç­–
- en: '[](https://shawhin.medium.com/?source=post_page-----dac9592f4b7f--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page-----dac9592f4b7f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dac9592f4b7f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dac9592f4b7f--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page-----dac9592f4b7f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://shawhin.medium.com/?source=post_page-----dac9592f4b7f--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page-----dac9592f4b7f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dac9592f4b7f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dac9592f4b7f--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page-----dac9592f4b7f--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dac9592f4b7f--------------------------------)
    Â·10 min readÂ·Feb 10, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dac9592f4b7f--------------------------------)
    Â·é˜…è¯»æ—¶é—´ 10 åˆ†é’ŸÂ·2023å¹´2æœˆ10æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/13f81f75cc709437ff2192216ceadf8c.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13f81f75cc709437ff2192216ceadf8c.png)'
- en: Photo by [niko photos](https://unsplash.com/@niko_photos?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) peppered
    with thinking emojis.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± [niko photos](https://unsplash.com/@niko_photos?utm_source=medium&utm_medium=referral)
    æä¾›ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)ï¼Œå¹¶é…æœ‰æ€è€ƒçš„è¡¨æƒ…ç¬¦å·ã€‚
- en: This is the first article in a series on Decision Trees. In this post, I introduce
    decision trees and describe how to *grow* them using data. The post concludes
    with example Python code showing how to create and use a decision tree to help
    make medical prognoses.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å…³äºå†³ç­–æ ‘ç³»åˆ—æ–‡ç« ä¸­çš„ç¬¬ä¸€ç¯‡ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»‹ç»äº†å†³ç­–æ ‘ï¼Œå¹¶æè¿°äº†å¦‚ä½•ä½¿ç”¨æ•°æ®æ¥*ç”Ÿæˆ*å®ƒä»¬ã€‚æ–‡ç« æœ€ååŒ…å«äº†ç¤ºä¾‹ Python ä»£ç ï¼Œå±•ç¤ºäº†å¦‚ä½•åˆ›å»ºå’Œä½¿ç”¨å†³ç­–æ ‘æ¥å¸®åŠ©è¿›è¡ŒåŒ»å­¦é¢„æµ‹ã€‚
- en: 'Key Points:'
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é‡ç‚¹ï¼š
- en: Decision Trees are a widely-used and intuitive machine learning technique used
    to solve prediction problems.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å†³ç­–æ ‘æ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨ä¸”ç›´è§‚çš„æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œç”¨äºè§£å†³é¢„æµ‹é—®é¢˜ã€‚
- en: We can *grow* decision trees from data.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä»æ•°æ®ä¸­*ç”Ÿæˆ*å†³ç­–æ ‘ã€‚
- en: Hyperparameter tuning can be used to help avoid the *overfitting* problem.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¶…å‚æ•°è°ƒæ•´å¯ä»¥ç”¨æ¥å¸®åŠ©é¿å…*è¿‡æ‹Ÿåˆ*é—®é¢˜ã€‚
- en: '**What are Decision Trees?**'
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ä»€ä¹ˆæ˜¯å†³ç­–æ ‘ï¼Ÿ**'
- en: '**Decision trees** are a **widely-used and intuitive machine learning technique**.
    Typically, they are **used to solve prediction problems**. For example, predicting
    tomorrowâ€™s weather forecast or estimating an individual''s probability of developing
    heart disease.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**å†³ç­–æ ‘**æ˜¯ä¸€ç§**å¹¿æ³›ä½¿ç”¨ä¸”ç›´è§‚çš„æœºå™¨å­¦ä¹ æŠ€æœ¯**ã€‚é€šå¸¸ï¼Œå®ƒä»¬**ç”¨äºè§£å†³é¢„æµ‹é—®é¢˜**ã€‚ä¾‹å¦‚ï¼Œé¢„æµ‹æ˜å¤©çš„å¤©æ°”é¢„æŠ¥æˆ–ä¼°ç®—ä¸€ä¸ªäººæ‚£å¿ƒè„ç—…çš„æ¦‚ç‡ã€‚'
- en: They work through a series of yes-no questions, which are used to narrow down
    possible choices and arrive at an outcome. A simple example of a decision tree
    is shown below.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å†³ç­–æ ‘é€šè¿‡ä¸€ç³»åˆ—æ˜¯éé—®é¢˜è¿›è¡Œå·¥ä½œï¼Œè¿™äº›é—®é¢˜ç”¨äºç¼©å°å¯èƒ½çš„é€‰æ‹©èŒƒå›´å¹¶å¾—å‡ºç»“æœã€‚ä¸‹é¢å±•ç¤ºäº†ä¸€ä¸ªç®€å•çš„å†³ç­–æ ‘ç¤ºä¾‹ã€‚
- en: '![](../Images/19d95fea62f210ec0bc0b398fdea68a8.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/19d95fea62f210ec0bc0b398fdea68a8.png)'
- en: Example decision tree to predict whether I will drink tea or coffee. Image by
    author.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹å†³ç­–æ ‘é¢„æµ‹æˆ‘æ˜¯å¦ä¼šå–èŒ¶æˆ–å’–å•¡ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: As shown in the above figure, a decision tree consists of nodes connected by
    directed edges. Each node in a decision tree corresponds to a conditional statement
    based on a predictor variable.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œå†³ç­–æ ‘ç”±é€šè¿‡æœ‰å‘è¾¹è¿æ¥çš„èŠ‚ç‚¹ç»„æˆã€‚å†³ç­–æ ‘ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹å¯¹åº”äºä¸€ä¸ªåŸºäºé¢„æµ‹å˜é‡çš„æ¡ä»¶è¯­å¥ã€‚
- en: At the top of the decision tree shown above is the **root node**, which **sets
    the initial splitting of data** records. Here we evaluate whether it is after
    4 PM or not. Each possible response (yes or no) follows a different path in our
    tree.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šé¢æ˜¾ç¤ºçš„å†³ç­–æ ‘çš„é¡¶éƒ¨æ˜¯**æ ¹èŠ‚ç‚¹**ï¼Œå®ƒ**è®¾ç½®äº†æ•°æ®è®°å½•çš„åˆå§‹åˆ†è£‚**ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è¯„ä¼°æ—¶é—´æ˜¯å¦åœ¨ä¸‹åˆ 4 ç‚¹ä¹‹åã€‚æ¯ä¸ªå¯èƒ½çš„å“åº”ï¼ˆæ˜¯æˆ–å¦ï¼‰åœ¨æ ‘ä¸­éµå¾ªä¸åŒçš„è·¯å¾„ã€‚
- en: If yes, we follow the left branch and end up at a **leaf node** (also called
    the terminal node). **No further splits are required to determine the outcome
    at this type of node**. In this case, we go with tea over coffee so we can get
    to bed at a reasonable hour.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ˜¯ï¼Œæˆ‘ä»¬æ²¿ç€å·¦ä¾§åˆ†æ”¯å‰è¿›ï¼Œæœ€ç»ˆåˆ°è¾¾ä¸€ä¸ª**å¶èŠ‚ç‚¹**ï¼ˆä¹Ÿç§°ä¸ºç»ˆç«¯èŠ‚ç‚¹ï¼‰ã€‚**åœ¨è¿™ç§ç±»å‹çš„èŠ‚ç‚¹ä¸Šä¸éœ€è¦è¿›ä¸€æ­¥åˆ†è£‚æ¥ç¡®å®šç»“æœ**ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬é€‰æ‹©èŒ¶è€Œä¸æ˜¯å’–å•¡ï¼Œä»¥ä¾¿èƒ½åœ¨åˆç†çš„æ—¶é—´ä¸ŠåºŠç¡è§‰ã€‚
- en: Conversely, if it is 4 PM or earlier, we follow the right branch and end up
    at a so-called **splitting node**. These nodes **further split data records**
    based on conditional statements. From here, we evaluate whether the hours of sleep
    from last night were more than 6 hours. If yes, we go with tea again, but if no,
    we go with coffee â˜•ï¸.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸åï¼Œå¦‚æœæ—¶é—´æ˜¯ä¸‹åˆ4ç‚¹æˆ–æ›´æ—©ï¼Œæˆ‘ä»¬ä¼šæ²¿ç€å³ä¾§åˆ†æ”¯å‰è¿›ï¼Œæœ€ç»ˆåˆ°è¾¾ä¸€ä¸ªæ‰€è°“çš„**åˆ†è£‚èŠ‚ç‚¹**ã€‚è¿™äº›èŠ‚ç‚¹**è¿›ä¸€æ­¥åˆ†å‰²æ•°æ®è®°å½•**ï¼ŒåŸºäºæ¡ä»¶è¯­å¥è¿›è¡Œåˆ’åˆ†ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è¯„ä¼°æ˜¨æ™šçš„ç¡çœ æ—¶é—´æ˜¯å¦è¶…è¿‡6å°æ—¶ã€‚å¦‚æœæ˜¯ï¼Œæˆ‘ä»¬ç»§ç»­é€‰æ‹©èŒ¶ï¼Œä½†å¦‚æœä¸æ˜¯ï¼Œæˆ‘ä»¬åˆ™é€‰æ‹©å’–å•¡â˜•ï¸ã€‚
- en: Using Decision Trees
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å†³ç­–æ ‘
- en: In practice, we often donâ€™t use decision trees like we did just now *(*i.e.
    looking at a decision tree and following along for a particular data record).
    Rather, have a computer evaluate data for us. All we have to do is give the computer
    the data it needs in the form of a table.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®é™…æ“ä½œä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä¸ä¼šåƒåˆšæ‰é‚£æ ·ä½¿ç”¨å†³ç­–æ ‘ï¼ˆå³æŸ¥çœ‹å†³ç­–æ ‘å¹¶è·Ÿéšç‰¹å®šæ•°æ®è®°å½•ï¼‰ã€‚ç›¸åï¼Œæˆ‘ä»¬è®©è®¡ç®—æœºä¸ºæˆ‘ä»¬è¯„ä¼°æ•°æ®ã€‚æˆ‘ä»¬åªéœ€å°†æ‰€éœ€æ•°æ®ä»¥è¡¨æ ¼å½¢å¼æä¾›ç»™è®¡ç®—æœºå³å¯ã€‚
- en: 'An example of this is shown below. Here we have tabular data with two variables:
    time of day and hours of sleep from the previous night (blue columns). Then using
    the decision tree above, we can assign an appropriate caffeinated beverage to
    each record (green column).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æ˜¯ä¸€ä¸ªç¤ºä¾‹ã€‚è¿™é‡Œæˆ‘ä»¬æœ‰ä¸¤ä¸ªå˜é‡çš„è¡¨æ ¼æ•°æ®ï¼šæ—¶é—´å’Œå‰ä¸€æ™šçš„ç¡çœ å°æ—¶æ•°ï¼ˆè“è‰²åˆ—ï¼‰ã€‚ç„¶åï¼Œä½¿ç”¨ä¸Šè¿°å†³ç­–æ ‘ï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºæ¯æ¡è®°å½•åˆ†é…é€‚å½“çš„å«å’–å•¡å› é¥®æ–™ï¼ˆç»¿è‰²åˆ—ï¼‰ã€‚
- en: '![](../Images/5fcefe3250fcb94b3929efba85702486.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5fcefe3250fcb94b3929efba85702486.png)'
- en: Example table of input data and the resulting decision tree prediction. Image
    by author.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥æ•°æ®çš„ç¤ºä¾‹è¡¨æ ¼åŠå…¶ç”Ÿæˆçš„å†³ç­–æ ‘é¢„æµ‹ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: '**Graphical View of a Decision Tree**'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**å†³ç­–æ ‘çš„å›¾å½¢è§†å›¾**'
- en: Another way to think about decision trees is **graphically**. (*This is personally
    the intuition I carry around for decision trees.)*
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ç§æ€è€ƒå†³ç­–æ ‘çš„æ–¹æ³•æ˜¯**å›¾å½¢åŒ–**ã€‚*ï¼ˆè¿™æ˜¯æˆ‘ä¸ªäººå¯¹å†³ç­–æ ‘çš„ç›´è§‚ç†è§£ã€‚ï¼‰*
- en: Imagine we take the two predictor variables from the example decision tree and
    visualize them on a 2D plot. We can then represent the decision tree splits as
    lines that divide our plot into different sections. This then allows us to identify
    the beverage choice by simply looking at which quadrant a data point lies.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ³è±¡ä¸€ä¸‹ï¼Œæˆ‘ä»¬å°†ç¤ºä¾‹å†³ç­–æ ‘ä¸­çš„ä¸¤ä¸ªé¢„æµ‹å˜é‡ç»˜åˆ¶åœ¨ä¸€ä¸ªäºŒç»´å›¾ä¸Šã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥å°†å†³ç­–æ ‘çš„åˆ†è£‚è¡¨ç¤ºä¸ºå°†å›¾åˆ’åˆ†ä¸ºä¸åŒåŒºåŸŸçš„çº¿æ¡ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ç®€å•åœ°æŸ¥çœ‹æ•°æ®ç‚¹æ‰€åœ¨çš„è±¡é™æ¥ç¡®å®šé¥®æ–™é€‰æ‹©ã€‚
- en: Intuitively, this is all a decision tree is doing. **Partitioning the predictor
    space into sections and assigning a label (or probability) to each section**.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ç›´è§‚ä¸Šçœ‹ï¼Œè¿™å°±æ˜¯å†³ç­–æ ‘çš„ä½œç”¨ã€‚**å°†é¢„æµ‹ç©ºé—´åˆ’åˆ†ä¸ºä¸åŒçš„éƒ¨åˆ†ï¼Œå¹¶ä¸ºæ¯ä¸ªéƒ¨åˆ†åˆ†é…ä¸€ä¸ªæ ‡ç­¾ï¼ˆæˆ–æ¦‚ç‡ï¼‰**ã€‚
- en: '![](../Images/1dca41b0e04b37add727a37582954df4.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1dca41b0e04b37add727a37582954df4.png)'
- en: Graphical view of decision tree predictions for tea or coffee example. Image
    by author.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å†³ç­–æ ‘å¯¹èŒ¶æˆ–å’–å•¡çš„é¢„æµ‹çš„å›¾å½¢è§†å›¾ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: '**How to Grow a Decision Tree?**'
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**å¦‚ä½•æ„å»ºå†³ç­–æ ‘ï¼Ÿ**'
- en: Decision trees are an intuitive way to partition data. However, it may not be
    easy to draw out an appropriate decision tree by hand using data. In such cases,
    **we can use machine learning** strategies to learn the â€œbestâ€ decision tree for
    a given dataset.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å†³ç­–æ ‘æ˜¯ä¸€ç§ç›´è§‚çš„æ•°æ®åˆ’åˆ†æ–¹æ³•ã€‚ç„¶è€Œï¼Œä½¿ç”¨æ•°æ®æ‰‹åŠ¨ç»˜åˆ¶ä¸€ä¸ªåˆé€‚çš„å†³ç­–æ ‘å¯èƒ½å¹¶ä¸å®¹æ˜“ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ**æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æœºå™¨å­¦ä¹ **ç­–ç•¥æ¥å­¦ä¹ é€‚ç”¨äºç»™å®šæ•°æ®é›†çš„â€œæœ€ä½³â€å†³ç­–æ ‘ã€‚
- en: Data can be used to grow decision trees in an optimization process called **training**.
    Training requires a training dataset consisting of predictor variables pre-labeled
    with target values.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®å¯ä»¥ç”¨äºåœ¨ä¸€ç§ç§°ä¸º**è®­ç»ƒ**çš„ä¼˜åŒ–è¿‡ç¨‹ä¸­æ„å»ºå†³ç­–æ ‘ã€‚è®­ç»ƒéœ€è¦ä¸€ä¸ªè®­ç»ƒæ•°æ®é›†ï¼Œå…¶ä¸­çš„é¢„æµ‹å˜é‡é¢„å…ˆæ ‡è®°äº†ç›®æ ‡å€¼ã€‚
- en: A standard strategy for training a decision tree uses something called **Greedy
    Search**. This is a popular technique in optimization, where we simplify a more
    complicated optimization problem by finding *locally* optimal solutions instead
    of *globally* optimal ones. (*I give an intuition for greedy search in a previous
    article on* [*causal discovery*](https://medium.com/towards-data-science/causal-discovery-6858f9af6dcb)*.)*
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§æ ‡å‡†çš„è®­ç»ƒå†³ç­–æ ‘çš„ç­–ç•¥ä½¿ç”¨è¢«ç§°ä¸º**è´ªå©ªæœç´¢**çš„æ–¹æ³•ã€‚è¿™æ˜¯ä¸€ç§åœ¨ä¼˜åŒ–ä¸­æµè¡Œçš„æŠ€æœ¯ï¼Œæˆ‘ä»¬é€šè¿‡æ‰¾åˆ°*å±€éƒ¨*æœ€ä¼˜è§£æ¥ç®€åŒ–æ›´å¤æ‚çš„ä¼˜åŒ–é—®é¢˜ï¼Œè€Œä¸æ˜¯*å…¨å±€*æœ€ä¼˜è§£ã€‚(*æˆ‘åœ¨ä¹‹å‰çš„ä¸€ç¯‡å…³äº*
    [*å› æœå‘ç°*](https://medium.com/towards-data-science/causal-discovery-6858f9af6dcb)*çš„æ–‡ç« ä¸­ç»™å‡ºäº†è´ªå©ªæœç´¢çš„ç›´è§‚è§£é‡Šã€‚*)
- en: In the case of decision trees, the Greedy Search determines the gain from each
    possible splitting option and then chooses the one that provides the greatest
    gain [1,2]. Here â€œ**gain**â€ is determined by the **split criterion**, which can
    be based on a few different quantities, e.g. **Gini impurity**, **information
    gain**, **mean squared error (MSE)**, among others. This process is repeated recursively
    until the decision tree is fully grown.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å†³ç­–æ ‘çš„æƒ…å†µä¸‹ï¼Œè´ªå©ªæœç´¢ç¡®å®šæ¯ä¸ªå¯èƒ½çš„åˆ†è£‚é€‰é¡¹çš„**å¢ç›Š**ï¼Œç„¶åé€‰æ‹©æä¾›æœ€å¤§å¢ç›Šçš„é‚£ä¸ªé€‰é¡¹[1,2]ã€‚è¿™é‡Œçš„â€œ**å¢ç›Š**â€ç”±**åˆ†è£‚å‡†åˆ™**å†³å®šï¼Œè¿™å¯ä»¥åŸºäºå‡ ç§ä¸åŒçš„é‡åº¦ï¼Œä¾‹å¦‚**åŸºå°¼
    impurity**ã€**ä¿¡æ¯å¢ç›Š**ã€**å‡æ–¹è¯¯å·® (MSE)**ç­‰ã€‚è¿™ä¸ªè¿‡ç¨‹ä¼šé€’å½’åœ°é‡å¤ï¼Œç›´åˆ°å†³ç­–æ ‘å®Œå…¨ç”Ÿæˆã€‚
- en: For example, if using Gini impurity, data records are recursively split into
    two groups such that the **weighted average impurity of the resulting groups is
    minimized**. This splitting procedure can continue until all data partitions are
    *pure*, meaning all data records in a given partition corresponds to a single
    target value.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¦‚æœä½¿ç”¨åŸºå°¼ impurityï¼Œæ•°æ®è®°å½•ä¼šé€’å½’åœ°åˆ†æˆä¸¤ä¸ªç»„ï¼Œä»¥ä½¿**ç»“æœç»„çš„åŠ æƒå¹³å‡ impurity æœ€å°åŒ–**ã€‚è¿™ä¸ªåˆ†è£‚è¿‡ç¨‹å¯ä»¥ç»§ç»­ï¼Œç›´åˆ°æ‰€æœ‰æ•°æ®åˆ†åŒºéƒ½æ˜¯*çº¯å‡€çš„*ï¼Œå³æ¯ä¸ªåˆ†åŒºä¸­çš„æ‰€æœ‰æ•°æ®è®°å½•éƒ½å¯¹åº”äºä¸€ä¸ªå•ä¸€çš„ç›®æ ‡å€¼ã€‚
- en: Although this implies decision trees can be *perfect* estimators, such an approach
    would result in **overfitting**. The trained **decision tree would not perform
    well on data sufficiently different than the training dataset**.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡è¿™æ„å‘³ç€å†³ç­–æ ‘å¯ä»¥æ˜¯*å®Œç¾*çš„ä¼°è®¡å™¨ï¼Œä½†è¿™ç§æ–¹æ³•å¯èƒ½ä¼šå¯¼è‡´**è¿‡æ‹Ÿåˆ**ã€‚è®­ç»ƒå¥½çš„**å†³ç­–æ ‘åœ¨ä¸è®­ç»ƒæ•°æ®é›†æœ‰æ˜¾è‘—ä¸åŒçš„æ•°æ®ä¸Šè¡¨ç°ä¸ä½³**ã€‚
- en: '**Hyperparameter Tuning**'
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**è¶…å‚æ•°è°ƒä¼˜**'
- en: One way to combat the overfitting problem is hyperparameter tuning. **Hyperparameters**
    are **values that constrain the growth of a decision tree**.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æŠ—è¿‡æ‹Ÿåˆé—®é¢˜çš„ä¸€ç§æ–¹æ³•æ˜¯è¶…å‚æ•°è°ƒä¼˜ã€‚**è¶…å‚æ•°**æ˜¯**é™åˆ¶å†³ç­–æ ‘å¢é•¿çš„å€¼**ã€‚
- en: Common decision tree hyperparameters are the maximum number of splits, minimum
    leaf size, and the number of splitting variables. The **key result** of setting
    decision tree hyperparameters is to **limit the treeâ€™s size**, which can help
    avoid overfitting and improve generalizability.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¸è§çš„å†³ç­–æ ‘è¶…å‚æ•°åŒ…æ‹¬æœ€å¤§åˆ†è£‚æ¬¡æ•°ã€æœ€å°å¶å­èŠ‚ç‚¹å¤§å°å’Œåˆ†è£‚å˜é‡çš„æ•°é‡ã€‚è®¾ç½®å†³ç­–æ ‘è¶…å‚æ•°çš„**å…³é”®ç»“æœ**æ˜¯**é™åˆ¶æ ‘çš„å¤§å°**ï¼Œè¿™æœ‰åŠ©äºé¿å…è¿‡æ‹Ÿåˆå¹¶æé«˜æ³›åŒ–èƒ½åŠ›ã€‚
- en: '**Alternative Training Strategies**'
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**æ›¿ä»£è®­ç»ƒç­–ç•¥**'
- en: While the training process I have described above is widely-used for decision
    trees, there are alternative approaches we can use.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æˆ‘ä¸Šè¿°æè¿°çš„è®­ç»ƒè¿‡ç¨‹åœ¨å†³ç­–æ ‘ä¸­å¹¿æ³›ä½¿ç”¨ï¼Œä½†æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å…¶ä»–æ›¿ä»£æ–¹æ³•ã€‚
- en: '**Pruning** â€” One such approach is called pruning [3]. In a sense, pruning
    is the opposite of growing a decision tree. Instead of starting from a root node
    and recursively adding nodes, we start with a fully grown tree and iteratively
    remove nodes.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**å‰ªæ**â€”â€”ä¸€ç§è¿™æ ·çš„æ–¹å¼å«åšå‰ªæ[3]ã€‚ä»æŸç§æ„ä¹‰ä¸Šè®²ï¼Œå‰ªææ˜¯å†³ç­–æ ‘ç”Ÿé•¿çš„åé¢ã€‚æˆ‘ä»¬ä¸æ˜¯ä»æ ¹èŠ‚ç‚¹å¼€å§‹é€’å½’åœ°æ·»åŠ èŠ‚ç‚¹ï¼Œè€Œæ˜¯ä»å®Œå…¨ç”Ÿæˆçš„æ ‘å¼€å§‹ï¼Œé€æ­¥å»é™¤èŠ‚ç‚¹ã€‚'
- en: While the pruning process can be done in multiple ways, it commonly will drop
    nodes that do not significantly increase model error. This is an alternative way
    to avoid overfitting in lieu of hyperparameter tuning to limit tree growth [3].
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰ªæè¿‡ç¨‹å¯ä»¥é€šè¿‡å¤šç§æ–¹å¼è¿›è¡Œï¼Œä½†é€šå¸¸ä¼šåˆ é™¤é‚£äº›ä¸ä¼šæ˜¾è‘—å¢åŠ æ¨¡å‹è¯¯å·®çš„èŠ‚ç‚¹ã€‚è¿™æ˜¯ä¸€ç§æ›¿ä»£è¶…å‚æ•°è°ƒä¼˜æ¥é™åˆ¶æ ‘çš„ç”Ÿé•¿ä»¥é¿å…è¿‡æ‹Ÿåˆçš„æ–¹æ³•[3]ã€‚
- en: '**Maximum Likelihood** â€” We can train a decision tree using the maximum likelihood
    framework [4]. While this approach is less well-known, it sits on a strong theoretical
    framework. It allows us to use information criteria such as AIC and BIC to objectively
    optimize the number of parameters in the tree and its performance, which helps
    side-step the need for extensive hyperparameter tuning.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**æœ€å¤§ä¼¼ç„¶**â€”â€”æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æœ€å¤§ä¼¼ç„¶æ¡†æ¶è®­ç»ƒå†³ç­–æ ‘[4]ã€‚è™½ç„¶è¿™ç§æ–¹æ³•ä¸å¤ªä¸ºäººçŸ¥ï¼Œä½†å®ƒåŸºäºä¸€ä¸ªå¼ºå¤§çš„ç†è®ºæ¡†æ¶ã€‚å®ƒå…è®¸æˆ‘ä»¬ä½¿ç”¨ä¿¡æ¯å‡†åˆ™å¦‚AICå’ŒBICæ¥å®¢è§‚ä¼˜åŒ–æ ‘ä¸­çš„å‚æ•°æ•°é‡åŠå…¶æ€§èƒ½ï¼Œè¿™æœ‰åŠ©äºé¿å…å¹¿æ³›çš„è¶…å‚æ•°è°ƒä¼˜ã€‚'
- en: 'Example code: Sepsis Survival Prediction Using a Decision Tree'
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ä»£ç ï¼šä½¿ç”¨å†³ç­–æ ‘è¿›è¡Œè„“æ¯’ç—‡ç”Ÿå­˜é¢„æµ‹
- en: Now, with a basic understanding of decision trees and how we can develop one
    from data, letâ€™s dive into a concrete example using Python. Here we will use a
    dataset from the [UCI machine learning repository](https://archive.ics.uci.edu/ml/datasets/Sepsis+survival+minimal+clinical+records)
    to train a decision tree to predict whether a patient will survive based on their
    age, sex, and number of sepsis episodes theyâ€™ve experienced [5,6].
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œäº†è§£äº†å†³ç­–æ ‘çš„åŸºæœ¬æ¦‚å¿µä»¥åŠå¦‚ä½•ä»æ•°æ®ä¸­æ„å»ºå†³ç­–æ ‘åï¼Œè®©æˆ‘ä»¬ä½¿ç”¨Pythonæ·±å…¥æ¢è®¨ä¸€ä¸ªå…·ä½“çš„ä¾‹å­ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ¥è‡ª[UCIæœºå™¨å­¦ä¹ åº“](https://archive.ics.uci.edu/ml/datasets/Sepsis+survival+minimal+clinical+records)çš„æ•°æ®é›†æ¥è®­ç»ƒä¸€ä¸ªå†³ç­–æ ‘ï¼Œä»¥é¢„æµ‹æ‚£è€…æ˜¯å¦ä¼šç”Ÿå­˜ï¼ŒåŸºäºä»–ä»¬çš„å¹´é¾„ã€æ€§åˆ«å’Œç»å†çš„è„“æ¯’ç—‡å‘ä½œæ¬¡æ•°[5,6]ã€‚
- en: For the decision tree training, we will use the sklearn Python library [7].
    The code for this example is freely available in the [GitHub repository](https://github.com/ShawhinT/YouTube-Blog/tree/main/decision-tree/decision_tree).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå†³ç­–æ ‘çš„è®­ç»ƒï¼Œæˆ‘ä»¬å°†ä½¿ç”¨sklearn Pythonåº“[7]ã€‚æ­¤ç¤ºä¾‹çš„ä»£ç å¯ä»¥åœ¨[GitHubä»“åº“](https://github.com/ShawhinT/YouTube-Blog/tree/main/decision-tree/decision_tree)ä¸­è‡ªç”±è·å–ã€‚
- en: We start by importing some helpful libraries.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»å¯¼å…¥ä¸€äº›æœ‰ç”¨çš„åº“å¼€å§‹ã€‚
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next, we load our data from a .csv file and do some data preparation.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä».csvæ–‡ä»¶ä¸­åŠ è½½æ•°æ®å¹¶è¿›è¡Œä¸€äº›æ•°æ®å‡†å¤‡ã€‚
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/fea3c309833e691f553f7aaf79ce4e0e.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fea3c309833e691f553f7aaf79ce4e0e.png)'
- en: Histograms for each variable in dataset. Image by author.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›†ä¸­æ¯ä¸ªå˜é‡çš„ç›´æ–¹å›¾ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: Notice in the bottom-right histogram we have many more alive records than dead.
    This is called an **imbalanced dataset**. For a simple decision tree classifier,
    learning from imbalanced data can lead to the **decision tree *over-predicting*
    the majority class**.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œåœ¨å³ä¸‹è§’çš„ç›´æ–¹å›¾ä¸­ï¼Œæˆ‘ä»¬æœ‰æ›´å¤šçš„å­˜æ´»è®°å½•è€Œéæ­»äº¡è®°å½•ã€‚è¿™è¢«ç§°ä¸º**ä¸å¹³è¡¡æ•°æ®é›†**ã€‚å¯¹äºä¸€ä¸ªç®€å•çš„å†³ç­–æ ‘åˆ†ç±»å™¨ï¼Œä»ä¸å¹³è¡¡çš„æ•°æ®ä¸­å­¦ä¹ å¯èƒ½ä¼šå¯¼è‡´**å†³ç­–æ ‘*è¿‡åº¦é¢„æµ‹*å¤šæ•°ç±»**ã€‚
- en: To handle this situation, we can over-sample the minority class to make our
    data more balanced. One way to do this is using a technique called **Synthetic
    Minority Over-sampling Technique** (**SMOTE)**. While I will leave further details
    of SMOTE for a future article, for now, it will suffice to say this helps us balance
    our data and improve our decision tree model.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å¤„ç†è¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è¿‡é‡‡æ ·å°‘æ•°ç±»æ¥ä½¿æ•°æ®æ›´å¹³è¡¡ã€‚ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨ä¸€ç§å«åš**åˆæˆå°‘æ•°ç±»è¿‡é‡‡æ ·æŠ€æœ¯**ï¼ˆ**SMOTE**ï¼‰çš„æŠ€æœ¯ã€‚è™½ç„¶æˆ‘ä¼šåœ¨æœªæ¥çš„æ–‡ç« ä¸­è¯¦ç»†ä»‹ç»SMOTEï¼Œä½†ç›®å‰åªéœ€çŸ¥é“è¿™æœ‰åŠ©äºæˆ‘ä»¬å¹³è¡¡æ•°æ®å¹¶æ”¹è¿›å†³ç­–æ ‘æ¨¡å‹å³å¯ã€‚
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/045d096e7d85b27e2f9ba1701d3cb90b.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/045d096e7d85b27e2f9ba1701d3cb90b.png)'
- en: Outcome histogram after SMOTE. Image by author.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: SMOTEä¹‹åçš„ç»“æœç›´æ–¹å›¾ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: The final step for our data preparation is to split our resampled data into
    training and testing datasets. The **training data** will be used to **grow the
    decision tree,** and **testing data** will be used to **evaluate its performance**.
    Here we use an 80â€“20 train-test split.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®å‡†å¤‡çš„æœ€åä¸€æ­¥æ˜¯å°†æˆ‘ä»¬çš„é‡é‡‡æ ·æ•°æ®æ‹†åˆ†ä¸ºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ã€‚**è®­ç»ƒæ•°æ®**å°†ç”¨äº**æ„å»ºå†³ç­–æ ‘**ï¼Œè€Œ**æµ‹è¯•æ•°æ®**å°†ç”¨äº**è¯„ä¼°å…¶æ€§èƒ½**ã€‚è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨80â€“20çš„è®­ç»ƒ-æµ‹è¯•æ‹†åˆ†ã€‚
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now with our training data, we can create our decision tree. Sklearn makes this
    super easy, with just two lines of code, we have a decision tree.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æœ‰äº†è®­ç»ƒæ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºæˆ‘ä»¬çš„å†³ç­–æ ‘ã€‚Sklearnä½¿è¿™ä¸€è¿‡ç¨‹éå¸¸ç®€å•ï¼Œä»…ç”¨ä¸¤è¡Œä»£ç ï¼Œæˆ‘ä»¬å°±èƒ½å¾—åˆ°ä¸€ä¸ªå†³ç­–æ ‘ã€‚
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Letâ€™s take a look at the result.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ç»“æœã€‚
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/cad6d35f2ad5a40779dfc1f54b3ee1e9.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cad6d35f2ad5a40779dfc1f54b3ee1e9.png)'
- en: Fully grown decision tree. Image by author.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œå…¨ç”Ÿé•¿çš„å†³ç­–æ ‘ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: Needless to say, this is a very big decision tree, which can make interpreting
    the results difficult. However, letâ€™s put that point aside for now and evaluate
    the modelâ€™s performance.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ç”¨è¯´ï¼Œè¿™æ˜¯ä¸€æ£µéå¸¸å¤§çš„å†³ç­–æ ‘ï¼Œè¿™å¯èƒ½ä¼šä½¿è§£é‡Šç»“æœå˜å¾—å›°éš¾ã€‚ç„¶è€Œï¼Œè®©æˆ‘ä»¬æš‚æ—¶æç½®è¿™ä¸€ç‚¹ï¼Œè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚
- en: For evaluating performance, we use a **confusion matrix**, which **displays
    the number of true positives (TP), true negatives (TN), false positives (FP),
    and false negatives (FN)**.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¯„ä¼°æ€§èƒ½ï¼Œæˆ‘ä»¬ä½¿ç”¨**æ··æ·†çŸ©é˜µ**ï¼Œ**å®ƒæ˜¾ç¤ºäº†çœŸæ­£ä¾‹ï¼ˆTPï¼‰ã€çœŸè´Ÿä¾‹ï¼ˆTNï¼‰ã€å‡æ­£ä¾‹ï¼ˆFPï¼‰å’Œå‡è´Ÿä¾‹ï¼ˆFNï¼‰çš„æ•°é‡**ã€‚
- en: I wonâ€™t get into a discussion of confusion matrices here, but for now what **we
    want** is for the **on-diagonal numbers to be big** and the **off-diagonal terms
    to be small**.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¸ä¼šåœ¨è¿™é‡Œè®¨è®ºæ··æ·†çŸ©é˜µï¼Œä½†ç›®å‰**æˆ‘ä»¬å¸Œæœ›**çš„æ˜¯**å¯¹è§’çº¿ä¸Šçš„æ•°å­—è¦å¤§**ï¼Œè€Œ**éå¯¹è§’çº¿ä¸Šçš„æ•°å­—è¦å°**ã€‚
- en: '![](../Images/847c78b12ad89d7bf2bf0e5f6bc326b9.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/847c78b12ad89d7bf2bf0e5f6bc326b9.png)'
- en: Confusion matrices for fully-grown decision tree. (Left) Training data set.
    (Right) Testing dataset. Image by author.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œå…¨ç”Ÿé•¿çš„å†³ç­–æ ‘çš„æ··æ·†çŸ©é˜µã€‚ï¼ˆå·¦ï¼‰è®­ç»ƒæ•°æ®é›†ã€‚ï¼ˆå³ï¼‰æµ‹è¯•æ•°æ®é›†ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: 'We can take the numbers from our confusion matrices and compute three different
    performance metrics: precision, recall, and f1-score**.** Briefly, **precision**
    = TP / (TP + FP), **recall** = TP / (TP + FN), and the **f1-score** is the harmonic
    mean of precision and recall.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä»æ··æ·†çŸ©é˜µä¸­è·å–æ•°æ®ï¼Œå¹¶è®¡ç®—ä¸‰ä¸ªä¸åŒçš„æ€§èƒ½æŒ‡æ ‡ï¼šç²¾ç¡®åº¦ã€å¬å›ç‡å’Œ f1-score**ã€‚** ç®€å•æ¥è¯´ï¼Œ**ç²¾ç¡®åº¦** = TP / (TP +
    FP)ï¼Œ**å¬å›ç‡** = TP / (TP + FN)ï¼Œ**f1-score** æ˜¯ç²¾ç¡®åº¦å’Œå¬å›ç‡çš„è°ƒå’Œå‡å€¼ã€‚
- en: '![](../Images/867103da82f0cb2b241406a704a3ed82.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/867103da82f0cb2b241406a704a3ed82.png)'
- en: Three performance metrics for fully-grown decision tree. Image by author.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œå…¨æˆé•¿çš„å†³ç­–æ ‘çš„ä¸‰ä¸ªæ€§èƒ½æŒ‡æ ‡ã€‚ å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: The code to generate these results is given below.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆè¿™äº›ç»“æœçš„ä»£ç å¦‚ä¸‹ã€‚
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Hyperparameter Tuning
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¶…å‚æ•°è°ƒæ•´
- en: While the decision tree has decent performance on the data used here, there
    is still the lingering issue of **interpretability**. Looking at the decision
    tree displayed previously, it would be challenging for a clinician to extract
    any meaningful insights from the decision treeâ€™s logic.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å†³ç­–æ ‘åœ¨è¿™é‡Œä½¿ç”¨çš„æ•°æ®ä¸Šè¡¨ç°ä¸é”™ï¼Œä½†ä»ç„¶å­˜åœ¨**å¯è§£é‡Šæ€§**çš„é—®é¢˜ã€‚æŸ¥çœ‹ä¹‹å‰å±•ç¤ºçš„å†³ç­–æ ‘ï¼Œä¸´åºŠåŒ»ç”Ÿä»å†³ç­–æ ‘çš„é€»è¾‘ä¸­æå–æœ‰æ„ä¹‰çš„è§è§£å°†æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚
- en: This is where hyperparameter tuning can help. To do this with sklearn, we can
    simply add input arguments to our decision tree training step.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯è¶…å‚æ•°è°ƒæ•´å¯ä»¥å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚ä¸ºäº†ä½¿ç”¨ sklearn å®Œæˆè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬åªéœ€åœ¨å†³ç­–æ ‘è®­ç»ƒæ­¥éª¤ä¸­æ·»åŠ è¾“å…¥å‚æ•°å³å¯ã€‚
- en: Here we will try setting the max_depth = 3.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œæˆ‘ä»¬å°†å°è¯•è®¾ç½® max_depth = 3ã€‚
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now, letâ€™s take a look at the resulting decision tree.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ç»“æœå†³ç­–æ ‘ã€‚
- en: '![](../Images/f9b0eaf963d0a43c1b277088afe9ea3f.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f9b0eaf963d0a43c1b277088afe9ea3f.png)'
- en: Tuned decision tree so that max_depth=3\. Image by author.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒæ•´åçš„å†³ç­–æ ‘ï¼Œmax_depth=3ã€‚ å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: Since we constrained the max depth of the tree, we can plainly see what splits
    are happening here.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæˆ‘ä»¬é™åˆ¶äº†æ ‘çš„æœ€å¤§æ·±åº¦ï¼Œæˆ‘ä»¬å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°è¿™é‡Œå‘ç”Ÿäº†å“ªäº›åˆ†è£‚ã€‚
- en: We again evaluate the modelâ€™s performance using confusion matrices and the same
    three performance metrics as before.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å†æ¬¡ä½¿ç”¨æ··æ·†çŸ©é˜µå’Œä¹‹å‰ç›¸åŒçš„ä¸‰ä¸ªæ€§èƒ½æŒ‡æ ‡æ¥è¯„ä¼°æ¨¡å‹çš„è¡¨ç°ã€‚
- en: '![](../Images/9540e446233230f2be271a1a4f1045aa.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9540e446233230f2be271a1a4f1045aa.png)'
- en: Confusion matrices for hyperparameter tuned decision tree. (Left) Training data
    set. (Right) Testing dataset. Image by author
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: è¶…å‚æ•°è°ƒæ•´å†³ç­–æ ‘çš„æ··æ·†çŸ©é˜µã€‚ ï¼ˆå·¦ï¼‰è®­ç»ƒæ•°æ®é›†ã€‚ ï¼ˆå³ï¼‰æµ‹è¯•æ•°æ®é›†ã€‚ å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: '![](../Images/0246f2318fcfd809c78fd1598db48ef0.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0246f2318fcfd809c78fd1598db48ef0.png)'
- en: 3 performance metrics for hyperparameter tuned decision tree. Image by author.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: è¶…å‚æ•°è°ƒæ•´åçš„å†³ç­–æ ‘çš„ä¸‰ä¸ªæ€§èƒ½æŒ‡æ ‡ã€‚ å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: Although it may seem the fully-grown tree is preferable to the hyperparameter-tuned
    one, this goes back to the discussion on overfitting. Yes, the fully-grown tree
    performance is better on the current data, but I would not expect this to be the
    case for other data.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å®Œå…¨æˆé•¿çš„æ ‘å¯èƒ½çœ‹èµ·æ¥æ¯”è¶…å‚æ•°è°ƒæ•´çš„æ ‘æ›´å¯å–ï¼Œä½†è¿™å›åˆ°äº†è¿‡æ‹Ÿåˆçš„è®¨è®ºã€‚æ˜¯çš„ï¼Œå®Œå…¨æˆé•¿çš„æ ‘åœ¨å½“å‰æ•°æ®ä¸Šçš„è¡¨ç°æ›´å¥½ï¼Œä½†æˆ‘ä¸è®¤ä¸ºè¿™é€‚ç”¨äºå…¶ä»–æ•°æ®ã€‚
- en: Put another way, **although the simpler decision tree has worse performance
    here, it will likely generalize better than the fully-grown tree.**
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢å¥è¯è¯´ï¼Œ**è™½ç„¶ç®€å•çš„å†³ç­–æ ‘åœ¨è¿™é‡Œçš„è¡¨ç°è¾ƒå·®ï¼Œä½†å®ƒå¯èƒ½æ¯”å®Œå…¨æˆé•¿çš„æ ‘æ›´å…·æ³›åŒ–èƒ½åŠ›ã€‚**
- en: This hypothesis can be tested by applying each model to the other two datasets
    available in the GitHub [repo](https://github.com/ShawhinT/YouTube-Blog/tree/main/decision-tree/decision_tree).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå‡è®¾å¯ä»¥é€šè¿‡å°†æ¯ä¸ªæ¨¡å‹åº”ç”¨äº GitHub [repo](https://github.com/ShawhinT/YouTube-Blog/tree/main/decision-tree/decision_tree)
    ä¸­çš„å…¶ä»–ä¸¤ä¸ªæ•°æ®é›†æ¥è¿›è¡Œæµ‹è¯•ã€‚
- en: '[](https://github.com/ShawhinT/YouTube-Blog/tree/main/decision-tree/decision_tree?source=post_page-----dac9592f4b7f--------------------------------)
    [## YouTube-Blog/decision-tree/decision_tree at main Â· ShawhinT/YouTube-Blog'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/ShawhinT/YouTube-Blog/tree/main/decision-tree/decision_tree?source=post_page-----dac9592f4b7f--------------------------------)
    [## YouTube-Blog/decision-tree/decision_tree at main Â· ShawhinT/YouTube-Blog'
- en: Codes to complement YouTube videos and blog posts on Medium. - YouTube-Blog/decision-tree/decision_tree
    at main Â·â€¦
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä»£ç è¡¥å……äº† YouTube è§†é¢‘å’Œ Medium ä¸Šçš„åšå®¢æ–‡ç« ã€‚ - YouTube-Blog/decision-tree/decision_tree
    at main Â·â€¦
- en: github.com](https://github.com/ShawhinT/YouTube-Blog/tree/main/decision-tree/decision_tree?source=post_page-----dac9592f4b7f--------------------------------)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/ShawhinT/YouTube-Blog/tree/main/decision-tree/decision_tree?source=post_page-----dac9592f4b7f--------------------------------)
- en: Decision Tree Ensembles
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å†³ç­–æ ‘é›†æˆ
- en: While hyperparameter tuning can improve the generalizability of a decision tree,
    it still leaves something to be desired in regard to performance. In our example
    above, after hyperparameter tuning, the decision tree still mislabelled the training
    data **35%** of the time, which is a big deal when talking about life and death
    (*like with the example here*).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶è¶…å‚æ•°è°ƒä¼˜å¯ä»¥æé«˜å†³ç­–æ ‘çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†åœ¨æ€§èƒ½æ–¹é¢ä»æœ‰ä¸è¶³ã€‚åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œç»è¿‡è¶…å‚æ•°è°ƒä¼˜åï¼Œå†³ç­–æ ‘ä»ç„¶å°†è®­ç»ƒæ•°æ®**35%**çš„æ—¶é—´æ ‡è®°é”™è¯¯ï¼Œè¿™åœ¨è®¨è®ºç”Ÿæ­»é—®é¢˜æ—¶ï¼ˆ*å¦‚æ­¤å¤„çš„ä¾‹å­æ‰€ç¤º*ï¼‰æ˜¯ä¸€ä¸ªå¤§é—®é¢˜ã€‚
- en: A popular solution to this problem is to use an **ensemble of trees rather than
    a single decision tree to make predictions.** These are called **decision tree
    ensembles** and will be the topic of the [next article](https://medium.com/towards-data-science/10-decision-trees-are-better-than-1-719406680564)
    in this series.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: è§£å†³è¿™ä¸ªé—®é¢˜çš„ä¸€ç§æµè¡Œæ–¹æ³•æ˜¯ä½¿ç”¨**å¤šä¸ªå†³ç­–æ ‘è€Œä¸æ˜¯å•ä¸€çš„å†³ç­–æ ‘æ¥è¿›è¡Œé¢„æµ‹**ã€‚è¿™äº›è¢«ç§°ä¸º**å†³ç­–æ ‘é›†æˆ**ï¼Œå°†æˆä¸ºæœ¬ç³»åˆ—çš„[ä¸‹ä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/10-decision-trees-are-better-than-1-719406680564)çš„ä¸»é¢˜ã€‚
- en: '[](/10-decision-trees-are-better-than-1-719406680564?source=post_page-----dac9592f4b7f--------------------------------)
    [## 10 Decision Trees are Better Than 1'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/10-decision-trees-are-better-than-1-719406680564?source=post_page-----dac9592f4b7f--------------------------------)
    [## 10 å†³ç­–æ ‘æ¯” 1 ä¸ªæ›´å¥½'
- en: Breaking down bagging, boosting, Random Forest, and AdaBoost
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è§£æ baggingã€boostingã€éšæœºæ£®æ—å’Œ AdaBoost
- en: towardsdatascience.com](/10-decision-trees-are-better-than-1-719406680564?source=post_page-----dac9592f4b7f--------------------------------)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/10-decision-trees-are-better-than-1-719406680564?source=post_page-----dac9592f4b7f--------------------------------)
- en: Resources
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: èµ„æº
- en: '**Connect**: [My website](https://shawhintalebi.com/) | [Book a call](https://calendly.com/shawhintalebi)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**è”ç³»**: [æˆ‘çš„ç½‘ç«™](https://shawhintalebi.com/) | [é¢„çº¦ç”µè¯](https://calendly.com/shawhintalebi)'
- en: '**Socials**: [YouTube ğŸ¥](https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA)
    | [LinkedIn](https://www.linkedin.com/in/shawhintalebi/) | [Twitter](https://twitter.com/ShawhinT)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¤¾äº¤åª’ä½“**: [YouTube ğŸ¥](https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA)
    | [LinkedIn](https://www.linkedin.com/in/shawhintalebi/) | [Twitter](https://twitter.com/ShawhinT)'
- en: '**Support**: [Buy me a coffee](https://www.buymeacoffee.com/shawhint) â˜•ï¸'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ”¯æŒ**: [è¯·æˆ‘å–å’–å•¡](https://www.buymeacoffee.com/shawhint) â˜•ï¸'
- en: '[](https://shawhin.medium.com/subscribe?source=post_page-----dac9592f4b7f--------------------------------)
    [## Get FREE access to every new story I write'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://shawhin.medium.com/subscribe?source=post_page-----dac9592f4b7f--------------------------------)
    [## å…è´¹è·å–æˆ‘å†™çš„æ¯ä¸ªæ–°æ•…äº‹'
- en: Get FREE access to every new story I write P.S. I do not share your email with
    anyone By signing up, you will create aâ€¦
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å…è´¹è·å–æˆ‘å†™çš„æ¯ä¸ªæ–°æ•…äº‹ P.S. æˆ‘ä¸ä¼šå°†ä½ çš„é‚®ç®±ä¸ä»»ä½•äººåˆ†äº«ã€‚é€šè¿‡æ³¨å†Œï¼Œä½ å°†åˆ›å»ºä¸€ä¸ªâ€¦
- en: shawhin.medium.com](https://shawhin.medium.com/subscribe?source=post_page-----dac9592f4b7f--------------------------------)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: shawhin.medium.com](https://shawhin.medium.com/subscribe?source=post_page-----dac9592f4b7f--------------------------------)
- en: '[1] [*Classification and Regression Trees by* Breiman et al.](https://doi.org/10.1201/9781315139470)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [*åˆ†ç±»ä¸å›å½’æ ‘* ç”± Breiman ç­‰äººè‘—](https://doi.org/10.1201/9781315139470)'
- en: '[2] [Decision trees: a recent overview by Kotsiantis, S. B.](https://link.springer.com/article/10.1007/s10462-011-9272-4)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [å†³ç­–æ ‘ï¼šKotsiantis, S. B. æœ€è¿‘çš„æ¦‚è¿°](https://link.springer.com/article/10.1007/s10462-011-9272-4)'
- en: '[3] [A comparative analysis of methods for pruning decision trees by Esposito
    et al.](https://doi.org/10.1109/34.589207)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [Esposito ç­‰äººå¯¹å‰ªæå†³ç­–æ ‘æ–¹æ³•çš„æ¯”è¾ƒåˆ†æ](https://doi.org/10.1109/34.589207)'
- en: '[4] [Maximum likelihood regression trees by Su et al.](https://doi.org/10.1198/106186004X2165)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [Su ç­‰äººæå‡ºçš„æœ€å¤§ä¼¼ç„¶å›å½’æ ‘](https://doi.org/10.1198/106186004X2165)'
- en: '[5] Dua, D. and Graff, C. (2019). [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/census+income)
    [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School
    of Information and Computer Science. (CC BY 4.0)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Dua, D. å’Œ Graff, C. (2019). [UCI æœºå™¨å­¦ä¹ åº“](https://archive.ics.uci.edu/ml/datasets/census+income)
    [http://archive.ics.uci.edu/ml]ã€‚åŠ åˆ©ç¦å°¼äºšå·å°”æ¹¾: åŠ å·å¤§å­¦ä¿¡æ¯ä¸è®¡ç®—æœºç§‘å­¦å­¦é™¢ã€‚ï¼ˆCC BY 4.0ï¼‰'
- en: '[6] [Survival prediction of patients with sepsis from age, sex, and septic
    episode number alone by Chicco & Jurman](https://www.nature.com/articles/s41598-020-73558-3)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] [Chicco å’Œ Jurman é€šè¿‡å¹´é¾„ã€æ€§åˆ«å’Œè„“æ¯’ç—‡å‘ä½œæ¬¡æ•°æ¥é¢„æµ‹è„“æ¯’ç—‡æ‚£è€…çš„ç”Ÿå­˜ç‡](https://www.nature.com/articles/s41598-020-73558-3)'
- en: '[7] [Scikit-learn: Machine Learning in Python](https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html),
    Pedregosa *et al.*, JMLR 12, pp. 2825â€“2830, 2011.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] [Scikit-learn: Pythonä¸­çš„æœºå™¨å­¦ä¹ ](https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html)ï¼ŒPedregosa
    *ç­‰äºº*ï¼ŒJMLR 12ï¼Œ2825â€“2830é¡µï¼Œ2011å¹´ã€‚'
