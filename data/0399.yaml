- en: 'Not All Rainbows and Sunshine: The Darker Side of ChatGPT'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并非全是彩虹和阳光：ChatGPT 的阴暗面
- en: 原文：[https://towardsdatascience.com/not-all-rainbows-and-sunshine-the-darker-side-of-chatgpt-75917472b9c?source=collection_archive---------0-----------------------#2023-01-27](https://towardsdatascience.com/not-all-rainbows-and-sunshine-the-darker-side-of-chatgpt-75917472b9c?source=collection_archive---------0-----------------------#2023-01-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/not-all-rainbows-and-sunshine-the-darker-side-of-chatgpt-75917472b9c?source=collection_archive---------0-----------------------#2023-01-27](https://towardsdatascience.com/not-all-rainbows-and-sunshine-the-darker-side-of-chatgpt-75917472b9c?source=collection_archive---------0-----------------------#2023-01-27)
- en: 'Part 1: The Risks and Ethical Issues Associated with Large Language Models'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1部分：大型语言模型的风险和伦理问题
- en: '[](https://mary-reagan.medium.com/?source=post_page-----75917472b9c--------------------------------)[![Mary
    Reagan PhD](../Images/e3e149519419faf20a50f44600378755.png)](https://mary-reagan.medium.com/?source=post_page-----75917472b9c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----75917472b9c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----75917472b9c--------------------------------)
    [Mary Reagan PhD](https://mary-reagan.medium.com/?source=post_page-----75917472b9c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mary-reagan.medium.com/?source=post_page-----75917472b9c--------------------------------)[![Mary
    Reagan PhD](../Images/e3e149519419faf20a50f44600378755.png)](https://mary-reagan.medium.com/?source=post_page-----75917472b9c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----75917472b9c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----75917472b9c--------------------------------)
    [Mary Reagan PhD](https://mary-reagan.medium.com/?source=post_page-----75917472b9c--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4a596f4380a0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnot-all-rainbows-and-sunshine-the-darker-side-of-chatgpt-75917472b9c&user=Mary+Reagan+PhD&userId=4a596f4380a0&source=post_page-4a596f4380a0----75917472b9c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----75917472b9c--------------------------------)
    ·9 min read·Jan 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F75917472b9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnot-all-rainbows-and-sunshine-the-darker-side-of-chatgpt-75917472b9c&user=Mary+Reagan+PhD&userId=4a596f4380a0&source=-----75917472b9c---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4a596f4380a0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnot-all-rainbows-and-sunshine-the-darker-side-of-chatgpt-75917472b9c&user=Mary+Reagan+PhD&userId=4a596f4380a0&source=post_page-4a596f4380a0----75917472b9c---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----75917472b9c--------------------------------)
    · 9 分钟阅读 · 2023年1月27日'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F75917472b9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnot-all-rainbows-and-sunshine-the-darker-side-of-chatgpt-75917472b9c&source=-----75917472b9c---------------------bookmark_footer-----------)![](../Images/352cf5e838507e7a22b65e9a6aea2a22.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F75917472b9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnot-all-rainbows-and-sunshine-the-darker-side-of-chatgpt-75917472b9c&source=-----75917472b9c---------------------bookmark_footer-----------)![](../Images/352cf5e838507e7a22b65e9a6aea2a22.png)'
- en: '[Image](https://drive.google.com/file/d/1tPFnutb_CTri72gX8RuJAVOLWtPYRXAh/view?usp=share_link)
    credit Fiddler AI with permission'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[图片](https://drive.google.com/file/d/1tPFnutb_CTri72gX8RuJAVOLWtPYRXAh/view?usp=share_link)
    由 Fiddler AI 提供，已获许可'
- en: If you haven’t heard about ChatGPT, you must be hiding under a very large rock.
    The viral chatbot, used for natural language processing tasks like text generation,
    is hitting the news everywhere. OpenAI, the company behind it, was recently in
    talks to get a valuation of $29 billion¹ and Microsoft may soon invest another
    $10 billion².
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没听说过 ChatGPT，你一定是躲在一块非常大的石头下。这款病毒式聊天机器人被用于自然语言处理任务，如文本生成，已经引起了广泛关注。其背后的公司
    OpenAI 最近在谈判以获得 290 亿美元的估值¹，微软可能很快会再投资 100 亿美元²。
- en: ChatGPT is an autoregressive language model that uses deep learning to produce
    text. It has amazed users by its detailed answers across a variety of domains.
    Its answers are so convincing that it can be difficult to tell whether or not
    they were written by a human. Built on OpenAI’s GPT-3 family of large language
    models (LLMs), ChatGPT was launched on November 30, 2022\. It is one of the largest
    LLMs and can write eloquent essays and poems, produce usable code, and generate
    charts and websites from text description, all with limited to no supervision.
    ChatGPT’s answers are so good, it is showing itself to be a potential rival to
    the ubiquitous Google search engine³.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT是一个自回归语言模型，使用深度学习生成文本。它通过在各种领域提供详细的回答让用户感到惊讶。它的回答非常有说服力，以至于很难判断这些回答是否由人类撰写。ChatGPT建立在OpenAI的GPT-3系列大型语言模型（LLMs）基础上，于2022年11月30日推出。它是最大的LLM之一，可以写出优美的文章和诗歌，生成可用的代码，以及根据文本描述生成图表和网站，所有这些都不需要或几乎不需要监督。ChatGPT的回答如此出色，它显示出有可能成为无处不在的Google搜索引擎的潜在对手。
- en: Large language models are … well… ***large***. They are trained on enormous
    amounts of text data which can be on the order of petabytes and have billions
    of parameters. The resulting multi-layer neural networks are often several terabytes
    in size. The hype and media attention surrounding ChatGPT and other LLMs is understandable
    — they are indeed remarkable developments of human ingenuity, sometimes surprising
    the developers of these models with emergent behaviors. For example, GPT-3’s answers
    are improved by using the certain ‘magic’ phrases like “Let’s think step by step”
    at the beginning of a prompt⁴. These emergent behaviors point to their model’s
    incredible complexity combined with a current lack of explainability, have even
    made developers ponder whether the models are sentient⁵.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型是……嗯……***庞大***的。它们在大量的文本数据上进行训练，这些数据可以达到PB级，并且具有数十亿个参数。最终的多层神经网络通常有几个TB大。围绕ChatGPT和其他LLM的炒作和媒体关注是可以理解的——它们确实是人类智慧的杰出成果，有时会以新兴行为让这些模型的开发者感到惊讶。例如，通过在提示的开头使用某些“魔法”短语，如“让我们一步一步思考”，可以改善GPT-3的回答。这些新兴行为表明了模型的巨大复杂性以及当前的可解释性缺失，甚至让开发者思考这些模型是否具有意识。
- en: The Spectre of Large Language Models
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型的幽灵
- en: 'With all the positive buzz and hype, there has been a smaller, forceful chorus
    of warnings from those within the [Responsible AI](https://www.fiddler.ai/blog/responsible-ai-by-design)
    community. Notably in 2021, Timit Gebru, a prominent researcher working on responsible
    AI, published a paper⁶ that warned of the many ethical issues related to LLMs
    which led to her be fired from Google. These warnings span a wide range of issues⁷:
    lack of interpretability, plagiarism, privacy, bias, model robustness, and their
    environmental impact. Let’s dive a little into each of these topics.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有许多积极的宣传和炒作，但[负责任的人工智能](https://www.fiddler.ai/blog/responsible-ai-by-design)社区中的一些人发出了强烈的警告。值得注意的是，在2021年，负责人工智能领域的著名研究员Timit
    Gebru发表了一篇论文，警告了与LLM相关的许多伦理问题，这也导致她被谷歌解雇。这些警告涉及广泛的问题：缺乏可解释性、抄袭、隐私、偏见、模型鲁棒性以及其环境影响。让我们稍微探讨一下这些话题。
- en: 'Trust and Lack of Interpretability:'
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信任与缺乏可解释性：
- en: Deep learning models, and LLMs in particular, have become so large and opaque
    that even the model developers are often unable to understand why their models
    are making certain predictions. This lack of interpretability is a significant
    concern, especially in settings where users would like to know why and how a model
    generated a particular output.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型，尤其是LLM，已经变得如此庞大和不透明，以至于即使是模型的开发者也常常无法理解为什么他们的模型会做出某些预测。这种缺乏可解释性是一个重大问题，特别是在用户希望了解模型生成特定输出的原因和方式的情况下。
- en: In a lighter vein, our CEO, Krishna Gade, used ChatGPT to create a poem⁸ on
    explainable AI in the style of John Keats, and, frankly, I think it turned out
    pretty well.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在轻松的风格下，我们的首席执行官Krishna Gade使用了ChatGPT创作了一首关于可解释人工智能的诗歌，风格模仿了**约翰·济慈**，坦率地说，我认为效果相当不错。
- en: '![](../Images/7c8dff325094a5cb5f589745e90a4bf0.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c8dff325094a5cb5f589745e90a4bf0.png)'
- en: Krishna rightfully pointed out that the transparency around how the model arrived
    at this output is lacking. For pieces of work produced by LLMs, the lack of transparency
    around which sources of data the output is drawing on means that the answers provided
    by ChatGPT are impossible to properly cite and therefore impossible for users
    to validate or trust its output⁹. This has led to bans of ChatGPT-created answers
    on forums like Stack Overflow¹⁰.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 克里希纳正确指出，关于模型如何得出输出的透明度不足。对于LLMs生成的作品，缺乏关于输出所依赖数据来源的透明度意味着ChatGPT提供的答案无法被正确引用，因此用户无法验证或信任其输出⁹。这导致ChatGPT创建的答案在像Stack
    Overflow¹⁰这样的论坛上被禁止。
- en: Transparency and an understanding of how a model arrived at its output becomes
    especially important when using something like OpenAI’s Embedding Model¹¹, which
    inherently contains a layer of obscurity, or in other cases where models are used
    for high-stakes decisions. For example, if someone were to use ChatGPT to get
    first aid instructions, users need to know the response is reliable, accurate,
    and derived from trustworthy sources. While various post-hoc methods to explain
    a model’s choices exist, these explanations are often overlooked when a model
    is deployed.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用像OpenAI的嵌入模型¹¹这样的工具时，透明度和理解模型如何得出输出变得尤为重要，因为这些模型本质上包含了一层模糊性，或者在模型用于高风险决策的情况下。例如，如果有人使用ChatGPT获取急救指示，用户需要知道回应是可靠的、准确的，并且来源可信。虽然存在各种事后方法来解释模型的选择，但这些解释在模型部署时往往被忽视。
- en: The ramifications of such a lack of transparency and trustworthiness are particularly
    troubling in the era of fake news and misinformation, where LLMs could be fine-tuned
    to spread misinformation and threaten political stability. While Open AI is working
    on various approaches to identify its model’s output and plans to embed cryptographic
    tags to watermark the outputs¹², these Responsible AI solutions can’t come fast
    enough and may be insufficient.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这种缺乏透明度和可信度的后果在假新闻和虚假信息泛滥的时代尤其令人担忧，因为LLMs可能会被微调以传播虚假信息并威胁政治稳定。虽然Open AI正在研究各种方法来识别其模型的输出，并计划嵌入加密标签以对输出进行水印¹²，这些负责任的AI解决方案的速度仍然不足，可能也不够充分。
- en: This leads to issues around …
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这引发了关于……的问题。
- en: 'Plagiarism:'
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 剽窃：
- en: Difficulty in tracing the origin of a perfectly crafted ChatGPT essay naturally
    leads to conversations on plagiarism. But is this *really* a problem? This author
    does not think so. Before the arrival of ChatGPT, students already had access
    to services that would write essays for them¹³, and there has always been a small
    percentage of students who are determined to cheat. But hand-wringing over ChatGPT’s
    ability to turn all of our children into mindless, plagiarizing cheats has been
    on the top of many educators’ minds and has led some school districts to ban the
    use of ChatGPT¹⁴.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 精心制作的ChatGPT文章的来源难以追踪自然引发了关于剽窃的讨论。但这是否*真的*是个问题？作者认为不是。在ChatGPT出现之前，学生们已经可以利用写作服务¹³，且一直有少数学生决心作弊。但对ChatGPT使所有孩子变成无脑的剽窃者的担忧已成为许多教育者的关注重点，并导致一些学区禁止使用ChatGPT¹⁴。
- en: Conversations on the possibility of plagiarism detract from the larger and more
    important ethical issues related to LLMs. Given that there has been so much buzz
    on this topic, I’d be remiss to not mention it.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 关于剽窃的讨论掩盖了与LLMs相关的更大、更重要的伦理问题。鉴于这一话题的广泛关注，我不得不提及它。
- en: 'Privacy:'
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐私：
- en: Large language models are at risk for data privacy breaches if they are used
    to handle sensitive data. Training sets are drawn from a range of data, at times
    including personally identifiable information¹⁵ — names, email addresses¹⁶, phone
    numbers, addresses, medical information — and therefore, may be in the model’s
    output. While this is an issue with any model trained on sensitive data, given
    how large training sets are for LLMs, this problem could impact many people.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果大型语言模型用于处理敏感数据，则面临数据隐私泄露的风险。训练集来源于各种数据，有时包括个人身份信息¹⁵ — 姓名、电子邮件地址¹⁶、电话号码、地址、医疗信息
    — 因此，可能会出现在模型的输出中。虽然这是任何使用敏感数据训练的模型都会面临的问题，但考虑到LLMs的训练集规模庞大，这个问题可能影响到很多人。
- en: 'Baked in Bias:'
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 潜在偏见：
- en: As previously mentioned, these models are trained on huge corpuses of data.
    When data training sets are so large, they become very difficult to audit and
    are therefore inherently risky⁵. This [data contains societal and historical biases](/understanding-bias-and-fairness-in-ai-systems-6f7fbfe267f3)¹⁷
    and thus any model trained on it is likely to reproduce these biases if safeguards
    are not put in place. Many popular language models were found to contain biases
    which can result in increases in the dissemination of prejudiced ideas and perpetuate
    harm against certain groups. GPT-3 has been shown to exhibit common gender stereotypes¹⁸,
    associating women with family and appearance and describing them as less powerful
    than male characters. Sadly, it also associates Muslims with violence¹⁹, where
    two-thirds of responses to a prompt containing the word “Muslim” contained references
    to violence. It is likely that even more biased associations exist and have yet
    to be uncovered.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这些模型在大量的数据语料库上进行训练。当数据训练集如此庞大时，它们变得非常难以审计，因此固有地存在风险⁵。这个[数据包含社会和历史偏见](/understanding-bias-and-fairness-in-ai-systems-6f7fbfe267f3)¹⁷，因此任何在这些数据上训练的模型都可能会再现这些偏见，除非采取适当的保护措施。许多流行的语言模型被发现含有偏见，这可能导致偏见思想的传播增加，并对某些群体造成持续伤害。GPT-3
    被发现表现出常见的性别刻板印象¹⁸，将女性与家庭和外貌联系起来，并将她们描述为比男性角色更没有权力。令人遗憾的是，它还将穆斯林与暴力联系在一起¹⁹，其中三分之二对包含“穆斯林”一词的提示的回应中都包含了暴力的参考。很可能存在更多的偏见关联尚未被发现。
- en: Notably, Microsoft’s chatbot quickly became a parrot of the worst internet trolls
    in 2016²⁰, spewing racist, sexist, and other abusive language. While ChatGPT has
    a filter to attempt to avoid the worst of this kind of language, it may not be
    foolproof. OpenAI pays for human labelers to flag the most abusive and disturbing
    pieces of data, but the company they contract with has faced criticism for only
    paying their workers $2 per hour and the workers report suffering from deep psychological
    harm²¹.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，微软的聊天机器人在 2016 年迅速变成了最糟糕的网络恶搞者的模仿者²⁰，吐露种族主义、性别歧视和其他辱骂性语言。尽管 ChatGPT 设有过滤器以尝试避免最糟糕的此类语言，但它可能并非万无一失。OpenAI
    为人工标注员支付费用，以标记最具攻击性和令人不安的数据，但其合作公司因每小时仅支付 2 美元而受到批评，工人报告称遭受了深刻的心理伤害²¹。
- en: 'Model Robustness and Security:'
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型的鲁棒性和安全性：
- en: Since LLMs come pre-trained and are subsequently fine tuned to specific tasks,
    they create a number of issues and security risks. Notably, LLMs lack the ability
    to provide uncertainty estimates²². Without knowing the degree of confidence (or
    uncertainty) of the model, it’s difficult for us to decide when to trust the model’s
    output and when to take it with a grain of salt²³. This affects their ability
    to perform well when fine-tuned to new tasks and to avoid overfitting. Interpretable
    uncertainty estimates have the potential to improve the robustness of model predictions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大语言模型（LLMs）是预先训练的，并随后根据特定任务进行微调，这会导致一系列问题和安全风险。值得注意的是，LLMs 缺乏提供不确定性估计的能力²²。没有了解模型的置信度（或不确定性），我们很难判断何时可以信任模型的输出，何时需要保持怀疑态度²³。这影响了它们在微调到新任务时的表现以及避免过拟合的能力。可解释的不确定性估计有潜力提高模型预测的鲁棒性。
- en: Model security is a looming issue due to an LLM’s parent model’s generality
    before the fine tuning step. Subsequently, a model may become a single point of
    failure and a prime target for attacks that will affect any applications derived
    from the model of origin. Additionally, with the lack of supervised training,
    LLMs can be vulnerable to data poisoning²⁵ which could lead to the injection of
    hateful speech to target a specific company, group or individual.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 模型安全性是一个迫在眉睫的问题，原因在于 LLM 的父模型在微调步骤之前的普遍性。因此，模型可能成为单点故障和攻击的主要目标，这会影响从原始模型派生的任何应用。此外，由于缺乏监督训练，LLMs
    可能会受到数据投毒的威胁²⁵，这可能导致恶意言论被注入以针对特定公司、群体或个人。
- en: LLM’s training corpuses are created by crawling the internet for a variety of
    language and subject sources, however they are only a reflection of the people
    who are most likely to have access and frequently use the internet. Therefore,
    AI-generated language is homogenized and often reflects the practices of the wealthiest
    communities and countries⁶. LLMs applied to languages not in the training data
    are more likely to fail and more research is needed on addressing issues around
    out-of-distribution data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 的训练语料库是通过爬取互联网上各种语言和主题来源创建的，然而它们仅仅反映了最有可能接触并频繁使用互联网的人群。因此，AI 生成的语言趋于同质化，并且通常反映了最富有的社区和国家的实践⁶。对于不在训练数据中的语言，LLMs
    更容易失败，需要更多的研究来解决与分布外数据相关的问题。
- en: 'Environmental Impact and Sustainability:'
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 环境影响与可持续性：
- en: A 2019 paper by Strubell and collaborators outlined the enormous carbon footprint
    of the training lifecycle of an LLM²⁴ ²⁶, where training a neural architecture
    search based model with 213 million parameters was estimated to produce more than
    five times the lifetime carbon emissions from the average car. Remembering that
    GPT-3 has 175 *billion* parameters, and the next generation GPT-4 is rumored to
    have 100 *trillion* parameters, this is an important aspect in a world that is
    facing the increasing horrors and devastation of a changing climate.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一篇由 Strubell 和合作者于 2019 年发表的论文概述了 LLM 训练生命周期的巨大碳足迹²⁴ ²⁶，其中，训练一个拥有 2.13 亿参数的神经架构搜索模型被估算产生的碳排放量是普通汽车生命周期排放量的五倍以上。考虑到
    GPT-3 拥有 175 *亿* 个参数，而下一代 GPT-4 传闻拥有 100 *万亿* 个参数，这在面临气候变化带来的日益严重的恐怖和破坏的世界中是一个重要的方面。
- en: What Now?
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现在怎么办？
- en: Any new technology will bring advantages and disadvantages. I have given an
    overview of many of the issues related to LLMs, but I want to stress that I am
    also excited by the new possibilities and the promise these models hold for each
    of us. It is society’s responsibility to put in the proper safeguards and use
    this new tech wisely. Any model used on the public or let into the public domain
    needs to be monitored, explained, and regularly audited for model bias. In part
    2, I will outline recommendations for AI/ML practitioners, enterprises, and government
    agencies on how to address some of the issues particular to LLMs.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 任何新技术都会带来优势和劣势。我已经概述了许多与大语言模型（LLMs）相关的问题，但我想强调的是，我也对这些模型为我们每个人带来的新可能性和承诺感到兴奋。社会有责任采取适当的保障措施，并明智地使用这项新技术。任何在公众中使用或公开的模型都需要进行监控、解释，并定期审计模型的偏差。在第2部分中，我将概述对
    AI/ML 从业者、企业和政府机构的建议，说明如何解决特定于 LLMs 的一些问题。
- en: '**References:**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献：**'
- en: '[ChatGPT Creator Is Talking to Investors About Selling Shares at $29 Billion
    Valuation](https://www.wsj.com/articles/chatgpt-creator-openai-is-in-talks-for-tender-offer-that-would-value-it-at-29-billion-11672949279),
    WSJ, 2023.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ChatGPT 创始人与投资者谈论以 290 亿美元估值出售股份](https://www.wsj.com/articles/chatgpt-creator-openai-is-in-talks-for-tender-offer-that-would-value-it-at-29-billion-11672949279)，华尔街日报，2023年。'
- en: Todd Bishop, [Microsoft looks to tighten ties with OpenAI through potential
    $10B investment and new integrations](https://www.geekwire.com/2023/microsoft-looks-to-tighten-ties-with-openai-through-potential-10b-investment-and-new-integrations/),
    GeekWire, 2023.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Todd Bishop, [微软计划通过潜在的 100 亿美元投资和新的整合来巩固与 OpenAI 的关系](https://www.geekwire.com/2023/microsoft-looks-to-tighten-ties-with-openai-through-potential-10b-investment-and-new-integrations/)，GeekWire，2023年。
- en: Parmy Olson, [ChatGPT Should Worry Google and Alphabet. Why Search When You
    Can Ask AI?](https://www.bloomberg.com/opinion/articles/2022-12-07/chatgpt-should-worry-google-and-alphabet-why-search-when-you-can-ask-ai),
    Bloomberg, 2022.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Parmy Olson, [ChatGPT 应该让 Google 和 Alphabet 感到担忧。为什么要搜索，当你可以问 AI？](https://www.bloomberg.com/opinion/articles/2022-12-07/chatgpt-should-worry-google-and-alphabet-why-search-when-you-can-ask-ai)，彭博社，2022年。
- en: Subbarao Kambhampati, [Changing the Nature of AI Research](https://cacm.acm.org/magazines/2022/9/263799-changing-the-nature-of-ai-research/fulltext),
    Communications of the ACM, 2022.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Subbarao Kambhampati, [改变人工智能研究的性质](https://cacm.acm.org/magazines/2022/9/263799-changing-the-nature-of-ai-research/fulltext)，ACM
    通信，2022年。
- en: Roger Montti, [What Is Google LaMDA & Why Did Someone Believe It’s Sentient?](https://www.searchenginejournal.com/google-lamda-sentient/454820/),
    Search Engine Journal, 2022.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Roger Montti, [什么是 Google LaMDA，为什么有人相信它具有意识？](https://www.searchenginejournal.com/google-lamda-sentient/454820/)，搜索引擎期刊，2022年。
- en: 'Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, Shmargaret Shmitchell,
    [On the Dangers of Stochastic Parrots: Can Language Models be Too Big?](https://dl.acm.org/doi/10.1145/3442188.3445922)
    FAccT 2021.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 艾米莉·M·本德，蒂姆尼特·戈布鲁，安吉丽娜·麦克米伦-梅杰，施玛尔格·施密切，[关于随机鹦鹉的危险：语言模型会不会过大？](https://dl.acm.org/doi/10.1145/3442188.3445922)
    FAccT 2021。
- en: Timnit Gebru, [Effective Altruism Is Pushing a Dangerous Brand of ‘AI Safety’](https://www.wired.com/story/effective-altruism-artificial-intelligence-sam-bankman-fried/),
    Wired, 2022.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 蒂姆尼特·戈布鲁，[有效利他主义推动一种危险的“AI安全”品牌](https://www.wired.com/story/effective-altruism-artificial-intelligence-sam-bankman-fried/)，Wired，2022年。
- en: Krishna Gade, [https://www.linkedin.com/feed/update/urn:li:activity:7005573991251804160/](https://www.linkedin.com/feed/update/urn:li:activity:7005573991251804160/)
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 克里希纳·加德，[https://www.linkedin.com/feed/update/urn:li:activity:7005573991251804160/](https://www.linkedin.com/feed/update/urn:li:activity:7005573991251804160/)
- en: Chirag Shah, Emily Bender, [Situating Search](https://dl.acm.org/doi/abs/10.1145/3498366.3505816),
    CHIIR 2022.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 奇拉格·沙，艾米莉·本德，[情境化搜索](https://dl.acm.org/doi/abs/10.1145/3498366.3505816)，CHIIR
    2022。
- en: '[Why posting GPT and ChatGPT generated answers is not currently acceptable](https://stackoverflow.com/help/gpt-policy),
    Stack Overflow, 2022.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[为什么发布GPT和ChatGPT生成的回答目前不可接受](https://stackoverflow.com/help/gpt-policy)，Stack
    Overflow，2022年。'
- en: '[https://openai.com/blog/new-and-improved-embedding-model/](https://openai.com/blog/new-and-improved-embedding-model/)'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://openai.com/blog/new-and-improved-embedding-model/](https://openai.com/blog/new-and-improved-embedding-model/)'
- en: Kyle Wiggers, [OpenAI’s attempts to watermark AI text hit limits](https://techcrunch.com/2022/12/10/openais-attempts-to-watermark-ai-text-hit-limits/),
    TechCrunch, 2022.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 凯尔·维格斯，[OpenAI尝试给AI文本加水印遇到的限制](https://techcrunch.com/2022/12/10/openais-attempts-to-watermark-ai-text-hit-limits/)，TechCrunch，2022年。
- en: '[7 Best College Essay Writing Services: Reviews & Rankings](https://www.globenewswire.com/news-release/2021/03/27/2200368/0/en/7-Best-College-Essay-Writing-Services-Reviews-Rankings.html),
    GlobeNewswire, 2021.'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7个最佳大学论文写作服务：评论和排名](https://www.globenewswire.com/news-release/2021/03/27/2200368/0/en/7-Best-College-Essay-Writing-Services-Reviews-Rankings.html)，GlobeNewswire，2021年。'
- en: Kalhan Rosenblatt, [ChatGPT banned from New York City public schools’ devices
    and networks](https://www.nbcnews.com/tech/tech-news/new-york-city-public-schools-ban-chatgpt-devices-networks-rcna64446),
    NBC News, 2023.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 卡尔汉·罗森布拉特，[ChatGPT被禁止在纽约市公立学校的设备和网络上使用](https://www.nbcnews.com/tech/tech-news/new-york-city-public-schools-ban-chatgpt-devices-networks-rcna64446)，NBC新闻，2023年。
- en: Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss,
    Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea,
    Colin Raffel, [Extracting Training Data from Large Language Models](https://arxiv.org/abs/2012.07805),
    USENIX Security Symposium, 2021.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尼古拉斯·卡尔尼，弗洛里安·特拉默，埃里克·沃勒斯，马修·贾吉尔斯基，阿里尔·赫伯特-沃斯，凯瑟琳·李，亚当·罗伯茨，汤姆·布朗，道恩·宋，乌尔法尔·厄尔林森，阿丽娜·奥普雷亚，科林·拉费尔，[从大型语言模型中提取训练数据](https://arxiv.org/abs/2012.07805)，USENIX安全研讨会，2021年。
- en: Martin Anderson, [Retrieving Real-World Email Addresses From Pretrained Natural
    Language Models](https://www.unite.ai/retrieving-real-world-email-addresses-from-pretrained-natural-language-models/),
    Unite.AI, 2022.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 马丁·安德森，[从预训练自然语言模型中检索现实世界的电子邮件地址](https://www.unite.ai/retrieving-real-world-email-addresses-from-pretrained-natural-language-models/)，Unite.AI，2022年。
- en: Mary Reagan, [Understanding Bias and Fairness in AI Systems](https://www.fiddler.ai/blog/ai-explained-understanding-bias-and-fairness-in-ai-systems),
    Fiddler AI Blog, 2021.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 玛丽·里根，[理解AI系统中的偏见和公平性](https://www.fiddler.ai/blog/ai-explained-understanding-bias-and-fairness-in-ai-systems)，Fiddler
    AI博客，2021年。
- en: Li Lucy, David Bamman, [Gender and Representation Bias in GPT-3 Generated Stories](https://aclanthology.org/2021.nuse-1.5.pdf),
    ACL Workshop on Narrative Understanding, 2021.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 李·露西，戴维·班曼，[GPT-3生成故事中的性别和表现偏见](https://aclanthology.org/2021.nuse-1.5.pdf)，ACL叙事理解研讨会，2021年。
- en: Andrew Myers, [Rooting Out Anti-Muslim Bias in Popular Language Model GPT-3](https://hai.stanford.edu/news/rooting-out-anti-muslim-bias-popular-language-model-gpt-3),
    Stanford HAI News, 2021.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安德鲁·迈尔斯，[剔除流行语言模型GPT-3中的反穆斯林偏见](https://hai.stanford.edu/news/rooting-out-anti-muslim-bias-popular-language-model-gpt-3)，斯坦福HAI新闻，2021年。
- en: James Vincent, [Twitter taught Microsoft’s AI chatbot to be a racist asshole
    in less than a day](https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist),
    The Verge, 2016.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 詹姆斯·文森特，[推特让微软的AI聊天机器人在不到一天的时间里变成了一个种族主义者](https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist)，The
    Verge，2016年。
- en: Billy Perrigo, [OpenAI Used Kenyan Workers on Less Than $2 Per Hour to Make
    ChatGPT Less Toxic](https://time.com/6247678/openai-chatgpt-kenya-workers/), Time,
    2023.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比利·佩里戈，[OpenAI在肯尼亚以每小时不到2美元的工资雇佣工人来减少ChatGPT的毒性](https://time.com/6247678/openai-chatgpt-kenya-workers/)，《时代》杂志，2023年。
- en: 'Karthik Abinav Sankararaman, Sinong Wang, Han Fang, [BayesFormer: Transformer
    with Uncertainty Estimation](https://arxiv.org/abs/2206.00826), arxiv, 2022.'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Karthik Abinav Sankararaman, Sinong Wang, Han Fang, [BayesFormer：具有不确定性估计的 Transformer](https://arxiv.org/abs/2206.00826)，arxiv，2022年。
- en: Andrew Ng, [ChatGPT Mania!, Crypto Fiasco Defunds AI Safety, Alexa Tells Bedtime
    Stories](https://www.deeplearning.ai/the-batch/issue-174/), The Batch — Deeplearning.ai
    newsletter, 2022.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Andrew Ng, [ChatGPT 疯狂！加密混乱削减了AI安全资金，Alexa 讲睡前故事](https://www.deeplearning.ai/the-batch/issue-174/)，《The
    Batch — Deeplearning.ai 通讯》，2022年。
- en: Emma Strubell, Ananya Ganesh, Andrew McCallum, [Energy and Policy Considerations
    for Deep Learning in NLP](https://arxiv.org/abs/1906.02243), ACL 2019.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Emma Strubell, Ananya Ganesh, Andrew McCallum, [深度学习在 NLP 中的能源和政策考虑](https://arxiv.org/abs/1906.02243)，ACL
    2019年。
- en: Eric Wallace, Tony Z. Zhao, Shi Feng, Sameer Singh, [Concealed Data Poisoning
    Attacks on NLP Models](https://arxiv.org/abs/2010.12563), NAACL 2021.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Eric Wallace, Tony Z. Zhao, Shi Feng, Sameer Singh, [对 NLP 模型的隐蔽数据污染攻击](https://arxiv.org/abs/2010.12563)，NAACL
    2021年。
- en: Karen Hao, [Training a single AI model can emit as much carbon as five cars
    in their lifetimes](https://www.technologyreview.com/2019/06/06/239031/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes/),
    MIT Technology Review, 2019.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Karen Hao, [训练单一AI模型可能排放相当于五辆汽车使用寿命的碳](https://www.technologyreview.com/2019/06/06/239031/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes/)，《麻省理工学院技术评论》，2019年。
