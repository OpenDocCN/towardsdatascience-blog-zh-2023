- en: 'Segment Anything: Promptable Segmentation of Arbitrary Objects'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Segment Anything: Promptable Segmentation of Arbitrary Objects'
- en: 原文：[https://towardsdatascience.com/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?source=collection_archive---------1-----------------------#2023-09-14](https://towardsdatascience.com/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?source=collection_archive---------1-----------------------#2023-09-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?source=collection_archive---------1-----------------------#2023-09-14](https://towardsdatascience.com/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?source=collection_archive---------1-----------------------#2023-09-14)
- en: '[🚀Sascha’s Paper Club](https://towardsdatascience.com/tagged/saschas-paper-club)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[🚀Sascha’s Paper Club](https://towardsdatascience.com/tagged/saschas-paper-club)'
- en: Segment Anything by A. Krillov et. al.
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**Segment Anything** 由A. Krillov等人撰写。'
- en: '[](https://medium.com/@SaschaKirch?source=post_page-----f28958c5612d--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----f28958c5612d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f28958c5612d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f28958c5612d--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----f28958c5612d--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@SaschaKirch?source=post_page-----f28958c5612d--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----f28958c5612d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f28958c5612d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f28958c5612d--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----f28958c5612d--------------------------------)'
- en: ·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5c38dace9d5e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsegment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d&user=Sascha+Kirch&userId=5c38dace9d5e&source=post_page-5c38dace9d5e----f28958c5612d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f28958c5612d--------------------------------)
    ·12 min read·Sep 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff28958c5612d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsegment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d&user=Sascha+Kirch&userId=5c38dace9d5e&source=-----f28958c5612d---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5c38dace9d5e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsegment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d&user=Sascha+Kirch&userId=5c38dace9d5e&source=post_page-5c38dace9d5e----f28958c5612d---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f28958c5612d--------------------------------)
    ·12 min read·2023年9月14日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff28958c5612d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsegment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d&user=Sascha+Kirch&userId=5c38dace9d5e&source=-----f28958c5612d---------------------clap_footer-----------)'
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff28958c5612d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsegment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d&source=-----f28958c5612d---------------------bookmark_footer-----------)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff28958c5612d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsegment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d&source=-----f28958c5612d---------------------bookmark_footer-----------)'
- en: Today’s paper walkthrough it is going to be visual! We will analyze S*egment
    Anything*, a paper by Meta’s AI research team that made headlines not only in
    the research community but also by all sorts of deep learning practitioners and
    advocates.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的论文讲解将会非常直观！我们将深入分析**Segment Anything**，这是Meta AI研究团队的一篇论文，该论文不仅在研究界引起了轰动，也引起了各类深度学习从业者和支持者的关注。
- en: Segment Anything introduces the task of promptable segmentation, it introduces
    the segment anything model (SAM), and it details the generation of a new publicly
    available dataset of 11 million images containing more than 1 billion masks. SAM
    has been widely adopted by the community and resulted in some new state-of-the-art
    foundation models such as [Grounded-SAM](https://github.com/IDEA-Research/Grounded-Segment-Anything)
    that combines [Grounding DINO](https://arxiv.org/abs/2303.05499) with SAM.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything 引入了可提示分割的任务，介绍了 Segment Anything 模型（SAM），并详细描述了生成一个包含超过10亿个掩膜的1100万张图像的新公开数据集。SAM
    已被社区广泛采用，并产生了一些新的最先进的基础模型，如 [Grounded-SAM](https://github.com/IDEA-Research/Grounded-Segment-Anything)，它将
    [Grounding DINO](https://arxiv.org/abs/2303.05499) 与 SAM 结合。
- en: '![](../Images/3655fd5fedea80f78ac89299c8bd7b30.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3655fd5fedea80f78ac89299c8bd7b30.png)'
- en: Image created from [publication](https://arxiv.org/abs/2304.02643) by [Sascha
    Kirch](https://medium.com/@SaschaKirch)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [出版物](https://arxiv.org/abs/2304.02643) 由 [Sascha Kirch](https://medium.com/@SaschaKirch)
    创作
- en: '**Paper:** [Segment Anything](https://arxiv.org/abs/2304.02643) by [Alexander
    Kirillov](https://arxiv.org/search/cs?searchtype=author&query=Kirillov%2C+A) et.
    al., 5 Apr. 2023'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**论文:** [Segment Anything](https://arxiv.org/abs/2304.02643) 由 [Alexander Kirillov](https://arxiv.org/search/cs?searchtype=author&query=Kirillov%2C+A)
    等人，2023年4月5日'
- en: ''
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Resources:** [GitHub](https://github.com/facebookresearch/segment-anything)
    — [Demo](https://segment-anything.com/demo) — [Project Page](https://segment-anything.com/)
    — [Dataset](https://segment-anything.com/dataset/index.html) — [HuggingFace](https://huggingface.co/docs/transformers/main/model_doc/sam)'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**资源:** [GitHub](https://github.com/facebookresearch/segment-anything) — [演示](https://segment-anything.com/demo)
    — [项目页面](https://segment-anything.com/) — [数据集](https://segment-anything.com/dataset/index.html)
    — [HuggingFace](https://huggingface.co/docs/transformers/main/model_doc/sam)'
- en: ''
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Category:** segmentation, zero-shot prediction, computer vison, prompting,
    large-scale'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**类别:** 分割，零样本预测，计算机视觉，提示，大规模'
- en: ''
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[**Other Walkthroughs**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**:**'
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[**其他讲解**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**:**'
- en: '[[BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)]
    — [[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)]
    — [[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)]
    — [[Depth Anything](https://medium.com/towards-data-science/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?sk=fc6197edd68e6137c3396c83e50f65cb)]
    — [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    — [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[[BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)]
    — [[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)]
    — [[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)]
    — [[Depth Anything](https://medium.com/towards-data-science/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?sk=fc6197edd68e6137c3396c83e50f65cb)]
    — [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    — [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
- en: Outline
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大纲
- en: Context & Background
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 背景与上下文
- en: SAM — Segment Anything Model
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SAM — Segment Anything 模型
