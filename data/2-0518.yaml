- en: Co-operative Graph Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 合作图神经网络
- en: 原文：[https://towardsdatascience.com/co-operative-graph-neural-networks-34c59bf6805e](https://towardsdatascience.com/co-operative-graph-neural-networks-34c59bf6805e)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/co-operative-graph-neural-networks-34c59bf6805e](https://towardsdatascience.com/co-operative-graph-neural-networks-34c59bf6805e)
- en: New GNN architecture
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 新的 GNN 架构
- en: '***A large majority of Graph Neural Networks (GNNs) follow the message-passing
    paradigm, where node states are updated based on aggregated neighbour messages.
    In this post, we describe Co-operative GNNs (Co-GNNs), a new type of message-passing
    architecture where every node is viewed as a player that can choose to either
    ‘listen’, ‘broadcast’, ‘listen & broadcast’, or to ‘isolate.’ Standard message
    passing is a special case where every node ‘listens & broadcasts’ to all neighbours.
    We show that Co-GNNs are asynchronous, more expressive, and can address common
    plights of standard message-passing GNNs such as over-squashing and over-smoothing.***'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '***绝大多数图神经网络（GNNs）遵循信息传递范式，其中节点状态基于聚合的邻居消息进行更新。在本文中，我们描述了合作图神经网络（Co-GNNs），这是一种新的信息传递架构，其中每个节点被视为一个玩家，可以选择‘监听’、‘广播’、‘监听
    & 广播’或‘隔离’。标准的信息传递是一种特殊情况，其中每个节点‘监听 & 广播’所有邻居。我们展示了 Co-GNNs 是异步的、更具表现力的，并且可以解决标准信息传递
    GNNs 的常见问题，如过度压缩和过度平滑。***'
- en: '[](https://michael-bronstein.medium.com/?source=post_page-----34c59bf6805e--------------------------------)[![Michael
    Bronstein](../Images/1aa876fce70bb07bef159fecb74e85bf.png)](https://michael-bronstein.medium.com/?source=post_page-----34c59bf6805e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----34c59bf6805e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----34c59bf6805e--------------------------------)
    [Michael Bronstein](https://michael-bronstein.medium.com/?source=post_page-----34c59bf6805e--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://michael-bronstein.medium.com/?source=post_page-----34c59bf6805e--------------------------------)[![Michael
    Bronstein](../Images/1aa876fce70bb07bef159fecb74e85bf.png)](https://michael-bronstein.medium.com/?source=post_page-----34c59bf6805e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----34c59bf6805e--------------------------------)[![数据科学之路](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----34c59bf6805e--------------------------------)
    [Michael Bronstein](https://michael-bronstein.medium.com/?source=post_page-----34c59bf6805e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----34c59bf6805e--------------------------------)
    ·11 min read·Dec 6, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [数据科学之路](https://towardsdatascience.com/?source=post_page-----34c59bf6805e--------------------------------)
    ·阅读时长 11 分钟·2023 年 12 月 6 日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a2870ac967748470424f265c9e2a651e.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2870ac967748470424f265c9e2a651e.png)'
- en: '*Illustration of node actions in Co-GNNs: standard, listen, broadcast, and
    isolate. Image credit: DALL-E 3.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*Co-GNNs 中节点操作的示意图：标准、监听、广播和隔离。图片来源：DALL-E 3。*'
- en: This post was co-authored with Ben Finkelshtein, Ismail Ceylan, and Xingyue
    Huang and is based on the paper B. Finkelshtein et al., [Cooperative Graph Neural
    Networks](https://arxiv.org/abs/2310.01267) (2023) arXiv:2310.01267.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本文由 Ben Finkelshtein、Ismail Ceylan 和 Xingyue Huang 共同撰写，基于论文 B. Finkelshtein
    等人，[合作图神经网络](https://arxiv.org/abs/2310.01267)（2023）arXiv:2310.01267。
- en: Graph Neural Networks (GNNs) are a popular class of architectures used for learning
    on graph-structured data such as molecules, biological interactomes, and social
    networks. The majority of GNNs follow the message-passing paradigm [1], where
    at every layer graph nodes exchange information along the edges of the graph.
    The state of every node is updated using a permutation-invariant aggregation operation
    (typically, a sum or a mean) on the messages sent from adjacent nodes [2].
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络（GNNs）是一类用于在图结构数据上进行学习的流行架构，例如分子、生物相互作用组和社交网络。大多数 GNN 遵循信息传递范式 [1]，在每一层中，图节点沿图的边缘交换信息。每个节点的状态通过对来自相邻节点的消息进行排列不变的聚合操作（通常是求和或取均值）来更新
    [2]。
- en: While the message-passing paradigm has been very influential in graph ML, it
    has well-known theoretical and practical limitations. The formal equivalence of
    message-passing graph neural networks (MPNNs) to graph isomorphism tests [3] provides
    a theoretical upper bound on their expressive power. As a result, distinguishing
    between even very simple non-isomorphic graphs (such as a 6-cycle and two triangles
    in the example below) is impossible by means of message passing without additional
    information such as positional or structural encoding [4] or more complex information
    propagation mechanisms [5].
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然消息传递范式在图 ML 中具有很大影响力，但它有着公认的理论和实际限制。消息传递图神经网络（MPNN）与图同构测试的形式等价 [3] 为它们的表达能力提供了理论上的上限。因此，即使是非常简单的非同构图（例如下面的
    6-圈和两个三角形）也无法通过消息传递区分，除非有额外的信息，如位置编码或结构编码 [4]，或更复杂的信息传播机制 [5]。
- en: '![](../Images/373c33892be7725f428431c22828700c.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/373c33892be7725f428431c22828700c.png)'
- en: '*Example of two non-isomorphic graphs indistinguishable by 1-WL (and consequently,
    by message-passing) without additional information.*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*两个非同构图的示例，通过 1-WL（因此，通过消息传递）在没有额外信息的情况下无法区分。*'
- en: '**Message Passing & Information bottlenecks**'
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**消息传递与信息瓶颈**'
- en: A practical limitation of message passing is related to the information flow
    on a graph. In order to receive information from *k*-hop neighbours, an MPNN needs
    at least *k* layers, which often leads to an exponential growth of a node’s receptive
    field. The growing amount of information must then be compressed into fixed-sized
    node embeddings, possibly leading to information loss, referred to as *over-squashing*
    [6].
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 消息传递的一个实际限制与图上的信息流相关。为了从 *k* 跳邻居处接收信息，一个 MPNN 需要至少 *k* 层，这通常导致节点接收域的指数增长。增长的信息量必须被压缩为固定大小的节点嵌入，这可能导致信息丢失，称为
    *过度压缩* [6]。
- en: We have recently shown that whether over-squashing occurs in an MPNN depends
    on the task, architectural choices (e.g., the number of layers), and the graph
    [7]. Making the graph “friendlier” for message passing is a common technique of
    coping with over-squashing known in graph ML literature under the generic name
    *“graph rewiring.”*
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最近表明，MPNN 中是否发生过度压缩取决于任务、架构选择（例如层数）和图 [7]。使图“更友好”以应对过度压缩是图 ML 文献中一种常见的技术，通用名称为
    *“图重连线。”*
- en: The fact that every node both *sends* and *receives* information from its neighbours
    is another limitation of classical message passing. Mechanisms such as dynamic
    graph rewiring [8], attention [9], or gating [10] allow to modify the node’s neighbourhood
    or downweight neighbour messages.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点既 *发送* 又 *接收* 来自其邻居的信息是经典消息传递的另一个限制。动态图重连线 [8]、注意力 [9] 或门控 [10] 等机制允许修改节点的邻域或降低邻居消息的权重。
- en: '**Co-operative Graph Neural Networks**'
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**合作图神经网络**'
- en: 'In a recent paper [11], we propose a learnable generalisation of message passing
    by allowing each node to decide how to propagate information from or to its neighbours,
    thus enabling a more flexible flow of information. We regard the nodes as players
    that can each take the following action in each layer:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在最近的一篇论文 [11] 中，我们提出了一种可学习的消息传递推广方法，允许每个节点决定如何从或向邻居传播信息，从而实现更灵活的信息流动。我们将节点视为可以在每一层采取以下动作的参与者：
- en: '**STANDARD:** Broadcast to neighbors that listen *and* listen to neighbors
    that broadcast. Having all nodes choose this action yields the classical message-passing
    scheme.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**标准：** 广播给那些监听的邻居 *和* 监听广播的邻居。所有节点选择此动作将产生经典的消息传递方案。'
- en: '**LISTEN:** Listen to neighbors that broadcast.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**监听：** 监听广播的邻居。'
- en: '**BROADCAST:** Broadcast to neighbors that listen.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**广播：** 广播给监听的邻居。'
- en: '**ISOLATE:** Neither listen nor broadcast. With all nodes isolating, no information
    is propagated on the graph and predictions are made in a node-wise manner, similar
    to DeepSets [12].'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**隔离：** 既不监听也不广播。当所有节点都隔离时，图上不会传播任何信息，预测以节点为单位进行，类似于 DeepSets [12]。'
- en: '![](../Images/23235dc00c77bb64c839f05888858f2f.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23235dc00c77bb64c839f05888858f2f.png)'
- en: 'Examples of node actions on a graph: all nodes broadcast & listen (standard
    message passing denoted by “S”), all nodes isolating (node-wise prediction, denoted
    by “I”), and a general case with mixed actions (listen “L”, broadcast “B”, listen
    & broadcast “S”, and isolate “I”).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图上的节点操作示例：所有节点广播和听（标准消息传递用“S”表示），所有节点隔离（节点预测，用“I”表示），以及混合操作的通用案例（听“L”，广播“B”，听和广播“S”，隔离“I”）。
- en: The interplay between these actions and the ability to change them locally and
    dynamically makes the overall approach richer than standard message passing. It
    allows us to decouple the input graph from the computational in a way that is
    learnable (non-heuristic) and task-dependant.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作之间的相互作用以及局部和动态改变它们的能力使整体方法比标准消息传递更丰富。它允许我们以可学习（非启发式）和任务依赖的方式将输入图与计算解耦。
- en: To implement this novel message-passing scheme, we introduce a new class of
    GNN architectures called *Co-operative GNNs* (“Co-GNNs”). The main difference
    compared to traditional MPNNs is an additional *“action network”* choosing one
    of the four actions for every node at every layer. The chosen actions influence
    the way node features are then updated by another *environment network,* which
    is trained jointly with the action one [13].
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一新颖的消息传递方案，我们引入了一类新的GNN架构，称为*合作GNN*（“Co-GNN”）。与传统MPNN的主要区别在于，额外的*“行动网络”*为每一层的每个节点选择四个操作之一。所选择的操作会影响节点特征的更新方式，由另一个*环境网络*进行更新，该网络与行动网络共同训练[13]。
- en: Advantages of Co-GNNs
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Co-GNN的优势
- en: '**Task-specific:** Standard message-passing updates nodes based on their local
    neighborhood, which is completely task-agnostic. By allowing each node to listen
    to the information only from relevant neighbours, Co-GNNs can determine a computation
    graph best suited for the target task [14].'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**任务特定的：** 标准消息传递根据节点的局部邻域更新节点，这完全与任务无关。通过允许每个节点只从相关邻居处接收信息，Co-GNN可以确定最适合目标任务的计算图[14]。'
- en: '**Directed:**The outcome of the actions that the nodes can take amounts to
    a special form of “directed rewiring” of the input graph. An edge can be dropped
    (e.g., if two neighbors listen without broadcasting); remain undirected (e.g.,
    if both neighbors apply the standard action of listening and broadcasting); or
    become directed (e.g., if one neighbor listens while its neighbor broadcasts).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**定向的：** 节点所能采取的行动结果相当于对输入图的特殊形式的“定向重连”。一条边可以被删除（例如，如果两个邻居都在听而没有广播）；保持无向（例如，如果两个邻居都执行标准的听和广播操作）；或变成有向（例如，如果一个邻居在听而其邻居在广播）。'
- en: '**Dynamic:** Co-GNNs do not operate on a pre-fixed computational graph, but
    rather on a computational graph learned through the choice of node actions, which
    is dynamic across layers. Each node learns to interact with the relevant neighbours
    and does so only as long as they remain relevant.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**动态的：** Co-GNN不在预先固定的计算图上操作，而是在通过选择节点操作学习到的计算图上操作，这在各层之间是动态的。每个节点学习与相关邻居交互，并且只有在这些邻居仍然相关时才这样做。'
- en: '![](../Images/290902180e6fb5078723411b7594642e.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/290902180e6fb5078723411b7594642e.png)'
- en: Example of the computational graph in a classical MPNN (a special case of our
    architecture where all nodes choose the “STANDARD” action, denoted by black outline)
    and a Co-GNN. This example illustrates the possibility of routing messages from
    source node w directly to target node v.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 经典MPNN中的计算图示例（这是我们架构的一个特殊案例，其中所有节点选择“标准”操作，用黑色轮廓表示）和Co-GNN。这一示例展示了将消息从源节点w直接路由到目标节点v的可能性。
- en: '**Both feature- and structure-based:** Standard message-passing is completely
    determined by the structure of the graph: two nodes with the same neighborhood
    receive the same aggregated message. This is not necessarily the case in Co-GNNs,
    which can learn different actions for two nodes with different node features.
    This allows to pass different messages for different nodes even if their neighborhoods
    are identical.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**既基于特征也基于结构：** 标准消息传递完全由图的结构决定：具有相同邻域的两个节点接收相同的聚合消息。在Co-GNN中情况并非如此，它可以为具有不同节点特征的两个节点学习不同的操作。这允许即使邻域相同，也为不同节点传递不同的消息。'
- en: '**Asynchronous:** Standard message-passing updates all nodes synchronously
    at every iteration, which is not always optimal [15]. By design, Co-GNNs enable
    asynchronous updates across nodes.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**异步的：** 标准消息传递在每次迭代中同步更新所有节点，这并不总是最优的[15]。Co-GNN的设计允许在节点之间进行异步更新。'
- en: '**More expressive than 1-WL:** Fora pair of nodes indistinguishable by 1-WL,
    there is a non-trivial probability of sampling different actions, which in turn
    makes their direct neighborhood differ [16]. This yields unique node identifiers
    with high probability and allows us to distinguish any pair of graphs assuming
    an injective graph pooling function [17].'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**比1-WL更具表现力：** 对于1-WL无法区分的节点对，有非平凡的概率会采样不同的动作，从而使其直接邻域不同[16]。这产生了高概率的唯一节点标识符，并允许我们区分任何一对图，前提是存在注射图池化函数[17]。'
- en: '**Suitable for long-range tasks:** Long-range tasks necessitate to propagate
    information between distant nodes. Our message-passing paradigm can efficiently
    filter irrelevant information by learning to focus on the shortest path connecting
    these two nodes, hence maximising the information flow to the target node [18].'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**适合长程任务：** 长程任务需要在远距离节点之间传播信息。我们的消息传递范式可以通过学习专注于连接这两个节点的最短路径，来有效地过滤无关信息，从而最大化信息流向目标节点[18]。'
- en: '**Can prevent over-squashing:** In our previous works [19–20], we formalised
    over-squashing as the *lack of sensitivity* of the *r*-layer MPNN output at a
    node *u* to the input at a distant node *v*. This can be quantified by a bound
    on the partial derivative (*Jacobian*) of the form'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**可以防止过度压缩：** 在我们之前的工作[19–20]中，我们将过度压缩形式化为*第*r层MPNN输出在节点*u*处对远处节点*v*的输入的*缺乏敏感性*。这可以通过形式为的偏导数（*雅可比矩阵*）的界限来量化'
- en: '|∂**x***ᵤ*⁽*ʳ*⁾/∂**x***ᵥ*⁽⁰⁾| < *c*(**A***ʳ*)*ᵤᵥ,*'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '|∂**x***ᵤ*⁽*ʳ*⁾/∂**x***ᵥ*⁽⁰⁾| < *c*(**A***ʳ*)*ᵤᵥ,*'
- en: where **x***ᵤ*⁽*ʳ*⁾ denotes the features at node *u* at layer *r*, *c* encapsulates
    architecture-related constants (e.g., Lipschitz regularity of the activation function,
    width, etc.), and **A** is the normalised adjacency matrix capturing the effect
    of the graph. Graph rewiring techniques amount to modifying **A** so as to increase
    the upper bound and thereby reduce the effect of over-squashing. In Co-GNNs, the
    actions of every node result in an effective graph rewiring that that transmit
    the features from one node to another (as illustrated in the above example), resulting
    in the maximization of the bound on the Jacobian [21].
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其中**x***ᵤ*⁽*ʳ*⁾表示第*r*层节点*u*处的特征，*c*封装了与架构相关的常数（例如，激活函数的Lipschitz正则性、宽度等），**A**是归一化的邻接矩阵，捕捉图的效果。图重连技术相当于修改**A**以增加上界，从而减少过度压缩的影响。在Co-GNNs中，每个节点的动作导致有效的图重连，传递特征从一个节点到另一个节点（如上例所示），从而最大化雅可比矩阵的界限[21]。
- en: '**Can prevent over-smoothing: “**Over-smoothing” is the tendency of node embeddings
    to become increasingly similar across the graph with the increase in the number
    of message-passing layers. We showed in [10] that over-smoothing can be mitigated
    through the gradient gating mechanism, which adaptively disables the update of
    a node from neighbors with similar features. Co-GNNs, through the choice of BROADCAST
    or ISOLATE actions, allow mimicking this mechanism.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**可以防止过度平滑：** “**过度平滑**”是指节点嵌入随着消息传递层数的增加而在图中变得越来越相似的趋势。我们在[10]中表明，通过梯度门控机制可以缓解过度平滑，该机制自适应地禁用来自具有相似特征的邻居的节点更新。Co-GNNs通过选择BROADCAST或ISOLATE动作来模拟这一机制。'
- en: Experimental results
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验结果
- en: To better understand the effect of the learned computational graph in our new
    message-passing scheme and how it adapts to different tasks, we observe the ratio
    of the directed edges that are kept across different layers of a Co-GNN (a classical
    MPNN performing message passing on the input graph has the ratio of 1).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解我们新的消息传递方案中学习到的计算图的效果以及它如何适应不同任务，我们观察了在Co-GNN不同层之间保留的有向边的比例（一个经典的MPNN在输入图上执行消息传递，其比例为1）。
- en: '![](../Images/598c08e0cf91dbd328d0f149d16a225b.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/598c08e0cf91dbd328d0f149d16a225b.png)'
- en: '*The ratio of directed edges that are kept in a homophilic graph Cora (blue)
    a heterophilic graph Roman-empire (red) in each layer shows different adaptive
    behaviour in different types of graphs.*'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*在同质图Cora（蓝色）和异质图Roman-empire（红色）中，每一层保留的有向边的比例显示了不同类型图中不同的自适应行为。*'
- en: 'We trained a 10-layer Co-GNN on two datasets: homophilic *Cora* and heterophilic
    *Roman-Empire* [22]. We observe *opposite trends* in terms of the evolution of
    the ratio of edges that are kept across layers. On the homophilic dataset, the
    ratio of edges that are kept gradually *decreases* with depth, whereas on the
    heterophilic dataset, it *increases*. The decrease in the ratio of kept edges
    implies that information is propagated among fewer nodes, which we see as a way
    of coping with the phenomenon of oversmoothing, similarly to gradient gating [10].'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两个数据集上训练了一个10层的Co-GNN：同质*Cora*和异质*Roman-Empire* [22]。我们观察到在保留的边的比例演变方面的*相反趋势*。在同质数据集中，保留的边的比例随着深度逐渐*减少*，而在异质数据集中则*增加*。保留的边的比例减少意味着信息在较少的节点之间传播，我们认为这是一种应对过度平滑现象的方法，类似于梯度门控[10]。
- en: On heterophilic datasets [23], which are considered a hard test case for GNNs,
    Co-GNNs achieve state-of-the-art results across the board, despite using relatively
    simple architectures as their action and environment networks, surpassing the
    performance of more complex models such as Graph Transformers. These results are
    reassuring as they establish Co-GNNs as a strong method in the heterophilic setting.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在被认为是GNNs的困难测试案例的异质数据集[23]上，Co-GNNs在各个方面都取得了最先进的结果，尽管它们使用的行动和环境网络架构相对简单，超越了更复杂的模型如Graph
    Transformers。这些结果令人振奋，因为它们确立了Co-GNNs作为异质环境中一种强有力的方法。
- en: '![](../Images/961bb14be374653b691623e684fc278d.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/961bb14be374653b691623e684fc278d.png)'
- en: '*Performance results (accuracy %) on heterophilic node classification. Top
    three models are colored by Red, Blue, Gray, respectively.*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*异质节点分类的性能结果（准确率 %）。前三名模型分别用红色、蓝色、灰色标记。*'
- en: Visualizing the actions
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化行动
- en: We visualize the topology in each layer of Co-GNN using the *Minesweepers* dataset
    [23], a semi-supervised node classification task on a regular 100×100 grid where
    each node is connected to eight neighboring nodes. Each node has an input feature
    of one-hot-encoded representations, showing the number of adjacent mines. A randomly
    chosen 50% subset of the nodes has an unknown feature, indicated by a separate
    binary feature. The task is to correctly identify if a node is a mine.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用*Minesweepers*数据集[23]可视化Co-GNN在每一层的拓扑，该数据集是一个半监督节点分类任务，使用一个规则的100×100网格，其中每个节点连接到八个邻居节点。每个节点有一个热编码的输入特征，显示相邻地雷的数量。随机选择的50%节点具有未知特征，用一个单独的二进制特征表示。任务是正确识别节点是否为地雷。
- en: '![](../Images/187b36d7e584352e26d9efd6b2c41d28.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/187b36d7e584352e26d9efd6b2c41d28.png)'
- en: 'We observe that in the early layers (1–4), the action network learns to isolate
    the right section of the black node, similar to how humans would play this game:
    the bulk of the nodes without neighboring mines (0 labeled nodes) initially do
    not help in determining whether the black node is a mine or not. Thus, the action
    network prioritizes the information flowing from the left sections of the grid
    where more mines are present, thus initially focusing mostly on nodes that are
    more informative for the task.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到在早期层次（1-4），行动网络学习隔离黑色节点的正确部分，这类似于人类玩这个游戏的方式：没有邻近地雷（标记为0的节点）的节点在确定黑色节点是否为地雷时最初没有帮助。因此，行动网络优先处理来自网格左侧区域的信息，那里的地雷更多，因此最初主要关注对任务更具信息性的节点。
- en: After identifying the most crucial information and propagating it through the
    network, it then requires this information to also be communicated with the nodes
    that initially were labeled with 0\. This leads to an almost fully connected grid
    in the deeper layers (7–8).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定最重要的信息并将其传播通过网络之后，这些信息还需要与最初标记为0的节点进行通信。这导致在更深的层次（7-8）中几乎完全连接的网格。
- en: Conclusions
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Our novel message-passing scheme where every node can adaptively choose its
    action and its architectural incarnation in the form of Co-GNNs offers multiple
    advantages and helps overcome known drawbacks of traditional message-passing used
    in the majority of modern GNNs. We believe this is a promising direction from
    both theoretical and practical perspectives.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的新型消息传递方案使每个节点可以自适应地选择其行动及其在Co-GNNs形式下的架构，提供了多种优势，并帮助克服了传统消息传递方法的已知缺陷。我们相信这在理论和实践方面都是一个有前景的方向。
- en: '[1] J. Gilmer et al., [Neural message passing for quantum chemistry](https://arxiv.org/abs/1704.01212)
    (2017) *ICML*.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] J. Gilmer 等，[量子化学的神经消息传递](https://arxiv.org/abs/1704.01212)（2017）*ICML*。'
- en: '[2] This makes the layer permutation-equivariant.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] 这使得层具有置换等变性。'
- en: '[3] K. Xu et al., How powerful are graph neural networks? (2019) *ICLR*, and
    C. Morris et al., Weisfeiler and Leman go neural: Higher-order graph neural networks
    (2019) *AAAI* established the equivalence between message passing and the graph
    isomorphism test described in the classical paper of B. Weisfeiler and A. Lehman,
    The reduction of a graph to canonical form and the algebra which appears therein
    (1968) *Nauchno-Technicheskaya Informatsia* 2(9):12–16\. See our [previous blog
    post](https://medium.com/towards-data-science/expressive-power-of-graph-neural-networks-and-the-weisefeiler-lehman-test-b883db3c7c49)
    on this topic.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] K. Xu 等人，《图神经网络的强大能力？》（2019）*ICLR*，以及 C. Morris 等人，《魏斯费勒和莱曼走向神经网络：高阶图神经网络》（2019）*AAAI*
    证明了消息传递与 B. Weisfeiler 和 A. Lehman 经典论文中描述的图同构测试之间的等价关系，《图的规范形式的化简及其中出现的代数》（1968）*Nauchno-Technicheskaya
    Informatsia* 2(9):12–16。请参见我们关于这一主题的[之前的博客文章](https://medium.com/towards-data-science/expressive-power-of-graph-neural-networks-and-the-weisefeiler-lehman-test-b883db3c7c49)。'
- en: '[4] See our [previous blog post](/beyond-weisfeiler-lehman-using-substructures-for-provably-expressive-graph-neural-networks-d476ad665fa3?sk=bc0d14c28a380b4d51debc4935345b73)
    on structural encoding in GNNs.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] 请参见我们关于 GNNs 中结构编码的[之前的博客文章](/beyond-weisfeiler-lehman-using-substructures-for-provably-expressive-graph-neural-networks-d476ad665fa3?sk=bc0d14c28a380b4d51debc4935345b73)。'
- en: '[5] For a few examples of such constructions, see our previous blog posts on
    [subgraph GNNs](/using-subgraphs-for-more-expressive-gnns-8d06418d5ab?sk=8806ffcd9ecf74c440d40df53528c1c7)
    and [topological message passing](/a-new-computational-fabric-for-graph-neural-networks-280ea7e3ed1a?sk=3d4185067ef3c1cf81793deada08e18f).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] 关于这些构造的几个示例，请参见我们之前的博客文章，[子图 GNNs](/using-subgraphs-for-more-expressive-gnns-8d06418d5ab?sk=8806ffcd9ecf74c440d40df53528c1c7)和[拓扑消息传递](/a-new-computational-fabric-for-graph-neural-networks-280ea7e3ed1a?sk=3d4185067ef3c1cf81793deada08e18f)。'
- en: '[6] U. Alon and E. Yahav, [On the bottleneck of graph neural networks and its
    practical implications](https://openreview.net/pdf?id=i80OPhOCVH2) (2021) *ICML*.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] U. Alon 和 E. Yahav，[图神经网络瓶颈及其实际影响](https://openreview.net/pdf?id=i80OPhOCVH2)（2021）*ICML*。'
- en: '[7] Theoretical analysis of oversquashing from the geometric perspective was
    first done by J. Topping, F. Di Giovanni *et al.*, [Understanding over-squashing
    and bottlenecks on graphs via curvature](https://arxiv.org/pdf/2111.14522.pdf)
    (2022), *ICLR,* and further extended by F. Di Giovanni *et al.*, [On over-squashing
    in Message Passing Neural Networks: The impact of width, depth, and topology](https://arxiv.org/abs/2302.02941)
    (2023), *ICML*. Oversquashing was more recently linked to expressive power by
    F. Di Giovanni *et al.*, [How does over-squashing affect the power of GNNs?](https://arxiv.org/abs/2306.03589)
    (2023), arXiv:2306.03589.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] 从几何角度对过度挤压的理论分析首先由 J. Topping 和 F. Di Giovanni *等人* 完成，[通过曲率理解图上的过度挤压和瓶颈](https://arxiv.org/pdf/2111.14522.pdf)（2022），*ICLR*，并由
    F. Di Giovanni *等人* 进一步扩展，[关于消息传递神经网络中的过度挤压：宽度、深度和拓扑的影响](https://arxiv.org/abs/2302.02941)（2023），*ICML*。最近，过度挤压与表达能力的关系由
    F. Di Giovanni *等人* 链接，[过度挤压如何影响 GNNs 的能力？](https://arxiv.org/abs/2306.03589)（2023），arXiv:2306.03589。'
- en: '[8] Wang et al. [Dynamic graph CNN for learning on point clouds](https://arxiv.org/pdf/1801.07829.pdf)
    (2019) *ACM Trans. Graphics* 38(5):146, and also see our [blog post](/manifold-learning-2-99a25eeb677d?sk=1c855a020f09b72edfa50a8aba5f24a0)
    on latent graph learning.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Wang 等人，[点云上的动态图卷积神经网络](https://arxiv.org/pdf/1801.07829.pdf)（2019）*ACM
    Trans. Graphics* 38(5):146，同时参见我们关于潜在图学习的[博客文章](/manifold-learning-2-99a25eeb677d?sk=1c855a020f09b72edfa50a8aba5f24a0)。'
- en: '[9] P. Veličković et al., [Graph Attention Networks](https://arxiv.org/abs/1710.10903)
    (2018) *ICLR*.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] P. Veličković 等人，[图注意力网络](https://arxiv.org/abs/1710.10903)（2018）*ICLR*。'
- en: '[10] K. Rusch et al., [Gradient Gating for deep multi-rate learning on graphs](https://arxiv.org/pdf/2210.00513.pdf)
    (2023) *ICLR*.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] K. Rusch 等人，[图上的深度多率学习的梯度门控](https://arxiv.org/pdf/2210.00513.pdf)（2023）*ICLR*。'
- en: '[11] B. Finkelshtein et al., [Cooperative Graph Neural Networks](https://arxiv.org/abs/2310.01267)
    (2023) arXiv:2310.01267.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] B. Finkelshtein 等人，[合作图神经网络](https://arxiv.org/abs/2310.01267)（2023）arXiv:2310.01267。'
- en: '[12] M. Zaheer *et al.*, [DeepSets](https://proceedings.neurips.cc/paper_files/paper/2017/file/f22e4747da1aa27e363d86d40ff442fe-Paper.pdf)
    (2017), *NIPS*.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] M. Zaheer *等人*，[DeepSets](https://proceedings.neurips.cc/paper_files/paper/2017/file/f22e4747da1aa27e363d86d40ff442fe-Paper.pdf)（2017），*NIPS*。'
- en: '[13] The action network predicts the action probability for each node (equation
    1 in our paper [11]), from which a node action is sampled using the straight-through
    Gumbel-softmax estimator. Then, the environment network is used to update the
    state of each node in accordance with the sampled actions, according to equation
    1 in our paper [11].'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] 动作网络预测每个节点的动作概率（参见我们论文[11]中的方程1），然后使用直通Gumbel-softmax估计器对节点动作进行采样。随后，环境网络根据采样动作更新每个节点的状态，依据是我们论文[11]中的方程1。'
- en: '[14] For example, if the task requires information only from the neighbors
    with a certain degree, then the action network can learn to listen only to these
    nodes (see an experiment in Section 6.1 of our paper [11).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] 例如，如果任务只需要来自具有某种程度的邻居的信息，那么动作网络可以学习仅关注这些节点（参见我们论文[11]第6.1节中的实验）。'
- en: '[15] L. Faber and R. Wattenhofer, [Asynchronous neural networks for learning
    in graphs](https://arxiv.org/abs/2205.12245) (2022), arXiv:2205.12245.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] L. Faber 和 R. Wattenhofer，[图学习中的异步神经网络](https://arxiv.org/abs/2205.12245)（2022），arXiv:2205.12245。'
- en: '[16] Proposition 5.1 in our paper [11]. The variance introduced by the sampling
    process helps to discriminate nodes that are 1-WL indistinguishable but also makes
    Co-GNN models invariant only in expectation.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] 在我们的论文[11]中，第5.1条命题。采样过程引入的方差有助于区分那些1-WL不可区分的节点，但也使得Co-GNN模型仅在期望值上不变。'
- en: '[17] See e.g. A. Loukas, [What graph neural networks cannot learn: depth vs
    width](https://arxiv.org/abs/1907.03199) (2020) *ICLR* and R. Abboud, R. Dimitrov,
    and I. Ceylan, [Shortest path networks for graph property prediction](https://arxiv.org/abs/2206.01003)
    (2022) *LoG*.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] 参见例如 A. Loukas，[图神经网络不能学习的内容：深度与宽度](https://arxiv.org/abs/1907.03199)（2020）
    *ICLR* 和 R. Abboud、R. Dimitrov 和 I. Ceylan，[用于图属性预测的最短路径网络](https://arxiv.org/abs/2206.01003)（2022）
    *LoG*。'
- en: '[18] Theorem 5.2 in our paper [11].'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] 在我们的论文[11]中，第5.2条定理。'
- en: '[19] J. Topping *et al.*, Understanding over-squashing and bottlenecks on graphs
    via curvature (2022), *ICLR.*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] J. Topping *等*，通过曲率理解图上的过度挤压和瓶颈（2022），*ICLR*。'
- en: '[20] F. Di Giovanni *et al.*, [On over-squashing in message passing neural
    networks: The impact of width, depth, and topology](https://arxiv.org/pdf/2302.02941.pdf)
    (2023), *ICML*.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] F. Di Giovanni *等*，[关于消息传递神经网络中的过度挤压：宽度、深度和拓扑的影响](https://arxiv.org/pdf/2302.02941.pdf)（2023），*ICML*。'
- en: '[21] Theorem 5.2 in our paper [11].'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] 在我们的论文[11]中，第5.2条定理。'
- en: '[22] *Homophily* implies that the neighbours of a node have similar properties
    to the node itself. Early GNN benchmarks such as *Cora* and *Pubmed* were predominantly
    homophilic. The more recent evaluations include heterophilic graphs, which are
    more challenging for GNNs.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] *同质性* 表明节点的邻居具有与节点本身相似的属性。早期的GNN基准如*Cora* 和 *Pubmed* 主要是同质的。更近期的评估包括异质图，这对GNN来说更具挑战性。'
- en: '[23] O. Platonov *et al.*, [A critical look at the evaluation of GNNs under
    heterophily: Are we really making progress?](https://arxiv.org/pdf/2302.11640.pdf)
    (2023), *ICLR.*'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[23] O. Platonov *等*， [对GNN在异质性下评估的批判性分析：我们真的在进步吗？](https://arxiv.org/pdf/2302.11640.pdf)（2023），*ICLR*。'
- en: '*For additional articles about deep learning on graphs, see Michael’s* [*other
    posts*](https://towardsdatascience.com/graph-deep-learning/home) *in Towards Data
    Science,* [*subscribe*](https://michael-bronstein.medium.com/subscribe) *to his
    posts and* [*YouTube channel*](https://www.youtube.com/c/MichaelBronsteinGDL)*,
    get* [*Medium membership*](https://michael-bronstein.medium.com/membership)*,
    or follow* [*Michael*](https://twitter.com/mmbronstein), [*Ben*](https://twitter.com/benfinkelshtein)*,*
    [*Xingyue*](https://twitter.com/hxyscott)*, and* [*Ismail*](https://twitter.com/ismaililkanc)
    *on Twitter.*'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*有关图上的深度学习的更多文章，请查看Michael的* [*其他文章*](https://towardsdatascience.com/graph-deep-learning/home)
    *，订阅* [*他的帖子*](https://michael-bronstein.medium.com/subscribe) *以及* [*YouTube频道*](https://www.youtube.com/c/MichaelBronsteinGDL)*，获取*
    [*Medium会员资格*](https://michael-bronstein.medium.com/membership)*，或关注* [*Michael*](https://twitter.com/mmbronstein)、[*Ben*](https://twitter.com/benfinkelshtein)*、[*Xingyue*](https://twitter.com/hxyscott)*
    和* [*Ismail*](https://twitter.com/ismaililkanc) *在Twitter上的动态。*'
