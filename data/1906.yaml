- en: Motivating Self-Attention
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激励自注意力
- en: 原文：[https://towardsdatascience.com/motivating-self-attention-aead09a02f70?source=collection_archive---------2-----------------------#2023-06-10](https://towardsdatascience.com/motivating-self-attention-aead09a02f70?source=collection_archive---------2-----------------------#2023-06-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/motivating-self-attention-aead09a02f70?source=collection_archive---------2-----------------------#2023-06-10](https://towardsdatascience.com/motivating-self-attention-aead09a02f70?source=collection_archive---------2-----------------------#2023-06-10)
- en: Why do we need queries, keys AND values?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么我们需要查询、键和值？
- en: '[](https://medium.com/@ryxu?source=post_page-----aead09a02f70--------------------------------)[![Ryan
    Xu](../Images/246c35755c45386273206ff44b31c691.png)](https://medium.com/@ryxu?source=post_page-----aead09a02f70--------------------------------)[](https://towardsdatascience.com/?source=post_page-----aead09a02f70--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----aead09a02f70--------------------------------)
    [Ryan Xu](https://medium.com/@ryxu?source=post_page-----aead09a02f70--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ryxu?source=post_page-----aead09a02f70--------------------------------)[![Ryan
    Xu](../Images/246c35755c45386273206ff44b31c691.png)](https://medium.com/@ryxu?source=post_page-----aead09a02f70--------------------------------)[](https://towardsdatascience.com/?source=post_page-----aead09a02f70--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----aead09a02f70--------------------------------)
    [Ryan Xu](https://medium.com/@ryxu?source=post_page-----aead09a02f70--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fff9d266441dd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmotivating-self-attention-aead09a02f70&user=Ryan+Xu&userId=ff9d266441dd&source=post_page-ff9d266441dd----aead09a02f70---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----aead09a02f70--------------------------------)
    ·9 min read·Jun 10, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Faead09a02f70&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmotivating-self-attention-aead09a02f70&user=Ryan+Xu&userId=ff9d266441dd&source=-----aead09a02f70---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fff9d266441dd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmotivating-self-attention-aead09a02f70&user=Ryan+Xu&userId=ff9d266441dd&source=post_page-ff9d266441dd----aead09a02f70---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----aead09a02f70--------------------------------)
    ·9分钟阅读·2023年6月10日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Faead09a02f70&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmotivating-self-attention-aead09a02f70&user=Ryan+Xu&userId=ff9d266441dd&source=-----aead09a02f70---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faead09a02f70&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmotivating-self-attention-aead09a02f70&source=-----aead09a02f70---------------------bookmark_footer-----------)![](../Images/190f2dea8254b6d693ca7a50fdecd509.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faead09a02f70&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmotivating-self-attention-aead09a02f70&source=-----aead09a02f70---------------------bookmark_footer-----------)![](../Images/190f2dea8254b6d693ca7a50fdecd509.png)'
- en: …self-self-attention?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: …自注意力？
- en: The goal of this article is to offer an explanation not for *how* the self-attention
    mechanism in transformers works, but rather for *why* it was designed that way.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的目标是提供一个解释，解释不是*自注意力机制如何*在变压器中工作，而是*为什么*它是这样设计的。
- en: We’ll start with a discussion of the kinds of abilities we would like a language
    understanding model to have, followed by a interactive construction of the self-attention
    mechanism. In the process, we’ll discover why we need queries, keys, AND values
    in order to model relationships between words in a natural way, and that QKV attention
    is one of the simplest ways to do so.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从讨论我们希望语言理解模型具备的能力开始，接着进行自注意力机制的互动构建。在这个过程中，我们将发现为什么我们需要查询、键和值来以自然的方式建模单词之间的关系，并且QKV注意力是实现这一点的最简单方法之一。
- en: This article will be most insightful to readers who have encountered transformers
    and self-attention before, but should be accessible to anybody familiar with some
    basic linear algebra. To those who are looking to better understand transformers,
    I will happily refer you to [this blog post](http://jalammar.github.io/illustrated-transformer/).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文对那些曾经接触过变压器和自注意力的读者将最有启发性，但应该对任何熟悉一些基础线性代数的人也能理解。对于那些希望更好地理解变压器的人，我会很高兴地推荐你们阅读[这篇博客文章](http://jalammar.github.io/illustrated-transformer/)。
- en: '*All images are by the author.*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*所有图片均由作者提供。*'
- en: Transformers are frequently presented in the context of sequence-to-sequence
    modeling tasks such as language translation or more saliently, sentence completion.
    However, I think that it’s easier to start off by thinking about the problem of
    sequence modeling and specifically, language understanding.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器通常在序列到序列建模任务中呈现，如语言翻译或更明显的句子完成。然而，我认为从思考序列建模问题和具体的语言理解问题开始更容易。
- en: 'So, here’s a sentence that we want to understand:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这里有一个我们想要理解的句子：
- en: '![](../Images/703ccfa08b74c3079cbcd25a9a1c939f.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/703ccfa08b74c3079cbcd25a9a1c939f.png)'
- en: Let’s think a little bit about how we understand this sentence.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微思考一下我们如何理解这个句子。
- en: '*Evan’s dog Riley…* From this, we know that Riley is the name of the dog, and
    that Evan owns Riley.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Evan的狗Riley…* 从中我们知道Riley是狗的名字，而Evan拥有Riley。'
- en: '*…is so hyper…* Simple enough, “hyper” refers to the dog Riley, influencing
    our impression of Riley.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*…如此超级…* 简单来说，“hyper”指的是狗Riley，影响了我们对Riley的印象。'
- en: '*…she never stops moving*. This one is interesting. “she” refers to Riley,
    since the dog is the subject of the first phrase. This tells us that Evan’s dog
    Riley is fact female, which was previously ambiguous due to the commonly unisex
    dog name, “Riley”. “never stops moving” is a slightly more complex set of words
    that elaborates on “hyper”.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*…她从不停下* 这很有趣。“她”指的是Riley，因为狗是第一句话的主语。这告诉我们，Evan的狗Riley实际上是母狗，这在之前由于狗名“Riley”常用性别中性而模糊不清。“从不停下”是一组稍复杂的词汇，进一步阐述了“hyper”。'
- en: The key takeaway here is that to build up our understanding of the sentence,
    we’re constantly considering how words relate to other words to augment their
    meaning.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 关键是要建立对句子的理解，我们不断考虑词与词之间的关系来增强它们的含义。
- en: In the machine learning community, the process of augmenting the meaning of
    a word **a** by the presence of another word **b** is colloquially referred to
    as “**a** attends to **b**”, as in word **a** pays attention to word **b**.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习社区中，通过另一个词 **b** 的存在来增强词 **a** 的含义的过程被口语化地称为“**a** 关注 **b**”，即词 **a** 关注词
    **b**。
- en: '![](../Images/e71804c1195ba214ee007bf1b63fb1d4.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e71804c1195ba214ee007bf1b63fb1d4.png)'
- en: An arrow **a** => **b** indicates that “**a** attends to **b**”
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一个箭头 **a** => **b** 表示“**a** 关注 **b**”
- en: And so, if we would like a machine learning model to understand language, we
    might understandably want the model to have the ability to have a word attend
    to another and somehow update its meaning accordingly.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们希望机器学习模型理解语言，我们可能会合理地希望模型能够让一个词关注另一个词，并以某种方式更新其含义。
- en: This is precisely the ability that we hope to emulate as we build up the aptly
    named (self) attention mechanism in 3 parts.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们希望在构建恰如其分地命名的（自）注意力机制的三个部分时模仿的能力。
- en: '*In the following, I will pose a number of questions in italics. I strongly
    encourage the reader to stop and consider the question for a minute before continuing.*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*接下来，我将提出一些斜体的问题。我强烈鼓励读者在继续之前停下来考虑这些问题一分钟。*'
- en: Part 1
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1部分
- en: For now, let’s focus on the relationship between the words “dog” and “Riley”.
    The word “dog” strongly influences the meaning of the word, “Riley”, and so we
    want “Riley” to attend to “dog”, and so the goal here is to somehow update the
    meaning of the word “Riley” accordingly.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们关注词“dog”和“Riley”之间的关系。词“dog”强烈影响词“Riley”的含义，因此我们希望“Riley”能够关注“dog”，因此这里的目标是以某种方式更新词“Riley”的含义。
- en: '![](../Images/ece19bb21c5d776d23dc696aa847314c.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ece19bb21c5d776d23dc696aa847314c.png)'
- en: To make this example more concrete, let’s say that we begin with vector representations
    of each word, each of length **n**, based on a context-free understanding of the
    word. We will assume that this vector space is fairly well organized, meaning
    that words that are more similar in meaning are associated with vectors that are
    closer in the space.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个例子更具体，假设我们以每个词的向量表示开始，每个向量的长度为**n**，基于对词语的无上下文理解。我们将假设这个向量空间相当有序，即在空间中，意义更相似的词与更接近的向量相关联。
- en: So, we have two vectors, **v_dog** and **v_Riley**, that capture the meaning
    of the two words.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们有两个向量，**v_dog** 和**v_Riley**，它们捕捉了这两个词的意义。
- en: '*How can we update the value of* ***v_Riley*** *using* ***v_dog*** *to obtain
    a new value for the word “Riley” that incorporates the meaning of “dog”?*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们如何使用* ***v_dog*** *来更新* ***v_Riley*** *的值，从而获得一个包含“狗”意义的新词“Riley”的值？*'
- en: 'We don’t want to completely replace the value of **v_Riley** with **v_dog**,
    so let’s say that we take a linear combination of **v_Riley** and **v_dog** as
    the new value for **v_Riley**:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不想完全用**v_dog** 替代**v_Riley** 的值，所以我们可以将**v_Riley** 和**v_dog** 的线性组合作为**v_Riley**
    的新值：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This seems to work alright, we’ve embedded a bit of the meaning of the word
    “dog” into the word “Riley”.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这似乎还不错，我们已经将“狗”这个词的一部分意义嵌入到了“Riley”这个词中。
- en: Now we would like to try and apply this form of attention to the whole sentence
    by updating the vector representations of every single word by the vector representations
    of every other word.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们希望尝试将这种形式的注意力应用于整个句子，通过每个词的向量表示更新每个其他词的向量表示。
- en: '*What goes wrong here?*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*这里出了什么问题？*'
- en: The core problem is that we don’t know which words should take on the meanings
    of other words. We would also like some measure of how much the value of each
    word should contribute to each other word.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 核心问题是我们不知道哪些词应该承担其他词的意义。我们还希望有一些衡量标准来确定每个词的值应该对其他词贡献多少。
- en: '**Part 2**'
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**第二部分**'
- en: Alright. So we need to know how much two words should be related.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。所以我们需要知道两个词之间的相关程度。
- en: Time for attempt number 2.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 进行第二次尝试。
- en: I’ve redesigned our vector database so that each word actually has two associated
    vectors. The first is the same value vector that we had before, still denoted
    by **v**. In addition, we now have unit vectors denoted by **k** that store some
    notion of word relations. Specifically, if two **k** vectors are close together,
    it means that the values associated with these words are likely to influence each
    other’s meanings.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经重新设计了我们的向量数据库，使每个词实际上都有两个关联向量。第一个是我们之前拥有的相同值向量，仍然用**v** 表示。此外，我们现在有单位向量，记作**k**，它们存储一些关于词语关系的概念。具体来说，如果两个**k**
    向量接近，这意味着这些词的值可能会相互影响。
- en: '*With our new* ***k*** *and* ***v*** *vectors, how can we modify our previous
    scheme to update* ***v_Riley****’s value with* ***v_dog*** *in a way that respects
    how much two words are related?*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*使用我们新的* ***k*** *和* ***v*** *向量，我们如何修改之前的方案，以便用* ***v_dog*** *更新* ***v_Riley***
    *的值，并且尊重两个词之间的关系？*'
- en: Let’s continue with the same linear combination business as before, but only
    if the k vectors of both are close in embedding space. Even better, we can use
    the dot product of the two k vectors (which range from 0–1 since they are unit
    vectors) to tell us how much we should update **v_Riley** with **v_dog**.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续使用之前的线性组合方法，但仅当这两个向量的 k 维接近时。更好的是，我们可以利用这两个 k 向量的点积（它们的范围是 0-1，因为它们是单位向量）来告诉我们应该如何用**v_Riley**
    更新**v_dog**。
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This is a little bit strange since if relevance is 1, **v_Riley** gets completely
    replaced by **v_dog**, but let’s ignore that for a minute.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点奇怪，因为如果相关性为 1，**v_Riley** 会完全被**v_dog** 替代，但我们先忽略这一点。
- en: I want to instead think about what happens when we apply this kind of idea to
    the whole sequence. The word “Riley” will have a relevance value with each other
    word via dot product of **k**s. So, maybe we can instead update the value of each
    word proportionally to the value of the dot product. For simplicity, let’s also
    include it’s dot product with itself as a way to preserve it’s own value.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来考虑将这种想法应用于整个序列。当我们通过**k**的点积来计算每个词的相关性值时，“Riley”将与其他每个词都有一个相关性值。因此，也许我们可以根据点积的值按比例更新每个词的值。为了简单起见，我们还可以将其与自身的点积包含在内，以保持其自身的值。
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Ok that’s good enough for now.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，目前为止这些就足够了。
- en: But once again, I claim that there’s something wrong with this approach. It’s
    not that any of our ideas have been implemented incorrectly, but rather there’s
    something fundamentally different between this approach and how we actually think
    about relationships between words.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 但我再次声明，这种方法存在问题。并不是说我们的任何想法实现不正确，而是这种方法与我们实际思考单词之间关系的方式存在根本性差异。
- en: '*If there’s any point in this article where I* ***really really*** *think that
    you should stop and think, it’s here. Even those of you who think you fully understand
    attention. What’s wrong with our approach?*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果在本文中有任何一点我* ***非常非常*** *认为你应该停下来思考，那就是这里。即使是那些认为自己完全理解注意力机制的人。我们的方法有什么问题？*'
- en: '![](../Images/7b84bc5ca1fe623192e96921b64083c5.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b84bc5ca1fe623192e96921b64083c5.png)'
- en: A hint
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一个提示
- en: Relationships between words are inherently asymmetric! The way that “Riley”
    attends to “dog” is different from the way that “dog” attends to “Riley”. It’s
    a much bigger deal that “Riley” refers to a dog, not a human, than the name of
    the dog.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 单词之间的关系本质上是非对称的！“Riley”关注“dog”的方式与“dog”关注“Riley”的方式不同。“Riley”指的是一只狗，而不是人类，这比狗的名字更重要。
- en: In contrast, the dot product is a symmetric operation, which means that in our
    current setup, if a attends to b, then b attends equally strong to a! Actually,
    this is somewhat false because we’re normalizing the relevance scores, but the
    point is that the words should have the option of attending in an asymmetric way,
    even if the other tokens are held constant.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，点积是对称操作，这意味着在我们当前的设置中，如果a关注b，那么b也会同样强烈地关注a！实际上，这有些不准确，因为我们在规范化相关性分数，但重点是单词应该有机会以非对称的方式进行关注，即使其他标记保持不变。
- en: Part 3
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三部分
- en: 'We’re almost there! Finally, the question becomes:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们快到了！最后，问题变成了：
- en: '*How can we most naturally extend our current setup to allow for asymmetric
    relationships?*'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们如何最自然地扩展当前设置以允许非对称关系？*'
- en: Well what can we do with one more vector type? We still have our value vectors
    **v**, and our relation vector **k**. Now we have yet another vector **q** for
    each token.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们可以用另一种向量类型做什么呢？我们仍然有值向量 **v** 和关系向量 **k**。现在每个标记还多了一个向量 **q**。
- en: '*How can we modify our setup and use* ***q*** *to achieve the asymmetric relationship
    that we want?*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们如何修改我们的设置并使用* ***q*** *来实现我们想要的非对称关系？*'
- en: Those of you who are familiar with how self-attention works will hopefully be
    smirking at this point.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 那些熟悉自注意力工作原理的人可能会在这一点上露出会心的微笑。
- en: Instead of computing relevance **k_dog** · **k_Riley** when “dog” attends to
    “Riley”, we can instead *query* **q_Riley** against the *key* **k_dog** by taking
    their dot product. When computing the other way around, we will have **q_dog**
    · **k_Riley** instead — asymmetric relevance!
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当“dog”关注“Riley”时，我们计算相关性 **k_dog** · **k_Riley**，我们可以改为用 *query* **q_Riley**
    与 *key* **k_dog** 的点积来进行计算。当反向计算时，我们将得到 **q_dog** · **k_Riley** —— 非对称相关性！
- en: Here’s the whole thing together, computing the update for every value at once!
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是整体内容，一次性计算每个值的更新！
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: And that’s basically self-attention!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上就是自注意力机制！
- en: There are a few more details that I left out, but the important ideas are all
    there.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我遗漏了一些细节，但重要的概念都在这里。
- en: To recap, we started with value vectors (**v**) to represent the meaning of
    each word, but quickly found that we need key vectors (**k**) to account for how
    words relate to each other. Finally, to properly model the asymmetric nature of
    word relationships, we introduced query vectors (**q**). It almost feels like
    if all we’re allowed are dot products and such, 3 is the minimal number of vectors
    per word needed to properly model relationships between words.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们开始使用值向量 (**v**) 来表示每个单词的意义，但很快发现需要键向量 (**k**) 来考虑单词之间的关系。最后，为了正确建模单词关系的非对称性质，我们引入了查询向量
    (**q**)。感觉如果我们只能使用点积等操作，3 是每个单词建模关系所需的最小向量数量。
- en: The purpose of this article is to unravel the self-attention mechanism in a
    way that’s less overwhelming than the traditional algorithm-first approach. I
    hope that from this more language-motivated perspective, the elegance and simplicity
    of the query-key-value design can show through.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的目的是以一种比传统算法优先方法更少让人感到压倒的方式揭示自注意力机制。我希望通过这种更具语言动机的视角，查询-键-值设计的优雅和简洁能够展现出来。
- en: 'Some details that I left out:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我遗漏的一些细节：
- en: Instead of storing 3 vectors for each token, we instead store a single embedding
    vector from which we can extract our **q**-**k**-**v** vectors. The extraction
    process is just a linear projection.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不再为每个 token 存储 3 个向量，而是存储一个单一的嵌入向量，从中可以提取我们的 **q**-**k**-**v** 向量。提取过程只是一个线性投影。
- en: Technically, in this whole setup, each word has no idea where the other words
    are in the sentence. Self-attention is really a set operation. So, we need to
    embed positional knowledge, typically done by adding a position vector to the
    embedding vector. This isn’t completely trivial since transformers should allow
    for sequences of arbitrary length. How this works in practice is outside the scope
    of this article.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从技术上讲，在这个整体设置中，每个词都不知道其他词在句子中的位置。自注意力实际上是一种集合操作。因此，我们需要嵌入位置信息，通常是通过将位置向量添加到嵌入向量来完成的。这并不是完全简单的，因为
    transformers 应该允许任意长度的序列。具体如何在实践中运作超出了本文的范围。
- en: A single self-attention layer only allows us to represent two-word relationships.
    But by composing self-attention layers, we can model higher level relationships
    between words. Since the output of a self-attention layer is of the same sequence
    length as the original sequence, this means that we can compose them. In fact,
    transformer blocks are just self-attention layers followed by position-wise feed-forward
    blocks. Stack a few hundred of these, pay a few million dollars, and you have
    yourself an LLM! :)
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个自注意力层只能表示两个词之间的关系。但是通过组合自注意力层，我们可以建模更高级别的词之间关系。由于自注意力层的输出与原始序列的长度相同，这意味着我们可以对它们进行组合。实际上，transformer
    模块就是由自注意力层和逐位置前馈模块组成。堆叠几百个这样的模块，花费几百万美元，你就拥有了一个LLM！:)
- en: '![](../Images/fc2b1b4d79199119a9f07e5f7fc59fd8.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc2b1b4d79199119a9f07e5f7fc59fd8.png)'
- en: OpenAI showed that it takes 512 transformer blocks to understand that second
    half.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 显示出理解第二部分需要 512 个 transformer 模块。
