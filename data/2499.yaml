- en: Latest in CNN Kernels for Large Image Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型图像模型中的最新CNN核
- en: 原文：[https://towardsdatascience.com/latest-in-cnn-kernels-for-large-image-models-b48b27e75638?source=collection_archive---------3-----------------------#2023-08-04](https://towardsdatascience.com/latest-in-cnn-kernels-for-large-image-models-b48b27e75638?source=collection_archive---------3-----------------------#2023-08-04)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/latest-in-cnn-kernels-for-large-image-models-b48b27e75638?source=collection_archive---------3-----------------------#2023-08-04](https://towardsdatascience.com/latest-in-cnn-kernels-for-large-image-models-b48b27e75638?source=collection_archive---------3-----------------------#2023-08-04)
- en: A high-level overview of the latest convolutional kernel structures in Deformable
    Convolutional Networks, DCNv2, DCNv3
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对可变形卷积网络中的最新卷积核结构、DCNv2、DCNv3进行的高级概述
- en: '[](https://medium.com/@1996hwm?source=post_page-----b48b27e75638--------------------------------)[![Wanming
    Huang](../Images/05051c8e4fe9c6ffc1d28a9578bf6537.png)](https://medium.com/@1996hwm?source=post_page-----b48b27e75638--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b48b27e75638--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b48b27e75638--------------------------------)
    [Wanming Huang](https://medium.com/@1996hwm?source=post_page-----b48b27e75638--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@1996hwm?source=post_page-----b48b27e75638--------------------------------)[![Wanming
    Huang](../Images/05051c8e4fe9c6ffc1d28a9578bf6537.png)](https://medium.com/@1996hwm?source=post_page-----b48b27e75638--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b48b27e75638--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b48b27e75638--------------------------------)
    [Wanming Huang](https://medium.com/@1996hwm?source=post_page-----b48b27e75638--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1cad9f97731c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-in-cnn-kernels-for-large-image-models-b48b27e75638&user=Wanming+Huang&userId=1cad9f97731c&source=post_page-1cad9f97731c----b48b27e75638---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b48b27e75638--------------------------------)
    ·7 min read·Aug 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb48b27e75638&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-in-cnn-kernels-for-large-image-models-b48b27e75638&user=Wanming+Huang&userId=1cad9f97731c&source=-----b48b27e75638---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1cad9f97731c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-in-cnn-kernels-for-large-image-models-b48b27e75638&user=Wanming+Huang&userId=1cad9f97731c&source=post_page-1cad9f97731c----b48b27e75638---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b48b27e75638--------------------------------)
    ·7分钟阅读·2023年8月4日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb48b27e75638&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-in-cnn-kernels-for-large-image-models-b48b27e75638&user=Wanming+Huang&userId=1cad9f97731c&source=-----b48b27e75638---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb48b27e75638&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-in-cnn-kernels-for-large-image-models-b48b27e75638&source=-----b48b27e75638---------------------bookmark_footer-----------)![](../Images/53825b3549061898e455c5ff37ab15ed.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb48b27e75638&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-in-cnn-kernels-for-large-image-models-b48b27e75638&source=-----b48b27e75638---------------------bookmark_footer-----------)![](../Images/53825b3549061898e455c5ff37ab15ed.png)'
- en: Cape Byron Lighthouse, Australia | photo by author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 澳大利亚拜伦湾灯塔 | 作者拍摄
- en: As the remarkable success of OpenAI’s ChatGPT has sparked the boom of large
    language models, many people foresee the next breakthrough in large image models.
    In this domain, vision models can be prompted to analyze and even generate images
    and videos in a similar manner to how we currently prompt ChatGPT.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着OpenAI的ChatGPT的显著成功引发了大型语言模型的热潮，许多人预测下一次突破将发生在大型图像模型领域。在这个领域，视觉模型可以像我们目前对ChatGPT的提示一样被提示去分析甚至生成图像和视频。
- en: 'The latest deep learning approaches for large image models have branched into
    two main directions: those based on convolutional neural networks (CNNs) and those
    based on transformers. This article will focus on the CNN side and provide a high-level
    overview of those improved CNN kernel structures.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最新的大型图像模型深度学习方法已分为两个主要方向：基于卷积神经网络（CNN）的方法和基于变换器的方法。本文将重点介绍 CNN 方面，并提供改进的 CNN
    内核结构的高级概述。
- en: Table of Content
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目录
- en: '[DCN](#00f3)'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[DCN](#00f3)'
- en: '[DCNv2](#1709)'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[DCNv2](#1709)'
- en: '[DCNv3](#22a3)'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[DCNv3](#22a3)'
- en: 1\. Deformable Convolutional Networks (DCN)
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 可变形卷积网络（DCN）
- en: Traditionally, CNN kernels have been applied to fixed locations in each layer,
    resulting in all activation units having the same receptive field.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，CNN 内核在每一层中应用于固定位置，导致所有激活单元具有相同的感受野。
- en: As in the figure below, to perform convolution on an input feature map ***x***,
    the value at each output location ***p****0* is calculated as an element-wise
    multiplication and summation between kernel weight ***w*** and a sliding window
    on ***x.*** The sliding window is defined by a grid ***R***, which is also the
    receptive field for ***p****0****.*** The size of ***R*** remains the same across
    all locations within the same layer of *y*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，为了在输入特征图 ***x*** 上执行卷积，计算每个输出位置 ***p****0* 的值是通过内核权重 ***w*** 和在 ***x***
    上的滑动窗口之间的逐元素乘法和求和来完成的。滑动窗口由网格 ***R*** 定义，这也是 ***p****0**** 的感受野。*** 在 *y* 的同一层中的所有位置，***R***
    的大小保持不变。
- en: '![](../Images/cbb617318368ccf22a3c93a1182e37b6.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cbb617318368ccf22a3c93a1182e37b6.png)'
- en: Regular convolution operation with 3x3 kernel.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 常规 3x3 内核的卷积操作。
- en: 'Each output value is calculated as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 每个输出值的计算如下：
- en: '![](../Images/4060a93bc9894d787e43933e6adbef99.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4060a93bc9894d787e43933e6adbef99.png)'
- en: Regular convolution operation function from [paper](https://arxiv.org/pdf/1703.06211v3.pdf).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[论文](https://arxiv.org/pdf/1703.06211v3.pdf)中的常规卷积操作函数。'
- en: where ***p****n* enumerates locations in the sliding window (grid ***R***).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ***p****n* 枚举滑动窗口（网格 ***R***）中的位置。
- en: 'The RoI (region of interest) pooling operation, too, operates on bins with
    a fixed size in each layer. For (*i, j*)-th bin containing *nij* pixels, its pooling
    outcome is computed as:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: RoI（兴趣区域）池化操作也在每一层中固定大小的 bin 上操作。对于 (*i, j*)-th bin 包含 *nij* 像素，其池化结果计算如下：
- en: '![](../Images/7eb8c29a407505e0889f7429f7b574bb.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7eb8c29a407505e0889f7429f7b574bb.png)'
- en: Regular average RoI pooling function from [paper](https://arxiv.org/pdf/1703.06211v3.pdf).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[论文](https://arxiv.org/pdf/1703.06211v3.pdf)中的常规平均 RoI 池化函数。'
- en: Again shape and size of bins are the same in each layer.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 每一层中的 bin 的形状和大小再次保持不变。
- en: '![](../Images/6bc09b7c45eef4cf0b3a7ead1634614e.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6bc09b7c45eef4cf0b3a7ead1634614e.png)'
- en: Regular average RoI pooling operation with 3x3 bin.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 3x3 bin 的常规平均 RoI 池化操作。
- en: Both operations thus become particularly problematic for high-level layers that
    encode semantics, e.g., objects with varying scales.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于编码语义的高层，如具有不同尺度的对象，这两个操作特别具有挑战性。
- en: '[DCN](https://arxiv.org/pdf/1703.06211v3.pdf) proposes deformable convolution
    and deformable pooling that are more flexible to model those geometric structures.
    Both operate on the 2D spatial domain, i.e., the operation remains the same across
    the channel dimension.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[DCN](https://arxiv.org/pdf/1703.06211v3.pdf) 提出了可变形卷积和可变形池化，这些方法对建模这些几何结构更加灵活。两者都在
    2D 空间域上操作，即操作在通道维度上保持不变。'
- en: '**Deformable convolution**'
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**可变形卷积**'
- en: '![](../Images/ac617c6bd395fa1da6bc84592033e74a.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac617c6bd395fa1da6bc84592033e74a.png)'
- en: Deformable convolution operation with 3x3 kernel.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 3x3 内核的可变形卷积操作。
- en: Given input feature map **x**, for each location ***p****0* in the output feature
    map **y**, DCN adds 2D offsets △***p****n* when enumerating each location ***p****n*
    in a regular grid ***R***.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 给定输入特征图 **x**，对于输出特征图 **y** 中的每个位置 ***p****0*，DCN 在枚举常规网格 ***R*** 中的每个位置 ***p****n*
    时添加 2D 偏移量 △***p****n*。
- en: '![](../Images/252f21b12aa8ce2e3daa7fbb8152e2ae.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/252f21b12aa8ce2e3daa7fbb8152e2ae.png)'
- en: Deformable convolution function from [paper](https://arxiv.org/pdf/1703.06211v3.pdf).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[论文](https://arxiv.org/pdf/1703.06211v3.pdf)中的可变形卷积函数。'
- en: These offsets are learned from preceding feature maps, obtained via an additional
    conv layer over the feature map. As these offsets are typically fractional, they
    are implemented via bilinear interpolation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这些偏移量从前面的特征图中学习得到，通过在特征图上应用额外的卷积层获得。由于这些偏移量通常是小数，它们通过双线性插值来实现。
- en: '**Deformable RoI pooling**'
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**可变形 RoI 池化**'
- en: Similar to the convolution operation, pooling offsets △***p****ij* are added
    to the original binning positions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于卷积操作，池化偏移量 △***p****ij* 被添加到原始 bin 位置。
- en: '![](../Images/959d5a7c4599ec6b37fe78d6f2b673a9.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/959d5a7c4599ec6b37fe78d6f2b673a9.png)'
- en: Deformable RoI pooling function from [paper](https://arxiv.org/pdf/1703.06211v3.pdf).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 可变形 RoI 池化函数来自 [论文](https://arxiv.org/pdf/1703.06211v3.pdf)。
- en: As in the figure below, these offsets are learned through a fully connected
    (FC) layer after the original pooling result.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，这些偏移量通过原始池化结果后的全连接（FC）层进行学习。
- en: '![](../Images/0069fe84c6c266e72c62cba93337ff30.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0069fe84c6c266e72c62cba93337ff30.png)'
- en: Deformable average RoI pooling operation with 3x3 bin.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 可变形平均 RoI 池化操作，使用 3x3 bin。
- en: '**Deformable Position-Sentitive (PS) RoI pooling**'
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**可变形位置敏感（PS）RoI 池化**'
- en: When applying deformable operations to PS RoI pooling ([Dai et al., n.d.](https://arxiv.org/pdf/1605.06409v2.pdf)),
    as illustrated in the figure below, offsets are applied to each score map instead
    of the input feature map. These offsets are learned through a conv layer instead
    of an FC layer.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在将可变形操作应用于 PS RoI 池化 ([Dai et al., n.d.](https://arxiv.org/pdf/1605.06409v2.pdf))
    时，如下图所示，偏移量应用于每个分数图而不是输入特征图。这些偏移量通过卷积层而不是全连接层进行学习。
- en: '*Position-Sensitive RoI pooling* ([Dai et al., n.d.](https://arxiv.org/pdf/1605.06409v2.pdf))*:
    Traditional RoI pooling loses information regarding which object part each region
    represents. PS RoI pooling is proposed to retain this information by converting
    input feature maps to k² score maps for each object class, where each score map
    represents a specific spatial part. So for C object classes, there are total k²
    (C+1) score maps.*'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*位置敏感 RoI 池化* ([Dai et al., n.d.](https://arxiv.org/pdf/1605.06409v2.pdf))*：传统的
    RoI 池化丢失了每个区域所代表的对象部分的信息。PS RoI 池化通过将输入特征图转换为每个对象类别的 k² 分数图来保留这些信息，其中每个分数图表示一个特定的空间部分。因此，对于
    C 个对象类别，总共有 k² (C+1) 个分数图。*'
- en: '![](../Images/2ce91cae1a55879f64a3a8c6b7f74cce.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ce91cae1a55879f64a3a8c6b7f74cce.png)'
- en: Illustration of 3x3 deformable PS RoI pooling | source from [paper](https://arxiv.org/pdf/1703.06211v3.pdf).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 3x3 可变形 PS RoI 池化的示意图 | 来源于 [论文](https://arxiv.org/pdf/1703.06211v3.pdf)。
- en: 2\. DCNv2
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. DCNv2
- en: 'Although DCN allows for more flexible modelling of the receptive field, it
    assumes pixels within each receptive field contribute equally to the response,
    which is often not the case. To better understand the contribution behaviour,
    authors use three methods to visualize the spatial support:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 DCN 允许更灵活地建模感受野，但它假设每个感受野中的像素对响应的贡献是相等的，这种情况通常不成立。为了更好地理解贡献行为，作者使用三种方法来可视化空间支持：
- en: 'Effective receptive fields: gradient of the node response with respect to intensity
    perturbations of each image pixel'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有效的感受野：节点响应关于每个图像像素的强度扰动的梯度
- en: 'Effective sampling/bin locations: gradient of the network node with respect
    to the sampling/bin locations'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有效的采样/ bin 位置：网络节点相对于采样/ bin 位置的梯度
- en: 'Error-bounded saliency regions: progressively masking the parts of the image
    to find the smallest image region that produces the same response as the entire
    image'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 错误界限显著区域：逐步遮蔽图像的部分，以找到与整个图像产生相同响应的最小图像区域
- en: 'To assign learnable feature amplitude to locations within the receptive field,
    [DCNv2](https://arxiv.org/pdf/1811.11168.pdf) introduces modulated deformable
    modules:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将可学习的特征幅度分配到感受野中的位置，[DCNv2](https://arxiv.org/pdf/1811.11168.pdf) 引入了调制可变形模块：
- en: '![](../Images/27af2f6ab5d795a12b0e24917a48cab3.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/27af2f6ab5d795a12b0e24917a48cab3.png)'
- en: DCNv2 convolution function from [paper](https://arxiv.org/pdf/1811.11168.pdf),
    notations revised to match ones in DCN paper.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: DCNv2 卷积函数来自 [论文](https://arxiv.org/pdf/1811.11168.pdf)，符号已修订以匹配 DCN 论文中的符号。
- en: For location ***p****0*, the offset △***pn*** and its amplitude △***m****n*
    are learnable through separate conv layers applied to the same input feature map.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于位置 ***p****0*，偏移量 △***pn*** 及其幅度 △***m****n* 通过应用于相同输入特征图的单独卷积层进行学习。
- en: DCNv2 revised deformable RoI pooling similarly by adding a learnable amplitude
    △***m****ij* for each (i,j)-th bin.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: DCNv2 通过为每个 (i,j)-th bin 添加一个可学习的幅度 △***m****ij* 来修订可变形 RoI 池化。
- en: '![](../Images/3717e68140640eabaf86a43988b688a3.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3717e68140640eabaf86a43988b688a3.png)'
- en: DCNv2 pooling function from [paper](https://arxiv.org/pdf/1811.11168.pdf), notations
    revised to match ones in DCN paper.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: DCNv2 池化函数来自 [论文](https://arxiv.org/pdf/1811.11168.pdf)，符号已修订以匹配 DCN 论文中的符号。
- en: DCNv2 also expands the use of deformable conv layers to replace regular conv
    layers in conv3 to conv5 stages in ResNet-50.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: DCNv2 还扩展了变形卷积层的使用，以替代 ResNet-50 中 conv3 到 conv5 阶段的常规卷积层。
- en: '**3\. DCNv3**'
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**3\. DCNv3**'
- en: To reduce the parameter size and memory complexity from DCNv2, [DCNv3](https://arxiv.org/pdf/2211.05778.pdf)
    makes the following adjustments to the kernel structure.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少参数大小和内存复杂度， [DCNv3](https://arxiv.org/pdf/2211.05778.pdf) 对卷积核结构进行了以下调整。
- en: Inspired by [depthwise separable convolution (Chollet, 2017)](https://arxiv.org/pdf/1610.02357.pdf)
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 灵感来自于[深度可分离卷积（Chollet，2017）](https://arxiv.org/pdf/1610.02357.pdf)
- en: '*Depthwise separable convolution decouples traditional convolution into: 1\.
    depth-wise convolution: each channel of the input feature is convolved separately
    with a filter; 2\. point-wise convolution: a 1x1 convolution applied across channels.*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*深度可分离卷积将传统卷积解耦为：1\. 深度卷积：每个输入特征通道与一个滤波器单独进行卷积；2\. 点卷积：在通道上应用 1x1 卷积。*'
- en: The authors propose to let the feature amplitude *m* be the depth-wise part,
    and the projection weight *w* shared among locations in the grid as the point-wise
    part.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提出将特征幅度 *m* 设为深度卷积部分，将投影权重 *w* 在网格中的位置共享作为点卷积部分。
- en: 2\. Inspired by [group convolution](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
    (Krizhevsky, Sutskever and Hinton, 2012)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 灵感来自于[组卷积](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)（Krizhevsky，Sutskever
    和 Hinton，2012）
- en: '*Group convolution: Split input channels and output channels into groups and
    apply separate convolution to each group.*'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*组卷积：将输入通道和输出通道拆分为多个组，并对每个组分别应用卷积。*'
- en: '[DCNv3](https://arxiv.org/pdf/2211.05778.pdf) (Wang et al., 2023) propose splitting
    the convolution into G groups, each having separate offset △***p****gn* and feature
    amplitude △***m****gn*.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[DCNv3](https://arxiv.org/pdf/2211.05778.pdf)（Wang 等，2023）提出将卷积拆分为 G 组，每组具有单独的偏移量
    △***p****gn* 和特征幅度 △***m****gn*。'
- en: 'DCNv3 is hence formulated as:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，DCNv3 被表述为：
- en: '![](../Images/cb0312cbf8c926d9e223e8254ec7fda7.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb0312cbf8c926d9e223e8254ec7fda7.png)'
- en: DCNv3 convolution function from [paper,](https://arxiv.org/pdf/2211.05778.pdf)
    notations revised to match ones in DCN paper.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: DCNv3 卷积功能来自于[论文，](https://arxiv.org/pdf/2211.05778.pdf)符号修订以匹配 DCN 论文中的符号。
- en: where *G* is the total number of convolution groups, ***w****g* is location
    irrelevant, △***m****gn* is normalized by the softmax function so that the sum
    over grid ***R*** is 1.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *G* 是卷积组的总数，***w****g* 与位置无关，△***m****gn* 通过 softmax 函数进行标准化，使得网格 ***R***
    上的和为 1。
- en: Performance
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能
- en: So far DCNv3 based InternImage has demonstrated superior performance in multiple
    downstream tasks such as detection and segmentation, as shown in the table below,
    as well as the leaderboard on papers with code. Refer to the original paper for
    more detailed comparisons.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，基于 DCNv3 的 InternImage 在检测和分割等多个下游任务中表现优越，如下表所示，以及 papers with code 上的排行榜。有关更详细的比较，请参考原论文。
- en: '![](../Images/e0b2852403fca3c51778ab822abfde1a.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e0b2852403fca3c51778ab822abfde1a.png)'
- en: Object detection and instance segmentation performance on COCO val2017\. The
    FLOPs are measured with 1280×800 inputs. AP’ and AP’ represent box AP and mask
    AP, respectively. “MS” means multi-scale training. Source from [paper](https://arxiv.org/pdf/2211.05778.pdf).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: COCO val2017上的目标检测和实例分割性能。FLOPs 是用 1280×800 输入测量的。AP’ 和 AP’ 分别表示框 AP 和掩码 AP。“MS”代表多尺度训练。来源于[论文](https://arxiv.org/pdf/2211.05778.pdf)。
- en: '![](../Images/7bdd9ad06e7904179df50b42de38d53f.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7bdd9ad06e7904179df50b42de38d53f.png)'
- en: Screenshot of the leaderboard for object detection from [paperswithcode.com](https://paperswithcode.com/task/object-detection).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[paperswithcode.com](https://paperswithcode.com/task/object-detection)的目标检测排行榜截图。
- en: '![](../Images/a582193e0ed5faab979a487ba6e3a102.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a582193e0ed5faab979a487ba6e3a102.png)'
- en: Screenshot of the leaderboard for semantic segmentation from [paperswithcode.com](https://paperswithcode.com/task/semantic-segmentation).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[paperswithcode.com](https://paperswithcode.com/task/semantic-segmentation)的语义分割排行榜截图。
- en: Summary
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this article, we have reviewed kernel structures for regular convolutional
    networks, along with their latest improvements, including deformable convolutional
    networks (DCN) and two newer versions: DCNv2 and DCNv3\. We discussed the limitations
    of traditional structures and highlighted the advancements in innovation built
    upon previous versions. For a deeper understanding of these models, please refer
    to the papers in the References section.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们回顾了常规卷积网络的核结构及其最新改进，包括可变形卷积网络（DCN）及两个更新版本：DCNv2 和 DCNv3。我们讨论了传统结构的局限性，并突出了在前一版本基础上的创新进展。欲深入了解这些模型，请参阅参考文献中的论文。
- en: Acknowledgement
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢
- en: Special thanks to [Kenneth Leung](https://medium.com/u/dcd08e36f2d0?source=post_page-----b48b27e75638--------------------------------),
    who inspired me to create this piece and shared amazing ideas. A huge thank you
    to Kenneth, [Melissa Han](https://medium.com/u/2c1cee38ff69?source=post_page-----b48b27e75638--------------------------------),
    and Annie Liao, who contributed to improving this piece. Your insightful suggestions
    and constructive feedback have significantly impacted the quality and depth of
    the content.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 特别感谢[Kenneth Leung](https://medium.com/u/dcd08e36f2d0?source=post_page-----b48b27e75638--------------------------------)，他激发了我创作这篇文章的灵感，并分享了惊人的想法。对Kenneth、[Melissa
    Han](https://medium.com/u/2c1cee38ff69?source=post_page-----b48b27e75638--------------------------------)和Annie
    Liao致以深深的谢意，他们对改进这篇文章做出了贡献。你们的深刻建议和建设性反馈显著提升了内容的质量和深度。
- en: References
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Dai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H. and Wei, Y. (n.d.). *Deformable
    Convolutional Networks*. [online] Available at: [https://arxiv.org/pdf/1703.06211v3.pdf.](https://arxiv.org/pdf/1703.06211v3.pdf.)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Dai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H. 和 Wei, Y. (n.d.). *可变形卷积网络*。[在线]
    可在：[https://arxiv.org/pdf/1703.06211v3.pdf.](https://arxiv.org/pdf/1703.06211v3.pdf.)
- en: '‌Zhu, X., Hu, H., Lin, S. and Dai, J. (n.d.). *Deformable ConvNets v2: More
    Deformable, Better Results*. [online] Available at: [https://arxiv.org/pdf/1811.11168.pdf.](https://arxiv.org/pdf/1811.11168.pdf.)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '‌Zhu, X., Hu, H., Lin, S. 和 Dai, J. (n.d.). *Deformable ConvNets v2: 更可变形，效果更佳*。[在线]
    可在：[https://arxiv.org/pdf/1811.11168.pdf.](https://arxiv.org/pdf/1811.11168.pdf.)'
- en: '‌Wang, W., Dai, J., Chen, Z., Huang, Z., Li, Z., Zhu, X., Hu, X., Lu, T., Lu,
    L., Li, H., Wang, X. and Qiao, Y. (n.d.). *InternImage: Exploring Large-Scale
    Vision Foundation Models with Deformable Convolutions*. [online] Available at:
    [https://arxiv.org/pdf/2211.05778.pdf](https://arxiv.org/pdf/2211.05778.pdf) [Accessed
    31 Jul. 2023].'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '‌Wang, W., Dai, J., Chen, Z., Huang, Z., Li, Z., Zhu, X., Hu, X., Lu, T., Lu,
    L., Li, H., Wang, X. 和 Qiao, Y. (n.d.). *InternImage: 利用可变形卷积探索大规模视觉基础模型*。[在线]
    可在：[https://arxiv.org/pdf/2211.05778.pdf](https://arxiv.org/pdf/2211.05778.pdf)
    [访问日期 2023年7月31日]。'
- en: 'Chollet, F. (n.d.). *Xception: Deep Learning with Depthwise Separable Convolutions*.
    [online] Available at: [https://arxiv.org/pdf/1610.02357.pdf.](https://arxiv.org/pdf/1610.02357.pdf.)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 'Chollet, F. (n.d.). *Xception: 深度学习中的深度可分离卷积*。[在线] 可在：[https://arxiv.org/pdf/1610.02357.pdf.](https://arxiv.org/pdf/1610.02357.pdf.)'
- en: ‌Krizhevsky, A., Sutskever, I. and Hinton, G.E. (2012). ImageNet classification
    with deep convolutional neural networks. *Communications of the ACM*, 60(6), pp.84–90\.
    doi:https://doi.org/10.1145/3065386.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ‌Krizhevsky, A., Sutskever, I. 和 Hinton, G.E. (2012). 使用深度卷积神经网络的ImageNet分类。*ACM通讯*,
    60(6), 页84–90\. doi:https://doi.org/10.1145/3065386.
- en: 'Dai, J., Li, Y., He, K. and Sun, J. (n.d.). *R-FCN: Object Detection via Region-based
    Fully Convolutional Networks*. [online] Available at: [https://arxiv.org/pdf/1605.06409v2.pdf.](https://arxiv.org/pdf/1605.06409v2.pdf.)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 'Dai, J., Li, Y., He, K. 和 Sun, J. (n.d.). *R-FCN: 基于区域的全卷积网络进行物体检测*。[在线] 可在：[https://arxiv.org/pdf/1605.06409v2.pdf.](https://arxiv.org/pdf/1605.06409v2.pdf.)'
- en: ‌‌
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ‌
- en: ‌
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ‌
