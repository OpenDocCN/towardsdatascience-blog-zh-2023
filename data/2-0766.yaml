- en: Drivable Space in Autonomous Driving — The Academia
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动驾驶中的可驾驶空间 — 学术界
- en: 原文：[https://towardsdatascience.com/drivable-space-in-autonomous-driving-a-review-of-academia-ef1a6aa4dc15](https://towardsdatascience.com/drivable-space-in-autonomous-driving-a-review-of-academia-ef1a6aa4dc15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/drivable-space-in-autonomous-driving-a-review-of-academia-ef1a6aa4dc15](https://towardsdatascience.com/drivable-space-in-autonomous-driving-a-review-of-academia-ef1a6aa4dc15)
- en: Recent trends in academic research on drivable space as of 2023
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于2023年可驾驶空间的学术研究的最新趋势
- en: '[](https://medium.com/@patrickllgc?source=post_page-----ef1a6aa4dc15--------------------------------)[![Patrick
    Langechuan Liu](../Images/fecbf85146a9bde21e6b2251538ddd65.png)](https://medium.com/@patrickllgc?source=post_page-----ef1a6aa4dc15--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ef1a6aa4dc15--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ef1a6aa4dc15--------------------------------)
    [Patrick Langechuan Liu](https://medium.com/@patrickllgc?source=post_page-----ef1a6aa4dc15--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@patrickllgc?source=post_page-----ef1a6aa4dc15--------------------------------)[![Patrick
    Langechuan Liu](../Images/fecbf85146a9bde21e6b2251538ddd65.png)](https://medium.com/@patrickllgc?source=post_page-----ef1a6aa4dc15--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ef1a6aa4dc15--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ef1a6aa4dc15--------------------------------)
    [Patrick Langechuan Liu](https://medium.com/@patrickllgc?source=post_page-----ef1a6aa4dc15--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ef1a6aa4dc15--------------------------------)
    ·13 min read·May 18, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----ef1a6aa4dc15--------------------------------)
    ·13分钟阅读·2023年5月18日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Drivable space, or Free Space, plays a safety-critical role in autonomous driving.
    In a previous blog post, we reviewed the definition and importance of this often-overlooked
    perception feature. In this article, we will review recent trends in academic
    research.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 可驾驶空间，或称为自由空间，在自动驾驶中扮演着至关重要的安全角色。在上一篇博客文章中，我们回顾了这一经常被忽视的感知特征的定义和重要性。在本文中，我们将回顾近期学术研究中的趋势。
- en: '[](/drivable-space-in-autonomous-driving-the-concept-df699bb8682f?source=post_page-----ef1a6aa4dc15--------------------------------)
    [## Drivable Space in Autonomous Driving — The Concept'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/drivable-space-in-autonomous-driving-the-concept-df699bb8682f?source=post_page-----ef1a6aa4dc15--------------------------------)
    [## 自动驾驶中的可驾驶空间 — 概念'
- en: The what and why of drivable space
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可驾驶空间的定义与原因
- en: towardsdatascience.com](/drivable-space-in-autonomous-driving-the-concept-df699bb8682f?source=post_page-----ef1a6aa4dc15--------------------------------)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/drivable-space-in-autonomous-driving-the-concept-df699bb8682f?source=post_page-----ef1a6aa4dc15--------------------------------)
- en: 'Drivable space detection algorithms can be measured in two dimensions: inputs
    and outputs. Regarding the **input sensor modality**, drivable space detection
    methods can be categorized as vision-based, LiDAR-based, or vision-LiDAR fusion
    methods. Regarding the **output spatial representation**, they can be categorized
    as 2D perspective image space, 3D space, and Bird’s Eye View (BEV) space.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 可驾驶空间检测算法可以在两个维度上进行测量：输入和输出。关于**输入传感器模态**，可驾驶空间检测方法可以分为基于视觉的、基于激光雷达的或视觉-激光雷达融合的方法。关于**输出空间表示**，它们可以分为2D透视图像空间、3D空间和鸟瞰视图（BEV）空间。
- en: 'Visual images are intrinsically 2D, and lidar point cloud measurements are
    intrinsically 3D. As discussed in [a previous blog post](/monocular-bev-perception-with-transformers-in-autonomous-driving-c41e4a893944),
    BEV space is essentially a simplified or degenerate 3D space, and we will use
    BEV space and 3D space interchangeably throughout this blog. In essence, we have
    a 2x2 input-output matrix for the evaluation of all drivable space algorithms,
    as shown in the image below. The top right quadrant in green is the North Star
    which has the best representation power while being the most cost-effective. In
    the following sections, we will discuss various algorithms that fall into three
    categories: **2D-to-2D, 3D-to-3D, and 2D-to-3D**.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉图像本质上是2D的，而激光雷达点云测量本质上是3D的。正如在[上一篇博客文章](/monocular-bev-perception-with-transformers-in-autonomous-driving-c41e4a893944)中讨论的那样，BEV空间本质上是简化的或退化的3D空间，我们将在本博客中将BEV空间和3D空间互换使用。实质上，我们有一个2x2的输入输出矩阵用于评估所有可驾驶空间算法，如下图所示。绿色的右上象限是北极星，它具有最佳的表现力同时也是最具成本效益的。在接下来的章节中，我们将讨论三类算法：**2D-to-2D、3D-to-3D和2D-to-3D**。
- en: '![](../Images/2fc36e8aad7caa9e28b19941dd748157.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2fc36e8aad7caa9e28b19941dd748157.png)'
- en: The perception algorithm paradigm matrix (image created by author)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 感知算法范式矩阵（图像由作者创建）
- en: It is worth noting that there is currently no universally recognized standard
    for expressing and evaluating the accuracy of drivable space in autonomous driving.
    In this post, we will review related tasks, which can take a variety of formulations.
    We also provide some insights on the future direction of the academic community,
    with the aim of accelerating research on this crucial task in autonomous driving.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，目前还没有一个普遍认可的标准来表达和评估自动驾驶中可驾驶空间的准确性。在这篇文章中，我们将回顾相关任务，这些任务可以有多种公式化方式。我们还提供了一些对学术界未来方向的见解，旨在加速对这一关键任务的研究。
- en: 2D-to-2D Methods (with Images)
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2D 到 2D 方法（带图像）
- en: 'The detection of drivable space in perspective 2D image space is essentially
    the task of image segmentation. There are two main approaches: one is *stixel*-based
    obstacle detection, and the other is drivable space semantic segmentation.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在透视 2D 图像空间中检测可驾驶空间本质上是图像分割的任务。主要有两种方法：一种是基于*stixel*的障碍物检测，另一种是可驾驶空间的语义分割。
- en: The stixel representation
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Stixel 表示法
- en: The concept of the [*stixel* (a combination of *stick* and *pixel*)](https://en.wikipedia.org/wiki/Stixel)
    approach assumes that the area corresponding to the pixel at the bottom of the
    image is drivable from the driving perspective. It then extends and grows a stick
    toward the top of the image until it encounters obstacles, thus obtaining a drivable
    space for that column. One of the most representative works in this approach is
    the [StixelNet](http://www.bmva.org/bmvc/2015/papers/paper109/paper109.pdf) [series](https://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w3/Garnett_Real-Time_Category-Based_and_ICCV_2017_paper.pdf).
    Stixel abstracts general obstacles on the ground into sticks and divides image
    space into drivable space and obstacles. The stixel representation strikes a good
    balance between pixels and objects, reaching good accuracy and efficiency.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[*stixel*（*stick* 和 *pixel* 的组合）](https://en.wikipedia.org/wiki/Stixel) 方法的概念假设图像底部的像素对应的区域从驾驶角度来看是可驾驶的。然后，它向图像顶部延伸并生长一个棍子，直到遇到障碍物，从而获得该列的可驾驶空间。该方法最具代表性的工作之一是
    [StixelNet](http://www.bmva.org/bmvc/2015/papers/paper109/paper109.pdf) [系列](https://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w3/Garnett_Real-Time_Category-Based_and_ICCV_2017_paper.pdf)。Stixel
    将地面上的一般障碍物抽象为棍子，并将图像空间划分为可驾驶空间和障碍物。Stixel 表示法在像素和对象之间取得了良好的平衡，实现了良好的准确性和效率。'
- en: '![](../Images/34b37da9bb0da69b9169e44aac6599a9.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/34b37da9bb0da69b9169e44aac6599a9.png)'
- en: '*An illustration of the stixel concept in the detection of drivable space (Source:*
    [*StixelNet for segmentation*](https://arxiv.org/abs/2107.03070)*)*'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*Stixel 概念在可驾驶空间检测中的示意图（来源：*[StixelNet for segmentation*](https://arxiv.org/abs/2107.03070)*)*'
- en: Semantic segmentation
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语义分割
- en: Deep learning has made rapid progress in recent years, allowing detection to
    be directly modeled as a semantic segmentation problem using convolutional neural
    networks (CNNs). This approach differs from the method based on image list representation,
    as it directly classifies whether the pixels of 2D images are drivable spaces.
    Typical work include [DS-Generator](https://arxiv.org/abs/2012.07890) and [RPP](https://link.springer.com/article/10.1007/s12559-017-9524-y).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习近年来取得了快速进展，使得检测可以直接通过卷积神经网络（CNN）建模为语义分割问题。这种方法与基于图像列表表示的方法不同，因为它直接对 2D 图像的像素进行是否为可驾驶空间的分类。典型的工作包括
    [DS-Generator](https://arxiv.org/abs/2012.07890) 和 [RPP](https://link.springer.com/article/10.1007/s12559-017-9524-y)。
- en: Compared to the Stixel approach, the general semantic segmentation approach
    is more flexible. However, it requires more complex post-processing to make it
    useful for downstream components. For example, the semantic prediction may not
    be as contiguous (as clean as the result shown in the following illustration).
    In the Stixel approach, only one pixel per column is selected to be converted
    to 3D information.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Stixel 方法相比，一般的语义分割方法更为灵活。然而，它需要更复杂的后处理以使其对下游组件有用。例如，语义预测可能没有那么连续（不像以下示例中显示的结果那样干净）。在
    Stixel 方法中，每列只选择一个像素以转换为 3D 信息。
- en: '![](../Images/51210d9381631ab8775d6d43fc2170a8.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51210d9381631ab8775d6d43fc2170a8.png)'
- en: 'Semantic segmentation The formulation of drivable space as (source: [DS-Generator](https://arxiv.org/abs/2012.07890))'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割 将可驾驶空间公式化为（来源：[DS-Generator](https://arxiv.org/abs/2012.07890)）
- en: Lifting to 3D
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升到 3D
- en: Since the downstream tasks of prediction and planning calculation occur in 3D
    space, it is necessary to convert the drivable space results obtained on 2D images
    to 3D or the degenerated BEV. Common 2D-to-3D post-processing techniques include
    [Inverse Perspective Mapping (IPM)](/monocular-birds-eye-view-semantic-segmentation-for-autonomous-driving-ee2f771afb59),
    monocular/stereo depth estimation, and using direct 3D physical measurements such
    as LiDAR point clouds. In addition, the 2D-to-2D algorithm takes each camera stream
    separately and thus needs explicit rules to piece them together for 360 deg perception
    in a multi-camera setup.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 由于预测和规划计算的下游任务发生在3D空间中，因此需要将2D图像中获得的可行驶空间结果转换为3D或退化的BEV。常见的2D到3D后处理技术包括[逆透视映射
    (IPM)](/monocular-birds-eye-view-semantic-segmentation-for-autonomous-driving-ee2f771afb59)、单目/立体深度估计以及使用直接的3D物理测量，如激光雷达点云。此外，2D到2D算法将每个相机流单独处理，因此需要明确的规则将它们拼接在一起，以实现多相机设置中的360度感知。
- en: The tedious postprocessing in both 2D perspective space and 2D-to-3D conversion
    are typically handcrafted and these brittle logics are susceptible to corner cases.
    In reality, 2D-to-2D algorithms are rarely used in autonomous driving, except
    in low-speed scenarios such as parking.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在2D透视空间和2D到3D转换中的繁琐后处理通常是手工制作的，这些脆弱的逻辑容易受到特殊情况的影响。实际上，2D到2D算法在自动驾驶中很少使用，除了低速场景如停车。
- en: '![](../Images/09801a16898992325219337b6037033e.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09801a16898992325219337b6037033e.png)'
- en: 2D DS needs to be lifted to 3D DS for downstream consumption (image created
    by author)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 2D DS需要提升到3D DS以供下游使用（图像由作者创建）
- en: 3D-to-3D Methods (with Lidar)
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3D到3D方法（配合激光雷达）
- en: These 3D drivable space algorithms take in lidar point clouds and generate 3D
    drivable space directly. While early studies are mostly based on LiDAR, recently
    (in early 2023) we see an explosion of vision-based semantic 3D occupancy prediction,
    which we will explore in the next section.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这些3D可行驶空间算法接收激光雷达点云并直接生成3D可行驶空间。虽然早期研究主要基于激光雷达，但最近（2023年初）我们看到基于视觉的语义3D占用预测的爆炸性增长，这将在下一节中深入探讨。
- en: Lidar Ground segmentation
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激光雷达地面分割
- en: Ground segmentation-based methods aim to divide LiDAR point cloud data into
    ground and non-ground parts. These methods can be categorized into geometric rule-based
    algorithms and deep learning-based algorithms. Even before the widespread adoption
    of deep learning, algorithms based on geometric rules were already widely used
    in LiDAR point clouds to achieve ground detection (or the complementary task of
    curb detection) and general obstacle detection tasks. These efforts typically
    rely on plane fitting and region growth algorithms, which were first introduced
    in 2007 and 2009 in the [DARPA Urban Challenge](https://link.springer.com/article/10.1007/BF03224975).
    However, simple ground plane assumptions fail when encountering uneven ground,
    potholes, and uphill-and-downhill scenes. To account for local uneven roads and
    overall smoothness, [several](https://link.springer.com/article/10.1007/s10846-013-9889-4)
    [studies](https://arxiv.org/abs/2111.10638) proposed introducing algorithm optimization
    based on the Gaussian process.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 基于地面分割的方法旨在将激光雷达点云数据分为地面部分和非地面部分。这些方法可以分为几何规则基础算法和基于深度学习的算法。即使在深度学习广泛应用之前，基于几何规则的算法也已广泛应用于激光雷达点云中，以实现地面检测（或边石检测的补充任务）和一般障碍物检测任务。这些方法通常依赖于平面拟合和区域生长算法，这些算法首次在2007年和2009年于[DARPA
    Urban Challenge](https://link.springer.com/article/10.1007/BF03224975)中介绍。然而，简单的地面平面假设在遇到不平整的地面、坑洞以及上下坡场景时会失败。为了考虑局部不平整的道路和整体平滑度，[一些](https://link.springer.com/article/10.1007/s10846-013-9889-4)[研究](https://arxiv.org/abs/2111.10638)建议引入基于高斯过程的算法优化。
- en: Deep learning-based approaches are gaining popularity with the availability
    of more computational resources and large-scale datasets. The task of ground segmentation
    can be formulated as a general semantic segmentation of the lidar point cloud.
    [LidarMTL](https://arxiv.org/abs/2103.04056) is one typical work that proposes
    a multitask model with six tasks, including road structure understanding added
    on top of dynamic obstacle detection. For road scene understanding, two semantic
    segmentation tasks of drivable space and ground are designed, along with a ground
    height regression task. Interestingly, auxiliary tasks, such as foreground segmentation
    and intra-object part location, have also been shown to benefit dynamic object
    detection.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的方法由于计算资源和大规模数据集的增加而逐渐流行。地面分割任务可以被形式化为对LiDAR点云的通用语义分割。[LidarMTL](https://arxiv.org/abs/2103.04056)
    是一个典型的工作，提出了一个具有六个任务的多任务模型，其中包括动态障碍物检测之上的道路结构理解。对于道路场景理解，设计了两个语义分割任务：可驾驶空间和地面，并且还有一个地面高度回归任务。有趣的是，辅助任务，例如前景分割和物体内部部件定位，也被证明对动态物体检测有帮助。
- en: '![](../Images/79c23a0dc4bed69de114a2875488268a.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/79c23a0dc4bed69de114a2875488268a.png)'
- en: '*Multitask semantic segmentation of lidar point cloud (source:* [*LidarMTL*](https://arxiv.org/abs/2103.04056)*)*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*LiDAR点云的多任务语义分割（来源：* [*LidarMTL*](https://arxiv.org/abs/2103.04056)*)*'
- en: Freespace-Centric representation
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 以自由空间为中心的表示
- en: '[Freespace Forecaster](https://openaccess.thecvf.com/content/CVPR2021/html/Hu_Safe_Local_Motion_Planning_With_Self-Supervised_Freespace_Forecasting_CVPR_2021_paper.html)
    and [its differentiable version](https://arxiv.org/abs/2210.01917) use freespace-centric
    representation to predict a drivable space for motion planning. These methods
    obtain ground and obstacle point cloud or grid information from the vehicle as
    the center, in polar coordinates, through simple ray-casting to calculate the
    accessibility relationship. The representation is quite similar to the Stixel
    representation in 2D perspective space in the sense that the closest obstacle
    is located per bin (stixel in 2D and polar angle bin in 3D).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[Freespace Forecaster](https://openaccess.thecvf.com/content/CVPR2021/html/Hu_Safe_Local_Motion_Planning_With_Self-Supervised_Freespace_Forecasting_CVPR_2021_paper.html)
    和 [其可微分版本](https://arxiv.org/abs/2210.01917) 使用以自由空间为中心的表示来预测用于运动规划的可驾驶空间。这些方法从车辆作为中心，通过简单的射线投射获取地面和障碍物的点云或网格信息，使用极坐标计算可达关系。这种表示方式与2D透视空间中的Stixel表示非常相似，最近的障碍物位于每个区间内（2D中的stixel和3D中的极角区间）。'
- en: '![](../Images/9a8a65540e5443f77b64f120d0cd8d53.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9a8a65540e5443f77b64f120d0cd8d53.png)'
- en: '*Freespace-centric representation for freespace forecasting (source:* [*Differentiable
    Raycasting*](https://arxiv.org/abs/2210.01917)*)*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*以自由空间为中心的表示用于自由空间预测（来源：* [*可微分射线投射*](https://arxiv.org/abs/2210.01917)*)*'
- en: Occupancy Grid and Scene Flow Representation
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 占用网格和场景流表示
- en: Among LiDAR-based algorithms, the most general approach uses Occupancy Grid
    and Scene Flow to express the position and movement of general obstacles, respectively.
    Two representative papers are [MotionNet](https://arxiv.org/abs/2003.06754) and
    [PointMotionNet](https://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Wang_PointMotionNet_Point-Wise_Motion_Learning_for_Large-Scale_LiDAR_Point_Clouds_Sequences_CVPRW_2022_paper.pdf).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于LiDAR的算法中，最通用的方法使用占用网格和场景流来分别表示一般障碍物的位置和移动。两个具有代表性的论文是 [MotionNet](https://arxiv.org/abs/2003.06754)
    和 [PointMotionNet](https://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Wang_PointMotionNet_Point-Wise_Motion_Learning_for_Large-Scale_LiDAR_Point_Clouds_Sequences_CVPRW_2022_paper.pdf)。
- en: Dynamic object detection and tracking tasks are prevalent in public datasets,
    enabling academic researchers to generate ground truth for occupancy grid and
    scene flow. Occupancy representation, *in theory*, allows for the detection of
    general obstacles, covering unknown objects with arbitrary shapes, as well as
    static obstacles and road structures. However, *in practice,* quantifying the
    effectiveness of the algorithm is challenging, as the majority of objects in public
    datasets are regular. **A dataset and benchmark for the detection of general obstacles
    are necessary to further advance the research in this field.**
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 动态物体检测和跟踪任务在公共数据集中很常见，使得学术研究人员能够生成占用网格和场景流的真实标签。占用表示，*理论上*，可以检测一般障碍物，涵盖具有任意形状的未知物体，以及静态障碍物和道路结构。然而，*在实践中*，量化算法的有效性是具有挑战性的，因为公共数据集中大多数物体是规则的。**为了进一步推进这一领域的研究，需要一个检测一般障碍物的数据集和基准。**
- en: '![](../Images/025ca3e9e5f0191987fd53d7c4875679.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/025ca3e9e5f0191987fd53d7c4875679.png)'
- en: '*Occupancy and Flow prediction with point cloud (source:* [MotionNet](https://arxiv.org/abs/2003.06754)*)*'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*点云中的占用与流量预测（来源：* [MotionNet](https://arxiv.org/abs/2003.06754)*)*'
- en: At first glance, it may seem counterintuitive that an object detection algorithm
    can generate ground-truth to power even more powerful and flexible algorithms
    of occupancy prediction. However, there are two important aspects to consider.
    Firstly, object detection can assist with the heavy lifting and bootstrapping
    during ground-truth annotation, while **human quality assurance and minor adjustments
    are still necessary in real production environment**. Secondly, algorithm formulation
    plays a crucial role. The formulation of occupancy prediction makes it more flexible
    and capable of learning subtleties that may have been missed by object detection.
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 乍一看，物体检测算法生成的真实数据可以为更强大、更灵活的占用预测算法提供支持似乎不符合直觉。然而，有两个重要方面需要考虑。首先，物体检测可以帮助完成真实数据标注过程中的繁重工作和启动工作，而**在实际生产环境中仍需人工质量保证和细微调整**。其次，算法的制定起着关键作用。占用预测的制定使其更具灵活性，能够学习物体检测可能遗漏的细微差别。
- en: The lidar-based occupancy algorithms are not to be confused with the Occupancy
    Network proposed by Tesla. The Occupancy Network idea is a vision-centric algorithm.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 基于激光雷达的占用算法与特斯拉提出的占用网络（Occupancy Network）不同。占用网络的概念是以视觉为中心的算法。
- en: 2D-to-3D Method (BEV perception and more)
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2D到3D的方法（BEV感知及更多）
- en: The multi-camera BEV perception framework (see [my previous blog post on BEV
    perception](/monocular-bev-perception-with-transformers-in-autonomous-driving-c41e4a893944))
    raises the visual 3D perception performance to a new level by simplifying the
    steps of multi-camera post-processing and post-fusion. Additionally, the framework
    successfully unifies camera and LiDAR algorithms in the expression space, providing
    a convenient framework for sensor fusion, including both front and post-fusion.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 多摄像头BEV感知框架（参见[我之前关于BEV感知的博客文章](/monocular-bev-perception-with-transformers-in-autonomous-driving-c41e4a893944)）通过简化多摄像头后处理和后融合步骤，将视觉3D感知性能提升到一个新的水平。此外，该框架成功地在表达空间中统一了摄像头和LiDAR算法，为传感器融合提供了便捷的框架，包括前融合和后融合。
- en: 'Detecting the physical edge of a road in BEV space is a subset of the drivable
    space detection task. While road edges and lane lines can be expressed by vector
    lines, road boundaries lack constraints such as parallelism, fixed lane width,
    and the intersection at extinction points, making their shapes and positions more
    flexible. These more free and diverse road edges can be modeled in several ways:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在BEV空间中检测道路的物理边缘是可驾驶空间检测任务的一个子集。虽然道路边缘和车道线可以用矢量线表示，但道路边界缺乏如平行性、固定车道宽度以及消失点的交集等约束，使得其形状和位置更加灵活。这些更加自由和多样的道路边缘可以通过几种方式建模：
- en: 'Heatmap-based: a heatmap is produced by a semantic segmentation-like decoder.
    The heatmap needs to be processed to vector elements to be consumed by downstream
    components.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于热图的：由类似语义分割的解码器生成热图。热图需要处理为矢量元素，以供下游组件使用。
- en: 'Voxel-based: an extension of heatmap-based method. The 2D BEV grid in a heatmap
    is extended to 3D voxels.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于体素的：热图方法的扩展。热图中的2D BEV网格扩展为3D体素。
- en: 'Vector-based: vectorized output is produced, based on primitive geometric elements
    such as polylines and polygons. These outputs can be passed for downstream consumption
    directly.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于矢量的：生成基于原始几何元素（如多边形和多边形）的矢量化输出。这些输出可以直接传递给下游使用。
- en: Heatmap decoder
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 热图解码器
- en: 'The semantic segmentation-based method can be divided into two categories based
    on the target modeling approach: **Road Boundary** semantic segmentation and **Road
    Layout** semantic segmentation. The former, such as [HDMapNet](https://arxiv.org/abs/2107.06307),
    can predict lane lines while outputting road edges. The neural network output
    is a heat map that requires binarization and clustering to produce vector outputs
    that can be used for downstream prediction and regulation.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 基于语义分割的方法可以根据目标建模方法分为两类：**道路边界**语义分割和**道路布局**语义分割。前者，如[HDMapNet](https://arxiv.org/abs/2107.06307)，可以预测车道线，同时输出道路边缘。神经网络的输出是一个热图，需要进行二值化和聚类，以生成可用于下游预测和调节的矢量输出。
- en: '![](../Images/b0c02ae42a48093216acab46a7e4b111.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b0c02ae42a48093216acab46a7e4b111.png)'
- en: '*Architecture diagram of HDMapNet (source:* [HDMapNet](https://arxiv.org/abs/2107.06307))'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*HDMapNet的架构图（来源：* [HDMapNet](https://arxiv.org/abs/2107.06307))'
- en: Other methods are based on semantic segmentation of the road structure, such
    as [PETRv2](https://arxiv.org/abs/2206.01256), [CVT](https://arxiv.org/abs/2205.02833),
    and [Monolayout](https://arxiv.org/abs/2002.08394). The output is the road body
    itself, and its edge is the road boundary. The neural network output is still
    a heat map that needs to be binarized, and taking the edge operation can obtain
    the vectorized road boundary. If downstream regulation can directly consume the
    road structure itself, such as planning based on occupying a grid, using this
    perception method is more direct. However, this topic is more related to regulation
    and control, so I will not expand on it in detail here.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其他方法基于道路结构的语义分割，如[PETRv2](https://arxiv.org/abs/2206.01256)、[CVT](https://arxiv.org/abs/2205.02833)和[Monolayout](https://arxiv.org/abs/2002.08394)。输出是道路本身，其边缘是道路边界。神经网络输出仍然是需要二值化的热图，通过边缘操作可以得到向量化的道路边界。如果下游应用可以直接消耗道路结构本身，如基于占用网格的规划，使用这种感知方法会更直接。然而，这一话题更多与规制和控制相关，因此我在此不会详细展开。
- en: '![](../Images/740fdc45a50ac8e6910c8c55fbc6867a.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/740fdc45a50ac8e6910c8c55fbc6867a.png)'
- en: '*Architecture diagram of CVT (source:* [CVT](https://arxiv.org/abs/2205.02833))'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*CVT的架构图（来源:* [CVT](https://arxiv.org/abs/2205.02833))'
- en: Voxel decoder (semantic occupancy prediction)
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 体素解码器（语义占用预测）
- en: The idea of vision-only occupancy prediction has seen explosive growth in the
    early months of 2023, following the proposal of the Occupancy Network by Tesla
    in 2022\. This voxel output representation can be seen as an extension of the
    heatmap representation, with one additional dimension of height predicted for
    each heatmap grid location.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 随着特斯拉在2022年提出占用网络的提案，2023年初，单纯基于视觉的占用预测取得了爆炸性增长。这种体素输出表示可以看作是热图表示的扩展，每个热图网格位置预测一个额外的高度维度。
- en: One notable work in this area is [SurroundOcc](https://arxiv.org/abs/2303.09551).
    It first designed an automatic pipeline to generate dense occupancy ground truth
    from sparse point clouds and then used this dense ground truth to supervise the
    learning of a dense occupancy grid from multi-camera image streams. Please refer
    to [this literature review of semantic occupancy prediction](https://medium.com/p/16a46dbd6f65),
    for detailed comparisons of various approaches and discussions on potential roadblocks
    to industrial deployment.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一领域，一个值得注意的工作是[SurroundOcc](https://arxiv.org/abs/2303.09551)。它首先设计了一个自动化流程，从稀疏点云中生成密集占用真值，然后利用这些密集真值来监督从多相机图像流中学习密集占用网格。有关各种方法的详细比较和工业应用中的潜在障碍，请参阅[这篇语义占用预测的文献综述](https://medium.com/p/16a46dbd6f65)。
- en: '[](/vision-centric-semantic-occupancy-prediction-for-autonomous-driving-16a46dbd6f65?source=post_page-----ef1a6aa4dc15--------------------------------)
    [## Vision-centric Semantic Occupancy Prediction for Autonomous Driving'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 以视觉为中心的自动驾驶语义占用预测](https://vision-centric-semantic-occupancy-prediction-for-autonomous-driving-16a46dbd6f65?source=post_page-----ef1a6aa4dc15--------------------------------)'
- en: A literature review of the academic “Occupancy Network” as of 2023H1
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2023年上半年的学术“占用网络”文献综述
- en: towardsdatascience.com](/vision-centric-semantic-occupancy-prediction-for-autonomous-driving-16a46dbd6f65?source=post_page-----ef1a6aa4dc15--------------------------------)
    ![](../Images/40b7ee9cf193666053222c4943a47f89.png)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](https://vision-centric-semantic-occupancy-prediction-for-autonomous-driving-16a46dbd6f65?source=post_page-----ef1a6aa4dc15--------------------------------)
    ![](../Images/40b7ee9cf193666053222c4943a47f89.png)'
- en: 'Annotation and training pipeline of semantic occupancy prediction (source:
    [SurroundOcc](https://arxiv.org/abs/2303.09551))'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 语义占用预测的标注和训练流程（来源：[SurroundOcc](https://arxiv.org/abs/2303.09551)）
- en: Vector decoder
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量解码器
- en: Both the heatmap-based and voxel-based methods are based on semantic segmentation
    and require heavy post-processing for downstream consumption. In comparison, methods
    based on direct vector output are more straightforward. Representative methods
    include [STSU](https://arxiv.org/pdf/2110.01997.pdf), [MapTR](https://arxiv.org/abs/2208.14437),
    and [VectorMapNet](https://arxiv.org/abs/2206.08920), which directly output **vectorized
    road edges**. This method can be considered a variant of the anchor-based object
    detection method, where the basic geometric elements are multi-segment lines or
    polygons with 2N degrees of freedom, where N is the number of points of the multi-segment
    line or polygon. [MapTR](https://arxiv.org/abs/2208.14437) and [VectorMapNet](https://arxiv.org/abs/2206.08920)
    are examples of such methods. It is worth mentioning that [STSU](https://arxiv.org/pdf/2110.01997.pdf)
    uses the Bezier Curve with 3 control points, which is novel but not as flexible
    as multi-segment lines and polygons, and its current effect is not as good.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 基于热图和体素的方法都依赖于语义分割，并且需要大量后处理来满足下游需求。相比之下，基于直接向量输出的方法更加直接。代表性的方法包括[STSU](https://arxiv.org/pdf/2110.01997.pdf)、[MapTR](https://arxiv.org/abs/2208.14437)和[VectorMapNet](https://arxiv.org/abs/2206.08920)，这些方法直接输出**向量化道路边缘**。这种方法可以视为基于锚点的目标检测方法的变体，其中基本几何元素是具有2N自由度的多段线或多边形，其中N是多段线或多边形的点数。[MapTR](https://arxiv.org/abs/2208.14437)和[VectorMapNet](https://arxiv.org/abs/2206.08920)就是这样的例子。值得一提的是，[STSU](https://arxiv.org/pdf/2110.01997.pdf)使用了具有3个控制点的Bezier曲线，这虽然新颖，但不如多段线和多边形灵活，其当前效果也不如后者。
- en: '![](../Images/9bc1ea106634a4efdf7f0e1dbcbf4776.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9bc1ea106634a4efdf7f0e1dbcbf4776.png)'
- en: '*Vectorization in the BEV decoder (Source:* [MapTR](https://arxiv.org/abs/2208.14437))'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*BEV解码器中的向量化（来源：* [MapTR](https://arxiv.org/abs/2208.14437))'
- en: Camera and Lidar fusion
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摄像头和激光雷达融合
- en: The above 2D-to-3D methods leverage the rich texture and semantic information
    of camera images to reason about the 3D environment around the ego vehicle. While
    LiDAR point cloud data may lack these rich semantics, it provides accurate 3D
    position measurement information.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 上述2D到3D方法利用了摄像头图像中的丰富纹理和语义信息来推断自车周围的3D环境。虽然激光雷达点云数据可能缺乏这些丰富的语义，但它提供了准确的3D位置测量信息。
- en: Multi-camera BEV space and LiDAR BEV space can be easily unified in BEV fusion.
    For example, in [HDMapNet](https://arxiv.org/abs/2107.06307), the LiDAR-only method
    and the Camera-LiDAR Fusion method are also compared to the camera-only baseline.
    Even though the camera may perform slightly worse than the LiDAR-only method in
    BEV positioning, multi-camera IoU indicators are still better for lane dividers
    and pedestrian crossing, while LiDAR is better at detecting road boundaries. This
    is understandable as road boundaries are typically accompanied by height changes
    and are easier to be detected by active lidar measurements.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 多摄像头BEV空间和激光雷达BEV空间可以在BEV融合中轻松统一。例如，在[HDMapNet](https://arxiv.org/abs/2107.06307)中，激光雷达单独方法和摄像头-激光雷达融合方法也与摄像头单独基线进行了比较。尽管摄像头在BEV定位中可能表现稍差于激光雷达单独方法，但多摄像头IoU指标在车道分隔线和行人过街处仍然更好，而激光雷达在检测道路边界方面更优。这是可以理解的，因为道路边界通常伴随高度变化，更容易通过主动激光雷达测量来检测。
- en: '![](../Images/a07b5275905a3047ff7ab9d55ca749dc.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a07b5275905a3047ff7ab9d55ca749dc.png)'
- en: '*Performance comparison between vision and lidar for 2D-to-3D DS algorithm
    (source:* [HDMapNet](https://arxiv.org/abs/2107.06307))'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*2D到3D DS算法中视觉和激光雷达的性能比较（来源：* [HDMapNet](https://arxiv.org/abs/2107.06307))'
- en: Public Dataset
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 公共数据集
- en: Currently, there is no widely accepted dataset for driveable space in autonomous
    driving. However, there are some related tasks, such as point cloud segmentation
    in the 3D-to-3D scheme and HD map prediction for the 2D-to-3D scheme. Unfortunately,
    the cost of producing 3D point cloud segmentation and HD maps is high, which limits
    academic research to a few public datasets. These datasets include NuScenes, Waymo,
    KITTI360, and Lyft. These datasets offer 3D point cloud segmentation and labeling,
    with some road information like road surface, roadside, and crosswalk. The Lyft
    dataset also includes map information of the road area, which can help understand
    the road layout.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 目前在自动驾驶领域，没有广泛接受的可驾驶空间数据集。然而，有一些相关任务，如3D到3D方案中的点云分割和2D到3D方案中的HD地图预测。不幸的是，生成3D点云分割和HD地图的成本较高，这限制了学术研究使用少数公共数据集。这些数据集包括NuScenes、Waymo、KITTI360和Lyft。这些数据集提供了3D点云分割和标注，并包含一些道路信息，如路面、路边和人行道。Lyft数据集还包括道路区域的地图信息，有助于理解道路布局。
- en: A public dataset and evaluation metric benchmark needs to be established to
    further the development in this field. Two things need special attention.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 需要建立公共数据集和评估指标基准，以促进该领域的发展。特别需要关注两件事。
- en: '**Long-tail corner cases.** It is essential to pay attention to the detection
    effect of difficult samples to ensure the system’s reliability. However, long-tail
    data can differ in various regions and scenes, and collecting and labeling them
    requires time and technological expertise. It is worth exploring ways to balance
    small sample data and improve the learning effect.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长尾角落情况。** 必须关注困难样本的检测效果，以确保系统的可靠性。然而，长尾数据在不同区域和场景中可能有所不同，收集和标记这些数据需要时间和技术专长。值得探索平衡小样本数据和提高学习效果的方法。'
- en: '**Output format.** The definition and implementation of 3D drivable space are
    closely related to the downstream consumption logic and the design of autonomous
    driving systems. It is challenging to establish a unified standard in the industry,
    and it is not often used as a clear and independently defined module in academic
    research or public dataset competitions. **Polygons may be a flexible enough format.**
    The detected polygons across frames can be evaluated by object detection score
    and the shape consistency across time, as downstream requires consistent shape
    for precise vehicle maneuver.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出格式。** 3D 可驾驶空间的定义和实现与下游消费逻辑及自动驾驶系统设计密切相关。行业中很难建立统一标准，也不常作为学术研究或公共数据集竞赛中明确和独立定义的模块。**多边形可能是一种足够灵活的格式。**
    通过物体检测得分和时间一致性可以评估跨帧检测到的多边形，因为下游需要一致的形状以实现精确的车辆操作。'
- en: Takeaways
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重点
- en: There is no universally accepted definition or evaluation metrics for drivable
    space yet in academia. Relabeling the public datasets is one possible way, but
    long-tail corner cases need to be enriched.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在学术界尚未有普遍接受的可驾驶空间定义或评估指标。重新标记公共数据集是一种可能的方法，但需要丰富长尾角落情况。
- en: 2D image spatial pixel level drivable space detection relies on depth information
    from IPM or other modules, but position error increases with distance. In 3D space,
    LiDAR offers high geometric accuracy but weak semantic classification. It can
    detect road edges and general obstacles using ground fitting and other methods,
    and can also identify unknown dynamic and static obstacles through raycasting
    freespace or Occupancy expressions.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2D 图像空间像素级可驾驶空间检测依赖于来自 IPM 或其他模块的深度信息，但位置误差随着距离增加。在 3D 空间中，LiDAR 提供高几何精度但语义分类较弱。它可以通过地面拟合和其他方法检测道路边缘和一般障碍物，还可以通过光线投射自由空间或占用表达识别未知的动态和静态障碍物。
- en: 2D-to-3D BEV perception methods are promising due to their cost-effectiveness
    and powerful representation capability. However, the lack of a standard for drivable
    space has resulted in various output formats. The output format depends on the
    requirements from downstream planning and control.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2D 到 3D BEV 感知方法因其性价比高和强大的表示能力而具有前景。然而，缺乏可驾驶空间的标准导致了各种输出格式。输出格式取决于下游规划和控制的要求。
- en: 'This is the second post on drivable space, focusing on recent academic progress.
    The first post explored the concept of drivable space. In our next post, we will
    discuss the current industrial applications of drivable space, including how it
    can be extended to general robotics beyond autonomous driving. (Update: the third
    blog post is already out.)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这是关于可驾驶空间的第二篇文章，重点介绍了最近的学术进展。第一篇文章探讨了可驾驶空间的概念。在下一篇文章中，我们将讨论可驾驶空间的当前工业应用，包括如何将其扩展到自动驾驶之外的一般机器人领域。（更新：第三篇博客文章已经发布。）
- en: '[](https://medium.com/@patrickllgc/drivable-space-in-autonomous-driving-the-industry-7a4624b94d41?source=post_page-----ef1a6aa4dc15--------------------------------)
    [## Drivable Space in Autonomous Driving — The Industry'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 自动驾驶中的可驾驶空间——行业](https://medium.com/@patrickllgc/drivable-space-in-autonomous-driving-the-industry-7a4624b94d41?source=post_page-----ef1a6aa4dc15--------------------------------)'
- en: Recent trends in industry application as of 2023
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 截至 2023 年的行业应用最新趋势
- en: medium.com](https://medium.com/@patrickllgc/drivable-space-in-autonomous-driving-the-industry-7a4624b94d41?source=post_page-----ef1a6aa4dc15--------------------------------)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/@patrickllgc/drivable-space-in-autonomous-driving-the-industry-7a4624b94d41?source=post_page-----ef1a6aa4dc15--------------------------------)'
- en: 'Note: All images in this blog post are either created by the author, or from
    academic papers publicly available. See captions for details.'
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：本博客文章中的所有图片要么由作者创建，要么来自公开的学术论文。有关详细信息，请参见说明。
- en: References
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[**StixelNet**: A Deep Convolutional Network for Obstacle Detection and Road
    Segmentation](http://www.bmva.org/bmvc/2015/papers/paper109/paper109.pdf), BMVC
    2015.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**StixelNet**：一种用于障碍物检测和道路分割的深度卷积网络](http://www.bmva.org/bmvc/2015/papers/paper109/paper109.pdf)，BMVC
    2015'
- en: '[Real-Time Category-Based and General Obstacle Detection for Autonomous Driving](https://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w3/Garnett_Real-Time_Category-Based_and_ICCV_2017_paper.pdf),
    ICCV 2018'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[实时基于类别和通用的自动驾驶障碍物检测](https://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w3/Garnett_Real-Time_Category-Based_and_ICCV_2017_paper.pdf)，ICCV
    2018'
- en: '[**DS-Generator**: Learning Collision-Free Space Detection from Stereo Images](https://arxiv.org/abs/2012.07890),
    IEEE/ASME Transactions on Mechatronics 2022'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**DS-Generator**：从立体图像中学习无碰撞空间检测](https://arxiv.org/abs/2012.07890)，IEEE/ASME
    Transactions on Mechatronics 2022'
- en: '**RPP**: [Segmentation of Drivable Road Using Deep Fully Convolutional Residual
    Network with Pyramid Pooling](https://link.springer.com/article/10.1007/s12559-017-9524-y),
    Cognitive Computation, 2018'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RPP**：[使用深度全卷积残差网络与金字塔池化进行可驾驶道路分割](https://link.springer.com/article/10.1007/s12559-017-9524-y)，Cognitive
    Computation，2018'
- en: '[**STSU**: Structured Bird’s-Eye-View Traffic Scene Understanding from Onboard
    Images](https://arxiv.org/pdf/2110.01997.pdf), ICCV 2021'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**STSU**：基于车载图像的结构化鸟瞰视图交通场景理解](https://arxiv.org/pdf/2110.01997.pdf)，ICCV
    2021'
- en: '[**HDMapNet**: An Online HD Map Construction and Evaluation Framework](https://arxiv.org/abs/2107.06307),
    Arxiv 2021'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**HDMapNet**：在线高清地图构建与评估框架](https://arxiv.org/abs/2107.06307)，Arxiv 2021'
- en: '[**PETRv2**: A Unified Framework for 3D Perception from Multi-Camera Images](https://arxiv.org/abs/2206.01256),
    Arxiv 2022'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**PETRv2**：一种统一的多摄像头图像3D感知框架](https://arxiv.org/abs/2206.01256)，Arxiv 2022'
- en: '[**CVT**: Cross-view Transformers for real-time Map-view Semantic Segmentation](https://arxiv.org/abs/2205.02833),
    CVPR 2022'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**CVT**：用于实时地图视图语义分割的跨视图变换器](https://arxiv.org/abs/2205.02833)，CVPR 2022'
- en: '[**Monolayout**: Amodal scene layout from a single image](https://arxiv.org/abs/2002.08394),
    WACV 2020'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Monolayout**：从单张图像中获得的模态场景布局](https://arxiv.org/abs/2002.08394)，WACV 2020'
- en: '[Darpa urban challenge 2007](https://link.springer.com/article/10.1007/BF03224975),
    ATZ worldwide 2008'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Darpa城市挑战2007](https://link.springer.com/article/10.1007/BF03224975)，ATZ worldwide
    2008'
- en: '[The DARPA urban challenge: autonomous vehicles in city traffic](https://link.springer.com/book/10.1007/978-3-642-03991-1),
    springer, 2009.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DARPA城市挑战：城市交通中的自动驾驶车辆](https://link.springer.com/book/10.1007/978-3-642-03991-1)，springer，2009'
- en: '[Gaussian-process-based real-time ground segmentation for autonomous land vehicles](https://link.springer.com/article/10.1007/s10846-013-9889-4),
    Journal of Intelligent & Robotic Systems, 2014'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[基于高斯过程的自动驾驶地面实时分割](https://link.springer.com/article/10.1007/s10846-013-9889-4)，Journal
    of Intelligent & Robotic Systems，2014'
- en: '[A Gaussian Process-Based Ground Segmentation for Sloped Terrains](https://arxiv.org/abs/2111.10638),
    ICRoM 2021'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[基于高斯过程的倾斜地形地面分割](https://arxiv.org/abs/2111.10638)，ICRoM 2021'
- en: '[**LidarMTL**: A Simple and Efficient Multi-task Network for 3D Object Detection
    and Road Understanding](https://arxiv.org/abs/2103.04056), Arxiv 2021'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**LidarMTL**：一种简单高效的多任务网络，用于3D物体检测和道路理解](https://arxiv.org/abs/2103.04056)，Arxiv
    2021'
- en: '**Freespace Forecaster:** [Safe local motion planning with self-supervised
    freespace forecasting](https://openaccess.thecvf.com/content/CVPR2021/html/Hu_Safe_Local_Motion_Planning_With_Self-Supervised_Freespace_Forecasting_CVPR_2021_paper.html),
    CVPR 2021'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Freespace Forecaster：**[基于自监督的安全局部运动规划与自由空间预测](https://openaccess.thecvf.com/content/CVPR2021/html/Hu_Safe_Local_Motion_Planning_With_Self-Supervised_Freespace_Forecasting_CVPR_2021_paper.html)，CVPR
    2021'
- en: '[**MotionNet**: Joint perception and motion prediction for autonomous driving
    based on bird’s eye view maps](https://arxiv.org/abs/2003.06754), CVPR 2020'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**MotionNet**：基于鸟瞰图的自动驾驶联合感知与运动预测](https://arxiv.org/abs/2003.06754)，CVPR
    2020'
- en: '[**PointMotionNet**: Point-Wise Motion Learning for Large-Scale LiDAR Point
    Clouds Sequences](https://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Wang_PointMotionNet_Point-Wise_Motion_Learning_for_Large-Scale_LiDAR_Point_Clouds_Sequences_CVPRW_2022_paper.pdf),
    CVPR 2022'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**PointMotionNet**：针对大规模LiDAR点云序列的逐点运动学习](https://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Wang_PointMotionNet_Point-Wise_Motion_Learning_for_Large-Scale_LiDAR_Point_Clouds_Sequences_CVPRW_2022_paper.pdf)，CVPR
    2022'
- en: '[**MapTR**: Structured Modeling and Learning for Online Vectorized HD Map Construction](https://arxiv.org/abs/2208.14437),
    ICLR 2023'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**MapTR**：用于在线矢量化高清地图构建的结构化建模与学习](https://arxiv.org/abs/2208.14437)，ICLR 2023'
- en: '[**VectorMapNet**: End-to-end Vectorized HD Map Learning](https://arxiv.org/abs/2206.08920),
    Arxiv 2022'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**VectorMapNet**：端到端矢量化高清地图学习](https://arxiv.org/abs/2206.08920)，Arxiv 2022'
- en: '[**SurroundOcc**: Multi-Camera 3D Occupancy Prediction for Autonomous Driving](https://arxiv.org/abs/2303.09551),
    Arxiv 2023'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**SurroundOcc**：用于自动驾驶的多摄像头 3D 占用预测](https://arxiv.org/abs/2303.09551)，Arxiv
    2023'
