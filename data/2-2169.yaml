- en: Unboxing DINOv2, Metaâ€™s new all-purpose computer vision backbone
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¼€ç®±DINOv2ï¼ŒMetaçš„æ–°å‹å…¨èƒ½è®¡ç®—æœºè§†è§‰éª¨å¹²ç½‘ç»œ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/unboxing-dinov2-metas-new-all-purpose-computer-vision-backbone-d8e22c059040](https://towardsdatascience.com/unboxing-dinov2-metas-new-all-purpose-computer-vision-backbone-d8e22c059040)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/unboxing-dinov2-metas-new-all-purpose-computer-vision-backbone-d8e22c059040](https://towardsdatascience.com/unboxing-dinov2-metas-new-all-purpose-computer-vision-backbone-d8e22c059040)
- en: Artificial Intelligence
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: äººå·¥æ™ºèƒ½
- en: Are vision foundational models catching up with LLMs?
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è§†è§‰åŸºç¡€æ¨¡å‹æ˜¯å¦åœ¨è¿½èµ¶LLMsï¼Ÿ
- en: '[](https://michaloleszak.medium.com/?source=post_page-----d8e22c059040--------------------------------)[![MichaÅ‚
    Oleszak](../Images/61b32e70cec4ba54612a8ca22e977176.png)](https://michaloleszak.medium.com/?source=post_page-----d8e22c059040--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d8e22c059040--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d8e22c059040--------------------------------)
    [MichaÅ‚ Oleszak](https://michaloleszak.medium.com/?source=post_page-----d8e22c059040--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://michaloleszak.medium.com/?source=post_page-----d8e22c059040--------------------------------)[![MichaÅ‚
    Oleszak](../Images/61b32e70cec4ba54612a8ca22e977176.png)](https://michaloleszak.medium.com/?source=post_page-----d8e22c059040--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d8e22c059040--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d8e22c059040--------------------------------)
    [MichaÅ‚ Oleszak](https://michaloleszak.medium.com/?source=post_page-----d8e22c059040--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d8e22c059040--------------------------------)
    Â·8 min readÂ·May 7, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d8e22c059040--------------------------------)
    Â·é˜…è¯»æ—¶é—´8åˆ†é’ŸÂ·2023å¹´5æœˆ7æ—¥
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a82bce14167223d4394f5857aa29d74f.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a82bce14167223d4394f5857aa29d74f.png)'
- en: Self-supervised training methods continue to deliver breakthrough after breakthrough.
    Last week, Meta AI released the second version of their self-DIstillation with
    NO labels or DINO model. The model can supposedly be used as a backbone to solve
    virtually any computer vision task without fine-tuning! Have the foundational
    models in computer vision caught up to the level of versatility that Large Language
    Models have held for some time? Letâ€™s take DINO for a walk to see what it can
    do!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªç›‘ç£è®­ç»ƒæ–¹æ³•ä¸æ–­å–å¾—çªç ´ã€‚ä¸Šå‘¨ï¼ŒMeta AIå‘å¸ƒäº†ä»–ä»¬çš„ç¬¬äºŒç‰ˆè‡ªç›‘ç£è’¸é¦æ¨¡å‹DINOã€‚è¯¥æ¨¡å‹æ®è¯´å¯ä»¥ä½œä¸ºéª¨å¹²ç½‘ç»œæ¥è§£å†³å‡ ä¹ä»»ä½•è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œè€Œæ— éœ€å¾®è°ƒï¼è®¡ç®—æœºè§†è§‰ä¸­çš„åŸºç¡€æ¨¡å‹æ˜¯å¦å·²ç»èµ¶ä¸Šäº†å¤§å‹è¯­è¨€æ¨¡å‹é•¿æœŸä»¥æ¥çš„å¤šåŠŸèƒ½æ€§ï¼Ÿè®©æˆ‘ä»¬å¸¦ç€DINOå»æ¢ç´¢å®ƒèƒ½åšäº›ä»€ä¹ˆï¼
- en: '*If youâ€™re mainly interested in playing with the new DINO, feel free to scroll
    down to the â€œTesting DINOv2â€ section. Before that, we look in more detail at the
    modelâ€™s architecture and training routine.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¦‚æœä½ ä¸»è¦å¯¹å°è¯•æ–°çš„DINOæ„Ÿå…´è¶£ï¼Œå¯ä»¥ç›´æ¥æ»šåŠ¨åˆ°â€œæµ‹è¯•DINOv2â€éƒ¨åˆ†ã€‚åœ¨æ­¤ä¹‹å‰ï¼Œæˆ‘ä»¬å°†æ›´è¯¦ç»†åœ°æ¢è®¨æ¨¡å‹çš„æ¶æ„å’Œè®­ç»ƒè¿‡ç¨‹ã€‚*'
- en: '![](../Images/9e80a57e6282193550776bff71480756.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e80a57e6282193550776bff71480756.png)'
- en: ğŸ¦– Self-supervised learning in computer vision
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ¦– è®¡ç®—æœºè§†è§‰ä¸­çš„è‡ªç›‘ç£å­¦ä¹ 
- en: 'Self-supervision has been gaining popularity in computer vision applications
    for a couple of years now. And to no surprise: the possibility to train models
    without labeled examples allows for using a much larger pool of training data,
    and in some [applications where labels are hard or expensive to get](/self-supervised-learning-in-computer-vision-fd43719b1625),
    it may even enable training where it was previously impossible.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªç›‘ç£åœ¨è®¡ç®—æœºè§†è§‰åº”ç”¨ä¸­é€æ¸å—åˆ°å…³æ³¨å·²æœ‰å‡ å¹´ä¹‹ä¹…ã€‚è¿™ä¸è¶³ä¸ºå¥‡ï¼šæ²¡æœ‰æ ‡ç­¾ç¤ºä¾‹çš„æ¨¡å‹è®­ç»ƒå¯èƒ½ä½¿ç”¨æ›´å¤§èŒƒå›´çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶ä¸”åœ¨ä¸€äº›[æ ‡ç­¾éš¾ä»¥è·å¾—æˆ–æˆæœ¬é«˜æ˜‚çš„åº”ç”¨ä¸­](/self-supervised-learning-in-computer-vision-fd43719b1625)ï¼Œç”šè‡³å¯èƒ½å®ç°ä»¥å‰æ— æ³•å®Œæˆçš„è®­ç»ƒã€‚
- en: Models trained in a self-supervised way learn from the images alone, without
    annotations. Indeed, they create their own pseudo-labels from unlabeled data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥è‡ªç›‘ç£æ–¹å¼è®­ç»ƒçš„æ¨¡å‹ä»…ä»å›¾åƒä¸­å­¦ä¹ ï¼Œæ— éœ€æ³¨é‡Šã€‚å®é™…ä¸Šï¼Œå®ƒä»¬ä»æœªæ ‡è®°çš„æ•°æ®ä¸­åˆ›å»ºè‡ªå·±çš„ä¼ªæ ‡ç­¾ã€‚
- en: This has been an established practice in NLP for a time now, where language
    models are often trained to predict the next word in a sentence. Given the input
    body of text, the features and labels for training can be created automatically.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå·²æˆä¸ºä¸€ç§æ—¢å®šå®è·µï¼Œè¯­è¨€æ¨¡å‹é€šå¸¸è¢«è®­ç»ƒä»¥é¢„æµ‹å¥å­ä¸­çš„ä¸‹ä¸€ä¸ªè¯ã€‚ç»™å®šè¾“å…¥æ–‡æœ¬ï¼Œå¯ä»¥è‡ªåŠ¨ç”Ÿæˆè®­ç»ƒæ‰€éœ€çš„ç‰¹å¾å’Œæ ‡ç­¾ã€‚
- en: In computer vision, however, self-supervised approaches havenâ€™t really taken
    off until a couple of contrastive models from Google and Meta ([SimCLR](https://arxiv.org/abs/2002.05709),
    [MoCo](https://arxiv.org/abs/1911.05722), [SwAV](https://arxiv.org/abs/2006.09882),
    and [BYOL](https://arxiv.org/abs/2006.07733)) showed state-of-the-art results,
    sometimes matching or even exceeding those of fully supervised models with access
    to labeled training data. In [my earlier work](/self-supervised-learning-in-computer-vision-fd43719b1625),
    I have shown how MoCo improves the performance of X-ray diagnosis in an environment
    where annotated training examples are scarce.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œåœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸï¼Œè‡ªç›‘ç£æ–¹æ³•ç›´åˆ°è°·æ­Œå’ŒMetaçš„ä¸€äº›å¯¹æ¯”æ¨¡å‹ï¼ˆ[SimCLR](https://arxiv.org/abs/2002.05709)ï¼Œ[MoCo](https://arxiv.org/abs/1911.05722)ï¼Œ[SwAV](https://arxiv.org/abs/2006.09882)ï¼Œå’Œ[BYOL](https://arxiv.org/abs/2006.07733)ï¼‰å±•ç¤ºäº†æœ€å…ˆè¿›çš„ç»“æœä¹‹å‰ï¼Œå°šæœªçœŸæ­£èµ·é£ï¼Œè¿™äº›ç»“æœæœ‰æ—¶ä¸å®Œå…¨ç›‘ç£æ¨¡å‹åŒ¹æ•Œç”šè‡³è¶…è¶Šäº†é‚£äº›æœ‰æ ‡æ³¨è®­ç»ƒæ•°æ®çš„æ¨¡å‹ã€‚åœ¨[æˆ‘çš„æ—©æœŸå·¥ä½œ](/self-supervised-learning-in-computer-vision-fd43719b1625)ä¸­ï¼Œæˆ‘å±•ç¤ºäº†MoCoå¦‚ä½•åœ¨æ ‡æ³¨è®­ç»ƒæ ·æœ¬ç¨€ç¼ºçš„ç¯å¢ƒä¸­æé«˜Xå°„çº¿è¯Šæ–­çš„æ€§èƒ½ã€‚
- en: In 2021, Meta described their first DINO in the paper titled [Emerging Properties
    in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294). Their
    model, although inspired by the previously reigning contrastive architectures,
    took a slightly different approach. Letâ€™s take a look at the original DINO first
    since its second version is very similar to it.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨2021å¹´ï¼ŒMetaåœ¨é¢˜ä¸º[ã€Šè‡ªç›‘ç£è§†è§‰å˜æ¢å™¨ä¸­çš„æ–°å…´ç‰¹æ€§ã€‹](https://arxiv.org/abs/2104.14294)çš„è®ºæ–‡ä¸­æè¿°äº†ä»–ä»¬çš„ç¬¬ä¸€ä¸ªDINOã€‚å°½ç®¡ä»–ä»¬çš„æ¨¡å‹å—åˆ°ä¹‹å‰ä¸»å¯¼å¯¹æ¯”æ¶æ„çš„å¯å‘ï¼Œä½†é‡‡å–äº†ç¨æœ‰ä¸åŒçš„æ–¹æ³•ã€‚è®©æˆ‘ä»¬å…ˆçœ‹çœ‹åŸå§‹çš„DINOï¼Œå› ä¸ºå®ƒçš„ç¬¬äºŒä¸ªç‰ˆæœ¬ä¸ä¹‹éå¸¸ç›¸ä¼¼ã€‚
- en: '![](../Images/3254f09427bd8711da7fecfb0bbe8a30.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3254f09427bd8711da7fecfb0bbe8a30.png)'
- en: ğŸ¦– The DINO model
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ¦– DINO æ¨¡å‹
- en: 'â€œDINOâ€ is actually sort of an acronym, standing for self-**di**stillation with
    **no** labels. As the name suggests, it combines two learning techniques: self-supervised
    learning with no labels which we have already discussed, and knowledge distillation.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: â€œDINOâ€ å®é™…ä¸Šæ˜¯ä¸€ç§é¦–å­—æ¯ç¼©ç•¥è¯ï¼Œä»£è¡¨è‡ª**di**è’¸é¦ä¸**no**æ ‡ç­¾ã€‚æ­£å¦‚å…¶åç§°æ‰€ç¤ºï¼Œå®ƒç»“åˆäº†ä¸¤ç§å­¦ä¹ æŠ€æœ¯ï¼šæˆ‘ä»¬å·²ç»è®¨è®ºè¿‡çš„æ— æ ‡ç­¾è‡ªç›‘ç£å­¦ä¹ å’ŒçŸ¥è¯†è’¸é¦ã€‚
- en: Knowledge distillation is a method typically used to compress model size. In
    it, a smaller model (referred to as a â€œstudentâ€) is typically trained to produce
    the same predictions as a larger, already-trained model (called the â€œteacherâ€).
    If the student can learn to mimic the teacher truthfully, we can keep the same
    performance while using a smaller model.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: çŸ¥è¯†è’¸é¦æ˜¯ä¸€ç§é€šå¸¸ç”¨äºå‹ç¼©æ¨¡å‹å¤§å°çš„æ–¹æ³•ã€‚åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œä¸€ä¸ªè¾ƒå°çš„æ¨¡å‹ï¼ˆç§°ä¸ºâ€œå­¦ç”Ÿâ€ï¼‰é€šå¸¸è¢«è®­ç»ƒä»¥äº§ç”Ÿä¸ä¸€ä¸ªè¾ƒå¤§ã€å·²è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆç§°ä¸ºâ€œæ•™å¸ˆâ€ï¼‰ç›¸åŒçš„é¢„æµ‹ã€‚å¦‚æœå­¦ç”Ÿèƒ½å¤ŸçœŸå®åœ°æ¨¡ä»¿æ•™å¸ˆï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä½¿ç”¨è¾ƒå°æ¨¡å‹çš„åŒæ—¶ä¿æŒç›¸åŒçš„æ€§èƒ½ã€‚
- en: 'DINO uses what the authors call self-distillation, in which the two models
    â€” the student and the teacher â€” are effectively the same model: they have the
    same size and architecture. They only differ in how their parameters get updated
    during training.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: DINOä½¿ç”¨ä½œè€…ç§°ä¹‹ä¸ºè‡ªè’¸é¦çš„æ–¹æ³•ï¼Œå…¶ä¸­ä¸¤ä¸ªæ¨¡å‹â€”â€”å­¦ç”Ÿå’Œæ•™å¸ˆâ€”â€”å®é™…ä¸Šæ˜¯ç›¸åŒçš„æ¨¡å‹ï¼šå®ƒä»¬å…·æœ‰ç›¸åŒçš„å¤§å°å’Œæ¶æ„ã€‚å®ƒä»¬ä»…åœ¨è®­ç»ƒæœŸé—´å¦‚ä½•æ›´æ–°å…¶å‚æ•°ä¸Šæœ‰æ‰€ä¸åŒã€‚
- en: '![](../Images/40458d754516b8e13ff093e42259abdc.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40458d754516b8e13ff093e42259abdc.png)'
- en: 'DINOâ€™s training process. Image source: [arXiv:2104.14294](https://arxiv.org/abs/2104.14294)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: DINOçš„è®­ç»ƒè¿‡ç¨‹ã€‚å›¾ç‰‡æ¥æºï¼š[arXiv:2104.14294](https://arxiv.org/abs/2104.14294)
- en: To train DINO, we set up two identical networks â€” the authors originally use
    Vision Transformers (ViTs). As mentioned, both networks have the same architecture
    but different parameters.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è®­ç»ƒDINOï¼Œæˆ‘ä»¬è®¾ç½®äº†ä¸¤ä¸ªç›¸åŒçš„ç½‘ç»œâ€”â€”ä½œè€…æœ€åˆä½¿ç”¨çš„æ˜¯è§†è§‰å˜æ¢å™¨ï¼ˆViTsï¼‰ã€‚å¦‚å‰æ‰€è¿°ï¼Œè¿™ä¸¤ä¸ªç½‘ç»œå…·æœ‰ç›¸åŒçš„æ¶æ„ä½†å‚æ•°ä¸åŒã€‚
- en: Then, from each training image, a number of random crops are cut out. Some of
    these crops cover just a small area of the original image â€” we will call them
    local views. Other crops are larger and cover a significant part of the original
    image â€” these are global views.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œä»æ¯å¼ è®­ç»ƒå›¾åƒä¸­ï¼Œéšæœºè£å‰ªå‡ºä¸€äº›åŒºåŸŸã€‚è¿™äº›è£å‰ªåŒºåŸŸä¸­ï¼Œæœ‰äº›åªè¦†ç›–äº†åŸå§‹å›¾åƒçš„ä¸€å°éƒ¨åˆ†â€”â€”æˆ‘ä»¬ç§°ä¹‹ä¸ºå±€éƒ¨è§†å›¾ã€‚å…¶ä»–è£å‰ªåŒºåŸŸè¾ƒå¤§ï¼Œè¦†ç›–äº†åŸå§‹å›¾åƒçš„æ˜¾è‘—éƒ¨åˆ†â€”â€”è¿™äº›æ˜¯å…¨å±€è§†å›¾ã€‚
- en: Next, all the crops are passed through the student network while only the global
    views are passed through the teacher network. Each network produces latent representations,
    or embeddings, of the crops it got as inputs. The similarity between the embeddings
    from the student and the teacher is then evaluated with a cross-entropy loss.
    This idea is based on [SwAV](https://arxiv.org/abs/2006.09882) and its goal is
    to encourage the model to learn global-to-local correspondence.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæ‰€æœ‰çš„ä½œç‰©é€šè¿‡å­¦ç”Ÿç½‘ç»œè¿›è¡Œå¤„ç†ï¼Œè€Œåªæœ‰å…¨å±€è§†å›¾é€šè¿‡æ•™å¸ˆç½‘ç»œè¿›è¡Œå¤„ç†ã€‚æ¯ä¸ªç½‘ç»œç”Ÿæˆå…¶è¾“å…¥ä½œç‰©çš„æ½œåœ¨è¡¨ç¤ºæˆ–åµŒå…¥ã€‚ç„¶åï¼Œé€šè¿‡äº¤å‰ç†µæŸå¤±æ¥è¯„ä¼°å­¦ç”Ÿå’Œæ•™å¸ˆçš„åµŒå…¥ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚è¿™ä¸ªæƒ³æ³•åŸºäº[SwAV](https://arxiv.org/abs/2006.09882)ï¼Œæ—¨åœ¨é¼“åŠ±æ¨¡å‹å­¦ä¹ å…¨å±€åˆ°å±€éƒ¨çš„å¯¹åº”å…³ç³»ã€‚
- en: Finally, the gradients based on the loss are propagated back through the student
    network to teach it to produce representations similar to those of the teacher.
    The teacherâ€™s weights, on the other hand, are updated with an exponential moving
    average of the studentâ€™s weights. This idea is based on [the MoCo model](https://arxiv.org/abs/1911.05722),
    but in contrast to it, DINO doesnâ€™t use any memory bank.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼ŒåŸºäºæŸå¤±çš„æ¢¯åº¦è¢«åå‘ä¼ æ’­é€šè¿‡å­¦ç”Ÿç½‘ç»œï¼Œä»¥æ•™ä¼šå®ƒç”Ÿæˆç±»ä¼¼äºæ•™å¸ˆçš„è¡¨ç¤ºã€‚å¦ä¸€æ–¹é¢ï¼Œæ•™å¸ˆçš„æƒé‡é€šè¿‡å­¦ç”Ÿæƒé‡çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡è¿›è¡Œæ›´æ–°ã€‚è¿™ä¸ªæƒ³æ³•åŸºäº[MoCoæ¨¡å‹](https://arxiv.org/abs/1911.05722)ï¼Œä½†ä¸ä¹‹ä¸åŒçš„æ˜¯ï¼ŒDINOä¸ä½¿ç”¨ä»»ä½•è®°å¿†åº“ã€‚
- en: The original DINO paper was titled â€œ[Emerging Properties in Self-Supervised
    Vision Transformers](https://arxiv.org/abs/2104.14294)â€ since the authors were
    somewhat amazed at the properties that emerged from the model. The DINO backbone
    turned out to comprise information about the semantic segmentation of an image,
    as well as to allow for a great performance on downstream image classification
    tasks.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹DINOè®ºæ–‡çš„æ ‡é¢˜æ˜¯â€œ[è‡ªç›‘ç£è§†è§‰å˜æ¢å™¨ä¸­çš„æ–°å…´å±æ€§](https://arxiv.org/abs/2104.14294)â€ï¼Œå› ä¸ºä½œè€…å¯¹æ¨¡å‹ä¸­å‡ºç°çš„å±æ€§æ„Ÿåˆ°æƒŠè®¶ã€‚DINOéª¨å¹²ç½‘ç»œåŒ…å«æœ‰å…³å›¾åƒè¯­ä¹‰åˆ†å‰²çš„ä¿¡æ¯ï¼Œå¹¶ä¸”åœ¨ä¸‹æ¸¸å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚
- en: Whatâ€™s new in V2?
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V2æœ‰ä»€ä¹ˆæ–°å˜åŒ–ï¼Ÿ
- en: How does DINOv2 differ from its predecessor, I hear you asking. Well, not that
    much, at least not in terms of the model architecture or training routine. The
    authors themselves confess that in the [DINOv2 paper](https://arxiv.org/abs/2304.07193),
    â€œmost of the technical contributions aim at accelerating and stabilizing the training
    at scaleâ€.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: DINOv2ä¸å…¶å‰èº«æœ‰ä½•ä¸åŒï¼Œæˆ‘å¬åˆ°ä½ åœ¨é—®ã€‚å—¯ï¼Œå˜åŒ–ä¸å¤§ï¼Œè‡³å°‘åœ¨æ¨¡å‹æ¶æ„æˆ–è®­ç»ƒä¾‹ç¨‹æ–¹é¢æ²¡æœ‰å¤ªå¤§å˜åŒ–ã€‚ä½œè€…è‡ªå·±æ‰¿è®¤ï¼Œåœ¨[DINOv2è®ºæ–‡](https://arxiv.org/abs/2304.07193)ä¸­ï¼Œâ€œå¤§å¤šæ•°æŠ€æœ¯è´¡çŒ®æ—¨åœ¨åŠ é€Ÿå’Œç¨³å®šå¤§è§„æ¨¡è®­ç»ƒâ€ã€‚
- en: The one thing thatâ€™s different is the data that DINOv2 was trained on. So far,
    most advances in self-supervised learning for vision applications were made while
    pre-training the models on small datasets such as the infamous ImageNet, whose
    lack of diversity impedes learning useful features.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸åŒä¹‹å¤„åœ¨äºDINOv2è®­ç»ƒæ‰€ç”¨çš„æ•°æ®ã€‚è¿„ä»Šä¸ºæ­¢ï¼Œè§†è§‰åº”ç”¨è‡ªç›‘ç£å­¦ä¹ çš„å¤§å¤šæ•°è¿›å±•éƒ½æ˜¯åœ¨å°å‹æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œæ¯”å¦‚è‡­åæ˜­è‘—çš„ImageNetï¼Œå…¶ç¼ºä¹å¤šæ ·æ€§é˜»ç¢äº†æœ‰ç”¨ç‰¹å¾çš„å­¦ä¹ ã€‚
- en: DINOv2 authors build a data pipeline allowing them to curate a relatively large
    and diverse dataset. To do this, they employ a clustering algorithm to group candidate
    images into semantically similar clusters, and then they rebalance the clusters
    to prevent the model from overfitting to a couple of the most dominants modes
    in the data.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: DINOv2çš„ä½œè€…å»ºç«‹äº†ä¸€ä¸ªæ•°æ®ç®¡é“ï¼Œä½¿ä»–ä»¬èƒ½å¤Ÿç­–åˆ’ä¸€ä¸ªç›¸å¯¹è¾ƒå¤§ä¸”å¤šæ ·åŒ–çš„æ•°æ®é›†ã€‚ä¸ºæ­¤ï¼Œä»–ä»¬ä½¿ç”¨èšç±»ç®—æ³•å°†å€™é€‰å›¾åƒåˆ†ç»„ä¸ºè¯­ä¹‰ç›¸ä¼¼çš„é›†ç¾¤ï¼Œç„¶åé‡æ–°å¹³è¡¡é›†ç¾¤ï¼Œä»¥é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆæ•°æ®ä¸­çš„å°‘æ•°å‡ ä¸ªä¸»è¦æ¨¡å¼ã€‚
- en: '![](../Images/5cc85a38afec33445b0d2afe9d507252.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5cc85a38afec33445b0d2afe9d507252.png)'
- en: ğŸ¦– Testing DINOv2
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ¦– æµ‹è¯•DINOv2
- en: Letâ€™s put the model to a simple test! The paper claims the DINOv2 backbone can
    be used as a feature extractor without fine-tuning. Letâ€™s see how well it does.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¯¹æ¨¡å‹è¿›è¡Œä¸€ä¸ªç®€å•çš„æµ‹è¯•å§ï¼è®ºæ–‡å£°ç§°DINOv2éª¨å¹²ç½‘ç»œå¯ä»¥ä½œä¸ºç‰¹å¾æå–å™¨ä½¿ç”¨ï¼Œè€Œæ— éœ€å¾®è°ƒã€‚è®©æˆ‘ä»¬çœ‹çœ‹å®ƒçš„è¡¨ç°å¦‚ä½•ã€‚
- en: As the test task, we will have DINO recognize what alphabet a handwritten character
    comes from using a subset of the Omniglot dataset.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºæµ‹è¯•ä»»åŠ¡ï¼Œæˆ‘ä»¬å°†è®©DINOè¯†åˆ«æ‰‹å†™å­—ç¬¦æ¥è‡ªå“ªä¸ªå­—æ¯è¡¨ï¼Œä½¿ç”¨Omniglotæ•°æ®é›†çš„ä¸€ä¸ªå­é›†ã€‚
- en: '![](../Images/00a55d913bcd15b0eca61de2c218aced.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/00a55d913bcd15b0eca61de2c218aced.png)'
- en: 'A sample from the Omniglot dataset. Source: [https://github.com/brendenlake/omniglot](https://github.com/brendenlake/omniglot).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Omniglotæ•°æ®é›†ä¸­çš„ä¸€ä¸ªæ ·æœ¬ã€‚æ¥æºï¼š[https://github.com/brendenlake/omniglot](https://github.com/brendenlake/omniglot)ã€‚
- en: Specifically, we will pass 9543 character drawings (964 different characters
    from 30 different alphabets) through the DINOv2 backbone. Then, we will split
    the embeddings we get into training and testing sets, and train a logistic regression
    classifier on top of them to classify the images to one of the 30 alphabets. This
    evaluation method is known as a linear readout â€” we just read the embeddings from
    the frozen backbone and put a single linear layer (or a liner classifier) on top.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°† 9543 ä¸ªå­—ç¬¦å›¾åƒï¼ˆæ¥è‡ª 30 ç§ä¸åŒå­—æ¯è¡¨çš„ 964 ä¸ªä¸åŒå­—ç¬¦ï¼‰é€šè¿‡ DINOv2 ä¸»å¹²ç½‘ç»œã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è·å¾—çš„åµŒå…¥åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå¹¶åœ¨å…¶ä¸Šè®­ç»ƒä¸€ä¸ªé€»è¾‘å›å½’åˆ†ç±»å™¨ï¼Œä»¥å°†å›¾åƒåˆ†ç±»åˆ°
    30 ç§å­—æ¯è¡¨ä¹‹ä¸€ã€‚è¿™ç§è¯„ä¼°æ–¹æ³•è¢«ç§°ä¸ºçº¿æ€§è¯»å–â€”â€”æˆ‘ä»¬ä»…ä»å†»ç»“çš„ä¸»å¹²ç½‘ç»œä¸­è¯»å–åµŒå…¥ï¼Œå¹¶åœ¨å…¶ä¸Šæ”¾ç½®ä¸€ä¸ªçº¿æ€§å±‚ï¼ˆæˆ–çº¿æ€§åˆ†ç±»å™¨ï¼‰ã€‚
- en: 'This is quite a challenging task: with around 9.6k images and around 960 distinct
    characters, there are only 10 images per character (and only 7 end up in the training
    data â€” the rest are used for testing). Effectively, we create a few-shot learning
    problem in which a random classifier would score an accuracy of 1/30, or 3.3%.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªç›¸å½“å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼šå¤§çº¦ 9.6k å¼ å›¾åƒå’Œ 960 ä¸ªä¸åŒå­—ç¬¦ï¼Œæ¯ä¸ªå­—ç¬¦åªæœ‰ 10 å¼ å›¾åƒï¼ˆå…¶ä¸­åªæœ‰ 7 å¼ ç”¨äºè®­ç»ƒâ€”â€”å…¶ä½™ç”¨äºæµ‹è¯•ï¼‰ã€‚å®é™…ä¸Šï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªå°‘æ ·æœ¬å­¦ä¹ é—®é¢˜ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸€ä¸ªéšæœºåˆ†ç±»å™¨çš„å‡†ç¡®ç‡ä¸º
    1/30ï¼Œå³ 3.3%ã€‚
- en: Let's start with setting up a dataloader.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»è®¾ç½®æ•°æ®åŠ è½½å™¨å¼€å§‹ã€‚
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Then, we load the DINOv2 model. [Four different architectures are available](https://github.com/facebookresearch/dinov2#pretrained-models)
    from PyTorch Hub, with varying sizes and performances. Letâ€™s use the lightest
    *ViT-S/14 distilled* with 21M parameters and the heaviest *ViT-L/14 distilled*
    with 300M parameters (there is also an undistilled version of 1'100M params, but
    itâ€™s quite heavy and very close in performance to the 300M params version). Here
    is the snippet to load *ViT-S/14 distilled.*
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åŠ è½½ DINOv2 æ¨¡å‹ã€‚[PyTorch Hub ä¸Šæä¾›äº†å››ç§ä¸åŒçš„æ¶æ„](https://github.com/facebookresearch/dinov2#pretrained-models)ï¼Œå®ƒä»¬æœ‰ä¸åŒçš„å¤§å°å’Œæ€§èƒ½ã€‚æˆ‘ä»¬ä½¿ç”¨æœ€è½»çš„
    *ViT-S/14 distilled*ï¼ˆ21M å‚æ•°ï¼‰å’Œæœ€é‡çš„ *ViT-L/14 distilled*ï¼ˆ300M å‚æ•°ï¼‰ï¼ˆè¿˜æœ‰ä¸€ä¸ªæœªè’¸é¦ç‰ˆæœ¬çš„ 1100M
    å‚æ•°ï¼Œä½†å®ƒç›¸å½“é‡ï¼Œå¹¶ä¸”ä¸ 300M å‚æ•°ç‰ˆæœ¬çš„æ€§èƒ½éå¸¸æ¥è¿‘ï¼‰ã€‚è¿™é‡Œæ˜¯åŠ è½½ *ViT-S/14 distilled* çš„ä»£ç ç‰‡æ®µã€‚
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: With that done, letâ€™s pass all the images through the DINOv2 backbone and collect
    the embeddings and their associated target labels.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæˆè¿™äº›ä¹‹åï¼Œæˆ‘ä»¬å°†æ‰€æœ‰å›¾åƒé€šè¿‡ DINOv2 ä¸»å¹²ç½‘ç»œï¼Œå¹¶æ”¶é›†åµŒå…¥åŠå…¶ç›¸å…³çš„ç›®æ ‡æ ‡ç­¾ã€‚
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Next, we split the data into train and test sets and train a logistic regression
    classifier on top of it.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ•°æ®åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå¹¶åœ¨å…¶ä¸Šè®­ç»ƒä¸€ä¸ªé€»è¾‘å›å½’åˆ†ç±»å™¨ã€‚
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We got a test accuracy of just over 54%. Much better than random guessing, but
    far from perfect. Letâ€™s see how it compares to a larger, 300M params DINO, and
    to a ResNet50.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¾—åˆ°äº†ç¨é«˜äº 54% çš„æµ‹è¯•å‡†ç¡®ç‡ã€‚æ¯”éšæœºçŒœæµ‹å¥½å¾—å¤šï¼Œä½†è¿˜è¿œæœªå®Œç¾ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å®ƒä¸ä¸€ä¸ªæ›´å¤§çš„ 300M å‚æ•° DINO å’Œä¸€ä¸ª ResNet50 çš„è¡¨ç°å¦‚ä½•æ¯”è¾ƒã€‚
- en: '![](../Images/50657b79b9d684275db82125c58a8928.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/50657b79b9d684275db82125c58a8928.png)'
- en: 'Model comparison: two DINOs and a ResNet.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ¯”è¾ƒï¼šä¸¤ä¸ª DINO å’Œä¸€ä¸ª ResNetã€‚
- en: ResNet50 and the small DINOv2 using *ViT-S/14* are of similar size â€” DINO is
    actually even smaller â€” but DINO yields an accuracy score roughly 15 percentage
    points higher. A larger DINO can bump the score by another 10 to 15 percentage
    points, that is to 65â€“70% accuracy.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet50 å’Œå°å‹ DINOv2 ä½¿ç”¨çš„*ViT-S/14*å¤§å°ç›¸ä¼¼â€”â€”DINO å®é™…ä¸Šæ›´å°â€”â€”ä½† DINO çš„å‡†ç¡®ç‡é«˜å‡ºå¤§çº¦ 15 ä¸ªç™¾åˆ†ç‚¹ã€‚ä¸€ä¸ªæ›´å¤§çš„
    DINO å¯ä»¥å°†å‡†ç¡®ç‡å†æé«˜ 10 åˆ° 15 ä¸ªç™¾åˆ†ç‚¹ï¼Œå³ 65â€“70%ã€‚
- en: Is this a good score? Upon getting the results, my first reaction was a slight
    disappointment. Unconsciously, I must have been hoping for an accuracy score in
    the 90s. But after all, the task is not easy and we only used (the equivalent
    of) a single linear layer to train. DINOv2 definitely does a better job than a
    similarly-sized ResNet, which is often used as a go-to visual features extractor.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªå¥½çš„åˆ†æ•°å—ï¼Ÿåœ¨å¾—åˆ°ç»“æœæ—¶ï¼Œæˆ‘çš„ç¬¬ä¸€ååº”æ˜¯ç•¥å¾®å¤±æœ›ã€‚ä¸‹æ„è¯†åœ°ï¼Œæˆ‘å¯èƒ½æœŸæœ›å¾—åˆ° 90% ä»¥ä¸Šçš„å‡†ç¡®ç‡ã€‚ä½†æ¯•ç«Ÿï¼Œè¿™ä¸ªä»»åŠ¡ä¸å®¹æ˜“ï¼Œè€Œä¸”æˆ‘ä»¬ä»…ä½¿ç”¨äº†ï¼ˆç›¸å½“äºï¼‰ä¸€ä¸ªçº¿æ€§å±‚æ¥è¿›è¡Œè®­ç»ƒã€‚DINOv2
    çš„è¡¨ç°ç¡®å®ä¼˜äºç±»ä¼¼å¤§å°çš„ ResNetï¼Œåè€…é€šå¸¸è¢«ç”¨ä½œä¸»æµçš„è§†è§‰ç‰¹å¾æå–å™¨ã€‚
- en: What do you think about these results? Let me know in the comments!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯¹è¿™äº›ç»“æœæ€ä¹ˆçœ‹ï¼Ÿåœ¨è¯„è®ºä¸­å‘Šè¯‰æˆ‘å§ï¼
- en: '![](../Images/81d2029b4efe2686af74c4fc3e095953.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81d2029b4efe2686af74c4fc3e095953.png)'
- en: Thanks for reading!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢é˜…è¯»ï¼
- en: If you liked this post, why donâ€™t you [**subscribe for email updates**](https://michaloleszak.medium.com/subscribe)
    on my new articles? And by [**becoming a Medium member**](https://michaloleszak.medium.com/membership),
    you can support my writing and get unlimited access to all stories by other authors
    and yours truly.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œä¸ºä»€ä¹ˆä¸ [**è®¢é˜…ç”µå­é‚®ä»¶æ›´æ–°**](https://michaloleszak.medium.com/subscribe) ä»¥è·å–æˆ‘æ–°æ–‡ç« çš„é€šçŸ¥å‘¢ï¼Ÿè€Œé€šè¿‡
    [**æˆä¸º Medium ä¼šå‘˜**](https://michaloleszak.medium.com/membership)ï¼Œä½ å¯ä»¥æ”¯æŒæˆ‘çš„å†™ä½œï¼Œå¹¶æ— é™åˆ¶è®¿é—®æ‰€æœ‰å…¶ä»–ä½œè€…å’Œæˆ‘çš„æ•…äº‹ã€‚
- en: Want to always keep your finger on the pulse of the increasingly faster-developing
    field of machine learning and AI? Check out my new newsletter, [**AI Pulse**](https://pulseofai.substack.com/).
    Need consulting? You can ask me anything or book me for a 1:1 [**here**](https://topmate.io/michaloleszak).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ³è¦æ—¶åˆ»æŒæ¡æ—¥ç›Šå¿«é€Ÿå‘å±•çš„æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½é¢†åŸŸçš„æœ€æ–°åŠ¨æ€ï¼ŸæŸ¥çœ‹æˆ‘çš„æ–°é€šè®¯ï¼Œ[**AI Pulse**](https://pulseofai.substack.com/)ã€‚éœ€è¦å’¨è¯¢ï¼Ÿä½ å¯ä»¥éšæ—¶é—®æˆ‘é—®é¢˜æˆ–[**åœ¨è¿™é‡Œ**](https://topmate.io/michaloleszak)é¢„çº¦ä¸€å¯¹ä¸€å’¨è¯¢ã€‚
- en: 'You can also try one of [my other articles](https://michaloleszak.github.io/blog/).
    Canâ€™t choose? Pick one of these:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ è¿˜å¯ä»¥å°è¯•[æˆ‘å…¶ä»–çš„æ–‡ç« ](https://michaloleszak.github.io/blog/)ã€‚éš¾ä»¥é€‰æ‹©ï¼Ÿå¯ä»¥ä»è¿™äº›ä¸­æŒ‘ä¸€ä¸ªï¼š
- en: '[](/self-supervised-learning-in-computer-vision-fd43719b1625?source=post_page-----d8e22c059040--------------------------------)
    [## Self-Supervised Learning in Computer Vision'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/self-supervised-learning-in-computer-vision-fd43719b1625?source=post_page-----d8e22c059040--------------------------------)
    [## è®¡ç®—æœºè§†è§‰ä¸­çš„è‡ªç›‘ç£å­¦ä¹ '
- en: How to train models with only a few labeled examples
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¦‚ä½•ç”¨å°‘é‡æ ‡è®°æ ·æœ¬è®­ç»ƒæ¨¡å‹
- en: towardsdatascience.com](/self-supervised-learning-in-computer-vision-fd43719b1625?source=post_page-----d8e22c059040--------------------------------)
    [](/model-optimization-with-tensorflow-629342d1a96f?source=post_page-----d8e22c059040--------------------------------)
    [## Model Optimization with TensorFlow
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/self-supervised-learning-in-computer-vision-fd43719b1625?source=post_page-----d8e22c059040--------------------------------)
    [](/model-optimization-with-tensorflow-629342d1a96f?source=post_page-----d8e22c059040--------------------------------)
    [## ä½¿ç”¨ TensorFlow è¿›è¡Œæ¨¡å‹ä¼˜åŒ–
- en: Reduce your models' latency, storage, and inference costs with quantization
    and pruning
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é€šè¿‡é‡åŒ–å’Œå‰ªææ¥å‡å°‘æ¨¡å‹çš„å»¶è¿Ÿã€å­˜å‚¨å’Œæ¨ç†æˆæœ¬
- en: towardsdatascience.com](/model-optimization-with-tensorflow-629342d1a96f?source=post_page-----d8e22c059040--------------------------------)
    [](https://pub.towardsai.net/forget-about-chatgpt-f17a7f5089c3?source=post_page-----d8e22c059040--------------------------------)
    [## Forget About ChatGPT
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/model-optimization-with-tensorflow-629342d1a96f?source=post_page-----d8e22c059040--------------------------------)
    [](https://pub.towardsai.net/forget-about-chatgpt-f17a7f5089c3?source=post_page-----d8e22c059040--------------------------------)
    [## å¿˜è®° ChatGPT
- en: Bard, Sparrow, and multimodal chatbots will render it obsolete soon, and here
    is why.
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Bardã€Sparrow å’Œå¤šæ¨¡æ€èŠå¤©æœºå™¨äººå°†å¾ˆå¿«ä½¿å…¶è¿‡æ—¶ï¼ŒåŸå› å¦‚ä¸‹ã€‚
- en: pub.towardsai.net](https://pub.towardsai.net/forget-about-chatgpt-f17a7f5089c3?source=post_page-----d8e22c059040--------------------------------)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: pub.towardsai.net](https://pub.towardsai.net/forget-about-chatgpt-f17a7f5089c3?source=post_page-----d8e22c059040--------------------------------)
- en: All images, unless otherwise noted, are by the author.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤éå¦æœ‰è¯´æ˜ï¼Œæ‰€æœ‰å›¾ç‰‡å‡ç”±ä½œè€…æä¾›ã€‚
