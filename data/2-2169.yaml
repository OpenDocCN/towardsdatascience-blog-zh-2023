- en: Unboxing DINOv2, Meta’s new all-purpose computer vision backbone
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开箱DINOv2，Meta的新型全能计算机视觉骨干网络
- en: 原文：[https://towardsdatascience.com/unboxing-dinov2-metas-new-all-purpose-computer-vision-backbone-d8e22c059040](https://towardsdatascience.com/unboxing-dinov2-metas-new-all-purpose-computer-vision-backbone-d8e22c059040)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/unboxing-dinov2-metas-new-all-purpose-computer-vision-backbone-d8e22c059040](https://towardsdatascience.com/unboxing-dinov2-metas-new-all-purpose-computer-vision-backbone-d8e22c059040)
- en: Artificial Intelligence
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人工智能
- en: Are vision foundational models catching up with LLMs?
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视觉基础模型是否在追赶LLMs？
- en: '[](https://michaloleszak.medium.com/?source=post_page-----d8e22c059040--------------------------------)[![Michał
    Oleszak](../Images/61b32e70cec4ba54612a8ca22e977176.png)](https://michaloleszak.medium.com/?source=post_page-----d8e22c059040--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d8e22c059040--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d8e22c059040--------------------------------)
    [Michał Oleszak](https://michaloleszak.medium.com/?source=post_page-----d8e22c059040--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://michaloleszak.medium.com/?source=post_page-----d8e22c059040--------------------------------)[![Michał
    Oleszak](../Images/61b32e70cec4ba54612a8ca22e977176.png)](https://michaloleszak.medium.com/?source=post_page-----d8e22c059040--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d8e22c059040--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d8e22c059040--------------------------------)
    [Michał Oleszak](https://michaloleszak.medium.com/?source=post_page-----d8e22c059040--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d8e22c059040--------------------------------)
    ·8 min read·May 7, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d8e22c059040--------------------------------)
    ·阅读时间8分钟·2023年5月7日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a82bce14167223d4394f5857aa29d74f.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a82bce14167223d4394f5857aa29d74f.png)'
- en: Self-supervised training methods continue to deliver breakthrough after breakthrough.
    Last week, Meta AI released the second version of their self-DIstillation with
    NO labels or DINO model. The model can supposedly be used as a backbone to solve
    virtually any computer vision task without fine-tuning! Have the foundational
    models in computer vision caught up to the level of versatility that Large Language
    Models have held for some time? Let’s take DINO for a walk to see what it can
    do!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督训练方法不断取得突破。上周，Meta AI发布了他们的第二版自监督蒸馏模型DINO。该模型据说可以作为骨干网络来解决几乎任何计算机视觉任务，而无需微调！计算机视觉中的基础模型是否已经赶上了大型语言模型长期以来的多功能性？让我们带着DINO去探索它能做些什么！
- en: '*If you’re mainly interested in playing with the new DINO, feel free to scroll
    down to the “Testing DINOv2” section. Before that, we look in more detail at the
    model’s architecture and training routine.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你主要对尝试新的DINO感兴趣，可以直接滚动到“测试DINOv2”部分。在此之前，我们将更详细地探讨模型的架构和训练过程。*'
- en: '![](../Images/9e80a57e6282193550776bff71480756.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e80a57e6282193550776bff71480756.png)'
- en: 🦖 Self-supervised learning in computer vision
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🦖 计算机视觉中的自监督学习
- en: 'Self-supervision has been gaining popularity in computer vision applications
    for a couple of years now. And to no surprise: the possibility to train models
    without labeled examples allows for using a much larger pool of training data,
    and in some [applications where labels are hard or expensive to get](/self-supervised-learning-in-computer-vision-fd43719b1625),
    it may even enable training where it was previously impossible.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督在计算机视觉应用中逐渐受到关注已有几年之久。这不足为奇：没有标签示例的模型训练可能使用更大范围的训练数据，并且在一些[标签难以获得或成本高昂的应用中](/self-supervised-learning-in-computer-vision-fd43719b1625)，甚至可能实现以前无法完成的训练。
- en: Models trained in a self-supervised way learn from the images alone, without
    annotations. Indeed, they create their own pseudo-labels from unlabeled data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以自监督方式训练的模型仅从图像中学习，无需注释。实际上，它们从未标记的数据中创建自己的伪标签。
- en: This has been an established practice in NLP for a time now, where language
    models are often trained to predict the next word in a sentence. Given the input
    body of text, the features and labels for training can be created automatically.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这在自然语言处理领域已成为一种既定实践，语言模型通常被训练以预测句子中的下一个词。给定输入文本，可以自动生成训练所需的特征和标签。
- en: In computer vision, however, self-supervised approaches haven’t really taken
    off until a couple of contrastive models from Google and Meta ([SimCLR](https://arxiv.org/abs/2002.05709),
    [MoCo](https://arxiv.org/abs/1911.05722), [SwAV](https://arxiv.org/abs/2006.09882),
    and [BYOL](https://arxiv.org/abs/2006.07733)) showed state-of-the-art results,
    sometimes matching or even exceeding those of fully supervised models with access
    to labeled training data. In [my earlier work](/self-supervised-learning-in-computer-vision-fd43719b1625),
    I have shown how MoCo improves the performance of X-ray diagnosis in an environment
    where annotated training examples are scarce.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在计算机视觉领域，自监督方法直到谷歌和Meta的一些对比模型（[SimCLR](https://arxiv.org/abs/2002.05709)，[MoCo](https://arxiv.org/abs/1911.05722)，[SwAV](https://arxiv.org/abs/2006.09882)，和[BYOL](https://arxiv.org/abs/2006.07733)）展示了最先进的结果之前，尚未真正起飞，这些结果有时与完全监督模型匹敌甚至超越了那些有标注训练数据的模型。在[我的早期工作](/self-supervised-learning-in-computer-vision-fd43719b1625)中，我展示了MoCo如何在标注训练样本稀缺的环境中提高X射线诊断的性能。
- en: In 2021, Meta described their first DINO in the paper titled [Emerging Properties
    in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294). Their
    model, although inspired by the previously reigning contrastive architectures,
    took a slightly different approach. Let’s take a look at the original DINO first
    since its second version is very similar to it.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在2021年，Meta在题为[《自监督视觉变换器中的新兴特性》](https://arxiv.org/abs/2104.14294)的论文中描述了他们的第一个DINO。尽管他们的模型受到之前主导对比架构的启发，但采取了稍有不同的方法。让我们先看看原始的DINO，因为它的第二个版本与之非常相似。
- en: '![](../Images/3254f09427bd8711da7fecfb0bbe8a30.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3254f09427bd8711da7fecfb0bbe8a30.png)'
- en: 🦖 The DINO model
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🦖 DINO 模型
- en: '“DINO” is actually sort of an acronym, standing for self-**di**stillation with
    **no** labels. As the name suggests, it combines two learning techniques: self-supervised
    learning with no labels which we have already discussed, and knowledge distillation.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: “DINO” 实际上是一种首字母缩略词，代表自**di**蒸馏与**no**标签。正如其名称所示，它结合了两种学习技术：我们已经讨论过的无标签自监督学习和知识蒸馏。
- en: Knowledge distillation is a method typically used to compress model size. In
    it, a smaller model (referred to as a “student”) is typically trained to produce
    the same predictions as a larger, already-trained model (called the “teacher”).
    If the student can learn to mimic the teacher truthfully, we can keep the same
    performance while using a smaller model.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏是一种通常用于压缩模型大小的方法。在这种方法中，一个较小的模型（称为“学生”）通常被训练以产生与一个较大、已训练好的模型（称为“教师”）相同的预测。如果学生能够真实地模仿教师，我们可以在使用较小模型的同时保持相同的性能。
- en: 'DINO uses what the authors call self-distillation, in which the two models
    — the student and the teacher — are effectively the same model: they have the
    same size and architecture. They only differ in how their parameters get updated
    during training.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: DINO使用作者称之为自蒸馏的方法，其中两个模型——学生和教师——实际上是相同的模型：它们具有相同的大小和架构。它们仅在训练期间如何更新其参数上有所不同。
- en: '![](../Images/40458d754516b8e13ff093e42259abdc.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40458d754516b8e13ff093e42259abdc.png)'
- en: 'DINO’s training process. Image source: [arXiv:2104.14294](https://arxiv.org/abs/2104.14294)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: DINO的训练过程。图片来源：[arXiv:2104.14294](https://arxiv.org/abs/2104.14294)
- en: To train DINO, we set up two identical networks — the authors originally use
    Vision Transformers (ViTs). As mentioned, both networks have the same architecture
    but different parameters.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练DINO，我们设置了两个相同的网络——作者最初使用的是视觉变换器（ViTs）。如前所述，这两个网络具有相同的架构但参数不同。
- en: Then, from each training image, a number of random crops are cut out. Some of
    these crops cover just a small area of the original image — we will call them
    local views. Other crops are larger and cover a significant part of the original
    image — these are global views.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，从每张训练图像中，随机裁剪出一些区域。这些裁剪区域中，有些只覆盖了原始图像的一小部分——我们称之为局部视图。其他裁剪区域较大，覆盖了原始图像的显著部分——这些是全局视图。
- en: Next, all the crops are passed through the student network while only the global
    views are passed through the teacher network. Each network produces latent representations,
    or embeddings, of the crops it got as inputs. The similarity between the embeddings
    from the student and the teacher is then evaluated with a cross-entropy loss.
    This idea is based on [SwAV](https://arxiv.org/abs/2006.09882) and its goal is
    to encourage the model to learn global-to-local correspondence.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，所有的作物通过学生网络进行处理，而只有全局视图通过教师网络进行处理。每个网络生成其输入作物的潜在表示或嵌入。然后，通过交叉熵损失来评估学生和教师的嵌入之间的相似性。这个想法基于[SwAV](https://arxiv.org/abs/2006.09882)，旨在鼓励模型学习全局到局部的对应关系。
- en: Finally, the gradients based on the loss are propagated back through the student
    network to teach it to produce representations similar to those of the teacher.
    The teacher’s weights, on the other hand, are updated with an exponential moving
    average of the student’s weights. This idea is based on [the MoCo model](https://arxiv.org/abs/1911.05722),
    but in contrast to it, DINO doesn’t use any memory bank.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，基于损失的梯度被反向传播通过学生网络，以教会它生成类似于教师的表示。另一方面，教师的权重通过学生权重的指数移动平均进行更新。这个想法基于[MoCo模型](https://arxiv.org/abs/1911.05722)，但与之不同的是，DINO不使用任何记忆库。
- en: The original DINO paper was titled “[Emerging Properties in Self-Supervised
    Vision Transformers](https://arxiv.org/abs/2104.14294)” since the authors were
    somewhat amazed at the properties that emerged from the model. The DINO backbone
    turned out to comprise information about the semantic segmentation of an image,
    as well as to allow for a great performance on downstream image classification
    tasks.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 原始DINO论文的标题是“[自监督视觉变换器中的新兴属性](https://arxiv.org/abs/2104.14294)”，因为作者对模型中出现的属性感到惊讶。DINO骨干网络包含有关图像语义分割的信息，并且在下游图像分类任务中表现出色。
- en: What’s new in V2?
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V2有什么新变化？
- en: How does DINOv2 differ from its predecessor, I hear you asking. Well, not that
    much, at least not in terms of the model architecture or training routine. The
    authors themselves confess that in the [DINOv2 paper](https://arxiv.org/abs/2304.07193),
    “most of the technical contributions aim at accelerating and stabilizing the training
    at scale”.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: DINOv2与其前身有何不同，我听到你在问。嗯，变化不大，至少在模型架构或训练例程方面没有太大变化。作者自己承认，在[DINOv2论文](https://arxiv.org/abs/2304.07193)中，“大多数技术贡献旨在加速和稳定大规模训练”。
- en: The one thing that’s different is the data that DINOv2 was trained on. So far,
    most advances in self-supervised learning for vision applications were made while
    pre-training the models on small datasets such as the infamous ImageNet, whose
    lack of diversity impedes learning useful features.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 不同之处在于DINOv2训练所用的数据。迄今为止，视觉应用自监督学习的大多数进展都是在小型数据集上进行预训练的，比如臭名昭著的ImageNet，其缺乏多样性阻碍了有用特征的学习。
- en: DINOv2 authors build a data pipeline allowing them to curate a relatively large
    and diverse dataset. To do this, they employ a clustering algorithm to group candidate
    images into semantically similar clusters, and then they rebalance the clusters
    to prevent the model from overfitting to a couple of the most dominants modes
    in the data.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: DINOv2的作者建立了一个数据管道，使他们能够策划一个相对较大且多样化的数据集。为此，他们使用聚类算法将候选图像分组为语义相似的集群，然后重新平衡集群，以防止模型过拟合数据中的少数几个主要模式。
- en: '![](../Images/5cc85a38afec33445b0d2afe9d507252.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5cc85a38afec33445b0d2afe9d507252.png)'
- en: 🦖 Testing DINOv2
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🦖 测试DINOv2
- en: Let’s put the model to a simple test! The paper claims the DINOv2 backbone can
    be used as a feature extractor without fine-tuning. Let’s see how well it does.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对模型进行一个简单的测试吧！论文声称DINOv2骨干网络可以作为特征提取器使用，而无需微调。让我们看看它的表现如何。
- en: As the test task, we will have DINO recognize what alphabet a handwritten character
    comes from using a subset of the Omniglot dataset.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 作为测试任务，我们将让DINO识别手写字符来自哪个字母表，使用Omniglot数据集的一个子集。
- en: '![](../Images/00a55d913bcd15b0eca61de2c218aced.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/00a55d913bcd15b0eca61de2c218aced.png)'
- en: 'A sample from the Omniglot dataset. Source: [https://github.com/brendenlake/omniglot](https://github.com/brendenlake/omniglot).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Omniglot数据集中的一个样本。来源：[https://github.com/brendenlake/omniglot](https://github.com/brendenlake/omniglot)。
- en: Specifically, we will pass 9543 character drawings (964 different characters
    from 30 different alphabets) through the DINOv2 backbone. Then, we will split
    the embeddings we get into training and testing sets, and train a logistic regression
    classifier on top of them to classify the images to one of the 30 alphabets. This
    evaluation method is known as a linear readout — we just read the embeddings from
    the frozen backbone and put a single linear layer (or a liner classifier) on top.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将 9543 个字符图像（来自 30 种不同字母表的 964 个不同字符）通过 DINOv2 主干网络。然后，我们将获得的嵌入分为训练集和测试集，并在其上训练一个逻辑回归分类器，以将图像分类到
    30 种字母表之一。这种评估方法被称为线性读取——我们仅从冻结的主干网络中读取嵌入，并在其上放置一个线性层（或线性分类器）。
- en: 'This is quite a challenging task: with around 9.6k images and around 960 distinct
    characters, there are only 10 images per character (and only 7 end up in the training
    data — the rest are used for testing). Effectively, we create a few-shot learning
    problem in which a random classifier would score an accuracy of 1/30, or 3.3%.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当具有挑战性的任务：大约 9.6k 张图像和 960 个不同字符，每个字符只有 10 张图像（其中只有 7 张用于训练——其余用于测试）。实际上，我们创建了一个少样本学习问题，在这种情况下，一个随机分类器的准确率为
    1/30，即 3.3%。
- en: Let's start with setting up a dataloader.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从设置数据加载器开始。
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Then, we load the DINOv2 model. [Four different architectures are available](https://github.com/facebookresearch/dinov2#pretrained-models)
    from PyTorch Hub, with varying sizes and performances. Let’s use the lightest
    *ViT-S/14 distilled* with 21M parameters and the heaviest *ViT-L/14 distilled*
    with 300M parameters (there is also an undistilled version of 1'100M params, but
    it’s quite heavy and very close in performance to the 300M params version). Here
    is the snippet to load *ViT-S/14 distilled.*
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们加载 DINOv2 模型。[PyTorch Hub 上提供了四种不同的架构](https://github.com/facebookresearch/dinov2#pretrained-models)，它们有不同的大小和性能。我们使用最轻的
    *ViT-S/14 distilled*（21M 参数）和最重的 *ViT-L/14 distilled*（300M 参数）（还有一个未蒸馏版本的 1100M
    参数，但它相当重，并且与 300M 参数版本的性能非常接近）。这里是加载 *ViT-S/14 distilled* 的代码片段。
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: With that done, let’s pass all the images through the DINOv2 backbone and collect
    the embeddings and their associated target labels.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些之后，我们将所有图像通过 DINOv2 主干网络，并收集嵌入及其相关的目标标签。
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Next, we split the data into train and test sets and train a logistic regression
    classifier on top of it.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将数据分为训练集和测试集，并在其上训练一个逻辑回归分类器。
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We got a test accuracy of just over 54%. Much better than random guessing, but
    far from perfect. Let’s see how it compares to a larger, 300M params DINO, and
    to a ResNet50.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了稍高于 54% 的测试准确率。比随机猜测好得多，但还远未完美。让我们看看它与一个更大的 300M 参数 DINO 和一个 ResNet50 的表现如何比较。
- en: '![](../Images/50657b79b9d684275db82125c58a8928.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/50657b79b9d684275db82125c58a8928.png)'
- en: 'Model comparison: two DINOs and a ResNet.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 模型比较：两个 DINO 和一个 ResNet。
- en: ResNet50 and the small DINOv2 using *ViT-S/14* are of similar size — DINO is
    actually even smaller — but DINO yields an accuracy score roughly 15 percentage
    points higher. A larger DINO can bump the score by another 10 to 15 percentage
    points, that is to 65–70% accuracy.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet50 和小型 DINOv2 使用的*ViT-S/14*大小相似——DINO 实际上更小——但 DINO 的准确率高出大约 15 个百分点。一个更大的
    DINO 可以将准确率再提高 10 到 15 个百分点，即 65–70%。
- en: Is this a good score? Upon getting the results, my first reaction was a slight
    disappointment. Unconsciously, I must have been hoping for an accuracy score in
    the 90s. But after all, the task is not easy and we only used (the equivalent
    of) a single linear layer to train. DINOv2 definitely does a better job than a
    similarly-sized ResNet, which is often used as a go-to visual features extractor.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个好的分数吗？在得到结果时，我的第一反应是略微失望。下意识地，我可能期望得到 90% 以上的准确率。但毕竟，这个任务不容易，而且我们仅使用了（相当于）一个线性层来进行训练。DINOv2
    的表现确实优于类似大小的 ResNet，后者通常被用作主流的视觉特征提取器。
- en: What do you think about these results? Let me know in the comments!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你对这些结果怎么看？在评论中告诉我吧！
- en: '![](../Images/81d2029b4efe2686af74c4fc3e095953.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81d2029b4efe2686af74c4fc3e095953.png)'
- en: Thanks for reading!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: If you liked this post, why don’t you [**subscribe for email updates**](https://michaloleszak.medium.com/subscribe)
    on my new articles? And by [**becoming a Medium member**](https://michaloleszak.medium.com/membership),
    you can support my writing and get unlimited access to all stories by other authors
    and yours truly.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这篇文章，为什么不 [**订阅电子邮件更新**](https://michaloleszak.medium.com/subscribe) 以获取我新文章的通知呢？而通过
    [**成为 Medium 会员**](https://michaloleszak.medium.com/membership)，你可以支持我的写作，并无限制访问所有其他作者和我的故事。
- en: Want to always keep your finger on the pulse of the increasingly faster-developing
    field of machine learning and AI? Check out my new newsletter, [**AI Pulse**](https://pulseofai.substack.com/).
    Need consulting? You can ask me anything or book me for a 1:1 [**here**](https://topmate.io/michaloleszak).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 想要时刻掌握日益快速发展的机器学习和人工智能领域的最新动态？查看我的新通讯，[**AI Pulse**](https://pulseofai.substack.com/)。需要咨询？你可以随时问我问题或[**在这里**](https://topmate.io/michaloleszak)预约一对一咨询。
- en: 'You can also try one of [my other articles](https://michaloleszak.github.io/blog/).
    Can’t choose? Pick one of these:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以尝试[我其他的文章](https://michaloleszak.github.io/blog/)。难以选择？可以从这些中挑一个：
- en: '[](/self-supervised-learning-in-computer-vision-fd43719b1625?source=post_page-----d8e22c059040--------------------------------)
    [## Self-Supervised Learning in Computer Vision'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/self-supervised-learning-in-computer-vision-fd43719b1625?source=post_page-----d8e22c059040--------------------------------)
    [## 计算机视觉中的自监督学习'
- en: How to train models with only a few labeled examples
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何用少量标记样本训练模型
- en: towardsdatascience.com](/self-supervised-learning-in-computer-vision-fd43719b1625?source=post_page-----d8e22c059040--------------------------------)
    [](/model-optimization-with-tensorflow-629342d1a96f?source=post_page-----d8e22c059040--------------------------------)
    [## Model Optimization with TensorFlow
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/self-supervised-learning-in-computer-vision-fd43719b1625?source=post_page-----d8e22c059040--------------------------------)
    [](/model-optimization-with-tensorflow-629342d1a96f?source=post_page-----d8e22c059040--------------------------------)
    [## 使用 TensorFlow 进行模型优化
- en: Reduce your models' latency, storage, and inference costs with quantization
    and pruning
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过量化和剪枝来减少模型的延迟、存储和推理成本
- en: towardsdatascience.com](/model-optimization-with-tensorflow-629342d1a96f?source=post_page-----d8e22c059040--------------------------------)
    [](https://pub.towardsai.net/forget-about-chatgpt-f17a7f5089c3?source=post_page-----d8e22c059040--------------------------------)
    [## Forget About ChatGPT
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/model-optimization-with-tensorflow-629342d1a96f?source=post_page-----d8e22c059040--------------------------------)
    [](https://pub.towardsai.net/forget-about-chatgpt-f17a7f5089c3?source=post_page-----d8e22c059040--------------------------------)
    [## 忘记 ChatGPT
- en: Bard, Sparrow, and multimodal chatbots will render it obsolete soon, and here
    is why.
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Bard、Sparrow 和多模态聊天机器人将很快使其过时，原因如下。
- en: pub.towardsai.net](https://pub.towardsai.net/forget-about-chatgpt-f17a7f5089c3?source=post_page-----d8e22c059040--------------------------------)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: pub.towardsai.net](https://pub.towardsai.net/forget-about-chatgpt-f17a7f5089c3?source=post_page-----d8e22c059040--------------------------------)
- en: All images, unless otherwise noted, are by the author.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图片均由作者提供。
