- en: 'Beyond The VIF: Collinearity Analysis for Bias Mitigation and Predictive Accuracy'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越VIF：用于偏差缓解和预测准确性的共线性分析
- en: 原文：[https://towardsdatascience.com/beyond-the-vif-collinearity-analysis-for-bias-mitigation-and-predictive-accuracy-18fbba3f7aa2?source=collection_archive---------5-----------------------#2023-07-31](https://towardsdatascience.com/beyond-the-vif-collinearity-analysis-for-bias-mitigation-and-predictive-accuracy-18fbba3f7aa2?source=collection_archive---------5-----------------------#2023-07-31)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/beyond-the-vif-collinearity-analysis-for-bias-mitigation-and-predictive-accuracy-18fbba3f7aa2?source=collection_archive---------5-----------------------#2023-07-31](https://towardsdatascience.com/beyond-the-vif-collinearity-analysis-for-bias-mitigation-and-predictive-accuracy-18fbba3f7aa2?source=collection_archive---------5-----------------------#2023-07-31)
- en: '[](https://goodrobotsai.medium.com/?source=post_page-----18fbba3f7aa2--------------------------------)[![Good
    Robots](../Images/f2fe19ea6712bbe0e4ff20763eaf61b5.png)](https://goodrobotsai.medium.com/?source=post_page-----18fbba3f7aa2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----18fbba3f7aa2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----18fbba3f7aa2--------------------------------)
    [Good Robots](https://goodrobotsai.medium.com/?source=post_page-----18fbba3f7aa2--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://goodrobotsai.medium.com/?source=post_page-----18fbba3f7aa2--------------------------------)[![Good
    Robots](../Images/f2fe19ea6712bbe0e4ff20763eaf61b5.png)](https://goodrobotsai.medium.com/?source=post_page-----18fbba3f7aa2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----18fbba3f7aa2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----18fbba3f7aa2--------------------------------)
    [Good Robots](https://goodrobotsai.medium.com/?source=post_page-----18fbba3f7aa2--------------------------------)'
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3abbfbfa9c59&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-the-vif-collinearity-analysis-for-bias-mitigation-and-predictive-accuracy-18fbba3f7aa2&user=Good+Robots&userId=3abbfbfa9c59&source=post_page-3abbfbfa9c59----18fbba3f7aa2---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----18fbba3f7aa2--------------------------------)
    ·12 min read·Jul 31, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F18fbba3f7aa2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-the-vif-collinearity-analysis-for-bias-mitigation-and-predictive-accuracy-18fbba3f7aa2&user=Good+Robots&userId=3abbfbfa9c59&source=-----18fbba3f7aa2---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3abbfbfa9c59&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-the-vif-collinearity-analysis-for-bias-mitigation-and-predictive-accuracy-18fbba3f7aa2&user=Good+Robots&userId=3abbfbfa9c59&source=post_page-3abbfbfa9c59----18fbba3f7aa2---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----18fbba3f7aa2--------------------------------)
    ·12分钟阅读·2023年7月31日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F18fbba3f7aa2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-the-vif-collinearity-analysis-for-bias-mitigation-and-predictive-accuracy-18fbba3f7aa2&user=Good+Robots&userId=3abbfbfa9c59&source=-----18fbba3f7aa2---------------------clap_footer-----------)'
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F18fbba3f7aa2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-the-vif-collinearity-analysis-for-bias-mitigation-and-predictive-accuracy-18fbba3f7aa2&source=-----18fbba3f7aa2---------------------bookmark_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F18fbba3f7aa2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-the-vif-collinearity-analysis-for-bias-mitigation-and-predictive-accuracy-18fbba3f7aa2&source=-----18fbba3f7aa2---------------------bookmark_footer-----------)'
- en: In machine learning, collinearity is a complex puzzle for both seasoned professionals
    and newbies alike. Machine learning (ML) algorithms are optimised for predictive
    accuracy not explainability of predictors on the target. Also, most solutions
    for addressing collinearity, such as the ‘[***Variance Inflation Score***](https://www.statisticshowto.com/variance-inflation-factor/)***’***,
    and ‘[***Pearson’s cross-correlation analysis***](http://pubs.sciepub.com/ajams/8/2/1/index.html)’
    potentially leads to massive information loss in pre-processing.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，共线性是一个复杂的难题，无论是经验丰富的专家还是新手都面临挑战。机器学习（ML）算法的优化侧重于预测准确性，而非对目标的解释。此外，处理共线性的多数解决方案，如‘[***方差膨胀因子***](https://www.statisticshowto.com/variance-inflation-factor/)***’***和‘[***皮尔逊交叉相关分析***](http://pubs.sciepub.com/ajams/8/2/1/index.html)’，在预处理阶段可能会导致大量信息丢失。
- en: Most machine learning algorithms will select the best possible combination of
    features to optimize predictive accuracy. Therefore, even with collinearity, provided
    the correlations observed in training remain true in the real-world, collinearity
    is not a problem in machine learning. However, for a model’s explainability, the
    unchecked effects of collinearity are a potential source of bias.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习算法会选择最佳的特征组合以优化预测准确性。因此，即使存在共线性，只要训练中观察到的相关性在实际世界中仍然成立，共线性在机器学习中也不是问题。然而，对于模型的可解释性来说，共线性的
    unchecked 效果是潜在的偏差来源。
- en: '![](../Images/ecdab118843501b78f93e4362fba284d.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ecdab118843501b78f93e4362fba284d.png)'
- en: 'Figure 1: Collinearity Overview in The Boston Housing Dataset'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: 波士顿住房数据集中的共线性概览'
- en: '***Collinearity***, which refers to high correlation between independent variables
    (**IVs**) in a dataset, often presents unique challenges in the interpretation
    of regression models. Particularly, it interferes with determining the true reasons
    for the relationships in the data, which can lead to biased interpretations and
    unfair decisions. For example, in figure 1, the independent variables (TAX), (B)
    and (RAD) are collinear IVs and also good predictors of the dependent variable
    (MEDV). While ML algorithms will select the best combination of predictors, they
    may fail to account for the effect of adding another collinear variable (RM) to
    a model with any of these three variables.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '***共线性***指的是数据集中独立变量（**IVs**）之间的高相关性，通常在回归模型的解释中提出独特的挑战。特别是，它干扰了确定数据中关系的真实原因，这可能导致偏颇的解释和不公平的决策。例如，在图
    1 中，独立变量 (TAX)、(B) 和 (RAD) 是共线性 IVs，同时也是因变量 (MEDV) 的良好预测因子。虽然 ML 算法会选择最佳的预测因子组合，但它们可能无法考虑将另一个共线性变量
    (RM) 添加到这些三个变量中的任何一个模型的效果。'
- en: To encourage machine learners to take collinearity analysis seriously as a pre-processing
    step, there must be a way to balance the ***inflationary cost of keeping collinear
    variables*** and the ***predictive cost of dropping them***.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了鼓励机器学习者将共线性分析作为预处理步骤认真对待，必须有一种方法来平衡***保持共线性变量的膨胀成本***和***去除它们的预测成本***。
- en: Understanding Collinearity
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解共线性
- en: 'To demonstrate how unchecked collinearity leads to unintended bias, let’s use
    the cautionary tale of “*How not to collect data*”: the [***Boston Housing Dataset***](https://github.com/Good-Robots/Collinearity-Analysis/blob/main/housing.csv)***.***
    This dataset has since been debunked and withdrawn from public use because it
    contains a [“non-invertible](https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8)”
    variable “B”. The Collinear relationship between independent variables “B”, “RM”
    and “TAX” is a perfect case-study of how spurious correlations can suppress true
    relationships between IVs. The ‘[non-invertible](https://www.sciencedirect.com/science/article/abs/pii/0095069678900062)’
    transformation on “B”, (*A binary IV masquerading as a continuous IV*) introduces
    a moderating bias which may not be picked up by ML algorithms.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示 unchecked 共线性如何导致无意的偏差，让我们使用“*如何不收集数据*”的警示故事： [***波士顿住房数据集***](https://github.com/Good-Robots/Collinearity-Analysis/blob/main/housing.csv)***。***
    该数据集已经被揭穿并从公开使用中撤回，因为它包含了一个 [“不可逆](https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8)”
    变量 “B”。独立变量 “B”、“RM” 和 “TAX” 之间的共线性关系是虚假相关如何压制 IVs 之间真实关系的典型案例。对 “B” 的 [‘不可逆](https://www.sciencedirect.com/science/article/abs/pii/0095069678900062)’
    转换，（*一个伪装成连续 IV 的二元 IV*）引入了一种可能不会被 ML 算法检测到的调节偏差。
- en: '![](../Images/3eb95cd84eabbd62eafda2dacbdd7cea.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3eb95cd84eabbd62eafda2dacbdd7cea.png)'
- en: 'Figure 2: IVs — Boston Housing Dataset'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: IVs — 波士顿住房数据集'
- en: Consider the 13 ***independent variables (IVs)*** in the Boston housing dataset,
    with the median value of owner-occupied homes (MEDV) in a town as the dependent
    variable (***DV***). Certain features may appear to be strong predictors of the
    outcome, but this influence lies in their variance being largely explained by
    other predictors.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑波士顿住房数据集中的 13 个***独立变量（IVs）***，以城镇中的自有住房中位值（MEDV）作为因变量（***DV***）。某些特征可能看起来是结果的强预测因子，但这种影响在于它们的方差在很大程度上由其他预测因子解释。
- en: '![](../Images/16c5fdc0b0a3d4779938a257e4f8f625.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/16c5fdc0b0a3d4779938a257e4f8f625.png)'
- en: 'Figure 2: Cross Correlation Analysis — Boston Housing Dataset'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 交叉相关分析 — 波士顿住房数据集'
- en: 'In a bivariate relationship between an independent and a dependent variable,
    there is one of four possibilities when a new independent variable is introduced:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在独立变量与因变量之间的双变量关系中，当引入新的自变量时，有以下四种可能性：
- en: '**Spurious inflation**: The inclusion of the third IV significantly escalates
    the influence of the first IV on the DV.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**虚假膨胀**：引入第三个自变量显著增加了第一个自变量对因变量的影响。'
- en: '**Masking or suppression**: The new IV hides or suppresses the influence of
    the initial IV and on the dependent variable.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**掩盖或抑制**：新的自变量掩盖或抑制了初始自变量对因变量的影响。'
- en: '**Moderates or alters**: The new variable changes the direction of the original
    relationship for all or some observations in the independent variable.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**调节或改变**：新变量改变了自变量对因变量的原始关系的方向，适用于所有或某些观察值。'
- en: '**No effect**: The third IV provides no new information and has no effect on
    the IV and the DV.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**没有影响**：第三个自变量没有提供新信息，对自变量和因变量没有影响。'
- en: For machine learners, off-the-shelf solutions to collinearity often results
    in loss of predictive power, overfitted models, and bias. Hence, a solution that
    mitigates information loss is crucial.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习者来说，现成的共线性解决方案往往会导致预测能力丧失、过拟合模型和偏差。因此，减轻信息损失的解决方案至关重要。
- en: Evaluating Collinearity
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估共线性
- en: If two or more independent variables are highly correlated (RAD and TAX), the
    intuition behind collinearity is that they potentially provide exactly the same
    information about the influence of some ‘latent’ concept (Big Suburb houses/City
    Apartments) on the dependent variable (Property Value). In the presence of ‘Property
    Tax’, accessibility to radial highways provides no new information to property
    value (or vice versa). When IVs are meaninglessly highly correlated, the coefficients
    of a regression model becomes large which in turn leads to over-estimated inferences
    about the effects of the some factors on an outcome.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个或更多自变量高度相关（如RAD和TAX），共线性的直观解释是，它们可能提供了关于某个“潜在”概念（如大郊区房屋/城市公寓）对因变量（如房地产价值）影响的完全相同的信息。在存在“物业税”的情况下，通往辐射高速公路的可达性对房地产价值没有提供新的信息（反之亦然）。当自变量之间毫无意义地高度相关时，回归模型的系数会变得很大，从而导致对某些因素对结果影响的过度估计。
- en: Currently, there are two ways to deal with collinearity, none of which takes
    into account the dependent variable.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，有两种处理共线性的方法，但都没有考虑到因变量。
- en: '**Pairwise Correlation**: How many IVs are ‘highly’ correlated with each other.
    A cutoff for the correlation coefficient of ‘highly correlated’ features is subjective.
    However, it is the [general consensus](https://onlinelibrary.wiley.com/doi/full/10.1111/j.1600-0587.2012.07348.x)
    that collinearity becomes a serious problem at a correlation coefficient of +/-
    0.7.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**成对相关**：有多少自变量彼此“高度”相关。‘高度相关’特征的相关系数的阈值是主观的。然而，[普遍共识](https://onlinelibrary.wiley.com/doi/full/10.1111/j.1600-0587.2012.07348.x)是，当相关系数达到+/-
    0.7时，共线性成为一个严重问题。'
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**2\. Variance Inflation**: While a correlation coefficient confirms a degree
    of corresponding change between two IVs, it tells us little about the ***IV’s
    importance***. This is because, IVs, in a multivariate relationship, are not truly
    ***independent in their influence*** on the dependent variable (see Figure 1)
    and the true ***significance of their influence*** is in the presence of a combination
    of other **IV**s.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 方差膨胀**：虽然相关系数确认了两个自变量之间的变化程度，但它对***自变量的重要性***了解甚少。这是因为，在多元关系中，自变量在其对因变量的影响上并不真正***独立***（参见图1），它们的真实***影响显著性***是在其他**自变量**的组合存在下才显现出来。'
- en: 'The variance inflation score is the size of influence added to the coefficients
    of independent variables due to their dependence on other IVs. The VIF uses a
    ***‘leave-one-out’*** approach on the IVs, by treating each ***‘leave-out’***
    as a dependent variable and all ***‘leave-ins’*** as independent variables. So
    all IVs become dependent variables and each model produces an (***R2***) value.
    This R2 value indicates the percentage of variance in the ***‘leave-out’*** IV
    explained the by the ***‘leave-in’*** IVs. The VIF score is estimated as:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 方差膨胀评分是由于自变量之间的相互依赖而对回归系数的影响大小。VIF采用***“留一法”***方法，处理每一个***“留出”***作为因变量，所有***“留在”***作为自变量。因此，所有自变量变成因变量，每个模型生成一个(***R2***)值。这个R2值表示了***“留出”***自变量的方差百分比由***“留在”***自变量解释。VIF评分估算如下：
- en: '![](../Images/11aa5178c75d032ce66a4f3c6cf9961f.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11aa5178c75d032ce66a4f3c6cf9961f.png)'
- en: 'Figure 4: Variable Inflation Estimation — Boston Housing Dataset'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：变量膨胀估计 — 波士顿住房数据集
- en: Following the VIF estimations above, we would need to drop 11 of 13 IVs to fully
    address collinearity. Not only would this lead to massive information loss, but
    can potentially produce over-fitted or under-fitted models that perform poorly
    in the real-world.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述 VIF 估计，我们需要剔除 13 个 IV 中的 11 个，以彻底解决共线性问题。这不仅会导致大量信息丢失，还可能产生过度拟合或欠拟合的模型，这些模型在实际应用中表现不佳。
- en: In Machine learning, pairwise multi-correlation and VIF scores should not be
    the sole criteria for discarding or retaining features.
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在机器学习中，成对的多重相关性和 VIF 分数不应作为丢弃或保留特征的唯一标准。
- en: Certain features may still offer significant predictive value or contribute
    to the interpretation of the model despite high correlation and VIF scores.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管特征具有高相关性和 VIF 分数，但某些特征仍可能提供显著的预测价值或有助于模型解释。
- en: Inflationary VS Predictive Costs to Collinear Feature Selection
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 膨胀成本与预测成本在共线性特征选择中的对比
- en: To mitigate information loss, we can compare two values to measure ***inflationary
    cost of keeping collinear features*** and the ***predictive cost of dropping them***.
    Note that the VIF analysis is done independently of the outcome variable, so does
    not fully account for independent influence of individual IVs on the dependent
    variable.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻信息损失，我们可以比较两个值来衡量 ***保留共线性特征的膨胀成本*** 和 ***丢弃它们的预测成本***。请注意，VIF 分析是独立于结果变量的，因此未完全考虑单个
    IV 对因变量的独立影响。
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The first measure is the independent influence of an IV on the dependent variable
    i.e amount of variance in the dependent variable, ***independently*** explained
    by the IV (R_squared). For consistency, we will also estimate the VIF score from
    this R_squared value and call this VIF(IY) — Independent Importance. The second
    measure is the influence of the independent variable on the dependent variable
    in the presence of all IVs i.e the VIF estimated above. Lets call this VIF(IX)
    — Collective Importance.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个指标是 IV 对因变量的独立影响，即由 IV（R_squared）***独立地***解释的因变量方差。为了保持一致性，我们还将根据这个 R_squared
    值估算 VIF 分数，并称之为 VIF(IY) —— 独立重要性。第二个指标是在所有 IV 存在的情况下，独立变量对因变量的影响，即上述估算的 VIF。我们称之为
    VIF(IX) —— 集体重要性。
- en: '![](../Images/e7392eabce2644ac817325271855a666.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7392eabce2644ac817325271855a666.png)'
- en: 'Figure 5: Estimating Inflationary VS Predictive Cost of features to an ML Model'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：估算特征对 ML 模型的膨胀成本与预测成本
- en: Now, we can confidently estimate the true amount of ‘surprise’ you are letting
    go by dropping a collinear feature. In the graph below, the X-axis represents
    the variance in Y explained by each IV (a measure of potential ‘usefulness’ to
    prediction) while the Y axis represents the variance in the IV explained by other
    IVs (a measure of potential ‘bias’ to model). The bubble size in subplot 1 is
    the inflationary cost of keeping these variables in the model and in subplot 2
    is the predictive cost of dropping them.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以自信地估算通过丢弃一个共线性特征你将放弃的真正“惊讶”量。在下图中，X 轴代表每个 IV 解释的 Y 方差（对预测的潜在“有用性”度量），而
    Y 轴代表其他 IV 解释的 IV 方差（对模型的潜在“偏差”度量）。子图 1 中的气泡大小是将这些变量保留在模型中的膨胀成本，而子图 2 中则是丢弃它们的预测成本。
- en: Since this is a pre-processing step let’s use very liberal ‘cut-offs’ for the
    VIF(IX) factor of 20 (the redline) and VIF(IY) of 1.15 (the blue line). So that
    features under the redline are least explained by other IVs and features behind
    the blue line cannot independently predict (Y).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个预处理步骤，我们将对 VIF(IX) 因子的 20（红线）和 VIF(IY) 的 1.15（蓝线）使用非常宽松的“临界值”。这样，红线以下的特征由其他
    IV 解释最少，而蓝线后的特征无法独立预测（Y）。
- en: '![](../Images/5c82c220afce449655caf835e2f1f50a.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5c82c220afce449655caf835e2f1f50a.png)'
- en: 'Figure 6: Feature Inflationary VS Predictive Cost'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：特征的膨胀成本与预测成本
- en: This graph summarises the independent predictive power of an IV **VS** its potential
    for bias.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 该图总结了 IV 的独立预测能力 **VS** 其潜在偏差。
- en: '***Quadrant 1 — Potentially True Collinears***: These features (NOX, PTRATIO)
    are explained by some linear combination of IVs in the model and cannot independently
    predict the dependent variable (they may rely on some other set of IVs to be useful).
    Furthermore, whatever predictive power they have maybe cancelled out by some linear
    combination of other IVs (subplot 2). Their influence is significantly suppressed
    by the addition of some other IV(s).'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***象限 1 — 潜在真实共线性变量***：这些特征 (NOX, PTRATIO) 是由模型中某些 IV 的线性组合解释的，不能独立预测因变量（它们可能依赖于其他一组
    IV 才能有效）。此外，它们可能的预测能力可能被其他 IV 的线性组合所抵消（子图 2）。它们的影响被添加的其他 IV(s) 显著抑制。'
- en: '***Quadrant 2 — Potential Biasers***: These are related with both the dependent
    and other independent variable. Features (RM, TAX, B) seem independently predictive
    of (Y) but their variance is also explained by some other combination of independent
    variables. Their so-called independent predictive power on (Y) cannot be considered
    in isolation of some other (IVs). They can be extremely powerful predictors of
    the outcome or a become a source of bias when interpreting their significance
    to the outcome.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***象限 2 — 潜在偏差者***：这些变量与因变量和其他独立变量有关。特征 (RM, TAX, B) 似乎能独立预测 (Y)，但它们的方差也由其他独立变量组合解释。它们对
    (Y) 的所谓独立预测能力不能孤立于其他 (IVs) 考虑。当解释它们对结果的重要性时，它们可能是极其强大的预测变量或成为偏差来源。'
- en: '***Quadrant 3 — Dependents***: Although they are not independently predictive
    of (Y), there is a high predictive cost to dropping some of them. This is because
    they have unique information that is not explained by any other IV. The usefulness
    of this ‘unique’ information to predicting (Y), can only be considered in combination
    of other IV(s).'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***象限 3 — 依赖变量***：虽然它们并不是对 (Y) 的独立预测，但丢弃其中一些变量的预测成本很高。这是因为它们包含的独特信息是其他 IV 所无法解释的。这种‘独特’信息对预测
    (Y) 的有用性只能与其他 IV(s) 结合考虑。'
- en: '***Quadrant 4 — True Independent Predictors***: These variables are independently
    predictive of (Y). These variables also have unique information that is not explained
    by any other IV (more than Quadrant 3). The usefulness of this ‘unique’ information
    to predicting (Y) is independent of other IV(s). However, a linear combination
    of other IVs may have higher predictive power than their independent predictive.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***象限 4 — 真实独立预测变量***：这些变量能独立预测 (Y)。这些变量还包含其他 IV 所无法解释的独特信息（比象限 3 更多）。这种‘独特’信息对预测
    (Y) 的有用性独立于其他 IV(s)。然而，其他 IV 的线性组合可能比它们的独立预测能力具有更高的预测能力。'
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The variables ***B*** , ***TAX*** , and RM significantly predict each other
    and also independently predict the outcome. This could be the linear combination
    of IVs that bests predicts the DV (***MEDV)***. Alternatively, the predictive
    relevance any two of these IVs could be inflated or suppressed due to the presence
    of the third IV. To investigate this, each variable should be sequentially removed
    from a baseline model comprising all independent variables.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 变量 ***B***、***TAX*** 和 RM 彼此之间具有显著的预测关系，同时也独立预测结果。这可能是最能预测 DV (***MEDV***) 的
    IV 线性组合。或者，这些 IV 中任意两个的预测相关性可能因第三个 IV 的存在而被夸大或抑制。为了调查这一点，应将每个变量依次从包含所有独立变量的基线模型中移除。
- en: '![](../Images/a316c8bf4766c61b9fa51229d88ab9c0.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a316c8bf4766c61b9fa51229d88ab9c0.png)'
- en: 'Figure 7: Effects of IV Drops on Baseline Model'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：IV 丢弃对基线模型的影响
- en: Subsequently, the corresponding change in the significance of the remaining
    variables on the outcome should be quantified in terms of percentage. This procedure
    will help expose IVs explained by other IVs, pretending to be important.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，剩余变量对结果的显著性变化应以百分比形式量化。这一过程将有助于揭示被其他 IV 解释的 IV，假装自己很重要。
- en: Above the redline, removing collinear bias
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 红线以上，去除共线性偏差
- en: There are three main effects (of concern) collinear variables above the red
    line may have on the relationship between other IV(s) and the dependent variable.
    They may mediate (suppress), confound (exaggerate) or moderate (change).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 三个主要的影响（关注点）是，位于红线以上的共线性变量可能对其他 IV(s) 和因变量之间的关系产生的影响。它们可能会中介（抑制）、混淆（夸大）或调节（改变）。
- en: The concepts of moderators, mediators and confounders is not really discussed
    in Machine Learning. These concepts are often left to the ‘social scientists’,
    after all, they are ones who ever need to ‘interprete’ their coefficients. However,
    these concepts explain how collinearity can introduce bias to ML models.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 调节变量、中介变量和混杂变量的概念在机器学习中并没有真正讨论。这些概念通常留给“社会科学家”，毕竟，他们是需要“解释”他们的系数的人。然而，这些概念解释了共线性如何对机器学习模型引入偏差。
- en: Note that these effects cannot be truly established without deeper causal analysis,
    but for a bias removal pre-processing step, we can use simple definitions of these
    concepts to filter these relationships.
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请注意，这些效应在没有深入因果分析的情况下无法真正确定，但作为去除偏差的预处理步骤，我们可以使用这些概念的简单定义来过滤这些关系。
- en: '![](../Images/4f423c333f8ccfdb99bb3d26268ce125.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f423c333f8ccfdb99bb3d26268ce125.png)'
- en: 'Figure 8: mediating Relationships in Boston Housing Dataset'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：波士顿住房数据集中调解关系
- en: '***A mediator*** explains ‘how’ the IV and DV are related i.e the process by
    which they are related. A mediator must meet three criteria:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '***中介变量***解释了自变量和因变量是如何相关的，即它们之间相关的过程。一个中介变量必须满足三个标准：'
- en: a) Be significantly predictive of the first IV, b) be significantly predictive
    of the dependent variable and c) be significantly predictive of the dependent
    variable in the presence of the first IV.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: a) 对第一个自变量具有显著预测能力，b) 对因变量具有显著预测能力，c) 在第一个自变量存在的情况下，对因变量具有显著预测能力。
- en: It ‘mediates’ because its inclusion does not change the direction of the relationship
    between the first IV and the dependent variable. If a mediator is removed from
    a model, the strength of the relationship between the first IV and dependent variable
    should become stronger because the mediator was truly accounting for part of that
    effect.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 它“调解”是因为它的包含不会改变第一个自变量和因变量之间的关系方向。如果从模型中移除一个中介变量，第一个自变量和因变量之间的关系强度应该会变强，因为中介变量确实解释了部分效果。
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: For example, in the relationship between (RM), (TAX) and (MEDV), the number
    of rooms potentially explains how property tax is related to its property value.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在（RM）、（TAX）和（MEDV）之间的关系中，房间数量可能解释了物业税与其物业价值的关系。
- en: '***Confounders*** are elusive as it is difficult to define them in terms of
    correlations and significance. A confounding variable is an external variable
    that correlates with both the dependent and independent variables, thus potentially
    distorting the perceived relationship between them. As opposed to mediators, the
    relationship between the first IV and the dependent variable is meaningless. There
    is also no guarantee that removing the confounder will weaken or strengthen the
    relationship between the first IV and the dependent variable.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '***混杂变量***是难以捉摸的，因为很难用相关性和显著性来定义它们。混杂变量是一个与因变量和自变量都相关的外部变量，因此可能会扭曲它们之间的感知关系。与中介变量不同，第一个自变量和因变量之间的关系没有意义。也没有保证移除混杂变量会削弱或增强第一个自变量和因变量之间的关系。'
- en: The number of rooms in a house can either mediate or confound the relationship
    between the proportion of black population and property value. Well, according
    to [this paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2819361/#:~:text=Mediation%20involves%20a%20distinctly%20causal,examine%20undistorted%20estimates%20of%20effects.),
    it depends on the relationship between (B) and (RM). If the relationship between
    (RM <-> MEDV) and (RM <-> B) are ***in the same direction***, removing (RM) should
    weaken the effect of (B) on (MEDV). However, if relationship between (RM <-> MEDV)
    and (RM <-> B) are ***in the opposite direction***, removing (RM) should strengthen
    (B).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 房间数量可以调解或混淆黑人人口比例与物业价值之间的关系。根据[这篇论文](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2819361/#:~:text=Mediation%20involves%20a%20distinctly%20causal,examine%20undistorted%20estimates%20of%20effects.)，这取决于（B）和（RM）之间的关系。如果（RM
    <-> MEDV）和（RM <-> B）之间的关系是***相同方向的***，移除（RM）应该会削弱（B）对（MEDV）的影响。然而，如果（RM <-> MEDV）和（RM
    <-> B）之间的关系是***相反方向的***，移除（RM）应该会增强（B）。
- en: (RM <—> MEDV) and (RM <-> B) are in the same direction (subplot 3 of figure
    1), however, removing (RM) strengthens the effect of (B).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: （RM <—> MEDV）和（RM <-> B）是相同方向的（图1的子图3），然而，移除（RM）增强了（B）的效果。
- en: But see the figure below, where we there is a good decision boundary for a third
    IV in the relationship between the first IV and DV. This indicates a different
    type of relationship between (RM) and (TAX) based on the value of (B).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 但请参见下图，其中我们看到在第一个自变量与因变量之间的关系中，第三个自变量有一个良好的决策边界。这表明根据(B)的值，(RM)与(TAX)之间存在不同类型的关系。
- en: '![](../Images/b5182633423e496832bd3e96b7e44ad5.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5182633423e496832bd3e96b7e44ad5.png)'
- en: 'Figure 9: Moderating Regression'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：调节回归
- en: With ***moderators***, the relationship between the first IV and the dependent
    variable is different based on the value of the moderator. What property tax can
    you expect to pay on a house that costs $100,00? Well, it depends on the proportion
    of black population in the town and the number of rooms in that house. Infact,
    there are a set of towns whose property tax stays consistent, regardless of the
    number of rooms, provided (B) remains below a certain threshold.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 有了***调节变量***，第一个自变量与因变量之间的关系会根据调节变量的值有所不同。你可以期望对价值$100,000的房子支付多少物业税？这取决于该镇的黑人比例和房间数量。事实上，有一组城镇的物业税保持一致，无论房间数量如何，只要(B)保持在某个阈值以下。
- en: '![](../Images/d5b1251f303feb908eae9b2c2bb77612.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d5b1251f303feb908eae9b2c2bb77612.png)'
- en: 'Figure 10: Moderating Relationship Boston Housing Dataset'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：调节关系 波士顿住房数据集
- en: Moderators are usually categorical features or groups in the data. Conventional
    pre-processing steps for groups create dummy variables for each group label. This
    [potentially addresses](https://www.researchgate.net/post/Do-you-need-to-dummy-code-sex-to-run-it-as-a-moderator-variable-in-moderated-multiple-regression)
    any moderating effect from that group on the dependent variable. However, ranked
    variables or continuous variables with low variance (B) can also be moderators.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 调节变量通常是数据中的类别特征或组。常规的预处理步骤为每个组标签创建虚拟变量。这[可能解决](https://www.researchgate.net/post/Do-you-need-to-dummy-code-sex-to-run-it-as-a-moderator-variable-in-moderated-multiple-regression)来自该组对因变量的任何调节效应。然而，排名变量或具有低方差的连续变量(B)也可以是调节变量。
- en: Conclusion
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In conclusion, while collinearity is a challenging issue in regression modelling,
    its careful evaluation and management can enhance the predictive power and reliability
    of machine learning models. The ability to account for information loss, provides
    an effective framework for feature selection, enabling the balance of explainability
    and predictive accuracy.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，虽然共线性是回归建模中的一个挑战性问题，但通过仔细评估和管理，可以增强机器学习模型的预测能力和可靠性。能够考虑信息损失，提供了一个有效的特征选择框架，平衡了解释性和预测准确性。
