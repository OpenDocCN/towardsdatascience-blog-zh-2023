- en: 'Demystifying DreamBooth: A New Tool for Personalizing Text-To-Image Generation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 揭秘DreamBooth：一种个性化文本到图像生成的新工具
- en: 原文：[https://towardsdatascience.com/demystifying-dreambooth-a-new-tool-for-personalizing-text-to-image-generation-70f8bb0cfa30](https://towardsdatascience.com/demystifying-dreambooth-a-new-tool-for-personalizing-text-to-image-generation-70f8bb0cfa30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/demystifying-dreambooth-a-new-tool-for-personalizing-text-to-image-generation-70f8bb0cfa30](https://towardsdatascience.com/demystifying-dreambooth-a-new-tool-for-personalizing-text-to-image-generation-70f8bb0cfa30)
- en: Exploring the technology that turns boring images into creative masterpieces
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索将无聊图像转化为创意杰作的技术
- en: '[](https://mnslarcher.medium.com/?source=post_page-----70f8bb0cfa30--------------------------------)[![Mario
    Larcher](../Images/b5b443807fe06f096ed4fc5139b3cb42.png)](https://mnslarcher.medium.com/?source=post_page-----70f8bb0cfa30--------------------------------)[](https://towardsdatascience.com/?source=post_page-----70f8bb0cfa30--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----70f8bb0cfa30--------------------------------)
    [Mario Larcher](https://mnslarcher.medium.com/?source=post_page-----70f8bb0cfa30--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mnslarcher.medium.com/?source=post_page-----70f8bb0cfa30--------------------------------)[![Mario
    Larcher](../Images/b5b443807fe06f096ed4fc5139b3cb42.png)](https://mnslarcher.medium.com/?source=post_page-----70f8bb0cfa30--------------------------------)[](https://towardsdatascience.com/?source=post_page-----70f8bb0cfa30--------------------------------)[![数据科学前沿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----70f8bb0cfa30--------------------------------)
    [Mario Larcher](https://mnslarcher.medium.com/?source=post_page-----70f8bb0cfa30--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----70f8bb0cfa30--------------------------------)
    ·13 min read·Jun 13, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[数据科学前沿](https://towardsdatascience.com/?source=post_page-----70f8bb0cfa30--------------------------------)
    ·阅读时间13分钟·2023年6月13日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/cd417c1c9a70877d284ffb874caaea6a.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cd417c1c9a70877d284ffb874caaea6a.png)'
- en: Dougie and his new personality created with DreamBooth by the author. Can you
    guess the prompt?
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Dougie和他的新个性由作者使用DreamBooth创建。你能猜出提示是什么吗？
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Imagine the joy of effortlessly generating a new image of your beloved puppy
    against the backdrop of the Acropolis in Athens. Not satisfied yet, you would
    like to see how Van Gogh would have painted your best friend or what he would
    look like if he had been conceived by a lion 😱! Thanks to DreamBooth, all of this
    is a reality, and today it is possible to make any animal, object or ourselves
    travel in fantasy from a handful of images.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你轻松地生成一张你心爱的幼犬在雅典卫城背景下的新图像的喜悦。如果还不满足，你还想看看梵高会如何绘制你的好友，或者他如果被狮子所构思会是什么样子😱！感谢DreamBooth，这一切都变为现实，如今可以让任何动物、物体或我们自己从一小堆图像中旅行于幻想世界。
- en: 'While many of us have already seen on social media the mind-blowing results
    that can be achieved with this technology and there is no shortage of tutorials
    so that we can try it on our own photographs, rarely someone has tried to answer
    the question: yes, but how the hell does it work?'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们许多人已经在社交媒体上看到了利用这项技术可以取得的令人瞩目的成果，而且有大量教程可以让我们在自己的照片上进行尝试，但很少有人尝试回答这样一个问题：是的，那么它到底是如何工作的呢？
- en: 'In this article, I will do my best tobreak down the scientific paper[DreamBooth:
    Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation](https://arxiv.org/abs/2208.12242)
    by Ruiz et al. from which everything started. But don’t worry, I’ll simplify the
    complex parts and give explanations where they might require some prior knowledge.
    Now, fair warning, this is an advanced topic, so I assume you’ve got the basics
    of deep learning and related stuff covered. But hey, if you want to dig deeper
    into diffusion models or other cool topics, I’ll drop some references along the
    way. Let’s dive in!'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '在本文中，我将尽力解析Ruiz等人发表的科学论文[DreamBooth: 针对主题驱动生成的文本到图像扩散模型微调](https://arxiv.org/abs/2208.12242)，这篇论文是所有这一切的起点。但别担心，我会简化复杂的部分，并在需要一些先验知识的地方进行解释。现在，请注意，这是一项高级话题，因此我假设你已经掌握了深度学习及相关内容的基础知识。如果你想深入了解扩散模型或其他有趣的话题，我会在过程中提供一些参考。让我们开始吧！'
- en: Related Work
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相关工作
- en: '![](../Images/5008a1303ab977b9e8dd3ce5fdd17410.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5008a1303ab977b9e8dd3ce5fdd17410.png)'
- en: 'Fig. 7 from [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven
    Generation](https://arxiv.org/abs/2208.12242).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '图7来自[DreamBooth: 针对主题驱动生成的文本到图像扩散模型微调](https://arxiv.org/abs/2208.12242)。'
- en: Before we get into the nitty-gritty of DreamBooth’s approach, let’s take a closer
    look at the related work and tasks associated with this technique.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨DreamBooth的方法之前，让我们先仔细了解一下与该技术相关的工作和任务。
- en: Image Composition
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像合成
- en: Amidst the chaos of everyday life, it’s been far too long since your beloved
    backpack embarked on a globetrotting journey. It is time to infuse it with an
    exciting dose of adventure while you are planning your next vacation. Enter image
    composition, merge your subject seamlessly into new backgrounds, letting your
    backpack travel from the Grand Canyon to Boston in seconds.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在日常生活的喧嚣中，你心爱的背包已经很久没有踏上环球之旅。现在是给它注入刺激冒险的时刻，同时你也在规划下一次假期。通过图像合成，将你的背包无缝融入新的背景，让它在几秒钟内从大峡谷到波士顿。
- en: If simply copy-pasting the subject doesn’t fulfill your desire for new perspectives,
    one possibility is to explore the application of 3D reconstruction techniques.
    However, it’s important to note that these techniques are primarily designed for
    rigid objects and often require a substantial number of starting views.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果简单地复制粘贴主题不能满足你对新视角的渴望，可以尝试探索3D重建技术的应用。然而，需要注意的是，这些技术主要针对刚性物体，并且通常需要大量的起始视图。
- en: DreamBooth introduces a remarkable capability to generate fresh poses within
    new contexts while smoothly incorporating crucial elements such as lighting, shadows,
    and other scene-relative aspects. Achieving such consistency has proven challenging
    with prior methodologies. In the paper, this task is also denoted by the name
    recontextualization.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: DreamBooth引入了一项卓越的能力，可以在新的背景中生成新姿势，同时顺畅地融入关键元素，如光线、阴影和其他与场景相关的方面。实现这种一致性在以往的方法中一直是一个挑战。在论文中，这项任务也被称为重新背景化。
- en: Text-to-Image Editing and Synthesis
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本到图像编辑与合成
- en: Image editing based on textual input is a secret dream cherished by many avid
    users of photo editing software. Early methodologies, such as those employing
    GANs, demonstrated impressive results, but only in well-structured scenarios like
    editing human faces.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 基于文本输入的图像编辑是许多照片编辑软件爱好者的一个秘密梦想。早期的方法，例如使用GANs的方法，展示了令人印象深刻的结果，但仅限于像编辑人脸这样结构良好的场景。
- en: Even new approaches that take advantage of diffusion models have limitations
    and are usually restricted to global editing. Only recently have advances such
    as [Text2LIVE](https://text2live.github.io/) emerged that allow localized editing.
    However, none of these techniques allow the generation of a given subject in new
    contexts.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是利用扩散模型的新方法也有其局限性，通常仅限于全局编辑。直到最近，像[Text2LIVE](https://text2live.github.io/)这样的进展才允许局部编辑。然而，这些技术都无法在新的背景中生成特定的主题。
- en: Although text-image synthesis models like [Imagen](https://imagen.research.google/),
    [DALL·E 2](https://openai.com/product/dall-e-2), and [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release)
    have made significant strides, the attainment of fine-grained control and the
    preservation of subject identity in synthesized images continue to pose substantial
    challenges.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管像[Imagen](https://imagen.research.google/)、[DALL·E 2](https://openai.com/product/dall-e-2)和[Stable
    Diffusion](https://stability.ai/blog/stable-diffusion-public-release)这样的文本图像合成模型取得了显著进展，但在合成图像中实现精细控制并保留主题身份仍然面临重大挑战。
- en: Controllable Generative Models
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可控生成模型
- en: To avoid subject modification, many approaches rely on a user-provided mask
    that limits the area to be modified. Inversion techniques, such as the one used
    by [DALL·E 2](https://openai.com/product/dall-e-2), present an effective solution
    for preserving the subject while modifying the context.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免对主题进行修改，许多方法依赖于用户提供的掩码来限制修改的区域。逆转技术，如[DALL·E 2](https://openai.com/product/dall-e-2)使用的技术，提供了一个有效的解决方案，可以在修改背景的同时保留主题。
- en: '[Prompt-to-Prompt](https://prompt-to-prompt.github.io/) enables both local
    and global editing without the need for an input mask.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[Prompt-to-Prompt](https://prompt-to-prompt.github.io/)使得本地和全局编辑成为可能，无需输入掩码。'
- en: However, these methods do not adequately preserve the identity while generating
    novel samples of a subject.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些方法在生成新样本时无法充分保留主题的身份。
- en: While some GAN-based methods focus on generating instance variations, they often
    have limitations. For instance, they are primarily designed for the face domain,
    require many instances of the input subject, struggle with unique subjects, and
    fail to preserve important subject details.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管一些基于GAN的方法专注于生成实例变体，但它们往往有局限性。例如，它们主要设计用于面部领域，需要大量的输入样本，难以处理独特的主题，并且无法保留重要的主题细节。
- en: Finally, recently Gal et. al. presented Textual Inversion, a methodology with
    features common to DreamBooth but which, as we will see, is limited by the expressiveness
    of the frozen diffusion model on which it is based.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，最近Gal等人提出了文本反演，这是一种具有DreamBooth共同特征的方法论，但正如我们将看到的，它受到基于其的冻结扩散模型表现力的限制。
- en: '![](../Images/d3075f2bc0ac3f085e9b3dd435e0f272.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d3075f2bc0ac3f085e9b3dd435e0f272.png)'
- en: 'Fig. 2 from [An Image is Worth One Word: Personalizing Text-to-Image Generation
    using Textual Inversion](https://arxiv.org/abs/2208.01618).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图2来自[图像胜于千言：使用文本反演个性化文本到图像生成](https://arxiv.org/abs/2208.01618)。
- en: Since this is the work with which the authors compare DreamBooth, it is worth
    providing a brief description of it.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是作者用来与DreamBooth进行比较的工作，值得提供一个简要的描述。
- en: '**Textual Inversion** starts from a pre-trained diffusion model, such as Latent
    Diffusion, and defines a new placeholder string S*, to represent the new concept
    to be learned. At this point, keeping the diffusion model frozen, the new embedding
    is fine-tuned from just 3–5 images, similar to DreamBooth. If this brief description
    is not clear enough, wait until you read the more detailed description of DreamBooth,
    which has many points in common with this work.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**文本反演**从一个预训练的扩散模型开始，如潜在扩散模型，并定义一个新的占位符字符串S*，以表示需要学习的新概念。在此阶段，保持扩散模型冻结，新的嵌入从仅3-5张图像中进行微调，类似于DreamBooth。如果这个简要描述不够清楚，请等到你阅读更详细的DreamBooth描述时，它与这项工作有许多共同点。'
- en: Method
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法
- en: '![](../Images/98f3ebfcca65a8f1de98a0d286677bf1.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/98f3ebfcca65a8f1de98a0d286677bf1.png)'
- en: 'Fig. 3 from [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven
    Generation](https://arxiv.org/abs/2208.12242).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图3来自[DreamBooth：为主题驱动生成微调文本到图像扩散模型](https://arxiv.org/abs/2208.12242)。
- en: 'Before describing the components of **DreamBooth** in detail, let’s see **schematically**
    how this technology works:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在详细描述**DreamBooth**的组件之前，让我们**简要**了解一下这项技术的工作原理：
- en: Choose 3–5 images of your favorite subject, it can be an animal, an object or
    even an abstract concept such as an art style.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择3-5张你喜欢的主题图像，可以是动物、物体，甚至是像艺术风格这样的抽象概念。
- en: Associate this concept with a rare word to which corresponds a unique token
    that will represent it from now on, in the scientific paper the authors call this
    word [V].
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这个概念与一个稀有词汇关联，该词汇对应一个唯一的标记，将从现在开始表示它，在科学论文中，作者称这个词为[V]。
- en: Fine-tune the model using images of the subject of interest with a simple prompt
    such as “A [V] [class noun]”, for example “A [V] dog” if the input images are
    photographs of your dog.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用兴趣主题的图像，通过简单的提示如“一个[V] [类别名]”来微调模型，例如，如果输入图像是你的狗的照片，则为“一个[V] 狗”。
- en: Since we are fine-tuning all the parameters of the model, there is a risk that
    at this point all dogs (or whatever class our subject is) will become the same
    as our input images. To avoid this degradation of our model, we generate images
    from our frozen model with a prompt such as “A dog” (or “A [class noun]”) and
    add a loss that penalizes when the images generated by our model that we are fine-tuning
    for this prompt deviate from those generated by the frozen model.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们正在微调模型的所有参数，因此有风险在这个阶段所有的狗（或我们主题的任何类别）都会变成与我们的输入图像相同。为了避免模型的这种退化，我们从冻结的模型生成图像，使用像“狗”（或“[类别名]”）这样的提示，并添加一个损失函数，当我们为这个提示微调的模型生成的图像偏离冻结模型生成的图像时，会受到惩罚。
- en: Okay, now that we have a high-level idea of the procedure, let’s go into more
    detail about the various components.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们对过程有了一个高层次的了解，让我们详细讨论各种组件。
- en: Text to Image Diffusion Models
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本到图像扩散模型
- en: Do you really want to learn how diffusion models work and, in particular, latent
    diffusion models such as Stable Diffusion? Read my previous article below, I will
    be waiting for you here when you are done!
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你真的想了解扩散模型的工作原理，尤其是像稳定扩散这样的潜在扩散模型吗？请阅读我之前的文章，当你读完后，我会在这里等你！
- en: '[](/paper-explained-high-resolution-image-synthesis-with-latent-diffusion-models-f372f7636d42?source=post_page-----70f8bb0cfa30--------------------------------)
    [## Paper Explained — High-Resolution Image Synthesis with Latent Diffusion Models'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 论文解读——基于潜在扩散模型的高分辨率图像合成](/paper-explained-high-resolution-image-synthesis-with-latent-diffusion-models-f372f7636d42?source=post_page-----70f8bb0cfa30--------------------------------)'
- en: While OpenAI has dominated the field of natural language processing with their
    generative text models, their image…
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 虽然OpenAI凭借其生成文本模型主导了自然语言处理领域，但他们的图像…
- en: towardsdatascience.com](/paper-explained-high-resolution-image-synthesis-with-latent-diffusion-models-f372f7636d42?source=post_page-----70f8bb0cfa30--------------------------------)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/paper-explained-high-resolution-image-synthesis-with-latent-diffusion-models-f372f7636d42?source=post_page-----70f8bb0cfa30--------------------------------)'
- en: OK, maybe you don’t want a whole explanation, in which case I will give here
    the intuition behind **diffusion models**, which is very simple.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，也许你不需要完整的解释，在这种情况下，我将提供**扩散模型**背后的直观理解，这非常简单。
- en: '![](../Images/13faf499b66a3c034aa4a875c8587d68.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13faf499b66a3c034aa4a875c8587d68.png)'
- en: Fig. 2\. from [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 来自 [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)。
- en: Take an image x0 and add a certain amount of noise (e.g. Gaussian noise) proportional
    to a certain timestep *t*. If *t* is zero the added noise will be zero, if *t*
    > 0 the added noise will be as large as *t* is, until you arrive at an image that
    is just noise.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取一个图像x0，并添加一定量的噪声（例如，高斯噪声），噪声量与某个时间步* t *成比例。如果*t*为零，则添加的噪声为零，如果*t* > 0，则添加的噪声将与*t*的大小一样，直到你得到一个仅由噪声组成的图像。
- en: Train a model, such as a U-Net, to predict the noise-free image (or the noise
    that has been added) by giving as input to the model the timestep *t* and the
    corrupted image.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个模型，如U-Net，通过将时间步* t *和受损图像作为输入来预测无噪声图像（或添加的噪声）。
- en: At this point, having trained a model that can remove noise from an image, we
    can sample an image composed only of noise and gradually remove it (doing it all
    at once works poorly) either by predicting the image without noise or by predicting
    the noise and subtracting it from the image.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此时，经过训练一个可以去除图像噪声的模型，我们可以采样一个仅由噪声组成的图像，并逐渐去除噪声（一次性完成效果不好），可以通过预测无噪声的图像或预测噪声并从图像中减去来实现。
- en: The first three points describe an unconditional diffusion model. In order to
    produce a conditional output based on textual prompt, the text is encoded using
    models like [CLIP](https://openai.com/research/clip), or language models such
    as [BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html),
    [T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html),
    and others. This encoding step allows for the integration of additional information,
    which is then fed as input to the model alongside the corrupted image and the
    timestep *t*.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前三点描述了无条件扩散模型。为了根据文本提示生成条件输出，文本使用像 [CLIP](https://openai.com/research/clip)
    的模型进行编码，或者使用如 [BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)、[T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)
    等语言模型。这个编码步骤允许集成额外的信息，然后将其与受损图像和时间步* t *一起输入模型。
- en: 'The authors in the paper use two diffusion models: Google’s [Imagen](https://imagen.research.google/)
    (also DreamBooth is from [Google Research](https://research.google/)) and [Stable
    Diffusion](https://stability.ai/blog/stable-diffusion-public-release)from [Stability
    AI](https://stability.ai/), the main open-source text-to-image model.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 论文中的作者使用了两个扩散模型：Google的 [Imagen](https://imagen.research.google/)（DreamBooth
    也来自 [Google Research](https://research.google/)）和 [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release)来自
    [Stability AI](https://stability.ai/)，这是主要的开源文本到图像模型。
- en: '**Imagen** employs a multi-resolution strategy to enhance the quality of generated
    images. Initially, a diffusion model is trained using low-resolution 64x64 images.
    The output of the low-resolution model is then upscaled by two additional diffusion
    models that operate at higher resolutions, 256x256 and 1024x1024\. The first model
    specializes in capturing macro-details, while the subsequent models refine the
    output by leveraging the conditioning effect of the lower resolution model’s generated
    image. This iterative refinement facilitates the generation of high-resolution
    images with improved quality and fidelity.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**Imagen** 采用多分辨率策略来提高生成图像的质量。最初，使用低分辨率 64x64 图像训练扩散模型。然后，低分辨率模型的输出通过两个额外的扩散模型进行放大，这些模型在更高分辨率下操作，分别为
    256x256 和 1024x1024。第一个模型专注于捕捉宏观细节，而随后的模型则通过利用较低分辨率模型生成图像的条件效应来精细化输出。这种迭代优化有助于生成高分辨率的图像，具有更好的质量和保真度。'
- en: '**Stable Diffusion** instead, as a latent diffusion model, introduces a three-step
    approach to enhance the efficiency of training and generating high-resolution
    images. Initially, a Variational Autoencoder (VAE) is trained to compress a high-resolution
    image. From this point onward, the process closely resembles that of standard
    diffusion models, with one key distinction: instead of employing the original
    image as input, the latent representation generated by the VAE encoder is utilized.
    Subsequently, the output of the inverse diffusion process is then restored to
    the original resolution using the VAE decoder. For a more comprehensive understanding
    of this entire procedure, I delve into greater detail in the aforementioned article.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**Stable Diffusion** 作为一种潜在扩散模型，采用三步法来提高训练和生成高分辨率图像的效率。最初，训练一个变分自编码器（VAE）以压缩高分辨率图像。从此之后，过程与标准扩散模型非常相似，一个关键区别在于：不是使用原始图像作为输入，而是使用由
    VAE 编码器生成的潜在表示。随后，逆扩散过程的输出通过 VAE 解码器恢复到原始分辨率。为了更全面地理解整个过程，我在上述文章中进行了更详细的探讨。'
- en: Personalization of Text to Image Models
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本到图像模型的个性化
- en: DreamBooth aims to place the subject instance (e.g. your dog) within the output
    domain of the model, enabling the model to generate fresh images of the subject
    upon query. An advantage of diffusion models, as opposed to GANs, is their ability
    to effectively incorporate new information into their domain while retaining knowledge
    of previous data and avoiding overfitting to a limited training image set.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: DreamBooth 旨在将主题实例（例如你的狗）置于模型的输出领域内，使模型能够在查询时生成主题的新图像。扩散模型的一个优势是，与 GANs 相比，它们能够有效地将新信息纳入其领域，同时保留对先前数据的知识，并避免对有限的训练图像集的过拟合。
- en: Designing Prompts for Few-Shot Personalization
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为少样本个性化设计提示
- en: As mentioned above, the model undergoes training using simplistic prompts structured
    as “a [identifier] [class noun]”. Here, [identifier] represents a distinct identifier
    associated with the subject, and [class noun] serves as a general description
    of the subject’s category (such as cat, dog, watch, etc.). The authors incorporate
    the class noun into the prompt to establish a connection between the general class
    and our individual subject, observing that using an incorrect or missing class
    noun leads to longer training times and language drift, ultimately affecting performance.
    Essentially, the main aim is to capitalize on the relationship between the specific
    class and our subject, utilizing the existing knowledge acquired by the model
    about the class. This enables us to generate fresh poses and variations of the
    subject across various contexts.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，该模型通过使用“一个 [identifier] [class noun]”结构的简单提示进行训练。这里，[identifier] 代表与主题相关的独特标识符，而
    [class noun] 作为主题类别的一般描述（如猫、狗、手表等）。作者将类名纳入提示中，以建立通用类别与我们个体主题之间的联系，观察到使用不正确或缺失的类名会导致更长的训练时间和语言漂移，*最终影响性能*。本质上，主要目的是利用特定类别与我们的主题之间的关系，利用模型对该类别已有的知识。这使我们能够在各种上下文中生成新颖的姿势和变体。
- en: Rare-Token Identifiers
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 稀有标记标识符
- en: The paper highlights that common English words are not ideal in this context
    since the model needs to disassociate them from their original meaning and reintegrate
    them to refer to our subject.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 论文强调，普通的英语单词在这种情况下并不理想，因为模型需要将它们与原始含义脱离，并重新整合以指代我们的主题。
- en: To address this, the authors propose using an identifier that has a weak prior
    in both the language and diffusion models. While selecting random characters like
    “xxy5syt00” may initially appear appealing, it poses potential risks. It is important
    to consider that the tokenizer could tokenize each letter individually. So, what
    is the solution? The most effective approach involves identifying uncommon tokens
    in the vocabulary and then inverting these tokens within the text space. This
    minimizes the likelihood of the identifier having a strong prior.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，作者提出使用在语言和扩散模型中都有弱先验的标识符。虽然选择像“xxy5syt00”这样的随机字符可能最初看起来很吸引人，但这也存在潜在风险。需要考虑的是，分词器可能会将每个字母单独分词。那么解决方案是什么？最有效的方法是识别词汇表中不常见的标记，然后在文本空间中反转这些标记。这可以最小化标识符具有强先验的可能性。
- en: Funny enough, most tutorials use “sks” for this purpose but, as pointed out
    by one of the authors, this seemingly harmless word can have side effects…
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，大多数教程使用“sks”来实现这一目的，但正如其中一位作者指出的，这个看似无害的词可能会产生副作用……
- en: Class-Specific Prior Preservation Loss
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 类别特定先验保持损失
- en: '![](../Images/5aa015342afc90ecfbdf58e6f45ff2ce.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5aa015342afc90ecfbdf58e6f45ff2ce.png)'
- en: 'Fig. 6 from [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven
    Generation](https://arxiv.org/abs/2208.12242).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven
    Generation](https://arxiv.org/abs/2208.12242)中的图6。'
- en: Unlike Textual Inversion, DreamBooth fine-tunes all layers of the model to maximize
    performance. Unfortunately, doing so runs into the well-known problem of **language
    drift**, i.e. when a model is initially pre-trained on an extensive text corpus
    and subsequently fine-tuned for a particular task, it gradually diminishes its
    understanding of the language’s syntax and semantics.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 与文本反演不同，DreamBooth 微调模型的所有层以最大化性能。不幸的是，这样做会遇到众所周知的**语言漂移**问题，即当一个模型最初在一个广泛的文本语料库上进行预训练，然后再针对特定任务进行微调时，它会逐渐减少对语言语法和语义的理解。
- en: An additional issue arises from the potential reduction in output diversity.
    This can be observed in the second row of Fig. 6, where the model, unless further
    adapted, has a tendency to replicate solely the poses found in the input images.
    This effect becomes more pronounced when the model is trained for an extended
    duration.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是输出多样性的潜在减少。这可以从图6的第二行中观察到，在该图中，模型，除非进一步调整，否则有倾向仅复制输入图像中找到的姿势。当模型训练时间较长时，这种效果变得更加明显。
- en: 'To mitigate these problems, the authors introduce a class-specific prior preservation
    loss, let’s look at its overall loss formula and then explain its components:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻这些问题，作者引入了类别特定先验保持损失，让我们先看看其整体损失公式，然后再解释其组成部分。
- en: '![](../Images/ed2415b9465140a1046fad3604722833.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed2415b9465140a1046fad3604722833.png)'
- en: 'Eq. 2 from [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven
    Generation](https://arxiv.org/abs/2208.12242).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven
    Generation](https://arxiv.org/abs/2208.12242)中的公式2。'
- en: The first part is the standard L2 denoising error, typical of any diffusion
    model. *α_t* scales the initial image **x** to which is then added a Gaussian
    noise **ε** ∼ *N* (0, I), multiplied by *σ_t*. The random variable **z_*t*** :=
    *α_t****x** + *σ_t****ε** then has distribution *N*(*α_t****x**, *σ_t²*). The
    model **x**ˆ*_θ* at this point will try to predict the original image from **z**_*t*,
    *t* and the conditioning vector **c** = Γ(**P**), where Γ in the case of DreamBooth
    is [T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)
    and the prompt **P** has the form “a [identifier] [class noun]”.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分是标准的L2去噪误差，这是任何扩散模型的典型特征。*α_t* 将初始图像**x** 缩放，然后添加高斯噪声 **ε** ∼ *N* (0, I)，乘以
    *σ_t*。随机变量 **z_*t*** := *α_t****x** + *σ_t****ε** 的分布为 *N*(*α_t****x**, *σ_t²*)。此时，模型
    **x**ˆ*_θ* 将尝试从 **z**_*t*、*t* 和条件向量 **c** = Γ(**P**) 预测原始图像，其中在 DreamBooth 的情况下，Γ
    是 [T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)，而提示
    **P** 的形式为“一个 [标识符] [类别名词]”。
- en: The second part is the **prior preservation loss**, in this instead of **x**
    we have **x**_pr, an image generated by the model with frozen weights (before
    fine-tuning) from a random initial noise **z**_*1* ∼ *N* (0, I) and conditioning
    vector **c**_pr = Γ(“a [class noun]”). This part then pushes the model to re-obtain
    **x**_pr from a corrupted version of it, thus pushing the model to generate images
    similar to those generated before the fine-tuning process for instances of the
    subject class.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 第二部分是 **先验保留损失**，在这里，**x** 被替换为 **x**_pr，即由模型生成的图像，模型的权重被冻结（在微调之前），从随机初始噪声 **z**_*1*
    ∼ *N* (0, I) 和条件向量 **c**_pr = Γ(“一个 [类别名词]”)。这一部分促使模型从其损坏版本中重新获取 **x**_pr，从而促使模型生成类似于在微调过程之前生成的图像。
- en: Finally, *w_t* and *w_t*’ are terms related to the noise schedule and *λ* defines
    the relative weight to be given to the two losses.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，*w_t* 和 *w_t*’ 是与噪声调度相关的术语，*λ* 定义了两个损失之间的相对权重。
- en: Experiments
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验
- en: '![](../Images/03e0a4db15e6f5cf7791ceb1e992ad69.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03e0a4db15e6f5cf7791ceb1e992ad69.png)'
- en: 'Fig. 4 from [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven
    Generation](https://arxiv.org/abs/2208.12242).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4 来自 [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven
    Generation](https://arxiv.org/abs/2208.12242)。'
- en: Dataset
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: The dataset used for the experiments was generated by the authors and consists
    of 30 subjects including unique objects such as backpacks or sunglasses and animals
    such as dogs, cats, etc. Of the 30 subjects, 21 of the 30 subjects are objects,
    and 9 are live subjects/pets.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 实验使用的数据集由作者生成，包含 30 个主题，包括独特的物品，如背包或太阳镜，以及动物，如狗、猫等。在这 30 个主题中，21 个是物体，9 个是活体主题/宠物。
- en: 'The authors then define 25 prompts: 20 recontextualization and 5 property modification
    prompts for objects; 10 recontextualization, 10 accessorization and 5 property
    modification prompts for living subjects/pets.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 作者定义了 25 个提示：20 个重新背景化提示和 5 个物体属性修改提示；10 个重新背景化提示，10 个配件化提示和 5 个活体主题/宠物属性修改提示。
- en: Evaluation Metrics
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估指标
- en: For the evaluation, four images are generated per subject and per prompt, for
    a total of 3,000 images.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估，每个主题和每个提示生成四张图像，共计 3,000 张图像。
- en: To measure **subject fidelity**, CLIP-I and DINO are used.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测量 **主题一致性**，使用 CLIP-I 和 DINO。
- en: '**CLIP-I**, already used in previous work, calculates the average pairwise
    cosine similarity between [CLIP](https://openai.com/research/clip) embeddings
    of generated and real images. CLIP is trained to make a text description have
    the same embedding as an image to which it refers, so that if two images represent
    the same text they will have similar embeddings.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**CLIP-I**，在之前的工作中已经使用，计算生成图像和真实图像的[CLIP](https://openai.com/research/clip)嵌入的平均成对余弦相似度。CLIP的训练目标是使文本描述的嵌入与其所指的图像具有相同的嵌入，因此如果两个图像表示相同的文本，它们将具有相似的嵌入。'
- en: '**DINO**, a new metric introduced by the authors, is analogous to CLIP-I but
    instead of generating embeddings with CLIP these are generated with [ViT-S/16
    DINO](https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/),
    a self-supervised trained model.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**DINO**，由作者引入的新指标，类似于 CLIP-I，但生成嵌入的方式是使用[ViT-S/16 DINO](https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/)，一个自监督训练的模型。'
- en: In the paper, is observed that CLIP-I, due to the way CLIP is trained, does
    not distinguish between different subjects that might have very similar textual
    descriptions. On the other hand, DINO (the model, not the metric) is trained in
    a self-supervised manner, which facilitates the distinction of unique features
    within a subject or image. For this reason, they regard DINO as their primary
    metric.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 论文中观察到，由于 CLIP 的训练方式，CLIP-I 不区分可能具有非常相似文本描述的不同主题。另一方面，DINO（指的是模型，而不是指标）以自监督的方式进行训练，这有助于区分主题或图像中的独特特征。因此，他们将
    DINO 视为主要指标。
- en: 'Finally, a third metric, CLIP-T, is introduced to measure, another important
    aspect: **prompt fidelity**, that is, when the generated image is close to the
    input prompt.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，引入了第三个度量指标 CLIP-T，用来衡量另一个重要方面：**提示一致性**，即生成的图像与输入提示的接近程度。
- en: '**CLIP-T** is similar to the previous metrics as well, measuring the average
    cosine similarity between two embeddings obtained with CLIP: one from the prompt
    and the other from the image. It is important to note that CLIP was specifically
    trained to generate textual embeddings that are similar to the ones of the corresponding
    images.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**CLIP-T** 与之前的指标类似，测量从CLIP中获得的两个嵌入之间的平均余弦相似度：一个来自提示，另一个来自图像。值得注意的是，CLIP特别训练以生成与对应图像的文本嵌入相似的嵌入。'
- en: Comparisons
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较
- en: '![](../Images/b78a564495b55a43c1c143aa0fa5f4cd.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b78a564495b55a43c1c143aa0fa5f4cd.png)'
- en: 'Table 1 from [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven
    Generation](https://arxiv.org/abs/2208.12242).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '表1来自于 [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven
    Generation](https://arxiv.org/abs/2208.12242)。'
- en: As we can see from Table 1, DreamBooth is clearly superior to Textual Inversion
    when measured using the DINO and CLIP-T metrics, while the distance is less when
    measured with CLIP-I, which, however, as already mentioned, is not a good metric
    for measuring fidelity to a specific subject.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 从表1可以看出，当使用DINO和CLIP-T指标测量时，DreamBooth明显优于Textual Inversion，而在使用CLIP-I测量时差距较小，但如前所述，CLIP-I并不是一个好的衡量特定主题保真度的指标。
- en: '![](../Images/3b847e303394e05cc38aa89975d9fcbe.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3b847e303394e05cc38aa89975d9fcbe.png)'
- en: 'Table 2 from [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven
    Generation](https://arxiv.org/abs/2208.12242).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '表2来自于 [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven
    Generation](https://arxiv.org/abs/2208.12242)。'
- en: It is very difficult to find a metric that correlates perfectly with what a
    person would judge to be a more or less good outcome. For this reason, the authors
    also measure the preference of a sample of 72 users. The results highlight that
    the percentage of users who prefer DreamBooth for both subject fidelity and prompt
    fidelity is larger than one would be led to believe by looking at the previous
    metrics alone. We can judge for ourselves by looking at the images shown in Fig.
    4 of the paper, where the stark difference between the two methodologies is clear,
    at least for this specific example.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 很难找到一个与个人判断结果好坏完全一致的指标。因此，作者们还测量了一组72名用户的偏好。结果突显出，对于主题保真度和提示保真度，偏好DreamBooth的用户百分比要高于仅凭之前的指标所能得出的结论。我们可以通过查看论文中的图4来判断，两种方法之间的显著差异在这个特定例子中是显而易见的。
- en: Conclusion
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The field of image generation and generative AI has gained significant attention
    in recent times. Advancements in image synthesis, particularly through the use
    of diffusion models, have propelled this area forward.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图像生成和生成式AI领域近年来获得了显著关注。特别是通过扩散模型的使用，图像合成的进展推动了这一领域的发展。
- en: In this article, we delved into the scientific paper of DreamBooth — an impressive
    solution that enables the generation of new images with varying poses and contexts
    while maintaining fidelity to the desired subject. This innovative approach showcases
    the remarkable progress made in the realm of image synthesis and holds great potential
    for future developments.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们深入探讨了DreamBooth的科学论文——这是一种令人印象深刻的解决方案，能够生成具有不同姿势和背景的新图像，同时保持对期望主题的忠实。这种创新的方法展示了图像合成领域取得的显著进展，并对未来的发展具有巨大潜力。
- en: Thank you for taking the time to read this article, and please feel free to
    leave a comment or connect with me to share your thoughts or ask any questions.
    To stay updated on my latest articles, you can follow me on [Medium](https://medium.com/@mnslarcher),
    [LinkedIn](https://www.linkedin.com/in/mnslarcher/) or [Twitter](https://twitter.com/mnslarcher).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢您抽出时间阅读本文，如有任何意见或问题，请随时留言或与我联系。要了解我的最新文章，您可以关注我在 [Medium](https://medium.com/@mnslarcher)、[LinkedIn](https://www.linkedin.com/in/mnslarcher/)
    或 [Twitter](https://twitter.com/mnslarcher) 上的动态。
- en: '[](https://medium.com/@mnslarcher/membership?source=post_page-----70f8bb0cfa30--------------------------------)
    [## Join Medium with my referral link - Mario Namtao Shianti Larcher'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mnslarcher/membership?source=post_page-----70f8bb0cfa30--------------------------------)
    [## 通过我的推荐链接加入Medium - Mario Namtao Shianti Larcher'
- en: Read every story from Mario Namtao Shianti Larcher (and thousands of other writers
    on Medium). Your membership fee…
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阅读Mario Namtao Shianti Larcher的每一个故事（以及Medium上的其他成千上万位作家的故事）。您的会员费……
- en: medium.com](https://medium.com/@mnslarcher/membership?source=post_page-----70f8bb0cfa30--------------------------------)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@mnslarcher/membership?source=post_page-----70f8bb0cfa30--------------------------------)
