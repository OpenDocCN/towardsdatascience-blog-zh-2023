- en: 'Demystifying DreamBooth: A New Tool for Personalizing Text-To-Image Generation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ­ç§˜DreamBoothï¼šä¸€ç§ä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æ–°å·¥å…·
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/demystifying-dreambooth-a-new-tool-for-personalizing-text-to-image-generation-70f8bb0cfa30](https://towardsdatascience.com/demystifying-dreambooth-a-new-tool-for-personalizing-text-to-image-generation-70f8bb0cfa30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/demystifying-dreambooth-a-new-tool-for-personalizing-text-to-image-generation-70f8bb0cfa30](https://towardsdatascience.com/demystifying-dreambooth-a-new-tool-for-personalizing-text-to-image-generation-70f8bb0cfa30)
- en: Exploring the technology that turns boring images into creative masterpieces
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¢ç´¢å°†æ— èŠå›¾åƒè½¬åŒ–ä¸ºåˆ›æ„æ°ä½œçš„æŠ€æœ¯
- en: '[](https://mnslarcher.medium.com/?source=post_page-----70f8bb0cfa30--------------------------------)[![Mario
    Larcher](../Images/b5b443807fe06f096ed4fc5139b3cb42.png)](https://mnslarcher.medium.com/?source=post_page-----70f8bb0cfa30--------------------------------)[](https://towardsdatascience.com/?source=post_page-----70f8bb0cfa30--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----70f8bb0cfa30--------------------------------)
    [Mario Larcher](https://mnslarcher.medium.com/?source=post_page-----70f8bb0cfa30--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mnslarcher.medium.com/?source=post_page-----70f8bb0cfa30--------------------------------)[![Mario
    Larcher](../Images/b5b443807fe06f096ed4fc5139b3cb42.png)](https://mnslarcher.medium.com/?source=post_page-----70f8bb0cfa30--------------------------------)[](https://towardsdatascience.com/?source=post_page-----70f8bb0cfa30--------------------------------)[![æ•°æ®ç§‘å­¦å‰æ²¿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----70f8bb0cfa30--------------------------------)
    [Mario Larcher](https://mnslarcher.medium.com/?source=post_page-----70f8bb0cfa30--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----70f8bb0cfa30--------------------------------)
    Â·13 min readÂ·Jun 13, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[æ•°æ®ç§‘å­¦å‰æ²¿](https://towardsdatascience.com/?source=post_page-----70f8bb0cfa30--------------------------------)
    Â·é˜…è¯»æ—¶é—´13åˆ†é’ŸÂ·2023å¹´6æœˆ13æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/cd417c1c9a70877d284ffb874caaea6a.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cd417c1c9a70877d284ffb874caaea6a.png)'
- en: Dougie and his new personality created with DreamBooth by the author. Can you
    guess the prompt?
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Dougieå’Œä»–çš„æ–°ä¸ªæ€§ç”±ä½œè€…ä½¿ç”¨DreamBoothåˆ›å»ºã€‚ä½ èƒ½çŒœå‡ºæç¤ºæ˜¯ä»€ä¹ˆå—ï¼Ÿ
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: Imagine the joy of effortlessly generating a new image of your beloved puppy
    against the backdrop of the Acropolis in Athens. Not satisfied yet, you would
    like to see how Van Gogh would have painted your best friend or what he would
    look like if he had been conceived by a lion ğŸ˜±! Thanks to DreamBooth, all of this
    is a reality, and today it is possible to make any animal, object or ourselves
    travel in fantasy from a handful of images.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ³è±¡ä¸€ä¸‹ï¼Œä½ è½»æ¾åœ°ç”Ÿæˆä¸€å¼ ä½ å¿ƒçˆ±çš„å¹¼çŠ¬åœ¨é›…å…¸å«åŸèƒŒæ™¯ä¸‹çš„æ–°å›¾åƒçš„å–œæ‚¦ã€‚å¦‚æœè¿˜ä¸æ»¡è¶³ï¼Œä½ è¿˜æƒ³çœ‹çœ‹æ¢µé«˜ä¼šå¦‚ä½•ç»˜åˆ¶ä½ çš„å¥½å‹ï¼Œæˆ–è€…ä»–å¦‚æœè¢«ç‹®å­æ‰€æ„æ€ä¼šæ˜¯ä»€ä¹ˆæ ·å­ğŸ˜±ï¼æ„Ÿè°¢DreamBoothï¼Œè¿™ä¸€åˆ‡éƒ½å˜ä¸ºç°å®ï¼Œå¦‚ä»Šå¯ä»¥è®©ä»»ä½•åŠ¨ç‰©ã€ç‰©ä½“æˆ–æˆ‘ä»¬è‡ªå·±ä»ä¸€å°å †å›¾åƒä¸­æ—…è¡Œäºå¹»æƒ³ä¸–ç•Œã€‚
- en: 'While many of us have already seen on social media the mind-blowing results
    that can be achieved with this technology and there is no shortage of tutorials
    so that we can try it on our own photographs, rarely someone has tried to answer
    the question: yes, but how the hell does it work?'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æˆ‘ä»¬è®¸å¤šäººå·²ç»åœ¨ç¤¾äº¤åª’ä½“ä¸Šçœ‹åˆ°äº†åˆ©ç”¨è¿™é¡¹æŠ€æœ¯å¯ä»¥å–å¾—çš„ä»¤äººç©ç›®çš„æˆæœï¼Œè€Œä¸”æœ‰å¤§é‡æ•™ç¨‹å¯ä»¥è®©æˆ‘ä»¬åœ¨è‡ªå·±çš„ç…§ç‰‡ä¸Šè¿›è¡Œå°è¯•ï¼Œä½†å¾ˆå°‘æœ‰äººå°è¯•å›ç­”è¿™æ ·ä¸€ä¸ªé—®é¢˜ï¼šæ˜¯çš„ï¼Œé‚£ä¹ˆå®ƒåˆ°åº•æ˜¯å¦‚ä½•å·¥ä½œçš„å‘¢ï¼Ÿ
- en: 'In this article, I will do my best tobreak down the scientific paper[DreamBooth:
    Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation](https://arxiv.org/abs/2208.12242)
    by Ruiz et al. from which everything started. But donâ€™t worry, Iâ€™ll simplify the
    complex parts and give explanations where they might require some prior knowledge.
    Now, fair warning, this is an advanced topic, so I assume youâ€™ve got the basics
    of deep learning and related stuff covered. But hey, if you want to dig deeper
    into diffusion models or other cool topics, Iâ€™ll drop some references along the
    way. Letâ€™s dive in!'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†å°½åŠ›è§£æRuizç­‰äººå‘è¡¨çš„ç§‘å­¦è®ºæ–‡[DreamBooth: é’ˆå¯¹ä¸»é¢˜é©±åŠ¨ç”Ÿæˆçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¾®è°ƒ](https://arxiv.org/abs/2208.12242)ï¼Œè¿™ç¯‡è®ºæ–‡æ˜¯æ‰€æœ‰è¿™ä¸€åˆ‡çš„èµ·ç‚¹ã€‚ä½†åˆ«æ‹…å¿ƒï¼Œæˆ‘ä¼šç®€åŒ–å¤æ‚çš„éƒ¨åˆ†ï¼Œå¹¶åœ¨éœ€è¦ä¸€äº›å…ˆéªŒçŸ¥è¯†çš„åœ°æ–¹è¿›è¡Œè§£é‡Šã€‚ç°åœ¨ï¼Œè¯·æ³¨æ„ï¼Œè¿™æ˜¯ä¸€é¡¹é«˜çº§è¯é¢˜ï¼Œå› æ­¤æˆ‘å‡è®¾ä½ å·²ç»æŒæ¡äº†æ·±åº¦å­¦ä¹ åŠç›¸å…³å†…å®¹çš„åŸºç¡€çŸ¥è¯†ã€‚å¦‚æœä½ æƒ³æ·±å…¥äº†è§£æ‰©æ•£æ¨¡å‹æˆ–å…¶ä»–æœ‰è¶£çš„è¯é¢˜ï¼Œæˆ‘ä¼šåœ¨è¿‡ç¨‹ä¸­æä¾›ä¸€äº›å‚è€ƒã€‚è®©æˆ‘ä»¬å¼€å§‹å§ï¼'
- en: Related Work
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç›¸å…³å·¥ä½œ
- en: '![](../Images/5008a1303ab977b9e8dd3ce5fdd17410.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5008a1303ab977b9e8dd3ce5fdd17410.png)'
- en: 'Fig. 7 from [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven
    Generation](https://arxiv.org/abs/2208.12242).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾7æ¥è‡ª[DreamBooth: é’ˆå¯¹ä¸»é¢˜é©±åŠ¨ç”Ÿæˆçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¾®è°ƒ](https://arxiv.org/abs/2208.12242)ã€‚'
- en: Before we get into the nitty-gritty of DreamBoothâ€™s approach, letâ€™s take a closer
    look at the related work and tasks associated with this technique.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬æ·±å…¥æ¢è®¨DreamBoothçš„æ–¹æ³•ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆä»”ç»†äº†è§£ä¸€ä¸‹ä¸è¯¥æŠ€æœ¯ç›¸å…³çš„å·¥ä½œå’Œä»»åŠ¡ã€‚
- en: Image Composition
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å›¾åƒåˆæˆ
- en: Amidst the chaos of everyday life, itâ€™s been far too long since your beloved
    backpack embarked on a globetrotting journey. It is time to infuse it with an
    exciting dose of adventure while you are planning your next vacation. Enter image
    composition, merge your subject seamlessly into new backgrounds, letting your
    backpack travel from the Grand Canyon to Boston in seconds.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ—¥å¸¸ç”Ÿæ´»çš„å–§åš£ä¸­ï¼Œä½ å¿ƒçˆ±çš„èƒŒåŒ…å·²ç»å¾ˆä¹…æ²¡æœ‰è¸ä¸Šç¯çƒä¹‹æ—…ã€‚ç°åœ¨æ˜¯ç»™å®ƒæ³¨å…¥åˆºæ¿€å†’é™©çš„æ—¶åˆ»ï¼ŒåŒæ—¶ä½ ä¹Ÿåœ¨è§„åˆ’ä¸‹ä¸€æ¬¡å‡æœŸã€‚é€šè¿‡å›¾åƒåˆæˆï¼Œå°†ä½ çš„èƒŒåŒ…æ— ç¼èå…¥æ–°çš„èƒŒæ™¯ï¼Œè®©å®ƒåœ¨å‡ ç§’é’Ÿå†…ä»å¤§å³¡è°·åˆ°æ³¢å£«é¡¿ã€‚
- en: If simply copy-pasting the subject doesnâ€™t fulfill your desire for new perspectives,
    one possibility is to explore the application of 3D reconstruction techniques.
    However, itâ€™s important to note that these techniques are primarily designed for
    rigid objects and often require a substantial number of starting views.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœç®€å•åœ°å¤åˆ¶ç²˜è´´ä¸»é¢˜ä¸èƒ½æ»¡è¶³ä½ å¯¹æ–°è§†è§’çš„æ¸´æœ›ï¼Œå¯ä»¥å°è¯•æ¢ç´¢3Dé‡å»ºæŠ€æœ¯çš„åº”ç”¨ã€‚ç„¶è€Œï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›æŠ€æœ¯ä¸»è¦é’ˆå¯¹åˆšæ€§ç‰©ä½“ï¼Œå¹¶ä¸”é€šå¸¸éœ€è¦å¤§é‡çš„èµ·å§‹è§†å›¾ã€‚
- en: DreamBooth introduces a remarkable capability to generate fresh poses within
    new contexts while smoothly incorporating crucial elements such as lighting, shadows,
    and other scene-relative aspects. Achieving such consistency has proven challenging
    with prior methodologies. In the paper, this task is also denoted by the name
    recontextualization.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: DreamBoothå¼•å…¥äº†ä¸€é¡¹å“è¶Šçš„èƒ½åŠ›ï¼Œå¯ä»¥åœ¨æ–°çš„èƒŒæ™¯ä¸­ç”Ÿæˆæ–°å§¿åŠ¿ï¼ŒåŒæ—¶é¡ºç•…åœ°èå…¥å…³é”®å…ƒç´ ï¼Œå¦‚å…‰çº¿ã€é˜´å½±å’Œå…¶ä»–ä¸åœºæ™¯ç›¸å…³çš„æ–¹é¢ã€‚å®ç°è¿™ç§ä¸€è‡´æ€§åœ¨ä»¥å¾€çš„æ–¹æ³•ä¸­ä¸€ç›´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚åœ¨è®ºæ–‡ä¸­ï¼Œè¿™é¡¹ä»»åŠ¡ä¹Ÿè¢«ç§°ä¸ºé‡æ–°èƒŒæ™¯åŒ–ã€‚
- en: Text-to-Image Editing and Synthesis
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ–‡æœ¬åˆ°å›¾åƒç¼–è¾‘ä¸åˆæˆ
- en: Image editing based on textual input is a secret dream cherished by many avid
    users of photo editing software. Early methodologies, such as those employing
    GANs, demonstrated impressive results, but only in well-structured scenarios like
    editing human faces.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæ–‡æœ¬è¾“å…¥çš„å›¾åƒç¼–è¾‘æ˜¯è®¸å¤šç…§ç‰‡ç¼–è¾‘è½¯ä»¶çˆ±å¥½è€…çš„ä¸€ä¸ªç§˜å¯†æ¢¦æƒ³ã€‚æ—©æœŸçš„æ–¹æ³•ï¼Œä¾‹å¦‚ä½¿ç”¨GANsçš„æ–¹æ³•ï¼Œå±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œä½†ä»…é™äºåƒç¼–è¾‘äººè„¸è¿™æ ·ç»“æ„è‰¯å¥½çš„åœºæ™¯ã€‚
- en: Even new approaches that take advantage of diffusion models have limitations
    and are usually restricted to global editing. Only recently have advances such
    as [Text2LIVE](https://text2live.github.io/) emerged that allow localized editing.
    However, none of these techniques allow the generation of a given subject in new
    contexts.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿æ˜¯åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•ä¹Ÿæœ‰å…¶å±€é™æ€§ï¼Œé€šå¸¸ä»…é™äºå…¨å±€ç¼–è¾‘ã€‚ç›´åˆ°æœ€è¿‘ï¼Œåƒ[Text2LIVE](https://text2live.github.io/)è¿™æ ·çš„è¿›å±•æ‰å…è®¸å±€éƒ¨ç¼–è¾‘ã€‚ç„¶è€Œï¼Œè¿™äº›æŠ€æœ¯éƒ½æ— æ³•åœ¨æ–°çš„èƒŒæ™¯ä¸­ç”Ÿæˆç‰¹å®šçš„ä¸»é¢˜ã€‚
- en: Although text-image synthesis models like [Imagen](https://imagen.research.google/),
    [DALLÂ·E 2](https://openai.com/product/dall-e-2), and [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release)
    have made significant strides, the attainment of fine-grained control and the
    preservation of subject identity in synthesized images continue to pose substantial
    challenges.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡åƒ[Imagen](https://imagen.research.google/)ã€[DALLÂ·E 2](https://openai.com/product/dall-e-2)å’Œ[Stable
    Diffusion](https://stability.ai/blog/stable-diffusion-public-release)è¿™æ ·çš„æ–‡æœ¬å›¾åƒåˆæˆæ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨åˆæˆå›¾åƒä¸­å®ç°ç²¾ç»†æ§åˆ¶å¹¶ä¿ç•™ä¸»é¢˜èº«ä»½ä»ç„¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚
- en: Controllable Generative Models
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯æ§ç”Ÿæˆæ¨¡å‹
- en: To avoid subject modification, many approaches rely on a user-provided mask
    that limits the area to be modified. Inversion techniques, such as the one used
    by [DALLÂ·E 2](https://openai.com/product/dall-e-2), present an effective solution
    for preserving the subject while modifying the context.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†é¿å…å¯¹ä¸»é¢˜è¿›è¡Œä¿®æ”¹ï¼Œè®¸å¤šæ–¹æ³•ä¾èµ–äºç”¨æˆ·æä¾›çš„æ©ç æ¥é™åˆ¶ä¿®æ”¹çš„åŒºåŸŸã€‚é€†è½¬æŠ€æœ¯ï¼Œå¦‚[DALLÂ·E 2](https://openai.com/product/dall-e-2)ä½¿ç”¨çš„æŠ€æœ¯ï¼Œæä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥åœ¨ä¿®æ”¹èƒŒæ™¯çš„åŒæ—¶ä¿ç•™ä¸»é¢˜ã€‚
- en: '[Prompt-to-Prompt](https://prompt-to-prompt.github.io/) enables both local
    and global editing without the need for an input mask.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[Prompt-to-Prompt](https://prompt-to-prompt.github.io/)ä½¿å¾—æœ¬åœ°å’Œå…¨å±€ç¼–è¾‘æˆä¸ºå¯èƒ½ï¼Œæ— éœ€è¾“å…¥æ©ç ã€‚'
- en: However, these methods do not adequately preserve the identity while generating
    novel samples of a subject.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨ç”Ÿæˆæ–°æ ·æœ¬æ—¶æ— æ³•å……åˆ†ä¿ç•™ä¸»é¢˜çš„èº«ä»½ã€‚
- en: While some GAN-based methods focus on generating instance variations, they often
    have limitations. For instance, they are primarily designed for the face domain,
    require many instances of the input subject, struggle with unique subjects, and
    fail to preserve important subject details.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ä¸€äº›åŸºäºGANçš„æ–¹æ³•ä¸“æ³¨äºç”Ÿæˆå®ä¾‹å˜ä½“ï¼Œä½†å®ƒä»¬å¾€å¾€æœ‰å±€é™æ€§ã€‚ä¾‹å¦‚ï¼Œå®ƒä»¬ä¸»è¦è®¾è®¡ç”¨äºé¢éƒ¨é¢†åŸŸï¼Œéœ€è¦å¤§é‡çš„è¾“å…¥æ ·æœ¬ï¼Œéš¾ä»¥å¤„ç†ç‹¬ç‰¹çš„ä¸»é¢˜ï¼Œå¹¶ä¸”æ— æ³•ä¿ç•™é‡è¦çš„ä¸»é¢˜ç»†èŠ‚ã€‚
- en: Finally, recently Gal et. al. presented Textual Inversion, a methodology with
    features common to DreamBooth but which, as we will see, is limited by the expressiveness
    of the frozen diffusion model on which it is based.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæœ€è¿‘Galç­‰äººæå‡ºäº†æ–‡æœ¬åæ¼”ï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰DreamBoothå…±åŒç‰¹å¾çš„æ–¹æ³•è®ºï¼Œä½†æ­£å¦‚æˆ‘ä»¬å°†çœ‹åˆ°çš„ï¼Œå®ƒå—åˆ°åŸºäºå…¶çš„å†»ç»“æ‰©æ•£æ¨¡å‹è¡¨ç°åŠ›çš„é™åˆ¶ã€‚
- en: '![](../Images/d3075f2bc0ac3f085e9b3dd435e0f272.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d3075f2bc0ac3f085e9b3dd435e0f272.png)'
- en: 'Fig. 2 from [An Image is Worth One Word: Personalizing Text-to-Image Generation
    using Textual Inversion](https://arxiv.org/abs/2208.01618).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2æ¥è‡ª[å›¾åƒèƒœäºåƒè¨€ï¼šä½¿ç”¨æ–‡æœ¬åæ¼”ä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ](https://arxiv.org/abs/2208.01618)ã€‚
- en: Since this is the work with which the authors compare DreamBooth, it is worth
    providing a brief description of it.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºè¿™æ˜¯ä½œè€…ç”¨æ¥ä¸DreamBoothè¿›è¡Œæ¯”è¾ƒçš„å·¥ä½œï¼Œå€¼å¾—æä¾›ä¸€ä¸ªç®€è¦çš„æè¿°ã€‚
- en: '**Textual Inversion** starts from a pre-trained diffusion model, such as Latent
    Diffusion, and defines a new placeholder string S*, to represent the new concept
    to be learned. At this point, keeping the diffusion model frozen, the new embedding
    is fine-tuned from just 3â€“5 images, similar to DreamBooth. If this brief description
    is not clear enough, wait until you read the more detailed description of DreamBooth,
    which has many points in common with this work.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ–‡æœ¬åæ¼”**ä»ä¸€ä¸ªé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å¼€å§‹ï¼Œå¦‚æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œå¹¶å®šä¹‰ä¸€ä¸ªæ–°çš„å ä½ç¬¦å­—ç¬¦ä¸²S*ï¼Œä»¥è¡¨ç¤ºéœ€è¦å­¦ä¹ çš„æ–°æ¦‚å¿µã€‚åœ¨æ­¤é˜¶æ®µï¼Œä¿æŒæ‰©æ•£æ¨¡å‹å†»ç»“ï¼Œæ–°çš„åµŒå…¥ä»ä»…3-5å¼ å›¾åƒä¸­è¿›è¡Œå¾®è°ƒï¼Œç±»ä¼¼äºDreamBoothã€‚å¦‚æœè¿™ä¸ªç®€è¦æè¿°ä¸å¤Ÿæ¸…æ¥šï¼Œè¯·ç­‰åˆ°ä½ é˜…è¯»æ›´è¯¦ç»†çš„DreamBoothæè¿°æ—¶ï¼Œå®ƒä¸è¿™é¡¹å·¥ä½œæœ‰è®¸å¤šå…±åŒç‚¹ã€‚'
- en: Method
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ–¹æ³•
- en: '![](../Images/98f3ebfcca65a8f1de98a0d286677bf1.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/98f3ebfcca65a8f1de98a0d286677bf1.png)'
- en: 'Fig. 3 from [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven
    Generation](https://arxiv.org/abs/2208.12242).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾3æ¥è‡ª[DreamBoothï¼šä¸ºä¸»é¢˜é©±åŠ¨ç”Ÿæˆå¾®è°ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹](https://arxiv.org/abs/2208.12242)ã€‚
- en: 'Before describing the components of **DreamBooth** in detail, letâ€™s see **schematically**
    how this technology works:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¯¦ç»†æè¿°**DreamBooth**çš„ç»„ä»¶ä¹‹å‰ï¼Œè®©æˆ‘ä»¬**ç®€è¦**äº†è§£ä¸€ä¸‹è¿™é¡¹æŠ€æœ¯çš„å·¥ä½œåŸç†ï¼š
- en: Choose 3â€“5 images of your favorite subject, it can be an animal, an object or
    even an abstract concept such as an art style.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€‰æ‹©3-5å¼ ä½ å–œæ¬¢çš„ä¸»é¢˜å›¾åƒï¼Œå¯ä»¥æ˜¯åŠ¨ç‰©ã€ç‰©ä½“ï¼Œç”šè‡³æ˜¯åƒè‰ºæœ¯é£æ ¼è¿™æ ·çš„æŠ½è±¡æ¦‚å¿µã€‚
- en: Associate this concept with a rare word to which corresponds a unique token
    that will represent it from now on, in the scientific paper the authors call this
    word [V].
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†è¿™ä¸ªæ¦‚å¿µä¸ä¸€ä¸ªç¨€æœ‰è¯æ±‡å…³è”ï¼Œè¯¥è¯æ±‡å¯¹åº”ä¸€ä¸ªå”¯ä¸€çš„æ ‡è®°ï¼Œå°†ä»ç°åœ¨å¼€å§‹è¡¨ç¤ºå®ƒï¼Œåœ¨ç§‘å­¦è®ºæ–‡ä¸­ï¼Œä½œè€…ç§°è¿™ä¸ªè¯ä¸º[V]ã€‚
- en: Fine-tune the model using images of the subject of interest with a simple prompt
    such as â€œA [V] [class noun]â€, for example â€œA [V] dogâ€ if the input images are
    photographs of your dog.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å…´è¶£ä¸»é¢˜çš„å›¾åƒï¼Œé€šè¿‡ç®€å•çš„æç¤ºå¦‚â€œä¸€ä¸ª[V] [ç±»åˆ«å]â€æ¥å¾®è°ƒæ¨¡å‹ï¼Œä¾‹å¦‚ï¼Œå¦‚æœè¾“å…¥å›¾åƒæ˜¯ä½ çš„ç‹—çš„ç…§ç‰‡ï¼Œåˆ™ä¸ºâ€œä¸€ä¸ª[V] ç‹—â€ã€‚
- en: Since we are fine-tuning all the parameters of the model, there is a risk that
    at this point all dogs (or whatever class our subject is) will become the same
    as our input images. To avoid this degradation of our model, we generate images
    from our frozen model with a prompt such as â€œA dogâ€ (or â€œA [class noun]â€) and
    add a loss that penalizes when the images generated by our model that we are fine-tuning
    for this prompt deviate from those generated by the frozen model.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç”±äºæˆ‘ä»¬æ­£åœ¨å¾®è°ƒæ¨¡å‹çš„æ‰€æœ‰å‚æ•°ï¼Œå› æ­¤æœ‰é£é™©åœ¨è¿™ä¸ªé˜¶æ®µæ‰€æœ‰çš„ç‹—ï¼ˆæˆ–æˆ‘ä»¬ä¸»é¢˜çš„ä»»ä½•ç±»åˆ«ï¼‰éƒ½ä¼šå˜æˆä¸æˆ‘ä»¬çš„è¾“å…¥å›¾åƒç›¸åŒã€‚ä¸ºäº†é¿å…æ¨¡å‹çš„è¿™ç§é€€åŒ–ï¼Œæˆ‘ä»¬ä»å†»ç»“çš„æ¨¡å‹ç”Ÿæˆå›¾åƒï¼Œä½¿ç”¨åƒâ€œç‹—â€ï¼ˆæˆ–â€œ[ç±»åˆ«å]â€ï¼‰è¿™æ ·çš„æç¤ºï¼Œå¹¶æ·»åŠ ä¸€ä¸ªæŸå¤±å‡½æ•°ï¼Œå½“æˆ‘ä»¬ä¸ºè¿™ä¸ªæç¤ºå¾®è°ƒçš„æ¨¡å‹ç”Ÿæˆçš„å›¾åƒåç¦»å†»ç»“æ¨¡å‹ç”Ÿæˆçš„å›¾åƒæ—¶ï¼Œä¼šå—åˆ°æƒ©ç½šã€‚
- en: Okay, now that we have a high-level idea of the procedure, letâ€™s go into more
    detail about the various components.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œç°åœ¨æˆ‘ä»¬å¯¹è¿‡ç¨‹æœ‰äº†ä¸€ä¸ªé«˜å±‚æ¬¡çš„äº†è§£ï¼Œè®©æˆ‘ä»¬è¯¦ç»†è®¨è®ºå„ç§ç»„ä»¶ã€‚
- en: Text to Image Diffusion Models
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹
- en: Do you really want to learn how diffusion models work and, in particular, latent
    diffusion models such as Stable Diffusion? Read my previous article below, I will
    be waiting for you here when you are done!
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ çœŸçš„æƒ³äº†è§£æ‰©æ•£æ¨¡å‹çš„å·¥ä½œåŸç†ï¼Œå°¤å…¶æ˜¯åƒç¨³å®šæ‰©æ•£è¿™æ ·çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹å—ï¼Ÿè¯·é˜…è¯»æˆ‘ä¹‹å‰çš„æ–‡ç« ï¼Œå½“ä½ è¯»å®Œåï¼Œæˆ‘ä¼šåœ¨è¿™é‡Œç­‰ä½ ï¼
- en: '[](/paper-explained-high-resolution-image-synthesis-with-latent-diffusion-models-f372f7636d42?source=post_page-----70f8bb0cfa30--------------------------------)
    [## Paper Explained â€” High-Resolution Image Synthesis with Latent Diffusion Models'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[## è®ºæ–‡è§£è¯»â€”â€”åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆ](/paper-explained-high-resolution-image-synthesis-with-latent-diffusion-models-f372f7636d42?source=post_page-----70f8bb0cfa30--------------------------------)'
- en: While OpenAI has dominated the field of natural language processing with their
    generative text models, their imageâ€¦
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è™½ç„¶OpenAIå‡­å€Ÿå…¶ç”Ÿæˆæ–‡æœ¬æ¨¡å‹ä¸»å¯¼äº†è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œä½†ä»–ä»¬çš„å›¾åƒâ€¦
- en: towardsdatascience.com](/paper-explained-high-resolution-image-synthesis-with-latent-diffusion-models-f372f7636d42?source=post_page-----70f8bb0cfa30--------------------------------)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/paper-explained-high-resolution-image-synthesis-with-latent-diffusion-models-f372f7636d42?source=post_page-----70f8bb0cfa30--------------------------------)'
- en: OK, maybe you donâ€™t want a whole explanation, in which case I will give here
    the intuition behind **diffusion models**, which is very simple.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼Œä¹Ÿè®¸ä½ ä¸éœ€è¦å®Œæ•´çš„è§£é‡Šï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘å°†æä¾›**æ‰©æ•£æ¨¡å‹**èƒŒåçš„ç›´è§‚ç†è§£ï¼Œè¿™éå¸¸ç®€å•ã€‚
- en: '![](../Images/13faf499b66a3c034aa4a875c8587d68.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13faf499b66a3c034aa4a875c8587d68.png)'
- en: Fig. 2\. from [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 2\. æ¥è‡ª [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)ã€‚
- en: Take an image x0 and add a certain amount of noise (e.g. Gaussian noise) proportional
    to a certain timestep *t*. If *t* is zero the added noise will be zero, if *t*
    > 0 the added noise will be as large as *t* is, until you arrive at an image that
    is just noise.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å–ä¸€ä¸ªå›¾åƒx0ï¼Œå¹¶æ·»åŠ ä¸€å®šé‡çš„å™ªå£°ï¼ˆä¾‹å¦‚ï¼Œé«˜æ–¯å™ªå£°ï¼‰ï¼Œå™ªå£°é‡ä¸æŸä¸ªæ—¶é—´æ­¥* t *æˆæ¯”ä¾‹ã€‚å¦‚æœ*t*ä¸ºé›¶ï¼Œåˆ™æ·»åŠ çš„å™ªå£°ä¸ºé›¶ï¼Œå¦‚æœ*t* > 0ï¼Œåˆ™æ·»åŠ çš„å™ªå£°å°†ä¸*t*çš„å¤§å°ä¸€æ ·ï¼Œç›´åˆ°ä½ å¾—åˆ°ä¸€ä¸ªä»…ç”±å™ªå£°ç»„æˆçš„å›¾åƒã€‚
- en: Train a model, such as a U-Net, to predict the noise-free image (or the noise
    that has been added) by giving as input to the model the timestep *t* and the
    corrupted image.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œå¦‚U-Netï¼Œé€šè¿‡å°†æ—¶é—´æ­¥* t *å’Œå—æŸå›¾åƒä½œä¸ºè¾“å…¥æ¥é¢„æµ‹æ— å™ªå£°å›¾åƒï¼ˆæˆ–æ·»åŠ çš„å™ªå£°ï¼‰ã€‚
- en: At this point, having trained a model that can remove noise from an image, we
    can sample an image composed only of noise and gradually remove it (doing it all
    at once works poorly) either by predicting the image without noise or by predicting
    the noise and subtracting it from the image.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ­¤æ—¶ï¼Œç»è¿‡è®­ç»ƒä¸€ä¸ªå¯ä»¥å»é™¤å›¾åƒå™ªå£°çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡æ ·ä¸€ä¸ªä»…ç”±å™ªå£°ç»„æˆçš„å›¾åƒï¼Œå¹¶é€æ¸å»é™¤å™ªå£°ï¼ˆä¸€æ¬¡æ€§å®Œæˆæ•ˆæœä¸å¥½ï¼‰ï¼Œå¯ä»¥é€šè¿‡é¢„æµ‹æ— å™ªå£°çš„å›¾åƒæˆ–é¢„æµ‹å™ªå£°å¹¶ä»å›¾åƒä¸­å‡å»æ¥å®ç°ã€‚
- en: The first three points describe an unconditional diffusion model. In order to
    produce a conditional output based on textual prompt, the text is encoded using
    models like [CLIP](https://openai.com/research/clip), or language models such
    as [BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html),
    [T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html),
    and others. This encoding step allows for the integration of additional information,
    which is then fed as input to the model alongside the corrupted image and the
    timestep *t*.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å‰ä¸‰ç‚¹æè¿°äº†æ— æ¡ä»¶æ‰©æ•£æ¨¡å‹ã€‚ä¸ºäº†æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆæ¡ä»¶è¾“å‡ºï¼Œæ–‡æœ¬ä½¿ç”¨åƒ [CLIP](https://openai.com/research/clip)
    çš„æ¨¡å‹è¿›è¡Œç¼–ç ï¼Œæˆ–è€…ä½¿ç”¨å¦‚ [BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)ã€[T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)
    ç­‰è¯­è¨€æ¨¡å‹ã€‚è¿™ä¸ªç¼–ç æ­¥éª¤å…è®¸é›†æˆé¢å¤–çš„ä¿¡æ¯ï¼Œç„¶åå°†å…¶ä¸å—æŸå›¾åƒå’Œæ—¶é—´æ­¥* t *ä¸€èµ·è¾“å…¥æ¨¡å‹ã€‚
- en: 'The authors in the paper use two diffusion models: Googleâ€™s [Imagen](https://imagen.research.google/)
    (also DreamBooth is from [Google Research](https://research.google/)) and [Stable
    Diffusion](https://stability.ai/blog/stable-diffusion-public-release)from [Stability
    AI](https://stability.ai/), the main open-source text-to-image model.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡ä¸­çš„ä½œè€…ä½¿ç”¨äº†ä¸¤ä¸ªæ‰©æ•£æ¨¡å‹ï¼šGoogleçš„ [Imagen](https://imagen.research.google/)ï¼ˆDreamBooth
    ä¹Ÿæ¥è‡ª [Google Research](https://research.google/)ï¼‰å’Œ [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release)æ¥è‡ª
    [Stability AI](https://stability.ai/)ï¼Œè¿™æ˜¯ä¸»è¦çš„å¼€æºæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ã€‚
- en: '**Imagen** employs a multi-resolution strategy to enhance the quality of generated
    images. Initially, a diffusion model is trained using low-resolution 64x64 images.
    The output of the low-resolution model is then upscaled by two additional diffusion
    models that operate at higher resolutions, 256x256 and 1024x1024\. The first model
    specializes in capturing macro-details, while the subsequent models refine the
    output by leveraging the conditioning effect of the lower resolution modelâ€™s generated
    image. This iterative refinement facilitates the generation of high-resolution
    images with improved quality and fidelity.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**Imagen** é‡‡ç”¨å¤šåˆ†è¾¨ç‡ç­–ç•¥æ¥æé«˜ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚æœ€åˆï¼Œä½¿ç”¨ä½åˆ†è¾¨ç‡ 64x64 å›¾åƒè®­ç»ƒæ‰©æ•£æ¨¡å‹ã€‚ç„¶åï¼Œä½åˆ†è¾¨ç‡æ¨¡å‹çš„è¾“å‡ºé€šè¿‡ä¸¤ä¸ªé¢å¤–çš„æ‰©æ•£æ¨¡å‹è¿›è¡Œæ”¾å¤§ï¼Œè¿™äº›æ¨¡å‹åœ¨æ›´é«˜åˆ†è¾¨ç‡ä¸‹æ“ä½œï¼Œåˆ†åˆ«ä¸º
    256x256 å’Œ 1024x1024ã€‚ç¬¬ä¸€ä¸ªæ¨¡å‹ä¸“æ³¨äºæ•æ‰å®è§‚ç»†èŠ‚ï¼Œè€Œéšåçš„æ¨¡å‹åˆ™é€šè¿‡åˆ©ç”¨è¾ƒä½åˆ†è¾¨ç‡æ¨¡å‹ç”Ÿæˆå›¾åƒçš„æ¡ä»¶æ•ˆåº”æ¥ç²¾ç»†åŒ–è¾“å‡ºã€‚è¿™ç§è¿­ä»£ä¼˜åŒ–æœ‰åŠ©äºç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„å›¾åƒï¼Œå…·æœ‰æ›´å¥½çš„è´¨é‡å’Œä¿çœŸåº¦ã€‚'
- en: '**Stable Diffusion** instead, as a latent diffusion model, introduces a three-step
    approach to enhance the efficiency of training and generating high-resolution
    images. Initially, a Variational Autoencoder (VAE) is trained to compress a high-resolution
    image. From this point onward, the process closely resembles that of standard
    diffusion models, with one key distinction: instead of employing the original
    image as input, the latent representation generated by the VAE encoder is utilized.
    Subsequently, the output of the inverse diffusion process is then restored to
    the original resolution using the VAE decoder. For a more comprehensive understanding
    of this entire procedure, I delve into greater detail in the aforementioned article.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**Stable Diffusion** ä½œä¸ºä¸€ç§æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œé‡‡ç”¨ä¸‰æ­¥æ³•æ¥æé«˜è®­ç»ƒå’Œç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒçš„æ•ˆç‡ã€‚æœ€åˆï¼Œè®­ç»ƒä¸€ä¸ªå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ä»¥å‹ç¼©é«˜åˆ†è¾¨ç‡å›¾åƒã€‚ä»æ­¤ä¹‹åï¼Œè¿‡ç¨‹ä¸æ ‡å‡†æ‰©æ•£æ¨¡å‹éå¸¸ç›¸ä¼¼ï¼Œä¸€ä¸ªå…³é”®åŒºåˆ«åœ¨äºï¼šä¸æ˜¯ä½¿ç”¨åŸå§‹å›¾åƒä½œä¸ºè¾“å…¥ï¼Œè€Œæ˜¯ä½¿ç”¨ç”±
    VAE ç¼–ç å™¨ç”Ÿæˆçš„æ½œåœ¨è¡¨ç¤ºã€‚éšåï¼Œé€†æ‰©æ•£è¿‡ç¨‹çš„è¾“å‡ºé€šè¿‡ VAE è§£ç å™¨æ¢å¤åˆ°åŸå§‹åˆ†è¾¨ç‡ã€‚ä¸ºäº†æ›´å…¨é¢åœ°ç†è§£æ•´ä¸ªè¿‡ç¨‹ï¼Œæˆ‘åœ¨ä¸Šè¿°æ–‡ç« ä¸­è¿›è¡Œäº†æ›´è¯¦ç»†çš„æ¢è®¨ã€‚'
- en: Personalization of Text to Image Models
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„ä¸ªæ€§åŒ–
- en: DreamBooth aims to place the subject instance (e.g. your dog) within the output
    domain of the model, enabling the model to generate fresh images of the subject
    upon query. An advantage of diffusion models, as opposed to GANs, is their ability
    to effectively incorporate new information into their domain while retaining knowledge
    of previous data and avoiding overfitting to a limited training image set.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: DreamBooth æ—¨åœ¨å°†ä¸»é¢˜å®ä¾‹ï¼ˆä¾‹å¦‚ä½ çš„ç‹—ï¼‰ç½®äºæ¨¡å‹çš„è¾“å‡ºé¢†åŸŸå†…ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æŸ¥è¯¢æ—¶ç”Ÿæˆä¸»é¢˜çš„æ–°å›¾åƒã€‚æ‰©æ•£æ¨¡å‹çš„ä¸€ä¸ªä¼˜åŠ¿æ˜¯ï¼Œä¸ GANs ç›¸æ¯”ï¼Œå®ƒä»¬èƒ½å¤Ÿæœ‰æ•ˆåœ°å°†æ–°ä¿¡æ¯çº³å…¥å…¶é¢†åŸŸï¼ŒåŒæ—¶ä¿ç•™å¯¹å…ˆå‰æ•°æ®çš„çŸ¥è¯†ï¼Œå¹¶é¿å…å¯¹æœ‰é™çš„è®­ç»ƒå›¾åƒé›†çš„è¿‡æ‹Ÿåˆã€‚
- en: Designing Prompts for Few-Shot Personalization
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸ºå°‘æ ·æœ¬ä¸ªæ€§åŒ–è®¾è®¡æç¤º
- en: As mentioned above, the model undergoes training using simplistic prompts structured
    as â€œa [identifier] [class noun]â€. Here, [identifier] represents a distinct identifier
    associated with the subject, and [class noun] serves as a general description
    of the subjectâ€™s category (such as cat, dog, watch, etc.). The authors incorporate
    the class noun into the prompt to establish a connection between the general class
    and our individual subject, observing that using an incorrect or missing class
    noun leads to longer training times and language drift, ultimately affecting performance.
    Essentially, the main aim is to capitalize on the relationship between the specific
    class and our subject, utilizing the existing knowledge acquired by the model
    about the class. This enables us to generate fresh poses and variations of the
    subject across various contexts.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼Œè¯¥æ¨¡å‹é€šè¿‡ä½¿ç”¨â€œä¸€ä¸ª [identifier] [class noun]â€ç»“æ„çš„ç®€å•æç¤ºè¿›è¡Œè®­ç»ƒã€‚è¿™é‡Œï¼Œ[identifier] ä»£è¡¨ä¸ä¸»é¢˜ç›¸å…³çš„ç‹¬ç‰¹æ ‡è¯†ç¬¦ï¼Œè€Œ
    [class noun] ä½œä¸ºä¸»é¢˜ç±»åˆ«çš„ä¸€èˆ¬æè¿°ï¼ˆå¦‚çŒ«ã€ç‹—ã€æ‰‹è¡¨ç­‰ï¼‰ã€‚ä½œè€…å°†ç±»åçº³å…¥æç¤ºä¸­ï¼Œä»¥å»ºç«‹é€šç”¨ç±»åˆ«ä¸æˆ‘ä»¬ä¸ªä½“ä¸»é¢˜ä¹‹é—´çš„è”ç³»ï¼Œè§‚å¯Ÿåˆ°ä½¿ç”¨ä¸æ­£ç¡®æˆ–ç¼ºå¤±çš„ç±»åä¼šå¯¼è‡´æ›´é•¿çš„è®­ç»ƒæ—¶é—´å’Œè¯­è¨€æ¼‚ç§»ï¼Œ*æœ€ç»ˆå½±å“æ€§èƒ½*ã€‚æœ¬è´¨ä¸Šï¼Œä¸»è¦ç›®çš„æ˜¯åˆ©ç”¨ç‰¹å®šç±»åˆ«ä¸æˆ‘ä»¬çš„ä¸»é¢˜ä¹‹é—´çš„å…³ç³»ï¼Œåˆ©ç”¨æ¨¡å‹å¯¹è¯¥ç±»åˆ«å·²æœ‰çš„çŸ¥è¯†ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨å„ç§ä¸Šä¸‹æ–‡ä¸­ç”Ÿæˆæ–°é¢–çš„å§¿åŠ¿å’Œå˜ä½“ã€‚
- en: Rare-Token Identifiers
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¨€æœ‰æ ‡è®°æ ‡è¯†ç¬¦
- en: The paper highlights that common English words are not ideal in this context
    since the model needs to disassociate them from their original meaning and reintegrate
    them to refer to our subject.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡å¼ºè°ƒï¼Œæ™®é€šçš„è‹±è¯­å•è¯åœ¨è¿™ç§æƒ…å†µä¸‹å¹¶ä¸ç†æƒ³ï¼Œå› ä¸ºæ¨¡å‹éœ€è¦å°†å®ƒä»¬ä¸åŸå§‹å«ä¹‰è„±ç¦»ï¼Œå¹¶é‡æ–°æ•´åˆä»¥æŒ‡ä»£æˆ‘ä»¬çš„ä¸»é¢˜ã€‚
- en: To address this, the authors propose using an identifier that has a weak prior
    in both the language and diffusion models. While selecting random characters like
    â€œxxy5syt00â€ may initially appear appealing, it poses potential risks. It is important
    to consider that the tokenizer could tokenize each letter individually. So, what
    is the solution? The most effective approach involves identifying uncommon tokens
    in the vocabulary and then inverting these tokens within the text space. This
    minimizes the likelihood of the identifier having a strong prior.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºä½¿ç”¨åœ¨è¯­è¨€å’Œæ‰©æ•£æ¨¡å‹ä¸­éƒ½æœ‰å¼±å…ˆéªŒçš„æ ‡è¯†ç¬¦ã€‚è™½ç„¶é€‰æ‹©åƒâ€œxxy5syt00â€è¿™æ ·çš„éšæœºå­—ç¬¦å¯èƒ½æœ€åˆçœ‹èµ·æ¥å¾ˆå¸å¼•äººï¼Œä½†è¿™ä¹Ÿå­˜åœ¨æ½œåœ¨é£é™©ã€‚éœ€è¦è€ƒè™‘çš„æ˜¯ï¼Œåˆ†è¯å™¨å¯èƒ½ä¼šå°†æ¯ä¸ªå­—æ¯å•ç‹¬åˆ†è¯ã€‚é‚£ä¹ˆè§£å†³æ–¹æ¡ˆæ˜¯ä»€ä¹ˆï¼Ÿæœ€æœ‰æ•ˆçš„æ–¹æ³•æ˜¯è¯†åˆ«è¯æ±‡è¡¨ä¸­ä¸å¸¸è§çš„æ ‡è®°ï¼Œç„¶ååœ¨æ–‡æœ¬ç©ºé—´ä¸­åè½¬è¿™äº›æ ‡è®°ã€‚è¿™å¯ä»¥æœ€å°åŒ–æ ‡è¯†ç¬¦å…·æœ‰å¼ºå…ˆéªŒçš„å¯èƒ½æ€§ã€‚
- en: Funny enough, most tutorials use â€œsksâ€ for this purpose but, as pointed out
    by one of the authors, this seemingly harmless word can have side effectsâ€¦
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰è¶£çš„æ˜¯ï¼Œå¤§å¤šæ•°æ•™ç¨‹ä½¿ç”¨â€œsksâ€æ¥å®ç°è¿™ä¸€ç›®çš„ï¼Œä½†æ­£å¦‚å…¶ä¸­ä¸€ä½ä½œè€…æŒ‡å‡ºçš„ï¼Œè¿™ä¸ªçœ‹ä¼¼æ— å®³çš„è¯å¯èƒ½ä¼šäº§ç”Ÿå‰¯ä½œç”¨â€¦â€¦
- en: Class-Specific Prior Preservation Loss
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç±»åˆ«ç‰¹å®šå…ˆéªŒä¿æŒæŸå¤±
- en: '![](../Images/5aa015342afc90ecfbdf58e6f45ff2ce.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5aa015342afc90ecfbdf58e6f45ff2ce.png)'
- en: 'Fig. 6 from [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven
    Generation](https://arxiv.org/abs/2208.12242).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven
    Generation](https://arxiv.org/abs/2208.12242)ä¸­çš„å›¾6ã€‚'
- en: Unlike Textual Inversion, DreamBooth fine-tunes all layers of the model to maximize
    performance. Unfortunately, doing so runs into the well-known problem of **language
    drift**, i.e. when a model is initially pre-trained on an extensive text corpus
    and subsequently fine-tuned for a particular task, it gradually diminishes its
    understanding of the languageâ€™s syntax and semantics.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æ–‡æœ¬åæ¼”ä¸åŒï¼ŒDreamBooth å¾®è°ƒæ¨¡å‹çš„æ‰€æœ‰å±‚ä»¥æœ€å¤§åŒ–æ€§èƒ½ã€‚ä¸å¹¸çš„æ˜¯ï¼Œè¿™æ ·åšä¼šé‡åˆ°ä¼—æ‰€å‘¨çŸ¥çš„**è¯­è¨€æ¼‚ç§»**é—®é¢˜ï¼Œå³å½“ä¸€ä¸ªæ¨¡å‹æœ€åˆåœ¨ä¸€ä¸ªå¹¿æ³›çš„æ–‡æœ¬è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åå†é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒæ—¶ï¼Œå®ƒä¼šé€æ¸å‡å°‘å¯¹è¯­è¨€è¯­æ³•å’Œè¯­ä¹‰çš„ç†è§£ã€‚
- en: An additional issue arises from the potential reduction in output diversity.
    This can be observed in the second row of Fig. 6, where the model, unless further
    adapted, has a tendency to replicate solely the poses found in the input images.
    This effect becomes more pronounced when the model is trained for an extended
    duration.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªé—®é¢˜æ˜¯è¾“å‡ºå¤šæ ·æ€§çš„æ½œåœ¨å‡å°‘ã€‚è¿™å¯ä»¥ä»å›¾6çš„ç¬¬äºŒè¡Œä¸­è§‚å¯Ÿåˆ°ï¼Œåœ¨è¯¥å›¾ä¸­ï¼Œæ¨¡å‹ï¼Œé™¤éè¿›ä¸€æ­¥è°ƒæ•´ï¼Œå¦åˆ™æœ‰å€¾å‘ä»…å¤åˆ¶è¾“å…¥å›¾åƒä¸­æ‰¾åˆ°çš„å§¿åŠ¿ã€‚å½“æ¨¡å‹è®­ç»ƒæ—¶é—´è¾ƒé•¿æ—¶ï¼Œè¿™ç§æ•ˆæœå˜å¾—æ›´åŠ æ˜æ˜¾ã€‚
- en: 'To mitigate these problems, the authors introduce a class-specific prior preservation
    loss, letâ€™s look at its overall loss formula and then explain its components:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å‡è½»è¿™äº›é—®é¢˜ï¼Œä½œè€…å¼•å…¥äº†ç±»åˆ«ç‰¹å®šå…ˆéªŒä¿æŒæŸå¤±ï¼Œè®©æˆ‘ä»¬å…ˆçœ‹çœ‹å…¶æ•´ä½“æŸå¤±å…¬å¼ï¼Œç„¶åå†è§£é‡Šå…¶ç»„æˆéƒ¨åˆ†ã€‚
- en: '![](../Images/ed2415b9465140a1046fad3604722833.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed2415b9465140a1046fad3604722833.png)'
- en: 'Eq. 2 from [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven
    Generation](https://arxiv.org/abs/2208.12242).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven
    Generation](https://arxiv.org/abs/2208.12242)ä¸­çš„å…¬å¼2ã€‚'
- en: The first part is the standard L2 denoising error, typical of any diffusion
    model. *Î±_t* scales the initial image **x** to which is then added a Gaussian
    noise **Îµ** âˆ¼ *N* (0, I), multiplied by *Ïƒ_t*. The random variable **z_*t*** :=
    *Î±_t****x** + *Ïƒ_t****Îµ** then has distribution *N*(*Î±_t****x**, *Ïƒ_tÂ²*). The
    model **x**Ë†*_Î¸* at this point will try to predict the original image from **z**_*t*,
    *t* and the conditioning vector **c** = Î“(**P**), where Î“ in the case of DreamBooth
    is [T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)
    and the prompt **P** has the form â€œa [identifier] [class noun]â€.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€éƒ¨åˆ†æ˜¯æ ‡å‡†çš„L2å»å™ªè¯¯å·®ï¼Œè¿™æ˜¯ä»»ä½•æ‰©æ•£æ¨¡å‹çš„å…¸å‹ç‰¹å¾ã€‚*Î±_t* å°†åˆå§‹å›¾åƒ**x** ç¼©æ”¾ï¼Œç„¶åæ·»åŠ é«˜æ–¯å™ªå£° **Îµ** âˆ¼ *N* (0, I)ï¼Œä¹˜ä»¥
    *Ïƒ_t*ã€‚éšæœºå˜é‡ **z_*t*** := *Î±_t****x** + *Ïƒ_t****Îµ** çš„åˆ†å¸ƒä¸º *N*(*Î±_t****x**, *Ïƒ_tÂ²*)ã€‚æ­¤æ—¶ï¼Œæ¨¡å‹
    **x**Ë†*_Î¸* å°†å°è¯•ä» **z**_*t*ã€*t* å’Œæ¡ä»¶å‘é‡ **c** = Î“(**P**) é¢„æµ‹åŸå§‹å›¾åƒï¼Œå…¶ä¸­åœ¨ DreamBooth çš„æƒ…å†µä¸‹ï¼ŒÎ“
    æ˜¯ [T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)ï¼Œè€Œæç¤º
    **P** çš„å½¢å¼ä¸ºâ€œä¸€ä¸ª [æ ‡è¯†ç¬¦] [ç±»åˆ«åè¯]â€ã€‚
- en: The second part is the **prior preservation loss**, in this instead of **x**
    we have **x**_pr, an image generated by the model with frozen weights (before
    fine-tuning) from a random initial noise **z**_*1* âˆ¼ *N* (0, I) and conditioning
    vector **c**_pr = Î“(â€œa [class noun]â€). This part then pushes the model to re-obtain
    **x**_pr from a corrupted version of it, thus pushing the model to generate images
    similar to those generated before the fine-tuning process for instances of the
    subject class.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒéƒ¨åˆ†æ˜¯ **å…ˆéªŒä¿ç•™æŸå¤±**ï¼Œåœ¨è¿™é‡Œï¼Œ**x** è¢«æ›¿æ¢ä¸º **x**_prï¼Œå³ç”±æ¨¡å‹ç”Ÿæˆçš„å›¾åƒï¼Œæ¨¡å‹çš„æƒé‡è¢«å†»ç»“ï¼ˆåœ¨å¾®è°ƒä¹‹å‰ï¼‰ï¼Œä»éšæœºåˆå§‹å™ªå£° **z**_*1*
    âˆ¼ *N* (0, I) å’Œæ¡ä»¶å‘é‡ **c**_pr = Î“(â€œä¸€ä¸ª [ç±»åˆ«åè¯]â€)ã€‚è¿™ä¸€éƒ¨åˆ†ä¿ƒä½¿æ¨¡å‹ä»å…¶æŸåç‰ˆæœ¬ä¸­é‡æ–°è·å– **x**_prï¼Œä»è€Œä¿ƒä½¿æ¨¡å‹ç”Ÿæˆç±»ä¼¼äºåœ¨å¾®è°ƒè¿‡ç¨‹ä¹‹å‰ç”Ÿæˆçš„å›¾åƒã€‚
- en: Finally, *w_t* and *w_t*â€™ are terms related to the noise schedule and *Î»* defines
    the relative weight to be given to the two losses.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œ*w_t* å’Œ *w_t*â€™ æ˜¯ä¸å™ªå£°è°ƒåº¦ç›¸å…³çš„æœ¯è¯­ï¼Œ*Î»* å®šä¹‰äº†ä¸¤ä¸ªæŸå¤±ä¹‹é—´çš„ç›¸å¯¹æƒé‡ã€‚
- en: Experiments
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®éªŒ
- en: '![](../Images/03e0a4db15e6f5cf7791ceb1e992ad69.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03e0a4db15e6f5cf7791ceb1e992ad69.png)'
- en: 'Fig. 4 from [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven
    Generation](https://arxiv.org/abs/2208.12242).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 4 æ¥è‡ª [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven
    Generation](https://arxiv.org/abs/2208.12242)ã€‚'
- en: Dataset
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°æ®é›†
- en: The dataset used for the experiments was generated by the authors and consists
    of 30 subjects including unique objects such as backpacks or sunglasses and animals
    such as dogs, cats, etc. Of the 30 subjects, 21 of the 30 subjects are objects,
    and 9 are live subjects/pets.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: å®éªŒä½¿ç”¨çš„æ•°æ®é›†ç”±ä½œè€…ç”Ÿæˆï¼ŒåŒ…å« 30 ä¸ªä¸»é¢˜ï¼ŒåŒ…æ‹¬ç‹¬ç‰¹çš„ç‰©å“ï¼Œå¦‚èƒŒåŒ…æˆ–å¤ªé˜³é•œï¼Œä»¥åŠåŠ¨ç‰©ï¼Œå¦‚ç‹—ã€çŒ«ç­‰ã€‚åœ¨è¿™ 30 ä¸ªä¸»é¢˜ä¸­ï¼Œ21 ä¸ªæ˜¯ç‰©ä½“ï¼Œ9 ä¸ªæ˜¯æ´»ä½“ä¸»é¢˜/å® ç‰©ã€‚
- en: 'The authors then define 25 prompts: 20 recontextualization and 5 property modification
    prompts for objects; 10 recontextualization, 10 accessorization and 5 property
    modification prompts for living subjects/pets.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…å®šä¹‰äº† 25 ä¸ªæç¤ºï¼š20 ä¸ªé‡æ–°èƒŒæ™¯åŒ–æç¤ºå’Œ 5 ä¸ªç‰©ä½“å±æ€§ä¿®æ”¹æç¤ºï¼›10 ä¸ªé‡æ–°èƒŒæ™¯åŒ–æç¤ºï¼Œ10 ä¸ªé…ä»¶åŒ–æç¤ºå’Œ 5 ä¸ªæ´»ä½“ä¸»é¢˜/å® ç‰©å±æ€§ä¿®æ”¹æç¤ºã€‚
- en: Evaluation Metrics
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯„ä¼°æŒ‡æ ‡
- en: For the evaluation, four images are generated per subject and per prompt, for
    a total of 3,000 images.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¯„ä¼°ï¼Œæ¯ä¸ªä¸»é¢˜å’Œæ¯ä¸ªæç¤ºç”Ÿæˆå››å¼ å›¾åƒï¼Œå…±è®¡ 3,000 å¼ å›¾åƒã€‚
- en: To measure **subject fidelity**, CLIP-I and DINO are used.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æµ‹é‡ **ä¸»é¢˜ä¸€è‡´æ€§**ï¼Œä½¿ç”¨ CLIP-I å’Œ DINOã€‚
- en: '**CLIP-I**, already used in previous work, calculates the average pairwise
    cosine similarity between [CLIP](https://openai.com/research/clip) embeddings
    of generated and real images. CLIP is trained to make a text description have
    the same embedding as an image to which it refers, so that if two images represent
    the same text they will have similar embeddings.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**CLIP-I**ï¼Œåœ¨ä¹‹å‰çš„å·¥ä½œä¸­å·²ç»ä½¿ç”¨ï¼Œè®¡ç®—ç”Ÿæˆå›¾åƒå’ŒçœŸå®å›¾åƒçš„[CLIP](https://openai.com/research/clip)åµŒå…¥çš„å¹³å‡æˆå¯¹ä½™å¼¦ç›¸ä¼¼åº¦ã€‚CLIPçš„è®­ç»ƒç›®æ ‡æ˜¯ä½¿æ–‡æœ¬æè¿°çš„åµŒå…¥ä¸å…¶æ‰€æŒ‡çš„å›¾åƒå…·æœ‰ç›¸åŒçš„åµŒå…¥ï¼Œå› æ­¤å¦‚æœä¸¤ä¸ªå›¾åƒè¡¨ç¤ºç›¸åŒçš„æ–‡æœ¬ï¼Œå®ƒä»¬å°†å…·æœ‰ç›¸ä¼¼çš„åµŒå…¥ã€‚'
- en: '**DINO**, a new metric introduced by the authors, is analogous to CLIP-I but
    instead of generating embeddings with CLIP these are generated with [ViT-S/16
    DINO](https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/),
    a self-supervised trained model.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**DINO**ï¼Œç”±ä½œè€…å¼•å…¥çš„æ–°æŒ‡æ ‡ï¼Œç±»ä¼¼äº CLIP-Iï¼Œä½†ç”ŸæˆåµŒå…¥çš„æ–¹å¼æ˜¯ä½¿ç”¨[ViT-S/16 DINO](https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/)ï¼Œä¸€ä¸ªè‡ªç›‘ç£è®­ç»ƒçš„æ¨¡å‹ã€‚'
- en: In the paper, is observed that CLIP-I, due to the way CLIP is trained, does
    not distinguish between different subjects that might have very similar textual
    descriptions. On the other hand, DINO (the model, not the metric) is trained in
    a self-supervised manner, which facilitates the distinction of unique features
    within a subject or image. For this reason, they regard DINO as their primary
    metric.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡ä¸­è§‚å¯Ÿåˆ°ï¼Œç”±äº CLIP çš„è®­ç»ƒæ–¹å¼ï¼ŒCLIP-I ä¸åŒºåˆ†å¯èƒ½å…·æœ‰éå¸¸ç›¸ä¼¼æ–‡æœ¬æè¿°çš„ä¸åŒä¸»é¢˜ã€‚å¦ä¸€æ–¹é¢ï¼ŒDINOï¼ˆæŒ‡çš„æ˜¯æ¨¡å‹ï¼Œè€Œä¸æ˜¯æŒ‡æ ‡ï¼‰ä»¥è‡ªç›‘ç£çš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œè¿™æœ‰åŠ©äºåŒºåˆ†ä¸»é¢˜æˆ–å›¾åƒä¸­çš„ç‹¬ç‰¹ç‰¹å¾ã€‚å› æ­¤ï¼Œä»–ä»¬å°†
    DINO è§†ä¸ºä¸»è¦æŒ‡æ ‡ã€‚
- en: 'Finally, a third metric, CLIP-T, is introduced to measure, another important
    aspect: **prompt fidelity**, that is, when the generated image is close to the
    input prompt.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œå¼•å…¥äº†ç¬¬ä¸‰ä¸ªåº¦é‡æŒ‡æ ‡ CLIP-Tï¼Œç”¨æ¥è¡¡é‡å¦ä¸€ä¸ªé‡è¦æ–¹é¢ï¼š**æç¤ºä¸€è‡´æ€§**ï¼Œå³ç”Ÿæˆçš„å›¾åƒä¸è¾“å…¥æç¤ºçš„æ¥è¿‘ç¨‹åº¦ã€‚
- en: '**CLIP-T** is similar to the previous metrics as well, measuring the average
    cosine similarity between two embeddings obtained with CLIP: one from the prompt
    and the other from the image. It is important to note that CLIP was specifically
    trained to generate textual embeddings that are similar to the ones of the corresponding
    images.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**CLIP-T** ä¸ä¹‹å‰çš„æŒ‡æ ‡ç±»ä¼¼ï¼Œæµ‹é‡ä»CLIPä¸­è·å¾—çš„ä¸¤ä¸ªåµŒå…¥ä¹‹é—´çš„å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦ï¼šä¸€ä¸ªæ¥è‡ªæç¤ºï¼Œå¦ä¸€ä¸ªæ¥è‡ªå›¾åƒã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒCLIPç‰¹åˆ«è®­ç»ƒä»¥ç”Ÿæˆä¸å¯¹åº”å›¾åƒçš„æ–‡æœ¬åµŒå…¥ç›¸ä¼¼çš„åµŒå…¥ã€‚'
- en: Comparisons
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¯”è¾ƒ
- en: '![](../Images/b78a564495b55a43c1c143aa0fa5f4cd.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b78a564495b55a43c1c143aa0fa5f4cd.png)'
- en: 'Table 1 from [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven
    Generation](https://arxiv.org/abs/2208.12242).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¡¨1æ¥è‡ªäº [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven
    Generation](https://arxiv.org/abs/2208.12242)ã€‚'
- en: As we can see from Table 1, DreamBooth is clearly superior to Textual Inversion
    when measured using the DINO and CLIP-T metrics, while the distance is less when
    measured with CLIP-I, which, however, as already mentioned, is not a good metric
    for measuring fidelity to a specific subject.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¡¨1å¯ä»¥çœ‹å‡ºï¼Œå½“ä½¿ç”¨DINOå’ŒCLIP-TæŒ‡æ ‡æµ‹é‡æ—¶ï¼ŒDreamBoothæ˜æ˜¾ä¼˜äºTextual Inversionï¼Œè€Œåœ¨ä½¿ç”¨CLIP-Iæµ‹é‡æ—¶å·®è·è¾ƒå°ï¼Œä½†å¦‚å‰æ‰€è¿°ï¼ŒCLIP-Iå¹¶ä¸æ˜¯ä¸€ä¸ªå¥½çš„è¡¡é‡ç‰¹å®šä¸»é¢˜ä¿çœŸåº¦çš„æŒ‡æ ‡ã€‚
- en: '![](../Images/3b847e303394e05cc38aa89975d9fcbe.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3b847e303394e05cc38aa89975d9fcbe.png)'
- en: 'Table 2 from [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven
    Generation](https://arxiv.org/abs/2208.12242).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¡¨2æ¥è‡ªäº [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven
    Generation](https://arxiv.org/abs/2208.12242)ã€‚'
- en: It is very difficult to find a metric that correlates perfectly with what a
    person would judge to be a more or less good outcome. For this reason, the authors
    also measure the preference of a sample of 72 users. The results highlight that
    the percentage of users who prefer DreamBooth for both subject fidelity and prompt
    fidelity is larger than one would be led to believe by looking at the previous
    metrics alone. We can judge for ourselves by looking at the images shown in Fig.
    4 of the paper, where the stark difference between the two methodologies is clear,
    at least for this specific example.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆéš¾æ‰¾åˆ°ä¸€ä¸ªä¸ä¸ªäººåˆ¤æ–­ç»“æœå¥½åå®Œå…¨ä¸€è‡´çš„æŒ‡æ ‡ã€‚å› æ­¤ï¼Œä½œè€…ä»¬è¿˜æµ‹é‡äº†ä¸€ç»„72åç”¨æˆ·çš„åå¥½ã€‚ç»“æœçªæ˜¾å‡ºï¼Œå¯¹äºä¸»é¢˜ä¿çœŸåº¦å’Œæç¤ºä¿çœŸåº¦ï¼Œåå¥½DreamBoothçš„ç”¨æˆ·ç™¾åˆ†æ¯”è¦é«˜äºä»…å‡­ä¹‹å‰çš„æŒ‡æ ‡æ‰€èƒ½å¾—å‡ºçš„ç»“è®ºã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡æŸ¥çœ‹è®ºæ–‡ä¸­çš„å›¾4æ¥åˆ¤æ–­ï¼Œä¸¤ç§æ–¹æ³•ä¹‹é—´çš„æ˜¾è‘—å·®å¼‚åœ¨è¿™ä¸ªç‰¹å®šä¾‹å­ä¸­æ˜¯æ˜¾è€Œæ˜“è§çš„ã€‚
- en: Conclusion
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: The field of image generation and generative AI has gained significant attention
    in recent times. Advancements in image synthesis, particularly through the use
    of diffusion models, have propelled this area forward.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒç”Ÿæˆå’Œç”Ÿæˆå¼AIé¢†åŸŸè¿‘å¹´æ¥è·å¾—äº†æ˜¾è‘—å…³æ³¨ã€‚ç‰¹åˆ«æ˜¯é€šè¿‡æ‰©æ•£æ¨¡å‹çš„ä½¿ç”¨ï¼Œå›¾åƒåˆæˆçš„è¿›å±•æ¨åŠ¨äº†è¿™ä¸€é¢†åŸŸçš„å‘å±•ã€‚
- en: In this article, we delved into the scientific paper of DreamBooth â€” an impressive
    solution that enables the generation of new images with varying poses and contexts
    while maintaining fidelity to the desired subject. This innovative approach showcases
    the remarkable progress made in the realm of image synthesis and holds great potential
    for future developments.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥æ¢è®¨äº†DreamBoothçš„ç§‘å­¦è®ºæ–‡â€”â€”è¿™æ˜¯ä¸€ç§ä»¤äººå°è±¡æ·±åˆ»çš„è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿç”Ÿæˆå…·æœ‰ä¸åŒå§¿åŠ¿å’ŒèƒŒæ™¯çš„æ–°å›¾åƒï¼ŒåŒæ—¶ä¿æŒå¯¹æœŸæœ›ä¸»é¢˜çš„å¿ å®ã€‚è¿™ç§åˆ›æ–°çš„æ–¹æ³•å±•ç¤ºäº†å›¾åƒåˆæˆé¢†åŸŸå–å¾—çš„æ˜¾è‘—è¿›å±•ï¼Œå¹¶å¯¹æœªæ¥çš„å‘å±•å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚
- en: Thank you for taking the time to read this article, and please feel free to
    leave a comment or connect with me to share your thoughts or ask any questions.
    To stay updated on my latest articles, you can follow me on [Medium](https://medium.com/@mnslarcher),
    [LinkedIn](https://www.linkedin.com/in/mnslarcher/) or [Twitter](https://twitter.com/mnslarcher).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢æ‚¨æŠ½å‡ºæ—¶é—´é˜…è¯»æœ¬æ–‡ï¼Œå¦‚æœ‰ä»»ä½•æ„è§æˆ–é—®é¢˜ï¼Œè¯·éšæ—¶ç•™è¨€æˆ–ä¸æˆ‘è”ç³»ã€‚è¦äº†è§£æˆ‘çš„æœ€æ–°æ–‡ç« ï¼Œæ‚¨å¯ä»¥å…³æ³¨æˆ‘åœ¨ [Medium](https://medium.com/@mnslarcher)ã€[LinkedIn](https://www.linkedin.com/in/mnslarcher/)
    æˆ– [Twitter](https://twitter.com/mnslarcher) ä¸Šçš„åŠ¨æ€ã€‚
- en: '[](https://medium.com/@mnslarcher/membership?source=post_page-----70f8bb0cfa30--------------------------------)
    [## Join Medium with my referral link - Mario Namtao Shianti Larcher'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mnslarcher/membership?source=post_page-----70f8bb0cfa30--------------------------------)
    [## é€šè¿‡æˆ‘çš„æ¨èé“¾æ¥åŠ å…¥Medium - Mario Namtao Shianti Larcher'
- en: Read every story from Mario Namtao Shianti Larcher (and thousands of other writers
    on Medium). Your membership feeâ€¦
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é˜…è¯»Mario Namtao Shianti Larcherçš„æ¯ä¸€ä¸ªæ•…äº‹ï¼ˆä»¥åŠMediumä¸Šçš„å…¶ä»–æˆåƒä¸Šä¸‡ä½ä½œå®¶çš„æ•…äº‹ï¼‰ã€‚æ‚¨çš„ä¼šå‘˜è´¹â€¦â€¦
- en: medium.com](https://medium.com/@mnslarcher/membership?source=post_page-----70f8bb0cfa30--------------------------------)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@mnslarcher/membership?source=post_page-----70f8bb0cfa30--------------------------------)
