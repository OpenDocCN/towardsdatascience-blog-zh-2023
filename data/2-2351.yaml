- en: When Stochastic Policies Are Better Than Deterministic Ones
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机策略何时优于确定性策略
- en: 原文：[https://towardsdatascience.com/when-stochastic-policies-are-better-than-deterministic-ones-b950cd0d60f4](https://towardsdatascience.com/when-stochastic-policies-are-better-than-deterministic-ones-b950cd0d60f4)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/when-stochastic-policies-are-better-than-deterministic-ones-b950cd0d60f4](https://towardsdatascience.com/when-stochastic-policies-are-better-than-deterministic-ones-b950cd0d60f4)
- en: Why we let randomness dictate our action selection in Reinforcement Learning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么我们让随机性决定强化学习中的动作选择
- en: '[](https://wvheeswijk.medium.com/?source=post_page-----b950cd0d60f4--------------------------------)[![Wouter
    van Heeswijk, PhD](../Images/9c996bccd6fdfb6d9aa8b50b93338eb9.png)](https://wvheeswijk.medium.com/?source=post_page-----b950cd0d60f4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b950cd0d60f4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b950cd0d60f4--------------------------------)
    [Wouter van Heeswijk, PhD](https://wvheeswijk.medium.com/?source=post_page-----b950cd0d60f4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wvheeswijk.medium.com/?source=post_page-----b950cd0d60f4--------------------------------)[![Wouter
    van Heeswijk, PhD](../Images/9c996bccd6fdfb6d9aa8b50b93338eb9.png)](https://wvheeswijk.medium.com/?source=post_page-----b950cd0d60f4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b950cd0d60f4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b950cd0d60f4--------------------------------)
    [Wouter van Heeswijk, PhD](https://wvheeswijk.medium.com/?source=post_page-----b950cd0d60f4--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b950cd0d60f4--------------------------------)
    ·6 min read·Feb 18, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b950cd0d60f4--------------------------------)
    ·6分钟阅读·2023年2月18日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/8175bb620e2361f15241cbfd404190f9.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8175bb620e2361f15241cbfd404190f9.png)'
- en: Rock-paper-scissors would be a dull affair with deterministic policies [Photo
    by [Marcus Wallis](https://unsplash.com/@marcus_wallis?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)]
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用确定性策略进行石头剪刀布游戏将变得非常无聊 [照片来源： [Marcus Wallis](https://unsplash.com/@marcus_wallis?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)]
- en: If you are used to deterministic decision-making policies (e.g., as in [Deep
    Q-learning](https://medium.com/towards-data-science/deep-q-learning-for-the-cliff-walking-problem-b54835409046)),
    the need for and use of stochastic policies might elude you. After all, **deterministic
    policies** offer a convenient state-action mapping *π:s ↦ a*, ideally even the
    optimal mapping (that is, if all the Bellman equations are learned to perfection).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你习惯于确定性决策策略（例如，[Deep Q-learning](https://medium.com/towards-data-science/deep-q-learning-for-the-cliff-walking-problem-b54835409046)），你可能会忽视随机策略的必要性和使用。毕竟，**确定性策略**提供了一个方便的状态-动作映射
    *π:s ↦ a*，理想情况下甚至是最佳映射（即，如果所有贝尔曼方程都被完美地学习了）。
- en: In contrast, **stochastic policies —** represented by a conditional probability
    distribution over the actions in a given state, *π:P(a|s)* — seem rather inconvenient
    and imprecise. Why would we allow randomness to direct our actions, why leave
    the selection of the best known decisions to chance?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，**随机策略 —** 通过给定状态下的动作的条件概率分布表示，*π:P(a|s)* — 看起来相当不方便且不精确。我们为什么要让随机性来指导我们的行动，为什么要将最佳已知决策的选择留给偶然？
- en: In reality, a huge number of Reinforcement Learning (RL) algorithms indeed deploys
    stochastic policies, judging by the sheer number of actor-critic algorithms out
    there. Evidently, there must be some benefit to this approach. This article discusses
    four cases in which stochastic policies are superior to their deterministic counterparts.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，从现有的演员-评论家算法数量来看，大量的强化学习（RL）算法确实使用了随机策略。显然，这种方法必然有其好处。本文讨论了四种情况下，随机策略优于其确定性对手的情况。
- en: '[](/the-four-policy-classes-of-reinforcement-learning-38185daa6c8a?source=post_page-----b950cd0d60f4--------------------------------)
    [## The Four Policy Classes of Reinforcement Learning'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/the-four-policy-classes-of-reinforcement-learning-38185daa6c8a?source=post_page-----b950cd0d60f4--------------------------------)
    [## 强化学习的四种策略类别'
- en: towardsdatascience.com](/the-four-policy-classes-of-reinforcement-learning-38185daa6c8a?source=post_page-----b950cd0d60f4--------------------------------)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/the-four-policy-classes-of-reinforcement-learning-38185daa6c8a?source=post_page-----b950cd0d60f4--------------------------------)
- en: '**I. Multi-agent environments**'
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**I. 多智能体环境**'
- en: Predictable is not always good.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 可预测性并不总是好事。
- en: 'From a game of **rock-paper-scissors**, it is perfectly clear that a deterministic
    policy would fail miserably. The opponent would quickly figure out that you *always*
    play rock, and choose the counter-action accordingly. Evidently, the Nash equilibrium
    here is a uniformly distributed policy that selects each action with probability
    1/3\. How to learn it? You guessed it: stochastic policies.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 从**剪刀石头布**的游戏中可以清楚地看到，确定性策略会惨遭失败。对手会很快发现你*总是*玩石头，并据此选择反制动作。显然，纳什均衡是一个均匀分布的策略，以概率
    1/3 选择每个动作。如何学习它？你猜对了：随机策略。
- en: Especially in adversarial environments — in which opponents have diverging objectives
    and try to anticipate your decisions — it is often beneficial to have a degree
    of randomness in a policy. After all, game theory dictates there is often no pure
    strategy that always formulates a single optimal response to an opponent, instead
    propagating **mixed strategies** as the best action selection mechanism for many
    games.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是在对抗性环境中——对手有不同的目标并尝试预测你的决策——在策略中引入一定程度的随机性通常是有益的。毕竟，博弈论规定通常没有纯策略能够始终对对手做出单一的最佳回应，而是将**混合策略**作为许多游戏的最佳动作选择机制。
- en: Unpredictability is a powerful competitive tool, and if this trait is desired,
    stochastic policies are the clear way to go.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 不可预测性是一个强大的竞争工具，如果需要这种特质，随机策略显然是最佳选择。
- en: '![](../Images/abe1c928c3bb5f601addc0b7417fd458.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/abe1c928c3bb5f601addc0b7417fd458.png)'
- en: When facing a worthy adversary, always playing the same game will quickly backfire.
    [Photo by [Christian Tenguan](https://unsplash.com/@christiantenguan?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)]
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 面对一个值得的对手时，总是玩相同的游戏会很快适得其反。[照片由 [Christian Tenguan](https://unsplash.com/@christiantenguan?utm_source=medium&utm_medium=referral)
    提供，来源 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)]
- en: II. Partial observations (POMDP)
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: II. 部分观察（POMDP）
- en: In many situations, we do not have a perfect picture of the true problem state,
    but instead try to infer them from **imperfect observations**. The field of Partially
    Observed Markov Decision Processes (POMDPs) is built around this discrepancy between
    state and observation. The same imperfection applies when we represent states
    by means of **features**, as is often needed to handle large state spaces.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，我们对真实问题状态的了解并不完美，而是试图从**不完美的观察**中推测。部分观察马尔可夫决策过程（POMDPs）领域就是围绕这种状态和观察之间的差异建立的。当我们通过**特征**来表示状态时，这种不完美同样适用，因为在处理大状态空间时通常需要这样做。
- en: Consider the famous *aliased* *GridWorld* by David Silver. Here, states are
    represented by the *observation* of surrounding walls. In the illustration below,
    in both shaded states the agent observes a wall on the upside and one on the downside.
    Although the **true states are distinct** and require different actions, they
    are **identical in the observation**.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑大卫·银的著名*别名* *GridWorld*。在这里，状态通过*观察*周围的墙来表示。在下面的插图中，在两个阴影状态中，智能体观察到上方和下方都有墙。尽管**真实状态是不同的**并且需要不同的动作，但它们在**观察中是相同的**。
- en: Based on the imperfect observation alone, the agent must make a decision. A
    **value function approximation** (e.g., [Q-learning](https://medium.com/towards-data-science/walking-off-the-cliff-with-off-policy-reinforcement-learning-7fdbcdfe31ff))
    can easily get stuck here, always picking the same action (e.g., always left)
    and thus never reaching the reward. An ϵ-greedy reward might mitigate the situation,
    but still gets stuck most of the time.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 基于不完美的观察，智能体必须做出决策。**价值函数近似**（例如，[Q-learning](https://medium.com/towards-data-science/walking-off-the-cliff-with-off-policy-reinforcement-learning-7fdbcdfe31ff)）可能很容易陷入困境，总是选择相同的动作（例如，总是向左），从而永远无法获得奖励。ϵ-贪婪奖励可能会缓解这种情况，但仍然大多数时候会陷入困境。
- en: In contrast, a policy gradient algorithm will **learn to go left or right with
    probability 0.5** for these identical observations, thus finding the treasure
    much quicker. By acknowledging that the agent has an imperfect perception of its
    environment, it deliberately takes probabilistic actions to counteract the inherent
    uncertainty.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 相对而言，策略梯度算法将**以 0.5 的概率学会向左或向右走**，因此可以更快地找到宝藏。通过认识到智能体对环境的感知不完美，它故意采取概率性行动以对抗固有的不确定性。
- en: '![](../Images/0b759c9c47c9075f1d04d6a3e71596e7.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0b759c9c47c9075f1d04d6a3e71596e7.png)'
- en: Deterministic policy in Aliased GridWorld. The agent can only observe walls
    and is thus unable to distinguish the shaded states. A deterministic policy will
    make identical decisions in both states, which will often trap the agent in the
    upper-left corner. [Image by author, based on example by David Silver, clipart
    by [GDJ](https://openclipart.org/artist/GDJ) [[1](https://openclipart.org/detail/221913/pirate-paraphernalia),
    [2](https://openclipart.org/detail/314002/pirate-finds-treasure-by-richardsdrawings)]
    via [OpenClipArt.org](https://openclipart.org/)]
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Aliased GridWorld中的确定性策略。代理只能观察墙壁，因此无法区分阴影状态。确定性策略将在两个状态中做出相同的决策，这通常会将代理困在左上角。[图片由作者提供，基于David
    Silver的示例，剪贴画来自[GDJ](https://openclipart.org/artist/GDJ) [[1](https://openclipart.org/detail/221913/pirate-paraphernalia),
    [2](https://openclipart.org/detail/314002/pirate-finds-treasure-by-richardsdrawings)]，通过[OpenClipArt.org](https://openclipart.org/)]
- en: '![](../Images/0319c39fdc70c401803e026423da6f36.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0319c39fdc70c401803e026423da6f36.png)'
- en: Stochastic policy in Aliased GridWorld. The agent can only observe walls and
    is thus unable to distinguish the shaded states. An optimal stochastic policy
    will choose left and right with equal probability in the aliased states, making
    it much more likely to find the treasure. [Image by author, based on example by
    David Silver, clipart by [GDJ](https://openclipart.org/artist/GDJ) [[1](https://openclipart.org/detail/221913/pirate-paraphernalia),
    [2](https://openclipart.org/detail/314002/pirate-finds-treasure-by-richardsdrawings)]
    via [OpenClipArt.org](https://openclipart.org/)]
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Aliased GridWorld中的随机策略。代理只能观察墙壁，因此无法区分阴影状态。一个最佳的随机策略将在别名状态中以相等的概率选择左和右，从而更有可能找到宝藏。[图片由作者提供，基于David
    Silver的示例，剪贴画来自[GDJ](https://openclipart.org/artist/GDJ) [[1](https://openclipart.org/detail/221913/pirate-paraphernalia),
    [2](https://openclipart.org/detail/314002/pirate-finds-treasure-by-richardsdrawings)]，通过[OpenClipArt.org](https://openclipart.org/)]
- en: III. Stochastic environments
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: III. 随机环境
- en: Most environments — especially those in real life — exhibit **substantial uncertainty**.
    Even if we were to make the exact same decision in the exact same state, the corresponding
    reward trajectories may vary wildly. Before we have a reasonable estimate of expected
    downstream values, we may have to perform many, many training iterations.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数环境——尤其是现实生活中的环境——都表现出**显著的不确定性**。即使我们在完全相同的状态下做出完全相同的决策，相应的奖励轨迹也可能大相径庭。在我们对期望的下游值有一个合理的估计之前，我们可能需要进行许多训练迭代。
- en: If we encounter such considerable **uncertainty in the environment** itself
    — as reflected in its [transition function](https://medium.com/towards-data-science/the-five-building-blocks-of-markov-decision-processes-997dc1ab48a7)
    — stochastic policies often aid in its discovery. Policy gradient methods offer
    a powerful and inherent exploration mechanism that is not present in vanilla implementations
    of value-based methods.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在环境本身中遇到如此巨大的**不确定性**——如其[转移函数](https://medium.com/towards-data-science/the-five-building-blocks-of-markov-decision-processes-997dc1ab48a7)所反映的——随机策略通常有助于发现这些不确定性。策略梯度方法提供了一种强大且固有的探索机制，这在基于价值的方法的普通实现中是不存在的。
- en: In this context, we do not necessarily seek an inherently probabilistic policy
    as an end goal, but it surely helps while exploring the environment. The combination
    of probabilistic action selection and policy gradient updates directs our improvement
    steps in uncertain environments, even if that search ultimately guides us to a
    **near-deterministic policy**.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们不一定要将固有的概率策略作为最终目标，但在探索环境时它确实有帮助。概率性动作选择和策略梯度更新的结合引导我们在不确定环境中改进，即使这种搜索最终将我们引导到一个**接近确定性的策略**。
- en: '![](../Images/1aceafe692cfa1f208f9c14ccc38799c.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1aceafe692cfa1f208f9c14ccc38799c.png)'
- en: When deploying stochastic policies, the interplay between control and environment
    generate a powerful exploration dynamic [image by author]
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署随机策略时，控制和环境之间的相互作用生成了强大的探索动态[图片由作者提供]
- en: 'Truth be told, if we look past the standard ϵ-greedy algorithm in value function
    approximation, there are number of powerful exploration strategies that work perfectly
    well while learning deterministic policies:'
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 说实话，如果我们抛开价值函数逼近中的标准ϵ-贪婪算法，还有许多强大的探索策略在学习确定性策略时效果非常好：
- en: '[](/seven-exploration-strategies-in-reinforcement-learning-you-should-know-8eca7dec503b?source=post_page-----b950cd0d60f4--------------------------------)
    [## Seven Exploration Strategies In Reinforcement Learning You Should Know'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/seven-exploration-strategies-in-reinforcement-learning-you-should-know-8eca7dec503b?source=post_page-----b950cd0d60f4--------------------------------)
    [## 你应该知道的七种强化学习探索策略'
- en: Pure exploration and -exploitation, ϵ-greedy, Boltzmann exploration, optimistic
    initialization, confidence intervals…
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 纯探索与**利用**、ϵ-贪心、玻尔兹曼探索、乐观初始化、置信区间……
- en: towardsdatascience.com](/seven-exploration-strategies-in-reinforcement-learning-you-should-know-8eca7dec503b?source=post_page-----b950cd0d60f4--------------------------------)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/seven-exploration-strategies-in-reinforcement-learning-you-should-know-8eca7dec503b?source=post_page-----b950cd0d60f4--------------------------------)
- en: IV. Continuous action spaces
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: IV. 连续动作空间
- en: Although there are some workarounds, to apply value-based methods in continuous
    spaces we are generally required to **discretize the action space**. The more
    fine-grained the discretization, the closer the original problem is approximated.
    However, this comes at the cost of increased computational complexity.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有一些变通方法，但在连续空间中应用基于价值的方法通常需要**离散化动作空间**。离散化越精细，原始问题的逼近程度就越高。然而，这会增加计算复杂性。
- en: Consider a self-driving car. How hard to hit the gas, how hard to hit the breaks,
    how much to throttle the gas — they are all **intrinsically continuous actions**.
    In a continuous action space, they can be represented by three variables that
    can each adopt values within a certain range.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以自驾车为例。踩油门的力度、踩刹车的力度、加油的程度——这些都是**内在的连续动作**。在连续动作空间中，它们可以通过三个变量来表示，每个变量都可以在一定范围内取值。
- en: Suppose we define 100 intensity levels for both gas and break and 360 degrees
    for the steering wheel. With 100*100*360=3.6M combinations, we have a pretty big
    action space, while still lacking the fine touch of continuous control. Clearly,
    the **combination of high dimensionality and continuous variables** is particularly
    hard to handle through discretization.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们为油门和刹车定义了100个强度级别，为方向盘定义了360度。这样有100*100*360=3.6M种组合，我们有一个相当大的动作空间，但仍然缺乏连续控制的细腻触感。显然，**高维度和连续变量的组合**通过离散化处理起来特别困难。
- en: In contrast, policy gradient methods are perfectly capable of **drawing continuous
    actions from representative probability distributions**, making them the default
    choice for continuous control problems. For instance, we might represent the policy
    by three parameterized Gaussian distributions, learning both mean and standard
    deviation.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，策略梯度方法完全能够**从代表性概率分布中抽取连续动作**，使其成为连续控制问题的默认选择。例如，我们可以通过三个参数化的高斯分布来表示策略，学习均值和标准差。
- en: '![](../Images/a099554fafbb8ebc152abef4efb2091b.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a099554fafbb8ebc152abef4efb2091b.png)'
- en: Driving a car is an example of inherently continuous control. The finer the
    action space is discretized, the more cumbersome value-based learning becomes
    [Photo by [Nathan Van Egmond](https://unsplash.com/@thevanegmond?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)]
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 驾驶汽车是一个固有的连续控制的例子。动作空间离散化得越细致，基于价值的学习就变得越繁琐 [照片来源 [Nathan Van Egmond](https://unsplash.com/@thevanegmond?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)]
- en: Convergence to near-deterministic policies
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向近乎确定性策略的收敛
- en: Before concluding the article, it is important to emphasize that a stochastic
    policy does not imply that we keep making semi-random decisions until the end
    of time.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束本文之前，重要的是要强调一个随机策略并不意味着我们会一直做半随机决策直到时间的尽头。
- en: In some cases (e.g., the aforementioned rock-paper-scissors or Aliased GridWorld)
    the **optimal policy requires mixed action selection** (with probabilities 30%/30%/30%
    and 50%/50%, respectively).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下（例如上述的石头剪子布或别名网格世界），**最佳策略需要混合动作选择**（分别为30%/30%/30%和50%/50%）。
- en: In other cases, (e.g., [identifying the best slot machine](https://medium.com/towards-data-science/a-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7))
    the optimal response may in fact be a deterministic one. In such cases, the stochastic
    policy will converge to a **near-deterministic** one, e.g., selecting a particular
    action with 99.999% probability. For continuous action spaces, the policy will
    converge to very small standard deviations.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下（例如，[识别最佳老虎机](https://medium.com/towards-data-science/a-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7)），最佳响应实际上可能是确定性的。在这种情况下，随机策略将收敛到一个**近乎确定性的**策略，例如，以99.999%的概率选择某个特定动作。对于连续动作空间，策略将收敛到非常小的标准差。
- en: That said, the policy will never be *completely* deterministic. For mathematicians
    that write **convergence proofs** this is actually a nice property, ensuring infinite
    exploration in the limit. Real-world practitioners may have to be a bit pragmatic
    to avoid the occasional idiotic action.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，政策将永远不会是*完全*确定性的。对于撰写**收敛性证明**的数学家来说，这实际上是一个不错的特性，确保在极限情况下无限探索。现实世界的从业者可能需要稍微务实，以避免偶尔的愚蠢行为。
- en: '![](../Images/a52248119e5cdba726c85f18d7c7d10f.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a52248119e5cdba726c85f18d7c7d10f.png)'
- en: Depending on the problem, stochastic policies may converge to near-deterministic
    ones, virtually always picking the action yielding the highest expected reward
    [image by author]
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 根据问题的不同，随机政策可能会收敛为接近确定性的政策，几乎总是选择产生最高预期奖励的行动 [图片来源于作者]
- en: Wrapping up
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'And there you have it, four cases in which stochastic policies are preferable
    over deterministic ones:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有四种情况下，随机政策比确定性政策更可取：
- en: '**Multi-agent environments**: Our predictability gets punished by other agents.
    Adding randomness to our actions makes it hard for the opponent to anticipate.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多智能体环境**：我们的可预测性会被其他智能体惩罚。对我们的行动添加随机性使得对手难以预判。'
- en: '**Stochastic environments**: Uncertain environments beg a high degree of exploration,
    which is not inherently provided by algorithms based on deterministic policies.
    Stochastic policies automatically explore the environment.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机环境**：不确定的环境需要较高的探索程度，而基于确定性政策的算法并未固有地提供这种探索。随机政策会自动探索环境。'
- en: '**Partially observable environments**: As observations (e.g., feature representations
    of states) are imperfect representations of the true system states, we struggle
    to distinguish between states that require different actions. Mixing up our decisions
    may resolve the problem.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**部分可观察环境**：由于观察（例如，状态的特征表示）是不完美的真实系统状态的表示，我们难以区分需要不同操作的状态。混合我们的决策可能会解决这个问题。'
- en: '**Continuous action spaces**: We must otherwise finely discretize the action
    space to learn value functions. In contrast, policy-based methods gracefully explore
    continuous action spaces by drawing from corresponding probability density functions.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连续动作空间**：否则，我们必须对动作空间进行精细离散化以学习价值函数。相反，基于政策的方法通过从相应的概率密度函数中抽样优雅地探索连续动作空间。'
- en: References
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[https://www.davidsilver.uk/wp-content/uploads/2020/03/pg.pdf](https://www.davidsilver.uk/wp-content/uploads/2020/03/pg.pdf)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.davidsilver.uk/wp-content/uploads/2020/03/pg.pdf](https://www.davidsilver.uk/wp-content/uploads/2020/03/pg.pdf)'
- en: '[https://www.freecodecamp.org/news/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f/](https://www.freecodecamp.org/news/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f/#:~:text=On%20the%20other%20hand%2C%20a,when%20the%20environment%20is%20uncertain)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.freecodecamp.org/news/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f/](https://www.freecodecamp.org/news/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f/#:~:text=On%20the%20other%20hand%2C%20a,when%20the%20environment%20is%20uncertain)'
- en: '[https://en.wikipedia.org/wiki/Strategy_(game_theory)](https://en.wikipedia.org/wiki/Strategy_(game_theory))'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Strategy_(game_theory)](https://en.wikipedia.org/wiki/Strategy_(game_theory))'
