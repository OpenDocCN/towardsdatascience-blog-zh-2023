- en: 10 Ways to Improve the Performance of Retrieval Augmented Generation Systems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升检索增强生成系统性能的10种方法
- en: 原文：[https://towardsdatascience.com/10-ways-to-improve-the-performance-of-retrieval-augmented-generation-systems-5fa2cee7cd5c?source=collection_archive---------0-----------------------#2023-09-18](https://towardsdatascience.com/10-ways-to-improve-the-performance-of-retrieval-augmented-generation-systems-5fa2cee7cd5c?source=collection_archive---------0-----------------------#2023-09-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/10-ways-to-improve-the-performance-of-retrieval-augmented-generation-systems-5fa2cee7cd5c?source=collection_archive---------0-----------------------#2023-09-18](https://towardsdatascience.com/10-ways-to-improve-the-performance-of-retrieval-augmented-generation-systems-5fa2cee7cd5c?source=collection_archive---------0-----------------------#2023-09-18)
- en: Tools to go from prototype to production
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从原型到生产的工具
- en: '[](https://mattambrogi.medium.com/?source=post_page-----5fa2cee7cd5c--------------------------------)[![Matt
    Ambrogi](../Images/60aec1bdc756f49ecf556056ab82f950.png)](https://mattambrogi.medium.com/?source=post_page-----5fa2cee7cd5c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5fa2cee7cd5c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5fa2cee7cd5c--------------------------------)
    [Matt Ambrogi](https://mattambrogi.medium.com/?source=post_page-----5fa2cee7cd5c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mattambrogi.medium.com/?source=post_page-----5fa2cee7cd5c--------------------------------)[![Matt
    Ambrogi](../Images/60aec1bdc756f49ecf556056ab82f950.png)](https://mattambrogi.medium.com/?source=post_page-----5fa2cee7cd5c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5fa2cee7cd5c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5fa2cee7cd5c--------------------------------)
    [Matt Ambrogi](https://mattambrogi.medium.com/?source=post_page-----5fa2cee7cd5c--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1e23ad8f92c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-ways-to-improve-the-performance-of-retrieval-augmented-generation-systems-5fa2cee7cd5c&user=Matt+Ambrogi&userId=1e23ad8f92c5&source=post_page-1e23ad8f92c5----5fa2cee7cd5c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5fa2cee7cd5c--------------------------------)
    ·9 min read·Sep 18, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5fa2cee7cd5c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-ways-to-improve-the-performance-of-retrieval-augmented-generation-systems-5fa2cee7cd5c&user=Matt+Ambrogi&userId=1e23ad8f92c5&source=-----5fa2cee7cd5c---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1e23ad8f92c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-ways-to-improve-the-performance-of-retrieval-augmented-generation-systems-5fa2cee7cd5c&user=Matt+Ambrogi&userId=1e23ad8f92c5&source=post_page-1e23ad8f92c5----5fa2cee7cd5c---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5fa2cee7cd5c--------------------------------)
    ·9分钟阅读·2023年9月18日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5fa2cee7cd5c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-ways-to-improve-the-performance-of-retrieval-augmented-generation-systems-5fa2cee7cd5c&user=Matt+Ambrogi&userId=1e23ad8f92c5&source=-----5fa2cee7cd5c---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5fa2cee7cd5c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-ways-to-improve-the-performance-of-retrieval-augmented-generation-systems-5fa2cee7cd5c&source=-----5fa2cee7cd5c---------------------bookmark_footer-----------)![](../Images/abcb15689ece03425fe79b2d191b0ae0.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5fa2cee7cd5c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-ways-to-improve-the-performance-of-retrieval-augmented-generation-systems-5fa2cee7cd5c&source=-----5fa2cee7cd5c---------------------bookmark_footer-----------)![](../Images/abcb15689ece03425fe79b2d191b0ae0.png)'
- en: The Quick-start Guide Isn’t Enough
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速入门指南并不够
- en: “Retrieval augmented generation is the process of supplementing a user’s input
    to a large language model (LLM) like ChatGPT with additional information that
    you (the system) have *retrieved* from somewhere else. The LLM can then use that
    information to *augment* the response that it *generates*.” — Cory Zue
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “检索增强生成是补充用户对大型语言模型（LLM）如ChatGPT的输入，加入你（系统）从其他地方*检索*到的额外信息的过程。LLM然后可以利用这些信息来*增强*它所*生成*的响应。”
    — Cory Zue
- en: LLMs are an amazing invention, prone to one key issue. They make stuff up. RAG
    makes LLMs far more useful by giving them factual context to use while answering
    queries.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs是一个了不起的发明，但存在一个关键问题。它们会编造内容。RAG通过为LLMs提供事实背景，使其在回答查询时更有用。
- en: Using the quick-start guide to a framework like LangChain or LlamaIndex, anyone
    can build a simple RAG system, like a chatbot for your docs, with about five lines
    of code.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LangChain 或 LlamaIndex 等框架的快速入门指南，任何人都可以用大约五行代码构建一个简单的 RAG 系统，比如一个文档聊天机器人。
- en: But, the bot built with those five lines of code isn’t going to work very well.
    RAG is easy to prototype, but very hard to productionize — i.e. get to a point
    where users would be happy with it. A basic tutorial might get RAG working at
    80%. But bridging the next 20% often takes some serious experimentation. Best
    practices are yet to be ironed out and can vary depending on use case. But figuring
    out the best practices is well worth our time, because RAG is probably the single
    most effective way to use LLMs.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，用那五行代码构建的机器人效果不会很好。RAG 易于原型设计，但很难投入生产——即达到用户满意的程度。一个基础教程可能使 RAG 达到 80% 的效果，但弥补剩下的
    20% 通常需要一些认真的实验。最佳实践尚未确定，并且可能因使用场景而异。但是，弄清楚最佳实践是非常值得我们花时间的，因为 RAG 可能是使用 LLMs 的最有效方式。
- en: 'This post will survey strategies for improving the quality of RAG systems.
    It’s tailored for those building with RAG who want to bridge the gap between basic
    setups and production-level performance. For the purposes of this post, improving
    means increasing the proportion of queries for which the system: 1\. Finds the
    proper context and 2\. Generates and appropriate response. I will assume the reader
    already has an understanding of how RAG works. If not, I’d suggest reading [this
    article](https://scriv.ai/guides/retrieval-augmented-generation-overview/) by
    Cory Zue for a good introduction. It will also assume some basic familiarity with
    the common frameworks used to build these tools: [LangChain](https://www.langchain.com/)
    and [LlamaIndex](https://gpt-index.readthedocs.io/en/stable/getting_started/starter_example.html).
    However the ideas discussed here are framework-agnostic.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将探讨提高 RAG 系统质量的策略。它专为那些使用 RAG 构建系统、希望弥补基础设置与生产级性能之间差距的读者量身定制。就本文而言，提高意味着增加系统处理的查询比例，其中系统：1.
    查找适当的上下文 2. 生成适当的响应。我将假设读者已经理解 RAG 的工作原理。如果没有，我建议阅读 [这篇文章](https://scriv.ai/guides/retrieval-augmented-generation-overview/)
    作为一个很好的介绍。还假设读者对用于构建这些工具的常见框架有一些基本了解：[LangChain](https://www.langchain.com/) 和
    [LlamaIndex](https://gpt-index.readthedocs.io/en/stable/getting_started/starter_example.html)。然而，本文讨论的思想与框架无关。
- en: I won’t dive into the details of exactly how to implement each strategy I cover,
    but rather I will try to give an idea of when and why it might useful. Given how
    fast the space is moving, it is impossible to provide and exhaustive, or perfectly
    up to date, list of best practices. Instead, I aim to outline some things you
    might consider and try when attempting to improve your retrieval augmented generation
    application.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会深入探讨每种策略的具体实施细节，而是会尝试给出何时以及为何它可能有用的想法。鉴于这一领域的快速发展，提供一个详尽或完全最新的最佳实践清单是不可能的。相反，我旨在概述一些你在尝试改进检索增强生成应用程序时可以考虑和尝试的内容。
- en: 10 Ways to Improve the Performance of Retrieval Augmented Generation
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提高检索增强生成性能的 10 种方法
- en: '**1\. Clean your data.**'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**1. 清理你的数据。**'
- en: RAG connects the capabilities of an LLM to your data. If your data is confusing,
    in substance or layout, then your system will suffer. If you’re using data with
    conflicting or redundant information, your retrieval will struggle to find the
    right context. And when it does, the generation step performed by the LLM may
    be suboptimal. Say you’re building a chatbot for your startup’s help docs and
    you find it is not working well. The first thing you should take a look at is
    the data you are feeding into the system. Are topics broken out logically? Are
    topics covered in one place or many separate places? If you, as a human, can’t
    easily tell which document you would need to look at to answer common queries,
    your retrieval system won’t be able to either.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 将 LLM 的能力连接到你的数据上。如果你的数据在实质上或布局上令人困惑，那么你的系统将会受挫。如果你使用的数据包含冲突或冗余的信息，你的检索将难以找到正确的上下文。当它找到时，LLM
    执行的生成步骤可能会不尽如人意。假设你正在为你的创业公司帮助文档构建一个聊天机器人，你发现它的效果不好。你首先应该查看的是你输入系统的数据。主题是否按逻辑分解？主题是否集中在一个地方还是多个分开的位置？如果你作为一个人，不能轻松判断需要查看哪个文档来回答常见问题，你的检索系统也无法做到。
- en: This process can be as simple as manually combining documents on the same topic,
    but you can take it further. One of the more creative approaches I’ve seen is
    to use the LLM to create summaries of all the documents provided as context. The
    retrieval step can then first run a search over these summaries, and dive into
    the details only when necessary. Some framework even have this as a built in abstraction.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程可以像手动合并同一主题的文档一样简单，但你可以更进一步。我见过的一个更具创意的方法是使用LLM创建所有提供的文档的摘要。检索步骤可以首先在这些摘要上进行搜索，仅在必要时才深入详细信息。一些框架甚至将其作为内置抽象。
- en: '**2\. Explore different index types.**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 探索不同的索引类型。**'
- en: The index is the core pillar of LlamaIndex and LangChain. It is the object that
    holds your retrieval system. The standard approach to RAG involves embeddings
    and similarity search. Chunk up the context data, embed everything, when a query
    comes, find similar pieces from the context. This works very well, but isn’t the
    best approach for every use case. Will queries relate to specific items, such
    as products in an e-commerce store? You may want to explore key-word based search.
    It doesn’t have to be one or the other, many applications use a hybrid. For example,
    you may use a key-word based index for queries relating to a specific product,
    but rely on embeddings for general customer support.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 索引是LlamaIndex和LangChain的核心支柱。它是承载检索系统的对象。RAG的标准方法涉及嵌入和相似性搜索。将上下文数据分块，嵌入所有内容，当有查询时，从上下文中找到相似的部分。这种方法效果很好，但并非所有用例的最佳方法。查询是否与特定项目有关，例如电子商务商店中的产品？你可能需要探索基于关键词的搜索。它不一定是非此即彼，许多应用程序使用混合方法。例如，你可以对特定产品的查询使用基于关键词的索引，但对一般客户支持则依赖嵌入。
- en: '**3\. Experiment with your chunking approach.**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. 尝试不同的分块方法。**'
- en: Chunking up the context data is a core part of building a RAG system. Frameworks
    abstract away the chunking process and allow you to get away without thinking
    about it. But you should think about it. Chunk size matters. You should explore
    what works best for your application. In general, smaller chunks often improve
    retrieval but may cause generation to suffer from a lack of surrounding context.
    There are a lot of ways you can approach chunking. The one thing that doesn’t
    work is approaching it blindly. [This post from PineCone](https://www.pinecone.io/learn/chunking-strategies/)
    lays out some strategies to consider. I have a test set of questions. I approached
    this by [running an experiment](https://www.mattambrogi.com/posts/chunk-size-matters/).
    I looped through each set one time with a small, medium, and large chunk size
    and found small to be best.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 分块上下文数据是构建RAG系统的核心部分。框架抽象了分块过程，让你无需考虑。但你应该考虑。分块大小很重要。你应该探索哪种方式最适合你的应用程序。一般来说，更小的块通常会改善检索，但可能会导致生成缺乏周围上下文。有很多方法可以处理分块。唯一不适用的是盲目处理。[这篇来自PineCone的文章](https://www.pinecone.io/learn/chunking-strategies/)列出了一些策略。我有一组测试问题。我通过[运行实验](https://www.mattambrogi.com/posts/chunk-size-matters/)来处理这些问题。我用小、中、大块大小循环了一遍，发现小块效果最好。
- en: '**4\. Play around with your base prompt.**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**4\. 玩转你的基础提示。**'
- en: 'One example of a base prompt used in LlamaIndex is:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在LlamaIndex中使用的一个基础提示的例子是：
- en: '*‘Context information is below. Given the context information and not prior
    knowledge, answer the query.’*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*“上下文信息如下。根据上下文信息而非先前知识，回答查询。”*'
- en: 'You can overwrite this and experiment with other options. You can even hack
    the RAG such that you do allow the LLM to rely on its own knowledge if it can’t
    find a good answer in the context. You may also adjust the prompt to help steer
    the types of queries it accepts, for example, instructing it to respond a certain
    way for subjective questions. At a minimum it’s helpful to overwrite the prompt
    such that the LLM has context on what jobs it’s doing. For example:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以覆盖这个设置并尝试其他选项。你甚至可以破解RAG，使其在找不到好的答案时允许LLM依赖自己的知识。你还可以调整提示来帮助引导它接受的查询类型，例如，指示它对主观问题以某种方式回应。至少，覆盖提示是有帮助的，这样LLM可以了解自己正在做的工作。例如：
- en: '*‘You are a customer support agent. You are designed to be as helpful as possible
    while providing only factual information. You should be friendly, but not overly
    chatty. Context information is below. Given the context information and not prior
    knowledge, answer the query.’*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*“你是一个客户支持代表。你旨在尽可能提供有用的信息，同时只提供事实信息。你应该友好，但不要过于多话。上下文信息如下。根据上下文信息而非先前知识，回答查询。”*'
- en: '**5\. Try meta-data filtering.**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**5\. 尝试元数据过滤。**'
- en: 'A very effective strategy for improving retrieval is to add meta-data to your
    chunks, and then use it to help process results. Date is a common meta-data tag
    to add because it allows you to [filter by recency](https://www.mattambrogi.com/posts/recency-for-chatbots/).
    Imagine you are building an app that allows users to query their email history.
    It’s likely that more recent emails will be more relevant. But we don’t know that
    they’ll be the most similar, from an embedding standpoint, to the user’s query.
    This brings up a general concept to keep in mind when building RAG: similar ≠
    relevant. You can append the date of each email to its meta-data and then then
    prioritize most recent context during retrieval. LlamaIndex has a built in class
    of Node Post-Processors that help with exactly this.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 提高检索效果的一个非常有效的策略是为你的块添加元数据，然后利用它来帮助处理结果。日期是一个常见的元数据标签，因为它允许你进行[按时间过滤](https://www.mattambrogi.com/posts/recency-for-chatbots/)。假设你正在构建一个允许用户查询其电子邮件历史的应用程序。较新的电子邮件可能会更相关。但从嵌入角度来看，我们不能确定它们与用户查询的相似性最高。这提出了一个在构建RAG时需要记住的普遍概念：相似性
    ≠ 相关性。你可以将每封电子邮件的日期附加到其元数据中，然后在检索时优先考虑最新的上下文。LlamaIndex有一个内置的Node Post-Processors类，专门帮助处理这个问题。
- en: '**6\. Use query routing.**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**6\. 使用查询路由。**'
- en: It’s often useful to have more than one index. You then route queries to the
    appropriate index when they come in. For example, you may have one index that
    handles summarization questions, another that handles pointed questions, and another
    that works well for date sensitive questions. If you try to optimize one index
    for all of these behaviors, you’ll end up compromising how well it does at all
    of them. Instead you can route the query to the proper index. Another use case
    would be to direct some queries to a key-word based index as discussed in section
    2.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通常拥有多个索引是很有用的。然后在查询到达时，将它们路由到适当的索引。例如，你可能会有一个处理总结性问题的索引，一个处理指向性问题的索引，还有一个适用于与日期相关的问题的索引。如果你尝试优化一个索引以应对所有这些行为，你会发现它在所有这些行为上的表现都将有所折中。相反，你可以将查询路由到合适的索引。另一种用例是将一些查询定向到基于关键词的索引，如第2节所述。
- en: Once you have constructed you indexes, you just have to define in text what
    each should be used for. Then at query time, the LLM will choose the appropriate
    option. Both LlamaIndex and LangChain have tools for this.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你构建了你的索引，你只需要在文本中定义每个索引的用途。然后在查询时，LLM将选择合适的选项。LlamaIndex和LangChain都提供了这样的工具。
- en: '**7\. Look into reranking.**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**7\. 关注重新排名。**'
- en: Reranking is one solution to the issue of discrepancy between similarity and
    relevance. With reranking, your retrieval system gets the top nodes for context
    as usual. It then re-ranks them based on relevance. [Cohere Rereanker](https://docs.cohere.com/docs/reranking)
    is commonly used for this. This strategy is one I see experts recommend often.
    No matter the use case, if you’re building with RAG, you should experiment with
    reranking and see if it improves your system. Both LangChain and LlamaIndex have
    abstractions that make it easy to set up.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 重新排名是解决相似性与相关性之间差异问题的一种方法。通过重新排名，你的检索系统会像往常一样获取顶级节点作为上下文。然后它会根据相关性对这些节点进行重新排序。[Cohere
    Rereanker](https://docs.cohere.com/docs/reranking)通常用于此策略。这是我看到专家们经常推荐的策略。无论用例是什么，如果你在使用RAG，你应该尝试重新排名，看看它是否能改善你的系统。LangChain和LlamaIndex都提供了简化设置的抽象。
- en: '**8\. Consider query transformations.**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**8\. 考虑查询转换。**'
- en: 'You already alter your user’s query by placing it within your base prompt.
    It can make sense to alter it even further. Here are a few examples:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经通过将用户的查询放入基本提示中来改变用户的查询。进一步改变它可能是有意义的。这里有几个例子：
- en: 'Rephrasing: if your system doesn’t find relevant context for the query, you
    can have the LLM rephrase the query and try again. Two questions that seem the
    same to humans don’t always look that similar in embedding space.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 重新表述：如果你的系统没有找到查询的相关上下文，你可以让LLM重新表述查询并再次尝试。对人类来说看似相同的两个问题，在嵌入空间中不总是看起来那么相似。
- en: 'HyDE: [HyDE](http://boston.lti.cs.cmu.edu/luyug/HyDE/HyDE.pdf) is a strategy
    which takes a query, generates a hypothetical response, and then uses both for
    embedding look up. Researches have found this can dramatically improve performance.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 'HyDE: [HyDE](http://boston.lti.cs.cmu.edu/luyug/HyDE/HyDE.pdf)是一种策略，它会获取一个查询，生成一个假设响应，然后使用这两者进行嵌入查找。研究发现，这可以显著提高性能。'
- en: 'Sub-queries: LLMs tend to work better when they break down complex queries.
    You can build this into your RAG system such that a query is decomposed into multiple
    questions.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 子查询：LLM 在处理复杂查询时通常表现更好。你可以将这种功能集成到你的 RAG 系统中，以便将查询分解为多个问题。
- en: LLamaIndex has docs covering these types of [query transformations](https://gpt-index.readthedocs.io/en/v0.6.9/how_to/query/query_transformations.html).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: LLamaIndex 有关于这些类型的 [查询转换](https://gpt-index.readthedocs.io/en/v0.6.9/how_to/query/query_transformations.html)
    的文档。
- en: '**9\. Fine-tune your embedding model.**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**9\. 微调你的嵌入模型。**'
- en: Embedding based similarity is the standard retrieval mechanism for RAG. Your
    data is broken up and embedded inside the index. When a query comes in, it is
    also embedded for comparison against the embedding in the index. But what is doig
    the embedding? Usually, a pre-trained model such as OpenAI’ text-*embedding*-*ada*-*002.*
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 基于嵌入的相似性是 RAG 的标准检索机制。你的数据被分解并嵌入到索引中。当查询到来时，它也会被嵌入以便与索引中的嵌入进行比较。但是，是什么在进行嵌入呢？通常是一个预训练模型，如
    OpenAI 的 text-*embedding*-*ada*-*002*。
- en: The issue is, the pre-trained model’s concept of what is similar in embedding
    space may not align very well with what is similar in your context. Imagine you
    are working with legal documents. You would like your embedding to base its judgement
    of similarity more on your domain specific terms like “intellectual property”
    or “breach of contract” and less on general terms like “hereby” and “agreement.”
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，预训练模型对嵌入空间中的相似性的理解可能与你实际上下文中的相似性不太一致。假设你在处理法律文件。你希望你的嵌入模型更多地依据你的领域特定术语，如“知识产权”或“合同违约”，而不是像“特此”和“协议”这样的通用术语。
- en: You can fine-tune you embedding model to resolve this issue. Doing so can boost
    your retrieval metrics by 5–10%. This requires a bit more effort, but can make
    a significant difference in your retrieval performance. The process is easier
    than you might think, as LlamaIndex can help you generate a training set. For
    more information, you can check out [this post](https://medium.com/llamaindex-blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971)
    by Jerry Liu on how LlamaIndex approaches fine-tuning embeddings, or [this post](https://betterprogramming.pub/fine-tuning-your-embedding-model-to-maximize-relevance-retrieval-in-rag-pipeline-2ea3fa231149)
    which walks through the process of fine-tuning.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以微调你的嵌入模型来解决这个问题。这样做可以提升你的检索指标 5%–10%。这需要更多的努力，但可以显著改善你的检索性能。这个过程比你想象的要简单，LlamaIndex
    可以帮助你生成训练集。有关更多信息，你可以查看 Jerry Liu 关于 LlamaIndex 如何进行嵌入微调的 [这篇文章](https://medium.com/llamaindex-blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971)，或
    [这篇文章](https://betterprogramming.pub/fine-tuning-your-embedding-model-to-maximize-relevance-retrieval-in-rag-pipeline-2ea3fa231149)，其中详细介绍了微调的过程。
- en: '**10\. Start using LLM dev tools.**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**10\. 开始使用 LLM 开发工具。**'
- en: You’re likely already using LlamaIndex or LangChain to build your system. Both
    frameworks have helpful debugging tools which allow you to define callbacks, see
    what context is used, what document your retrieval comes from, and more.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经在使用 LlamaIndex 或 LangChain 来构建你的系统。这两个框架都有有用的调试工具，允许你定义回调，查看使用了什么上下文，检索来自哪个文档等等。
- en: If you’re finding that the tools built into these frameworks are lacking, there
    is a growing ecosystem of tools which can you help you dive into the inner working
    of your RAG system. Arize AI has an [in-notebook tool](https://arize.com/resource/arizeobserve-introducing-phoenix-ml-observability-in-your-notebook/)
    that allows you to explore how which context is being retrieved and why. [Rivet](https://news.ycombinator.com/item?id=37433218)
    is a tool which provides a visual interface for helping your build complex agents.
    It was just open-sourced by the legal technology company Ironclad. New tools are
    constantly being released and it’s worth experimenting to see which are helpful
    in your workflow.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你发现这些框架内置的工具不够完善，那么有一个不断扩展的工具生态系统可以帮助你深入了解你的RAG系统的内部工作。Arize AI 提供了一个 [notebook
    内工具](https://arize.com/resource/arizeobserve-introducing-phoenix-ml-observability-in-your-notebook/)，允许你探索上下文是如何被检索的以及为什么。
    [Rivet](https://news.ycombinator.com/item?id=37433218) 是一个提供可视化界面的工具，帮助你构建复杂的代理。它刚刚由法律科技公司
    Ironclad 开源。新工具不断推出，值得尝试看看哪些工具对你的工作流程有帮助。
- en: Conclusion
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Building with RAG can be frustrating because it’s so easy to get working and
    so hard to get working well. I hope the strategies above can provide some inspiration
    for how you might bridge the gap. No one of these ideas works all the time and
    the process is one of experimentation, trial, and error. I didn’t dive into evaluation,
    how you can measure the performance of your system, in this post. Evaluation is
    more of an art than a science at the moment, but it’s important to set some type
    of system up that you can consistently check in on. This is the only way to tell
    if the changes you are implementing make a difference. I wrote about [how to evaluate
    RAG](https://www.mattambrogi.com/posts/evaluating-chatbots/) system previously.
    For more information, you can explore [LlamaIndex Evals](https://gpt-index.readthedocs.io/en/v0.7.2/how_to/evaluation/evaluation.html),
    [LangChain Evals](https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/),
    and a really promising new framework called [RAGAS](https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 RAG 构建系统可能令人沮丧，因为它很容易上手，但要做到完美却非常困难。我希望以上策略能为你提供一些灵感，帮助你弥合这一差距。这些想法中的任何一个都不是全能的，整个过程充满了实验、尝试和错误。我在这篇文章中没有深入探讨评估，即如何衡量系统的性能。目前，评估更像是一门艺术而非科学，但设立某种可以持续检查的系统是很重要的。这是判断你所实施的变化是否有效的唯一方法。我之前写过关于[如何评估
    RAG](https://www.mattambrogi.com/posts/evaluating-chatbots/)系统的文章。欲了解更多信息，你可以探索[LlamaIndex
    Evals](https://gpt-index.readthedocs.io/en/v0.7.2/how_to/evaluation/evaluation.html)、[LangChain
    Evals](https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/)和一个非常有前景的新框架，[RAGAS](https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/)。
