- en: Streaming in Data Engineering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ•°æ®å·¥ç¨‹ä¸­çš„æµæ•°æ®
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/streaming-in-data-engineering-2bb2b9b3b603](https://towardsdatascience.com/streaming-in-data-engineering-2bb2b9b3b603)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/streaming-in-data-engineering-2bb2b9b3b603](https://towardsdatascience.com/streaming-in-data-engineering-2bb2b9b3b603)
- en: Streaming data pipelines and real-time analytics
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æµæ•°æ®ç®¡é“å’Œå®æ—¶åˆ†æ
- en: '[](https://mshakhomirov.medium.com/?source=post_page-----2bb2b9b3b603--------------------------------)[![ğŸ’¡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----2bb2b9b3b603--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2bb2b9b3b603--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2bb2b9b3b603--------------------------------)
    [ğŸ’¡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----2bb2b9b3b603--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mshakhomirov.medium.com/?source=post_page-----2bb2b9b3b603--------------------------------)[![ğŸ’¡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----2bb2b9b3b603--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2bb2b9b3b603--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2bb2b9b3b603--------------------------------)
    [ğŸ’¡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----2bb2b9b3b603--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2bb2b9b3b603--------------------------------)
    Â·9 min readÂ·Dec 12, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----2bb2b9b3b603--------------------------------)
    Â·é˜…è¯»æ—¶é—´9åˆ†é’ŸÂ·2023å¹´12æœˆ12æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/b5bdd62c71b5b0a4888786ad3318772d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5bdd62c71b5b0a4888786ad3318772d.png)'
- en: Photo by [DESIGNECOLOGIST](https://unsplash.com/@designecologist?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±[DESIGNECOLOGIST](https://unsplash.com/@designecologist?utm_source=medium&utm_medium=referral)æä¾›ï¼Œåœ¨[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)ä¸Š
- en: '**Streaming** is one of the most popular data pipeline design patterns. Using
    an event as a single data point creates a constant flow of data from one point
    to another enabling an opportunity for real-time data ingestion and analytics.
    If you want to familiarise yourself with data streaming and learn how to build
    real-time data pipelines this story is for you. Learn how to test the solution,
    and mock test data to simulate event streams. This article is a great opportunity
    to acquire some sought-after data engineering skills working with popular streaming
    tools and frameworks, i.e. Kinesis, Kafka and Spark. I would like to speak about
    the benefits, examples, and use cases of Data Streaming.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**æµæ•°æ®**æ˜¯æœ€å—æ¬¢è¿çš„æ•°æ®ç®¡é“è®¾è®¡æ¨¡å¼ä¹‹ä¸€ã€‚å°†äº‹ä»¶ä½œä¸ºå•ä¸ªæ•°æ®ç‚¹åˆ›å»ºäº†ä»ä¸€ä¸ªç‚¹åˆ°å¦ä¸€ä¸ªç‚¹çš„æŒç»­æ•°æ®æµï¼Œä»è€Œä¸ºå®æ—¶æ•°æ®æ‘„å–å’Œåˆ†ææä¾›äº†æœºä¼šã€‚å¦‚æœä½ æƒ³äº†è§£æ•°æ®æµå¹¶å­¦ä¹ å¦‚ä½•æ„å»ºå®æ—¶æ•°æ®ç®¡é“ï¼Œè¿™ç¯‡æ–‡ç« é€‚åˆä½ ã€‚äº†è§£å¦‚ä½•æµ‹è¯•è§£å†³æ–¹æ¡ˆï¼Œå¹¶æ¨¡æ‹Ÿäº‹ä»¶æµçš„æµ‹è¯•æ•°æ®ã€‚è¿™ç¯‡æ–‡ç« æ˜¯ä¸€ä¸ªç»ä½³çš„æœºä¼šï¼Œè®©ä½ æŒæ¡ä¸€äº›å—æ¬¢è¿çš„æ•°æ®å·¥ç¨‹æŠ€èƒ½ï¼Œä½¿ç”¨æµè¡Œçš„æµå¤„ç†å·¥å…·å’Œæ¡†æ¶ï¼Œå³Kinesisã€Kafkaå’ŒSparkã€‚æˆ‘æƒ³è°ˆè°ˆæ•°æ®æµçš„å¥½å¤„ã€ç¤ºä¾‹å’Œç”¨ä¾‹ã€‚'
- en: What exactly is data streaming?
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°æ®æµç©¶ç«Ÿæ˜¯ä»€ä¹ˆï¼Ÿ
- en: Streaming data, also known as event stream processing, is a data pipeline design
    pattern when data points flow constantly from the source to the destination. It
    can be processed in real-time, enabling real-time analytics capabilities to act
    on data streams and analytics events super fast. Applications can trigger immediate
    responses to new data events thanks to stream processing and typically it would
    be one of the most popular solutions to process the data on an enterprise level.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æµæ•°æ®ï¼Œä¹Ÿç§°ä¸ºäº‹ä»¶æµå¤„ç†ï¼Œæ˜¯ä¸€ç§æ•°æ®ç®¡é“è®¾è®¡æ¨¡å¼ï¼Œå½“æ•°æ®ç‚¹ä¸æ–­ä»æºå¤´æµå‘ç›®çš„åœ°æ—¶ä½¿ç”¨ã€‚è¿™å¯ä»¥å®æ—¶å¤„ç†ï¼Œä½¿å®æ—¶åˆ†æåŠŸèƒ½èƒ½å¤Ÿå¿«é€Ÿå¯¹æ•°æ®æµå’Œåˆ†æäº‹ä»¶ä½œå‡ºååº”ã€‚ç”±äºæµå¤„ç†ï¼Œåº”ç”¨ç¨‹åºå¯ä»¥å¯¹æ–°æ•°æ®äº‹ä»¶è§¦å‘å³æ—¶å“åº”ï¼Œé€šå¸¸è¿™å°†æ˜¯å¤„ç†ä¼ä¸šçº§æ•°æ®æœ€å—æ¬¢è¿çš„è§£å†³æ–¹æ¡ˆä¹‹ä¸€ã€‚
- en: There is a data pipeline whenever there is data processing between points A
    and B [1].
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åªè¦åœ¨ç‚¹Aå’Œç‚¹Bä¹‹é—´è¿›è¡Œæ•°æ®å¤„ç†ï¼Œå°±ä¼šæœ‰ä¸€ä¸ªæ•°æ®ç®¡é“ [1]ã€‚
- en: '![](../Images/334d567596ab164146151fe1c0d7a31c.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/334d567596ab164146151fe1c0d7a31c.png)'
- en: Streaming data pipeline example. Image by author
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æµæ•°æ®ç®¡é“ç¤ºä¾‹ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: In this example, we can create an **ELT streaming** data pipeline to **AWS Redshift**.
    AWS **Firehose delivery stream** can offer this type of seamless integration when
    it creates a data feed directly into the data warehouse table. Then data will
    be transformed to create reports with **AWS Quicksight** as a BI tool.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ª**ELTæµå¤„ç†**æ•°æ®ç®¡é“åˆ°**AWS Redshift**ã€‚AWS **Firehose delivery stream**å¯ä»¥æä¾›è¿™ç§æ— ç¼é›†æˆï¼Œå°†æ•°æ®ç›´æ¥åˆ›å»ºåˆ°æ•°æ®ä»“åº“è¡¨ä¸­ã€‚ç„¶åï¼Œæ•°æ®å°†è¢«è½¬åŒ–ï¼Œä»¥ä½¿ç”¨**AWS
    Quicksight**ä½œä¸ºBIå·¥å…·ç”ŸæˆæŠ¥å‘Šã€‚
- en: Letâ€™s imagine we need to create a reporting dashboard to display revenue streams
    in our company. In many scenarios, a business requirement is to generate insights
    in real-time. This is exactly the case when we would want to use **streaming**.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªæŠ¥å‘Šä»ªè¡¨ç›˜æ¥å±•ç¤ºå…¬å¸ä¸­çš„æ”¶å…¥æ¥æºã€‚åœ¨è®¸å¤šåœºæ™¯ä¸­ï¼Œä¸šåŠ¡éœ€æ±‚æ˜¯å®æ—¶ç”Ÿæˆæ´å¯Ÿã€‚è¿™æ­£æ˜¯æˆ‘ä»¬éœ€è¦ä½¿ç”¨**æµå¤„ç†**çš„æƒ…å†µã€‚
- en: Data streams can be generated by various data sources, i.e. IoT, server data
    streams, marketing in-app events, user activity, payment transactions, etc. This
    data can flow in different formats and often vary in volume. The idea of the streaming
    pattern is to apply ETL on the go and process event streams seamlessly.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®æµå¯ä»¥ç”±å„ç§æ•°æ®æºç”Ÿæˆï¼Œä¾‹å¦‚ç‰©è”ç½‘ã€æœåŠ¡å™¨æ•°æ®æµã€è¥é”€åº”ç”¨å†…äº‹ä»¶ã€ç”¨æˆ·æ´»åŠ¨ã€æ”¯ä»˜äº¤æ˜“ç­‰ã€‚è¿™äº›æ•°æ®å¯ä»¥ä»¥ä¸åŒæ ¼å¼æµåŠ¨ï¼Œå¹¶ä¸”ç»å¸¸å˜åŒ–ã€‚æµå¤„ç†æ¨¡å¼çš„ç†å¿µæ˜¯å®æ—¶åº”ç”¨ETLå¹¶æ— ç¼å¤„ç†äº‹ä»¶æµã€‚
- en: Whenever we need to act on up-to-the-millisecond data latency streaming is the
    right way to go.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯å½“æˆ‘ä»¬éœ€è¦å¤„ç†æ¯«ç§’çº§çš„æ•°æ®å»¶è¿Ÿæ—¶ï¼Œæµå¤„ç†æ˜¯æ­£ç¡®çš„é€‰æ‹©ã€‚
- en: Consider this example below to better understand it. All applications use OLTP
    databases [4], MySQL for example. Your app is one of those but you need this data
    in the data warehouse solution (DWH), i.e. Snowflake or BigQuery.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘ä¸‹é¢çš„ä¾‹å­ä»¥æ›´å¥½åœ°ç†è§£å®ƒã€‚æ‰€æœ‰åº”ç”¨ç¨‹åºéƒ½ä½¿ç”¨OLTPæ•°æ®åº“[4]ï¼Œä¾‹å¦‚MySQLã€‚ä½ çš„åº”ç”¨ç¨‹åºä¹Ÿæ˜¯å…¶ä¸­ä¹‹ä¸€ï¼Œä½†ä½ éœ€è¦å°†è¿™äº›æ•°æ®å­˜å‚¨åˆ°æ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆï¼ˆDWHï¼‰ï¼Œå³Snowflakeæˆ–BigQueryã€‚
- en: '[](/data-modelling-for-data-engineers-93d058efa302?source=post_page-----2bb2b9b3b603--------------------------------)
    [## Data Modelling For Data Engineers'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[## æ•°æ®å»ºæ¨¡ä¸ºæ•°æ®å·¥ç¨‹å¸ˆ](https://example.org/data-modelling-for-data-engineers-93d058efa302?source=post_page-----2bb2b9b3b603--------------------------------)'
- en: The definitive guide for beginners
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åˆå­¦è€…çš„ç»ˆææŒ‡å—
- en: towardsdatascience.com](/data-modelling-for-data-engineers-93d058efa302?source=post_page-----2bb2b9b3b603--------------------------------)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](https://example.org/data-modelling-for-data-engineers-93d058efa302?source=post_page-----2bb2b9b3b603--------------------------------)'
- en: Using a batch data pipeline solution we would want to load from MySQL to DWH
    once a day/hour/every five minutes, etc. Stream opposite to that would use a dedicated
    system, such as Kafka Connect for example. It will process data as soon as it
    lands in our database.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ‰¹é‡æ•°æ®ç®¡é“è§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬å¯èƒ½å¸Œæœ›ä»MySQLåŠ è½½åˆ°DWHä¸€æ¬¡/æ¯å¤©/æ¯å°æ—¶/æ¯äº”åˆ†é’Ÿç­‰ã€‚æµå¤„ç†åˆ™ç›¸åï¼Œå®ƒå°†ä½¿ç”¨ä¸“ç”¨ç³»ç»Ÿï¼Œä¾‹å¦‚Kafka Connectã€‚å®ƒä¼šåœ¨æ•°æ®è¿›å…¥æ•°æ®åº“æ—¶ç«‹å³å¤„ç†æ•°æ®ã€‚
- en: Popular data streaming tools
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æµè¡Œçš„æ•°æ®æµå·¥å…·
- en: Letâ€™s take a look into popular data streaming platforms and frameworks that
    proved themselves most useful over the last couple of years.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ·±å…¥äº†è§£è¿‡å»å‡ å¹´ä¸­è¢«è¯æ˜æœ€æœ‰ç”¨çš„æµæ•°æ®å¹³å°å’Œæ¡†æ¶ã€‚
- en: '**Apache Spark** â€” frameworks for distributed data computing for large-scale
    analytics and complex data transformations'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Spark** â€” ç”¨äºå¤§è§„æ¨¡åˆ†æå’Œå¤æ‚æ•°æ®è½¬æ¢çš„åˆ†å¸ƒå¼æ•°æ®è®¡ç®—æ¡†æ¶'
- en: '**Apache Kafka** â€” a real-time data pipeline tool with a distributed messaging
    system for apps'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Kafka** â€” ä¸€ä¸ªå®æ—¶æ•°æ®ç®¡é“å·¥å…·ï¼Œå…·æœ‰åˆ†å¸ƒå¼æ¶ˆæ¯ç³»ç»Ÿç”¨äºåº”ç”¨ç¨‹åº'
- en: '**AWS Kinesis** â€” a real-time streaming platform for analytics and applications'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS Kinesis** â€” ä¸€ä¸ªç”¨äºåˆ†æå’Œåº”ç”¨ç¨‹åºçš„å®æ—¶æµå¹³å°'
- en: '**Google Cloud Dataflow** â€” Googleâ€™s streaming platform for real-time event
    processing and analytics pipelines'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google Cloud Dataflow** â€” è°·æ­Œçš„å®æ—¶äº‹ä»¶å¤„ç†å’Œåˆ†æç®¡é“çš„æµå¹³å°'
- en: '**Apache Flink** â€” a distributed streaming data platform designed for low-latency
    data processing.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Flink** â€” ä¸€ä¸ªåˆ†å¸ƒå¼æµæ•°æ®å¹³å°ï¼Œæ—¨åœ¨è¿›è¡Œä½å»¶è¿Ÿæ•°æ®å¤„ç†ã€‚'
- en: Almost all of them have their managed cloud-based services (AWs Kinesis, Google
    Cloud Dataflow) and can be seamlessly integrated with other services such as storage
    (S3), queuing (SQS, pub/sub), data warehouses (Redshift) or AI platforms.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å‡ ä¹æ‰€æœ‰è¿™äº›å¹³å°éƒ½æœ‰å®ƒä»¬çš„æ‰˜ç®¡äº‘æœåŠ¡ï¼ˆå¦‚AWS Kinesisã€Google Cloud Dataflowï¼‰ï¼Œå¹¶ä¸”å¯ä»¥ä¸å…¶ä»–æœåŠ¡ï¼ˆå¦‚å­˜å‚¨ï¼ˆS3ï¼‰ã€é˜Ÿåˆ—ï¼ˆSQSã€pub/subï¼‰ã€æ•°æ®ä»“åº“ï¼ˆRedshiftï¼‰æˆ–AIå¹³å°ï¼‰æ— ç¼é›†æˆã€‚
- en: All of them can be deployed on Kubernetes, Docker or Hadoop and aim to solve
    one problem â€” dealing with high-volume and high-velocity data streams.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è¿™äº›å·¥å…·éƒ½å¯ä»¥éƒ¨ç½²åœ¨Kubernetesã€Dockeræˆ–Hadoopä¸Šï¼Œæ—¨åœ¨è§£å†³ä¸€ä¸ªé—®é¢˜â€”â€”å¤„ç†é«˜å®¹é‡å’Œé«˜é€Ÿåº¦çš„æ•°æ®æµã€‚
- en: Benefits of data streaming
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°æ®æµçš„å¥½å¤„
- en: Streaming data pipeline design patterns helps organisations to proactively mitigate
    the impact of adverse business events related to delay in data processing, i.e.
    various losses and outages, customer churn and financial downturns. Because of
    the complexity of todayâ€™s business needs, conventional **batch data processing**
    is a â€˜No Goâ€™ solution, as it can only process data as groups of transactions accumulated
    over time.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æµæ•°æ®ç®¡é“è®¾è®¡æ¨¡å¼å¸®åŠ©ç»„ç»‡ä¸»åŠ¨å‡è½»ä¸æ•°æ®å¤„ç†å»¶è¿Ÿç›¸å…³çš„ä¸åˆ©ä¸šåŠ¡äº‹ä»¶çš„å½±å“ï¼Œä¾‹å¦‚å„ç§æŸå¤±å’Œä¸­æ–­ã€å®¢æˆ·æµå¤±å’Œè´¢åŠ¡è¡°é€€ã€‚ç”±äºä»Šå¤©ä¸šåŠ¡éœ€æ±‚çš„å¤æ‚æ€§ï¼Œä¼ ç»Ÿçš„**æ‰¹å¤„ç†æ•°æ®å¤„ç†**æ˜¯â€˜ä¸å¯è¡Œâ€™çš„è§£å†³æ–¹æ¡ˆï¼Œå› ä¸ºå®ƒåªèƒ½å¤„ç†ç´¯ç§¯æ—¶é—´çš„äº‹åŠ¡æ•°æ®ç»„ã€‚
- en: 'So here are some business advantages of using data streaming:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½¿ç”¨æ•°æ®æµçš„ä¸šåŠ¡ä¼˜åŠ¿å¦‚ä¸‹ï¼š
- en: '**Increase in customer satisfaction** and as a result increased retention rates'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æé«˜å®¢æˆ·æ»¡æ„åº¦**ï¼Œè¿›è€Œæé«˜å®¢æˆ·ä¿ç•™ç‡'
- en: '**Reduced operational losses** as it can give real-time insights on system
    outages and breaches'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å‡å°‘æ“ä½œæŸå¤±**ï¼Œå› ä¸ºå®ƒå¯ä»¥æä¾›å…³äºç³»ç»Ÿä¸­æ–­å’Œæ¼æ´çš„å®æ—¶æ´å¯Ÿã€‚'
- en: '**Increased return on investment** as companies can now act faster on business
    data with increased responsiveness to customer needs and market trends.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æŠ•èµ„å›æŠ¥ç‡æé«˜**ï¼Œå› ä¸ºå…¬å¸ç°åœ¨å¯ä»¥æ›´å¿«åœ°å¯¹ä¸šåŠ¡æ•°æ®åšå‡ºååº”ï¼Œå¯¹å®¢æˆ·éœ€æ±‚å’Œå¸‚åœºè¶‹åŠ¿çš„å“åº”èƒ½åŠ›æé«˜ã€‚'
- en: The main **technical advantage** is in data processing as it runs event processing
    strictly ***one by one***. Opposite to batch processing, it has a better fault
    tolerance and if one event in the pipeline canâ€™t be processed due to some reason
    then it is only this event. In the batch pipeline, the whole chunk of data processing
    will fail because of this single data point which might have a wrong schema or
    incorrect data format.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸»è¦çš„**æŠ€æœ¯ä¼˜åŠ¿**åœ¨äºæ•°æ®å¤„ç†ï¼Œå› ä¸ºå®ƒä»¥ä¸¥æ ¼çš„***é€ä¸ªå¤„ç†***æ–¹å¼è¿è¡Œäº‹ä»¶å¤„ç†ã€‚ä¸æ‰¹å¤„ç†å¤„ç†ç›¸æ¯”ï¼Œå®ƒå…·æœ‰æ›´å¥½çš„æ•…éšœå®¹å¿æ€§ï¼Œå¦‚æœç®¡é“ä¸­çš„ä¸€ä¸ªäº‹ä»¶å› æŸäº›åŸå› æ— æ³•å¤„ç†ï¼Œé‚£ä¹ˆåªæœ‰è¿™ä¸ªäº‹ä»¶ä¼šå—åˆ°å½±å“ã€‚åœ¨æ‰¹å¤„ç†ç®¡é“ä¸­ï¼Œç”±äºå•ä¸ªæ•°æ®ç‚¹å¯èƒ½å…·æœ‰é”™è¯¯çš„æ¨¡å¼æˆ–æ•°æ®æ ¼å¼ï¼Œæ•´ä¸ªæ•°æ®å¤„ç†å—ä¼šå› æ­¤å¤±è´¥ã€‚
- en: The main disadvantage of streaming data pipelines is the cost
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æµæ•°æ®ç®¡é“çš„ä¸»è¦ç¼ºç‚¹æ˜¯æˆæœ¬
- en: Each time our stream processor hits the endpoint it will require compute power.
    Typically streaming data processing will result in higher costs related to this
    particular data pipeline.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯æ¬¡æˆ‘ä»¬çš„æµå¤„ç†å™¨è¾¾åˆ°ç«¯ç‚¹æ—¶ï¼Œå®ƒéƒ½éœ€è¦è®¡ç®—èƒ½åŠ›ã€‚é€šå¸¸ï¼Œæµæ•°æ®å¤„ç†ä¼šå¯¼è‡´ä¸ç‰¹å®šæ•°æ®ç®¡é“ç›¸å…³çš„æ›´é«˜æˆæœ¬ã€‚
- en: Challenges in building streaming data pipelines
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ„å»ºæµæ•°æ®ç®¡é“çš„æŒ‘æˆ˜
- en: '**Fault Tolerance** â€” Can we design and build a data platform that handles
    data processing failures from a single point of data event? Very often data comes
    from different data sources. It might be coming even in different formats. Data
    availability and durability becomes important consideration while designing the
    data platform [3] with streaming component.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ•…éšœå®¹å¿æ€§** â€” æˆ‘ä»¬èƒ½å¦è®¾è®¡å’Œæ„å»ºä¸€ä¸ªèƒ½å¤Ÿå¤„ç†å•ä¸€æ•°æ®äº‹ä»¶å¤„ç†å¤±è´¥çš„æ•°æ®å¹³å°ï¼Ÿæ•°æ®é€šå¸¸æ¥è‡ªä¸åŒçš„æ•°æ®æºï¼Œç”šè‡³å¯èƒ½ä»¥ä¸åŒçš„æ ¼å¼å‡ºç°ã€‚åœ¨è®¾è®¡å¸¦æœ‰æµç»„ä»¶çš„æ•°æ®å¹³å°æ—¶ï¼Œæ•°æ®çš„å¯ç”¨æ€§å’ŒæŒä¹…æ€§æˆä¸ºé‡è¦çš„è€ƒè™‘å› ç´ [3]ã€‚'
- en: '[](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----2bb2b9b3b603--------------------------------)
    [## Data Platform Architecture Types'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[## æ•°æ®å¹³å°æ¶æ„ç±»å‹](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----2bb2b9b3b603--------------------------------)'
- en: How well does it answer your business needs? Dilemma of a choice.
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å®ƒèƒ½å¤šå¥½åœ°æ»¡è¶³ä½ çš„ä¸šåŠ¡éœ€æ±‚ï¼Ÿé€‰æ‹©çš„å›°å¢ƒã€‚
- en: towardsdatascience.com](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----2bb2b9b3b603--------------------------------)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----2bb2b9b3b603--------------------------------)'
- en: '**Queuing and ordering** â€” Events in the data stream must be ordered correctly.
    Otherwise, data processing might fail. Indeed, in-app messaging will not make
    sense if ordered incorrectly, for example.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ’é˜Ÿå’Œæ’åº** â€” æ•°æ®æµä¸­çš„äº‹ä»¶å¿…é¡»æ­£ç¡®æ’åºã€‚å¦åˆ™ï¼Œæ•°æ®å¤„ç†å¯èƒ½ä¼šå¤±è´¥ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ’åºä¸æ­£ç¡®ï¼Œåº”ç”¨å†…æ¶ˆæ¯å°†æ²¡æœ‰æ„ä¹‰ã€‚'
- en: '**Scalability** â€” Applications scale. It is as simple as that. Designing a
    data pipeline that responds well to an increased number of events coming from
    the source is not a trivial task. Being able to add more resources and data processing
    capacity to our data pipeline is a crucial component of a robust data platform.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¯æ‰©å±•æ€§** â€” åº”ç”¨ç¨‹åºéœ€è¦æ‰©å±•ã€‚å°±è¿™ä¹ˆç®€å•ã€‚è®¾è®¡ä¸€ä¸ªèƒ½å¤Ÿå¾ˆå¥½åº”å¯¹æ¥è‡ªæºçš„äº‹ä»¶æ•°é‡å¢åŠ çš„æ•°æ®ç®¡é“å¹¶éæ˜“äº‹ã€‚èƒ½å¤Ÿä¸ºæ•°æ®ç®¡é“æ·»åŠ æ›´å¤šèµ„æºå’Œæ•°æ®å¤„ç†èƒ½åŠ›æ˜¯ä¸€ä¸ªå¼ºå¤§æ•°æ®å¹³å°çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚'
- en: '**Data consistency â€”** Often in distributed data platforms data is being processed
    in parallel. This might become a challenge as data in one data processor could
    already be modified and become stale in another one.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ•°æ®ä¸€è‡´æ€§ â€”** åœ¨åˆ†å¸ƒå¼æ•°æ®å¹³å°ä¸­ï¼Œæ•°æ®ç»å¸¸æ˜¯å¹¶è¡Œå¤„ç†çš„ã€‚è¿™å¯èƒ½ä¼šæˆä¸ºæŒ‘æˆ˜ï¼Œå› ä¸ºåœ¨ä¸€ä¸ªæ•°æ®å¤„ç†å™¨ä¸­æ•°æ®å¯èƒ½å·²ç»è¢«ä¿®æ”¹ï¼Œè€Œåœ¨å¦ä¸€ä¸ªå¤„ç†å™¨ä¸­å˜å¾—é™ˆæ—§ã€‚'
- en: A real-world example
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç°å®ä¸–ç•Œä¸­çš„ä¸€ä¸ªä¾‹å­
- en: Letâ€™s take a look at this example of a streaming data pipeline built with AWS
    Kinesis and Redshift.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹è¿™ä¸ªä½¿ç”¨ AWS Kinesis å’Œ Redshift æ„å»ºçš„æµæ•°æ®ç®¡é“ç¤ºä¾‹ã€‚
- en: '![](../Images/17b2472521ad2fec136aa2de06456924.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17b2472521ad2fec136aa2de06456924.png)'
- en: Example pipeline. Image by author
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ç®¡é“ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: Amazon Kinesis Data Firehose is an ETL service that collects, transforms, and
    distributes streaming data to data lakes, data storage, and analytics services
    with high reliability.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Kinesis Data Firehose æ˜¯ä¸€ä¸ª ETL æœåŠ¡ï¼Œå¯ä»¥é«˜å¯é æ€§åœ°æ”¶é›†ã€è½¬æ¢å¹¶åˆ†å‘æµæ•°æ®åˆ°æ•°æ®æ¹–ã€æ•°æ®å­˜å‚¨å’Œåˆ†ææœåŠ¡ã€‚
- en: We can use it to stream data into Amazon S3 and convert it to the formats needed
    for analysis without having to develop processing pipelines. It is also great
    for Machine learning (ML) pipelines where models are used to examine data and
    forecast inference endpoints as streams flow to their destination.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ç”¨å®ƒå°†æ•°æ®æµä¼ è¾“åˆ° Amazon S3ï¼Œå¹¶å°†æ•°æ®è½¬æ¢ä¸ºåˆ†ææ‰€éœ€çš„æ ¼å¼ï¼Œæ— éœ€å¼€å‘å¤„ç†ç®¡é“ã€‚å®ƒå¯¹äºæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ç®¡é“ä¹Ÿéå¸¸é€‚åˆï¼Œå…¶ä¸­æ¨¡å‹ç”¨äºæ£€æŸ¥æ•°æ®å¹¶é¢„æµ‹æ¨æ–­ç«¯ç‚¹ï¼Œå› ä¸ºæ•°æ®æµå‘å…¶ç›®æ ‡ã€‚
- en: '**Kinesis Data Streams vs Kinesis Data Firehose**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kinesis Data Streams ä¸ Kinesis Data Firehose**'
- en: Kinesis Data Streams is primarily focused on consuming and storing data streams.
    Kinesis Data Firehose is designed to deliver data streams to specific destinations.
    Both can consume data streams, but which one to use depends on where we want our
    streaming data to go.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Kinesis Data Streams ä¸»è¦å…³æ³¨äºæ¶ˆè´¹å’Œå­˜å‚¨æ•°æ®æµã€‚Kinesis Data Firehose æ—¨åœ¨å°†æ•°æ®æµä¼ é€’åˆ°ç‰¹å®šçš„ç›®æ ‡ã€‚ä¸¤è€…éƒ½å¯ä»¥æ¶ˆè´¹æ•°æ®æµï¼Œä½†ä½¿ç”¨å“ªä¸ªå–å†³äºæˆ‘ä»¬å¸Œæœ›æ•°æ®æµå»å¾€ä½•å¤„ã€‚
- en: AWS Kinesis Data Firehose allows us to redirect data streams into AWS data storage.
    Kinesis Data Firehose is the most straightforward method for gathering, processing,
    and loading data streams into AWS data storage.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Kinesis Data Firehose å…è®¸æˆ‘ä»¬å°†æ•°æ®æµé‡å®šå‘åˆ° AWS æ•°æ®å­˜å‚¨ã€‚Kinesis Data Firehose æ˜¯æ”¶é›†ã€å¤„ç†å’ŒåŠ è½½æ•°æ®æµåˆ°
    AWS æ•°æ®å­˜å‚¨çš„æœ€ç›´æ¥æ–¹æ³•ã€‚
- en: Amazon Kinesis Data Firehose supports batch operations, encryption, and compression
    of streaming data, as well as automated scalability in the terabytes per second
    range. Firehose can seamlessly integrate with S3 data lakes, RedShift data warehouse
    solutions or ElasticSearch service.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Kinesis Data Firehose æ”¯æŒæ‰¹å¤„ç†æ“ä½œã€åŠ å¯†å’Œæµæ•°æ®å‹ç¼©ï¼Œä»¥åŠè‡ªåŠ¨åŒ–çš„æ¯ç§’ TB çº§åˆ«çš„å¯æ‰©å±•æ€§ã€‚Firehose å¯ä»¥æ— ç¼é›†æˆ
    S3 æ•°æ®æ¹–ã€RedShift æ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆæˆ– ElasticSearch æœåŠ¡ã€‚
- en: AWS Kinesis Data Streams is an Amazon Kinesis real-time data streaming solution
    with exceptional scalability and durability where data streams are available 24/7
    for any consumer. It makes it more expensive than the Kinesis Data Firehose.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Kinesis Data Streams æ˜¯ä¸€ä¸ª Amazon Kinesis å®æ—¶æ•°æ®æµè§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰å“è¶Šçš„å¯æ‰©å±•æ€§å’Œè€ç”¨æ€§ï¼Œæ•°æ®æµå…¨å¤©å€™ 24/7
    å¯ç”¨ç»™ä»»ä½•æ¶ˆè´¹è€…ã€‚è¿™ä½¿å¾—å®ƒæ¯” Kinesis Data Firehose æ›´æ˜‚è´µã€‚
- en: '**How to create a Firehose resource using AWS Cloudformation**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¦‚ä½•ä½¿ç”¨ AWS CloudFormation åˆ›å»º Firehose èµ„æº**'
- en: Consider this CloudFormation template below. It deploys the required resources
    including the Firehose we need.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æŸ¥çœ‹ä¸‹é¢çš„ CloudFormation æ¨¡æ¿ã€‚å®ƒéƒ¨ç½²äº†åŒ…æ‹¬æˆ‘ä»¬éœ€è¦çš„ Firehose åœ¨å†…çš„æ‰€æœ‰å¿…è¦èµ„æºã€‚
- en: '[PRE0]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'It can be deployed in AWS using the AWS CLI tool. We need to run this on our
    command line (replace with unique bucket names in your account):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨ AWS CLI å·¥å…·åœ¨ AWS ä¸­éƒ¨ç½²å®ƒã€‚æˆ‘ä»¬éœ€è¦åœ¨å‘½ä»¤è¡Œä¸­è¿è¡Œè¿™ä¸ªï¼ˆåœ¨ä½ çš„è´¦æˆ·ä¸­æ›¿æ¢ä¸ºå”¯ä¸€çš„å­˜å‚¨æ¡¶åç§°ï¼‰ï¼š
- en: '[PRE1]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Our shell script would look like this:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ shell è„šæœ¬å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/515379a7556b2ccb17c34b3d84a59dd7.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/515379a7556b2ccb17c34b3d84a59dd7.png)'
- en: Firehose resources created. Image by author
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å·²åˆ›å»º Firehose èµ„æºã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: 'Now we would want to create an event producer. We can do it in Python and the
    code for our `app.py` would look like this:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªäº‹ä»¶ç”Ÿäº§è€…ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Python å®Œæˆè¿™ä¸ªæ“ä½œï¼Œ`app.py` çš„ä»£ç å¦‚ä¸‹ï¼š
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`put_record_batch` method writes many data records into a delivery stream in
    a single call, allowing for better throughput per producer than single record
    writing. `PutRecord` is used to write single data records into a delivery stream.
    It is up to you which one to choose in this tutorial.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`put_record_batch` æ–¹æ³•å¯ä»¥åœ¨ä¸€æ¬¡è°ƒç”¨ä¸­å°†å¤šä¸ªæ•°æ®è®°å½•å†™å…¥äº¤ä»˜æµï¼Œè¿™æ¯”å•æ¡è®°å½•å†™å…¥æ–¹å¼èƒ½æä¾›æ›´å¥½çš„æ¯ç”Ÿäº§è€…ååé‡ã€‚`PutRecord`
    ç”¨äºå°†å•æ¡æ•°æ®è®°å½•å†™å…¥äº¤ä»˜æµã€‚åœ¨æœ¬æ•™ç¨‹ä¸­é€‰æ‹©å“ªä¸ªæ–¹æ³•ç”±ä½ å†³å®šã€‚'
- en: We can generate some synthetic data in our `app.py` using this helper function
    below.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥åœ¨ `app.py` ä¸­ä½¿ç”¨ä¸‹é¢çš„è¾…åŠ©å‡½æ•°ç”Ÿæˆä¸€äº›åˆæˆæ•°æ®ã€‚
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now this data can be send to our event producer like so:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è¿™äº›æ•°æ®å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å‘é€åˆ°æˆ‘ä»¬çš„äº‹ä»¶ç”Ÿäº§è€…ï¼š
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Done! We have created a simple streaming data pipeline that outputs aggregated
    results into cloud storage (AWS S3).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæˆï¼æˆ‘ä»¬å·²ç»åˆ›å»ºäº†ä¸€ä¸ªç®€å•çš„æµæ•°æ®ç®¡é“ï¼Œå°†æ±‡æ€»ç»“æœè¾“å‡ºåˆ°äº‘å­˜å‚¨ï¼ˆAWS S3ï¼‰ã€‚
- en: 'Run `python app.py` in your command line:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‘½ä»¤è¡Œä¸­è¿è¡Œ `python app.py`ï¼š
- en: '![](../Images/53af7f1ce72757790dce09aa4c3f438e.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/53af7f1ce72757790dce09aa4c3f438e.png)'
- en: Events connector example. Image by author
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: äº‹ä»¶è¿æ¥å™¨ç¤ºä¾‹ã€‚ä½œè€…æä¾›çš„å›¾ç‰‡
- en: Check my tutorial below for a more advanced data pipeline example [2]
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹ä¸‹é¢çš„æ•™ç¨‹ï¼Œäº†è§£æ›´é«˜çº§çš„æ•°æ®ç®¡é“ç¤ºä¾‹ [2]
- en: '[](/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2?source=post_page-----2bb2b9b3b603--------------------------------)
    [## Building a Streaming Data Pipeline with Redshift Serverless and Kinesis'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2?source=post_page-----2bb2b9b3b603--------------------------------)
    [## ä½¿ç”¨ Redshift Serverless å’Œ Kinesis æ„å»ºæµæ•°æ®ç®¡é“'
- en: An End-To-End Tutorial for Beginners
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é¢å‘åˆå­¦è€…çš„ç«¯åˆ°ç«¯æ•™ç¨‹
- en: towardsdatascience.com](/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2?source=post_page-----2bb2b9b3b603--------------------------------)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2?source=post_page-----2bb2b9b3b603--------------------------------)
- en: Conclusion
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: The ideal streaming data platform for your project doesnâ€™t exist. The streaming
    design has its benefits but also we can see some obvious challenges while using
    it. Which streaming tool to choose is not an easy choice. It depends on your business
    goals and functional data requirements. You would want to try and compare multiple
    streaming platforms based on characteristics such as functionality, performance,
    cost, simplicity of use, and compatibility. Is it going to be a machine-learning
    pipeline? Do we need to work with partitions, windows and joins? Do we need high
    throughput, fault tolerance or low latency?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: é¡¹ç›®ç†æƒ³çš„æµæ•°æ®å¹³å°å¹¶ä¸å­˜åœ¨ã€‚æµè®¾è®¡æœ‰å…¶å¥½å¤„ï¼Œä½†åœ¨ä½¿ç”¨æ—¶ä¹Ÿä¼šé‡åˆ°ä¸€äº›æ˜æ˜¾çš„æŒ‘æˆ˜ã€‚é€‰æ‹©å“ªä¸ªæµå·¥å…·ä¸æ˜¯ä¸€ä¸ªå®¹æ˜“çš„å†³å®šã€‚è¿™å–å†³äºä½ çš„ä¸šåŠ¡ç›®æ ‡å’ŒåŠŸèƒ½æ•°æ®éœ€æ±‚ã€‚ä½ å¯èƒ½éœ€è¦å°è¯•å¹¶æ¯”è¾ƒå¤šä¸ªæµå¹³å°ï¼Œè€ƒè™‘åŠŸèƒ½ã€æ€§èƒ½ã€æˆæœ¬ã€æ˜“ç”¨æ€§å’Œå…¼å®¹æ€§ç­‰ç‰¹å¾ã€‚å®ƒä¼šæ˜¯ä¸€ä¸ªæœºå™¨å­¦ä¹ ç®¡é“å—ï¼Ÿæˆ‘ä»¬éœ€è¦å¤„ç†åˆ†åŒºã€çª—å£å’Œè¿æ¥å—ï¼Ÿæˆ‘ä»¬éœ€è¦é«˜ååé‡ã€å®¹é”™æ€§è¿˜æ˜¯ä½å»¶è¿Ÿï¼Ÿ
- en: Different streaming frameworks have different capabilities, for example, Kafka
    has a handy **session** **library** which can be easily integrated into your analytics
    pipeline.
  id: totrans-86
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä¸åŒçš„æµæ¡†æ¶å…·æœ‰ä¸åŒçš„èƒ½åŠ›ï¼Œä¾‹å¦‚ï¼ŒKafka æœ‰ä¸€ä¸ªæ–¹ä¾¿çš„**ä¼šè¯** **åº“**ï¼Œå¯ä»¥å¾ˆå®¹æ˜“åœ°é›†æˆåˆ°ä½ çš„åˆ†æç®¡é“ä¸­ã€‚
- en: What frequency of data delivery and consumption do we need in our pipeline?
    Is it going to be a delivery into a DWH solution or into a data lake? Some platforms
    can offer better integration features than others.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç®¡é“éœ€è¦ä»€ä¹ˆé¢‘ç‡çš„æ•°æ®ä¼ è¾“å’Œæ¶ˆè´¹ï¼Ÿå®ƒå°†äº¤ä»˜åˆ°æ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆè¿˜æ˜¯æ•°æ®æ¹–ä¸­ï¼Ÿä¸€äº›å¹³å°æ¯”å…¶ä»–å¹³å°æä¾›æ›´å¥½çš„é›†æˆåŠŸèƒ½ã€‚
- en: Another essential element to consider is the type and **complexity of data processing**
    and analysis that must be performed on your streaming data.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªé‡è¦çš„è€ƒè™‘å› ç´ æ˜¯å¿…é¡»å¯¹æµæ•°æ®è¿›è¡Œçš„**æ•°æ®å¤„ç†**å’Œåˆ†æçš„ç±»å‹å’Œ**å¤æ‚æ€§**ã€‚
- en: I would recommend creating a prototype based on your own data pipeline scenario
    and requirements gathered from the main stakeholders inside the company. The optimal
    streaming data pipeline would be the one that adds value to the business and also
    meets your data engineering goals.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å»ºè®®æ ¹æ®ä½ è‡ªå·±æ•°æ®ç®¡é“åœºæ™¯å’Œå…¬å¸ä¸»è¦åˆ©ç›Šç›¸å…³è€…æ”¶é›†çš„éœ€æ±‚æ¥åˆ›å»ºä¸€ä¸ªåŸå‹ã€‚æœ€ç»ˆçš„æµæ•°æ®ç®¡é“åº”è¯¥æ˜¯èƒ½å¤Ÿä¸ºä¸šåŠ¡å¢å€¼å¹¶æ»¡è¶³ä½ çš„æ•°æ®å·¥ç¨‹ç›®æ ‡çš„ã€‚
- en: 'Recommended read:'
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨èé˜…è¯»ï¼š
- en: '[1] [https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3](/data-pipeline-design-patterns-100afa4b93e3)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3](/data-pipeline-design-patterns-100afa4b93e3)'
- en: '[2] [https://towardsdatascience.com/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2](/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [https://towardsdatascience.com/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2](/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2)'
- en: '[3] [https://medium.com/towards-data-science/data-platform-architecture-types-f255ac6e0b7](https://medium.com/towards-data-science/data-platform-architecture-types-f255ac6e0b7)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [https://medium.com/towards-data-science/data-platform-architecture-types-f255ac6e0b7](https://medium.com/towards-data-science/data-platform-architecture-types-f255ac6e0b7)'
- en: '[4] [https://medium.com/towards-data-science/data-modelling-for-data-engineers-93d058efa302](https://medium.com/towards-data-science/data-modelling-for-data-engineers-93d058efa302)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [https://medium.com/towards-data-science/data-modelling-for-data-engineers-93d058efa302](https://medium.com/towards-data-science/data-modelling-for-data-engineers-93d058efa302)'
