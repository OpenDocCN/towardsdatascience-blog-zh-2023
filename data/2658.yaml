- en: 'Parameter-Efficient Fine-Tuning (PEFT) for LLMs: A Comprehensive Introduction'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对大语言模型（LLMs）进行参数高效微调（PEFT）：全面介绍
- en: 原文：[https://towardsdatascience.com/parameter-efficient-fine-tuning-peft-for-llms-a-comprehensive-introduction-e52d03117f95?source=collection_archive---------1-----------------------#2023-08-22](https://towardsdatascience.com/parameter-efficient-fine-tuning-peft-for-llms-a-comprehensive-introduction-e52d03117f95?source=collection_archive---------1-----------------------#2023-08-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/parameter-efficient-fine-tuning-peft-for-llms-a-comprehensive-introduction-e52d03117f95?source=collection_archive---------1-----------------------#2023-08-22](https://towardsdatascience.com/parameter-efficient-fine-tuning-peft-for-llms-a-comprehensive-introduction-e52d03117f95?source=collection_archive---------1-----------------------#2023-08-22)
- en: A conceptual survey of PEFT methods used by Hugging Face, Google’s Vertex AI,
    and eventually OpenAI
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一项关于Hugging Face、谷歌的Vertex AI以及最终OpenAI使用的PEFT方法的概念调查
- en: '[](https://medium.com/@smsmith714?source=post_page-----e52d03117f95--------------------------------)[![Sean
    Smith](../Images/611395d113b10ec4bbfaf781301139c7.png)](https://medium.com/@smsmith714?source=post_page-----e52d03117f95--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e52d03117f95--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e52d03117f95--------------------------------)
    [Sean Smith](https://medium.com/@smsmith714?source=post_page-----e52d03117f95--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@smsmith714?source=post_page-----e52d03117f95--------------------------------)[![Sean
    Smith](../Images/611395d113b10ec4bbfaf781301139c7.png)](https://medium.com/@smsmith714?source=post_page-----e52d03117f95--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e52d03117f95--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e52d03117f95--------------------------------)
    [Sean Smith](https://medium.com/@smsmith714?source=post_page-----e52d03117f95--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6957f6523097&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fparameter-efficient-fine-tuning-peft-for-llms-a-comprehensive-introduction-e52d03117f95&user=Sean+Smith&userId=6957f6523097&source=post_page-6957f6523097----e52d03117f95---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e52d03117f95--------------------------------)
    ·19 min read·Aug 22, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe52d03117f95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fparameter-efficient-fine-tuning-peft-for-llms-a-comprehensive-introduction-e52d03117f95&user=Sean+Smith&userId=6957f6523097&source=-----e52d03117f95---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6957f6523097&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fparameter-efficient-fine-tuning-peft-for-llms-a-comprehensive-introduction-e52d03117f95&user=Sean+Smith&userId=6957f6523097&source=post_page-6957f6523097----e52d03117f95---------------------post_header-----------)
    发表在 [数据科学前沿](https://towardsdatascience.com/?source=post_page-----e52d03117f95--------------------------------)
    ·19分钟阅读·2023年8月22日'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe52d03117f95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fparameter-efficient-fine-tuning-peft-for-llms-a-comprehensive-introduction-e52d03117f95&source=-----e52d03117f95---------------------bookmark_footer-----------)![](../Images/425a0569c2f36a07f176e6742754e25f.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe52d03117f95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fparameter-efficient-fine-tuning-peft-for-llms-a-comprehensive-introduction-e52d03117f95&source=-----e52d03117f95---------------------bookmark_footer-----------)![](../Images/425a0569c2f36a07f176e6742754e25f.png)'
- en: Image created by DALL-E. A Sunday Afternoon on the Island of La Grande Jatte
    but everyone is a humanoid.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由DALL-E创建的图像。《大岛上的一个星期日下午》，但每个人都是类人。
- en: Large Language Models (LLMs) are quite large by name. These models usually have
    anywhere from 7 to 70 billion parameters. To load a 70 billion parameter model
    in full precision would require 280 GB of GPU memory! To train that model you
    would update billions of tokens over millions or billions of documents. The computation
    required is substantial for updating those parameters. The self-supervised training
    of these models is expensive, [costing companies up to $100 million](https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的名字中含有“大型”二字。这些模型通常拥有从70亿到700亿不等的参数。要完全加载一个700亿参数的模型需要280 GB的GPU内存！训练这个模型时，你需要在数百万或数十亿个文档中更新数十亿个标记。更新这些参数所需的计算量是巨大的。这些模型的自监督训练非常昂贵，[公司花费高达1亿美元](https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/)。
- en: For the rest of us, there is significant interest in adapting our data to these
    models. With our limited datasets (in comparison) and lacking computing power,
    how do we create models that can improve on the major players at a fraction of
    the cost?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们其他人来说，如何将我们的数据适应这些模型是一个重要的问题。面对有限的数据集（相比之下）和不足的计算能力，我们如何在更低成本的情况下创建可以超越主要玩家的模型？
- en: This is where the research field of Parameter-Efficient Fine-Tuning (PEFT) comes
    into play. Through various techniques, which we will soon explore in detail, we
    can augment small sections of these models so they are better suited to the tasks
    we aim to complete.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是参数高效微调（PEFT）研究领域发挥作用的地方。通过各种技术，我们将很快详细探讨这些技术，我们可以增强这些模型的小部分，使其更好地适应我们要完成的任务。
- en: After reading this article, you will conceptually grasp each PEFT technique
    applied in Hugging Face and…
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完这篇文章后，你将对Hugging Face中应用的每种PEFT技术有一个概念上的把握，并且……
