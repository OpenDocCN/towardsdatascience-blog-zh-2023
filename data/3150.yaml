- en: How to Optimize Object Detection Models for Specific Domains
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何优化特定领域的目标检测模型
- en: 原文：[https://towardsdatascience.com/how-to-optimize-object-detection-models-for-specific-domains-d8512c63a9c3?source=collection_archive---------3-----------------------#2023-10-19](https://towardsdatascience.com/how-to-optimize-object-detection-models-for-specific-domains-d8512c63a9c3?source=collection_archive---------3-----------------------#2023-10-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-optimize-object-detection-models-for-specific-domains-d8512c63a9c3?source=collection_archive---------3-----------------------#2023-10-19](https://towardsdatascience.com/how-to-optimize-object-detection-models-for-specific-domains-d8512c63a9c3?source=collection_archive---------3-----------------------#2023-10-19)
- en: Design better and faster models to solve your specific problem
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计更好更快的模型以解决你的特定问题
- en: '[](https://alvaroleandrocavalcante.medium.com/?source=post_page-----d8512c63a9c3--------------------------------)[![Alvaro
    Leandro Cavalcante Carneiro](../Images/d23fc10840c906a43ae002185a100e40.png)](https://alvaroleandrocavalcante.medium.com/?source=post_page-----d8512c63a9c3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d8512c63a9c3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d8512c63a9c3--------------------------------)
    [Alvaro Leandro Cavalcante Carneiro](https://alvaroleandrocavalcante.medium.com/?source=post_page-----d8512c63a9c3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://alvaroleandrocavalcante.medium.com/?source=post_page-----d8512c63a9c3--------------------------------)[![Alvaro
    Leandro Cavalcante Carneiro](../Images/d23fc10840c906a43ae002185a100e40.png)](https://alvaroleandrocavalcante.medium.com/?source=post_page-----d8512c63a9c3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d8512c63a9c3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d8512c63a9c3--------------------------------)
    [Alvaro Leandro Cavalcante Carneiro](https://alvaroleandrocavalcante.medium.com/?source=post_page-----d8512c63a9c3--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F948868831251&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-optimize-object-detection-models-for-specific-domains-d8512c63a9c3&user=Alvaro+Leandro+Cavalcante+Carneiro&userId=948868831251&source=post_page-948868831251----d8512c63a9c3---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d8512c63a9c3--------------------------------)
    ·13 min read·Oct 19, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd8512c63a9c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-optimize-object-detection-models-for-specific-domains-d8512c63a9c3&user=Alvaro+Leandro+Cavalcante+Carneiro&userId=948868831251&source=-----d8512c63a9c3---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F948868831251&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-optimize-object-detection-models-for-specific-domains-d8512c63a9c3&user=Alvaro+Leandro+Cavalcante+Carneiro&userId=948868831251&source=post_page-948868831251----d8512c63a9c3---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d8512c63a9c3--------------------------------)
    ·13 min read·2023年10月19日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd8512c63a9c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-optimize-object-detection-models-for-specific-domains-d8512c63a9c3&user=Alvaro+Leandro+Cavalcante+Carneiro&userId=948868831251&source=-----d8512c63a9c3---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd8512c63a9c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-optimize-object-detection-models-for-specific-domains-d8512c63a9c3&source=-----d8512c63a9c3---------------------bookmark_footer-----------)![](../Images/2db005eef3f79d4f5f0e360dbbcd6ec6.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd8512c63a9c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-optimize-object-detection-models-for-specific-domains-d8512c63a9c3&source=-----d8512c63a9c3---------------------bookmark_footer-----------)![](../Images/2db005eef3f79d4f5f0e360dbbcd6ec6.png)'
- en: Photo by [paolo candelo](https://unsplash.com/@paolocandelo?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [paolo candelo](https://unsplash.com/@paolocandelo?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Object detection is widely employed across different domains, from academia
    to industry sectors, thanks to its ability to provide great results at a low computational
    cost. However, despite the abundance of open-source architectures publicly available,
    most of these models are designed to address general-purpose problems and may
    not be a good fit for specific contexts.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测在从学术界到工业领域的不同领域中被广泛使用，因为它能够以低计算成本提供良好的结果。然而，尽管有许多开源架构可公开获取，大多数模型设计用于解决通用问题，可能不适用于特定的上下文。
- en: As an example, we can mention the [Common Objects in Context](https://arxiv.org/pdf/1405.0312v3.pdf)
    (COCO) dataset, which is typically used as a baseline for research in this field,
    influencing the hyperparameters and architectural details of the models. This
    dataset comprises 90 distinct classes under various lighting conditions, backgrounds,
    and sizes. It turns out that, sometimes, the detection problem you are facing
    is relatively simple. You may want to detect just a few distinct objects without
    many scene or size variations. In this case, if you train your model using a generic
    set of hyperparameters, you would likely end up with a model that incurs unnecessary
    computational costs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，我们可以提到[COCO数据集](https://arxiv.org/pdf/1405.0312v3.pdf)，这是在这一领域中通常作为研究基线的数据集，对模型的超参数和架构细节产生影响。该数据集包含90个不同的类别，涵盖了各种光照条件、背景和大小。事实证明，有时你面临的检测问题相对简单。你可能只需检测几个不同的对象，而没有太多的场景或大小变化。在这种情况下，如果你使用通用的超参数集来训练你的模型，你可能会得到一个产生不必要计算成本的模型。
- en: With this perspective in mind, the primary goal of this article is to provide
    guidance on optimizing various object detection models for less complex tasks.
    I want to assist you in selecting a more efficient configuration that reduces
    computational costs without prejudicing the mean Average Precision (mAP).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个角度来看，本文的主要目标是提供优化各种目标检测模型以处理较简单任务的指导。我希望帮助你选择一个更高效的配置，以减少计算成本而不影响平均精度（mAP）。
- en: Providing some context
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提供一些背景信息
- en: 'One of the goals of my master’s degree was to develop a [sign language recognition
    system](https://www.esann.org/sites/default/files/proceedings/2023/ES2023-185.pdf)
    with minimal computational requirements. A crucial component of this system is
    the preprocessing stage, which involves the detection of the interpreter’s hands
    and face, as depicted in the figure below:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我的硕士学位目标之一是开发一个[手语识别系统](https://www.esann.org/sites/default/files/proceedings/2023/ES2023-185.pdf)，以最小的计算要求为前提。该系统的一个关键组成部分是预处理阶段，包括检测翻译员的手和脸，如下图所示：
- en: '![](../Images/0f1367786a2c8281270592926b136d9d.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f1367786a2c8281270592926b136d9d.png)'
- en: Samples from the HFSL dataset that were created in this work. Image by author.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这是本研究中创建的HFSL数据集的样本。图片作者提供。
- en: As illustrated, this problem is relatively straightforward, involving only two
    distinct classes and three concurrently appearing objects in the image. For this
    reason, my aim was to optimize the models' hyperparameters to maintain a high
    mAP while reducing the computational cost, thus enabling efficient execution on
    edge devices such as smartphones.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，这个问题相对简单，只涉及两类不同的对象和图像中同时出现的三种对象。因此，我的目标是优化模型的超参数，以保持高的mAP，同时减少计算成本，从而使其能够在智能手机等边缘设备上高效执行。
- en: Object detection architectures and setup
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标检测架构和设置
- en: 'In this project, the following object detection architectures were tested:
    **EfficientDetD0, Faster-RCNN, SDD320, SDD640, and YoloV7**. However, the concepts
    presented here can be applied to adapt various other architectures.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，测试了以下目标检测架构：**EfficientDetD0、Faster-RCNN、SDD320、SDD640和YoloV7**。然而，这里提出的概念可以应用于适应各种其他架构。
- en: For model development, I primarily utilized Python 3.8 and the TensorFlow framework,
    with the exception of YoloV7, where PyTorch was employed. While most examples
    provided here relate to TensorFlow, you can adapt these principles to your preferred
    framework.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型开发方面，我主要使用了Python 3.8和TensorFlow框架，除了YoloV7使用了PyTorch。虽然这里提供的大多数示例与TensorFlow相关，但你可以将这些原则适应于你所选择的框架。
- en: In terms of hardware, the testing was conducted using an RTX 3060 GPU and an
    Intel Core i5–10400 CPU. All the source code and models are available on [GitHub](https://github.com/AlvaroCavalcante/hand-face-detector).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在硬件方面，测试使用了RTX 3060 GPU和Intel Core i5–10400 CPU。所有源代码和模型都可以在[GitHub](https://github.com/AlvaroCavalcante/hand-face-detector)上找到。
- en: Fine-tuning of object detectors
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标检测器的微调
- en: When using TensorFlow for object detection, it’s essential to understand that
    all the hyperparameters are stored in a file named “[**pipeline.config**](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/configuring_jobs.md)”.
    This protobuf file holds the configurations used to train and evaluate the model,
    and you’ll find it in any pre-trained model downloaded from [TF Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md),
    for instance. In this context, I will describe the modifications I’ve implemented
    in the pipeline files to optimize the object detectors.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 进行目标检测时，了解所有超参数都存储在一个名为“[**pipeline.config**](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/configuring_jobs.md)”的文件中是至关重要的。这个
    protobuf 文件保存了用于训练和评估模型的配置，你可以在任何从[TF 模型库](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)下载的预训练模型中找到它。在这个背景下，我将描述我在管道文件中实施的修改，以优化目标检测器。
- en: It’s important to note that the hyperparameters provided here were specifically
    designed for hand and face detection (2 classes, 3 objects). Be sure to adapt
    them for your own problem domain.
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 需要注意的是，这里提供的超参数是专门为手部和面部检测（2 类，3 对象）设计的。务必根据你自己的问题领域进行调整。
- en: General simplifications
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一般简化
- en: 'The first change that can applied to all models is reducing the maximum number
    of predictions per class and the number of generated bounding boxes from 100 to
    2 and 4, respectively. You can achieve this by adjusting the “**max_number_of_boxes**”
    property inside the “**train_config**” object:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有模型可以应用的第一个变化是将每个类别的最大预测数量和生成的边界框数量从100分别减少到2和4。你可以通过调整“**max_number_of_boxes**”属性来实现这一点，该属性位于“**train_config**”对象中：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After that, change the “**max_total_detections**” and the “**max_detections_per_class**”
    that are inside the “**post_processing”** of the object detector:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，更改“**max_total_detections**” 和 “**max_detections_per_class**”，它们位于目标检测器的“**post_processing**”中：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Those changes are important, especially in my case, as there are only three
    objects and two classes appearing in the image simultaneously. By decreasing the
    number of predictions, fewer iterations are required to eliminate overlapping
    bounding boxes through Non-maximum Suppression (NMS). Therefore, if you have a
    limited number of classes to detect and objects appearing in the scene, it could
    be a good idea to change this hyperparameter.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这些变化很重要，特别是对我来说，因为图像中同时出现的对象只有三个且类别只有两个。通过减少预测次数，消除重叠边界框所需的迭代次数也减少了，这通过非极大值抑制
    (NMS) 实现。因此，如果你需要检测的类别和场景中的对象数量有限，改变这个超参数可能是个好主意。
- en: Additional adjustments were applied individually, taking into account the specific
    architectural details of each object detection model.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 额外的调整是逐个应用的，考虑了每个目标检测模型的具体架构细节。
- en: Single Shot Multibox Detector (SSD)
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单次检测多框 (SSD)
- en: It’s always a good idea to test different resolutions when working with object
    detection. In this project, I utilized two versions of the model, SSD320 and SSD640,
    with input image resolutions of 320x320 and 640x640 pixels, respectively.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行目标检测时，测试不同的分辨率总是一个好主意。在这个项目中，我使用了两个版本的模型，SSD320和SSD640，分别具有320x320和640x640像素的输入图像分辨率。
- en: 'For both models, one of the primary modifications was to reduce the depth of
    the [**Feature Pyramid Network**](https://jonathan-hui.medium.com/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c)
    **(FPN)** from 5 to 4 by removing the most superficial layer. FPN is a powerful
    feature extraction mechanism that operates on multiple feature map sizes. However,
    for larger objects, the most superficial layer, designed for higher image resolutions,
    might not be necessary. That said, if the objects that you are trying to detect
    are not too small, it’s probably a good idea to remove this layer. To implement
    this change, adjust the “**min_level**” attribute from 3 to 4 within the “**fpn**”
    object:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这两个模型，主要的修改之一是将[**特征金字塔网络**](https://jonathan-hui.medium.com/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c)
    **(FPN)** 的深度从5减少到4，通过移除最表层。FPN是一个强大的特征提取机制，能够处理多种特征图尺寸。然而，对于较大的对象，最表层是为较高图像分辨率设计的，可能并不必要。也就是说，如果你要检测的对象不太小，移除这一层可能是个好主意。为了实现这一变化，将“**min_level**”属性从3调整到4，位于“**fpn**”对象中：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'I also simplified the higher-resolution model (SSD640) by reducing the “**additional_layer_depth**”
    from 128 to 108\. Likewise, I adjusted the “**multiscale_anchor_generator**” depth
    from 5 to 4 layers for both models, as shown below:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我还通过将“**additional_layer_depth**”从128减少到108简化了高分辨率模型（SSD640）。同样，我将两个模型的“**multiscale_anchor_generator**”深度从5调整到4层，如下所示：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, the network responsible for generating the bounding box predictions
    (“**box_predictor**”) had the number of layers reduced from 4 to 3\. Regarding
    SSD640, the box predictor depth was also decreased from 128 to 96, as shown below:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，生成边界框预测的网络（“**box_predictor**”）的层数从4减少到3。关于SSD640，边框预测的深度也从128减少到96，如下所示：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: These simplifications were driven by the fact that we have a limited number
    of distinct classes with relatively straightforward patterns to detect. Therefore,
    it’s possible to reduce the number of layers and the depth of the model, since
    even with fewer feature maps we can still effectively extract the desired features
    from the images.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这些简化是因为我们拥有的不同类别数量有限，且模式相对简单。因此，可以减少模型的层数和深度，因为即使特征图较少，我们仍能有效地从图像中提取所需的特征。
- en: EfficinetDet-D0
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: EfficientDet-D0
- en: 'Concerning EfficientDet-D0, I reduced the depth of the [**Bidirectional Feature
    Pyramid Network**](https://paperswithcode.com/method/bifpn) **(**Bi-FPN**)** from
    5 to 4\. Additionally, I decreased the Bi-FPN iterations from 3 to 2 and feature
    map kernels from 64 to 48\. Bi-FPN is a sophisticated technique of multi-scale
    feature fusion, which can yield excellent results. However, it comes at the cost
    of higher computational demands, which can be a waste of resources for simpler
    problems. To implement the aforementioned adjustments, simply update the attributes
    of the “**bifpn**” object as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 关于EfficientDet-D0，我将[**双向特征金字塔网络**](https://paperswithcode.com/method/bifpn)
    **（Bi-FPN）**的深度从5减少到4。此外，我将Bi-FPN的迭代次数从3减少到2，将特征图内核从64减少到48。Bi-FPN是一种复杂的多尺度特征融合技术，可以产生出色的结果。然而，它也需要较高的计算资源，对于简单的问题，这可能会浪费资源。要实施上述调整，只需按如下方式更新“**bifpn**”对象的属性：
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Besides that, it’s also important to reduce the depth of the “**multiscale_anchor_generator**”
    in the same manner as we did with SSD. Lastly, I reduced the layers of the box
    predictor network from 3 to 2:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，还需要像处理SSD时一样，减少“**multiscale_anchor_generator**”的深度。最后，我将边框预测网络的层数从3减少到2：
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Faster R-CNN
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Faster R-CNN
- en: The Faster R-CNN model relies on the [**Region Proposal Network**](https://paperswithcode.com/method/rpn#:~:text=A%20Region%20Proposal%20Network%2C%20or,generate%20high%2Dquality%20region%20proposals.)
    (RPN) and anchor boxes as its primary techniques. Anchors are the central point
    of a sliding window that iterates over the last feature map of the backbone CNN.
    For each iteration, a classifier determines the probability of a proposal containing
    an object, while a regressor adjusts the bounding box coordinates. To ensure the
    detector is translation-invariant, it employs three different scales and three
    aspect ratios for the anchor boxes, which increases the number of proposals per
    iteration.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Faster R-CNN模型依赖于[**区域建议网络**](https://paperswithcode.com/method/rpn#:~:text=A%20Region%20Proposal%20Network%2C%20or,generate%20high%2Dquality%20region%20proposals.)（RPN）和锚框作为其主要技术。锚框是一个滑动窗口的中心点，它在骨干CNN的最后特征图上迭代。每次迭代时，分类器会确定一个建议中是否包含物体的概率，而回归器则调整边界框坐标。为了确保检测器具有平移不变性，它使用三种不同的尺度和三种宽高比的锚框，这增加了每次迭代的建议数量。
- en: Although this is a shallow explanation, it’s apparent that this model is considerably
    more complex than the others due to its two-stage detection process. However,
    it’s possible to simplify it and enhance its speed while retaining its high accuracy.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这是一个简单的解释，但显而易见，由于其两阶段检测过程，这个模型比其他模型复杂得多。然而，可以简化它并提高其速度，同时保持高精度。
- en: 'To do so, the first important modification involves reducing the number of
    generated proposals from 300 to 50\. This reduction is feasible because there
    are only a few objects present in the image simultaneously. You can implement
    this change by adjusting the “**first_stage_max_proposals**” property, as demonstrated
    below:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，第一个重要的修改是将生成的建议数量从300减少到50。这一减少是可行的，因为图像中同时存在的物体数量较少。你可以通过调整“**first_stage_max_proposals**”属性来实现这一变化，如下所示：
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'After that, I eliminated the largest anchor box scale (**2.0**) from the model.
    This change was made because the hands and face maintain a consistent size due
    to the interpreter’s fixed distance from the camera, and having large anchor boxes
    might not be useful for proposal generation. Additionally, I removed one of the
    aspect ratios of the anchor boxes, given that my objects have similar shapes with
    minimal variation in the dataset. These adjustments are visually represented below:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我从模型中移除了最大的锚框尺度 (**2.0**)。这个改变是因为手部和面部由于解释器与相机的固定距离而保持一致的大小，大锚框可能对提议生成没有用。此外，我移除了一个锚框的长宽比，因为我的物体在数据集中具有类似的形状，变化很小。下面的调整以视觉形式展示：
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: That said, it’s crucial to consider the size and aspect ratios of your target
    objects. This consideration allows you to eliminate less useful anchor boxes and
    significantly decrease the computational cost of the model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，考虑目标物体的尺寸和长宽比至关重要。这种考虑可以帮助你排除不太有用的锚框，并显著减少模型的计算成本。
- en: YoloV7
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: YoloV7
- en: 'In contrast, minimal changes were applied to YoloV7 to preserve the architecture’s
    functionality. The main modification involved simplifying the CNN responsible
    for feature extraction, in both the backbone and the model’s head. To achieve
    this, I decreased the number of kernels/feature maps for nearly every convolutional
    layer, creating the following model:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，对 YoloV7 应用的更改很少，以保留架构的功能性。主要的修改是简化了负责特征提取的 CNN，包括骨干网和模型的头部。为此，我减少了几乎每个卷积层的内核/特征图数量，创建了以下模型：
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As discussed earlier, removing some layers and feature maps from the detectors
    is typically a good approach for simpler problems, since feature extractors are
    initially designed to detect dozens or even hundreds of classes in diverse scenarios,
    requiring a more robust model to address these complexities and ensure high accuracy.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，从检测器中移除一些层和特征图通常是处理简单问题的好方法，因为特征提取器最初是设计用于检测数十种甚至数百种不同类别，在多种场景中需要更强大的模型来应对这些复杂性并确保高准确性。
- en: With these adjustments, I decreased the number of parameters from 36.4 million
    to just 14.1 million, representing a reduction of approximately 61%. Furthermore,
    I used an input resolution of 512x512 pixels instead of the suggested 640x640
    pixels in the original paper.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些调整，我将参数数量从 3640 万减少到仅 1410 万，减少约 61%。此外，我使用了 512x512 像素的输入分辨率，而不是原论文中建议的
    640x640 像素。
- en: Additional tip
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 额外提示
- en: Another valuable tip in the training of object detectors is to utilize the [Kmeans
    model for unsupervised adjustment of the anchor box proportions](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/generate_ssd_anchor_box_aspect_ratios_using_k_means_clustering.ipynb),
    fitting the width and height of the figures to maximize the ratio of Intersection
    over Union (IoU) within the training set. By doing this, we can better adapt the
    anchors to the given problem domain, thereby enhancing model convergence by starting
    with adequate aspect ratios. The figure below exemplifies this process, comparing
    three anchor boxes used by default in the SSD algorithm (in red) next to three
    boxes with optimized proportions for the hand and face detection task (in green).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个在目标检测训练中的宝贵提示是利用 [Kmeans 模型进行锚框比例的无监督调整](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/generate_ssd_anchor_box_aspect_ratios_using_k_means_clustering.ipynb)，将图形的宽度和高度调整到最大化训练集中的交并比
    (IoU) 比例。通过这样做，我们可以更好地适应给定的问题领域，从而通过以合适的长宽比开始，提升模型收敛。下图展示了这个过程，将 SSD 算法中默认使用的三个锚框（红色）与用于手部和面部检测任务的三个优化比例的框（绿色）进行比较。
- en: '![](../Images/bec419cc78db42499e6ebebf3fcd54c3.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bec419cc78db42499e6ebebf3fcd54c3.png)'
- en: Comparing different bounding boxes' aspect ratios. Image by author.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 比较不同边界框的长宽比。图片由作者提供。
- en: Showing the results
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 展示结果
- en: I trained and evaluated each detector using my own dataset, called the [Hand
    and Face Sign Language](https://github.com/AlvaroCavalcante/hand-face-detector)
    (HFSL) dataset, considering the mAP and the Frames Per Second (FPS) as the main
    metrics. The table below provides a summary of the results, with values in parentheses
    representing the FPS of the detector before implementing any of the described
    optimizations.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用自己的数据集，即[手部和面部手语](https://github.com/AlvaroCavalcante/hand-face-detector)（HFSL）数据集，训练和评估了每个检测器，考虑了mAP和每秒帧数（FPS）作为主要指标。下表提供了结果的总结，表中的值（括号中的数字）表示在实施任何描述的优化之前检测器的FPS。
- en: '![](../Images/7b70a868c04ab549fc667045dddee8b1.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b70a868c04ab549fc667045dddee8b1.png)'
- en: Object detection results.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测结果。
- en: We can observe that most of the models showed a significant reduction in inference
    time while maintaining a high mAP across various levels of [Intersection over
    Union](https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/)
    (IoU). More complex architectures, such as Faster R-CNN and EfficientDet, increased
    the FPS on GPU by 200.80% and 231.78%, respectively. Even SSD-based architectures
    showed a huge increase in performance, with 280.23% and 159.59% improvements for
    the 640 and 320 versions, respectively. Considering YoloV7, although the FPS difference
    is most noticeable on the CPU, the optimized model has 61% fewer parameters, reducing
    memory requirements and making it more suitable for edge devices.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到，大多数模型在保持各个层次的[交并比](https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/)（IoU）高mAP的同时，推理时间显著减少。更复杂的架构，如Faster
    R-CNN和EfficientDet，分别将GPU上的FPS提高了200.80%和231.78%。即使是基于SSD的架构，其性能也有了巨大的提升，640版本和320版本的性能分别提高了280.23%和159.59%。考虑到YoloV7，尽管在CPU上的FPS差异最为明显，但优化后的模型参数减少了61%，降低了内存需求，使其更适合边缘设备。
- en: Conclusion
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: There are instances when computational resources are limited, or tasks must
    be executed quickly. In such scenarios, we can further optimize the open-source
    object detection models to find a combination of hyperparameters that can reduce
    the computational requirements without affecting the results, thereby offering
    a suitable solution for diverse problem domains.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算资源有限或任务必须快速执行的情况下，我们可以进一步优化开源目标检测模型，以寻找能够减少计算需求而不影响结果的超参数组合，从而为各种问题领域提供合适的解决方案。
- en: I hope this article has assisted you in making better choices to train your
    object detectors, resulting in significant efficiency gains with minimal effort.
    If you didn’t understand some of the explained concepts, I recommend you dive
    deeper into how your object detection architecture works. Additionally, consider
    experimenting with different hyperparameter values to further streamline your
    models based on the specific problem you are addressing!
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这篇文章能帮助你在训练目标检测器时做出更好的选择，从而在付出最小努力的情况下实现显著的效率提升。如果你对某些解释的概念不太理解，我建议你深入了解你的目标检测架构是如何工作的。此外，考虑尝试不同的超参数值，以根据你所解决的具体问题进一步优化你的模型！
