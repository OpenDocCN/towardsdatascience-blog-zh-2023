- en: Why does Regularization actually work?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化为何真的有效？
- en: 原文：[https://towardsdatascience.com/regularization-996a4967984a](https://towardsdatascience.com/regularization-996a4967984a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/regularization-996a4967984a](https://towardsdatascience.com/regularization-996a4967984a)
- en: Deconstructing the mathematical concept of regularization in machine learning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解构机器学习中的正则化数学概念
- en: '[](https://guenterroehrich.medium.com/?source=post_page-----996a4967984a--------------------------------)[![Günter
    Röhrich](../Images/31a1d0dc835c7ad31197f8c387023d10.png)](https://guenterroehrich.medium.com/?source=post_page-----996a4967984a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----996a4967984a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----996a4967984a--------------------------------)
    [Günter Röhrich](https://guenterroehrich.medium.com/?source=post_page-----996a4967984a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://guenterroehrich.medium.com/?source=post_page-----996a4967984a--------------------------------)[![Günter
    Röhrich](../Images/31a1d0dc835c7ad31197f8c387023d10.png)](https://guenterroehrich.medium.com/?source=post_page-----996a4967984a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----996a4967984a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----996a4967984a--------------------------------)
    [Günter Röhrich](https://guenterroehrich.medium.com/?source=post_page-----996a4967984a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----996a4967984a--------------------------------)
    ·9 min read·Jan 2, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----996a4967984a--------------------------------)
    ·9 分钟阅读·2023年1月2日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'While there are many good resources that show how regularization is implemented
    in machine learning, this guide is a brief introduction to the question, why adding
    a term actually has beneficial properties for our models. This post will discuss
    the idea of regularization in the context of linear regression and the following
    topics will be covered:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有许多优秀的资源展示了正则化在机器学习中的实现，这个指南是对为何添加一个项对我们的模型具有有益性质的简要介绍。本文将讨论在线性回归背景下的正则化概念，并涵盖以下主题：
- en: '[Underlying Idea of OLS](#a40e)'
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[OLS 的基本概念](#a40e)'
- en: '[Tikhonov Regularization (L2 Regularization)](#cea1)'
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[提赫诺夫正则化（L2 正则化）](#cea1)'
- en: '[L1 Regularization](#ac08)'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[L1 正则化](#ac08)'
- en: '[Specifying Lambda](#3a78)'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[指定 Lambda](#3a78)'
- en: '[Regularization in Deep Learning](#32fe)'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[深度学习中的正则化](#32fe)'
- en: '[Conclusion](#877d)'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[结论](#877d)'
- en: Underlying Idea of OLS
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OLS 的基本概念
- en: 'To begin with, lets quickly go over the basic ordinary least squares (OLS)
    equation which plays a crucial role in the course of linear regression. In linear
    regression, our goal is to find a prediction for a value (y-hat) that is very
    close to the true (observed) value. To achieve this, we usually try to find a
    set of coefficients that allow us to draw a straight line through the data points.
    This line is regarded optimal when it **minimizes the distances to the data points**.
    Here is an example using the famous cars dataset in R:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们快速回顾一下基本的普通最小二乘（OLS）方程，它在线性回归中起着至关重要的作用。在线性回归中，我们的目标是找到一个预测值（y-hat），使其尽可能接近真实（观测）值。为实现这一目标，我们通常尝试找出一组系数，以便通过数据点绘制一条直线。当这条直线**最小化数据点的距离**时，被认为是最优的。以下是使用
    R 语言中的著名汽车数据集的示例：
- en: '![](../Images/71919088f3dd9ad31d586bdad3cb08cd.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/71919088f3dd9ad31d586bdad3cb08cd.png)'
- en: A simple Linear Regression demo — Image by Author using “cars” dataset
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的线性回归演示 — 作者使用“汽车”数据集制作的图像
- en: The red line is an estimate of the response value “*y*”. By the definition of
    an estimate, the value we calculate is very likely close to, **but not exactly
    equal to**, the true *response variable, y*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 红线是响应值“*y*”的估计值。根据估计值的定义，我们计算出的值很可能接近**但不完全等于**真实的*响应变量 y*。
- en: '![](../Images/a86a18d85ac0070a31e585d98c4a8068.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a86a18d85ac0070a31e585d98c4a8068.png)'
- en: The estimate (y-hat) which is shown as red line in the chart
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 估计值（y-hat）在图表中以红线显示
- en: As mentioned, OLS describes the basic idea of **minimizing the distance between
    the red line (estimate, y-hats) and the true values (response, y), but how to
    actually find this optimal line?** There are several ways to fit the line to the
    data, as an example, we could use the idea of gradient descent to gradually adjust
    (shift, tilt) the red line in a way, so that it ends up in the position shown
    above. In the **regression context, we would rather prefer to use matrix inversion**
    which yields the same result for our estimates.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，OLS 描述了**最小化红线（估计值，y-hats）和真实值（响应值，y）之间的距离**的基本思想，但实际如何找到这条最佳线呢？有几种方法可以将线拟合到数据上，例如，我们可以使用梯度下降的思想逐步调整（移动、倾斜）红线，使其最终达到上述位置。在**回归背景下，我们更倾向于使用矩阵求逆**，这会为我们的估计结果提供相同的结果。
- en: 'If the gradient descent approach sounds interesting to you, the following step-by-step
    guide might be interesting to read:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果梯度下降方法对你来说很有趣，接下来的逐步指南可能会很有趣：
- en: '[](/gradient-descent-f7458de38365?source=post_page-----996a4967984a--------------------------------)
    [## A Simple Guide to Gradient Descent — A Linear Regression Example'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 梯度下降简单指南——线性回归示例](/gradient-descent-f7458de38365?source=post_page-----996a4967984a--------------------------------)'
- en: When opening a machine learning book it is very unlikely that the first ten
    pages can be survived without encountering…
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 当翻开一本机器学习书籍时，很难在前十页内不遇到…
- en: towardsdatascience.com](/gradient-descent-f7458de38365?source=post_page-----996a4967984a--------------------------------)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/gradient-descent-f7458de38365?source=post_page-----996a4967984a--------------------------------)'
- en: 'Our starting point for either case is the loss or objective function which
    can be expressed as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 无论哪种情况，我们的起点都是损失或目标函数，可以表达为如下：
- en: '![](../Images/9ff9656bc547c554ab7f35f55f1d98a7.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ff9656bc547c554ab7f35f55f1d98a7.png)'
- en: The loss function simply formalizes the idea of “minimizing the distance between
    line and true values”
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数简单地形式化了“最小化线与真实值之间距离”的思想。
- en: 'To minimize this function, we are looking for its derivative with respect to
    the set of coefficients (betas). This can be shown as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最小化这个函数，我们需要寻找相对于系数（betas）集合的导数。可以如下所示：
- en: '![](../Images/4f46c3f3fdb9d10d841024c7ddd113a8.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f46c3f3fdb9d10d841024c7ddd113a8.png)'
- en: The minimum of the function can be obtained by setting the above derivation
    to zero. Further, we are able to solve for the values beta, our model coefficients.
    As you can see in the exponent, we have to invert the matrix of X values — and
    this is where we need to pay attention to linear independence of these columns!
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将上述导数设为零，可以得到函数的最小值。此外，我们能够求解出beta值，即我们的模型系数。正如你在指数中看到的，我们需要对X值的矩阵进行求逆——这就是我们需要注意这些列的线性独立性的地方！
- en: '![](../Images/21720ed80417e5c12c6f20717246fcfb.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21720ed80417e5c12c6f20717246fcfb.png)'
- en: Note that the loss function is a convex function so we should get to a local
    minimum, if we were interested *showing this*, we would need to look into the
    second derivative (which is equivalent to the *Hessian matrix*).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，损失函数是一个凸函数，因此我们应该能找到局部最小值。如果我们有兴趣*展示这一点*，我们需要查看二阶导数（这等同于*Hessian矩阵*）。
- en: '**The key takeaway here is that we can find the estimated coefficients by deriving
    the loss function as demonstrated.**'
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**这里的关键是，我们可以通过对损失函数进行导数运算来找到估计系数，如演示所示。**'
- en: Let’s now move on with the question “**why and how OLS is related to the idea
    of regularization?**”.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续讨论“**OLS如何以及为何与正则化的概念相关？**”这个问题。
- en: Tikhonov Regularization (Ridge)
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提赫诺夫正则化（岭回归）
- en: 'In the course of regularization you might often be confronted an equation that
    looks like this one:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在正则化过程中，你可能经常会遇到类似下面的方程：
- en: '![](../Images/25f43606d1e5bbb0ffecd0fc770adf82.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/25f43606d1e5bbb0ffecd0fc770adf82.png)'
- en: Quickly broken down, the equation shows two parts, the OLS part we just analyzed
    above, as well as a new term that adds a sum of squared betas to the overall loss
    function.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 快速拆解，方程显示了两个部分，即我们刚刚分析过的OLS部分，以及一个新项，该项将平方betas的总和添加到整体损失函数中。
- en: I decided to first provide the L2 regularization (“norm squared” as shown above)
    because this is common not only in linear regression, but also quite popular in
    **regularization for weight matrices in deep learning**.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我决定首先提供L2正则化（如上所示的“平方范数”），因为这在不仅在线性回归中很常见，而且在**深度学习中的权重矩阵正则化**中也非常流行。
- en: '*But why do we add this new last term to our loss (or objective) function?*'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*但为什么我们要将这个新的最后一项添加到我们的损失（或目标）函数中？*'
- en: '*Could we add the same term to the n-th power instead?*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们可以将相同的项添加到n次幂吗？*'
- en: OLS makes several assumptions about the data. As mentioned, one of these assumptions
    implies that due to linear independence ([to assure we can invert the matrix](https://textbooks.math.gatech.edu/ila/invertible-matrix-thm.html)),
    we are able to find a set of coefficients that provide a unique solution to the
    given problem, however, this may not be the case. There are scenarios where **no
    set** of betas or even **several** sets of betas could be found, resulting in
    a non-unique solution.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: OLS对数据做出了一些假设。如前所述，其中一个假设是由于线性独立性（[为了确保我们可以反转矩阵](https://textbooks.math.gatech.edu/ila/invertible-matrix-thm.html)），我们能够找到一组系数，以为给定的问题提供唯一解决方案，但这可能不是事实。在某些情况下，**没有一组**
    betas或甚至**多个** betas组可以找到，导致非唯一解决方案。
- en: '**Why is this a problem?** Regression asks for linearly independent observations,
    and given the fact we use matrix inversion, also requires linearly independent
    columns. This is problematic because we might find ourselves in the position with
    an over- or underdetermined system of equations — meaning there are several non-unique
    or even no solutions at all! This is exactly where and why regularization comes
    into play:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**这为什么是个问题？** 回归要求线性独立的观测值，并且考虑到我们使用矩阵反演，还需要线性独立的列。这是一个问题，因为我们可能会发现自己处于一个过度或不足确定的方程组中——这意味着有几个非唯一或甚至没有解决方案！这正是正则化发挥作用的地方：'
- en: '**Introducing further constraints to our optimization problem allows us to
    improve the limitation/conditioning of the equation which may further allow us
    to determine a unique solution set.**'
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**引入更多的约束到我们的优化问题中，可以改善方程的限制/条件，这可能进一步允许我们确定唯一的解集。**'
- en: '![](../Images/e4aa39534062cce47fb4d4f72dc00d24.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4aa39534062cce47fb4d4f72dc00d24.png)'
- en: 'As we can see, there is a term accompanied by a Lagrangian multiplier added
    to the equation. This Lagrangian adds the following constraint:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，方程中添加了一个带有拉格朗日乘数的项。这个拉格朗日引入了以下约束：
- en: '![](../Images/43cb3b763f3b044fa4cbfb2ae1cc9c80.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/43cb3b763f3b044fa4cbfb2ae1cc9c80.png)'
- en: Note that c is a constant and will be omitted when deriving the function leaving
    us with a “beta²” term. What becomes fairly obvious when considering minimizing
    the whole equation is, that many large coefficients contradict with minimizing
    the sum of coefficients. This means, we can expect to see smaller coefficients.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，c 是一个常数，在推导函数时将被省略，留下一个“beta²”项。考虑到最小化整个方程时，许多大系数与最小化系数的总和相矛盾，这一点相当明显。这意味着，我们可以预期看到较小的系数。
- en: 'This gets us to the following steps:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这将引导我们进入以下步骤：
- en: '![](../Images/ec0429886a4f9eca51a3f075a7864306.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ec0429886a4f9eca51a3f075a7864306.png)'
- en: In contrast to the OLS derivative, we now introduced a “+ lambda * Identity”
    term
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 与OLS导数相比，我们现在引入了一个“+ lambda * Identity”项
- en: L1 Regularization
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: L1 正则化
- en: 'With regard to the previous question, whether we could use different variations
    of the regularization (e.g. power), the simple answer is yes. The most popular
    example is again very closely tied to OLS and is called LASSO regression. LASSO
    adds a constraint that introduces an absolute value penalty as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 关于前一个问题，即我们是否可以使用不同的正则化变体（例如，幂），简单的答案是肯定的。最常见的例子还是与OLS紧密相关，被称为LASSO回归。LASSO增加了一个引入绝对值惩罚的约束，如下所示：
- en: '![](../Images/83ce4521df5e8a71c7b316ed83af9936.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/83ce4521df5e8a71c7b316ed83af9936.png)'
- en: Loss under the LASSO L1 constraint
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: LASSO L1约束下的损失
- en: As you can see, the Lagrangian term is quite similar to what we saw before for
    Ridge regression, in this case however, we see a change in the norm and power
    for the betas.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，拉格朗日项与我们之前在Ridge回归中看到的相似，不过在这种情况下，我们看到betas的范数和幂发生了变化。
- en: 'In contrast to the L2 norm (where betas are squared), betas under L1 can be
    negative! For this reason we have to look at a few cases of what values the betas
    could have and obtain the derivation for each scenario:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 与L2范数（其中betas被平方）相比，L1下的betas可以是负值！因此，我们需要查看betas可能的几个值，并为每种情况得出推导：
- en: 'Starting with the expansion of the above equation:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的方程展开开始：
- en: '![](../Images/b142154fe899cb076fa0f6637bad57b3.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b142154fe899cb076fa0f6637bad57b3.png)'
- en: the following derivations can be obtained
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 以下推导可以得到
- en: '**Betas > 0:**'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**Betas > 0:**'
- en: '![](../Images/881998a6083c6d7eb49be141836c1aa2.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/881998a6083c6d7eb49be141836c1aa2.png)'
- en: '**Betas < 0:**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**Betas < 0:**'
- en: '![](../Images/5d6d15be1183d4e7067b17dfd9fe6cf4.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d6d15be1183d4e7067b17dfd9fe6cf4.png)'
- en: '**Betas = 0:** *This corresponds to the standard OLS case*'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**贝塔 = 0：** *这对应于标准OLS情况*'
- en: LASSO is not only known as regularization method as such, but is also used as
    a technique for variable selection when dealing with a variety of features. The
    below illustration provides a geometric interpretation that shows why L1 norm
    can be used for variable selection tasks as some weights are pushed to 0 (and
    can therefore be omitted).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: LASSO不仅仅被称为正则化方法，还被用作处理多种特征时的变量选择技术。下面的插图提供了一个几何解释，展示了为什么L1范数可以用于变量选择任务，因为一些权重被推到0（因此可以被省略）。
- en: The L2 penalty (right) can be expressed as circle and the penalty leads to the
    result that the diameter of the circle shrinks. This means, the coefficients or
    betas (named as “weights”, w1 and w2 here) will shrink towards zero.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: L2惩罚（右侧）可以表示为圆形，这种惩罚导致圆的直径缩小。这意味着，系数或贝塔（在这里称为“权重”，w1和w2）将收缩趋向于零。
- en: '![](../Images/62db78bec16065c7e33627f48da4ec90.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62db78bec16065c7e33627f48da4ec90.png)'
- en: Image from [Wikipedia](https://en.wikipedia.org/wiki/Lasso_(statistics)#/media/File:L1_and_L2_balls.svg)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自[维基百科](https://en.wikipedia.org/wiki/Lasso_(statistics)#/media/File:L1_and_L2_balls.svg)
- en: As you may have noted, a *convex object that is tangent to the constraint boundary
    is likely to encounter a corner under the L1 norm,* meaning one or more betas
    is exactly zero, while for the L2 case, the points at which the convex object
    is tangent to the boundary, are no more likely than all other points on the circle.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能注意到的，*一个与约束边界相切的凸对象在L1范数下更可能遇到角点，* 这意味着一个或多个贝塔正好为零，而对于L2情况，凸对象与边界相切的点，不比圆上的其他点更有可能。
- en: As L1 allows coefficients to be zero, the penalty is commonly known as “inducing
    sparsity” on the set of weights (or betas, as we called them in the regression
    context). Coefficients may be shrunk down to 0 which is not likely to happen under
    the quadratic constraint.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 由于L1允许系数为零，因此惩罚通常被称为对权重（或在回归上下文中称为贝塔）集的“引入稀疏性”。系数可能被缩小到0，而在二次约束下不太可能发生。
- en: Specifying Lambda
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指定Lambda
- en: Looking at the regularization examples above, it becomes clear that the choice
    of lambda is crucial to the modeling approach. Lambda plays an important role
    to balance the bias-variance-tradeoff and should therefore be carefully selected.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 查看上述正则化示例后，很明显lambda的选择对建模方法至关重要。Lambda在平衡偏差-方差权衡中发挥了重要作用，因此应仔细选择。
- en: 'In practice, **cross validation** is used to find a value of lambda that keeps
    the model error low, while also considering reasonable levels of variance. Feel
    free to read more about this topic in the link below:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 实际操作中，**交叉验证**用于找到一个lambda值，该值使模型误差保持低，同时考虑合理的方差水平。可以在下面的链接中进一步阅读这个话题：
- en: '[](/cross-validation-7c0163460ea0?source=post_page-----996a4967984a--------------------------------)
    [## Proper Model Selection through Cross Validation'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/cross-validation-7c0163460ea0?source=post_page-----996a4967984a--------------------------------)
    [## 通过交叉验证进行正确的模型选择'
- en: Cross validation is state of the art. It is nothing close to a fun task, but
    yet vital for everyone dealing with data…
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 交叉验证是最先进的技术。这不是一项有趣的任务，但对于所有处理数据的人来说却至关重要……
- en: towardsdatascience.com](/cross-validation-7c0163460ea0?source=post_page-----996a4967984a--------------------------------)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/cross-validation-7c0163460ea0?source=post_page-----996a4967984a--------------------------------)
- en: Regularization in Deep Learning
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习中的正则化
- en: So far we have looked at L1 and L2 regularization as well as taken a quick reference
    on “how to determine lambda”. Regularization is not only crucial in the course
    of regression, but will also lead to deep learning models that generalize much
    better than “vanilla” implementations without regularization.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经查看了L1和L2正则化，并快速参考了“如何确定lambda”。正则化不仅在回归过程中至关重要，而且还会导致比没有正则化的“普通”实现更好的泛化能力的深度学习模型。
- en: 'Nevertheless, regularization in deep learning is not entirely re-inventing
    the wheel. Similar to the idea of squared betas (L2) loss, we can easily translate
    this to the weight matrix, as used in a neural network:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，深度学习中的正则化并不是完全重新发明轮子。类似于平方贝塔（L2）损失的想法，我们可以很容易地将其转化为神经网络中使用的权重矩阵：
- en: '![](../Images/3c7e9ce508b466798b364a90b5c888e7.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c7e9ce508b466798b364a90b5c888e7.png)'
- en: 'This equation results in a single scalar value, comprising the squared sum
    over the whole weight matrix. This is equivalent to the sum of beta squared, as
    shown earlier. Putting together the regularization loss above as well as the “data
    loss” that is obtained from the objective function (e.g. Softmax or Sigmoid functions)
    and denoted as Li:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程会得到一个单一的标量值，由整个权重矩阵上的平方和组成。这等同于之前展示的 beta 的平方和。将上述正则化损失与从目标函数（例如 Softmax
    或 Sigmoid 函数）中获得的“数据损失”相结合，记作 Li：
- en: '![](../Images/e25d64ed873ef587b3f0b642525aa9d4.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e25d64ed873ef587b3f0b642525aa9d4.png)'
- en: Loss = L(objective function) + lambda*L(Weight Matrix)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 损失 = L（目标函数） + lambda*L（权重矩阵）
- en: 'Having shown the L2 regularization for a neural network, please be aware that
    there are many different ways to achieve regularization or regularization similar
    behavior. The right choice may save you a lot of training time and will also make
    the model more general. Very popular regularization (like) options are:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在展示了神经网络的 L2 正则化后，请注意有很多不同的方式可以实现正则化或类似的行为。正确的选择可以节省大量训练时间，并使模型更具通用性。非常受欢迎的正则化（如）选项包括：
- en: L1 and L2 regularization
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: L1 和 L2 正则化
- en: Dropout
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Dropout
- en: '[Batch / Layer / Spatial Normalization](http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43442.pdf)
    (that prevents covariate shifts)'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[批量/层/空间归一化](http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43442.pdf)（可以防止协变量偏移）'
- en: “Early stopping”
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “早停”
- en: You may sometimes also find the idea of “early stopping”, which means the training
    is stopped when a certain criteria is met (e.g. the Loss is not improving < epsilon
    for the last x epochs). Although this method is really just looking at the model
    outcome / loss, it often proves to be a great tool to leverage. Early stopping
    may be applied under the term “Callbacks” in deep learning frameworks.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 有时你还会发现“早停”这个概念，即当满足某个条件时（例如，损失在最后 x 轮次中没有改善 < epsilon），训练就会停止。虽然这个方法实际上只是查看模型结果/损失，但它常常被证明是一个很好的工具。早停可以在深度学习框架中应用于“回调”一词。
- en: Conclusion
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: Regularization is a great way of incorporating a set of constraints to the underlying
    optimization problem. Adding additional constraints to the loss function can significantly
    help improving model generalization, reduce the risk of overfitting, overcome
    ill-posed problems (like multi-collinearity in linear regression), but also support
    the variable selection process.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化是一种将约束集纳入基础优化问题的好方法。向损失函数添加额外的约束可以显著帮助提高模型的泛化能力，减少过拟合风险，克服不适定问题（如线性回归中的多重共线性），同时也支持变量选择过程。
- en: '*If you found this post interesting, I would appreciate you hitting the follow
    button!*'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你觉得这篇文章有趣，请点关注！*'
- en: '*{Take care of yourself, and if you can, someone else too}*'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*{照顾好自己，如果可以的话，也照顾他人}*'
- en: '*— borrowed from Stephen Dubner*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*— 借用自 Stephen Dubner*'
- en: 'Interesting Reads:'
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有趣的阅读：
- en: '**L1 Regularization**'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**L1 正则化**'
- en: '[https://aswani.ieor.berkeley.edu/teaching/SP15/265/lecture_notes/ieor265_lec6.pdf](https://aswani.ieor.berkeley.edu/teaching/SP15/265/lecture_notes/ieor265_lec6.pdf)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://aswani.ieor.berkeley.edu/teaching/SP15/265/lecture_notes/ieor265_lec6.pdf](https://aswani.ieor.berkeley.edu/teaching/SP15/265/lecture_notes/ieor265_lec6.pdf)'
- en: '**L2 Regularization**'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**L2 正则化**'
- en: '[https://cs229.stanford.edu/notes2021fall/lecture10-ridge-regression.pdf](https://cs229.stanford.edu/notes2021fall/lecture10-ridge-regresion.pdf)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://cs229.stanford.edu/notes2021fall/lecture10-ridge-regression.pdf](https://cs229.stanford.edu/notes2021fall/lecture10-ridge-regression.pdf)'
- en: '**L2 Regularization on weight matrices (deep learning)**'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**权重矩阵的 L2 正则化（深度学习）**'
- en: '[](https://cs231n.github.io/linear-classify/?source=post_page-----996a4967984a--------------------------------#multiclass-support-vector-machine-loss)
    [## CS231n Convolutional Neural Networks for Visual Recognition'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://cs231n.github.io/linear-classify/?source=post_page-----996a4967984a--------------------------------#multiclass-support-vector-machine-loss)
    [## CS231n 卷积神经网络用于视觉识别'
- en: 'Table of Contents: In the last section we introduced the problem of Image Classification,
    which is the task of…'
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目录：在上一节中，我们介绍了图像分类的问题，这是一项任务…
- en: cs231n.github.io](https://cs231n.github.io/linear-classify/?source=post_page-----996a4967984a--------------------------------#multiclass-support-vector-machine-loss)
    ![](../Images/beaadac02474b27233320e2c07741bfd.png)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: cs231n.github.io](https://cs231n.github.io/linear-classify/?source=post_page-----996a4967984a--------------------------------#multiclass-support-vector-machine-loss)
    ![](../Images/beaadac02474b27233320e2c07741bfd.png)
- en: Not a constraint boundary — by Christoph Ungerböck
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 不是一个约束边界 — 作者 Christoph Ungerböck
