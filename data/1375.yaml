- en: How to Build Graph Transformers with O(N) Complexity
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何构建具有 O(N) 复杂度的图 Transformer
- en: 原文：[https://towardsdatascience.com/how-to-build-graph-transformers-with-o-n-complexity-d507e103d30a?source=collection_archive---------6-----------------------#2023-04-19](https://towardsdatascience.com/how-to-build-graph-transformers-with-o-n-complexity-d507e103d30a?source=collection_archive---------6-----------------------#2023-04-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-build-graph-transformers-with-o-n-complexity-d507e103d30a?source=collection_archive---------6-----------------------#2023-04-19](https://towardsdatascience.com/how-to-build-graph-transformers-with-o-n-complexity-d507e103d30a?source=collection_archive---------6-----------------------#2023-04-19)
- en: Tutorial on Large-Graph Transformers
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大图网络 Transformer 教程
- en: '[](https://medium.com/@qitianwu228?source=post_page-----d507e103d30a--------------------------------)[![Qitian
    Wu](../Images/363e62ead857be0af76f9654c19f2b8c.png)](https://medium.com/@qitianwu228?source=post_page-----d507e103d30a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d507e103d30a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d507e103d30a--------------------------------)
    [Qitian Wu](https://medium.com/@qitianwu228?source=post_page-----d507e103d30a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@qitianwu228?source=post_page-----d507e103d30a--------------------------------)[![Qitian
    Wu](../Images/363e62ead857be0af76f9654c19f2b8c.png)](https://medium.com/@qitianwu228?source=post_page-----d507e103d30a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d507e103d30a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d507e103d30a--------------------------------)
    [Qitian Wu](https://medium.com/@qitianwu228?source=post_page-----d507e103d30a--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F268302525672&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-graph-transformers-with-o-n-complexity-d507e103d30a&user=Qitian+Wu&userId=268302525672&source=post_page-268302525672----d507e103d30a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d507e103d30a--------------------------------)
    ·7 min read·Apr 19, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd507e103d30a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-graph-transformers-with-o-n-complexity-d507e103d30a&user=Qitian+Wu&userId=268302525672&source=-----d507e103d30a---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F268302525672&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-graph-transformers-with-o-n-complexity-d507e103d30a&user=Qitian+Wu&userId=268302525672&source=post_page-268302525672----d507e103d30a---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d507e103d30a--------------------------------)
    ·7 分钟阅读·2023年4月19日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd507e103d30a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-graph-transformers-with-o-n-complexity-d507e103d30a&user=Qitian+Wu&userId=268302525672&source=-----d507e103d30a---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd507e103d30a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-graph-transformers-with-o-n-complexity-d507e103d30a&source=-----d507e103d30a---------------------bookmark_footer-----------)![](../Images/d7dd98f0eb38aa61ae00d1bff995fd15.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd507e103d30a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-graph-transformers-with-o-n-complexity-d507e103d30a&source=-----d507e103d30a---------------------bookmark_footer-----------)![](../Images/d7dd98f0eb38aa61ae00d1bff995fd15.png)'
- en: 'Image: [Unsplash](https://unsplash.com/photos/OgvqXGL7XO4).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片： [Unsplash](https://unsplash.com/photos/OgvqXGL7XO4)。
- en: Building powerful graph Transformers has become a trending topic in the graph
    machine learning community, as a surge of recent efforts have shown that pure
    Transformer-based models can perform competitively or even superiorly on quite
    a few GNN benchmarks (see some typical works along this direction [1, 2, 3]).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 构建强大的图 Transformer 已成为图机器学习社区的热门话题，因为最近的研究表明，纯 Transformer 基础的模型在许多 GNN 基准测试中表现出色，甚至优于其他模型（可以参见一些典型的工作
    [1, 2, 3]）。
- en: The challenge, however, is that the key design of Transformers [4], i.e., attention
    mechanism, often requires **quadratic complexity w.r.t. the input tokens**. In
    the context of graph learning, the input tokens for Transformers are nodes in
    a graph, and the global attention aimed at capturing long-range interactions among
    nodes is hard to scale for graphs with arbitrary numbers of nodes. For example,
    on the common node classification dataset Pubmed (with ~10K nodes), *running a
    one-layer single-head Transformer with all-pair attention in a GPU with 16GB memory
    is infeasible.*
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，挑战在于 Transformers 的关键设计[4]，即注意力机制，通常需要**相对于输入标记的二次复杂度**。在图学习的背景下，Transformers
    的输入标记是图中的节点，旨在捕捉节点间长距离交互的全局注意力对于具有任意数量节点的图而言难以扩展。例如，在常见的节点分类数据集 Pubmed（约 10K 节点）上，*在
    16GB 内存的 GPU 上运行一个单层单头 Transformer 并进行全对全注意力计算是不可行的。*
- en: 'This tutorial will introduce two recent scalable graph Transformers [5, 6]
    that design special global attention mechanisms with **linear complexity w.r.t.
    the number of tokens (nodes)**. The goal of this tutorial is to provide hands-on
    guidance on:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程将介绍两个最近的可扩展图 Transformer[5, 6]，它们设计了具有**相对于标记（节点）数量的线性复杂度**的特殊全局注意力机制。本教程的目标是提供关于：
- en: '*how the linear complexity is achieved when preserving the all-pair attention;*'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*如何在保留全对全注意力的情况下实现线性复杂度；*'
- en: '*how the new attention functions are implemented using Pytorch codes.*'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*如何使用 Pytorch 代码实现新的注意力函数。*'
- en: These are complementary to the published scientific papers that focus on the
    high-level idea description.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这些与专注于高层次思想描述的已发表科学论文是互补的。
- en: Where does the O(N²) Comes From?
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: O(N²) 从哪里来？
- en: Transformers can be seen as a generalization of graph neural networks (GNNs)
    where the fixed adjacency matrix in GNNs is extended to a variable attention matrix
    in Transformers. From a graph view, GNNs operate message passing over a fixed
    observed graph (that often has sparse connection), while the message passing of
    Transformers is anchored on a densely connected latent graph (whose edge weights
    are generated by pairwise attention scores.)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers 可以被视为图神经网络（GNNs）的推广，其中 GNNs 中的固定邻接矩阵扩展为 Transformers 中的可变注意力矩阵。从图的角度来看，GNNs
    在固定观察图（通常具有稀疏连接）上进行消息传递，而 Transformers 的消息传递则基于密集连接的潜在图（其边权由成对注意力分数生成）。
- en: '![](../Images/e48812660c4b49eb9f84766eb3ab1fa1.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e48812660c4b49eb9f84766eb3ab1fa1.png)'
- en: 'Comparison of GNNs and Transformers in terms of message passing over different
    structures: GNNs propagate signals over a sparse observed graph, while Transformers
    can be seen as propagating signals over a densely connected graph with layer-wise
    edge weights. The latter requires estimation for the N*N attention matrix and
    feature propagation over such a dense matrix.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: GNNs 和 Transformers 在不同结构上的消息传递比较：GNNs 在稀疏观察图上传播信号，而 Transformers 可以被视为在具有层次边权的密集连接图上传播信号。后者需要对
    N*N 注意力矩阵进行估计，并在如此密集的矩阵上进行特征传播。
- en: 'We next recapsulate the standard attention computation in original Transformer
    [4]. The embeddings at the current layer are first mapped into query, key and
    value vectors, and then calculate the all-pair attention for feature aggregation:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将总结原始 Transformer[4] 中的标准注意力计算。当前层的嵌入首先映射到查询、键和值向量，然后计算全对全注意力以进行特征聚合：
- en: '![](../Images/58e7b1ec741a5407b2d29a53b9dcaea8.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/58e7b1ec741a5407b2d29a53b9dcaea8.png)'
- en: We use z to denote the node embeddings, and q, k and v to denote the query,
    key and value vectors, respectively. The W_Q, W_K and W_V are learnable weights
    at the k-th layer.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 z 表示节点嵌入，q、k 和 v 分别表示查询、键和值向量。W_Q、W_K 和 W_V 是第 k 层的可学习权重。
- en: Since the computation of the above updating requires O(N), the total complexity
    for updating N nodes in one layer would require O(N²). A more intuitive way to
    see the O(N²) complexity can be from a matrix view which is practically considered
    for implementation using the deep learning tools (e.g., Pytorch, Tensorflow, etc.).
    In specific, we can illustrate the computation flow of one attention layer below.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由于上述更新的计算需要 O(N)，因此在一层中更新 N 个节点的总复杂度将需要 O(N²)。从矩阵视角看 O(N²) 复杂度的一个更直观的方式是，它在实际应用中使用深度学习工具（如
    Pytorch、Tensorflow 等）时被考虑。具体而言，我们可以在下面说明一个注意力层的计算流程。
- en: '![](../Images/470e3679b67218fc975a88df51c142ac.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/470e3679b67218fc975a88df51c142ac.png)'
- en: The left part illustrates the global attention layer from a matrix view, and
    the right part presents the corresponding data flow where the matrix product marked
    by red color introduces O(N²) complexity.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧部分展示了从矩阵视角看待的全局注意力层，右侧部分展示了对应的数据流，其中红色标记的矩阵乘积引入了O(N²)的复杂度。
- en: 'The attention layer above can be easily implemented with PyTorch (here we use
    the “einsum” function that is a generalized matrix product implementation, see
    [here](https://pytorch.org/docs/stable/generated/torch.einsum.html?highlight=einsum#torch.einsum)
    for its detailed information):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 上述注意力层可以通过PyTorch轻松实现（在这里我们使用“einsum”函数，这是一个广义矩阵乘积实现，详细信息请参见[这里](https://pytorch.org/docs/stable/generated/torch.einsum.html?highlight=einsum#torch.einsum)）：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: While the quadratic complexity is cumbersome, we next introduce two effective
    ways that can *strictly* reduce the O(N²) to O(N) and more importantly, *still
    preserve the expressivity for explicitly modeling the all-pair influence.*
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管二次复杂度很麻烦，我们接下来介绍两种有效的方法，可以*严格*地将O(N²)减少到O(N)，更重要的是，*仍然保持了对所有对之间影响的明确建模的表达能力*。
- en: 'NodeFormer: Kernelized Softmax Message Passing'
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'NodeFormer: 核化Softmax消息传递'
- en: 'The recent work NodeFormer [5] ([a scalable graph structure learning Transformer
    for node classification](https://openreview.net/pdf?id=sMezXGG5So)) leverages
    the random Fourier features [8, 9] to convert the *dot-then-exponential* operation
    into a *map-then-dot* alternative by kernel approximation:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的工作NodeFormer [5] ([一种用于节点分类的可扩展图结构学习Transformer](https://openreview.net/pdf?id=sMezXGG5So))利用随机傅里叶特征[8,
    9]通过核近似将*点积然后指数*操作转换为*映射然后点积*替代方案：
- en: '![](../Images/cf60cac345f332ce426bf7402f541f02.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf60cac345f332ce426bf7402f541f02.png)'
- en: The \phi function here is a non-parametric random feature map where the feature
    dimension m controls the approximation power for the original exponential term.
    See reference [8] for more introduction on the random feature map and theoretical
    its properties.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的\phi函数是一个非参数随机特征映射，其中特征维度m控制了对原始指数项的近似能力。有关随机特征映射及其理论属性的更多介绍，请参见参考文献[8]。
- en: 'In this way, the original Softmax attention can be transformed into an efficient
    version:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，原始的Softmax注意力可以被转化为一种高效的版本：
- en: '![](../Images/1ef1d90bf4789fadd08cf503977c2bf2.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ef1d90bf4789fadd08cf503977c2bf2.png)'
- en: The derivation from the LHS to the RHS is according the basic association rule
    of matrix product that changes the order of matrix product
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从左侧到右侧的推导是依据矩阵乘积的基本关联规则，即改变矩阵乘积的顺序。
- en: Notice that in the RHS, the two summation terms over N nodes are independent
    from node u, which means they can be re-used by all the nodes after once computation.
    Thereby, for updating N nodes in each layer, one can first spend O(N) to compute
    the two summation terms and based on this, the computation for the next-layer
    embeddings of all the N nodes would only require O(N) which induce the total complexity
    O(N).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在右侧，N个节点上的两个求和项与节点u无关，这意味着它们在计算一次后可以被所有节点重复使用。因此，对于每一层更新N个节点，可以先花费O(N)来计算这两个求和项，然后基于此，计算所有N个节点的下一层嵌入只需O(N)，从而总复杂度为O(N)。
- en: To better understand how the linear complexity is achieved, we can write the
    matrix-form computation flow as follows.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解如何实现线性复杂度，我们可以将矩阵形式的计算流程写成如下。
- en: '![](../Images/c10a094a5b8298f27f107d560600a551.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c10a094a5b8298f27f107d560600a551.png)'
- en: The left part illustrates the global attention layer of NodeFormer from a matrix
    view, and the right part presents the corresponding data flow where the matrix
    products marked by red color are the computation bottleneck which requires O(Nmd).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧部分展示了从矩阵视角看待的NodeFormer的全局注意力层，右侧部分展示了对应的数据流，其中红色标记的矩阵乘积是计算瓶颈，需要O(Nmd)。
- en: Notice that the order of matrix product plays an important role in reducing
    the complexity. In the above computation, the N*N attention matrix is avoided
    though we successfully achieve all-pair attentive aggregation. The exact complexity
    of one layer is O(Nmd). Since for large graphs, N is often orders-of-magnitude
    larger than m and d, the computational efficiency can be significantly improved
    in practice. For example, NodeFormer with three Transformer layers only requires
    4GB GPU memory for computing the all-pair attention among 0.1M nodes. Below is
    the Pytorch codes for implementing the above efficient all-pair attention. The
    complete open-source model implementation is publicly available at [GitHub](https://github.com/qitianwu/NodeFormer/blob/main/nodeformer.py).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，矩阵乘积的顺序在减少复杂度中发挥了重要作用。在上述计算中，尽管我们成功实现了全对全注意力聚合，但避免了N*N的注意力矩阵。一层的确切复杂度是O(Nmd)。由于对于大图，N通常比m和d大几个数量级，因此在实际应用中计算效率可以显著提高。例如，具有三层Transformer的NodeFormer仅需4GB
    GPU内存即可计算0.1M节点之间的全对全注意力。以下是实现上述高效全对全注意力的Pytorch代码。完整的开源模型实现公开可用，地址为[GitHub](https://github.com/qitianwu/NodeFormer/blob/main/nodeformer.py)。
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'DIFFormer: Simplified Attention Computation'
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'DIFFormer: 简化的注意力计算'
- en: The lesson we can learn from NodeFormer is that the crux of complexity reduction
    lies in the order of matrix product w.r.t. attentive aggregation. We can next
    leverage this idea to design another efficient attention function without any
    stochastic approximation, i.e., the simple attention in [DIFFormer](https://arxiv.org/abs/2301.09474)
    [6] (a. k. a. simple diffusivity model in the original paper motivated from the
    diffusion over latent structures).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从NodeFormer中学到的教训是，复杂性减少的关键在于矩阵乘积相对于注意力聚合的顺序。接下来，我们可以利用这个思想设计另一种高效的注意力函数，而无需任何随机近似，即[DIFFormer](https://arxiv.org/abs/2301.09474)中提到的简单注意力（在原始论文中也称为简单扩散模型，灵感来自于潜在结构上的扩散）。
- en: 'Our observation is from the Taylor expansion of the exponential function that
    can be used to motivate a new attention function:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的观察来源于指数函数的泰勒展开，这可以用来激发一个新的注意力函数：
- en: '![](../Images/051c6b20ac205f0926d02bafb714e690.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/051c6b20ac205f0926d02bafb714e690.png)'
- en: Notice that though the new attention function is motivated from the first-order
    Taylor expansion of e^x, it is not required to be a well-posed approximation for
    the original Softmax attention. That being said, we found it works stably well
    and in practice through extensive experiments.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，尽管新的注意力函数是从e^x的一级泰勒展开中激发出来的，但它并不要求是对原始Softmax注意力的良好近似。也就是说，我们发现它在实际应用中通过广泛的实验表现稳定良好。
- en: 'This new attention layer can be efficiently computed using linear complexity
    due to that we can inherit the trick to re-order the matrix product:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们可以继承重新排序矩阵乘积的技巧，这个新的注意力层可以通过线性复杂度高效计算：
- en: '![](../Images/4508eadbc004b0dd8432b338e1bc5e92.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4508eadbc004b0dd8432b338e1bc5e92.png)'
- en: Again, the two summation terms in the RHS are shared by all the nodes and thereby
    only need once computation. To clearly see the O(N) complexity, we can write down
    the computation flow with a matrix view.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，右侧的两个求和项被所有节点共享，因此只需要计算一次。为了清楚地看到O(N)复杂度，我们可以用矩阵视图写出计算流程。
- en: '![](../Images/af63ca103e8a4fb2d611e1b2cf22fcc7.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af63ca103e8a4fb2d611e1b2cf22fcc7.png)'
- en: 'The computation bottleneck of matrix products is marked by red color in the
    right part, inducing O(Nd²) complexity. Notice again that d is orders-of-magnitude
    smaller than N in practice: for example, d could range from 32 to 256, while N
    could be up to million or even billion.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘积的计算瓶颈在右侧的红色部分标记，导致O(Nd²)复杂度。再次注意，d在实际应用中通常比N小几个数量级：例如，d的范围可能从32到256，而N可以达到百万甚至十亿。
- en: The following shows the Pytorch implementation of one-layer DIFFormer’s simple
    attention, and the complete model implementation is publicly avaialble at [GitHub](https://github.com/qitianwu/DIFFormer/blob/main/node%20classification/difformer.py).
    In particular, when equipped with the simple attention, DIFFormer (a. k. a. DIFFormer-s
    in the original paper) can scale to large-scale graphs with millions of nodes.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是单层DIFFormer简单注意力的Pytorch实现，完整的模型实现公开可用，地址为[GitHub](https://github.com/qitianwu/DIFFormer/blob/main/node%20classification/difformer.py)。特别是，当配备简单注意力时，DIFFormer（在原始论文中也称为DIFFormer-s）可以扩展到具有百万节点的大规模图。
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: References
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] *Chengxuan Ying et al.,* [*Do Transformers Really Perform Bad for Graph
    Representation?*](https://arxiv.org/abs/2106.05234), NeurIPS 2021.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] *邢成轩等人,* [*变换器在图表示中真的表现不好吗？*](https://arxiv.org/abs/2106.05234), NeurIPS
    2021.'
- en: '[2] *Ladislav Rampášek et al.,* [*Recipe for a General, Powerful, Scalable
    Graph Transformer*](https://arxiv.org/abs/2205.12454), NeurIPS 2022.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] *拉迪斯拉夫·兰帕塞克等人,* [*通用、强大、可扩展的图形变换器的配方*](https://arxiv.org/abs/2205.12454),
    NeurIPS 2022.'
- en: '[3] *Jinwoo Kim et al.,* [*Pure Transformers are Powerful Graph Learners*](https://arxiv.org/abs/2207.02505),
    NeurIPS 2022.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] *金宇等人,* [*纯变换器是强大的图学习者*](https://arxiv.org/abs/2207.02505), NeurIPS 2022.'
- en: '[4] *Ashish Vaswani et al., Attention is All you Need, NeurIPS 2017.*'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] *阿什希什·瓦斯瓦尼等人,* Attention is All you Need, NeurIPS 2017.'
- en: '[5] *Qitian Wu et al.,* [*NodeFormer: A Scalable Graph Structure Learning Transformer
    for Node Classification*](https://openreview.net/pdf?id=sMezXGG5So)*, NeurIPS
    2022\.* This paper proposes an efficient Transformer for large node classification
    graphs. The key design is the kernelized Softmax message passing that achieves
    linear complexity w.r.t. number of nodes, and furthermore, the authors extend
    the kernel trick to Gumbel-Softmax that can learn sparse latent structures from
    a potentially all-pair connected graph.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] *吴启天等人,* [*NodeFormer: 一种可扩展的图结构学习 Transformer 用于节点分类*](https://openreview.net/pdf?id=sMezXGG5So)*,
    NeurIPS 2022\.* 本文提出了一种用于大规模节点分类图的高效 Transformer。关键设计是具有线性复杂度的核化 Softmax 消息传递，并且作者进一步将核技巧扩展到
    Gumbel-Softmax，从可能的全对连接图中学习稀疏潜在结构。'
- en: '[6] *Qitian Wu et al.,* [*DIFFormer: Scalable (Graph) Transformers Induced
    by Energy Constrained Diffusion*](https://arxiv.org/abs/2301.09474#)*, ICLR 2023.*
    This work designs a scalable graph Transformer whose attention functions are motivated
    from diffusivity estimates for diffusion over latent structures. In terms of the
    model architecture, DIFFormer generalizes the key idea used in NodeFormer for
    achieving O(N) complexity, and therefore, can be seen as the 2.0 version of NodeFormer.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] *吴启天等人,* [*DIFFormer: 由能量约束扩散引发的可扩展（图形）变换器*](https://arxiv.org/abs/2301.09474#)*,
    ICLR 2023.* 本工作设计了一种可扩展的图形 Transformer，其注意力函数来源于对潜在结构扩散的扩散性估计。在模型架构方面，DIFFormer
    将 NodeFormer 中用于实现 O(N) 复杂度的关键思想进行了泛化，因此可以被视为 NodeFormer 的 2.0 版本。'
- en: '[7] [*A round-up of linear transformers*](https://desh2608.github.io/2021-07-11-linear-transformers/)
    This blog introduces several typical strategies in recent efficient Transformers
    that successfully reduce the attention complexity to O(N), e.g., low-rank approximation,
    local-global attention and using softmax as a kernel.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] [*线性变换器综述*](https://desh2608.github.io/2021-07-11-linear-transformers/)
    本博客介绍了最近高效变换器中成功将注意力复杂度降低到 O(N) 的几种典型策略，例如低秩近似、局部-全局注意力以及将 softmax 用作核函数。'
- en: '[8] *Ali Rahimi and Benjamin Recht.* [*Random features for large-scale kernel
    machines*](https://papers.nips.cc/paper_files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf)*,
    NeurIPS 2007\.* This early work introduces random feature map as an effective
    approximation technique for computation over large numbers of data points, along
    with its theoretical properties.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] *阿里·拉希米和本杰明·雷赫特.* [*大规模核机器的随机特征*](https://papers.nips.cc/paper_files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf)*,
    NeurIPS 2007\.* 本早期工作介绍了随机特征图作为处理大量数据点的有效近似技术，以及其理论性质。'
- en: '[9] *Fanghui Liu et al.,* [*Random Features for Kernel Approximation- A Survey
    on Algorithms, Theory, and Beyond*](https://arxiv.org/abs/2004.11154)*, IEEE TPAMI
    2022\.* This survey summarizes an exhaustive set of different random features
    for kernel approximation and discusses their different properties and applicability.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] *刘方辉等人,* [*核近似中的随机特征- 算法、理论及其应用的综述*](https://arxiv.org/abs/2004.11154)*,
    IEEE TPAMI 2022\.* 本综述总结了不同随机特征在核近似中的全面集合，并讨论了它们的不同特性和适用性。'
- en: '*All images unless otherwise noted are by the author.*'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，否则所有图片均由作者提供。*'
