- en: 'Segment Anything 3D for Point Clouds: Complete Guide (SAM 3D)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Segment Anything 3D for Point Clouds: å®Œæ•´æŒ‡å— (SAM 3D)'
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/segment-anything-3d-for-point-clouds-complete-guide-sam-3d-80c06be99a18](https://towardsdatascience.com/segment-anything-3d-for-point-clouds-complete-guide-sam-3d-80c06be99a18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/segment-anything-3d-for-point-clouds-complete-guide-sam-3d-80c06be99a18](https://towardsdatascience.com/segment-anything-3d-for-point-clouds-complete-guide-sam-3d-80c06be99a18)
- en: 3D Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3D Python
- en: 'How to build a semantic segmentation application for 3D point clouds leveraging
    SAM and Python. Bonus: code for projections and relationships between 3D points
    and 2D pixels.'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¦‚ä½•åˆ©ç”¨ SAM å’Œ Python æ„å»º 3D ç‚¹äº‘çš„è¯­ä¹‰åˆ†å‰²åº”ç”¨ç¨‹åºã€‚é™„åŠ å†…å®¹ï¼šæŠ•å½±å’Œ 3D ç‚¹ä¸ 2D åƒç´ ä¹‹é—´å…³ç³»çš„ä»£ç ã€‚
- en: '[](https://medium.com/@florentpoux?source=post_page-----80c06be99a18--------------------------------)[![Florent
    Poux, Ph.D.](../Images/74df1e559b2edefba71ffd0d1294a251.png)](https://medium.com/@florentpoux?source=post_page-----80c06be99a18--------------------------------)[](https://towardsdatascience.com/?source=post_page-----80c06be99a18--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----80c06be99a18--------------------------------)
    [Florent Poux, Ph.D.](https://medium.com/@florentpoux?source=post_page-----80c06be99a18--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@florentpoux?source=post_page-----80c06be99a18--------------------------------)[![Florent
    Poux, Ph.D.](../Images/74df1e559b2edefba71ffd0d1294a251.png)](https://medium.com/@florentpoux?source=post_page-----80c06be99a18--------------------------------)[](https://towardsdatascience.com/?source=post_page-----80c06be99a18--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----80c06be99a18--------------------------------)
    [Florent Poux, Ph.D.](https://medium.com/@florentpoux?source=post_page-----80c06be99a18--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----80c06be99a18--------------------------------)
    Â·27 min readÂ·Dec 13, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----80c06be99a18--------------------------------)
    Â·é˜…è¯»æ—¶é—´ 27 åˆ†é’ŸÂ·2023å¹´12æœˆ13æ—¥
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/cbaf694227e7e6b933f01064d27a96d6.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cbaf694227e7e6b933f01064d27a96d6.png)'
- en: The Segment Anything Model for 3D Environments. We will detect objects in indoor
    spaces using 3D point cloud datasets. Credit goes to [Mimatelier](https://linktr.ee/mimatelier),
    the talented illustrator who created this image.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: é’ˆå¯¹ 3D ç¯å¢ƒçš„ Segment Anything æ¨¡å‹ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ 3D ç‚¹äº‘æ•°æ®é›†æ£€æµ‹å®¤å†…ç©ºé—´çš„ç‰©ä½“ã€‚ç‰¹åˆ«é¸£è°¢ [Mimatelier](https://linktr.ee/mimatelier)ï¼Œè¿™ä½æ‰åæ¨ªæº¢çš„æ’ç”»å¸ˆåˆ›ä½œäº†è¿™å¼ å›¾ç‰‡ã€‚
- en: Technological leaps are just plain crazy, especially looking at Artificial Intelligence
    (AI) applied to 3D challenges. Having the ability to leverage the latest cutting-edge
    research for advanced 3D applications is very empowering. Especially when looking
    at bringing human-level reasoning capabilities to a computer, there is a clear
    need to extract a formalized meaning from the 3D entities that we observe.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ç§‘æŠ€çš„é£è·ƒçœŸæ˜¯ç–¯ç‹‚ï¼Œç‰¹åˆ«æ˜¯çœ‹åˆ°äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åº”ç”¨äº 3D æŒ‘æˆ˜æ—¶ã€‚èƒ½å¤Ÿåˆ©ç”¨æœ€æ–°çš„å‰æ²¿ç ”ç©¶æ¥è¿›è¡Œé«˜çº§ 3D åº”ç”¨æ˜¯éå¸¸ä»¤äººæŒ¯å¥‹çš„ã€‚å°¤å…¶æ˜¯åœ¨å°†äººç±»çº§åˆ«çš„æ¨ç†èƒ½åŠ›å¸¦å…¥è®¡ç®—æœºæ—¶ï¼Œæ˜ç¡®ä»æˆ‘ä»¬è§‚å¯Ÿåˆ°çš„
    3D å®ä½“ä¸­æå–å‡ºæ­£å¼åŒ–çš„æ„ä¹‰æ˜¾å¾—å°¤ä¸ºé‡è¦ã€‚
- en: In this tutorial, we are here to make sure that we can bind amazing AI advancements
    with 3D applications that make use of 3D Point Clouds. â€” *ğŸ²* **Florent & Ville**
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨å°†ä»¤äººæƒŠå¹çš„äººå·¥æ™ºèƒ½è¿›å±•ä¸åˆ©ç”¨ 3D ç‚¹äº‘çš„ 3D åº”ç”¨ç¨‹åºç»“åˆèµ·æ¥ã€‚â€” *ğŸ²* **Florent & Ville**
- en: This is no easy feat, but once mastered, the fusion of 3D point clouds and deep
    learning gives birth to new dimensions of understanding and interpreting our visual
    world.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸æ˜¯ä¸€ä»¶å®¹æ˜“çš„äº‹ï¼Œä½†ä¸€æ—¦æŒæ¡ï¼Œ3D ç‚¹äº‘ä¸æ·±åº¦å­¦ä¹ çš„èåˆå°†å¼€è¾Ÿå¯¹æˆ‘ä»¬è§†è§‰ä¸–ç•Œçš„æ–°ç»´åº¦çš„ç†è§£å’Œè§£é‡Šã€‚
- en: '![](../Images/46849f11c579adff452243fd483b5eef.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/46849f11c579adff452243fd483b5eef.png)'
- en: Artificial Intelligence ft. 3D point clouds. Â© [F. Poux](https://learngeodata.eu/)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: äººå·¥æ™ºèƒ½ä¸ 3D ç‚¹äº‘ã€‚Â© [F. Poux](https://learngeodata.eu/)
- en: Among these advancements, the Segment Anything Model is a recent beacon of innovation,
    especially for full automation without supervision.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™äº›è¿›å±•ä¸­ï¼ŒSegment Anything æ¨¡å‹æ˜¯æœ€è¿‘çš„åˆ›æ–°ç¯å¡”ï¼Œå°¤å…¶æ˜¯åœ¨æ— éœ€ç›‘ç£çš„å…¨è‡ªåŠ¨åŒ–æ–¹é¢ã€‚
- en: '![](../Images/1b49832a749ed53fb71db522ff96117b.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b49832a749ed53fb71db522ff96117b.png)'
- en: The Segment Anything Model Architecture that we use for 3D Data. It comprises
    an image encoder, image embeddings, and some pre-processing operations to finally
    pass into the decoder and prompt encoder, giving the results as masks. Â© [F. Poux](https://learngeodata.eu/)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç”¨äº 3D æ•°æ®çš„ Segment Anything æ¨¡å‹æ¶æ„ã€‚å®ƒåŒ…æ‹¬ä¸€ä¸ªå›¾åƒç¼–ç å™¨ã€å›¾åƒåµŒå…¥ä»¥åŠä¸€äº›é¢„å¤„ç†æ“ä½œï¼Œæœ€åä¼ é€’ç»™è§£ç å™¨å’Œæç¤ºç¼–ç å™¨ï¼Œè¾“å‡ºç»“æœä¸ºæ©æ¨¡ã€‚Â©
    [F. Poux](https://learngeodata.eu/)
- en: In this ultimate guide, we embark on a pragmatic voyage to explore this cutting-edge
    model, from its inception to its practical segmentation applications. But what
    is the objective here?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä»½**ç»ˆææŒ‡å—**ä¸­ï¼Œæˆ‘ä»¬å°†å¼€å§‹ä¸€ä¸ªå®é™…çš„æ—…ç¨‹ï¼Œä»æ¨¡å‹çš„è¯ç”Ÿåˆ°å®é™…åˆ†å‰²åº”ç”¨ã€‚ä½†è¿™é‡Œçš„ç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ
- en: The Mission ğŸ¥·
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»»åŠ¡ ğŸ¥·
- en: 'Okay, itâ€™s time for the mission brief! You are now a multi-class member of
    your countryâ€™s special forces, and you must find some dangerous materials hidden
    inside a specific building without ever being detected (here: the ITC building).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½äº†ï¼Œä»»åŠ¡ç®€æŠ¥æ—¶é—´åˆ°äº†ï¼ä½ ç°åœ¨æ˜¯ä½ å›½å®¶ç‰¹ç§éƒ¨é˜Ÿçš„å¤šç±»æˆå‘˜ï¼Œä½ å¿…é¡»åœ¨ä¸è¢«å‘ç°çš„æƒ…å†µä¸‹æ‰¾åˆ°éšè—åœ¨ç‰¹å®šå»ºç­‘ç‰©ä¸­çš„å±é™©ææ–™ï¼ˆè¿™é‡Œæ˜¯ITCå¤§æ¥¼ï¼‰ã€‚
- en: With your superb internet hacking skills, you manage to find the 3D scans for
    that part of the building you are interested in. You now need to find a way to
    define the path for your dangerous material recovery team quickly. After that,
    the team can proceed unnoticed to recover the materials, and you have saved the
    day!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ©ç”¨ä½ å‡ºè‰²çš„äº’è”ç½‘é»‘å®¢æŠ€èƒ½ï¼Œä½ æˆåŠŸæ‰¾åˆ°äº†ä½ æ„Ÿå…´è¶£çš„å»ºç­‘éƒ¨åˆ†çš„3Dæ‰«æã€‚ä½ ç°åœ¨éœ€è¦å¿«é€Ÿæ‰¾åˆ°å®šä¹‰ä½ å±é™©ææ–™å›æ”¶é˜Ÿçš„è·¯å¾„çš„æ–¹æ³•ã€‚ä¹‹åï¼Œé˜Ÿä¼å¯ä»¥åœ¨ä¸è¢«å¯Ÿè§‰çš„æƒ…å†µä¸‹è¿›è¡Œææ–™å›æ”¶ï¼Œä½ ä¹Ÿå°±æˆåŠŸæ‹¯æ•‘äº†ä¸–ç•Œï¼
- en: After careful research and using your various skills, you develop a 3D data
    processing workflow that involves setting up a 3D Python code environment to process
    the 3D point cloud by using the Segment Anything Model to highlight the composition
    of the scene, as shown below.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç»è¿‡ä»”ç»†ç ”ç©¶å¹¶åˆ©ç”¨ä½ çš„å„ç§æŠ€èƒ½ï¼Œä½ å¼€å‘äº†ä¸€ä¸ª3Dæ•°æ®å¤„ç†å·¥ä½œæµç¨‹ï¼Œå…¶ä¸­åŒ…æ‹¬è®¾ç½®3D Pythonä»£ç ç¯å¢ƒï¼Œé€šè¿‡Segment Anythingæ¨¡å‹å¤„ç†3Dç‚¹äº‘ï¼Œä»¥çªå‡ºåœºæ™¯çš„ç»„æˆï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚
- en: '![](../Images/640db9a177c74e1f31baa1f69ea491f6.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/640db9a177c74e1f31baa1f69ea491f6.png)'
- en: The workflow for Segment Anything 3D. We have five main steps (3D Project Setup,
    Segment Anything Model, 3D Point Cloud Projections, Unsupervised Segmentation,
    and Qualitative Analysis) further refined in the substeps, as highlighted. Â© [F.
    Poux](https://learngeodata.eu/)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything 3Dçš„å·¥ä½œæµç¨‹ã€‚æˆ‘ä»¬æœ‰äº”ä¸ªä¸»è¦æ­¥éª¤ï¼ˆ3Dé¡¹ç›®è®¾ç½®ã€Segment Anythingæ¨¡å‹ã€3Dç‚¹äº‘æŠ•å½±ã€æ— ç›‘ç£åˆ†å‰²å’Œå®šæ€§åˆ†æï¼‰ï¼Œå¹¶åœ¨ä¸‹å›¾ä¸­è¿›ä¸€æ­¥ç»†åŒ–ã€‚Â©
    [F. Poux](https://learngeodata.eu/)
- en: This will allow you to produce a 3D semantic map that will permit pinpointing
    the location of the materials within ninety minutes before the team is on-site.
    Are you ready?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†å…è®¸ä½ ç”Ÿæˆä¸€ä¸ª3Dè¯­ä¹‰åœ°å›¾ï¼Œåœ¨é˜Ÿä¼åˆ°è¾¾ç°åœºä¹‹å‰çš„ä¹ååˆ†é’Ÿå†…ï¼Œèƒ½å¤Ÿå‡†ç¡®å®šä½ææ–™çš„ä½ç½®ã€‚ä½ å‡†å¤‡å¥½äº†å—ï¼Ÿ
- en: 'ğŸµ**Note to Readers***: This hands-on guide is part of a* [***UTWENTE***](https://www.itc.nl/)
    *joint work with co-authors* ***F. Poux*** *and* ***V. Lehtola****. We acknowledge
    the financial contribution from the digital twins* [*@ITC*](http://twitter.com/ITC)
    *-project granted by the ITC faculty of the University of Twente.*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğŸµ**è¯»è€…æ³¨æ„**: è¿™ä¸ªå®è·µæŒ‡å—æ˜¯* [***UTWENTE***](https://www.itc.nl/) *ä¸åˆè‘—è€…***F. Poux***
    *å’Œ***V. Lehtola*** *çš„è”åˆå·¥ä½œçš„ä¸€éƒ¨åˆ†ã€‚æˆ‘ä»¬æ„Ÿè°¢æ¥è‡ªæ•°å­—åŒèƒèƒ* [*@ITC*](http://twitter.com/ITC) *é¡¹ç›®çš„èµ„åŠ©ï¼Œè¯¥é¡¹ç›®ç”±ç‰¹æ¸©ç‰¹å¤§å­¦ITCå­¦é™¢æˆäºˆã€‚*'
- en: 1\. 3D Project Setup
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 3Dé¡¹ç›®è®¾ç½®
- en: Before we dive into the marvels of the Segment Anything Model, it is crucial
    to establish a robust foundation. Setting up the appropriate environment ensures
    smooth sailing throughout our journey, allowing seamless experimentation and exploration.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬æ·±å…¥æ¢è®¨Segment Anythingæ¨¡å‹çš„å¥‡è¿¹ä¹‹å‰ï¼Œå»ºç«‹ä¸€ä¸ªç¨³å›ºçš„åŸºç¡€è‡³å…³é‡è¦ã€‚è®¾ç½®é€‚å½“çš„ç¯å¢ƒå¯ä»¥ç¡®ä¿æˆ‘ä»¬åœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­é¡ºåˆ©è¿›è¡Œï¼Œä»è€Œå®ç°æ— ç¼çš„å®éªŒå’Œæ¢ç´¢ã€‚
- en: At this stage, we want to ensure that our coding environment is correctly set
    with robust libraries. Ready?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬è¦ç¡®ä¿æˆ‘ä»¬çš„ç¼–ç ç¯å¢ƒè®¾ç½®æ­£ç¡®ï¼Œå¹¶ä¸”å…·æœ‰å¼ºå¤§çš„åº“ã€‚å‡†å¤‡å¥½äº†å—ï¼Ÿ
- en: 'ğŸ¤  **Ville**: *This is before the action starts. Please reserve an hour or two
    separately for this if you are doing it from scratch and, e.g., might need to
    update CUDA drivers. Youâ€™ll be downloading gigabytes of stuff.*'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğŸ¤  **Ville**: *è¿™æ˜¯åœ¨è¡ŒåŠ¨å¼€å§‹ä¹‹å‰ã€‚å¦‚æœä½ æ˜¯ä»é›¶å¼€å§‹åšè¿™ä»¶äº‹ï¼Œæ¯”å¦‚å¯èƒ½éœ€è¦æ›´æ–°CUDAé©±åŠ¨ç¨‹åºï¼Œè¯·å•ç‹¬é¢„ç•™ä¸€ä¸ªæˆ–ä¸¤ä¸ªå°æ—¶ã€‚ä½ å°†ä¸‹è½½å‡ GBçš„å†…å®¹ã€‚*'
- en: '![](../Images/dce37818ba43403acd021f11131b0685.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dce37818ba43403acd021f11131b0685.png)'
- en: The 3D Project Setup. We first set up the environment, on which we attach base
    libraries, deep learning libraries, and the IDE Setup. Â© [F. Poux](https://learngeodata.eu/)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 3Dé¡¹ç›®è®¾ç½®ã€‚æˆ‘ä»¬é¦–å…ˆè®¾ç½®ç¯å¢ƒï¼Œç„¶åé™„åŠ åŸºç¡€åº“ã€æ·±åº¦å­¦ä¹ åº“å’ŒIDEè®¾ç½®ã€‚Â© [F. Poux](https://learngeodata.eu/)
- en: 1.1\. 3D Code Environment setup
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1\. 3D ä»£ç ç¯å¢ƒè®¾ç½®
- en: 'It is time to get our hands in the dirt! We aim to use the Segment Anything
    Model to Semantically Segment a 3D Point Cloud. And that is no easy feat! So,
    of course, the first reflex is to check out the segment anything dependencies:
    [Access SAM Github](https://github.com/facebookresearch/segment-anything).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ˜¯æ—¶å€™åŠ¨æ‰‹äº†ï¼æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä½¿ç”¨ Segment Anything Model å¯¹ 3D ç‚¹äº‘è¿›è¡Œè¯­ä¹‰åˆ†å‰²ã€‚è¿™ç»éæ˜“äº‹ï¼æ‰€ä»¥ï¼Œé¦–å…ˆçš„ååº”æ˜¯æŸ¥çœ‹ Segment
    Anything çš„ä¾èµ–é¡¹ï¼š[è®¿é—® SAM Github](https://github.com/facebookresearch/segment-anything)ã€‚
- en: 'From there, we check out the necessary pre-requisites of the package:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä»é‚£é‡Œï¼Œæˆ‘ä»¬æ£€æŸ¥åŒ…çš„å¿…è¦å‰ææ¡ä»¶ï¼š
- en: '![](../Images/9dd556baea6fef87e6c02751f663835a.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9dd556baea6fef87e6c02751f663835a.png)'
- en: The dependencies highlighted in Segment Anything.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything ä¸­çªå‡ºçš„ä¾èµ–é¡¹ã€‚
- en: 'ğŸ¦Š **Florent**: *Whenever you are dealing with deep learning libraries or deep
    learning new research code, it is essential to check out the dependencies and
    installation recommendations. Indeed, this will strongly influence the follow-up
    of your experiments and the time needed for replication.*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¦Š **Florent**ï¼š*æ— è®ºä½•æ—¶å¤„ç†æ·±åº¦å­¦ä¹ åº“æˆ–æ·±åº¦å­¦ä¹ çš„æ–°ç ”ç©¶ä»£ç ï¼Œéƒ½å¿…é¡»æ£€æŸ¥ä¾èµ–é¡¹å’Œå®‰è£…å»ºè®®ã€‚è¿™å°†æå¤§åœ°å½±å“å®éªŒçš„åç»­è¿›å±•å’Œå¤åˆ¶æ‰€éœ€çš„æ—¶é—´ã€‚*
- en: 'As you can see, we need to use the following library version:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ä»¥ä¸‹åº“ç‰ˆæœ¬ï¼š
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now that this is out, we will generate a virtual environment to ensure smooth
    sailing! If you want a detailed view of the process, I recommend you jump aboard
    the following guide:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è¿™ä¸€ç‚¹æå®šåï¼Œæˆ‘ä»¬å°†ç”Ÿæˆä¸€ä¸ªè™šæ‹Ÿç¯å¢ƒä»¥ç¡®ä¿é¡ºåˆ©è¿›è¡Œï¼å¦‚æœä½ æƒ³è¯¦ç»†äº†è§£è¿™ä¸€è¿‡ç¨‹ï¼Œå»ºè®®ä½ æŸ¥çœ‹ä»¥ä¸‹æŒ‡å—ï¼š
- en: '[](/3d-python-workflows-for-lidar-point-clouds-100ff40e4ff0?source=post_page-----80c06be99a18--------------------------------)
    [## 3D Python Workflows for LiDAR City Models: A Step-by-Step Guide'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/3d-python-workflows-for-lidar-point-clouds-100ff40e4ff0?source=post_page-----80c06be99a18--------------------------------)
    [## 3D Python Workflows for LiDAR City Models: A Step-by-Step Guide'
- en: The Ultimate Guide to unlocking a streamlined workflow for 3D City Modelling
    Applications. The tutorial covers Pythonâ€¦
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è§£é” 3D åŸå¸‚å»ºæ¨¡åº”ç”¨ç¨‹åºçš„ç²¾ç®€å·¥ä½œæµçš„ç»ˆææŒ‡å—ã€‚æ•™ç¨‹æ¶µç›–äº† Pythonâ€¦
- en: towardsdatascience.com](/3d-python-workflows-for-lidar-point-clouds-100ff40e4ff0?source=post_page-----80c06be99a18--------------------------------)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/3d-python-workflows-for-lidar-point-clouds-100ff40e4ff0?source=post_page-----80c06be99a18--------------------------------)
- en: But, not to keep you high and dry, here is another strategy for a quick and
    lightweight setup using [Miniconda](https://docs.conda.io/projects/miniconda/en/latest/miniconda-install.html).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä¸è®©ä½ æ„Ÿåˆ°æ— åŠ©ï¼Œä¸‹é¢æ˜¯å¦ä¸€ç§å¿«é€Ÿè½»é‡çº§çš„è®¾ç½®ç­–ç•¥ï¼Œä½¿ç”¨ [Miniconda](https://docs.conda.io/projects/miniconda/en/latest/miniconda-install.html)ã€‚
- en: 'ğŸ’¡ **Note**: *Miniconda is a free minimal installer for conda. It is a â€œminiatureâ€
    version of Anaconda that includes only a minimal amount of dependencies. These
    are the conda package manager, a Python version, the packages they both depend
    on and other valuable packages like pip and zlib. This allows us only to install
    what we need in a lightweight manner.*'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ **æ³¨æ„**ï¼š*Miniconda æ˜¯ä¸€ä¸ªå…è´¹çš„ Conda æœ€å°å®‰è£…ç¨‹åºã€‚å®ƒæ˜¯ Anaconda çš„â€œå¾®å‹â€ç‰ˆæœ¬ï¼Œä»…åŒ…å«æœ€å°‘çš„ä¾èµ–é¡¹ã€‚è¿™äº›åŒ…æ‹¬ Conda
    åŒ…ç®¡ç†å™¨ã€ä¸€ä¸ª Python ç‰ˆæœ¬ã€å®ƒä»¬æ‰€ä¾èµ–çš„åŒ…ä»¥åŠå…¶ä»–æœ‰ä»·å€¼çš„åŒ…å¦‚ pip å’Œ zlibã€‚è¿™ä½¿å¾—æˆ‘ä»¬å¯ä»¥ä»¥è½»é‡çš„æ–¹å¼åªå®‰è£…æˆ‘ä»¬éœ€è¦çš„ä¸œè¥¿ã€‚*
- en: 'ğŸ¤ **Ville***: The cool stuff about virtual environments is that you can export
    it and run your code as-is on powerful Linux computing machines and superclusters!
    This is very handy for training networks!*'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤ **Ville***ï¼šè™šæ‹Ÿç¯å¢ƒçš„é…·ç‚«ä¹‹å¤„åœ¨äºï¼Œä½ å¯ä»¥å°†å…¶å¯¼å‡ºå¹¶åœ¨å¼ºå¤§çš„ Linux è®¡ç®—æœºå’Œè¶…çº§é›†ç¾¤ä¸Šç›´æ¥è¿è¡Œä½ çš„ä»£ç ï¼è¿™å¯¹äºè®­ç»ƒç½‘ç»œéå¸¸æœ‰ç”¨ï¼*
- en: After downloading a version of Miniconda from [here](https://docs.conda.io/projects/miniconda/en/latest/miniconda-other-installer-links.html)
    for your OS (I recommend you choose a Python 3.9 or 3.10 version to ensure proper
    compatibility with packages), you can install it following the various steps of
    the installation process.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä» [è¿™é‡Œ](https://docs.conda.io/projects/miniconda/en/latest/miniconda-other-installer-links.html)
    ä¸‹è½½é€‚åˆä½ æ“ä½œç³»ç»Ÿçš„ Miniconda ç‰ˆæœ¬åï¼ˆå»ºè®®é€‰æ‹© Python 3.9 æˆ– 3.10 ç‰ˆæœ¬ä»¥ç¡®ä¿ä¸åŒ…çš„å…¼å®¹æ€§ï¼‰ï¼Œä½ å¯ä»¥æŒ‰ç…§å®‰è£…è¿‡ç¨‹ä¸­çš„å„ç§æ­¥éª¤è¿›è¡Œå®‰è£…ã€‚
- en: '![](../Images/0f94957e88eef5951bad35f2d5ceaa61.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f94957e88eef5951bad35f2d5ceaa61.png)'
- en: The miniconda installer window. Â© [F. Poux](https://learngeodata.eu/)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: miniconda å®‰è£…ç¨‹åºçª—å£ã€‚Â© [F. Poux](https://learngeodata.eu/)
- en: 'And that is it! You now have secured the most uncomplicated Python installation
    with the lightweight miniconda that will make isolating a controlled virtual environment
    super easy. Before moving on to the following steps, we launch miniconda with
    its command line access:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å°±è¿™æ ·ï¼ä½ ç°åœ¨å·²ç»å®Œæˆäº†æœ€ç®€å•çš„ Python å®‰è£…ï¼Œä½¿ç”¨è½»é‡çº§çš„ miniconda ä½¿å¾—éš”ç¦»å—æ§çš„è™šæ‹Ÿç¯å¢ƒå˜å¾—éå¸¸å®¹æ˜“ã€‚åœ¨ç»§ç»­ä¸‹ä¸€æ­¥ä¹‹å‰ï¼Œæˆ‘ä»¬å¯åŠ¨
    miniconda åŠå…¶å‘½ä»¤è¡Œè®¿é—®ï¼š
- en: '![](../Images/df2bb60a544049eca1139b2b848577df.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df2bb60a544049eca1139b2b848577df.png)'
- en: In Windows, just searching â€œminicondaâ€ should yield this
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ Windows ä¸­ï¼Œåªéœ€æœç´¢â€œminicondaâ€å³å¯æ‰¾åˆ°
- en: Once in the Anaconda Prompt, we follow a simple four-step process to be up and
    running, as shown below.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦è¿›å…¥ Anaconda æç¤ºç¬¦ï¼Œæˆ‘ä»¬æŒ‰ç…§ä¸‹é¢æ˜¾ç¤ºçš„ç®€å•å››æ­¥è¿‡ç¨‹è¿›è¡Œæ“ä½œã€‚
- en: '![](../Images/1cdedcd8fdd91ee19cf54b3a8cb8ca12.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1cdedcd8fdd91ee19cf54b3a8cb8ca12.png)'
- en: The Workflow to set up a Python environment for 3D Segment Anything Model. Â©
    [F. Poux](https://learngeodata.eu/)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ç½® Python ç¯å¢ƒä»¥ä½¿ç”¨ 3D Segment Anything Model çš„å·¥ä½œæµç¨‹ã€‚Â© [F. Poux](https://learngeodata.eu/)
- en: 'To create a new environment, we write the line: `conda create -n GEOSAM python=3.10`'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¦åˆ›å»ºæ–°ç¯å¢ƒï¼Œæˆ‘ä»¬è¾“å…¥ï¼š`conda create -n GEOSAM python=3.10`
- en: 'To switch to the newly created environment, we write: `conda activate GEOSAM`'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆ‡æ¢åˆ°æ–°åˆ›å»ºçš„ç¯å¢ƒï¼Œæˆ‘ä»¬è¾“å…¥ï¼š`conda activate GEOSAM`
- en: 'To check the Python version, `python --version`, and the installed packages:
    `conda list`. This should yield Python 3.10 and the list of base libraries, respectively.'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¦æ£€æŸ¥ Python ç‰ˆæœ¬ï¼Œè¾“å…¥ `python --version`ï¼Œæ£€æŸ¥å·²å®‰è£…çš„è½¯ä»¶åŒ…ï¼š`conda list`ã€‚è¿™åº”åˆ†åˆ«æ˜¾ç¤º Python 3.10
    å’ŒåŸºç¡€åº“åˆ—è¡¨ã€‚
- en: 'To install pip in the new environment, we write: `conda install pip`'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨æ–°ç¯å¢ƒä¸­å®‰è£… pipï¼Œæˆ‘ä»¬è¾“å…¥ï¼š`conda install pip`
- en: And that is it! We are now ready to move on installing the necessary libraries
    for playing with SAM.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å°±è¿™äº›ï¼æˆ‘ä»¬ç°åœ¨å‡†å¤‡å®‰è£…å¿…è¦çš„åº“ä»¥è¿›è¡Œ SAM çš„æ“ä½œã€‚
- en: '[](https://learngeodata.eu/3d-newsletter/?source=post_page-----80c06be99a18--------------------------------)
    [## 3D Innovator Newsletter'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 3D åˆ›æ–°è€…é€šè®¯'
- en: Weekly practical content, insights, code and resources to master 3D Data Science.
    I write about Point Clouds, AIâ€¦
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¯å‘¨æä¾›å®ç”¨å†…å®¹ã€è§è§£ã€ä»£ç å’Œèµ„æºï¼ŒæŒæ¡ 3D æ•°æ®ç§‘å­¦ã€‚æˆ‘å†™å…³äºç‚¹äº‘ã€äººå·¥æ™ºèƒ½â€¦â€¦
- en: learngeodata.eu](https://learngeodata.eu/3d-newsletter/?source=post_page-----80c06be99a18--------------------------------)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[learngeodata.eu](https://learngeodata.eu/3d-newsletter/?source=post_page-----80c06be99a18--------------------------------)'
- en: 1.2\. Base library
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2\. åŸºç¡€åº“
- en: '![](../Images/06b0547b42b6f690713e39f65c151892.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06b0547b42b6f690713e39f65c151892.png)'
- en: The base libraries used in this tutorial (Numpy, Matplotlib, Laspy). Â© [F. Poux](https://learngeodata.eu/)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ•™ç¨‹ä¸­ä½¿ç”¨çš„åŸºç¡€åº“ï¼ˆNumpyã€Matplotlibã€Laspyï¼‰ã€‚Â© [F. Poux](https://learngeodata.eu/)
- en: 'We now install our base libraries for using SAM: `NumPy`, `LasPy`, `OpenCV`,
    and `Matplotlib`. `NumPy` may be the most recommended library for numerical computations,
    `OpenCV` is used for computer vision tasks, `Laspy` deals with processing LIDAR
    data, and `Matplotlib` is a plotting and data visualization library in Python.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å®‰è£…ç”¨äº SAM çš„åŸºç¡€åº“ï¼š`NumPy`ã€`LasPy`ã€`OpenCV` å’Œ `Matplotlib`ã€‚`NumPy` å¯èƒ½æ˜¯æœ€æ¨èçš„æ•°å€¼è®¡ç®—åº“ï¼Œ`OpenCV`
    ç”¨äºè®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œ`Laspy` å¤„ç† LIDAR æ•°æ®ï¼Œè€Œ `Matplotlib` æ˜¯ä¸€ä¸ªç»˜å›¾å’Œæ•°æ®å¯è§†åŒ–åº“ã€‚
- en: 'ğŸ¦Š **Florent**: *These libraries are the base and robust cornerstones of any
    3D project. If you want to deepen their understanding, I suggest you dive into*
    [*this tutorial*](https://medium.com/towards-data-science/how-to-automate-lidar-point-cloud-processing-with-python-a027454a536c)
    *that explores its dark depths* ğŸª¸.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¦Š **Florent**ï¼š*è¿™äº›åº“æ˜¯ä»»ä½• 3D é¡¹ç›®çš„åŸºç¡€å’Œåšå®çš„åŸºçŸ³ã€‚å¦‚æœä½ æƒ³æ·±å…¥äº†è§£å®ƒä»¬ï¼Œæˆ‘å»ºè®®ä½ å»* [*è¿™ä¸ªæ•™ç¨‹*](https://medium.com/towards-data-science/how-to-automate-lidar-point-cloud-processing-with-python-a027454a536c)
    *ï¼Œå®ƒæ¢è®¨äº†å…¶æ·±å¥¥çš„å†…å®¹* ğŸª¸ã€‚
- en: 'To install these libraries, we can use pip in one single line:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å®‰è£…è¿™äº›åº“ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸€è¡Œä»£ç é€šè¿‡ pipï¼š
- en: '[PRE1]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This is great; it's time for the deep learning libraries setup!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¥½ï¼›æ˜¯æ—¶å€™è®¾ç½®æ·±åº¦å­¦ä¹ åº“äº†ï¼
- en: 1.2 Deep Learning Libraries
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 æ·±åº¦å­¦ä¹ åº“
- en: '![](../Images/50677cd193ffb1d9e37ce1fb9b27b84e.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/50677cd193ffb1d9e37ce1fb9b27b84e.png)'
- en: The deep learning libraries.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦å­¦ä¹ åº“ã€‚
- en: 'We will now look into installing deep-learning libraries. And, of course, the
    first one that we explore is my favorite one so far: Pytorch. Since its launch
    in 2017, Pytorch has improved its flexibility and hackability as a priority and
    performance as a close second. Therefore, today, using Pytorch for Deep Learning
    applications is excellent if you want (1) high-performance execution, (2) Pythonic
    internals, and (3) good abstractions for valuable tasks.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å°†ç€æ‰‹å®‰è£…æ·±åº¦å­¦ä¹ åº“ã€‚å½“ç„¶ï¼Œæˆ‘ä»¬é¦–å…ˆæ¢ç´¢çš„æ˜¯æˆ‘è¿„ä»Šä¸ºæ­¢æœ€å–œæ¬¢çš„ï¼šPytorchã€‚è‡ª 2017 å¹´æ¨å‡ºä»¥æ¥ï¼ŒPytorch ä¼˜å…ˆè€ƒè™‘å…¶çµæ´»æ€§å’Œå¯é»‘å®¢æ€§ï¼Œå…¶æ¬¡æ˜¯æ€§èƒ½ã€‚å› æ­¤ï¼Œä»Šå¤©ï¼Œä½¿ç”¨
    Pytorch è¿›è¡Œæ·±åº¦å­¦ä¹ åº”ç”¨æ˜¯ç»ä½³çš„é€‰æ‹©ï¼Œå¦‚æœä½ éœ€è¦ (1) é«˜æ€§èƒ½æ‰§è¡Œï¼Œ(2) Pythonic å†…éƒ¨å®ç°ï¼Œä»¥åŠ (3) æœ‰ä»·å€¼ä»»åŠ¡çš„è‰¯å¥½æŠ½è±¡ã€‚
- en: 'ğŸ¦Š **Florent**: *Since 2017, Hardware accelerators (such as GPUs) have become
    ~15x faster in computing tasks. You can only guess what is to come in the next
    few years. Therefore, it is essential to be on the lookout for flexible libraries
    that can move quickly, even on refactoring â€œinternalsâ€ to languages such as C++,
    like Pytorch does.*'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğŸ¦Š **Florent**: *è‡ª2017å¹´ä»¥æ¥ï¼Œç¡¬ä»¶åŠ é€Ÿå™¨ï¼ˆå¦‚GPUï¼‰åœ¨è®¡ç®—ä»»åŠ¡ä¸­çš„é€Ÿåº¦æé«˜äº†çº¦15å€ã€‚ä½ åªèƒ½çŒœæµ‹æ¥ä¸‹æ¥å‡ å¹´ä¼šå‘ç”Ÿä»€ä¹ˆã€‚å› æ­¤ï¼Œå¿…é¡»å…³æ³¨çµæ´»çš„åº“ï¼Œå®ƒä»¬å¯ä»¥å¿«é€Ÿé€‚åº”ï¼Œç”šè‡³å¯¹â€œå†…éƒ¨â€è¿›è¡Œé‡æ„ï¼Œå¦‚Pytorchæ‰€åšçš„é‚£æ ·ã€‚*'
- en: 'ğŸ¤ **Ville***: SAM authors recommend using a GPU with 8GB memory. However, we
    give some tips on how to do the tutorial with less memory. Use them if you get
    â€˜MemoryErrorâ€™ or â€˜Out-of-bounds memory accessâ€™ or â€˜Illegal memory accessâ€™ messages.
    I got it working with 6GB.*'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğŸ¤ **Ville***: SAMä½œè€…æ¨èä½¿ç”¨8GBå†…å­˜çš„GPUã€‚ç„¶è€Œï¼Œæˆ‘ä»¬æä¾›äº†ä¸€äº›å¦‚ä½•åœ¨å†…å­˜è¾ƒå°‘çš„æƒ…å†µä¸‹è¿›è¡Œæ•™ç¨‹çš„æŠ€å·§ã€‚å¦‚æœä½ æ”¶åˆ°â€˜MemoryErrorâ€™æˆ–â€˜Out-of-bounds
    memory accessâ€™æˆ–â€˜Illegal memory accessâ€™æ¶ˆæ¯ï¼Œè¯·ä½¿ç”¨è¿™äº›æŠ€å·§ã€‚æˆ‘ä½¿ç”¨6GBå†…å­˜æˆåŠŸè¿è¡Œäº†å®ƒã€‚*'
- en: To install a relevant distribution of Pytorch without headaches figuring out
    how to install CUDA (which is not so straightforward), they made a straightforward
    web app that generates the code to copy and paste into your command line. For
    this, you can jump on this [Pytorch Getting Started page](https://pytorch.org/get-started/locally/)
    and select the most relevant way to install your distribution, as shown below.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ— å¿§åœ°å®‰è£…Pytorchçš„ç›¸å…³å‘è¡Œç‰ˆï¼Œè€Œä¸å¿…ä¸ºå¦‚ä½•å®‰è£…CUDAï¼ˆè¿™å¹¶ä¸ç®€å•ï¼‰è€Œçƒ¦æ¼ï¼Œä»–ä»¬åˆ¶ä½œäº†ä¸€ä¸ªç®€å•çš„ç½‘é¡µåº”ç”¨ç¨‹åºï¼Œç”Ÿæˆä»£ç ä»¥å¤åˆ¶å¹¶ç²˜è´´åˆ°ä½ çš„å‘½ä»¤è¡Œä¸­ã€‚ä¸ºæ­¤ï¼Œä½ å¯ä»¥è®¿é—®è¿™ä¸ª
    [Pytorchå…¥é—¨é¡µé¢](https://pytorch.org/get-started/locally/) å¹¶é€‰æ‹©æœ€ç›¸å…³çš„å®‰è£…æ–¹å¼ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚
- en: '![](../Images/a8cda5c93dd1245d31400555bddb4216.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8cda5c93dd1245d31400555bddb4216.png)'
- en: How to install Pytorch for your OS and configuration.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½•æ ¹æ®ä½ çš„æ“ä½œç³»ç»Ÿå’Œé…ç½®å®‰è£…Pytorchã€‚
- en: 'ğŸ’¡ **Note**: *We want to leverage our GPU. Therefore, it is essential to note
    that we want an installation with CUDA. But this is possible only if you have
    a Nvidia GPU at the time of writing. If not, you may want to use the CPU or switch
    to a Cloud computing service such as Google Colab.*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğŸ’¡ **æ³¨æ„**: *æˆ‘ä»¬å¸Œæœ›å……åˆ†åˆ©ç”¨æˆ‘ä»¬çš„GPUã€‚å› æ­¤ï¼Œé‡è¦çš„æ˜¯è¦æ³¨æ„æˆ‘ä»¬å¸Œæœ›è¿›è¡ŒCUDAå®‰è£…ã€‚ä½†è¿™åªæœ‰åœ¨ä½ å†™è¿™ç¯‡æ–‡ç« æ—¶æ‹¥æœ‰Nvidia GPUæ—¶æ‰å¯èƒ½ã€‚å¦‚æœæ²¡æœ‰ï¼Œä½ å¯èƒ½éœ€è¦ä½¿ç”¨CPUæˆ–åˆ‡æ¢åˆ°åƒGoogle
    Colabè¿™æ ·çš„äº‘è®¡ç®—æœåŠ¡ã€‚*'
- en: 'Therefore, our code line is the following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬çš„ä»£ç è¡Œå¦‚ä¸‹ï¼š
- en: '[PRE2]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This line will trigger the retrieval and installation of the necessary elements
    for Pytorch to function coherently.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è¡Œä»£ç å°†è§¦å‘å¿…è¦å…ƒç´ çš„æ£€ç´¢å’Œå®‰è£…ï¼Œä»¥ä¾¿Pytorchå¯ä»¥é¡ºåˆ©è¿è¡Œã€‚
- en: '![](../Images/7bb2a5f949df042d424767ce6448a145.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7bb2a5f949df042d424767ce6448a145.png)'
- en: The installation of Pytorch.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorchçš„å®‰è£…ã€‚
- en: 'The second deep-learning library that we want to use is Segment Anything. While
    Pytorch is being installed, we can download and install â€œsoftwareâ€ that will make
    it easier for us to manage versions and access online libraries. This is **Git**
    and is accessible here: [Git Website](https://git-scm.com/download/win).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¦ä½¿ç”¨çš„ç¬¬äºŒä¸ªæ·±åº¦å­¦ä¹ åº“æ˜¯Segment Anythingã€‚åœ¨Pytorchå®‰è£…çš„åŒæ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ä¸‹è½½å¹¶å®‰è£…â€œè½¯ä»¶â€ï¼Œè¿™å°†ä½¿æˆ‘ä»¬æ›´å®¹æ˜“ç®¡ç†ç‰ˆæœ¬å’Œè®¿é—®åœ¨çº¿åº“ã€‚è¿™å°±æ˜¯**Git**ï¼Œå¯ä»¥åœ¨è¿™é‡Œè®¿é—®ï¼š[Gitå®˜ç½‘](https://git-scm.com/download/win)ã€‚
- en: 'You can download and install git, and once the installation is finished, Pytorch
    should also be nicely installed in your environment. Therefore, to install segment-anything,
    we can write the line below:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥ä¸‹è½½å¹¶å®‰è£…gitï¼Œä¸€æ—¦å®‰è£…å®Œæˆï¼ŒPytorchä¹Ÿåº”è¯¥åœ¨ä½ çš„ç¯å¢ƒä¸­é¡ºåˆ©å®‰è£…ã€‚å› æ­¤ï¼Œä¸ºäº†å®‰è£…segment-anythingï¼Œæˆ‘ä»¬å¯ä»¥å†™å¦‚ä¸‹ä»£ç ï¼š
- en: '[PRE3]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This will again take some time until you get such a message below.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†å†æ¬¡éœ€è¦ä¸€äº›æ—¶é—´ï¼Œç›´åˆ°ä½ çœ‹åˆ°å¦‚ä¸‹æ¶ˆæ¯ã€‚
- en: '![](../Images/04e42602deac265942e73e53219ee11f.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04e42602deac265942e73e53219ee11f.png)'
- en: The CLI results of installing Pytorch.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å®‰è£…Pytorchçš„CLIç»“æœã€‚
- en: At this stage, we have the base libraries as well as the deep learning libraries
    installed. Before using them, let us install an IDE to use everything smoothly.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬å·²ç»å®‰è£…äº†åŸºç¡€åº“ä»¥åŠæ·±åº¦å­¦ä¹ åº“ã€‚åœ¨ä½¿ç”¨å®ƒä»¬ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å®‰è£…ä¸€ä¸ªIDEï¼Œä»¥ä¾¿ä¸€åˆ‡é¡ºåˆ©è¿è¡Œã€‚
- en: 4\. Setting up an IDE
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. è®¾ç½®IDE
- en: '![](../Images/53e45018ba59d7fa3a7dc0a1622bff21.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/53e45018ba59d7fa3a7dc0a1622bff21.png)'
- en: The Jupyter lab IDE after installation.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter lab IDEå®‰è£…åçš„ç•Œé¢ã€‚
- en: 'The last step of our setup is to install an IDE. We are still in the command
    line interface within the environment, and we type: `pip install jupyterlab`,
    which will install jupyterlab on our environment.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¾ç½®çš„æœ€åä¸€æ­¥æ˜¯å®‰è£…ä¸€ä¸ªIDEã€‚æˆ‘ä»¬ä»åœ¨ç¯å¢ƒä¸­çš„å‘½ä»¤è¡Œç•Œé¢ä¸‹ï¼Œè¾“å…¥ï¼š`pip install jupyterlab`ï¼Œè¿™å°†ä¼šåœ¨æˆ‘ä»¬çš„ç¯å¢ƒä¸­å®‰è£…jupyterlabã€‚
- en: To use it in a defined local folder, we can first create a parent directory
    for our project (let us call it `SAM`), which will hold both a `CODE` folder and
    a `DATA` Folder. Once this is done, in the console, we change our pointer to the
    created directory by writing. `cd C://COURSES/POUX/SAM`.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨æŒ‡å®šçš„æœ¬åœ°æ–‡ä»¶å¤¹ä¸­ä½¿ç”¨å®ƒï¼Œæˆ‘ä»¬å¯ä»¥é¦–å…ˆä¸ºæˆ‘ä»¬çš„é¡¹ç›®åˆ›å»ºä¸€ä¸ªçˆ¶ç›®å½•ï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸º `SAM`ï¼‰ï¼Œè¯¥ç›®å½•å°†åŒ…å«ä¸€ä¸ª `CODE` æ–‡ä»¶å¤¹å’Œä¸€ä¸ª `DATA`
    æ–‡ä»¶å¤¹ã€‚å®Œæˆåï¼Œåœ¨æ§åˆ¶å°ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å†™å…¥ `cd C://COURSES/POUX/SAM` æ¥åˆ‡æ¢åˆ°åˆ›å»ºçš„ç›®å½•ã€‚
- en: 'We launch jupyterlab from this location by typing in the console: `jupyter
    lab`, which will open a new localhost page in your web browser (Chrome, Firefox,
    or Safari).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€šè¿‡åœ¨æ§åˆ¶å°è¾“å…¥ `jupyter lab` æ¥ä»è¿™ä¸ªä½ç½®å¯åŠ¨ jupyterlabï¼Œè¿™å°†ä¼šåœ¨ä½ çš„ç½‘é¡µæµè§ˆå™¨ï¼ˆChromeã€Firefox æˆ– Safariï¼‰ä¸­æ‰“å¼€ä¸€ä¸ªæ–°çš„æœ¬åœ°ä¸»æœºé¡µé¢ã€‚
- en: 'In Jupyter, you can create a notebook (.ipynb) and write in the first cell
    of the import statements to use all the installed packages:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ Jupyter ä¸­ï¼Œä½ å¯ä»¥åˆ›å»ºä¸€ä¸ªç¬”è®°æœ¬ (.ipynb)ï¼Œå¹¶åœ¨ç¬¬ä¸€ä¸ªå•å…ƒæ ¼ä¸­å†™å…¥å¯¼å…¥è¯­å¥ï¼Œä»¥ä½¿ç”¨æ‰€æœ‰å·²å®‰è£…çš„è½¯ä»¶åŒ…ï¼š
- en: '[PRE4]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Alright! We are all set up. Before getting on the other steps of coding our
    model, now is just the time to retrieve our 3D Dataset.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½äº†ä¸€åˆ‡ã€‚åœ¨å¼€å§‹ç¼–ç æ¨¡å‹çš„å…¶ä»–æ­¥éª¤ä¹‹å‰ï¼Œç°åœ¨æ­£æ˜¯æå–æˆ‘ä»¬3Dæ•°æ®é›†çš„å¥½æ—¶æœºã€‚
- en: 5\. 3D Dataset Curation
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 3D æ•°æ®é›†æ•´ç†
- en: In previous tutorials, we illustrated point cloud processing and meshing over
    several 3D datasets, some of which use aerial LiDAR from the AHN4 LiDAR Campaign.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¹‹å‰çš„æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¤šä¸ª3Dæ•°æ®é›†ä¸­çš„ç‚¹äº‘å¤„ç†å’Œç½‘æ ¼åŒ–ï¼Œå…¶ä¸­ä¸€äº›ä½¿ç”¨äº†AHN4 LiDARæ´»åŠ¨çš„èˆªç©ºLiDARã€‚
- en: '[](/3d-deep-learning-python-tutorial-pointnet-data-preparation-90398f880c9f?source=post_page-----80c06be99a18--------------------------------)
    [## 3D Deep Learning Python Tutorial: PointNet Data Preparation'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/3d-deep-learning-python-tutorial-pointnet-data-preparation-90398f880c9f?source=post_page-----80c06be99a18--------------------------------)
    [## 3D æ·±åº¦å­¦ä¹ Pythonæ•™ç¨‹ï¼šPointNet æ•°æ®å‡†å¤‡'
- en: The Ultimate Python Guide to structure large LiDAR point cloud for training
    a 3D Deep Learning Semantic Segmentationâ€¦
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç»ˆæPythonæŒ‡å—ï¼Œç”¨äºç»“æ„åŒ–å¤§å‹LiDARç‚¹äº‘ï¼Œä»¥è®­ç»ƒ3Dæ·±åº¦å­¦ä¹ è¯­ä¹‰åˆ†å‰²â€¦â€¦
- en: towardsdatascience.com](/3d-deep-learning-python-tutorial-pointnet-data-preparation-90398f880c9f?source=post_page-----80c06be99a18--------------------------------)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/3d-deep-learning-python-tutorial-pointnet-data-preparation-90398f880c9f?source=post_page-----80c06be99a18--------------------------------)
- en: 'This time, we will use a dataset gathered using a Terrestrial Laser Scanner:
    the ITC of the University of Twente''s new 2023 building, as shown below. It consists
    of an indoor green area with nice tricky foliage to assess after running the segmentation.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ¬¡ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªä½¿ç”¨åœ°é¢æ¿€å…‰æ‰«æä»ªæ”¶é›†çš„æ•°æ®é›†ï¼š2023å¹´ä¹Œç‰¹å‹’æ”¯å¤§å­¦ITCçš„æ–°å»ºç­‘ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚å®ƒåŒ…å«ä¸€ä¸ªå®¤å†…ç»¿è‰²åŒºåŸŸï¼Œå†…æœ‰ç²¾ç¾çš„å¤æ‚æ ‘å¶ï¼Œåˆ†å‰²åå¯ä»¥è¿›è¡Œè¯„ä¼°ã€‚
- en: '![](../Images/29970d4d0fb77f18812b960e2601f444.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29970d4d0fb77f18812b960e2601f444.png)'
- en: The 3D Point Cloud of the ITC UTwente new building, with its indoor â€œjungleâ€.
    Â© [F. Poux](https://learngeodata.eu/)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ITC UTwenteæ–°å»ºç­‘çš„3Dç‚¹äº‘ï¼ŒåŒ…æ‹¬å…¶å®¤å†…â€œä¸›æ—â€ã€‚Â© [F. Poux](https://learngeodata.eu/)
- en: 'You can download the data from the Drive Folder here: [Guide Datasets (Google
    Drive)](https://drive.google.com/drive/folders/1pIaP-vJAWh8cFtk9zQ3poxcR5DuEIkmt?usp=sharing),
    and put it in your folder that holds datasets (in my case, â€œDATAâ€).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥ä»è¿™é‡Œä¸‹è½½æ•°æ®ï¼š [æŒ‡å—æ•°æ®é›†ï¼ˆGoogle Driveï¼‰](https://drive.google.com/drive/folders/1pIaP-vJAWh8cFtk9zQ3poxcR5DuEIkmt?usp=sharing)ï¼Œå¹¶å°†å…¶æ”¾å…¥ä½ ä¿å­˜æ•°æ®é›†çš„æ–‡ä»¶å¤¹ä¸­ï¼ˆåœ¨æˆ‘çš„ä¾‹å­ä¸­æ˜¯â€œDATAâ€ï¼‰ã€‚
- en: At this process stage, we have a nice warm coding setup, with all the necessary
    libraries in a lightweight, isolated GEOSAM conda environment.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­é˜¶æ®µï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªè‰¯å¥½çš„ç¼–ç è®¾ç½®ï¼Œæ‰€æœ‰å¿…è¦çš„åº“éƒ½åœ¨ä¸€ä¸ªè½»é‡çº§ã€éš”ç¦»çš„ GEOSAM conda ç¯å¢ƒä¸­ã€‚
- en: 'ğŸ¦Š **Florent**: Great job so far! If you are eager to run some tests to check
    that Pytorch is working as it should, i.e., CUDA is recognized, you can write
    these lines of code:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğŸ¦Š **Florent**: åˆ°ç›®å‰ä¸ºæ­¢åšå¾—å¾ˆå¥½ï¼å¦‚æœä½ æ€¥äºè¿è¡Œä¸€äº›æµ‹è¯•ä»¥æ£€æŸ¥Pytorchæ˜¯å¦æ­£å¸¸å·¥ä½œï¼Œå³CUDAæ˜¯å¦è¢«è¯†åˆ«ï¼Œä½ å¯ä»¥å†™ä¸‹ä»¥ä¸‹ä»£ç è¡Œï¼š'
- en: '[PRE5]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/06b2a254f99419bdf2ab27549acbfb2e.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06b2a254f99419bdf2ab27549acbfb2e.png)'
- en: My configuration from the print results above.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°æ‰“å°ç»“æœçš„é…ç½®ã€‚
- en: It is now time to segment stuff!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ˜¯æ—¶å€™å¯¹æ•°æ®è¿›è¡Œåˆ†å‰²äº†ï¼
- en: '[](https://learngeodata.eu/3d-newsletter/?source=post_page-----80c06be99a18--------------------------------)
    [## 3D Innovator Newsletter'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://learngeodata.eu/3d-newsletter/?source=post_page-----80c06be99a18--------------------------------)
    [## 3D åˆ›æ–°è€…é€šè®¯'
- en: Weekly practical content, insights, code and resources to master 3D Data Science.
    I write about Point Clouds, AIâ€¦
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¯å‘¨å®ç”¨å†…å®¹ã€è§è§£ã€ä»£ç å’Œèµ„æºï¼Œä»¥æŒæ¡3Dæ•°æ®ç§‘å­¦ã€‚æˆ‘å†™å…³äºç‚¹äº‘ã€äººå·¥æ™ºèƒ½ç­‰å†…å®¹â€¦â€¦
- en: learngeodata.eu](https://learngeodata.eu/3d-newsletter/?source=post_page-----80c06be99a18--------------------------------)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[learngeodata.eu](https://learngeodata.eu/3d-newsletter/?source=post_page-----80c06be99a18--------------------------------)'
- en: 2\. Setting up the Segment Anything Model
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. è®¾ç½® Segment Anything æ¨¡å‹
- en: At the heart of our little adventure lies the Segment Anything Model, a powerful
    creation with excellent potential for 3D point cloud semantic segmentation. With
    its innovative architecture and training process, this model is the perfect candidate
    to be tested on indoor applications. Let us first play around with its core concepts.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„å°å†’é™©çš„æ ¸å¿ƒæ˜¯ Segment Anything Modelï¼Œè¿™æ˜¯ä¸€ç§å¼ºå¤§çš„åˆ›ä½œï¼Œå…·æœ‰æå¥½çš„ 3D ç‚¹äº‘è¯­ä¹‰åˆ†å‰²æ½œåŠ›ã€‚å‡­å€Ÿå…¶åˆ›æ–°çš„æ¶æ„å’Œè®­ç»ƒè¿‡ç¨‹ï¼Œè¯¥æ¨¡å‹æ˜¯å®¤å†…åº”ç”¨æµ‹è¯•çš„ç†æƒ³å€™é€‰è€…ã€‚è®©æˆ‘ä»¬å…ˆæ¥äº†è§£ä¸€ä¸‹å…¶æ ¸å¿ƒæ¦‚å¿µã€‚
- en: 2.1\. Segment Anything Basics
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1\. Segment Anything åŸºç¡€çŸ¥è¯†
- en: MetaAI has delved into the fascinating realm of Natural Language Processing
    (NLP) and computer vision with their Segment Anything Model, which enables **zero-shot**
    and **few-shot learning** on novel datasets and tasks using foundation models.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: MetaAI å·²æ·±å…¥æ¢è®¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å’Œè®¡ç®—æœºè§†è§‰çš„è¿·äººé¢†åŸŸï¼Œå…¶ Segment Anything Model ä½¿ **é›¶-shot** å’Œ **å°‘-shot
    å­¦ä¹ ** åœ¨æ–°æ•°æ®é›†å’Œä»»åŠ¡ä¸Šæˆä¸ºå¯èƒ½ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹ã€‚
- en: 'ğŸ¦Š **Florent**: *Okay, there are a lot of swear words, I admit. For clarity
    concerns, here is my tentative to summarize each complex terminology. Zero-shot
    learning refers to the ability to recognize something without having seen it (seen
    it zero times). Somewhat similarly, few-shot learning uses a limited number of
    labeled examples for each new class, and the goal is to make predictions for new
    classes based on just these few examples of labeled data.*'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğŸ¦Š **Florent**: *å¥½å§ï¼Œæˆ‘æ‰¿è®¤æœ‰å¾ˆå¤šè„è¯ã€‚ä¸ºäº†æ¸…æ™°èµ·è§ï¼Œè¿™é‡Œæ˜¯æˆ‘å¯¹æ¯ä¸ªå¤æ‚æœ¯è¯­çš„æ€»ç»“å°è¯•ã€‚é›¶-shot å­¦ä¹ æŒ‡çš„æ˜¯åœ¨æœªè§è¿‡æŸç‰©çš„æƒ…å†µä¸‹è¯†åˆ«å®ƒï¼ˆé›¶æ¬¡è§è¿‡ï¼‰ã€‚ç±»ä¼¼åœ°ï¼Œå°‘-shot
    å­¦ä¹ ä½¿ç”¨æœ‰é™æ•°é‡çš„æ ‡è®°ç¤ºä¾‹æ¥å¤„ç†æ¯ä¸ªæ–°ç±»åˆ«ï¼Œç›®æ ‡æ˜¯æ ¹æ®è¿™äº›å°‘é‡çš„æ ‡è®°æ•°æ®è¿›è¡Œé¢„æµ‹ã€‚*'
- en: 'ğŸ¤ **Ville***: Also, a so-called* ***foundation model*** *is a model that is
    trained on a lot and a lot of data at a scale. it is so large that it can be adapted
    to various tasks from different scenarios.*'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğŸ¤ **Ville***: æ­¤å¤–ï¼Œæ‰€è°“çš„* ***åŸºç¡€æ¨¡å‹*** *æ˜¯ä¸€ä¸ªåœ¨å¤§é‡æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚å®ƒå¦‚æ­¤åºå¤§ï¼Œå¯ä»¥é€‚åº”æ¥è‡ªä¸åŒåœºæ™¯çš„å„ç§ä»»åŠ¡ã€‚*'
- en: 'Letâ€™s break this down for you:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä¸ºä½ æ‹†è§£ä¸€ä¸‹ï¼š
- en: Overall, the SAM â€œAIâ€ algorithm can significantly reduce the human effort required
    for image segmentation. To do so, you provide the model with foreground/background
    points, a rough box or mask, some text, or any other input that indicates what
    you want to segment in an image. The Meta AI team has trained the Segment Anything
    Model to generate a proper segmentation mask. This mask is the modelâ€™s output
    and should be a suitable mask to delineate one of the things that the prompt might
    refer to. For instance, if you indicate a point on the roof of the house, the
    output should correctly identify whether you meant the roof or the house.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä½“è€Œè¨€ï¼ŒSAM â€œAIâ€ ç®—æ³•å¯ä»¥æ˜¾è‘—å‡å°‘è¿›è¡Œå›¾åƒåˆ†å‰²æ‰€éœ€çš„äººåŠ›ã€‚ä¸ºæ­¤ï¼Œä½ éœ€è¦å‘æ¨¡å‹æä¾›å‰æ™¯/èƒŒæ™¯ç‚¹ã€ç²—ç•¥çš„æ¡†æˆ–æ©ç ã€ä¸€äº›æ–‡æœ¬æˆ–ä»»ä½•å…¶ä»–æŒ‡ç¤ºä½ æƒ³è¦åœ¨å›¾åƒä¸­åˆ†å‰²çš„è¾“å…¥ã€‚Meta
    AI å›¢é˜Ÿå·²è®­ç»ƒ Segment Anything Model ä»¥ç”Ÿæˆåˆé€‚çš„åˆ†å‰²æ©ç ã€‚è¿™ä¸ªæ©ç æ˜¯æ¨¡å‹çš„è¾“å‡ºï¼Œåº”è¯¥æ˜¯ä¸€ä¸ªé€‚åˆåˆ’å®šæç¤ºå¯èƒ½æŒ‡å‘çš„äº‹ç‰©çš„æ©ç ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ åœ¨æˆ¿å­å±‹é¡¶ä¸Šæ ‡å‡ºä¸€ä¸ªç‚¹ï¼Œè¾“å‡ºåº”è¯¥æ­£ç¡®è¯†åˆ«ä½ æ˜¯æŒ‡å±‹é¡¶è¿˜æ˜¯æˆ¿å­ã€‚
- en: '![](../Images/e2a0375836c5937774047d2f113b288a.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2a0375836c5937774047d2f113b288a.png)'
- en: How does the Segment Anything Model (SAM) work? Explanation of the segmentation
    prompt to generate valid masks (case of a house). Â© [F. Poux](https://learngeodata.eu/)
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything Model (SAM) æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿè§£é‡Šåˆ†å‰²æç¤ºä»¥ç”Ÿæˆæœ‰æ•ˆçš„æ©ç ï¼ˆä»¥æˆ¿å±‹ä¸ºä¾‹ï¼‰ã€‚Â© [F. Poux](https://learngeodata.eu/)
- en: This segmentation task can then serve for model pre-training and guiding solutions
    for various downstream segmentation problems.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥åˆ†å‰²ä»»åŠ¡å¯ä»¥ç”¨äºæ¨¡å‹é¢„è®­ç»ƒï¼Œå¹¶æŒ‡å¯¼è§£å†³å„ç§ä¸‹æ¸¸åˆ†å‰²é—®é¢˜ã€‚
- en: On the technical side, what we call an image encoder creates a unique embedding
    (representation) for each image, and a lightweight encoder swiftly transforms
    any query into an embedding vector. These two data sources are merged using a
    (lightweight) mask decoder to predict segmentation masks, as shown below.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æŠ€æœ¯è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬æ‰€ç§°çš„å›¾åƒç¼–ç å™¨ä¸ºæ¯å¼ å›¾åƒåˆ›å»ºäº†ä¸€ä¸ªç‹¬ç‰¹çš„åµŒå…¥ï¼ˆè¡¨ç¤ºï¼‰ï¼Œè€Œä¸€ä¸ªè½»é‡çº§çš„ç¼–ç å™¨è¿…é€Ÿå°†ä»»ä½•æŸ¥è¯¢è½¬æ¢ä¸ºåµŒå…¥å‘é‡ã€‚è¿™ä¸¤ä¸ªæ•°æ®æºé€šè¿‡ä¸€ä¸ªï¼ˆè½»é‡çº§çš„ï¼‰æ©ç è§£ç å™¨åˆå¹¶ï¼Œä»¥é¢„æµ‹åˆ†å‰²æ©ç ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚
- en: '![](../Images/84684c89b03497988c14b8439f88afd8.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84684c89b03497988c14b8439f88afd8.png)'
- en: Flowchart of the functioning of the Segment Anything Model. The image goes through
    the image encoder. Then it is embedded, to finally be combined after using a prompt
    followed by a prompt encoder, to generate final masks for our 3D point clouds.
    Â© [F. Poux](https://learngeodata.eu/)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything Model çš„å·¥ä½œæµç¨‹å›¾ã€‚å›¾åƒç»è¿‡å›¾åƒç¼–ç å™¨å¤„ç†ã€‚ç„¶åå®ƒè¢«åµŒå…¥ï¼Œæœ€ååœ¨ä½¿ç”¨æç¤ºå’Œæç¤ºç¼–ç å™¨ååˆå¹¶ï¼Œä»¥ç”Ÿæˆæˆ‘ä»¬ 3D ç‚¹äº‘çš„æœ€ç»ˆæ©ç ã€‚Â©
    [F. Poux](https://learngeodata.eu/)
- en: 'This effective architecture, combined with a massive scale training phase,
    allows the Segment Anything Model to reach four milestones:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æœ‰æ•ˆçš„æ¶æ„ï¼ŒåŠ ä¸Šå¤§è§„æ¨¡çš„è®­ç»ƒé˜¶æ®µï¼Œä½¿Segment Anything Modelè¾¾åˆ°äº†å››ä¸ªé‡Œç¨‹ç¢‘ï¼š
- en: '**Effortless Object Segmentation ğŸ”¥**: With SAM, users can effortlessly segment
    objects by simply selecting the points they want to include or exclude from the
    segmentation. You can also use a bounding box as a cue for the model.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è½»æ¾å¯¹è±¡åˆ†å‰² ğŸ”¥**: ä½¿ç”¨SAMï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ç®€å•é€‰æ‹©è¦åŒ…æ‹¬æˆ–æ’é™¤çš„ç‚¹æ¥è½»æ¾åˆ†å‰²å¯¹è±¡ã€‚ä½ è¿˜å¯ä»¥ä½¿ç”¨è¾¹ç•Œæ¡†ä½œä¸ºæ¨¡å‹çš„æç¤ºã€‚'
- en: '**Handling Uncertainty ğŸ”¥**: SAM is equipped to handle situations with uncertainty
    about the object to be segmented. It can generate multiple valid masks, which
    is crucial for solving real-world segmentation challenges effectively.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¤„ç†ä¸ç¡®å®šæ€§ ğŸ”¥**: SAMèƒ½å¤Ÿå¤„ç†å¯¹è±¡åˆ†å‰²ä¸­çš„ä¸ç¡®å®šæƒ…å†µã€‚å®ƒå¯ä»¥ç”Ÿæˆå¤šä¸ªæœ‰æ•ˆçš„æ©ç ï¼Œè¿™å¯¹äºæœ‰æ•ˆè§£å†³å®é™…çš„åˆ†å‰²æŒ‘æˆ˜è‡³å…³é‡è¦ã€‚'
- en: '**Automatic Object Detection and Masking ğŸ”¥**: SAM makes automatic object detection
    and masking a breeze. It simplifies these tasks, saving you time and effort.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è‡ªåŠ¨å¯¹è±¡æ£€æµ‹ä¸æ©è†œ ğŸ”¥**: SAMä½¿å¾—è‡ªåŠ¨å¯¹è±¡æ£€æµ‹å’Œæ©è†œå˜å¾—è½»è€Œæ˜“ä¸¾ã€‚å®ƒç®€åŒ–äº†è¿™äº›ä»»åŠ¡ï¼ŒèŠ‚çœäº†ä½ çš„æ—¶é—´å’Œç²¾åŠ›ã€‚'
- en: '**Real-time Interaction ğŸ”¥**: Thanks to precomputed image embeddings, SAM can
    instantly provide a segmentation mask for any prompt. This means you can have
    real-time interactions with the model.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å®æ—¶äº¤äº’ ğŸ”¥**: å¾—ç›Šäºé¢„è®¡ç®—çš„å›¾åƒåµŒå…¥ï¼ŒSAMå¯ä»¥å³æ—¶æä¾›ä»»ä½•æç¤ºçš„åˆ†å‰²æ©è†œã€‚è¿™æ„å‘³ç€ä½ å¯ä»¥ä¸æ¨¡å‹è¿›è¡Œå®æ—¶äº¤äº’ã€‚'
- en: Now that this is out of the way, are you ready to use it?
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¢ç„¶è¿™äº›éƒ½è§£å†³äº†ï¼Œä½ å‡†å¤‡å¥½ä½¿ç”¨å®ƒäº†å—ï¼Ÿ
- en: 2.1\. SAM Parameters
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1\. SAM å‚æ•°
- en: 'The SAM model can be loaded with three different encoders: ViT-B, ViT-L, and
    ViT-H. ViT-H gives better results than ViT-B but has only marginal gains over
    ViT-L.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: SAMæ¨¡å‹å¯ä»¥é€šè¿‡ä¸‰ç§ä¸åŒçš„ç¼–ç å™¨åŠ è½½ï¼šViT-Bã€ViT-Lå’ŒViT-Hã€‚ViT-Hçš„ç»“æœä¼˜äºViT-Bï¼Œä½†ä¸ViT-Lç›¸æ¯”ä»…æœ‰å¾®å°çš„æå‡ã€‚
- en: '[PRE6]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**ğŸ¤  Ville**: *To help with the choice, I tested ViT-B on NVIDIA GeForce GTX
    1650, 6 Gb VRAM with Win11.*'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸ¤  Ville**: *ä¸ºäº†å¸®åŠ©é€‰æ‹©ï¼Œæˆ‘åœ¨NVIDIA GeForce GTX 1650ã€6 Gb VRAMå’ŒWin11ä¸Šæµ‹è¯•äº†ViT-Bã€‚*'
- en: These three encoders have different parameter counts that give a bit more freedom
    to tune an application. ViT-B (the smallest) has 91 Million parameters, ViT-L
    has 308 Million parameters, and ViT-H (the biggest) has 636 Million parameters.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸‰ç§ç¼–ç å™¨å…·æœ‰ä¸åŒçš„å‚æ•°æ•°é‡ï¼Œè¿™ä¸ºåº”ç”¨ç¨‹åºçš„è°ƒä¼˜æä¾›äº†æ›´å¤šè‡ªç”±ã€‚ViT-Bï¼ˆæœ€å°ï¼‰æœ‰9100ä¸‡ä¸ªå‚æ•°ï¼ŒViT-Læœ‰3.08äº¿ä¸ªå‚æ•°ï¼Œè€ŒViT-Hï¼ˆæœ€å¤§ï¼‰æœ‰6.36äº¿ä¸ªå‚æ•°ã€‚
- en: 'This difference in size also influences the speed of inference, so this should
    help you decide the encoder for your specific use case. Following this guide,
    we will get with the heavy artillery: The ViT-H, with a Model Checkpoint that
    you can download from [Github](https://github.com/facebookresearch/segment-anything#model-checkpoints)
    (2.4 Gb) and place in your current parent folder, for example.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§å¤§å°å·®å¼‚ä¹Ÿä¼šå½±å“æ¨æ–­é€Ÿåº¦ï¼Œå› æ­¤è¿™åº”æœ‰åŠ©äºä½ ä¸ºä½ çš„å…·ä½“ç”¨ä¾‹é€‰æ‹©ç¼–ç å™¨ã€‚æŒ‰ç…§æœ¬æŒ‡å—ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨é‡å‹æ­¦å™¨ï¼šViT-Hï¼Œå¸¦æœ‰ä¸€ä¸ªå¯ä»¥ä»[Github](https://github.com/facebookresearch/segment-anything#model-checkpoints)ï¼ˆ2.4
    Gbï¼‰ä¸‹è½½çš„æ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œå¹¶å°†å…¶æ”¾ç½®åœ¨ä½ çš„å½“å‰çˆ¶æ–‡ä»¶å¤¹ä¸­ï¼Œä¾‹å¦‚ã€‚
- en: 'This is where we can define two variables to make your code a bit more flexible
    afterward:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸¤ä¸ªå˜é‡ï¼Œä»¥ä½¿ä½ çš„ä»£ç åœ¨ä¹‹åç¨å¾®æ›´çµæ´»ä¸€äº›ï¼š
- en: '[PRE7]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'From there, we can initialize our SAM model with the following two lines of
    code:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä»¥ä¸‹ä¸¤è¡Œä»£ç åˆå§‹åŒ–æˆ‘ä»¬çš„SAMæ¨¡å‹ï¼š
- en: '[PRE8]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: And we are all set up! Maybe one last step, trying to see how it performs on
    a random image that you have on your desktop?
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€åˆ‡éƒ½å‡†å¤‡å¥½äº†ï¼ä¹Ÿè®¸æœ€åä¸€æ­¥ï¼Œè¯•è¯•çœ‹å®ƒåœ¨ä½ æ¡Œé¢ä¸Šçš„éšæœºå›¾åƒä¸Šçš„è¡¨ç°å¦‚ä½•ï¼Ÿ
- en: 2.2 Performances on 2D images
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 åœ¨2Då›¾åƒä¸Šçš„æ€§èƒ½
- en: 'Let us test if all works as expected on a random image. We are interested in
    geospatial applications, so I go to [Google Earth](https://earth.google.com/)
    and zoom in on a spot of interest:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æµ‹è¯•ä¸€ä¸‹åœ¨éšæœºå›¾åƒä¸Šçš„æ•ˆæœæ˜¯å¦å¦‚é¢„æœŸã€‚æˆ‘ä»¬å¯¹åœ°ç†ç©ºé—´åº”ç”¨æ„Ÿå…´è¶£ï¼Œæ‰€ä»¥æˆ‘å»[Google Earth](https://earth.google.com/)å¹¶æ”¾å¤§ä¸€ä¸ªæ„Ÿå…´è¶£çš„ç‚¹ï¼š
- en: '![](../Images/a541e6bccb901f9898489865f2107af8.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a541e6bccb901f9898489865f2107af8.png)'
- en: Selection of an imagery dataset from Biscarosse. Â© [F. Poux](https://learngeodata.eu/)
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: é€‰æ‹©æ¥è‡ªBiscarosseçš„å›¾åƒæ•°æ®é›†ã€‚Â© [F. Poux](https://learngeodata.eu/)
- en: 'ğŸ¦Š **Florent**: *This spot is biased, right? Hopefully, this gives you a bit
    of French holiday vibes, which you are proud to take followed by a marvelous year
    full of exciting projects*!'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğŸ¦Š **Florent**: *è¿™ä¸ªç‚¹æœ‰åè§ï¼Œå¯¹å§ï¼Ÿå¸Œæœ›è¿™èƒ½ç»™ä½ ä¸€äº›æ³•å›½å‡æœŸçš„æ„Ÿè§‰ï¼Œä½ å¾ˆè‡ªè±ªåœ°ç»å†äº†è¿™æ®µç¾å¦™çš„å²æœˆï¼Œå……æ»¡äº†æ¿€åŠ¨äººå¿ƒçš„é¡¹ç›®ï¼*'
- en: 'From there, I take a screenshot of a zone of interest:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ä»é‚£é‡Œï¼Œæˆ‘ä¼šæˆªå–ä¸€ä¸ªæ„Ÿå…´è¶£åŒºåŸŸçš„å±å¹•æˆªå›¾ï¼š
- en: '![](../Images/3a887690665dfcb7368dafd20b5fb139.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a887690665dfcb7368dafd20b5fb139.png)'
- en: The image dataset of a zone of Biscarosse plage. Â© [F. Poux](https://learngeodata.eu/)
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ¥è‡ªBiscarosse plageåŒºåŸŸçš„å›¾åƒæ•°æ®é›†ã€‚Â© [F. Poux](https://learngeodata.eu/)
- en: 'and I load the image into memory with openCV:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ç”¨openCVå°†å›¾åƒåŠ è½½åˆ°å†…å­˜ä¸­ï¼š
- en: '[PRE9]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'ğŸ¦š **Note**: *As you can see, by default, OpenCV loads an image by switching
    to Blue, Green, and Red channels (BGR) that we order as RGB with the second line
    and store in the* `image_rgb` *variable.*'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğŸ¦š **æ³¨æ„**: *å¦‚ä½ æ‰€è§ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼ŒOpenCVé€šè¿‡åˆ‡æ¢åˆ°è“è‰²ã€ç»¿è‰²å’Œçº¢è‰²é€šé“ï¼ˆBGRï¼‰æ¥åŠ è½½å›¾åƒï¼Œæˆ‘ä»¬é€šè¿‡ç¬¬äºŒè¡Œå°†å…¶æ’åºä¸ºRGBï¼Œå¹¶å­˜å‚¨åœ¨*
    `image_rgb` *å˜é‡ä¸­ã€‚*'
- en: 'Now, it is time for us to apply SAM on the image with two lines of code:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸¤è¡Œä»£ç åœ¨å›¾åƒä¸Šåº”ç”¨SAMï¼š
- en: '[PRE10]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In around 6 seconds, this returns us a list filled with dictionaries, each
    representing a mask for a specific object automatically extracted, accompanied
    by its scores and metadata. For a detailed view, the result is a list of dictionaries
    where each `dict` holds the following information:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§çº¦6ç§’é’Ÿåï¼Œè¿™å°†è¿”å›ä¸€ä¸ªå¡«å……äº†å­—å…¸çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸ä»£è¡¨ä¸€ä¸ªç‰¹å®šå¯¹è±¡çš„è‡ªåŠ¨æå–æ©è†œï¼Œå¹¶é™„æœ‰å…¶åˆ†æ•°å’Œå…ƒæ•°æ®ã€‚è¯¦ç»†æŸ¥çœ‹æ—¶ï¼Œç»“æœæ˜¯ä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ª`dict`åŒ…å«ä»¥ä¸‹ä¿¡æ¯ï¼š
- en: '`segmentation` : this brings out the mask with `(W, H)` shape (and `bool` type),
    where `W` (width) and `H` (height) target the original image dimensions;'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segmentation` : è¿™ä¼šç”Ÿæˆå½¢çŠ¶ä¸º`(W, H)`ï¼ˆå’Œ`bool`ç±»å‹ï¼‰çš„æ©è†œï¼Œå…¶ä¸­`W`ï¼ˆå®½åº¦ï¼‰å’Œ`H`ï¼ˆé«˜åº¦ï¼‰é’ˆå¯¹åŸå§‹å›¾åƒå°ºå¯¸ï¼›'
- en: '`area` : this is the area of the mask expressed in pixels'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`area` : è¿™æ˜¯ä»¥åƒç´ ä¸ºå•ä½çš„æ©è†œé¢ç§¯'
- en: '`bbox` : this is the boundary box detection in `xywh` format'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bbox` : è¿™æ˜¯`xywh`æ ¼å¼çš„è¾¹ç•Œæ¡†æ£€æµ‹'
- en: '`predicted_iou` : the model''s prediction IoU metric for the quality of the
    mask.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predicted_iou` : æ¨¡å‹å¯¹æ©è†œè´¨é‡çš„é¢„æµ‹IoUæŒ‡æ ‡ã€‚'
- en: '`point_coords` : This is a list of the sampled input points that were used
    to generate the mask'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`point_coords` : è¿™æ˜¯ç”¨äºç”Ÿæˆæ©è†œçš„é‡‡æ ·è¾“å…¥ç‚¹çš„åˆ—è¡¨'
- en: '`stability_score` : The stability score is an additional measure of the mask
    quality. Check out the paper for more details ğŸ˜‰'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stability_score` : ç¨³å®šæ€§å¾—åˆ†æ˜¯æ©è†œè´¨é‡çš„é™„åŠ è¡¡é‡æŒ‡æ ‡ã€‚æŸ¥çœ‹è®ºæ–‡è·å–æ›´å¤šç»†èŠ‚ ğŸ˜‰'
- en: '`crop_box` : this is a list of the crop_boxe coordinates used to generate this
    mask in `xywh` format (it may differ from the Bounding-Box)'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`crop_box` : è¿™æ˜¯ç”¨äºç”Ÿæˆè¯¥æ©è†œçš„crop_boxåæ ‡åˆ—è¡¨ï¼Œæ ¼å¼ä¸º`xywh`ï¼ˆå¯èƒ½ä¸è¾¹ç•Œæ¡†ä¸åŒï¼‰'
- en: 'Now that you have a better idea about what we are dealing with, to check out
    the results, we can plot the masks on top of the image with the following function:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ å¯¹æˆ‘ä»¬æ­£åœ¨å¤„ç†çš„å†…å®¹æœ‰äº†æ›´å¥½çš„äº†è§£ï¼Œè¦æŸ¥çœ‹ç»“æœï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä»¥ä¸‹å‡½æ•°åœ¨å›¾åƒä¸Šç»˜åˆ¶æ©è†œï¼š
- en: '[PRE11]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'ğŸ¦Š **Florent**: *I admit, this is a bit blunt. But what happens in this function
    is that I will sort out the masks by their area to plot them with a random color
    on top of the image with a transparency parameter*.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğŸ¦Š **Florent**: *æˆ‘æ‰¿è®¤ï¼Œè¿™æœ‰ç‚¹ç›´æ¥ã€‚ä½†åœ¨è¿™ä¸ªå‡½æ•°ä¸­ï¼Œæˆ‘ä¼šæŒ‰æ©è†œé¢ç§¯å¯¹å®ƒä»¬è¿›è¡Œæ’åºï¼Œä»¥éšæœºé¢œè‰²å’Œé€æ˜åº¦å‚æ•°åœ¨å›¾åƒä¸Šç»˜åˆ¶å®ƒä»¬ã€‚*'
- en: 'ğŸ¤ **Ville***: Memory errors can ruin French holiday vibes! Remember the Google
    Colab option too! If rebooting does not solve the issue and allocated memory is
    too high, the following piece of code clears the GPU memory of extra allocations.
    Use it to address memory problems.*'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğŸ¤ **Ville***: å†…å­˜é”™è¯¯å¯èƒ½ä¼šæ¯æ‰æ³•å›½å‡æœŸçš„æ°›å›´ï¼è®°å¾—ä½¿ç”¨Google Colabé€‰é¡¹ï¼å¦‚æœé‡å¯ä¸èƒ½è§£å†³é—®é¢˜ä¸”åˆ†é…å†…å­˜è¿‡é«˜ï¼Œä»¥ä¸‹ä»£ç å¯ä»¥æ¸…é™¤GPUå†…å­˜ä¸­çš„é¢å¤–åˆ†é…ã€‚ç”¨å®ƒæ¥è§£å†³å†…å­˜é—®é¢˜ã€‚*'
- en: '[PRE12]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '*If the GPU memory is not freed enough, try rebooting your (Windows) computer.
    Also, try using the following line if memory problems persist:* `mask_generator
    = SamAutomaticMaskGenerator(sam, points_per_batch=16)`'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¦‚æœGPUå†…å­˜æœªå……åˆ†é‡Šæ”¾ï¼Œè¯·å°è¯•é‡å¯ä½ çš„ï¼ˆWindowsï¼‰è®¡ç®—æœºã€‚å¦‚æœå†…å­˜é—®é¢˜ä»ç„¶å­˜åœ¨ï¼Œå¯ä»¥å°è¯•ä½¿ç”¨ä»¥ä¸‹è¡Œï¼š* `mask_generator =
    SamAutomaticMaskGenerator(sam, points_per_batch=16)`'
- en: 'Now, to plot and export the image, we write the following:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè¦ç»˜åˆ¶å’Œå¯¼å‡ºå›¾åƒï¼Œæˆ‘ä»¬å†™ä¸‹ä»¥ä¸‹å†…å®¹ï¼š
- en: '[PRE13]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Which results in:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯¼è‡´ï¼š
- en: '![](../Images/77cc7cf3fcc62d5fdab3af87a5895117.png)![](../Images/8913306a437571893ac027bede9a81d3.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77cc7cf3fcc62d5fdab3af87a5895117.png)![](../Images/8913306a437571893ac027bede9a81d3.png)'
- en: Before and after the Segment Anything Model. Â© [F. Poux](https://learngeodata.eu/)
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Segment Anything Modelä¹‹å‰å’Œä¹‹åã€‚Â© [F. Poux](https://learngeodata.eu/)
- en: So, already at this stage, we have exciting results, and SAM is working really
    nicely! For example, you can see that almost all roofs are part of segments and
    that the three pools (2 blue and one green) are also part of segments. Therefore,
    this could well be a starting point for complete automatic detection
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œåœ¨è¿™ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬å·²ç»æœ‰äº†ä»¤äººå…´å¥‹çš„ç»“æœï¼ŒSAMå·¥ä½œå¾—éå¸¸å¥½ï¼ä¾‹å¦‚ï¼Œä½ å¯ä»¥çœ‹åˆ°å‡ ä¹æ‰€æœ‰çš„å±‹é¡¶éƒ½æ˜¯åˆ†æ®µçš„ä¸€éƒ¨åˆ†ï¼Œä¸‰ä¸ªæ³³æ± ï¼ˆä¸¤ä¸ªè“è‰²å’Œä¸€ä¸ªç»¿è‰²ï¼‰ä¹Ÿæ˜¯åˆ†æ®µçš„ä¸€éƒ¨åˆ†ã€‚å› æ­¤ï¼Œè¿™å¯èƒ½æ˜¯å®Œå…¨è‡ªåŠ¨æ£€æµ‹çš„èµ·ç‚¹
- en: 'ğŸ¦Š **Florent**: *You may run into Memory Errors depending on your computer setup
    while plotting the masks. In this case, loading a lighter SAM model should solve
    your problem.* ğŸ˜‰'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğŸ¦Š **Florent**: *æ ¹æ®ä½ çš„è®¡ç®—æœºè®¾ç½®ï¼Œç»˜åˆ¶æ©è†œæ—¶å¯èƒ½ä¼šé‡åˆ°å†…å­˜é”™è¯¯ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒåŠ è½½ä¸€ä¸ªæ›´è½»çš„SAMæ¨¡å‹åº”è¯¥èƒ½è§£å†³ä½ çš„é—®é¢˜ã€‚* ğŸ˜‰'
- en: Now that we have a working SAM setup, let us apply all this hard-earned know-how
    to 3D point clouds.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¢ç„¶æˆ‘ä»¬å·²ç»æœ‰äº†ä¸€ä¸ªæœ‰æ•ˆçš„SAMè®¾ç½®ï¼Œè®©æˆ‘ä»¬å°†è¿™äº›è¾›è‹¦è·å¾—çš„çŸ¥è¯†åº”ç”¨äº3Dç‚¹äº‘ã€‚
- en: 3\. 3D Point Cloud to Image Projections
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 3D ç‚¹äº‘åˆ°å›¾åƒæŠ•å½±
- en: To make sense of the complex 3D world, we delve into the art of point cloud
    projection. Through techniques like ortho and spherical projections, we bridge
    the gap between dimensions, enabling us to visualize the intricacies of the point
    cloud in a 2D realm, which is the input needed for SAM. Point cloud mapping adds
    a layer of understanding to this projection process.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç†è§£å¤æ‚çš„3Dä¸–ç•Œï¼Œæˆ‘ä»¬*æ·±å…¥æ¢è®¨*ç‚¹äº‘æŠ•å½±çš„è‰ºæœ¯ã€‚é€šè¿‡åƒæ­£å°„å’Œçƒé¢æŠ•å½±è¿™æ ·çš„æŠ€æœ¯ï¼Œæˆ‘ä»¬å¼¥åˆäº†ç»´åº¦ä¹‹é—´çš„å·®è·ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨2Dé¢†åŸŸä¸­å¯è§†åŒ–ç‚¹äº‘çš„å¤æ‚æ€§ï¼Œè¿™æ­£æ˜¯SAMæ‰€éœ€çš„è¾“å…¥ã€‚ç‚¹äº‘æ˜ å°„ä¸ºè¿™ä¸€æŠ•å½±è¿‡ç¨‹å¢æ·»äº†ä¸€å±‚ç†è§£ã€‚
- en: '3.1 Ortho Projection: Flattening Dimensions, Expanding Insights'
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 æ­£å°„æŠ•å½±ï¼šå¹³å±•ç»´åº¦ï¼Œæ‰©å±•æ´å¯Ÿ
- en: Let us look at the transformative technique of Ortho Projection. This method
    serves as an excellent bridge between the multi-dimensional complexities of 3D
    point clouds and the comprehensible world of 2D images. Through Ortho Projection,
    we â€œflattenâ€ dimensions but also unveil a direct way to manage segmentation with
    SAM.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹æ­£å°„æŠ•å½±çš„å˜é©æ€§æŠ€æœ¯ã€‚æ­¤æ–¹æ³•ä½œä¸º3Dç‚¹äº‘çš„å¤šç»´å¤æ‚æ€§ä¸2Då›¾åƒçš„å¯ç†è§£ä¸–ç•Œä¹‹é—´çš„ç»ä½³æ¡¥æ¢ã€‚é€šè¿‡æ­£å°„æŠ•å½±ï¼Œæˆ‘ä»¬â€œå¹³å±•â€ç»´åº¦ï¼ŒåŒæ—¶æ­ç¤ºäº†ä½¿ç”¨SAMè¿›è¡Œåˆ†å‰²çš„ç›´æ¥æ–¹æ³•ã€‚
- en: The idea is basically to generate a top-down view plane and generate an image
    that is not constrained by a single perspective. You could see ortho-projection
    as a process of pushing visible points from the point cloud (highest ones) onto
    the plane that holds the empty image to fill all the necessary pixels just above
    those points. You can see the difference from a perspective view, as illustrated
    below.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæƒ³æ³•åŸºæœ¬ä¸Šæ˜¯ç”Ÿæˆä¸€ä¸ªä¿¯è§†è§†å›¾å¹³é¢ï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªä¸å—å•ä¸€è§†è§’é™åˆ¶çš„å›¾åƒã€‚ä½ å¯ä»¥å°†æ­£å°„æŠ•å½±è§†ä¸ºå°†ç‚¹äº‘ä¸­çš„å¯è§ç‚¹ï¼ˆæœ€é«˜ç‚¹ï¼‰æ¨é€åˆ°æŒæœ‰ç©ºå›¾åƒçš„å¹³é¢ä¸Šï¼Œä»¥å¡«å……æ‰€æœ‰å¿…è¦çš„åƒç´ ã€‚ä½ å¯ä»¥çœ‹åˆ°ä¸é€è§†è§†å›¾çš„ä¸åŒä¹‹å¤„ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚
- en: '![](../Images/efcf4ea1b6062d00cb933ab9b645ff17.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/efcf4ea1b6062d00cb933ab9b645ff17.png)'
- en: Explanation of the difference between Orthographic View, and Perspective View
    for 3D Projections. The Perspective View is linked to a single point of view that
    skews dimensions. Â© [F. Poux](https://learngeodata.eu/)
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: è§£é‡Šæ­£å°„è§†å›¾å’Œé€è§†è§†å›¾åœ¨3DæŠ•å½±ä¸­çš„åŒºåˆ«ã€‚é€è§†è§†å›¾ä¸ä¸€ä¸ªå•ç‹¬çš„è§†ç‚¹ç›¸å…³ï¼Œè¿™ä¼šæ‰­æ›²ç»´åº¦ã€‚Â© [F. Poux](https://learngeodata.eu/)
- en: 'To work out this process, we can define a 3D-to-2D projection function that
    would take the points of a point cloud alongside its color and a wanted resolution
    to compute the ortho-projection and return an orthoimage from the point cloud.
    This would translate into the following:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®Œæˆè¿™ä¸ªè¿‡ç¨‹ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ª3Dåˆ°2Dçš„æŠ•å½±å‡½æ•°ï¼Œå®ƒå°†ç‚¹äº‘çš„ç‚¹åŠå…¶é¢œè‰²å’Œæ‰€éœ€åˆ†è¾¨ç‡ä½œä¸ºè¾“å…¥ï¼Œè®¡ç®—æ­£å°„æŠ•å½±å¹¶ä»ç‚¹äº‘ä¸­è¿”å›æ­£å°„å›¾åƒã€‚è¿™å°†è½¬åŒ–ä¸ºä»¥ä¸‹å†…å®¹ï¼š
- en: '[PRE14]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Great, now it is time for a test, do you agree? To do so, let us load a point
    cloud dataset, transform it to a numpy array, apply the function, and export an
    image of this point cloud:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¥½ï¼Œç°åœ¨æ˜¯æ—¶å€™è¿›è¡Œæµ‹è¯•äº†ï¼Œä½ åŒæ„å—ï¼Ÿä¸ºæ­¤ï¼Œè®©æˆ‘ä»¬åŠ è½½ä¸€ä¸ªç‚¹äº‘æ•°æ®é›†ï¼Œå°†å…¶è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼Œåº”ç”¨è¿™ä¸ªå‡½æ•°ï¼Œå¹¶å¯¼å‡ºè¿™ä¸ªç‚¹äº‘çš„å›¾åƒï¼š
- en: '[PRE15]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This permits us to obtain the following:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å…è®¸æˆ‘ä»¬è·å¾—ä»¥ä¸‹å†…å®¹ï¼š
- en: '![](../Images/65931fd258f06d536ead6c908d1c2c17.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/65931fd258f06d536ead6c908d1c2c17.png)'
- en: The workflow to go from 3D Point Clouds to Orthoimages. We first project the
    point cloud following an ortho-projection mode, then we make sure to include a
    point-to-pixel mapping for back-projection. Â© [F. Poux](https://learngeodata.eu/)
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ä»3Dç‚¹äº‘åˆ°æ­£å°„å›¾åƒçš„å·¥ä½œæµç¨‹ã€‚æˆ‘ä»¬é¦–å…ˆæŒ‰ç…§æ­£å°„æŠ•å½±æ¨¡å¼å¯¹ç‚¹äº‘è¿›è¡ŒæŠ•å½±ï¼Œç„¶åç¡®ä¿åŒ…æ‹¬ç‚¹åˆ°åƒç´ çš„æ˜ å°„ç”¨äºåæŠ•å½±ã€‚Â© [F. Poux](https://learngeodata.eu/)
- en: Let us move on to spherical projections
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç»§ç»­çƒé¢æŠ•å½±
- en: 3.2 3D Point Cloud Spherical Projection
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 3D ç‚¹äº‘çƒé¢æŠ•å½±
- en: 'Our journey takes an intriguing turn as we encounter Spherical Projection.
    This technique offers a unique perspective, enabling us to visualize the data
    by â€œsimulatingâ€ a virtual scan station. To do just this, we proceed in four steps
    by: (1) Considering the 3D Point Cloud, (2) Projecting these points onto a sphere,
    (3) defining a geometry that will retrieve the pixels, (4) â€œflatteningâ€ this geometry
    to produce an image.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„æ—…ç¨‹åœ¨é‡åˆ°çƒé¢æŠ•å½±æ—¶å˜å¾—éå¸¸æœ‰è¶£ã€‚è¿™ç§æŠ€æœ¯æä¾›äº†ç‹¬ç‰¹çš„è§†è§’ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡â€œæ¨¡æ‹Ÿâ€è™šæ‹Ÿæ‰«æç«™æ¥å¯è§†åŒ–æ•°æ®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é€šè¿‡ä»¥ä¸‹å››ä¸ªæ­¥éª¤è¿›è¡Œï¼š (1)
    è€ƒè™‘3Dç‚¹äº‘ï¼Œ (2) å°†è¿™äº›ç‚¹æŠ•å½±åˆ°çƒä½“ä¸Šï¼Œ (3) å®šä¹‰ä¸€ä¸ªå‡ ä½•ä½“æ¥æ£€ç´¢åƒç´ ï¼Œ (4) â€œå¹³å±•â€è¿™ä¸ªå‡ ä½•ä½“ä»¥ç”Ÿæˆå›¾åƒã€‚
- en: '**ğŸ¤  Ville**: *Spherical projection is like being inside a 3D point cloud and
    taking a 360-degree photo of what you see*.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸ¤  Ville**: *çƒé¢æŠ•å½±å°±åƒæ˜¯åœ¨3Dç‚¹äº‘å†…éƒ¨ï¼Œæ‹æ‘„ä½ çœ‹åˆ°çš„360åº¦ç…§ç‰‡*ã€‚'
- en: '![](../Images/503ca251cad894ed496dd046f9732807.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/503ca251cad894ed496dd046f9732807.png)'
- en: The 3D Point Cloud Spherical Projection Workflow. We take a 3D Point Cloud,
    we create a 3D Projection Sphere, we define the mapping plane, and we produce
    an equirectangular projection. Â© [F. Poux](https://learngeodata.eu/)
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 3D ç‚¹äº‘çƒé¢æŠ•å½±å·¥ä½œæµã€‚æˆ‘ä»¬è·å– 3D ç‚¹äº‘ï¼Œåˆ›å»ºä¸€ä¸ª 3D æŠ•å½±çƒä½“ï¼Œå®šä¹‰æ˜ å°„å¹³é¢ï¼Œå¹¶ç”Ÿæˆç­‰è·åœ†æŸ±æŠ•å½±ã€‚Â© [F. Poux](https://learngeodata.eu/)
- en: To achieve the 3D Projection onto a sphere, we want to obtain points as illustrated
    below.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®ç° 3D æŠ•å½±åˆ°çƒé¢ï¼Œæˆ‘ä»¬å¸Œæœ›è·å¾—å¦‚ä¸‹æ‰€ç¤ºçš„ç‚¹ã€‚
- en: '![](../Images/4d82f7a78bb0e5f5eb5b32ed2c379f96.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d82f7a78bb0e5f5eb5b32ed2c379f96.png)'
- en: How to project points of a 3D point clouds onto a sphere. Â© [F. Poux](https://learngeodata.eu/)
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½•å°† 3D ç‚¹äº‘çš„ç‚¹æŠ•å½±åˆ°çƒé¢ã€‚Â© [F. Poux](https://learngeodata.eu/)
- en: Then, we will unroll following our geometry (cylinder) to obtain an equirectangular
    image, as shown below.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å°†æ ¹æ®å‡ ä½•å½¢çŠ¶ï¼ˆåœ†æŸ±ä½“ï¼‰å±•å¼€ï¼Œä»¥è·å¾—ç­‰è·åœ†æŸ±å›¾åƒï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚
- en: '![](../Images/1f0023fe4cecfde6980edba0c234293d.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f0023fe4cecfde6980edba0c234293d.png)'
- en: How to go from a sphere to an equirectangular image. We have a projection mechanism
    that permits us to â€œunrollâ€ the pixels onto a cylinder. Â© [F. Poux](https://learngeodata.eu/)
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½•ä»çƒé¢åˆ°ç­‰è·åœ†æŸ±å›¾åƒã€‚æˆ‘ä»¬æœ‰ä¸€ä¸ªæŠ•å½±æœºåˆ¶ï¼Œå…è®¸æˆ‘ä»¬å°†åƒç´ â€œå±•å¼€â€åˆ°åœ†æŸ±ä½“ä¸Šã€‚Â© [F. Poux](https://learngeodata.eu/)
- en: 'Let me now detail the function that allows just this:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘è¯¦ç»†ä»‹ç»å…è®¸å®ç°è¿™ä¸€ç‚¹çš„åŠŸèƒ½ï¼š
- en: '[PRE16]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'ğŸŒ± **Growing**: *It is essential to digest this function. It looks like it is
    pretty straightforward, but there are nice tricks at several stages. For example,
    what do you think about the 3D point cloud to spherical coordinates step? What
    does the mapping do? What is the point of using the mapping as a conditional statement
    while assigning points to pixels?*'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸŒ± **Growing**ï¼š*æ¶ˆåŒ–è¿™ä¸ªåŠŸèƒ½æ˜¯è‡³å…³é‡è¦çš„ã€‚å®ƒçœ‹èµ·æ¥å¾ˆç®€å•ï¼Œä½†åœ¨å¤šä¸ªé˜¶æ®µæœ‰ä¸€äº›å·§å¦™çš„æŠ€å·§ã€‚ä¾‹å¦‚ï¼Œä½ å¯¹ 3D ç‚¹äº‘åˆ°çƒé¢åæ ‡æ­¥éª¤æœ‰ä»€ä¹ˆçœ‹æ³•ï¼Ÿæ˜ å°„çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿåœ¨å°†ç‚¹åˆ†é…ç»™åƒç´ æ—¶ï¼Œä½¿ç”¨æ˜ å°„ä½œä¸ºæ¡ä»¶è¯­å¥çš„æ„ä¹‰ä½•åœ¨ï¼Ÿ*
- en: 'Now, to use this handy function, let us load and prepare the ITC indoor point
    cloud first:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œä¸ºäº†ä½¿ç”¨è¿™ä¸ªæ–¹ä¾¿çš„åŠŸèƒ½ï¼Œæˆ‘ä»¬é¦–å…ˆåŠ è½½å¹¶å‡†å¤‡ ITC å®¤å†…ç‚¹äº‘ï¼š
- en: '[PRE17]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Once prepared, we can define the necessary parameters for projection. These
    are the center of projection (basically the position from which we want a virtual
    scan station) and the resolution of the final image (expressed in pixels, as the
    smallest side of the image).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: å‡†å¤‡å¥½åï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰æŠ•å½±æ‰€éœ€çš„å‚æ•°ã€‚è¿™äº›å‚æ•°åŒ…æ‹¬æŠ•å½±ä¸­å¿ƒï¼ˆåŸºæœ¬ä¸Šæ˜¯æˆ‘ä»¬å¸Œæœ›è™šæ‹Ÿæ‰«æç«™çš„ä½ç½®ï¼‰å’Œæœ€ç»ˆå›¾åƒçš„åˆ†è¾¨ç‡ï¼ˆä»¥åƒç´ è¡¨ç¤ºï¼Œå³å›¾åƒçš„æœ€å°è¾¹ï¼‰ã€‚
- en: '[PRE18]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Finally, we can call the new function, plot and export the results as an image
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å¯ä»¥è°ƒç”¨æ–°çš„å‡½æ•°ï¼Œç»˜åˆ¶å¹¶å°†ç»“æœå¯¼å‡ºä¸ºå›¾åƒã€‚
- en: '[PRE19]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'All this process results in the following image:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è¿™äº›è¿‡ç¨‹ä¼šäº§ç”Ÿä»¥ä¸‹å›¾åƒï¼š
- en: '![](../Images/94689a7ca448ddb2e27d84adc9e14cb1.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94689a7ca448ddb2e27d84adc9e14cb1.png)'
- en: The 3D point cloud transformed as an equirectangular image from the projection.
    Â© [F. Poux](https://learngeodata.eu/)
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 3D ç‚¹äº‘é€šè¿‡æŠ•å½±è½¬åŒ–ä¸ºç­‰è·åœ†æŸ±å›¾åƒã€‚Â© [F. Poux](https://learngeodata.eu/)
- en: How do you like that? You can play around with the various parameters, such
    as the resolution or the center of projection, to ensure that you get a nice balance
    between â€œno dataâ€ pixels and relevant panorama.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ è§‰å¾—æ€ä¹ˆæ ·ï¼Ÿä½ å¯ä»¥è°ƒæ•´å„ç§å‚æ•°ï¼Œå¦‚åˆ†è¾¨ç‡æˆ–æŠ•å½±ä¸­å¿ƒï¼Œä»¥ç¡®ä¿åœ¨â€œæ— æ•°æ®â€åƒç´ å’Œç›¸å…³å…¨æ™¯ä¹‹é—´å–å¾—è‰¯å¥½çš„å¹³è¡¡ã€‚
- en: 'ğŸ¦Š **Florent**: *You just unlocked a powerful new skill with 3D Point Cloud
    to Equirectangular image creation. Indeed, it allows you to generate virtual scans
    basically wherever you believe it makes sense and then unlock the possibility
    to use image processing and deep learning techniques for images. You can also
    extend the provided function to other mapping projections to add arrows to your
    quiver.*'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¦Š **Florent**ï¼š*ä½ åˆšåˆšè§£é”äº†ä¸€é¡¹å¼ºå¤§çš„æ–°æŠ€èƒ½â€”â€”å°† 3D ç‚¹äº‘è½¬æ¢ä¸ºç­‰è·åœ†æŸ±å›¾åƒã€‚ç¡®å®ï¼Œå®ƒå…è®¸ä½ åœ¨ä½ è®¤ä¸ºæœ‰æ„ä¹‰çš„åœ°æ–¹ç”Ÿæˆè™šæ‹Ÿæ‰«æï¼Œå¹¶å¼€å¯ä½¿ç”¨å›¾åƒå¤„ç†å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯å¤„ç†å›¾åƒçš„å¯èƒ½æ€§ã€‚ä½ è¿˜å¯ä»¥å°†æä¾›çš„åŠŸèƒ½æ‰©å±•åˆ°å…¶ä»–æ˜ å°„æŠ•å½±ï¼Œä»¥å¢åŠ ä½ çš„å·¥å…·åº“ã€‚*
- en: 'ğŸ¤ **Ville**: *I can almost see the lecture halls and my office, Dutch working
    vibes!*'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤ **Ville**ï¼š*æˆ‘å‡ ä¹å¯ä»¥çœ‹åˆ°è®²åº§å¤§å…å’Œæˆ‘çš„åŠå…¬å®¤ï¼Œè·å…°çš„å·¥ä½œæ°›å›´ï¼*
- en: 3.3 3D Point-to-Pixel Mapping
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 3D ç‚¹åˆ°åƒç´ çš„æ˜ å°„
- en: 'We transform raw point data into structured raster representations, making
    sense of the seemingly scattered information. Point Cloud Mapping is the compass
    that guides us for 3D point cloud processing through 2D projection. The good news:
    we already took care of this mapping.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†åŸå§‹ç‚¹æ•°æ®è½¬æ¢ä¸ºç»“æ„åŒ–çš„æ …æ ¼è¡¨ç¤ºï¼Œç†æ¸…çœ‹ä¼¼æ•£ä¹±çš„ä¿¡æ¯ã€‚ç‚¹äº‘æ˜ å°„æ˜¯æˆ‘ä»¬åœ¨ 2D æŠ•å½±ä¸­å¤„ç† 3D ç‚¹äº‘çš„æŒ‡å—é’ˆã€‚å¥½æ¶ˆæ¯æ˜¯ï¼šæˆ‘ä»¬å·²ç»å¤„ç†äº†è¿™ä¸ªæ˜ å°„ã€‚
- en: Indeed, if you take a close look at the function `generate_spherical_image`,
    you can see that we return the `mapping` variable and capture it in another variable
    for downward processes. This ensures that we can have a coherent 3D Point-to-Pixel
    Mapping.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®å®ï¼Œå¦‚æœä½ ä»”ç»†æŸ¥çœ‹å‡½æ•°`generate_spherical_image`ï¼Œä½ ä¼šå‘ç°æˆ‘ä»¬è¿”å›äº†`mapping`å˜é‡å¹¶å°†å…¶æ•è·åˆ°å¦ä¸€ä¸ªå˜é‡ä¸­ä»¥ä¾¿åç»­å¤„ç†ã€‚è¿™ç¡®ä¿äº†æˆ‘ä»¬å¯ä»¥æ‹¥æœ‰ä¸€è‡´çš„3Dç‚¹åˆ°åƒç´ çš„æ˜ å°„ã€‚
- en: 4\. Unsupervised Segmentation with SAM
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. ä½¿ç”¨SAMçš„æ— ç›‘ç£åˆ†å‰²
- en: Unsupervised segmentation enters the scene in the form of the Segment Anything
    Model. We are, in the case of non-labeled outputs, through SAMâ€™s segmentation
    architecture, which falls within clustering applications. This is opposed to most
    supervised learning approaches that will provide labeled outputs, as illustrated
    below.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: æ— ç›‘ç£åˆ†å‰²ä»¥Segment Anythingæ¨¡å‹çš„å½¢å¼å‡ºç°ã€‚åœ¨éæ ‡è®°è¾“å‡ºçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬é€šè¿‡SAMçš„åˆ†å‰²æ¶æ„ï¼Œè¿™å±äºèšç±»åº”ç”¨ã€‚è¿™ä¸å¤§å¤šæ•°ç›‘ç£å­¦ä¹ æ–¹æ³•æä¾›æ ‡è®°è¾“å‡ºçš„æ–¹å¼ç›¸å¯¹ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚
- en: '![](../Images/8dc14d35b848c2681114cb297c086c0d.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8dc14d35b848c2681114cb297c086c0d.png)'
- en: The distinction between unsupervised learning and supervised learning. In unsupervised
    learning, we aim at defining groups of data â€œpointsâ€ that share some similarity,
    whereas in supervised learning, we aim at approaching the supervision needs (usually
    by feeding labeled data). Â© [F. Poux](https://learngeodata.eu/)
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: æ— ç›‘ç£å­¦ä¹ å’Œç›‘ç£å­¦ä¹ ä¹‹é—´çš„åŒºåˆ«ã€‚åœ¨æ— ç›‘ç£å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨å®šä¹‰ä¸€äº›ç›¸ä¼¼çš„æ•°æ®â€œç‚¹â€ç»„ï¼Œè€Œåœ¨ç›‘ç£å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨æ»¡è¶³ç›‘ç£éœ€æ±‚ï¼ˆé€šå¸¸é€šè¿‡æä¾›æ ‡è®°æ•°æ®ï¼‰ã€‚Â© [F.
    Poux](https://learngeodata.eu/)
- en: Therefore, the transfer of pixel predictions, coupled with seamless point cloud
    export, showcases the potential for revolutionizing applications like object detection
    and scene understanding.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œåƒç´ é¢„æµ‹çš„è½¬ç§»ï¼ŒåŠ ä¸Šæ— ç¼çš„ç‚¹äº‘å¯¼å‡ºï¼Œå±•ç¤ºäº†é©æ–°ç‰©ä½“æ£€æµ‹å’Œåœºæ™¯ç†è§£ç­‰åº”ç”¨çš„æ½œåŠ›ã€‚
- en: 4.1\. SAM Segmentation
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1\. SAMåˆ†å‰²
- en: 'To execute the program, we can re-execute the code snippets that we used to
    test our SAM functionalities on 2D images, which are:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æ‰§è¡Œç¨‹åºï¼Œæˆ‘ä»¬å¯ä»¥é‡æ–°æ‰§è¡Œæˆ‘ä»¬ç”¨äºæµ‹è¯•SAMåŠŸèƒ½åœ¨2Då›¾åƒä¸Šçš„ä»£ç ç‰‡æ®µï¼Œè¿™äº›ä»£ç ç‰‡æ®µæ˜¯ï¼š
- en: '[PRE20]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: and later, we can plot the results on the image itself
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å›¾åƒä¸Šç»˜åˆ¶ç»“æœ
- en: '[PRE21]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Which results in:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†å¾—åˆ°ï¼š
- en: '![](../Images/33b40e186df07c0e404488d127f5a6ec.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/33b40e186df07c0e404488d127f5a6ec.png)'
- en: Results of the Segment Anything Model on the 3D point cloud projection. Â© [F.
    Poux](https://learngeodata.eu/)
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 3Dç‚¹äº‘æŠ•å½±ä¸Šçš„Segment Anythingæ¨¡å‹çš„ç»“æœã€‚Â© [F. Poux](https://learngeodata.eu/)
- en: This already looks like we are delineating significant parts of the image. Let
    us move forward with this.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™çœ‹èµ·æ¥æˆ‘ä»¬æ­£åœ¨å‹¾ç”»å‡ºå›¾åƒä¸­çš„é‡è¦éƒ¨åˆ†ã€‚è®©æˆ‘ä»¬ç»§ç»­å‰è¿›ã€‚
- en: 4.2\. Point Prediction Transfer
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2\. ç‚¹é¢„æµ‹è½¬ç§»
- en: 'Let us color the point cloud with this image. We thus define a coloring function:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç”¨è¿™å¼ å›¾åƒä¸ºç‚¹äº‘ä¸Šè‰²ã€‚å› æ­¤æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªä¸Šè‰²å‡½æ•°ï¼š
- en: '[PRE22]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This means that to color our point cloud, we can use the following code line
    that calls our new function:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€è¦ä¸ºæˆ‘ä»¬çš„ç‚¹äº‘ä¸Šè‰²ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç è¡Œè°ƒç”¨æˆ‘ä»¬çš„æ–°å‡½æ•°ï¼š
- en: '[PRE23]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This line returns a numpy array that holds the point cloud.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€è¡Œè¿”å›ä¸€ä¸ªnumpyæ•°ç»„ï¼Œè¯¥æ•°ç»„ä¿å­˜äº†ç‚¹äº‘ã€‚
- en: It is now time for 3D Point Cloud Export!
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ˜¯3Dç‚¹äº‘å¯¼å‡ºçš„æ—¶å€™äº†ï¼
- en: 4.3\. Point Cloud Export
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3\. ç‚¹äº‘å¯¼å‡º
- en: 'To export the point cloud, you can use numpy or laspy to extract a .las file
    directly. We will proceed with the second solution:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å¯¼å‡ºç‚¹äº‘ï¼Œä½ å¯ä»¥ä½¿ç”¨numpyæˆ–laspyç›´æ¥æå–ä¸€ä¸ª.lasæ–‡ä»¶ã€‚æˆ‘ä»¬å°†é‡‡ç”¨ç¬¬äºŒç§è§£å†³æ–¹æ¡ˆï¼š
- en: '[PRE24]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'And with this, we can export our modified_point_cloud variable:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·ï¼Œæˆ‘ä»¬å¯ä»¥å¯¼å‡ºæˆ‘ä»¬çš„modified_point_cloudå˜é‡ï¼š
- en: '[PRE25]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: After this stage, we successfully taken our various 2D images resulting from
    the 3D point cloud projection process. We applied SAM algorithm to it, colorized
    it based on its prediction, and exported a colored point cloud. We can thus move
    to getting some insights about what we are getting.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬æˆåŠŸåœ°è·å–äº†å„ç§æ¥è‡ª3Dç‚¹äº‘æŠ•å½±è¿‡ç¨‹çš„2Då›¾åƒã€‚æˆ‘ä»¬å¯¹å…¶åº”ç”¨äº†SAMç®—æ³•ï¼ŒåŸºäºå…¶é¢„æµ‹å¯¹å…¶ä¸Šè‰²ï¼Œå¹¶å¯¼å‡ºäº†ä¸€ä¸ªå½©è‰²ç‚¹äº‘ã€‚å› æ­¤æˆ‘ä»¬å¯ä»¥å¼€å§‹è·å–ä¸€äº›å…³äºæˆ‘ä»¬æ‰€å¾—åˆ°çš„ä¸œè¥¿çš„è§è§£ã€‚
- en: 'ğŸ¦Š **Florent**: *To analyze the result quickly outside Python, I recommend using
    the CloudCompare Open-Source Software. If you want a clear guide on how to use
    it efficiently, you can read and follow through the article below.*'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¦Š **Florent**ï¼š*ä¸ºäº†å¿«é€Ÿåœ¨Pythonä¹‹å¤–åˆ†æç»“æœï¼Œæˆ‘å»ºè®®ä½¿ç”¨CloudCompareå¼€æºè½¯ä»¶ã€‚å¦‚æœä½ æƒ³è¦ä¸€ä¸ªæ¸…æ™°çš„ä½¿ç”¨æŒ‡å—ï¼Œå¯ä»¥é˜…è¯»ä¸‹é¢çš„æ–‡ç« ã€‚*
- en: '[](/3d-deep-learning-python-tutorial-pointnet-data-preparation-90398f880c9f?source=post_page-----80c06be99a18--------------------------------)
    [## 3D Deep Learning Python Tutorial: PointNet Data Preparation'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/3d-deep-learning-python-tutorial-pointnet-data-preparation-90398f880c9f?source=post_page-----80c06be99a18--------------------------------)
    [## 3Dæ·±åº¦å­¦ä¹ Pythonæ•™ç¨‹ï¼šPointNetæ•°æ®å‡†å¤‡'
- en: The Ultimate Python Guide to structure large LiDAR point cloud for training
    a 3D Deep Learning Semantic Segmentationâ€¦
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ã€Šç»ˆæPythonæŒ‡å—ã€‹ç”¨äºæ„å»ºå¤§å‹LiDARç‚¹äº‘ä»¥è®­ç»ƒ3Dæ·±åº¦å­¦ä¹ è¯­ä¹‰åˆ†å‰²â€¦
- en: towardsdatascience.com](/3d-deep-learning-python-tutorial-pointnet-data-preparation-90398f880c9f?source=post_page-----80c06be99a18--------------------------------)
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/3d-deep-learning-python-tutorial-pointnet-data-preparation-90398f880c9f?source=post_page-----80c06be99a18--------------------------------)'
- en: 5\. Qualitative Analysis and discussions
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5. å®šæ€§åˆ†æä¸è®¨è®º
- en: With our journey nearing its zenith, itâ€™s time to focus on qualitative analysis.
    Exceptionally, we will not conduct a quantitative analysis, as we would need proper
    labels for that at this stage.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€æˆ‘ä»¬çš„æ—…ç¨‹æ¥è¿‘å·…å³°ï¼Œç°åœ¨æ˜¯å…³æ³¨å®šæ€§åˆ†æçš„æ—¶å€™äº†ã€‚ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬ä¸ä¼šè¿›è¡Œå®šé‡åˆ†æï¼Œå› ä¸ºåœ¨è¿™ä¸ªé˜¶æ®µæˆ‘ä»¬éœ€è¦é€‚å½“çš„æ ‡ç­¾ã€‚
- en: 'ğŸ¤  **Ville**: No labels? What you just did was zero-shot learning (Bang!) or
    few-shot learning (Bang! Bang!). We cannot be sure which because we donâ€™t know
    exactly how SAM was trained by Meta. Therefore it is a bit of a black box for
    us, but thatâ€™s ok.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğŸ¤  **Ville**: æ²¡æœ‰æ ‡ç­¾ï¼Ÿä½ åˆšåˆšåšçš„æ˜¯é›¶æ ·æœ¬å­¦ä¹ ï¼ˆç °ï¼ï¼‰æˆ–å°‘æ ·æœ¬å­¦ä¹ ï¼ˆç °ï¼ç °ï¼ï¼‰ã€‚æˆ‘ä»¬ä¸èƒ½ç¡®å®šæ˜¯å“ªç§ï¼Œå› ä¸ºæˆ‘ä»¬ä¸çŸ¥é“SAMçš„è®­ç»ƒæ–¹å¼ã€‚å› æ­¤å¯¹æˆ‘ä»¬æ¥è¯´å®ƒæœ‰ç‚¹åƒé»‘ç®±ï¼Œä½†æ²¡å…³ç³»ã€‚'
- en: We meticulously examine the raster and point cloud results, drawing insights
    that shed light on SAMâ€™s performance. Also, let us remain grounded by acknowledging
    the modelâ€™s limitations while setting our sights on the future.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»”ç»†æ£€æŸ¥å…‰æ …å’Œç‚¹äº‘ç»“æœï¼Œå¾—å‡ºçš„è§è§£æ­ç¤ºäº†SAMçš„æ€§èƒ½ã€‚åŒæ—¶ï¼Œè®©æˆ‘ä»¬ä¿æŒè„šè¸å®åœ°ï¼Œæ‰¿è®¤æ¨¡å‹çš„å±€é™æ€§ï¼ŒåŒæ—¶å±•æœ›æœªæ¥ã€‚
- en: 5.1 Raster results
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 å…‰æ …ç»“æœ
- en: The output of SAMâ€™s efforts with our implementation is eloquently depicted through
    the below raster results. These visuals serve as a canvas on which SAMâ€™s segmentation
    can be quickly assessed, enabling us to comprehend the modelâ€™s understanding of
    the scene in a 2D representation.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: SAMåœ¨æˆ‘ä»¬å®æ–½ä¸‹çš„æˆæœé€šè¿‡ä¸‹è¿°å…‰æ …ç»“æœå¾—åˆ°äº†ç”ŸåŠ¨çš„å±•ç¤ºã€‚è¿™äº›è§†è§‰æ•ˆæœä½œä¸ºç”»å¸ƒï¼Œå¿«é€Ÿè¯„ä¼°SAMçš„åˆ†å‰²ï¼Œå¸®åŠ©æˆ‘ä»¬ç†è§£æ¨¡å‹å¯¹åœºæ™¯çš„2Dè¡¨ç¤ºã€‚
- en: '![](../Images/77d9f078a781d656b12f76e839573662.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77d9f078a781d656b12f76e839573662.png)'
- en: Another result of the Segment Anything Model on the 3D point cloud projection.
    Â© [F. Poux](https://learngeodata.eu/)
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything Modelåœ¨3Dç‚¹äº‘æŠ•å½±ä¸Šçš„å¦ä¸€ä¸ªç»“æœã€‚Â© [F. Poux](https://learngeodata.eu/)
- en: As you can see, even with the uneven point distribution and â€œblack zonesâ€, SAM
    is able to pick up what the main parts of the point cloud are about. Specifically,
    it likely highlights a green on the left, the place where the dangerous materials
    are, and the doors and windows for our extraction team to have the most direct
    route!
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œå³ä½¿åœ¨ç‚¹åˆ†å¸ƒä¸å‡å’Œâ€œé»‘åŒºâ€ä¸‹ï¼ŒSAMä»èƒ½è¯†åˆ«å‡ºç‚¹äº‘çš„ä¸»è¦éƒ¨åˆ†ã€‚å…·ä½“è€Œè¨€ï¼Œå®ƒå¯èƒ½çªå‡ºäº†å·¦ä¾§çš„ç»¿è‰²åŒºåŸŸï¼Œå³å±é™©ææ–™æ‰€åœ¨çš„ä½ç½®ï¼Œä»¥åŠä¸ºæˆ‘ä»¬æå–å›¢é˜Ÿæä¾›æœ€ç›´æ¥è·¯çº¿çš„é—¨çª—ï¼
- en: 5.2\. Point Cloud results
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 ç‚¹äº‘ç»“æœ
- en: Yet, itâ€™s in the point cloud results that the true depth of SAMâ€™s abilities
    emerges. As we navigate through the cloud of points, SAMâ€™s segmented predictions
    bring clarity to the classical â€œmess of pointsâ€, showcasing its potential in real-world
    applications.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæ­£æ˜¯åœ¨ç‚¹äº‘ç»“æœä¸­ï¼ŒSAMçš„çœŸæ­£èƒ½åŠ›å¾—ä»¥æ˜¾ç°ã€‚å½“æˆ‘ä»¬åœ¨ç‚¹äº‘ä¸­ç©¿æ¢­æ—¶ï¼ŒSAMçš„åˆ†å‰²é¢„æµ‹ä¸ºç»å…¸çš„â€œç‚¹äº‘æ··ä¹±â€å¸¦æ¥äº†æ¸…æ™°åº¦ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚
- en: '![](../Images/aa21106712342d39785489b7db32c5eb.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa21106712342d39785489b7db32c5eb.png)'
- en: The 3D Point Cloud unsupervised segmentation results by using Segment Anything
    3D. We see the great distinction of the major elements that compose the scene.
    Â© [F. Poux](https://learngeodata.eu/)
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Segment Anything 3Dè¿›è¡Œçš„3Dç‚¹äº‘æ— ç›‘ç£åˆ†å‰²ç»“æœã€‚æˆ‘ä»¬çœ‹åˆ°æ„æˆåœºæ™¯çš„ä¸»è¦å…ƒç´ çš„æ˜æ˜¾åŒºåˆ«ã€‚Â© [F. Poux](https://learngeodata.eu/)
- en: As we can see, we can have a direct link with the underlying points, and that
    is massively awesome! Think only about what this can unlock for your applications.
    A 100% automated segmentation process that has under five main breakpoints? Not
    bad!
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬æ‰€è§ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥é“¾æ¥åˆ°åŸºç¡€ç‚¹ï¼Œè¿™çœŸæ˜¯æå…¶æ£’ï¼æƒ³æƒ³è¿™èƒ½ä¸ºä½ çš„åº”ç”¨ç¨‹åºè§£é”ä»€ä¹ˆã€‚ä¸€ä¸ªæ‹¥æœ‰ä¸åˆ°äº”ä¸ªä¸»è¦æ–­ç‚¹çš„100%è‡ªåŠ¨åŒ–åˆ†å‰²è¿‡ç¨‹ï¼Ÿä¸é”™ï¼
- en: 5.3\. Shortcomings
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3. å±€é™æ€§
- en: But, our expedition would only be complete with acknowledging the rough patches
    along the way. SAM, while impressive, is not exempt from limitations. By recognizing
    these shortcomings, we pave the way for refinement and growth
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ï¼Œæˆ‘ä»¬çš„æ¢é™©åªæœ‰åœ¨æ‰¿è®¤è¿‡ç¨‹ä¸­å­˜åœ¨çš„ç²—ç³™ç‚¹åæ‰ç®—å®Œæˆã€‚SAMï¼Œå°½ç®¡ä»¤äººå°è±¡æ·±åˆ»ï¼Œä¹Ÿä¸ä¾‹å¤–åœ°å­˜åœ¨å±€é™æ€§ã€‚é€šè¿‡è®¤è¯†è¿™äº›ä¸è¶³ï¼Œæˆ‘ä»¬ä¸ºæ”¹è¿›å’Œæˆé•¿é“ºå¹³é“è·¯ã€‚
- en: The first thing is that all the â€œunseenâ€ points remain unlabelled (white points
    below). This could prove to be a limitation for complete processing, and if you
    use the basic or the large model you will see more unlabelled points than when
    using the huge model.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆæ˜¯æ‰€æœ‰â€œæœªè§â€çš„ç‚¹ä»ç„¶ä¿æŒæœªæ ‡è®°ï¼ˆä¸‹å›¾ä¸­çš„ç™½ç‚¹ï¼‰ã€‚è¿™å¯èƒ½ä¼šæˆä¸ºå®Œæ•´å¤„ç†çš„ä¸€ä¸ªé™åˆ¶ï¼Œå¦‚æœä½ ä½¿ç”¨åŸºæœ¬æ¨¡å‹æˆ–å¤§æ¨¡å‹ï¼Œä½ ä¼šçœ‹åˆ°æ¯”ä½¿ç”¨å·¨å¤§æ¨¡å‹æ—¶æ›´å¤šçš„æœªæ ‡è®°ç‚¹ã€‚
- en: '![](../Images/07d064a4ed2ff0080884fc84c4ce052f.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07d064a4ed2ff0080884fc84c4ce052f.png)'
- en: The ratio of unlabelled points vs. labeled points from the first pass with a
    central perspective 360Â° simulated scan position. Â© [F. Poux](https://learngeodata.eu/)
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä¸­å¤®è§†è§’360Â°æ¨¡æ‹Ÿæ‰«æä½ç½®çš„ç¬¬ä¸€æ¬¡æ‰«æä¸­ï¼Œæœªæ ‡è®°ç‚¹ä¸æ ‡è®°ç‚¹çš„æ¯”ä¾‹ã€‚ Â© [F. Poux](https://learngeodata.eu/)
- en: Also, at this stage, we used the automatic prompting engine that triggered around
    50 points of interest, the seeds of the segmentation task. While this is great
    for getting a direct result, having the possibility to tune that would be awesome.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œåœ¨è¿™ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨äº†è‡ªåŠ¨æç¤ºå¼•æ“ï¼Œå®ƒè§¦å‘äº†å¤§çº¦50ä¸ªå…´è¶£ç‚¹ï¼Œå³åˆ†å‰²ä»»åŠ¡çš„ç§å­ã€‚è™½ç„¶è¿™å¯¹äºè·å¾—ç›´æ¥ç»“æœéå¸¸å¥½ï¼Œä½†å¦‚æœèƒ½å¤Ÿè¿›è¡Œè°ƒæ•´ä¼šæ›´æ£’ã€‚
- en: Finally, the mapping is somewhat simple at this stage; it would largely benefit
    from occlusion culling and point selection for a specific pixel of interest.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæ­¤é˜¶æ®µçš„æ˜ å°„ç›¸å¯¹ç®€å•ï¼›å®ƒå°†å¤§å¤§å—ç›Šäºé®æŒ¡å‰”é™¤å’Œç‰¹å®šåƒç´ çš„ç‚¹é€‰æ‹©ã€‚
- en: 5.4\. Perspectives
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4. è§†è§’
- en: The Segment Anything Model marks just a single step in the larger landscape
    of 3D point cloud segmentation. However, as it stands and is given to you, our
    implementation should work pretty well for any application where you can have
    some kind of distinctive initial features for SAM. As you can see below, it also
    works for top-down aerial point clouds.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anythingæ¨¡å‹åªæ˜¯3Dç‚¹äº‘åˆ†å‰²æ›´å¤§é¢†åŸŸä¸­çš„ä¸€æ­¥ã€‚ç„¶è€Œï¼Œå¦‚ä»Šçš„å®ç°åº”è¯¥èƒ½å¤Ÿå¾ˆå¥½åœ°é€‚ç”¨äºä»»ä½•å…·æœ‰æŸç§ç‹¬ç‰¹åˆå§‹ç‰¹å¾çš„SAMåº”ç”¨ç¨‹åºã€‚æ­£å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå®ƒä¹Ÿé€‚ç”¨äºä¿¯è§†çš„èˆªç©ºç‚¹äº‘ã€‚
- en: '![](../Images/b0a11f9854a98146441b7b7e2e605a7d.png)![](../Images/1e97b6d8f49191db58ba5df7d5e05ccf.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b0a11f9854a98146441b7b7e2e605a7d.png)![](../Images/1e97b6d8f49191db58ba5df7d5e05ccf.png)'
- en: The results of Segment Anything 3D for aerial point clouds.Â© [F. Poux](https://learngeodata.eu/)
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything 3Dåœ¨èˆªç©ºç‚¹äº‘ä¸­çš„ç»“æœã€‚Â© [F. Poux](https://learngeodata.eu/)
- en: Extending to indoor scenarios, you can see that you also will get some pretty
    decent and interesting results. This is even useful for changing the light bulb
    of the light fixtures in the hall, automatically by a robot, of course (How many
    robots does it take to change a light bulb?)!
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰©å±•åˆ°å®¤å†…åœºæ™¯ï¼Œä½ ä¼šå‘ç°ä¹Ÿèƒ½å¾—åˆ°ä¸€äº›ç›¸å½“ä¸é”™å’Œæœ‰è¶£çš„ç»“æœã€‚è¿™ç”šè‡³å¯¹è‡ªåŠ¨æ›´æ¢å¤§å…ç¯å…·çš„ç¯æ³¡æ˜¯æœ‰ç”¨çš„ï¼Œå½“ç„¶æ˜¯ç”±æœºå™¨äººè‡ªåŠ¨å®Œæˆçš„ï¼ˆæ›´æ¢ä¸€ä¸ªç¯æ³¡éœ€è¦å¤šå°‘æœºå™¨äººï¼Ÿï¼‰ï¼
- en: '![](../Images/878f5e7b1124d9b8c4d9ca60ce76d4fd.png)![](../Images/8b550accb9d81579d0dbe3c232e953cf.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/878f5e7b1124d9b8c4d9ca60ce76d4fd.png)![](../Images/8b550accb9d81579d0dbe3c232e953cf.png)'
- en: The results of Segment Anything 3D for another indoor scenario.Â© [F. Poux](https://learngeodata.eu/)
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything 3Dåœ¨å¦ä¸€ä¸ªå®¤å†…åœºæ™¯ä¸­çš„ç»“æœã€‚Â© [F. Poux](https://learngeodata.eu/)
- en: Therefore, aside from generalization, one first perspective is to unlock a way
    to generate panoramas and fuse the prediction of the different points of view.
    Of course, another one would be to expand to custom prompts and, finally, address
    the challenge of improving point-to-pixel accuracy in 2D-3D mapping.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œé™¤äº†æ³›åŒ–å¤–ï¼Œç¬¬ä¸€ä¸ªè§†è§’æ˜¯è§£é”ç”Ÿæˆå…¨æ™¯å›¾å’Œèåˆä¸åŒè§†è§’é¢„æµ‹çš„æ–¹æ³•ã€‚å½“ç„¶ï¼Œå¦ä¸€ä¸ªè§†è§’æ˜¯æ‰©å±•åˆ°è‡ªå®šä¹‰æç¤ºï¼Œæœ€ç»ˆè§£å†³åœ¨2D-3Dæ˜ å°„ä¸­æé«˜ç‚¹åˆ°åƒç´ ç²¾åº¦çš„æŒ‘æˆ˜ã€‚
- en: Conclusion
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: If you are part of the 13.37% of 3D creators that went ahead and actually made
    the code work, then massive kudos to you!
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æ˜¯13.37%ä¸­å®é™…ä½¿ä»£ç æ­£å¸¸å·¥ä½œçš„3Dåˆ›ä½œè€…ä¸­çš„ä¸€å‘˜ï¼Œé‚£ä¹ˆå¯¹ä½ è¡¨ç¤ºç”±è¡·çš„èµèµï¼
- en: '![](../Images/640db9a177c74e1f31baa1f69ea491f6.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/640db9a177c74e1f31baa1f69ea491f6.png)'
- en: The workflow that we covered in this article. Â© [F. Poux](https://learngeodata.eu/)
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨è¿™ç¯‡æ–‡ç« ä¸­è¦†ç›–çš„å·¥ä½œæµç¨‹ã€‚ Â© [F. Poux](https://learngeodata.eu/)
- en: This is a tremendous achievement, and you now have a very powerful asset for
    attacking semantic extraction tasks for 3D Scene Understanding. With the Segment
    Anything Model, you can now encapsulate innovation in many products, transforming
    how we perceive and interpret 3D point clouds.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªå·¨å¤§çš„æˆå°±ï¼Œä½ ç°åœ¨æ‹¥æœ‰äº†ä¸€ä¸ªéå¸¸å¼ºå¤§çš„å·¥å…·æ¥å¤„ç†3Dåœºæ™¯ç†è§£çš„è¯­ä¹‰æå–ä»»åŠ¡ã€‚é€šè¿‡Segment Anythingæ¨¡å‹ï¼Œä½ å¯ä»¥åœ¨è®¸å¤šäº§å“ä¸­å°è£…åˆ›æ–°ï¼Œæ”¹å˜æˆ‘ä»¬æ„ŸçŸ¥å’Œè§£è¯»3Dç‚¹äº‘çš„æ–¹å¼ã€‚
- en: Our exploration should have painted a comprehensive, usable picture of this
    groundbreaking model from its inception to its implications. You may now explore
    the variants and extend their pertinency based on the limitations that were spotted
    in the previous parts.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„æ¢ç´¢åº”è¯¥ä¸ºè¿™ä¸€å¼€åˆ›æ€§æ¨¡å‹ä»èµ·æ­¥åˆ°å…¶å½±å“æç»˜äº†ä¸€ä¸ªå…¨é¢ã€å®ç”¨çš„å›¾æ™¯ã€‚ä½ ç°åœ¨å¯ä»¥æ¢ç´¢è¿™äº›å˜ä½“ï¼Œå¹¶æ ¹æ®ä¹‹å‰éƒ¨åˆ†å‘ç°çš„é™åˆ¶æ‰©å±•å…¶ç›¸å…³æ€§ã€‚
- en: 'ğŸ¦Š **Florent**: *I am looking forward to your future projects that make use
    of it!*'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¦Š **Florent**ï¼š*æˆ‘æœŸå¾…ä½ æœªæ¥çš„é¡¹ç›®èƒ½åŠ ä»¥åˆ©ç”¨ï¼*
- en: 'ğŸ¤  **Ville**: *Code on!*'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤  **Ville**ï¼š*ç»§ç»­ç¼–ç ï¼*
- en: References
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao,
    T., Whitehead, S., Berg, A.C., Lo, W.Y. and DollÃ¡r, P., 2023\. Segment anything.
    [*arXiv preprint arXiv:2304.02643*](https://arxiv.org/pdf/2304.02643).
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kirillov, A.ï¼ŒMintun, E.ï¼ŒRavi, N.ï¼ŒMao, H.ï¼ŒRolland, C.ï¼ŒGustafson, L.ï¼ŒXiao, T.ï¼ŒWhitehead,
    S.ï¼ŒBerg, A.C.ï¼ŒLo, W.Y. å’Œ DollÃ¡r, P.ï¼Œ2023\. åˆ†å‰²ä»»ä½•ä¸œè¥¿ã€‚[*arXiv é¢„å°æœ¬ arXiv:2304.02643*](https://arxiv.org/pdf/2304.02643)ã€‚
- en: '**Poux, Florent**, Mattes, C., Selman, Z. and Kobbelt, L., 2022\. Automatic
    region-growing system for the segmentation of large point clouds. *Automation
    in Construction*, *138*, p.104250\. [Elsevier Link](https://www.sciencedirect.com/science/article/pii/S0926580522001236?casa_token=xnUGwDXoM5gAAAAA%3A1mJlkrTyNZGnbmJnn-9p2qehNHReZvXLX3uJEuXVa6Y5chGjmM-vVAJhezR8wqeKf8XdeR6eng)'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Poux, Florent**ï¼ŒMattes, C.ï¼ŒSelman, Z. å’Œ Kobbelt, L.ï¼Œ2022\. ç”¨äºå¤§è§„æ¨¡ç‚¹äº‘åˆ†å‰²çš„è‡ªåŠ¨åŒºåŸŸç”Ÿé•¿ç³»ç»Ÿã€‚*å»ºç­‘è‡ªåŠ¨åŒ–*ï¼Œ*138*ï¼Œç¬¬
    104250 é¡µã€‚ [Elsevier é“¾æ¥](https://www.sciencedirect.com/science/article/pii/S0926580522001236?casa_token=xnUGwDXoM5gAAAAA%3A1mJlkrTyNZGnbmJnn-9p2qehNHReZvXLX3uJEuXVa6Y5chGjmM-vVAJhezR8wqeKf8XdeR6eng)'
- en: '**Lehtola, Ville**, Kaartinen, H., NÃ¼chter, A., Kaijaluoto, R., Kukko, A.,
    Litkey, P., Honkavaara, E., Rosnell, T., Vaaja, M.T., Virtanen, J.P. and Kurkela,
    M., 2017\. Comparison of the selected state-of-the-art 3D indoor scanning and
    point cloud generation methods. *Remote sensing*, *9*(8), p.796\. [MDPI Link](https://www.mdpi.com/2072-4292/9/8/796/pdf)'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Lehtola, Ville**ï¼ŒKaartinen, H.ï¼ŒNÃ¼chter, A.ï¼ŒKaijaluoto, R.ï¼ŒKukko, A.ï¼ŒLitkey,
    P.ï¼ŒHonkavaara, E.ï¼ŒRosnell, T.ï¼ŒVaaja, M.T.ï¼ŒVirtanen, J.P. å’Œ Kurkela, M.ï¼Œ2017\.
    å¯¹é€‰å®šçš„å…ˆè¿› 3D å®¤å†…æ‰«æå’Œç‚¹äº‘ç”Ÿæˆæ–¹æ³•çš„æ¯”è¾ƒã€‚*é¥æ„Ÿ*ï¼Œ*9*(8)ï¼Œç¬¬ 796 é¡µã€‚ [MDPI é“¾æ¥](https://www.mdpi.com/2072-4292/9/8/796/pdf)'
- en: '![](../Images/f85f3cdf9e16e572c3a0759b83ebfe22.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f85f3cdf9e16e572c3a0759b83ebfe22.png)'
- en: ğŸ”·Other Resources
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ğŸ”·å…¶ä»–èµ„æº
- en: 'ğŸ‡ Get Access to the Data here: [3D Datasets](https://drive.google.com/drive/folders/1RPM69xjPpdBqdwZKC-chRBxkmhDiND9v?usp=sharing)'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğŸ‡ åœ¨è¿™é‡Œè·å–æ•°æ®ï¼š[3D æ•°æ®é›†](https://drive.google.com/drive/folders/1RPM69xjPpdBqdwZKC-chRBxkmhDiND9v?usp=sharing)
- en: 'ğŸ‘¨â€ğŸ« 3D Online Data Science Courses: [3D Academy](https://learngeodata.eu/)'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğŸ‘¨â€ğŸ« 3D åœ¨çº¿æ•°æ®ç§‘å­¦è¯¾ç¨‹ï¼š[3D å­¦é™¢](https://learngeodata.eu/)
- en: 'ğŸ“– Subscribe for early access to 3D Tutorials: [3D AI Automation](https://medium.com/@florentpoux/subscribe)'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğŸ“– è®¢é˜…ä»¥è·å¾— 3D æ•™ç¨‹çš„æ—©æœŸè®¿é—®æƒé™ï¼š[3D AI è‡ªåŠ¨åŒ–](https://medium.com/@florentpoux/subscribe)
- en: 'ğŸ§‘â€ğŸ“Get a Masterâ€™s Degree: [ITC Utwente](https://www.itc.nl/education/?_ga=2.57523758.507481808.1702288503-2061064516.1700473140)'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğŸ§‘â€ğŸ“è·å¾—ç¡•å£«å­¦ä½ï¼š[ITC Utwente](https://www.itc.nl/education/?_ga=2.57523758.507481808.1702288503-2061064516.1700473140)
- en: ğŸ“Authorâ€™s Recommendation
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ğŸ“ä½œè€…æ¨è
- en: 'To build full Indoor Semantic Extraction Scenarios, you can combine this approach
    with the one explained in the â€œ*3D Point Cloud Shape Detection for Indoor Modelling*â€
    article:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æ„å»ºå®Œæ•´çš„å®¤å†…è¯­ä¹‰æå–åœºæ™¯ï¼Œä½ å¯ä»¥å°†è¿™ç§æ–¹æ³•ä¸â€œ*3D ç‚¹äº‘å½¢çŠ¶æ£€æµ‹ç”¨äºå®¤å†…å»ºæ¨¡*â€æ–‡ç« ä¸­è§£é‡Šçš„æ–¹æ³•ç»“åˆèµ·æ¥ï¼š
- en: '[](/3d-point-cloud-shape-detection-for-indoor-modelling-70e36e5f2511?source=post_page-----80c06be99a18--------------------------------)
    [## 3D Point Cloud Shape Detection for Indoor Modelling'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/3d-point-cloud-shape-detection-for-indoor-modelling-70e36e5f2511?source=post_page-----80c06be99a18--------------------------------)
    [## 3D ç‚¹äº‘å½¢çŠ¶æ£€æµ‹ç”¨äºå®¤å†…å»ºæ¨¡'
- en: A 10-step Python Guide to Automate 3D Shape Detection, Segmentation, Clustering,
    and Voxelization for Space Occupancyâ€¦
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¸€ä»½ 10 æ­¥ Python æŒ‡å—ï¼Œç”¨äºè‡ªåŠ¨åŒ– 3D å½¢çŠ¶æ£€æµ‹ã€åˆ†å‰²ã€èšç±»å’Œä½“ç´ åŒ–â€¦
- en: towardsdatascience.com](/3d-point-cloud-shape-detection-for-indoor-modelling-70e36e5f2511?source=post_page-----80c06be99a18--------------------------------)
    [](https://learngeodata.eu/3d-newsletter/?source=post_page-----80c06be99a18--------------------------------)
    [## 3D Innovator Newsletter
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/3d-point-cloud-shape-detection-for-indoor-modelling-70e36e5f2511?source=post_page-----80c06be99a18--------------------------------)
    [](https://learngeodata.eu/3d-newsletter/?source=post_page-----80c06be99a18--------------------------------)
    [## 3D åˆ›æ–°è€…é€šè®¯
- en: Weekly practical content, insights, code and resources to master 3D Data Science.
    I write about Point Clouds, AIâ€¦
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¯å‘¨æä¾›å®ç”¨å†…å®¹ã€è§è§£ã€ä»£ç å’Œèµ„æºï¼Œä»¥æŒæ¡ 3D æ•°æ®ç§‘å­¦ã€‚æˆ‘å†™å…³äºç‚¹äº‘ã€äººå·¥æ™ºèƒ½â€¦
- en: learngeodata.eu](https://learngeodata.eu/3d-newsletter/?source=post_page-----80c06be99a18--------------------------------)
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: learngeodata.eu](https://learngeodata.eu/3d-newsletter/?source=post_page-----80c06be99a18--------------------------------)
