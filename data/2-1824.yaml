- en: 'Segment Anything 3D for Point Clouds: Complete Guide (SAM 3D)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Segment Anything 3D for Point Clouds: 完整指南 (SAM 3D)'
- en: 原文：[https://towardsdatascience.com/segment-anything-3d-for-point-clouds-complete-guide-sam-3d-80c06be99a18](https://towardsdatascience.com/segment-anything-3d-for-point-clouds-complete-guide-sam-3d-80c06be99a18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/segment-anything-3d-for-point-clouds-complete-guide-sam-3d-80c06be99a18](https://towardsdatascience.com/segment-anything-3d-for-point-clouds-complete-guide-sam-3d-80c06be99a18)
- en: 3D Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3D Python
- en: 'How to build a semantic segmentation application for 3D point clouds leveraging
    SAM and Python. Bonus: code for projections and relationships between 3D points
    and 2D pixels.'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何利用 SAM 和 Python 构建 3D 点云的语义分割应用程序。附加内容：投影和 3D 点与 2D 像素之间关系的代码。
- en: '[](https://medium.com/@florentpoux?source=post_page-----80c06be99a18--------------------------------)[![Florent
    Poux, Ph.D.](../Images/74df1e559b2edefba71ffd0d1294a251.png)](https://medium.com/@florentpoux?source=post_page-----80c06be99a18--------------------------------)[](https://towardsdatascience.com/?source=post_page-----80c06be99a18--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----80c06be99a18--------------------------------)
    [Florent Poux, Ph.D.](https://medium.com/@florentpoux?source=post_page-----80c06be99a18--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@florentpoux?source=post_page-----80c06be99a18--------------------------------)[![Florent
    Poux, Ph.D.](../Images/74df1e559b2edefba71ffd0d1294a251.png)](https://medium.com/@florentpoux?source=post_page-----80c06be99a18--------------------------------)[](https://towardsdatascience.com/?source=post_page-----80c06be99a18--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----80c06be99a18--------------------------------)
    [Florent Poux, Ph.D.](https://medium.com/@florentpoux?source=post_page-----80c06be99a18--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----80c06be99a18--------------------------------)
    ·27 min read·Dec 13, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----80c06be99a18--------------------------------)
    ·阅读时间 27 分钟·2023年12月13日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/cbaf694227e7e6b933f01064d27a96d6.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cbaf694227e7e6b933f01064d27a96d6.png)'
- en: The Segment Anything Model for 3D Environments. We will detect objects in indoor
    spaces using 3D point cloud datasets. Credit goes to [Mimatelier](https://linktr.ee/mimatelier),
    the talented illustrator who created this image.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 针对 3D 环境的 Segment Anything 模型。我们将使用 3D 点云数据集检测室内空间的物体。特别鸣谢 [Mimatelier](https://linktr.ee/mimatelier)，这位才华横溢的插画师创作了这张图片。
- en: Technological leaps are just plain crazy, especially looking at Artificial Intelligence
    (AI) applied to 3D challenges. Having the ability to leverage the latest cutting-edge
    research for advanced 3D applications is very empowering. Especially when looking
    at bringing human-level reasoning capabilities to a computer, there is a clear
    need to extract a formalized meaning from the 3D entities that we observe.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 科技的飞跃真是疯狂，特别是看到人工智能（AI）应用于 3D 挑战时。能够利用最新的前沿研究来进行高级 3D 应用是非常令人振奋的。尤其是在将人类级别的推理能力带入计算机时，明确从我们观察到的
    3D 实体中提取出正式化的意义显得尤为重要。
- en: In this tutorial, we are here to make sure that we can bind amazing AI advancements
    with 3D applications that make use of 3D Point Clouds. — *🐲* **Florent & Ville**
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在本教程中，我们旨在将令人惊叹的人工智能进展与利用 3D 点云的 3D 应用程序结合起来。— *🐲* **Florent & Ville**
- en: This is no easy feat, but once mastered, the fusion of 3D point clouds and deep
    learning gives birth to new dimensions of understanding and interpreting our visual
    world.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一件容易的事，但一旦掌握，3D 点云与深度学习的融合将开辟对我们视觉世界的新维度的理解和解释。
- en: '![](../Images/46849f11c579adff452243fd483b5eef.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/46849f11c579adff452243fd483b5eef.png)'
- en: Artificial Intelligence ft. 3D point clouds. © [F. Poux](https://learngeodata.eu/)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能与 3D 点云。© [F. Poux](https://learngeodata.eu/)
- en: Among these advancements, the Segment Anything Model is a recent beacon of innovation,
    especially for full automation without supervision.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些进展中，Segment Anything 模型是最近的创新灯塔，尤其是在无需监督的全自动化方面。
- en: '![](../Images/1b49832a749ed53fb71db522ff96117b.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b49832a749ed53fb71db522ff96117b.png)'
- en: The Segment Anything Model Architecture that we use for 3D Data. It comprises
    an image encoder, image embeddings, and some pre-processing operations to finally
    pass into the decoder and prompt encoder, giving the results as masks. © [F. Poux](https://learngeodata.eu/)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用于 3D 数据的 Segment Anything 模型架构。它包括一个图像编码器、图像嵌入以及一些预处理操作，最后传递给解码器和提示编码器，输出结果为掩模。©
    [F. Poux](https://learngeodata.eu/)
- en: In this ultimate guide, we embark on a pragmatic voyage to explore this cutting-edge
    model, from its inception to its practical segmentation applications. But what
    is the objective here?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这份**终极指南**中，我们将开始一个实际的旅程，从模型的诞生到实际分割应用。但这里的目标是什么？
- en: The Mission 🥷
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务 🥷
- en: 'Okay, it’s time for the mission brief! You are now a multi-class member of
    your country’s special forces, and you must find some dangerous materials hidden
    inside a specific building without ever being detected (here: the ITC building).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，任务简报时间到了！你现在是你国家特种部队的多类成员，你必须在不被发现的情况下找到隐藏在特定建筑物中的危险材料（这里是ITC大楼）。
- en: With your superb internet hacking skills, you manage to find the 3D scans for
    that part of the building you are interested in. You now need to find a way to
    define the path for your dangerous material recovery team quickly. After that,
    the team can proceed unnoticed to recover the materials, and you have saved the
    day!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 利用你出色的互联网黑客技能，你成功找到了你感兴趣的建筑部分的3D扫描。你现在需要快速找到定义你危险材料回收队的路径的方法。之后，队伍可以在不被察觉的情况下进行材料回收，你也就成功拯救了世界！
- en: After careful research and using your various skills, you develop a 3D data
    processing workflow that involves setting up a 3D Python code environment to process
    the 3D point cloud by using the Segment Anything Model to highlight the composition
    of the scene, as shown below.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 经过仔细研究并利用你的各种技能，你开发了一个3D数据处理工作流程，其中包括设置3D Python代码环境，通过Segment Anything模型处理3D点云，以突出场景的组成，如下所示。
- en: '![](../Images/640db9a177c74e1f31baa1f69ea491f6.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/640db9a177c74e1f31baa1f69ea491f6.png)'
- en: The workflow for Segment Anything 3D. We have five main steps (3D Project Setup,
    Segment Anything Model, 3D Point Cloud Projections, Unsupervised Segmentation,
    and Qualitative Analysis) further refined in the substeps, as highlighted. © [F.
    Poux](https://learngeodata.eu/)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything 3D的工作流程。我们有五个主要步骤（3D项目设置、Segment Anything模型、3D点云投影、无监督分割和定性分析），并在下图中进一步细化。©
    [F. Poux](https://learngeodata.eu/)
- en: This will allow you to produce a 3D semantic map that will permit pinpointing
    the location of the materials within ninety minutes before the team is on-site.
    Are you ready?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这将允许你生成一个3D语义地图，在队伍到达现场之前的九十分钟内，能够准确定位材料的位置。你准备好了吗？
- en: '🎵**Note to Readers***: This hands-on guide is part of a* [***UTWENTE***](https://www.itc.nl/)
    *joint work with co-authors* ***F. Poux*** *and* ***V. Lehtola****. We acknowledge
    the financial contribution from the digital twins* [*@ITC*](http://twitter.com/ITC)
    *-project granted by the ITC faculty of the University of Twente.*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '🎵**读者注意**: 这个实践指南是* [***UTWENTE***](https://www.itc.nl/) *与合著者***F. Poux***
    *和***V. Lehtola*** *的联合工作的一部分。我们感谢来自数字双胞胎* [*@ITC*](http://twitter.com/ITC) *项目的资助，该项目由特温特大学ITC学院授予。*'
- en: 1\. 3D Project Setup
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 3D项目设置
- en: Before we dive into the marvels of the Segment Anything Model, it is crucial
    to establish a robust foundation. Setting up the appropriate environment ensures
    smooth sailing throughout our journey, allowing seamless experimentation and exploration.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨Segment Anything模型的奇迹之前，建立一个稳固的基础至关重要。设置适当的环境可以确保我们在整个过程中顺利进行，从而实现无缝的实验和探索。
- en: At this stage, we want to ensure that our coding environment is correctly set
    with robust libraries. Ready?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们要确保我们的编码环境设置正确，并且具有强大的库。准备好了吗？
- en: '🤠 **Ville**: *This is before the action starts. Please reserve an hour or two
    separately for this if you are doing it from scratch and, e.g., might need to
    update CUDA drivers. You’ll be downloading gigabytes of stuff.*'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '🤠 **Ville**: *这是在行动开始之前。如果你是从零开始做这件事，比如可能需要更新CUDA驱动程序，请单独预留一个或两个小时。你将下载几GB的内容。*'
- en: '![](../Images/dce37818ba43403acd021f11131b0685.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dce37818ba43403acd021f11131b0685.png)'
- en: The 3D Project Setup. We first set up the environment, on which we attach base
    libraries, deep learning libraries, and the IDE Setup. © [F. Poux](https://learngeodata.eu/)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 3D项目设置。我们首先设置环境，然后附加基础库、深度学习库和IDE设置。© [F. Poux](https://learngeodata.eu/)
- en: 1.1\. 3D Code Environment setup
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1\. 3D 代码环境设置
- en: 'It is time to get our hands in the dirt! We aim to use the Segment Anything
    Model to Semantically Segment a 3D Point Cloud. And that is no easy feat! So,
    of course, the first reflex is to check out the segment anything dependencies:
    [Access SAM Github](https://github.com/facebookresearch/segment-anything).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候动手了！我们的目标是使用 Segment Anything Model 对 3D 点云进行语义分割。这绝非易事！所以，首先的反应是查看 Segment
    Anything 的依赖项：[访问 SAM Github](https://github.com/facebookresearch/segment-anything)。
- en: 'From there, we check out the necessary pre-requisites of the package:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从那里，我们检查包的必要前提条件：
- en: '![](../Images/9dd556baea6fef87e6c02751f663835a.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9dd556baea6fef87e6c02751f663835a.png)'
- en: The dependencies highlighted in Segment Anything.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything 中突出的依赖项。
- en: '🦊 **Florent**: *Whenever you are dealing with deep learning libraries or deep
    learning new research code, it is essential to check out the dependencies and
    installation recommendations. Indeed, this will strongly influence the follow-up
    of your experiments and the time needed for replication.*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 🦊 **Florent**：*无论何时处理深度学习库或深度学习的新研究代码，都必须检查依赖项和安装建议。这将极大地影响实验的后续进展和复制所需的时间。*
- en: 'As you can see, we need to use the following library version:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们需要使用以下库版本：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now that this is out, we will generate a virtual environment to ensure smooth
    sailing! If you want a detailed view of the process, I recommend you jump aboard
    the following guide:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这一点搞定后，我们将生成一个虚拟环境以确保顺利进行！如果你想详细了解这一过程，建议你查看以下指南：
- en: '[](/3d-python-workflows-for-lidar-point-clouds-100ff40e4ff0?source=post_page-----80c06be99a18--------------------------------)
    [## 3D Python Workflows for LiDAR City Models: A Step-by-Step Guide'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/3d-python-workflows-for-lidar-point-clouds-100ff40e4ff0?source=post_page-----80c06be99a18--------------------------------)
    [## 3D Python Workflows for LiDAR City Models: A Step-by-Step Guide'
- en: The Ultimate Guide to unlocking a streamlined workflow for 3D City Modelling
    Applications. The tutorial covers Python…
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解锁 3D 城市建模应用程序的精简工作流的终极指南。教程涵盖了 Python…
- en: towardsdatascience.com](/3d-python-workflows-for-lidar-point-clouds-100ff40e4ff0?source=post_page-----80c06be99a18--------------------------------)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/3d-python-workflows-for-lidar-point-clouds-100ff40e4ff0?source=post_page-----80c06be99a18--------------------------------)
- en: But, not to keep you high and dry, here is another strategy for a quick and
    lightweight setup using [Miniconda](https://docs.conda.io/projects/miniconda/en/latest/miniconda-install.html).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了不让你感到无助，下面是另一种快速轻量级的设置策略，使用 [Miniconda](https://docs.conda.io/projects/miniconda/en/latest/miniconda-install.html)。
- en: '💡 **Note**: *Miniconda is a free minimal installer for conda. It is a “miniature”
    version of Anaconda that includes only a minimal amount of dependencies. These
    are the conda package manager, a Python version, the packages they both depend
    on and other valuable packages like pip and zlib. This allows us only to install
    what we need in a lightweight manner.*'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 💡 **注意**：*Miniconda 是一个免费的 Conda 最小安装程序。它是 Anaconda 的“微型”版本，仅包含最少的依赖项。这些包括 Conda
    包管理器、一个 Python 版本、它们所依赖的包以及其他有价值的包如 pip 和 zlib。这使得我们可以以轻量的方式只安装我们需要的东西。*
- en: '🤠**Ville***: The cool stuff about virtual environments is that you can export
    it and run your code as-is on powerful Linux computing machines and superclusters!
    This is very handy for training networks!*'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 🤠**Ville***：虚拟环境的酷炫之处在于，你可以将其导出并在强大的 Linux 计算机和超级集群上直接运行你的代码！这对于训练网络非常有用！*
- en: After downloading a version of Miniconda from [here](https://docs.conda.io/projects/miniconda/en/latest/miniconda-other-installer-links.html)
    for your OS (I recommend you choose a Python 3.9 or 3.10 version to ensure proper
    compatibility with packages), you can install it following the various steps of
    the installation process.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在从 [这里](https://docs.conda.io/projects/miniconda/en/latest/miniconda-other-installer-links.html)
    下载适合你操作系统的 Miniconda 版本后（建议选择 Python 3.9 或 3.10 版本以确保与包的兼容性），你可以按照安装过程中的各种步骤进行安装。
- en: '![](../Images/0f94957e88eef5951bad35f2d5ceaa61.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f94957e88eef5951bad35f2d5ceaa61.png)'
- en: The miniconda installer window. © [F. Poux](https://learngeodata.eu/)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: miniconda 安装程序窗口。© [F. Poux](https://learngeodata.eu/)
- en: 'And that is it! You now have secured the most uncomplicated Python installation
    with the lightweight miniconda that will make isolating a controlled virtual environment
    super easy. Before moving on to the following steps, we launch miniconda with
    its command line access:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！你现在已经完成了最简单的 Python 安装，使用轻量级的 miniconda 使得隔离受控的虚拟环境变得非常容易。在继续下一步之前，我们启动
    miniconda 及其命令行访问：
- en: '![](../Images/df2bb60a544049eca1139b2b848577df.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df2bb60a544049eca1139b2b848577df.png)'
- en: In Windows, just searching “miniconda” should yield this
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Windows 中，只需搜索“miniconda”即可找到
- en: Once in the Anaconda Prompt, we follow a simple four-step process to be up and
    running, as shown below.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦进入 Anaconda 提示符，我们按照下面显示的简单四步过程进行操作。
- en: '![](../Images/1cdedcd8fdd91ee19cf54b3a8cb8ca12.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1cdedcd8fdd91ee19cf54b3a8cb8ca12.png)'
- en: The Workflow to set up a Python environment for 3D Segment Anything Model. ©
    [F. Poux](https://learngeodata.eu/)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 Python 环境以使用 3D Segment Anything Model 的工作流程。© [F. Poux](https://learngeodata.eu/)
- en: 'To create a new environment, we write the line: `conda create -n GEOSAM python=3.10`'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要创建新环境，我们输入：`conda create -n GEOSAM python=3.10`
- en: 'To switch to the newly created environment, we write: `conda activate GEOSAM`'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 切换到新创建的环境，我们输入：`conda activate GEOSAM`
- en: 'To check the Python version, `python --version`, and the installed packages:
    `conda list`. This should yield Python 3.10 and the list of base libraries, respectively.'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要检查 Python 版本，输入 `python --version`，检查已安装的软件包：`conda list`。这应分别显示 Python 3.10
    和基础库列表。
- en: 'To install pip in the new environment, we write: `conda install pip`'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在新环境中安装 pip，我们输入：`conda install pip`
- en: And that is it! We are now ready to move on installing the necessary libraries
    for playing with SAM.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些！我们现在准备安装必要的库以进行 SAM 的操作。
- en: '[](https://learngeodata.eu/3d-newsletter/?source=post_page-----80c06be99a18--------------------------------)
    [## 3D Innovator Newsletter'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 3D 创新者通讯'
- en: Weekly practical content, insights, code and resources to master 3D Data Science.
    I write about Point Clouds, AI…
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每周提供实用内容、见解、代码和资源，掌握 3D 数据科学。我写关于点云、人工智能……
- en: learngeodata.eu](https://learngeodata.eu/3d-newsletter/?source=post_page-----80c06be99a18--------------------------------)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[learngeodata.eu](https://learngeodata.eu/3d-newsletter/?source=post_page-----80c06be99a18--------------------------------)'
- en: 1.2\. Base library
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2\. 基础库
- en: '![](../Images/06b0547b42b6f690713e39f65c151892.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06b0547b42b6f690713e39f65c151892.png)'
- en: The base libraries used in this tutorial (Numpy, Matplotlib, Laspy). © [F. Poux](https://learngeodata.eu/)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程中使用的基础库（Numpy、Matplotlib、Laspy）。© [F. Poux](https://learngeodata.eu/)
- en: 'We now install our base libraries for using SAM: `NumPy`, `LasPy`, `OpenCV`,
    and `Matplotlib`. `NumPy` may be the most recommended library for numerical computations,
    `OpenCV` is used for computer vision tasks, `Laspy` deals with processing LIDAR
    data, and `Matplotlib` is a plotting and data visualization library in Python.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在安装用于 SAM 的基础库：`NumPy`、`LasPy`、`OpenCV` 和 `Matplotlib`。`NumPy` 可能是最推荐的数值计算库，`OpenCV`
    用于计算机视觉任务，`Laspy` 处理 LIDAR 数据，而 `Matplotlib` 是一个绘图和数据可视化库。
- en: '🦊 **Florent**: *These libraries are the base and robust cornerstones of any
    3D project. If you want to deepen their understanding, I suggest you dive into*
    [*this tutorial*](https://medium.com/towards-data-science/how-to-automate-lidar-point-cloud-processing-with-python-a027454a536c)
    *that explores its dark depths* 🪸.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 🦊 **Florent**：*这些库是任何 3D 项目的基础和坚实的基石。如果你想深入了解它们，我建议你去* [*这个教程*](https://medium.com/towards-data-science/how-to-automate-lidar-point-cloud-processing-with-python-a027454a536c)
    *，它探讨了其深奥的内容* 🪸。
- en: 'To install these libraries, we can use pip in one single line:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装这些库，我们可以用一行代码通过 pip：
- en: '[PRE1]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This is great; it's time for the deep learning libraries setup!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 很好；是时候设置深度学习库了！
- en: 1.2 Deep Learning Libraries
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 深度学习库
- en: '![](../Images/50677cd193ffb1d9e37ce1fb9b27b84e.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/50677cd193ffb1d9e37ce1fb9b27b84e.png)'
- en: The deep learning libraries.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习库。
- en: 'We will now look into installing deep-learning libraries. And, of course, the
    first one that we explore is my favorite one so far: Pytorch. Since its launch
    in 2017, Pytorch has improved its flexibility and hackability as a priority and
    performance as a close second. Therefore, today, using Pytorch for Deep Learning
    applications is excellent if you want (1) high-performance execution, (2) Pythonic
    internals, and (3) good abstractions for valuable tasks.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将着手安装深度学习库。当然，我们首先探索的是我迄今为止最喜欢的：Pytorch。自 2017 年推出以来，Pytorch 优先考虑其灵活性和可黑客性，其次是性能。因此，今天，使用
    Pytorch 进行深度学习应用是绝佳的选择，如果你需要 (1) 高性能执行，(2) Pythonic 内部实现，以及 (3) 有价值任务的良好抽象。
- en: '🦊 **Florent**: *Since 2017, Hardware accelerators (such as GPUs) have become
    ~15x faster in computing tasks. You can only guess what is to come in the next
    few years. Therefore, it is essential to be on the lookout for flexible libraries
    that can move quickly, even on refactoring “internals” to languages such as C++,
    like Pytorch does.*'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '🦊 **Florent**: *自2017年以来，硬件加速器（如GPU）在计算任务中的速度提高了约15倍。你只能猜测接下来几年会发生什么。因此，必须关注灵活的库，它们可以快速适应，甚至对“内部”进行重构，如Pytorch所做的那样。*'
- en: '🤠**Ville***: SAM authors recommend using a GPU with 8GB memory. However, we
    give some tips on how to do the tutorial with less memory. Use them if you get
    ‘MemoryError’ or ‘Out-of-bounds memory access’ or ‘Illegal memory access’ messages.
    I got it working with 6GB.*'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '🤠**Ville***: SAM作者推荐使用8GB内存的GPU。然而，我们提供了一些如何在内存较少的情况下进行教程的技巧。如果你收到‘MemoryError’或‘Out-of-bounds
    memory access’或‘Illegal memory access’消息，请使用这些技巧。我使用6GB内存成功运行了它。*'
- en: To install a relevant distribution of Pytorch without headaches figuring out
    how to install CUDA (which is not so straightforward), they made a straightforward
    web app that generates the code to copy and paste into your command line. For
    this, you can jump on this [Pytorch Getting Started page](https://pytorch.org/get-started/locally/)
    and select the most relevant way to install your distribution, as shown below.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了无忧地安装Pytorch的相关发行版，而不必为如何安装CUDA（这并不简单）而烦恼，他们制作了一个简单的网页应用程序，生成代码以复制并粘贴到你的命令行中。为此，你可以访问这个
    [Pytorch入门页面](https://pytorch.org/get-started/locally/) 并选择最相关的安装方式，如下所示。
- en: '![](../Images/a8cda5c93dd1245d31400555bddb4216.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8cda5c93dd1245d31400555bddb4216.png)'
- en: How to install Pytorch for your OS and configuration.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如何根据你的操作系统和配置安装Pytorch。
- en: '💡 **Note**: *We want to leverage our GPU. Therefore, it is essential to note
    that we want an installation with CUDA. But this is possible only if you have
    a Nvidia GPU at the time of writing. If not, you may want to use the CPU or switch
    to a Cloud computing service such as Google Colab.*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '💡 **注意**: *我们希望充分利用我们的GPU。因此，重要的是要注意我们希望进行CUDA安装。但这只有在你写这篇文章时拥有Nvidia GPU时才可能。如果没有，你可能需要使用CPU或切换到像Google
    Colab这样的云计算服务。*'
- en: 'Therefore, our code line is the following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的代码行如下：
- en: '[PRE2]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This line will trigger the retrieval and installation of the necessary elements
    for Pytorch to function coherently.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这行代码将触发必要元素的检索和安装，以便Pytorch可以顺利运行。
- en: '![](../Images/7bb2a5f949df042d424767ce6448a145.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7bb2a5f949df042d424767ce6448a145.png)'
- en: The installation of Pytorch.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch的安装。
- en: 'The second deep-learning library that we want to use is Segment Anything. While
    Pytorch is being installed, we can download and install “software” that will make
    it easier for us to manage versions and access online libraries. This is **Git**
    and is accessible here: [Git Website](https://git-scm.com/download/win).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要使用的第二个深度学习库是Segment Anything。在Pytorch安装的同时，我们可以下载并安装“软件”，这将使我们更容易管理版本和访问在线库。这就是**Git**，可以在这里访问：[Git官网](https://git-scm.com/download/win)。
- en: 'You can download and install git, and once the installation is finished, Pytorch
    should also be nicely installed in your environment. Therefore, to install segment-anything,
    we can write the line below:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以下载并安装git，一旦安装完成，Pytorch也应该在你的环境中顺利安装。因此，为了安装segment-anything，我们可以写如下代码：
- en: '[PRE3]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This will again take some time until you get such a message below.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这将再次需要一些时间，直到你看到如下消息。
- en: '![](../Images/04e42602deac265942e73e53219ee11f.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04e42602deac265942e73e53219ee11f.png)'
- en: The CLI results of installing Pytorch.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 安装Pytorch的CLI结果。
- en: At this stage, we have the base libraries as well as the deep learning libraries
    installed. Before using them, let us install an IDE to use everything smoothly.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一阶段，我们已经安装了基础库以及深度学习库。在使用它们之前，让我们安装一个IDE，以便一切顺利运行。
- en: 4\. Setting up an IDE
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 设置IDE
- en: '![](../Images/53e45018ba59d7fa3a7dc0a1622bff21.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/53e45018ba59d7fa3a7dc0a1622bff21.png)'
- en: The Jupyter lab IDE after installation.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter lab IDE安装后的界面。
- en: 'The last step of our setup is to install an IDE. We are still in the command
    line interface within the environment, and we type: `pip install jupyterlab`,
    which will install jupyterlab on our environment.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设置的最后一步是安装一个IDE。我们仍在环境中的命令行界面下，输入：`pip install jupyterlab`，这将会在我们的环境中安装jupyterlab。
- en: To use it in a defined local folder, we can first create a parent directory
    for our project (let us call it `SAM`), which will hold both a `CODE` folder and
    a `DATA` Folder. Once this is done, in the console, we change our pointer to the
    created directory by writing. `cd C://COURSES/POUX/SAM`.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 要在指定的本地文件夹中使用它，我们可以首先为我们的项目创建一个父目录（我们称之为 `SAM`），该目录将包含一个 `CODE` 文件夹和一个 `DATA`
    文件夹。完成后，在控制台中，我们通过写入 `cd C://COURSES/POUX/SAM` 来切换到创建的目录。
- en: 'We launch jupyterlab from this location by typing in the console: `jupyter
    lab`, which will open a new localhost page in your web browser (Chrome, Firefox,
    or Safari).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过在控制台输入 `jupyter lab` 来从这个位置启动 jupyterlab，这将会在你的网页浏览器（Chrome、Firefox 或 Safari）中打开一个新的本地主机页面。
- en: 'In Jupyter, you can create a notebook (.ipynb) and write in the first cell
    of the import statements to use all the installed packages:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Jupyter 中，你可以创建一个笔记本 (.ipynb)，并在第一个单元格中写入导入语句，以使用所有已安装的软件包：
- en: '[PRE4]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Alright! We are all set up. Before getting on the other steps of coding our
    model, now is just the time to retrieve our 3D Dataset.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 好的！我们已经准备好了一切。在开始编码模型的其他步骤之前，现在正是提取我们3D数据集的好时机。
- en: 5\. 3D Dataset Curation
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 3D 数据集整理
- en: In previous tutorials, we illustrated point cloud processing and meshing over
    several 3D datasets, some of which use aerial LiDAR from the AHN4 LiDAR Campaign.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的教程中，我们展示了多个3D数据集中的点云处理和网格化，其中一些使用了AHN4 LiDAR活动的航空LiDAR。
- en: '[](/3d-deep-learning-python-tutorial-pointnet-data-preparation-90398f880c9f?source=post_page-----80c06be99a18--------------------------------)
    [## 3D Deep Learning Python Tutorial: PointNet Data Preparation'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/3d-deep-learning-python-tutorial-pointnet-data-preparation-90398f880c9f?source=post_page-----80c06be99a18--------------------------------)
    [## 3D 深度学习Python教程：PointNet 数据准备'
- en: The Ultimate Python Guide to structure large LiDAR point cloud for training
    a 3D Deep Learning Semantic Segmentation…
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 终极Python指南，用于结构化大型LiDAR点云，以训练3D深度学习语义分割……
- en: towardsdatascience.com](/3d-deep-learning-python-tutorial-pointnet-data-preparation-90398f880c9f?source=post_page-----80c06be99a18--------------------------------)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/3d-deep-learning-python-tutorial-pointnet-data-preparation-90398f880c9f?source=post_page-----80c06be99a18--------------------------------)
- en: 'This time, we will use a dataset gathered using a Terrestrial Laser Scanner:
    the ITC of the University of Twente''s new 2023 building, as shown below. It consists
    of an indoor green area with nice tricky foliage to assess after running the segmentation.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们将使用一个使用地面激光扫描仪收集的数据集：2023年乌特勒支大学ITC的新建筑，如下所示。它包含一个室内绿色区域，内有精美的复杂树叶，分割后可以进行评估。
- en: '![](../Images/29970d4d0fb77f18812b960e2601f444.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29970d4d0fb77f18812b960e2601f444.png)'
- en: The 3D Point Cloud of the ITC UTwente new building, with its indoor “jungle”.
    © [F. Poux](https://learngeodata.eu/)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ITC UTwente新建筑的3D点云，包括其室内“丛林”。© [F. Poux](https://learngeodata.eu/)
- en: 'You can download the data from the Drive Folder here: [Guide Datasets (Google
    Drive)](https://drive.google.com/drive/folders/1pIaP-vJAWh8cFtk9zQ3poxcR5DuEIkmt?usp=sharing),
    and put it in your folder that holds datasets (in my case, “DATA”).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从这里下载数据： [指南数据集（Google Drive）](https://drive.google.com/drive/folders/1pIaP-vJAWh8cFtk9zQ3poxcR5DuEIkmt?usp=sharing)，并将其放入你保存数据集的文件夹中（在我的例子中是“DATA”）。
- en: At this process stage, we have a nice warm coding setup, with all the necessary
    libraries in a lightweight, isolated GEOSAM conda environment.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中阶段，我们有一个良好的编码设置，所有必要的库都在一个轻量级、隔离的 GEOSAM conda 环境中。
- en: '🦊 **Florent**: Great job so far! If you are eager to run some tests to check
    that Pytorch is working as it should, i.e., CUDA is recognized, you can write
    these lines of code:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '🦊 **Florent**: 到目前为止做得很好！如果你急于运行一些测试以检查Pytorch是否正常工作，即CUDA是否被识别，你可以写下以下代码行：'
- en: '[PRE5]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/06b2a254f99419bdf2ab27549acbfb2e.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06b2a254f99419bdf2ab27549acbfb2e.png)'
- en: My configuration from the print results above.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 上述打印结果的配置。
- en: It is now time to segment stuff!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候对数据进行分割了！
- en: '[](https://learngeodata.eu/3d-newsletter/?source=post_page-----80c06be99a18--------------------------------)
    [## 3D Innovator Newsletter'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://learngeodata.eu/3d-newsletter/?source=post_page-----80c06be99a18--------------------------------)
    [## 3D 创新者通讯'
- en: Weekly practical content, insights, code and resources to master 3D Data Science.
    I write about Point Clouds, AI…
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每周实用内容、见解、代码和资源，以掌握3D数据科学。我写关于点云、人工智能等内容……
- en: learngeodata.eu](https://learngeodata.eu/3d-newsletter/?source=post_page-----80c06be99a18--------------------------------)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[learngeodata.eu](https://learngeodata.eu/3d-newsletter/?source=post_page-----80c06be99a18--------------------------------)'
- en: 2\. Setting up the Segment Anything Model
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 设置 Segment Anything 模型
- en: At the heart of our little adventure lies the Segment Anything Model, a powerful
    creation with excellent potential for 3D point cloud semantic segmentation. With
    its innovative architecture and training process, this model is the perfect candidate
    to be tested on indoor applications. Let us first play around with its core concepts.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的小冒险的核心是 Segment Anything Model，这是一种强大的创作，具有极好的 3D 点云语义分割潜力。凭借其创新的架构和训练过程，该模型是室内应用测试的理想候选者。让我们先来了解一下其核心概念。
- en: 2.1\. Segment Anything Basics
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1\. Segment Anything 基础知识
- en: MetaAI has delved into the fascinating realm of Natural Language Processing
    (NLP) and computer vision with their Segment Anything Model, which enables **zero-shot**
    and **few-shot learning** on novel datasets and tasks using foundation models.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: MetaAI 已深入探讨自然语言处理（NLP）和计算机视觉的迷人领域，其 Segment Anything Model 使 **零-shot** 和 **少-shot
    学习** 在新数据集和任务上成为可能，使用基础模型。
- en: '🦊 **Florent**: *Okay, there are a lot of swear words, I admit. For clarity
    concerns, here is my tentative to summarize each complex terminology. Zero-shot
    learning refers to the ability to recognize something without having seen it (seen
    it zero times). Somewhat similarly, few-shot learning uses a limited number of
    labeled examples for each new class, and the goal is to make predictions for new
    classes based on just these few examples of labeled data.*'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '🦊 **Florent**: *好吧，我承认有很多脏话。为了清晰起见，这里是我对每个复杂术语的总结尝试。零-shot 学习指的是在未见过某物的情况下识别它（零次见过）。类似地，少-shot
    学习使用有限数量的标记示例来处理每个新类别，目标是根据这些少量的标记数据进行预测。*'
- en: '🤠**Ville***: Also, a so-called* ***foundation model*** *is a model that is
    trained on a lot and a lot of data at a scale. it is so large that it can be adapted
    to various tasks from different scenarios.*'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '🤠**Ville***: 此外，所谓的* ***基础模型*** *是一个在大量数据上训练的模型。它如此庞大，可以适应来自不同场景的各种任务。*'
- en: 'Let’s break this down for you:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为你拆解一下：
- en: Overall, the SAM “AI” algorithm can significantly reduce the human effort required
    for image segmentation. To do so, you provide the model with foreground/background
    points, a rough box or mask, some text, or any other input that indicates what
    you want to segment in an image. The Meta AI team has trained the Segment Anything
    Model to generate a proper segmentation mask. This mask is the model’s output
    and should be a suitable mask to delineate one of the things that the prompt might
    refer to. For instance, if you indicate a point on the roof of the house, the
    output should correctly identify whether you meant the roof or the house.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，SAM “AI” 算法可以显著减少进行图像分割所需的人力。为此，你需要向模型提供前景/背景点、粗略的框或掩码、一些文本或任何其他指示你想要在图像中分割的输入。Meta
    AI 团队已训练 Segment Anything Model 以生成合适的分割掩码。这个掩码是模型的输出，应该是一个适合划定提示可能指向的事物的掩码。例如，如果你在房子屋顶上标出一个点，输出应该正确识别你是指屋顶还是房子。
- en: '![](../Images/e2a0375836c5937774047d2f113b288a.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2a0375836c5937774047d2f113b288a.png)'
- en: How does the Segment Anything Model (SAM) work? Explanation of the segmentation
    prompt to generate valid masks (case of a house). © [F. Poux](https://learngeodata.eu/)
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything Model (SAM) 是如何工作的？解释分割提示以生成有效的掩码（以房屋为例）。© [F. Poux](https://learngeodata.eu/)
- en: This segmentation task can then serve for model pre-training and guiding solutions
    for various downstream segmentation problems.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 该分割任务可以用于模型预训练，并指导解决各种下游分割问题。
- en: On the technical side, what we call an image encoder creates a unique embedding
    (representation) for each image, and a lightweight encoder swiftly transforms
    any query into an embedding vector. These two data sources are merged using a
    (lightweight) mask decoder to predict segmentation masks, as shown below.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术角度来看，我们所称的图像编码器为每张图像创建了一个独特的嵌入（表示），而一个轻量级的编码器迅速将任何查询转换为嵌入向量。这两个数据源通过一个（轻量级的）掩码解码器合并，以预测分割掩码，如下所示。
- en: '![](../Images/84684c89b03497988c14b8439f88afd8.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84684c89b03497988c14b8439f88afd8.png)'
- en: Flowchart of the functioning of the Segment Anything Model. The image goes through
    the image encoder. Then it is embedded, to finally be combined after using a prompt
    followed by a prompt encoder, to generate final masks for our 3D point clouds.
    © [F. Poux](https://learngeodata.eu/)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything Model 的工作流程图。图像经过图像编码器处理。然后它被嵌入，最后在使用提示和提示编码器后合并，以生成我们 3D 点云的最终掩码。©
    [F. Poux](https://learngeodata.eu/)
- en: 'This effective architecture, combined with a massive scale training phase,
    allows the Segment Anything Model to reach four milestones:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这种有效的架构，加上大规模的训练阶段，使Segment Anything Model达到了四个里程碑：
- en: '**Effortless Object Segmentation 🔥**: With SAM, users can effortlessly segment
    objects by simply selecting the points they want to include or exclude from the
    segmentation. You can also use a bounding box as a cue for the model.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**轻松对象分割 🔥**: 使用SAM，用户可以通过简单选择要包括或排除的点来轻松分割对象。你还可以使用边界框作为模型的提示。'
- en: '**Handling Uncertainty 🔥**: SAM is equipped to handle situations with uncertainty
    about the object to be segmented. It can generate multiple valid masks, which
    is crucial for solving real-world segmentation challenges effectively.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理不确定性 🔥**: SAM能够处理对象分割中的不确定情况。它可以生成多个有效的掩码，这对于有效解决实际的分割挑战至关重要。'
- en: '**Automatic Object Detection and Masking 🔥**: SAM makes automatic object detection
    and masking a breeze. It simplifies these tasks, saving you time and effort.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动对象检测与掩膜 🔥**: SAM使得自动对象检测和掩膜变得轻而易举。它简化了这些任务，节省了你的时间和精力。'
- en: '**Real-time Interaction 🔥**: Thanks to precomputed image embeddings, SAM can
    instantly provide a segmentation mask for any prompt. This means you can have
    real-time interactions with the model.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时交互 🔥**: 得益于预计算的图像嵌入，SAM可以即时提供任何提示的分割掩膜。这意味着你可以与模型进行实时交互。'
- en: Now that this is out of the way, are you ready to use it?
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 既然这些都解决了，你准备好使用它了吗？
- en: 2.1\. SAM Parameters
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1\. SAM 参数
- en: 'The SAM model can be loaded with three different encoders: ViT-B, ViT-L, and
    ViT-H. ViT-H gives better results than ViT-B but has only marginal gains over
    ViT-L.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: SAM模型可以通过三种不同的编码器加载：ViT-B、ViT-L和ViT-H。ViT-H的结果优于ViT-B，但与ViT-L相比仅有微小的提升。
- en: '[PRE6]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**🤠 Ville**: *To help with the choice, I tested ViT-B on NVIDIA GeForce GTX
    1650, 6 Gb VRAM with Win11.*'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**🤠 Ville**: *为了帮助选择，我在NVIDIA GeForce GTX 1650、6 Gb VRAM和Win11上测试了ViT-B。*'
- en: These three encoders have different parameter counts that give a bit more freedom
    to tune an application. ViT-B (the smallest) has 91 Million parameters, ViT-L
    has 308 Million parameters, and ViT-H (the biggest) has 636 Million parameters.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种编码器具有不同的参数数量，这为应用程序的调优提供了更多自由。ViT-B（最小）有9100万个参数，ViT-L有3.08亿个参数，而ViT-H（最大）有6.36亿个参数。
- en: 'This difference in size also influences the speed of inference, so this should
    help you decide the encoder for your specific use case. Following this guide,
    we will get with the heavy artillery: The ViT-H, with a Model Checkpoint that
    you can download from [Github](https://github.com/facebookresearch/segment-anything#model-checkpoints)
    (2.4 Gb) and place in your current parent folder, for example.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这种大小差异也会影响推断速度，因此这应有助于你为你的具体用例选择编码器。按照本指南，我们将使用重型武器：ViT-H，带有一个可以从[Github](https://github.com/facebookresearch/segment-anything#model-checkpoints)（2.4
    Gb）下载的模型检查点，并将其放置在你的当前父文件夹中，例如。
- en: 'This is where we can define two variables to make your code a bit more flexible
    afterward:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以定义两个变量，以使你的代码在之后稍微更灵活一些：
- en: '[PRE7]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'From there, we can initialize our SAM model with the following two lines of
    code:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，我们可以用以下两行代码初始化我们的SAM模型：
- en: '[PRE8]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: And we are all set up! Maybe one last step, trying to see how it performs on
    a random image that you have on your desktop?
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 一切都准备好了！也许最后一步，试试看它在你桌面上的随机图像上的表现如何？
- en: 2.2 Performances on 2D images
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 在2D图像上的性能
- en: 'Let us test if all works as expected on a random image. We are interested in
    geospatial applications, so I go to [Google Earth](https://earth.google.com/)
    and zoom in on a spot of interest:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们测试一下在随机图像上的效果是否如预期。我们对地理空间应用感兴趣，所以我去[Google Earth](https://earth.google.com/)并放大一个感兴趣的点：
- en: '![](../Images/a541e6bccb901f9898489865f2107af8.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a541e6bccb901f9898489865f2107af8.png)'
- en: Selection of an imagery dataset from Biscarosse. © [F. Poux](https://learngeodata.eu/)
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 选择来自Biscarosse的图像数据集。© [F. Poux](https://learngeodata.eu/)
- en: '🦊 **Florent**: *This spot is biased, right? Hopefully, this gives you a bit
    of French holiday vibes, which you are proud to take followed by a marvelous year
    full of exciting projects*!'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '🦊 **Florent**: *这个点有偏见，对吧？希望这能给你一些法国假期的感觉，你很自豪地经历了这段美妙的岁月，充满了激动人心的项目！*'
- en: 'From there, I take a screenshot of a zone of interest:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 从那里，我会截取一个感兴趣区域的屏幕截图：
- en: '![](../Images/3a887690665dfcb7368dafd20b5fb139.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a887690665dfcb7368dafd20b5fb139.png)'
- en: The image dataset of a zone of Biscarosse plage. © [F. Poux](https://learngeodata.eu/)
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 一个来自Biscarosse plage区域的图像数据集。© [F. Poux](https://learngeodata.eu/)
- en: 'and I load the image into memory with openCV:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我用openCV将图像加载到内存中：
- en: '[PRE9]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '🦚 **Note**: *As you can see, by default, OpenCV loads an image by switching
    to Blue, Green, and Red channels (BGR) that we order as RGB with the second line
    and store in the* `image_rgb` *variable.*'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '🦚 **注意**: *如你所见，默认情况下，OpenCV通过切换到蓝色、绿色和红色通道（BGR）来加载图像，我们通过第二行将其排序为RGB，并存储在*
    `image_rgb` *变量中。*'
- en: 'Now, it is time for us to apply SAM on the image with two lines of code:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以用两行代码在图像上应用SAM：
- en: '[PRE10]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In around 6 seconds, this returns us a list filled with dictionaries, each
    representing a mask for a specific object automatically extracted, accompanied
    by its scores and metadata. For a detailed view, the result is a list of dictionaries
    where each `dict` holds the following information:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 大约6秒钟后，这将返回一个填充了字典的列表，每个字典代表一个特定对象的自动提取掩膜，并附有其分数和元数据。详细查看时，结果是一个字典列表，每个`dict`包含以下信息：
- en: '`segmentation` : this brings out the mask with `(W, H)` shape (and `bool` type),
    where `W` (width) and `H` (height) target the original image dimensions;'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segmentation` : 这会生成形状为`(W, H)`（和`bool`类型）的掩膜，其中`W`（宽度）和`H`（高度）针对原始图像尺寸；'
- en: '`area` : this is the area of the mask expressed in pixels'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`area` : 这是以像素为单位的掩膜面积'
- en: '`bbox` : this is the boundary box detection in `xywh` format'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bbox` : 这是`xywh`格式的边界框检测'
- en: '`predicted_iou` : the model''s prediction IoU metric for the quality of the
    mask.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predicted_iou` : 模型对掩膜质量的预测IoU指标。'
- en: '`point_coords` : This is a list of the sampled input points that were used
    to generate the mask'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`point_coords` : 这是用于生成掩膜的采样输入点的列表'
- en: '`stability_score` : The stability score is an additional measure of the mask
    quality. Check out the paper for more details 😉'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stability_score` : 稳定性得分是掩膜质量的附加衡量指标。查看论文获取更多细节 😉'
- en: '`crop_box` : this is a list of the crop_boxe coordinates used to generate this
    mask in `xywh` format (it may differ from the Bounding-Box)'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`crop_box` : 这是用于生成该掩膜的crop_box坐标列表，格式为`xywh`（可能与边界框不同）'
- en: 'Now that you have a better idea about what we are dealing with, to check out
    the results, we can plot the masks on top of the image with the following function:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你对我们正在处理的内容有了更好的了解，要查看结果，我们可以用以下函数在图像上绘制掩膜：
- en: '[PRE11]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '🦊 **Florent**: *I admit, this is a bit blunt. But what happens in this function
    is that I will sort out the masks by their area to plot them with a random color
    on top of the image with a transparency parameter*.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '🦊 **Florent**: *我承认，这有点直接。但在这个函数中，我会按掩膜面积对它们进行排序，以随机颜色和透明度参数在图像上绘制它们。*'
- en: '🤠**Ville***: Memory errors can ruin French holiday vibes! Remember the Google
    Colab option too! If rebooting does not solve the issue and allocated memory is
    too high, the following piece of code clears the GPU memory of extra allocations.
    Use it to address memory problems.*'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '🤠**Ville***: 内存错误可能会毁掉法国假期的氛围！记得使用Google Colab选项！如果重启不能解决问题且分配内存过高，以下代码可以清除GPU内存中的额外分配。用它来解决内存问题。*'
- en: '[PRE12]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '*If the GPU memory is not freed enough, try rebooting your (Windows) computer.
    Also, try using the following line if memory problems persist:* `mask_generator
    = SamAutomaticMaskGenerator(sam, points_per_batch=16)`'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果GPU内存未充分释放，请尝试重启你的（Windows）计算机。如果内存问题仍然存在，可以尝试使用以下行：* `mask_generator =
    SamAutomaticMaskGenerator(sam, points_per_batch=16)`'
- en: 'Now, to plot and export the image, we write the following:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，要绘制和导出图像，我们写下以下内容：
- en: '[PRE13]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Which results in:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致：
- en: '![](../Images/77cc7cf3fcc62d5fdab3af87a5895117.png)![](../Images/8913306a437571893ac027bede9a81d3.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77cc7cf3fcc62d5fdab3af87a5895117.png)![](../Images/8913306a437571893ac027bede9a81d3.png)'
- en: Before and after the Segment Anything Model. © [F. Poux](https://learngeodata.eu/)
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在Segment Anything Model之前和之后。© [F. Poux](https://learngeodata.eu/)
- en: So, already at this stage, we have exciting results, and SAM is working really
    nicely! For example, you can see that almost all roofs are part of segments and
    that the three pools (2 blue and one green) are also part of segments. Therefore,
    this could well be a starting point for complete automatic detection
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这个阶段，我们已经有了令人兴奋的结果，SAM工作得非常好！例如，你可以看到几乎所有的屋顶都是分段的一部分，三个泳池（两个蓝色和一个绿色）也是分段的一部分。因此，这可能是完全自动检测的起点
- en: '🦊 **Florent**: *You may run into Memory Errors depending on your computer setup
    while plotting the masks. In this case, loading a lighter SAM model should solve
    your problem.* 😉'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '🦊 **Florent**: *根据你的计算机设置，绘制掩膜时可能会遇到内存错误。在这种情况下，加载一个更轻的SAM模型应该能解决你的问题。* 😉'
- en: Now that we have a working SAM setup, let us apply all this hard-earned know-how
    to 3D point clouds.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经有了一个有效的SAM设置，让我们将这些辛苦获得的知识应用于3D点云。
- en: 3\. 3D Point Cloud to Image Projections
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 3D 点云到图像投影
- en: To make sense of the complex 3D world, we delve into the art of point cloud
    projection. Through techniques like ortho and spherical projections, we bridge
    the gap between dimensions, enabling us to visualize the intricacies of the point
    cloud in a 2D realm, which is the input needed for SAM. Point cloud mapping adds
    a layer of understanding to this projection process.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解复杂的3D世界，我们*深入探讨*点云投影的艺术。通过像正射和球面投影这样的技术，我们弥合了维度之间的差距，使我们能够在2D领域中可视化点云的复杂性，这正是SAM所需的输入。点云映射为这一投影过程增添了一层理解。
- en: '3.1 Ortho Projection: Flattening Dimensions, Expanding Insights'
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 正射投影：平展维度，扩展洞察
- en: Let us look at the transformative technique of Ortho Projection. This method
    serves as an excellent bridge between the multi-dimensional complexities of 3D
    point clouds and the comprehensible world of 2D images. Through Ortho Projection,
    we “flatten” dimensions but also unveil a direct way to manage segmentation with
    SAM.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看正射投影的变革性技术。此方法作为3D点云的多维复杂性与2D图像的可理解世界之间的绝佳桥梁。通过正射投影，我们“平展”维度，同时揭示了使用SAM进行分割的直接方法。
- en: The idea is basically to generate a top-down view plane and generate an image
    that is not constrained by a single perspective. You could see ortho-projection
    as a process of pushing visible points from the point cloud (highest ones) onto
    the plane that holds the empty image to fill all the necessary pixels just above
    those points. You can see the difference from a perspective view, as illustrated
    below.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法基本上是生成一个俯视视图平面，并生成一个不受单一视角限制的图像。你可以将正射投影视为将点云中的可见点（最高点）推送到持有空图像的平面上，以填充所有必要的像素。你可以看到与透视视图的不同之处，如下所示。
- en: '![](../Images/efcf4ea1b6062d00cb933ab9b645ff17.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/efcf4ea1b6062d00cb933ab9b645ff17.png)'
- en: Explanation of the difference between Orthographic View, and Perspective View
    for 3D Projections. The Perspective View is linked to a single point of view that
    skews dimensions. © [F. Poux](https://learngeodata.eu/)
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 解释正射视图和透视视图在3D投影中的区别。透视视图与一个单独的视点相关，这会扭曲维度。© [F. Poux](https://learngeodata.eu/)
- en: 'To work out this process, we can define a 3D-to-2D projection function that
    would take the points of a point cloud alongside its color and a wanted resolution
    to compute the ortho-projection and return an orthoimage from the point cloud.
    This would translate into the following:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成这个过程，我们可以定义一个3D到2D的投影函数，它将点云的点及其颜色和所需分辨率作为输入，计算正射投影并从点云中返回正射图像。这将转化为以下内容：
- en: '[PRE14]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Great, now it is time for a test, do you agree? To do so, let us load a point
    cloud dataset, transform it to a numpy array, apply the function, and export an
    image of this point cloud:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，现在是时候进行测试了，你同意吗？为此，让我们加载一个点云数据集，将其转换为numpy数组，应用这个函数，并导出这个点云的图像：
- en: '[PRE15]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This permits us to obtain the following:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这允许我们获得以下内容：
- en: '![](../Images/65931fd258f06d536ead6c908d1c2c17.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/65931fd258f06d536ead6c908d1c2c17.png)'
- en: The workflow to go from 3D Point Clouds to Orthoimages. We first project the
    point cloud following an ortho-projection mode, then we make sure to include a
    point-to-pixel mapping for back-projection. © [F. Poux](https://learngeodata.eu/)
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 从3D点云到正射图像的工作流程。我们首先按照正射投影模式对点云进行投影，然后确保包括点到像素的映射用于反投影。© [F. Poux](https://learngeodata.eu/)
- en: Let us move on to spherical projections
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续球面投影
- en: 3.2 3D Point Cloud Spherical Projection
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 3D 点云球面投影
- en: 'Our journey takes an intriguing turn as we encounter Spherical Projection.
    This technique offers a unique perspective, enabling us to visualize the data
    by “simulating” a virtual scan station. To do just this, we proceed in four steps
    by: (1) Considering the 3D Point Cloud, (2) Projecting these points onto a sphere,
    (3) defining a geometry that will retrieve the pixels, (4) “flattening” this geometry
    to produce an image.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的旅程在遇到球面投影时变得非常有趣。这种技术提供了独特的视角，使我们能够通过“模拟”虚拟扫描站来可视化数据。为此，我们通过以下四个步骤进行： (1)
    考虑3D点云， (2) 将这些点投影到球体上， (3) 定义一个几何体来检索像素， (4) “平展”这个几何体以生成图像。
- en: '**🤠 Ville**: *Spherical projection is like being inside a 3D point cloud and
    taking a 360-degree photo of what you see*.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '**🤠 Ville**: *球面投影就像是在3D点云内部，拍摄你看到的360度照片*。'
- en: '![](../Images/503ca251cad894ed496dd046f9732807.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/503ca251cad894ed496dd046f9732807.png)'
- en: The 3D Point Cloud Spherical Projection Workflow. We take a 3D Point Cloud,
    we create a 3D Projection Sphere, we define the mapping plane, and we produce
    an equirectangular projection. © [F. Poux](https://learngeodata.eu/)
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 点云球面投影工作流。我们获取 3D 点云，创建一个 3D 投影球体，定义映射平面，并生成等距圆柱投影。© [F. Poux](https://learngeodata.eu/)
- en: To achieve the 3D Projection onto a sphere, we want to obtain points as illustrated
    below.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现 3D 投影到球面，我们希望获得如下所示的点。
- en: '![](../Images/4d82f7a78bb0e5f5eb5b32ed2c379f96.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d82f7a78bb0e5f5eb5b32ed2c379f96.png)'
- en: How to project points of a 3D point clouds onto a sphere. © [F. Poux](https://learngeodata.eu/)
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如何将 3D 点云的点投影到球面。© [F. Poux](https://learngeodata.eu/)
- en: Then, we will unroll following our geometry (cylinder) to obtain an equirectangular
    image, as shown below.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将根据几何形状（圆柱体）展开，以获得等距圆柱图像，如下所示。
- en: '![](../Images/1f0023fe4cecfde6980edba0c234293d.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f0023fe4cecfde6980edba0c234293d.png)'
- en: How to go from a sphere to an equirectangular image. We have a projection mechanism
    that permits us to “unroll” the pixels onto a cylinder. © [F. Poux](https://learngeodata.eu/)
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如何从球面到等距圆柱图像。我们有一个投影机制，允许我们将像素“展开”到圆柱体上。© [F. Poux](https://learngeodata.eu/)
- en: 'Let me now detail the function that allows just this:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我详细介绍允许实现这一点的功能：
- en: '[PRE16]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '🌱 **Growing**: *It is essential to digest this function. It looks like it is
    pretty straightforward, but there are nice tricks at several stages. For example,
    what do you think about the 3D point cloud to spherical coordinates step? What
    does the mapping do? What is the point of using the mapping as a conditional statement
    while assigning points to pixels?*'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 🌱 **Growing**：*消化这个功能是至关重要的。它看起来很简单，但在多个阶段有一些巧妙的技巧。例如，你对 3D 点云到球面坐标步骤有什么看法？映射的作用是什么？在将点分配给像素时，使用映射作为条件语句的意义何在？*
- en: 'Now, to use this handy function, let us load and prepare the ITC indoor point
    cloud first:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了使用这个方便的功能，我们首先加载并准备 ITC 室内点云：
- en: '[PRE17]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Once prepared, we can define the necessary parameters for projection. These
    are the center of projection (basically the position from which we want a virtual
    scan station) and the resolution of the final image (expressed in pixels, as the
    smallest side of the image).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好后，我们可以定义投影所需的参数。这些参数包括投影中心（基本上是我们希望虚拟扫描站的位置）和最终图像的分辨率（以像素表示，即图像的最小边）。
- en: '[PRE18]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Finally, we can call the new function, plot and export the results as an image
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以调用新的函数，绘制并将结果导出为图像。
- en: '[PRE19]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'All this process results in the following image:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些过程会产生以下图像：
- en: '![](../Images/94689a7ca448ddb2e27d84adc9e14cb1.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94689a7ca448ddb2e27d84adc9e14cb1.png)'
- en: The 3D point cloud transformed as an equirectangular image from the projection.
    © [F. Poux](https://learngeodata.eu/)
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 点云通过投影转化为等距圆柱图像。© [F. Poux](https://learngeodata.eu/)
- en: How do you like that? You can play around with the various parameters, such
    as the resolution or the center of projection, to ensure that you get a nice balance
    between “no data” pixels and relevant panorama.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 你觉得怎么样？你可以调整各种参数，如分辨率或投影中心，以确保在“无数据”像素和相关全景之间取得良好的平衡。
- en: '🦊 **Florent**: *You just unlocked a powerful new skill with 3D Point Cloud
    to Equirectangular image creation. Indeed, it allows you to generate virtual scans
    basically wherever you believe it makes sense and then unlock the possibility
    to use image processing and deep learning techniques for images. You can also
    extend the provided function to other mapping projections to add arrows to your
    quiver.*'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 🦊 **Florent**：*你刚刚解锁了一项强大的新技能——将 3D 点云转换为等距圆柱图像。确实，它允许你在你认为有意义的地方生成虚拟扫描，并开启使用图像处理和深度学习技术处理图像的可能性。你还可以将提供的功能扩展到其他映射投影，以增加你的工具库。*
- en: '🤠**Ville**: *I can almost see the lecture halls and my office, Dutch working
    vibes!*'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 🤠**Ville**：*我几乎可以看到讲座大厅和我的办公室，荷兰的工作氛围！*
- en: 3.3 3D Point-to-Pixel Mapping
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 3D 点到像素的映射
- en: 'We transform raw point data into structured raster representations, making
    sense of the seemingly scattered information. Point Cloud Mapping is the compass
    that guides us for 3D point cloud processing through 2D projection. The good news:
    we already took care of this mapping.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将原始点数据转换为结构化的栅格表示，理清看似散乱的信息。点云映射是我们在 2D 投影中处理 3D 点云的指南针。好消息是：我们已经处理了这个映射。
- en: Indeed, if you take a close look at the function `generate_spherical_image`,
    you can see that we return the `mapping` variable and capture it in another variable
    for downward processes. This ensures that we can have a coherent 3D Point-to-Pixel
    Mapping.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，如果你仔细查看函数`generate_spherical_image`，你会发现我们返回了`mapping`变量并将其捕获到另一个变量中以便后续处理。这确保了我们可以拥有一致的3D点到像素的映射。
- en: 4\. Unsupervised Segmentation with SAM
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 使用SAM的无监督分割
- en: Unsupervised segmentation enters the scene in the form of the Segment Anything
    Model. We are, in the case of non-labeled outputs, through SAM’s segmentation
    architecture, which falls within clustering applications. This is opposed to most
    supervised learning approaches that will provide labeled outputs, as illustrated
    below.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督分割以Segment Anything模型的形式出现。在非标记输出的情况下，我们通过SAM的分割架构，这属于聚类应用。这与大多数监督学习方法提供标记输出的方式相对，如下所示。
- en: '![](../Images/8dc14d35b848c2681114cb297c086c0d.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8dc14d35b848c2681114cb297c086c0d.png)'
- en: The distinction between unsupervised learning and supervised learning. In unsupervised
    learning, we aim at defining groups of data “points” that share some similarity,
    whereas in supervised learning, we aim at approaching the supervision needs (usually
    by feeding labeled data). © [F. Poux](https://learngeodata.eu/)
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习和监督学习之间的区别。在无监督学习中，我们旨在定义一些相似的数据“点”组，而在监督学习中，我们旨在满足监督需求（通常通过提供标记数据）。© [F.
    Poux](https://learngeodata.eu/)
- en: Therefore, the transfer of pixel predictions, coupled with seamless point cloud
    export, showcases the potential for revolutionizing applications like object detection
    and scene understanding.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，像素预测的转移，加上无缝的点云导出，展示了革新物体检测和场景理解等应用的潜力。
- en: 4.1\. SAM Segmentation
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1\. SAM分割
- en: 'To execute the program, we can re-execute the code snippets that we used to
    test our SAM functionalities on 2D images, which are:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行程序，我们可以重新执行我们用于测试SAM功能在2D图像上的代码片段，这些代码片段是：
- en: '[PRE20]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: and later, we can plot the results on the image itself
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以在图像上绘制结果
- en: '[PRE21]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Which results in:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这将得到：
- en: '![](../Images/33b40e186df07c0e404488d127f5a6ec.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/33b40e186df07c0e404488d127f5a6ec.png)'
- en: Results of the Segment Anything Model on the 3D point cloud projection. © [F.
    Poux](https://learngeodata.eu/)
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 3D点云投影上的Segment Anything模型的结果。© [F. Poux](https://learngeodata.eu/)
- en: This already looks like we are delineating significant parts of the image. Let
    us move forward with this.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来我们正在勾画出图像中的重要部分。让我们继续前进。
- en: 4.2\. Point Prediction Transfer
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2\. 点预测转移
- en: 'Let us color the point cloud with this image. We thus define a coloring function:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用这张图像为点云上色。因此我们定义一个上色函数：
- en: '[PRE22]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This means that to color our point cloud, we can use the following code line
    that calls our new function:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着要为我们的点云上色，我们可以使用以下代码行调用我们的新函数：
- en: '[PRE23]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This line returns a numpy array that holds the point cloud.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这一行返回一个numpy数组，该数组保存了点云。
- en: It is now time for 3D Point Cloud Export!
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是3D点云导出的时候了！
- en: 4.3\. Point Cloud Export
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3\. 点云导出
- en: 'To export the point cloud, you can use numpy or laspy to extract a .las file
    directly. We will proceed with the second solution:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 要导出点云，你可以使用numpy或laspy直接提取一个.las文件。我们将采用第二种解决方案：
- en: '[PRE24]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'And with this, we can export our modified_point_cloud variable:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们可以导出我们的modified_point_cloud变量：
- en: '[PRE25]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: After this stage, we successfully taken our various 2D images resulting from
    the 3D point cloud projection process. We applied SAM algorithm to it, colorized
    it based on its prediction, and exported a colored point cloud. We can thus move
    to getting some insights about what we are getting.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一阶段，我们成功地获取了各种来自3D点云投影过程的2D图像。我们对其应用了SAM算法，基于其预测对其上色，并导出了一个彩色点云。因此我们可以开始获取一些关于我们所得到的东西的见解。
- en: '🦊 **Florent**: *To analyze the result quickly outside Python, I recommend using
    the CloudCompare Open-Source Software. If you want a clear guide on how to use
    it efficiently, you can read and follow through the article below.*'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 🦊 **Florent**：*为了快速在Python之外分析结果，我建议使用CloudCompare开源软件。如果你想要一个清晰的使用指南，可以阅读下面的文章。*
- en: '[](/3d-deep-learning-python-tutorial-pointnet-data-preparation-90398f880c9f?source=post_page-----80c06be99a18--------------------------------)
    [## 3D Deep Learning Python Tutorial: PointNet Data Preparation'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/3d-deep-learning-python-tutorial-pointnet-data-preparation-90398f880c9f?source=post_page-----80c06be99a18--------------------------------)
    [## 3D深度学习Python教程：PointNet数据准备'
- en: The Ultimate Python Guide to structure large LiDAR point cloud for training
    a 3D Deep Learning Semantic Segmentation…
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 《终极Python指南》用于构建大型LiDAR点云以训练3D深度学习语义分割…
- en: towardsdatascience.com](/3d-deep-learning-python-tutorial-pointnet-data-preparation-90398f880c9f?source=post_page-----80c06be99a18--------------------------------)
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/3d-deep-learning-python-tutorial-pointnet-data-preparation-90398f880c9f?source=post_page-----80c06be99a18--------------------------------)'
- en: 5\. Qualitative Analysis and discussions
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5. 定性分析与讨论
- en: With our journey nearing its zenith, it’s time to focus on qualitative analysis.
    Exceptionally, we will not conduct a quantitative analysis, as we would need proper
    labels for that at this stage.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们的旅程接近巅峰，现在是关注定性分析的时候了。特别地，我们不会进行定量分析，因为在这个阶段我们需要适当的标签。
- en: '🤠 **Ville**: No labels? What you just did was zero-shot learning (Bang!) or
    few-shot learning (Bang! Bang!). We cannot be sure which because we don’t know
    exactly how SAM was trained by Meta. Therefore it is a bit of a black box for
    us, but that’s ok.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '🤠 **Ville**: 没有标签？你刚刚做的是零样本学习（砰！）或少样本学习（砰！砰！）。我们不能确定是哪种，因为我们不知道SAM的训练方式。因此对我们来说它有点像黑箱，但没关系。'
- en: We meticulously examine the raster and point cloud results, drawing insights
    that shed light on SAM’s performance. Also, let us remain grounded by acknowledging
    the model’s limitations while setting our sights on the future.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仔细检查光栅和点云结果，得出的见解揭示了SAM的性能。同时，让我们保持脚踏实地，承认模型的局限性，同时展望未来。
- en: 5.1 Raster results
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 光栅结果
- en: The output of SAM’s efforts with our implementation is eloquently depicted through
    the below raster results. These visuals serve as a canvas on which SAM’s segmentation
    can be quickly assessed, enabling us to comprehend the model’s understanding of
    the scene in a 2D representation.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: SAM在我们实施下的成果通过下述光栅结果得到了生动的展示。这些视觉效果作为画布，快速评估SAM的分割，帮助我们理解模型对场景的2D表示。
- en: '![](../Images/77d9f078a781d656b12f76e839573662.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77d9f078a781d656b12f76e839573662.png)'
- en: Another result of the Segment Anything Model on the 3D point cloud projection.
    © [F. Poux](https://learngeodata.eu/)
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything Model在3D点云投影上的另一个结果。© [F. Poux](https://learngeodata.eu/)
- en: As you can see, even with the uneven point distribution and “black zones”, SAM
    is able to pick up what the main parts of the point cloud are about. Specifically,
    it likely highlights a green on the left, the place where the dangerous materials
    are, and the doors and windows for our extraction team to have the most direct
    route!
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，即使在点分布不均和“黑区”下，SAM仍能识别出点云的主要部分。具体而言，它可能突出了左侧的绿色区域，即危险材料所在的位置，以及为我们提取团队提供最直接路线的门窗！
- en: 5.2\. Point Cloud results
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 点云结果
- en: Yet, it’s in the point cloud results that the true depth of SAM’s abilities
    emerges. As we navigate through the cloud of points, SAM’s segmented predictions
    bring clarity to the classical “mess of points”, showcasing its potential in real-world
    applications.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正是在点云结果中，SAM的真正能力得以显现。当我们在点云中穿梭时，SAM的分割预测为经典的“点云混乱”带来了清晰度，展示了其在实际应用中的潜力。
- en: '![](../Images/aa21106712342d39785489b7db32c5eb.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa21106712342d39785489b7db32c5eb.png)'
- en: The 3D Point Cloud unsupervised segmentation results by using Segment Anything
    3D. We see the great distinction of the major elements that compose the scene.
    © [F. Poux](https://learngeodata.eu/)
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Segment Anything 3D进行的3D点云无监督分割结果。我们看到构成场景的主要元素的明显区别。© [F. Poux](https://learngeodata.eu/)
- en: As we can see, we can have a direct link with the underlying points, and that
    is massively awesome! Think only about what this can unlock for your applications.
    A 100% automated segmentation process that has under five main breakpoints? Not
    bad!
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们可以直接链接到基础点，这真是极其棒！想想这能为你的应用程序解锁什么。一个拥有不到五个主要断点的100%自动化分割过程？不错！
- en: 5.3\. Shortcomings
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3. 局限性
- en: But, our expedition would only be complete with acknowledging the rough patches
    along the way. SAM, while impressive, is not exempt from limitations. By recognizing
    these shortcomings, we pave the way for refinement and growth
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 但，我们的探险只有在承认过程中存在的粗糙点后才算完成。SAM，尽管令人印象深刻，也不例外地存在局限性。通过认识这些不足，我们为改进和成长铺平道路。
- en: The first thing is that all the “unseen” points remain unlabelled (white points
    below). This could prove to be a limitation for complete processing, and if you
    use the basic or the large model you will see more unlabelled points than when
    using the huge model.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 首先是所有“未见”的点仍然保持未标记（下图中的白点）。这可能会成为完整处理的一个限制，如果你使用基本模型或大模型，你会看到比使用巨大模型时更多的未标记点。
- en: '![](../Images/07d064a4ed2ff0080884fc84c4ce052f.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07d064a4ed2ff0080884fc84c4ce052f.png)'
- en: The ratio of unlabelled points vs. labeled points from the first pass with a
    central perspective 360° simulated scan position. © [F. Poux](https://learngeodata.eu/)
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 从中央视角360°模拟扫描位置的第一次扫描中，未标记点与标记点的比例。 © [F. Poux](https://learngeodata.eu/)
- en: Also, at this stage, we used the automatic prompting engine that triggered around
    50 points of interest, the seeds of the segmentation task. While this is great
    for getting a direct result, having the possibility to tune that would be awesome.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在这个阶段，我们使用了自动提示引擎，它触发了大约50个兴趣点，即分割任务的种子。虽然这对于获得直接结果非常好，但如果能够进行调整会更棒。
- en: Finally, the mapping is somewhat simple at this stage; it would largely benefit
    from occlusion culling and point selection for a specific pixel of interest.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，此阶段的映射相对简单；它将大大受益于遮挡剔除和特定像素的点选择。
- en: 5.4\. Perspectives
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4. 视角
- en: The Segment Anything Model marks just a single step in the larger landscape
    of 3D point cloud segmentation. However, as it stands and is given to you, our
    implementation should work pretty well for any application where you can have
    some kind of distinctive initial features for SAM. As you can see below, it also
    works for top-down aerial point clouds.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything模型只是3D点云分割更大领域中的一步。然而，如今的实现应该能够很好地适用于任何具有某种独特初始特征的SAM应用程序。正如下图所示，它也适用于俯视的航空点云。
- en: '![](../Images/b0a11f9854a98146441b7b7e2e605a7d.png)![](../Images/1e97b6d8f49191db58ba5df7d5e05ccf.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b0a11f9854a98146441b7b7e2e605a7d.png)![](../Images/1e97b6d8f49191db58ba5df7d5e05ccf.png)'
- en: The results of Segment Anything 3D for aerial point clouds.© [F. Poux](https://learngeodata.eu/)
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything 3D在航空点云中的结果。© [F. Poux](https://learngeodata.eu/)
- en: Extending to indoor scenarios, you can see that you also will get some pretty
    decent and interesting results. This is even useful for changing the light bulb
    of the light fixtures in the hall, automatically by a robot, of course (How many
    robots does it take to change a light bulb?)!
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展到室内场景，你会发现也能得到一些相当不错和有趣的结果。这甚至对自动更换大厅灯具的灯泡是有用的，当然是由机器人自动完成的（更换一个灯泡需要多少机器人？）！
- en: '![](../Images/878f5e7b1124d9b8c4d9ca60ce76d4fd.png)![](../Images/8b550accb9d81579d0dbe3c232e953cf.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/878f5e7b1124d9b8c4d9ca60ce76d4fd.png)![](../Images/8b550accb9d81579d0dbe3c232e953cf.png)'
- en: The results of Segment Anything 3D for another indoor scenario.© [F. Poux](https://learngeodata.eu/)
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything 3D在另一个室内场景中的结果。© [F. Poux](https://learngeodata.eu/)
- en: Therefore, aside from generalization, one first perspective is to unlock a way
    to generate panoramas and fuse the prediction of the different points of view.
    Of course, another one would be to expand to custom prompts and, finally, address
    the challenge of improving point-to-pixel accuracy in 2D-3D mapping.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，除了泛化外，第一个视角是解锁生成全景图和融合不同视角预测的方法。当然，另一个视角是扩展到自定义提示，最终解决在2D-3D映射中提高点到像素精度的挑战。
- en: Conclusion
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: If you are part of the 13.37% of 3D creators that went ahead and actually made
    the code work, then massive kudos to you!
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是13.37%中实际使代码正常工作的3D创作者中的一员，那么对你表示由衷的赞赏！
- en: '![](../Images/640db9a177c74e1f31baa1f69ea491f6.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/640db9a177c74e1f31baa1f69ea491f6.png)'
- en: The workflow that we covered in this article. © [F. Poux](https://learngeodata.eu/)
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这篇文章中覆盖的工作流程。 © [F. Poux](https://learngeodata.eu/)
- en: This is a tremendous achievement, and you now have a very powerful asset for
    attacking semantic extraction tasks for 3D Scene Understanding. With the Segment
    Anything Model, you can now encapsulate innovation in many products, transforming
    how we perceive and interpret 3D point clouds.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个巨大的成就，你现在拥有了一个非常强大的工具来处理3D场景理解的语义提取任务。通过Segment Anything模型，你可以在许多产品中封装创新，改变我们感知和解读3D点云的方式。
- en: Our exploration should have painted a comprehensive, usable picture of this
    groundbreaking model from its inception to its implications. You may now explore
    the variants and extend their pertinency based on the limitations that were spotted
    in the previous parts.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的探索应该为这一开创性模型从起步到其影响描绘了一个全面、实用的图景。你现在可以探索这些变体，并根据之前部分发现的限制扩展其相关性。
- en: '🦊 **Florent**: *I am looking forward to your future projects that make use
    of it!*'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 🦊 **Florent**：*我期待你未来的项目能加以利用！*
- en: '🤠 **Ville**: *Code on!*'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 🤠 **Ville**：*继续编码！*
- en: References
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao,
    T., Whitehead, S., Berg, A.C., Lo, W.Y. and Dollár, P., 2023\. Segment anything.
    [*arXiv preprint arXiv:2304.02643*](https://arxiv.org/pdf/2304.02643).
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kirillov, A.，Mintun, E.，Ravi, N.，Mao, H.，Rolland, C.，Gustafson, L.，Xiao, T.，Whitehead,
    S.，Berg, A.C.，Lo, W.Y. 和 Dollár, P.，2023\. 分割任何东西。[*arXiv 预印本 arXiv:2304.02643*](https://arxiv.org/pdf/2304.02643)。
- en: '**Poux, Florent**, Mattes, C., Selman, Z. and Kobbelt, L., 2022\. Automatic
    region-growing system for the segmentation of large point clouds. *Automation
    in Construction*, *138*, p.104250\. [Elsevier Link](https://www.sciencedirect.com/science/article/pii/S0926580522001236?casa_token=xnUGwDXoM5gAAAAA%3A1mJlkrTyNZGnbmJnn-9p2qehNHReZvXLX3uJEuXVa6Y5chGjmM-vVAJhezR8wqeKf8XdeR6eng)'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Poux, Florent**，Mattes, C.，Selman, Z. 和 Kobbelt, L.，2022\. 用于大规模点云分割的自动区域生长系统。*建筑自动化*，*138*，第
    104250 页。 [Elsevier 链接](https://www.sciencedirect.com/science/article/pii/S0926580522001236?casa_token=xnUGwDXoM5gAAAAA%3A1mJlkrTyNZGnbmJnn-9p2qehNHReZvXLX3uJEuXVa6Y5chGjmM-vVAJhezR8wqeKf8XdeR6eng)'
- en: '**Lehtola, Ville**, Kaartinen, H., Nüchter, A., Kaijaluoto, R., Kukko, A.,
    Litkey, P., Honkavaara, E., Rosnell, T., Vaaja, M.T., Virtanen, J.P. and Kurkela,
    M., 2017\. Comparison of the selected state-of-the-art 3D indoor scanning and
    point cloud generation methods. *Remote sensing*, *9*(8), p.796\. [MDPI Link](https://www.mdpi.com/2072-4292/9/8/796/pdf)'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Lehtola, Ville**，Kaartinen, H.，Nüchter, A.，Kaijaluoto, R.，Kukko, A.，Litkey,
    P.，Honkavaara, E.，Rosnell, T.，Vaaja, M.T.，Virtanen, J.P. 和 Kurkela, M.，2017\.
    对选定的先进 3D 室内扫描和点云生成方法的比较。*遥感*，*9*(8)，第 796 页。 [MDPI 链接](https://www.mdpi.com/2072-4292/9/8/796/pdf)'
- en: '![](../Images/f85f3cdf9e16e572c3a0759b83ebfe22.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f85f3cdf9e16e572c3a0759b83ebfe22.png)'
- en: 🔷Other Resources
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 🔷其他资源
- en: '🍇 Get Access to the Data here: [3D Datasets](https://drive.google.com/drive/folders/1RPM69xjPpdBqdwZKC-chRBxkmhDiND9v?usp=sharing)'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 🍇 在这里获取数据：[3D 数据集](https://drive.google.com/drive/folders/1RPM69xjPpdBqdwZKC-chRBxkmhDiND9v?usp=sharing)
- en: '👨‍🏫 3D Online Data Science Courses: [3D Academy](https://learngeodata.eu/)'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 👨‍🏫 3D 在线数据科学课程：[3D 学院](https://learngeodata.eu/)
- en: '📖 Subscribe for early access to 3D Tutorials: [3D AI Automation](https://medium.com/@florentpoux/subscribe)'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 📖 订阅以获得 3D 教程的早期访问权限：[3D AI 自动化](https://medium.com/@florentpoux/subscribe)
- en: '🧑‍🎓Get a Master’s Degree: [ITC Utwente](https://www.itc.nl/education/?_ga=2.57523758.507481808.1702288503-2061064516.1700473140)'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 🧑‍🎓获得硕士学位：[ITC Utwente](https://www.itc.nl/education/?_ga=2.57523758.507481808.1702288503-2061064516.1700473140)
- en: 🎓Author’s Recommendation
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 🎓作者推荐
- en: 'To build full Indoor Semantic Extraction Scenarios, you can combine this approach
    with the one explained in the “*3D Point Cloud Shape Detection for Indoor Modelling*”
    article:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建完整的室内语义提取场景，你可以将这种方法与“*3D 点云形状检测用于室内建模*”文章中解释的方法结合起来：
- en: '[](/3d-point-cloud-shape-detection-for-indoor-modelling-70e36e5f2511?source=post_page-----80c06be99a18--------------------------------)
    [## 3D Point Cloud Shape Detection for Indoor Modelling'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/3d-point-cloud-shape-detection-for-indoor-modelling-70e36e5f2511?source=post_page-----80c06be99a18--------------------------------)
    [## 3D 点云形状检测用于室内建模'
- en: A 10-step Python Guide to Automate 3D Shape Detection, Segmentation, Clustering,
    and Voxelization for Space Occupancy…
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一份 10 步 Python 指南，用于自动化 3D 形状检测、分割、聚类和体素化…
- en: towardsdatascience.com](/3d-point-cloud-shape-detection-for-indoor-modelling-70e36e5f2511?source=post_page-----80c06be99a18--------------------------------)
    [](https://learngeodata.eu/3d-newsletter/?source=post_page-----80c06be99a18--------------------------------)
    [## 3D Innovator Newsletter
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/3d-point-cloud-shape-detection-for-indoor-modelling-70e36e5f2511?source=post_page-----80c06be99a18--------------------------------)
    [](https://learngeodata.eu/3d-newsletter/?source=post_page-----80c06be99a18--------------------------------)
    [## 3D 创新者通讯
- en: Weekly practical content, insights, code and resources to master 3D Data Science.
    I write about Point Clouds, AI…
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每周提供实用内容、见解、代码和资源，以掌握 3D 数据科学。我写关于点云、人工智能…
- en: learngeodata.eu](https://learngeodata.eu/3d-newsletter/?source=post_page-----80c06be99a18--------------------------------)
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: learngeodata.eu](https://learngeodata.eu/3d-newsletter/?source=post_page-----80c06be99a18--------------------------------)
