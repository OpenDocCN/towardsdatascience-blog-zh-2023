- en: Demystifying the Random Forest
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解密随机森林
- en: 原文：[https://towardsdatascience.com/demystifying-the-random-forest-8a46f4fd416f?source=collection_archive---------7-----------------------#2023-02-07](https://towardsdatascience.com/demystifying-the-random-forest-8a46f4fd416f?source=collection_archive---------7-----------------------#2023-02-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/demystifying-the-random-forest-8a46f4fd416f?source=collection_archive---------7-----------------------#2023-02-07](https://towardsdatascience.com/demystifying-the-random-forest-8a46f4fd416f?source=collection_archive---------7-----------------------#2023-02-07)
- en: Deconstructing and Understanding this Beautiful Algorithm
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解构并理解这个美妙的算法
- en: '[](https://medium.com/@siddarth.ramesh?source=post_page-----8a46f4fd416f--------------------------------)[![Siddarth
    Ramesh](../Images/645d2850a35ef0175a2fb1eeee9472d5.png)](https://medium.com/@siddarth.ramesh?source=post_page-----8a46f4fd416f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8a46f4fd416f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8a46f4fd416f--------------------------------)
    [Siddarth Ramesh](https://medium.com/@siddarth.ramesh?source=post_page-----8a46f4fd416f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@siddarth.ramesh?source=post_page-----8a46f4fd416f--------------------------------)[![Siddarth
    Ramesh](../Images/645d2850a35ef0175a2fb1eeee9472d5.png)](https://medium.com/@siddarth.ramesh?source=post_page-----8a46f4fd416f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8a46f4fd416f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8a46f4fd416f--------------------------------)
    [Siddarth Ramesh](https://medium.com/@siddarth.ramesh?source=post_page-----8a46f4fd416f--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcf4e627f4995&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-the-random-forest-8a46f4fd416f&user=Siddarth+Ramesh&userId=cf4e627f4995&source=post_page-cf4e627f4995----8a46f4fd416f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8a46f4fd416f--------------------------------)
    ·15 min read·Feb 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8a46f4fd416f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-the-random-forest-8a46f4fd416f&user=Siddarth+Ramesh&userId=cf4e627f4995&source=-----8a46f4fd416f---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcf4e627f4995&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-the-random-forest-8a46f4fd416f&user=Siddarth+Ramesh&userId=cf4e627f4995&source=post_page-cf4e627f4995----8a46f4fd416f---------------------post_header-----------)
    发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----8a46f4fd416f--------------------------------)
    ·15分钟阅读·2023年2月7日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8a46f4fd416f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-the-random-forest-8a46f4fd416f&user=Siddarth+Ramesh&userId=cf4e627f4995&source=-----8a46f4fd416f---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8a46f4fd416f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-the-random-forest-8a46f4fd416f&source=-----8a46f4fd416f---------------------bookmark_footer-----------)![](../Images/bd5376dded404c13d44399eb9e2aa93f.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8a46f4fd416f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-the-random-forest-8a46f4fd416f&source=-----8a46f4fd416f---------------------bookmark_footer-----------)![](../Images/bd5376dded404c13d44399eb9e2aa93f.png)'
- en: Photo by [Inggrid Koe](https://unsplash.com/@inggridkoe?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/kbKEuU-YEIw?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Inggrid Koe](https://unsplash.com/@inggridkoe?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)拍摄，发布在[Unsplash](https://unsplash.com/photos/kbKEuU-YEIw?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: In classical Machine Learning, Random Forests have been a silver bullet type
    of model.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在经典的机器学习中，随机森林一直是一种万灵药式的模型。
- en: 'The model is great for a few reasons:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型有几个优点：
- en: Requires less preprocessing of data compared to many other algorithms, which
    makes it easy to set up
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相比许多其他算法，需要更少的数据预处理，这使得设置更加简单
- en: Acts as either a classification or regression model
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为分类模型或回归模型均可使用
- en: Less prone to overfitting
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不容易过拟合
- en: Easily can compute feature importance
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以轻松计算特征重要性
- en: In this post, I want to better understand the components that make up a Random
    Forest. To accomplish this, I am going to deconstruct the Random Forest into its
    most basic components and explain what is going on in each level of computation.
    By the end, we will have attained a much deeper understanding of how Random Forests
    work and how to work with them with more intuition. The examples we will use will
    be focused on **classification**, but many of the principles apply to the regression
    scenarios as well.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我想更好地理解构成随机森林的组件。为了实现这一点，我将随机森林分解为其最基本的组件，并解释每个计算层级的情况。最后，我们将更深入地了解随机森林的工作原理以及如何更直观地处理它们。我们将使用的示例将重点放在**分类**上，但许多原则也适用于回归场景。
- en: Running a Random Forest
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行随机森林
- en: Let’s start by invoking a classic Random Forest pattern. This is the highest
    level, and what many people do when training Random Forests in python.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们先来调用一个经典的随机森林模式。这是最高水平的，在python训练随机森林的时候，很多人都会这样做。
- en: '![](../Images/c6e2edf8aedaeb3086ca8411e9f80d70.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c6e2edf8aedaeb3086ca8411e9f80d70.png)'
- en: Simulated data. Image by Author
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟数据。图片由作者提供
- en: If I wanted to run a Random Forest to predict my `target` column, I would just
    do the following
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我想运行一个随机森林来预测我的`target`列，我只需要做以下操作
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Running the Random Forest classifier was super simple. I just defined the `n_estimators`
    parameter and set the `random_state` to 0\. I can tell you from personal experience
    that a lot of people would just look at that `.93` , be happy, and deploy that
    thing in the wild. We won’t be doing that today.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 运行随机森林分类器非常简单。我只定义了`n_estimators`参数，并将`random_state`设置为0。我可以告诉你，从个人经验来看，很多人只会看到那个`.93`，感到高兴，并在实际应用中部署它。但我们今天不会这样做。
- en: Let’s re-examine this innocuous line
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新审视一下这条无害的线
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: A random state is a feature of most Data Science models that ensures that someone
    else can reproduce your work. We won’t worry too much about that parameter.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 随机种子是大多数数据科学模型的一个特性，确保别人可以复现你的工作。我们不会太担心这个参数。
- en: 'Let’s look deeply at `n_estimators`. If we look at the `scikit-learn` [docs](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html),
    the definition states:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入看看`n_estimators`。如果我们看看`scikit-learn` [文档](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)，定义如下：
- en: The number of trees in the forest.
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 森林中树的数量。
- en: Investigating the Number of Trees
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调查决策树的数量
- en: At this point, let’s define a Random Forest a little more specifically. A **Random
    Forest** is an **ensemble** model that is a consensus of many **Decision Trees.**
    The definition is probably incomplete, but we will come back to it.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，让我们更具体地定义一下随机森林。**随机森林**是一个由许多**决策树**共识的**集成**模型。这个定义可能还不完整，但我们会再回来讨论。
- en: '![](../Images/91cfa0ae2df0113cc4757ec75d3e0fc8.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91cfa0ae2df0113cc4757ec75d3e0fc8.png)'
- en: Many trees talk to each other and arrive at a consensus. Image by Author
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 许多决策树相互交流并达成共识。图片由作者提供
- en: 'That *might* make you think that if you broke it down into something like the
    following, you might arrive at a Random Forest:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能会让你觉得，如果你将它分解成以下的东西，你可能会得到一个随机森林：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In that above example, we trained 3 decision trees on `X_train`, which means
    that `n_estimators = 3`. After training the 3 trees, we predicted each tree on
    the same test set and then finally take the prediction that 2 out of 3 trees pick.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的例子中，我们在`X_train`上训练了3棵决策树，这意味着`n_estimators = 3`。在训练了这3棵树之后，我们对同样的测试集上的每棵树进行预测，最后取出其中2棵树的预测结果。
- en: That sort of makes sense, but this doesn’t look totally right. If all the decision
    trees were trained on the same data, wouldn’t they mostly reach the same conclusion,
    thereby negating the advantage of an ensemble?
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这种解释有点有道理，但这看起来不完全正确。如果所有的决策树都是在同样的数据上训练的，它们不会大部分达成相同的结论吗，从而抵消了集成的优势？
- en: Demystifying Sampling With Replacement
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解密带替换的抽样
- en: 'Let’s add a word to our definition from earlier: A Random Forestis an ensemblemodel
    that is a consensus of many ***uncorrelated***Decision Trees**.**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在之前的定义中加上一个词：随机森林是一个由许多***不相关的***决策树**共识**的**集成**模型。
- en: 'The Decision Trees can become uncorrelated in 2 ways:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树可以变得不相关，有两种方式：
- en: You have a large enough dataset size where you can sample unique parts of your
    data to each decision tree. This is not as popular and often requires a huge amount
    of data.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你有足够大的数据集大小，可以对每个决策树抽样数据的唯一部分。这并不那么流行，通常需要大量的数据。
- en: You can leverage a technique called **sampling with replacement.** Sampling
    with replacement means that a sample that is drawn from a population is returned
    to the population before the next sample is drawn.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以利用一种叫做**带放回抽样**的技术。带放回抽样意味着从总体中抽取的样本在下一个样本抽取之前会被放回总体。
- en: 'To explain sampling with replacement, let’s say I had 5 marbles with 3 colors,
    so my population looks like this:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明带放回抽样的概念，假设我有5颗珠子，颜色有3种，因此我的总体如下：
- en: '`blue, blue, red, green, red`'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`blue, blue, red, green, red`'
- en: 'If I wanted to sample some marbles, I’d normally pluck out a couple and perhaps
    end up with:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我想抽取一些珠子，我通常会抽出几颗，也许会得到：
- en: '`blue, red`'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`blue, red`'
- en: This is because once I picked up red, I didn’t return it to my original stack
    of marbles.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为一旦我捡起了红色的珠子，我就没有把它放回到原来的珠子堆里。
- en: However, if I was doing sampling with replacement, I can actually pick up any
    of my marbles twice. Because red was returned to my stack, I had a chance of picking
    it up again.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果我在进行带放回抽样，我实际上可以两次抽取任何一颗珠子。因为红色的珠子被放回了我的堆里，所以我有可能再次抽到它。
- en: '`red, red`'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`red, red`'
- en: In Random Forests, the default is to build samples that are about 2/3 of the
    original population size. If my original train data was 1000 rows, then the train
    data samples I feed to my trees might be around 670 rows. That being said, trying
    different sampling ratios would be a great parameter to tune as you construct
    your Random Forest.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机森林中，默认情况下是构建约2/3原始总体大小的样本。如果我的原始训练数据是1000行，那么我输入到树中的训练数据样本可能会有大约670行。也就是说，尝试不同的抽样比例将是构建随机森林时调节的一个很好的参数。
- en: The below code, unlike the last snippet, is much closer to that of a Random
    Forest where `n_estimators = 3`.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码，与之前的代码片段不同，更接近于`n_estimators = 3`的随机森林。
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/30d1e6ff15b476f306ff621175bf2f9f.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30d1e6ff15b476f306ff621175bf2f9f.png)'
- en: We sample with replacement, feed those samples to trees, produce results, and
    achieve consensus. Image by Author.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行带放回抽样，将这些样本输入到树中，生成结果，并达成共识。图片由作者提供。
- en: Bagging Classifiers
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Bagging 分类器
- en: '![](../Images/3ef504d907cfa554a6767b1917c186f7.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ef504d907cfa554a6767b1917c186f7.png)'
- en: The earlier architecture was actually a Bagging Classifier. Image by Author.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的架构实际上是一个Bagging分类器。图片由作者提供。
- en: We’re going to introduce a *new* algorithm at this point called **Bootstrap
    Aggregation**, also known as Bagging, but rest assured that this will tie back
    to Random Forests. The reason we are introducing this new concept is because as
    we can see in the below figure, everything we just did up to now is actually what
    a `BaggingClassifier` does!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在此引入一种*新的*算法，称为**Bootstrap Aggregation**，也称为Bagging，但请放心，这将与随机森林相关联。我们引入这个新概念的原因是因为正如我们在下图中看到的，直到现在我们所做的一切实际上就是`BaggingClassifier`所做的！
- en: In the code below, the `BaggingClassifier` has a parameter called `bootstrap`
    which actually executes the sampling-with-replacement step we just manually did.
    This same parameter exists for the `sklearn` Random Forest implementation. If
    bootstrapping was false, we would use the entire population for each classifier.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，`BaggingClassifier`有一个叫做`bootstrap`的参数，它实际上执行了我们刚刚手动完成的带放回抽样步骤。这个参数在`sklearn`的随机森林实现中也存在。如果bootstrapping为false，我们将使用整个总体来训练每个分类器。
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`BaggingClassifiers` are awesome because you can use them with estimators not
    named Decision Trees! You can plug in many algorithms, and Bagging turns it into
    an ensemble solution. A Random Forest Algorithm actually *extends* the Bagging
    Algorithm (if `bootstrapping = true`) because it partially leverages the bagging
    to form uncorrelated decision trees.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`BaggingClassifiers`非常棒，因为你可以将它们与非决策树的估计器一起使用！你可以插入许多算法，Bagging将其转化为一个集成解决方案。随机森林算法实际上*扩展*了Bagging算法（如果`bootstrapping
    = true`），因为它部分利用了Bagging来形成不相关的决策树。'
- en: However even if `bootstrapping = false`, Random Forests go one step extra to
    *really* make sure the trees are not correlated — feature sampling.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，即使`bootstrapping = false`，随机森林也会进一步*确保*树之间没有相关性——特征抽样。
- en: Demystifying Feature Sampling
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解密特征抽样
- en: Feature sampling means that not only are the rowssampled, but the columns too.
    Unlike the rows, the columns of the Random Forest are sampled *without* replacement,
    which means we won’t have duplicate columns training 1 tree.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 特征抽样意味着不仅对行进行抽样，还对列进行抽样。与行不同，随机森林的列是*不带放回*抽样的，这意味着我们不会有重复的列训练一棵树。
- en: There are many ways to sample the features. You can specify a fixed max number
    of features to sample, take the square-root of the total number of features, or
    try using logs. Each of these approaches has tradeoffs and will depend on your
    data and use-case.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 采样特征的方法有很多种。你可以指定一个固定的特征最大数量进行采样，取特征总数的平方根，或者尝试使用对数。这些方法各有利弊，具体取决于你的数据和使用场景。
- en: '![](../Images/ebba61e751cb491987d4eed373570aba.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ebba61e751cb491987d4eed373570aba.png)'
- en: Bagging gets extended with Feature Sampling. Image by Author.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging 扩展了特征采样。图片由作者提供。
- en: The below snippet samples the columns using the `sqrt` technique, samples the
    rows, trains 3 Decision trees, and uses majority-rules to make the prediction.
    We first perform sampling-with-replacement, them sample the columns, train our
    individual trees, let our trees predict on our test data, then take the majority
    rules consensus.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码片段使用`sqrt`技术采样列，采样行，训练3棵决策树，并使用多数规则进行预测。我们首先进行有放回采样，然后采样列，训练我们的单独决策树，让树在测试数据上进行预测，然后采用多数规则的共识。
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: When I ran this code, I found that my decision trees started predicting different
    things, which indicated that we had removed a lot of the correlations between
    the trees.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当我运行这段代码时，我发现我的决策树开始预测不同的结果，这表明我们已经去除了树之间的许多相关性。
- en: '![](../Images/b87df62d480d87d06d536fc11c9cdff3.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b87df62d480d87d06d536fc11c9cdff3.png)'
- en: My trees no longer are agreeing with each other all the time! Image by Author.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我的树不再总是彼此一致！图片由作者提供。
- en: Decision Tree Basics
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树基础
- en: Up to this point, we’ve deconstructed how data is fed into a multitude of Decision
    Trees. In the previous code examples, we used the `DecisionTreeClassifier()` to
    train Decision Trees, but we will need to deconstruct Decision Trees in order
    to completely understand Random Forests.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经拆解了数据如何被输入到众多决策树中。在之前的代码示例中，我们使用了`DecisionTreeClassifier()`来训练决策树，但为了完全理解随机森林，我们需要进一步拆解决策树。
- en: A Decision Tree, true to its name, looks like an upside-down tree. At a high
    level, the algorithm tries to ask questions to split data into different nodes.
    The below image shows an example of what a decision tree looks like.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树，顾名思义，看起来像一棵倒立的树。从高层次来看，该算法试图通过提问将数据分裂成不同的节点。下图展示了决策树的样子。
- en: '![](../Images/691a0400401b95fbc6a92344d3266917.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/691a0400401b95fbc6a92344d3266917.png)'
- en: Example Tree. Image by author
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 示例树。图片由作者提供
- en: A decision tree asks a series of questions based on the answer of the previous
    question. For every question it asks, there might be multiple answers, and we
    visualize that as **splitting the node**. The answer of the previous question
    will determine the next question the tree will ask. At some point after asking
    a series of questions, you arrive at the answer.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树根据前一个问题的答案提出一系列问题。每个问题可能有多个答案，我们将其可视化为**节点的分裂**。前一个问题的答案将决定树接下来会提出什么问题。在提出一系列问题后，最终你会得到答案。
- en: But how do you know your answer is accurate, or that you have asked the right
    questions? You can actually evaluate your decision trees in a couple different
    ways, and we will of course break down those approaches as well.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 但你如何知道你的答案是准确的，或者你问了正确的问题？你实际上可以通过几种不同的方式评估你的决策树，我们当然也会详细解析这些方法。
- en: Entropy and Information Gain
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 熵与信息增益
- en: At this point, we need to discuss a new term called **entropy**. At a high level,
    entropy is one way to measure the level of *impurity* or *randomness* in a node.
    As an aside, there is another popular to way measure the impurity of a node called
    **Gini impurity**, but we won’t deconstruct that method in this post since it
    overlaps with a lot of the concepts around entropy albeit with a slightly different
    computation. The general idea is that higher the entropy or Gini impurity, the
    more variance there is in the node, and our goal is to reduce that uncertainty.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们需要讨论一个新的术语——**熵**。从高层次来看，熵是一种衡量节点中*杂质*或*随机性*水平的方法。顺便提一下，还有一种叫做**基尼杂质**的流行方法来测量节点的杂质，但由于它与熵的概念重叠，我们在这篇文章中不会拆解这个方法，尽管它的计算方式略有不同。一般来说，熵或基尼杂质越高，节点中的方差越大，我们的目标是减少这种不确定性。
- en: Decision trees try to minimize entropy by splitting the node they ask about
    into smaller more homogenous nodes. The actual formula for entropy is
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树试图通过将它们提问的节点分裂成更小、更均匀的节点来最小化熵。熵的实际公式是
- en: '![](../Images/fce8f09b156d8ccd46d1559dcf681163.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fce8f09b156d8ccd46d1559dcf681163.png)'
- en: 'To break down entropy, let’s go back to that marble example:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了分解熵，我们回到弹珠的例子：
- en: 'Let’s say I have 10 marbles. 5 of them are blue and 5 of them are green. The
    entropy of my collective dataset would be1.0,and the code to calculate entropy
    is below:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我有10个弹珠，其中5个是蓝色的，5个是绿色的。我数据集的熵为1.0，计算熵的代码如下：
- en: '[PRE6]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If the `data` was completely filled with green marbles, the entropy would have
    been 0, and the entropy would increase the closer we got to that 50% split.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`data`完全被绿色弹珠填充，那么熵将为0，而熵将随着接近50%分裂而增加。
- en: 'Every time we reduce entropy, we gain some information about the dataset since
    we have reduced the randomness. **Information gain** tells us which feature relatively
    allowed us best to minimize our entropy. The way you calculate information gain
    is:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 每次我们减少熵时，我们就获得了一些关于数据集的信息，因为我们减少了随机性。**信息增益**告诉我们哪个特征相对最好地减少了熵。计算信息增益的方法是：
- en: '`entropy(parent) — [weighted_average_of_entropy(children)]`'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`entropy(parent) — [weighted_average_of_entropy(children)]`'
- en: In this case, the parent is the original node and the children are the results
    of splitting the node.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，父节点是原始节点，子节点是分裂节点后的结果。
- en: '![](../Images/9cdb3546196da9296c15c4dbe3cc344b.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9cdb3546196da9296c15c4dbe3cc344b.png)'
- en: Splitting a node. Image by Author
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 分裂一个节点。图片来源：作者
- en: 'To calculate the information gain, we perform the following operations:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算信息增益，我们执行以下操作：
- en: Calculate the entropy of the parent node
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算父节点的熵
- en: Split the parent node into child nodes
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将父节点分裂为子节点
- en: Create the `weight` for each child node. This is measured by `number_of_samples_in_child_node
    / number_of_samples_in_parent_node`
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每个子节点创建`weight`。这是通过`number_of_samples_in_child_node / number_of_samples_in_parent_node`来测量的
- en: Calculate the entropy of each child node
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算每个子节点的熵
- en: Create `[weighted_average_of_entropy(children)]` by calculating `weight*entropy_of_child1
    + weight*entropy_of_child2`
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过计算`weight*entropy_of_child1 + weight*entropy_of_child2`来创建`[weighted_average_of_entropy(children)]`
- en: Subtract this weighted entropy from the entropy of the parent node
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从父节点的熵中减去加权熵
- en: The below code implements a simple information gain for a parent node being
    split into 2 child nodes
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码实现了父节点被分裂成两个子节点的简单信息增益
- en: '[PRE7]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Deconstructing the Decision Tree
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解构决策树
- en: With these concepts in mind, we are ready to implement a small decision tree!
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 了解这些概念后，我们准备实现一个小的决策树！
- en: Without any guidance, a decision tree will keep splitting nodes until all the
    final leaf nodes are pure. The idea of controlling the complexity of the tree
    is called **pruning,** and we can either prune the tree after it is completely
    built, or pre-prune the tree before the growing phase with certain parameter.
    Some ways to pre-prunetree complexity are to control the number of splits, limit
    the **max depth** (longest distance from root node to leaf node), or set an information
    gain .
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 没有任何指导下，决策树将不断分裂节点，直到所有最终的叶节点都是纯净的。控制树复杂度的想法称为**剪枝**，我们可以在树完全构建后剪枝，或者在生长阶段之前用某些参数进行预剪枝。预剪枝树复杂度的一些方法是控制分裂次数，限制**最大深度**（从根节点到叶节点的最长距离），或设置信息增益。
- en: The below code ties all these concepts back together
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码将所有这些概念结合在一起
- en: Start with an dataset where you have a target variable to predict
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从一个包含目标变量的数据集开始
- en: Compute entropy (or Gini impurity) of the original dataset (root node)
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算原始数据集（根节点）的熵（或基尼不纯度）
- en: Look at each feature and compute the information gain
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看每个特征并计算信息增益
- en: Choose the best feature that had the best information gain, which is the same
    as which feature caused the entropy to decrease the most
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择信息增益最佳的特征，即导致熵减少最多的特征
- en: Keep growing until our stopping conditions are met — in this case it is our
    max depth limit and nodes having an entropy of 0.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 继续生长，直到满足我们的停止条件——在这种情况下是我们的最大深度限制和熵为0的节点。
- en: '[PRE8]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: To predict on this tree means to traverse your grown tree with your new data
    until it arrives at the leaf node. The final leaf node is the prediction.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这棵树上进行预测意味着用你的新数据遍历已生长的树，直到到达叶节点。最终的叶节点就是预测结果。
- en: Some Interesting Things About Random Forests
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于随机森林的一些有趣事项
- en: Everything we discussed in the last section was how an individual decision tree
    makes its decisions. The below image connects those concepts with the earlier
    concepts we discussed around sampling for Random Forests.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了单棵决策树如何做出决策。下面的图像将这些概念与我们之前讨论的关于随机森林采样的概念联系起来。
- en: '![](../Images/718efa8483662a21fc33d3bef4c6c0fc.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/718efa8483662a21fc33d3bef4c6c0fc.png)'
- en: The Random Forest Architecture with a Deconstructed Decision Tree. Image by
    Author
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林架构与解构决策树。图像来源：作者
- en: Because a Decision Tree literally checks the information gain of each feature,
    you can calculate **feature importance** in a Random Forest. The calculation of
    a feature importance is usually seen as the average decrease in impurity across
    *all* the trees. Random Forests are not as interpretable as models like Logistic
    Regressions, so feature importance provide us with a little insight into how our
    trees grew.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 由于决策树实际上检查了每个特征的信息增益，你可以在随机森林中计算**特征重要性**。特征重要性的计算通常被视为*所有*树中杂质的平均减少。随机森林不像逻辑回归那样易于解释，因此特征重要性为我们提供了有关树如何生长的一些洞察。
- en: And finally, there are a couple ways you can test your trained Random Forest.
    You can always do the classic Machine Learning approach and use a test set to
    measure how well your model generalizes to unseen data. However, that usually
    requires a second computation. Random Forest has this unique property called **out-of-bag
    error**, or **OOB error**. Remember how we only sample a part of our dataset to
    build each tree? You can actually use the rest of the samples to do a validation
    *at the time of training*, and this is really only possible because of the ensemble
    nature of the algorithm. This means that in one shot, we can understand how well
    our model generalizes to unseen data.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以通过几种方式来测试训练好的随机森林。你可以采用经典的机器学习方法，使用测试集来衡量模型对未见数据的泛化能力。然而，这通常需要额外的计算。随机森林具有一种独特的属性，称为**袋外误差**或**OOB误差**。还记得我们如何只对数据集的一部分进行采样来构建每棵树吗？实际上，你可以使用剩余的样本在*训练时*进行验证，这仅仅因为算法的集成性质才成为可能。这意味着我们可以一次性了解模型对未见数据的泛化能力。
- en: Conclusion and Final Thoughts
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论和最终思考
- en: 'To summarize what we learned:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 总结我们所学到的：
- en: Random Forests are actually an ensemble of un-correlated **Decision Trees**
    making predictions and reaching a consensus. This consensus is an average of scores
    for regression problems and a majority rules for classification problems
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林实际上是由多个不相关的**决策树**组成的集成，它们进行预测并达成共识。对于回归问题，这个共识是得分的平均值，对于分类问题则是多数规则。
- en: Random Forests mitigate correlation by leveraging **bagging** and **feature
    sampling**. By leveraging both these techniques, the individual decision trees
    are viewing a particular dimension of our set and making predictions based on
    different factors.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林通过利用**袋装**和**特征采样**来减轻相关性。通过利用这两种技术，个体决策树在观察数据集的特定维度，并基于不同因素做出预测。
- en: Decision Trees are grown by splitting data on the feature that produces the
    highest **information gain**. Information gain is measure as the highest decrease
    in impurity. Impurity is usually measured via **entropy** or **Gini impurity**.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树通过在产生最高**信息增益**的特征上进行数据分裂来生长。信息增益是通过最大发生的杂质减少来度量的。杂质通常通过**熵**或**基尼杂质**来度量。
- en: Random Forests are able to achieve a limited level of interpretability via **feature
    importance**, which is a measure of average information gain of a feature.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林通过**特征重要性**实现了有限的可解释性，特征重要性是对特征的平均信息增益的度量。
- en: Random Forests also have the ability to a form of cross-validation at training
    time, a unique technique known as **OOB** error. This is possible because of the
    way the algorithm samples data upstream.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林还具有在训练时进行交叉验证的能力，这是一种独特的技术，称为**OOB**误差。这是由于算法在上游采样数据的方式所使然。
- en: '[PRE9]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: When looking at the original code to train a Random Forest, it’s amazing to
    me just how many different computations and evaluations happen in these few lines
    of code. There are a multitude of considerations taken to prevent overfitting,
    evaluate both on a tree and forest level, and achieve some basic level of interpretability
    — plus it is very easy to set up thanks to all the frameworks out there.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看训练随机森林的原始代码时，我对这些少数代码行中发生的各种计算和评估感到惊讶。为了防止过拟合、在树和森林层面进行评估并实现一定程度的可解释性，考虑了多种因素——而且由于各种框架的存在，设置起来非常简单。
- en: I hope next time you train a Random Forest model, you are able to check out
    the `[scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)`
    [documentation page](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)
    on Random Forests and better understand all the options you have. While there
    are some intuitive defaults set, it should be clear just how many different tweaks
    you can make and how many of these techniques can extend to other models as well.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望下次你训练一个随机森林模型时，能够查看一下有关随机森林的`[scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)`
    [文档页面](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)，更好地理解你所拥有的所有选项。虽然有一些直观的默认设置，但应该清楚你可以做多少不同的调整，以及这些技术有多少可以扩展到其他模型中。
- en: I had a lot of fun writing this post, and personally learned a ton about how
    this beautiful algorithm works. I hope that you take something away too!
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我在写这篇文章时非常开心，并且个人对这个美妙算法的工作原理学到了很多。我希望你也能有所收获！
