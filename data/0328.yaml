- en: Overcoming the Limitations of Large Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 克服大型语言模型的局限性
- en: 原文：[https://towardsdatascience.com/overcoming-the-limitations-of-large-language-models-9d4e92ad9823?source=collection_archive---------0-----------------------#2023-01-23](https://towardsdatascience.com/overcoming-the-limitations-of-large-language-models-9d4e92ad9823?source=collection_archive---------0-----------------------#2023-01-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/overcoming-the-limitations-of-large-language-models-9d4e92ad9823?source=collection_archive---------0-----------------------#2023-01-23](https://towardsdatascience.com/overcoming-the-limitations-of-large-language-models-9d4e92ad9823?source=collection_archive---------0-----------------------#2023-01-23)
- en: How to enhance LLMs with human-like cognitive skills
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何通过类人认知技能增强大型语言模型
- en: '[](https://medium.com/@janna.lipenkova_52659?source=post_page-----9d4e92ad9823--------------------------------)[![Dr.
    Janna Lipenkova](../Images/112fe9a8c5936869243f2a43fde6dfee.png)](https://medium.com/@janna.lipenkova_52659?source=post_page-----9d4e92ad9823--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9d4e92ad9823--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9d4e92ad9823--------------------------------)
    [Dr. Janna Lipenkova](https://medium.com/@janna.lipenkova_52659?source=post_page-----9d4e92ad9823--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@janna.lipenkova_52659?source=post_page-----9d4e92ad9823--------------------------------)[![Dr.
    Janna Lipenkova](../Images/112fe9a8c5936869243f2a43fde6dfee.png)](https://medium.com/@janna.lipenkova_52659?source=post_page-----9d4e92ad9823--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9d4e92ad9823--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9d4e92ad9823--------------------------------)
    [Dr. Janna Lipenkova](https://medium.com/@janna.lipenkova_52659?source=post_page-----9d4e92ad9823--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff215f8e427a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fovercoming-the-limitations-of-large-language-models-9d4e92ad9823&user=Dr.+Janna+Lipenkova&userId=f215f8e427a2&source=post_page-f215f8e427a2----9d4e92ad9823---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9d4e92ad9823--------------------------------)
    ·16 min read·Jan 23, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9d4e92ad9823&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fovercoming-the-limitations-of-large-language-models-9d4e92ad9823&user=Dr.+Janna+Lipenkova&userId=f215f8e427a2&source=-----9d4e92ad9823---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff215f8e427a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fovercoming-the-limitations-of-large-language-models-9d4e92ad9823&user=Dr.+Janna+Lipenkova&userId=f215f8e427a2&source=post_page-f215f8e427a2----9d4e92ad9823---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9d4e92ad9823--------------------------------)
    ·16 分钟阅读·2023年1月23日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9d4e92ad9823&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fovercoming-the-limitations-of-large-language-models-9d4e92ad9823&user=Dr.+Janna+Lipenkova&userId=f215f8e427a2&source=-----9d4e92ad9823---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9d4e92ad9823&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fovercoming-the-limitations-of-large-language-models-9d4e92ad9823&source=-----9d4e92ad9823---------------------bookmark_footer-----------)![](../Images/fbb74c1f81efccf5fc35f935c5bd8dad.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9d4e92ad9823&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fovercoming-the-limitations-of-large-language-models-9d4e92ad9823&source=-----9d4e92ad9823---------------------bookmark_footer-----------)![](../Images/fbb74c1f81efccf5fc35f935c5bd8dad.png)'
- en: 'How popular LLMs score along human cognitive skills (source: semantic embedding
    analysis of ca. 400k AI-related online texts since 2021)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型在人类认知技能方面的表现（来源：自2021年以来约40万篇AI相关在线文本的语义嵌入分析）
- en: '*Disclaimer: This article was written without the support of ChatGPT.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*免责声明：本文在没有 ChatGPT 支持的情况下撰写。*'
- en: In the last couple of years, Large Language Models (LLMs) such as ChatGPT, T5
    and LaMDA have developed amazing skills to produce human language. We are quick
    to attribute intelligence to models and algorithms, but how much of this is emulation,
    and how much is really reminiscent of the rich language capability of humans?
    When confronted with the natural-sounding, confident outputs of these models,
    it is sometimes easy to forget that language *per se* is only the tip of the communication
    iceberg. Its full power unfolds in combination with a wide range of complex cognitive
    skills relating to perception, reasoning and communication. While humans acquire
    these skills naturally from the surrounding world as they grow, the learning inputs
    and signals for LLMs are rather meagre. They are forced to learn only from the
    surface form of language, and their success criterion is not communicative efficiency
    but the reproduction of high-probability linguistic patterns.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几年里，像 ChatGPT、T5 和 LaMDA 这样的大型语言模型（LLMs）发展出了惊人的人类语言生成技能。我们很容易将智能归因于模型和算法，但这其中有多少是模仿的，又有多少真正类似于人类丰富的语言能力？当面对这些模型自然流畅、自信的输出时，有时很容易忘记语言*本身*只是交流冰山的一角。其真正的力量在于与各种复杂的认知技能（包括感知、推理和沟通）结合时展开。虽然人类在成长过程中自然从周围世界中获得这些技能，但
    LLMs 的学习输入和信号却相对稀少。它们被迫仅从语言的表面形式中学习，它们的成功标准不是交流效率，而是高概率语言模式的再现。
- en: In the business context, this can lead to bad surprises when too much power
    is given to an LLM. Facing its own limitations, it will not admit them and rather
    gravitate to the other extreme — produce non-sense, toxic content or even dangerous
    advice with a high level of confidence. For example, a medical virtual assistant
    driven by GPT-3 can advise its user to kill themselves at a certain point in the
    conversation.[4]
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在商业环境中，当 LLM 被赋予过多权力时，可能会导致意外的负面结果。面对自身的局限性，它不会承认这些局限，而是倾向于走向另一个极端——生成无意义、有毒的内容，甚至给出具有高度自信的危险建议。例如，由
    GPT-3 驱动的医疗虚拟助手可能在对话的某个时点建议用户自杀。[4]
- en: Considering these risks, how can we safely benefit from the power of LLMs when
    integrating them in our product development? On the one hand, it is important
    to be aware of inherent weak points and use rigorous evaluation and probing methods
    to target them in specific use cases, instead of relying on happy-path interactions.
    On the other hand, the race is on — all major AI labs are planting their seeds
    to enhance LLMs with additional capabilities, and there is plenty of space for
    a cheerful glance into the future. In this article, we will look into the limitations
    of LLMs and discuss ongoing efforts to control and enhance LLM behaviour. A basic
    knowledge of the workings of language models is assumed — if you are a newbie,
    please refer to [this article](https://medium.com/p/1288ef3c4929).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些风险，我们如何在将 LLMs 融入产品开发时安全地利用其力量？一方面，重要的是要意识到固有的弱点，并使用严格的评估和探测方法来针对特定用例中的这些弱点，而不是依赖于理想的互动。另一方面，竞赛已经开始——所有主要的
    AI 实验室都在播种以增强 LLMs 的额外能力，未来充满了乐观的前景。本文将探讨 LLMs 的局限性，并讨论控制和增强 LLM 行为的持续努力。假设你对语言模型的工作原理有基本的了解——如果你是新手，请参考[这篇文章](https://medium.com/p/1288ef3c4929)。
- en: Before diving into the technology, let’s set the scene with a thought experiment
    — the “Octopus test” as proposed by Emily Bender — to understand how differently
    humans and LLMs see the world.[1]
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入技术之前，让我们通过一个思想实验——由 Emily Bender 提出的“章鱼测试”——来了解人类和 LLMs 看待世界的方式有多么不同。[1]
- en: In the skin of an octopus
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在章鱼的皮肤上
- en: Imagine that Anna and Maria are stranded on two uninhabited islands. Luckily,
    they have discovered two telegraphs and an underwater cable left behind by previous
    visitors and start communicating with each other. Their conversations are “overheard”
    by a quick-witted octopus who has never seen the world above water but is exceptionally
    good at statistical learning. He picks up the words, syntactic patterns and communication
    flows between the two ladies and thus masters the external form of their language
    without understanding how it is actually grounded in the real world. As Ludwig
    Wittgenstein once put it, “the limits of language are the limits of my world”
    — while we know today that the world models of humans are composed of much more
    than language, the octopus would sympathise with this statement, at least regarding
    his knowledge of the world above water.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，安娜和玛丽亚被困在两个无人居住的岛上。幸运的是，他们发现了两个电报机和一条由之前的访客留下的水下电缆，并开始互相通信。他们的对话被一只机智的章鱼“窃听”了，这只章鱼从未见过水面上的世界，但在统计学习方面非常出色。他拾取了两位女士之间的词汇、句法模式和交流流动，从而掌握了他们语言的外在形式，但并不了解它在现实世界中的实际基础。正如路德维希·维特根斯坦曾经说过的：“语言的界限就是我的世界的界限”——虽然我们现在知道人类的世界模型包含了比语言更多的东西，但章鱼至少在了解水面上世界的知识时会同情这一说法。
- en: 'At some point, listening is not enough. Our octopus decides to take control,
    cuts the cable on Maria’s side and starts chatting with Anna. The interesting
    question is, when will Anna detect the change? As long as the two parties exchange
    social pleasantries, there is a reasonable chance that Anna will not suspect anything.
    Their small talk might go on as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在某个时刻，仅仅听是不够的。我们的章鱼决定掌控局面，切断玛丽亚那边的电缆，开始与安娜聊天。有趣的问题是，安娜何时会发现变化？只要双方交换社交客套话，安娜有很大可能不会怀疑什么。他们的闲聊可能会这样进行：
- en: '*A: Hi Maria!*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*A: 嗨，玛丽亚！*'
- en: '*O: Hi Anna, how are you?*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*O: 嗨，安娜，你好吗？*'
- en: '*A: Thanks, I’m good, just enjoyed a coconut breakfast!*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*A: 谢谢，我很好，只是享受了一顿椰子早餐！*'
- en: '*O: You are lucky, there are no coconuts on my island. What are your plans?*'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*O: 你真幸运，我的岛上没有椰子。你有什么计划？*'
- en: '*A: I wanted to go swimming but I am afraid there will be a storm. And you?*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*A: 我本来想去游泳，但我担心会有风暴。你呢？*'
- en: '*O: I am having my breakfast now and will do some woodwork afterwards.*'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*O: 我现在正在吃早餐，之后会做一些木工。*'
- en: '*A: Have a nice day, talk later!*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*A: 祝你有美好的一天，稍后再聊！*'
- en: '*O: Bye!*'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*O: 再见！*'
- en: However, as their relationship deepens, their communication also grows in intensity
    and sophistication. Over the next sections, we will take the octopus through a
    couple of scenes from island life that require the mastery of common-sense knowledge,
    communicative context and reasoning. As we go, we will also survey approaches
    to incorporate additional intelligence into agents — be they fictive octopusses
    or LLMs — that are originally only trained from the surface form of language.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着他们关系的加深，他们的沟通也变得更为强烈和复杂。在接下来的章节中，我们将带着章鱼经历几个需要掌握常识、交流背景和推理的岛屿生活场景。与此同时，我们还将探讨将额外智能融入代理的方法——无论是虚构的章鱼还是LLMs——这些代理最初只接受了语言表层的训练。
- en: Injecting world knowledge into LLMs
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向LLMs注入世界知识
- en: One morning, Anna is planning a hunting trip and tries to forecast the weather
    for the day. Since the wind is coming from Maria’s direction, she asks “Maria”
    for a report on current weather conditions as an important piece of information.
    Being caught in deep waters, our octopus grows embarrassed about describing the
    weather conditions. Even if he had a chance to glance into the skies, he would
    not know what specific weather terms like “rain”, “wind”, “cloudy” etc. refer
    to in the real world. He desperately makes up some weather facts. Later in the
    day, while hunting in the woods, Anna is surprised by a dangerous thunderstorm.
    She attributes her failure to predict the storm to a lack of meteorological knowledge
    rather than a deliberate hallucination by her conversation partner.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一天早上，安娜计划去打猎，并试图预测当天的天气。由于风来自玛丽亚的方向，她向“玛丽亚”询问当前天气情况的报告作为重要信息。被困在深水中的章鱼对描述天气情况感到尴尬。即使他有机会瞥一眼天空，他也不知道像“雨”、“风”、“多云”等具体天气术语在现实世界中指的是什么。他拼命编造一些天气事实。当天晚些时候，安娜在树林里打猎时被一场危险的雷暴惊到。她把预测风暴的失败归因于缺乏气象知识，而不是她对话伙伴的故意幻想。
- en: At the surface, LLMs are able to truthfully reflect a lot of true facts about
    the world. However, their knowledge is limited to concepts and facts that they
    explicitly encountered in the training data. Even with huge training data, this
    knowledge cannot be complete. For example, it might miss domain-specific knowledge
    that is required for commercial use cases. Another important limitation, as of
    now, is the recency of the information. Since language models lack a notion of
    temporal context, they can’t work with dynamic information such as the current
    weather, stock prices or even today’s date.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 从表面上看，大型语言模型（LLMs）能够真实地反映许多关于世界的事实。然而，它们的知识仅限于在训练数据中明确遇到的概念和事实。即使有大量的训练数据，这些知识也无法完整。例如，它可能缺少用于商业用例的特定领域知识。目前的另一个重要限制是信息的时效性。由于语言模型缺乏时间上下文的概念，它们无法处理动态信息，如当前天气、股市价格或今天的日期。
- en: This problem can be solved by systematically “injecting” additional knowledge
    into the LLM. This new input can come from various sources, such as structured
    external databases (e.g. FreeBase or WikiData), company-specific data sources
    and APIs. One possibility to inject it is via adapter networks that are “plugged
    in” between the LLM layers to learn the new knowledge:[2]
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题可以通过系统地“注入”额外的知识到LLM中来解决。这些新输入可以来自各种来源，例如结构化的外部数据库（如 FreeBase 或 WikiData）、公司特定的数据源和API。一种注入方法是通过在LLM层之间“插入”的适配器网络来学习新知识：[2]
- en: '![](../Images/a8a3a7fd1cbed601a86842e218eb0c7c.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8a3a7fd1cbed601a86842e218eb0c7c.png)'
- en: '*Figure 1: Architecture of adapter-based knowledge injection into LLMs [2]*'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1：适配器基于知识注入到LLMs的架构 [2]*'
- en: The training of this architecture happens in two steps, namely **memorisation**
    and **utilisation:**
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构的训练分为两个步骤，即**记忆化**和**利用**：
- en: '1\. During **memorisation**, the LLM is frozen and the adapter networks learn
    the new facts from the knowledge base. The learning signal is provided via masked
    language modelling, whereby parts of the facts are hidden and the adapters learn
    to reproduce them:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 在**记忆化**阶段，LLM被冻结，适配器网络从知识库中学习新事实。学习信号通过掩蔽语言建模提供，其中部分事实被隐藏，适配器学习重现这些事实：
- en: '![](../Images/eb3b2b046ab0bcbc9bbadd84b51bdf18.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb3b2b046ab0bcbc9bbadd84b51bdf18.png)'
- en: '*Figure 2: Adapters are trained during the* ***memorisation*** *step*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2：适配器在* ***记忆化*** *步骤中进行训练*'
- en: '2\. During **utilisation**, the LM learns to leverage the facts memorised by
    the adapters in the respective downstream tasks. Here, in turn, the adapter networks
    are frozen while the weights of the model are optimised:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 在**利用**阶段，语言模型学习在各自的下游任务中利用适配器记忆的事实。在这里，适配器网络被冻结，同时模型的权重被优化：
- en: '![](../Images/8a498c1f61d6a9a7f8df5e8450e33dce.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a498c1f61d6a9a7f8df5e8450e33dce.png)'
- en: '*Figure 3: LLM learns to use adapter knowledge during the* ***utilisation***
    *step*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3：LLM在* ***利用*** *步骤中学习使用适配器知识*'
- en: During inference, the hidden state that the LLM provides to the adapter is fused
    with the adapter’s output using a fusion function to produce the final answer.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中，LLM提供给适配器的隐藏状态通过融合函数与适配器的输出融合，以生成最终答案。
- en: While architecture-level knowledge injection allows for efficient modular retraining
    of smaller adapter networks, the modification of the architecture also requires
    considerable engineering skill and effort. The easier alternative is input-level
    injection, where the model is directly fine-tuned on the new facts (cf. [3] for
    an example). The downside is the expensive fine-tuning required after each change
    — thus, it is not suitable for dynamic knowledge sources. A complete overview
    over existing knowledge injection approaches can be found in [this article](https://arxiv.org/pdf/2101.12294.pdf).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然架构级别的知识注入允许对较小的适配器网络进行高效的模块化再训练，但修改架构也需要相当的工程技能和努力。更简单的替代方案是输入级别注入，即模型直接在新事实上进行微调（参见[3]中的示例）。缺点是每次更改后都需要昂贵的微调——因此不适合动态知识来源。有关现有知识注入方法的完整概述可以在[这篇文章](https://arxiv.org/pdf/2101.12294.pdf)中找到。
- en: Knowledge injection helps you build domain intelligence, which is becoming a
    key differentiator for vertical AI products. In addition, you can use it to establish
    traceability so the model can point a user to the original sources of information.
    Beyond structured knowledge injection, efforts are underway to integrate multimodal
    information and knowledge into LLMs. For instance, in April 2022, DeepMind introduced
    Flamingo, a visual language model that can seamlessly ingest text, images and
    video.[5] At the same time, Google is working on Socratic Models, a modular framework
    in which multiple pre-trained models may be composed zero-shot i.e., via multi-modal
    prompting, to exchange information with each other.[6]
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 知识注入帮助你构建领域智能，这已成为垂直AI产品的关键差异化因素。此外，你还可以利用它建立可追溯性，以便模型可以指引用户到原始信息来源。除了结构化知识注入外，目前还在努力将多模态信息和知识整合到LLM中。例如，在2022年4月，DeepMind推出了Flamingo，一个能够无缝摄取文本、图像和视频的视觉语言模型。[5]
    与此同时，谷歌正在开发Socratic Models，这是一个模块化框架，其中多个预训练模型可以通过零-shot，即通过多模态提示，进行组合，以便相互交换信息。[6]
- en: Embracing communicative context and intent
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拥抱交流背景和意图
- en: As Anna wants to share not only her thoughts about life, but also the delicious
    coconuts from her island with Maria, she invents a coconut catapult. She sends
    Maria a detailed instruction on how she did it and asks her for instructions to
    optimise it. At the receiving end, the octopus falls short of a meaningful reply.
    Even if he had a way of constructing the catapult underwater, he does not know
    what words such as rope and coconut refer to, and thus can’t physically reproduce
    and improve the experiment. So he simply says “Cool idea, great job! I need to
    go hunting now, bye!”. Anna is bothered by the uncooperative response, but she
    also needs to go on with her daily business and forgets about the incident.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 由于安娜不仅想分享她对生活的想法，还想与玛丽亚分享她岛上的美味椰子，她发明了一个椰子弹射器。她给玛丽亚发送了一份关于如何制作椰子弹射器的详细说明，并请她提供优化建议。然而，接收方的章鱼却没有给出有意义的回应。即使他能在水下构建弹射器，他也不知道像绳子和椰子这样的词指的是什么，因此无法在物理上复制和改进这个实验。他只是简单地说“很棒的主意，干得好！我现在需要去打猎了，再见！”安娜对这个不合作的回应感到困扰，但她也需要继续她的日常工作，忘记了这件事。
- en: When we use language, we do so for a specific purpose, which is our **communicative
    intent**. For example, the communicative intent can be to convey information,
    socialise or ask someone to do something. While the first two are rather straightforward
    for an LLM (as long as it has seen the required information in the data), the
    latter is already more challenging. Let’s forget about the fact that the LLM does
    not have an ability to act in the real world and limit ourselves to tasks in its
    realm of language — writing a speech, an application letter etc. Not only does
    the LLM need to combine and structure the related information in a coherent way,
    but it also needs to set the right emotional tone in terms of soft criteria such
    as formality, creativity, humour etc.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用语言时，我们是为了特定的目的，即我们的**交流意图**。例如，交流意图可以是传递信息、社交或要求别人做某事。前两者对于大型语言模型（LLM）来说比较直接（只要它在数据中看到了所需的信息），而后者则更具挑战性。我们不考虑LLM在现实世界中没有行动能力的事实，而是将自己限制在语言领域内的任务——撰写演讲稿、申请信等。LLM不仅需要以连贯的方式组合和组织相关信息，还需要在形式性、创造力、幽默等软标准方面设定正确的情感基调。
- en: 'Making the transition from classical language generation to recognising and
    responding to specific communicative intents is an important step to achieve better
    acceptance of user-facing NLP systems, especially in Conversational AI. One method
    for this is Reinforcement Learning from Human Feedback (RLHF), which has been
    recently implemented in ChatGPT ([7]) but has a longer history in human preference
    learning.[8] In a nutshell, RLHF “redirects” the learning process of the LLM from
    the straightforward but artificial next-token prediction task towards learning
    human preferences in a given communicative situation. These human preferences
    are directly encoded in the training data: during the annotation process, humans
    are presented with prompts and either write the desired response or rank a series
    of existing responses. The behaviour of the LLM is then optimised to reflect the
    human preference. Technically, RLHF is performed in three steps:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从经典语言生成转向识别和响应特定的传达意图是实现更好用户接受度的重要步骤，特别是在对话 AI 中。一种方法是来自人类反馈的强化学习（RLHF），该方法最近在
    ChatGPT 中实施过（[7]），但在人类偏好学习中已有更长的历史。[8] 简而言之，RLHF “重新引导” LLM 的学习过程，从直接但人为的下一个词预测任务转向学习特定传达情境中的人类偏好。这些人类偏好直接编码在训练数据中：在标注过程中，人们会看到提示并编写期望的回应或对一系列现有回应进行排序。然后优化
    LLM 的行为以反映人类偏好。从技术上讲，RLHF 分为三个步骤：
- en: '**Pre-training and fine-tuning of an initial LLM**: An LLM is trained with
    a classical pre-training objective. Additionally, it can be fine-tuned with human-annotated
    data (as in the case of InstructGPT and ChatGPT).'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**初始 LLM 的预训练和微调**：LLM 先以经典的预训练目标进行训练。此外，它还可以通过人类标注的数据进行微调（例如 InstructGPT 和
    ChatGPT 的情况）。'
- en: '**Reward model training**: The reward model is trained based on human annotations
    that reflect communicative preferences in a given situation. Specifically, humans
    are presented with multiple outputs for a given prompt and rank these according
    to their suitability. The model learns to reward the higher-ranked outputs and
    penalise the lower-ranked outputs. The reward is a single scalar number, which
    makes it compatible with reinforcement learning in the next step.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**奖励模型训练**：奖励模型基于反映特定情境中传达偏好的人工标注数据进行训练。具体来说，人们会看到一个提示的多个输出，并根据其适用性进行排序。模型学习奖励排名较高的输出，并惩罚排名较低的输出。奖励是一个单一的标量数字，这使得它与下一步的强化学习兼容。'
- en: '**Reinforcement Learning**: the policy is the initial LLM, while the reward
    function combines two scores for a given text input:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**强化学习**：策略是初始 LLM，而奖励函数结合了给定文本输入的两个评分：'
- en: The reward model score which ensures that the text **responds to the communicative
    intent**.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励模型评分确保文本**响应传达的意图**。
- en: A penalty for generating texts that are too far away from the initial LLM output
    (e. g. [Kullback-Leibler divergence](https://machinelearningmastery.com/divergence-between-probability-distributions/)),
    making sure that the text is **semantically meaningful**.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对生成与初始 LLM 输出差距过大的文本（例如 [Kullback-Leibler 散度](https://machinelearningmastery.com/divergence-between-probability-distributions/)）施加处罚，确保文本**语义上有意义**。
- en: The LLM is thus fine-tuned to produce **useful outputs that maximise human preferences
    in a given communicative situation**, for example using [Proximal Policy Optimisation](https://arxiv.org/pdf/1707.06347.pdf)
    (PPO).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，LLM 被微调以产生**在给定传达情境中最大化人类偏好的有用输出**，例如使用 [近端策略优化](https://arxiv.org/pdf/1707.06347.pdf)（PPO）。
- en: For a more in-depth introduction into RLHF, please check out the excellent materials
    by Huggingface ([article](https://huggingface.co/blog/rlhf) and [video](https://www.youtube.com/watch?v=2MBJOuVq380)).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 想要深入了解 RLHF，请查看 Huggingface 提供的优秀资料（[文章](https://huggingface.co/blog/rlhf) 和
    [视频](https://www.youtube.com/watch?v=2MBJOuVq380)）。
- en: The RLHF methodology had a mind-blowing success with ChatGPT, especially in
    the areas of conversational AI and creative content creation. In fact, it not
    only leads to more authentic and purposeful conversations, but can also positively
    “bias” the model towards ethical values while mitigating unethical, discriminatory
    or even dangerous outputs. However, what is often left unsaid amidst the excitement
    about RLHF is that, while not introducing significant technological breakthroughs,
    its mega-power comes from linear human annotation effort. RLHF is prohibitively
    expensive in terms of labelled data, the known bottleneck for all supervised and
    reinforcement learning endeavours. Beyond human rankings for LLM outputs, OpenAI’s
    data for ChatGPT also include human-written responses to prompts that are used
    to fine-tune the initial LLM. It is obvious that only big companies committed
    to AI innovation can afford the necessary budget for data labelling at this scale.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF 方法在 ChatGPT 上取得了令人震惊的成功，特别是在对话 AI 和创意内容创作领域。实际上，它不仅能导致更真实和有目的的对话，还能在减轻不道德、歧视性或甚至危险的输出的同时，积极地“偏向”伦理价值。然而，在对
    RLHF 的兴奋中，常常被忽视的是，尽管没有引入显著的技术突破，其巨大的力量来自于线性的人工注释工作。RLHF 在标注数据方面的成本非常高，这是所有监督和强化学习努力的已知瓶颈。除了对
    LLM 输出的人工排名外，OpenAI 的 ChatGPT 数据还包括人类撰写的对提示的回应，这些回应用于微调初始 LLM。显然，只有致力于 AI 创新的大公司才能承担如此规模的数据标注预算。
- en: With the help of a brainy community, most bottlenecks eventually get solved.
    In the past, the Deep Learning community solved the data shortage with self-supervision
    — pre-training LLMs using next-token prediction, a learning signal that is available
    “for free” since it is inherent to any text. The Reinforcement Learning community
    is using algorithms such as Variational Autoencoders or Generative Adversarial
    Networks to generate synthetic data — with varying degrees of success. To make
    RLHF broadly accessible, we will also need to figure out a way to crowdsource
    communicative reward data and/or to build it in a self-supervised or automated
    way. One possibility is to use ranking datasets that are available “in the wild”,
    for example Reddit or Stackoverflow conversations where answers to questions are
    rated by users. Beyond simple ratings and thumbs up/down labels, some conversational
    AI systems also allow the user to directly edit the response to demonstrate the
    desired behaviour, which creates a more differentiated learning signal.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在聪明的社区的帮助下，大多数瓶颈最终都会得到解决。过去，深度学习社区通过自我监督解决了数据短缺的问题——使用下一个词预测来预训练 LLM，这是一种“免费”获得的学习信号，因为它是任何文本固有的。强化学习社区正在使用如变分自编码器或生成对抗网络等算法生成合成数据——成功程度各不相同。为了使
    RLHF 广泛可用，我们还需要找到一种众包交流奖励数据和/或以自我监督或自动化方式构建它的方法。一种可能性是使用“野外”中可用的排名数据集，例如 Reddit
    或 Stackoverflow 对话，其中用户对问题的回答进行评分。除了简单的评分和点赞/点踩标签外，一些对话 AI 系统还允许用户直接编辑回应以展示期望的行为，从而创造出更细致的学习信号。
- en: Modelling reasoning processes
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建模推理过程
- en: Finally, Anna faces an emergency. She is pursued by an angry bear. In a panic,
    she grabs a couple of metal sticks and asks Maria to tell her how defend herself.
    Of course, the octopus has no clue what Anna means. Not only has he never faced
    a bear — he also doesn’t know how to behave in a bear attack and how the sticks
    can help Anna. Solving a task like this not only requires the ability to map accurately
    between words and objects in the real world, but also to reason about how these
    objects can be leveraged. The octopus miserably fails and Anna discovers the delusion
    in the lethal encounter.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，安娜遇到了紧急情况。她被一只愤怒的熊追赶。在惊慌中，她抓起几根金属棒，并请玛利亚告诉她如何自卫。当然，章鱼完全不明白安娜的意思。它不仅从未面对过熊——它也不知道在熊攻击中应该如何行为以及这些棒子如何帮助安娜。解决这样一个任务不仅需要能够准确地将词汇与现实世界中的物体对应起来，还需要推理这些物体如何被利用。章鱼惨遭失败，安娜在致命的遭遇中发现了这种错觉。
- en: 'Now, what if Maria was still there? Most humans can reason logically, even
    if there are huge individual differences in the mastery of this skill. Using reasoning,
    Maria could solve the task as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果玛利亚还在这里会怎样？大多数人能够进行逻辑推理，即使在掌握这项技能方面存在巨大个体差异。利用推理，玛利亚可以如下解决任务：
- en: '*Premise 1 (based on situation)*: Anna has a couple of metal sticks.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*前提1（基于情境）*：安娜有几根金属棒。'
- en: '*Premise 2 (based on common-sense knowledge)*: Bears are intimidated by noise.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*前提2（基于常识知识）*：熊对噪音感到害怕。'
- en: '*Conclusion*: Anna can try and use her sticks to make noise and scare the bear
    away.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*结论*：安娜可以尝试用她的棍子制造噪音来吓跑熊。'
- en: 'LLMs often produce outputs with a valid reasoning chain. Yet, on closer inspection,
    most of this coherence is the result of pattern learning rather than a deliberate
    and novel combination of facts. DeepMind has been on a quest to solve causality
    for years, and a recent attempt is the **faithful reasoning framework for question
    answering**.[9] The architecture consists of two LLMs — one for the selection
    of relevant premises and another for inferring the final, conclusive answer to
    the question. When prompted with a question and its context, the selection LLM
    first picks the related statements from its data corpus and passes them to the
    inference LLM. The inference LLM deduces new statements and adds them to the context.
    This iterative reasoning process comes to an end when all statements line up into
    a coherent reasoning chain that provides a complete answer to the question:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 经常生成具有有效推理链的输出。然而，经过仔细检查，大多数这种连贯性是模式学习的结果，而不是有意且新颖的事实组合。DeepMind 多年来一直在寻求解决因果关系，最近的一次尝试是**忠实推理框架**。[9]
    该架构由两个 LLMs 组成——一个用于选择相关前提，另一个用于推断问题的最终结论性答案。当收到一个问题及其上下文时，选择 LLM 首先从数据语料库中挑选相关陈述，并将其传递给推理
    LLM。推理 LLM 推导出新的陈述并将其添加到上下文中。当所有陈述排成一个连贯的推理链，提供一个完整的答案时，这一迭代推理过程就结束了：
- en: '![](../Images/c74ca4eb7ede6229573406d0fddb4f39.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c74ca4eb7ede6229573406d0fddb4f39.png)'
- en: '*Figure 4: Faithful reasoning components*'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4：忠实推理组件*'
- en: 'The following shows the reasoning chain for our island incident:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以下展示了我们岛屿事件的推理链：
- en: '![](../Images/b34ddaf7d392221669fe823fb8415b7f.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b34ddaf7d392221669fe823fb8415b7f.png)'
- en: '*Figure 5: Constructing a reasoning trace*'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5：构建推理痕迹*'
- en: Another method to perform reasoning with LLMs is chain-of-thought prompting.
    Here, the user first provides one or more examples of the reasoning process as
    part of the prompt, and the LLM “imitates” this reasoning process with new inputs.[13]
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LLMs 进行推理的另一种方法是链式思维提示。在这里，用户首先提供一个或多个推理过程的示例作为提示的一部分，然后 LLM “模仿”这个推理过程来处理新的输入。[13]
- en: Beyond this general ability to reason logically, humans also access a whole
    toolbox of more specific reasoning skills. A classical example is mathematical
    calculation. LLMs can produce these calculations up to a certain level — for example,
    modern LLMs can confidently perform 2- or 3-digit addition. However, they start
    to fail systematically when complexity increases, for example when more digits
    are added or multiple operations need to be performed to solve a mathematical
    task. And “verbal” tasks formulated in natural language (for example, “*I had
    10 mangoes and lost 3\. How many mangoes do I have left?*”) are much more challenging
    than explicit computations (”*ten minus three equals…*”). While LLM performance
    can be improved by increasing training time, training data, and parameter sizes,
    using a simple calculator will still remain the more reliable alternative.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这种一般性的逻辑推理能力，人类还掌握了一整套更具体的推理技能。一个经典的例子是数学计算。LLMs 可以进行一定程度的计算——例如，现代 LLMs 可以自信地进行
    2 位或 3 位的加法。然而，当复杂度增加时，例如添加更多数字或需要执行多个操作来解决数学任务，它们开始系统性地失败。而用自然语言提出的“*我有 10 个芒果，丢了
    3 个。我还剩下多少个芒果？*”这类“语言”任务比明确的计算任务（“*十减三等于…*”）要困难得多。虽然通过增加训练时间、训练数据和参数规模可以提高 LLM
    的性能，但使用简单计算器仍然是更可靠的选择。
- en: Just as children who explicitly learn the laws of mathematics and other exact
    sciences, LLMs can also benefit from hard-coded rules. This sounds like a case
    for neuro-symbolic AI — and indeed, modular systems like MRKL (pronounced “miracle”)
    by AI21 Labs split the workload of understanding the task, executing the computation
    and formulating the output result between different models.[12] MRKL stands for
    Modular Reasoning, Knowledge and Language and combines AI modules in a pragmatic
    plug-and-play fashion, switching back and forth between structured knowledge,
    symbolic methods and neural models. Coming back to our example, to perform mathematical
    calculations, an LLM is first fine-tuned to extract the formal arguments from
    a verbal arithmetic task (numbers, operands, parentheses). The calculation itself
    is then “routed” to a deterministic mathematical module, and the final the result
    is formatted in natural language using the output LLM.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 就像明确学习数学和其他精确科学定律的孩子一样，LLM也可以从硬编码规则中受益。这听起来像是神经符号AI的一个案例——实际上，像AI21 Labs的MRKL（发音为“奇迹”）这样的模块化系统将理解任务、执行计算和制定输出结果的工作负载分配到不同的模型中。[12]
    MRKL代表模块化推理、知识和语言，它以务实的插拔式方式结合AI模块，在结构化知识、符号方法和神经模型之间来回切换。回到我们的例子中，为了进行数学计算，LLM首先被微调以提取口头算术任务中的正式参数（数字、操作数、括号）。然后，计算本身被“路由”到确定性的数学模块，最终结果使用输出LLM以自然语言格式呈现。
- en: As opposed to black-box, monolithic LLMs, reasoning add-ons create transparency
    and trust since they decompose the “thinking” process into individual steps. They
    are particularly useful for supporting complex, multi-step decision and action
    paths. For example, they can be used by virtual assistants that make data-driven
    recommendations and need to perform multiple steps of analytics and aggregation
    to get to a conclusion.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 与黑箱、单一的LLM不同，推理附加组件通过将“思考”过程分解为单独的步骤来创造透明度和信任。它们尤其适用于支持复杂的多步骤决策和行动路径。例如，它们可以被虚拟助手使用，这些助手基于数据驱动的推荐并需要执行多个分析和聚合步骤以得出结论。
- en: Conclusion and take-aways
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论和要点
- en: 'In this article, we have provided an overview of approaches to complement the
    intelligence of LLMs. Let’s summarise our guidelines for maximising the benefits
    of LLMs and potential enhancements:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提供了补充LLM智能的各种方法的概述。让我们总结一下最大化LLM益处和潜在增强的指导方针：
- en: '**Make them fail**: Don’t be fooled by initial results — language models can
    produce impressive outputs when you start working with them and anyway, we humans
    have a bias to attribute too much intelligence to machines. Jump into the role
    of a mean, adversarial user, stress-test your models and explore their weak points.
    Do this early on, before too much skin has been put in the game.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**让它们失败**：不要被初步结果迷惑——当你开始使用语言模型时，它们可以生成令人印象深刻的输出，然而，我们人类倾向于将过多的智能归因于机器。充当一个刻薄的对手用户，对你的模型进行压力测试，探索其弱点。在投入过多精力之前，尽早进行这一过程。'
- en: '**Evaluation and dedicated probing**: The design of your training task and
    evaluation procedure are of central importance. As much as possible, it should
    reflect the context of natural language use. Knowing the pitfalls of LLMs, dedicate
    your evaluation to them.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估和专门探测**：培训任务和评估程序的设计至关重要。尽可能地，它应反映自然语言使用的背景。了解LLM的陷阱，专注于对它们的评估。'
- en: '**Benefit from neuro-symbolic AI:** Symbolic AI is not out — in the context
    of an individual business or product, setting some of your domain knowledge in
    stone can be an efficient approach to increase precision. It allows you to control
    the behaviour of the LLM where it is crucial for your business, while still unfolding
    its power at generating language based on wide external knowledge.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**利用神经符号AI的优势**：符号AI并没有过时——在个别业务或产品的背景下，将部分领域知识固定下来可以是提高精准度的有效方法。它允许你在对业务至关重要的地方控制LLM的行为，同时仍能发挥其基于广泛外部知识生成语言的强大能力。'
- en: '**Strive towards a flexible architecture**: On the surface, LLMs sometimes
    feel like blackboxes. However, as we have seen, numerous approaches exist — and
    will become available in the future — not only for fine-tuning, but also for “tweaking”
    their internal behaviour and learning. Use to open-source models and solutions
    if you have the technical capability — this will allow you to adapt and maximise
    the value-add of LLMs in your product.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**朝着灵活的架构努力**：表面上，LLM 有时感觉像黑箱。然而，正如我们所见，存在许多方法——并且未来还会出现——不仅用于微调，还有用于“调整”其内部行为和学习。如果你具备技术能力，可以使用开源模型和解决方案——这将使你能够在你的产品中适应并最大化
    LLM 的价值。'
- en: And even with the described enhancements, LLMs stay far behind human understanding
    and language use — they simply lack the unique, powerful and mysterious synergy
    of cultural knowledge, intuition and experience that humans build up as they go
    through their lifes. According to Yann LeCun, “it is clear that these models are
    doomed to a shallow understanding that will never approximate the full-bodied
    thinking we see in humans.”[11] When using AI, it is important to appreciate the
    wonders and complexity we find in language and cognition. Looking at smart machines
    from the right distance, we can differentiate between tasks that can be delegated
    to them and those that will remain the privilege of humans in the foreseeable
    future.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 即使有上述增强，LLM 仍远远落后于人类的理解和语言使用——它们缺乏人类在生活中积累的独特、强大且神秘的文化知识、直觉和经验的协同作用。根据 Yann
    LeCun 的说法，“这些模型注定只能获得肤浅的理解，永远无法接近我们在人类思维中看到的全面思考。”[11] 使用 AI 时，重要的是欣赏我们在语言和认知中发现的奇迹和复杂性。以正确的距离看待智能机器，我们可以区分可以委托给它们的任务和那些在可预见的未来将仍然是人类特权的任务。
- en: References
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Emily M. Bender and Alexander Koller. 2020\. [Climbing towards NLU: On
    Meaning, Form, and Understanding in the Age of Data](https://aclanthology.org/2020.acl-main.463).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 5185–5198, Online. Association for Computational Linguistics.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Emily M. Bender 和 Alexander Koller. 2020\. [迈向自然语言理解：在数据时代的意义、形式与理解](https://aclanthology.org/2020.acl-main.463)。载于
    *第58届计算语言学协会年会论文集*，第5185–5198页，在线。计算语言学协会。'
- en: '[2] Emelin, Denis & Bonadiman, Daniele & Alqahtani, Sawsan & Zhang, Yi & Mansour,
    Saab. (2022). Injecting Domain Knowledge in Language Models for Task-Oriented
    Dialogue Systems. 10.48550/arXiv.2212.08120.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Emelin, Denis & Bonadiman, Daniele & Alqahtani, Sawsan & Zhang, Yi & Mansour,
    Saab. (2022). 向任务导向对话系统的语言模型注入领域知识。10.48550/arXiv.2212.08120。'
- en: '[3] Fedor Moiseev et al. 2022\. [SKILL: Structured Knowledge Infusion for Large
    Language Models](https://aclanthology.org/2022.naacl-main.113). In *Proceedings
    of the 2022 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, pages 1581–1588, Seattle, United States.
    Association for Computational Linguistics.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Fedor Moiseev 等. 2022\. [SKILL：大型语言模型的结构化知识注入](https://aclanthology.org/2022.naacl-main.113)。载于
    *2022年北美计算语言学协会年会：人类语言技术会议论文集*，第1581–1588页，美国西雅图。计算语言学协会。'
- en: '[4] Ryan Daws. 2020\. [Medical chatbot using OpenAI’s GPT-3 told a fake patient
    to kill themselves](https://www.artificialintelligence-news.com/2020/10/28/medical-chatbot-openai-gpt3-patient-kill-themselves/).
    Retrieved on January 13, 2022.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Ryan Daws. 2020\. [使用 OpenAI 的 GPT-3 的医疗聊天机器人告诉虚假的病人自杀](https://www.artificialintelligence-news.com/2020/10/28/medical-chatbot-openai-gpt3-patient-kill-themselves/)。检索日期：2022年1月13日。'
- en: '[5] DeepMind. 2022\. [Tackling multiple tasks with a single visual language
    model](https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model).
    Retrieved on January 13, 2022.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] DeepMind. 2022\. [用单一视觉语言模型应对多重任务](https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model)。检索日期：2022年1月13日。'
- en: '[6] Zeng et al. 2022\. [Socratic Models: Composing Zero-Shot Multimodal Reasoning
    with Language](https://arxiv.org/pdf/2204.00598.pdf). Preprint.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Zeng 等. 2022\. [苏格拉底模型：用语言进行零样本多模态推理](https://arxiv.org/pdf/2204.00598.pdf)。预印本。'
- en: '[7] OpenAI. 2022\. [ChatGPT: Optimizing Language Models for Dialogue](https://openai.com/blog/chatgpt/).
    Retrieved on January 13, 2022.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] OpenAI. 2022\. [ChatGPT：优化对话的语言模型](https://openai.com/blog/chatgpt/)。检索日期：2022年1月13日。'
- en: '[8] Christiano et al. 2017\. Deep reinforcement learning from human preferences.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Christiano 等. 2017\. 从人类偏好中进行深度强化学习。'
- en: '[9] Creswell & Shanahan. 2022\. [Faithful Reasoning Using Large Language Models](https://arxiv.org/pdf/2208.14271.pdf).
    DeepMind.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Creswell 和 Shanahan 2022\. [使用大型语言模型进行忠实推理](https://arxiv.org/pdf/2208.14271.pdf)。DeepMind。'
- en: '[10] Karpas et al. 2022\. [MRKL Systems: A modular, neuro-symbolic architecture
    that combines large language models, external knowledge sources and discrete reasoning](https://arxiv.org/pdf/2205.00445.pdf).
    AI21 Labs.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Karpas 等人 2022\. [MRKL 系统：一种模块化的神经符号架构，结合了大型语言模型、外部知识源和离散推理](https://arxiv.org/pdf/2205.00445.pdf)。AI21
    Labs。'
- en: '[11] Jacob Browning & Yann LeCun. 2022\. [AI And The Limits Of Language](https://www.noemamag.com/ai-and-the-limits-of-language/).
    Retrieved on January 13, 2022.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Jacob Browning 和 Yann LeCun 2022\. [人工智能与语言的极限](https://www.noemamag.com/ai-and-the-limits-of-language/)。取自
    2022 年 1 月 13 日。'
- en: '[12] Karpas et al. 2022\. [MRKL Systems — A modular, neuro-symbolic architecture
    that combines large language models, external knowledge sources and discrete reasoning](https://arxiv.org/pdf/2205.00445.pdf).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Karpas 等人 2022\. [MRKL 系统——一种模块化的神经符号架构，结合了大型语言模型、外部知识源和离散推理](https://arxiv.org/pdf/2205.00445.pdf)。'
- en: '[13] Wei et al. 2022\. [Chain-of-Thought Prompting Elicits Reasoning in Large
    Language Models](https://arxiv.org/pdf/2201.11903.pdf). In Proceedings of NeurIPS
    2022.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Wei 等人 2022\. [链式思考提示引发大型语言模型的推理](https://arxiv.org/pdf/2201.11903.pdf)。发表于
    NeurIPS 2022 会议。'
- en: All images unless otherwise noted are by the author.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图片均由作者提供。
