- en: 'Unleash the Hidden Patterns: A Guide to Unsupervised Machine Learning for Article
    Recommender System'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/unleash-the-hidden-patterns-a-guide-to-unsupervised-machine-learning-for-article-recommender-d1e7eb219c8d?source=collection_archive---------12-----------------------#2023-03-07](https://towardsdatascience.com/unleash-the-hidden-patterns-a-guide-to-unsupervised-machine-learning-for-article-recommender-d1e7eb219c8d?source=collection_archive---------12-----------------------#2023-03-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Build an article recommender system with unsupervised machine learning and generate
    features and patterns that aid in the recommendation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://suhas-maddali007.medium.com/?source=post_page-----d1e7eb219c8d--------------------------------)[![Suhas
    Maddali](../Images/933f27eab8ba9ee1f06ed2f24746d788.png)](https://suhas-maddali007.medium.com/?source=post_page-----d1e7eb219c8d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d1e7eb219c8d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d1e7eb219c8d--------------------------------)
    [Suhas Maddali](https://suhas-maddali007.medium.com/?source=post_page-----d1e7eb219c8d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2a74f90399ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funleash-the-hidden-patterns-a-guide-to-unsupervised-machine-learning-for-article-recommender-d1e7eb219c8d&user=Suhas+Maddali&userId=2a74f90399ae&source=post_page-2a74f90399ae----d1e7eb219c8d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d1e7eb219c8d--------------------------------)
    ·8 min read·Mar 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd1e7eb219c8d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funleash-the-hidden-patterns-a-guide-to-unsupervised-machine-learning-for-article-recommender-d1e7eb219c8d&user=Suhas+Maddali&userId=2a74f90399ae&source=-----d1e7eb219c8d---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd1e7eb219c8d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funleash-the-hidden-patterns-a-guide-to-unsupervised-machine-learning-for-article-recommender-d1e7eb219c8d&source=-----d1e7eb219c8d---------------------bookmark_footer-----------)![](../Images/ad11778233ec9ecd731942319dd78857.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Salomé Watel](https://unsplash.com/@samefaisrire?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: There have been a lot of talks lately about the incredible capabilities of **artificial
    intelligence** and **machine learning**. As we go on to see various frontiers
    at which ML could be applied, there is a growing possibility of higher value generated
    as a result of this transition. Companies such as **Google**, **Microsoft**, and
    **NVIDIA** are pushing the boundaries at which AI and machine learning are used
    to build a great society with technological advancements.
  prefs: []
  type: TYPE_NORMAL
- en: Now that there is a lot of **hype** in this AI, an often less talked about topic
    is that of unsupervised machine learning. I bet that people might have had the
    opportunity to enjoy some of the greatest streaming services such as **Netflix**
    and **Amazon Videos** with their state-of-the-art recommendation systems. However,
    there has been less talk about the incredible capabilities of recommendation system
    which is also part of the unsupervised machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will mainly focus on building an article recommender system
    based on the previously read articles by the user, giving them a positive experience
    and an inclination to read more of these similar types of articles. This comes
    under unsupervised machine learning as we do not have **labels** about whether
    a particular article is related to others or not. Instead, we are only given **textual
    information** based on which, we must highlight and visualize the similarities
    before recommending items to users.
  prefs: []
  type: TYPE_NORMAL
- en: Initially, we start with the data and understand their types before continuing
    our effort to recommend articles. After going through the data, we will visualize
    key insights from the data. Finally, we build a recommender system based on **cosine
    similarity** scores between an article and a list of all the articles to get a
    recommendation.
  prefs: []
  type: TYPE_NORMAL
- en: Reading the Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step would be to read the data. Below is the code for reading the
    data. After reading the data, we would be performing exploratory data analysis.
    Here is the code and the representation of data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** The dataset was taken from [Articles sharing and reading from CI&T
    DeskDrop | Kaggle](https://www.kaggle.com/datasets/gspmoreira/articles-sharing-reading-from-cit-deskdrop?select=shared_articles.csv)
    under [Database Contents License (DbCL) v1.0 — Open Data Commons: legal tools
    for open data](https://opendatacommons.org/licenses/dbcl/1-0/) license'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/357c5fa254723584339801ef66c9b52f.png)'
  prefs: []
  type: TYPE_IMG
- en: Input Data (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: There are title, text, and language features that are most important in our
    data. Other features such as content type and author user agent are not very suitable
    for this task of building a recommendation system. Steps would be taken to **remove**
    such features and only consider the features such as title, text, and language
    respectively. Note that not all the features are shown in the figure as this would
    become a large image.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/51fe2588cb6962002f249a003357111b.png)'
  prefs: []
  type: TYPE_IMG
- en: Data Information (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: We get the information about the data and determine the number of **non-null**
    values present in the dataset. It is seen that there are features such as authorSessionID
    and authorUserAgent that contain more than **50 percent** missing values. We are
    going to be removing all of these features along with others as discussed above
    and would be focusing only on the title, text, and language features.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exploratory Data Analysis (EDA)**'
  prefs: []
  type: TYPE_NORMAL
- en: Let us now explore the dataset and get an understanding of it by using visuals
    and graphs. The first step would be to explore the type of text content present
    in the data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b237e493067952b2625ea4b637cc48bc.png)'
  prefs: []
  type: TYPE_IMG
- en: Countplot of content types (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Observing from the plot, a large portion of the contents are in the form of
    **HTML links** to be used for recommendation. These HTML links contain the actual
    text data and the titles. Our focus here is to extract meaningful insights from
    text instead of using other formats such as ‘video’ and ‘rich’ formats. Therefore,
    we can remove these categories from our project.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/59d222818e2d296342ecb3ff074f885c.png)'
  prefs: []
  type: TYPE_IMG
- en: Countplot of languages (Image of Author)
  prefs: []
  type: TYPE_NORMAL
- en: A large portion of our text and titles are English titles. Since our focus is
    only on the English language, we can take the steps to remove other categories
    to improve our recommendation model efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d3ee51376c882208a425f67167ff63ab.png)'
  prefs: []
  type: TYPE_IMG
- en: Title Wordcloud (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Wordcloud gives a good understanding of the most frequently occurring words
    in our corpus of text. The higher the **occurrence** of words in a corpus, the
    **bigger** the text sizes in a word cloud plot. As seen from the plot, word titles
    having terms such as ‘Google’, ‘Machine Learning’, and ‘Apple’ are often used
    in our title space.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/099d57db38d7844dbaf9fa6604a82fef.png)'
  prefs: []
  type: TYPE_IMG
- en: Text Wordcloud (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Now that we explored the title and found the most commonly occurring words,
    it is time to explore the **text** itself in our list of articles to find interesting
    trends and patterns. A large proportion of articles contain words such as ‘data’,
    ‘user’, and ‘time’.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1134156961b15899686a8f994dcd2f12.png)'
  prefs: []
  type: TYPE_IMG
- en: Percentage of Variance explained (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Converting the given set of titles into a **TFIDF** format and applying **principal
    component analysis**, the above plot showcases the cumulative percentage of variance
    explained by each of the components. It is to be noted that with the increase
    in the number of components, there is an increase in the variance explained as
    more information from components leads to more explanation and information.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3ad5a2a5a74e670bb48fa69f0861f8a5.png)'
  prefs: []
  type: TYPE_IMG
- en: Percentage of Variance explained (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: This plot shows the variance explained by the tfidf components on the entire
    text instead of only considering the title. This plot is quite different from
    the previous plot in that only a few sets of principal components are able to
    explain a large portion of the variance in the data. Therefore, this can help
    in dimensionality reduction as fewer features are able to explain a large portion
    of the variance.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c0c53704dd1dce352c0b037c23a1df32.png)'
  prefs: []
  type: TYPE_IMG
- en: K-means clustering plot (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Clustering is a technique where data points that are similar are grouped together
    to find interesting patterns and commonalities among them. In this way, recommendations
    are given based on the cluster at which a data point is present.
  prefs: []
  type: TYPE_NORMAL
- en: In order to determine the right number of clusters, it is handy to use the **k-means
    clustering model** and follow the **elbow method** to find the optimum number
    of clusters. In our case, the best value of **k is 11** as this follows the shape
    of an elbow.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6834bec07faa8e4bc45bcbd4c8463084.png)'
  prefs: []
  type: TYPE_IMG
- en: PCA with clustering 2D plot (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: After performing PCA and clustering based on the optimum number of clusters,
    it is time to visualize the results of the clustering. Based on the plot above,
    the clustering is working quite well as there is a pattern found in the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/519d128c07d150757e9e564f1fc2e106.png)'
  prefs: []
  type: TYPE_IMG
- en: PCA with clustering 3D plot (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s determine how the plot looks in **3D** to find the underlying patterns.
    As can be seen, there is a lot of room for the separation of clusters, hence guiding
    our recommender system to give good suggestions to the user based on the previously
    read text.
  prefs: []
  type: TYPE_NORMAL
- en: There are other dimensionality reduction techniques such as **TSNE** and **Kernal
    PCA**. Going through each of them to determine the best clustering would be handy
    for a recommender system. Steps must be taken to visualize the data points from
    the text and generate interesting patterns.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dd356cc21d74bc6c1a684b5f869e9e59.png)'
  prefs: []
  type: TYPE_IMG
- en: TSNE with clustering 2D plot (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Reducing the dimensions using TSNE and visualizing the representations with
    11 clusters, it is seen that the data points are spread out quite well. Therefore,
    there are lower chances for articles that are similar to be in the same cluster.
    As a result, PCA was performing well in terms of clustering and determining the
    optimum number of clusters. We will also use 3D visualizations to guide our thinking
    and understanding of clustering mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cea23c0ebc383f52dd08393f6aed5cd1.png)'
  prefs: []
  type: TYPE_IMG
- en: TSNE with clustering 3D plot (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: After performing the task of clustering, it is seen that a lot of data embeddings
    of the text are quite **close** to each other. Therefore, it can be challenging
    to accurately perform clustering when the points are not spread in various directions.
    Hence, we might look for alternative clustering approaches and dimensionality
    reduction techniques.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/66066aa94a40745751f618b8b3e3a95b.png)'
  prefs: []
  type: TYPE_IMG
- en: Kernel PCA with clustering 2D Plot (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Kernel PCA is another popular method that is used for dimensionality reduction.
    As could be seen, the data representations are spread out well for determining
    and using the number of clusters. Overall, the algorithm reduces the dimensions
    well and separated data points well. Let us also go over the **3d representation**
    of clusters as a result of performing dimensionality reduction with this technique.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/04c47e153c7e68f581fd4b107379e71d.png)'
  prefs: []
  type: TYPE_IMG
- en: Kernel PCA with clustering 3D Plot (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: After plotting a 3d generated representation with kernel PCA, clustering took
    place quite well and the points are quite spread out. Therefore, this approach
    can help generate recommendations based on the clustering approach.
  prefs: []
  type: TYPE_NORMAL
- en: Say, a user visits a **particular website** and reads an interesting piece of
    text. After this step, the items that are present in the cluster based on kernel
    PCA representation would be recommended to the user. The user can find these articles
    fascinating, resulting in the growth of the business.
  prefs: []
  type: TYPE_NORMAL
- en: After performing the previous steps, we define a function that generates a list
    of **useful features** that could be used by various recommender system models
    to make recommendations. These features used in the function below are important
    and give a good representation of the type of text along with its content and
    readability.
  prefs: []
  type: TYPE_NORMAL
- en: We apply this function to our data frame and generate a new set of features
    that are used by recommender systems. Finally, **cosine similarity** is taken
    into account when determining the distance between the text of interest with the
    list of all the other possible texts and articles.
  prefs: []
  type: TYPE_NORMAL
- en: After getting the cosine difference between the present text based on the features
    generated and a kernel PCA representation, it is compared with the list of existing
    articles to determine the ones with the lowest distance from clusters. As a result,
    those articles are recommended to users which makes them engaging and fun to read.
  prefs: []
  type: TYPE_NORMAL
- en: We import various useful libraries for measuring the cosine similarities and
    generating recommendations for articles that are mostly similar in structure and
    content.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After going through this exhaustive article, you should be getting a good idea
    about the working details of a particular implementation of **recommendation systems**
    on articles. Performing dimensionality reduction can ensure that we would reduce
    **compute resources** and also reduce the **impact of outliers** when making predictions
    or providing recommendations. Thanks for taking the time to read this article.
  prefs: []
  type: TYPE_NORMAL
- en: '*Below are the ways where you could contact me or take a look at my work.*'
  prefs: []
  type: TYPE_NORMAL
- en: '***GitHub:***[*suhasmaddali (Suhas Maddali ) (github.com)*](https://github.com/suhasmaddali)'
  prefs: []
  type: TYPE_NORMAL
- en: '***YouTube:***[*https://www.youtube.com/channel/UCymdyoyJBC_i7QVfbrIs-4Q*](https://www.youtube.com/channel/UCymdyoyJBC_i7QVfbrIs-4Q)'
  prefs: []
  type: TYPE_NORMAL
- en: '***LinkedIn:***[*(1) Suhas Maddali, Northeastern University, Data Science |
    LinkedIn*](https://www.linkedin.com/in/suhas-maddali/)'
  prefs: []
  type: TYPE_NORMAL
- en: '***Medium:*** [*Suhas Maddali — Medium*](https://suhas-maddali007.medium.com/)'
  prefs: []
  type: TYPE_NORMAL
- en: '***Kaggle:***[*Suhas Maddali | Contributor | Kaggle*](https://www.kaggle.com/suhasmaddali007)'
  prefs: []
  type: TYPE_NORMAL
