- en: 'Efficient Image Segmentation Using PyTorch: Part 2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyTorch进行高效图像分割：第二部分
- en: 原文：[https://towardsdatascience.com/efficient-image-segmentation-using-pytorch-part-2-bed68cadd7c7?source=collection_archive---------4-----------------------#2023-06-27](https://towardsdatascience.com/efficient-image-segmentation-using-pytorch-part-2-bed68cadd7c7?source=collection_archive---------4-----------------------#2023-06-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/efficient-image-segmentation-using-pytorch-part-2-bed68cadd7c7?source=collection_archive---------4-----------------------#2023-06-27](https://towardsdatascience.com/efficient-image-segmentation-using-pytorch-part-2-bed68cadd7c7?source=collection_archive---------4-----------------------#2023-06-27)
- en: A CNN-based model
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于CNN的模型
- en: '[](https://medium.com/@dhruvbird?source=post_page-----bed68cadd7c7--------------------------------)[![Dhruv
    Matani](../Images/d63bf7776c28a29c02b985b1f64abdd3.png)](https://medium.com/@dhruvbird?source=post_page-----bed68cadd7c7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bed68cadd7c7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bed68cadd7c7--------------------------------)
    [Dhruv Matani](https://medium.com/@dhruvbird?source=post_page-----bed68cadd7c7--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@dhruvbird?source=post_page-----bed68cadd7c7--------------------------------)[![Dhruv
    Matani](../Images/d63bf7776c28a29c02b985b1f64abdd3.png)](https://medium.com/@dhruvbird?source=post_page-----bed68cadd7c7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bed68cadd7c7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bed68cadd7c7--------------------------------)
    [Dhruv Matani](https://medium.com/@dhruvbird?source=post_page-----bed68cadd7c7--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F63f5d5495279&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficient-image-segmentation-using-pytorch-part-2-bed68cadd7c7&user=Dhruv+Matani&userId=63f5d5495279&source=post_page-63f5d5495279----bed68cadd7c7---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bed68cadd7c7--------------------------------)
    ·11 min read·Jun 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbed68cadd7c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficient-image-segmentation-using-pytorch-part-2-bed68cadd7c7&user=Dhruv+Matani&userId=63f5d5495279&source=-----bed68cadd7c7---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F63f5d5495279&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficient-image-segmentation-using-pytorch-part-2-bed68cadd7c7&user=Dhruv+Matani&userId=63f5d5495279&source=post_page-63f5d5495279----bed68cadd7c7---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bed68cadd7c7--------------------------------)
    ·11分钟阅读·2023年6月27日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbed68cadd7c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficient-image-segmentation-using-pytorch-part-2-bed68cadd7c7&user=Dhruv+Matani&userId=63f5d5495279&source=-----bed68cadd7c7---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbed68cadd7c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficient-image-segmentation-using-pytorch-part-2-bed68cadd7c7&source=-----bed68cadd7c7---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbed68cadd7c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficient-image-segmentation-using-pytorch-part-2-bed68cadd7c7&source=-----bed68cadd7c7---------------------bookmark_footer-----------)'
- en: This is the second part of the 4-part series to implement image segmentation
    step by step from scratch using deep learning techniques in PyTorch. This part
    will focus on implementing a baseline image segmentation Convolutional Neural
    Network (CNN) model.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个4部分系列中的第二部分，旨在使用PyTorch中的深度学习技术一步步实现图像分割。本部分将重点介绍实现一个基线图像分割卷积神经网络（CNN）模型。
- en: Co-authored with [Naresh Singh](https://medium.com/@brocolishbroxoli)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 与[Naresh Singh](https://medium.com/@brocolishbroxoli)共同撰写
- en: '![](../Images/71d6ffa1c9d4d8f16cc63e2c2f47ce90.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/71d6ffa1c9d4d8f16cc63e2c2f47ce90.png)'
- en: 'Figure 1: Result of running image segmentation using a CNN. In order from top
    to bottom, input images, ground truth segmentation masks, predicted segmentation
    masks. Source: Author(s)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：使用CNN进行图像分割的结果。从上到下依次为输入图像、真实分割掩码、预测的分割掩码。来源：作者
- en: Article outline
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文章大纲
- en: In this article, we will implement a [Convolutional Neural Network (CNN)](https://en.wikipedia.org/wiki/Convolutional_neural_network)
    based architecture called [SegNet](https://arxiv.org/abs/1511.00561) which will
    assign each pixel in an input image to a corresponding pet such as a cat or a
    dog. The pixels which don’t belong to any pet will be classified as background
    pixels. We will build and train this model on the [Oxford Pets dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/)
    using [PyTorch](https://pytorch.org/) to develop a sense of what it takes to deliver
    a successful image segmentation task. The model building process will be hands-on
    where we will discuss in detail the role of each layer in our model. The article
    will contain plenty of references to research papers and articles for further
    learning.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将实现一种基于[卷积神经网络（CNN）](https://en.wikipedia.org/wiki/Convolutional_neural_network)的架构，称为[SegNet](https://arxiv.org/abs/1511.00561)，它将为输入图像中的每个像素分配相应的宠物标签，例如猫或狗。那些不属于任何宠物的像素将被归类为背景像素。我们将使用[Oxford
    Pets 数据集](https://www.robots.ox.ac.uk/~vgg/data/pets/)和[PyTorch](https://pytorch.org/)来构建和训练此模型，以便深入了解成功完成图像分割任务所需的内容。模型构建过程将是动手操作的，我们将详细讨论模型中每一层的作用。文章中将包含大量研究论文和文章的参考资料以供进一步学习。
- en: Throughout this article, we will reference the code and results from this [notebook](https://github.com/dhruvbird/ml-notebooks/blob/main/pets_segmentation/oxford-iiit-pets-segmentation-using-pytorch-segnet-and-depth-wise-separable-convs.ipynb).
    If you wish to reproduce the results, you’ll need a GPU to ensure that the notebook
    completes running in a reasonable amount of time.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将参考来自这个[笔记本](https://github.com/dhruvbird/ml-notebooks/blob/main/pets_segmentation/oxford-iiit-pets-segmentation-using-pytorch-segnet-and-depth-wise-separable-convs.ipynb)的代码和结果。如果你希望复现结果，你需要一个GPU，以确保笔记本在合理的时间内完成运行。
- en: Articles in this series
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本系列文章
- en: 'This series is for readers at all experience levels with deep learning. If
    you want to learn about the practice of deep learning and vision AI along with
    some solid theory and hands-on experience, you’ve come to the right place! This
    is expected to be a 4-part series with the following articles:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本系列适用于所有深度学习经验水平的读者。如果你想了解深度学习和视觉AI的实践，同时获得一些扎实的理论和实践经验，你来对地方了！预计这是一个四部分的系列，包含以下文章：
- en: '[Concepts and Ideas](https://medium.com/p/89e8297a0923/)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[概念和思想](https://medium.com/p/89e8297a0923/)'
- en: '**A CNN-based model (this article)**'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**基于CNN的模型（本文）**'
- en: '[Depthwise separable convolutions](https://medium.com/p/3534cf04fb89/)'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[深度可分离卷积](https://medium.com/p/3534cf04fb89/)'
- en: '[A Vision Transformer-based model](https://medium.com/p/6c86da083432/)'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[基于视觉变换器的模型](https://medium.com/p/6c86da083432/)'
- en: Let’s start this discussion with a short introduction to the Convolution layers
    and a few other layers that are typically used together as a convolution block.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从卷积层的简短介绍开始讨论，以及一些通常一起使用的其他层，作为卷积块。
- en: Conv-BatchNorm-ReLU and Max Pooling/Unpooling
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积-批归一化-ReLU 和 最大池化/反池化
- en: A convolution, batch-normalization, ReLU block is the holy trinity of vision
    AI. You’ll see it used frequently with CNN-based vision AI models. Each of these
    terms stands for a distinct layer implemented in PyTorch. The convolution layer
    is responsible for performing a cross-correlation operation of learned filters
    on the input tensor. Batch Normalization centers the elements in the batch to
    zero mean and unit variance, and ReLU is a non-linear activation function that
    keeps just the positive values in the input.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积、批归一化、ReLU块是视觉AI的圣三位一体。你会在基于CNN的视觉AI模型中频繁看到它的使用。这些术语分别代表在PyTorch中实现的不同层。卷积层负责对输入张量进行学习到的滤波器的交叉相关操作。批归一化将批中的元素中心化到零均值和单位方差，而ReLU是一个非线性激活函数，只保留输入中的正值。
- en: A typical CNN progressively reduces the input spatial dimensions as layers are
    stacked. The motivation behind the reduction of spatial dimensions is discussed
    in the next section. This reduction is achieved by pooling the neighboring values
    using a simple function such as max or average. We will discuss this further in
    the Max-Pooling section. In classification problems, the stack of Conv-BN-ReLU-Pool
    blocks is followed by a classification head which predicts the probability that
    input belongs to one of the target classes. Some sets of problems such as Semantic
    Segmentation require per-pixel prediction. For such cases, a stack of upsampling
    blocks are appended after the downsampling blocks to project their output to the
    required spatial dimension. The upsampling blocks are nothing but Conv-BN-ReLU-Unpool
    blocks which replace the pooling layer with an un-pooling layer. We will talk
    more about un-pooling in the Max-Pooling section.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的卷积神经网络（CNN）在层叠的过程中逐步减少输入的空间维度。空间维度减少的动机将在下一节讨论。这个减少是通过使用最大值或平均值等简单函数对邻近值进行池化来实现的。我们将在最大池化部分进一步讨论这个问题。在分类问题中，一系列Conv-BN-ReLU-Pool块后面跟着一个分类头，它预测输入属于目标类之一的概率。某些问题集，如语义分割，要求逐像素预测。对于这种情况，会在下采样块后添加一系列上采样块，以将其输出投影到所需的空间维度。上采样块实际上是Conv-BN-ReLU-Unpool块，它用反池化层替换了池化层。我们将在最大池化部分进一步讨论反池化。
- en: Now, let’s further elaborate on the motivation behind convolution layers.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进一步阐述卷积层背后的动机。
- en: Convolution
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积
- en: 'Convolutions are the basic building blocks of vision AI models. They are used
    heavily in computer vision and have historically been used to implement vision
    transformations such as:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积是视觉AI模型的基本构建块。它们在计算机视觉中被广泛使用，并且历史上被用于实现视觉变换，例如：
- en: Edge detection
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 边缘检测
- en: Image blurring and sharpening
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图像模糊和锐化
- en: Embossing
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 压花
- en: Intensification
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 强化
- en: A convolution operation is an elementwise multiplication and aggregation of
    two matrices. An example convolution operation is shown in Figure 2.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积操作是两个矩阵的逐元素乘法和聚合。图2展示了一个卷积操作的例子。
- en: '![](../Images/be30104d93750453b246b4c05e9c4998.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be30104d93750453b246b4c05e9c4998.png)'
- en: 'Figure 2: An illustration of the convolution operation. Source: Author(s)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：卷积操作的示意图。来源：作者
- en: In a deep learning context, convolution is carried out between an *n-dimensional*
    parameter matrix called a filter or a kernel over a larger-sized input. This is
    achieved by sliding the filter over the input and applying convolution to the
    corresponding section. The extent of the slide is configured using a stride parameter.
    A stride of one means the kernel slides over by one step to operate on the next
    section. As opposed to the traditional approaches where a fixed filter is used,
    deep learning learns the filter from the data using backpropagation.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习环境中，卷积是在较大尺寸的输入上进行的，卷积滤波器或内核是一个*n维*参数矩阵。通过在输入上滑动滤波器并对相应部分应用卷积来实现这一点。滑动的范围由步幅参数配置。步幅为一表示内核滑动一步以处理下一个部分。与使用固定滤波器的传统方法不同，深度学习通过反向传播从数据中学习滤波器。
- en: '**So how do convolutions assist in deep learning?**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**那么，卷积如何在深度学习中提供帮助？**'
- en: In deep learning, a convolution layer is used to detect visual features. A typical
    CNN model contains a stack of such layers. The bottom layers in the stack detect
    simple features such as lines and edges. As we move up in the stack, the layers
    detect increasingly complex features. Middle layers in the stack detect combinations
    of lines and edges and the top layers detect complex shapes such as a car, a face
    or an airplane. Figure 3 shows visually the output of top and bottom layers for
    a trained model.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，卷积层用于检测视觉特征。一个典型的CNN模型包含这样一系列层。栈底层检测简单特征，如线条和边缘。随着我们向上移动，这些层检测越来越复杂的特征。栈中的中间层检测线条和边缘的组合，而顶层检测复杂的形状，如汽车、面孔或飞机。图3直观地展示了训练模型的顶层和底层的输出。
- en: '![](../Images/9886c8e540b981a81ab8ae4da0ce4e96.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9886c8e540b981a81ab8ae4da0ce4e96.png)'
- en: 'Figure 3: What convolutional filters learn to identify. Source: [Convolutional
    Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations](http://web.eecs.umich.edu/~honglak/icml09-ConvolutionalDeepBeliefNetworks.pdf)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：卷积滤波器学习识别的内容。来源：[用于可扩展无监督层次表示学习的卷积深度置信网络](http://web.eecs.umich.edu/~honglak/icml09-ConvolutionalDeepBeliefNetworks.pdf)
- en: A convolution layer has a set of learnable filters that act on small regions
    in the input to produce a representative output value for each region. For example,
    a 3x3 filter operates over a 3x3 size region and produces a value representative
    of the region. The repeated application of a filter over input regions produces
    an output which becomes the input to the next layer in the stack. Intuitively,
    the layers higher up get to “see” a larger region of the input. For example, a
    3x3 filter in the second convolution layer operates on the output of the first
    convolution layer where each cell contains information about the 3x3 sized region
    in the input. If we assume a convolution operation with stride=1, then the filter
    in the second layer will “see’’ the 5x5 sized region of the original input. This
    is called the [receptive field](https://theaisummer.com/receptive-field/) of the
    convolution. The repeated application of convolutional layers progressively reduces
    the spatial dimensions of the input image and increases the field of vision of
    the filters which enables them to “see” complex shapes. Figure 4 shows the processing
    of a 1-D input by a convolution network. An element in the output layer is a representative
    of a relatively larger input chunk.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3302985b5a8d105588b693f168f7581a.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Receptive field of a 1d convolution with kernel size=3, applied 3
    times. Assume stride=1 and no padding. After the 3rd successive application of
    the convolutional kernel, a single pixel is able to see 7 pixels in the original
    input image. Source: Author(s)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'Once a convolutional layer can detect these objects and is able to generate
    their representations, we can use these representations for image classification,
    image segmentation, and object detection and localization. Broadly speaking, CNNs
    adhere to the following general principles:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: A Convolution layer either keeps the number of output channels © intact or doubles
    them.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It keeps the spatial dimensions intact using a stride=1 or reduces them to a
    half using stride=2.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It’s common to pool the output of a convolution block to change the spatial
    dimensions of an image.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A convolution layer applies the kernel independently to each input. This could
    cause its output to vary for different inputs. A Batch Normalization layer typically
    follows a convolution layer to address this problem. Let’s understand its role
    in detail in the next section.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Batch Normalization
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Batch Normalization layer normalizes the channel values in the batch input
    to have a zero mean and a unit variance. This normalization is performed independently
    for each channel in the batch to ensure that the channel values for the inputs
    have the same distribution. Batch Normalization has the following benefits:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: It stabilizes the training process by preventing the gradients from becoming
    too small.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It achieves faster convergence on our tasks.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If all we had was a stack of convolution layers, it would essentially be equivalent
    to a single convolution layer network because of the cascading effect of linear
    transformations. In other words, a sequence of linear transformations can be replaced
    with a single linear transformation which has the same effect. Intuitively, if
    we multiply a vector with a constant *k₁* followed by multiplication with another
    constant *k₂*, it is equivalent to a single multiplication by a constant *k₁k₂*.
    Hence, for the networks to be realistically deep, they must have a non-linearity
    to prevent their *collapse*. We will discuss ReLU in the next section which is
    frequently used as a non-linearity.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只有一堆卷积层，这实际上等同于一个单卷积层网络，因为线性变换的级联效应。换句话说，一系列线性变换可以用一个具有相同效果的单一线性变换来替代。直观上，如果我们用一个常数*k₁*乘以一个向量，再乘以另一个常数*k₂*，这等同于用常数*k₁k₂*进行一次乘法。因此，为了使网络实际具有深度，它们必须具有非线性以防止其*崩溃*。我们将在下一节中讨论ReLU，它通常用作非线性函数。
- en: ReLU
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ReLU
- en: ReLU is a simple non-linear activation function which clips the lowest input
    values to be greater than or equal to 0\. It also helps with the vanishing gradients
    problem limiting the outputs to be greater than or equal to 0\. The ReLU layer
    is typically followed by a pooling layer to shrink the spatial dimensions in the
    downscaling subnetwork or an un-pooling layer to bump the spatial dimensions in
    the upscaling subnetwork. The details are provided in the next section.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU是一个简单的非线性激活函数，它将最低输入值剪裁为大于或等于0。它还帮助解决梯度消失问题，将输出限制为大于或等于0。ReLU层通常后接一个池化层，以在下采样子网络中缩小空间维度，或者一个反池化层，以在上采样子网络中扩大空间维度。详细信息将在下一节中提供。
- en: Pooling
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 池化
- en: A pooling layer is used to shrink the spatial dimensions of our inputs. Pooling
    with *stride=2* will transform an input with spatial dimensions (H, W) to (H/2,
    W/2). [Max-pooling](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html)
    is the most commonly used pooling technique in deep CNNs. It projects the maximum
    value in a grid of (say) 2x2 onto the output. Then, we slide the 2x2 pooling window
    to the next section based on the stride similar to convolutions. Doing this repeatedly
    with a *stride=2* results in an output that is half the height and half the width
    of the input. Another commonly used pooling layer is the [average-pooling](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html#torch.nn.AvgPool2d)
    layer, which computes the average instead of the max.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层用于缩小输入的空间维度。使用*stride=2*的池化将输入的空间维度从(H, W)转换为(H/2, W/2)。[最大池化](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html)是深度CNN中最常用的池化技术。它将2x2网格中的最大值投影到输出上。然后，我们根据与卷积类似的步幅滑动2x2池化窗口到下一个区域。重复此过程，使用*stride=2*的结果是输出的高度和宽度都是输入的一半。另一种常用的池化层是[平均池化](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html#torch.nn.AvgPool2d)层，它计算平均值而不是最大值。
- en: The reverse of a pooling layer is called an un-pooling layer. It takes an (H,
    W) dimension input and converts it into a (2H, 2W) dimension output for *stride=2*.
    A necessary ingredient of this transformation is selecting the location in the
    2x2 section of the output to project the input value. To do this, we need a [max-unpooling](https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html#torch.nn.MaxUnpool2d)-index-map
    which tells us the target locations in the output section. This unpooling-map
    is produced by a previous max-pooling operation. Figure 5 shows examples of pooling
    and un-pooling operations.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层的反向操作称为反池化层。它将一个(H, W)维度的输入转换为一个(2H, 2W)维度的输出，适用于*stride=2*。这一转换的必要成分是选择在输出的2x2区域中投影输入值的位置。为此，我们需要一个[max-unpooling](https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html#torch.nn.MaxUnpool2d)索引图，它告诉我们输出区域中的目标位置。这个反池化图是由之前的最大池化操作生成的。图5展示了池化和反池化操作的示例。
- en: '![](../Images/e30783dc4a997c5fc7e750d938b02a0e.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e30783dc4a997c5fc7e750d938b02a0e.png)'
- en: 'Figure 5: Max pooling and un-pooling. Source: [DeepPainter: Painter Classification
    Using Deep Convolutional Autoencoders](https://www.researchgate.net/publication/306081538_DeepPainter_Painter_Classification_Using_Deep_Convolutional_Autoencoders)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '图5：最大池化和反池化。来源：[DeepPainter: Painter Classification Using Deep Convolutional
    Autoencoders](https://www.researchgate.net/publication/306081538_DeepPainter_Painter_Classification_Using_Deep_Convolutional_Autoencoders)'
- en: We can consider max-pooling as a type of non-linear activation function. However,
    it’s [reported](https://ai.stackexchange.com/questions/17937/is-a-non-linear-activation-function-needed-if-we-perform-max-pooling-after-the-c)
    that using it to replace a non-linearity such as ReLU [affects the network’s performance](https://arxiv.org/pdf/1606.02228.pdf).
    In contrast, average pooling can not be considered as a nonlinear function since
    it uses all its inputs to produce an output that is a linear combination of its
    inputs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将最大池化视为一种非线性激活函数。然而，*据报道*，[使用它来替代如 ReLU 这样的非线性函数会影响网络性能](https://arxiv.org/pdf/1606.02228.pdf)。相比之下，平均池化不能被视为非线性函数，因为它使用所有输入生成一个线性组合的输出。
- en: This covers all the basic building blocks of deep CNNs. Now, let’s put them
    together to create a model. The model we have chosen for this exercise is called
    a SegNet. We’ll discuss it next.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这涵盖了深度 CNN 的所有基本构建块。现在，让我们将它们组合起来创建一个模型。我们为这次练习选择的模型称为 SegNet。接下来我们将讨论它。
- en: 'SegNet: A CNN-based model'
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'SegNet: 一种基于 CNN 的模型'
- en: '[SegNet](https://arxiv.org/abs/1511.00561) is a deep CNN model based on the
    fundamental blocks that we have discussed in this article. It has two distinct
    sections. The bottom section, also called an encoder, down-samples the input to
    generate features representative of the input. The top decoder section up-samples
    the features to create per-pixel classification. Each section is composed of a
    sequence of Conv-BN-ReLU blocks. These blocks also incorporate pooling or un-pooling
    layers in downsampling and upsampling paths respectively. Figure 6 shows the arrangement
    of the layers in more detail. SegNet uses the pooling indices from the max-pooling
    operation in the encoder to determine which values to copy over during the max-unpooling
    operation in the decoder. While each element of an activation tensor is 4-bytes
    (32-bits), an offset within a 2x2 square can be stored using just 2-bits. This
    is more efficient in terms of memory used since these activations (or indices
    in the case of SegNet) need to be stored while the model runs.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[SegNet](https://arxiv.org/abs/1511.00561) 是一个基于本文讨论的基本块的深度 CNN 模型。它有两个不同的部分。底部部分，也称为编码器，进行下采样以生成代表输入的特征。顶部解码器部分上采样特征以进行逐像素分类。每个部分由一系列
    Conv-BN-ReLU 块组成。这些块还在下采样和上采样路径中分别包含池化或反池化层。图 6 显示了层的更详细排列。SegNet 使用编码器中的最大池化操作的池化索引来确定在解码器中的最大反池化操作期间复制哪些值。虽然激活张量的每个元素是
    4 字节（32 位），但在 2x2 的方块内可以仅用 2 位来存储偏移量。这在内存使用方面更有效，因为这些激活（或 SegNet 中的索引）在模型运行时需要被存储。'
- en: '![](../Images/c2781431e13509544ca760404da8b1f5.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2781431e13509544ca760404da8b1f5.png)'
- en: 'Figure 6: The SegNet model architecture for image segmentation. Source: SegNet:
    [A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation](https://arxiv.org/pdf/1511.00561.pdf)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: SegNet 模型架构用于图像分割。来源: SegNet: [用于图像分割的深度卷积编码器-解码器架构](https://arxiv.org/pdf/1511.00561.pdf)'
- en: '[This notebook](https://github.com/dhruvbird/ml-notebooks/blob/main/pets_segmentation/oxford-iiit-pets-segmentation-using-pytorch-segnet-and-depth-wise-separable-convs.ipynb)
    contains all the code for this section.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[此笔记本](https://github.com/dhruvbird/ml-notebooks/blob/main/pets_segmentation/oxford-iiit-pets-segmentation-using-pytorch-segnet-and-depth-wise-separable-convs.ipynb)包含本节的所有代码。'
- en: This model has 15.27M trainable parameters.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型具有 15.27M 可训练参数。
- en: The following configuration was used during model training and validation.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型训练和验证期间使用了以下配置。
- en: The *random horizontal flip* and *color jitter* data augmentations are applied
    to the training set to prevent overfitting
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*随机水平翻转* 和 *颜色抖动* 数据增强被应用于训练集以防止过拟合'
- en: The images are resized to 128x128 pixels in a non-aspect preserving resize operation
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图像在不保持纵横比的调整操作中被调整为 128x128 像素
- en: No input normalization is applied to the images; instead a [batch normalization
    layer is used as the first layer of the model](/replace-manual-normalization-with-batch-normalization-in-vision-ai-models-e7782e82193c)
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对图像没有应用输入标准化；而是使用了 [作为模型第一层的批量归一化层](/replace-manual-normalization-with-batch-normalization-in-vision-ai-models-e7782e82193c)
- en: The model is trained for 20 epochs using the Adam optimizer with a LR of 0.001
    and a StepLR Scheduler that decays the learning rate by 0.7 every 7 epochs
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型使用 Adam 优化器训练 20 个周期，学习率为 0.001，并且使用 StepLR 调度器，每 7 个周期将学习率衰减 0.7。
- en: The cross-entropy loss function is used to classify a pixel as belonging to
    a pet, the background, or a pet border
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 交叉熵损失函数用于将像素分类为属于宠物、背景或宠物边界。
- en: The model achieved a validation accuracy of 88.28% after 20 training epochs.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 经过20个训练轮次，模型达到了88.28%的验证准确率。
- en: We plotted a gif showing how the model is learning to predict the segmentation
    masks for 21 images in the validation set.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们绘制了一个 gif，展示了模型如何学习预测验证集21张图像的分割掩码。
- en: '![](../Images/f0bfdb501845cc50d9025cebf77968ba.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f0bfdb501845cc50d9025cebf77968ba.png)'
- en: 'Figure 6: A gif showing how the SegNet model is learning to predict segmentation
    masks for 21 images in the validation set. Source: Author(s)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：一个 gif，展示了 SegNet 模型如何学习预测验证集21张图像的分割掩码。来源：作者
- en: The definitions of all the validation metrics are described in [Part-1 of this
    series](https://medium.com/p/89e8297a0923/).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 所有验证指标的定义在[本系列的第1部分](https://medium.com/p/89e8297a0923/)中描述。
- en: 'If you’d like to see a fully-convolutional model for segmenting pet images
    implemented using Tensorflow, please see Chapter-4: Efficient Architectures of
    the [Efficient Deep Learning Book](https://efficientdlbook.com/#table-of-contents).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想查看一个使用 Tensorflow 实现的用于分割宠物图像的全卷积模型，请参阅[《高效深度学习书》](https://efficientdlbook.com/#table-of-contents)的第4章：高效架构。
- en: Observations from model learning
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型学习的观察
- en: Based on the development of the predictions that the trained model makes after
    every epoch, we can observe the following.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 根据经过训练的模型在每个轮次后的预测发展情况，我们可以观察到以下几点。
- en: The model is able to learn enough to make the output look in the right ballpark
    of the pet in the image even as early as 1 training epoch
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型能够在仅经过1个训练轮次时，就学会使输出在图像中的宠物位置上看起来正确。
- en: The border pixels are harder to segment since we’re using an unweighted loss
    function that treats each success (or failure) equally, so getting the border
    pixels wrong doesn’t cost the model much in terms of the loss. We would encourage
    you to investigate this and check what strategies you could try to fix this issue.
    Try using [Focal Loss](/focal-loss-a-better-alternative-for-cross-entropy-1d073d92d075)
    and see how it performs
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 边界像素更难以分割，因为我们使用的是一个未加权的损失函数，该函数对每次成功（或失败）的处理是一样的，因此，边界像素的错误对模型的损失影响不大。我们鼓励你研究这个问题，并查看你可以尝试哪些策略来解决它。试试使用[Focal
    Loss](/focal-loss-a-better-alternative-for-cross-entropy-1d073d92d075)并看看效果如何。
- en: The model seems to be learning even after 20 training epochs. This suggests
    that we could improve validation accuracy if we trained the model longer
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 即使在20个训练轮次后，模型似乎仍在学习。这表明，如果我们训练模型更长时间，可能会提高验证准确率。
- en: Some of the ground-truth labels themselves are hard to figure out — for example,
    the mask of the dog in the middle row, last column has a lot of unknown pixels
    in the area where the body of the dog is occluded by plants. This is very hard
    for the model to figure out, so one should always expect loss in accuracy for
    such examples. However, this doesn’t mean that the model isn’t doing well. One
    should always spot check the predictions to develop a sense of the model’s behavior
    in addition to looking at the overall validation metrics.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有些真实标签本身也很难确定——例如，中间一行最后一列的狗的掩码在狗的身体被植物遮挡的区域有很多未知像素。这对模型来说很难确定，因此对于这样的示例，通常会有准确率损失。然而，这并不意味着模型表现不好。除了查看整体验证指标之外，还应随时检查预测，以了解模型的行为。
- en: '![](../Images/7de58141bb2a950e94576c23ceddc981.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7de58141bb2a950e94576c23ceddc981.png)'
- en: 'Figure 7: An example of the ground truth segmentation mask containing a lot
    of unknown pixels. This is a very hard input for any ML model. Source: Author(s)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：一个包含大量未知像素的真实分割掩码示例。这对任何机器学习模型来说都是一个非常困难的输入。来源：作者
- en: Conclusion
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In part-2 of this series, we learned about the basic building blocks of deep
    CNNs for vision AI. We saw how to implement the SegNet model from scratch in PyTorch,
    and we visualized how the model trained on successive epochs performs on 21 validation
    images. This should help you appreciate how quickly models can learn enough to
    make the output look somewhere in the right ballpark. In this case, we can see
    segmentation masks that roughly resemble the actual segmentation mask as early
    as the first training epoch!
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在本系列的第2部分中，我们学习了深度卷积神经网络（CNN）在视觉AI中的基本构建块。我们展示了如何从零开始在PyTorch中实现SegNet模型，并可视化了模型在连续训练周期上的表现，这有助于你理解模型如何迅速学习到足够的知识以使输出大致接近正确范围。在这种情况下，我们可以看到分割掩码在第一次训练周期时就大致类似于实际的分割掩码！
- en: In the [next part of this series](https://medium.com/p/3534cf04fb89/), we’ll
    take a look at how we can optimize our model for on-device inference and reduce
    the number of trainable parameters (and hence model size) while keeping the validation
    accuracy roughly the same.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在[本系列的下一部分](https://medium.com/p/3534cf04fb89/)，我们将探讨如何优化我们的模型以实现设备上的推理，并减少可训练参数的数量（从而减少模型大小），同时保持验证精度大致不变。
- en: Further Reading
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Read more about convolutions here:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里阅读更多关于卷积的内容：
- en: The course titled “[Ancient Secrets of computer vision](https://pjreddie.com/courses/computer-vision/)”
    at The University of Washington taught by Joseph Redmon has an excellent set of
    videos on Convolutions (especially chapters 4, 5, and 13), which we highly recommend
    watching
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由约瑟夫·雷德蒙教授在华盛顿大学讲授的课程 “[计算机视觉的古老秘密](https://pjreddie.com/courses/computer-vision/)”
    提供了一套关于卷积（特别是第4、5和13章）的优秀视频，我们强烈推荐观看。
- en: '[A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285)
    (highly recommended)'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[深度学习中的卷积算术指南](https://arxiv.org/abs/1603.07285)（强烈推荐）'
- en: '[https://towardsdatascience.com/computer-vision-convolution-basics-2d0ae3b79346](/computer-vision-convolution-basics-2d0ae3b79346)'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://towardsdatascience.com/computer-vision-convolution-basics-2d0ae3b79346](/computer-vision-convolution-basics-2d0ae3b79346)'
- en: '[The Conv2d layer in PyTorch (documentation)](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[PyTorch中的Conv2d层（文档）](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)'
- en: '[What do convolutions learn?](https://www.kdnuggets.com/2016/11/intuitive-explanation-convolutional-neural-networks.html/3)'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[卷积学习了什么？](https://www.kdnuggets.com/2016/11/intuitive-explanation-convolutional-neural-networks.html/3)'
- en: '[Convolution visualizer](https://ezyang.github.io/convolution-visualizer/index.html)'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[卷积可视化工具](https://ezyang.github.io/convolution-visualizer/index.html)'
- en: 'Read more about batch normalization here:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里阅读更多关于批量归一化的内容：
- en: '[Batch normalization: Wikipedia](https://en.wikipedia.org/wiki/Batch_normalization)'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[批量归一化：维基百科](https://en.wikipedia.org/wiki/Batch_normalization)'
- en: '[Batch normalization: Machine learning mastery](https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/)'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[批量归一化：机器学习大师](https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/)'
- en: '[BatchNorm2d layer in PyTorch here](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html).'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[PyTorch中的BatchNorm2d层](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)。'
- en: 'Read more about activation functions and ReLU here:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里阅读更多关于激活函数和ReLU的内容：
- en: '[ReLU: Machine learning mastery](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/)'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ReLU：机器学习大师](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/)'
- en: '[ReLU: Wikipedia](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ReLU：维基百科](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))'
- en: '[ReLU: Quora](https://www.quora.com/Why-is-ReLU-used-so-much-in-neural-networks)'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ReLU：Quora](https://www.quora.com/Why-is-ReLU-used-so-much-in-neural-networks)'
- en: '[ReLU API in PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[PyTorch中的ReLU API](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)'
