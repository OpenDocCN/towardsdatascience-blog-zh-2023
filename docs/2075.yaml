- en: 'Efficient Image Segmentation Using PyTorch: Part 2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/efficient-image-segmentation-using-pytorch-part-2-bed68cadd7c7?source=collection_archive---------4-----------------------#2023-06-27](https://towardsdatascience.com/efficient-image-segmentation-using-pytorch-part-2-bed68cadd7c7?source=collection_archive---------4-----------------------#2023-06-27)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A CNN-based model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@dhruvbird?source=post_page-----bed68cadd7c7--------------------------------)[![Dhruv
    Matani](../Images/d63bf7776c28a29c02b985b1f64abdd3.png)](https://medium.com/@dhruvbird?source=post_page-----bed68cadd7c7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bed68cadd7c7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bed68cadd7c7--------------------------------)
    [Dhruv Matani](https://medium.com/@dhruvbird?source=post_page-----bed68cadd7c7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F63f5d5495279&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficient-image-segmentation-using-pytorch-part-2-bed68cadd7c7&user=Dhruv+Matani&userId=63f5d5495279&source=post_page-63f5d5495279----bed68cadd7c7---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bed68cadd7c7--------------------------------)
    ·11 min read·Jun 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbed68cadd7c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficient-image-segmentation-using-pytorch-part-2-bed68cadd7c7&user=Dhruv+Matani&userId=63f5d5495279&source=-----bed68cadd7c7---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbed68cadd7c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficient-image-segmentation-using-pytorch-part-2-bed68cadd7c7&source=-----bed68cadd7c7---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: This is the second part of the 4-part series to implement image segmentation
    step by step from scratch using deep learning techniques in PyTorch. This part
    will focus on implementing a baseline image segmentation Convolutional Neural
    Network (CNN) model.
  prefs: []
  type: TYPE_NORMAL
- en: Co-authored with [Naresh Singh](https://medium.com/@brocolishbroxoli)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/71d6ffa1c9d4d8f16cc63e2c2f47ce90.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Result of running image segmentation using a CNN. In order from top
    to bottom, input images, ground truth segmentation masks, predicted segmentation
    masks. Source: Author(s)'
  prefs: []
  type: TYPE_NORMAL
- en: Article outline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we will implement a [Convolutional Neural Network (CNN)](https://en.wikipedia.org/wiki/Convolutional_neural_network)
    based architecture called [SegNet](https://arxiv.org/abs/1511.00561) which will
    assign each pixel in an input image to a corresponding pet such as a cat or a
    dog. The pixels which don’t belong to any pet will be classified as background
    pixels. We will build and train this model on the [Oxford Pets dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/)
    using [PyTorch](https://pytorch.org/) to develop a sense of what it takes to deliver
    a successful image segmentation task. The model building process will be hands-on
    where we will discuss in detail the role of each layer in our model. The article
    will contain plenty of references to research papers and articles for further
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this article, we will reference the code and results from this [notebook](https://github.com/dhruvbird/ml-notebooks/blob/main/pets_segmentation/oxford-iiit-pets-segmentation-using-pytorch-segnet-and-depth-wise-separable-convs.ipynb).
    If you wish to reproduce the results, you’ll need a GPU to ensure that the notebook
    completes running in a reasonable amount of time.
  prefs: []
  type: TYPE_NORMAL
- en: Articles in this series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This series is for readers at all experience levels with deep learning. If
    you want to learn about the practice of deep learning and vision AI along with
    some solid theory and hands-on experience, you’ve come to the right place! This
    is expected to be a 4-part series with the following articles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Concepts and Ideas](https://medium.com/p/89e8297a0923/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**A CNN-based model (this article)**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Depthwise separable convolutions](https://medium.com/p/3534cf04fb89/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[A Vision Transformer-based model](https://medium.com/p/6c86da083432/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s start this discussion with a short introduction to the Convolution layers
    and a few other layers that are typically used together as a convolution block.
  prefs: []
  type: TYPE_NORMAL
- en: Conv-BatchNorm-ReLU and Max Pooling/Unpooling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A convolution, batch-normalization, ReLU block is the holy trinity of vision
    AI. You’ll see it used frequently with CNN-based vision AI models. Each of these
    terms stands for a distinct layer implemented in PyTorch. The convolution layer
    is responsible for performing a cross-correlation operation of learned filters
    on the input tensor. Batch Normalization centers the elements in the batch to
    zero mean and unit variance, and ReLU is a non-linear activation function that
    keeps just the positive values in the input.
  prefs: []
  type: TYPE_NORMAL
- en: A typical CNN progressively reduces the input spatial dimensions as layers are
    stacked. The motivation behind the reduction of spatial dimensions is discussed
    in the next section. This reduction is achieved by pooling the neighboring values
    using a simple function such as max or average. We will discuss this further in
    the Max-Pooling section. In classification problems, the stack of Conv-BN-ReLU-Pool
    blocks is followed by a classification head which predicts the probability that
    input belongs to one of the target classes. Some sets of problems such as Semantic
    Segmentation require per-pixel prediction. For such cases, a stack of upsampling
    blocks are appended after the downsampling blocks to project their output to the
    required spatial dimension. The upsampling blocks are nothing but Conv-BN-ReLU-Unpool
    blocks which replace the pooling layer with an un-pooling layer. We will talk
    more about un-pooling in the Max-Pooling section.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s further elaborate on the motivation behind convolution layers.
  prefs: []
  type: TYPE_NORMAL
- en: Convolution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Convolutions are the basic building blocks of vision AI models. They are used
    heavily in computer vision and have historically been used to implement vision
    transformations such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Edge detection
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Image blurring and sharpening
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Embossing
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Intensification
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A convolution operation is an elementwise multiplication and aggregation of
    two matrices. An example convolution operation is shown in Figure 2.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be30104d93750453b246b4c05e9c4998.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: An illustration of the convolution operation. Source: Author(s)'
  prefs: []
  type: TYPE_NORMAL
- en: In a deep learning context, convolution is carried out between an *n-dimensional*
    parameter matrix called a filter or a kernel over a larger-sized input. This is
    achieved by sliding the filter over the input and applying convolution to the
    corresponding section. The extent of the slide is configured using a stride parameter.
    A stride of one means the kernel slides over by one step to operate on the next
    section. As opposed to the traditional approaches where a fixed filter is used,
    deep learning learns the filter from the data using backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: '**So how do convolutions assist in deep learning?**'
  prefs: []
  type: TYPE_NORMAL
- en: In deep learning, a convolution layer is used to detect visual features. A typical
    CNN model contains a stack of such layers. The bottom layers in the stack detect
    simple features such as lines and edges. As we move up in the stack, the layers
    detect increasingly complex features. Middle layers in the stack detect combinations
    of lines and edges and the top layers detect complex shapes such as a car, a face
    or an airplane. Figure 3 shows visually the output of top and bottom layers for
    a trained model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9886c8e540b981a81ab8ae4da0ce4e96.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: What convolutional filters learn to identify. Source: [Convolutional
    Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations](http://web.eecs.umich.edu/~honglak/icml09-ConvolutionalDeepBeliefNetworks.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: A convolution layer has a set of learnable filters that act on small regions
    in the input to produce a representative output value for each region. For example,
    a 3x3 filter operates over a 3x3 size region and produces a value representative
    of the region. The repeated application of a filter over input regions produces
    an output which becomes the input to the next layer in the stack. Intuitively,
    the layers higher up get to “see” a larger region of the input. For example, a
    3x3 filter in the second convolution layer operates on the output of the first
    convolution layer where each cell contains information about the 3x3 sized region
    in the input. If we assume a convolution operation with stride=1, then the filter
    in the second layer will “see’’ the 5x5 sized region of the original input. This
    is called the [receptive field](https://theaisummer.com/receptive-field/) of the
    convolution. The repeated application of convolutional layers progressively reduces
    the spatial dimensions of the input image and increases the field of vision of
    the filters which enables them to “see” complex shapes. Figure 4 shows the processing
    of a 1-D input by a convolution network. An element in the output layer is a representative
    of a relatively larger input chunk.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3302985b5a8d105588b693f168f7581a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Receptive field of a 1d convolution with kernel size=3, applied 3
    times. Assume stride=1 and no padding. After the 3rd successive application of
    the convolutional kernel, a single pixel is able to see 7 pixels in the original
    input image. Source: Author(s)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once a convolutional layer can detect these objects and is able to generate
    their representations, we can use these representations for image classification,
    image segmentation, and object detection and localization. Broadly speaking, CNNs
    adhere to the following general principles:'
  prefs: []
  type: TYPE_NORMAL
- en: A Convolution layer either keeps the number of output channels © intact or doubles
    them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It keeps the spatial dimensions intact using a stride=1 or reduces them to a
    half using stride=2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It’s common to pool the output of a convolution block to change the spatial
    dimensions of an image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A convolution layer applies the kernel independently to each input. This could
    cause its output to vary for different inputs. A Batch Normalization layer typically
    follows a convolution layer to address this problem. Let’s understand its role
    in detail in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Batch Normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Batch Normalization layer normalizes the channel values in the batch input
    to have a zero mean and a unit variance. This normalization is performed independently
    for each channel in the batch to ensure that the channel values for the inputs
    have the same distribution. Batch Normalization has the following benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: It stabilizes the training process by preventing the gradients from becoming
    too small.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It achieves faster convergence on our tasks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If all we had was a stack of convolution layers, it would essentially be equivalent
    to a single convolution layer network because of the cascading effect of linear
    transformations. In other words, a sequence of linear transformations can be replaced
    with a single linear transformation which has the same effect. Intuitively, if
    we multiply a vector with a constant *k₁* followed by multiplication with another
    constant *k₂*, it is equivalent to a single multiplication by a constant *k₁k₂*.
    Hence, for the networks to be realistically deep, they must have a non-linearity
    to prevent their *collapse*. We will discuss ReLU in the next section which is
    frequently used as a non-linearity.
  prefs: []
  type: TYPE_NORMAL
- en: ReLU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ReLU is a simple non-linear activation function which clips the lowest input
    values to be greater than or equal to 0\. It also helps with the vanishing gradients
    problem limiting the outputs to be greater than or equal to 0\. The ReLU layer
    is typically followed by a pooling layer to shrink the spatial dimensions in the
    downscaling subnetwork or an un-pooling layer to bump the spatial dimensions in
    the upscaling subnetwork. The details are provided in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A pooling layer is used to shrink the spatial dimensions of our inputs. Pooling
    with *stride=2* will transform an input with spatial dimensions (H, W) to (H/2,
    W/2). [Max-pooling](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html)
    is the most commonly used pooling technique in deep CNNs. It projects the maximum
    value in a grid of (say) 2x2 onto the output. Then, we slide the 2x2 pooling window
    to the next section based on the stride similar to convolutions. Doing this repeatedly
    with a *stride=2* results in an output that is half the height and half the width
    of the input. Another commonly used pooling layer is the [average-pooling](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html#torch.nn.AvgPool2d)
    layer, which computes the average instead of the max.
  prefs: []
  type: TYPE_NORMAL
- en: The reverse of a pooling layer is called an un-pooling layer. It takes an (H,
    W) dimension input and converts it into a (2H, 2W) dimension output for *stride=2*.
    A necessary ingredient of this transformation is selecting the location in the
    2x2 section of the output to project the input value. To do this, we need a [max-unpooling](https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html#torch.nn.MaxUnpool2d)-index-map
    which tells us the target locations in the output section. This unpooling-map
    is produced by a previous max-pooling operation. Figure 5 shows examples of pooling
    and un-pooling operations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e30783dc4a997c5fc7e750d938b02a0e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Max pooling and un-pooling. Source: [DeepPainter: Painter Classification
    Using Deep Convolutional Autoencoders](https://www.researchgate.net/publication/306081538_DeepPainter_Painter_Classification_Using_Deep_Convolutional_Autoencoders)'
  prefs: []
  type: TYPE_NORMAL
- en: We can consider max-pooling as a type of non-linear activation function. However,
    it’s [reported](https://ai.stackexchange.com/questions/17937/is-a-non-linear-activation-function-needed-if-we-perform-max-pooling-after-the-c)
    that using it to replace a non-linearity such as ReLU [affects the network’s performance](https://arxiv.org/pdf/1606.02228.pdf).
    In contrast, average pooling can not be considered as a nonlinear function since
    it uses all its inputs to produce an output that is a linear combination of its
    inputs.
  prefs: []
  type: TYPE_NORMAL
- en: This covers all the basic building blocks of deep CNNs. Now, let’s put them
    together to create a model. The model we have chosen for this exercise is called
    a SegNet. We’ll discuss it next.
  prefs: []
  type: TYPE_NORMAL
- en: 'SegNet: A CNN-based model'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[SegNet](https://arxiv.org/abs/1511.00561) is a deep CNN model based on the
    fundamental blocks that we have discussed in this article. It has two distinct
    sections. The bottom section, also called an encoder, down-samples the input to
    generate features representative of the input. The top decoder section up-samples
    the features to create per-pixel classification. Each section is composed of a
    sequence of Conv-BN-ReLU blocks. These blocks also incorporate pooling or un-pooling
    layers in downsampling and upsampling paths respectively. Figure 6 shows the arrangement
    of the layers in more detail. SegNet uses the pooling indices from the max-pooling
    operation in the encoder to determine which values to copy over during the max-unpooling
    operation in the decoder. While each element of an activation tensor is 4-bytes
    (32-bits), an offset within a 2x2 square can be stored using just 2-bits. This
    is more efficient in terms of memory used since these activations (or indices
    in the case of SegNet) need to be stored while the model runs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c2781431e13509544ca760404da8b1f5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The SegNet model architecture for image segmentation. Source: SegNet:
    [A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation](https://arxiv.org/pdf/1511.00561.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '[This notebook](https://github.com/dhruvbird/ml-notebooks/blob/main/pets_segmentation/oxford-iiit-pets-segmentation-using-pytorch-segnet-and-depth-wise-separable-convs.ipynb)
    contains all the code for this section.'
  prefs: []
  type: TYPE_NORMAL
- en: This model has 15.27M trainable parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The following configuration was used during model training and validation.
  prefs: []
  type: TYPE_NORMAL
- en: The *random horizontal flip* and *color jitter* data augmentations are applied
    to the training set to prevent overfitting
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The images are resized to 128x128 pixels in a non-aspect preserving resize operation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: No input normalization is applied to the images; instead a [batch normalization
    layer is used as the first layer of the model](/replace-manual-normalization-with-batch-normalization-in-vision-ai-models-e7782e82193c)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model is trained for 20 epochs using the Adam optimizer with a LR of 0.001
    and a StepLR Scheduler that decays the learning rate by 0.7 every 7 epochs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The cross-entropy loss function is used to classify a pixel as belonging to
    a pet, the background, or a pet border
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model achieved a validation accuracy of 88.28% after 20 training epochs.
  prefs: []
  type: TYPE_NORMAL
- en: We plotted a gif showing how the model is learning to predict the segmentation
    masks for 21 images in the validation set.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f0bfdb501845cc50d9025cebf77968ba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: A gif showing how the SegNet model is learning to predict segmentation
    masks for 21 images in the validation set. Source: Author(s)'
  prefs: []
  type: TYPE_NORMAL
- en: The definitions of all the validation metrics are described in [Part-1 of this
    series](https://medium.com/p/89e8297a0923/).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’d like to see a fully-convolutional model for segmenting pet images
    implemented using Tensorflow, please see Chapter-4: Efficient Architectures of
    the [Efficient Deep Learning Book](https://efficientdlbook.com/#table-of-contents).'
  prefs: []
  type: TYPE_NORMAL
- en: Observations from model learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Based on the development of the predictions that the trained model makes after
    every epoch, we can observe the following.
  prefs: []
  type: TYPE_NORMAL
- en: The model is able to learn enough to make the output look in the right ballpark
    of the pet in the image even as early as 1 training epoch
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The border pixels are harder to segment since we’re using an unweighted loss
    function that treats each success (or failure) equally, so getting the border
    pixels wrong doesn’t cost the model much in terms of the loss. We would encourage
    you to investigate this and check what strategies you could try to fix this issue.
    Try using [Focal Loss](/focal-loss-a-better-alternative-for-cross-entropy-1d073d92d075)
    and see how it performs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model seems to be learning even after 20 training epochs. This suggests
    that we could improve validation accuracy if we trained the model longer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Some of the ground-truth labels themselves are hard to figure out — for example,
    the mask of the dog in the middle row, last column has a lot of unknown pixels
    in the area where the body of the dog is occluded by plants. This is very hard
    for the model to figure out, so one should always expect loss in accuracy for
    such examples. However, this doesn’t mean that the model isn’t doing well. One
    should always spot check the predictions to develop a sense of the model’s behavior
    in addition to looking at the overall validation metrics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/7de58141bb2a950e94576c23ceddc981.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: An example of the ground truth segmentation mask containing a lot
    of unknown pixels. This is a very hard input for any ML model. Source: Author(s)'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In part-2 of this series, we learned about the basic building blocks of deep
    CNNs for vision AI. We saw how to implement the SegNet model from scratch in PyTorch,
    and we visualized how the model trained on successive epochs performs on 21 validation
    images. This should help you appreciate how quickly models can learn enough to
    make the output look somewhere in the right ballpark. In this case, we can see
    segmentation masks that roughly resemble the actual segmentation mask as early
    as the first training epoch!
  prefs: []
  type: TYPE_NORMAL
- en: In the [next part of this series](https://medium.com/p/3534cf04fb89/), we’ll
    take a look at how we can optimize our model for on-device inference and reduce
    the number of trainable parameters (and hence model size) while keeping the validation
    accuracy roughly the same.
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Read more about convolutions here:'
  prefs: []
  type: TYPE_NORMAL
- en: The course titled “[Ancient Secrets of computer vision](https://pjreddie.com/courses/computer-vision/)”
    at The University of Washington taught by Joseph Redmon has an excellent set of
    videos on Convolutions (especially chapters 4, 5, and 13), which we highly recommend
    watching
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285)
    (highly recommended)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://towardsdatascience.com/computer-vision-convolution-basics-2d0ae3b79346](/computer-vision-convolution-basics-2d0ae3b79346)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[The Conv2d layer in PyTorch (documentation)](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[What do convolutions learn?](https://www.kdnuggets.com/2016/11/intuitive-explanation-convolutional-neural-networks.html/3)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Convolution visualizer](https://ezyang.github.io/convolution-visualizer/index.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Read more about batch normalization here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Batch normalization: Wikipedia](https://en.wikipedia.org/wiki/Batch_normalization)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Batch normalization: Machine learning mastery](https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[BatchNorm2d layer in PyTorch here](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Read more about activation functions and ReLU here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[ReLU: Machine learning mastery](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[ReLU: Wikipedia](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[ReLU: Quora](https://www.quora.com/Why-is-ReLU-used-so-much-in-neural-networks)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[ReLU API in PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
