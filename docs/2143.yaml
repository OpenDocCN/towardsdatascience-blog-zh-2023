- en: 'Domain Adaptation: Fine-Tune Pre-Trained NLP Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/domain-adaption-fine-tune-pre-trained-nlp-models-a06659ca6668?source=collection_archive---------1-----------------------#2023-07-04](https://towardsdatascience.com/domain-adaption-fine-tune-pre-trained-nlp-models-a06659ca6668?source=collection_archive---------1-----------------------#2023-07-04)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/60b46d488ea28b302a949054bded404e.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Pietro Jeng](https://unsplash.com/@pietrozj?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '[Hands-on Tutorials](https://towardsdatascience.com/tagged/hands-on-tutorials)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A step-by-step guide to fine-tuning pre-trained NLP models for any domain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@shashank.kapadia?source=post_page-----a06659ca6668--------------------------------)[![Shashank
    Kapadia](../Images/347e4cb92a7d27f032c5761e4526f2fa.png)](https://medium.com/@shashank.kapadia?source=post_page-----a06659ca6668--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a06659ca6668--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a06659ca6668--------------------------------)
    [Shashank Kapadia](https://medium.com/@shashank.kapadia?source=post_page-----a06659ca6668--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcc7314ace45c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdomain-adaption-fine-tune-pre-trained-nlp-models-a06659ca6668&user=Shashank+Kapadia&userId=cc7314ace45c&source=post_page-cc7314ace45c----a06659ca6668---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a06659ca6668--------------------------------)
    ·9 min read·Jul 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa06659ca6668&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdomain-adaption-fine-tune-pre-trained-nlp-models-a06659ca6668&user=Shashank+Kapadia&userId=cc7314ace45c&source=-----a06659ca6668---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa06659ca6668&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdomain-adaption-fine-tune-pre-trained-nlp-models-a06659ca6668&source=-----a06659ca6668---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Preface: This article presents a summary of information about the given topic.
    It should not be considered original research. The information and code included
    in this article have may be influenced by things I have read or seen in the past
    from various online articles, research papers, books, and open-source code.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Co-Author:* [*Billy Hines*](https://medium.com/u/f6bf67256839?source=post_page-----a06659ca6668--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Table of Content
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Theoretical Framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data Overview
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Starting Point: The Baseline Model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-Tuning the Model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the Results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Closing Thoughts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In today’s world, the availability of pre-trained NLP models has greatly simplified
    the interpretation of textual data using deep learning techniques. However, while
    these models excel in general tasks, they often lack adaptability to specific
    domains. This comprehensive guide aims to walk you through the process of fine-tuning
    pre-trained NLP models to achieve improved performance in a particular domain.
  prefs: []
  type: TYPE_NORMAL
- en: Motivation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although pre-trained NLP models like BERT and the Universal Sentence Encoder
    (USE) are effective in capturing linguistic intricacies, their performance in
    domain-specific applications can be limited due to the diverse range of datasets
    they are trained on. This limitation becomes evident when analyzing relationships
    within a specific domain.
  prefs: []
  type: TYPE_NORMAL
- en: For example, when working with employment data, we expect the model to recognize
    the closer proximity between the roles of ‘Data Scientist’ and ‘Machine Learning
    Engineer’, or the stronger association between ‘Python’ and ‘TensorFlow’. Unfortunately,
    general-purpose models often miss these nuanced relationships.
  prefs: []
  type: TYPE_NORMAL
- en: 'The table below demonstrates the discrepancies in the similarity obtained from
    a base multilingual USE model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fd70480995a62829117960a1a4fa1d88.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 1\. Similarity score between two text vectors from base [MultiLingual Universal
    Sentence Encoder Model](https://tfhub.dev/google/universal-sentence-encoder-multilingual/3)
  prefs: []
  type: TYPE_NORMAL
- en: To address this issue, we can fine-tune pre-trained models with high-quality,
    domain-specific datasets. This adaptation process significantly enhances the model’s
    performance and precision, fully unlocking the potential of the NLP model.
  prefs: []
  type: TYPE_NORMAL
- en: When dealing with large pre-trained NLP models, it is advisable to initially
    deploy the base model and consider fine-tuning only if its performance falls short
    for the specific problem at hand.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This tutorial focuses on fine-tuning the Universal Sentence Encoder (USE) model
    using easily accessible open-source data.
  prefs: []
  type: TYPE_NORMAL
- en: Theoretical Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fine-tuning an ML model can be achieved through various strategies, such as
    supervised learning and reinforcement learning. In this tutorial, we will concentrate
    on a one(few)-shot learning approach combined with a siamese architecture for
    the fine-tuning process.
  prefs: []
  type: TYPE_NORMAL
- en: Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this tutorial, we utilize a siamese neural network, which is a specific type
    of Artificial Neural Network. This network leverages shared weights while simultaneously
    processing two distinct input vectors to compute comparable output vectors. Inspired
    by one-shot learning, this approach has proven to be particularly effective in
    capturing semantic similarity, although it may require longer training times and
    lack probabilistic output.
  prefs: []
  type: TYPE_NORMAL
- en: A Siamese Neural Network creates an ‘embedding space’ where related concepts
    are positioned closely, enabling the model to better discern semantic relations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e04c290c089ac82b1e99511c2e8c6df0.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 2\. Siamese architecture to fine-tune pre-trained NLP model
  prefs: []
  type: TYPE_NORMAL
- en: '**Twin Branches and Shared Weights**: The architecture consists of two identical
    branches, each containing an embedding layer with shared weights. These dual branches
    handle two inputs simultaneously, either similar or dissimilar.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Similarity and Transformation**: The inputs are transformed into vector embeddings
    using the pre-trained NLP model. The architecture then calculates the similarity
    between the vectors. The similarity score, ranging between -1 and 1, quantifies
    the angular distance between the two vectors, serving as a metric for their semantic
    similarity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contrastive Loss and Learning**: The model’s learning is guided by the “Contrastive
    Loss,” which is the difference between the expected output (similarity score from
    the training data) and the computed similarity. This loss guides the adjustment
    of the model’s weights to minimize the loss and enhance the quality of the learned
    embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To learn more about one(few)-shot learning, siamese architecture, and contrastive
    loss, refer to the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.projectpro.io/article/siamese-neural-networks/718?source=post_page-----a06659ca6668--------------------------------)
    [## A Gentle Introduction to Siamese Neural Networks Architecture'
  prefs: []
  type: TYPE_NORMAL
- en: 'Siamese Neural Networks Architecture : An Overview and Key Concepts Explained
    with Examples | ProjectPro'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.projectpro.io](https://www.projectpro.io/article/siamese-neural-networks/718?source=post_page-----a06659ca6668--------------------------------)
    [](https://bdtechtalks.com/2020/08/12/what-is-one-shot-learning/?source=post_page-----a06659ca6668--------------------------------)
    [## What is one-shot learning? — TechTalks
  prefs: []
  type: TYPE_NORMAL
- en: One-shot learning allows deep learning algorithms to measure the similarity
    and difference between two images.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: bdtechtalks.com](https://bdtechtalks.com/2020/08/12/what-is-one-shot-learning/?source=post_page-----a06659ca6668--------------------------------)
    [](/contrastive-loss-explaned-159f2d4a87ec?source=post_page-----a06659ca6668--------------------------------)
    [## Contrastive Loss Explained
  prefs: []
  type: TYPE_NORMAL
- en: Contrastive loss has been used recently in a number of papers showing state
    of the art results with unsupervised…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/contrastive-loss-explaned-159f2d4a87ec?source=post_page-----a06659ca6668--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: The complete code is available as a [Jupyter Notebook on GitHub](https://github.com/kapadias/medium-articles/blob/master/natural-language-processing/embedding-models/domain_adaption_fine_tune_nlp_model.ipynb)
  prefs: []
  type: TYPE_NORMAL
- en: Data Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the fine-tuning of pre-trained NLP models using this method, the training
    data should consist of pairs of text strings accompanied by similarity scores
    between them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The training data follows the format shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/002a9075851dd4c80026dc760b2c3255.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 3\. Sample Format for Training Data
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, we use a dataset sourced from the [ESCO classification dataset](https://esco.ec.europa.eu/en),
    which has been transformed to generate similarity scores based on the relationships
    between different data elements.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the training data is a crucial step in the fine-tuning process. It
    is assumed that you have access to the required data and a method to transform
    it into the specified format. Since the focus of this article is to demonstrate
    the fine-tuning process, we will omit the details of how the data was generated
    using the ESCO dataset.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The ESCO dataset is available for developers to freely utilize as a foundation
    for various applications that offer services like autocomplete, suggestion systems,
    job search algorithms, and job matching algorithms. The dataset used in this tutorial
    has been transformed and provided as a sample, allowing unrestricted usage for
    any purpose.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Let’s start by examining the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/f63c7e896d9f23fa4d83168d3cf29e1f.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 4\. Sample data used for fine-tuning the model
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting Point: The Baseline Model'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To begin, we establish the [multilingual universal sentence encoder](https://tfhub.dev/google/universal-sentence-encoder-multilingual/3)
    as our baseline model. It is essential to set this baseline before proceeding
    with the fine-tuning process.
  prefs: []
  type: TYPE_NORMAL
- en: For this tutorial, we will use the STS benchmark and a sample similarity visualization
    as metrics to evaluate the changes and improvements achieved through the fine-tuning
    process.
  prefs: []
  type: TYPE_NORMAL
- en: The STS Benchmark dataset consists of English sentence pairs, each associated
    with a similarity score. During the model training process, we evaluate the model’s
    performance on this benchmark set. The persisted scores for each training run
    are the Pearson correlation between the predicted similarity scores and the actual
    similarity scores in the dataset.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: These scores ensure that as the model is fine-tuned with our context-specific
    training data, it maintains some level of generalizability.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ddf3d02dfb50c6ebf6c98aba0e9a055a.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 5\. Similarity visualtions across test words
  prefs: []
  type: TYPE_NORMAL
- en: 'STS Benchmark (dev): 0.8325'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Fine Tuning the Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next step involves constructing the siamese model architecture using the
    baseline model and fine-tuning it with our domain-specific data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/efd1cd25e0ddc3ef8fe842e8185ea96d.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 6\. Model architecture for fine-tuning
  prefs: []
  type: TYPE_NORMAL
- en: Fit the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have the fine-tuned model, let’s re-evaluate it and compare the
    results to those of the base model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c882c7db246a72c1d21308cd8dc462a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'STS Benchmark (dev): 0.8349'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Based on fine-tuning the model on the relatively small dataset, the STS benchmark
    score is comparable to that of the baseline model, indicating that the tuned model
    still exhibits generalizability. However, the similarity visualization demonstrates
    strengthened similarity scores between similar titles and a reduction in scores
    for dissimilar ones.
  prefs: []
  type: TYPE_NORMAL
- en: Closing Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fine-tuning pre-trained NLP models for domain adaptation is a powerful technique
    to improve their performance and precision in specific contexts. By utilizing
    quality, domain-specific datasets and leveraging siamese neural networks, we can
    enhance the model’s ability to capture semantic similarity.
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial provided a step-by-step guide to the fine-tuning process, using
    the Universal Sentence Encoder (USE) model as an example. We explored the theoretical
    framework, data preparation, baseline model evaluation, and the actual fine-tuning
    process. The results demonstrated the effectiveness of fine-tuning in strengthening
    similarity scores within a domain.
  prefs: []
  type: TYPE_NORMAL
- en: By following this approach and adapting it to your specific domain, you can
    unlock the full potential of pre-trained NLP models and achieve better results
    in your natural language processing tasks
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading. *If you have any feedback, please feel to reach out by commenting
    on this post, messaging me on* [*LinkedIn*](https://www.linkedin.com/in/shashankkapadia/)*,
    or shooting me an email (smhkapadia[at]gmail.com)*
  prefs: []
  type: TYPE_NORMAL
- en: '*If you enjoyed this article, visit my other articles*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0?source=post_page-----a06659ca6668--------------------------------)
    [## Evaluate Topic Models: Latent Dirichlet Allocation (LDA)'
  prefs: []
  type: TYPE_NORMAL
- en: A step-by-step guide to building interpretable topic models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0?source=post_page-----a06659ca6668--------------------------------)
    [](https://medium.com/aimonks/the-evolution-of-natural-language-processing-56ce27916e10?source=post_page-----a06659ca6668--------------------------------)
    [## The Evolution of Natural Language Processing
  prefs: []
  type: TYPE_NORMAL
- en: A Historical Perspective on the Development of Language Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'medium.com](https://medium.com/aimonks/the-evolution-of-natural-language-processing-56ce27916e10?source=post_page-----a06659ca6668--------------------------------)
    [](/recommendation-system-in-python-lightfm-61c85010ce17?source=post_page-----a06659ca6668--------------------------------)
    [## Recommendation System in Python: LightFM'
  prefs: []
  type: TYPE_NORMAL
- en: A Step-by-Step guide to building a recommender system in Python using LightFM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/recommendation-system-in-python-lightfm-61c85010ce17?source=post_page-----a06659ca6668--------------------------------)
  prefs: []
  type: TYPE_NORMAL
