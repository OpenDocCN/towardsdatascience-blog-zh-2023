["```py\ntrain_tfrecord_file = '/kaggle/input/plants-dataset/leaves.tfrecord'\nval_tfrecord_file = '/kaggle/input/plants-dataset/test_leaves.tfrecord'\n\n# Create a TFRecordDataset\ntrain_dataset = tf.data.TFRecordDataset([train_tfrecord_file])\nval_dataset = tf.data.TFRecordDataset([val_tfrecord_file])\n\n# Iterate over a few entries and print their content. Uncomment this to look at the raw data\nfor record in train_dataset.take(1):\n  example = tf.train.Example()\n  example.ParseFromString(record.numpy())\n  print(example)\n```", "```py\ndef parse_tfrecord_fn(example):\n    feature_description = {\n        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n        'image/height': tf.io.FixedLenFeature([], tf.int64),\n        'image/width': tf.io.FixedLenFeature([], tf.int64),\n        'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32),\n        'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),\n        'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),\n        'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),\n        'image/object/class/label': tf.io.VarLenFeature(tf.int64),\n    }\n\n    parsed_example = tf.io.parse_single_example(example, feature_description)\n\n    # Decode the JPEG image and normalize the pixel values to the [0, 255] range.\n    img = tf.image.decode_jpeg(parsed_example['image/encoded'], channels=3) # Returned as uint8\n\n    # Get the bounding box coordinates and class labels.\n    xmin = tf.sparse.to_dense(parsed_example['image/object/bbox/xmin'])\n    xmax = tf.sparse.to_dense(parsed_example['image/object/bbox/xmax'])\n    ymin = tf.sparse.to_dense(parsed_example['image/object/bbox/ymin'])\n    ymax = tf.sparse.to_dense(parsed_example['image/object/bbox/ymax'])\n    labels = tf.sparse.to_dense(parsed_example['image/object/class/label'])\n\n    # Stack the bounding box coordinates to create a [num_boxes, 4] tensor.\n    rel_boxes = tf.stack([xmin, ymin, xmax, ymax], axis=-1)\n    boxes = keras_cv.bounding_box.convert_format(rel_boxes, source='rel_xyxy', target='xyxy', images=img)\n\n    # Create the final dictionary.\n    image_dataset = {\n        'images': img,\n        'bounding_boxes': {\n            'classes': labels,\n            'boxes': boxes\n        }\n    }\n    return image_dataset\n```", "```py\nimage_dataset = {\n  \"images\": [width, height, channels],\n  bounding_boxes = {\n    \"classes\": [num_boxes],\n    \"boxes\": [num_boxes, 4]\n  }\n}\n```", "```py\ntrain_dataset = train_dataset.map(parse_tfrecord_fn)\nval_dataset = val_dataset.map(parse_tfrecord_fn)\n\n# Inspecting the data\nfor data in train_dataset.take(1):\n    print(data)\n```", "```py\n# Batching\nBATCH_SIZE = 32\n# Adding autotune for pre-fetching\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\ntrain_dataset = train_dataset.ragged_batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\nval_dataset = val_dataset.ragged_batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n\nNUM_ROWS = 4\nNUM_COLS = 8\nIMG_SIZE = 416\nBBOX_FORMAT = \"xyxy\"\n```", "```py\naugmenter = keras.Sequential(\n    [\n        keras_cv.layers.JitteredResize(\n            target_size=(IMG_SIZE, IMG_SIZE), scale_factor=(0.8, 1.25), bounding_box_format=BBOX_FORMAT\n        ),\n        keras_cv.layers.RandomFlip(mode=\"horizontal_and_vertical\", bounding_box_format=BBOX_FORMAT),\n        keras_cv.layers.RandomRotation(factor=0.25, bounding_box_format=BBOX_FORMAT),\n        keras_cv.layers.RandomSaturation(factor=(0.4, 0.6)),\n        keras_cv.layers.RandomHue(factor=0.2, value_range=[0,255])\n    ]\n)\n\ntrain_dataset = train_dataset.map(augmenter, num_parallel_calls=tf.data.AUTOTUNE)\n```", "```py\n# Resize and pad images\ninference_resizing = keras_cv.layers.Resizing(\n    IMG_SIZE, IMG_SIZE, pad_to_aspect_ratio=True, bounding_box_format=BBOX_FORMAT\n)\n\nval_dataset = val_dataset.map(inference_resizing, num_parallel_calls=tf.data.AUTOTUNE)\n```", "```py\nclass_mapping = {\n    1: 'Apple Scab Leaf',\n    2: 'Apple leaf',\n    3: 'Apple rust leaf',\n    4: 'Bell_pepper leaf',\n    5: 'Bell_pepper leaf spot',\n    6: 'Blueberry leaf',\n    7: 'Cherry leaf',\n    8: 'Corn Gray leaf spot',\n    9: 'Corn leaf blight',\n    10: 'Corn rust leaf',\n    11: 'Peach leaf',\n    12: 'Potato leaf',\n    13: 'Potato leaf early blight',\n    14: 'Potato leaf late blight',\n    15: 'Raspberry leaf',\n    16: 'Soyabean leaf',\n    17: 'Soybean leaf',\n    18: 'Squash Powdery mildew leaf',\n    19: 'Strawberry leaf',\n    20: 'Tomato Early blight leaf',\n    21: 'Tomato Septoria leaf spot',\n    22: 'Tomato leaf',\n    23: 'Tomato leaf bacterial spot',\n    24: 'Tomato leaf late blight',\n    25: 'Tomato leaf mosaic virus',\n    26: 'Tomato leaf yellow virus',\n    27: 'Tomato mold leaf',\n    28: 'Tomato two spotted spider mites leaf',\n    29: 'grape leaf',\n    30: 'grape leaf black rot'\n}\n\ndef visualize_dataset(inputs, value_range, rows, cols, bounding_box_format):\n    inputs = next(iter(inputs.take(1)))\n    images, bounding_boxes = inputs[\"images\"], inputs[\"bounding_boxes\"]\n    visualization.plot_bounding_box_gallery(\n        images,\n        value_range=value_range,\n        rows=rows,\n        cols=cols,\n        y_true=bounding_boxes,\n        scale=5,\n        font_scale=0.7,\n        bounding_box_format=bounding_box_format,\n        class_mapping=class_mapping,\n    )\n\n# Visualize training set\nvisualize_dataset(\n    train_dataset, bounding_box_format=BBOX_FORMAT, value_range=(0, 255), rows=NUM_ROWS, cols=NUM_COLS\n)\n\n# Visualize validation set\nvisualize_dataset(\n    val_dataset, bounding_box_format=BBOX_FORMAT, value_range=(0, 255), rows=NUM_ROWS, cols=NUM_COLS\n)\n```", "```py\ndef dict_to_tuple(inputs):\n    return inputs[\"images\"], bounding_box.to_dense(\n        inputs[\"bounding_boxes\"], max_boxes=32\n    )\n\ntrain_dataset = train_dataset.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\nvalidation_dataset = val_dataset.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n```", "```py\nbase_lr = 0.0001\n# including a global_clipnorm is extremely important in object detection tasks\noptimizer_Adam = tf.keras.optimizers.Adam(\n    learning_rate=base_lr,\n    global_clipnorm=10.0\n)\n```", "```py\ncoco_metrics = keras_cv.metrics.BoxCOCOMetrics(\n    bounding_box_format=BBOX_FORMAT, evaluate_freq=5\n)\n```", "```py\nclass VisualizeDetections(keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs):\n        if (epoch+1)%5==0:\n            visualize_detections(\n                self.model, bounding_box_format=BBOX_FORMAT, dataset=val_dataset, rows=NUM_ROWS, cols=NUM_COLS\n            )\n\ncheckpoint_path=\"best-custom-model\"\n\ncallbacks_list = [\n    # Conducting early stopping to stop after 6 epochs of non-improving validation loss\n    keras.callbacks.EarlyStopping(\n        monitor=\"val_loss\",\n        patience=6,\n    ),\n\n    # Saving the best model\n    keras.callbacks.ModelCheckpoint(\n        filepath=checkpoint_path,\n        monitor=\"val_loss\",\n        save_best_only=True,\n        save_weights_only=True\n    ),\n\n    # Custom metrics printing after each epoch\n    tf.keras.callbacks.LambdaCallback(\n    on_epoch_end=lambda epoch, logs: \n        print(f\"\\nEpoch #{epoch+1} \\n\" +\n              f\"Loss: {logs['loss']:.4f} \\n\" + \n              f\"mAP: {logs['MaP']:.4f} \\n\" + \n              f\"Validation Loss: {logs['val_loss']:.4f} \\n\" + \n              f\"Validation mAP: {logs['val_MaP']:.4f} \\n\") \n    ),\n\n    # Visualizing results after each five epochs\n    VisualizeDetections()\n]\n```", "```py\n# Building a RetinaNet model with a backbone trained on coco datset\ndef create_model():        \n    model = keras_cv.models.RetinaNet.from_preset(\n        \"yolo_v8_m_backbone_coco\",\n        num_classes=len(class_mapping),\n        bounding_box_format=BBOX_FORMAT\n    )\n    return model\n\nmodel = create_model()\n```", "```py\n# Using focal classification loss and smoothl1 box loss with coco metrics\nmodel.compile(\n    classification_loss=\"focal\",\n    box_loss=\"smoothl1\",\n    optimizer=optimizer_Adam,\n    metrics=[coco_metrics]\n)\n\nhistory = model.fit(\n    train_dataset,\n    validation_data=validation_dataset,\n    epochs=40,\n    callbacks=callbacks_list,\n    verbose=0,\n)\n```", "```py\n# Create model with the weights of the best model\nmodel = create_model()\nmodel.load_weights(checkpoint_path)\n\n# Customizing non-max supression of model prediction. I found these numbers to work fairly well\nmodel.prediction_decoder = keras_cv.layers.MultiClassNonMaxSuppression(\n    bounding_box_format=BBOX_FORMAT,\n    from_logits=True,\n    iou_threshold=0.2,\n    confidence_threshold=0.6,\n)\n\n# Visuaize on validation set\nvisualize_detections(model, dataset=val_dataset, bounding_box_format=BBOX_FORMAT, rows=NUM_ROWS, cols=NUM_COLS)\n```"]