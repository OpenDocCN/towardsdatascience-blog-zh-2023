- en: Summing Coin Values in Images using Lang-SAM and Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/coin-counting-using-lang-sam-b469827808a7?source=collection_archive---------9-----------------------#2023-10-03](https://towardsdatascience.com/coin-counting-using-lang-sam-b469827808a7?source=collection_archive---------9-----------------------#2023-10-03)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@gamzsaglam?source=post_page-----b469827808a7--------------------------------)[![Gamze
    Zorlubas](../Images/30d8e1604a22dc6f1bd3ef96c8092cc1.png)](https://medium.com/@gamzsaglam?source=post_page-----b469827808a7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b469827808a7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b469827808a7--------------------------------)
    [Gamze Zorlubas](https://medium.com/@gamzsaglam?source=post_page-----b469827808a7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd24f99cbdd78&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcoin-counting-using-lang-sam-b469827808a7&user=Gamze+Zorlubas&userId=d24f99cbdd78&source=post_page-d24f99cbdd78----b469827808a7---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b469827808a7--------------------------------)
    ·10 min read·Oct 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb469827808a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcoin-counting-using-lang-sam-b469827808a7&user=Gamze+Zorlubas&userId=d24f99cbdd78&source=-----b469827808a7---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb469827808a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcoin-counting-using-lang-sam-b469827808a7&source=-----b469827808a7---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: In the latest developments of computer vision, image segmentation has seen impressive
    progress. A standout example is the “Segment Anything”Model (SAM), a dynamic deep-learning
    tool that predicts object masks from images using input prompts. Thanks to its
    advanced encoding and decoding capabilities, SAM can manage diverse segmentation
    challenges, proving invaluable for both researchers and developers.
  prefs: []
  type: TYPE_NORMAL
- en: Lang-SAM is a project built on SAM. It extracts the masks of all instances of
    the objects within the image we want with a text prompt. It intelligently incorporates
    textual descriptions, bridging the gap between natural language processing and
    computer vision. This fusion allows for more context-aware, precise, and detailed
    segmentations, expanding the scope of intricate imaging challenges beyond traditional
    capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having explored the capabilities of the SAM model, I found an exemplary use-case:
    estimating the total value of coins in an image that also includes various other
    objects. Let’s dive deeper into how SAM operates and examine how I’ve incorporated
    it into my coin summing project for both generating datasets and testing the neural
    network.'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Segment-Anything
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Facebook’s research team, FAIR, introduced their segmentation model, SAM, in
    2022\. What’s amazing about SAM is that it can recognize and separate parts of
    images in ways it wasn’t specifically trained for.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6ed1295bd8cc2faed82d6065a98be3a4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: SAM’s model architecture provided by Meta, downloaded by [https://segment-anything.com/](https://segment-anything.com/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'At its core, SAM has three main parts: it understands the image, takes a prompt
    or command, and then creates a mask based on that command. To train SAM, Facebook
    created the biggest image dataset ever, called SA-1B, through a detailed three-step
    process. In technical terms, SAM uses a system similar to other popular models,
    but with its own unique features. Sometimes when given a vague command, it makes
    multiple guesses and picks the best one. In tests, SAM did better than other models
    on 23 different datasets. They even combined SAM with another tool for tasks like
    finding and highlighting specific objects in images.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Although SAM was trained with a text encoder for text prompts, Meta hasn’t
    released the weights with text encoder. Therefore, only box or point prompts are
    available in the current public model.**'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Language-Segment-Anything (Lang-SAM)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To overcome the block of text prompts of SAM, Luca Medeiras made an open-source
    project called [Language-Segment-Anything (Lang-SAM)](https://github.com/luca-medeiros/lang-segment-anything/tree/main).
    Lang-SAM deploys GroundingDino and SAM sequentially. GroundingDino is a text-to-bounding
    box model to which user feeds an image and a text prompt; the model finds the
    masks of those objects based on the text prompt. These bounding boxes are then
    used as input prompts for the SAM model, which generates precise segmentation
    masks for the identified objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code snippet for running Lang-SAM in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Using the given code above, I made a test of segmentation of bicycles in an
    image. Results are visualized in the figure below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7ba5ef5077ca262ef8595466b2f3e893.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Example segmentation result of Lang-SAM — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use-case for Lang-SAM: Counting the sum of coins'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/d17dbba0ca3aa37fcb96ce499b19cc08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Workflow of coin summing — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s firstly decide on the worklow of the coin counting. As a general idea,
    we will have images in which there are various coins.
  prefs: []
  type: TYPE_NORMAL
- en: As a first step of the workflow, we can segment each coin in input image. This
    step can be done by using Lang-SAM as it enables us simply enter ‘coin’ as the
    text prompt. After getting coin masks, we can use a convolutional neural network
    to estimate the classes of coins. This neural network can be a custom one which
    we train with a dataset generated using Lang-SAM. Details of the architecture
    and how I trained it is given in **Step 2**, below. In the last step, estimated
    classes are simply summed up.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Usage of Lang-SAM'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For segmenting coins in an image, I wrote the following function which takes
    an image as input and returns the masks and boxes of each coin in the image by
    using Lang-SAM model. Boxes of individual coins are used only in the visualisation
    purposes later on; thus not significant for now.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: From the input image containing multiple coins, the provided function above
    produces segmentation masks, which are showcased in the image below. However,
    the segmentation masks generated are in the image’s original resolution. Given
    that a significant portion of the segmented image, roughly 95%, is dominated by
    empty sections, this can be seen as redundant information. Such excessive data
    causes computational challenges when inputting into a neural network for subsequent
    training phases. To address this, I’ll be introducing a subsequent function to
    crop and focus on the relevant segmented regions, optimizing the data for further
    processing.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dbf501cf27da63a18333f6e904b57db5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: input and outputs of find_coin_masks function — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: I’ve created another function named `generate_coin_images`. This function starts
    by using `find_coin_mask` to get the masks in their original size. Next, it crops
    out the black area around the masks. The final mask is then resized to a standard
    size of 500x500 pixels. If the section with the coin is bigger than this size,
    it adjusts it to fit the 500x500 size, making sure we have a consistent input
    for our next steps.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `generate_coin_images` function produces coin images, as exemplified below.
    Later on we will use this function when we create the dataset to train the neural
    network and also in our test pipeline . We can say that this function is the backbone
    of the project.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e4edaed257f58eacfadb2f69270d6a02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: input and outputs of generate_coin_images function — Image by the
    author'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Creating a Coin Estimator Neural Network'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Step 2.1: Dataset Generation*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recognizing the absence of a dedicated dataset for European coins, I took the
    initiative to create one for my project. I sourced photos of six distinct European
    coin denominations from this [GitHub page](https://github.com/kaa/coins-dataset)
    : 2 euros, 1 euro, 50 cents, 20 cents, 10 cents, and 5 cents. Each image contained
    only a single coin, ensuring consistency in the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing the `generate_coin_image` function (which I previously described),
    I extracted and saved a masked version of each coin. These images were then systematically
    organized into folders based on their respective denominations.
  prefs: []
  type: TYPE_NORMAL
- en: 'For clarity, the training dataset consists of 2,739 images, distributed across
    the six classes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '2 euros: 292 images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '1 euro: 301 images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '50 cents: 747 images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '20 cents: 444 images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '10 cents: 662 images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '5 cents: 293 images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And the validation set consists of 73 images, distributed across the six classes
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '2 euros: 5 images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '1 euro: 12 images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '50 cents: 8 images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '20 cents: 17 images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '10 cents: 16 images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '5 cents: 15 images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The image below provides a visual representation of our segmentation procedure,
    showing the processing of a 1 euro coin photo for dataset creation. Post-segmentation,
    the individual coin images are stored in the ‘1e/’ directory.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dc5dda0f1d1fbadeaa359f872afa156e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Dataset generation workflow’s input and outputs — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 2.2: Training*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The architecture of the neural network comprises two main components: several
    convolutional layers designed to extract spatial features from the input image,
    and two dense layers responsible for the final classification.'
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, the network starts by takes and RGB input image with 500x500x3
    shape. As it progresses through the convolutional layers, the number of channels
    increases, with each convolution being followed by a ReLU activation. By a stride
    of 2 in these layers, the spatial dimensions of the feature maps are reduced at
    each stage, resulting in a encoding effect.
  prefs: []
  type: TYPE_NORMAL
- en: Following the convolutional stages, the spatial features are then flattened
    and passed to two fully connected layers. The output from the final layer provides
    a probability distribution across the classes, with a softmax activation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/de99e36e9f39987edc9b380251e9f72a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: NN architecture of coin estimator — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: I used Adam optimizer and Cross Entropy Loss to train the model. The training
    was continued until a point of saturation was noticed in the validation loss,
    which occurred at the 15th epoch.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 2.3: Performance benchmark*'
  prefs: []
  type: TYPE_NORMAL
- en: Upon concluding the training, I proceeded to benchmark the checkpoint from the
    final epoch utilizing the script provided below. I used the function `compute_accuracy`
    given below, which takes the model and data loader as arguments, and computes
    the percentage of accurate predictions within the given dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The subsequent computation of the average accuracy for both training and validation
    datasets are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'training set: 87%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'validation set: 95%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The validation accuracy exceeded the training accuracy, potentially due to the
    relatively smaller size of the validation set. It’s vital to note that the primary
    intent of this project is to illustrate a potential application for new segmentation
    models rather than to construct a high-performance coin estimation network. Therefore,
    a deeper analytical dive into these observations will not be conducted.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Pipeline of Coin Counting'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having trained the coin estimator network, all steps of the workflow, outlined
    in Figure 3, are now complete. Now, let’s come up with the pipeline that leverages
    both Lang-SAM and our custom Neural Network (NN) from end to end, aiming to calculate
    the total value of coins in an image.
  prefs: []
  type: TYPE_NORMAL
- en: I created a Python notebook named `coin_counter.ipynb` which guides through
    the counting steps. Just like in the process we used to create our dataset, initially
    the `generate_coin_images` function is employed to make a mask for each coin in
    the image. Then, each of these masks is individually fed into the coin estimator
    network. Finally, the predicted coin values are added up to find the total amount
    of money in the image.
  prefs: []
  type: TYPE_NORMAL
- en: Test Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The coin image shown in Figure 3 was fed into the coin counting pipeline. The
    image below, which includes estimated class overlays, is presented below. While
    there are some incorrect estimations, the overall performance is acceptable.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b9434b38d6a441ba7128792b92ccdc96.png)'
  prefs: []
  type: TYPE_IMG
- en: As mentioned earlier, the primary goal of this project is to demonstrate a potential
    application for new segmentation models that can take text input rather than to
    build a high-performance coin estimation network.
  prefs: []
  type: TYPE_NORMAL
- en: Here is my [Github repo](https://github.com/gamzez/lang-segment-anything) in
    which you can find the codes used throughout this blog.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading my blog!
  prefs: []
  type: TYPE_NORMAL
