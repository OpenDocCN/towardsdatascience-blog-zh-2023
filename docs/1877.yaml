- en: 'Introduction to the Open LLM Falcon-40B: Performance, Training Data, and Architecture'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Open LLM Falcon-40B简介：性能、训练数据和架构
- en: 原文：[https://towardsdatascience.com/introduction-to-the-open-llm-falcon-40b-performance-training-data-and-architecture-98388fa40226?source=collection_archive---------4-----------------------#2023-06-07](https://towardsdatascience.com/introduction-to-the-open-llm-falcon-40b-performance-training-data-and-architecture-98388fa40226?source=collection_archive---------4-----------------------#2023-06-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/introduction-to-the-open-llm-falcon-40b-performance-training-data-and-architecture-98388fa40226?source=collection_archive---------4-----------------------#2023-06-07](https://towardsdatascience.com/introduction-to-the-open-llm-falcon-40b-performance-training-data-and-architecture-98388fa40226?source=collection_archive---------4-----------------------#2023-06-07)
- en: Get started using Falcon-7B, Falcon-40B, and their instruct versions
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开始使用 Falcon-7B、Falcon-40B 及其指令版本
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----98388fa40226--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----98388fa40226--------------------------------)[](https://towardsdatascience.com/?source=post_page-----98388fa40226--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----98388fa40226--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----98388fa40226--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie?source=post_page-----98388fa40226--------------------------------)[![本杰明·玛丽](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----98388fa40226--------------------------------)[](https://towardsdatascience.com/?source=post_page-----98388fa40226--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----98388fa40226--------------------------------)
    [本杰明·玛丽](https://medium.com/@bnjmn_marie?source=post_page-----98388fa40226--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-the-open-llm-falcon-40b-performance-training-data-and-architecture-98388fa40226&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----98388fa40226---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----98388fa40226--------------------------------)
    ·6 min read·Jun 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F98388fa40226&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-the-open-llm-falcon-40b-performance-training-data-and-architecture-98388fa40226&user=Benjamin+Marie&userId=ad2a414578b3&source=-----98388fa40226---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-the-open-llm-falcon-40b-performance-training-data-and-architecture-98388fa40226&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----98388fa40226---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----98388fa40226--------------------------------)
    ·6分钟阅读·2023年6月7日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F98388fa40226&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-the-open-llm-falcon-40b-performance-training-data-and-architecture-98388fa40226&user=Benjamin+Marie&userId=ad2a414578b3&source=-----98388fa40226---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F98388fa40226&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-the-open-llm-falcon-40b-performance-training-data-and-architecture-98388fa40226&source=-----98388fa40226---------------------bookmark_footer-----------)![](../Images/d5fa442c9fa2a7cd9bcf139978783e6e.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F98388fa40226&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-the-open-llm-falcon-40b-performance-training-data-and-architecture-98388fa40226&source=-----98388fa40226---------------------bookmark_footer-----------)![](../Images/d5fa442c9fa2a7cd9bcf139978783e6e.png)'
- en: Photo by [Brandon](https://unsplash.com/@greener_30?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Brandon](https://unsplash.com/@greener_30?utm_source=medium&utm_medium=referral)拍摄，发布于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: The [Falcon models](https://huggingface.co/tiiuae/falcon-40b) have drawn a lot
    of attention since they have been released in May 2023.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[Falcon模型](https://huggingface.co/tiiuae/falcon-40b)自2023年5月发布以来引起了广泛关注。'
- en: They are causal large language models (LLM), or so-called “decoder-only” models,
    very much like GPT.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 它们是因果大语言模型（LLM），或所谓的“仅解码器”模型，非常类似于GPT。
- en: 'Definition: Causal Language Model'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 定义：因果语言模型
- en: ''
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Causal language modeling involves predicting the token that follows a sequence
    of tokens. During training, the model’s attention is solely directed toward the
    left context. The right context is masked. These models are usually trained on
    billion words.
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 因果语言建模涉及预测一个令牌后面的令牌。在训练期间，模型的注意力仅集中在左侧上下文上。右侧上下文被屏蔽。这些模型通常在数十亿个词上进行训练。
- en: The Falcon models are [completely free, even for commercial use (Apache 2.0
    License), since May 31st](https://www.tii.ae/news/uaes-falcon-40b-now-royalty-free).
    The Falcon models are developed and trained by the [Technology Innovation Institute
    (TII) of Abu Dhabi](https://www.tii.ae/).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Falcon模型自5月31日起[完全免费，甚至用于商业用途（Apache 2.0许可证）](https://www.tii.ae/news/uaes-falcon-40b-now-royalty-free)。Falcon模型由[阿布扎比技术创新研究所（TII）](https://www.tii.ae/)开发和训练。
- en: According to the first results, Falcon-40B, the biggest of the Falcon models,
    outperforms all the other causal LLMs, including LLaMa-65B and MPT-7B.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 根据初步结果，Falcon-40B，作为Falcon模型中最大的一个，超越了所有其他因果LLM，包括LLaMa-65B和MPT-7B。
- en: In this blog post, I introduce in detail Falcon-40B, Falcon-7B, and their instruct
    versions. We will see how they perform compared to other models, how they were
    trained, and how to run Falcon7-B on your own GPU with QLoRa.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我详细介绍了Falcon-40B、Falcon-7B及其指令版本。我们将看到它们与其他模型的表现对比、训练方式以及如何在自己的GPU上使用QLoRa运行Falcon7-B。
- en: Performance on OpenLLM
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenLLM上的表现
- en: The instruct version of Falcon-40B is ranked first on the [OpenLLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).
    The standard version is ranked second.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Falcon-40B的指令版本在[OpenLLM排行榜](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)中排名第一。标准版本排名第二。
- en: 'The OpenLLM leaderboard evaluates the performance of LLMs on 4 tasks:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: OpenLLM排行榜评估LLM在4个任务上的表现：
- en: '[AI2 Reasoning Challenge](https://arxiv.org/abs/1803.05457) (25-shot): Questions
    of grade-school science.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AI2 Reasoning Challenge](https://arxiv.org/abs/1803.05457) (25-shot)：小学科学问题。'
- en: '[HellaSwag](https://arxiv.org/abs/1905.07830) (10-shot): A commonsense inference
    benchmark.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[HellaSwag](https://arxiv.org/abs/1905.07830) (10-shot)：一个常识推理基准测试。'
- en: '[MMLU](https://arxiv.org/abs/2009.03300) (5-shot): 57 tasks in various domains
    such as maths, computer science, and law.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MMLU](https://arxiv.org/abs/2009.03300) (5-shot)：涵盖数学、计算机科学和法律等多个领域的57个任务。'
- en: '[TruthfulQA](https://arxiv.org/abs/2109.07958) (0-shot): A benchmark that evaluates
    how truthful is the model when answering questions.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TruthfulQA](https://arxiv.org/abs/2109.07958) (0-shot)：一个评估模型回答问题时真实性的基准。'
- en: Falcon-40B outperforms Meta AI’s LLaMa-65B on all these tasks.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Falcon-40B在所有这些任务上都超越了Meta AI的LLaMa-65B。
- en: Falcon RefinedWeb
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Falcon RefinedWeb
- en: The Falcon models were mainly trained on the [Falcon RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)
    dataset. It was also created by TII and is distributed under an Apache 2.0 license.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Falcon模型主要在[Falcon RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)数据集上进行训练。它也是由TII创建的，并在Apache
    2.0许可证下分发。
- en: RefinedWeb was extracted from CommonCrawl and has been thoroughly curated. TII
    claims it is multimodal-friendly since they preserved links and alt texts of images.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: RefinedWeb从CommonCrawl中提取，并经过彻底策划。TII声称它对多模态友好，因为保留了图片的链接和替代文本。
- en: 'In the dataset card published in the Hugging Face Hub, TII wrote: “***This
    public extract*** *[…]*”. To me, it is thus unclear whether the Falcon models
    have been trained on this public version of the dataset, which is only an “extract”,
    or whether they have used a bigger internal version.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hugging Face Hub发布的数据集卡片中，TII写道：“***这个公开提取*** *[…]*”。对我来说，因此不清楚Falcon模型是否在这个仅为“提取”的公开版本数据集上进行训练，还是使用了更大的内部版本。
- en: This extract requires 2.8 Tb of hard drive space to be unpacked.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这个提取文件解压需要2.8Tb的硬盘空间。
- en: 'Since it is available in the Hugging Face Hub, you only need to run the following
    lines to start using it:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它在Hugging Face Hub中可用，你只需运行以下代码即可开始使用：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Note: You need the “datasets” library. If you don’t have it, you can install
    it with “pip install datasets”.*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：你需要“datasets”库。如果没有，你可以通过“pip install datasets”安装。*'
- en: RefinedWeb was combined with curated corpora to train the Falcon models.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: RefinedWeb与策划的语料库结合，以训练Falcon模型。
- en: 'This dataset represents 75% of the pre-training data of the Falcon models.
    It covers only English. To add more languages, they have also prepared the “RefinedWeb-Europe”
    which covers several European languages: German, Spanish, French, Italian, Portuguese,
    Polish, Dutch, Romanian, Czech, and Swedish.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集代表了Falcon模型预训练数据的75%。它仅涵盖英语。为了增加更多语言，他们还准备了“RefinedWeb-Europe”，涵盖了多种欧洲语言：德语、西班牙语、法语、意大利语、葡萄牙语、波兰语、荷兰语、罗马尼亚语、捷克语和瑞典语。
- en: 'Finally, to cover more genres and domains, they added corpora of books, conversations
    (e.g., from Reddit), code, technical reports, and scientific papers (e.g., from
    arXiv). *Note: They didn’t disclose the source for “code”. It is also unclear
    what are the licenses of the datasets they compiled.*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了覆盖更多的体裁和领域，他们添加了书籍、对话（例如，来自Reddit）、代码、技术报告和科学论文（例如，来自arXiv）的语料库。*注意：他们没有透露“代码”的来源。也不清楚他们编制的数据集的许可证是什么。*
- en: In total, that’s 1,500 billion tokens used to pre-trained the Falcon models.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 总共，这相当于1500亿个标记用于预训练Falcon模型。
- en: Pre-Training of Falcon-40B and Falcon-7B
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Falcon-40B和Falcon-7B的预训练
- en: 'For pre-training, they used:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于预训练，他们使用了：
- en: Rotary positional embeddings ([Su et al., 2021](https://arxiv.org/abs/2104.09864))
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 旋转位置嵌入 ([Su et al., 2021](https://arxiv.org/abs/2104.09864))
- en: Multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)) and FlashAttention
    ([Dao et al., 2022](https://arxiv.org/abs/2205.14135))
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)) 和 FlashAttention
    ([Dao et al., 2022](https://arxiv.org/abs/2205.14135))
- en: Parallel attention/MLP with two-layer norms
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行注意力/MLP与双层规范
- en: 'The Falcon-40B has the following architecture:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Falcon-40B具有以下架构：
- en: 'Layers: 60'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层数：60
- en: 'Embedding dimensions: 8,192'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入维度：8,192
- en: 'Heads: 64'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 头数：64
- en: 'Vocabulary size: 65,024'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词汇表大小：65,024
- en: 'Sequence length: 2,048'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列长度：2,048
- en: This is very similar to the architecture of LLaMa, except that the vocabulary
    is twice bigger.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这与LLaMa的架构非常相似，只是词汇表大了一倍。
- en: In my opinion, the sequence length is quite short at a time when we see LLMs
    accepting sequences of more than 10,000 tokens, such as GPT-4 and [Claude](https://www.anthropic.com/index/introducing-claude).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在我看来，序列长度在我们看到LLM接受超过10,000个标记的情况下，如GPT-4和[Claude](https://www.anthropic.com/index/introducing-claude)，显得相当短。
- en: 'The Falcon-7B has a smaller architecture that enables its fine-tuning on consumer
    hardware. The only differences with the 40B version are that the number of layers
    and embedding dimensions are halved:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Falcon-7B有一个较小的架构，使其能够在消费者硬件上进行微调。与40B版本唯一的区别是层数和嵌入维度减少了一半：
- en: 'Layers: 60'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层数：60
- en: 'Embedding dimensions: 4,544'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入维度：4,544
- en: Both versions were trained with bfloat16 precision and AdamW. They used AWS
    SageMaker with 384 A100 40GB GPUs in P4d instances but didn’t disclose yet how
    long the training lasted.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 两个版本都使用了bfloat16精度和AdamW。它们使用了AWS SageMaker和384个A100 40GB GPU的P4d实例，但尚未透露训练持续了多长时间。
- en: Instruct versions of Falcon-40B/7B
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Falcon-40B/7B的指令版本
- en: The instruct versions of [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b-instruct)
    and [7B](https://huggingface.co/tiiuae/falcon-7b-instruct) perform even better.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[Falcon-40B](https://huggingface.co/tiiuae/falcon-40b-instruct) 和 [7B](https://huggingface.co/tiiuae/falcon-7b-instruct)
    的指令版本表现更好。'
- en: Falcon-40B-Instruct was trained on AWS SageMaker, utilizing P4d instances equipped
    with 64 A100 40GB GPUs. For Falcon-7B-Instruct, they only used 32 A100.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Falcon-40B-Instruct在AWS SageMaker上训练，使用了配备64个A100 40GB GPU的P4d实例。对于Falcon-7B-Instruct，他们仅使用了32个A100。
- en: They were fine-tuned on 250 million tokens of a mixture of chat/instruct datasets
    sourced from [Bai ze](https://github.com/project-baize/baize-chatbot), [GPT4all](https://github.com/nomic-ai/gpt4all),
    [GPTeacher](https://github.com/teknium1/GPTeacher), and 13 million tokens from
    the RefinedWeb corpus.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 它们在250百万个标记的聊天/指令数据集混合数据上进行了微调，这些数据来源于[Bai ze](https://github.com/project-baize/baize-chatbot)、[GPT4all](https://github.com/nomic-ai/gpt4all)、[GPTeacher](https://github.com/teknium1/GPTeacher)和1300万标记的RefinedWeb语料库。
- en: 'Bai ze is a dataset generated by ChatGPT. I would be cautious about using the
    instruct version of Falcon models in commercial applications. As per OpenAI’s
    [terms of use](https://openai.com/policies/terms-of-use):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Bai ze是由ChatGPT生成的数据集。我会对在商业应用中使用Falcon模型的指令版本保持谨慎。根据OpenAI的[使用条款](https://openai.com/policies/terms-of-use)：
- en: “Restrictions. You may not […] (iii) use output from the Services to develop
    models that compete with OpenAI”
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “限制。你不得 […] (iii) 使用服务的输出开发与OpenAI竞争的模型”
- en: “Services” includes ChatGPT. And Falcon-40B is a model that can “compete” with
    OpenAI’s GPT models.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: “服务”包括ChatGPT。而Falcon-40B是一个可以与OpenAI的GPT模型“竞争”的模型。
- en: How to Use Falcon-7B on Your GPU with QLoRa
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在你的 GPU 上使用 Falcon-7B 和 QLoRa
- en: 'In a previous article, I introduced QLoRa to fine-tune LLMs on consumer hardware:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一篇文章中，我介绍了 QLoRa 在消费级硬件上微调 LLMs 的方法：
- en: '[](/qlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b?source=post_page-----98388fa40226--------------------------------)
    [## QLoRa: Fine-Tune a Large Language Model on Your GPU'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[## QLoRa: 在你的 GPU 上微调大型语言模型](/qlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b?source=post_page-----98388fa40226--------------------------------)'
- en: Fine-tuning models with billions of parameters is now possible on consumer hardware
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 现在可以在消费级硬件上对具有数十亿参数的模型进行微调。
- en: towardsdatascience.com](/qlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b?source=post_page-----98388fa40226--------------------------------)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/qlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b?source=post_page-----98388fa40226--------------------------------)'
- en: You can follow the same steps for Falcon-7B but it won’t work on the free instance
    of Google Colab. The model requires too much CPU RAM.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以对 Falcon-7B 按相同的步骤操作，但它不会在 Google Colab 的免费实例上运行。模型需要过多的 CPU 内存。
- en: If you have 32 Gb of RAM in your computer, this should work. If you don’t have
    that much RAM, you will have to opt for cloud computing or Google Colab Pro, for
    instance.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的计算机有 32 Gb 的 RAM，这应该可以工作。如果没有这么多 RAM，你将需要选择云计算或 Google Colab Pro 等方式。
- en: Once you have an environment that can support Falcon-7B, there are still some
    minor modifications to perform to my QLoRa tutorial.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了支持 Falcon-7B 的环境，还有一些小修改需要对我的 QLoRa 教程进行调整。
- en: 'First, you must install “einops”:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你必须安装“einops”：
- en: '[PRE1]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, modify the loading of the model as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，按如下方式修改模型的加载：
- en: '[PRE2]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In this line, “trust_remote_code=True” is necessary. This is the way Hugging
    Face gets your consent that some code is directly executed on your machine by
    the model. Here, Falcon runs a configuration script.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一行中，“trust_remote_code=True” 是必要的。这是 Hugging Face 获得你同意一些代码由模型直接在你的机器上执行的方式。在这里，Falcon
    运行一个配置脚本。
- en: Other than that, everything else should work the same as in my tutorial.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，其它方面应该与我的教程相同。
- en: 'If you don’t want to use QLoRa and have access to a GPU cluster, the standard
    way of loading and running Falcon-7B/[Falcon-40B would be as described in the
    Hugging Face models’ cards](https://huggingface.co/tiiuae/falcon-40b):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不想使用 QLoRa 并且有 GPU 集群访问权限，标准的加载和运行 Falcon-7B/[Falcon-40B 的方法可以参考 Hugging
    Face 模型卡片](https://huggingface.co/tiiuae/falcon-40b)。
- en: '[PRE3]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Conclusion
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The Falcon models are pre-trained LLMs. You can use them for any natural language
    processing task if you have the data to fine-tune them. Note that, even without
    fine-tuning, the standard (non-instruct) versions already perform very well for
    many tasks as shown on the [OpenLLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
    for answering questions from various domains and for commonsense inference.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Falcon 模型是预训练的 LLMs。如果你有数据进行微调，你可以将它们用于任何自然语言处理任务。请注意，即使不进行微调，标准（非指令型）版本在许多任务中已经表现得非常好，如在
    [OpenLLM 排行榜](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)上所示，能够回答来自各个领域的问题并进行常识推理。
- en: The “instruct” versions of the Falcon models are already fine-tuned. They behave
    like ChatGPT, i.e., a chatbot with general knowledge.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Falcon 模型的“指令”版本已经过微调。它们表现得像 ChatGPT，即具有一般知识的聊天机器人。
- en: 'The Falcon models are also very interesting alternatives to the popular LLaMa
    model. Falcon-40B is:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Falcon 模型也是流行的 LLaMa 模型的非常有趣的替代品。Falcon-40B 是：
- en: '**Smaller**: LLaMa is 65 billion parameters while Falcon-40B is only 40 billion
    parameters, so it requires less memory.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更小**：LLaMa 有 65 亿参数，而 Falcon-40B 只有 40 亿参数，因此需要的内存较少。'
- en: '**Better**: On the OpenLLM leaderboard, Falcon-40B is ranked first.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更好**：在 OpenLLM 排行榜上，Falcon-40B 排名第一。'
- en: '**Free**: Falcon models are distributed under an Apache 2.0 license allowing
    commercial use while LLaMa can only be used for research purposes.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**免费**：Falcon 模型在 Apache 2.0 许可证下分发，允许商业使用，而 LLaMa 只能用于研究目的。'
- en: If you are interested in getting more information about these models, keep an
    eye on this blog post. TII will release a scientific paper/technical paper describing
    in more detail what they did. I’ll drop the link here once it is online.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对这些模型感兴趣，关注这篇博客文章。TII 将发布一篇科学论文/技术论文，更详细地描述他们的工作。一旦上线，我会在这里提供链接。
- en: '*If you like this article and would be interested to read the next ones, the
    best way to support my work is to become a Medium member using this link:*'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你喜欢这篇文章并希望阅读接下来的文章，支持我的最佳方式是使用这个链接成为 Medium 会员：*'
- en: '[](https://medium.com/@bnjmn_marie/membership?source=post_page-----98388fa40226--------------------------------)
    [## Join Medium with my referral link - Benjamin Marie'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie/membership?source=post_page-----98388fa40226--------------------------------)
    [## 通过我的推荐链接加入 Medium - 本杰明·玛丽'
- en: Join Our AI Community and Get Access to Cutting-Edge Research This blog aims
    to demystify recent advances in AI for…
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加入我们的人工智能社区，获取前沿研究的访问权限。这个博客旨在揭示人工智能领域的最新进展。
- en: medium.com](https://medium.com/@bnjmn_marie/membership?source=post_page-----98388fa40226--------------------------------)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@bnjmn_marie/membership?source=post_page-----98388fa40226--------------------------------)
- en: '*If you are already a member and want to support this work,* [*just follow
    me on Medium*](https://medium.com/@bnjmn_marie)*.*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你已经是会员并希望支持这项工作，* [*请在 Medium 上关注我*](https://medium.com/@bnjmn_marie)*。*'
