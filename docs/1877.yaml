- en: 'Introduction to the Open LLM Falcon-40B: Performance, Training Data, and Architecture'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/introduction-to-the-open-llm-falcon-40b-performance-training-data-and-architecture-98388fa40226?source=collection_archive---------4-----------------------#2023-06-07](https://towardsdatascience.com/introduction-to-the-open-llm-falcon-40b-performance-training-data-and-architecture-98388fa40226?source=collection_archive---------4-----------------------#2023-06-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Get started using Falcon-7B, Falcon-40B, and their instruct versions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----98388fa40226--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----98388fa40226--------------------------------)[](https://towardsdatascience.com/?source=post_page-----98388fa40226--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----98388fa40226--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----98388fa40226--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-the-open-llm-falcon-40b-performance-training-data-and-architecture-98388fa40226&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----98388fa40226---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----98388fa40226--------------------------------)
    ·6 min read·Jun 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F98388fa40226&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-the-open-llm-falcon-40b-performance-training-data-and-architecture-98388fa40226&user=Benjamin+Marie&userId=ad2a414578b3&source=-----98388fa40226---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F98388fa40226&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-the-open-llm-falcon-40b-performance-training-data-and-architecture-98388fa40226&source=-----98388fa40226---------------------bookmark_footer-----------)![](../Images/d5fa442c9fa2a7cd9bcf139978783e6e.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Brandon](https://unsplash.com/@greener_30?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The [Falcon models](https://huggingface.co/tiiuae/falcon-40b) have drawn a lot
    of attention since they have been released in May 2023.
  prefs: []
  type: TYPE_NORMAL
- en: They are causal large language models (LLM), or so-called “decoder-only” models,
    very much like GPT.
  prefs: []
  type: TYPE_NORMAL
- en: 'Definition: Causal Language Model'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Causal language modeling involves predicting the token that follows a sequence
    of tokens. During training, the model’s attention is solely directed toward the
    left context. The right context is masked. These models are usually trained on
    billion words.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Falcon models are [completely free, even for commercial use (Apache 2.0
    License), since May 31st](https://www.tii.ae/news/uaes-falcon-40b-now-royalty-free).
    The Falcon models are developed and trained by the [Technology Innovation Institute
    (TII) of Abu Dhabi](https://www.tii.ae/).
  prefs: []
  type: TYPE_NORMAL
- en: According to the first results, Falcon-40B, the biggest of the Falcon models,
    outperforms all the other causal LLMs, including LLaMa-65B and MPT-7B.
  prefs: []
  type: TYPE_NORMAL
- en: In this blog post, I introduce in detail Falcon-40B, Falcon-7B, and their instruct
    versions. We will see how they perform compared to other models, how they were
    trained, and how to run Falcon7-B on your own GPU with QLoRa.
  prefs: []
  type: TYPE_NORMAL
- en: Performance on OpenLLM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The instruct version of Falcon-40B is ranked first on the [OpenLLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).
    The standard version is ranked second.
  prefs: []
  type: TYPE_NORMAL
- en: 'The OpenLLM leaderboard evaluates the performance of LLMs on 4 tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[AI2 Reasoning Challenge](https://arxiv.org/abs/1803.05457) (25-shot): Questions
    of grade-school science.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[HellaSwag](https://arxiv.org/abs/1905.07830) (10-shot): A commonsense inference
    benchmark.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[MMLU](https://arxiv.org/abs/2009.03300) (5-shot): 57 tasks in various domains
    such as maths, computer science, and law.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TruthfulQA](https://arxiv.org/abs/2109.07958) (0-shot): A benchmark that evaluates
    how truthful is the model when answering questions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Falcon-40B outperforms Meta AI’s LLaMa-65B on all these tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Falcon RefinedWeb
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Falcon models were mainly trained on the [Falcon RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)
    dataset. It was also created by TII and is distributed under an Apache 2.0 license.
  prefs: []
  type: TYPE_NORMAL
- en: RefinedWeb was extracted from CommonCrawl and has been thoroughly curated. TII
    claims it is multimodal-friendly since they preserved links and alt texts of images.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the dataset card published in the Hugging Face Hub, TII wrote: “***This
    public extract*** *[…]*”. To me, it is thus unclear whether the Falcon models
    have been trained on this public version of the dataset, which is only an “extract”,
    or whether they have used a bigger internal version.'
  prefs: []
  type: TYPE_NORMAL
- en: This extract requires 2.8 Tb of hard drive space to be unpacked.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since it is available in the Hugging Face Hub, you only need to run the following
    lines to start using it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*Note: You need the “datasets” library. If you don’t have it, you can install
    it with “pip install datasets”.*'
  prefs: []
  type: TYPE_NORMAL
- en: RefinedWeb was combined with curated corpora to train the Falcon models.
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset represents 75% of the pre-training data of the Falcon models.
    It covers only English. To add more languages, they have also prepared the “RefinedWeb-Europe”
    which covers several European languages: German, Spanish, French, Italian, Portuguese,
    Polish, Dutch, Romanian, Czech, and Swedish.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, to cover more genres and domains, they added corpora of books, conversations
    (e.g., from Reddit), code, technical reports, and scientific papers (e.g., from
    arXiv). *Note: They didn’t disclose the source for “code”. It is also unclear
    what are the licenses of the datasets they compiled.*'
  prefs: []
  type: TYPE_NORMAL
- en: In total, that’s 1,500 billion tokens used to pre-trained the Falcon models.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-Training of Falcon-40B and Falcon-7B
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For pre-training, they used:'
  prefs: []
  type: TYPE_NORMAL
- en: Rotary positional embeddings ([Su et al., 2021](https://arxiv.org/abs/2104.09864))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)) and FlashAttention
    ([Dao et al., 2022](https://arxiv.org/abs/2205.14135))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallel attention/MLP with two-layer norms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Falcon-40B has the following architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Layers: 60'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Embedding dimensions: 8,192'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Heads: 64'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vocabulary size: 65,024'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sequence length: 2,048'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is very similar to the architecture of LLaMa, except that the vocabulary
    is twice bigger.
  prefs: []
  type: TYPE_NORMAL
- en: In my opinion, the sequence length is quite short at a time when we see LLMs
    accepting sequences of more than 10,000 tokens, such as GPT-4 and [Claude](https://www.anthropic.com/index/introducing-claude).
  prefs: []
  type: TYPE_NORMAL
- en: 'The Falcon-7B has a smaller architecture that enables its fine-tuning on consumer
    hardware. The only differences with the 40B version are that the number of layers
    and embedding dimensions are halved:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Layers: 60'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Embedding dimensions: 4,544'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both versions were trained with bfloat16 precision and AdamW. They used AWS
    SageMaker with 384 A100 40GB GPUs in P4d instances but didn’t disclose yet how
    long the training lasted.
  prefs: []
  type: TYPE_NORMAL
- en: Instruct versions of Falcon-40B/7B
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The instruct versions of [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b-instruct)
    and [7B](https://huggingface.co/tiiuae/falcon-7b-instruct) perform even better.
  prefs: []
  type: TYPE_NORMAL
- en: Falcon-40B-Instruct was trained on AWS SageMaker, utilizing P4d instances equipped
    with 64 A100 40GB GPUs. For Falcon-7B-Instruct, they only used 32 A100.
  prefs: []
  type: TYPE_NORMAL
- en: They were fine-tuned on 250 million tokens of a mixture of chat/instruct datasets
    sourced from [Bai ze](https://github.com/project-baize/baize-chatbot), [GPT4all](https://github.com/nomic-ai/gpt4all),
    [GPTeacher](https://github.com/teknium1/GPTeacher), and 13 million tokens from
    the RefinedWeb corpus.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bai ze is a dataset generated by ChatGPT. I would be cautious about using the
    instruct version of Falcon models in commercial applications. As per OpenAI’s
    [terms of use](https://openai.com/policies/terms-of-use):'
  prefs: []
  type: TYPE_NORMAL
- en: “Restrictions. You may not […] (iii) use output from the Services to develop
    models that compete with OpenAI”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: “Services” includes ChatGPT. And Falcon-40B is a model that can “compete” with
    OpenAI’s GPT models.
  prefs: []
  type: TYPE_NORMAL
- en: How to Use Falcon-7B on Your GPU with QLoRa
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a previous article, I introduced QLoRa to fine-tune LLMs on consumer hardware:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/qlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b?source=post_page-----98388fa40226--------------------------------)
    [## QLoRa: Fine-Tune a Large Language Model on Your GPU'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning models with billions of parameters is now possible on consumer hardware
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/qlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b?source=post_page-----98388fa40226--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: You can follow the same steps for Falcon-7B but it won’t work on the free instance
    of Google Colab. The model requires too much CPU RAM.
  prefs: []
  type: TYPE_NORMAL
- en: If you have 32 Gb of RAM in your computer, this should work. If you don’t have
    that much RAM, you will have to opt for cloud computing or Google Colab Pro, for
    instance.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have an environment that can support Falcon-7B, there are still some
    minor modifications to perform to my QLoRa tutorial.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you must install “einops”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, modify the loading of the model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In this line, “trust_remote_code=True” is necessary. This is the way Hugging
    Face gets your consent that some code is directly executed on your machine by
    the model. Here, Falcon runs a configuration script.
  prefs: []
  type: TYPE_NORMAL
- en: Other than that, everything else should work the same as in my tutorial.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you don’t want to use QLoRa and have access to a GPU cluster, the standard
    way of loading and running Falcon-7B/[Falcon-40B would be as described in the
    Hugging Face models’ cards](https://huggingface.co/tiiuae/falcon-40b):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Falcon models are pre-trained LLMs. You can use them for any natural language
    processing task if you have the data to fine-tune them. Note that, even without
    fine-tuning, the standard (non-instruct) versions already perform very well for
    many tasks as shown on the [OpenLLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
    for answering questions from various domains and for commonsense inference.
  prefs: []
  type: TYPE_NORMAL
- en: The “instruct” versions of the Falcon models are already fine-tuned. They behave
    like ChatGPT, i.e., a chatbot with general knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Falcon models are also very interesting alternatives to the popular LLaMa
    model. Falcon-40B is:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Smaller**: LLaMa is 65 billion parameters while Falcon-40B is only 40 billion
    parameters, so it requires less memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Better**: On the OpenLLM leaderboard, Falcon-40B is ranked first.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Free**: Falcon models are distributed under an Apache 2.0 license allowing
    commercial use while LLaMa can only be used for research purposes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are interested in getting more information about these models, keep an
    eye on this blog post. TII will release a scientific paper/technical paper describing
    in more detail what they did. I’ll drop the link here once it is online.
  prefs: []
  type: TYPE_NORMAL
- en: '*If you like this article and would be interested to read the next ones, the
    best way to support my work is to become a Medium member using this link:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie/membership?source=post_page-----98388fa40226--------------------------------)
    [## Join Medium with my referral link - Benjamin Marie'
  prefs: []
  type: TYPE_NORMAL
- en: Join Our AI Community and Get Access to Cutting-Edge Research This blog aims
    to demystify recent advances in AI for…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@bnjmn_marie/membership?source=post_page-----98388fa40226--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*If you are already a member and want to support this work,* [*just follow
    me on Medium*](https://medium.com/@bnjmn_marie)*.*'
  prefs: []
  type: TYPE_NORMAL
