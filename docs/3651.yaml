- en: Evaluating RAG Applications with RAGAs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/evaluating-rag-applications-with-ragas-81d67b0ee31a?source=collection_archive---------0-----------------------#2023-12-13](https://towardsdatascience.com/evaluating-rag-applications-with-ragas-81d67b0ee31a?source=collection_archive---------0-----------------------#2023-12-13)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A framework with metrics and LLM-generated data to evaluate the performance
    of your Retrieval-Augmented Generation pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@iamleonie?source=post_page-----81d67b0ee31a--------------------------------)[![Leonie
    Monigatti](../Images/4044b1685ada53a30160b03dc78f9626.png)](https://medium.com/@iamleonie?source=post_page-----81d67b0ee31a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----81d67b0ee31a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----81d67b0ee31a--------------------------------)
    [Leonie Monigatti](https://medium.com/@iamleonie?source=post_page-----81d67b0ee31a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3a38da70d8dc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevaluating-rag-applications-with-ragas-81d67b0ee31a&user=Leonie+Monigatti&userId=3a38da70d8dc&source=post_page-3a38da70d8dc----81d67b0ee31a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----81d67b0ee31a--------------------------------)
    ·8 min read·Dec 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F81d67b0ee31a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevaluating-rag-applications-with-ragas-81d67b0ee31a&user=Leonie+Monigatti&userId=3a38da70d8dc&source=-----81d67b0ee31a---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F81d67b0ee31a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevaluating-rag-applications-with-ragas-81d67b0ee31a&source=-----81d67b0ee31a---------------------bookmark_footer-----------)![](../Images/26286ee67d773bec8a04467f80f5faae.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Stylized performance dashboard for Retrieval-Augmented Generation
  prefs: []
  type: TYPE_NORMAL
- en: 'By now, we know that building a proof of concept for a [Retrieval-Augmented
    Generation (RAG) application](https://medium.com/towards-data-science/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2)
    is easy, but making it production-ready is very difficult. Getting the RAG pipeline''s
    performance to a satisfying state is especially difficult because of the different
    components in a RAG pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Retriever component:** retrieves additional context from an external database
    for the LLM to answer the query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generator component:** generates an answer based on a prompt augmented with
    the retrieved information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When evaluating a RAG pipeline, you must evaluate both components separately
    and together to understand if and where the RAG pipeline still needs improvement.
    Additionally, to understand whether your RAG application’s performance is improving,
    you must evaluate it quantitatively. For this, you will need two ingredients:
    **An evaluation metric and an evaluation dataset.**'
  prefs: []
  type: TYPE_NORMAL
- en: Currently, determining the right evaluation metrics and collecting good validation
    data is an active research field. As this is a quickly evolving topic, we are
    currently witnessing the appearance of various approaches for RAG evaluation frameworks,
    such as the [RAG Triad of metrics](https://learn.deeplearning.ai/building-evaluating-advanced-rag/lesson/3/rag-triad-of-metrics),
    [ROUGE](https://aclanthology.org/W04-1013/), [ARES](https://arxiv.org/abs/2311.09476),
    [BLEU](https://dl.acm.org/doi/10.3115/1073083.1073135), and [RAGAs](https://arxiv.org/pdf/2309.15217v1.pdf)
    [1]. This article will focus on how you can evaluate a RAG pipeline using [RAGAs](https://arxiv.org/pdf/2309.15217v1.pdf)
    [1].
  prefs: []
  type: TYPE_NORMAL
- en: What is RAGAs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RAGAs (**R**etrieval-**A**ugmented **G**eneration **As**sessment) is a framework
    ([GitHub](https://github.com/explodinggradients/ragas), [Docs](https://docs.ragas.io/en/latest/))
    that provides you with the necessary ingredients to help you evaluate your RAG
    pipeline on a component level.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What’s interesting about RAGAs is that it started out as a framework for “reference-free”
    evaluation [1]. That means, instead of having to rely on human-annotated ground
    truth labels in the evaluation dataset, RAGAs leverages LLMs under the hood to
    conduct the evaluations.
  prefs: []
  type: TYPE_NORMAL
- en: 'To evaluate the RAG pipeline, RAGAs expects the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: '`question`: The user query that is the input of the RAG pipeline. The input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`answer`: The generated answer from the RAG pipeline. The output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`contexts`: The contexts retrieved from the external knowledge source used
    to answer the `question`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ground_truths`: The ground truth answer to the `question`. This is the only
    human-annotated information. This information is only required for the metric
    `context_recall` (see [Evaluation Metrics](#c52f)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leveraging LLMs for reference-free evaluation is an active research topic. While
    using as little human-annotated data as possible makes it a cheaper and faster
    evaluation method, there is still some discussion about its shortcomings, such
    as bias [3]. However, some papers have already shown promising results [4]. For
    detailed information, see the “Related Work” section of the RAGAs [1] paper.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the framework has expanded to provide metrics and paradigms that require
    ground truth labels (e.g., `context_recall` and `answer_correctness`, see [Evaluation
    Metrics](#c52f)).
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the framework provides you with tooling for [automatic test data
    generation](https://docs.ragas.io/en/latest/concepts/testset_generation.html).
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RAGAs provide you with a few [metrics](https://docs.ragas.io/en/latest/concepts/metrics/index.html)
    to evaluate a RAG pipeline component-wise as well as end-to-end.
  prefs: []
  type: TYPE_NORMAL
- en: 'On a **component level**, RAGAs provides you with metrics to evaluate the retrieval
    component (`context_relevancy` and `context_recall`) and the generative component
    (`faithfulness` and `answer_relevancy`) separately [2]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Context precision**](https://docs.ragas.io/en/latest/concepts/metrics/context_precision.html)measures
    the signal-to-noise ratio of the retrieved context. This metric is computed using
    the `question` and the `contexts`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Context recall**](https://docs.ragas.io/en/latest/concepts/metrics/context_recall.html)measures
    if all the relevant information required to answer the question was retrieved.
    This metric is computed based on the `ground_truth` (this is the only metric in
    the framework that relies on human-annotated ground truth labels) and the `contexts`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Faithfulness**](https://docs.ragas.io/en/latest/concepts/metrics/faithfulness.html)
    measures the factual accuracy of the generated answer. The number of correct statements
    from the given contexts is divided by the total number of statements in the generated
    answer. This metric uses the `question`, `contexts`and the `answer`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Answer relevancy**](https://docs.ragas.io/en/latest/concepts/metrics/answer_relevance.html)
    measures how relevant the generated answer is to the question. This metric is
    computed using the `question` and the `answer`. For example, the answer “*France
    is in western Europe*.” to the question “*Where is France and what is it’s capital?*”
    would achieve a low answer relevancy because it only answers half of the question.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All metrics are scaled to the range [0, 1], with higher values indicating a
    better performance.
  prefs: []
  type: TYPE_NORMAL
- en: RAGAs also provides you with metrics to evaluate the RAG pipeline **end-to-end,**
    such as [answer semantic similarity](https://docs.ragas.io/en/latest/concepts/metrics/semantic_similarity.html)
    and [answer correctness](https://docs.ragas.io/en/latest/concepts/metrics/answer_correctness.html).
    This article focuses on the component-level metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating a RAG Application with RAGAs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section uses RAGAs to evaluate a minimal vanilla RAG pipeline to show you
    how to use RAGAs and to give you an intuition about its evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Make sure you have installed the required Python packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '`langchain`, `openai`, and `weaviate-client` for the RAG pipeline'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ragas` for evaluating the RAG pipeline'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Additionally, define your relevant environment variables in a .env file in your
    root directory. To obtain an OpenAI API Key, you need an OpenAI account and then
    “Create new secret key” under [API keys](https://platform.openai.com/account/api-keys).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Setting up the RAG application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before you can evaluate your RAG application, you need to set it up. We will
    use a vanilla RAG pipeline. We will keep this section short since we will use
    the same setup described in detail in the following article.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2?source=post_page-----81d67b0ee31a--------------------------------)
    [## Retrieval-Augmented Generation (RAG): From Theory to LangChain Implementation'
  prefs: []
  type: TYPE_NORMAL
- en: From the theory of the original academic paper to its Python implementation
    with OpenAI, Weaviate, and LangChain
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2?source=post_page-----81d67b0ee31a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: First, you must prepare the data by loading and chunking the documents.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Next, generate the vector embeddings for each chunk with the OpenAI embedding
    model and store them in the vector database.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Finally, set up a prompt template and the OpenAI LLM and combine them with the
    retriever component to a RAG pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Preparing the Evaluation Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As RAGAs aims to be a reference-free evaluation framework, the required preparations
    of the evaluation dataset are minimal. You will need to prepare `question` and
    `ground_truths` pairs from which you can prepare the remaining information through
    inference as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: If you are not interested in the `context_recall` metric, you don’t need to
    provide the `ground_truths` information. In this case, all you need to prepare
    are the `question`s.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the RAG application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, import all the metrics you want to use from `ragas.metrics`. Then, you
    can use the `evaluate()` function and simply pass in the relevant metrics and
    the prepared dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Below, you can see the resulting RAGAs scores for the examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/abfb4ba04ce5db876abf3ef239e61064.png)'
  prefs: []
  type: TYPE_IMG
- en: RAGAs scores context precision, context recall, faithfulness, and answer relevancy.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '`context_relevancy` (signal-to-noise ratio of the retrieved context): While
    the LLM judges all of the context as relevant for the last question, it also judges
    that most of the retrieved context for the second question is irrelevant. Depending
    on this metric, you could experiment with different numbers of retrieved contexts
    to reduce the noise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_recall`(if all the relevant information required to answer the question
    was retrieved): The LLMs evaluate that the retrieved contexts contain the relevant
    information required to answer the questions correctly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`faithfulness` (factual accuracy of the generated answer): While the LLM judges
    that the first and last questions are answered correctly, the answer to the second
    question, which wrongly states that the president did not mention Intel’s CEO,
    is judged with a faithfulness of 0.5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`answer_relevancy` (how relevant is the generated answer to the question):
    All of the generated answers are judged as fairly relevant to the questions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As mentioned in [Evaluation Data](#836f), using LLMs for reference-free evaluation
    is an active research field. I am curious to see how this topic will evolve.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building a proof-of-concept RAG application is easy, but getting its performance
    production-ready is hard. Like a machine learning project, you should evaluate
    the RAG pipeline’s performance with a validation dataset and an evaluation metric.
  prefs: []
  type: TYPE_NORMAL
- en: However, since a RAG pipeline consists of multiple components that must be evaluated
    separately and in combinations, you will require a set of evaluation metrics.
    Additionally, generating a high-quality validation dataset from human annotators
    is hard, time-consuming, and expensive.
  prefs: []
  type: TYPE_NORMAL
- en: This article has introduced the [RAGAs](https://arxiv.org/pdf/2309.15217v1.pdf)
    [1] evaluation framework. The framework proposes four evaluation metrics — `context_relevancy`,
    `context_recall`, `faithfulness` and `answer_relevancy` — that together make up
    the RAGAs score. Additionally, RAGAs leverages LLMs under the hood for reference-free
    evaluation to save costs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you have the tools to evaluate your RAG application’s performance,
    I recommend [setting up an experimentation pipeline](https://medium.com/@iamleonie/intro-to-mlops-experiment-tracking-for-machine-learning-858e432bd133)
    and start tweaking the performance with the following tuning strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439?source=post_page-----81d67b0ee31a--------------------------------)
    [## A Guide on 12 Tuning Strategies for Production-Ready RAG Applications'
  prefs: []
  type: TYPE_NORMAL
- en: How to improve the performance of your Retrieval-Augmented Generation (RAG)
    pipeline with these “hyperparameters” and…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439?source=post_page-----81d67b0ee31a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code to generate this dataset in [this GitHub repository](https://github.com/weaviate/recipes/blob/main/evaluation/RAGAs-RAG-langchain.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Enjoyed This Story?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[*Subscribe for free*](https://medium.com/subscribe/@iamleonie) *to get notified
    when I publish a new story.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@iamleonie/subscribe?source=post_page-----81d67b0ee31a--------------------------------)
    [## Get an email whenever Leonie Monigatti publishes.'
  prefs: []
  type: TYPE_NORMAL
- en: Get an email whenever Leonie Monigatti publishes. By signing up, you will create
    a Medium account if you don’t already…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@iamleonie/subscribe?source=post_page-----81d67b0ee31a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*Find me on* [*LinkedIn*](https://www.linkedin.com/in/804250ab/),[*Twitter*](https://twitter.com/helloiamleonie)*,
    and* [*Kaggle*](https://www.kaggle.com/iamleonie)*!*'
  prefs: []
  type: TYPE_NORMAL
- en: Disclaimer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I am a Developer Advocate at Weaviate, an open source vector database, at the
    time of this writing.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Es, S., James, J., Espinosa-Anke, L., & Schockaert, S. (2023). RAGAs: Automated
    Evaluation of Retrieval Augmented Generation. [*arXiv preprint arXiv:2309.15217*](https://arxiv.org/pdf/2309.15217v1.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] RAGAs Documentation (2023). [Documentation](https://docs.ragas.io/en/latest/index.html)
    (accessed Dec 11, 2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Wang, P., Li, L., Chen, L., Zhu, D., Lin, B., Cao, Y., … & Sui, Z. (2023).
    Large language models are not fair evaluators. [*arXiv preprint arXiv:2305.17926*.](https://arxiv.org/abs/2305.17926)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Liu, Y., Iter, D., Xu, Y., Wang, S., Xu, R., & Zhu, C. (2023). G-eval:
    Nlg evaluation using gpt-4 with better human alignment, may 2023\. [*arXiv preprint
    arXiv:2303.16634*, *6*.](https://arxiv.org/abs/2303.16634)'
  prefs: []
  type: TYPE_NORMAL
- en: Images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If not otherwise stated, all images are created by the author.
  prefs: []
  type: TYPE_NORMAL
