- en: 'PID Controller Optimization: A Gradient Descent Approach'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/pid-controller-optimization-a-gradient-descent-approach-58876e14eef2?source=collection_archive---------4-----------------------#2023-08-01](https://towardsdatascience.com/pid-controller-optimization-a-gradient-descent-approach-58876e14eef2?source=collection_archive---------4-----------------------#2023-08-01)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Using machine learning to solve engineering optimization problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@callum.bruce1?source=post_page-----58876e14eef2--------------------------------)[![Callum
    Bruce](../Images/4833a199a9449434777fdf5ce913a9cb.png)](https://medium.com/@callum.bruce1?source=post_page-----58876e14eef2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----58876e14eef2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----58876e14eef2--------------------------------)
    [Callum Bruce](https://medium.com/@callum.bruce1?source=post_page-----58876e14eef2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa9c915837ab3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpid-controller-optimization-a-gradient-descent-approach-58876e14eef2&user=Callum+Bruce&userId=a9c915837ab3&source=post_page-a9c915837ab3----58876e14eef2---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----58876e14eef2--------------------------------)
    ·10 min read·Aug 1, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F58876e14eef2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpid-controller-optimization-a-gradient-descent-approach-58876e14eef2&user=Callum+Bruce&userId=a9c915837ab3&source=-----58876e14eef2---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F58876e14eef2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpid-controller-optimization-a-gradient-descent-approach-58876e14eef2&source=-----58876e14eef2---------------------bookmark_footer-----------)![](../Images/4350abf8767e1ec8d3ce9a121cb47b01.png)'
  prefs: []
  type: TYPE_NORMAL
- en: The gradient descent algorithm steps downhill to minimize a cost function
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning. Deep learning. AI. More and more people use these technologies
    every day. This has largely been driven by the rise of Large Language Models deployed
    by the likes of ChatGPT, Bard and others. Despite their widespread use, relatively
    few people are familiar with the methods underpinning these technologies.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we dive into one of the fundamental methods deployed in machine
    learning: the gradient descent algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of looking at gradient descent through the lens of neural networks,
    where it is used to optimize network weights and biases, we will instead examine
    the algorithm as a tool for solving classic engineering optimization problems.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, we will use gradient descent to tune the gains of a PID (Proportional-Integral-Derivative)
    controller for a car cruise control system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The motivation for following this approach is twofold:'
  prefs: []
  type: TYPE_NORMAL
- en: First, optimizing the weights and biases in a neural network is a high-dimension
    problem. There are a lot of moving parts and I think these distract from the underlying
    utility of gradient descent for solving optimization problems.
  prefs: []
  type: TYPE_NORMAL
