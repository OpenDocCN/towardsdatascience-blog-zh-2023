["```py\nupdates, optimizer_state = optimizer.update(grads, optimizer_state)\n```", "```py\n# target network predictions\nself.model.apply(target_net_params, None, state)\n# online network predictions\nself.model.apply(online_net_params, None, state)\n```", "```py\nN_ACTIONS = 2\nNEURONS_PER_LAYER = [64, 64, 64, N_ACTIONS]\nonline_key, target_key = vmap(random.PRNGKey)(jnp.arange(2) + RANDOM_SEED)\n\n@hk.transform\ndef model(x):\n    # simple multi-layer perceptron\n    mlp = hk.nets.MLP(output_sizes=NEURONS_PER_LAYER)\n    return mlp(x)\n\nonline_net_params = model.init(online_key, jnp.zeros((STATE_SHAPE,)))\ntarget_net_params = model.init(target_key, jnp.zeros((STATE_SHAPE,)))\n\nprediction = model.apply(online_net_params, None, state)\n```", "```py\nbuffer_state = {\n    \"states\": jnp.empty((BUFFER_SIZE, STATE_SHAPE), dtype=jnp.float32),\n    \"actions\": jnp.empty((BUFFER_SIZE,), dtype=jnp.int32),\n    \"rewards\": jnp.empty((BUFFER_SIZE,), dtype=jnp.int32),\n    \"next_states\": jnp.empty((BUFFER_SIZE, STATE_SHAPE), dtype=jnp.float32),\n    \"dones\": jnp.empty((BUFFER_SIZE,), dtype=jnp.bool_),\n}\n```", "```py\n# Python implementation\nforce = self.force_mag if action == 1 else -self.force_mag\n# Jax implementation\nforce = lax.select(jnp.all(action) == 1, self.force_mag, -self.force_mag)            )\n\n# Python\ncostheta, sintheta = math.cos(theta), math.sin(theta)\n# Jax\ncos_theta, sin_theta = jnp.cos(theta), jnp.sin(theta)\n\n# Python\nif not terminated:\n  reward = 1.0\n...\nelse: \n  reward = 0.0\n# Jax\nreward = jnp.float32(jnp.invert(done))\n```", "```py\n1\\. Initialization:\n  * Create empty arrays that will store the states, actions, rewards \n    and done flags for each timestep. Initialize the networks and optimizer\n    with dummy arrays.\n  * Wrap all the initialized objects in a val tuple\n\n2\\. Training loop (repeat for i steps):\n  * Unpack the val tuple\n  * (Optional) Decay epsilon using a decay function\n  * Take an action depending on the state and model parameters\n  * Perform an environment step and observe the next state, reward \n    and done flag\n  * Create an experience tuple (state, action, reward, new_state, done)\n    and add it to the replay buffer\n  * Sample a batch of experiences depending on the current buffer size\n    (i.e. sample only from experiences that have non-zero values)\n  * Update the model parameters using experience batch\n  * Every N steps, update the target network's weights \n    (set target_params = online_params)\n  * Store the experience's values for the current episode and return \n    the updated `val` tuple\n```", "```py\nRunning for 20,000 iterations: 100%|██████████| 20000/20000 [00:01<00:00, 15807.81it/s]\n```"]