- en: 'Unbox the Cox: A Hidden Dark Secret of Cox Regression'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/unbox-the-cox-a-hidden-dark-secret-of-cox-regression-fb432137fd6?source=collection_archive---------9-----------------------#2023-06-27](https://towardsdatascience.com/unbox-the-cox-a-hidden-dark-secret-of-cox-regression-fb432137fd6?source=collection_archive---------9-----------------------#2023-06-27)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why do perfect predictors result in a p-value of 0.93?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@igor-s?source=post_page-----fb432137fd6--------------------------------)[![Igor
    ≈†egota](../Images/17c592b71fef9526a0679d47937837f6.png)](https://medium.com/@igor-s?source=post_page-----fb432137fd6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fb432137fd6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fb432137fd6--------------------------------)
    [Igor ≈†egota](https://medium.com/@igor-s?source=post_page-----fb432137fd6--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe5f8ebca4ad8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funbox-the-cox-a-hidden-dark-secret-of-cox-regression-fb432137fd6&user=Igor+%C5%A0egota&userId=e5f8ebca4ad8&source=post_page-e5f8ebca4ad8----fb432137fd6---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fb432137fd6--------------------------------)
    ¬∑8 min read¬∑Jun 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffb432137fd6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funbox-the-cox-a-hidden-dark-secret-of-cox-regression-fb432137fd6&user=Igor+%C5%A0egota&userId=e5f8ebca4ad8&source=-----fb432137fd6---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffb432137fd6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funbox-the-cox-a-hidden-dark-secret-of-cox-regression-fb432137fd6&source=-----fb432137fd6---------------------bookmark_footer-----------)![](../Images/5854f8dc3addbe438ea7037fcb639fe1.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Dima Pechurin](https://unsplash.com/pt-br/@pechka?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Diving into the perfect predictors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have been following my previous blog posts, you might recall that logistic
    regression encounters a problem when trying to fit perfectly separated data, [leading
    to an infinite odds ratio](https://medium.com/towards-data-science/logistic-regression-faceoff-67560de4f492)*.*
    In Cox regression, where hazard replaces odds, you might wonder if a similar issue
    arises with perfect predictors. It does occur, but unlike logistic regression,
    it is much less apparent how this occurs here and even what constitutes ‚Äúperfect
    predictors‚Äù. As will become more clear later, perfect predictors are defined as
    predictors *x* whose ranks exactly match the ranks of event times (their Spearman
    correlation is one).
  prefs: []
  type: TYPE_NORMAL
- en: 'Previously, on ‚ÄúUnbox the Cox‚Äù:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/unbox-the-cox-intuitive-guide-to-cox-regressions-c485408ae15d?source=post_page-----fb432137fd6--------------------------------)
    [## Unbox the Cox: Intuitive Guide to Cox Regressions'
  prefs: []
  type: TYPE_NORMAL
- en: How do hazards and maximum likelihood estimates predict event rankings?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/unbox-the-cox-intuitive-guide-to-cox-regressions-c485408ae15d?source=post_page-----fb432137fd6--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'we explained maximum likelihood estimation and introduced a made-up dataset
    with five subjects, where a single predictor, *x*, represented the dosage of a
    life-extending drug. To make *x* a perfect predictor of event times, here we swapped
    the event times for subjects C and D:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/15880abec14a17404267728d9a3afd82.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand why these ‚Äúperfect predictors‚Äù can be problematic, let‚Äôs pick
    up right where we left off and check the negative log-likelihood cost plotted
    against *Œ≤*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/84e4aa14eea51ed69bc51e9c23ff5840.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see right away that there is no minimum value of *Œ≤* any longer: if
    we use very large negative values of *Œ≤*, we end up with log-likelihood fits that
    are near-perfect for all events.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let‚Äôs dive into the math behind this and take a look at the likelihood
    of event A. We‚Äôll dig into how the numerator and denominator change as we tweak
    *Œ≤*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f5509f0ba94030c965b8c45e143af6d3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When *Œ≤* is high or a large positive number, the last term in the denominator
    (with the largest *x* of 1.2), representing the hazard of subject E, dominates
    the entire denominator and becomes exceedingly large. So, the likelihood becomes
    small and approaches zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4a33d3c071776416288bc2bd87eed8bb.png)'
  prefs: []
  type: TYPE_IMG
- en: This results in a big negative log-likelihood. The same thing happens for each
    individual likelihood because the last hazard of subject E, will always exceed
    any hazard in the numerator. As a result, the negative log-likelihood increases
    for subjects A to D. In this case, when we have high *Œ≤*, it brings down all the
    likelihoods, resulting in poor fits for all events.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, when *Œ≤* is low or a big negative number, the first term in the denominator,
    representing the hazard of subject A, dominates since it has the lowest *x* value.
    As the same hazard of subject A also appears in the numerator, the likelihood
    L(A) can be arbitrarily close to 1 by making Œ≤ increasingly negative, thereby
    creating an almost perfect fit:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/606a470b8a080c47cafdcf39a47ac2d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The same deal goes for all the other individual likelihoods: negative *Œ≤s*
    now boost the likelihoods of all events at the same time. Basically, having a
    negativeŒ≤ doesn‚Äôt come with any downsides. At the same time, certain individual
    hazards increase (subjects A and B with negativex), some stay the same (subject
    C with *x =* 0), and others decrease (subject D with positivex). But remember,
    what really matters here are ratios of hazards. We can verify this hand-waving
    math by plotting individual hazards:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e3607c24c61922b16d990450fa47d835.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'The way likelihoods are put together, as a ratio of a hazard to the sum of
    hazards of all subjects still at risk, means that negative *Œ≤* values make a **perfect
    fit for the likelihood of each subject whose event time rank is greater than or
    equal to the predictor rank**! As a side note, if *x* had a perfect *negative*
    Spearman correlation with event times, things would be flipped around: arbitrarily
    positive *Œ≤*s would give us arbitrarily good fits.'
  prefs: []
  type: TYPE_NORMAL
- en: Misaligned predictor and time ranks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can actually see this and show you what goes down when event time ranks
    and predictor ranks do not line up using a another made-up example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3c639010a4db2b20b224393ffed42ef7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this particular example, the `time` column ranges from 1 to 8, where each
    value represents its own rank. We also have an `x_rank` column, which ranks the
    predictors *x*. Now, here''s the key observation: for subjects D and G, their
    `x_rank` is actually higher than their corresponding `time` rank. As a result,
    the likelihoods of D and G won''t experience the cancellation effect between the
    numerator and denominator when we have large negative values of *Œ≤*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a59a9dda57fca6a1bf85aa40863c81c7.png)![](../Images/f6fb2510e40f11c7ed08408509ac2da5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Their likelihoods are now maximal at some intermediate finite values of *Œ≤*.
    Let‚Äôs take a look at a plot of individual likelihoods to see this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/963e381927c49fa06858b34c9ac5f2dc.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'These ‚Äúmisaligned‚Äù ranks between a time and a predictor play a crucial role:
    they stop all likelihoods from essentially collapsing into one when we have significantly
    negative *Œ≤*s.'
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, in Cox regression, in order to obtain a finite coefficient *Œ≤*
    for a predictor *x*, we require at least one instance where the rank of the predictor
    *x* is lower than the rank of the event time.
  prefs: []
  type: TYPE_NORMAL
- en: Perfect is indeed the enemy of the good (p-value)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, how do these perfect predictors actually behave in real-life scenarios?
    Well, to find out, let‚Äôs once again turn to the lifelines library for some investigation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/f2e4431769f8c082ba5d63a695e27887.png)'
  prefs: []
  type: TYPE_IMG
- en: Just like in logistic regression, we‚Äôre encountering convergence warnings and
    getting extremely wide confidence intervals for our predictor coefficient *Œ≤*.
    **As a result, we end up with a p-value of 0.93**!
  prefs: []
  type: TYPE_NORMAL
- en: If we simply filter models based on p-values without taking this issue into
    account or conducting further investigation, we would overlook these perfect predictors.
  prefs: []
  type: TYPE_NORMAL
- en: 'To tackle this convergence problem, the lifelines library documentation and
    some helpful StackOverflow threads suggest a potential solution: incorporating
    a regularization term into the cost function. This term effectively increases
    the cost for large coefficient values, and you can activate L2 regularization
    by setting the `penalizer` argument to a value greater than zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/6b85fb942524535ad537bab989e4d1a7.png)'
  prefs: []
  type: TYPE_IMG
- en: This approach fixes the convergence warning, but it doesn‚Äôt really make a huge
    dent in shrinking that pesky p-value. Even with this regularization trick, the
    p-value for a perfect predictor still hangs around at a somewhat large value 0.11.
  prefs: []
  type: TYPE_NORMAL
- en: 'Time is relative: only ranks matter'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Lastly, we‚Äôll verify that the absolute values of event times have no impact
    on a Cox regression fit, using our previous example. To do this, we‚Äôll introduce
    a new column called `time2`, which will contain random numbers in the same order
    as the `time` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c640ed3ed5010c5ab0d8da500b31c714.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Their fits are indeed identical:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/99c7b05224f1a308f298c14adcce3a87.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c49f79ac66e6621a31ffc051e8188e87.png)'
  prefs: []
  type: TYPE_IMG
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What did we learn from all of this?
  prefs: []
  type: TYPE_NORMAL
- en: Perfect predictors in survival models are those predictors whose ranks perfectly
    match the ranks of event times.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cox regression cannot fit these perfect predictors with a finite coefficient
    *Œ≤*, leading to wide confidence intervals and big p-values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The actual values of event times don‚Äôt really matter ‚Äî it‚Äôs all about their
    ranks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the ranks of event times and predictors don‚Äôt align, we don‚Äôt get that
    handy cancellation effect for large *Œ≤* values in likelihoods. So, we need at
    least one case where the ranks don‚Äôt match to have a fit with a finite coefficient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even if we try some fancy regularization techniques, perfect predictors can
    still give us those annoyingly wide confidence intervals and high p-values in
    real-life situations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just like in logistic regression, if we don‚Äôt really care about those p-values,
    using a regularization method can still provide us with a handy model fit that
    gets the prediction right!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you would like to run the code yourself, feel free to use IPython notebooks
    from my Github: [https://github.com/igor-sb/blog/blob/main/posts/cox_perfect.ipynb](https://github.com/igor-sb/blog/blob/main/posts/cox_perfect.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: Farewell until the next post! üëã
  prefs: []
  type: TYPE_NORMAL
