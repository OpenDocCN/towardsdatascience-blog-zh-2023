- en: In-Context Learning Approaches in Large Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型中的上下文学习方法
- en: 原文：[https://towardsdatascience.com/in-context-learning-approaches-in-large-language-models-9c0c53b116a1?source=collection_archive---------0-----------------------#2023-07-01](https://towardsdatascience.com/in-context-learning-approaches-in-large-language-models-9c0c53b116a1?source=collection_archive---------0-----------------------#2023-07-01)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/in-context-learning-approaches-in-large-language-models-9c0c53b116a1?source=collection_archive---------0-----------------------#2023-07-01](https://towardsdatascience.com/in-context-learning-approaches-in-large-language-models-9c0c53b116a1?source=collection_archive---------0-----------------------#2023-07-01)
- en: Simple and powerful techniques to make LLMs learn new tasks at inference time
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单而强大的技术，使大型语言模型在推理时能够学习新任务
- en: '[](https://medium.com/@javaid.nabi?source=post_page-----9c0c53b116a1--------------------------------)[![Javaid
    Nabi](../Images/a306349c22ed74db6409541a7d64cae7.png)](https://medium.com/@javaid.nabi?source=post_page-----9c0c53b116a1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9c0c53b116a1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9c0c53b116a1--------------------------------)
    [Javaid Nabi](https://medium.com/@javaid.nabi?source=post_page-----9c0c53b116a1--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@javaid.nabi?source=post_page-----9c0c53b116a1--------------------------------)[![Javaid
    Nabi](../Images/a306349c22ed74db6409541a7d64cae7.png)](https://medium.com/@javaid.nabi?source=post_page-----9c0c53b116a1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9c0c53b116a1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9c0c53b116a1--------------------------------)
    [Javaid Nabi](https://medium.com/@javaid.nabi?source=post_page-----9c0c53b116a1--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F70c04bf6660e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fin-context-learning-approaches-in-large-language-models-9c0c53b116a1&user=Javaid+Nabi&userId=70c04bf6660e&source=post_page-70c04bf6660e----9c0c53b116a1---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9c0c53b116a1--------------------------------)
    ·17 min read·Jul 1, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9c0c53b116a1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fin-context-learning-approaches-in-large-language-models-9c0c53b116a1&user=Javaid+Nabi&userId=70c04bf6660e&source=-----9c0c53b116a1---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F70c04bf6660e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fin-context-learning-approaches-in-large-language-models-9c0c53b116a1&user=Javaid+Nabi&userId=70c04bf6660e&source=post_page-70c04bf6660e----9c0c53b116a1---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9c0c53b116a1--------------------------------)
    ·17 min read·2023年7月1日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9c0c53b116a1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fin-context-learning-approaches-in-large-language-models-9c0c53b116a1&user=Javaid+Nabi&userId=70c04bf6660e&source=-----9c0c53b116a1---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9c0c53b116a1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fin-context-learning-approaches-in-large-language-models-9c0c53b116a1&source=-----9c0c53b116a1---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9c0c53b116a1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fin-context-learning-approaches-in-large-language-models-9c0c53b116a1&source=-----9c0c53b116a1---------------------bookmark_footer-----------)'
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Language modeling (LM) aims to model the generative likelihood of word sequences,
    so as to predict the probabilities of future (or missing) tokens. Language models
    have revolutionized natural language processing (NLP) in recent years. It is now
    well-known that increasing the scale of language models (e.g., training compute,
    model parameters, etc.) can lead to better performance and sample efficiency on
    a range of downstream NLP tasks. The survey paper “*A Survey of Large Language
    Models*” [1] covers almost every aspect of the large language models. The paper
    provides an up-to-date review of the literature on LLMs, details about the training
    mechanisms like pre-training approaches along with instruction tuning techniques
    & further alignment training with the recent RLHF approach. The approaches of
    instruction tuning and alignment tuning is used to adapt LLMs according to specific
    goals.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 语言建模（LM）旨在对词序列的生成概率进行建模，以预测未来（或缺失）标记的概率。近年来，语言模型在自然语言处理（NLP）领域带来了革命性变化。现在已经广泛认识到，增加语言模型的规模（例如训练计算、模型参数等）可以在一系列下游NLP任务中带来更好的性能和样本效率。调查论文“*大型语言模型调查*”
    [1] 涵盖了大型语言模型几乎所有的方面。该论文提供了对LLMs文献的最新回顾，详细介绍了预训练方法、指令调优技术及最新的RLHF方法的进一步对齐训练。指令调优和对齐调优方法用于根据具体目标调整LLMs。
- en: '*After pre-training or adaptation tuning, a major approach to using LLMs is
    to design suitable prompting strategies for solving various tasks.* *A typical
    prompting method also known as in-context learning (ICL), formulates the task
    description and/or demonstrations (examples) in the form of natural language text.*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*在预训练或适应性调优之后，使用LLMs的主要方法是设计合适的提示策略来解决各种任务。* *一种典型的提示方法，也称为上下文学习（ICL），以自然语言文本的形式制定任务描述和/或示例。*'
- en: In-Context Learning
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 上下文学习
- en: LLMs demonstrate an in-context learning (ICL) ability, that is, learning from
    a few examples in the context. Many studies have shown that LLMs can perform a
    series of complex tasks through ICL, such as solving mathematical reasoning problems.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs展示了一种上下文学习（ICL）能力，即从上下文中的几个示例中学习。许多研究表明，LLMs可以通过ICL执行一系列复杂任务，例如解决数学推理问题。
- en: The key idea of in-context learning is to learn from analogy. The figure below
    gives an example describing how language models make decisions with ICL. First,
    ICL requires a few examples to form a demonstration context. These examples are
    usually written in natural language templates. Then, ICL concatenates a query
    question and a piece of demonstration context together to form a prompt, which
    is then fed into the language model for prediction [2].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文学习的关键思想是通过类比进行学习。下图展示了语言模型如何通过ICL进行决策的示例。首先，ICL需要几个示例来形成演示上下文。这些示例通常以自然语言模板的形式编写。然后，ICL将查询问题和一段演示上下文拼接在一起，形成提示，接着将其输入语言模型进行预测
    [2]。
- en: '![](../Images/5bca8c4a159d204dcaed9f45112fcf29.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5bca8c4a159d204dcaed9f45112fcf29.png)'
- en: '[Example of In-context Learning](https://arxiv.org/pdf/2301.00234.pdf)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[上下文学习示例](https://arxiv.org/pdf/2301.00234.pdf)'
- en: Different from supervised learning requiring a training stage that uses backward
    gradients to update model parameters, ICL does not conduct parameter updates and
    directly performs predictions on the pre-trained language models. The model is
    expected to learn the pattern hidden in the demonstration and accordingly make
    the right prediction.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 与需要使用反向梯度更新模型参数的监督学习不同，ICL不进行参数更新，而是直接对预训练的语言模型进行预测。模型期望从示例中学习隐藏的模式，从而做出正确的预测。
- en: What makes ICL attractive?
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么使得ICL具有吸引力？
- en: Examples written in natural language provide an interpretable interface to communicate
    with LLMs. This paradigm makes it much easier to incorporate human knowledge into
    LLMs by changing the examples and templates
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用自然语言编写的示例提供了一个可解释的接口，与LLMs进行交流。这个范式使得通过更改示例和模板将人类知识融入LLMs变得更加容易。
- en: It is similar to the decision process of human beings by learning from analogy.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这类似于人类通过类比进行的决策过程。
- en: Compared with supervised training, ICL is a training-free learning framework.
    This not only greatly reduces the computation costs for adapting the model to
    new tasks, but also makes language-model-as-service possible and can be easily
    applied to large-scale real-world tasks.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与监督训练相比，ICL是一种无需训练的学习框架。这不仅大大降低了将模型调整到新任务的计算成本，还使语言模型即服务成为可能，并且可以轻松应用于大规模的真实任务。
- en: But how does this work?
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 但这如何运作呢？
- en: After pre-training, LLMs can exhibit intriguing ICL capabilities (emergent capabilities)
    without being updated [3]. While intuitively reasonable, the working mechanism
    of the ICL remains unclear, and few studies have provided preliminary explanations
    for the two questions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 经过预训练，LLM 能够展示出引人注目的 ICL 能力（新兴能力），而无需更新[3]。虽然直观上合理，但 ICL 的工作机制仍不清楚，且少有研究对这两个问题提供了初步解释。
- en: How does pre-training affect the ICL ability?
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预训练如何影响 ICL 能力？
- en: Researchers suggested that a pre-trained model acquires some emergent ICL abilities
    when it achieves a large scale of pre-training steps or model parameters [3].
    Some studies also showed that the ICL ability grows as the parameters of LLMs
    increase from 0.1 billion to 175 billion. Research suggests that the design of
    training tasks is an important influence factor on the ICL capability of LLMs.
    Besides training tasks, recent studies have also investigated the relationship
    between ICL and the pre-training corpora. It has been shown that the performance
    of ICL heavily depends on the source of pre-training corpora rather than the scale.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员建议，当预训练模型达到大规模预训练步骤或模型参数时，它会获得一些新兴的 ICL 能力[3]。一些研究还表明，ICL 能力随着 LLM 参数从 1
    亿增加到 1750 亿而增长。研究表明，训练任务的设计是影响 LLM ICL 能力的重要因素。除了训练任务，最近的研究还探讨了 ICL 与预训练语料库之间的关系。研究显示，ICL
    的表现更多依赖于预训练语料库的来源，而非规模。
- en: How do LLMs perform ICL during inference?
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM 在推理过程中如何执行 ICL？
- en: 'In the paper “*Why Can GPT Learn In-Context?*” [4], researchers figured out
    a dual form between Transformer attention and gradient descent and further proposed
    to understand ICL as implicit fine-tuning. They compared GPT-based ICL and explicit
    fine-tuning on real tasks and found that ICL behaves similarly to fine-tuning
    from multiple perspectives. Under this framework, the ICL process can be explained
    as follows: by means of forward computation, LLMs generate meta-gradients with
    respect to demonstrations and implicitly perform gradient descent via the attention
    mechanism.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在论文“*为什么 GPT 能在上下文中学习？*”[4]中，研究人员发现了 Transformer 注意力和梯度下降之间的双重形式，并进一步提出将 ICL
    理解为隐式微调。他们比较了基于 GPT 的 ICL 和真实任务上的显式微调，发现 ICL 在多个方面表现得类似于微调。在这个框架下，ICL 过程可以解释为：通过前向计算，LLM
    生成相对于演示的元梯度，并通过注意力机制隐式执行梯度下降。
- en: Another perspective from Stanford research [5] explains ‘*In-context learning
    as Implicit Bayesian Inference’.* The authors provide a framework where the LM
    does in-context learning by using the prompt to “locate” the relevant concept
    it has learned during pre-training to do the task. We can theoretically view this
    as Bayesian inference of a latent concept conditioned on the prompt, and this
    capability comes from structure (long-term coherence) in the pre-training data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 斯坦福研究的另一个视角[5]解释了‘*上下文学习作为隐式贝叶斯推断*’。作者提供了一个框架，其中语言模型通过使用提示来“定位”其在预训练过程中学到的相关概念以完成任务。从理论上讲，我们可以将其视为在提示条件下的潜在概念的贝叶斯推断，这一能力来自于预训练数据中的结构（长期一致性）。
- en: Even though there are some answers, this research is still evolving to understand
    the mechanism and underlying reasons better.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管已有一些答案，这项研究仍在不断发展，以更好地理解其机制和潜在原因。
- en: In-Context Learning Approaches
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 上下文学习方法
- en: Now let us explore some popular ICL methods.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们探讨一些流行的 ICL 方法。
- en: Chain of thought (COT)
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 思维链（COT）
- en: Self-consistency COT
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自一致性 COT
- en: Tree of Thoughts
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 思维树
- en: Chain of thought (COT)
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 思维链（COT）
- en: It is observed that standard prompting techniques (also known as general input-output
    prompting) do not perform well on complex reasoning tasks, such as arithmetic
    reasoning, commonsense reasoning, and symbolic reasoning. CoT is an improved prompting
    strategy to boost the performance of LLMs such non-trivial cases involving reasoning
    [6]. Instead of simply constructing the prompts with input-output pairs as in
    ICL, CoT incorporates intermediate reasoning steps that can lead to the final
    output into the prompts. As can be seen from the example below.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 观察发现，标准提示技术（也称为一般输入输出提示）在复杂推理任务（如算术推理、常识推理和符号推理）上表现不佳。思维链是一种改进的提示策略，用于提升 LLM
    在涉及推理的复杂案例中的表现[6]。与 ICL 中仅使用输入输出对构建提示不同，思维链将可以导致最终输出的中间推理步骤纳入提示中。如下例所示。
- en: '![](../Images/9eacd78a63e972890918b6bf8a4076bb.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9eacd78a63e972890918b6bf8a4076bb.png)'
- en: '[Reference[6]](https://arxiv.org/pdf/2201.11903.pdf)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[参考文献[6]](https://arxiv.org/pdf/2201.11903.pdf)'
- en: The figure above shows an example of a model producing a chain of thought to
    solve a math word problem that it would have otherwise gotten incorrect. On the
    left side, in ICL, the model is provided with examples or demonstrations of mathematical
    reasoning questions and a direct answer. But the model is not able to predict
    the correct answer.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示了一个模型生成思维链来解决数学词汇问题的示例，否则它会得到错误的答案。在左侧的 ICL 中，模型提供了数学推理问题的示例或演示及直接答案。但模型无法预测正确答案。
- en: On the right side, in COT, the model is presented with an intermediate step
    to help arrive at an answer of the example/demonstration given. We can see when
    a model is now asked a similar reasoning question, it is able to predict the answer
    correctly, thus proving the efficacy of the COT approach for such use cases.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在右侧的 COT 中，模型呈现一个中间步骤以帮助得出给定示例/演示的答案。我们可以看到，当现在要求模型回答类似的推理问题时，它能够正确预测答案，从而证明了
    COT 方法在此类用例中的有效性。
- en: If you see, COT or ICL in general provide some examples to demonstrate the use
    cases this is called **Few-Shot (few examples)**. There is one more paper [7]
    that brought out interesting prompting *“Let us think step by step..”* without
    any examples to demonstrate the use case, this is called **Zero-short (no examples)**.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看到，一般来说 COT 或 ICL 提供一些示例来演示用例，这称为 **少样本（少量示例）**。还有一篇论文 [7] 提出了有趣的提示 *“让我们一步步思考……”*，没有任何示例来演示用例，这称为
    **零样本（没有示例）**。
- en: In **Zero-shot CoT,** LLM is first prompted by *“Let’s think step by step”*
    to generate reasoning steps and then prompted by *“Therefore, the answer is”*
    to derive the final answer. They find that such a strategy drastically boosts
    the performance when the model scale exceeds a certain size, but is not effective
    with small-scale models, showing a significant pattern of emergent abilities.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **零样本 CoT** 中，LLM 首先通过 *“让我们一步步思考”* 生成推理步骤，然后通过 *“因此，答案是”* 推导出最终答案。他们发现，当模型规模超过某一大小时，这种策略显著提升了性能，但对小规模模型效果不佳，显示出显著的突现能力模式。
- en: '![](../Images/1687abfbc0752763e4d26aa6b2f80812.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1687abfbc0752763e4d26aa6b2f80812.png)'
- en: '[Reference[7]](https://arxiv.org/pdf/2205.11916.pdf)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[参考文献[7]](https://arxiv.org/pdf/2205.11916.pdf)'
- en: 'Above: Example inputs and outputs of GPT-3 with (a) standard Few-shot (ICL),
    (b) Few-shot-CoT, (c) standard Zero-shot (ICL), and (d) ours (Zero-shot-CoT).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 上图：GPT-3 的示例输入和输出，包括 (a) 标准少样本 (ICL)、(b) 少样本-CoT、(c) 标准零样本 (ICL) 和 (d) 我们的方法
    (零样本-CoT)。
- en: Similar to Few-shot-CoT, Zero-shot-CoT facilitates multi-step reasoning (blue
    text) and reaches the correct answer where standard prompting fails. Unlike Few-shot-CoT
    using step-by-step reasoning examples per task, Zero-Shot does not need any examples
    and just uses the same prompt “Let’s think step by step” across all tasks (arithmetic,
    symbolic, commonsense, and other logical reasoning tasks).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于少样本-CoT，零样本-CoT 促进了多步骤推理（蓝色文本），并在标准提示失败的情况下得出正确答案。与每个任务使用逐步推理示例的少样本-CoT 不同，零样本-CoT
    不需要任何示例，只需在所有任务中使用相同的提示 *“让我们一步步思考”*（算术、符号、常识和其他逻辑推理任务）。
- en: This research shows LLMs are decent zero-shot reasoners by adding a simple prompt,
    *Let’s think step by step*, to facilitate step-by-step thinking before answering
    each question.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这项研究表明，通过添加一个简单的提示 *“让我们一步步思考”*，LLMs 是相当不错的零样本推理器，以促进在回答每个问题之前逐步思考。
- en: 'Let us see what happens underneath:'
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们看看下面发生了什么：
- en: While Zero-shot-CoT is conceptually simple, it uses prompting twice to extract
    both reasoning and answer, as explained in the figure below.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管零样本-CoT 概念上很简单，但它使用了两次提示来提取推理和答案，如下图所示。
- en: '![](../Images/59279a565f1fd4c5e7d33177f1fa249a.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/59279a565f1fd4c5e7d33177f1fa249a.png)'
- en: '[Reference[7]](https://arxiv.org/pdf/2205.11916.pdf)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[参考文献[7]](https://arxiv.org/pdf/2205.11916.pdf)'
- en: 'The process involves two steps: first “**reasoning prompt extraction**” to
    extract a full reasoning path from a language model, and then use the second “**answer
    prompt extraction**” to extract the answer in the correct format from the reasoning
    text.'
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 该过程包括两个步骤：首先是“**推理提示提取**”以从语言模型中提取完整的推理路径，然后使用第二步“**答案提示提取**”以从推理文本中提取正确格式的答案。
- en: '**1st prompt — reasoning extraction**'
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**第一个提示 — 推理提取**'
- en: 'In this step first modify the input question x into a prompt x’ using a simple
    template **“Q: [X]. A: [T]”**, where [X] is an input slot for x and [T] is a slot
    for hand-crafted trigger sentence t that would extract chain of thought to answer
    the question x. For example, if we use *“Let’s think step by step”* as a trigger
    sentence, the prompt x’ would be **“Q: [X]. A: Let’s think step by step.”** Prompted
    text x’ is then fed into a language model and generates subsequent sentence z.
    We can use any decoding strategy.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '在此步骤中，首先使用简单模板 **“Q: [X]. A: [T]”** 将输入问题 x 修改为提示 x’，其中 [X] 是 x 的输入槽，[T] 是用于提取思考链以回答问题
    x 的手工触发句 t 的槽。例如，如果我们使用 *“让我们一步一步地思考”* 作为触发句，那么提示 x’ 将是 **“Q: [X]. A: 让我们一步一步地思考。”**
    提示文本 x’ 然后输入到语言模型中，生成随后的句子 z。我们可以使用任何解码策略。'
- en: 'Some other examples of such prompts:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其他一些此类提示的例子：
- en: Let’s think about this logically.
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 让我们从逻辑上考虑一下这个问题。
- en: ''
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s solve this problem by splitting it into steps.
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 让我们通过将问题拆分成步骤来解决这个问题。
- en: ''
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s think like a detective step by step.
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 让我们像侦探一样一步一步思考。
- en: ''
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Before we dive into the answer.
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在我们深入回答之前。
- en: '**2nd prompt — answer extraction**'
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**第二步提示——答案提取**'
- en: 'In the second step, the generated sentence z along with prompted sentence x’
    is used to extract the final answer from the language model. To be concrete, simply
    concatenate three elements as with **“[X’] [Z] [A]”: [X’]** for 1st prompt x’,
    [Z] for sentence z generated at the first step, and [A] for a trigger sentence
    to extract the answer. The prompt for this step is self-augmented since the prompt
    contains the sentence z generated by the same language model. In experiments,
    authors use slightly different answer trigger depending on the answer format.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二步中，生成的句子 z 与提示的句子 x’ 一起用于从语言模型中提取最终答案。具体来说，将三个元素简单地连接在一起，如 **“[X’] [Z] [A]”：
    [X’]** 为第一步提示 x’，[Z] 为第一步生成的句子 z，[A] 为提取答案的触发句。这个步骤的提示是自我增强的，因为提示中包含了由相同语言模型生成的句子
    z。在实验中，作者根据答案格式使用了略微不同的答案触发器。
- en: For example, the use of *“Therefore, among A through E, the answer is”* for
    **multi-choice QA**, and *“Therefore, the answer (Arabic numerals) is”* for math
    problems requiring a **numerical answer**.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，使用 *“因此，在 A 到 E 之间，答案是”* 来处理 **多项选择题**，以及 *“因此，答案（阿拉伯数字）是”* 来处理需要 **数值答案**
    的数学问题。
- en: The paper [7] has interesting ideas, the performance of various prompts, etc.,
    please read for more details.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 论文 [7] 提出了有趣的想法、各种提示的表现等，请阅读以获取更多细节。
- en: '**When CoT works for LLMs?**'
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**CoT 何时对 LLMs 有效？**'
- en: It only has a positive effect on sufficiently large models (e.g., typically
    containing 10B or more parameters but not on small models. This phenomenon is
    referred to as the ‘*emergent abilities*’ of large language models. An ability
    is considered to be emergent if it is not present in smaller models but is present
    in larger models [3].
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 仅对足够大的模型（例如，通常包含 10B 或更多参数）有正面效果，而对小模型没有。这一现象被称为大型语言模型的‘*涌现能力*’。如果某种能力在较小的模型中不存在，但在较大的模型中存在，则该能力被认为是涌现的
    [3]。
- en: It is mainly effective to improve the tasks that require step-by-step reasoning,
    such as arithmetic reasoning, commonsense reasoning, and symbolic reasoning.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这主要有效于改进需要逐步推理的任务，如算术推理、常识推理和符号推理。
- en: For other tasks that do not rely on complex reasoning, it might show worse performance
    than standard. Interestingly, it seems that the performance gain brought by CoT
    prompting could be significant only when standard prompting yields poor results.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于那些不依赖于复杂推理的任务，它可能表现得比标准方法更差。有趣的是，CoT 提示所带来的性能提升似乎只有在标准提示效果较差时才会显著。
- en: '**Why LLMs Can Perform CoT Reasoning?**'
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**为什么 LLMs 可以进行 CoT 推理？**'
- en: It is widely *hypothesized* that it can be attributed to training on code since
    models trained on it show a strong reasoning ability. Intuitively, code data is
    well organized with algorithmic logic and programming flow, which may be useful
    to improve the reasoning performance of LLMs. **However, this hypothesis still
    lacks publicly reported evidence of ablation experiments (with and without training
    on code).**
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 广泛 *假设* 这可以归因于对代码的训练，因为在其上训练的模型表现出强大的推理能力。直观地看，代码数据有良好的算法逻辑和编程流程，这可能有助于提高 LLMs
    的推理表现。**然而，这一假设仍然缺乏公开报告的消融实验证据（有无对代码的训练）。**
- en: The major distinction between CoT prompting and standard prompting is the *incorporation
    of reasoning paths prior to the final answer*. Thus, some researchers investigate
    the effect of different components in the reasoning paths. Specifically, a recent
    study identifies three key components in CoT prompting, namely symbols (e.g.,
    numerical quantities in arithmetic reasoning), patterns (e.g., equations in arithmetic
    reasoning), and text (i.e., the rest of tokens that are not symbols or patterns).
    It is shown that the latter two parts (i.e., patterns and text) are essential
    to the model performance, and removing either one would lead to a significant
    performance drop.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CoT 提示与标准提示的主要区别在于*在最终答案之前融入了推理路径*。因此，一些研究人员探讨了推理路径中不同组成部分的效果。具体来说，一项最近的研究确定了
    CoT 提示中的三个关键组成部分，即符号（例如，算术推理中的数值量）、模式（例如，算术推理中的方程式）和文本（即其余的非符号或模式的令牌）。研究表明，后两部分（即模式和文本）对模型性能至关重要，移除其中任何一个都会导致性能显著下降。
- en: This is an active area of research, for an in-depth discussion on this, please
    read [2]. There is one more interesting research [8] that discusses possible reasons
    for in-context learning in transformer models.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个活跃的研究领域，关于这一点的深入讨论请阅读 [2]。还有一项有趣的研究 [8] 讨论了变压器模型中的上下文学习的可能原因。
- en: Self-consistency COT
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自一致性 COT
- en: Instead of using the greedy decoding strategy in COT, the authors in [9] propose
    another decoding strategy called self-consistency to replace the greedy decoding
    strategy used in chain-of-thought prompting, that further improves language models’
    reasoning performance by a significant margin. Self-consistency leverages the
    intuition that complex reasoning tasks typically admit multiple reasoning paths
    that reach a correct answer. The more that deliberate thinking and analysis is
    required for a problem, the greater the diversity of reasoning paths that can
    recover the answer.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在 COT 中，作者在 [9] 中提出了一种称为自一致性的解码策略，以取代在链式思维提示中使用的贪婪解码策略，这种策略显著提高了语言模型的推理性能。自一致性利用了这样一个直觉，即复杂的推理任务通常允许多个推理路径达到正确答案。问题需要更多的深思熟虑和分析时，可以恢复答案的推理路径的多样性就越大。
- en: First, prompt the language model with chain-of-thought prompting, then instead
    of greedily decoding the optimal reasoning path, authors propose **“sample-and-marginalize”**
    decoding procedure.
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 首先用链式思维提示语言模型，然后作者提出了**“采样和边际化”**解码过程，而不是贪婪地解码最优推理路径。
- en: The figure below illustrates the self-consistency method with an example.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 下图通过一个例子说明了自一致性方法。
- en: '![](../Images/07b6e0f78bf2a1c96edcf631d68d0f49.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07b6e0f78bf2a1c96edcf631d68d0f49.png)'
- en: '[Reference[9]](https://arxiv.org/pdf/2203.11171.pdf)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[参考文献[9]](https://arxiv.org/pdf/2203.11171.pdf)'
- en: First sample from the language model’s decoder to generate a diverse set of
    reasoning paths; each reasoning path might lead to a different final answer, so
    determine the optimal answer by marginalizing out the sampled reasoning paths
    to find the most consistent answer in the final answer set. Or in other words,
    from the model’s decoder, by taking a majority vote over the answers, we arrive
    at the most “consistent” answer among the final answer set.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 首先从语言模型的解码器中生成一组多样化的推理路径；每条推理路径可能会导致不同的最终答案，因此通过对采样的推理路径进行边际化来确定最一致的答案。换句话说，通过对模型的解码器中的答案进行多数投票，我们可以在最终答案集中得到最“一致”的答案。
- en: '![](../Images/47905a9ce38151fa4c27d87fb5c06ac7.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47905a9ce38151fa4c27d87fb5c06ac7.png)'
- en: '[Majority Voting Example](https://www.arxiv-vanity.com/papers/2210.11610/)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[多数投票示例](https://www.arxiv-vanity.com/papers/2210.11610/)'
- en: Such an approach is analogous to the human experience that if multiple different
    ways of thinking lead to the same answer, one has greater confidence that the
    final answer is correct. Compared to other decoding methods, self-consistency
    avoids the repetitiveness and local optimality that plague greedy decoding, while
    mitigating the stochasticity of a single sampled generation.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法类似于人类经验，如果多种不同的思维方式得到相同的答案，则对最终答案的正确性信心更大。与其他解码方法相比，自一致性避免了贪婪解码中的重复性和局部最优性，同时减轻了单次采样生成的随机性。
- en: Extensive empirical evaluation shows that self-consistency boosts the performance
    of chain-of-thought prompting with a striking margin on a range of popular arithmetic
    and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%),
    AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 大量实证评估显示，自一致性显著提高了链式思维提示在多个流行的算术和常识推理基准上的表现，包括 GSM8K（+17.9%）、SVAMP（+11.0%）、AQuA（+12.2%）、StrategyQA（+6.4%）和
    ARC-challenge（+3.9%）。
- en: One **limitation** of self-consistency is that it incurs more computation cost.
    In practice, people can try a small number of paths (e.g., 5 or 10) as a starting
    point to realize most of the gains while not incurring too much cost, as in most
    cases the performance saturates quickly.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 自一致性的一个**限制**是它带来了更高的计算成本。实际上，人们可以尝试少量路径（例如 5 或 10）作为起点，以实现大多数收益，同时不产生过多成本，因为在大多数情况下，性能会迅速饱和。
- en: Tree of thoughts
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 思维树
- en: 'Authors in [10] propose “*Tree of Thoughts*” (ToT), which generalizes over
    the “*Chain of Thoughts*” approach to prompting language models and enables exploration
    over coherent units of text (“thoughts”) that serve as intermediate steps toward
    problem-solving. ToT allows LMs to perform deliberate decision-making by considering
    multiple different reasoning paths and self-evaluating choices to decide the next
    course of action, as well as looking ahead or backtracking when necessary to make
    global choices. The results/experiments show that ToT significantly enhances language
    models’ problem-solving abilities on three novel tasks requiring non-trivial planning
    or search: Game of 24, Creative Writing, and Mini Crosswords.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] 的作者提出了“*思维树*”（ToT），它在“*链式思维*”方法上进行了概括，以提示语言模型，并允许在作为解决问题的中间步骤的连贯文本单元（“思维”）上进行探索。ToT
    允许语言模型通过考虑多个不同的推理路径并自我评估选择来进行深思熟虑的决策，同时在必要时前瞻或回溯以做出全局选择。结果/实验表明，ToT 在需要非平凡规划或搜索的三项新任务（24
    点游戏、创意写作和迷你填字游戏）上显著提升了语言模型的解决问题能力。'
- en: '![](../Images/f3e4b7ab005d0fb5d1c44d2751a924e5.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3e4b7ab005d0fb5d1c44d2751a924e5.png)'
- en: '[Schematic illustrating various prompting approaches, each rectangle box represents
    a thought](https://arxiv.org/pdf/2305.10601.pdf)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[示意图说明了各种提示方法，每个矩形框代表一个思维](https://arxiv.org/pdf/2305.10601.pdf)'
- en: 'Tree of Thoughts (ToT) allows LMs to explore multiple reasoning paths over
    thoughts (above Figure). ToT frames any problem as a search over a tree, where
    each node is a state s = [x, z1···i] representing a partial solution with the
    input x and the sequence of thoughts so far zi. The ToT does 4 things: **thought
    decomposition, thought generator, state evaluator, and search algorithm**.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 思维树（ToT）允许语言模型（LMs）在思维上探索多个推理路径（见上图）。ToT 将任何问题框架化为对树的搜索，其中每个节点是一个状态 s = [x,
    z1···i]，代表具有输入 x 和到目前为止的思维序列 zi 的部分解决方案。ToT 做了 4 件事：**思维分解、思维生成器、状态评估器和搜索算法**。
- en: '1\. **Thought decomposition:** Decompose the intermediate process into thought
    steps:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. **思维分解：** 将中间过程分解为思维步骤：
- en: While CoT samples thoughts coherently without explicit decomposition, ToT leverages
    problem properties to design and decompose intermediate thought steps. As *Table
    1* shows, depending on different problems, a thought could be a couple of words
    (Crosswords), a line of equation (Game of 24), or a whole paragraph of writing
    plan (Creative Writing). It is like how you divide the question into several tasks.
    Each task is a step Zn that we discuss. Note that, this part is only about decomposing
    the questions into tasks. It is like planning, we don’t actually do any thoughts
    in this part.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 CoT 在没有明确分解的情况下连贯地采样思维，ToT 利用问题特性设计和分解中间思维步骤。如*表 1*所示，依赖于不同的问题，思维可以是几个单词（填字游戏）、一行方程式（24
    点游戏）或一整段写作计划（创意写作）。这就像你将问题分解成几个任务。每个任务是我们讨论的步骤 Zn。请注意，这部分仅涉及将问题分解为任务。就像规划一样，我们在这部分并不实际进行任何思维。
- en: '![](../Images/4469b6444417dee2c9975ad60047356d.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4469b6444417dee2c9975ad60047356d.png)'
- en: '[Reference [10]](https://arxiv.org/pdf/2305.10601.pdf)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[参考文献 [10]](https://arxiv.org/pdf/2305.10601.pdf)'
- en: '2\. **Thought generation:** So after we define the task for each step in thought
    decomposition. We now actually generate the thoughts. We try to generate k thoughts
    as candidates for given a step Zn. There are two ways for generating thoughts:
    sample and propose.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. **思维生成：** 在我们为每一步定义任务后，实际生成思维。我们尝试生成 k 个思维作为给定步骤 Zn 的候选项。生成思维有两种方式：采样和提出。
- en: a. Sample i.i.d. thoughts from a CoT prompt. We repeat the generation process
    k times independently. This works better when the thought space is rich (e.g.
    each thought is a paragraph), and i.i.d. samples lead to diversity.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: a. 从 CoT 提示中抽取 i.i.d. 思维。我们独立重复生成过程 k 次。当思维空间丰富时（例如，每个思维是一个段落），i.i.d. 样本能带来多样性。
- en: '![](../Images/6259a1564f5ae127d0750afe316150d3.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6259a1564f5ae127d0750afe316150d3.png)'
- en: '[A step of deliberate search in a randomly picked Creative Writing task.](https://arxiv.org/pdf/2305.10601.pdf)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[在随机选择的创造性写作任务中的一次深思熟虑的搜索步骤。](https://arxiv.org/pdf/2305.10601.pdf)'
- en: In the above figure, a step of deliberate search in a randomly picked **Creative
    Writing task**. Given the input, the LM samples 5 different plans, then votes
    5 times to decide which plan is best. The majority choice is used to consequently
    write the output passage with the same sample-vote procedure.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，展示了在随机选择的**创造性写作任务**中的一步深思熟虑的搜索。给定输入，LM 采样了5个不同的计划，然后投票5次决定哪个计划最佳。多数选择被用来随后用相同的样本-投票程序写出输出段落。
- en: b. Propose thoughts sequentially using a “propose prompt”. This works better
    when the thought space is more constrained (e.g. each thought is just a word or
    a line), so proposing different thoughts in the same context avoids duplication.
    In this, we generate k thoughts at one inference. So, these k thoughts may not
    be independent.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: b. 使用“提出提示”顺序提出思维。当思维空间更受限时（例如，每个思维只是一个词或一行），在同一上下文中提出不同的思维可以避免重复。在这种情况下，我们在一次推理中生成
    k 个思维。因此，这些 k 个思维可能并不独立。
- en: '3\. **Evaluate states:** In this part, we define a state evaluation function:
    v(s). To expand the tree, we use this function to find the good path, like in
    chess programming. We evaluate the given path of the tree *s=[x, z1…i]*. There
    are two ways to define the evaluation function:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. **评估状态：** 在这一部分，我们定义一个状态评估函数：v(s)。为了扩展树，我们使用这个函数找到好的路径，就像在棋类编程中一样。我们评估给定的树路径*s=[x,
    z1…i]*。有两种方法来定义评估函数：
- en: 'Value each state independently: each state ‘s’ (or path) will be evaluated
    independently. [*Example: Game of 24*]'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 独立评估每个状态：每个状态‘s’（或路径）将被独立评估。[*示例：24点游戏*]
- en: 'Vote across states: each state ‘s’ will be evaluated given the set of all states
    S. Just like you compare the states in S to each other as in self-consistency
    COT. [*Example: creative writing task*]'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跨状态投票：每个状态‘s’ 将在所有状态集合 S 中进行评估。就像你在自我一致性 COT 中比较 S 中的状态一样。[*示例：创造性写作任务*]
- en: '**Example Game of 24:**'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**24点游戏示例：**'
- en: Game of 24 is a mathematical reasoning challenge, where the goal is to use 4
    numbers and basic arithmetic operations (+-*/) to obtain 24\. For example, given
    input “4 9 10 13”, a solution output could be “(10–4) * (13–9) = 24”.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 24点游戏是一种数学推理挑战，其目标是使用4个数字和基本的算术运算（+-*/）得到24。例如，给定输入“4 9 10 13”，一种解决方案可能是“(10–4)
    * (13–9) = 24”。
- en: '![](../Images/823f42fd6852510d10f72d6408f1409a.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/823f42fd6852510d10f72d6408f1409a.png)'
- en: '[‘Game of 24’ ToT Decomposition. The LM is prompted for (a) thought generation
    and (b) valuation.](https://arxiv.org/pdf/2305.10601.pdf)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[‘24点游戏’ ToT 分解。LM 被提示进行 (a) 思维生成和 (b) 评估。](https://arxiv.org/pdf/2305.10601.pdf)'
- en: To frame ‘*Game of 24*’ into ToT, we decompose the thoughts into 3 steps, each
    an intermediate equation. As shown in Figure above (a), at each tree node, we
    exact the “left” numbers and prompt the LM to propose some possible next steps.
    The same “propose prompt” is used for all 3 thought steps, though it only has
    one example with 4 input numbers. We perform a breadth-first search (BFS) in ToT,
    where at each step we keep the best b = 5 candidates. To perform deliberate BFS
    in ToT, as shown in Figure (b), we prompt LM to evaluate each thought candidate
    as “sure/maybe/impossible” with regard to reaching 24\. The aim is to promote
    correct partial solutions that can be verdicted within few look-ahead trials,
    and eliminate impossible partial solutions based on “too big/small” commonsense,
    and keep the rest “maybe”. We sample values 3 times for each thought.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将‘*24点游戏*’框架转入 ToT，我们将思维分解为3个步骤，每个步骤是一个中间方程。如上图（a）所示，在每个树节点，我们提取“左侧”数字，并提示
    LM 提出一些可能的下一步。所有3个思维步骤使用相同的“提出提示”，尽管它只有一个包含4个输入数字的示例。我们在 ToT 中执行广度优先搜索（BFS），在每一步我们保留最佳的
    b = 5 个候选项。为了在 ToT 中执行深思熟虑的 BFS，如图（b）所示，我们提示 LM 评估每个思维候选项为“确定/可能/不可能”，以判断是否能达到24。目的是推广可以在少量前瞻试验中判定的正确部分解决方案，并根据“过大/过小”的常识消除不可能的部分解决方案，保留其余的“可能”。我们对每个思维进行3次采样。
- en: 4\. **Search algorithm:** We try to expand the tree. For each leaf node, we
    evaluate it with the state evaluation function. To choose which leaf node for
    evaluation, we use a search algorithm. It could be a breadth-first search and
    a depth-first search. One can plug and play different search algorithms depending
    on the tree structure.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. **搜索算法**：我们尝试扩展树。对于每个叶子节点，我们使用状态评估函数对其进行评估。选择哪个叶子节点进行评估时，我们使用搜索算法。它可以是广度优先搜索或深度优先搜索。根据树的结构，可以插入不同的搜索算法。
- en: 'Conceptually, ToT has several benefits as a method for general problem-solving
    with LMs:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，ToT 作为一种通用问题解决方法具有若干优点：
- en: '**Generality**: IO, CoT, CoT-SC, and self-refinement can be seen as special
    cases of ToT (i.e. trees of limited depth and breadth'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通用性**：IO、CoT、CoT-SC 和自我修正可以视为 ToT 的特例（即有限深度和广度的树）。'
- en: '**Modularity**: The base LM, as well as the thought decomposition, generation,
    evaluation, and search procedures, can all be varied independently.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模块化**：基础语言模型以及思维分解、生成、评估和搜索过程都可以独立变化。'
- en: '**Adaptability**: Different problem properties, LM capabilities, and resource
    constraints can be accommodated.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**适应性**：可以适应不同的问题属性、语言模型能力和资源限制。'
- en: '**Convenience**: No extra training is needed, just a pre-trained LM is sufficient.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**便利性**：无需额外培训，只需一个预训练的语言模型即可。'
- en: ToT framework empowers LMs to more autonomously and intelligently make decisions
    and solve problems.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ToT 框架使语言模型能够更自主和智能地做出决策和解决问题。
- en: '**Limitations**. ToT requires more resources (e.g. model API cost) than sampling
    methods in order to improve task performances, but the modular flexibility of
    ToT allows users to customize such performance-cost tradeoffs, and ongoing open-source
    efforts should readily reduce such costs in the near future.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**局限性**：ToT 需要比采样方法更多的资源（例如模型 API 成本）以提高任务表现，但 ToT 的模块化灵活性允许用户自定义这种性能-成本权衡，并且持续的开源努力应该能在不久的将来降低这些成本。'
- en: Auto Prompt Techniques
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动提示技术
- en: Prompt engineering is an empirical science and the effect of prompt engineering
    methods can vary a lot among models, thus requiring heavy experimentation and
    heuristics. *Can we automate this process of prompt engineering?* This is an active
    research area and the following section discusses some attempts towards automatic
    prompt design approaches.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程是一门经验科学，提示工程方法的效果在模型之间可能差异很大，因此需要大量实验和启发式方法。*我们能否自动化这种提示工程过程？* 这是一个活跃的研究领域，以下部分讨论了一些自动提示设计方法的尝试。
- en: Automatic Prompt Augmentation and Selection COT
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动提示增强与选择 COT
- en: 'In the paper titled “*Automatic Prompt Augmentation and Selection with Chain-of-Thought
    from Labeled Data*” [11]. Most CoT studies rely on carefully designed human-annotated
    rational chains to prompt the language model, which poses challenges for real-world
    applications where labeled training data is available without human-annotated
    rational chains. To construct chain-of-thought prompts automatically, authors
    suggested augment-prune-select, a three-step process:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在题为“*基于标记数据的链式思维自动提示增强与选择*”的论文中[11]。大多数 CoT 研究依赖于精心设计的人类标注的理性链来提示语言模型，这在实际应用中提出了挑战，因为标记的训练数据可用，但没有人类标注的理性链。为了自动构建链式思维提示，作者建议了增强-修剪-选择的三步过程：
- en: '**Augment**: Generate multiple pseudo-chains of thought given question using
    few-shot or zero-shot CoT prompts;'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**增强**：使用少量示例或零示例 CoT 提示生成多个伪链式思维；'
- en: '**Prune**: Prune pseudo chains based on whether generated answers match ground
    truths.'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**修剪**：根据生成的答案是否与真实值匹配来修剪伪链。'
- en: '**Select**: Apply a variance-reduced policy gradient strategy to learn the
    probability distribution over selected examples, while considering the probability
    distribution over examples as policy and the validation set accuracy as reward.'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择**：应用减少方差的策略梯度策略来学习选定示例的概率分布，同时将示例的概率分布视为策略，将验证集的准确性视为奖励。'
- en: 'Auto-CoT: Automatic Chain-of-Thought Prompting'
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Auto-CoT：自动链式思维提示
- en: In “*Automatic Chain-of-Thought Prompting in Large Language Models*” [12], the
    authors propose Auto-CoT paradigm to automatically construct demonstrations with
    questions and reasoning chains. In this technique, authors adopted clustering
    techniques to sample questions and then generates chains. They observed that LLMs
    tend to make certain types of mistakes. One type of errors can be similar in the
    embedding space and thus get grouped together. By only sampling one or a few from
    frequent-error clusters, we can prevent too many wrong demonstrations of one error
    type and collect a diverse set of examples.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在“*大规模语言模型中的自动化链式思维提示*”[12]中，作者提出了Auto-CoT范式，以自动构建带有问题和推理链的示例。在这一技术中，作者采用了聚类技术来抽样问题，然后生成链。作者观察到，LLM往往会犯某些类型的错误。一种错误可能在嵌入空间中类似，因此被分组在一起。通过仅从频繁错误簇中抽取一个或几个样本，我们可以防止过多错误类型的错误示例，并收集多样的例子。
- en: '![](../Images/4c5f48633aedff23b526f5e76b36405b.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c5f48633aedff23b526f5e76b36405b.png)'
- en: '[Auto-COT : Automatic Chain-of-Though Prompting](https://arxiv.org/pdf/2210.03493.pdf)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[Auto-COT : 自动化链式思维提示](https://arxiv.org/pdf/2210.03493.pdf)'
- en: '**Auto-CoT** consists of the following main stages:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**Auto-CoT**包括以下主要阶段：'
- en: '**Question clustering**: Perform cluster analysis for a given set of questions
    Q. First compute a vector representation for each question in Q by Sentence-BERT.
    The contextualized vectors are averaged to form a fix-sized question representation.
    Then, the question representations are processed by the k-means clustering algorithm
    to produce k clusters of questions.'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**问题聚类**：对给定的问题集Q进行聚类分析。首先通过Sentence-BERT计算Q中每个问题的向量表示。将上下文化的向量平均化以形成固定大小的问题表示。然后，使用k-means聚类算法处理问题表示，生成k个问题簇。'
- en: '**Demonstration selection**: Select a set of representative questions from
    each cluster; i.e. one demonstration from one cluster. Samples in each cluster
    are sorted by distance to the cluster centroid and those closer to the centroid
    are selected first.'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**示例选择**：从每个簇中选择一组具有代表性的问题；即从一个簇中选择一个示例。每个簇中的样本按距离簇中心的远近排序，距离中心较近的样本优先选择。'
- en: '**Rationale generation**: Use zero-shot CoT to generate reasoning chains for
    selected questions and construct few-shot prompt to run inference.'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**推理生成**：使用零-shot CoT为选定的问题生成推理链，并构建少-shot提示以进行推理。'
- en: LLMs have shown reasoning capabilities with CoT prompting. The superior performance
    of Manual-CoT hinges on the hand-crafting of demonstrations. To eliminate such
    manual designs, the proposed Auto-CoT automatically constructs demonstrations.
    It samples questions with diversity and generates reasoning chains to construct
    demonstrations. Experimental results on reasoning datasets showed that with GPT-3,
    Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that
    requires manual designs of demonstrations.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: LLM在CoT提示下展示了推理能力。Manual-CoT的优越性能依赖于手工制作示例。为了消除这种手工设计，提出的Auto-CoT自动构建示例。它抽样具有多样性的问题并生成推理链以构建示例。对推理数据集的实验结果表明，在GPT-3上，Auto-CoT的表现始终与需要手工设计示例的CoT范式相匹配或超越。
- en: Conclusion
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In-context learning or prompting helps us to communicate with LLM to steer its
    behavior for desired outcomes. It is an attractive approach to extracting information
    because you don’t need a large offline training set, you don’t need offline access
    to a model, and it feels intuitive even for non-engineers. Prompt engineering
    aims to utilize prompting as a way to build reliable functionality for real-world
    applications. It is an empirical science and the effect of prompt engineering
    methods can vary a lot among models, thus requiring heavy experimentation and
    heuristics. Prompting requires significant human efforts to create and adapt to
    new datasets. The annotation process is nontrivial because humans need to not
    only select the questions but also carefully design the reasoning steps for each
    question, so there is a need for automation of the prompting techniques.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文学习或提示有助于我们与LLM沟通，以引导其行为实现期望的结果。这是一种提取信息的有吸引力的方法，因为你不需要大量的离线训练集，不需要离线访问模型，并且即使对于非工程师也感觉直观。提示工程旨在利用提示作为为实际应用构建可靠功能的方法。这是一门经验科学，提示工程方法的效果在模型之间可能差异很大，因此需要大量实验和启发式方法。提示需要大量人力来创建和适应新的数据集。注释过程并不简单，因为人类不仅需要选择问题，还需要仔细设计每个问题的推理步骤，因此有必要对提示技术进行自动化。
- en: References
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] A Survey of Large Language Models, [https://arxiv.org/pdf/2303.18223.pdf](https://arxiv.org/pdf/2303.18223.pdf)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 大型语言模型调查，[https://arxiv.org/pdf/2303.18223.pdf](https://arxiv.org/pdf/2303.18223.pdf)'
- en: '[2] A Survey on In-Context Learning, [https://arxiv.org/pdf/2301.00234.pdf](https://arxiv.org/pdf/2301.00234.pdf)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] 上下文学习调查，[https://arxiv.org/pdf/2301.00234.pdf](https://arxiv.org/pdf/2301.00234.pdf)'
- en: '[3] Emergent Abilities of Large Language Models, [https://arxiv.org/pdf/2206.07682.pdf](https://arxiv.org/pdf/2206.07682.pdf)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] 大型语言模型的突现能力，[https://arxiv.org/pdf/2206.07682.pdf](https://arxiv.org/pdf/2206.07682.pdf)'
- en: '[4] Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient
    Descent as Meta-Optimizers, [https://arxiv.org/pdf/2212.10559.pdf](https://arxiv.org/pdf/2212.10559.pdf)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] 为什么GPT可以进行上下文学习？语言模型隐式地执行梯度下降作为元优化器，[https://arxiv.org/pdf/2212.10559.pdf](https://arxiv.org/pdf/2212.10559.pdf)'
- en: '[5] An Explanation of In-context Learning as Implicit Bayesian Inference, [http://ai.stanford.edu/blog/understanding-incontext/](http://ai.stanford.edu/blog/understanding-incontext/)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] 将上下文学习解释为隐式贝叶斯推理，[http://ai.stanford.edu/blog/understanding-incontext/](http://ai.stanford.edu/blog/understanding-incontext/)'
- en: '[6] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,
    [https://arxiv.org/pdf/2201.11903.pdf](https://arxiv.org/pdf/2201.11903.pdf)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] 链式思维提示激发大型语言模型中的推理，[https://arxiv.org/pdf/2201.11903.pdf](https://arxiv.org/pdf/2201.11903.pdf)'
- en: '[7] Large Language Models are Zero-shot Reasoners, [https://arxiv.org/pdf/2205.11916.pdf](https://arxiv.org/pdf/2205.11916.pdf)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] 大型语言模型是零样本推理者，[https://arxiv.org/pdf/2205.11916.pdf](https://arxiv.org/pdf/2205.11916.pdf)'
- en: '[8] In-context learning and induction heads. Transformer Circuits, 2022\. [https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)
    .'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] 上下文学习与归纳头。Transformer电路，2022。[https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)'
- en: '[9] Self-consistency improves chain-of-thought reasoning in LLM, [https://arxiv.org/pdf/2203.11171.pdf](https://arxiv.org/pdf/2203.11171.pdf)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] 自洽性提升了LLM中的链式思维推理，[https://arxiv.org/pdf/2203.11171.pdf](https://arxiv.org/pdf/2203.11171.pdf)'
- en: '[10] Tree of Thoughts, [https://arxiv.org/pdf/2305.10601.pdf](https://arxiv.org/pdf/2305.10601.pdf)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] 思维树，[https://arxiv.org/pdf/2305.10601.pdf](https://arxiv.org/pdf/2305.10601.pdf)'
- en: '[11] Automatic Prompt Augmentation and Selection with Chain-of-Thought from
    Labeled Data [https://arxiv.org/pdf/2302.12822.pdf](https://arxiv.org/pdf/2302.12822.pdf)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] 自动提示增强与从标注数据中链式思维的选择 [https://arxiv.org/pdf/2302.12822.pdf](https://arxiv.org/pdf/2302.12822.pdf)'
- en: '[12] Automatic Chain-of-Thought Prompting in Large Language Models, [https://arxiv.org/pdf/2210.03493.pdf](https://arxiv.org/pdf/2210.03493.pdf)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] 大型语言模型中的自动链式思维提示，[https://arxiv.org/pdf/2210.03493.pdf](https://arxiv.org/pdf/2210.03493.pdf)'
- en: '[13] Large Language models can Self Improve, [https://www.arxiv-vanity.com/papers/2210.11610/](https://www.arxiv-vanity.com/papers/2210.11610/)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] 大型语言模型可以自我提升，[https://www.arxiv-vanity.com/papers/2210.11610/](https://www.arxiv-vanity.com/papers/2210.11610/)'
