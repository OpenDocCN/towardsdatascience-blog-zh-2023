- en: 'Tensor Quantization: The Untold Story'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/tensor-quantization-the-untold-story-d798c30e7646?source=collection_archive---------2-----------------------#2023-09-08](https://towardsdatascience.com/tensor-quantization-the-untold-story-d798c30e7646?source=collection_archive---------2-----------------------#2023-09-08)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A close look at the implementation details of quantization in machine learning
    frameworks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@dhruvbird?source=post_page-----d798c30e7646--------------------------------)[![Dhruv
    Matani](../Images/d63bf7776c28a29c02b985b1f64abdd3.png)](https://medium.com/@dhruvbird?source=post_page-----d798c30e7646--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d798c30e7646--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d798c30e7646--------------------------------)
    [Dhruv Matani](https://medium.com/@dhruvbird?source=post_page-----d798c30e7646--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F63f5d5495279&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftensor-quantization-the-untold-story-d798c30e7646&user=Dhruv+Matani&userId=63f5d5495279&source=post_page-63f5d5495279----d798c30e7646---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d798c30e7646--------------------------------)
    ·8 min read·Sep 8, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd798c30e7646&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftensor-quantization-the-untold-story-d798c30e7646&user=Dhruv+Matani&userId=63f5d5495279&source=-----d798c30e7646---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd798c30e7646&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftensor-quantization-the-untold-story-d798c30e7646&source=-----d798c30e7646---------------------bookmark_footer-----------)![](../Images/a22f43f23f8aac57d5a7eb5b46fcbb6b.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Co-authored with [Naresh Singh](https://medium.com/@brocolishbroxoli).
  prefs: []
  type: TYPE_NORMAL
- en: Table Of Contents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Introduction](#1d38)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What do the terms scale and zero-point mean for quantization?](#038c)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Types of Quantization Schemes](#b2df)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Quantization Scale and Zero Point examples](#0dd2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Quantization and Activation Normalization](#3046)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Conclusion](#8bf9)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[References](#390b)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout the rest of this article, we will try to answer the following questions
    with concrete examples.
  prefs: []
  type: TYPE_NORMAL
- en: What do the terms scale and zero-point mean for quantization?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the different types of quantization schemes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to compute scale and zero-point for the different quantization schemes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is zero-point important for quantization?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do normalization techniques benefit quantization
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What do the terms scale and zero-point mean for quantization?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Scale:** When quantizing a floating point range, one would typically represent
    a floating point range [Fmin..Fmax] in the quantized range [Qmin..Qmax]. In this
    case, the scale is the ratio of the floating point range and the quantized range.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5581bf2fe1446f4e490eb3e7364dac38.png)'
  prefs: []
  type: TYPE_IMG
- en: We will see an example of how to compute it later.
  prefs: []
  type: TYPE_NORMAL
- en: '**Zero-point:** The zero-point for quantization is the representation of floating
    point 0.0 in the quantized range. Specifically, the zero-point is a quantized
    value, and it represents the floating point value 0.0 for all practical purposes.
    We shall see how it’s computed with examples later, along with why such a representation
    is of practical interest to us.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s take a look at the main quantization schemes used in practice and
    familiarize ourselves with how they are similar and how they differ.
  prefs: []
  type: TYPE_NORMAL
- en: Types of Quantization Schemes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When considering the types of quantization available for use during model compression,
    there are 2 main types to pick from
  prefs: []
  type: TYPE_NORMAL
- en: '**Symmetric quantization**: In this case, the zero-point is zero — i.e. 0.0
    of the floating point range is the same as 0 in the quantized range. Typically,
    this is more efficient to compute at runtime but may result in lower accuracy
    if the floating point range is unequally distributed around the floating point
    0.0.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Affine (or asymmetric) quantization**: This is the one that has a zero-point
    that is non-zero in value.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: But before we jump into the details, let’s try to define what zero-point means.
  prefs: []
  type: TYPE_NORMAL
- en: Quantization Scale and Zero Point examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s start with a very simple example and build it up.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example-1: Symmetric uint8 quantization'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s say we wish to map the floating point range [0.0 .. 1000.0] to the quantized
    range [0 .. 255]. The range [0 .. 255] is the set of values that can fit in an
    unsigned 8-bit integer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/850c70ba5537ced2267365e6c52f714b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To perform this transformation, we want to rescale the floating point range
    so that the following is true:'
  prefs: []
  type: TYPE_NORMAL
- en: Floating point 0.0 = Quantized 0
  prefs: []
  type: TYPE_NORMAL
- en: Floating point 1000.0 = Quantized 255
  prefs: []
  type: TYPE_NORMAL
- en: This is called symmetric quantization because the floating point 0.0 is quantized
    0.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, we define a scale, which is equal to
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c82de58dacb9ed664f5ec470c9c1af06.png)'
  prefs: []
  type: TYPE_IMG
- en: Where,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/33ff9ebd7202cf25d953b8c6b1484bfd.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, scale = 3.9215
  prefs: []
  type: TYPE_NORMAL
- en: To convert from a floating point value to a quantized value, we can simply divide
    the floating point value by the scale. For example, the floating point value 500.0
    corresponds to the quantized value
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/87ad1feedc2e3b4ceff1e37848a943ec.png)'
  prefs: []
  type: TYPE_IMG
- en: In this simple example, the 0.0 of the floating point range maps exactly to
    the 0 in the quantized range. This is called symmetric quantization. Let’s see
    what happens when this is not the case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example-2: Affine uint8 quantization'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s say we wish to map the floating point range [-20.0 .. 1000.0] to the quantized
    range [0 .. 255].
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/def44e61e639cd134c4e764456991cae.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, we have a different scaling factor since our *xmin* is different.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/054faad1712ca2ccce30b743311a235a.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s see what the floating point number 0.0 is represented by in the quantized
    range if we apply the scaling factor to 0.0
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1f7c7e7499505ca2a530c2822ddb729e.png)'
  prefs: []
  type: TYPE_IMG
- en: Well, this doesn’t quite seem right since, according to the diagram above, we
    would have expected the floating point value -20.0 to map to the quantized value
    0.
  prefs: []
  type: TYPE_NORMAL
- en: This is where the concept of zero-point comes in. **The zero-point acts as a
    bias for shifting the scaled floating point value and corresponds to the value
    in the quantized range that represents the floating point value 0.0.** In our
    case, the zero point is the negative of the scaled floating point representation
    of -20.0, which is -(-5) = 5\. The zero point is always the negative of the representation
    of the minimum floating point value since the minimum will always be negative
    or zero. We’ll find out more about why this is the case in the section that explains
    example 4.
  prefs: []
  type: TYPE_NORMAL
- en: Whenever we quantize a value, we will always add the zero-point to this scaled
    value to get the actual quantized value in the valid quantization range. In case
    we wish to quantize the value -20.0, we compute it as the scaled value of -20.0
    plus the zero-point, which is -5 + 5 = 0\. Hence, quantized(-20.0, scale=4, zp=5)
    = 0.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b8df43ccbd85a458a7d4d59729b98f18.png)![](../Images/d74f91262f01c6d0d31cf207cffff488.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Example-3: Affine int8 quantization'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What happens if our quantized range is a signed 8-bit integer instead of an
    unsigned 8-bit integer? Well, the range is now [-128 .. 127].
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2a5ec545adf0c908cafa4d0a9c21bad1.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, -20.0 in the float range maps to -128 in the quantized range,
    and 1000.0 in the float range maps to 127 in the quantized range.
  prefs: []
  type: TYPE_NORMAL
- en: The way we calculate zero point is that we compute it as if the quantized range
    is [0 .. 255] and then offset it with -128, so the zero point in the new range
    is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/106ec18c82f700492b347018a489562c.png)![](../Images/1ace2dcbf84ab21bbebf4aa4d7b98e81.png)'
  prefs: []
  type: TYPE_IMG
- en: Hence, the zero-point for the new range is -123.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we’ve looked at examples where the floating point range includes the
    value 0.0\. In the next set of examples, we’ll take a look at what happens when
    the floating point range doesn’t include the value 0.0
  prefs: []
  type: TYPE_NORMAL
- en: The importance of 0.0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Why is it important for the floating point value 0.0 to be represented in the
    floating point range?
  prefs: []
  type: TYPE_NORMAL
- en: When using a padded convolution, we expect the border pixels to be padded using
    the value 0.0 in the most common case. Hence, it’s important for 0.0 to be represented
    in the floating point range. Similarly, if the value X is going to be used for
    padding in your network, you need to make sure that the value X is represented
    in the floating point range and that quantization is aware of this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example-4: The untold story — skewed floating point range'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, let’s take a look at what happens if 0.0 isn’t part of the floating point
    range.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7ee42df4a6d2444005594e318c0dfbb5.png)'
  prefs: []
  type: TYPE_IMG
- en: In this example, we’re trying to quantize the floating point range [40.0 ..
    1000.0] into the quantized range [0 .. 255].
  prefs: []
  type: TYPE_NORMAL
- en: Since we can’t represent the value 0.0 in the floating point range, we need
    to extend the lower limit of the range to 0.0.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3b04eb04bfaea206e994e16609f49ec5.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that some part of the quantized range is wasted. To determine how
    much, let’s compute the quantized value that the floating point value 40.0 maps
    to.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/08e2bd0a246d71073117f3672df4b756.png)'
  prefs: []
  type: TYPE_IMG
- en: Hence, we’re wasting the range [0 .. 9] in the quantized range, which is about
    3.92% of the range. This could significantly affect the model’s accuracy post-quantization.
  prefs: []
  type: TYPE_NORMAL
- en: This skewing is necessary if we wish to make sure that the value 0.0 in the
    floating point range can be represented in the quantized range.
  prefs: []
  type: TYPE_NORMAL
- en: Another reason for including the value 0.0 in the floating point range is that
    efficiently comparing a quantized value to check if it’s 0.0 in the floating point
    range is very valuable. Think of operators such as ReLU, which clip all values
    below 0.0 in the floating point range to 0.0.
  prefs: []
  type: TYPE_NORMAL
- en: It is important for us to be able to **represent the zero-point using the same
    data type** (signed or unsigned int8) **as the quantized values**. This enables
    us to perform these comparisons quickly and efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s take a look at how activation normalization helps with model quantization.
    We’ll specifically focus on how the standardization of the activation values allows
    us to use the entire quantized range effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Quantization and Activation Normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Batch/Layer Normalization changes the activation tensor to have zero mean and
    unit variance per channel or per layer.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have an input tensor with a floating point range of [2000.0 .. 4000.0].
    This is what the quantized range would look like.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c4127fea06de80d1d7e866f5a177c431.png)'
  prefs: []
  type: TYPE_IMG
- en: We observe that half of the quantized range [-127 .. -1] is unused. This is
    problematic since we’re quantizing the entire floating point range using just
    7 of the available 8 bits. This will undoubtedly result in a higher quantization
    error and reduced model accuracy. To address this let’s apply layer normalization
    to the activation tensor.
  prefs: []
  type: TYPE_NORMAL
- en: After applying layer normalization to an activation tensor, the activation tensor
    will have a floating point range of [-2.0 .. 2.0]. This can be represented in
    the signed int8 range [-128 .. 127]. To ensure symmetry of the distribution, we
    restrict the quantized range to be [-127 .. 127].
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4ed26298d21e7890be02a9a27e2ba69f.png)'
  prefs: []
  type: TYPE_IMG
- en: Hence, normalization avoids holes or unused parts in the quantized range.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We saw what affine (asymmetric) and symmetric quantization are and how they
    differ. We also learned about what scale and zero-point mean and how to compute
    them for both these types of quantization schemes.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we saw the need to include float 0.0 in the floating point range and why
    and how this is done in practice. This results in a downside, namely wasted space
    in the quantized range.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we saw how normalization helps quantization by bringing the activations
    in a fixed range and avoiding wasted space in the quantized range. In fact, the
    0 mean based normalization can help convert affine quantization to symmetric quantization,
    and that can speed things up during inference.
  prefs: []
  type: TYPE_NORMAL
- en: All images in this post are created by the author(s).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Efficient Deep Learning Book, Chapter-2: Introduction to Compression Techniques](https://github.com/EfficientDL/book/raw/main/book/%5BEDL%5D%20Chapter%202%20-%20Compression%20Techniques.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Hugging Face: Quantization](https://huggingface.co/docs/optimum/concept_guides/quantization)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[TensorRT: Quantization](https://docs.nvidia.com/deeplearning/tensorrt/tensorflow-quantization-toolkit/docs/docs/intro_to_quantization.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Neural Network Distiller: Quantization](https://intellabs.github.io/distiller/algo_quantization.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Lei Mao: Quantization](https://leimao.github.io/article/Neural-Networks-Quantization/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Quantizing floats](https://zeux.io/2010/12/14/quantizing-floats/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
