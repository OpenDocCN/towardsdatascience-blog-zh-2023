- en: Understanding LoRA — Low Rank Adaptation For Finetuning Large Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/understanding-lora-low-rank-adaptation-for-finetuning-large-models-936bce1a07c6?source=collection_archive---------1-----------------------#2023-12-22](https://towardsdatascience.com/understanding-lora-low-rank-adaptation-for-finetuning-large-models-936bce1a07c6?source=collection_archive---------1-----------------------#2023-12-22)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Math behind this parameter efficient finetuning method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://bhavinjawade.medium.com/?source=post_page-----936bce1a07c6--------------------------------)[![Bhavin
    Jawade](../Images/d10f204b2d0a8a3fd6887048ecaa307d.png)](https://bhavinjawade.medium.com/?source=post_page-----936bce1a07c6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----936bce1a07c6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----936bce1a07c6--------------------------------)
    [Bhavin Jawade](https://bhavinjawade.medium.com/?source=post_page-----936bce1a07c6--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F11a205eeb0d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-lora-low-rank-adaptation-for-finetuning-large-models-936bce1a07c6&user=Bhavin+Jawade&userId=11a205eeb0d3&source=post_page-11a205eeb0d3----936bce1a07c6---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----936bce1a07c6--------------------------------)
    ·4 min read·Dec 22, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F936bce1a07c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-lora-low-rank-adaptation-for-finetuning-large-models-936bce1a07c6&user=Bhavin+Jawade&userId=11a205eeb0d3&source=-----936bce1a07c6---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F936bce1a07c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-lora-low-rank-adaptation-for-finetuning-large-models-936bce1a07c6&source=-----936bce1a07c6---------------------bookmark_footer-----------)![](../Images/5703f134a85ea9f30a74e384e46bba18.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Source — Image generated using DALLE-3\. Prompt: a smaller robot shaking hands
    with a bigger robot. The smaller robot is purple, lightning, active and energetic,
    while the bigger robot is frozen in ice and gray.'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning large pre-trained models is computationally challenging, often involving
    adjustment of millions of parameters. This traditional fine-tuning approach, while
    effective, demands substantial computational resources and time, posing a bottleneck
    for adapting these models to specific tasks. LoRA presented an effective solution
    to this problem by decomposing the update matrix during finetuing. To study LoRA,
    let us start by first revisiting traditional finetuing.
  prefs: []
  type: TYPE_NORMAL
- en: Decomposition of ( Δ W )
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In traditional fine-tuning, we modify a pre-trained neural network’s weights
    to adapt to a new task. This adjustment involves altering the original weight
    matrix ( W ) of the network. The changes made to ( W ) during fine-tuning are
    collectively represented by ( Δ W ), such that the updated weights can be expressed
    as ( W + Δ W ).
  prefs: []
  type: TYPE_NORMAL
- en: Now, rather than modifying ( W ) directly, the LoRA approach seeks to decompose
    ( Δ W ). This decomposition is a crucial step in reducing the computational overhead
    associated with fine-tuning large models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f82bfcfa1f214d4914baa83392d0f3e5.png)'
  prefs: []
  type: TYPE_IMG
- en: Traditional finetuning can be reimagined us above. Here W is frozen where as
    ΔW is trainable (Image by the blog author)
  prefs: []
  type: TYPE_NORMAL
- en: The Intrinsic Rank Hypothesis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The intrinsic rank hypothesis suggests that significant changes to the neural
    network can be captured using a lower-dimensional representation. Essentially,
    it posits that not all elements of ( Δ W ) are equally important; instead, a smaller
    subset of these changes can effectively encapsulate the necessary adjustments.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Matrices ( A ) and ( B )
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Building on this hypothesis, LoRA proposes representing ( Δ W ) as the product
    of two smaller matrices, ( A ) and ( B ), with a lower rank. The updated weight
    matrix ( W’ ) thus becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[ W’ = W + BA ]'
  prefs: []
  type: TYPE_NORMAL
- en: In this equation, ( W ) remains frozen (i.e., it is not updated during training).
    The matrices ( B ) and ( A ) are of lower dimensionality, with their product (
    BA ) representing a low-rank approximation of ( Δ W ).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a57eb454cfa654b203ca8adb8fe7c233.png)'
  prefs: []
  type: TYPE_IMG
- en: ΔW is decomposed into two matrices A and B where both have lower dimensionality
    then d x d. (Image by the blog author)
  prefs: []
  type: TYPE_NORMAL
- en: Impact of Lower Rank on Trainable Parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By choosing matrices ( A ) and ( B ) to have a lower rank ( r ), the number
    of trainable parameters is significantly reduced. For example, if ( W ) is a (
    d x d ) matrix, traditionally, updating ( W ) would involve ( d² ) parameters.
    However, with ( B ) and ( A ) of sizes ( d x r ) and ( r x d ) respectively, the
    total number of parameters reduces to ( 2dr ), which is much smaller when ( r
    << d ).
  prefs: []
  type: TYPE_NORMAL
- en: 'The reduction in the number of trainable parameters, as achieved through the
    Low-Rank Adaptation (LoRA) method, offers several significant benefits, particularly
    when fine-tuning large-scale neural networks:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reduced Memory Footprint: LoRA decreases memory needs by lowering the number
    of parameters to update, aiding in the management of large-scale models.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Faster Training and Adaptation: By simplifying computational demands, LoRA
    accelerates the training and fine-tuning of large models for new tasks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Feasibility for Smaller Hardware: LoRA’s lower parameter count enables the
    fine-tuning of substantial models on less powerful hardware, like modest GPUs
    or CPUs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scaling to Larger Models: LoRA facilitates the expansion of AI models without
    a corresponding increase in computational resources, making the management of
    growing model sizes more practical.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the context of LoRA, the concept of rank plays a pivotal role in determining
    the efficiency and effectiveness of the adaptation process. Remarkably, the paper
    highlights that the rank of the matrices *A* and *B* can be astonishingly low,
    sometimes as low as one.
  prefs: []
  type: TYPE_NORMAL
- en: Although the LoRA paper predominantly showcases experiments within the realm
    of Natural Language Processing (NLP), the underlying approach of low-rank adaptation
    holds broad applicability and could be effectively employed in training various
    types of neural networks across different domains.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LoRA’s approach to decomposing ( Δ W ) into a product of lower rank matrices
    effectively balances the need to adapt large pre-trained models to new tasks while
    maintaining computational efficiency. The intrinsic rank concept is key to this
    balance, ensuring that the essence of the model’s learning capability is preserved
    with significantly fewer parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'References:'
  prefs: []
  type: TYPE_NORMAL
- en: '[[1] Hu, Edward J., et al. “Lora: Low-rank adaptation of large language models.”
    *arXiv preprint arXiv:2106.09685* (2021).](https://arxiv.org/abs/2106.09685)'
  prefs: []
  type: TYPE_NORMAL
