- en: Fine-Tune Your LLM Without Maxing Out Your GPU
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在不超负荷 GPU 的情况下微调你的大型语言模型
- en: 原文：[https://towardsdatascience.com/fine-tune-your-llm-without-maxing-out-your-gpu-db2278603d78?source=collection_archive---------5-----------------------#2023-08-01](https://towardsdatascience.com/fine-tune-your-llm-without-maxing-out-your-gpu-db2278603d78?source=collection_archive---------5-----------------------#2023-08-01)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/fine-tune-your-llm-without-maxing-out-your-gpu-db2278603d78?source=collection_archive---------5-----------------------#2023-08-01](https://towardsdatascience.com/fine-tune-your-llm-without-maxing-out-your-gpu-db2278603d78?source=collection_archive---------5-----------------------#2023-08-01)
- en: How you can fine-tune your LLMs with limited hardware and a tight budget
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何在有限的硬件和紧张的预算下微调你的大型语言模型
- en: '[](https://johnadeojo.medium.com/?source=post_page-----db2278603d78--------------------------------)[![John
    Adeojo](../Images/f6460fae462b055d36dce16fefcd142c.png)](https://johnadeojo.medium.com/?source=post_page-----db2278603d78--------------------------------)[](https://towardsdatascience.com/?source=post_page-----db2278603d78--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----db2278603d78--------------------------------)
    [John Adeojo](https://johnadeojo.medium.com/?source=post_page-----db2278603d78--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://johnadeojo.medium.com/?source=post_page-----db2278603d78--------------------------------)[![John
    Adeojo](../Images/f6460fae462b055d36dce16fefcd142c.png)](https://johnadeojo.medium.com/?source=post_page-----db2278603d78--------------------------------)[](https://towardsdatascience.com/?source=post_page-----db2278603d78--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----db2278603d78--------------------------------)
    [John Adeojo](https://johnadeojo.medium.com/?source=post_page-----db2278603d78--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff933e1637e40&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-your-llm-without-maxing-out-your-gpu-db2278603d78&user=John+Adeojo&userId=f933e1637e40&source=post_page-f933e1637e40----db2278603d78---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----db2278603d78--------------------------------)
    ·8 min read·Aug 1, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdb2278603d78&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-your-llm-without-maxing-out-your-gpu-db2278603d78&user=John+Adeojo&userId=f933e1637e40&source=-----db2278603d78---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff933e1637e40&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-your-llm-without-maxing-out-your-gpu-db2278603d78&user=John+Adeojo&userId=f933e1637e40&source=post_page-f933e1637e40----db2278603d78---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----db2278603d78--------------------------------)
    ·8 min read·2023年8月1日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdb2278603d78&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-your-llm-without-maxing-out-your-gpu-db2278603d78&user=John+Adeojo&userId=f933e1637e40&source=-----db2278603d78---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdb2278603d78&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-your-llm-without-maxing-out-your-gpu-db2278603d78&source=-----db2278603d78---------------------bookmark_footer-----------)![](../Images/ebaa7adc1f03dc619e66b6d330a31748.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdb2278603d78&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-your-llm-without-maxing-out-your-gpu-db2278603d78&source=-----db2278603d78---------------------bookmark_footer-----------)![](../Images/ebaa7adc1f03dc619e66b6d330a31748.png)'
- en: 'Image by Author: Generated with Midjourney'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作者图片：由 Midjourney 生成
- en: Demand for Bespoke LLMs
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定制大型语言模型的需求
- en: With the success of ChatGPT, we have witnessed a surge in demand for bespoke
    large language models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 ChatGPT 的成功，我们见证了对定制大型语言模型需求的激增。
- en: However, there has been a barrier to adoption. As these models are so large,
    it has been challenging for businesses, researchers, or hobbyists with a modest
    budget to customise them for their own datasets.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种模型的庞大规模也造成了采纳上的障碍。由于这些模型非常大，对于预算有限的企业、研究人员或爱好者来说，定制它们以适应自己的数据集一直是个挑战。
- en: Now with innovations in parameter efficient fine-tuning (PEFT) methods, it is
    entirely possible to fine-tune large language models at a relatively low cost.
    In this article, I demonstrate how to achieve this in a [Google Colab](https://colab.research.google.com/).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，随着参数高效微调（PEFT）方法的创新，完全可以在相对较低的成本下微调大型语言模型。在本文中，我将展示如何在 [Google Colab](https://colab.research.google.com/)
    中实现这一目标。
- en: I anticipate that this article will prove valuable for practitioners, hobbyists,
    learners, and even hands-on start-up founders.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我预期这篇文章对从业者、爱好者、学习者，甚至是动手实践的初创公司创始人都会有很大的价值。
- en: So, if you need to mock up a cheap prototype, test an idea, or create a cool
    data science project to stand out from the crowd — keep reading.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果你需要制作一个便宜的原型、测试一个想法，或者创建一个酷炫的数据科学项目来脱颖而出——继续阅读。
- en: Why Do We Fine-tune?
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们为什么要进行微调？
- en: Businesses often have private datasets that drive some of their processes.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 企业通常拥有驱动其某些流程的私有数据集。
- en: To give you an example, I worked for a bank where we logged customer complaints
    in an Excel spreadsheet. An…
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，我曾在一家银行工作，我们将客户投诉记录在一个Excel电子表格中。一个……
