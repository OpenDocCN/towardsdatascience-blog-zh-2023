- en: Hyperparameter Optimization — Intro and Implementation of Grid Search, Random
    Search and Bayesian Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/hyperparameter-optimization-intro-and-implementation-of-grid-search-random-search-and-bayesian-b2f16c00578a?source=collection_archive---------10-----------------------#2023-03-13](https://towardsdatascience.com/hyperparameter-optimization-intro-and-implementation-of-grid-search-random-search-and-bayesian-b2f16c00578a?source=collection_archive---------10-----------------------#2023-03-13)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Most common hyperparameter optimization methodologies to boost machine learning
    outcomes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@fmnobar?source=post_page-----b2f16c00578a--------------------------------)[![Farzad
    Mahmoodinobar](../Images/2d75209693b712300e6f0796bd2487d0.png)](https://medium.com/@fmnobar?source=post_page-----b2f16c00578a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b2f16c00578a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b2f16c00578a--------------------------------)
    [Farzad Mahmoodinobar](https://medium.com/@fmnobar?source=post_page-----b2f16c00578a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3c56b7d4893e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-intro-and-implementation-of-grid-search-random-search-and-bayesian-b2f16c00578a&user=Farzad+Mahmoodinobar&userId=3c56b7d4893e&source=post_page-3c56b7d4893e----b2f16c00578a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b2f16c00578a--------------------------------)
    ·10 min read·Mar 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb2f16c00578a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-intro-and-implementation-of-grid-search-random-search-and-bayesian-b2f16c00578a&user=Farzad+Mahmoodinobar&userId=3c56b7d4893e&source=-----b2f16c00578a---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb2f16c00578a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-intro-and-implementation-of-grid-search-random-search-and-bayesian-b2f16c00578a&source=-----b2f16c00578a---------------------bookmark_footer-----------)![](../Images/13056882b4fc4ae3b84019b4de503886.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Jonas Jaeken](https://unsplash.com/@jonasjaekenmedia) on [Unsplash](https://unsplash.com/photos/Gg2ttawakqE)
  prefs: []
  type: TYPE_NORMAL
- en: Usually the first solution that comes to mind when trying to improve a machine
    learning model is to just add more training data. Additional data usually helps
    (barring certain situations) but generating high-quality data can be quite expensive.
    Hyperparameter optimization can save us time and resources by getting the best
    model performance using the existing data.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter optimization, as the name suggests, is the process of identifying
    the best combination of hyperparameters for a machine learning model to satisfy
    an optimization function (i.e. maximize the performance of the model, given the
    data set in study). In other words, each model comes with multiple knobs and levers
    that we can change, until we get to the optimized combination. A few examples
    of parameters that we can change during hyperparameter optimization can be learning
    rate, architecture of a neural network (e.g. number of hidden layers), regularization,
    etc.
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we are going to conceptually walk through the three most common
    hyperparameter optimization methodologies, namely Grid…
  prefs: []
  type: TYPE_NORMAL
