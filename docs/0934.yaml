- en: Hyperparameter Optimization — Intro and Implementation of Grid Search, Random
    Search and Bayesian Optimization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数优化——网格搜索、随机搜索和贝叶斯优化的简介及实施
- en: 原文：[https://towardsdatascience.com/hyperparameter-optimization-intro-and-implementation-of-grid-search-random-search-and-bayesian-b2f16c00578a?source=collection_archive---------10-----------------------#2023-03-13](https://towardsdatascience.com/hyperparameter-optimization-intro-and-implementation-of-grid-search-random-search-and-bayesian-b2f16c00578a?source=collection_archive---------10-----------------------#2023-03-13)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/hyperparameter-optimization-intro-and-implementation-of-grid-search-random-search-and-bayesian-b2f16c00578a?source=collection_archive---------10-----------------------#2023-03-13](https://towardsdatascience.com/hyperparameter-optimization-intro-and-implementation-of-grid-search-random-search-and-bayesian-b2f16c00578a?source=collection_archive---------10-----------------------#2023-03-13)
- en: Most common hyperparameter optimization methodologies to boost machine learning
    outcomes
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最常见的超参数优化方法，提升机器学习效果
- en: '[](https://medium.com/@fmnobar?source=post_page-----b2f16c00578a--------------------------------)[![Farzad
    Mahmoodinobar](../Images/2d75209693b712300e6f0796bd2487d0.png)](https://medium.com/@fmnobar?source=post_page-----b2f16c00578a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b2f16c00578a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b2f16c00578a--------------------------------)
    [Farzad Mahmoodinobar](https://medium.com/@fmnobar?source=post_page-----b2f16c00578a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@fmnobar?source=post_page-----b2f16c00578a--------------------------------)[![Farzad
    Mahmoodinobar](../Images/2d75209693b712300e6f0796bd2487d0.png)](https://medium.com/@fmnobar?source=post_page-----b2f16c00578a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b2f16c00578a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b2f16c00578a--------------------------------)
    [Farzad Mahmoodinobar](https://medium.com/@fmnobar?source=post_page-----b2f16c00578a--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3c56b7d4893e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-intro-and-implementation-of-grid-search-random-search-and-bayesian-b2f16c00578a&user=Farzad+Mahmoodinobar&userId=3c56b7d4893e&source=post_page-3c56b7d4893e----b2f16c00578a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b2f16c00578a--------------------------------)
    ·10 min read·Mar 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb2f16c00578a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-intro-and-implementation-of-grid-search-random-search-and-bayesian-b2f16c00578a&user=Farzad+Mahmoodinobar&userId=3c56b7d4893e&source=-----b2f16c00578a---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3c56b7d4893e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-intro-and-implementation-of-grid-search-random-search-and-bayesian-b2f16c00578a&user=Farzad+Mahmoodinobar&userId=3c56b7d4893e&source=post_page-3c56b7d4893e----b2f16c00578a---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b2f16c00578a--------------------------------)
    ·10分钟阅读·2023年3月13日'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb2f16c00578a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-intro-and-implementation-of-grid-search-random-search-and-bayesian-b2f16c00578a&source=-----b2f16c00578a---------------------bookmark_footer-----------)![](../Images/13056882b4fc4ae3b84019b4de503886.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb2f16c00578a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-intro-and-implementation-of-grid-search-random-search-and-bayesian-b2f16c00578a&source=-----b2f16c00578a---------------------bookmark_footer-----------)![](../Images/13056882b4fc4ae3b84019b4de503886.png)'
- en: Photo by [Jonas Jaeken](https://unsplash.com/@jonasjaekenmedia) on [Unsplash](https://unsplash.com/photos/Gg2ttawakqE)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Jonas Jaeken](https://unsplash.com/@jonasjaekenmedia) 提供，来源于 [Unsplash](https://unsplash.com/photos/Gg2ttawakqE)
- en: Usually the first solution that comes to mind when trying to improve a machine
    learning model is to just add more training data. Additional data usually helps
    (barring certain situations) but generating high-quality data can be quite expensive.
    Hyperparameter optimization can save us time and resources by getting the best
    model performance using the existing data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当我们尝试改进机器学习模型时，第一个想到的解决方案就是增加更多的训练数据。额外的数据通常会有所帮助（除了某些情况），但生成高质量的数据可能相当昂贵。超参数优化可以通过利用现有数据来节省时间和资源，从而获得最佳模型性能。
- en: Hyperparameter optimization, as the name suggests, is the process of identifying
    the best combination of hyperparameters for a machine learning model to satisfy
    an optimization function (i.e. maximize the performance of the model, given the
    data set in study). In other words, each model comes with multiple knobs and levers
    that we can change, until we get to the optimized combination. A few examples
    of parameters that we can change during hyperparameter optimization can be learning
    rate, architecture of a neural network (e.g. number of hidden layers), regularization,
    etc.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数优化，顾名思义，是识别机器学习模型最佳超参数组合的过程，以满足优化函数（即在给定数据集的情况下最大化模型的性能）。换句话说，每个模型都有多个可调整的参数，我们可以进行调整，直到找到优化组合。在超参数优化过程中可以更改的一些参数示例包括学习率、神经网络的架构（例如，隐藏层的数量）、正则化等。
- en: In this post, we are going to conceptually walk through the three most common
    hyperparameter optimization methodologies, namely Grid…
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将从概念上介绍三种最常见的超参数优化方法，即网格搜索（Grid）……
