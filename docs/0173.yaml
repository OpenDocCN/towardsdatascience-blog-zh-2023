- en: Secure MLOps with extended Databricks MLFlow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/secure-mlops-with-extended-databricks-mlflow-ee9b7310c5b3?source=collection_archive---------31-----------------------#2023-01-10](https://towardsdatascience.com/secure-mlops-with-extended-databricks-mlflow-ee9b7310c5b3?source=collection_archive---------31-----------------------#2023-01-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Manage model target environments safely
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://luukvandervelden.medium.com/?source=post_page-----ee9b7310c5b3--------------------------------)[![Luuk
    van der Velden](../Images/56280e17ccb18454acbb5fb8be94a850.png)](https://luukvandervelden.medium.com/?source=post_page-----ee9b7310c5b3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ee9b7310c5b3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ee9b7310c5b3--------------------------------)
    [Luuk van der Velden](https://luukvandervelden.medium.com/?source=post_page-----ee9b7310c5b3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff6c293b24331&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsecure-mlops-with-extended-databricks-mlflow-ee9b7310c5b3&user=Luuk+van+der+Velden&userId=f6c293b24331&source=post_page-f6c293b24331----ee9b7310c5b3---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ee9b7310c5b3--------------------------------)
    ·6 min read·Jan 10, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fee9b7310c5b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsecure-mlops-with-extended-databricks-mlflow-ee9b7310c5b3&user=Luuk+van+der+Velden&userId=f6c293b24331&source=-----ee9b7310c5b3---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fee9b7310c5b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsecure-mlops-with-extended-databricks-mlflow-ee9b7310c5b3&source=-----ee9b7310c5b3---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: MLFlow is an opensource Databricks product that supports part of the Machine
    Learning model lifecycle. Its model registry allows model versions to be registered
    under model names to facilitate model deployment. We want to use one MLFlow instance
    and one Databricks workspace to support multiple deployment targets (acceptance,
    staging, production, etc), while providing security guarantees for production
    models. We extended the MLFlow client to manage multiple environments in one model
    registry in a secure manner. This client works in tandem with Databricks permissions
    of which we will show the Azure terraform snippets.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 'github repo: [environment mlflow client repository](https://github.com/nederlandsespoorwegen/environment_mlflow_client)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/48f61f10f7c7dcae556cfa6085454c82.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
- en: Photo by [Aleksa Kalajdzic](https://www.pexels.com/photo/mushroom-on-brown-tree-log-3780917/)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 1 Why one Databricks workspace?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A secure Databricks workspace is deployed in virtual networks, which require
    separate subnets for the control and the compute planes. The IP range of the compute
    plane subnet constrains the maximum amount of parallel compute nodes that can
    be used simultaneously. Due to company wide IP range size limitations we chose
    to use one large subnet for one Databricks workspace instead of multiple smaller
    subnets tied to multiple Databricks workspaces. Thus, we want to manage several
    logical environments (acceptance, preproduction and production) within one Databricks
    workspace.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Scaling with Databricks pools
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Databricks pools allow reuse of idle instances (virtual machines) during cluster
    creation. Pools provide moderate start and auto-scaling speed benefits. Each instance
    in the pool requires one IP from the compute plane subnet. If the maximum is reached
    the next instance creation request will fail, without any elegant error handling
    or backoff mechanism. This limitation of the Databricks pools makes it important
    to have a large subnet for your activities as you can hit the maximum limit at
    any time and your applications will crash.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 2 Identity based security
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our security approach is based on identities, mostly application registrations
    in Azure and Databricks users backed by Azure Active Directory (AAD) users. For
    instance, we run our acceptance tests in the same Databricks workspace on production
    data using separate identities that have read-only permissions copying production
    data to transient acceptance test storage accounts. Identities are very convenient
    on Azure as Databricks supports credential passthrough leveraging all the AAD
    roles and groups that we set on the identities. We also rely on Databricks permissions
    and groups as we work with multiple identities in one Databricks workspace.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Databricks groups
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Each logical environment in our single Databricks workspace has a Databricks
    group for its application principals named f”apps_{env_name}” (i.e. apps_acc,
    apps_prod) and a group that contains all active application principals “apps_all”.
    The groups and their permissions are managed with terraform.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Databricks groups terraform excerpt
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: 3 Environment MLFlow client
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 3.1 MLFlow experiment tracking
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The MLFlow experiments storage is adapted to support multiple logical environments
    by managing the storage location per environment. Our solution assigns directories
    such as “/experiments/acc” and “experiments/prod” to store experiment data. Databricks
    permissions management is used to give directory rights to the respective Databricks
    groups (in this example “apps_acc” and “apps_prod”). This allows for secure logging
    of experiments and models for each of the logical environments, without the user
    having to think about it.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: MLFlow experiments Databricks permssions terraform excerpt
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 MLFlow model registry
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The MLFlow model registry is a central place to register models and model versions
    for use in ML systems. Models logged and registered during data science experiments
    in different environments end up in the same model registry. The MLFlow model
    registry is harder to adapt for use with multiple logical environments. Actually,
    the deployment target concept in MLFlow assumes that each version of the a specific
    model can have a different “stage”. The model stage is a property of a model version
    registered in the MLFlow model registry. It can be set to “None”, “Staging”, “Production”
    and “Archived”. Although helpful the model stage is quite limiting because we
    cannot define our own values for it, thus it does not fully support our need to
    define various logical environments. We do use it to manage permissions on our
    model versions with Databricks, which we will describe later.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Model naming
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We decided that model naming will be our first layer of differentiating between
    any number of logical environments. Every registered model name is postfixed with
    a environment identifier f”{model_name}_{env_name}”. Our MLFlow client manages
    this naming transparently during model registration and retrieval based on the
    environment name it gets from a system environment variable or the environment
    name passed into its constructor. Similar to the experiment management the interface
    is mostly unchanged compared to vanilla MLFlow, due to our abstraction within
    the environment MLFlow client.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: MLFlow client excerpt showing environment model naming
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Model permissions
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our goal is to separate production models from any other environment and prevent
    any non-production principal to register or retrieve production model versions.
    As mentioned, the model Stage can be set to “None”, “Staging”, “Production” and
    “Archived”. The Databricks permission management hooks into the values of the
    model Stage. We can control who can set the “Staging” and “Production” model Stage
    values and who can manage models with these model Stage values using the “CAN_MANAGE_STAGING_VERSIONS”
    and “CAN_MANAGE_PRODUCTION_VERSIONS” permissions. The “Production” Stage permission
    also allows principals to access “Staging” Stage model versions and transition
    them to production model versions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: To separate Production models from models from other environments we assign
    the model Stage automatically when registering a model version. All non-production
    environments assign the model stage value to “Staging”. Model versions registered
    from the production environment are assigned the “Production” stage value. We
    leverage the Databricks permissions assigned to our Databricks groups shown below
    to securely manage our Production models separate from other logical environments.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将生产模型与其他环境中的模型区分开来，我们在注册模型版本时自动分配模型阶段。所有非生产环境将模型阶段值分配为“Staging”。从生产环境注册的模型版本分配“Production”阶段值。我们利用分配给我们
    Databricks 组的 Databricks 权限，安全地将我们的生产模型与其他逻辑环境分开管理。
- en: Databricks MLFLow model permission Azure terraform excerpt
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks MLFlow 模型权限 Azure terraform 摘录
- en: 3.3 Getting it together
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3.3 汇总
- en: Apart from abstractions on top of the vanilla MLFlow client we have added a
    few methods wrapping various actions for your convenience. One of the the extra
    methods is the “log_model_helper”, which handles the various steps to log and
    register a model version and set the appropriate model stage.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对原生 MLFlow 客户端进行的抽象之外，我们还添加了一些封装各种操作的方法以方便使用。其中一个额外的方法是“log_model_helper”，它处理记录和注册模型版本以及设置适当模型阶段的各种步骤。
- en: Registering a model version is commonly done in two steps, first we log a model
    during an experiment run and then we register the logged model as a model version
    of a registered model. Logging a model during an experiment run returns a ModelInfo
    object, which contains a model_uri pointing to the local artifact location. We
    use the model_uri to register a model version under a registered model name and
    upload it to the MLFlow registry. Registering a model version returns a ModelVersion
    object that tells us the autoincremented model version and current_stage of the
    model which will always be “None” right after creation.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 注册模型版本通常分为两个步骤，首先在实验运行期间记录一个模型，然后将记录的模型注册为已注册模型的模型版本。在实验运行期间记录模型会返回一个 ModelInfo
    对象，其中包含一个指向本地工件位置的 model_uri。我们使用 model_uri 将模型版本注册到已注册模型名称下，并将其上传到 MLFlow 注册表。注册模型版本会返回一个
    ModelVersion 对象，该对象告诉我们自动递增的模型版本和模型的 current_stage，创建后会始终是“None”。
- en: Our third step during model version registration is to transition the model
    version stage from “None” to “Staging” or “Production” depending on the logical
    environment. Everyone can create model versions, but the Stage transitions are
    restricted with the Databricks permissions. These three steps are wrapped in the
    “log_model_helper” method. Using this helper method, we can assume that all registered
    model versions have an environment aware name and appropriate stage value.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型版本注册期间的第三步是根据逻辑环境将模型版本阶段从“None”过渡到“Staging”或“Production”。每个人都可以创建模型版本，但阶段转换受到
    Databricks 权限的限制。这三个步骤都封装在“log_model_helper”方法中。使用这个辅助方法，我们可以假设所有已注册的模型版本都有环境感知的名称和适当的阶段值。
- en: log_model_helper excerpt
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: log_model_helper 摘录
- en: 3.4 Testing our MLFlow client
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3.4 测试我们的 MLFlow 客户端
- en: For us to maintain our own client we need to test it in detail to check compliance
    to our permissions design. Any changes in the upstream MLFlow client are tested
    at the same time, for instance a restructuring of the MLFlow modules. We start
    a local MLFlow server within a PyTest fixture that is scoped to the whole testing
    session. The Python tempfile module is used to generate a temporary artifact location
    and a temporary sqlite database file. The goal of our test session is to log and
    register a model and perform various mutations and retrievals on it. The first
    step is to log and register a model version in the empty model registry. We perform
    this inside a PyTest fixture as all other tests depend on it, although technically
    it is a test itself as it includes asserts.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了维护我们自己的客户端，我们需要详细测试它，以检查是否符合我们的权限设计。对上游 MLFlow 客户端的任何更改也会同时进行测试，例如 MLFlow
    模块的重构。我们在一个 PyTest 固定装置内启动一个本地 MLFlow 服务器，该装置的作用范围是整个测试会话。使用 Python tempfile 模块生成一个临时工件位置和一个临时
    sqlite 数据库文件。我们测试会话的目标是记录和注册模型，并对其进行各种变更和检索。第一步是在空模型注册表中记录和注册一个模型版本。我们在 PyTest
    固定装置内执行此操作，因为所有其他测试都依赖于它，尽管从技术上讲，它本身就是一个测试，因为它包括断言。
- en: MLFlow test server fixture, thanks to @Wessel Radstok and [Rik Jongerius](https://medium.com/u/8d3b69256f17?source=post_page-----ee9b7310c5b3--------------------------------)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: MLFlow 测试服务器固定装置，感谢 @Wessel Radstok 和 [Rik Jongerius](https://medium.com/u/8d3b69256f17?source=post_page-----ee9b7310c5b3--------------------------------)
- en: test fixture for model registration
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our extended environment MLFlow client allows us to register models from multiple
    logical environments into the same model registry. It leverages the minimal permissions
    options in Databricks to securely separate development environments from production
    during model registration and retrieval for deployment. In addition, the MLFlow
    API is largely unchanged and the same across environments.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '*Originally published at* [*https://codebeez.nl*](https://codebeez.nl/blogs/secure-mlops-with-databricks-mlflow/)*.*'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
