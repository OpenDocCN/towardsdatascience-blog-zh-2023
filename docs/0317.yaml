- en: 'Applied Reinforcement Learning V: Normalized Advantage Function (NAF) for Continuous
    Control'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095?source=collection_archive---------11-----------------------#2023-01-19](https://towardsdatascience.com/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095?source=collection_archive---------11-----------------------#2023-01-19)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Introduction and explanation of the NAF algorithm, widely used in continuous
    control tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@JavierMtz5?source=post_page-----62ad143d3095--------------------------------)[![Javier
    Martínez Ojeda](../Images/5b5df4220fa64c13232c29de9b4177af.png)](https://medium.com/@JavierMtz5?source=post_page-----62ad143d3095--------------------------------)[](https://towardsdatascience.com/?source=post_page-----62ad143d3095--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----62ad143d3095--------------------------------)
    [Javier Martínez Ojeda](https://medium.com/@JavierMtz5?source=post_page-----62ad143d3095--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F74d7213a71a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapplied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095&user=Javier+Mart%C3%ADnez+Ojeda&userId=74d7213a71a8&source=post_page-74d7213a71a8----62ad143d3095---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----62ad143d3095--------------------------------)
    ·9 min read·Jan 19, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F62ad143d3095&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapplied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095&user=Javier+Mart%C3%ADnez+Ojeda&userId=74d7213a71a8&source=-----62ad143d3095---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F62ad143d3095&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapplied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095&source=-----62ad143d3095---------------------bookmark_footer-----------)![](../Images/980806b6e676bcf2f2e28c3dca2000f0.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Sufyan](https://unsplash.com/@blenderdesigner_1688?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: If you want to read this article without a Premium Medium account, you can do
    it from this friend link :)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[https://www.learnml.wiki/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control/](https://www.learnml.wiki/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control/)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Previous articles in this series have introduced and explained two Reinforcement
    Learning algorithms that have been widely used since their inception:[**Q-Learning**](https://medium.com/towards-data-science/applied-reinforcement-learning-i-q-learning-d6086c1f437)
    and [**DQN**](https://medium.com/towards-data-science/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9).
  prefs: []
  type: TYPE_NORMAL
- en: '**Q-Learning** stores the Q-Values in an action-state matrix, such that to
    obtain the action *a* with the largest Q-Value in state *s*, the largest element
    of the Q-Value matrix for row *s* must be found, which makes its application to
    continuous state or action spaces impossible since the Q-Value matrix would be
    infinite.'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, **DQN** partially solves this problem by making use of a
    neural network to obtain the Q-Values associated to a state *s*, such that the
    output of the neural network are the Q-Values for each possible action of the
    agent (the equivalent to a row in the action-state matrix of Q-Learning). This
    algorithm allows training in environments with a continuous state…
  prefs: []
  type: TYPE_NORMAL
