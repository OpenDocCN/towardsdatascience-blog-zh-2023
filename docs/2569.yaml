- en: Optimising Output File Size in Apache Spark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Apache Spark 中优化输出文件大小
- en: 原文：[https://towardsdatascience.com/optimizing-output-file-size-in-apache-spark-5ce28784934c?source=collection_archive---------0-----------------------#2023-08-11](https://towardsdatascience.com/optimizing-output-file-size-in-apache-spark-5ce28784934c?source=collection_archive---------0-----------------------#2023-08-11)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/optimizing-output-file-size-in-apache-spark-5ce28784934c?source=collection_archive---------0-----------------------#2023-08-11](https://towardsdatascience.com/optimizing-output-file-size-in-apache-spark-5ce28784934c?source=collection_archive---------0-----------------------#2023-08-11)
- en: A Comprehensive Guide on Managing Partitions, Repartition, and Coalesce Operations
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于管理分区、重新分区和合并操作的全面指南
- en: '[](https://medium.com/@gianpiero.colonna?source=post_page-----5ce28784934c--------------------------------)[![Gianpi
    Colonna](../Images/253921adb9c6f4a7e65cc5f954164b10.png)](https://medium.com/@gianpiero.colonna?source=post_page-----5ce28784934c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5ce28784934c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5ce28784934c--------------------------------)
    [Gianpi Colonna](https://medium.com/@gianpiero.colonna?source=post_page-----5ce28784934c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@gianpiero.colonna?source=post_page-----5ce28784934c--------------------------------)[![Gianpi
    Colonna](../Images/253921adb9c6f4a7e65cc5f954164b10.png)](https://medium.com/@gianpiero.colonna?source=post_page-----5ce28784934c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5ce28784934c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5ce28784934c--------------------------------)
    [Gianpi Colonna](https://medium.com/@gianpiero.colonna?source=post_page-----5ce28784934c--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5767480ab9f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-output-file-size-in-apache-spark-5ce28784934c&user=Gianpi+Colonna&userId=5767480ab9f9&source=post_page-5767480ab9f9----5ce28784934c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5ce28784934c--------------------------------)
    ·6 min read·Aug 11, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5ce28784934c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-output-file-size-in-apache-spark-5ce28784934c&user=Gianpi+Colonna&userId=5767480ab9f9&source=-----5ce28784934c---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5767480ab9f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-output-file-size-in-apache-spark-5ce28784934c&user=Gianpi+Colonna&userId=5767480ab9f9&source=post_page-5767480ab9f9----5ce28784934c---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5ce28784934c--------------------------------)
    ·6 min read·2023年8月11日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5ce28784934c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-output-file-size-in-apache-spark-5ce28784934c&user=Gianpi+Colonna&userId=5767480ab9f9&source=-----5ce28784934c---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5ce28784934c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-output-file-size-in-apache-spark-5ce28784934c&source=-----5ce28784934c---------------------bookmark_footer-----------)![](../Images/802577c4c3cea67e8213d120ffea945d.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5ce28784934c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-output-file-size-in-apache-spark-5ce28784934c&source=-----5ce28784934c---------------------bookmark_footer-----------)![](../Images/802577c4c3cea67e8213d120ffea945d.png)'
- en: Photo by [zhao chen](https://unsplash.com/@zhaochen1975?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [zhao chen](https://unsplash.com/@zhaochen1975?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Picture yourself at the helm of a large Spark data processing operation. One
    often-mentioned rule of thumb in Spark optimisation discourse is that for the
    best I/O performance and enhanced parallelism, each data file should hover around
    the size of 128Mb, which is the default partition size when reading a file [[1]](https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下你在掌管一个大型 Spark 数据处理操作。在 Spark 优化讨论中，一个经常提到的经验法则是，为了获得最佳的 I/O 性能和增强的并行性，每个数据文件的大小应接近
    128Mb，这也是读取文件时的默认分区大小 [[1]](https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/)。
- en: Imagine your files as vessels navigating the sea of data processing. If the
    vessels are too small, they waste a lot of time docking and setting sail again,
    a metaphor for the execution engine spending extra time opening files, listing
    directories, getting object metadata, setting up data transfer, and reading files.
    Conversely, if your vessels are too large and you don’t use the many docks of
    the port, they have to wait for a single lengthy loading and unloading process,
    a metaphor for the query processing waiting until a single reader has finished
    reading the entire file, which reduces parallelism [fig. 1].
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你的文件像在数据处理海洋中航行的船只。如果船只太小，它们会浪费很多时间停靠和重新起航，这比喻为执行引擎花费额外时间打开文件、列出目录、获取对象元数据、设置数据传输和读取文件。相反，如果你的船只太大且未使用港口的许多码头，它们不得不等待单一的长时间装卸过程，这比喻为查询处理等待直到一个读者完成读取整个文件，这会减少并行性[图1]。
- en: '![](../Images/a1af1a6222430de899b305f06746b67b.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1af1a6222430de899b305f06746b67b.png)'
- en: Fig. 1 — Image by the author
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图1 — 作者提供的图片
- en: To vividly illustrate the significance of file size optimisation, refer to the
    following figure. In this specific example, every table holds 8 GB of data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生动地说明文件大小优化的重要性，请参阅下图。在这个特定的例子中，每个表包含8 GB的数据。
- en: '![](../Images/44c0668315da3fa6112bf60702d1a6da.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44c0668315da3fa6112bf60702d1a6da.png)'
- en: However, navigating this delicate balance is no easy task, especially when dealing
    with large batch jobs. You may feel like you’ve lost control over the number of
    output files. This guide will help you regain it.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，驾驭这种微妙的平衡并非易事，特别是在处理大型批处理作业时。你可能会觉得你失去了对输出文件数量的控制。本指南将帮助你重新获得控制权。
- en: 'The Key to Understanding: Partitions'
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解的关键：分区
- en: '**The number of output files saved to the disk is equal to the number of partitions
    in the Spark executors when the write operation is performed.** However, gauging
    the number of partitions before performing the write operation can be tricky.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**写操作执行时，保存到磁盘的输出文件数量等于Spark执行器中的分区数量。** 然而，在执行写操作之前评估分区数量可能很棘手。'
- en: When reading a table, Spark defaults to read blocks with a maximum size of 128Mb
    (though you can change this with `sql.files.maxPartitionBytes`). Thus, the number
    of partitions relies on the size of the input. Yet in reality, the number of partitions
    will most likely equal the `sql.shuffle.partitions` parameter. This number defaults
    to 200, but for larger workloads, it rarely is enough. Check out [this](https://youtu.be/daXEp4HmS-E?t=1200)
    video to learn how to set the ideal number of shuffle partitions.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在读取表时，Spark 默认读取最大大小为128Mb的块（尽管你可以使用`sql.files.maxPartitionBytes`来更改这一点）。因此，分区的数量依赖于输入的大小。然而，实际上，分区的数量很可能等于`sql.shuffle.partitions`参数。这个数字默认为200，但对于较大的工作负载，这通常是不够的。查看[这个](https://youtu.be/daXEp4HmS-E?t=1200)视频以了解如何设置理想的洗牌分区数量。
- en: The number of partitions in Spark executors equals `sql.shuffle.partitions`
    if there is at least one wide transformation in the ETL. If only narrow transformations
    are applied, the number of partitions would match the number created when reading
    the file.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在ETL过程中至少有一个宽变换，Spark执行器中的分区数量等于`sql.shuffle.partitions`。如果仅应用窄变换，则分区数量将与读取文件时创建的分区数量相匹配。
- en: Setting the number of shuffle partitions gives us high-level control of the
    total partitions only when dealing with non-partitioned tables. Once we enter
    the territory of partitioned tables, changing the `sql.shuffle.partitions` parameter
    won’t easily steer the size of each data file.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 设置洗牌分区的数量仅在处理未分区的表时可以高层次地控制总分区数量。一旦进入分区表的领域，改变`sql.shuffle.partitions`参数不会轻易地调整每个数据文件的大小。
- en: 'The Steering Wheel: Repartition and Coalesce'
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转轮：重新分区与合并
- en: 'We have two main ways to manage the number of partitions at runtime: `repartition()`
    and `coalesce()`. Here''s a quick breakdown:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两种主要的方法来管理运行时的分区数量：`repartition()`和`coalesce()`。以下是简要说明：
- en: '`**Repartition**`: `repartition(partitionCols, n_partitions)` is a lazy transformation
    with two parameters - the number of partitions and the partitioning column(s).
    When performed, Spark shuffles the partitions across the cluster according to
    the partitioning column. However, **once the table is saved, information about
    the repartitioning is lost.** Therefore, this useful piece of information won’t
    be used when reading the file.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**重新分区**`：`repartition(partitionCols, n_partitions)` 是一种延迟转换，具有两个参数——分区数和分区列。当执行时，Spark
    会根据分区列在集群中重新排列分区。然而，**一旦表被保存，关于重新分区的信息会丢失。** 因此，在读取文件时，这个有用的信息将不会被使用。'
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`**Coalesce**`: `coalesce(num_partitions)` is also a lazy transformation, but
    it only takes one argument - the number of partitions. **Importantly, the coalesce
    operation doesn’t shuffle data across the cluster — therefore it’s faster than**
    `**repartition**`**.** Also, coalesce can only reduce the number of partitions,
    it won’t work if trying to increase the number of partitions.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**合并**`：`coalesce(num_partitions)` 也是一种延迟转换，但只需一个参数——分区数。**重要的是，合并操作不会在集群中洗牌数据——因此比**
    `**重新分区**`**更快。** 此外，合并只能减少分区数，如果试图增加分区数则无效。'
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**The primary insight to take away here is that using the coalesce method is
    generally more beneficial.** That’s not to say that repartitioning isn’t useful;
    it certainly is, particularly when we need to adjust the number of partitions
    in a dataframe at runtime.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**主要的见解是，使用合并方法通常更有利。** 这并不是说重新分区没有用；它确实有用，特别是当我们需要在运行时调整数据框的分区数时。'
- en: In my experience with ETL processes, where I deal with multiple tables of varying
    sizes and carry out complex transformations and joins, I’ve found that `sql.shuffle.partitions`
    doesn’t offer the precise control I need. For instance, using the same number
    of shuffle partitions for joining two small tables and two large tables in the
    same ETL would be inefficient — leading to an overabundance of small partitions
    for the small tables or insufficient partitions for the large tables. Repartitioning
    also has the added benefit of helping me sidestep issues with skewed joins and
    skewed data [[2](/the-art-of-joining-in-spark-dcbd33d693c)].
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在我处理多个大小不一的表，并进行复杂转换和连接的 ETL 过程中，我发现 `sql.shuffle.partitions` 并不能提供我所需的精确控制。例如，在相同的
    ETL 中，为两个小表和两个大表使用相同数量的洗牌分区会很低效——导致小表的分区过多或大表的分区不足。重新分区还有助于我避开倾斜连接和倾斜数据的问题 [[2](/the-art-of-joining-in-spark-dcbd33d693c)]。
- en: 'That being said, repartitioning is less suitable prior to writing the table
    to disk, and in most cases, it can be replaced with coalesce. Coalesce takes the
    upper hand over repartition before writing to disk for a couple of reasons:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，重新分区在将表写入磁盘之前较不适用，并且在大多数情况下，可以用合并替代。在写入磁盘之前，合并比重新分区更具优势，原因有几个：
- en: It prevents an unnecessary reshuffling of data across the cluster.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它防止了不必要的数据在集群中的重新洗牌。
- en: It allows data ordering according to a logical heuristic. When using the repartition
    method before writing, data is reshuffled across the cluster, causing a loss in
    its order. On the other hand, using coalesce retains the order as data is gathered
    together rather than being redistributed.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它允许根据逻辑启发式排序数据。在写入之前使用重新分区方法时，数据在集群中重新洗牌，导致顺序丢失。另一方面，使用合并可以保留数据的顺序，因为数据是被收集在一起而不是重新分配。
- en: Let’s see why ordering the data is crucial.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看为什么数据排序至关重要。
- en: 'Order on the Horizon: Importance of Ordering Data'
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 未来的排序：数据排序的重要性
- en: 'We mentioned above how when we apply the `repartition`method, Spark won’t save
    the partitioning information in the metadata of the table. However, when dealing
    with big data, this is a crucial piece of information for two reasons:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们上面提到，当我们应用 `repartition` 方法时，Spark 不会将分区信息保存到表的元数据中。然而，在处理大数据时，这是一条至关重要的信息，原因有二：
- en: It allows scanning through the table much more quickly at query time.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它允许在查询时更快地扫描表。
- en: It allows better compression — if dealing with a compressible format (such as
    parquet, CSV, Json, etc). [This](https://medium.com/datadenys/how-to-improve-clickhouse-table-compression-697ef8f4ccb3)
    is a great article to understand why.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它允许更好的压缩——如果处理的是可压缩格式（如 parquet、CSV、Json 等）。[这篇文章](https://medium.com/datadenys/how-to-improve-clickhouse-table-compression-697ef8f4ccb3)
    是理解原因的好资料。
- en: The key takeaway is to **order the data before saving**. The information will
    be retained in the metadata, and it will be used at query time, making the query
    much faster.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 关键要点是**在保存之前排序数据**。这些信息将保存在元数据中，并在查询时使用，从而使查询速度更快。
- en: Let’s now explore the differences between saving to a non-partitioned table
    and a partitioned table and why saving to a partitioned table requires some extra
    adjustments.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们探讨一下保存到非分区表和分区表之间的区别，以及为什么保存到分区表需要额外的调整。
- en: Managing File Size in Partitioned Tables
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理分区表中的文件大小
- en: When it comes to non-partitioned tables, managing the number of files during
    the save operation is a direct process. Utilising the `coalesce`method before
    saving will accomplish the task, regardless of whether the data is sorted or not.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非分区表，在保存操作过程中管理文件数量是一个直接的过程。无论数据是否排序，使用 `coalesce` 方法在保存之前都能完成任务。
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: However, this method isn’t effective when handling partitioned tables, unless
    the data is arranged prior to coalescing. To grasp why this happens, we need to
    delve into the actions taking place within Spark executors when the data is ordered
    versus when it isn’t [fig.2].
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当处理分区表时，这种方法并不有效，除非在合并之前数据已经被排序。为了理解为什么会这样，我们需要深入探讨在 Spark 执行器中数据排序与未排序时发生的操作
    [fig.2]。
- en: '![](../Images/cbbfacc17e298d16bd96f20d3db05f1d.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cbbfacc17e298d16bd96f20d3db05f1d.png)'
- en: Fig. 2 — Image by the author
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2 — 作者提供的图片
- en: 'Therefore, the standard process to save data to a partition table should be:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，保存数据到分区表的标准流程应该是：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Other Navigational Aids
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他导航工具
- en: Beyond `repartition` and `coalesce`, you might find `maxnumberofrecords` helpful.
    It's a handy method to prevent files from getting too large and can be used alongside
    the methods above.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 `repartition` 和 `coalesce`，你可能会发现 `maxnumberofrecords` 很有用。这是一个防止文件过大的实用方法，并可以与上述方法一起使用。
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Final Thoughts
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终思考
- en: Mastering file size in a Spark job often involves trial and error. It’s easy
    to overlook optimisation in an era where storage space is cheap and processing
    power is just a click away. But as tera and petabytes of data processing become
    the norm, forgetting these simple optimisation techniques can have significant
    costs in monetary, time, and environmental terms.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 作业中掌握文件大小通常涉及试错。在存储空间便宜且处理能力触手可及的时代，容易忽视优化。但随着数据处理量达到 tera 和 petabytes，忘记这些简单的优化技巧可能会在金钱、时间和环境方面产生重大成本。
- en: I hope this article empowers you to make efficient adjustments to your ETL processes.
    Like a seasoned sea captain, may you navigate the waters of Spark with confidence
    and clarity.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这篇文章能帮助你有效调整 ETL 过程。愿你像经验丰富的海船船长一样，自信且清晰地驾驭 Spark 的海洋。
