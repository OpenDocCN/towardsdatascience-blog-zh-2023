- en: 'Multi-Task Architectures: A Comprehensive Guide'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/multi-task-architectures-9bee2e080456?source=collection_archive---------13-----------------------#2023-07-18](https://towardsdatascience.com/multi-task-architectures-9bee2e080456?source=collection_archive---------13-----------------------#2023-07-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Lightweight models for real-time multi-task inference
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@itberrios6?source=post_page-----9bee2e080456--------------------------------)[![Isaac
    Berrios](../Images/e36cdbfc91b71ceb91c6e4191e1e0833.png)](https://medium.com/@itberrios6?source=post_page-----9bee2e080456--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9bee2e080456--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9bee2e080456--------------------------------)
    [Isaac Berrios](https://medium.com/@itberrios6?source=post_page-----9bee2e080456--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffbadc8e8ee44&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-task-architectures-9bee2e080456&user=Isaac+Berrios&userId=fbadc8e8ee44&source=post_page-fbadc8e8ee44----9bee2e080456---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9bee2e080456--------------------------------)
    ·10 min read·Jul 18, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9bee2e080456&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-task-architectures-9bee2e080456&user=Isaac+Berrios&userId=fbadc8e8ee44&source=-----9bee2e080456---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9bee2e080456&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-task-architectures-9bee2e080456&source=-----9bee2e080456---------------------bookmark_footer-----------)![](../Images/27378ce18d84699cea51c1e7d3601715.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Julien Duduoglu](https://unsplash.com/@julien_duduoglu?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Have you ever wondered how to train a deep neural network to do many things?
    Such a model is referred to as a Multi-Task Architecture and can have benefits
    over a traditional approach that uses individual models for each task. A Multi-Task
    Architecture is a subset of [Multi-Task Learning](https://medium.com/ai-mind-labs/introduction-to-multi-task-learning-5da02afabb61)
    which is a general approach to training a model or set of models to perform multiple
    tasks simultaneously.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'In this post we will learn how to train a single model to perform both classification
    and regression tasks simultaneously. Code for this post can be found on [GitHub](https://github.com/itberrios/CV_projects/tree/main/multitask_depth_seg).
    Here’s an overview:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '[**Motivation**](#47bc) **— Why would we do this?**'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Approach**](#3ba3) **— How are we going to do this?**'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Model Architecture**](#e55e)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Training Approach**](#46fc)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Inference**](#4c64) **— Check performance and learn from an** [***interesting
    failure***](#a6dd)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Conclusion**](#cfa3)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Motivation
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Why would we want to use a light weight model? Won’t that decrease performance?
    If we are not deploying to the edge shouldn’t we use as big of a model as possible?*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Edge applications need light weight models to perform real-time inference with
    low power consumption. Other applications can benefit from them as well, but how?
    An overlooked benefit of lightweight models is their lower compute requirement.
    In general this can lower server usage and therefore decrease power consumption.
    This has the overall effect of *reducing costs* and *lowering carbon emissions*,
    the later of which could become a major [issue](https://www.bloomberg.com/news/articles/2023-03-09/how-much-energy-do-ai-and-chatgpt-use-no-one-knows-for-sure)
    in the future of AI.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Lightweight models can help reduce costs and lower carbon emissions via less
    power consumption
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: With all this being said, a multi-task architecture is a just a tool in the
    toolbox, and all project requirements should be considered before deciding which
    tools to use. Now let’s dive into an example of how train one of these!
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Approach
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To build our Multi-Task Architecture, we will loosely cover the approach from
    this [paper](https://arxiv.org/pdf/1809.04766.pdf), where a single model was trained
    for simultaneous segmentation and depth estimation. The underlying goal was to
    perform these tasks in a fast and efficient manner with a trade-off being an acceptable
    loss of performance. In Multi-Task Learning, we typically group similar tasks
    together. During training, we can also add an auxiliary task that may assist our
    model’s learning, but we may decide not to use it during inference [[1](https://arxiv.org/pdf/1809.04766.pdf),
    [2](https://arxiv.org/pdf/1905.07553.pdf)]. For simplicity, we will not use any
    auxiliary tasks during training.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Depth and Segmentation are both dense prediction tasks, and have similarities.
    For example, the depth of a single object will likely be consistent across all
    areas of the object, forming a very narrow distribution. The main idea is that
    each individual object should a have it’s own depth value, and we should be able
    to recognize individual objects just by looking at a depth map. In the same manner,
    we should be able to recognize the same individual objects by looking at a segmentation
    map. While there are likely to be some outliers, we will assume that this relationship
    holds.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Dataset
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use the [City Scapes dataset](https://www.cityscapes-dataset.com/) to
    provide (left camera) input images segmentation masks, and depth maps. For the
    segmentation maps, we choose to use the standard training labels, with 19 classes
    + 1 unlabeled category.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用[City Scapes数据集](https://www.cityscapes-dataset.com/)提供（左侧摄像头）输入图像分割掩码和深度图。对于分割图，我们选择使用标准训练标签，共19类+
    1类未标记类别。
- en: Depth Map Preparation — default disparity
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度图准备—默认视差
- en: 'Disparity maps created with [SteroSGBM](https://core.ac.uk/download/pdf/11134866.pdf)
    are readily available from the CityScapes website. The Disparity describes the
    pixel difference between objects as viewed from each stereo camera’s perspective,
    and it is inversely proportional to the depth which can be computed with:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[SteroSGBM](https://core.ac.uk/download/pdf/11134866.pdf)创建的视差图可以从CityScapes网站上轻松获取。视差描述了从每个立体摄像头的角度观察物体的像素差异，它与深度成反比，可以通过以下公式计算：
- en: '![](../Images/8145aa8170b00192bab9898d51a62b30.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8145aa8170b00192bab9898d51a62b30.png)'
- en: 'City Scapes depth computation, units are in parenthesis. Source: Author.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: City Scapes深度计算，单位见括号。来源：作者。
- en: 'However, the default disparity maps are noisy with many holes corresponding
    to infinite depth and a portion where the ego vehicle is always shown. A common
    approach to cleaning these disparity maps involves:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，默认的视差图存在噪声，许多孔对应于无限深度，以及一个总是显示自车的区域。清理这些视差图的常见方法包括：
- en: Crop out the bottom 20% along with parts of the left and top edges
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 裁剪底部20%以及左边和顶部边缘的部分
- en: Resize to original scale
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整到原始比例
- en: Apply a smoothing filter
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用平滑滤波器
- en: Perform [inpainting](https://docs.opencv.org/3.4/df/d3d/tutorial_py_inpainting.html)
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行[inpainting](https://docs.opencv.org/3.4/df/d3d/tutorial_py_inpainting.html)
- en: 'Once we clean the disparity we can compute the depth, which results in:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们清理了视差图，就可以计算深度，结果如下：
- en: '![](../Images/b2e6a6278f6c2f5e928fa7619e1ad651.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2e6a6278f6c2f5e928fa7619e1ad651.png)'
- en: Figure 1\. Depth data from City Scapes. Source Author.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 来自City Scapes的深度数据。来源：作者。
- en: The fine details of this approach are outside the scope of this post, but if
    your interested here’s a video explanation on [YouTube](https://www.youtube.com/watch?v=jlZZu1t39Zs&feature=youtu.be).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的详细信息超出了本文的范围，但如果你感兴趣，这里有一个[YouTube](https://www.youtube.com/watch?v=jlZZu1t39Zs&feature=youtu.be)视频解释。
- en: The crop and resize step means that the disparity (as well as the depth) map
    won’t exactly align with the input image. Even though we could do the same crop
    and resize with the input image to correct for this, we opted to explore a new
    approach.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 裁剪和调整步骤意味着视差（以及深度）图将不会完全与输入图像对齐。虽然我们可以对输入图像进行相同的裁剪和调整以纠正此问题，但我们选择探索一种新方法。
- en: Depth Map Preparation — CreStereo disparity
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度图准备—CreStereo视差
- en: We explored using [CreStereo](https://arxiv.org/pdf/2203.11483.pdf) to produce
    high quality disparity maps from both the left and right images. CreStereo is
    an advanced model that is able to predict smooth disparity maps from stereo image
    pairs. This approach introduces a paradigm known as [knowledge distillation](https://en.wikipedia.org/wiki/Knowledge_distillation),
    where CreStereo is a teacher network and our model will be the student network
    (at least for the depth estimation). This details of this approach are outside
    the scope of this post, but here’s a [YouTube](https://www.youtube.com/watch?v=e3Zuc8AbwoA)
    link if you’re interested.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探索了使用[CreStereo](https://arxiv.org/pdf/2203.11483.pdf)从左右图像生成高质量视差图。CreStereo是一个先进的模型，能够从立体图像对中预测平滑的视差图。这种方法引入了一个称为[知识蒸馏](https://en.wikipedia.org/wiki/Knowledge_distillation)的范式，其中CreStereo是教师网络，我们的模型将是学生网络（至少在深度估计方面）。这种方法的细节超出了本文的范围，但如果你感兴趣，这里有一个[YouTube](https://www.youtube.com/watch?v=e3Zuc8AbwoA)链接。
- en: 'In general, the CreStereo depth maps have minimal noise so there’s no need
    to crop and resize. However, the ego vehicle present in the segmentation masks
    could cause issues with generalization so the bottom 20% was removed on all training
    images. A training sample is shown below:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，CreStereo深度图的噪声最小，因此不需要裁剪和调整。然而，分割掩码中存在的自车可能会导致泛化问题，因此我们在所有训练图像中去除了底部20%。训练样本如下所示：
- en: '![](../Images/bfb06ddc24959a047a970a383d948b34.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bfb06ddc24959a047a970a383d948b34.png)'
- en: Figure 2\. Training Sample. Source Author.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. 训练样本。来源：作者。
- en: Now that we have our data, let’s see the architecture.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了数据，让我们来看看架构。
- en: Model Architecture
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型架构
- en: Following [[1](https://arxiv.org/pdf/1809.04766.pdf)], the architecture will
    consist of a MobileNet backbone/encoder, a [LightWeight RefineNet](https://arxiv.org/pdf/1810.03272.pdf)
    Decoder, and Heads for each individual task. The overall architecture is shown
    below in figure 3.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8a495fd1f3517f2a2e13c128f21792e2.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. Model Architecture. [Source](https://arxiv.org/pdf/1809.04766.pdf).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: For the encoder/backbone, we will use a [MobileNetV3](https://arxiv.org/pdf/1905.02244.pdf)
    and pass skip connections at 1/4, 1/8, 1/16, and 1/32 resolutions to the Light
    Weight Refine Net. Finally, the output is passed to each head that is responsible
    for a different task. Notice how we can even add more tasks to this architecture
    if we wanted to.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ebee8bae8088ebffd1530f3cb04ea4f8.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. (Left) Detailed Encoder-Decoder Multi-Task Architecture. (Right)
    details of the LightWeight RefineNet Blocks. Modified from [Source](https://arxiv.org/pdf/1810.03272.pdf).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: To implement the encoder, we use a pre-trained MobileNetV3 encoder, where we
    will pass in the MobileNetV3 encoder to a custom PyTorch Module. The output of
    it’s forward function is a [ParameterDict](https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html)
    of skip connections for input to the LightWeight Refine Net. The code snippet
    below shows how to do this.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The LightWeight RefineNet Decoder is very similar to the one [implemented](https://github.com/DrSleep/multi-task-refinenet/blob/master/src/models.py)
    in [[1](https://arxiv.org/pdf/1809.04766.pdf)], except with a few modifications
    to make it compatible with [MobileNetV3](https://arxiv.org/pdf/1905.02244.pdf)
    as opposed to MobileNetV2\. We also note that the decoder portion consists of
    the Segmentation and Depth heads. The full code for the model is available on
    [GitHub](https://github.com/itberrios/CV_projects/blob/main/multitask_depth_seg/model.py).
    We can piece together the model as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Training Approach
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We divide training into three phases, the first at 1/4 resolution, the second
    at 1/2 resolution, and the final at full resolution. All of the weights were updated,
    since freezing the encoder weights didn’t seem to produce good results.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Transformations
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During each phase, we perform random crop resize, color jitter, random flips,
    and normalization. The left input image is normalized with standard image net
    mean and standard deviation.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Depth Transformation
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In general the depth maps contains mostly smaller values, since most of the
    information contained in a depth map consists of objects and surfaces close to
    the camera. Since the depth map has most of it’s depth concentrated around lower
    values (see left of figure 4 below), it will need to be transformed to be effectively
    learned by a neural network. The depth map is clipped between 0 and 250, this
    is because stereo disparity/depth data at large distances is typically unreliable
    and in this case we want a way to *discard* it. Then we take the natural log and
    divide it by 5 to condense the distribution around a smaller range of numbers.
    See this [notebook](https://github.com/itberrios/CV_projects/blob/main/multitask_depth_seg/depth_normalization.ipynb)
    for more details.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9f0ab68fac873956f7adfcbb1401ed3d.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. Left — Clipped Depth Distribution. Right — Transformed Depth Distribution.
    Depth was sampled form 64 random full size training depth masks. Source Author.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: I’ll be honest, I wasn’t sure of the best way to transform the depth data. If
    there’s a better way or if you would do it differently I would interested to learn
    more in the comments :).
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Loss Functions
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We keep the loss functions simple, Cross Entropy Loss for segmentation and Mean
    Squared Error for Depth Estimation. We add them together with no weighting and
    optimize them jointly.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Learning Rate
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We use a One Cycle Cosine Annealed Learning Rate with a max at 5e-4 and train
    for 150 epochs at 1/4 resolution. The notebook used for training is located [here](https://github.com/itberrios/CV_projects/blob/main/multitask_depth_seg/training.ipynb).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ba43475521b4feca32a2bd1c86a42cf1.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: Figure 5\. One Cycle Cosine Annealed Learning Rate. Source Author.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: We fine then tune at 1/2 resolution for 25 epochs and again at full resolution
    for another 25 epochs both with a learning rate of 5e-6\. Note that we needed
    to reduce the batch size each time we fine tuned at an increased resolution.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Inference
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For inference we normalized the input image and ran a forward pass through the
    model. Figure 6 shows training results from both validation and test data
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6c98035d61d2cffd1a40047375e98269.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
- en: Figure 6\. Inference results (top 2 are from test set and bottom 2 are from
    validation set). Source Author.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: In general it seems like the model is able to segment and estimate depth when
    there are larger objects in an image. When more finely detailed objects such as
    pedestrians are present, the model tends to struggle to segment them entirely.
    The model is able to estimate their depth to some degree of accuracy.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: An Interesting Failure
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The bottom of figure 6 shows an interesting failure case to fully segment the
    light pole in the left side of the image. The segmentation only covers the bottom
    half of the light pole, while the depth shows that the bottom half of the light
    pole is much closer than the top half. The depth failure, could be due the bias
    of bottom pixels generally corresponding to closer depth; notice the horizon line
    around pixel 500, there is a clear divide between closer pixels and further way
    pixels. It seems like this bias could have leaked into the model’s segmentation
    task. This type of *task leakage* should be considered when training multi-task
    models.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: In Multi-task Learning, training data from one task can impact performance on
    another task
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Depth Distributions
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s check how the predicted depth is distributed compared to the truth. For
    simplicity, we will just use a sample of 94 true/predicted full resolution depth
    map pairs.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/95fd89fb4612f50c13cbbf6fbfd907be.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
- en: Figure 8\. True (left) and predicted (right) depth map distributions, each with
    1000 bins. Source Author.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: It seems like the model has learned two distributions, a distribution with a
    peak around 4 and distribution with a peak around 30\. Notice that the clipping
    artifact did not seem to make a difference. The overall distribution contains
    a long tail which is characteristic of the fact that only a small portion of an
    image will contain far away depth data.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: The predicted depth distribution is much more smooth than the ground truth.
    The roughness of the ground truth distribution could come from the fact that each
    object contains similar depth values. It may be possible to use this information
    to apply some sort of regularization to force the model to follow this paradigm,
    but that will be for another time.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'Bonus: Inference Speed'
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since this is a lightweight model intended for speed, let’s see how fast it
    will inference on GPU. The code below has been modified from this [article](https://deci.ai/blog/measure-inference-time-deep-neural-networks/).
    In this test, the input image has been scaled down to 400x1024.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The inference test shows that this model can run at 18.69+/-0.44 ms or about
    55Hz. It’s important to note that this is just a Python prototype ran on a laptop
    with a NVIDIA RTX 3060 GPU, different hardware will change inference speed. We
    should also note, an SDK like [Torch-TensorRt](https://developer.nvidia.com/blog/accelerating-inference-up-to-6x-faster-in-pytorch-with-torch-tensorrt/)
    could provide significant speed up if deployed on a NVIDIA GPU.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this post we learned how Multi-Task Learning can save costs and reduce carbon
    emissions. We learned how to build a lightweight multi-task architecture capable
    of performing classification and regression simultaneously on the CityScapes data
    set. We also leveraged CreStereo and Knowledge Distillation to help our model
    learn to predict better depth maps.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: This lightweight model presents a trade-off where we sacrifice some performance
    for speed and efficiency. Even with this tradeoff, the trained model was able
    to predict reasonable depth and segmentation results on test data. Furthermore,
    it was able learn to predict a similar depth distribution to the ground truth
    depth maps.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这个轻量级模型在速度和效率之间做出了权衡。即使有这个权衡，训练后的模型仍然能够在测试数据上预测出合理的深度和分割结果。此外，它还能够学习预测与真实深度图类似的深度分布。
- en: References
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Nekrasov, Vladimir, et al. ‘Real-Time Joint Semantic Segmentation and Depth
    Estimation Using Asymmetric Annotations’. *CoRR*, vol. abs/1809.04766, 2018, [http://arxiv.org/abs/1809.04766](http://arxiv.org/abs/1809.04766.)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Nekrasov, Vladimir 等人. ‘实时联合语义分割和深度估计使用不对称注释’. *CoRR*, vol. abs/1809.04766,
    2018, [http://arxiv.org/abs/1809.04766](http://arxiv.org/abs/1809.04766.)'
- en: '[2] Standley, Trevor, et al. ‘Which Tasks Should Be Learned Together in Multi-Task
    Learning?’ *CoRR*, vol. abs/1905.07553, 2019, [http://arxiv.org/abs/1905.07553](http://arxiv.org/abs/1905.07553.)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Standley, Trevor 等人. ‘在多任务学习中应该一起学习哪些任务？’ *CoRR*, vol. abs/1905.07553,
    2019, [http://arxiv.org/abs/1905.07553](http://arxiv.org/abs/1905.07553.)'
- en: '[3] Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson,
    R., Franke, U., Roth, S., & Schiele, B. (2016). The cityscapes dataset for Semantic
    Urban Scene understanding. *2016 IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR)*. [https://doi.org/10.1109/cvpr.2016.350](https://doi.org/10.1109/cvpr.2016.350)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson,
    R., Franke, U., Roth, S., & Schiele, B. (2016). 城市景观数据集用于语义城市场景理解. *2016 IEEE
    计算机视觉与模式识别会议 (CVPR)*. [https://doi.org/10.1109/cvpr.2016.350](https://doi.org/10.1109/cvpr.2016.350)'
