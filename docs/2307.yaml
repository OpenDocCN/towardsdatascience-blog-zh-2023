- en: 'Multi-Task Architectures: A Comprehensive Guide'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多任务架构：综合指南
- en: 原文：[https://towardsdatascience.com/multi-task-architectures-9bee2e080456?source=collection_archive---------13-----------------------#2023-07-18](https://towardsdatascience.com/multi-task-architectures-9bee2e080456?source=collection_archive---------13-----------------------#2023-07-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/multi-task-architectures-9bee2e080456?source=collection_archive---------13-----------------------#2023-07-18](https://towardsdatascience.com/multi-task-architectures-9bee2e080456?source=collection_archive---------13-----------------------#2023-07-18)
- en: Lightweight models for real-time multi-task inference
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实时多任务推理的轻量级模型
- en: '[](https://medium.com/@itberrios6?source=post_page-----9bee2e080456--------------------------------)[![Isaac
    Berrios](../Images/e36cdbfc91b71ceb91c6e4191e1e0833.png)](https://medium.com/@itberrios6?source=post_page-----9bee2e080456--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9bee2e080456--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9bee2e080456--------------------------------)
    [Isaac Berrios](https://medium.com/@itberrios6?source=post_page-----9bee2e080456--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@itberrios6?source=post_page-----9bee2e080456--------------------------------)[![Isaac
    Berrios](../Images/e36cdbfc91b71ceb91c6e4191e1e0833.png)](https://medium.com/@itberrios6?source=post_page-----9bee2e080456--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9bee2e080456--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9bee2e080456--------------------------------)
    [Isaac Berrios](https://medium.com/@itberrios6?source=post_page-----9bee2e080456--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffbadc8e8ee44&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-task-architectures-9bee2e080456&user=Isaac+Berrios&userId=fbadc8e8ee44&source=post_page-fbadc8e8ee44----9bee2e080456---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9bee2e080456--------------------------------)
    ·10 min read·Jul 18, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9bee2e080456&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-task-architectures-9bee2e080456&user=Isaac+Berrios&userId=fbadc8e8ee44&source=-----9bee2e080456---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffbadc8e8ee44&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-task-architectures-9bee2e080456&user=Isaac+Berrios&userId=fbadc8e8ee44&source=post_page-fbadc8e8ee44----9bee2e080456---------------------post_header-----------)
    发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9bee2e080456--------------------------------)
    ·10分钟阅读·2023年7月18日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9bee2e080456&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-task-architectures-9bee2e080456&user=Isaac+Berrios&userId=fbadc8e8ee44&source=-----9bee2e080456---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9bee2e080456&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-task-architectures-9bee2e080456&source=-----9bee2e080456---------------------bookmark_footer-----------)![](../Images/27378ce18d84699cea51c1e7d3601715.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9bee2e080456&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-task-architectures-9bee2e080456&source=-----9bee2e080456---------------------bookmark_footer-----------)![](../Images/27378ce18d84699cea51c1e7d3601715.png)'
- en: Photo by [Julien Duduoglu](https://unsplash.com/@julien_duduoglu?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Julien Duduoglu](https://unsplash.com/@julien_duduoglu?utm_source=medium&utm_medium=referral)在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)提供
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Have you ever wondered how to train a deep neural network to do many things?
    Such a model is referred to as a Multi-Task Architecture and can have benefits
    over a traditional approach that uses individual models for each task. A Multi-Task
    Architecture is a subset of [Multi-Task Learning](https://medium.com/ai-mind-labs/introduction-to-multi-task-learning-5da02afabb61)
    which is a general approach to training a model or set of models to perform multiple
    tasks simultaneously.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否曾经想过如何训练一个深度神经网络来完成多项任务？这样的模型被称为多任务架构，相较于使用单独模型来处理每个任务的传统方法，它具有一定的优势。多任务架构是[多任务学习](https://medium.com/ai-mind-labs/introduction-to-multi-task-learning-5da02afabb61)的一个子集，这是一种训练模型或模型集合以同时执行多个任务的通用方法。
- en: 'In this post we will learn how to train a single model to perform both classification
    and regression tasks simultaneously. Code for this post can be found on [GitHub](https://github.com/itberrios/CV_projects/tree/main/multitask_depth_seg).
    Here’s an overview:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将学习如何训练一个模型同时执行分类和回归任务。本文的代码可以在[GitHub](https://github.com/itberrios/CV_projects/tree/main/multitask_depth_seg)找到。以下是概述：
- en: '[**Motivation**](#47bc) **— Why would we do this?**'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**动机**](#47bc) **— 我们为什么要这样做？**'
- en: '[**Approach**](#3ba3) **— How are we going to do this?**'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**方法**](#3ba3) **— 我们将如何实现这一目标？**'
- en: '[**Model Architecture**](#e55e)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**模型架构**](#e55e)'
- en: '[**Training Approach**](#46fc)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**训练方法**](#46fc)'
- en: '[**Inference**](#4c64) **— Check performance and learn from an** [***interesting
    failure***](#a6dd)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**推理**](#4c64) **— 检查性能并从一个** [***有趣的失败***](#a6dd) **中学习**'
- en: '[**Conclusion**](#cfa3)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**结论**](#cfa3)'
- en: Motivation
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动机
- en: '*Why would we want to use a light weight model? Won’t that decrease performance?
    If we are not deploying to the edge shouldn’t we use as big of a model as possible?*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们为什么要使用轻量级模型？这不会降低性能吗？如果我们不部署到边缘，难道不应该使用尽可能大的模型吗？*'
- en: Edge applications need light weight models to perform real-time inference with
    low power consumption. Other applications can benefit from them as well, but how?
    An overlooked benefit of lightweight models is their lower compute requirement.
    In general this can lower server usage and therefore decrease power consumption.
    This has the overall effect of *reducing costs* and *lowering carbon emissions*,
    the later of which could become a major [issue](https://www.bloomberg.com/news/articles/2023-03-09/how-much-energy-do-ai-and-chatgpt-use-no-one-knows-for-sure)
    in the future of AI.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘应用需要轻量级模型以进行实时推理并减少功耗。其他应用也可以从中受益，但具体如何受益呢？轻量级模型的一个被忽视的好处是它们对计算的要求较低。通常，这可以降低服务器使用，从而减少功耗。这总体上具有*降低成本*和*减少碳排放*的效果，后者可能在未来的AI中成为一个主要的[问题](https://www.bloomberg.com/news/articles/2023-03-09/how-much-energy-do-ai-and-chatgpt-use-no-one-knows-for-sure)。
- en: Lightweight models can help reduce costs and lower carbon emissions via less
    power consumption
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 轻量级模型有助于降低成本和减少碳排放，因为它们消耗更少的电力
- en: With all this being said, a multi-task architecture is a just a tool in the
    toolbox, and all project requirements should be considered before deciding which
    tools to use. Now let’s dive into an example of how train one of these!
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，多任务架构只是工具箱中的一个工具，所有项目需求应在决定使用哪些工具之前加以考虑。现在让我们深入探讨一下如何训练其中一个模型吧！
- en: Approach
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法
- en: To build our Multi-Task Architecture, we will loosely cover the approach from
    this [paper](https://arxiv.org/pdf/1809.04766.pdf), where a single model was trained
    for simultaneous segmentation and depth estimation. The underlying goal was to
    perform these tasks in a fast and efficient manner with a trade-off being an acceptable
    loss of performance. In Multi-Task Learning, we typically group similar tasks
    together. During training, we can also add an auxiliary task that may assist our
    model’s learning, but we may decide not to use it during inference [[1](https://arxiv.org/pdf/1809.04766.pdf),
    [2](https://arxiv.org/pdf/1905.07553.pdf)]. For simplicity, we will not use any
    auxiliary tasks during training.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建我们的多任务架构，我们将大致涵盖这篇[论文](https://arxiv.org/pdf/1809.04766.pdf)中的方法，该论文中训练了一个模型以同时进行分割和深度估计。其基本目标是以快速高效的方式完成这些任务，同时可以接受性能上的损失。在多任务学习中，我们通常将相似的任务组合在一起。在训练过程中，我们还可以添加一个辅助任务，这可能有助于模型的学习，但我们可能会选择在推理过程中不使用它[[1](https://arxiv.org/pdf/1809.04766.pdf),
    [2](https://arxiv.org/pdf/1905.07553.pdf)]。为简化起见，我们在训练过程中不会使用任何辅助任务。
- en: Depth and Segmentation are both dense prediction tasks, and have similarities.
    For example, the depth of a single object will likely be consistent across all
    areas of the object, forming a very narrow distribution. The main idea is that
    each individual object should a have it’s own depth value, and we should be able
    to recognize individual objects just by looking at a depth map. In the same manner,
    we should be able to recognize the same individual objects by looking at a segmentation
    map. While there are likely to be some outliers, we will assume that this relationship
    holds.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 深度和分割都是密集预测任务，并且有相似之处。例如，单个物体的深度在物体的所有区域内可能是一致的，形成一个非常狭窄的分布。主要的想法是每个物体应该有其自己的深度值，我们应该能够仅通过查看深度图来识别单个物体。以同样的方式，我们应该能够通过查看分割图来识别相同的单个物体。虽然可能会有一些异常值，但我们将假设这种关系是成立的。
- en: Dataset
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: We will use the [City Scapes dataset](https://www.cityscapes-dataset.com/) to
    provide (left camera) input images segmentation masks, and depth maps. For the
    segmentation maps, we choose to use the standard training labels, with 19 classes
    + 1 unlabeled category.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用[City Scapes数据集](https://www.cityscapes-dataset.com/)提供（左侧摄像头）输入图像分割掩码和深度图。对于分割图，我们选择使用标准训练标签，共19类+
    1类未标记类别。
- en: Depth Map Preparation — default disparity
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度图准备—默认视差
- en: 'Disparity maps created with [SteroSGBM](https://core.ac.uk/download/pdf/11134866.pdf)
    are readily available from the CityScapes website. The Disparity describes the
    pixel difference between objects as viewed from each stereo camera’s perspective,
    and it is inversely proportional to the depth which can be computed with:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[SteroSGBM](https://core.ac.uk/download/pdf/11134866.pdf)创建的视差图可以从CityScapes网站上轻松获取。视差描述了从每个立体摄像头的角度观察物体的像素差异，它与深度成反比，可以通过以下公式计算：
- en: '![](../Images/8145aa8170b00192bab9898d51a62b30.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8145aa8170b00192bab9898d51a62b30.png)'
- en: 'City Scapes depth computation, units are in parenthesis. Source: Author.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: City Scapes深度计算，单位见括号。来源：作者。
- en: 'However, the default disparity maps are noisy with many holes corresponding
    to infinite depth and a portion where the ego vehicle is always shown. A common
    approach to cleaning these disparity maps involves:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，默认的视差图存在噪声，许多孔对应于无限深度，以及一个总是显示自车的区域。清理这些视差图的常见方法包括：
- en: Crop out the bottom 20% along with parts of the left and top edges
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 裁剪底部20%以及左边和顶部边缘的部分
- en: Resize to original scale
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整到原始比例
- en: Apply a smoothing filter
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用平滑滤波器
- en: Perform [inpainting](https://docs.opencv.org/3.4/df/d3d/tutorial_py_inpainting.html)
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行[inpainting](https://docs.opencv.org/3.4/df/d3d/tutorial_py_inpainting.html)
- en: 'Once we clean the disparity we can compute the depth, which results in:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们清理了视差图，就可以计算深度，结果如下：
- en: '![](../Images/b2e6a6278f6c2f5e928fa7619e1ad651.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2e6a6278f6c2f5e928fa7619e1ad651.png)'
- en: Figure 1\. Depth data from City Scapes. Source Author.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 来自City Scapes的深度数据。来源：作者。
- en: The fine details of this approach are outside the scope of this post, but if
    your interested here’s a video explanation on [YouTube](https://www.youtube.com/watch?v=jlZZu1t39Zs&feature=youtu.be).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的详细信息超出了本文的范围，但如果你感兴趣，这里有一个[YouTube](https://www.youtube.com/watch?v=jlZZu1t39Zs&feature=youtu.be)视频解释。
- en: The crop and resize step means that the disparity (as well as the depth) map
    won’t exactly align with the input image. Even though we could do the same crop
    and resize with the input image to correct for this, we opted to explore a new
    approach.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 裁剪和调整步骤意味着视差（以及深度）图将不会完全与输入图像对齐。虽然我们可以对输入图像进行相同的裁剪和调整以纠正此问题，但我们选择探索一种新方法。
- en: Depth Map Preparation — CreStereo disparity
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度图准备—CreStereo视差
- en: We explored using [CreStereo](https://arxiv.org/pdf/2203.11483.pdf) to produce
    high quality disparity maps from both the left and right images. CreStereo is
    an advanced model that is able to predict smooth disparity maps from stereo image
    pairs. This approach introduces a paradigm known as [knowledge distillation](https://en.wikipedia.org/wiki/Knowledge_distillation),
    where CreStereo is a teacher network and our model will be the student network
    (at least for the depth estimation). This details of this approach are outside
    the scope of this post, but here’s a [YouTube](https://www.youtube.com/watch?v=e3Zuc8AbwoA)
    link if you’re interested.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探索了使用[CreStereo](https://arxiv.org/pdf/2203.11483.pdf)从左右图像生成高质量视差图。CreStereo是一个先进的模型，能够从立体图像对中预测平滑的视差图。这种方法引入了一个称为[知识蒸馏](https://en.wikipedia.org/wiki/Knowledge_distillation)的范式，其中CreStereo是教师网络，我们的模型将是学生网络（至少在深度估计方面）。这种方法的细节超出了本文的范围，但如果你感兴趣，这里有一个[YouTube](https://www.youtube.com/watch?v=e3Zuc8AbwoA)链接。
- en: 'In general, the CreStereo depth maps have minimal noise so there’s no need
    to crop and resize. However, the ego vehicle present in the segmentation masks
    could cause issues with generalization so the bottom 20% was removed on all training
    images. A training sample is shown below:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，CreStereo深度图的噪声最小，因此不需要裁剪和调整。然而，分割掩码中存在的自车可能会导致泛化问题，因此我们在所有训练图像中去除了底部20%。训练样本如下所示：
- en: '![](../Images/bfb06ddc24959a047a970a383d948b34.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bfb06ddc24959a047a970a383d948b34.png)'
- en: Figure 2\. Training Sample. Source Author.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. 训练样本。来源：作者。
- en: Now that we have our data, let’s see the architecture.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了数据，让我们来看看架构。
- en: Model Architecture
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型架构
- en: Following [[1](https://arxiv.org/pdf/1809.04766.pdf)], the architecture will
    consist of a MobileNet backbone/encoder, a [LightWeight RefineNet](https://arxiv.org/pdf/1810.03272.pdf)
    Decoder, and Heads for each individual task. The overall architecture is shown
    below in figure 3.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[[1](https://arxiv.org/pdf/1809.04766.pdf)]，架构将包括一个 MobileNet 主干/编码器，一个[LightWeight
    RefineNet](https://arxiv.org/pdf/1810.03272.pdf) 解码器，以及用于每个单独任务的头部。整体架构如图3所示。
- en: '![](../Images/8a495fd1f3517f2a2e13c128f21792e2.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a495fd1f3517f2a2e13c128f21792e2.png)'
- en: Figure 3\. Model Architecture. [Source](https://arxiv.org/pdf/1809.04766.pdf).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. 模型架构。[源](https://arxiv.org/pdf/1809.04766.pdf)。
- en: For the encoder/backbone, we will use a [MobileNetV3](https://arxiv.org/pdf/1905.02244.pdf)
    and pass skip connections at 1/4, 1/8, 1/16, and 1/32 resolutions to the Light
    Weight Refine Net. Finally, the output is passed to each head that is responsible
    for a different task. Notice how we can even add more tasks to this architecture
    if we wanted to.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于编码器/主干，我们将使用一个[MobileNetV3](https://arxiv.org/pdf/1905.02244.pdf)并将1/4、1/8、1/16和1/32分辨率的跳跃连接传递到
    Light Weight Refine Net。最后，输出将传递到每个负责不同任务的头部。请注意，如果需要，我们甚至可以向该架构中添加更多任务。
- en: '![](../Images/ebee8bae8088ebffd1530f3cb04ea4f8.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ebee8bae8088ebffd1530f3cb04ea4f8.png)'
- en: Figure 4\. (Left) Detailed Encoder-Decoder Multi-Task Architecture. (Right)
    details of the LightWeight RefineNet Blocks. Modified from [Source](https://arxiv.org/pdf/1810.03272.pdf).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\.（左）详细的编码器-解码器多任务架构。（右）LightWeight RefineNet 块的详细信息。修改自[源](https://arxiv.org/pdf/1810.03272.pdf)。
- en: To implement the encoder, we use a pre-trained MobileNetV3 encoder, where we
    will pass in the MobileNetV3 encoder to a custom PyTorch Module. The output of
    it’s forward function is a [ParameterDict](https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html)
    of skip connections for input to the LightWeight Refine Net. The code snippet
    below shows how to do this.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现编码器，我们使用预训练的 MobileNetV3 编码器，将 MobileNetV3 编码器传递给自定义 PyTorch 模块。其前向函数的输出是一个[ParameterDict](https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html)的跳跃连接，用于输入到
    LightWeight Refine Net。下面的代码片段展示了如何实现这一点。
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The LightWeight RefineNet Decoder is very similar to the one [implemented](https://github.com/DrSleep/multi-task-refinenet/blob/master/src/models.py)
    in [[1](https://arxiv.org/pdf/1809.04766.pdf)], except with a few modifications
    to make it compatible with [MobileNetV3](https://arxiv.org/pdf/1905.02244.pdf)
    as opposed to MobileNetV2\. We also note that the decoder portion consists of
    the Segmentation and Depth heads. The full code for the model is available on
    [GitHub](https://github.com/itberrios/CV_projects/blob/main/multitask_depth_seg/model.py).
    We can piece together the model as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: LightWeight RefineNet 解码器与[[1](https://arxiv.org/pdf/1809.04766.pdf)]中[实现的](https://github.com/DrSleep/multi-task-refinenet/blob/master/src/models.py)解码器非常相似，只是进行了少许修改以使其与[MobileNetV3](https://arxiv.org/pdf/1905.02244.pdf)兼容，而不是
    MobileNetV2。我们还注意到解码器部分包含了分割和深度头。该模型的完整代码可以在[GitHub](https://github.com/itberrios/CV_projects/blob/main/multitask_depth_seg/model.py)上找到。我们可以按如下方式组装模型：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Training Approach
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练方法
- en: We divide training into three phases, the first at 1/4 resolution, the second
    at 1/2 resolution, and the final at full resolution. All of the weights were updated,
    since freezing the encoder weights didn’t seem to produce good results.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练分为三个阶段，第一阶段为1/4分辨率，第二阶段为1/2分辨率，最后阶段为全分辨率。所有权重都会更新，因为冻结编码器权重似乎效果不佳。
- en: Transformations
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换
- en: During each phase, we perform random crop resize, color jitter, random flips,
    and normalization. The left input image is normalized with standard image net
    mean and standard deviation.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个阶段，我们执行随机裁剪调整大小、颜色抖动、随机翻转和归一化。左侧输入图像使用标准图像网均值和标准差进行归一化。
- en: Depth Transformation
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度转换
- en: In general the depth maps contains mostly smaller values, since most of the
    information contained in a depth map consists of objects and surfaces close to
    the camera. Since the depth map has most of it’s depth concentrated around lower
    values (see left of figure 4 below), it will need to be transformed to be effectively
    learned by a neural network. The depth map is clipped between 0 and 250, this
    is because stereo disparity/depth data at large distances is typically unreliable
    and in this case we want a way to *discard* it. Then we take the natural log and
    divide it by 5 to condense the distribution around a smaller range of numbers.
    See this [notebook](https://github.com/itberrios/CV_projects/blob/main/multitask_depth_seg/depth_normalization.ipynb)
    for more details.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 通常深度图包含大多数较小的值，因为深度图中包含的大部分信息都是相机附近的物体和表面。由于深度图的大部分深度集中在较低的值附近（见下图4的左侧），因此需要对其进行变换，以便神经网络能够有效学习。深度图被裁剪在0到250之间，这是因为大距离的立体视差/深度数据通常不可靠，在这种情况下，我们希望*丢弃*它。然后，我们取自然对数并除以5，以便将分布浓缩到较小范围的数字上。有关更多细节，请参见这个
    [笔记本](https://github.com/itberrios/CV_projects/blob/main/multitask_depth_seg/depth_normalization.ipynb)。
- en: '![](../Images/9f0ab68fac873956f7adfcbb1401ed3d.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f0ab68fac873956f7adfcbb1401ed3d.png)'
- en: Figure 4\. Left — Clipped Depth Distribution. Right — Transformed Depth Distribution.
    Depth was sampled form 64 random full size training depth masks. Source Author.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. 左 — 裁剪后的深度分布。右 — 变换后的深度分布。深度是从64个随机全尺寸训练深度掩模中采样的。来源作者。
- en: I’ll be honest, I wasn’t sure of the best way to transform the depth data. If
    there’s a better way or if you would do it differently I would interested to learn
    more in the comments :).
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 老实说，我不确定最佳的深度数据变换方法。如果有更好的方法或您会以不同的方式进行，我很想在评论中了解更多 :).
- en: Loss Functions
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数
- en: We keep the loss functions simple, Cross Entropy Loss for segmentation and Mean
    Squared Error for Depth Estimation. We add them together with no weighting and
    optimize them jointly.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们保持损失函数简单，分割使用交叉熵损失，深度估计使用均方误差。我们将它们加在一起，不加权，并联合优化。
- en: Learning Rate
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习率
- en: We use a One Cycle Cosine Annealed Learning Rate with a max at 5e-4 and train
    for 150 epochs at 1/4 resolution. The notebook used for training is located [here](https://github.com/itberrios/CV_projects/blob/main/multitask_depth_seg/training.ipynb).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了一个周期余弦退火学习率，最大值为5e-4，并在1/4分辨率下训练150个周期。用于训练的笔记本位于 [这里](https://github.com/itberrios/CV_projects/blob/main/multitask_depth_seg/training.ipynb)。
- en: '![](../Images/ba43475521b4feca32a2bd1c86a42cf1.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba43475521b4feca32a2bd1c86a42cf1.png)'
- en: Figure 5\. One Cycle Cosine Annealed Learning Rate. Source Author.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图5\. 一个周期余弦退火学习率。来源作者。
- en: We fine then tune at 1/2 resolution for 25 epochs and again at full resolution
    for another 25 epochs both with a learning rate of 5e-6\. Note that we needed
    to reduce the batch size each time we fine tuned at an increased resolution.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在1/2分辨率下微调了25个周期，然后在全分辨率下再次微调了25个周期，学习率为5e-6。请注意，每次我们在增加分辨率时微调时，都需要减少批量大小。
- en: Inference
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推断
- en: For inference we normalized the input image and ran a forward pass through the
    model. Figure 6 shows training results from both validation and test data
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在推断过程中，我们对输入图像进行了归一化，并通过模型进行了前向传播。图6显示了来自验证和测试数据的训练结果。
- en: '![](../Images/6c98035d61d2cffd1a40047375e98269.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c98035d61d2cffd1a40047375e98269.png)'
- en: Figure 6\. Inference results (top 2 are from test set and bottom 2 are from
    validation set). Source Author.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图6\. 推断结果（前两个是来自测试集，后两个是来自验证集）。来源作者。
- en: In general it seems like the model is able to segment and estimate depth when
    there are larger objects in an image. When more finely detailed objects such as
    pedestrians are present, the model tends to struggle to segment them entirely.
    The model is able to estimate their depth to some degree of accuracy.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，当图像中存在较大的物体时，模型似乎能够进行分割和深度估计。当出现更精细的物体，如行人时，模型往往难以完全分割它们。模型能够在一定程度上准确估计它们的深度。
- en: An Interesting Failure
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个有趣的失败
- en: The bottom of figure 6 shows an interesting failure case to fully segment the
    light pole in the left side of the image. The segmentation only covers the bottom
    half of the light pole, while the depth shows that the bottom half of the light
    pole is much closer than the top half. The depth failure, could be due the bias
    of bottom pixels generally corresponding to closer depth; notice the horizon line
    around pixel 500, there is a clear divide between closer pixels and further way
    pixels. It seems like this bias could have leaked into the model’s segmentation
    task. This type of *task leakage* should be considered when training multi-task
    models.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图6的底部展示了一个有趣的失败案例，即未能完全分割图像左侧的灯柱。分割仅覆盖了灯柱的下半部分，而深度图显示灯柱的下半部分比上半部分更近。深度的失败可能是由于下部像素通常对应于较近的深度；注意到像素500附近的地平线，存在明显的分界线，将较近的像素和较远的像素区分开来。似乎这种偏差可能泄漏到了模型的分割任务中。这种*任务泄漏*应在训练多任务模型时考虑。
- en: In Multi-task Learning, training data from one task can impact performance on
    another task
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在多任务学习中，一个任务的训练数据可以影响另一个任务的表现。
- en: Depth Distributions
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度分布
- en: Let’s check how the predicted depth is distributed compared to the truth. For
    simplicity, we will just use a sample of 94 true/predicted full resolution depth
    map pairs.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查预测的深度与真实深度的分布情况。为了简化，我们将只使用94对真实/预测的全分辨率深度图样本。
- en: '![](../Images/95fd89fb4612f50c13cbbf6fbfd907be.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/95fd89fb4612f50c13cbbf6fbfd907be.png)'
- en: Figure 8\. True (left) and predicted (right) depth map distributions, each with
    1000 bins. Source Author.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图8\. 真实（左）和预测（右）深度图分布，每个分布有1000个区间。来源作者。
- en: It seems like the model has learned two distributions, a distribution with a
    peak around 4 and distribution with a peak around 30\. Notice that the clipping
    artifact did not seem to make a difference. The overall distribution contains
    a long tail which is characteristic of the fact that only a small portion of an
    image will contain far away depth data.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 似乎模型学习到了两个分布，一个在4附近有峰值，另一个在30附近有峰值。注意到剪切伪影似乎没有产生影响。总体分布包含了一个长尾，这表明只有图像的一小部分包含远处的深度数据。
- en: The predicted depth distribution is much more smooth than the ground truth.
    The roughness of the ground truth distribution could come from the fact that each
    object contains similar depth values. It may be possible to use this information
    to apply some sort of regularization to force the model to follow this paradigm,
    but that will be for another time.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 预测的深度分布比真实值更平滑。真实值分布的粗糙度可能是由于每个物体包含相似的深度值。可能可以利用这些信息应用某种正则化来强制模型遵循这一范式，但那将是另一个问题。
- en: 'Bonus: Inference Speed'
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 额外内容：推理速度
- en: Since this is a lightweight model intended for speed, let’s see how fast it
    will inference on GPU. The code below has been modified from this [article](https://deci.ai/blog/measure-inference-time-deep-neural-networks/).
    In this test, the input image has been scaled down to 400x1024.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个旨在追求速度的轻量级模型，让我们看看它在GPU上的推理速度。下面的代码已从这篇[文章](https://deci.ai/blog/measure-inference-time-deep-neural-networks/)中修改。在这个测试中，输入图像被缩小到400x1024。
- en: '[PRE2]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The inference test shows that this model can run at 18.69+/-0.44 ms or about
    55Hz. It’s important to note that this is just a Python prototype ran on a laptop
    with a NVIDIA RTX 3060 GPU, different hardware will change inference speed. We
    should also note, an SDK like [Torch-TensorRt](https://developer.nvidia.com/blog/accelerating-inference-up-to-6x-faster-in-pytorch-with-torch-tensorrt/)
    could provide significant speed up if deployed on a NVIDIA GPU.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 推理测试表明，该模型可以以18.69+/-0.44毫秒或约55Hz的速度运行。需要注意的是，这只是一个在搭载NVIDIA RTX 3060 GPU的笔记本电脑上运行的Python原型，不同的硬件会改变推理速度。我们还应该注意，如果在NVIDIA
    GPU上部署像[Torch-TensorRt](https://developer.nvidia.com/blog/accelerating-inference-up-to-6x-faster-in-pytorch-with-torch-tensorrt/)这样的SDK，可以显著提高速度。
- en: Conclusion
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this post we learned how Multi-Task Learning can save costs and reduce carbon
    emissions. We learned how to build a lightweight multi-task architecture capable
    of performing classification and regression simultaneously on the CityScapes data
    set. We also leveraged CreStereo and Knowledge Distillation to help our model
    learn to predict better depth maps.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们了解了多任务学习如何节省成本和减少碳排放。我们学习了如何构建一个轻量级的多任务架构，能够在CityScapes数据集上同时执行分类和回归。我们还利用了CreStereo和知识蒸馏来帮助我们的模型更好地预测深度图。
- en: This lightweight model presents a trade-off where we sacrifice some performance
    for speed and efficiency. Even with this tradeoff, the trained model was able
    to predict reasonable depth and segmentation results on test data. Furthermore,
    it was able learn to predict a similar depth distribution to the ground truth
    depth maps.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这个轻量级模型在速度和效率之间做出了权衡。即使有这个权衡，训练后的模型仍然能够在测试数据上预测出合理的深度和分割结果。此外，它还能够学习预测与真实深度图类似的深度分布。
- en: References
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Nekrasov, Vladimir, et al. ‘Real-Time Joint Semantic Segmentation and Depth
    Estimation Using Asymmetric Annotations’. *CoRR*, vol. abs/1809.04766, 2018, [http://arxiv.org/abs/1809.04766](http://arxiv.org/abs/1809.04766.)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Nekrasov, Vladimir 等人. ‘实时联合语义分割和深度估计使用不对称注释’. *CoRR*, vol. abs/1809.04766,
    2018, [http://arxiv.org/abs/1809.04766](http://arxiv.org/abs/1809.04766.)'
- en: '[2] Standley, Trevor, et al. ‘Which Tasks Should Be Learned Together in Multi-Task
    Learning?’ *CoRR*, vol. abs/1905.07553, 2019, [http://arxiv.org/abs/1905.07553](http://arxiv.org/abs/1905.07553.)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Standley, Trevor 等人. ‘在多任务学习中应该一起学习哪些任务？’ *CoRR*, vol. abs/1905.07553,
    2019, [http://arxiv.org/abs/1905.07553](http://arxiv.org/abs/1905.07553.)'
- en: '[3] Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson,
    R., Franke, U., Roth, S., & Schiele, B. (2016). The cityscapes dataset for Semantic
    Urban Scene understanding. *2016 IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR)*. [https://doi.org/10.1109/cvpr.2016.350](https://doi.org/10.1109/cvpr.2016.350)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson,
    R., Franke, U., Roth, S., & Schiele, B. (2016). 城市景观数据集用于语义城市场景理解. *2016 IEEE
    计算机视觉与模式识别会议 (CVPR)*. [https://doi.org/10.1109/cvpr.2016.350](https://doi.org/10.1109/cvpr.2016.350)'
