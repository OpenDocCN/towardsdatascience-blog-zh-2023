- en: 'Multi-Task Architectures: A Comprehensive Guide'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/multi-task-architectures-9bee2e080456?source=collection_archive---------13-----------------------#2023-07-18](https://towardsdatascience.com/multi-task-architectures-9bee2e080456?source=collection_archive---------13-----------------------#2023-07-18)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Lightweight models for real-time multi-task inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@itberrios6?source=post_page-----9bee2e080456--------------------------------)[![Isaac
    Berrios](../Images/e36cdbfc91b71ceb91c6e4191e1e0833.png)](https://medium.com/@itberrios6?source=post_page-----9bee2e080456--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9bee2e080456--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9bee2e080456--------------------------------)
    [Isaac Berrios](https://medium.com/@itberrios6?source=post_page-----9bee2e080456--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffbadc8e8ee44&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-task-architectures-9bee2e080456&user=Isaac+Berrios&userId=fbadc8e8ee44&source=post_page-fbadc8e8ee44----9bee2e080456---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9bee2e080456--------------------------------)
    ·10 min read·Jul 18, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9bee2e080456&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-task-architectures-9bee2e080456&user=Isaac+Berrios&userId=fbadc8e8ee44&source=-----9bee2e080456---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9bee2e080456&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-task-architectures-9bee2e080456&source=-----9bee2e080456---------------------bookmark_footer-----------)![](../Images/27378ce18d84699cea51c1e7d3601715.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Julien Duduoglu](https://unsplash.com/@julien_duduoglu?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Have you ever wondered how to train a deep neural network to do many things?
    Such a model is referred to as a Multi-Task Architecture and can have benefits
    over a traditional approach that uses individual models for each task. A Multi-Task
    Architecture is a subset of [Multi-Task Learning](https://medium.com/ai-mind-labs/introduction-to-multi-task-learning-5da02afabb61)
    which is a general approach to training a model or set of models to perform multiple
    tasks simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this post we will learn how to train a single model to perform both classification
    and regression tasks simultaneously. Code for this post can be found on [GitHub](https://github.com/itberrios/CV_projects/tree/main/multitask_depth_seg).
    Here’s an overview:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Motivation**](#47bc) **— Why would we do this?**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Approach**](#3ba3) **— How are we going to do this?**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Model Architecture**](#e55e)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Training Approach**](#46fc)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Inference**](#4c64) **— Check performance and learn from an** [***interesting
    failure***](#a6dd)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Conclusion**](#cfa3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Motivation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Why would we want to use a light weight model? Won’t that decrease performance?
    If we are not deploying to the edge shouldn’t we use as big of a model as possible?*'
  prefs: []
  type: TYPE_NORMAL
- en: Edge applications need light weight models to perform real-time inference with
    low power consumption. Other applications can benefit from them as well, but how?
    An overlooked benefit of lightweight models is their lower compute requirement.
    In general this can lower server usage and therefore decrease power consumption.
    This has the overall effect of *reducing costs* and *lowering carbon emissions*,
    the later of which could become a major [issue](https://www.bloomberg.com/news/articles/2023-03-09/how-much-energy-do-ai-and-chatgpt-use-no-one-knows-for-sure)
    in the future of AI.
  prefs: []
  type: TYPE_NORMAL
- en: Lightweight models can help reduce costs and lower carbon emissions via less
    power consumption
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: With all this being said, a multi-task architecture is a just a tool in the
    toolbox, and all project requirements should be considered before deciding which
    tools to use. Now let’s dive into an example of how train one of these!
  prefs: []
  type: TYPE_NORMAL
- en: Approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To build our Multi-Task Architecture, we will loosely cover the approach from
    this [paper](https://arxiv.org/pdf/1809.04766.pdf), where a single model was trained
    for simultaneous segmentation and depth estimation. The underlying goal was to
    perform these tasks in a fast and efficient manner with a trade-off being an acceptable
    loss of performance. In Multi-Task Learning, we typically group similar tasks
    together. During training, we can also add an auxiliary task that may assist our
    model’s learning, but we may decide not to use it during inference [[1](https://arxiv.org/pdf/1809.04766.pdf),
    [2](https://arxiv.org/pdf/1905.07553.pdf)]. For simplicity, we will not use any
    auxiliary tasks during training.
  prefs: []
  type: TYPE_NORMAL
- en: Depth and Segmentation are both dense prediction tasks, and have similarities.
    For example, the depth of a single object will likely be consistent across all
    areas of the object, forming a very narrow distribution. The main idea is that
    each individual object should a have it’s own depth value, and we should be able
    to recognize individual objects just by looking at a depth map. In the same manner,
    we should be able to recognize the same individual objects by looking at a segmentation
    map. While there are likely to be some outliers, we will assume that this relationship
    holds.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use the [City Scapes dataset](https://www.cityscapes-dataset.com/) to
    provide (left camera) input images segmentation masks, and depth maps. For the
    segmentation maps, we choose to use the standard training labels, with 19 classes
    + 1 unlabeled category.
  prefs: []
  type: TYPE_NORMAL
- en: Depth Map Preparation — default disparity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Disparity maps created with [SteroSGBM](https://core.ac.uk/download/pdf/11134866.pdf)
    are readily available from the CityScapes website. The Disparity describes the
    pixel difference between objects as viewed from each stereo camera’s perspective,
    and it is inversely proportional to the depth which can be computed with:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8145aa8170b00192bab9898d51a62b30.png)'
  prefs: []
  type: TYPE_IMG
- en: 'City Scapes depth computation, units are in parenthesis. Source: Author.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the default disparity maps are noisy with many holes corresponding
    to infinite depth and a portion where the ego vehicle is always shown. A common
    approach to cleaning these disparity maps involves:'
  prefs: []
  type: TYPE_NORMAL
- en: Crop out the bottom 20% along with parts of the left and top edges
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Resize to original scale
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply a smoothing filter
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform [inpainting](https://docs.opencv.org/3.4/df/d3d/tutorial_py_inpainting.html)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once we clean the disparity we can compute the depth, which results in:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b2e6a6278f6c2f5e928fa7619e1ad651.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. Depth data from City Scapes. Source Author.
  prefs: []
  type: TYPE_NORMAL
- en: The fine details of this approach are outside the scope of this post, but if
    your interested here’s a video explanation on [YouTube](https://www.youtube.com/watch?v=jlZZu1t39Zs&feature=youtu.be).
  prefs: []
  type: TYPE_NORMAL
- en: The crop and resize step means that the disparity (as well as the depth) map
    won’t exactly align with the input image. Even though we could do the same crop
    and resize with the input image to correct for this, we opted to explore a new
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: Depth Map Preparation — CreStereo disparity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We explored using [CreStereo](https://arxiv.org/pdf/2203.11483.pdf) to produce
    high quality disparity maps from both the left and right images. CreStereo is
    an advanced model that is able to predict smooth disparity maps from stereo image
    pairs. This approach introduces a paradigm known as [knowledge distillation](https://en.wikipedia.org/wiki/Knowledge_distillation),
    where CreStereo is a teacher network and our model will be the student network
    (at least for the depth estimation). This details of this approach are outside
    the scope of this post, but here’s a [YouTube](https://www.youtube.com/watch?v=e3Zuc8AbwoA)
    link if you’re interested.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, the CreStereo depth maps have minimal noise so there’s no need
    to crop and resize. However, the ego vehicle present in the segmentation masks
    could cause issues with generalization so the bottom 20% was removed on all training
    images. A training sample is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bfb06ddc24959a047a970a383d948b34.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. Training Sample. Source Author.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our data, let’s see the architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Model Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Following [[1](https://arxiv.org/pdf/1809.04766.pdf)], the architecture will
    consist of a MobileNet backbone/encoder, a [LightWeight RefineNet](https://arxiv.org/pdf/1810.03272.pdf)
    Decoder, and Heads for each individual task. The overall architecture is shown
    below in figure 3.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8a495fd1f3517f2a2e13c128f21792e2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. Model Architecture. [Source](https://arxiv.org/pdf/1809.04766.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: For the encoder/backbone, we will use a [MobileNetV3](https://arxiv.org/pdf/1905.02244.pdf)
    and pass skip connections at 1/4, 1/8, 1/16, and 1/32 resolutions to the Light
    Weight Refine Net. Finally, the output is passed to each head that is responsible
    for a different task. Notice how we can even add more tasks to this architecture
    if we wanted to.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ebee8bae8088ebffd1530f3cb04ea4f8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. (Left) Detailed Encoder-Decoder Multi-Task Architecture. (Right)
    details of the LightWeight RefineNet Blocks. Modified from [Source](https://arxiv.org/pdf/1810.03272.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: To implement the encoder, we use a pre-trained MobileNetV3 encoder, where we
    will pass in the MobileNetV3 encoder to a custom PyTorch Module. The output of
    it’s forward function is a [ParameterDict](https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html)
    of skip connections for input to the LightWeight Refine Net. The code snippet
    below shows how to do this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The LightWeight RefineNet Decoder is very similar to the one [implemented](https://github.com/DrSleep/multi-task-refinenet/blob/master/src/models.py)
    in [[1](https://arxiv.org/pdf/1809.04766.pdf)], except with a few modifications
    to make it compatible with [MobileNetV3](https://arxiv.org/pdf/1905.02244.pdf)
    as opposed to MobileNetV2\. We also note that the decoder portion consists of
    the Segmentation and Depth heads. The full code for the model is available on
    [GitHub](https://github.com/itberrios/CV_projects/blob/main/multitask_depth_seg/model.py).
    We can piece together the model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Training Approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We divide training into three phases, the first at 1/4 resolution, the second
    at 1/2 resolution, and the final at full resolution. All of the weights were updated,
    since freezing the encoder weights didn’t seem to produce good results.
  prefs: []
  type: TYPE_NORMAL
- en: Transformations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During each phase, we perform random crop resize, color jitter, random flips,
    and normalization. The left input image is normalized with standard image net
    mean and standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: Depth Transformation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In general the depth maps contains mostly smaller values, since most of the
    information contained in a depth map consists of objects and surfaces close to
    the camera. Since the depth map has most of it’s depth concentrated around lower
    values (see left of figure 4 below), it will need to be transformed to be effectively
    learned by a neural network. The depth map is clipped between 0 and 250, this
    is because stereo disparity/depth data at large distances is typically unreliable
    and in this case we want a way to *discard* it. Then we take the natural log and
    divide it by 5 to condense the distribution around a smaller range of numbers.
    See this [notebook](https://github.com/itberrios/CV_projects/blob/main/multitask_depth_seg/depth_normalization.ipynb)
    for more details.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9f0ab68fac873956f7adfcbb1401ed3d.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. Left — Clipped Depth Distribution. Right — Transformed Depth Distribution.
    Depth was sampled form 64 random full size training depth masks. Source Author.
  prefs: []
  type: TYPE_NORMAL
- en: I’ll be honest, I wasn’t sure of the best way to transform the depth data. If
    there’s a better way or if you would do it differently I would interested to learn
    more in the comments :).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Loss Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We keep the loss functions simple, Cross Entropy Loss for segmentation and Mean
    Squared Error for Depth Estimation. We add them together with no weighting and
    optimize them jointly.
  prefs: []
  type: TYPE_NORMAL
- en: Learning Rate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We use a One Cycle Cosine Annealed Learning Rate with a max at 5e-4 and train
    for 150 epochs at 1/4 resolution. The notebook used for training is located [here](https://github.com/itberrios/CV_projects/blob/main/multitask_depth_seg/training.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ba43475521b4feca32a2bd1c86a42cf1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5\. One Cycle Cosine Annealed Learning Rate. Source Author.
  prefs: []
  type: TYPE_NORMAL
- en: We fine then tune at 1/2 resolution for 25 epochs and again at full resolution
    for another 25 epochs both with a learning rate of 5e-6\. Note that we needed
    to reduce the batch size each time we fine tuned at an increased resolution.
  prefs: []
  type: TYPE_NORMAL
- en: Inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For inference we normalized the input image and ran a forward pass through the
    model. Figure 6 shows training results from both validation and test data
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6c98035d61d2cffd1a40047375e98269.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6\. Inference results (top 2 are from test set and bottom 2 are from
    validation set). Source Author.
  prefs: []
  type: TYPE_NORMAL
- en: In general it seems like the model is able to segment and estimate depth when
    there are larger objects in an image. When more finely detailed objects such as
    pedestrians are present, the model tends to struggle to segment them entirely.
    The model is able to estimate their depth to some degree of accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: An Interesting Failure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The bottom of figure 6 shows an interesting failure case to fully segment the
    light pole in the left side of the image. The segmentation only covers the bottom
    half of the light pole, while the depth shows that the bottom half of the light
    pole is much closer than the top half. The depth failure, could be due the bias
    of bottom pixels generally corresponding to closer depth; notice the horizon line
    around pixel 500, there is a clear divide between closer pixels and further way
    pixels. It seems like this bias could have leaked into the model’s segmentation
    task. This type of *task leakage* should be considered when training multi-task
    models.
  prefs: []
  type: TYPE_NORMAL
- en: In Multi-task Learning, training data from one task can impact performance on
    another task
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Depth Distributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s check how the predicted depth is distributed compared to the truth. For
    simplicity, we will just use a sample of 94 true/predicted full resolution depth
    map pairs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/95fd89fb4612f50c13cbbf6fbfd907be.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8\. True (left) and predicted (right) depth map distributions, each with
    1000 bins. Source Author.
  prefs: []
  type: TYPE_NORMAL
- en: It seems like the model has learned two distributions, a distribution with a
    peak around 4 and distribution with a peak around 30\. Notice that the clipping
    artifact did not seem to make a difference. The overall distribution contains
    a long tail which is characteristic of the fact that only a small portion of an
    image will contain far away depth data.
  prefs: []
  type: TYPE_NORMAL
- en: The predicted depth distribution is much more smooth than the ground truth.
    The roughness of the ground truth distribution could come from the fact that each
    object contains similar depth values. It may be possible to use this information
    to apply some sort of regularization to force the model to follow this paradigm,
    but that will be for another time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bonus: Inference Speed'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since this is a lightweight model intended for speed, let’s see how fast it
    will inference on GPU. The code below has been modified from this [article](https://deci.ai/blog/measure-inference-time-deep-neural-networks/).
    In this test, the input image has been scaled down to 400x1024.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The inference test shows that this model can run at 18.69+/-0.44 ms or about
    55Hz. It’s important to note that this is just a Python prototype ran on a laptop
    with a NVIDIA RTX 3060 GPU, different hardware will change inference speed. We
    should also note, an SDK like [Torch-TensorRt](https://developer.nvidia.com/blog/accelerating-inference-up-to-6x-faster-in-pytorch-with-torch-tensorrt/)
    could provide significant speed up if deployed on a NVIDIA GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this post we learned how Multi-Task Learning can save costs and reduce carbon
    emissions. We learned how to build a lightweight multi-task architecture capable
    of performing classification and regression simultaneously on the CityScapes data
    set. We also leveraged CreStereo and Knowledge Distillation to help our model
    learn to predict better depth maps.
  prefs: []
  type: TYPE_NORMAL
- en: This lightweight model presents a trade-off where we sacrifice some performance
    for speed and efficiency. Even with this tradeoff, the trained model was able
    to predict reasonable depth and segmentation results on test data. Furthermore,
    it was able learn to predict a similar depth distribution to the ground truth
    depth maps.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Nekrasov, Vladimir, et al. ‘Real-Time Joint Semantic Segmentation and Depth
    Estimation Using Asymmetric Annotations’. *CoRR*, vol. abs/1809.04766, 2018, [http://arxiv.org/abs/1809.04766](http://arxiv.org/abs/1809.04766.)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Standley, Trevor, et al. ‘Which Tasks Should Be Learned Together in Multi-Task
    Learning?’ *CoRR*, vol. abs/1905.07553, 2019, [http://arxiv.org/abs/1905.07553](http://arxiv.org/abs/1905.07553.)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson,
    R., Franke, U., Roth, S., & Schiele, B. (2016). The cityscapes dataset for Semantic
    Urban Scene understanding. *2016 IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR)*. [https://doi.org/10.1109/cvpr.2016.350](https://doi.org/10.1109/cvpr.2016.350)'
  prefs: []
  type: TYPE_NORMAL
