- en: Datasets to Train, Validate, and Evaluate Machine Translation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练、验证和评估机器翻译的数据集
- en: 原文：[https://towardsdatascience.com/datasets-to-train-validate-and-evaluate-machine-translation-d61905d126aa?source=collection_archive---------2-----------------------#2023-02-04](https://towardsdatascience.com/datasets-to-train-validate-and-evaluate-machine-translation-d61905d126aa?source=collection_archive---------2-----------------------#2023-02-04)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/datasets-to-train-validate-and-evaluate-machine-translation-d61905d126aa?source=collection_archive---------2-----------------------#2023-02-04](https://towardsdatascience.com/datasets-to-train-validate-and-evaluate-machine-translation-d61905d126aa?source=collection_archive---------2-----------------------#2023-02-04)
- en: Select, check, and split
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择、检查和拆分
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----d61905d126aa--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----d61905d126aa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d61905d126aa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d61905d126aa--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----d61905d126aa--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie?source=post_page-----d61905d126aa--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----d61905d126aa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d61905d126aa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d61905d126aa--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----d61905d126aa--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdatasets-to-train-validate-and-evaluate-machine-translation-d61905d126aa&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----d61905d126aa---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d61905d126aa--------------------------------)
    ·13 min read·Feb 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd61905d126aa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdatasets-to-train-validate-and-evaluate-machine-translation-d61905d126aa&user=Benjamin+Marie&userId=ad2a414578b3&source=-----d61905d126aa---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdatasets-to-train-validate-and-evaluate-machine-translation-d61905d126aa&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----d61905d126aa---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d61905d126aa--------------------------------)
    ·13 min 阅读·2023年2月4日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd61905d126aa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdatasets-to-train-validate-and-evaluate-machine-translation-d61905d126aa&user=Benjamin+Marie&userId=ad2a414578b3&source=-----d61905d126aa---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd61905d126aa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdatasets-to-train-validate-and-evaluate-machine-translation-d61905d126aa&source=-----d61905d126aa---------------------bookmark_footer-----------)![](../Images/b19a0e5aa70cbc0a6adef86313a71daf.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd61905d126aa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdatasets-to-train-validate-and-evaluate-machine-translation-d61905d126aa&source=-----d61905d126aa---------------------bookmark_footer-----------)![](../Images/b19a0e5aa70cbc0a6adef86313a71daf.png)'
- en: Image from [Pixabay](https://pixabay.com/illustrations/translate-translation-web-service-3324171/)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [Pixabay](https://pixabay.com/illustrations/translate-translation-web-service-3324171/)
- en: For most natural language processing (NLP) tasks, an important step is the selection
    of the datasets to train, validate, and evaluate a system. Machine translation
    is no exception, but has some specificities inherent to the multilinguality of
    the task.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数自然语言处理（NLP）任务，一个重要的步骤是选择用于训练、验证和评估系统的数据集。机器翻译也不例外，但由于任务的多语言特性，它有一些特定的要求。
- en: In this article, I explain how to select, check, and split datasets to make
    a machine translation system. I show with examples what are the most important
    properties of a dataset for machine translation and how to set the trade-off between
    the quality and the quantity of data, depending on the objective of the machine
    translation systems.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我解释了如何选择、检查和拆分数据集，以构建一个机器翻译系统。我通过实例展示了数据集中最重要的属性，并根据机器翻译系统的目标如何在数据的质量和数量之间进行权衡。
- en: Train, validate, and evaluate
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练、验证和评估
- en: 'To build a machine translation system, we need as much data as possible for:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建一个机器翻译系统，我们需要尽可能多的数据：
- en: '**Training**: A machine translation system must be trained to learn how to
    translate. If we plan to use a neural model, this step is by far the most costly
    one in terms of data and compute resources.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练**：机器翻译系统必须经过训练才能学习如何翻译。如果我们计划使用神经模型，这一步骤在数据和计算资源方面是最昂贵的。'
- en: '**Validation**: A validation dataset can be used during training to monitor
    the performance of the model being trained. For instance, if the performance doesn’t
    improve after some time, we can decide to stop the training early. Then, if we
    have saved models at different training steps, we can select the one performing
    the best on the validation data, and use this model for evaluation.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证**：可以在训练过程中使用验证数据集来监控模型的表现。例如，如果表现一段时间后没有改善，我们可以决定提前停止训练。然后，如果我们在不同训练步骤保存了模型，我们可以选择在验证数据上表现最佳的模型，并用这个模型进行评估。'
- en: '**Evaluation**: This step automatically yields the performance of our selected
    model on a dataset that is as close as possible to the text our system will translate
    once deployed. If the performance is satisfying, then we can deploy our model.
    If not, we would have to retrain the model with different hyperparameters or training
    data.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估**：这一步骤自动生成我们选择的模型在尽可能接近我们系统实际部署后将要翻译的文本的数据集上的表现。如果表现令人满意，那么我们可以部署我们的模型。如果不满意，我们需要用不同的超参数或训练数据重新训练模型。'
- en: '[](/watch-out-for-your-beam-search-hyperparameters-9c4daf6668d6?source=post_page-----d61905d126aa--------------------------------)
    [## Watch Out For Your Beam Search Hyperparameters'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/watch-out-for-your-beam-search-hyperparameters-9c4daf6668d6?source=post_page-----d61905d126aa--------------------------------)
    [## 注意你的束搜索超参数'
- en: The default values are never the best
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 默认值永远不是最优的。
- en: towardsdatascience.com](/watch-out-for-your-beam-search-hyperparameters-9c4daf6668d6?source=post_page-----d61905d126aa--------------------------------)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/watch-out-for-your-beam-search-hyperparameters-9c4daf6668d6?source=post_page-----d61905d126aa--------------------------------)
- en: All these datasets are **parallel corpora** in **source** and **target languages**,
    and ideally in the **target domain**.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些数据集都是**平行语料库**，包括**源语言**和**目标语言**，并且理想情况下是**目标领域**的。
- en: That’s a lot of keywords in a single sentence. Let’s explain them one by one.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这句中包含了很多关键词。让我们逐一解释它们。
- en: '**Source language**: This is the language of the text that will be translated
    by our machine translation system.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**源语言**：这是将由我们的机器翻译系统翻译的文本的语言。'
- en: '**Target language**: This is the language of the translation generated by the
    machine translation system.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标语言**：这是机器翻译系统生成的翻译的语言。'
- en: '**Target domain**: This notion is more complex to define. Let’s say that the
    data used to build our system should look as close as possible to the data that
    the system will be translated once deployed: **the same style, genre, and topic**
    for instance. If we want our system to translate tweets, it would be much better
    if trained on tweets than if it was trained on scientific abstracts. It may seem
    obvious, but usually finding a large dataset in the target domain is challenging
    so we have to approximate it.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标领域**：这个概念更复杂。假设用于构建我们系统的数据应尽可能接近系统部署后将要翻译的数据，例如**相同的风格、体裁和主题**。如果我们希望系统翻译推文，那么用推文训练模型比用科学摘要训练要好得多。这可能显而易见，但通常找到一个大型目标领域数据集是具有挑战性的，因此我们必须尽量接近它。'
- en: '**Parallel corpora**: This is usually in the form of sentences or **segments**
    in the source language paired with their translations in the target language.
    We use parallel data to teach the system how to translate. This type of data has
    many other names: parallel data, bilingual corpora, bitext, etc. “Parallel data”
    is probably the most common one.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平行语料库**：这通常以源语言中的句子或**段落**形式出现，并与其在目标语言中的翻译配对。我们使用平行数据来教系统如何翻译。这类数据还有许多其他名称：平行数据、双语语料库、双语文本等。“平行数据”可能是最常见的名称。'
- en: 'For example, the following dataset is parallel:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下数据集是平行的：
- en: '![](../Images/e7b0aea558f481a2f2e339fc8e20bb06.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7b0aea558f481a2f2e339fc8e20bb06.png)'
- en: Extracted from the Paracrawl English-Dutch parallel corpus ([CC0](https://www.paracrawl.eu/)).
    Screenshot by the author.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Paracrawl 英荷平行语料库中提取（[CC0](https://www.paracrawl.eu/)）。作者截图。
- en: Quality
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 质量
- en: To get the best machine translation system, we need a large parallel corpus
    to train the system. But we **shouldn’t sacrifice quality for quantity**.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得最佳的机器翻译系统，我们需要一个大规模的平行语料库来训练系统。但我们**不应为数量而牺牲质量**。
- en: Depending on whether we talk about training or validation/evaluation data, the
    quality of the data used will have a different impact.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们讨论的是训练数据还是验证/评估数据，所使用数据的质量会产生不同的影响。
- en: But first, let’s define what are the most important characteristics for a parallel
    data of good quality to build a system from scratch.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，让我们定义一下构建系统所需的高质量平行数据的最重要特征。
- en: Correct
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正确
- en: The translations in the parallel data **should be correct and natural**. Ideally,
    it means that the translations should have been produced from scratch (i.e., not
    post-edited) by professional translators and independently checked. Very often
    parallel corpora are produced via crowdsourcing by non-professional translators.
    The data can also be simply crawled from the web and automatically paired which
    is definitely not perfect especially for domains and language pairs with only
    small data available. Even though the quality of such datasets is far from optimal,
    we may not have a choice but to use them when they are the only resource available
    for a given language pair.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 平行数据中的翻译**应当正确且自然**。理想情况下，这意味着翻译应该由专业翻译人员从零开始制作（即未后期编辑）并经过独立检查。平行语料库常常通过非专业翻译人员众包制作。数据也可以直接从网络上爬取并自动配对，这对于只有少量数据的领域和语言对而言绝对不完美。即便这类数据集的质量远非理想，我们可能也不得不在它们是唯一资源时使用它们。
- en: Aligned
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对齐
- en: The segments, or documents, in the parallel data **should be correctly aligned**.
    If segments are not paired properly, the system will learn wrong translations
    at training time.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 平行数据中的段落或文档**应当正确对齐**。如果段落配对不正确，系统在训练时将学到错误的翻译。
- en: Original
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 原始
- en: The source side of the parallel data **should not be a translation** from another
    language. This point is maybe a bit complex to fully understand. We want our system
    to learn how to translate text in the source language. But, if at training time,
    we provide our system with text that was not originally in the source language,
    i.e., text that is already a translation from another source language, then it
    would learn how to translate translations better than original text. I’ll detail
    why this is important below.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 平行数据的源语言部分**不应是其他语言的翻译**。这一点可能稍微复杂一些。我们希望我们的系统学习如何翻译源语言中的文本。然而，如果在训练时，我们提供的文本不是原本的源语言文本，即已经是从其他语言翻译过来的文本，那么系统将更擅长翻译翻译文本而非原始文本。下面我会详细说明为什么这一点很重要。
- en: In-domain
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 领域内
- en: The data **should be in the target domain.** This is arguable and suits the
    ideal scenario. We can train a very good system on out-of-domain dataset and fine-tune
    it later on a smaller dataset in the target domain.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 数据**应当在目标领域内**。这是一个有争议的理想情境。我们可以在非目标领域的数据集上训练一个非常好的系统，然后在目标领域的较小数据集上进行微调。
- en: Raw
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 原始
- en: The data **should be close to raw**. Using an already pre-processed dataset
    is often a bad idea. By pre-processing, I mean any process that altered the original
    text. It can be tokenization, truecasing, punctuation normalization, etc. Very
    often, all these pre-processing steps are under-specified with the consequence
    that we can’t exactly reproduce them on the text our system will actually translate
    once deployed. It is way safer, and sometimes faster, to define our own pre-processing
    steps.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 数据**应该接近原始状态**。使用已经预处理过的数据集通常是个坏主意。这里的预处理指的是任何改变了原始文本的过程。这可以是分词、大小写还原、标点规范化等。很常见的是，所有这些预处理步骤都没有明确说明，这导致我们无法在我们的系统实际翻译的文本上准确重现它们。定义我们自己的预处理步骤要安全得多，有时也更快捷。
- en: To have a rough idea about the quality of the dataset, we **should always know
    where the data come from and how it was created**. I’ll write more about this
    below.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 要大致了解数据集的质量，我们**应该始终知道数据的来源及其创建方式**。我会在下面详细介绍。
- en: At training time, the machine translation system will learn the properties of
    the parallel data. Neural models are rather robust to noise but if our training
    data is very noisy, i.e., misaligned or with many translation errors, the system
    will learn to generate translations with errors.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练时，机器翻译系统将学习平行数据的属性。神经模型对噪声相对较为鲁棒，但如果我们的训练数据非常嘈杂，即对齐不准确或有许多翻译错误，系统将学习生成带有错误的翻译。
- en: At validation/evaluation time, the quality of the parallel data used is even
    more critical. If our dataset is of a poor quality, the evaluation step will only
    tell us how good our system is at poorly translating. In other words, it would
    be a useless evaluation, but that may convince us to deploy a machine translation
    system poorly trained.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在验证/评估时，使用的平行数据的质量更加关键。如果我们的数据集质量较差，评估步骤将仅告诉我们系统在翻译不佳方面的表现。换句话说，这将是一次无用的评估，但可能会让我们误以为可以部署一个训练不充分的机器翻译系统。
- en: Quantity
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数量
- en: In addition to quality, the quantity of data used is also critical.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 除了质量，数据的数量也至关重要。
- en: “quantity” often refers to the number of parallel segments in the parallel corpora.
    I will use this definition here.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: “数量”通常指的是平行语料库中平行段落的数量。我在这里将使用这个定义。
- en: 'For training, **using as much data as possible is a good rule of thumb** provided
    that the data is of a reasonable quality. I classify training scenarios into 3
    categories:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练来说，**尽可能使用更多的数据是一个好的经验法则**，前提是数据的质量合理。我将训练场景分为3类：
- en: '**low-resource**: the training data contains less than 100,000 parallel segments
    (or so-called sentences)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低资源**：训练数据包含少于100,000个平行段落（或称为句子）'
- en: '**medium-resource**: the training data contains between 100,000 and 1,000,000
    parallel segments'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**中等资源**：训练数据包含100,000到1,000,000个平行段落'
- en: '**high-resource**: the training data contains more than 1,000,000 parallel
    segments'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高资源**：训练数据包含超过1,000,000个平行段落'
- en: For validation and evaluation, using a lot of data may seem to be the right
    choice to get an accurate evaluation of our models, but usually we actually prefer
    to **use more data for training rather than for validation and evaluation**.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于验证和评估，使用大量数据可能看似是获取我们模型准确评估的正确选择，但通常我们更倾向于**将更多的数据用于训练而不是验证和评估**。
- en: If you look at best practices in research and development, you will find that
    validation and evaluation datasets for machine translation usually contain between
    1,000 and 3,000 parallel segments. Keep in mind here that the quality of these
    datasets is much more important than its quantity, in contrast to the training
    dataset. We want the evaluation dataset perfectly translated and as close as possible
    to the text our system will translate.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看研究和开发中的最佳实践，你会发现机器翻译的验证和评估数据集通常包含1,000到3,000个平行段落。请记住，这些数据集的质量比数量重要得多，与训练数据集不同。我们希望评估数据集的翻译尽可能完美，并且尽可能接近我们的系统将要翻译的文本。
- en: Monolingual data
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单语数据
- en: Monolingual data, as opposed to the parallel data I described above, are texts
    in a single language. It can be the source or the target language.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 单语数据，与我之前描述的平行数据不同，是指单一语言的文本。可以是源语言或目标语言。
- en: Since this data is monolingual, it is far easier to collect in **very large
    quantities** than parallel data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些数据是单语的，因此比平行数据更容易以**非常大**的数量进行收集。
- en: It is usually exploited to generate synthetic parallel data that is then used
    to augment the training parallel data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 它通常被用于生成合成平行数据，然后用来增强训练平行数据。
- en: There are many strategies to generate synthetic data, such as backtranslation
    and forward translation. They can be quite complex techniques with a negative
    impact in training if not handled properly.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 生成合成数据有许多策略，如回译和正向翻译。如果处理不当，它们可能会对训练产生负面影响。
- en: I’ll discuss them in detail in another blog post. Stay tuned!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在另一篇博客文章中详细讨论这些策略。敬请关注！
- en: Data leakage prevention
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据泄漏预防
- en: If you are familiar with machine learning, you probably already know what data
    leakage is.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对机器学习有所了解，你可能已经知道什么是数据泄漏。
- en: We want the training data to be as close as possible to the validation and evaluation
    data but **without any overlapping**.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望训练数据与验证和评估数据尽可能接近，但**没有任何重叠**。
- en: If there is an overlap, we talk about data leakage.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果存在重叠，我们就谈论数据泄漏。
- en: It means that our system is partly trained on data also used for validation/evaluation.
    This is a critical issue since it artificially improves the results obtained for
    validation/evaluation. The system would be indeed particularly good at translating
    its validation/evaluation data since it saw it at training time, while once in
    production the system will likely be exposed to unseen texts to translate.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们的系统部分地在用于验证/评估的数据上进行训练。这是一个关键问题，因为它人为地提高了验证/评估结果。系统确实会特别擅长翻译其验证/评估数据，因为它在训练时已经见过这些数据，而一旦投入生产，系统很可能会面临未见过的文本进行翻译。
- en: Preventing data leakage is much more difficult than it sounds, and to make things
    more complicated there are many different levels of data leakage.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 预防数据泄漏比听起来要困难得多，并且使问题更加复杂的是，数据泄漏有许多不同的层次。
- en: The most obvious case of data leakage is when pairs of segments, or documents,
    from the evaluation data are also in the training data. These segments should
    be excluded.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最明显的数据泄漏情况是评估数据中的段落或文档也出现在训练数据中。这些段落应该被排除。
- en: Another form of data leakage is when training and evaluation data were made
    from the same documents. For instance, shuffling the order of the segments of
    a dataset, then picking the first 95% for training and the last 5% for validation/evaluation
    can lead to data leakage. In this situation, we are potentially using pairs of
    segments that were originally from the same documents, probably created by the
    same translator, in both training and validation/evaluation data. It is also possible
    that segments in the training data were directly used as context to create the
    translations of the segments in the validation/evaluations data. Consequently,
    the validation/evaluation data artificially becomes easier to translate.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种数据泄漏的形式是当训练和评估数据来自相同的文档时。例如，将数据集的段落顺序打乱，然后选择前95%用于训练，最后5%用于验证/评估，可能会导致数据泄漏。在这种情况下，我们可能在训练和验证/评估数据中使用了原本来自同一文档的段落对，这些文档可能由同一译者创建。也有可能训练数据中的段落直接用于创建验证/评估数据段落的翻译。因此，验证/评估数据在翻译时人为变得更容易。
- en: To prevent data leakage, always know where the data come from, and how the data
    was made and split into training/validation/evaluation datasets.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止数据泄漏，总是要了解数据的来源，以及数据是如何制作和划分为训练/验证/评估数据集的。
- en: A word about translationese
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于翻译腔
- en: Parallel corpora have two sides. Ideally the source side is an **original text**
    written by a native speaker of the source language and the target side is a translation
    produced by native speakers of the target language.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 平行语料库有两个方面。理想情况下，源语言端是由源语言母语者编写的**原始文本**，而目标语言端是由目标语言母语者生产的翻译。
- en: 'The target side is not an original text: It is a translation. A translation
    can have errors. Studies have also demonstrated that translations are lexically
    less diverse and syntactically more simple than original texts. These **translation
    artifacts** define “translationese.”'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 目标端不是原始文本：它是翻译。翻译可能包含错误。研究还表明，翻译在词汇上不如原文多样，在句法上也比原文简单。这些**翻译伪影**定义了“翻译腔”。
- en: '*Why is it important in machine translation?*'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*为什么在机器翻译中这很重要？*'
- en: Let’s say you have a parallel corpus with an original source side in Spanish
    and its translation in English. This is perfect for a Spanish-to-English machine
    translation system.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个平行语料库，原始源语言为西班牙语，翻译为英语。这对于一个西班牙语到英语的机器翻译系统来说是完美的。
- en: 'But if you want an English-to-Spanish system, you may be tempted to just swap
    both sides of the parallel corpus: The original text would be on the target side
    and the translation on the source side.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你想要一个英语到西班牙语的系统，你可能会想直接交换平行语料库的两边：原文在目标语言侧，翻译在源语言侧。
- en: Then, your system will learn to translate… translations! Since translations
    are easier to translate than original text, the task is much more simple to learn
    for the neural network. But then, the machine translation system will be underperforming
    when translating the original texts input by the users.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你的系统将学习翻译……翻译！由于翻译比原文更容易翻译，神经网络学习任务要简单得多。但然后，机器翻译系统在翻译用户输入的原文时会表现不佳。
- en: 'The bottom line is: **Check the origin of the data** to be sure, at least,
    that you don’t have translations on the source side.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 底线是：**检查数据的来源**，以确保至少你不会在源语言中找到翻译内容。
- en: Note that sometimes this situation is inevitable, especially when tackling low-resource
    languages.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，有时这种情况是不可避免的，特别是在处理低资源语言时。
- en: Sources of parallel corpora
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平行语料库的来源
- en: Fortunately, there are many parallel corpora available online in various domains
    and languages.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，网上有许多领域和语言的平行语料库。
- en: 'I mainly use the following websites to get what I need:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我主要使用以下网站获取所需的内容：
- en: '[OPUS](https://opus.nlpl.eu/): This is probably the most extensive source of
    parallel corpora. There are dozens of corpora available for 300+ languages. They
    are downloadable in plain text (2 files: 1 for the source language and 1 for the
    target language) or in the TMX format which is an XML format often used in the
    translation industry. For each corpus, the size and length (in number of segments
    and tokens) is also given.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OPUS](https://opus.nlpl.eu/)：这可能是最广泛的平行语料库来源。为 300 多种语言提供了数十种语料库。它们可以以纯文本格式（2
    个文件：1 个用于源语言，1 个用于目标语言）或 TMX 格式下载，TMX 是翻译行业中常用的 XML 格式。每个语料库的大小和长度（以段落和标记数量计）也会给出。'
- en: '[Dataset from Hugging Face](https://huggingface.co/datasets): This one is not
    specialized in resources for machine translation but you can find there a lot
    of parallel corpora if you select the “translation” tag. The intersection between
    OPUS and Dataset is huge, but you will find some parallel corpora that are not
    available on OPUS.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hugging Face 的数据集](https://huggingface.co/datasets)：这个数据集并不是专门针对机器翻译资源，但你可以通过选择“translation”标签找到许多平行语料库。OPUS
    和 Dataset 的交集非常大，但你会发现一些 OPUS 上没有的平行语料库。'
- en: This is by far the two biggest sources of parallel corpora. If you know others,
    please indicate them in the comments.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这是目前最大的两个平行语料库来源。如果你知道其他来源，请在评论中指出。
- en: Be aware that most of the parallel corpora you will find there can be used for
    research and academic purposes, but not for commercial purposes. OPUS doesn’t
    show the license for each dataset. If you need to know it, you will have to directly
    check the original source of the dataset or contact the people who created it.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，你会发现的大多数平行语料库只能用于研究和学术目的，而不能用于商业目的。OPUS 不会显示每个数据集的许可信息。如果你需要知道，请直接检查数据集的原始来源或联系创建者。
- en: Examples
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例
- en: 'Now let’s be more practical and manipulate some datasets. I created two tasks
    for which I need parallel data:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更实际地操作一些数据集。我创建了两个需要平行数据的任务：
- en: 'Task 1: A general machine translation system to translate Spanish into English
    (Es→En)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务 1：一个通用的机器翻译系统，将西班牙语翻译成英语（Es→En）
- en: 'Task 2: A specialized machine translation system to translate COVID-19 related
    content from Swahili to English (Sw→En)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务 2：一个专门的机器翻译系统，将 COVID-19 相关内容从斯瓦希里语翻译成英语（Sw→En）
- en: We will first focus on *Task 1*.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先专注于*任务 1*。
- en: We can start to search on OPUS to find whether there are parallel corpora for
    this task.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以开始在 OPUS 上搜索，看是否有针对这个任务的平行语料库。
- en: 'Fortunately, Es→En is a high-resource task. Plenty of parallel corpora are
    available in various domains. For instance, from OPUS we can get:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，西班牙语到英语（Es→En）是一个高资源任务。在各种领域有大量平行语料库。例如，从 OPUS，我们可以获得：
- en: '![](../Images/ccad4f1ad23139ba0371f719defe4caf.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ccad4f1ad23139ba0371f719defe4caf.png)'
- en: Screenshot from [OPUS](https://opus.nlpl.eu).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[OPUS](https://opus.nlpl.eu)的截图。
- en: 'The first one, “ParaCrawl v9” is one of the largest. It has been automatically
    created but is good enough to train a machine translation system. We should always
    check the license to be sure we can use it for our target application. As I mentioned
    above, OPUS doesn’t provide license information, but it does provide the source
    of the dataset once you click on it. For license information, we have to check
    the original source of the data: [https://www.paracrawl.eu/](https://www.paracrawl.eu/).
    This corpus is provided under a [CC0 license](https://creativecommons.org/share-your-work/public-domain/cc0/).
    Academic and commercial uses are allowed.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个，“ParaCrawl v9”是最大的之一。它是自动创建的，但足够好以训练机器翻译系统。我们应该始终检查许可证，以确保我们可以将其用于目标应用程序。如上所述，OPUS没有提供许可证信息，但一旦点击，它会提供数据集的来源。有关许可证信息，我们必须检查数据的原始来源：[https://www.paracrawl.eu/](https://www.paracrawl.eu/)。该语料库在[CC0许可证](https://creativecommons.org/share-your-work/public-domain/cc0/)下提供。允许学术和商业用途。
- en: 'This is a huge corpus containing 264M pairs of segments. This is more than
    enough to split it into train/validation/evaluating datasets. I would split the
    data like this to avoid data leakage:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个包含264M对段的大型语料库。这个数据量足够将其拆分为训练/验证/评估数据集。我会这样拆分数据以避免数据泄露：
- en: '![](../Images/8645063b165ced63106b235166e7b22c.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8645063b165ced63106b235166e7b22c.png)'
- en: Illustration by the author.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 作者插图。
- en: Since this is a lot of segments, we can split the data into consecutive chunks
    of 10M segments. I would extract a chunk, the last one for instance, that I would
    resplit into smaller consecutive chunks of 1M. Finally, I would randomly extract
    3,000 segments for validation, from the first smaller chunk, and another 3,000
    segments for evaluation, from the last smaller chunk.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 由于段对数量很多，我们可以将数据拆分为连续的10M段对。我会提取一个段，对例如最后一个段进行重新拆分成更小的连续段对1M。最后，我会从第一个较小段提取3,000个段用于验证，从最后一个较小段提取另外3,000个段用于评估。
- en: It is enough distance between training, validation, and evaluation datasets.
    This is a very simple way to do it but far from optimal. It doesn’t prevent data
    leakage if the segments in the corpus were already shuffled.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 训练、验证和评估数据集之间的距离足够。这是一种非常简单的方法，但远非最佳。如果语料库中的段对已经被打乱，它不会防止数据泄露。
- en: There are other methods, that I won’t discuss here, to better guarantee the
    absence of data leakage while extracting the most useful segment pairs for each
    datasets.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他方法，我在这里不讨论，以更好地保证在提取每个数据集最有用的段对时数据不会泄露。
- en: For training, you can begin for instance with the first 2 chunks of 10M segments.
    If you are not satisfied by the translation quality you can add more chunks into
    your training data.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练，你可以从例如前两个10M段开始。如果对翻译质量不满意，你可以将更多的段加入训练数据中。
- en: If the quality of the translation doesn’t improve much, it means that you may
    not need to use the remaining 200M+ segment pairs.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果翻译质量没有太大改善，说明你可能不需要使用剩余的200M+段对。
- en: '*Task 2* is much more challenging.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*任务2* 要困难得多。'
- en: We want to translate Swahili. African languages are notoriously low-resource.
    In addition, we target a relatively new domain, COVID-19, so we can expect the
    data available for this task to be extremely small.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要翻译斯瓦希里语。非洲语言通常资源较少。此外，我们针对的是一个相对较新的领域——COVID-19，因此我们可以预期这个任务可用的数据会非常少。
- en: 'As expected, on OPUS far fewer datasets are available:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，在OPUS上可用的数据集要少得多：
- en: '![](../Images/a5918577f225704639f73c1990b50506.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a5918577f225704639f73c1990b50506.png)'
- en: Screenshot from [OPUS](https://opus.nlpl.eu).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[OPUS](https://opus.nlpl.eu)的截图。
- en: A good point here is that Paracrawl is also available for Sw→En, but is fairly
    small with its 100,000 segment pairs. Yet, this is one of the largest resources
    available with a CC0 license. I would use it for training, and then try to add
    other sources of data (such as CCMatrix or CCAligned) to observe how the performance
    improves.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的点是，Paracrawl也提供Sw→En的资源，但其10万个段对相对较小。然而，这是一个最大的CC0许可证资源之一。我会用它进行训练，然后尝试添加其他数据源（如CCMatrix或CCAligned）以观察性能如何改进。
- en: '*But how to evaluate a machine translation system specialized for translating
    COVID-19 content?*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*但如何评估一个专门用于翻译COVID-19内容的机器翻译系统？*'
- en: Following the COVID-19 outbreak, an effort has been made by the research community
    to make translation resources in many languages. The [TICO-19](https://tico-19.github.io/)
    corpus is one of them, and provided with a CC0 license. It is available on OPUS.
    It is small but provides the translations of 3,100 segments in Swahili and English.
    This is enough to make validation/evaluation datasets. Here, I would take the
    1,000 for validation and the remaining segments for evaluation. Then, you will
    know how your system trained on Paracrawl performs in translating COVID-19 content.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在 COVID-19 爆发后，研究社区致力于制作多语言的翻译资源。 [TICO-19](https://tico-19.github.io/) 语料库就是其中之一，并且提供了
    CC0 许可。它可以在 OPUS 上获取。虽然它很小，但提供了 3,100 段斯瓦希里语和英语的翻译。这足以制作验证/评估数据集。在这里，我会选择 1,000
    段用于验证，其余段落用于评估。然后，你将了解你的系统在 Paracrawl 上训练后在翻译 COVID-19 内容方面的表现。
- en: Note that I didn’t talk about translationese for these two tasks. Paracrawl
    is very likely to have non-original Spanish and Swahili on its source side. The
    TICO-19 corpus has been created from English. The Swahili side is non-original.
    In other words, we can’t avoid translationese for these two tasks.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我没有讨论这两个任务中的翻译腔。Paracrawl 很可能在其源语言侧有非原始的西班牙语和斯瓦希里语。TICO-19 语料库是从英语创建的。斯瓦希里语侧是非原始的。换句话说，我们不能避免这两个任务中的翻译腔。
- en: Conclusion
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, I described how to select and split your datasets to make your
    own machine translation system.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我描述了如何选择和拆分你的数据集，以创建你自己的机器翻译系统。
- en: To conclude, I would say that the most important point is to find the best trade-off
    between quality and quantity, especially if you target low-resource languages.
    Also, it is critical to know your datasets very well. If left unchecked, you may
    obtain a system that totally misses its target while being biased and unfair.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我认为最重要的点是找到质量与数量之间的最佳折中点，特别是当你针对低资源语言时。此外，深入了解你的数据集也至关重要。如果不加以检查，你可能会得到一个完全偏离目标且存在偏见和不公正的系统。
- en: In a next article, I will show you how to pre-process these datasets to improve
    them and facilitate the training of machine translation.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一篇文章中，我将展示如何预处理这些数据集以改进它们，并促进机器翻译的训练。
- en: All my articles are published in The Kaitchup, my newsletter. Subscribe to receive
    weekly news, tips, and tutorials on running large language models and machine
    translation systems on your computer.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我所有的文章都发布在我的新闻通讯《The Kaitchup》中。订阅以接收每周有关运行大型语言模型和机器翻译系统的新闻、技巧和教程。
- en: '[](https://kaitchup.substack.com/?source=post_page-----d61905d126aa--------------------------------)
    [## The Kaitchup - AI on a Budget | Benjamin Marie, PhD | Substack'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://kaitchup.substack.com/?source=post_page-----d61905d126aa--------------------------------)
    [## The Kaitchup - AI on a Budget | Benjamin Marie, PhD | Substack'
- en: Subscribe for weekly AI news, tips, and tutorials on fine-tuning, running, and
    serving large language models on your…
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 订阅每周 AI 新闻、技巧和有关调优、大型语言模型运行和服务的教程…
- en: kaitchup.substack.com](https://kaitchup.substack.com/?source=post_page-----d61905d126aa--------------------------------)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: kaitchup.substack.com](https://kaitchup.substack.com/?source=post_page-----d61905d126aa--------------------------------)
