# 揭示Word2Vec的开创之旅及人工智能科学的现状

> 原文：[https://towardsdatascience.com/uncovering-the-pioneering-journey-of-word2vec-and-the-state-of-ai-science-an-in-depth-interview-fbca93d8f4ff?source=collection_archive---------7-----------------------#2023-02-03](https://towardsdatascience.com/uncovering-the-pioneering-journey-of-word2vec-and-the-state-of-ai-science-an-in-depth-interview-fbca93d8f4ff?source=collection_archive---------7-----------------------#2023-02-03)

![](../Images/d6208a43e3d289661fd36719745b6683.png)

图片由[Finding Dan | Dan Grinwis](https://unsplash.com/@finding_dan?utm_source=medium&utm_medium=referral)拍摄，发布在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

## 与Dr. Tomas Mikolov的深入访谈

[](https://emilrijcken.medium.com/?source=post_page-----fbca93d8f4ff--------------------------------)[![Emil Rijcken](../Images/d79e867934f45729e6590a20dcf0a440.png)](https://emilrijcken.medium.com/?source=post_page-----fbca93d8f4ff--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fbca93d8f4ff--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fbca93d8f4ff--------------------------------) [Emil Rijcken](https://emilrijcken.medium.com/?source=post_page-----fbca93d8f4ff--------------------------------)

·

[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F95ae6f4e7791&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcovering-the-pioneering-journey-of-word2vec-and-the-state-of-ai-science-an-in-depth-interview-fbca93d8f4ff&user=Emil+Rijcken&userId=95ae6f4e7791&source=post_page-95ae6f4e7791----fbca93d8f4ff---------------------post_header-----------) 发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fbca93d8f4ff--------------------------------) ·19 min read·2023年2月3日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffbca93d8f4ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcovering-the-pioneering-journey-of-word2vec-and-the-state-of-ai-science-an-in-depth-interview-fbca93d8f4ff&user=Emil+Rijcken&userId=95ae6f4e7791&source=-----fbca93d8f4ff---------------------clap_footer-----------)

--

[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffbca93d8f4ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcovering-the-pioneering-journey-of-word2vec-and-the-state-of-ai-science-an-in-depth-interview-fbca93d8f4ff&source=-----fbca93d8f4ff---------------------bookmark_footer-----------)

*2012 年，托马斯·米科洛夫博士在捷克共和国的布尔诺技术大学获得了人工智能博士学位，论文题为《基于神经网络的统计语言模型》。在谷歌研究部门工作一年后，他发表了两篇极具影响力的论文，介绍了连续词袋模型（CBOW）和跳字模型，也称为 Word2Vec。因此，单词可以在一个稠密的连续空间中用数字表示，遵循简单的训练过程。这是最早有效捕捉单词语义的数值方法之一，并允许处理更大的词汇表。许多最先进的自然语言处理任务使用这种技术取得了超越性的成果，而 Word2Vec 的继承者仍在如今被认为是最先进的语言模型中扮演重要角色。米科洛夫博士认为复杂系统可能是通向智能语言模型的下一步。然而，要实现这样的智能语言模型，科学范式需要改变，以创建一个平等的竞争环境并允许新颖性。*

*在他为其博士论文辩护后的十年里，他的研究成果被引用超过 125,000 次，h-指数为 49，i-10 指数为 85，依据 Google Scholar 的数据。2014 年，他移居 Facebook，随后在 2020 年返回捷克共和国。他在捷克信息学、机器人学和网络安全研究所组建了团队，开发一个系统，该系统有望逐渐演变为强人工智能。*

**你的 Word2Vec 算法在自然语言处理领域是革命性的。你能描述一下促成你工作的那些出版物吗？**

一个非常有影响力的研究小组，由心理学家**大卫·鲁梅哈特**领导，早在80年代就开始研究类似的概念。**鲁梅哈特**的学生之一是**杰夫·辛顿**，他因在神经网络方面的工作而闻名。在80年代，他们已经使用神经网络和分布式表示来表示单词，并展示了有趣的特性。在90年代，**杰夫·艾尔曼**使用递归神经网络来建模语言。他使用了由简单的手工编写的语法生成的人工数据。因此，他的工作有许多简化，并不像今天那样复杂；甚至与我们当前的最先进水平相去甚远。但这是一个非常有启发性和前瞻性的方法来表示语言。1991年的一篇非常有影响力的出版物《*Finding structure in time*》讨论了在连接主义模型中表示时间的方法。这对我作为学生在工作语言模型时很有启发性。**约书亚·本吉奥**在2002年左右发表了一篇有影响力的神经语言建模论文，他在小数据集上超越了标准语言建模基准。后来，我和**约书亚**发表了几篇论文，并在他的团队中待了半年。最后，我发现第一个使用神经网络进行通用序列预测并在具有挑战性的基准上取得最先进性能的人是**马特·马洪**——他的PAQ算法基本上是用于数据压缩的神经语言模型，表现惊人。

**谁对你影响最大？**

对我影响最大的人最初是**马特·马洪**，后来是**霍尔格·施温克**；我发现他的论文比**约书亚**的更易读。它包含了可以快速实现的方法，而不是使用不必要的复杂方法。因此，我尝试自己做一些类似的事情。当我在2006年开始我的硕士论文时，我实现的第一个模型是递归神经语言模型。那时我对这种我刚刚发明的递归网络想法感到非常兴奋，但一开始效果不好——虽然比n-gram模型好，但不如简单的前馈神经网络。当时，让这种模型正常工作非常具有挑战性，因为我们不知道如何处理梯度爆炸和消失。在80年代和90年代，“社区”对递归网络中的学习记忆非常感兴趣。然而，没人知道随机梯度下降是否有效，一些论文声称它无效。此外，尽管人们在小数据集上取得了有限的成功，但没有人能成功地在大数据集上训练递归网络，至少没有牺牲大部分性能。现在，我们知道它们可以这样做，这些过去的故事很难理解。

对我来说，这是一个令人兴奋的故事。我在2007年夏天想到从神经语言模型生成文本的想法，并将其与n-gram模型生成的文本进行比较（灵感来自SRILM工具包）。流畅度的提高非常显著，我立即知道这就是未来。看到这些结果的同时知道我是第一个看到这些结果的人，感觉非常酷——就像发现了一个充满奇怪动物的未知岛屿一样。

当我开始研究RNN时，我不知道梯度消失和梯度爆炸的问题。经过一段时间，我成功地让RNN在小数据集上表现得非常好。这本身就是一个挑战——评估各种语言模型并进行比较，因为当时所有发布的模型通常都在私人数据上进行评估。此外，代码也没有发布。幸运的是，在2010年我在约翰霍普金斯大学的Fred Jelinek研究组实习期间，我设法获得了一个数据集。经过一些小的调整，我将其发布在我的网站上，这就是现在非常著名的Penn Treebank语言建模基准的由来。它与树库完全无关——它只是我用来比较不同语言建模技术的，同时与JHU研究人员之前发布的结果兼容。

我还在2010年发布了我的RNNLM代码，包括文本生成部分，以便其他研究人员可以轻松地复制我的结果。这是至关重要的：我获得的相对于n-grams的改进非常显著，当时几乎没有人相信我的结果是正确的。

然而，随着数据集大小的增加，我的递归网络未能收敛的可能性也在增加。这种在大数据集上混乱的行为是不可预测的。虽然大约90%的在Penn Treebank上训练的模型能够收敛到良好的性能，但在更大的数据集上，这个比例降到了10%左右。由于RNN从头实现很困难，我认为我的代码中一定有错误。我认为我只是计算梯度时出错了，或者遇到了一些数值问题。

我找了几天的错误。最终，我找到了熵激增的地方，并且情况变得更糟。一些梯度变得非常大，覆盖了模型的权重，导致训练出现问题。

**你做了什么来解决这个问题？**

我的解决方案很粗糙。我将梯度值截断到一个阈值以上。任何数学家看到这个技巧都会觉得很糟糕。不过，主要问题是梯度很少爆炸，因此任何防止爆炸的方法都是一个足够好的解决方案。这种启发式方法有效地使递归神经语言模型能够扩展到更大的数据集。如今，调试代码要容易得多，因为你知道标准模型在标准数据集上期望的结果。但在我的时代，这情况不同。我获得了新的最先进结果，却不知道还可以走多远。这很令人兴奋；我是在攀登一座无人到达过的山峰，而我不知道它有多高。最终，我在宾夕法尼亚树库上的困惑度达到了大约70，大约是n-grams的一半。这一结果保持了相当多年的最先进水平。虽然在这里我可以抱怨，语言建模结果在2014年左右被错误报告：随着dropouts的发明，研究者们开始专注于用单一模型实现最佳结果。但随后我的所有结果都被丢弃了，这些结果是模型集成的。然而，dropout技术本质上是一种伪装的集成。

> 许多人将深度学习的流行上升归因于计算能力的提高和大数据集。但这并不是全部故事。真正让它开始有效的是我们弄清楚了如何正确使用这些算法。

经过这一经验，我发现了深度学习叙事中的另一个不准确之处。在2014-2016年，深度学习的流行度猛增，出现了关于为什么此时而非之前出现这种热潮的解释。许多人将这一流行的上升归因于计算能力的提高和大数据集。但这并不是全部故事。真正让它开始有效的是我们*弄清楚了如何正确使用这些算法。例如，你可以拿我的RNNLM代码，在90年代的硬件和数据集上运行——你会得到远远超过当时技术的最先进结果。

显然，拥有更多的计算能力永远不会有害，这对行业的采用至关重要。然而，研究界对这些算法的正确使用才是决定其受欢迎程度的关键；增加的计算能力是次要的。我还认为开源和整体可重复性也是非常重要的因素。深度‘ ‘学习的历史比许多人现在认为的要丰富得多。

**当然，这不仅仅是我；亚历克斯·克里热夫斯基让卷积神经网络（CNNs）在图像分类中发挥了作用，乔治·达尔、阿卜杜勒-拉赫曼·穆罕默德和其他人则弄清楚了如何利用深度神经网络进行语音识别，我们这一代的许多博士生也做出了贡献。**

**你在博士期间是否已经考虑过以不同方式表示词汇？**

确实，当我在谷歌工作时，我并没有想出Word2Vec；我在那之前已经做过类似的工作。我做的第一件事，与Word2Vec类似，是在2006年的硕士论文中完成的。当时我对神经网络了解不多。我看到了一篇Yoshua Bengio的论文，它使用了一个投影和一个隐藏层。我不知道如何处理具有多个隐藏层的神经网络，所以我决定把模型分成两部分。第一部分就像Word2vec一样——它从训练集中学习单词表示。第二个网络则使用这些拼接的表示作为输入来表示上下文并预测下一个单词。两个网络都只有一个隐藏层，结果相当不错——与Yoshua的论文相似。

在我的博士期间，我在一次国际会议上发表的第一篇论文就是关于这个模型的。虽然它并不是特别令人印象深刻，但我知道可以通过相当简单的模型来学习好的词向量。后来，我看到几篇论文使用了更复杂的神经网络架构来学习词向量。这在我看来相当愚蠢——人们会训练一个完整的神经语言模型，然后把它扔掉，只保留第一个权重矩阵。但对这个研究领域感兴趣的社区非常小，我认为在这个话题上没有发表任何东西的必要。后来，当我完成博士学业时，我在微软研究院实习，与Geoff Zweig合作。他是一个了不起的导师，但有时他会对神经网络是否是语言建模的未来表示怀疑——所以我在考虑如何让他印象深刻。

**你做了什么来说服他？**

这是一个有趣的故事。我进行了些计算，并在接触他之前仔细检查了结果。然后，我问他是否可以对词向量应用简单的加法和减法。我问他在从‘king’中减去‘man’并添加‘woman’之后，最接近的向量是什么（除了输入词，否则你经常会回到你开始的地方）。

他告诉我这是个相当愚蠢的想法，认为这样没有任何意义。因此，我立即把他带到我的电脑前，展示了实验结果——它返回了‘*queen*’。他非常惊讶，开始尝试各种操作。他尝试了动词的过去时和复数形式等等。有些想法有效，有些则无效。但这比随机猜测要好得多。第一次看到这些类比非常令人着迷。这引发了基本的问题。为什么会出现这些规律？为什么这是完全线性的？为什么不乘以向量，而是相加和相减？

**你的谷歌同事也像你的导师一样持怀疑态度吗？**

我不会称Geoff Zweig为怀疑，但可以说他非常谨慎。他实际上非常支持，很容易说服他相信某些想法值得追求。我在职业生涯初期遇到过更多麻烦。当我开始研究神经语言模型时，我收到了来自布尔诺理工大学一位当地语言学家的极其负面的评价。他甚至说，使用神经网络来建模语言的整个想法完全是胡扯，而且我的结果一定是假的。他差点儿让我被踢出博士项目。

当我加入Google Brain时，一些同事已经在尝试学习词语表示。然而，他们试图训练大型语言模型以获得词向量。在大型语言模型中，99.9%的训练时间，你在更新与词向量无关的参数。从2006年我的硕士论文中，我知道如果最终任务不是语言建模，这样的大型语言模型是不必要的。相反，使用更简单的模型来计算词向量就足够了。

我将这一见解与一些同事分享了。然而，没有人真正听取。一些人跟随的是一篇斯坦福论文，这篇论文复杂且包含许多不必要的内容。刚刚开始在Google Brain工作时，我的第一个目标是展示如何高效地解决这个问题。我开始尝试，很快就取得了成功。使用普通的台式电脑，我可以在几个小时内训练使用数亿个单词的模型。我的模型击败了一个在许多机器上训练了几周的Google内部模型。

**那时发生了什么？**

Yoshua刚刚组织了一个新的会议，ICLR，并问我是否可以提交一篇关于词语类比的论文，因为那时这是一个相当令人惊讶的结果。他认为这会是一篇很酷的论文。他在12月中旬联系了我；截止日期是在1月初。所以我在加州的圣诞假期中写了Word2Vec论文。论文写得不是很好，但我更关心的是实现和结果，而不是论文。在同事的支持下，我向ICLR提交了论文。但不幸的是，评论非常负面（这是一个公开评审，因此应该仍然可以访问）。一位评审抱怨模型没有考虑词序。另一位评审试图强迫我更多地引用其他论文，而这些论文我已经引用过，并且是在我的硕士论文（其中已经包含了主要想法）之后发表的。

> ICLR 2013的接受率约为70%。但Word2Vec论文被拒绝了。今天，它可能被引用的次数比ICLR 2013上所有接受的论文加起来还要多。

这里有一个有趣的细节。虽然现在是一个著名的会议，但这是ICLR的第一届，规模很小。接受率约为70%，所以几乎所有不是完全糟糕的论文都会被接受。但Word2Vec论文被拒绝了，尽管今天它可能比ICLR 2013上所有接受的论文加起来的引用次数还要多。于是，我决定写另一篇扩展版的论文。这篇论文最终被接受到NIPS。

**你从未在其他地方发表过你的第一篇论文，对吧？**

第一篇论文在被ICLR会议拒绝后被接受到一个研讨会。但我不认为研讨会算作发表。此外，它被发布在Arxiv上，我很高兴人们可以阅读。当我发布它时，我知道它比目前可用的要好——至少在我关心的方面。算法并不复杂，实际上提供了非常好的结果。

**你是否预料到这篇论文会被如此广泛引用？**

神经语言建模社区在我发布这篇论文时还很小。然而，我非常乐观，预期至少会有五十个人在一年内使用它。论文发布六个月后，它仍然未被注意。这是因为谷歌没有批准我开源代码。最初，他们认为代码是竞争优势。然而，我一直在推动开源。周围的前辈告诉我停止尝试，因为我永远无法获得批准。幸运的是，我认识谷歌脑的高层，他们成功绕过了阻碍。最后，谷歌在2013年8月左右批准了开源代码。这也是代码有些过度优化的原因：在等待批准的过程中，我对代码进行了调整，使其更短更快。代码开源后，兴趣激增。许多人对谷歌的机器学习活动感兴趣，并喜欢谷歌开源代码。这帮助极大。我确实很惊讶有这么多人开始使用这段代码和预训练模型，甚至在一些情况下超出了建模词汇和语言的范围。

**你为什么倡导开源？**

作为学生，我发现很难比较不同算法，因为这通常是不可能的。十五年前，发布在私有数据集上评估的语言建模论文而没有任何开源实现是很正常的。在我看来，这就是语言建模研究在过去几十年中没有取得太大进展的主要原因。我曾联系过几位研究人员，询问他们的数据集，但都没有成功。到了某个阶段，没有人能验证已发表的结果，社区也陷入了停滞。我发现某些人甚至在报告结果时作弊，例如，使用弱基线或在测试集上调整超参数后报告最佳结果（甚至在测试集上训练模型，这虽然罕见但并非闻所未闻）。我受到了Matt Mahoney在数据压缩社区工作的启发，想要重建我对统计语言建模的兴趣，因此我希望在可能的情况下发布我的代码和数据。当然，一个重要方面是，当我开始发布我的大规模神经语言模型结果时，我的改进幅度之大，以至于几乎整个研究社区都不相信我的结果可能是正确的。但由于没有人能在我的代码中找到任何错误（许多人尝试过——我收到过很多邮件，表示他们终于找到了我代码中的“bug”），我的RNNLM工具包被几家大公司使用，语言建模研究终于起飞。这就是自然语言处理领域深度学习的开始。

**开源有缺点吗？**

我认为有。当新的学生加入人工智能社区时，他们应该尝试开发自己的模型并发现新想法。然而，这非常困难，因为他们最终要与多年由许多研究人员逐步优化的最先进模型竞争。

另一种情况是，学生可以下载别人的代码甚至预训练模型，这些通常很复杂，他们可能并未完全理解。然后他们对其进行调整，做出增量变化，并在论文中发布结果。这种方法要容易得多。然而，这对科学来说是一个危险的发展，因为它将我们锁定在局部最优解中。几个主流观点被过度探索，而很少有人思考可以带来新范式转变的新方法。开源和“发布或死亡”共同促成了一个环境，在这里冒险没有回报。

> “拥有最多GPU的团队相对于其他团队有很大优势。这使得学术界的人们感到沮丧，并创造了不公平的竞争。对某些基准测试轨道上的已发表论文施加计算限制将是一个简单的解决方案。”

**所以开源代码有好处。同时，不利影响也很明显。是否存在中间的‘最佳’方法？**

鉴于计算能力的重要性，拥有最多GPU的团队相较于其他人具有显著优势。这使得学术界的人们受到挫折，并且造成了不公平的竞争；并不是每个人的起点条件都相同。这就好比你去参加奥运会跑步比赛，但比赛时你却是在与骑自行车的人竞争。不论你多么优秀，你都会输。学生们在资源有限的情况下与科技巨头竞争时也会遇到同样的问题。他们可能有更好的想法，但仍会因为不够前沿而被拒绝。这一问题需要社区来解决。

解决这个问题的一个简单方法是对某些基准测试中的论文发布应用计算限制。按照这种方法，论文应当与能够在*X*小时内在标准化机器上进行训练的代码一起提交。不过，人们仍可以详尽地探索搜索空间，并提交具有最佳超参数的代码，因此拥有更多计算能力的人仍会占有优势。但至少这样竞争会公平些。顺便说一下，当Matt Mahoney提出压缩挑战时，他已经考虑到了这一点。

> “许多人认为好的模型看起来复杂且充满了超参数和微调。简单的想法常常被认为不值得发表，因为任何人都可以做到。我认为这种心态完全是愚蠢的。”

**机器学习社区还有哪些其他问题？**

随着AI社区每隔几年就翻倍增长，主导科学家容易左右初级研究者的思维。然而，那些发大量推文和Facebook帖子，对所有事情都有强烈意见的主导科学家，并不总是那些做出强大贡献的人。一个由少数主导的资深科学家领导大量初级研究者的社区看起来就像某种邪教。这意味着一些想法、技术或模型被盲目推动，没有真实证据表明这些想法值得付出所有努力。例如，生成对抗网络（GANs）看起来被过度炒作了。这不是新现象。当我还是学生时，我记得对Latent Dirichlet分配的受欢迎程度感到困惑——它似乎也不比简单的基线方法更有效。但如今我认为这是一个更大的问题，因为信息传播得更快。

对通过蛮力取得的结果的过度强调体现了这个问题。许多人认为好的模型是看起来复杂且充满超参数的小调整。如果提出一个有效的简单想法，审稿人通常会争辩说任何人都可以做到，因此不值得发表。我已经见过这种情况几次，并且认为这完全愚蠢。实际上，我相信相反的观点：在实践中有效的简单想法是最有价值且最难发现的。就像物理学中，科学家们试图发展越来越通用的理论来解释尽可能多的现象一样。

实际上，这种情况发生在 Word2Vec 上，也发生在我一些语言建模工作上。当一个差劲的审稿人看到两篇有类似想法的论文，但其中一篇还添加了十几个不必要的改动时，这个差劲的审稿人会选择复杂的论文作为更好的那一篇，因为看起来投入的工作更多。实际上，情况往往正好相反——如果你能用一个简单的想法获得最先进的结果，那么这个想法可能真的非常好。

**我们如何才能获得更好的审稿人？**

机器学习可以从物理学中获得启发。在几个世纪的研究中，物理学家们旨在创建简单的理论以解释一切。与此同时，在机器学习领域则正好相反。我们应该放弃对最先进结果和复杂模型的强调，专注于发现有趣的新想法。当然，这高度主观，如果我们能将机器学习变成一个具有明确规则的奥林匹克项目来决定谁更优秀，那将更好。但正如我之前提到的，我认为这并不容易实现。今天，你可以提出一个惊人的新想法，可能成为下一个最先进的成果，但仍然会因为在某些大型基准上不够最先进而受到社区的打击和拒绝。博士生没有足够的时间来发展自己的方法和思路。我们应该改变这种情况，开始奖励新颖性和简洁性，即使这很难衡量。

或许你听说过 NIPS 的审稿实验。更多的审稿小组对论文进行评审，以查看接受/拒绝决定之间的相关性。结果发现，只有对非常差的论文才有很强的相关性。换句话说，审稿系统是非常随机的。

我们应该致力于创建一个更好的审稿系统。目前，我们在审稿系统中没有质量反馈；系统允许审稿人持续犯错，并且仍然能够审阅更多的论文。我们应该有审稿人数据库，自动跟踪他们的表现。他们的质量应该根据预测成功论文的能力来计算。例如，拥有优秀想法但英语较差的论文应该被接受。

**在IEEE SMC大会的全体报告中，你提到将复杂系统作为人工智能的下一步发展方向。这是一种优雅地简化计算机科学规则的方法吗？**

复杂系统是简单系统中通过你未指定的涌现/进化机制产生的复杂性。以《生命游戏》为例。你从简单的东西开始，然后模拟系统直到各种复杂的结构出现。这一直是我对宇宙的看法。我们周围的许多事物看起来很复杂。然而，这些复杂性可以被视为进化的副产品。自然智能是进化的产物。如果我们想通过人工智能来模拟这一点，我们应该采用类似的方法——允许人工智能进化，并有潜力自发地增加其复杂性。

**这与进化算法相比如何？**

可以使用进化算法来接近这一点。然而，我认为这些算法并没有很好地捕捉进化。它们进行随机优化。如果适应度函数有所改进，那么你就沿着这个随机方向前进。因此，梯度是随机选择的，而不是计算得出的。但在我看来，这不是进化——毕竟，进化算法往往很快陷入停滞。真实的进化可以在复杂系统中找到，即使是确定性的系统也是如此。《生命游戏》中没有任何随机性；你不需要掷骰子。即便如此，你仍然可以看到新颖的模式出现。我的目标是创建能够自发进化的系统，基于复杂性的涌现。我觉得发现能够在复杂性上隐式增长的机器学习模型具有使我们的AI模型更强大的潜力。这可能是让机器学习真正具有创造性的一种方式。

**你将如何创建这样的系统？**

我怀疑理解涌现现象是解决AI问题所必需的。然而，我们对这一方向的理解还不够深入。当我开始研究递归神经网络时，我希望这些能够成为通向有趣的复杂系统的捷径，其中涌现发生在模型的记忆中。但典型的递归网络架构具有一定的记忆容量限制。我们需要设计新颖的机器学习模型、训练算法和评估指标。我正与我的学生一起致力于这个工作。

**这将如何为机器学习社区做出贡献？**

社区已经体现出了一种群体文化。我们都朝着同一个方向前进，建立在现有的基础上。这种心态可能因为我强烈倡导的开源和公共基准测试而得到了强化。然而，我们所扩展的方法可能是错误的。如果是这样的话，每个人都在建立在有缺陷的假设之上。如果是这样的话，就需要修正。正如我提到的，我们应该探索不同的想法，并在研究社区中奖励新颖性。

**这听起来像是一个不再开源的理由。**

在我看来，开源是很棒的，我们应该继续这样做。请记住，当几乎没有人发布代码和数据集都是私有的时，研究人员通常不会互相信任对方的结果。语言建模社区几乎已经死去。

与此同时，我们应该避免开源的危险：过多的增量工作、提供微小改进的细微调整（有时仅仅如此），以及对探索新想法的气馁。

**这标志着采访的结束，您还有什么最后的评论吗？**

我们应该对原创性和新方向更加开放。然而，这很难判断。我们是否希望在会议上看到看似疯狂的想法？作为一个社区，我们需要让会议变得更加有趣，而不仅仅是看到数百种Transformers的修改或它们在数百个数据集上的应用。让我们更有雄心，更具探索性。

*本次采访由* [*BNVKI*](http://www.bnvki.org/)*，即贝尔赫斯人工智能协会，进行。我们汇聚了来自比利时、荷兰和卢森堡的人工智能研究人员。*
