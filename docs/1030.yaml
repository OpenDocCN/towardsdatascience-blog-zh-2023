- en: Please Stop Drawing Neural Networks Wrong
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/please-stop-drawing-neural-networks-wrong-ffd02b67ad77?source=collection_archive---------0-----------------------#2023-03-21](https://towardsdatascience.com/please-stop-drawing-neural-networks-wrong-ffd02b67ad77?source=collection_archive---------0-----------------------#2023-03-21)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The case for GOOD diagrams
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@amaster_37400?source=post_page-----ffd02b67ad77--------------------------------)[![Aaron
    Master](../Images/f2f04fa2937d5a05b29c433bf336b3b0.png)](https://medium.com/@amaster_37400?source=post_page-----ffd02b67ad77--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ffd02b67ad77--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ffd02b67ad77--------------------------------)
    [Aaron Master](https://medium.com/@amaster_37400?source=post_page-----ffd02b67ad77--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F31905cfe67ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplease-stop-drawing-neural-networks-wrong-ffd02b67ad77&user=Aaron+Master&userId=31905cfe67ce&source=post_page-31905cfe67ce----ffd02b67ad77---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ffd02b67ad77--------------------------------)
    ·12 min read·Mar 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fffd02b67ad77&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplease-stop-drawing-neural-networks-wrong-ffd02b67ad77&user=Aaron+Master&userId=31905cfe67ce&source=-----ffd02b67ad77---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fffd02b67ad77&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplease-stop-drawing-neural-networks-wrong-ffd02b67ad77&source=-----ffd02b67ad77---------------------bookmark_footer-----------)![](../Images/bf6a18f456c9c142f1789a3e38ccc8d0.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image by the authors, adapted from [https://tikz.net/neural_networks/](https://tikz.net/neural_networks/)
    (CC BY-SA 4.0)
  prefs: []
  type: TYPE_NORMAL
- en: '*By Aaron Master and Doron Bergman*'
  prefs: []
  type: TYPE_NORMAL
- en: If you’re one of the millions of people who has tried to learn neural networks,
    odds are you’ve seen something like the above image.
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s just one problem with this diagram: it’s nonsense.'
  prefs: []
  type: TYPE_NORMAL
- en: By which we mean confusing, incomplete, and probably wrong. The diagram, inspired
    by one in a famous online Deep Learning course, excludes all of the bias coefficients
    and shows data as if it were a function or node. It “probably” shows the inputs
    incorrectly. We say probably, because even after one of us earned certificates
    for completing courses this kind of diagram is used in, it’s more or less impossible
    to determine what it’s trying to show.¹
  prefs: []
  type: TYPE_NORMAL
- en: 'Other neural network diagrams are bad in different ways. Here’s an illustration
    inspired by one in a TensorFlow course from a certain Mountain View — based advertising
    company:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6ceaceee2542a860499bae0ec0dd9596.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the authors, adapted from the previous image.
  prefs: []
  type: TYPE_NORMAL
- en: 'This one shows the inputs more clearly than the first one, which is an improvement.
    But it does other weird stuff: it shows the bias by name but doesn’t diagram it
    visually, and also shows quantities out of the order in which they are used or
    created.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Why?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s hard to guess how these diagrams came to be. The first one looks superficially
    similar to [flow network](https://en.wikipedia.org/wiki/Flow_network) diagrams
    used in graph theory. But it violates a core rule of such flow diagrams, which
    is that the amount of flow into a node equals the amount of flow out of it (with
    exceptions that don’t apply here). The second diagram looks like maybe it started
    as the first one, but then wound up being edited to show both parameters *and*
    data, which then wound up in the wrong order.² Neither of these diagrams shows
    the bias visually (and neither do most others we’ve seen) but this choice doesn’t
    save much space, as we will see below.
  prefs: []
  type: TYPE_NORMAL
- en: We did not cherry pick these diagrams. An internet image search for “neural
    network diagrams” reveals that those above are the norm, not the exception.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bad diagrams are bad for students**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The diagrams above would probably be fine if they were being used only among
    seasoned professionals. But alas, they are being deployed for pedagogical purposes
    on hapless students of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Learners encountering such weirdness must make written or mental notes such
    as “there is bias here, but they aren’t showing it,” or “the thing they are drawing
    *inside* a circle is actually the *output* of *processing* shown inside the same
    circle two slides ago” or “the inputs don’t actually work they way they are drawn.”
    The famous (and generally excellent) course mentioned above features lectures
    where the instructor patiently repeats several times that a given network doesn’t
    actually work the way a diagram shows it working. In the third week of the course,
    he valiantly tries to split the difference, alternating between special, accurate
    depictions which show what happens inside a node, and more typical diagrams which
    show something else. (If you want to see those better node depictions, this [blog
    post](/the-concept-of-artificial-neurons-perceptrons-in-neural-networks-fab22249cbfc)
    nicely shows them.)
  prefs: []
  type: TYPE_NORMAL
- en: '**A better way**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Learning neural networks should not be an exercise in decoding misleading
    diagrams.* We propose a constructive, novel approach for teaching and learning
    neural networks: use good diagrams. We want diagrams that succinctly and faithfully
    represent the math — as seen in Feynman diagrams, Venn diagrams, digital filter
    diagrams, and circuit diagrams.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Let’s make GOOD diagrams**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, what exactly do we propose? Let’s start with basics. Neural networks involve
    many simple operations which *already* have representations in flow diagrams that
    electrical engineers have used for decades. Specifically, we can depict copying
    data, multiplying data, adding data, and inputting data to a function which outputs
    data. We can then assemble abbreviated versions of these symbols into an accurate
    whole, which we will call Generally Objective Observable Depiction diagrams, or
    GOOD diagrams for short. (Sorry, backronym haters.)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the building blocks. To start, here’s how you show a total of
    three copies of data coming from a single source of data. It’s pretty intuitive.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dd669c4a4fe15dfc0bbde03f1ddd5958.png)'
  prefs: []
  type: TYPE_IMG
- en: And here is a way to show scaling an input. It’s just a triangle.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f6e5e33bb539312a566b338640b07afa.png)'
  prefs: []
  type: TYPE_IMG
- en: The triangle indicates that the input value *x*₁ going into it is scaled by
    some number *w*₁, to produce a result *w*₁times *x*₁. For example *w*₁could be
    0.5 or 1.2\. Later on it will be easier if we move this triangle to the right
    end of the diagram (merging it with the arrow) and make it pretty small, so let’s
    draw it that way.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/480e3a0af518525cf51d860eb429e936.png)'
  prefs: []
  type: TYPE_IMG
- en: 'OK, we admit it: this is just an arrow with a solid triangle tip. The point,
    as it were, is that the triangle tip multiplies the data on the arrow.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, here’s a way to show adding two or more things together. Let’s call the
    sum *z*. Also simple.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f49ec5101595caf1f90d1f5da7defb5d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we showed addition and multiplication above with some standard symbols.
    But what if we have a more general function that takes an input and produces an
    output? More specifically, when we’re making neural nets, we will use an *activation
    function* that is often a Sigmoid or ReLU. Either way, it’s no problem to diagram;
    we just show this as a box. For example, say the input to our function is called
    *z* and the function of *z* is called *g(z)* and produces an output *a*. It looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ead3416d2a3f9a0ad35d369db36383c7.png)'
  prefs: []
  type: TYPE_IMG
- en: Optionally, we can note that *g(z)* has a given input — output characteristic,
    which can be placed near the function box. Here’s a diagram including a *g(z)*
    plot for ReLU, along with the function box. In practice, there are only a few
    commonly used activation functions, so it would also be sufficient to note the
    function name (e.g. ReLU) somewhere near the layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a396d0a24bc4ad8fb184c8bb7da878c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Or, we can abbreviate even more, since there will be many, often identical,
    activation functions in a typical neural network. We propose using a single stylized
    script letter inside the function box for a specific activation, e.g. *R* for
    ReLU:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aad0af6c07e6f7a7953a35562d3c119e.png)'
  prefs: []
  type: TYPE_IMG
- en: Similarly, Sigmoid could be represented with a stylized *S* and other functions
    with a another specified letter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before moving on, let’s make note of a simple but key fact: we show *data*
    (and its direction of travel) as **arrows**, and we show *operations* (multiplying,
    adding, general functions) as **shapes** (triangle, circle, square). *This is
    standard in electrical engineering flow diagrams.* But for some reason, perhaps
    inspiration from early computer science research which physically colocated memory
    and operations, this convention is ignored or even reversed when drawing neural
    networks.³ Nonetheless, the distinction matters because we *do* train the function
    parameters, but we *do not* train the data, each instance of which is immutable.'
  prefs: []
  type: TYPE_NORMAL
- en: OK, back to our story. Since we will soon be constructing a neural network diagram,
    it will need to depict a lot of “summing then function” processing. To make our
    diagrams more compact, we will make an abbreviated symbol that combines these
    two functions. To start, let’s draw them together, again assuming a ReLU for *g(z)*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7f066771006af1fd5b397ca2748206ba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since we are about to abbreviate things, let’s see how they look when placed
    really close together. We will also drop the internal variable and function symbols
    from the plot, and add some dotted lines to hint at a proposed shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2c209d869462a6c9d497dfb8b75ea53a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Based on this, let’s introduce a new summary symbol for “sum then function”:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3fe28efe9d28b920c643cf20340af6a1.png)'
  prefs: []
  type: TYPE_IMG
- en: Its special shape serves as a reminder of what it is doing. It also looks different
    than other symbols on purpose, to help us remember that it is special.⁴
  prefs: []
  type: TYPE_NORMAL
- en: '**A GOOD thing**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let’s put all the diagrammed operations above together using a simple example
    of *logistic regression*. Our example starts with a two-dimensional input, multiplies
    each input dimension’s value by a unique constant, adds together the result along
    with a constant *b* (which we call *bias*), and passes the sum through a Sigmoid
    activation function. For reasons that will make sense later, we show the bias
    as the number 1 times the value *b*. For completeness (and foreshadowing) we give
    all these values names which we can show on the diagram. The inputs are *x*₁ and
    *x*₂, and the multiplication factors include the weights *w*₁ and *w*₂ as well
    as the bias *b*. The sum of the weighted inputs and bias is *z*, and the output
    of function *g(z)* is a.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b3e5d8aa58e7669eec82a9255535d1c.png)'
  prefs: []
  type: TYPE_IMG
- en: About that number “1” shown lower left on the diagram. The number 1 is not an
    input, but by showing this number in addition to the inputs, we clarify that each
    of these values is multiplied by a parameter contributing to the sum. This way
    we can show both values of *w* (input weights) and values of *b* (bias) on the
    same diagram. Bad diagrams usually skip showing the bias but GOOD ones don’t.
    Skipping bias in a diagram is especially risky in situations where a network might
    sometimes *intentionally omit* the bias; if the bias is not shown, a viewer is
    left to *guess* whether it is part of the network or not. So please deliberately
    include or exclude bias in your diagrams.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s clean this up a bit by using the “sum then function” symbol we defined
    above. We also show the variable names below the diagram. Note that we indicate
    a Sigmoid function with the stylized script letter *S* in the “sum then function”
    symbol.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b73bac3067c9fdf3f2b12132be0b67b2.png)'
  prefs: []
  type: TYPE_IMG
- en: That looks pretty simple. It is a GOOD thing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s build something a little more interesting: an actual neural network
    (NN) with a hidden layer of three units with ReLU activations, and one output
    layer with a Sigmoid activation. (If you’re not familiar, a hidden layer is any
    layer except the input or the output.) Note that this is the same architecture
    used in the Mountain View network diagram above. In this case, each input dimension,
    and the input layer bias, connects to every node in the hidden layer, then the
    hidden layer outputs (plus bias value again) connect to the output node. The output
    of each function is still called *a* but we use bracketed superscripts and subscripts
    to respectively denote which layer and node we are outputting from. Similarly,
    we use bracketed superscripts to indicate the layer to which the *w* and *b* values
    point. Using the style from the previous example, it looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/003654464a98a9be9d4f0c143efd4e19.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we are getting somewhere. At this point, we also see that the dimensions
    of *W* and *b* for each layer are specified by the dimensions of the inputs and
    the number of nodes in each layer. Let’s clean up the above diagram by not labeling
    every *w* and *b* value individually.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9961a3129f2eb4028000eb89ac253293.png)'
  prefs: []
  type: TYPE_IMG
- en: All images in this article by the authors.
  prefs: []
  type: TYPE_NORMAL
- en: Ta-dah! We have a GOOD neural network diagram that is also good. The learnable
    *parameters* are both shown on the diagram (as triangles) and summarized below
    it, while the *data* is shown directly on the diagram as labeled arrows. The architecture
    and activation functions for the network, typically called *hyperparameters,*
    are seen by inspecting the layout and nodes on the diagram itself.
  prefs: []
  type: TYPE_NORMAL
- en: '**It’s all GOOD**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s consider the benefits of GOOD diagrams, independent of bad ones:'
  prefs: []
  type: TYPE_NORMAL
- en: It’s easy to see the order of operations for each layer. The multiplications
    happen first, then the sum, then the activation function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s easy to see (immutable) data flowing through the network as separate from
    (trainable) parameters belonging to the network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s easy to see the dimensionality of the *w* matrix and *b* vector for each
    layer. For a layer with *N* nodes, it’s clear that we need *b* to be of shape
    [*N*,1]. For a layer with *N* nodes coming after a layer with *M* nodes (or inputs),
    it’s clear that *w* is of shape [*N,M*]. (However, one still must memorize that
    the shape is [outputs, inputs] not [inputs, outputs].)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Related, we see *where* exactly the weights and bias exist, which is *between*
    layers. Conventionally they are named as belonging to the layer they *output*
    to but apt students using GOOD diagrams are reminded that this is just a naming
    convention.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s also review how GOOD diagrams differ from bad ones:'
  prefs: []
  type: TYPE_NORMAL
- en: They show the bias at each layer. They do not omit the bias.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They show data as data, and functions as functions. They do not confuse the
    two.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They show when data is copied and sent to functions. They do not skip this step.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They show all the steps in the correct order. They do not incorrectly reorder
    or omit steps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are reasonably clean and concise. OK, the bad ones are a bit more concise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Do some GOOD**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article has spilled a lot of ink covering what’s wrong with bad diagrams
    and justifying GOOD ones. But if you are an ML instructor, we encourage you to
    just start using GOOD diagrams, without fanfare. GOOD diagrams are more self-explanatory
    than other options. You’ll be covering how neural nets work in your course anyway,
    so introducing GOOD diagrams at that point is a good idea.
  prefs: []
  type: TYPE_NORMAL
- en: And of course, as a service to your students, it’s a good idea to show some
    bad diagrams too. It’s important to know how the outside world is drawing things,
    even when it’s nonsense. In our estimation, it’s much easier to learn from something
    accurate and then to consider something confusing, than it is to do the reverse.
  prefs: []
  type: TYPE_NORMAL
- en: '**More GOOD stuff is ahead**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This completes the first article in what will become a series if it catches
    on. In particular, we have our collective eye on *Simplified Network* diagrams
    which compactly represent the kinds of fully connected networks shown above, and
    which could also stand some improvement. Convolutional Network diagrams deserve
    their own treatment. We are also looking into developing a software package which
    automates drawing of GOOD diagrams.
  prefs: []
  type: TYPE_NORMAL
- en: '*Acknowledgements*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The authors thank Jeremy Schiff and Mikiko Bazeley for their assistance with
    this piece.
  prefs: []
  type: TYPE_NORMAL
- en: '*References and endnotes*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 1) Based on the other layers, maybe the first diagram is running the inputs
    into nontrivial activation functions, from which we get values likely *different*
    from the inputs. But there have been no examples in the associated courses that
    work this way, so it wouldn’t make sense to include such a diagram as the *only*
    fully connected diagram on the cheat sheet. Or maybe the first layer “*a*” values
    shown are *identical* to the inputs, in which case the activation functions are
    identity functions which incur trivial and unnecessary processing. Either way,
    the diagram is ambiguous and therefore bad.
  prefs: []
  type: TYPE_NORMAL
- en: 2) Either of the first two diagrams looks like it could be an unfortunate condensation
    of better, older diagrams, such as those in chapter 6 of the second edition of
    Pattern Classification by Duda, Hart and Stork (which one of us still has in hard
    copy from CS229 at Stanford in 2002). That book shows activation functions in
    circular units (which is better than showing their outputs *inside* the units),
    and correctly shows outputs leaving the units before copies are made and split
    off to the next layer. (It also shows the inputs and bias oddly, though.)
  prefs: []
  type: TYPE_NORMAL
- en: 3) If your study has progressed to include Convolutional Nets (CNs), you will
    see that CN diagrams routinely show *data* as blocks and *processes* as annotated
    arrows. Don’t fret. For now, just remember that there’s an essential difference
    between data and processes, and, for fully connected neural nets, a good (or GOOD)
    diagram will make clear which is which.
  prefs: []
  type: TYPE_NORMAL
- en: 4) For you logic fans out there that see the “sum then function” symbol as an
    *and gate* operating in reverse, remember that *and gates* are *irreversible*.
    Therefore, this new symbol must have another meaning, which we define here.
  prefs: []
  type: TYPE_NORMAL
