- en: Please Stop Drawing Neural Networks Wrong
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 请停止错误绘制神经网络
- en: 原文：[https://towardsdatascience.com/please-stop-drawing-neural-networks-wrong-ffd02b67ad77?source=collection_archive---------0-----------------------#2023-03-21](https://towardsdatascience.com/please-stop-drawing-neural-networks-wrong-ffd02b67ad77?source=collection_archive---------0-----------------------#2023-03-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/please-stop-drawing-neural-networks-wrong-ffd02b67ad77?source=collection_archive---------0-----------------------#2023-03-21](https://towardsdatascience.com/please-stop-drawing-neural-networks-wrong-ffd02b67ad77?source=collection_archive---------0-----------------------#2023-03-21)
- en: The case for GOOD diagrams
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优质图表的案例
- en: '[](https://medium.com/@amaster_37400?source=post_page-----ffd02b67ad77--------------------------------)[![Aaron
    Master](../Images/f2f04fa2937d5a05b29c433bf336b3b0.png)](https://medium.com/@amaster_37400?source=post_page-----ffd02b67ad77--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ffd02b67ad77--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ffd02b67ad77--------------------------------)
    [Aaron Master](https://medium.com/@amaster_37400?source=post_page-----ffd02b67ad77--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@amaster_37400?source=post_page-----ffd02b67ad77--------------------------------)[![亚伦·马斯特](../Images/f2f04fa2937d5a05b29c433bf336b3b0.png)](https://medium.com/@amaster_37400?source=post_page-----ffd02b67ad77--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ffd02b67ad77--------------------------------)[![数据科学](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ffd02b67ad77--------------------------------)
    [亚伦·马斯特](https://medium.com/@amaster_37400?source=post_page-----ffd02b67ad77--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F31905cfe67ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplease-stop-drawing-neural-networks-wrong-ffd02b67ad77&user=Aaron+Master&userId=31905cfe67ce&source=post_page-31905cfe67ce----ffd02b67ad77---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ffd02b67ad77--------------------------------)
    ·12 min read·Mar 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fffd02b67ad77&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplease-stop-drawing-neural-networks-wrong-ffd02b67ad77&user=Aaron+Master&userId=31905cfe67ce&source=-----ffd02b67ad77---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F31905cfe67ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplease-stop-drawing-neural-networks-wrong-ffd02b67ad77&user=Aaron+Master&userId=31905cfe67ce&source=post_page-31905cfe67ce----ffd02b67ad77---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ffd02b67ad77--------------------------------)
    ·12 分钟阅读·2023年3月21日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fffd02b67ad77&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplease-stop-drawing-neural-networks-wrong-ffd02b67ad77&user=Aaron+Master&userId=31905cfe67ce&source=-----ffd02b67ad77---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fffd02b67ad77&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplease-stop-drawing-neural-networks-wrong-ffd02b67ad77&source=-----ffd02b67ad77---------------------bookmark_footer-----------)![](../Images/bf6a18f456c9c142f1789a3e38ccc8d0.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fffd02b67ad77&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplease-stop-drawing-neural-networks-wrong-ffd02b67ad77&source=-----ffd02b67ad77---------------------bookmark_footer-----------)![](../Images/bf6a18f456c9c142f1789a3e38ccc8d0.png)'
- en: Image by the authors, adapted from [https://tikz.net/neural_networks/](https://tikz.net/neural_networks/)
    (CC BY-SA 4.0)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供，改编自 [https://tikz.net/neural_networks/](https://tikz.net/neural_networks/)
    (CC BY-SA 4.0)
- en: '*By Aaron Master and Doron Bergman*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*作者：亚伦·马斯特和多龙·伯格曼*'
- en: If you’re one of the millions of people who has tried to learn neural networks,
    odds are you’ve seen something like the above image.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是数百万尝试学习神经网络的人之一，那么你可能见过类似上述的图像。
- en: 'There’s just one problem with this diagram: it’s nonsense.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图表只有一个问题：那就是毫无意义。
- en: By which we mean confusing, incomplete, and probably wrong. The diagram, inspired
    by one in a famous online Deep Learning course, excludes all of the bias coefficients
    and shows data as if it were a function or node. It “probably” shows the inputs
    incorrectly. We say probably, because even after one of us earned certificates
    for completing courses this kind of diagram is used in, it’s more or less impossible
    to determine what it’s trying to show.¹
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着困惑、不完整，且可能是错误的。这个图表受到了某个著名在线深度学习课程中的图表的启发，排除了所有的偏差系数，并将数据展示成好像它是一个函数或节点。它“可能”错误地显示了输入。我们之所以说“可能”，是因为即使其中一位我们获得了完成这类课程的证书，还是很难确定它试图展示的内容。¹
- en: 'Other neural network diagrams are bad in different ways. Here’s an illustration
    inspired by one in a TensorFlow course from a certain Mountain View — based advertising
    company:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 其他神经网络图表有不同的缺陷。以下是一个受某个来自山景城——基于广告公司的TensorFlow课程图示启发的插图：
- en: '![](../Images/6ceaceee2542a860499bae0ec0dd9596.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6ceaceee2542a860499bae0ec0dd9596.png)'
- en: Image by the authors, adapted from the previous image.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供，改编自之前的图片。
- en: 'This one shows the inputs more clearly than the first one, which is an improvement.
    But it does other weird stuff: it shows the bias by name but doesn’t diagram it
    visually, and also shows quantities out of the order in which they are used or
    created.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图比第一个图更清晰地展示了输入，这是一个进步。但它也做了其他奇怪的事情：它按名称展示了偏差，但没有用视觉图示显示它，还将数量以不同于它们使用或创建的顺序展示出来。
- en: '**Why?**'
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**为什么？**'
- en: It’s hard to guess how these diagrams came to be. The first one looks superficially
    similar to [flow network](https://en.wikipedia.org/wiki/Flow_network) diagrams
    used in graph theory. But it violates a core rule of such flow diagrams, which
    is that the amount of flow into a node equals the amount of flow out of it (with
    exceptions that don’t apply here). The second diagram looks like maybe it started
    as the first one, but then wound up being edited to show both parameters *and*
    data, which then wound up in the wrong order.² Neither of these diagrams shows
    the bias visually (and neither do most others we’ve seen) but this choice doesn’t
    save much space, as we will see below.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 很难猜测这些图表是如何形成的。第一个图在表面上看起来类似于[流网络](https://en.wikipedia.org/wiki/Flow_network)图表，这些图表在图论中使用。但它违反了这种流图的核心规则，即流入节点的量等于流出节点的量（这里没有适用的例外）。第二个图看起来可能起初是第一个图，但随后被编辑成展示参数*和*数据，结果顺序错乱。²
    这些图表都没有视觉上展示偏差（我们看到的大多数图表也没有），但这种选择并没有节省多少空间，如下所示。
- en: We did not cherry pick these diagrams. An internet image search for “neural
    network diagrams” reveals that those above are the norm, not the exception.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有挑选这些图表。对“神经网络图表”的互联网图片搜索显示，上述图表是常态，而非例外。
- en: '**Bad diagrams are bad for students**'
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**糟糕的图表对学生不好**'
- en: The diagrams above would probably be fine if they were being used only among
    seasoned professionals. But alas, they are being deployed for pedagogical purposes
    on hapless students of machine learning.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表如果仅在经验丰富的专业人士之间使用，可能会还不错。但遗憾的是，它们被用于教学目的，给无辜的机器学习学生带来了困扰。
- en: Learners encountering such weirdness must make written or mental notes such
    as “there is bias here, but they aren’t showing it,” or “the thing they are drawing
    *inside* a circle is actually the *output* of *processing* shown inside the same
    circle two slides ago” or “the inputs don’t actually work they way they are drawn.”
    The famous (and generally excellent) course mentioned above features lectures
    where the instructor patiently repeats several times that a given network doesn’t
    actually work the way a diagram shows it working. In the third week of the course,
    he valiantly tries to split the difference, alternating between special, accurate
    depictions which show what happens inside a node, and more typical diagrams which
    show something else. (If you want to see those better node depictions, this [blog
    post](/the-concept-of-artificial-neurons-perceptrons-in-neural-networks-fab22249cbfc)
    nicely shows them.)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 遇到这种奇怪情况的学习者必须做出书面或心理笔记，例如“这里有偏差，但他们没有显示出来”或“他们画在圆圈*内部*的东西实际上是*处理*的*输出*，而这个圆圈两张幻灯片前就显示过”或“输入实际上并不像图中显示的那样工作。”
    上述提到的著名（且通常很优秀的）课程包含讲师耐心地多次重复某个网络的实际工作方式与图示方式不符的讲座。在课程的第三周，他勇敢地尝试折衷，交替使用特殊的、准确的图示来展示节点内部发生的情况，以及更典型的图示来展示其他内容。（如果你想看到那些更好的节点图示，可以参考这篇[博客文章](/the-concept-of-artificial-neurons-perceptrons-in-neural-networks-fab22249cbfc)。）
- en: '**A better way**'
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**一种更好的方法**'
- en: '*Learning neural networks should not be an exercise in decoding misleading
    diagrams.* We propose a constructive, novel approach for teaching and learning
    neural networks: use good diagrams. We want diagrams that succinctly and faithfully
    represent the math — as seen in Feynman diagrams, Venn diagrams, digital filter
    diagrams, and circuit diagrams.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*学习神经网络不应该是解码误导性图表的练习。* 我们提出了一种建设性的、新颖的方法来教授和学习神经网络：使用好的图表。我们希望图表能够简洁而忠实地表示数学内容——如费曼图、文氏图、数字滤波器图和电路图中所示。'
- en: '**Let’s make GOOD diagrams**'
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**制作GOOD图**'
- en: So, what exactly do we propose? Let’s start with basics. Neural networks involve
    many simple operations which *already* have representations in flow diagrams that
    electrical engineers have used for decades. Specifically, we can depict copying
    data, multiplying data, adding data, and inputting data to a function which outputs
    data. We can then assemble abbreviated versions of these symbols into an accurate
    whole, which we will call Generally Objective Observable Depiction diagrams, or
    GOOD diagrams for short. (Sorry, backronym haters.)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们究竟提出了什么？让我们从基础开始。神经网络涉及许多简单的操作，这些操作*已经*在电气工程师使用了几十年的流程图中有表示。具体来说，我们可以描绘数据复制、数据相乘、数据相加和将数据输入函数以输出数据。然后，我们可以将这些符号的缩略版本组装成一个准确的整体，我们将其称为一般目标可观察描绘图，简称GOOD图。（抱歉，反缩写者。）
- en: Let’s look at the building blocks. To start, here’s how you show a total of
    three copies of data coming from a single source of data. It’s pretty intuitive.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些构建块。首先，这里是如何展示来自单一数据源的三个数据副本的。很直观。
- en: '![](../Images/dd669c4a4fe15dfc0bbde03f1ddd5958.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd669c4a4fe15dfc0bbde03f1ddd5958.png)'
- en: And here is a way to show scaling an input. It’s just a triangle.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是显示缩放输入的方法。它就是一个三角形。
- en: '![](../Images/f6e5e33bb539312a566b338640b07afa.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6e5e33bb539312a566b338640b07afa.png)'
- en: The triangle indicates that the input value *x*₁ going into it is scaled by
    some number *w*₁, to produce a result *w*₁times *x*₁. For example *w*₁could be
    0.5 or 1.2\. Later on it will be easier if we move this triangle to the right
    end of the diagram (merging it with the arrow) and make it pretty small, so let’s
    draw it that way.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 三角形表示输入值*x*₁进入它后被某个数*w*₁缩放，产生结果*w*₁乘*x*₁。例如，*w*₁可以是0.5或1.2。稍后如果我们将这个三角形移到图的右端（与箭头合并）并将其缩小，会更方便，所以我们就这样画吧。
- en: '![](../Images/480e3a0af518525cf51d860eb429e936.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/480e3a0af518525cf51d860eb429e936.png)'
- en: 'OK, we admit it: this is just an arrow with a solid triangle tip. The point,
    as it were, is that the triangle tip multiplies the data on the arrow.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我们承认：这只是一个带实心三角形尖端的箭头。关键是，三角形尖端乘以箭头上的数据。
- en: Next, here’s a way to show adding two or more things together. Let’s call the
    sum *z*. Also simple.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，这是一种显示将两个或更多东西加在一起的方法。我们称和为*z*。也很简单。
- en: '![](../Images/f49ec5101595caf1f90d1f5da7defb5d.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f49ec5101595caf1f90d1f5da7defb5d.png)'
- en: 'Now, we showed addition and multiplication above with some standard symbols.
    But what if we have a more general function that takes an input and produces an
    output? More specifically, when we’re making neural nets, we will use an *activation
    function* that is often a Sigmoid or ReLU. Either way, it’s no problem to diagram;
    we just show this as a box. For example, say the input to our function is called
    *z* and the function of *z* is called *g(z)* and produces an output *a*. It looks
    like this:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们用一些标准符号展示了加法和乘法。但如果我们有一个更通用的函数，它接受一个输入并产生一个输出呢？更具体地说，当我们制作神经网络时，我们会使用一个*激活函数*，它通常是Sigmoid或ReLU。无论哪种方式，都不成问题；我们只需将其表示为一个框。例如，假设我们函数的输入叫做*z*，函数*z*的结果叫做*g(z)*，产生一个输出*a*。它看起来是这样的：
- en: '![](../Images/ead3416d2a3f9a0ad35d369db36383c7.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ead3416d2a3f9a0ad35d369db36383c7.png)'
- en: Optionally, we can note that *g(z)* has a given input — output characteristic,
    which can be placed near the function box. Here’s a diagram including a *g(z)*
    plot for ReLU, along with the function box. In practice, there are only a few
    commonly used activation functions, so it would also be sufficient to note the
    function name (e.g. ReLU) somewhere near the layer.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，我们可以指出*g(z)*具有给定的输入—输出特性，这可以放在函数框附近。这是一个包括ReLU的*g(z)*图以及函数框的图。在实际应用中，通常只有少数几种激活函数，因此在图层附近注明函数名称（例如ReLU）也足够了。
- en: '![](../Images/a396d0a24bc4ad8fb184c8bb7da878c0.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a396d0a24bc4ad8fb184c8bb7da878c0.png)'
- en: 'Or, we can abbreviate even more, since there will be many, often identical,
    activation functions in a typical neural network. We propose using a single stylized
    script letter inside the function box for a specific activation, e.g. *R* for
    ReLU:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以进一步缩略，因为在典型的神经网络中将有许多，通常是相同的激活函数。我们建议在函数框内使用单个风格化的脚本字母来表示特定激活，例如*R*表示ReLU：
- en: '![](../Images/aad0af6c07e6f7a7953a35562d3c119e.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aad0af6c07e6f7a7953a35562d3c119e.png)'
- en: Similarly, Sigmoid could be represented with a stylized *S* and other functions
    with a another specified letter.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，Sigmoid可以用风格化的*S*表示，其他函数可以用另一指定字母表示。
- en: 'Before moving on, let’s make note of a simple but key fact: we show *data*
    (and its direction of travel) as **arrows**, and we show *operations* (multiplying,
    adding, general functions) as **shapes** (triangle, circle, square). *This is
    standard in electrical engineering flow diagrams.* But for some reason, perhaps
    inspiration from early computer science research which physically colocated memory
    and operations, this convention is ignored or even reversed when drawing neural
    networks.³ Nonetheless, the distinction matters because we *do* train the function
    parameters, but we *do not* train the data, each instance of which is immutable.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们注意一个简单但关键的事实：我们将*数据*（及其传输方向）表示为**箭头**，将*操作*（如乘法、加法、一般函数）表示为**形状**（如三角形、圆形、方形）。*这是电气工程流程图中的标准做法。*
    但由于某些原因，也许是受到早期计算机科学研究的启发，该研究物理上将记忆和操作结合在一起，当绘制神经网络时，这一惯例被忽视或甚至被颠倒。³ 尽管如此，这一区分仍然很重要，因为我们*确实*训练函数参数，但我们*不*训练数据，每个数据实例都是不可变的。
- en: OK, back to our story. Since we will soon be constructing a neural network diagram,
    it will need to depict a lot of “summing then function” processing. To make our
    diagrams more compact, we will make an abbreviated symbol that combines these
    two functions. To start, let’s draw them together, again assuming a ReLU for *g(z)*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，回到我们的故事。由于我们很快会构建神经网络图，因此需要描绘大量的“求和然后函数”处理。为了使我们的图示更加紧凑，我们将创建一个缩略符号来结合这两个功能。首先，让我们将它们绘制在一起，假设*g(z)*是ReLU。
- en: '![](../Images/7f066771006af1fd5b397ca2748206ba.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7f066771006af1fd5b397ca2748206ba.png)'
- en: 'Since we are about to abbreviate things, let’s see how they look when placed
    really close together. We will also drop the internal variable and function symbols
    from the plot, and add some dotted lines to hint at a proposed shape:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们要缩略符号，那么我们来看一下它们放得非常近时的样子。我们还将从图中删除内部变量和函数符号，并添加一些虚线以提示建议的形状：
- en: '![](../Images/2c209d869462a6c9d497dfb8b75ea53a.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2c209d869462a6c9d497dfb8b75ea53a.png)'
- en: 'Based on this, let’s introduce a new summary symbol for “sum then function”:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此，我们引入一个新的总结符号来表示“求和然后函数”：
- en: '![](../Images/3fe28efe9d28b920c643cf20340af6a1.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3fe28efe9d28b920c643cf20340af6a1.png)'
- en: Its special shape serves as a reminder of what it is doing. It also looks different
    than other symbols on purpose, to help us remember that it is special.⁴
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 它特殊的形状提醒我们它的作用。它也故意看起来与其他符号不同，以帮助我们记住它是特别的。⁴
- en: '**A GOOD thing**'
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**一个好事**'
- en: Now, let’s put all the diagrammed operations above together using a simple example
    of *logistic regression*. Our example starts with a two-dimensional input, multiplies
    each input dimension’s value by a unique constant, adds together the result along
    with a constant *b* (which we call *bias*), and passes the sum through a Sigmoid
    activation function. For reasons that will make sense later, we show the bias
    as the number 1 times the value *b*. For completeness (and foreshadowing) we give
    all these values names which we can show on the diagram. The inputs are *x*₁ and
    *x*₂, and the multiplication factors include the weights *w*₁ and *w*₂ as well
    as the bias *b*. The sum of the weighted inputs and bias is *z*, and the output
    of function *g(z)* is a.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过一个简单的*逻辑回归*示例，将上述所有图示操作整合在一起。我们的示例从二维输入开始，将每个输入维度的值乘以一个唯一的常数，然后将结果与常数*b*（我们称之为*偏置*）相加，并将和通过Sigmoid激活函数。由于稍后会讲解的原因，我们将偏置表示为1乘以值*b*。为了完整性（并作前瞻），我们为所有这些值指定名称，并在图示中显示出来。输入为*x*₁和*x*₂，乘法因子包括权重*w*₁和*w*₂以及偏置*b*。加权输入和偏置的总和为*z*，函数*g(z)*的输出为a。
- en: '![](../Images/4b3e5d8aa58e7669eec82a9255535d1c.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b3e5d8aa58e7669eec82a9255535d1c.png)'
- en: About that number “1” shown lower left on the diagram. The number 1 is not an
    input, but by showing this number in addition to the inputs, we clarify that each
    of these values is multiplied by a parameter contributing to the sum. This way
    we can show both values of *w* (input weights) and values of *b* (bias) on the
    same diagram. Bad diagrams usually skip showing the bias but GOOD ones don’t.
    Skipping bias in a diagram is especially risky in situations where a network might
    sometimes *intentionally omit* the bias; if the bias is not shown, a viewer is
    left to *guess* whether it is part of the network or not. So please deliberately
    include or exclude bias in your diagrams.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 关于图表左下角显示的那个数字“1”。这个数字1不是输入值，但通过显示这个数字而不仅仅是输入，我们明确了这些值中的每一个都乘以一个参与求和的参数。这样我们可以在同一个图表中展示*
    w*（输入权重）和值* b*（偏置）。不好的图表通常省略偏置，但好的图表不会。特别是在网络可能*故意省略*偏置的情况下，省略偏置尤其危险；如果未显示偏置，观众就不得不*猜测*它是否是网络的一部分。因此，请在图表中故意包括或排除偏置。
- en: Now let’s clean this up a bit by using the “sum then function” symbol we defined
    above. We also show the variable names below the diagram. Note that we indicate
    a Sigmoid function with the stylized script letter *S* in the “sum then function”
    symbol.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在通过使用上面定义的“求和然后函数”符号来稍微清理一下。我们还在图表下方显示了变量名称。请注意，我们用“求和然后函数”符号中的* S*来表示Sigmoid函数。
- en: '![](../Images/b73bac3067c9fdf3f2b12132be0b67b2.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b73bac3067c9fdf3f2b12132be0b67b2.png)'
- en: That looks pretty simple. It is a GOOD thing.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来很简单。这是件好事。
- en: 'Now, let’s build something a little more interesting: an actual neural network
    (NN) with a hidden layer of three units with ReLU activations, and one output
    layer with a Sigmoid activation. (If you’re not familiar, a hidden layer is any
    layer except the input or the output.) Note that this is the same architecture
    used in the Mountain View network diagram above. In this case, each input dimension,
    and the input layer bias, connects to every node in the hidden layer, then the
    hidden layer outputs (plus bias value again) connect to the output node. The output
    of each function is still called *a* but we use bracketed superscripts and subscripts
    to respectively denote which layer and node we are outputting from. Similarly,
    we use bracketed superscripts to indicate the layer to which the *w* and *b* values
    point. Using the style from the previous example, it looks like this:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们构建一些更有趣的东西：一个具有三个单位的隐藏层（使用ReLU激活）和一个使用Sigmoid激活的输出层的实际神经网络（NN）。 （如果你不熟悉，隐藏层是除了输入层或输出层以外的任何层。）请注意，这是上面Mountain
    View网络图中使用的相同架构。在这种情况下，每个输入维度和输入层偏置连接到隐藏层中的每个节点，然后隐藏层输出（加上偏置值）连接到输出节点。每个函数的输出仍然称为*a*，但我们使用带括号的上标和下标分别表示我们从哪个层和节点输出。类似地，我们使用带括号的上标来表示*
    w*和* b*值指向的层。使用之前示例中的风格，它看起来像这样：
- en: '![](../Images/003654464a98a9be9d4f0c143efd4e19.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/003654464a98a9be9d4f0c143efd4e19.png)'
- en: Now we are getting somewhere. At this point, we also see that the dimensions
    of *W* and *b* for each layer are specified by the dimensions of the inputs and
    the number of nodes in each layer. Let’s clean up the above diagram by not labeling
    every *w* and *b* value individually.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了进展。此时，我们还可以看到每层的* W*和* b*的维度由输入的维度和每层的节点数指定。让我们通过不单独标记每个* w*和* b*值来清理上述图表。
- en: '![](../Images/9961a3129f2eb4028000eb89ac253293.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9961a3129f2eb4028000eb89ac253293.png)'
- en: All images in this article by the authors.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 本文所有图片均由作者提供。
- en: Ta-dah! We have a GOOD neural network diagram that is also good. The learnable
    *parameters* are both shown on the diagram (as triangles) and summarized below
    it, while the *data* is shown directly on the diagram as labeled arrows. The architecture
    and activation functions for the network, typically called *hyperparameters,*
    are seen by inspecting the layout and nodes on the diagram itself.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 哒哒！我们有了一个很好的神经网络图表，同时也是好的。可学习的*参数*在图表上（作为三角形）和图表下方的总结中都显示出来，而*数据*直接在图表上以标记的箭头显示。网络的架构和激活函数，通常称为*超参数*，可以通过检查图表本身的布局和节点来查看。
- en: '**It’s all GOOD**'
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**一切都很棒**'
- en: 'Let’s consider the benefits of GOOD diagrams, independent of bad ones:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一下好的图表的好处，不考虑坏图表：
- en: It’s easy to see the order of operations for each layer. The multiplications
    happen first, then the sum, then the activation function.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 很容易看出每层的操作顺序。先进行乘法运算，然后是求和，然后是激活函数。
- en: It’s easy to see (immutable) data flowing through the network as separate from
    (trainable) parameters belonging to the network.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容易将（不可变的）数据流经网络与属于网络的（可训练的）参数区分开来。
- en: It’s easy to see the dimensionality of the *w* matrix and *b* vector for each
    layer. For a layer with *N* nodes, it’s clear that we need *b* to be of shape
    [*N*,1]. For a layer with *N* nodes coming after a layer with *M* nodes (or inputs),
    it’s clear that *w* is of shape [*N,M*]. (However, one still must memorize that
    the shape is [outputs, inputs] not [inputs, outputs].)
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容易看出每层的*w*矩阵和*b*向量的维度。对于一个有*N*节点的层，很明显我们需要*b*的形状为[*N*,1]。对于一个在有*M*节点（或输入）的层之后的有*N*节点的层，很明显*w*的形状为[*N*,*M*]。
    （然而，仍然需要记住形状是[输出，输入]而不是[输入，输出]。）
- en: Related, we see *where* exactly the weights and bias exist, which is *between*
    layers. Conventionally they are named as belonging to the layer they *output*
    to but apt students using GOOD diagrams are reminded that this is just a naming
    convention.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相关的，我们可以看到*权重*和偏差确切存在的位置，即*层与层之间*。传统上，它们被命名为属于它们*输出*到的层，但使用优秀图示的学生会被提醒，这只是一个命名惯例。
- en: 'Let’s also review how GOOD diagrams differ from bad ones:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们还来回顾一下优秀图示与差的图示的区别：
- en: They show the bias at each layer. They do not omit the bias.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们显示了每一层的偏差。它们不会遗漏偏差。
- en: They show data as data, and functions as functions. They do not confuse the
    two.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们将数据视为数据，将函数视为函数。它们不会混淆两者。
- en: They show when data is copied and sent to functions. They do not skip this step.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们显示了数据何时被复制并发送到函数。它们不会跳过这一步。
- en: They show all the steps in the correct order. They do not incorrectly reorder
    or omit steps.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们按照正确的顺序显示所有步骤。它们不会错误地重新排序或遗漏步骤。
- en: They are reasonably clean and concise. OK, the bad ones are a bit more concise.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们相对简洁明了。好吧，不好的那一些稍显简洁。
- en: '**Do some GOOD**'
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**做一些好事**'
- en: This article has spilled a lot of ink covering what’s wrong with bad diagrams
    and justifying GOOD ones. But if you are an ML instructor, we encourage you to
    just start using GOOD diagrams, without fanfare. GOOD diagrams are more self-explanatory
    than other options. You’ll be covering how neural nets work in your course anyway,
    so introducing GOOD diagrams at that point is a good idea.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 本文详细讨论了差的图示存在的问题，并为优秀图示提供了理由。但如果你是机器学习讲师，我们鼓励你直接开始使用优秀图示，而不需要额外的宣传。优秀图示比其他选项更具自解释性。反正你在课程中会讲解神经网络，所以在那个时候引入优秀图示是个好主意。
- en: And of course, as a service to your students, it’s a good idea to show some
    bad diagrams too. It’s important to know how the outside world is drawing things,
    even when it’s nonsense. In our estimation, it’s much easier to learn from something
    accurate and then to consider something confusing, than it is to do the reverse.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，为了服务你的学生，展示一些差的图示也是个好主意。了解外界是如何绘制图示的，即使它们毫无意义，也很重要。我们认为，从准确的东西中学习，然后考虑混乱的东西，比反过来要容易得多。
- en: '**More GOOD stuff is ahead**'
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**更多精彩内容在前面**'
- en: This completes the first article in what will become a series if it catches
    on. In particular, we have our collective eye on *Simplified Network* diagrams
    which compactly represent the kinds of fully connected networks shown above, and
    which could also stand some improvement. Convolutional Network diagrams deserve
    their own treatment. We are also looking into developing a software package which
    automates drawing of GOOD diagrams.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章完成了系列中的第一篇，如果它受到欢迎的话。特别是，我们关注的是*简化网络*图示，它们紧凑地表示了上面展示的全连接网络类型，并且也有改进的空间。卷积网络图示值得单独讨论。我们也在开发一个软件包，用于自动绘制优秀图示。
- en: '*Acknowledgements*'
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*致谢*'
- en: The authors thank Jeremy Schiff and Mikiko Bazeley for their assistance with
    this piece.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 作者感谢Jeremy Schiff和Mikiko Bazeley对本篇文章的协助。
- en: '*References and endnotes*'
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*参考文献和附注*'
- en: 1) Based on the other layers, maybe the first diagram is running the inputs
    into nontrivial activation functions, from which we get values likely *different*
    from the inputs. But there have been no examples in the associated courses that
    work this way, so it wouldn’t make sense to include such a diagram as the *only*
    fully connected diagram on the cheat sheet. Or maybe the first layer “*a*” values
    shown are *identical* to the inputs, in which case the activation functions are
    identity functions which incur trivial and unnecessary processing. Either way,
    the diagram is ambiguous and therefore bad.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 基于其他层，可能第一个图示正在将输入传递给非平凡的激活函数，从中得到的值可能与输入*不同*。但在相关课程中没有这样工作的例子，因此将这样的图示作为备忘单上*唯一*的全连接图示是不合适的。或者，第一个层中显示的“*a*”值可能*与输入相同*，在这种情况下，激活函数是恒等函数，这会引起平凡且不必要的处理。无论哪种情况，图示都是模糊的，因此不佳。
- en: 2) Either of the first two diagrams looks like it could be an unfortunate condensation
    of better, older diagrams, such as those in chapter 6 of the second edition of
    Pattern Classification by Duda, Hart and Stork (which one of us still has in hard
    copy from CS229 at Stanford in 2002). That book shows activation functions in
    circular units (which is better than showing their outputs *inside* the units),
    and correctly shows outputs leaving the units before copies are made and split
    off to the next layer. (It also shows the inputs and bias oddly, though.)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 前两个图示中的任意一个看起来都可能是更好、更旧的图示的一个不幸缩略，例如 Duda、Hart 和 Stork 第二版《模式分类》第六章中的图示（其中一位我们还保留着2002年CS229时的纸质版）。那本书将激活函数显示为圆形单元（比将其输出*放在*单元内要好），并正确地显示输出在制作副本并分离到下一层之前离开单元。（虽然它也奇怪地显示了输入和偏置。）
- en: 3) If your study has progressed to include Convolutional Nets (CNs), you will
    see that CN diagrams routinely show *data* as blocks and *processes* as annotated
    arrows. Don’t fret. For now, just remember that there’s an essential difference
    between data and processes, and, for fully connected neural nets, a good (or GOOD)
    diagram will make clear which is which.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 如果你的学习已经进展到包括卷积网络（CNs），你会发现 CN 图示通常将*数据*显示为块，将*处理过程*显示为标注的箭头。不要担心。现在，只需记住数据和处理过程之间有本质的区别，对于全连接神经网络，一个好的（或优秀的）图示会清晰地表明它们的不同。
- en: 4) For you logic fans out there that see the “sum then function” symbol as an
    *and gate* operating in reverse, remember that *and gates* are *irreversible*.
    Therefore, this new symbol must have another meaning, which we define here.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 4) 对于那些将“和再函数”符号视为*反向操作的与门*的逻辑爱好者，请记住，*与门*是*不可逆的*。因此，这个新符号必然有其他含义，我们在这里定义。
