- en: Improving Physics-Informed Neural Networks through Adaptive Loss Balancing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过自适应损失平衡提升物理信息神经网络
- en: 原文：[https://towardsdatascience.com/improving-pinns-through-adaptive-loss-balancing-55662759e701?source=collection_archive---------6-----------------------#2023-01-31](https://towardsdatascience.com/improving-pinns-through-adaptive-loss-balancing-55662759e701?source=collection_archive---------6-----------------------#2023-01-31)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/improving-pinns-through-adaptive-loss-balancing-55662759e701?source=collection_archive---------6-----------------------#2023-01-31](https://towardsdatascience.com/improving-pinns-through-adaptive-loss-balancing-55662759e701?source=collection_archive---------6-----------------------#2023-01-31)
- en: How to boost your PINN’s performance with ReLoBRaLo, Learning Rate Annealing
    and co.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何通过 ReLoBRaLo、学习率退火等方法提升 PINN 的性能
- en: '[](https://rabischof.medium.com/?source=post_page-----55662759e701--------------------------------)[![Rafael
    Bischof](../Images/a1d468ea5b61c26a18541f0c0f42c5c6.png)](https://rabischof.medium.com/?source=post_page-----55662759e701--------------------------------)[](https://towardsdatascience.com/?source=post_page-----55662759e701--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----55662759e701--------------------------------)
    [Rafael Bischof](https://rabischof.medium.com/?source=post_page-----55662759e701--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://rabischof.medium.com/?source=post_page-----55662759e701--------------------------------)[![Rafael
    Bischof](../Images/a1d468ea5b61c26a18541f0c0f42c5c6.png)](https://rabischof.medium.com/?source=post_page-----55662759e701--------------------------------)[](https://towardsdatascience.com/?source=post_page-----55662759e701--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----55662759e701--------------------------------)
    [Rafael Bischof](https://rabischof.medium.com/?source=post_page-----55662759e701--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F913c6c1e6a94&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-pinns-through-adaptive-loss-balancing-55662759e701&user=Rafael+Bischof&userId=913c6c1e6a94&source=post_page-913c6c1e6a94----55662759e701---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----55662759e701--------------------------------)
    ·14 min read·Jan 31, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F55662759e701&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-pinns-through-adaptive-loss-balancing-55662759e701&user=Rafael+Bischof&userId=913c6c1e6a94&source=-----55662759e701---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[阅读](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F913c6c1e6a94&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-pinns-through-adaptive-loss-balancing-55662759e701&user=Rafael+Bischof&userId=913c6c1e6a94&source=post_page-913c6c1e6a94----55662759e701---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----55662759e701--------------------------------)
    ·14 分钟阅读·2023年1月31日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F55662759e701&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-pinns-through-adaptive-loss-balancing-55662759e701&user=Rafael+Bischof&userId=913c6c1e6a94&source=-----55662759e701---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F55662759e701&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-pinns-through-adaptive-loss-balancing-55662759e701&source=-----55662759e701---------------------bookmark_footer-----------)![](../Images/78a596b4145a3b2807cb6d025b3d9cf1.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F55662759e701&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-pinns-through-adaptive-loss-balancing-55662759e701&source=-----55662759e701---------------------bookmark_footer-----------)![](../Images/78a596b4145a3b2807cb6d025b3d9cf1.png)'
- en: Photo by [David Clode](https://unsplash.com/@davidclode?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [David Clode](https://unsplash.com/@davidclode?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'In this article, we review the basics of PINNs, explore the issue of imbalanced
    losses, and show how the balancing scheme [ReLoBRaLo (Relative Loss Balancing
    with Random Lookbacks)](https://arxiv.org/abs/2110.09813) [1], proposed by Michael
    Kraus and myself, can significantly boost the training process. Plus, experience
    the technique in action with two accompanying notebooks solving real-world PDEs:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们回顾了 PINNs 的基础知识，探讨了不平衡损失的问题，并展示了由 Michael Kraus 和我提出的平衡方案 [ReLoBRaLo
    (相对损失平衡与随机回溯)](https://arxiv.org/abs/2110.09813) [1] 如何显著提升训练过程。还可以通过两个附带的笔记本体验这种技术在实际
    PDE 问题中的应用：
- en: '[**Notebook for Kirchhoff Plate bending PDE**](https://colab.research.google.com/drive/1_PmLv8OWh9GZTzqPgFpCXYYBlxGqszOn?usp=sharing)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**基尔霍夫板弯曲 PDE 笔记本**](https://colab.research.google.com/drive/1_PmLv8OWh9GZTzqPgFpCXYYBlxGqszOn?usp=sharing)'
- en: '[**Notebook for Helmholtz PDE**](https://colab.research.google.com/drive/1R6aPThhp1wrQVaydl8pj1G0s4gLiH8Wc?usp=sharing)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**亥姆霍兹 PDE 笔记本**](https://colab.research.google.com/drive/1R6aPThhp1wrQVaydl8pj1G0s4gLiH8Wc?usp=sharing)'
- en: If you clicked on this article, it is probably because you already have quite
    a good understanding of what [Physics-Informed Neural Networks (PINN)](https://www.sciencedirect.com/science/article/pii/S0021999118307125)
    [2] are. Maybe you have found some tutorials online and implemented PINNs on well-known
    benchmarks like the Burgers or Helmholtz equation. The idea of harnessing the
    power of neural networks to solve complex partial differential equations (PDEs)
    is certainly an appealing one. But as many of us have painfully discovered, the
    reality of working with PINNs can also be quite a frustrating process. If you
    went ahead and tried applying these tools on a PDE that you encountered in your
    own research, one that may not yet be well-documented in the literature, then
    it is very likely that the vanilla PINNs may not have performed as well as you
    had hoped. Worse, that they even converged more slowly than established approaches
    like the Finite Elements Method (FEM)!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你点击了这篇文章，可能是因为你已经对[物理信息神经网络 (PINN)](https://www.sciencedirect.com/science/article/pii/S0021999118307125)
    [2]有了相当好的理解。也许你在网上找到了一些教程，并在像 Burgers 或 Helmholtz 方程这样的知名基准上实现了 PINNs。利用神经网络的力量来解决复杂的偏微分方程
    (PDEs) 的想法确实很有吸引力。但正如我们许多人痛苦地发现的那样，使用 PINNs 的现实过程也可能非常令人沮丧。如果你继续尝试将这些工具应用于你自己研究中遇到的尚未在文献中很好记录的
    PDE，那么很可能 vanilla PINNs 的表现不会如你所愿。更糟的是，它们可能比像有限元方法 (FEM) 这样的成熟方法收敛得更慢！
- en: Bear with me, I have been there, in fact, basically every time that I tried
    applying PINNs to a new problem. Despite the progress that has been made in the
    five years since their proposal, and the decades of [research on using differentiable
    Neural Networks to solve differential equations](https://www.sciencedirect.com/science/article/pii/002199919090007N)
    [3], there is still no easy, plug-and-play version of PINNs that can be seamlessly
    transferred to any type of problem.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 请耐心点，我曾经经历过这一切，事实上，每次我尝试将 PINNs 应用到新问题上时，都是如此。尽管自其提出以来的五年中取得了一些进展，并且有几十年的[利用可微分神经网络解决微分方程的研究](https://www.sciencedirect.com/science/article/pii/002199919090007N)
    [3]，但目前仍没有一种易于使用、即插即用的 PINNs 版本可以无缝转移到任何类型的问题上。
- en: You see, PINNs make use of differential equations in their loss function by
    taking multiple higher-order derivatives of the output with respect to the input.
    These derivatives are then used to construct the residual and boundary conditions
    that should be approximated. This means that each partial differential equation
    fundamentally changes the PINN’s training procedure. Not only may it become necessary
    to adapt the architecture, such as adding or removing layers and nodes, but other,
    more intricate hyperparameters may have a crucial impact on the modelling capabilities,
    many of which are pecularities of PINNs and can not be found in the literature
    of classical Neural Networks. These may include the choice of activation functions,
    the sampling procedure on the physical domain, or, very treacherous, the choice
    of units of measurements in the differential equation.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你看，PINNs 通过对输出相对于输入进行多次高阶导数，将这些导数用于构建残差和边界条件，从而在其损失函数中利用微分方程。这意味着每个偏微分方程从根本上改变了
    PINN 的训练过程。可能需要调整架构，例如添加或删除层和节点，此外，其他更复杂的超参数也可能对建模能力产生关键影响，其中许多是 PINNs 的特性，无法在经典神经网络的文献中找到。这些可能包括激活函数的选择、物理域上的采样程序，或者，非常棘手的是，微分方程中测量单位的选择。
- en: While I can unfortunately not give you all the ingredients necessary to make
    your PINNs work, I can most certainly tell you what the fundamental steps are,
    without which your endeavour will most likely be fruitless. But before I reveal
    these crucial tools, allow me to provide some context to better understand my
    arguments. Let us take a step back and open a parenthesis, if you will. (
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我无法给你使 PINNs 工作所需的所有配方，但我可以告诉你一些基本步骤，如果没有这些步骤，你的努力很可能是徒劳的。但在我揭示这些关键工具之前，请允许我提供一些背景，以更好地理解我的论点。让我们退后一步，打开一个括号。
- en: Benchmark PDEs
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基准 PDE
- en: For the sake of illustration, let us introduce the Helmholtz and Kirchhoff plate
    bending equations. But before you start feeling overwhelmed, let me assure you
    that understanding the intricacies of these PDEs is not necessary for following
    the rest of this article. If you want to skip this section, just know that the
    Helmholtz PDE is a second-order PDE with zeroth order (Dirichlet) boundary conditions,
    and the Kirchhoff plate bending equation is a fourth-order PDE with boundary conditions
    on the zeroth and second-order derivatives.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明，让我们引入赫尔姆霍兹和**Kirchhoff** 板弯曲方程。但在你开始感到不知所措之前，让我向你保证，理解这些偏微分方程的复杂性并不是跟随本文其余部分的必要条件。如果你想跳过这一部分，只需知道赫尔姆霍兹偏微分方程是一个具有零阶（Dirichlet）边界条件的二阶偏微分方程，而**Kirchhoff**
    板弯曲方程是一个具有零阶和二阶导数边界条件的四阶偏微分方程。
- en: This equation is a fourth-order partial differential equation (PDE) that describes
    the deformation of a plate under load. The unknown function u in the equation
    represents the vertical displacement (in meters, for example) from the initial
    state of the plate at a given point (x, y). The load applied on the plate is represented
    by the function p(x, y). The constant D in the equation encapsulates various properties
    of the plate such as its thickness, modulus of elasticity, and density.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程是一个四阶偏微分方程（PDE），描述了板在载荷下的变形。方程中的未知函数 u 表示板在给定点 (x, y) 的垂直位移（例如，以米为单位）。施加在板上的载荷由函数
    p(x, y) 表示。方程中的常数 D 包含了板的各种属性，如其厚度、弹性模量和密度。
- en: Kirchhoff plate bending equation.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kirchhoff** 板弯曲方程。'
- en: So, Kirchhoff’s equation states that the fourth-order derivative of the deformation
    is equal to the load applied on the plate divided by a constant factor. Fairly
    straightforward, right?
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，**Kirchhoff** 方程说明了变形的四阶导数等于施加在板上的载荷除以一个常数因子。相当直接，对吧？
- en: Of course every experienced PDE-tian knows that a governing equation, as elegant
    as it may seem, is nothing but a meaningless abstraction without the proper boundary
    conditions. After all, there are an infinite number of equations that could fulfill
    it.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，每个经验丰富的 PDE 专家都知道，尽管主方程看起来优雅，但没有适当的边界条件，它只是一种毫无意义的抽象。毕竟，有无限多的方程可以满足它。
- en: 'So let us also introduce the boundary conditions:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们也引入边界条件：
- en: Boundary conditions for simply supported edges
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 简支边缘的边界条件
- en: where W and H define the the plate’s width and height, respectively. The first
    row of the boundary conditions shows the zeroth (Dirichlet) and states that the
    edges of the plate are not allowed to bend. The second row shows the second-order
    derivatives, which enforce the moments on the edges to be zero. This can be illustrated
    with an edge of a plate that is supported below by a beam (hence zero 0th order
    derivative) and squeezed by another beam by above (resulting in zero moments).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 W 和 H 分别定义了板的宽度和高度。第一行的边界条件显示了零阶（Dirichlet），并指出板的边缘不允许弯曲。第二行显示了二阶导数，这要求边缘的弯矩为零。这可以通过一个被下面的梁支撑（因此零阶导数为零）并被上面的另一根梁挤压（导致弯矩为零）的板边缘来说明。
- en: '![](../Images/099f79c55b98b5e1bf1a6c4e071fe2a8.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/099f79c55b98b5e1bf1a6c4e071fe2a8.png)'
- en: Example of a Kirchhoff plate and its deformation (in m) under a sine load and
    W = H = 10 and D = 20.83\. Figure by author.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一个**Kirchhoff** 板及其在正弦载荷下的变形（以米为单位），W = H = 10 和 D = 20.83。图由作者提供。
- en: 'The Helmholtz Equation: Modeling Waves in a Medium'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 赫尔姆霍兹方程：介质中波的建模
- en: The Helmholtz equation is a partial differential equation that describes the
    propagation of waves in a medium. It is a second-order equation and named after
    the German physicist Hermann von Helmholtz.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 赫尔姆霍兹方程是一个描述波在介质中传播的偏微分方程。它是一个二阶方程，以德国物理学家赫尔曼·冯·赫尔姆霍兹命名。
- en: Helmholtz equation, where u(x, y) is the unknown function and k is the wave
    number
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 赫尔姆霍兹方程，其中 u(x, y) 是未知函数，k 是波数
- en: 'where k is the wave number and u(x, y) the unknown function to be found. For
    this problem, we will use the zeroth-order Dirichlet boundary conditions on all
    four edges of the domain:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 k 是波数，u(x, y) 是待求的未知函数。对于这个问题，我们将使用域四个边缘上的零阶 Dirichlet 边界条件：
- en: Dirichlet boundary conditions for the Helmholtz PDE
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 赫尔姆霍兹偏微分方程的**Dirichlet边界条件**
- en: '![](../Images/2886293b8233c59f6685f4398d25dc8f.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2886293b8233c59f6685f4398d25dc8f.png)'
- en: Example of a Helmholtz wave propagation with zero Dirichlet boundaries. Figure
    by author.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一个赫尔姆霍兹波传播的例子，边界条件为零 Dirichlet。图由作者提供。
- en: PINN Loss Function
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**PINN** 损失函数'
- en: If I lost you anywhere during the definition of the Kirchhoff or Helmholtz functions,
    do not worry. It took me over half a year, and countless explanations from patient
    civil engineers, before being able to ruminate these formulas to you.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在定义Kirchhoff或Helmholtz函数的过程中我让你迷失了，不必担心。我花了半年多时间，以及无数耐心的土木工程师的解释，才得以向你讲解这些公式。
- en: 'The real key is understanding how to translate these equations into a loss
    function that can be used to train our PINN, here for the Helmholtz equation:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于理解如何将这些方程转化为可用于训练我们PINN的损失函数，这里针对Helmholtz方程：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You can find the full code in the notebooks implementing ReLoBRaLo for the [Helmholtz](https://colab.research.google.com/drive/1R6aPThhp1wrQVaydl8pj1G0s4gLiH8Wc?usp=sharing)
    and the [Kirchhoff PDEs](https://colab.research.google.com/drive/1_PmLv8OWh9GZTzqPgFpCXYYBlxGqszOn?usp=sharing).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在实现ReLoBRaLo的笔记本中找到完整的代码，链接为[Helmholtz](https://colab.research.google.com/drive/1R6aPThhp1wrQVaydl8pj1G0s4gLiH8Wc?usp=sharing)和[Kirchhoff
    PDEs](https://colab.research.google.com/drive/1_PmLv8OWh9GZTzqPgFpCXYYBlxGqszOn?usp=sharing)。
- en: Multi-Objective Optimisation
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多目标优化
- en: 'As we have already established, the final loss function for our Helmholtz PDE
    will consist of two, the Kirchhoff PDE of three objectives:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经确定的，我们的Helmholtz PDE的最终损失函数将包含两个，而Kirchhoff PDE则包含三个目标：
- en: 'Helmholtz: the loss for the governing equation L_f and the loss for the 0th
    order boundary condition L_b0.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Helmholtz：控制方程的损失L_f和0阶边界条件的损失L_b0。
- en: 'Kirchhoff: in addition to L_f and L_b0, Kirchhoff also has a term for the second-order
    boundary condition L_b2.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kirchhoff：除了L_f和L_b0之外，Kirchhoff还包括一个用于二阶边界条件L_b2的项。
- en: Therefore, these losses fall into the category of Multi-Objective Optimisation
    (MOO), as is the case for most applications involving PINNs.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这些损失属于多目标优化（MOO）范畴，就像大多数涉及PINNs的应用一样。
- en: 'The way the several objectives are aggregated into a single loss is usually
    done through linear scalarisation:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 将多个目标聚合为单一损失的方式通常是通过线性标量化：
- en: where the lambdas are scaling factors for controlling each term’s contribution
    towards the total loss. But why are they necessary?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其中lambda是用于控制每个项对总损失贡献的缩放因子。但为什么它们是必要的？
- en: The Issue of imbalanced Gradients
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不平衡梯度问题
- en: After this detour for gathering the necessary context, we can finally close
    the open parenthesis ) and continue exploring why the units of measurements in
    the PDE have an influence on the convergence of PINNs. You see, the several objectives
    in our loss function — L_f, L_b0, and L_b2 — each have different units of measurement.
    L_b0 for Kirchhoff may be measured in meters, while L_b2 is measured in Nm, and
    the load on the plate is measured in MN per square meter. This creates a significant
    disparity in the magnitude of each term, leading to a computation of gradients
    that heavily favours the terms with the highest magnitude. The same is true for
    Helmholtz and any other PDE.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集必要背景信息后，我们可以最终关闭括号）并继续探索为何PDE中的测量单位会影响PINNs的收敛性。你看，我们的损失函数中的几个目标——L_f、L_b0和L_b2——每个都有不同的测量单位。L_b0对于Kirchhoff可能以米为单位测量，而L_b2以Nm为单位测量，板上的负载以MN每平方米为单位测量。这在每个项的大小上产生了显著的差异，导致梯度计算严重偏向具有最高幅度的项。Helmholtz方程及任何其他PDE也是如此。
- en: Let us have a look at what this means in our example with the Helmholtz equation.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这在Helmholtz方程中的含义。
- en: '![](../Images/6367883a371059a849c42e9f27ef8483.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6367883a371059a849c42e9f27ef8483.png)'
- en: Evolution of the losses when training a PINN on Helmholtz’ equation. L_f is
    the squared loss on the governing equation, L_b is the squared loss on the boundary
    conditions and L_u is the square loss of the predictions against the analytical
    solution. Figure by author.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 训练PINN模型时对Helmholtz方程损失的演变。L_f是对控制方程的平方损失，L_b是对边界条件的平方损失，L_u是预测值与解析解之间的平方损失。图示由作者提供。
- en: Notice how the governing equation loss L_f is several orders of magnitude larger
    than the losses for the boundary conditions at the beginning of training and,
    as a consequence, how the value of L_b starts off by actually INCREASING. This
    discrepancy in magnitude can lead to a PINN that prioritizes L_f over L_b, ultimately
    converging towards a solution that satisfies the governing equation but neglects
    the crucial boundary conditions. This effect can be observed in the plot by the
    fact that the validation loss L_u follows the same pattern as the boundary loss
    L_b, suggesting that the validation performance is closely related to the performance
    on the boundaries.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到在训练开始时，控制方程损失 L_f 比边界条件的损失大几个数量级，因此，L_b 的值实际上会**增加**。这种量级上的差异可能导致 PINN 优先考虑
    L_f 而忽视 L_b，最终收敛到一个满足控制方程但忽略关键边界条件的解。这种效果可以通过验证损失 L_u 的图表观察到，验证损失 L_u 与边界损失 L_b
    遵循相同的模式，表明验证性能与边界上的表现密切相关。
- en: What about the Kirchhoff PDE?
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 那么 Kirchhoff PDE 呢？
- en: '![](../Images/22e01b59520f242a04d2e9942fcd2ca8.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22e01b59520f242a04d2e9942fcd2ca8.png)'
- en: Evolution of the losses when training a PINN on Kirchhoff’s equation. L_f is
    the squared loss on the governing equation, L_b0 is the squared loss on the Dirichlet
    boundary conditions, L_b2 on the moments’ boundary conditions, and L_u is the
    square loss of the predictions against the analytical solution. Figure by author.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练 PINN 以解决 Kirchhoff 方程时损失的演变。L_f 是控制方程的平方损失，L_b0 是 Dirichlet 边界条件的平方损失，L_b2
    是矩的边界条件的平方损失，而 L_u 是预测与解析解之间的平方损失。图由作者提供。
- en: In the case of Kirchhoff, the inverse holds true. Here, the boundary conditions
    converge much more rapidly, while the governing equation makes little progress.
    The most likely explanation is that the governing equation involves fourth-order
    derivatives and is therefore a particularly hard objective to optimise for. This
    shows that the causes for **imbalanced losses** are not limited to differences
    in magnitudes between the terms. They range from the choice of activation function
    to the complexity of the function being approximated by each term.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Kirchhoff 方程来说，情况正好相反。这里，边界条件的收敛速度远远快于控制方程。最可能的解释是，控制方程涉及四阶导数，因此优化起来尤其困难。这表明**不平衡损失**的原因不仅仅是项之间量级的差异，还包括激活函数的选择和每一项所逼近函数的复杂性。
- en: I**mbalanced gradients** are by no means limited to the Helmholtz or Kirchhoff
    PDEs alone. Many studies have [documented this issue in various PINN applications](https://arxiv.org/pdf/2001.04536.pdf)
    [4]. The key takeaway here is that, in order to arrive at accurate solutions,
    it is essential to strike a balance between all the objectives in the loss function.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**不平衡的梯度**并不限于 Helmholtz 或 Kirchhoff PDEs。许多研究已经[记录了在各种 PINN 应用中存在这一问题](https://arxiv.org/pdf/2001.04536.pdf)
    [4]。这里的关键点是，为了得到准确的解，必须在损失函数中的所有目标之间取得平衡。'
- en: Adaptive Loss balancing Schemes
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自适应损失平衡方案
- en: To mitigate the issue of imbalanced losses and gradients, one can resort to
    the scaling factors lambda in the linear scalarisation of the Multi-Objective
    Optimisation introduced earlier. Selecting larger values of lambda for terms with
    smaller magnitudes or harder objectives can help evening the contributions to
    the final gradient, and thus make sure that all terms are appropriately approximated.
    However, doing this by hand is a tedious task, requiring many iterations and thus
    a lot of resources in terms of time and compute.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻不平衡损失和梯度的问题，可以借助之前介绍的多目标优化中的线性标量化的缩放因子 lambda。选择较大的 lambda 值用于量级较小或更难优化的项可以帮助平衡最终梯度的贡献，从而确保所有项都被适当地逼近。然而，手动完成这项任务是繁琐的，需要许多迭代，因此在时间和计算资源方面耗费较大。
- en: This is why researchers have proposed loss balancing schemes, such as [Gradnorm](https://arxiv.org/abs/1711.02257)
    [5], [SoftAdapt](https://arxiv.org/abs/1912.12355) [6] or [Learning Rate Annealing](https://arxiv.org/pdf/2001.04536.pdf)
    [4].
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么研究人员提出了损失平衡方案，如 [Gradnorm](https://arxiv.org/abs/1711.02257) [5]、[SoftAdapt](https://arxiv.org/abs/1912.12355)
    [6] 或 [学习率退火](https://arxiv.org/pdf/2001.04536.pdf) [4]。
- en: Relative Loss Balancing with Random Lookbacks (ReLoBRaLo)
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相对损失平衡与随机回溯（ReLoBRaLo）
- en: In this article, we will focus on a scheme called [Relative Loss Balancing with
    Random Lookbacks (ReLoBRaLo)](https://arxiv.org/abs/2110.09813) [1], which is
    a combination of the aforementioned methods.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将重点介绍一种名为[相对损失平衡与随机回顾（ReLoBRaLo）](https://arxiv.org/abs/2110.09813)的方案[1]，它是上述方法的结合体。
- en: The goal of ReLoBRaLo is to ensure that each term in the loss function makes
    the same amount of progress over time, relative to its value at the start of training.
    For example, if L_f improves by 50% since the beginning of training, we want the
    other terms to improve at about the same rate and achieve a reduction of 50%.
    However, if there is a term that consistently improves at a slower rate, ReLoBRaLo
    incrementally increases the scaling lambda of this term, thus increasing its contribution
    to the gradient calculation.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ReLoBRaLo 的目标是确保损失函数中的每一项随着时间的推移都有相同的进展，相对于其在训练开始时的值。例如，如果 L_f 从训练开始以来提高了 50%，我们希望其他项也以大致相同的速度提高，并达到
    50% 的减少。然而，如果某一项始终以较慢的速度提高，ReLoBRaLo 会逐渐增加该项的缩放因子 lambda，从而增加其对梯度计算的贡献。
- en: 'Let us say that we have n loss terms L_i and let us denote the function L_i(t)
    to be the value of this term at training iteration t. One way that we can measure
    its progress since the start of training is by dividing the value at the current
    iteration L_i(t) by the value at the beginning of training, L_i(0):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有 n 个损失项 L_i，并且用 L_i(t) 表示训练迭代 t 时该项的值。我们可以通过将当前迭代的值 L_i(t) 除以训练开始时的值 L_i(0)
    来衡量其进展：
- en: ReLoBRaLo measures the progress each term i made since the start of training
    by dividing the current value of the Loss L_i(t) by the value at the first iteration
    L_i(0) for each of the n terms.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ReLoBRaLo 通过将每个损失项 L_i(t) 的当前值除以第一个迭代的值 L_i(0)，来衡量每个项 i 从训练开始以来的进展。
- en: 'The greater the progress since the beginning of training was, the smaller the
    result of this operation will be. Observe how this is exactly what we are looking
    for: our scheme should attribute high scalings to terms that improved slowly,
    and small scalings to terms that improved fast — and all of that should happen
    independent of the absolute values of the terms. Therefore, we can use L_i(t)
    / L_i(0) for calculating the scalings of the terms in the loss function.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 训练开始以来的进展越大，这个操作的结果就会越小。观察这正是我们所期望的：我们的方案应该对进展缓慢的项赋予较高的缩放因子，而对进展较快的项赋予较小的缩放因子——而且所有这一切都应该独立于项的绝对值。因此，我们可以使用
    L_i(t) / L_i(0) 来计算损失函数中各项的缩放因子。
- en: While this is the key component of ReLoBRaLo, it contains a number of additional
    extension that have been found to further improve the performance. However, for
    the sake of readability of this article, I leave it to the interested reader to
    [have a look at the paper](https://arxiv.org/abs/2110.09813) and learn more about
    the methods used and their motivation.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这是 ReLoBRaLo 的关键组件，但它还包含了许多额外的扩展，这些扩展被发现可以进一步提高性能。然而，为了便于本文的可读性，我将这些内容留给感兴趣的读者[查看论文](https://arxiv.org/abs/2110.09813)并了解更多关于所用方法及其动机的信息。
- en: 'But does it work? Well let us have a look at the loss evolution on the Helmholtz
    PDE, but this time by using ReLoBRsLo for balancing the contributions of the terms
    to the total loss:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 那么它有效吗？让我们来看看使用 ReLoBRaLo 平衡各项对总损失的贡献时 Helmholtz PDE 的损失演变情况：
- en: '![](../Images/6f120a085525018f63a88739c940f103.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f120a085525018f63a88739c940f103.png)'
- en: Evolution of the losses when training a PINN on Helmholtz’ equation and using
    ReLoBRaLo. L_f is the squared loss on the governing equation, L_b is the squared
    loss on the boundary conditions and L_u is the square loss of the predictions
    against the analytical solution. Figure by author.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Helmholtz 方程上训练 PINN 并使用 ReLoBRaLo 时损失的演变。L_f 是对主方程的平方损失，L_b 是对边界条件的平方损失，L_u
    是预测值与解析解之间的平方损失。图由作者提供。
- en: 'While the loss for the governing equation L_f does not make as much progress
    anymore (it converged at around -3.8 in the previous plot), the boundary conditions
    L_b and in consequence the validation loss L_u receive much more weight. The final
    validation loss against the analytical solution yields a **65% improvement over
    the unscaled training run**. Let us have a look at the scaling values that ReLoBRaLo
    computed:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然主要方程 L_f 的损失不再有太大进展（在前一个图中收敛于大约 -3.8），但边界条件 L_b 以及由此产生的验证损失 L_u 收到了更多的权重。最终的验证损失相对于解析解有了
    **65% 的改善**。让我们看看 ReLoBRaLo 计算的缩放值：
- en: '![](../Images/f8a93c6fd313f29335315568ae2f069f.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8a93c6fd313f29335315568ae2f069f.png)'
- en: Scaling factors lambda_i obtained through ReLoBRaLo for the governing equation
    term L_f (blue) and boundary conditions L_b (orange) on the Helmholtz PDE. Figure
    by author.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 ReLoBRaLo 获得的 Helmholtz PDE 主要方程项 L_f（蓝色）和边界条件 L_b（橙色）的缩放因子 lambda_i。图由作者提供。
- en: 'The same goes for Kirchhoff:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Kirchhoff 也是如此：
- en: '![](../Images/984c55984aeba4b757521f9ff03d4ef5.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/984c55984aeba4b757521f9ff03d4ef5.png)'
- en: Evolution of the losses when training a PINN on Kirchhoff’s equation and using
    ReLoBRaLo. L_f is the squared loss on the governing equation, L_b0 is the squared
    loss on the Dirichlet boundary conditions, L_b2 on the moments’ boundary conditions,
    and L_u is the square loss of the predictions against the analytical solution.
    Figure by author.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kirchhoff 方程上训练 PINN 并使用 ReLoBRaLo 时，损失的演变。L_f 是主要方程的平方损失，L_b0 是 Dirichlet
    边界条件的平方损失，L_b2 是矩边界条件的平方损失，L_u 是预测值与解析解的平方损失。图由作者提供。
- en: '![](../Images/927cdde2e032e638cd94dc9cf03dd3ef.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/927cdde2e032e638cd94dc9cf03dd3ef.png)'
- en: Scaling factors lambda_i obtained through ReLoBRaLo for the governing equation
    term L_f (blue), Dirichlet boundary conditions L_b0 (orange) and moments’ boundary
    conditions L_b2 (green) on the Kirchhoff PDE. Figure by author.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 ReLoBRaLo 获得的 Kirchhoff PDE 主要方程项 L_f（蓝色）、Dirichlet 边界条件 L_b0（橙色）和矩边界条件 L_b2（绿色）的缩放因子。图由作者提供。
- en: Again, ReLoBRaLo improved the error against the analytical solution by over
    an order of magnitude. It is also worth noting that this balancing scheme adds
    almost no computational overhead (cf. the paper). It is this effectiveness and
    efficiency that earned ReLoBRaLo its way into [Nvidia’s Modulus framework for
    Physics-Informed Deep Learning](https://docs.nvidia.com/deeplearning/modulus/api/modulus.loss.html#modulus.loss.aggregator.Relobralo).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，ReLoBRaLo 将误差相对于解析解的改善了一个数量级。值得注意的是，这种平衡方案几乎没有增加计算开销（参见论文）。正是这种有效性和效率让 ReLoBRaLo
    成为 [Nvidia 物理信息深度学习框架 Modulus](https://docs.nvidia.com/deeplearning/modulus/api/modulus.loss.html#modulus.loss.aggregator.Relobralo)
    的一部分。
- en: 'But the real question is: can you use ReLoBRaLo in your own projects? The answer
    is a resounding yes! As it happens, the scheme can be neatly wrapped into a keras
    loss that can either be added to your keras model through model.compile(), or,
    in case you defined your own custom training loop, by explicitly calling it at
    each iteration.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 但真正的问题是：你能在自己的项目中使用 ReLoBRaLo 吗？答案是一个响亮的“能”！实际上，该方案可以整洁地封装成一个 keras 损失，可以通过
    model.compile() 添加到你的 keras 模型中，或者在定义了自定义训练循环的情况下，在每次迭代时显式调用它。
- en: You can find the full code in the notebooks implementing ReLoBRaLo for the [Helmholtz](https://colab.research.google.com/drive/1R6aPThhp1wrQVaydl8pj1G0s4gLiH8Wc?usp=sharing)
    and the [Kirchhoff PDEs](https://colab.research.google.com/drive/1_PmLv8OWh9GZTzqPgFpCXYYBlxGqszOn?usp=sharing).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在实现 ReLoBRaLo 的笔记本中找到完整的代码，适用于 [Helmholtz](https://colab.research.google.com/drive/1R6aPThhp1wrQVaydl8pj1G0s4gLiH8Wc?usp=sharing)
    和 [Kirchhoff PDEs](https://colab.research.google.com/drive/1_PmLv8OWh9GZTzqPgFpCXYYBlxGqszOn?usp=sharing)。
- en: '[PRE1]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Thank you a lot for reading until the end of this article! If you found this
    article helpful and would like to use ReLoBRaLo or the notebooks in your own work,
    please use [this citation](https://ui.adsabs.harvard.edu/abs/2021arXiv211009813B/exportcitation).
    You can find more information about me on [rabischof.ch](http://rabischof.ch)
    and my colleague on [mkrausai.com](http://mkrausai.com).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢你阅读到本文的最后！如果你觉得这篇文章对你有帮助，并希望在自己的工作中使用 ReLoBRaLo 或笔记本，请使用 [此引用](https://ui.adsabs.harvard.edu/abs/2021arXiv211009813B/exportcitation)。你可以在
    [rabischof.ch](http://rabischof.ch) 和我的同事在 [mkrausai.com](http://mkrausai.com)
    上找到更多关于我的信息。
- en: '[1] Rafael Bischof and Michael Kraus. Multi-objective loss balancing for physics-informed
    deep learning. arXiv preprint arXiv:2110.09813, 2021.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Rafael Bischof 和 Michael Kraus. 面向物理的深度学习的多目标损失平衡。arXiv 预印本 arXiv:2110.09813,
    2021。'
- en: '[2] M. Raissi, P. Perdikaris, and G. E. Karniadakis, Physics-informed neural
    networks: A deep learning framework for solving forward and inverse problems involving
    nonlinear partial differential equations, Journal of Computational Physics 378
    (2019), 686–707.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] M. Raissi, P. Perdikaris, 和 G. E. Karniadakis，《物理信息神经网络：解决涉及非线性偏微分方程的正向和逆向问题的深度学习框架》，《计算物理学杂志》378
    (2019)，686–707。'
- en: '[3] H. Lee and I. S. Kang, Neural algorithm for solving differential equations,
    J. Comput. Phys. 91 (1990), no. 1, 110–131'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] H. Lee 和 I. S. Kang，《求解微分方程的神经算法》，《计算物理学杂志》91 (1990)，第 1 期，110–131'
- en: '[4] Wang, S., Teng, Y., and Perdikaris, P. Understanding and mitigating gradient
    pathologies in physics-informed neural networks. arXiv e-prints (Jan. 2020), arXiv:2001.04536.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Wang, S., Teng, Y., 和 Perdikaris, P. 理解和缓解物理信息神经网络中的梯度病态。arXiv 预印本 (2020年1月)，arXiv:2001.04536。'
- en: '[5] Chen, Z., Badrinarayanan, V., Lee, C.-Y., and Rabinovich, A. GradNorm:
    Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks.
    arXiv e-prints (Nov. 2017), arXiv:1711.02257.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Chen, Z., Badrinarayanan, V., Lee, C.-Y., 和 Rabinovich, A. GradNorm: 用于深度多任务网络的自适应损失平衡的梯度归一化。arXiv
    预印本 (2017年11月)，arXiv:1711.02257。'
- en: '[6] Heydari, A. A., Thompson, C. A., and Mehmood, A. SoftAdapt: Techniques
    for Adaptive Loss Weighting of Neural Networks with Multi-Part Loss Functions.
    arXiv e-prints (Dec. 2019), arXiv:1912.12355.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Heydari, A. A., Thompson, C. A., 和 Mehmood, A. SoftAdapt: 用于多部分损失函数的神经网络的自适应损失加权技术。arXiv
    预印本 (2019年12月)，arXiv:1912.12355。'
