- en: Building LLMs-Powered Apps with OPL Stack
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨OPLå †æ ˆæ„å»ºLLMsé©±åŠ¨çš„åº”ç”¨ç¨‹åº
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/building-llms-powered-apps-with-opl-stack-c1d31b17110f?source=collection_archive---------2-----------------------#2023-04-03](https://towardsdatascience.com/building-llms-powered-apps-with-opl-stack-c1d31b17110f?source=collection_archive---------2-----------------------#2023-04-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/building-llms-powered-apps-with-opl-stack-c1d31b17110f?source=collection_archive---------2-----------------------#2023-04-03](https://towardsdatascience.com/building-llms-powered-apps-with-opl-stack-c1d31b17110f?source=collection_archive---------2-----------------------#2023-04-03)
- en: 'OPL: OpenAI, Pinecone, and Langchain for knowledge-based AI assistant'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OPLï¼šOpenAIã€Pinecone å’Œ Langchain ç”¨äºçŸ¥è¯†é©±åŠ¨çš„AIåŠ©æ‰‹
- en: '[](https://medium.com/@wen_yang?source=post_page-----c1d31b17110f--------------------------------)[![Wen
    Yang](../Images/5eac438762d015a0ab128757cc951967.png)](https://medium.com/@wen_yang?source=post_page-----c1d31b17110f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c1d31b17110f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c1d31b17110f--------------------------------)
    [Wen Yang](https://medium.com/@wen_yang?source=post_page-----c1d31b17110f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@wen_yang?source=post_page-----c1d31b17110f--------------------------------)[![Wen
    Yang](../Images/5eac438762d015a0ab128757cc951967.png)](https://medium.com/@wen_yang?source=post_page-----c1d31b17110f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c1d31b17110f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c1d31b17110f--------------------------------)
    [Wen Yang](https://medium.com/@wen_yang?source=post_page-----c1d31b17110f--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcbb5383bd438&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-llms-powered-apps-with-opl-stack-c1d31b17110f&user=Wen+Yang&userId=cbb5383bd438&source=post_page-cbb5383bd438----c1d31b17110f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c1d31b17110f--------------------------------)
    Â·12 min readÂ·Apr 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc1d31b17110f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-llms-powered-apps-with-opl-stack-c1d31b17110f&user=Wen+Yang&userId=cbb5383bd438&source=-----c1d31b17110f---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcbb5383bd438&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-llms-powered-apps-with-opl-stack-c1d31b17110f&user=Wen+Yang&userId=cbb5383bd438&source=post_page-cbb5383bd438----c1d31b17110f---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c1d31b17110f--------------------------------)
    Â·12 min é˜…è¯»Â·2023å¹´4æœˆ3æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc1d31b17110f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-llms-powered-apps-with-opl-stack-c1d31b17110f&user=Wen+Yang&userId=cbb5383bd438&source=-----c1d31b17110f---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc1d31b17110f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-llms-powered-apps-with-opl-stack-c1d31b17110f&source=-----c1d31b17110f---------------------bookmark_footer-----------)![](../Images/017e9316f7ecbd4f794d51802ab23255.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc1d31b17110f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-llms-powered-apps-with-opl-stack-c1d31b17110f&source=-----c1d31b17110f---------------------bookmark_footer-----------)![](../Images/017e9316f7ecbd4f794d51802ab23255.png)'
- en: 'Midjourney Prompt: a girl building a lego bridge from multiple blocks'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Midjourney æç¤ºï¼šä¸€ä¸ªå¥³å­©ç”¨å¤šä¸ªç§¯æœ¨å—å»ºé€ ä¹é«˜æ¡¥æ¢
- en: 'I remember a month ago, Eugene Yan posted a [poll](https://www.linkedin.com/posts/eugeneyan_activity-7029289248209977344-oteC?utm_source=share&utm_medium=member_desktop)
    on Linkedin:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è®°å¾—ä¸€ä¸ªæœˆå‰ï¼ŒEugene Yan åœ¨ LinkedIn ä¸Šå‘å¸ƒäº†ä¸€é¡¹ [æŠ•ç¥¨](https://www.linkedin.com/posts/eugeneyan_activity-7029289248209977344-oteC?utm_source=share&utm_medium=member_desktop)ï¼š
- en: Are you feeling the FOMO from not working on LLMs/Generative AI?
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä½ æ˜¯å¦æ„Ÿåˆ°å› ä¸ºæ²¡æœ‰å‚ä¸LLMs/ç”Ÿæˆå‹AIè€Œé”™å¤±è‰¯æœºï¼Ÿ
- en: 'Most answered â€œYesâ€. Itâ€™s easy to understand why, given the sweeping attention
    generated by chatGPT and now the release of gpt-4\. People describe the rise of
    Large Language Models (LLMs) feels like the iPhone moment. Yet I think thereâ€™s
    really no need to feel the FOMO. Consider this: missing out on the opportunity
    to develop iPhones doesnâ€™t preclude the ample potential for creating innovative
    iPhone apps. So too with LLMs. We have just entered the dawn of a new era and
    now itâ€™s the perfect time to harness the magic of integrating LLMs to build powerful
    applications.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•°äººå›ç­”äº†â€œæ˜¯â€ã€‚è€ƒè™‘åˆ°chatGPTå¼•å‘çš„å¹¿æ³›å…³æ³¨ä»¥åŠç°åœ¨gpt-4çš„å‘å¸ƒï¼Œå¾ˆå®¹æ˜“ç†è§£ä¸ºä»€ä¹ˆä¼šè¿™æ ·ã€‚äººä»¬å½¢å®¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å´›èµ·å°±åƒiPhoneçš„æ—¶åˆ»ã€‚ç„¶è€Œï¼Œæˆ‘è®¤ä¸ºçœŸçš„æ²¡æœ‰å¿…è¦æ„Ÿåˆ°FOMOã€‚è€ƒè™‘ä¸€ä¸‹ï¼šé”™è¿‡äº†å¼€å‘iPhonesçš„æœºä¼šå¹¶ä¸æ’é™¤åˆ›é€ åˆ›æ–°iPhoneåº”ç”¨ç¨‹åºçš„å·¨å¤§æ½œåŠ›ã€‚LLMsä¹Ÿæ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬åˆšåˆšè¿›å…¥äº†ä¸€ä¸ªæ–°æ—¶ä»£ï¼Œç°åœ¨æ­£æ˜¯åˆ©ç”¨LLMsæ•´åˆæ„å»ºå¼ºå¤§åº”ç”¨ç¨‹åºçš„ç»ä½³æ—¶æœºã€‚
- en: 'In this post, Iâ€™ll cover below topics:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†æ¶µç›–ä»¥ä¸‹ä¸»é¢˜ï¼š
- en: What is the OPL stack?
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯OPLæŠ€æœ¯æ ˆï¼Ÿ
- en: How to use the OPL to build chatGPT with domain knowledge? (Essential components
    with code walkthrough)
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚ä½•ä½¿ç”¨OPLæ„å»ºå…·æœ‰é¢†åŸŸçŸ¥è¯†çš„chatGPTï¼Ÿï¼ˆåŒ…å«ä»£ç æ¼”ç¤ºçš„å…³é”®ç»„ä»¶ï¼‰
- en: Production considerations
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç”Ÿäº§è€ƒè™‘å› ç´ 
- en: Common misconceptions
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¸¸è§è¯¯è§£
- en: 1\. What is the OPL stack?
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. ä»€ä¹ˆæ˜¯OPLæŠ€æœ¯æ ˆï¼Ÿ
- en: '![](../Images/5efe1d72069d166bbe99a2cee6c8a240.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5efe1d72069d166bbe99a2cee6c8a240.png)'
- en: Image created by the author
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…åˆ›å»ºçš„å›¾åƒ
- en: '**OPL stands for OpenAI, Pinecone, and Langchain,** which has increasingly
    become the industry solution toovercome the two limitations of LLMs:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**OPLä»£è¡¨OpenAIã€Pineconeå’ŒLangchainï¼Œ** å®ƒå·²é€æ¸æˆä¸ºå…‹æœLLMsä¸¤ä¸ªå±€é™æ€§çš„è¡Œä¸šè§£å†³æ–¹æ¡ˆï¼š'
- en: '**LLMs hallucination:** chatGPT will sometimes provide wrong answers with overconfidence.
    One of the underlying causes is that those language models are trained to predict
    the next word very effectively, or the next token to be precise. Given an input
    text, chatGPT will return words with high probability, which doesnâ€™t mean that
    chatGPT has reasoning ability.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**LLMså¹»è§‰ï¼š** chatGPTæœ‰æ—¶ä¼šæä¾›è¿‡åº¦è‡ªä¿¡çš„é”™è¯¯å›ç­”ã€‚ä¸€ä¸ªæ½œåœ¨çš„åŸå› æ˜¯è¿™äº›è¯­è¨€æ¨¡å‹è¢«è®­ç»ƒå¾—éå¸¸æœ‰æ•ˆåœ°é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼Œæˆ–è€…æ›´å‡†ç¡®åœ°è¯´æ˜¯ä¸‹ä¸€ä¸ªæ ‡è®°ã€‚ç»™å®šè¾“å…¥æ–‡æœ¬ï¼ŒchatGPTä¼šè¿”å›é«˜æ¦‚ç‡çš„è¯ï¼Œè¿™å¹¶ä¸æ„å‘³ç€chatGPTå…·æœ‰æ¨ç†èƒ½åŠ›ã€‚'
- en: '**Less up-to-date knowledge:** chatGPTâ€™s training data is limited to internet
    data prior to Sep 2021\. Therefore, it will produce less desirable answers if
    your questions are about recent trends or topics.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**çŸ¥è¯†æ›´æ–°ä¸å¤ŸåŠæ—¶ï¼š** chatGPTçš„è®­ç»ƒæ•°æ®ä»…é™äº2021å¹´9æœˆä¹‹å‰çš„äº’è”ç½‘æ•°æ®ã€‚å› æ­¤ï¼Œå¦‚æœä½ çš„é—®é¢˜æ¶‰åŠæœ€è¿‘çš„è¶‹åŠ¿æˆ–è¯é¢˜ï¼Œå®ƒå¯èƒ½ä¼šäº§ç”Ÿä¸å¤ªç†æƒ³çš„å›ç­”ã€‚'
- en: 'The common solution is to add a knowledge base on top of LLMs and use Langchain
    as a framework to build the pipeline. The essential components of each technology
    can be summarized below:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¸è§çš„è§£å†³æ–¹æ¡ˆæ˜¯åœ¨LLMsä¸Šæ·»åŠ çŸ¥è¯†åº“ï¼Œå¹¶ä½¿ç”¨Langchainä½œä¸ºæ„å»ºæµæ°´çº¿çš„æ¡†æ¶ã€‚æ¯é¡¹æŠ€æœ¯çš„å…³é”®ç»„ä»¶å¯ä»¥æ€»ç»“å¦‚ä¸‹ï¼š
- en: '**OpenAI**:'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenAI**ï¼š'
- en: '- provides API access to powerful LLMs such as chatGPT and gpt-4'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- æä¾›å¯¹å¼ºå¤§LLMså¦‚chatGPTå’Œgpt-4çš„APIè®¿é—®'
- en: '- provides embedding models to convert text to embeddings.'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- æä¾›å°†æ–‡æœ¬è½¬æ¢ä¸ºåµŒå…¥çš„æ¨¡å‹ã€‚'
- en: '**Pinecone**: it provides embedding vector storage, semantic similarity comparison,
    and fast retrieval.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pinecone**ï¼šæä¾›åµŒå…¥å‘é‡å­˜å‚¨ã€è¯­ä¹‰ç›¸ä¼¼åº¦æ¯”è¾ƒå’Œå¿«é€Ÿæ£€ç´¢ã€‚'
- en: '**Langchain**: it comprises 6 modules (`Models`, `Prompts`, `Indexes`, `Memory`,
    `Chains` and `Agents`).'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Langchain**ï¼šå®ƒåŒ…å«6ä¸ªæ¨¡å—ï¼ˆ`Models`ã€`Prompts`ã€`Indexes`ã€`Memory`ã€`Chains`å’Œ`Agents`ï¼‰ã€‚'
- en: '- `Models` offers flexibility in embedding models, chat models, and LLMs, including
    but not limited to OpenAIâ€™s offerings. You can also use other models from Hugging
    Face like BLOOM and FLAN-T5.'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- `Models` æä¾›äº†çµæ´»çš„åµŒå…¥æ¨¡å‹ã€èŠå¤©æ¨¡å‹å’ŒLLMsï¼ŒåŒ…æ‹¬ä½†ä¸é™äºOpenAIçš„äº§å“ã€‚ä½ è¿˜å¯ä»¥ä½¿ç”¨Hugging Faceä¸Šçš„å…¶ä»–æ¨¡å‹ï¼Œå¦‚BLOOMå’ŒFLAN-T5ã€‚'
- en: '- `Memory` : there are a variety of ways to allow chatbots to remember past
    conversation memory. From my experience, entity memory works well and is efficient.'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- `Memory` : æœ‰å¤šç§æ–¹å¼å¯ä»¥è®©èŠå¤©æœºå™¨äººè®°ä½è¿‡å»çš„å¯¹è¯è®°å½•ã€‚æ ¹æ®æˆ‘çš„ç»éªŒï¼Œå®ä½“è®°å¿†æ•ˆæœå¥½ä¸”é«˜æ•ˆã€‚'
- en: '- `Chains` : If youâ€™re new to Langchain, Chains is a great starting point.
    It follows a pipeline-like structure to process user input, select the LLM model,
    apply a Prompt template, and search the relevant context from the knowledge base.'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- `Chains` : å¦‚æœä½ æ˜¯Langchainçš„æ–°æ‰‹ï¼ŒChainsæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ã€‚å®ƒéµå¾ªç±»ä¼¼æµæ°´çº¿çš„ç»“æ„æ¥å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œé€‰æ‹©LLMæ¨¡å‹ï¼Œåº”ç”¨Promptæ¨¡æ¿ï¼Œå¹¶ä»çŸ¥è¯†åº“ä¸­æœç´¢ç›¸å…³ä¸Šä¸‹æ–‡ã€‚'
- en: Next, Iâ€™ll walk through the app I built using the OPL stack.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘å°†ä»‹ç»æˆ‘ä½¿ç”¨OPLæŠ€æœ¯æ ˆæ„å»ºçš„åº”ç”¨ç¨‹åºã€‚
- en: 2\. How to use the OPL to build chatGPT with domain knowledge? (Essential components
    with code walkthrough)
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. å¦‚ä½•ä½¿ç”¨OPLæ„å»ºå…·æœ‰é¢†åŸŸçŸ¥è¯†çš„chatGPTï¼Ÿï¼ˆåŒ…å«ä»£ç æ¼”ç¤ºçš„å…³é”®ç»„ä»¶ï¼‰
- en: 'The app I built is called [chatOutside](https://outsidechat.streamlit.app/)
    , which has two primary sections:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ„å»ºçš„åº”ç”¨ç¨‹åºç§°ä¸º[chatOutside](https://outsidechat.streamlit.app/)ï¼Œå®ƒæœ‰ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼š
- en: '**chatGPT**: lets you chat with chatGPT directly, and the format is similar
    to a Q&A app, where you receive a single input and output at a time.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**chatGPT**ï¼šè®©ä½ ç›´æ¥ä¸chatGPTèŠå¤©ï¼Œæ ¼å¼ç±»ä¼¼äºé—®ç­”åº”ç”¨ï¼Œæ¯æ¬¡æ¥æ”¶ä¸€ä¸ªè¾“å…¥å’Œè¾“å‡ºã€‚'
- en: '**chatOutside**: allows you to chat with a version of chatGPT with expert knowledge
    of Outdoor activities and trends. The format is more like a chatbot style, where
    all messages are recorded as the conversation progresses. Iâ€™ve also included a
    section that provides source links, which can boost user confidence and is always
    useful to have.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**chatOutside**ï¼šè®©ä½ ä¸å…·æœ‰æˆ·å¤–æ´»åŠ¨åŠè¶‹åŠ¿ä¸“ä¸šçŸ¥è¯†çš„chatGPTç‰ˆæœ¬è¿›è¡Œå¯¹è¯ã€‚æ ¼å¼æ›´ç±»ä¼¼äºèŠå¤©æœºå™¨äººçš„é£æ ¼ï¼Œæ‰€æœ‰æ¶ˆæ¯åœ¨å¯¹è¯è¿‡ç¨‹ä¸­éƒ½ä¼šè¢«è®°å½•ã€‚æˆ‘è¿˜åŒ…å«äº†ä¸€ä¸ªæä¾›æºé“¾æ¥çš„éƒ¨åˆ†ï¼Œè¿™å¯ä»¥å¢å¼ºç”¨æˆ·ä¿¡å¿ƒï¼Œå¹¶ä¸”æ€»æ˜¯æœ‰ç”¨çš„ã€‚'
- en: 'As you can see, if you ask the same question: â€œWhatâ€™re the best running shoes
    in 2023? My budget is around $200â€. chatGPT will say â€œas an AI language model,
    I donâ€™t have access to information from the future.â€ While chatOutside will provide
    you with more up-to-date answers, along with source links.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œå¦‚æœä½ é—®åŒæ ·çš„é—®é¢˜ï¼šâ€œ2023å¹´æœ€å¥½çš„è·‘é‹æ˜¯ä»€ä¹ˆï¼Ÿæˆ‘çš„é¢„ç®—åœ¨$200å·¦å³ã€‚â€chatGPTä¼šè¯´â€œä½œä¸ºä¸€ä¸ªAIè¯­è¨€æ¨¡å‹ï¼Œæˆ‘æ— æ³•è®¿é—®æœªæ¥çš„ä¿¡æ¯ã€‚â€è€ŒchatOutsideä¼šä¸ºä½ æä¾›æ›´åŠæ—¶çš„ç­”æ¡ˆï¼Œå¹¶é™„ä¸Šæºé“¾æ¥ã€‚
- en: '![](../Images/90480e9dfc8f81b47e0fe6bf4e2e3f4b.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90480e9dfc8f81b47e0fe6bf4e2e3f4b.png)'
- en: 'There are three major steps involved in the development process:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å¼€å‘è¿‡ç¨‹æ¶‰åŠä¸‰ä¸ªä¸»è¦æ­¥éª¤ï¼š
- en: 'Step 1: Build an Outside Knowledge Base in Pinecone'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ­¥ï¼šåœ¨Pineconeä¸­æ„å»ºå¤–éƒ¨çŸ¥è¯†åº“
- en: 'Step 2: Use Langchain for Question & Answering Service'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¬¬äºŒæ­¥ï¼šä½¿ç”¨Langchainè¿›è¡Œé—®ç­”æœåŠ¡
- en: 'Step 3: Build our app in Streamlit'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰æ­¥ï¼šåœ¨Streamlitä¸­æ„å»ºæˆ‘ä»¬çš„åº”ç”¨ç¨‹åº
- en: Implementation details for each step are discussed below.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªæ­¥éª¤çš„å®æ–½ç»†èŠ‚å°†åœ¨ä¸‹é¢è®¨è®ºã€‚
- en: '**Step 1:** Build an Outside Knowledge Base in Pinecone'
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤1ï¼š** åœ¨Pineconeä¸­æ„å»ºå¤–éƒ¨çŸ¥è¯†åº“'
- en: '**Step 1.1**: I connected to our Outside catalog database and selected articles
    published between January 1st, 2022, and March 29th, 2023\. This provided us with
    approximately 20,000 records.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤1.1ï¼š** æˆ‘è¿æ¥åˆ°æˆ‘ä»¬çš„å¤–éƒ¨ç›®å½•æ•°æ®åº“ï¼Œå¹¶é€‰æ‹©äº†åœ¨2022å¹´1æœˆ1æ—¥è‡³2023å¹´3æœˆ29æ—¥ä¹‹é—´å‘å¸ƒçš„æ–‡ç« ã€‚è¿™ä¸ºæˆ‘ä»¬æä¾›äº†å¤§çº¦20,000æ¡è®°å½•ã€‚'
- en: '![](../Images/a55bed6eacdefdd3ad7e8a6bb57cae7c.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a55bed6eacdefdd3ad7e8a6bb57cae7c.png)'
- en: sample data preview from Outside
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å¤–éƒ¨çš„ç¤ºä¾‹æ•°æ®é¢„è§ˆ
- en: Next, we need to perform two data transformations.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦è¿›è¡Œä¸¤ä¸ªæ•°æ®è½¬æ¢ã€‚
- en: '**Step 1.2:** convert the above dataframe to a list of dictionaries to ensure
    data can be upserted correctly into Pinecone.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤1.2ï¼š** å°†ä¸Šè¿°æ•°æ®æ¡†è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨ï¼Œä»¥ç¡®ä¿æ•°æ®å¯ä»¥æ­£ç¡®åœ°æ’å…¥åˆ°Pineconeä¸­ã€‚'
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Step 1.3:** Split the `content`into smaller chunks using Langchainâ€™s `RecursiveCharacterTextSplitter`
    . The benefit of breaking down documents into smaller chunks is twofold:'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤1.3ï¼š** ä½¿ç”¨Langchainçš„`RecursiveCharacterTextSplitter`å°†`content`æ‹†åˆ†æˆæ›´å°çš„å—ã€‚å°†æ–‡æ¡£æ‹†åˆ†ä¸ºæ›´å°çš„å—æœ‰ä¸¤ä¸ªå¥½å¤„ï¼š'
- en: '- A typical article might be more than 1000 characters, which is very long.
    Imagine we want to retrieve top-3 articles as context to prompt the chatGPT, we
    could easily hit the 4000 token limit.'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- ä¸€ç¯‡å…¸å‹æ–‡ç« å¯èƒ½è¶…è¿‡1000ä¸ªå­—ç¬¦ï¼Œè¿™éå¸¸é•¿ã€‚æƒ³è±¡ä¸€ä¸‹æˆ‘ä»¬æƒ³æ£€ç´¢å‰3ç¯‡æ–‡ç« ä½œä¸ºæç¤ºç»™chatGPTï¼Œæˆ‘ä»¬å¾ˆå®¹æ˜“å°±ä¼šè¶…è¿‡4000ä¸ªå­—å…ƒé™åˆ¶ã€‚'
- en: '- Smaller chunks provide more relevant information, resulting in better context
    to prompt chatGPT.'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- æ›´å°çš„å—æä¾›æ›´ç›¸å…³çš„ä¿¡æ¯ï¼Œä»è€Œä¸ºchatGPTæä¾›æ›´å¥½çš„æç¤ºä¸Šä¸‹æ–‡ã€‚'
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: After splitting, each recordâ€™s content was broken down into multiple chunks,
    each having less than 400 tokens.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†å‰²åï¼Œæ¯æ¡è®°å½•çš„å†…å®¹è¢«æ‹†åˆ†æˆå¤šä¸ªéƒ¨åˆ†ï¼Œæ¯ä¸ªéƒ¨åˆ†å°‘äº400ä¸ªå­—å…ƒã€‚
- en: '![](../Images/309b912e454adbd6526dcc71bbd2aab6.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/309b912e454adbd6526dcc71bbd2aab6.png)'
- en: Break the content into multiple chunks
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å°†å†…å®¹æ‹†åˆ†ä¸ºå¤šä¸ªå—
- en: One thing worth noting is that the text splitter used is called `RecursiveCharacterTextSplitter`
    , which is recommended use by Harrison Chase, the creator of Langchain. The basic
    idea is to first split by the paragraph, then split by sentence, with overlapping
    (20 tokens). This helps preserve meaningful information and context from the surrounding
    sentences.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨çš„æ–‡æœ¬åˆ†å‰²å™¨ç§°ä¸º`RecursiveCharacterTextSplitter`ï¼Œè¿™æ˜¯Langchainçš„åˆ›å»ºè€…Harrison Chaseæ¨èä½¿ç”¨çš„ã€‚åŸºæœ¬æ€è·¯æ˜¯é¦–å…ˆæŒ‰æ®µè½æ‹†åˆ†ï¼Œç„¶åæŒ‰å¥å­æ‹†åˆ†ï¼Œé‡å ï¼ˆ20å­—å…ƒï¼‰ã€‚è¿™æœ‰åŠ©äºä¿ç•™æ¥è‡ªå‘¨å›´å¥å­çš„æœ‰æ„ä¹‰ä¿¡æ¯å’Œä¸Šä¸‹æ–‡ã€‚
- en: '**Step 1.4:** Upsert data to Pinecone. The below code is adapted from [James
    Briggs](https://medium.com/u/b9d77a4ca1d1?source=post_page-----c1d31b17110f--------------------------------)â€™s
    wonderful [tutorial](https://github.com/pinecone-io/examples/blob/master/generation/langchain/handbook/05-langchain-retrieval-augmentation.ipynb).'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 1.4ï¼š** å°†æ•°æ®æ’å…¥ Pineconeã€‚ä¸‹é¢çš„ä»£ç æ”¹ç¼–è‡ª [James Briggs](https://medium.com/u/b9d77a4ca1d1?source=post_page-----c1d31b17110f--------------------------------)
    çš„ç²¾å½© [æ•™ç¨‹](https://github.com/pinecone-io/examples/blob/master/generation/langchain/handbook/05-langchain-retrieval-augmentation.ipynb)ã€‚'
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We batch upload and embed all articles. which took about 20 minutes to upsert
    20k records. Be sure to adjust the `tqdm`import accordingly based on your env
    (you donâ€™t need to import both!)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ‰¹é‡ä¸Šä¼ å¹¶åµŒå…¥æ‰€æœ‰æ–‡ç« ï¼Œè¿™èŠ±è´¹äº†å¤§çº¦ 20 åˆ†é’Ÿæ¥æ’å…¥ 2 ä¸‡æ¡è®°å½•ã€‚è¯·æ ¹æ®ä½ çš„ç¯å¢ƒè°ƒæ•´ `tqdm` å¯¼å…¥ï¼ˆä½ ä¸éœ€è¦åŒæ—¶å¯¼å…¥ä¸¤ä¸ªï¼ï¼‰
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: After upserting the Outside articles data, we can inspect our pinecone index
    by using `index.describe_index_stats()` . One of the stats to pay attention to
    is `index_fullness`, which was 0.2 in our case. This means the Pinecone pod was
    20% full, suggesting that a single p1 pod can store approximately 100k articles.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å°† Outside æ–‡ç« æ•°æ®æ’å…¥åï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ `index.describe_index_stats()` æ£€æŸ¥æˆ‘ä»¬çš„ Pinecone ç´¢å¼•ã€‚éœ€è¦æ³¨æ„çš„ç»Ÿè®¡æ•°æ®ä¹‹ä¸€æ˜¯
    `index_fullness`ï¼Œåœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ä¸º 0.2ã€‚è¿™æ„å‘³ç€ Pinecone pod å·²æ»¡ 20%ï¼Œæš—ç¤ºä¸€ä¸ª p1 pod å¤§çº¦å¯ä»¥å­˜å‚¨ 10 ä¸‡ç¯‡æ–‡ç« ã€‚
- en: '![](../Images/e58bbd39a40d1f5ca391d0328c3206d7.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e58bbd39a40d1f5ca391d0328c3206d7.png)'
- en: After upserting data into Pinecone
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å°†æ•°æ®æ’å…¥ Pinecone å
- en: 'Step 2: Use Langchain for Question & Answering Service'
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ­¥éª¤ 2ï¼šä½¿ç”¨ Langchain è¿›è¡Œé—®ç­”æœåŠ¡
- en: '*Note: Langchain updates so fast these days, the version used below code is*
    `*0.0.118*` *.*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ³¨æ„ï¼šLangchain æœ€è¿‘æ›´æ–°éå¸¸å¿«ï¼Œä¸‹é¢ä»£ç ä½¿ç”¨çš„ç‰ˆæœ¬æ˜¯* `*0.0.118*` *ã€‚*'
- en: '![](../Images/84f252a1857dc61124514ae1c921ecfe.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84f252a1857dc61124514ae1c921ecfe.png)'
- en: Data flow in OPL stack
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: OPL å †æ ˆä¸­çš„æ•°æ®æµ
- en: 'The above sketchnote illustrates how data flows during the inference stage:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šé¢çš„è‰å›¾è¯´æ˜äº†æ¨ç†é˜¶æ®µæ•°æ®æµåŠ¨çš„æ–¹å¼ï¼š
- en: 'The user asks a question: â€œWhat are the best running shoes in 2023?â€.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”¨æˆ·æé—®ï¼šâ€œ2023 å¹´æœ€å¥½çš„è·‘é‹æ˜¯ä»€ä¹ˆï¼Ÿâ€
- en: The question is converted into embedding using the `ada-002`model.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é—®é¢˜ä½¿ç”¨ `ada-002` æ¨¡å‹è½¬æ¢ä¸ºåµŒå…¥ã€‚
- en: The user question embedding is compared with all vectors stored in Pinecone
    using `similarity_search`function, which retrieves the top 3 text chunks that
    are most likely to answer the question.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”¨æˆ·é—®é¢˜åµŒå…¥ä¸ Pinecone ä¸­å­˜å‚¨çš„æ‰€æœ‰å‘é‡é€šè¿‡ `similarity_search` å‡½æ•°è¿›è¡Œæ¯”è¾ƒï¼Œè¯¥å‡½æ•°æ£€ç´¢æœ€æœ‰å¯èƒ½å›ç­”é—®é¢˜çš„å‰ä¸‰ä¸ªæ–‡æœ¬å—ã€‚
- en: Langchain then passes the top 3 text chunks as `context` , along with the user
    question to gpt-3.5 ( `ChatCompletion` ) to generate the answers.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Langchain ç„¶åå°†å‰ä¸‰ä¸ªæ–‡æœ¬å—ä½œä¸º `context`ï¼Œä¸ç”¨æˆ·é—®é¢˜ä¸€èµ·ä¼ é€’ç»™ gpt-3.5ï¼ˆ`ChatCompletion`ï¼‰ç”Ÿæˆç­”æ¡ˆã€‚
- en: 'All can be achieved with less than 30 lines of code:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è¿™äº›éƒ½å¯ä»¥é€šè¿‡ä¸åˆ° 30 è¡Œä»£ç å®ç°ï¼š
- en: '[PRE4]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now we can test by asking a hiking-related question: â€œCan you recommend some
    advanced hiking trails with views of water in California bay area?â€'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥é€šè¿‡æå‡ºä¸€ä¸ªä¸å¾’æ­¥æ—…è¡Œç›¸å…³çš„é—®é¢˜è¿›è¡Œæµ‹è¯•ï¼šâ€œä½ èƒ½æ¨èä¸€äº›åŠ å·æ¹¾åŒºå¸¦æ°´æ™¯çš„é«˜çº§å¾’æ­¥æ—…è¡Œè·¯çº¿å—ï¼Ÿâ€
- en: '![](../Images/20894ccdec7880545e25efa3b0beb363.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20894ccdec7880545e25efa3b0beb363.png)'
- en: Langchain VectorDBQA with source
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Langchain VectorDBQA å¸¦æ¥æº
- en: 'Step 3: Build our app in Streamlit'
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ­¥éª¤ 3ï¼šåœ¨ Streamlit ä¸­æ„å»ºæˆ‘ä»¬çš„åº”ç”¨
- en: 'After verifying the logic is working in Jupyter notebook, we can assemble everything
    together and build a frontend using streamlit. In our streamlit app, there are
    two python files:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ Jupyter notebook ä¸­éªŒè¯é€»è¾‘æœ‰æ•ˆåï¼Œæˆ‘ä»¬å¯ä»¥å°†æ‰€æœ‰å†…å®¹æ•´åˆåœ¨ä¸€èµ·ï¼Œå¹¶ä½¿ç”¨ streamlit æ„å»ºå‰ç«¯ã€‚åœ¨æˆ‘ä»¬çš„ streamlit
    åº”ç”¨ä¸­ï¼Œæœ‰ä¸¤ä¸ª python æ–‡ä»¶ï¼š
- en: '- `app.py` : the main python file for frontend and power the app'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '- `app.py`ï¼šå‰ç«¯çš„ä¸»è¦ python æ–‡ä»¶ï¼Œé©±åŠ¨åº”ç”¨'
- en: '- `utils.py` : the supporting function which will be called by `app.py`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '- `utils.py`ï¼šç”± `app.py` è°ƒç”¨çš„æ”¯æŒå‡½æ•°'
- en: 'Hereâ€™s what my `utils.py` looks like:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯æˆ‘çš„ `utils.py` çœ‹èµ·æ¥çš„æ ·å­ï¼š
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'And finally, hereâ€™s what my `app.py`looks like:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œè¿™å°±æ˜¯æˆ‘çš„ `app.py` çœ‹èµ·æ¥çš„æ ·å­ï¼š
- en: '[PRE6]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 3\. Production considerations
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. ç”Ÿäº§è€ƒè™‘
- en: Alrighty, enough coding!
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½äº†ï¼Œå¤Ÿäº†ï¼
- en: 'The app is actually pretty nice already as it is. But if we want to move to
    production, there are a few additional things to consider:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: åº”ç”¨å®é™…ä¸Šå·²ç»å¾ˆä¸é”™äº†ã€‚ä½†æ˜¯å¦‚æœæˆ‘ä»¬æƒ³è¿›å…¥ç”Ÿäº§ç¯å¢ƒï¼Œè¿˜æœ‰ä¸€äº›é¢å¤–çš„äº‹é¡¹éœ€è¦è€ƒè™‘ï¼š
- en: 'Ingesting new and updated data in Pinecone: we did a one-time batch upsert
    for article data. In reality, new articles are added to our websites every day,
    and some fields may get updated for data already ingested into Pinecone. This
    is not a machine learning problem but itâ€™s ever-present for media companies: how
    to keep your data updated in every service. The potential solution is to set up
    a cron job to run the upsert and update the job periodically. Thereâ€™s an instruction
    on how to send upserts in parallel, which might be quite useful if we can use
    asynchronous tasks with Django and Celery.'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨ Pinecone ä¸­æ‘„å–æ–°çš„å’Œæ›´æ–°çš„æ•°æ®ï¼šæˆ‘ä»¬å¯¹æ–‡ç« æ•°æ®è¿›è¡Œäº†å•æ¬¡æ‰¹é‡æ’å…¥ã€‚å®é™…ä¸Šï¼Œæ¯å¤©éƒ½æœ‰æ–°çš„æ–‡ç« æ·»åŠ åˆ°æˆ‘ä»¬çš„ç½‘ç«™ä¸­ï¼Œè€Œä¸”ä¸€äº›å­—æ®µå¯èƒ½ä¼šæ›´æ–°å·²ç»æ‘„å–åˆ°
    Pinecone çš„æ•°æ®ã€‚è¿™ä¸æ˜¯æœºå™¨å­¦ä¹ çš„é—®é¢˜ï¼Œè€Œæ˜¯å¯¹åª’ä½“å…¬å¸è€Œè¨€å¸¸å¸¸å­˜åœ¨çš„é—®é¢˜ï¼šå¦‚ä½•ä¿æŒæ¯ä¸ªæœåŠ¡ä¸­çš„æ•°æ®æ›´æ–°ã€‚æ½œåœ¨çš„è§£å†³æ–¹æ¡ˆæ˜¯è®¾ç½®ä¸€ä¸ªå®šæ—¶ä»»åŠ¡æ¥å®šæœŸæ‰§è¡Œæ’å…¥å’Œæ›´æ–°ä»»åŠ¡ã€‚æœ‰å…³å¦‚ä½•å¹¶è¡Œå‘é€æ’å…¥æ“ä½œçš„æŒ‡ä»¤ï¼Œå¦‚æœæˆ‘ä»¬å¯ä»¥ä½¿ç”¨
    Django å’Œ Celery çš„å¼‚æ­¥ä»»åŠ¡ï¼Œè¿™å¯èƒ½ä¼šéå¸¸æœ‰ç”¨ã€‚
- en: 'Limitation for [Pinecone pod storage](https://docs.pinecone.io/docs/limits#:~:text=Pod%20storage%20capacity,5M%20vectors%20with%20768%20dimensions.):
    the app is currently using p1 pod, which can store up to 1M vectors with 768 dimensions,
    or roughly 500k vectors if we use OpenAIâ€™s `ada-002`embedding model (which has
    a dimension of 1536).'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Pinecone pod å­˜å‚¨](https://docs.pinecone.io/docs/limits#:~:text=Pod%20storage%20capacity,5M%20vectors%20with%20768%20dimensions.)çš„é™åˆ¶ï¼šå½“å‰åº”ç”¨ä½¿ç”¨çš„æ˜¯
    p1 podï¼Œèƒ½å¤Ÿå­˜å‚¨æœ€å¤š 1M ä¸ª 768 ç»´å‘é‡ï¼Œæˆ–è€…å¦‚æœæˆ‘ä»¬ä½¿ç”¨ OpenAI çš„ `ada-002` åµŒå…¥æ¨¡å‹ï¼ˆå…¶ç»´åº¦ä¸º 1536ï¼‰ï¼Œåˆ™å¤§çº¦èƒ½å­˜å‚¨
    50 ä¸‡ä¸ªå‘é‡ã€‚'
- en: 'Stream capability for faster response times: To reduce the perceived latency
    for users, it may be helpful to add stream capability to the app. This would mimic
    the chatGPT by returning generated output token by token, rather than showing
    the entire response at once. While this functionality works for REST API using
    LangChain function, it poses a unique challenge for us since we use GraphQL instead
    of REST.'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æµå¼å¤„ç†åŠŸèƒ½ä»¥æé«˜å“åº”é€Ÿåº¦ï¼šä¸ºäº†å‡å°‘ç”¨æˆ·æ„ŸçŸ¥çš„å»¶è¿Ÿï¼Œå¯èƒ½æœ‰åŠ©äºå‘åº”ç”¨æ·»åŠ æµå¼å¤„ç†åŠŸèƒ½ã€‚è¿™å°†æ¨¡æ‹Ÿ chatGPT æŒ‰ token è¿”å›ç”Ÿæˆçš„è¾“å‡ºï¼Œè€Œä¸æ˜¯ä¸€æ¬¡æ€§æ˜¾ç¤ºæ•´ä¸ªå“åº”ã€‚è™½ç„¶è¿™ç§åŠŸèƒ½åœ¨ä½¿ç”¨
    LangChain å‡½æ•°çš„ REST API ä¸­æœ‰æ•ˆï¼Œä½†å¯¹äºæˆ‘ä»¬æ¥è¯´ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨ GraphQL è€Œä¸æ˜¯ RESTï¼Œè¿™å¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚
- en: 4\. Common misconceptions and questions
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. å¸¸è§è¯¯è§£å’Œé—®é¢˜
- en: '**chatGPT remembers the internet data up to Sep 2021\. And it is retrieve answers
    based on memory.**'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**chatGPT è®°ä½äº†æˆªè‡³ 2021 å¹´ 9 æœˆçš„äº’è”ç½‘æ•°æ®ï¼Œå¹¶ä¸”åŸºäºè®°å¿†æ£€ç´¢ç­”æ¡ˆã€‚**'
- en: '- This is not how it works. After training, chatGPT deletes the data from memory
    and uses its 175 billion parameters (weights) to predict whatâ€™s the most probable
    token (text). It doesnâ€™t retrieve answers based on memory. Thatâ€™s why if you just
    copy the answer generated by chatGPT, itâ€™s unlikely that you can find any source
    from the internet.'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- è¿™å¹¶ä¸æ˜¯å®ƒçš„å·¥ä½œæ–¹å¼ã€‚è®­ç»ƒä¹‹åï¼ŒchatGPT ä»å†…å­˜ä¸­åˆ é™¤æ•°æ®ï¼Œå¹¶ä½¿ç”¨å…¶ 1750 äº¿ä¸ªå‚æ•°ï¼ˆæƒé‡ï¼‰æ¥é¢„æµ‹æœ€å¯èƒ½çš„ tokenï¼ˆæ–‡æœ¬ï¼‰ã€‚å®ƒä¸ä¼šåŸºäºè®°å¿†æ£€ç´¢ç­”æ¡ˆã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå¦‚æœä½ åªæ˜¯å¤åˆ¶
    chatGPT ç”Ÿæˆçš„ç­”æ¡ˆï¼Œä½ ä¸å¤ªå¯èƒ½æ‰¾åˆ°æ¥è‡ªäº’è”ç½‘çš„ä»»ä½•æ¥æºã€‚'
- en: '**We can train/fine-tune/prompt-engineering chatGPT.**'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æˆ‘ä»¬å¯ä»¥è®­ç»ƒ/å¾®è°ƒ/æç¤ºå·¥ç¨‹ chatGPTã€‚**'
- en: '- Training and fine-tuning Large Language Models meant actually changing the
    model parameters. You need to have access to the actual model file and guide the
    model for your specific use cases. In most cases, we wouldnâ€™t train or fine-tune
    chatGPT. Prompt engineering is all we need: providing extra context to chatGPT
    and allowing it to answer based on the contexts.'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- è®­ç»ƒå’Œå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹æ„å‘³ç€å®é™…æ›´æ”¹æ¨¡å‹å‚æ•°ã€‚ä½ éœ€è¦è®¿é—®å®é™…çš„æ¨¡å‹æ–‡ä»¶ï¼Œå¹¶æ ¹æ®ä½ çš„å…·ä½“ä½¿ç”¨æƒ…å†µæŒ‡å¯¼æ¨¡å‹ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¸ä¼šè®­ç»ƒæˆ–å¾®è°ƒ chatGPTã€‚æˆ‘ä»¬æ‰€éœ€è¦çš„åªæ˜¯æç¤ºå·¥ç¨‹ï¼šä¸º
    chatGPT æä¾›é¢å¤–çš„ä¸Šä¸‹æ–‡ï¼Œå¹¶è®©å®ƒæ ¹æ®è¿™äº›ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚'
- en: '**Whatâ€™s the difference between a token and a word?'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**token å’Œè¯æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ**'
- en: -** Token is a word piece. 100 tokens are roughly equal to 75 words. For example,
    â€œUnbelievableâ€ is one word but 3 tokens (un, belie, able).
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: -** Token æ˜¯ä¸€ä¸ªè¯ç‰‡æ®µã€‚100 ä¸ª token å¤§çº¦ç­‰äº 75 ä¸ªè¯ã€‚ä¾‹å¦‚ï¼Œâ€œUnbelievableâ€ æ˜¯ä¸€ä¸ªè¯ï¼Œä½†åŒ…å« 3 ä¸ª tokenï¼ˆun,
    belie, ableï¼‰ã€‚**
- en: '**What does the 4000-token limitation mean?'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**4000-token é™åˆ¶æ„å‘³ç€ä»€ä¹ˆï¼Ÿ**'
- en: -** OpenAI gpt-3.5 has a token limitation of 4096 for combining user input,
    context, and response. When using Langchainâ€™s memory, the total number of words
    used in (user question + context + memory + chatGPT response) needs to be less
    than 3000 words (4000 tokens).
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: -** OpenAI gpt-3.5 çš„ token é™åˆ¶ä¸º 4096ï¼Œç”¨äºç»“åˆç”¨æˆ·è¾“å…¥ã€ä¸Šä¸‹æ–‡å’Œå“åº”ã€‚å½“ä½¿ç”¨ Langchain çš„å†…å­˜æ—¶ï¼Œï¼ˆç”¨æˆ·é—®é¢˜
    + ä¸Šä¸‹æ–‡ + å†…å­˜ + chatGPT å“åº”ï¼‰ä½¿ç”¨çš„æ€»è¯æ•°éœ€è¦å°‘äº 3000 ä¸ªè¯ï¼ˆ4000 ä¸ª tokenï¼‰ã€‚**
- en: '- gpt-4 has a higher token limit but itâ€™s also 20X more expensive! (gpt-3.5:
    $0.002/1K tokens; gpt-4: $0.045/1K tokens assuming 500 for prompt and 500 for
    completion).'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- gpt-4 æœ‰æ›´é«˜çš„ token é™åˆ¶ï¼Œä½†ä¹Ÿè´µäº† 20 å€ï¼ï¼ˆgpt-3.5ï¼š$0.002/1000 tokensï¼›gpt-4ï¼š$0.045/1000
    tokensï¼Œå‡è®¾ 500 ä¸ªç”¨äºæç¤ºå’Œ 500 ä¸ªç”¨äºå®Œæˆï¼‰ã€‚'
- en: '**Do I have to use Vector Store like Pinecone?'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æˆ‘æ˜¯å¦å¿…é¡»ä½¿ç”¨åƒ Pinecone è¿™æ ·çš„å‘é‡å­˜å‚¨ï¼Ÿ**'
- en: -** No. Pinecone is not the only option for vector storage. Other vector store
    options include Chroma, FAISS, Redis, and more. Additionally, you donâ€™t always
    need a vector store. For example, if you want to build a Q&A for a specific website,
    you can crawl the web page and follow this [openai-cookbook-recipe](https://github.com/openai/openai-cookbook/blob/main/apps/web-crawl-q-and-a/web-qa.ipynb).
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: -** ä¸ï¼ŒPinecone ä¸æ˜¯å”¯ä¸€çš„å‘é‡å­˜å‚¨é€‰é¡¹ã€‚å…¶ä»–å‘é‡å­˜å‚¨é€‰é¡¹åŒ…æ‹¬ Chromaã€FAISSã€Redis ç­‰ã€‚æ­¤å¤–ï¼Œä½ å¹¶ä¸æ€»æ˜¯éœ€è¦å‘é‡å­˜å‚¨ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æƒ³ä¸ºç‰¹å®šç½‘ç«™æ„å»ºä¸€ä¸ªé—®ç­”ç³»ç»Ÿï¼Œä½ å¯ä»¥çˆ¬å–ç½‘é¡µå¹¶å‚è€ƒè¿™ä¸ª
    [openai-cookbook-recipe](https://github.com/openai/openai-cookbook/blob/main/apps/web-crawl-q-and-a/web-qa.ipynb)ã€‚
- en: Parting Words
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‘Šåˆ«çš„è¯
- en: Thank you for reading this lengthy post! If you have any questions or tips on
    using Langchain, please feel free to reach out.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢ä½ é˜…è¯»è¿™ç¯‡é•¿æ–‡ï¼å¦‚æœä½ å¯¹ä½¿ç”¨ Langchain æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·éšæ—¶è”ç³»æˆ‘ã€‚
- en: Also, I will be going to the [LLM Bootcamp](https://fullstackdeeplearning.com/llm-bootcamp/)
    where I hope to learn more best practices on productionize LLMs-powered apps.
    If you are interested in this topic, stay tuned for my future posts! ğŸ˜ƒ
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæˆ‘å°†å‚åŠ  [LLM Bootcamp](https://fullstackdeeplearning.com/llm-bootcamp/)ï¼Œå¸Œæœ›èƒ½å­¦ä¹ åˆ°æ›´å¤šå…³äºå°†
    LLMs åº”ç”¨åˆ°ç”Ÿäº§ä¸­çš„æœ€ä½³å®è·µã€‚å¦‚æœä½ å¯¹è¿™ä¸ªè¯é¢˜æ„Ÿå…´è¶£ï¼Œè¯·å…³æ³¨æˆ‘æœªæ¥çš„å¸–å­ï¼ğŸ˜ƒ
