- en: Building LLMs-Powered Apps with OPL Stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/building-llms-powered-apps-with-opl-stack-c1d31b17110f?source=collection_archive---------2-----------------------#2023-04-03](https://towardsdatascience.com/building-llms-powered-apps-with-opl-stack-c1d31b17110f?source=collection_archive---------2-----------------------#2023-04-03)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'OPL: OpenAI, Pinecone, and Langchain for knowledge-based AI assistant'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@wen_yang?source=post_page-----c1d31b17110f--------------------------------)[![Wen
    Yang](../Images/5eac438762d015a0ab128757cc951967.png)](https://medium.com/@wen_yang?source=post_page-----c1d31b17110f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c1d31b17110f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c1d31b17110f--------------------------------)
    [Wen Yang](https://medium.com/@wen_yang?source=post_page-----c1d31b17110f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcbb5383bd438&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-llms-powered-apps-with-opl-stack-c1d31b17110f&user=Wen+Yang&userId=cbb5383bd438&source=post_page-cbb5383bd438----c1d31b17110f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c1d31b17110f--------------------------------)
    ·12 min read·Apr 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc1d31b17110f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-llms-powered-apps-with-opl-stack-c1d31b17110f&user=Wen+Yang&userId=cbb5383bd438&source=-----c1d31b17110f---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc1d31b17110f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-llms-powered-apps-with-opl-stack-c1d31b17110f&source=-----c1d31b17110f---------------------bookmark_footer-----------)![](../Images/017e9316f7ecbd4f794d51802ab23255.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Midjourney Prompt: a girl building a lego bridge from multiple blocks'
  prefs: []
  type: TYPE_NORMAL
- en: 'I remember a month ago, Eugene Yan posted a [poll](https://www.linkedin.com/posts/eugeneyan_activity-7029289248209977344-oteC?utm_source=share&utm_medium=member_desktop)
    on Linkedin:'
  prefs: []
  type: TYPE_NORMAL
- en: Are you feeling the FOMO from not working on LLMs/Generative AI?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Most answered “Yes”. It’s easy to understand why, given the sweeping attention
    generated by chatGPT and now the release of gpt-4\. People describe the rise of
    Large Language Models (LLMs) feels like the iPhone moment. Yet I think there’s
    really no need to feel the FOMO. Consider this: missing out on the opportunity
    to develop iPhones doesn’t preclude the ample potential for creating innovative
    iPhone apps. So too with LLMs. We have just entered the dawn of a new era and
    now it’s the perfect time to harness the magic of integrating LLMs to build powerful
    applications.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this post, I’ll cover below topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the OPL stack?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to use the OPL to build chatGPT with domain knowledge? (Essential components
    with code walkthrough)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Production considerations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Common misconceptions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1\. What is the OPL stack?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/5efe1d72069d166bbe99a2cee6c8a240.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by the author
  prefs: []
  type: TYPE_NORMAL
- en: '**OPL stands for OpenAI, Pinecone, and Langchain,** which has increasingly
    become the industry solution toovercome the two limitations of LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**LLMs hallucination:** chatGPT will sometimes provide wrong answers with overconfidence.
    One of the underlying causes is that those language models are trained to predict
    the next word very effectively, or the next token to be precise. Given an input
    text, chatGPT will return words with high probability, which doesn’t mean that
    chatGPT has reasoning ability.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Less up-to-date knowledge:** chatGPT’s training data is limited to internet
    data prior to Sep 2021\. Therefore, it will produce less desirable answers if
    your questions are about recent trends or topics.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The common solution is to add a knowledge base on top of LLMs and use Langchain
    as a framework to build the pipeline. The essential components of each technology
    can be summarized below:'
  prefs: []
  type: TYPE_NORMAL
- en: '**OpenAI**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '- provides API access to powerful LLMs such as chatGPT and gpt-4'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- provides embedding models to convert text to embeddings.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Pinecone**: it provides embedding vector storage, semantic similarity comparison,
    and fast retrieval.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Langchain**: it comprises 6 modules (`Models`, `Prompts`, `Indexes`, `Memory`,
    `Chains` and `Agents`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '- `Models` offers flexibility in embedding models, chat models, and LLMs, including
    but not limited to OpenAI’s offerings. You can also use other models from Hugging
    Face like BLOOM and FLAN-T5.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- `Memory` : there are a variety of ways to allow chatbots to remember past
    conversation memory. From my experience, entity memory works well and is efficient.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- `Chains` : If you’re new to Langchain, Chains is a great starting point.
    It follows a pipeline-like structure to process user input, select the LLM model,
    apply a Prompt template, and search the relevant context from the knowledge base.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, I’ll walk through the app I built using the OPL stack.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. How to use the OPL to build chatGPT with domain knowledge? (Essential components
    with code walkthrough)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The app I built is called [chatOutside](https://outsidechat.streamlit.app/)
    , which has two primary sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '**chatGPT**: lets you chat with chatGPT directly, and the format is similar
    to a Q&A app, where you receive a single input and output at a time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**chatOutside**: allows you to chat with a version of chatGPT with expert knowledge
    of Outdoor activities and trends. The format is more like a chatbot style, where
    all messages are recorded as the conversation progresses. I’ve also included a
    section that provides source links, which can boost user confidence and is always
    useful to have.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As you can see, if you ask the same question: “What’re the best running shoes
    in 2023? My budget is around $200”. chatGPT will say “as an AI language model,
    I don’t have access to information from the future.” While chatOutside will provide
    you with more up-to-date answers, along with source links.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/90480e9dfc8f81b47e0fe6bf4e2e3f4b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are three major steps involved in the development process:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Build an Outside Knowledge Base in Pinecone'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 2: Use Langchain for Question & Answering Service'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 3: Build our app in Streamlit'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation details for each step are discussed below.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1:** Build an Outside Knowledge Base in Pinecone'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Step 1.1**: I connected to our Outside catalog database and selected articles
    published between January 1st, 2022, and March 29th, 2023\. This provided us with
    approximately 20,000 records.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/a55bed6eacdefdd3ad7e8a6bb57cae7c.png)'
  prefs: []
  type: TYPE_IMG
- en: sample data preview from Outside
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need to perform two data transformations.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1.2:** convert the above dataframe to a list of dictionaries to ensure
    data can be upserted correctly into Pinecone.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 1.3:** Split the `content`into smaller chunks using Langchain’s `RecursiveCharacterTextSplitter`
    . The benefit of breaking down documents into smaller chunks is twofold:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '- A typical article might be more than 1000 characters, which is very long.
    Imagine we want to retrieve top-3 articles as context to prompt the chatGPT, we
    could easily hit the 4000 token limit.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- Smaller chunks provide more relevant information, resulting in better context
    to prompt chatGPT.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: After splitting, each record’s content was broken down into multiple chunks,
    each having less than 400 tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/309b912e454adbd6526dcc71bbd2aab6.png)'
  prefs: []
  type: TYPE_IMG
- en: Break the content into multiple chunks
  prefs: []
  type: TYPE_NORMAL
- en: One thing worth noting is that the text splitter used is called `RecursiveCharacterTextSplitter`
    , which is recommended use by Harrison Chase, the creator of Langchain. The basic
    idea is to first split by the paragraph, then split by sentence, with overlapping
    (20 tokens). This helps preserve meaningful information and context from the surrounding
    sentences.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1.4:** Upsert data to Pinecone. The below code is adapted from [James
    Briggs](https://medium.com/u/b9d77a4ca1d1?source=post_page-----c1d31b17110f--------------------------------)’s
    wonderful [tutorial](https://github.com/pinecone-io/examples/blob/master/generation/langchain/handbook/05-langchain-retrieval-augmentation.ipynb).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We batch upload and embed all articles. which took about 20 minutes to upsert
    20k records. Be sure to adjust the `tqdm`import accordingly based on your env
    (you don’t need to import both!)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: After upserting the Outside articles data, we can inspect our pinecone index
    by using `index.describe_index_stats()` . One of the stats to pay attention to
    is `index_fullness`, which was 0.2 in our case. This means the Pinecone pod was
    20% full, suggesting that a single p1 pod can store approximately 100k articles.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e58bbd39a40d1f5ca391d0328c3206d7.png)'
  prefs: []
  type: TYPE_IMG
- en: After upserting data into Pinecone
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Use Langchain for Question & Answering Service'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Note: Langchain updates so fast these days, the version used below code is*
    `*0.0.118*` *.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/84f252a1857dc61124514ae1c921ecfe.png)'
  prefs: []
  type: TYPE_IMG
- en: Data flow in OPL stack
  prefs: []
  type: TYPE_NORMAL
- en: 'The above sketchnote illustrates how data flows during the inference stage:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The user asks a question: “What are the best running shoes in 2023?”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The question is converted into embedding using the `ada-002`model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The user question embedding is compared with all vectors stored in Pinecone
    using `similarity_search`function, which retrieves the top 3 text chunks that
    are most likely to answer the question.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Langchain then passes the top 3 text chunks as `context` , along with the user
    question to gpt-3.5 ( `ChatCompletion` ) to generate the answers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All can be achieved with less than 30 lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can test by asking a hiking-related question: “Can you recommend some
    advanced hiking trails with views of water in California bay area?”'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/20894ccdec7880545e25efa3b0beb363.png)'
  prefs: []
  type: TYPE_IMG
- en: Langchain VectorDBQA with source
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Build our app in Streamlit'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After verifying the logic is working in Jupyter notebook, we can assemble everything
    together and build a frontend using streamlit. In our streamlit app, there are
    two python files:'
  prefs: []
  type: TYPE_NORMAL
- en: '- `app.py` : the main python file for frontend and power the app'
  prefs: []
  type: TYPE_NORMAL
- en: '- `utils.py` : the supporting function which will be called by `app.py`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s what my `utils.py` looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally, here’s what my `app.py`looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 3\. Production considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Alrighty, enough coding!
  prefs: []
  type: TYPE_NORMAL
- en: 'The app is actually pretty nice already as it is. But if we want to move to
    production, there are a few additional things to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ingesting new and updated data in Pinecone: we did a one-time batch upsert
    for article data. In reality, new articles are added to our websites every day,
    and some fields may get updated for data already ingested into Pinecone. This
    is not a machine learning problem but it’s ever-present for media companies: how
    to keep your data updated in every service. The potential solution is to set up
    a cron job to run the upsert and update the job periodically. There’s an instruction
    on how to send upserts in parallel, which might be quite useful if we can use
    asynchronous tasks with Django and Celery.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Limitation for [Pinecone pod storage](https://docs.pinecone.io/docs/limits#:~:text=Pod%20storage%20capacity,5M%20vectors%20with%20768%20dimensions.):
    the app is currently using p1 pod, which can store up to 1M vectors with 768 dimensions,
    or roughly 500k vectors if we use OpenAI’s `ada-002`embedding model (which has
    a dimension of 1536).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Stream capability for faster response times: To reduce the perceived latency
    for users, it may be helpful to add stream capability to the app. This would mimic
    the chatGPT by returning generated output token by token, rather than showing
    the entire response at once. While this functionality works for REST API using
    LangChain function, it poses a unique challenge for us since we use GraphQL instead
    of REST.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 4\. Common misconceptions and questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**chatGPT remembers the internet data up to Sep 2021\. And it is retrieve answers
    based on memory.**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '- This is not how it works. After training, chatGPT deletes the data from memory
    and uses its 175 billion parameters (weights) to predict what’s the most probable
    token (text). It doesn’t retrieve answers based on memory. That’s why if you just
    copy the answer generated by chatGPT, it’s unlikely that you can find any source
    from the internet.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**We can train/fine-tune/prompt-engineering chatGPT.**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '- Training and fine-tuning Large Language Models meant actually changing the
    model parameters. You need to have access to the actual model file and guide the
    model for your specific use cases. In most cases, we wouldn’t train or fine-tune
    chatGPT. Prompt engineering is all we need: providing extra context to chatGPT
    and allowing it to answer based on the contexts.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**What’s the difference between a token and a word?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: -** Token is a word piece. 100 tokens are roughly equal to 75 words. For example,
    “Unbelievable” is one word but 3 tokens (un, belie, able).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**What does the 4000-token limitation mean?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: -** OpenAI gpt-3.5 has a token limitation of 4096 for combining user input,
    context, and response. When using Langchain’s memory, the total number of words
    used in (user question + context + memory + chatGPT response) needs to be less
    than 3000 words (4000 tokens).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- gpt-4 has a higher token limit but it’s also 20X more expensive! (gpt-3.5:
    $0.002/1K tokens; gpt-4: $0.045/1K tokens assuming 500 for prompt and 500 for
    completion).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Do I have to use Vector Store like Pinecone?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: -** No. Pinecone is not the only option for vector storage. Other vector store
    options include Chroma, FAISS, Redis, and more. Additionally, you don’t always
    need a vector store. For example, if you want to build a Q&A for a specific website,
    you can crawl the web page and follow this [openai-cookbook-recipe](https://github.com/openai/openai-cookbook/blob/main/apps/web-crawl-q-and-a/web-qa.ipynb).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Parting Words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thank you for reading this lengthy post! If you have any questions or tips on
    using Langchain, please feel free to reach out.
  prefs: []
  type: TYPE_NORMAL
- en: Also, I will be going to the [LLM Bootcamp](https://fullstackdeeplearning.com/llm-bootcamp/)
    where I hope to learn more best practices on productionize LLMs-powered apps.
    If you are interested in this topic, stay tuned for my future posts! 😃
  prefs: []
  type: TYPE_NORMAL
