- en: 'Sentence Transformers: Meanings in Disguise'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/sentence-transformers-meanings-in-disguise-323cf6ac1e52?source=collection_archive---------10-----------------------#2023-01-03](https://towardsdatascience.com/sentence-transformers-meanings-in-disguise-323cf6ac1e52?source=collection_archive---------10-----------------------#2023-01-03)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[NLP For Semantic Search](https://jamescalam.medium.com/list/nlp-for-semantic-search-d3a4b96a52fe)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How modern language models capture meaning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://jamescalam.medium.com/?source=post_page-----323cf6ac1e52--------------------------------)[![James
    Briggs](../Images/cb34b7011748e4d8607b7ff4a8510a93.png)](https://jamescalam.medium.com/?source=post_page-----323cf6ac1e52--------------------------------)[](https://towardsdatascience.com/?source=post_page-----323cf6ac1e52--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----323cf6ac1e52--------------------------------)
    [James Briggs](https://jamescalam.medium.com/?source=post_page-----323cf6ac1e52--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb9d77a4ca1d1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsentence-transformers-meanings-in-disguise-323cf6ac1e52&user=James+Briggs&userId=b9d77a4ca1d1&source=post_page-b9d77a4ca1d1----323cf6ac1e52---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----323cf6ac1e52--------------------------------)
    ·12 min read·Jan 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F323cf6ac1e52&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsentence-transformers-meanings-in-disguise-323cf6ac1e52&user=James+Briggs&userId=b9d77a4ca1d1&source=-----323cf6ac1e52---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F323cf6ac1e52&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsentence-transformers-meanings-in-disguise-323cf6ac1e52&source=-----323cf6ac1e52---------------------bookmark_footer-----------)![](../Images/edabd3baac27a7fd707b7855ea93c5c2.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Brian Suh](https://unsplash.com/@_briansuh?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral). Originally
    posted in the [NLP for Semantic Search ebook](https://www.pinecone.io/learn/sentence-embeddings/)
    at Pinecone (where the author is employed).
  prefs: []
  type: TYPE_NORMAL
- en: Transformers have wholly rebuilt the landscape of natural language processing
    (NLP). Before transformers, we had *okay* translation and language classification
    thanks to recurrent neural nets (RNNs) — their language comprehension was limited
    and led to many minor mistakes, and coherence over larger chunks of text was practically
    impossible.
  prefs: []
  type: TYPE_NORMAL
- en: Since the introduction of the first transformer model in the 2017 paper *‘Attention
    is all you need’* [1], NLP has moved from RNNs to models like BERT and GPT. These
    new models can answer questions, write articles *(maybe GPT-3 wrote this)*, enable
    incredibly intuitive semantic search — and much more.
  prefs: []
  type: TYPE_NORMAL
- en: The funny thing is, for many tasks, the latter parts of these models are the
    same as those in RNNs — often a couple of feedforward NNs that output model predictions.
  prefs: []
  type: TYPE_NORMAL
- en: It’s the *input* to these layers that changed. The [dense embeddings](https://www.pinecone.io/learn/dense-vector-embeddings-nlp/)
    created by transformer models are so much richer in information that we get massive
    performance benefits despite using the same final outward layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'These increasingly rich sentence embeddings can be used to quickly compare
    sentence similarity for various use cases. Such as:'
  prefs: []
  type: TYPE_NORMAL
