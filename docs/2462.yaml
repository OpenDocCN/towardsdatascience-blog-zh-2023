- en: How to Build a Fully Automated Data Drift Detection Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-build-a-fully-automated-data-drift-detection-pipeline-e9278584e58d?source=collection_archive---------2-----------------------#2023-08-01](https://towardsdatascience.com/how-to-build-a-fully-automated-data-drift-detection-pipeline-e9278584e58d?source=collection_archive---------2-----------------------#2023-08-01)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An Automate Guide to Detect and Handle Data Drift
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://khuyentran1476.medium.com/?source=post_page-----e9278584e58d--------------------------------)[![Khuyen
    Tran](../Images/98aa66025ad29b618e875c75f1c400a5.png)](https://khuyentran1476.medium.com/?source=post_page-----e9278584e58d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e9278584e58d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e9278584e58d--------------------------------)
    [Khuyen Tran](https://khuyentran1476.medium.com/?source=post_page-----e9278584e58d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F84a02493194a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-fully-automated-data-drift-detection-pipeline-e9278584e58d&user=Khuyen+Tran&userId=84a02493194a&source=post_page-84a02493194a----e9278584e58d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e9278584e58d--------------------------------)
    ·10 min read·Aug 1, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe9278584e58d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-fully-automated-data-drift-detection-pipeline-e9278584e58d&user=Khuyen+Tran&userId=84a02493194a&source=-----e9278584e58d---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe9278584e58d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-fully-automated-data-drift-detection-pipeline-e9278584e58d&source=-----e9278584e58d---------------------bookmark_footer-----------)![](../Images/c433a55c93060b21965daf484c047144.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Motivation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data drift occurs when the distribution of input features in the production
    environment differs from the training data, leading to potential inaccuracies
    and decreased model performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1a58af829c62bf069cfd2e99d4203f0d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: To mitigate the impact of data drift on model performance, we can design a workflow
    that detects drift, notifies the data team, and triggers model retraining.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/677b5b6931730a825eff3e9288beab99.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Workflows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The workflow comprises the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Fetch reference data from the Postgres database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the current production data from the web.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Detect data drift by comparing the reference and current data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Append the current data to the existing Postgres database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When there is data drift, the following actions are taken:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Send a Slack message to alert the data team.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrain the model to update its performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Push the updated model to S3 for storage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This workflow is scheduled to run at specific times, such as 11:00 AM every
    Monday.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b5b8d48a2b308df0226e2b5963b320e1.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, the workflow includes two types of tasks: data science and data engineering
    tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Data science tasks, represented by pink boxes, are performed by data scientists
    and involve data drift detection, data processing, and model training.
  prefs: []
  type: TYPE_NORMAL
- en: Data engineering tasks, represented by blue and purple boxes, are performed
    by data engineers and involve tasks related to data movement and sending notifications.
  prefs: []
  type: TYPE_NORMAL
- en: Data Science Tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Detect Data Drift
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To detect data drift, we will create a Python script that takes two CSV files
    “data/reference.csv” (reference data) and “data/current.csv” (current data).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/12ed1960bcc3d5295dacaa4c1ba12e6c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: We will use [Evidently](https://www.evidentlyai.com/), an open-source ML observability
    platform, to compare the reference data, serving as a baseline, with the current
    production data.
  prefs: []
  type: TYPE_NORMAL
- en: If dataset drift is detected, the “drift_detected” output will be True; otherwise,
    it will be False.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[Full code.](https://github.com/khuyentran1401/detect-data-drift-pipeline/blob/main/src/detect/detect_data_drift.py)'
  prefs: []
  type: TYPE_NORMAL
- en: Retrain the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we will create a Python script responsible for model training. This script
    takes the combined past and current data as input and saves the trained model
    as a “model.pkl” file.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d7a01c96325ffca329b924fe15b782c4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[Full code.](https://github.com/khuyentran1401/detect-data-drift-pipeline/blob/main/src/train/train_model.py)'
  prefs: []
  type: TYPE_NORMAL
- en: Push to GitHub
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After finishing developing these two scripts, data scientists can push them
    to GitHub, allowing data engineers to use them in creating workflows.
  prefs: []
  type: TYPE_NORMAL
- en: 'View the GitHub repository for these files here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/khuyentran1401/detect-data-drift-pipeline?source=post_page-----e9278584e58d--------------------------------)
    [## GitHub - khuyentran1401/detect-data-drift-pipeline: A pipeline to detect data
    drift and retrain the…'
  prefs: []
  type: TYPE_NORMAL
- en: A pipeline to detect data drift and retrain the model when there is drift -
    GitHub …
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/khuyentran1401/detect-data-drift-pipeline?source=post_page-----e9278584e58d--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Data Engineering Tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Popular orchestration libraries such as Airflow, Prefect, and Dagster require
    modifications to the Python code to use their functionalities.
  prefs: []
  type: TYPE_NORMAL
- en: When Python scripts are tightly integrated into the data workflows, the overall
    codebase can become more complex and harder to maintain. Without independent Python
    script development, data engineers may need to modify the data science code to
    add orchestration logic.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/64c035d84a1ea8d5f62045eacc58bb41.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, [Kestra](https://kestra.io/), an open-source library, allows
    you to develop your Python scripts independently and then ​​seamlessly incorporate
    them into data workflows using YAML files.
  prefs: []
  type: TYPE_NORMAL
- en: This way, data scientists can focus on model processing and training, while
    data engineers can focus on handling orchestration.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we will use Kestra to design a more modular and efficient workflow.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7723cb5f1e607561113267c99407f63a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Clone the [detect-data-drift-pipeline repository](https://github.com/khuyentran1401/detect-data-drift-pipeline)
    to get the docker-compose file for Kestra, then run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to [localhost:8080](http://localhost:8080/) to access and explore the
    Kestra UI.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1a838ca283c40d4d82b9376507f305fd.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Follow these [instructions](https://github.com/khuyentran1401/detect-data-drift-pipeline)
    to configure the required environment for this tutorial.
  prefs: []
  type: TYPE_NORMAL
- en: Before developing the target flows, let’s get familiar with Kestra by creating
    some simple flows.
  prefs: []
  type: TYPE_NORMAL
- en: Access Postgres Tables From a Python Script
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will create a flow that includes the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '`getReferenceTable`: Exports a CSV file from a Postgres table.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`saveReferenceToCSV`: Creates a local CSV file that can be accessed by the
    Python task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`runPythonScript`: Reads the local CSV file with Python.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To enable data passing between the `saveReferenceToCSV` and `runPythonScript`
    tasks, we will place these two tasks in the same working directory by enclosing
    them inside the `wdir` task.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/44e62914c8c5b0c999d474d9d9666370.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Executing the flow will show the following logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0229ba03295229c369b01a8935459d49.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Parameterize Flow with Inputs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s create another flow that can be parameterized with inputs. This flow
    will have the following inputs: `startDate`, `endDate`, and `dataURL`.'
  prefs: []
  type: TYPE_NORMAL
- en: The `getCurrentCSV` task can access these inputs using the `{{inputs.name}}`
    notation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/87d1ffd8c99d5157d0a0150b8d72fe34.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The values of these inputs can be specified in each flow execution.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d1b49a9a4e18af13b7733b23d54cbc33.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Load a CSV File into a Postgres Table
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following flow does the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '`getCurrentCSV`: Runs a Python script to create a CSV file in the working directory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`saveFiles`: Sends the CSV file from the working directory to Kestra''s internal
    storage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`saveToCurrentTable`: Loads the CSV file into a Postgres table.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/1de966b034e4885dacb6ac0314a3e300.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: After running this flow, you will see the resulting data in the “current” table
    within your Postgres database.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fbc70170398295680a9ac5bbb5719cd4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Run a File From a GitHub Repository
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This flow includes the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cloneRepository`: Clones a public GitHub repository'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`runPythonCommand`: Executes a Python script from a CLI'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both of these tasks will operate within the same working directory.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/6b2586cf15d9c1f6c82c384c0c489220.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'After running the flow, you will see the following logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0c4413d165a8d794188b80e3da871b71.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Run a Flow on Schedule
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will create another flow that runs a flow based on a specific schedule. The
    following flow runs at 11:00 AM every Monday.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Upload to S3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This flow includes the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '`createPickle`: Generates a pickle file in Python'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`savetoPickle`: Transfers the pickle file to Kestra''s internal storage'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`upload`: Uploads the pickle file to an S3 bucket'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/fd9fb0b08e7002ba19882ac87c1d13f4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: After running this flow, the `data.pkl` file will be uploaded to the "bike-sharing"
    bucket.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fb126a6249d2ca4bcaa71dd87ca96432.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Put Everything Together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Build a Flow to Detect Data Drift
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let’s combine what we have learned to create a flow to detect data drift.
    At 11:0 AM every Monday, this flow executes the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Fetches reference data from the Postgres database.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Runs a Python script to get the current production data from the web.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clones the GitHub repository containing the drift detection code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Runs a Python script to data drift by comparing the reference and current data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appends the current data to the existing Postgres database.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/47a206ff95c59d1fcf51066f79f58875.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Build a Flow to Send Slack Messages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we will create a flow to send Slack messages via a [Slack Webhook URL](https://api.slack.com/messaging/webhooks)
    when the `detectDataDrift` task inside the `detect-data-drift` flow returns `drift_detected=true`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/def45e005820f105f401b936d94c72c7.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: After running the `detect-data-drift` flow, the `send-slack-message` flow will
    send a message on Slack.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/354ca0f5c2948f71f2d0d3d6e697e04d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Build a Flow to Retrain the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Lastly, we will create a flow to retrain the model. This flow executes the
    following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Exports a CSV file from the current table in the Postgres database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clones the GitHub repository containing the model training code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Runs a Python script to train the model and generates a pickle file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uploads the pickle file to S3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/253b848b5ef7fa84acbb3d646ff324ac.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: After running this flow, the `model.pkl` file will be uploaded to the “bike-sharing”
    bucket.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/032f439354c4a375bd0af16db6b4a192.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Ideas to Extend This Workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rather than relying on scheduled data pulls to identify data drift, we can leverage
    [Grafana’s outgoing webhook](https://grafana.com/docs/oncall/latest/outgoing-webhooks/)
    and [Kestra’s inbound webhook](https://kestra.io/plugins/core/triggers/io.kestra.core.models.triggers.types.webhook)
    to establish real-time data monitoring and trigger a flow instantly when data
    drift occurs. This approach enables the detection of data drift as soon as it
    happens, eliminating the need to wait for a scheduled script to run.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4f595292d9b246c072b5f1a5e60c0e39.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Let me know in the comments how you think this workflow could be extended and
    what other use cases you would like to see in future content.
  prefs: []
  type: TYPE_NORMAL
- en: 'I love writing about data science concepts and playing with different data
    science tools. You can stay up-to-date with my latest posts by:'
  prefs: []
  type: TYPE_NORMAL
- en: Subscribing to my newsletter on [Data Science Simplified](https://mathdatasimplified.com/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connect with me on [LinkedIn](https://www.linkedin.com/in/khuyen-tran-1401/)
    and [Twitter](https://twitter.com/KhuyenTran16).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
