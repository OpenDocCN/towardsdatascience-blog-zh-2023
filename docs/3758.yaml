- en: Demystifying GQA — Grouped Query Attention for Efficient LLM Pre-training
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解密 GQA — 高效 LLM 预训练的分组查询注意力
- en: 原文：[https://towardsdatascience.com/demystifying-gqa-grouped-query-attention-3fb97b678e4a?source=collection_archive---------0-----------------------#2023-12-27](https://towardsdatascience.com/demystifying-gqa-grouped-query-attention-3fb97b678e4a?source=collection_archive---------0-----------------------#2023-12-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/demystifying-gqa-grouped-query-attention-3fb97b678e4a?source=collection_archive---------0-----------------------#2023-12-27](https://towardsdatascience.com/demystifying-gqa-grouped-query-attention-3fb97b678e4a?source=collection_archive---------0-----------------------#2023-12-27)
- en: The variant of multi-head attention powering LLMs like LLaMA-2, Mistral7B, etc.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 驱动像 LLaMA-2、Mistral7B 等大语言模型的多头注意力变体
- en: '[](https://bhavinjawade.medium.com/?source=post_page-----3fb97b678e4a--------------------------------)[![Bhavin
    Jawade](../Images/d10f204b2d0a8a3fd6887048ecaa307d.png)](https://bhavinjawade.medium.com/?source=post_page-----3fb97b678e4a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3fb97b678e4a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3fb97b678e4a--------------------------------)
    [Bhavin Jawade](https://bhavinjawade.medium.com/?source=post_page-----3fb97b678e4a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://bhavinjawade.medium.com/?source=post_page-----3fb97b678e4a--------------------------------)[![Bhavin
    Jawade](../Images/d10f204b2d0a8a3fd6887048ecaa307d.png)](https://bhavinjawade.medium.com/?source=post_page-----3fb97b678e4a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3fb97b678e4a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3fb97b678e4a--------------------------------)
    [Bhavin Jawade](https://bhavinjawade.medium.com/?source=post_page-----3fb97b678e4a--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F11a205eeb0d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-gqa-grouped-query-attention-3fb97b678e4a&user=Bhavin+Jawade&userId=11a205eeb0d3&source=post_page-11a205eeb0d3----3fb97b678e4a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3fb97b678e4a--------------------------------)
    ·6 min read·Dec 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3fb97b678e4a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-gqa-grouped-query-attention-3fb97b678e4a&user=Bhavin+Jawade&userId=11a205eeb0d3&source=-----3fb97b678e4a---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F11a205eeb0d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-gqa-grouped-query-attention-3fb97b678e4a&user=Bhavin+Jawade&userId=11a205eeb0d3&source=post_page-11a205eeb0d3----3fb97b678e4a---------------------post_header-----------)
    发布在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3fb97b678e4a--------------------------------)
    ·6分钟阅读·2023年12月27日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3fb97b678e4a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-gqa-grouped-query-attention-3fb97b678e4a&user=Bhavin+Jawade&userId=11a205eeb0d3&source=-----3fb97b678e4a---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3fb97b678e4a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-gqa-grouped-query-attention-3fb97b678e4a&source=-----3fb97b678e4a---------------------bookmark_footer-----------)![](../Images/b93c237b4616df1eddea573e0336115d.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3fb97b678e4a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-gqa-grouped-query-attention-3fb97b678e4a&source=-----3fb97b678e4a---------------------bookmark_footer-----------)![](../Images/b93c237b4616df1eddea573e0336115d.png)'
- en: A “Group” of Llamas (Source — Image created by the author using Dalle-3)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 一组“驼鹿”（来源 — 作者使用 Dalle-3 创建的图像）
- en: In the previous article on training large-scale models, we looked at [LoRA](/understanding-lora-low-rank-adaptation-for-finetuning-large-models-936bce1a07c6).
    In this article, we will examine another strategy adopted by different large language
    models for efficient training — Grouped Query Attention (GQA). In short, Grouped
    Query Attention (GQA) is a generalization of multi-head attention (MHA) and multi-query
    attention (MQA) — with each of them being a special case of GQA. Therefore, before
    we dive into Grouped Query Attention, let’s revisit traditional multi-head attention
    proposed by Vaswani et al. in the seminal “Attention is All You Need” paper. Following
    that, we will explore Multi-query attention and how it addresses challenges with
    MHA. Finally, we will answer the questions “What is GQA?” and “How does it give
    us the best of both worlds?”
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一篇关于训练大规模模型的文章中，我们探讨了[LoRA](/understanding-lora-low-rank-adaptation-for-finetuning-large-models-936bce1a07c6)。在这篇文章中，我们将研究另一种被不同大语言模型采用的高效训练策略——分组查询注意力（GQA）。简而言之，分组查询注意力（GQA）是多头注意力（MHA）和多查询注意力（MQA）的推广——它们都是GQA的特例。因此，在深入探讨分组查询注意力之前，我们先回顾一下Vaswani等人提出的经典“Attention
    is All You Need”论文中的传统多头注意力。接下来，我们将探索多查询注意力及其如何解决MHA面临的挑战。最后，我们将回答“什么是GQA？”和“它如何使我们兼得两全？”的问题。
- en: Multi-head attention is a critical component of Transformer models, enabling
    them to efficiently process and understand complex sequences in tasks like language
    translation, summarization, and more. To grasp its intricacies, we must delve
    into the mathematical underpinnings and understand how multiple heads in the attention
    mechanism function.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力是变换器模型的关键组件，使其能够高效地处理和理解复杂序列任务，如语言翻译、摘要生成等。要掌握其复杂性，我们必须深入研究其数学基础，并理解注意力机制中多个头的功能。
- en: 'The basic attention mechanism computes a weighted sum of values, with weights
    dependent on a query and a set of keys. Mathematically, this is expressed as:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的注意力机制计算值的加权和，加权依赖于查询和一组键。在数学上，这表示为：
- en: '![](../Images/21426445ffa7878019ea64e4e511a990.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21426445ffa7878019ea64e4e511a990.png)'
- en: This is referred to as **scaled dot product attention**. In this equation, Q
    (Query) and K (Key) are matrices representing the queries and keys. V (Value)
    is the matrix for values. “d_k” is the dimensionality of keys, which is used for
    scaling.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为**缩放点积注意力**。在这个方程中，Q（查询）和K（键）是表示查询和键的矩阵。V（值）是值的矩阵。“d_k”是键的维度，用于缩放。
- en: Expanding with Multi-Head Attention (MHA)
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展到多头注意力（MHA）
- en: 'Multi-head attention employs multiple ‘heads’ of attention layers, enabling
    the model to attend to information from different representation subspaces. In
    each head, there is an independent set of linear layers (projection matrices)
    for the query, key, and values (this is an important point that we will revisit
    in GQA). For each head (numbered h):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力使用多个‘头’的注意力层，使模型能够关注来自不同表示子空间的信息。在每个头中，有一组独立的线性层（投影矩阵）用于查询、键和值（这是一个重要的点，我们将在GQA中重述）。对于每个头（编号为h）：
- en: headʰ = Attention(*Q.Wq*ʰ,*K.Wk*ʰ,*V.Wv*ʰ)
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: headʰ = Attention(*Q.Wq*ʰ,*K.Wk*ʰ,*V.Wv*ʰ)
- en: Concatenating Head Outputs
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连接头输出
- en: The outputs of the individual heads are concatenated and then linearly transformed.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 各个头的输出被连接起来，然后进行线性变换。
- en: MultiHead(*Q*,*K*,*V*) = Concat(head¹,head²,…,headʰ) .*W*ᵒ
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: MultiHead(*Q*,*K*,*V*) = Concat(head¹,head²,…,headʰ) .*W*ᵒ
- en: Wᵒ is another weight matrix that linearly transforms the concatenated vector
    to the final output dimension.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Wᵒ是另一个权重矩阵，用于将连接向量线性变换为最终输出维度。
- en: The intuition behind multi-head attention is that by applying the attention
    mechanism multiple times in parallel, the model can capture different types of
    relationships in the data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力的直观理解是，通过并行地多次应用注意力机制，模型可以捕捉数据中不同类型的关系。
- en: '![](../Images/8bcc53ddc81a6a101601a80e2d0e7798.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8bcc53ddc81a6a101601a80e2d0e7798.png)'
- en: Illustration depicting scaled dot product attention, multi-head attention within
    a transformer’s encoder block. (Source — sections of the diagram from attention
    is all you need paper [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762),
    composition by the author)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 插图描绘了缩放点积注意力、多头注意力在变换器编码器块中的应用。（来源——图示部分来自《Attention is All You Need》论文 [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)，作者编排）
- en: However, MHA enables a nuanced understanding of the relationships between different
    parts of the input. Nevertheless, this complexity comes at a cost — a significant
    demand on memory bandwidth, especially during decoder inference.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: The Memory Bandwidth Challenge in Multi-Head Attention
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The crux of the issue lies in the memory overhead. Each decoding step in autoregressive
    models like Transformers requires loading decoder weights along with all attention
    keys and values. This process is not only computationally intensive but also memory
    bandwidth-intensive. As model sizes grow, this overhead also increases, making
    scaling up an increasingly arduous task.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Emergence of Multi-Query Attention (MQA)
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Multi-query attention (MQA) emerged as a solution to mitigate this bottleneck.
    The idea is simple yet effective: use multiple query heads but only a single key
    and value head. This approach significantly reduces the memory load, enhancing
    inference speed. It has been employed in multiple large-scale models such as PaLM,
    StarCoder, and Falcon.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: In multi-query attention, we average the heads for keys and values so that all
    query heads share the same key and value head. This is achieved by replicating
    the mean-pooled “head” H times, where H is the number of query heads.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/682067ebcba8140c3452c1127219ae42.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: Left — Multihead attention, Middle — MultiQuery Attention, Right — Converting
    existing MHA checkpoint to MQA (Source — [https://arxiv.org/pdf/2305.13245.pdf](https://arxiv.org/pdf/2305.13245.pdf))
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'An interesting question to ask here is — how does one convert an existing pre-trained
    Multi-head attention model into a multi-query attention model (MQA)? The creation
    of a multi-query attention model from an existing multi-head model involves a
    two-step process: conversion of the model’s structure and subsequent pre-training.
    [1]'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'Conversion of Checkpoint: This step transforms the structure of a multi-head
    model into a multi-query model. It is achieved by merging (mean pooling) the projection
    matrices (linear layers) for keys and values from the multiple heads of the original
    model into single projection matrices for keys and values. This approach of mean
    pooling is found to be more effective than either selecting one of the existing
    key and value heads or initializing new key and value heads from scratch. The
    resulting structure has a consolidated key and value projection, characteristic
    of the multi-query model.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'Pre-Training the Converted Model: After the structural transformation, the
    model undergoes additional training. This training is not as extensive as the
    original model training; it’s a fraction (denoted as α) of the original model’s
    training steps. The purpose of this pre-training phase is to allow the model to
    adjust and optimize its performance according to its new, simplified attention
    mechanism. The training follows the same recipe as the original, ensuring consistency
    in learning dynamics.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: However, MQA is not without its drawbacks. The reduced complexity can lead to
    quality degradation and training instability.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，MQA 也并非没有缺点。减少的复杂性可能导致质量下降和训练不稳定。
- en: Grouped Query Attention
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分组查询注意力
- en: 'Grouped-query attention (GQA) is a simple approach that blends elements of
    multi-head attention (MHA) and multi-query attention (MQA) to create a more efficient
    attention mechanism. The mathematical framework of GQA can be understood as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 分组查询注意力（GQA）是一种简单的方法，它结合了多头注意力（MHA）和多查询注意力（MQA）的元素，以创建一个更高效的注意力机制。GQA 的数学框架可以理解如下：
- en: 'Division into Groups: In GQA, the query heads (Q) from a traditional multi-head
    model are divided into G groups. Each group is assigned a single key (K) and value
    (V) head. This configuration is denoted as GQA-G, where G represents the number
    of groups.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 分组：在 GQA 中，传统多头模型中的查询头（Q）被分成 G 组。每组分配一个单独的键（K）和值（V）头。这种配置被称为 GQA-G，其中 G 代表组的数量。
- en: 'Special Cases of GQA:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: GQA 的特殊情况：
- en: 'GQA-1 = MQA: With only one group (G = 1), GQA becomes equivalent to MQA, as
    there’s only a single key and value head for all query heads.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GQA-1 = MQA：当只有一个组（G = 1）时，GQA 等同于 MQA，因为所有查询头只有一个键和值头。
- en: 'GQA-H = MHA: When the number of groups equals the number of heads (G = H),
    GQA behaves like traditional MHA, with each query head having its unique key and
    value head.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GQA-H = MHA：当组的数量等于头的数量（G = H）时，GQA 的行为类似于传统的 MHA，每个查询头都有其独特的键和值头。
- en: '![](../Images/b26048bc6686cff8d049e63552eb7825.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b26048bc6686cff8d049e63552eb7825.png)'
- en: Difference between MHA, GQA, and MQA (Source — [https://arxiv.org/pdf/2305.13245.pdf](https://arxiv.org/pdf/2305.13245.pdf))
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: MHA、GQA 和 MQA 的区别（来源 — [https://arxiv.org/pdf/2305.13245.pdf](https://arxiv.org/pdf/2305.13245.pdf)）
- en: We mean-pool the key and value projection matrices of the original heads within
    each group to convert a multi-head model into a GQA model. This technique averages
    the projection matrices of each head in a group, resulting in a single key and
    value projection for that group.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对每组内原始头的键和值投影矩阵进行均值池化，将多头模型转换为 GQA 模型。这种技术对每组中每个头的投影矩阵进行平均，从而为该组生成一个单独的键和值投影。
- en: 'By utilizing GQA, the model maintains a balance between MHA quality and MQA
    speed. Because there are fewer key-value pairs, memory bandwidth and data loading
    needs are minimized. The choice of G presents a trade-off: more groups (closer
    to MHA) result in higher quality but slower performance, whereas fewer groups
    (near to MQA) boost speed at the risk of sacrificing quality. Furthermore, as
    the model size grows, GQA allows for a proportional decrease in memory bandwidth
    and model capacity, corresponding with the model’s scale. In contrast, for bigger
    models, the reduction to a single key and value head can be unduly severe in MQA.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用 GQA，模型在 MHA 的质量和 MQA 的速度之间保持平衡。由于键值对较少，内存带宽和数据加载需求被最小化。G 的选择呈现一种折中：更多的组（接近
    MHA）会导致更高的质量但较慢的性能，而较少的组（接近 MQA）则提高了速度，但可能牺牲质量。此外，随着模型规模的增长，GQA 允许内存带宽和模型容量按比例减少，与模型的规模相匹配。相比之下，对于更大的模型，在
    MQA 中将其减少到一个单独的键和值头可能会过于严苛。
- en: Conclusion
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this post, we first looked at traditional multi-head attention (MHA) and
    its variant Multi-query attention. Then we looked at a more generic formulation
    GQA, which is used by many LLM models for effective pre-training. GQA combines
    multi-head attention (MHA) with multi-query attention (MQA), providing a fair
    trade-off between quality and speed. GQA minimizes memory bandwidth demands by
    grouping query heads, making it appropriate for scaling up models. GQA has been
    used in place of typical multi-head attention in recent models such as the LLaMA-2
    and Mistral7B.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们首先介绍了传统的多头注意力（MHA）及其变体多查询注意力。然后我们探讨了一个更通用的公式 GQA，它被许多 LLM 模型用于有效的预训练。GQA
    将多头注意力（MHA）与多查询注意力（MQA）结合起来，在质量和速度之间提供了一个公平的折中。GQA 通过将查询头分组来最小化内存带宽需求，使其适合于模型的扩展。GQA
    已被用于取代典型的多头注意力，在最近的模型中如 LLaMA-2 和 Mistral7B。
- en: '**References:** [1] GQA: Training Generalized Multi-Query Transformer Models
    from Multi-Head Checkpoints — [https://arxiv.org/pdf/2305.13245.pdf](https://arxiv.org/pdf/2305.13245.pdf)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献：** [1] GQA：从多头检查点训练通用化的多查询 Transformer 模型 — [https://arxiv.org/pdf/2305.13245.pdf](https://arxiv.org/pdf/2305.13245.pdf)'
- en: '[2] MQA: Fast Transformer Decoding: One Write-Head is All You Need — [https://arxiv.org/abs/1911.02150](https://arxiv.org/abs/1911.02150)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] MQA：快速 Transformer 解码：一个写头就足够了 — [https://arxiv.org/abs/1911.02150](https://arxiv.org/abs/1911.02150)'
- en: '[3] MHA: Attention is all you need: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
