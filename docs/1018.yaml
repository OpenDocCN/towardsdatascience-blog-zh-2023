- en: Neural Networks — A Beginner’s Guide (1.1)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络 — 初学者指南 (1.1)
- en: 原文：[https://towardsdatascience.com/neural-networks-a-beginners-guide-7b374b66441a?source=collection_archive---------11-----------------------#2023-03-20](https://towardsdatascience.com/neural-networks-a-beginners-guide-7b374b66441a?source=collection_archive---------11-----------------------#2023-03-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/neural-networks-a-beginners-guide-7b374b66441a?source=collection_archive---------11-----------------------#2023-03-20](https://towardsdatascience.com/neural-networks-a-beginners-guide-7b374b66441a?source=collection_archive---------11-----------------------#2023-03-20)
- en: Building intuition about Neural Networks
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建立关于神经网络的直觉
- en: '[](https://shwetat20.medium.com/?source=post_page-----7b374b66441a--------------------------------)[![Shweta](../Images/18ffd9eedda819fc4459875cd52c7f3a.png)](https://shwetat20.medium.com/?source=post_page-----7b374b66441a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7b374b66441a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7b374b66441a--------------------------------)
    [Shweta](https://shwetat20.medium.com/?source=post_page-----7b374b66441a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://shwetat20.medium.com/?source=post_page-----7b374b66441a--------------------------------)[![Shweta](../Images/18ffd9eedda819fc4459875cd52c7f3a.png)](https://shwetat20.medium.com/?source=post_page-----7b374b66441a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7b374b66441a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7b374b66441a--------------------------------)
    [Shweta](https://shwetat20.medium.com/?source=post_page-----7b374b66441a--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6cac7e4f2c0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-a-beginners-guide-7b374b66441a&user=Shweta&userId=6cac7e4f2c0&source=post_page-6cac7e4f2c0----7b374b66441a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7b374b66441a--------------------------------)
    ·10 min read·Mar 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7b374b66441a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-a-beginners-guide-7b374b66441a&user=Shweta&userId=6cac7e4f2c0&source=-----7b374b66441a---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6cac7e4f2c0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-a-beginners-guide-7b374b66441a&user=Shweta&userId=6cac7e4f2c0&source=post_page-6cac7e4f2c0----7b374b66441a---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7b374b66441a--------------------------------)
    ·10 min read·2023年3月20日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7b374b66441a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-a-beginners-guide-7b374b66441a&user=Shweta&userId=6cac7e4f2c0&source=-----7b374b66441a---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7b374b66441a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-a-beginners-guide-7b374b66441a&source=-----7b374b66441a---------------------bookmark_footer-----------)![](../Images/10df32f5ff8bc6b41142c5ad091c15c2.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7b374b66441a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-a-beginners-guide-7b374b66441a&source=-----7b374b66441a---------------------bookmark_footer-----------)![](../Images/10df32f5ff8bc6b41142c5ad091c15c2.png)'
- en: Photo by [La-Rel Easter](https://unsplash.com/@lastnameeaster?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [La-Rel Easter](https://unsplash.com/@lastnameeaster?utm_source=medium&utm_medium=referral)
    拍摄，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Deep Learning has witnessed tremendous growth in the last decade. With applications
    in image classification, speech recognition, text to speech conversion, self driving
    cars etc., the list of problems that Deep Learning has addressed is very significant.
    It is therefore necessary to understand the basic structure and working of Neural
    Networks to appreciate these advancements.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在过去十年中经历了巨大的增长。它在图像分类、语音识别、文本转语音、自驾车等方面都有应用，深度学习解决的问题列表非常重要。因此，理解神经网络的基本结构和工作原理对于欣赏这些进展是必要的。
- en: Let us deep dive into learning.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨学习。
- en: '**1\. Building Blocks of Neural Networks**'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**1\. 神经网络的构建模块**'
- en: A neural network is a computational learning system that **maps input variables
    to the output variable** using an underlying mapping **function that is non linear
    in nature.**
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是一个计算学习系统，通过使用底层的**非线性映射函数来**将输入变量映射到输出变量。
- en: 'It comprises five essential components:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 它包含五个基本组件：
- en: '*a. Nodes and Layers*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*a. 节点和层*'
- en: '*b. Activation Function*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*b. 激活函数*'
- en: '*c. Loss Function*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*c. 损失函数*'
- en: '*d. Optimizer*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*d. 优化器*'
- en: We will learn about each of these components in detail.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将详细了解这些组件。
- en: '**Layers:**'
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**层：**'
- en: 'Simply put, a Neural Network is a stack of layers, inter connected to each
    other. There are three types of layers in a Neural Network : *Input Layer —* takes
    the input data , *Hidden Layer —* transforms the input data, *Output Layer —*
    generates prediction for the given inputs after applying transformations. The
    layers close to the Input Layer are called the **Lower layers**, the layers close
    to the Output Layer are called the **Upper Layers**.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，神经网络是一系列相互连接的层。神经网络中有三种层类型：*输入层 —* 接受输入数据，*隐藏层 —* 转换输入数据，*输出层 —* 在应用转换后为给定的输入生成预测。接近输入层的层称为**下层**，接近输出层的层称为**上层**。
- en: Each layer consists of multiple neurons, also called **Nodes**. Each node in
    a given layer is connected to each node in the next layer. The nodes take the
    weighted sum of the inputs from the previous layer, applies a non linear activation
    function to it and generates an output which then becomes an input to the nodes
    in the next layer.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 每一层由多个神经元组成，也称为**节点**。给定层中的每个节点与下一层中的每个节点相连。节点接收来自上一层的加权输入总和，应用非线性激活函数，并生成一个输出，该输出随后成为下一层节点的输入。
- en: Consider a common classification problem of predicting whether a loan applicant
    will default. The input variables include factors like applicant age, employment
    type, number of dependents, place of residence, LTV ratio, etc. These variables
    will make up the input layer.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个常见的分类问题，例如预测贷款申请者是否会违约。输入变量包括申请者年龄、就业类型、赡养人数、居住地、贷款价值比等。这些变量将组成输入层。
- en: The number of nodes in the input layer correspond to the number of independent
    variables in the data . The number of hidden layers and the nodes in these layers
    is a hyperparameter and usually is a function of the complexity of the problem
    and the data available.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层中的节点数量对应于数据中的独立变量数量。隐藏层的数量以及这些层中的节点数是超参数，通常是问题复杂性和可用数据的函数。
- en: In a complex problem, the number of layers and the nodes in each layer will
    be more, each hidden layer will learn representations not learned at the previous
    layer. These neural nets are called ‘**Deep Neural Networks**’.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在复杂问题中，层的数量和每层中的节点数量将更多，每个隐藏层将学习在上一层未学到的表示。这些神经网络被称为‘**深度神经网络**’。
- en: For a regression problem, the number of nodes in the output layer is one; for
    a multiclassification problem, the number of nodes in the output layer is equal
    to the number of labels/categories, for a binary classification problem, the number
    of nodes in the output layer is equal to 1.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归问题，输出层中的节点数量为1；对于多分类问题，输出层中的节点数量等于标签/类别数量；对于二分类问题，输出层中的节点数量为1。
- en: The working of a neural network can be broken down to a single node in a given
    layer.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的工作原理可以归结为给定层中的单个节点。
- en: '![](../Images/ad36a4369eaa567c63fc353eb6c7ceaf.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad36a4369eaa567c63fc353eb6c7ceaf.png)'
- en: '***Working of a Single Node in a Neural Network (Image by Author)***'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '***神经网络中单个节点的工作原理（图片由作者提供）***'
- en: As shown above, the single node takes in the following inputs — bias b and input
    variables x1 and x2\. It also takes in another parameter as inputs — the weights
    for each independent variable. The weights indicate the importance of the input
    variables.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，单个节点接受以下输入 — 偏置b和输入变量x1及x2。它还接受另一个参数作为输入 — 每个独立变量的权重。权重表示输入变量的重要性。
- en: 'The node will process the weighted sum of the inputs given as:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 节点将处理加权输入总和，如下所示：
- en: z = w1x1 + w2x2 + bias (Equation 1)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: z = w1x1 + w2x2 + bias（公式1）
- en: An activation function is then applied at each node in a given layer to give
    an output. The output generated by the node after applying activation function
    is a.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在给定层中的每个节点上应用激活函数以生成输出。应用激活函数后由节点生成的输出是a。
- en: f(z) = a (Equation 2)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: f(z) = a（公式2）
- en: This is the working of a single node in a single layer of a neural network.
    Networks with multiple layers and nodes also operate using the same principle.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这是神经网络中单层单节点的工作原理。具有多个层和节点的网络也按照相同的原则运行。
- en: '![](../Images/54dca79fc1fe42ba422fabe5935d89de.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/54dca79fc1fe42ba422fabe5935d89de.png)'
- en: '**2 Layer Neural Network (Image by Author)**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**2 层神经网络（作者提供的图片）**'
- en: In addition to the weighted inputs, we can see that there is another term called
    bias ‘b’ in the equation 1 above. ***What is the role of bias in a neural network?***
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 除了加权输入外，我们还可以看到在上述公式 1 中有一个叫做偏置 ‘b’ 的项。***偏置在神经网络中有什么作用？***
- en: A **Bias** is a variable that helps in activation of the node. Bias is the negative
    of the threshold value that is required to activate the node. There is a single
    bias value used for all nodes in a given layer.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**偏置** 是一个帮助激活节点的变量。偏置是激活节点所需的阈值的负值。在给定层中的所有节点中使用一个单独的偏置值。'
- en: The data in batches is passed through the input layer which sends it to the
    first hidden layer. The neurons in the first hidden layer will activate based
    on the output of the *activation function*, that takes in the **weighted sum of
    the inputs and the bias** and computes a number in a specific range.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 批次数据通过输入层传递，输入层将其发送到第一个隐藏层。第一个隐藏层中的神经元将基于*激活函数*的输出进行激活，激活函数接收**输入的加权和与偏置**并计算特定范围内的一个数字。
- en: This brings us to the next question — ***What is an activation function and
    why do we require it?***
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了下一个问题 — ***什么是激活函数，我们为什么需要它？***
- en: '**Activation Function**'
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**激活函数**'
- en: 'In *simple* terms :'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 用*简单*的术语来说：
- en: An activation function is used to transform the input from a node to an output
    value that is fed to the node in the next hidden layer.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数用于将节点的输入转换为传递到下一个隐藏层节点的输出值。
- en: 'In *technical* terms:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 用*技术*术语来说：
- en: An activation function, also known as a **transfer function**, defines how the
    weighted sum of the inputs and the bias is transformed into an output from the
    node in a given layer. It maps the output value in a given range i.e. 0 to 1 or
    -1 to +1 depending on the type of function used.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数，也称为**传递函数**，定义了如何将输入的加权和与偏置转换为给定层中的节点的输出。它将输出值映射到特定范围，即 0 到 1 或 -1 到 +1，具体取决于所用的函数类型。
- en: Activation functions used in Neural Networks are of two types — linear and non-linear.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中使用的激活函数有两种类型 — 线性和非线性。
- en: '**Linear Activation Function:**'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**线性激活函数：**'
- en: The equation is given by f(x) = b+ Sigma( wi * xi), indexed over all input variables
    (i)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 公式为 f(x) = b + Sigma( wi * xi)，对所有输入变量 (i) 进行索引。
- en: '![](../Images/a9c6dbc138a047bbf5a880b94845eb69.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9c6dbc138a047bbf5a880b94845eb69.png)'
- en: 'The range of this function is : — infinity to + infinity.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数的范围是：— 无穷大到 + 无穷大。
- en: A linear activation function is used in outer layer of the neural network when
    solving regression problems. It is not a good idea to use it in the input or hidden
    layers cause the network will not be able to capture the complex relationships
    in the underlying data.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 线性激活函数用于神经网络的外层，以解决回归问题。在输入层或隐藏层中使用它不是一个好主意，因为网络将无法捕捉底层数据中的复杂关系。
- en: '**2\. Non-Linear Activation Function:**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 非线性激活函数：**'
- en: Non Linear activation functions are by default, the most used activation function
    in Deep Learning. These include Sigmoid or Logistic function, Rectified Linear
    Activation(ReLU), and Hyperbolic Tangent (Tanh).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性激活函数默认是深度学习中最常用的激活函数。这些包括Sigmoid或Logistic函数、修正线性激活函数（ReLU）和双曲正切函数（Tanh）。
- en: Let’s understand each of them in more detail.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地了解每个。
- en: '**Sigmoid Activation Function**:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Sigmoid 激活函数**：'
- en: Also called the Logistic function, it takes in any real value as input and gives
    an output in the range of 0 and 1.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 也称为Logistic函数，它接受任何实值作为输入，并在 0 和 1 之间给出输出。
- en: Given as y = 1/(1+ e^-z), it has a S shaped curve. Here z = b + sigma(xi * wi),
    indexed over i input variables.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 公式为 y = 1/(1+ e^-z)，具有S形曲线。这里的 z = b + sigma(xi * wi)，对 i 输入变量进行索引。
- en: For a very large positive number z, e^-z will be 0 and the output of the function
    will be 1\. For a very large negative number z, e^-z will be a large number and
    thus the output of the function will be 0.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非常大的正数 z，e^-z 将为 0，函数的输出将为 1。对于非常大的负数 z，e^-z 将是一个大数，因此函数的输出将为 0。
- en: '![](../Images/f8bd0cde0599311d3275a2333f642995.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8bd0cde0599311d3275a2333f642995.png)'
- en: 2\. **Rectified Linear Activation (ReLU):**
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. **修正线性激活函数 (ReLU):**
- en: It is today, the most used activation function. ReLU has a property of being
    linear for all input values greater than 0 and non-linear otherwise.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 它是今天使用最广泛的激活函数。ReLU 具有对所有大于 0 的输入值是线性的，而对其他值则是非线性的属性。
- en: '![](../Images/a389e5cfc51007af2605852b7221b136.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a389e5cfc51007af2605852b7221b136.png)'
- en: It is given as **f(x) = max(0,x)**
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 它表示为 **f(x) = max(0,x)**
- en: 3\. **Hyperbolic Tangent Activation:**
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. **双曲正切激活函数：**
- en: Similar to the logistic function, it takes in any real number as an input and
    outputs a value in the range of -1 and +1.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于逻辑函数，它接受任何实数作为输入，并输出范围在 -1 和 +1 之间的值。
- en: 'It is given as : f(x) = (e^z — e^-z) / (e^z+e^-z). Here z = b + sigma(xi *
    wi), indexed over i input variables.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 它表示为：f(x) = (e^z — e^-z) / (e^z+e^-z)。其中 z = b + sigma(xi * wi)，索引为 i 个输入变量。
- en: The shape of Tanh function is also S shaped but the range is different.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Tanh 函数的形状也是 S 形的，但范围不同。
- en: '![](../Images/f7711ead74c6971a81edbe50d8cbdad5.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f7711ead74c6971a81edbe50d8cbdad5.png)'
- en: Generally one activation function is used across all layers, exception being
    the output layer. The activation function used in the output layer depends on
    whether the problem statement requires us to predict a continuous value i.e. Regression
    or a categorical value i.e. Binary or Multi label classification.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 通常在所有层中使用一个激活函数，唯一的例外是输出层。输出层使用的激活函数取决于问题陈述是否要求我们预测一个连续值，即回归，或一个分类值，即二分类或多标签分类。
- en: '***A neuron can thus be defined as an operation that has two parts — linear
    component and an activation component i.e. Neuron = Linear + Activation.***'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '***因此，神经元可以定义为一个包含两个部分的操作——线性组件和激活组件，即神经元 = 线性 + 激活。***'
- en: All the functions mentioned above along with its variants have some limitations
    which I will cover in the next article.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 上述所有函数及其变体都有一些限制，我将在下一篇文章中介绍。
- en: '***So how does a Neural Network learn?***'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '***那么神经网络是如何学习的？***'
- en: The weights of all parameters are initialized with some random values. The weighted
    sum is passed to the first hidden layer of the network.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 所有参数的权重都以一些随机值进行初始化。加权和被传递到网络的第一个隐藏层。
- en: The first hidden layer will compute the output of all the neurons and will pass
    it to the neurons in the next hidden layer. Do note that the input values at each
    layer is being transformed by the activation function and then sent to the next
    layer.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个隐藏层将计算所有神经元的输出，并将其传递给下一个隐藏层中的神经元。请注意，每层的输入值都通过激活函数进行转换，然后发送到下一层。
- en: This flow continues till the last layer is reached which then computes the final
    prediction. This unidirectional flow from the Input Layer to the Output Layer
    is called a ‘**Forward Pass’ or ‘Forward Propagation’**.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这种流动会持续到达最后一层，然后计算最终的预测。这种从输入层到输出层的单向流动称为‘**前向传播**’或‘**前向传递**’。
- en: Our network has now generated a final output. What happens next?
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的网络现在已经生成了最终输出。接下来发生什么？
- en: Loss Function
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失函数
- en: The predicted value is compared with actual value and the error is computed.
    The magnitude of the error is given by the loss function.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 将预测值与实际值进行比较并计算误差。误差的大小由损失函数给出。
- en: The loss function will estimate how close the distribution of the predicted
    value is to distribution of the actual target variable in the training data.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数将估计预测值的分布与训练数据中实际目标变量的分布的接近程度。
- en: The Maximum Likelihood Estimation (MLE) framework is used to compute the error
    over the entire training data. It does this by estimating how closely the distribution
    of the predictions matches with the distribution of the target variable in the
    training data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最大似然估计（MLE）框架用于计算整个训练数据上的误差。它通过估计预测的分布与训练数据中目标变量的分布的匹配程度来完成这一点。
- en: The loss function under the MLE framework for classification problem is **Cross
    Entropy,** and for regression problem is **Mean Squared Error**.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MLE 框架下，分类问题的损失函数是 **交叉熵**，回归问题的损失函数是 **均方误差**。
- en: '***Cross Entropy*** gives the measure of the difference between two probability
    distributions of a random variable. In the context of the Neural Networks, it
    gives the difference between the predicted probability distribution and the distribution
    of the target variable in the training data set **for a given set of weights or
    parameters.**'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '***交叉熵*** 量度了两个概率分布之间的差异。在神经网络的背景下，它表示预测概率分布与训练数据集中目标变量分布之间的差异 **对于给定的一组权重或参数。**'
- en: For a binary classification problem, the loss function used is binary cross
    entropy and for a multiclass classification problem, the loss function used is
    categorical cross entropy.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二分类问题，使用的损失函数是二分类交叉熵；对于多分类问题，使用的损失函数是类别交叉熵。
- en: For e.g. consider a binary classification problem related to customer loan default.
    Suppose the training data consists of 5 customers.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个与客户贷款违约相关的二分类问题。假设训练数据包含5个客户。
- en: The neural network in the first forward pass will compute the probability of
    a customer to default . The output generated by the network for all the 5 customers
    respectively is [0.65, 0.25,0.9,0.33,0.45].
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络在第一次前向传播中将计算客户违约的概率。网络为所有5个客户生成的输出分别是[0.65, 0.25, 0.9, 0.33, 0.45]。
- en: The actual values for the observations in the training data is [1,1,1,1,1].
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据中观测值的实际值为[1, 1, 1, 1, 1]。
- en: 'Cross Entropy Loss is given as:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵损失定义如下：
- en: '![](../Images/5f43646e2a22481314b8c42e9e68272a.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5f43646e2a22481314b8c42e9e68272a.png)'
- en: Image by Author
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'Using this equation, the cross entropy loss (CEL) for the above problem is
    calculated as:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个方程，上述问题的交叉熵损失（CEL）计算如下：
- en: '![](../Images/813f824f81ee7983213596357e04872f.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/813f824f81ee7983213596357e04872f.png)'
- en: Image by Author
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Here, the binary cross entropy calculates the score that summarizes the average
    difference between the actual and predicted probability distributions for predicting
    class 1\. The loss given the actual and predicted values of the target variable
    is **0.404\.** How do we interpret this value? It has a relative interpretation.
    The final model will have a loss value much lower than 0.404\. The fifth and the
    last building block will enable us to reach that optimal value. It does this by
    searching for the most optimal values of weights and bias that minimizes the loss
    function.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，二分类交叉熵计算了一个分数，这个分数总结了实际概率分布和预测概率分布之间的平均差异，以预测类别1。给定目标变量的实际值和预测值的损失为**0.404**。我们如何解释这个值？它有一个相对的解释。最终模型的损失值将远低于0.404。第五个也是最后一个构建块将帮助我们达到那个最优值。它通过寻找最优的权重和偏置值来最小化损失函数，从而实现这一点。
- en: In case of a multiclass classification problem, where the target variable is
    encoded as 1 to n-1 categories, the categorical cross entropy will calculate the
    score that summarizes the average difference between the actual and predicted
    probability distributions for all the classes.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在多分类问题中，其中目标变量编码为1到n-1类，类别交叉熵将计算一个分数，这个分数总结了所有类别的实际概率分布和预测概率分布之间的平均差异。
- en: Similarly for Regression, *Mean Squared Error* (MSE) is the most commonly used
    loss function for a regression problem. MSE is calculated as the average of the
    squared difference between the predicted and actual values of the target variable.
    The output is always positive as it is a square of the error.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，对于回归问题，*均方误差*（MSE）是最常用的回归损失函数。MSE计算为目标变量的预测值与实际值之间的平方差的平均值。由于它是误差的平方，输出总是为正。
- en: There are variants to the MSE like the Mean Squared Logarithmic Error Loss (MSLE)
    and Mean Absolute Error (MAE). The choice depends on number of factors like presence
    of outliers , distribution of the target variable and others.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: MSE有一些变体，如均方对数误差损失（MSLE）和均值绝对误差（MAE）。选择取决于多个因素，如异常值的存在、目标变量的分布等。
- en: '***The output generated by the network in the first forward pass is a result
    of the weights that were initialized to some random values. The loss function
    compares the actual and predicted values and computes the error. The next step
    is to minimize the error by changing the weights. How does the network achieve
    this?***'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '***网络在第一次前向传播中生成的输出是由初始化为某些随机值的权重决定的。损失函数比较实际值和预测值并计算误差。下一步是通过改变权重来最小化误差。网络如何实现这一点？***'
- en: This brings us the last building block of Neural Network i.e. Optimizer.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这将我们引入神经网络的最后一个构建块，即优化器。
- en: '**5\. Optimizer**'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**5. 优化器**'
- en: As we discussed in the earlier section, in the neural network, the learning
    takes place in the weights. Training a neural network involves learning the correct
    weights associated with all the neurons in all the layers. **This is achieved
    by using Stochastic Gradient Descent algorithm together with the Back Propagation
    algorithm.**
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面部分所讨论的，在神经网络中，学习发生在权重中。训练神经网络涉及到学习所有层中所有神经元的正确权重。**这通过使用随机梯度下降算法结合反向传播算法来实现。**
- en: Given this is a much more complex concept as compared to the ones covered above,
    let us look at this in detail in the next article. All the building blocks covered
    here also deserve a more detailed explanation which will be done in the subsequent
    articles.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这是一个比上述内容更复杂的概念，我们将在下一篇文章中详细探讨这一点。这里涉及的所有构建块也值得在后续文章中做更详细的解释。
- en: '*The key takeaway from this article is that the* ***final Neural Network model
    is a function of the overall architecture i.e number of nodes, layers etc. and
    the optimal value of the parameters a.k.a. weights. Once we have addressed both
    these components, we can go ahead with confidently predicting the target variable.***'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*本文的关键要点是* ***最终的神经网络模型是整体架构的函数，即节点数量、层数等，以及参数（也称为权重）的最佳值。一旦我们解决了这两个组件，就可以自信地预测目标变量。***'
- en: Here are the few links I found extremely helpful in understanding this concept.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我找到的一些在理解这个概念方面非常有帮助的链接。
- en: '[https://youtu.be/PySo_6S4ZAg](https://youtu.be/PySo_6S4ZAg) — This is the
    Stanford CS230 course on Neural Networks by Andrew Ng.'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://youtu.be/PySo_6S4ZAg](https://youtu.be/PySo_6S4ZAg) — 这是斯坦福大学CS230神经网络课程，由Andrew
    Ng主讲。'
- en: '[https://amzn.eu/d/6U4c3GR](https://amzn.eu/d/6U4c3GR) — Deep Learning with
    Python, Second Edition. Amazing book. The concepts are explained in a very simple
    language.'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://amzn.eu/d/6U4c3GR](https://amzn.eu/d/6U4c3GR) — 《用Python进行深度学习（第2版）》。一本很棒的书。概念用非常简单的语言解释。'
- en: '[https://machinelearningmastery.com/](https://machinelearningmastery.com/)
    — one source for all basic and intermediate questions on Deep Learning and Machine
    Learning.'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://machinelearningmastery.com/](https://machinelearningmastery.com/)
    — 这是一个涵盖深度学习和机器学习所有基础和中级问题的资源。'
- en: Hope by now you have some understanding of what neural nets are and how the
    various building blocks come together to solve a deep learning problem. Let me
    know your thoughts.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 希望到现在你对神经网络有了一些理解，并且了解了各种构建块如何结合在一起解决深度学习问题。请告诉我你的想法。
