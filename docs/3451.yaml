- en: Build a Language Model on Your WhatsApp Chats
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在你的 WhatsApp 聊天中构建一个语言模型
- en: 原文：[https://towardsdatascience.com/build-a-language-model-on-your-whatsapp-chats-31264a9ced90?source=collection_archive---------1-----------------------#2023-11-21](https://towardsdatascience.com/build-a-language-model-on-your-whatsapp-chats-31264a9ced90?source=collection_archive---------1-----------------------#2023-11-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/build-a-language-model-on-your-whatsapp-chats-31264a9ced90?source=collection_archive---------1-----------------------#2023-11-21](https://towardsdatascience.com/build-a-language-model-on-your-whatsapp-chats-31264a9ced90?source=collection_archive---------1-----------------------#2023-11-21)
- en: A visual guide through the GPT architecture with an application
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过应用程序了解 GPT 架构的视觉指南
- en: '[](https://bernhard-pfann.medium.com/?source=post_page-----31264a9ced90--------------------------------)[![Bernhard
    Pfann, CFA](../Images/00988e4640d9dfc4f999bb67204c8109.png)](https://bernhard-pfann.medium.com/?source=post_page-----31264a9ced90--------------------------------)[](https://towardsdatascience.com/?source=post_page-----31264a9ced90--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----31264a9ced90--------------------------------)
    [Bernhard Pfann, CFA](https://bernhard-pfann.medium.com/?source=post_page-----31264a9ced90--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://bernhard-pfann.medium.com/?source=post_page-----31264a9ced90--------------------------------)[![Bernhard
    Pfann, CFA](../Images/00988e4640d9dfc4f999bb67204c8109.png)](https://bernhard-pfann.medium.com/?source=post_page-----31264a9ced90--------------------------------)[](https://towardsdatascience.com/?source=post_page-----31264a9ced90--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----31264a9ced90--------------------------------)
    [Bernhard Pfann, CFA](https://bernhard-pfann.medium.com/?source=post_page-----31264a9ced90--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb2add0f92c53&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-language-model-on-your-whatsapp-chats-31264a9ced90&user=Bernhard+Pfann%2C+CFA&userId=b2add0f92c53&source=post_page-b2add0f92c53----31264a9ced90---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----31264a9ced90--------------------------------)
    ·16 min read·Nov 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F31264a9ced90&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-language-model-on-your-whatsapp-chats-31264a9ced90&user=Bernhard+Pfann%2C+CFA&userId=b2add0f92c53&source=-----31264a9ced90---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb2add0f92c53&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-language-model-on-your-whatsapp-chats-31264a9ced90&user=Bernhard+Pfann%2C+CFA&userId=b2add0f92c53&source=post_page-b2add0f92c53----31264a9ced90---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----31264a9ced90--------------------------------)
    ·16 分钟阅读·2023年11月21日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F31264a9ced90&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-language-model-on-your-whatsapp-chats-31264a9ced90&user=Bernhard+Pfann%2C+CFA&userId=b2add0f92c53&source=-----31264a9ced90---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F31264a9ced90&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-language-model-on-your-whatsapp-chats-31264a9ced90&source=-----31264a9ced90---------------------bookmark_footer-----------)![](../Images/0acb87053cc35cfd27f5cef359322dd3.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F31264a9ced90&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-language-model-on-your-whatsapp-chats-31264a9ced90&source=-----31264a9ced90---------------------bookmark_footer-----------)![](../Images/0acb87053cc35cfd27f5cef359322dd3.png)'
- en: Photo by [Volodymyr Hryshchenko](https://unsplash.com/@lunarts?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Volodymyr Hryshchenko](https://unsplash.com/@lunarts?utm_source=medium&utm_medium=referral)
    提供，来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Chatbots have undeniably transformed our interaction with digital platforms.
    Despite the impressive advancements in the capabilities of underlying language
    models to handle complex tasks, the user experience often falls short, feeling
    impersonal and detached.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天机器人无疑改变了我们与数字平台的互动。尽管基础语言模型在处理复杂任务方面取得了令人印象深刻的进展，但用户体验仍然常常显得不够个人化和疏离。
- en: To make conversations more relatable, I envisioned a chatbot that could emulate
    my casual writing style, akin to texting a friend over WhatsApp.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使对话更加贴近实际，我设想了一个可以模拟我随意写作风格的聊天机器人，就像在 WhatsApp 上给朋友发短信一样。
- en: In this article, I’ll walk you through my journey of building a (small) language
    model that generates synthetic conversations, using my WhatsApp chat messages
    as input data. Along the way, I try to unravel the inner workings of the GPT architecture
    in a visual and hopefully digestible way, complemented by the actual Python implementation.
    You can find the full project on my [GitHub](https://github.com/bernhard-pfann/lad-gpt).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将带你了解我构建一个（小型）语言模型的过程，该模型生成合成对话，使用我的 WhatsApp 聊天记录作为输入数据。过程中，我尝试以可视化且希望易于理解的方式解开
    GPT 架构的内部工作机制，并附有实际的 Python 实现。你可以在我的 [GitHub](https://github.com/bernhard-pfann/lad-gpt)
    上找到完整项目。
- en: '**Note:** The [model class](https://github.com/bernhard-pfann/lad-gpt/blob/main/src/model.py)
    itself is in large chunks taken from the [video series of Andrej Karpathy](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&pp=iAQB)and
    adapted for my needs. I can highly recommend his tutorials.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** [模型类](https://github.com/bernhard-pfann/lad-gpt/blob/main/src/model.py)
    本身大块取自 [Andrej Karpathy 的视频系列](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&pp=iAQB)，并根据我的需要进行了调整。我强烈推荐他的教程。'
- en: '[](https://github.com/bernhard-pfann/lad-gpt?source=post_page-----31264a9ced90--------------------------------)
    [## lad-gpt'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/bernhard-pfann/lad-gpt?source=post_page-----31264a9ced90--------------------------------)
    [## lad-gpt'
- en: Train a language model from scratch and entirely based on your WhatsApp group
    chats.
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从头开始训练一个语言模型，完全基于你的 WhatsApp 群聊。
- en: github.com](https://github.com/bernhard-pfann/lad-gpt?source=post_page-----31264a9ced90--------------------------------)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/bernhard-pfann/lad-gpt?source=post_page-----31264a9ced90--------------------------------)
- en: Table of Contents
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目录
- en: Selected Approach
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选定的方法
- en: Data Source
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据来源
- en: Tokenization
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分词
- en: Indexing
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 索引
- en: Model Architecture
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型架构
- en: Model Training
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型训练
- en: Chat-Mode
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 聊天模式
- en: 1\. Selected Approach
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1. 选定的方法
- en: 'When it comes to tailoring a language model to a specific corpus of data, there
    are several approaches one can take:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在将语言模型定制为特定语料库时，可以采取几种方法：
- en: '**Model building:** This involves constructing and training a model from scratch,
    providing the utmost flexibility in terms of model architecture and training data
    selection.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型构建：** 这涉及从头开始构建和训练模型，在模型架构和训练数据选择方面提供了最大的灵活性。'
- en: '**Fine-tuning:** This approach leverages an existing pre-trained model, adjusting
    its weights to align more closely with the specific data at hand.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**微调：** 这种方法利用现有的预训练模型，调整其权重以更紧密地与手头的特定数据对齐。'
- en: '**Prompt engineering:** This also utilizes an existing pre-trained model, but
    here, the unique corpus is directly incorporated into the prompt, without changing
    the model’s weights.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**提示工程：** 这也利用了现有的预训练模型，但在这里，独特的语料库直接融入提示中，而不改变模型的权重。'
- en: As my motivation for this project is mainly self-educational, and I am rather
    interested in the architecture of modern language models, I opted for the first
    approach. Yet, this choice comes with obvious limitations. Given the size of my
    data and available computational resources, I didn’t anticipate results on par
    with any state-of-the-art pre-trained models.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我对这个项目的动机主要是自我教育，并且对现代语言模型的架构非常感兴趣，我选择了第一种方法。然而，这种选择带来了明显的限制。鉴于我的数据量和可用计算资源，我并没有期望与任何最先进的预训练模型相当的结果。
- en: Nevertheless, I was hopeful that my model would pick up some interesting linguistic
    patterns, which it ultimately did. Exploring the second option (fine-tuning) might
    be the focus of a future article.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，我仍希望我的模型能发现一些有趣的语言模式，最终它确实做到了。探索第二种选项（微调）可能会成为未来文章的重点。
- en: 2\. Data Source
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2. 数据来源
- en: WhatsApp, my primary communication channel, was an ideal source for capturing
    my conversational style. Exporting over six years of group chat history, totaling
    more than 1.5 million words was straightforward.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: WhatsApp，作为我的主要沟通渠道，是捕捉我的对话风格的理想来源。导出超过六年的群聊记录，总计超过150万字是非常简单的。
- en: The data was parsed using a regex pattern into a list of tuples containing the
    date, contact name, and chat message.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 数据使用正则表达式模式解析成包含日期、联系人姓名和聊天消息的元组列表。
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now, each element was processed separately.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，每个元素都被单独处理。
- en: '**Sending date:** Aside from converting it into a datetime object, I have not
    utilized this information. However, one could look at the time deltas to differentiate
    the start and end of topic discussions.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**发送日期：** 除了将其转换为日期时间对象外，我没有利用这些信息。然而，可以查看时间差异以区分话题讨论的开始和结束。'
- en: '**Contact name:** When tokenizing the text, each contact name is treated as
    a unique token. This ensures that the combination of first and last names is still
    considered a single entity.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**联系人姓名：** 在分词文本时，每个联系人姓名被视为一个唯一的标记。这确保了名和姓的组合仍被视为一个整体。'
- en: '**Chat message:** A special “<END>” token is added at the end of each message.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聊天消息：** 在每条消息的末尾添加了一个特殊的“<END>”标记。'
- en: 3\. Tokenization
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 分词
- en: To train a language model, we need to break language into pieces (so-called
    tokens) and feed them to the model incrementally. Tokenization can be performed
    on multiple levels.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练语言模型，我们需要将语言分解成片段（所谓的标记），并逐步输入模型。分词可以在多个层次上进行。
- en: '**Character-level:** Text is perceived as a sequence of individual characters
    (including white spaces). This granular approach allows every possible word to
    be formed from a sequence of characters. However, it is more difficult to capture
    semantic relationships between words.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**字符级别：** 文本被视为一系列单独的字符（包括空格）。这种细粒度的方法允许从字符序列中形成每一个可能的单词。然而，捕捉单词之间的语义关系会更困难。'
- en: '**Word-level:** Text is represented as a sequence of words. However, the model’s
    vocabulary is limited by the existing words in the training data.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词级别：** 文本被表示为一个单词序列。然而，模型的词汇量受到训练数据中现有单词的限制。'
- en: '**Sub-word-level:** Text is broken down into sub-word units, which are smaller
    than words but larger than characters.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**子词级别：** 文本被拆分成比单词小但比字符大的子词单元。'
- en: While I started off with a character-level tokenizer, I felt that training time
    was wasted, learning character sequences of repetitive words, rather than focusing
    on the semantic relationship between words across the sentence.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我最初使用的是字符级别的分词器，但我觉得训练时间被浪费在了学习重复单词的字符序列上，而不是关注句子中单词之间的语义关系。
- en: For the sake of conceptual simplicity, I decided to switch to a word-level tokenizer,
    keeping aside the available libraries for more sophisticated tokenization strategies.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了概念上的简洁，我决定切换到词级别的分词器，暂时搁置了用于更复杂分词策略的现有库。
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: It turned out that my training data has a vocabulary of ~70,000 unique words.
    However, as many words appear only once or twice, I decided to replace such rare
    words by a “<UNK>” special token. This had the effect of reducing vocabulary to
    ~25,000 words, which leads to a smaller model that needs to be trained later.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示我的训练数据有大约 70,000 个独特的单词。然而，由于许多单词仅出现一次或两次，我决定用“<UNK>”特殊标记替代这些稀有单词。这减少了词汇量至大约
    25,000 个单词，从而得到一个较小的模型，后续训练也会更简单。
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 4\. Indexing
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 索引
- en: After tokenization, the next step is to convert the words and special tokens
    into numerical representations. Using a fixed vocabulary list, each word was indexed
    by its position. The encoded words were then prepared as PyTorch tensors.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在分词之后，下一步是将单词和特殊标记转换为数值表示。使用固定的词汇表，每个单词按其位置进行了索引。编码后的单词随后被准备为 PyTorch 张量。
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As we need to evaluate the quality of our model against some unseen data, we
    split the tensor into two parts. And voila, we have our training and validation
    sets, ready to feed to the language model.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们需要评估模型在一些未见数据上的质量，我们将张量分成两部分。这样，我们就得到了训练集和验证集，可以准备好喂给语言模型。
- en: '![](../Images/ee2c75bbcfee5677b0b6d39ca209f864.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee2c75bbcfee5677b0b6d39ca209f864.png)'
- en: Image by author
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 5\. Model Architecture
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 模型架构
- en: I decided to apply the GPT architecture, which has been promoted by the influential
    paper “[Attention is All you Need](https://arxiv.org/abs/1706.03762)”. Since I
    tried to build a language generator and not a question-answer bot, the decoder-only
    (right side) architecture was sufficient for this purpose.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我决定应用 GPT 架构，这一架构在具有影响力的论文“[Attention is All you Need](https://arxiv.org/abs/1706.03762)”中得到了推广。由于我试图构建的是语言生成器而不是问答机器人，因此仅使用解码器（右侧）架构足以满足这一目的。
- en: '![](../Images/8c46ef5120214bc2b9b854d7989a07ff.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c46ef5120214bc2b9b854d7989a07ff.png)'
- en: '“[Attention is All you Need](https://arxiv.org/abs/1706.03762)” by A. Vaswani
    et. al. (Retrieved from arXiv: 1706.03762)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '“[Attention is All you Need](https://arxiv.org/abs/1706.03762)” 由 A. Vaswani
    等人撰写（取自 arXiv: 1706.03762）'
- en: In the following sections, I will break down each component of the GPT architecture,
    explaining its role and the underlying matrix operations. Starting off with the
    prepared training test, I will trace an exemplary context of 3 words through the
    model, until it leads to a prediction of the next token.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我将分解 GPT 架构的每个组件，解释其作用以及基础的矩阵运算。从准备好的训练测试开始，我将追踪一个示例上下文的3个词，通过模型，直到它预测下一个令牌。
- en: 5.1\. Model Objective
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1\. 模型目标
- en: Before delving into technical specifics, it's crucial to understand our model's
    primary objective. In a decoder-only setup, our aim is to decode the structure
    of language to accurately predict the next token in a sequence, given the context
    of preceding tokens.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入技术细节之前，了解我们模型的主要目标至关重要。在仅解码器的设置中，我们的目标是解码语言的结构，以准确预测序列中的下一个令牌，前提是给定前面的令牌上下文。
- en: '![](../Images/4b54ee5ec49172e5d5534cd0f3eb7e1f.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b54ee5ec49172e5d5534cd0f3eb7e1f.png)'
- en: Image by author
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: As we feed our indexed token sequence into the model, it undergoes a series
    of matrix multiplications with various weight matrices. The output is a vector
    representing the probability of each token being the next in the sequence, based
    on the input context.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将索引的令牌序列输入模型时，它会经历一系列与各种权重矩阵的矩阵乘法运算。输出是一个向量，表示每个令牌在序列中作为下一个令牌的概率，这个概率基于输入上下文。
- en: '**Model Evaluation:**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型评估：**'
- en: Our model’s performance is evaluated against the training data, where the actual
    next token is known. The objective is to maximize the probability of correctly
    predicting this next token.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型性能通过训练数据来评估，其中实际的下一个令牌是已知的。目标是最大化正确预测这个下一个令牌的概率。
- en: However, in machine learning, we often focus on the concept of “loss”, which
    quantifies the error or the likelihood of incorrect predictions. To calculate
    this, we compare the model’s output probabilities with the actual next token (using
    [cross-entropy](https://en.wikipedia.org/wiki/Cross-entropy)).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在机器学习中，我们常常关注“损失”这一概念，它量化了错误或不正确预测的可能性。为了计算这一点，我们将模型的输出概率与实际的下一个令牌进行比较（使用[cross-entropy](https://en.wikipedia.org/wiki/Cross-entropy)）。
- en: '**Optimization:**'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**优化：**'
- en: With an understanding of our current loss, we aim to minimize it through [backpropagation](https://en.wikipedia.org/wiki/Backpropagation).
    This process involves iteratively feeding token sequences into the model and adjusting
    the weight matrices to enhance performance.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 通过了解当前的损失，我们旨在通过[反向传播](https://en.wikipedia.org/wiki/Backpropagation)来最小化它。这个过程涉及迭代地将令牌序列输入模型，并调整权重矩阵以提升性能。
- en: In each figure, I will highlight in yellow the weight matrices that will be
    optimized during that procedure.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在每张图中，我将用黄色标出在该过程中将被优化的权重矩阵。
- en: 5.2\. Output Embedding
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2\. 输出嵌入
- en: Up to this point, each token in our sequence has been represented by an integer
    index. However, this simplistic form doesn’t reflect word relationships or similarities.
    To address this, we elevate our one-dimensional indices into higher-dimensional
    spaces through embedding.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们序列中的每个令牌都由一个整数索引表示。然而，这种简单的形式无法反映单词之间的关系或相似性。为了解决这个问题，我们将一维索引提升到更高维度的空间中，通过嵌入实现。
- en: '**Word-embeddings:** The essence of a word is captured by an n-dimensional
    vector of floats.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词嵌入：** 单词的本质由一个n维的浮点向量来捕捉。'
- en: '**Positional-embeddings:** These embeddings highlight the importance of a word’s
    position within a sentence, also represented as n-dimensional vectors of floats.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**位置嵌入：** 这些嵌入强调了单词在句子中的位置的重要性，也表示为n维的浮点向量。'
- en: For each token, we look up its word-embedding and positional embedding and then
    sum them up element-wise. This results in the output embedding of each token in
    the context.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个令牌，我们查找其词嵌入和位置嵌入，然后逐元素相加。这就得出了每个令牌在上下文中的输出嵌入。
- en: In the below example, the context consists of 3 tokens. At the end of the embedding
    process, each token is represented by an n-dimensional vector (where n is the
    embedding size, a tunable hyperparameter).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，上下文包含3个令牌。在嵌入过程结束时，每个令牌由一个n维向量表示（其中n是嵌入大小，一个可调的超参数）。
- en: '![](../Images/e927158ac25f6689bebf09e273e94d17.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e927158ac25f6689bebf09e273e94d17.png)'
- en: Image by author
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'PyTorch offers dedicated classes for such embeddings. Within our model class,
    we define the word-, and positional embeddings as follows (passing matrix dimensions
    as arguments):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 提供了专门的类来处理这些嵌入。在我们的模型类中，我们定义了词嵌入和位置嵌入，如下所示（传递矩阵维度作为参数）：
- en: '[PRE8]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 5.3\. Self-Attention Head
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3. 自注意力头
- en: While word embeddings provide a general sense of word similarity, the true meaning
    of a word often hinges on its surrounding context. For example, “bat” could refer
    to either an animal or a sports equipment, depending on the sentence. This is
    where the self-attention mechanism, a key component of the GPT architecture, comes
    into play.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然词嵌入提供了词语相似性的整体感觉，但一个词的真实含义往往取决于其周围的上下文。例如，“bat”可能指的是动物或运动器材，这取决于句子。这就是自注意力机制（GPT架构的关键组成部分）发挥作用的地方。
- en: 'The self-attention mechanism focuses on three main concepts: Query (Q), Key
    (K), and Value (V).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力机制关注三个主要概念：查询（Q）、键（K）和值（V）。
- en: '**Query (Q):** The query is essentially a representation of the current token
    for which the attention needs to be calculated. It’s like asking, “What should
    I, as the current token, pay attention to in the rest of the context?”'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**查询（Q）：** 查询本质上是当前标记的表示，注意力需要计算它。就像在问：“作为当前标记，我应该关注上下文中的什么？”'
- en: '**Keys (K):** Keys are representations of each token in the input sequence.
    They are paired with the Query to determine the attention scores. This comparison
    measures how much focus the query token should put on other tokens in the context.
    High scores mean that more attention should be paid.'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**键（K）：** 键是输入序列中每个标记的表示。它们与查询配对，以确定注意力分数。这种比较衡量了查询标记应将多少关注放在上下文中的其他标记上。高分意味着应该更多关注。 '
- en: '**Value (V):** Values are also representations of each token in the input sequence.
    However, their role is different, as they apply a final weighting to the attention-scores.'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**值（V）：** 值也是输入序列中每个标记的表示。然而，它们的作用不同，因为它们对注意力分数施加最终加权。'
- en: '![](../Images/569e854e38e6d0982ebc8bf9c59b1ac1.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/569e854e38e6d0982ebc8bf9c59b1ac1.png)'
- en: Image by author
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: '**Example:**'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：**'
- en: In our example, each token of the context is already in embedded form, as n-dimension
    vectors (e1, e2, e3). The self-attention head takes them as input, to output a
    contextualized version for each of them, one at a time.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，上下文中的每个标记已经是嵌入形式，作为n维向量（e1, e2, e3）。自注意力头将它们作为输入，以逐一输出每个标记的上下文化版本。
- en: When evaluating the token “name”, a query vector **q** is obtained by multiplying
    its embedded vector **v2** with the trainable matrix **M_Q**.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在评估“name”这个标记时，通过将其嵌入向量**v2**与可训练矩阵**M_Q**相乘，得到一个查询向量**q**。
- en: At the same time key vectors **(k1, k2, k3)** are calculated for each token
    in the context, by multiplying each embedded vector **(e1, e2, e3)** with the
    trainable matrix **M_K.**
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同时，为上下文中的每个标记计算键向量**(k1, k2, k3)**，通过将每个嵌入向量**(e1, e2, e3)**与可训练矩阵**M_K**相乘。
- en: The value vectors **(v1, v2, v3)** are obtained in the same way, just multiplied
    by a different trainable matrix **M_V**.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 值向量**(v1, v2, v3)** 以相同的方式获得，只是乘以不同的可训练矩阵**M_V**。
- en: The attention-scores **w** are calculated as dot-product between the query vector
    and each key vector separately.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意力分数**w**通过查询向量与每个键向量之间的点积来计算。
- en: Finally we stack all value vectors to a matrix, and multiply that by the attention
    scores, to obtained the **contextualized vector** for the token “name”.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将所有值向量堆叠成一个矩阵，并将其与注意力分数相乘，以获得标记“name”的**上下文化向量**。
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 5.4\. Masked Multi-Head Attention
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4. 掩蔽多头注意力
- en: Language is complex, and capturing all its nuances isn’t straightforward. A
    single set of attention calculations often isn’t enough to catch the subtleties
    of how words work together. That’s where the idea of multi-head attention in the
    GPT model comes in handy.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 语言是复杂的，捕捉其所有的细微差别并不简单。一组注意力计算通常不足以捕捉词语如何相互作用的细微之处。这就是GPT模型中的多头注意力的理念派上用场的地方。
- en: Think of multi-head attention as having several pairs of eyes looking at the
    data in different ways, each noticing unique details. These separate observations
    are then put together into one big picture. To keep this big picture manageable
    and compatible with the rest of our model, we use a linear layer (trainable weights)
    to compress it back to our original embedding size.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以把多头注意力想象成多个“眼睛”以不同的方式观察数据，每个“眼睛”注意到独特的细节。这些独立的观察结果然后被整合成一个大图景。为了使这个大图景易于管理并与我们模型的其余部分兼容，我们使用线性层（可训练权重）将其压缩回原始的嵌入大小。
- en: Finally, to make sure our model doesn’t just memorize the training data but
    also gets good at making predictions on new text, we use a dropout layer. This
    randomly turns off parts of the data during training, which helps the model become
    more adaptable.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了确保我们的模型不仅仅记住训练数据，还能在新文本上进行良好的预测，我们使用了一个 dropout 层。这个层在训练过程中随机关闭数据的部分内容，帮助模型变得更加适应。
- en: '![](../Images/2760acd6c26176c932ffba3413dbd5f4.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2760acd6c26176c932ffba3413dbd5f4.png)'
- en: Image by author
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 5.5\. Feed Forward
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5\. 前馈
- en: The multi-head attention layer initially captures the contextual relationships
    within the sequence. More depth is added to the network via two consecutive linear
    layers, which collectively constitute the feed-forward neural network.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力层最初捕捉了序列中的上下文关系。通过两个连续的线性层为网络添加了更多深度，这两个层共同构成了前馈神经网络。
- en: '![](../Images/4ea0fae415491644685ddbe6e75da8c3.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ea0fae415491644685ddbe6e75da8c3.png)'
- en: Image by author
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: In the **initial linear layer**, we increase the dimensionality (by a factor
    of 4 in our case) which effectively broadens the network’s capacity to learn and
    represent more complex features. A [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))
    function is applied on each element of the resulting matrix, which enables non-linear
    pattern be to recognized.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在**初始线性层**中，我们增加了维度（在我们的例子中是增加了4倍），这有效地拓宽了网络学习和表示更复杂特征的能力。对结果矩阵的每个元素应用[ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))函数，使得非线性模式能够被识别。
- en: Subsequently, the **second linear layer** acts as a compressor, reducing the
    expanded dimensions back to the original shape (block-size x embedding-size).
    A **dropout layer** concludes the process, randomly deactivating elements of the
    matrix, for the sake of model generalization.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，**第二个线性层**作为一个压缩器，将扩展的维度减少回原始形状（块大小 x 嵌入大小）。**Dropout 层**结束了这个过程，随机地停用矩阵的部分元素，以实现模型的泛化。
- en: '[PRE11]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 5.6\. Add & Norm
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.6\. Add & Norm
- en: 'Now we link together the multi-head attention and feed-forward component, by
    introducing two more crucial elements:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们通过引入两个关键元素，将多头注意力和前馈组件连接在一起：
- en: '**Residual Connections (Add):** These connections perform an element-wise addition
    of a layer’s output to its unaltered input. During training, the model adjusts
    the emphasis on layer transformations based on their usefulness. If a transformation
    is deemed nonessential, its weights and consequently its layer output tend towards
    zero. In this case at least the unaltered input is passed through the residual
    connection. This technique helps mitigating the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem).'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**残差连接（Add）：** 这些连接执行层输出与其未更改输入的逐元素加法。在训练过程中，模型根据层变换的有用性调整对层变换的重视程度。如果某个变换被认为是不必要的，其权重和相应的层输出将趋向于零。在这种情况下，至少未更改的输入会通过残差连接传递。这种技术有助于缓解[梯度消失问题](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)。'
- en: '**Layer Normalization (Norm):** This method normalizes each embedded vector
    in the context by subtracting its mean and dividing by its standard deviation.
    This process also ensures that the gradients during backpropagation neither explode
    nor vanish.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层归一化（Norm）：** 这种方法通过减去嵌入向量的均值并除以其标准差来归一化上下文中的每个嵌入向量。这个过程还确保了在反向传播过程中梯度不会爆炸或消失。'
- en: '![](../Images/6aecc16dc453b3e4359fce4392d6d89c.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6aecc16dc453b3e4359fce4392d6d89c.png)'
- en: Image by author
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: The chain of multi-head attention and feed-forward layers, linked with “Add
    & Norm” is consolidated into a block. This modular design allows us to form a
    sequence of blocks. The number of these blocks is a hyper-parameter, which determines
    the depth of the model architecture.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力和前馈层的链条，通过“Add & Norm”连接，合并成一个块。这种模块化设计使我们能够形成一系列块。这些块的数量是一个超参数，它决定了模型架构的深度。
- en: '[PRE12]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 5.7\. Softmax
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.7\. Softmax
- en: Upon traversing multiple block components, we obtain a matrix of dimensions
    (block-size x embed-size). To reshape this matrix to the required dimensions (block-size
    x vocab size), we pass it through a final linear layer. This shape represents
    an entry for each word in the vocabulary at each position in the context.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在遍历多个块组件后，我们获得了一个维度为（块大小 x 嵌入大小）的矩阵。为了将这个矩阵重塑为所需的维度（块大小 x 词汇表大小），我们将其通过一个最终的线性层。这个形状表示了上下文中每个位置词汇表中每个词的一个条目。
- en: Finally, we apply the soft-max transformation to these values, converting them
    into probabilities. We have successfully obtained a probability distribution for
    the next token at every position in the context.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们对这些值应用soft-max变换，将其转换为概率。我们成功地获得了上下文中每个位置的下一个标记的概率分布。
- en: 6\. Model Training
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6\. 模型训练
- en: To train the language model, I selected token sequences from random positions
    within my training data. Given the fast-paced nature of WhatsApp conversations,
    I determined a context length of 32 words to be sufficient. Thus, I chose random
    32-word chunks as the context input and used the corresponding vectors, shifted
    by one word, as the targets for comparison.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练语言模型，我从训练数据中的随机位置选择了令牌序列。鉴于WhatsApp对话的快节奏，我确定32个词的上下文长度足够。因此，我选择了随机的32词块作为上下文输入，并使用相应的向量（向后移动一个词）作为比较的目标。
- en: 'The training process was looping through the following steps:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程循环执行以下步骤：
- en: Sample multiple batches of context.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对多个批次的上下文进行采样。
- en: Feed these samples into the model to calculate the current loss.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些样本输入模型，以计算当前损失。
- en: Apply backpropagation based on the current loss and model weights.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据当前的损失和模型权重应用反向传播。
- en: Evaluate the loss more comprehensively every 500th iteration.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每500次迭代更全面地评估损失。
- en: Once all other model hyperparameters (like embedding size, number of self-attention
    heads, etc.) were fixed, I finalized a model with 2.5 million parameters. Given
    my limitations on input data size and computational resources, I found this to
    be the optimal setup for me.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有其他模型超参数（如嵌入大小、自注意力头数量等）确定后，我最终选择了一个具有250万参数的模型。考虑到我对输入数据大小和计算资源的限制，我发现这是对我而言的**最佳**设置。
- en: The training process took approximately 12 hours for 10,000 iterations. One
    can see that training could have been stopped earlier, as the spread between the
    loss on the validation and training sets widens.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程大约花费了12小时，完成了10,000次迭代。可以看到，训练本可以更早停止，因为验证集和训练集上的损失差距在扩大。
- en: '![](../Images/f097aae828c46fb03488c7b46270e671.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f097aae828c46fb03488c7b46270e671.png)'
- en: Image by author
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: '[PRE13]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 7\. Chat-Mode
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7\. 聊天模式
- en: To interact with the trained model, I created a function that allows selecting
    a contact name via a dropdown menu and inputting a message for the model to respond
    to. The parameter “n_chats” determines the number of responses the model generates
    at once. The model concludes a generated message when it predicts the <END> token
    as the next token.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与训练好的模型进行交互，我创建了一个函数，允许通过下拉菜单选择联系人姓名，并输入消息供模型响应。参数“n_chats”决定模型一次生成的响应数量。当模型预测<END>标记为下一个标记时，模型结束生成消息。
- en: '[PRE14]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Conclusion:**'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**结论：**'
- en: Due to the privacy of my personal chats, I am unable to present example prompts
    and conversations here.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 由于个人聊天的隐私，我无法在这里展示示例提示和对话。
- en: Nonetheless, you can expect a model of this scale to successfully learn the
    general structure of sentences, producing meaningful outputs in terms of word
    order. In my case, it also picked up context for certain topics that were prominent
    in the training data. For instance, as my personal chats often revolve around
    tennis, the names of tennis players and tennis-related words were typically output
    together.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，你可以期待这样规模的模型能够成功学习句子的总体结构，产生有意义的输出，尤其是在词序方面。在我的案例中，它也掌握了训练数据中某些重要话题的上下文。例如，由于我的个人聊天经常围绕网球展开，网球运动员的名字和与网球相关的词通常会一起输出。
- en: However, when evaluating the coherence of the generated sentences, I concede
    that the results did not quite meet my already modest expectations. But of course,
    I could also blame my friends for chatting too much nonsense, limiting the models'
    ability to learn something useful...
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在评估生成句子的连贯性时，我承认结果并没有达到我已经很低的期望。当然，我也可以责怪我的朋友们聊了太多无聊的话，限制了模型学习有用内容的能力...
- en: To show at least some example output at the end, you can see how the dummy model
    performs on 200 trained dummy messages ;)
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在结尾展示至少一些示例输出，你可以查看虚拟模型在200条训练虚拟消息上的表现 ;)
- en: '![](../Images/4ead9603facfac34d9b5e0d5f3cdf9ff.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ead9603facfac34d9b5e0d5f3cdf9ff.png)'
- en: Image by author
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
