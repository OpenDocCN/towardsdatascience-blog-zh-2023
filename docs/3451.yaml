- en: Build a Language Model on Your WhatsApp Chats
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/build-a-language-model-on-your-whatsapp-chats-31264a9ced90?source=collection_archive---------1-----------------------#2023-11-21](https://towardsdatascience.com/build-a-language-model-on-your-whatsapp-chats-31264a9ced90?source=collection_archive---------1-----------------------#2023-11-21)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A visual guide through the GPT architecture with an application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://bernhard-pfann.medium.com/?source=post_page-----31264a9ced90--------------------------------)[![Bernhard
    Pfann, CFA](../Images/00988e4640d9dfc4f999bb67204c8109.png)](https://bernhard-pfann.medium.com/?source=post_page-----31264a9ced90--------------------------------)[](https://towardsdatascience.com/?source=post_page-----31264a9ced90--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----31264a9ced90--------------------------------)
    [Bernhard Pfann, CFA](https://bernhard-pfann.medium.com/?source=post_page-----31264a9ced90--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb2add0f92c53&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-language-model-on-your-whatsapp-chats-31264a9ced90&user=Bernhard+Pfann%2C+CFA&userId=b2add0f92c53&source=post_page-b2add0f92c53----31264a9ced90---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----31264a9ced90--------------------------------)
    ·16 min read·Nov 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F31264a9ced90&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-language-model-on-your-whatsapp-chats-31264a9ced90&user=Bernhard+Pfann%2C+CFA&userId=b2add0f92c53&source=-----31264a9ced90---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F31264a9ced90&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-language-model-on-your-whatsapp-chats-31264a9ced90&source=-----31264a9ced90---------------------bookmark_footer-----------)![](../Images/0acb87053cc35cfd27f5cef359322dd3.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Volodymyr Hryshchenko](https://unsplash.com/@lunarts?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Chatbots have undeniably transformed our interaction with digital platforms.
    Despite the impressive advancements in the capabilities of underlying language
    models to handle complex tasks, the user experience often falls short, feeling
    impersonal and detached.
  prefs: []
  type: TYPE_NORMAL
- en: To make conversations more relatable, I envisioned a chatbot that could emulate
    my casual writing style, akin to texting a friend over WhatsApp.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I’ll walk you through my journey of building a (small) language
    model that generates synthetic conversations, using my WhatsApp chat messages
    as input data. Along the way, I try to unravel the inner workings of the GPT architecture
    in a visual and hopefully digestible way, complemented by the actual Python implementation.
    You can find the full project on my [GitHub](https://github.com/bernhard-pfann/lad-gpt).
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** The [model class](https://github.com/bernhard-pfann/lad-gpt/blob/main/src/model.py)
    itself is in large chunks taken from the [video series of Andrej Karpathy](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&pp=iAQB)and
    adapted for my needs. I can highly recommend his tutorials.'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/bernhard-pfann/lad-gpt?source=post_page-----31264a9ced90--------------------------------)
    [## lad-gpt'
  prefs: []
  type: TYPE_NORMAL
- en: Train a language model from scratch and entirely based on your WhatsApp group
    chats.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/bernhard-pfann/lad-gpt?source=post_page-----31264a9ced90--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Table of Contents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Selected Approach
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data Source
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Indexing
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model Architecture
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model Training
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chat-Mode
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1\. Selected Approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When it comes to tailoring a language model to a specific corpus of data, there
    are several approaches one can take:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model building:** This involves constructing and training a model from scratch,
    providing the utmost flexibility in terms of model architecture and training data
    selection.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Fine-tuning:** This approach leverages an existing pre-trained model, adjusting
    its weights to align more closely with the specific data at hand.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Prompt engineering:** This also utilizes an existing pre-trained model, but
    here, the unique corpus is directly incorporated into the prompt, without changing
    the model’s weights.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As my motivation for this project is mainly self-educational, and I am rather
    interested in the architecture of modern language models, I opted for the first
    approach. Yet, this choice comes with obvious limitations. Given the size of my
    data and available computational resources, I didn’t anticipate results on par
    with any state-of-the-art pre-trained models.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, I was hopeful that my model would pick up some interesting linguistic
    patterns, which it ultimately did. Exploring the second option (fine-tuning) might
    be the focus of a future article.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Data Source
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: WhatsApp, my primary communication channel, was an ideal source for capturing
    my conversational style. Exporting over six years of group chat history, totaling
    more than 1.5 million words was straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: The data was parsed using a regex pattern into a list of tuples containing the
    date, contact name, and chat message.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now, each element was processed separately.
  prefs: []
  type: TYPE_NORMAL
- en: '**Sending date:** Aside from converting it into a datetime object, I have not
    utilized this information. However, one could look at the time deltas to differentiate
    the start and end of topic discussions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contact name:** When tokenizing the text, each contact name is treated as
    a unique token. This ensures that the combination of first and last names is still
    considered a single entity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chat message:** A special “<END>” token is added at the end of each message.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3\. Tokenization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To train a language model, we need to break language into pieces (so-called
    tokens) and feed them to the model incrementally. Tokenization can be performed
    on multiple levels.
  prefs: []
  type: TYPE_NORMAL
- en: '**Character-level:** Text is perceived as a sequence of individual characters
    (including white spaces). This granular approach allows every possible word to
    be formed from a sequence of characters. However, it is more difficult to capture
    semantic relationships between words.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Word-level:** Text is represented as a sequence of words. However, the model’s
    vocabulary is limited by the existing words in the training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sub-word-level:** Text is broken down into sub-word units, which are smaller
    than words but larger than characters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While I started off with a character-level tokenizer, I felt that training time
    was wasted, learning character sequences of repetitive words, rather than focusing
    on the semantic relationship between words across the sentence.
  prefs: []
  type: TYPE_NORMAL
- en: For the sake of conceptual simplicity, I decided to switch to a word-level tokenizer,
    keeping aside the available libraries for more sophisticated tokenization strategies.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: It turned out that my training data has a vocabulary of ~70,000 unique words.
    However, as many words appear only once or twice, I decided to replace such rare
    words by a “<UNK>” special token. This had the effect of reducing vocabulary to
    ~25,000 words, which leads to a smaller model that needs to be trained later.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 4\. Indexing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After tokenization, the next step is to convert the words and special tokens
    into numerical representations. Using a fixed vocabulary list, each word was indexed
    by its position. The encoded words were then prepared as PyTorch tensors.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As we need to evaluate the quality of our model against some unseen data, we
    split the tensor into two parts. And voila, we have our training and validation
    sets, ready to feed to the language model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ee2c75bbcfee5677b0b6d39ca209f864.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Model Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I decided to apply the GPT architecture, which has been promoted by the influential
    paper “[Attention is All you Need](https://arxiv.org/abs/1706.03762)”. Since I
    tried to build a language generator and not a question-answer bot, the decoder-only
    (right side) architecture was sufficient for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8c46ef5120214bc2b9b854d7989a07ff.png)'
  prefs: []
  type: TYPE_IMG
- en: '“[Attention is All you Need](https://arxiv.org/abs/1706.03762)” by A. Vaswani
    et. al. (Retrieved from arXiv: 1706.03762)'
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, I will break down each component of the GPT architecture,
    explaining its role and the underlying matrix operations. Starting off with the
    prepared training test, I will trace an exemplary context of 3 words through the
    model, until it leads to a prediction of the next token.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. Model Objective
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before delving into technical specifics, it's crucial to understand our model's
    primary objective. In a decoder-only setup, our aim is to decode the structure
    of language to accurately predict the next token in a sequence, given the context
    of preceding tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b54ee5ec49172e5d5534cd0f3eb7e1f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: As we feed our indexed token sequence into the model, it undergoes a series
    of matrix multiplications with various weight matrices. The output is a vector
    representing the probability of each token being the next in the sequence, based
    on the input context.
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Evaluation:**'
  prefs: []
  type: TYPE_NORMAL
- en: Our model’s performance is evaluated against the training data, where the actual
    next token is known. The objective is to maximize the probability of correctly
    predicting this next token.
  prefs: []
  type: TYPE_NORMAL
- en: However, in machine learning, we often focus on the concept of “loss”, which
    quantifies the error or the likelihood of incorrect predictions. To calculate
    this, we compare the model’s output probabilities with the actual next token (using
    [cross-entropy](https://en.wikipedia.org/wiki/Cross-entropy)).
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimization:**'
  prefs: []
  type: TYPE_NORMAL
- en: With an understanding of our current loss, we aim to minimize it through [backpropagation](https://en.wikipedia.org/wiki/Backpropagation).
    This process involves iteratively feeding token sequences into the model and adjusting
    the weight matrices to enhance performance.
  prefs: []
  type: TYPE_NORMAL
- en: In each figure, I will highlight in yellow the weight matrices that will be
    optimized during that procedure.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Output Embedding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Up to this point, each token in our sequence has been represented by an integer
    index. However, this simplistic form doesn’t reflect word relationships or similarities.
    To address this, we elevate our one-dimensional indices into higher-dimensional
    spaces through embedding.
  prefs: []
  type: TYPE_NORMAL
- en: '**Word-embeddings:** The essence of a word is captured by an n-dimensional
    vector of floats.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Positional-embeddings:** These embeddings highlight the importance of a word’s
    position within a sentence, also represented as n-dimensional vectors of floats.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each token, we look up its word-embedding and positional embedding and then
    sum them up element-wise. This results in the output embedding of each token in
    the context.
  prefs: []
  type: TYPE_NORMAL
- en: In the below example, the context consists of 3 tokens. At the end of the embedding
    process, each token is represented by an n-dimensional vector (where n is the
    embedding size, a tunable hyperparameter).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e927158ac25f6689bebf09e273e94d17.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch offers dedicated classes for such embeddings. Within our model class,
    we define the word-, and positional embeddings as follows (passing matrix dimensions
    as arguments):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 5.3\. Self-Attention Head
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While word embeddings provide a general sense of word similarity, the true meaning
    of a word often hinges on its surrounding context. For example, “bat” could refer
    to either an animal or a sports equipment, depending on the sentence. This is
    where the self-attention mechanism, a key component of the GPT architecture, comes
    into play.
  prefs: []
  type: TYPE_NORMAL
- en: 'The self-attention mechanism focuses on three main concepts: Query (Q), Key
    (K), and Value (V).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Query (Q):** The query is essentially a representation of the current token
    for which the attention needs to be calculated. It’s like asking, “What should
    I, as the current token, pay attention to in the rest of the context?”'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Keys (K):** Keys are representations of each token in the input sequence.
    They are paired with the Query to determine the attention scores. This comparison
    measures how much focus the query token should put on other tokens in the context.
    High scores mean that more attention should be paid.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Value (V):** Values are also representations of each token in the input sequence.
    However, their role is different, as they apply a final weighting to the attention-scores.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/569e854e38e6d0982ebc8bf9c59b1ac1.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '**Example:**'
  prefs: []
  type: TYPE_NORMAL
- en: In our example, each token of the context is already in embedded form, as n-dimension
    vectors (e1, e2, e3). The self-attention head takes them as input, to output a
    contextualized version for each of them, one at a time.
  prefs: []
  type: TYPE_NORMAL
- en: When evaluating the token “name”, a query vector **q** is obtained by multiplying
    its embedded vector **v2** with the trainable matrix **M_Q**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the same time key vectors **(k1, k2, k3)** are calculated for each token
    in the context, by multiplying each embedded vector **(e1, e2, e3)** with the
    trainable matrix **M_K.**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The value vectors **(v1, v2, v3)** are obtained in the same way, just multiplied
    by a different trainable matrix **M_V**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The attention-scores **w** are calculated as dot-product between the query vector
    and each key vector separately.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally we stack all value vectors to a matrix, and multiply that by the attention
    scores, to obtained the **contextualized vector** for the token “name”.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 5.4\. Masked Multi-Head Attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Language is complex, and capturing all its nuances isn’t straightforward. A
    single set of attention calculations often isn’t enough to catch the subtleties
    of how words work together. That’s where the idea of multi-head attention in the
    GPT model comes in handy.
  prefs: []
  type: TYPE_NORMAL
- en: Think of multi-head attention as having several pairs of eyes looking at the
    data in different ways, each noticing unique details. These separate observations
    are then put together into one big picture. To keep this big picture manageable
    and compatible with the rest of our model, we use a linear layer (trainable weights)
    to compress it back to our original embedding size.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, to make sure our model doesn’t just memorize the training data but
    also gets good at making predictions on new text, we use a dropout layer. This
    randomly turns off parts of the data during training, which helps the model become
    more adaptable.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2760acd6c26176c932ffba3413dbd5f4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 5.5\. Feed Forward
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The multi-head attention layer initially captures the contextual relationships
    within the sequence. More depth is added to the network via two consecutive linear
    layers, which collectively constitute the feed-forward neural network.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4ea0fae415491644685ddbe6e75da8c3.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: In the **initial linear layer**, we increase the dimensionality (by a factor
    of 4 in our case) which effectively broadens the network’s capacity to learn and
    represent more complex features. A [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))
    function is applied on each element of the resulting matrix, which enables non-linear
    pattern be to recognized.
  prefs: []
  type: TYPE_NORMAL
- en: Subsequently, the **second linear layer** acts as a compressor, reducing the
    expanded dimensions back to the original shape (block-size x embedding-size).
    A **dropout layer** concludes the process, randomly deactivating elements of the
    matrix, for the sake of model generalization.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 5.6\. Add & Norm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we link together the multi-head attention and feed-forward component, by
    introducing two more crucial elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Residual Connections (Add):** These connections perform an element-wise addition
    of a layer’s output to its unaltered input. During training, the model adjusts
    the emphasis on layer transformations based on their usefulness. If a transformation
    is deemed nonessential, its weights and consequently its layer output tend towards
    zero. In this case at least the unaltered input is passed through the residual
    connection. This technique helps mitigating the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Layer Normalization (Norm):** This method normalizes each embedded vector
    in the context by subtracting its mean and dividing by its standard deviation.
    This process also ensures that the gradients during backpropagation neither explode
    nor vanish.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/6aecc16dc453b3e4359fce4392d6d89c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: The chain of multi-head attention and feed-forward layers, linked with “Add
    & Norm” is consolidated into a block. This modular design allows us to form a
    sequence of blocks. The number of these blocks is a hyper-parameter, which determines
    the depth of the model architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 5.7\. Softmax
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Upon traversing multiple block components, we obtain a matrix of dimensions
    (block-size x embed-size). To reshape this matrix to the required dimensions (block-size
    x vocab size), we pass it through a final linear layer. This shape represents
    an entry for each word in the vocabulary at each position in the context.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we apply the soft-max transformation to these values, converting them
    into probabilities. We have successfully obtained a probability distribution for
    the next token at every position in the context.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Model Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To train the language model, I selected token sequences from random positions
    within my training data. Given the fast-paced nature of WhatsApp conversations,
    I determined a context length of 32 words to be sufficient. Thus, I chose random
    32-word chunks as the context input and used the corresponding vectors, shifted
    by one word, as the targets for comparison.
  prefs: []
  type: TYPE_NORMAL
- en: 'The training process was looping through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Sample multiple batches of context.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feed these samples into the model to calculate the current loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply backpropagation based on the current loss and model weights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the loss more comprehensively every 500th iteration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once all other model hyperparameters (like embedding size, number of self-attention
    heads, etc.) were fixed, I finalized a model with 2.5 million parameters. Given
    my limitations on input data size and computational resources, I found this to
    be the optimal setup for me.
  prefs: []
  type: TYPE_NORMAL
- en: The training process took approximately 12 hours for 10,000 iterations. One
    can see that training could have been stopped earlier, as the spread between the
    loss on the validation and training sets widens.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f097aae828c46fb03488c7b46270e671.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 7\. Chat-Mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To interact with the trained model, I created a function that allows selecting
    a contact name via a dropdown menu and inputting a message for the model to respond
    to. The parameter “n_chats” determines the number of responses the model generates
    at once. The model concludes a generated message when it predicts the <END> token
    as the next token.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '**Conclusion:**'
  prefs: []
  type: TYPE_NORMAL
- en: Due to the privacy of my personal chats, I am unable to present example prompts
    and conversations here.
  prefs: []
  type: TYPE_NORMAL
- en: Nonetheless, you can expect a model of this scale to successfully learn the
    general structure of sentences, producing meaningful outputs in terms of word
    order. In my case, it also picked up context for certain topics that were prominent
    in the training data. For instance, as my personal chats often revolve around
    tennis, the names of tennis players and tennis-related words were typically output
    together.
  prefs: []
  type: TYPE_NORMAL
- en: However, when evaluating the coherence of the generated sentences, I concede
    that the results did not quite meet my already modest expectations. But of course,
    I could also blame my friends for chatting too much nonsense, limiting the models'
    ability to learn something useful...
  prefs: []
  type: TYPE_NORMAL
- en: To show at least some example output at the end, you can see how the dummy model
    performs on 200 trained dummy messages ;)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4ead9603facfac34d9b5e0d5f3cdf9ff.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
