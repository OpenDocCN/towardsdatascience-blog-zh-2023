- en: Build a Language Model on Your WhatsApp Chats
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/build-a-language-model-on-your-whatsapp-chats-31264a9ced90?source=collection_archive---------1-----------------------#2023-11-21](https://towardsdatascience.com/build-a-language-model-on-your-whatsapp-chats-31264a9ced90?source=collection_archive---------1-----------------------#2023-11-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A visual guide through the GPT architecture with an application
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://bernhard-pfann.medium.com/?source=post_page-----31264a9ced90--------------------------------)[![Bernhard
    Pfann, CFA](../Images/00988e4640d9dfc4f999bb67204c8109.png)](https://bernhard-pfann.medium.com/?source=post_page-----31264a9ced90--------------------------------)[](https://towardsdatascience.com/?source=post_page-----31264a9ced90--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----31264a9ced90--------------------------------)
    [Bernhard Pfann, CFA](https://bernhard-pfann.medium.com/?source=post_page-----31264a9ced90--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb2add0f92c53&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-language-model-on-your-whatsapp-chats-31264a9ced90&user=Bernhard+Pfann%2C+CFA&userId=b2add0f92c53&source=post_page-b2add0f92c53----31264a9ced90---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----31264a9ced90--------------------------------)
    ·16 min read·Nov 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F31264a9ced90&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-language-model-on-your-whatsapp-chats-31264a9ced90&user=Bernhard+Pfann%2C+CFA&userId=b2add0f92c53&source=-----31264a9ced90---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F31264a9ced90&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-language-model-on-your-whatsapp-chats-31264a9ced90&source=-----31264a9ced90---------------------bookmark_footer-----------)![](../Images/0acb87053cc35cfd27f5cef359322dd3.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Volodymyr Hryshchenko](https://unsplash.com/@lunarts?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Chatbots have undeniably transformed our interaction with digital platforms.
    Despite the impressive advancements in the capabilities of underlying language
    models to handle complex tasks, the user experience often falls short, feeling
    impersonal and detached.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: To make conversations more relatable, I envisioned a chatbot that could emulate
    my casual writing style, akin to texting a friend over WhatsApp.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I’ll walk you through my journey of building a (small) language
    model that generates synthetic conversations, using my WhatsApp chat messages
    as input data. Along the way, I try to unravel the inner workings of the GPT architecture
    in a visual and hopefully digestible way, complemented by the actual Python implementation.
    You can find the full project on my [GitHub](https://github.com/bernhard-pfann/lad-gpt).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将带你了解我构建一个（小型）语言模型的过程，该模型生成合成对话，使用我的 WhatsApp 聊天记录作为输入数据。过程中，我尝试以可视化且希望易于理解的方式解开
    GPT 架构的内部工作机制，并附有实际的 Python 实现。你可以在我的 [GitHub](https://github.com/bernhard-pfann/lad-gpt)
    上找到完整项目。
- en: '**Note:** The [model class](https://github.com/bernhard-pfann/lad-gpt/blob/main/src/model.py)
    itself is in large chunks taken from the [video series of Andrej Karpathy](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&pp=iAQB)and
    adapted for my needs. I can highly recommend his tutorials.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** [模型类](https://github.com/bernhard-pfann/lad-gpt/blob/main/src/model.py)
    本身大块取自 [Andrej Karpathy 的视频系列](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&pp=iAQB)，并根据我的需要进行了调整。我强烈推荐他的教程。'
- en: '[](https://github.com/bernhard-pfann/lad-gpt?source=post_page-----31264a9ced90--------------------------------)
    [## lad-gpt'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/bernhard-pfann/lad-gpt?source=post_page-----31264a9ced90--------------------------------)
    [## lad-gpt'
- en: Train a language model from scratch and entirely based on your WhatsApp group
    chats.
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从头开始训练一个语言模型，完全基于你的 WhatsApp 群聊。
- en: github.com](https://github.com/bernhard-pfann/lad-gpt?source=post_page-----31264a9ced90--------------------------------)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/bernhard-pfann/lad-gpt?source=post_page-----31264a9ced90--------------------------------)
- en: Table of Contents
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目录
- en: Selected Approach
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选定的方法
- en: Data Source
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据来源
- en: Tokenization
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分词
- en: Indexing
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 索引
- en: Model Architecture
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型架构
- en: Model Training
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型训练
- en: Chat-Mode
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 聊天模式
- en: 1\. Selected Approach
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1. 选定的方法
- en: 'When it comes to tailoring a language model to a specific corpus of data, there
    are several approaches one can take:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在将语言模型定制为特定语料库时，可以采取几种方法：
- en: '**Model building:** This involves constructing and training a model from scratch,
    providing the utmost flexibility in terms of model architecture and training data
    selection.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型构建：** 这涉及从头开始构建和训练模型，在模型架构和训练数据选择方面提供了最大的灵活性。'
- en: '**Fine-tuning:** This approach leverages an existing pre-trained model, adjusting
    its weights to align more closely with the specific data at hand.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**微调：** 这种方法利用现有的预训练模型，调整其权重以更紧密地与手头的特定数据对齐。'
- en: '**Prompt engineering:** This also utilizes an existing pre-trained model, but
    here, the unique corpus is directly incorporated into the prompt, without changing
    the model’s weights.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**提示工程：** 这也利用了现有的预训练模型，但在这里，独特的语料库直接融入提示中，而不改变模型的权重。'
- en: As my motivation for this project is mainly self-educational, and I am rather
    interested in the architecture of modern language models, I opted for the first
    approach. Yet, this choice comes with obvious limitations. Given the size of my
    data and available computational resources, I didn’t anticipate results on par
    with any state-of-the-art pre-trained models.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我对这个项目的动机主要是自我教育，并且对现代语言模型的架构非常感兴趣，我选择了第一种方法。然而，这种选择带来了明显的限制。鉴于我的数据量和可用计算资源，我并没有期望与任何最先进的预训练模型相当的结果。
- en: Nevertheless, I was hopeful that my model would pick up some interesting linguistic
    patterns, which it ultimately did. Exploring the second option (fine-tuning) might
    be the focus of a future article.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，我仍希望我的模型能发现一些有趣的语言模式，最终它确实做到了。探索第二种选项（微调）可能会成为未来文章的重点。
- en: 2\. Data Source
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2. 数据来源
- en: WhatsApp, my primary communication channel, was an ideal source for capturing
    my conversational style. Exporting over six years of group chat history, totaling
    more than 1.5 million words was straightforward.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: WhatsApp，作为我的主要沟通渠道，是捕捉我的对话风格的理想来源。导出超过六年的群聊记录，总计超过150万字是非常简单的。
- en: The data was parsed using a regex pattern into a list of tuples containing the
    date, contact name, and chat message.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 数据使用正则表达式模式解析成包含日期、联系人姓名和聊天消息的元组列表。
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now, each element was processed separately.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，每个元素都被单独处理。
- en: '**Sending date:** Aside from converting it into a datetime object, I have not
    utilized this information. However, one could look at the time deltas to differentiate
    the start and end of topic discussions.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**发送日期：** 除了将其转换为日期时间对象外，我没有利用这些信息。然而，可以查看时间差异以区分话题讨论的开始和结束。'
- en: '**Contact name:** When tokenizing the text, each contact name is treated as
    a unique token. This ensures that the combination of first and last names is still
    considered a single entity.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**联系人姓名：** 在分词文本时，每个联系人姓名被视为一个唯一的标记。这确保了名和姓的组合仍被视为一个整体。'
- en: '**Chat message:** A special “<END>” token is added at the end of each message.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聊天消息：** 在每条消息的末尾添加了一个特殊的“<END>”标记。'
- en: 3\. Tokenization
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 分词
- en: To train a language model, we need to break language into pieces (so-called
    tokens) and feed them to the model incrementally. Tokenization can be performed
    on multiple levels.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练语言模型，我们需要将语言分解成片段（所谓的标记），并逐步输入模型。分词可以在多个层次上进行。
- en: '**Character-level:** Text is perceived as a sequence of individual characters
    (including white spaces). This granular approach allows every possible word to
    be formed from a sequence of characters. However, it is more difficult to capture
    semantic relationships between words.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**字符级别：** 文本被视为一系列单独的字符（包括空格）。这种细粒度的方法允许从字符序列中形成每一个可能的单词。然而，捕捉单词之间的语义关系会更困难。'
- en: '**Word-level:** Text is represented as a sequence of words. However, the model’s
    vocabulary is limited by the existing words in the training data.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词级别：** 文本被表示为一个单词序列。然而，模型的词汇量受到训练数据中现有单词的限制。'
- en: '**Sub-word-level:** Text is broken down into sub-word units, which are smaller
    than words but larger than characters.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**子词级别：** 文本被拆分成比单词小但比字符大的子词单元。'
- en: While I started off with a character-level tokenizer, I felt that training time
    was wasted, learning character sequences of repetitive words, rather than focusing
    on the semantic relationship between words across the sentence.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我最初使用的是字符级别的分词器，但我觉得训练时间被浪费在了学习重复单词的字符序列上，而不是关注句子中单词之间的语义关系。
- en: For the sake of conceptual simplicity, I decided to switch to a word-level tokenizer,
    keeping aside the available libraries for more sophisticated tokenization strategies.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了概念上的简洁，我决定切换到词级别的分词器，暂时搁置了用于更复杂分词策略的现有库。
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: It turned out that my training data has a vocabulary of ~70,000 unique words.
    However, as many words appear only once or twice, I decided to replace such rare
    words by a “<UNK>” special token. This had the effect of reducing vocabulary to
    ~25,000 words, which leads to a smaller model that needs to be trained later.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示我的训练数据有大约 70,000 个独特的单词。然而，由于许多单词仅出现一次或两次，我决定用“<UNK>”特殊标记替代这些稀有单词。这减少了词汇量至大约
    25,000 个单词，从而得到一个较小的模型，后续训练也会更简单。
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 4\. Indexing
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 索引
- en: After tokenization, the next step is to convert the words and special tokens
    into numerical representations. Using a fixed vocabulary list, each word was indexed
    by its position. The encoded words were then prepared as PyTorch tensors.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在分词之后，下一步是将单词和特殊标记转换为数值表示。使用固定的词汇表，每个单词按其位置进行了索引。编码后的单词随后被准备为 PyTorch 张量。
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As we need to evaluate the quality of our model against some unseen data, we
    split the tensor into two parts. And voila, we have our training and validation
    sets, ready to feed to the language model.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们需要评估模型在一些未见数据上的质量，我们将张量分成两部分。这样，我们就得到了训练集和验证集，可以准备好喂给语言模型。
- en: '![](../Images/ee2c75bbcfee5677b0b6d39ca209f864.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee2c75bbcfee5677b0b6d39ca209f864.png)'
- en: Image by author
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 5\. Model Architecture
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 模型架构
- en: I decided to apply the GPT architecture, which has been promoted by the influential
    paper “[Attention is All you Need](https://arxiv.org/abs/1706.03762)”. Since I
    tried to build a language generator and not a question-answer bot, the decoder-only
    (right side) architecture was sufficient for this purpose.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我决定应用 GPT 架构，这一架构在具有影响力的论文“[Attention is All you Need](https://arxiv.org/abs/1706.03762)”中得到了推广。由于我试图构建的是语言生成器而不是问答机器人，因此仅使用解码器（右侧）架构足以满足这一目的。
- en: '![](../Images/8c46ef5120214bc2b9b854d7989a07ff.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c46ef5120214bc2b9b854d7989a07ff.png)'
- en: '“[Attention is All you Need](https://arxiv.org/abs/1706.03762)” by A. Vaswani
    et. al. (Retrieved from arXiv: 1706.03762)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '“[Attention is All you Need](https://arxiv.org/abs/1706.03762)” 由 A. Vaswani
    等人撰写（取自 arXiv: 1706.03762）'
- en: In the following sections, I will break down each component of the GPT architecture,
    explaining its role and the underlying matrix operations. Starting off with the
    prepared training test, I will trace an exemplary context of 3 words through the
    model, until it leads to a prediction of the next token.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. Model Objective
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before delving into technical specifics, it's crucial to understand our model's
    primary objective. In a decoder-only setup, our aim is to decode the structure
    of language to accurately predict the next token in a sequence, given the context
    of preceding tokens.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b54ee5ec49172e5d5534cd0f3eb7e1f.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: Image by author
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: As we feed our indexed token sequence into the model, it undergoes a series
    of matrix multiplications with various weight matrices. The output is a vector
    representing the probability of each token being the next in the sequence, based
    on the input context.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Evaluation:**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Our model’s performance is evaluated against the training data, where the actual
    next token is known. The objective is to maximize the probability of correctly
    predicting this next token.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: However, in machine learning, we often focus on the concept of “loss”, which
    quantifies the error or the likelihood of incorrect predictions. To calculate
    this, we compare the model’s output probabilities with the actual next token (using
    [cross-entropy](https://en.wikipedia.org/wiki/Cross-entropy)).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimization:**'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: With an understanding of our current loss, we aim to minimize it through [backpropagation](https://en.wikipedia.org/wiki/Backpropagation).
    This process involves iteratively feeding token sequences into the model and adjusting
    the weight matrices to enhance performance.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: In each figure, I will highlight in yellow the weight matrices that will be
    optimized during that procedure.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Output Embedding
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Up to this point, each token in our sequence has been represented by an integer
    index. However, this simplistic form doesn’t reflect word relationships or similarities.
    To address this, we elevate our one-dimensional indices into higher-dimensional
    spaces through embedding.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '**Word-embeddings:** The essence of a word is captured by an n-dimensional
    vector of floats.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Positional-embeddings:** These embeddings highlight the importance of a word’s
    position within a sentence, also represented as n-dimensional vectors of floats.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each token, we look up its word-embedding and positional embedding and then
    sum them up element-wise. This results in the output embedding of each token in
    the context.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: In the below example, the context consists of 3 tokens. At the end of the embedding
    process, each token is represented by an n-dimensional vector (where n is the
    embedding size, a tunable hyperparameter).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e927158ac25f6689bebf09e273e94d17.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
- en: Image by author
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch offers dedicated classes for such embeddings. Within our model class,
    we define the word-, and positional embeddings as follows (passing matrix dimensions
    as arguments):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 5.3\. Self-Attention Head
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While word embeddings provide a general sense of word similarity, the true meaning
    of a word often hinges on its surrounding context. For example, “bat” could refer
    to either an animal or a sports equipment, depending on the sentence. This is
    where the self-attention mechanism, a key component of the GPT architecture, comes
    into play.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'The self-attention mechanism focuses on three main concepts: Query (Q), Key
    (K), and Value (V).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '**Query (Q):** The query is essentially a representation of the current token
    for which the attention needs to be calculated. It’s like asking, “What should
    I, as the current token, pay attention to in the rest of the context?”'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Keys (K):** Keys are representations of each token in the input sequence.
    They are paired with the Query to determine the attention scores. This comparison
    measures how much focus the query token should put on other tokens in the context.
    High scores mean that more attention should be paid.'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Value (V):** Values are also representations of each token in the input sequence.
    However, their role is different, as they apply a final weighting to the attention-scores.'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/569e854e38e6d0982ebc8bf9c59b1ac1.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
- en: Image by author
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '**Example:**'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: In our example, each token of the context is already in embedded form, as n-dimension
    vectors (e1, e2, e3). The self-attention head takes them as input, to output a
    contextualized version for each of them, one at a time.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: When evaluating the token “name”, a query vector **q** is obtained by multiplying
    its embedded vector **v2** with the trainable matrix **M_Q**.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the same time key vectors **(k1, k2, k3)** are calculated for each token
    in the context, by multiplying each embedded vector **(e1, e2, e3)** with the
    trainable matrix **M_K.**
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The value vectors **(v1, v2, v3)** are obtained in the same way, just multiplied
    by a different trainable matrix **M_V**.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The attention-scores **w** are calculated as dot-product between the query vector
    and each key vector separately.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally we stack all value vectors to a matrix, and multiply that by the attention
    scores, to obtained the **contextualized vector** for the token “name”.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 5.4\. Masked Multi-Head Attention
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Language is complex, and capturing all its nuances isn’t straightforward. A
    single set of attention calculations often isn’t enough to catch the subtleties
    of how words work together. That’s where the idea of multi-head attention in the
    GPT model comes in handy.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Think of multi-head attention as having several pairs of eyes looking at the
    data in different ways, each noticing unique details. These separate observations
    are then put together into one big picture. To keep this big picture manageable
    and compatible with the rest of our model, we use a linear layer (trainable weights)
    to compress it back to our original embedding size.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Finally, to make sure our model doesn’t just memorize the training data but
    also gets good at making predictions on new text, we use a dropout layer. This
    randomly turns off parts of the data during training, which helps the model become
    more adaptable.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2760acd6c26176c932ffba3413dbd5f4.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
- en: Image by author
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 5.5\. Feed Forward
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The multi-head attention layer initially captures the contextual relationships
    within the sequence. More depth is added to the network via two consecutive linear
    layers, which collectively constitute the feed-forward neural network.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4ea0fae415491644685ddbe6e75da8c3.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: Image by author
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: In the **initial linear layer**, we increase the dimensionality (by a factor
    of 4 in our case) which effectively broadens the network’s capacity to learn and
    represent more complex features. A [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))
    function is applied on each element of the resulting matrix, which enables non-linear
    pattern be to recognized.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Subsequently, the **second linear layer** acts as a compressor, reducing the
    expanded dimensions back to the original shape (block-size x embedding-size).
    A **dropout layer** concludes the process, randomly deactivating elements of the
    matrix, for the sake of model generalization.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 5.6\. Add & Norm
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we link together the multi-head attention and feed-forward component, by
    introducing two more crucial elements:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '**Residual Connections (Add):** These connections perform an element-wise addition
    of a layer’s output to its unaltered input. During training, the model adjusts
    the emphasis on layer transformations based on their usefulness. If a transformation
    is deemed nonessential, its weights and consequently its layer output tend towards
    zero. In this case at least the unaltered input is passed through the residual
    connection. This technique helps mitigating the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem).'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Layer Normalization (Norm):** This method normalizes each embedded vector
    in the context by subtracting its mean and dividing by its standard deviation.
    This process also ensures that the gradients during backpropagation neither explode
    nor vanish.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/6aecc16dc453b3e4359fce4392d6d89c.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
- en: Image by author
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: The chain of multi-head attention and feed-forward layers, linked with “Add
    & Norm” is consolidated into a block. This modular design allows us to form a
    sequence of blocks. The number of these blocks is a hyper-parameter, which determines
    the depth of the model architecture.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 5.7\. Softmax
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Upon traversing multiple block components, we obtain a matrix of dimensions
    (block-size x embed-size). To reshape this matrix to the required dimensions (block-size
    x vocab size), we pass it through a final linear layer. This shape represents
    an entry for each word in the vocabulary at each position in the context.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we apply the soft-max transformation to these values, converting them
    into probabilities. We have successfully obtained a probability distribution for
    the next token at every position in the context.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Model Training
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To train the language model, I selected token sequences from random positions
    within my training data. Given the fast-paced nature of WhatsApp conversations,
    I determined a context length of 32 words to be sufficient. Thus, I chose random
    32-word chunks as the context input and used the corresponding vectors, shifted
    by one word, as the targets for comparison.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'The training process was looping through the following steps:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Sample multiple batches of context.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feed these samples into the model to calculate the current loss.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply backpropagation based on the current loss and model weights.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the loss more comprehensively every 500th iteration.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once all other model hyperparameters (like embedding size, number of self-attention
    heads, etc.) were fixed, I finalized a model with 2.5 million parameters. Given
    my limitations on input data size and computational resources, I found this to
    be the optimal setup for me.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: The training process took approximately 12 hours for 10,000 iterations. One
    can see that training could have been stopped earlier, as the spread between the
    loss on the validation and training sets widens.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f097aae828c46fb03488c7b46270e671.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
- en: Image by author
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 7\. Chat-Mode
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To interact with the trained model, I created a function that allows selecting
    a contact name via a dropdown menu and inputting a message for the model to respond
    to. The parameter “n_chats” determines the number of responses the model generates
    at once. The model concludes a generated message when it predicts the <END> token
    as the next token.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Conclusion:**'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Due to the privacy of my personal chats, I am unable to present example prompts
    and conversations here.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Nonetheless, you can expect a model of this scale to successfully learn the
    general structure of sentences, producing meaningful outputs in terms of word
    order. In my case, it also picked up context for certain topics that were prominent
    in the training data. For instance, as my personal chats often revolve around
    tennis, the names of tennis players and tennis-related words were typically output
    together.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: However, when evaluating the coherence of the generated sentences, I concede
    that the results did not quite meet my already modest expectations. But of course,
    I could also blame my friends for chatting too much nonsense, limiting the models'
    ability to learn something useful...
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: To show at least some example output at the end, you can see how the dummy model
    performs on 200 trained dummy messages ;)
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4ead9603facfac34d9b5e0d5f3cdf9ff.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
- en: Image by author
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
