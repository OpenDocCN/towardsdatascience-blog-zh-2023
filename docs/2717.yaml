- en: Randomizing Very Large Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/randomizing-very-large-datasets-e2b14e507725?source=collection_archive---------7-----------------------#2023-08-26](https://towardsdatascience.com/randomizing-very-large-datasets-e2b14e507725?source=collection_archive---------7-----------------------#2023-08-26)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Consider the problem of randomizing a dataset that is so large, it doesn’t even
    fit into memory. This article describes how you can do it easily and (relatively)
    quickly in Python.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@doug.blank?source=post_page-----e2b14e507725--------------------------------)[![Douglas
    Blank, PhD](../Images/b2fa86b9fe63a8bcb4f218ef5a6791e9.png)](https://medium.com/@doug.blank?source=post_page-----e2b14e507725--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e2b14e507725--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e2b14e507725--------------------------------)
    [Douglas Blank, PhD](https://medium.com/@doug.blank?source=post_page-----e2b14e507725--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F66e2bac7e7d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandomizing-very-large-datasets-e2b14e507725&user=Douglas+Blank%2C+PhD&userId=66e2bac7e7d8&source=post_page-66e2bac7e7d8----e2b14e507725---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e2b14e507725--------------------------------)
    ·6 min read·Aug 26, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe2b14e507725&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandomizing-very-large-datasets-e2b14e507725&user=Douglas+Blank%2C+PhD&userId=66e2bac7e7d8&source=-----e2b14e507725---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe2b14e507725&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandomizing-very-large-datasets-e2b14e507725&source=-----e2b14e507725---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: These days it is not at all uncommon to find datasets that are measured in Gigabytes,
    or even Terabytes, in size. That much data can help tremendously in the training
    process to create robust Machine Learning models. But how can you randomize such
    large datasets?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/85fddbf8a9cde6f37e48e2320879b217.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Jess Bailey](https://unsplash.com/@jessbaileydesigns?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that you have a very large dataset with one item per line in a file.
    The details of the data are irrelevant for our goals here. The dataset could be
    lines in a CSV (comma-separate values) or TSV (tab-separated values) file, or
    each line could be a JSON object, or a list of X, Y, Z values of a point in a
    large point cloud. All we need is that the dataset is formatted with one item
    per line.
  prefs: []
  type: TYPE_NORMAL
- en: 'For files containing smaller datasets, one could randomize the file (called
    “shuffling”) in memory using a simple Python function like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The **shuffle_in_memory()** function takes an input filename and an output filename,
    shuffles the lines in memory using the builtin **random.shuffle(),** and writes
    the randomized data out. Like its name implies, this function requires that all
    of the lines of the file be loaded into memory at once.
  prefs: []
  type: TYPE_NORMAL
- en: To test this function out, let’s make some test files. The function **make_file()**
    takes the number lines that you would like in the test file. The function will
    create the file and return the filename.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, to make a file named “test-1000.txt” with 100 lines in it looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'After running this function, you should find in your current directory you
    should find a file named “test-1000.txt” with 1,000 lines of text, like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To test out our **shuffle_in_memory()** function, we’ll name an output file,
    save the string in the variable **filename_out**, and call the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, there should be a second file in your directory named “test-randomized-1000.txt”.
    It should be exactly the same size as “test-1000.txt” with exactly the same lines,
    but in a randomized order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Okay, now for the big question: what to do if we have a very large file? Let’s
    make a medium-sized one with, say, 10 million lines. (For most computers this
    is still small enough to randomize in memory, but it is of sufficiently large
    enough size to practice with.) As before, we make the input file with a call to
    **make_file()**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This will take a few seconds. Afterwards you should have a file named “test-10000000.txt”
    in your directory. It should begin as before, but will have 10 million lines.
    The file is about 128 MB.
  prefs: []
  type: TYPE_NORMAL
- en: How to randomize it? If we don’t want to use all of our RAM, or we don’t have
    enough RAM, we can use the hard disk instead. Here is a recursive algorithm based
    on a similar problem, sorting. The following function **shuffle()** is based on
    the merge sort algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: First, it checks to see if a file is small enough to be shuffled in memory (the
    base case in recursive function parlance). The parameter **memory_limit** is given
    in bytes. If a file size is smaller than **memory_limit**, then it will be shuffled
    in memory. If it is too big, then it is randomly split into a number of smaller
    files, and each of those is recursively shuffled. Finally, the contents of the
    smaller shuffled files are merged back together.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'If this was a sorting algorithm, we would merge the files back together in
    a careful manner to create a sorted ordering. However, for shuffling, we don’t
    care about merging them in a particular order as we want them randomized. The
    **merge_files()** function therefore looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we are careful to not read all of the lines of the files into memory
    all at once. Let’s test this out by giving the limit for memory shuffling to be
    exactly the size of the file. Since the file size is not smaller than 128,888,890
    it will be broken into a number of smaller files. For this example, let’s break
    the big file into 2, each of which will be small enough to shuffle in memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This call results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'And the contents of the resulting “test-randomized-10000000.txt” file should
    have 10 million lines, all randomized. A better test would be to reduce the memory
    needed to be much smaller than the file to randomize, and to break the too-big
    files into more than 2\. Let’s say we only want to use about 1 MB of RAM, and
    break the files into 20 smaller files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This example will use no more than 1 MB of RAM, and recursively decompose the
    subfiles that are bigger than that, 20 at a time.
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm will work on files of any size (well, you need enough disk space!).
    The more memory you can allocate for the **shuffle_in_memory()** the faster it
    will run. If the number of smaller files is too large, then you’ll spend too much
    time opening and closing files. You can try different numbers for **memory_limit**,
    but I’ve had good luck with between 20 and 200\. The larger the initial file,
    you’ll probably want more subfiles.
  prefs: []
  type: TYPE_NORMAL
- en: There are other algorithms that you can also use. I had high hopes for writing
    all of the lines into a SQLite database, SELECTing them in a random order, but
    it was no faster that the above code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Can you beat the recursive shuffle algorithm, staying purely in Python? If so,
    I’d love to hear about it!
  prefs: []
  type: TYPE_NORMAL
- en: '***Interested in Artificial Intelligence, Machine Learning, and Data Science?
    Consider a clap and a follow. Let me know what you are interested in!***'
  prefs: []
  type: TYPE_NORMAL
