- en: Three challenges in deploying generative models in production
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/three-challenges-in-deploying-generative-models-in-production-8e4c0fcf63c3?source=collection_archive---------5-----------------------#2023-08-07](https://towardsdatascience.com/three-challenges-in-deploying-generative-models-in-production-8e4c0fcf63c3?source=collection_archive---------5-----------------------#2023-08-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to deploy Large Language and Diffusion models for your product without scaring
    the users away.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://mikhailiuk.medium.com/?source=post_page-----8e4c0fcf63c3--------------------------------)[![Aliaksei
    Mikhailiuk](../Images/f4bf3f15f3e0b42f34e50b3ffc436b2a.png)](https://mikhailiuk.medium.com/?source=post_page-----8e4c0fcf63c3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8e4c0fcf63c3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8e4c0fcf63c3--------------------------------)
    [Aliaksei Mikhailiuk](https://mikhailiuk.medium.com/?source=post_page-----8e4c0fcf63c3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F30bef13bba71&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthree-challenges-in-deploying-generative-models-in-production-8e4c0fcf63c3&user=Aliaksei+Mikhailiuk&userId=30bef13bba71&source=post_page-30bef13bba71----8e4c0fcf63c3---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8e4c0fcf63c3--------------------------------)
    ·9 min read·Aug 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8e4c0fcf63c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthree-challenges-in-deploying-generative-models-in-production-8e4c0fcf63c3&user=Aliaksei+Mikhailiuk&userId=30bef13bba71&source=-----8e4c0fcf63c3---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8e4c0fcf63c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthree-challenges-in-deploying-generative-models-in-production-8e4c0fcf63c3&source=-----8e4c0fcf63c3---------------------bookmark_footer-----------)![](../Images/4d0d67dbb4548f550d2b5fbbc3b28114.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Image generated by the Author in [SDXL 1.0](https://stability.ai/blog/stable-diffusion-sdxl-1-announcement).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '[OpenAI](https://openai.com/), [Google](https://ai.google/discover/generativeai/),
    [Microsoft](https://blogs.microsoft.com/blog/2023/05/23/microsoft-build-brings-ai-tools-to-the-forefront-for-developers/),
    [Midjourney](https://www.midjourney.com/home/?callbackUrl=%2Fapp%2F), [StabilityAI](https://stability.ai/),
    [CharacterAI](https://beta.character.ai/) and many more — everyone is racing to
    bring the best solution for text-to-text, text-to-image, image-to-image and image-to-text
    models.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: The reason is simple — the vast field of opportunities the space offers; after
    all, it’s not only entertainment but also utility that was impossible to unlock.
    From [better search engines](https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/)
    to more impressive and [personalized ad campaigns](https://www.optimatik.ai/ai-product-photos)
    and friendly chatbots, like [Snap’s MyAI](https://newsroom.snap.com/en-GB/say-hi-to-my-ai).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: And while the space is very fluid, with lots of moving parts and model checkpoints
    released every few days, there are challenges that every company working with
    Generative AI is looking to address.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Here, I will talk about the major challenges and how to address them in deploying
    generative models in production. While there are many different kinds of generative
    models, in this article, I will focus on the recent advancements in diffusion
    and GPT-based models. However, many topics discussed here would apply to other
    models as well.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: What is generative AI?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative AI broadly describes a set of models that can generate new content.
    Widely known Generative Adversarial Networks do so by learning the distribution
    of real data and generating variability from the added noise.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: The recent boom in Generative AI comes from the models attaining human-level
    quality at scale. The reason for unlocking this transformation is simple — we
    only now have enough compute power (hence the [NVIDIA skyrocketing stock price](https://www.investors.com/news/nvidia-stock-2023-buy-now/))
    for training and maintaining models with enough capacity to achieve high-quality
    results. Current advancement is fuelled by two base architectures — transformers
    and diffusion models.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps the most significant breakthrough of the recent year was OpenAI’s ChatGPT
    — a text-based generative model, with 175 billion for one of the latest ChatGPT-3.5
    versions that has a knowledge base sufficient to maintain conversations on various
    topics. While ChatGPT is a single modality model, as it can only support text,
    multimodal models can take as input and output several kinds of input, e.g. text
    and images.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Image-to-text and text-to-image multimodal architectures operate in a latent
    space shared by textual and image concepts. The latent space is obtained by training
    on a task requiring both concepts (for example, image captioning) by penalizing
    the distance in the latent space between the same concept in two different modalities.
    Once this latent space is obtained, it can be re-used for other tasks.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图像到文本和文本到图像的多模态架构在一个由文本和图像概念共享的潜在空间中操作。通过在需要两个概念的任务（例如，图像标注）上训练来获得潜在空间，通过惩罚同一概念在两个不同模态中的潜在空间距离。一旦获得了这个潜在空间，它可以用于其他任务。
- en: '![](../Images/20b680f4fdfaaeea4a1b4ecae945ed5b.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20b680f4fdfaaeea4a1b4ecae945ed5b.png)'
- en: Example of an Image-to-Text model. Image by the Author.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图像到文本模型的示例。图片由作者提供。
- en: Notable generative models released this year are [DALLE](https://openai.com/dall-e-2)/[Stable-Diffusion](https://stability.ai/blog)
    (text-to-image / image-to-image) and [BLIP](https://arxiv.org/abs/2201.12086)
    (image-to-text [implementation](https://github.com/salesforce/BLIP)). DALLE models
    take as input either a prompt or an image and a prompt generates an image as a
    response, while BLIP-based models can answer questions about the contents of the
    picture.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 今年发布的著名生成模型有[DALL·E](https://openai.com/dall-e-2)/[Stable-Diffusion](https://stability.ai/blog)（文本到图像/图像到图像）和[BLIP](https://arxiv.org/abs/2201.12086)（图像到文本[实现](https://github.com/salesforce/BLIP)）。DALL·E模型以提示或图像作为输入，提示生成图像作为响应，而基于BLIP的模型可以回答关于图像内容的问题。
- en: Challenges and Solutions
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 挑战与解决方案
- en: Unfortunately, there is no free lunch when it comes to machine learning, and
    large-scale generative models stumble upon a few challenges when it comes to their
    deployment in production — size and latency, bias and fairness, and the quality
    of the generated results.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，机器学习没有免费的午餐，大规模生成模型在生产部署时遇到了一些挑战——大小和延迟、偏差和公平性以及生成结果的质量。
- en: Model size and latency
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型大小和延迟
- en: '![](../Images/809bb30040a03db448990c63422f1c66.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/809bb30040a03db448990c63422f1c66.png)'
- en: Model size trends. Data from [P. Villalobos](https://arxiv.org/pdf/2207.02852.pdf).
    Image by the Author
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 模型大小趋势。数据来自[P. Villalobos](https://arxiv.org/pdf/2207.02852.pdf)。图片由作者提供
- en: State-of-the-art GenAI models are huge. For example, text-to-text [Meta’s LLaMA](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/)
    models range between 7 and 65 billion parameters, and ChatGPT-3.5 is 175B parameters.
    These numbers are justified — in a simplified world, the rule of thumb is the
    larger the model the more data is used for training, the better the quality.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 先进的GenAI模型非常庞大。例如，文本到文本的[Meta的LLaMA](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/)模型范围在7亿到65亿参数之间，而ChatGPT-3.5有1750亿参数。这些数字是合理的——在简化的世界里，经验法则是模型越大，训练所用的数据越多，质量也越好。
- en: Text-to-image models, while smaller, are still significantly bigger than their
    Generative Adversarial Network predecessors — Stable Diffusion 1.5 checkpoints
    are just under 1B parameter (taking over three gigabytes of space), and DALLE
    2.0 has 3.5B parameters. Few GPUs would have enough memory to maintain these models
    and typically you would need a fleet to maintain a single large model, which can
    become very costly very soon, not even speaking of deploying these models on mobile
    devices.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管文本到图像模型较小，但仍显著大于其生成对抗网络的前身——Stable Diffusion 1.5检查点略低于10亿参数（占用超过三GB的空间），而DALL·E
    2.0有35亿参数。很少有GPU能够拥有足够的内存来维持这些模型，通常你需要一整套设备来维护一个大型模型，这可能很快变得非常昂贵，更不用说将这些模型部署到移动设备上。
- en: Generative models take time to produce the output. For some, the latency is
    due to their size — propagating the signal through several billions of parameters
    even on a fleet of GPUs takes time, while for others, it’s due to the iterative
    nature of producing high-quality results. Diffusion models, in their default configuration,
    take 50 steps to generate an image, making a smaller number of steps deteriorates
    the quality of the output image.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型需要时间来产生输出。对于某些模型，延迟是由于其大小——在多个GPU上传播信号通过数十亿个参数即使需要时间，而对于其他模型，则是由于生成高质量结果的迭代性质。扩散模型在默认配置下需要50步来生成图像，减少步数会降低输出图像的质量。
- en: '**Solutions:** Making the model smaller often helps make it faster — [distilling,
    compressing and quantizing](/on-the-edge-deploying-deep-applications-on-constrained-devices-f2dac997dd4d)
    the model would also reduce the latency. [Qualcomm has paved the way](https://www.qualcomm.com/news/onq/2023/02/worlds-first-on-device-demonstration-of-stable-diffusion-on-android)
    by compressing the stable diffusion model enough to be deployed on mobile. Recently
    smaller, distilled and much quicker versions of [Stable Diffusion (tiny and small)
    have been released](https://blog.segmind.com/introducing-sd-small-and-sd-tiny-stable-diffusion-models/).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案：** 将模型缩小通常有助于提高其速度 — [提炼、压缩和量化](/on-the-edge-deploying-deep-applications-on-constrained-devices-f2dac997dd4d)
    模型也会减少延迟。[高通为此铺平了道路](https://www.qualcomm.com/news/onq/2023/02/worlds-first-on-device-demonstration-of-stable-diffusion-on-android)，通过将稳定扩散模型压缩到可以在移动设备上部署的程度。最近，[稳定扩散（tiny
    和 small）的小型、提炼和更快速版本已被发布](https://blog.segmind.com/introducing-sd-small-and-sd-tiny-stable-diffusion-models/)。'
- en: Model-specific optimization can also aid in speeding up the inference — for
    diffusion models; one might generate low-resolution output and then upscale it
    or use a lower number of steps and a different scheduler, as some work best with
    the lower number of steps, while others generate superior quality for a higher
    number of iterations. For example, [Snap recently showed that eight steps would
    be](https://snap-research.github.io/SnapFusion/) enough to create high-quality
    results with Stable Diffusion 1.5, employing various optimizations at training
    time.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 特定模型的优化也有助于加快推理速度 — 对于扩散模型；可以生成低分辨率的输出，然后进行放大，或使用更少的步骤和不同的调度器，因为有些模型在较少的步骤下效果最好，而其他模型在更多迭代下会生成更高质量的结果。例如，[Snap
    最近展示了八个步骤就足以](https://snap-research.github.io/SnapFusion/) 使用稳定扩散 1.5 创建高质量的结果，并在训练时采用了各种优化。
- en: Compiling the model with, for example, NVIDIAs [tensorrt](https://developer.nvidia.com/tensorrt)
    and [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)
    could substantially reduce the latency with minimal engineering effort.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，使用 NVIDIAs [tensorrt](https://developer.nvidia.com/tensorrt) 和 [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)
    编译模型，可以在最小的工程努力下大幅减少延迟。
- en: '[](/on-the-edge-deploying-deep-applications-on-constrained-devices-f2dac997dd4d?source=post_page-----8e4c0fcf63c3--------------------------------)
    [## On the edge — deploying deep learning applications on mobile'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/on-the-edge-deploying-deep-applications-on-constrained-devices-f2dac997dd4d?source=post_page-----8e4c0fcf63c3--------------------------------)
    [## 边缘 — 在移动设备上部署深度学习应用'
- en: Techniques on striking the efficiency-accuracy trade-off for deep neural networks
    on constrained devices
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在受限设备上打破效率和准确性权衡的技术
- en: towardsdatascience.com](/on-the-edge-deploying-deep-applications-on-constrained-devices-f2dac997dd4d?source=post_page-----8e4c0fcf63c3--------------------------------)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/on-the-edge-deploying-deep-applications-on-constrained-devices-f2dac997dd4d?source=post_page-----8e4c0fcf63c3--------------------------------)
- en: Bias, fairness and safety
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 偏见、公平性和安全性
- en: Have you ever tried to break ChatGPT? Many have succeeded in uncovering bias
    and fairness issues, and kudos to [OpenAI is doing a great job](https://openai.com/blog/how-should-ai-systems-behave)
    addressing these. Without fixes at scale, chatbots can create real-world problems
    by propagating harmful and unsafe ideas and behaviours.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否尝试过破解 ChatGPT？许多人成功揭露了偏见和公平性问题，值得称赞的是 [OpenAI 做得很好](https://openai.com/blog/how-should-ai-systems-behave)
    解决这些问题。如果没有大规模的修复，聊天机器人可能会通过传播有害和不安全的思想和行为而造成现实世界的问题。
- en: Examples where people managed to break the model, are in [politics](https://www.brookings.edu/articles/the-politics-of-ai-chatgpt-and-political-bias/);
    for instance, ChatGPT [refused to create poems about Trump but would create one
    about Biden](https://www.forbes.com/sites/ariannajohnson/2023/02/03/is-chatgpt-partisan-poems-about-trump-and-biden-raise-questions-about-the-ai-bots-bias-heres-what-experts-think/),
    gender equality and jobs in particular — implying that some [professions are for
    men and some are for women](https://nationalcentreforai.jiscinvolve.org/wp/2023/01/26/exploring-the-potential-for-bias-in-chatgpt/)
    and [race](https://www.forbes.com/sites/janicegassam/2023/01/28/the-dark-side-of-chatgpt/).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 破坏模型的示例见[政治](https://www.brookings.edu/articles/the-politics-of-ai-chatgpt-and-political-bias/)；例如，ChatGPT[拒绝创作关于特朗普的诗歌，但会创作关于拜登的诗歌](https://www.forbes.com/sites/ariannajohnson/2023/02/03/is-chatgpt-partisan-poems-about-trump-and-biden-raise-questions-about-the-ai-bots-bias-heres-what-experts-think/)，特别是性别平等和职业——暗示某些[职业适合男性，某些适合女性](https://nationalcentreforai.jiscinvolve.org/wp/2023/01/26/exploring-the-potential-for-bias-in-chatgpt/)以及[种族](https://www.forbes.com/sites/janicegassam/2023/01/28/the-dark-side-of-chatgpt/)。
- en: Like text-to-text models, text-to-image and image-to-text models also contain
    biases and fairness issues. [The Stable Diffusion 2.1](https://huggingface.co/spaces/stabilityai/stable-diffusion)
    model when asked to generate images of a doctor and a nurse, produces a white
    male for the former and a white female for the latter. Interestingly, the bias
    would depend on the country specified in the prompt — e.g., a Japanese doctor
    or Brazilian nurse.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 与文本到文本模型类似，文本到图像和图像到文本模型也存在偏见和公平性问题。[Stable Diffusion 2.1](https://huggingface.co/spaces/stabilityai/stable-diffusion)模型在生成医生和护士的图像时，通常生成的是白人男性作为医生和白人女性作为护士。有趣的是，偏见会根据提示中指定的国家而有所不同——例如，日本医生或巴西护士。
- en: '![](../Images/baa3dac2aafafa0b794e59e99ca6c871.png)![](../Images/8b3a9600a74a17914d49faa08ad66012.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/baa3dac2aafafa0b794e59e99ca6c871.png)![](../Images/8b3a9600a74a17914d49faa08ad66012.png)'
- en: Asking a Stable Diffusion model to generate an image of doctor and and an image
    of a nurse. Image generated by the Author with [SD 2.1 interface](https://huggingface.co/spaces/stabilityai/stable-diffusion).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 询问Stable Diffusion模型生成一位医生和一位护士的图像。图片由作者使用[SD 2.1接口](https://huggingface.co/spaces/stabilityai/stable-diffusion)生成。
- en: Playing with [BLIP image-to-text model](https://replicate.com/salesforce/blip)
    and feeding in an image of an overweight person and male and female doctors I
    was getting judgemental and biased image descriptions — “a fat man”, “a male doctor”,
    “a woman in a lab coat with a stethoscope ”.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 玩弄[BLIP图像到文本模型](https://replicate.com/salesforce/blip)，将一张超重人士和男女医生的图片输入时，我得到了一些带有评判和偏见的图像描述——“一个胖男人”，“一个男性医生”，“一位穿着实验室大褂、带着听诊器的女性”。
- en: '![](../Images/cf2feb754f75ca4371738cb1d773b8f1.png)![](../Images/fdc551bf4f5e9c224a664c52bc7adf34.png)![](../Images/d21ef055d5108c9a708d0614c95e4af6.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf2feb754f75ca4371738cb1d773b8f1.png)![](../Images/fdc551bf4f5e9c224a664c52bc7adf34.png)![](../Images/d21ef055d5108c9a708d0614c95e4af6.png)'
- en: Images Generated by the author with Stable Diffusion 2.1 and captions generated
    with BLIP model from left to right [1] a fat man eating an ice cream cone; [2]
    a male doctor in a white coat and tie; [3] a woman in a white lab coat with a
    stethoscope on her neck. Images are generated by the Author with [SD 2.1 interface](https://huggingface.co/spaces/stabilityai/stable-diffusion)
    and passed through [BLIP](https://replicate.com/salesforce/blip).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用Stable Diffusion 2.1生成，描述由BLIP模型生成，依次为[1]一个胖男人吃冰淇淋；[2]一位穿着白大褂和领带的男性医生；[3]一位穿着白色实验室大褂、脖子上挂着听诊器的女性。图片由作者使用[SD
    2.1接口](https://huggingface.co/spaces/stabilityai/stable-diffusion)生成，并通过[BLIP](https://replicate.com/salesforce/blip)处理。
- en: '**How to test for the issue:** It is a fairly hard-to-diagnose issue — in many
    cases, you would need to know what to look for. Having a separate benchmark dataset
    with a wide variety of prompts, where things might or might not go wrong, template
    responses and red flags for each answer that we would detect, as well as a dataset
    of humans from various backgrounds and in multiple scenarios storing all possible
    attributes about the person in the picture would help. These datasets need to
    have hundreds of thousands of entries to obtain reliable statistics.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**如何测试这个问题：** 这是一个相当难以诊断的问题——在许多情况下，你需要知道要查找什么。拥有一个包含各种提示的独立基准数据集，其中可能会出现问题的情况、模板响应和我们将检测的每个答案的警告标志，以及来自不同背景和多个场景的人类数据集，存储关于图像中人物的所有可能属性，将会有所帮助。这些数据集需要包含数十万条记录，以获得可靠的统计数据。'
- en: '**Solution:** Almost all bias, fairness, and safety issues come from the training
    data. I like the analogy that AI models are a mirror of humanity that exacerbates
    all our biases. Training on clean, unbiased data would drastically improve the
    results. However, even with these, models make mistakes.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案：**几乎所有的偏见、公平性和安全性问题都源于训练数据。我喜欢这样的类比：AI模型是人性的镜像，加剧了我们所有的偏见。在干净、公正的数据上训练会大大改善结果。然而，即使有了这些，模型仍然会出错。'
- en: Result post-processing and filtering is another possible solution; for example,
    Stable Diffusion trained on data that contains nude images has an NSFW content
    detector to catch potential issues. Similar filters can be applied to the output
    of text-to-text models.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 结果后处理和过滤是另一个可能的解决方案；例如，Stable Diffusion 在包含裸露图像的数据上进行训练，配有 NSFW 内容检测器以捕捉潜在问题。类似的过滤器可以应用于文本到文本模型的输出。
- en: Output quality, relevance and correctness
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输出质量、相关性和正确性
- en: Generative models can be very creative in interpreting user requests, and while
    recent large-scale models attain human-level quality, for each use-case, they
    wouldn’t be working out of the box, requiring additional tweaks and prompt engineering.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型在解释用户请求时可以非常有创意，虽然最近的大规模模型达到了人类水平的质量，但对于每种使用情况，它们不会开箱即用，需要额外的调整和提示工程。
- en: Evaluating quality in early, very limited image-to-text and text-to-text models
    was relatively straightforward — after all, an improvement over gibberish is obvious.
    High-quality generative models start exhibiting behaviours that are harder to
    detect; for example, text-to-text models can become evasive, confidently spitting
    out incorrect and outdated information.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期阶段，图像到文本和文本到文本模型的质量评估相对简单——毕竟，相较于无意义的内容，改进是显而易见的。高质量生成模型开始展现出更难以察觉的行为；例如，文本到文本模型可能变得含糊其辞，自信地输出错误和过时的信息。
- en: Diffusion models exhibit output imperfections in other ways. Typical problems
    attributed to image-based models are broken geometry, mutated anatomy, a mismatch
    between the prompt and the image result, skin colour and gender mismatch in the
    case of image transfer. Automated assessment of aesthetics and realism is lagging,
    with typical metrics, like FID, incapable of capturing these variations.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型在其他方面表现出输出缺陷。典型的图像基础模型问题包括几何破损、变异的解剖结构、提示与图像结果不匹配、图像传输中的肤色和性别不匹配。自动化美学和现实性评估滞后，典型的度量指标，如FID，无法捕捉这些变化。
- en: '![](../Images/2b8d4859a35e13936fa651c4f8e2f811.png)![](../Images/32cd2b6b56dc856a362fff98f44f625a.png)![](../Images/45c5f9cfd8461d6ab0e52c1424f8b012.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b8d4859a35e13936fa651c4f8e2f811.png)![](../Images/32cd2b6b56dc856a362fff98f44f625a.png)![](../Images/45c5f9cfd8461d6ab0e52c1424f8b012.png)'
- en: 'From left to right: [1] two people hugging; [2] a man with thumbs up; [3] a
    dog is running in the park. An image from [Stable Diffusion 1.5](https://stablediffusionweb.com/#demo)
    generated by the Author'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从左到右：[1] 两个人拥抱；[2] 一个竖起大拇指的男人；[3] 一只狗在公园里跑步。一张由作者生成的来自[Stable Diffusion 1.5](https://stablediffusionweb.com/#demo)的图像
- en: '**How to test for the issue**: Testing the quality of generative models is
    challenging; after all, there is no ground truth for these models — they are made
    to provide novel outputs. And hence, so far, no metric would reliably capture
    quality aspects. And the most reliable metric is human evaluation.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**如何测试问题：** 测试生成模型的质量具有挑战性；毕竟，这些模型没有真实的标准——它们被设计用于提供新颖的输出。因此，到目前为止，没有一种度量标准能可靠地捕捉质量方面。最可靠的度量标准是人工评估。'
- en: Like in bias and fairness evaluation, the best way is to have a large dataset
    of prompts and images to test the quality. With text-to-text models becoming more
    personalized and tuning to each user, in addition to the coherence of the dialogues,
    correctness and relevance, we would also want to evaluate how well they can remember
    information about the conversation.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 与偏见和公平性评估一样，最好的方法是拥有一个大规模的提示和图像数据集来测试质量。随着文本到文本模型变得更加个性化并调整到每个用户，除了对话的连贯性、正确性和相关性，我们还希望评估它们记住关于对话的信息的能力。
- en: '**Solution:** Many of the quality problems can be attributed to the training
    data and size of the models; they might yet be just a little bit too small to
    have another quality leap ([think GPT-3.5 versus GPT-4](https://www.linkedin.com/pulse/chatgpt-35-vs-chatgpt-4-comparison-latest-gpt-based-chatbot-kunerth/))
    — current latent space is an abstraction and is not designed to store precise
    information.Many problems can be resolved with better prompt engineering — both
    prompt augmentation for text-to-text and text-to-image and negative prompts for
    text-to-image models.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Text-to-image and image-to-image models can have additional tools that might
    boost the quality — image enhancement, either through conventional deep learning
    methods or diffusion-based refiners. Addition modules like [ControlNet](https://huggingface.co/blog/controlnet)
    and orthogonal to the diffusion architecture can aid with additional control over
    the generated results. Dreambooth techniques for fine-tuning the model for a specific
    application would also help to get that edge in the results. Playing with additional
    parameters, like scheduler, CFG, and the number of diffusion steps, can drastically
    affect quality.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative models have opened a new scope of applications both fun, like [AI
    Lenses](https://www.snapchat.com/lens/87aca516bfa74b86b708849c617e51c1?locale=en-GB)
    and commercial, like better search engines, co-pilots and advertisements. At the
    same time the rush to get the product out by the companies and consumer excitement
    about the new functionalities sometimes making apparent flows in the technology
    go unnoticed.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: And while there is a general push to make the model benchmarking more transparent
    by publishing large scale open-source datasets, training code and evaluation results,
    there is also a push for stricter regulation of the large scale AI models. In
    an ideal world both wouldn’t go to an extreme and would aid each other in making
    AI safer and more fun to use.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Liked the author? Stay connected!
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Have I missed anything? Do not hesitate to leave a note, comment or message
    me directly on [LinkedIn](https://www.linkedin.com/in/aliakseimikhailiuk/) or
    [Twitter](https://twitter.com/mikhailiuka)!
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[](/deep-image-quality-assessment-30ad71641fac?source=post_page-----8e4c0fcf63c3--------------------------------)
    [## Deep Image Quality Assessment'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Deep dive into full-reference image quality assessment. From subjective image
    quality experiments to deep objective…
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/deep-image-quality-assessment-30ad71641fac?source=post_page-----8e4c0fcf63c3--------------------------------)
    [](/perceptual-losses-for-image-restoration-dd3c9de4113?source=post_page-----8e4c0fcf63c3--------------------------------)
    [## Perceptual Losses for Deep Image Restoration
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: From mean squared error to GANs — what makes a good perceptual loss function?
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/perceptual-losses-for-image-restoration-dd3c9de4113?source=post_page-----8e4c0fcf63c3--------------------------------)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/perceptual-losses-for-image-restoration-dd3c9de4113?source=post_page-----8e4c0fcf63c3--------------------------------)'
- en: '**The opinions in this blog are my own and not attributable to or on behalf
    of Snap.**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**本博客中的观点仅代表我个人，与Snap无关，也不代表Snap的立场。**'
