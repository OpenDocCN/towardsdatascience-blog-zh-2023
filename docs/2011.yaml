- en: Fine-tune MPT-7B on Amazon SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://towardsdatascience.com/fine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa?source=collection_archive---------5-----------------------#2023-06-20](https://towardsdatascience.com/fine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa?source=collection_archive---------5-----------------------#2023-06-20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/2f5691f2830e17d1af6c907a144f4567.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Jeffery Ho](https://unsplash.com/@jefferyho?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral).
  prefs: []
  type: TYPE_NORMAL
- en: Learn how to prepare a dataset and create a training job to fine-tune MPT-7B
    on Amazon SageMaker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@joao.pereira.abt?source=post_page-----1e68e71051fa--------------------------------)[![JoÃ£o
    Pereira](../Images/2946b185eb134ddfaa71cf5af5e3cda6.png)](https://medium.com/@joao.pereira.abt?source=post_page-----1e68e71051fa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1e68e71051fa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1e68e71051fa--------------------------------)
    [JoÃ£o Pereira](https://medium.com/@joao.pereira.abt?source=post_page-----1e68e71051fa--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Â·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6743ea128017&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa&user=Jo%C3%A3o+Pereira&userId=6743ea128017&source=post_page-6743ea128017----1e68e71051fa---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1e68e71051fa--------------------------------)
    Â·9 min readÂ·Jun 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1e68e71051fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa&user=Jo%C3%A3o+Pereira&userId=6743ea128017&source=-----1e68e71051fa---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1e68e71051fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa&source=-----1e68e71051fa---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: New large language models (LLMs) are being announced every week, each trying
    to beat its predecessor and take over the evaluation leaderboards. One of the
    latest models out there is [MPT-7B](https://www.mosaicml.com/blog/mpt-7b) that
    was released by MosaicML. Unlike other models of its kind, this 7-billion-parameter
    model is open-source and licensed for commercial use ([Apache 2.0 license](https://github.com/mosaicml/llm-foundry/blob/main/LICENSE))
    ðŸš€.
  prefs: []
  type: TYPE_NORMAL
- en: Foundation models like MPT-7B are pre-trained on datasets with trillions of
    tokens (100 tokens ~ 75 words) crawled from the web and, when prompted well, they
    can produce impressive outputs. However, to truly unlock the value of large language
    models in real-world applications, smart prompt-engineering might not be enough
    to make them work for your use case and, therefore, fine-tuning a foundation model
    on a domain-specific dataset is required.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs have billions of parameters and, consequently, fine-tuning such large models
    is challenging. Good news is that fine-tuning is much cheaper and faster as compared
    to pre-training the foundation model given that 1) the domain-specific datasets
    are "small" and 2) fine-tuning requires only a few passes over the training data.
  prefs: []
  type: TYPE_NORMAL
