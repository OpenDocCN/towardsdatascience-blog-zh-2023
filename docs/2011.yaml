- en: Fine-tune MPT-7B on Amazon SageMaker
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åœ¨ Amazon SageMaker ä¸Šå¾®è°ƒ MPT-7B
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/fine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa?source=collection_archive---------5-----------------------#2023-06-20](https://towardsdatascience.com/fine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa?source=collection_archive---------5-----------------------#2023-06-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/fine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa?source=collection_archive---------5-----------------------#2023-06-20](https://towardsdatascience.com/fine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa?source=collection_archive---------5-----------------------#2023-06-20)
- en: '![](../Images/2f5691f2830e17d1af6c907a144f4567.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f5691f2830e17d1af6c907a144f4567.png)'
- en: Photo by [Jeffery Ho](https://unsplash.com/@jefferyho?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± [Jeffery Ho](https://unsplash.com/@jefferyho?utm_source=medium&utm_medium=referral)
    æä¾›ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)ã€‚
- en: Learn how to prepare a dataset and create a training job to fine-tune MPT-7B
    on Amazon SageMaker
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: äº†è§£å¦‚ä½•å‡†å¤‡æ•°æ®é›†å¹¶åˆ›å»ºè®­ç»ƒä½œä¸šï¼Œä»¥åœ¨ Amazon SageMaker ä¸Šå¾®è°ƒ MPT-7B
- en: '[](https://medium.com/@joao.pereira.abt?source=post_page-----1e68e71051fa--------------------------------)[![JoÃ£o
    Pereira](../Images/2946b185eb134ddfaa71cf5af5e3cda6.png)](https://medium.com/@joao.pereira.abt?source=post_page-----1e68e71051fa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1e68e71051fa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1e68e71051fa--------------------------------)
    [JoÃ£o Pereira](https://medium.com/@joao.pereira.abt?source=post_page-----1e68e71051fa--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@joao.pereira.abt?source=post_page-----1e68e71051fa--------------------------------)[![JoÃ£o
    Pereira](../Images/2946b185eb134ddfaa71cf5af5e3cda6.png)](https://medium.com/@joao.pereira.abt?source=post_page-----1e68e71051fa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1e68e71051fa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1e68e71051fa--------------------------------)
    [JoÃ£o Pereira](https://medium.com/@joao.pereira.abt?source=post_page-----1e68e71051fa--------------------------------)'
- en: Â·
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6743ea128017&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa&user=Jo%C3%A3o+Pereira&userId=6743ea128017&source=post_page-6743ea128017----1e68e71051fa---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1e68e71051fa--------------------------------)
    Â·9 min readÂ·Jun 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1e68e71051fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa&user=Jo%C3%A3o+Pereira&userId=6743ea128017&source=-----1e68e71051fa---------------------clap_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6743ea128017&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa&user=Jo%C3%A3o+Pereira&userId=6743ea128017&source=post_page-6743ea128017----1e68e71051fa---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1e68e71051fa--------------------------------)
    Â·9åˆ†é’Ÿé˜…è¯»Â·2023å¹´6æœˆ20æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1e68e71051fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa&user=Jo%C3%A3o+Pereira&userId=6743ea128017&source=-----1e68e71051fa---------------------clap_footer-----------)'
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1e68e71051fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa&source=-----1e68e71051fa---------------------bookmark_footer-----------)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1e68e71051fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa&source=-----1e68e71051fa---------------------bookmark_footer-----------)'
- en: New large language models (LLMs) are being announced every week, each trying
    to beat its predecessor and take over the evaluation leaderboards. One of the
    latest models out there is [MPT-7B](https://www.mosaicml.com/blog/mpt-7b) that
    was released by MosaicML. Unlike other models of its kind, this 7-billion-parameter
    model is open-source and licensed for commercial use ([Apache 2.0 license](https://github.com/mosaicml/llm-foundry/blob/main/LICENSE))
    ğŸš€.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯å‘¨éƒ½æœ‰æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å‘å¸ƒï¼Œæ¯ä¸ªæ¨¡å‹éƒ½è¯•å›¾è¶…è¶Šå…¶å‰ä»»å¹¶ç™»ä¸Šè¯„ä¼°æ’è¡Œæ¦œã€‚æœ€æ–°å‘å¸ƒçš„æ¨¡å‹ä¹‹ä¸€æ˜¯ [MPT-7B](https://www.mosaicml.com/blog/mpt-7b)ï¼Œç”±
    MosaicML å‘å¸ƒã€‚ä¸å…¶ä»–åŒç±»æ¨¡å‹ä¸åŒçš„æ˜¯ï¼Œè¿™æ¬¾æ‹¥æœ‰70äº¿å‚æ•°çš„æ¨¡å‹æ˜¯å¼€æºçš„ï¼Œå¹¶ä¸”å…·æœ‰å•†ä¸šä½¿ç”¨è®¸å¯ï¼ˆ[Apache 2.0 è®¸å¯è¯](https://github.com/mosaicml/llm-foundry/blob/main/LICENSE)ï¼‰ğŸš€ã€‚
- en: Foundation models like MPT-7B are pre-trained on datasets with trillions of
    tokens (100 tokens ~ 75 words) crawled from the web and, when prompted well, they
    can produce impressive outputs. However, to truly unlock the value of large language
    models in real-world applications, smart prompt-engineering might not be enough
    to make them work for your use case and, therefore, fine-tuning a foundation model
    on a domain-specific dataset is required.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼ MPT-7B çš„åŸºç¡€æ¨¡å‹æ˜¯åœ¨ä»ç½‘ç»œçˆ¬å–çš„æ‹¥æœ‰ä¸‡äº¿ä¸ªæ ‡è®°ï¼ˆ100 tokens ~ 75 wordsï¼‰çš„æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œå½“æç¤ºå¾—å½“æ—¶ï¼Œå®ƒä»¬èƒ½å¤Ÿç”Ÿæˆä»¤äººå°è±¡æ·±åˆ»çš„è¾“å‡ºã€‚ç„¶è€Œï¼Œè¦çœŸæ­£å‘æ˜å¤§è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„ä»·å€¼ï¼Œä»…ä»…ä¾é æ™ºèƒ½æç¤ºå·¥ç¨‹å¯èƒ½ä¸è¶³ä»¥ä½¿å…¶é€‚åº”ä½ çš„ç”¨ä¾‹ï¼Œå› æ­¤ï¼Œéœ€è¦åœ¨ç‰¹å®šé¢†åŸŸçš„æ•°æ®é›†ä¸Šå¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚
- en: LLMs have billions of parameters and, consequently, fine-tuning such large models
    is challenging. Good news is that fine-tuning is much cheaper and faster as compared
    to pre-training the foundation model given that 1) the domain-specific datasets
    are "small" and 2) fine-tuning requires only a few passes over the training data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·æœ‰æ•°åäº¿ä¸ªå‚æ•°ï¼Œå› æ­¤ï¼Œå¾®è°ƒå¦‚æ­¤å¤§å‹çš„æ¨¡å‹æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚å¥½æ¶ˆæ¯æ˜¯ï¼Œç›¸è¾ƒäºå¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œå¾®è°ƒçš„æˆæœ¬å’Œæ—¶é—´éƒ½è¦ä¾¿å®œå¾—å¤šï¼Œå› ä¸º
    1) ç‰¹å®šé¢†åŸŸçš„æ•°æ®é›†æ˜¯â€œè¾ƒå°â€çš„ï¼Œ2) å¾®è°ƒåªéœ€å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œå°‘é‡éå†ã€‚
