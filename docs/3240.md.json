["```py\n!pip install --upgrade langchain\n```", "```py\nfrom langchain.document_loaders import TextLoader, DirectoryLoader\n\ntext_loader_kwargs={'autodetect_encoding': True}\nloader = DirectoryLoader('./hotels/london', show_progress=True, \n    loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n\ndocs = loader.load()\nlen(docs)\n82\n```", "```py\nfrom langchain.text_splitter import CharacterTextSplitter\n\ntext_splitter = CharacterTextSplitter(\n    separator = \"\\n\",\n    chunk_size = 1,\n    chunk_overlap  = 0,\n    length_function = lambda x: 1, # usually len is used \n    is_separator_regex = False\n)\nsplit_docs = text_splitter.split_documents(docs)\nlen(split_docs) \n12890\n```", "```py\nfor i in range(len(split_docs)):\n    split_docs[i].metadata['id'] = i\n```", "```py\nlist(filter(\n    lambda x: 'travelodge' in x.metadata['source'],\n    split_docs\n))\n```", "```py\nfrom langchain.chat_models import ChatOpenAI\n\nchat = ChatOpenAI(temperature=0.0, model=\"gpt-3.5-turbo\", \n  openai_api_key = \"your_key\")\n```", "```py\nfrom langchain.output_parsers import PydanticOutputParser\nfrom langchain.pydantic_v1 import BaseModel, Field\nfrom typing import List\n\nclass CustomerCommentData(BaseModel):\n    doc_id: int = Field(description=\"doc_id from the input parameters\")\n    topics: List[str] = Field(description=\"List of the relevant topics \\\n        for the customer review. Please, include only topics from \\\n        the provided list.\")\n    sentiment: str = Field(description=\"sentiment of the comment (positive, neutral or negative\")\n\noutput_parser = PydanticOutputParser(pydantic_object=CustomerCommentData)\n```", "```py\nformat_instructions = output_parser.get_format_instructions()\nprint(format_instructions)\n```", "```py\nfrom langchain.prompts import ChatPromptTemplate\n\ndocs_batch_data = []\nfor rec in docs_batch:\n    docs_batch_data.append(\n        {\n            'id': rec.metadata['id'],\n            'review': rec.page_content\n        }\n    )\n\ntopic_assignment_msg = '''\nBelow is a list of customer reviews in JSON format with the following keys:\n1\\. doc_id - identifier for the review\n2\\. review - text of customer review\nPlease, analyse provided reviews and identify the main topics and sentiment. Include only topics from the provided below list.\n\nList of topics with descriptions (delimited with \":\"):\n{topics_descr_list}\n\nOutput format:\n{format_instructions}\n\nCustomer reviews:\n```", "```py\n'''\n\ntopic_assignment_template = ChatPromptTemplate.from_messages([\n    (\"system\", \"You're a helpful assistant. Your task is to analyse hotel reviews.\"),\n    (\"human\", topic_assignment_msg)\n])\n\ntopics_list = '\\n'.join(\n    map(lambda x: '%s: %s' % (x['topic_name'], x['topic_description']), \n      topics))\n\nmessages = topic_assignment_template.format_messages(\n    topics_descr_list = topics_list,\n    format_instructions = format_instructions,\n    input_data = json.dumps(docs_batch_data)\n)\n```", "```py\nresponse = chat(messages)\ntype(response.content)\nstr\n\nprint(response.content)\n```", "```py\nresponse_dict = list(map(lambda x: output_parser.parse(x), \n  response.content.split('\\n')))\nresponse_dict\n```", "```py\nfrom langchain.chains import LLMChain\n\ntopic_assignment_chain = LLMChain(llm=chat, prompt=topic_assignment_template)\nresponse = topic_assignment_chain.run(\n    topics_descr_list = topics_list,\n    format_instructions = format_instructions,\n    input_data = json.dumps(docs_batch_data)\n) \n```", "```py\nchain = topic_assignment_template | chat\nresponse = chain.invoke(\n    {\n        'topics_descr_list': topics_list,\n        'format_instructions': format_instructions,\n        'input_data': json.dumps(docs_batch_data)\n    }\n)\n```", "```py\nfrom langchain.schema import StrOutputParser\nfrom operator import itemgetter\n\n# translation\n\ntranslate_msg = '''\nBelow is a list of customer reviews in JSON format with the following keys:\n1\\. doc_id - identifier for the review\n2\\. review - text of customer review\n\nPlease, translate review into English and return the same JSON back. Please, return in the output ONLY valid JSON without any other information.\n\nCustomer reviews:\n```", "```py\n'''\n\ntranslate_template = ChatPromptTemplate.from_messages([\n    (\"system\", \"You're an API, so you return only valid JSON without any comments.\"),\n    (\"human\", translate_msg)\n])\n\n# topic assignment & sentiment analysis\n\ntopic_assignment_msg = '''\nBelow is a list of customer reviews in JSON format with the following keys:\n1\\. doc_id - identifier for the review\n2\\. review - text of customer review\nPlease, analyse provided reviews and identify the main topics and sentiment. Include only topics from the provided below list.\n\nList of topics with descriptions (delimited with \":\"):\n{topics_descr_list}\n\nOutput format:\n{format_instructions}\n\nCustomer reviews:\n```", "```py\n'''\n\ntopic_assignment_template = ChatPromptTemplate.from_messages([\n    (\"system\", \"You're a helpful assistant. Your task is to analyse hotel reviews.\"),\n    (\"human\", topic_assignment_msg)\n])\n\n# defining chains\n\ntranslate_chain = translate_template | chat | StrOutputParser()\ntopic_assignment_chain = {'topics_descr_list': itemgetter('topics_descr_list'), \n                          'translated_data': translate_chain, \n                          'format_instructions': itemgetter('format_instructions')} \n                        | topic_assignment_template | chat \n\n# execution\n\nresponse = topic_assignment_chain.invoke(\n    {\n        'topics_descr_list': topics_list,\n        'format_instructions': format_instructions,\n        'input_data': json.dumps(docs_batch_data)\n    }\n)\n```", "```py\nimport langchain\nlangchain.debug = True\n```", "```py\nsentiment_msg = '''\nGiven the customer comment below please classify whether it's negative. If it's negative, return \"negative\", otherwise return \"positive\".\nDo not respond with more than one word.\n\nCustomer comment:\n```", "```py\n'''\n\nsentiment_template = ChatPromptTemplate.from_messages([\n    (\"system\", \"You're an assistant. Your task is to markup sentiment for hotel reviews.\"),\n    (\"human\", sentiment_msg)\n])\n\nsentiment_chain = sentiment_template | chat | StrOutputParser()\n```", "```py\nfrom langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n\n# defining structure for positive and negative cases \nclass PositiveCustomerCommentData(BaseModel):\n    topics: List[str] = Field(description=\"List of the relevant topics for the customer review. Please, include only topics from the provided list.\")\n\n    advantages: List[str] = Field(description = \"List the good points from that customer mentioned\")\n    sentiment: str = Field(description=\"sentiment of the comment (positive, neutral or negative\")\n\nclass NegativeCustomerCommentData(BaseModel):\n    topics: List[str] = Field(description=\"List of the relevant topics for the customer review. Please, include only topics from the provided list.\")\n\n    problems: List[str] = Field(description = \"List the problems that customer mentioned.\")\n    sentiment: str = Field(description=\"sentiment of the comment (positive, neutral or negative\")\n\n# defining output parsers and generating instructions\npositive_output_parser = PydanticOutputParser(pydantic_object=PositiveCustomerCommentData)\npositive_format_instructions = positive_output_parser.get_format_instructions()\n\nnegative_output_parser = PydanticOutputParser(pydantic_object=NegativeCustomerCommentData)\nnegative_format_instructions = negative_output_parser.get_format_instructions()\n\ngeneral_topic_assignment_msg = '''\nBelow is a customer review delimited by ```", "```py\n{input_data}\n```", "```py\n\nSo, now we need just to build the full chain. The main logic is defined using `RunnableBranch` and condition based on sentiment, an output of `sentiment_chain`.\n\n```", "```py\n\nHere are a couple of examples. It works pretty well and returns different objects depending on the sentiment.\n\n![](../Images/47cd0e4548c96667425a56e62d254b10.png)\n\nWe’ve looked in detail at the modular approach to do Topic Modelling using LangChain and introduce more complex logic. Now, it’s time to move on to the second part and discuss how we could assess the model’s performance.\n\n# Evaluation\n\nThe crucial part of any system running in production is [evaluation](https://python.langchain.com/docs/guides/evaluation). When we have an LLM model running in production, we want to ensure quality and keep an eye on it over time.\n\nIn many cases, you could use not only human-in-the-loop (when people are checking the model results for a small sample over time to control performance) but also leverage LLM for this task as well. It could be a good idea to use a more complex model for runtime checks. For example, we used ChatGPT 3.5 for our topic assignments, but we could use GPT 4 for evaluation (similar to the concept of supervision in real life when you are asking more senior colleagues for a code review).\n\nLangchain can help us with this task as well since it provides some tools to evaluate results:\n\n*   [String Evaluators](https://python.langchain.com/docs/guides/evaluation/string/) help to evaluate results from your model. There is quite a broad set of tools, from validating the format to assessing correctness based on provided context or reference. We will talk about these methods in detail below.\n*   The other class of evaluators are [Comparison evaluators](https://python.langchain.com/docs/guides/evaluation/comparison/). They will be handy if you want to assess the performance of 2 different LLM models (A/B testing use case). We won’t go into their details today.\n\n## Exact match\n\nThe most straightforward approach is to compare the model’s output to the correct answer (i.e. from experts or a training set) using an exact match. For that, we could use `ExactMatchStringEvaluator`, for example, to assess the performance of our sentiment analysis. In this case, we don’t need LLMs.\n\n```", "```py\n\nYou can build your own [custom String Evaluator](https://python.langchain.com/docs/guides/evaluation/string/custom) or match output to [a regular expression](https://python.langchain.com/docs/guides/evaluation/string/regex_match).\n\nAlso, there are helpful tools to validate structured output, whether the output is a valid JSON, has the expected structure and is close to the reference by distance. You can find more details about it in [the documentation](https://python.langchain.com/docs/guides/evaluation/string/json).\n\n## Embeddings distance evaluation\n\nThe other handy approach is to look at [the distance between embeddings](https://python.langchain.com/docs/guides/evaluation/string/embedding_distance). You will get a score in the result: the lower the score — the better, since answers are closer to each other. For example, we can compare found good points by Euclidean distance.\n\n```", "```py\n\nWe got a distance of 0.2\\. However, the results of such evaluation might be more difficult to interpret since you will need to look at your data distributions and define thresholds. Let’s move on to approaches based on LLMs since we will be able to interpret their results effortlessly.\n\n## Criteria evaluation\n\nYou can use [LangChain](https://python.langchain.com/docs/guides/evaluation/string/criteria_eval_chain) to validate LLM’s answer against some rubric or criteria. There’s a list of predefined criteria, or you can create a custom one.\n\n```", "```py\n\nSome of them don’t require reference (for example, `harmfulness` or `conciseness`). But for `correctness`, you need to know the answer.\nLet’s try to use it for our data.\n\n```", "```py\n\nAs a result, we got the answer (whether the results fit the specified criterion) and chain-of-thought reasoning so that we could understand the logic behind the result and potentially tweak the prompt.\n\n![](../Images/8714d05f4b2e5cfac200311caf698c5c.png)\n\nIf you’re interested in how it works, you could switch on `langchain.debug = True` and see the prompt sent to LLM.\n\n![](../Images/0da298b09e09569e208799682be0e438.png)\n\nLet’s look at the correctness criterion. To assess it, we need to provide a reference (the correct answer).\n\n```", "```py\n\n![](../Images/53ca5ff20f20000706d084f3caee8754.png)\n\nYou can even create your own custom criteria, for example, whether multiple points are mentioned in the answer.\n\n```", "```py\n\n![](../Images/267f33d58f6493ce44003b49671882f0.png)\n\n## Scoring evaluation\n\nWith criteria evaluation, we got only a Yes or No answer, but in many cases, it is not enough. For example, in our example, the prediction has 3 out of 4 mentioned points, which is a good result, but we got N when evaluating it for correctness. So, using this approach, answers “well-designed rooms, clean, great location” and “fast internet” will be equal in terms of our metrics, which won’t give us enough information to understand the model’s performance.\n\nThere’s another pretty close [technique of scoring](https://python.langchain.com/docs/guides/evaluation/string/regex_match) when you’re asking LLM to provide the score in the output, which might help to get more granular results. Let’s try it.\n\n```", "```py. Provide the list the good points that customer mentioned in the customer review.\n    Customer review:\n    ```", "```py\n    \"\"\",\n    reference=\"well designed rooms, clean, great location, good atmosphere\"\n)\n```", "```py\naccuracy_criteria = {\n    \"recall\": \"The asisstant's answer should include all mentioned in the question. If information is missing, score answer lower.\",\n    \"precision\": \"The assistant's answer should not have any points not present in the question.\"\n}\n\nevaluator = load_evaluator(\"score_string\", criteria=accuracy_criteria,\n   llm=ChatOpenAI(model=\"gpt-4\"))\n\neval_result = evaluator.evaluate_strings(\n    prediction=\"well designed rooms, clean, great location\",\n    input=\"\"\"Below is a customer review delimited by ```", "```py\n    Small but well designed rooms, clean, great location, good atmosphere. I would stay there again. Continental breakfast is weak but ok.\n    ```"]