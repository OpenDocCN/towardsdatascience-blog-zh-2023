- en: 'Graph Machine Learning: An Overview'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/graph-machine-learning-an-overview-c996e53fab90?source=collection_archive---------0-----------------------#2023-04-04](https://towardsdatascience.com/graph-machine-learning-an-overview-c996e53fab90?source=collection_archive---------0-----------------------#2023-04-04)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Demystifying Graph Neural Networks — Part 1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Key concepts for getting started
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@zach.blumenfeld?source=post_page-----c996e53fab90--------------------------------)[![Zach
    Blumenfeld](../Images/1fdc5fa3ef7af354ed9bbd07d198bf2f.png)](https://medium.com/@zach.blumenfeld?source=post_page-----c996e53fab90--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c996e53fab90--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c996e53fab90--------------------------------)
    [Zach Blumenfeld](https://medium.com/@zach.blumenfeld?source=post_page-----c996e53fab90--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe0dd307022f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-machine-learning-an-overview-c996e53fab90&user=Zach+Blumenfeld&userId=e0dd307022f5&source=post_page-e0dd307022f5----c996e53fab90---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c996e53fab90--------------------------------)
    ·9 min read·Apr 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc996e53fab90&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-machine-learning-an-overview-c996e53fab90&user=Zach+Blumenfeld&userId=e0dd307022f5&source=-----c996e53fab90---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc996e53fab90&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-machine-learning-an-overview-c996e53fab90&source=-----c996e53fab90---------------------bookmark_footer-----------)![](../Images/4eff6e54db2a87f3f900cd83700610a2.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Graph Neural Networks (GNNs) are gaining attention in data science and machine
    learning but still remain poorly understood outside expert circles. To grasp this
    exciting approach, we must start with the broader field of Graph Machine Learning
    (GML). Many online resources talk about GNNs and GML as if they are interchangeable
    concepts or as if GNNs are a panacea approach that makes other GML approaches
    obsolete. This is simply not the case. One of GML’s primary purposes is to compress
    large sparse graph data structures to enable feasible prediction and inference.
    GNNs are one way to accomplish this, perhaps the most advanced way, but not the
    only way. Understanding this will help create a better foundation for future parts
    of this series, where we will cover specific types of GNNs and related GML approaches
    in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this post, we will:'
  prefs: []
  type: TYPE_NORMAL
- en: Go over a brief recap on graph data structures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cover GML tasks and the types of problems they solve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examine the concept of compression and its importance in driving different GML
    approaches, including GNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are Graphs?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you’re reading this article, you likely already have some background on
    graph data structures. If not, I recommend reading [this resource on property
    graphs](http://graphdatamodeling.com/Graph%20Data%20Modeling/GraphDataModeling/page/PropertyGraphs.html)
    or [this resource on graph database concepts](https://neo4j.com/docs/getting-started/current/appendix/graphdb-concepts/).
    I will give a *very* brief recap here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A graph consists of nodes connected by relationships. There are [a couple of
    different ways to model graph data](https://neo4j.com/blog/rdf-triple-store-vs-labeled-property-graph-difference/).
    For simplicity, I will use the property graph model, which has three primary components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Nodes** that represent entities (sometimes called vertices),'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Relationships** that represent associations or interactions between nodes
    (sometimes called edges or links), and'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Properties** that represent attributes of nodes or relationships.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/8f9d10d56a67cf1f247f714c5a133c65.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: What is Graph Machine Learning (GML)?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At its core, Graph machine learning (GML) is the application of machine learning
    to graphs specifically for predictive and prescriptive tasks. GML has a variety
    of use cases across supply chain, fraud detection, recommendations, customer 360,
    drug discovery, and more.
  prefs: []
  type: TYPE_NORMAL
- en: One of the best ways to understand GML is through the different types of ML
    tasks it can accomplish. I break this out for [supervised](https://en.wikipedia.org/wiki/Supervised_learning)
    and [unsupervised](https://en.wikipedia.org/wiki/Unsupervised_learning) learning
    below.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised GML Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The below diagram outlines three of the most common GML tasks for supervised
    learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/25d468929ed1916091457f490f9cdebb.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'To expand further:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Node property prediction:** Predicting a discrete or continuous node property.
    One can think of node property prediction as ***predicting an adjective about
    a thing***, such as whether an account on a financial services platform should
    be classified as fraud or how to categorize a product on an online retail store.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Link prediction:** Predicting whether or not a relationship should exist
    between two nodes and potentially some properties about the relationship. Link
    prediction is helpful for applications like entity resolution, where we want to
    predict whether two nodes reflect the same underlying entity; recommendation systems
    where we want to predict what a user will want to purchase or interact with next;
    and bioinformatics, for predicting things like protein and drug interactions.
    For each case, we care about ***predicting an association, similarity, or potential
    action or interaction between entities***.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Graph property prediction:** Predicting a discrete or continuous property
    of a graph or subgraph. Graph property prediction is useful in domains where you
    want to ***model each entity as an individual graph for prediction*** rather than
    modeling entities as nodes within a larger graph representing a complete dataset.
    Use cases include material sciences, bioinformatics, and drug discovery, where
    individual graphs can represent molecules or proteins that you want to make predictions
    about.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unsupervised GML Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Below are four of the most common GML tasks for unsupervised learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e4dc8f6d21ab870368180ef0a6e2299f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'To elaborate on these further:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Representation Learning**: Reducing dimensionality while maintaining important
    signals is a central theme for GML applications. Graph representation learning
    does this explicitly by generating low-dimensional features from graphstructures,
    usually to use them for downstream exploratory data analysis (EDA) and ML.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Community Detection (clustering for relationships):** [Community detection](https://neo4j.com/docs/graph-data-science/current/algorithms/community/)
    is a clustering technique for identifying groups of densely interconnected nodes
    within a graph. Community detection has various practical applications in anomaly
    detection, fraud and investigative analytics, social network analysis, and biology.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Similarity:** [Similarity](https://neo4j.com/docs/graph-data-science/current/algorithms/similarity/)
    in GML refers to finding and measuring similar pairs of nodes in a graph. Similarity
    is applicable to many use cases, including recommendation, entity resolution,
    and anomaly and fraud detection. Common Similarity techniques include [node similarity
    algorithms](https://neo4j.com/docs/graph-data-science/current/algorithms/node-similarity/),
    [topological link prediction](https://neo4j.com/docs/graph-data-science/current/algorithms/linkprediction/),
    and [K-Nearest-Neibor (KNN)](https://neo4j.com/docs/graph-data-science/current/algorithms/knn/).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Centrality & Pathfinding:** I’m grouping these together as they tend to be
    less associated with ML tasks and more with analytical measures. However, they
    still technically fit here, so I will cover them for completeness. [Centrality](https://neo4j.com/docs/graph-data-science/current/algorithms/centrality/)
    finds important or influential nodes in a graph. Centrality is ubiquitous throughout
    many use cases, including fraud and anomaly detection, recommendation, supply
    chain, logistics, and infrastructure problems. [Pathfinding](https://neo4j.com/docs/graph-data-science/current/algorithms/pathfinding/)
    is used to find the lowest cost paths in a graph or to evaluate the quality and
    availability of paths. Pathfinding can benefit many use cases dealing with physical
    systems such as logistics, supply chain, transportation, and infrastructure.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How Compression is Key to GML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I came across [this interesting blog post](https://www.singlelunch.com/2020/12/28/why-im-lukewarm-on-graph-neural-networks/)
    by Matt Ranger which explains this point beautifully: One of the most significant
    objectives with GML, and to a large extent natural language processing too, is
    compressing large sparse data structures while maintaining important signals for
    prediction and inference.'
  prefs: []
  type: TYPE_NORMAL
- en: Consider a [regular graph](https://en.wikipedia.org/wiki/Regular_graph) represented
    by an adjacency matrix, a square matrix where each row and column represents a
    node. If a relationship goes from node A to node B, the cell at the intersection
    of row A and column B is 1; otherwise, it is 0\. Below is an illustration of some
    small regular graphs and their adjacency matrices.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/89e6f836e96c1f9dafcc9c3dc60f4ed9.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Notice that many of the cells in the above adjacency matrices are 0\. If you
    scale this to large graphs, particularly those found in real-world applications,
    the proportion of zeros increases, and the adjacency matrix becomes mostly zeros.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1133b9639ec505533a506578c5b1da85.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustrative example created using Last.fm recommendation graph visual from
    [Large Graph Visualization Tools and Approaches](/large-graph-visualization-tools-and-approaches-2b8758a1cd59)
    and matrix image from Beck, Fabian, et al. [Identifying modularization patterns
    by visual comparison of multiple hierarchies](https://www.researchgate.net/figure/Adjacency-matrix-visualization-showing-the-evolutionary-coupling-graph-ECConf-and-the_fig2_305026899)
  prefs: []
  type: TYPE_NORMAL
- en: This happens because as these graphs grow, the average [degree centrality](https://neo4j.com/docs/graph-data-science/current/algorithms/degree-centrality/)
    grows much slower or not at all. In social networks, this is evidenced by concepts
    like [the Dunbar Number](https://en.wikipedia.org/wiki/Dunbar%27s_number), a cognitive
    limit to the number of people with whom one can maintain stable social relationships.
    You can intuit this for other types of graphs too, such as graphs of financial
    transactions or graphs of user purchases for recommendation systems. As these
    graphs grow, the number of potential unique transactions or purchases a single
    individual can participate in grows much faster than their capacity to do so.
    I.e. If there are six products on a website, one user buying half of them is feasible,
    but if there are hundreds of thousands, then not so much. As a result, you end
    up with very large and sparse data structures.
  prefs: []
  type: TYPE_NORMAL
- en: If you could use these sparse data structures directly for machine learning,
    you wouldn’t need GNNs or any GML — you would just plug them as features into
    conventional ML models. However, this isn’t possible. It wouldn’t scale, and even
    beyond that, it would cause mathematical issues around convergence and estimation
    that would render ML models ill-specified and infeasible. As a result, a fundamental
    key to GML is compressing these data structures; arguably, it is the entire point
    of GML.
  prefs: []
  type: TYPE_NORMAL
- en: How to Accomplish Compression? — Graph Machine Learning Approaches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the highest level, three GML approaches exist for accomplishing this compression.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e86a796f24f8061b5ed8d39b7b163651.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '**Classic Graph Algorithms**'
  prefs: []
  type: TYPE_NORMAL
- en: Classic graph algorithms include things like [PageRank](https://neo4j.com/docs/graph-data-science/current/algorithms/page-rank/),
    [Louvain](https://neo4j.com/docs/graph-data-science/current/algorithms/louvain/),
    and [Dijkstra’s Shortest Path](https://neo4j.com/docs/graph-data-science/current/algorithms/dijkstra-source-target/).
    They can be used independently for unsupervised community detection, similarity,
    centrality, or pathfinding. The results of classic algorithms can also be used
    as features for conventional downstream models, such as linear and logistic regressions,
    random forests, or neural networks to perform GML tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Classic graph algorithms tend to be simple, easy to get started with, and relatively
    interpretable and explainable. However, they can require more manual work and
    subject matter expertise (SME) than other GML approaches. This makes classic graph
    algorithms good first choices in experimentation and development to help understand
    what works best on your graph. They can also do well in production for simpler
    problems, but more complex use cases may require graduating to another GML approach.
  prefs: []
  type: TYPE_NORMAL
- en: '**Non-GNN Graph Embeddings**'
  prefs: []
  type: TYPE_NORMAL
- en: Graph embeddings are a form of representation learning. Some graph embedding
    techniques leverage GNN architectures while others do not. The latter group, non-GNN,
    is the focus of this approach. These embedding techniques instead rely on matrix
    factorization/decomposition, random projections, random walks, or hashing function
    architectures. Some examples include [Node2vec (random walk based)](https://neo4j.com/docs/graph-data-science/current/machine-learning/node-embeddings/node2vec/),
    [FastRP (random projection and matrix operations)](https://neo4j.com/docs/graph-data-science/current/machine-learning/node-embeddings/fastrp/),
    and [HashGNN (hashing function architecture)](https://neo4j.com/docs/graph-data-science/current/machine-learning/node-embeddings/hashgnn/).
  prefs: []
  type: TYPE_NORMAL
- en: Graph embedding involves generating numeric or binary feature vectors to represent
    nodes, relationships, paths, or entire graphs. The foremost of those, node embedding,
    is among the most fundamental and commonly used. The basic idea is to generate
    a vector for each node such that the similarity between vectors (e.g. dot product)
    approximates the similarity between nodes in the graph. Below is an illustrative
    example of the small [Zachary’s karate club network](https://en.wikipedia.org/wiki/Zachary%27s_karate_club).
    Note how the adjacency matrix is compressed to 2-d embedding vectors for each
    node and how those vectors cluster together to reflect the graph community structure.
    Most real-world embeddings will have more than two dimensions (128 to 256 or higher)
    to represent larger real-world graphs with millions or billions of nodes, but
    the basic intuition is the same.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f14b6a0d24f1016edfe7c9bab275bec2.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'The same logic as above applies to relationship, path, and entire graph embeddings:
    similarity in the embedding vectors should approximate similarity in the graph
    structure. This accomplishes the compression while maintaining important signals,
    making the embeddings useful for various downstream ML tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Non-GNN embeddings can benefit from reduced manual workload and required SME
    compared to classic graph algorithms. While non-GNN embeddings often require hyperparameter
    tuning to get right, they tend to be easier to automate and generalize across
    different graphs. Additionally, some non-GNN embeddings like [FastRP](https://neo4j.com/docs/graph-data-science/current/machine-learning/node-embeddings/fastrp/)
    and [HashGNN](https://neo4j.com/docs/graph-data-science/current/machine-learning/node-embeddings/hashgnn/)
    can scale incredibly well to large graphs on commodity hardware since they don’t
    require model training. This can be a massive benefit over GNN-based approaches.
  prefs: []
  type: TYPE_NORMAL
- en: However, non-GNN embeddings come with some trade-offs too. They are less interpretable
    and explainable than classic graph algorithms due to the more generalized mathematical
    operations involved. They are also generally [transductive](https://en.wikipedia.org/wiki/Transduction_(machine_learning)),
    though recent improvements in Neo4j Graph Data Science allow some of them to effectively
    behave inductively in certain applications. We will cover transductive and inductive
    settings in more depth later in this series; it has to do with the ability to
    predict on new unseen data and is an essential point of consideration for GML.
  prefs: []
  type: TYPE_NORMAL
- en: '**Graph Neural Networks (GNNs)**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3e34911e86e0fd5bb89f9d753da9b6b8.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: A GNN is a neural network model that takes graph data as input, transforms it
    into intermediate embeddings, and feeds the embeddings to a final layer aligned
    to a prediction task. This prediction task can be supervised (node property prediction,
    link prediction, graph property prediction) or unsupervised (clustering, similarity,
    or simply a final output embedding for representation learning). So unlike classic
    algorithms and non-GNN embeddings, which pass results as features to downstream
    ML models, particularly for supervised tasks, GNNs are fully end-to-end graph
    native solutions.
  prefs: []
  type: TYPE_NORMAL
- en: GNNs have a variety of benefits related to being complete end-to-end solutions.
    Notably, intermediate embeddings are learned during training and, in theory, automatically
    infer the most important information from the graph. Most recent GNNs are also
    inductive due to having a trained model.
  prefs: []
  type: TYPE_NORMAL
- en: GNNs also come with some weaknesses. This includes high complexity, scaling
    difficulties, and low interpretability and explainability. GNNs can also have
    limitations around depth due to over-smoothing and other mathematical principles.
  prefs: []
  type: TYPE_NORMAL
- en: 'I’ll discuss GNNs more in my next blog, *GNNs: What They Are and Why They Matter*.
    In the meantime, **if you want to get started with graph machine learning, please
    take a look at** [**Neo4j Graph Data Science**](https://neo4j.com/product/graph-data-science/).
    Data scientists and engineers can find technical documentation for getting started
    [here](https://neo4j.com/docs/graph-data-science/current/).'
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping Things Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Biggest takeaways from this post:'
  prefs: []
  type: TYPE_NORMAL
- en: Graph Machine Learning (GML) is a broad field with many use case applications
    and comprising multiple different supervised and unsupervised ML tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the primary purposes of GML is compressing large sparse graph structures
    while maintaining important signals for prediction and inference.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GNNs are one of multiple GML approaches that accomplish this compression.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
