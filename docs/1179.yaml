- en: 'Graph Machine Learning: An Overview'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图机器学习：概述
- en: 原文：[https://towardsdatascience.com/graph-machine-learning-an-overview-c996e53fab90?source=collection_archive---------0-----------------------#2023-04-04](https://towardsdatascience.com/graph-machine-learning-an-overview-c996e53fab90?source=collection_archive---------0-----------------------#2023-04-04)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/graph-machine-learning-an-overview-c996e53fab90?source=collection_archive---------0-----------------------#2023-04-04](https://towardsdatascience.com/graph-machine-learning-an-overview-c996e53fab90?source=collection_archive---------0-----------------------#2023-04-04)
- en: Demystifying Graph Neural Networks — Part 1
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解密图神经网络——第一部分
- en: Key concepts for getting started
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 入门的关键概念
- en: '[](https://medium.com/@zach.blumenfeld?source=post_page-----c996e53fab90--------------------------------)[![Zach
    Blumenfeld](../Images/1fdc5fa3ef7af354ed9bbd07d198bf2f.png)](https://medium.com/@zach.blumenfeld?source=post_page-----c996e53fab90--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c996e53fab90--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c996e53fab90--------------------------------)
    [Zach Blumenfeld](https://medium.com/@zach.blumenfeld?source=post_page-----c996e53fab90--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@zach.blumenfeld?source=post_page-----c996e53fab90--------------------------------)[![Zach
    Blumenfeld](../Images/1fdc5fa3ef7af354ed9bbd07d198bf2f.png)](https://medium.com/@zach.blumenfeld?source=post_page-----c996e53fab90--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c996e53fab90--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c996e53fab90--------------------------------)
    [Zach Blumenfeld](https://medium.com/@zach.blumenfeld?source=post_page-----c996e53fab90--------------------------------)'
- en: ·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe0dd307022f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-machine-learning-an-overview-c996e53fab90&user=Zach+Blumenfeld&userId=e0dd307022f5&source=post_page-e0dd307022f5----c996e53fab90---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c996e53fab90--------------------------------)
    ·9 min read·Apr 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc996e53fab90&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-machine-learning-an-overview-c996e53fab90&user=Zach+Blumenfeld&userId=e0dd307022f5&source=-----c996e53fab90---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe0dd307022f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-machine-learning-an-overview-c996e53fab90&user=Zach+Blumenfeld&userId=e0dd307022f5&source=post_page-e0dd307022f5----c996e53fab90---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c996e53fab90--------------------------------)
    ·9分钟阅读·2023年4月4日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc996e53fab90&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-machine-learning-an-overview-c996e53fab90&user=Zach+Blumenfeld&userId=e0dd307022f5&source=-----c996e53fab90---------------------clap_footer-----------)'
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc996e53fab90&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-machine-learning-an-overview-c996e53fab90&source=-----c996e53fab90---------------------bookmark_footer-----------)![](../Images/4eff6e54db2a87f3f900cd83700610a2.png)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc996e53fab90&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-machine-learning-an-overview-c996e53fab90&source=-----c996e53fab90---------------------bookmark_footer-----------)![](../Images/4eff6e54db2a87f3f900cd83700610a2.png)'
- en: Image by Author
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 作者图片
- en: Graph Neural Networks (GNNs) are gaining attention in data science and machine
    learning but still remain poorly understood outside expert circles. To grasp this
    exciting approach, we must start with the broader field of Graph Machine Learning
    (GML). Many online resources talk about GNNs and GML as if they are interchangeable
    concepts or as if GNNs are a panacea approach that makes other GML approaches
    obsolete. This is simply not the case. One of GML’s primary purposes is to compress
    large sparse graph data structures to enable feasible prediction and inference.
    GNNs are one way to accomplish this, perhaps the most advanced way, but not the
    only way. Understanding this will help create a better foundation for future parts
    of this series, where we will cover specific types of GNNs and related GML approaches
    in more detail.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络（GNN）在数据科学和机器学习中受到关注，但在专家圈外仍然了解不多。要理解这一激动人心的方法，我们必须从更广泛的图机器学习（GML）领域入手。许多在线资源谈论
    GNN 和 GML 时，好像它们是可以互换的概念，或者好像 GNN 是一种万灵药，使其他 GML 方法变得过时。事实并非如此。GML 的主要目的之一是压缩大型稀疏图数据结构，以便进行可行的预测和推断。GNN
    是实现这一目标的一种方式，也许是最先进的方式，但不是唯一的方式。理解这一点将帮助为本系列的未来部分奠定更好的基础，我们将在其中更详细地讨论特定类型的 GNN
    和相关的 GML 方法。
- en: 'In this post, we will:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将：
- en: Go over a brief recap on graph data structures
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回顾一下图数据结构的简要概述
- en: Cover GML tasks and the types of problems they solve
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 GML 任务及其解决的问题类型
- en: Examine the concept of compression and its importance in driving different GML
    approaches, including GNNs
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探讨压缩的概念及其在推动不同 GML 方法（包括 GNN）中的重要性
- en: What are Graphs?
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是图？
- en: 'If you’re reading this article, you likely already have some background on
    graph data structures. If not, I recommend reading [this resource on property
    graphs](http://graphdatamodeling.com/Graph%20Data%20Modeling/GraphDataModeling/page/PropertyGraphs.html)
    or [this resource on graph database concepts](https://neo4j.com/docs/getting-started/current/appendix/graphdb-concepts/).
    I will give a *very* brief recap here:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在阅读这篇文章，你可能已经对图数据结构有一定的了解。如果没有，我建议阅读[这个关于属性图的资源](http://graphdatamodeling.com/Graph%20Data%20Modeling/GraphDataModeling/page/PropertyGraphs.html)或[这个关于图数据库概念的资源](https://neo4j.com/docs/getting-started/current/appendix/graphdb-concepts/)。我将在这里做一个*非常*简要的回顾：
- en: 'A graph consists of nodes connected by relationships. There are [a couple of
    different ways to model graph data](https://neo4j.com/blog/rdf-triple-store-vs-labeled-property-graph-difference/).
    For simplicity, I will use the property graph model, which has three primary components:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图由节点和通过关系连接的节点组成。有[几种不同的方式来建模图数据](https://neo4j.com/blog/rdf-triple-store-vs-labeled-property-graph-difference/)。为了简化，我将使用属性图模型，该模型有三个主要组成部分：
- en: '**Nodes** that represent entities (sometimes called vertices),'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**节点**，代表实体（有时称为顶点），'
- en: '**Relationships** that represent associations or interactions between nodes
    (sometimes called edges or links), and'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**关系**，代表节点之间的关联或互动（有时称为边或链接），以及'
- en: '**Properties** that represent attributes of nodes or relationships.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**属性**，代表节点或关系的特征。'
- en: '![](../Images/8f9d10d56a67cf1f247f714c5a133c65.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f9d10d56a67cf1f247f714c5a133c65.png)'
- en: Image by Author
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: What is Graph Machine Learning (GML)?
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是图机器学习（GML）？
- en: At its core, Graph machine learning (GML) is the application of machine learning
    to graphs specifically for predictive and prescriptive tasks. GML has a variety
    of use cases across supply chain, fraud detection, recommendations, customer 360,
    drug discovery, and more.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从根本上说，图机器学习（GML）是将机器学习应用于图，专门用于预测和指导任务。GML 在供应链、欺诈检测、推荐、客户360、药物发现等领域有各种应用。
- en: One of the best ways to understand GML is through the different types of ML
    tasks it can accomplish. I break this out for [supervised](https://en.wikipedia.org/wiki/Supervised_learning)
    and [unsupervised](https://en.wikipedia.org/wiki/Unsupervised_learning) learning
    below.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 理解 GML 的最佳方法之一是通过它可以完成的不同类型的机器学习任务。我将[监督式](https://en.wikipedia.org/wiki/Supervised_learning)和[无监督式](https://en.wikipedia.org/wiki/Unsupervised_learning)学习分开说明。
- en: Supervised GML Tasks
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督式 GML 任务
- en: 'The below diagram outlines three of the most common GML tasks for supervised
    learning:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 下图概述了监督学习中最常见的三种 GML 任务：
- en: '![](../Images/25d468929ed1916091457f490f9cdebb.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/25d468929ed1916091457f490f9cdebb.png)'
- en: Image by Author
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'To expand further:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步扩展：
- en: '**Node property prediction:** Predicting a discrete or continuous node property.
    One can think of node property prediction as ***predicting an adjective about
    a thing***, such as whether an account on a financial services platform should
    be classified as fraud or how to categorize a product on an online retail store.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**节点属性预测：** 预测离散或连续的节点属性。可以将节点属性预测视为***预测事物的形容词***，例如，在金融服务平台上，某账户是否应被分类为欺诈，或者如何在在线零售店中对产品进行分类。'
- en: '**Link prediction:** Predicting whether or not a relationship should exist
    between two nodes and potentially some properties about the relationship. Link
    prediction is helpful for applications like entity resolution, where we want to
    predict whether two nodes reflect the same underlying entity; recommendation systems
    where we want to predict what a user will want to purchase or interact with next;
    and bioinformatics, for predicting things like protein and drug interactions.
    For each case, we care about ***predicting an association, similarity, or potential
    action or interaction between entities***.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**链接预测：** 预测两个节点之间是否存在关系以及该关系的一些潜在属性。链接预测对实体解析等应用非常有帮助，我们希望预测两个节点是否反映了相同的基础实体；推荐系统中，我们希望预测用户接下来可能想购买或互动的内容；以及生物信息学中，用于预测蛋白质和药物的相互作用。在每种情况下，我们关注的是***预测实体之间的关联、相似性或潜在的动作或互动***。'
- en: '**Graph property prediction:** Predicting a discrete or continuous property
    of a graph or subgraph. Graph property prediction is useful in domains where you
    want to ***model each entity as an individual graph for prediction*** rather than
    modeling entities as nodes within a larger graph representing a complete dataset.
    Use cases include material sciences, bioinformatics, and drug discovery, where
    individual graphs can represent molecules or proteins that you want to make predictions
    about.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**图属性预测：** 预测图或子图的离散或连续属性。在需要***将每个实体建模为独立图进行预测***的领域，图属性预测非常有用，而不是将实体建模为大型图中的节点，后者表示完整的数据集。用例包括材料科学、生物信息学和药物发现，其中单个图可以代表分子或蛋白质，我们希望对其进行预测。'
- en: Unsupervised GML Tasks
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督GML任务
- en: 'Below are four of the most common GML tasks for unsupervised learning:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是无监督学习中最常见的四种GML任务：
- en: '![](../Images/e4dc8f6d21ab870368180ef0a6e2299f.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4dc8f6d21ab870368180ef0a6e2299f.png)'
- en: Image by Author
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: 'To elaborate on these further:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步阐述：
- en: '**Representation Learning**: Reducing dimensionality while maintaining important
    signals is a central theme for GML applications. Graph representation learning
    does this explicitly by generating low-dimensional features from graphstructures,
    usually to use them for downstream exploratory data analysis (EDA) and ML.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**表示学习：** 降维同时保持重要信号是GML应用中的核心主题。图表示学习通过从图结构生成低维特征来明确地做到这一点，通常用于后续的探索性数据分析（EDA）和机器学习。'
- en: '**Community Detection (clustering for relationships):** [Community detection](https://neo4j.com/docs/graph-data-science/current/algorithms/community/)
    is a clustering technique for identifying groups of densely interconnected nodes
    within a graph. Community detection has various practical applications in anomaly
    detection, fraud and investigative analytics, social network analysis, and biology.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**社区检测（关系的聚类）：** [社区检测](https://neo4j.com/docs/graph-data-science/current/algorithms/community/)是一种用于识别图中密切互连节点组的聚类技术。社区检测在异常检测、欺诈和调查分析、社交网络分析以及生物学中具有各种实际应用。'
- en: '**Similarity:** [Similarity](https://neo4j.com/docs/graph-data-science/current/algorithms/similarity/)
    in GML refers to finding and measuring similar pairs of nodes in a graph. Similarity
    is applicable to many use cases, including recommendation, entity resolution,
    and anomaly and fraud detection. Common Similarity techniques include [node similarity
    algorithms](https://neo4j.com/docs/graph-data-science/current/algorithms/node-similarity/),
    [topological link prediction](https://neo4j.com/docs/graph-data-science/current/algorithms/linkprediction/),
    and [K-Nearest-Neibor (KNN)](https://neo4j.com/docs/graph-data-science/current/algorithms/knn/).'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**相似性：** [相似性](https://neo4j.com/docs/graph-data-science/current/algorithms/similarity/)在GML中指的是在图中找到并测量相似节点对。相似性适用于许多用例，包括推荐、实体解析以及异常和欺诈检测。常见的相似性技术包括
    [节点相似性算法](https://neo4j.com/docs/graph-data-science/current/algorithms/node-similarity/)、[拓扑链接预测](https://neo4j.com/docs/graph-data-science/current/algorithms/linkprediction/)
    和 [K-最近邻 (KNN)](https://neo4j.com/docs/graph-data-science/current/algorithms/knn/)。'
- en: '**Centrality & Pathfinding:** I’m grouping these together as they tend to be
    less associated with ML tasks and more with analytical measures. However, they
    still technically fit here, so I will cover them for completeness. [Centrality](https://neo4j.com/docs/graph-data-science/current/algorithms/centrality/)
    finds important or influential nodes in a graph. Centrality is ubiquitous throughout
    many use cases, including fraud and anomaly detection, recommendation, supply
    chain, logistics, and infrastructure problems. [Pathfinding](https://neo4j.com/docs/graph-data-science/current/algorithms/pathfinding/)
    is used to find the lowest cost paths in a graph or to evaluate the quality and
    availability of paths. Pathfinding can benefit many use cases dealing with physical
    systems such as logistics, supply chain, transportation, and infrastructure.'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**中心性与路径寻找：** 我将这两个概念归为一类，因为它们通常与机器学习任务关系较少，而更多涉及分析测量。然而，它们在技术上仍然适用，因此我会为了完整性而涵盖它们。[中心性](https://neo4j.com/docs/graph-data-science/current/algorithms/centrality/)用于查找图中的重要或有影响力的节点。中心性在许多用例中普遍存在，包括欺诈和异常检测、推荐、供应链、物流和基础设施问题。[路径寻找](https://neo4j.com/docs/graph-data-science/current/algorithms/pathfinding/)用于找到图中的最低成本路径或评估路径的质量和可用性。路径寻找可以使许多涉及物理系统的用例受益，例如物流、供应链、交通运输和基础设施。'
- en: How Compression is Key to GML
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 压缩如何成为GML的关键
- en: 'I came across [this interesting blog post](https://www.singlelunch.com/2020/12/28/why-im-lukewarm-on-graph-neural-networks/)
    by Matt Ranger which explains this point beautifully: One of the most significant
    objectives with GML, and to a large extent natural language processing too, is
    compressing large sparse data structures while maintaining important signals for
    prediction and inference.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我遇到了[这篇有趣的博客文章](https://www.singlelunch.com/2020/12/28/why-im-lukewarm-on-graph-neural-networks/)
    by Matt Ranger，它美妙地解释了这一点：GML的一个重要目标，以及在很大程度上自然语言处理的目标，是在保持重要信号用于预测和推断的同时，压缩大型稀疏数据结构。
- en: Consider a [regular graph](https://en.wikipedia.org/wiki/Regular_graph) represented
    by an adjacency matrix, a square matrix where each row and column represents a
    node. If a relationship goes from node A to node B, the cell at the intersection
    of row A and column B is 1; otherwise, it is 0\. Below is an illustration of some
    small regular graphs and their adjacency matrices.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个由邻接矩阵表示的[正则图](https://en.wikipedia.org/wiki/Regular_graph)，这是一个方阵，每行和每列代表一个节点。如果从节点A到节点B的关系存在，则A行与B列交叉的单元格为1；否则为0。下面是一些小型正则图及其邻接矩阵的示例。
- en: '![](../Images/89e6f836e96c1f9dafcc9c3dc60f4ed9.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89e6f836e96c1f9dafcc9c3dc60f4ed9.png)'
- en: Image by Author
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: Notice that many of the cells in the above adjacency matrices are 0\. If you
    scale this to large graphs, particularly those found in real-world applications,
    the proportion of zeros increases, and the adjacency matrix becomes mostly zeros.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，上述邻接矩阵中的许多单元格是0。如果将其扩展到大型图形，特别是现实世界应用中发现的那些图形，零的比例会增加，邻接矩阵将变成大多数是零。
- en: '![](../Images/1133b9639ec505533a506578c5b1da85.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1133b9639ec505533a506578c5b1da85.png)'
- en: Illustrative example created using Last.fm recommendation graph visual from
    [Large Graph Visualization Tools and Approaches](/large-graph-visualization-tools-and-approaches-2b8758a1cd59)
    and matrix image from Beck, Fabian, et al. [Identifying modularization patterns
    by visual comparison of multiple hierarchies](https://www.researchgate.net/figure/Adjacency-matrix-visualization-showing-the-evolutionary-coupling-graph-ECConf-and-the_fig2_305026899)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 说明示例使用了来自[大型图可视化工具和方法](/large-graph-visualization-tools-and-approaches-2b8758a1cd59)的Last.fm推荐图可视化以及来自Beck,
    Fabian等的矩阵图像，[通过对比多个层级识别模块化模式](https://www.researchgate.net/figure/Adjacency-matrix-visualization-showing-the-evolutionary-coupling-graph-ECConf-and-the_fig2_305026899)。
- en: This happens because as these graphs grow, the average [degree centrality](https://neo4j.com/docs/graph-data-science/current/algorithms/degree-centrality/)
    grows much slower or not at all. In social networks, this is evidenced by concepts
    like [the Dunbar Number](https://en.wikipedia.org/wiki/Dunbar%27s_number), a cognitive
    limit to the number of people with whom one can maintain stable social relationships.
    You can intuit this for other types of graphs too, such as graphs of financial
    transactions or graphs of user purchases for recommendation systems. As these
    graphs grow, the number of potential unique transactions or purchases a single
    individual can participate in grows much faster than their capacity to do so.
    I.e. If there are six products on a website, one user buying half of them is feasible,
    but if there are hundreds of thousands, then not so much. As a result, you end
    up with very large and sparse data structures.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为随着这些图的增长，平均[度中心性](https://neo4j.com/docs/graph-data-science/current/algorithms/degree-centrality/)增长得要慢得多，甚至几乎没有增长。在社交网络中，这可以通过像[邓巴数字](https://en.wikipedia.org/wiki/Dunbar%27s_number)这样的概念得到证实，它是一个人能够维持稳定社会关系的认知极限。你也可以对其他类型的图有直观认识，比如金融交易图或推荐系统的用户购买图。随着这些图的增长，一个人参与的潜在独特交易或购买的数量增长得要比他们的能力快得多。也就是说，如果网站上有六种产品，一个用户购买其中一半是可行的，但如果有数十万种产品，则不那么可行。因此，你最终会得到非常大且稀疏的数据结构。
- en: If you could use these sparse data structures directly for machine learning,
    you wouldn’t need GNNs or any GML — you would just plug them as features into
    conventional ML models. However, this isn’t possible. It wouldn’t scale, and even
    beyond that, it would cause mathematical issues around convergence and estimation
    that would render ML models ill-specified and infeasible. As a result, a fundamental
    key to GML is compressing these data structures; arguably, it is the entire point
    of GML.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你能直接将这些稀疏数据结构用于机器学习，你就不需要GNNs或任何GML——你只需将它们作为特征插入到传统的ML模型中即可。然而，这并不可行。它不会扩展，而且即使超越这一点，它还会引发关于收敛和估计的数学问题，使ML模型不准确且不可行。因此，GML的一个基本关键是压缩这些数据结构；可以说，这正是GML的全部意义所在。
- en: How to Accomplish Compression? — Graph Machine Learning Approaches
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现压缩？——图机器学习方法
- en: At the highest level, three GML approaches exist for accomplishing this compression.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在最高层次上，存在三种GML方法来实现这种压缩。
- en: '![](../Images/e86a796f24f8061b5ed8d39b7b163651.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e86a796f24f8061b5ed8d39b7b163651.png)'
- en: Image by Author
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: '**Classic Graph Algorithms**'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**经典图算法**'
- en: Classic graph algorithms include things like [PageRank](https://neo4j.com/docs/graph-data-science/current/algorithms/page-rank/),
    [Louvain](https://neo4j.com/docs/graph-data-science/current/algorithms/louvain/),
    and [Dijkstra’s Shortest Path](https://neo4j.com/docs/graph-data-science/current/algorithms/dijkstra-source-target/).
    They can be used independently for unsupervised community detection, similarity,
    centrality, or pathfinding. The results of classic algorithms can also be used
    as features for conventional downstream models, such as linear and logistic regressions,
    random forests, or neural networks to perform GML tasks.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 经典图算法包括像[PageRank](https://neo4j.com/docs/graph-data-science/current/algorithms/page-rank/)、[Louvain](https://neo4j.com/docs/graph-data-science/current/algorithms/louvain/)和[Dijkstra的最短路径](https://neo4j.com/docs/graph-data-science/current/algorithms/dijkstra-source-target/)。它们可以独立用于无监督社区检测、相似性、中心性或路径查找。经典算法的结果也可以作为传统下游模型的特征，如线性和逻辑回归、随机森林或神经网络，以执行GML任务。
- en: Classic graph algorithms tend to be simple, easy to get started with, and relatively
    interpretable and explainable. However, they can require more manual work and
    subject matter expertise (SME) than other GML approaches. This makes classic graph
    algorithms good first choices in experimentation and development to help understand
    what works best on your graph. They can also do well in production for simpler
    problems, but more complex use cases may require graduating to another GML approach.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 经典图算法往往简单、易于入门，并且相对可解释和解释。然而，它们可能需要比其他GML方法更多的手动工作和主题专家（SME）。这使得经典图算法在实验和开发中成为良好的初步选择，有助于了解在你的图上什么效果最佳。它们在生产中也可以在简单问题上表现良好，但更复杂的用例可能需要升级到另一种GML方法。
- en: '**Non-GNN Graph Embeddings**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**非GNN图嵌入**'
- en: Graph embeddings are a form of representation learning. Some graph embedding
    techniques leverage GNN architectures while others do not. The latter group, non-GNN,
    is the focus of this approach. These embedding techniques instead rely on matrix
    factorization/decomposition, random projections, random walks, or hashing function
    architectures. Some examples include [Node2vec (random walk based)](https://neo4j.com/docs/graph-data-science/current/machine-learning/node-embeddings/node2vec/),
    [FastRP (random projection and matrix operations)](https://neo4j.com/docs/graph-data-science/current/machine-learning/node-embeddings/fastrp/),
    and [HashGNN (hashing function architecture)](https://neo4j.com/docs/graph-data-science/current/machine-learning/node-embeddings/hashgnn/).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图嵌入是一种表示学习形式。一些图嵌入技术利用 GNN 架构，而另一些则没有。后者，即非 GNN，是这种方法的重点。这些嵌入技术则依赖于矩阵分解/分解、随机投影、随机游走或哈希函数架构。一些示例包括
    [Node2vec（基于随机游走）](https://neo4j.com/docs/graph-data-science/current/machine-learning/node-embeddings/node2vec/)、[FastRP（随机投影和矩阵操作）](https://neo4j.com/docs/graph-data-science/current/machine-learning/node-embeddings/fastrp/)
    和 [HashGNN（哈希函数架构）](https://neo4j.com/docs/graph-data-science/current/machine-learning/node-embeddings/hashgnn/)。
- en: Graph embedding involves generating numeric or binary feature vectors to represent
    nodes, relationships, paths, or entire graphs. The foremost of those, node embedding,
    is among the most fundamental and commonly used. The basic idea is to generate
    a vector for each node such that the similarity between vectors (e.g. dot product)
    approximates the similarity between nodes in the graph. Below is an illustrative
    example of the small [Zachary’s karate club network](https://en.wikipedia.org/wiki/Zachary%27s_karate_club).
    Note how the adjacency matrix is compressed to 2-d embedding vectors for each
    node and how those vectors cluster together to reflect the graph community structure.
    Most real-world embeddings will have more than two dimensions (128 to 256 or higher)
    to represent larger real-world graphs with millions or billions of nodes, but
    the basic intuition is the same.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图嵌入涉及生成数值或二进制特征向量来表示节点、关系、路径或整个图。其中，节点嵌入是最基本且最常用的。基本思想是为每个节点生成一个向量，使得向量之间的相似性（例如点积）近似于图中节点之间的相似性。下面是一个小型
    [扎卡里空手道俱乐部网络](https://en.wikipedia.org/wiki/Zachary%27s_karate_club) 的示例。请注意，邻接矩阵是如何被压缩为每个节点的
    2 维嵌入向量，以及这些向量如何聚集在一起以反映图的社区结构。大多数现实世界的嵌入将具有多于两个维度（128 到 256 或更高），以表示具有数百万或数十亿节点的大型现实世界图，但基本直觉是相同的。
- en: '![](../Images/f14b6a0d24f1016edfe7c9bab275bec2.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f14b6a0d24f1016edfe7c9bab275bec2.png)'
- en: Image by Author
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: 'The same logic as above applies to relationship, path, and entire graph embeddings:
    similarity in the embedding vectors should approximate similarity in the graph
    structure. This accomplishes the compression while maintaining important signals,
    making the embeddings useful for various downstream ML tasks.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 上述相同的逻辑也适用于关系、路径和整个图嵌入：嵌入向量中的相似性应当近似于图结构中的相似性。这实现了压缩，同时保持了重要信号，使得嵌入在各种下游机器学习任务中具有用处。
- en: Non-GNN embeddings can benefit from reduced manual workload and required SME
    compared to classic graph algorithms. While non-GNN embeddings often require hyperparameter
    tuning to get right, they tend to be easier to automate and generalize across
    different graphs. Additionally, some non-GNN embeddings like [FastRP](https://neo4j.com/docs/graph-data-science/current/machine-learning/node-embeddings/fastrp/)
    and [HashGNN](https://neo4j.com/docs/graph-data-science/current/machine-learning/node-embeddings/hashgnn/)
    can scale incredibly well to large graphs on commodity hardware since they don’t
    require model training. This can be a massive benefit over GNN-based approaches.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 与经典图算法相比，非 GNN 嵌入可以减少手动工作量和对 SME 的需求。尽管非 GNN 嵌入通常需要调整超参数才能得到最佳效果，但它们往往更容易自动化并在不同图上进行泛化。此外，一些非
    GNN 嵌入如 [FastRP](https://neo4j.com/docs/graph-data-science/current/machine-learning/node-embeddings/fastrp/)
    和 [HashGNN](https://neo4j.com/docs/graph-data-science/current/machine-learning/node-embeddings/hashgnn/)
    由于不需要模型训练，可以在普通硬件上极好地扩展到大图。这相比于基于 GNN 的方法可以带来巨大的好处。
- en: However, non-GNN embeddings come with some trade-offs too. They are less interpretable
    and explainable than classic graph algorithms due to the more generalized mathematical
    operations involved. They are also generally [transductive](https://en.wikipedia.org/wiki/Transduction_(machine_learning)),
    though recent improvements in Neo4j Graph Data Science allow some of them to effectively
    behave inductively in certain applications. We will cover transductive and inductive
    settings in more depth later in this series; it has to do with the ability to
    predict on new unseen data and is an essential point of consideration for GML.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，非GNN嵌入也有一些权衡。由于涉及的数学操作更为一般化，它们的可解释性和解释能力不如经典图算法。它们通常是[转导式的](https://en.wikipedia.org/wiki/Transduction_(machine_learning))，不过
    Neo4j 图数据科学的近期改进使得其中一些在特定应用中可以有效地表现为归纳式。我们将在本系列后续内容中更深入地讨论转导式和归纳式设置；这与对新未见数据的预测能力有关，是
    GML 的一个重要考虑点。
- en: '**Graph Neural Networks (GNNs)**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**图神经网络（GNNs）**'
- en: '![](../Images/3e34911e86e0fd5bb89f9d753da9b6b8.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e34911e86e0fd5bb89f9d753da9b6b8.png)'
- en: Image by Author
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: A GNN is a neural network model that takes graph data as input, transforms it
    into intermediate embeddings, and feeds the embeddings to a final layer aligned
    to a prediction task. This prediction task can be supervised (node property prediction,
    link prediction, graph property prediction) or unsupervised (clustering, similarity,
    or simply a final output embedding for representation learning). So unlike classic
    algorithms and non-GNN embeddings, which pass results as features to downstream
    ML models, particularly for supervised tasks, GNNs are fully end-to-end graph
    native solutions.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: GNN 是一种神经网络模型，它以图数据作为输入，将其转换为中间嵌入，然后将这些嵌入传递到对齐预测任务的最终层。这个预测任务可以是监督性的（节点属性预测、链接预测、图属性预测）或无监督的（聚类、相似性，或仅仅是用于表示学习的最终输出嵌入）。因此，与经典算法和非GNN嵌入不同，后者将结果作为特征传递给下游机器学习模型，特别是在监督任务中，GNNs
    是完全端到端的图原生解决方案。
- en: GNNs have a variety of benefits related to being complete end-to-end solutions.
    Notably, intermediate embeddings are learned during training and, in theory, automatically
    infer the most important information from the graph. Most recent GNNs are also
    inductive due to having a trained model.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: GNNs 作为完整的端到端解决方案，具有多种好处。值得注意的是，在训练过程中学习到的中间嵌入理论上能够自动推断图中的最重要信息。最近的大多数 GNNs
    也因拥有经过训练的模型而具备归纳能力。
- en: GNNs also come with some weaknesses. This includes high complexity, scaling
    difficulties, and low interpretability and explainability. GNNs can also have
    limitations around depth due to over-smoothing and other mathematical principles.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: GNNs（图神经网络）也存在一些弱点，包括高复杂性、扩展困难，以及低可解释性和解释能力。GNNs 由于过度平滑和其他数学原理，还可能在深度方面存在局限性。
- en: 'I’ll discuss GNNs more in my next blog, *GNNs: What They Are and Why They Matter*.
    In the meantime, **if you want to get started with graph machine learning, please
    take a look at** [**Neo4j Graph Data Science**](https://neo4j.com/product/graph-data-science/).
    Data scientists and engineers can find technical documentation for getting started
    [here](https://neo4j.com/docs/graph-data-science/current/).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在下一个博客中进一步讨论 GNNs，*《GNNs：它们是什么以及为何重要》*。与此同时，**如果你想开始图机器学习，请查看** [**Neo4j Graph
    Data Science**](https://neo4j.com/product/graph-data-science/)。数据科学家和工程师可以在[这里](https://neo4j.com/docs/graph-data-science/current/)找到入门的技术文档。
- en: Wrapping Things Up
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'Biggest takeaways from this post:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的主要收获：
- en: Graph Machine Learning (GML) is a broad field with many use case applications
    and comprising multiple different supervised and unsupervised ML tasks
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图机器学习（GML）是一个广泛的领域，涵盖了许多用例应用，包括多个不同的监督和无监督机器学习任务。
- en: One of the primary purposes of GML is compressing large sparse graph structures
    while maintaining important signals for prediction and inference.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GML（图机器学习）的主要目的之一是压缩大型稀疏图结构，同时保持预测和推断所需的重要信号。
- en: GNNs are one of multiple GML approaches that accomplish this compression.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GNNs 是实现这种压缩的多种 GML 方法之一。
