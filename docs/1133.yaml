- en: 'Overcoming Automatic Speech Recognition Challenges: The Next Frontier'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 克服自动语音识别挑战：下一个前沿
- en: 原文：[https://towardsdatascience.com/overcoming-automatic-speech-recognition-challenges-the-next-frontier-e26c31d643cc?source=collection_archive---------2-----------------------#2023-03-30](https://towardsdatascience.com/overcoming-automatic-speech-recognition-challenges-the-next-frontier-e26c31d643cc?source=collection_archive---------2-----------------------#2023-03-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/overcoming-automatic-speech-recognition-challenges-the-next-frontier-e26c31d643cc?source=collection_archive---------2-----------------------#2023-03-30](https://towardsdatascience.com/overcoming-automatic-speech-recognition-challenges-the-next-frontier-e26c31d643cc?source=collection_archive---------2-----------------------#2023-03-30)
- en: Advancements, Opportunities, and Impacts of Automatic Speech Recognition Technology
    in Various Domains
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动语音识别技术在各个领域的进展、机遇和影响
- en: '[](https://medium.com/@talrosenwein?source=post_page-----e26c31d643cc--------------------------------)[![Tal
    Rosenwein](../Images/c5839727d9f63df6b5f26c7aa781679b.png)](https://medium.com/@talrosenwein?source=post_page-----e26c31d643cc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e26c31d643cc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e26c31d643cc--------------------------------)
    [Tal Rosenwein](https://medium.com/@talrosenwein?source=post_page-----e26c31d643cc--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@talrosenwein?source=post_page-----e26c31d643cc--------------------------------)[![Tal
    Rosenwein](../Images/c5839727d9f63df6b5f26c7aa781679b.png)](https://medium.com/@talrosenwein?source=post_page-----e26c31d643cc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e26c31d643cc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e26c31d643cc--------------------------------)
    [Tal Rosenwein](https://medium.com/@talrosenwein?source=post_page-----e26c31d643cc--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc25fa765131b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fovercoming-automatic-speech-recognition-challenges-the-next-frontier-e26c31d643cc&user=Tal+Rosenwein&userId=c25fa765131b&source=post_page-c25fa765131b----e26c31d643cc---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e26c31d643cc--------------------------------)
    ·17 min read·Mar 30, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe26c31d643cc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fovercoming-automatic-speech-recognition-challenges-the-next-frontier-e26c31d643cc&user=Tal+Rosenwein&userId=c25fa765131b&source=-----e26c31d643cc---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc25fa765131b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fovercoming-automatic-speech-recognition-challenges-the-next-frontier-e26c31d643cc&user=Tal+Rosenwein&userId=c25fa765131b&source=post_page-c25fa765131b----e26c31d643cc---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e26c31d643cc--------------------------------)
    ·17 min 阅读·2023年3月30日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe26c31d643cc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fovercoming-automatic-speech-recognition-challenges-the-next-frontier-e26c31d643cc&user=Tal+Rosenwein&userId=c25fa765131b&source=-----e26c31d643cc---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe26c31d643cc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fovercoming-automatic-speech-recognition-challenges-the-next-frontier-e26c31d643cc&source=-----e26c31d643cc---------------------bookmark_footer-----------)![](../Images/f292fc6b02eed72e8b2ba7f32a1dc8c9.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe26c31d643cc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fovercoming-automatic-speech-recognition-challenges-the-next-frontier-e26c31d643cc&source=-----e26c31d643cc---------------------bookmark_footer-----------)![](../Images/f292fc6b02eed72e8b2ba7f32a1dc8c9.png)'
- en: Photo by [Andrew DesLauriers](https://unsplash.com/@andrewdesla?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由 [Andrew DesLauriers](https://unsplash.com/@andrewdesla?utm_source=medium&utm_medium=referral)
    拍摄于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'TL;DR:'
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'TL;DR:'
- en: '*This post focuses on the advancements in Automatic Speech Recognition (ASR)
    technology and its impact on various domains. ASR has become prevalent in multiple
    industries, with improved accuracy driven by scaling model size and constructing
    larger labeled and unlabelled training datasets.*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*这篇文章聚焦于自动语音识别（ASR）技术的进展及其对各个领域的影响。ASR 已在多个行业中变得普及，其准确性通过扩大模型规模和构建更大规模的标注和未标注训练数据集得到了提升。*'
- en: '*Looking ahead, ASR technology is expected to continue improving with the scaling
    of the acoustic model size and the enhancement of the internal language model.
    Additionally, self-supervised and multi-task training techniques will enable low-resource
    languages to benefit from ASR technology, while multilingual training will boost
    performance even further, allowing for basic usage such as voice commands in many
    low-resource languages.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*展望未来，ASR技术预计将随着声学模型规模的扩大和内部语言模型的增强而持续改进。此外，自监督和多任务训练技术将使低资源语言受益于ASR技术，而多语言训练将进一步提升性能，使许多低资源语言能够进行基本使用，如语音命令。*'
- en: '*ASR will also play a significant role in Generative AI, as interaction with
    avatars will be via an audio/text interface. With the emergence of textless NLP,
    some end-tasks, such as speech-2-speech translation, may be solved without using
    any explicit ASR model. Multimodal models that can be prompted using text, audio,
    or both will be released and generate text or synthesize audio as an output.*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*ASR在生成式AI中也将发挥重要作用，因为与虚拟角色的交互将通过音频/文本接口进行。随着无文本NLP的出现，一些最终任务，如语音对语音翻译，可能在不使用任何明确的ASR模型的情况下得到解决。将发布能够通过文本、音频或两者进行提示的多模态模型，并生成文本或合成音频作为输出。*'
- en: '*Furthermore, open-ended dialogue systems with voice-based human-machine interfaces
    will improve robustness to transcription errors and differences between written
    and spoken forms. This will provide robustness to challenging accents and children’s
    speech, enabling ASR technology to become an essential tool for many applications.*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*此外，具有语音交互界面的开放式对话系统将提高对转录错误以及书面和口头形式之间差异的鲁棒性。这将增强对挑战性口音和儿童语言的鲁棒性，使ASR技术成为许多应用中的重要工具。*'
- en: '*An end-to-end speech enhancement-ASR-diarization system is set to be released,
    enabling the personalization of ASR models and improving performance on overlapped
    speech and challenging acoustic scenarios. This is a significant step towards
    solving ASR technology’s challenges in real-world scenarios.*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*一个端到端的语音增强-ASR-分段系统将发布，使ASR模型能够个性化，并提高在重叠语音和挑战性声学场景中的表现。这是解决ASR技术在现实世界场景中的挑战的重要一步。*'
- en: '*Lastly, A wave of speech APIs is expected. And still, there are opportunities
    for small startups to outperform big tech companies in domains with more legal
    or regulatory restrictions on the use of technology/data acquisition and in populations
    with low technology adoption rates.*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*最后，预计将出现一波语音API。同时，仍然存在小型初创公司在技术/数据采集使用和技术采纳率低的群体中超越大型科技公司的机会。*'
- en: '**2022 In A Review**'
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**2022年回顾**'
- en: Automatic Speech Recognition (ASR) technology is gaining momentum across various
    industries such as education, podcasts, social media, telemedicine, call centers,
    and more. A great example is the growing prevalence of voice-based human-machine
    interface (HMI) in consumer products, such as smart cars, smart homes, smart assistive
    technology [1], smartphones, and even artificial intelligence (AI) assistants
    in hotels [2]. In order to meet the increasing demand for fast and accurate responses,
    low-latency ASR models have been deployed for tasks like keyword spotting [3],
    endpointing [4], and transcription [5]. Speaker-attributed ASR models [6–7] are
    also gaining attention as they enable product personalization, providing greater
    value to end-users.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 自动语音识别（ASR）技术在教育、播客、社交媒体、远程医疗、呼叫中心等多个行业中正在获得越来越多的关注。一个很好的例子是语音交互界面（HMI）在消费产品中的普及，如智能汽车、智能家居、智能辅助技术[1]、智能手机，甚至酒店中的人工智能（AI）助手[2]。为了满足对快速且准确响应的日益增长的需求，低延迟ASR模型已被部署用于关键词检测[3]、端点检测[4]和转录[5]等任务。带有说话人属性的ASR模型[6–7]也越来越受到关注，因为它们能够实现产品个性化，为终端用户提供更大的价值。
- en: '**Prevalence of Data.** Streaming audio and video platforms such as social
    media and YouTube have led to the easy acquisition of unlabeled audio data [8].
    New self-supervised techniques have been introduced to utilize this audio without
    needing ground truth [9–10]. These techniques improve the performance of ASR systems
    in the target domain, even without fine-tuning on labeled data for that domain
    [11]. Another approach gaining attention due to its ability to utilize this unlabeled
    data is self-training using pseudo-labeling [12–13]. The main concept is to automatically
    transcribe unlabeled audio data using an automatic speech recognition (ASR) system
    and then use the generated transcription as ground truth for training a different
    ASR system in a supervised fashion. OpenAI took a different approach, assuming
    they can find human-generated transcripts at scale online. They generated a high-quality
    and large-scale (640K hours) training dataset by crawling publicly available audio
    data with human-generated subtitles. Using this dataset, they trained an ASR model
    (a.k.a Whisper) in a fully supervised manner, achieving state-of-the-art (SoTA)
    results on several benchmarks in zero-shot settings [14].'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据的普遍性。** 流媒体音频和视频平台，如社交媒体和YouTube，导致了未标记音频数据的轻松获取[8]。新引入的自监督技术可以在无需真实标签的情况下利用这些音频[9–10]。这些技术在目标领域提高了ASR系统的性能，即使在该领域未对标记数据进行微调[11]。另一种因能够利用这些未标记数据而受到关注的方法是使用伪标签的自训练[12–13]。主要概念是使用自动语音识别（ASR）系统自动转录未标记的音频数据，然后将生成的转录本作为真实标签，用于以监督方式训练另一个ASR系统。OpenAI采取了不同的方法，假设他们可以在网上大规模找到人工生成的转录本。他们通过抓取公开的音频数据及其人工生成的字幕，生成了一个高质量且大规模（640K小时）的训练数据集。利用这个数据集，他们以完全监督的方式训练了一个ASR模型（即Whisper），在零-shot设置下在多个基准测试中达到了**最先进的**结果[14]。'
- en: '**Losses.** Despite End-2-end (E2E) losses dominating SoTA ASR models [15–17],
    new losses are still being published. A new technique called hybrid autoregressive
    transducer (HAT) [18] has been introduced, enabling to measure the quality of
    the internal language model (ILM) by separating the blank and label posteriors.
    Later work [19] used this factorization to effectively adapt the ILM using only
    textual data, which improved the overall performance of ASR systems, particularly
    the transcription of named entities, slang terms, and nouns, which are major pain
    points for ASR systems. New metrics have also been developed to **better align
    with human perception** and overcome word error rate (WER) semantic issues [20].'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**损失。** 尽管End-2-end（E2E）损失主导了**最先进的**ASR模型[15–17]，新的损失函数仍在不断发布。一种名为混合自回归转换器（HAT）的新技术[18]被引入，能够通过分离空白和标签后验来测量内部语言模型（ILM）的质量。后来工作[19]使用这种因子分解有效地适配了ILM，仅利用文本数据，提高了ASR系统的整体性能，特别是对专有名词、俚语和名词的转录，这些是ASR系统的主要痛点。还开发了新的度量标准，以**更好地对齐人类感知**，克服了词错误率（WER）的语义问题[20]。'
- en: '**Architecture Choice.** Regarding the acoustic model’s architectural choices,
    Conformer [21] remained preferred for streaming models, while Transformers [22]
    is the default architecture for non-streaming models. As for the latter, encoder-only
    (wav2vec2 based [23–24]) and encoder-decoder (Whisper [14]) multi-lingual models
    were introduced and improved over the SoTA results across several benchmarks in
    zero-shot settings. These models outperform their streaming counterparts due to
    model size, training data size, and their larger context.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**架构选择。** 在声学模型的架构选择方面，Conformer [21] 仍然是流媒体模型的首选，而Transformers [22] 是非流媒体模型的默认架构。至于后者，引入了仅编码器（基于wav2vec2
    [23–24]）和编码器-解码器（Whisper [14]）的多语言模型，并在多个基准测试中超越了**最先进的**结果。由于模型大小、训练数据量和更大的上下文，这些模型优于流媒体模型。'
- en: '**Multilingual AI Developments from Tech Giants.** Google has announced its
    “1,000 Languages Initiative” to build an AI model that supports the 1,000 most
    spoken languages [25], while Meta AI has announced its long-term effort to build
    language and machine translation (MT) tools that include most of the world’s languages
    [26].'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**科技巨头的多语言AI发展。** Google宣布了其“1,000语言计划”，以构建支持1000种最常用语言的AI模型[25]，而Meta AI则宣布了其长期努力，旨在构建包含大多数世界语言的语言和机器翻译（MT）工具[26]。'
- en: '**Spoken Language Breakthrough.** Multi-modal (speech/text) and multi-task
    pre-trained seq-2-seq (encoder-decoder) models such as SpeechT5 [27] were released,
    showing great success on a wide variety of spoken language processing tasks, including
    ASR, speech synthesis, speech translation, voice conversion, speech enhancement,
    and speaker identification.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**语音语言突破。** 多模态（语音/文本）和多任务预训练 seq-2-seq（编码器-解码器）模型，如 SpeechT5 [27]，已经发布，在各种语音语言处理任务中取得了巨大成功，包括
    ASR、语音合成、语音翻译、语音转换、语音增强和说话人识别。'
- en: These advancements in ASR technology are expected to drive further innovation
    and impact a wide range of industries in the years to come.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 ASR 技术的进步预计将推动进一步创新，并在未来几年对广泛的行业产生影响。
- en: '**A Look Ahead**'
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**展望未来**'
- en: Despite its challenges, the field of Automatic Speech Recognition (ASR) is expected
    to make significant advancements in various domains, ranging from acoustic and
    semantic modeling to conversational and generative AI, and even speaker-attributed
    ASR. This section provides detailed insights into these areas and shares my predictions
    for the future of ASR technology.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管面临挑战，但自动语音识别（ASR）领域预计将在多个领域取得重大进展，从声学和语义建模到对话式和生成式 AI，甚至包括说话人归属的 ASR。本节提供了这些领域的详细见解，并分享了我对
    ASR 技术未来的预测。
- en: '![](../Images/09a0b0698a10de53a291ac944b8d7b5e.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09a0b0698a10de53a291ac944b8d7b5e.png)'
- en: Photo by [Nik](https://unsplash.com/@helloimnik?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Nik](https://unsplash.com/@helloimnik?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '**General Improvements:**'
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**一般改进：**'
- en: The improvement of ASR systems is expected on both the acoustic and semantic
    parts.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 预计 ASR 系统在声学和语义方面都会有所改进。
- en: On the acoustic model side, larger model and training data sizes are anticipated
    to enhance the overall performance of ASR systems, similar to the progress observed
    in LLMs. Although scaling Transformer encoders, such as Wav2Vec or Conformer,
    poses a challenge, a breakthrough is expected to enable their scaling or see a
    shift towards encoder-decoder architectures as in Whisper. However, encoder-decoder
    architectures have drawbacks that need to be addressed, such as hallucinations.
    Optimizations such as faster-whisper [28] and NVIDIA-wav2vec2 [29] will reduce
    training and inference time, lowering the barrier to deploying large ASR models.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在声学模型方面，预计更大的模型和训练数据集将提升 ASR 系统的整体性能，类似于 LLMs 领域的进展。尽管扩展 Transformer 编码器，如 Wav2Vec
    或 Conformer，存在挑战，但预计会有突破使其能够扩展，或出现向编码器-解码器架构（如 Whisper）转变的趋势。然而，编码器-解码器架构有一些需要解决的缺陷，如幻觉。优化技术，如
    faster-whisper [28] 和 NVIDIA-wav2vec2 [29]，将减少训练和推理时间，从而降低部署大型 ASR 模型的门槛。
- en: On the semantic side, researchers will focus on improving ASR models by incorporating
    larger acoustic or textual contexts. Injecting large-scale unpaired text into
    the ILM during E2E training, as in JEIT [30], will also be explored. These efforts
    will help to overcome key challenges such as accurately transcribing named entities,
    slang terms, and nouns.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在语义方面，研究人员将重点关注通过融入更大的声学或文本上下文来改进 ASR 模型。还将探索在 E2E 训练期间向 ILM 注入大规模无配对文本，如 JEIT
    [30]。这些努力将有助于克服准确转录命名实体、俚语和名词等关键挑战。
- en: Although Whisper and Google’s universal speech model (USM) [31] have improved
    ASR system performances over several benchmarks, some benchmarks still need to
    be solved as the word error rate (WER) remains around 20% [32]. Using speech **foundation
    models**, adding more diverse training data, and applying multi-task learning
    will significantly improve performance in such scenarios, opening up new business
    opportunities. Moreover, new metrics and benchmarks are expected to emerge to
    **better align new end-tasks and domains**, such as non-lexical conversational
    sounds [33] in the medical domain and filler word detection and classification
    [34] in media editing and educational domains. Task-specific fine-tuned models
    may be developed for this purpose. Finally, with the growth of multi-modality,
    more models, training datasets, and new benchmarks for several tasks are also
    expected to be released [35–36].
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Whisper和谷歌的通用语音模型（USM）[31]在多个基准测试中提高了ASR系统的性能，但一些基准测试仍需解决，因为词错误率（WER）仍然约为20%
    [32]。使用语音**基础模型**、添加更多多样化的训练数据和应用多任务学习将显著提升此类场景中的性能，从而开辟新的商业机会。此外，预计将出现新的指标和基准，以**更好地对齐新的最终任务和领域**，例如医学领域中的非词汇对话声音[33]以及媒体编辑和教育领域中的填充词检测与分类[34]。可能会为此目的开发特定任务的微调模型。最后，随着多模态的发展，预计还会发布更多模型、训练数据集和新任务的基准[35–36]。
- en: As progress continues, a wave of speech APIs is expected, similar to natural
    language processing (NLP). Google’s USM, OpenAI’s Whisper, and Assembly’s Conformer-1
    [37] are some of the early examples.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 随着进展的不断推进，预计会出现一波类似自然语言处理（NLP）的语音API。谷歌的USM、OpenAI的Whisper和Assembly的Conformer-1
    [37]是一些早期的例子。
- en: Although it sounds silly, force alignment is still challenging for many companies.
    An open-source code for that may help many achieve accurate alignment between
    audio segments and their corresponding transcript.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管听起来有些荒谬，但强制对齐对许多公司来说仍然具有挑战性。一个开源代码可能会帮助许多人实现音频片段与其对应转录文本之间的准确对齐。
- en: '**Low Resources Languages:**'
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**低资源语言：**'
- en: Advancements in self-supervised learning, multi-task learning, and multi-lingual
    models are expected to improve performance on low-resource and unwritten languages
    significantly. These methods will achieve acceptable performances by utilizing
    pre-trained models and fine-tuning on a relatively small number of labeled samples
    [24]. Another promising approach is dual learning [38], a paradigm for semi-supervised
    machine learning that seeks to leverage unsupervised data by solving two opposite
    tasks (text-to-speech (TTS) and ASR in our case) at once. In this method, each
    model produces pseudo-labels for unlabeled examples, which are used to train the
    other model.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习、多任务学习和多语言模型的进展预计将显著提高低资源和未书写语言的性能。这些方法将通过利用预训练模型和在相对较少的标记样本上进行微调来实现可接受的性能[24]。另一种有前途的方法是双重学习[38]，这是一种半监督机器学习的范式，旨在通过同时解决两个对立任务（在我们的案例中为文本到语音（TTS）和ASR）来利用无监督数据。在这种方法中，每个模型生成未标记样本的伪标签，这些伪标签用于训练另一个模型。
- en: Additionally, improving ILM using unpaired text can enhance model robustness,
    which will be especially advantageous for closed-set challenges such as voice
    commands. The performance will be acceptable but not flawless in some applications,
    such as captioning YouTube videos, while in others, such as generating verbatim
    transcripts in court, it may take more time for models to meet the threshold.
    We anticipate that companies will gather data based on these models while manually
    correcting transcripts in 2023, and we will see significant improvements in low-resource
    languages after fine-tuned on proprietary data in 2024.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，使用无配对文本改进ILM可以增强模型的鲁棒性，这将特别有利于封闭集挑战，例如语音命令。在一些应用中，如YouTube视频的字幕，性能将是可接受的，但不是完美的，而在其他应用中，如法庭上的逐字记录，模型可能需要更多时间才能达到标准。我们预计公司将在2023年根据这些模型收集数据，同时手动纠正转录文本，并且我们将在2024年在专有数据上微调后看到低资源语言的显著改进。
- en: '**Generative AI:**'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**生成式AI：**'
- en: The use of avatars is expected to revolutionize human interaction with digital
    assets. In the short term, ASR will serve as one of the foundations in Generative
    AI as these avatars will communicate through textual/auditory interface.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟形象的使用预计将彻底改变人与数字资产的互动。在短期内，ASR将作为生成式AI的基础之一，因为这些虚拟形象将通过文本/听觉界面进行交流。
- en: 'But in the future, changes could occur as attention shifts towards new research
    directions. For example, an emerging technology that is likely to be adopted is
    Textless NLP, which represents a new language modeling approach to audio generation
    [39]. This approach uses learnable discrete audio units [40], and auto-regressively
    generates the next discrete audio unit one unit at a time, similar to text generation.
    These discrete units can be later decoded back to the audio domain. Thus far,
    this technology has been able to generate syntactically and semantically plausible
    speech continuations while also maintaining speaker identity and prosody for unseen
    speakers, as can be seen in GSLM/AudioLM [39, 41]. The potential of this technology
    is enormous, as one can skip the ASR component (and its errors) in many tasks.
    For example, traditional speech-2-speech (S2S) translation methods work as follows:
    They transcribe the utterance in the source language, then translate the text
    to the target language using a machine translation model, and finally generate
    the audio in the target languages using a TTS engine. Using textless-NLP technology,
    S2S translation can be done using a single encoder-decoder architecture that works
    directly on discrete audio units without using any explicit ASR model [42]. We
    predict that future Textless NLP models will solve many other tasks without going
    through explicit transcription, such as question-answering systems. However, the
    main drawback of this method is backtracking errors and debugging, as things will
    get less intuitive when working on the discrete units space rather than working
    on the transcription.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 但未来，随着注意力转向新的研究方向，可能会发生变化。例如，一个可能被采用的新兴技术是Textless NLP，它代表了一种新的语言建模方法，用于音频生成[39]。这种方法使用可学习的离散音频单元[40]，并以自回归的方式一次生成一个离散音频单元，类似于文本生成。这些离散单元可以随后解码回音频领域。目前，这项技术已经能够生成在句法和语义上都合理的语音继续，同时保持说话人的身份和韵律，对于未见过的说话人也能做到，如GSLM/AudioLM
    [39, 41]所示。这项技术的潜力巨大，因为可以在许多任务中跳过ASR组件（及其错误）。例如，传统的语音到语音（S2S）翻译方法如下：它们首先将源语言中的话语转录出来，然后使用机器翻译模型将文本翻译为目标语言，最后使用TTS引擎生成目标语言的音频。使用textless-NLP技术，S2S翻译可以通过一个直接作用于离散音频单元的编码-解码架构来完成，而不使用任何显式的ASR模型[42]。我们预测，未来的Textless
    NLP模型将能够解决许多其他任务，而无需经过显式转录，例如问答系统。然而，这种方法的主要缺点是回溯错误和调试，因为在离散单元空间中工作时，事情将变得不那么直观。
- en: T5 [43] and T0 [44] showed great success in NLP by utilizing their multi-task
    training and showing zero-shot task generalization. In 2021 SpeechT5 [27] was
    published, showing great success in various spoken language processing tasks.
    Earlier this year, VALL-E [45] and VALL-EX [46] were released. They showed impressive
    in-context learning capabilities for TTS models by using textless NLP technology,
    enabling cloning speaker’s voice by using only a few seconds of their audio, and
    without requiring any fine-tuning, doing it even in cross-lingual settings.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: T5 [43] 和 T0 [44] 在NLP领域通过利用其多任务训练和展示零样本任务泛化取得了巨大成功。2021年发布的SpeechT5 [27] 在各种口语语言处理任务中表现出色。今年早些时候，VALL-E
    [45] 和 VALL-EX [46] 被发布。它们通过使用textless NLP技术展示了TTS模型的令人印象深刻的上下文学习能力，仅通过几秒钟的音频就能克隆说话人的声音，并且无需任何微调，甚至在跨语言环境中也能做到。
- en: By joining the concepts taken from SpeechT5 and VALL-E, we can expect the release
    of T0-like models that can be prompted using either text, audio, or both, and
    generate text or synthesize audio as an output, depending on the task. A new era
    of models will begin, as in-context learning will enable generalization in zero-shot
    settings to new tasks. This will allow semantic search over audio, transcribing
    a target speaker using speaker-attributed ASR or describing it in free text, e.g.,
    ‘what did the young kid that coughed say?”. Furthermore, it will enable us to
    classify or synthesize audio using audio or textual description and solve NLP
    tasks directly from audio using explicit/implicit ASR.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合来自SpeechT5和VALL-E的概念，我们可以期待类似T0的模型的发布，这些模型可以通过文本、音频或两者来进行提示，并根据任务生成文本或合成音频。一个新时代的模型将开始出现，因为上下文学习将使得在零样本设置中对新任务进行泛化成为可能。这将允许在音频上进行语义搜索，通过带有说话人属性的ASR转录目标说话人，或用自由文本描述它，例如，“那个咳嗽的年轻孩子说了什么？”此外，它将使我们能够使用音频或文本描述来分类或合成音频，并通过显式/隐式ASR直接解决NLP任务。
- en: '**Conversational AI:**'
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**对话AI：**'
- en: Conversational AI has been adopted mainly through task-oriented dialogue systems,
    namely AI personal assistants (PA) such as Amazon’s Alexa and Apple’s Siri. These
    PAs have become popular due to their ability to provide quick access to features
    and information through voice commands. As big tech companies dominate this technology,
    new regulations on AI assistants will force them to offer third-party options
    for voice assistants, opening up competition [47]. As this happens, we can expect
    interoperability between personal assistants, meaning they will start communicating.
    This will be great as one can use any device to connect to any conversational
    agent anywhere in the world [48]. From the ASR perspective, this will pose new
    challenges as the contextualization will be much broader, and assistants will
    must have the robustness to different accents and possibly support multilingualism.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对话型AI主要通过任务导向的对话系统被采纳，即如亚马逊的Alexa和苹果的Siri这样的AI个人助理（PA）。这些PA因其通过语音命令快速访问功能和信息的能力而变得流行。随着大科技公司主导这一技术，对AI助理的新规定将迫使它们提供第三方语音助理选项，从而打开竞争[47]。随着这一变化，我们可以预期个人助理之间会实现互操作性，也就是说它们将开始进行通信。这将非常棒，因为人们可以使用任何设备连接到世界任何地方的对话代理[48]。从ASR的角度来看，这将带来新的挑战，因为上下文化将变得更加广泛，助理必须具备对不同口音的鲁棒性，并可能支持多语言。
- en: Over the past few years, a great technological leap has happened in text-based
    open-ended dialogue systems, e.g., Blender-Bot and LaMDA [49–50]. Initially, these
    dialogue systems were text-based, meaning they were fed by text and trained to
    output text, all in the written-form domain. As ASR performances improved, open-ended
    dialogue systems were augmented with voice-based HMI, which resulted in **misalignment
    between modalities** due to differences between the spoken and written forms.
    One of the main challenges is to bridge this gap by overcoming new types of errors
    introduced due to the audio-related processing, e.g., differences between spoken
    and written forms such as disfluencies and entity resolution, and transcription
    errors such pronunciation errors [51–52].
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，基于文本的开放式对话系统发生了巨大的技术飞跃，例如Blender-Bot和LaMDA [49–50]。最初，这些对话系统是基于文本的，即它们通过文本输入并训练输出文本，全部在书面领域内。随着ASR性能的提高，开放式对话系统被增强了语音基础的人机界面（HMI），这导致了由于口语和书面形式之间的差异而出现的**模态不一致**。主要挑战之一是弥合这一差距，通过克服由于音频相关处理引入的新类型错误，例如口语和书面形式之间的流利度差异和实体解析，以及转录错误，如发音错误[51–52]。
- en: Possible solutions can be derived from improved transcription quality and robust
    NLP models that can effectively handle transcription and pronunciation errors.
    A reliable acoustic model’s confidence score [53] will serve as a key player in
    these systems, enabling it to point out speaker errors or serve as another input
    to the NLP model or decoding logic. Furthermore, we expect that ASR models will
    predict non-verbal cues such as sarcasm, enabling agents to understand the conversation
    more deeply and provide better responses.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 可能的解决方案包括改进转录质量和强大的NLP模型，这些模型能够有效处理转录和发音错误。可靠的声学模型置信度评分[53]将作为这些系统的关键角色，帮助指出说话者错误或作为NLP模型或解码逻辑的另一个输入。此外，我们预计ASR模型将能够预测非语言线索，如讽刺，使代理能够更深入地理解对话并提供更好的回应。
- en: These improvements will enable to push further dialogue systems with an auditory
    HMI to support challenging accents and children’s speech, such as in Loora [54]
    and Speaks [55].
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这些改进将推动进一步的发展，使对话系统具备语音HMI，以支持挑战性的口音和儿童语言，例如在Loora [54]和Speaks [55]中。
- en: Pushing the limits even further, we expect the release of an E2E multi-task
    learning framework for spoken language tasks using joint modeling of the speech
    and NLP problems as in MTL-SLT [56]. These models will train in an E2E fashion
    that will reduce the cumulative error between sequential modules and will address
    tasks such as spoken language understanding, spoken summarization, and spoken
    question answering, by taking speech as input and emitting various outputs such
    as transcription, intent, named entities, summaries, and answers to text queries.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 更进一步，我们预计将发布一个E2E多任务学习框架，用于语音语言任务，采用语音和NLP问题的联合建模，如MTL-SLT [56]。这些模型将以E2E的方式进行训练，减少顺序模块之间的累计误差，并处理如口语理解、口语总结和口语问答等任务，通过将语音作为输入，产生转录、意图、命名实体、摘要和文本查询答案等各种输出。
- en: 'Personalization will play a huge factor for AI assistants and open-ended dialogue
    systems, leading us to the next point: speaker-attributed ASR.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 个性化将对AI助手和开放式对话系统产生巨大影响，这将引领我们进入下一个要点：说话人属性ASR。
- en: '**Speaker Attributed ASR:**'
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**说话人属性ASR：**'
- en: There is still a challenge in transcribing distant conversations involving multiple
    microphones and parties in home environments. Even state-of-the-art (SoTA) systems
    can only achieve around 35% WER [57].
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在家庭环境中涉及多个麦克风和方的远程对话的转录仍然存在挑战。即使是最先进（SoTA）的系统也只能实现约35%的WER[57]。
- en: Early birds of joint ASR and diarization were released in 2019 [58]. This year,
    we can expect a release of an end-to-end speech enhancement-ASR-diarization system
    which will improve performance on overlapped speech and enable better performance
    in challenging acoustic scenarios such as reverberant rooms, far-field settings,
    and low Signal-to-Noise (SNR) ratios. The improvement will be achieved through
    joint task optimization, improved pre-training methods (such as WavLM [10]), applying
    architectural changes [59], data augmentation, and training on in-domain data
    during pre-training and fine-tuning [11]. Moreover, we can expect the deployment
    of speaker-attributed ASR systems for personalized speech recognition. This will
    further improve the transcription accuracy of the target speaker’s voice and bias
    the transcript towards user-defined words, such as contact names, proper nouns,
    and other named entities, which are crucial for smart assistants [60]. Additionally,
    low latency models will continue to be a significant area of focus to enhance
    edge devices’ overall experience and response time [61–62].
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的联合ASR和分段系统于2019年发布[58]。今年，我们可以期待发布一种端到端的语音增强-ASR-分段系统，该系统将改善重叠语音的性能，并在挑战性的声学场景（如混响房间、远场设置和低信噪比(SNR)）中提供更好的表现。通过联合任务优化、改进的预训练方法（如WavLM
    [10]）、应用架构更改[59]、数据增强以及在预训练和微调期间在领域内数据上的训练[11]，将实现这些改进。此外，我们可以期待针对个性化语音识别的说话人属性ASR系统的部署。这将进一步提高目标说话人语音的转录准确性，并将转录偏向用户定义的词汇，如联系人姓名、专有名词和其他命名实体，这些对智能助手至关重要[60]。此外，低延迟模型将继续成为重点领域，以增强边缘设备的整体体验和响应时间[61–62]。
- en: '**The Role of Startups Compared to Big Tech Companies in The ASR Landscape**'
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**初创公司在ASR领域相对于大科技公司的角色**'
- en: Although big tech companies are expected to continue dominating the market with
    their APIs, small startups can still outperform them in specific domains. These
    include areas that are underrepresented in the big tech’s training data due to
    regulations, such as the medical domain and children’s speech, and populations
    that have not yet adopted technology, such as immigrants with challenging accents
    or individuals learning English worldwide. In markets where there isn’t enough
    demand for big tech companies to invest in, such as languages that are not widely
    spokem small startups may find opportunities to succeed and generate profit.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大科技公司预计将继续凭借其API主导市场，但小型初创企业在特定领域仍然可以超越它们。这些领域包括由于法规原因在大科技公司的训练数据中被低估的领域，如医疗领域和儿童语言，以及尚未采用技术的群体，如具有挑战性口音的移民或全球学习英语的个人。在大科技公司不愿投资的市场中，例如那些不广泛使用的语言，小型初创企业可能会发现成功和获利的机会。
- en: To create a win-win situation, big tech companies can provide APIs that offer
    full access to the output of their acoustic models while allowing others to write
    the decoding logic (WFST/beam-search) instead of merely adding customizable vocabulary
    or using current model adaptation features [63–64]. This approach will enable
    small startups to excel in their domains by incorporating priming or multiple
    language models during inference on top of the given acoustic model, rather than
    having to train the acoustic models themselves, which can be costly in terms of
    human capital and domain knowledge. In turn, big tech companies will benefit from
    broader adoption of their paid models.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创造双赢的局面，大科技公司可以提供API，允许完全访问其声学模型的输出，同时允许其他人编写解码逻辑（WFST/beam-search），而不仅仅是添加可定制词汇或使用当前模型适应功能[63–64]。这种方法将使小型初创企业能够通过在给定声学模型之上进行推理时结合引导或多语言模型来在其领域中脱颖而出，而不必自己训练声学模型，这在人力资本和领域知识上可能成本高昂。反过来，大科技公司将从其付费模型的更广泛采用中受益。
- en: '**How Does ASR Fit Into The Broader Machine Learning Landscape?**'
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ASR如何融入更广泛的机器学习领域？**'
- en: On one hand, ASR is on par with the importance of computer vision (CV) and NLP
    when considering it as the end task. This is the current situation in low-resource
    languages and domains where the transcript is the main business, e.g., court,
    medical records, movie subtitles, etc.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，考虑到 ASR 作为最终任务时，其重要性与计算机视觉（CV）和 NLP 相当。这是低资源语言和领域中，转录是主要业务的当前情况，例如法庭、医疗记录、电影字幕等。
- en: On the other hand, ASR is no longer the bottleneck in other domains where it
    has passed a certain usability threshold. In these cases, the NLP is the bottleneck,
    which means that improving ASR performances toward perfectionism is not essential
    for extracting insights for the end task. For example, meeting summarization or
    action item extraction can be achieved in many cases using current ASR quality.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在其他领域，ASR 已不再是瓶颈，只要它达到了某种可用性阈值。在这些情况下，NLP 才是瓶颈，这意味着将 ASR 性能提升到完美主义并不是提取最终任务见解的关键。例如，会议总结或行动项提取在许多情况下可以利用当前的
    ASR 质量实现。
- en: '**Closing Remarks**'
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**结束语**'
- en: The advancements in ASR technology have brought us closer to achieving seamless
    communication between humans and machines, for example in Conversational AI and
    Generative AI. With the continued development of speech enhancement-ASR-diarization
    systems and the emergence of textless NLP, we are poised to witness exciting breakthroughs
    in this field. As we look forward to the future, we can’t help but anticipate
    the endless possibilities that ASR technology will unlock.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ASR 技术的进步使我们更接近于实现人与机器之间的无缝沟通，例如在对话 AI 和生成 AI 中。随着语音增强-ASR-标记系统的持续发展以及无文本 NLP
    的出现，我们有望在这个领域见证激动人心的突破。展望未来，我们不禁期待 ASR 技术将解锁的无限可能。
- en: '*Thank you for taking the time to read this post! Your thoughts and feedback
    on these projections are highly valued and appreciated. Please feel free to share
    your comments and ideas.*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*感谢您花时间阅读这篇文章！我们非常重视和感激您对这些预测的看法和反馈。请随时分享您的评论和想法。*'
- en: '**References:**'
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**参考文献：**'
- en: '[1] [https://www.orcam.com/en/home/](https://www.orcam.com/en/home/)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [https://www.orcam.com/en/home/](https://www.orcam.com/en/home/)'
- en: '[2] [https://voicebot.ai/2022/12/01/hey-disney-custom-alexa-assistant-rolls-out-at-disney-world/](https://voicebot.ai/2022/12/01/hey-disney-custom-alexa-assistant-rolls-out-at-disney-world/)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [https://voicebot.ai/2022/12/01/hey-disney-custom-alexa-assistant-rolls-out-at-disney-world/](https://voicebot.ai/2022/12/01/hey-disney-custom-alexa-assistant-rolls-out-at-disney-world/)'
- en: '[3] Jose, Christin, et al. “Latency Control for Keyword Spotting.” ArXiv, 2022,
    [https://doi.org/10.21437/Interspeech.2022-10608](https://doi.org/10.21437/Interspeech.2022-10608).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Jose, Christin 等人。“关键词检测的延迟控制。” ArXiv，2022，[https://doi.org/10.21437/Interspeech.2022-10608](https://doi.org/10.21437/Interspeech.2022-10608)。'
- en: '[4] Bijwadia, Shaan, et al. “Unified End-to-End Speech Recognition and Endpointing
    for Fast and Efficient Speech Systems.” ArXiv, 2022, [https://doi.org/10.1109/SLT54892.2023.10022338](https://doi.org/10.1109/SLT54892.2023.10022338).'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Bijwadia, Shaan 等人。“统一的端到端语音识别与端点检测以实现快速高效的语音系统。” ArXiv，2022，[https://doi.org/10.1109/SLT54892.2023.10022338](https://doi.org/10.1109/SLT54892.2023.10022338)。'
- en: '[5] Yoon, Ji, et al. “HuBERT-EE: Early Exiting HuBERT for Efficient Speech
    Recognition.” ArXiv, 2022, [https://doi.org/10.48550/arXiv.2204.06328](https://doi.org/10.48550/arXiv.2204.06328).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Yoon, Ji 等人。“HuBERT-EE：早期退出的HuBERT以实现高效语音识别。” ArXiv，2022，[https://doi.org/10.48550/arXiv.2204.06328](https://doi.org/10.48550/arXiv.2204.06328)。'
- en: '[6] Kanda, Naoyuki, et al. “Transcribe-to-Diarize: Neural Speaker Diarization
    for Unlimited Number of Speakers Using End-to-End Speaker-Attributed ASR.” ArXiv,
    2021, [https://doi.org/10.48550/arXiv.2110.03151](https://doi.org/10.48550/arXiv.2110.03151).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Kanda, Naoyuki 等人。“转录到说话者标记：使用端到端说话者标记ASR进行无限数量的说话者的神经说话者标记。” ArXiv，2021，[https://doi.org/10.48550/arXiv.2110.03151](https://doi.org/10.48550/arXiv.2110.03151)。'
- en: '[7] Kanda, Naoyuki, et al. “Streaming Speaker-Attributed ASR with Token-Level
    Speaker Embeddings.” ArXiv, 2022, [https://doi.org/10.48550/arXiv.2203.16685](https://doi.org/10.48550/arXiv.2203.16685).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Kanda, Naoyuki 等人。“基于令牌级说话者嵌入的流式说话者标记ASR。” ArXiv，2022，[https://doi.org/10.48550/arXiv.2203.16685](https://doi.org/10.48550/arXiv.2203.16685)。'
- en: '[8] [https://www.fiercevideo.com/video/video-will-account-for-82-all-internet-traffic-by-2022-cisco-says](https://www.fiercevideo.com/video/video-will-account-for-82-all-internet-traffic-by-2022-cisco-says)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] [https://www.fiercevideo.com/video/video-will-account-for-82-all-internet-traffic-by-2022-cisco-says](https://www.fiercevideo.com/video/video-will-account-for-82-all-internet-traffic-by-2022-cisco-says)'
- en: '[9] Chiu, Chung, et al. “Self-Supervised Learning with Random-Projection Quantizer
    for Speech Recognition.” ArXiv, 2022, [https://doi.org/10.48550/arXiv.2202.01855](https://doi.org/10.48550/arXiv.2202.01855).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Chiu, Chung, 等. “用于语音识别的随机投影量化器自监督学习。” ArXiv, 2022, [https://doi.org/10.48550/arXiv.2202.01855](https://doi.org/10.48550/arXiv.2202.01855)。'
- en: '[10] Chen, Sanyuan, et al. “WavLM: Large-Scale Self-Supervised Pre-Training
    for Full Stack Speech Processing.” ArXiv, 2021, [https://doi.org/10.1109/JSTSP.2022.3188113](https://doi.org/10.1109/JSTSP.2022.3188113).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Chen, Sanyuan, 等. “WavLM：用于全栈语音处理的大规模自监督预训练。” ArXiv, 2021, [https://doi.org/10.1109/JSTSP.2022.3188113](https://doi.org/10.1109/JSTSP.2022.3188113)。'
- en: '[11] Hsu, Wei, et al. “Robust Wav2vec 2.0: Analyzing Domain Shift in Self-Supervised
    Pre-Training.” ArXiv, 2021, [https://doi.org/10.48550/arXiv.2104.01027](https://doi.org/10.48550/arXiv.2104.01027).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Hsu, Wei, 等. “鲁棒的 Wav2vec 2.0：自监督预训练中的领域偏移分析。” ArXiv, 2021, [https://doi.org/10.48550/arXiv.2104.01027](https://doi.org/10.48550/arXiv.2104.01027)。'
- en: '[12] Lugosch, Loren, et al. “Pseudo-Labeling for Massively Multilingual Speech
    Recognition.” ArXiv, 2021, [https://doi.org/10.48550/arXiv.2111.00161](https://doi.org/10.48550/arXiv.2111.00161).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Lugosch, Loren, 等. “大规模多语言语音识别的伪标签。” ArXiv, 2021, [https://doi.org/10.48550/arXiv.2111.00161](https://doi.org/10.48550/arXiv.2111.00161)。'
- en: '[13] Berrebbi, Dan, et al. “Continuous Pseudo-Labeling from the Start.” ArXiv,
    2022, [https://doi.org/10.48550/arXiv.2210.08711](https://doi.org/10.48550/arXiv.2210.08711).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Berrebbi, Dan, 等. “从开始进行连续伪标签。” ArXiv, 2022, [https://doi.org/10.48550/arXiv.2210.08711](https://doi.org/10.48550/arXiv.2210.08711)。'
- en: '[14] Radford, Alec, et al. “Robust Speech Recognition via Large-Scale Weak
    Supervision.” ArXiv, 2022, [https://doi.org/10.48550/arXiv.2212.04356](https://doi.org/10.48550/arXiv.2212.04356).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Radford, Alec, 等. “通过大规模弱监督进行鲁棒的语音识别。” ArXiv, 2022, [https://doi.org/10.48550/arXiv.2212.04356](https://doi.org/10.48550/arXiv.2212.04356)。'
- en: '[15] Graves, Alex, et al. “Connectionist Temporal Classification: Labelling
    Unsegmented Sequence Data with Recurrent Neural Networks.” ICML, 2016, [https://www.cs.toronto.edu/~graves/icml_2006.pdf](https://www.cs.toronto.edu/~graves/icml_2006.pdf)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Graves, Alex, 等. “连接主义时间分类：使用递归神经网络标记未分段序列数据。” ICML, 2016, [https://www.cs.toronto.edu/~graves/icml_2006.pdf](https://www.cs.toronto.edu/~graves/icml_2006.pdf)。'
- en: '[16] Graves, Alex. “Sequence Transduction with Recurrent Neural Networks.”
    ArXiv, 2012, [https://doi.org/10.48550/arXiv.1211.3711](https://doi.org/10.48550/arXiv.1211.3711).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Graves, Alex. “使用递归神经网络进行序列转导。” ArXiv, 2012, [https://doi.org/10.48550/arXiv.1211.3711](https://doi.org/10.48550/arXiv.1211.3711)。'
- en: '[17] Chan, William, et al. “Listen, Attend and Spell.” ArXiv, 2015, [https://doi.org/10.48550/arXiv.1508.01211](https://doi.org/10.48550/arXiv.1508.01211).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Chan, William, 等. “听、注意和拼写。” ArXiv, 2015, [https://doi.org/10.48550/arXiv.1508.01211](https://doi.org/10.48550/arXiv.1508.01211)。'
- en: '[18] Variani, Ehsan, et al. “Hybrid Autoregressive Transducer (Hat).” ArXiv,
    2020, [https://doi.org/10.48550/arXiv.2003.07705](https://doi.org/10.48550/arXiv.2003.07705).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Variani, Ehsan, 等. “混合自回归传输器（Hat）。” ArXiv, 2020, [https://doi.org/10.48550/arXiv.2003.07705](https://doi.org/10.48550/arXiv.2003.07705)。'
- en: '[19] Meng, Zhong, et al. “Modular Hybrid Autoregressive Transducer.” ArXiv,
    2022, [https://doi.org/10.48550/arXiv.2210.17049](https://doi.org/10.48550/arXiv.2210.17049).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Meng, Zhong, 等. “模块化混合自回归传输器。” ArXiv, 2022, [https://doi.org/10.48550/arXiv.2210.17049](https://doi.org/10.48550/arXiv.2210.17049)。'
- en: '[20] Kim, Suyoun, et al. “Evaluating User Perception of Speech Recognition
    System Quality with Semantic Distance Metric.” ArXiv, 2021, [https://doi.org/10.48550/arXiv.2110.05376](https://doi.org/10.48550/arXiv.2110.05376).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] Kim, Suyoun, 等. “使用语义距离度量评估用户对语音识别系统质量的感知。” ArXiv, 2021, [https://doi.org/10.48550/arXiv.2110.05376](https://doi.org/10.48550/arXiv.2110.05376)。'
- en: '[21] Gulati, Anmol, et al. “Conformer: Convolution-Augmented Transformer for
    Speech Recognition.” ArXiv, 2020, [https://doi.org/10.48550/arXiv.2005.08100](https://doi.org/10.48550/arXiv.2005.08100).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] Gulati, Anmol, 等. “Conformer：用于语音识别的卷积增强变换器。” ArXiv, 2020, [https://doi.org/10.48550/arXiv.2005.08100](https://doi.org/10.48550/arXiv.2005.08100)。'
- en: '[22] Vaswani, Ashish, et al. “Attention Is All You Need.” ArXiv, 2017, [https://doi.org/10.48550/arXiv.1706.03762](https://doi.org/10.48550/arXiv.1706.03762).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] Vaswani, Ashish, 等. “注意力即一切。” ArXiv, 2017, [https://doi.org/10.48550/arXiv.1706.03762](https://doi.org/10.48550/arXiv.1706.03762)。'
- en: '[23] Baevski, Alexei, et al. “Wav2vec 2.0: A Framework for Self-Supervised
    Learning of Speech Representations.” ArXiv, 2020, [https://doi.org/10.48550/arXiv.2006.11477](https://doi.org/10.48550/arXiv.2006.11477).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[23] Baevski, Alexei, 等. “Wav2vec 2.0：自监督语音表示学习框架。” ArXiv, 2020, [https://doi.org/10.48550/arXiv.2006.11477](https://doi.org/10.48550/arXiv.2006.11477)。'
- en: '[24] Babu, Arun, et al. “XLS-R: Self-Supervised Cross-Lingual Speech Representation
    Learning at Scale.” ArXiv, 2021, [https://doi.org/10.48550/arXiv.2111.09296](https://doi.org/10.48550/arXiv.2111.09296).'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[24] Babu, Arun 等. “XLS-R: 大规模自监督跨语言语音表示学习。” ArXiv, 2021, [https://doi.org/10.48550/arXiv.2111.09296](https://doi.org/10.48550/arXiv.2111.09296)。'
- en: '[25] [https://blog.google/technology/ai/ways-ai-is-scaling-helpful/](https://blog.google/technology/ai/ways-ai-is-scaling-helpful/)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[25] [https://blog.google/technology/ai/ways-ai-is-scaling-helpful/](https://blog.google/technology/ai/ways-ai-is-scaling-helpful/)'
- en: '[26] [https://ai.facebook.com/blog/teaching-ai-to-translate-100s-of-spoken-and-written-languages-in-real-time/](https://ai.facebook.com/blog/teaching-ai-to-translate-100s-of-spoken-and-written-languages-in-real-time/)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[26] [https://ai.facebook.com/blog/teaching-ai-to-translate-100s-of-spoken-and-written-languages-in-real-time/](https://ai.facebook.com/blog/teaching-ai-to-translate-100s-of-spoken-and-written-languages-in-real-time/)'
- en: '[27] Ao, Junyi, et al. “SpeechT5: Unified-Modal Encoder-Decoder Pre-Training
    for Spoken Language Processing.” ArXiv, 2021, [https://doi.org/10.48550/arXiv.2110.07205](https://doi.org/10.48550/arXiv.2110.07205).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[27] Ao, Junyi 等. “SpeechT5: 统一模态编码器-解码器预训练用于口语语言处理。” ArXiv, 2021, [https://doi.org/10.48550/arXiv.2110.07205](https://doi.org/10.48550/arXiv.2110.07205)。'
- en: '[28] [https://github.com/guillaumekln/faster-whisper](https://github.com/guillaumekln/faster-whisper)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[28] [https://github.com/guillaumekln/faster-whisper](https://github.com/guillaumekln/faster-whisper)'
- en: '[29] [https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechRecognition/wav2vec2](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechRecognition/wav2vec2)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[29] [https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechRecognition/wav2vec2](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechRecognition/wav2vec2)'
- en: '[30] Meng, Zhong, et al. “JEIT: Joint End-to-End Model and Internal Language
    Model Training for Speech Recognition.” ArXiv, 2023, [https://doi.org/10.48550/arXiv.2302.08583](https://doi.org/10.48550/arXiv.2302.08583).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[30] Meng, Zhong 等. “JEIT: 语音识别的联合端到端模型与内部语言模型训练。” ArXiv, 2023, [https://doi.org/10.48550/arXiv.2302.08583](https://doi.org/10.48550/arXiv.2302.08583)。'
- en: '[31] Zhang, Yu, et al. “Google USM: Scaling Automatic Speech Recognition Beyond
    100 Languages.” ArXiv, 2023, [https://doi.org/10.48550/arXiv.2303.01037](https://doi.org/10.48550/arXiv.2303.01037).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[31] Zhang, Yu 等. “Google USM: 将自动语音识别扩展到 100 多种语言。” ArXiv, 2023, [https://doi.org/10.48550/arXiv.2303.01037](https://doi.org/10.48550/arXiv.2303.01037)。'
- en: '[32] Kendall, T. and Farrington, C. “The corpus of regional african american
    language”. Version 2021.07\. Eugene, OR: The Online Resources for African American
    Language Project. [http://oraal.uoregon.edu/coraal](http://oraal.uoregon.edu/coraal),
    2021'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[32] Kendall, T. 和 Farrington, C. “区域非裔美国语言语料库。” 版本 2021.07\. Eugene, OR: 非裔美国语言在线资源项目。[http://oraal.uoregon.edu/coraal](http://oraal.uoregon.edu/coraal),
    2021'
- en: '[33] Brian, D Tran, et al. ‘“Mm-hm,” “Uh-uh”: are non-lexical conversational
    sounds deal breakers for the ambient clinical documentation technology?,’ Journal
    of the American Medical Informatics Association, 2023, [https://doi.org/10.1093/jamia/ocad001](https://doi.org/10.1093/jamia/ocad001)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[33] Brian, D Tran 等. “‘嗯哼’，‘呃呃’：非词汇性对话声音是否会成为环境临床文档技术的致命缺陷？”，《美国医学信息学协会杂志》，2023,
    [https://doi.org/10.1093/jamia/ocad001](https://doi.org/10.1093/jamia/ocad001)'
- en: '[34] Zhu, Ge, et al. “Filler Word Detection and Classification: A Dataset and
    Benchmark.” ArXiv, 2022, [https://doi.org/10.48550/arXiv.2203.15135](https://doi.org/10.48550/arXiv.2203.15135).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[34] Zhu, Ge 等. “填充词检测与分类：数据集与基准。” ArXiv, 2022, [https://doi.org/10.48550/arXiv.2203.15135](https://doi.org/10.48550/arXiv.2203.15135)。'
- en: '[35] Anwar, Mohamed, et al. “MuAViC: A Multilingual Audio-Visual Corpus for
    Robust Speech Recognition and Robust Speech-to-Text Translation.” ArXiv, 2023,
    [https://doi.org/10.48550/arXiv.2303.00628](https://doi.org/10.48550/arXiv.2303.00628).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[35] Anwar, Mohamed 等. “MuAViC: 多语言音频视觉语料库用于稳健的语音识别和稳健的语音到文本翻译。” ArXiv, 2023,
    [https://doi.org/10.48550/arXiv.2303.00628](https://doi.org/10.48550/arXiv.2303.00628)。'
- en: '[36] Jaegle, Andrew, et al. “Perceiver IO: A General Architecture for Structured
    Inputs & Outputs.” ArXiv, 2021, [https://doi.org/10.48550/arXiv.2107.14795](https://doi.org/10.48550/arXiv.2107.14795).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[36] Jaegle, Andrew 等. “Perceiver IO: 一种用于结构化输入和输出的通用架构。” ArXiv, 2021, [https://doi.org/10.48550/arXiv.2107.14795](https://doi.org/10.48550/arXiv.2107.14795)。'
- en: '[37] [https://www.assemblyai.com/blog/conformer-1/](https://www.assemblyai.com/blog/conformer-1/)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[37] [https://www.assemblyai.com/blog/conformer-1/](https://www.assemblyai.com/blog/conformer-1/)'
- en: '[38] Peyser, Cal, et al. “Dual Learning for Large Vocabulary On-Device ASR.”
    ArXiv, 2023, [https://doi.org/10.48550/arXiv.2301.04327](https://doi.org/10.48550/arXiv.2301.04327).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[38] Peyser, Cal 等. “大词汇量设备端自动语音识别的双重学习。” ArXiv, 2023, [https://doi.org/10.48550/arXiv.2301.04327](https://doi.org/10.48550/arXiv.2301.04327)。'
- en: '[39] Lakhotia, Kushal, et al. “Generative Spoken Language Modeling from Raw
    Audio.” ArXiv, 2021, [https://doi.org/10.48550/arXiv.2102.01192](https://doi.org/10.48550/arXiv.2102.01192).'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[39] Lakhotia, Kushal 等. “从原始音频生成的语言建模。” ArXiv, 2021, [https://doi.org/10.48550/arXiv.2102.01192](https://doi.org/10.48550/arXiv.2102.01192)。'
- en: '[40] Zeghidour, Neil, et al. “SoundStream: An End-to-End Neural Audio Codec.”
    ArXiv, 2021, [https://doi.org/10.48550/arXiv.2107.03312](https://doi.org/10.48550/arXiv.2107.03312).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[40] Zeghidour, Neil 等. “SoundStream: 一种端到端神经音频编解码器。” ArXiv, 2021, [https://doi.org/10.48550/arXiv.2107.03312](https://doi.org/10.48550/arXiv.2107.03312)。'
- en: '[41] Borsos, Zalán, et al. “AudioLM: a Language Modeling Approach to Audio
    Generation.” ArXiv, 2022, [https://doi.org/10.48550/arXiv.2209.03143](https://doi.org/10.48550/arXiv.2209.03143).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[41] Borsos, Zalán 等. “AudioLM: 一种音频生成的语言建模方法。” ArXiv, 2022, [https://doi.org/10.48550/arXiv.2209.03143](https://doi.org/10.48550/arXiv.2209.03143)。'
- en: '[42] [https://about.fb.com/news/2022/10/hokkien-ai-speech-translation/](https://about.fb.com/news/2022/10/hokkien-ai-speech-translation/)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[42] [https://about.fb.com/news/2022/10/hokkien-ai-speech-translation/](https://about.fb.com/news/2022/10/hokkien-ai-speech-translation/)'
- en: '[43] Raffel, Colin, et al. “Exploring the Limits of Transfer Learning with
    a Unified Text-to-Text Transformer.” ArXiv, 2019, /abs/1910.10683.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[43] Raffel, Colin 等. “探索统一文本到文本变换器的迁移学习极限。” ArXiv, 2019, /abs/1910.10683。'
- en: '[44] Sanh, Victor, et al. “Multitask Prompted Training Enables Zero-Shot Task
    Generalization.” ArXiv, 2021, [https://doi.org/10.48550/arXiv.2110.08207](https://doi.org/10.48550/arXiv.2110.08207).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[44] Sanh, Victor 等. “多任务提示训练实现零样本任务泛化。” ArXiv, 2021, [https://doi.org/10.48550/arXiv.2110.08207](https://doi.org/10.48550/arXiv.2110.08207)。'
- en: '[45] Wang, Chengyi, et al. “Neural Codec Language Models Are Zero-Shot Text
    to Speech Synthesizers.” ArXiv, 2023, [https://doi.org/10.48550/arXiv.2301.02111](https://doi.org/10.48550/arXiv.2301.02111).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[45] 王成毅 等. “神经编解码语言模型是零样本文本到语音合成器。” ArXiv, 2023, [https://doi.org/10.48550/arXiv.2301.02111](https://doi.org/10.48550/arXiv.2301.02111)。'
- en: '[46] Zhang, Ziqiang, et al. “Speak Foreign Languages with Your Own Voice: Cross-Lingual
    Neural Codec Language Modeling.” ArXiv, 2023, [https://doi.org/10.48550/arXiv.2303.03926](https://doi.org/10.48550/arXiv.2303.03926).'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[46] 张子强 等. “用你自己的声音说外语：跨语言神经编解码语言建模。” ArXiv, 2023, [https://doi.org/10.48550/arXiv.2303.03926](https://doi.org/10.48550/arXiv.2303.03926)。'
- en: '[47] [https://voicebot.ai/2022/07/05/eu-passes-new-regulations-for-voice-ai-and-digital-technology/](https://voicebot.ai/2022/07/05/eu-passes-new-regulations-for-voice-ai-and-digital-technology/)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[47] [https://voicebot.ai/2022/07/05/eu-passes-new-regulations-for-voice-ai-and-digital-technology/](https://voicebot.ai/2022/07/05/eu-passes-new-regulations-for-voice-ai-and-digital-technology/)'
- en: '[48] [https://www.speechtechmag.com/Articles/ReadArticle.aspx?ArticleID=154094](https://www.speechtechmag.com/Articles/ReadArticle.aspx?ArticleID=154094)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[48] [https://www.speechtechmag.com/Articles/ReadArticle.aspx?ArticleID=154094](https://www.speechtechmag.com/Articles/ReadArticle.aspx?ArticleID=154094)'
- en: '[49] Thoppilan, Romal, et al. “LaMDA: Language Models for Dialog Applications.”
    ArXiv, 2022, [https://doi.org/10.48550/arXiv.2201.08239](https://doi.org/10.48550/arXiv.2201.08239).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[49] Thoppilan, Romal 等. “LaMDA: 对话应用的语言模型。” ArXiv, 2022, [https://doi.org/10.48550/arXiv.2201.08239](https://doi.org/10.48550/arXiv.2201.08239)。'
- en: '[50] Shuster, Kurt, et al. “BlenderBot 3: a Deployed Conversational Agent that
    Continually Learns to Responsibly Engage.” ArXiv, 2022, [https://doi.org/10.48550/arXiv.2208.03188](https://doi.org/10.48550/arXiv.2208.03188).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[50] Shuster, Kurt 等. “BlenderBot 3: 一个持续学习以负责任地互动的部署对话代理。” ArXiv, 2022, [https://doi.org/10.48550/arXiv.2208.03188](https://doi.org/10.48550/arXiv.2208.03188)。'
- en: '[51] Xiaozhou, Zhou, et al. “Phonetic Embedding for ASR Robustness in Entity
    Resolution.” Proc. Interspeech 2022, 3268–3272, doi: 10.21437/Interspeech.2022–10956'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[51] 肖舟 等. “用于实体解析的 ASR 鲁棒性语音嵌入。” Proc. Interspeech 2022, 3268–3272, doi: 10.21437/Interspeech.2022–10956'
- en: '[52] Chen, Angelica, et al. “Teaching BERT to Wait: Balancing Accuracy and
    Latency for Streaming Disfluency Detection.” ArXiv, 2022, [https://doi.org/10.48550/arXiv.2205.00620](https://doi.org/10.48550/arXiv.2205.00620).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[52] 陈安琪 等. “教 BERT 等待：平衡流媒体不流畅检测的准确性和延迟。” ArXiv, 2022, [https://doi.org/10.48550/arXiv.2205.00620](https://doi.org/10.48550/arXiv.2205.00620)。'
- en: '[53] Li, Qiujia, et al. “Improving Confidence Estimation on Out-of-Domain Data
    for End-to-End Speech Recognition.” ArXiv, 2021, [https://doi.org/10.48550/arXiv.2110.03327](https://doi.org/10.48550/arXiv.2110.03327).'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[53] 李秋佳 等. “提高端到端语音识别的领域外数据置信度估计。” ArXiv, 2021, [https://doi.org/10.48550/arXiv.2110.03327](https://doi.org/10.48550/arXiv.2110.03327)。'
- en: '[54] [https://loora.ai/](https://loora.ai/)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[54] [https://loora.ai/](https://loora.ai/)'
- en: '[55] [https://techcrunch.com/2022/11/17/speak-lands-investment-from-openai-to-expand-its-language-learning-platform/](https://techcrunch.com/2022/11/17/speak-lands-investment-from-openai-to-expand-its-language-learning-platform/)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[55] [https://techcrunch.com/2022/11/17/speak-lands-investment-from-openai-to-expand-its-language-learning-platform/](https://techcrunch.com/2022/11/17/speak-lands-investment-from-openai-to-expand-its-language-learning-platform/)'
- en: '[56] Zhiqi, Huang, et al. “MTL-SLT: Multi-Task Learning for Spoken Language
    Tasks.” NLP4ConvAI, 2022, [https://aclanthology.org/2022.nlp4convai-1.11](https://aclanthology.org/2022.nlp4convai-1.11)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[56] Zhiqi, Huang 等. “MTL-SLT: 多任务学习用于口语语言任务。” NLP4ConvAI, 2022, [https://aclanthology.org/2022.nlp4convai-1.11](https://aclanthology.org/2022.nlp4convai-1.11)'
- en: '[57] Watanabe, Shinji, et al. “CHiME-6 Challenge: Tackling Multispeaker Speech
    Recognition for Unsegmented Recordings.” ArXiv, 2020, [https://doi.org/10.48550/arXiv.2004.09249](https://doi.org/10.48550/arXiv.2004.09249).'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[57] Watanabe, Shinji 等. “CHiME-6 挑战：应对未分段录音中的多说话者语音识别。” ArXiv, 2020, [https://doi.org/10.48550/arXiv.2004.09249](https://doi.org/10.48550/arXiv.2004.09249).'
- en: '[58] Shafey, Laurent, et al. “Joint Speech Recognition and Speaker Diarization
    via Sequence Transduction.” ArXiv, 2019, [https://doi.org/10.48550/arXiv.1907.05337](https://doi.org/10.48550/arXiv.1907.05337).'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[58] Shafey, Laurent 等. “通过序列转导联合语音识别和说话者分离。” ArXiv, 2019, [https://doi.org/10.48550/arXiv.1907.05337](https://doi.org/10.48550/arXiv.1907.05337).'
- en: '[59] Kim, Juntae, and Lee, Jeehye. “Generalizing RNN-Transducer to Out-Domain
    Audio via Sparse Self-Attention Layers.” ArXiv, 2021, [https://doi.org/10.48550/arXiv.2108.10752](https://doi.org/10.48550/arXiv.2108.10752).'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[59] Kim, Juntae 和 Lee, Jeehye. “通过稀疏自注意力层将 RNN-转导器推广到域外音频。” ArXiv, 2021, [https://doi.org/10.48550/arXiv.2108.10752](https://doi.org/10.48550/arXiv.2108.10752).'
- en: '[60] Sathyendra, Kanthashree, et al. “Contextual Adapters for Personalized
    Speech Recognition in Neural Transducers.” ArXiv, 2022, [https://doi.org/10.48550/arXiv.2205.13660](https://doi.org/10.48550/arXiv.2205.13660).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[60] Sathyendra, Kanthashree 等. “针对神经转导器的个性化语音识别的上下文适配器。” ArXiv, 2022, [https://doi.org/10.48550/arXiv.2205.13660](https://doi.org/10.48550/arXiv.2205.13660).'
- en: '[61] Tian, Jinchuan, et al. “Bayes Risk CTC: Controllable CTC Alignment in
    Sequence-to-Sequence Tasks.” ArXiv, 2022, [https://doi.org/10.48550/arXiv.2210.07499](https://doi.org/10.48550/arXiv.2210.07499).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[61] Tian, Jinchuan 等. “Bayes 风险 CTC: 在序列到序列任务中可控的 CTC 对齐。” ArXiv, 2022, [https://doi.org/10.48550/arXiv.2210.07499](https://doi.org/10.48550/arXiv.2210.07499).'
- en: '[62] Tian, Zhengkun, et al. “Peak-First CTC: Reducing the Peak Latency of CTC
    Models by Applying Peak-First Regularization.” ArXiv, 2022, [https://doi.org/10.48550/arXiv.2211.03284](https://doi.org/10.48550/arXiv.2211.03284).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[62] Tian, Zhengkun 等. “Peak-First CTC: 通过应用 Peak-First 正则化减少 CTC 模型的峰值延迟。”
    ArXiv, 2022, [https://doi.org/10.48550/arXiv.2211.03284](https://doi.org/10.48550/arXiv.2211.03284).'
- en: '[63] [https://docs.rev.ai/api/custom-vocabulary/](https://docs.rev.ai/api/custom-vocabulary/)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[63] [https://docs.rev.ai/api/custom-vocabulary/](https://docs.rev.ai/api/custom-vocabulary/)'
- en: '[64] [https://cloud.google.com/speech-to-text/docs/adaptation-model](https://cloud.google.com/speech-to-text/docs/adaptation-model)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[64] [https://cloud.google.com/speech-to-text/docs/adaptation-model](https://cloud.google.com/speech-to-text/docs/adaptation-model)'
