# 无监督机器学习：探索一系列无需输出标签的模型

> 原文：[`towardsdatascience.com/unsupervised-machine-learning-explore-a-list-of-models-that-work-without-output-labels-89322c2dd47c?source=collection_archive---------8-----------------------#2023-05-25`](https://towardsdatascience.com/unsupervised-machine-learning-explore-a-list-of-models-that-work-without-output-labels-89322c2dd47c?source=collection_archive---------8-----------------------#2023-05-25)

## 了解各种无监督机器学习模型的基本原理，以及它们如何在没有输出标签的情况下生成预测

[](https://suhas-maddali007.medium.com/?source=post_page-----89322c2dd47c--------------------------------)![Suhas Maddali](https://suhas-maddali007.medium.com/?source=post_page-----89322c2dd47c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----89322c2dd47c--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page-----89322c2dd47c--------------------------------) [Suhas Maddali](https://suhas-maddali007.medium.com/?source=post_page-----89322c2dd47c--------------------------------)

·

[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2a74f90399ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-machine-learning-explore-a-list-of-models-that-work-without-output-labels-89322c2dd47c&user=Suhas+Maddali&userId=2a74f90399ae&source=post_page-2a74f90399ae----89322c2dd47c---------------------post_header-----------) 发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----89322c2dd47c--------------------------------) · 12 分钟阅读 · 2023 年 5 月 25 日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F89322c2dd47c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-machine-learning-explore-a-list-of-models-that-work-without-output-labels-89322c2dd47c&user=Suhas+Maddali&userId=2a74f90399ae&source=-----89322c2dd47c---------------------clap_footer-----------)

--

[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F89322c2dd47c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-machine-learning-explore-a-list-of-models-that-work-without-output-labels-89322c2dd47c&source=-----89322c2dd47c---------------------bookmark_footer-----------)![](img/4c734dd8ce705ae62262520e855084db.png)

照片由 [Jacopo Maia](https://unsplash.com/@ja_ma?utm_source=medium&utm_medium=referral) 提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

在过去的十年中，机器学习和人工智能取得了**巨大的**进展。许多不同行业的公司都创建了更新、更强大的模型。**大型语言模型(LLM)**的引入和实施显著增加了人们对人工智能领域的关注。来自许多行业的公司正准备迎接机器学习和通用智能领域的巨大变革。

尽管这些机器学习方面的重大进展激励人们学习最新的人工智能技术，但往往存在一个问题——这个领域成功应用的高质量、标记数据的**缺乏**。为了让机器学习在监督下运作，对训练数据需要有标签。获得这些数据可能是相当具有挑战性的。因此，获取这些数据可能会很昂贵且费时，因为需要人工进行手动标记。此外，这个过程可能会导致标签不一致的问题。

那么，我们如何利用未标记的数据而不需要手动标记呢？这就是无监督机器学习的用武之地。顾名思义，这种方法认为在无监督机器学习模型的训练过程中可以使用没有输出标签的数据。这些模型会**识别**数据中固有的模式和趋势，并根据一组特定的属性或特征将其分组到不同的类别中。在将它们分组在一起之后，我们可以找出不同组之间的共同点，并利用这些信息将业务引导到正确的方向。

![](img/6ceb06cf1e93dc14a50da6b62ae31d72.png)

照片由[Lucrezia Carnelos](https://unsplash.com/@ciabattespugnose?utm_source=medium&utm_medium=referral)在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)上提供引用。

举例来说，考虑一个希望向各种类型的客户销售产品并提供折扣的商店。然而，他们不确定谁可能会**转换**成他们的服务对象。如果客户对他们的广告没有积极反应，这将导致企业收入的损失。在这种情况下，可以使用无监督机器学习来执行客户细分，将客户分组成不同的类别。因此，公司可以调整其广告策略并针对特定的群体进行定向，确保成功转化购买产品。这只是无监督机器学习在商业上的简单应用。然而，在异常检测、文本挖掘、图像识别、降维和欺诈检测等领域，还有无数其他的应用。

既然已经看到无监督机器学习的有用性，现在是时候更深入地探索这些模型及其在不同场景中的潜在用例。需要注意的是，使用这些模型时总是有优缺点的，最佳选择在很大程度上取决于公司的具体用例和限制。

## K-means 聚类

K-means 聚类是一种流行的无监督机器学习模型，用于将一组项分组到不同的类别中。在此模型中，K 是一个超参数，必须根据领域知识或通过标准技术（如肘部法则或轮廓分析）来选择。下面是通常用来将数据中的元素分组到 k 个簇中的步骤列表。

1.  随机初始化一组数据点作为质心，基于 k 的值。例如，如果 k 的值是 10，则会随机选择 10 个数据点并将其标记为质心。

1.  在此步骤之后，会计算每个数据点与这些质心之间的距离。距离最小的点会被分配到相应的质心。距离度量可以是欧几里得距离、余弦距离或曼哈顿距离。

1.  这样，所有的数据点都属于在步骤 1 中随机分配的质心中的一个。

1.  为了从每个簇中确定一个新的质心，需要计算每个簇的均值，并将其指定为新的质心。这个步骤会对所有形成的簇执行。

1.  在为每个簇找到新的质心集后，步骤 2 到步骤 4 会重复进行，直到收敛。换句话说，寻找最佳簇的过程会停止，因为上一轮迭代的质心与当前迭代的质心之间没有太大差异。

尽管这种方法直观且具有很好的潜力，但它也有一些缺点，我们将在下文中讨论这些缺点以及优点。

## **优点**

1.  K-means 聚类因其可扩展性和处理大数据集的能力而闻名。

1.  该算法的可解释性也很好，因为将数据点分配到簇的过程是直观的。

## 缺点

1.  该算法对初始选择的质心集非常敏感。在每次运行时，我们可能会得到不同的初始质心集，这些初始质心会影响最终的聚类结果。

1.  如果数据中存在一个或少数几个离群点，它们会对该算法的最终聚类结构产生影响。

1.  最优的簇数量应该由程序员确定，而不是由算法自动寻找。这有时可能会耗时较长。

## 层次聚类

使用 k-means 聚类方法的一个缺点是，在生成聚类时需要事先确定聚类的总数。然而，这种方法往往耗时，并且导致结果不一致。然而，在层次聚类方法中，最佳的聚类数目可以在没有人工干预的情况下确定。我们将讨论层次聚类的两个子类别。

**凝聚层次聚类：** 这种方法专注于以自下而上的方式逐个构建聚类。最初，每个数据点被假定为一个单独的聚类。之后，将彼此最接近的聚类合并成一个合并的聚类。此步骤会一直进行，直到我们能够将所有数据点合并为一个最终的单一聚类。该方法可以帮助我们确定聚类形成的层次结构以及它们之间的相似性。树状图显示了这种树状结构，展示了所有聚类的形成方式。

**分裂聚类：** 这种方法遵循自上而下的方法，将所有数据点分配到一个单一的聚类中。我们根据数据点之间的距离将其分割成聚类。这些步骤一直进行，直到我们得到**n**个聚类，其中**n**表示选择用于聚类的数据点总数。n 的值较大将导致更多的聚类和更高的计算成本。通过树状图可以更容易地可视化结果，该图也显示了用于划分聚类的相似性。

## 优点

1.  使用层次聚类方法时不需要指定聚类的数量。

1.  这种方法更加直观，因为结构中有一个层次，解释起来更容易。

1.  多次运行层次聚类方法会产生确定性的结果，而不会改变聚类模式。

## 缺点

1.  运行此算法的时间复杂度相当高。如果我们有一个包含数百万条记录的大型数据集，寻找替代算法可能更好。

1.  这种算法对数据中的异常值也很敏感，这可能会影响聚类的形成或生成方式。

1.  用于拆分或合并聚类的距离度量选择可能会对最终的聚类结果产生重大影响。

## 基于密度的空间聚类算法（DBSCAN）

这是一种无监督的机器学习方法，通过相对密度来确定聚类。以下是 DBSCAN 方法中遵循的步骤列表。

1.  **选择初始参数：** 选择一个尚未访问的任意起始数据点。设置你的参数：`eps`（epsilon）是两个样本之间被认为在同一邻域中的最大距离，`minPts`是邻域内样本的最小数量，以使数据点符合核心点的条件。

1.  **确定核心点：** 对于你的起始点，计算在`eps`半径内的数据点数量以确定其密度。如果在此半径内至少有`minPts`，则将起始数据点标记为核心点；否则，标记为噪声（这可能会在以后更新）。

1.  **扩展簇：** 对于每个核心点，如果它尚未被分配到簇中，则创建一个新簇。找到距离核心点`eps`范围内的所有点（包括核心点本身）并将它们分配到同一个簇。如果这些点在距离`eps`范围内也有`minPts`，它们也是核心点，因此对这些点重复该过程。

1.  **分配边界点：** 如果一个数据点在多个簇的`eps`距离内，则将其分配给第一个遇到的核心点所在的簇。

1.  **迭代：** 继续这个过程，直到所有点都被访问、分配到簇中或标记为噪声。这可能需要回到之前标记为噪声的点，检查它们是否在新找到的核心点的`eps`半径内。

1.  **结束：** 当所有点都被标记为核心点、边界点或噪声，并且已被适当地分配到簇中时，算法停止。

## 优点

1.  不需要像 k-means 聚类方法那样指定簇的数量。

1.  它对数据中的异常值更加鲁棒，因为它基于核心点和噪声点的概念。

1.  它能够处理比上述讨论的算法更加任意形状的簇。

## 缺点

1.  根据数据集的维度，需要仔细调整`minPoints`和`epsilon`参数，这可能比较麻烦。

1.  它不能用于对新数据样本进行预测，而只能对训练期间使用的数据点进行聚类。

1.  运行 DBSCAN 模型多次可能不会总是产生相似的结果，因为它是非确定性的。

## 高斯混合模型（GMMs）

该方法基于期望最大化原理，将数据点根据其概率得分放入簇中。此外，这种方法假设数据点服从正态分布。为此，首先需要初始化均值、方差和混合权重。在此步骤之后，我们按照期望最大化方法确定数据点的簇。以下是使用高斯混合模型时采取的详细步骤解释。

1.  第一步是初始化均值、方差和混合权重的集合。这可以随机完成，也可以借助 k-means 聚类方法完成。

1.  在这一步之后，我们确定每个数据点属于这些 k 个簇中的任何一个的后验概率。这也称为期望阶段。

1.  一旦确定了每个簇中的数据点，其组合均值、协方差和混合权重会被重新计算，以最大化期望。

1.  步骤 2 和步骤 3 会重复进行，直到与期望最大化方法确定的簇相比，数据点在其他簇中的变化不再发生。

1.  最终，我们得到的数据点被聚类在一起，假设每个数据点都符合正态分布。

## 优点

1.  使用这种方法的一个优点是，它能够处理比 k-means 聚类方法更为复杂的椭圆形或球形簇。

1.  这也导致了软聚类，这非常方便，因为我们可以知道模型在将数据点分配到每个簇时的置信度分数。

1.  与其他模型如 k-means 相比，它们具有更高的灵活性，因为它们可以处理不同形状和大小的簇。

## 缺点

1.  算法对最初选择的均值和协方差高度敏感。因此，较差的初始化可能会影响模型的性能。

1.  确定混合模型的最佳数量可能很困难，通常通过试错法完成。

1.  如果我们有一个大型数据集，由于这个原因，计算复杂度更高，模型可能会慢速收敛。

## 自编码器

这些模型能够将输入表示编码为较低维度的表示。它们不需要输出标签，而是只需要特征形式的输入数据。一旦生成了低维数据，也会用于解码，以获得原始数据。通过这种方式，我们能够使用编码版本重建原始数据。注意，解码输出与原始输入之间可能存在差异。这种错误也称为重建损失，在训练过程中进行优化。下面是自编码器工作原理的详细描述。

1.  通过使用数据中的输入特征，进行编码以获得低维表示。通常，这种选择是为了更好地表示数据。

1.  在这一步之后，编码后的信号被送入解码器，解码器能够重建原始输入。在此过程中，信号可能无法被完美重建。

1.  因此，我们使用具有重建损失的优化。通过这种优化，调整权重使得重建的数据输出与输入数据紧密匹配。

1.  更新每个权重时损失会减少。因此，我们得到的权重包含了将给定输入集转换为低维表示所需的信息，从而便于计算。

## 优点

1.  与其他方法相比，它们更适合用于确定数据中存在的异常。

1.  它们可以用来减少输入数据的维度，从而提高计算速度。

1.  它们比其他模型更能有效地减少输入数据中的噪声。

## 缺点

1.  当输入数据非常庞大时，运行多次训练来构建编码表示可能会非常耗费计算资源。

1.  解释编码表示（隐藏表示）的含义可能会很困难，因为它们不直观。

## 主成分分析（PCA）

这是一种统计技术，它依赖于特征之间的协方差来确定低维空间。此外，该算法不像奇异值分解（SVD）那样需要输出训练标签来确定最佳特征。在这种方式下，我们得到的最终向量可以通过特征值和特征向量表示数据集的大部分方差。以下是执行主成分分析（PCA）的步骤列表。

1.  在应用主成分分析（PCA）之前的初始步骤是对数据进行标准化。PCA 对数据的尺度非常敏感。由于机器学习的特性，我们通常输入的数据尺度各异。因此，在将这些信息提供给模型之前，对数据进行标准化是很重要的。

1.  我们找到所有特征相互之间的协方差。通过这种方式，我们可以很好地理解数据中基于其他特征集的变化程度。

1.  在执行这一步后，我们使用一组公式和方程来计算特征值和特征向量。为了做到这一点，我们应有一个突出重要信息的协方差矩阵。

1.  在确定特征值和特征向量后，我们应按降序排列特征值，以确定每个特征相对于其他特征解释的方差。根据我们希望 PCA 分解的组件数量，我们将设置组件数量的值。

1.  我们将对原始数据与特征向量进行矩阵乘法，以获得转换后的特征。这表示减少后的数据集，这样的训练和预测可能计算上更为高效。

## 优点

1.  使用这种方法可以实现维度减少，这对于数据中特征数量非常多的场景是有用的。

1.  它还可以帮助以能够在编码单元中可视化这些特征的方式减少维度。

1.  当特征数量大于数据集的规模时，这可以减少过拟合。

## 缺点

1.  由于主成分分析，解释存在的组件是困难的。原始特征更容易解释，因为它们代表了现实世界的数据，而主成分不能作为特征来解释，而是作为变换来理解。

1.  对于缩放非常敏感，数据的不同尺度会极大地影响 PCA 的性能。

1.  如果数据中存在离群值，它的表现可能不如其他无监督机器学习模型。

## 结论

探索无监督机器学习模型是一项引人入胜的工作。每个模型，如主成分分析，都具有独特的优点和局限性，这影响了我们解决问题的方法。关键在于根据数据和目标的具体情况来选择合适的工具。

通过深入了解这些模型，你可以为自己制定有效的解决方案并在数据科学领域产生有意义的影响。尽管面临挑战，但回报是显著的——将数据转化为有价值的洞察力并解决复杂问题的能力。

因此，继续学习、质疑和创新。每一步进步都有助于塑造技术的未来。感谢你将时间和好奇心投入到这一重要领域。机器学习的领域在等待你的贡献。

*以下是你可以联系我或查看我工作的方式。*

***GitHub：***[*suhasmaddali (Suhas Maddali ) (github.com)*](https://github.com/suhasmaddali)

***YouTube：***[*https://www.youtube.com/channel/UCymdyoyJBC_i7QVfbrIs-4Q*](https://www.youtube.com/channel/UCymdyoyJBC_i7QVfbrIs-4Q)

***领英：***[*(1) Suhas Maddali, Northeastern University, Data Science | LinkedIn*](https://www.linkedin.com/in/suhas-maddali/)

***中等：*** [*Suhas Maddali — Medium*](https://suhas-maddali007.medium.com/)

***Kaggle：***[*Suhas Maddali | Contributor | Kaggle*](https://www.kaggle.com/suhasmaddali007)
