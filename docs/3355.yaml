- en: Create your Vision Chat Assistant with LLaVA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/create-your-vision-chat-assistant-with-llava-610b02c3283e?source=collection_archive---------3-----------------------#2023-11-11](https://towardsdatascience.com/create-your-vision-chat-assistant-with-llava-610b02c3283e?source=collection_archive---------3-----------------------#2023-11-11)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Get started with multimodal conversational models using the open-source LLaVA
    model.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@gabrielesgroi94?source=post_page-----610b02c3283e--------------------------------)[![Gabriele
    Sgroi, PhD](../Images/b81978d35e6238d160457de2affc2b0e.png)](https://medium.com/@gabrielesgroi94?source=post_page-----610b02c3283e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----610b02c3283e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----610b02c3283e--------------------------------)
    [Gabriele Sgroi, PhD](https://medium.com/@gabrielesgroi94?source=post_page-----610b02c3283e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F97ea0c34751b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-your-vision-chat-assistant-with-llava-610b02c3283e&user=Gabriele+Sgroi%2C+PhD&userId=97ea0c34751b&source=post_page-97ea0c34751b----610b02c3283e---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----610b02c3283e--------------------------------)
    ·17 min read·Nov 11, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F610b02c3283e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-your-vision-chat-assistant-with-llava-610b02c3283e&user=Gabriele+Sgroi%2C+PhD&userId=97ea0c34751b&source=-----610b02c3283e---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F610b02c3283e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-your-vision-chat-assistant-with-llava-610b02c3283e&source=-----610b02c3283e---------------------bookmark_footer-----------)![](../Images/a2fcdb6fb47b3c5be1cdf24d8bbb67b4.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Izabela Kraus](https://unsplash.com/@izabelakraus?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Large Language Models have proved themselves to be a revolutionary technology.
    Numerous applications exploiting their capabilities have been already developed
    and many more are expected to come soon. One of the most interesting applications
    of Large Language Models is their deployment as intelligent assistants able to
    help human users in a variety of tasks. Chat models trained with instruction tuning
    and Reinforcement Learning from Human Feedback (RLHF) have shown very promising
    capabilities of following human instructions and carrying out the assigned tasks.
    However, they are limited in their applicability to language-only tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal conversational models aim to unleash the power of Large Language
    Models to tackle problems that require combining natural language with other modalities
    to be solved. In particular, vision-language models have received increasing attention
    since the introduction of vision capabilities to GPT-4V. Empowering the natural
    language capabilities of GPT-4 with image understanding has led to a powerful
    chat assistant that can help users with tasks requiring both vision and language
    understanding. While the vision capabilities of GPT-4V are impressive, closed-source
    models limit the potential for research and experimentation with this amazing
    technology. Fortunately, some open-source models appeared bringing the power of
    vision language models to the community in an easily accessible and transparent
    way. These models also continue the trend of increased focus on computing and
    memory efficiency, a trend already seen for open-source Large Language Models.
    This is an important feature because it facilitates the widespread adoption of
    these models.
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, I will walk through the process of creating a vision chat
    assistant using the LLaVA (Large Language and Vision Assistant) model introduced
    in the [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) paper. I
    will first give a brief introduction to the LLaVA model and its improvements before
    discussing a simple code implementation of a vision chat assistant using the code
    provided in the [official repository](https://github.com/haotian-liu/LLaVA). I
    will then present some examples I crafted to showcase the capabilities and limitations
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: LLaVA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The LLaVA model was introduced in the paper [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485),
    and then further improved in [Improved Baselines with Visual Instruction Tuning](https://arxiv.org/abs/2310.03744)
    (also referred to as LLaVA-1.5). The idea behind it is to extract visual embeddings
    from an image and treat them in the same way as embeddings coming from language
    tokens by feeding them to a Large Language Model. Intuitively, we can think that
    the image will be described with “words” that the language model will use to generate
    its answer. To choose the right “words” the model uses a pre-trained CLIP visual
    encoder to extract the visual embeddings and then projects them into the word
    embedding space of the language model. The latter operation is accomplished with
    a vision-language connector, which was originally chosen to be a simple linear
    layer in the first paper [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485),
    and later replaced with a more expressive Multilayer Perceptron (MLP) in [Improved
    Baselines with Visual Instruction](https://arxiv.org/abs/2310.03744). The architecture
    of the model is depicted below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5bd728a3d4d590a808f4a3982572fa4d.png)'
  prefs: []
  type: TYPE_IMG
- en: Architecture of the LLaVA model. The projection W is a simple linear layer in
    LLaVA or an MLP in LLaVA-1.5\. Image from the paper [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485).
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the advantages of the method is that by using a pre-trained vision encoder
    and a pre-trained language model, only the vision-language connector (which is
    a lightweight module) must be learned from scratch. In particular, the training
    of LLava consists of two stages:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pre-training for feature alignment: both the pre-trained vision encoder and
    language model are frozen, and only the weights of the vision-language connector
    are updated. All training samples consist of text-image pairs packed into a single-turn
    conversation. This stage aims to train the vision-language connector to align
    the embeddings of the vision encoder with the text embeddings of the language
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fine-tuning with visual instructions: in this stage, only the weights of the
    vision encoder are frozen while the vision-language connector and the language
    model are fine-tuned together. The model is fine-tuned on image-based instruction-following
    tasks. It is interesting to notice that some of this data has been created by
    using language-only GPT4 to create instruction-following samples from the caption
    of the images and the coordinates of the bounding boxes of the entities depicted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vision Chatbot Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Creating a vision chatbot using the code provided in the [official repository](https://github.com/haotian-liu/LLaVA)
    is fairly easy. The repository also provides standardized chat templates that
    can be used to parse the inputs in the right format. Following the right format
    used in training is essential for the quality of the answer generated by the model.
    The exact template depends on the language model used. The template for LLaVA-1.5
    with a pre-trained Vicuna language model will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The first few lines are the general system prompt used by the model. The special
    tokens <im_start>, <image>, and <im_end> are used to indicate where embeddings
    representing the image will be placed.
  prefs: []
  type: TYPE_NORMAL
- en: The chatbot can be defined in just one simple Python class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If you are familiar with the [transformers](https://github.com/huggingface/transformers)
    library, you will recognize many of the usual features, and the operations performed
    should be straightforward to understand. Let’s go quickly over the methods of
    the LLaVAChatBot class defined above.
  prefs: []
  type: TYPE_NORMAL
- en: 'load_models: this method loads the language models, the tokenizer, and the
    image processor with the specified parameters for quantization using the BitsAndBytes
    library. The code shadows the from_pretrained method used by Hugging Face transformers
    models. BitsAndBytes allows quantizing to model to 8bit or 4bit for reduced GPU
    memory requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'setup_image: it loads the image from a local path or a URL and converts it
    to a tensor using the image processor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'generate_answer: this method returns the model''s answer continuing the current
    conversation about the provided image. Again the generate method of the LLaVa
    model is analogous to the generate method of Hugging Face transformers models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'get_conv_text: this method returns the raw text of the conversation so far.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'start_new_chat: this is one of the two main methods of the chatbot, it is used
    to start a new chat with the model. It creates a new conversation given the image
    and the initial prompt to pass to the model. It takes care of setting up the conversation
    using the templates defined in the repository following the format discussed in
    the previous section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'continue_chat: the other main method, it continues an existing conversation
    about an image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I have provided the full code in [this Colab notebook](https://colab.research.google.com/drive/1tq9K3utBJ4VeMmuUarMEFCmaekMUG_Zd?usp=sharing)
    so that you can try it yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, I collect some interesting examples obtained while testing
    the capabilities of the model. All the examples in this post were created using
    the model llava-v1.5–7b with 8-bit quantization.
  prefs: []
  type: TYPE_NORMAL
- en: For a start, let’s look at the model's capabilities to describe and understand
    images. Below I asked the model to describe an image picturing a white tiger.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/240edd0c191583c4a31c4cab91338c91.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Joshua J. Cotten](https://unsplash.com/@jcotten?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The model seems to be able to describe the image in detail and also to reason
    about the subject of the image, correctly assessing that a white tiger is a rare
    occurrence.
  prefs: []
  type: TYPE_NORMAL
- en: Next, I tested if the model is able to provide the user with actionable instructions
    to perform various tasks. This is what distinguishes a “captioning” model from
    a true vision-chat assistant. For example, can the model provide recipes from
    the ingredients present in an image? The result is reported below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5cca84ad35e4a7587b31ef017dff4cbd.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [ThermoPro](https://unsplash.com/@thermopro?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The model seems to have a good understanding of the ingredients present in the
    image and it is also capable of connecting the individual words to more abstract
    concepts (i.e. a meal with meat is not vegetarian).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now see how the model can be used to get pieces of advice. This is a useful
    feature to get new ideas when brainstorming and can provide real value to the
    user. In the conversation below, I asked the model for suggestions to change the
    look of a café.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cf321a55ad85606eab4203e9fd846305.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Michał Mancewicz](https://unsplash.com/@kreyatif?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The advice looks on point, although quite generic, identifying the current elements
    that could be changed to give a more vintage style.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s continue with something practical that can aid the users in their daily
    activities. Can the model help with writing advertisements or listings?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/58fbea207cd49b2ab4ee9a211bf1af02.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Eugene Chystiakov](https://unsplash.com/@eugenechystiakov?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The model is able to pinpoint the features of the chair correctly and creates
    an appealing advertisement, correctly customizing the answer to different targets
    when asked to.
  prefs: []
  type: TYPE_NORMAL
- en: As a last test, let us see if prompting techniques that have been demonstrated
    to be beneficial for Large Language Models can be used with LLaVA as well. Below
    is an example of attempting zero-shot Chain of Thought prompting.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cb0ed3882d16fa029375b9b6d6413222.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Arnold Middelkoop](https://unsplash.com/@arnoldmid?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The model is able to explain why the fish would not float, correctly identifying
    that it is not a real fish. It also follows the instruction to think step by step
    before giving the final answer. For comparison, here is an example of the conversation
    without the “Think step by step” prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: While the final answer is still correct, the explanation given in this case
    is much less detailed and the answer is given at the start of the sentence. Since
    the generative language model used by LLaVA is causal, this means that the provided
    answer does not rely on the subsequent explanation. It would be interesting to
    carry out more extensive experiments to test if Chain of Thought (CoT) can, in
    general, improve vision-language models' performance in complex reasoning tasks
    similar to what has been observed for Large Language Models in [Chain-of-Thought
    Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)
    and [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916).
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While very powerful, LLaVA comes with its limitations. For example, the model
    has been trained to use only one image per chat so it isn’t able to handle more
    complex conversations that need interactions with multiple images. It is worth
    noticing, however, that this limitation is not intrinsic to the architecture of
    the model and can be amended by including multi-image chat data during training.
  prefs: []
  type: TYPE_NORMAL
- en: During the tests, I noticed that the model (at least the 8bit quantized version
    of the 1.5 7b model) has trouble with counting elements in an image and in general
    with handling numerical tasks. Below is an example.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/da758143f4be167c44018ab5e580cb7a.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [John Matychuk](https://unsplash.com/@john_matychuk?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The model, like all Large Language Models, is also prone to hallucinations and
    can be easily tricked into making mistakes using appropriate prompts. For example,
    if provided with a picture of the desert and asked what species is the fish in
    the image, it hallucinates a desert fish.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/afc885c1294f8b11eca34827ea9e8eaa.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Wolfgang Hasselmann](https://unsplash.com/@wolfgang_hasselmann?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LLaVA shows impressive capabilities in vision-language understanding. It marks
    a clear step forward for multimodal open-source vision-language models. One of
    the biggest advantages of LLaVA is that it is lightweight to train and fine-tune.
    For instance, the full training of LLaVA 1.5 13b took only 1.2M data and roughly
    1 day on a single 8-A100 node. This makes it suitable for fine-tuning on specific
    domains to get an expert assistant, as was done, for example, in [LLaVA-Med: Training
    a Large Language-and-Vision Assistant for Biomedicine in One Day](https://arxiv.org/abs/2306.00890).'
  prefs: []
  type: TYPE_NORMAL
- en: Adding vision capabilities to chat assistants expands the area of applications
    of such models, bringing their revolutionizing potential to more complex and nuanced
    tasks. Treating image features as language tokens also brings up the possibility
    of using all the advanced prompting techniques used with text-only language models
    and further expands them. For example, one could expand the power of Retrieval
    Augmented Generation by retrieving both texts and images that are relevant to
    the conversation. In fact, using the shared image-text embedding space of CLIP
    it is possible to retrieve both external documents and external images starting
    with either an input text or picture!
  prefs: []
  type: TYPE_NORMAL
- en: 'Another interesting direction to expand the capabilities of the model is presented
    in [LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation
    and Editing](https://arxiv.org/abs/2311.00571). The main idea is to combine the
    various capabilities of vision-language chat models, text-to-image generative
    models, and other vision models (such as image segmentation models) to get an
    assistant capable of handling multimodal inputs and generating multimodal outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, LLaVA marked an important step for open-source multimodal generative
    models, which have shown impressive capabilities and are attracting a lot of interest.
    With the more widespread adoption of open-source models, I believe we will soon
    witness a rapid increase in new applications of these powerful models.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading! If you want to try out the code yourself you can look
    at [this Colab notebook](https://colab.research.google.com/drive/1tq9K3utBJ4VeMmuUarMEFCmaekMUG_Zd?usp=sharing).
  prefs: []
  type: TYPE_NORMAL
