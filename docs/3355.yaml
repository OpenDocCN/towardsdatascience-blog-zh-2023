- en: Create your Vision Chat Assistant with LLaVA
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LLaVA创建你的视觉聊天助手
- en: 原文：[https://towardsdatascience.com/create-your-vision-chat-assistant-with-llava-610b02c3283e?source=collection_archive---------3-----------------------#2023-11-11](https://towardsdatascience.com/create-your-vision-chat-assistant-with-llava-610b02c3283e?source=collection_archive---------3-----------------------#2023-11-11)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/create-your-vision-chat-assistant-with-llava-610b02c3283e?source=collection_archive---------3-----------------------#2023-11-11](https://towardsdatascience.com/create-your-vision-chat-assistant-with-llava-610b02c3283e?source=collection_archive---------3-----------------------#2023-11-11)
- en: Get started with multimodal conversational models using the open-source LLaVA
    model.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开始使用开源的LLaVA模型来创建多模态对话模型。
- en: '[](https://medium.com/@gabrielesgroi94?source=post_page-----610b02c3283e--------------------------------)[![Gabriele
    Sgroi, PhD](../Images/b81978d35e6238d160457de2affc2b0e.png)](https://medium.com/@gabrielesgroi94?source=post_page-----610b02c3283e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----610b02c3283e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----610b02c3283e--------------------------------)
    [Gabriele Sgroi, PhD](https://medium.com/@gabrielesgroi94?source=post_page-----610b02c3283e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@gabrielesgroi94?source=post_page-----610b02c3283e--------------------------------)[![Gabriele
    Sgroi, PhD](../Images/b81978d35e6238d160457de2affc2b0e.png)](https://medium.com/@gabrielesgroi94?source=post_page-----610b02c3283e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----610b02c3283e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----610b02c3283e--------------------------------)
    [Gabriele Sgroi, PhD](https://medium.com/@gabrielesgroi94?source=post_page-----610b02c3283e--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F97ea0c34751b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-your-vision-chat-assistant-with-llava-610b02c3283e&user=Gabriele+Sgroi%2C+PhD&userId=97ea0c34751b&source=post_page-97ea0c34751b----610b02c3283e---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----610b02c3283e--------------------------------)
    ·17 min read·Nov 11, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F610b02c3283e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-your-vision-chat-assistant-with-llava-610b02c3283e&user=Gabriele+Sgroi%2C+PhD&userId=97ea0c34751b&source=-----610b02c3283e---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F97ea0c34751b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-your-vision-chat-assistant-with-llava-610b02c3283e&user=Gabriele+Sgroi%2C+PhD&userId=97ea0c34751b&source=post_page-97ea0c34751b----610b02c3283e---------------------post_header-----------)
    发布在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----610b02c3283e--------------------------------)
    ·17分钟阅读·2023年11月11日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F610b02c3283e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-your-vision-chat-assistant-with-llava-610b02c3283e&user=Gabriele+Sgroi%2C+PhD&userId=97ea0c34751b&source=-----610b02c3283e---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F610b02c3283e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-your-vision-chat-assistant-with-llava-610b02c3283e&source=-----610b02c3283e---------------------bookmark_footer-----------)![](../Images/a2fcdb6fb47b3c5be1cdf24d8bbb67b4.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F610b02c3283e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-your-vision-chat-assistant-with-llava-610b02c3283e&source=-----610b02c3283e---------------------bookmark_footer-----------)![](../Images/a2fcdb6fb47b3c5be1cdf24d8bbb67b4.png)'
- en: Photo by [Izabela Kraus](https://unsplash.com/@izabelakraus?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Izabela Kraus](https://unsplash.com/@izabelakraus?utm_source=medium&utm_medium=referral)在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)上拍摄。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: Large Language Models have proved themselves to be a revolutionary technology.
    Numerous applications exploiting their capabilities have been already developed
    and many more are expected to come soon. One of the most interesting applications
    of Large Language Models is their deployment as intelligent assistants able to
    help human users in a variety of tasks. Chat models trained with instruction tuning
    and Reinforcement Learning from Human Feedback (RLHF) have shown very promising
    capabilities of following human instructions and carrying out the assigned tasks.
    However, they are limited in their applicability to language-only tasks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型已经证明自己是一项革命性的技术。利用其能力的众多应用已经被开发出来，预计还会有更多的应用出现。大语言模型的一个最有趣的应用是将其作为智能助手，能够在各种任务中帮助用户。经过指令调优和从人类反馈中学习的聊天模型显示出很有前景的能力，能够遵循人类指令并完成指定任务。然而，它们在仅语言任务的适用性上存在局限。
- en: Multimodal conversational models aim to unleash the power of Large Language
    Models to tackle problems that require combining natural language with other modalities
    to be solved. In particular, vision-language models have received increasing attention
    since the introduction of vision capabilities to GPT-4V. Empowering the natural
    language capabilities of GPT-4 with image understanding has led to a powerful
    chat assistant that can help users with tasks requiring both vision and language
    understanding. While the vision capabilities of GPT-4V are impressive, closed-source
    models limit the potential for research and experimentation with this amazing
    technology. Fortunately, some open-source models appeared bringing the power of
    vision language models to the community in an easily accessible and transparent
    way. These models also continue the trend of increased focus on computing and
    memory efficiency, a trend already seen for open-source Large Language Models.
    This is an important feature because it facilitates the widespread adoption of
    these models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态对话模型旨在释放大语言模型的潜力，以解决需要将自然语言与其他模态结合的问题。特别是，自从将视觉能力引入GPT-4V以来，视觉语言模型受到了越来越多的关注。赋予GPT-4的自然语言能力以图像理解，已导致一个强大的聊天助手，可以帮助用户处理需要视觉和语言理解的任务。尽管GPT-4V的视觉能力令人印象深刻，但闭源模型限制了对这一令人惊叹的技术进行研究和实验的潜力。幸运的是，一些开源模型出现了，将视觉语言模型的力量以易于访问和透明的方式带给了社区。这些模型还延续了对计算和内存效率的关注趋势，这一趋势在开源大语言模型中已经显现。这是一个重要的特性，因为它促进了这些模型的广泛应用。
- en: In this tutorial, I will walk through the process of creating a vision chat
    assistant using the LLaVA (Large Language and Vision Assistant) model introduced
    in the [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) paper. I
    will first give a brief introduction to the LLaVA model and its improvements before
    discussing a simple code implementation of a vision chat assistant using the code
    provided in the [official repository](https://github.com/haotian-liu/LLaVA). I
    will then present some examples I crafted to showcase the capabilities and limitations
    of the model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我将介绍如何使用在[Visual Instruction Tuning](https://arxiv.org/abs/2304.08485)论文中介绍的LLaVA（大语言与视觉助手）模型创建一个视觉聊天助手。我将首先简要介绍LLaVA模型及其改进，然后讨论使用[官方代码库](https://github.com/haotian-liu/LLaVA)中提供的代码实现一个简单的视觉聊天助手。我还将展示一些我设计的示例，以展示该模型的能力和局限性。
- en: LLaVA
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLaVA
- en: The LLaVA model was introduced in the paper [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485),
    and then further improved in [Improved Baselines with Visual Instruction Tuning](https://arxiv.org/abs/2310.03744)
    (also referred to as LLaVA-1.5). The idea behind it is to extract visual embeddings
    from an image and treat them in the same way as embeddings coming from language
    tokens by feeding them to a Large Language Model. Intuitively, we can think that
    the image will be described with “words” that the language model will use to generate
    its answer. To choose the right “words” the model uses a pre-trained CLIP visual
    encoder to extract the visual embeddings and then projects them into the word
    embedding space of the language model. The latter operation is accomplished with
    a vision-language connector, which was originally chosen to be a simple linear
    layer in the first paper [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485),
    and later replaced with a more expressive Multilayer Perceptron (MLP) in [Improved
    Baselines with Visual Instruction](https://arxiv.org/abs/2310.03744). The architecture
    of the model is depicted below.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: LLaVA模型首次在论文[视觉指令调整](https://arxiv.org/abs/2304.08485)中提出，随后在[改进的基线与视觉指令调整](https://arxiv.org/abs/2310.03744)中进一步改进（也称为LLaVA-1.5）。其背后的理念是从图像中提取视觉嵌入，并像处理语言标记生成答案一样处理它们，通过将它们馈送给大型语言模型。直观地说，我们可以认为图像将用语言模型生成答案所需的“单词”。为了选择正确的“单词”，模型使用预训练的CLIP视觉编码器提取视觉嵌入，然后将其投影到语言模型的单词嵌入空间中。后者的操作是通过一个视觉语言连接器完成的，最初在第一篇论文[视觉指令调整](https://arxiv.org/abs/2304.08485)中选择为简单的线性层，后来在[改进的基线与视觉指令](https://arxiv.org/abs/2310.03744)中替换为更具表现力的多层感知器（MLP）。模型的架构如下所示。
- en: '![](../Images/5bd728a3d4d590a808f4a3982572fa4d.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5bd728a3d4d590a808f4a3982572fa4d.png)'
- en: Architecture of the LLaVA model. The projection W is a simple linear layer in
    LLaVA or an MLP in LLaVA-1.5\. Image from the paper [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: LLaVA模型的架构。在LLaVA中，投影W是一个简单的线性层，而在LLaVA-1.5中是一个MLP。图片来源于论文[视觉指令调整](https://arxiv.org/abs/2304.08485)。
- en: 'One of the advantages of the method is that by using a pre-trained vision encoder
    and a pre-trained language model, only the vision-language connector (which is
    a lightweight module) must be learned from scratch. In particular, the training
    of LLava consists of two stages:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法的优势之一在于通过使用预训练的视觉编码器和预训练语言模型，只需学习轻量级模块——视觉语言连接器，从头开始。具体而言，LLaVA的训练包括两个阶段：
- en: 'Pre-training for feature alignment: both the pre-trained vision encoder and
    language model are frozen, and only the weights of the vision-language connector
    are updated. All training samples consist of text-image pairs packed into a single-turn
    conversation. This stage aims to train the vision-language connector to align
    the embeddings of the vision encoder with the text embeddings of the language
    model.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征对齐的预训练：冻结预训练的视觉编码器和语言模型的权重，仅更新视觉语言连接器的权重。所有训练样本由文本-图像对打包成单回合对话。此阶段旨在训练视觉语言连接器将视觉编码器的嵌入与语言模型的文本嵌入对齐。
- en: 'Fine-tuning with visual instructions: in this stage, only the weights of the
    vision encoder are frozen while the vision-language connector and the language
    model are fine-tuned together. The model is fine-tuned on image-based instruction-following
    tasks. It is interesting to notice that some of this data has been created by
    using language-only GPT4 to create instruction-following samples from the caption
    of the images and the coordinates of the bounding boxes of the entities depicted.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用视觉指令进行微调：在此阶段，仅冻结视觉编码器的权重，同时对视觉语言连接器和语言模型进行微调。模型在基于图像的指令跟随任务上进行微调。有趣的是，一些数据是通过使用仅包含语言的GPT4从图像的标题和实体边界框的坐标创建指令跟随样本。
- en: Vision Chatbot Implementation
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视觉聊天机器人实现
- en: 'Creating a vision chatbot using the code provided in the [official repository](https://github.com/haotian-liu/LLaVA)
    is fairly easy. The repository also provides standardized chat templates that
    can be used to parse the inputs in the right format. Following the right format
    used in training is essential for the quality of the answer generated by the model.
    The exact template depends on the language model used. The template for LLaVA-1.5
    with a pre-trained Vicuna language model will look like this:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[官方存储库](https://github.com/haotian-liu/LLaVA)中提供的代码创建视觉聊天机器人非常简单。该存储库还提供了标准化的聊天模板，可用于解析正确格式的输入。遵循训练中使用的正确格式对生成的答案质量至关重要。确切的模板取决于使用的语言模型。使用预训练的Vicuna语言模型的LLaVA-1.5的模板如下：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The first few lines are the general system prompt used by the model. The special
    tokens <im_start>, <image>, and <im_end> are used to indicate where embeddings
    representing the image will be placed.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 前几行是模型使用的一般系统提示。特殊标记<im_start>、<image>和<im_end>用于指示将放置表示图像的嵌入的位置。
- en: The chatbot can be defined in just one simple Python class.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天机器人可以在一个简单的Python类中定义。
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you are familiar with the [transformers](https://github.com/huggingface/transformers)
    library, you will recognize many of the usual features, and the operations performed
    should be straightforward to understand. Let’s go quickly over the methods of
    the LLaVAChatBot class defined above.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您熟悉[transformers](https://github.com/huggingface/transformers)库，您将会认识到许多常见功能，并且执行的操作应该很容易理解。让我们快速浏览上面定义的LLaVAChatBot类的方法。
- en: 'load_models: this method loads the language models, the tokenizer, and the
    image processor with the specified parameters for quantization using the BitsAndBytes
    library. The code shadows the from_pretrained method used by Hugging Face transformers
    models. BitsAndBytes allows quantizing to model to 8bit or 4bit for reduced GPU
    memory requirements.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'load_models: 该方法加载语言模型、分词器和图像处理器，使用BitsAndBytes库进行量化，以减少GPU内存需求。该代码阴影了Hugging
    Face transformers模型使用的from_pretrained方法。BitsAndBytes允许将模型量化为8位或4位。'
- en: 'setup_image: it loads the image from a local path or a URL and converts it
    to a tensor using the image processor.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'setup_image: 它从本地路径或URL加载图像，并使用图像处理器将其转换为张量。'
- en: 'generate_answer: this method returns the model''s answer continuing the current
    conversation about the provided image. Again the generate method of the LLaVa
    model is analogous to the generate method of Hugging Face transformers models.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'generate_answer: 该方法返回模型关于所提供图像当前对话的答案。LLaVa模型的generate方法类似于Hugging Face transformers模型的generate方法。'
- en: 'get_conv_text: this method returns the raw text of the conversation so far.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'get_conv_text: 该方法返回到目前为止对话的原始文本。'
- en: 'start_new_chat: this is one of the two main methods of the chatbot, it is used
    to start a new chat with the model. It creates a new conversation given the image
    and the initial prompt to pass to the model. It takes care of setting up the conversation
    using the templates defined in the repository following the format discussed in
    the previous section.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'start_new_chat: 这是聊天机器人的两个主要方法之一，用于与模型开始新的聊天。它创建一个新的对话，给定图像和初始提示传递给模型。它通过使用存储库中定义的模板设置对话，遵循前面部分讨论的格式。'
- en: 'continue_chat: the other main method, it continues an existing conversation
    about an image.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'continue_chat: 另一个主要方法，它继续关于图像的现有对话。'
- en: I have provided the full code in [this Colab notebook](https://colab.research.google.com/drive/1tq9K3utBJ4VeMmuUarMEFCmaekMUG_Zd?usp=sharing)
    so that you can try it yourself.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我在[此Colab笔记本](https://colab.research.google.com/drive/1tq9K3utBJ4VeMmuUarMEFCmaekMUG_Zd?usp=sharing)中提供了完整的代码，这样你就可以自己尝试。
- en: Examples
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例
- en: In this section, I collect some interesting examples obtained while testing
    the capabilities of the model. All the examples in this post were created using
    the model llava-v1.5–7b with 8-bit quantization.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我收集了在测试模型能力时获得的一些有趣示例。本文中的所有示例都是使用8位量化的模型llava-v1.5–7b创建的。
- en: For a start, let’s look at the model's capabilities to describe and understand
    images. Below I asked the model to describe an image picturing a white tiger.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看该模型描述和理解图像的能力。下面我要求模型描述一张描绘白老虎的图片。
- en: '![](../Images/240edd0c191583c4a31c4cab91338c91.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/240edd0c191583c4a31c4cab91338c91.png)'
- en: Photo by [Joshua J. Cotten](https://unsplash.com/@jcotten?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Joshua J. Cotten](https://unsplash.com/@jcotten?utm_source=medium&utm_medium=referral)提供，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)。
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The model seems to be able to describe the image in detail and also to reason
    about the subject of the image, correctly assessing that a white tiger is a rare
    occurrence.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 模型似乎能够详细描述图像，并且能够推理图像的主题，正确评估白老虎是一种罕见的情况。
- en: Next, I tested if the model is able to provide the user with actionable instructions
    to perform various tasks. This is what distinguishes a “captioning” model from
    a true vision-chat assistant. For example, can the model provide recipes from
    the ingredients present in an image? The result is reported below.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我测试了模型是否能够为用户提供执行各种任务的可操作指导。这是区分“字幕生成”模型和真正的视觉对话助手的关键点。例如，模型能否根据图像中存在的成分提供食谱？结果如下报告。
- en: '![](../Images/5cca84ad35e4a7587b31ef017dff4cbd.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5cca84ad35e4a7587b31ef017dff4cbd.png)'
- en: Photo by [ThermoPro](https://unsplash.com/@thermopro?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[ThermoPro](https://unsplash.com/@thermopro?utm_source=medium&utm_medium=referral)提供，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)。
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The model seems to have a good understanding of the ingredients present in the
    image and it is also capable of connecting the individual words to more abstract
    concepts (i.e. a meal with meat is not vegetarian).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 模型似乎对图像中存在的成分有很好的理解，并且能够将单个词语连接到更抽象的概念（即肉类餐品不是素食）。
- en: Let’s now see how the model can be used to get pieces of advice. This is a useful
    feature to get new ideas when brainstorming and can provide real value to the
    user. In the conversation below, I asked the model for suggestions to change the
    look of a café.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看模型如何用于获得建议。这是在头脑风暴时获取新想法的有用功能，并且可以为用户提供真正的价值。在下面的对话中，我请求模型提供关于改变咖啡馆外观的建议。
- en: '![](../Images/cf321a55ad85606eab4203e9fd846305.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf321a55ad85606eab4203e9fd846305.png)'
- en: Photo by [Michał Mancewicz](https://unsplash.com/@kreyatif?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Michał Mancewicz](https://unsplash.com/@kreyatif?utm_source=medium&utm_medium=referral)提供，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)。
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The advice looks on point, although quite generic, identifying the current elements
    that could be changed to give a more vintage style.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 建议看起来切合要点，尽管相当通用，识别出可以更改的当前元素，以赋予更复古的风格。
- en: Let’s continue with something practical that can aid the users in their daily
    activities. Can the model help with writing advertisements or listings?
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续实际的内容，可以帮助用户在日常活动中获得帮助。模型是否能够帮助撰写广告或列表？
- en: '![](../Images/58fbea207cd49b2ab4ee9a211bf1af02.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/58fbea207cd49b2ab4ee9a211bf1af02.png)'
- en: Photo by [Eugene Chystiakov](https://unsplash.com/@eugenechystiakov?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Eugene Chystiakov](https://unsplash.com/@eugenechystiakov?utm_source=medium&utm_medium=referral)提供，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)。
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The model is able to pinpoint the features of the chair correctly and creates
    an appealing advertisement, correctly customizing the answer to different targets
    when asked to.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 模型能够准确指出椅子的特征，并且能够创建吸引人的广告，当要求时正确地定制答案以适应不同的目标。
- en: As a last test, let us see if prompting techniques that have been demonstrated
    to be beneficial for Large Language Models can be used with LLaVA as well. Below
    is an example of attempting zero-shot Chain of Thought prompting.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后的测试，让我们看看是否可以像大型语言模型那样使用已被证明对LLaVA有益的提示技术。以下是尝试零点启发的链式思维提示的示例。
- en: '![](../Images/cb0ed3882d16fa029375b9b6d6413222.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb0ed3882d16fa029375b9b6d6413222.png)'
- en: Photo by [Arnold Middelkoop](https://unsplash.com/@arnoldmid?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Arnold Middelkoop](https://unsplash.com/@arnoldmid?utm_source=medium&utm_medium=referral)提供，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)。
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The model is able to explain why the fish would not float, correctly identifying
    that it is not a real fish. It also follows the instruction to think step by step
    before giving the final answer. For comparison, here is an example of the conversation
    without the “Think step by step” prompt.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型能够解释为什么鱼不会浮起，正确地识别出它不是真正的鱼。它还遵循了在给出最终答案之前逐步思考的指示。作为比较，这是一个没有“逐步思考”提示的对话示例。
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: While the final answer is still correct, the explanation given in this case
    is much less detailed and the answer is given at the start of the sentence. Since
    the generative language model used by LLaVA is causal, this means that the provided
    answer does not rely on the subsequent explanation. It would be interesting to
    carry out more extensive experiments to test if Chain of Thought (CoT) can, in
    general, improve vision-language models' performance in complex reasoning tasks
    similar to what has been observed for Large Language Models in [Chain-of-Thought
    Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)
    and [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然最终答案仍然是正确的，但在这种情况下给出的解释要少得多，并且答案是在句子开头给出的。由于LLaVA使用的生成语言模型是因果的，这意味着提供的答案不依赖于后续的解释。进行更广泛的实验将很有趣，以测试Chain
    of Thought（CoT）是否通常能够改善视觉语言模型在复杂推理任务中的表现，类似于观察到的大型语言模型在[Chain-of-Thought Prompting
    Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)和[Large
    Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)中所述的情况。
- en: Limitations
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 限制
- en: While very powerful, LLaVA comes with its limitations. For example, the model
    has been trained to use only one image per chat so it isn’t able to handle more
    complex conversations that need interactions with multiple images. It is worth
    noticing, however, that this limitation is not intrinsic to the architecture of
    the model and can be amended by including multi-image chat data during training.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管非常强大，LLaVA也有其局限性。例如，该模型已经训练成每次只使用一张图片，因此无法处理需要与多张图片交互的更复杂对话。然而，值得注意的是，这种限制并非模型架构固有的，可以通过在训练期间包含多图片对话数据来修正。
- en: During the tests, I noticed that the model (at least the 8bit quantized version
    of the 1.5 7b model) has trouble with counting elements in an image and in general
    with handling numerical tasks. Below is an example.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试中，我注意到该模型（至少是1.5 7b模型的8位量化版本）在计算图像中的元素数量以及处理数值任务方面存在困难。以下是一个例子。
- en: '![](../Images/da758143f4be167c44018ab5e580cb7a.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da758143f4be167c44018ab5e580cb7a.png)'
- en: Photo by [John Matychuk](https://unsplash.com/@john_matychuk?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[John Matychuk](https://unsplash.com/@john_matychuk?utm_source=medium&utm_medium=referral)拍摄，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The model, like all Large Language Models, is also prone to hallucinations and
    can be easily tricked into making mistakes using appropriate prompts. For example,
    if provided with a picture of the desert and asked what species is the fish in
    the image, it hallucinates a desert fish.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 像所有大型语言模型一样，该模型也容易产生幻觉，并且可以通过适当的提示轻易地被欺骗而犯错。例如，如果提供了沙漠的图片，并问这张图片中的鱼是哪种物种，它会产生沙漠鱼的幻觉。
- en: '![](../Images/afc885c1294f8b11eca34827ea9e8eaa.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/afc885c1294f8b11eca34827ea9e8eaa.png)'
- en: Photo by [Wolfgang Hasselmann](https://unsplash.com/@wolfgang_hasselmann?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Wolfgang Hasselmann](https://unsplash.com/@wolfgang_hasselmann?utm_source=medium&utm_medium=referral)拍摄，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Conclusion
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'LLaVA shows impressive capabilities in vision-language understanding. It marks
    a clear step forward for multimodal open-source vision-language models. One of
    the biggest advantages of LLaVA is that it is lightweight to train and fine-tune.
    For instance, the full training of LLaVA 1.5 13b took only 1.2M data and roughly
    1 day on a single 8-A100 node. This makes it suitable for fine-tuning on specific
    domains to get an expert assistant, as was done, for example, in [LLaVA-Med: Training
    a Large Language-and-Vision Assistant for Biomedicine in One Day](https://arxiv.org/abs/2306.00890).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLaVA在视觉语言理解方面显示出令人印象深刻的能力。这标志着多模式开源视觉语言模型的明显进步。LLaVA最大的优势之一是它易于训练和微调。例如，LLaVA
    1.5 13b的完整训练仅使用了1.2M数据，在单个8-A100节点上大约耗时1天。这使其适合在特定领域进行微调，以获得专家助理，例如在[LLaVA-Med:
    Training a Large Language-and-Vision Assistant for Biomedicine in One Day](https://arxiv.org/abs/2306.00890)中所做的那样。'
- en: Adding vision capabilities to chat assistants expands the area of applications
    of such models, bringing their revolutionizing potential to more complex and nuanced
    tasks. Treating image features as language tokens also brings up the possibility
    of using all the advanced prompting techniques used with text-only language models
    and further expands them. For example, one could expand the power of Retrieval
    Augmented Generation by retrieving both texts and images that are relevant to
    the conversation. In fact, using the shared image-text embedding space of CLIP
    it is possible to retrieve both external documents and external images starting
    with either an input text or picture!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为聊天助手添加视觉能力扩展了此类模型的应用领域，使其革命性潜力能够应用于更复杂和更细致的任务。将图像特征视为语言标记也带来了使用所有先进提示技术的可能性，这些技术通常用于纯文本语言模型，并进一步扩展了这些技术。例如，可以通过检索与对话相关的文本和图像来扩展检索增强生成的能力。实际上，利用CLIP的共享图像文本嵌入空间，可以通过输入文本或图片来检索外部文档和外部图像！
- en: 'Another interesting direction to expand the capabilities of the model is presented
    in [LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation
    and Editing](https://arxiv.org/abs/2311.00571). The main idea is to combine the
    various capabilities of vision-language chat models, text-to-image generative
    models, and other vision models (such as image segmentation models) to get an
    assistant capable of handling multimodal inputs and generating multimodal outputs.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '扩展模型能力的另一个有趣方向可以在[LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation,
    Generation and Editing](https://arxiv.org/abs/2311.00571)中找到。主要思路是结合视觉语言聊天模型、文本生成图像模型以及其他视觉模型（如图像分割模型）的各种能力，以获得一个能够处理多模态输入并生成多模态输出的助手。'
- en: In conclusion, LLaVA marked an important step for open-source multimodal generative
    models, which have shown impressive capabilities and are attracting a lot of interest.
    With the more widespread adoption of open-source models, I believe we will soon
    witness a rapid increase in new applications of these powerful models.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，LLaVA为开源多模态生成模型标志着一个重要的步骤，这些模型展现了令人印象深刻的能力，并引起了广泛关注。随着开源模型的更广泛采用，我相信我们很快将见证这些强大模型的新应用的迅速增长。
- en: Thank you for reading! If you want to try out the code yourself you can look
    at [this Colab notebook](https://colab.research.google.com/drive/1tq9K3utBJ4VeMmuUarMEFCmaekMUG_Zd?usp=sharing).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！如果你想亲自尝试代码，可以查看[这个 Colab 笔记本](https://colab.research.google.com/drive/1tq9K3utBJ4VeMmuUarMEFCmaekMUG_Zd?usp=sharing)。
