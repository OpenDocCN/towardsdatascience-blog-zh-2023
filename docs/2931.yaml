- en: 'Now You See Me (CME): Concept-based Model Extraction'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '现在你看到我 (CME): 基于概念的模型提取'
- en: 原文：[https://towardsdatascience.com/now-you-see-me-cme-concept-based-model-extraction-97231105f8fa?source=collection_archive---------5-----------------------#2023-09-22](https://towardsdatascience.com/now-you-see-me-cme-concept-based-model-extraction-97231105f8fa?source=collection_archive---------5-----------------------#2023-09-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/now-you-see-me-cme-concept-based-model-extraction-97231105f8fa?source=collection_archive---------5-----------------------#2023-09-22](https://towardsdatascience.com/now-you-see-me-cme-concept-based-model-extraction-97231105f8fa?source=collection_archive---------5-----------------------#2023-09-22)
- en: A label-efficient approach to Concept-based Models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一种标签高效的基于概念的模型方法
- en: '[](https://medium.com/@kazhdan.dmitry?source=post_page-----97231105f8fa--------------------------------)[![Dmitry
    Kazhdan](../Images/6243af19a00a52c7732349442af36e3b.png)](https://medium.com/@kazhdan.dmitry?source=post_page-----97231105f8fa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----97231105f8fa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----97231105f8fa--------------------------------)
    [Dmitry Kazhdan](https://medium.com/@kazhdan.dmitry?source=post_page-----97231105f8fa--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@kazhdan.dmitry?source=post_page-----97231105f8fa--------------------------------)[![Dmitry
    Kazhdan](../Images/6243af19a00a52c7732349442af36e3b.png)](https://medium.com/@kazhdan.dmitry?source=post_page-----97231105f8fa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----97231105f8fa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----97231105f8fa--------------------------------)
    [Dmitry Kazhdan](https://medium.com/@kazhdan.dmitry?source=post_page-----97231105f8fa--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe322093479&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnow-you-see-me-cme-concept-based-model-extraction-97231105f8fa&user=Dmitry+Kazhdan&userId=e322093479&source=post_page-e322093479----97231105f8fa---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----97231105f8fa--------------------------------)
    ·6 min read·Sep 22, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F97231105f8fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnow-you-see-me-cme-concept-based-model-extraction-97231105f8fa&user=Dmitry+Kazhdan&userId=e322093479&source=-----97231105f8fa---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe322093479&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnow-you-see-me-cme-concept-based-model-extraction-97231105f8fa&user=Dmitry+Kazhdan&userId=e322093479&source=post_page-e322093479----97231105f8fa---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----97231105f8fa--------------------------------)
    ·6分钟阅读·2023年9月22日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F97231105f8fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnow-you-see-me-cme-concept-based-model-extraction-97231105f8fa&user=Dmitry+Kazhdan&userId=e322093479&source=-----97231105f8fa---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F97231105f8fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnow-you-see-me-cme-concept-based-model-extraction-97231105f8fa&source=-----97231105f8fa---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F97231105f8fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnow-you-see-me-cme-concept-based-model-extraction-97231105f8fa&source=-----97231105f8fa---------------------bookmark_footer-----------)'
- en: 'From the AIMLAI workshop paper presented at the CIKM conference: “[Now You
    See Me (CME): Concept-based Model Extraction](https://arxiv.org/abs/2010.13233)”
    ([GitHub](https://github.com/dmitrykazhdan/concept-based-xai))'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '来自CIKM会议上展示的AIMLAI研讨会论文：“[Now You See Me (CME): 基于概念的模型提取](https://arxiv.org/abs/2010.13233)”
    ([GitHub](https://github.com/dmitrykazhdan/concept-based-xai))'
- en: '![](../Images/e4cdb7ef047c2d8286f26dc76bd181b1.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4cdb7ef047c2d8286f26dc76bd181b1.png)'
- en: Visual abstract. Image by the author.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉摘要。图片由作者提供。
- en: '**TL;DR**'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**总结** '
- en: '**Problem —** Deep Neural Network models are black boxes, which cannot be interpreted
    directly. As a result — it is difficult to build trust in such models. Existing
    methods, such as Concept Bottleneck Models, make such models more interpretable,
    but require a high annotation cost for annotating underlying concepts'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 —** 深度神经网络模型是黑箱，无法直接解释。因此 — 很难建立对这些模型的信任。现有方法，如概念瓶颈模型，能够使这些模型更具可解释性，但需要高昂的标注成本来标注基础概念。'
- en: '**Key Innovation —** A method for generating Concept-based Models in a *weakly-supervised
    fashion*, requiring vastly fewer annotations as a result'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键创新 —** 一种以 *弱监督方式* 生成基于概念的模型的方法，从而显著减少注释需求'
- en: '**Solution —** Our [Concept-based Model Extraction (CME) framework](https://arxiv.org/abs/2010.13233),
    capable of extracting Concept-based Models from pre-trained vanilla Convolutional
    Neural Networks (CNNs) in a *semi-supervised* fashion, whilst preserving end-task
    performance'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案 —** 我们的 [基于概念的模型提取（CME）框架](https://arxiv.org/abs/2010.13233)，能够以 *半监督*
    方式从预训练的原始卷积神经网络（CNN）中提取基于概念的模型，同时保持最终任务性能。'
- en: '![](../Images/be26db1273cd5e11276d419dbecd5917.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be26db1273cd5e11276d419dbecd5917.png)'
- en: Vanilla CNN End-to-End input processing. Image by the author.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 原始 CNN 的端到端输入处理。作者提供的图像。
- en: '![](../Images/d1dee7d5aa792dfb8ba814fdae4760c4.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1dee7d5aa792dfb8ba814fdae4760c4.png)'
- en: Two-stage Concept-based Model processing. Image by the author.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 两阶段概念模型处理。作者提供的图像。
- en: '**Concept Bottleneck Models (CBMs)**'
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**概念瓶颈模型（CBMs）**'
- en: 'In recent years, the realm of Explainable Artificial Intelligence (XAI) [1]
    has witnessed a surging interest in Concept Bottleneck Model (CBM) approaches
    [2]. These methods introduce an innovative model architecture, in which input
    images are processed in two distinct phases: *concept encoding* and *concept processing*.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，解释性人工智能（XAI）[1] 领域对概念瓶颈模型（CBM）方法 [2] 的兴趣激增。这些方法引入了一种创新的模型架构，其中输入图像分为两个不同的阶段处理：*概念编码*
    和 *概念处理*。
- en: 'During concept encoding, concept information is extracted from the high-dimensional
    input data. Subsequently, in the concept processing phase, this extracted concept
    information is used to generate the desired output task label. A salient feature
    of CBMs is their reliance on a semantically-meaningful *concept representation*,
    serving as an intermediate, interpretable representation for downstream task predictions,
    as shown below:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在概念编码过程中，概念信息从高维输入数据中提取。随后，在概念处理阶段，提取的概念信息用于生成所需的输出任务标签。CBMs 的一个显著特点是它们依赖于具有语义意义的
    *概念表示*，作为下游任务预测的中间、可解释的表示，如下所示：
- en: '![](../Images/60c35c44941241ebca1adb934e5a9035.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/60c35c44941241ebca1adb934e5a9035.png)'
- en: Concept Bottleneck Model Processing. Image by the author.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 概念瓶颈模型处理。作者提供的图像。
- en: As shown above, CBM models are trained with a combination of *task loss* for
    ensuring accurate task label prediction, as well as *concept loss*, ensuring accurate
    intermediate concept prediction. Importantly, CBMs enhance model transparency,
    since the underlying concept representation provides a way to explain and better-understand
    underlying model behaviour.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，CBM 模型通过结合 *任务损失* 确保准确的任务标签预测，以及 *概念损失* 确保准确的中间概念预测进行训练。重要的是，CBMs 增强了模型的透明度，因为底层概念表示提供了一种解释和更好理解模型行为的方法。
- en: Concept Bottleneck Models offer a novel type of CNNs interpretable-by-design,
    allowing users to encode existing domain knowledge into models via concepts.
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 概念瓶颈模型提供了一种新型的设计可解释的 CNN，允许用户通过概念将现有领域知识编码到模型中。
- en: Overall, CBMs serve as an important innovation, bringing us closer to more transparent
    and trustworthy models.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，CBMs 是一项重要的创新，使我们更接近于更透明和可信的模型。
- en: '**Challenge: CBMs have a high concept annotation cost**'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**挑战：CBMs 具有高概念注释成本**'
- en: Unfortunately, CBMs require a high amount of concept annotations during training.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，CBMs 在训练期间需要大量的概念注释。
- en: At present, CBM approaches require *all* training samples to be annotated explicitly
    with *both* end-task, and concept annotations. Hence, for a dataset with *N* samples
    and *C* concepts, the annotation cost rises from *N* annotations (one task label
    per sample), to *N*(C+1)* annotations (one task label per sample, and one concept
    label for every concept). In practice, this can quickly get unwieldy, particularly
    for datasets with a large amount of concepts and training samples.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，CBM 方法要求对 *所有* 训练样本进行显式注释，*同时* 包括最终任务和概念注释。因此，对于一个包含 *N* 个样本和 *C* 个概念的数据集，注释成本从
    *N* 个注释（每个样本一个任务标签），增加到 *N*(C+1)* 个注释（每个样本一个任务标签，且每个概念一个概念标签）。在实践中，这可能迅速变得难以管理，特别是对于具有大量概念和训练样本的数据集。
- en: For example, for a dataset of 10,000 images with 50 concepts, the annotation
    cost will increase by 50*10,000=500,000 labels, i.e. by *half a million* extra
    annotations.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于一个包含 10,000 张图片和 50 个概念的数据集，注释成本将增加 50*10,000=500,000 个标签，即增加 *半百万* 个额外注释。
- en: Unfortunately, Concept Bottleneck Models require a substantial amount of concept
    annotations for training.
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 不幸的是，概念瓶颈模型需要大量的概念标注进行训练。
- en: '**Leveraging Semi-Supervised Concept-based Models with CME**'
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**利用CME的半监督概念模型**'
- en: CME relies on a similar observation highlighted in [3], where it was observed
    that vanilla CNN models often retain a high amount of information pertaining to
    concepts in their *hidden space,* which may be used for concept information mining
    at no extra annotation cost. Importantly, this work considered the scenario where
    the underlying concepts are *unknown*, and had to be extracted from a model’s
    hidden space in an unsupervised fashion.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: CME 依赖于 [3] 中强调的类似观察，其中观察到原始 CNN 模型通常在其 *隐藏空间* 中保留大量有关概念的信息，这可以用于无额外标注成本的概念信息挖掘。重要的是，这项工作考虑了基础概念
    *未知* 的场景，并且必须以无监督的方式从模型的隐藏空间中提取。
- en: 'With CME, we make use of the above observation, and consider a scenario where
    we *have* knowledge of the underlying concepts, but we only have a small amount
    of sample annotations for each these concepts. Similarly to [3], CME relies on
    a given pre-trained vanilla CNN and the small amount of concept annotations in
    order to extract further concept annotations in a *semi-supervised fashion,* as
    shown below:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 CME，我们利用上述观察，考虑一个场景，在该场景中，我们 *已经* 了解基础概念，但每个概念只有少量样本标注。类似于 [3]，CME 依赖于给定的预训练原始
    CNN 和少量概念标注，以 *半监督的方式* 提取进一步的概念标注，如下所示：
- en: '![](../Images/d4e9376dc4b8e9c8fbb35d2646206727.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d4e9376dc4b8e9c8fbb35d2646206727.png)'
- en: CME model processing. Image by the author.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: CME 模型处理。图片来源于作者。
- en: As shown above, CME extracts the concept representation using a pre-trained
    model’s hidden space in a *post-hoc* fashion. Further details are given below.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，CME 使用预训练模型的隐藏空间以 *事后* 方式提取概念表示。详细信息见下文。
- en: '***Concept Encoder Training****:* instead of training concept encoders from
    scratch on the raw data, as done in case of CBMs, we setup our concept encoder
    model training in a *semi-supervised fashion*, using the vanilla CNN’s hidden
    space:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '***概念编码器训练***：与 CBMs 处理原始数据的概念编码器从零开始训练不同，我们以 *半监督的方式* 设置概念编码器模型训练，使用原始 CNN
    的隐藏空间：'
- en: We begin by pre-specifying a set of layers L from the vanilla CNN to use for
    concept extraction. This can range from *all* layers, to just the last few, depending
    on available compute capacity.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们首先预先指定一组层 L，从原始 CNN 中用于概念提取。这可以是 *所有* 层，也可以只是最后几层，具体取决于可用的计算能力。
- en: Next, for each concept, we train a separate model on top of the hidden space
    of *each* layer in L to predict that concept’s values from the layer’s hidden
    space
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，对于每个概念，我们在 L 中 *每个* 层的隐藏空间上训练一个独立的模型，以预测该概念的值。
- en: We proceed to selecting the model and corresponding layer with the best model
    accuracy as the “best” model and layer for predicting that concept.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们继续选择具有最佳模型准确度的模型和相应层作为“最佳”模型和层来预测该概念。
- en: Consequently, when making concept predictions for a concept *i*, we first retrieve
    the hidden space representation of the best layer for that concept, and then pass
    it through the corresponding predictive model for inference.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，在为概念 *i* 做出预测时，我们首先检索该概念的最佳层的隐藏空间表示，然后将其通过相应的预测模型进行推断。
- en: 'Overall, the *concept encoder* function can be summarised as follows (assuming
    there are *k* concepts in total):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，*概念编码器* 功能可以总结如下（假设总共有 *k* 个概念）：
- en: '![](../Images/8d407a7a7dbd5271ff98dc23b120582d.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8d407a7a7dbd5271ff98dc23b120582d.png)'
- en: CME Concept Encoder equation. Image by the author.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: CME 概念编码器方程。图片来源于作者。
- en: Here, p-hat on the LHS represents the concept encoder function
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里，LHS 上的 p-hat 代表概念编码器函数。
- en: The ***g***ᵢ terms represent the hidden-space-to-concept models trained on top
    of the different layer hidden spaces, with *i* representing the concept index,
    ranging from 1 to *k*. In practice, these models can be fairly simple, such as
    Linear Regressors, or Gradient Boosted Classifiers
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***g***ᵢ 项代表在不同层隐藏空间上训练的隐藏空间到概念模型，*i* 代表概念索引，范围从 1 到 *k*。在实际应用中，这些模型可以非常简单，例如线性回归器或梯度提升分类器。'
- en: The *f(***x***)* terms represent the sub-models of the original vanilla CNN,
    extracting the input’s hidden representation at a particular layer
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*f(***x***)* 项代表原始原始 CNN 的子模型，提取输入在特定层的隐藏表示。'
- en: In both cases above, *lʲ* superscripts specify the “best” layers these two models
    are operating on
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在以上两种情况下，*lʲ* 上标指定了这两种模型操作的“最佳”层
- en: '***Concept Processor Training****:* concept processor model training in CME
    is setup by training models using task labels as outputs, and *concept encoder*
    predictions as inputs. Importantly, these models are operating on a much more
    compact input representation, and can consequently be represented directly via
    *interpretable* models, such as Decision Trees (DTs), or Logistic Regression (LR)
    models.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '***概念处理器训练***：CME 中的概念处理器模型训练是通过使用任务标签作为输出、*概念编码器* 预测作为输入来设置的。重要的是，这些模型操作在更紧凑的输入表示上，因此可以通过*可解释的*模型（如决策树（DTs）或逻辑回归（LR）模型）直接表示。'
- en: '**CME Experiments & Results**'
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**CME 实验与结果**'
- en: 'Our experiments on both synthetic ([dSprites](https://github.com/google-deepmind/dsprites-dataset)
    and [shapes3d](https://github.com/google-deepmind/3d-shapes)) and challenging
    real-world datasets ([CUB](https://paperswithcode.com/dataset/cub-200-2011)) demonstrated
    that CME models:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在合成数据集（[dSprites](https://github.com/google-deepmind/dsprites-dataset) 和 [shapes3d](https://github.com/google-deepmind/3d-shapes)）以及具有挑战性的真实数据集（[CUB](https://paperswithcode.com/dataset/cub-200-2011)）上的实验表明，CME
    模型：
- en: '**Achieve high concept predictive accuracy** comparable to that of CBMs in
    many cases, *even on concepts irrelevant to the end-task:*'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实现高概念预测准确度**，在许多情况下可与 CBM 相媲美，*即使在与最终任务无关的概念上：*'
- en: '![](../Images/fc498f365fd28af1a22c1673209dfeee.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc498f365fd28af1a22c1673209dfeee.png)'
- en: Concept accuracies of CBM and CME models, plotted for all concepts across three
    different predictive tasks. Image by the author.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: CBM 和 CME 模型的概念准确度，绘制了三个不同预测任务中的所有概念。图像由作者提供。
- en: '**Enable human interventions on concepts** — i.e. allowing humans to quickly
    improve model performance by fixing small sets of chosen concepts:'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**允许对概念进行人为干预** — 即允许人们通过修正少量选定概念来快速改善模型性能：'
- en: '![](../Images/6729e306dcee0d9dab97d1f8567d40f1.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6729e306dcee0d9dab97d1f8567d40f1.png)'
- en: CME and CBM model performance changes for different degress of concept interventions.
    Image by the author.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: CME 和 CBM 模型性能在不同概念干预程度下的变化。图像由作者提供。
- en: '**Explain model decision-making in terms of concepts,** by allowing practitioners
    to plot concept processor models directly:'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从概念的角度解释模型决策，** 允许实践者直接绘制概念处理器模型：'
- en: '![](../Images/4bad7e0572912c8a27684492f3574d23.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4bad7e0572912c8a27684492f3574d23.png)'
- en: An example of a concept processor model visualised directly for one of the chosen
    tasks. Image by the author.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一个概念处理器模型直接可视化的示例，针对一个选定任务。图像由作者提供。
- en: '**Help understand model processing of concepts** by analysing the hidden space
    of underlying concepts across model layers:'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过分析模型层间的隐藏空间，帮助理解模型对概念的处理：**'
- en: '![](../Images/d3c4f3811571ce29c8d968ac0db1d97d.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d3c4f3811571ce29c8d968ac0db1d97d.png)'
- en: An example of hidden space visualisation for a few layers of the vanilla CNN.
    The columns represent the different layers. The rows represent the different concepts,
    with every row’s colour corresponding to that concept’s values. The “best” CME
    layers are indicated by a *. Image by the author.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单 CNN 的隐藏空间可视化示例。列代表不同的层，行代表不同的概念，每行的颜色对应于该概念的值。标有 * 的为“最佳” CME 层。图像由作者提供。
- en: By defining Concept-based Models in the weakly-supervised domain with CME, we
    can develop significantly more label-efficient Concept-based Models
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 通过在弱监督领域定义基于概念的模型（CME），我们可以开发出显著更具标签效率的基于概念的模型
- en: '**Take Home Message**'
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**主要结论**'
- en: By leveraging pre-trained vanilla Deep Neural Networks, we may obtain concept
    annotations and Concept-based Models at a *vastly* lower annotation cost, compared
    to standard CBM approaches.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用预训练的普通深度神经网络，我们可以在*极大*降低注释成本的情况下获得概念注释和基于概念的模型，与标准 CBM 方法相比。
- en: Furthermore, this does not strictly apply *just* to concepts that are highly
    correlated to the end-task, but in certain cases also applies to concepts that
    are *independent* of the end-task.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这不仅严格适用于与最终任务高度相关的概念，在某些情况下，也适用于与最终任务*独立*的概念。
- en: '**References**'
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: '[1] Chris Molnar. Interpretable Machine Learning. [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Chris Molnar. 解释性机器学习。 [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/)'
- en: '[2] Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson,
    Been Kim, and Percy Liang. Concept bottleneck models. *In International Conference
    on Machine Learning, pages 5338–5348\. PMLR* (2020).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson,
    Been Kim, 和 Percy Liang. 概念瓶颈模型。*在国际机器学习会议*，第 5338–5348 页。PMLR*（2020）。'
- en: '[3] Amirata Ghorbani, James Wexler, James Zou, and Been Kim. Towards Automatic
    Concept-based Explanations. *In* *Advances in neural information processing systems*,
    *32*.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Amirata Ghorbani, James Wexler, James Zou, 和 Been Kim. 朝向自动化基于概念的解释。*在*
    *神经信息处理系统的进展*，*32*。'
