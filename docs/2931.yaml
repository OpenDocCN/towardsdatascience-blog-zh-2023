- en: 'Now You See Me (CME): Concept-based Model Extraction'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/now-you-see-me-cme-concept-based-model-extraction-97231105f8fa?source=collection_archive---------5-----------------------#2023-09-22](https://towardsdatascience.com/now-you-see-me-cme-concept-based-model-extraction-97231105f8fa?source=collection_archive---------5-----------------------#2023-09-22)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A label-efficient approach to Concept-based Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@kazhdan.dmitry?source=post_page-----97231105f8fa--------------------------------)[![Dmitry
    Kazhdan](../Images/6243af19a00a52c7732349442af36e3b.png)](https://medium.com/@kazhdan.dmitry?source=post_page-----97231105f8fa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----97231105f8fa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----97231105f8fa--------------------------------)
    [Dmitry Kazhdan](https://medium.com/@kazhdan.dmitry?source=post_page-----97231105f8fa--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe322093479&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnow-you-see-me-cme-concept-based-model-extraction-97231105f8fa&user=Dmitry+Kazhdan&userId=e322093479&source=post_page-e322093479----97231105f8fa---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----97231105f8fa--------------------------------)
    ·6 min read·Sep 22, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F97231105f8fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnow-you-see-me-cme-concept-based-model-extraction-97231105f8fa&user=Dmitry+Kazhdan&userId=e322093479&source=-----97231105f8fa---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F97231105f8fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnow-you-see-me-cme-concept-based-model-extraction-97231105f8fa&source=-----97231105f8fa---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the AIMLAI workshop paper presented at the CIKM conference: “[Now You
    See Me (CME): Concept-based Model Extraction](https://arxiv.org/abs/2010.13233)”
    ([GitHub](https://github.com/dmitrykazhdan/concept-based-xai))'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e4cdb7ef047c2d8286f26dc76bd181b1.png)'
  prefs: []
  type: TYPE_IMG
- en: Visual abstract. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '**TL;DR**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Problem —** Deep Neural Network models are black boxes, which cannot be interpreted
    directly. As a result — it is difficult to build trust in such models. Existing
    methods, such as Concept Bottleneck Models, make such models more interpretable,
    but require a high annotation cost for annotating underlying concepts'
  prefs: []
  type: TYPE_NORMAL
- en: '**Key Innovation —** A method for generating Concept-based Models in a *weakly-supervised
    fashion*, requiring vastly fewer annotations as a result'
  prefs: []
  type: TYPE_NORMAL
- en: '**Solution —** Our [Concept-based Model Extraction (CME) framework](https://arxiv.org/abs/2010.13233),
    capable of extracting Concept-based Models from pre-trained vanilla Convolutional
    Neural Networks (CNNs) in a *semi-supervised* fashion, whilst preserving end-task
    performance'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be26db1273cd5e11276d419dbecd5917.png)'
  prefs: []
  type: TYPE_IMG
- en: Vanilla CNN End-to-End input processing. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d1dee7d5aa792dfb8ba814fdae4760c4.png)'
  prefs: []
  type: TYPE_IMG
- en: Two-stage Concept-based Model processing. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Concept Bottleneck Models (CBMs)**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In recent years, the realm of Explainable Artificial Intelligence (XAI) [1]
    has witnessed a surging interest in Concept Bottleneck Model (CBM) approaches
    [2]. These methods introduce an innovative model architecture, in which input
    images are processed in two distinct phases: *concept encoding* and *concept processing*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'During concept encoding, concept information is extracted from the high-dimensional
    input data. Subsequently, in the concept processing phase, this extracted concept
    information is used to generate the desired output task label. A salient feature
    of CBMs is their reliance on a semantically-meaningful *concept representation*,
    serving as an intermediate, interpretable representation for downstream task predictions,
    as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/60c35c44941241ebca1adb934e5a9035.png)'
  prefs: []
  type: TYPE_IMG
- en: Concept Bottleneck Model Processing. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: As shown above, CBM models are trained with a combination of *task loss* for
    ensuring accurate task label prediction, as well as *concept loss*, ensuring accurate
    intermediate concept prediction. Importantly, CBMs enhance model transparency,
    since the underlying concept representation provides a way to explain and better-understand
    underlying model behaviour.
  prefs: []
  type: TYPE_NORMAL
- en: Concept Bottleneck Models offer a novel type of CNNs interpretable-by-design,
    allowing users to encode existing domain knowledge into models via concepts.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Overall, CBMs serve as an important innovation, bringing us closer to more transparent
    and trustworthy models.
  prefs: []
  type: TYPE_NORMAL
- en: '**Challenge: CBMs have a high concept annotation cost**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unfortunately, CBMs require a high amount of concept annotations during training.
  prefs: []
  type: TYPE_NORMAL
- en: At present, CBM approaches require *all* training samples to be annotated explicitly
    with *both* end-task, and concept annotations. Hence, for a dataset with *N* samples
    and *C* concepts, the annotation cost rises from *N* annotations (one task label
    per sample), to *N*(C+1)* annotations (one task label per sample, and one concept
    label for every concept). In practice, this can quickly get unwieldy, particularly
    for datasets with a large amount of concepts and training samples.
  prefs: []
  type: TYPE_NORMAL
- en: For example, for a dataset of 10,000 images with 50 concepts, the annotation
    cost will increase by 50*10,000=500,000 labels, i.e. by *half a million* extra
    annotations.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, Concept Bottleneck Models require a substantial amount of concept
    annotations for training.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Leveraging Semi-Supervised Concept-based Models with CME**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CME relies on a similar observation highlighted in [3], where it was observed
    that vanilla CNN models often retain a high amount of information pertaining to
    concepts in their *hidden space,* which may be used for concept information mining
    at no extra annotation cost. Importantly, this work considered the scenario where
    the underlying concepts are *unknown*, and had to be extracted from a model’s
    hidden space in an unsupervised fashion.
  prefs: []
  type: TYPE_NORMAL
- en: 'With CME, we make use of the above observation, and consider a scenario where
    we *have* knowledge of the underlying concepts, but we only have a small amount
    of sample annotations for each these concepts. Similarly to [3], CME relies on
    a given pre-trained vanilla CNN and the small amount of concept annotations in
    order to extract further concept annotations in a *semi-supervised fashion,* as
    shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d4e9376dc4b8e9c8fbb35d2646206727.png)'
  prefs: []
  type: TYPE_IMG
- en: CME model processing. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: As shown above, CME extracts the concept representation using a pre-trained
    model’s hidden space in a *post-hoc* fashion. Further details are given below.
  prefs: []
  type: TYPE_NORMAL
- en: '***Concept Encoder Training****:* instead of training concept encoders from
    scratch on the raw data, as done in case of CBMs, we setup our concept encoder
    model training in a *semi-supervised fashion*, using the vanilla CNN’s hidden
    space:'
  prefs: []
  type: TYPE_NORMAL
- en: We begin by pre-specifying a set of layers L from the vanilla CNN to use for
    concept extraction. This can range from *all* layers, to just the last few, depending
    on available compute capacity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, for each concept, we train a separate model on top of the hidden space
    of *each* layer in L to predict that concept’s values from the layer’s hidden
    space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We proceed to selecting the model and corresponding layer with the best model
    accuracy as the “best” model and layer for predicting that concept.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consequently, when making concept predictions for a concept *i*, we first retrieve
    the hidden space representation of the best layer for that concept, and then pass
    it through the corresponding predictive model for inference.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Overall, the *concept encoder* function can be summarised as follows (assuming
    there are *k* concepts in total):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8d407a7a7dbd5271ff98dc23b120582d.png)'
  prefs: []
  type: TYPE_IMG
- en: CME Concept Encoder equation. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Here, p-hat on the LHS represents the concept encoder function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ***g***ᵢ terms represent the hidden-space-to-concept models trained on top
    of the different layer hidden spaces, with *i* representing the concept index,
    ranging from 1 to *k*. In practice, these models can be fairly simple, such as
    Linear Regressors, or Gradient Boosted Classifiers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *f(***x***)* terms represent the sub-models of the original vanilla CNN,
    extracting the input’s hidden representation at a particular layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In both cases above, *lʲ* superscripts specify the “best” layers these two models
    are operating on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Concept Processor Training****:* concept processor model training in CME
    is setup by training models using task labels as outputs, and *concept encoder*
    predictions as inputs. Importantly, these models are operating on a much more
    compact input representation, and can consequently be represented directly via
    *interpretable* models, such as Decision Trees (DTs), or Logistic Regression (LR)
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: '**CME Experiments & Results**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our experiments on both synthetic ([dSprites](https://github.com/google-deepmind/dsprites-dataset)
    and [shapes3d](https://github.com/google-deepmind/3d-shapes)) and challenging
    real-world datasets ([CUB](https://paperswithcode.com/dataset/cub-200-2011)) demonstrated
    that CME models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Achieve high concept predictive accuracy** comparable to that of CBMs in
    many cases, *even on concepts irrelevant to the end-task:*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/fc498f365fd28af1a22c1673209dfeee.png)'
  prefs: []
  type: TYPE_IMG
- en: Concept accuracies of CBM and CME models, plotted for all concepts across three
    different predictive tasks. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Enable human interventions on concepts** — i.e. allowing humans to quickly
    improve model performance by fixing small sets of chosen concepts:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/6729e306dcee0d9dab97d1f8567d40f1.png)'
  prefs: []
  type: TYPE_IMG
- en: CME and CBM model performance changes for different degress of concept interventions.
    Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Explain model decision-making in terms of concepts,** by allowing practitioners
    to plot concept processor models directly:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/4bad7e0572912c8a27684492f3574d23.png)'
  prefs: []
  type: TYPE_IMG
- en: An example of a concept processor model visualised directly for one of the chosen
    tasks. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Help understand model processing of concepts** by analysing the hidden space
    of underlying concepts across model layers:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/d3c4f3811571ce29c8d968ac0db1d97d.png)'
  prefs: []
  type: TYPE_IMG
- en: An example of hidden space visualisation for a few layers of the vanilla CNN.
    The columns represent the different layers. The rows represent the different concepts,
    with every row’s colour corresponding to that concept’s values. The “best” CME
    layers are indicated by a *. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: By defining Concept-based Models in the weakly-supervised domain with CME, we
    can develop significantly more label-efficient Concept-based Models
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Take Home Message**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By leveraging pre-trained vanilla Deep Neural Networks, we may obtain concept
    annotations and Concept-based Models at a *vastly* lower annotation cost, compared
    to standard CBM approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, this does not strictly apply *just* to concepts that are highly
    correlated to the end-task, but in certain cases also applies to concepts that
    are *independent* of the end-task.
  prefs: []
  type: TYPE_NORMAL
- en: '**References**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Chris Molnar. Interpretable Machine Learning. [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson,
    Been Kim, and Percy Liang. Concept bottleneck models. *In International Conference
    on Machine Learning, pages 5338–5348\. PMLR* (2020).'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Amirata Ghorbani, James Wexler, James Zou, and Been Kim. Towards Automatic
    Concept-based Explanations. *In* *Advances in neural information processing systems*,
    *32*.'
  prefs: []
  type: TYPE_NORMAL
