- en: Black-Box Chemical Process Optimization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é»‘ç®±åŒ–å­¦è¿‡ç¨‹ä¼˜åŒ–
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/black-box-chemical-process-optimization-5d7cbb9be0cf?source=collection_archive---------8-----------------------#2023-10-24](https://towardsdatascience.com/black-box-chemical-process-optimization-5d7cbb9be0cf?source=collection_archive---------8-----------------------#2023-10-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/black-box-chemical-process-optimization-5d7cbb9be0cf?source=collection_archive---------8-----------------------#2023-10-24](https://towardsdatascience.com/black-box-chemical-process-optimization-5d7cbb9be0cf?source=collection_archive---------8-----------------------#2023-10-24)
- en: Smart Chemical Systems
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ™ºèƒ½åŒ–åŒ–å­¦ç³»ç»Ÿ
- en: Relying on decision support for which experiment to perform next.
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¾èµ–å†³ç­–æ”¯æŒæ¥å†³å®šä¸‹ä¸€æ­¥è¿›è¡Œå“ªä¸ªå®éªŒã€‚
- en: '[](https://gtancev.medium.com/?source=post_page-----5d7cbb9be0cf--------------------------------)[![Georgi
    Tancev](../Images/4529168ec26d51265185189298c81677.png)](https://gtancev.medium.com/?source=post_page-----5d7cbb9be0cf--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5d7cbb9be0cf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5d7cbb9be0cf--------------------------------)
    [Georgi Tancev](https://gtancev.medium.com/?source=post_page-----5d7cbb9be0cf--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://gtancev.medium.com/?source=post_page-----5d7cbb9be0cf--------------------------------)[![Georgi
    Tancev](../Images/4529168ec26d51265185189298c81677.png)](https://gtancev.medium.com/?source=post_page-----5d7cbb9be0cf--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5d7cbb9be0cf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5d7cbb9be0cf--------------------------------)
    [Georgi Tancev](https://gtancev.medium.com/?source=post_page-----5d7cbb9be0cf--------------------------------)'
- en: Â·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F54224776d918&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fblack-box-chemical-process-optimization-5d7cbb9be0cf&user=Georgi+Tancev&userId=54224776d918&source=post_page-54224776d918----5d7cbb9be0cf---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5d7cbb9be0cf--------------------------------)
    Â·12 min readÂ·Oct 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5d7cbb9be0cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fblack-box-chemical-process-optimization-5d7cbb9be0cf&user=Georgi+Tancev&userId=54224776d918&source=-----5d7cbb9be0cf---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F54224776d918&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fblack-box-chemical-process-optimization-5d7cbb9be0cf&user=Georgi+Tancev&userId=54224776d918&source=post_page-54224776d918----5d7cbb9be0cf---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5d7cbb9be0cf--------------------------------)
    Â· 12åˆ†é’Ÿé˜…è¯» Â· 2023å¹´10æœˆ24æ—¥ [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5d7cbb9be0cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fblack-box-chemical-process-optimization-5d7cbb9be0cf&user=Georgi+Tancev&userId=54224776d918&source=-----5d7cbb9be0cf---------------------clap_footer-----------)'
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5d7cbb9be0cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fblack-box-chemical-process-optimization-5d7cbb9be0cf&source=-----5d7cbb9be0cf---------------------bookmark_footer-----------)![](../Images/83e86152aab2f66efa8ec7fba3c819f6.png)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5d7cbb9be0cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fblack-box-chemical-process-optimization-5d7cbb9be0cf&source=-----5d7cbb9be0cf---------------------bookmark_footer-----------)![](../Images/83e86152aab2f66efa8ec7fba3c819f6.png)'
- en: Photo by [National Cancer Institute](https://unsplash.com/@nci?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± [å›½å®¶ç™Œç—‡ç ”ç©¶æ‰€](https://unsplash.com/@nci?utm_source=medium&utm_medium=referral)
    æä¾›ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: Designing and optimizing chemical processes is one of the main tasks in process
    engineering. When setting up a chemicalsystem (e.g., a unit operation) with a
    large number of **design parameters**, the question of how to quickly arrive at
    the optimal design(s) arises frequently. If one had a model of the system, one
    could solve the problem numerically, i.e., by optimizing with respect to a specified
    metric (e.g., yield, material properties, cost, and so on). Often, however, this
    is not possible because the relationships (e.g., kinetics, physical phenomena)
    are not fully understood â€” or perhaps even not known at all. Therefore, formulating
    equations is not possible.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾è®¡å’Œä¼˜åŒ–åŒ–å­¦è¿‡ç¨‹æ˜¯è¿‡ç¨‹å·¥ç¨‹ä¸­çš„ä¸»è¦ä»»åŠ¡ä¹‹ä¸€ã€‚åœ¨è®¾ç«‹ä¸€ä¸ªæœ‰å¤§é‡**è®¾è®¡å‚æ•°**çš„åŒ–å­¦ç³»ç»Ÿï¼ˆä¾‹å¦‚ï¼Œä¸€ä¸ªå•å…ƒæ“ä½œï¼‰æ—¶ï¼Œå¦‚ä½•å¿«é€Ÿè·å¾—æœ€ä½³è®¾è®¡å¸¸å¸¸æˆä¸ºä¸€ä¸ªé—®é¢˜ã€‚å¦‚æœæœ‰ä¸€ä¸ªç³»ç»Ÿæ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡æ•°å€¼æ–¹æ³•è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå³æ ¹æ®æŒ‡å®šçš„æŒ‡æ ‡ï¼ˆä¾‹å¦‚ï¼Œäº§ç‡ã€ææ–™æ€§è´¨ã€æˆæœ¬ç­‰ï¼‰è¿›è¡Œä¼˜åŒ–ã€‚ç„¶è€Œï¼Œè¿™é€šå¸¸æ˜¯ä¸å¯èƒ½çš„ï¼Œå› ä¸ºå…³ç³»ï¼ˆä¾‹å¦‚ï¼ŒåŠ¨åŠ›å­¦ã€ç‰©ç†ç°è±¡ï¼‰æœªè¢«å®Œå…¨ç†è§£â€”â€”ç”šè‡³å¯èƒ½å®Œå…¨æœªçŸ¥ã€‚å› æ­¤ï¼Œæ— æ³•åˆ¶å®šæ–¹ç¨‹ã€‚
- en: In such cases, the only option left is to find an optimal design by means of
    empirical **models** fed with **data** from **experiments**. Traditionally, for
    instance, one can refer to [**response surfaces**](https://en.wikipedia.org/wiki/Response_surface_methodology)and[**central
    composite designs**](https://en.wikipedia.org/wiki/Central_composite_design)to
    [identify optimal operating conditions](https://www.sciencedirect.com/science/article/abs/pii/S0960852416317515?via%3Dihub=).
    [These make use of local second-order approximations and gradient ascent/descent
    to locate the best configuration](https://n.ethz.ch/~kahans/doe2020/ch-rsm.html#sequential-experiments).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå”¯ä¸€çš„é€‰æ‹©æ˜¯é€šè¿‡ç»éªŒ**æ¨¡å‹**å’Œä»**å®éªŒ**ä¸­è·å¾—çš„**æ•°æ®**æ¥å¯»æ‰¾æœ€ä½³è®¾è®¡ã€‚ä¾‹å¦‚ï¼Œä¼ ç»Ÿä¸Šå¯ä»¥å‚è€ƒ[**å“åº”é¢**](https://en.wikipedia.org/wiki/Response_surface_methodology)å’Œ[**ä¸­å¤®å¤åˆè®¾è®¡**](https://en.wikipedia.org/wiki/Central_composite_design)æ¥[ç¡®å®šæœ€ä½³æ“ä½œæ¡ä»¶](https://www.sciencedirect.com/science/article/abs/pii/S0960852416317515?via%3Dihub=)ã€‚[è¿™äº›æ–¹æ³•åˆ©ç”¨å±€éƒ¨äºŒé˜¶è¿‘ä¼¼å’Œæ¢¯åº¦ä¸Šå‡/ä¸‹é™æ¥æ‰¾åˆ°æœ€ä½³é…ç½®](https://n.ethz.ch/~kahans/doe2020/ch-rsm.html#sequential-experiments)ã€‚
- en: This article, however, is devoted to an alternative strategy, namely [**Bayesian
    optimization**](https://arxiv.org/abs/1807.02811), whichis related to reinforcement
    learning andhas been successfully applied to the design of [materials](https://link.springer.com/chapter/10.1007/978-3-319-23871-5_3),
    [chemical reactions](https://www.nature.com/articles/s43586-023-00266-3), and
    [drugs](https://pubs.rsc.org/en/content/articlehtml/2019/sc/c9sc04026a). It offers
    advantages like **higher flexibility** of models and processing of **multi-fidelity**
    information. The latter refers to the fact that mixed-quality data from different
    sources can be used for optimization, for example when physical models are at
    least rudimentarily available.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæœ¬æ–‡è‡´åŠ›äºä¸€ç§æ›¿ä»£ç­–ç•¥ï¼Œå³[**è´å¶æ–¯ä¼˜åŒ–**](https://arxiv.org/abs/1807.02811)ï¼Œå®ƒä¸å¼ºåŒ–å­¦ä¹ ç›¸å…³ï¼Œå¹¶å·²æˆåŠŸåº”ç”¨äº[ææ–™](https://link.springer.com/chapter/10.1007/978-3-319-23871-5_3)ã€[åŒ–å­¦ååº”](https://www.nature.com/articles/s43586-023-00266-3)å’Œ[è¯ç‰©](https://pubs.rsc.org/en/content/articlehtml/2019/sc/c9sc04026a)ã€‚å®ƒæä¾›äº†å¦‚**æ¨¡å‹æ›´é«˜çµæ´»æ€§**å’Œå¤„ç†**å¤šä¿çœŸåº¦**ä¿¡æ¯ç­‰ä¼˜ç‚¹ã€‚åè€…æŒ‡çš„æ˜¯å¯ä»¥åˆ©ç”¨æ¥è‡ªä¸åŒæ¥æºçš„æ··åˆè´¨é‡æ•°æ®è¿›è¡Œä¼˜åŒ–ï¼Œä¾‹å¦‚å½“ç‰©ç†æ¨¡å‹è‡³å°‘æœ‰åˆæ­¥å¯ç”¨æ—¶ã€‚
- en: Problem Definition
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é—®é¢˜å®šä¹‰
- en: Multi-Armed Bandits
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¤šè‡‚è€è™æœº
- en: You may ask, why do we need yet another optimization method? Let me answer this
    question by setting up the following scenario for you. You find yourself in front
    of a **slot machine** with *k* arms, that is, you have *k* arms to pull, and each
    arm *i* has a probability *páµ¢* to give you a reward *ráµ¢*. Evidently, your goal
    will be to maximize your total reward *R*, but you only have a finite number of
    attempts (or budget) *T*. This is the so-called [**multi-armed bandit**](https://en.wikipedia.org/wiki/Multi-armed_bandit)
    **problem**.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½ä¼šé—®ï¼Œä¸ºä»€ä¹ˆæˆ‘ä»¬è¿˜éœ€è¦å¦ä¸€ç§ä¼˜åŒ–æ–¹æ³•ï¼Ÿè®©æˆ‘é€šè¿‡ä»¥ä¸‹æƒ…å¢ƒæ¥å›ç­”è¿™ä¸ªé—®é¢˜ã€‚ä½ å‘ç°è‡ªå·±é¢å‰æœ‰ä¸€å°**è€è™æœº**ï¼Œå®ƒæœ‰*k*ä¸ªè‡‚ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œä½ æœ‰*k*ä¸ªè‡‚å¯ä»¥æ‹‰åŠ¨ï¼Œæ¯ä¸ªè‡‚*i*æœ‰ä¸€ä¸ªæ¦‚ç‡*páµ¢*ç»™ä½ å¥–åŠ±*ráµ¢*ã€‚æ˜¾ç„¶ï¼Œä½ çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–ä½ çš„æ€»å¥–åŠ±*R*ï¼Œä½†ä½ åªæœ‰æœ‰é™çš„å°è¯•æ¬¡æ•°ï¼ˆæˆ–é¢„ç®—ï¼‰*T*ã€‚è¿™å°±æ˜¯æ‰€è°“çš„[**å¤šè‡‚è€è™æœº**](https://en.wikipedia.org/wiki/Multi-armed_bandit)
    **é—®é¢˜**ã€‚
- en: This problem is difficult because we do not initially know the probabilities
    or the rewards. With our *T* attempts, we have to simultaneously **explore** in
    order to â€œlearnâ€ the *páµ¢*â€™s and *ráµ¢*â€™s but also to **exploit** rewarding arms
    (i.e., pull arms with a high expected reward *páµ¢ráµ¢* as frequently as possible)
    in order to accumulate them. [This is the exploration-exploitation dilemma](https://en.wikipedia.org/wiki/Exploration-exploitation_dilemma).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªé—®é¢˜å¾ˆå›°éš¾ï¼Œå› ä¸ºæˆ‘ä»¬æœ€åˆä¸çŸ¥é“æ¦‚ç‡æˆ–å¥–åŠ±ã€‚é€šè¿‡æˆ‘ä»¬çš„ *T* æ¬¡å°è¯•ï¼Œæˆ‘ä»¬å¿…é¡»åŒæ—¶**æ¢ç´¢**ä»¥â€œå­¦ä¹ â€ *páµ¢* å’Œ *ráµ¢*ï¼Œä½†ä¹Ÿå¿…é¡»**åˆ©ç”¨**æœ‰å¥–åŠ±çš„è‡‚ï¼ˆå³ï¼Œå°½å¯èƒ½é¢‘ç¹åœ°æ‹‰åŠ¨å…·æœ‰é«˜é¢„æœŸå¥–åŠ±
    *páµ¢ráµ¢* çš„è‡‚ï¼‰ä»¥ä¾¿ç§¯ç´¯å®ƒä»¬ã€‚[è¿™å°±æ˜¯æ¢ç´¢-å¼€å‘å›°å¢ƒ](https://en.wikipedia.org/wiki/Exploration-exploitation_dilemma)ã€‚
- en: On the one side, we need to experiment and try out different arms, on the other
    side we have to stick to the same, promising arm(s) â€” and we have to balance both
    objectives carefully due to the limited budget. Similarly, we have to find optimal
    design settings. We need to learn as much as possible about our objective function
    through experiments, but also remain in the proximity of promising maximizers.
    We transition into the realm of Bayesian optimization when we move from the **discrete**
    setting with *k* arms to the **continuous** setting with infinite arms.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬éœ€è¦å®éªŒå¹¶å°è¯•ä¸åŒçš„è‡‚ï¼Œå¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬å¿…é¡»åšæŒç›¸åŒçš„ã€æœ‰å‰æ™¯çš„è‡‚ â€”â€” ç”±äºé¢„ç®—æœ‰é™ï¼Œæˆ‘ä»¬å¿…é¡»å°å¿ƒå¹³è¡¡è¿™ä¸¤ä¸ªç›®æ ‡ã€‚åŒæ ·ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°æœ€ä¼˜çš„è®¾è®¡è®¾ç½®ã€‚æˆ‘ä»¬éœ€è¦é€šè¿‡å®éªŒå°½å¯èƒ½å¤šåœ°äº†è§£æˆ‘ä»¬çš„ç›®æ ‡å‡½æ•°ï¼Œä½†ä¹Ÿè¦ä¿æŒåœ¨æœ‰å‰æ™¯çš„æœ€å¤§åŒ–å™¨é™„è¿‘ã€‚å½“æˆ‘ä»¬ä»å…·æœ‰
    *k* è‡‚çš„ **ç¦»æ•£** è®¾ç½®è½¬å‘å…·æœ‰æ— é™è‡‚çš„ **è¿ç»­** è®¾ç½®æ—¶ï¼Œæˆ‘ä»¬å°±è¿›å…¥äº†è´å¶æ–¯ä¼˜åŒ–çš„é¢†åŸŸã€‚
- en: Bayesian Optimization
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è´å¶æ–¯ä¼˜åŒ–
- en: Roughly speaking, the idea is to sequentially learn the function *f*(x) that
    we want to optimize over a domain of interest ğ’³ and to move towards its maximum
    xáµ’áµ–áµ—,
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§è‡´æ¥è¯´ï¼Œè¿™ä¸ªæƒ³æ³•æ˜¯é€æ­¥å­¦ä¹ æˆ‘ä»¬å¸Œæœ›åœ¨æ„Ÿå…´è¶£çš„é¢†åŸŸ ğ’³ ä¸Šä¼˜åŒ–çš„å‡½æ•° *f*(x)ï¼Œå¹¶æœç€å…¶æœ€å¤§å€¼ xáµ’áµ–áµ— ç§»åŠ¨ï¼Œ
- en: xáµ’áµ–áµ— = arg max *f*(x), s.t. x âˆˆ ğ’³.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: xáµ’áµ–áµ— = arg max *f*(x)ï¼Œä½¿å¾— x âˆˆ ğ’³ã€‚
- en: 'Typically, Bayesian optimization works well for optimization over continuous
    domains of less than 20 dimensions, and it tolerates stochastic noise in function
    evaluations. The method has two main â€œingredientsâ€: [**Gaussian processes**](http://gaussianprocess.org)
    and **acquisition functions**.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œè´å¶æ–¯ä¼˜åŒ–é€‚ç”¨äºç»´åº¦å°‘äº 20 çš„è¿ç»­é¢†åŸŸä¼˜åŒ–ï¼Œå¹¶ä¸”å®ƒå¯ä»¥å®¹å¿å‡½æ•°è¯„ä¼°ä¸­çš„éšæœºå™ªå£°ã€‚è¯¥æ–¹æ³•æœ‰ä¸¤ä¸ªä¸»è¦â€œæˆåˆ†â€ï¼š[**é«˜æ–¯è¿‡ç¨‹**](http://gaussianprocess.org)
    å’Œ **é‡‡é›†å‡½æ•°**ã€‚
- en: Gaussian Processes
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é«˜æ–¯è¿‡ç¨‹
- en: The first ingredient in Bayesian optimization is the Gaussian process, which
    is best understood by deriving it from [**Bayesian linear regression**](https://en.wikipedia.org/wiki/Bayesian_linear_regression).
    [**Bayesian statistics**](http://www.stat.columbia.edu/~gelman/book/) is an alternative
    theory in the field of statistics based on the Bayesian interpretation of probability
    in which probability expresses a degree of **belief** or **information** (i.e.,
    knowledge) about an event. This differs from the frequentist interpretation that
    views probability as the limit of the relative frequency of an event after many
    trials.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è´å¶æ–¯ä¼˜åŒ–çš„ç¬¬ä¸€ä¸ªæˆåˆ†æ˜¯é«˜æ–¯è¿‡ç¨‹ï¼Œè¿™æœ€å¥½é€šè¿‡ä» [**è´å¶æ–¯çº¿æ€§å›å½’**](https://en.wikipedia.org/wiki/Bayesian_linear_regression)
    æ¨å¯¼æ¥ç†è§£ã€‚[**è´å¶æ–¯ç»Ÿè®¡**](http://www.stat.columbia.edu/~gelman/book/) æ˜¯ç»Ÿè®¡å­¦é¢†åŸŸçš„å¦ä¸€ç§ç†è®ºï¼ŒåŸºäºæ¦‚ç‡çš„è´å¶æ–¯è§£é‡Šï¼Œå…¶ä¸­æ¦‚ç‡è¡¨ç¤ºå¯¹äº‹ä»¶çš„
    **ä¿¡å¿µ** æˆ– **ä¿¡æ¯**ï¼ˆå³çŸ¥è¯†ï¼‰çš„ç¨‹åº¦ã€‚è¿™ä¸é¢‘ç‡å­¦æ´¾è§£é‡Šçš„æ¦‚ç‡è§†ä¸ºäº‹ä»¶åœ¨å¤šæ¬¡è¯•éªŒåçš„ç›¸å¯¹é¢‘ç‡çš„æé™ä¸åŒã€‚
- en: Let us assume that the function *f*(x) that we want to maximize follows a model
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å‡è®¾æˆ‘ä»¬è¦æœ€å¤§åŒ–çš„å‡½æ•° *f*(x) éµå¾ªä¸€ä¸ªæ¨¡å‹
- en: y= *f*(x) + *Ïµ* = *Î²*áµ€x *+ Ïµ*, with x âˆˆ â„*áµˆ*.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: y= *f*(x) + *Ïµ* = *Î²*áµ€x *+ Ïµ*ï¼Œå…¶ä¸­ x âˆˆ â„*áµˆ*ã€‚
- en: Although this model is linear in parameters, the basis vectors in **X** can
    also represent non-linearity through **basis expansion**. Furthermore, we stack
    the *n* data points/samples in a matrix **X** =[xâ‚áµ€, â€¦, xâ‚™áµ€] and **y** = [yâ‚,
    â€¦, yâ‚™].
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡è¿™ä¸ªæ¨¡å‹åœ¨å‚æ•°ä¸Šæ˜¯çº¿æ€§çš„ï¼Œä½† **X** ä¸­çš„åŸºå‘é‡ä¹Ÿå¯ä»¥é€šè¿‡ **åŸºæ‰©å±•** è¡¨ç¤ºéçº¿æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°† *n* ä¸ªæ•°æ®ç‚¹/æ ·æœ¬å †å åœ¨çŸ©é˜µ **X**
    =[xâ‚áµ€, â€¦, xâ‚™áµ€] å’Œ **y** = [yâ‚, â€¦, yâ‚™] ä¸­ã€‚
- en: Bayesian statistics revolves around the [**Bayesâ€™ theorem**](https://en.wikipedia.org/wiki/Bayes%27_theorem),
    which states that
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è´å¶æ–¯ç»Ÿè®¡å›´ç»•ç€ [**è´å¶æ–¯å®šç†**](https://en.wikipedia.org/wiki/Bayes%27_theorem)ï¼Œå…¶å£°æ˜
- en: '![](../Images/2d6ce4aaf594bff3eaff02d81cbafc25.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d6ce4aaf594bff3eaff02d81cbafc25.png)'
- en: Although not explicitly stated, some of the distributions are also conditionally
    dependent on the data realization **X**. In particular, we are interested in the
    **posterior distribution** p(*Î²*|*y*), given our **prior knowledge** p(*Î²*) and
    the **data distribution** p(*y*|*Î²*). The prior expresses our knowledge before
    and the posterior after seeing the data. In the Bayesian interpretation of statistics,
    a parameter realization of the posterior distribution is essentially one possible
    model among many. If one interprets a model as a theory, then the prior is â€œall
    possible theoriesâ€, but data support some theories more than others, resulting
    in higher posterior probabilities for certain theories â€” or parameter combinations.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æ²¡æœ‰æ˜ç¡®è¯´æ˜ï¼Œä½†ä¸€äº›åˆ†å¸ƒä¹Ÿä¾èµ–äºæ•°æ®å®ç°**X**ã€‚ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬å¯¹**åéªŒåˆ†å¸ƒ**p(*Î²*|*y*)æ„Ÿå…´è¶£ï¼Œç»™å®šæˆ‘ä»¬çš„**å…ˆéªŒçŸ¥è¯†**p(*Î²*)å’Œ**æ•°æ®åˆ†å¸ƒ**p(*y*|*Î²*)ã€‚å…ˆéªŒè¡¨ç¤ºæˆ‘ä»¬åœ¨çœ‹åˆ°æ•°æ®ä¹‹å‰çš„çŸ¥è¯†ï¼Œè€ŒåéªŒåˆ™æ˜¯åœ¨çœ‹åˆ°æ•°æ®ä¹‹åçš„çŸ¥è¯†ã€‚åœ¨è´å¶æ–¯ç»Ÿè®¡çš„è§£é‡Šä¸­ï¼ŒåéªŒåˆ†å¸ƒçš„å‚æ•°å®ç°æœ¬è´¨ä¸Šæ˜¯ä¼—å¤šå¯èƒ½æ¨¡å‹ä¸­çš„ä¸€ä¸ªã€‚å¦‚æœå°†æ¨¡å‹è§£é‡Šä¸ºç†è®ºï¼Œé‚£ä¹ˆå…ˆéªŒæ˜¯â€œæ‰€æœ‰å¯èƒ½çš„ç†è®ºâ€ï¼Œä½†æ•°æ®ä¼šæ›´æ”¯æŒæŸäº›ç†è®ºï¼Œä»è€Œä½¿æŸäº›ç†è®ºæˆ–å‚æ•°ç»„åˆçš„åéªŒæ¦‚ç‡æ›´é«˜ã€‚
- en: If we assume a normal (i.e., Gaussian) prior p(*Î²*) = ğ’©(0, **I**) for the parameters,
    and a normal likelihood p(*y*|*Î²*) = ğ’©(**X***Î²,* Ïƒâ‚™Â²) of our data, the analytical
    solution will be normal as well and the multi-variate posterior distribution p(*Î²*|*y*)
    = ğ’©(Î¼*áµ¦,* Î£*áµ¦*) becomes
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å¯¹å‚æ•°å‡è®¾æ­£æ€ï¼ˆå³é«˜æ–¯ï¼‰å…ˆéªŒp(*Î²*) = ğ’©(0, **I**)ï¼Œå¹¶ä¸”å¯¹æ•°æ®æœ‰æ­£æ€ä¼¼ç„¶p(*y*|*Î²*) = ğ’©(**X***Î²,* Ïƒâ‚™Â²)ï¼Œåˆ™è§£æè§£ä¹Ÿå°†æ˜¯æ­£æ€çš„ï¼Œå¤šå…ƒåéªŒåˆ†å¸ƒp(*Î²*|*y*)
    = ğ’©(Î¼*áµ¦,* Î£*áµ¦*)å˜ä¸º
- en: Î¼*áµ¦* = (**X**áµ€**X +** Ïƒâ‚™Â²**I**)â»Â¹**X**áµ€**y**,
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Î¼*áµ¦* = (**X**áµ€**X** + Ïƒâ‚™Â²**I**)â»Â¹**X**áµ€**y**ï¼Œ
- en: Î£*áµ¦* = (Ïƒâ‚™â»Â²**X**áµ€**X + I**)â»Â¹.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Î£*áµ¦* = (Ïƒâ‚™â»Â²**X**áµ€**X** + I**)â»Â¹ã€‚
- en: If we take the samples in **X**, we get a **posterior predictive distribution**
    for the output vector
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å–**X**ä¸­çš„æ ·æœ¬ï¼Œå°±ä¼šå¾—åˆ°è¾“å‡ºå‘é‡çš„**åéªŒé¢„æµ‹åˆ†å¸ƒ**ã€‚
- en: p(**y**|**X**) = ğ’©(**X**Î¼*áµ¦*, **X**Î£*áµ¦***X**áµ€ + Ïƒâ‚™Â²**I**).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: p(**y**|**X**) = ğ’©(**X**Î¼*áµ¦*, **X**Î£*áµ¦***X**áµ€ + Ïƒâ‚™Â²**I**).
- en: Fig. 1 illustrates one simple example (a polynomial function of degree four)
    in the one-dimensional case. Note how the ground truth is contained in the predicted
    interval.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾1å±•ç¤ºäº†ä¸€ä¸ªç®€å•ç¤ºä¾‹ï¼ˆä¸€ä¸ªå››æ¬¡å¤šé¡¹å¼å‡½æ•°ï¼‰åœ¨ä¸€ç»´æƒ…å†µä¸‹ã€‚æ³¨æ„å®é™…å€¼è¢«åŒ…å«åœ¨é¢„æµ‹åŒºé—´ä¸­ã€‚
- en: '![](../Images/e6953fe678b9046fc01252e32d1806c3.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e6953fe678b9046fc01252e32d1806c3.png)'
- en: '**Fig. 1:** Predictive posterior distribution (Î¼ Â± 2Ïƒ). Â© Georgi Tancev.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾1:** é¢„æµ‹åéªŒåˆ†å¸ƒï¼ˆÎ¼ Â± 2Ïƒï¼‰ã€‚Â© Georgi Tancevã€‚'
- en: The plot shows how the **uncertainty** increases with increasing distance from
    the data. The overall uncertainty consists of an **epistemic** part (**X**Î£*áµ¦***X**áµ€)
    that decreases as the amount of data increases (the elements in the term **X**áµ€**X**
    increases with more data, its inverse decreases, and so does Î£*áµ¦*), and an **aleatoric**
    part (Ïƒâ‚™Â²**I**) that remains constant no matter what. Moreover, this uncertainty
    is key because it suggests where knowledge is missing but also where the maximum
    might be, so we could make a new measurement at those locations in the domain
    ğ’³.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾æ˜¾ç¤ºäº†éšç€è·ç¦»æ•°æ®çš„å¢åŠ ï¼Œ**ä¸ç¡®å®šæ€§**æ˜¯å¦‚ä½•å¢åŠ çš„ã€‚æ€»ä½“ä¸ç¡®å®šæ€§åŒ…æ‹¬ä¸€ä¸ª**è®¤è¯†è®º**éƒ¨åˆ†ï¼ˆ**X**áµ€**X** + Ïƒâ‚™Â²**I**ï¼‰éšç€æ•°æ®é‡çš„å¢åŠ è€Œå‡å°‘ï¼ˆé¡¹**X**áµ€**X**ä¸­çš„å…ƒç´ éšç€æ•°æ®çš„å¢åŠ è€Œå¢åŠ ï¼Œå…¶é€†çŸ©é˜µå‡å°‘ï¼ŒÎ£*áµ¦*ä¹Ÿå‡å°‘ï¼‰ï¼Œä»¥åŠä¸€ä¸ª**éšæœº**éƒ¨åˆ†ï¼ˆÏƒâ‚™Â²**I**ï¼‰ï¼Œæ— è®ºå¦‚ä½•ä¿æŒä¸å˜ã€‚æ­¤å¤–ï¼Œè¿™ç§ä¸ç¡®å®šæ€§å¾ˆå…³é”®ï¼Œå› ä¸ºå®ƒè¡¨æ˜çŸ¥è¯†ç¼ºå¤±çš„åœ°æ–¹ï¼Œä¹Ÿå¯èƒ½æ˜¯æœ€å¤§å€¼çš„åœ°æ–¹ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥åœ¨é¢†åŸŸğ’³ä¸­çš„è¿™äº›ä½ç½®è¿›è¡Œæ–°çš„æµ‹é‡ã€‚
- en: Observe that p(**y**|**X**) is a joint distribution over the **output** values,
    and it captures their covariance (or similarity) with each other. In general,
    points that are closer to each other will have more similar *y*-values than points
    that are further away. The key insight is that we can directly operate on the
    function values (instead of the parameter values) through a [**kernel**](https://en.wikipedia.org/wiki/Kernel_method#Mathematics:_the_kernel_trick)(or
    covariance) function*k*(xâ‚, xâ‚‚)that measures the similarity between samples (i.e.,
    data points), e.g., a **linear** kernel (with a scale parameter Ïƒâ‚›)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„åˆ°p(**y**|**X**)æ˜¯**è¾“å‡º**å€¼çš„è”åˆåˆ†å¸ƒï¼Œå®ƒæ•æ‰äº†å®ƒä»¬ä¹‹é—´çš„åæ–¹å·®ï¼ˆæˆ–ç›¸ä¼¼æ€§ï¼‰ã€‚é€šå¸¸ï¼Œå½¼æ­¤è·ç¦»è¾ƒè¿‘çš„ç‚¹ä¼šæœ‰æ¯”è·ç¦»è¾ƒè¿œçš„ç‚¹æ›´ç›¸ä¼¼çš„*y*å€¼ã€‚å…³é”®çš„è§è§£æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€ä¸ª[**æ ¸å‡½æ•°**](https://en.wikipedia.org/wiki/Kernel_method#Mathematics:_the_kernel_trick)ï¼ˆæˆ–åæ–¹å·®ï¼‰å‡½æ•°*k*(xâ‚,
    xâ‚‚)ç›´æ¥æ“ä½œå‡½æ•°å€¼ï¼ˆè€Œä¸æ˜¯å‚æ•°å€¼ï¼‰ï¼Œè¯¥å‡½æ•°æµ‹é‡æ ·æœ¬ï¼ˆå³æ•°æ®ç‚¹ï¼‰ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œä¾‹å¦‚ï¼Œå…·æœ‰å°ºåº¦å‚æ•°Ïƒâ‚›çš„**çº¿æ€§**æ ¸å‡½æ•°ã€‚
- en: '*k*(xâ‚, xâ‚‚) = Ïƒâ‚›â‹… xâ‚áµ€xâ‚‚.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*(xâ‚, xâ‚‚) = Ïƒâ‚›â‹… xâ‚áµ€xâ‚‚ã€‚'
- en: However, there are more expressive/flexible kernels such as the **exponential**
    kernel
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œè¿˜æœ‰æ›´å…·è¡¨ç°åŠ›/çµæ´»æ€§çš„æ ¸å‡½æ•°ï¼Œå¦‚**æŒ‡æ•°**æ ¸å‡½æ•°
- en: '*k*(xâ‚, xâ‚‚) = Ïƒâ‚›â‹… exp(-*Î³* â‹… â€–xâ‚ *-* xâ‚‚â€–Â²).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*(xâ‚, xâ‚‚) = Ïƒâ‚›â‹… exp(-*Î³* â‹… â€–xâ‚ *-* xâ‚‚â€–Â²)ã€‚'
- en: A kernel has to be **symmetric** and **positive semidefinite** â€” just like the
    covariance matrix. Furthermore, more sophisticated kernels are obtained through
    **kernel engineering**. For instance, for two kernels *k*â‚(xâ‚, xâ‚‚) and *k*â‚‚(xâ‚,
    xâ‚‚), their sums and products are also kernels. With such kernels, a kernel matrix
    **K** with pair-wise similarities can then be constructed. This is how the Gaussian
    process arises.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ ¸å¿…é¡»æ˜¯**å¯¹ç§°çš„**å’Œ**æ­£åŠå®šçš„**â€”â€”å°±åƒåæ–¹å·®çŸ©é˜µä¸€æ ·ã€‚æ­¤å¤–ï¼Œæ›´å¤æ‚çš„æ ¸æ˜¯é€šè¿‡**æ ¸å·¥ç¨‹**è·å¾—çš„ã€‚ä¾‹å¦‚ï¼Œå¯¹äºä¸¤ä¸ªæ ¸ *k*â‚(xâ‚, xâ‚‚)
    å’Œ *k*â‚‚(xâ‚, xâ‚‚)ï¼Œå®ƒä»¬çš„å’Œä¸ç§¯ä¹Ÿæ˜¯æ ¸ã€‚åˆ©ç”¨è¿™æ ·çš„æ ¸ï¼Œå¯ä»¥æ„é€ å…·æœ‰æˆå¯¹ç›¸ä¼¼æ€§çš„æ ¸çŸ©é˜µ **K**ã€‚è¿™å°±æ˜¯é«˜æ–¯è¿‡ç¨‹çš„ç”±æ¥ã€‚
- en: A Gaussian process *y* âˆ¼ ğ’¢ğ’«(*Î¼*, *k*) is defined as a **stochastic** process
    in which every **finite** collection of **random variables** has a multi-variate
    normal distribution. Simply put, the kernel matrix â€œcapturesâ€ how the individual
    points correlate with each other, and how this projects to (new) function values.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: é«˜æ–¯è¿‡ç¨‹ *y* âˆ¼ ğ’¢ğ’«(*Î¼*, *k*) è¢«å®šä¹‰ä¸º**éšæœº**è¿‡ç¨‹ï¼Œå…¶ä¸­æ¯ä¸€ä¸ª**æœ‰é™**çš„**éšæœºå˜é‡**é›†åˆå…·æœ‰å¤šå…ƒæ­£æ€åˆ†å¸ƒã€‚ç®€å•æ¥è¯´ï¼Œæ ¸çŸ©é˜µâ€œæ•æ‰â€äº†ä¸ªä½“ç‚¹ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œä»¥åŠè¿™äº›ç›¸å…³æ€§å¦‚ä½•æ˜ å°„åˆ°ï¼ˆæ–°çš„ï¼‰å‡½æ•°å€¼ä¸Šã€‚
- en: '![](../Images/af17f90e534bfd2cd2923f2df89b9e44.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af17f90e534bfd2cd2923f2df89b9e44.png)'
- en: It is usually assumed that the (prior) mean function is zero. This can be easily
    ensured by standardizing the output values (i.e., subtracting the empirical average).
    To obtain the value for a new test point x*, we simply condition on the known
    data ğ’Ÿ = {**X**, **y**}.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸å‡è®¾ï¼ˆå…ˆéªŒï¼‰å‡å€¼å‡½æ•°ä¸ºé›¶ã€‚è¿™å¯ä»¥é€šè¿‡æ ‡å‡†åŒ–è¾“å‡ºå€¼ï¼ˆå³ï¼Œå‡å»ç»éªŒå¹³å‡å€¼ï¼‰æ¥è½»æ¾ç¡®ä¿ã€‚ä¸ºäº†è·å¾—æ–°æµ‹è¯•ç‚¹ x* çš„å€¼ï¼Œæˆ‘ä»¬åªéœ€å¯¹å·²çŸ¥æ•°æ® ğ’Ÿ = {**X**ï¼Œ**y**}
    è¿›è¡Œæ¡ä»¶åŒ–ã€‚
- en: '![](../Images/56f3dd8dc32d9fc8a2abf56d36f11794.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56f3dd8dc32d9fc8a2abf56d36f11794.png)'
- en: An important observation is that this method makes inferences based on memorized
    data. In other methods, loss functions must first be optimized. Nonetheless, kernel
    hyperparameters (e.g., Ïƒâ‚›, *Î³,* Ïƒâ‚™) need to be fixed, which can be done through
    maximization of the **marginal likelihood** p(*y*), which itself is also an optimization
    method.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªé‡è¦çš„è§‚å¯Ÿæ˜¯ï¼Œè¿™ç§æ–¹æ³•åŸºäºè®°å¿†æ•°æ®è¿›è¡Œæ¨æ–­ã€‚åœ¨å…¶ä»–æ–¹æ³•ä¸­ï¼ŒæŸå¤±å‡½æ•°å¿…é¡»é¦–å…ˆè¢«ä¼˜åŒ–ã€‚ç„¶è€Œï¼Œæ ¸è¶…å‚æ•°ï¼ˆä¾‹å¦‚ï¼ŒÏƒâ‚›ï¼Œ*Î³*ï¼ŒÏƒâ‚™ï¼‰éœ€è¦è¢«å›ºå®šï¼Œè¿™å¯ä»¥é€šè¿‡æœ€å¤§åŒ–**è¾¹é™…ä¼¼ç„¶**
    p(*y*) æ¥å®Œæˆï¼Œè¿™æœ¬èº«ä¹Ÿæ˜¯ä¸€ç§ä¼˜åŒ–æ–¹æ³•ã€‚
- en: Acquisition Functions
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è·å–å‡½æ•°
- en: With the same data as before, we can now fit a Gaussian process with an exponential
    kernel including diagonal noise term (Fig. 2).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä¹‹å‰çš„æ•°æ®ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥æ‹Ÿåˆä¸€ä¸ªåŒ…å«å¯¹è§’å™ªå£°é¡¹çš„æŒ‡æ•°æ ¸é«˜æ–¯è¿‡ç¨‹ï¼ˆå›¾ 2ï¼‰ã€‚
- en: '![](../Images/08ce328645799be99fefc3af49cd0cab.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08ce328645799be99fefc3af49cd0cab.png)'
- en: '**Fig. 2:** Predictive posterior distribution (Î¼ Â± 2Ïƒ) with a Gaussian process.
    Â© Georgi Tancev.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 2ï¼š** ä½¿ç”¨é«˜æ–¯è¿‡ç¨‹çš„é¢„æµ‹åéªŒåˆ†å¸ƒ (Î¼ Â± 2Ïƒ)ã€‚Â© Georgi Tancev.'
- en: The uncertainty or confidence band suggests plausible function values at different
    locations in our domain. Observe that the uncertainty is larger than in Fig. 1,
    as the exponential kernel offers more flexibility by [including more basis functions](https://en.wikipedia.org/wiki/Radial_basis_function_kernel),
    making a larger set of function values plausible.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ç¡®å®šæ€§æˆ–ç½®ä¿¡å¸¦åœ¨æˆ‘ä»¬é¢†åŸŸçš„ä¸åŒä½ç½®å»ºè®®äº†åˆç†çš„å‡½æ•°å€¼ã€‚è§‚å¯Ÿåˆ°ä¸ç¡®å®šæ€§æ¯”å›¾ 1 ä¸­è¦å¤§ï¼Œå› ä¸ºæŒ‡æ•°æ ¸é€šè¿‡[åŒ…å«æ›´å¤šåŸºç¡€å‡½æ•°](https://en.wikipedia.org/wiki/Radial_basis_function_kernel)æä¾›äº†æ›´å¤§çš„çµæ´»æ€§ï¼Œä½¿å¾—æ›´å¤šçš„å‡½æ•°å€¼å˜å¾—åˆç†ã€‚
- en: What we are asking is where in the domain do we think the maximum is. In particular,
    the maximum of the **upper confidence bound** provides information about where
    in the domain the highest function values can be expected. Using this upper confidence
    bound
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¦é—®çš„æ˜¯åœ¨é¢†åŸŸä¸­æˆ‘ä»¬è®¤ä¸ºæœ€å¤§å€¼çš„ä½ç½®ã€‚ç‰¹åˆ«æ˜¯ï¼Œ**ä¸Šç½®ä¿¡ç•Œ**çš„æœ€å¤§å€¼æä¾›äº†æœ‰å…³é¢†åŸŸä¸­å¯ä»¥æœŸå¾…çš„æœ€é«˜å‡½æ•°å€¼çš„ä¿¡æ¯ã€‚ä½¿ç”¨è¿™ä¸ªä¸Šç½®ä¿¡ç•Œ
- en: xâ€² = arg max Î¼*(x) + *Î¸â‚œ* âˆšk*(x, x), s.t. x âˆˆ ğ’³,
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: xâ€² = arg max Î¼*(x) + *Î¸â‚œ* âˆšk*(x, x), s.t. x âˆˆ ğ’³,
- en: as an acquisition function, we can acquire a plausible maximizer of *f*(x),
    i.e., a point xâ€² at which we believe (or expect) the optimum of *f*(x) to be â€”
    with our current knowledge about *f*(x). The factor *Î¸â‚œ* balances exploration
    and exploitation. On the one hand, we get stuck in local optima, i.e., in the
    proximity of previously discovered maxima if we are too exploitative (low *Î¸â‚œ*).
    On the other hand, we will use less information from previously identified optima
    if we are too explorative (high *Î¸â‚œ*).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºä¸€ç§è·å–å‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥è·å¾— *f*(x) çš„åˆç†æœ€å¤§åŒ–ç‚¹ï¼Œå³æˆ‘ä»¬ç›¸ä¿¡ï¼ˆæˆ–æœŸæœ›ï¼‰*f*(x) çš„æœ€ä¼˜ç‚¹ xâ€²â€”â€”åŸºäºæˆ‘ä»¬å½“å‰å¯¹ *f*(x) çš„çŸ¥è¯†ã€‚å› å­
    *Î¸â‚œ* å¹³è¡¡äº†æ¢ç´¢ä¸åˆ©ç”¨ã€‚ä¸€æ–¹é¢ï¼Œå¦‚æœæˆ‘ä»¬è¿‡äºåå‘åˆ©ç”¨ï¼ˆä½ *Î¸â‚œ*ï¼‰ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šé™·å…¥å±€éƒ¨æœ€ä¼˜ï¼Œå³ä¹‹å‰å‘ç°çš„æœ€å¤§å€¼é™„è¿‘ã€‚å¦ä¸€æ–¹é¢ï¼Œå¦‚æœæˆ‘ä»¬è¿‡äºåå‘æ¢ç´¢ï¼ˆé«˜
    *Î¸â‚œ*ï¼‰ï¼Œæˆ‘ä»¬å°†ä¼šå‡å°‘å¯¹ä¹‹å‰è¯†åˆ«å‡ºçš„æœ€ä¼˜ç‚¹çš„ä¿¡æ¯ä½¿ç”¨ã€‚
- en: '[Theoretical results suggest schedulers such as *Î¸â‚œ* âˆ âˆšlog t for optimal performance,
    although leading to excessive exploration](https://arxiv.org/abs/2302.01511).
    Alternatively, *Î¸â‚œ* can be kept constant; to avoid falling into a local optimum,
    a random experiment can occasionally be run instead of the experiment suggested
    by the acquisition function.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[ç†è®ºç»“æœå»ºè®®è°ƒåº¦å™¨å¦‚ *Î¸â‚œ* âˆ âˆšlog t ä»¥è·å¾—æœ€ä½³æ€§èƒ½ï¼Œå°½ç®¡è¿™ä¼šå¯¼è‡´è¿‡åº¦æ¢ç´¢](https://arxiv.org/abs/2302.01511)ã€‚å¦ä¸€ç§é€‰æ‹©æ˜¯ä¿æŒ
    *Î¸â‚œ* ä¸å˜ï¼›ä¸ºäº†é¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼Œå¯ä»¥å¶å°”è¿›è¡Œéšæœºå®éªŒï¼Œè€Œä¸æ˜¯æ‰§è¡Œè·å–å‡½æ•°å»ºè®®çš„å®éªŒã€‚'
- en: Examples
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹
- en: An example is illustrated in the code block below; â€œGPâ€ refers to the Gaussian
    process model instance, be it from [*scikit-learn*](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html#sklearn.gaussian_process.GaussianProcessRegressor),
    [*GPy*](https://gpy.readthedocs.io/en/deploy/GPy.models.html#module-GPy.models.gp_regression),
    or [*GPflow*](https://gpflow.org).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢çš„ä»£ç å—ä¸­å±•ç¤ºäº†ä¸€ä¸ªç¤ºä¾‹ï¼›â€œGPâ€æŒ‡çš„æ˜¯é«˜æ–¯è¿‡ç¨‹æ¨¡å‹å®ä¾‹ï¼Œæ— è®ºæ˜¯æ¥è‡ª[*scikit-learn*](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html#sklearn.gaussian_process.GaussianProcessRegressor)ã€[*GPy*](https://gpy.readthedocs.io/en/deploy/GPy.models.html#module-GPy.models.gp_regression)
    è¿˜æ˜¯ [*GPflow*](https://gpflow.org)ã€‚
- en: '[PRE0]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Since this is not a convex problem, it is strongly recommended to run the optimization
    several times and to pick the best solution. In a second step, we then conduct
    an experiment under conditions xâ€², add the freshly collected data {xâ€², yâ€²} to
    our dataset, ğ’Ÿ â† ğ’Ÿ âˆª {xâ€², yâ€²}, and refit our model (Fig. 3). We can see that after
    one such iteration, we have not quite reached the optimum yet, but we have learned
    more about *f*(x). If we repeated the same process and asked for a new experimental
    condition, we may already be done.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºè¿™ä¸æ˜¯ä¸€ä¸ªå‡¸ä¼˜åŒ–é—®é¢˜ï¼Œå¼ºçƒˆå»ºè®®å¤šæ¬¡è¿è¡Œä¼˜åŒ–ï¼Œå¹¶é€‰æ‹©æœ€ä½³è§£å†³æ–¹æ¡ˆã€‚åœ¨ç¬¬äºŒæ­¥ä¸­ï¼Œæˆ‘ä»¬åœ¨æ¡ä»¶ xâ€² ä¸‹è¿›è¡Œå®éªŒï¼Œå°†æ–°æ”¶é›†çš„æ•°æ® {xâ€², yâ€²} æ·»åŠ åˆ°æ•°æ®é›†
    ğ’Ÿ ä¸­ï¼Œğ’Ÿ â† ğ’Ÿ âˆª {xâ€², yâ€²}ï¼Œç„¶åé‡æ–°æ‹Ÿåˆæˆ‘ä»¬çš„æ¨¡å‹ï¼ˆå›¾ 3ï¼‰ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œåœ¨è¿›è¡Œä¸€æ¬¡è¿™æ ·çš„è¿­ä»£åï¼Œæˆ‘ä»¬å°šæœªè¾¾åˆ°æœ€ä¼˜ï¼Œä½†æˆ‘ä»¬å¯¹ *f*(x)
    æœ‰äº†æ›´å¤šäº†è§£ã€‚å¦‚æœæˆ‘ä»¬é‡å¤ç›¸åŒçš„è¿‡ç¨‹å¹¶è¦æ±‚æ–°çš„å®éªŒæ¡ä»¶ï¼Œæˆ‘ä»¬å¯èƒ½å·²ç»å®Œæˆã€‚
- en: '![](../Images/bebf65811ddd12b6367772c20be8e336.png)![](../Images/a7d2da11a36296221dbcd0ec470a4cd7.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bebf65811ddd12b6367772c20be8e336.png)![](../Images/a7d2da11a36296221dbcd0ec470a4cd7.png)'
- en: '**Fig. 3:** Predictive posterior distribution (Î¼ Â± 2Ïƒ) and updated predictive
    posterior distribution (Î¼ Â± 2Ïƒ) with a Gaussian process. Â© Georgi Tancev.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 3ï¼š** é¢„æµ‹åéªŒåˆ†å¸ƒï¼ˆÎ¼ Â± 2Ïƒï¼‰å’Œæ›´æ–°åçš„é¢„æµ‹åéªŒåˆ†å¸ƒï¼ˆÎ¼ Â± 2Ïƒï¼‰ï¼Œä½¿ç”¨é«˜æ–¯è¿‡ç¨‹ã€‚Â© Georgi Tancevã€‚'
- en: Let us look at another dataset, i.e., realization from the same distribution,
    before and after acquiring a new data point (Fig. 4).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¥çœ‹å¦ä¸€ä¸ªæ•°æ®é›†ï¼Œå³æ¥è‡ªåŒä¸€åˆ†å¸ƒçš„å®ä¾‹ï¼Œåœ¨è·å–æ–°æ•°æ®ç‚¹ä¹‹å‰å’Œä¹‹åï¼ˆå›¾ 4ï¼‰ã€‚
- en: '![](../Images/e33609f08a2ee328f9f08c0f6fcca729.png)![](../Images/bbcb86cd731174db0e35a23ba28e3bf9.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e33609f08a2ee328f9f08c0f6fcca729.png)![](../Images/bbcb86cd731174db0e35a23ba28e3bf9.png)'
- en: '**Fig. 4:** Predictive posterior distribution (Î¼ Â± 2Ïƒ) and updated predictive
    posterior distribution (Î¼ Â± 2Ïƒ) with a Gaussian process. Â© Georgi Tancev.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 4ï¼š** é¢„æµ‹åéªŒåˆ†å¸ƒï¼ˆÎ¼ Â± 2Ïƒï¼‰å’Œæ›´æ–°åçš„é¢„æµ‹åéªŒåˆ†å¸ƒï¼ˆÎ¼ Â± 2Ïƒï¼‰ï¼Œä½¿ç”¨é«˜æ–¯è¿‡ç¨‹ã€‚Â© Georgi Tancevã€‚'
- en: In this example, the acquisition function suggests that the maximum of *f*(x)
    is at the border of the domain. After performing this experiment, we realize that
    this seems not to be the case. However, we have learned with this experiment,
    and in the following experiment we will then identify the maximizer of *f*(x).
    We can show this with a final dataset (Fig. 5).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œè·å–å‡½æ•°å»ºè®® *f*(x) çš„æœ€å¤§å€¼ä½äºåŸŸçš„è¾¹ç•Œã€‚è¿›è¡Œæ­¤å®éªŒåï¼Œæˆ‘ä»¬å‘ç°æƒ…å†µä¼¼ä¹å¹¶éå¦‚æ­¤ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬é€šè¿‡è¿™ä¸ªå®éªŒè·å¾—äº†æ›´å¤šä¿¡æ¯ï¼Œåœ¨æ¥ä¸‹æ¥çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬å°†è¯†åˆ«
    *f*(x) çš„æœ€å¤§å€¼ç‚¹ã€‚æˆ‘ä»¬å¯ä»¥ç”¨æœ€ç»ˆæ•°æ®é›†ï¼ˆå›¾ 5ï¼‰æ¥å±•ç¤ºè¿™ä¸€ç‚¹ã€‚
- en: '![](../Images/baedfbc2521fb97fce7791a0d7cfff62.png)![](../Images/cecf4a90fcf0d9ed6e62bf9edaa816c8.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/baedfbc2521fb97fce7791a0d7cfff62.png)![](../Images/cecf4a90fcf0d9ed6e62bf9edaa816c8.png)'
- en: '**Fig. 5:** Predictive posterior distribution (Î¼ Â± 2Ïƒ) and updated predictive
    posterior distribution (Î¼ Â± 2Ïƒ) with a Gaussian process. Â© Georgi Tancev.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 5ï¼š** é¢„æµ‹åéªŒåˆ†å¸ƒï¼ˆÎ¼ Â± 2Ïƒï¼‰å’Œæ›´æ–°åçš„é¢„æµ‹åéªŒåˆ†å¸ƒï¼ˆÎ¼ Â± 2Ïƒï¼‰ï¼Œä½¿ç”¨é«˜æ–¯è¿‡ç¨‹ã€‚Â© Georgi Tancevã€‚'
- en: '[The upper confidence bound is not the only acquisition function available](https://proceedings.neurips.cc/paper_files/paper/2018/hash/498f2c21688f6451d9f5fd09d53edda7-Abstract.html).
    Other popular choices are **Thompson sampling**, **probability of improvement**,
    or **expected improvement**. They differ, for example, in how they balance exploration
    and exploitation. Whether and how fast we actually end up identifying (i.e., converging
    to) the maximizer also depends on the sequence of *Î¸â‚œ*â€™s and on a correct model
    specification (i.e., kernel choice).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[ä¸Šç½®ä¿¡ç•Œå¹¶ä¸æ˜¯å”¯ä¸€å¯ç”¨çš„è·å–å‡½æ•°](https://proceedings.neurips.cc/paper_files/paper/2018/hash/498f2c21688f6451d9f5fd09d53edda7-Abstract.html)ã€‚å…¶ä»–æµè¡Œçš„é€‰æ‹©åŒ…æ‹¬
    **æ±¤æ™®æ£®é‡‡æ ·**ã€**æ”¹è¿›æ¦‚ç‡** æˆ– **æœŸæœ›æ”¹è¿›**ã€‚å®ƒä»¬åœ¨å¹³è¡¡æ¢ç´¢å’Œå¼€å‘æ–¹é¢æœ‰æ‰€ä¸åŒã€‚æˆ‘ä»¬æœ€ç»ˆè¯†åˆ«ï¼ˆå³æ”¶æ•›åˆ°ï¼‰æœ€å¤§å€¼çš„é€Ÿåº¦ä¹Ÿå–å†³äº *Î¸â‚œ* çš„åºåˆ—å’Œæ­£ç¡®çš„æ¨¡å‹è§„æ ¼ï¼ˆå³æ ¸é€‰æ‹©ï¼‰ã€‚'
- en: This is just a very short summary of the things we need to know about Gaussian
    processes, acquisition functions, and Bayesian optimization. Evidently, many details
    had to be left out. For other topics (kernel engineering, optimization of kernel
    parameters, and so on) it is worth taking a look at the relevant literature. Let
    us now move on to a short case study on multi-fidelity information.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åªæ˜¯å…³äºé«˜æ–¯è¿‡ç¨‹ã€è·å–å‡½æ•°å’Œè´å¶æ–¯ä¼˜åŒ–çš„éå¸¸ç®€çŸ­çš„æ€»ç»“ã€‚æ˜¾ç„¶ï¼Œè®¸å¤šç»†èŠ‚è¢«çœç•¥äº†ã€‚å¯¹äºå…¶ä»–ä¸»é¢˜ï¼ˆæ ¸å·¥ç¨‹ã€æ ¸å‚æ•°ä¼˜åŒ–ç­‰ï¼‰ï¼Œå€¼å¾—æŸ¥çœ‹ç›¸å…³æ–‡çŒ®ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è½¬åˆ°å…³äºå¤šä¿çœŸä¿¡æ¯çš„ç®€çŸ­æ¡ˆä¾‹ç ”ç©¶ã€‚
- en: Multi-Fidelity Information
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤šä¿çœŸä¿¡æ¯
- en: In practice, we may have partial knowledge about our system, i.e., data of lower
    fidelity from a simple model. For instance, we may have a numerical simulations,
    or mathematical model with a subset of potential effects or within a subset of
    our domain. It would be sensible to include that somehow in our experimental designs.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬å¯èƒ½å¯¹ç³»ç»Ÿæœ‰éƒ¨åˆ†äº†è§£ï¼Œå³æ¥è‡ªç®€å•æ¨¡å‹çš„æ•°æ®ï¼Œæˆ–ä¾‹å¦‚æ•°å€¼æ¨¡æ‹Ÿï¼Œæˆ–è€…åŒ…å«æ½œåœ¨æ•ˆåº”å­é›†çš„æ•°å­¦æ¨¡å‹ã€‚è¿™äº›ä¿¡æ¯åœ¨å®éªŒè®¾è®¡ä¸­çº³å…¥å°†æ˜¯æ˜æ™ºçš„ã€‚
- en: Let us assume that a data point can come either from a model or from an experiment,
    resulting in multi-fidelity data. Furthermore, we have to correct our model for
    data points that come from experiments. [Thus, we introduce a new variable *w*
    that tracks the origin of a data point; it will be the case that *w* = 1 if the
    sample comes from an experiment, and *w* = 0 otherwise](https://arxiv.org/abs/1703.01250).
    Then, we can decompose our function as follows.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾ä¸€ä¸ªæ•°æ®ç‚¹å¯èƒ½æ¥è‡ªæ¨¡å‹æˆ–å®éªŒï¼Œä»è€Œäº§ç”Ÿå¤šä¿çœŸæ•°æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜éœ€è¦å¯¹æ¥è‡ªå®éªŒçš„æ•°æ®ç‚¹çº æ­£æ¨¡å‹ã€‚[å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„å˜é‡ *w* æ¥è¿½è¸ªæ•°æ®ç‚¹çš„æ¥æºï¼›å¦‚æœæ ·æœ¬æ¥è‡ªå®éªŒï¼Œ*w*
    = 1ï¼Œå¦åˆ™ *w* = 0](https://arxiv.org/abs/1703.01250)ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥å¦‚ä¸‹åˆ†è§£æˆ‘ä»¬çš„å‡½æ•°ã€‚
- en: '*f* = *fâ‚˜* + *w* â‹… *fáµ£*'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*f* = *fâ‚˜* + *w* â‹… *fáµ£*'
- en: '*fâ‚˜* refers to the contribution from our simple model and *fáµ£* is a correction
    term, which is only present for laboratory experiments (*w* = 1). By requesting
    **independence** (i.e., orthogonality) between *fâ‚˜* and *fáµ£*, [the resulting kernel
    will be](https://www.cs.toronto.edu/~duvenaud/cookbook/)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*fâ‚˜* æŒ‡çš„æ˜¯æˆ‘ä»¬ç®€å•æ¨¡å‹çš„è´¡çŒ®ï¼Œè€Œ *fáµ£* æ˜¯ä¸€ä¸ªä¿®æ­£é¡¹ï¼Œåªåœ¨å®éªŒå®¤å®éªŒä¸­å­˜åœ¨ï¼ˆ*w* = 1ï¼‰ã€‚é€šè¿‡è¦æ±‚ **ç‹¬ç«‹æ€§**ï¼ˆå³æ­£äº¤æ€§ï¼‰åœ¨ *fâ‚˜*
    å’Œ *fáµ£* ä¹‹é—´ï¼Œ[å¾—åˆ°çš„æ ¸å‡½æ•°å°†ä¼šæ˜¯](https://www.cs.toronto.edu/~duvenaud/cookbook/)ã€‚'
- en: '*k*(xâ‚, xâ‚‚) = *kâ‚˜*(xâ‚, xâ‚‚) + *w*â‚*w*â‚‚ â‹… *káµ£*(xâ‚, xâ‚‚).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*(xâ‚, xâ‚‚) = *kâ‚˜*(xâ‚, xâ‚‚) + *w*â‚*w*â‚‚ â‹… *káµ£*(xâ‚, xâ‚‚)ã€‚'
- en: 'In *GPy*, we can define this as follows (assume *w* is in the last dimension
    of an array of length 2):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ *GPy* ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å¦‚ä¸‹å®šä¹‰ï¼ˆå‡è®¾ *w* åœ¨é•¿åº¦ä¸º 2 çš„æ•°ç»„çš„æœ€åä¸€ä¸ªç»´åº¦ä¸­ï¼‰ï¼š
- en: '[PRE1]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The noise is handled by the *GPRegression* class; we donâ€™t need to specify it
    unless we expect different noise terms for experiments and models. Fig. 6 illustrates
    an example with the resulting Gaussian process, trained only on some model data.
    Evidently, our knowledge fails at the borders of our domain, as the ground truth
    deviates from our simple model and the resulting low-fidelity data.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: å™ªå£°ç”± *GPRegression* ç±»å¤„ç†ï¼›é™¤éæˆ‘ä»¬æœŸæœ›å®éªŒå’Œæ¨¡å‹æœ‰ä¸åŒçš„å™ªå£°é¡¹ï¼Œå¦åˆ™ä¸éœ€è¦æŒ‡å®šã€‚å›¾ 6 è¯´æ˜äº†ä¸€ä¸ªä»…åœ¨æŸäº›æ¨¡å‹æ•°æ®ä¸Šè®­ç»ƒçš„é«˜æ–¯è¿‡ç¨‹ç¤ºä¾‹ã€‚æ˜¾ç„¶ï¼Œæˆ‘ä»¬çš„çŸ¥è¯†åœ¨é¢†åŸŸè¾¹ç•Œå¤„å¤±æ•ˆï¼Œå› ä¸ºçœŸå®æƒ…å†µåç¦»äº†æˆ‘ä»¬çš„ç®€å•æ¨¡å‹å’Œå¾—åˆ°çš„ä½ä¿çœŸæ•°æ®ã€‚
- en: '![](../Images/e667d693e8e55f350f2670e317407717.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e667d693e8e55f350f2670e317407717.png)'
- en: '**Fig. 6:** Multi-fidelity data. Â© Georgi Tancev.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 6ï¼š** å¤šä¿çœŸæ•°æ®ã€‚Â© Georgi Tancevã€‚'
- en: We are now ready to perform our experiments. We query the acquisition function
    for proposals. This is shown in the simulations below (Fig. 7) with two different
    *Î¸â‚œ*â€™s.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å‡†å¤‡è¿›è¡Œå®éªŒã€‚æˆ‘ä»¬æŸ¥è¯¢è·å–å‡½æ•°ä»¥è·å¾—å»ºè®®ã€‚è¿™åœ¨ä¸‹é¢çš„æ¨¡æ‹Ÿï¼ˆå›¾ 7ï¼‰ä¸­æ˜¾ç¤ºäº†ä¸¤ä¸ªä¸åŒçš„ *Î¸â‚œ*ã€‚
- en: '![](../Images/34912729791d2de269aeb7582b6a9d83.png)![](../Images/710456abf9260156c750a600659b802e.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/34912729791d2de269aeb7582b6a9d83.png)![](../Images/710456abf9260156c750a600659b802e.png)'
- en: '**Fig. 7**: Bayesian optimization with multi-fidelity data; **left**: *Î¸â‚œ=0.75,*
    ***right****:* *Î¸â‚œ=0.95*. Â© Georgi Tancev.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 7**ï¼šå…·æœ‰å¤šä¿çœŸåº¦æ•°æ®çš„è´å¶æ–¯ä¼˜åŒ–ï¼›**å·¦**ï¼š*Î¸â‚œ=0.75,* ***å³***ï¼š*Î¸â‚œ=0.95*ã€‚Â© Georgi Tancevã€‚'
- en: The acquisition starts directly at the borders of the domain, as this is where
    we expect the maximizer to be according to the behavior of our low-fidelity data
    (Fig. 6). In both cases, we find the maximum relatively quickly. If we notice
    that successive experiments are relatively close to each other, we can assume
    that we are done and can stop â€” even before we have used up our budget.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: è·å–è¿‡ç¨‹ç›´æ¥ä»é¢†åŸŸè¾¹ç•Œå¼€å§‹ï¼Œå› ä¸ºæ ¹æ®æˆ‘ä»¬ä½ä¿çœŸæ•°æ®çš„è¡Œä¸ºï¼ˆå›¾ 6ï¼‰ï¼Œæˆ‘ä»¬æœŸæœ›æœ€å¤§å€¼å‡ºç°åœ¨è¿™ä¸ªä½ç½®ã€‚åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éƒ½ç›¸å¯¹è¾ƒå¿«åœ°æ‰¾åˆ°æœ€å¤§å€¼ã€‚å¦‚æœæˆ‘ä»¬æ³¨æ„åˆ°è¿ç»­å®éªŒä¹‹é—´çš„è·ç¦»ç›¸å¯¹è¾ƒè¿‘ï¼Œæˆ‘ä»¬å¯ä»¥å‡è®¾å®éªŒå·²ç»å®Œæˆï¼Œå¯ä»¥åœæ­¢â€”å³ä½¿åœ¨æˆ‘ä»¬è€—å°½é¢„ç®—ä¹‹å‰ã€‚
- en: Conclusion
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: Having reached the end of the article, I am confident that I have been able
    to convince you about the merits of Bayesian optimization, which is currently
    a large research field. I personally believe that such tools will have a wide
    application in decision support in the future, especially in experimental science
    and engineering due to their ability to process knowledge from different sources.
    In this way, time and money can be saved, and new ideas can be generated, for
    example experiments that no one has ever thought of before.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°è¾¾æ–‡ç« æœ«å°¾ï¼Œæˆ‘ç›¸ä¿¡æˆ‘å·²ç»èƒ½å¤Ÿè®©ä½ ä¿¡æœè´å¶æ–¯ä¼˜åŒ–çš„ä¼˜ç‚¹ï¼Œè¿™åœ¨å½“å‰æ˜¯ä¸€ä¸ªå¤§å‹ç ”ç©¶é¢†åŸŸã€‚æˆ‘ä¸ªäººè®¤ä¸ºï¼Œè¿™äº›å·¥å…·åœ¨æœªæ¥å†³ç­–æ”¯æŒä¸­å°†å…·æœ‰å¹¿æ³›çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å®éªŒç§‘å­¦å’Œå·¥ç¨‹é¢†åŸŸï¼Œå› ä¸ºå®ƒä»¬èƒ½å¤Ÿå¤„ç†æ¥è‡ªä¸åŒæ¥æºçš„çŸ¥è¯†ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥èŠ‚çœæ—¶é—´å’Œé‡‘é’±ï¼Œäº§ç”Ÿæ–°çš„æƒ³æ³•ï¼Œä¾‹å¦‚ä»¥å‰ä»æœªæœ‰äººæƒ³åˆ°çš„å®éªŒã€‚
