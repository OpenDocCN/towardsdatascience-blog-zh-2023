- en: 'ChatGPT: Automated Prompt Scoring'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/chatgpt-automated-prompt-scoring-c972f9ee2c4f?source=collection_archive---------4-----------------------#2023-04-10](https://towardsdatascience.com/chatgpt-automated-prompt-scoring-c972f9ee2c4f?source=collection_archive---------4-----------------------#2023-04-10)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/1b9b1e6bc46d2e359537b4b1d0aff68d.png)'
  prefs: []
  type: TYPE_IMG
- en: This image was created with the assistance of DALL·E 2
  prefs: []
  type: TYPE_NORMAL
- en: Guide
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How to objectively choose and improve your ChatGPT prompts using python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://michael-malin.medium.com/?source=post_page-----c972f9ee2c4f--------------------------------)[![Michael
    Malin](../Images/070604c68a50e8f2996f2c8837df3ec9.png)](https://michael-malin.medium.com/?source=post_page-----c972f9ee2c4f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c972f9ee2c4f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c972f9ee2c4f--------------------------------)
    [Michael Malin](https://michael-malin.medium.com/?source=post_page-----c972f9ee2c4f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8225885ee2a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchatgpt-automated-prompt-scoring-c972f9ee2c4f&user=Michael+Malin&userId=8225885ee2a7&source=post_page-8225885ee2a7----c972f9ee2c4f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c972f9ee2c4f--------------------------------)
    ·10 min read·Apr 10, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc972f9ee2c4f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchatgpt-automated-prompt-scoring-c972f9ee2c4f&user=Michael+Malin&userId=8225885ee2a7&source=-----c972f9ee2c4f---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc972f9ee2c4f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchatgpt-automated-prompt-scoring-c972f9ee2c4f&source=-----c972f9ee2c4f---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: Large language models (LLM) like ChatGPT are having a huge impact. They are
    also just the beginning. Over the next year, companies big and small will begin
    to roll out domain/persona specialized LLM models. Indeed, this is already becoming
    a reality with new products like the finance-specialized [BloombergGPT](https://www.bloomberg.com/company/press/bloomberggpt-50-billion-parameter-llm-tuned-finance/)
    and Microsoft’s developer-focused [Copilot](https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/).
    We will soon see AI personal trainers, health coaches, councilors, legal assistants,
    and many more. While some cases may require fine-tuned models on domain-specific
    data, the majority can be accomplished with simple prompt engineering. But how
    do you know when your prompt is good enough? How can we generate objective accuracy
    scores on subjective text?
  prefs: []
  type: TYPE_NORMAL
- en: 'This guide will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Theory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt Testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt Scoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt Feedback
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Theory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The difficulty in testing LLM prompt outputs is that the outcomes are subjective.
    I may find the results perfect while you find them subpar. Both opinions are equally
    valid. This makes a purely scientific approach to scoring very…
  prefs: []
  type: TYPE_NORMAL
