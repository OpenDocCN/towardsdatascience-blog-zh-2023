- en: What Does It Mean When Machine Learning Makes a Mistake?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/what-does-it-mean-when-machine-learning-makes-a-mistake-37b213200697?source=collection_archive---------2-----------------------#2023-09-17](https://towardsdatascience.com/what-does-it-mean-when-machine-learning-makes-a-mistake-37b213200697?source=collection_archive---------2-----------------------#2023-09-17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Do our definitions of “mistake” make sense when it comes to ML/AI? If not, why
    not?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@s.kirmer?source=post_page-----37b213200697--------------------------------)[![Stephanie
    Kirmer](../Images/f9d9ef9167febde974c223dd4d8d6293.png)](https://medium.com/@s.kirmer?source=post_page-----37b213200697--------------------------------)[](https://towardsdatascience.com/?source=post_page-----37b213200697--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----37b213200697--------------------------------)
    [Stephanie Kirmer](https://medium.com/@s.kirmer?source=post_page-----37b213200697--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa8dc77209ef3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-does-it-mean-when-machine-learning-makes-a-mistake-37b213200697&user=Stephanie+Kirmer&userId=a8dc77209ef3&source=post_page-a8dc77209ef3----37b213200697---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----37b213200697--------------------------------)
    ·11 min read·Sep 17, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F37b213200697&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-does-it-mean-when-machine-learning-makes-a-mistake-37b213200697&user=Stephanie+Kirmer&userId=a8dc77209ef3&source=-----37b213200697---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F37b213200697&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-does-it-mean-when-machine-learning-makes-a-mistake-37b213200697&source=-----37b213200697---------------------bookmark_footer-----------)![](../Images/f53d085f3208ccdbd80d9f536cade3d6.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Kind and Curious](https://unsplash.com/@kindandcurious?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: A comment on my recent post about the public perception of machine learning
    got me thinking about the meaning of error in machine learning. The reader asked
    if I thought machine learning models would always “make mistakes”. As I described
    in that post, people have a strong tendency to anthropomorphize machine learning
    models. When we interact with an LLM chatbot, we apply techniques to those engagements
    that we have learned by communicating with other people—persuasion, phrasing,
    argument, etc. However, this is often ineffective and will result in unsatisfying
    responses.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我最近关于机器学习公众认知的帖子中有一个评论让我思考机器学习中“错误”的含义。读者问我是否认为机器学习模型会一直“犯错误”。正如我在那篇帖子中所描述的，人们有很强的倾向将机器学习模型拟人化。当我们与LLM聊天机器人互动时，我们会运用从与其他人交流中学到的技巧——说服、措辞、论证等。然而，这通常效果不好，最终会导致不满意的回应。
- en: In my own day to day work, I see similar sorts of issues related to classifiers
    and regression models as well. My team and I spend a lot of time and energy trying
    to help customers and colleagues understand that machine learning is not perfect
    (and realistically never will be). “Why did the model say X when the truth turned
    out to be X-5?” is a perpetual theme. I don’t blame the question askers, at least
    not entirely, because as I wrote in my last piece, we in the broader machine learning
    community have not done a good job of teaching fundamental ML literacy.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在我自己日常的工作中，我也会看到与分类器和回归模型相关的类似问题。我和我的团队花费了大量的时间和精力来帮助客户和同事理解机器学习并不完美（而且现实中也永远不会完美）。“为什么模型说X，而事实是X-5？”是一个永恒的主题。我不完全责怪提问者，因为正如我在上一篇文章中所写，我们广泛的机器学习社区在教授基础机器学习素养方面做得不够好。
- en: But this brings up a core question that needs more interrogation before we can
    really fix that literacy problem.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 但这提出了一个核心问题，我们需要更多的探讨才能真正解决这个素养问题。
- en: What do we mean (and what do other people mean) when they say that a model made
    a mistake, or failed, or hallucinated, or lied?
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当我们说一个模型犯了错误、失败了、产生了幻觉或说谎时，这是什么意思（以及其他人是什么意思）？
- en: Before we can answer that, we need to start from the beginning.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能回答这个问题之前，我们需要从头开始。
- en: What is a machine learning model?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是机器学习模型？
- en: In a very, very general sense, a machine learning model is an algorithm that
    accepts some input or prompt and returns some response that is probabilistically
    determined. How it decides what the response should be can vary dramatically -
    it might use a decision tree, or a neural network, or a linear regression, or
    any number of other types of machine learning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个非常、非常通用的角度来看，机器学习模型是一个接受一些输入或提示并返回某种概率确定的响应的算法。它决定响应应该是什么的方式可能会有很大差异——它可能使用决策树、神经网络、线性回归或其他任何类型的机器学习方法。
- en: To create a model, we start with sample data that reflects the results we are
    looking for. The input samples can be all kinds of things—for generative AI, they
    could be large bodies of human-written text, or music or images. For other kinds
    of ML, they could be large datasets containing things like object characteristics,
    or classifications of things like images or texts into categories, or much more.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个模型，我们从反映我们寻找结果的样本数据开始。输入样本可以是各种各样的东西——对于生成式AI，它们可能是大量的人类书写的文本、音乐或图像。对于其他类型的机器学习，它们可能是包含对象特征的大型数据集，或者将图像或文本等分类到类别中的数据集，或更多。
- en: Sometimes those will be “labeled” so that the model learns which ones are desirable
    or not, or which ones fall into a specific category and which other ones don’t.
    Other times, the model will learn patterns in the underlying samples and derive
    their own understanding of those patterns, to either replicate the characteristics
    of inputs, choose between options, divide inputs into groups, or other activities.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，这些回应会被“标记”以便模型学习哪些是可取的，哪些不是，或者哪些属于特定类别，哪些不属于。其他时候，模型会学习底层样本中的模式，并形成自己对这些模式的理解，以复制输入的特征、在选项之间做出选择、将输入分组或进行其他活动。
- en: Ways generative AI models are trained
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成式AI模型的训练方式
- en: The way we train generative models is specific, and more complicated than training
    a model to estimate the simple probability of one answer. Instead, these models
    are estimating the probabilities of many different elements and putting them together
    to create their response. Here are very simple explanations of a few of the ways
    we do this training. (These are all EXTREME oversimplications, so please forgive
    the lack of detail and any generalizations.)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练生成模型的方式是特定的，比训练一个简单的概率模型要复杂得多。相反，这些模型是估计许多不同元素的概率，并将它们组合在一起以生成响应。以下是我们进行这种训练的一些非常简单的解释。（这些都是极度的过度简化，请原谅细节的缺乏和任何概括。）
- en: When we’re generating sound or images, we may use Generative Adversarial Networks.
    Here, models are pitted against each other, with one model generating new content
    and the other model attempting to tell if that content came from a model or not,
    back and forth. The two models compete for thousands and thousands of cases, each
    getting better at their task as they go. In the end, the generating model will
    be able to produce content that is nearly indistinguishable from what reality
    would produce. (The distinguishing model also gets very good at telling when an
    input was human generated, as a side effect.)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们生成声音或图像时，可能会使用生成对抗网络。在这种情况下，模型相互对抗，一个模型生成新内容，另一个模型尝试判断该内容是否来自某个模型，来回交替。这两个模型在成千上万的案例中竞争，每个模型在过程中逐渐变得更好。最终，生成模型将能够产生几乎无法与现实所产生的内容区分开来的内容。（作为副作用，区分模型也会变得非常擅长识别输入是否是人类生成的。）
- en: For LLMs, and text generation like the GPT models, we use what we call Transformers.
    This training involves teaching the model an understanding of how words’ meanings
    relate to each other, as well as how to produce text content that is near-indistinguishable
    from human production. The results sound very convincing because the model knows
    which words are likely to go together (with probabilities based on how real human
    language used in training has put them together).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大型语言模型（LLMs）和像 GPT 模型这样的文本生成，我们使用所谓的 Transformer。这种训练包括教模型理解单词的意义如何相互关联，以及如何生成接近于人类生产的文本内容。结果听起来非常有说服力，因为模型知道哪些单词可能会一起出现（基于训练中真实人类语言如何将它们组合在一起的概率）。
- en: To generate images from text inputs, like Dall-E, we use Diffusion. In this
    case, we teach the model to calculate what kinds of features of an image are most
    probably desired, based on the text that’s been provided. The model essentially
    starts with an image that’s just static, and based on your text, applies details/colors/features.
    This is based on what it has learned about how text *usually* corresponds to images,
    according to its training data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从文本输入生成图像，例如 Dall-E，我们使用扩散。在这种情况下，我们教模型计算图像中最可能需要的特征，基于所提供的文本。模型本质上从一个静态图像开始，根据你的文本应用细节/颜色/特征。这是基于模型关于文本*通常*如何与图像对应的学习结果。
- en: Using these techniques, we teach models to decipher patterns in inputs — sometimes
    patterns we ourselves can’t even really explain or detect (especially true for
    deep learning), and then the model is able to interpret and apply those patterns.
    All of this is mathematics under the surface, even though the patterns may be
    in text, or images, or many other things.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些技术，我们教会模型解读输入中的模式——有时是我们自己甚至无法真正解释或检测的模式（尤其对于深度学习来说），然后模型能够解释和应用这些模式。所有这些在表面下都是数学，即使模式可能存在于文本、图像或许多其他事物中。
- en: Now that we know this, we can talk about what the outputs are, and what it means
    when the output isn’t what we wanted.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了这些内容，我们可以讨论输出是什么，以及当输出不是我们想要的结果时意味着什么。
- en: Outputs
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输出
- en: The things a machine learning model produces can vary widely. Generative AI
    in particular produces images, video, audio, and text of every imaginable kind.
    Other kinds of models give us the likelihood of events/phenomena, estimates of
    unknown values, translations of text into different languages, labels or groupings
    for content, and more.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型生成的东西可以有很大的不同。特别是生成型人工智能生成的图像、视频、音频和各种文本无所不包。其他类型的模型则给出事件/现象的可能性、未知值的估计、文本翻译成不同语言、内容的标签或分组等等。
- en: In all of these cases, complex mathematical calculations are employed to estimate
    the best response based on the given input. “Best” is a very specific thing, however.
    In the process of creating the model, you have indicated to the model what character
    you want its responses to have.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些情况下，复杂的数学计算被用来根据给定输入估算最佳响应。然而，“最佳”是一个非常具体的概念。在创建模型的过程中，你已经向模型指明了你希望其响应具备什么特性。
- en: In the process of creating the model, you have indicated to the model what character
    you want its responses to have.
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在创建模型的过程中，你已经向模型指明了你希望其响应具备什么特性。
- en: What does it mean when we get something we didn’t expect?
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 当我们得到意料之外的结果时，这意味着什么？
- en: '**This is as much about us as it is about the model.** It’s essentially like
    any product in the tech space. The product designers and creators put together
    “user stories” when they are developing something to sell to folks, and this is
    comprised of narratives about who will use this product, how, and why, and what
    they will want to get out of it.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**这与我们本身一样重要，也与模型一样重要。** 这基本上就像任何技术领域的产品一样。产品设计师和创作者在开发要出售给用户的产品时，会制定“用户故事”，这些故事包括谁将使用该产品、如何使用以及为什么使用，以及他们希望从中获得什么。'
- en: For example, say we’re designing a spreadsheet tool. We’d use user stories to
    think about Anne, the accountant, and we’d talk to accountants to decide what
    kind of features an accountant needs in their spreadsheet software. Then we’d
    think about Bob, the business analyst, and we’d talk to BI analysts about what
    their feature needs would be. We’d put all these on our list when planning the
    spreadsheet tool and use this to guide our design. You get the idea.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们正在设计一个电子表格工具。我们会使用用户故事来考虑安妮这位会计师，并与会计师交谈，以确定会计师在电子表格软件中需要什么样的功能。然后我们会考虑商业分析师鲍勃，并与商业智能分析师讨论他们的功能需求。我们会将所有这些列入电子表格工具的规划清单，并以此来指导我们的设计。你明白了吧。
- en: '**Who’s the user for a machine learning model?** It depends completely on what
    kind of model it is. If your model, for example, predicts home prices based on
    property features, it might be realtors or mortgage lenders or home buyers. A
    relatively specific model that has clear, bounded applications is easy to tailor
    for the user. We data scientists can make sure this model meets the expectations
    of people who will be using it.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**谁是机器学习模型的用户？** 这完全取决于模型的类型。例如，如果你的模型预测基于房产特征的房价，可能的用户是房地产经纪人、抵押贷款机构或购房者。具有明确、有限应用范围的模型容易为用户量身定制。我们数据科学家可以确保这个模型满足使用者的期望。'
- en: Sometimes the prediction will not be accurate, but that’s a mathematical problem,
    and we can likely decipher why that happened. Maybe we gave the model wrong data,
    or this house is exceptional for some reason we couldn’t tell the model, for example.
    If the model has never been taught to interpret the effect of, say, a backyard
    zoo on house price, it won’t have a way to incorporate that information. What
    if there’s been a housing price crash? We saw that not long ago, and you can imagine
    the patterns the model learned from before the crash would no longer apply.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 有时预测结果可能不准确，但这是一个数学问题，我们可能能够解码为什么会这样。也许我们给模型提供了错误的数据，或者这个房子有某种我们无法告诉模型的特殊原因。例如，如果模型从未被教会如何解释后院动物园对房价的影响，它将无法整合这些信息。如果出现了房价崩溃？我们不久前看到过这样的情况，你可以想象模型从崩溃前学到的模式将不再适用。
- en: 'However, in a case like this, we have two things:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在这种情况下，我们有两个问题：
- en: A clear goal the model is meant to achieve, that the data scientists and users
    are both aware of;
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型旨在实现的明确目标，数据科学家和用户都知道；
- en: A quantifiable way to measure whether the model got close to its goal or not.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一种可量化的方法来衡量模型是否接近其目标。
- en: This means it is clear and straightforward when we want to define whether the
    model was successful. After we make that determination, we can explore why the
    model did what it did — this is what’s called “model explainability” or “model
    interpretability” in the field.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着当我们想要定义模型是否成功时，这一过程是明确和简单的。在做出这个判断之后，我们可以探讨模型为何会有这样的表现——这就是在该领域中所说的“模型可解释性”或“模型解释性”。
- en: But for LLMs?
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 但对于LLMs（大语言模型）呢？
- en: What does this whole framework mean for something like an LLM? Who’s the user
    for ChatGPT? (Did you just mentally say “everybody”?) When a model’s output can
    be as complex and varied as that of an LLM, we start to have questions.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个整体框架对像LLM这样的东西意味着什么？谁是ChatGPT的用户？（你刚刚在脑海里回答“每个人”了吗？）当模型的输出可以像LLM那样复杂和多样时，我们开始产生疑问。
- en: For the data scientists who built generative AI models, though they may employ
    different training methods, we are generally always trying to create content that
    is as close as possible to the training data, which is usually human or nature
    generated. To make that happen, the model is trained on sample content that people
    or nature DID produce. We do our best to give the model a mathematical way to
    understand how and why this content feels “real”, so it can replicate that. This
    is how a generative AI model becomes able to create efficiencies and make certain
    human work obsolete.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些构建生成性AI模型的数据科学家来说，虽然他们可能采用不同的训练方法，但我们通常始终尝试创造尽可能接近训练数据的内容，这些数据通常是由人类或自然生成的。为了实现这一目标，模型是通过人类或自然产生的样本内容进行训练的。我们尽力给模型一个数学方式来理解这些内容为何感觉“真实”，以便它可以复制这一点。这就是生成性AI模型能够创造效率并使某些人类工作变得过时的方式。
- en: For the data scientists who built generative AI models, the goal is to create
    content that is as close as possible to the training data, which is usually human
    or nature generated.
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对于那些构建生成性AI模型的数据科学家来说，目标是创造尽可能接近训练数据的内容，这些数据通常是由人类或自然生成的。
- en: These models, by and large, are excellent at what they do in this regard! However,
    this creates some pitfalls. Because LLM models are so convincing at imitating
    human responses, users shorthand think of them as like people. It’s like the way
    children learn about animals — you teach a child that a furry creature with four
    legs and a wet nose is a dog, but then you put a cat in front of them, and they’re
    inclined to think that’s probably also a dog, because the basic features seem
    so similar. Only when you explain that the cat is a different thing do they start
    to interpret the differences and build a different mental model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型在这方面做得非常出色！然而，这也带来了一些陷阱。由于LLM模型在模仿人类回应方面非常逼真，用户在思维上容易把它们当作人来看待。这就像孩子学习动物的方式——你教孩子说一只有四条腿和湿鼻子的毛茸茸的动物是狗，但当你把一只猫放在他们面前时，他们会倾向于认为那也是狗，因为基本特征似乎非常相似。只有当你解释猫是另一种动物时，他们才开始理解差异并建立不同的心理模型。
- en: Because these models are so convincing at imitating human responses, users shorthand
    think of them as like people.
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 由于这些模型在模仿人类回应方面非常逼真，用户在思维上容易把它们当作人来看待。
- en: Right now, I think most of the public is still building that different mental
    model to distinguish an LLM from a person. (As I wrote previously, data scientists
    need to be the adult explaining that a dog is not the same as a cat, to carry
    on the metaphor.)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我认为大多数公众仍在建立不同的心理模型，以区分LLM和人类。（正如我之前所写，数据科学家需要像大人一样解释狗和猫的不同，以继续这个比喻。）
- en: But I’ve gotten a little off track. What this really means is that people interacting
    with a very basic model (home prices) understand this is a limited algorithm.
    It’s more like a spreadsheet formula than like a person, and this shapes our expectations.
    But when we use ChatGPT, for example, it carries a lot of the characteristics
    of chatting online with a human being, and this affects us. Instead of expecting
    only limited things, like “text that sounds like human language”, we start expecting
    that statements will always be accurate, that the results will include cohesive
    critical thinking, and that facts from today’s news will be retrievable from the
    model, even though it was trained last year, for example.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 不过我稍微跑题了。这真正意味着与一个非常基础的模型（例如房价）互动的人理解这是一个有限的算法。它更像一个电子表格公式，而不像一个人，这塑造了我们的期望。但当我们使用ChatGPT时，它带有许多人类在线聊天的特征，这影响了我们。我们开始期望陈述总是准确的，结果包括连贯的批判性思维，并且期望从模型中检索到今天新闻的事实，即使它是去年训练的。
- en: '[P]eople interacting with a very basic model understand this is a limited algorithm.
    … But when we use ChatGPT, for example, it carries a lot of the characteristics
    of chatting online with a human being, and this affects us.'
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[P]与一个非常基础的模型互动的人理解这是一个有限的算法。……但当我们使用ChatGPT时，它带有许多人类在线聊天的特征，这影响了我们。'
- en: To the extent that the appearance of critical thinking may occur in model results,
    that’s because the model has learned that the arrangements of text that we interpret
    as “critical thinking” from real human sources sound more “human”, and so it is
    imitating these arrangements for that purpose. When we talk to people, we infer
    from what they say that they are using critical thinking. We can’t do that with
    machine learning, however.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型结果中出现批判性思维的迹象，是因为模型学会了我们从真实人类来源中解释为“批判性思维”的文本排列听起来更“人类”，因此它模仿这些排列以达到这一目的。当我们与人交谈时，我们会从他们说的话中推断出他们正在进行批判性思维。然而，我们不能用机器学习来做到这一点。
- en: 'Remember the two key elements I described above that the house price model
    has:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 记住我上面描述的房价模型的两个关键要素：
- en: 1\. A clear goal the model is meant to achieve, that the data scientists and
    users are both aware of;
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 1\. 模型旨在实现的明确目标，数据科学家和用户都应当了解；
- en: ''
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 2\. A quantifiable way to measure whether the model got close to its goal or
    not.
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 2\. 一种量化衡量模型是否接近其目标的方式。
- en: With generative AI, including but not limited to LLMs, we’ve got problems with
    1, partly because the goal is not actually that clear (“return material that is
    indistinguishable from what a human would produce”) but mostly because the data
    scientists are definitely not communicating successfully to users what that goal
    is. Data scientists working on these models have got number 2, because they are
    using complex mathematical systems to teach the models when they have produced
    sufficiently “real” or human-like content. But for the regular user on the street,
    this is a lot harder. Determining if the model did a good job is much like grading
    papers, rather than checking the result of a math problem. Subjectivity creeps
    in.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生成式 AI，包括但不限于大语言模型，我们在第1点上存在问题，部分原因是目标实际上并不清晰（“返回与人类产生的内容无法区分的材料”），但主要原因是数据科学家确实没有成功地向用户传达这个目标。数据科学家在这些模型上达到了第2点，因为他们使用复杂的数学系统来教模型何时生成足够“真实”或类似人类的内容。然而，对于普通用户来说，这要困难得多。判断模型是否做得好的过程更像是评分论文，而不是检查数学问题的结果。主观性悄然渗入。
- en: But even if it was easier to measure, I strongly argue that users, even some
    technically savvy and highly educated ones, are not really clear on what these
    models have been trained to do, and therefore can’t know what is realistic to
    expect and what is not. Therefore, results that are perfectly appropriate for
    the model, such as a fluid, eloquent, perfectly “human” paragraph describing how
    the moon is made of green cheese, is going to be seen as a “mistake”. It’s not,
    though- this output met its training goals- and that’s the source of a lot of
    our confusion.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 即使测量更为简单，我仍然坚决认为，即使是一些技术娴熟且受过高等教育的用户，也未必真正清楚这些模型已经训练成什么样，因此也无法知道什么是现实的期望，什么不是。因此，对于模型而言完全合适的结果，比如一个流畅、雄辩、完美“人类风格”的段落描述月亮是由绿色奶酪制成的，也会被视为“错误”。然而这并不是错误——这个输出达到了它的训练目标——这也是我们许多困惑的根源。
- en: Calibrating expectations
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整期望
- en: This suggests that we need to calibrate expectations of these models, and I
    hope this article perhaps can help with that. To successfully use a machine learning
    model, and tell the difference between error and the expected behavior, you need
    to have an understanding of what tasks the models have been trained to perform
    and the nature of the training data. If you get really fancy, you’d also have
    clear context for how the data scientists behind that model measured success,
    because that dramatically shapes the model’s behavior.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明我们需要调整对这些模型的期望，我希望这篇文章能有所帮助。要成功使用机器学习模型，并区分错误和预期行为，你需要了解模型被训练来执行的任务以及训练数据的性质。如果你能更深入一点，你还需要清楚数据科学家如何衡量成功，因为这会极大地影响模型的行为。
- en: By incorporating these elements, you will have the context required to understand
    what the model’s results mean, and will be able to interpret them accurately —
    your expectations will be reasonable and you’ll know whether they were met. And,
    you’ll know what a “mistake” really means when it comes to machine learning.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 通过融合这些元素，你将拥有理解模型结果含义所需的背景信息，并能够准确解读它们——你的期望将会是合理的，你也会知道这些期望是否得到了满足。而且，当涉及到机器学习时，你会真正理解“错误”意味着什么。
- en: There is some useful material out there clarifying a lot of this (how they’re
    trained, what the responses really mean) for popular generative machine learning
    models, and I’ve added a few links to some below. (I am not endorsing all opinions
    in these materials, but just offering them as options for those who want to learn
    more about generative AI.)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有一些有用的材料澄清了许多关于流行生成式机器学习模型的内容（如它们是如何训练的，回应的真正含义等），我在下面添加了一些链接。（我并不 endorsing
    这些材料中的所有观点，只是提供给那些想了解生成式 AI 的人作为参考。）
- en: '[](https://techcrunch.com/2023/09/04/are-language-models-doomed-to-always-hallucinate/?source=post_page-----37b213200697--------------------------------)
    [## Are AI models doomed to always hallucinate? | TechCrunch'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://techcrunch.com/2023/09/04/are-language-models-doomed-to-always-hallucinate/?source=post_page-----37b213200697--------------------------------)
    [## AI 模型注定总是要产生幻觉吗？ | TechCrunch'
- en: Large language models like ChatGPT have a bad habit of making up facts. But
    can this be solved at the technical level?
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 像 ChatGPT 这样的语言模型有一个不好的习惯，就是捏造事实。但这能在技术层面上解决吗？
- en: techcrunch.com](https://techcrunch.com/2023/09/04/are-language-models-doomed-to-always-hallucinate/?source=post_page-----37b213200697--------------------------------)
    [](https://www.cloudskillsboost.google/journeys/118?source=post_page-----37b213200697--------------------------------)
    [## Google Cloud Skills Boost
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: techcrunch.com](https://techcrunch.com/2023/09/04/are-language-models-doomed-to-always-hallucinate/?source=post_page-----37b213200697--------------------------------)
    [](https://www.cloudskillsboost.google/journeys/118?source=post_page-----37b213200697--------------------------------)
    [## Google Cloud Skills Boost
- en: Qwiklabs provides real Google Cloud environments that help developers and IT
    professionals learn cloud platforms and…
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Qwiklabs 提供真实的 Google Cloud 环境，帮助开发者和 IT 专业人员学习云平台以及其他相关内容……
- en: www.cloudskillsboost.google](https://www.cloudskillsboost.google/journeys/118?source=post_page-----37b213200697--------------------------------)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: www.cloudskillsboost.google](https://www.cloudskillsboost.google/journeys/118?source=post_page-----37b213200697--------------------------------)
- en: 'Garon, Jon M., A Practical Introduction to Generative AI, Synthetic Media,
    and the Messages Found in the Latest Medium (March 14, 2023). Available at SSRN:
    [https://ssrn.com/abstract=4388437](https://ssrn.com/abstract=4388437) or [http://dx.doi.org/10.2139/ssrn.4388437](https://dx.doi.org/10.2139/ssrn.4388437)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Garon, Jon M., 《生成式 AI、合成媒体及最新媒体中的信息实用入门》 (2023年3月14日)。可在 SSRN 上获取：[https://ssrn.com/abstract=4388437](https://ssrn.com/abstract=4388437)
    或 [http://dx.doi.org/10.2139/ssrn.4388437](https://dx.doi.org/10.2139/ssrn.4388437)
- en: '*See more of my work at* [*www.stephaniekirmer.com*](http://www.stephaniekirmer.com/)*.*'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*查看更多我的工作请访问* [*www.stephaniekirmer.com*](http://www.stephaniekirmer.com/)*.*'
- en: '*Note: I usually say “machine learning” and not AI, but in the case of “generative
    AI” I choose to use that phrase because of its wide adoption across the field.*'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：我通常说“机器学习”而不是 AI，但在“生成式 AI”这种情况下，我选择使用这个短语，因为它在这个领域中得到了广泛的应用。*'
