- en: A Framework for Analyzing Churn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-framework-for-analyzing-churn-370d2283b75c?source=collection_archive---------4-----------------------#2023-01-13](https://towardsdatascience.com/a-framework-for-analyzing-churn-370d2283b75c?source=collection_archive---------4-----------------------#2023-01-13)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A step-by-step guide to performing a customer churn analysis, using a simulated
    dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://gabri-albini.medium.com/?source=post_page-----370d2283b75c--------------------------------)[![Gabriele
    Albini](../Images/153b88c71ea4e5e221a90de3caa71cdb.png)](https://gabri-albini.medium.com/?source=post_page-----370d2283b75c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----370d2283b75c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----370d2283b75c--------------------------------)
    [Gabriele Albini](https://gabri-albini.medium.com/?source=post_page-----370d2283b75c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F93c18fcb4ee6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-framework-for-analyzing-churn-370d2283b75c&user=Gabriele+Albini&userId=93c18fcb4ee6&source=post_page-93c18fcb4ee6----370d2283b75c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----370d2283b75c--------------------------------)
    ·14 min read·Jan 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F370d2283b75c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-framework-for-analyzing-churn-370d2283b75c&user=Gabriele+Albini&userId=93c18fcb4ee6&source=-----370d2283b75c---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F370d2283b75c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-framework-for-analyzing-churn-370d2283b75c&source=-----370d2283b75c---------------------bookmark_footer-----------)![](../Images/62f0f24f3727ca4e6109e0031b5222d6.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [JESHOOTS.COM](https://unsplash.com/@jeshoots?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '“*Churn*” has become a common business word that refers to the concept of churn
    rate, defined by Wikipedia as the:'
  prefs: []
  type: TYPE_NORMAL
- en: “proportion of contractual customers or subscribers who leave a supplier during
    a given time period”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'When analyzing churn from a data perspective, we usually mean to use the available
    tools to extract information about the existing customer base, specifically: quantify
    the current churn rate and understand what could influence/prevent future churn.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, when we develop a “churn model”, we should use existing data with two objectives
    in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: Predict churn for our existing active customers
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make some hypotheses about what influenced the customers’ churn decision, identifying
    some potential actions that could reduce churn
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Predicting churn requires a lot of work and it is not an easy task but, more
    importantly, it is not even the ultimate goal: it is the starting point to design
    and implement a customer “*retention*” strategy!'
  prefs: []
  type: TYPE_NORMAL
- en: 'This article will focus on the implementation of a churn analysis framework,
    inspired by the book: [1]*“Fighting Churn with Data” by Carl S. Gold*. This is
    a great book that I recommend to anybody who is working with churn data: the book
    goes into lots of details and examples (with explained code!) showing a churn
    analysis end-to-end. From all the suggested steps, I’ve taken what has been most
    relevant and successful in my experience and adapted it to the context and dataset
    I am familiar with. In this article, the framework has been applied to a simulated
    data set, inspired by a real business case ([link to the Github repository](https://github.com/gabri-al/churn_analysis)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table of Content:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 1- [The Data](#5487)
  prefs: []
  type: TYPE_NORMAL
- en: 1.1- [What data should be considered when developing a churn model?](#d95f)
  prefs: []
  type: TYPE_NORMAL
- en: 1.2- [The raw data](#d95f)
  prefs: []
  type: TYPE_NORMAL
- en: '2- [Data preprocessing: churn metrics](#c74a)'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1- [Creating customer metrics](#652c)
  prefs: []
  type: TYPE_NORMAL
- en: 2.2- [Analysing churn metrics](#048e)
  prefs: []
  type: TYPE_NORMAL
- en: 3- [Churn prediction with Machine Learning](#179c)
  prefs: []
  type: TYPE_NORMAL
- en: 3.1- [Logistic Regression](#179c)
  prefs: []
  type: TYPE_NORMAL
- en: 3.2- [Random Forest](#ba22)
  prefs: []
  type: TYPE_NORMAL
- en: 3.3- [XGBoost](#481c)
  prefs: []
  type: TYPE_NORMAL
- en: 4- [Generating churn predictions](#bd49)
  prefs: []
  type: TYPE_NORMAL
- en: 5- [Next steps](#f279)
  prefs: []
  type: TYPE_NORMAL
- en: '[References](#f062)'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. The Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 1.1 What data should be considered when developing a churn model?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is not a trivial question! A lot of different information may be related
    to churn and setting up general rules would never cover all possible businesses,
    systems, contexts, etc. For example, when thinking about churn-related information,
    we may consider:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Demographic information about the clients (or accounts): gender, location,
    age, tenure, …'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Subscriptions related information: the products customers have subscribed to,
    adds-on activated, activation and cancellation dates …'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Payment information: How much do clients pay? What payment method do they use?
    Are they paying regularly? …'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Product usage information: login information, clicks information, minutes of
    interaction with the product, …'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Information related to the interaction with customer support: chats or calls
    made by clients, rating of support services, claims details, …'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: …
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translated into systems, this data would need to come from various transactional
    systems (CRMs, ERPs, billing, …) and should be properly organized into some DataLake
    / Data Warehouse (ideally taking frequent snapshots covering several months).
    Given that this is happening, a lot of know-how should then be available to understand
    what field is representing what information, and, usually, lots of approvals are
    needed to access this data, especially if some external consultant would like
    to use it.
  prefs: []
  type: TYPE_NORMAL
- en: From my experience, all this data (and the related considerable history) is
    ***rarely available***. Usually, what’s available and organized, is the data that
    companies are *required* to store and use, due to financial or legal regulations
    or simply because the data is needed to run the daily business. This data must
    be available *somewhere*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance: suppose we’re a company offering on-demand video training, we
    have to know what client has what subscription and how much they’re paying to
    offer our services and produce our balance sheet. However, we don’t necessarily
    need to store the information about the minute that client XYZ paused a specific
    video before finishing it.'
  prefs: []
  type: TYPE_NORMAL
- en: For all these reasons, in order to keep the article simple and realistic, I’ll
    focus on a “small” dataset, ideally coming from the data that should be available
    in any CRM.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 The raw data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s imagine we’re a B2C (business to consumer) company offering online video
    courses through our website. Our business works in this way:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A new user could subscribe to courses on two domains: machine learning (domain
    A) and guitar (domain B). They can purchase several subscriptions which allow
    them to have different user logging in at the same time. Additionally, they may
    opt to include an “add-on” which consists in receiving weekly live sessions with
    a specialist from the chosen domain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once subscribed, a user has a monthly payment and may or may not have discounts.
    They can cancel their subscription anytime, meaning that the subscription won’t
    be renewed at the end of the month.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Users can open a live chat and contact a support team for any issue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The raw data would look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6a32434a393847910e9f5f5026ede0a6.png)'
  prefs: []
  type: TYPE_IMG
- en: Dummy raw data | Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'This business context is quite common and should apply to any B2C business
    with monthly subscriptions and the option to include an add-on to a base offer
    (e.g. businesses like: on-demand media content, telco, utility, e-commerce, insurance,
    and premium software).'
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Data preprocessing: churn metrics'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Starting from the raw data, we will have to predict if a client is going to
    churn. We will consider clients as a whole, regardless of how many subscriptions
    they have.
  prefs: []
  type: TYPE_NORMAL
- en: Since our goal is to predict churn to develop a retention strategy, we need
    to know, in advance, if a client will be churning, so that we can do something
    to influence this decision.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Creating customer metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'What KPIs can we produce considering the raw data above? Here are some ideas
    (that are the columns of our [dataset](https://github.com/gabri-al/churn_analysis)):'
  prefs: []
  type: TYPE_NORMAL
- en: '“mrr_ratio” = this is the monthly recurrent revenue by subscription. So, for
    every client: we sum([monthly fee — discount]) for every active subscription,
    we count the nr of active subscriptions and we divide the two.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “mrr_ratio_A” and “mrr_ratio_B” = these are the monthly recurrent revenues by
    domain (A is machine learning; B is guitar), considering the mrr and the nr of
    active subscriptions by domain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “subs_A” and “subs_B” = active subscription count by domain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“discount_ratio” = discount % that the client has, obtained as: 1 — ([monthly
    fee — discount] / [monthly fee])'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “has_addon” = a flag indicating if the client has at least one subscription
    with an add on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “support_chats” = count of chats initiated by the client in a period
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “is_churn” = a flag indicating if the client is going to churn (1) or not (0)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The best method to use our historic raw data to calculate these KPIs is, in
    my opinion, to:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify some fixed observation period (e.g. the 20th of every month), leaving
    some reasonable time from our renewal (which we suppose to occur on the 30th of
    every month).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a table “A” in which, for every “lost” client, we include their past
    churn dates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create another table “B” in which, for every client, on the 20th of the month,
    we calculate the KPIs based on the past 30 days’ data. In other words, we take
    monthly screenshots of the client metrics, but we do it on the 20th of every month.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We join the tables “A” and “B” on client ID and flag all rows that will end
    up in a churn by the next observation date.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These observation periods and KPIs are normally calculated on the data warehouse
    and then exported into Python. The data that I simulated for the project represents
    exactly this scenario. Suppose we just obtained the following dataset from our
    data warehouse:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d22561b15385296e33661df0a17cb897.png)'
  prefs: []
  type: TYPE_IMG
- en: Dummy churn metrics | Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '(*Note: this is a simulated dataset. All the continuous metrics have been drawn
    from a multivariate Gaussian distribution that approximates the real data. This
    is why we have negative values and decimal values on KPIs that shouldn’t be negative
    or decimals. Additionally, each row should correspond to a client ID, but this
    information is not relevant*).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Analysing churn metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once we have some metrics, we can finally start checking what’s their relationship
    with churn.
  prefs: []
  type: TYPE_NORMAL
- en: The most intuitive way to investigate this relationship is via a **cohort analysis**.
    Usually, 10 cohorts are generated by splitting each metric data into 10 equal-size
    buckets, depending on their values. We then relate each metric with the “is_churn”
    flag by calculating the churn rate in each cohort. If a metric is not continuous
    and has less than 10 categorical values, then we just consider one cohort per
    category.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the left chart, we can see that the clients who have a higher mrr_ratio,
    on average, churn more, as they are paying more per subscription:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/635ff9d13ee409337f67d4cb586b2783.png)'
  prefs: []
  type: TYPE_IMG
- en: Churn metric cohorts | Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Whenever we see such behavior, which makes logical sense and where, depending
    on the metric value, we see a considerable difference in churn, then we can expect
    the metric to be relevant in our analysis.
  prefs: []
  type: TYPE_NORMAL
- en: On the contrary, if we see metrics where, regardless of the cohort average value,
    there’s no impact on churn (e.g. horizontal line), then we may consider excluding
    the metric from the model.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Churn prediction with Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now use the dataset to predict churn.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that churn is *not simple* to predict. Deciding to churn is subjective
    and it may not always be a logical choice: one client may churn because of costs-related
    issues and others may churn because of quality. Additionally, bad customer service
    or a perceived negative feeling about the product/brand may trigger the decision
    to churn subjectively.'
  prefs: []
  type: TYPE_NORMAL
- en: For these reasons, model performances won’t be as high as in other ML tasks.
    According to Carl S. Gold [1], a healthy churn prediction model would perform
    with an AUC score between 0.6 and 0.8.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some considerations to take into account:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Churn is a binary classification task: the model would learn to predict if
    a record belongs to class 1 (churned client) or class 0 (not churn). However,
    we will be interested in the ***probability that each record belongs to each class***.
    Keep this in mind when selecting a model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model performances cannot be measured using accuracy score. Usually, a low
    minority of clients churn and therefore our dataset is unbalanced: only approx.
    10% of the dummy data belongs to class 1 (churned clients). Any model that always
    predicts class 0, will have a 90% accuracy but such model wouldn’t help at all.
    Instead, we will use the ***roc_auc score*** to measure performances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will be using cross-validation to tune models’ hyperparameters. Since we’re
    working with a timed dataset, we cannot simply use a random record assignment
    to each fold. We need to train our model to use present or past data and never
    use future data. So, the best practice suggests using a ***Time Series Split***
    (from [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html)
    [2]), which works on any time-sorted dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*(Note: in cross-validation, 10 splits are normally used. Here, 3 were used
    due to the limited data size and very unbalanced data towards class 0).*'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now compare three classification models.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Logistic Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Logistic regression is a generalized linear regression model, which is a very
    common classification technique, especially used for binary classification. Since
    it’s a regression model, many hypotheses should be verified beforehand; for instance,
    we should not violate the “no multicollinearity” assumption, meaning that we should
    ensure that no features are correlated, i.e. each feature should provide unique
    and independent information.
  prefs: []
  type: TYPE_NORMAL
- en: Although this would be easy to verify, I can anticipate that logistic regression
    won’t be the most performing model, so we won’t be using any invalid results we
    may obtain.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ddf9b140349ae80c8918d589f967f132.png)'
  prefs: []
  type: TYPE_IMG
- en: Logisitc Regression | Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Random Forest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Random Forest is an ensemble tree-based method.
  prefs: []
  type: TYPE_NORMAL
- en: '**Tree-based methods** are very powerful classification (or regression) algorithms
    that consist in splitting our train data according to several decision nodes.
    Each decision node performs a “split” with a True / False decision, based on a
    specific feature. The split decision is determined so that, on the next level
    of the tree, the “entropy” of our dataset is reduced the most. Entropy is a measure
    of the disorder of the data and it is linked to the “information gain” that we
    can get, in our classification/regression task.'
  prefs: []
  type: TYPE_NORMAL
- en: For instance, in a binary classification problem, if we notice that by splitting
    data according to a feature, we get — for each True/False resulting branch — 95%
    percent of the data belonging to one class and 5% belonging to the opposite class,
    then we managed to gain more information from our data, reducing its level of
    disorder or “entropy”.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random Forest (RF)** builds several different trees and then takes the average/most
    frequent result to make a final prediction. RF ensures that each tree is built
    differently from the others thanks to two methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Bagging (bootstrap aggregating)*: each tree is trained by using a sample of
    the entire training set, so, each tree is built using different data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Feature randomness*: each tree is built by limiting the available features,
    using a subset of all the available features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s now tune RF hyperparameters on our dataset, select the best model and
    show the most “important” features (i.e. the frequency representing how often
    each feature is used to generate a decision split):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7740711313d8cd5ad8592673f7393a6e.png)'
  prefs: []
  type: TYPE_IMG
- en: Random Forest | Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 XGBoost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**XGBoost** stands for Extreme Gradient Boosting and it is another tree-based
    ensemble technique that, similarly to RF, allows to combine the predictions made
    by several decision trees.'
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost is an evolution (“Extreme”) of the “*Gradient boosting*” methodology.
    So, to illustrate XGBoost, let’s examine these two aspects separately.
  prefs: []
  type: TYPE_NORMAL
- en: 'In “**gradient boosting**” methods, differently from RF, the trees that are
    built are very much related. Predictions are made by “weak learners” (i.e. simple
    trees) that are improved over and over. Typically, the initial prediction is the
    average target value, and it is then refined by creating new trees. Each new tree
    is built from the previous trees’ errors: so, starting from the residuals/wrong
    predictions of the previous “weak learners”, a new tree is built, minimizing a
    cost function and assigning more weights to the attributes that generated the
    errors. Finally, results are combined by weighting the results from each tree.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Starting from “gradient boosting”, “**Extreme Gradient Boosting**” is a complete
    algorithm that includes several improvements to the gradient boosting method,
    such as: performance optimizations and a regularization parameter (which allows
    to avoid overfitting). Most importantly, thanks to these additional elements,
    XGBoost can run on simple machines like normal laptops.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/96ab1b8c02fcb003325095f5156fd0db.png)'
  prefs: []
  type: TYPE_IMG
- en: 4\. Generating churn predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The top-performing model is XGBoost and we will now use it to predict the churn
    probabilities of a [test set](https://github.com/gabri-al/churn_analysis) (containing
    new records not used in the training phase).
  prefs: []
  type: TYPE_NORMAL
- en: 'After importing the test set, we calculate, for each record, the model’s predicted
    probabilities of belonging to class 1 (churned clients) and plot the ROC_AUC score:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4d6614cfc76923e1c41fea03710f7904.png)'
  prefs: []
  type: TYPE_IMG
- en: Test set AUC score | Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s add the predicted class to the original data. By default, all records
    with a predicted probability ≥ .5 will be assigned to class 1\. We can lower this
    threshold and compare the resulting confusion matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/73e0e8b5c3e031a17c9c1ba91e67bca9.png)'
  prefs: []
  type: TYPE_IMG
- en: Confusion Matrices | Image by author
  prefs: []
  type: TYPE_NORMAL
- en: By lowering the threshold, we can identify more churning clients (true positives
    and false positives) but there is still a considerable number of clients that
    will churn but we fail to identify (false negatives), despite the good performances
    of our xgboost model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We could try to find a better model, but predicting churn is generally hard.
    So, instead of using a simple churn vs non-churn distinction, an idea is to use
    our predicted probabilities to define some different retention strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: For clients with a predicted_proba > .75 = high risk of churning, we can design
    a “strong” retention strategy. Since we expect few false positives, we can afford
    to invest in these clients with higher confidence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clients with predicted_proba between .5 and .75 = moderate risk of churning
    and “moderate” retention strategy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clients with predicted_proba between .25 and .5 = low risk of churning and “weak”
    retention strategy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5\. Next steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this stage, we should have a working model which can assign a “probability
    of churn” to any new data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step of our analysis is to further define the retention strategies
    mentioned before. Our strategy should address: (a) actions to take which could
    lead to a churn reduction; (b) how to measure the success of our actions; (c)
    finally, a rollout plan.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some ideas to address these points:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Identifying actions that lead to a churn reduction:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s combine the importance of the features seen above, together with our
    predictions. For instance, both tree-based models ranked “**subs_B**” as the most
    used feature in the trees. We would need to dig more into it and understand what
    churn and non-churn clients look like with respect to subs_B. The cohort analysis
    seen earlier would help here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1da13d54c6f5d5cd0201a9bd691951a4.png)'
  prefs: []
  type: TYPE_IMG
- en: Cohort Analysis on train data | Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'It seems that clients with high churn have the minimum value (i.e. 0 subscriptions,
    the data has been transformed so the x-axis values are not much interpretable
    here), or a too high number of “subs_B”. We have to be careful to draw some causal
    conclusions between “subs_B” and “is_churn”, as this analysis is not proving any
    sort of causal relationship. However, we could test some hypotheses:'
  prefs: []
  type: TYPE_NORMAL
- en: It seems that clients are satisfied with our B product, could it help to cross-sell
    “B” to clients having only the A product, to reduce churn?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We should also understand what is the business reason behind clients with so
    many “B” subscriptions. We may educate them to use our products more efficiently
    and reduce B subscriptions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**How to measure the success of our actions**:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once we identified some actions that we would like to suggest, we can plan our
    measurement methodology.
  prefs: []
  type: TYPE_NORMAL
- en: 'A/B tests are a very common way to do that:'
  prefs: []
  type: TYPE_NORMAL
- en: We create two comparable samples from clients with similar predicted churn probabilities.
    One sample will represent our treatment group and will be exposed to our churn
    reduction strategy, the other sample will represent our control group and won’t
    be exposed to any retention action.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We would hope to prove that the churn rate of our treatment group is significantly
    lower than the one of the control group.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The rollout plan:**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When suggesting retention actions, we shouldn’t forget to take other contextual
    aspects into account. To name a few: how much churn is a concern? (i.e. are there
    a lot of new clients acquired to compensate for churn?) what’s the budget to address
    the problem? how long should we wait to see the results? can we use any other
    data to improve our models? what has been done already?'
  prefs: []
  type: TYPE_NORMAL
- en: This will help us understand if what we’re suggesting is feasible.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading!!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Carl S. Gold — “Fighting Churn with Data: The science and strategy of customer
    retention”, 2020'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [Scikit-learn: Machine Learning in Python](https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html),
    Pedregosa *et al.*, JMLR 12, pp. 2825–2830, 2011'
  prefs: []
  type: TYPE_NORMAL
