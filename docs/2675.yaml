- en: 'Not-So-Large Language Models: Good Data Overthrows the Goliath'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**不那么庞大的语言模型：优质数据打败巨人**'
- en: 原文：[https://towardsdatascience.com/not-so-large-language-models-good-data-overthrows-the-goliath-a8226bd1ae61?source=collection_archive---------6-----------------------#2023-08-23](https://towardsdatascience.com/not-so-large-language-models-good-data-overthrows-the-goliath-a8226bd1ae61?source=collection_archive---------6-----------------------#2023-08-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/not-so-large-language-models-good-data-overthrows-the-goliath-a8226bd1ae61?source=collection_archive---------6-----------------------#2023-08-23](https://towardsdatascience.com/not-so-large-language-models-good-data-overthrows-the-goliath-a8226bd1ae61?source=collection_archive---------6-----------------------#2023-08-23)
- en: '![](../Images/268952a364e31551f9803b9456936b95.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/268952a364e31551f9803b9456936b95.png)'
- en: (Image generated by DALL·E)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: （图像由 DALL·E 生成）
- en: How to make a million-sized language model that tops a billion-size one
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何制造一个百万级别的语言模型来超越十亿级别的模型
- en: '[](https://medium.com/@gennsev?source=post_page-----a8226bd1ae61--------------------------------)[![Gennaro
    S. Rodrigues](../Images/df7b2d2312c1344def0f65254c96a8d4.png)](https://medium.com/@gennsev?source=post_page-----a8226bd1ae61--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a8226bd1ae61--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a8226bd1ae61--------------------------------)
    [Gennaro S. Rodrigues](https://medium.com/@gennsev?source=post_page-----a8226bd1ae61--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@gennsev?source=post_page-----a8226bd1ae61--------------------------------)[![Gennaro
    S. Rodrigues](../Images/df7b2d2312c1344def0f65254c96a8d4.png)](https://medium.com/@gennsev?source=post_page-----a8226bd1ae61--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a8226bd1ae61--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a8226bd1ae61--------------------------------)
    [Gennaro S. Rodrigues](https://medium.com/@gennsev?source=post_page-----a8226bd1ae61--------------------------------)'
- en: ·
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F603bda3d0d35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnot-so-large-language-models-good-data-overthrows-the-goliath-a8226bd1ae61&user=Gennaro+S.+Rodrigues&userId=603bda3d0d35&source=post_page-603bda3d0d35----a8226bd1ae61---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a8226bd1ae61--------------------------------)
    ·6 min read·Aug 23, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa8226bd1ae61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnot-so-large-language-models-good-data-overthrows-the-goliath-a8226bd1ae61&user=Gennaro+S.+Rodrigues&userId=603bda3d0d35&source=-----a8226bd1ae61---------------------clap_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F603bda3d0d35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnot-so-large-language-models-good-data-overthrows-the-goliath-a8226bd1ae61&user=Gennaro+S.+Rodrigues&userId=603bda3d0d35&source=post_page-603bda3d0d35----a8226bd1ae61---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a8226bd1ae61--------------------------------)
    · 6 min read · 2023年8月23日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa8226bd1ae61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnot-so-large-language-models-good-data-overthrows-the-goliath-a8226bd1ae61&user=Gennaro+S.+Rodrigues&userId=603bda3d0d35&source=-----a8226bd1ae61---------------------clap_footer-----------)'
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa8226bd1ae61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnot-so-large-language-models-good-data-overthrows-the-goliath-a8226bd1ae61&source=-----a8226bd1ae61---------------------bookmark_footer-----------)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa8226bd1ae61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnot-so-large-language-models-good-data-overthrows-the-goliath-a8226bd1ae61&source=-----a8226bd1ae61---------------------bookmark_footer-----------)'
- en: In this article, we will see how Language Models (LM) can focus on better data
    and training strategies rather than just brute size to achieve LLM-like results
    (sometimes even better) and how people are already doing it successfully and democratically.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将探讨语言模型（LM）如何通过关注更好的数据和训练策略，而不仅仅依赖庞大的规模，来实现类似 LLM 的结果（有时甚至更好），以及人们如何已经成功且民主地做到这一点。
- en: Large Language Models (LLMs) have evolved significantly. They bring remarkable
    features, from generating human-like text to understanding intricate contexts.
    While much of the initial excitement revolved around models with a massive number
    of parameters, recent developments suggest that size isn’t the only thing that
    matters. Lately, a new concept called Small Language Models (SLM) has risen with
    justice as a motivation to develop language models more intelligently.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: The Rise of Large Models
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As LLMs entered the stage, the narrative was straightforward — bigger is better.
    Models with more parameters are expected to understand the context better, make
    fewer mistakes, and provide better answers. But as the models grew, so did their
    hunger for computational resources. Training these behemoths became an expensive
    task, one that not everyone is willing (nor able) to pay for.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: An Emphasis on Quality and Efficiency
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recognizing the unsustainability and diminishing returns of just adding more
    parameters, researchers began to rethink strategies. Instead of merely throwing
    dollars into the cloud fire (adding another billion more parameters), some researchers
    shifted to utilizing better data and more efficient training strategies. The idea
    is elegant: a well-trained smaller model might outperform a poorly trained larger
    model. But can it?'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Chinchilla and the Optimal Point for LLMs Training
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The “Chinchilla paper” [1], a significant contribution to the field, offers
    intriguing insights into LLMs’ training. Experiments seem to indicate that there
    is an “optimal point” when training LLMs. Beyond this point, pouring more resources
    into training in the form of more parameters does not necessarily result in a
    proportional increase in performance. The paper emphasizes that it’s not only
    the size of a model that defines its performance. Instead, it’s about the quality
    of that data and how much data you use. The authors found that for compute-optimal
    training, the model size and the number of training tokens should be scaled equally:
    for every doubling of the model size, the number of training tokens should also
    be doubled.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: They test this by training Chinchilla, a 70 billion parameters model trained
    on 1.4 trillion tokens. Despite being much smaller, Chinchilla outperforms Gopher
    on almost all evaluations, including language modeling, question answering, common
    sense tasks, etc.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0221e7e2842d981f7874787daed1c2ec.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: 'Chinchilla Size and Training Tokens vs. SOTA LLMs. (Source: [1])'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: 'Even with its reduced size, Chinchilla performs better than its SOTA counterparts
    on a variety of tasks:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a31e58e0a9a6830246cb7194a07e7613.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
- en: 'Massive Multitask Language Understanding (MMLU).Reporting the average 5-shot
    accuracy over 57 tasks with model and human accuracy comparisons taken from [2],
    and the average prediction for SOTA accuracy in June 2022/2023 made by 73 competitive
    human forecasters in [3]. (Source: [1])'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Reading comprehension and automated reasoning are standard tasks a language
    model is typically tested on. It tests the model’s ability to understand the broader
    context of the text. In our case, it could be exemplified as predicting words
    that could only be expected if the model could understand the relation between
    this word and the context that came before it (sometimes far from this word’s
    position). It is usually evaluated using benchmarks and datasets such as RACE-h,
    RACE-m [4], and LAMBADA [5]. Chinchilla outperforms much bigger models even on
    this type of hard-to-define and test tasks.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读理解和自动推理是语言模型通常会测试的标准任务。它测试模型理解文本更广泛背景的能力。在我们的案例中，可以通过预测那些仅在模型能够理解单词与之前上下文关系的情况下才会预期到的单词来进行示例。通常使用基准测试和数据集，如
    RACE-h、RACE-m [4] 和 LAMBADA [5] 进行评估。即使在这种难以定义和测试的任务中，Chinchilla 也超越了更大的模型。
- en: '![](../Images/bcbfb441780047b6fac6c5fb92c57417.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bcbfb441780047b6fac6c5fb92c57417.png)'
- en: 'On Reading Comprehension, Chinchilla considerably improves performance over
    *Gopher.* (Source: [1])'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读理解方面，Chinchilla 相比于 *Gopher* 显著提升了性能。（来源：[1]）
- en: And Chinchilla is one of many LMs showing promising results despite not focusing
    on augmenting size.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Chinchilla 是许多尽管没有注重扩展规模但仍展现出有希望结果的语言模型之一。
- en: LLaMA
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLaMA
- en: LLaMA[6] goes even further. The authors introduce smaller foundation language
    models ranging from 7B to 65B parameters. They are trained on over 1 trillion
    tokens using only publicly available data, making them compatible with open sourcing.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA[6] 甚至更进一步。作者引入了从 7B 到 65B 参数的较小基础语言模型。它们在超过 1 万亿个标记的数据上进行训练，使用的仅是公开数据，使其兼容开源。
- en: LLaMA-13B outperforms the much larger 175B parameter GPT-3 on most benchmarks
    while being over 10x smaller. The authors argue that given a target performance
    level, smaller models trained longer are preferable to larger models for a given
    compute budget due to better inference efficiency.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA-13B 在大多数基准测试中超过了参数多达 175B 的 GPT-3，而其体积小于 GPT-3 的 10 倍。作者认为，考虑到目标性能水平，训练时间更长的小型模型在给定计算预算下比大型模型更具优势，因为推理效率更高。
- en: '![](../Images/982e81ac291997ead025a8de60254adf.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/982e81ac291997ead025a8de60254adf.png)'
- en: 'LLaMA zero-shot performance on Common Sense Reasoning tasks. (Source: [6])'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA 在常识推理任务中的零-shot 表现。（来源：[6]）
- en: Some projects have even managed to run LLaMA (or rather a version of it) on
    budget Android smartphones, further proving that we are on the right path to democratizing
    access to performative LMs using low computing resources (LLaMA.c [7]).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一些项目甚至成功在预算有限的安卓智能手机上运行 LLaMA（或其版本），进一步证明我们正走在通过低计算资源实现语言模型民主化的正确道路上（LLaMA.c
    [7]）。
- en: LLaMA-65B (I know, not that small anymore, but still…) is competitive with the
    current state-of-the-art models like PaLM-540B, which use proprietary datasets.
    This clearly indicates how good data not only improves a model’s performance but
    can also make it democratic. A machine learning engineer would not need enormous
    budgets to get good model training on a good dataset.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA-65B（我知道，现在不算那么小，但仍然……）在与使用专有数据集的现有最先进模型如 PaLM-540B 的竞争中表现良好。这清楚地表明，优质数据不仅能提升模型的性能，还能使其变得民主化。机器学习工程师无需巨额预算就能在优质数据集上获得良好的模型训练。
- en: Good data trumps the Goliath
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优质数据胜过巨无霸
- en: Further reinforcing the thesis that LMs don’t need to be gigantic to perform
    well, TinyStories [8] presents a synthetic dataset of stories containing only
    words that small children (up to four years old) can understand. It can be used
    to train small language models (SLMs) with under 10 million parameters that can
    generate multi-paragraph stories with good grammar, reasoning, and coherence.
    This contrasts previous works where 125M+ parameter models — such as GPT-Neo (small)
    and GPT-2 (small) — struggled to produce a coherent text.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步巩固了语言模型不需要庞大才能表现良好的论点，TinyStories [8] 提供了一个合成数据集，其中包含仅供小孩子（最多四岁）理解的单词。它可以用来训练参数少于
    1000 万的小型语言模型（SLMs），这些模型能够生成语法、推理和连贯性良好的多段故事。这与先前的研究形成对比，125M+ 参数的模型——如 GPT-Neo（小型）和
    GPT-2（小型）——在生成连贯文本方面存在困难。
- en: '![](../Images/ac3423e2d8717a7727f69d0b31869604.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac3423e2d8717a7727f69d0b31869604.png)'
- en: 'A model trained with TinyStories can produce output comparable to one from
    a model that is two orders of magnitude bigger. (Source: [8])'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 训练了 TinyStories 的模型能产生与参数大两个数量级的模型相当的输出。（来源：[8]）
- en: One of the exciting aspects of TinyStories is that the dataset itself was created
    by GPT-3.5 and GPT-4\. The authors also introduce a new SLM evaluation paradigm
    using GPT-4 to “grade” generated stories on dimensions like grammar, plot, and
    creativity. This overcomes the limitations of standard benchmarks requiring constrained
    outputs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: TinyStories 的一个令人兴奋的方面是数据集本身是由 GPT-3.5 和 GPT-4 创建的。作者们还引入了一种新的 SLM 评估范式，使用 GPT-4
    对生成的故事在语法、情节和创意等维度上进行“评分”。这克服了标准基准测试要求受限输出的局限性。
- en: Conclusion
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'The journey of LMs showcases a pivotal lesson in AI: Bigger is not always better.
    As the community continues to evolve and innovate, there’s a realization that
    efficiency, quality of data, and optimized training strategies hold the key to
    the future of machine learning.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型的发展展示了 AI 中的一个关键教训：更大并不总是更好。随着社区的持续进化和创新，人们意识到效率、数据质量和优化的训练策略是机器学习未来的关键。
- en: Key Takeaways
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键要点
- en: Chinchilla proves that there is an optimal point when training LMs regarding
    the number of tokens and the quality of training data used. It is as important
    as (or more) defining the number of parameters of the model;
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chinchilla 证明了在训练语言模型时，令牌数量和训练数据质量之间存在一个最佳点。这一点与（或更重要于）模型参数的数量定义同样重要；
- en: LLaMa shows Chinchilla-like results are achievable using only publicly available
    data, proving this strategy to be democratically available;
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLaMa 显示了使用仅公开数据就能达到类似 Chinchilla 的结果，证明了这一策略具有普遍可用性；
- en: Datasets like TinyStories can be used to train small language models (less than
    100 million) that outperform billion-sized models on specific tasks.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像 TinyStories 这样的数据集可以用于训练小型语言模型（少于 1 亿），在特定任务上超越了十亿规模的模型。
- en: References
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Hoffmann, Jordan, et al. “Training compute-optimal large language models.”
    *arXiv preprint arXiv:2203.15556* (2022).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Hoffmann, Jordan 等. “训练计算最优的大型语言模型。” *arXiv 预印本 arXiv:2203.15556*（2022年）。'
- en: '[2] D. Hendrycks, et al. “Measuring massive multitask language understanding.”
    *arXiv preprint arXiv:2009.03300 (*2020).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] D. Hendrycks 等. “测量大规模多任务语言理解。” *arXiv 预印本 arXiv:2009.03300*（2020年）。'
- en: '[3] J. Steinhardt. Updates and lessons from AI forecasting, 2021\. URL https://bounded-regret.ghost.io/ai-forecasting/.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] J. Steinhardt. 来自 AI 预测的更新和经验教训，2021年。URL https://bounded-regret.ghost.io/ai-forecasting/。'
- en: '[4] Lai, Guokun, et al. “RACE: Large-scale ReAding Comprehension Dataset From
    Examinations.” In *Proceedings of the 2017 Conference on Empirical Methods in
    Natural Language Processing*, pages 785–794, Copenhagen, Denmark. Association
    for Computational Linguistics.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Lai, Guokun 等. “RACE: 大规模阅读理解数据集来自考试。” *2017年自然语言处理会议论文集*，页码785–794，哥本哈根，丹麦。计算语言学协会。'
- en: '[5] Paperno et al., 2016 “The LAMBADA dataset: Word prediction requiring a
    broad discourse context.” *arXiv:1606.06031* (2016).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Paperno 等，2016 “LAMBADA 数据集：需要广泛语篇背景的单词预测。” *arXiv:1606.06031*（2016年）。'
- en: '[6] Touvron, Hugo et al. “LLaMA: Open and Efficient Foundation Language Models.”
    *ArXiv* abs/2302.13971 (2023)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Touvron, Hugo 等. “LLaMA: 开放且高效的基础语言模型。” *ArXiv* abs/2302.13971（2023年）'
- en: '[7] [https://github.com/karpathy/llama2.c](https://github.com/karpathy/llama2.c)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] [https://github.com/karpathy/llama2.c](https://github.com/karpathy/llama2.c)'
- en: '[8] Eldan, Ronen and Yuan-Fang Li. “TinyStories: How Small Can Language Models
    Be and Still Speak Coherent English?” *ArXiv* abs/2305.07759 (2023)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Eldan, Ronen 和 Yuan-Fang Li. “TinyStories：语言模型可以小到什么程度仍然能够说出连贯的英语？” *ArXiv*
    abs/2305.07759（2023年）'
