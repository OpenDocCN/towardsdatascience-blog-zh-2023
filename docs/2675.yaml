- en: 'Not-So-Large Language Models: Good Data Overthrows the Goliath'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/not-so-large-language-models-good-data-overthrows-the-goliath-a8226bd1ae61?source=collection_archive---------6-----------------------#2023-08-23](https://towardsdatascience.com/not-so-large-language-models-good-data-overthrows-the-goliath-a8226bd1ae61?source=collection_archive---------6-----------------------#2023-08-23)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/268952a364e31551f9803b9456936b95.png)'
  prefs: []
  type: TYPE_IMG
- en: (Image generated by DALL·E)
  prefs: []
  type: TYPE_NORMAL
- en: How to make a million-sized language model that tops a billion-size one
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@gennsev?source=post_page-----a8226bd1ae61--------------------------------)[![Gennaro
    S. Rodrigues](../Images/df7b2d2312c1344def0f65254c96a8d4.png)](https://medium.com/@gennsev?source=post_page-----a8226bd1ae61--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a8226bd1ae61--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a8226bd1ae61--------------------------------)
    [Gennaro S. Rodrigues](https://medium.com/@gennsev?source=post_page-----a8226bd1ae61--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F603bda3d0d35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnot-so-large-language-models-good-data-overthrows-the-goliath-a8226bd1ae61&user=Gennaro+S.+Rodrigues&userId=603bda3d0d35&source=post_page-603bda3d0d35----a8226bd1ae61---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a8226bd1ae61--------------------------------)
    ·6 min read·Aug 23, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa8226bd1ae61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnot-so-large-language-models-good-data-overthrows-the-goliath-a8226bd1ae61&user=Gennaro+S.+Rodrigues&userId=603bda3d0d35&source=-----a8226bd1ae61---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa8226bd1ae61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnot-so-large-language-models-good-data-overthrows-the-goliath-a8226bd1ae61&source=-----a8226bd1ae61---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will see how Language Models (LM) can focus on better data
    and training strategies rather than just brute size to achieve LLM-like results
    (sometimes even better) and how people are already doing it successfully and democratically.
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) have evolved significantly. They bring remarkable
    features, from generating human-like text to understanding intricate contexts.
    While much of the initial excitement revolved around models with a massive number
    of parameters, recent developments suggest that size isn’t the only thing that
    matters. Lately, a new concept called Small Language Models (SLM) has risen with
    justice as a motivation to develop language models more intelligently.
  prefs: []
  type: TYPE_NORMAL
- en: The Rise of Large Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As LLMs entered the stage, the narrative was straightforward — bigger is better.
    Models with more parameters are expected to understand the context better, make
    fewer mistakes, and provide better answers. But as the models grew, so did their
    hunger for computational resources. Training these behemoths became an expensive
    task, one that not everyone is willing (nor able) to pay for.
  prefs: []
  type: TYPE_NORMAL
- en: An Emphasis on Quality and Efficiency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recognizing the unsustainability and diminishing returns of just adding more
    parameters, researchers began to rethink strategies. Instead of merely throwing
    dollars into the cloud fire (adding another billion more parameters), some researchers
    shifted to utilizing better data and more efficient training strategies. The idea
    is elegant: a well-trained smaller model might outperform a poorly trained larger
    model. But can it?'
  prefs: []
  type: TYPE_NORMAL
- en: Chinchilla and the Optimal Point for LLMs Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The “Chinchilla paper” [1], a significant contribution to the field, offers
    intriguing insights into LLMs’ training. Experiments seem to indicate that there
    is an “optimal point” when training LLMs. Beyond this point, pouring more resources
    into training in the form of more parameters does not necessarily result in a
    proportional increase in performance. The paper emphasizes that it’s not only
    the size of a model that defines its performance. Instead, it’s about the quality
    of that data and how much data you use. The authors found that for compute-optimal
    training, the model size and the number of training tokens should be scaled equally:
    for every doubling of the model size, the number of training tokens should also
    be doubled.'
  prefs: []
  type: TYPE_NORMAL
- en: They test this by training Chinchilla, a 70 billion parameters model trained
    on 1.4 trillion tokens. Despite being much smaller, Chinchilla outperforms Gopher
    on almost all evaluations, including language modeling, question answering, common
    sense tasks, etc.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0221e7e2842d981f7874787daed1c2ec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Chinchilla Size and Training Tokens vs. SOTA LLMs. (Source: [1])'
  prefs: []
  type: TYPE_NORMAL
- en: 'Even with its reduced size, Chinchilla performs better than its SOTA counterparts
    on a variety of tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a31e58e0a9a6830246cb7194a07e7613.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Massive Multitask Language Understanding (MMLU).Reporting the average 5-shot
    accuracy over 57 tasks with model and human accuracy comparisons taken from [2],
    and the average prediction for SOTA accuracy in June 2022/2023 made by 73 competitive
    human forecasters in [3]. (Source: [1])'
  prefs: []
  type: TYPE_NORMAL
- en: Reading comprehension and automated reasoning are standard tasks a language
    model is typically tested on. It tests the model’s ability to understand the broader
    context of the text. In our case, it could be exemplified as predicting words
    that could only be expected if the model could understand the relation between
    this word and the context that came before it (sometimes far from this word’s
    position). It is usually evaluated using benchmarks and datasets such as RACE-h,
    RACE-m [4], and LAMBADA [5]. Chinchilla outperforms much bigger models even on
    this type of hard-to-define and test tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bcbfb441780047b6fac6c5fb92c57417.png)'
  prefs: []
  type: TYPE_IMG
- en: 'On Reading Comprehension, Chinchilla considerably improves performance over
    *Gopher.* (Source: [1])'
  prefs: []
  type: TYPE_NORMAL
- en: And Chinchilla is one of many LMs showing promising results despite not focusing
    on augmenting size.
  prefs: []
  type: TYPE_NORMAL
- en: LLaMA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLaMA[6] goes even further. The authors introduce smaller foundation language
    models ranging from 7B to 65B parameters. They are trained on over 1 trillion
    tokens using only publicly available data, making them compatible with open sourcing.
  prefs: []
  type: TYPE_NORMAL
- en: LLaMA-13B outperforms the much larger 175B parameter GPT-3 on most benchmarks
    while being over 10x smaller. The authors argue that given a target performance
    level, smaller models trained longer are preferable to larger models for a given
    compute budget due to better inference efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/982e81ac291997ead025a8de60254adf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'LLaMA zero-shot performance on Common Sense Reasoning tasks. (Source: [6])'
  prefs: []
  type: TYPE_NORMAL
- en: Some projects have even managed to run LLaMA (or rather a version of it) on
    budget Android smartphones, further proving that we are on the right path to democratizing
    access to performative LMs using low computing resources (LLaMA.c [7]).
  prefs: []
  type: TYPE_NORMAL
- en: LLaMA-65B (I know, not that small anymore, but still…) is competitive with the
    current state-of-the-art models like PaLM-540B, which use proprietary datasets.
    This clearly indicates how good data not only improves a model’s performance but
    can also make it democratic. A machine learning engineer would not need enormous
    budgets to get good model training on a good dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Good data trumps the Goliath
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Further reinforcing the thesis that LMs don’t need to be gigantic to perform
    well, TinyStories [8] presents a synthetic dataset of stories containing only
    words that small children (up to four years old) can understand. It can be used
    to train small language models (SLMs) with under 10 million parameters that can
    generate multi-paragraph stories with good grammar, reasoning, and coherence.
    This contrasts previous works where 125M+ parameter models — such as GPT-Neo (small)
    and GPT-2 (small) — struggled to produce a coherent text.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ac3423e2d8717a7727f69d0b31869604.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A model trained with TinyStories can produce output comparable to one from
    a model that is two orders of magnitude bigger. (Source: [8])'
  prefs: []
  type: TYPE_NORMAL
- en: One of the exciting aspects of TinyStories is that the dataset itself was created
    by GPT-3.5 and GPT-4\. The authors also introduce a new SLM evaluation paradigm
    using GPT-4 to “grade” generated stories on dimensions like grammar, plot, and
    creativity. This overcomes the limitations of standard benchmarks requiring constrained
    outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The journey of LMs showcases a pivotal lesson in AI: Bigger is not always better.
    As the community continues to evolve and innovate, there’s a realization that
    efficiency, quality of data, and optimized training strategies hold the key to
    the future of machine learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Key Takeaways
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Chinchilla proves that there is an optimal point when training LMs regarding
    the number of tokens and the quality of training data used. It is as important
    as (or more) defining the number of parameters of the model;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLaMa shows Chinchilla-like results are achievable using only publicly available
    data, proving this strategy to be democratically available;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Datasets like TinyStories can be used to train small language models (less than
    100 million) that outperform billion-sized models on specific tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Hoffmann, Jordan, et al. “Training compute-optimal large language models.”
    *arXiv preprint arXiv:2203.15556* (2022).'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] D. Hendrycks, et al. “Measuring massive multitask language understanding.”
    *arXiv preprint arXiv:2009.03300 (*2020).'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] J. Steinhardt. Updates and lessons from AI forecasting, 2021\. URL https://bounded-regret.ghost.io/ai-forecasting/.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Lai, Guokun, et al. “RACE: Large-scale ReAding Comprehension Dataset From
    Examinations.” In *Proceedings of the 2017 Conference on Empirical Methods in
    Natural Language Processing*, pages 785–794, Copenhagen, Denmark. Association
    for Computational Linguistics.'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Paperno et al., 2016 “The LAMBADA dataset: Word prediction requiring a
    broad discourse context.” *arXiv:1606.06031* (2016).'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Touvron, Hugo et al. “LLaMA: Open and Efficient Foundation Language Models.”
    *ArXiv* abs/2302.13971 (2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] [https://github.com/karpathy/llama2.c](https://github.com/karpathy/llama2.c)'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] Eldan, Ronen and Yuan-Fang Li. “TinyStories: How Small Can Language Models
    Be and Still Speak Coherent English?” *ArXiv* abs/2305.07759 (2023)'
  prefs: []
  type: TYPE_NORMAL
