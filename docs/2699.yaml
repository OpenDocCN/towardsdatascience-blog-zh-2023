- en: 'GPTQ or bitsandbytes: Which Quantization Method to Use for LLMs — Examples
    with Llama 2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPTQ 或 bitsandbytes：LLMs 的量化方法选择 — Llama 2 示例
- en: 原文：[https://towardsdatascience.com/gptq-or-bitsandbytes-which-quantization-method-to-use-for-llms-examples-with-llama-2-f79bc03046dc?source=collection_archive---------2-----------------------#2023-08-25](https://towardsdatascience.com/gptq-or-bitsandbytes-which-quantization-method-to-use-for-llms-examples-with-llama-2-f79bc03046dc?source=collection_archive---------2-----------------------#2023-08-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/gptq-or-bitsandbytes-which-quantization-method-to-use-for-llms-examples-with-llama-2-f79bc03046dc?source=collection_archive---------2-----------------------#2023-08-25](https://towardsdatascience.com/gptq-or-bitsandbytes-which-quantization-method-to-use-for-llms-examples-with-llama-2-f79bc03046dc?source=collection_archive---------2-----------------------#2023-08-25)
- en: Large language model quantization for affordable fine-tuning and inference on
    your computer
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大型语言模型量化：实现经济实惠的微调和计算机推理
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----f79bc03046dc--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----f79bc03046dc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f79bc03046dc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f79bc03046dc--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----f79bc03046dc--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie?source=post_page-----f79bc03046dc--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----f79bc03046dc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f79bc03046dc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f79bc03046dc--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----f79bc03046dc--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgptq-or-bitsandbytes-which-quantization-method-to-use-for-llms-examples-with-llama-2-f79bc03046dc&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----f79bc03046dc---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f79bc03046dc--------------------------------)
    ·7 min read·Aug 25, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff79bc03046dc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgptq-or-bitsandbytes-which-quantization-method-to-use-for-llms-examples-with-llama-2-f79bc03046dc&user=Benjamin+Marie&userId=ad2a414578b3&source=-----f79bc03046dc---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgptq-or-bitsandbytes-which-quantization-method-to-use-for-llms-examples-with-llama-2-f79bc03046dc&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----f79bc03046dc---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f79bc03046dc--------------------------------)
    ·7 分钟阅读·2023年8月25日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff79bc03046dc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgptq-or-bitsandbytes-which-quantization-method-to-use-for-llms-examples-with-llama-2-f79bc03046dc&user=Benjamin+Marie&userId=ad2a414578b3&source=-----f79bc03046dc---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff79bc03046dc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgptq-or-bitsandbytes-which-quantization-method-to-use-for-llms-examples-with-llama-2-f79bc03046dc&source=-----f79bc03046dc---------------------bookmark_footer-----------)![](../Images/ff5993f4f2ee78297a0c1cd107099ea9.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff79bc03046dc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgptq-or-bitsandbytes-which-quantization-method-to-use-for-llms-examples-with-llama-2-f79bc03046dc&source=-----f79bc03046dc---------------------bookmark_footer-----------)![](../Images/ff5993f4f2ee78297a0c1cd107099ea9.png)'
- en: Image by the author — Made with an illustration from [Pixabay](https://pixabay.com/vectors/llama-alpaca-animal-mammal-zoo-297668/)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作者插图 — 制作自 [Pixabay](https://pixabay.com/vectors/llama-alpaca-animal-mammal-zoo-297668/)
- en: As large language models (LLM) got bigger with more and more parameters, new
    techniques to reduce their memory usage have also been proposed.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型语言模型（LLM）参数的增多，提出了新的技术来减少其内存使用。
- en: One of the most effective methods to reduce the model size in memory is **quantization**.
    You can see quantization as a compression technique for LLMs. In practice, the
    main goal of quantization is to lower the precision of the LLM’s weights, typically
    from 16-bit to 8-bit, 4-bit, or even 3-bit, with minimal performance degradation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 减少内存中模型大小的最有效方法之一是**量化**。你可以将量化视为对大型语言模型（LLM）的一种压缩技术。实际上，量化的主要目标是降低LLM权重的精度，通常从16位降到8位、4位，甚至3位，同时尽量减少性能下降。
- en: 'There are two popular quantization methods for LLMs: GPTQ and bitsandbytes.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有两种流行的LLM量化方法：GPTQ和bitsandbytes。
- en: In this article, I discuss what the main differences between these two approaches
    are. They both have their own advantages and disadvantages that make them suitable
    for different use cases. I present a comparison of their memory usage and inference
    speed using Llama 2\. I also discuss their performance based on experiments from
    previous work.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我讨论了这两种方法之间的主要区别。它们各自有自己的优缺点，使其适用于不同的使用场景。我展示了使用Llama 2的内存使用情况和推理速度的对比。我还根据以往工作的实验结果讨论了它们的性能。
- en: '*Note: If you want to know more about quantization, I recommend reading this
    excellent introduction by* [*Maxime Labonne*](https://medium.com/u/dc89da634938?source=post_page-----f79bc03046dc--------------------------------)*:*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：如果你想了解更多关于量化的信息，我推荐阅读* [*Maxime Labonne*](https://medium.com/u/dc89da634938?source=post_page-----f79bc03046dc--------------------------------)*的这篇优秀介绍：*'
