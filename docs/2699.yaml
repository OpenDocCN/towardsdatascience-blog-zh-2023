- en: 'GPTQ or bitsandbytes: Which Quantization Method to Use for LLMs — Examples
    with Llama 2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/gptq-or-bitsandbytes-which-quantization-method-to-use-for-llms-examples-with-llama-2-f79bc03046dc?source=collection_archive---------2-----------------------#2023-08-25](https://towardsdatascience.com/gptq-or-bitsandbytes-which-quantization-method-to-use-for-llms-examples-with-llama-2-f79bc03046dc?source=collection_archive---------2-----------------------#2023-08-25)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Large language model quantization for affordable fine-tuning and inference on
    your computer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----f79bc03046dc--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----f79bc03046dc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f79bc03046dc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f79bc03046dc--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----f79bc03046dc--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgptq-or-bitsandbytes-which-quantization-method-to-use-for-llms-examples-with-llama-2-f79bc03046dc&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----f79bc03046dc---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f79bc03046dc--------------------------------)
    ·7 min read·Aug 25, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff79bc03046dc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgptq-or-bitsandbytes-which-quantization-method-to-use-for-llms-examples-with-llama-2-f79bc03046dc&user=Benjamin+Marie&userId=ad2a414578b3&source=-----f79bc03046dc---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff79bc03046dc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgptq-or-bitsandbytes-which-quantization-method-to-use-for-llms-examples-with-llama-2-f79bc03046dc&source=-----f79bc03046dc---------------------bookmark_footer-----------)![](../Images/ff5993f4f2ee78297a0c1cd107099ea9.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image by the author — Made with an illustration from [Pixabay](https://pixabay.com/vectors/llama-alpaca-animal-mammal-zoo-297668/)
  prefs: []
  type: TYPE_NORMAL
- en: As large language models (LLM) got bigger with more and more parameters, new
    techniques to reduce their memory usage have also been proposed.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most effective methods to reduce the model size in memory is **quantization**.
    You can see quantization as a compression technique for LLMs. In practice, the
    main goal of quantization is to lower the precision of the LLM’s weights, typically
    from 16-bit to 8-bit, 4-bit, or even 3-bit, with minimal performance degradation.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two popular quantization methods for LLMs: GPTQ and bitsandbytes.'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I discuss what the main differences between these two approaches
    are. They both have their own advantages and disadvantages that make them suitable
    for different use cases. I present a comparison of their memory usage and inference
    speed using Llama 2\. I also discuss their performance based on experiments from
    previous work.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: If you want to know more about quantization, I recommend reading this
    excellent introduction by* [*Maxime Labonne*](https://medium.com/u/dc89da634938?source=post_page-----f79bc03046dc--------------------------------)*:*'
  prefs: []
  type: TYPE_NORMAL
