- en: Fine-tune Better Chat Models with Distilled Identity Preference Optimization
    (IPO)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/fine-tune-better-chat-models-with-distilled-identity-preference-optimization-ipo-99cddc819a48?source=collection_archive---------7-----------------------#2023-12-13](https://towardsdatascience.com/fine-tune-better-chat-models-with-distilled-identity-preference-optimization-ipo-99cddc819a48?source=collection_archive---------7-----------------------#2023-12-13)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Mistral 7B aligned with IPO
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----99cddc819a48--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----99cddc819a48--------------------------------)[](https://towardsdatascience.com/?source=post_page-----99cddc819a48--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----99cddc819a48--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----99cddc819a48--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-better-chat-models-with-distilled-identity-preference-optimization-ipo-99cddc819a48&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----99cddc819a48---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----99cddc819a48--------------------------------)
    ·6 min read·Dec 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F99cddc819a48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-better-chat-models-with-distilled-identity-preference-optimization-ipo-99cddc819a48&user=Benjamin+Marie&userId=ad2a414578b3&source=-----99cddc819a48---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F99cddc819a48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-better-chat-models-with-distilled-identity-preference-optimization-ipo-99cddc819a48&source=-----99cddc819a48---------------------bookmark_footer-----------)![](../Images/e762c4828a29a687b5f127b736a29db8.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Rishabh Dharmani](https://unsplash.com/@rishabhdharmani?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: To become chat models, pre-trained large language models (LLMs) are fine-tuned
    on large datasets of instructions/questions paired with expected answers. While
    this simple fine-tuning yields convincing chat models, their answers may still
    be incoherent, biased, unethical, and unsafe from a human perspective. This is
    why we usually perform an additional training step to better align the LLM with
    humans.
  prefs: []
  type: TYPE_NORMAL
- en: This alignment can be done using reinforcement learning with human feedback
    (RLHF). As demonstrated by OpenAI and the success of ChatGPT, RLHF can yield state-of-the-art
    chat models. However, RLHF is expensive to run. It requires large datasets annotated
    by humans and the training of several auxiliary models (reference and reward models).
  prefs: []
  type: TYPE_NORMAL
- en: As a simpler and cheaper alternative to RLHF, [direct preference optimization
    (DPO)](https://kaitchup.substack.com/p/fine-tune-your-own-instruct-version) has
    recently been applied with success to align LLMs, such as Hugging Face’s [Zephyr](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)
    and Intel’s [Neural Chat](https://huggingface.co/Intel/neural-chat-7b-v3).
  prefs: []
  type: TYPE_NORMAL
- en: In this article, based on a work by Google DeepMind, we will see that, while
    RLHF and DPO perform well at aligning LLMs, they are far from optimal given the
    datasets used for training. DeepMind also demonstrates why DPO is prone to overfitting.
    I’ll explain, in…
  prefs: []
  type: TYPE_NORMAL
