- en: Fine-tune Better Chat Models with Distilled Identity Preference Optimization
    (IPO)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过蒸馏身份偏好优化（IPO）微调更好的聊天模型
- en: 原文：[https://towardsdatascience.com/fine-tune-better-chat-models-with-distilled-identity-preference-optimization-ipo-99cddc819a48?source=collection_archive---------7-----------------------#2023-12-13](https://towardsdatascience.com/fine-tune-better-chat-models-with-distilled-identity-preference-optimization-ipo-99cddc819a48?source=collection_archive---------7-----------------------#2023-12-13)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/fine-tune-better-chat-models-with-distilled-identity-preference-optimization-ipo-99cddc819a48?source=collection_archive---------7-----------------------#2023-12-13](https://towardsdatascience.com/fine-tune-better-chat-models-with-distilled-identity-preference-optimization-ipo-99cddc819a48?source=collection_archive---------7-----------------------#2023-12-13)
- en: Mistral 7B aligned with IPO
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Mistral 7B 与 IPO 对齐
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----99cddc819a48--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----99cddc819a48--------------------------------)[](https://towardsdatascience.com/?source=post_page-----99cddc819a48--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----99cddc819a48--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----99cddc819a48--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie?source=post_page-----99cddc819a48--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----99cddc819a48--------------------------------)[](https://towardsdatascience.com/?source=post_page-----99cddc819a48--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----99cddc819a48--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----99cddc819a48--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-better-chat-models-with-distilled-identity-preference-optimization-ipo-99cddc819a48&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----99cddc819a48---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----99cddc819a48--------------------------------)
    ·6 min read·Dec 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F99cddc819a48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-better-chat-models-with-distilled-identity-preference-optimization-ipo-99cddc819a48&user=Benjamin+Marie&userId=ad2a414578b3&source=-----99cddc819a48---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-better-chat-models-with-distilled-identity-preference-optimization-ipo-99cddc819a48&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----99cddc819a48---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----99cddc819a48--------------------------------)
    ·6分钟阅读·2023年12月13日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F99cddc819a48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-better-chat-models-with-distilled-identity-preference-optimization-ipo-99cddc819a48&user=Benjamin+Marie&userId=ad2a414578b3&source=-----99cddc819a48---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F99cddc819a48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-better-chat-models-with-distilled-identity-preference-optimization-ipo-99cddc819a48&source=-----99cddc819a48---------------------bookmark_footer-----------)![](../Images/e762c4828a29a687b5f127b736a29db8.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F99cddc819a48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-better-chat-models-with-distilled-identity-preference-optimization-ipo-99cddc819a48&source=-----99cddc819a48---------------------bookmark_footer-----------)![](../Images/e762c4828a29a687b5f127b736a29db8.png)'
- en: Photo by [Rishabh Dharmani](https://unsplash.com/@rishabhdharmani?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Rishabh Dharmani](https://unsplash.com/@rishabhdharmani?utm_source=medium&utm_medium=referral)
    提供，拍摄于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: To become chat models, pre-trained large language models (LLMs) are fine-tuned
    on large datasets of instructions/questions paired with expected answers. While
    this simple fine-tuning yields convincing chat models, their answers may still
    be incoherent, biased, unethical, and unsafe from a human perspective. This is
    why we usually perform an additional training step to better align the LLM with
    humans.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: This alignment can be done using reinforcement learning with human feedback
    (RLHF). As demonstrated by OpenAI and the success of ChatGPT, RLHF can yield state-of-the-art
    chat models. However, RLHF is expensive to run. It requires large datasets annotated
    by humans and the training of several auxiliary models (reference and reward models).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: As a simpler and cheaper alternative to RLHF, [direct preference optimization
    (DPO)](https://kaitchup.substack.com/p/fine-tune-your-own-instruct-version) has
    recently been applied with success to align LLMs, such as Hugging Face’s [Zephyr](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)
    and Intel’s [Neural Chat](https://huggingface.co/Intel/neural-chat-7b-v3).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: In this article, based on a work by Google DeepMind, we will see that, while
    RLHF and DPO perform well at aligning LLMs, they are far from optimal given the
    datasets used for training. DeepMind also demonstrates why DPO is prone to overfitting.
    I’ll explain, in…
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
