["```py\nimport requests\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\nfrom pandas.tseries.offsets import MonthEnd\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.stattools import grangercausalitytests\n\nfrom pmdarima import auto_arima\n```", "```py\nFRED_API_KEY = '__YOUR_API_KEY__'\n\n# Function to read data from FRED API\ndef get_fred_data(data_id, data_name):\n  response = requests.get(f'https://api.stlouisfed.org/fred/series/observations?series_id={data_id}&api_key={FRED_API_KEY}&file_type=json')\n  df = pd.DataFrame(response.json()['observations'])[['date', 'value']].rename(columns={'value': data_name})\n  df[data_name] = pd.to_numeric(df[data_name], errors='coerce')\n  df['date'] = pd.to_datetime(df['date']) + MonthEnd(1)\n  df.set_index('date', inplace=True)\n  df.index.freq='M'\n  return df\n```", "```py\ndependent_timeseries_id = 'MRTSSM4453USN'\ndependent_timeseries_name = 'liquor_sales'\n\npotential_leading_indicators = {\n    'USACPIALLMINMEI': 'consumer_price_index',\n    'PCU44534453': 'liquor_ppi',\n    'DSPIC96': 'real_income',\n    'LFWA25TTUSM647S': 'working_age_population',\n    'UNRATENSA': 'unemployment_rate',\n    'LNU01300000': 'labor_force_participation',\n    'A063RC1': 'social_benefits',\n    'CCLACBM027NBOG': 'consumer_loans',\n}\n```", "```py\n# Read dependent time series\ntimeseries = get_fred_data(dependent_timeseries_id, dependent_timeseries_name)\n\n# Join timeseries with potential leading indicators\nfor data_id, data_name in potential_leading_indicators.items():\n  df = get_fred_data(data_id, data_name)\n  timeseries = timeseries.join(df)\n\n# We will start our analysis from Jan-2010\ntimeseries = timeseries['2010':]\n\n# add month we want to predict liquor_sales \ntimeseries = timeseries.reindex(timeseries.index.union([timeseries.index[-1] + timeseries.index.freq]))\n\ntimeseries\n```", "```py\ntimeseries[dependent_timeseries_name].plot(figsize=(20,8));\n```", "```py\n# create a copy of the timeseries to use for tests. Be sure to exclude the additional row we added in the previous task\ntimeseries_for_gc_tests = timeseries[:-1]\nall_cols = timeseries_for_gc_tests.columns\n\nstationary_cols = []\ndiff_times = 0\n\nwhile True:\n\n  # Test for stationarity\n  for col in all_cols:\n    adf, pvalue, lagsused, observations, critical_values, icbest = adfuller(timeseries_for_gc_tests[col])\n    if pvalue <= 0.05:\n      stationary_cols.append(col)\n\n  # Difference the time series if at least one column fails the stationary test\n  if set(stationary_cols) != set(all_cols):\n    timeseries_for_gc_tests = timeseries_for_gc_tests.diff().dropna()\n    diff_times += 1\n    stationary_cols = []\n  else:\n    print(f'No of Differencing: {diff_times}')\n    break\n```", "```py\nmaxlag = 6 # represents the maximum number of past time periods to look for potential causality. We cap ours at 6 months\nleading_indicators = []\n\nfor x in all_cols[1:]:\n    gc_res = grangercausalitytests(timeseries_for_gc_tests[[dependent_timeseries_name, x]], maxlag=maxlag, verbose=0)\n    leading_indicators_tmp = []\n    for lag in range(1, maxlag+1):\n        ftest_stat = gc_res[lag][0]['ssr_ftest'][0]\n        ftest_pvalue = gc_res[lag][0]['ssr_ftest'][1]\n        if ftest_pvalue <= 0.05:\n            leading_indicators_tmp.append({'x': x, 'lag': lag, 'ftest_pvalue': ftest_pvalue, 'ftest_stat': ftest_stat, 'xlabel': f'{x}__{lag}_mths_ago'})\n    if leading_indicators_tmp:\n        leading_indicators.append(max(leading_indicators_tmp, key=lambda x:x['ftest_stat']))\n\n# Display leading indicators as a dataframe\npd.DataFrame(leading_indicators).reset_index(drop=True).reset_index(drop=True)\n```", "```py\n# shift the leading indicators by their corresponding lag periods\nfor i in leading_indicators:\n  timeseries[i['xlabel']] = timeseries[i['x']].shift(periods=i['lag'], freq='M')\n\n# select only the dependent_timeseries_name and leading indicators for further analysis\ntimeseries = timeseries[[dependent_timeseries_name, *[i['xlabel'] for i in leading_indicators]]].dropna(subset=[i['xlabel'] for i in leading_indicators], axis=0)\ntimeseries\n```", "```py\n# Scale dependent timeseries\ny_scaler = StandardScaler()\ndependent_timeseries_scaled = y_scaler.fit_transform(timeseries[[dependent_timeseries_name]])\n\n# Scale leading indicators\nX_scaler = StandardScaler()\nleading_indicators_scaled = X_scaler.fit_transform(timeseries[[i['xlabel'] for i in leading_indicators]])\n```", "```py\n# Reduce dimensionality of the leading indicators\npca = PCA(n_components=0.90)\nleading_indicators_scaled_components = pca.fit_transform(leading_indicators_scaled)\n\nleading_indicators_scaled_components.shape\n```", "```py\n# Build SARIMAX model\nperiods_in_cycle = 12 # number of periods per cycle. In our case, its 12 months\nmodel = auto_arima(y=dependent_timeseries_scaled[:-2], X=leading_indicators_scaled_components[:-2], seasonal=True, m=periods_in_cycle)\nmodel.summary()\n```", "```py\n# Forecast the next two periods\npreds_scaled = model.predict(n_periods=2, X=leading_indicators_scaled_components[-2:])\npred_2024_06_30, pred_2024_07_31 = np.round(y_scaler.inverse_transform([preds_scaled]))[0]\n\nprint(\"TEST\\n----\")\nprint(f\"Actual Liquor Sales for 2024-06-30: {timeseries[dependent_timeseries_name]['2024-06-30']}\")\nprint(f\"Predicted Liquor Sales for 2024-06-30: {pred_2024_06_30}\")\nprint(f\"MAPE: {mean_absolute_percentage_error([timeseries[dependent_timeseries_name]['2024-06-30']], [pred_2024_06_30]):.1%}\")\n\nprint(\"\\nFORECAST\\n--------\")\nprint(f\"Forecasted Liquor Sales for 2024-07-31: {pred_2024_07_31}\")\n```"]