- en: 4 Ways to Do Question Answering in LangChain
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LangChain中的4种问题回答方式
- en: 原文：[https://towardsdatascience.com/4-ways-of-question-answering-in-langchain-188c6707cc5a?source=collection_archive---------0-----------------------#2023-04-08](https://towardsdatascience.com/4-ways-of-question-answering-in-langchain-188c6707cc5a?source=collection_archive---------0-----------------------#2023-04-08)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/4-ways-of-question-answering-in-langchain-188c6707cc5a?source=collection_archive---------0-----------------------#2023-04-08](https://towardsdatascience.com/4-ways-of-question-answering-in-langchain-188c6707cc5a?source=collection_archive---------0-----------------------#2023-04-08)
- en: '**Chat with your long PDF docs: load_qa_chain, RetrievalQA, VectorstoreIndexCreator,
    ConversationalRetrievalChain**'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**与长PDF文档对话：load_qa_chain、RetrievalQA、VectorstoreIndexCreator、ConversationalRetrievalChain**'
- en: '[](https://sophiamyang.medium.com/?source=post_page-----188c6707cc5a--------------------------------)[![Sophia
    Yang, Ph.D.](../Images/c133f918245ea4857dc46df3a07fc2b1.png)](https://sophiamyang.medium.com/?source=post_page-----188c6707cc5a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----188c6707cc5a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----188c6707cc5a--------------------------------)
    [Sophia Yang, Ph.D.](https://sophiamyang.medium.com/?source=post_page-----188c6707cc5a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://sophiamyang.medium.com/?source=post_page-----188c6707cc5a--------------------------------)[![Sophia
    Yang, Ph.D.](../Images/c133f918245ea4857dc46df3a07fc2b1.png)](https://sophiamyang.medium.com/?source=post_page-----188c6707cc5a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----188c6707cc5a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----188c6707cc5a--------------------------------)
    [Sophia Yang, Ph.D.](https://sophiamyang.medium.com/?source=post_page-----188c6707cc5a--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fae9cae9cbcd2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-of-question-answering-in-langchain-188c6707cc5a&user=Sophia+Yang%2C+Ph.D.&userId=ae9cae9cbcd2&source=post_page-ae9cae9cbcd2----188c6707cc5a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----188c6707cc5a--------------------------------)
    ·6 min read·Apr 8, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F188c6707cc5a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-of-question-answering-in-langchain-188c6707cc5a&user=Sophia+Yang%2C+Ph.D.&userId=ae9cae9cbcd2&source=-----188c6707cc5a---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fae9cae9cbcd2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-of-question-answering-in-langchain-188c6707cc5a&user=Sophia+Yang%2C+Ph.D.&userId=ae9cae9cbcd2&source=post_page-ae9cae9cbcd2----188c6707cc5a---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----188c6707cc5a--------------------------------)
    ·6分钟阅读·2023年4月8日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F188c6707cc5a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-of-question-answering-in-langchain-188c6707cc5a&user=Sophia+Yang%2C+Ph.D.&userId=ae9cae9cbcd2&source=-----188c6707cc5a---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F188c6707cc5a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-of-question-answering-in-langchain-188c6707cc5a&source=-----188c6707cc5a---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F188c6707cc5a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-of-question-answering-in-langchain-188c6707cc5a&source=-----188c6707cc5a---------------------bookmark_footer-----------)'
- en: Are you interested in chatting with your own documents, whether it is a text
    file, a PDF, or a website? LangChain makes it easy for you to do question answering
    with your documents. But do you know that there are at least 4 ways to do question
    answering in LangChain? In this blog post, we are going to explore four different
    ways to do question-answering and the various options you could consider for your
    use cases.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否对与自己的文档对话感兴趣，无论是文本文件、PDF还是网站？LangChain使得在文档中进行问题回答变得简单。但你知道在LangChain中至少有4种方式可以进行问题回答吗？在这篇博客文章中，我们将深入探讨四种不同的提问方式以及你可以考虑的各种选项。
- en: 'Before we dive into question answering, you may wonder: what is LangChain?
    Great question! In my opinion, LangChain is the easiest way to interact with language
    models and build applications. It is an **open-source** tool that wraps around
    many LLMs and tools. Check out my previous [blog post](/the-easiest-way-to-interact-with-language-models-4da158cfb5c5?sk=271c9c82a16282f93ef3df37f034babe)
    and [video](https://www.youtube.com/watch?v=kmbS6FDQh7c) on an overview of LangChain.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨问答之前，你可能会问：什么是 LangChain？好问题！在我看来，LangChain 是与语言模型互动并构建应用程序的最简单方法。它是一个**开源**工具，封装了许多
    LLM 和工具。查看我之前的 [博客文章](/the-easiest-way-to-interact-with-language-models-4da158cfb5c5?sk=271c9c82a16282f93ef3df37f034babe)
    和 [视频](https://www.youtube.com/watch?v=kmbS6FDQh7c) 以了解 LangChain 的概述。
- en: Okay, now let’s get started with question-answering on external documents.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在让我们开始对外部文档进行问答。
- en: '**Code**: Check out the code for this blog post [here](https://github.com/sophiamyang/tutorials-LangChain/blob/main/LangChain_QA.ipynb).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码**：查看本博客文章的代码 [这里](https://github.com/sophiamyang/tutorials-LangChain/blob/main/LangChain_QA.ipynb)。'
- en: Set up OpenAI API
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置 OpenAI API
- en: 'Create an account at OpenAI and create an API key: [https://platform.openai.com/account](https://platform.openai.com/account).
    Note that OpenAI API is not free. You will need to set up billing information
    there to be able to use OpenAI API. Alternatively, you can use models from HuggingFace
    Hub or other places. Check out my previous [blog post](/the-easiest-way-to-interact-with-language-models-4da158cfb5c5?sk=271c9c82a16282f93ef3df37f034babe)
    and [video](https://www.youtube.com/watch?v=kmbS6FDQh7c) on how to use other models.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在 OpenAI 上创建一个帐户并生成 API 密钥：[https://platform.openai.com/account](https://platform.openai.com/account)。请注意，OpenAI
    API 并非免费。你需要在此处设置账单信息才能使用 OpenAI API。或者，你可以使用 HuggingFace Hub 或其他地方的模型。查看我之前的
    [博客文章](/the-easiest-way-to-interact-with-language-models-4da158cfb5c5?sk=271c9c82a16282f93ef3df37f034babe)
    和 [视频](https://www.youtube.com/watch?v=kmbS6FDQh7c) 来了解如何使用其他模型。
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Load documents
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载文档
- en: LangChain supports many many [Document Loaders](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html)
    such as Notion, YouTube, and Figma. In this example, I’d like to chat with my
    PDF file. Thus, I used the PyPDFLoader to load my file. I’m actually using Chapter
    1 of the [AI index report](https://aiindex.stanford.edu/report/), which includes
    55 pages, and I saved it in the materials directory of my Github [repo](https://github.com/sophiamyang/tutorials-LangChain).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 支持许多 [文档加载器](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html)，例如
    Notion、YouTube 和 Figma。在这个例子中，我想与我的 PDF 文件进行对话。因此，我使用了 PyPDFLoader 来加载我的文件。我实际使用的是
    [AI index report](https://aiindex.stanford.edu/report/) 的第 1 章，其中包括 55 页，并将其保存在我
    GitHub [库](https://github.com/sophiamyang/tutorials-LangChain) 的材料目录中。
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Method 1: load_qa_chain**'
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**方法 1：load_qa_chain**'
- en: '`load_qa_chain` provides the most generic interface for answering questions.
    It loads a chain that you can do QA for your input documents and uses ALL of the
    text in the documents.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`load_qa_chain` 提供了最通用的问答接口。它加载一个链，你可以对输入文档进行问答，并使用文档中的所有文本。'
- en: '![](../Images/b1c7f091af31506e27712127834f80ab.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b1c7f091af31506e27712127834f80ab.png)'
- en: 'It also lets you do QA over a set of documents:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 它还允许你对一组文档进行问答：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 🤔***But what if my document is super long that it exceeds the token limit?***
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 🤔***但如果我的文档非常长，超出了令牌限制怎么办？***
- en: 'There are two ways to fix it:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种解决方法：
- en: '**Solution 1: Chain Type**'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**解决方案 1：链式类型**'
- en: The default `chain_type="stuff"` uses ALL of the text from the documents in
    the prompt. It actually doesn’t work with our example because it exceeds the token
    limit and causes rate-limiting errors. That’s why in this example, we had to use
    other chain types for example `"map_reduce"`. What are the other chain types?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的 `chain_type="stuff"` 使用了文档中所有的文本。实际上，它不适用于我们的例子，因为它超出了令牌限制并导致速率限制错误。这就是为什么在这个例子中，我们不得不使用其他链式类型，比如
    `"map_reduce"`。还有哪些其他链式类型？
- en: '`map_reduce`: It separates texts into batches (as an example, you can define
    batch size in `llm=OpenAI(batch_size=5)`), feeds each batch with the question
    to LLM separately, and comes up with the final answer based on the answers from
    each batch.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map_reduce`：它将文本分成批次（例如，你可以在 `llm=OpenAI(batch_size=5)` 中定义批次大小），分别将每个批次的问题传递给
    LLM，并根据每个批次的答案得出最终答案。'
- en: '`refine` : It separates texts into batches, feeds the first batch to LLM, and
    feeds the answer and the second batch to LLM. It refines the answer by going through
    all the batches.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`refine`：它将文本分成批次，将第一个批次传递给 LLM，然后将答案和第二个批次传递给 LLM。它通过处理所有批次来完善答案。'
- en: '`map-rerank`: It separates texts into batches, feeds each batch to LLM, returns
    a score of how fully it answers the question, and comes up with the final answer
    based on the high-scored answers from each batch.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map-rerank`：它将文本分成批次，逐个批次地输入到 LLM 中，返回每个批次对问题的回答完整程度的评分，并根据每个批次中高评分的回答得出最终答案。'
- en: 'Solution 2: RetrievalQA'
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案 2：RetrievalQA
- en: One issue with using ALL of the text is that it can be very costly because you
    are feeding all the texts to OpenAI API and the API is charged by the number of
    tokens. A better solution is to retrieve relevant text chunks first and only use
    the relevant text chunks in the language model. I’m going to go through the details
    of RetrievalQA next.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用所有文本的一个问题是，成本可能非常高，因为你将所有文本都传送给 OpenAI API，而 API 是按令牌数量收费的。更好的解决方案是先检索相关文本块，然后仅在语言模型中使用相关文本块。接下来，我将详细介绍
    RetrievalQA。
- en: 'Method 2: RetrievalQA'
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法 2：RetrievalQA
- en: '`RetrievalQA` chain actually uses `load_qa_chain` under the hood. We retrieve
    the most relevant chunk of text and feed those to the language model.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`RetrievalQA` 链实际上在底层使用 `load_qa_chain`。我们检索最相关的文本块，并将这些文本块输入到语言模型中。'
- en: 'Here is how it works:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其工作原理如下：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the result, we can see the answer and two source documents because we defined
    k as 2 meaning that we are only interested in getting two relevant text chunks.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在结果中，我们可以看到答案和两个源文档，因为我们将 k 定义为 2，这意味着我们只关心获取两个相关的文本块。
- en: '![](../Images/17962d4e7c9bcaa236b0fe1365859aff.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17962d4e7c9bcaa236b0fe1365859aff.png)'
- en: 'Options:'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选项：
- en: 'There are various options for you to choose from in this process:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中你可以选择各种选项：
- en: '[embeddings](https://python.langchain.com/en/latest/reference/modules/embeddings.html):
    In the example, we used OpenAI Embeddings. But there are many other embedding
    options such as Cohere Embeddings, and HuggingFaceEmbeddings from specific models.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[嵌入](https://python.langchain.com/en/latest/reference/modules/embeddings.html)：在示例中，我们使用了
    OpenAI 嵌入。但还有许多其他嵌入选项，如 Cohere Embeddings 和来自特定模型的 HuggingFaceEmbeddings。'
- en: '[TextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html):
    We used Character Text Splitter in the example where the text is split by a single
    character. You can also different text splitters and different tokens mentioned
    in this [doc](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html).'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文本分割器](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html)：在示例中，我们使用了按单个字符分割的
    Character Text Splitter。你也可以使用不同的文本分割器和不同的令牌，具体请参见此 [文档](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html)。'
- en: '[VectorStore](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html):
    We used Chroma as our vector database where we store our embedded text vectors.
    Other popular options are FAISS, Mulvus, and Pinecone.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[向量存储](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html)：我们使用了
    Chroma 作为我们的向量数据库，用于存储嵌入的文本向量。其他流行的选项包括 FAISS、Milvus 和 Pinecone。'
- en: '[Retrievers](https://python.langchain.com/en/latest/modules/indexes/retrievers.html):
    We used a VectoreStoreRetriver, which is backed by a VectorStore. To retrieve
    text, there are two search types you can choose: [search_type](https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/chroma.html#mmr):
    “similarity” or “mmr”. `search_type="similarity"` uses similarity search in the
    retriever object where it selects text chunk vectors that are most similar to
    the question vector. `search_type="mmr"` uses the maximum marginal relevance search
    where it optimizes for similarity to query AND diversity among selected documents.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[检索器](https://python.langchain.com/en/latest/modules/indexes/retrievers.html)：我们使用了一个
    VectoreStoreRetriver，它由 VectorStore 支持。为了检索文本，你可以选择两种搜索类型：[search_type](https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/chroma.html#mmr)：“similarity”
    或 “mmr”。`search_type="similarity"` 使用检索器对象中的相似性搜索，其中选择最与问题向量相似的文本块向量。`search_type="mmr"`
    使用最大边际相关性搜索，它在优化查询相似性和选择文档的多样性之间取得平衡。'
- en: '[Chain Type](https://python.langchain.com/en/latest/modules/chains/index_examples/question_answering.html):
    same as method 1\. You can also define the chain type as one of the four options:
    “stuff”, “map reduce”, “refine”, “map_rerank”.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[链类型](https://python.langchain.com/en/latest/modules/chains/index_examples/question_answering.html)：与方法
    1 相同。你也可以将链类型定义为四种选项之一：“stuff”、“map reduce”、“refine”、“map_rerank”。'
- en: '**Method 3: VectorstoreIndexCreator**'
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**方法 3：VectorstoreIndexCreator**'
- en: 'VectorstoreIndexCreator is a wrapper around the above functionality. It is
    exactly the same under the hood, but just exposes a higher-level interface to
    let you get started in three lines of code:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: VectorstoreIndexCreator 是上述功能的封装器。它在底层功能上完全相同，但提供了更高级的接口，让你可以用三行代码开始使用：
- en: '![](../Images/001cc8d911b3430c307620adb8637998.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/001cc8d911b3430c307620adb8637998.png)'
- en: 'Of course, you can also specify different options in this wrapper:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你也可以在这个包装器中指定不同的选项。
- en: '![](../Images/064f4d89bdedd54ae1a94b11a6095052.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/064f4d89bdedd54ae1a94b11a6095052.png)'
- en: '**Method 4: ConversationalRetrievalChain**'
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**方法 4：ConversationalRetrievalChain**'
- en: ConversationalRetrievalChain is very similar to method 2 RetrievalQA. It added
    an additional parameter `chat_history` to pass in chat history which can be used
    for follow-up questions.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ConversationalRetrievalChain 与方法 2 RetrievalQA 非常相似。它增加了一个额外的参数 `chat_history`
    用于传递聊天记录，这可以用于后续的问题。
- en: '*ConversationalRetrievalChain = conversation memory + RetrievalQAChain*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*ConversationalRetrievalChain = 对话记忆 + RetrievalQAChain*'
- en: If you would like your language model to have a memory of the previous conversation,
    use this method. In my example below, I asked about the number of AI publications
    and got the result of 500,000\. Then I asked the LLM to divide this number by
    2\. Since it has all the chat history, the model knows the number I was referring
    to is 500,000 and the result returned is 250,000.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望你的语言模型记住之前的对话，可以使用这种方法。在我下面的例子中，我询问了 AI 发表的数量，并得到了 500,000 的结果。然后我让 LLM
    将这个数字除以 2。由于它拥有所有聊天记录，模型知道我指的是 500,000，因此返回的结果是 250,000。
- en: '![](../Images/13079f5b3ba06bcc78db50535722b4e5.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13079f5b3ba06bcc78db50535722b4e5.png)'
- en: Conclusion
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Now you know four ways to do question answering with LLMs in LangChain. In summary,
    load_qa_chain uses all texts and accepts multiple documents; RetrievalQA uses
    load_qa_chain under the hood but retrieves relevant text chunks first; VectorstoreIndexCreator
    is the same as RetrievalQA with a higher-level interface; ConversationalRetrievalChain
    is useful when you want to pass in your chat history to the model.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你知道了四种使用 LangChain 中 LLM 进行问答的方法。总结起来，load_qa_chain 使用所有文本并接受多个文档；RetrievalQA
    在底层使用 load_qa_chain，但首先检索相关的文本块；VectorstoreIndexCreator 与 RetrievalQA 相同，但提供了更高层次的接口；ConversationalRetrievalChain
    在你想将聊天记录传递给模型时非常有用。
- en: '**Acknowledgment**:'
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**致谢**：'
- en: Thank you Harrison Chase for the guidance!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢 Harrison Chase 的指导！
- en: '![](../Images/40a328419509baad0b94db6142a3c393.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40a328419509baad0b94db6142a3c393.png)'
- en: Photo by [FLY:D](https://unsplash.com/@flyd2069?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/ZNOxwCEj5mw?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [FLY:D](https://unsplash.com/@flyd2069?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来源于 [Unsplash](https://unsplash.com/photos/ZNOxwCEj5mw?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)。
- en: . . .
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: . . .
- en: By [Sophia Yang](https://www.linkedin.com/in/sophiamyang/) on April 8, 2023
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 由 [Sophia Yang](https://www.linkedin.com/in/sophiamyang/) 提供，日期为 2023 年 4 月
    8 日
- en: Sophia Yang is a Senior Data Scientist. Connect with me on [LinkedIn](https://www.linkedin.com/in/sophiamyang/),
    [Twitter](https://twitter.com/sophiamyang), and [YouTube](https://www.youtube.com/SophiaYangDS)
    and join the DS/ML [Book Club](https://dsbookclub.github.io/) ❤️
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Sophia Yang 是高级数据科学家。可以在 [LinkedIn](https://www.linkedin.com/in/sophiamyang/)、[Twitter](https://twitter.com/sophiamyang)
    和 [YouTube](https://www.youtube.com/SophiaYangDS) 上与我联系，并加入 DS/ML [书友会](https://dsbookclub.github.io/)
    ❤️
