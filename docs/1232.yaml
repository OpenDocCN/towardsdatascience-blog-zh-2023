- en: 4 Ways to Do Question Answering in LangChain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/4-ways-of-question-answering-in-langchain-188c6707cc5a?source=collection_archive---------0-----------------------#2023-04-08](https://towardsdatascience.com/4-ways-of-question-answering-in-langchain-188c6707cc5a?source=collection_archive---------0-----------------------#2023-04-08)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Chat with your long PDF docs: load_qa_chain, RetrievalQA, VectorstoreIndexCreator,
    ConversationalRetrievalChain**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://sophiamyang.medium.com/?source=post_page-----188c6707cc5a--------------------------------)[![Sophia
    Yang, Ph.D.](../Images/c133f918245ea4857dc46df3a07fc2b1.png)](https://sophiamyang.medium.com/?source=post_page-----188c6707cc5a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----188c6707cc5a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----188c6707cc5a--------------------------------)
    [Sophia Yang, Ph.D.](https://sophiamyang.medium.com/?source=post_page-----188c6707cc5a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fae9cae9cbcd2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-of-question-answering-in-langchain-188c6707cc5a&user=Sophia+Yang%2C+Ph.D.&userId=ae9cae9cbcd2&source=post_page-ae9cae9cbcd2----188c6707cc5a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----188c6707cc5a--------------------------------)
    ¬∑6 min read¬∑Apr 8, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F188c6707cc5a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-of-question-answering-in-langchain-188c6707cc5a&user=Sophia+Yang%2C+Ph.D.&userId=ae9cae9cbcd2&source=-----188c6707cc5a---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F188c6707cc5a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-of-question-answering-in-langchain-188c6707cc5a&source=-----188c6707cc5a---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: Are you interested in chatting with your own documents, whether it is a text
    file, a PDF, or a website? LangChain makes it easy for you to do question answering
    with your documents. But do you know that there are at least 4 ways to do question
    answering in LangChain? In this blog post, we are going to explore four different
    ways to do question-answering and the various options you could consider for your
    use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we dive into question answering, you may wonder: what is LangChain?
    Great question! In my opinion, LangChain is the easiest way to interact with language
    models and build applications. It is an **open-source** tool that wraps around
    many LLMs and tools. Check out my previous [blog post](/the-easiest-way-to-interact-with-language-models-4da158cfb5c5?sk=271c9c82a16282f93ef3df37f034babe)
    and [video](https://www.youtube.com/watch?v=kmbS6FDQh7c) on an overview of LangChain.'
  prefs: []
  type: TYPE_NORMAL
- en: Okay, now let‚Äôs get started with question-answering on external documents.
  prefs: []
  type: TYPE_NORMAL
- en: '**Code**: Check out the code for this blog post [here](https://github.com/sophiamyang/tutorials-LangChain/blob/main/LangChain_QA.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Set up OpenAI API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create an account at OpenAI and create an API key: [https://platform.openai.com/account](https://platform.openai.com/account).
    Note that OpenAI API is not free. You will need to set up billing information
    there to be able to use OpenAI API. Alternatively, you can use models from HuggingFace
    Hub or other places. Check out my previous [blog post](/the-easiest-way-to-interact-with-language-models-4da158cfb5c5?sk=271c9c82a16282f93ef3df37f034babe)
    and [video](https://www.youtube.com/watch?v=kmbS6FDQh7c) on how to use other models.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Load documents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LangChain supports many many [Document Loaders](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html)
    such as Notion, YouTube, and Figma. In this example, I‚Äôd like to chat with my
    PDF file. Thus, I used the PyPDFLoader to load my file. I‚Äôm actually using Chapter
    1 of the [AI index report](https://aiindex.stanford.edu/report/), which includes
    55 pages, and I saved it in the materials directory of my Github [repo](https://github.com/sophiamyang/tutorials-LangChain).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Method 1: load_qa_chain**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`load_qa_chain` provides the most generic interface for answering questions.
    It loads a chain that you can do QA for your input documents and uses ALL of the
    text in the documents.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b1c7f091af31506e27712127834f80ab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It also lets you do QA over a set of documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: ü§î***But what if my document is super long that it exceeds the token limit?***
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two ways to fix it:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Solution 1: Chain Type**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The default `chain_type="stuff"` uses ALL of the text from the documents in
    the prompt. It actually doesn‚Äôt work with our example because it exceeds the token
    limit and causes rate-limiting errors. That‚Äôs why in this example, we had to use
    other chain types for example `"map_reduce"`. What are the other chain types?
  prefs: []
  type: TYPE_NORMAL
- en: '`map_reduce`: It separates texts into batches (as an example, you can define
    batch size in `llm=OpenAI(batch_size=5)`), feeds each batch with the question
    to LLM separately, and comes up with the final answer based on the answers from
    each batch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`refine` : It separates texts into batches, feeds the first batch to LLM, and
    feeds the answer and the second batch to LLM. It refines the answer by going through
    all the batches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`map-rerank`: It separates texts into batches, feeds each batch to LLM, returns
    a score of how fully it answers the question, and comes up with the final answer
    based on the high-scored answers from each batch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Solution 2: RetrievalQA'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One issue with using ALL of the text is that it can be very costly because you
    are feeding all the texts to OpenAI API and the API is charged by the number of
    tokens. A better solution is to retrieve relevant text chunks first and only use
    the relevant text chunks in the language model. I‚Äôm going to go through the details
    of RetrievalQA next.
  prefs: []
  type: TYPE_NORMAL
- en: 'Method 2: RetrievalQA'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`RetrievalQA` chain actually uses `load_qa_chain` under the hood. We retrieve
    the most relevant chunk of text and feed those to the language model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In the result, we can see the answer and two source documents because we defined
    k as 2 meaning that we are only interested in getting two relevant text chunks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/17962d4e7c9bcaa236b0fe1365859aff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Options:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are various options for you to choose from in this process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[embeddings](https://python.langchain.com/en/latest/reference/modules/embeddings.html):
    In the example, we used OpenAI Embeddings. But there are many other embedding
    options such as Cohere Embeddings, and HuggingFaceEmbeddings from specific models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html):
    We used Character Text Splitter in the example where the text is split by a single
    character. You can also different text splitters and different tokens mentioned
    in this [doc](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[VectorStore](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html):
    We used Chroma as our vector database where we store our embedded text vectors.
    Other popular options are FAISS, Mulvus, and Pinecone.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Retrievers](https://python.langchain.com/en/latest/modules/indexes/retrievers.html):
    We used a VectoreStoreRetriver, which is backed by a VectorStore. To retrieve
    text, there are two search types you can choose: [search_type](https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/chroma.html#mmr):
    ‚Äúsimilarity‚Äù or ‚Äúmmr‚Äù. `search_type="similarity"` uses similarity search in the
    retriever object where it selects text chunk vectors that are most similar to
    the question vector. `search_type="mmr"` uses the maximum marginal relevance search
    where it optimizes for similarity to query AND diversity among selected documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chain Type](https://python.langchain.com/en/latest/modules/chains/index_examples/question_answering.html):
    same as method 1\. You can also define the chain type as one of the four options:
    ‚Äústuff‚Äù, ‚Äúmap reduce‚Äù, ‚Äúrefine‚Äù, ‚Äúmap_rerank‚Äù.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Method 3: VectorstoreIndexCreator**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'VectorstoreIndexCreator is a wrapper around the above functionality. It is
    exactly the same under the hood, but just exposes a higher-level interface to
    let you get started in three lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/001cc8d911b3430c307620adb8637998.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Of course, you can also specify different options in this wrapper:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/064f4d89bdedd54ae1a94b11a6095052.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Method 4: ConversationalRetrievalChain**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ConversationalRetrievalChain is very similar to method 2 RetrievalQA. It added
    an additional parameter `chat_history` to pass in chat history which can be used
    for follow-up questions.
  prefs: []
  type: TYPE_NORMAL
- en: '*ConversationalRetrievalChain = conversation memory + RetrievalQAChain*'
  prefs: []
  type: TYPE_NORMAL
- en: If you would like your language model to have a memory of the previous conversation,
    use this method. In my example below, I asked about the number of AI publications
    and got the result of 500,000\. Then I asked the LLM to divide this number by
    2\. Since it has all the chat history, the model knows the number I was referring
    to is 500,000 and the result returned is 250,000.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/13079f5b3ba06bcc78db50535722b4e5.png)'
  prefs: []
  type: TYPE_IMG
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now you know four ways to do question answering with LLMs in LangChain. In summary,
    load_qa_chain uses all texts and accepts multiple documents; RetrievalQA uses
    load_qa_chain under the hood but retrieves relevant text chunks first; VectorstoreIndexCreator
    is the same as RetrievalQA with a higher-level interface; ConversationalRetrievalChain
    is useful when you want to pass in your chat history to the model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Acknowledgment**:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thank you Harrison Chase for the guidance!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/40a328419509baad0b94db6142a3c393.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [FLY:D](https://unsplash.com/@flyd2069?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/ZNOxwCEj5mw?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: . . .
  prefs: []
  type: TYPE_NORMAL
- en: By [Sophia Yang](https://www.linkedin.com/in/sophiamyang/) on April 8, 2023
  prefs: []
  type: TYPE_NORMAL
- en: Sophia Yang is a Senior Data Scientist. Connect with me on [LinkedIn](https://www.linkedin.com/in/sophiamyang/),
    [Twitter](https://twitter.com/sophiamyang), and [YouTube](https://www.youtube.com/SophiaYangDS)
    and join the DS/ML [Book Club](https://dsbookclub.github.io/) ‚ù§Ô∏è
  prefs: []
  type: TYPE_NORMAL
