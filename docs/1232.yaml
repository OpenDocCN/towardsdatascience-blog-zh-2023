- en: 4 Ways to Do Question Answering in LangChain
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LangChainä¸­çš„4ç§é—®é¢˜å›ç­”æ–¹å¼
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/4-ways-of-question-answering-in-langchain-188c6707cc5a?source=collection_archive---------0-----------------------#2023-04-08](https://towardsdatascience.com/4-ways-of-question-answering-in-langchain-188c6707cc5a?source=collection_archive---------0-----------------------#2023-04-08)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/4-ways-of-question-answering-in-langchain-188c6707cc5a?source=collection_archive---------0-----------------------#2023-04-08](https://towardsdatascience.com/4-ways-of-question-answering-in-langchain-188c6707cc5a?source=collection_archive---------0-----------------------#2023-04-08)
- en: '**Chat with your long PDF docs: load_qa_chain, RetrievalQA, VectorstoreIndexCreator,
    ConversationalRetrievalChain**'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ä¸é•¿PDFæ–‡æ¡£å¯¹è¯ï¼šload_qa_chainã€RetrievalQAã€VectorstoreIndexCreatorã€ConversationalRetrievalChain**'
- en: '[](https://sophiamyang.medium.com/?source=post_page-----188c6707cc5a--------------------------------)[![Sophia
    Yang, Ph.D.](../Images/c133f918245ea4857dc46df3a07fc2b1.png)](https://sophiamyang.medium.com/?source=post_page-----188c6707cc5a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----188c6707cc5a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----188c6707cc5a--------------------------------)
    [Sophia Yang, Ph.D.](https://sophiamyang.medium.com/?source=post_page-----188c6707cc5a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://sophiamyang.medium.com/?source=post_page-----188c6707cc5a--------------------------------)[![Sophia
    Yang, Ph.D.](../Images/c133f918245ea4857dc46df3a07fc2b1.png)](https://sophiamyang.medium.com/?source=post_page-----188c6707cc5a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----188c6707cc5a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----188c6707cc5a--------------------------------)
    [Sophia Yang, Ph.D.](https://sophiamyang.medium.com/?source=post_page-----188c6707cc5a--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fae9cae9cbcd2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-of-question-answering-in-langchain-188c6707cc5a&user=Sophia+Yang%2C+Ph.D.&userId=ae9cae9cbcd2&source=post_page-ae9cae9cbcd2----188c6707cc5a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----188c6707cc5a--------------------------------)
    Â·6 min readÂ·Apr 8, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F188c6707cc5a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-of-question-answering-in-langchain-188c6707cc5a&user=Sophia+Yang%2C+Ph.D.&userId=ae9cae9cbcd2&source=-----188c6707cc5a---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fae9cae9cbcd2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-of-question-answering-in-langchain-188c6707cc5a&user=Sophia+Yang%2C+Ph.D.&userId=ae9cae9cbcd2&source=post_page-ae9cae9cbcd2----188c6707cc5a---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----188c6707cc5a--------------------------------)
    Â·6åˆ†é’Ÿé˜…è¯»Â·2023å¹´4æœˆ8æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F188c6707cc5a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-of-question-answering-in-langchain-188c6707cc5a&user=Sophia+Yang%2C+Ph.D.&userId=ae9cae9cbcd2&source=-----188c6707cc5a---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F188c6707cc5a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-of-question-answering-in-langchain-188c6707cc5a&source=-----188c6707cc5a---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F188c6707cc5a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-of-question-answering-in-langchain-188c6707cc5a&source=-----188c6707cc5a---------------------bookmark_footer-----------)'
- en: Are you interested in chatting with your own documents, whether it is a text
    file, a PDF, or a website? LangChain makes it easy for you to do question answering
    with your documents. But do you know that there are at least 4 ways to do question
    answering in LangChain? In this blog post, we are going to explore four different
    ways to do question-answering and the various options you could consider for your
    use cases.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ æ˜¯å¦å¯¹ä¸è‡ªå·±çš„æ–‡æ¡£å¯¹è¯æ„Ÿå…´è¶£ï¼Œæ— è®ºæ˜¯æ–‡æœ¬æ–‡ä»¶ã€PDFè¿˜æ˜¯ç½‘ç«™ï¼ŸLangChainä½¿å¾—åœ¨æ–‡æ¡£ä¸­è¿›è¡Œé—®é¢˜å›ç­”å˜å¾—ç®€å•ã€‚ä½†ä½ çŸ¥é“åœ¨LangChainä¸­è‡³å°‘æœ‰4ç§æ–¹å¼å¯ä»¥è¿›è¡Œé—®é¢˜å›ç­”å—ï¼Ÿåœ¨è¿™ç¯‡åšå®¢æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨å››ç§ä¸åŒçš„æé—®æ–¹å¼ä»¥åŠä½ å¯ä»¥è€ƒè™‘çš„å„ç§é€‰é¡¹ã€‚
- en: 'Before we dive into question answering, you may wonder: what is LangChain?
    Great question! In my opinion, LangChain is the easiest way to interact with language
    models and build applications. It is an **open-source** tool that wraps around
    many LLMs and tools. Check out my previous [blog post](/the-easiest-way-to-interact-with-language-models-4da158cfb5c5?sk=271c9c82a16282f93ef3df37f034babe)
    and [video](https://www.youtube.com/watch?v=kmbS6FDQh7c) on an overview of LangChain.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬æ·±å…¥æ¢è®¨é—®ç­”ä¹‹å‰ï¼Œä½ å¯èƒ½ä¼šé—®ï¼šä»€ä¹ˆæ˜¯ LangChainï¼Ÿå¥½é—®é¢˜ï¼åœ¨æˆ‘çœ‹æ¥ï¼ŒLangChain æ˜¯ä¸è¯­è¨€æ¨¡å‹äº’åŠ¨å¹¶æ„å»ºåº”ç”¨ç¨‹åºçš„æœ€ç®€å•æ–¹æ³•ã€‚å®ƒæ˜¯ä¸€ä¸ª**å¼€æº**å·¥å…·ï¼Œå°è£…äº†è®¸å¤š
    LLM å’Œå·¥å…·ã€‚æŸ¥çœ‹æˆ‘ä¹‹å‰çš„ [åšå®¢æ–‡ç« ](/the-easiest-way-to-interact-with-language-models-4da158cfb5c5?sk=271c9c82a16282f93ef3df37f034babe)
    å’Œ [è§†é¢‘](https://www.youtube.com/watch?v=kmbS6FDQh7c) ä»¥äº†è§£ LangChain çš„æ¦‚è¿°ã€‚
- en: Okay, now letâ€™s get started with question-answering on external documents.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œç°åœ¨è®©æˆ‘ä»¬å¼€å§‹å¯¹å¤–éƒ¨æ–‡æ¡£è¿›è¡Œé—®ç­”ã€‚
- en: '**Code**: Check out the code for this blog post [here](https://github.com/sophiamyang/tutorials-LangChain/blob/main/LangChain_QA.ipynb).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç **ï¼šæŸ¥çœ‹æœ¬åšå®¢æ–‡ç« çš„ä»£ç  [è¿™é‡Œ](https://github.com/sophiamyang/tutorials-LangChain/blob/main/LangChain_QA.ipynb)ã€‚'
- en: Set up OpenAI API
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®¾ç½® OpenAI API
- en: 'Create an account at OpenAI and create an API key: [https://platform.openai.com/account](https://platform.openai.com/account).
    Note that OpenAI API is not free. You will need to set up billing information
    there to be able to use OpenAI API. Alternatively, you can use models from HuggingFace
    Hub or other places. Check out my previous [blog post](/the-easiest-way-to-interact-with-language-models-4da158cfb5c5?sk=271c9c82a16282f93ef3df37f034babe)
    and [video](https://www.youtube.com/watch?v=kmbS6FDQh7c) on how to use other models.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ OpenAI ä¸Šåˆ›å»ºä¸€ä¸ªå¸æˆ·å¹¶ç”Ÿæˆ API å¯†é’¥ï¼š[https://platform.openai.com/account](https://platform.openai.com/account)ã€‚è¯·æ³¨æ„ï¼ŒOpenAI
    API å¹¶éå…è´¹ã€‚ä½ éœ€è¦åœ¨æ­¤å¤„è®¾ç½®è´¦å•ä¿¡æ¯æ‰èƒ½ä½¿ç”¨ OpenAI APIã€‚æˆ–è€…ï¼Œä½ å¯ä»¥ä½¿ç”¨ HuggingFace Hub æˆ–å…¶ä»–åœ°æ–¹çš„æ¨¡å‹ã€‚æŸ¥çœ‹æˆ‘ä¹‹å‰çš„
    [åšå®¢æ–‡ç« ](/the-easiest-way-to-interact-with-language-models-4da158cfb5c5?sk=271c9c82a16282f93ef3df37f034babe)
    å’Œ [è§†é¢‘](https://www.youtube.com/watch?v=kmbS6FDQh7c) æ¥äº†è§£å¦‚ä½•ä½¿ç”¨å…¶ä»–æ¨¡å‹ã€‚
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Load documents
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŠ è½½æ–‡æ¡£
- en: LangChain supports many many [Document Loaders](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html)
    such as Notion, YouTube, and Figma. In this example, Iâ€™d like to chat with my
    PDF file. Thus, I used the PyPDFLoader to load my file. Iâ€™m actually using Chapter
    1 of the [AI index report](https://aiindex.stanford.edu/report/), which includes
    55 pages, and I saved it in the materials directory of my Github [repo](https://github.com/sophiamyang/tutorials-LangChain).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain æ”¯æŒè®¸å¤š [æ–‡æ¡£åŠ è½½å™¨](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html)ï¼Œä¾‹å¦‚
    Notionã€YouTube å’Œ Figmaã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘æƒ³ä¸æˆ‘çš„ PDF æ–‡ä»¶è¿›è¡Œå¯¹è¯ã€‚å› æ­¤ï¼Œæˆ‘ä½¿ç”¨äº† PyPDFLoader æ¥åŠ è½½æˆ‘çš„æ–‡ä»¶ã€‚æˆ‘å®é™…ä½¿ç”¨çš„æ˜¯
    [AI index report](https://aiindex.stanford.edu/report/) çš„ç¬¬ 1 ç« ï¼Œå…¶ä¸­åŒ…æ‹¬ 55 é¡µï¼Œå¹¶å°†å…¶ä¿å­˜åœ¨æˆ‘
    GitHub [åº“](https://github.com/sophiamyang/tutorials-LangChain) çš„ææ–™ç›®å½•ä¸­ã€‚
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Method 1: load_qa_chain**'
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**æ–¹æ³• 1ï¼šload_qa_chain**'
- en: '`load_qa_chain` provides the most generic interface for answering questions.
    It loads a chain that you can do QA for your input documents and uses ALL of the
    text in the documents.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`load_qa_chain` æä¾›äº†æœ€é€šç”¨çš„é—®ç­”æ¥å£ã€‚å®ƒåŠ è½½ä¸€ä¸ªé“¾ï¼Œä½ å¯ä»¥å¯¹è¾“å…¥æ–‡æ¡£è¿›è¡Œé—®ç­”ï¼Œå¹¶ä½¿ç”¨æ–‡æ¡£ä¸­çš„æ‰€æœ‰æ–‡æœ¬ã€‚'
- en: '![](../Images/b1c7f091af31506e27712127834f80ab.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b1c7f091af31506e27712127834f80ab.png)'
- en: 'It also lets you do QA over a set of documents:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒè¿˜å…è®¸ä½ å¯¹ä¸€ç»„æ–‡æ¡£è¿›è¡Œé—®ç­”ï¼š
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ğŸ¤”***But what if my document is super long that it exceeds the token limit?***
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤”***ä½†å¦‚æœæˆ‘çš„æ–‡æ¡£éå¸¸é•¿ï¼Œè¶…å‡ºäº†ä»¤ç‰Œé™åˆ¶æ€ä¹ˆåŠï¼Ÿ***
- en: 'There are two ways to fix it:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸¤ç§è§£å†³æ–¹æ³•ï¼š
- en: '**Solution 1: Chain Type**'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**è§£å†³æ–¹æ¡ˆ 1ï¼šé“¾å¼ç±»å‹**'
- en: The default `chain_type="stuff"` uses ALL of the text from the documents in
    the prompt. It actually doesnâ€™t work with our example because it exceeds the token
    limit and causes rate-limiting errors. Thatâ€™s why in this example, we had to use
    other chain types for example `"map_reduce"`. What are the other chain types?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤çš„ `chain_type="stuff"` ä½¿ç”¨äº†æ–‡æ¡£ä¸­æ‰€æœ‰çš„æ–‡æœ¬ã€‚å®é™…ä¸Šï¼Œå®ƒä¸é€‚ç”¨äºæˆ‘ä»¬çš„ä¾‹å­ï¼Œå› ä¸ºå®ƒè¶…å‡ºäº†ä»¤ç‰Œé™åˆ¶å¹¶å¯¼è‡´é€Ÿç‡é™åˆ¶é”™è¯¯ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä¸å¾—ä¸ä½¿ç”¨å…¶ä»–é“¾å¼ç±»å‹ï¼Œæ¯”å¦‚
    `"map_reduce"`ã€‚è¿˜æœ‰å“ªäº›å…¶ä»–é“¾å¼ç±»å‹ï¼Ÿ
- en: '`map_reduce`: It separates texts into batches (as an example, you can define
    batch size in `llm=OpenAI(batch_size=5)`), feeds each batch with the question
    to LLM separately, and comes up with the final answer based on the answers from
    each batch.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map_reduce`ï¼šå®ƒå°†æ–‡æœ¬åˆ†æˆæ‰¹æ¬¡ï¼ˆä¾‹å¦‚ï¼Œä½ å¯ä»¥åœ¨ `llm=OpenAI(batch_size=5)` ä¸­å®šä¹‰æ‰¹æ¬¡å¤§å°ï¼‰ï¼Œåˆ†åˆ«å°†æ¯ä¸ªæ‰¹æ¬¡çš„é—®é¢˜ä¼ é€’ç»™
    LLMï¼Œå¹¶æ ¹æ®æ¯ä¸ªæ‰¹æ¬¡çš„ç­”æ¡ˆå¾—å‡ºæœ€ç»ˆç­”æ¡ˆã€‚'
- en: '`refine` : It separates texts into batches, feeds the first batch to LLM, and
    feeds the answer and the second batch to LLM. It refines the answer by going through
    all the batches.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`refine`ï¼šå®ƒå°†æ–‡æœ¬åˆ†æˆæ‰¹æ¬¡ï¼Œå°†ç¬¬ä¸€ä¸ªæ‰¹æ¬¡ä¼ é€’ç»™ LLMï¼Œç„¶åå°†ç­”æ¡ˆå’Œç¬¬äºŒä¸ªæ‰¹æ¬¡ä¼ é€’ç»™ LLMã€‚å®ƒé€šè¿‡å¤„ç†æ‰€æœ‰æ‰¹æ¬¡æ¥å®Œå–„ç­”æ¡ˆã€‚'
- en: '`map-rerank`: It separates texts into batches, feeds each batch to LLM, returns
    a score of how fully it answers the question, and comes up with the final answer
    based on the high-scored answers from each batch.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map-rerank`ï¼šå®ƒå°†æ–‡æœ¬åˆ†æˆæ‰¹æ¬¡ï¼Œé€ä¸ªæ‰¹æ¬¡åœ°è¾“å…¥åˆ° LLM ä¸­ï¼Œè¿”å›æ¯ä¸ªæ‰¹æ¬¡å¯¹é—®é¢˜çš„å›ç­”å®Œæ•´ç¨‹åº¦çš„è¯„åˆ†ï¼Œå¹¶æ ¹æ®æ¯ä¸ªæ‰¹æ¬¡ä¸­é«˜è¯„åˆ†çš„å›ç­”å¾—å‡ºæœ€ç»ˆç­”æ¡ˆã€‚'
- en: 'Solution 2: RetrievalQA'
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è§£å†³æ–¹æ¡ˆ 2ï¼šRetrievalQA
- en: One issue with using ALL of the text is that it can be very costly because you
    are feeding all the texts to OpenAI API and the API is charged by the number of
    tokens. A better solution is to retrieve relevant text chunks first and only use
    the relevant text chunks in the language model. Iâ€™m going to go through the details
    of RetrievalQA next.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ‰€æœ‰æ–‡æœ¬çš„ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œæˆæœ¬å¯èƒ½éå¸¸é«˜ï¼Œå› ä¸ºä½ å°†æ‰€æœ‰æ–‡æœ¬éƒ½ä¼ é€ç»™ OpenAI APIï¼Œè€Œ API æ˜¯æŒ‰ä»¤ç‰Œæ•°é‡æ”¶è´¹çš„ã€‚æ›´å¥½çš„è§£å†³æ–¹æ¡ˆæ˜¯å…ˆæ£€ç´¢ç›¸å…³æ–‡æœ¬å—ï¼Œç„¶åä»…åœ¨è¯­è¨€æ¨¡å‹ä¸­ä½¿ç”¨ç›¸å…³æ–‡æœ¬å—ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘å°†è¯¦ç»†ä»‹ç»
    RetrievalQAã€‚
- en: 'Method 2: RetrievalQA'
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ–¹æ³• 2ï¼šRetrievalQA
- en: '`RetrievalQA` chain actually uses `load_qa_chain` under the hood. We retrieve
    the most relevant chunk of text and feed those to the language model.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`RetrievalQA` é“¾å®é™…ä¸Šåœ¨åº•å±‚ä½¿ç”¨ `load_qa_chain`ã€‚æˆ‘ä»¬æ£€ç´¢æœ€ç›¸å…³çš„æ–‡æœ¬å—ï¼Œå¹¶å°†è¿™äº›æ–‡æœ¬å—è¾“å…¥åˆ°è¯­è¨€æ¨¡å‹ä¸­ã€‚'
- en: 'Here is how it works:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶å·¥ä½œåŸç†å¦‚ä¸‹ï¼š
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the result, we can see the answer and two source documents because we defined
    k as 2 meaning that we are only interested in getting two relevant text chunks.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç»“æœä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç­”æ¡ˆå’Œä¸¤ä¸ªæºæ–‡æ¡£ï¼Œå› ä¸ºæˆ‘ä»¬å°† k å®šä¹‰ä¸º 2ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬åªå…³å¿ƒè·å–ä¸¤ä¸ªç›¸å…³çš„æ–‡æœ¬å—ã€‚
- en: '![](../Images/17962d4e7c9bcaa236b0fe1365859aff.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17962d4e7c9bcaa236b0fe1365859aff.png)'
- en: 'Options:'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é€‰é¡¹ï¼š
- en: 'There are various options for you to choose from in this process:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ä½ å¯ä»¥é€‰æ‹©å„ç§é€‰é¡¹ï¼š
- en: '[embeddings](https://python.langchain.com/en/latest/reference/modules/embeddings.html):
    In the example, we used OpenAI Embeddings. But there are many other embedding
    options such as Cohere Embeddings, and HuggingFaceEmbeddings from specific models.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[åµŒå…¥](https://python.langchain.com/en/latest/reference/modules/embeddings.html)ï¼šåœ¨ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†
    OpenAI åµŒå…¥ã€‚ä½†è¿˜æœ‰è®¸å¤šå…¶ä»–åµŒå…¥é€‰é¡¹ï¼Œå¦‚ Cohere Embeddings å’Œæ¥è‡ªç‰¹å®šæ¨¡å‹çš„ HuggingFaceEmbeddingsã€‚'
- en: '[TextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html):
    We used Character Text Splitter in the example where the text is split by a single
    character. You can also different text splitters and different tokens mentioned
    in this [doc](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html).'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ–‡æœ¬åˆ†å‰²å™¨](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html)ï¼šåœ¨ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æŒ‰å•ä¸ªå­—ç¬¦åˆ†å‰²çš„
    Character Text Splitterã€‚ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ä¸åŒçš„æ–‡æœ¬åˆ†å‰²å™¨å’Œä¸åŒçš„ä»¤ç‰Œï¼Œå…·ä½“è¯·å‚è§æ­¤ [æ–‡æ¡£](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html)ã€‚'
- en: '[VectorStore](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html):
    We used Chroma as our vector database where we store our embedded text vectors.
    Other popular options are FAISS, Mulvus, and Pinecone.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å‘é‡å­˜å‚¨](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html)ï¼šæˆ‘ä»¬ä½¿ç”¨äº†
    Chroma ä½œä¸ºæˆ‘ä»¬çš„å‘é‡æ•°æ®åº“ï¼Œç”¨äºå­˜å‚¨åµŒå…¥çš„æ–‡æœ¬å‘é‡ã€‚å…¶ä»–æµè¡Œçš„é€‰é¡¹åŒ…æ‹¬ FAISSã€Milvus å’Œ Pineconeã€‚'
- en: '[Retrievers](https://python.langchain.com/en/latest/modules/indexes/retrievers.html):
    We used a VectoreStoreRetriver, which is backed by a VectorStore. To retrieve
    text, there are two search types you can choose: [search_type](https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/chroma.html#mmr):
    â€œsimilarityâ€ or â€œmmrâ€. `search_type="similarity"` uses similarity search in the
    retriever object where it selects text chunk vectors that are most similar to
    the question vector. `search_type="mmr"` uses the maximum marginal relevance search
    where it optimizes for similarity to query AND diversity among selected documents.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ£€ç´¢å™¨](https://python.langchain.com/en/latest/modules/indexes/retrievers.html)ï¼šæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ª
    VectoreStoreRetriverï¼Œå®ƒç”± VectorStore æ”¯æŒã€‚ä¸ºäº†æ£€ç´¢æ–‡æœ¬ï¼Œä½ å¯ä»¥é€‰æ‹©ä¸¤ç§æœç´¢ç±»å‹ï¼š[search_type](https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/chroma.html#mmr)ï¼šâ€œsimilarityâ€
    æˆ– â€œmmrâ€ã€‚`search_type="similarity"` ä½¿ç”¨æ£€ç´¢å™¨å¯¹è±¡ä¸­çš„ç›¸ä¼¼æ€§æœç´¢ï¼Œå…¶ä¸­é€‰æ‹©æœ€ä¸é—®é¢˜å‘é‡ç›¸ä¼¼çš„æ–‡æœ¬å—å‘é‡ã€‚`search_type="mmr"`
    ä½¿ç”¨æœ€å¤§è¾¹é™…ç›¸å…³æ€§æœç´¢ï¼Œå®ƒåœ¨ä¼˜åŒ–æŸ¥è¯¢ç›¸ä¼¼æ€§å’Œé€‰æ‹©æ–‡æ¡£çš„å¤šæ ·æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚'
- en: '[Chain Type](https://python.langchain.com/en/latest/modules/chains/index_examples/question_answering.html):
    same as method 1\. You can also define the chain type as one of the four options:
    â€œstuffâ€, â€œmap reduceâ€, â€œrefineâ€, â€œmap_rerankâ€.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[é“¾ç±»å‹](https://python.langchain.com/en/latest/modules/chains/index_examples/question_answering.html)ï¼šä¸æ–¹æ³•
    1 ç›¸åŒã€‚ä½ ä¹Ÿå¯ä»¥å°†é“¾ç±»å‹å®šä¹‰ä¸ºå››ç§é€‰é¡¹ä¹‹ä¸€ï¼šâ€œstuffâ€ã€â€œmap reduceâ€ã€â€œrefineâ€ã€â€œmap_rerankâ€ã€‚'
- en: '**Method 3: VectorstoreIndexCreator**'
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**æ–¹æ³• 3ï¼šVectorstoreIndexCreator**'
- en: 'VectorstoreIndexCreator is a wrapper around the above functionality. It is
    exactly the same under the hood, but just exposes a higher-level interface to
    let you get started in three lines of code:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: VectorstoreIndexCreator æ˜¯ä¸Šè¿°åŠŸèƒ½çš„å°è£…å™¨ã€‚å®ƒåœ¨åº•å±‚åŠŸèƒ½ä¸Šå®Œå…¨ç›¸åŒï¼Œä½†æä¾›äº†æ›´é«˜çº§çš„æ¥å£ï¼Œè®©ä½ å¯ä»¥ç”¨ä¸‰è¡Œä»£ç å¼€å§‹ä½¿ç”¨ï¼š
- en: '![](../Images/001cc8d911b3430c307620adb8637998.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/001cc8d911b3430c307620adb8637998.png)'
- en: 'Of course, you can also specify different options in this wrapper:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œä½ ä¹Ÿå¯ä»¥åœ¨è¿™ä¸ªåŒ…è£…å™¨ä¸­æŒ‡å®šä¸åŒçš„é€‰é¡¹ã€‚
- en: '![](../Images/064f4d89bdedd54ae1a94b11a6095052.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/064f4d89bdedd54ae1a94b11a6095052.png)'
- en: '**Method 4: ConversationalRetrievalChain**'
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**æ–¹æ³• 4ï¼šConversationalRetrievalChain**'
- en: ConversationalRetrievalChain is very similar to method 2 RetrievalQA. It added
    an additional parameter `chat_history` to pass in chat history which can be used
    for follow-up questions.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ConversationalRetrievalChain ä¸æ–¹æ³• 2 RetrievalQA éå¸¸ç›¸ä¼¼ã€‚å®ƒå¢åŠ äº†ä¸€ä¸ªé¢å¤–çš„å‚æ•° `chat_history`
    ç”¨äºä¼ é€’èŠå¤©è®°å½•ï¼Œè¿™å¯ä»¥ç”¨äºåç»­çš„é—®é¢˜ã€‚
- en: '*ConversationalRetrievalChain = conversation memory + RetrievalQAChain*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*ConversationalRetrievalChain = å¯¹è¯è®°å¿† + RetrievalQAChain*'
- en: If you would like your language model to have a memory of the previous conversation,
    use this method. In my example below, I asked about the number of AI publications
    and got the result of 500,000\. Then I asked the LLM to divide this number by
    2\. Since it has all the chat history, the model knows the number I was referring
    to is 500,000 and the result returned is 250,000.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å¸Œæœ›ä½ çš„è¯­è¨€æ¨¡å‹è®°ä½ä¹‹å‰çš„å¯¹è¯ï¼Œå¯ä»¥ä½¿ç”¨è¿™ç§æ–¹æ³•ã€‚åœ¨æˆ‘ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘è¯¢é—®äº† AI å‘è¡¨çš„æ•°é‡ï¼Œå¹¶å¾—åˆ°äº† 500,000 çš„ç»“æœã€‚ç„¶åæˆ‘è®© LLM
    å°†è¿™ä¸ªæ•°å­—é™¤ä»¥ 2ã€‚ç”±äºå®ƒæ‹¥æœ‰æ‰€æœ‰èŠå¤©è®°å½•ï¼Œæ¨¡å‹çŸ¥é“æˆ‘æŒ‡çš„æ˜¯ 500,000ï¼Œå› æ­¤è¿”å›çš„ç»“æœæ˜¯ 250,000ã€‚
- en: '![](../Images/13079f5b3ba06bcc78db50535722b4e5.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13079f5b3ba06bcc78db50535722b4e5.png)'
- en: Conclusion
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: Now you know four ways to do question answering with LLMs in LangChain. In summary,
    load_qa_chain uses all texts and accepts multiple documents; RetrievalQA uses
    load_qa_chain under the hood but retrieves relevant text chunks first; VectorstoreIndexCreator
    is the same as RetrievalQA with a higher-level interface; ConversationalRetrievalChain
    is useful when you want to pass in your chat history to the model.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ çŸ¥é“äº†å››ç§ä½¿ç”¨ LangChain ä¸­ LLM è¿›è¡Œé—®ç­”çš„æ–¹æ³•ã€‚æ€»ç»“èµ·æ¥ï¼Œload_qa_chain ä½¿ç”¨æ‰€æœ‰æ–‡æœ¬å¹¶æ¥å—å¤šä¸ªæ–‡æ¡£ï¼›RetrievalQA
    åœ¨åº•å±‚ä½¿ç”¨ load_qa_chainï¼Œä½†é¦–å…ˆæ£€ç´¢ç›¸å…³çš„æ–‡æœ¬å—ï¼›VectorstoreIndexCreator ä¸ RetrievalQA ç›¸åŒï¼Œä½†æä¾›äº†æ›´é«˜å±‚æ¬¡çš„æ¥å£ï¼›ConversationalRetrievalChain
    åœ¨ä½ æƒ³å°†èŠå¤©è®°å½•ä¼ é€’ç»™æ¨¡å‹æ—¶éå¸¸æœ‰ç”¨ã€‚
- en: '**Acknowledgment**:'
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**è‡´è°¢**ï¼š'
- en: Thank you Harrison Chase for the guidance!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢ Harrison Chase çš„æŒ‡å¯¼ï¼
- en: '![](../Images/40a328419509baad0b94db6142a3c393.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40a328419509baad0b94db6142a3c393.png)'
- en: Photo by [FLY:D](https://unsplash.com/@flyd2069?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/ZNOxwCEj5mw?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ç…§ç‰‡ç”± [FLY:D](https://unsplash.com/@flyd2069?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    æä¾›ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/photos/ZNOxwCEj5mw?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)ã€‚
- en: . . .
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: . . .
- en: By [Sophia Yang](https://www.linkedin.com/in/sophiamyang/) on April 8, 2023
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ç”± [Sophia Yang](https://www.linkedin.com/in/sophiamyang/) æä¾›ï¼Œæ—¥æœŸä¸º 2023 å¹´ 4 æœˆ
    8 æ—¥
- en: Sophia Yang is a Senior Data Scientist. Connect with me on [LinkedIn](https://www.linkedin.com/in/sophiamyang/),
    [Twitter](https://twitter.com/sophiamyang), and [YouTube](https://www.youtube.com/SophiaYangDS)
    and join the DS/ML [Book Club](https://dsbookclub.github.io/) â¤ï¸
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Sophia Yang æ˜¯é«˜çº§æ•°æ®ç§‘å­¦å®¶ã€‚å¯ä»¥åœ¨ [LinkedIn](https://www.linkedin.com/in/sophiamyang/)ã€[Twitter](https://twitter.com/sophiamyang)
    å’Œ [YouTube](https://www.youtube.com/SophiaYangDS) ä¸Šä¸æˆ‘è”ç³»ï¼Œå¹¶åŠ å…¥ DS/ML [ä¹¦å‹ä¼š](https://dsbookclub.github.io/)
    â¤ï¸
