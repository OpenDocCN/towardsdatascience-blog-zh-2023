- en: A Foundation Model for Medical AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-foundation-model-for-medical-ai-7b97e3ab3893?source=collection_archive---------2-----------------------#2023-09-19](https://towardsdatascience.com/a-foundation-model-for-medical-ai-7b97e3ab3893?source=collection_archive---------2-----------------------#2023-09-19)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Introducing PLIP, a foundation model for pathology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://fede-bianchi.medium.com/?source=post_page-----7b97e3ab3893--------------------------------)[![Federico
    Bianchi](../Images/fa38ff2051af04df7803af7d84c5cd4d.png)](https://fede-bianchi.medium.com/?source=post_page-----7b97e3ab3893--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7b97e3ab3893--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7b97e3ab3893--------------------------------)
    [Federico Bianchi](https://fede-bianchi.medium.com/?source=post_page-----7b97e3ab3893--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2aff872fe60e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-foundation-model-for-medical-ai-7b97e3ab3893&user=Federico+Bianchi&userId=2aff872fe60e&source=post_page-2aff872fe60e----7b97e3ab3893---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7b97e3ab3893--------------------------------)
    ·10 min read·Sep 19, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7b97e3ab3893&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-foundation-model-for-medical-ai-7b97e3ab3893&user=Federico+Bianchi&userId=2aff872fe60e&source=-----7b97e3ab3893---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7b97e3ab3893&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-foundation-model-for-medical-ai-7b97e3ab3893&source=-----7b97e3ab3893---------------------bookmark_footer-----------)![](../Images/c8ec0d1dd469532a43f24deab0ce8690.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Photo by Tara Winstead: [https://www.pexels.com/photo/person-reaching-out-to-a-robot-8386434/](https://www.pexels.com/photo/person-reaching-out-to-a-robot-8386434/)'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ongoing AI revolution is bringing us innovations in all directions. OpenAI
    GPT(s) models are leading the development and showing how much foundation models
    can actually make some of our daily tasks easier. From helping us write better
    to streamlining some of our tasks, every day we see new models being announced.
  prefs: []
  type: TYPE_NORMAL
- en: Many opportunities are opening up in front of us. AI products that can help
    us in our work life are going to be one of the most important tools we are going
    to get in the next years.
  prefs: []
  type: TYPE_NORMAL
- en: Where are we going to see the most impactful changes? Where can we help people
    accomplish their tasks faster? One of the most exciting avenues for AI models
    is the one that brings us to Medical AI tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this blog post, I describe [**PLIP**](https://github.com/PathologyFoundation/plip)
    (Pathology Language and Image Pre-Training) as one of the first foundation models
    for pathology. PLIP is a vision-language model that can be used to embed images
    and text in the same vector space, thus allowing multi-modal applications. PLIP
    is derived from the original [CLIP](https://openai.com/research/clip) model proposed
    by OpenAI in 2021 and has been recently published in Nature Medicine:'
  prefs: []
  type: TYPE_NORMAL
- en: Huang, Z., Bianchi, F., Yuksekgonul, M., Montine, T., Zou, J., [**A visual–language
    foundation model for pathology image analysis using medical Twitter.**](https://rdcu.be/djLSK)2023,
    Nature Medicine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some useful links before starting our adventure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[HuggingFace Weights](https://huggingface.co/vinid/plip);'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Nature Medicine Paper](https://rdcu.be/djLSK);'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[GitHub Repository](https://github.com/PathologyFoundation/pathology-language-image-pretraining/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All images, unless otherwise noted, are by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Contrastive Pre-Training 101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We show that, through the use of data collection on social media and with some
    additional tricks, we can build a model that can be used in Medical AI pathology
    tasks with good results — and without the need for annotated data.
  prefs: []
  type: TYPE_NORMAL
- en: While introducing CLIP (the model from which PLIP is derived) and its contrastive
    loss is a bit out of the scope of this blog post, it is still good to get a first
    intro/refresher. The very simple idea behind CLIP is that we can build a model
    that puts images and text in a vector space in which “images and their descriptions
    are going to be close together”.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3291eac40c88216823339f8f74b79690.png)'
  prefs: []
  type: TYPE_IMG
- en: A contrastive model — like PLIP/CLIP — puts images and text in the same vector
    space to be compared. The description in the yellow box matches the image in the
    yellow box and thus they are also close in the vector space.
  prefs: []
  type: TYPE_NORMAL
- en: 'The GIF above also shows an example of how a model that embeds images and text
    in the same vector space can be used for classification: by putting everything
    in the same vector space we can associate each image with one or more labels by
    considering the distance in the vector space: the closer the description is to
    the image, the better. We expect the closest label to be the real label of the
    image.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To be clear: Once CLIP is trained you can embed **any** image or **any** text
    you have. Consider that this GIF shows a 2D space, but in general, the spaces
    used in CLIP are of much higher dimensionality.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that once images and text are in the same vector spaces, there are
    many things we can do: from *zero-shot classification* (**find which text label
    is more similar to an image**) to *retrieval* (**find which image is more similar
    to a given description**).'
  prefs: []
  type: TYPE_NORMAL
- en: How do we train CLIP? To put it simply, the model is fed with **MANY** image-text
    pairs and tries to put similar matching items close together (as in the image
    above) and all the rest far away. The more image-text pairs you have, the better
    the representation you are going to learn.
  prefs: []
  type: TYPE_NORMAL
- en: We will stop here with the CLIP background, this should be enough to understand
    the rest of this post. I have a more in-depth blog post about CLIP on Towards
    Data Science.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/how-to-train-your-clip-45a451dcd303?source=post_page-----7b97e3ab3893--------------------------------)
    [## How to Train your CLIP'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to CLIP and to how we fine-tuned it for the Italian Language during
    the HuggingFace Community Week.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-to-train-your-clip-45a451dcd303?source=post_page-----7b97e3ab3893--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: CLIP has been trained to be a very general image-text model, but it does not
    work as well for specific use cases (e.g., [Fashion](https://www.nature.com/articles/s41598-022-23052-9)
    (Chia et al., 2022)) and there are also cases in which CLIP underperforms and
    domain-specific implementations perform better (Zhang et al., 2023).
  prefs: []
  type: TYPE_NORMAL
- en: Pathology Language and Image Pre-Training (PLIP)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now describe how we built PLIP, our fine-tuned version of the original CLIP
    model that is specifically designed for Pathology.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Dataset for Pathology Language and Image Pre-Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need data, and this data has to be good enough to be used to train a model.
    **The question is how do we find these data?** What we need is images with relevant
    descriptions — like the one we saw in the GIF above.
  prefs: []
  type: TYPE_NORMAL
- en: Although there is a significant amount of pathology data available on the web,
    it is often lacking annotations and it may be in non-standard formats such as
    PDF files, slides, or YouTube videos.
  prefs: []
  type: TYPE_NORMAL
- en: We need to look somewhere else, and this somewhere else is going to be social
    media. By leveraging social media platforms, we can potentially access a wealth
    of pathology-related content. Pathologists use social media to share their own
    research online and to ask questions to their fellow colleagues (see Isom et al.,
    2017, for a discussion on how pathologists use social media). There is also a
    [set](https://www.symplur.com/healthcare-hashtags/ontology/pathology/) of generally
    recommended **Twitter hashtags** that pathologists can use to communicate.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to Twitter data, we also collect a subset of images from the **LAION
    dataset** (Schuhmann et al., 2022), a vast collection of 5B image-text pairs.
    LAION has been collected by scraping the web and it is the dataset that was used
    to train many of the popular OpenCLIP models.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pathology Twitter**'
  prefs: []
  type: TYPE_NORMAL
- en: We collect more than 100K tweets using pathology Twitter hashtags. The process
    is rather simple, we use the API to collect tweets that relate to a set of specific
    hashtags. We remove tweets that contain a question mark because these tweets often
    contain requests for other pathologies (e.g., “Which kind of tumor is this?”)
    and not information we might actually need to build our model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6fa4d4137808c45ca72028b3096e3bba.png)'
  prefs: []
  type: TYPE_IMG
- en: We extract tweets with specific keywords and we remove sensitive content. In
    addition to this, we also remove all the tweets that contain question marks, which
    appear in tweets used by pathologists to ask questions to their colleagues about
    some possible rare cases.
  prefs: []
  type: TYPE_NORMAL
- en: '**Sampling from LAION**'
  prefs: []
  type: TYPE_NORMAL
- en: 'LAION contains 5B image-text pairs, and our plan to collect our data is going
    to be as follows: we can use our own images that come from Twitter and find similar
    images in this large corpus; in this way, we should be able to get reasonably
    similar images and hopefully, these similar images are also pathology images.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, doing this manually would be infeasible, embedding and searching over 5B
    embeddings is a very time-consuming task. Luckily there are pre-computed vector
    indexes for LAION that we can query with actual images using APIs! We thus simply
    embed our images and use K-NN search to find similar images in LAION. Remember,
    each of these images comes with a caption, something that is perfect for our use
    case.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e1ad8d3c652c24965be1ac321ad7e7bb.png)'
  prefs: []
  type: TYPE_IMG
- en: Very simple setup of how we extend our dataset by using K-NN search on the LAION
    dataset. We start with our own image from our original corpus and then search
    for similar images on the LAION dataset. Each of the images we get comes with
    an actual caption.
  prefs: []
  type: TYPE_NORMAL
- en: '**Ensuring Data Quality**'
  prefs: []
  type: TYPE_NORMAL
- en: Not all the images we collect are good. For example, from Twitter, we collected
    lots of group photos from Medical conferences. From LAION, we sometimes got some
    fractal-like images that could vaguely resemble some pathology pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'What we did was very simple: we trained a classifier by using some pathology
    data as positive class data and ImageNet data as negative class data. This kind
    of classifier has an incredibly high precision (it’s actually easy to distinguish
    pathology images from random images on the web).'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to this, for LAION data we apply an English language classifier
    to remove examples that are not in English.
  prefs: []
  type: TYPE_NORMAL
- en: Training Pathology Language and Image Pre-Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data collection was the hardest part. Once that is done and we trust our data,
    we can start training.
  prefs: []
  type: TYPE_NORMAL
- en: To train PLIP we used the original OpenAI code to do training — we implemented
    the training loop, added a cosine annealing for the loss, and a couple of tweaks
    here and there to make everything ran smoothly and in a verifiable way (e.g. [Comet
    ML](https://www.comet.com/) tracking).
  prefs: []
  type: TYPE_NORMAL
- en: We trained many different models (hundreds) and compared parameters and optimization
    techniques, Eventually, we were able to come up with a model we were pleased with.
    There are more details in the paper, but one of the most important components
    when building this kind of contrastive model is making sure that the batch size
    is as large as possible during training, this allows the model to learn to distinguish
    as many elements as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Pathology Language and Image Pre-Training for Medical AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is now time to put our PLIP to the test. Is this foundation model good on
    standard benchmarks?
  prefs: []
  type: TYPE_NORMAL
- en: We run different tests to evaluate the performance of our PLIP model. The three
    most interesting ones are zero-shot classification, linear probing, and retrieval,
    but I’ll mainly focus on the first two here. I’ll ignore experimental configuration
    for the sake of brevity, but these are all available in the manuscript.
  prefs: []
  type: TYPE_NORMAL
- en: '**PLIP as a Zero-Shot Classifier**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The GIF below illustrates how to do zero-shot classification with a model like
    PLIP. We use the dot product as a measure of similarity in the vector space (the
    higher, the more similar).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fc2a79c5e939d3e789458d61ed11663f.png)'
  prefs: []
  type: TYPE_IMG
- en: The process to do zero-shot classification. We embed an image and all the labels
    and find which labels are closer to the image in the vector space.
  prefs: []
  type: TYPE_NORMAL
- en: In the following plot, you can see a quick comparison of PLIP vs CLIP on one
    of the dataset we used for zero-shot classification. There is a significant gain
    in terms of performance when using PLIP to replace CLIP.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c595c8c56829e0fbef4c21836279d2b4.png)'
  prefs: []
  type: TYPE_IMG
- en: PLIP vs CLIP performance (Weighted Macro F1) on two datasets for zero-shot classification.
    Note that y-axis stops at around 0.6 and not 1.
  prefs: []
  type: TYPE_NORMAL
- en: '**PLIP as a Feature Extractor for Linear Probing**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another way to use PLIP is as a feature extractor for pathology images. During
    training, PLIP sees many pathology images and learns to build vector embeddings
    for them.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say you have some annotated data and you want to train a new pathology
    classifier. You can extract image embeddings with PLIP and then train a logistic
    regression (or any kind of regressor you like) on top of these embeddings. This
    is an easy and effective way to perform a classification task.
  prefs: []
  type: TYPE_NORMAL
- en: Why does this work? The idea is that to train a classifier PLIP embeddings,
    being pathology-specific, should be better than CLIP embeddings, which are general
    purpose.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fde1892750497b46a5c647b4034eed2e.png)'
  prefs: []
  type: TYPE_IMG
- en: PLIP Image Encoder allows us to extract a vector for each image and train an
    image classifier on top of it.
  prefs: []
  type: TYPE_NORMAL
- en: Here is an example of the comparison between the performance of CLIP and PLIP
    on two datasets. While CLIP gets good performance, the results we get using PLIP
    are much higher.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8b2401d70041e726c5e708a3596f37fa.png)'
  prefs: []
  type: TYPE_IMG
- en: PLIP vs CLIP performance (Macro F1) on two datasets for linear probing. Note
    that y-axis starts from 0.65 and not 0.
  prefs: []
  type: TYPE_NORMAL
- en: Using Pathology Language and Image Pre-Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How to use PLIP? here are some examples of how to use PLIP in Python and a Streamlit
    demo you can use to play a bit with the mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code: APIs to Use PLIP'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our GitHub repository offers a couple of additional examples you can follow.
    We have built an API that allows you to interact with the model easily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also use the more standard HF API to load and use the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Demo: PLIP as an Educational Tool'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We also believe PLIP and future models can be effectively used as educational
    tools for Medical AI. PLIP allows users to do zero-shot retrieval: a user can
    search for specific keywords and PLIP will try to find the most similar/matching
    image. We built a simple web app in Streamlit that you can find [here](https://huggingface.co/spaces/vinid/webplip).'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thanks for reading all of this! We are excited about the possible future evolutions
    of this technology.
  prefs: []
  type: TYPE_NORMAL
- en: I will close this blog post by discussing some very important limitations of
    PLIP and by suggesting some additional things I have written that might be of
    interest.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While our results are interesting, PLIP comes with lots of different limitations.
    Data is not enough to learn all the complex aspects of pathology. We have built
    data filters to ensure data quality, but we need better evaluation metrics to
    understand what the model is getting right and what the model is getting wrong.
  prefs: []
  type: TYPE_NORMAL
- en: More importantly, PLIP does not solve the current challenges of pathology; PLIP
    is not a perfect tool and can make many errors that require investigation. The
    results we see are definitely promising and they open up a range of possibilities
    for future models in pathology that combine vision and language. However, there
    is still lots of work to do before we can see these tools used in everyday medicine.
  prefs: []
  type: TYPE_NORMAL
- en: Miscellanea
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I have a couple of other blog posts regarding CLIP modeling and CLIP limitations.
    For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/teaching-clip-some-fashion-3005ac3fdcc3?source=post_page-----7b97e3ab3893--------------------------------)
    [## Teaching CLIP Some Fashion'
  prefs: []
  type: TYPE_NORMAL
- en: Training FashionCLIP, a domain-specific CLIP model for Fashion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/teaching-clip-some-fashion-3005ac3fdcc3?source=post_page-----7b97e3ab3893--------------------------------)
    [](/your-vision-language-model-might-be-a-bag-of-words-30b1beaef7f8?source=post_page-----7b97e3ab3893--------------------------------)
    [## Your Vision-Language Model Might Be a Bag of Words
  prefs: []
  type: TYPE_NORMAL
- en: We explore the limits of what vision-language models get about language in our
    Oral Paper at ICLR 2023
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/your-vision-language-model-might-be-a-bag-of-words-30b1beaef7f8?source=post_page-----7b97e3ab3893--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Chia, P.J., Attanasio, G., Bianchi, F., Terragni, S., Magalhães, A.R., Gonçalves,
    D., Greco, C., & Tagliabue, J. (2022). Contrastive language and vision learning
    of general fashion concepts. *Scientific Reports, 12*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Isom, J.A., Walsh, M., & Gardner, J.M. (2017). Social Media and Pathology:
    Where Are We Now and Why Does it Matter? *Advances in Anatomic Pathology*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M.,
    Coombes, T., Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kundurthy,
    S., Crowson, K., Schmidt, L., Kaczmarczyk, R., & Jitsev, J. (2022). LAION-5B:
    An open large-scale dataset for training next generation image-text models. *ArXiv,
    abs/2210.08402*.'
  prefs: []
  type: TYPE_NORMAL
- en: Zhang, S., Xu, Y., Usuyama, N., Bagga, J.K., Tinn, R., Preston, S., Rao, R.N.,
    Wei, M., Valluri, N., Wong, C., Lungren, M.P., Naumann, T., & Poon, H. (2023).
    Large-Scale Domain-Specific Pretraining for Biomedical Vision-Language Processing.
    *ArXiv, abs/2303.00915*.
  prefs: []
  type: TYPE_NORMAL
