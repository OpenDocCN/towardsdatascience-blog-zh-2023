# ä½¿ç”¨ Pytest å¯¹ PySpark ä»£ç è¿›è¡Œå•å…ƒæµ‹è¯•

> åŸæ–‡ï¼š[`towardsdatascience.com/unit-testing-pyspark-code-using-pytest-b5ab2fd54415`](https://towardsdatascience.com/unit-testing-pyspark-code-using-pytest-b5ab2fd54415)

[](https://julianwest155.medium.com/?source=post_page-----b5ab2fd54415--------------------------------)![Julian West](https://julianwest155.medium.com/?source=post_page-----b5ab2fd54415--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b5ab2fd54415--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page-----b5ab2fd54415--------------------------------) [Julian West](https://julianwest155.medium.com/?source=post_page-----b5ab2fd54415--------------------------------)

Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----b5ab2fd54415--------------------------------) Â·10 åˆ†é’Ÿé˜…è¯»Â·2023 å¹´ 1 æœˆ 16 æ—¥

--

![](img/314b8ebf1cb2596498d48e1f09861a50.png)

å›¾ç‰‡æ¥æºäº[Jez Timms](https://unsplash.com/@jeztimms?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)åœ¨[Unsplash](https://unsplash.com/photos/_Ch_onWf38o?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)

**æˆ‘éå¸¸å–œæ¬¢å•å…ƒæµ‹è¯•ã€‚**

**é˜…è¯»äº†ä¸¤æœ¬ä¹¦â€”â€”** [**ã€ŠåŠ¡å®ç¨‹åºå‘˜ã€‹**](https://engineeringfordatascience.com/book-notes/pragmatic-programmer/) **å’Œ** [**ã€Šé‡æ„ã€‹**](https://engineeringfordatascience.com/book-notes/refactoring/) **â€”â€”å½»åº•æ”¹å˜äº†æˆ‘å¯¹å•å…ƒæµ‹è¯•çš„çœ‹æ³•ã€‚**

> â€œæµ‹è¯•ä¸æ˜¯ä¸ºäº†æ‰¾å‡ºæ¼æ´ã€‚
> 
> æˆ‘ä»¬ç›¸ä¿¡ï¼Œæµ‹è¯•çš„ä¸»è¦å¥½å¤„å‘ç”Ÿåœ¨ä½ æ€è€ƒå’Œç¼–å†™æµ‹è¯•çš„æ—¶å€™ï¼Œè€Œä¸æ˜¯è¿è¡Œå®ƒä»¬çš„æ—¶å€™ã€‚â€
> 
> *â€” ã€ŠåŠ¡å®ç¨‹åºå‘˜ã€‹ï¼Œå¤§å«Â·æ‰˜é©¬æ–¯å’Œå®‰å¾·é²Â·äº¨ç‰¹*

æˆ‘ä¸å†æŠŠæµ‹è¯•è§†ä¸ºåœ¨å®Œæˆæ•°æ®ç®¡é“åéœ€è¦å®Œæˆçš„ç¹çå·¥ä½œï¼Œè€Œæ˜¯å°†å…¶è§†ä¸ºä¸€ä¸ªå¼ºå¤§çš„å·¥å…·ï¼Œç”¨äºæ”¹å–„ä»£ç è®¾è®¡ï¼Œå‡å°‘è€¦åˆï¼Œæ›´å¿«é€Ÿåœ°è¿­ä»£ï¼Œå¹¶ä¸å·¥ä½œä¸­çš„å…¶ä»–äººå»ºç«‹ä¿¡ä»»ã€‚

ç„¶è€Œï¼Œä¸ºæ•°æ®åº”ç”¨ç¨‹åºç¼–å†™è‰¯å¥½çš„æµ‹è¯•å¯èƒ½å¾ˆå›°éš¾ã€‚

ä¸ä¼ ç»Ÿçš„è½¯ä»¶åº”ç”¨ç¨‹åºå…·æœ‰ç›¸å¯¹æ˜ç¡®çš„è¾“å…¥ä¸åŒï¼Œç”Ÿäº§ç¯å¢ƒä¸­çš„æ•°æ®åº”ç”¨ç¨‹åºä¾èµ–äºå¤§é‡ä¸”ä¸æ–­å˜åŒ–çš„è¾“å…¥æ•°æ®ã€‚

åœ¨æµ‹è¯•ç¯å¢ƒä¸­å‡†ç¡®åœ°è¡¨ç¤ºè¿™äº›æ•°æ®ï¼Œä»¥è¶³å¤Ÿçš„ç»†èŠ‚è¦†ç›–æ‰€æœ‰è¾¹ç•Œæƒ…å†µï¼Œå¯èƒ½æ˜¯éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚æœ‰äº›äººè®¤ä¸ºå¯¹æ•°æ®ç®¡é“è¿›è¡Œå•å…ƒæµ‹è¯•æ²¡æœ‰ä»€ä¹ˆæ„ä¹‰ï¼Œè€Œæ˜¯æ›´å…³æ³¨æ•°æ®éªŒè¯æŠ€æœ¯ã€‚

æˆ‘åšä¿¡åœ¨æ•°æ®ç®¡é“ä¸­åŒæ—¶å®ç°å•å…ƒæµ‹è¯•å’Œæ•°æ®éªŒè¯ã€‚**å•å…ƒæµ‹è¯•ä¸ä»…ä»…æ˜¯ä¸ºäº†æ‰¾å‡ºæ¼æ´ï¼Œæ›´æ˜¯ä¸ºäº†åˆ›å»ºè®¾è®¡æ›´å¥½çš„ä»£ç ï¼Œå¹¶ä¸åŒäº‹å’Œæœ€ç»ˆç”¨æˆ·å»ºç«‹ä¿¡ä»»ã€‚**

å¦‚æœä½ èƒ½å…»æˆç¼–å†™æµ‹è¯•çš„ä¹ æƒ¯ï¼Œä½ å°†ç¼–å†™å‡ºè®¾è®¡æ›´å¥½çš„ä»£ç ï¼ŒèŠ‚çœé•¿è¿œçš„æ—¶é—´ï¼Œå¹¶å‡å°‘ç®¡é“å¤±è´¥æˆ–åœ¨ç”Ÿäº§ä¸­ç»™å‡ºä¸æ­£ç¡®ç»“æœçš„ç—›è‹¦ã€‚

# å•å…ƒæµ‹è¯• PySpark ä»£ç çš„æŒ‘æˆ˜

ä¸€ä¸ªå¥½çš„å•å…ƒæµ‹è¯•åº”å…·å¤‡ä»¥ä¸‹ç‰¹å¾ï¼š

+   **ä¸“æ³¨**ã€‚æ¯ä¸ªæµ‹è¯•åº”æµ‹è¯•ä¸€ä¸ªå•ä¸€çš„è¡Œä¸º/åŠŸèƒ½ã€‚

+   **å¿«é€Ÿ**ã€‚å…è®¸ä½ å¿«é€Ÿè¿­ä»£å¹¶è·å¾—åé¦ˆã€‚

+   **å­¤ç«‹**ã€‚æ¯ä¸ªæµ‹è¯•åº”è¯¥è´Ÿè´£æµ‹è¯•ç‰¹å®šçš„åŠŸèƒ½ï¼Œå¹¶ä¸”ä¸ä¾èµ–äºå¤–éƒ¨å› ç´ ä»¥ä¾¿æˆåŠŸè¿è¡Œã€‚

+   **ç®€æ´**ã€‚åˆ›å»ºæµ‹è¯•ä¸åº”åŒ…å«å¤§é‡çš„æ ·æ¿ä»£ç æ¥æ¨¡æ‹Ÿ/åˆ›å»ºå¤æ‚å¯¹è±¡ä»¥ä¾¿æµ‹è¯•è¿è¡Œã€‚

**å½“æ¶‰åŠåˆ°ä¸“é—¨ä¸º PySpark ç®¡é“ç¼–å†™å•å…ƒæµ‹è¯•æ—¶ï¼Œç¼–å†™ä¸“æ³¨ã€å¿«é€Ÿã€å­¤ç«‹å’Œç®€æ´çš„æµ‹è¯•å¯èƒ½æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚**

ä½ å¯èƒ½ä¼šé‡åˆ°ä¸€äº›éšœç¢â€¦â€¦

## é¦–å…ˆç¼–å†™å¯æµ‹è¯•çš„ä»£ç 

PySpark ç®¡é“å¾€å¾€è¢«ç¼–å†™æˆä¸€ä¸ªè´Ÿè´£å¤šä¸ªè½¬æ¢çš„å·¨å‹å‡½æ•°ã€‚

ä¾‹å¦‚ï¼š

```py
def main(spark, country):
    """Example function for processing data received from different countries"""
    # fetch input data
    ...
    # preprocess based on individual country requirements
    if country == 'US':
       # preprocessing for US
    elif country == 'GB':
       # preprocessing for UK
    else:
       # more preprocessing etc..
    # join dataframes together
    ...
    # run some calculations/aggregate
    ...
    # save results
    ...
```

ä»è¿™ä¸ªè§’åº¦è€ƒè™‘è½¬æ¢æ˜¯åˆç†çš„ï¼Œå¹¶ä¸”åœ¨å¾ˆå¤šæ–¹é¢æ›´å®¹æ˜“ç†è§£å’Œé˜…è¯»ã€‚

ä½†æ˜¯ï¼Œå½“ä½ å¼€å§‹å°è¯•ä¸ºè¿™ä¸ªå‡½æ•°ç¼–å†™æµ‹è¯•æ—¶ï¼Œä½ ä¼šå¾ˆå¿«æ„è¯†åˆ°ï¼Œç¼–å†™ä¸€ä¸ªè¦†ç›–æ‰€æœ‰åŠŸèƒ½çš„æµ‹è¯•æ˜¯éå¸¸å›°éš¾çš„ã€‚

è¿™æ˜¯å› ä¸ºå‡½æ•°é«˜åº¦è€¦åˆï¼Œå¹¶ä¸”å‡½æ•°å¯èƒ½æœ‰è®¸å¤šä¸åŒçš„è·¯å¾„ã€‚

å³ä½¿ä½ ç¼–å†™äº†ä¸€ä¸ªæµ‹è¯•æ¥éªŒè¯è¾“å…¥æ•°æ®å’Œè¾“å‡ºæ•°æ®æ˜¯å¦ç¬¦åˆé¢„æœŸã€‚å¦‚æœæµ‹è¯•å› ä»»ä½•åŸå› å¤±è´¥ï¼Œå°†å¾ˆéš¾ç†è§£é•¿å‡½æ•°çš„å“ªä¸ªéƒ¨åˆ†å‡ºç°äº†é—®é¢˜ã€‚

ç›¸åï¼Œä½ åº”è¯¥å°†ä½ çš„è½¬æ¢æ‹†åˆ†ä¸ºè´Ÿè´£å•ä¸€ä»»åŠ¡çš„å¯é‡ç”¨å‡½æ•°å—ã€‚ç„¶åä½ å¯ä»¥ä¸ºæ¯ä¸ªå•ç‹¬çš„å‡½æ•°ï¼ˆä»»åŠ¡ï¼‰ç¼–å†™å•å…ƒæµ‹è¯•ã€‚å½“è¿™äº›å•å…ƒæµ‹è¯•éƒ½é€šè¿‡æ—¶ï¼Œä½ å¯ä»¥æ›´æœ‰ä¿¡å¿ƒåœ°å°†æ‰€æœ‰å‡½æ•°ç»„åˆåœ¨ä¸€èµ·ï¼Œå¾—åˆ°æœ€ç»ˆç®¡é“çš„è¾“å‡ºã€‚

ç¼–å†™æµ‹è¯•æ˜¯ä¸€ç§è‰¯å¥½çš„å®è·µï¼Œå¹¶è¿«ä½¿ä½ è€ƒè™‘è®¾è®¡åŸåˆ™ã€‚å¦‚æœæµ‹è¯•ä½ çš„ä»£ç å¾ˆå›°éš¾ï¼Œé‚£ä¹ˆä½ å¯èƒ½éœ€è¦é‡æ–°è€ƒè™‘ä½ çš„ä»£ç è®¾è®¡ã€‚

## é€Ÿåº¦

Spark è¢«ä¼˜åŒ–ç”¨äºå¤„ç†éå¸¸å¤§çš„æ•°æ®ï¼Œè®¡ç®—ä¹Ÿè¢«ä¼˜åŒ–ä»¥åˆ†å¸ƒåœ¨å¤šå°æœºå™¨ä¸Šã€‚

è¿™åœ¨å¤§å‹é›†ç¾¤ä¸­è¡¨ç°å¾—å¾ˆå¥½ï¼Œä½†å®é™…ä¸Šåœ¨ä½ å¯èƒ½ç”¨äºå•å…ƒæµ‹è¯•çš„å•å°æœºå™¨ä¸Šä¼šä¸¥é‡å½±å“æ€§èƒ½ã€‚

å½“ä½ è¿è¡Œ PySpark ç®¡é“æ—¶ï¼Œspark ä¼šè¯„ä¼°æ•´ä¸ªç®¡é“å¹¶è®¡ç®—ä¸€ä¸ªä¼˜åŒ–çš„â€˜è®¡åˆ’â€™ä»¥åœ¨åˆ†å¸ƒå¼é›†ç¾¤ä¸Šæ‰§è¡Œè®¡ç®—ã€‚

è®¡åˆ’å¸¦æ¥äº†æ˜¾è‘—çš„å¼€é”€ã€‚å½“ä½ åœ¨åˆ†å¸ƒå¼é›†ç¾¤ä¸Šå¤„ç†æ•° TB çš„æ•°æ®æ—¶ï¼Œè¿™æ˜¯åˆç†çš„ã€‚ä½†åœ¨å•å°æœºå™¨ä¸Šå¤„ç†å°æ•°æ®é›†æ—¶ï¼Œé€Ÿåº¦å¯èƒ½ä¼šå‡ºå¥‡åœ°æ…¢ã€‚å°¤å…¶æ˜¯ä¸ Pandas çš„ä½“éªŒç›¸æ¯”ã€‚

å¦‚æœä¸ä¼˜åŒ–ä½ çš„ SparkSession é…ç½®å‚æ•°ï¼Œä½ çš„å•å…ƒæµ‹è¯•å°†ä¼šè¿è¡Œå¾—éå¸¸ç¼“æ…¢ã€‚

## ä¾èµ– Spark Session

è¦åœ¨ä½ çš„å•å…ƒæµ‹è¯•ä¸­è¿è¡Œ PySpark ä»£ç ï¼Œä½ éœ€è¦ä¸€ä¸ª SparkSessionã€‚

å¦‚ä¸Šæ‰€è¿°ï¼Œç†æƒ³æƒ…å†µä¸‹ï¼Œæ¯ä¸ªæµ‹è¯•åº”ä¸å…¶ä»–æµ‹è¯•éš”ç¦»ï¼Œå¹¶ä¸”ä¸éœ€è¦å¤æ‚çš„å¤–éƒ¨å¯¹è±¡ã€‚ä¸å¹¸çš„æ˜¯ï¼Œä¸èƒ½é¿å…ä¸ºå•å…ƒæµ‹è¯•å¯åŠ¨ Spark ä¼šè¯çš„è¦æ±‚ã€‚

åˆ›å»º Spark ä¼šè¯æ˜¯ç¼–å†™ PySpark ç®¡é“å•å…ƒæµ‹è¯•æ—¶è¦å…‹æœçš„ç¬¬ä¸€ä¸ªéšœç¢ã€‚

ä½ åº”è¯¥å¦‚ä½•ä¸ºæµ‹è¯•åˆ›å»º SparkSessionï¼Ÿ

ä¸ºæ¯ä¸ªæµ‹è¯•åˆå§‹åŒ–ä¸€ä¸ªæ–°çš„ Spark ä¼šè¯ä¼šæ˜¾è‘—å¢åŠ è¿è¡Œæµ‹è¯•çš„æ—¶é—´ï¼Œå¹¶ç»™æµ‹è¯•å¼•å…¥å¤§é‡æ ·æ¿ä»£ç ã€‚

é«˜æ•ˆåœ°åˆ›å»ºå’Œå…±äº« SparkSession å¯¹äºä¿æŒæµ‹è¯•æ€§èƒ½åœ¨å¯æ¥å—çš„æ°´å¹³è‡³å…³é‡è¦ã€‚

## æ•°æ®

ä½ çš„æµ‹è¯•å°†éœ€è¦è¾“å…¥æ•°æ®ã€‚

åˆ›å»ºå¤§æ•°æ®ç®¡é“çš„ç¤ºä¾‹æ•°æ®å­˜åœ¨ä¸¤ä¸ªä¸»è¦é—®é¢˜ã€‚

é¦–å…ˆæ˜¯å¤§å°ã€‚æ˜¾ç„¶ï¼Œä½ ä¸èƒ½åœ¨ç”Ÿäº§ä¸­ä½¿ç”¨çš„å®Œæ•´æ•°æ®é›†ä¸Šè¿è¡Œæµ‹è¯•ã€‚ä½ å¿…é¡»ä½¿ç”¨ä¸€ä¸ªæ›´å°çš„å­é›†ã€‚

ä½†æ˜¯ï¼Œé€šè¿‡ä½¿ç”¨ä¸€ä¸ªå°æ•°æ®é›†ï¼Œä½ ä¼šé‡åˆ°ç¬¬äºŒä¸ªé—®é¢˜ï¼Œå³æä¾›è¶³å¤Ÿçš„æµ‹è¯•æ•°æ®æ¥è¦†ç›–ä½ æƒ³è¦å¤„ç†çš„æ‰€æœ‰è¾¹ç•Œæƒ…å†µã€‚

*çœŸæ­£*å¾ˆéš¾ä¸ºæµ‹è¯•æ¨¡æ‹Ÿç°å®æ•°æ®ã€‚è™½ç„¶å¯¹æ­¤æ²¡æœ‰å¤ªå¤šåŠæ³•ï¼Œä½†ä½ å¯ä»¥ä½¿ç”¨æ›´å°ã€ç›®æ ‡æ˜ç¡®çš„æ•°æ®é›†è¿›è¡Œæµ‹è¯•ã€‚

# ä½¿ç”¨ Pytest å¯¹ PySpark ä»£ç è¿›è¡Œå•å…ƒæµ‹è¯•çš„æ­¥éª¤

è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªä½¿ç”¨[PyTest](https://docs.pytest.org/en/7.2.x/)ä¸º PySpark ç®¡é“ç¼–å†™å•å…ƒæµ‹è¯•çš„ç¤ºä¾‹æ¥è¿›è¡Œå®è·µã€‚

> *ğŸ’» å®Œæ•´ä»£ç å¯åœ¨æ­¤* [*GitHub å­˜å‚¨åº“*](https://github.com/julian-west/e4ds-snippets/tree/master/pyspark/pyspark_unit_testing) *ä¸­è·å–*

## ç¤ºä¾‹ä»£ç 

è¿™æ˜¯ä¸€ä¸ªå¤„ç†é“¶è¡Œäº¤æ˜“çš„ PySpark ç®¡é“ç¤ºä¾‹ã€‚åœ¨è¿™ä¸ªåœºæ™¯ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›å°†åŸå§‹äº¤æ˜“åˆ†ç±»ä¸ºå€Ÿè®°è´¦æˆ·æˆ–ä¿¡ç”¨è´¦æˆ·äº¤æ˜“ï¼Œé€šè¿‡å°†å…¶ä¸ä¸€äº›å‚è€ƒæ•°æ®è¿æ¥èµ·æ¥ã€‚

æ¯ä¸ªäº¤æ˜“è®°å½•éƒ½é™„å¸¦ä¸€ä¸ªè´¦æˆ· IDã€‚æˆ‘ä»¬å°†ä½¿ç”¨è¿™ä¸ªè´¦æˆ· ID è¿æ¥åˆ°åŒ…å«è¯¥è´¦æˆ· ID æ˜¯å€Ÿè®°è´¦æˆ·è¿˜æ˜¯ä¿¡ç”¨è´¦æˆ·çš„ä¿¡æ¯çš„è´¦æˆ·ä¿¡æ¯è¡¨ã€‚

```py
import pyspark.sql.functions as F
from pyspark.sql import DataFrame

def classify_debit_credit_transactions(
    transactionsDf: DataFrame, accountDf: DataFrame
) -> DataFrame:
    """Join transactions with account information and classify as debit/credit"""
    # normalise strings
    transactionsDf = transactionsDf.withColumn(
        "transaction_information_cleaned",
        F.regexp_replace(F.col("transaction_information"), r"[^A-Z0-9]+", ""),
    )
    # join on customer account using first 9 characters
    transactions_accountsDf = transactionsDf.join(
        accountDf,
        on=F.substring(F.col("transaction_information_cleaned"), 1, 9)
        == F.col("account_number"),
        how="inner",
    )
    # classify transactions as from debit or credit account customers
    credit_account_ids = ["100", "101", "102"]
    debit_account_ids = ["200", "201", "202"]
    transactions_accountsDf = transactions_accountsDf.withColumn(
        "business_line",
        F.when(F.col("business_line_id").isin(credit_account_ids), F.lit("credit"))
        .when(F.col("business_line_id").isin(debit_account_ids), F.lit("debit"))
        .otherwise(F.lit("other")),
    )
    return transactions_accountsDf 
```

è¿™ä¸ªç¤ºä¾‹ç®¡é“å­˜åœ¨å‡ ä¸ªé—®é¢˜ï¼š

+   éš¾ä»¥é˜…è¯»ã€‚ä¸€ä¸ªåœ°æ–¹åŒ…å«äº†å¤§é‡å¤æ‚çš„é€»è¾‘ã€‚ä¾‹å¦‚ï¼Œæ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢ã€æŒ‰å­å­—ç¬¦ä¸²è¿æ¥ç­‰ã€‚

+   éš¾ä»¥æµ‹è¯•ã€‚å•ä¸ªå‡½æ•°è´Ÿè´£å¤šä¸ªæ“ä½œ

+   éš¾ä»¥é‡ç”¨ã€‚å€Ÿè®°/ä¿¡ç”¨åˆ†ç±»æ˜¯ä¸šåŠ¡é€»è¾‘ï¼Œæ— æ³•è½»æ¾åœ¨é¡¹ç›®ä¸­é‡ç”¨

## æ­¥éª¤ 1ï¼šå°†ä»£ç é‡æ„ä¸ºæ›´å°çš„é€»è¾‘å•å…ƒ

è®©æˆ‘ä»¬é¦–å…ˆå°†ä»£ç é‡æ„ä¸ºå•ç‹¬çš„å‡½æ•°ï¼Œç„¶åå°†è¿™äº›å‡½æ•°ç»„åˆæˆä¸»`classify_debit_credit_transactions`å‡½æ•°ã€‚

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºæ¯ä¸ªå•ç‹¬çš„å‡½æ•°ç¼–å†™æµ‹è¯•ï¼Œä»¥ç¡®ä¿å…¶æŒ‰é¢„æœŸè¡Œä¸ºã€‚

è™½ç„¶è¿™å¢åŠ äº†ä»£ç çš„æ€»ä½“è¡Œæ•°ï¼Œä½†æµ‹è¯•èµ·æ¥æ›´å®¹æ˜“ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥åœ¨é¡¹ç›®çš„å…¶ä»–éƒ¨åˆ†é‡ç”¨è¿™äº›å‡½æ•°ã€‚

```py
import pyspark.sql.functions as F
from pyspark.sql import DataFrame

def classify_debit_credit_transactions(
    transactionsDf: DataFrame, accountsDf: DataFrame
) -> DataFrame:
    """Join transactions with account information and classify as debit/credit"""
    transactionsDf = normalise_transaction_information(transactionsDf)
    transactions_accountsDf = join_transactionsDf_to_accountsDf(
        transactionsDf, accountsDf
    )
    transactions_accountsDf = apply_debit_credit_business_classification(
        transactions_accountsDf
    )
    return transactions_accountsDf
```

```py
def normalise_transaction_information(transactionsDf: DataFrame) -> DataFrame:
    """Remove special characters from transaction information"""
    return transactionsDf.withColumn(
        "transaction_information_cleaned",
        F.regexp_replace(F.col("transaction_information"), r"[^A-Z0-9]+", ""),
    )
```

```py
def join_transactionsDf_to_accountsDf(
    transactionsDf: DataFrame, accountsDf: DataFrame
) -> DataFrame:
    """Join transactions to accounts information"""
    return transactionsDf.join(
        accountsDf,
        on=F.substring(F.col("transaction_information_cleaned"), 1, 9)
        == F.col("account_number"),
        how="inner",
    )
```

```py
def apply_debit_credit_business_classification(
    transactions_accountsDf: DataFrame,
) -> DataFrame:
    """Classify transactions as coming from debit or credit account customers"""

    CREDIT_ACCOUNT_IDS = ["101", "102", "103"]
    DEBIT_ACCOUNT_IDS = ["202", "202", "203"]

    return transactions_accountsDf.withColumn(
        "business_line",
        F.when(F.col("business_line_id").isin(CREDIT_ACCOUNT_IDS), F.lit("credit"))
        .when(F.col("business_line_id").isin(DEBIT_ACCOUNT_IDS), F.lit("debit"))
        .otherwise(F.lit("other")),
    )
```

## 3\. ä½¿ç”¨ Fixtures åˆ›å»ºå¯é‡ç”¨çš„ SparkSession

åœ¨ç¼–å†™å•å…ƒæµ‹è¯•ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªå¯ä»¥åœ¨æ‰€æœ‰æµ‹è¯•ä¸­é‡ç”¨çš„ SparkSessionã€‚

ä¸ºæ­¤ï¼Œæˆ‘ä»¬åœ¨ `conftest.py` æ–‡ä»¶ä¸­åˆ›å»ºä¸€ä¸ª [PyTest fixture](https://docs.pytest.org/en/6.2.x/fixture.html)ã€‚

Pytest fixtures æ˜¯ä¸€æ¬¡åˆ›å»ºç„¶ååœ¨å¤šä¸ªæµ‹è¯•ä¸­é‡å¤ä½¿ç”¨çš„å¯¹è±¡ã€‚è¿™å¯¹äºåƒ SparkSession è¿™æ ·çš„å¤æ‚å¯¹è±¡ç‰¹åˆ«æœ‰ç”¨ï¼Œå› ä¸ºåˆ›å»ºå®ƒä»¬çš„å¼€é”€å¾ˆå¤§ã€‚

```py
# conftest.py
from pyspark.sql import SparkSession
import pytest

@pytest.fixture(scope="session")
def spark():
    spark = (
        SparkSession.builder.master("local[1]")
        .appName("local-tests")
        .config("spark.executor.cores", "1")
        .config("spark.executor.instances", "1")
        .config("spark.sql.shuffle.partitions", "1")
        .config("spark.driver.bindAddress", "127.0.0.1")
        .getOrCreate()
    )
    yield spark
    spark.stop()
```

è®¾ç½®ä¸€äº›é…ç½®å‚æ•°ä»¥ä¼˜åŒ– SparkSessionï¼Œç”¨äºåœ¨å•å°æœºå™¨ä¸Šå¤„ç†å°æ•°æ®è¿›è¡Œæµ‹è¯•æ˜¯å¾ˆé‡è¦çš„ï¼š

+   `master = local[1]` â€“ æŒ‡å®š spark åœ¨ä¸€å°æœ¬åœ°æœºå™¨ä¸Šè¿è¡Œï¼Œå¹¶ä¸”ä½¿ç”¨ä¸€ä¸ªçº¿ç¨‹

+   `spark.executor.cores = 1` â€“ è®¾ç½®æ ¸å¿ƒæ•°ä¸ºä¸€ä¸ª

+   `spark.executor.instances = 1` - è®¾ç½®æ‰§è¡Œå™¨ä¸ºä¸€ä¸ª

+   `spark.sql.shuffle.partitions = 1` - è®¾ç½®æœ€å¤§åˆ†åŒºæ•°ä¸º 1

+   `spark.driver.bindAddress = 127.0.0.1` â€“ ï¼ˆå¯é€‰ï¼‰æ˜¾å¼æŒ‡å®šé©±åŠ¨ç¨‹åºç»‘å®šåœ°å€ã€‚å¦‚æœä½ çš„æœºå™¨ä¹Ÿä¸è¿œç¨‹é›†ç¾¤æœ‰æ´»åŠ¨è¿æ¥ï¼Œè¿™å°†å¾ˆæœ‰ç”¨ã€‚

è¿™äº›é…ç½®å‚æ•°æœ¬è´¨ä¸Šå‘Šè¯‰ spark ä½ åœ¨å•å°æœºå™¨ä¸Šå¤„ç†æ•°æ®ï¼Œå¹¶ä¸” spark ä¸åº”è¯¥å°è¯•åˆ†å‘è®¡ç®—ã€‚è¿™å°†å¤§å¤§èŠ‚çœç®¡é“æ‰§è¡Œå’Œè®¡ç®—æœ¬èº«çš„æ—¶é—´ã€‚

æ³¨æ„ï¼Œå»ºè®®ä½¿ç”¨ `yield` è€Œä¸æ˜¯ `return` æ¥è¿”å› spark sessionã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·é˜…è¯» [PyTest æ–‡æ¡£](https://docs.pytest.org/en/7.1.x/how-to/fixtures.html#yield-fixtures-recommended)ã€‚ä½¿ç”¨ `yield` è¿˜å…è®¸ä½ åœ¨æµ‹è¯•è¿è¡Œåæ‰§è¡Œä»»ä½•æ¸…ç†æ“ä½œï¼ˆä¾‹å¦‚ï¼Œåˆ é™¤æœ¬åœ°ä¸´æ—¶ç›®å½•ã€æ•°æ®åº“æˆ–è¡¨ç­‰ï¼‰ã€‚

## 4\. ä¸ºä»£ç åˆ›å»ºå•å…ƒæµ‹è¯•

ç°åœ¨è®©æˆ‘ä»¬ä¸ºæˆ‘ä»¬çš„ä»£ç ç¼–å†™ä¸€äº›æµ‹è¯•ã€‚

æˆ‘å‘ç°æœ€æœ‰æ•ˆçš„æ–¹å¼æ˜¯ç”¨ä»¥ä¸‹ç»“æ„ç»„ç»‡æˆ‘çš„ PySpark å•å…ƒæµ‹è¯•ï¼š

+   åˆ›å»ºè¾“å…¥ DataFrame

+   ä½¿ç”¨æˆ‘ä»¬è¦æµ‹è¯•çš„å‡½æ•°åˆ›å»ºè¾“å‡º DataFrame

+   æŒ‡å®šé¢„æœŸçš„è¾“å‡ºå€¼

+   æ¯”è¾ƒç»“æœ

æˆ‘è¿˜å°½é‡ç¡®ä¿æµ‹è¯•è¦†ç›–æ­£é¢æµ‹è¯•ç”¨ä¾‹å’Œè‡³å°‘ä¸€ä¸ªè´Ÿé¢æµ‹è¯•ç”¨ä¾‹ã€‚

```py
from src.data_processing import (
    apply_debit_credit_business_classification,
    classify_debit_credit_transactions,
    join_transactionsDf_to_accountsDf,
    normalise_transaction_information,
)
```

```py
def test_classify_debit_credit_transactions(spark):
    # create input test dataframes
    transactionsDf = spark.createDataFrame(
        data=[
            ("1", 1000.00, "123-456-789"),
            ("3", 3000.00, "222222222EUR"),
        ],
        schema=["transaction_id", "amount", "transaction_information"],
    )
    accountsDf = spark.createDataFrame(
        data=[
            ("123456789", "101"),
            ("222222222", "202"),
            ("000000000", "302"),
        ],
        schema=["account_number", "business_line_id"],
    )

    # output dataframe after applying function
    output = classify_debit_credit_transactions(transactionsDf, accountsDf)

    # expected outputs in the target column
    expected_classifications = ["credit", "debit"]

    # assert results are as expected
    assert output.count() == 2
    assert [row.business_line for row in output.collect()] == expected_classifications
```

```py
def test_normalise_transaction_information(spark):
    data = ["123-456-789", "123456789", "123456789EUR", "TEXT*?WITH.*CHARACTERS"]
    test_df = spark.createDataFrame(data, "string").toDF("transaction_information")
    expected = ["123456789", "123456789", "123456789EUR", "TEXTWITHCHARACTERS"]
    output = normalise_transaction_information(test_df)
    assert [row.transaction_information_cleaned for row in output.collect()] == expected
```

```py
def test_join_transactionsDf_to_accountsDf(spark):
    data = ["123456789", "222222222EUR"]
    transactionsDf = spark.createDataFrame(data, "string").toDF(
        "transaction_information_cleaned"
    )
    data = [
        "123456789",  # match
        "222222222",  # match
        "000000000",  # no-match
    ]
    accountsDf = spark.createDataFrame(data, "string").toDF("account_number")
    output = join_transactionsDf_to_accountsDf(transactionsDf, accountsDf)
    assert output.count() == 2
```

```py
def test_apply_debit_credit_business_classification(spark):
    data = [
        "101",  # credit
        "202",  # debit
        "000",  # other
    ]
    df = spark.createDataFrame(data, "string").toDF("business_line_id")
    output = apply_debit_credit_business_classification(df)
    expected = ["credit", "debit", "other"]
    assert [row.business_line for row in output.collect()] == expected
```

æˆ‘ä»¬ç°åœ¨ä¸º PySpark ç®¡é“ä¸­çš„æ¯ä¸ªç»„ä»¶éƒ½æœ‰äº†å•å…ƒæµ‹è¯•ã€‚

ç”±äºæ¯ä¸ªæµ‹è¯•éƒ½é‡ç”¨ç›¸åŒçš„ SparkSessionï¼Œè¿è¡Œå¤šä¸ªæµ‹è¯•çš„å¼€é”€å¤§å¤§å‡å°‘ã€‚

# PySpark ä»£ç å•å…ƒæµ‹è¯•çš„è¿›ä¸€æ­¥æç¤º

## åˆ›å»ºåŒ…å«æœ€å°‘æ‰€éœ€ä¿¡æ¯çš„æµ‹è¯• DataFrame

åœ¨åˆ›å»ºæµ‹è¯•æ•°æ®çš„ DataFrame æ—¶ï¼Œåªåˆ›å»ºä¸è½¬æ¢ç›¸å…³çš„åˆ—ã€‚

ä½ åªéœ€è¦åˆ›å»ºå‡½æ•°æ‰€éœ€çš„åˆ—çš„æ•°æ®ã€‚ä½ ä¸éœ€è¦æ‰€æœ‰å¯èƒ½å­˜åœ¨äºç”Ÿäº§æ•°æ®ä¸­çš„å…¶ä»–åˆ—ã€‚

è¿™æœ‰åŠ©äºç¼–å†™ç®€æ´çš„å‡½æ•°ï¼Œå¹¶ä¸”æ›´å…·å¯è¯»æ€§ï¼Œå› ä¸ºå¯ä»¥æ¸…æ¥šåœ°çŸ¥é“å“ªäº›åˆ—æ˜¯å‡½æ•°æ‰€éœ€çš„ä»¥åŠå—å‡½æ•°å½±å“çš„ã€‚å¦‚æœä½ å‘ç°éœ€è¦ä¸€ä¸ªåŒ…å«è®¸å¤šåˆ—çš„å¤§ DataFrame æ¥æ‰§è¡Œè½¬æ¢ï¼Œä½ å¯èƒ½ä¸€æ¬¡å°è¯•åšå¾—å¤ªå¤šäº†ã€‚

è¿™åªæ˜¯ä¸€ä¸ªæŒ‡å¯¼æ–¹é’ˆï¼Œä½ è‡ªå·±çš„ç”¨ä¾‹å¯èƒ½éœ€è¦æ›´å¤æ‚çš„æµ‹è¯•æ•°æ®ï¼Œä½†å¦‚æœå¯èƒ½ï¼Œä¿æŒæ•°æ®å°å·§ã€ç®€æ´ï¼Œå¹¶å±€é™äºæµ‹è¯•èŒƒå›´ã€‚

## è®°å¾—è°ƒç”¨ä¸€ä¸ªâ€œæ“ä½œâ€ä»¥è§¦å‘ PySpark è®¡ç®—

PySpark ä½¿ç”¨æƒ°æ€§è®¡ç®—ã€‚ä½ éœ€è¦åœ¨æµ‹è¯•æœŸé—´è°ƒç”¨ä¸€ä¸ªâ€˜actionâ€™ï¼ˆä¾‹å¦‚ `collect`ã€`count` ç­‰ï¼‰ï¼Œä»¥è®¡ç®—ä¸€ä¸ªå¯ä»¥ä¸é¢„æœŸè¾“å‡ºè¿›è¡Œæ¯”è¾ƒçš„ç»“æœã€‚

## å¦‚æœä¸éœ€è¦çš„è¯ï¼Œä¸è¦è¿è¡Œæ‰€æœ‰çš„ PySpark æµ‹è¯•

PySpark æµ‹è¯•é€šå¸¸æ¯”æ­£å¸¸å•å…ƒæµ‹è¯•è¿è¡Œæ—¶é—´æ›´é•¿ï¼Œå› ä¸ºæœ‰è®¡ç®—è®¡åˆ’å’Œæ‰§è¡Œçš„å¼€é”€ã€‚

åœ¨å¼€å‘è¿‡ç¨‹ä¸­ï¼Œåˆ©ç”¨ Pytest çš„ä¸€äº›åŠŸèƒ½ï¼Œä¾‹å¦‚ `-k` æ ‡å¿—æ¥è¿è¡Œå•ä¸ªæµ‹è¯•æˆ–ä»…è¿è¡Œå•ä¸ªæ–‡ä»¶ä¸­çš„æµ‹è¯•ã€‚ç„¶åï¼Œä»…åœ¨æäº¤ä»£ç ä¹‹å‰è¿è¡Œå®Œæ•´çš„æµ‹è¯•å¥—ä»¶ã€‚

## ä¿æŒå•å…ƒæµ‹è¯•çš„éš”ç¦»æ€§

æ³¨æ„ä¸è¦åœ¨æµ‹è¯•æœŸé—´ä¿®æ”¹ä½ çš„ spark ä¼šè¯ï¼ˆä¾‹å¦‚ï¼Œåˆ›å»ºä¸€ä¸ªè¡¨ï¼Œä½†ä¹‹åä¸åˆ é™¤å®ƒï¼‰ã€‚

è¡¨æ ¼å°†åœ¨æ‰€æœ‰æµ‹è¯•ä¸­æŒç»­å­˜åœ¨ï¼Œè¿™å¯èƒ½ä¼šå¹²æ‰°é¢„æœŸè¡Œä¸ºã€‚

## å°½é‡å°†æ•°æ®çš„åˆ›å»ºä¿æŒåœ¨ä½¿ç”¨å®ƒçš„åœ°æ–¹é™„è¿‘ã€‚

ä½ å¯ä»¥ä½¿ç”¨ Pytest fixtures åœ¨å¤šä¸ªæµ‹è¯•ä¹‹é—´å…±äº«æ•°æ®æ¡†ï¼Œç”šè‡³ä» CSV æ–‡ä»¶ç­‰ä¸­åŠ è½½æµ‹è¯•æ•°æ®ã€‚

ç„¶è€Œï¼Œæ ¹æ®æˆ‘çš„ç»éªŒï¼Œä¸ºæ¯ä¸ªå•ç‹¬çš„æµ‹è¯•åˆ›å»ºæ‰€éœ€çš„æ•°æ®æ›´å®¹æ˜“ä¸”æ›´å…·å¯è¯»æ€§ã€‚

## æµ‹è¯•æ­£é¢å’Œè´Ÿé¢ç»“æœ

ä¾‹å¦‚ï¼Œåœ¨æµ‹è¯•è¿æ¥æ¡ä»¶æ—¶ï¼Œä½ åº”è¯¥åŒ…æ‹¬ä¸€äº›ä¸æ»¡è¶³è¿æ¥æ¡ä»¶çš„æ•°æ®ã€‚è¿™æœ‰åŠ©äºç¡®ä¿ä½ æ—¢æ’é™¤äº†é”™è¯¯çš„æ•°æ®ï¼ŒåˆåŒ…å«äº†æ­£ç¡®çš„æ•°æ®ã€‚

> æœ¬åšå®¢æœ€åˆå‘å¸ƒåœ¨ [engineeringfordatascience.com](https://engineeringfordatascience.com/posts/pyspark_unit_testing_with_pytest/)

å¦‚æœä½ ä½¿ç”¨ Pytest è¿›è¡Œå•å…ƒæµ‹è¯•ï¼Œå¯ä»¥æŸ¥çœ‹æˆ‘å¦ä¸€ç¯‡åŒ…å« Pytest ä½¿ç”¨æŠ€å·§çš„æ–‡ç« ï¼š

[13 Tips for using PyTest](https://towardsdatascience.com/13-tips-for-using-pytest-5341e3366d2d?source=post_page-----b5ab2fd54415--------------------------------) [## 13 Tips for using PyTest

### å•å…ƒæµ‹è¯•æ˜¯è½¯ä»¶å¼€å‘ä¸­éå¸¸é‡è¦çš„æŠ€èƒ½ã€‚æœ‰ä¸€äº›å¾ˆæ£’çš„ Python åº“å¯ä»¥å¸®åŠ©æˆ‘ä»¬â€¦â€¦

[13 Tips for using PyTest](https://towardsdatascience.com/13-tips-for-using-pytest-5341e3366d2d?source=post_page-----b5ab2fd54415--------------------------------)
