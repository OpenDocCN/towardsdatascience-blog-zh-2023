- en: 'Towards LLM Explainability: Why Did My Model Produce This Output ?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/towards-llm-explainability-why-did-my-model-produce-this-output-8f730fc73713?source=collection_archive---------6-----------------------#2023-12-15](https://towardsdatascience.com/towards-llm-explainability-why-did-my-model-produce-this-output-8f730fc73713?source=collection_archive---------6-----------------------#2023-12-15)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The release in these last few months of larger, better Large Language Models,
    that showcase new capabilities, has been paired with overall growing concerns
    over AI Safety. LMM explainability research tries to expand our understanding
    of how these models work.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@georgiadeaconu?source=post_page-----8f730fc73713--------------------------------)[![Georgia
    Deaconu](../Images/39ba1bea77aa46bb39b2975108c3adaa.png)](https://medium.com/@georgiadeaconu?source=post_page-----8f730fc73713--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8f730fc73713--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8f730fc73713--------------------------------)
    [Georgia Deaconu](https://medium.com/@georgiadeaconu?source=post_page-----8f730fc73713--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc4a98f38b0e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftowards-llm-explainability-why-did-my-model-produce-this-output-8f730fc73713&user=Georgia+Deaconu&userId=c4a98f38b0e&source=post_page-c4a98f38b0e----8f730fc73713---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8f730fc73713--------------------------------)
    ·9 min read·Dec 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8f730fc73713&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftowards-llm-explainability-why-did-my-model-produce-this-output-8f730fc73713&user=Georgia+Deaconu&userId=c4a98f38b0e&source=-----8f730fc73713---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8f730fc73713&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftowards-llm-explainability-why-did-my-model-produce-this-output-8f730fc73713&source=-----8f730fc73713---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) saw a lot of development this past year, such as
    the recent release of GPT-4 and Claude 2\. These models display new abilities
    with respect to their previous versions, but most of them are discovered through
    post-hoc analysis and weren’t part of a purposeful training plan. They are a consequence
    of the model scaling in terms of number of parameters, training data and compute
    resources.
  prefs: []
  type: TYPE_NORMAL
- en: On a conceptual level, I like the analogy between LLMs and compression algorithms.
    Terabytes of internet data go in and many FLOPS later we get a file of a few hundreds
    GB containing the parameters of an LLM. The model is unable to precisely retrieve
    the initial knowledge, but still produces a pertinent output most of the times.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dbaf98a73c1b3f342cb9792dc0587ca1.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author and DALL-E 3 (inspired by Karpathy’s [llmintro](https://drive.google.com/file/d/1pxx_ZI7O-Nwl7ZLNk5hI3WzAsTLwvNU7/view))
  prefs: []
  type: TYPE_NORMAL
- en: The mystery of the LLMs does not reside in the technical architecture or the
    complexity of their computations. If the…
  prefs: []
  type: TYPE_NORMAL
