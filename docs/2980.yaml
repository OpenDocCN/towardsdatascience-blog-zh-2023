- en: Retrieval-Augmented Generation (RAG)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/add-your-own-data-to-an-llm-using-retrieval-augmented-generation-rag-b1958bf56a5a?source=collection_archive---------1-----------------------#2023-09-29](https://towardsdatascience.com/add-your-own-data-to-an-llm-using-retrieval-augmented-generation-rag-b1958bf56a5a?source=collection_archive---------1-----------------------#2023-09-29)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn how to add your own proprietary data to a pre-trained LLM using a prompt-based
    technique called Retrieval-Augmented Generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bea_684?source=post_page-----b1958bf56a5a--------------------------------)[![Beatriz
    Stollnitz](../Images/63a2a7daeca6d93e26b3ac0556c42aa1.png)](https://medium.com/@bea_684?source=post_page-----b1958bf56a5a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b1958bf56a5a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b1958bf56a5a--------------------------------)
    [Beatriz Stollnitz](https://medium.com/@bea_684?source=post_page-----b1958bf56a5a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1c8863892480&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadd-your-own-data-to-an-llm-using-retrieval-augmented-generation-rag-b1958bf56a5a&user=Beatriz+Stollnitz&userId=1c8863892480&source=post_page-1c8863892480----b1958bf56a5a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b1958bf56a5a--------------------------------)
    ·21 min read·Sep 29, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb1958bf56a5a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadd-your-own-data-to-an-llm-using-retrieval-augmented-generation-rag-b1958bf56a5a&user=Beatriz+Stollnitz&userId=1c8863892480&source=-----b1958bf56a5a---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb1958bf56a5a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadd-your-own-data-to-an-llm-using-retrieval-augmented-generation-rag-b1958bf56a5a&source=-----b1958bf56a5a---------------------bookmark_footer-----------)![](../Images/cd0c3b0f21d72762c8a25dcf3ac3c039.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Joshua Sortino](https://unsplash.com/@sortino?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '**Introduction**'
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) know a lot about the world, but they don’t know
    everything. Since training these models takes a long time, the data they were
    last trained on can be pretty old. And although LLMs know about general-purpose
    facts available on the internet, they don’t know about your proprietary data,
    which is often the data you need in your AI-based application. So it’s no surprise
    that extending LLMs with new data has been a considerable area of focus lately,
    both in academia and in the industry.
  prefs: []
  type: TYPE_NORMAL
- en: Before this new era of large language models, we would often extend models with
    new data by simply fine-tuning them. But now that our models are much larger and
    have been trained with much more data, fine-tuning is only appropriate for a few
    scenarios. Fine-tuning performs particularly well when we want to make our LLM
    communicate in a different style or tone. One great example of fine-tuning is
    OpenAI’s adaptation of their older completion-style GPT-3.5 models into their
    newer chat-style GPT-3.5-turbo (ChatGPT) models. Their completion-style models,
    if given the input “Can you tell me about…
  prefs: []
  type: TYPE_NORMAL
