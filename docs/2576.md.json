["```py\nimport pandas as pd\nimport numpy as np\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.ensemble import GradientBoostingClassifier, IsolationForest\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n\n# read in data\nairlines2022 = pd.read_csv('myPath/Combined_Flights_2022.csv')\nprint(airlines2022.shape)\n# (4078318, 61)\n\n# subset by my target departure city\nairlines2022PIT = airlines2022[airlines2022.Origin == 'PIT']\nprint(airlines2022PIT.shape)\n# (24078, 61)\n\n# combine cancellations, diversions, and 30+ minute delays into one Bad Flight outcome\nairlines2022PIT = airlines2022PIT.assign(arrDel30 = airlines2022PIT['ArrDelayMinutes'] >= 30)\nairlines2022PIT = (airlines2022PIT\n                   .assign(badFlight = 1 * (airlines2022PIT.Cancelled \n                                            + airlines2022PIT.Diverted\n                                            + airlines2022PIT.arrDel30))\n                  )\nprint(airlines2022PIT.badFlight.mean())\n# 0.15873411412908048\n```", "```py\n# categorize columns by feature type\ntoFactor = ['Airline', 'Dest', 'Month', 'DayOfWeek'\n            , 'Marketing_Airline_Network', 'Operating_Airline']\ntoScale = ['Distance']\n\n# drop fields that don't look helpful for prediction\nairlines2022PIT = airlines2022PIT[toFactor + toScale + ['badFlight']]\nprint(airlines2022PIT.shape)\n# (24078, 8)\n\n# split original training data into training and validation sets\ntrain, test = train_test_split(airlines2022PIT\n                               , test_size = 0.2\n                               , random_state = 412)\nprint(train.shape)\n# (19262, 8)\nprint(test.shape)\n# (4816, 8)\n\n# manually scale distance feature\nmn = train.Distance.min()\nrng = train.Distance.max() - train.Distance.min()\ntrain = train.assign(Distance_sc = (train.Distance - mn) / rng)\ntest = test.assign(Distance_sc = (test.Distance - mn) / rng)\ntrain.drop('Distance', axis = 1, inplace = True)\ntest.drop('Distance', axis = 1, inplace = True)\n\n# make an encoder\nenc = make_column_transformer(\n    (OneHotEncoder(min_frequency = 0.025, handle_unknown = 'ignore'), toFactor)\n    , remainder = 'passthrough'\n    , sparse_threshold = 0)\n\n# apply it to the training dataset\ntrain_enc = enc.fit_transform(train)\n\n# convert it back to a Pandas dataframe for ease of use\ntrain_enc_pd = pd.DataFrame(train_enc, columns = enc.get_feature_names_out())\n\n# encode the test set in the same way\ntest_enc = enc.transform(test)\ntest_enc_pd = pd.DataFrame(test_enc, columns = enc.get_feature_names_out())\n```", "```py\n# feature selection - drop low importance terms|\nlowimp = ['onehotencoder__Airline_Delta Air Lines Inc.'\n          , 'onehotencoder__Dest_IAD'\n          , 'onehotencoder__Operating_Airline_AA'\n          , 'onehotencoder__Airline_American Airlines Inc.'\n          , 'onehotencoder__Airline_Comair Inc.'\n          , 'onehotencoder__Airline_Southwest Airlines Co.'\n          , 'onehotencoder__Airline_Spirit Air Lines'\n          , 'onehotencoder__Airline_United Air Lines Inc.'\n          , 'onehotencoder__Airline_infrequent_sklearn'\n          , 'onehotencoder__Dest_ATL'\n          , 'onehotencoder__Dest_BOS'\n          , 'onehotencoder__Dest_BWI'\n          , 'onehotencoder__Dest_CLT'\n          , 'onehotencoder__Dest_DCA'\n          , 'onehotencoder__Dest_DEN'\n          , 'onehotencoder__Dest_DFW'\n          , 'onehotencoder__Dest_DTW'\n          , 'onehotencoder__Dest_JFK'\n          , 'onehotencoder__Dest_MDW'\n          , 'onehotencoder__Dest_MSP'\n          , 'onehotencoder__Dest_ORD'\n          , 'onehotencoder__Dest_PHL'\n          , 'onehotencoder__Dest_infrequent_sklearn'\n          , 'onehotencoder__Marketing_Airline_Network_AA'\n          , 'onehotencoder__Marketing_Airline_Network_DL'\n          , 'onehotencoder__Marketing_Airline_Network_G4'\n          , 'onehotencoder__Marketing_Airline_Network_NK'\n          , 'onehotencoder__Marketing_Airline_Network_WN'\n          , 'onehotencoder__Marketing_Airline_Network_infrequent_sklearn'\n          , 'onehotencoder__Operating_Airline_9E'\n          , 'onehotencoder__Operating_Airline_DL'\n          , 'onehotencoder__Operating_Airline_NK'\n          , 'onehotencoder__Operating_Airline_OH'\n          , 'onehotencoder__Operating_Airline_OO'\n          , 'onehotencoder__Operating_Airline_UA'\n          , 'onehotencoder__Operating_Airline_WN'\n          , 'onehotencoder__Operating_Airline_infrequent_sklearn']\nlowimp = [x for x in lowimp if x in train_enc_pd.columns]\ntrain_enc_pd = train_enc_pd.drop(lowimp, axis = 1)\ntest_enc_pd = test_enc_pd.drop(lowimp, axis = 1)\n\n# separate potential predictors from outcome\ntrain_x = train_enc_pd.drop('remainder__badFlight', axis = 1); train_y = train_enc_pd['remainder__badFlight']\ntest_x = test_enc_pd.drop('remainder__badFlight', axis = 1); test_y = test_enc_pd['remainder__badFlight']\nprint(train_x.shape)\nprint(test_x.shape)\n\n# (19262, 25)\n# (4816, 25)\n\n# build model\ngbt = GradientBoostingClassifier(learning_rate = 0.1\n                                 , n_estimators = 100\n                                 , subsample = 0.7\n                                 , max_depth = 5\n                                 , random_state = 412)\n\n# fit it to the training data\ngbt.fit(train_x, train_y)\n\n# calculate the probability scores for each test observation\ngbtPreds1Test = gbt.predict_proba(test_x)[:,1]\n\n# use a custom threshold to convert these to binary scores\ngbtThresh = np.percentile(gbtPreds1Test, 100 * (1 - obsRate))\ngbtPredsCTest = 1 * (gbtPreds1Test > gbtThresh)\n\n# check accuracy of model\nacc = accuracy_score(gbtPredsCTest, test_y)\nprint(acc)\n# 0.7742940199335548\n\n# check lift\ntopDecile = test_y[gbtPreds1Test > np.percentile(gbtPreds1Test, 90)]\nlift = sum(topDecile) / len(topDecile) / test_y.mean()\nprint(lift)\n# 1.8591454794381614\n\n# view confusion matrix\ncm = (confusion_matrix(gbtPredsCTest, test_y) / len(test_y)).round(2)\nprint(cm)\n# [[0.73 0.11]\n# [0.12 0.04]]\n```", "```py\n# build an isolation forest\nisf = IsolationForest(n_estimators = 800\n                      , max_samples = 0.15\n                      , max_features = 0.1\n                      , random_state = 412)\n\n# fit it to the same training data\nisf.fit(train_x)\n\n# calculate the anomaly score of each test observation (lower values are more anomalous)\nisfPreds1Test = isf.score_samples(test_x)\n\n# use a custom threshold to convert these to binary scores\nisfThresh = np.percentile(isfPreds1Test, 100 * (obsRate / 2))\nisfPredsCTest = 1 * (isfPreds1Test < isfThresh)\n```", "```py\n# combine predictions, anomaly scores, and survival data\ncomb = pd.concat([pd.Series(gbtPredsCTest), pd.Series(isfPredsCTest), pd.Series(test_y)]\n                 , keys = ['Prediction', 'Outlier', 'badFlight']\n                 , axis = 1)\ncomb = comb.assign(Correct = 1 * (comb.badFlight == comb.Prediction))\n\nprint(comb.mean())\n#Prediction    0.159676\n#Outlier       0.079942\n#badFlight     0.153239\n#Correct       0.774294\n#dtype: float64\n\n# better accuracy in majority class\nprint(comb.groupby('badFlight').agg(accuracy = ('Correct', 'mean')))\n #          accuracy\n#badFlight          \n#0.0        0.862923\n#1.0        0.284553\n\n# more bad flights among outliers\nprint(comb.groupby('Outlier').agg(badFlightRate = ('badFlight', 'mean')))\n\n#        badFlightRate\n#Outlier               \n#0             0.148951\n#1             0.202597\n```", "```py\n# build a second model with outlier labels as input features\nisfPreds1Train = isf.score_samples(train_x)\nisfPredsCTrain = 1 * (isfPreds1Train < isfThresh)\n\nmn = isfPreds1Train.min(); rng = isfPreds1Train.max() - isfPreds1Train.min()\nisfPreds1SCTrain = (isfPreds1Train - mn) / rng\nisfPreds1SCTest = (isfPreds1Test - mn) / rng\n\ntrain_2_x = (pd.concat([train_x, pd.Series(isfPredsCTrain)]\n                       , axis = 1)\n             .rename(columns = {0:'isfPreds1'}))\ntest_2_x = (pd.concat([test_x, pd.Series(isfPredsCTest)]\n                      , axis = 1)\n            .rename(columns = {0:'isfPreds1'}))\n\n# build model\ngbt2 = GradientBoostingClassifier(learning_rate = 0.1\n                                  , n_estimators = 100\n                                  , subsample = 0.7\n                                  , max_depth = 5\n                                  , random_state = 412)\n\n# fit it to the training data\ngbt2.fit(train_2_x, train_y)\n\n# calculate the probability scores for each test observation\ngbt2Preds1Test = gbt2.predict_proba(test_2_x)[:,1]\n\n# use a custom threshold to convert these to binary scores\ngbtThresh = np.percentile(gbt2Preds1Test, 100 * (1 - obsRate))\ngbt2PredsCTest = 1 * (gbt2Preds1Test > gbtThresh)\n\n# check accuracy of model\nacc = accuracy_score(gbt2PredsCTest, test_y)\nprint(acc)\n#0.7796926910299004\n\n# check lift\ntopDecile = test_y[gbt2Preds1Test > np.percentile(gbt2Preds1Test, 90)]\nlift = sum(topDecile) / len(topDecile) / test_y.mean()\nprint(lift)\n#1.9138477764819217\n\n# view confusion matrix\ncm = (confusion_matrix(gbt2PredsCTest, test_y) / len(test_y)).round(2)\nprint(cm)\n#[[0.73 0.11]\n# [0.11 0.05]]\n```"]