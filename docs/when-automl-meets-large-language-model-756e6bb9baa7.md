# å½“ AutoML é‡ä¸Šå¤§å‹è¯­è¨€æ¨¡å‹

> åŸæ–‡ï¼š[`towardsdatascience.com/when-automl-meets-large-language-model-756e6bb9baa7`](https://towardsdatascience.com/when-automl-meets-large-language-model-756e6bb9baa7)

## åˆ©ç”¨ LLMs çš„åŠ›é‡æ¥æŒ‡å¯¼è¶…å‚æ•°æœç´¢

[](https://shuaiguo.medium.com/?source=post_page-----756e6bb9baa7--------------------------------)![Shuai Guo](https://shuaiguo.medium.com/?source=post_page-----756e6bb9baa7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----756e6bb9baa7--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page-----756e6bb9baa7--------------------------------) [Shuai Guo](https://shuaiguo.medium.com/?source=post_page-----756e6bb9baa7--------------------------------)

Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----756e6bb9baa7--------------------------------) Â·23 åˆ†é’Ÿé˜…è¯»Â·2023 å¹´ 10 æœˆ 5 æ—¥

--

![](img/cf26341128d5b5fbb82f432835e76c2d.png)

[çº¦ç¿°Â·æ–½è¯ºå¸ƒé‡Œå¥‡](https://unsplash.com/@johnschno?utm_source=medium&utm_medium=referral)æ‹æ‘„äº[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

*è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ *ï¼Œç®€ç§° AutoMLï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ æµç¨‹ä¸­çš„å„ä¸ªæ­¥éª¤ã€‚å…¶ä¸­ä¸€ä¸ªå…³é”®æ–¹é¢æ˜¯**è¶…å‚æ•°è°ƒä¼˜**ã€‚è¶…å‚æ•°æ˜¯æŒ‡æ§åˆ¶æœºå™¨å­¦ä¹ ç®—æ³•ç»“æ„å’Œè¡Œä¸ºçš„å‚æ•°ï¼ˆä¾‹å¦‚ï¼Œç¥ç»ç½‘ç»œæ¨¡å‹ä¸­çš„å±‚æ•°ï¼‰ï¼Œè¿™äº›å‚æ•°çš„å€¼åœ¨è®­ç»ƒä¹‹å‰è®¾å®šï¼Œè€Œä¸æ˜¯ä»æ•°æ®ä¸­å­¦ä¹ ï¼Œè¿™ä¸å…¶ä»–æœºå™¨å­¦ä¹ å‚æ•°ï¼ˆä¾‹å¦‚ç¥ç»ç½‘ç»œå±‚çš„æƒé‡å’Œåç½®ï¼‰ä¸åŒã€‚

å½“å‰åœ¨ AutoML ä¸­ç”¨äºè¶…å‚æ•°è°ƒä¼˜çš„åšæ³• heavily rely on sophisticated algorithms (such as [è´å¶æ–¯ä¼˜åŒ–](https://medium.com/towards-data-science/an-introduction-to-surrogate-optimization-intuition-illustration-case-study-and-the-code-5d9364aed51b)) to automatically identify the optimal combination of the hyperparameters that yields the best model performance.

ä»çº¯ç®—æ³•çš„è§’åº¦è§£å†³è¶…å‚æ•°è°ƒä¼˜é—®é¢˜è™½ç„¶æœ‰æ•ˆï¼Œä½†è¿˜æœ‰å¦ä¸€ä¸ªé‡è¦çš„ä¿¡æ¯å¯ä»¥è¡¥å……ç®—æ³•æœç´¢ç­–ç•¥ï¼š**äººç±»ä¸“ä¸šçŸ¥è¯†**ã€‚

èµ„æ·±æ•°æ®ç§‘å­¦å®¶å‡­å€Ÿå¤šå¹´çš„ç»éªŒå’Œå¯¹æœºå™¨å­¦ä¹ ç®—æ³•çš„æ·±åˆ»ç†è§£ï¼Œé€šå¸¸èƒ½å¤Ÿç›´è§‚åœ°çŸ¥é“ä»å“ªé‡Œå¼€å§‹æœç´¢ï¼Œå“ªäº›æœç´¢ç©ºé—´çš„åŒºåŸŸå¯èƒ½æ›´æœ‰å‰æ™¯ï¼Œæˆ–ä½•æ—¶ç¼©å°æˆ–æ‰©å¤§å¯èƒ½æ€§ã€‚

å› æ­¤ï¼Œè¿™ç»™äº†æˆ‘ä»¬ä¸€ä¸ªéå¸¸æœ‰è¶£çš„æƒ³æ³•ï¼šæˆ‘ä»¬èƒ½å¦è®¾è®¡ä¸€ä¸ª**å¯æ‰©å±•çš„ä¸“å®¶æŒ‡å¯¼æœç´¢ç­–ç•¥**ï¼Œæ—¢åˆ©ç”¨ä¸“å®¶æä¾›çš„ç»†è‡´è§è§£ï¼Œåˆç»“åˆ AutoML ç®—æ³•æä¾›çš„æœç´¢æ•ˆç‡ï¼Ÿ

è¿™æ˜¯**å¤§è¯­è¨€æ¨¡å‹**ï¼ˆLLMï¼‰ï¼Œä¾‹å¦‚ GPT-4ï¼Œå¯ä»¥å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚åœ¨å®ƒä»¬çš„å¤§é‡è®­ç»ƒæ•°æ®ä¸­ï¼Œæœ‰ä¸€éƒ¨åˆ†æ–‡æœ¬ä¸“é—¨ç”¨äºè§£é‡Šå’Œè®¨è®ºæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰çš„æœ€ä½³å®è·µã€‚ç”±äºè¿™ä¸€ç‚¹ï¼ŒLLM èƒ½å¤Ÿå†…åŒ–å¤§é‡çš„ ML ä¸“ä¸šçŸ¥è¯†ï¼Œå¹¶è·å¾—å¤§é‡çš„é›†ä½“ ML æ™ºæ…§ã€‚è¿™ä½¿å¾— LLM ä½œä¸ºæ½œåœ¨çš„çŸ¥è¯†æ¸Šåšçš„ ML ä¸“å®¶ï¼Œå¯ä»¥ä¸ç°æœ‰çš„ AutoML å·¥å…·è¿›è¡Œäº’åŠ¨ï¼Œå…±åŒè¿›è¡Œä¸“å®¶æŒ‡å¯¼çš„è¶…å‚æ•°è°ƒæ•´ã€‚

![](img/6ddfb39d0a9b05fc40b3bbe3f19050ec.png)

LLM æŒ‡å¯¼çš„è¶…å‚æ•°è°ƒæ•´ç¤ºä¾‹ã€‚ï¼ˆå›¾åƒä½œè€…æä¾›ï¼‰

åœ¨è¿™ç¯‡åšå®¢æ–‡ç« ä¸­ï¼Œè®©æˆ‘ä»¬å°è¯•è¿™ä¸ªæƒ³æ³•ï¼Œå¹¶å®ç°ä¸€ä¸ª**ç”± LLM æŒ‡å¯¼çš„è¶…å‚æ•°è°ƒæ•´å·¥ä½œæµ**ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†å¼€å‘ä¸€ä¸ªå°† LLM æŒ‡å¯¼ä¸ç®€å•éšæœºæœç´¢ç›¸ç»“åˆçš„å·¥ä½œæµï¼Œç„¶åå°†å…¶è°ƒæ•´ç»“æœä¸ä¸€ç§åä¸º[FLAML](https://microsoft.github.io/FLAML/)ï¼ˆå¾®è½¯ç ”ç©¶é™¢å¼€å‘çš„æœ€å…ˆè¿›çš„ç®—æ³•é©±åŠ¨çš„ AutoML å·¥å…·ï¼‰çš„ç°æˆå·¥å…·è¿›è¡Œæ¯”è¾ƒã€‚è¿›è¡Œè¿™ç§æ¯”è¾ƒçš„ç†ç”±æ˜¯è¯„ä¼°å³ä½¿ä¸ç®€å•çš„æœç´¢ç®—æ³•é…å¯¹ï¼Œå€ŸåŠ© ML é¢†åŸŸä¸“ä¸šçŸ¥è¯†æ˜¯å¦çœŸçš„èƒ½å¸¦æ¥ä»·å€¼ã€‚

è¿™ä¸ªæƒ³æ³•ä¸ä½ æœ‰å…±é¸£å—ï¼Ÿé‚£å°±å¼€å§‹å§ï¼

> [æ³¨]: æœ¬åšå®¢ä¸­å±•ç¤ºçš„æ‰€æœ‰æç¤ºå‡ç”± ChatGPT (GPT-4) ç”Ÿæˆå¹¶ä¼˜åŒ–ã€‚è¿™æ˜¯å¿…è¦çš„ï¼Œå› ä¸ºå®ƒç¡®ä¿äº†æç¤ºçš„è´¨é‡ï¼Œå¹¶ä¸”æœ‰åŠ©äºé¿å…ç¹ççš„æ‰‹åŠ¨æç¤ºå·¥ç¨‹ã€‚
> 
> è¿™æ˜¯æˆ‘å…³äº LLM é¡¹ç›®çš„ç³»åˆ—åšå®¢ä¸­çš„ç¬¬å››ç¯‡ã€‚ç¬¬ä¸€ç¯‡æ˜¯ æ„å»º AI é©±åŠ¨çš„è¯­è¨€å­¦ä¹ åº”ç”¨ï¼Œç¬¬äºŒç¯‡æ˜¯ å¼€å‘ç”¨äºç ”ç©¶è®ºæ–‡æ‘˜è¦çš„è‡ªä¸»åŒèŠå¤©æœºå™¨äººç³»ç»Ÿï¼Œç¬¬ä¸‰ç¯‡æ˜¯ é€šè¿‡ç°å®ç”Ÿæ´»æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®ç§‘å­¦ä¸­çš„é—®é¢˜è§£å†³æŠ€èƒ½ã€‚æ¬¢è¿æŸ¥çœ‹ï¼

## ç›®å½•

**Â·** **1\. æ¡ˆä¾‹ç ”ç©¶**

âˆ˜ 1.1 æ•°æ®é›†æè¿°

âˆ˜ 1.2 æ¨¡å‹æè¿°

**Â·** **2\. å·¥ä½œæµè®¾è®¡** **Â·** **3\. é…ç½®èŠå¤©æœºå™¨äºº** **Â·** **4\. å»ºè®®ä¼˜åŒ–åº¦é‡** **Â·** **5\. å®šä¹‰åˆå§‹æœç´¢ç©ºé—´** **Â·** **6\. ç²¾ç‚¼æœç´¢ç©ºé—´** **Â·** **7\. è°ƒæ•´æ—¥å¿—åˆ†æ**

âˆ˜ 7.1 éšæœºæœç´¢ä¸è¿ç»­å‡åŠ

âˆ˜ 7.2 æ—¥å¿—åˆ†æ

**Â·** **8\. æ¡ˆä¾‹ç ”ç©¶**

âˆ˜ 8.1 ç¡®å®šåº¦é‡

âˆ˜ 8.2 ç¬¬ä¸€æ¬¡æœç´¢è¿­ä»£

âˆ˜ 8.3 ç¬¬äºŒæ¬¡æœç´¢è¿­ä»£

âˆ˜ 8.4 ç¬¬ä¸‰æ¬¡æœç´¢è¿­ä»£

âˆ˜ 8.5 æµ‹è¯•

**Â·** **9\. ä¸ç°æˆ AutoML å·¥å…·çš„æ¯”è¾ƒ** **Â·** **10\. ç»“è®º**

# 1\. æ¡ˆä¾‹ç ”ç©¶

ä¸ºäº†å°†æˆ‘ä»¬çš„è®¨è®ºå…·ä½“åŒ–ï¼Œè®©æˆ‘ä»¬é¦–å…ˆä»‹ç»ä¸€ä¸‹æˆ‘ä»¬å°†è¦è°ƒæŸ¥çš„æ¡ˆä¾‹ç ”ç©¶ã€‚

åœ¨æœ¬åšå®¢ä¸­ï¼Œæˆ‘ä»¬å°†æŸ¥çœ‹ä¸€ä¸ª**äºŒåˆ†ç±»**é—®é¢˜çš„è¶…å‚æ•°è°ƒä¼˜ä»»åŠ¡ã€‚æ›´å…·ä½“åœ°ï¼Œæˆ‘ä»¬å°†ç ”ç©¶ä¸€ä¸ªåä¸º[**NSL-KDD**](https://www.unb.ca/cic/datasets/nsl.html)çš„ç½‘ç»œå®‰å…¨æ•°æ®é›†ï¼Œå¹¶è¯†åˆ«[**XGBoost**](https://xgboost.readthedocs.io/en/stable/)æ¨¡å‹çš„æœ€ä½³è¶…å‚æ•°ï¼Œä»¥ä¾¿è®­ç»ƒå‡ºçš„æ¨¡å‹å¯ä»¥å‡†ç¡®åŒºåˆ†è‰¯æ€§å’Œæ”»å‡»æ´»åŠ¨ã€‚

## 1.1 æ•°æ®é›†æè¿°

NSL-KDD æ•°æ®é›†æ˜¯ç½‘ç»œå…¥ä¾µæ£€æµ‹é¢†åŸŸå¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†ã€‚å®Œæ•´çš„æ•°æ®é›†åŒ…å«å››ç§æ”»å‡»ç±»åˆ«ï¼Œå³ dosï¼ˆæ‹’ç»æœåŠ¡ï¼‰ã€r2lï¼ˆè¿œç¨‹æœºå™¨çš„æœªç»æˆæƒè®¿é—®ï¼‰ã€u2rï¼ˆæƒé™æå‡å°è¯•ï¼‰ä»¥åŠ probeï¼ˆæš´åŠ›æ¢æµ‹æ”»å‡»ï¼‰ã€‚åœ¨æˆ‘ä»¬å½“å‰çš„æ¡ˆä¾‹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶ä¸€ä¸ª*äºŒåˆ†ç±»*é—®é¢˜ï¼šæˆ‘ä»¬åªè€ƒè™‘â€œè‰¯æ€§â€æˆ–â€œæ¢æµ‹â€æ€§è´¨çš„æ•°æ®æ ·æœ¬ï¼Œå¹¶è®­ç»ƒä¸€ä¸ªèƒ½å¤ŸåŒºåˆ†è¿™ä¸¤ç§çŠ¶æ€çš„åˆ†ç±»å™¨ã€‚

NSL-KDD æ•°æ®é›†åŒ…å« 40 ä¸ªç‰¹å¾ï¼ˆä¾‹å¦‚ï¼Œè¿æ¥é•¿åº¦ã€åè®®ç±»å‹ã€ä¼ è¾“æ•°æ®å­—èŠ‚ç­‰ï¼‰ï¼Œè¿™äº›ç‰¹å¾æºè‡ªåŸå§‹ç½‘ç»œæµé‡æ•°æ®ï¼Œæ•è·äº†å•ä¸ªç½‘ç»œè¿æ¥çš„å„ç§ç‰¹å¾ã€‚æ•°æ®é›†å·²ç»é¢„å…ˆåˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚ä¸‹è¡¨æ˜¾ç¤ºäº†ä¸¤ä¸ªé›†åˆä¸­çš„æ ·æœ¬æ•°é‡ï¼š

![](img/147d68e05fb8230c6c1f69a5970e892d.png)

è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­æ ·æœ¬çš„æ•°é‡ã€‚ï¼ˆå›¾ç‰‡ä½œè€…æä¾›ï¼‰

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å½“å‰çš„æ•°æ®é›†æ˜¯ä¸å¹³è¡¡çš„ã€‚è¿™åœ¨ç½‘ç»œå®‰å…¨åº”ç”¨ä¸­å¾ˆå¸¸è§ï¼Œå› ä¸ºå¯ç”¨çš„æ”»å‡»æ ·æœ¬æ•°é‡é€šå¸¸è¿œå°äºè‰¯æ€§æ ·æœ¬çš„æ•°é‡ã€‚

ä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/ShuaiGuo16/LLM-guided-AutoML/tree/main/dataset)æ‰¾åˆ°é¢„å¤„ç†çš„æ•°æ®é›†ã€‚

## 1.2 æ¨¡å‹æè¿°

å°½ç®¡å®Œæ•´çš„ AutoML åŒ…æ‹¬æ¨¡å‹é€‰æ‹©ï¼Œä½†åœ¨å½“å‰çš„æ¡ˆä¾‹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å°†èŒƒå›´é™åˆ¶åœ¨ä»…ä¼˜åŒ– XGBoost æ¨¡å‹ã€‚XGBoost æ¨¡å‹ä»¥å…¶å¤šæ‰å¤šè‰ºçš„æ€§èƒ½è€Œé—»åï¼Œä½†å‰ææ˜¯ä½¿ç”¨äº†æ­£ç¡®çš„æ¨¡å‹è¶…å‚æ•°ã€‚ä¸å¹¸çš„æ˜¯ï¼Œç”±äº XGBoost çš„è¶…å‚æ•°æ•°é‡ä¼—å¤šï¼Œç¡®å®šç»™å®šæ•°æ®é›†çš„æœ€ä½³è¶…å‚æ•°ç»„åˆå¹¶ä¸å®¹æ˜“ã€‚è¿™æ­£æ˜¯ AutoML å¯ä»¥å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚

æˆ‘ä»¬è€ƒè™‘è°ƒæ•´ä»¥ä¸‹ XGBoost è¶…å‚æ•°ï¼š

> n_estimators, max_depth, min_child_weight, gamma, scale_pos_weight, learning_rate, subsample, colsample_bylevel, colsample_bytree, reg_alpha, å’Œ reg_lambdaã€‚

å…³äºä¸Šè¿°è¶…å‚æ•°çš„è¯¦ç»†æè¿°ï¼Œè¯·å‚é˜…[å®˜æ–¹æ–‡æ¡£](https://xgboost.readthedocs.io/en/stable/)ã€‚

# 2\. å·¥ä½œæµè®¾è®¡

è¦å®ç° LLM æŒ‡å¯¼çš„è¶…å‚æ•°è°ƒä¼˜ï¼Œæˆ‘ä»¬éœ€è¦å›ç­”ä¸¤ä¸ªé—®é¢˜ï¼šæˆ‘ä»¬åº”è¯¥å¦‚ä½•å°† LLM çš„æœºå™¨å­¦ä¹ ä¸“ä¸šçŸ¥è¯†èå…¥è°ƒä¼˜è¿‡ç¨‹ï¼Ÿæˆ‘ä»¬åº”è¯¥å¦‚ä½•è®© LLM ä¸è°ƒä¼˜å·¥å…·äº’åŠ¨ï¼Ÿ

1ï¸âƒ£ å¦‚ä½•å°† LLM çš„æœºå™¨å­¦ä¹ ä¸“ä¸šçŸ¥è¯†èå…¥è°ƒæ•´è¿‡ç¨‹ä¸­ï¼Ÿ

å¥½å§ï¼Œåœ¨è°ƒæ•´è¿‡ç¨‹ä¸­è‡³å°‘æœ‰ä¸‰ä¸ªåœ°æ–¹ï¼ŒLLM çš„æœºå™¨å­¦ä¹ ä¸“ä¸šçŸ¥è¯†å¯ä»¥æä¾›æŒ‡å¯¼ï¼š

+   **å»ºè®®ä¼˜åŒ–æŒ‡æ ‡**ï¼šè¶…å‚æ•°è°ƒæ•´é€šå¸¸éœ€è¦ä¸€ä¸ªæŒ‡æ ‡æ¥å®šä¹‰åœ¨å„ç§ç«äº‰çš„è¶…å‚æ•°ç»„åˆä¸­ä»€ä¹ˆè¢«è®¤ä¸ºæ˜¯â€œæœ€ä¼˜â€çš„ã€‚è¿™ä¸ºåŸºç¡€çš„ä¼˜åŒ–ç®—æ³•ï¼ˆä¾‹å¦‚ï¼Œè´å¶æ–¯ä¼˜åŒ–ï¼‰è®¾å®šäº†ç›®æ ‡ã€‚LLMs å¯ä»¥æä¾›å…³äºå“ªäº›æŒ‡æ ‡æ›´é€‚åˆç‰¹å®šç±»å‹é—®é¢˜å’Œæ•°æ®é›†ç‰¹å¾çš„è§è§£ï¼Œå¹¶å¯èƒ½æä¾›å¯¹å€™é€‰æŒ‡æ ‡çš„ä¼˜ç¼ºç‚¹è§£é‡Šã€‚

+   **å»ºè®®åˆå§‹æœç´¢ç©ºé—´**ï¼šç”±äºå¤§å¤šæ•°è¶…å‚æ•°è°ƒæ•´ä»»åŠ¡æ˜¯ä»¥è¿­ä»£æ–¹å¼è¿›è¡Œçš„ï¼Œé€šå¸¸éœ€è¦é…ç½®åˆå§‹æœç´¢ç©ºé—´ä»¥ä¸ºä¼˜åŒ–è¿‡ç¨‹å¥ å®šåŸºç¡€ã€‚åŸºäºå…¶å­¦ä¹ çš„æœºå™¨å­¦ä¹ æœ€ä½³å®è·µï¼ŒLLMs å¯ä»¥æ¨èå¯¹ç ”ç©¶æ•°æ®é›†ç‰¹å¾å’Œé€‰å®šæœºå™¨å­¦ä¹ æ¨¡å‹æœ‰æ„ä¹‰çš„è¶…å‚æ•°èŒƒå›´ã€‚è¿™å¯ä»¥æ½œåœ¨åœ°å‡å°‘ä¸å¿…è¦çš„æ¢ç´¢ï¼Œä»è€ŒèŠ‚çœå¤§é‡è®¡ç®—æ—¶é—´ã€‚

+   **å»ºè®®æœç´¢ç©ºé—´çš„ç»†åŒ–**ï¼šéšç€è°ƒæ•´è¿‡ç¨‹çš„æ¨è¿›ï¼Œé€šå¸¸éœ€è¦å¯¹é…ç½®çš„æœç´¢ç©ºé—´è¿›è¡Œç»†åŒ–ã€‚ç»†åŒ–å¯ä»¥æœä¸¤ä¸ªæ–¹å‘è¿›è¡Œï¼Œå³ç¼©å°æŸäº›è¶…å‚æ•°çš„èŒƒå›´åˆ°æ˜¾ç¤ºå‡ºå‰æ™¯çš„åŒºåŸŸï¼Œæˆ–æ‰©å±•æŸäº›è¶…å‚æ•°çš„èŒƒå›´ä»¥æ¢ç´¢æ–°çš„é¢†åŸŸã€‚é€šè¿‡åˆ†æå‰å‡ è½®çš„ä¼˜åŒ–æ—¥å¿—ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯ä»¥è‡ªåŠ¨æå‡ºæ–°çš„ç»†åŒ–å»ºè®®ï¼Œå°†è°ƒæ•´è¿‡ç¨‹å¼•å¯¼å‘æ›´æœ‰å‰æ™¯çš„ç»“æœã€‚

2ï¸âƒ£ å¦‚ä½•è®© LLM ä¸è°ƒæ•´å·¥å…·è¿›è¡Œäº’åŠ¨ï¼Ÿ

ç†æƒ³æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥å°†éšæœºæœç´¢å·¥å…·å°è£…ä¸º APIï¼Œå¹¶å®ç°ä¸€ä¸ªåŸºäº LLM çš„**ä»£ç†**ï¼Œè¯¥ä»£ç†å¯ä»¥è®¿é—®è¿™ä¸ª API æ¥è¿›è¡Œè°ƒæ•´ã€‚ç„¶è€Œï¼Œç”±äºæ—¶é—´æœ‰é™ï¼Œæˆ‘æœªèƒ½é…ç½®ä¸€ä¸ªèƒ½å¤Ÿå¯é æ‰§è¡Œä¸Šè¿°è¿­ä»£è°ƒæ•´è¿‡ç¨‹çš„å·¥ä½œä»£ç†ï¼šæœ‰æ—¶ä»£ç†ç”±äºè¾“å…¥æ¨¡å¼ä¸æ­£ç¡®è€Œæ— æ³•æ­£ç¡®ä½¿ç”¨å·¥å…·ï¼›å…¶ä»–æ—¶å€™ä»£ç†åˆ™å®Œå…¨åç¦»äº†ä»»åŠ¡ã€‚

ä½œä¸ºæ›¿ä»£ï¼Œæˆ‘å®ç°äº†ä¸€ä¸ªç®€å•çš„åŸºäºèŠå¤©æœºå™¨äººçš„å·¥ä½œæµï¼Œå…¶ä¸­åŒ…å«ä»¥ä¸‹ä¸¤ä¸ªç»„ä»¶ï¼š

+   å…·æœ‰è®°å¿†çš„èŠå¤©æœºå™¨äººã€‚åœ¨è¿™é‡Œï¼Œè®°å¿†å¾ˆé‡è¦ï¼Œå› ä¸ºèŠå¤©æœºå™¨äººéœ€è¦å›å¿†ä¹‹å‰å»ºè®®çš„æœç´¢ç©ºé—´ã€‚

+   ä¸‰ä¸ªæç¤ºåˆ†åˆ«å¯¹åº”å»ºè®®ä¼˜åŒ–æŒ‡æ ‡ã€å»ºè®®åˆå§‹æœç´¢ç©ºé—´å’Œå»ºè®®æœç´¢ç©ºé—´çš„ç»†åŒ–ã€‚

ä¸ºäº†å¯åŠ¨å·¥ä½œæµï¼Œé¦–å…ˆæç¤ºèŠå¤©æœºå™¨äººæ ¹æ®é—®é¢˜èƒŒæ™¯å’Œæ•°æ®é›†ç‰¹å¾å»ºè®®ä¸€ä¸ªåˆé€‚çš„ä¼˜åŒ–æŒ‡æ ‡ã€‚ç„¶åè§£æèŠå¤©æœºå™¨äººçš„å“åº”ï¼Œæå–å¹¶å­˜å‚¨æŒ‡æ ‡åç§°ä½œä¸ºå˜é‡ã€‚ç¬¬äºŒæ­¥ï¼Œæç¤ºèŠå¤©æœºå™¨äººå»ºè®®ä¸€ä¸ªåˆå§‹æœç´¢ç©ºé—´ã€‚ä¸ç¬¬ä¸€æ­¥ä¸€æ ·ï¼Œè§£æå“åº”å¹¶æå–æœç´¢ç©ºé—´ï¼Œå­˜å‚¨ä¸ºå˜é‡ã€‚è·å¾—è¿™ä¸¤ä¸ªä¿¡æ¯åï¼Œå°†è°ƒç”¨éšæœºæœç´¢å·¥å…·ï¼Œä½¿ç”¨èŠå¤©æœºå™¨äººå»ºè®®çš„æŒ‡æ ‡å’Œæœç´¢ç©ºé—´ã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™æ„æˆäº†ç¬¬ä¸€è½®è¿­ä»£ã€‚

ä¸€æ—¦éšæœºæœç´¢å·¥å…·å®Œæˆæœç´¢ï¼Œå°†æç¤ºèŠå¤©æœºå™¨äººæ ¹æ®ä¸Šä¸€æ¬¡è¿è¡Œçš„ç»“æœæ¨èæœç´¢ç©ºé—´çš„ç»†åŒ–ã€‚åœ¨æˆåŠŸè§£æèŠå¤©æœºå™¨äººçš„å“åº”ä¸­çš„æ–°æœç´¢ç©ºé—´åï¼Œå°†è¿›è¡Œå¦ä¸€è½®éšæœºæœç´¢ã€‚è¿™ä¸ªè¿‡ç¨‹ä¼šè¿­ä»£ï¼Œç›´åˆ°è®¡ç®—é¢„ç®—è€—å°½æˆ–è¾¾åˆ°æ”¶æ•›ï¼Œä¾‹å¦‚ï¼Œè¾ƒä¸Šæ¬¡è¿è¡Œæ²¡æœ‰æ›´å¤šæ”¹è¿›ã€‚

![](img/0c217e54637dd58fd79a95279472c13d.png)

åŸºäºèŠå¤©æœºå™¨äººçš„è§£å†³æ–¹æ¡ˆï¼Œç”¨äºå®ç° LLM å¼•å¯¼çš„è¶…å‚æ•°è°ƒä¼˜ã€‚ï¼ˆå›¾åƒç”±ä½œè€…æä¾›ï¼‰

å°½ç®¡ä¸å¦‚åŸºäºä»£ç†çš„æ–¹æ³•ä¼˜é›…ï¼Œè¿™ç§åŸºäºèŠå¤©æœºå™¨äººçš„å·¥ä½œæµå…·æœ‰å‡ ä¸ªå¥½å¤„ï¼š

+   **æ˜“äºå®ç°å’Œå®ç°è´¨é‡æ§åˆ¶**ã€‚å°†èŠå¤©æœºå™¨äººä¸è°ƒä¼˜å·¥å…·é›†æˆç®€åŒ–ä¸ºè®¾è®¡ä¸‰ä¸ªæç¤ºå’Œå‡ ä¸ªè¾…åŠ©å‡½æ•°ï¼Œä»¥ä»èŠå¤©æœºå™¨äººçš„å“åº”ä¸­æå–ç›®æ ‡ä¿¡æ¯ã€‚è¿™æ¯”å®Œå…¨é›†æˆçš„åŸºäºä»£ç†çš„æ–¹æ³•è¦ç®€å•å¾—å¤šã€‚æ­¤å¤–ï¼Œç”±äºè¶…å‚æ•°è°ƒä¼˜ä»»åŠ¡è¢«åˆ†è§£ä¸ºæ˜ç¡®çš„æ­¥éª¤ï¼Œæ¯ä¸€æ­¥éƒ½å¯ä»¥è¢«ç›‘æ§å’Œè°ƒæ•´ï¼Œä»è€Œä½¿è´¨é‡æ§åˆ¶å˜å¾—æ›´å¯æ§ï¼Œç¡®ä¿æœç´¢è¿‡ç¨‹ä¿æŒåœ¨è½¨é“ä¸Šã€‚

+   **å®Œå…¨é€æ˜çš„å†³ç­–è¿‡ç¨‹**ã€‚ç”±äºèŠå¤©æœºå™¨äººä¼šæ¸…æ™°åœ°è¯´æ˜å…¶åšå‡ºçš„æ¯ä¸ªå†³ç­–æˆ–å»ºè®®ï¼Œè¶…å‚æ•°è°ƒä¼˜è¿‡ç¨‹ä¸å†æ˜¯å¯¹ç”¨æˆ·çš„é»‘ç®±è¿‡ç¨‹ã€‚è¿™å¯¹äºå®ç°*å¯è§£é‡Šçš„*å’Œ*å€¼å¾—ä¿¡èµ–çš„* AutoML è‡³å…³é‡è¦ã€‚

+   **å…è®¸ç»“åˆäººç±»ç›´è§‰**ã€‚å°½ç®¡å½“å‰çš„å·¥ä½œæµè®¾è®¡ä¸ºè‡ªåŠ¨åŒ–ï¼Œä½†å¯ä»¥è½»æ¾æ‰©å±•ï¼Œä»¥å…è®¸äººç±»ä¸“å®¶åœ¨æ¯æ¬¡æœç´¢è¿­ä»£ä¹‹å‰é€‰æ‹©æ¥å—å»ºè®®æˆ–æ ¹æ®è‡ªå·±çš„ä¸“ä¸šçŸ¥è¯†è¿›è¡Œå¿…è¦çš„è°ƒæ•´ã€‚è¿™ç§çµæ´»æ€§ä¸º*äººç±»å‚ä¸çš„è°ƒä¼˜*æ‰“å¼€äº†å¤§é—¨ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ›´å¥½çš„ä¼˜åŒ–ç»“æœã€‚

åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†é€ä¸€è¯¦ç»†ä»‹ç»é…ç½®èŠå¤©æœºå™¨äººä»¥åŠå»ºè®®æŒ‡æ ‡ã€åˆå§‹æœç´¢ç©ºé—´å’Œæœç´¢ç©ºé—´ç»†åŒ–çš„æç¤ºã€‚

# 3\. é…ç½®èŠå¤©æœºå™¨äºº

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ LangChain è½»æ¾è®¾ç½®ä¸€ä¸ªå…·æœ‰è®°å¿†çš„èŠå¤©æœºå™¨äººã€‚æˆ‘ä»¬é¦–å…ˆå¯¼å…¥å¿…è¦çš„åº“ï¼š

```py
from langchain.prompts import (
    ChatPromptTemplate, 
    MessagesPlaceholder, 
    SystemMessagePromptTemplate, 
    HumanMessagePromptTemplate
)
from langchain.chains import ConversationChain
from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory
```

åœ¨ LangChain ä¸­ï¼ŒèŠå¤©æœºå™¨äººå¯ä»¥é€šè¿‡`ConversationChain`å¯¹è±¡è¿›è¡Œé…ç½®ï¼Œè¿™éœ€è¦ä¸€ä¸ªä¸»å¹² LLMã€ä¸€ä¸ªç”¨äºä¿å­˜å¯¹è¯å†å²çš„å†…å­˜å¯¹è±¡ä»¥åŠä¸€ä¸ªç”¨äºæŒ‡å®šèŠå¤©æœºå™¨äººè¡Œä¸ºçš„æç¤ºæ¨¡æ¿ï¼š

```py
# Set up LLM
llm = ChatOpenAI(temperature=0.8)

# Set up memory
memory = ConversationBufferMemory(return_messages=True)

# Set up the prompt
prompt = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template(system_message),
    MessagesPlaceholder(variable_name="history"),
    HumanMessagePromptTemplate.from_template("""{input}""")
])

# Create conversation chain
conversation = ConversationChain(memory=memory, prompt=prompt, 
                                llm=llm, verbose=False)
```

åœ¨ä¸Šé¢çš„ä»£ç ç‰‡æ®µä¸­ï¼Œå˜é‡`system_message`è®¾ç½®äº†èŠå¤©æœºå™¨äººçš„ä¸Šä¸‹æ–‡ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```py
# Note: the prompt is generated and optimized by ChatGPT (GPT-4)
system_message = f"""
You are a senior data scientist tasked with guiding the use of an AutoML 
tool to discover the best XGBoost model configurations for a given binary 
classification dataset. Your role involves understanding the dataset 
characteristics, proposing suitable metrics, hyperparameters, and their 
search spaces, analyzing results, and iterating on configurations. 
"""
```

æ³¨æ„ï¼Œæˆ‘ä»¬åœ¨ç³»ç»Ÿæ¶ˆæ¯ä¸­æŒ‡å®šäº†èŠå¤©æœºå™¨äººçš„è§’è‰²ã€ç›®çš„å’Œé¢„æœŸè¡Œä¸ºã€‚ç¨åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨é…ç½®å¥½çš„èŠå¤©æœºå™¨äººè¿›è¡Œæ¨ç†ï¼š

```py
# Invoke chatbot with the input prompt
response = conversation.predict(input=prompt)
```

å…¶ä¸­ `prompt` æ˜¯æˆ‘ä»¬å¯¹èŠå¤©æœºå™¨äººçš„å…·ä½“æŒ‡ä»¤ï¼Œå³å»ºè®®ä¼˜åŒ–åº¦é‡æ ‡å‡†ã€å»ºè®®åˆå§‹æœç´¢ç©ºé—´æˆ–å»ºè®®ä¼˜åŒ–æœç´¢ç©ºé—´ã€‚æˆ‘ä»¬å°†åœ¨æ¥ä¸‹æ¥çš„å‡ èŠ‚ä¸­ä»‹ç»è¿™äº›æç¤ºã€‚

# 4\. å»ºè®®ä¼˜åŒ–åº¦é‡æ ‡å‡†

ä½œä¸º LLM å¼•å¯¼çš„è¶…å‚æ•°è°ƒæ•´çš„ç¬¬ä¸€æ­¥ï¼Œæˆ‘ä»¬å¸Œæœ›èŠå¤©æœºå™¨äººæå‡ºä¸€ä¸ªåˆé€‚çš„åº¦é‡æ ‡å‡†ï¼Œä»¥ä¾¿è°ƒæ•´å·¥å…·è¿›è¡Œä¼˜åŒ–ã€‚è¿™ä¸ªå†³å®šåº”åŸºäºå‡ ä¸ªå› ç´ ï¼ŒåŒ…æ‹¬é—®é¢˜çš„æ€§è´¨ï¼ˆå³å›å½’è¿˜æ˜¯åˆ†ç±»ï¼‰ã€ç»™å®šæ•°æ®é›†çš„ç‰¹å¾ï¼Œä»¥åŠæ¥è‡ªä¸šåŠ¡æˆ–å®é™…åº”ç”¨çš„å…¶ä»–å…·ä½“è¦æ±‚ã€‚

ä»¥ä¸‹ç‰‡æ®µå®šä¹‰äº†å®ç°æˆ‘ä»¬ç›®æ ‡çš„æç¤ºï¼š

```py
def suggest_metrics(report):

    # Note: The prompt is generated and optimized by ChatGPT (GPT-4)
    prompt = f"""
    The classification problem under investigation is based on a network 
    intrusion detection dataset. This dataset contains Probe attack type, 
    which are all grouped under the "attack" class (label: 1). Conversely, 
    the "normal" class is represented by label 0\. Below are the dataset's 
    characteristics:
    {report}.

    For this specific inquiry, you are tasked with recommending a suitable 
    hyperparameter optimization metric for training a XGBoost model. It is 
    crucial that the model should accurately identify genuine threats (attacks) 
    without raising excessive false alarms on benign activities. They are equally 
    important. Given the problem context and dataset characteristics, suggest 
    only the name of one of the built-in metrics: 
    - 'accuracy'
    - 'roc_auc' (ROCAUC score)
    - 'f1' (F1 score)
    - 'balanced_accuracy' (It is the macro-average of recall scores per class 
    or, equivalently, raw accuracy where each sample is weighted according to 
    the inverse prevalence of its true class) 
    - 'average_precision'
    - 'precision'
    - 'recall'
    - 'neg_brier_score'

    Please first briefly explain your reasoning and then provide the 
    recommended metric name. Your recommendation should be enclosed between 
    markers [BEGIN] and [END], with standalone string for indicating the 
    metric name.
    Do not provide other settings or configurations.
    """

    return prompt
```

åœ¨ä¸Šé¢çš„æç¤ºä¸­ï¼Œæˆ‘ä»¬å·²ç»å‘èŠå¤©æœºå™¨äººè¯´æ˜äº†é—®é¢˜çš„èƒŒæ™¯ã€æ•°æ®é›†ç‰¹å¾ï¼ˆå°è£…åœ¨å˜é‡`report`ä¸­ï¼Œæˆ‘ä»¬ç¨åå°†è®¨è®ºï¼‰ï¼Œç›®æ ‡ï¼ˆæ¨èä¸€ä¸ªé€‚åˆçš„è¶…å‚æ•°ä¼˜åŒ–åº¦é‡æ ‡å‡†ç”¨äºè®­ç»ƒ XGBoost æ¨¡å‹ï¼‰ã€å…·ä½“è¦æ±‚ï¼ˆé«˜æ£€æµ‹ç‡å’Œä½è¯¯æŠ¥ç‡ï¼‰ä»¥åŠå€™é€‰åº¦é‡æ ‡å‡†ï¼ˆå®ƒä»¬éƒ½åœ¨ sci-kit learn ä¸­å—æ”¯æŒï¼‰ã€‚æ³¨æ„ï¼Œæˆ‘ä»¬æ˜ç¡®è¦æ±‚èŠå¤©æœºå™¨äººå°†åº¦é‡æ ‡å‡†åç§°è¾“å‡ºåœ¨[BEGIN]-[END]å—å†…ï¼Œè¿™æ ·å¯ä»¥æ–¹ä¾¿åœ°è‡ªåŠ¨æå–ä¿¡æ¯ã€‚

ä¸ºäº†ç”Ÿæˆæ•°æ®æŠ¥å‘Šï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ä»¥ä¸‹å‡½æ•°ï¼š

```py
def data_report(df, num_feats, bin_feats, nom_feats):
    """
    Generate data characteristics report.

    Inputs:
    -------
    df: dataframe for the dataset.
    num_feats: list of names of numerical features.
    bin_feats: list of names of binary features.
    nom_feats: list of names of nominal features.

    Outputs:
    --------
    report: data characteristics report.
    """

    # Label column 
    target = df.iloc[:, -1]
    features = df.iloc[:, :-1]

    # General dataset info
    num_instances = len(df)
    num_features = features.shape[1]

    # Class imbalance analysis
    class_counts = target.value_counts()
    class_distribution = class_counts/num_instances

    # Create report
    # Note: The format of the report is generated and optimized
    # by ChatGPT (GPT-4)
    report = f"""Data Characteristics Report:

- General information:
  - Number of Instances: {num_instances}
  - Number of Features: {num_features}

- Class distribution analysis:
  - Class Distribution: {class_distribution.to_string()}

- Feature analysis:
  - Feature names: {features.columns.to_list()}
  - Number of numerical features: {len(num_feats)}
  - Number of binary features: {len(bin_feats)}
  - Binary feature names: {bin_feats}
  - Number of nominal features: {len(nom_feats)}
  - Nominal feature names: {nom_feats}
"""

    return report
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç‰¹åˆ«è®¡ç®—äº†ç»™å®šçš„æ•°æ®é›†æ˜¯å¦å­˜åœ¨ä¸å¹³è¡¡ï¼Œå› ä¸ºè¿™ä¸€ä¿¡æ¯å¯èƒ½ä¼šå½±å“ä¼˜åŒ–åº¦é‡æ ‡å‡†çš„é€‰æ‹©ã€‚

ç°åœ¨æˆ‘ä»¬å·²ç»å®Œæˆäº†ç¬¬ä¸€ä¸ªæç¤ºï¼ŒæŒ‡ç¤ºèŠå¤©æœºå™¨äººå»ºè®®ä¸€ä¸ªé€‚åˆçš„åº¦é‡æ ‡å‡†æ¥è¯„ä¼°å„ç§è¶…å‚æ•°é…ç½®çš„æ€§èƒ½ã€‚æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•æ„å»ºå®šä¹‰åˆå§‹æœç´¢ç©ºé—´çš„æç¤ºã€‚

# **5\. å®šä¹‰åˆå§‹æœç´¢ç©ºé—´**

é™¤äº†ä¼˜åŒ–åº¦é‡æ ‡å‡†ï¼Œæˆ‘ä»¬è¿˜éœ€è¦ä¸€ä¸ªè¶…å‚æ•°çš„åˆå§‹æœç´¢ç©ºé—´æ¥å¼€å§‹è°ƒæ•´è¿‡ç¨‹ã€‚ä»¥ä¸‹ç‰‡æ®µå±•ç¤ºäº†å®ç°è¿™ä¸€ç›®æ ‡çš„æç¤ºï¼š

```py
def suggest_initial_search_space():

    # Note: The prompt is generated and optimized by ChatGPT (GPT-4) 
    prompt = f"""
    Given your understanding of XGBoost and general best practices in machine 
    learning, suggest an initial search space for hyperparameters. 

    Tunable hyperparameters include:
    - n_estimators (integer): Number of boosting rounds or trees to be trained.
    - max_depth (integer): Maximum tree depth for base learners.
    - min_child_weight (integer or float): Minimum sum of instance weight 
    (hessian) needed in a leaf node. 
    - gamma (float): Minimum loss reduction required to make a further 
    partition on a leaf node of the tree.
    - scale_pos_weight (float): Balancing of positive and negative weights.
    - learning_rate (float): Step size shrinkage used during each boosting 
    round to prevent overfitting. 
    - subsample (float): Fraction of the training data sampled to train each 
    tree. 
    - colsample_bylevel (float): Fraction of features that can be randomly 
    sampled for building each level (or depth) of the tree.
    - colsample_bytree (float): Fraction of features that can be randomly 
    sampled for building each tree. 
    - reg_alpha (float): L1 regularization term on weights. 
    - reg_lambda (float): L2 regularization term on weights. 

    The search space is defined as a dict with keys being hyperparameter names, 
    and values are the search space associated with the hyperparameter. 
    For example:
        search_space = {{
            "learning_rate": loguniform(1e-4, 1e-3)
        }}

    Available types of domains include: 
    - scipy.stats.uniform(loc, scale), it samples values uniformly between 
    loc and loc + scale.
    - scipy.stats.loguniform(a, b), it samples values between a and b in a 
    logarithmic scale.
    - scipy.stats.randint(low, high), it samples integers uniformly between 
    low (inclusive) and high (exclusive).
    - a list of possible discrete value, e.g., ["a", "b", "c"]

    Please first briefly explain your reasoning, then provide the 
    configurations of the initial search space. Enclose your suggested 
    configurations between markers [BEGIN] and [END], and assign your 
    configuration to a variable named search_space.
    """

    return prompt
```

ä¸Šé¢çš„æç¤ºåŒ…å«äº†å¤§é‡ä¿¡æ¯ï¼Œæ‰€ä»¥æˆ‘ä»¬æ¥é€æ­¥æ‹†è§£ï¼š

+   æˆ‘ä»¬é¦–å…ˆå‘èŠå¤©æœºå™¨äººè¯´æ˜äº†æˆ‘ä»¬çš„ç›®æ ‡ã€‚

+   æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå¯è°ƒè¶…å‚æ•°åŠå…¶å«ä¹‰çš„åˆ—è¡¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æŒ‡å‡ºäº†æ¯ä¸ªè¶…å‚æ•°çš„é¢„æœŸæ•°æ®ç±»å‹ã€‚è¿™äº›ä¿¡æ¯å¯¹ LLM å†³å®šé‡‡æ ·åˆ†å¸ƒè‡³å…³é‡è¦ã€‚

+   æˆ‘ä»¬å®šä¹‰äº†æœç´¢ç©ºé—´çš„é¢„æœŸè¾“å‡ºæ ¼å¼ï¼Œä»¥ä¾¿è¿›è¡Œæœ‰æ•ˆè§£æã€‚

+   æˆ‘ä»¬æŒ‡æ˜äº† LLM å¯ä»¥å»ºè®®çš„å¯ç”¨é‡‡æ ·åˆ†å¸ƒã€‚

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬æ˜ç¡®è¦æ±‚èŠå¤©æœºå™¨äººç®€è¦è¯´æ˜å»ºè®®çš„æœç´¢ç©ºé—´çš„ç†ç”±ã€‚è¿™å¯¹äºå®ç°é€æ˜åº¦å’Œå¯è§£é‡Šæ€§è‡³å…³é‡è¦ã€‚

è¿™å°±æ˜¯æ¨èåˆå§‹æœç´¢ç©ºé—´çš„å†…å®¹ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹å¦‚ä½•ä¼˜åŒ–æœç´¢ç©ºé—´ã€‚

# 6\. ä¼˜åŒ–æœç´¢ç©ºé—´

åœ¨ä»èŠå¤©æœºå™¨äººè·å–ä¼˜åŒ–æŒ‡æ ‡å’Œåˆå§‹æœç´¢ç©ºé—´åï¼Œæˆ‘ä»¬å¯ä»¥å¯åŠ¨ä¸€æ¬¡è°ƒä¼˜è½®æ¬¡ã€‚éšåï¼Œæˆ‘ä»¬å¯ä»¥å°†ç”Ÿæˆçš„ AutoML æ—¥å¿—è¾“å…¥åˆ°èŠå¤©æœºå™¨äººä¸­ï¼Œå¹¶æç¤ºå®ƒå»ºè®®å¯¹æœç´¢ç©ºé—´è¿›è¡Œä¼˜åŒ–ã€‚

ä»¥ä¸‹ä»£ç ç‰‡æ®µå±•ç¤ºäº†å®ç°ç›®æ ‡çš„æç¤ºï¼š

```py
def suggest_refine_search_space(top_n, last_run_best_score, all_time_best_score):
    """
    Generate prompt for refining the search space.

    Inputs:
    -------
    top_n: string representation of the top-5 best-performing configurations.
    last_run_best_score: best test score from the last run.
    all_time_best_score: best test score from all previous runs.

    Outputs:
    --------
    prompt: generated prompt.
    """

    # Note: The prompt is generated and optimized by ChatGPT (GPT-4)
    prompt = f"""
    Given your previously suggested search space, the obtained top configurations 
    with their test scores:
    {top_n}

    The best score from the last run was {last_run_best_score}, while the best 
    score ever achieved in all previous runs is {all_time_best_score}

    Remember, tunable hyperparameters are: n_estimators, max_depth, min_child_samples, 
    gamma, scale_pos_weight, learning_rate, subsample, colsample_bylevel, 
    colsample_bytree, reg_alpha, and reg_lambda.

    Given the insights from the search history, your expertise in ML, and the 
    need to further explore the search space, please suggest refinements for 
    the search space in the next optimization round. Consider both narrowing 
    and expanding the search space for hyperparameters where appropriate.

    For each recommendation, please:
    1\. Explicitly tie back to any general best practices or patterns you are 
    aware of regarding XGBoost tuning
    2\. Then, relate to the insights from the search history and explain how 
    they align or deviate from these practices or patterns.
    3\. If suggesting an expansion of the search space, please provide a 
    rationale for why a broader range could be beneficial.

    Briefly summarize your reasoning for the refinements and then present the 
    adjusted configurations. Enclose your refined configurations between 
    markers [BEGIN] and [END], and assign your configuration to a variable 
    named search_space.
    """

    return prompt
```

æœ‰å‡ ä¸ªå€¼å¾—è§£é‡Šçš„åœ°æ–¹ï¼š

+   æˆ‘ä»¬å‘èŠå¤©æœºå™¨äººæä¾›äº†ä¸Šä¸€æ¬¡è°ƒä¼˜è½®æ¬¡ä¸­è¡¨ç°æœ€å¥½çš„å‰ 5 ä¸ªé…ç½®åŠå…¶ç›¸å…³æµ‹è¯•åˆ†æ•°ã€‚è¿™å¯ä»¥ä½œä¸ºèŠå¤©æœºå™¨äººç¡®å®šä¸‹ä¸€è½®ä¼˜åŒ–çš„åŸºç¡€ã€‚

+   é€šè¿‡åŒ…æ‹¬ä¸Šä¸€æ¬¡è¿è¡Œçš„æœ€ä½³æµ‹è¯•åˆ†æ•°å’Œæ‰€æœ‰ä¹‹å‰çš„è¿è¡Œï¼ŒèŠå¤©æœºå™¨äººå¯ä»¥åˆ¤æ–­ä¸Šæ¬¡è¿è¡Œä¸­å»ºè®®çš„æœç´¢ç©ºé—´æ˜¯å¦æœ‰æ•ˆã€‚

+   æˆ‘ä»¬æ˜ç¡®è¦æ±‚èŠå¤©æœºå™¨äººè€ƒè™‘è¿›ä¸€æ­¥æ¢ç´¢æœç´¢ç©ºé—´ã€‚ä¸€èˆ¬è€Œè¨€ï¼Œä¼˜åŒ–çš„æœ€ä½³å®è·µæ˜¯**å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨**ã€‚ç”±äºæˆ‘ä»¬ä½¿ç”¨çš„éšæœºæœç´¢ç®—æ³•ï¼ˆå°†åœ¨ç»“æœéƒ¨åˆ†è®¨è®ºï¼‰å·²ç»æ¶µç›–äº†*åˆ©ç”¨*ï¼Œå› æ­¤è®© LLM æ›´å¤šåœ°å…³æ³¨*æ¢ç´¢*æ˜¯åˆç†çš„ã€‚

+   ä¸å…¶ä»–æç¤ºç±»ä¼¼ï¼Œæˆ‘ä»¬è¦æ±‚ LLM å¯¹å…¶å»ºè®®è¿›è¡Œæ¨ç†ï¼Œä»¥ç¡®ä¿é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚

åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ç”Ÿæˆ`top_n`å˜é‡çš„æ—¥å¿—åˆ†æé€»è¾‘ã€‚

# 7\. è°ƒä¼˜æ—¥å¿—åˆ†æ

ä½œä¸ºä¸€ç§è¿­ä»£æ–¹æ³•ï¼Œæˆ‘ä»¬å¸Œæœ›èŠå¤©æœºå™¨äººåœ¨ä¸Šä¸€è½®æœç´¢å®Œæˆåæå‡ºæ–°çš„æœç´¢ç©ºé—´ï¼Œè€Œå†³ç­–è¿‡ç¨‹çš„åŸºç¡€åº”è¯¥æ˜¯ä¸Šä¸€è½®æœç´¢ä¸­ç”Ÿæˆçš„æ—¥å¿—ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆä»‹ç»äº†å½“å‰æ¡ˆä¾‹ç ”ç©¶ä¸­ä½¿ç”¨çš„æœç´¢ç®—æ³•ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¨è®ºäº†æ—¥å¿—ç»“æ„å’Œæå–æœ‰ç”¨è§è§£çš„ä»£ç ã€‚

## 7.1 é€æ­¥æŠ˜åŠçš„éšæœºæœç´¢

æ­£å¦‚æœ¬æ–‡å¼€å¤´æ‰€æåˆ°çš„ï¼Œæˆ‘ä»¬å¸Œæœ›å°†ä¸€ç§ç®€å•çš„è¶…å‚æ•°æœç´¢ç®—æ³•ä¸ LLM ç»“åˆï¼Œä»¥æ£€éªŒé¢†åŸŸä¸“ä¸šçŸ¥è¯†æ˜¯å¦èƒ½å¸¦æ¥ä»·å€¼ã€‚

æœ€ç®€å•çš„è°ƒä¼˜æ–¹æ³•ä¹‹ä¸€æ˜¯**éšæœºæœç´¢**ï¼šå¯¹äºä¸€ä¸ªå®šä¹‰å¥½çš„æœç´¢ç©ºé—´ï¼ˆé€šè¿‡å°†é‡‡æ ·åˆ†å¸ƒé™„åŠ åˆ°è¶…å‚æ•°ä¸Šæ¥æŒ‡å®šï¼‰ï¼Œå¯¹ç»™å®šæ•°é‡çš„è¶…å‚æ•°ç»„åˆå®ä¾‹è¿›è¡Œé‡‡æ ·ï¼Œå¹¶è¯„ä¼°å…¶ç›¸å…³çš„æ¨¡å‹æ€§èƒ½ã€‚äº§ç”Ÿæœ€ä½³æ€§èƒ½çš„é‡‡æ ·è¶…å‚æ•°é…ç½®è¢«è®¤ä¸ºæ˜¯æœ€ä½³çš„ã€‚

å°½ç®¡å…¶ç®€å•æ€§ï¼Œéšæœºæœç´¢çš„æœ´ç´ ç‰ˆæœ¬å¯èƒ½å¯¼è‡´è®¡ç®—èµ„æºçš„ä½æ•ˆä½¿ç”¨ï¼Œå› ä¸ºå®ƒä¸åŒºåˆ†è¶…å‚æ•°é…ç½®ï¼Œè€Œä¸è‰¯çš„è¶…å‚æ•°é€‰æ‹©ä»ç„¶ä¼šè¢«è®­ç»ƒã€‚

ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†**è¿ç»­æŠ˜åŠ**æŠ€æœ¯ï¼Œä»¥å¢å¼ºåŸºæœ¬çš„éšæœºæœç´¢ç­–ç•¥ã€‚åŸºæœ¬ä¸Šï¼Œè¿ç»­æŠ˜åŠç­–ç•¥é¦–å…ˆä½¿ç”¨å°‘é‡èµ„æºè¯„ä¼°è®¸å¤šé…ç½®ï¼Œç„¶åé€æ¸å°†æ›´å¤šèµ„æºåˆ†é…ç»™æœ‰å‰æ™¯çš„é…ç½®ã€‚å› æ­¤ï¼ŒåŠ£è´¨é…ç½®å¯ä»¥åœ¨æ—©æœŸæœ‰æ•ˆåœ°è¢«æ·˜æ±°ï¼Œä»è€Œæé«˜æœç´¢æ•ˆç‡ã€‚Sci-kit Learn æä¾›äº†ç²¾ç¡®å®ç°è¿™ä¸€ç­–ç•¥çš„`[HalvingRandomSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingRandomSearchCV.html)`ä¼°ç®—å™¨ï¼Œæˆ‘ä»¬å°†åœ¨å½“å‰æ¡ˆä¾‹ç ”ç©¶ä¸­é‡‡ç”¨ã€‚

## 7.2 æ—¥å¿—åˆ†æ

å½“è¿è¡Œ`[HalvingRandomSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingRandomSearchCV.html)`æœç´¢æ—¶ï¼Œæœç´¢æ—¥å¿—å­˜å‚¨åœ¨å±æ€§`cv_results_`ä¸­ã€‚åŸå§‹æ—¥å¿—ä»¥å­—å…¸æ ¼å¼å­˜å‚¨ï¼Œå¯ä»¥è½¬æ¢ä¸º Pandas æ•°æ®æ¡†ä»¥ä¾¿äºæå–è§è§£ï¼š

![](img/d792bd9f77cdd9399c163cc7a0f550f6.png)

æœç´¢ç®—æ³•ç”Ÿæˆçš„æ—¥å¿—ã€‚ï¼ˆç¤ºä¾‹å–è‡ª[å®˜æ–¹ç”¨æˆ·æŒ‡å—](https://scikit-learn.org/stable/modules/grid_search.html#successive-halving-cv-results)ã€‚ï¼‰

å¯¹äºæˆ‘ä»¬å½“å‰çš„ç›®çš„ï¼Œæˆ‘ä»¬å¸Œæœ›æå–è¡¨ç°æœ€å¥½çš„å‰ N ä¸ªï¼ˆé»˜è®¤ 5 ä¸ªï¼‰é…ç½®åŠå…¶ç›¸å…³çš„æµ‹è¯•åˆ†æ•°ã€‚ä»¥ä¸‹å‡½æ•°å±•ç¤ºäº†æˆ‘ä»¬å¦‚ä½•å®ç°è¿™ä¸€ç›®æ ‡ï¼š

```py
def logs_analysis(results, N):
    """
    Extracting the top performing configs from the logs.

    Inputs:
    -------
    results: results dict produced by the sklearn search object.
    N: the number of top performing configs to consider.

    Outputs:
    --------
    top_config_summary: a string summary of the top-N best performing configs 
    and their associated test scores.
    last_run_best_score: the best test score obtained in the current search run.
    """

    # Convert to Dataframe
    df = pd.DataFrame(search.cv_results_)

    # Rank configs' performance
    top_configs = df.nsmallest(N, 'rank_test_score').reset_index(drop=True)

    # Considered hyparameters
    hyperparameter_columns = [
        'param_colsample_bylevel', 'param_colsample_bytree', 'param_gamma',
        'param_learning_rate', 'param_max_depth', 'param_min_child_weight',
        'param_n_estimators', 'param_reg_alpha', 'param_reg_lambda',
        'param_scale_pos_weight', 'param_subsample'
    ]

    # Convert to string
    config_strings = []
    for i, row in top_configs.iterrows():
        config = ', '.join([f"{col[6:]}: {row[col]}" for col in hyperparameter_columns])
        config_strings.append(f"Configuration {i + 1} ({row['mean_test_score']:.4f} test score): {config}")

    top_config_summary = '\n'.join(config_strings)

    # Best test score
    last_run_best_score = top_configs.loc[0, 'mean_test_score']

    return top_config_summary, last_run_best_score
```

# 8\. æ¡ˆä¾‹ç ”ç©¶

ç°åœ¨æˆ‘ä»¬æ‹¥æœ‰äº†æ‰€æœ‰çš„è¦ç´ ï¼Œæ˜¯æ—¶å€™å°† LLM æŒ‡å¯¼çš„å·¥ä½œæµç¨‹åº”ç”¨äºæˆ‘ä»¬çš„æ¡ˆä¾‹ç ”ç©¶äº†ã€‚

## 8.1 ç¡®å®šåº¦é‡æ ‡å‡†

é¦–å…ˆï¼Œæˆ‘ä»¬æç¤º LLM å»ºè®®ä¸€ä¸ªåˆé€‚çš„ä¼˜åŒ–åº¦é‡æ ‡å‡†ï¼š

```py
# Suggest metrics
prompt = suggest_metrics(report)
response = conversation.predict(input=prompt)
print(response)
```

ä»¥ä¸‹æ˜¯ LLM ç”Ÿæˆçš„å“åº”ï¼š

![](img/4de8c745b8dcf8e34ba6db0c3e505e68.png)

LLM å»ºè®®äº†ä¸€ä¸ªåˆé€‚çš„åº¦é‡æ ‡å‡†ï¼Œå¹¶æä¾›äº†ç†ç”±ã€‚ï¼ˆå›¾ç‰‡ä½œè€…æä¾›ï¼‰

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒLLM æ ¹æ®é—®é¢˜èƒŒæ™¯å’Œæ•°æ®é›†ç‰¹å¾æ¨èäº†â€œF1â€åˆ†æ•°ä½œä¸ºåº¦é‡æ ‡å‡†ï¼Œè¿™ä¸æˆ‘ä»¬çš„æœŸæœ›ä¸€è‡´ã€‚æ­¤å¤–ï¼Œåº¦é‡æ ‡å‡†åç§°è¢«æ­£ç¡®åœ°å°è£…åœ¨æˆ‘ä»¬æŒ‡å®šçš„åå¤„ç†æ ‡è®°ä¹‹é—´ã€‚

## 8.2 ç¬¬ä¸€æ¬¡æœç´¢è¿­ä»£

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æç¤º LLM å»ºè®®ä¸€ä¸ªåˆæ­¥çš„æœç´¢ç©ºé—´ï¼š

```py
# Initial search space
prompt = suggest_initial_search_space()
response = conversation.predict(input=prompt)
print(response)
```

LLM çš„è¾“å‡ºå¦‚ä¸‹æ‰€ç¤ºï¼š

![](img/600300d6a76769e45671c9ec4a119283.png)![](img/4f43e2b130e9c79be142e94115b2f1ce.png)

LLM å»ºè®®äº†ä¸€ä¸ªåˆæ­¥çš„æœç´¢ç©ºé—´ï¼Œå¹¶æä¾›äº†ç†ç”±ã€‚ï¼ˆå›¾ç‰‡ä½œè€…æä¾›ï¼‰

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒLLM å¿ å®åœ°éµå¾ªäº†æˆ‘ä»¬çš„æŒ‡ç¤ºï¼Œå¹¶æä¾›äº†æœ‰å…³è®¾ç½®æ¯ä¸ªå¯è°ƒè¶…å‚æ•°çš„å˜å¼‚èŒƒå›´çš„è¯¦ç»†è¯´æ˜ã€‚è¿™å¯¹ç¡®ä¿è¶…å‚æ•°è°ƒä¼˜è¿‡ç¨‹çš„é€æ˜æ€§è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œè¯·æ³¨æ„ LLM æˆåŠŸè¾“å‡ºäº†æ­£ç¡®æ ¼å¼çš„æœç´¢ç©ºé—´ã€‚è¿™æ˜¾ç¤ºäº†åœ¨æç¤ºè®¾è®¡ä¸­æä¾›å…·ä½“ç¤ºä¾‹çš„é‡è¦æ€§ã€‚

ç„¶åï¼Œæˆ‘ä»¬è°ƒç”¨éšæœºæœç´¢ä¸è¿ç»­æŠ˜åŠï¼Œå¹¶è¿è¡Œç¬¬ä¸€æ¬¡è¿­ä»£ï¼š

```py
from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import HalvingRandomSearchCV
import xgboost as xgb

clf = xgb.XGBClassifier(seed=42, objective='binary:logistic', 
                        eval_metric='logloss', n_jobs=-1, 
                        use_label_encoder=False)
search = HalvingRandomSearchCV(clf, search_space, scoring='f1', 
                               n_candidates=500, cv=5, 
                               min_resources='exhaust', factor=3, 
                               verbose=1).fit(X_train, y_train)
```

è¯·æ³¨æ„ï¼Œ`HalvingRandomSearchCV` ä¼°è®¡å™¨ä»å¤„äºå®éªŒé˜¶æ®µã€‚å› æ­¤ï¼Œåœ¨ä½¿ç”¨ä¼°è®¡å™¨ä¹‹å‰ï¼Œå¿…é¡»é¦–å…ˆæ˜¾å¼å¯¼å…¥ `enable_halving_search_cv`ã€‚

ä½¿ç”¨ `HalvingRandomSearchCV` ä¼°è®¡å™¨éœ€è¦è®¾ç½®å‡ ä¸ªå‚æ•°ã€‚é™¤äº†æŒ‡å®šä¼°è®¡å™¨ï¼ˆclfï¼‰ã€å‚æ•°åˆ†å¸ƒï¼ˆæœç´¢ç©ºé—´ï¼‰å’Œä¼˜åŒ–æŒ‡æ ‡ï¼ˆf1ï¼‰å¤–ï¼Œæˆ‘ä»¬è¿˜éœ€è¦æŒ‡å®šæ§åˆ¶æ—¶é—´é¢„ç®—çš„å‚æ•°ã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬å°† `n_candidates` è®¾ç½®ä¸º 500ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å°†åœ¨ç¬¬ä¸€æ¬¡è¿­ä»£ä¸­æŠ½å– 500 ä¸ªå€™é€‰é…ç½®ï¼ˆæ¯ä¸ªé…ç½®æœ‰ä¸åŒçš„è¶…å‚æ•°ç»„åˆï¼‰ã€‚æ­¤å¤–ï¼Œ`factor` è®¾ç½®ä¸º 3ï¼Œè¿™æ„å‘³ç€æ¯æ¬¡åç»­è¿­ä»£ä¸­ä»…é€‰æ‹©ä¸‰åˆ†ä¹‹ä¸€çš„å€™é€‰è€…ã€‚åŒæ—¶ï¼Œè¿™äº›é€‰ä¸­çš„ä¸‰åˆ†ä¹‹ä¸€çš„å€™é€‰è€…å°†åœ¨åç»­è¿­ä»£ä¸­ä½¿ç”¨ä¸‰å€çš„èµ„æºï¼ˆå³è®­ç»ƒæ ·æœ¬æ•°ï¼‰ã€‚æœ€åï¼Œæˆ‘ä»¬å°† `min_resource` è®¾ç½®ä¸ºâ€œexhaustâ€ï¼Œè¿™æ„å‘³ç€åœ¨æœ€åä¸€æ¬¡è¿­ä»£ä¸­ï¼Œå‰©ä½™çš„å€™é€‰è€…å°†ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„è®­ç»ƒæ ·æœ¬ã€‚æœ‰å…³è®¾ç½® `HalvingRandomSearchCV` ä¼°è®¡å™¨çš„è¯¦ç»†è¯´æ˜ï¼Œè¯·å‚é˜…æ­¤å¸–å­ï¼šä½¿ç”¨ HalvingGridSearch é€Ÿåº¦æå‡ 11 å€çš„è¶…å‚æ•°è°ƒä¼˜ã€‚

ä¸‹é¢æ˜¾ç¤ºäº†è¿è¡Œ `HalvingRandomSearchCV` ä¼°è®¡å™¨ç”Ÿæˆçš„æ—¥å¿—å¿«ç…§ã€‚åœ¨æˆ‘çš„ç”µè„‘ä¸Šè¿è¡Œæœç´¢çš„å®é™…æ—¶é—´çº¦ä¸º 20 åˆ†é’Ÿã€‚

![](img/4b3d4f7ac96bd8387075ff4b8535ca6d.png)

è¿ç»­ç¼©å°éšæœºæœç´¢ç®—æ³•ç”Ÿæˆçš„æ—¥å¿—å¿«ç…§ã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼šä½œè€…ï¼‰

## 8.3 ç¬¬äºŒæ¬¡æœç´¢è¿­ä»£

ä¸€æ—¦æœç´¢å®Œæˆï¼Œæˆ‘ä»¬å¯ä»¥ä» `search.cv_results` ä¸­æ£€ç´¢æœç´¢å†å²è®°å½•ï¼Œå¹¶å°†å…¶å‘é€åˆ°ä¹‹å‰å®šä¹‰çš„ `log_analysis()` å‡½æ•°ä»¥æå–è°ƒä¼˜è§è§£ã€‚ä¹‹åï¼Œæˆ‘ä»¬è°ƒç”¨ `suggest_refine_search_space()` å‡½æ•°æ¥æç¤º LLM æ ¹æ®ä¹‹å‰çš„æœç´¢ç»“æœæ¨èæ–°çš„æœç´¢ç©ºé—´ï¼š

```py
# Configure prompt
prompt = suggest_refine_search_space(top_n, last_run_best_score, all_time_best_score)

# Refine search space
response = conversation.predict(input=prompt)
print(response)
```

LLM çš„å“åº”å¦‚ä¸‹æ‰€ç¤ºï¼š

![](img/99bf92d70ab4368587e5e9b67f529dff.png)![](img/9d33863c6bfdb562d5621ae313802eea.png)

LLM å»ºè®®äº†æœç´¢ç©ºé—´çš„ä¼˜åŒ–å¹¶æä¾›äº†ç†ç”±ã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼šä½œè€…ï¼‰

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ° LLM å»ºè®®å¯¹ä¸€äº›è¶…å‚æ•°è¿›è¡Œç¼©å°ç©ºé—´ï¼Œå¯¹å…¶ä»–è¶…å‚æ•°è¿›è¡Œæ‰©å±•ç©ºé—´ã€‚è¿™äº›å»ºè®®çš„ç†ç”±ä¹Ÿè¢«æ¸…æ™°åœ°é˜è¿°ï¼Œä»è€Œæé«˜äº†å¯è§£é‡Šæ€§å’Œé€æ˜åº¦ã€‚

åŸºäºä¼˜åŒ–åçš„æœç´¢ç©ºé—´ï¼Œæˆ‘ä»¬å¯ä»¥ç¬¬äºŒæ¬¡è¿è¡Œ `HalvingRandomSearchCV` ä¼°è®¡å™¨ï¼š

```py
clf = xgb.XGBClassifier(seed=42, objective='binary:logistic', 
                        eval_metric='logloss', n_jobs=-1, 
                        use_label_encoder=False)
search = HalvingRandomSearchCV(clf, search_space, scoring='f1', 
                               n_candidates=500, cv=5, 
                               min_resources='exhaust', factor=3, 
                               verbose=1).fit(X_train, y_train)
```

åœ¨æˆ‘çš„ç”µè„‘ä¸Šè¿è¡Œæœç´¢çš„å®é™…æ—¶é—´çº¦ä¸º 29 åˆ†é’Ÿã€‚

## 8.4 ç¬¬ä¸‰æ¬¡æœç´¢è¿­ä»£

è®©æˆ‘ä»¬å†è¿è¡Œä¸€æ¬¡ LLM å¼•å¯¼çš„æœç´¢ã€‚ä¸ä¹‹å‰ä¸€æ ·ï¼Œæˆ‘ä»¬é¦–å…ˆä»ä¹‹å‰çš„æœç´¢æ—¥å¿—ä¸­æå–æœ‰ç”¨çš„è§è§£ï¼Œç„¶åæç¤º LLM è¿›ä¸€æ­¥ä¼˜åŒ–æœç´¢ç©ºé—´ã€‚LLM ç”Ÿæˆçš„å“åº”å¦‚ä¸‹æ‰€ç¤ºï¼š

![](img/540632b029e66cbdc8a8e703cf94cb62.png)![](img/fa9ed4b32ca9bd7882a8a1d4f81d0240.png)

LLM å»ºè®®äº†æœç´¢ç©ºé—´çš„ä¼˜åŒ–å¹¶æä¾›äº†ç†ç”±ã€‚ï¼ˆå›¾åƒç”±ä½œè€…æä¾›ï¼‰

åœ¨æ–°å®šä¹‰çš„æœç´¢ç©ºé—´ä¸‹ï¼Œæˆ‘ä»¬ç¬¬ä¸‰æ¬¡è¿è¡Œäº† `HalvingRandomSearchCV` ä¼°è®¡å™¨ï¼š

```py
clf = xgb.XGBClassifier(seed=42, objective='binary:logistic', 
                        eval_metric='logloss', n_jobs=-1, 
                        use_label_encoder=False)
search = HalvingRandomSearchCV(clf, search_space, scoring='f1', 
                               n_candidates=100, cv=5, 
                               min_resources='exhaust', factor=3, 
                               verbose=1).fit(X_train, y_train)
```

åœ¨æˆ‘çš„ PC ä¸Šè¿è¡Œæœç´¢çš„æ—¶é—´å¤§çº¦ä¸º 11 åˆ†é’Ÿã€‚

## 8.5 æµ‹è¯•

ç»è¿‡ä¸‰è½®éšæœºæœç´¢åï¼Œè®©æˆ‘ä»¬æµ‹è¯•è·å¾—çš„ XGBoost æ¨¡å‹çš„æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ªè¾…åŠ©å‡½æ•°æ¥è®¡ç®—å„ç§æ€§èƒ½æŒ‡æ ‡ï¼š

```py
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import (
    precision_score,
    recall_score,
    accuracy_score,
    roc_auc_score,
    f1_score,
    matthews_corrcoef
)

def metrics_display(y_test, y_pred, y_pred_proba):

    # Obtain confusion matrix
    cm = confusion_matrix(y_test, y_pred)

    # Output classification metrics
    tn, fp, fn, tp = cm.ravel()

    print(f'ROC_AUC score: {roc_auc_score(y_test, y_pred_proba):.3f}')
    print(f'f1 score: {f1_score(y_test, y_pred):.3f}')
    print(f'Accuracy: {accuracy_score(y_test, y_pred)*100:.2f}%')
    print(f'Precision: {precision_score(y_test, y_pred)*100:.2f}%')
    print(f'Detection rate: {recall_score(y_test, y_pred)*100:.2f}%')
    print(f'False alarm rate: {fp / (tn+fp)*100}%')
    print(f'MCC: {matthews_corrcoef(y_test, y_pred):.2f}')

    # Display confusion matrix
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot()

# Calculate performance metrics
y_pred = search.predict(X_test)
y_pred_proba = search.predict_proba(X_test)
metrics_display(y_test, y_pred, y_pred_proba[:, 1])
```

åœ¨ä¸Šè¿°ä»£ç ç‰‡æ®µä¸­ï¼Œæˆ‘ä»¬å°†è®­ç»ƒå¥½çš„ XGBoost æ¨¡å‹åº”ç”¨äºæµ‹è¯•æ•°æ®é›†ï¼Œå¹¶è¯„ä¼°å…¶æ€§èƒ½ã€‚è·å¾—çš„æ··æ·†çŸ©é˜µå¦‚ä¸‹æ‰€ç¤ºï¼š

![](img/120b41a0c443fd2cf2227f2d8da24863.png)

è®­ç»ƒåçš„ XGBoost æ¨¡å‹åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šçš„æ··æ·†çŸ©é˜µã€‚ï¼ˆå›¾åƒç”±ä½œè€…æä¾›ï¼‰

æ ¹æ®æ··æ·†çŸ©é˜µï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—å„ç§æ€§èƒ½æŒ‡æ ‡ï¼š

+   å‡†ç¡®ç‡ï¼š94.22%

+   ç²¾ç¡®åº¦ï¼š89.5%

+   æ£€æµ‹ç‡ï¼ˆå¬å›ç‡ï¼‰ï¼š80.52%

+   è™šè­¦ç‡ï¼ˆ1-ç‰¹å¼‚æ€§ï¼‰ï¼š2.35%

+   ROC-AUC åˆ†æ•°ï¼š0.983

+   F1 åˆ†æ•°ï¼š0.848

+   é©¬ä¿®æ–¯ç›¸å…³ç³»æ•°ï¼š0.81

# 9. ä¸å¼€ç®±å³ç”¨ AutoML å·¥å…·çš„æ¯”è¾ƒ

æ­£å¦‚æˆ‘ä»¬åœ¨å¼€å¤´æåˆ°çš„ï¼Œæˆ‘ä»¬å¸Œæœ›å°†å¼€å‘çš„ LLM å¼•å¯¼æœç´¢çš„è°ƒä¼˜ç»“æœä¸ç°æˆçš„ã€åŸºäºç®—æ³•çš„ AutoML å·¥å…·è¿›è¡Œæ¯”è¾ƒï¼Œä»¥è¯„ä¼°æ˜¯å¦å€ŸåŠ© ML é¢†åŸŸä¸“ä¸šçŸ¥è¯†ç¡®å®èƒ½å¸¦æ¥ä»·å€¼ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨çš„ AutoML å·¥å…·å«åš [FLAML](https://microsoft.github.io/FLAML/)ï¼Œç”±å¾®è½¯ç ”ç©¶é™¢å¼€å‘ï¼Œä»£è¡¨ *ä¸€ç§ç”¨äºè‡ªåŠ¨æœºå™¨å­¦ä¹ å’Œè°ƒæ•´çš„å¿«é€Ÿåº“*ã€‚æ­¤å·¥å…·æ˜¯æœ€å…ˆè¿›çš„ï¼Œæ”¯æŒå¿«é€Ÿä¸”ç»æµçš„è‡ªåŠ¨è°ƒä¼˜ï¼Œèƒ½å¤Ÿå¤„ç†å…·æœ‰å¼‚è´¨è¯„ä¼°æˆæœ¬å’Œå¤æ‚çº¦æŸ/æŒ‡å¯¼/æå‰åœæ­¢çš„å¤§å‹æœç´¢ç©ºé—´ã€‚æœ‰å…³å®‰è£…åº“çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [å®˜æ–¹é¡µé¢](https://microsoft.github.io/FLAML/docs/Installation)ã€‚

ä½¿ç”¨æ­¤å·¥å…·æå…¶ç®€å•ï¼šåœ¨ä¸‹é¢çš„ä»£ç ç‰‡æ®µä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå®ä¾‹åŒ–ä¸€ä¸ª`AutoML`å¯¹è±¡ï¼Œç„¶åè°ƒç”¨å…¶`fit()`æ–¹æ³•å¯åŠ¨è¶…å‚æ•°è°ƒæ•´è¿‡ç¨‹ã€‚æˆ‘ä»¬å°†è°ƒæ•´é™åˆ¶åœ¨ XGBoost æ¨¡å‹ä¸Šï¼ˆFLAML ä¹Ÿæ”¯æŒå…¶ä»–æ¨¡å‹ç±»å‹ï¼‰ï¼Œå¹¶è®¾ç½® 3600 ç§’çš„æ—¶é—´é¢„ç®—ï¼Œè¿™å¤§è‡´ç­‰äºæˆ‘ä»¬åœ¨ 3 è½® LLM å¼•å¯¼æœç´¢ä¸­èŠ±è´¹çš„æ€»æ—¶é—´ï¼ˆéšæœºæœç´¢æ—¶é—´+ LLM å“åº”æ—¶é—´ï¼‰ã€‚

```py
from flaml import AutoML

automl = AutoML()
automl.fit(X_train, y_train, task="classification", time_budget=3600, 
          estimator_list=['xgboost'], log_file_name='automl.log', 
          log_type='best')
```

ä»¥ä¸‹æ˜¯ LLM å¼•å¯¼æœç´¢ä¸å¼€ç®±å³ç”¨çš„ FLAML ç»“æœçš„æ¯”è¾ƒï¼š

![](img/3201aa335fbd481e84542172ca070b9c.png)

ä¸¤ç§è°ƒä¼˜æ–¹æ³•çš„ç»“æœæ¯”è¾ƒã€‚ï¼ˆå›¾åƒç”±ä½œè€…æä¾›ï¼‰

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒLLM-guided æœç´¢åœ¨æ‰€æœ‰è€ƒè™‘çš„æŒ‡æ ‡ä¸Šéƒ½æ¯”å¼€ç®±å³ç”¨çš„ FLAML è¡¨ç°æ›´å¥½ã€‚ç”±äºæˆ‘ä»¬å…³æ³¨çš„æ˜¯ç½‘ç»œå®‰å…¨åº”ç”¨ï¼Œä¸¤ä¸ªæœ€é‡è¦çš„æŒ‡æ ‡æ˜¯**æ£€æµ‹ç‡**å’Œ**è¯¯æŠ¥ç‡**ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ° LLM-guided æœç´¢æˆåŠŸæ˜¾è‘—æé«˜äº†æ£€æµ‹ç‡ï¼ŒåŒæ—¶ç•¥å¾®é™ä½äº†è¯¯æŠ¥ç‡ã€‚å› æ­¤ï¼Œé€šè¿‡ LLM-guided æœç´¢è®­ç»ƒçš„ XGBoost æ¨¡å‹å°†æ˜¯ä¸€ä¸ªä¼˜äº FLAML æœç´¢æ¨¡å‹çš„å¼‚å¸¸æ£€æµ‹å™¨ã€‚

æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºç»“è®ºï¼Œå¯¹äºæˆ‘ä»¬å½“å‰çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œåˆ©ç”¨åµŒå…¥åœ¨ LLM ä¸­çš„æœºå™¨å­¦ä¹ é¢†åŸŸä¸“ä¸šçŸ¥è¯†ç¡®å®å¯ä»¥åœ¨è¶…å‚æ•°è°ƒä¼˜ä¸­å¸¦æ¥ä»·å€¼ï¼Œå³ä½¿å®ƒä¸ç®€å•çš„æœç´¢ç®—æ³•é…å¯¹ã€‚

# 10\. ç»“è®º

åœ¨æœ¬åšå®¢ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†ä¸€ç§æ–°çš„ AutoML èŒƒå¼ï¼šLLM-guided è¶…å‚æ•°è°ƒä¼˜ã€‚è¿™é‡Œçš„å…³é”®æ€æƒ³æ˜¯å°†å¤§å‹è¯­è¨€æ¨¡å‹è§†ä¸ºæœºå™¨å­¦ä¹ ä¸“å®¶ï¼Œå¹¶åˆ©ç”¨å…¶æœºå™¨å­¦ä¹ é¢†åŸŸçŸ¥è¯†æ¥æå‡ºåˆé€‚çš„ä¼˜åŒ–æŒ‡æ ‡ã€å»ºè®®åˆå§‹æœç´¢ç©ºé—´ä»¥åŠæ¨èæœç´¢ç©ºé—´çš„ç»†åŒ–ã€‚

éšåï¼Œæˆ‘ä»¬å°†è¿™ç§æ–¹æ³•åº”ç”¨äºè¯†åˆ«ç½‘ç»œå®‰å…¨æ•°æ®é›†çš„æœ€ä½³ XGBoost æ¨¡å‹ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä¿¡æ¯é©±åŠ¨çš„è¶…å‚æ•°æœç´¢ï¼ˆå³ LLM-guided æœç´¢ï¼‰æ¯”çº¯ç®—æ³•é©±åŠ¨çš„ AutoML å·¥å…· FLAML äº§ç”Ÿäº†æ›´å¥½çš„å¼‚å¸¸æ£€æµ‹æ¨¡å‹ï¼Œè¾¾åˆ°äº†æ›´é«˜çš„æ£€æµ‹ç‡å’Œæ›´ä½çš„è¯¯æŠ¥ç‡ã€‚

å¦‚æœä½ è§‰å¾—æˆ‘çš„å†…å®¹æœ‰ç”¨ï¼Œä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://www.buymeacoffee.com/Shuaiguo09f)è¯·æˆ‘å–æ¯å’–å•¡ ğŸ¤—å’Œå¾€å¸¸ä¸€æ ·ï¼Œä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/ShuaiGuo16/LLM-guided-AutoML/tree/main)æ‰¾åˆ°åŒ…å«å®Œæ•´ä»£ç çš„ä¼´éšç¬”è®°æœ¬ğŸ’»å¦‚æœä½ æƒ³æ·±å…¥äº†è§£ï¼Œå¯ä»¥æŸ¥çœ‹ä»¥ä¸‹ä¸¤ç¯‡æœ€è¿‘çš„ç ”ç©¶è®ºæ–‡ï¼Œå®ƒä»¬ç ”ç©¶äº†ç›¸åŒçš„ä¸»é¢˜ï¼š

+   [AutoML-GPTï¼šä¸ GPT çš„è‡ªåŠ¨æœºå™¨å­¦ä¹ ](https://arxiv.org/pdf/2305.02499.pdf)

+   [å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ä»£çš„ AutoMLï¼šå½“å‰æŒ‘æˆ˜ã€æœªæ¥æœºä¼šä¸é£é™©](https://arxiv.org/pdf/2306.08107.pdf)

æ­¤å¤–ï¼Œå¦‚æœä½ å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„å…¶ä»–æœ‰è¶£åº”ç”¨æ„Ÿå…´è¶£ï¼Œå¯ä»¥çœ‹çœ‹æˆ‘ä¹‹å‰çš„åšå®¢ï¼š

+   æ„å»ºä¸€ä¸ª AI é©±åŠ¨çš„è¯­è¨€å­¦ä¹ åº”ç”¨

+   å¼€å‘è‡ªä¸»åŒèŠå¤©æœºå™¨äººç³»ç»Ÿä»¥æ¶ˆåŒ–ç ”ç©¶è®ºæ–‡

+   é€šè¿‡ç°å®ç”Ÿæ´»æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®ç§‘å­¦ä¸­çš„é—®é¢˜è§£å†³æŠ€èƒ½ã€‚

æœŸå¾…ä¸ä½ åˆ†äº«æ›´å¤šä»¤äººå…´å¥‹çš„ LLM é¡¹ç›®ã€‚æ•¬è¯·å…³æ³¨ï¼
