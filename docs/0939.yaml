- en: ChatGPT — Handle With Care
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/chat-gpt3-handle-with-care-8b6634781608?source=collection_archive---------15-----------------------#2023-03-13](https://towardsdatascience.com/chat-gpt3-handle-with-care-8b6634781608?source=collection_archive---------15-----------------------#2023-03-13)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Behind the Hype — Understanding what ChatGPT can do and what it cannot do, is
    critical to make the most of the technology. A recent research paper from the
    Center of Artificial Intelligence Research of Hong Kong University weights the
    limits and strengths of the OpenAi algorithm.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://misclassified.medium.com/?source=post_page-----8b6634781608--------------------------------)[![Giovanni
    Bruner](../Images/95283ba8cd3e700cdcb592975c501c47.png)](https://misclassified.medium.com/?source=post_page-----8b6634781608--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8b6634781608--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8b6634781608--------------------------------)
    [Giovanni Bruner](https://misclassified.medium.com/?source=post_page-----8b6634781608--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F14f159c7c0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchat-gpt3-handle-with-care-8b6634781608&user=Giovanni+Bruner&userId=14f159c7c0&source=post_page-14f159c7c0----8b6634781608---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8b6634781608--------------------------------)
    ·6 min read·Mar 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8b6634781608&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchat-gpt3-handle-with-care-8b6634781608&user=Giovanni+Bruner&userId=14f159c7c0&source=-----8b6634781608---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8b6634781608&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchat-gpt3-handle-with-care-8b6634781608&source=-----8b6634781608---------------------bookmark_footer-----------)![](../Images/ffc519eec3fa34c4ea5ccb1ca9d769bb.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Possessed Photography](https://unsplash.com/@possessedphotography?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 'First, it came the language model. The intuition was easy: the next word in
    a sequence of words can be modeled with a probability distribution and is heavily
    dependent on the previous words. Words are part of a vocabulary, a limited corpus
    (170,000 tokens in the English vocabulary). Each word has a limited amount of
    meanings. A sequence of words, following a slow-changing, inner set of metadata:
    a grammar. Which is a predictable structure. You can expect a verb to be followed
    by a noun and not by another verb. Grammar and Meaning, act as constraints to
    limit the amount of randomness in the next word prediction. Which is arguably
    an easier task than predicting the next day''s share price value of a thousand
    companies. Plus, language models are by nature auto-regressive, the next word
    prediction depends on the previous words, and there are not that many latent,
    unobservable, variables to account for.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，语言模型出现了。直觉上很简单：一个词序列中的下一个词可以通过概率分布来建模，并且严重依赖于前面的词。词汇是一个有限的语料库的一部分（英语词汇表中有170,000个词元）。每个词都有有限的含义。一个词序列遵循一个缓慢变化的内部元数据集：语法。这是一种可预测的结构。你可以预期动词后面跟着名词，而不是另一个动词。语法和意义作为约束，限制了下一个词预测的随机性。这可以说比预测一千家公司下一天的股价更简单。而且，语言模型本质上是自回归的，下一个词的预测依赖于前面的词，并且需要考虑的潜在、不可观测的变量并不多。
