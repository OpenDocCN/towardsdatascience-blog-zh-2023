["```py\npip install -q -U bitsandbytes\npip install -q -U git+https://github.com/huggingface/transformers.git \npip install -q -U git+https://github.com/huggingface/peft.git\n#pip install -q -U git+https://github.com/huggingface/accelerate.git\n#current version of Accelerate on GitHub breaks QLoRa\n#Using standard pip instead\npip install -q -U accelerate\npip install -q -U datasets\n```", "```py\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n```", "```py\nmodel_name = \"EleutherAI/gpt-neox-20b\"\n\n#Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```", "```py\nquant_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n```", "```py\nmodel = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quant_config, device_map={\"\":0})\n```", "```py\nmodel.gradient_checkpointing_enable()\n```", "```py\nfrom peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n\nmodel = prepare_model_for_kbit_training(model)\n\nconfig = LoraConfig(\n    r=8, \n    lora_alpha=32, \n    target_modules=[\"query_key_value\"], \n    lora_dropout=0.05, \n    bias=\"none\", \n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, config)\n```", "```py\nfrom datasets import load_dataset\ndata = load_dataset(\"Abirate/english_quotes\")\ndata = data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)\n```", "```py\nimport transformers\n\ntokenizer.pad_token = tokenizer.eos_token\n\ntrainer = transformers.Trainer(\n    model=model,\n    train_dataset=data[\"train\"],\n    args=transformers.TrainingArguments(\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=8,\n        warmup_steps=2,\n        max_steps=20,\n        learning_rate=2e-4,\n        fp16=True,\n        logging_steps=1,\n        output_dir=\"outputs\",\n        optim=\"paged_adamw_8bit\"\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\ntrainer.train()\n```", "```py\ntext = \"Ask not what your country\"\ndevice = \"cuda:0\"\ninputs = tokenizer(text, return_tensors=\"pt\").to(device)\n\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```", "```py\nAsk not what your country can do for you, ask what you can do for your country.”\n\n– John F.\n```"]