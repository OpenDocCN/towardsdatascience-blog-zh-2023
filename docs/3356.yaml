- en: 'Zephyr 7B Beta: A Good Teacher Is All You Need'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/zephyr-7b-beta-a-good-teacher-is-all-you-need-c931fcd0bfe7?source=collection_archive---------4-----------------------#2023-11-11](https://towardsdatascience.com/zephyr-7b-beta-a-good-teacher-is-all-you-need-c931fcd0bfe7?source=collection_archive---------4-----------------------#2023-11-11)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Knowledge distillation for Mistral 7B
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----c931fcd0bfe7--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----c931fcd0bfe7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c931fcd0bfe7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c931fcd0bfe7--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----c931fcd0bfe7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzephyr-7b-beta-a-good-teacher-is-all-you-need-c931fcd0bfe7&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----c931fcd0bfe7---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c931fcd0bfe7--------------------------------)
    ·8 min read·Nov 11, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc931fcd0bfe7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzephyr-7b-beta-a-good-teacher-is-all-you-need-c931fcd0bfe7&user=Benjamin+Marie&userId=ad2a414578b3&source=-----c931fcd0bfe7---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc931fcd0bfe7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzephyr-7b-beta-a-good-teacher-is-all-you-need-c931fcd0bfe7&source=-----c931fcd0bfe7---------------------bookmark_footer-----------)![](../Images/c9f381fbf16adcf625a9df3101552363.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image from [Pixabay](https://pixabay.com/illustrations/man-drinking-booze-drinker-5334659/)
  prefs: []
  type: TYPE_NORMAL
- en: Mistral 7B is one of the [best pre-trained large language modes (LLMs)](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).
    By releasing [Zephyr 7B Alpha](https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha),
    Hugging Face has demonstrated that Mistral 7B fine-tuned with DPO can outperform
    chat models that are 10 times bigger and even match the performance of GPT-4 for
    some tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the “Alpha” in the name of the model, Hugging Face was obviously planning
    to release better versions of Zephyr 7B. And they indeed released [Zephyr 7B Beta](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)
    only 2 weeks later. There is a technical report on arXiv describing the model
    and its evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Zephyr: Direct Distillation of LM Alignment](https://arxiv.org/abs/2310.16944)
    (Tunstall et al., 2023)'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will see what makes Zephyr 7B Beta better than larger LLMs.
    More particularly, we will see how Hugging Face leveraged larger LLMs, such as
    GPT-4, to teach Mistral 7B to answer instructions and align the answers with human
    preferences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Distillation: When Smaller LLMs Learn from Larger Ones'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since Hugging Face relied on knowledge distillation (KD) to train Zephyr, let’s
    have a brief reminder of what KD is in the context of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Most LLMs are trained on texts written by humans. Human texts present a high…
  prefs: []
  type: TYPE_NORMAL
