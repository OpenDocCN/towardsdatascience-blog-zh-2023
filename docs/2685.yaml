- en: A comparison of Temporal-Difference(0) and Constant-α Monte Carlo methods on
    the Random Walk Task
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间差(0)与常数-α蒙特卡洛方法在随机游走任务中的比较
- en: 原文：[https://towardsdatascience.com/a-comparison-of-temporal-difference-0-and-constant-%CE%B1-monte-carlo-methods-on-the-random-walk-task-bc6497eb7c92?source=collection_archive---------4-----------------------#2023-08-24](https://towardsdatascience.com/a-comparison-of-temporal-difference-0-and-constant-%CE%B1-monte-carlo-methods-on-the-random-walk-task-bc6497eb7c92?source=collection_archive---------4-----------------------#2023-08-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-comparison-of-temporal-difference-0-and-constant-%CE%B1-monte-carlo-methods-on-the-random-walk-task-bc6497eb7c92?source=collection_archive---------4-----------------------#2023-08-24](https://towardsdatascience.com/a-comparison-of-temporal-difference-0-and-constant-%CE%B1-monte-carlo-methods-on-the-random-walk-task-bc6497eb7c92?source=collection_archive---------4-----------------------#2023-08-24)
- en: '[](https://medium.com/@outerrencedl?source=post_page-----bc6497eb7c92--------------------------------)[![Tingsong
    Ou](../Images/459edc4bbd2353895acfb0f57eeddaa3.png)](https://medium.com/@outerrencedl?source=post_page-----bc6497eb7c92--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bc6497eb7c92--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bc6497eb7c92--------------------------------)
    [Tingsong Ou](https://medium.com/@outerrencedl?source=post_page-----bc6497eb7c92--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[![Tingsong Ou](../Images/459edc4bbd2353895acfb0f57eeddaa3.png)](https://medium.com/@outerrencedl?source=post_page-----bc6497eb7c92--------------------------------)
    [![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bc6497eb7c92--------------------------------)
    [Tingsong Ou](https://medium.com/@outerrencedl?source=post_page-----bc6497eb7c92--------------------------------)'
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa7aefc686327&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comparison-of-temporal-difference-0-and-constant-%CE%B1-monte-carlo-methods-on-the-random-walk-task-bc6497eb7c92&user=Tingsong+Ou&userId=a7aefc686327&source=post_page-a7aefc686327----bc6497eb7c92---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bc6497eb7c92--------------------------------)
    ·9 min read·Aug 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbc6497eb7c92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comparison-of-temporal-difference-0-and-constant-%25CE%25B1-monte-carlo-methods-on-the-random-walk-task-bc6497eb7c92&user=Tingsong+Ou&userId=a7aefc686327&source=-----bc6497eb7c92---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa7aefc686327&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comparison-of-temporal-difference-0-and-constant-%CE%B1-monte-carlo-methods-on-the-random-walk-task-bc6497eb7c92&user=Tingsong+Ou&userId=a7aefc686327&source=post_page-a7aefc686327----bc6497eb7c92---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bc6497eb7c92--------------------------------)
    ·9分钟阅读·2023年8月24日'
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbc6497eb7c92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comparison-of-temporal-difference-0-and-constant-%25CE%25B1-monte-carlo-methods-on-the-random-walk-task-bc6497eb7c92&source=-----bc6497eb7c92---------------------bookmark_footer-----------)![](../Images/ab9d686103362e65da2658ebd5e1455b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/ab9d686103362e65da2658ebd5e1455b.png)'
- en: Image generated by Midjourney with a paid subscription, which complies general
    commercial terms [1].
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由 Midjourney 生成的图像，使用了付费订阅，符合一般商业条款[1]。
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: The Monte Carlo (MC) and the Temporal-Difference (TD) methods are both fundamental
    technics in the field of reinforcement learning; they solve the prediction problem
    based on the experiences from interacting with the environment rather than the
    environment’s model. However, the TD method is a combination of MC methods and
    Dynamic Programming (DP), making it differs from the MC method in the aspects
    of the update rule, the bootstrapping, and the bias/variance. TD methods are also
    proven to have better performance and faster convergence compared to the MC in
    most cases.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡洛（MC）方法和时序差分（TD）方法都是强化学习领域的基本技术；它们通过与环境互动的经验解决预测问题，而不是依赖于环境模型。然而，TD 方法是 MC
    方法和动态规划（DP）的结合，因此在更新规则、引导和偏差/方差方面与 MC 方法有所不同。与 MC 方法相比，TD 方法在大多数情况下被证明具有更好的性能和更快的收敛速度。
- en: In this post, we’ll compare TD and MC, or more specifically, the TD(0) and constant-α
    MC methods, on a simple grid environment and a more comprehensive Random Walk
    [2] environment. Hoping this post can help readers interested in Reinforcement
    Learning better understand how each method updates the state-value function and
    how their performance differs in the same testing environment.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将比较 TD 和 MC，或更具体地说，比较 TD(0) 和常数-α MC 方法，在一个简单的网格环境和一个更全面的随机游走 [2] 环境中。希望这篇文章能够帮助对强化学习感兴趣的读者更好地理解每种方法如何更新状态值函数，以及它们在相同测试环境中的性能差异。
- en: 'We will implement algorithms and comparisons in Python, and libraries used
    in this post are as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 Python 中实现算法和比较，本帖中使用的库如下：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The difference between TD and MC
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TD 与 MC 的差异
- en: The introduction of TD(0) and constant-α MC
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TD(0) 和常数-α MC 的介绍
- en: 'The constant-α MC method is a regular MC method with a constant step size parameter
    α, and this constant parameter helps to make the value estimate more sensible
    to the recent experience. In practice, the choice of the α value depends on a
    trade-off between stability and adaptability. The following is the MC method’s
    equation for updating the state-value function at time t:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 常数-α MC 方法是一种常数步长参数 α 的常规 MC 方法，这个常数参数有助于使价值估计对最近的经验更加敏感。在实践中，α 值的选择取决于稳定性和适应性之间的权衡。以下是
    MC 方法在时间 t 更新状态值函数的方程：
- en: '![](../Images/13431a73d76db109b060dd58241e7e67.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13431a73d76db109b060dd58241e7e67.png)'
- en: 'The TD(0) is a special case of TD(λ) that only looks one step ahead and is
    the simplest form of TD learning. This method updates the state-value function
    with TD error, the difference between the estimated value of the state and the
    reward plus the estimated value of the next state. A constant step size parameter
    α works the same as in the MC method above. The following is the TD(0)’s equation
    for updating the state-value function at time t:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: TD(0) 是 TD(λ) 的一个特例，它只看一步前的状态，是最简单的 TD 学习形式。该方法使用 TD 误差更新状态值函数，TD 误差是指状态的估计值与奖励加上下一个状态的估计值之间的差异。一个常数步长参数
    α 与上述 MC 方法中的作用相同。以下是 TD(0) 在时间 t 更新状态值函数的方程：
- en: '![](../Images/07e6c9dc15bef8c2e744d21213d111e3.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07e6c9dc15bef8c2e744d21213d111e3.png)'
- en: 'Generally speaking, the difference between MC and TD methods happens on three
    aspects:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，MC 和 TD 方法之间的差异体现在三个方面：
- en: '**Update rule:** MC methods update values only after the episode ends; this
    could be problematic if the episode is very long, which slows down the program,
    or in the continuing task that does not have episodes at all. On the contrary,
    TD method updates value estimates at each time step; this is online learning and
    can be particularly useful in continuing tasks.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**更新规则**：MC 方法仅在回合结束后更新值；如果回合非常长，这可能会导致程序变慢，或者在没有回合的持续任务中，这可能会成为问题。相反，TD 方法在每个时间步更新价值估计；这是一种在线学习，特别适用于持续任务。'
- en: '**Bootstrapping**: The term “bootstrapping” in reinforcement learning refers
    to updating value estimates based on other value estimates. TD(0) method bases
    its update on the value of the following state, so it is a bootstrapping method;
    on the contrary, MC does not use bootstrapping as it updates value directly from
    the returns (G).'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**引导**：在强化学习中，“引导”一词指的是基于其他价值估计来更新价值估计。TD(0) 方法基于下一个状态的价值来进行更新，因此它是一种引导方法；相反，MC
    不使用引导，因为它直接从回报（G）中更新价值。'
- en: '**Bias/Variance**: MC methods are unbiased because they estimate the value
    by weighing the actual returns observed without making estimates during the episode;
    however, MC methods have high variance, especially when the number of samples
    is low. On the contrary, TD methods have biases because they use bootstrapping,
    and the bias can vary based on the actual implementation; TD methods have low
    variance because it uses the immediate reward plus the estimate of the next state,
    which smooths out the fluctuation that raises from the randomness in rewards and
    actions.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**偏差/方差**：MC 方法没有偏差，因为它们通过加权实际观察到的回报来估计值，而在过程中不进行估计；然而，MC 方法有较高的方差，尤其是在样本数量较少时。相反，TD
    方法有偏差，因为它们使用了自助法，偏差可能会根据实际实现而有所不同；TD 方法方差较低，因为它使用了即时奖励加上对下一状态的估计，这平滑了因奖励和行动的随机性引起的波动。'
- en: Evaluating TD(0) and constant-α MC on simple gridworld setup
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在简单的网格世界设置中评估 TD(0) 和常数-α MC
- en: To make their difference more straightforward, we can set up a simple Gridworld
    test environment with two fixed trajectories, run both algorithms on the setup
    until converged, and check how they update the values differently.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使它们的差异更加直观，我们可以设置一个简单的网格世界测试环境，具有两个固定轨迹，运行这两种算法直到收敛，并检查它们如何不同地更新值。
- en: 'First we can setup the testing environment with the following codes:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以用以下代码设置测试环境：
- en: '![](../Images/035c80a603c34f01682cf5eda5591d09.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/035c80a603c34f01682cf5eda5591d09.png)'
- en: 'Figure 1 Left: environment setup. Right: preset paths. Source: figure by the
    author'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1 左：环境设置。右：预设路径。来源：作者绘制的图
- en: 'The left figure above shows a simple gridworld environment setup. All the colored
    cells represent terminal states; the agent gets a +1 reward when stepping into
    the red cell but gets a -1 reward when stepping into blue cells. All other steps
    on the grid return a reward of zero. The right figure above marks two preset paths:
    one arrives at the blue cell while another stops at the red cell; the intersection
    of the paths helps maximize the value difference between the two methods.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 上图左侧显示了一个简单的网格世界环境设置。所有彩色单元格表示终端状态；代理在进入红色单元格时获得 +1 奖励，但在进入蓝色单元格时获得 -1 奖励。网格上的所有其他步骤返回零奖励。上图右侧标记了两个预设路径：一条到达蓝色单元格，另一条停在红色单元格；路径的交点有助于最大化两种方法之间的值差异。
- en: Then we can use the equations in the previous section to evaluate the environment.
    We do not discount the return nor the estimate, and set the α to a small value
    1e-3\. When the absolute sum of the value increments is lower than a threshold
    of 1e-3, we consider the values converged.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以使用上一节中的方程来评估环境。我们不对回报或估计进行折扣，并将 α 设置为一个小值 1e-3。当值增量的绝对和低于 1e-3 的阈值时，我们认为值已收敛。
- en: 'The result of the evaluation is as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 评估结果如下：
- en: '![](../Images/e8aa51108e8a85e174624eb62ccfa1c4.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e8aa51108e8a85e174624eb62ccfa1c4.png)'
- en: 'Figure 2 The result of TD(0) and constant-alpha MC evaluations. Source: figure
    by the author'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2 TD(0) 和常数-α MC 评估结果。来源：作者绘制的图
- en: The two algorithms’ different ways of estimating values become pretty apparent
    in the image above. The MC method is loyal to the returns of the path, so the
    values on each path directly represent how it ends. Nevertheless, the TD method
    provides a better prediction, especially on the blue path — the values on the
    blue path before the intersection also indicate the possibility of getting to
    the red cell.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图像中，两种算法在估计值的方式上的不同变得非常明显。MC 方法忠实于路径的回报，因此每条路径上的值直接表示其结束状态。然而，TD 方法提供了更好的预测，特别是在蓝色路径上——在交点之前的蓝色路径上的值也表示到达红色单元格的可能性。
- en: With this minimal case in mind, we are ready to move to a much more complicated
    example and try to find out the difference in performance between the two methods.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以这个最小的案例为基础，我们准备转向一个更复杂的示例，尝试找出两种方法之间的性能差异。
- en: The Random Walk Task
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机游走任务
- en: The Random Walk task is a simple *Markov reward process* proposed by Sutton
    et al. for TD and MC prediction purposes[2], as the images below show. In this
    task, the agent starts from the center node C. The agent takes a right or left
    step with equal probability on each node. There are two terminal states on both
    ends of the chain. The reward of getting into the left end is 0, and getting into
    the right end is +1\. All the steps before the termination generate a reward of
    0.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 随机游走任务是Sutton等人提出的一个简单的*马尔可夫奖励过程*，用于TD和MC预测目的[2]，如下图所示。在此任务中，代理从中心节点C开始。代理在每个节点上以相等的概率向右或向左迈一步。链条的两端有两个终止状态。进入左端的奖励为0，进入右端的奖励为+1。在终止之前的所有步骤生成的奖励为0。
- en: '![](../Images/b9ba3d8ff93a94f91a9f551d52a30843.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b9ba3d8ff93a94f91a9f551d52a30843.png)'
- en: 'Figure 3 Random Walk. Source: figure by the author'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图3 随机游走。来源：作者提供的图
- en: 'And we can use the following code to create the Random Walk environment:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码来创建随机游走环境：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The true value for each node of the environment under a random policy is [1/6,
    2/6, 3/6, 4/6, 5/6]. The value was calculated by the policy evaluation with Bellam
    equation:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机策略下，环境中每个节点的真实值为[1/6, 2/6, 3/6, 4/6, 5/6]。该值通过使用贝尔曼方程的策略评估计算得出：
- en: '![](../Images/482b1eb6918db8f72ff9f6a2eec4812c.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/482b1eb6918db8f72ff9f6a2eec4812c.png)'
- en: Our task here is to find how close the values estimated by both algorithms are
    to the true value; we can arbitrarily assume that the algorithm produces a closer
    value function to the true value function, measured by the averaged root mean
    square error (RMS), indicating a better performance.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的任务是找出两个算法估计的值与真实值的接近程度；我们可以任意假设算法产生的值函数离真实值函数更近，通过平均均方根误差（RMS）来衡量，表示性能更好。
- en: The Performance of TD(0) and Constant-a MC on the Random Walk
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TD(0)和常数-a MC在随机游走中的表现
- en: Algorithms
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 算法
- en: 'With the environment ready, we can start running both methods on the Random
    Walk environment and compare they performance. First let’s take a look at both
    algorithms:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 环境准备好后，我们可以开始在随机游走环境中运行这两种方法，并比较它们的表现。首先，让我们看一下这两个算法：
- en: '![](../Images/8ea1e405f6d2f5574118c28586af4257.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8ea1e405f6d2f5574118c28586af4257.png)'
- en: 'Source: Algorithm written in [latex](https://gist.github.com/terrence-ou/10dc3571be8fb9ae9dad92a7505633b6)
    by author'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[latex](https://gist.github.com/terrence-ou/10dc3571be8fb9ae9dad92a7505633b6)中由作者编写的算法
- en: '![](../Images/6b0e1dd91eb93046dfaa8b54d3ca21d5.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b0e1dd91eb93046dfaa8b54d3ca21d5.png)'
- en: 'Source: Algorithm written in [latex](https://gist.github.com/terrence-ou/5ab148962748c371ccd60b35c4feec51)
    by author'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[latex](https://gist.github.com/terrence-ou/5ab148962748c371ccd60b35c4feec51)中由作者编写的算法
- en: 'As mentioned earlier, the MC method should wait until the episode ends to update
    the values from the tail of the trajectory, while the TD method updates the values
    incrementally. This difference brings a trick when initializing the state-value
    function: In MC, the state-value function does not include the terminal states,
    while in TD(0), the function should include the terminal state with the value
    of 0 because the TD(0) method always looks one step ahead before the episode ends.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，MC方法应该等到回合结束后才能更新从轨迹尾部得到的值，而TD方法则是逐步更新值。这种差异带来了初始化状态值函数时的一个技巧：在MC中，状态值函数不包括终止状态，而在TD(0)中，该函数应包括终止状态，并且值为0，因为TD(0)方法总是提前一步看未来的状态，直到回合结束。
- en: Implementation
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现
- en: 'The α parameter selection in this implementation references the that proposed
    in the book [2]; the parameters for the MC method are [0.01, 0.02, 0.03, 0.04]
    while that for the TD method are [0.05, 0.10, 0.15]. I wondered why the author
    didn’t choose the same parameter set on both algorithms until I ran the MD method
    with parameters for TD: the TD parameters are too high for the MC method and thus
    cannot unveil the MC’s best performance. Therefore, we will stick with the book’s
    setup in the parameter sweep. Now, let’s run both algorithms to find out their
    performance on the Random Walk setup.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在此实现中的α参数选择参考了书中[2]提出的参数；MC方法的参数为[0.01, 0.02, 0.03, 0.04]，而TD方法的参数为[0.05, 0.10,
    0.15]。我曾经疑惑为何作者没有在两个算法中选择相同的参数集，直到我用TD参数运行MD方法：TD参数对MC方法来说太高，因此不能展现MC的最佳性能。因此，我们将坚持书中的参数设置。现在，让我们运行这两个算法，找出它们在随机游走设置下的表现。
- en: Result
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: '![](../Images/eac2e265f1bfe8545a65cb8f5dc1187b.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eac2e265f1bfe8545a65cb8f5dc1187b.png)'
- en: 'Figure 4 Algorithm comparison result. Source: figure by the author'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图4 算法比较结果。来源：作者提供的图
- en: The result after 100 runs of comparison is as the image above shows. The TD
    method generally yields better value estimations than the MC methods, and the
    TD with α = 0.05 can get really close to the true value. The graph also shows
    the MC method has a higher variance compared to the TD ones, as the orchid lines
    fluctuate more than the steel blue lines.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 100 次比较后的结果如上图所示。TD 方法通常比 MC 方法提供更好的值估计，且α = 0.05 的 TD 方法可以非常接近真实值。图表还显示，MC
    方法的方差比 TD 方法更高，因为兰花线的波动大于钢蓝线。
- en: It is worth noticing that for both algorithms, when the α is (relatively) high,
    the RMS loss first goes down and then up again. Such a phenomenon is due to the
    combined effect of the value initialization and the α value. We initialized a
    relatively high value of 0.5, which is higher than the true value of Nodes A and
    B. As the random policy makes a 50% chance of choosing a “wrong” step, which takes
    the agent away from the right terminal state, a higher α value will also emphasize
    the wrong steps and make the result away from the true value.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，对于这两种算法，当α值（相对）较高时，RMS 损失首先下降然后再上升。这种现象是由于值初始化和α值的共同作用。我们初始化了一个相对较高的
    0.5，超过了节点 A 和 B 的真实值。由于随机策略使得有 50% 的机会选择“错误”步骤，从而使智能体远离正确的终端状态，因此更高的α值也会强调错误步骤，使结果偏离真实值。
- en: 'Now let’s try to reduce the initial value to 0.1 and run the comparison again,
    see if the problem alleviated:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试将初始值降低到 0.1，并再次进行比较，看看问题是否得到缓解：
- en: '![](../Images/b1c80040596857301b3f6e5d2318af9c.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b1c80040596857301b3f6e5d2318af9c.png)'
- en: 'Figure 5 Algorithm comparison result with initial value 0.1\. Source: figure
    by the author'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5 初始值为 0.1 的算法比较结果。来源：作者绘制的图
- en: The lower initial value apparently helps relieve the problem; there are no noticeable
    “go down, then go up” effects. However, the side effect of a lower initial value
    is that the learning is less efficient, as the RMS loss never goes below 0.05
    after 150 episodes. Therefore, there is a trade-on between the initial value,
    the parameter, and the algorithm’s performance.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 较低的初始值显然有助于缓解问题；没有明显的“下降然后上升”现象。然而，较低的初始值的副作用是学习效率较低，因为 RMS 损失在 150 轮后从未低于 0.05。因此，初始值、参数和算法性能之间存在权衡。
- en: Batch Training
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批量训练
- en: One last piece I want to bring up in this post is the comparison of batch training
    on both algorithms.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我想提到的最后一点是对两种算法的批量训练比较。
- en: 'Consider we’re facing the following situation: we only accumulated a limited
    number of experiences on the Random Walk task, or we can only run a certain number
    of episodes due to the limitation of time and computation. The idea of batch updating
    [2] was proposed to deal with such a situation by fully utilizing the existing
    trajectories.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们面临以下情况：我们只在随机游走任务上积累了有限数量的经验，或者由于时间和计算限制，我们只能运行一定数量的轮次。批量更新 [2] 的想法是通过充分利用现有轨迹来应对这种情况。
- en: The idea of batch training is to update the values on a batch of trajectories
    repeatedly until the values converge to an answer. The values will only be updated
    once all batch experiences are fully processed. Let’s implement the batch training
    for both algorithms on the Random Walk environment to see if the TD method still
    performs better than the MC method.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 批量训练的想法是反复更新一批轨迹上的值，直到值收敛到一个答案。只有在所有批次经验完全处理后，值才会被更新。让我们在随机游走环境中对这两种算法实施批量训练，看看
    TD 方法是否仍优于 MC 方法。
- en: Result
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: '![](../Images/9fa0460989237ec36464dafe755bbc81.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9fa0460989237ec36464dafe755bbc81.png)'
- en: 'Figure 6 The batch training result. Source: figure by the author'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6 批量训练结果。来源：作者绘制的图
- en: The result of batch training shows the TD method is still doing better than
    the MC method with limited experience, and the gap between the performance of
    the two algorithms is quite apparent.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 批量训练结果显示，TD 方法在有限经验下仍优于 MC 方法，两种算法的性能差距十分明显。
- en: Conclusion
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'In this post, we discussed the difference between the constant-α MC method
    and TD(0) methods and compared their performance on the Random Walk task. The
    TD method overrides the MC methods in all the tests in this post, so considering
    TD as a method for reinforcement learning tasks is a preferable choice. However,
    it doesn’t mean that TD is always better than MC, as the latter has a most obvious
    advantage: no bias. If we’re facing a task that has no tolerance for biases, then
    MC could be a better choice; otherwise, TD could better deal with general cases.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们讨论了常数-α MC 方法和 TD(0) 方法之间的区别，并比较了它们在随机游走任务中的表现。TD 方法在本文所有测试中都优于 MC
    方法，因此将 TD 作为强化学习任务的方法是一个更可取的选择。然而，这并不意味着 TD 总是优于 MC，因为后者有一个最明显的优势：无偏差。如果我们面对的是一个不能容忍偏差的任务，那么
    MC 可能是更好的选择；否则，TD 更能处理一般情况。
- en: Reference
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Midjourney Terms of Service: [https://docs.midjourney.com/docs/terms-of-service](https://docs.midjourney.com/docs/terms-of-service)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Midjourney 服务条款: [https://docs.midjourney.com/docs/terms-of-service](https://docs.midjourney.com/docs/terms-of-service)'
- en: '[2] Sutton, Richard S., and Andrew G. Barto. *Reinforcement learning: An introduction*.
    MIT Press, 2018.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Sutton, Richard S., 和 Andrew G. Barto. *强化学习：导论*。麻省理工学院出版社，2018。'
- en: 'My GitHub repo of this post: [[Link](https://github.com/terrence-ou/Reinforcement-Learning-2nd-Edition-Notes-Codes/tree/main/chapter_06_temporal_difference_learning)].'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的 GitHub 仓库：[链接](https://github.com/terrence-ou/Reinforcement-Learning-2nd-Edition-Notes-Codes/tree/main/chapter_06_temporal_difference_learning)。
