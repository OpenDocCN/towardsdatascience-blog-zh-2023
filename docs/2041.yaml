- en: CI/CD for Multi-Model Endpoints in AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/ci-cd-for-multi-model-endpoints-in-aws-18bf939e0a48?source=collection_archive---------7-----------------------#2023-06-22](https://towardsdatascience.com/ci-cd-for-multi-model-endpoints-in-aws-18bf939e0a48?source=collection_archive---------7-----------------------#2023-06-22)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A simple, flexible alternative for sustainable ML solutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@andrewcharabin?source=post_page-----18bf939e0a48--------------------------------)[![Andrew
    Charabin](../Images/8cfe2657a9cd16c3ce30b98e3c9e9945.png)](https://medium.com/@andrewcharabin?source=post_page-----18bf939e0a48--------------------------------)[](https://towardsdatascience.com/?source=post_page-----18bf939e0a48--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----18bf939e0a48--------------------------------)
    [Andrew Charabin](https://medium.com/@andrewcharabin?source=post_page-----18bf939e0a48--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff282e085f18e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fci-cd-for-multi-model-endpoints-in-aws-18bf939e0a48&user=Andrew+Charabin&userId=f282e085f18e&source=post_page-f282e085f18e----18bf939e0a48---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----18bf939e0a48--------------------------------)
    ·14 min read·Jun 22, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F18bf939e0a48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fci-cd-for-multi-model-endpoints-in-aws-18bf939e0a48&user=Andrew+Charabin&userId=f282e085f18e&source=-----18bf939e0a48---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F18bf939e0a48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fci-cd-for-multi-model-endpoints-in-aws-18bf939e0a48&source=-----18bf939e0a48---------------------bookmark_footer-----------)![](../Images/c1424ad7b2588399204bb9dd6c2e504c.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image via VectorStock under license to Andrew Charabin
  prefs: []
  type: TYPE_NORMAL
- en: Automating the retraining and deployment of production machine learning solutions
    is a pivotal step to ensure models account for [covariate shift](https://www.seldon.io/what-is-covariate-shift#:~:text=It%20is%20when%20the%20distribution,issue%20encountered%20in%20machine%20learning.)
    while limiting error-prone and unnecessary human effort.
  prefs: []
  type: TYPE_NORMAL
- en: For models deployed using the AWS stack and particularly SageMaker, AWS offers
    a standard CI/CD solution using [SageMaker Pipelines](https://aws.amazon.com/sagemaker/pipelines/)
    to automate retraining/deployment, and the [SageMaker Model Registry](https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html)
    to track lineage of a model.
  prefs: []
  type: TYPE_NORMAL
- en: 'While the standard solution works well for standard cases, there are several
    limitations for more intricate cases:'
  prefs: []
  type: TYPE_NORMAL
- en: Input data needs to be sourced from AWS s3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Difficulty setting up dynamic warm start hyperparameter tuning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Additional model training steps are required to train multiple models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Long bootstrapping time to execute a pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Limited debugging tools.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Fortunately, AWS has launched new functionality that can be used to build a
    CI/CD pipeline that overcomes these limitations. The following functionality can
    be accessed within [SageMaker Studio](https://aws.amazon.com/sagemaker/studio/),
    AWS’ integrated development environment for machine learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Use a Custom SageMaker Image](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-byoi-launch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Git/SageMaker Studio Integration](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tasks-git.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Warm Start Hyperparameter Tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-warm-start.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SageMaker Studio Notebook Jobs](https://aws.amazon.com/blogs/machine-learning/operationalize-your-amazon-sagemaker-studio-notebooks-as-scheduled-notebook-jobs/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Cross-Account Model Registry](https://aws.amazon.com/blogs/machine-learning/build-a-cross-account-mlops-workflow-using-the-amazon-sagemaker-model-registry/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***The purpose of this article…***'
  prefs: []
  type: TYPE_NORMAL
- en: is to go over key details of an alternative CI/CD solution via the AWS cloud
    that provides more flexibility and faster speed to market.
  prefs: []
  type: TYPE_NORMAL
- en: '***Solution Component Overview:***'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*1\. Custom SageMaker Studio image for PostgreSQL querying*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*2\. Dynamic warm start hyperparameter tuning*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*3\. Register multiple models to the Model Registry in a single interactive
    python notebook*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*4\. Refresh a multi-model endpoint with new models*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*5\. Schedule retrain/redeploy notebooks to run on a set cadence*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Custom SageMaker Studio image *for PostgreSQL querying*
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While SageMaker pipelines allows input data from s3, what if new input data
    resides in a data warehouse like AWS Redshift or Google BigQuery? Of course, an
    [ETL](https://www.ibm.com/topics/etl) or comparable process can be used to move
    data to s3 in batches, but that simply adds unnecessary complexity/rigidity in
    comparison to querying the data directly from the data warehouse in the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker Studio provides several default images to initialize an environment,
    one example being ‘Data Science’ which includes common packages like numpy and
    pandas. However, to connect to a PostgreSQL database in Python, a driver or adapter
    is required. [Psycopg2](https://pypi.org/project/psycopg2/) is the most popular
    PostgreSQL database adapter for the Python programming language. Fortunately,
    custom images can be used to initialize a Studio environment although there are
    specific requirements. I’ve prepackaged a Docker image that meets these requirements
    and build on top of the Python Julia-1.5.2 image by adding the psycopg2 driver.
    The image can be found in [this](https://github.com/acharabin/SageMaker-Studio-Image-With-Psycopg2)
    git repository. The steps outlined [here](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-byoi.html)
    can then be used to make the image accessible in a Studio domain.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Dynamic warm start hyperparameter tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Model retraining is different in nature than initial model training. It isn’t
    practical to invest the same amount of resources to search for the best model
    hyperparameters and over the same large search space when retraining a model.
    This is especially true when only minor adjustments to the best hyperparameters
    are expected from the last production model.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this reason, the hyperparameter tuning solution recommended for CI/CD in
    this article doesn’t try to blitz retuning with K fold cross-validation, warm
    pools, etc. All that can work great for an initial model training. For retraining,
    however, we want to start with what worked great in production already, and make
    small adjustments to account for newly available data. As such, using warm start
    hyperparameter tuning is the perfect solution. Going further, a dynamic warm start
    tuning system can be created that uses the latest production tuning job as the
    parent. The solution can look as follows for an example XGBoost baysian tuning
    job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Tuning job history will be saved in a log file in the base directory, with
    example output as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/329069d473afac7188601c22592ee081.png)'
  prefs: []
  type: TYPE_IMG
- en: Chart by author
  prefs: []
  type: TYPE_NORMAL
- en: The date/time stamp as well as the name of the tuning job and metadata are stored
    in .csv format, with new tuning jobs being appended to the file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The system will dynamically warm start using the latest tuning job that meets
    required conditions. In this example the conditions are noted in the following
    line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Because we’ll want to test the pipeline works, a testing=True run option is
    available that forces only one hyperparameter tuning job. A condition is added
    to only consider jobs with more than 1 tuned model as parents, given these are
    non-testing. Furthermore, the tuning job log file can be used across different
    models, as one could in theory use a parent job across models. In this case the
    model is tracked with the ‘metric’ field, and eligible tuning jobs are filtered
    to match the metric in the current training instance.
  prefs: []
  type: TYPE_NORMAL
- en: Once the retraining has been done, we’ll then append the log file with the new
    hyperparameter tuning job and write it locally as well as to s3 with [versioning](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html)
    turned on.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**3\. Register multiple models to the Model Registry in a single interactive
    python notebook**'
  prefs: []
  type: TYPE_NORMAL
- en: Often, organizations will have multiple AWS accounts for different use cases
    (i.e. sandbox, QA, and production). You’ll need to determine which account to
    use for each for each step of the CI/CD solution, then add the cross-account permissions
    noted in [this guide](https://aws.amazon.com/blogs/machine-learning/build-a-cross-account-mlops-workflow-using-the-amazon-sagemaker-model-registry/).
  prefs: []
  type: TYPE_NORMAL
- en: The recommendation is to perform model training and model registration in the
    same account, specifically a sandbox or testing account. So in the below chart
    the ‘Data Science’ and ‘Shared Services’ account will be the same. Within this
    account an s3 bucket will be needed to house model artifacts and track lineage
    on other files related to the pipeline. Models/endpoints will be deployed separately
    within each ‘deployment’ account (i.e. sandbox, QA, production) by referencing
    the model artifacts and the registry in the training/registration account.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/62a4d64e3bf6d781dd23ffc4aeb9bfa0.png)'
  prefs: []
  type: TYPE_IMG
- en: Chart from [AWS Documentation](https://aws.amazon.com/blogs/machine-learning/build-a-cross-account-mlops-workflow-using-the-amazon-sagemaker-model-registry/)
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve decided which AWS account will be used for training and to house
    the model registry, we can now build an initial model and develop the CI/CD solution.
  prefs: []
  type: TYPE_NORMAL
- en: When using SageMaker Pipelines, separate pipeline steps are created for data
    preprocessing, training/tuning, evaluation, registration, and any post-processing.
    While that’s fine for a single model pipeline, it creates a lot of pipeline code
    duplicity when there are multiple models required for a machine learning solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a result, the recommended solution is instead to build and schedule three
    interactive python notebooks in SageMaker Studio. They are run in sequence and
    together accomplish the CI/CD pipeline once automated with a notebook job:'
  prefs: []
  type: TYPE_NORMAL
- en: '***A. Data preparation***'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '***B. Model training, evaluation, and registration***'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '***C. Endpoint refresh with the latest approved models***'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A. Data preparation
  prefs: []
  type: TYPE_NORMAL
- en: Here we will query and load data from the data warehouse and write it locally
    and to s3\. We can set dynamic date/time conditions using the current date and
    pass the resulting date floor and ceiling into the SQL query.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This step ends with saving the prepared data for training locally as well as
    in s3 for lineage tracking.
  prefs: []
  type: TYPE_NORMAL
- en: B. Model training, evaluation, and registration
  prefs: []
  type: TYPE_NORMAL
- en: By using an interactive python notebook in Studio, we can now complete model
    training, evaluation, and registration all in one notebook. All these steps can
    be built into a function and that is applied for additional models that need to
    be retrained. For illustrative purposes, the code has been provided without using
    a function.
  prefs: []
  type: TYPE_NORMAL
- en: Prior to proceeding, model package groups need to be created in the Registry
    (either in the console or via Python) for each model that’s part of the solution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: By opening the model package group in the registry, you can see all the model
    versions that have been registered, the date they were registered, and their approval
    status.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0a582302948d0320e279dda5eb3f2bb5.png)'
  prefs: []
  type: TYPE_IMG
- en: Chart from [AWS Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/modelregistryfaq.html)
  prefs: []
  type: TYPE_NORMAL
- en: The supervisor of the pipeline can then review the evaluation report saved locally
    in the previous step, which contains the history of all past model evaluations,
    and determine if they’d like to approve or deny the model based on the testing
    set evaluation metrics. Later on, criteria can be set to only update production
    (or QA) endpoints with the latest model if it was approved.
  prefs: []
  type: TYPE_NORMAL
- en: '**4\. Refreshing a multi-model endpoint with new models**'
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker has a [MultiDataModel](https://sagemaker.readthedocs.io/en/stable/api/inference/multi_data_model.html)
    class that allows deploying SageMaker endpoints that can host more than one model.
    The rationale is that multiple models can be loaded in the same compute instance,
    sharing resources and saving costs. Furthermore, it simplifies model retraining/admin
    as only one endpoint needs to be reflected with the new models and managed, vs.
    having to duplicate steps across each dedicated endpoint (which can be done as
    an alternative). The MultiDataModel class can also be used to deploy a single
    model, which could make sense if there are plans to add additional models to the
    solution in the future.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll need to create the model and endpoint on the first go in the training
    account. The MultiDataModel class requires a location to store model artifacts
    that can be loaded into the endpoint when they are invoked; below we’ll use the
    ‘models’ directory in the s3 bucket being used.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, the MultiDataModel can be referenced as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Models can be added to the MultiDataModel by copying the artifact to the {s3
    bucket}/models directory which the endpoint will use to load models. All we need
    is the model package group name and the Model Registry will provide the respective
    source artifact location and approval status.
  prefs: []
  type: TYPE_NORMAL
- en: We can add a condition to only add the latest model if it’s approved, illustrated
    below. This condition may be omitted in the sandbox account in case an immediate
    deploy is needed for data science QA and to ultimately approve the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then list the models that have been added with the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: To remove a model, one can navigate to the associated s3 directory in the console
    and delete any of them; they will be gone when relisting the available models.
  prefs: []
  type: TYPE_NORMAL
- en: 'A model can be invoked in the deployed endpoint once it’s been added by using
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Upon the first invocation of a model, the endpoint will load the target model,
    resulting in additional latency. For future invocations where the model is already
    loaded, inferences will be obtained immediately. In the [multi-model endpoint
    developer guide](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html),
    AWS notes that models which that haven’t been invoked recently will be ‘unloaded’
    when the endpoint reaches a memory utilization threshold. The models will then
    be reloaded upon their next invocation.
  prefs: []
  type: TYPE_NORMAL
- en: When an existing model artifact is overwritten via mme.add_model() or in the
    s3 console, the deployed endpoint won’t be reflected immediately. To force the
    endpoint to reload the latest model artifacts upon their next invocation, we can
    use a trick of updating the endpoint with an arbitrary new endpoint configuration.
    This creates a new endpoint where models need to be loaded, and safely manages
    the transition between the old and new endpoint. Because each endpoint configuration
    requires a unique name, we can add a suffix with a date/time stamp.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Once this code is run, you’ll see that the associated endpoint will have an
    ‘updating’ status when viewing it in the console. During this updating period,
    the previous endpoint will be available for use, and it will be swapped with the
    new endpoint once it’s ready, after which the status will adjust to ‘in service.’
    The new models added will then be loaded upon their next invocation.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve now built out the three notebooks required for the CI/CD solution — data
    preparation, training/evaluation, and endpoint updating. However, these files
    are currently only in the training AWS account. We need to adapt the third notebook
    to work in any deployment AWS account where a respective endpoint will be created/updated.
  prefs: []
  type: TYPE_NORMAL
- en: To do this we can add conditional logic based on the AWS Account ID. s3 buckets
    will also be required in the new AWS accounts to house model artifacts. Since
    s3 bucket names need to be unique across AWS, such conditional logic can be used
    for this. It can also be applied to adjust the endpoint instance type and conditions
    for adding new models (i.e. approval status).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The steps to initially create and deploy the MultiDataModel will need to be
    repeated in each new deployment account.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have one working notebook that references the AWS Account ID and
    can be run across different AWS accounts, we’ll want to set up a git repo that
    contains this notebook (and likely the other two for lineage tracking), then clone
    the repo in the SageMaker Studio domains of these accounts. Fortunately, with
    the Studio/Git integration these steps are straightforward/seamless and are outlined
    in the following [document](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tasks-git.html).
    Based on my experience, it’s recommended to create the repo outside of SageMaker
    Studio and clone it within each AWS account domain.
  prefs: []
  type: TYPE_NORMAL
- en: Any future changes to the notebooks can be done in the training account and
    pushed to the repo. They can then be reflected in the other deployment accounts
    by pulling in the changes. Make sure to create a .gitignore file so only the 3
    notebooks are considered vs. any of the log or other files; lineage there will
    be tracked in s3\. Furthermore, one should recognize that anytime a notebook is
    run the console output will change. To avoid conflicts when pulling the file changes
    in the other deployments accounts, any file changes since the last pull in these
    accounts should be restored prior to pulling the latest updates.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. *Schedule retrain/redeploy notebooks to run on a set cadence*
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally we can schedule all three notebooks to run concurrently in the training
    account. We can use the new [SageMaker Studio notebook jobs](https://aws.amazon.com/blogs/machine-learning/operationalize-your-amazon-sagemaker-studio-notebooks-as-scheduled-notebook-jobs/)
    feature to do this. The schedules should be considered as environment/account
    dependent — i.e. in the deployment accounts we can create separate notebook jobs
    but now just to update endpoints with the latest models, and provide some lag-time
    between when newly approved models are automatically deployed in the sandbox,
    QA, and production accounts. The beauty is that the only manual part of the process
    once the solution is released becomes model approval/denying in the registry.
    And if anything goes wrong with a newly deployed model, the model can be denied/deleted
    in the registry after which the endpoint update notebook can be manually run to
    revert to the previous production model version, buying time for further investigation.
    In this case, we set the pipeline to run on set time intervals (i.e. monthly/quarterly),
    although this solution can be adapted to work upon conditions (i.e. data drift
    or declining production model accuracy).
  prefs: []
  type: TYPE_NORMAL
- en: Closing remarks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CI/CD is currently hot topic in the machine learning operations space. This
    is warranted as many times less thought occurs on the continuity of a machine
    learning solution after it’s initial deployment. To ensure production machine
    learning solutions are robust to covariate drift and are sustainable over time,
    a simple and flexible CI/CD solution is required. Fortunately, AWS has released
    a host of new features within it’s SageMaker ecosystem that makes such a solution
    possible. This article shows a path to successfully accomplish this for a wide
    array of tailored ML solutions, requiring only a single manual model verification
    step.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading! If you liked this article, follow me to get notified of
    my new posts. Also, feel free to share any comments/suggestions.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
