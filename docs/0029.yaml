- en: DataHub Hands-On Part II
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/datahub-hands-on-part-ii-169c36ee082d?source=collection_archive---------8-----------------------#2023-01-03](https://towardsdatascience.com/datahub-hands-on-part-ii-169c36ee082d?source=collection_archive---------8-----------------------#2023-01-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Data Ingestion, Data Discovery and Entity Linkage
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://simon-j-preis.medium.com/?source=post_page-----169c36ee082d--------------------------------)[![Professor
    Simon J. Preis, Ph.D.](../Images/ea0586e8d1fd6ba434fa2cfb9a71e4a5.png)](https://simon-j-preis.medium.com/?source=post_page-----169c36ee082d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----169c36ee082d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----169c36ee082d--------------------------------)
    [Professor Simon J. Preis, Ph.D.](https://simon-j-preis.medium.com/?source=post_page-----169c36ee082d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8c9dbf8eb438&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdatahub-hands-on-part-ii-169c36ee082d&user=Professor+Simon+J.+Preis%2C+Ph.D.&userId=8c9dbf8eb438&source=post_page-8c9dbf8eb438----169c36ee082d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----169c36ee082d--------------------------------)
    ·10 min read·Jan 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F169c36ee082d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdatahub-hands-on-part-ii-169c36ee082d&user=Professor+Simon+J.+Preis%2C+Ph.D.&userId=8c9dbf8eb438&source=-----169c36ee082d---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F169c36ee082d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdatahub-hands-on-part-ii-169c36ee082d&source=-----169c36ee082d---------------------bookmark_footer-----------)![](../Images/bba8d0b890e4cff0bc0ca4d6958243ae.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Duy Pham](https://unsplash.com/@miinyuii?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Welcome to part II of my DataHub Hands-On story! In the [previous part,](https://medium.com/towards-data-science/datahub-hands-on-part-i-f0709e7efec9)
    we have discussed how to setup a data catalog from scratch, how to do semantic
    modelling with business terms and what’s the motivation for Data Governance (DG)
    at all. In this part, we will have a closer look on the rather technical parts
    of DataHub in order to ingest and link metadata.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Data Ingestion
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The data ingestion (DI) section of DataHub can be accessed via the top right
    menu bar:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/520149b88d360de658bd8d23463bead3.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/520149b88d360de658bd8d23463bead3.png)'
- en: 'Source: Author'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 'Source: Author'
- en: Once we have entered the DI area, we see a list of already configured data sources
    including an overview that shows execution statistics. From there we can now run
    a configured DI process, we can edit the configuration, we can delete or copy
    a configuration, or we create a new source.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦进入 DI 区域，我们会看到已配置的数据源列表，包括显示执行统计的概览。从这里，我们可以运行已配置的 DI 过程，可以编辑配置，可以删除或复制配置，或创建一个新源。
- en: '![](../Images/90b56e0c2c7fdb85f6fe146cb0050028.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90b56e0c2c7fdb85f6fe146cb0050028.png)'
- en: 'Source: Author'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 'Source: Author'
- en: There is also another button “Secrets” that can be used to configure passwords
    for accessing data sources. This feature helps to store the passwords secured
    in the DataHub database. A secret can be associated to a data source during the
    source configuration.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 另有一个“Secrets”按钮，可用于配置访问数据源的密码。此功能有助于将密码安全地存储在 DataHub 数据库中。在源配置过程中，可以将秘密与数据源关联。
- en: Important Terms
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重要术语
- en: To understand how the DI configuration works, let’s quickly define a few important
    terms. As briefly discussed above, there are *sources* for metadata. Sources refer
    to the databases — typically from a productive environment — that we want to manage
    in our data catalog. Besides sources, DataHub also provides the concept of *sinks*.
    Sinks are destinations for metadata. Typically and by default, the DI engine sends
    the collected metadata to the “datahub-rest” sink to make them available in the
    DataHub UI. However, there are also options for Kafka and File export, if required.
    Then we have *recipes*, which are configuration files that tell the ingestion
    scripts where to pull the data from and where to put it. Recipes can be configured
    through the UI, if an adapter is available, or via a YAML script[1].
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解 DI 配置的工作原理，让我们快速定义一些重要术语。如上所述，*sources* 指的是元数据的来源。Sources 指我们希望在数据目录中管理的数据库——通常来自生产环境。除了
    sources，DataHub 还提供了 *sinks* 的概念。Sinks 是元数据的目的地。通常，DI 引擎将收集到的元数据发送到“datahub-rest”
    sink，使其在 DataHub UI 中可用。但是，如果需要，也可以选择 Kafka 和文件导出。接下来是 *recipes*，这是配置文件，指示摄取脚本从哪里提取数据以及将数据放在哪里。配方可以通过
    UI 配置（如果有适配器），也可以通过 YAML 脚本[1]进行配置。
- en: Data Sources and Adapters
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据源和适配器
- en: 'Now, Let’s have a a look on the pre-configured database adapters (a.k.a connectors)
    that DataHub provides out of the box:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看 DataHub 提供的预配置数据库适配器（即连接器）：
- en: '![](../Images/b205b659900d7a9ff409006bb1a1a011.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b205b659900d7a9ff409006bb1a1a011.png)'
- en: 'Source: Author'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 'Source: Author'
- en: We see a number of established SQL and NoSQL technology icons. To understand
    that this is not just for marketing purposes, let’s compare two selected adapters.
    On the left side of the following image we see a ingestion recipe for BigQuery,
    on the right side we see a recipe for SQLServer. The ingestion UI obviously expects
    different information to establish a database connection depending on the technology.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到一些已经建立的 SQL 和 NoSQL 技术图标。为了理解这不仅仅是为了营销目的，让我们比较两个选定的适配器。在下图的左侧，我们看到一个 BigQuery
    的摄取配方，在右侧，我们看到一个 SQLServer 的配方。显然，摄取 UI 根据技术期望不同的信息来建立数据库连接。
- en: '![](../Images/4203c5c1d30308f43c3cdf33569e42e4.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4203c5c1d30308f43c3cdf33569e42e4.png)'
- en: 'Source: Author'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 'Source: Author'
- en: Most of the data sources follow a pull-based integration using a Python based
    metadata-ingestion system. In a first step, the metadata is pulled from the source
    systems, and in a second step, the information is pushed to the DataHub service
    tier either via HTTP or Kafka². Each source adapter has different capabilities,
    hence, reading the online documentation is recommended to avoid confusions. For
    instance, the [MySQL](https://datahubproject.io/docs/generated/ingestion/sources/mysql)
    and SAP HANA adapters enable data profiling and detection of deleted entities,
    which is not supported by the MongoDB or Oracle adapters. In contrast, the [BigQuery](https://datahubproject.io/docs/generated/ingestion/sources/bigquery)
    adapter supports these features as well as table-level lineage and a few more.
    To understand why there are differences between the adapter capabilities, we just
    need to visit github to view the code for each adapter. Reviewing the entire adapter
    logic is beyond the scope of this story, but we can see, for instance, that sqlalchemy
    is used to connect to [MySQL](https://github.com/datahub-project/datahub/blob/master/metadata-ingestion/src/datahub/ingestion/source/sql/mysql.py)
    sources whereas the [BigQuery](https://github.com/datahub-project/datahub/blob/master/metadata-ingestion/src/datahub/ingestion/source/bigquery_v2/bigquery.py)
    adapter is applying the standard connection package provided by Google. So let’s
    conclude that each adapter is implemented independently and potentially uses different
    connection packages. This and the fact that also database technologies differ
    in metadata that they can provide at all (e.g. there is no foreign key concept
    in MongoDB) have implications on the ingestion capabilities of an adapter.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数数据源使用基于Python的元数据摄取系统进行拉取式集成。在第一步中，元数据从源系统中拉取，在第二步中，通过HTTP或Kafka²将信息推送到DataHub服务层。每个源适配器具有不同的功能，因此建议阅读在线文档以避免混淆。例如，[MySQL](https://datahubproject.io/docs/generated/ingestion/sources/mysql)和SAP
    HANA适配器支持数据分析和检测已删除的实体，而MongoDB或Oracle适配器不支持这些功能。相反，[BigQuery](https://datahubproject.io/docs/generated/ingestion/sources/bigquery)适配器支持这些功能以及表级血缘关系等其他功能。要了解适配器功能之间的差异，我们只需访问github查看每个适配器的代码。例如，我们可以看到，sqlalchemy用于连接[MySQL](https://github.com/datahub-project/datahub/blob/master/metadata-ingestion/src/datahub/ingestion/source/sql/mysql.py)源，而[BigQuery](https://github.com/datahub-project/datahub/blob/master/metadata-ingestion/src/datahub/ingestion/source/bigquery_v2/bigquery.py)适配器则使用Google提供的标准连接包。因此，可以得出结论，每个适配器都是独立实现的，并可能使用不同的连接包。这一点以及数据库技术在其能提供的元数据方面的不同（例如，MongoDB中没有外键概念）对适配器的摄取能力有影响。
- en: Finalizing the Recipe
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完成配方
- en: 'There is also a filter section that helps to narrow down the schema or tables
    to ingest. If no filter is set, the DI engine executes the metadata scanning over
    all schemes, tables and views where the configured database user has access — this
    should be considered to avoid performance impacts on both the scanned database
    and the ingestion process. From security perspective, it is rather recommended
    to restrict the user privileges directly at the database (e.g. by creating a particular
    read-only DataHub user). You can toggle advanced settings in an additional section,
    e.g. if views should also be considered for scanning or only tables or vice versa,
    or both. Also important: you can select if tables or columns should be profiled
    (e.g. how many records in a table, how many missing values in a column). This
    is useful to gain transparency over the company data landscape.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个过滤器部分，有助于缩小要摄取的模式或表。如果没有设置过滤器，DI引擎会对配置的数据库用户有权限的所有方案、表和视图执行元数据扫描——这应考虑到以避免对扫描的数据库和摄取过程造成性能影响。从安全角度来看，建议直接在数据库中限制用户权限（例如，通过创建一个特定的只读DataHub用户）。你可以在附加部分切换高级设置，例如是否应考虑视图进行扫描，或仅考虑表，或两者都考虑。同样重要的是：你可以选择是否对表或列进行分析（例如，表中有多少记录，列中有多少缺失值）。这对于了解公司数据环境的透明度非常有用。
- en: Once the recipe is configured, you can specify the execution schedule. Here,
    you define the frequency (e.g. daily, monthly, hourly) of the DI execution. In
    the final step of the creation wizard, you have to assign a unique name to the
    new source and click “save & run” which directly triggers the first metadata scanning
    process.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦配置了配方，你可以指定执行计划。在这里，你定义了DI执行的频率（例如每日、每月、每小时）。在创建向导的最后一步，你必须为新源分配一个唯一名称，然后点击“保存并运行”，这将直接触发第一次元数据扫描过程。
- en: '![](../Images/33d47afe85804f592295aee118a04b18.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: 'Source: Author'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Once the process has been executed, you can view the details in order to get
    a first impression of the ingested assets, or to see details on the failures.
    In my test case, we have a MySQL database server with 78 views and 38 tables from
    4 databases.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/65646c38886c8e330d4a00c56f8887ae.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: 'Source: Author'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Data Discovery
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Searching and filtering
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once our metadata was pushed to the DataHub service tier, we can start our data
    discovery journey through the DataHub UI. As a fan of the 90s, I use the “northwind”
    database from Microsoft in my testcase. If I want to search for the schema, I
    can scroll through the tables and views or I can select some filters (e.g. to
    exclude views).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e2cb329f6fd41e8aee2fa811f4e254c0.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: 'Source: Author'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Table Profiling Results
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s assume I want to have a closer look on the product data, then I simply
    click on the the product dataset from that list. The first information I see is
    a table schema overview that shows the fields, their data type and even some data
    model details such as primary and foreign keys. At the top (under the dataset
    name), we also see at one glance that there are currently 79 records in our table.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6c0f9495accfbad08f4510f2fb69aca2.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
- en: 'Source: Author'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: The second sheet refers to documentation which is empty by default. Here, data
    custodians could add technical knowledge on that table in DataHub and/or add links
    to already existing material. It is an important tool to foster the data architecture
    maturity in a company — the data catalog is only as helpful as the knowledge shared
    and entered by experts. The dataset has a few more interesting sheets such as
    lineage and validation, which we will cover in part III — stay tuned :)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Column Profiling Results
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, I want to show what the DI engine provides by simply selecting the “column
    profiling” option in our recipe earlier. For that purpose, let’s enter the sheet
    called “stats”. Here we see statistical information for each column in the selected
    table.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ac7a1f63a3a051b4ddf29b5fdf16bbf7.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: 'Source: Author'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: For instance, we see in the first row that the field “ProductID” has 100% distinct
    values. This finding is not surprising, because this field is the primary key
    of the table — it is technically defined that the values have to differ per record.
    We also see a NULL count and NULL percentage information, e.g. we see that two
    product records do not have a supplier reference. This can be considered as a
    critical data quality issue, because without an associated supplier, you cannot
    physically provide those products to customers.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: What else do we have? We see descriptive measures such as min, max, mean, which
    are more interesting in transactional tables with higher amount of records. At
    least, we see at one glance some statistical measures regarding unit price of
    our product portfolio (e.g. half of our products cost less than $ 19.45 → Median).
    In addition, we see sample values for each column in our dataset.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 还有什么呢？我们可以看到描述性度量，例如最小值、最大值、均值，这在记录量较多的交易表中更为有趣。至少，我们可以一眼看到有关我们产品组合的单位价格的统计度量（例如，半数产品的价格低于
    $19.45 → 中位数）。此外，我们还可以看到数据集中每列的样本值。
- en: Bringing together what belongs together
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将属于同一类别的内容结合起来
- en: Entity linkage
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实体链接
- en: 'Sure, the data discovery was already insightful and fosters transparency, especially
    if you never experienced such a feature in practice. However, the full power of
    a data catalog is only unleashed when we bring the IT world and business world
    together. Sounds great — but what does that mean? Well, the “IT world” in this
    sense refers to all the technical information around the datasets. The “business
    world” is what we have discussed in the previous part: all the business information
    we maintain in the glossary. Looking at real companies, it must be emphasized
    that (especially in bigger companies) there are different groups of people responsible
    for datasets and glossary. We have data custodians on the one hand who are experts
    on technical topics, e.g. to develop and improve databases. And we have data stewards
    on the other hand, who are experts on business topics, e.g. how do we indicate
    which product can be manufactured at which site due to which conditions — and
    which data is required to decide. So when we talk about linking entities, we implicitly
    mean connecting people!'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，数据发现已经非常有启发性，并促进了透明度，特别是如果你从未在实践中体验过这种功能。然而，数据目录的真正力量只有在我们将IT世界和商业世界结合起来时才能释放出来。这听起来很棒——但这意味着什么呢？在这个意义上，“IT世界”指的是所有与数据集相关的技术信息。“商业世界”是我们在前一部分讨论的内容：我们在术语表中维护的所有商业信息。以现实公司为例，必须强调的是（特别是在较大的公司中），有不同的小组负责数据集和术语表。一方面，我们有数据保管员，他们是技术话题的专家，例如开发和改进数据库。另一方面，我们有数据管理员，他们是商业话题的专家，例如如何根据条件指示哪些产品可以在哪个地点生产——以及做出决定所需的数据。因此，当我们谈论链接实体时，我们实际上是在连接人员！
- en: Big Words! So how does that work with DataHub? Let’s continue with our dataset
    “products” which reflects a physical database table. This is our starting point
    in DataHub. In the screenshot below we see various marked information, which refer
    to multiple types of links to the “business world” that can be used jointly.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 大词汇！那么这与DataHub如何配合呢？让我们继续讨论我们的数据集“products”，它反映了一个实际的数据库表。这是我们在DataHub中的起点。在下面的屏幕截图中，我们可以看到各种标记的信息，这些信息指向可以共同使用的多种类型的与“商业世界”的链接。
- en: 'Option: We can add a glossary term to a table column. This link indicates that
    raw data of a certain term can be found in this column, e.g. the actual product
    names.'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选项：我们可以在表格列中添加术语表项。此链接指示某个术语的原始数据可以在该列中找到，例如实际的产品名称。
- en: 'Option: We can add a glossary term to the whole table. This link indicates
    that a certain term is rather complex and described by multiple attributes that
    can be found in that table.'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选项：我们可以在整个表格中添加术语表项。此链接指示某个术语相当复杂，并由多个属性描述，这些属性可以在该表中找到。
- en: 'Option: We can add a domain where the dataset belongs to. This link is similar
    to the linkage of glossary terms to a domain (see part I for more information).'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选项：我们可以添加一个数据集所属的领域。此链接类似于术语表项与领域的链接（有关更多信息，请参见第一部分）。
- en: '![](../Images/7b0e37958e78e1c7d4ea86641f1d62b1.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b0e37958e78e1c7d4ea86641f1d62b1.png)'
- en: 'Source: Author'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者
- en: People
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人员
- en: 'However, we still don’t have people in our model. So let’s add an owner for
    the dataset “products”:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们的模型中仍然没有人员。因此，让我们为数据集“products”添加一个所有者：
- en: '![](../Images/de32cdc4b1e23f4ecd45a57027f5b877.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de32cdc4b1e23f4ecd45a57027f5b877.png)'
- en: 'Source: Author'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者
- en: DataHub suggests that this person should be a “technical owner” which is similar
    to the role of a “data custodian” in other DG frameworks. Now we need to add people
    to the associated glossary terms. We can simply click on the “Product” term and
    DataHub navigates us to the glossary record. There, we also can add owners. For
    instance, we can add Brian as “data steward” and Lilly as a “business owner”.
    If we hover over the names, we see the role assigned to the name. DataHub allows
    to add persons multiple times — this can be useful if a person has multiple roles
    (e.g. business owner + data steward). Unfortunately, the tool also allows to add
    the same combination of person and roles multiple times, which is not useful.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: DataHub建议这个人应该是“技术所有者”，这类似于其他数据治理框架中的“数据保管员”角色。现在我们需要将人员添加到相关的术语中。我们可以简单地点击“产品”术语，DataHub将我们导航到术语记录。在那里，我们也可以添加所有者。例如，我们可以将Brian添加为“数据管理员”，将Lilly添加为“业务所有者”。如果我们将鼠标悬停在名字上，会看到分配给该名字的角色。DataHub允许多次添加人员——如果一个人有多个角色（例如业务所有者+数据管理员），这可能会很有用。不幸的是，该工具也允许多次添加相同的人员和角色组合，这并不实用。
- en: '![](../Images/27acc92107ee5c74962be79e4c8f30cb.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/27acc92107ee5c74962be79e4c8f30cb.png)'
- en: 'Source: Author'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者
- en: 'Now let’s assume that Brian is new at the company and wants to change something
    on the product master data table, so he needs to find the right IT expert to clarify
    the implementation details. In this case, he simply clicks on the “products” dataset
    which is visible in the “Related Entities” section (see screenshot above). Then
    he is forwarded to the dataset (which we have seen further above) and can see:
    my IT counterpart is John! And of course, also other people in the organization
    who need support or have change requests for product data can use this linked
    information — they can simply search for “product” and need only 2 clicks to collect
    the right data stakeholders (Lilly, Brian and John).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设Brian是公司新员工，他想在产品主数据表上做一些更改，因此他需要找到合适的IT专家来澄清实施细节。在这种情况下，他只需点击“产品”数据集，该数据集在“相关实体”部分可见（见上图）。然后他会被转到数据集（我们在上面看到过）并可以看到：我的IT联系人是John！当然，组织中需要支持或有产品数据更改请求的其他人员也可以使用这个链接信息——他们只需搜索“产品”，并只需2次点击即可收集正确的数据相关人员（Lilly、Brian和John）。
- en: Part II Conclusions
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分结论
- en: We have seen that setting up a data ingestion process is done within a few minutes
    as DataHub provides a configuration wizard for a number of established database
    technologies. We have also seen that data discovery is a powerful feature to gain
    transparency over a company’s data architecture. The profiling of tables and columns
    is performed automatically and the statistical results can be viewed in the UI.
    Linking datasets and glossary terms or adding owners is also very simple, however,
    we have seen that consistency checks are not always performed by DataHub. We can
    add the same owners with the same role multiple times, but the tool prevents us
    at least from adding the same glossary term multiple times to a dataset. It is
    also possible to assign the same glossary term to multiple columns in the same
    table — maybe there are scenarios in practice where this flexibility is useful.
    In general, my practice advice is that — at least within one table — there should
    be a 1:1 relationship between columns and terms. If not, you should review the
    level of granularity in your glossary — or you have a problem in your physical
    data model.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，设置数据摄取过程只需几分钟，因为DataHub提供了多个已建立的数据库技术的配置向导。我们还看到，数据发现是一个强大的功能，可以提高公司数据架构的透明度。表和列的分析是自动执行的，统计结果可以在用户界面中查看。数据集和术语的链接或添加所有者也非常简单，不过，我们看到数据一致性检查并不总是由DataHub执行。我们可以多次添加相同角色的相同所有者，但该工具至少能阻止我们多次将相同术语添加到数据集中。将相同的术语分配给同一表中的多个列也是可能的——也许在实际应用中这种灵活性是有用的。总的来说，我的实践建议是——至少在一个表内——列和术语之间应保持1:1关系。如果不是这样，你应审查你的术语表的粒度，或者你的物理数据模型存在问题。
- en: Again — well done if you made it this far! In the next and final part, we will
    dive deeper into data quality management with data lineage and data validation.
    Maybe, I will write a further story to discuss the DG role concept of DataHub
    in comparison to other DG frameworks.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 再次恭喜你读到这里！在下一部分，也是最后一部分，我们将**深入探讨**数据质量管理，包括数据溯源和数据验证。也许，我会写一个进一步的故事，讨论DataHub在数据治理（DG）角色概念方面与其他DG框架的比较。
- en: '[1] [https://datahubproject.io/docs/metadata-ingestion](https://datahubproject.io/docs/metadata-ingestion)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [https://datahubproject.io/docs/metadata-ingestion](https://datahubproject.io/docs/metadata-ingestion)'
- en: '[2] [https://datahubproject.io/docs/architecture/metadata-ingestion](https://datahubproject.io/docs/architecture/metadata-ingestion)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [https://datahubproject.io/docs/architecture/metadata-ingestion](https://datahubproject.io/docs/architecture/metadata-ingestion)'
