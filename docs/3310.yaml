- en: Large Language Models, ALBERT — A Lite BERT for Self-supervised Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型，ALBERT——用于自监督学习的轻量级BERT
- en: 原文：[https://towardsdatascience.com/albert-22983090d062?source=collection_archive---------4-----------------------#2023-11-07](https://towardsdatascience.com/albert-22983090d062?source=collection_archive---------4-----------------------#2023-11-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/albert-22983090d062?source=collection_archive---------4-----------------------#2023-11-07](https://towardsdatascience.com/albert-22983090d062?source=collection_archive---------4-----------------------#2023-11-07)
- en: Understand essential techniques behind BERT architecture choices for producing
    a compact and efficient model
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 BERT 架构选择背后的基本技术，以生产出紧凑而高效的模型
- en: '[](https://medium.com/@slavahead?source=post_page-----22983090d062--------------------------------)[![Vyacheslav
    Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----22983090d062--------------------------------)[](https://towardsdatascience.com/?source=post_page-----22983090d062--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----22983090d062--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----22983090d062--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@slavahead?source=post_page-----22983090d062--------------------------------)[![Vyacheslav
    Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----22983090d062--------------------------------)[](https://towardsdatascience.com/?source=post_page-----22983090d062--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----22983090d062--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----22983090d062--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falbert-22983090d062&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----22983090d062---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----22983090d062--------------------------------)
    ·7 min read·Nov 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F22983090d062&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falbert-22983090d062&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----22983090d062---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falbert-22983090d062&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----22983090d062---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----22983090d062--------------------------------)
    · 7 分钟阅读 · 2023年11月7日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F22983090d062&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falbert-22983090d062&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----22983090d062---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F22983090d062&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falbert-22983090d062&source=-----22983090d062---------------------bookmark_footer-----------)![](../Images/4513d64c7ebd927038674efd312b98ff.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F22983090d062&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falbert-22983090d062&source=-----22983090d062---------------------bookmark_footer-----------)![](../Images/4513d64c7ebd927038674efd312b98ff.png)'
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In recent years, the evolution of large language models has skyrocketed. BERT
    became one of the most popular and efficient models allowing to solve a wide range
    of NLP tasks with high accuracy. After BERT, a set of other models appeared later
    on the scene demonstrating outstanding results as well.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，大型语言模型的发展迅猛。BERT 成为了最受欢迎和高效的模型之一，能够以高精度解决广泛的 NLP 任务。在 BERT 之后，出现了一系列其他模型，表现也非常出色。
- en: The obvious trend that became easy to observe is the fact that **with time large
    language models (LLMs) tend to become more complex by exponentially augmenting
    the number of parameters and data they are trained on**. Research in deep learning
    showed that such techniques usually lead to better results. Unfortunately, the
    machine learning world has already dealt with several problems regarding LLMs,
    and scalability has become the main obstacle in effective training, storing and
    using them.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 显而易见的趋势是，**随着时间的推移，大型语言模型（LLMs）趋向于通过指数增加其参数和训练数据的数量变得更复杂**。深度学习的研究表明，这种技术通常会带来更好的结果。不幸的是，机器学习领域已经处理过几个有关
    LLMs 的问题，而可扩展性已成为有效训练、存储和使用它们的主要障碍。
- en: As a consequence, new LLMs have been recently developed to tackle scalability
    issues. In this article, we will discuss ALBERT which was invented in 2020 with
    an objective of significant reduction of BERT parameters.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最近开发了新的大语言模型（LLMs）以解决可扩展性问题。在本文中，我们将讨论 ALBERT，它在 2020 年被发明，目标是显著减少 BERT 参数。
- en: ALBERT
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ALBERT
- en: To understand the underlying mechanisms in ALBERT, we are going to refer to
    its [official paper](https://arxiv.org/pdf/1909.11942.pdf). For the most part,
    ALBERT derives the same architecture from BERT. There are three principal differences
    in the choice of the model’s architecture which are going to be addressed and
    explained below.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解 ALBERT 的潜在机制，我们将参考其[官方论文](https://arxiv.org/pdf/1909.11942.pdf)。大部分情况下，ALBERT
    从 BERT 派生出相同的架构。模型架构选择上有三个主要区别，下面将进行说明和解释。
- en: Training and fine-tuning procedures in ALBERT are analogous to those in BERT.
    Like BERT, ALBERT is pretrained on English Wikipedia (2500M words) and BookCorpus
    (800M words).
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ALBERT 的训练和微调过程类似于 BERT。与 BERT 一样，ALBERT 在英文维基百科（2500M 单词）和 BookCorpus（800M
    单词）上进行预训练。
- en: 1\. Factorized Parameter Embedding
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 分解参数嵌入
- en: When an input sequence is tokenized, each of the tokens is then mapped to one
    of the vocabulary embeddings. These embeddings are used for the input to BERT.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入序列被标记化时，每个标记被映射到词汇嵌入中的一个。这些嵌入用于 BERT 的输入。
- en: Let *V* be the vocabulary size (the total number of possible embeddings) and
    *H* — embedding dimensionality. Then for each of the *V* embeddings, we need to
    store *H* values resulting in a *V x H* embedding matrix. As turns out in practice,
    this matrix usually has huge sizes and requires a lot of memory to store it. But
    a more global problem is that most of the time the elements of an embedding matrix
    are trainable and it requires a lot of resources for the model to learn appropriate
    parameters.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 设 *V* 为词汇大小（可能的嵌入总数）和 *H* — 嵌入维度。那么，对于每个 *V* 个嵌入，我们需要存储 *H* 个值，从而形成一个 *V x H*
    的嵌入矩阵。实际上，这个矩阵通常具有巨大的尺寸，并且需要大量内存来存储。但是，更大的问题是，大多数情况下，嵌入矩阵的元素是可训练的，这需要大量资源来使模型学习合适的参数。
- en: 'For instance, let us take the BERT base model: it has a vocabulary of 30K tokens,
    each represented by a 768-component embedding. In total, this results in 23M weights
    to be stored and trained. For larger models, this number is even larger.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以 BERT 基础模型为例：它有 30K 个词汇，每个词汇由 768 维嵌入表示。总共，这导致了 23M 权重需要存储和训练。对于更大的模型，这个数字甚至更大。
- en: This problem can be avoided by using matrix factorization. The original vocabulary
    matrix *V x H* can be decomposed into a pair of smaller matrices of sizes *V x
    E* and *E x H*.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题可以通过使用矩阵分解来避免。原始词汇矩阵 *V x H* 可以分解成一对较小的矩阵，尺寸为 *V x E* 和 *E x H*。
- en: '![](../Images/840860bb19b5dc0b624f748598cc2431.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/840860bb19b5dc0b624f748598cc2431.png)'
- en: Vocabulary matrix factorization
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇矩阵分解
- en: As a consequence, instead of using *O(V x H)* parameters, decomposition results
    in only *O(V x E + E x H)* weights. Obviously, this method is effective when *H
    >> E*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用*O(V x H)*参数的情况下，分解结果仅需 *O(V x E + E x H)* 权重。显然，当 *H >> E* 时，这种方法是有效的。
- en: '**Another great aspect of matrix factorization is the fact that it does not
    change the lookup process for obtaining token embeddings**: each row of the left
    decomposed matrix *V x E* maps a token to its corresponding embedding in the same
    simple way as it was in the original matrix *V x H*. This way, the dimensionality
    of embeddings decreases from *H* to *E*.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**矩阵分解的另一个伟大方面是它不会改变获取标记嵌入的查找过程**：左分解矩阵 *V x E* 的每一行以与原始矩阵 *V x H* 相同的简单方式将标记映射到其对应的嵌入。这样，嵌入的维度从
    *H* 降低到 *E*。'
- en: 'Nevertheless, in the case of decomposed matrices, to obtain the input for BERT,
    the mapped embeddings need then to be projected into hidden BERT space: this is
    done by multiplying a corresponding row of the left matrix by columns of the right
    matrix.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于分解矩阵而言，为了获得BERT的输入，需要将映射的嵌入投影到隐藏的BERT空间：这通过将左矩阵的相应行与右矩阵的列相乘来完成。
- en: 2\. Cross-layer Parameter Sharing
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 跨层参数共享
- en: One of the ways to reduce the model’s parameters is to make them shareable.
    This means that they all share the same values. For the most part, it simply reduces
    the memory required to store weights. However, **standard algorithms like backpropagation
    or inference will still have to be executed on all parameters**.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 减少模型参数的一种方法是使它们可共享。这意味着它们共享相同的值。大部分情况下，这只是减少了存储权重所需的内存。然而，**标准算法如反向传播或推理仍需在所有参数上执行**。
- en: '**One of the most optimal ways to share weights occurs when they are located
    in different but similar blocks of the model**. Putting them into similar blocks
    results in a higher chance that most of the calculations for shareable parameters
    during forward propagation or backpropagation will be the same. This gives more
    opportunities for designing an efficient computation framework.'
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**共享权重的最优方式之一是将它们放置在模型中不同但相似的块中**。将它们放入相似的块中会导致在前向传播或反向传播过程中大多数可共享参数的计算相同，从而提供更多设计高效计算框架的机会。'
- en: 'The mentioned idea is implemented in ALBERT which consists of a set of Transformer
    blocks with the same structure making parameter sharing more efficient. In fact,
    there exist several ways of parameter sharing in Transformers across layers:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 上述思想在ALBERT中得到了实现，ALBERT由一组结构相同的变压器块组成，使得参数共享更高效。事实上，在变压器中跨层共享参数存在几种方法：
- en: share only attention parameters;
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅共享注意力参数；
- en: share only forward neural network (FNN) parameters;
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅共享前向神经网络（FNN）参数；
- en: share all parameters (used in ALBERT).
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享所有参数（用于ALBERT）。
- en: '![](../Images/ece8501f78ebd011ebf2f535dbc363e2.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ece8501f78ebd011ebf2f535dbc363e2.png)'
- en: Different parameter sharing strategies
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的参数共享策略
- en: In general, it is possible to divide all transformer layers into N groups of
    size M each where every group shares parameters within layers it has. Researchers
    found out that the smaller the group size M is, the better the results are. However,
    decreasing group size M leads to a significant increase in total parameters.
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 通常，可以将所有变压器层分成N组，每组大小为M，每组内的层共享参数。研究人员发现，组大小M越小，结果越好。然而，减少组大小M会显著增加总参数量。
- en: 3\. Sentence Order Prediction
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 句子顺序预测
- en: 'BERT focuses on mastering two objectives when pretraining: masked language
    modeling (MSM) and next sentence prediction (NSP). In general, MSM was designed
    to improve BERT’s ability to gain linguistic knowledge and the goal of NSP was
    to improve BERT’s performance on particular downstream tasks.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: BERT在预训练时专注于掌握两个目标：掩码语言模型（MSM）和下一句预测（NSP）。一般来说，MSM旨在提高BERT获取语言知识的能力，而NSP的目标是提升BERT在特定下游任务上的表现。
- en: Nevertheless, multiple studies showed that it might be beneficial to get rid
    of the NSP objective mainly because of its simplicity, compared to MLM. Following
    this idea, ALBERT researchers also decided to remove the NSP task and replace
    it with sentence order prediction (SOP) problem whose goal is to predict whether
    both sentences are located in correct or inverse order.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，多项研究表明，去除NSP目标可能是有益的，因为与MLM相比，它过于简单。基于这一理念，ALBERT的研究人员决定去除NSP任务，并用句子顺序预测（SOP）问题替代，其目标是预测两个句子是否按正确或相反顺序排列。
- en: Speaking of the training dataset, all positive pairs of input sentences are
    collected sequentially within the same text passage (the same method as in BERT).
    For negative sentences, the principle is the same except for the fact that both
    sentences go in inverse order.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 说到训练数据集，所有输入句子的正对会在同一文本段落中按顺序收集（与BERT中的方法相同）。对于负句子，原则相同，只是两个句子的顺序相反。
- en: '![](../Images/f80b54ffb9831c235cb11aa7d9a94c34.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f80b54ffb9831c235cb11aa7d9a94c34.png)'
- en: Composition of positive and negative training pairs in BERT and in ALBERT
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: BERT和ALBERT中正负训练对的组合
- en: It was shown that models trained with the NSP objective cannot accurately solve
    SOP tasks while models trained with the SOP objective perform well on NSP problems.
    These experiments prove that ALBERT is better adapted for solving various downstream
    tasks than BERT.
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 研究表明，使用NSP目标训练的模型不能准确解决SOP任务，而使用SOP目标训练的模型在NSP问题上表现良好。这些实验证明了ALBERT在解决各种下游任务方面比BERT更为适应。
- en: BERT vs ALBERT
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BERT与ALBERT
- en: The detailed comparison between BERT and ALBERT is illustrated in the diagram
    below.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: BERT和ALBERT的详细比较在下面的图中展示。
- en: '![](../Images/6a187fe7542f3b96ce1e044f44544e3f.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a187fe7542f3b96ce1e044f44544e3f.png)'
- en: Comparison between different variations of BERT and ALBERT models. The speed
    measured under the same configurations shows how fast models iterated through
    the training data. The speed values are shown relatively for each model (BERT
    large is taken as a baseline whose speed equals 1x). The accuracy score was measured
    on GLUE, SQuAD and RACE benchmarks.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: BERT和ALBERT模型不同变体的比较。在相同配置下测量的速度显示了模型迭代训练数据的速度。速度值是相对每个模型显示的（BERT large作为基线，其速度等于1x）。准确性评分是在GLUE、SQuAD和RACE基准上测量的。
- en: 'Here are the most interesting observations:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些最有趣的观察结果：
- en: By having only 70% of the parameters of BERT large, the xxlarge version of ALBERT
    achieves a better performance on downstream tasks.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管xxlarge版本的ALBERT仅有BERT large 70%的参数，但在下游任务中表现更佳。
- en: ALBERT large achieves comparable performance, compared to BERT large, and is
    faster 1.7x times due to the massive parameter size compression.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相比于BERT large，ALBERT large在性能上可比，并且由于大规模的参数压缩，其速度提高了1.7倍。
- en: All ALBERT models have an embedding size of 128\. As was shown in the ablation
    studies in the paper, this is the optimal value. Increasing the embedding size,
    for example, up to 768, improves metrics but no more than 1% in absolute values
    which is not so much regarding the increasing complexity of the model.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有ALBERT模型的嵌入大小为128。如论文中的消融研究所示，这是一项最优值。增加嵌入大小，例如，达到768，会改善指标，但绝对值提升不超过1%，考虑到模型复杂性的增加，这一提升并不算多。
- en: Though ALBERT xxlarge processes a single iteration of data 3.3x slower than
    BERT large, experiments showed that if training both of these models for the same
    amount of time, then ALBERT xxlarge demonstrates a considerably better average
    performance on benchmarks than BERT large (88.7% vs 87.2%).
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管ALBERT xxlarge处理单次数据迭代的速度比BERT large慢3.3倍，但实验表明，如果将这两种模型训练相同的时间，那么ALBERT xxlarge在基准测试中显示了明显更好的平均性能（88.7%
    vs 87.2%）。
- en: Experiments showed that ALBERT models with wide hidden sizes (≥ 1024) do not
    benefit a lot from an increase in the number of layers. That is one of the reasons
    why the number of layers was reduced from 24 in ALBERT large to 12 in the xxlarge
    version.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实验表明，具有较宽隐藏层大小（≥ 1024）的ALBERT模型从增加层数中获益不大。这也是为什么ALBERT large的层数从24减少到xxlarge版本的12的原因之一。
- en: '![](../Images/1d427426e58d971a379755edce79eff8.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d427426e58d971a379755edce79eff8.png)'
- en: Performance of ALBERT large (18M parameters) with the increase of number of
    layers. Models in the diagram with ≥ 3 layers were fine-tuned from the checkpoint
    of the previous model. It can be observed that after reaching 12 layers, the performance
    increase gets slower and gradually falls after 24 layers.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ALBERT large（1800万参数）在增加层数时的性能表现。图中的模型具有≥ 3层，这些模型都是从之前模型的检查点进行微调的。可以观察到，在达到12层后，性能提升变得更慢，并且在24层后逐渐下降。
- en: A similar phenomenon occurs with the increase of in hidden-layer size. Increasing
    it with values larger than 4096 degrades the model performance.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似的现象发生在隐藏层大小增加时。将其增加到大于4096的值会导致模型性能下降。
- en: '![](../Images/cc8e835a310620303d87a13acdf67135.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc8e835a310620303d87a13acdf67135.png)'
- en: Performance of ALBERT large (3-layer configuration from the diagram above) with
    the increase of hidden size. The hidden size of 4096 is the optimal value.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ALBERT large（来自上图的3层配置）在增加隐藏层大小时的性能表现。4096的隐藏大小是最优值。
- en: Conclusion
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: At first sight, ALBERT seems a preferable choice over original BERT models as
    it outperforms them on downstream tasks. Nevertheless, ALBERT requires much more
    computations due to its longer structures. A good example of this issue is ALBERT
    xxlarge which has 235M parameters and 12 encoder layers. The majority of these
    235M weights belong to a single transformer block. The weights are then shared
    for each of the 12 layers. Therefore, during training or inference, the algorithm
    has to be executed on more than 2 billion parameters!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 初看，ALBERT 似乎是一个比原始 BERT 模型更好的选择，因为它在下游任务中表现优于它们。然而，由于其更长的结构，ALBERT 需要更多的计算。例如，ALBERT
    xxlarge 具有 2.35 亿个参数和 12 层编码器。这 2.35 亿个权重中的大多数属于单个变换器块。这些权重随后被共享到 12 层中。因此，在训练或推理过程中，算法必须在超过
    20 亿个参数上执行！
- en: Due to these reasons, ALBERT is suited better for problems when the speed can
    be traded off for achieving higher accuracy. Ultimately, the NLP domain never
    stops and is constantly progressing towards new optimisation techniques. It is
    very likely that the speed rate in ALBERT will be improved in the near future.
    The paper’s authors have already mentioned methods like **sparse attention** and
    **block attention** as potential algorithms for ALBERT acceleration.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些原因，ALBERT 更适合于在速度可以权衡的情况下实现更高的准确性。*最终*，NLP 领域从未停滞不前，并且不断向新的优化技术发展。ALBERT
    的速度很可能在不久的将来得到改善。论文的作者已经提到了如**稀疏注意力**和**块注意力**等方法，作为 ALBERT 加速的潜在算法。
- en: Resources
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源
- en: '[ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/pdf/1909.11942.pdf)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ALBERT：用于自监督学习语言表示的轻量级 BERT](https://arxiv.org/pdf/1909.11942.pdf)'
- en: '*All images unless otherwise noted are by the author*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，否则所有图片均由作者提供*'
