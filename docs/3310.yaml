- en: Large Language Models, ALBERT — A Lite BERT for Self-supervised Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/albert-22983090d062?source=collection_archive---------4-----------------------#2023-11-07](https://towardsdatascience.com/albert-22983090d062?source=collection_archive---------4-----------------------#2023-11-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Understand essential techniques behind BERT architecture choices for producing
    a compact and efficient model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@slavahead?source=post_page-----22983090d062--------------------------------)[![Vyacheslav
    Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----22983090d062--------------------------------)[](https://towardsdatascience.com/?source=post_page-----22983090d062--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----22983090d062--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----22983090d062--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falbert-22983090d062&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----22983090d062---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----22983090d062--------------------------------)
    ·7 min read·Nov 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F22983090d062&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falbert-22983090d062&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----22983090d062---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F22983090d062&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falbert-22983090d062&source=-----22983090d062---------------------bookmark_footer-----------)![](../Images/4513d64c7ebd927038674efd312b98ff.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent years, the evolution of large language models has skyrocketed. BERT
    became one of the most popular and efficient models allowing to solve a wide range
    of NLP tasks with high accuracy. After BERT, a set of other models appeared later
    on the scene demonstrating outstanding results as well.
  prefs: []
  type: TYPE_NORMAL
- en: The obvious trend that became easy to observe is the fact that **with time large
    language models (LLMs) tend to become more complex by exponentially augmenting
    the number of parameters and data they are trained on**. Research in deep learning
    showed that such techniques usually lead to better results. Unfortunately, the
    machine learning world has already dealt with several problems regarding LLMs,
    and scalability has become the main obstacle in effective training, storing and
    using them.
  prefs: []
  type: TYPE_NORMAL
- en: As a consequence, new LLMs have been recently developed to tackle scalability
    issues. In this article, we will discuss ALBERT which was invented in 2020 with
    an objective of significant reduction of BERT parameters.
  prefs: []
  type: TYPE_NORMAL
- en: ALBERT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand the underlying mechanisms in ALBERT, we are going to refer to
    its [official paper](https://arxiv.org/pdf/1909.11942.pdf). For the most part,
    ALBERT derives the same architecture from BERT. There are three principal differences
    in the choice of the model’s architecture which are going to be addressed and
    explained below.
  prefs: []
  type: TYPE_NORMAL
- en: Training and fine-tuning procedures in ALBERT are analogous to those in BERT.
    Like BERT, ALBERT is pretrained on English Wikipedia (2500M words) and BookCorpus
    (800M words).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 1\. Factorized Parameter Embedding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When an input sequence is tokenized, each of the tokens is then mapped to one
    of the vocabulary embeddings. These embeddings are used for the input to BERT.
  prefs: []
  type: TYPE_NORMAL
- en: Let *V* be the vocabulary size (the total number of possible embeddings) and
    *H* — embedding dimensionality. Then for each of the *V* embeddings, we need to
    store *H* values resulting in a *V x H* embedding matrix. As turns out in practice,
    this matrix usually has huge sizes and requires a lot of memory to store it. But
    a more global problem is that most of the time the elements of an embedding matrix
    are trainable and it requires a lot of resources for the model to learn appropriate
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, let us take the BERT base model: it has a vocabulary of 30K tokens,
    each represented by a 768-component embedding. In total, this results in 23M weights
    to be stored and trained. For larger models, this number is even larger.'
  prefs: []
  type: TYPE_NORMAL
- en: This problem can be avoided by using matrix factorization. The original vocabulary
    matrix *V x H* can be decomposed into a pair of smaller matrices of sizes *V x
    E* and *E x H*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/840860bb19b5dc0b624f748598cc2431.png)'
  prefs: []
  type: TYPE_IMG
- en: Vocabulary matrix factorization
  prefs: []
  type: TYPE_NORMAL
- en: As a consequence, instead of using *O(V x H)* parameters, decomposition results
    in only *O(V x E + E x H)* weights. Obviously, this method is effective when *H
    >> E*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Another great aspect of matrix factorization is the fact that it does not
    change the lookup process for obtaining token embeddings**: each row of the left
    decomposed matrix *V x E* maps a token to its corresponding embedding in the same
    simple way as it was in the original matrix *V x H*. This way, the dimensionality
    of embeddings decreases from *H* to *E*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, in the case of decomposed matrices, to obtain the input for BERT,
    the mapped embeddings need then to be projected into hidden BERT space: this is
    done by multiplying a corresponding row of the left matrix by columns of the right
    matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Cross-layer Parameter Sharing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the ways to reduce the model’s parameters is to make them shareable.
    This means that they all share the same values. For the most part, it simply reduces
    the memory required to store weights. However, **standard algorithms like backpropagation
    or inference will still have to be executed on all parameters**.
  prefs: []
  type: TYPE_NORMAL
- en: '**One of the most optimal ways to share weights occurs when they are located
    in different but similar blocks of the model**. Putting them into similar blocks
    results in a higher chance that most of the calculations for shareable parameters
    during forward propagation or backpropagation will be the same. This gives more
    opportunities for designing an efficient computation framework.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The mentioned idea is implemented in ALBERT which consists of a set of Transformer
    blocks with the same structure making parameter sharing more efficient. In fact,
    there exist several ways of parameter sharing in Transformers across layers:'
  prefs: []
  type: TYPE_NORMAL
- en: share only attention parameters;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: share only forward neural network (FNN) parameters;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: share all parameters (used in ALBERT).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/ece8501f78ebd011ebf2f535dbc363e2.png)'
  prefs: []
  type: TYPE_IMG
- en: Different parameter sharing strategies
  prefs: []
  type: TYPE_NORMAL
- en: In general, it is possible to divide all transformer layers into N groups of
    size M each where every group shares parameters within layers it has. Researchers
    found out that the smaller the group size M is, the better the results are. However,
    decreasing group size M leads to a significant increase in total parameters.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 3\. Sentence Order Prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'BERT focuses on mastering two objectives when pretraining: masked language
    modeling (MSM) and next sentence prediction (NSP). In general, MSM was designed
    to improve BERT’s ability to gain linguistic knowledge and the goal of NSP was
    to improve BERT’s performance on particular downstream tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, multiple studies showed that it might be beneficial to get rid
    of the NSP objective mainly because of its simplicity, compared to MLM. Following
    this idea, ALBERT researchers also decided to remove the NSP task and replace
    it with sentence order prediction (SOP) problem whose goal is to predict whether
    both sentences are located in correct or inverse order.
  prefs: []
  type: TYPE_NORMAL
- en: Speaking of the training dataset, all positive pairs of input sentences are
    collected sequentially within the same text passage (the same method as in BERT).
    For negative sentences, the principle is the same except for the fact that both
    sentences go in inverse order.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f80b54ffb9831c235cb11aa7d9a94c34.png)'
  prefs: []
  type: TYPE_IMG
- en: Composition of positive and negative training pairs in BERT and in ALBERT
  prefs: []
  type: TYPE_NORMAL
- en: It was shown that models trained with the NSP objective cannot accurately solve
    SOP tasks while models trained with the SOP objective perform well on NSP problems.
    These experiments prove that ALBERT is better adapted for solving various downstream
    tasks than BERT.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: BERT vs ALBERT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The detailed comparison between BERT and ALBERT is illustrated in the diagram
    below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6a187fe7542f3b96ce1e044f44544e3f.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparison between different variations of BERT and ALBERT models. The speed
    measured under the same configurations shows how fast models iterated through
    the training data. The speed values are shown relatively for each model (BERT
    large is taken as a baseline whose speed equals 1x). The accuracy score was measured
    on GLUE, SQuAD and RACE benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the most interesting observations:'
  prefs: []
  type: TYPE_NORMAL
- en: By having only 70% of the parameters of BERT large, the xxlarge version of ALBERT
    achieves a better performance on downstream tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ALBERT large achieves comparable performance, compared to BERT large, and is
    faster 1.7x times due to the massive parameter size compression.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All ALBERT models have an embedding size of 128\. As was shown in the ablation
    studies in the paper, this is the optimal value. Increasing the embedding size,
    for example, up to 768, improves metrics but no more than 1% in absolute values
    which is not so much regarding the increasing complexity of the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Though ALBERT xxlarge processes a single iteration of data 3.3x slower than
    BERT large, experiments showed that if training both of these models for the same
    amount of time, then ALBERT xxlarge demonstrates a considerably better average
    performance on benchmarks than BERT large (88.7% vs 87.2%).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experiments showed that ALBERT models with wide hidden sizes (≥ 1024) do not
    benefit a lot from an increase in the number of layers. That is one of the reasons
    why the number of layers was reduced from 24 in ALBERT large to 12 in the xxlarge
    version.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/1d427426e58d971a379755edce79eff8.png)'
  prefs: []
  type: TYPE_IMG
- en: Performance of ALBERT large (18M parameters) with the increase of number of
    layers. Models in the diagram with ≥ 3 layers were fine-tuned from the checkpoint
    of the previous model. It can be observed that after reaching 12 layers, the performance
    increase gets slower and gradually falls after 24 layers.
  prefs: []
  type: TYPE_NORMAL
- en: A similar phenomenon occurs with the increase of in hidden-layer size. Increasing
    it with values larger than 4096 degrades the model performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/cc8e835a310620303d87a13acdf67135.png)'
  prefs: []
  type: TYPE_IMG
- en: Performance of ALBERT large (3-layer configuration from the diagram above) with
    the increase of hidden size. The hidden size of 4096 is the optimal value.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At first sight, ALBERT seems a preferable choice over original BERT models as
    it outperforms them on downstream tasks. Nevertheless, ALBERT requires much more
    computations due to its longer structures. A good example of this issue is ALBERT
    xxlarge which has 235M parameters and 12 encoder layers. The majority of these
    235M weights belong to a single transformer block. The weights are then shared
    for each of the 12 layers. Therefore, during training or inference, the algorithm
    has to be executed on more than 2 billion parameters!
  prefs: []
  type: TYPE_NORMAL
- en: Due to these reasons, ALBERT is suited better for problems when the speed can
    be traded off for achieving higher accuracy. Ultimately, the NLP domain never
    stops and is constantly progressing towards new optimisation techniques. It is
    very likely that the speed rate in ALBERT will be improved in the near future.
    The paper’s authors have already mentioned methods like **sparse attention** and
    **block attention** as potential algorithms for ALBERT acceleration.
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/pdf/1909.11942.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*All images unless otherwise noted are by the author*'
  prefs: []
  type: TYPE_NORMAL
