["```py\npattern = r'\\[(.*?)\\] (.*?): (.*)'\nmatches = re.findall(pattern, text)\ntext = [(x1, x2.lower()) for x0, x1, x2 in matches]\n```", "```py\n[\n    (2018-03-12 16:03:59, \"Alice\", \"Hi, how are you guys?\"),\n    (2018-03-12 16:05:36, \"Tom\", \"I am good thanks!\"),\n    ...\n]\n```", "```py\nfrom nltk.tokenize import RegexpTokenizer\n\ndef custom_tokenizer(txt: str, spec_tokens: List[str], pattern: str=\"|\\d|\\\\w+|[^\\\\s]\") -> List[str]:\n    \"\"\"\n    Tokenize text into words or characters using NLTK's RegexpTokenizer, considerung \n    given special combinations as single tokens.\n\n    :param txt: The corpus as a single string element.\n    :param spec_tokens: A list of special tokens (e.g. ending, out-of-vocab).\n    :param pattern: By default the corpus is tokenized on a word level (split by spaces).\n                    Numbers are considered single tokens.\n\n    >> note: The pattern for character level tokenization is '|.'\n    \"\"\"\n    pattern = \"|\".join(spec_tokens) + pattern\n    tokenizer = RegexpTokenizer(pattern)\n    tokens = tokenizer.tokenize(txt)\n    return tokens\n```", "```py\n[\"Alice:\", \"Hi\", \"how\", \"are\", \"you\", \"guys\", \"?\", \"<END>\", \"Tom:\", ... ]\n```", "```py\nfrom collections import Counter\n\ndef get_infrequent_tokens(tokens: Union[List[str], str], min_count: int) -> List[str]:\n    \"\"\"\n    Identify tokens that appear less than a minimum count.\n\n    :param tokens: When it is the raw text in a string, frequencies are counted on character level.\n                   When it is the tokenized corpus as list, frequencies are counted on token level.\n    :min_count: Threshold of occurence to flag a token.\n    :return: List of tokens that appear infrequently. \n    \"\"\"\n    counts = Counter(tokens)\n    infreq_tokens = set([k for k,v in counts.items() if v<=min_count])\n    return infreq_tokens\n\ndef mask_tokens(tokens: List[str], mask: Set[str]) -> List[str]:\n    \"\"\"\n    Iterate through all tokens. Any token that is part of the set, is replaced by the unknown token.\n\n    :param tokens: The tokenized corpus.\n    :param mask: Set of tokens that shall be masked in the corpus.\n    :return: List of tokenized corpus after the masking operation.\n    \"\"\"\n    return [t.replace(t, unknown_token) if t in mask else t for t in tokens]\n\ninfreq_tokens = get_infrequent_tokens(tokens, min_count=2)\ntokens = mask_tokens(tokens, infreq_tokens)\n```", "```py\n[\"Alice:\", \"Hi\", \"how\", \"are\", \"you\", \"<UNK>\", \"?\", \"<END>\", \"Tom:\", ... ]\n```", "```py\nimport torch\n\ndef encode(s: list, vocab: list) -> torch.tensor:\n    \"\"\"\n    Encode a list of tokens into a tensor of integers, given a fixed vocabulary. \n    When a token is not found in the vocabulary, the special unknown token is assigned. \n    When the training set did not use that special token, a random token is assigned.\n    \"\"\"\n    rand_token = random.randint(0, len(vocab))\n\n    map = {s:i for i,s in enumerate(vocab)}\n    enc = [map.get(c, map.get(unknown_token, rand_token)) for c in s]\n    enc = torch.tensor(enc, dtype=torch.long)\n    return enc\n```", "```py\ntorch.tensor([8127, 115, 2363, 3, ..., 14028])\n```", "```py\nself.word_embedding = nn.Embedding(vocab_size, embed_size)\nself.pos_embedding = nn.Embedding(block_size, embed_size)\n```", "```py\nclass Head(nn.Module):\n    \"\"\"\n    This module performs self-attention operations on the input tensor, producing \n    an output tensor with the same time-steps but different channels. \n\n    :param head_size: The size of the head in the multi-head attention mechanism.\n    \"\"\"\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(embed_size, head_size, bias=False)\n        self.query = nn.Linear(embed_size, head_size, bias=False)\n        self.value = nn.Linear(embed_size, head_size, bias=False)\n\n    def forward(self, x):\n        \"\"\"\n        # input of size (batch, time-step, channels)\n        # output of size (batch, time-step, head size)\n        \"\"\"\n        B,T,C = x.shape\n        k = self.key(x)                                     \n        q = self.query(x)                                   \n\n        # compute attention scores\n        wei = q @ k.transpose(-2,-1)                        \n        wei /= math.sqrt(k.shape[-1])                       \n\n        # avoid look-ahead\n        tril = torch.tril(torch.ones(T, T))\n        wei = wei.masked_fill(tril == 0, float(\"-inf\"))    \n        wei = F.softmax(wei, dim=-1)\n\n        # weighted aggregation of the values\n        v = self.value(x)    \n        out = wei @ v\n        return out\n```", "```py\nclass MultiHeadAttention(nn.Module):\n    \"\"\"\n    This class contains multiple `Head` objects, which perform self-attention \n    operations in parallel.\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        head_size = embed_size // n_heads\n        heads_list = [Head(head_size) for _ in range(n_heads)]\n\n        self.heads = nn.ModuleList(heads_list)\n        self.linear = nn.Linear(n_heads * head_size, embed_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        heads_list = [h(x) for h in self.heads]\n        out = torch.cat(heads_list, dim=-1)\n        out = self.linear(out)\n        out = self.dropout(out)\n        return out\n```", "```py\nclass FeedFoward(nn.Module):\n    \"\"\"\n    This module passes the input tensor through a series of linear transformations \n    and non-linear activations.\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(embed_size, 4 * embed_size), \n            nn.ReLU(),\n            nn.Linear(4 * embed_size, embed_size),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n```", "```py\nclass Block(nn.Module):\n    \"\"\"\n    This module contains a single transformer block, which consists of multi-head \n    self-attention followed by feed-forward neural networks.\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n\n        self.sa = MultiHeadAttention()\n        self.ffwd = FeedFoward()\n        self.ln1 = nn.LayerNorm(embed_size)\n        self.ln2 = nn.LayerNorm(embed_size)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n```", "```py\nimport json\n\nimport torch\n\nfrom config import eval_interval, learn_rate, max_iters\nfrom src.model import GPTLanguageModel\nfrom src.utils import current_time, estimate_loss, get_batch\n\ndef model_training(update: bool) -> None:\n    \"\"\"\n    Trains or updates a GPTLanguageModel using pre-loaded data.\n\n    This function either initializes a new model or loads an existing model based\n    on the `update` parameter. It then trains the model using the AdamW optimizer\n    on the training and validation data sets. Finally the trained model is saved.\n\n    :param update: Boolean flag to indicate whether to update an existing model.\n    \"\"\"\n    # LOAD DATA -----------------------------------------------------------------\n\n    train_data = torch.load(\"assets/output/train.pt\")\n    valid_data = torch.load(\"assets/output/valid.pt\")\n\n    with open(\"assets/output/vocab.txt\", \"r\", encoding=\"utf-8\") as f:\n        vocab = json.loads(f.read())\n\n    # INITIALIZE / LOAD MODEL ---------------------------------------------------\n\n    if update:\n        try:\n            model = torch.load(\"assets/models/model.pt\")\n            print(\"Loaded existing model to continue training.\")\n        except FileNotFoundError:\n            print(\"No existing model found. Initializing a new model.\")\n            model = GPTLanguageModel(vocab_size=len(vocab))\n\n    else:\n        print(\"Initializing a new model.\")\n        model = GPTLanguageModel(vocab_size=len(vocab))\n\n    # initialize optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=learn_rate)\n\n    # number of model parameters\n    n_params = sum(p.numel() for p in model.parameters())\n    print(f\"Parameters to be optimized: {n_params}\\n\", )\n\n    # MODEL TRAINING ------------------------------------------------------------\n\n    for i in range(max_iters):\n\n        # evaluate the loss on train and valid sets every 'eval_interval' steps\n        if i % eval_interval == 0 or i == max_iters - 1:\n            train_loss = estimate_loss(model, train_data)\n            valid_loss = estimate_loss(model, valid_data)\n\n            time = current_time()\n            print(f\"{time} | step {i}: train loss {train_loss:.4f}, valid loss {valid_loss:.4f}\")\n\n        # sample batch of data\n        x_batch, y_batch = get_batch(train_data)\n\n        # evaluate the loss\n        logits, loss = model(x_batch, y_batch)\n        optimizer.zero_grad(set_to_none=True)\n        loss.backward()\n        optimizer.step()\n\n    torch.save(model, \"assets/models/model.pt\")\n    print(\"Model saved\")\n```", "```py\nimport json\nimport random\n\nimport torch\nfrom prompt_toolkit import prompt\nfrom prompt_toolkit.completion import WordCompleter\n\nfrom config import end_token, n_chats\nfrom src.utils import custom_tokenizer, decode, encode, print_delayed\n\ndef conversation() -> None:\n    \"\"\"\n    Emulates chat conversations by sampling from a pre-trained GPTLanguageModel.\n\n    This function loads a trained GPTLanguageModel along with vocabulary and \n    the list of special tokens. It then enters into a loop where the user specifies \n    a contact. Given this input, the model generates a sample response. The conversation \n    continues until the user inputs the end token.\n    \"\"\"\n    with open(\"assets/output/vocab.txt\", \"r\", encoding=\"utf-8\") as f:\n        vocab = json.loads(f.read())\n\n    with open(\"assets/output/contacts.txt\", \"r\", encoding=\"utf-8\") as f:\n        contacts = json.loads(f.read())   \n\n    spec_tokens = contacts + [end_token]\n    model = torch.load(\"assets/models/model.pt\")\n    completer = WordCompleter(spec_tokens, ignore_case=True)\n\n    input = prompt(\"message >> \", completer=completer, default=\"\")\n    output = torch.tensor([], dtype=torch.long)\n    print()\n\n    while input != end_token:\n        for _ in range(n_chats):\n\n            add_tokens = custom_tokenizer(input, spec_tokens)\n            add_context = encode(add_tokens, vocab)\n            context = torch.cat((output, add_context)).unsqueeze(1).T\n\n            n0 = len(output)\n            output = model.generate(context, vocab)\n            n1 = len(output)\n\n            print_delayed(decode(output[n0-n1:], vocab))\n            input = random.choice(contacts)\n\n        input = prompt(\"\\nresponse >> \", completer=completer, default=\"\")\n        print()\n```"]