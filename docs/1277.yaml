- en: A Gentle Introduction to GPT Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-gentle-introduction-to-gpt-models-e02b093a495b?source=collection_archive---------1-----------------------#2023-04-12](https://towardsdatascience.com/a-gentle-introduction-to-gpt-models-e02b093a495b?source=collection_archive---------1-----------------------#2023-04-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Welcome to the new world of token generators
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----e02b093a495b--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----e02b093a495b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e02b093a495b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e02b093a495b--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----e02b093a495b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-gpt-models-e02b093a495b&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----e02b093a495b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e02b093a495b--------------------------------)
    ·9 min read·Apr 12, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe02b093a495b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-gpt-models-e02b093a495b&user=Benjamin+Marie&userId=ad2a414578b3&source=-----e02b093a495b---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe02b093a495b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-gpt-models-e02b093a495b&source=-----e02b093a495b---------------------bookmark_footer-----------)![](../Images/db0b10a3b127c9e3fba8e78be613555e.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Image from [Pixabay](https://pixabay.com/illustrations/dinosaur-t-rex-animal-dino-5631999/)
    — Modified by author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: With the recent releases of ChatGPT and GPT-4, GPT models have drawn a lot of
    interest from the scientific community. These new versions of OpenAI’s GPT models
    are so powerful and versatile that it may take a lot of time before we can exploit
    their full potential.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Even though they are impressive, what you may not know is that the main ideas
    and algorithms behind GPT models are far from new.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Whether you are a seasoned data scientist or just someone curious about GPT,
    knowing how GPT models evolved is particularly insightful on the impact of data
    and what to expect for the coming years.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I explain how GPT models became what they are today. I’ll mainly
    focus on how OpenAI scaled GPT models over the years. I’ll also give some pointers
    if you want to get started using GPT models.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Generative pre-trained language models
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GPT models are language models.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Language models have existed for [more than 50 years](https://web.stanford.edu/~jurafsky/slp3/3.pdf).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: The first generation of language models was “*n*-gram based”. They modeled the
    probability of a word given some previous words.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, if you have the sentence:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '*The cat sleeps in the kitchen.*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: With *n*=3, you can get from a 3-gram language model the probability of having
    “*in*” following “*cat sleeps*”.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '*n*-gram models remained useful in many natural language and speech processing
    tasks until the beginning of the 2010s.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: They suffer several limitations. The computational complexity dramatically increases
    with a higher *n*. So these models were often limited to *n*=5 or lower.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Then, thanks to neural networks and the use of more powerful machines, this
    main limitation was alleviated and it became possible to compute the probability
    for much longer n-grams, for instance for *n*=20 or higher.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Generating text with these models was also possible but their outputs were of
    a so poor quality that they were rarely used for this purpose.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Then, in 2018, [OpenAI proposed the first GPT model](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: GPT stands for “generative pre-trained”. “Pre-trained” means that the model
    was simply trained on a large amount of text to model probabilities without any
    other purpose than language modeling. GPT models can then be fine-tuned, i.e.,
    further trained, to perform more specific tasks.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: For instance, you can use a small dataset of news summaries to obtain a GPT
    model very good at news summarization. Or fine-tune it on French-English translations
    to obtain a machine translation system capable of translating from French to English.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: The term “pre-training” suggests that the models are not fully trained
    and that another step is needed. With recent models, the need for fine-tuning
    tends to disappear. The pre-trained models are now directly used in applications.*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT models are now very good in almost all natural language processing tasks.
    I particularly studied their ability to do machine translation, as you can read
    in the following article:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '[](/translate-with-gpt-3-9903c4a6f385?source=post_page-----e02b093a495b--------------------------------)
    [## Translate with GPT-3'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Machine translation but without a machine translation system
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/translate-with-gpt-3-9903c4a6f385?source=post_page-----e02b093a495b--------------------------------)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: The scale of the training, and the Transformer neural network architecture that
    they exploit, are the main reasons why they can generate fluent text.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Since 2018 and the first GPT, several versions and subversions of GPT followed.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 4 versions and many more subversions
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GPT and GPT-2
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[GPT-2 came out only a few months after the first GPT](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
    was announced. *Note: The term “GPT” was never mentioned in the scientific paper
    describing the first GPT. Arguably, we could say that “GPT-1” never existed. To
    the best of my knowledge, it was also never released.*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '*What is the difference between GPT and GPT-2?*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: The scale. GPT-2 is much larger than GPT.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: GPT was trained on the BookCorpus which contains 7,000 books. The model has
    120 million parameters.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: What’s a parameter?
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A parameter is a variable learned during the model training. Typically, a model
    with more parameters is bigger and better.
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 120 million was a huge number in 2018.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: With GPT-2, OpenAI proposed an even bigger model containing 1.5 billion parameters.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: It was trained on an undisclosed corpus called WebText. This corpus is 10 times
    larger than BookCorpus ([according to the paper describing GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI gradually released 4 versions of GPT-2:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'small: 124 million parameters'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'medium: 355 million parameters'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'large: 774 million parameters'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'xl: 1.5 billion parameters'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are [all publicly available](https://openai.com/research/gpt-2-1-5b-release)
    and can be used in commercial products.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: While GPT-2-XL excels at generating fluent text in the wild, i.e., without any
    particular instructions or fine-tuning, it remains far less powerful than more
    recent GPT models for specific tasks.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: The release of GPT-2-XL was the last open release of a GPT model by OpenAI.
    GPT-3 and GPT-4 can only be used through OpenAI’s API.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GPT-3 was announced in 2020\. With its 175 billion parameters, it was an even
    bigger jump from GPT-2 than GPT-2 from the first GPT.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[This is also from GPT-3 that OpenAI stopped to disclose precise training information
    about GPT models.](https://arxiv.org/pdf/2005.14165.pdf)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Today, there are 7 GPT-3 models available through OpenAI’s API but we only know
    little about them.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: With GPT-3, OpenAI demonstrated that GPT models can be extremely good for specific
    language generation tasks if the users provide a few examples of the task they
    want the model to achieve.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3.5
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the GPT-3 models running in the API and attracting more and more users,
    OpenAI could collect a very large dataset of user inputs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: They exploited these inputs to further improve their models.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: They used a technique called [reinforcement learning from human feedback (RLHF)](https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/).
    I won’t explain the details here but you can find them in [a blog post](https://openai.com/research/instruction-following)
    published by OpenAI.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, thanks to RLHF, GPT-3.5 is much better at following user instructions
    than GPT-3\. OpenAI denotes this class of GPT models as “instructGPT”.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: With GPT-3.5, you can “prompt” the model to perform a specific task without
    the need to give it examples of the task. You just have to write the “right” prompt
    to get the best result. This is where “prompt engineering” becomes important and
    why skilled [prompt engineers are receiving incredible job offers](https://www.businessinsider.com/ai-prompt-engineer-jobs-pay-salary-requirements-no-tech-background-2023-3).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3.5 is the current model used to power ChatGPT.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GPT-4 has been released in March 2023.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: We know almost nothing about its training.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'The main difference with GPT-3/GPT-3.5 is that GPT-4 is bimodal: It can take
    as input images and text.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'It can generate text but won’t directly generate images. *Note: GPT-4 can generate
    the code that can generate an image, or retrieve one from the Web.*'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing these lines, GPT-4 is still in a “limited beta”.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ChatGPT is just a user interface with chat functionalities. When you write something
    with ChatGPT, it’s a GPT-3.5 model that generates the answer.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'A particularity of ChatGPT is that it’s not just taking as input the current
    query of the user as an out-of-the-box GPT model would do. To properly work as
    a chat engine, ChatGPT must keep track of the conversation: What has been said,
    what is the user goal, etc.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI didn’t disclose how it does that. Given that GPT models can only accept
    a prompt of a limited length (I’ll explain this later), ChatGPT can’t just concatenate
    all the dialogue turns together to put them in the same prompt. This kind of prompt
    could be way too large to be handled by GPT-3.5.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: How to use GPT models?
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can easily get GPT-2 models online and use them on your computer. If you
    want to run large language models on your machine, you may be interested in reading
    my tutorial:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://pub.towardsai.net/run-very-large-language-models-on-your-computer-390dd33838bb?source=post_page-----e02b093a495b--------------------------------)
    [## Run Very Large Language Models on Your Computer'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: With PyTorch and Hugging Face’s device_map
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pub.towardsai.net](https://pub.towardsai.net/run-very-large-language-models-on-your-computer-390dd33838bb?source=post_page-----e02b093a495b--------------------------------)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: For GPT-3 and GPT-3.5, we have no other choice than to use OpenAI’s API. You
    will first need to create an OpenAI account on [their website](https://openai.com/).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Once you have an account, you can start playing with the models inside the “playground”
    which is a sandbox that OpenAI proposes to experiment with the models. You can
    access it only when you are logged in.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: If you want to directly use the models in your application, OpenAI and the open-source
    community offer libraries in many languages, such as [Python, Node.js, and PHP,](https://platform.openai.com/docs/libraries)
    to call the models using OpenAI API.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'You can create and get your OpenAI API key in your OpenAI account. *Note: Keep
    this key secret. Anyone who has it can consume your OpenAI credits.*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在你的 OpenAI 账户中创建并获取你的 OpenAI API 密钥。*注意：请保密此密钥。任何拥有它的人都可以消耗你的 OpenAI 额度。*
- en: Each model has different settings that you can adjust. Be aware that GPT models
    are *non-deterministic.* If you prompt a model twice with the same prompt there
    is a high chance that you will have two close but different answers.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型有不同的设置，你可以进行调整。请注意，GPT 模型是*非确定性的*。如果你用相同的提示两次调用模型，很有可能会得到两个相似但不同的回答。
- en: '*Note: If you want to reduce the variations between answers given the same
    prompt, you can set to 0 the “temperature” parameter of the model. As a side effect,
    it will also significantly decrease the diversity of the answers, in other words,
    the generated text may be more redundant.*'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：如果你想减少相同提示下回答的变异性，可以将模型的“温度”参数设置为0。副作用是，它也会显著减少答案的多样性，换句话说，生成的文本可能会更冗余。*'
- en: You will also have to care about the “maximum content length”. This is the length
    of your prompt in addition to the length of the answer generated by GPT. For instance,
    GPT-3.5-turbo has a “maximum content length” of 4,096 *tokens*.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要注意“最大内容长度”。这是你的提示长度加上 GPT 生成的回答长度。例如，GPT-3.5-turbo 的“最大内容长度”是 4,096 *令牌*。
- en: '**A token is not a word.**'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**一个令牌不是一个单词。**'
- en: A token is the minimal unit of text used by the GPT models to generate text.
    Yes, GPT models are not exactly word generators but rather token generators. A
    token can be a character, a piece of word, a word, or even a sequence of words
    for some languages.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 令牌是 GPT 模型用于生成文本的最小单位。是的，GPT 模型并非真正的单词生成器，而是令牌生成器。令牌可以是一个字符、一个词的一部分、一个单词，甚至是某些语言中的词组。
- en: OpenAI gives an example in the [API documentation](https://platform.openai.com/docs/guides/chat/introduction).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 在[API 文档](https://platform.openai.com/docs/guides/chat/introduction)中给出了一个示例。
- en: '`*"ChatGPT is great!"*` *is encoded into six tokens:* `*["Chat", "G", "PT",
    " is", " great", "!"]*`*.*'
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`*"ChatGPT is great!"*` *被编码成六个令牌：* `*["Chat", "G", "PT", " is", " great",
    "!"]*`*。*'
- en: As a rule of thumb, count that 750 English words yield 1,000 tokens.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，750 个英语单词约等于 1,000 个令牌。
- en: In my opinion, managing the “maximum content length” is the most tedious part
    of working with the OpenAI API. First, there is no straightforward way to know
    how many tokens your prompt contains. Then, you can’t know in advance how many
    tokens will be in the answer of the model.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为，管理“最大内容长度”是使用 OpenAI API 中最繁琐的部分。首先，没有简单的方法来知道你的提示包含多少个令牌。然后，你不能提前知道模型的回答将包含多少个令牌。
- en: You have to guess. And you can only guess right if you have some experience
    with the models. I recommend experimenting a lot with them to better gauge how
    long can be the answers given your prompts.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要猜测。只有当你有一定的模型经验时，才能猜对。我建议你多进行实验，以更好地评估在给定提示的情况下，回答可能有多长。
- en: If your prompt is too long, the answer will be cut-off.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的提示太长，回答将会被截断。
- en: I won’t give more details about the API here as it can become quite technical.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会在这里提供关于 API 的更多细节，因为它可能变得相当技术性。
- en: Limitations of GPT models
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT 模型的局限性
- en: GPT models are only token generators trained on the Web. They are biased by
    the content they were trained on and thus cannot be considered fully safe.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 模型仅仅是基于网络训练的令牌生成器。它们受限于训练数据的内容，因此不能被认为是完全安全的。
- en: Since GPT-3.5, OpenAI has trained its model to avoid answering harmful content.
    To achieve this, they used machine learning techniques and consequently this “self-moderation”
    of the model can’t be 100% trusted.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 自 GPT-3.5 起，OpenAI 已训练其模型以避免回答有害内容。为实现这一目标，他们使用了机器学习技术，因此这种“自我调节”无法100%被信任。
- en: This self-moderation may work for a given prompt, but may then completely fail
    after just changing one word in this prompt.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这种自我调节可能对某个特定提示有效，但在仅仅改变一个词后，可能会完全失效。
- en: I also recommend reading the [Terms of Use](https://openai.com/policies/terms-of-use)
    of OpenAI products. In this document, the limitations of GPT models appear more
    clearly in my opinion.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我还建议阅读 OpenAI 产品的[使用条款](https://openai.com/policies/terms-of-use)。在这份文档中，我认为
    GPT 模型的局限性更为清晰。
- en: 'If you plan to build your application with the API, you should particularly
    pay attention to this point:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你计划使用 API 构建应用程序，你应该特别注意这一点：
- en: You must be at least 13 years old to use the Services. If you are under 18 you
    must have your parent or legal guardian’s permission to use the Services. If you
    use the Services on behalf of another person or entity, you must have the authority
    to accept the Terms on their behalf. You must provide accurate and complete information
    to register for an account. You may not make your access credentials or account
    available to others outside your organization, and you are responsible for all
    activities that occur using your credentials.
  id: totrans-103
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Italy temporarily banned ChatGPT because it may generate inappropriate answers
    for people under 18, among other reasons.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie/italy-bans-chatgpt-europe-may-follow-c7222112f97e?source=post_page-----e02b093a495b--------------------------------)
    [## Italy Bans ChatGPT, Europe May Follow'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Towards a boom of ChatGPT wrappers “Ready for Italy”
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@bnjmn_marie/italy-bans-chatgpt-europe-may-follow-c7222112f97e?source=post_page-----e02b093a495b--------------------------------)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: If you are a developer building an application on top of OpenAI API, you must
    check the age of your users.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI also published a list of [usage policies pointing out all the prohibited
    usage of the models](https://openai.com/policies/usage-policies).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GPT models are very simple models and their architecture didn’t evolve much
    since 2018\. But when you train a simple model at a large scale on the right data
    and with the right hyperparameters, you can get extremely powerful AI models such
    as GPT-3 and GPT-4.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: They are so powerful that we have not nearly explored all their potential.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: While recent GPT models are not open-source, they remain easy to use with OpenAI’s
    API. You can also play with them through [ChatGPT](https://chat.openai.com/chat).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
