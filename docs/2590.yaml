- en: Prompt Ensembles Make LLMs More Reliable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/prompt-ensembles-make-llms-more-reliable-ae57ec35b5f7?source=collection_archive---------1-----------------------#2023-08-14](https://towardsdatascience.com/prompt-ensembles-make-llms-more-reliable-ae57ec35b5f7?source=collection_archive---------1-----------------------#2023-08-14)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Simple strategies for getting the most out of any language model…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wolfecameron.medium.com/?source=post_page-----ae57ec35b5f7--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----ae57ec35b5f7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ae57ec35b5f7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ae57ec35b5f7--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----ae57ec35b5f7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprompt-ensembles-make-llms-more-reliable-ae57ec35b5f7&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----ae57ec35b5f7---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ae57ec35b5f7--------------------------------)
    ·18 min read·Aug 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fae57ec35b5f7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprompt-ensembles-make-llms-more-reliable-ae57ec35b5f7&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----ae57ec35b5f7---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fae57ec35b5f7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprompt-ensembles-make-llms-more-reliable-ae57ec35b5f7&source=-----ae57ec35b5f7---------------------bookmark_footer-----------)![](../Images/917f4253782589036edfaf468a0ea247.png)'
  prefs: []
  type: TYPE_NORMAL
- en: (Photo by [Manuel Nägeli](https://unsplash.com/@gwundrig?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/7CcPLtywRso?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  prefs: []
  type: TYPE_NORMAL
- en: Anyone who has worked with large language models (LLMs) will know that prompt
    engineering is an informal and difficult process. Small changes to a prompt can
    cause massive changes to the model’s output, it is difficult (or even impossible
    in some cases) to know the impact that changing a prompt will have, and prompting
    behavior is highly dependent on the type of model being used. The fragile nature
    of prompt engineering is a harsh reality when we think about creating applications
    with LLMs. If we cannot predict how our model will behave, *how can we build a
    dependable system around this model?* Although LLMs are incredibly capable, this
    problem complicates their use in many practical scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: “Prompting is a brittle process wherein small modifications to the prompt can
    cause large variations in the model predictions, and therefore significant effort
    is dedicated towards designing a painstakingly perfect prompt for a task.” *—
    from [2]*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Given the fragile nature of LLMs, finding techniques that make these models
    more accurate and reliable has recently become a popular research topic. In this
    overview, we will focus on one technique in particular — *prompt ensembles.* Put
    simply, prompt ensembles are just sets of diverse…
  prefs: []
  type: TYPE_NORMAL
