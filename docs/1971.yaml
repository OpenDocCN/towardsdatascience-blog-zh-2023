- en: 'Variational Inference: The Basics'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/variational-inference-the-basics-f70ac511bcea?source=collection_archive---------1-----------------------#2023-06-16](https://towardsdatascience.com/variational-inference-the-basics-f70ac511bcea?source=collection_archive---------1-----------------------#2023-06-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@hylke.donker?source=post_page-----f70ac511bcea--------------------------------)[![Hylke
    C. Donker](../Images/bed587d1bb305ded80f7ce21bc4f4856.png)](https://medium.com/@hylke.donker?source=post_page-----f70ac511bcea--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f70ac511bcea--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f70ac511bcea--------------------------------)
    [Hylke C. Donker](https://medium.com/@hylke.donker?source=post_page-----f70ac511bcea--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fea1bfe4db7a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariational-inference-the-basics-f70ac511bcea&user=Hylke+C.+Donker&userId=ea1bfe4db7a8&source=post_page-ea1bfe4db7a8----f70ac511bcea---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f70ac511bcea--------------------------------)
    ·9 min read·Jun 16, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff70ac511bcea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariational-inference-the-basics-f70ac511bcea&user=Hylke+C.+Donker&userId=ea1bfe4db7a8&source=-----f70ac511bcea---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff70ac511bcea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariational-inference-the-basics-f70ac511bcea&source=-----f70ac511bcea---------------------bookmark_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: We live in the era of quantification. But rigorous quantification is easier
    said then done. In complex systems such as biology, data can be difficult and
    expensive to collect. While in high stakes applications, such as in medicine and
    finance, it is crucial to account for uncertainty. Variational inference — a methodology
    at the forefront of AI research — is a way to address these aspects.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'This tutorial introduces you to the basics: the when, why, and how of variational
    inference.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: When is variational inference useful?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Variational inference is appealing in the following three closely related usecases:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 1\. if you have little data (i.e., low number of observations),
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 2\. you care about uncertainty,
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 3\. for generative modelling.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: We will touch upon each usecase in our worked example.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Variational inference with little data
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/b18e9b277ce95df955571b2a22eacaf5.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b18e9b277ce95df955571b2a22eacaf5.png)'
- en: 'Fig. 1: Variational inference allows you to trade-of domain knowledge with
    information from examples. Image by Author.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：变分推断允许你在领域知识与样本信息之间进行权衡。图像由作者提供。
- en: Sometimes, data collection is expensive. For example, DNA or RNA measurements
    can easily cost a few thousand euros per observation. In this case, you can hardcode
    domain knowledge in lieu of extra samples. Variational inference can help to systematically
    “dial down” the domain knowledge as you gather more examples, and more heavily
    rely on the data (Fig. 1).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，数据收集是昂贵的。例如，DNA或RNA测量每次观察可能会花费几千欧元。在这种情况下，你可以用领域知识代替额外的样本进行硬编码。变分推断可以帮助你在收集更多样本时系统性地“减少”领域知识，并更多地依赖于数据（见图1）。
- en: 2\. Variational inference for uncertainty
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 不确定性的变分推断
- en: For safety critical applications, such as in finance and healthcare, uncertainty
    is important. Uncertainty can affect all aspects of the model, most obviously
    the predicted output. Less obvious are the model’s parameters (e.g., weights and
    biases). Instead of the usual arrays of numbers — the weights and biases — you
    can endow the parameters with a distribution to make them fuzzy. Variational inference
    allows you to infer the range(s) of reasonable values.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于安全关键型应用，如金融和医疗保健，确定性很重要。不确定性可以影响模型的所有方面，最明显的是预测输出。模型的参数（例如权重和偏置）则不那么明显。你可以将参数赋予一个分布，使其变得模糊，而不是通常的数字数组——权重和偏置。变分推断允许你推断出合理值的范围。
- en: 3\. Variational inference for generative modelling
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 用于生成建模的变分推断
- en: Generative models provide a complete specification how the data was generated.
    For example, how to generate an image of a cat or a dog. Usually, there is a latent
    representation ***z*** that carries semantic meaning (e.g., ***z*** descibes a
    siamese cat). Through a set of (non-linear) transformations and sampling steps,
    ***z*** is transformed into the actual image ***x*** (e.g., the pixel values of
    the siamese cat). Variational inference is a way to infer, and sample from, the
    latent semantic space ***z***. A well known example is the [variational auto encoder](https://en.wikipedia.org/wiki/Variational_autoencode).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型提供了数据生成的完整规范。例如，如何生成猫或狗的图像。通常，有一个潜在表示 ***z*** 具有语义意义（例如，***z*** 描述了一只暹罗猫）。通过一系列（非线性）变换和采样步骤，***z***
    被转换为实际的图像 ***x***（例如，暹罗猫的像素值）。变分推断是一种推断和从潜在语义空间 ***z*** 进行采样的方法。一个著名的例子是 [变分自编码器](https://en.wikipedia.org/wiki/Variational_autoencode)。
- en: What is variational inference?
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变分推断是什么？
- en: 'At its core, variational inference is a Bayesian undertaking [1]. In the Bayesian
    perspective, you still let the machine learn from the data, as usual. What is
    different, is that you give the model a hint ([a prior](https://en.wikipedia.org/wiki/Prior_probability))
    and allow the solution ([the posterior](https://en.wikipedia.org/wiki/Posterior_probability))
    to be more fuzzy. More concretely, say you have a training set ***X =* [*x***₁,
    ***x***₂,..,***x****ₘ*]ᵗ of *m* examples. We use Bayes’ theorem:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，变分推断是一种贝叶斯方法[1]。从贝叶斯的角度来看，你仍然让机器像往常一样从数据中学习。不同的是，你给模型一个提示（[先验](https://en.wikipedia.org/wiki/Prior_probability)），并允许解（[后验](https://en.wikipedia.org/wiki/Posterior_probability)）变得更加模糊。更具体地说，假设你有一个训练集
    ***X =* [*x***₁, ***x***₂,..,***x****ₘ*]ᵗ，共有 *m* 个样本。我们使用贝叶斯定理：
- en: '*p*(***Θ***|***X***) ***=*** *p*(***X***|***Θ***)*p*(***Θ***) /*p*(***X****),*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*(***Θ***|***X***) ***=*** *p*(***X***|***Θ***)*p*(***Θ***) /*p*(***X****),*'
- en: 'to infer a range — a distribution — of solutions ***Θ***. Contrast this with
    the conventional machine learning approach, where we minimise a loss *ℒ(****Θ,
    X****) =* ln *p*(***X***|***Θ***) to find one specific solution ***Θ****.* Bayesian
    inference revolves around finding a way to determine *p*(***Θ***|***X***): the
    *posterior* distribution of the parameters ***Θ*** given the training set ***X***.
    In general, this is a difficult problem. In practice, two ways are used to solve
    for *p*(***Θ***|***X***): (i) using simulation ([Markov chain Monte Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo))
    or (ii) through optimisation.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '推断一个范围——一个分布——的解决方案***Θ***。这与传统的机器学习方法形成对比，后者通过最小化损失 *ℒ(****Θ, X****) =* ln
    *p*(***X***|***Θ***)来寻找一个特定的解决方案***Θ****。贝叶斯推断的核心在于找到一种方法来确定 *p*(***Θ***|***X***):
    给定训练集 ***X*** 的参数 ***Θ*** 的*后验*分布。一般来说，这是一个困难的问题。实际上，有两种方法用于求解 *p*(***Θ***|***X***):
    (i) 使用模拟（[马尔科夫链蒙特卡罗](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo)）或
    (ii) 通过优化。'
- en: Variational inference is about option (ii).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 变分推断涉及选项 (ii)。
- en: The evidence lower bound (ELBO)
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 证据下界（ELBO）
- en: '![](../Images/7e9c650c4357a68ab040bb91dbf5e648.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e9c650c4357a68ab040bb91dbf5e648.png)'
- en: 'Fig. 2: **Sketch of variational inference.** We look for a distribution q(Θ)
    that is close to p(Θ|X). Image by Author.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: **变分推断的示意图。** 我们寻找一个接近 p(Θ|X) 的分布 q(Θ)。图像由作者提供。'
- en: 'The idea behind variational inference is to look for a distribution *q*(***Θ***)
    that is a stand-in (a surrogate) for *p*(***Θ***|***X***). We then try to make
    *q*[***Θ|Φ***(***X***)] look similar to *p*(***Θ***|***X***) by changing the values
    of ***Φ*** (Fig. 2). This is done by maximising the evidence lower bound (ELBO):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 变分推断的核心思想是寻找一个分布 *q*(***Θ***)，作为 *p*(***Θ***|***X***) 的替代（代理）。然后我们尝试通过改变 ***Φ***
    的值，使 *q*[***Θ|Φ***(***X***)] 看起来类似于 *p*(***Θ***|***X***)(见图 2)。这通过最大化证据下界（ELBO）来完成：
- en: '*ℒ*(***Φ***) *=* E[ln *p*(***X***,***Θ***) — ln *q*(***Θ|Φ)***],'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*ℒ*(***Φ***) *=* E[ln *p*(***X***,***Θ***) — ln *q*(***Θ|Φ***)],'
- en: where the expectation E[·] is taken over *q*(***Θ|Φ***). (Note that ***Φ***
    implicitly depends on the dataset ***X***, but for notational convenience we'll
    drop the explicit dependence.)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其中期望 E[·] 是对 *q*(***Θ|Φ***) 进行的。 （注意 ***Φ*** 隐式依赖于数据集 ***X***，但为了方便书写，我们将忽略这一显式依赖。）
- en: 'For gradient based optimisation of *ℒ* it looks, at first sight, like we have
    to be careful when taking derivatives (with respect to ***Φ***) because of the
    dependence of E[·] on *q*(***Θ|Φ***). Fortunately, autograd packages like [JAX](https://jax.readthedocs.io/en/latest/)
    support reparameterisation tricks [2] that allow you to directly take derivatives
    from random samples (e.g., of the gamma distribution) instead of relying on high
    variance black box variational approaches [3]. Long story short: estimate ∇ℒ(Φ)
    with a batch **[*Θ***₁, ***Θ***₂,..] ~ *q*(***Θ|Φ***) and let your autograd package
    worry about the details.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对 *ℒ* 进行基于梯度的优化时，乍一看，我们必须在对 ***Φ*** 求导时小心，因为 E[·] 对 *q*(***Θ|Φ***) 的依赖。幸运的是，像
    [JAX](https://jax.readthedocs.io/en/latest/) 这样的自动梯度包支持重参数化技巧 [2]，允许你直接从随机样本（例如伽马分布的样本）中进行求导，而无需依赖高方差的黑箱变分方法
    [3]。简而言之：使用一批 **[*Θ***₁, ***Θ***₂,..] ~ *q*(***Θ|Φ***) 来估计 ∇ℒ(Φ)，然后让你的自动梯度包处理细节。
- en: Variational inference from scratch
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头开始进行变分推断
- en: '![](../Images/314e056b8af484cffdf103e6a05339d0.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/314e056b8af484cffdf103e6a05339d0.png)'
- en: 'Fig. 3: Example image of a handwritten “zero” from sci-kit learn’s digits dataset.
    Image by Author.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 来自 scikit-learn 的手写“零”的示例图像。图像由作者提供。'
- en: To solidify our understanding let us implement variational inference from scratch
    using [JAX](https://jax.readthedocs.io/en/latest/). In this example, you will
    train a generative model on handwritten digits from [sci-kit learn](https://scikit-learn.org/).
    You can follow along with the [Colab notebook](https://colab.research.google.com/drive/1NPZfRkhrFfX8flbUJqfgKZ2z1w5JXs1R?usp=sharing).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了巩固我们的理解，让我们从头开始使用 [JAX](https://jax.readthedocs.io/en/latest/) 实现变分推断。在这个例子中，你将对来自
    [scikit-learn](https://scikit-learn.org/) 的手写数字进行生成模型训练。你可以按照 [Colab notebook](https://colab.research.google.com/drive/1NPZfRkhrFfX8flbUJqfgKZ2z1w5JXs1R?usp=sharing)
    进行操作。
- en: To keep it simple, we will only analyse the digit “zero”.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，我们将只分析数字“零”。
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Each image is a 8-by-8 array of discrete pixel values ranging from 0–16\. Since
    the pixels are count data, let’s model the pixels, ***x***, using the [Poisson
    distribution](https://en.wikipedia.org/wiki/Poisson_distribution) with a gamma
    *prior* for the rate ***Θ***. The rate ***Θ*** determines the average intensity
    of the pixels. Thus, the *joint distribution* is given by:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 每张图像是一个 8x8 的离散像素值数组，范围从 0 到 16。由于像素是计数数据，我们使用 [泊松分布](https://en.wikipedia.org/wiki/Poisson_distribution)
    和伽马 *先验* 来对像素 ***x*** 进行建模，其中 ***Θ*** 是速率参数。速率 ***Θ*** 决定了像素的平均强度。因此，*联合分布* 为：
- en: '*p*(***x***,***Θ***) ***=*** Poisson(***x***|***Θ***)Gamma(***Θ***|***a***,
    ***b***),'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*(***x***,***Θ***) ***=*** 泊松(***x***|***Θ***)伽马(***Θ***|***a***, ***b***),'
- en: where ***a*** and ***b*** are the shape and rate of the [gamma distribution](https://en.wikipedia.org/wiki/Gamma_distribution).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ***a*** 和 ***b*** 是 [伽马分布](https://en.wikipedia.org/wiki/Gamma_distribution)
    的形状参数和速率参数。
- en: '![](../Images/c38cb101c63a67e873e7972df308409a.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c38cb101c63a67e873e7972df308409a.png)'
- en: 'Fig. 4: Domain knowledge of the digit “zero” is used as prior. Image by Author.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 使用数字“零”的领域知识作为先验。图像由作者提供。'
- en: The prior — in this case, Gamma(***Θ***|***a***, ***b***) — is the place where
    you infuse your domain knowledge (usecase 1.). For example, you may have some
    idea what the “average” digit zero looks like (Fig. 4). You can use this *a priori*
    information to guide your choice of ***a*** and ***b***. To use Fig. 4 as prior
    information — let’s call it ***x***₀ — and weigh its importance as two examples,
    then set ***a*** = 2***x***₀; **b** = 2.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 先验——在这种情况下是 Gamma(***Θ***|***a***, ***b***)——是你注入领域知识的地方（用例 1）。例如，你可能对“平均”的数字零是什么样子有一些想法（见图
    4）。你可以使用这些 *a priori* 信息来指导你选择 ***a*** 和 ***b***。为了使用图 4 作为先验信息——我们称之为 ***x***₀——并将其重要性作为两个例子来加权，然后设置
    ***a*** = 2***x***₀； **b** = 2。
- en: 'Written down in Python this looks like:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 用 Python 写出来的代码如下：
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that we’ve used the [JAX](https://jax.readthedocs.io/) implementation of
    numpy and scipy, so that we can take derivatives.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用了 [JAX](https://jax.readthedocs.io/) 实现的 numpy 和 scipy，以便我们可以进行求导。
- en: 'Next, we need to choose a surrogate distribution *q*(***Θ|Φ***). To remind
    you, our goal is to change ***Φ*** so that the surrogate distribution *q*(***Θ|Φ***)
    matches *p*(***Θ|X)***. So, the choice of *q*(***Θ***) determines the level of
    approximation (we suppress the dependence on ***Φ*** where context permits). For
    illustration purposes, lets choose a variational distribution that is composed
    of (a product of) gamma’s:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要选择一个替代分布 *q*(***Θ|Φ***)。提醒一下，我们的目标是改变 ***Φ*** 使得替代分布 *q*(***Θ|Φ***)
    匹配 *p*(***Θ|X***)。因此，*q*(***Θ***) 的选择决定了近似的水平（我们在上下文允许的地方省略对 ***Φ*** 的依赖）。为了说明问题，我们选择一个由
    gamma 分布组成的变分分布：
- en: '*q*(***Θ|Φ***) = Gamma(***Θ***|***α***,***β***),'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*q*(***Θ|Φ***) = Gamma(***Θ***|***α***,***β***),'
- en: where we used the shorthand ***Φ*** = {***α***,***β***}.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了简写 ***Φ*** = {***α***,***β***}。
- en: 'Next, to implement the evidence lower bound *ℒ*(***Φ***) = E[ln *p*(***X***,***Θ***)
    — ln *q*(***Θ|Φ***)], first write down the term inside the expectation brackets:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，为了实现证据下界 *ℒ*(***Φ***) = E[ln *p*(***X***,***Θ***) — ln *q*(***Θ|Φ***)]，首先写下期望括号内的项：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, we used JAX’s [vmap](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html)
    to vectorise the function so that we can run it on a batch **[*Θ***₁, ***Θ***₂,..,***Θ***₁₂₈]ᵗ.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了 JAX 的 [vmap](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html)
    来矢量化函数，以便我们可以在批量 **[*Θ***₁, ***Θ***₂,..,***Θ***₁₂₈]ᵗ 上运行它。
- en: 'To complete the implementation of *ℒ*(***Φ***), we average the above function
    over realisations of the variational distribution ***Θ****ᵢ* ~ *q*(***Θ***):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成 *ℒ*(***Φ***) 的实现，我们对变分分布 ***Θ****ᵢ* ~ *q*(***Θ***) 的实现进行平均。
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'A few things to notice here about the arguments:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这些参数，有几个要点需要注意：
- en: We’ve packed ***Φ*** as a dictionary (or technically, a [pytree](https://jax.readthedocs.io/en/latest/pytrees.html))
    containing ln(***α***), and ln(***β***). This trick guarantees that ***α***>0
    and ***β***>0 — a requirement imposed by the gamma distribution — during optimisation.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将 ***Φ*** 打包为一个字典（或者技术上说是一个 [pytree](https://jax.readthedocs.io/en/latest/pytrees.html)），其中包含
    ln(***α***), 和 ln(***β***)。这个技巧确保了 ***α***>0 和 ***β***>0——这是 gamma 分布施加的一个要求——在优化过程中。
- en: The *loss* is a random estimate of the ELBO. In JAX, we need a new pseudo random
    number generator (PRNG) *key* every time we sample. In this case, we use *key*
    to sample **[*Θ***₁, ***Θ***₂,..,***Θ***₁₂₈]ᵗ.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*loss* 是 ELBO 的随机估计。在 JAX 中，我们每次采样时都需要一个新的伪随机数生成器（PRNG）*key*。在这种情况下，我们使用 *key*
    采样 **[*Θ***₁, ***Θ***₂,..,***Θ***₁₂₈]ᵗ。'
- en: This completes the specification of the model *p*(***x***,***Θ)***, the variational
    distribution *q*(***Θ***), and the loss *ℒ*(***Φ***).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了模型 *p*(***x***,***Θ***)、变分分布 *q*(***Θ***) 和损失 *ℒ*(***Φ***) 的规范说明。
- en: Model training
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型训练
- en: Next, we minimise the loss *ℒ*(***Φ***)by varying ***Φ*** = {***α***,***β***}so
    that *q*(***Θ|Φ***) matches the posterior *p*(***Θ***|***X***). How? Using old
    fashioned gradient descent! For convenience, we use the Adam optimiser from [Optax](https://github.com/deepmind/optax)
    and initialise the parameters with the prior ***α = a***, and ***β = b*** [remember,
    the prior wasGamma(***Θ***|***a***, ***b***) and codified our domain knowledge].
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过改变 ***Φ*** = {***α***,***β***} 来最小化损失 *ℒ*(***Φ***)，使 *q*(***Θ|Φ***)
    匹配后验 *p*(***Θ***|***X***)。怎么做？使用传统的梯度下降法！为了方便，我们使用了 [Optax](https://github.com/deepmind/optax)
    中的 Adam 优化器，并用先验 ***α = a*** 和 ***β = b*** 初始化参数 [记住，先验是 Gamma(***Θ***|***a***,
    ***b***) 并且编码了我们的领域知识]。
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, we use [*value_and_grad*](https://jax.readthedocs.io/en/latest/_autosummary/jax.value_and_grad.html)
    to simultaneously evaluate the ELBO and its derivative. Convenient for monitoring
    convergence! We then just-in-time compile the resulting function(with [*jit*](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html)*)*
    to make it snappy.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we’Il train the model for 5000 steps. Since *loss* is random, for each
    evaluation we need to supply it a pseudo random number generator (PRNG) key. We
    do this by allocating 5000 keys with [random.split](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.split.html).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Congrats! You’ve succesfully trained your first model using variational inference!
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: You can access the notebook with the full code [here on Colab](https://colab.research.google.com/drive/1NPZfRkhrFfX8flbUJqfgKZ2z1w5JXs1R?usp=sharing).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Results
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/f80c3477d3a3f24c3bdc9ba9a16097df.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 5: Comparison of variational distribution with exact posterior distribution.
    Image by Author.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a step back and appreciate what we’ve built (Fig. 5). For each pixel,
    the surrogate *q*(***Θ***) describes the uncertainty about the average pixel intensity
    (usecase 2.). In particular, our choice of *q*(***Θ***) captures two complementary
    elements:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: The typical pixel intensity.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much the intensity varies from image to image (the variability).
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It turns out that the joint distribution *p*(***x***,***Θ***) we chose has
    an exact solution:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '*p*(***Θ|X) =*** Gamma(***Θ***|***a +*** Σ***x***ᵢ, *m* + ***b***),'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: where *m* are the number of samples in the training set ***X***. Here, we see
    explicitly how the domain knowledge—codified in **a** and **b** — is dialed down
    as we gather more examples ***x***ᵢ.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: We can easily compare the learned shape ***α*** and rate ***β*** with the true
    values ***a +*** Σ***x***ᵢ and *m* + ***b***. In Fig. 5 we compare the distributions
    — *q*(***Θ|Φ***) versus *p*(***Θ|X) —*** for two specific pixels. Lo and behold,
    a perfect match!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 'Bonus: generating synthetic images'
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/92417d8e5b86b151147a2268acb42b47.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 6: Synthetically generated images using variational inference. Image by
    Author.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'Variational inference is great for generative modelling (usecase 3.). With
    the stand-in posterior *q*(***Θ***) in hand, generating new synthetic images is
    trivial. The two steps are:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Sample pixel intensities ***Θ ~*** *q*(***Θ***).
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Sample images using ***x*** ~ Poisson(***x***|***Θ***).
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You can see the result in Fig. 6\. Notice that the “zero” character is slightly
    less sharp than expected. This was part of our modelling assumptions: we modelled
    the pixels as mutually independent rather than correlated. To account for pixel
    correlations, you can expand the model to cluster pixel intensities: this is called
    Poisson factorisation [4].'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this tutorial, we introduced the basics of variational inference and applied
    it to a toy example: learning a handwritten digit zero. Thanks to autograd, implementing
    variational inference from scratch takes only a few lines of Python.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Variational inference is particularly powerful if you have little data. We saw
    how to infuse and trade-of domain knowledge with information from the data. The
    inferred surrogate distribution *q*(***Θ***) gives a “fuzzy” representation of
    the model parameters, instead of a fixed value. This is ideal if you are in a
    high-stakes application where uncertainty is important! Finally, we demonstrated
    generative modelling. Generating synthetic samples is easy once you can sample
    from *q*(***Θ***).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: In summary, by harnessing the power of variational inference, we can tackle
    complex problems, enabling us to make informed decisions, quantify uncertainties,
    and ultimately unlock the true potential of data science.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I would like to thank Dorien Neijzen and [Martin Banchero](https://medium.com/u/3c052f2ab36b?source=post_page-----f70ac511bcea--------------------------------)
    for proofreading.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'References:'
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Blei, David M., Alp Kucukelbir, and Jon D. McAuliffe. “[Variational inference:
    A review for statisticians.](https://doi.org/10.1080/01621459.2017.1285773)” *Journal
    of the American statistical Association* 112.518 (2017): 859–877.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Figurnov, Mikhail, Shakir Mohamed, and Andriy Mnih. “[Implicit reparameterization
    gradients.](https://proceedings.neurips.cc/paper/2018/file/92c8c96e4c37100777c7190b76d28233-Paper.pdf)”
    Advances in neural information processing systems 31 (2018).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Ranganath, Rajesh, Sean Gerrish, and David Blei. “[Black box variational
    inference](http://proceedings.mlr.press/v33/ranganath14.pdf).” *Artificial intelligence
    and statistics*. PMLR, 2014.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Gopalan, Prem, Jake M. Hofman, and David M. Blei. “[Scalable recommendation
    with poisson factorization.](https://arxiv.org/abs/1311.1704)” *arXiv preprint
    arXiv:1311.1704* (2013).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
