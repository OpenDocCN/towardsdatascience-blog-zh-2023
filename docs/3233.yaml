- en: Constrained Optimization and the KKT Conditions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/constrained-optimization-and-the-kkt-conditions-a3541d57a994?source=collection_archive---------2-----------------------#2023-10-28](https://towardsdatascience.com/constrained-optimization-and-the-kkt-conditions-a3541d57a994?source=collection_archive---------2-----------------------#2023-10-28)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: an insight into the Lagrangian function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://paribeshregmi.medium.com/?source=post_page-----a3541d57a994--------------------------------)[![Paribesh
    Regmi](../Images/7580cbd23a1269a1540d53acc54f5ebc.png)](https://paribeshregmi.medium.com/?source=post_page-----a3541d57a994--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a3541d57a994--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a3541d57a994--------------------------------)
    [Paribesh Regmi](https://paribeshregmi.medium.com/?source=post_page-----a3541d57a994--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe15368282264&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstrained-optimization-and-the-kkt-conditions-a3541d57a994&user=Paribesh+Regmi&userId=e15368282264&source=post_page-e15368282264----a3541d57a994---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a3541d57a994--------------------------------)
    ¬∑8 min read¬∑Oct 28, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa3541d57a994&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstrained-optimization-and-the-kkt-conditions-a3541d57a994&user=Paribesh+Regmi&userId=e15368282264&source=-----a3541d57a994---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa3541d57a994&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconstrained-optimization-and-the-kkt-conditions-a3541d57a994&source=-----a3541d57a994---------------------bookmark_footer-----------)![](../Images/a66337b86cce4b5a7cb921d37dff75a2.png)'
  prefs: []
  type: TYPE_NORMAL
- en: (Image by author, using math3d.org)
  prefs: []
  type: TYPE_NORMAL
- en: Optimization is ubiquitous in the realms of computer science, physics, mathematics,
    and economics. It stands as an essential tool for AI and machine learning (ML)
    professionals, applicable in diverse domains including decision-making, route
    planning, and learning parameters in ML models, such as Support Vector Machines
    (SVM) and neural networks. The most general form of optimization is finding a
    minimum/maximum of a function with respect to its independent variables, which
    can be achieved by applying basic concepts of differential calculus. Mathematically,
    at these extremities, the slope (first derivative) of a function is zero, referred
    to as **stationary points**. Determining whether such a point represents a maxima
    or a minima is done by evaluating the curvature (second derivative).
  prefs: []
  type: TYPE_NORMAL
- en: Taking this a step further, we can add constraints to the optimization problem
    that define a specific domain in space where the function is to be optimized.
    Consequently, instead of determining the maximum and minimum of a function in
    all of real (or complex) space, the optimization is now confined to this specific
    domain. The conventional approach of calculating stationary points is no longer
    a solution, as these points may fall outside the boundary set by the constraints.
    In the coming sections, we will analyze the intricacies of constrained optimization
    problems and explore strategies for their resolution.
  prefs: []
  type: TYPE_NORMAL
- en: Equality Constraints
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Optimization problems with equality constraints are of the form
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be3d18155635f94c7943f64fc8f75302.png)![](../Images/39cd92a7560c84ee0f809668b10ffb8d.png)'
  prefs: []
  type: TYPE_IMG
- en: (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: where ***f(x)*** is the function that we seek to minimize, and the constraint
    ***g(x) = 0***defines the domain within which the minimization is to be carried
    out. In these instances, the focus of minimization is inherently confined to the
    specific domain defined by the constraint. Nonetheless, as previously noted, the
    conventional application of differential calculus to determine stationary points
    does not account for the constraint, necessitating an alternative approach.
  prefs: []
  type: TYPE_NORMAL
- en: Lagrangian function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Given that this is a minimization problem, one approach to adapt to the conventional
    method is to assign a value of infinity to the function outside the specified
    domain. To achieve this, we introduce a new function ***f‚Äô(x)*** characterized
    by the following expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b22426dece319578bcb88fab60ecbeba.png)'
  prefs: []
  type: TYPE_IMG
- en: Such modification eliminates the possibility of minima occurring outside the
    domain, thereby ensuring that the optimal point occurs within it. Consequently,
    we can now reformulate the constrained optimization into an unconstrained optimization
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c9b8158a8acdfb48e099bda35fdf1787.png)'
  prefs: []
  type: TYPE_IMG
- en: However, there is a challenge that comes with this approach. Using differential
    calculus to optimize the above problem is not possible, since the function ***f‚Äô(x)***
    is not differentiable due to a sudden discontinuity at the at the boundary of
    the domain. Here is where Lagrangian comes into play. Rather than defining the
    function ***f‚Äô(x)*** as in (2), we formulate it as a maximization problem.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/019d487cce97c2bec7684cf3531e86e4.png)'
  prefs: []
  type: TYPE_IMG
- en: The expression on the RHS is called the **Lagrangian function** and the new
    variable ùû¥ is the **Lagrange multiplier**. It is evident from (4) that that at
    regions where **{*g(x)<0, g(x)>0*}**, ùû¥ can take the values **{-‚àû, ‚àû}** to maximize
    the expression to **‚àû**.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, the optimization in (3) takes the following form.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6203dcec8513faf805b7d41fcfe94d9a.png)'
  prefs: []
  type: TYPE_IMG
- en: It‚Äôs worth noting that the problem of non-differentiability still exists as
    the inner maximization results in the same discontinuous function. However, with
    the Lagrangian representation, we can use the max-min inequality to convert the
    max-min problem to the min-max problem to get over this issue.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eccf3a2e7de206ad9e303b9de5f657a1.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we first optimize with respect to the independent variable **x** and then
    with respect to the Lagrange multiplier ùû¥.
  prefs: []
  type: TYPE_NORMAL
- en: Inequality Constraints
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/7796e3a469b422e11f83a9349e47363e.png)'
  prefs: []
  type: TYPE_IMG
- en: (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'We‚Äôll now analyze the scenarios when the constraint is not an equation but
    an inequality. Such optimizations are of the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4831bd2ddfa59da35234f95667b35a15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can solve this using a similar approach: we define ***f‚Äô(x)*** to be the
    same as ***f(x)*** within the domain defined by the constraints and infinite elsewhere:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/46ce5189dfaf941bedee284bcb3e0d65.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Correspondingly, the Lagrangian function is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/38a60c9d79adefa382829f487f307979.png)'
  prefs: []
  type: TYPE_IMG
- en: The Lagrange multipliers corresponding to inequality constraints are denoted
    by *ùùª*. Equation (9) is different in that it also has constraints on the Lagrange
    multipliers, which was not in (4). Now the optimization problem in (7) takes the
    form
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/109ad31ca255673bd62ff14059b457a8.png)'
  prefs: []
  type: TYPE_IMG
- en: Applying min-max inequality,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e71552a27a584fc1b3a01c304bd84e0e.png)'
  prefs: []
  type: TYPE_IMG
- en: Interpretation of the Lagrange multiplier ***ùùª***
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When calculating the stationary point of (11) with respect to ***x***, we get
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aec23980b036a3e6d3d1b36ed02e2c20.png)'
  prefs: []
  type: TYPE_IMG
- en: The Lagrange multiplier *ùùª* is the ratio of the slope of ***f(x)*** to the constraint
    ***g(x).*** This essentially represents the sensitivity of the optimal value of
    ***f(x)*** concerning the constraint ***g(x).*** In other words, the value of
    the Lagrange multiplier quantifies the impact of the constraint on the optimality
    of ***f(x);*** a value of *ùùª = 0* would imply that the constraint has no influence
    on the optimality***.*** Further elaboration on this concept is presented in the
    subsequent discussion on KKT conditions.
  prefs: []
  type: TYPE_NORMAL
- en: KKT (Karush-Kuhn-Tucker) conditions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The optimization in (10) is called the primal version and (11) is its dual
    version. According to min-max inequality, the dual version lower bounds the primal
    version, suggesting that the two versions are not necessarily equal. However,
    there are conditions where the primal and dual versions are equal, which is called
    the **regularity condition**. Assuming regularity, for (***x*,*** *ùùª********)to
    be the solution point it has to satisfy the following **KKT conditions**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Primal Feasibility**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/caa5bd299b67243130ee2e8d5f138dcd.png)'
  prefs: []
  type: TYPE_IMG
- en: It follows from the problem definition.
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Dual Feasibility**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/55996cbdf99904b677f11cb6a3c105df.png)'
  prefs: []
  type: TYPE_IMG
- en: The dual feasibility follows from (9).
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. Stationarity**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8eeaafe5e1bda2d60579cbd57838f17f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is an interesting property. Since ùùª* is a zero or a positive, the stationarity
    condition implies that at the optimal point, the gradients of ***f(x)*** and ***g(x)***
    must be oriented in opposite directions. The rationale behind this is as follows:
    if the gradients of ***f(x)*** and***g(x)***were aligned in the same direction
    at the point ***x = x****, then both ***f(x)*** and ***g(x)*** would simultaneously
    decrease in a direction opposite to their gradients. This scenario would permit
    ***f(x)*** to continue decreasing beyond the value ***f(x*)*** without violating
    the constraint, in which case ***x**** no longer qualifies as the optimal point.
    Therefore for a point to be the optimum, the stationarity property must hold.'
  prefs: []
  type: TYPE_NORMAL
- en: '**4\. Complementary Slackness**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9be1045e3e15f10153bf7197e910e968.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is another interesting property that directly follows from equation (9).
    When the constraint ***g(x*) < 0***, the Lagrange multiplier ùùª* must equal to
    zero. Since the Lagrange multiplier also indicates how sensitive our solution
    is to the associated constraint, a value of ùùª* = 0 signifies that the associated
    constraint has no influence on determining the solution. In other words, whether
    we consider the solution with or without the constraint, the outcome remains unaltered.
    One straightforward example is when ***f(x)*** has a global minimum in the domain
    where ***g(x) ‚â§ 0***. For the other example, consider the minimization of the
    function ***f(x)*** subject to two constraints: ***g¬π(x) < 5*** and ***g¬≤(x) <
    -1***. In this case, the Lagrange multiplier ùùª¬π***** corresponding to the constraint
    ***g¬π*** is zero, as ***g¬≤*** already implies the conditions of ***g¬π***, rendering
    ***g¬π*** insignificant as a constraint.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Application: Support Vector Machine (SVM)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An example of constrained optimization with inequality constraints in machine
    learning is the Support Vector Machine (SVM). When given a dataset of data points
    **{(*x¬π, y¬π*), (*x¬≤, y¬≤*), ‚Ä¶}** with ***y ‚àà* {*-1, 1*}** representing the two
    classes, the objective is to identify a classifier that maximizes the margin between
    the classes. Specifically, we formulate SVM as the following minimization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/41e51eabb50c4437e7a21eeeb38d93a7.png)![](../Images/97d0afa5a091924522bfb33811228aa4.png)'
  prefs: []
  type: TYPE_IMG
- en: (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'The term ***||w||*** in the equation represents the inverse of the margin.
    It‚Äôs evident that there are numerous inequality constraints: in fact, we have
    a constraint tied to each data point. However, in practice, the solution is only
    guided by a few data points that lie in proximity to the classifier boundary;
    these are referred to as **support vectors**. As we discussed in complementary
    slackness, only the Lagrange multipliers corresponding to the constraints linked
    to the support vectors possess non-zero values. For all other data points, their
    associated constraints bear Lagrange multiplier values of zero, rendering them
    insignificant in determining the classifier boundary.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To summarize things in the article, we started with a brief introduction to
    the unconstrained optimization problem and gradually expanded it to incorporate
    the equality and inequality constraints. Moreover, we discussed how the Lagrangian
    function solves the challenges introduced by the constraints. Delving into the
    optimality of the Lagrangian, we gained insights into the KKT conditions. Lastly,
    we provided a succinct overview of how SVM is formulated as a constrained optimization
    problem and briefly discussed its solution.
  prefs: []
  type: TYPE_NORMAL
