- en: 'Transformer Models 101: Getting Started — Part 2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/transformer-models-101-getting-started-part-2-4c9a45bf8b81?source=collection_archive---------13-----------------------#2023-06-06](https://towardsdatascience.com/transformer-models-101-getting-started-part-2-4c9a45bf8b81?source=collection_archive---------13-----------------------#2023-06-06)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The complex math behind transformer models, in simple words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://nandinibansal1811.medium.com/?source=post_page-----4c9a45bf8b81--------------------------------)[![Nandini
    Bansal](../Images/a813ac8be6f89b2e856651fda66ab078.png)](https://nandinibansal1811.medium.com/?source=post_page-----4c9a45bf8b81--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4c9a45bf8b81--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4c9a45bf8b81--------------------------------)
    [Nandini Bansal](https://nandinibansal1811.medium.com/?source=post_page-----4c9a45bf8b81--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc41c26af0465&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-models-101-getting-started-part-2-4c9a45bf8b81&user=Nandini+Bansal&userId=c41c26af0465&source=post_page-c41c26af0465----4c9a45bf8b81---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4c9a45bf8b81--------------------------------)
    ·7 min read·Jun 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4c9a45bf8b81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-models-101-getting-started-part-2-4c9a45bf8b81&user=Nandini+Bansal&userId=c41c26af0465&source=-----4c9a45bf8b81---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4c9a45bf8b81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-models-101-getting-started-part-2-4c9a45bf8b81&source=-----4c9a45bf8b81---------------------bookmark_footer-----------)![](../Images/b598c04052b566c5fdd8e24dde8376ae.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image by [Dariusz Sankowski](https://pixabay.com/users/dariuszsankowski-1441456/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1052010)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1052010)
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous article, we looked at how the Encoder block of the Transformer
    model works in detail. If you haven’t read that article, I would recommend you
    to read it before starting with this one as the concepts covered there are carried
    forward in this article. You may head to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/transformer-models-101-getting-started-part-1-b3a77ccfa14d?source=post_page-----4c9a45bf8b81--------------------------------)
    [## Transformer Models 101: Getting Started — Part 1'
  prefs: []
  type: TYPE_NORMAL
- en: Complex maths behind transformer models in simple words...
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/transformer-models-101-getting-started-part-1-b3a77ccfa14d?source=post_page-----4c9a45bf8b81--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: If you have already read it, awesome! Let’s get started with a deep dive into
    the Decoder block and the complex maths associated with it.
  prefs: []
  type: TYPE_NORMAL
- en: Decoder of Transformer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Like the Encoder block of the Transformer models, the Decoder block consists
    of N stacked decoders that function sequentially and accept the input from the
    previous decoder. However, that is not the only input accepted by the decoder.
    The sentence representation generated by the Encoder block is fed to every decoder
    in the Decoder block. Therefore, we can conclude that each decoder accepts two
    different inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: Sentence representation from the Encoder Block
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output of the previous Decoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/1144b091a8021ae2198f3598dc9f9afe.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 1\. Encoder & Decoder blocks functioning together (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Before we delve any deeper into the different components that make up a Decoder,
    it is essential to have an intuition of how the decoder in general generates the
    output sentence or target sentence.
  prefs: []
  type: TYPE_NORMAL
- en: How is the target sentence generated?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At timestep t=1, only <sos> token or the ***start of the sentence*** is passed
    as input to the decoder block. Based on the <sos>, the first word of the target
    sentence is generated by the decoder block.
  prefs: []
  type: TYPE_NORMAL
- en: In the next timestamp i.e. t=2, the input to the decoder block includes the
    <sos> token as well as the first word generated by the decoder block. The next
    word is generated based on this input.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, with every timestamp increment, the length of the input to the decoder
    block increases as the word generated in the previous timestamp is added to the
    current input sentence.
  prefs: []
  type: TYPE_NORMAL
- en: When the decoder block completes the generation of the entire target sentence,
    <eos> or the ***end of the sentence*** token is generated.
  prefs: []
  type: TYPE_NORMAL
- en: You can think of it as a recursive process!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c4f83c9c11b7f68dfa0f7df38ef85dfc.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 2 Recursive generation of output tokens using Decoder (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Now, this is what is supposed to happen when input is given to the transformer
    model and we are expecting an output. But at the time of training/finetuning the
    transformer model, we already have the target sentence in the training dataset.
    So how does it work?
  prefs: []
  type: TYPE_NORMAL
- en: 'It brings us to an extremely important concept of Decoders: ***Masked Multi-head
    Attention***. Sounds familiar? Of course, it does. In the previous part, we understood
    the concept of ***Multi-head attention*** which is used in the Encoder block.
    Let us now understand how these two are different.'
  prefs: []
  type: TYPE_NORMAL
- en: Masked Multi-head Attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The decoder block generates the target sentence word by word and hence, the
    model has to be trained similarly so that it can make accurate predictions even
    with a limited set of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, as the name suggests, we mask all the tokens to the right of the sentence
    which have not been predicted yet before calculating the self-attention matrix.
    This will ensure that the self-attention mechanism only considers the tokens that
    will be available to the model at each recursive step of prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us take a simple example to understand it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f70b358e28c8796b99a4b8f0b931c107.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 3\. Masked Multi-head attention matrix representation (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: The steps and formula to calculate the self-attention matrix will be the same
    as we do in the Encoder block. We will cover the steps on a high level in this
    article. For a deeper understanding, please feel free to head to the previous
    part of this article series.
  prefs: []
  type: TYPE_NORMAL
- en: '*Generate embeddings for the target sentence and obtain* ***target matrix Y***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Transform the target sentence into* ***Q, K & V*** *by multiplying random
    weight matrices* ***Wq, Wk & Wv with target matrix Y***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Calculate the dot product of* ***Q and K-transpose***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Scale the dot product by dividing it by the* ***square root*** *of the embedding
    dimension (****dk****)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Apply masking on the scaled matrix by replacing all the cells with* ***<mask>
    with — inf***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Now* ***apply the softmax function*** *on the matrix and multiply it with
    the* ***Vi matrix*** *to generate the attention matrix Zi*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Concatenate multiple attention matrices Zi into a single* ***attention matrix
    M***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This attention matrix will be fed to the next component of the Decoder block
    along with the input sentence representation generated by the Encoder block. Let
    us now understand how both of these matrices are consumed by the Decoder block.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-head Attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This sublayer of the Decoder block is also known as the ***“Encoder-Decoder
    Attention Layer”*** as it accepts both **masked attention matrix (M)** and **sentence
    representation by Encoder (R)**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/42c44b268b7f30a8506a35f82570c2be.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 4\. Multihead Attention Mechanism (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: 'The calculation of the self-attention matrix is very similar to how it is done
    in the previous step with a small twist. Since we have two input matrices for
    this layer, they are transformed into Q, K & V as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Q is generated using Wq and M
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K & V matrices are generated using Wk & Wv with R
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By now you must’ve understood that every step and calculation that goes behind
    the Transformer model has a very specific reason. Similarly, there is also a reason
    why each of these matrices is generated using a different input matrix. Can you
    guess?
  prefs: []
  type: TYPE_NORMAL
- en: '***Quick Hint: The answer lies in how the self-attention matrix is calculated...***'
  prefs: []
  type: TYPE_NORMAL
- en: Yes, you got it right!
  prefs: []
  type: TYPE_NORMAL
- en: If you recall, when we understood the concept of self-attention using an input
    sentence, we talked about how it calculates attention scores while mapping the
    source sentence to itself. Every word in the source sentence is compared against
    every other word in the same sentence to quantify the relationships and understand
    the context.
  prefs: []
  type: TYPE_NORMAL
- en: Here also we are doing the same thing, the only difference being, we are comparing
    each word of the input sentence (K-transpose) to the target sentence words (Q).
    It will help us quantify how similar both of these sentences are to each other
    and understand the relationships between the words.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a2c20a503086986041db5f326a124c28.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 5\. Attention Matrix Representation with input sentence & target sentence
    (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: In the end, the attention matrix Zi generated will be of dimension N X 1 where
    N = word count of the target sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Since this is also a multi-head attention layer, to generate the final attention
    matrix, multiple attention matrices are concatenated.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this, we have covered all the unique components of the Decoder block.
    However, some other components function the same as in Encoder Block. Let us also
    look at them briefly:'
  prefs: []
  type: TYPE_NORMAL
- en: '***Positional Encoding*** — Just like the encoder block, to preserve the word
    order of the target sentence, we add positional encoding to the target embedding
    before feeding it to the Masked Multi-attention layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Feedforward Network*** — This sublayer in the decoder block is the classic
    neural network with two dense layers and ReLU activations. It accepts input from
    the multi-head attention layer, performs some non-linear transformations on the
    same and finally generates contextualised vectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Add & Norm Component*** — This is a *residual layer* followed by *layer
    normalisation.* It helps faster model training while ensuring no information from
    sub-layers is lost.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have covered these concepts in detail in [Part 1](/transformer-models-101-getting-started-part-1-b3a77ccfa14d).
  prefs: []
  type: TYPE_NORMAL
- en: With this, we have wrapped up the internal working of the Decoder block as well.
    As you might have guessed, both Encoder & Decoder blocks are used to process and
    generate contextualized vectors for the input sentence. So who does the actual
    next-word prediction task? Let’s find out.
  prefs: []
  type: TYPE_NORMAL
- en: Linear & Softmax Layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sitting on top of the Decoder network, it accepts the output matrix generated
    by the last decoder in the stack as input. This output matrix is transformed into
    a logit vector of the same size as the vocabulary size. We then apply the softmax
    function on this logit vector to generate probabilities corresponding to each
    word. The word with the highest probability is predicted as the next word. The
    model is optimized for cross-entropy loss using **Adam Optimizer**.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid overfitting, **dropout layers** are added after every sub-layer of
    the encoder/decoder network.
  prefs: []
  type: TYPE_NORMAL
- en: That’s all about the entire Transformer Model. With this, we have completed
    the in-depth walk-through of Transformer Model Architecture in the simplest language
    possible.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you know all about Transformer models, it shouldn’t be difficult for
    you to build your knowledge on top of this and delve into more complex LLM model
    architectures such as BERT, GPT, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may refer to the below resources for the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://huggingface.co/blog/bert-101](https://huggingface.co/blog/bert-101)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://jalammar.github.io/illustrated-gpt2/](https://jalammar.github.io/illustrated-gpt2/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I hope this 2-part article would make the Transformer Models a little less intimidating
    to understand. If you found it useful, please spread the good word.
  prefs: []
  type: TYPE_NORMAL
- en: Until next time!
  prefs: []
  type: TYPE_NORMAL
