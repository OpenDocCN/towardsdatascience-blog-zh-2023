["```py\n!pip install s3fs\n```", "```py\nimport requests\nimport json\nimport os\nimport s3fs\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark import SparkContext\nimport pyspark.sql.functions as F\n```", "```py\n# Define environment variables\nos.environ[\"MINIO_KEY\"] = \"minio\"\nos.environ[\"MINIO_SECRET\"] = \"minio123\"\nos.environ[\"MINIO_ENDPOINT\"] = \"http://minio1:9000\"\n```", "```py\n# Get data using REST API\ndef fetch_countries_data(url):\n    # Using session is particularly beneficial \n    # if you are making multiple requests to the same server, \n    # as it can reuse the underlying TCP connection, \n    # leading to performance improvements.\n    with requests.Session() as session:\n        response = session.get(url)\n        response.raise_for_status()\n\n        if response.status_code == 200:\n            return response.json()\n        else:\n            return f\"Error: {response.status_code}\"\n\n# Fetch data\ncountries_data = fetch_countries_data(\"https://restcountries.com/v3.1/all\")\n```", "```py\n# Write data to minIO as a JSON file\n\nfs = s3fs.S3FileSystem(\n    client_kwargs={'endpoint_url': os.environ[\"MINIO_ENDPOINT\"]}, # minio1 = minio container name\n    key=os.environ[\"MINIO_KEY\"],\n    secret=os.environ[\"MINIO_SECRET\"],\n    use_ssl=False  # Set to True if MinIO is set up with SSL\n)\n\nwith fs.open('mybucket/country_data.json', 'w', encoding='utf-8') as f:\n    json.dump(countries_data,f)\n```", "```py\nspark = SparkSession.builder \\\n    .appName(\"country_data_analysis\") \\\n    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.11.1026\") \\\n    .config(\"spark.hadoop.fs.s3a.endpoint\", os.environ[\"MINIO_ENDPOINT\"]) \\\n    .config(\"spark.hadoop.fs.s3a.access.key\", os.environ[\"MINIO_KEY\"]) \\\n    .config(\"spark.hadoop.fs.s3a.secret.key\", os.environ[\"MINIO_SECRET\"]) \\\n    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n    .enableHiveSupport() \\\n    .getOrCreate()\n```", "```py\n# Check PySpark version\nprint(pyspark.__version__)\n\n# Check Hadoop version\nsc = SparkContext.getOrCreate()\nhadoop_version = sc._gateway.jvm.org.apache.hadoop.util.VersionInfo.getVersion()\nprint(\"Hadoop version:\", hadoop_version)\n```", "```py\n# Read JSON data using PySpark\ndf = spark.read.option(\"inferSchema\",True).json(\"s3a://mybucket/country_data.json\")\n# Returns count of rows in the dataframe\ndf.count() \n```", "```py\n# Write same data as Parquet and re-read in dataframe\ndf.write.mode(\"overwrite\").format(\"parquet\").save(\"s3a://mybucket/country_raw_data.parquet\")\ncountry_raw_data = spark.read.parquet(\"s3a://mybucket/country_raw_data.parquet\")\ncountry_raw_data.count()\n```", "```py\n# Perform transformations to raw data\ncountry_trnsfm_data = (\n    country_raw_data\n    .selectExpr(\n        \"name.common as cntry_name\",\n        \"area as cntry_area\",\n        \"borders as border_cntry\",\n        \"capital as capital_cities\",\n        \"continents as cntry_continent\",\n        \"landlocked as is_landlocked\",\n        \"population\",\n        \"startOfWeek\",\n        \"timezones as nr_timezones\",\n        \"unMember as is_unmember\"\n    )\n    .withColumn(\"cntry_area\",F.when(F.col(\"cntry_area\") < 0, None).otherwise(F.col(\"cntry_area\")))\n    .withColumn(\"border_cntry\",F.when(F.col(\"border_cntry\").isNull(),F.array(F.lit(\"NA\"))).otherwise(F.col(\"border_cntry\")))\n    .withColumn(\"capital_cities\",F.when(F.col(\"capital_cities\").isNull(),F.array(F.lit(\"NA\"))).otherwise(F.col(\"capital_cities\")))    \n)\n\n# Print schema of transformed data\ncountry_trnsfm_data.printSchema()\n```", "```py\nroot\n |-- cntry_name: string (nullable = true)\n |-- cntry_area: double (nullable = true)\n |-- border_cntry: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- capital_cities: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- cntry_continent: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- is_landlocked: boolean (nullable = true)\n |-- population: long (nullable = true)\n |-- startOfWeek: string (nullable = true)\n |-- nr_timezones: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- is_unmember: boolean (nullable = true)\n```", "```py\n# Write transformed data as PARQUET\ncountry_trnsfm_data.write.mode(\"overwrite\").format(\"parquet\").save(\"s3a://mybucket/country_trnsfm_data.parquet\")\n```", "```py\n# Create external hive table using PARQUET\nspark.sql(\"\"\"\nCREATE EXTERNAL TABLE country_data (\n    cntry_name STRING,\n    cntry_area DOUBLE,\n    border_cntry ARRAY<STRING>,\n    capital_cities ARRAY<STRING>,\n    cntry_continent ARRAY<STRING>,\n    is_landlocked BOOLEAN,\n    population BIGINT,\n    startOfWeek STRING,\n    nr_timezones ARRAY<STRING>,\n    is_unmember BOOLEAN\n)\nSTORED AS PARQUET\nLOCATION 's3a://mybucket/country_trnsfm_data.parquet';\n\"\"\").show()\n```", "```py\n# Show table details\nspark.sql(\"DESCRIBE EXTENDED default.country_data\").show(100,truncate = False)\n```", "```py\n# Show first 5 records from the table\nspark.sql(\"SELECT * FROM default.country_data LIMIT 5\").show(truncate = False)\n```", "```py\n# Create temporary view using dataframe\nspark.table(\"default.country_data\").createOrReplaceTempView(\"country_data_processed_view\")\n```", "```py\n# Function to show Spark SQL results\ndef show_results(sql_string):\n    return spark.sql(\n        sql_string\n    ).show(truncate = False)\n```", "```py\n# 1\\. Which are the 10 largest countries in terms of area? (in sq. km.)\nsql_string = \"\"\"\n    SELECT cntry_name, cntry_area\n    FROM country_data_processed_view\n    ORDER BY cntry_area DESC\n    LIMIT 10\n    \"\"\"\nshow_results(sql_string)\n\n+-------------+-----------+\n|cntry_name   |cntry_area |\n+-------------+-----------+\n|Russia       |1.7098242E7|\n|Antarctica   |1.4E7      |\n|Canada       |9984670.0  |\n|China        |9706961.0  |\n|United States|9372610.0  |\n|Brazil       |8515767.0  |\n|Australia    |7692024.0  |\n|India        |3287590.0  |\n|Argentina    |2780400.0  |\n|Kazakhstan   |2724900.0  |\n+-------------+-----------+\n```", "```py\n# 2\\. Which country has the largest number of neighbouring countries?\nsql_string = \"\"\"\n    SELECT cntry_name, border_cntry, array_size(border_cntry) as ngbr_cntry_nr\n    FROM country_data_processed_view\n    WHERE NOT array_contains(border_cntry,'NA')\n    ORDER BY array_size(border_cntry) DESC\n    LIMIT 1\n    \"\"\"\nshow_results(sql_string)\n\n+----------+--------------------------------------------------------------------------------+-------------+\n|cntry_name|border_cntry                                                                    |ngbr_cntry_nr|\n+----------+--------------------------------------------------------------------------------+-------------+\n|China     |[AFG, BTN, MMR, HKG, IND, KAZ, NPL, PRK, KGZ, LAO, MAC, MNG, PAK, RUS, TJK, VNM]|16           |\n+----------+--------------------------------------------------------------------------------+-------------+\n```", "```py\n# 3\\. Which countries have the highest number of capital cities?\nsql_string = \"\"\"\n    SELECT cntry_name, capital_cities, array_size(capital_cities) as total_capital_cities\n    FROM country_data_processed_view\n    WHERE NOT array_contains(capital_cities,'NA')\n    ORDER BY array_size(capital_cities) DESC\n    LIMIT 2\n    \"\"\"\nshow_results(sql_string)\n\n+------------+-----------------------------------+--------------------+\n|cntry_name  |capital_cities                     |total_capital_cities|\n+------------+-----------------------------------+--------------------+\n|South Africa|[Pretoria, Bloemfontein, Cape Town]|3                   |\n|Palestine   |[Ramallah, Jerusalem]              |2                   |\n+------------+-----------------------------------+--------------------+\n```", "```py\n# 4\\. How many countries lie on two or more continents?\nsql_string = \"\"\"\n    SELECT cntry_name, cntry_continent, array_size(cntry_continent) as total_continents\n    FROM country_data_processed_view\n    ORDER BY array_size(cntry_continent) DESC\n    LIMIT 3\n    \"\"\"\nshow_results(sql_string)\n\n+----------+---------------+----------------+\n|cntry_name|cntry_continent|total_continents|\n+----------+---------------+----------------+\n|Turkey    |[Europe, Asia] |2               |\n|Azerbaijan|[Europe, Asia] |2               |\n|Russia    |[Europe, Asia] |2               |\n+----------+---------------+----------------+\n```", "```py\n# 5\\. How many landlocked countries per continent?\nsql_string = \"\"\"\n    SELECT continent, SUM(is_landlocked) as landlocked_nr\n    FROM (SELECT cntry_name, case when is_landlocked then 1 else 0 end as is_landlocked, explode(cntry_continent) as continent\n    FROM country_data_processed_view)\n    GROUP BY continent\n    ORDER BY SUM(is_landlocked) DESC\n    \"\"\"\nshow_results(sql_string)\n\n+-------------+-------------+\n|continent    |landlocked_nr|\n+-------------+-------------+\n|Europe       |16           |\n|Africa       |16           |\n|Asia         |12           |\n|South America|2            |\n|North America|0            |\n|Antarctica   |0            |\n|Oceania      |0            |\n+-------------+-------------+\n```", "```py\n# 6\\. Which country has the highest number of time zones?\nsql_string = \"\"\"\n    SELECT cntry_name, nr_timezones, array_size(nr_timezones) as total_timezones\n    FROM country_data_processed_view\n    ORDER BY array_size(nr_timezones) DESC\n    LIMIT 1\n    \"\"\"\nshow_results(sql_string)\n\n+----------+----------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+\n|cntry_name|nr_timezones                                                                                                                                              |total_timezones|\n+----------+----------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+\n|France    |[UTC-10:00, UTC-09:30, UTC-09:00, UTC-08:00, UTC-04:00, UTC-03:00, UTC+01:00, UTC+02:00, UTC+03:00, UTC+04:00, UTC+05:00, UTC+10:00, UTC+11:00, UTC+12:00]|14             |\n+----------+----------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+\n```", "```py\n# 7\\. How many countries are not UN members?\nsql_string = \"\"\"\n    SELECT COUNT(*) AS count\n    FROM country_data_processed_view\n    WHERE NOT is_unmember\n    \"\"\"\nshow_results(sql_string)\n\n+-----+\n|count|\n+-----+\n|57   |\n+-----+\n```"]