["```py\nimport pandas as pd\n\n# Read the CSV file into a pandas DataFrame\ndata = pd.read_csv(\"./data/training_data.csv\")\n\n# Print head\ndata.head()\n```", "```py\n# Loads the Universal Sentence Encoder Multilingual module from TensorFlow Hub.\nbase_model_url = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\"\nbase_model = tf.keras.Sequential([\n    hub.KerasLayer(base_model_url,\n                   input_shape=[],\n                   dtype=tf.string,\n                   trainable=False)\n])\n\n# Defines a list of test sentences. These sentences represent various job titles.\ntest_text = ['Data Scientist', 'Data Analyst', 'Data Engineer',\n             'Nurse Practitioner', 'Registered Nurse', 'Medical Assistant',\n             'Social Media Manager', 'Marketing Strategist', 'Product Marketing Manager']\n\n# Creates embeddings for the sentences in the test_text list. \n# The np.array() function is used to convert the result into a numpy array.\n# The .tolist() function is used to convert the numpy array into a list, which might be easier to work with.\nvectors = np.array(base_model.predict(test_text)).tolist()\n\n# Calls the plot_similarity function to create a similarity plot.\nplot_similarity(test_text, vectors, 90, \"base model\")\n\n# Computes STS benchmark score for the base model\npearsonr = sts_benchmark(base_model)\nprint(\"STS Benachmark: \" + str(pearsonr))\n```", "```py\n# Load the pre-trained word embedding model\nembedding_layer = hub.load(base_model_url)\n\n# Create a Keras layer from the loaded embedding model\nshared_embedding_layer = hub.KerasLayer(embedding_layer, trainable=True)\n\n# Define the inputs to the model\nleft_input = keras.Input(shape=(), dtype=tf.string)\nright_input = keras.Input(shape=(), dtype=tf.string)\n\n# Pass the inputs through the shared embedding layer\nembedding_left_output = shared_embedding_layer(left_input)\nembedding_right_output = shared_embedding_layer(right_input)\n\n# Compute the cosine similarity between the embedding vectors\ncosine_similarity = tf.keras.layers.Dot(axes=-1, normalize=True)(\n    [embedding_left_output, embedding_right_output]\n)\n\n# Convert the cosine similarity to angular distance\npi = tf.constant(math.pi, dtype=tf.float32)\nclip_cosine_similarities = tf.clip_by_value(\n    cosine_similarity, -0.99999, 0.99999\n)\nacos_distance = 1.0 - (tf.acos(clip_cosine_similarities) / pi)\n\n# Package the model\nencoder = tf.keras.Model([left_input, right_input], acos_distance)\n\n# Compile the model\nencoder.compile(\n    optimizer=tf.keras.optimizers.Adam(\n        learning_rate=0.00001,\n        beta_1=0.9,\n        beta_2=0.9999,\n        epsilon=0.0000001,\n        amsgrad=False,\n        clipnorm=1.0,\n        name=\"Adam\",\n    ),\n    loss=tf.keras.losses.MeanSquaredError(\n        reduction=keras.losses.Reduction.AUTO, name=\"mean_squared_error\"\n    ),\n    metrics=[\n        tf.keras.metrics.MeanAbsoluteError(),\n        tf.keras.metrics.MeanAbsolutePercentageError(),\n    ],\n)\n\n# Print the model summary\nencoder.summary()\n```", "```py\n# Define early stopping callback\nearly_stop = keras.callbacks.EarlyStopping(\n    monitor=\"loss\", patience=3, min_delta=0.001\n)\n\n# Define TensorBoard callback\nlogdir = os.path.join(\".\", \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\ntensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n\n# Model Input\nleft_inputs, right_inputs, similarity = process_model_input(data)\n\n# Train the encoder model\nhistory = encoder.fit(\n    [left_inputs, right_inputs],\n    similarity,\n    batch_size=8,\n    epochs=20,\n    validation_split=0.2,\n    callbacks=[early_stop, tensorboard_callback],\n)\n\n# Define model input\ninputs = keras.Input(shape=[], dtype=tf.string)\n\n# Pass the input through the embedding layer\nembedding = hub.KerasLayer(embedding_layer)(inputs)\n\n# Create the tuned model\ntuned_model = keras.Model(inputs=inputs, outputs=embedding)\n```", "```py\n# Creates embeddings for the sentences in the test_text list. \n# The np.array() function is used to convert the result into a numpy array.\n# The .tolist() function is used to convert the numpy array into a list, which might be easier to work with.\nvectors = np.array(tuned_model.predict(test_text)).tolist()\n\n# Calls the plot_similarity function to create a similarity plot.\nplot_similarity(test_text, vectors, 90, \"tuned model\")\n\n# Computes STS benchmark score for the tuned model\npearsonr = sts_benchmark(tuned_model)\nprint(\"STS Benachmark: \" + str(pearsonr))\n```"]