- en: 'Class Imbalance: From SMOTE to BorderlineSMOTE1, SMOTE-NC and SMOTE-N'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 类别不平衡：从SMOTE到BorderlineSMOTE1、SMOTE-NC和SMOTE-N
- en: 原文：[https://towardsdatascience.com/class-imbalance-from-smote-to-smote-n-759d364d535b?source=collection_archive---------3-----------------------#2023-08-30](https://towardsdatascience.com/class-imbalance-from-smote-to-smote-n-759d364d535b?source=collection_archive---------3-----------------------#2023-08-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/class-imbalance-from-smote-to-smote-n-759d364d535b?source=collection_archive---------3-----------------------#2023-08-30](https://towardsdatascience.com/class-imbalance-from-smote-to-smote-n-759d364d535b?source=collection_archive---------3-----------------------#2023-08-30)
- en: Exploring four algorithms to tackle the class imbalance problem
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索四种算法以解决类别不平衡问题
- en: '[](https://essamwissam.medium.com/?source=post_page-----759d364d535b--------------------------------)[![Essam
    Wisam](../Images/6320ce88ba2e5d56d70ce3e0f97ceb1d.png)](https://essamwissam.medium.com/?source=post_page-----759d364d535b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----759d364d535b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----759d364d535b--------------------------------)
    [Essam Wisam](https://essamwissam.medium.com/?source=post_page-----759d364d535b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://essamwissam.medium.com/?source=post_page-----759d364d535b--------------------------------)[![Essam
    Wisam](../Images/6320ce88ba2e5d56d70ce3e0f97ceb1d.png)](https://essamwissam.medium.com/?source=post_page-----759d364d535b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----759d364d535b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----759d364d535b--------------------------------)
    [Essam Wisam](https://essamwissam.medium.com/?source=post_page-----759d364d535b--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fccb82b9f3b87&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclass-imbalance-from-smote-to-smote-n-759d364d535b&user=Essam+Wisam&userId=ccb82b9f3b87&source=post_page-ccb82b9f3b87----759d364d535b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----759d364d535b--------------------------------)
    ·12 min read·Aug 30, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F759d364d535b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclass-imbalance-from-smote-to-smote-n-759d364d535b&user=Essam+Wisam&userId=ccb82b9f3b87&source=-----759d364d535b---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fccb82b9f3b87&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclass-imbalance-from-smote-to-smote-n-759d364d535b&user=Essam+Wisam&userId=ccb82b9f3b87&source=post_page-ccb82b9f3b87----759d364d535b---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----759d364d535b--------------------------------)
    ·12分钟阅读·2023年8月30日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F759d364d535b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclass-imbalance-from-smote-to-smote-n-759d364d535b&user=Essam+Wisam&userId=ccb82b9f3b87&source=-----759d364d535b---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F759d364d535b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclass-imbalance-from-smote-to-smote-n-759d364d535b&source=-----759d364d535b---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F759d364d535b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclass-imbalance-from-smote-to-smote-n-759d364d535b&source=-----759d364d535b---------------------bookmark_footer-----------)'
- en: In the [previous story](https://medium.com/towards-data-science/class-imbalance-from-random-oversampling-to-rose-517e06d7a9b)
    we explained how the naive random oversampling, random oversampling examples (ROSE),
    random walk oversampling (RWO) algorithms work. More importantly, we also defined
    the class imbalance problem and derived solutions for it with intuition. I highly
    recommend checking this [story](https://medium.com/@essamwissam/class-imbalance-and-oversampling-a-formal-introduction-c77b918e586d)
    to ensure clear understanding of class imbalance.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在[前一篇文章](https://medium.com/towards-data-science/class-imbalance-from-random-oversampling-to-rose-517e06d7a9b)中，我们解释了朴素随机过采样、随机过采样示例（ROSE）、随机游走过采样（RWO）算法的工作原理。更重要的是，我们还定义了类别不平衡问题，并直观地推导了其解决方案。我强烈推荐查看这篇[文章](https://medium.com/@essamwissam/class-imbalance-and-oversampling-a-formal-introduction-c77b918e586d)，以确保对类别不平衡有清晰的理解。
- en: Table of Contents
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目录
- en: ∘ [Introduction](#7af2)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [简介](#7af2)
- en: ∘ [SMOTE (Synthetic Minority Oversampling Technique)](#6edb)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [SMOTE（合成少数类过采样技术）](#6edb)
- en: ∘ [BorderlineSMOTE1](#4148)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [BorderlineSMOTE1](#4148)
- en: ∘ [SMOTE-NC (SMOTE-Nominal Continuous)](#051e)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [SMOTE-NC（SMOTE-名义连续）](#051e)
- en: ∘ [SMOTE-N (SMOTE-Nominal)](#e808)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [SMOTE-N (SMOTE-名义)](#e808)
- en: Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: 'In this story, we will continue by considering the SMOTE, BorderlineSMOTE1,
    SMOTE-NC and SMOTE-N algorithms. But before we do, it’s worthy to point out the
    two algorithms we considered in the last story fit the following implementation
    framework:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个故事中，我们将继续考虑 SMOTE、BorderlineSMOTE1、SMOTE-NC 和 SMOTE-N 算法。但在此之前，值得指出的是我们在上一个故事中考虑的两个算法适合以下实现框架：
- en: Define how the algorithm takes data belonging to class k that needs *N_k* examples
    and computes such examples by oversampling
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义算法如何对属于类 k 的数据进行 *N_k* 个样本的计算，并通过过采样生成这些样本
- en: Given some ratios hyperparameter, compute the number of points that need to
    be added for each class
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定一些比率超参数，计算每个类需要添加的点数
- en: For each class run the algorithm then combine all newly added points together
    with the original data to form the final oversampled dataset
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个类运行算法，然后将所有新增点与原始数据结合起来，形成最终的过采样数据集
- en: 'For both the random oversampling and ROSE algorithms (probably also random
    walk oversampling if ratios are big enough) it was also true that to generate
    *N_k* examples for class k the algorithm does the following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于随机过采样和 ROSE 算法（如果比率足够大，可能也包括随机游走过采样），生成 *N_k* 个类 k 示例的算法也做了以下操作：
- en: Choose *N_k* points randomly with replacement from the data belonging to class
    k
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从属于类 k 的数据中随机选择 *N_k* 个点，允许重复。
- en: Perform logic on each of the chosen points to generate a new point (e.g., replication
    or placing a Gaussian then sampling from it)
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对每个选择的点执行逻辑以生成新点（例如，复制或放置高斯分布然后从中采样）
- en: It holds that the rest of the algorithms we will consider in this story also
    fit the same framework.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本故事中考虑的其余算法也适合相同的框架。
- en: '**SMOTE (Synthetic Minority Oversampling Technique)**'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**SMOTE (合成少数类过采样技术)**'
- en: 'It thereby follows that to explain what SMOTE does we only need to answer one
    question: What logic is performed on each of the *N_k* randomly chosen with replacement
    examples from class k in order to generate *N_k* new examples?'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，要解释 SMOTE 的作用，我们只需回答一个问题：为了生成 *N_k* 个新的示例，对每个从类 k 随机选择的 *N_k* 个示例执行了什么逻辑？
- en: 'The answer is as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 答案如下：
- en: Find the k-nearest neighbors of the point (k is a hyperparameter of the algorithm)
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到该点的 k 最近邻（k 是算法的超参数）
- en: Choose one of them randomly
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机选择其中一个
- en: Draw a line segment from the point to that randomly chosen neighbor
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从点绘制到随机选择的邻居的线段
- en: Randomly choose a point on that line segment
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机选择该线段上的一个点
- en: Return it as the new point
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将其作为新点返回
- en: Mathematically,
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，
- en: If point *x_i* has nearest neighbors *z_i1, z_i2, …, z_ik*
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果点 *x_i* 有最近邻 *z_i1, z_i2, …, z_ik*
- en: And if *j* is a random number in *[1,k]*
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *j* 是 *[1,k]* 范围内的随机数
- en: And *r* is a random number in *[0, 1]*
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并且 *r* 是 *[0, 1]* 范围内的随机数
- en: 'then for each point *x_i* SMOTE generates a new point *x_i’* by simply applying:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，对于每个点 *x_i*，SMOTE 通过简单应用生成一个新的点 *x_i’*：
- en: '![](../Images/a41c1312ef4852d106601cc513ec0e7b.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a41c1312ef4852d106601cc513ec0e7b.png)'
- en: This is really all what the SMOTE algorithm does. From point *x_i,* walk a distance
    *r* along the vector *z_ij — x_i* then place a new point.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 SMOTE 算法所做的全部。从点 *x_i* 沿向量 *z_ij — x_i* 行进距离 *r*，然后放置一个新点。
- en: '![](../Images/0cd51923480cdaf66d3c69190a6bfdf1.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0cd51923480cdaf66d3c69190a6bfdf1.png)'
- en: Figure Drawn by the Author. Examples in black are synthetically generated.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图由作者绘制。黑色示例是合成生成的。
- en: A minor side note is that there is a small difference in how the algorithm operates
    compared to how the algorithm is presented in the paper. In particular, the authors
    assume that ratios are integers (and floor it if not). If the ratio for class
    k is an integer C then for each point in it choose a random neighbor with replacement
    C times then apply the SMOTE logic we described. **In practice**, when SMOTE is
    implemented it’s typically generalized to work with floating point ratios as we
    described by rather choosing *N_k* points randomly then applying SMOTE on each.
    For integer ratios such as C=2, it follows that each point is picked on average
    twice and we go back to original algorithm. This should make sense as it is the
    same transition from oversampling by repeating with integer ratios to random oversampling
    which was explained in the last story.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一个小的侧面说明是算法操作方式与论文中呈现的方式有少许差异。特别是，作者假设比率为整数（如果不是，则向下取整）。如果类别k的比率是整数C，则对于每个点，在其内选择一个随机邻居，重复C次，然后应用我们描述的SMOTE逻辑。**实际上**，当实现SMOTE时，通常将其泛化为按我们描述的浮点比率工作，而是随机选择*N_k*个点，然后对每个点应用SMOTE。对于整数比率如C=2，平均每个点被选中两次，我们回到了原始算法。这应该是有道理的，因为这是从通过重复具有整数比率的过采样到随机过采样的相同转换，这在上个故事中已经解释过了。
- en: '![](../Images/a9f395610785b80be87714f2efef03d6.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9f395610785b80be87714f2efef03d6.png)'
- en: Animation by the Author
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 作者制作的动画
- en: This animation shows how the decision regions of an SVM change by changing the
    oversampling ratio for the versicolor class over an imbalanced subset of the Iris
    dataset. The ratio here is relative to the size of the majority class. That is,
    a ratio of 1.0 would set *N_k* so that the versicolor class has as much examples
    as the virginica class.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 此动画显示了在不平衡的鸢尾花数据集的偏好类别上改变过采样比例时，SVM的决策区域如何变化。这里的比例相对于多数类的大小。也就是说，比率为1.0将*N_k*设置为使versicolor类具有与virginica类相同数量的示例。
- en: 'You may be thinking why would SMOTE ever be better than ROSE. After all, SMOTE’s
    logic of generating points has not been justified in the paper; meanwhile, sampling
    from an estimate of *P(x|y)* as done in ROSE is much more rational and intuitive.
    One problem is possibly that getting a good estimate of *P(x|y)* requires a lot
    of data; however, we know that minority classes often have little data. If we
    don’t have a lot of data we have two options:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，为什么SMOTE比ROSE更好。毕竟，SMOTE生成点的逻辑在论文中并未得到证明；与此同时，从*P(x|y)*的估计中进行采样，就像ROSE中那样，更为合理和直观。可能的一个问题是获取*P(x|y)*的良好估计需要大量数据；然而，我们知道少数类通常数据较少。如果我们没有大量数据，我们有两个选择：
- en: Choose the bandwidth to be too small where we go back to possible overfitting
    as was in random oversampling
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择带宽过小，我们回到了可能的过拟合，就像在随机过采样中一样。
- en: Choose it to be too big which in the extreme case is equivalent to just uniformly
    adding random points from the feature space (i.e., unrealistic examples)
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择带宽过大，在极端情况下等同于从特征空间中均匀添加随机点（即，不现实的示例）。
- en: If you think about it, we should worry less about this problem in SMOTE. If
    a hyperplane that linearly perfectly separates the data exists then that solution
    still exists after applying SMOTE. In fact, the way SMOTE generates points may
    cause a nonlinear hypersurface to be become more linear so it seems to be at a
    much lower risk of causing the model to overfit.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细想想，我们应该更少地担心SMOTE中的这个问题。如果存在一个完全线性分离数据的超平面，那么应用SMOTE后这个解决方案仍然存在。事实上，SMOTE生成点的方式可能使非线性超表面变得更加线性化，因此在导致模型过拟合的风险要低得多。
- en: '**BorderlineSMOTE1**'
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**BorderlineSMOTE1**'
- en: 'There is a myriad of algorithms in the literature that are simply modifications
    or improvements over SMOTE. One popular instance is BorderlineSMOTE1 which applies
    the following modification:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中有很多算法只是对SMOTE的修改或改进。一个流行的例子是BorderlineSMOTE1，它应用以下修改：
- en: '![](../Images/a41c1312ef4852d106601cc513ec0e7b.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a41c1312ef4852d106601cc513ec0e7b.png)'
- en: when choosing *x_i,* instead of choosing it randomly from the set of all points
    belonging to the class, choose it randomly from the set of borderline (called
    DANGER in the paper) points belonging to the class. A point is borderline if the
    majority of its k-nearest neighbors but not all of them are from the majority/another
    class. It indeed makes sense logically that points close to the decision boundary
    meet this condition; it also holds that these are the most important points for
    deciding the decision boundary or the performance of the classification model.
    The rationale behind ignoring points where all the neighbors are from another
    class is that these are potentially noise.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择 *x_i* 时，不是随机从属于该类的所有点集中选择，而是随机从属于该类的边界（论文中称为DANGER）点集中选择。 如果其k个最近邻的大多数，但不是全部，来自大多数/其他类，则该点是边界点。
    逻辑上讲，靠近决策边界的点满足此条件，这些点也是决定决策边界或分类模型性能最重要的点。 忽略所有邻居都来自另一个类的点背后的理由是，这些可能是噪音。
- en: Another variant BorderlineSMOTE2 is described in the same paper and is simply
    a modification of the condition used to find borderline points.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个变种BorderlineSMOTE2在同一篇论文中描述，并且只是修改了用于查找边界点的条件。
- en: The following is an example of applying BorderlineSMOTE1 using the Imbalance.jl
    package in Julia
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在Julia中使用Imbalance.jl包应用BorderlineSMOTE1的示例
- en: '![](../Images/6680d63060f700ced18501a855fb87d5.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6680d63060f700ced18501a855fb87d5.png)'
- en: Figure by author
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 作者绘制的图表
- en: 'This is the corresponding animation:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这是相应的动画：
- en: '![](../Images/072d1e4463e7674ec7377ebd3e947125.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/072d1e4463e7674ec7377ebd3e947125.png)'
- en: '**SMOTE-NC (SMOTE-Nominal Continuous)**'
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**SMOTE-NC (SMOTE-Nominal Continuous)**'
- en: While both ROSE and SMOTE appear to offer a significant improvement over naive
    random oversampling, they come with the drawback of losing the capability to handle
    categorical variables which was not a problem for naive random oversampling. The
    authors of SMOTE were bright enough to think of a way to circumvent this by developing
    this extension for the SMOTE algorithm to handle cases where categorical features
    are also present.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然ROSE和SMOTE似乎比朴素的随机过采样有显著改进，但它们的缺点是失去了处理分类变量的能力，这对于朴素的随机过采样来说并不是问题。 SMOTE的作者们足够聪明，想出了一种方法来规避这个问题，通过开发这个SMOTE算法的扩展来处理同时存在分类特征的情况。
- en: You may be thinking that encoding categorical features would be a way to get
    around this whatsoever; however, this is not absolutely right because SMOTE or
    ROSE will then treat them as continuous and generate invalid values for them.
    For instance, if a feature is binary then the chosen point along the line may
    be 0.57 for the new point which is not 0 and 1\. Rounding it is a bad idea because
    that is equivalent to randomly choosing whether it is 0 or 1.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能认为编码分类特征可以绕过这个问题，然而这并不完全正确，因为SMOTE或ROSE会将它们视为连续的并为其生成无效值。 例如，如果一个特征是二进制的，那么沿线选择的点可能是0.57，这不是0和1\.
    将其四舍五入是一个坏主意，因为这等同于随机选择它是0还是1。
- en: 'Recall that the following is how SMOTE generated a new point:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，以下是SMOTE生成新点的方式：
- en: Suppose point *x_i* has nearest neighbors *z_i1, z_i2, …, z_ik*
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设点 *x_i* 有最近的邻居 *z_i1, z_i2, …, z_ik*
- en: let *j* be a random number in *[1,k]*
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让 *j* 是 *[1, k]* 中的随机数
- en: let *r* be a random number in *[0, 1]*
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让 *r* 是 *[0, 1]* 中的随机数
- en: then for each point *x_i* SMOTE generates a new point *x_i’* by simply applying
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个点 *x_i*，SMOTE通过简单地应用公式生成一个新点 *x_i’*
- en: '![](../Images/a41c1312ef4852d106601cc513ec0e7b.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a41c1312ef4852d106601cc513ec0e7b.png)'
- en: It should be obvious that we can’t apply the same method in the presence of
    categorical features unless we extend it by answering the following two questions
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们无法在存在分类特征的情况下应用相同的方法，除非我们通过回答以下两个问题来扩展它
- en: How are the k-nearest neighbors found? The Euclidean distance metric only operates
    on continuous features.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何找到k个最近邻？ 欧几里得距离度量只在连续特征上运行
- en: How is the new point generated? We can’t apply the SMOTE equation to generate
    the categorical part of *x_i’*
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 新点是如何生成的？我们不能应用SMOTE方程来生成 *x_i’* 的分类部分
- en: For the first question, the author’s suggested a modification to the Euclidean
    distance to account for the categorical part. Suppose each of *x_i* and *z_ij*
    involve *m* continuous features and *n* categorical features then in the modified
    metric the continuous features are naturally subtracted and squared and then a
    constant penalty is added for each differing pair of categorical features. This
    penalty is in particular the median of variances of all continuous features which
    can be computed at the start of the algorithm.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个问题，作者建议对欧几里得距离进行修改，以考虑分类部分。假设每个*x_i*和*z_ij*涉及*m*个连续特征和*n*个分类特征，那么在修改后的度量中，连续特征自然被相减并平方，然后对于每对不同的分类特征添加一个常数惩罚。这个惩罚特别是所有连续特征方差的中位数，可以在算法开始时计算出来。
- en: As an example, to measure the distance between two points *x_1* and *x_2*
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要测量两个点*x_1*和*x_2*之间的距离
- en: '![](../Images/4f6a88f01c3e1bc54d7c5919b676a769.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f6a88f01c3e1bc54d7c5919b676a769.png)'
- en: then if the median of standard deviations is *m*, the distance is given by
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果标准差的中位数是*m*，则距离由下式给出
- en: '![](../Images/3058b57d4f0efd18e637dc37e4b75ffd.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3058b57d4f0efd18e637dc37e4b75ffd.png)'
- en: the last two terms account for how the last two categorical features are different.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的两个项考虑了最后两个分类特征的不同。
- en: Although authors provide no justification for the metric, it may make sense
    after observing that one of the most common way to measure distances among categorical
    features is Hamming distance. It simply adds 1 for each differing pair of categorical
    features. A Hamming distance of 6 suggests that the two points have different
    values in 6 of the categorical features. It’s obvious in our case that setting
    the penalty as 1 as in Hamming distance is not intuitive because if continuous
    features often strongly vary then the 1s will be very insignificant in the sum
    which is equivalent to ignore the categorical features in the measurement. It
    should make sense that using the average squared difference between any two continuous
    features as the penalty would get around this problem because then if the variance
    of continuous features is often large, the penalty is as large and is not negligible.
    The only catch is that the authors used the median of variances and not their
    mean which may be justified by its robustness to outliers.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然作者没有对度量提供理由，但观察到在分类特征之间测量距离的最常见方法之一是汉明距离。它简单地对每对不同的分类特征加1。汉明距离为6表示两个点在6个分类特征上有不同的值。在我们的案例中，将惩罚设置为1（如同汉明距离中）并不直观，因为如果连续特征经常发生强烈变化，那么1的值在总和中将非常微不足道，这等同于忽略分类特征的测量。使用任何两个连续特征之间的平均平方差作为惩罚应该能够解决这个问题，因为如果连续特征的方差通常很大，惩罚也会很大且不可忽略。唯一的问题是作者使用了方差的中位数而不是均值，这可能是由于其对异常值的鲁棒性。
- en: Answering the second question is far more simple, now that we have found the
    k-nearest neighbors using the modified metric we can generate the continuous part
    of the new point using the SMOTE equation as usual. To generate the categorical
    part of the new point, it makes sense to simply take the mode of the categorical
    parts of the k-nearest neighbors. I.e., let the neighbors vote over the values
    in the categorical part where the most common values will dominate.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 回答第二个问题要简单得多，现在我们已经使用修改后的度量找到k个最近邻，我们可以像往常一样使用SMOTE公式生成新点的连续部分。为了生成新点的分类部分，简单地取k个最近邻分类部分的众数是有意义的。即，让邻居对分类部分的值进行投票，其中最常见的值将占主导地位。
- en: It follow that what SMOTE-NC does to generate a new point is…
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，SMOTE-NC生成新点的过程是……
- en: Find the k-nearest neighbors of the point (k is a hyperparameter of the algorithm)
    using the modified Euclidean metric
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用修改后的欧几里得度量找到点的k个最近邻（k是算法的一个超参数）
- en: Choose one of them randomly
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机选择其中一个
- en: Draw a line segment from the point to that neighbor in the continuous features
    space
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从该点到邻居在连续特征空间中画一条线段
- en: Randomly choose a point on that line segment
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机选择线段上的一个点
- en: Let that be the continuous part of the new point
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让这条线段成为新点的连续部分
- en: For the categorical part of the new point, take the mode of the categorical
    parts of the k-nearest neighbors.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于新点的分类部分，取k个最近邻分类部分的众数。
- en: '**SMOTE-N (SMOTE-Nominal)**'
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**SMOTE-N (SMOTE-Nominal)**'
- en: It should be obvious that SMOTE-NC becomes SMOTE when no categorical features
    are involved because then the penalty is zero and the mode step in generation
    is skipped. However, if no continuous features are involved then the algorithm
    is in a precarious position because there is no penalty defined as there are no
    continuous features. Your workaround may be to set it as 1 or something and operate
    the algorithm as normal but that is not ideal because there will easily be many
    ties when computing the nearest neighbors. *If the Hamming distance between one
    point and another 10 points is 7 are they all really equally close to that point?*
    Or do they just share in common that they differ from the point in 7 of the features?
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 应该很明显，当没有分类特征参与时，SMOTE-NC会变成SMOTE，因为那时惩罚为零，生成中的模式步骤被跳过。然而，如果没有连续特征参与，则算法处于一种不稳定的状态，因为没有定义惩罚，因为没有连续特征。你的解决办法可能是将其设置为1或其他值，然后按正常方式操作算法，但这并不理想，因为在计算最近邻居时会容易出现许多平局。*如果一个点与另10个点之间的汉明距离是7，它们真的都同样接近那个点吗？*
    还是它们只是共享在7个特征上与该点不同的共同点？
- en: SMOTE-N is another algorithm that the authors present in the paper to deal with
    data that is purely categorical. It responds negatively to the italicized question
    above by employing another distance metric over the categorical features. Once
    the k nearest neighbors are found mode computation decides the new points; however,
    this time the point itself also is involved in the mode computation (voting).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: SMOTE-N是作者在论文中提出的另一种算法，用于处理纯粹分类的数据。它通过在分类特征上采用另一种距离度量来对上述*斜体*问题作出负面回应。一旦找到k个最近邻居，模式计算决定了新点；然而，这次点本身也参与模式计算（投票）。
- en: It hence suffices to explain the distance metric used in SMOTE-N to perform
    K-NN. The metric is called the “modified value distance metric” (Cost & Salzberg,
    1993) and it operates as follows given two feature vectors with q categorical
    features and p_1, p_2, …,p_q possible values for each of the categorical features
    respectively.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，足以解释在SMOTE-N中用于执行K-NN的距离度量。该度量称为“修改值距离度量”（Cost & Salzberg, 1993），其操作如下：给定具有q个分类特征和每个分类特征分别有p_1,
    p_2, …, p_q个可能值的两个特征向量。
- en: Encode each of the categorical values via a vector V of length K where K is
    the number of classes. V[i] should be the frequency of that value for the i_th
    class divided by its frequency over all classes.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过长度为K的向量V对每个分类值进行编码，其中K是类别的数量。V[i]应为该值在第i类中的频率除以其在所有类别中的频率。
- en: Now any categorical vector is represented by a tensor of q vectors each of length
    k
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在任何分类向量都由q个长度为k的向量组成的张量表示。
- en: Compute the distance between any two categorical vectors represented by that
    tensor by computing the Manhattan distance between each pair of vectors of length
    k then take the L2 norm of the result
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过计算每对长度为k的向量之间的曼哈顿距离，然后取结果的L2范数，来计算该张量所表示的任意两个分类向量之间的距离。
- en: For an example, suppose we want to find the distance between the following two
    categorical vectors
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们要找出以下两个分类向量之间的距离
- en: '![](../Images/6178857c9fb658a728c51cc0a807d815.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6178857c9fb658a728c51cc0a807d815.png)'
- en: then given 3 classes, after encoding them suppose we end up with
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然后给定3个类别，编码后假设我们得到了
- en: '![](../Images/ea692d9ecf0dca1f48c40ded514b267b.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea692d9ecf0dca1f48c40ded514b267b.png)'
- en: After computing the Manhattan distance between each pair of vectors we have
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 计算每对向量之间的曼哈顿距离后，我们得到
- en: '![](../Images/876b46aaf6f81c5eee83792f511f2a82.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/876b46aaf6f81c5eee83792f511f2a82.png)'
- en: which evaluates to 1.428 after talking the L2 norm.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这在取L2范数后评估为1.428。
- en: To be precise, the paper points out that it is possible to use either the L1
    norm or the L2 for the magnitude but does not decide which to use for the algorithm
    (here we chose L2).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 精确地说，论文指出可以使用L1范数或L2范数来表示大小，但没有决定算法使用哪种（这里我们选择了L2）。
- en: You may be asking yourself why this would be any better than using plain Hamming
    distance. The definite answer is that the authors have not justified. However,
    just to introduce some slight intuition, we argued earlier that the Hamming may
    often result in a lot of ties during the distance computation for KNN. Suppose
    we have three categorical vectors
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问为什么这比使用普通的汉明距离更好。明确的答案是作者并没有做出合理解释。然而，仅为了引入一些直观的理解，我们之前争论了汉明距离在KNN的距离计算中可能经常导致许多平局。假设我们有三个分类向量
- en: '![](../Images/08b11fb1254e2775d25df3f02061bf9b.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08b11fb1254e2775d25df3f02061bf9b.png)'
- en: Here Hamming distance would suggest that *x_2* and *x_3* are as close to *x_1*
    as in both cases the Hamming distance is 1\. Meanwhile, the modified value difference
    metric would look at how each value is distributed over the classes first before
    deciding which is closer. Suppose the frequencies per class for B2 is [0.3, 0.2,
    0.5] and for B3 is [0.1, 0.9, 0] and for B1 is [0.25, 0.25, 0.5]. In this case,
    MVDM would suggest that *x_3* is much closer to *x_1* because B1 is much closer
    to B2 than B3 is. From a probabilistic perspective, if we were to collect a new
    point with an unknown class then knowing whether the category is B2 or B3 does
    not help much predict the class and in this sense they are similar or interchangeable.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，汉明距离会建议 *x_2* 和 *x_3* 距离 *x_1* 相同，因为两者的汉明距离都是1。同时，修改值差异度量会先考虑每个值在各个类别中的分布，然后再决定哪个更接近。假设B2的类别频率为
    [0.3, 0.2, 0.5]，B3为 [0.1, 0.9, 0]，B1为 [0.25, 0.25, 0.5]。在这种情况下，MVDM 会建议 *x_3*
    更接近 *x_1*，因为B1比B3更接近B2。从概率的角度来看，如果我们收集一个类别未知的新点，那么知道该类别是B2还是B3对于预测类别帮助不大，因此它们在这个意义上是相似的或可以互换的。
- en: 'Hence, in summary the SMOTE-N algorithm operates as follows to generate a new
    point:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，总结来说，SMOTE-N算法生成新点的过程如下：
- en: Find the k-nearest neighbors of the point (k is a hyperparameter of the algorithm)
    using the modified value difference metric
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用修改值差异度量找到点的k近邻（k是算法的超参数）
- en: Return the mode of the categorical values of the neighbors (including the point
    itself) to generate the new point
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回邻居的类别值的众数（包括点自身）以生成新点
- en: That’s it! It should be now crystal clear to you how each of SMOTE, BorderlineSMOTE1,
    SMOTE-N and SMOTE-NC work. We conclude this series of explaining all resampling
    algorithms initially implemented in the Julia package [Imbalance.jl](https://github.com/JuliaAI/Imbalance.jl)
    with [this story](https://medium.com/towards-data-science/class-imbalance-exploring-undersampling-techniques-24009f55b255)
    on undersampling.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！现在你应该清楚了SMOTE、BorderlineSMOTE1、SMOTE-N和SMOTE-NC各自的工作原理。我们结束了对所有在Julia包 [Imbalance.jl](https://github.com/JuliaAI/Imbalance.jl)
    中最初实现的重采样算法的解释系列，并附上了 [这篇文章](https://medium.com/towards-data-science/class-imbalance-exploring-undersampling-techniques-24009f55b255)
    关于欠采样的故事。
- en: '**References:**'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献：**'
- en: '[1] N. V. Chawla, K. W. Bowyer, L. O.Hall, W. P. Kegelmeyer, “SMOTE: synthetic
    minority over-sampling technique,” Journal of artificial intelligence research,
    321–357, 2002.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] N. V. Chawla, K. W. Bowyer, L. O. Hall, W. P. Kegelmeyer，“SMOTE：合成少数类过采样技术，”人工智能研究杂志，321–357，2002年。'
- en: '[2] H. Han, W.-Y. Wang, and B.-H. Mao, “Borderline-SMOTE: A New Over-Sampling
    Method in Imbalanced Data Sets Learning,” in International Conference on Intelligent
    Computing, 2005, pp. [878–8871](https://sci2s.ugr.es/keel/keel-dataset/pdfs/2005-Han-LNCS.pdf)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] H. Han, W.-Y. Wang, 和 B.-H. Mao，“Borderline-SMOTE：一种新的不平衡数据集学习过采样方法，”国际智能计算会议，2005年，页码
    [878–8871](https://sci2s.ugr.es/keel/keel-dataset/pdfs/2005-Han-LNCS.pdf)'
