- en: A gentle introduction to Steerable Neural Networks (part 2)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-gentle-introduction-to-steerable-neural-networks-part-2-56dfc256b690?source=collection_archive---------9-----------------------#2023-11-21](https://towardsdatascience.com/a-gentle-introduction-to-steerable-neural-networks-part-2-56dfc256b690?source=collection_archive---------9-----------------------#2023-11-21)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to build a Steerable Filter and a steerable CNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mat.cip43?source=post_page-----56dfc256b690--------------------------------)[![Matteo
    Ciprian](../Images/61dab88069d99263e941a0e549473bdf.png)](https://medium.com/@mat.cip43?source=post_page-----56dfc256b690--------------------------------)[](https://towardsdatascience.com/?source=post_page-----56dfc256b690--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----56dfc256b690--------------------------------)
    [Matteo Ciprian](https://medium.com/@mat.cip43?source=post_page-----56dfc256b690--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F975b976da56a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-steerable-neural-networks-part-2-56dfc256b690&user=Matteo+Ciprian&userId=975b976da56a&source=post_page-975b976da56a----56dfc256b690---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----56dfc256b690--------------------------------)
    ·10 min read·Nov 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F56dfc256b690&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-steerable-neural-networks-part-2-56dfc256b690&user=Matteo+Ciprian&userId=975b976da56a&source=-----56dfc256b690---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F56dfc256b690&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-steerable-neural-networks-part-2-56dfc256b690&source=-----56dfc256b690---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: 1) Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article is the second and last part of the tutorial **“A gentle introduction
    to Steerable Neural Networks”**. It follows the article number one ([here](https://medium.com/@mat.cip43/a-gentle-introduction-to-steerable-neural-networks-part-1-32323d95b03f)).
  prefs: []
  type: TYPE_NORMAL
- en: The first article offers an accessible overview of Steerable Neural Networks
    (S-CNNs), explaining their purpose and applications. It also delves into the underlying
    formalism and key concepts, including the definition of **equivariance** and **steerable
    filters**. Although a quick recap of the formalism will be given in the next paragraph,
    we would recommend you read the first article for a complete understanding.
  prefs: []
  type: TYPE_NORMAL
- en: '*In this last part of the tutorial, we want to provide a guide on how to build
    a* ***Steerable Filter*** *and at the end , how to compose a Steerable Neural
    Network.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Quick recap of nomenclature:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0d083f156172448f1c8b6b6eb607045a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 3A: Representation of a neural network following the formalism.'
  prefs: []
  type: TYPE_NORMAL
- en: '**S** : input domain space. Space where the objects exists (usually ℝ³ or ℝ²).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***f*** *ₙ****:*** a map/function , *f ₙ:* S → ℝ ͨ ʿ*ⁿ* ʾ ( F*ₙ*) *,* which
    describes the n-th feature map of the NN . Note that the *f* ⁰ is the function
    describing the input (input layer), while *fₙ, with n>0,* describe the *n-th*
    feature map.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F***ₙ***= ℝ ͨ ʿ*ⁿ* ʾ**, it is codomain of the map *f ₙ.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Φ***ₙ***: F***ₙ****→ F*** *ₙ₊₁****,*** *n-th* ͑filterof the NN characterizable
    by the kernel function *k****ⁿ*** *: S →* ℝ ͨ ʿ*ⁿ* ʾ ˟ ͨ ʿ*ⁿ⁺ ¹* ʾ . The definition
    of convolution can be seen in the second equation above.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**G**: group of transformations ( single element *g).*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Considering all these concepts we have been able to define convolution as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a6e4eece3326f205c8b544d4ec5627b4.png)![](../Images/1fc9a17c9ca2c61643ca20e9da05e064.png)'
  prefs: []
  type: TYPE_IMG
- en: 2) Design of a Steerable CNN filter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/b7c3c0dab47626529d9cbdd3d2a6c5ce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig3A: Visual example of equivariant CNN filter. Given the transformation g
    acting on S and the consequent rotation of the input signal f given by Π₀(g),
    f1 is rotated by Π1(g).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Formalization of the problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can state that a CNN of n layers is equivariant with respect to a group
    of transformations G if, for every *g in G,0*: *when an input function f ₀ is
    transformed to Π₀(g), then the output function of the n-th layer is transformed
    to a transformation Πn(g).*'
  prefs: []
  type: TYPE_NORMAL
- en: 'A sufficient condition to make this statement true is that each contiguous
    layer is equivariant to transformations on it’s immediate inputs (see fig 3A).
    The equivariance of the network comes by induction. Following the definitions
    given in the second article, a filter Φ is equivariant if it satisfies the following
    condition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d5c3b29e939626aaa18c1f6770b5e3f7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Eq.0: Definition of equivariance'
  prefs: []
  type: TYPE_NORMAL
- en: It is now possible to claim the **main result** of the steerable neural network
    theory.
  prefs: []
  type: TYPE_NORMAL
- en: '*Be* ***k*** *the kernel connecting the layer* f *ₙwith* f *such that* fₙ₊₁
    = k* f ₙ*.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*The convolution* k* f ₙ*is equivariant with respect to a transformation g,
    if and only if :*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/702fd59bbe89895caaf8f03ed705cb3a.png)'
  prefs: []
  type: TYPE_IMG
- en: '*or simpler*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/82a6e1a92a10f1b410b49216cc6989f4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Eq.1: Necessary and Sufficient condition for equivariance of a kernel with
    respect to a transformation g.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the broader literature [2,3] kernels that adhere to this constraint are
    calledg-steerablekernels. As the kernel constraint operates in a linear manner,
    the solutions it generates constitute a linear subspace within the vector space
    of unconstrained kernels typically utilized in standard CNN. *Upon closer examination,
    this definition closely aligns with the concept of steerable filters introduced
    in paragraph 2 of the last article* [*here*](https://medium.com/@mat.cip43/a-gentle-introduction-to-steerable-neural-networks-part-2-a17b4139aa5f).
    In practical terms, to get this work we need a basis for this kernel subspace,
    denoted as {k_1, …k_D} , which adheres to Eq.1 . The size of this basis, denoted
    as D, can be calculated as D = cʿⁿ ʾ ˟ cʿⁿ⁺¹ʾ.The kernel *k(x)* is subsequently
    derived through a linear combination of this basis, with the network learning
    the weights in the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e8b0ba730141ddb8e89c464338a011ac.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Eq.2: The linearity of Eq(1) makes the solution be equal to the following linear
    combination.'
  prefs: []
  type: TYPE_NORMAL
- en: In a training scenario, our approach involves setting the sizes of both the
    input and output layers to specific values, namely cʿⁿ ʾ and cʿⁿ⁺¹. Then, based
    on the transformations we seek equivariance to, we solve the equation and determine
    a kernel basis. Subsequently, during the training process, we learn the weights
    associated with these kernels.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Solving the equation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The solution of the constrain presented in Eq.1 is from being trivial. It depends
    on three main elements:'
  prefs: []
  type: TYPE_NORMAL
- en: the space S, whether it is S= ℝ³ or S= ℝ².
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The group *G.*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The input out dimension of the layers: cʿ*ⁿ* ʾ and cʿ*ⁿ⁺ ¹* ʾ*.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More specifically we can say that the choice of the group G defines the type
    of the network. Specifically we are mainly interested on the following type of
    networks:'
  prefs: []
  type: TYPE_NORMAL
- en: 'SO Networks: Equivariant to rotations in the Special Orthogonal Group (SO).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SE Networks: Equivariant to rotations and translations within the Special Euclidean
    Group (SE).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'E Networks: Equivariant to rotations, translations, and reflections in the
    Euclidean Group (E).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we operate in a 2D input domain, we have SO(2), SE(2), and E(2) networks
    [[4]](https://arxiv.org/abs/1911.08251). Conversely, with a 3D input domain, we
    work with SO(3), SE(3), and E(3) networks[[1](http://link)], and indeed this can
    be extended to any E(n) space [[6]](https://openreview.net/pdf?id=WE4qe9xlnQw).
  prefs: []
  type: TYPE_NORMAL
- en: Extending this work into other spaces and symmetries is an area of ongoing research,
    an interested reader is encouraged to investigate the fields of mathematical study
    known as Hilbert spaces and Green’s functions, a discussion of which here is out
    of the scope of this article.
  prefs: []
  type: TYPE_NORMAL
- en: However it is possible to see that in case of SE(n) networks the general solution
    of Eq.1 is a harmonic basis function in S= ℝ*ⁿ*. In the image above (Fig 3B) it
    is possible the harmonic functions in ℝ² on the left and harmonic functions in
    ℝ³.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/643be215a340fa4459acfae5cf9f293d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 3B: Basis of harmonic functions in 2D (left) and in 3D (right). This basis
    constitutes a basis of steerable equivariant filters in SE(2) and SE(3) networks
    respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering a more filter design scenario, in the image Fig 3C below, we see
    for example how an SO2 steerable equivariant kernel is built for a input layer
    *f ₀*: ℝ²->ℝ³ and an output layer of *f₁*:ℝ²->ℝ².'
  prefs: []
  type: TYPE_NORMAL
- en: 'The kernel is a function *k*: ℝ²->ℝ³ˣ². Each single element of the matrix is
    obtained from a function resulting from the linear weighted combination of the
    D basis sampled at the position *(x₁,x₂)* . We see the example above for position
    *x=(1,2)*.'
  prefs: []
  type: TYPE_NORMAL
- en: As follows we will show some simple solutions of this equation considering ,
    S=ℝ² and G as group of rotation transformations impling SO2 networks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3ae0f08c22bf149cea91fecb975c2883.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 3C: Visualization representation of a steerable kernel 3x2 is built using
    a basis of 6 harmonic functions.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Practical solutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '- Case1A: SO2 networks , k: S=ℝ² → ℝ'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s imagine the practical case of having a greyscale image as input and we
    want to build a steerable filter to process it. First of all, we have to decide
    the dimension of the output layer (number of features). Let’s take for simplicity
    dimension 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this setup, we have an input function *f*: ℝ²-> ℝ and a similar output function
    *f₁*: ℝ²-> ℝ. Therefore the kernel function is *k*: ℝ² -> ℝ. We want our CNN layer
    to be equivariant to a group of transformations, G, which represents rotations
    by angle theta within [0,2π) (SO network). For this problem, the kernel function’s
    basis requires using Eq.1\. Given that both f and f¹ are scalar, Pi_out = 1 and
    Pi_in = 1\. This results *k[****g_****θ(x)] = k[x]* as written in Eq.3 .'
  prefs: []
  type: TYPE_NORMAL
- en: If *x = (x₁, x₂)* is in ℝ², g(theta) aligns with the 2D Euler matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/23227fb2d9498574821a1a0809d776c2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Eq.3: Rewriting Eq.(1) in case of k: S=ℝ² → ℝ'
  prefs: []
  type: TYPE_NORMAL
- en: It is trivial to see that this is solved by each isotropic function in (*x₁,
    x₂*) . Specifically this resolves with a one dimensional basis of isotropic (rotation
    invariant) kernels. (i.e. *k(x₁, x₂)* = *x₁*² +*x₂*²)
  prefs: []
  type: TYPE_NORMAL
- en: 'Case 2: SO2 filter , k: ℝ² → ℝ²'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s take now a more complex case. The input function is *f:* ℝ² → ℝ² and
    the output layer is a function *f ₁:* ℝ² → ℝ². The kernel can be therefore written
    as function *k: S=* ℝ² *→* ℝ² ˣ ²; in other words for each position x in ℝ² we
    have a bi-dimensional matrix 2x2 (see equation below). We want to build S02 filter
    so the group of transformations to consider is again G={g(*θ*)} ={r(*θ*)} , *θ*
    ∈ *[0,2Π[*. Being ℝ² the codomain of *f* and *f ₁, Π_out=Π_θ* and *Π_in=Π_θ ,*
    where Π_θ is the Euler matrix in ℝ². Considering all these conditions we can rewrite
    Eq.1 above in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9c16aa56e4b18daebd5ae7451eff30cc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Eq.(4): kernel function considering *k: S=* ℝ² *→* ℝ² ˣ ²'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/234b4cbcbdd561e26738e4c75aac7564.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Eq.(5): Rewriting Eq.(1) for a SO2 kernel k: S=ℝ² → ℝ².'
  prefs: []
  type: TYPE_NORMAL
- en: For a more comprehensive understanding of the solution to this equation and
    additional insights, please refer to the appendix section in the paper [4].
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Network non-linearities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far we have only considered equivariance with respect to the convolution
    operation, not considering the non-linear part given by the function σ*(f(x)):*
    **ℝ=**ℝ ͨ→ℝ ͨ’. In session 4.3 of paper [1] and 2.6 session of paper [4] this
    is widely treated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the function f(x) the condition of equivariance can be summarized as
    following :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/af2dadd3f7be62964669f540ae651f03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Eq.(5) : Condition of equivariance for the activation function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As also mentioned in a related YouTube lecture [here](https://www.youtube.com/watch?v=b8K6adf_zY0),
    it’s possible to create an activation function that meets this criterion by utilizing
    what’s known as a norm-based activation function like **σ*(u) =* σ*(||u||)***.
    The motivation for this is that a scalar norm is transparently invariant and so
    the application of any nonlinear function to it will result in an invariant output.
    To prove this, when we apply this formula to the aforementioned condition to Eq.(5)
    we obtain the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/16f45464bf676098d1c8a548920ba2a2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Eq.(6): Rewriting Eq.(5) as norm-based functions.'
  prefs: []
  type: TYPE_NORMAL
- en: If ‘g’ belongs to the group of E transformations, the norm remains constant.
    Consequently, the equation is universally valid when Π’(g) equals the Identity.
    This implies that the specially designed activation function is consistently rotation
    invariant. An example of this , *Norm-ReLUs*, defined by *η(|f(x)|) = ReLU(|f(x)|
    − b)*
  prefs: []
  type: TYPE_NORMAL
- en: Additional nonlinear activation functions have been suggested in the research
    papers and the lecture, such as non-gated activation functions. We refer the reader
    to these sources for further explanation.
  prefs: []
  type: TYPE_NORMAL
- en: '**3) Design a steerable CNN**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/4c4ab19a773efc0fc05e8dda9e506ba6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 3D: The architecture of a steerable CNN as described in [[3]](https://arxiv.org/abs/1711.07289).
    Notice the use of the steerable filters in layer 2 coupled together with a G-convolution.'
  prefs: []
  type: TYPE_NORMAL
- en: In the previous session, we grasped the fundamentals of constructing a single
    steerable filter. In this concluding segment, we will delve into the methodology
    of integrating these filters cohesively to establish a fully-functional steerable
    neural network.
  prefs: []
  type: TYPE_NORMAL
- en: In the picture above we can see an example taken by the paper [[3]](https://arxiv.org/abs/1711.07289).
    We are particularly interested in layer 2 where the steerable filters are used.
  prefs: []
  type: TYPE_NORMAL
- en: Here, each horizontal representation is a steerable filter — a composite of
    weighted harmonic functions — that yields a distinct output, denoted as single
    fⁿ. Observing the structure, it’s apparent that while harmonic functions remain
    consistent across the filters, their orientations vary from one to the next. This
    is emblematic of the G-convolution technique, a sophisticated method that contributes
    to the construction of networks invariant to transformation (you can find more
    information on this technique [here](https://chat.openai.com/c/64d8f147-a86f-497f-be18-2e09cb13da96#)).
    The network harnesses the power of max-pooling to funnel only the most robust
    responses from the array of steerable filters into the subsequent layer. This
    principle of selective transmission ensures that the strongest features are preserved
    and enhanced as they progress through the network. This approach mirrors the methodologies
    implemented in other works, such as in reference [5], which successfully crafted
    a scale-invariant steerable network. The architecture of such a steerable CNN
    benefits from this technique, as it naturally incorporates scale and rotation
    invariance, thereby enriching the network’s ability to recognize patterns and
    features in a more abstract yet robust manner.In any case, it is possible to see
    from the picture that the final result is a network equivariant to rotations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/206e02a4aa42a18b1aba73b7faab5f60.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 3E: A visual example of the application of a 2D steerable filter on a rotated
    image (original can be found [here](https://github.com/QUVA-Lab/e2cnn))'
  prefs: []
  type: TYPE_NORMAL
- en: An excellent step-by-step explanation about the design of a steerable neural
    network can be found at this [link](https://github.com/QUVA-Lab/e2cnn/blob/master/examples/introduction.ipynb),
    included in the Github repo *“e2cn” (*[link](https://github.com/QUVA-Lab/e2cnn)).
    In this repo, it is possible to find the PyTorch code for designing an SE2 steerable
    network. Useful code for the development of SE3 networks can be found instead
    at this [link](https://github.com/mariogeiger/se3cnn), while a quick course on
    3D equivariant networks has been published [here](http://www.ipam.ucla.edu/abstract/?tid=16346&pcode=MLPWS1).
  prefs: []
  type: TYPE_NORMAL
- en: 'LITERATURE:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] “3D Steerable CNNs: Learning Rotationally Equivariant Features in Volumetric
    Data”, Weilier et al., ([link](https://arxiv.org/abs/1807.02547));'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] “Steerable CNNs”, Cohen et al. [( link](https://arxiv.org/abs/1612.08498));'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] “Learning Steerable Filters for Rotation Equivariant CNNs”,Weilier et al.,
    ([link](https://arxiv.org/abs/1711.07289))'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] “General E(2)-Equivariant Steerable CNNs” Weilier et al., ([link](https://arxiv.org/abs/1911.08251))'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] “Scale Steerable Filters for the Locally Scale-Invariant Convolutional
    Neural Network”, Ghosh et al. ([link](https://www.researchgate.net/publication/334480982_Scale_Steerable_Filters_for_the_Locally_Scale-Invariant_Convolutional_Neural_Network?enrichId=rgreq-7fc8b3654779eb94d36221a6e5fab2ff-XXX&enrichSource=Y292ZXJQYWdlOzMzNDQ4MDk4MjtBUzo3ODEyNTQ4ODI1MDg4MDBAMTU2MzI3NzA4NzY1NA%25253D%25253D&el=1_x_3&_esc=publicationCoverPdf))'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] “A program to build E(n)-equivariant steerable CNNs.” Cesa et al. ([link](https://openreview.net/pdf?id=WE4qe9xlnQw))'
  prefs: []
  type: TYPE_NORMAL
- en: '✍️ 📄. About the authors:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '***1️⃣ Matteo Ciprian*,** Machine Learning Engineer/Researcher'
  prefs: []
  type: TYPE_NORMAL
- en: MSc in Telecommunications Engineering at University of Padua. Currently working
    in the field of Sensor Fusion, Signal Processing and applied AI. Experience in
    projects related to AI applications in eHealth and wearable technologies (academic
    research and corporate domains). Specialized in developing Anomaly Detection algorithms,
    as well as advancing techniques in Deep Learning and Sensor Fusion.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Passionate about Philosophy. Content creator in Youtube.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**🔗 Links:** 💼 [Linkedin](https://www.linkedin.com/in/matteo-ciprian-ba30ab122/)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 📹 [Youtube](https://www.youtube.com/channel/UCF--7G3kkCmEsdPLm8wyPow)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 👨‍💻[Instagram](https://www.instagram.com/cip_mat/)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2️⃣ ***Robert Schoonmaker*,** Signal Processing/Machine Learning Researcher
  prefs: []
  type: TYPE_NORMAL
- en: PhD in Computational Condensed Matter Physics from Durham University. Specializes
    in applied machine learning and nonlinear statistics, currently investigating
    the uses of GPU compute methods on synthetic aperture radar and similar systems.
    Experience includes developing symmetric ML methods for use in sensor fusion and
    positioning techniques.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**🔗 Links:** 💼 [Linkedin](https://www.linkedin.com/in/robert-schoonmaker-951221b/)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
