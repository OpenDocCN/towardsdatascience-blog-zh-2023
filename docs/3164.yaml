- en: Reshaping the Model’s Memory without the Need for Retraining
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新塑造模型的记忆，无需重新训练
- en: 原文：[https://towardsdatascience.com/reshaping-the-models-memory-without-the-need-for-retraining-9ade69f56296?source=collection_archive---------5-----------------------#2023-10-20](https://towardsdatascience.com/reshaping-the-models-memory-without-the-need-for-retraining-9ade69f56296?source=collection_archive---------5-----------------------#2023-10-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/reshaping-the-models-memory-without-the-need-for-retraining-9ade69f56296?source=collection_archive---------5-----------------------#2023-10-20](https://towardsdatascience.com/reshaping-the-models-memory-without-the-need-for-retraining-9ade69f56296?source=collection_archive---------5-----------------------#2023-10-20)
- en: '| AI | LARGE LANGUAGE MODELS| MACHINE UNLEARNING|'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '| AI | 大型语言模型| 机器遗忘|'
- en: Erasing any echo of problematic content a large language model has learned
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 消除大型语言模型学习的任何问题内容的回声
- en: '[](https://salvatore-raieli.medium.com/?source=post_page-----9ade69f56296--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----9ade69f56296--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9ade69f56296--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9ade69f56296--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----9ade69f56296--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://salvatore-raieli.medium.com/?source=post_page-----9ade69f56296--------------------------------)[![萨尔瓦托雷·拉伊利](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----9ade69f56296--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9ade69f56296--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9ade69f56296--------------------------------)
    [萨尔瓦托雷·拉伊利](https://salvatore-raieli.medium.com/?source=post_page-----9ade69f56296--------------------------------)'
- en: ·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff1a08d9452cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freshaping-the-models-memory-without-the-need-for-retraining-9ade69f56296&user=Salvatore+Raieli&userId=f1a08d9452cd&source=post_page-f1a08d9452cd----9ade69f56296---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9ade69f56296--------------------------------)
    ·11 min read·Oct 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9ade69f56296&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freshaping-the-models-memory-without-the-need-for-retraining-9ade69f56296&user=Salvatore+Raieli&userId=f1a08d9452cd&source=-----9ade69f56296---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff1a08d9452cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freshaping-the-models-memory-without-the-need-for-retraining-9ade69f56296&user=Salvatore+Raieli&userId=f1a08d9452cd&source=post_page-f1a08d9452cd----9ade69f56296---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9ade69f56296--------------------------------)
    ·11 min read·2023年10月20日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9ade69f56296&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freshaping-the-models-memory-without-the-need-for-retraining-9ade69f56296&user=Salvatore+Raieli&userId=f1a08d9452cd&source=-----9ade69f56296---------------------clap_footer-----------)'
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9ade69f56296&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freshaping-the-models-memory-without-the-need-for-retraining-9ade69f56296&source=-----9ade69f56296---------------------bookmark_footer-----------)![](../Images/4d98c180635de8d1ec4607a94fdfa029.png)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9ade69f56296&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freshaping-the-models-memory-without-the-need-for-retraining-9ade69f56296&source=-----9ade69f56296---------------------bookmark_footer-----------)![](../Images/4d98c180635de8d1ec4607a94fdfa029.png)'
- en: Photo by [Drew Saurus](https://unsplash.com/@drew_saurus?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Drew Saurus](https://unsplash.com/@drew_saurus?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: “To forgive is wisdom, to forget is genius. ”
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “宽恕是智慧，遗忘是天才。”
- en: ― **Joyce Cary**
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ― **乔伊斯·卡里**
- en: '[Large language models](https://en.wikipedia.org/wiki/Large_language_model)
    (LLMs) have taken the world by storm. In less than a year they are ubiquitous
    and are now used by millions of users. These models are often trained with huge
    amounts of text (including problematic material and sensitive data). How do you
    make a model forget? The same that could store the entirety of human knowledge?'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[大型语言模型](https://en.wikipedia.org/wiki/Large_language_model)（LLMs）已经引起了全球的关注。在不到一年的时间里，它们变得无处不在，现在被数以百万计的用户使用。这些模型通常用大量文本（包括有问题的材料和敏感数据）进行训练。如何让一个能够存储整个人类知识的模型忘记信息呢？'
- en: To learn how to forget
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学会如何忘记
- en: '![](../Images/c7c7cf08a9381a67f4ce29c3f641e677.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7c7cf08a9381a67f4ce29c3f641e677.png)'
- en: Photo by [Paul Pastourmatzis](https://unsplash.com/@pueblovista?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Paul Pastourmatzis](https://unsplash.com/@pueblovista?utm_source=medium&utm_medium=referral)
    提供，来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: LLMs stand as a testament to both our accomplishments and the challenges that
    lie ahead — [source](https://arxiv.org/pdf/2310.02238.pdf)
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）不仅体现了我们的成就，也展示了面临的挑战 — [来源](https://arxiv.org/pdf/2310.02238.pdf)
- en: LLMs have surprised both users and researchers with their ability to learn from
    huge amounts of text and identify language patterns and cultural nuances. While
    they could be the basis for a new application and scientific…
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 以其从大量文本中学习并识别语言模式和文化细微差别的能力，令用户和研究人员都感到惊讶。虽然它们可能成为新应用和科学研究的基础，但...
