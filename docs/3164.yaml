- en: Reshaping the Model’s Memory without the Need for Retraining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/reshaping-the-models-memory-without-the-need-for-retraining-9ade69f56296?source=collection_archive---------5-----------------------#2023-10-20](https://towardsdatascience.com/reshaping-the-models-memory-without-the-need-for-retraining-9ade69f56296?source=collection_archive---------5-----------------------#2023-10-20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '| AI | LARGE LANGUAGE MODELS| MACHINE UNLEARNING|'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Erasing any echo of problematic content a large language model has learned
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://salvatore-raieli.medium.com/?source=post_page-----9ade69f56296--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----9ade69f56296--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9ade69f56296--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9ade69f56296--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----9ade69f56296--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff1a08d9452cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freshaping-the-models-memory-without-the-need-for-retraining-9ade69f56296&user=Salvatore+Raieli&userId=f1a08d9452cd&source=post_page-f1a08d9452cd----9ade69f56296---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9ade69f56296--------------------------------)
    ·11 min read·Oct 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9ade69f56296&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freshaping-the-models-memory-without-the-need-for-retraining-9ade69f56296&user=Salvatore+Raieli&userId=f1a08d9452cd&source=-----9ade69f56296---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9ade69f56296&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freshaping-the-models-memory-without-the-need-for-retraining-9ade69f56296&source=-----9ade69f56296---------------------bookmark_footer-----------)![](../Images/4d98c180635de8d1ec4607a94fdfa029.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Drew Saurus](https://unsplash.com/@drew_saurus?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: “To forgive is wisdom, to forget is genius. ”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ― **Joyce Cary**
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Large language models](https://en.wikipedia.org/wiki/Large_language_model)
    (LLMs) have taken the world by storm. In less than a year they are ubiquitous
    and are now used by millions of users. These models are often trained with huge
    amounts of text (including problematic material and sensitive data). How do you
    make a model forget? The same that could store the entirety of human knowledge?'
  prefs: []
  type: TYPE_NORMAL
- en: To learn how to forget
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/c7c7cf08a9381a67f4ce29c3f641e677.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Paul Pastourmatzis](https://unsplash.com/@pueblovista?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: LLMs stand as a testament to both our accomplishments and the challenges that
    lie ahead — [source](https://arxiv.org/pdf/2310.02238.pdf)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: LLMs have surprised both users and researchers with their ability to learn from
    huge amounts of text and identify language patterns and cultural nuances. While
    they could be the basis for a new application and scientific…
  prefs: []
  type: TYPE_NORMAL
