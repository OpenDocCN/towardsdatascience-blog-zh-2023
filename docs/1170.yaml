- en: When the dataset is small, features are your friends
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/when-the-dataset-is-small-features-are-your-friends-6e7f8dcc819e?source=collection_archive---------9-----------------------#2023-04-03](https://towardsdatascience.com/when-the-dataset-is-small-features-are-your-friends-6e7f8dcc819e?source=collection_archive---------9-----------------------#2023-04-03)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Feature engineering can compensate for the lack of data.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@aimagefrombydgoszcz?source=post_page-----6e7f8dcc819e--------------------------------)[![Krzysztof
    Pałczyński](../Images/0cce629a7cefc6c33e2759bb8a68b123.png)](https://medium.com/@aimagefrombydgoszcz?source=post_page-----6e7f8dcc819e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6e7f8dcc819e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6e7f8dcc819e--------------------------------)
    [Krzysztof Pałczyński](https://medium.com/@aimagefrombydgoszcz?source=post_page-----6e7f8dcc819e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7c74555dd91c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhen-the-dataset-is-small-features-are-your-friends-6e7f8dcc819e&user=Krzysztof+Pa%C5%82czy%C5%84ski&userId=7c74555dd91c&source=post_page-7c74555dd91c----6e7f8dcc819e---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6e7f8dcc819e--------------------------------)
    ·7 min read·Apr 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6e7f8dcc819e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhen-the-dataset-is-small-features-are-your-friends-6e7f8dcc819e&user=Krzysztof+Pa%C5%82czy%C5%84ski&userId=7c74555dd91c&source=-----6e7f8dcc819e---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6e7f8dcc819e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhen-the-dataset-is-small-features-are-your-friends-6e7f8dcc819e&source=-----6e7f8dcc819e---------------------bookmark_footer-----------)![](../Images/d18793d4116fea77e67b9b3e75e2180f.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Thomas T](https://unsplash.com/@pyssling240?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/OPpCbAAKWv8?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: In the rapidly evolving world of Artificial Intelligence (AI), data has become
    the lifeblood of countless innovative applications and solutions. Indeed, large
    datasets are often considered the backbone of robust and accurate AI models. However,
    what happens when the dataset at hand is relatively small? In this article, we
    explore the critical role of feature engineering in overcoming the limitations
    posed by small datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '**Toy dataset**'
  prefs: []
  type: TYPE_NORMAL
- en: Our journey starts with the creation of the dataset. In this example, we will
    perform nice and easy signal classification. The dataset has two classes; sine
    waves of frequency 1 belong to class 0, and sine waves of frequency 2 belong to
    class 1\. The code for signal generation is presented below. The code generates
    a sine wave, applies additive gaussian noise, and randomizes phase shift. Due
    to the addition of noise and phase shift, we obtain diverse signals, and the classification
    problem becomes non-trivial (albeit still easy with correct feature engineering).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a6fe3838540861160fcc95370a509e39.png)![](../Images/ec78692e74973b00b2092b8b00433287.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualizations of signals in class 0.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/00e0f11732157e1356ae7b76c589e818.png)![](../Images/2631abaaeb2b6811dbb49d5f04821b2f.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualizations of signals in class 1
  prefs: []
  type: TYPE_NORMAL
- en: '**Deep Learning performance**'
  prefs: []
  type: TYPE_NORMAL
- en: State-of-The-Art models for signal processing are Convolutional Neural Networks
    (CNN). So, let’s create one. This particular network contains two one-dimensional
    convolutional layers and two fully connected ones. The code is listed below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'CNNs are models that can process the raw signal. However, due to its parameter-heavy
    architecture, they tend to need a lot of data. However, in the beginning, let’s
    assume we have enough data to train neural networks. I used signal generation
    to create a dataset with 200 signals. Each experiment was repeated ten times to
    reduce the interference of random variables. The code is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: CNNs obtained a test accuracy of 99.2%, it was to be expected for the State-of-The-Art
    model. However, this metric was obtained for these experiment runs, where training
    was successful. By “successful,” I mean that accuracy on the training dataset
    exceeded 60%. In this example, CNNs weights initialization is a make-or-break
    for training, and it sometimes happens, as CNNs are complicated models prone to
    problems with unfortunate randomized weights initialization. The success rate
    of training was 70%.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s see what happens when the dataset is short. I reduced amount of signals
    in the dataset to 20\. As a result, CNNs obtained 71.4% test accuracy, and the
    accuracy dropped by 27.8 percentage points. That is not acceptable. Nonetheless,
    what to do now? The dataset needs to be longer to use State-of-The-Art models.
    In industrial applications, acquiring more data is either unfeasible or, at the
    very least, very expensive. Should we drop the project and move on?
  prefs: []
  type: TYPE_NORMAL
- en: No. When the dataset is small, features are your friends.
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature Engineering**'
  prefs: []
  type: TYPE_NORMAL
- en: This particular example involves the classification of signals based on their
    frequency. So, we can apply the good old Fourier Transform. The Fourier Transform
    decomposes the signal into a series of sine waves parametrized by frequency and
    amplitude. As a result, we can use Fourier Transform to examine the importance
    of each frequency in forming the signal. Such data representation should simplify
    the task enough for the small dataset to suffice. Also, Fourier Transform structures
    the data so that we can use simpler models like, for example, the Random Forest
    classifier.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c989e6107d8239d3eeece4087683c300.png)![](../Images/586366bcd10b608ebfbcccea868904e8.png)'
  prefs: []
  type: TYPE_IMG
- en: The visualization of signals transformed into spectrums. On the left is the
    spectrum of the signal from class 0, and on the right is the spectrum of the signal
    from class 1\. These plots have logarithmic scales for better visibility. The
    models used in this example interpreted signals on a linear scale.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for transforming the signal and training Random Forest Classifier
    is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The Random Forest classifier achieved 100% test accuracy on 20 and 200 signals-long
    datasets, and the training success rate is also 100% for each dataset. As a result,
    we obtained even better results than CNNs with a smaller amount of data required
    — all thanks to feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: '**Risk of overfitting**'
  prefs: []
  type: TYPE_NORMAL
- en: Although feature engineering is a powerful tool, one must also remember to reduce
    unnecessary features from the input data. The more features are in input vectors,
    the higher the chance of overfitting — especially in small datasets. Each unnecessary
    feature provides the risk of introducing random fluctuations that the machine
    learning model may consider important patterns. The less data in the dataset,
    the higher the risk of random fluctuations, creating a correlation that doesn’t
    exist in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: One of the mechanisms that may help in pruning too large feature collections
    are search heuristics like the genetic algorithm. The features pruning can be
    expressed as a task to find the smallest amount of features that facilitate successful
    training of the machine-learning model. It can be encoded by creating a binary
    vector of length equal to the size of feature data. The “0” determines that the
    feature is not present in the dataset, and the “1” indicates that feature is present.
    Then the fitness function of such a vector is a summation of the machine-learning
    model’s accuracy achieved on the pruned dataset and the count of zeros in the
    vector scaled down by sufficient weight.
  prefs: []
  type: TYPE_NORMAL
- en: This is only one of many solutions to remove unnecessary features. However,
    it is quite powerful.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion**'
  prefs: []
  type: TYPE_NORMAL
- en: Although the example presented is relatively simple, it presents typical problems
    with applying Artificial Intelligence systems in the industry. Currently, Deep
    Neural Networks can do almost everything we desire on condition of providing enough
    data. However, the data is usually scarce and expensive. So, industrial applications
    of Artificial Intelligence usually involve doing extensive features engineering
    to simplify the problem and, as a result, reduce the amount of data needed to
    train the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thanks for reading. The code for this example generation is accessible under
    the link: [https://github.com/aimagefrombydgoszcz/Notebooks/blob/main/when_dataset_is_small_features_are_your_friend.ipynb](https://github.com/aimagefrombydgoszcz/Notebooks/blob/main/when_dataset_is_small_features_are_your_friend.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: All images unless otherwise noted are by the author.
  prefs: []
  type: TYPE_NORMAL
