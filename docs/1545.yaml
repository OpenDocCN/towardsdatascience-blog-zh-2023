- en: 'The Art of Prompt Design: Prompt Boundaries and Token Healing'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-art-of-prompt-design-prompt-boundaries-and-token-healing-3b2448b0be38?source=collection_archive---------2-----------------------#2023-05-08](https://towardsdatascience.com/the-art-of-prompt-design-prompt-boundaries-and-token-healing-3b2448b0be38?source=collection_archive---------2-----------------------#2023-05-08)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@scottmlundberg?source=post_page-----3b2448b0be38--------------------------------)[![Scott
    Lundberg](../Images/99f1c984f0aaabfe4e348a92fa50a1ee.png)](https://medium.com/@scottmlundberg?source=post_page-----3b2448b0be38--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3b2448b0be38--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3b2448b0be38--------------------------------)
    [Scott Lundberg](https://medium.com/@scottmlundberg?source=post_page-----3b2448b0be38--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3a739af9ef3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-art-of-prompt-design-prompt-boundaries-and-token-healing-3b2448b0be38&user=Scott+Lundberg&userId=3a739af9ef3a&source=post_page-3a739af9ef3a----3b2448b0be38---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3b2448b0be38--------------------------------)
    ·7 min read·May 8, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3b2448b0be38&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-art-of-prompt-design-prompt-boundaries-and-token-healing-3b2448b0be38&user=Scott+Lundberg&userId=3a739af9ef3a&source=-----3b2448b0be38---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3b2448b0be38&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-art-of-prompt-design-prompt-boundaries-and-token-healing-3b2448b0be38&source=-----3b2448b0be38---------------------bookmark_footer-----------)![](../Images/7da96052b147da9162ebeb9229095ca5.png)'
  prefs: []
  type: TYPE_NORMAL
- en: All images are original creations.
  prefs: []
  type: TYPE_NORMAL
- en: This (written jointly with [Marco Tulio Ribeiro](https://medium.com/@marcotcr))
    is part 2 of a series on **the art of prompt design** (part 1 [here](https://medium.com/towards-data-science/the-art-of-prompt-design-use-clear-syntax-4fc846c1ebd5)),
    where we talk about controlling large language models (LLMs) with `[guidance](https://github.com/microsoft/guidance)`.
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we’ll discuss how the greedy/optimized tokenization methods used
    by language models can introduce a subtle and powerful bias into your prompts,
    leading to puzzling generations.
  prefs: []
  type: TYPE_NORMAL
- en: Language models are not trained on raw text, but rather on tokens, which are
    chunks of text that often occur together, similar to words. This impacts how language
    models ‘see’ text, including prompts (since prompts are just sets of tokens).
    GPT-style models utilize tokenization methods like [Byte Pair Encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding)
    (BPE), which map all input bytes to token ids in a greedy manner. This is fine
    for training, but it can lead to subtle issues during inference, as shown in the
    example below.
  prefs: []
  type: TYPE_NORMAL
- en: '**An example of a prompt boundary problem**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consider the following example, where we are trying to generate an HTTP URL
    string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9f08089bdd4af8fb7a57582e30d16f31.png)'
  prefs: []
  type: TYPE_IMG
- en: Notebook output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the output generated by the LLM does not complete the url with the
    obvious next characters (two forward slashes). It instead creates an invalid URL
    string with a space in the middle. This is surprising, because the `//` completion
    is extremely obvious after `http:`. To understand why this happens, let’s change
    our prompt boundary so that our prompt does not include the colon character:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9a83d183032d3ab835c0a1d65b13dac6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now the language model generates a valid url string like we expect. To understand
    why the `:` matters, we need to look at the tokenized representation of the prompts.
    Below is the tokenization of the prompt that ends in a colon (the prompt without
    the colon has the same tokenization, except for the last token):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/06ebdcbaf90a7dc92b1e504e2829498b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now note what the tokenization of a valid URL looks like, paying careful attention
    to token `1358`, right after `http`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a3c2d8e91a5a4fdc91400781b851273d.png)'
  prefs: []
  type: TYPE_IMG
- en: Most LLMs (including this one) use a greedy tokenization method, always preferring
    the longest possible token, i.e. `://` will always be preferred over `:` in full
    text (e.g. in training).
  prefs: []
  type: TYPE_NORMAL
- en: While URLs in training are encoded with token 1358 (`://`), our prompt makes
    the LLM see token `27` (`:`) instead, which throws off completion by artificially
    splitting `://`.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the model can be pretty sure that seeing token `27` (`:`) means what
    comes next is very unlikely to be anything that could have been encoded together
    with the colon using a “longer token” like `://`, since in the model’s training
    data those characters would have been encoded together with the colon (an exception
    to this that we will discuss later is subword regularization during training).
    The fact that seeing a token means both seeing the embedding of that token **and
    also** that whatever comes next wasn’t compressed by the greedy tokenizer is easy
    to forget, but it is important in prompt boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s search over the string representation of all the tokens in the model’s
    vocabulary, to see which ones start with a colon:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/bba5fa5bd4edeccb89dafdecbea690df.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that there are **34** different tokens starting with a colon, and thus
    ending a prompt with a colon means the model will likely not generate completions
    with any of these 34 token strings. *This subtle and powerful bias can have all
    kinds of unintended consequences.* And this applies to **any** string that could
    be potentially extended to make a longer single token (not just `:`). Even our
    “fixed” prompt ending with “http” has a built in bias as well, as it communicates
    to the model that what comes after “http” is likely not “s” (otherwise “http”
    would not have been encoded as a separate token):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5722e1eed5f87ab98014368cd2e2dbce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Lest you think this is an arcane problem that only touches URLs, remember that
    most tokenizers treat tokens differently depending on whether they start with
    a space, punctuation, quotes, etc, and thus **ending a prompt with any of these
    can lead to wrong token boundaries**, and break things:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/fab000c5aac694ca330aa3f9f98055a0.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c4b67b763845aed5ce9f75a575cefd3c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Another example of this is the “[“ character. Consider the following prompt
    and completion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2e193cde149e2a761afee84cf6cae3b6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Why is the second string not quoted? Because by ending our prompt with the
    “ `[`” token, we are telling the model that it should not generate completions
    that match the following 27 longer tokens (one of which adds the quote character,
    `15640`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/8d0086e91c6df84c0bcdd79736f6f867.png)'
  prefs: []
  type: TYPE_IMG
- en: Token boundary bias happens everywhere. About *70% of the 10k most-common tokens
    for the StableLM model used above are prefixes of longer possible tokens, and
    so cause token boundary bias when they are the last token in a prompt.*
  prefs: []
  type: TYPE_NORMAL
- en: '**Fixing unintended bias with “token healing”**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What can we do to avoid these unintended biases? One option is to always end
    our prompts with tokens that cannot be extended into longer tokens (for example
    a role tag for chat-based models), but this is a severe limitation.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, `guidance` has a feature called “token healing”, which automatically
    backs up the generation process by one token before the end of the prompt, then
    constrains the first token generated to have a prefix that matches the last token
    in the prompt. In our URL example, this would mean removing the `:`, and forcing
    generation of the first token to have a `:` prefix. Token healing allows users
    to express prompts however they wish, without worrying about token boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s re-run some of the URL examples above with token healing
    turned on (it’s on by default for Transformer models, so we remove `token_healing=False`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/076ec3d9d9243b506a86a061a6e4e78f.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/143c104e36c53354f3e659a68d58613f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, we don’t have to worry about extra spaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e217a0e6ec54dca35357504011974386.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/0f2d372e350aab9997ee5e61dbd818f9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And we now get quoted strings even when the prompt ends with a “ `[`” token:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/7aa6d716e7d702cae0a8567009775452.png)'
  prefs: []
  type: TYPE_IMG
- en: '**What about subword regularization?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are familiar with how language models are trained, you may be wondering
    how [subword regularization](https://arxiv.org/abs/1804.10959) fits into all this.
    Subword regularization is a technique where during training sub-optimal tokenizations
    are randomly introduced to increase the model’s robustness. This means that the
    model does not always see the best greedy tokenization. Subword regularization
    is great at helping the model be more robust to token boundaries, but it does
    not altogether remove the bias that the model has towards the standard greedy/optimized
    tokenization. This means that while depending on the amount of subword regularization
    during training models may exhibit more or less token boundaries bias, all models
    still have this bias. And as shown above it can still have a powerful and unexpected
    impact on the model output.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you write prompts, remember that greedy tokenization can have a significant
    impact on how language models interpret your prompts, particularly when the prompt
    ends with a token that could be extended into a longer token. This easy-to-miss
    source of bias can impact your results in surprising and unintended ways.
  prefs: []
  type: TYPE_NORMAL
- en: To address to this, either end your prompt with a non-extendable token, or use
    something like `guidance`’s “token healing” feature so you can to express your
    prompts however you wish, without worrying about token boundary artifacts.
  prefs: []
  type: TYPE_NORMAL
- en: '*To reproduce the results in this article yourself check out the* [*notebook*](https://github.com/microsoft/guidance/blob/main/notebooks/art_of_prompt_design/prompt_boundaries_and_token_healing.ipynb)
    *version.*'
  prefs: []
  type: TYPE_NORMAL
