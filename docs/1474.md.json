["```py\nimport os\nimport re\nfrom atlassian import Confluence\nfrom bs4 import BeautifulSoup\n\n# Set up Confluence API client\nconfluence = Confluence(\n    url='YOUR_CONFLUENCE URL',\n    username=\"YOUR_EMAIL\",\n    password=\"YOUR_API_KEY\",\n    cloud=True)\n\n# Replace SPACE_KEY with the desired Confluence space key\nspace_key = 'YOUR_SPACE'\n\ndef get_all_pages_from_space_with_pagination(space_key):\n    limit = 50\n    start = 0\n    all_pages = []\n\n    while True:\n        pages = confluence.get_all_pages_from_space(space_key, start=start, limit=limit)\n        if not pages:\n            break\n\n        all_pages.extend(pages)\n        start += limit\n\n    return all_pages\n\npages = get_all_pages_from_space_with_pagination(space_key)\n```", "```py\n# Function to sanitize filenames\ndef sanitize_filename(filename):\n    return \"\".join(c for c in filename if c.isalnum() or c in (' ', '.', '-', '_')).rstrip()\n\n# Create a directory for the text files if it doesn't exist\nif not os.path.exists('txt_files'):\n   os.makedirs('txt_files')\n\n# Extract pages and save to individual text files\nfor page in pages:\n   page_id = page['id']\n   page_title = page['title']\n\n   # Fetch the page content\n   page_content = confluence.get_page_by_id(page_id, expand='body.storage')\n\n   # Extract the content in the \"storage\" format\n   storage_value = page_content['body']['storage']['value']\n\n   # Clean the HTML tags to get the text content\n   text_content = process_html_document(storage_value)\n   file_name = f'txt_files/{sanitize_filename(page_title)}_{page_id}.txt'\n   with open(file_name, 'w', encoding='utf-8') as txtfile:\n       txtfile.write(text_content)\n```", "```py\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef html_table_to_text(html_table):\n    soup = BeautifulSoup(html_table, \"html.parser\")\n\n    # Extract table rows\n    rows = soup.find_all(\"tr\")\n\n    # Determine if the table has headers or not\n    has_headers = any(th for th in soup.find_all(\"th\"))\n\n    # Extract table headers, either from the first row or from the <th> elements\n    if has_headers:\n        headers = [th.get_text(strip=True) for th in soup.find_all(\"th\")]\n        row_start_index = 1  # Skip the first row, as it contains headers\n    else:\n        first_row = rows[0]\n        headers = [cell.get_text(strip=True) for cell in first_row.find_all(\"td\")]\n        row_start_index = 1\n\n    # Iterate through rows and cells, and use NLP to generate sentences\n    text_rows = []\n    for row in rows[row_start_index:]:\n        cells = row.find_all(\"td\")\n        cell_sentences = []\n        for header, cell in zip(headers, cells):\n            # Generate a sentence using the header and cell value\n            doc = nlp(f\"{header}: {cell.get_text(strip=True)}\")\n            sentence = \" \".join([token.text for token in doc if not token.is_stop])\n            cell_sentences.append(sentence)\n\n        # Combine cell sentences into a single row text\n        row_text = \", \".join(cell_sentences)\n        text_rows.append(row_text)\n\n    # Combine row texts into a single text\n    text = \"\\n\\n\".join(text_rows)\n    return text\n\ndef html_list_to_text(html_list):\n    soup = BeautifulSoup(html_list, \"html.parser\")\n    items = soup.find_all(\"li\")\n    text_items = []\n    for item in items:\n        item_text = item.get_text(strip=True)\n        text_items.append(f\"- {item_text}\")\n    text = \"\\n\".join(text_items)\n    return text\n\ndef process_html_document(html_document):\n    soup = BeautifulSoup(html_document, \"html.parser\")\n\n    # Replace tables with text using html_table_to_text\n    for table in soup.find_all(\"table\"):\n        table_text = html_table_to_text(str(table))\n        table.replace_with(BeautifulSoup(table_text, \"html.parser\"))\n\n    # Replace lists with text using html_list_to_text\n    for ul in soup.find_all(\"ul\"):\n        ul_text = html_list_to_text(str(ul))\n        ul.replace_with(BeautifulSoup(ul_text, \"html.parser\"))\n\n    for ol in soup.find_all(\"ol\"):\n        ol_text = html_list_to_text(str(ol))\n        ol.replace_with(BeautifulSoup(ol_text, \"html.parser\"))\n\n    # Replace all types of <br> with newlines\n    br_tags = re.compile('<br>|<br/>|<br />')\n    html_with_newlines = br_tags.sub('\\n', str(soup))\n\n    # Strip remaining HTML tags to isolate the text\n    soup_with_newlines = BeautifulSoup(html_with_newlines, \"html.parser\")\n\n    return soup_with_newlines.get_text()\n```", "```py\nimport numpy as np\nfrom bertopic import BERTopic\nfrom bertopic.vectorizers import ClassTfidfTransformer\nfrom bertopic.representation import MaximalMarginalRelevance\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer_model = CountVectorizer(stop_words=\"english\")\nrepresentation_model = MaximalMarginalRelevance(diversity=0.2)\nctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n\n# Collect text and filenames from chunks in the txt_files directory\ndocuments = []\nfilenames = []\n\nfor file in os.listdir('txt_files'):\n    if file.endswith('.txt'):\n        with open(os.path.join('txt_files', file), 'r', encoding='utf-8') as f:\n            documents.append(f.read())\n            filenames.append(file)\n```", "```py\ndef extract_topics(docs, n_topics):\n    model = BERTopic(nr_topics=n_topics, calculate_probabilities=True, language=\"english\",\n                     ctfidf_model=ctfidf_model, representation_model=representation_model, \n                     vectorizer_model=vectorizer_model)\n    topics, probabilities = model.fit_transform(docs)\n    return model, topics, probabilities\n\ndef find_outlier_topic(model):\n    topic_sizes = model.get_topic_freq()\n    outlier_topic = topic_sizes.iloc[-1][\"Topic\"]\n    return outlier_topic\n\noutlier_counts = np.zeros(len(documents))\noutlier_probs = np.zeros(len(documents))\n\n# Define the range of topics you want to try\nmin_topics = 5\nmax_topics = 10\n\nfor n_topics in range(min_topics, max_topics + 1):\n    model, topics, probabilities = extract_topics(documents, n_topics)\n    outlier_topic = find_outlier_topic(model)\n\n    for i, (topic, prob) in enumerate(zip(topics, probabilities)):\n        if topic == outlier_topic:\n            outlier_counts[i] += 1\n            outlier_probs[i] += prob[outlier_topic]\n```", "```py\ndef normalize(arr):\n    min_val, max_val = np.min(arr), np.max(arr)\n    return (arr - min_val) / (max_val - min_val)\n\n# Average the probabilities\navg_outlier_probs = np.divide(outlier_probs, outlier_counts, out=np.zeros_like(outlier_probs), where=outlier_counts != 0)\n\n# Normalize counts \nnormalized_counts = normalize(outlier_counts)\n\n# Compute the combined unrelatedness score by averaging the normalized counts and probabilities\nunrelatedness_scores = [(i, (count + prob) / 2) for i, (count, prob) in enumerate(zip(normalized_counts, avg_outlier_probs))]\nunrelatedness_scores.sort(key=lambda x: x[1], reverse=True)\n\n# Print the filtered results\nfor index, score in unrelatedness_scores:\n    if score > 0:\n        title = filenames[index]\n        preview = documents[index][:100] + \"...\" if len(documents[index]) > 100 else documents[index]\n        print(f\"Title: {title}, Preview: {preview}, Unrelatedness: {score:.2f}\")\n        print(\"\\n\")\n```"]