# XGBoost：深度学习如何替代梯度提升和决策树 — 第二部分：训练

> 原文：[`towardsdatascience.com/xgboost-how-deep-learning-can-replace-gradient-boosting-and-decision-trees-part-2-training-b432620750f8`](https://towardsdatascience.com/xgboost-how-deep-learning-can-replace-gradient-boosting-and-decision-trees-part-2-training-b432620750f8)

[](https://medium.com/@guillaume.saupin?source=post_page-----b432620750f8--------------------------------)![Saupin Guillaume](https://medium.com/@guillaume.saupin?source=post_page-----b432620750f8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b432620750f8--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page-----b432620750f8--------------------------------) [Saupin Guillaume](https://medium.com/@guillaume.saupin?source=post_page-----b432620750f8--------------------------------)

·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----b432620750f8--------------------------------) ·阅读时间 6 分钟·2023 年 9 月 21 日

--

![](img/528a94d279856509bbfad323f8112359.png)

图片由[Simon Wilkes](https://unsplash.com/@simonfromengland?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

# 一个没有*if*的世界

在上一篇文章中：

[](/xgboost-how-deep-learning-can-replace-gradient-boosting-and-decision-trees-291dc9365656?source=post_page-----b432620750f8--------------------------------) ## XGBoost：深度学习如何替代梯度提升和决策树 — 第一部分

### 在这篇文章中，你将了解如何使用可微编程方法重写决策树，如提议的…

towardsdatascience.com

你已经了解了如何使用可微编程方法重写决策树，这一方法由 NODE 论文提出。这篇论文的理念是用神经网络替代 XGBoost。

更具体地说，在解释了为什么构建决策树的过程不可微之后，介绍了用于正则化与决策节点相关的两个主要元素所需的数学工具：

+   特征选择

+   分支检测

NODE 论文表明，这两者都可以通过 entmax 函数来处理。

总结一下，我们展示了如何创建一个**不使用比较运算符**的二叉树。

之前的文章以关于训练正则化决策树的开放问题结尾。现在是时候回答这些问题了。

如果你对梯度提升方法感兴趣，可以看看我的书：

[## 实用梯度提升：深入探讨 Python 中的梯度提升](https://amzn.to/3EKdKsC?source=post_page-----b432620750f8--------------------------------)

### 这本关于梯度提升方法的书籍是为那些希望深入了解此方法的学生、学术人士、工程师和数据科学家们准备的……

[amzn.to](https://amzn.to/3EKdKsC?source=post_page-----b432620750f8--------------------------------)

# 平滑决策节点

首先，基于我们在上一篇文章中介绍的内容，我们创建一个新的 Python 类：`SmoothBinaryNode`。

这个类编码了平滑二叉节点的行为。其代码中有两个关键部分：

+   特征的选择，由函数`_choices`处理

+   这些特征的评估、相对于给定的阈值的评估，以及路径选择：`left`还是`right`。所有这些都由`left`和`right`方法管理。

平滑二叉节点。代码由作者提供。

正如在上一篇文章中所解释的，规范化一个二叉节点（从而允许其训练）的关键是使用`entmax`函数。

# 全能的点积

请注意，特征选择以及左分支和右分支的选择都是通过`点积`完成的。点积是一个简单的操作，只要只考虑其实现，但实际上它非常强大。

它可以用于做投影、计算`cosinus`、找到光线和三角形之间的交点……但这另当别论。

# 一个简单的二叉节点

让我们用几行代码看看如何在一个只有一个节点的非常基础的二叉树上使用这个新类：

使用平滑二叉节点。代码由作者提供。

在这个片段中，我们配置了我们的节点，使得其左侧包含一个`1`，右侧也包含一个`1`。这由参数`leaves`定义。我们还将阈值设置为 50，如参数`biais`中所示。最后，特征选择是通过`weights`定义的权重完成的。

这是另一个稍微复杂一点的示例，有一个两层的树：

2 层二叉树。代码由作者提供。

原则与之前的示例类似，只是根节点添加了 2 个节点。

# 需要学习的参数

在之前的示例中，我们手动定义了三个参数：

+   **权重**用于选择特征。借助`entmax`函数，权重最高的特征将被选择。

+   一旦特征被选择，我们需要找到最佳的**阈值**来将数据分成两个子集。

+   最后，我们必须定义树叶子上附加的值，这里是参数`leaves`。

这些是在训练过程中将被学习的参数。

# 定义目标

和往常一样，在进行机器学习时，目标是找到最小化某些成本函数的参数组合。

当前最常用的是`均方误差`，它计算起来相当简单，可以在 Python 中如下编写：

均方误差。代码由作者提供。

请注意，以上代码中有一个小的微妙之处，因为`mse`函数创建了一个闭包，捕获了树对象。

我们可以将此函数应用于下面定义的简单单层树：

误差如预期般为零。代码由作者提供。

如预期般，误差为零。

# 梯度下降学习

如果有一种数学工具与机器学习相关，那就是梯度下降。深度学习、简单的神经网络，甚至梯度提升（但在功能空间中）都使用梯度下降来最小化某种成本函数以训练模型。

这一过程的主要困难是计算复杂函数的梯度，该函数是线性函数（层输入）和非线性函数（激活函数）的数学组合。

尽管这种困难阻碍了神经网络方法的成功已有几十年，但基础的数学原理——微分规则——早已为人所知。

如今，计算复杂函数的梯度，这些函数由许多线性和非线性基础函数组合而成，可以通过一行代码简单**且**高效地完成，使用自动微分。

如您在`SmoothBinaryNode`类的代码中看到的，我们已将这 3 个参数`weights, bias, leaves`隔离到变量`params`中。

使用梯度下降法是优化参数以最小化误差的标准方法。

自动微分和梯度下降是我在我的书***70 个数学概念***中探讨的概念：

[](https://amzn.to/3ZpI8lm?source=post_page-----b432620750f8--------------------------------) [## 揭示 Python 中的 70 个数学概念：通过 Python 探索数学的实用指南

### 购买《揭示 Python 中的 70 个数学概念：通过 Python 探索数学的实用指南》…

amzn.to](https://amzn.to/3ZpI8lm?source=post_page-----b432620750f8--------------------------------)

正如其名称所示，梯度下降要求计算误差函数的梯度，相对于我们要优化的参数。

多亏了库`jax`，计算任何函数的梯度变得异常简单，使用`grad`函数。

Jax 使用`自动微分`来自动且高效地计算导数。如果你对这种引人入胜的方法感兴趣，可以查看我关于该主题的介绍文章：

[](/differentiable-programming-from-scratch-abba0ebebc1c?source=post_page-----b432620750f8--------------------------------) ## 从零开始的可微分编程

### 去年我有幸参加了 Yann Lecun 在 Facebook 人工智能会议上的演讲…

towardsdatascience.com

实现它的一种方法在这段代码中展示了：

使用梯度下降法训练平滑二叉节点。代码由作者提供。

这段代码首先使用库`jax`计算误差的梯度，以获得一个最优的参数集，即误差为零的参数集。因此，我们可以确保在这种情况下梯度确实为零。

然后我们稍微扰动`leaves`参数，并确保该参数的方向上的梯度非零。希望是这样。同样，修改`bias`参数时也是如此。

最后，从一个扰动过的、非最优的参数集开始，我们使用梯度下降法来迭代更新这些参数。

在经过任意 1000 次迭代后，参数会收敛到另一个最小化误差的参数集。

# 梯度消失

我必须坦白，在上面的例子中我有点作弊。我故意选择了误差函数中的非平坦区域的参数。

如果你记得`entmax`函数的形状，当使用高`alpha`时，从 0 到 1 的过渡是非常陡峭的。这意味着对于任何稍微偏离 0 的值，函数的曲线都是完全平坦的。

由于梯度定义上是曲线的斜率，因此在这个区域梯度为零。

由于梯度下降法通过添加一个小的增量（即`learning rate`和梯度的乘积）来更新参数，因此优化会失败。

避免这种烦人的限制的一种方法是执行批量归一化，以确保特征保持在`entmax`函数不平坦的区域。

![](https://www.buymeacoffee.com/guillaumes0)

[`www.buymeacoffee.com/guillaumes0`](https://www.buymeacoffee.com/guillaumes0)

# 结论

在这两篇文章系列中，我们已经看到如何对决策树进行正则化。我们展示了如何用`entmax`函数和`dot product`替换特征选择和分支选择。

这种正则化允许在`可微编程`的数学框架中使用`平滑决策树`。

能够使用这种形式主义是非常强大的，因为它允许我们将任何类型的神经网络（一个复杂的可微函数）与决策树混合使用。
