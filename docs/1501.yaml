- en: Automatically Managing Data Pipeline Infrastructures With Terraform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/automatically-managing-data-pipeline-infrastructures-with-terraform-323fd1808a47?source=collection_archive---------9-----------------------#2023-05-02](https://towardsdatascience.com/automatically-managing-data-pipeline-infrastructures-with-terraform-323fd1808a47?source=collection_archive---------9-----------------------#2023-05-02)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*I know the manual work you did last summer*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://joaopedro214.medium.com/?source=post_page-----323fd1808a47--------------------------------)[![João
    Pedro](../Images/64a0e14527be213e5fde0a02439fbfa7.png)](https://joaopedro214.medium.com/?source=post_page-----323fd1808a47--------------------------------)[](https://towardsdatascience.com/?source=post_page-----323fd1808a47--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----323fd1808a47--------------------------------)
    [João Pedro](https://joaopedro214.medium.com/?source=post_page-----323fd1808a47--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb111eee95c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatically-managing-data-pipeline-infrastructures-with-terraform-323fd1808a47&user=Jo%C3%A3o+Pedro&userId=b111eee95c&source=post_page-b111eee95c----323fd1808a47---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----323fd1808a47--------------------------------)
    ·15 min read·May 2, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F323fd1808a47&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatically-managing-data-pipeline-infrastructures-with-terraform-323fd1808a47&user=Jo%C3%A3o+Pedro&userId=b111eee95c&source=-----323fd1808a47---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F323fd1808a47&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatically-managing-data-pipeline-infrastructures-with-terraform-323fd1808a47&source=-----323fd1808a47---------------------bookmark_footer-----------)![](../Images/62fc203322b755adb5210f9bf185c849.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [EJ Yao](https://unsplash.com/fr/@hojipago?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A few weeks ago, I wrote a post about [developing a data pipeline using both
    on-premise and AWS tools](https://medium.com/towards-data-science/data-pipeline-with-airflow-and-aws-tools-s3-lambda-glue-18585d269761).
    This post is part of my recent effort in bringing more cloud-oriented data engineering
    posts.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, when mentally reviewing this post, I noticed a big problem: **the
    manual work**.'
  prefs: []
  type: TYPE_NORMAL
- en: Whenever I develop a new project, whether real or fictional, I always try to
    reduce the friction of configuring the environment (install dependencies, configure
    folders, obtain credentials, etc) and that’s why I always use Docker. With it,
    I just pass you a docker-compose.yaml file + a few Dockerfiles and you are capable
    of creating exactly the same environment as me with just one command — docker
    compose up.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, when we want to develop a new data project with cloud tools (S3, Lambda,
    Glue, EMR, etc) Docker can’t help us, as the components need to be instantiated
    in the providers’ infrastructure, and there are two main ways of doing this: Manually
    on the UI or programmatically through service APIs.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, you can access the AWS UI on your browser, search for S3 and create
    a new bucket manually, or write a code in Python to create this same instance
    making a request on the AWS API.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/556cd3cc054a57f1b8dce90fba077454.png)'
  prefs: []
  type: TYPE_IMG
- en: In the post mentioned earlier, I described step-by-step how to create the needed
    components **MANUALLY** through the AWS web interface. The result? Even trying
    to summarize as much as possible (and even omitting parts!), the post ended up
    with 17 min, 7 min more than I usually do, full of PRINTS of which screen you
    should access, where you should click, and which settings to choose.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to being a costly, confusing, and time-consuming process, it is
    still susceptible to human errors, which ends up bringing more headaches and possibly
    even bad surprises in the monthly bill. Definitely an unpleasant process.
  prefs: []
  type: TYPE_NORMAL
- en: And this is the exactly kind of problem that **Terraform** comes to solve.
  prefs: []
  type: TYPE_NORMAL
- en: not sponsored.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What is Terraform?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Terraform is an IaC (Infrastructure as Code) tool** that manages infrastructure
    in cloud providers in an automatic and programmatically manner.'
  prefs: []
  type: TYPE_NORMAL
- en: In Terraform, the desired infrastructures is described using a declarative language
    called HCL (HashiCorp Configuration Language), where the components are specified,
    *e.g.* a S3 bucket named “my-bucket” and an EC2 server with Ubuntu 22 in the us-east-1
    zone.
  prefs: []
  type: TYPE_NORMAL
- en: The described resources are materialized by Terraform through calls in the cloud
    provider’s service APIs. Beyond creation, it is also capable of destroying and
    updating the infrastructure, adding/removing only the resources needed to move
    from the actual state to the desired state, *e.g.* if 4 instances of EC2 are requested,
    it will create only 2 new instances if 2 others already exist. This behavior is
    achieved because Terraform stores the actual state of the infrastructure in state
    files.
  prefs: []
  type: TYPE_NORMAL
- en: Because of this, it's possible to manage a project’s infrastructure in a much
    more agile and secure way, as it removes the manual work needed of configuring
    each individual resource.
  prefs: []
  type: TYPE_NORMAL
- en: Terraform’s proposal is to be a cloud-agnostic IaC tool, so it uses a standardized
    language to mediate the interaction with the cloud providers’ APIs, removing the
    need of learning how to interact with them directly. Still in this line, HCL language
    also supports variables manipulation and a certain degree of ‘flux control’ (if-statements
    and loops), allowing the use of conditionals and loops in resource creation, e.g.
    create 100 EC2 instances.
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, Terraform also allows infrastructure versioning, as its
    plain-text files can be easily manipulated by git.
  prefs: []
  type: TYPE_NORMAL
- en: The implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned earlier, this post seeks to automate the process of infrastructure
    creation of my previous post.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9e4978df123ccfc0325b619453122674.png)'
  prefs: []
  type: TYPE_IMG
- en: To recap, the project developed aimed at creating a data pipeline to extract
    questions from the Brazillian ENEM (National Exam of High School, on literal translation)
    tests using the PDFs available on the MEC (Ministry of Education) [website](https://www.gov.br/inep/pt-br/areas-de-atuacao/avaliacao-e-exames-educacionais/enem/provas-e-gabaritos).
  prefs: []
  type: TYPE_NORMAL
- en: The process involved three steps, controlled by a local Airflow instance. These
    steps included downloading and uploading the PDF file to S3 storage, extracting
    texts from the PDFs using a Lambda function, and segmenting the extracted text
    into questions using a Glue Job.
  prefs: []
  type: TYPE_NORMAL
- en: Note that, for this pipeline to work, many AWS components have to be created
    and correctly configured.
  prefs: []
  type: TYPE_NORMAL
- en: 0\. Setting up the environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All the code used in this project is available in this [GitHub Repository](https://github.com/jaumpedro214/posts).
  prefs: []
  type: TYPE_NORMAL
- en: You’ll need a machine with Docker and an AWS account.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is configuring a new AWS IAM user for Terraform, this will be
    the only step executed in the AWS web console.
  prefs: []
  type: TYPE_NORMAL
- en: '**Create a new IAM user with FullAccess to S3, Glue, Lambda, and IAM and generate
    code credentials for it.**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/32738de4887661967fc6bd33ddd06e0e.png)'
  prefs: []
  type: TYPE_IMG
- en: This is a lot of permission for a single user, so keep the credentials safe.
  prefs: []
  type: TYPE_NORMAL
- en: I’m using FullAccess permissions because I wanna make things easier for now,
    but always consider the ‘least privileged’ approach when dealing with credentials.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Now, back to the local environment.
  prefs: []
  type: TYPE_NORMAL
- en: '**On the same path** as the *docker-compose.yaml* file, create a *.env* file
    and write your credentials:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: These variables will be passed to the docker-compose file to be used by Terraform.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 1\. Create the Terraform file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Still on the same folder create a new directory called **terraform**. Inside
    it, create a new file **main.tf**, this will be our main Terraform file.
  prefs: []
  type: TYPE_NORMAL
- en: This folder will be mapped inside the container when it runs, so the internal
    Terraform will be able to see this file.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Configure the AWS Provider
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first thing we need to do is to configure the cloud provider used.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This is what a Terraform configuration file looks like — a set of blocks with
    different types, each one with a specific function.
  prefs: []
  type: TYPE_NORMAL
- en: The **terraform** block fixes the versions for Terraform itself and for the
    AWS provider.
  prefs: []
  type: TYPE_NORMAL
- en: A **variable** is exactly what the name suggests — a value assigned to a name
    that can be referenced throughout the code.
  prefs: []
  type: TYPE_NORMAL
- en: As you probably already noticed, our variables don’t have a value assigned to
    them, so what’s going on? The answer is back in the docker-compose.yaml file,
    the value of these variables was set using environment variables in the system.
    When a variable value is not defined, Terraform will look at the value of the
    environment variable TF_VAR_<var_name> and use its value. I’ve opted for this
    approach to avoid hard-coding the keys.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The **provider** block is also self-explanatory — It references the cloud provider
    we’re using and configures its credentials. We set the provider’s arguments (access_key,
    secret_key, and region) with the variables defined earlier, referenced with the
    **var.<var_name>** notation.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this block defined, run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: To set up Terraform.
  prefs: []
  type: TYPE_NORMAL
- en: '3\. Creating our first resource: The S3 bucket'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Terraform uses the **resource** block to reference infrastructure components
    such as S3 buckets and EC2 instances, as well as actions like granting permissions
    to users or uploading files to a bucket.
  prefs: []
  type: TYPE_NORMAL
- en: The code below creates a new S3 bucket for our project.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'A **resource** definition follows the syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In the case above, “*aws_s3_bucket*” is the resource type, “*enem-bucket-terraform-jobs*”
    is the resource name, used to reference this resource in the file (it is not the
    bucket name in the AWS infrastructure). The argument *bucket=“enem-bucket-terraform-jobs”*
    assigns a name to our bucket.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, with the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Terraform will compare the current state of the infrastructure and infer what
    needs to be done to achieve the desired state described in the *main.tf* file.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ae83b2fbae13c9abd3147dc8b37bfa58.png)'
  prefs: []
  type: TYPE_IMG
- en: Because this bucket still does not exist, Terraform will plan to create it.
  prefs: []
  type: TYPE_NORMAL
- en: To apply Terraform’s plan, run
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/1d873cde7308f28ae0a43ff09f80950e.png)'
  prefs: []
  type: TYPE_IMG
- en: And, with only these few commands, our bucket is already created.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/87b7aa95c4c7680037fd00ca21c7931e.png)'
  prefs: []
  type: TYPE_IMG
- en: Easy, right?
  prefs: []
  type: TYPE_NORMAL
- en: 'To destroy the bucket, just type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/26f46fc9e91c4fd0764cf47b9e65bbe2.png)'
  prefs: []
  type: TYPE_IMG
- en: And Terraform takes care of the rest.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the basic commands that will follow us until the end of the post:
    **plan**, **apply**, **destroy**. From now on, all that we’re going to do is configure
    the *main.tf* file, adding the resources needed to materialize our data pipeline.'
  prefs: []
  type: TYPE_NORMAL
- en: '4\. Configuring the Lambda Function part I: Roles and permissions'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now on the Lambda Function definition.
  prefs: []
  type: TYPE_NORMAL
- en: This was one of the trickiest parts of my previous post because, by default,
    Lambda functions already need a set of basic permissions and, on top of that,
    we had also to give it read and write permissions to the S3 bucket previously
    created.
  prefs: []
  type: TYPE_NORMAL
- en: First of all, we must create a new IAM role.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: When developing these things, I strongly suggest that you first ask what you
    want in ChatGPT, GitHub Copilot, or any other LLM friend and then check the provider’s
    documentation on how this type of resource works.
  prefs: []
  type: TYPE_NORMAL
- en: The code above creates a new IAM role and allows AWS Lambda Functions to assume
    it. The next step is to attach the Lambda Basic Execution policy to this role
    to allow the Lambda Function to execute without errors.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The nice thing to note in the code above is that we can **reference resource
    attributes** **and pass them as arguments in the creation of new resources**.
    In the case above, instead of hard-coding the ‘*role’* argument with the name
    of the previously created role ‘*lambda_execution_role_terraform*’, we can reference
    this attribute using the syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: <*resource_type*>.<*resource_name*>.<*attribute>*
  prefs: []
  type: TYPE_NORMAL
- en: If you take some time to look into the Terraform documentation of a resource,
    you’ll note that it has *arguments* and *attributes*. **Arguments** are what you
    pass in order to create/configure a new resource and **attributes** are read-only
    properties about this resource available after its creation.
  prefs: []
  type: TYPE_NORMAL
- en: Because of this, attributes are used by Terraform to implicitly manage dependencies
    between resources, establishing the appropriate order of their creation.
  prefs: []
  type: TYPE_NORMAL
- en: The code below creates a new access policy for our S3 bucket, allowing basic
    CRUD operations on it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Again, instead of hard-coding the bucket’s ARN, we can reference this attribute
    using *aws_s3_bucket.enem-data-bucket.arn.*
  prefs: []
  type: TYPE_NORMAL
- en: With the Lambda role correctly configured, we can finally create the function
    itself.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The *lambda_function.zip* file is a compressed folder that must have a *lambda_function.py*
    file with a *lambda_handler(event, context)* function inside. It must be on the
    same path as the main.tf file.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/55218ad3da8f5fd729ad85c35abc6893.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '5\. Configuring the Lambda Function part II: Attaching a trigger'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, we need to configure a trigger for the Lambda Function: It must execute
    every time a new PDF is uploaded to the bucket.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This is a case where we must specify an **explicit dependency between resources**,
    as the “*bucket_notification*” resource needs to be created after the “*allow_bucket_execution*”.
  prefs: []
  type: TYPE_NORMAL
- en: This can be easily achieved by using the *depends_on* argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'And we’re done with the lambda function, just run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: And the Lambda Function will be created.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e8fce41371bbdd862c956c35f59f362e.png)![](../Images/1f2ebfd3ddd742f95516c69670a8bcbb.png)'
  prefs: []
  type: TYPE_IMG
- en: 6\. Adding a module to the Glue job
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our *main.tf* file is getting pretty big, and remember that this is just a simple
    data pipeline. To enhance the organization and reduce its size, we can use the
    concept of **modules**.
  prefs: []
  type: TYPE_NORMAL
- en: A **module** is a set of resources grouped in a separate file that can be referenced
    and reused by other configuration files. Modules enable us to abstract complex
    parts of the infrastructure to make our code more manageable, reusable, organized,
    and *modular.*
  prefs: []
  type: TYPE_NORMAL
- en: So, instead of coding all the resources needed to create our Glue job in the
    *main.tf* file, we’ll put them inside a **module.**
  prefs: []
  type: TYPE_NORMAL
- en: In the ./*terraform* folder, create a new folder ‘*glue*’ with a *glue.tf* file
    inside it.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3375413fdca889e5377830c6a4b95a74.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then add a new S3 bucket resource in the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Back in *main.tf*, just reference this module with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'And reinitialize terraform:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Terraform will restart its backend and initialize the module with it.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ee2d3d65250fa77aff8cd9899da16247.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, if we run terraform plan, it should include this new bucket in the creation
    list:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/92f1e71972ce18c1a8dfea2c35acfb96.png)'
  prefs: []
  type: TYPE_IMG
- en: Using this **module**, we’ll be able to encapsulate all the logic of creating
    the job in a single external file.
  prefs: []
  type: TYPE_NORMAL
- en: A requirement of AWS Glue jobs is that their job files are stored in an S3 bucket,
    and that’s why we created “*enem-bucket-terraform-jobs*”. Now, we must upload
    the job’s file itself.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cb3a6357ab4ecd7c13ac4410833e8fa5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the *terraform* path*,* I’d included a *myjob.py* file, it is just an empty
    file used to simulate this behavior. To upload a new object to a bucket, just
    use the “aws_s3_object” resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: From now on, it is just a matter of implementing the Glue role and creating
    the job itself.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '**Not so fast**. We must assure that this job has the same read and write permissions
    to the bucket “*enem-data-bucket*” as the Lambda Function, *i.e.* we need to attach
    the *aws_iam_policy.s3_access_policy* to its role*.*'
  prefs: []
  type: TYPE_NORMAL
- en: But, because this policy was defined in the main file, **we cannot reference
    it directly in our module**.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In order to achieve this behavior, we must pass the *access policy arn* as an
    **argument** to the **module**, and that’s pretty simple.
  prefs: []
  type: TYPE_NORMAL
- en: First, in the *glue.tf* file, create a new variable to receive the value.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Go back to the main file and, in the module reference, pass a value to this
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Finally, in the glue file, use the value of the variable in the resource.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Now, take a minute to think about the power of what we had just done. With **modules**
    and **arguments**, we can create **fully parametrized** **complex infrastructures**.
  prefs: []
  type: TYPE_NORMAL
- en: The code above doesn’t just create a specific job for our pipeline. By just
    changing the value of the *enem-data-bucket-access-policy-arn* variable, we can
    create a new job to process data from an entirely different bucket.
  prefs: []
  type: TYPE_NORMAL
- en: And that logic applies to **anything** you want. It’s possible, for example,
    to simultaneously create a complete infrastructure for a project for the development,
    testing, and production environments, using just variables to alternate between
    them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Without further talking, all that rests is to create the Glue job itself, and
    there is no novelty in that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: And our infrastructure is done. Run **terraform apply** to create the remaining
    resources.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/8758e74ee515b58b4122caba119e4357.png)'
  prefs: []
  type: TYPE_IMG
- en: And **terraform destroy** to get rid of everything.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/14cc69bea1ee680d9227381852cf0921.png)'
  prefs: []
  type: TYPE_IMG
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I met Terraform a few days after publishing my 2nd post about creating data
    pipelines using cloud providers, and it blew my mind. I instantly thought about
    all the manual work that I did to set up the project, all the prints captured
    to showcase the process and all the undocumented details that will haunt my nightmares
    when I need to reproduce the process.
  prefs: []
  type: TYPE_NORMAL
- en: Terraform solves all these problems. It is simple, easy to set up, and easy
    to use, all it needs are a few .*tf* files along with the providers’ credentials
    and we’re ready to go.
  prefs: []
  type: TYPE_NORMAL
- en: Terraform tackles that kind of problem that people usually don’t are so *excited*
    to think about. When developing data products, we all think about performance,
    optimization, delay, quality, accuracy, and other data-specific or domain-specific
    aspects of our product.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t get me wrong, we all study to apply our better mathematical and computational
    knowledge to solve these problems, but we also need to think about critical aspects
    of the *development process* of our product, like reproducibility, maintainability,
    documentation, versioning, integration, modularization, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: These are aspects that our *software engineer* colleagues have been concerned
    about for a long time, so we don’t have to reinvent the wheel, just learn one
    thing or two from their best practices.
  prefs: []
  type: TYPE_NORMAL
- en: That’s why I always use Docker in my projects and that’s also why I will probably
    add Terraform in my basic toolset.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this post helped you in understanding this tool — Terraform — including
    its objectives, basic functionalities, and practical benefits. As always, I’m
    not an expert in any of the subjects addressed in this post, and I strongly recommend
    further reading, see some references below.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading! ;)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the code is available in [this GitHub repository](https://github.com/jaumpedro214/posts).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Data used — [ENEM PDFs](https://www.gov.br/inep/pt-br/areas-de-atuacao/avaliacao-e-exames-educacionais/enem/provas-e-gabaritos),
    [CC BY-ND 3.0], MEC-Brazilian Gov.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: All the images are created by the Author, unless otherwise specified.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[1] *Add trigger to AWS Lambda functions via Terraform*. Stack Overflow. [Link](https://stackoverflow.com/questions/68245765/add-trigger-to-aws-lambda-functions-via-terraform).'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] *AWSLambdaBasicExecutionRole — AWS Managed Policy*. [Link](https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AWSLambdaBasicExecutionRole.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Brikman, Y. (2022, October 11). Terraform tips & tricks: loops, if-statements,
    and gotchas. [*Medium*](https://blog.gruntwork.io/terraform-tips-tricks-loops-if-statements-and-gotchas-f739bbae55f9)*.*
    [4] *Create Resource Dependencies | Terraform | HashiCorp Developer*. [Link](https://developer.hashicorp.com/terraform/tutorials/configuration-language/dependencies).'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] TechWorld with Nana. (2020, July 4). *Terraform explained in 15 mins |
    Terraform Tutorial for Beginners* [Video]. [YouTube](https://www.youtube.com/watch?v=l5k1ai_GBDE).'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] *Terraform Registry*. AWS provider. [Link](https://registry.terraform.io/providers/hashicorp/aws/latest).'
  prefs: []
  type: TYPE_NORMAL
