- en: Researching a Humanitarian Disaster Situation Report Chatbot — Using GPT-4-Turbo
    and full-context prompting
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 研究人道主义灾难情况报告聊天机器人 — 使用GPT-4-Turbo和完整上下文提示
- en: 原文：[https://towardsdatascience.com/researching-a-humanitarian-disaster-situation-report-chatbot-using-gpt-4-turbo-and-full-context-f742203d495a?source=collection_archive---------7-----------------------#2023-11-15](https://towardsdatascience.com/researching-a-humanitarian-disaster-situation-report-chatbot-using-gpt-4-turbo-and-full-context-f742203d495a?source=collection_archive---------7-----------------------#2023-11-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/researching-a-humanitarian-disaster-situation-report-chatbot-using-gpt-4-turbo-and-full-context-f742203d495a?source=collection_archive---------7-----------------------#2023-11-15](https://towardsdatascience.com/researching-a-humanitarian-disaster-situation-report-chatbot-using-gpt-4-turbo-and-full-context-f742203d495a?source=collection_archive---------7-----------------------#2023-11-15)
- en: '[](https://medium.com/@astrobagel?source=post_page-----f742203d495a--------------------------------)[![Matthew
    Harris](../Images/4fa3264bb8a028633cd8d37093c16214.png)](https://medium.com/@astrobagel?source=post_page-----f742203d495a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f742203d495a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f742203d495a--------------------------------)
    [Matthew Harris](https://medium.com/@astrobagel?source=post_page-----f742203d495a--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@astrobagel?source=post_page-----f742203d495a--------------------------------)[![Matthew
    Harris](../Images/4fa3264bb8a028633cd8d37093c16214.png)](https://medium.com/@astrobagel?source=post_page-----f742203d495a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f742203d495a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f742203d495a--------------------------------)
    [Matthew Harris](https://medium.com/@astrobagel?source=post_page-----f742203d495a--------------------------------)'
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4a2cd25b8ff9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fresearching-a-humanitarian-disaster-situation-report-chatbot-using-gpt-4-turbo-and-full-context-f742203d495a&user=Matthew+Harris&userId=4a2cd25b8ff9&source=post_page-4a2cd25b8ff9----f742203d495a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f742203d495a--------------------------------)
    ·16 min read·Nov 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff742203d495a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fresearching-a-humanitarian-disaster-situation-report-chatbot-using-gpt-4-turbo-and-full-context-f742203d495a&user=Matthew+Harris&userId=4a2cd25b8ff9&source=-----f742203d495a---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[跟进](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4a2cd25b8ff9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fresearching-a-humanitarian-disaster-situation-report-chatbot-using-gpt-4-turbo-and-full-context-f742203d495a&user=Matthew+Harris&userId=4a2cd25b8ff9&source=post_page-4a2cd25b8ff9----f742203d495a---------------------post_header-----------)
    发布在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----f742203d495a--------------------------------)
    ·16 分钟阅读·Nov 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff742203d495a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fresearching-a-humanitarian-disaster-situation-report-chatbot-using-gpt-4-turbo-and-full-context-f742203d495a&user=Matthew+Harris&userId=4a2cd25b8ff9&source=-----f742203d495a---------------------clap_footer-----------)'
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff742203d495a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fresearching-a-humanitarian-disaster-situation-report-chatbot-using-gpt-4-turbo-and-full-context-f742203d495a&source=-----f742203d495a---------------------bookmark_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff742203d495a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fresearching-a-humanitarian-disaster-situation-report-chatbot-using-gpt-4-turbo-and-full-context-f742203d495a&source=-----f742203d495a---------------------bookmark_footer-----------)'
- en: TL;DR
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: TL;DR
- en: '*In this article we explore OpenAI’s brand new GPT-4-Turbo model, using its
    increased 128k token context window to pass in a full document corpus for information
    retrieval. This crude brute force method — only possible with larger context windows
    — is simple to implement and doesn’t require document embeddings and semantic
    search as used in Retrieval Augmented Generation (RAG). Applied to humanitarian
    disaster situation reports published on the amazing ReliefWeb platform — compressed
    using Sparse Priming Representations (SPR)— we show that GPT-4-Turbo is able to
    answer basic questions about recent disasters. However, even with the recent decrease
    in OpenAI’s token costs, this approach is prohibitively expensive and prompting
    the preview GPT-4-Turbo model is very slow, sometimes taking up to a minute to
    respond. As with all LLM information retrieval patterns, it of course crucial
    to implement a validation framework to ensure hallucination and information ommision
    are controlled. That said, GPT-4-Turbo offers a great step forward in capabilities,
    especially as performance improves and costs come down, adding to the rapidly
    expanding LLM toolkit.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*在本文中，我们探索了 OpenAI 全新的 GPT-4-Turbo 模型，利用其增加的 128k 令牌上下文窗口来传递完整文档语料库以进行信息检索。这种粗略的蛮力方法——仅在更大的上下文窗口下才可能——实现简单，不需要文档嵌入和语义搜索，如检索增强生成（RAG）中使用的那样。应用于发布在令人惊叹的
    ReliefWeb 平台上的人道主义灾难情况报告——使用稀疏引导表示（SPR）进行压缩——我们展示了 GPT-4-Turbo 能够回答有关近期灾难的基本问题。然而，即使在
    OpenAI 最近降低了令牌成本的情况下，这种方法仍然代价高昂，提示预览版 GPT-4-Turbo 模型响应非常缓慢，有时需要一分钟才能回复。与所有 LLM
    信息检索模式一样，实施验证框架以确保控制虚假信息和信息遗漏当然至关重要。也就是说，GPT-4-Turbo 在能力上迈出了重要一步，尤其是随着性能的提升和成本的下降，它将加入快速扩展的
    LLM 工具包。*'
- en: '![](../Images/b9ce334d1988ebcccc7aa01a3ad3d2d1.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b9ce334d1988ebcccc7aa01a3ad3d2d1.png)'
- en: The new GPT-4-Turbo has a larger context window of 128k tokens. Image generated
    by GPT-4 + Dall-E-3.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 新的 GPT-4-Turbo 具有 128k 令牌的更大上下文窗口。图像由 GPT-4 + Dall-E-3 生成。
- en: Given the frantic pace of developments in the last couple of months with the
    release of [autogen](https://github.com/microsoft/autogen), [memgpt](https://memgpt.ai/),
    [Semantic Kern](https://github.com/microsoft/semantic-kernel)el, and [OpenAI’s
    GPTs and GPT-4-Turbo](https://openai.com/blog/new-models-and-developer-products-announced-at-devday),
    I thought I would do a series of articles which compare some of the techniques
    these new tools offer for conversational information retrieval. Each has its own
    advantages and disadvantages, and some present a potential paradigm shift in how
    we use Large Language Models (LLMs). It’s a pretty amazing time, but using these
    new techniques in real-world applications isn’t always as easy as initial exploration
    sometimes suggests.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于过去几个月中，[autogen](https://github.com/microsoft/autogen)、[memgpt](https://memgpt.ai/)、[Semantic
    Kern](https://github.com/microsoft/semantic-kernel)el，以及[OpenAI 的 GPTs 和 GPT-4-Turbo](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)的发布，发展步伐非常迅速，我想做一系列文章，比较这些新工具提供的一些对话信息检索技术。每种技术都有其优缺点，有些可能会在我们使用大型语言模型（LLMs）的方式上带来潜在的范式转变。这是一个非常令人惊叹的时刻，但将这些新技术应用于实际场景中，并不像初步探索时所建议的那样简单。
- en: OpenAI Release GPT-4-Turbo (Preview)
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI 发布 GPT-4-Turbo（预览）
- en: OpenAI’s [recent announcement at DevDay](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)
    included a few new features that could potentially change the Generative AI landscape.
    One of these was the release (in preview) of [GPT-4-Turbo](https://help.openai.com/en/articles/8555510-gpt-4-turbo)
    with an increased context window (prompt) limit of 128k tokens, compared to 32k
    previously. Previously [Claude AI](https://www.anthropic.com/index/claude-2) offered
    the largest commercial context limit of 100k tokens, so GPT-4-Turbo is a step
    beyond this. Additionally, OpenAI maintains that their new model is more proficient
    at instruction following and will be 3 times cheaper than GPT-4\. As the lead
    in many LLM benchmarks, any advance of GPT-4 is important.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 在[DevDay 的近期公告](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)中包括了一些可能改变生成
    AI 领域的新功能。其中之一是发布了具有 128k 令牌上下文窗口限制的[GPT-4-Turbo](https://help.openai.com/en/articles/8555510-gpt-4-turbo)（预览），相比之前的
    32k 令牌。此前，[Claude AI](https://www.anthropic.com/index/claude-2) 提供了 100k 令牌的最大商业上下文限制，因此
    GPT-4-Turbo 在这方面更进一步。此外，OpenAI 还表示，他们的新模型在遵循指令方面更为高效，成本将比 GPT-4 低 3 倍。作为许多 LLM
    基准测试的领先者，GPT-4 的任何进展都很重要。
- en: Increasing model token limits for an increased context
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 增加模型令牌限制以扩展上下文
- en: So why are increased token limits a big deal? When prompting an LLM you can
    include past conversation, so one immediate benefit is that it remembers what
    you were talking about earlier if you provide conversation history in every prompt.
    This is useful in order to reference facts in earlier conversations that might
    be important right now. A larger context window means you can also ‘Preload’ the
    chat with supporting information, such as document content and data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 那么为什么增加的令牌限制很重要呢？在提示 LLM 时，你可以包括过去的对话，所以一个直接的好处是，如果你在每次提示中提供对话历史，它会记住你之前谈论的内容。这对于引用在早期对话中可能现在重要的事实非常有用。更大的上下文窗口意味着你还可以‘预加载’聊天，加入支持信息，比如文档内容和数据。
- en: But there is a downside.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 但也有一个缺点。
- en: More tokens mean higher cost and slower performance because with the transformer
    architecture memory and computational requirements increase quadratically (much
    faster than a simple straight line). Also, there is some research that suggests
    that longer context windows degrade LLM accuracy ([Liu et al, 2023](https://arxiv.org/abs/2307.03172)).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 更多的令牌意味着更高的成本和更慢的性能，因为使用变换器架构时，内存和计算需求呈平方增加（远快于简单的直线）。另外，有研究表明较长的上下文窗口可能会降低
    LLM 的准确性（[刘等，2023](https://arxiv.org/abs/2307.03172)）。
- en: Using GPT-4-Turbo
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 GPT-4-Turbo
- en: At time of writing, GPT-4-Turbo is in preview mode only, available as model
    ‘gpt-4–1106-preview’. To call it we will use the openai Python pakage like this
    …
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在写作时，GPT-4-Turbo 仅处于预览模式，仅作为模型 ‘gpt-4–1106-preview’ 提供。调用时我们将使用 openai Python
    包，如下 …
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Analyzing ReliefWeb Disaster Situation Reports
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析 ReliefWeb 灾害情况报告
- en: We will explore GPT-4-Turbo’s increased token limit by using it to analyze [Humanitarian
    Disaster Situation Reports on the amazing ReliefWeb](https://reliefweb.int/disasters)
    platform. These reports (known as ‘Sitreps’) are vital for monitoring and reacting
    to humanitarian disasters around the world. They also provide a text corpus of
    data which can be compressed (summarized) to fit into GPT-4-Turbo’s context window
    for our analysis.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过使用 GPT-4-Turbo 来分析 [令人惊叹的 ReliefWeb 平台上的人道主义灾害情况报告](https://reliefweb.int/disasters)
    来探索其增加的令牌限制。这些报告（称为‘Sitreps’）对于监测和应对全球的人道主义灾害至关重要。它们还提供了一个文本数据语料库，可以被压缩（总结）以适应
    GPT-4-Turbo 的上下文窗口，以供我们的分析使用。
- en: Building a Corpus of Disaster Reports
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建立灾害报告语料库
- en: ReliefWeb has a [really great API](https://reliefweb.int/help/api) for accessing
    content, so we will use this to extract a list of disasters and situation reports
    …
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ReliefWeb 有一个 [非常棒的 API](https://reliefweb.int/help/api) 用于访问内容，因此我们将使用它来提取灾害和情况报告的列表
    …
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the above functions, some points of interest …
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述函数中，一些关注点 …
- en: If the ReliefWeb content refers to a PDF, we extract the text from that
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 ReliefWeb 内容提到 PDF，我们会从中提取文本
- en: Any text is auto-translated to English using Google Translate API
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 任何文本都会通过 Google Translate API 自动翻译成英语
- en: We always capture sources for attribution
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们总是捕捉来源以便归属
- en: We haven’t bothered with API response pagination for this quick analysis
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于这次快速分析，我们没有处理 API 响应的分页
- en: Here is how we call the function to get disaster situation reports since the
    1st of November 2023 …
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们调用函数以获取自 2023 年 11 月 1 日以来的灾害情况报告的方法 …
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The situation reports are saved to the file system as text files …
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 情况报告被保存到文件系统作为文本文件 …
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Compressing Content Using Sparse Priming Representations (SPR)
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用稀疏引导表示 (SPR) 压缩内容
- en: We could just prompt GPT-4-Turbo with raw text from the situation reports. However,
    much of that text is irrelevant — such as stop words, headers, and footers — so
    including all of it would very quickly exceed even the increased token limits
    of GPT-4-Turbo. We will instead use a technique called [Sparse Priming Representations
    (SPR)](https://github.com/daveshap/SparsePrimingRepresentations) to compress documents
    to their key facts, modified a little to try and preserve quantitative information.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接用原始文本从情况报告中提示 GPT-4-Turbo。然而，那些文本中很多都是不相关的——如停用词、标题和页脚——因此包含所有这些内容会很快超过
    GPT-4-Turbo 增加后的令牌限制。我们将使用一种叫做 [稀疏引导表示 (SPR)](https://github.com/daveshap/SparsePrimingRepresentations)
    的技术来压缩文档到其关键事实，并稍作修改以试图保留定量信息。
- en: Here is the system prompt we’ll provide to GPT-4-Turbo to compress our documents
    …
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将提供给 GPT-4-Turbo 的系统提示，用于压缩我们的文档 …
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This will summarize a raw text status report into something like this for a
    situation report titled “African Polio Laboratory Network Bulletin (week 1–42,
    2023)” …
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把原始文本状态报告总结成类似于标题为“非洲脊髓灰质炎实验室网络简报（第 1–42 周，2023 年）”的情况报告 …
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Which is of course much less text than the original document.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: I wouldn’t advise using this compression without significant analysis and checks
    to control for information omission, but for our tests, it will suffice.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Here is the code for compressing reports …
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You’ll notice in the above we added some metadata about the report plus the
    SPR summary returned by GPT-4-Turbo. The compressed reports are then saved as
    text files.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Extracting a high-level List of Disasters
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will also extract a high-level list of disasters from ReliefWeb to use in
    our system prompt, as an aid to information requests …
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This gives us a concise list …
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/008e025214645e66b0e5c853c919fa92.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: List of disasters as extracted using ReleiefWeb API disasters endpoint
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Creating a prompt for GPT-4-Turbo
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now have a list of disasters and compressed situation reports — from Nov
    1st to Nov 10th — listing key facts from those disasters.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Let’s combine them into one text file for use as part of the system prompt for
    GPT-4-Turbo …
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: How many tokens and what’s the cost?
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: So given the cost of [$0.01 per 1,000 tokens input](https://openai.com/pricing),
    the text created above comes out at $0.82 a prompt. There is also some completion
    token cost, $0.03 per 1000 tokens, but this should be much less than input cost
    as there are far fewer tokens.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Ouch!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: We knew this brute force technique isn’t the best way to tackle our particular
    task, but the high cost is another reason.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Now we have our text, we can build a system prompt …
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You’ll notice that the system prompt includes instructions about which sections
    in the concatenated text to use for different types of information. Through some
    quick prompt experimentation — I wouldn’t really call it ‘engineering’ — this
    produced better results.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Testing our Humanitarian Disaster Situation Report Bot
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OK, here goes …
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: On the whole, this is a pretty good summary of the disaster. The facts agree
    well compared with the [ReliefWeb disaster page](https://reliefweb.int/disaster/eq-2023-000214-npl)
    (note this page may change since this article, as the disaster evolves) and the
    [latest situation report](https://reliefweb.int/report/nepal/nepal-western-nepal-earthquake-2023-situation-report-no-02-10-november-2023)
    used for this study.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: One fact though is very slightly misaligned, the data said that tragically 153
    people died, whereas the model returned 154.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: So it is ‘known’ but hallucinated in the first prompt. As with all applications
    of LLM, validation and checks are key to catching issues like this.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get some information about the organizations creating situation reports
    (for the period November 1st-10th) …
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Compared to the [same query in ReliefWeb](https://reliefweb.int/updates?advanced-search=%28D51797%29_%28F10%29_%28DA20231101-20231110%29)
    this seems correct.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Let’s ask some more nuanced questions …
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Looking at the situation reports, I was unable to find this information either.
    A more detailed back-check is required to establish this is true though.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Let’s ask about potential disease and health impacts …
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨一下潜在的疾病和健康影响……
- en: '[PRE16]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Looking at the [UNICEF report](https://reliefweb.int/report/nepal/unicef-nepal-humanitarian-situation-report-no-1-earthquake-08-november-2023)
    in the data, even with our SPF summarization the above seems to capture the main
    points. However, we only have report number 1 in the data used here, but the above
    mentions report number 2\. Another minor hallucination, again illustrating that
    any LLM responses need automatic back-checking.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据中的 [UNICEF 报告](https://reliefweb.int/report/nepal/unicef-nepal-humanitarian-situation-report-no-1-earthquake-08-november-2023)来看，即使通过我们的
    SPF 摘要，上述内容似乎捕捉到了主要观点。然而，我们这里使用的数据只有报告第 1 号，但上述内容提到了报告第 2 号。这是另一个小的错误，再次表明任何 LLM
    的回应都需要自动回查。
- en: Conclusions
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: This is a very quick analysis to get an idea of how well GPT-4-Turbo — with
    its increased 128k context window and improved instruction following — performs
    by including all the text data needed for information retrieval as a system prompt,
    without any other processing. Is this brute force approach the best technique
    for our task?
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常快速的分析，旨在了解 GPT-4-Turbo — 利用其增加的 128k 上下文窗口和改进的指令跟随能力 — 在包括所有信息检索所需的文本数据作为系统提示时的表现，且没有其他处理。这种蛮力方法是否是我们任务的最佳技术？
- en: Probably not, at least not yet.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 可能不是，至少目前还不是。
- en: Performance is a serious consideration. The prompts were taking 30 seconds or
    longer, not great for a delightful user experience.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 性能是一个严重的考虑因素。提示处理时间为 30 秒或更长，这对用户体验来说不太理想。
- en: Cost is also prohibitive. With more tokens needed to provide a full corpus with
    each prompt, there is increased cost — in this analysis $0.82 for every prompt!
    — so other techniques will undoubtedly be more attractive to many organizations,
    at least while LLM costs are at a premium. That said, I am reminded of the [cost
    of storage over the years](https://ourworldindata.org/grapher/historical-cost-of-computer-memory-and-storage),
    and maybe we will see the same decrease for LLMs over time.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 成本也是一个限制因素。每次提示需要更多的令牌来提供完整语料库，这增加了成本——在这次分析中，每次提示需要 $0.82！——因此，其他技术无疑对许多组织更具吸引力，至少在
    LLM 成本高昂的情况下。不过，我想起了 [存储成本的历史](https://ourworldindata.org/grapher/historical-cost-of-computer-memory-and-storage)，也许我们会看到
    LLM 成本的相同下降。
- en: Alternatives such as generating code to query data, using functions registered
    with the LLM, and multiple agents for validating results may offer cheaper and
    more accurate options. They would also eliminate the need to compress documents
    in order to fit the corpus in the context window, thus avoiding information loss.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 生成代码以查询数据、使用注册到 LLM 的函数和多代理验证结果等替代方案可能提供更便宜和更准确的选择。它们还将消除为了适应上下文窗口而压缩文档的需要，从而避免信息丢失。
- en: That said, we were able to show that prompting GPT-4-Turbo can support basic
    information retrieval on a corpus of compressed documents provided in the system
    prompt. This has some benefits in being very straightforward to implement - you
    just give the LLM all your stuff and ask questions. As AI advances and costs decrease
    this may become a very common technique in the future.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，我们能够展示，提示 GPT-4-Turbo 可以支持在系统提示中提供的压缩文档语料库上的基本信息检索。这具有非常直接的实施优点——你只需将所有内容提供给语言模型并提问。随着
    AI 的进步和成本的降低，这在未来可能会成为一种非常普遍的技术。
