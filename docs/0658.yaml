- en: Why Do We Have Huge Language Models and Small Vision Transformers?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/why-do-we-have-huge-language-models-and-small-vision-transformers-5d59ac36c1d6?source=collection_archive---------3-----------------------#2023-02-17](https://towardsdatascience.com/why-do-we-have-huge-language-models-and-small-vision-transformers-5d59ac36c1d6?source=collection_archive---------3-----------------------#2023-02-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Artificial intelligence | computer vision | Vision Transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Google ViT-22 paves the way for new large transformers and to revolutionize
    computer vision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://salvatore-raieli.medium.com/?source=post_page-----5d59ac36c1d6--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----5d59ac36c1d6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5d59ac36c1d6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5d59ac36c1d6--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----5d59ac36c1d6--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff1a08d9452cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-do-we-have-huge-language-models-and-small-vision-transformers-5d59ac36c1d6&user=Salvatore+Raieli&userId=f1a08d9452cd&source=post_page-f1a08d9452cd----5d59ac36c1d6---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5d59ac36c1d6--------------------------------)
    ·9 min read·Feb 17, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5d59ac36c1d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-do-we-have-huge-language-models-and-small-vision-transformers-5d59ac36c1d6&user=Salvatore+Raieli&userId=f1a08d9452cd&source=-----5d59ac36c1d6---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5d59ac36c1d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-do-we-have-huge-language-models-and-small-vision-transformers-5d59ac36c1d6&source=-----5d59ac36c1d6---------------------bookmark_footer-----------)![](../Images/e70451dc343d2e3cbc3ea2e6b5bbd3ea.png)'
  prefs: []
  type: TYPE_NORMAL
- en: image by [Joshua Earle](https://unsplash.com/@joshuaearle) at unsplash.com
  prefs: []
  type: TYPE_NORMAL
- en: In recent years we have seen a growth in the number of transformer parameters.
    Looking closely though, these are Language Models (LMs), up to an incredible [540
    B of parameters](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html).
    **Why not for visual models?**
  prefs: []
  type: TYPE_NORMAL
- en: As for text models, an increase in dataset size, scalable architectures, and
    new training methods have enabled this growth in the number of parameters. This
    has not only served to increase performance in particular tasks (classification
    and so on), **but as the number of parameters has grown, we have seen emerging
    capabilities.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/57b77e6e745a2ea296895080a8738106.png)'
  prefs: []
  type: TYPE_IMG
- en: '“Trend of sizes of state-of-the-art Natural Language Processing (NLP) models
    with time. The number of floating-point operations to train these models is increasing
    at an exponential rate”. source: [here](https://arxiv.org/abs/2104.04473)'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, a large model can be used as a basis for [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning)
    and fine-tuning, so there is interest in developing increasingly high-performance
    models. As much as LMs have been used successfully in a number of tasks, there
    are many other tasks where a model capable of image analysis is needed.
  prefs: []
  type: TYPE_NORMAL
