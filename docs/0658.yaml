- en: Why Do We Have Huge Language Models and Small Vision Transformers?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/why-do-we-have-huge-language-models-and-small-vision-transformers-5d59ac36c1d6?source=collection_archive---------3-----------------------#2023-02-17](https://towardsdatascience.com/why-do-we-have-huge-language-models-and-small-vision-transformers-5d59ac36c1d6?source=collection_archive---------3-----------------------#2023-02-17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Artificial intelligence | computer vision | Vision Transformers
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Google ViT-22 paves the way for new large transformers and to revolutionize
    computer vision
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://salvatore-raieli.medium.com/?source=post_page-----5d59ac36c1d6--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----5d59ac36c1d6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5d59ac36c1d6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5d59ac36c1d6--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----5d59ac36c1d6--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff1a08d9452cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-do-we-have-huge-language-models-and-small-vision-transformers-5d59ac36c1d6&user=Salvatore+Raieli&userId=f1a08d9452cd&source=post_page-f1a08d9452cd----5d59ac36c1d6---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5d59ac36c1d6--------------------------------)
    ·9 min read·Feb 17, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5d59ac36c1d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-do-we-have-huge-language-models-and-small-vision-transformers-5d59ac36c1d6&user=Salvatore+Raieli&userId=f1a08d9452cd&source=-----5d59ac36c1d6---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5d59ac36c1d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-do-we-have-huge-language-models-and-small-vision-transformers-5d59ac36c1d6&source=-----5d59ac36c1d6---------------------bookmark_footer-----------)![](../Images/e70451dc343d2e3cbc3ea2e6b5bbd3ea.png)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: image by [Joshua Earle](https://unsplash.com/@joshuaearle) at unsplash.com
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: In recent years we have seen a growth in the number of transformer parameters.
    Looking closely though, these are Language Models (LMs), up to an incredible [540
    B of parameters](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html).
    **Why not for visual models?**
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: As for text models, an increase in dataset size, scalable architectures, and
    new training methods have enabled this growth in the number of parameters. This
    has not only served to increase performance in particular tasks (classification
    and so on), **but as the number of parameters has grown, we have seen emerging
    capabilities.**
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 至于文本模型，数据集规模的增加、可扩展架构和新的训练方法促进了参数数量的增长。这不仅提升了特定任务（如分类等）的性能，**而且随着参数数量的增加，我们看到了新兴的能力**。
- en: '![](../Images/57b77e6e745a2ea296895080a8738106.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57b77e6e745a2ea296895080a8738106.png)'
- en: '“Trend of sizes of state-of-the-art Natural Language Processing (NLP) models
    with time. The number of floating-point operations to train these models is increasing
    at an exponential rate”. source: [here](https://arxiv.org/abs/2104.04473)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: “随着时间推移，最先进的自然语言处理（NLP）模型的规模趋势。这些模型的训练所需的浮点运算次数以指数级速度增加。” 来源：[这里](https://arxiv.org/abs/2104.04473)
- en: In addition, a large model can be used as a basis for [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning)
    and fine-tuning, so there is interest in developing increasingly high-performance
    models. As much as LMs have been used successfully in a number of tasks, there
    are many other tasks where a model capable of image analysis is needed.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，大型模型可以作为[迁移学习](https://en.wikipedia.org/wiki/Transfer_learning)和微调的基础，因此有兴趣开发越来越高性能的模型。尽管语言模型在许多任务中取得了成功，但在图像分析方面仍需要一个能够胜任的模型。
