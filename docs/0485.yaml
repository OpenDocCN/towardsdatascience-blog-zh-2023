- en: Mixture of Experts for PINNs (MoE-PINNs)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PINNs的专家混合（MoE-PINNs）
- en: 原文：[https://towardsdatascience.com/mixture-of-experts-for-pinns-moe-pinns-6520adf32438?source=collection_archive---------7-----------------------#2023-02-02](https://towardsdatascience.com/mixture-of-experts-for-pinns-moe-pinns-6520adf32438?source=collection_archive---------7-----------------------#2023-02-02)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/mixture-of-experts-for-pinns-moe-pinns-6520adf32438?source=collection_archive---------7-----------------------#2023-02-02](https://towardsdatascience.com/mixture-of-experts-for-pinns-moe-pinns-6520adf32438?source=collection_archive---------7-----------------------#2023-02-02)
- en: Utilising Ensembles to enhance Physics-Informed Neural Networks
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用集成方法提升物理信息神经网络
- en: '[](https://rabischof.medium.com/?source=post_page-----6520adf32438--------------------------------)[![Rafael
    Bischof](../Images/a1d468ea5b61c26a18541f0c0f42c5c6.png)](https://rabischof.medium.com/?source=post_page-----6520adf32438--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6520adf32438--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6520adf32438--------------------------------)
    [Rafael Bischof](https://rabischof.medium.com/?source=post_page-----6520adf32438--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://rabischof.medium.com/?source=post_page-----6520adf32438--------------------------------)[![拉斐尔·比肖夫](../Images/a1d468ea5b61c26a18541f0c0f42c5c6.png)](https://rabischof.medium.com/?source=post_page-----6520adf32438--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6520adf32438--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6520adf32438--------------------------------)
    [拉斐尔·比肖夫](https://rabischof.medium.com/?source=post_page-----6520adf32438--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F913c6c1e6a94&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixture-of-experts-for-pinns-moe-pinns-6520adf32438&user=Rafael+Bischof&userId=913c6c1e6a94&source=post_page-913c6c1e6a94----6520adf32438---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6520adf32438--------------------------------)
    ·9 min read·Feb 2, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6520adf32438&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixture-of-experts-for-pinns-moe-pinns-6520adf32438&user=Rafael+Bischof&userId=913c6c1e6a94&source=-----6520adf32438---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F913c6c1e6a94&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixture-of-experts-for-pinns-moe-pinns-6520adf32438&user=Rafael+Bischof&userId=913c6c1e6a94&source=post_page-913c6c1e6a94----6520adf32438---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6520adf32438--------------------------------)
    · 9分钟阅读 · 2023年2月2日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6520adf32438&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixture-of-experts-for-pinns-moe-pinns-6520adf32438&user=Rafael+Bischof&userId=913c6c1e6a94&source=-----6520adf32438---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6520adf32438&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixture-of-experts-for-pinns-moe-pinns-6520adf32438&source=-----6520adf32438---------------------bookmark_footer-----------)![](../Images/5d1b9499921aea88cd946c6f39a435d7.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6520adf32438&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixture-of-experts-for-pinns-moe-pinns-6520adf32438&source=-----6520adf32438---------------------bookmark_footer-----------)![](../Images/5d1b9499921aea88cd946c6f39a435d7.png)'
- en: Photo by [Soviet Artefacts](https://unsplash.com/@sovietartefacts?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Soviet Artefacts](https://unsplash.com/@sovietartefacts?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '[Physics-Informed Neural Networks (PINNs)](https://www.sciencedirect.com/science/article/pii/S0021999118307125)
    [1] have emerged as a popular and promising approach for solving partial differential
    equations (PDEs). In our latest research, my colleague Michael Kraus and I introduce
    a new framework called [Mixture of Experts for Physics-Informed Neural Networks
    (MoE-PINNs)](https://mediatum.ub.tum.de/doc/1688403/uic8b0xn1c845e7rac1or092o.Bischof%20et%20Al.%202022.pdf)
    [2], which shows great potential on PDEs with complex and varied patterns.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[物理信息神经网络（PINNs）](https://www.sciencedirect.com/science/article/pii/S0021999118307125)
    [1] 已成为解决偏微分方程（PDEs）的一种流行且有前途的方法。在我们最新的研究中，我的同事迈克尔·克劳斯和我介绍了一种名为[用于解决具有复杂和多样模式的PDEs的混合专家物理信息神经网络（MoE-PINNs）](https://mediatum.ub.tum.de/doc/1688403/uic8b0xn1c845e7rac1or092o.Bischof%20et%20Al.%202022.pdf)
    [2] 的新框架，它在PDEs方面显示出巨大潜力。'
- en: 'In this post, we will discuss the benefits of MoE-PINNs and how they can be
    easily implemented to solve a wide range of PDE problems. It is structured the
    following way:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将讨论MoE-PINNs的好处以及如何轻松实现它们来解决各种PDE问题。结构如下：
- en: Introduction to MoE-PINNs
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MoE-PINNs入门
- en: Solving Burgers’ PDE using MoE-PINNs
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MoE-PINNs解决Burgers' PDE
- en: Reducing Hyperparameter Search with Sparsity Regularisation
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过稀疏正则化减少超参数搜索
- en: Example of Sparsity Regularisation by solving the Poisson PDE on an L-shaped
    Domain
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过解决L形域上的泊松PDE的MoE-PINNs示例
- en: Automated Architecture Search with Differentiable MoE-PINNs
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用可微MoE-PINN进行自动体系结构搜索
- en: 'To help you gain a better understanding of the concepts discussed in this article,
    we have provided accompanying notebooks that can be run directly on Colab:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助您更好地理解本文中讨论的概念，我们提供了伴随的笔记本，可以直接在Colab上运行：
- en: '[Notebook for MoE-PINNs on Burgers’ equation](https://drive.google.com/file/d/1JyejLXPS9LQdsNzZE5z-FTd8idRgrP1n/view?usp=sharing)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Burgers''方程上的MoE-PINNs笔记本](https://drive.google.com/file/d/1JyejLXPS9LQdsNzZE5z-FTd8idRgrP1n/view?usp=sharing)'
- en: '[Notebook for MoE-PINNs on the L-shaped Poisson equation](https://drive.google.com/file/d/1IRyZvl9OFU8a0PETjdEEppM1BBoda4e8/view?usp=sharing)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[L形泊松方程上的MoE-PINNs笔记本](https://drive.google.com/file/d/1IRyZvl9OFU8a0PETjdEEppM1BBoda4e8/view?usp=sharing)'
- en: PINNs use physical laws and automatic differentiation to solve Partial Differential
    Equations (PDEs) with just a few lines of code. However, they are also highly
    susceptible to their hyperparameters, such as the activation function or initialisation
    of the weights. This makes training PINNs particularly difficult and a laborious,
    iterative process. The depth of the network and the choice of activation function
    can greatly impact the solution accuracy. For example, deep networks work well
    for complex PDEs with varying patterns and discontinuities, such as Navier-Stokes
    equations. Meanwhile, shallow networks may suffice for simple PDEs with simpler
    patterns, like the Poisson equation on a square domain. The sine activation function,
    with its property of preserving shape under differentiation, may be ideal for
    high-order differentiation problems. On the other hand, activation functions like
    swish or softplus may perform better for problems with sharp discontinuities.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: PINNs利用物理定律和自动微分，仅需几行代码就能解决偏微分方程（PDEs）。然而，它们也对超参数非常敏感，例如激活函数或权重初始化。这使得训练PINNs特别困难，是一个费力的迭代过程。网络的深度和激活函数的选择可以极大地影响解决方案的准确性。例如，对于具有不同模式和不连续性的复杂PDEs，如Navier-Stokes方程，深度网络效果良好。而对于在正方形域上的泊松方程这样具有简单模式的简单PDEs，浅层网络可能已经足够。正弦激活函数以其在微分下保持形状的特性，可能是高阶微分问题的理想选择。另一方面，像swish或softplus这样的激活函数可以更好地处理具有尖锐不连续性的问题。
- en: But what if your problem requires a combination of both? How do we handle a
    PDE that is smooth and repeating in one part, and highly complex with sharp discontinuities
    in another? This is where the **Mixture of Experts (MoE)** framework for PINNs
    comes in. By leveraging multiple networks and a gate to divide the domain, each
    expert can specialise in solving a distinct part of the problem, allowing for
    improved accuracy and reduced bias-variance trade-off.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果您的问题需要结合两者呢？如果我们处理的PDE在一部分平滑重复，在另一部分则是高度复杂且具有尖锐不连续性？这就是**混合专家（MoE）**框架对PINNs的贡献之处。通过利用多个网络和一个门来划分域，每个专家可以专注于解决问题的不同部分，从而提高准确性并减少偏差-方差的折衷。
- en: 'Dividing the problem into smaller sub-problems has many advantages:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 将问题划分为较小的子问题有很多好处：
- en: By using several learners on distinct sub-domains, the complexity of the problem
    is reduced.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在不同子领域使用多个学习者，问题的复杂性得以降低。
- en: The gate in MoE-PINNs is a continuous function, resulting in smooth transitions
    between the domains. Therefore, more complex regions of the domain can be equally
    divided amongst several learners, whereas simpler regions can be attributed to
    a single expert.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MoE-PINNs 中的门控是一个连续函数，从而在领域之间实现平滑过渡。因此，领域中更复杂的区域可以在多个学习者之间平均分配，而较简单的区域则可以分配给单个专家。
- en: The gate can be any type of neural network, from a linear layer to a deep neural
    network, which allows it to adapt to different types of domains and divide them
    in an arbitrary way.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该门控可以是任何类型的神经网络，从线性层到深度神经网络，这使得它能够适应不同类型的领域，并以任意方式进行划分。
- en: MoE can be easily parallelised, since only the weights lambda need to be shared
    amongst learners on different devices. In theory, each learner could be placed
    on a distinct GPU.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MoE 可以很容易地并行化，因为只需在不同设备上的学习者之间共享权重 lambda。理论上，每个学习者可以放置在一个不同的 GPU 上。
- en: By initialising a large number of PINNs with different architectures, the need
    for labor-intensive hyperparameter tuning is reduced.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过初始化大量具有不同架构的 PINNs，可以减少劳动密集型的超参数调优需求。
- en: MoE-PINN Architecture
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MoE-PINN 架构
- en: '![](../Images/bf52d2dd67e79e742c039a1b83e89112.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bf52d2dd67e79e742c039a1b83e89112.png)'
- en: Mixture of Experts of PINNs. An arbitrary number m of PINNs, possibly with varying
    architectures and properties, is initialised together with a gating network. All
    models receive the same input and the gate produces weights that are used for
    aggregating the results. Figure by author [2].
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: PINNs 的专家混合。一个任意数量 *m* 的 PINNs，可能具有不同的架构和属性，与一个门控网络一起初始化。所有模型接收相同的输入，门控网络生成的权重用于聚合结果。图示由作者提供
    [2]。
- en: Unlike traditional PINNs, which use a single neural network to make predictions,
    MoE-PINNs employ an ensemble of multiple PINNs that are combined using a gating
    network.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的 PINNs 使用单一神经网络进行预测不同，MoE-PINNs 采用多个 PINNs 的集成，并通过门控网络进行组合。
- en: Just like all PINNs in the ensemble, the gating network is a fully-connected
    feed-forward network that takes in the spatial coordinates **x** and **y** (may
    differ from PDE to PDE). However, unlike the PINNs, its output is an *m*-dimensional
    vector of importances, where *m* is the number of PINNs in the ensemble. The importances
    are passed through a softmax function to convert them into a probability distribution,
    ensuring that the sum of all importances equals 1.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 与集成中的所有 PINNs 一样，门控网络是一个全连接的前馈网络，它接收空间坐标 **x** 和 **y**（可能会因 PDE 而异）。然而，与 PINNs
    不同的是，它的输出是一个 *m* 维的权重向量，其中 *m* 是集成中的 PINNs 数量。这些权重经过 softmax 函数处理，以转换为概率分布，确保所有权重之和等于
    1。
- en: The softmax function, mapping the unnormalised importances z_i to a probability
    distribution p.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: softmax 函数将未归一化的重要性 z_i 映射到概率分布 p。
- en: The final prediction of the MoE-PINN is obtained by aggregating the predictions
    of each PINN in the ensemble, weighted by their respective importances.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: MoE-PINN 的最终预测是通过聚合集成中每个 PINN 的预测结果获得的，权重由各自的重要性决定。
- en: 'MoE-PINNs can be very concisely built in TensorFlow with just a few lines of
    code:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: MoE-PINNs 可以在 TensorFlow 中非常简洁地构建，只需几行代码：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Burgers’ Equation
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Burgers’ 方程
- en: 'To illustrate the effectiveness of MoE-PINNs, let us have a look at an example.
    The Burgers’ equation is a PDE used to model phenomenons like shock waves, gas
    dynamics, or traffic flow. It takes the following form:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明 MoE-PINNs 的有效性，我们来看一个例子。Burgers’ 方程是一种用于建模冲击波、气体动力学或交通流等现象的 PDE。其形式如下：
- en: Burgers’ equation, with spatial coordinate x and temporal variable t. The initial
    condition is set to a sine function and the boundary conditions to zero.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Burgers’ 方程，空间坐标 x 和时间变量 t。初始条件设定为正弦函数，边界条件设定为零。
- en: '![](../Images/de56dc6570512a57c6d1a68851c8b863.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de56dc6570512a57c6d1a68851c8b863.png)'
- en: Visualisation of Burgers’ equation with the above initial and boundary conditions.
    Image by author.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述初始条件和边界条件可视化 Burgers’ 方程。图片由作者提供。
- en: 'Burgers’ PDE presents an intriguing challenge: it transitions from a smooth
    sine function to a sharp, step-like discontinuity over time. This unique property
    makes it a perfect benchmark for evaluating the performance of MoE-PINNs.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Burgers’ PDE 提出了一个有趣的挑战：它随着时间的推移，从平滑的正弦函数过渡到陡峭的阶跃型不连续性。这一独特属性使其成为评估 MoE-PINNs
    性能的理想基准。
- en: Training MoE-PINNs on Burgers’ equation
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练 MoE-PINNs 以解决 Burgers’ 方程
- en: 'Let us initialise a MoE-PINN with 5 PINNs as well as a gating network and train
    it on Burgers’ equation. The experts have the following architectures:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们初始化一个包含 5 个 PINNs 和一个门控网络的 MoE-PINN，并在 Burgers 方程上进行训练。这些专家具有以下架构：
- en: 'Expert 1: 2 layers with 64 nodes each and tanh activation'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '专家 1: 2 层，每层 64 个节点，激活函数为 tanh'
- en: 'Expert 2: 2 layers with 64 nodes each and sine activation'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '专家 2: 2 层，每层 64 个节点，激活函数为 sine'
- en: 'Expert 3: 2 layers with 128 nodes each and tanh activation'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '专家 3: 2 层，每层 128 个节点，激活函数为 tanh'
- en: 'Expert 4: 3 layers with 128 nodes each and tanh activation'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '专家 4: 3 层，每层 128 个节点，激活函数为 tanh'
- en: 'Expert 5: 2 layers with 256 nodes each and swish activation'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '专家 5: 2 层，每层 256 个节点，激活函数为 swish'
- en: '![](../Images/0b6c67eacb119f4754654bd2e62ac202.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0b6c67eacb119f4754654bd2e62ac202.png)'
- en: Prediction and squared error against the ground truth (Spectral Elements Method)
    on Burgers’ equation using a MoE-PINN with 5 experts. Image by author.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 5 个专家的 MoE-PINN 对 Burgers 方程进行预测和与实际结果（谱元方法）的平方误差比较。图像来源：作者。
- en: 'More interestingly we can now inspect how the experts were distributed over
    the domain and what their individual predictions look like:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 更有趣的是，我们现在可以检查专家在领域中的分布以及它们的单独预测情况：
- en: '![](../Images/fd5ef88b6c7b86bacb94be882d6d313f.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd5ef88b6c7b86bacb94be882d6d313f.png)'
- en: Weights lambda produced by the gating network (top row) for each of the expert
    (columns) as well as the predictions from each expert (bottom row) for Burgers’
    equation. Image by author.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 由门控网络产生的权重 λ（顶部行）对于每个专家（列），以及每个专家对 Burgers 方程的预测（底部行）。图像来源：作者。
- en: Observe how the gating network in MoE-PINNs was able to effectively allocate
    tasks to each expert based on their respective capacity for modelling different
    parts of the domain. The experts with fewer layers and nodes were assigned to
    the smoother regions, which are relatively easier to model, i.e. close to the
    initial conditions. Meanwhile, the more complex experts, i.e. those with deeper
    and wider architectures, were utilised in the regions with discontinuities, where
    a more complex model was necessary for accurate representation. This is particularly
    evident in the case of expert 3, which was solely dedicated to capturing the discontinuity.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 观察 MoE-PINNs 中的门控网络如何根据每个专家在建模领域不同部分的能力有效地分配任务。较少层数和节点的专家被分配到平滑区域，这些区域相对容易建模，即接近初始条件。与此同时，更复杂的专家，即具有更深更宽架构的专家，被利用在具有不连续性的区域，这些区域需要更复杂的模型来准确表示。特别是在专家
    3 的情况下，它完全致力于捕捉不连续性。
- en: Sparsity Regularisation for reduced hyperparameter search
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稀疏性正则化以减少超参数搜索
- en: 'MoE-PINNs reduce the need for tuning multiple hyperparameters by allowing you
    to initialise a diverse group of experts. However, one important hyperparameter
    remains: the number of experts, m. To get the best results, m should be as high
    as possible while still fitting within memory. But a large number of experts also
    comes with increased computational costs. To balance these trade-offs, it is important
    to determine the minimum number of experts needed to divide the physical domain
    optimally. One way to achieve this is by adding a regularisation term that encourages
    sparse weights, lambda, produced by the gating network.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: MoE-PINNs 通过允许初始化一组多样化的专家，减少了调整多个超参数的需求。然而，仍有一个重要的超参数：专家数量 m。为了获得最佳结果，m 应该尽可能高，同时仍能适应内存。但专家数量过多也会增加计算成本。为了平衡这些权衡，重要的是确定将物理领域划分为最佳所需的最小专家数量。一种方法是通过添加一个正则化项来鼓励由门控网络产生的稀疏权重
    λ。
- en: 'The regularisation loss can be expressed as:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化损失可以表示为：
- en: Regularisation term for enforcing sparsity on the weights lambda produced by
    the gating network.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 用于强制门控网络产生的权重 λ 稀疏的正则化项。
- en: where B is a batch of collocation points (x, t), and p is a hyperparameter that
    controls the strength of the regularisation. Values of p below 1 enforce sparsity,
    while values above 1 result in a more uniform distribution. To encourage sparsity,
    a good starting point for p is 0.5.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 B 是一批协同点 (x, t)，p 是控制正则化强度的超参数。p 值低于 1 强制稀疏，而 p 值高于 1 导致更均匀的分布。为了鼓励稀疏性，p
    的一个良好起始点是 0.5。
- en: Finally, to make the training procedure even more efficient, we can use a heuristic
    to drop experts that the gating network deems unimportant. For example, an expert
    can be dropped if its average weight in a batch falls below a certain threshold.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了使训练过程更加高效，我们可以使用启发式方法来排除门控网络认为不重要的专家。例如，如果某专家在一个批次中的平均权重低于某个阈值，则可以排除该专家。
- en: Let us have a look at another example to illustrate this procedure.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个其他的例子来说明这一过程。
- en: Poisson PDE on L-shaped Domain
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: L形区域上的泊松PDE
- en: 'The Poisson equation is a common tool for modelling physical processes in engineering
    and natural sciences. For example, it can be used to solve the elastostatic problem
    of a rod under a torsion load. For testing the sparsity regularisation, let us
    examine how well MoE-PINNs perform when solving the Poisson equation on a 2-dimensional
    L-shaped domain with homogeneous Dirichlet boundary conditions:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 泊松方程是用于建模工程和自然科学中物理过程的常用工具。例如，它可以用来解决杆在扭转负荷下的弹性静力学问题。为了测试稀疏性正则化，让我们检查MoE-PINNs在解决具有均匀Dirichlet边界条件的二维L形区域上的泊松方程时表现如何：
- en: Poisson equation on an L-shaped domain, where Gamma represents the points on
    the boundary around the L-shaped domain.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: L形区域上的泊松方程，其中Gamma代表L形区域边界上的点。
- en: '![](../Images/5fa9c1cbd83688d62e2e6d2f7db1d3e2.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5fa9c1cbd83688d62e2e6d2f7db1d3e2.png)'
- en: Visualisation of the Poisson equation on an L-shaped domain defined above solved
    using Finite Element Method. Image by author [2].
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 上述L形区域上泊松方程的可视化，使用有限元方法解决。图示由作者[2]提供。
- en: 'If an engineer had to subdivide this domain and employ various models, an intuitive
    choice would be to use a different expert in each of the three quadrants in the
    L-shaped domain: one in the top left, one in the bottom left, and one in the bottom
    right corner. It will be interesting to see how the MoE-PINN decides to divide
    the domain and assign experts.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果工程师必须对这个领域进行细分并使用不同的模型，一个直观的选择是在L形区域的三个象限中使用不同的专家：一个在左上角，一个在左下角，一个在右下角。观察MoE-PINN如何决定划分领域并分配专家将会很有趣。
- en: Training sparse MoE-PINNs on the Poisson PDE
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在泊松PDE上训练稀疏MoE-PINNs
- en: 'When initialising an ensemble with four identical experts, the results of the
    MoE-PINN look the following:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当用四个相同的专家初始化一个集合时，MoE-PINN的结果如下：
- en: '![](../Images/adc4ee9ce2dca4ced6ba896e718d5a0c.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/adc4ee9ce2dca4ced6ba896e718d5a0c.png)'
- en: Prediction and squared error against the ground truth (FEM solution) on the
    Poisson equation using a MoE-PINN with 4 experts. Image by author [2].
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用4个专家的MoE-PINN在泊松方程上的预测和平方误差与实际值（FEM解）对比。图示由作者[2]提供。
- en: 'But much more importantly, we can now inspect the importances that were attributed
    to the individual experts:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 但更重要的是，我们现在可以检查每个专家的重要性：
- en: '![](../Images/692d52ab0ba20ba65d1d62aaaa897beb.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/692d52ab0ba20ba65d1d62aaaa897beb.png)'
- en: Weights lambda produced by the gating network (top row) for each of the expert
    (columns) as well as the predictions from each expert (bottom row) for the Poisson
    equation. Figure by author [2].
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 门控网络生成的权重lambda（顶行）以及每个专家（列）的预测（底行）在泊松方程中的表现。图示由作者[2]提供。
- en: The figure shows that the gating network made the decision, under the influence
    of the sparsity regularisation, to almost completely drop expert 1 from the ensemble.
    This resulted in a more efficient and effective division of the domain amongst
    the remaining PINNs. The network assigned one dominant expert to each of the three
    quadrants, creating a symmetrical and intuitive distribution.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图示表明，在稀疏性正则化的影响下，门控网络决定几乎完全排除专家1。这导致了在剩余的PINNs之间更加高效和有效的领域划分。网络将一个主要专家分配给三个象限中的每一个，创建了一个对称且直观的分布。
- en: It is also worth noting that due to the low average importance of expert 1,
    if a new training run was to be initiated, this expert could be dropped from the
    ensemble and the remaining experts could be fine-tuned in a reduced-size, and
    thus more efficient, ensemble.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 同样值得注意的是，由于专家1的平均重要性较低，如果启动新的训练，这个专家可能会被从集合中排除，剩余的专家可以在一个减少规模的、更高效的集合中进行微调。
- en: Differentiable Architecture Search
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可微分架构搜索
- en: Finally, we want to make use of the introduced concepts to reduce the time needed
    for tuning hyperparameters. MoE-PINNs allow to initialise an ensemble of diverse
    experts and let the gating network decide, which experts should be retained and
    which experts could be discarded under the sparsity regularisation.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们希望利用引入的概念来减少调整超参数所需的时间。MoE-PINNs 允许初始化一个多样化的专家集合，并让门控网络决定应该保留哪些专家，哪些专家可以在稀疏正则化下被丢弃。
- en: '![](../Images/d84b7f932f36919d23e34dbaa75f0662.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d84b7f932f36919d23e34dbaa75f0662.png)'
- en: Importance in ensembles of three experts (left) and four experts (right) with
    different activation functions on the Poisson equation. Figure by author [2].
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在对泊松方程的三个专家（左）和四个专家（右）使用不同激活函数时的重要性。图由作者[2]提供。
- en: Surprisingly, when analysing the importance of experts in diverse ensembles
    using different activation functions, the gating network consistently discards
    networks with tanh activations, despite tanh being a commonly used activation
    in PINN literature. Conversely, the gating network consistently favours experts
    with sine activations. This preference suggests that using an ensemble of sine
    activation networks may enhance PINN performance, which aligns with the principles
    of signal decomposition using the Fourier Transform, stating that any function
    can be represented as a combination of sine functions of different frequencies.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 出人意料的是，当分析使用不同激活函数的多样化专家集合时，门控网络始终丢弃使用 tanh 激活的网络，尽管 tanh 是 PINN 文献中常用的激活函数。相反，门控网络始终偏爱使用正弦激活的专家。这种偏好表明，使用正弦激活网络的集合可能会提高
    PINN 的性能，这与使用傅里叶变换进行信号分解的原则一致，表明任何函数都可以表示为不同频率的正弦函数的组合。
- en: '![](../Images/06516fe4efd6b9b4cc4309adfe78cce0.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06516fe4efd6b9b4cc4309adfe78cce0.png)'
- en: Importance in ensembles of four experts when varying the depth (left) and the
    depth (right) of otherwise identical experts on the Poisson equation. Figure by
    author [2].
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在对泊松方程的四个专家进行深度（左）和相同专家的深度（右）变化时的重要性。图由作者[2]提供。
- en: Looking at ensembles with experts of varying depth and width, it appears as
    if a depth of two or three layers may be the optimal choice for PINNs on the poisson
    equation, whereas it seems like wider networks are to be preferred over narrow
    ones.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 从不同深度和宽度的专家集合来看，对于泊松方程，二层或三层的深度似乎是最佳选择，而较宽的网络似乎优于较窄的网络。
- en: Conclusion
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In conclusion, MoE-PINNs are a great extension for improving PINNs on PDEs exhibiting
    varying patterns and for reducing the time needed for tuning hyperparameters by
    letting the gating network decide on which architectures to utilise from a diverse
    set of experts.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，MoE-PINNs 是一种很好的扩展，能改善在表现出不同模式的 PDEs 上的 PINNs，并通过让门控网络决定从多样化的专家集合中使用哪些架构，减少调整超参数所需的时间。
- en: 'If you want to try out MoE-PINNs yourself, have a look at the following notebooks:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想自己试验 MoE-PINNs，可以查看以下笔记本：
- en: '[Notebook for MoE-PINNs on Burgers’ equation](https://drive.google.com/file/d/1JyejLXPS9LQdsNzZE5z-FTd8idRgrP1n/view?usp=sharing)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MoE-PINNs 在 Burgers 方程上的笔记本](https://drive.google.com/file/d/1JyejLXPS9LQdsNzZE5z-FTd8idRgrP1n/view?usp=sharing)'
- en: '[Notebook for MoE-PINNs on the L-shaped Poisson equation](https://drive.google.com/file/d/1IRyZvl9OFU8a0PETjdEEppM1BBoda4e8/view?usp=sharing)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MoE-PINNs 在 L 形泊松方程上的笔记本](https://drive.google.com/file/d/1IRyZvl9OFU8a0PETjdEEppM1BBoda4e8/view?usp=sharing)'
- en: 'Thank you a lot for reading until the end of this article! If you found this
    article helpful and would like to use MoE-PINNs or the notebooks in your own work,
    please use the following citation:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢你读完这篇文章！如果你觉得这篇文章对你有帮助，并希望在自己的工作中使用 MoE-PINNs 或这些笔记本，请使用以下引用：
- en: R. Bischof and M. A. Kraus, “[Mixture-of-Experts-Ensemble Meta-Learning for
    Physics-Informed Neural Networks](https://scholar.google.com/scholar?oi=bibs&cluster=12593425659851579449&btnI=1&hl=en)”,
    Proceedings of 33\. Forum Bauinformatik, 2022
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: R. Bischof 和 M. A. Kraus, “[Mixture-of-Experts-Ensemble Meta-Learning for Physics-Informed
    Neural Networks](https://scholar.google.com/scholar?oi=bibs&cluster=12593425659851579449&btnI=1&hl=en)”，《33rd
    Forum Bauinformatik 会议论文集》，2022
- en: You can find more information about my colleague Michael Kraus on [mkrausai.com](http://mkrausai.com/)
    and about myself on [rabischof.ch](http://rabischof.ch/).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 [mkrausai.com](http://mkrausai.com/) 查找有关我的同事 Michael Kraus 的更多信息，也可以在
    [rabischof.ch](http://rabischof.ch/) 了解我自己。
- en: '[1] M. Raissi, P. Perdikaris, and G. E. Karniadakis, Physics-informed neural
    networks: A deep learning framework for solving forward and inverse problems involving
    nonlinear partial differential equations, Journal of Computational Physics 378
    (2019), 686–707.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] M. Raissi、P. Perdikaris 和 G. E. Karniadakis，《物理信息神经网络：一种解决涉及非线性偏微分方程的前向和逆向问题的深度学习框架》，《计算物理学杂志》378（2019），686–707。'
- en: '[2] R. Bischof and M. A. Kraus, “[Mixture-of-Experts-Ensemble Meta-Learning
    for Physics-Informed Neural Networks](https://scholar.google.com/scholar?oi=bibs&cluster=12593425659851579449&btnI=1&hl=en)”,
    Proceedings of 33\. Forum Bauinformatik, 2022'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] R. Bischof 和 M. A. Kraus，“[物理信息神经网络的专家组合元学习](https://scholar.google.com/scholar?oi=bibs&cluster=12593425659851579449&btnI=1&hl=en)”，第33届建筑信息学论坛论文集，2022'
