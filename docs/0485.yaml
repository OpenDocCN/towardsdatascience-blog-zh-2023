- en: Mixture of Experts for PINNs (MoE-PINNs)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/mixture-of-experts-for-pinns-moe-pinns-6520adf32438?source=collection_archive---------7-----------------------#2023-02-02](https://towardsdatascience.com/mixture-of-experts-for-pinns-moe-pinns-6520adf32438?source=collection_archive---------7-----------------------#2023-02-02)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Utilising Ensembles to enhance Physics-Informed Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://rabischof.medium.com/?source=post_page-----6520adf32438--------------------------------)[![Rafael
    Bischof](../Images/a1d468ea5b61c26a18541f0c0f42c5c6.png)](https://rabischof.medium.com/?source=post_page-----6520adf32438--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6520adf32438--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6520adf32438--------------------------------)
    [Rafael Bischof](https://rabischof.medium.com/?source=post_page-----6520adf32438--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F913c6c1e6a94&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixture-of-experts-for-pinns-moe-pinns-6520adf32438&user=Rafael+Bischof&userId=913c6c1e6a94&source=post_page-913c6c1e6a94----6520adf32438---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6520adf32438--------------------------------)
    ·9 min read·Feb 2, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6520adf32438&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixture-of-experts-for-pinns-moe-pinns-6520adf32438&user=Rafael+Bischof&userId=913c6c1e6a94&source=-----6520adf32438---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6520adf32438&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixture-of-experts-for-pinns-moe-pinns-6520adf32438&source=-----6520adf32438---------------------bookmark_footer-----------)![](../Images/5d1b9499921aea88cd946c6f39a435d7.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Soviet Artefacts](https://unsplash.com/@sovietartefacts?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '[Physics-Informed Neural Networks (PINNs)](https://www.sciencedirect.com/science/article/pii/S0021999118307125)
    [1] have emerged as a popular and promising approach for solving partial differential
    equations (PDEs). In our latest research, my colleague Michael Kraus and I introduce
    a new framework called [Mixture of Experts for Physics-Informed Neural Networks
    (MoE-PINNs)](https://mediatum.ub.tum.de/doc/1688403/uic8b0xn1c845e7rac1or092o.Bischof%20et%20Al.%202022.pdf)
    [2], which shows great potential on PDEs with complex and varied patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this post, we will discuss the benefits of MoE-PINNs and how they can be
    easily implemented to solve a wide range of PDE problems. It is structured the
    following way:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to MoE-PINNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving Burgers’ PDE using MoE-PINNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing Hyperparameter Search with Sparsity Regularisation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example of Sparsity Regularisation by solving the Poisson PDE on an L-shaped
    Domain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automated Architecture Search with Differentiable MoE-PINNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To help you gain a better understanding of the concepts discussed in this article,
    we have provided accompanying notebooks that can be run directly on Colab:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Notebook for MoE-PINNs on Burgers’ equation](https://drive.google.com/file/d/1JyejLXPS9LQdsNzZE5z-FTd8idRgrP1n/view?usp=sharing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Notebook for MoE-PINNs on the L-shaped Poisson equation](https://drive.google.com/file/d/1IRyZvl9OFU8a0PETjdEEppM1BBoda4e8/view?usp=sharing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PINNs use physical laws and automatic differentiation to solve Partial Differential
    Equations (PDEs) with just a few lines of code. However, they are also highly
    susceptible to their hyperparameters, such as the activation function or initialisation
    of the weights. This makes training PINNs particularly difficult and a laborious,
    iterative process. The depth of the network and the choice of activation function
    can greatly impact the solution accuracy. For example, deep networks work well
    for complex PDEs with varying patterns and discontinuities, such as Navier-Stokes
    equations. Meanwhile, shallow networks may suffice for simple PDEs with simpler
    patterns, like the Poisson equation on a square domain. The sine activation function,
    with its property of preserving shape under differentiation, may be ideal for
    high-order differentiation problems. On the other hand, activation functions like
    swish or softplus may perform better for problems with sharp discontinuities.
  prefs: []
  type: TYPE_NORMAL
- en: But what if your problem requires a combination of both? How do we handle a
    PDE that is smooth and repeating in one part, and highly complex with sharp discontinuities
    in another? This is where the **Mixture of Experts (MoE)** framework for PINNs
    comes in. By leveraging multiple networks and a gate to divide the domain, each
    expert can specialise in solving a distinct part of the problem, allowing for
    improved accuracy and reduced bias-variance trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dividing the problem into smaller sub-problems has many advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: By using several learners on distinct sub-domains, the complexity of the problem
    is reduced.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The gate in MoE-PINNs is a continuous function, resulting in smooth transitions
    between the domains. Therefore, more complex regions of the domain can be equally
    divided amongst several learners, whereas simpler regions can be attributed to
    a single expert.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The gate can be any type of neural network, from a linear layer to a deep neural
    network, which allows it to adapt to different types of domains and divide them
    in an arbitrary way.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MoE can be easily parallelised, since only the weights lambda need to be shared
    amongst learners on different devices. In theory, each learner could be placed
    on a distinct GPU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By initialising a large number of PINNs with different architectures, the need
    for labor-intensive hyperparameter tuning is reduced.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MoE-PINN Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/bf52d2dd67e79e742c039a1b83e89112.png)'
  prefs: []
  type: TYPE_IMG
- en: Mixture of Experts of PINNs. An arbitrary number m of PINNs, possibly with varying
    architectures and properties, is initialised together with a gating network. All
    models receive the same input and the gate produces weights that are used for
    aggregating the results. Figure by author [2].
  prefs: []
  type: TYPE_NORMAL
- en: Unlike traditional PINNs, which use a single neural network to make predictions,
    MoE-PINNs employ an ensemble of multiple PINNs that are combined using a gating
    network.
  prefs: []
  type: TYPE_NORMAL
- en: Just like all PINNs in the ensemble, the gating network is a fully-connected
    feed-forward network that takes in the spatial coordinates **x** and **y** (may
    differ from PDE to PDE). However, unlike the PINNs, its output is an *m*-dimensional
    vector of importances, where *m* is the number of PINNs in the ensemble. The importances
    are passed through a softmax function to convert them into a probability distribution,
    ensuring that the sum of all importances equals 1.
  prefs: []
  type: TYPE_NORMAL
- en: The softmax function, mapping the unnormalised importances z_i to a probability
    distribution p.
  prefs: []
  type: TYPE_NORMAL
- en: The final prediction of the MoE-PINN is obtained by aggregating the predictions
    of each PINN in the ensemble, weighted by their respective importances.
  prefs: []
  type: TYPE_NORMAL
- en: 'MoE-PINNs can be very concisely built in TensorFlow with just a few lines of
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Burgers’ Equation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To illustrate the effectiveness of MoE-PINNs, let us have a look at an example.
    The Burgers’ equation is a PDE used to model phenomenons like shock waves, gas
    dynamics, or traffic flow. It takes the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: Burgers’ equation, with spatial coordinate x and temporal variable t. The initial
    condition is set to a sine function and the boundary conditions to zero.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/de56dc6570512a57c6d1a68851c8b863.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualisation of Burgers’ equation with the above initial and boundary conditions.
    Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Burgers’ PDE presents an intriguing challenge: it transitions from a smooth
    sine function to a sharp, step-like discontinuity over time. This unique property
    makes it a perfect benchmark for evaluating the performance of MoE-PINNs.'
  prefs: []
  type: TYPE_NORMAL
- en: Training MoE-PINNs on Burgers’ equation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let us initialise a MoE-PINN with 5 PINNs as well as a gating network and train
    it on Burgers’ equation. The experts have the following architectures:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Expert 1: 2 layers with 64 nodes each and tanh activation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Expert 2: 2 layers with 64 nodes each and sine activation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Expert 3: 2 layers with 128 nodes each and tanh activation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Expert 4: 3 layers with 128 nodes each and tanh activation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Expert 5: 2 layers with 256 nodes each and swish activation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/0b6c67eacb119f4754654bd2e62ac202.png)'
  prefs: []
  type: TYPE_IMG
- en: Prediction and squared error against the ground truth (Spectral Elements Method)
    on Burgers’ equation using a MoE-PINN with 5 experts. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'More interestingly we can now inspect how the experts were distributed over
    the domain and what their individual predictions look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fd5ef88b6c7b86bacb94be882d6d313f.png)'
  prefs: []
  type: TYPE_IMG
- en: Weights lambda produced by the gating network (top row) for each of the expert
    (columns) as well as the predictions from each expert (bottom row) for Burgers’
    equation. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Observe how the gating network in MoE-PINNs was able to effectively allocate
    tasks to each expert based on their respective capacity for modelling different
    parts of the domain. The experts with fewer layers and nodes were assigned to
    the smoother regions, which are relatively easier to model, i.e. close to the
    initial conditions. Meanwhile, the more complex experts, i.e. those with deeper
    and wider architectures, were utilised in the regions with discontinuities, where
    a more complex model was necessary for accurate representation. This is particularly
    evident in the case of expert 3, which was solely dedicated to capturing the discontinuity.
  prefs: []
  type: TYPE_NORMAL
- en: Sparsity Regularisation for reduced hyperparameter search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MoE-PINNs reduce the need for tuning multiple hyperparameters by allowing you
    to initialise a diverse group of experts. However, one important hyperparameter
    remains: the number of experts, m. To get the best results, m should be as high
    as possible while still fitting within memory. But a large number of experts also
    comes with increased computational costs. To balance these trade-offs, it is important
    to determine the minimum number of experts needed to divide the physical domain
    optimally. One way to achieve this is by adding a regularisation term that encourages
    sparse weights, lambda, produced by the gating network.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The regularisation loss can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: Regularisation term for enforcing sparsity on the weights lambda produced by
    the gating network.
  prefs: []
  type: TYPE_NORMAL
- en: where B is a batch of collocation points (x, t), and p is a hyperparameter that
    controls the strength of the regularisation. Values of p below 1 enforce sparsity,
    while values above 1 result in a more uniform distribution. To encourage sparsity,
    a good starting point for p is 0.5.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, to make the training procedure even more efficient, we can use a heuristic
    to drop experts that the gating network deems unimportant. For example, an expert
    can be dropped if its average weight in a batch falls below a certain threshold.
  prefs: []
  type: TYPE_NORMAL
- en: Let us have a look at another example to illustrate this procedure.
  prefs: []
  type: TYPE_NORMAL
- en: Poisson PDE on L-shaped Domain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Poisson equation is a common tool for modelling physical processes in engineering
    and natural sciences. For example, it can be used to solve the elastostatic problem
    of a rod under a torsion load. For testing the sparsity regularisation, let us
    examine how well MoE-PINNs perform when solving the Poisson equation on a 2-dimensional
    L-shaped domain with homogeneous Dirichlet boundary conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: Poisson equation on an L-shaped domain, where Gamma represents the points on
    the boundary around the L-shaped domain.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5fa9c1cbd83688d62e2e6d2f7db1d3e2.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualisation of the Poisson equation on an L-shaped domain defined above solved
    using Finite Element Method. Image by author [2].
  prefs: []
  type: TYPE_NORMAL
- en: 'If an engineer had to subdivide this domain and employ various models, an intuitive
    choice would be to use a different expert in each of the three quadrants in the
    L-shaped domain: one in the top left, one in the bottom left, and one in the bottom
    right corner. It will be interesting to see how the MoE-PINN decides to divide
    the domain and assign experts.'
  prefs: []
  type: TYPE_NORMAL
- en: Training sparse MoE-PINNs on the Poisson PDE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When initialising an ensemble with four identical experts, the results of the
    MoE-PINN look the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/adc4ee9ce2dca4ced6ba896e718d5a0c.png)'
  prefs: []
  type: TYPE_IMG
- en: Prediction and squared error against the ground truth (FEM solution) on the
    Poisson equation using a MoE-PINN with 4 experts. Image by author [2].
  prefs: []
  type: TYPE_NORMAL
- en: 'But much more importantly, we can now inspect the importances that were attributed
    to the individual experts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/692d52ab0ba20ba65d1d62aaaa897beb.png)'
  prefs: []
  type: TYPE_IMG
- en: Weights lambda produced by the gating network (top row) for each of the expert
    (columns) as well as the predictions from each expert (bottom row) for the Poisson
    equation. Figure by author [2].
  prefs: []
  type: TYPE_NORMAL
- en: The figure shows that the gating network made the decision, under the influence
    of the sparsity regularisation, to almost completely drop expert 1 from the ensemble.
    This resulted in a more efficient and effective division of the domain amongst
    the remaining PINNs. The network assigned one dominant expert to each of the three
    quadrants, creating a symmetrical and intuitive distribution.
  prefs: []
  type: TYPE_NORMAL
- en: It is also worth noting that due to the low average importance of expert 1,
    if a new training run was to be initiated, this expert could be dropped from the
    ensemble and the remaining experts could be fine-tuned in a reduced-size, and
    thus more efficient, ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: Differentiable Architecture Search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, we want to make use of the introduced concepts to reduce the time needed
    for tuning hyperparameters. MoE-PINNs allow to initialise an ensemble of diverse
    experts and let the gating network decide, which experts should be retained and
    which experts could be discarded under the sparsity regularisation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d84b7f932f36919d23e34dbaa75f0662.png)'
  prefs: []
  type: TYPE_IMG
- en: Importance in ensembles of three experts (left) and four experts (right) with
    different activation functions on the Poisson equation. Figure by author [2].
  prefs: []
  type: TYPE_NORMAL
- en: Surprisingly, when analysing the importance of experts in diverse ensembles
    using different activation functions, the gating network consistently discards
    networks with tanh activations, despite tanh being a commonly used activation
    in PINN literature. Conversely, the gating network consistently favours experts
    with sine activations. This preference suggests that using an ensemble of sine
    activation networks may enhance PINN performance, which aligns with the principles
    of signal decomposition using the Fourier Transform, stating that any function
    can be represented as a combination of sine functions of different frequencies.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/06516fe4efd6b9b4cc4309adfe78cce0.png)'
  prefs: []
  type: TYPE_IMG
- en: Importance in ensembles of four experts when varying the depth (left) and the
    depth (right) of otherwise identical experts on the Poisson equation. Figure by
    author [2].
  prefs: []
  type: TYPE_NORMAL
- en: Looking at ensembles with experts of varying depth and width, it appears as
    if a depth of two or three layers may be the optimal choice for PINNs on the poisson
    equation, whereas it seems like wider networks are to be preferred over narrow
    ones.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In conclusion, MoE-PINNs are a great extension for improving PINNs on PDEs exhibiting
    varying patterns and for reducing the time needed for tuning hyperparameters by
    letting the gating network decide on which architectures to utilise from a diverse
    set of experts.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to try out MoE-PINNs yourself, have a look at the following notebooks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Notebook for MoE-PINNs on Burgers’ equation](https://drive.google.com/file/d/1JyejLXPS9LQdsNzZE5z-FTd8idRgrP1n/view?usp=sharing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Notebook for MoE-PINNs on the L-shaped Poisson equation](https://drive.google.com/file/d/1IRyZvl9OFU8a0PETjdEEppM1BBoda4e8/view?usp=sharing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thank you a lot for reading until the end of this article! If you found this
    article helpful and would like to use MoE-PINNs or the notebooks in your own work,
    please use the following citation:'
  prefs: []
  type: TYPE_NORMAL
- en: R. Bischof and M. A. Kraus, “[Mixture-of-Experts-Ensemble Meta-Learning for
    Physics-Informed Neural Networks](https://scholar.google.com/scholar?oi=bibs&cluster=12593425659851579449&btnI=1&hl=en)”,
    Proceedings of 33\. Forum Bauinformatik, 2022
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You can find more information about my colleague Michael Kraus on [mkrausai.com](http://mkrausai.com/)
    and about myself on [rabischof.ch](http://rabischof.ch/).
  prefs: []
  type: TYPE_NORMAL
- en: '[1] M. Raissi, P. Perdikaris, and G. E. Karniadakis, Physics-informed neural
    networks: A deep learning framework for solving forward and inverse problems involving
    nonlinear partial differential equations, Journal of Computational Physics 378
    (2019), 686–707.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] R. Bischof and M. A. Kraus, “[Mixture-of-Experts-Ensemble Meta-Learning
    for Physics-Informed Neural Networks](https://scholar.google.com/scholar?oi=bibs&cluster=12593425659851579449&btnI=1&hl=en)”,
    Proceedings of 33\. Forum Bauinformatik, 2022'
  prefs: []
  type: TYPE_NORMAL
