- en: Cᵥ Topic Coherence Explained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/c%E1%B5%A5-topic-coherence-explained-fc70e2a85227?source=collection_archive---------3-----------------------#2023-01-12](https://towardsdatascience.com/c%E1%B5%A5-topic-coherence-explained-fc70e2a85227?source=collection_archive---------3-----------------------#2023-01-12)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/b2a235b931d6cc582afd7601f12f3759.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Emile Perron](https://unsplash.com/@emilep?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the metric that correlates the highest with humans
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://emilrijcken.medium.com/?source=post_page-----fc70e2a85227--------------------------------)[![Emil
    Rijcken](../Images/d79e867934f45729e6590a20dcf0a440.png)](https://emilrijcken.medium.com/?source=post_page-----fc70e2a85227--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fc70e2a85227--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fc70e2a85227--------------------------------)
    [Emil Rijcken](https://emilrijcken.medium.com/?source=post_page-----fc70e2a85227--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F95ae6f4e7791&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fc%E1%B5%A5-topic-coherence-explained-fc70e2a85227&user=Emil+Rijcken&userId=95ae6f4e7791&source=post_page-95ae6f4e7791----fc70e2a85227---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fc70e2a85227--------------------------------)
    ·7 min read·Jan 12, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffc70e2a85227&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fc%25E1%25B5%25A5-topic-coherence-explained-fc70e2a85227&user=Emil+Rijcken&userId=95ae6f4e7791&source=-----fc70e2a85227---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffc70e2a85227&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fc%25E1%25B5%25A5-topic-coherence-explained-fc70e2a85227&source=-----fc70e2a85227---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: In natural language processing (NLP), topic modeling is a popular task. The
    goal is to extract the hidden (*K*) topics in a corpus of documents. Each topic
    is a distribution over words. Typically, the *N* most probable words per topic
    represent that topic. The idea is that if the topic modeling algorithm works well,
    these top-*N* words are semantically related. The difficulty is how to evaluate
    these sets of words. Just as with any machine learning task, model evaluation
    is critical. In a large systematic study (Röder et al., 2015), Cᵥ coherence, which
    was unknown until then, was found to correlate the highest with human interpretation.
    Since then, this measure has been widely used to evaluate topic models and is
    the default setting in Gensim’s Coherencemodel. I couldn’t find an intuitive algorithm
    description online, so I wrote one. Many data scientists work with topic modeling
    to analyse their texts. Understanding the evaluation metrics will help in tuning
    both the model and evaluation metric.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by discussing some background information. This will lead the
    way to the study by Röder et al.. After discussing the study approach, we will
    go through the algorithm mathematically. Then, we will walk through the steps
    with an example (about roses and violets). Lastly, I will share some observations
    and considerations when using theCᵥ coherence.
  prefs: []
  type: TYPE_NORMAL
- en: Background
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A long-standing problem with many potential applications far beyond NLP is
    quantifying the coherence of a set of statements. Quantitative approaches in NLP
    are typically inspired by the *distributional hypothesis*. Several researchers
    in the 1950s (Martin Joos, Zellig S. Harris and John Rupert Firth) found that
    synonyms (e.g. oculist and eye-doctor) tended to occur in the same environment
    of words (near words such as ‘eye’ or ‘examined’). Hence, the distributional hypothesis
    states that words in the same contexts tend to have similar meanings. For this
    reason, the word context in a corpus is often used to assess whether a set of
    topic words are coherent. However, there are many ways to obtain information about
    ‘the context’ and reflect this in a single score. E.g.:'
  prefs: []
  type: TYPE_NORMAL
- en: Do you compare (all) single topic words or sets?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you consider co-occurrence, do you consider this for a whole document or
    only a specific window of words? If you choose a window, how many words are in
    the window?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given a comparison of words/word sets, how do you indicate how strongly these
    are semantically related?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having collected the scores for all topics, how do you aggregate these numbers
    to reflect one score only?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the Space of Topic Coherence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recognizing the considerations from above, Röder et al. (2015) define four
    dimensions that span the configuration space of coherence measures. Or in layman''s
    terms, they define coherence scores as a combination of the four considerations
    listed above. They call these dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: Segmentation of word subsets,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Probability estimation,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Confirmation measure,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aggregation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, they define potential settings for each of these dimensions. Based on
    these settings, they can make 237 912 combinations. They calculate the correlation
    between human evaluation scores of given topics and each combination. From this
    systematic study, an unknown measure appeared to have the highest correlation;
    Cᵥ coherence.
  prefs: []
  type: TYPE_NORMAL
- en: Cᵥ Coherence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In cᵥ coherence, each topic word is compared with the set of all topics. A boolean
    sliding window of size 110 is used to assess whether two words co-occur. Then,
    the confirmation measure consists of direct- and indirect confirmations. For all
    *N* most probable words per topic, a ‘*word vector*’of size *N* is created in
    which each cell contains the Normalized Pointwise Mutual Information (NPMI) between
    that word and word *i, i* in{1,2,…, *N*}. Then, all the word vectors in a topic
    are aggregated into one big topic vector. The average of all the cosine similarities
    between each topic word and its topic vector (this is the segmentation) is used
    to calculate the Cᵥ score.
  prefs: []
  type: TYPE_NORMAL
- en: Now comes a formal description. Since Medium does not do so well with mathematical
    notation, I am pasting some notation in here from another document my group and
    I created.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a80e0cc020b266dbb041f94ff7745ff9.png)'
  prefs: []
  type: TYPE_IMG
- en: The Cᵥ score is heavily based on the NPMI score, an advanced way to calculate
    the probability of two words co-occurring in a corpus.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/855297eb491e86340f06895325b6612a.png)'
  prefs: []
  type: TYPE_IMG
- en: The formula for Normalized Pointwise Mutual Information
  prefs: []
  type: TYPE_NORMAL
- en: The epsilon is a small constant used to avoid a logarithm of zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'This probability is based on a sliding window *s* (*s* = 110 for Cᵥ). With
    *j* being the index of the sliding window in a document, the probabilities in
    the NPMI formula are calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a8e483968223dd7095d4d0069f074696.png)'
  prefs: []
  type: TYPE_IMG
- en: The calculation of probabilities between two words, based on a sliding window
    s
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the NPMI, a word vector with length N is created for each topic word
    (direct confirmation):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/870c8dbf7e327fb3234c8d3dd7eeed1d.png)'
  prefs: []
  type: TYPE_IMG
- en: The creation of a word vector (direct confirmation measure)
  prefs: []
  type: TYPE_NORMAL
- en: 'For the segmentation, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e422002e97514051765c0f2315ced5d5.png)'
  prefs: []
  type: TYPE_IMG
- en: The segmentation of word subsets
  prefs: []
  type: TYPE_NORMAL
- en: 'To compare each word with the topic vector, we create *K* topic vectors as
    the sum of all *N* words per topic (mind you that the *W** refers to the same
    *w** as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d4e7fe411c49409fcc77131432100db7.png)'
  prefs: []
  type: TYPE_IMG
- en: The topic vector
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the topic vector and segmentation, the cosine similarity is calculated
    for each topic word vector with the topic vector. The cosine similarity is calculated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7b70692d75a7648de8c22d8ed9227460.png)'
  prefs: []
  type: TYPE_IMG
- en: The cosine similarity
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, the average of all *NxK* cosine similarities is taken to calculate the
    Cᵥ score:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fbf5008b2e3912c83d2fb498da74f162.png)'
  prefs: []
  type: TYPE_IMG
- en: The Cᵥ score is the average of all cosine similarities.
  prefs: []
  type: TYPE_NORMAL
- en: The beautiful thing about understanding complex things is that once you understand
    them, they seem not so complex anymore. Having made it to the end of this blog,
    I hope this is the case for you. If not, we will go to plan B. Let’s walk through
    an example.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/952fe040185449e0fd4a7f24509b5d32.png)'
  prefs: []
  type: TYPE_IMG
- en: Furthermore, we have three topics, with two most probable words per topic (hence,
    *K = 3, N = 2):*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e0df271229a04581094e6eedb308609b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using a sliding window size 3 (*s = 3*), we have the following windows:'
  prefs: []
  type: TYPE_NORMAL
- en: (’mary’,’had’,’a’), (’had’,’a’,’lamp’), (’mary’,’has’,’roses’), (’roses’,’are’,’red’),
    (’violets’,’ are’,’light’),(’are’,’light’,’blue’).
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/769c3e47fcc8862e17b9cfcf920455ef.png)![](../Images/09f7556721628387e48cd75b682e60f1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Based on these scores, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d9934e8aae383edff8764d9e45009fdb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Hence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9e015ada0ec0917216a8083b8327adf0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c781c88c3ba8b25a616fbb058ead7992.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This gives us:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c425bc9d8a02ba16b865419e75b665d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So that the Cᵥ score for these topics is calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c865146708ef77e4139143465dc16e5.png)'
  prefs: []
  type: TYPE_IMG
- en: Cᵥonsiderations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although Cᵥ came out as the best coherence score in Röder et al.’s study, there
    are some issues in practice. The Cᵥ score negatively correlates with other coherence
    measures in [some experiments](https://github.com/dice-group/Palmetto/issues/12).
    However, the author suspects this issue might be caused by the value for epsilon,
    and the error is not further investigated in the thread. Neither am I aware of
    any peer-reviewed work discussing the issue. My conclusion is that until proven
    guilty, Cᵥ remains innocent. But it is good to be aware of this issue. Also, word-embedding-based
    topic models are commonly used now. The author claims these correlate to human
    judgment, similar to the Cᵥ score. However, in [one of my studies](https://ieeexplore.ieee.org/document/9945594),
    my group and I found different (embedding-based-) coherence scores favoured different
    topics. Hence, the last word about topic coherence remains to be spoken.
  prefs: []
  type: TYPE_NORMAL
- en: If you like this work, you are likely interested in topic modeling. In that
    case, you might be interested in the following as well.
  prefs: []
  type: TYPE_NORMAL
- en: '**We have created a new topic modeling algorithm called FLSA-W** (the official
    page is [here](https://ieeexplore.ieee.org/abstract/document/9660139), but you
    can see the paper [here](https://pure.tue.nl/ws/portalfiles/portal/243684581/A_Comparative_Study_of_Fuzzy_Topic_Models_and_LDA_in_terms_of_Interpretability.pdf)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[**FLSA-W outperforms other state-of-the-art algorithms (such as LDA, ProdLDA,
    NMF, CTM and more) on several open datasets.**](https://pure.tue.nl/ws/files/222725628/Pure_ExperimentalStudyOfFlsa_wForTopicModeling.pdf)
    **This work has been submitted but is not peer-reviewed yet.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**If you want to use FLSA-W, you can download** [**the FuzzyTM package**](https://pypi.org/project/FuzzyTM/)
    **or the flsamodel in Gensim.** For citations, [please use this paper](https://ieeexplore.ieee.org/abstract/document/9882661?casa_token=UsYg7SvoSioAAAAA%3A3ltCVZexA9-lPveuGVeRDh5VQW6rw0pVRDxmYk39tXbx13u4OuB2sTEFZzIGJCkdRiZBg0eJ).'
  prefs: []
  type: TYPE_NORMAL
- en: Let me know if you have any questions or remarks.
  prefs: []
  type: TYPE_NORMAL
