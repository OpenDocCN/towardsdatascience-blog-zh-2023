["```py\ndependencies:\n  - python=3.10.1.0\n  - pip:\n    - semantic-kernel==0.9.3b1\n    - timm==0.9.5\n    - transformers==4.38.2\n    - sentence-transformers==2.2.2\n    - curated-transformers==1.1.0\n```", "```py\nimport semantic_kernel as sk\n\nkernel = sk.Kernel()\n```", "```py\nfrom semantic_kernel.connectors.ai.open_ai import (\n    AzureChatCompletion,\n    AzureTextCompletion,\n)\n\nkernel.add_service(\n    service=AzureTextCompletion(\n        service_id=\"azure_gpt35_text_completion\",\n        deployment_name=OPENAI_DEPLOYMENT_NAME,\n        endpoint=OPENAI_ENDPOINT,\n        api_key=OPENAI_API_KEY\n    ),\n)\n\ngpt35_chat_service = AzureChatCompletion(\n    service_id=\"azure_gpt35_chat_completion\",\n    deployment_name=OPENAI_DEPLOYMENT_NAME,\n    endpoint=OPENAI_ENDPOINT,\n    api_key=OPENAI_API_KEY,\n)\n\nkernel.add_service(gpt35_chat_service)\n```", "```py\nprompt = \"\"\"\n{{$input}} is the capital city of\n\"\"\" \n```", "```py\nfrom semantic_kernel.connectors.ai.open_ai import OpenAITextPromptExecutionSettings\n\nexecution_config = OpenAITextPromptExecutionSettings(service_id = \"azure_gpt35_text_completion\",\n                                                    max_tokens=100,\n                                                    temperature=0,\n                                                    top_p=0.0)\n```", "```py\ntarget_service_id = \"azure_gpt35_text_completion\"\n\nexecution_config = kernel.get_service(target_service_id).instantiate_prompt_execution_settings(\n        service_id=target_service_id,\n        max_tokens=100,\n        temperature=0,\n        seed=42\n    )\n```", "```py\ngenerate_capital_city_text = kernel.create_function_from_prompt(\n    prompt=prompt,\n    plugin_name=\"Generate_Capital_City_Completion\",\n    function_name=\"generate_city_completion\",\n    execution_settings=execution_config\n)\n```", "```py\nresponse = await kernel.invoke(generate_capital_city_text, input=\"Paris\")\n```", "```py\nfrom semantic_kernel.connectors.ai.hugging_face import HuggingFaceTextCompletion\n\nhf_model = HuggingFaceTextCompletion(service_id=\"hf_text_completion\", ai_model_id=\"facebook/opt-350m\", task=\"text-generation\")\nkernel.add_service(hf_model)\n```", "```py\ntarget_service_id = \"hf_text_completion\"\n\nexecution_config = kernel.get_service(target_service_id).instantiate_prompt_execution_settings(\n        service_id=target_service_id,\n        max_tokens=100,\n        temperature=0,\n        seed=42\n    )\n```", "```py\nhf_complete = kernel.create_function_from_prompt(\n    prompt=prompt,\n    plugin_name=\"Generate_Capital_City_Completion\",\n    function_name=\"generate_city_completion_opt\",\n    execution_settings=execution_config\n)\n\nresponse = await kernel.invoke(hf_complete, input='Paris')\n```", "```py\nfrom typing import Any, Dict, List, Optional, Union\n\nimport torch\nfrom curated_transformers.generation import (AutoGenerator,\n                                             SampleGeneratorConfig)\nfrom semantic_kernel.connectors.ai.prompt_execution_settings import \\\n    PromptExecutionSettings\nfrom semantic_kernel.connectors.ai.text_completion_client_base import \\\n    TextCompletionClientBase\n\nclass CuratedTransformersPromptExecutionSettings(PromptExecutionSettings):\n    temperature: float = 0.0\n    top_p: float = 1.0\n\n    def prepare_settings_dict(self, **kwargs) -> Dict[str, Any]:\n        settings = {\n            \"temperature\": self.temperature,\n            \"top_p\": self.top_p,\n        }\n        settings.update(kwargs)\n        return settings\n\nclass CuratedTransformersCompletion(TextCompletionClientBase):\n    device: Any\n    generator: Any\n\n    def __init__(\n        self,\n        service_id: str,\n        model_name: str,\n        device: Optional[int] = -1,\n    ) -> None:\n        \"\"\"\n        Use a curated transformer model for text completion.\n\n        Arguments:\n            model_name {str}\n            device_idx {Optional[int]} -- Device to run the model on, -1 for CPU, 0+ for GPU.\n\n        Note that this model will be downloaded from the Hugging Face model hub.\n        \"\"\"\n        device = (\n            \"cuda:\" + str(device)\n            if device >= 0 and torch.cuda.is_available()\n            else \"cpu\"\n        )\n        generator = AutoGenerator.from_hf_hub(\n            name=model_name, device=torch.device(device)\n        )\n        super().__init__(\n            service_id=service_id,\n            ai_model_id=model_name,\n            device=device,\n            generator=generator,\n        )\n\n    async def complete(\n        self, prompt: str, settings: CuratedTransformersPromptExecutionSettings\n    ) -> Union[str, List[str]]:\n        generator_config = SampleGeneratorConfig(**settings.prepare_settings_dict())\n        try:\n            with torch.no_grad():\n                result = self.generator([prompt], generator_config)\n\n            return result[0]\n\n        except Exception as e:\n            raise ValueError(\"CuratedTransformer completion failed\", e)\n\n    async def complete_stream(self, prompt: str, request_settings):\n        raise NotImplementedError(\n            \"Streaming is not supported for CuratedTransformersCompletion.\"\n        )\n\n    def get_prompt_execution_settings_from_settings(\n        self, settings: CuratedTransformersPromptExecutionSettings\n    ) -> CuratedTransformersPromptExecutionSettings:\n        return settings\n```", "```py\nkernel.add_service(\n        CuratedTransformersCompletion(\n            service_id=\"custom\",\n            model_name=\"tiiuae/falcon-7b\",\n            device=-1,\n        )\n    )\n\ncomplete = kernel.create_function_from_prompt(\n        prompt=prompt,\n        plugin_name=\"Generate_Capital_City_Completion\",\n        function_name=\"generate_city_completion_curated\",\n        prompt_execution_settings=CuratedTransformersPromptExecutionSettings(\n            service_id=\"custom\", temperature=0.0, top_p=0.0\n        ),\n    )\n\nprint(await kernel.invoke(complete, input=\"Paris\"))\n```", "```py\ngenerate_capital_city_chat = kernel.create_function_from_prompt(\n        prompt=prompt,\n        plugin_name=\"Generate_Capital_City\",\n        function_name=\"capital_city_chat_2\",\n        prompt_execution_settings=kernel.get_service(target_service_id).instantiate_prompt_execution_settings(\n            service_id=target_service_id, temperature=0.0, top_p=0.0, seed=42\n        ),\n    )\n\nprint(await kernel.invoke(generate_capital_city_chat, input=\"Paris\"))\n```", "```py\nchatbot = kernel.create_function_from_prompt(\n        prompt=\"{{$input}}\",\n        plugin_name=\"Chatbot\",\n        function_name=\"chatbot\",\n        prompt_execution_settings=kernel.get_service(target_service_id).instantiate_prompt_execution_settings(\n            service_id=target_service_id, temperature=0.0, top_p=0.0, seed=42\n        ),\n    )\n```", "```py\nasync def chat(user_input):\n    print(await kernel.invoke(generate_capital_city_chat, input=user_input))\n```", "```py\nchatbot_prompt = \"\"\"\n\"You are a chatbot to provide information about different cities and countries. \n For other questions not related to places, you should politely decline to answer the question, stating your purpose\"\n +++++\n\n{{$history}}\nUser: {{$input}}\nChatBot: \"\"\"\n```", "```py\nfrom semantic_kernel.prompt_template.input_variable import InputVariable\n\nexecution_config = kernel.get_service(target_service_id).instantiate_prompt_execution_settings(\n        service_id=target_service_id,\n        max_tokens=500,\n        temperature=0,\n        seed=42\n    )\n\nprompt_template_config = sk.PromptTemplateConfig(\n    template=chatbot_prompt,\n    name=\"chat\",\n    template_format=\"semantic-kernel\",\n    input_variables=[\n        InputVariable(name=\"input\", description=\"The user input\", is_required=True),\n        InputVariable(name=\"history\", description=\"The conversation history\", is_required=True),\n    ],\n    execution_settings=execution_config,\n)\n```", "```py\nchatbot = kernel.create_function_from_prompt(\n    function_name=\"chatbot_with_history\",\n    plugin_name=\"chatPlugin\",\n    prompt_template_config=prompt_template_config,\n)\n```", "```py\nfrom semantic_kernel.contents.chat_history import ChatHistory\n\nchat_history = ChatHistory()\n```", "```py\nfrom pprint import pprint\n\nasync def chat(input_text, verbose=True):\n    # Save new message in the context variables\n    context = KernelArguments(user_input=input_text, history=chat_history)\n\n    if verbose:\n        # print the full prompt before each interaction\n        print(\"Prompt:\")\n        print(\"-----\")\n        # inject the variables into our prompt\n        print(await chatbot.prompt_template.render(kernel, context))\n        print(\"-----\")\n\n    # Process the user message and get an answer\n    answer = await kernel.invoke(chatbot, context)\n\n    # Show the response\n    pprint(f\"ChatBot: {answer}\")\n\n    # Append the new interaction to the chat history\n    chat_history.add_user_message(input_text)\n    chat_history.add_assistant_message(str(answer))\n```", "```py\nresponse = chatbot(\n    \"\"\"Please provide a comprehensive overview of things to do in London. Structure your answer in 5 paragraphs, based on:\n- overview\n- landmarks\n- history\n- culture\n- food\n\nEach paragraph should be 100 tokens, do not add titles such as `Overview:` or `Food:` to the paragraphs in your response.\n\nDo not acknowledge the question, with a statement like \"Certainly, here's a comprehensive overview of things to do in London\". \nDo not provide a closing comment.\n\"\"\"\n)\n```", "```py\nfrom semantic_kernel.text import text_chunker as tc\n\nchunks = tc.split_plaintext_paragraph([london_info], max_tokens=100)\n```", "```py\nfrom semantic_kernel.connectors.ai.open_ai import AzureTextEmbedding\n\nembedding_service =  AzureTextEmbedding(\n        service_id=\"azure_openai_embedding\",\n        deployment_name=OPENAI_EMBEDDING_DEPLOYMENT_NAME,\n        endpoint=OPENAI_ENDPOINT,\n        api_key=OPENAI_API_KEY,\n    )\n\nkernel.add_service(embedding_service)\n```", "```py\nmemory_store = sk.memory.VolatileMemoryStore()\n```", "```py\nfrom semantic_kernel.memory.semantic_text_memory import SemanticTextMemory\n\nmemory = SemanticTextMemory(storage=memory_store, embeddings_generator=embedding_service)\n```", "```py\nfor i, chunk in enumerate(chunks):\n    await memory.save_information(\n        collection=\"London\", id=\"chunk\" + str(i), text=chunk\n    )\n```", "```py\nresults = await memory.search(\n    \"London\", \"what food should I eat in London?\", limit=2\n)\n```", "```py\nresults = await memory.search(\n    \"London\", \"Where can I eat non-british food in London?\", limit=2\n)\n```", "```py\nfrom semantic_kernel.connectors.ai.hugging_face import HuggingFaceTextEmbedding\n\nhf_embedding_service = HuggingFaceTextEmbedding(\n    service_id=\"hf_embedding_service\",\n    ai_model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n    device=-1\n)\nhf_memory = SemanticTextMemory(storage=sk.memory.VolatileMemoryStore(), embeddings_generator=hf_embedding_service)\n```", "```py\nfor i, chunk in enumerate(chunks):\n    await kernel.memory.save_information_async(\n        \"hf_London\", id=\"chunk\" + str(i), text=chunk\n    )\n\nhf_results = await hf_memory.search(\n    \"hf_London\", \"what food should I eat in London\", limit=2, min_relevance_score=0\n)\n```", "```py\nhf_results = await hf_memory.search(\n    \"hf_London\",\n    \"Where can I eat non-british food in London?\",\n    limit=2,\n    min_relevance_score=0,\n)\n```", "```py\nprompt_with_context = \"\"\"\n Use the following pieces of context to answer the users question.\n This is the only information that you should use to answer the question, do not reference information outside of this context.\n If the information required to answer the question is not provided in the context, just say that \"I don't know\", don't try to make up an answer.\n ----------------\n Context: {{$context}}\n ----------------\n User question: {{$question}}\n ----------------\n Answer:\n\"\"\"\n\nexecution_config = kernel.get_service(target_service_id).instantiate_prompt_execution_settings(\n        service_id=target_service_id,\n        max_tokens=500,\n        temperature=0,\n        seed=42\n    )\n\nprompt_template_config = sk.PromptTemplateConfig(\n    template=prompt_with_context,\n    name=\"chat\",\n    template_format=\"semantic-kernel\",\n    input_variables=[\n        InputVariable(name=\"question\", description=\"The user input\", is_required=True),\n        InputVariable(name=\"context\", description=\"The conversation history\", is_required=True),\n    ],\n    execution_settings=execution_config,\n)\n\nchatbot_with_context = kernel.create_function_from_prompt(\n    function_name=\"chatbot_with_memory_context\",\n    plugin_name=\"chatPluginWithContext\",\n    prompt_template_config=prompt_template_config,\n)\n```", "```py\nquestion = \"Where can I eat non-british food in London?\"\n```", "```py\nresults = await hf_memory.search(\"hf_London\", question, limit=2)\n```", "```py\ncontext = KernelArguments(question=question, context=\"\\n\".join([result.text for result in results]))\n```", "```py\nanswer = await kernel.invoke(chatbot_with_context, context)\n```", "```py\nprompt_with_context_plugin = \"\"\"\n Use the following pieces of context to answer the users question.\n This is the only information that you should use to answer the question, do not reference information outside of this context.\n If the information required to answer the question is not provided in the context, just say that \"I don't know\", don't try to make up an answer.\n ----------------\n Context: {{recall $question}}\n ----------------\n User question: {{$question}}\n ----------------\n Answer:\n\"\"\"\n```", "```py\nexecution_config = kernel.get_service(target_service_id).instantiate_prompt_execution_settings(\n        service_id=target_service_id,\n        max_tokens=500,\n        temperature=0,\n        seed=42\n    )\n\nprompt_template_config = sk.PromptTemplateConfig(\n    template=prompt_with_context_plugin,\n    name=\"chat\",\n    template_format=\"semantic-kernel\",\n    input_variables=[\n        InputVariable(name=\"question\", description=\"The user input\", is_required=True),\n        InputVariable(name=\"context\", description=\"The conversation history\", is_required=True),\n    ],\n    execution_settings=execution_config,\n)\n\nchatbot_with_context_plugin = kernel.create_function_from_prompt(\n    function_name=\"chatbot_with_context_plugin\",\n    plugin_name=\"chatPluginWithContextPlugin\",\n    prompt_template_config=prompt_template_config,\n)\n```", "```py\ncontext = KernelArguments(question=\"Where can I eat non-british food in London?\", collection='London', relevance=0.2, limit=2)\n\nanswer = await kernel.invoke(chatbot_with_context_plugin, context)\n```", "```py\nfrom pathlib import Path\n\nplugins_path = Path(\"Plugins\")\nplugins_path.mkdir(exist_ok=True)\n```", "```py\npoem_gen_plugin_path = plugins_path / \"PoemGeneratorPlugin\"\npoem_gen_plugin_path.mkdir(exist_ok=True)\n```", "```py\npoem_sc_path = poem_gen_plugin_path / \"write_poem\"\npoem_sc_path.mkdir(exist_ok=True)\n```", "```py\nconfig_path = poem_sc_path / \"config.json\"\n```", "```py\n%%writefile {config_path}\n\n{\n  \"schema\": 1,\n  \"description\": \"A poem generator, that writes a short poem based on user input\",\n  \"execution_settings\": {\n    \"azure_gpt35_chat_completion\": {\n      \"max_tokens\": 512,\n      \"temperature\": 0.8,\n      \"top_p\": 0.0,\n      \"presence_penalty\": 0.0,\n      \"frequency_penalty\": 0.0,\n      \"seed\": 42\n    }\n  },\n  \"input_variables\": [\n    {\n      \"name\": \"input\",\n      \"description\": \"The topic that the poem should be written about\",\n      \"default\": \"\",\n      \"is_required\": true\n    }\n  ]\n\n}\n```", "```py\npoem_gen_plugin = kernel.import_plugin_from_prompt_directory(\n    plugins_path, \"PoemGeneratorPlugin\"\n)\n```", "```py\nresult = await kernel.invoke(poem_gen_plugin[\"write_poem\"], KernelArguments(input=\"Munich\"))\n```", "```py\nprompt = \"\"\"\n{{PoemGeneratorPlugin.write_poem $input}}\n\"\"\"\n\ntarget_service_id = \"azure_gpt35_chat_completion\"\n\nexecution_config = kernel.get_service(target_service_id).instantiate_prompt_execution_settings(\n        service_id=target_service_id,\n        max_tokens=500,\n        temperature=0.8,\n        seed=42\n    )\n\nprompt_template_config = sk.PromptTemplateConfig(\n    template=prompt,\n    name=\"chat\",\n    template_format=\"semantic-kernel\",\n    input_variables=[\n        InputVariable(name=\"input\", description=\"The user input\", is_required=True),\n    ],\n    execution_settings=execution_config,\n)\n\nwrite_poem_wrapper = kernel.create_function_from_prompt(\n    function_name=\"poem_gen_wrapper\",\n    plugin_name=\"poemWrapper\",\n    prompt_template_config=prompt_template_config,\n)\n\nresult = await kernel.invoke(write_poem_wrapper, KernelArguments(input=\"Munich\"))\n```", "```py\nimage_classifier_plugin_path = plugins_path / \"ImageClassifierPlugin\"\nimage_classifier_plugin_path.mkdir(exist_ok=True)\n\ndownload_image_sc_path = image_classifier_plugin_path / \"download_image.py\"\ndownload_image_sc_path.mkdir(exist_ok=True)\n```", "```py\nimport requests\nfrom PIL import Image\nimport timm\nfrom timm.data.imagenet_info import ImageNetInfo\n\nfrom typing import Annotated\nfrom semantic_kernel.functions.kernel_function_decorator import kernel_function\n\nclass ImageClassifierPlugin:\n    def __init__(self):\n        self.model = timm.create_model(\"convnext_tiny.in12k_ft_in1k\", pretrained=True)\n        self.model.eval()\n        data_config = timm.data.resolve_model_data_config(self.model)\n        self.transforms = timm.data.create_transform(**data_config, is_training=False)\n        self.imagenet_info = ImageNetInfo()\n\n    @kernel_function(\n        description=\"Takes a url as an input and classifies the image\",\n        name=\"classify_image\"\n    )\n    def classify_image(self, input: Annotated[str, \"The url of the image to classify\"]) -> str:\n        image = self.download_image(input)\n        pred = self.model(self.transforms(image)[None])\n        return self.imagenet_info.index_to_description(pred.argmax())\n\n    def download_image(self, url):\n        return Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n```", "```py\nimage_classifier = ImageClassifierPlugin()\n\nclassify_plugin = kernel.import_plugin_from_object(image_classifier, plugin_name=\"classify_image\")\n```", "```py\nurl = \"https://cdn.pixabay.com/photo/2016/02/10/16/37/cat-1192026_1280.jpg\"\nresponse = await kernel.invoke(classify_plugin[\"classify_image\"], KernelArguments(input=url))\n```", "```py\nanswers = await kernel.invoke([classify_plugin[\"classify_image\"], poem_gen_plugin[\"write_poem\"]], arguments=KernelArguments(input=url))\n```", "```py\nfrom semantic_kernel.events.function_invoked_event_args import FunctionInvokedEventArgs\n\ndef store_results(kernel, invoked_function_info: FunctionInvokedEventArgs):\n    previous_step_result = str(invoked_function_info.function_result)\n    invoked_function_info.arguments['input'] = previous_step_result\n    invoked_function_info.updated_arguments = True\n```", "```py\nkernel.add_function_invoked_handler(store_results)\n```", "```py\nanswers = await kernel.invoke([classify_plugin[\"classify_image\"], poem_gen_plugin[\"write_poem\"]], arguments=KernelArguments(input=url))\n```", "```py\nkernel = sk.Kernel()\n```", "```py\nservice_id = \"azure_gpt35_chat_completion\"\n\nkernel.add_service(\n    service=AzureChatCompletion(\n        service_id=service_id,\n        deployment_name=OPENAI_DEPLOYMENT_NAME, \n        endpoint=OPENAI_ENDPOINT, \n        api_key=OPENAI_API_KEY\n    ),\n)\n```", "```py\nclassify_plugin = kernel.import_plugin_from_object(\n    ImageClassifierPlugin(), plugin_name=\"classify_image\"\n)\npoem_gen_plugin = kernel.import_plugin_from_prompt_directory(\n    plugins_path, \"PoemGeneratorPlugin\"\n)\n```", "```py\nfrom semantic_kernel.planners.basic_planner import BasicPlanner\n\nplanner = BasicPlanner(service_id)\n```", "```py\nask = f\"\"\"\nI would like you to write poem about what is contained in this image with this url: {url}. This url should be used as input.\n\n\"\"\"\n```", "```py\nplan = await planner.create_plan(ask, kernel)\n```", "```py\npoem = await planner.execute_plan(plan, kernel)\n```"]