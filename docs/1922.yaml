- en: 'Anomaly Detection Using Sigma Rules: Build Your Own Spark Streaming Detections'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/anomaly-detection-using-sigma-rules-build-your-own-spark-streaming-detections-657bcef3988a?source=collection_archive---------9-----------------------#2023-06-12](https://towardsdatascience.com/anomaly-detection-using-sigma-rules-build-your-own-spark-streaming-detections-657bcef3988a?source=collection_archive---------9-----------------------#2023-06-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Easily deploy Sigma rules in Spark streaming pipelines: a future-proof solution
    supporting the upcoming Sigma 2 specification'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@jean-claude.cote?source=post_page-----657bcef3988a--------------------------------)[![Jean-Claude
    Cote](../Images/aea2df9c7b95fc85cc336f64d64b0a76.png)](https://medium.com/@jean-claude.cote?source=post_page-----657bcef3988a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----657bcef3988a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----657bcef3988a--------------------------------)
    [Jean-Claude Cote](https://medium.com/@jean-claude.cote?source=post_page-----657bcef3988a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F444ed0089012&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanomaly-detection-using-sigma-rules-build-your-own-spark-streaming-detections-657bcef3988a&user=Jean-Claude+Cote&userId=444ed0089012&source=post_page-444ed0089012----657bcef3988a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----657bcef3988a--------------------------------)
    ·13 min read·Jun 12, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F657bcef3988a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanomaly-detection-using-sigma-rules-build-your-own-spark-streaming-detections-657bcef3988a&user=Jean-Claude+Cote&userId=444ed0089012&source=-----657bcef3988a---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F657bcef3988a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanomaly-detection-using-sigma-rules-build-your-own-spark-streaming-detections-657bcef3988a&source=-----657bcef3988a---------------------bookmark_footer-----------)![](../Images/4b42af8eea5eda23fe3f372446f76e3e.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Photo by Dana Walker on Unsplash
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: In our previous articles we elaborated and designed a stateful function named
    flux-capacitor.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: The flux-capacitor stateful function that can remember parent-child (and ancestor)
    relationships between log events. It can also remember events occurring on the
    same host in a certain window of time, the Sigma specification refers to this
    as [temporal proximity correlation](https://github.com/SigmaHQ/sigma-specification/blob/version_2/Sigma_meta_rules.md#temporal-proximity-temporal)
    .
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: For a deep-dive into the design of flux-capacitor refer to [part 1](/anomaly-detection-using-sigma-rules-part-1-leveraging-spark-sql-streaming-246900e95457)
    , [part 2](/anomaly-detection-using-sigma-rules-part-2-spark-stream-stream-join-6bb4734e912f),
    [part 3](https://medium.com/towards-data-science/anomaly-detection-using-sigma-rules-part-3-temporal-correlation-using-bloom-filters-a45ffd5e9069),
    [part 4](https://medium.com/towards-data-science/anomaly-detection-using-sigma-rules-part-4-flux-capacitor-design-70cb5c2cfb72),
    and [part5](https://medium.com/towards-data-science/anomaly-detection-using-sigma-rules-part-5-flux-capacitor-optimization-118e538cf8c4).
    However, you don’t need to understand the implementation of the function to use
    it.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: In this article we first show a Spark streaming job which performs discreet
    detections. A discreet detection is a Sigma rule which uses the features and values
    of a single log line (a single event).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Then we leverage the flux-capacitor function to handle stateful parent-child
    relationships between log events. The flux-capacitor is also able to detect a
    number of events occurring on the same host in a certain window of time; these
    are called [temporal proximity correlation](https://github.com/SigmaHQ/sigma-specification/blob/version_2/Sigma_meta_rules.md#temporal-proximity-temporal)
    in the upcoming Sigma specification. A complete demo of these spark streaming
    jobs is available in our [git repo](https://github.com/cccs-jc/flux-capacitor/tree/main/demo)
    .
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Discreet Detections
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performing discrete tests is fairly straightforward, thanks to all the built-in
    functions that come out-of-the-box in Spark. Spark has support for reading streaming
    sources, writing to sinks, checkpointing, stream-stream joins, windowed aggregations
    and many more. For a complete list of the possible functionalities, see the comprehensive
    Spark [Structured Streaming Programming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Here’s a high level diagram showing a Spark streaming job that consumes events
    from an Iceberg table of “start-process” windows events (1). A classic example
    of this is found in [Windows Security Logs (Event ID 4688)](https://www.ultimatewindowssecurity.com/securitylog/encyclopedia/event.aspx?eventID=4688).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0eb3451a5438b63e377a4817ab08a34b.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: Topology for discrete detections
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: The source table (1) is named `process_telemetry_table`. The Spark job reads
    all events, detects anomalous events, tags these events and writes them to table
    (3) named `tagged_telemetry_table`. Events deemed anomalous are also written to
    a table (4) containing alerts.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 源表（1）名为`process_telemetry_table`。Spark作业读取所有事件，检测异常事件，标记这些事件，并将其写入名为`tagged_telemetry_table`的表（3）。被认为异常的事件也会写入一个包含警报的表（4）。
- en: Periodically we poll a git repository (5) containing the SQL auto-generated
    from the Sigma rules we want to apply. If the SQL statements change, we restart
    the streaming job to add these new detections to the pipeline.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 定期我们轮询一个包含我们想要应用的Sigma规则自动生成的SQL的git仓库（5）。如果SQL语句发生变化，我们重新启动流处理作业以将这些新的检测添加到管道中。
- en: 'Let’s take [this Sigma rule](https://github.com/SigmaHQ/sigma/blob/master/rules/windows/process_creation/proc_creation_win_rundll32_sys.yml)
    as an example:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 以[这个Sigma规则](https://github.com/SigmaHQ/sigma/blob/master/rules/windows/process_creation/proc_creation_win_rundll32_sys.yml)为例：
- en: '![](../Images/4ca41e731f3209b3cd8ed45e6b459ab8.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ca41e731f3209b3cd8ed45e6b459ab8.png)'
- en: screenshot from *proc_creation_win_rundll32_sys.yml at* Sigma HQ
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 来自*proc_creation_win_rundll32_sys.yml在* Sigma HQ的截图
- en: The `detection` section is the heart of the Sigma rule and consists of a `condition`
    and 1 or more named tests. The `selection1` and `selection2` are named boolean
    tests. The author of the Sigma rule can give meaniningful names to these tests.
    The `condition` is where the user can combine the tests in a final evaluation.
    See the [Sigma specification](https://github.com/SigmaHQ/sigma-specification/blob/main/Sigma_specification.md)
    for more details on writing a Sigma rule.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`detection`部分是Sigma规则的核心，包括一个`condition`和一个或多个命名测试。`selection1`和`selection2`是命名的布尔测试。Sigma规则的作者可以为这些测试命名有意义的名称。`condition`是用户可以在最终评估中组合测试的地方。有关编写Sigma规则的更多细节，请参见[Sigma规范](https://github.com/SigmaHQ/sigma-specification/blob/main/Sigma_specification.md)。'
- en: From now on we will refer to these named boolean tests as **tags**.
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 从现在开始，我们将这些命名的布尔测试称为**标签**。
- en: 'The inner workings of the Spark streaming job is broken down into 4 logical
    steps:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Spark流处理作业的内部工作分为4个逻辑步骤：
- en: read the source table `process_telemetry_table`
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 读取源表`process_telemetry_table`
- en: perform pattern matching
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行模式匹配
- en: evaluate final condition
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估最终条件
- en: write the results
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 写入结果
- en: The **Pattern Match** step consist of evaluating the tags found in the Sigma
    rule and the **Eval final condition** evaluates the`condition.`
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**模式匹配**步骤包括评估在Sigma规则中找到的标签，**最终条件评估**评估`condition`。'
- en: '![](../Images/fd233b7e5570fa7d71ae6bb3ef22d63b.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd233b7e5570fa7d71ae6bb3ef22d63b.png)'
- en: On the right of this diagram we show what the row would look like at this stage
    of processing. The columns in blue represent values read from the source table.
    The **Pattern Match** step adds a column named `Sigma tags` which is a map of
    all the tests performed and whether the test passed or failed. The gray column
    contains the final Sigma rule evaluations. Finally, the brown columns are added
    in the foreachBatch function. A GUID is generated, the rule names that are true
    are extracted from the Sigma tags map and the detection `action` is retrieved
    from a lookup map of rule-name to rule-type. This gives context to the alerts
    produced.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在该图的右侧，我们展示了在此处理阶段该行的样子。蓝色的列表示从源表中读取的值。**模式匹配**步骤添加了一列名为`Sigma tags`，这是所有执行的测试及其通过或失败情况的映射。灰色列包含最终的Sigma规则评估。最后，棕色列是在foreachBatch函数中添加的。生成了一个GUID，从Sigma标签映射中提取出为真的规则名称，并从规则名到规则类型的查找映射中检索检测`action`。这为生成的警报提供了上下文。
- en: This diagram depicts how attributes of the event are combined into tags, final
    evaluation and finally contextual information.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 此图描绘了事件的属性如何组合成标签、最终评估和最终上下文信息。
- en: '![](../Images/b263bf773a143eb5cfb0592ee7008645.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b263bf773a143eb5cfb0592ee7008645.png)'
- en: Let’s now look at the actual pyspark code. First, we connect spark to the source
    table using the `readStream` function and specifying the name from which the iceberg
    table is read. The `load` function returns a dataframe, which we use to create
    a view named `process_telemetry_view`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看实际的pyspark代码。首先，我们使用`readStream`函数将spark连接到源表，并指定从中读取冰山表的名称。`load`函数返回一个数据帧，我们用它来创建一个名为`process_telemetry_view`的视图。
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The data in the `process_telemetry_view` looks like this:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`process_telemetry_view`中的数据如下所示：'
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'On this view we apply a **Pattern Matching** step which consists of an auto-generated
    SQL statement produced by the Sigma compiler. The `patern_match.sql` file looks
    like this:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We use `spark.sql()` to apply this statement to the `process_telemetry_view`
    view.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Notice that the results of each tag found in the Sigma rule are stored in a
    map of boolean values. The `sigma` column holds the results of each tag found
    in each Sigma rule. By using a **MapType** we can easily introduce new Sigma rules
    without affecting the schema of the table. Adding a new rule simply adds a new
    entry in the `sigma`column (a **MapType**) .
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Similarly, the **Eval final condition** step applies the conditions from the
    Sigma rules. The `condition`s are compiled into an SQL statement, which use [map](https://spark.apache.org/docs/latest/api/sql/#map),
    [map_filter](https://spark.apache.org/docs/latest/api/sql/#map_filter), [map_keys](https://spark.apache.org/docs/latest/api/sql/#map_keys),
    to build a column named `sigma_final`. This column holds the name of all the rules
    that have a `condition` that evaluates to true.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The auto-generated statement is applied using `spark.sql()`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here’s the results with the newly added `sigma_final` column, an array of rules
    that fire.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We are now ready to start the streaming job for our dataframe. Notice that we
    pass in a call back function `for_each_batch_function` to the `foreachBatch`.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `for_each_batch_function` is called at every micro-batch and is given the
    evaluated `batchdf` dataframe. The `for_each_batch_function` writes the entirety
    of `batchdf` to the `tagged_telementry_table` and also writes alerts for any of
    the Sigma rules that evaluated to true.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The details of [insert_into_tagged_telemetry.sql](https://github.com/cccs-jc/flux-capacitor/blob/main/demo/templates/static/insert_into_tagged_telemetry.sql)
    and [publish_suspected_anomalies.sql](https://github.com/cccs-jc/flux-capacitor/blob/main/demo/templates/static/publish_suspected_anomalies.sql)
    can be found in our git repo.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned above, writing a streaming anomaly detection handling discreet
    test is relatively straightforward using the built-in functionality found in Spark.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Detections Base on Past Events
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thus far we showed how to detect events with discrete Sigma rules. In this section
    we leverage the flux-capacitor function to enable caching tags and testing tags
    of past events. As discussed in our previous articles, the flux-capacitor lets
    us detect parent-child relationships and also sequences of arbitrating features
    of past events.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: These types of Sigma rules need to simultaneously consider the tags of the current
    event and of past events. In order to perform the final rule evaluation, we introduce
    a **Time travel tags** step to retrieve all of past tags for an event and merge
    them with the current event. This is what the flux-capacitor function is designed
    to do, it caches and retrieves past tags. Now that past tags and current tags
    are on the same row, the **Eval final condition** can be evaluated just like we
    did in our discreet example above.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'The detection now looks like this:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/72738e4cf51dcc5349ebc938550a7b6e.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: The flux-capacitor is given the `Sigma tags` produced by the **Pattern Match**
    step. The flux-capacitor stores these tags for later retrieval. The column in
    red has the same schema as the `Sigma tags` column we used before. However, it
    combines current and past tags, which the flux-capacitor retrieved from its internal
    state.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Adding caching and retrieval of past tags is easy thanks to the flux-capacitor
    function. Here’s how we applied the flux-capacitor function in our Spark anomaly
    detection. First, pass the dataframe produced by the **Pattern Match** step to
    the `flux_stateful_function` and the function returns another dataframe, which
    contains past tags.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: To control the behavior of the `flux_stateful_function` we pass in a `flux_update_spec`.
    The flux-capacitor specification is a yaml file produced by the Sigma compiler.
    The specification details which tags should be cached and retrieved and how they
    should be handled. The `action` attribute can be set to `parent`, `ancestor` or
    `temporal`.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use a concrete example from Sigma HQ [proc_creation_win_rundll32_executable_invalid_extension.yml](https://github.com/SigmaHQ/sigma/blob/cd71edc09ca915f389e50df5b1bbb5ecd4b7f89d/rules/windows/process_creation/proc_creation_win_rundll32_executable_invalid_extension.yml)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fde734fc81eb09c0f278e96b1e224af3.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: screenshot from Sigma HQ github
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Again the heart of the detection consists of tags and of a final `condition`
    which puts all these tags together. Note however that this rule (that we will
    refer to as Rule 1) involves tests against `CommandLine` and also test on the
    parent process `ParentImage`. **ParentImage** is not a field found in the start-process
    logs. Rather it refers to the **Image** field of the parent process.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: As seen before, this Sigma rule will be compiled into SQL to evaluate the tags
    and to combine them into a final condition.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: In order to propagate the parent tags, the Sigma compiler also produces a flux-capacitor
    specification. Rule 1 is a `parent` rule and thus the specification must specify
    what are the parent and child fields. In our logs these correspond to `id` and
    `parent_id`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'The specification also specifies which `tags` should be cached and retrieved
    by the flux-capacitor function. Here is the auto-generated specification:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note Rule 0 is not included in the flux-capacitor function since it has no temporal
    tags.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Illustrating Tag Propagation
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标记传播示例
- en: In order to better understand what the flux-capacitor does, you can use the
    function outside a streaming analytic. Here we show a simple **ancestor** example.
    We want to propagate the tag `pf`. For example `pf` might represent a `CommandLine`
    containing `rundll32.exe`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解 flux-capacitor 的作用，你可以在流分析之外使用该函数。这里我们展示了一个简单的 **祖先** 示例。我们想要传播 `pf`
    标签。例如，`pf` 可能表示包含 `rundll32.exe` 的 `CommandLine`。
- en: '[PRE12]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Printing the dataframe `df_input` we see that pid500 started and had a `CommandLine`
    with the `pf` feature. Then pid500 started pid600\. Later pid600 started pid700\.
    Pid700 had a child feature `cf`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 打印数据框 `df_input` 我们看到 pid500 启动并具有带有 `pf` 特性的 `CommandLine`。然后 pid500 启动了 pid600。之后
    pid600 启动了 pid700。Pid700 具有子特性 `cf`。
- en: '[PRE13]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The Sigma rule is a combination of both `pf` and `cf`. In order to bring the
    `pf` tag back on the current row, we need to apply time-travel to the `pf` tag.
    Applying the flux-capacitor function to the `df_input` dataframe
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Sigma 规则是 `pf` 和 `cf` 的组合。为了将 `pf` 标签带回当前行，我们需要对 `pf` 标签应用时间旅行。将 flux-capacitor
    函数应用于 `df_input` 数据框。
- en: '[PRE14]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We obtain the `df_output` dataframe. Notice how the `pf` tag is propagated through
    time.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获得了 `df_output` 数据框。注意 `pf` 标签如何在时间中传播。
- en: '[PRE15]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This notebook [TagPropagationIllustration.ipynb](https://github.com/cccs-jc/flux-capacitor/blob/main/TagPropagationIllustration.ipynb)
    contains more examples like this for parent-child and temporal proximity.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 本笔记本 [TagPropagationIllustration.ipynb](https://github.com/cccs-jc/flux-capacitor/blob/main/TagPropagationIllustration.ipynb)
    包含了更多类似的父子和时间邻近的示例。
- en: Building Alerts with Context
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建带有上下文的警报
- en: The flux-capacitor function caches all the past tags. In order to conserve memory,
    it caches these tags using bloom filter segments. Bloom filters have an extremely
    small memory footprint, are quick to query and to update. However, they do introduce
    possible false positive. It is thus possible that one of our detections is in
    fact a false positive. In order to remedy this we put the suspected anomalies
    in a queue (4) for re-evaluation.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: flux-capacitor 函数缓存所有过去的标签。为了节省内存，它使用布隆过滤器段缓存这些标签。布隆过滤器具有极小的内存占用，查询和更新都很快。然而，它们确实引入了可能的假阳性。因此，我们的检测可能实际上是一个假阳性。为了解决这个问题，我们将怀疑的异常放入队列
    (4) 进行重新评估。
- en: To eliminate false positives, the second Spark streaming job named the **Alert
    Builder** reads the suspected anomalies (5) and retrieves the events (6) that
    are required to re-evaluate the rule.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了消除假阳性，第二个名为 **警报生成器** 的 Spark 流作业读取怀疑的异常 (5) 并检索需要重新评估规则的事件 (6)。
- en: For example in the case of a parent-child Sigma rule, the **Alert Builder**
    will read the suspected anomaly (5) retrieving a child process event. Next, in
    (6) it will retrieve the parent process of this child event. Then using these
    two events it re-evaluates the Sigma rule. However, this time the flux-capacitor
    is configured to store tags in a hash map, rather than in bloom filters. This
    eliminates false positives and as a bonus we have all the events involved in this
    detection. We store this alert along with the rows of evidence (parent and child
    events) into an alert table (7).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在父子 Sigma 规则的情况下，**警报生成器**将读取怀疑的异常 (5)，检索子进程事件。接下来，在 (6) 中，它将检索该子事件的父进程。然后，利用这两个事件重新评估
    Sigma 规则。然而，这次 flux-capacitor 配置为将标签存储在哈希映射中，而不是布隆过滤器中。这消除了假阳性，并且作为额外的好处，我们获得了所有参与检测的事件。我们将此警报及证据行（父子事件）存储到警报表中
    (7)。
- en: '![](../Images/b6575682d95874f7fcb18edd3a782350.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b6575682d95874f7fcb18edd3a782350.png)'
- en: Topology with stateful detections (temporal)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 带有状态检测的拓扑（时间）
- en: The **Alert Builder** handles a fraction of the volume processed by (2) the
    **Streaming Detections**. Thanks to the low volume read in (5) historical searches
    into the tagged telemetry (6) are possible.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**警报生成器** 处理的量仅为 (2) **流检测** 的一小部分。由于在 (5) 中读取的低量，历史搜索到标记的遥测 (6) 是可能的。'
- en: For a more in-depth look, take a look at the Spark jobs for the **Streaming
    Detections** [streaming_detections.py](https://github.com/cccs-jc/flux-capacitor/blob/main/demo/streaming_detections.py)
    and the **Alert Builder** [streaming_alert_builder.py](https://github.com/cccs-jc/flux-capacitor/blob/main/demo/streaming_alert_builder.py)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 想要更深入地了解，请查看 **流检测** 的 Spark 作业 [streaming_detections.py](https://github.com/cccs-jc/flux-capacitor/blob/main/demo/streaming_detections.py)
    和 **警报生成器** [streaming_alert_builder.py](https://github.com/cccs-jc/flux-capacitor/blob/main/demo/streaming_alert_builder.py)。
- en: Performance
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能
- en: To evaluate the performance of this proof of concept we ran tests on machines
    with 16 CPU and 64G of ram. We wrote a simple data producer that creates 5,000
    synthetic events per seconds and ran the experiment for 30 days.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: The Spark Streaming Detections job runs on one machine. The job is configured
    to trigger every minute. Each micro-batch (trigger) reads 300,000 events and takes
    on average 20 seconds to complete. The job can easily keep up with the incoming
    events rate.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3bb4704c4c25749eb975554046d78d31.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
- en: Spark Streaming Detections
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: The Spark **Alert Builder** also runs on a single machine and is configured
    to trigger every minute. This job takes between 30 and 50 seconds to complete.
    This job is very sensitive to organization of the `tagged_telemetry_table` . Here
    we see the effect of the maintenance job which organizes and sorts the latest
    data at every hour. Thus at every hour, the Spark **Alert Builder**’s micro-batch
    execution time drops back to 30 seconds.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dd1823abdcf1deeef64c22eb56dbec6e.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
- en: Spark Streaming Alert Builder
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Table Maintenance
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our Spark streaming jobs trigger every minute and thus produce small data files
    every minute. In order to allow for fast searches and retrieval in this table,
    it’s important to compact and sort the data periodically. Fortunately Iceberg
    comes with [built-in procedures](https://iceberg.apache.org/docs/latest/maintenance/)
    to organize and maintain your tables.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: For example this script [maintenance.py](https://github.com/cccs-jc/flux-capacitor/blob/main/demo/maintenance.py)
    runs every hour to sort and compact the newly added files of the Iceberg `tagged_telemetry_table`.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: At the end of the day we also re-sort this table, yielding maximum search performance
    over long search periods (months of data).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Another maintenance task we do is deleting old data from the streaming tables.
    These tables are only used as buffers between producers and consumers. Thus every
    day we age off the streaming tables keeping 7 days of data.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Finally, every day we perform standard Iceberg table maintenance tasks, like
    expiring snapshots and removing orphan files. We run these maintenance jobs on
    all of our tables and schedule these jobs on [Airflow](https://airflow.apache.org/).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article we showed how build a Spark streaming anomaly detection framework
    that generically applies Sigma rules. New Sigma rules can easily be added to the
    system.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: This proof of concept was extensively tested on synthetic data to evaluate its
    stability and scalability. It shows great promise and further evaluation will
    be performed on a production system.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '*All images unless otherwise noted are by the author*'
  id: totrans-114
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
