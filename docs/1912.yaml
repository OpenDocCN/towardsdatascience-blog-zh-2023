- en: Understand Policy Gradient by Building Cross Entropy from Scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/understand-policy-gradient-by-building-cross-entropy-from-scratch-75ca18b53e94?source=collection_archive---------2-----------------------#2023-06-11](https://towardsdatascience.com/understand-policy-gradient-by-building-cross-entropy-from-scratch-75ca18b53e94?source=collection_archive---------2-----------------------#2023-06-11)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A unified view of how we train models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://tonychenxyz.medium.com/?source=post_page-----75ca18b53e94--------------------------------)[![Tony
    Chen](../Images/37a65cea36bc332c8777c4f599a592f5.png)](https://tonychenxyz.medium.com/?source=post_page-----75ca18b53e94--------------------------------)[](https://towardsdatascience.com/?source=post_page-----75ca18b53e94--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----75ca18b53e94--------------------------------)
    [Tony Chen](https://tonychenxyz.medium.com/?source=post_page-----75ca18b53e94--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1c86c3ef2039&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstand-policy-gradient-by-building-cross-entropy-from-scratch-75ca18b53e94&user=Tony+Chen&userId=1c86c3ef2039&source=post_page-1c86c3ef2039----75ca18b53e94---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----75ca18b53e94--------------------------------)
    ·16 min read·Jun 11, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F75ca18b53e94&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstand-policy-gradient-by-building-cross-entropy-from-scratch-75ca18b53e94&user=Tony+Chen&userId=1c86c3ef2039&source=-----75ca18b53e94---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F75ca18b53e94&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstand-policy-gradient-by-building-cross-entropy-from-scratch-75ca18b53e94&source=-----75ca18b53e94---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reinforcement learning (RL)** can do amazing stuff. Most recently, [ChatGPT
    is fine-tuned on human feedback with PPO](https://www.assemblyai.com/blog/how-chatgpt-actually-works/),
    a variant of a class of reinforcement learning algorithm called **Policy Gradient
    (PG)**. Understanding RL, especially policy gradient, could be non-trivial, particularly
    if you like grasping intuitions like I do. In this post, I will walk through a
    thread of thoughts that really helped me understand PG by starting from more familiar
    supervised learning setting.'
  prefs: []
  type: TYPE_NORMAL
- en: TL;DR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will start by designing a simple supervised training procedure of a binary
    classification robot by rewarding it +1 for correct answers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will formulate the objective for the procedure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will derive the gradient ascent formulation for the procedure (which will
    turn out to be the same procedure as gradient descent with Cross Entropy)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will compare our procedure to RL settings and relate our gradient ascent
    to policy gradient
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Who should read this?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: My goal is to provide a friendly, intuitive aid to understand PG. It is helpful
    if you have a general understanding of RL problem setting and know on high level
    what PG is.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I hope to help you better understand the relationship between RL with PG and
    supervised ML. So it is helpful if you know about how to train a supervised ML
    algorithm with cross entropy loss function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why this post?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Policy Gradient
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In an RL problem, an **agent** interacts with an **environment** to learn a
    **policy.** The policy tells the agent what to do in different **states** to maximize
    **reward**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c713f4a5b047c8393283f44de994e836.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The idea of PG seems straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: The **policy** that guides agent behavior at time *t* is *π_θ(a_t|s_t)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is **some kind of function** (often a neural network) with parameter *θ*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It **takes in information of states** *s_t* and spits out a **probability distribution
    of action to take** *a_t*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then it **receives a reward** *r(s_t, a_t)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we have history of many of such cycles of action and reward, we can update
    parameter *θ* in order to maximize expected reward produced by actions generated
    from *π_θ*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we do the update? Through…**gradient**! We update the model producing
    *π_θ* with the following gradient
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/33d815377e9ba0a3076a657fab933349.png)'
  prefs: []
  type: TYPE_IMG
- en: Something Feels Off
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This looks very familiar. When we train a neural network model in good old supervised
    learning, we also update the model parameters by doing the operation in the second
    line aka gradient descent (technically in PG case, since we are maximizing an
    objective, it’s gradient ascent).
  prefs: []
  type: TYPE_NORMAL
- en: 'But this also feels very different. If you look at its [derivation process](https://www.youtube.com/watch?v=GKoKNYaBvM0&list=PL_iWQOsE6TfXxKgI1GgyV1B_Xa0DxE5eH&index=23),
    you can see it **takes a bit of effort** to derive this equation. That’s very
    different from the more intuitive way of how we do supervised learning: feed an
    input into neural network, get an output, compare it with target and calculate
    a loss function, hit backprop button, and we are done!'
  prefs: []
  type: TYPE_NORMAL
- en: Also, for me, the **log term** always seems coming out of nowhere. Although
    the same online course in the link above walks through how do we get to the log
    term, the process seems to just be a bunch of math that’s correct but lacking
    motivation.
  prefs: []
  type: TYPE_NORMAL
- en: What is exactly the difference from supervised learning? It turns out diving
    into this question provides a great way of understanding PG. Furthermore, it is
    a good reminder for the **nature of some familiar things in supervised learning**
    that we do everyday.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s Build Cross Entropy from Scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we are thrown with some loss functions used in supervised ML, they would
    “make sense” immediately. But it takes some more efforts to understand where it
    comes from. For example, the good old mean square loss intuitively make sense:
    it just minimizes the distance between the prediction and target. But there’re
    so many distance metrics; why square distance? Y[ou have to look deeper to see
    that mean square error is the byproduct of doing maximum likelihood and assuming
    the underlying population distribution to be normal](https://tivadardanka.com/blog/mean-squared-error-explained).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Same with another good old loss function that we use everyday: Cross Entropy.
    While there are many [good interpretations of what Cross Entropy does](/entropy-cross-entropy-and-kl-divergence-17138ffab87b#:~:text=KL%20divergence%20is%20the%20relative,as%20the%20actual%20probability%20distribution.),
    **let’s try to build it from the most rudimentary manner**.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s Train a Classification Bot!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Say you want to train a robot to classify dog and cat image. It’s intuitive
    To train it by rewarding the right answer and punish it (or not reward it) for
    wrong answers. Here’s how it’s done:'
  prefs: []
  type: TYPE_NORMAL
- en: You **give the robot an image**. Let’s call it *s.* This image is sampled from
    a population distribution *D_s*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/90151735fcf9c358929bba56a796f060.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Dog Image Source: Unsplash; Other Parts By Author'
  prefs: []
  type: TYPE_NORMAL
- en: The robot will **give you an answer** if it think it’s a dog image (action *a_dog*)
    or it’s a cat image (action *a_cat*)*.*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The robot has **its own prediction** given the image about the probability
    of the image being dog or cat: *π_θ(a|s) = (a_dog, a_cat)*. For example, *π_θ(a|s)
    = (0.9, 0.1)* means it thinks there’s 0.9 probability it’s a dog, and 0.1 probably
    it’s a cat.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/31518402fb49d27bc9d2ced9334b2d01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Dog Image Source: Unsplash; Other Parts By Author'
  prefs: []
  type: TYPE_NORMAL
- en: 'But every time the robot will only give you a solid answer. It either says
    “it’s a dog” (*a_dog*) or “it’s a cat” *(a_cat)*. Every time **it gives you a
    response**, the responses (actions) are **randomly sampled from distribution**
    produced by *π_θ(a|s)*: *a = (a_dog, a_cat) ~ π_θ(a|s)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/89e8436eefd10f6faca3a83c91f37826.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Dog Image Source: Unsplash; Other Parts By Author'
  prefs: []
  type: TYPE_NORMAL
- en: Then you will **reward** the robot (maybe give it a treat?) with reward of 1
    when robot answers correctly. (*r(s,a) = 1*) There’s **no reward** (0 reward)
    when the answer is wrong. (*r(s,a) = 0*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/32a9c703d976a3082f0b2b7452894982.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Dog Image Source: Unsplash; Other Parts By Author'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cf7ff12b91caebf06f6324d5860acff4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Cat Image Source: Unsplash; Other Parts By Author'
  prefs: []
  type: TYPE_NORMAL
- en: This process is what I had in mind when first learning about supervised ML.
    Reward it when it’s correct. Punish it (or simply no reward in this training process
    we designed) for it’s wrong. Probably the most intuitive way to train something.
  prefs: []
  type: TYPE_NORMAL
- en: Maximizing Objective
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'What’s our objective for the robot? We want its response to be correct as often
    as possible. More precisely, we want to find optimal parameter *θ**, that produces
    a *π_θ(a|s),* suchthat over all possible occurrences of *s* (sampled from image
    population distribution *D_s*)and *a* (sampled from distribution given *s* produced
    by model *π_θ(a|s)*), we get **maximum average reward weighted by probability**
    of each occurence of *(s,a)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/51cddd132910509e631770148af36d81.png)'
  prefs: []
  type: TYPE_IMG
- en: In other words, we are **maximizing** the objective function *J(θ)* defined
    as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/68e8b3d62384b11eb799409224934b10.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradient of Objective
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we have an objective function, we could probably try to maximize it through…gradient
    ascent! Namely, we can optimize the function by iteratively do
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/31073f9fbb212dcb9a46f8b235dde998.png)'
  prefs: []
  type: TYPE_IMG
- en: But how should we calculate the gradient, i.e. derivative of *J* respect to
    *θ*? It’s a bit tricky in this case since
  prefs: []
  type: TYPE_NORMAL
- en: The function that we want to take derivative of is an **expectation**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the expectation is not over a distribution dependent on *θ,* then by linearity
    of expectation, we can just take the derivative of what’s inside of expectation
    and leave expectation there. However, in this case,tThe expectation is over *(s,a)
    ~ (D_s, π_θ(a|s)),* which is dependent *θ*. So the derivative is not obvious.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another way of thinking about this is that *J*(*θ*) changes value as the frequency
    changes for how often we sample *(s,a)* from a distribution **partially determined
    by *θ****.* We want more frequent occurrences of *s=dog image* and *a=a_dog* (similar
    pair for cat). How can we capture **changes of *θ* towards this direction** when
    we do gradient ascent?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, **ideally, we want gradient to be in the form**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/df2aff2595ad8db90cba253e1fd6529d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is because you train the bot from samples of interactions between robot
    and you. Every sample consists of a triplet of *(s,a,r)*. We can therefore approximate
    this gradient by taking average of *f(θ,s,a,r)* with *N* samples that we collect
    (by law of large number, i.e. do Stochastic Gradient Ascent):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/20b0441122d80c37d43bb51c281ffe07.png)'
  prefs: []
  type: TYPE_IMG
- en: We can then do gradient ascent by doing
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b7d2290df9fb82d565f032c8fadfb1e5.png)'
  prefs: []
  type: TYPE_IMG
- en: Now let’s find *f.*
  prefs: []
  type: TYPE_NORMAL
- en: Finding Gradient
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To recap, we want to start from (1) to get (2) for some *f(θ,s,a,r)*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be0cf74430c73014af3e3081d153a191.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s first rewrite (1) with the **definition of expectation**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7acfd07aa0eb750ddb7697d47f03b45d.png)'
  prefs: []
  type: TYPE_IMG
- en: This is basically the integration of reward weighted by probability over all
    possible *(s,a)* pair.
  prefs: []
  type: TYPE_NORMAL
- en: Now, what’s exactly the joint probability *P(s,a)* for one pair of *(s,a)* to
    appear? We can decompose it into the probability of image sample (*s*) appearing,
    and the probability that the robot randomly select action *a*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc1e000dca4f52ae85ddd28428ab0330.png)'
  prefs: []
  type: TYPE_IMG
- en: Since the robot randomly select action *a* from the robot’s internal model of
    prediction *π_θ(a|s)*, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3ae5aed9db62c8211b8a17c33f7a77a7.png)'
  prefs: []
  type: TYPE_IMG
- en: Out of all terms inside of the parentheses, only *π_θ(a|s)* is dependent on
    *θ*. The other terms are all constant. So we can **move the gradient operation
    inside of the integral sign** next to this term and get
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4947001861b23bf63fb5bc7e2581012c.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that we can also write the following. Nothing big here. Just multiplying
    the original lefthand side by 1 written as a fraction and rearranging the terms.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6052d91540c72ca207601583ea3cd79b.png)'
  prefs: []
  type: TYPE_IMG
- en: Replacing it back, and rearranging a bit, we get
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e83df6f3f4d826ed91a7760403e76b72.png)'
  prefs: []
  type: TYPE_IMG
- en: '*P(s)π_θ(a|s)* looks familiar. It’s *P(s,a)* that we decomposed earlier! Putting
    it back, we have'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b5532069617ea101e52c2021c02c0e9c.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we have an integral and *P(s,a)*, we can…fit it back into **the definition
    of expectation**!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b3bb809839bdcaf9aafc99d278fe540a.png)'
  prefs: []
  type: TYPE_IMG
- en: Which is exactly The form we want to get in (2), where *f* is the terms inside
    of the bracket!
  prefs: []
  type: TYPE_NORMAL
- en: You may have wondered why we rewrite gradient of *π_θ(a|s)* in the clunky fraction
    earlier? The idea was to create a *π_θ(a|s)* term (which we lost earlier by taking
    derivative of it), so we could **produce a *P(s,a)* term again**, and turn the
    integration back into an expectation!
  prefs: []
  type: TYPE_NORMAL
- en: Building Cross Entropy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now it’s magic time.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a5366bbd4ec40454be4bf04c966b6033.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Don’t believe me? Work from right hand side to left hand side with chain rule.
    ([Optional] Side thoughts: So if you are also confused about the motivation of
    the log term in policy gradient formula, it is a side product of simplifying a
    clunky equation we get, with the intention of extracting a *π_θ(a|s)* term to
    transform things back to expectation.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'So we can simplify gradient of *J(θ)* a bit:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a9e1880f242614f1079f6fb15b8c37fa.png)'
  prefs: []
  type: TYPE_IMG
- en: So each time we have a batch of *(s,a)* as samples, we can do gradient ascent
    by
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1640cda6d2fd5a21a65a02d0a6e8afd5.png)'
  prefs: []
  type: TYPE_IMG
- en: To bring it to a more familiar form, Moving the gradient sign outside of summation,
    we have
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/18707025a5e1fd5d9ff9c696371f86ad.png)'
  prefs: []
  type: TYPE_IMG
- en: We will also invert the sign by doing
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a709db8808ec96d0725e5a23dcbc114c.png)'
  prefs: []
  type: TYPE_IMG
- en: Does it ring a bell? Let’s compare it with what we do when doing **gradient
    descent on cross entropy loss**.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that cross entropy loss is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/46bc5ce54fb4dc9bc83393022d14bef5.png)'
  prefs: []
  type: TYPE_IMG
- en: where **y_i** is true label, a one hot vector (*y_i_1, y_i_2*) that describes
    whether an image is cat and dog (either (0,1) or (1,0)). **y_hat_i** is prediction
    from a model, a vector (*y_hat_i_1, y_hat_i_2*) where the two entries some up
    two one.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we do **gradient descent** on this loss function, we calculate the cross
    entropy loss function for the batch, and hit the backprop button:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8f7faf6a722147b90275b2e04cdc4556.png)'
  prefs: []
  type: TYPE_IMG
- en: The differences between this expression and the gradient ascent express we derived
    earlier are
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/847bf691725e8e3dad2692bc2c512639.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To bring the relationship into words, it basically means: on sample *x_i,*
    y_i'
  prefs: []
  type: TYPE_NORMAL
- en: The model **makes prediction** (*y_hat_i_1, y_hat_i_2*) given *x_i*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model **randomly samples a response** from the predicted distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We **reward** response 1 with *y_i_1* and response 2 with *y_i_2*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since when the label is class 1, *y_i_1 = 1, y_i_2 = 0*, we **reward the model
    with 1** when model correctly responds 1 and there’s **0 reward** when it incorrectly
    responds 0\. Similar with class 2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And that’s exactly what we’ve been doing!
  prefs: []
  type: TYPE_NORMAL
- en: So to recap,
  prefs: []
  type: TYPE_NORMAL
- en: We **designed a simple training** setting where we **reward** **robot** with
    1 point when it answers correctly and 0 point when it answers incorrectly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We summerize what we want to achieve in an **objective function** that describes
    reward robot gets weighted by chances of its response
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We find the gradient descent procedure to **maximize this objective function**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And we get… the exact procedure that we use when **training a model by calculating
    Cross Entropy loss first and backproping through it**!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coming Back to Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let’s turn our focus back to reinforcement learning setting. What are the
    differences between RL and the supervised ML setting.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Timesteps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first difference is that RL usually involves in **multiple states and multiple
    episodes**. In our setting, the robot starts with input of the image, i.e. state
    *s.* After the robot gives you back an answer based on its prediction and collects
    the reward, the interactions between the robot and you are done.
  prefs: []
  type: TYPE_NORMAL
- en: In contrary, in RL problems, agents often interact with environment in **multiple
    episodes**, and it might transition to other states after the initial state.
  prefs: []
  type: TYPE_NORMAL
- en: The objective function then becomes
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0db0ebac8f528b86c6f438801036b294.png)'
  prefs: []
  type: TYPE_IMG
- en: Putting in words, we maximize **average sum of rewards** **of all timesteps**
    **over all possible sequence (trajectory) of states and actions**, **weighted
    by the probability of each trajectory** occurring when actions are decided by
    parameter *θ*.
  prefs: []
  type: TYPE_NORMAL
- en: Notes that *p_θ* is the joint distribution of a sequence of states and actions
    when actions are decided by agent’s model parameter *θ*. At each time step, the
    agent’s action is decided by *π_θ(a_t|s_t)*, where *π_θ* is a model parameterized
    by *θ*. *p_θ* is a **high level abstraction** of how probable it is for a sequence
    of states and actions to occur when the agent makes decision based on *π_θ* (i.e.
    *p_θ* is a placeholder for theoretically how often the agent takes on the trajectory.
    On the other hand, *π_θ(a|s)* is the probability that the agent will take an action
    at a specific timestep. We don’t actually easily know the value *p_θ*, so later
    we will rewrite it with model output *π_θ(a|s)* that we actually know).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s compare it with the objective we had earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/68e8b3d62384b11eb799409224934b10.png)'
  prefs: []
  type: TYPE_IMG
- en: The **main differences** are
  prefs: []
  type: TYPE_NORMAL
- en: We calculate expectation over a sequence of *s* and *a* instead of just one
    pair.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We maximize the sum of rewards from all timesteps in the trajectory instead
    of only the one-timestep reward from the image and answer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing the Gradient Formula
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can do the similar manipulations to this objective to derive the gradient
    that we can use to update *θ* at every timestep.
  prefs: []
  type: TYPE_NORMAL
- en: To recap, our goal is to find gradient of *J(θ)* in the following form for some
    *f*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/51c03df091a50c12a0c49fd25857a142.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When we obtain a batch of sampled sequence of *s_1, a_1, r_1, … s_T, a_T, r_T*,
    we can then update *θ* via Stochastic Gradient Ascent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dcf0c87b185cef2d80e89fe317052566.png)'
  prefs: []
  type: TYPE_IMG
- en: To simplify things, let’s denote the sequence of state and with one variable
    *τ*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b5550a0ddf26d2a80114c2964221b459.png)'
  prefs: []
  type: TYPE_IMG
- en: So we hope to maximize the following objective function
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3845823a7665d560b48d15d159a05a05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can do the similar manipulations that we did:'
  prefs: []
  type: TYPE_NORMAL
- en: Write **expectation in terms of integration**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/32f0fda37bbd20c49cd30d7d635828e1.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Take derivative** with respect to *θ* onthe only term involving *θ: p_θ(τ)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/dfad136479b7d13f40f32b4e33e58d29.png)'
  prefs: []
  type: TYPE_IMG
- en: Rewrite the gradient of *p_θ(τ)* as **product of *p_θ(τ)* and something else**
    to recover the form that **defines an expectation**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/6038a11e5a858b4a7dc55648e5dabef6.png)'
  prefs: []
  type: TYPE_IMG
- en: So we obtain
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/814cbdc34ccc9e58fd5ddf31e88a3f0b.png)'
  prefs: []
  type: TYPE_IMG
- en: Voila! It is exactly what We want to find. To put in words, it means we are
    updating *θ* **to the direction of gradient of log probability** of samples *τ*
    under the actions determined by *θ,* **weighted by the total reward** along the
    sampled *τ*. **This is exactly the formulation of Policy Gradients.**
  prefs: []
  type: TYPE_NORMAL
- en: If we extend the cross entropy analogy from earlier, sum of rewards are basically
    labels for the trajectory, and *p_θ(τ)* is how likely *τ* happensunder model’s
    prediction. The training process **encourages the model to predict distributions
    similar to the distribution of rewards over different trajectories *τ****.* (This
    is in fact a mathematically accurate statement [correct me if I’m wrong]. If you
    know about KL-Divergence, compare what’s being taken gradient of to KL-Divergence).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can do some more manipulations with conditional probabilities and definition
    of *p_θ(τ)*. This process is well explained by [this video](https://www.youtube.com/watch?v=GKoKNYaBvM0&list=PL_iWQOsE6TfXxKgI1GgyV1B_Xa0DxE5eH&index=22&ab_channel=RAIL)
    (around 9:27). We finally obtains the following, which rewrites *p_θ(τ)* as *π_θ(a_t|s_t)*
    that we actually knows value of:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/02b546318170e0f0cec5b21354e454c4.png)'
  prefs: []
  type: TYPE_IMG
- en: Note That **when T = 1** (single episode), **this is same as** **the gradient
    that we obtained in our setting before**. In other words, supervised ML is a special
    case of RL where there’s only one episode, and reward is non-stochastic (see the
    next section).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another Difference: Estimating Rewards'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another difference between RL and supervised ML is how much can we trust the
    rewards. In supervised ML, the reward are ground truth label that come with the
    image samples. We are usually 100% sure that the rewards are correct, and our
    robot will adjust its behaviors toward those labels.
  prefs: []
  type: TYPE_NORMAL
- en: However, in RL problems, the rewards could be **more stochastic** (imagine when
    you play a game, you could be in the same place twice but get different scores).
    So we have to **estimate the reward** for a particular state-action pair with
    historical reward as we interact with environment.
  prefs: []
  type: TYPE_NORMAL
- en: '***[Optional]*** *Side thoughts: I was also thinking if there’s middle territory
    between supervised ML (where labels/rewards are 100% trustable ) and RL (where
    rewards are more stochastic). It seems like when labels are noisy (contains some
    wrong labels), we are kind of in the middle? So would* [*psuedo-labeling method*](https://www.kaggle.com/code/cdeotte/pseudo-labeling-qda-0-969)
    *share some flavor as RL problem? Let me know your thoughts.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Technically, in the long run, we should have enough historical reward to understand
    the average reward behavior, but in the short run, small sample number might produce
    **unstable**, biased estimate about it.
  prefs: []
  type: TYPE_NORMAL
- en: Worse, since agent behavior is updated by the reward collected, if we collect
    low-quality rewards, we might go into and stuck at a bad policy. And it will take
    a long time to get out of there and back on right track.
  prefs: []
  type: TYPE_NORMAL
- en: This is one the challenges in RL that is still an ongoing research area. [Doing
    some manipulations to rewards](https://www.youtube.com/watch?v=VgdSubQN35g&list=PL_iWQOsE6TfXxKgI1GgyV1B_Xa0DxE5eH&index=25&ab_channel=RAIL)
    and variants of policy gradient such as [TRPO](https://arxiv.org/abs/1502.05477)
    and [PPO](https://arxiv.org/abs/1707.06347) are designed to address this issue
    better, and have become more commonly used than vanilla PG.
  prefs: []
  type: TYPE_NORMAL
- en: '[Optional] Another Side Thought: Comparison to Sequential Supervised ML'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One difference between our supervised ML setting and RL is that RL often involves
    multiple timesteps. I immediately had the question: then how does RL differ from
    training a sequential model like Transformer or LSTM?'
  prefs: []
  type: TYPE_NORMAL
- en: The answer to this question definitely depends on the exact loss design of the
    training your favorite sequential model.
  prefs: []
  type: TYPE_NORMAL
- en: For now, let’s say you train a sequence model *f(***x_1,x_2,…x_T***)* to predict
    **y_1, y_2…y_T**For example, in a machine translation task, **x**’s could be words
    of input English sentence, and **y***’s* are words output French sentence (each
    of **x_t, y_t** is a one hot vector representation of the word).
  prefs: []
  type: TYPE_NORMAL
- en: We calculate the loss function on each sample by taking sum of cross entropy
    between each word output prediction and truth label. We the average it over a
    batch of samples and do backprop like following
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fc39808c28ea708c98fd3999d99a357a.png)'
  prefs: []
  type: TYPE_IMG
- en: Putting back into the Policy Gradient formulation, this to me is same as calculating
    gradient of objective function as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/341af3d224e7b768f617589c55d9830e.png)'
  prefs: []
  type: TYPE_IMG
- en: The difference between this formulation and PG’s formulation is that we are
    not multiplying sum log probability of all timestep’s prediction with sum of rewards
    from all steps. Instead, we take pairwise product of log probability and reward
    of each timestep and sum them up.
  prefs: []
  type: TYPE_NORMAL
- en: This removes a lot of terms thus greatly reduce variance of gradients, which
    might be what make training a Transformer/LSTM in supervised setting easier than
    RL algorithm? (in addition to the non-stochastic rewards in supervised setting).
  prefs: []
  type: TYPE_NORMAL
- en: 'A technique of reducing variance of PG is introduced in [this video](https://www.youtube.com/watch?v=VgdSubQN35g&list=PL_iWQOsE6TfXxKgI1GgyV1B_Xa0DxE5eH&index=25&ab_channel=RAIL):
    Change the sum of reward of all time steps in PG to rewards to go (i.e. sum from
    *t’ = t to t’ = T*). This shares a similar flavor as what different between PG
    and training a Transformer/LSTM in supervised setting. While rewards to go method
    make the agent to evaluate each state by possible future rewards, could we say
    that supervised sequential training make model only focus on current timestep’s
    correctness?'
  prefs: []
  type: TYPE_NORMAL
- en: Also, I tried to work backwards from this gradient expression and find the original
    *J(θ)* that results this gradient expression, so we can more directly interpret
    the objective of supervised sequential training. But I got stuck in the halfway.
    Let me know if you have any thoughts.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The connection between policy gradient and cross entropy is not my own original
    idea. Thanks [this post](https://amoudgl.github.io/blog/blog/policy-gradient/)
    for giving me thoughts on expanding it to more fundamentally understand what cross
    entropy and policy gradient are doing.
  prefs: []
  type: TYPE_NORMAL
