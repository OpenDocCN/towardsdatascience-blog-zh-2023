["```py\n## start with data\ndata = open('path-to-data', 'r').read() # should be simple plain text file\n\nchars = list(set(data))\ndata_size, vocab_size = len(data), len(chars)\n\nprint('data has {} characters, {} unique.'.format(data_size, vocab_size))\n\nchar_to_idx = { ch:i for i,ch in enumerate(chars) }\nidx_to_char = { i:ch for i,ch in enumerate(chars) }\n```", "```py\npointer, seq_length = 0, 8\n\nx = [char_to_idx[ch] for ch in data[pointer:pointer+seq_length]]\n\ny = [char_to_idx[ch] for ch in data[pointer+1:pointer+seq_length+1]]\n\nprint(x)\n>> [2, 54, 53, 62, 13, 28, 20, 54] # our RNN input sequence\n\nprint(y)\n>> [54, 53, 62, 13, 28, 20, 54, 13] # our RNN target sequence\n\nfor t in range(seq_length):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n\n>>  when input is [2] the target: 54\n    when input is [2, 54] the target: 53\n    when input is [2, 54, 53] the target: 62\n    when input is [2, 54, 53, 62] the target: 13\n    when input is [2, 54, 53, 62, 13] the target: 28\n    when input is [2, 54, 53, 62, 13, 28] the target: 20\n    when input is [2, 54, 53, 62, 13, 28, 20] the target: 54\n    when input is [2, 54, 53, 62, 13, 28, 20, 54] the target: 13\n```", "```py\nclass RNN:\n    def __init__(self, hidden_size, vocab_size, seq_length, num_layers):\n        pass \n\n    def __call__(self, *args: Any, **kwds: Any):\n        \"\"\"RNN Forward Pass\"\"\"\n\n        pass \n\n    def backward(self, targets, cache):\n        \"\"\"RNN Backward Pass\"\"\"\n\n        pass\n\n    def update(self, grads, lr):\n        \"\"\"Perform Parameter Update w/ Adagrad\"\"\"\n\n        pass\n\n    def predict(self, hprev, seed_ix, n):\n        \"\"\"\n        Make predictions using the trained RNN model.\n\n        Parameters:\n        hprev (numpy array): The previous hidden state.\n        seed_ix (int): The seed letter index to start the prediction with.\n        n (int): The number of characters to generate for the prediction.\n\n        Returns:\n        ixes (list): The list of predicted character indices.\n        hs (numpy array): The final hidden state after making the predictions.\n        \"\"\"\n\n        pass \n```", "```py\ndef __init__(self, hidden_size, vocab_size, seq_length, num_layers):\n    self.name = 'RNN'\n    self.hidden_size = hidden_size\n    self.vocab_size = vocab_size\n    self.num_layers = num_layers\n\n    # model parameters\n    self.Wxh = [np.random.randn(hidden_size, vocab_size)*0.01 for _ in range(num_layers)] # input to hidden\n    self.Whh = [np.random.randn(hidden_size, hidden_size)*0.01 for _ in range(num_layers)] # hidden to hidden\n    self.Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n    self.bh = [np.zeros((hidden_size, 1)) for _ in range(num_layers)] # hidden bias\n    self.by = np.zeros((vocab_size, 1)) # output bias\n\n    # memory variables for training (ada grad from karpathy's github)\n    self.iteration, self.pointer = 0, 0\n    self.mWxh = [np.zeros_like(w) for w in self.Wxh]\n    self.mWhh = [np.zeros_like(w) for w in self.Whh] \n    self.mWhy = np.zeros_like(self.Why)\n    self.mbh, self.mby = [np.zeros_like(b) for b in self.bh], np.zeros_like(self.by)\n    self.loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n\n    self.running_loss = []\n```", "```py\ndef __call__(self, *args: Any, **kwds: Any) -> Any:\n    \"\"\"RNN Forward Pass\"\"\"\n\n    x, y, hprev = kwds['inputs'], kwds['targets'], kwds['hprev']\n\n    loss = 0\n    xs, hs, ys, ps = {}, {}, {}, {} # inputs, hidden state, output, probabilities\n    hs[-1] = np.copy(hprev)\n\n    # forward pass\n    for t in range(len(x)):\n        xs[t] = np.zeros((self.vocab_size,1)) # encode in 1-of-k representation\n        xs[t][x[t]] = 1\n        hs[t] = np.copy(hprev)\n\n        if kwds.get('dropout', False): # use dropout layer (mask)\n\n            for l in range(self.num_layers):\n                dropout_mask = (np.random.rand(*hs[t-1][l].shape) < (1-0.5)).astype(float)\n                hs[t-1][l] *= dropout_mask\n                hs[t][l] = np.tanh(np.dot(self.Wxh[l], xs[t]) + np.dot(self.Whh[l], hs[t-1][l]) + self.bh[l]) # hidden state\n                hs[t][l] = hs[t][l] / (1 - 0.5)\n\n        else: # no dropout layer (mask)\n\n            for l in range(self.num_layers):\n                hs[t][l] = np.tanh(np.dot(self.Wxh[l], xs[t]) + np.dot(self.Whh[l], hs[t-1][l]) + self.bh[l]) # hidden state\n\n        ys[t] = np.dot(self.Why, hs[t][-1]) + self.by # unnormalized log probabilities for next chars\n        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n        loss += -np.log(ps[t][y[t],0]) # softmax (cross-entropy loss)\n\n    self.running_loss.append(loss)\n\n    return loss, hs[len(x)-1], {'xs':xs, 'hs':hs, 'ps':ps}\n```", "```py\n# Initialize RNN\nnum_layers = 3\nhidden_size = 100\nseq_length = 8\n\nrnn = RNN(hidden_size=hidden_size, vocab_size=vocab_size, seq_length=seq_length, num_layers=num_layers)\n\nx = [char_to_idx[ch] for ch in data[rnn.pointer:rnn.pointer+seq_length]]\n\ny = [char_to_idx[ch] for ch in data[rnn.pointer+1:rnn.pointer+seq_length+1]]\n\n# initialize hidden state with zeros\nhprev = [np.zeros((hidden_size, 1)) for _ in range(num_layers)] \n\n## Call RNN\nloss, hprev, cache = rnn(inputs=x, targets=y, hprev=hprev)\n\nprint(loss)\n>> 33.38852380987117\n```", "```py\ndef backward(self, targets, cache):\n    \"\"\"RNN Backward Pass\"\"\"\n\n    # unpack cache\n    xs, hs, ps = cache['xs'], cache['hs'], cache['ps']\n\n    # initialize gradients to zero\n    dWxh, dWhh, dWhy = [np.zeros_like(w) for w in self.Wxh], [np.zeros_like(w) for w in self.Whh], np.zeros_like(self.Why)\n    dbh, dby = [np.zeros_like(b) for b in self.bh], np.zeros_like(self.by)\n    dhnext = [np.zeros_like(h) for h in hs[0]]\n\n    for t in reversed(range(len(xs))):\n\n        dy = np.copy(ps[t])\n\n        # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n        dy[targets[t]] -= 1 \n\n        dWhy += np.dot(dy, hs[t][-1].T)\n        dby += dy\n\n        for l in reversed(range(self.num_layers)):\n            dh = np.dot(self.Why.T, dy) + dhnext[l]\n            dhraw = (1 - hs[t][l] * hs[t][l]) * dh # backprop through tanh nonlinearity\n            dbh[l] += dhraw\n            dWxh[l] += np.dot(dhraw, xs[t].T)\n            dWhh[l] += np.dot(dhraw, hs[t-1][l].T)\n            dhnext[l] = np.dot(self.Whh[l].T, dhraw)\n\n    return {'dWxh':dWxh, 'dWhh':dWhh, 'dWhy':dWhy, 'dbh':dbh, 'dby':dby}\n```", "```py\n# Initialize RNN\nnum_layers = 3\nhidden_size = 100\nseq_length = 8\n\nrnn = RNN(hidden_size=hidden_size, vocab_size=vocab_size, seq_length=seq_length, num_layers=num_layers)\n\nx = [char_to_idx[ch] for ch in data[rnn.pointer:rnn.pointer+seq_length]]\n\ny = [char_to_idx[ch] for ch in data[rnn.pointer+1:rnn.pointer+seq_length+1]]\n\n# initialize hidden state with zeros\nhprev = [np.zeros((hidden_size, 1)) for _ in range(num_layers)] \n\n## Call RNN\nloss, hprev, cache = rnn(inputs=x, targets=y, hprev=hprev)\ngrads = rnn.backward(targets=y, cache=cache)\n```", "```py\ndef update(self, grads, lr):\n    \"\"\"Perform Parameter Update w/ Adagrad\"\"\"\n\n    # unpack grads\n    dWxh, dWhh, dWhy = grads['dWxh'], grads['dWhh'], grads['dWhy']\n    dbh, dby = grads['dbh'], grads['dby']\n\n    # loop through each layer\n    for i in range(self.num_layers):\n\n        # clip gradients to mitigate exploding gradients\n        np.clip(dWxh[i], -5, 5, out=dWxh[i])\n        np.clip(dWhh[i], -5, 5, out=dWhh[i])\n        np.clip(dbh[i], -5, 5, out=dbh[i])\n\n        # perform parameter update with Adagrad\n        self.mWxh[i] += dWxh[i] * dWxh[i]\n        self.Wxh[i] -= lr * dWxh[i] / np.sqrt(self.mWxh[i] + 1e-8)\n        self.mWhh[i] += dWhh[i] * dWhh[i]\n        self.Whh[i] -= lr * dWhh[i] / np.sqrt(self.mWhh[i] + 1e-8)\n        self.mbh[i] += dbh[i] * dbh[i]\n        self.bh[i] -= lr * dbh[i] / np.sqrt(self.mbh[i] + 1e-8)\n\n    # clip gradients for Why and by\n    np.clip(dWhy, -5, 5, out=dWhy)\n    np.clip(dby, -5, 5, out=dby)\n\n    # perform parameter update with Adagrad\n    self.mWhy += dWhy * dWhy\n    self.Why -= lr * dWhy / np.sqrt(self.mWhy + 1e-8)\n    self.mby += dby * dby\n    self.by -= lr * dby / np.sqrt(self.mby + 1e-8)\n```", "```py\n# Initialize RNN\nnum_layers = 3\nhidden_size = 100\nseq_length = 8\n\nrnn = RNN(hidden_size=hidden_size, vocab_size=vocab_size, seq_length=seq_length, num_layers=num_layers)\n\nx = [char_to_idx[ch] for ch in data[rnn.pointer:rnn.pointer+seq_length]]\n\ny = [char_to_idx[ch] for ch in data[rnn.pointer+1:rnn.pointer+seq_length+1]]\n\n# initialize hidden state with zeros\nhprev = [np.zeros((hidden_size, 1)) for _ in range(num_layers)] \n\n## Call RNN\nloss, hprev, cache = rnn(inputs=x, targets=y, hprev=hprev)\ngrads = rnn.backward(targets=y, cache=cache)\nrnn.update(grads=grads, lr=1e-1)\n```", "```py\ndef train(rnn, epochs, data, lr=1e-1, use_drop=False):\n\n    for _ in range(epochs):\n\n        # prepare inputs (we're sweeping from left to right in steps seq_length long)\n        if rnn.pointer+seq_length+1 >= len(data) or rnn.iteration == 0:\n\n            hprev = [np.zeros((hidden_size, 1)) for _ in range(rnn.num_layers)]  # reset RNN memory\n\n            rnn.pointer = 0 # go from start of data\n\n        x = [char_to_idx[ch] for ch in data[rnn.pointer:rnn.pointer+seq_length]]\n        y = [char_to_idx[ch] for ch in data[rnn.pointer+1:rnn.pointer+seq_length+1]]\n\n        if use_drop:\n            loss, hprev, cache = rnn(inputs=x, targets=y, hprev=hprev, dropout=True)\n        else:\n            loss, hprev, cache = rnn(inputs=x, targets=y, hprev=hprev)\n\n        grads = rnn.backward(targets=y, cache=cache)\n        rnn.update(grads=grads, lr=lr)\n\n        # update loss\n        rnn.loss = rnn.loss * 0.999 + loss * 0.001\n\n        ## show progress now and then\n        if rnn.iteration % 1000 == 0: \n            print('iter {}, loss: {}'.format(rnn.iteration, rnn.loss))\n\n            sample_ix = rnn.predict(hprev, x[0], 200)\n            txt = ''.join(idx_to_char[ix] for ix in sample_ix)\n            print('Sample')\n            print ('----\\n {} \\n----'.format(txt))\n\n        rnn.pointer += seq_length # move data pointer\n        rnn.iteration += 1 # iteration counter\n\n## hyper-params\nnum_layers = 2\nhidden_size = 128\nseq_length = 13\n\n# Initialize RNN\nrnn = RNN(hidden_size=hidden_size, \n          vocab_size=vocab_size, \n          seq_length=seq_length, \n          num_layers=num_layers)\n\ntrain(rnn=rnn, epochs=15000, data=data)\n```"]