- en: Interpreting Random Forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/interpreting-random-forests-638bca8b49ea?source=collection_archive---------0-----------------------#2023-10-08](https://towardsdatascience.com/interpreting-random-forests-638bca8b49ea?source=collection_archive---------0-----------------------#2023-10-08)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Comprehensive guide on Random Forest algorithms and how to interpret them
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://miptgirl.medium.com/?source=post_page-----638bca8b49ea--------------------------------)[![Mariya
    Mansurova](../Images/b1dd377b0a1887db900cc5108bca8ea8.png)](https://miptgirl.medium.com/?source=post_page-----638bca8b49ea--------------------------------)[](https://towardsdatascience.com/?source=post_page-----638bca8b49ea--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----638bca8b49ea--------------------------------)
    [Mariya Mansurova](https://miptgirl.medium.com/?source=post_page-----638bca8b49ea--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F15a29a4fc6ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpreting-random-forests-638bca8b49ea&user=Mariya+Mansurova&userId=15a29a4fc6ad&source=post_page-15a29a4fc6ad----638bca8b49ea---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----638bca8b49ea--------------------------------)
    ·13 min read·Oct 8, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F638bca8b49ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpreting-random-forests-638bca8b49ea&user=Mariya+Mansurova&userId=15a29a4fc6ad&source=-----638bca8b49ea---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F638bca8b49ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpreting-random-forests-638bca8b49ea&source=-----638bca8b49ea---------------------bookmark_footer-----------)![](../Images/9f7911c6b8fe778a06e12e9223eea45a.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Sergei A](https://unsplash.com/@sakulich?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: There is a lot of hype about Large Language Models nowadays, but it doesn’t
    mean that old-school ML approaches now deserve extinction. I doubt that ChatGPT
    will be helpful if you give it a dataset with hundreds numeric features and ask
    it to predict a target value.
  prefs: []
  type: TYPE_NORMAL
- en: Neural Networks are usually the best solution in case of unstructured data (for
    example, texts, images or audio). But, for tabular data, we can still benefit
    from the good old Random Forest.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most significant advantages of Random Forest algorithms are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: You only need to do a little data preprocessing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s rather difficult to screw up with Random Forests. You won’t face overfitting
    issues if you have enough trees in your ensemble since adding more trees decreases
    the error.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s easy to interpret results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That’s why Random Forest could be a good candidate for your first model when
    starting a new task with tabular data.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I would like to cover the basics of Random Forests and go through
    approaches to interpreting model results.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will learn how to find answers to the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What features are important, and which ones are redundant and can be removed?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does each feature value affect our target metric?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the factors for each prediction?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to estimate the confidence of each prediction?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will be using [the Wine Quality dataset](https://archive.ics.uci.edu/dataset/186/wine+quality).
    It shows the relation between wine quality and physicochemical test for the different
    Portuguese “Vinho Verde” wine variants. We will try to predict wine quality based
    on wine characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: 'With decision trees, we don’t need to do a lot of preprocessing:'
  prefs: []
  type: TYPE_NORMAL
- en: We don’t need to create dummy variables since the algorithm can handle it automatically.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We don’t need to do normalisation or get rid of outliers because only ordering
    matters. So, Decision Tree based models are resistant to outliers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, the scikit-learn realisation of Decision Trees can’t work with categorical
    variables or Null values. So, we have to handle it ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, there are no missing values in our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: And we only need to transform the `type` variable (‘*red*’ or ‘*white*’) from
    `string` to `integer`. We can use pandas `Categorical` transformation for it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now, `df['type']` equals 0 for red wines and 1 for white vines.
  prefs: []
  type: TYPE_NORMAL
- en: The other crucial part of preprocessing is to split our dataset into train and
    validation sets. So, we can use a validation set to assess our model’s quality.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We’ve finished the preprocessing step and are ready to move on to the most exciting
    part — training models.
  prefs: []
  type: TYPE_NORMAL
- en: The basics of Decision Trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before jumping into the training, let’s spend some time understanding how Random
    Forests work.
  prefs: []
  type: TYPE_NORMAL
- en: Random Forest is an ensemble of Decision Trees. So, we should start with the
    elementary building block — Decision Tree.
  prefs: []
  type: TYPE_NORMAL
- en: In our example of predicting wine quality, we will be solving a regression task,
    so let’s start with it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Decision Tree: Regression'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s fit a default decision tree model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: One of the most significant advantages of Decision Trees is that we can easily
    interpret these models — it’s just a set of questions. Let’s visualise it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/f551a7b5db06e0750cdf6e88f832ebc9.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the Decision Tree consists of binary splits. On each node, we
    are splitting our dataset into 2.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we calculate predictions for the leaf nodes as an average of all data
    points in this node.
  prefs: []
  type: TYPE_NORMAL
- en: '**Side note:** Because Decision Tree returns an average of all data points
    for a leaf node, Decision Trees are pretty bad in extrapolation. So, you need
    to keep an eye on the feature distributions during training and inference.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s brainstorm how to identify the best split for our dataset. We can start
    with one variable and define the optimal division for it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have a feature with four unique values: 1, 2, 3 and 4\. Then, there
    are three possible thresholds between them.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ab26cac2feda483eeef4fd4f32694100.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: We can consequently take each threshold and calculate predicted values for our
    data as an average value for leaf nodes. Then, we can use these predicted values
    to get MSE (Mean Square Error) for each threshold. The best split will be the
    one with the lowest MSE. By default, [DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor)
    from scikit-learn works similarly and uses MSE as a criterion.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s calculate the best split for `sulphates` feature manually to understand
    better how it works.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We can see that for `sulphates`, the best threshold is 0.685 since it gives
    the lowest MSE.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can use this function for all features we have to define the best split
    overall.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We got absolutely the same result as our initial decision tree with the first
    split on `alcohol <= 10.625` .
  prefs: []
  type: TYPE_NORMAL
- en: To build the whole Decision Tree, we could recursively calculate the best splits
    for each of the datasets `alcohol <= 10.625` and `alcohol > 10.625` and get the
    next level of Decision Tree. Then, repeat.
  prefs: []
  type: TYPE_NORMAL
- en: The stopping criteria for recursion could be either the depth or the minimal
    size of the leaf node. Here’s an example of a Decision Tree with at least 420
    items in the leaf nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/d31101f15f8e1b0322f86dc3ffe8f735.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: Let’s calculate the mean absolute error on the validation set to understand
    how good our model is. I prefer MAE over MSE (Mean Squared Error) because it’s
    less affected by outliers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Decision Tree: Classification'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve looked at the regression example. In the case of classification, it’s
    a bit different. Even though we won’t go deep into classification examples in
    this article, it’s still worth discussing its basics.
  prefs: []
  type: TYPE_NORMAL
- en: For classification, instead of the average value, we use the most common class
    as a prediction for each leaf node.
  prefs: []
  type: TYPE_NORMAL
- en: We usually use [the Gini coefficient](https://en.wikipedia.org/wiki/Gini_coefficient)
    to estimate the binary split’s quality for classification. Imagine getting one
    random item from the sample and then the other. The Gini coefficient would be
    equal to the probability of the situation when items are from different classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say we have only two classes, and the share of items from the first class
    is equal to `p` . Then we can calculate the Gini coefficient using the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bbad8f997ff9894f263bdab832ae5b93.png)'
  prefs: []
  type: TYPE_IMG
- en: If our classification model is perfect, the Gini coefficient equals 0\. In the
    worst case (`p = 0.5`), the Gini coefficient equals 0.5.
  prefs: []
  type: TYPE_NORMAL
- en: To calculate the metric for binary split, we calculate Gini coefficients for
    both parts (left and right ones) and norm them on the number of samples in each
    partition.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/708b9e202a2290157cd93405ed5cdeec.png)'
  prefs: []
  type: TYPE_IMG
- en: Then, we can similarly calculate our optimisation metric for different thresholds
    and use the best option.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve trained a simple Decision Tree model and discussed how it works. Now,
    we are ready to move on to the Random Forests.
  prefs: []
  type: TYPE_NORMAL
- en: Random Forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Random Forests are based on the concept of Bagging. The idea is to fit a bunch
    of independent models and use an average prediction from them. Since models are
    independent, errors are not correlated. We assume that our models have no systematic
    errors, and the average of many errors should be close to zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'How could we get lots of independent models? It’s pretty straightforward: we
    can train Decision Trees on random subsets of rows and features. It will be a
    Random Forest.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s train a basic Random Forest with 100 trees and the minimal size of leaf
    nodes equal to 100.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'With random forest, we’ve achieved a much better quality than with one Decision
    Tree: 0.5592 vs. 0.5891.'
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The meaningful question is whether Random Forrest could overfit.
  prefs: []
  type: TYPE_NORMAL
- en: Actually, no. Since we are averaging not correlated errors, we cannot overfit
    the model by adding more trees. Quality will improve asymptotically with the increase
    in the number of trees.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/49be396d4feb06c09eeba98f918ff332.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: However, you might face overfitting if you have deep trees and not enough of
    them. It’s easy to overfit one Decision Tree.
  prefs: []
  type: TYPE_NORMAL
- en: Out-of-bag error
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since only part of the rows is used for each tree in Random Forest, we can use
    them to estimate the error. For each row, we can select only trees where this
    row wasn’t used and use them to make predictions. Then, we can calculate errors
    based on these predictions. Such an approach is called “out-of-bag error”.
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the OOB error is much closer to the error on the validation
    set than the one for training, which means it’s a good approximation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Interpreting the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As I mentioned in the beginning, the big advantage of Decision Trees is that
    it’s easy to interpret them. Let’s try to understand our model better.
  prefs: []
  type: TYPE_NORMAL
- en: Feature importances
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The calculation of the feature importance is pretty straightforward. We look
    at each decision tree in the ensemble and each binary split and calculate its
    impact on our metric (`squared_error` in our case).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the first split by `alcohol` for one of our initial decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3c28c776fe6bc0e1408a5e9ff3609c50.png)![](../Images/51b302bc71cd34b8b710b6e21c9ef5d0.png)'
  prefs: []
  type: TYPE_IMG
- en: Then, we can do the same calculations for all binary splits in all decision
    trees, add everything up, normalize and get the relative importance for each feature.
  prefs: []
  type: TYPE_NORMAL
- en: If you use scikit-learn, you don’t need to calculate feature importance manually.
    You can just take `model.feature_importances_`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the most important features overall are `alcohol` and `volatile
    acidity` .
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/063ae65bbcf00de796cfe4571a702c15.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: Partial Dependence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding how each feature affects our target metric is exciting and often
    useful. For example, whether quality increases/decreases with higher alcohol or
    there’s a more complex relation.
  prefs: []
  type: TYPE_NORMAL
- en: We could just get data from our dataset and plot averages by alcohol, but it
    won’t be correct since there might be some correlations. For example, higher alcohol
    in our dataset could also correspond to more elevated sugar and better quality.
  prefs: []
  type: TYPE_NORMAL
- en: 'To estimate the impact only from alcohol, we can take all rows in our dataset
    and, using the ML model, predict the quality for each row for different values
    of alcohol: 9, 9.1, 9.2, etc. Then, we can average results and get the actual
    relation between alcohol level and wine quality. So, all the data is equal, and
    we are just varying alcohol levels.'
  prefs: []
  type: TYPE_NORMAL
- en: This approach could be used with any ML model, not only Random Forest.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We can use `sklearn.inspection` module to easily plot this relations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can gain quite a lot of insights from these graphs, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: wine quality increases with the growth of free sulfur dioxide up to 30, but
    it’s stable after this threshold;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: with alcohol, the higher the level — the better the quality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/92b71652cf4a54d3b503ac3db8aa296e.png)'
  prefs: []
  type: TYPE_IMG
- en: We can even look at relations between two variables. It can be pretty complex.
    For example, if the alcohol level is above 11.5, volatile acidity has no effect.
    But, for lower alcohol levels, volatile acidity significantly impacts quality.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/6261b9202a6fb1282941e3c4a6ec3f03.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Confidence of predictions**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using Random Forests, we can also assess how confident each prediction is. For
    that, we could calculate predictions from each tree in the ensemble and look at
    variance or standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We can see that there are predictions with low standard deviation (i.e. below
    0.15) and the ones with `std` above 0.3.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4631de9eb676c2bf7915fcddba6c734f.png)'
  prefs: []
  type: TYPE_IMG
- en: If we use the model for business purposes, we can treat such cases differently.
    For example, do not take into account prediction if `std` above `X` or show to
    the customer intervals (i.e. percentile 25% and percentile 75%).
  prefs: []
  type: TYPE_NORMAL
- en: How prediction was made?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can also use packages `treeinterpreter` and `waterfallcharts` to understand
    how each prediction was made. It could be handy in some business cases, for example,
    when you need to tell customers why credit for them was rejected.
  prefs: []
  type: TYPE_NORMAL
- en: We will look at one of the wines as an example. It has relatively low alcohol
    and high volatile acidity.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d48d317caa71734745606b63cf9137fb.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The graph shows that this wine is better than average. The main factor that
    increases quality is a low level of volatile acidity, while the main disadvantage
    is a low level of alcohol.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a5ec8eadcafc2a31cf285a9ce472698d.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: So, there are a lot of handy tools that could help you to understand your data
    and model much better.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing number of trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The other cool feature of Random Forest is that we could use it to reduce the
    number of features for any tabular data. You can quickly fit a Random Forest and
    define a list of meaningful columns in your data.
  prefs: []
  type: TYPE_NORMAL
- en: More data doesn’t always mean better quality. Also, it can affect your model
    performance during training and inference.
  prefs: []
  type: TYPE_NORMAL
- en: Since in our initial wine dataset, there were only 12 features, for this case,
    we will use a slightly bigger dataset — [Online News Popularity](https://archive.ics.uci.edu/dataset/332/online+news+popularity).
  prefs: []
  type: TYPE_NORMAL
- en: Looking at feature importance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, let’s build a Random Forest and look at feature importances. 34 out of
    59 features have an importance lower than 0.01.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try to remove them and look at accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '***MAE on validation set for all features***: 2969.73'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***MAE on validation set for 25 important features***: 2975.61'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The difference in quality is not so big, but we could make our model faster
    in the training and inference stages. We’ve already removed almost 60% of the
    initial features — good job.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at redundant features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For the remaining features, let’s see whether there are redundant (highly correlated)
    ones. For that, we will use a Fast.AI tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/605fb7a9d7b4700df6e962317668ae53.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We could see that the following features are close to each other:'
  prefs: []
  type: TYPE_NORMAL
- en: '`self_reference_avg_sharess` and `self_reference_max_shares`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kw_min_avg` and `kw_min_max`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_non_stop_unique_tokens` and `n_unique_tokens` .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s remove them as well.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Quality even a little bit improved. So, we’ve reduced the number of features
    from 59 to 22 and increased the error only by 0.17%. It proves that such an approach
    works.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the full code on [GitHub](https://github.com/miptgirl/miptgirl_medium/tree/main/random_forests_101).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this article, we’ve discussed how Decision Tree and Random Forest algorithms
    work. Also, we’ve learned how to interpret Random Forests:'
  prefs: []
  type: TYPE_NORMAL
- en: How to use feature importance to get the list of the most significant features
    and reduce the number of parameters in your model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to define the effect of each feature value on the target metric using partial
    dependence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to estimate the impact of different features on each prediction using `treeinterpreter`
    library.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thank you a lot for reading this article. I hope it was insightful to you. If
    you have any follow-up questions or comments, please leave them in the comments
    section.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Cortez,Paulo, Cerdeira,A., Almeida,F., Matos,T., and Reis,J.. (2009). Wine
    Quality. UCI Machine Learning Repository.* [*https://doi.org/10.24432/C56S3T*](https://doi.org/10.24432/C56S3T)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Fernandes,Kelwin, Vinagre,Pedro, Cortez,Paulo, and Sernadela,Pedro. (2015).
    Online News Popularity. UCI Machine Learning Repository.* [*https://doi.org/10.24432/C5NS3V*](https://doi.org/10.24432/C5NS3V)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article was inspired by ***Fast.AI Deep Learning Course***
  prefs: []
  type: TYPE_NORMAL
- en: '[Lesson 6: Random Forests](https://course.fast.ai/Lessons/lesson6.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chapter 9](https://github.com/fastai/fastbook/blob/master/09_tabular.ipynb)
    of “Deep Learning for Coders with Fast.AI and Pytorch: AI Applications Without
    a PhD” by J. Howard and S. Gugger'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for [Decision Trees](https://scikit-learn.org/stable/modules/tree.html)
    and [Random Forests](https://scikit-learn.org/stable/modules/ensemble.html#random-forests-and-other-randomized-tree-ensembles)
    from `scikit-learn`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
