- en: Interpreting Random Forests
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解读随机森林
- en: 原文：[https://towardsdatascience.com/interpreting-random-forests-638bca8b49ea?source=collection_archive---------0-----------------------#2023-10-08](https://towardsdatascience.com/interpreting-random-forests-638bca8b49ea?source=collection_archive---------0-----------------------#2023-10-08)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/interpreting-random-forests-638bca8b49ea?source=collection_archive---------0-----------------------#2023-10-08](https://towardsdatascience.com/interpreting-random-forests-638bca8b49ea?source=collection_archive---------0-----------------------#2023-10-08)
- en: Comprehensive guide on Random Forest algorithms and how to interpret them
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于随机森林算法及其解读的全面指南
- en: '[](https://miptgirl.medium.com/?source=post_page-----638bca8b49ea--------------------------------)[![Mariya
    Mansurova](../Images/b1dd377b0a1887db900cc5108bca8ea8.png)](https://miptgirl.medium.com/?source=post_page-----638bca8b49ea--------------------------------)[](https://towardsdatascience.com/?source=post_page-----638bca8b49ea--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----638bca8b49ea--------------------------------)
    [Mariya Mansurova](https://miptgirl.medium.com/?source=post_page-----638bca8b49ea--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://miptgirl.medium.com/?source=post_page-----638bca8b49ea--------------------------------)[![Mariya
    Mansurova](../Images/b1dd377b0a1887db900cc5108bca8ea8.png)](https://miptgirl.medium.com/?source=post_page-----638bca8b49ea--------------------------------)[](https://towardsdatascience.com/?source=post_page-----638bca8b49ea--------------------------------)[![数据科学的前沿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----638bca8b49ea--------------------------------)
    [Mariya Mansurova](https://miptgirl.medium.com/?source=post_page-----638bca8b49ea--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F15a29a4fc6ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpreting-random-forests-638bca8b49ea&user=Mariya+Mansurova&userId=15a29a4fc6ad&source=post_page-15a29a4fc6ad----638bca8b49ea---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----638bca8b49ea--------------------------------)
    ·13 min read·Oct 8, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F638bca8b49ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpreting-random-forests-638bca8b49ea&user=Mariya+Mansurova&userId=15a29a4fc6ad&source=-----638bca8b49ea---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F15a29a4fc6ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpreting-random-forests-638bca8b49ea&user=Mariya+Mansurova&userId=15a29a4fc6ad&source=post_page-15a29a4fc6ad----638bca8b49ea---------------------post_header-----------)
    发表在 [数据科学的前沿](https://towardsdatascience.com/?source=post_page-----638bca8b49ea--------------------------------)
    ·13分钟阅读·2023年10月8日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F638bca8b49ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpreting-random-forests-638bca8b49ea&user=Mariya+Mansurova&userId=15a29a4fc6ad&source=-----638bca8b49ea---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F638bca8b49ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpreting-random-forests-638bca8b49ea&source=-----638bca8b49ea---------------------bookmark_footer-----------)![](../Images/9f7911c6b8fe778a06e12e9223eea45a.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F638bca8b49ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpreting-random-forests-638bca8b49ea&source=-----638bca8b49ea---------------------bookmark_footer-----------)![](../Images/9f7911c6b8fe778a06e12e9223eea45a.png)'
- en: Photo by [Sergei A](https://unsplash.com/@sakulich?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Sergei A](https://unsplash.com/@sakulich?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: There is a lot of hype about Large Language Models nowadays, but it doesn’t
    mean that old-school ML approaches now deserve extinction. I doubt that ChatGPT
    will be helpful if you give it a dataset with hundreds numeric features and ask
    it to predict a target value.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 现在对大语言模型的炒作很多，但这并不意味着老派的机器学习方法现在就该消失。我怀疑如果你给 ChatGPT 一个有数百个数值特征的数据集，并要求它预测目标值，它会有帮助。
- en: Neural Networks are usually the best solution in case of unstructured data (for
    example, texts, images or audio). But, for tabular data, we can still benefit
    from the good old Random Forest.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络通常是处理非结构化数据（例如文本、图像或音频）的最佳解决方案。但对于表格数据，我们仍然可以从传统的随机森林中受益。
- en: 'The most significant advantages of Random Forest algorithms are the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林算法的最重要优点如下：
- en: You only need to do a little data preprocessing.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你只需要做一些数据预处理。
- en: It’s rather difficult to screw up with Random Forests. You won’t face overfitting
    issues if you have enough trees in your ensemble since adding more trees decreases
    the error.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林算法很难出错。如果你的集成中有足够多的树，你不会面临过拟合问题，因为增加更多的树会降低误差。
- en: It’s easy to interpret results.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果很容易解释。
- en: That’s why Random Forest could be a good candidate for your first model when
    starting a new task with tabular data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么随机森林可能是你在处理表格数据的新任务时的第一个模型的好候选者。
- en: In this article, I would like to cover the basics of Random Forests and go through
    approaches to interpreting model results.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我希望覆盖随机森林的基础知识，并探讨解释模型结果的方法。
- en: 'We will learn how to find answers to the following questions:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将学习如何找到以下问题的答案：
- en: What features are important, and which ones are redundant and can be removed?
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪些特征是重要的，哪些是冗余的可以被移除？
- en: How does each feature value affect our target metric?
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个特征值如何影响我们的目标指标？
- en: What are the factors for each prediction?
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个预测的因素是什么？
- en: How to estimate the confidence of each prediction?
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何估计每个预测的置信度？
- en: Preprocessing
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理
- en: We will be using [the Wine Quality dataset](https://archive.ics.uci.edu/dataset/186/wine+quality).
    It shows the relation between wine quality and physicochemical test for the different
    Portuguese “Vinho Verde” wine variants. We will try to predict wine quality based
    on wine characteristics.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用[葡萄酒质量数据集](https://archive.ics.uci.edu/dataset/186/wine+quality)。它展示了葡萄酒质量与不同葡萄牙“Vinho
    Verde”葡萄酒变体的物理化学测试之间的关系。我们将尝试基于葡萄酒特性预测葡萄酒质量。
- en: 'With decision trees, we don’t need to do a lot of preprocessing:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用决策树时，我们不需要做很多预处理。
- en: We don’t need to create dummy variables since the algorithm can handle it automatically.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不需要创建虚拟变量，因为算法可以自动处理。
- en: We don’t need to do normalisation or get rid of outliers because only ordering
    matters. So, Decision Tree based models are resistant to outliers.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不需要做归一化或去除异常值，因为只有排序是重要的。因此，基于决策树的模型对异常值具有抗性。
- en: However, the scikit-learn realisation of Decision Trees can’t work with categorical
    variables or Null values. So, we have to handle it ourselves.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，scikit-learn 实现的决策树无法处理分类变量或缺失值。因此，我们需要自己处理这些问题。
- en: Fortunately, there are no missing values in our dataset.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们的数据集中没有缺失值。
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: And we only need to transform the `type` variable (‘*red*’ or ‘*white*’) from
    `string` to `integer`. We can use pandas `Categorical` transformation for it.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要将 `type` 变量（‘*red*’ 或 ‘*white*’）从 `string` 转换为 `integer`。我们可以使用 pandas
    的 `Categorical` 转换来实现。
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now, `df['type']` equals 0 for red wines and 1 for white vines.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`df['type']` 对于红酒等于0，对于白酒等于1。
- en: The other crucial part of preprocessing is to split our dataset into train and
    validation sets. So, we can use a validation set to assess our model’s quality.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理的另一个关键部分是将数据集拆分为训练集和验证集。这样，我们可以使用验证集来评估模型的质量。
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We’ve finished the preprocessing step and are ready to move on to the most exciting
    part — training models.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了预处理步骤，并准备好进入最激动人心的部分——训练模型。
- en: The basics of Decision Trees
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树的基础知识
- en: Before jumping into the training, let’s spend some time understanding how Random
    Forests work.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始训练之前，让我们花些时间理解随机森林的工作原理。
- en: Random Forest is an ensemble of Decision Trees. So, we should start with the
    elementary building block — Decision Tree.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是决策树的集成。因此，我们应该从基本的构建块——决策树开始。
- en: In our example of predicting wine quality, we will be solving a regression task,
    so let’s start with it.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的葡萄酒质量预测示例中，我们将解决一个回归任务，因此让我们从这开始。
- en: 'Decision Tree: Regression'
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树：回归
- en: Let’s fit a default decision tree model.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们拟合一个默认的决策树模型。
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: One of the most significant advantages of Decision Trees is that we can easily
    interpret these models — it’s just a set of questions. Let’s visualise it.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的一个显著优势是我们可以轻松解释这些模型——它只是一些问题。让我们可视化一下。
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/f551a7b5db06e0750cdf6e88f832ebc9.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f551a7b5db06e0750cdf6e88f832ebc9.png)'
- en: Graph by author
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图表
- en: As you can see, the Decision Tree consists of binary splits. On each node, we
    are splitting our dataset into 2.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，决策树由二元分裂组成。在每个节点上，我们将数据集拆分为2部分。
- en: Finally, we calculate predictions for the leaf nodes as an average of all data
    points in this node.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们计算叶子节点的预测值作为该节点中所有数据点的平均值。
- en: '**Side note:** Because Decision Tree returns an average of all data points
    for a leaf node, Decision Trees are pretty bad in extrapolation. So, you need
    to keep an eye on the feature distributions during training and inference.'
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**附注：** 由于决策树对叶节点的所有数据点取平均值，因此决策树在外推方面表现较差。因此，在训练和推理过程中需要关注特征分布。'
- en: Let’s brainstorm how to identify the best split for our dataset. We can start
    with one variable and define the optimal division for it.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们头脑风暴一下如何确定数据集的最佳分裂。我们可以从一个变量开始，定义其最佳划分。
- en: 'Suppose we have a feature with four unique values: 1, 2, 3 and 4\. Then, there
    are three possible thresholds between them.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个具有四个唯一值的特征：1、2、3和4。那么，它们之间有三个可能的阈值。
- en: '![](../Images/ab26cac2feda483eeef4fd4f32694100.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ab26cac2feda483eeef4fd4f32694100.png)'
- en: Graph by author
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图表由作者提供
- en: We can consequently take each threshold and calculate predicted values for our
    data as an average value for leaf nodes. Then, we can use these predicted values
    to get MSE (Mean Square Error) for each threshold. The best split will be the
    one with the lowest MSE. By default, [DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor)
    from scikit-learn works similarly and uses MSE as a criterion.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以逐一取每个阈值，并计算数据的预测值作为叶节点的平均值。然后，我们可以使用这些预测值来计算每个阈值的MSE（均方误差）。最佳分裂将是具有最低MSE的那个。默认情况下，[DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor)在scikit-learn中也以类似方式工作，并使用MSE作为标准。
- en: Let’s calculate the best split for `sulphates` feature manually to understand
    better how it works.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解其工作原理，我们可以手动计算`sulphates`特征的最佳分裂。
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can see that for `sulphates`, the best threshold is 0.685 since it gives
    the lowest MSE.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，对于`sulphates`，最佳阈值是0.685，因为它提供了最低的MSE。
- en: Now, we can use this function for all features we have to define the best split
    overall.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将此函数应用于我们所有的特征，以定义整体最佳分裂。
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We got absolutely the same result as our initial decision tree with the first
    split on `alcohol <= 10.625` .
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了与初始决策树完全相同的结果，第一次分裂是在`alcohol <= 10.625`上。
- en: To build the whole Decision Tree, we could recursively calculate the best splits
    for each of the datasets `alcohol <= 10.625` and `alcohol > 10.625` and get the
    next level of Decision Tree. Then, repeat.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建整个决策树，我们可以递归地计算每个数据集`alcohol <= 10.625`和`alcohol > 10.625`的最佳分裂，并获得决策树的下一层。然后，重复这一过程。
- en: The stopping criteria for recursion could be either the depth or the minimal
    size of the leaf node. Here’s an example of a Decision Tree with at least 420
    items in the leaf nodes.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 递归的停止标准可以是深度或叶节点的最小大小。这里是一个决策树的示例，叶节点中至少有420个项目。
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/d31101f15f8e1b0322f86dc3ffe8f735.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d31101f15f8e1b0322f86dc3ffe8f735.png)'
- en: Graph by author
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图表由作者提供
- en: Let’s calculate the mean absolute error on the validation set to understand
    how good our model is. I prefer MAE over MSE (Mean Squared Error) because it’s
    less affected by outliers.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算验证集上的平均绝对误差，以了解我们的模型表现如何。我更喜欢MAE而非MSE（均方误差），因为MAE对离群值的影响较小。
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Decision Tree: Classification'
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树：分类
- en: We’ve looked at the regression example. In the case of classification, it’s
    a bit different. Even though we won’t go deep into classification examples in
    this article, it’s still worth discussing its basics.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经查看了回归示例。在分类的情况下，情况有所不同。尽管在本文中我们不会深入探讨分类示例，但仍值得讨论其基本概念。
- en: For classification, instead of the average value, we use the most common class
    as a prediction for each leaf node.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类，我们使用最常见的类别作为每个叶节点的预测，而不是平均值。
- en: We usually use [the Gini coefficient](https://en.wikipedia.org/wiki/Gini_coefficient)
    to estimate the binary split’s quality for classification. Imagine getting one
    random item from the sample and then the other. The Gini coefficient would be
    equal to the probability of the situation when items are from different classes.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常使用[基尼系数](https://en.wikipedia.org/wiki/Gini_coefficient)来估计分类中二元分裂的质量。想象从样本中随机获取一个项目，然后获取另一个。基尼系数将等于项目来自不同类别的情况的概率。
- en: 'Let’s say we have only two classes, and the share of items from the first class
    is equal to `p` . Then we can calculate the Gini coefficient using the following
    formula:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们只有两个类别，且第一类的项目占比为`p`。然后，我们可以使用以下公式计算基尼系数：
- en: '![](../Images/bbad8f997ff9894f263bdab832ae5b93.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bbad8f997ff9894f263bdab832ae5b93.png)'
- en: If our classification model is perfect, the Gini coefficient equals 0\. In the
    worst case (`p = 0.5`), the Gini coefficient equals 0.5.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的分类模型是完美的，基尼系数等于0。最坏情况下（`p = 0.5`），基尼系数等于0.5。
- en: To calculate the metric for binary split, we calculate Gini coefficients for
    both parts (left and right ones) and norm them on the number of samples in each
    partition.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算二元分裂的度量，我们计算两个部分（左侧和右侧）的基尼系数，并对每个分区中的样本数进行归一化。
- en: '![](../Images/708b9e202a2290157cd93405ed5cdeec.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/708b9e202a2290157cd93405ed5cdeec.png)'
- en: Then, we can similarly calculate our optimisation metric for different thresholds
    and use the best option.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以类似地计算不同阈值下的优化度量，并使用最佳选项。
- en: We’ve trained a simple Decision Tree model and discussed how it works. Now,
    we are ready to move on to the Random Forests.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经训练了一个简单的决策树模型并讨论了它的工作原理。现在，我们准备继续讨论随机森林。
- en: Random Forests
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林
- en: Random Forests are based on the concept of Bagging. The idea is to fit a bunch
    of independent models and use an average prediction from them. Since models are
    independent, errors are not correlated. We assume that our models have no systematic
    errors, and the average of many errors should be close to zero.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林基于自助法（Bagging）的概念。这个想法是拟合一堆独立的模型，并使用它们的平均预测。由于模型是独立的，误差不会相关。我们假设我们的模型没有系统性误差，许多误差的平均值应该接近于零。
- en: 'How could we get lots of independent models? It’s pretty straightforward: we
    can train Decision Trees on random subsets of rows and features. It will be a
    Random Forest.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何获得许多独立的模型呢？这很简单：我们可以在随机的行和特征子集上训练决策树。这样就会形成一个随机森林。
- en: Let’s train a basic Random Forest with 100 trees and the minimal size of leaf
    nodes equal to 100.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们训练一个包含100棵树且叶节点最小大小为100的基本随机森林。
- en: '[PRE9]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'With random forest, we’ve achieved a much better quality than with one Decision
    Tree: 0.5592 vs. 0.5891.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机森林，我们实现了比单棵决策树更好的质量：0.5592对比0.5891。
- en: Overfitting
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过拟合
- en: The meaningful question is whether Random Forrest could overfit.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有意义的问题是随机森林是否会过拟合。
- en: Actually, no. Since we are averaging not correlated errors, we cannot overfit
    the model by adding more trees. Quality will improve asymptotically with the increase
    in the number of trees.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上不会。由于我们在平均不相关的误差，添加更多的树不会使模型过拟合。随着树木数量的增加，质量将渐近提高。
- en: '![](../Images/49be396d4feb06c09eeba98f918ff332.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/49be396d4feb06c09eeba98f918ff332.png)'
- en: Graph by author
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 作者绘图
- en: However, you might face overfitting if you have deep trees and not enough of
    them. It’s easy to overfit one Decision Tree.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你有深度较大的树且数量不够，你可能会遇到过拟合的问题。一个决策树很容易发生过拟合。
- en: Out-of-bag error
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 袋外误差
- en: Since only part of the rows is used for each tree in Random Forest, we can use
    them to estimate the error. For each row, we can select only trees where this
    row wasn’t used and use them to make predictions. Then, we can calculate errors
    based on these predictions. Such an approach is called “out-of-bag error”.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在随机森林中每棵树只使用部分行，我们可以利用它们来估计误差。对于每一行，我们可以选择没有使用该行的树，并用它们来进行预测。然后，我们可以根据这些预测计算误差。这种方法称为“袋外误差”。
- en: We can see that the OOB error is much closer to the error on the validation
    set than the one for training, which means it’s a good approximation.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到OOB误差比训练集上的误差更接近验证集上的误差，这意味着它是一个好的近似值。
- en: '[PRE10]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Interpreting the model
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释模型
- en: As I mentioned in the beginning, the big advantage of Decision Trees is that
    it’s easy to interpret them. Let’s try to understand our model better.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我一开始提到的，决策树的一个大优点是它们易于解释。让我们尝试更好地理解我们的模型。
- en: Feature importances
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征重要性
- en: The calculation of the feature importance is pretty straightforward. We look
    at each decision tree in the ensemble and each binary split and calculate its
    impact on our metric (`squared_error` in our case).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 特征重要性的计算相当简单。我们查看集成中的每棵决策树和每个二元分裂，计算其对我们度量标准的影响（在我们这里是`squared_error`）。
- en: Let’s look at the first split by `alcohol` for one of our initial decision trees.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们初始决策树的第一个`alcohol`分裂。
- en: '![](../Images/3c28c776fe6bc0e1408a5e9ff3609c50.png)![](../Images/51b302bc71cd34b8b710b6e21c9ef5d0.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c28c776fe6bc0e1408a5e9ff3609c50.png)![](../Images/51b302bc71cd34b8b710b6e21c9ef5d0.png)'
- en: Then, we can do the same calculations for all binary splits in all decision
    trees, add everything up, normalize and get the relative importance for each feature.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以对所有决策树中的所有二元分裂进行相同的计算，汇总所有结果，进行归一化，从而得到每个特征的相对重要性。
- en: If you use scikit-learn, you don’t need to calculate feature importance manually.
    You can just take `model.feature_importances_`.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用scikit-learn，你不需要手动计算特征重要性。你只需获取`model.feature_importances_`即可。
- en: '[PRE11]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We can see that the most important features overall are `alcohol` and `volatile
    acidity` .
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，总体上最重要的特征是`alcohol`和`volatile acidity`。
- en: '![](../Images/063ae65bbcf00de796cfe4571a702c15.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/063ae65bbcf00de796cfe4571a702c15.png)'
- en: Graph by author
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 作者绘制的图表
- en: Partial Dependence
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部分依赖
- en: Understanding how each feature affects our target metric is exciting and often
    useful. For example, whether quality increases/decreases with higher alcohol or
    there’s a more complex relation.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 了解每个特征如何影响我们的目标指标是令人兴奋的，且通常是有用的。例如，质量是否随着酒精含量的增加而提高或降低，或者是否存在更复杂的关系。
- en: We could just get data from our dataset and plot averages by alcohol, but it
    won’t be correct since there might be some correlations. For example, higher alcohol
    in our dataset could also correspond to more elevated sugar and better quality.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接从数据集中获取数据并按酒精含量绘制平均值，但这不准确，因为可能存在某些相关性。例如，我们的数据集中较高的酒精含量可能还对应更高的糖分和更好的质量。
- en: 'To estimate the impact only from alcohol, we can take all rows in our dataset
    and, using the ML model, predict the quality for each row for different values
    of alcohol: 9, 9.1, 9.2, etc. Then, we can average results and get the actual
    relation between alcohol level and wine quality. So, all the data is equal, and
    we are just varying alcohol levels.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了仅估计酒精的影响，我们可以取数据集中所有行，并使用机器学习模型预测每一行在不同酒精含量下的质量：9、9.1、9.2等。然后，我们可以平均结果，获得酒精水平与酒质之间的实际关系。所以，所有数据是相同的，我们只是改变酒精水平。
- en: This approach could be used with any ML model, not only Random Forest.
  id: totrans-111
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这种方法可以与任何机器学习模型一起使用，而不仅仅是随机森林。
- en: We can use `sklearn.inspection` module to easily plot this relations.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`sklearn.inspection`模块来轻松绘制这些关系。
- en: '[PRE12]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can gain quite a lot of insights from these graphs, for example:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从这些图表中获得很多洞察，例如：
- en: wine quality increases with the growth of free sulfur dioxide up to 30, but
    it’s stable after this threshold;
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 酒质随着游离二氧化硫的增加而提高，直到30，但在这一阈值后保持稳定；
- en: with alcohol, the higher the level — the better the quality.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 酒精含量越高，质量越好。
- en: '![](../Images/92b71652cf4a54d3b503ac3db8aa296e.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92b71652cf4a54d3b503ac3db8aa296e.png)'
- en: We can even look at relations between two variables. It can be pretty complex.
    For example, if the alcohol level is above 11.5, volatile acidity has no effect.
    But, for lower alcohol levels, volatile acidity significantly impacts quality.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至可以查看两个变量之间的关系。这可能相当复杂。例如，如果酒精含量超过11.5，挥发性酸度没有影响。但对于较低的酒精含量，挥发性酸度对质量有显著影响。
- en: '[PRE13]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/6261b9202a6fb1282941e3c4a6ec3f03.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6261b9202a6fb1282941e3c4a6ec3f03.png)'
- en: '**Confidence of predictions**'
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**预测的置信度**'
- en: Using Random Forests, we can also assess how confident each prediction is. For
    that, we could calculate predictions from each tree in the ensemble and look at
    variance or standard deviation.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机森林，我们还可以评估每个预测的置信度。为此，我们可以计算集成中每棵树的预测值，并查看方差或标准差。
- en: '[PRE14]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We can see that there are predictions with low standard deviation (i.e. below
    0.15) and the ones with `std` above 0.3.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，有些预测的标准差很低（即低于0.15），而有些预测的`std`则高于0.3。
- en: '![](../Images/4631de9eb676c2bf7915fcddba6c734f.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4631de9eb676c2bf7915fcddba6c734f.png)'
- en: If we use the model for business purposes, we can treat such cases differently.
    For example, do not take into account prediction if `std` above `X` or show to
    the customer intervals (i.e. percentile 25% and percentile 75%).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将模型用于业务目的，我们可以以不同的方式处理这种情况。例如，如果`std`超过`X`，则不考虑预测，或向客户展示区间（即25百分位数和75百分位数）。
- en: How prediction was made?
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测是如何产生的？
- en: We can also use packages `treeinterpreter` and `waterfallcharts` to understand
    how each prediction was made. It could be handy in some business cases, for example,
    when you need to tell customers why credit for them was rejected.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用`treeinterpreter`和`waterfallcharts`包来了解每个预测是如何产生的。这在某些业务场景中可能会很有用，例如，当你需要告诉客户为什么他们的信用申请被拒绝时。
- en: We will look at one of the wines as an example. It has relatively low alcohol
    and high volatile acidity.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以其中一种酒为例。它具有相对较低的酒精含量和较高的挥发性酸度。
- en: '![](../Images/d48d317caa71734745606b63cf9137fb.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d48d317caa71734745606b63cf9137fb.png)'
- en: '[PRE15]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The graph shows that this wine is better than average. The main factor that
    increases quality is a low level of volatile acidity, while the main disadvantage
    is a low level of alcohol.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图表显示这款酒的质量优于平均水平。提高质量的主要因素是低挥发性酸度，而主要缺点是低酒精度。
- en: '![](../Images/a5ec8eadcafc2a31cf285a9ce472698d.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a5ec8eadcafc2a31cf285a9ce472698d.png)'
- en: Graph by author
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 作者绘图
- en: So, there are a lot of handy tools that could help you to understand your data
    and model much better.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，有很多实用工具可以帮助你更好地理解数据和模型。
- en: Reducing number of trees
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 减少树的数量
- en: The other cool feature of Random Forest is that we could use it to reduce the
    number of features for any tabular data. You can quickly fit a Random Forest and
    define a list of meaningful columns in your data.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林的另一个酷炫特性是，我们可以使用它来减少任何表格数据的特征数量。你可以快速拟合一个随机森林并定义数据中有意义的列。
- en: More data doesn’t always mean better quality. Also, it can affect your model
    performance during training and inference.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 更多的数据并不总是意味着更好的质量。此外，它还可能影响你的模型在训练和推断阶段的表现。
- en: Since in our initial wine dataset, there were only 12 features, for this case,
    we will use a slightly bigger dataset — [Online News Popularity](https://archive.ics.uci.edu/dataset/332/online+news+popularity).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们初始的葡萄酒数据集中只有12个特征，所以在这种情况下，我们将使用一个稍大的数据集——[在线新闻受欢迎程度](https://archive.ics.uci.edu/dataset/332/online+news+popularity)。
- en: Looking at feature importance
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查看特征重要性
- en: First, let’s build a Random Forest and look at feature importances. 34 out of
    59 features have an importance lower than 0.01.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们构建一个随机森林并查看特征重要性。在59个特征中，有34个特征的重要性低于0.01。
- en: Let’s try to remove them and look at accuracy.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试移除这些特征并查看准确性。
- en: '[PRE16]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '***MAE on validation set for all features***: 2969.73'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***所有特征在验证集上的MAE***：2969.73'
- en: '***MAE on validation set for 25 important features***: 2975.61'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***25个重要特征在验证集上的MAE***：2975.61'
- en: The difference in quality is not so big, but we could make our model faster
    in the training and inference stages. We’ve already removed almost 60% of the
    initial features — good job.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 质量的差异并不大，但我们可以使模型在训练和推断阶段更快。我们已经去除了初始特征的近60%——做得不错。
- en: Looking at redundant features
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查看冗余特征
- en: 'For the remaining features, let’s see whether there are redundant (highly correlated)
    ones. For that, we will use a Fast.AI tool:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 对于剩余的特征，让我们看看是否有冗余（高度相关）的特征。为此，我们将使用一个 Fast.AI 工具：
- en: '[PRE17]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](../Images/605fb7a9d7b4700df6e962317668ae53.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/605fb7a9d7b4700df6e962317668ae53.png)'
- en: 'We could see that the following features are close to each other:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到以下特征彼此接近：
- en: '`self_reference_avg_sharess` and `self_reference_max_shares`'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`self_reference_avg_sharess` 和 `self_reference_max_shares`'
- en: '`kw_min_avg` and `kw_min_max`'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kw_min_avg` 和 `kw_min_max`'
- en: '`n_non_stop_unique_tokens` and `n_unique_tokens` .'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_non_stop_unique_tokens` 和 `n_unique_tokens`。'
- en: Let’s remove them as well.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也将它们移除。
- en: '[PRE18]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Quality even a little bit improved. So, we’ve reduced the number of features
    from 59 to 22 and increased the error only by 0.17%. It proves that such an approach
    works.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 质量即使稍微有所改善。因此，我们将特征数量从59个减少到22个，并且仅将错误增加了0.17%。这证明了这种方法的有效性。
- en: You can find the full code on [GitHub](https://github.com/miptgirl/miptgirl_medium/tree/main/random_forests_101).
  id: totrans-158
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可以在 [GitHub](https://github.com/miptgirl/miptgirl_medium/tree/main/random_forests_101)
    上找到完整的代码。
- en: Summary
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this article, we’ve discussed how Decision Tree and Random Forest algorithms
    work. Also, we’ve learned how to interpret Random Forests:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们讨论了决策树和随机森林算法的工作原理。此外，我们还学会了如何解释随机森林：
- en: How to use feature importance to get the list of the most significant features
    and reduce the number of parameters in your model.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用特征重要性获取最重要特征的列表并减少模型中的参数数量。
- en: How to define the effect of each feature value on the target metric using partial
    dependence.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用部分依赖定义每个特征值对目标指标的影响。
- en: How to estimate the impact of different features on each prediction using `treeinterpreter`
    library.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用 `treeinterpreter` 库估计不同特征对每次预测的影响。
- en: Thank you a lot for reading this article. I hope it was insightful to you. If
    you have any follow-up questions or comments, please leave them in the comments
    section.
  id: totrans-164
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 非常感谢你阅读这篇文章。希望它对你有所启发。如果你有任何后续问题或评论，请在评论区留言。
- en: References
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Datasets
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: '*Cortez,Paulo, Cerdeira,A., Almeida,F., Matos,T., and Reis,J.. (2009). Wine
    Quality. UCI Machine Learning Repository.* [*https://doi.org/10.24432/C56S3T*](https://doi.org/10.24432/C56S3T)'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Cortez, Paulo, Cerdeira, A., Almeida, F., Matos, T., 和 Reis, J.. (2009). 葡萄酒质量.
    UCI 机器学习库。* [*https://doi.org/10.24432/C56S3T*](https://doi.org/10.24432/C56S3T)'
- en: '*Fernandes,Kelwin, Vinagre,Pedro, Cortez,Paulo, and Sernadela,Pedro. (2015).
    Online News Popularity. UCI Machine Learning Repository.* [*https://doi.org/10.24432/C5NS3V*](https://doi.org/10.24432/C5NS3V)'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Fernandes, Kelwin, Vinagre, Pedro, Cortez, Paulo, 和 Sernadela, Pedro. (2015).
    在线新闻流行度. UCI机器学习库。* [*https://doi.org/10.24432/C5NS3V*](https://doi.org/10.24432/C5NS3V)'
- en: Sources
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 来源
- en: This article was inspired by ***Fast.AI Deep Learning Course***
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 本文灵感来自***Fast.AI 深度学习课程***
- en: '[Lesson 6: Random Forests](https://course.fast.ai/Lessons/lesson6.html)'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第6课：随机森林](https://course.fast.ai/Lessons/lesson6.html)'
- en: '[Chapter 9](https://github.com/fastai/fastbook/blob/master/09_tabular.ipynb)
    of “Deep Learning for Coders with Fast.AI and Pytorch: AI Applications Without
    a PhD” by J. Howard and S. Gugger'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “[《深度学习实战：Fast.AI与Pytorch实现AI应用，无需博士学位》](https://github.com/fastai/fastbook/blob/master/09_tabular.ipynb)”第9章由J.
    Howard和S. Gugger编写。
- en: Documentation for [Decision Trees](https://scikit-learn.org/stable/modules/tree.html)
    and [Random Forests](https://scikit-learn.org/stable/modules/ensemble.html#random-forests-and-other-randomized-tree-ensembles)
    from `scikit-learn`
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自`scikit-learn`的[决策树](https://scikit-learn.org/stable/modules/tree.html)和[随机森林](https://scikit-learn.org/stable/modules/ensemble.html#random-forests-and-other-randomized-tree-ensembles)文档
