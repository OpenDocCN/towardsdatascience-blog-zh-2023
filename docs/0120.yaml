- en: '2023 Predictions: What’s Next for AI Research?'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2023年的预测：AI研究的下一步是什么？
- en: 原文：[https://towardsdatascience.com/2023-predictions-whats-next-for-ai-research-de5b035bc448?source=collection_archive---------2-----------------------#2023-01-08](https://towardsdatascience.com/2023-predictions-whats-next-for-ai-research-de5b035bc448?source=collection_archive---------2-----------------------#2023-01-08)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/2023-predictions-whats-next-for-ai-research-de5b035bc448?source=collection_archive---------2-----------------------#2023-01-08](https://towardsdatascience.com/2023-predictions-whats-next-for-ai-research-de5b035bc448?source=collection_archive---------2-----------------------#2023-01-08)
- en: Excited by the past year, we looked forward to 2023, and wondered what it would
    look like
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对于过去的一年感到兴奋，我们展望2023年，想知道它会是什么样子。
- en: '[](https://medium.com/@talrosenwein?source=post_page-----de5b035bc448--------------------------------)[![Tal
    Rosenwein](../Images/c5839727d9f63df6b5f26c7aa781679b.png)](https://medium.com/@talrosenwein?source=post_page-----de5b035bc448--------------------------------)[](https://towardsdatascience.com/?source=post_page-----de5b035bc448--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----de5b035bc448--------------------------------)
    [Tal Rosenwein](https://medium.com/@talrosenwein?source=post_page-----de5b035bc448--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@talrosenwein?source=post_page-----de5b035bc448--------------------------------)[![Tal
    Rosenwein](../Images/c5839727d9f63df6b5f26c7aa781679b.png)](https://medium.com/@talrosenwein?source=post_page-----de5b035bc448--------------------------------)[](https://towardsdatascience.com/?source=post_page-----de5b035bc448--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----de5b035bc448--------------------------------)
    [Tal Rosenwein](https://medium.com/@talrosenwein?source=post_page-----de5b035bc448--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc25fa765131b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F2023-predictions-whats-next-for-ai-research-de5b035bc448&user=Tal+Rosenwein&userId=c25fa765131b&source=post_page-c25fa765131b----de5b035bc448---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----de5b035bc448--------------------------------)
    ·12 min read·Jan 8, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fde5b035bc448&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F2023-predictions-whats-next-for-ai-research-de5b035bc448&user=Tal+Rosenwein&userId=c25fa765131b&source=-----de5b035bc448---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[跟随](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc25fa765131b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F2023-predictions-whats-next-for-ai-research-de5b035bc448&user=Tal+Rosenwein&userId=c25fa765131b&source=post_page-c25fa765131b----de5b035bc448---------------------post_header-----------)
    发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----de5b035bc448--------------------------------)
    ·12分钟阅读·2023年1月8日'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fde5b035bc448&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F2023-predictions-whats-next-for-ai-research-de5b035bc448&source=-----de5b035bc448---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fde5b035bc448&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F2023-predictions-whats-next-for-ai-research-de5b035bc448&source=-----de5b035bc448---------------------bookmark_footer-----------)'
- en: This blog post was co-authored with [**Guy Eyal**](https://www.linkedin.com/in/guy-netser-eyal-84368445/),
    an NLP team leader at Gong
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇博客文章由[**Guy Eyal**](https://www.linkedin.com/in/guy-netser-eyal-84368445/)，Gong的NLP团队负责人，共同撰写。
- en: '**TL;DR:**'
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**简而言之：**'
- en: '*In 2022, large models achieved state-of-the-art results in various tasks and
    domains. A significant breakthrough in natural language processing (NLP) was achieved
    when models were trained to align with user intent and human preferences, leading
    to improved generation quality. Looking ahead to 2023, we can expect to see new
    methods to improve the alignment process (such as reinforcement learning with
    AI feedback), the development of automatic metrics for understanding alignment
    effectiveness, and the emergence of personalized aligned models, even in an online
    manner. There may also be a focus on addressing factuality issues as well as developing
    open-source tools and specialized compute resources to allow the industrial scale
    of aligned models. In addition to NLP, there will likely be progress in other
    modalities such as audio processing, computer vision, and robotics, and the development
    of multimodal models.*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*2022年，大型模型在各类任务和领域中取得了最先进的成果。在自然语言处理（NLP）方面，当模型被训练以对齐用户意图和人类偏好时，实现了显著突破，从而提高了生成质量。展望2023年，我们可以期待看到改进对齐过程的新方法（例如带有AI反馈的强化学习）、理解对齐效果的自动化指标的开发，以及个性化对齐模型的出现，即使是在线方式。可能还会关注解决事实性问题以及开发开源工具和专门的计算资源，以支持对齐模型的工业规模发展。除了NLP，还可能在音频处理、计算机视觉和机器人等其他模态中取得进展，并发展多模态模型。*'
- en: '2022 AI Research Progress: A Year in Review'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2022年AI研究进展：年度回顾
- en: 2022 was an excellent year for artificial intelligence/machine learning, with
    numerous large language models (LLMs) published and achieving state-of-the-art
    results across various benchmarks. These LLMs demonstrated their superior performance
    through few-shot learning, surpassing smaller models that had been fine-tuned
    on the same tasks [1–3]. This has the potential to reduce the need for specialized,
    in-domain datasets. Techniques like Chain of Thoughts [4] and Self Consistency
    [5] also helped to improve the reasoning capabilities of LLMs, leading to significant
    gains on reasoning benchmarks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 2022年对人工智能/机器学习来说是优秀的一年，发布了大量语言模型（LLMs）并在各种基准测试中取得了最先进的结果。这些LLMs通过少量学习展示了其优越的表现，超越了在相同任务上进行了微调的小型模型[1–3]。这有可能减少对专业领域数据集的需求。诸如思维链[4]和自洽性[5]等技术也帮助提升了LLMs的推理能力，在推理基准测试中取得了显著提升。
- en: There were also notable advancements in dialogue systems resulting in more helpful,
    safe, and faithful models that could stay up-to-date through fine-tuning on annotated
    data and the use of retrieval from external knowledge sources [6–7].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对话系统也取得了显著进展， resulting in more helpful, safe, and faithful models that could
    stay up-to-date through fine-tuning on annotated data and the use of retrieval
    from external knowledge sources [6–7]。
- en: In Automatic Speech Recognition (ASR), the use of an encoder-decoder transformer
    architecture allowed for more efficient scaling of model size, leading to a 50%
    reduction in word error rate on multiple ASR benchmarks without any domain adaptation
    [8].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动语音识别（ASR）中，使用编码器-解码器变换器架构实现了模型规模的更高效扩展，在多个ASR基准测试中减少了50%的词错误率而无需领域适配[8]。
- en: Diffusion models [9–10] trained on large image datasets, made impressive strides
    in computer vision and sparked a new trend in AI art. Additionally, we saw the
    beginnings of multimodal models that use pre-trained LLMs to improve performance
    in tasks ranging from vision to robotics [9–12].
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型[9–10]在大型图像数据集上进行训练，在计算机视觉领域取得了显著进展，并引发了AI艺术的新趋势。此外，我们还看到了利用预训练大语言模型（LLMs）来提升从视觉到机器人等任务表现的多模态模型的初步出现[9–12]。
- en: Finally, the release of ChatGPT [13] gave users a glimpse into the future of
    working with AI assistants in various fields and domains.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，ChatGPT [13] 的发布让用户瞥见了在各个领域和领域中与AI助手合作的未来。
- en: '![](../Images/564eb7d0f038ace83a24b12ab118876e.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/564eb7d0f038ace83a24b12ab118876e.png)'
- en: Photo by [Moritz Knöringer](https://unsplash.com/@mokngr?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Moritz Knöringer](https://unsplash.com/@mokngr?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '2023 Predictions: The Year of The Alignment'
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2023年预测：对齐年的到来
- en: 'Excited by the past year, we looked forward to 2023, and wondered what it would
    look like. Here are our thoughts:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对过去一年充满期待，我们展望2023年，想知道它会是什么样子。以下是我们的想法：
- en: Reinforcement Learning with Human Feedback (RLHF), a supervised approach that
    aligns models with user intent and human preferences, has become increasingly
    popular in recent months [15]. This supervised approach shows promising results
    in the generation quality as can be seen by comparing the outputs of vanilla GPT3
    [16] and ChatGPT. RLHF is so effective that a model trained with instruction tuning
    outperforms a model that is more than 100 times larger in size.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几个月，人工反馈的强化学习（RLHF），一种使模型与用户意图和人类偏好对齐的监督方法，变得越来越受欢迎 [15]。这种监督方法在生成质量方面表现出良好的结果，比较vanilla
    GPT3 [16] 和ChatGPT的输出可以看出。RLHF的效果如此显著，以至于经过指令调优的模型超越了一个大于100倍的模型。
- en: '![](../Images/bb40f68f7f8a25709fa01b4f07eab9e9.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bb40f68f7f8a25709fa01b4f07eab9e9.png)'
- en: Image source — InstructGPT [15]
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源 — InstructGPT [15]
- en: Going further with this year’s trend, we, like many others, expect that alignment
    will continue to be a significant factor. However, we predict active research
    work in additional core functionalities that are currently lacking in most models
    and therefore limit their applicability in many fields.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 随着今年趋势的进一步发展，我们和许多人一样，预计对齐将继续是一个重要因素。然而，我们预测将有更多的核心功能进行积极研究，这些功能目前在大多数模型中缺失，因此限制了它们在许多领域的适用性。
- en: '**Reinforcement Learning with AI Feedback (RLAIF)**'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**人工智能反馈的强化学习（RLAIF）**'
- en: Currently, RLHF requires human-curated data. Although small compared to the
    pre-training data size, it requires extensive and expensive human labor. For example,
    OpenAI used 40 annotators to write and label 64K samples for instruction tuning
    [15]. An interesting and exciting alternative that we think will be utilized this
    year is to use different LLMs as the instructors and labelers— Reinforcement Learning
    with AI Feedback (RLAIF). RLAIF will enable cost reduction and fast scaling of
    the alignment process as machines will do everything end-2-end. An interesting
    recent work by Anthropic [17] showed that with good prompting, one could guide
    an LM to classify harming outputs. These in turn, are used for the training of
    the reward model necessary for RLHF.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，RLHF需要人工策划的数据。虽然与预训练数据规模相比较小，但仍需要大量且昂贵的人力。例如，OpenAI使用了40名注释员编写和标注了64K个样本用于指令调优
    [15]。我们认为，今年将被利用的一个有趣且令人兴奋的替代方案是使用不同的LLM作为指导者和标注者——人工智能反馈的强化学习（RLAIF）。RLAIF将能够降低成本并快速扩展对齐过程，因为机器将完成所有的端到端工作。Anthropic
    [17] 最近的一项有趣工作表明，通过良好的提示，可以引导语言模型对有害输出进行分类。这些分类结果反过来用于训练RLHF所需的奖励模型。
- en: '**Metrics For Alignment**'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**对齐的度量标准**'
- en: We assume that many methods will be developed to achieve better alignment between
    the model’s outputs and user intent. This, in return, will improve even further
    generation quality.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设将会开发许多方法来实现模型输出与用户意图之间更好的对齐。这反过来将进一步提高生成质量。
- en: In order to understand which method is superior, automatic metrics should be
    developed alongside current human evaluation methods. This is a long-standing
    issue in NLP, as previous metrics fail to align with human annotations.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解哪种方法更优，自动度量标准应与当前的人类评估方法一起开发。这是NLP中的一个长期问题，因为以前的度量标准未能与人类注释对齐。
- en: 'Recently, two promising approaches have been introduced: MAUVE [18], which
    compares the distributions of human-generated and model-generated outputs using
    divergence frontiers, and model-written evaluations [19], which utilizes other
    language models to assess the quality of the generated output. Further research
    in these areas may be a valuable direction during 2023.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，引入了两种有前景的方法：MAUVE [18]，它通过使用发散前沿比较人类生成和模型生成输出的分布，以及模型编写的评估 [19]，它利用其他语言模型来评估生成输出的质量。在这些领域的进一步研究可能是2023年的一个有价值的方向。
- en: '**Personalized Aligned Models**'
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**个性化对齐模型**'
- en: Expecting a model to be aligned with the entire society does not make sense,
    as we are not aligned with each other. Therefore we expect to see many different
    models aligned with different usages and users. We term this as Personalized Aligned
    Models.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 期望一个模型与整个社会对齐是没有意义的，因为我们彼此之间并不对齐。因此，我们预计会看到许多不同的模型与不同的用途和用户对齐。我们称之为个性化对齐模型。
- en: We’ll see various companies align models with their own needs, and big companies
    align many models with their different users. This will greatly improve the end
    user’s experience when using LLMs in personal assistants, internet searches, text
    editing, and more.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到各种公司根据自己的需求调整模型，而大型公司则将多个模型与不同的用户需求对齐。这将大大改善最终用户在使用LLMs时的体验，包括个人助理、互联网搜索、文本编辑等。
- en: '**Open Source And a Specialized Compute**'
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**开源和专业计算**'
- en: 'To achieve personalization of aligned models at the industry scale, two components
    that don’t/partially exist today will have to be available for public use: models
    that can be aligned and compute resources.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现行业规模的个性化对齐模型，需要公开可用的两个组件：可对齐的模型和计算资源，这两个组件今天尚未完全存在。
- en: '**Models to be Aligned and** **Open Source Models** that are candidates for
    alignment will have to be developed, as the current models, such as Meta’s OPT
    [20], are not sufficient as they are not on par with paid APIs. Alternatively,
    we’ll see a paid API for the model’s alignment: non-public models by Google /
    OpenAI / Cohere / AI21 with full-serving options for consumers will be available
    and will serve as a valid business model.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**待对齐的模型和** **开源模型**作为对齐的候选者必须被开发，因为当前的模型，如Meta的OPT [20]，不够充分，因为它们与付费API不匹配。或者，我们将看到模型对齐的付费API：由Google
    / OpenAI / Cohere / AI21提供的非公开模型将提供全面的消费者服务选项，并将作为一种有效的商业模式。'
- en: '**Computational resources:** although the alignment is much cheaper than pre-training,
    it still requires very specialized computational resources. Therefore we predict
    a race towards generating such an accessible infrastructure for the public, probably
    on-cloud.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算资源：** 尽管对齐比预训练便宜得多，但仍然需要非常专业的计算资源。因此，我们预测将出现争相生成这种公共可用基础设施的情况，可能是在云端。'
- en: '**Handling Factuality Issues**'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**处理事实问题**'
- en: 'The apparent fluency of the output produced by LLMs may lead individuals to
    perceive the model as factually accurate and confident. However, a known limitation
    of LLMs that still needs to be solved by alignment is their tendency to generate
    hallucinated content. Therefore, we see two important research directions that
    will flourish this year: Outputting sources for text (citations) and outputting
    the model’s confidence.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs生成的输出的明显流畅性可能会导致个人将模型视为事实准确且自信。然而，LLMs的一个已知限制是它们倾向于生成虚假的内容，这仍需要通过对齐来解决。因此，我们看到两个重要的研究方向将在今年蓬勃发展：文本输出的源（引用）和模型的置信度。
- en: '**Outputting sources for the current output** can be achieved in many ways.
    One interesting direction is to connect LLMs with text retrievalmechanisms that
    will help ground/relate the outputs to known sources [21]. This may also help
    models stay relevant,although their training process stopped at some point in
    the past. Another recently suggested idea is to do this in post-process by searching
    for documents that are most proximal to the output [22]. While the latter will
    not solve hallucinations, it will make it easier for the user to validate the
    results.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**当前输出的源**可以通过多种方式实现。一个有趣的方向是将LLMs与文本检索机制连接，这将有助于将输出与已知来源进行关联[21]。这也可能帮助模型保持相关性，即使它们的训练过程在过去某个时间点停止了。另一个最近提出的想法是在后处理过程中通过搜索最接近输出的文档[22]来实现。虽然后者不能解决虚假信息，但可以使用户更容易验证结果。'
- en: 'Recent works in different domains (ASR for example [23]) trained **models that
    have two** **outputs: token prediction and a per-token confidence score**. Using
    similar methods while extending the confidence score to relate to the entire output
    will help the user to take the results with a grain of salt.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同领域（例如ASR [23]）的近期研究中训练了**具有两个输出的模型：令牌预测和每个令牌的置信度评分**。使用类似的方法，同时将置信度评分扩展到整个输出，将帮助用户谨慎对待结果。
- en: '**Online Alignment**'
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**在线对齐**'
- en: As people change over time, with shifts in interests, beliefs, jobs, and family
    status, it makes sense that their personal assistants should adapt as well. One
    very promising research direction we’re predicting is online alignment. Users
    will be able to continue personalizing their models after deployment. The alignment
    process will be continuously updated using an online learning algorithm by giving
    feedback to the models [24].
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人们的兴趣、信仰、工作和家庭状况的变化，他们的个人助理也应该适应这些变化是有意义的。我们预测的一个非常有前景的研究方向是在线对齐。用户将能够在部署后继续个性化他们的模型。对齐过程将通过向模型提供反馈的在线学习算法不断更新[24]。
- en: '**What About Other Modalities?**'
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**其他模态的情况如何？**'
- en: We expect to see considerable improvements in audio and speech recognition domains.
    We assume that Whisper [8] will be able to utilize unlabelled data (such as Wav2Vec
    2.0 [25] / HuBERT [26]), which will significantly improve performances in challenging
    acoustic scenarios.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们期望在音频和语音识别领域看到显著的改进。我们假设 Whisper [8] 将能够利用未标注的数据（如 Wav2Vec 2.0 [25] / HuBERT
    [26]），这将显著提高在挑战性声学场景中的表现。
- en: SpeechT5 [27] was an early bird, so we assume that T0-like models [28] for audio
    will be trained on scale (both training data and model size), resulting in improved
    audio embeddings. This will enable a unified speech enhancement, diarization,
    and transcription system. In the longer term, we expect auditory models to answer
    questions similar to natural language processing (NLP) models. The grounding context
    of these auditory models will be an audio segment, which will serve as the context
    for the query without the need for implicit transcription.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: SpeechT5 [27] 是一个早期的探索者，因此我们假设类似 T0 的模型 [28] 将在规模上（包括训练数据和模型大小）进行训练，从而改进音频嵌入。这将实现一个统一的语音增强、分离和转录系统。在长期来看，我们期待听觉模型能够回答类似于自然语言处理（NLP）模型的问题。这些听觉模型的基础上下文将是一个音频片段，该片段将作为查询的上下文，而无需隐式转录。
- en: '**Multi-Modal Models**'
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**多模态模型**'
- en: An important paradigm for the next year would be large multimodal models. What
    will they look like? We suspect they may look very similar to language models.
    By that we mean that the user will prompt the model with a given modality, and
    the model will be able to generate its output in a different modality (as in Unified-IO
    [29]).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 明年的一个重要范式将是大型多模态模型。它们会是什么样子？我们猜测它们可能会非常类似于语言模型。我们所指的是，用户将以某种模态提示模型，模型将能够以不同的模态生成其输出（如
    Unified-IO [29]）。
- en: Although very exciting, diffusion models [9] currently cannot classify images.
    This can be solved easily by outputting text similar to how we use LLMs today
    in classification tasks. Similarly, these models will be able to transcribe, generate
    and enhance audio and videos by good prompting.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管非常令人兴奋，扩散模型 [9] 目前无法对图像进行分类。这可以通过输出类似于我们今天在分类任务中使用的 LLM 的文本来轻松解决。类似地，这些模型将能够通过良好的提示来转录、生成和增强音频和视频。
- en: What about aligning multimodal models? This is for the far future! Or as we
    call it in the current pace of our field — in a few months.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 那么多模态模型的对齐情况如何？这是远未来的事！或者用我们当前领域的节奏来说——几个月后。
- en: '**Closing Thoughts**'
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**结束语**'
- en: This post presents our predictions regarding the needed advances in AI research
    in 2023\. Large models can perform a wide range of academic tasks, as shown by
    their impressive performance on standard benchmarks. However, their applicability
    needs to be improved, as in real-world scenarios, these models still encounter
    embarrassing failures (untruthful, toxic, or simply not helpful to the user).
    We believe that aligning the models with user needs and keeping them up-to-date
    can address many of these issues. To that end, we have focused on the scalability
    and adaptability of the alignment process. If our hypothesis is correct, the field
    of generative language models will undergo significant changes soon. The potential
    uses of these models are vast, ranging from editing tools to domain-specific AI
    assistants that can automate manual labor in industries such as law, accounting,
    and engineering. Combining the above statement with predicted progress in computing
    (GPT- 4) and using the same methods applied to domains such as vision and audio
    processing promises another exciting year.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 本文展示了我们对2023年AI研究中所需进展的预测。大型模型可以执行广泛的学术任务，正如它们在标准基准测试中的出色表现所示。然而，这些模型在现实场景中仍会遇到令人尴尬的失败（不真实、有毒，或对用户没有帮助）。我们相信，通过将模型与用户需求对齐并保持其最新状态，可以解决许多这些问题。为此，我们关注了对齐过程的可扩展性和适应性。如果我们的假设正确，生成语言模型领域将很快发生重大变化。这些模型的潜在用途广泛，从编辑工具到可以在法律、会计和工程等行业中自动化手工劳动的领域特定
    AI 助手。将上述声明与计算（GPT-4）的预测进展结合起来，并采用同样的方法应用于视觉和音频处理领域，预示着另一个令人兴奋的年头。
- en: '*Thank you for reading!! If you have any thoughts regarding this 2023 projection,
    we warmly welcome them in the comments.*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*感谢阅读！！如果您对这一2023年的预测有任何想法，我们热烈欢迎在评论中分享。*'
- en: '**References:**'
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**参考文献：**'
- en: '[1] Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts,
    A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,
    S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., .
    . . Fiedel, N. (2022). PaLM: Scaling Language Modeling with Pathways. *arXiv*.
    [https://doi.org/10.48550/arXiv.2204.02311](https://doi.org/10.48550/arXiv.2204.02311)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts,
    A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,
    S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., .
    . . Fiedel, N. (2022). PaLM: 通过路径扩展语言建模。*arXiv*。[https://doi.org/10.48550/arXiv.2204.02311](https://doi.org/10.48550/arXiv.2204.02311)'
- en: '[2] Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilić, S., Hesslow, D., Castagné,
    R., Luccioni, A. S., Yvon, F., Gallé, M., Tow, J., Rush, A. M., Biderman, S.,
    Webson, A., Ammanamanchi, P. S., Wang, T., Sagot, B., Muennighoff, N., . . . Wolf,
    T. (2022). BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. *arXiv*.
    [https://doi.org/10.48550/arXiv.2211.05100](https://doi.org/10.48550/arXiv.2211.05100)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilić, S., Hesslow, D., Castagné,
    R., Luccioni, A. S., Yvon, F., Gallé, M., Tow, J., Rush, A. M., Biderman, S.,
    Webson, A., Ammanamanchi, P. S., Wang, T., Sagot, B., Muennighoff, N., . . . Wolf,
    T. (2022). BLOOM: 一种176B参数的开放访问多语言模型。*arXiv*。[https://doi.org/10.48550/arXiv.2211.05100](https://doi.org/10.48550/arXiv.2211.05100)'
- en: '[3] Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan,
    C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster,
    K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., & Zettlemoyer, L. (2022).
    OPT: Open Pre-trained Transformer Language Models. *arXiv*. [https://doi.org/10.48550/arXiv.2205.01068](https://doi.org/10.48550/arXiv.2205.01068)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan,
    C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster,
    K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., & Zettlemoyer, L. (2022).
    OPT: 开放预训练变换器语言模型。*arXiv*。[https://doi.org/10.48550/arXiv.2205.01068](https://doi.org/10.48550/arXiv.2205.01068)'
- en: '[4] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi,
    E., Le, Q., & Zhou, D. (2022). Chain of Thought Prompting Elicits Reasoning in
    Large Language Models. *arXiv*. [https://doi.org/10.48550/arXiv.2201.11903](https://doi.org/10.48550/arXiv.2201.11903)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi,
    E., Le, Q., & Zhou, D. (2022). 思维链提示在大型语言模型中引发推理。*arXiv*。[https://doi.org/10.48550/arXiv.2201.11903](https://doi.org/10.48550/arXiv.2201.11903)'
- en: '[5] Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery,
    A., & Zhou, D. (2022). Self-Consistency Improves Chain of Thought Reasoning in
    Language Models. *arXiv*. [https://doi.org/10.48550/arXiv.2203.11171](https://doi.org/10.48550/arXiv.2203.11171)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery,
    A., & Zhou, D. (2022). 自我一致性改善语言模型中的思维链推理。*arXiv*。[https://doi.org/10.48550/arXiv.2203.11171](https://doi.org/10.48550/arXiv.2203.11171)'
- en: '[6] Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A.,
    Cheng, H., Jin, A., Bos, T., Baker, L., Du, Y., Li, Y., Lee, H., Zheng, H. S.,
    Ghafouri, A., Menegali, M., Huang, Y., Krikun, M., Lepikhin, D., Qin, J., . .
    . Le, Q. (2022). LaMDA: Language Models for Dialog Applications. *arXiv*. [https://doi.org/10.48550/arXiv.2201.08239](https://doi.org/10.48550/arXiv.2201.08239)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A.,
    Cheng, H., Jin, A., Bos, T., Baker, L., Du, Y., Li, Y., Lee, H., Zheng, H. S.,
    Ghafouri, A., Menegali, M., Huang, Y., Krikun, M., Lepikhin, D., Qin, J., . .
    . Le, Q. (2022). LaMDA: 对话应用的语言模型。*arXiv*。[https://doi.org/10.48550/arXiv.2201.08239](https://doi.org/10.48550/arXiv.2201.08239)'
- en: '[7] Shuster, K., Xu, J., Komeili, M., Ju, D., Smith, E. M., Roller, S., Ung,
    M., Chen, M., Arora, K., Lane, J., Behrooz, M., Ngan, W., Poff, S., Goyal, N.,
    Szlam, A., Boureau, Y., Kambadur, M., & Weston, J. (2022). BlenderBot 3: a deployed
    conversational agent that continually learns to responsibly engage. *arXiv*. [https://doi.org/10.48550/arXiv.2208.03188](https://doi.org/10.48550/arXiv.2208.03188)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Shuster, K., Xu, J., Komeili, M., Ju, D., Smith, E. M., Roller, S., Ung,
    M., Chen, M., Arora, K., Lane, J., Behrooz, M., Ngan, W., Poff, S., Goyal, N.,
    Szlam, A., Boureau, Y., Kambadur, M., & Weston, J. (2022). BlenderBot 3: 一个持续学习以负责任地互动的部署对话代理。*arXiv*。[https://doi.org/10.48550/arXiv.2208.03188](https://doi.org/10.48550/arXiv.2208.03188)'
- en: '[8] Radford, A., Kim, JW, Xu, T, Brockman, G., McLeavey, C., Sutskever, I.
    (2022). Robust Speech Recognition via Large-Scale Weak Supervision. OpenAI. [https://cdn.openai.com/papers/whisper.pdf](https://cdn.openai.com/papers/whisper.pdf)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Radford, A., Kim, JW, Xu, T, Brockman, G., McLeavey, C., Sutskever, I.
    (2022). 通过大规模弱监督实现稳健的语音识别。OpenAI。[https://cdn.openai.com/papers/whisper.pdf](https://cdn.openai.com/papers/whisper.pdf)'
- en: '[9] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2021).
    High-Resolution Image Synthesis with Latent Diffusion Models. *arXiv*. [https://doi.org/10.48550/arXiv.2112.10752](https://doi.org/10.48550/arXiv.2112.10752)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2021).
    《使用潜在扩散模型的高分辨率图像合成》。*arXiv*。 [https://doi.org/10.48550/arXiv.2112.10752](https://doi.org/10.48550/arXiv.2112.10752)'
- en: '[10] Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., & Chen, M. (2022). Hierarchical
    Text-Conditional Image Generation with CLIP Latents. *arXiv*. [https://doi.org/10.48550/arXiv.2204.06125](https://doi.org/10.48550/arXiv.2204.06125)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., & Chen, M. (2022). 《基于
    CLIP 潜变量的分层文本条件图像生成》。*arXiv*。 [https://doi.org/10.48550/arXiv.2204.06125](https://doi.org/10.48550/arXiv.2204.06125)'
- en: '[11] Tang, Z., Yang, Z., Wang, G., Fang, Y., Liu, Y., Zhu, C., Zeng, M., Zhang,
    C., & Bansal, M. (2022). Unifying Vision, Text, and Layout for Universal Document
    Processing. *arXiv*. [https://doi.org/10.48550/arXiv.2212.02623](https://doi.org/10.48550/arXiv.2212.02623)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Tang, Z., Yang, Z., Wang, G., Fang, Y., Liu, Y., Zhu, C., Zeng, M., Zhang,
    C., & Bansal, M. (2022). 《统一视觉、文本和布局以实现通用文档处理》。*arXiv*。 [https://doi.org/10.48550/arXiv.2212.02623](https://doi.org/10.48550/arXiv.2212.02623)'
- en: '[12] Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C.,
    Zhou, J., & Yang, H. (2022). OFA: Unifying Architectures, Tasks, and Modalities
    Through a Simple Sequence-to-Sequence Learning Framework. *arXiv*. [https://doi.org/10.48550/arXiv.2202.03052](https://doi.org/10.48550/arXiv.2202.03052)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C.,
    Zhou, J., & Yang, H. (2022). 《OFA：通过简单的序列到序列学习框架统一架构、任务和模态》。*arXiv*。 [https://doi.org/10.48550/arXiv.2202.03052](https://doi.org/10.48550/arXiv.2202.03052)'
- en: '[13] Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn,
    C., Fu, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Ho, D., Hsu, J., Ibarz,
    J., Ichter, B., Irpan, A., Jang, E., Ruano, R. J., Jeffrey, K., . . . Zeng, A.
    (2022). Do As I Can, Not As I Say: Grounding Language in Robotic Affordances.
    *arXiv*. [https://doi.org/10.48550/arXiv.2204.01691](https://doi.org/10.48550/arXiv.2204.01691)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn,
    C., Fu, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Ho, D., Hsu, J., Ibarz,
    J., Ichter, B., Irpan, A., Jang, E., Ruano, R. J., Jeffrey, K., . . . Zeng, A.
    (2022). 《做我能做的，不是我说的：将语言建立在机器人能力上》。*arXiv*。 [https://doi.org/10.48550/arXiv.2204.01691](https://doi.org/10.48550/arXiv.2204.01691)'
- en: '[14] [https://chat.openai.com/chat](https://chat.openai.com/chat)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] [https://chat.openai.com/chat](https://chat.openai.com/chat)'
- en: '[15] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin,
    P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton,
    F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J.,
    . . . Lowe, R. (2022). Training language models to follow instructions with human
    feedback. *arXiv*. [https://doi.org/10.48550/arXiv.2203.02155](https://doi.org/10.48550/arXiv.2203.02155)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin,
    P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton,
    F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J.,
    . . . Lowe, R. (2022). 《训练语言模型以遵循人类反馈的指令》。*arXiv*。 [https://doi.org/10.48550/arXiv.2203.02155](https://doi.org/10.48550/arXiv.2203.02155)'
- en: '[16] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal,
    P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Krueger,
    G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse,
    C., . . . Amodei, D. (2020). Language Models are Few-Shot Learners. *arXiv*. [https://doi.org/10.48550/arXiv.2005.14165](https://doi.org/10.48550/arXiv.2005.14165)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal,
    P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Krueger,
    G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse,
    C., . . . Amodei, D. (2020). 《语言模型是少样本学习者》。*arXiv*。 [https://doi.org/10.48550/arXiv.2005.14165](https://doi.org/10.48550/arXiv.2005.14165)'
- en: '[17] Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A.,
    Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., Chen, C., Olsson, C., Olah,
    C., Hernandez, D., Drain, D., Ganguli, D., Li, D., Perez, E., Kerr, J., . . .
    Kaplan, J. (2022). Constitutional AI: Harmlessness from AI Feedback. *arXiv*.
    [https://doi.org/10.48550/arXiv.2212.08073](https://doi.org/10.48550/arXiv.2212.08073)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A.,
    Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., Chen, C., Olsson, C., Olah,
    C., Hernandez, D., Drain, D., Ganguli, D., Li, D., Perez, E., Kerr, J., . . .
    Kaplan, J. (2022). 《宪法 AI：来自 AI 反馈的无害性》。*arXiv*。 [https://doi.org/10.48550/arXiv.2212.08073](https://doi.org/10.48550/arXiv.2212.08073)'
- en: '[18] Pillutla, K., Swayamdipta, S., Zellers, R., Thickstun, J., Welleck, S.,
    Choi, Y., & Harchaoui, Z. (2021). MAUVE: Measuring the Gap Between Neural Text
    and Human Text using Divergence Frontiers. *arXiv*. [https://doi.org/10.48550/arXiv.2102.01454](https://doi.org/10.48550/arXiv.2102.01454)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Pillutla, K., Swayamdipta, S., Zellers, R., Thickstun, J., Welleck, S.,
    Choi, Y., & Harchaoui, Z. (2021). MAUVE: 使用发散边界测量神经文本与人类文本之间的差距。*arXiv*。 [https://doi.org/10.48550/arXiv.2102.01454](https://doi.org/10.48550/arXiv.2102.01454)'
- en: '[19] Perez, E., Ringer, S., Lukošiūtė, K., Nguyen, K., Chen, E., Heiner, S.,
    Pettit, C., Olsson, C., Kundu, S., Kadavath, S., Jones, A., Chen, A., Mann, B.,
    Israel, B., Seethor, B., McKinnon, C., Olah, C., Yan, D., Amodei, D., . . . Kaplan,
    J. (2022). Discovering Language Model Behaviors with Model-Written Evaluations.
    *arXiv*. [https://doi.org/10.48550/arXiv.2212.09251](https://doi.org/10.48550/arXiv.2212.09251)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Perez, E., Ringer, S., Lukošiūtė, K., Nguyen, K., Chen, E., Heiner, S.,
    Pettit, C., Olsson, C., Kundu, S., Kadavath, S., Jones, A., Chen, A., Mann, B.,
    Israel, B., Seethor, B., McKinnon, C., Olah, C., Yan, D., Amodei, D., . . . Kaplan,
    J. (2022). 通过模型编写的评估发现语言模型行为。*arXiv*。 [https://doi.org/10.48550/arXiv.2212.09251](https://doi.org/10.48550/arXiv.2212.09251)'
- en: '[20] Iyer, S., Lin, X. V., Pasunuru, R., Mihaylov, T., Simig, D., Yu, P., Shuster,
    K., Wang, T., Liu, Q., Koura, P. S., Li, X., Pereyra, G., Wang, J., Dewan, C.,
    Celikyilmaz, A., Zettlemoyer, L., & Stoyanov, V. (2022). OPT-IML: Scaling Language
    Model Instruction Meta Learning through the Lens of Generalization. *arXiv*. [https://doi.org/10.48550/arXiv.2212.12017](https://doi.org/10.48550/arXiv.2212.12017)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] Iyer, S., Lin, X. V., Pasunuru, R., Mihaylov, T., Simig, D., Yu, P., Shuster,
    K., Wang, T., Liu, Q., Koura, P. S., Li, X., Pereyra, G., Wang, J., Dewan, C.,
    Celikyilmaz, A., Zettlemoyer, L., & Stoyanov, V. (2022). OPT-IML: 通过泛化视角扩展语言模型指令元学习。*arXiv*。
    [https://doi.org/10.48550/arXiv.2212.12017](https://doi.org/10.48550/arXiv.2212.12017)'
- en: '[21] He, H., Zhang, H., & Roth, D. (2022). Rethinking with Retrieval: Faithful
    Large Language Model Inference. *arXiv*. [https://doi.org/10.48550/arXiv.2301.00303](https://doi.org/10.48550/arXiv.2301.00303)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] He, H., Zhang, H., & Roth, D. (2022). 通过检索重新思考：忠实的大型语言模型推理。*arXiv*。 [https://doi.org/10.48550/arXiv.2301.00303](https://doi.org/10.48550/arXiv.2301.00303)'
- en: '[22] Bohnet, B., Tran, V. Q., Verga, P., Aharoni, R., Andor, D., Soares, L.
    B., Eisenstein, J., Ganchev, K., Herzig, J., Hui, K., Kwiatkowski, T., Ma, J.,
    Ni, J., Schuster, T., Cohen, W. W., Collins, M., Das, D., Metzler, D., Petrov,
    S., . . . Webster, K. (2022). Attributed Question Answering: Evaluation and Modeling
    for Attributed Large Language Models. *arXiv*. [https://doi.org/10.48550/arXiv.2212.08037](https://doi.org/10.48550/arXiv.2212.08037)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] Bohnet, B., Tran, V. Q., Verga, P., Aharoni, R., Andor, D., Soares, L.
    B., Eisenstein, J., Ganchev, K., Herzig, J., Hui, K., Kwiatkowski, T., Ma, J.,
    Ni, J., Schuster, T., Cohen, W. W., Collins, M., Das, D., Metzler, D., Petrov,
    S., . . . Webster, K. (2022). 属性问答：属性大型语言模型的评估与建模。*arXiv*。 [https://doi.org/10.48550/arXiv.2212.08037](https://doi.org/10.48550/arXiv.2212.08037)'
- en: '[23] Gekhman, Z., Zverinski, D., Mallinson, J., & Beryozkin, G. (2022). RED-ACE:
    Robust Error Detection for ASR using Confidence Embeddings. *arXiv*. [https://doi.org/10.48550/arXiv.2203.07172](https://doi.org/10.48550/arXiv.2203.07172)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[23] Gekhman, Z., Zverinski, D., Mallinson, J., & Beryozkin, G. (2022). RED-ACE:
    使用置信嵌入进行鲁棒的错误检测。*arXiv*。 [https://doi.org/10.48550/arXiv.2203.07172](https://doi.org/10.48550/arXiv.2203.07172)'
- en: '[24] Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain,
    D., Fort, S., Ganguli, D., Henighan, T., Joseph, N., Kadavath, S., Kernion, J.,
    Conerly, T., Elhage, N., Hernandez, D., Hume, T., Johnston, S., Kravec, S., .
    . . Kaplan, J. (2022). Training a Helpful and Harmless Assistant with Reinforcement
    Learning from Human Feedback. *arXiv*. [https://doi.org/10.48550/arXiv.2204.05862](https://doi.org/10.48550/arXiv.2204.05862)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[24] Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain,
    D., Fort, S., Ganguli, D., Henighan, T., Joseph, N., Kadavath, S., Kernion, J.,
    Conerly, T., Elhage, N., Hernandez, D., Hume, T., Johnston, S., Kravec, S., .
    . . Kaplan, J. (2022). 通过人类反馈的强化学习训练有用且无害的助手。*arXiv*。 [https://doi.org/10.48550/arXiv.2204.05862](https://doi.org/10.48550/arXiv.2204.05862)'
- en: '[25] Baevski, A., Zhou, H., Mohamed, A., & Auli, M. (2020). wav2vec 2.0: A
    Framework for Self-Supervised Learning of Speech Representations. *arXiv*. [https://doi.org/10.48550/arXiv.2006.11477](https://doi.org/10.48550/arXiv.2006.11477)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[25] Baevski, A., Zhou, H., Mohamed, A., & Auli, M. (2020). wav2vec 2.0：一种自监督学习语音表示的框架。*arXiv*。
    [https://doi.org/10.48550/arXiv.2006.11477](https://doi.org/10.48550/arXiv.2006.11477)'
- en: '[26] Hsu, W., Bolte, B., Tsai, Y., Lakhotia, K., Salakhutdinov, R., & Mohamed,
    A. (2021). HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction
    of Hidden Units. *arXiv*. [https://doi.org/10.48550/arXiv.2106.07447](https://doi.org/10.48550/arXiv.2106.07447)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[26] Hsu, W., Bolte, B., Tsai, Y., Lakhotia, K., Salakhutdinov, R., & Mohamed,
    A. (2021). HuBERT：通过掩蔽预测隐藏单元进行自监督语音表示学习。*arXiv*。 [https://doi.org/10.48550/arXiv.2106.07447](https://doi.org/10.48550/arXiv.2106.07447)'
- en: '[27] Ao, J., Wang, R., Zhou, L., Wang, C., Ren, S., Wu, Y., Liu, S., Ko, T.,
    Li, Q., Zhang, Y., Wei, Z., Qian, Y., Li, J., & Wei, F. (2021). SpeechT5: Unified-Modal
    Encoder-Decoder Pre-Training for Spoken Language Processing. *arXiv*. [https://doi.org/10.48550/arXiv.2110.07205](https://doi.org/10.48550/arXiv.2110.07205)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[27] Ao, J., Wang, R., Zhou, L., Wang, C., Ren, S., Wu, Y., Liu, S., Ko, T.,
    Li, Q., Zhang, Y., Wei, Z., Qian, Y., Li, J., & Wei, F. (2021). SpeechT5: 统一模态编码器-解码器预训练用于口语语言处理。*arXiv*。
    [https://doi.org/10.48550/arXiv.2110.07205](https://doi.org/10.48550/arXiv.2110.07205)'
- en: '[28] Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai,
    Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., Dey, M., Bari, M. S., Xu,
    C., Thakker, U., Sharma, S. S., Szczechla, E., Kim, T., Chhablani, G., Nayak,
    N., . . . Rush, A. M. (2021). Multitask Prompted Training Enables Zero-Shot Task
    Generalization. *arXiv*. [https://doi.org/10.48550/arXiv.2110.08207](https://doi.org/10.48550/arXiv.2110.08207)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[28] Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai,
    Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., Dey, M., Bari, M. S., Xu,
    C., Thakker, U., Sharma, S. S., Szczechla, E., Kim, T., Chhablani, G., Nayak,
    N., . . . Rush, A. M. (2021). 多任务提示训练实现零样本任务泛化。*arXiv*。 [https://doi.org/10.48550/arXiv.2110.08207](https://doi.org/10.48550/arXiv.2110.08207)'
- en: '[29] Lu, J., Clark, C., Zellers, R., Mottaghi, R., & Kembhavi, A. (2022). Unified-IO:
    A Unified Model for Vision, Language, and Multi-Modal Tasks. *arXiv*. [https://doi.org/10.48550/arXiv.2206.08916](https://doi.org/10.48550/arXiv.2206.08916)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[29] Lu, J., Clark, C., Zellers, R., Mottaghi, R., & Kembhavi, A. (2022). Unified-IO:
    统一模型用于视觉、语言和多模态任务。*arXiv*。 [https://doi.org/10.48550/arXiv.2206.08916](https://doi.org/10.48550/arXiv.2206.08916)'
- en: '[30] Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi,
    R., & Ng, R. (2020). NeRF: Representing Scenes as Neural Radiance Fields for View
    Synthesis. *arXiv*. [https://doi.org/10.48550/arXiv.2003.08934](https://doi.org/10.48550/arXiv.2003.08934)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[30] Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi,
    R., & Ng, R. (2020). NeRF: 将场景表示为神经辐射场用于视图合成。*arXiv*。 [https://doi.org/10.48550/arXiv.2003.08934](https://doi.org/10.48550/arXiv.2003.08934)'
- en: '[31] Chen, C., Gao, R., Calamia, P., & Grauman, K. (2022). Visual Acoustic
    Matching. *arXiv*. [https://doi.org/10.48550/arXiv.2202.06875](https://doi.org/10.48550/arXiv.2202.06875)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[31] Chen, C., Gao, R., Calamia, P., & Grauman, K. (2022). 视觉声学匹配。*arXiv*。
    [https://doi.org/10.48550/arXiv.2202.06875](https://doi.org/10.48550/arXiv.2202.06875)'
- en: '[32] Zhu, Z., Peng, S., Larsson, V., Xu, W., Bao, H., Cui, Z., Oswald, M. R.,
    & Pollefeys, M. (2021). NICE-SLAM: Neural Implicit Scalable Encoding for SLAM.
    *arXiv*. [https://doi.org/10.48550/arXiv.2112.12130](https://doi.org/10.48550/arXiv.2112.12130)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[32] Zhu, Z., Peng, S., Larsson, V., Xu, W., Bao, H., Cui, Z., Oswald, M. R.,
    & Pollefeys, M. (2021). NICE-SLAM: 神经隐式可扩展编码用于SLAM。*arXiv*。 [https://doi.org/10.48550/arXiv.2112.12130](https://doi.org/10.48550/arXiv.2112.12130)'
