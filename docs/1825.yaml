- en: Large Language Models in Molecular Biology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/large-language-models-in-molecular-biology-9eb6b65d8a30?source=collection_archive---------0-----------------------#2023-06-02](https://towardsdatascience.com/large-language-models-in-molecular-biology-9eb6b65d8a30?source=collection_archive---------0-----------------------#2023-06-02)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Deciphering the language of biology, from DNA to cells to human health
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@serafimbatzoglou?source=post_page-----9eb6b65d8a30--------------------------------)[![Serafim
    Batzoglou](../Images/8abba787ae344663a196cfce707a8718.png)](https://medium.com/@serafimbatzoglou?source=post_page-----9eb6b65d8a30--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9eb6b65d8a30--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9eb6b65d8a30--------------------------------)
    [Serafim Batzoglou](https://medium.com/@serafimbatzoglou?source=post_page-----9eb6b65d8a30--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fccf342949c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flarge-language-models-in-molecular-biology-9eb6b65d8a30&user=Serafim+Batzoglou&userId=ccf342949c4&source=post_page-ccf342949c4----9eb6b65d8a30---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9eb6b65d8a30--------------------------------)
    ·40 min read·Jun 2, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9eb6b65d8a30&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flarge-language-models-in-molecular-biology-9eb6b65d8a30&user=Serafim+Batzoglou&userId=ccf342949c4&source=-----9eb6b65d8a30---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9eb6b65d8a30&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flarge-language-models-in-molecular-biology-9eb6b65d8a30&source=-----9eb6b65d8a30---------------------bookmark_footer-----------)![](../Images/bdf1de98f9a4a97280c1fe604bfe3c5e.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image by author, created by Midjourney prompted with “DNA”.**'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Will we ever decipher the language of molecular biology? Here, I argue that
    we are just a few years away from having accurate in silico models of the primary
    biomolecular information highway — from DNA to gene expression to proteins — that
    rival experimental accuracy and can be used in medicine and pharmaceutical discovery.
  prefs: []
  type: TYPE_NORMAL
- en: Since I started my PhD in 1996, the computational biology community had embraced
    the mantra, “biology is becoming a computational science.” Our ultimate ambition
    has been to predict the activity of biomolecules within cells, and cells within
    our bodies, with precision and reproducibility akin to engineering disciplines.
    We have aimed to create computational models of biological systems, enabling accurate
    biomolecular experimentation in silico. The recent strides made in deep learning
    and particularly large language models (LLMs), in conjunction with affordable
    and large-scale data generation, are propelling this aspiration closer to reality.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs, already proven masters at modeling human language, have demonstrated extraordinary
    feats like passing the bar exam, writing code, crafting poetry in diverse styles,
    and arguably rendering the Turing test obsolete. However, their potential for
    modeling biomolecular systems may even surpass their proficiency in modeling human
    language. Human language mirrors human thought providing us with an inherent advantage,
    while molecular biology is intricate, messy, and counterintuitive. Biomolecular
    systems, despite their messy constitution, are robust and reproducible, comprising
    millions of components interacting in ways that have evolved over billions of
    years. The resulting systems are marvelously complex, beyond human comprehension.
    Biologists often resort to simplistic rules that work only 60% or 80% of the time,
    resulting in digestible but incomplete narratives. Our capacity to generate colossal
    biomolecular data currently outstrips our ability to understand the underlying
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: This article will provide an overview of some recent breakthroughs of deep learning-based
    language models in molecular biology. We will discuss how these advances will
    converge with the direct training of LLMs on large-scale biomolecular and population
    health data in the coming years and propel the field forward. Given that LLMs
    and deep learning are familiar to a broader audience than is molecular biology,
    we begin with a brief overview of LLMs, continue with a more detailed introduction
    to molecular biology, then proceed to describe a few recent LLM advances in molecular
    biology, and finally glance at the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the core of our discussion is an ongoing paradigm shift in biology. Despite
    the often overused term “paradigm shift,” it is genuinely fitting here. Traditionally,
    biology has been hypothesis-driven: researchers identify patterns, formulating
    hypotheses, designing experiments or studies to test these hypotheses, and adjusting
    their theories based on the results. This approach is progressively being replaced
    by a data-driven modeling methodology. In this emerging paradigm, researchers
    start with hypothesis-free, large-scale data generation, then train a model such
    as an LLM or incorporate the data into an existing LLM. Once the LLM can accurately
    model the system, approaching the fidelity seen between experimental replicates,
    researchers can interrogate the LLM to extract insights about the system and discern
    the underlying biological principles. This shift will be increasingly pronounced
    and allow accurate modeling of biomolecular systems at a granularity that goes
    well beyond human capacity.'
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/9d230a0fc0ea121c00c7ecb542663f46.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Image by author, created with Midjourney.**'
  prefs: []
  type: TYPE_NORMAL
- en: A Large Language Model (LLM) is a type of neural network that acquires the ability
    to generate text mirroring human language by scrutinizing vast amounts of textual
    data. It operates on the principle of “self-supervision,” where the model learns
    to predict the subsequent word in a sentence based on the preceding words. This
    process allows the LLM to identify patterns, relationships, and context within
    the text, equipping it to respond to queries, generate novel content, and even
    formulate predictions. LLMs can be viewed as an advanced form of autocomplete,
    predicting the next word you are likely to type but with a surprising ability
    to behave like they have a solid comprehension of language, context, and meaning.
    This enables them to generate coherent and knowledgeable responses across diverse
    topics.
  prefs: []
  type: TYPE_NORMAL
- en: 'The evolution of language models has seen each new generation boasting enhanced
    modeling capabilities. Let’s briefly traverse the primary types of language models
    and their unique features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Word grams:** These rudimentary models predict the next word in a sentence
    based on the frequency of word pairs or word bags (unordered sets of words) in
    the training data. They disregard context or word order, leading to less coherent
    predictions. Generating text using these models results in incoherent sentences
    that have little resemblance to human text.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**CNNs (Convolutional Neural Networks):** These models analyze text data by
    considering relationships between adjacent words in a fixed window. The window
    can be quite wide, using techniques like dilation. While CNNs excel at identifying
    local patterns, they fall short in capturing long-range dependencies or comprehending
    complex sentence structures.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**LSTMs (Long Short-Term Memory networks):** These are a variant of Recurrent
    Neural Networks (RNN) capable of storing and processing information from earlier
    parts of a text. LSTMs outperform CNNs in understanding context and managing long-range
    dependencies, but they still falter with complex sentences and long text.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Attention Mechanisms** enable models to concentrate on pertinent parts of
    the input when making predictions. A number of attention “heads” allow the model
    to focus on different parts of the previous text when predicting the next word.
    They function similarly to how you would revisit key points or details in a lengthy
    article, allowing the model to refer back to relevant parts of the text and incorporate
    that information into the current context. Transformers are a class of language
    models that implement attention mechanisms.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Large Language Models (LLMs):** models such as GPT-3 are transformers that
    leverage attention mechanisms and are trained on vast amounts of data. Their considerable
    size facilitates the learning of intricate patterns, relationships, and context
    within the text. LLMs represent the most advanced language models presently available,
    capable of generating remarkably accurate and coherent responses across a broad
    spectrum of topics.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Two LLMs that use the transformer architecture and which introduced major breakthroughs
    in the field deserve special mention: BERT and the GPT series.'
  prefs: []
  type: TYPE_NORMAL
- en: '**BERT (Bidirectional Encoder Representations from Transformers)** (Devlin
    et al. 2018) is a series of LLMs introduced by Google in 2018 and open sourced,
    with the code available on GitHub and a number of pre-trained models released.
    BERT is trained using **masked language modeling**. The idea is to hide or “mask”
    some percentage of the input tokens at random, and then predict those masked tokens.
    This forces the model to understand the context from both the left and the right
    sides of the input (hence “bidirectional”). BERT training also uses a next sentence
    prediction task. During training, the model is given pairs of sentences and has
    to predict whether the second sentence in the pair is the next sentence in the
    original document.'
  prefs: []
  type: TYPE_NORMAL
- en: '**GPT (Generative Pretrained Transformer)** is a series of LLMs introduced
    by OpenAI. Unlike BERT, GPT is trained using the traditional language modeling
    task of autocomplete: predict the next word in a sentence. Unlike BERT, GPT only
    attends to the left context (or previous tokens) during training, hence it is
    unidirectional. GPT is a generative model that is particularly strong in tasks
    involving text generation, such as writing essays, generating poetry, or completing
    sentences. The latest generation of GPT, GPT-4, has shown remarkable performance
    across a variety of tasks in multiple domains, leading to the characterization
    of it as exhibiting some sparks of general intelligence (Bubeck et al. 2023).
    It should be noted that not everyone agrees that GPT and similar LLMs show signs
    of general intelligence. To quote Rodney Brooks, “stop confusing performance with
    competence” ([https://spectrum.ieee.org/amp/gpt-4-calm-down-2660261157](https://spectrum.ieee.org/amp/gpt-4-calm-down-2660261157)).
    However, as we will see in this article, this is not a limitation for their effective
    application in molecular biology.'
  prefs: []
  type: TYPE_NORMAL
- en: The Genetic Dogma
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The biological trajectory of a human or any other organism, from embryonic
    development to the entirety of their lifespan, is a complex interplay between
    genetics and environment: a dialogue between an individual’s DNA and the environment
    the individual is exposed to (Figure 1).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/77b9c2729288937c905867147eb77dee.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 1\. Genotype-Phenotype-Environment.** The phenotypes of an individual
    are a dialogue between the individual’s DNA and the environment. Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The central dogma of molecular biology describes the flow of genetic information
    within living organisms. The source of this genetic information is our DNA, an
    exact replica of which is harbored in the nucleus of every cell in our body. Human
    DNA comprises about 3 billion nucleotides arranged in 23 chromosomes, with 22
    being autosomes and one being the sex chromosome, either X or Y. Each individual
    possesses two nearly identical copies of the human genome: one inherited by their
    mother and one by their father. Every one of the approximately 30 trillion cells
    in our body retains within the nucleus a nearly identical copy of our maternal
    and paternal genome.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/498b5e952af7806d55ba5bbc9177c6ff.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 2\. A human chromosome.** The chromatin is tightly packed in hierarchical
    coil structures. At the bottom level, 146 nucleotide pairs are wrapped around
    a histone, resembling a bead. Histones are then coiled and supercoiled to form
    a compact chromosome that fits within the nucleus of a cell. Image from VectorMine,
    iStock Content License Agreement.'
  prefs: []
  type: TYPE_NORMAL
- en: Within the genome lie around 20,000 genes, which are DNA segments accountable
    for protein synthesis. Approximately 1% of the genome codes for proteins, while
    the remainder comprises regions controlling gene expression, regions within genes
    that don’t code for proteins, regions contributing to DNA structure, and “junk”
    regions of selfish DNA that have “learned” to self-replicate.
  prefs: []
  type: TYPE_NORMAL
- en: The central dogma of molecular biology maps out the molecular information flow
    from the genome to the expression of genes and the subsequent production of proteins,
    which are fundamental building blocks of life.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/07c93465bafcd44f5add48dad168ed08.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 3\. The central dogma of molecular biology.** Our DNA is composed
    of about 20,000 genes and intergenic regions. Genes are expressed within cells
    by a process of **transcription**, which copies the gene into a single-stranded
    molecule, the mRNA, and **translation**, which translates the mRNA sequence into
    a protein sequence composed of amino acids. Thereby, the 4-letter nucleotide code
    of the DNA segment is translated into a 20-amino acid code of the protein sequence.
    The protein sequence then folds in 3D to form a functional protein structure.
    Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Protein synthesis encompasses three primary steps: **transcription** (Figure
    3), **splicing** (Figure 4), and **translation**. During transcription, a DNA
    segment corresponding to a gene serves as a template that is replicated into a
    molecule known as messenger RNA (mRNA). The mRNA molecule undergoes splicing,
    a process wherein certain segments of the molecule are excised, or spliced out,
    and the remaining segments are joined together to form the mature mRNA. The excised
    regions are known as **introns**, and the kept regions, the **exons**, constitute
    the protein-coding part of the mRNA. Each mature mRNA is assembled from an average
    of 7 exons, although the count varies in humans from 1 to 79 for the human dystrophin
    gene. Splicing is vital in higher organisms because a single gene can yield multiple
    different proteins by assembling different exon combinations during splicing.
    The 20,000 genes give rise to approximately 70,000 known standard splice forms
    and a substantially larger number of rare or aberrant splice forms. The expression
    timing of each protein variant is part of a cell’s molecular control toolkit.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c368887b663be0f122bd16f593741ebe.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 4\. Splicing of mRNA.** In humans and other eukaryotic organisms,
    an important process between transcription and translation is splicing. Certain
    regions of the mRNA are cut out, called the introns, and the rest are glued together
    in order, called the exons. The same gene can be spliced in multiple ways, leading
    to alternative splicing forms of the resulting protein, and contributing to the
    diversity of proteins. Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: Following transcription, the mRNA is transported to the cell’s protein-synthesizing
    machinery, the **ribosome**, where translation takes place. During translation,
    the mRNA sequence is decoded in groups of three nucleotides, known as codons.
    Each codon corresponds to exactly one of the 20 amino acids that form the building
    blocks of proteins. These amino acids are linked together in a chain to form a
    protein sequence, which subsequently folds into a functional, three-dimensional
    protein structure.
  prefs: []
  type: TYPE_NORMAL
- en: Proteins are the building blocks of life, playing pivotal roles in virtually
    every biological process. They provide the structural components of cells, act
    as enzymes to catalyze chemical reactions, and facilitate communication and transportation
    within cells.
  prefs: []
  type: TYPE_NORMAL
- en: '**Gene regulation** (Figure 5) pertains to the intricate processes that dictate
    when, where, and in what quantity genes are expressed within a cell. This ensures
    the timely production of the right proteins in the right quantities. Gene regulation
    takes place at various levels, including the structuring of chromatin, chemical
    modifications, and through the action of specific proteins known as transcription
    factors.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1fdcf591db6937930e4bf1bb5e9fb86c.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 5\. Gene Regulation.** The promoter of a gene, the region upstream
    (to the left) of the gene start contains control elements including motifs that
    bind certain proteins called transcription factors. Those play a role in recruiting
    the RNA polymerase and controlling when, where, and to what amount the gene will
    be expressed. Open chromatin is required for transcription to take place. Image
    adapted from a presentation by Anshul Kundaje, including here with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transcription factors (TFs)** are proteins that are instrumental in gene
    regulation. They bind to distinct DNA sequences near or within genes, known as
    transcription factor binding sites, thereby influencing the recruitment of RNA
    polymerase, the enzyme tasked with mRNA synthesis. Consequently, transcription
    factors modulate the expression of target genes, guaranteeing the appropriate
    gene expression in response to diverse cellular signals and environmental conditions.
    Transcription factors are themselves modulated by transcription factors, forming
    complex gene regulatory pathways.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Promoters and enhancers** are DNA regions that play a role in gene expression
    control. Promoters are located adjacent to the start of a gene (upstream, or to
    the left of the gene start, in the chemical direction of DNA), while enhancers
    are more distant regulatory elements situated within introns or between genes.
    Both promoters and enhancers harbor several transcription factor binding sites.
    With the assistance of transcription factors, a gene’s promoter and enhancers
    form three-dimensional structures that recruit and regulate the RNA polymerase
    responsible for mRNA synthesis.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Chromatin structure** (Figure 2) is an amalgamation of DNA and proteins (histones)
    that constitute our chromosomes. To fit compactly within each cell’s nucleus,
    DNA is wound around proteins known as histones. Histones are tetramers, structures
    formed by assembling four copies of the histone protein. Each such structure wraps
    around 146 nucleotide pairs of DNA, creating a rosary bead-like structure that
    subsequently folds into a higher-order helical structure, the chromatin. Chromatin
    organization determines which DNA regions are accessible for gene expression.
    For gene expression to occur, chromatin must be unfolded. Conversely, tightly
    packed chromatin prevents gene expression.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Histone modifications** refer to chemical modifications, such as acetylation
    or methylation, that can affect the histone beads, thereby influencing chromatin
    structure and gene accessibility. These modifications can either promote or inhibit
    gene expression, contingent upon the type and location of the modification. They
    are also part of the histone code, a sort of epigenetic code, i.e., an additional
    layer of code superimposed on the genetic code inscribed in the DNA. (“epi-” is
    a Greek root signifying “on top of”.)'
  prefs: []
  type: TYPE_NORMAL
- en: '**DNA methylation** is a chemical modification where a methyl group is added
    to the DNA molecule, often at specific cytosine bases. Methylation can influence
    gene expression by affecting the binding of transcription factors or changing
    the chromatin structure, making it more compact and less accessible for transcription.
    Methylation and other DNA chemical modifications are also part of the epigenetic
    code. Gene regulation is a dynamic process specific to each cell type. Different
    cells within our body exhibit unique gene expression profiles, enabling them to
    perform specialized functions. Through precise control of gene expression, cells
    can respond to environmental stimuli, sustain homeostasis, and execute the complex
    processes essential for life.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bidirectional flow of information.** Traditionally, the central dogma is
    described as a unidirectional flow of information: DNA to RNA to protein. However,
    there are exceptions to this and our knowledge of the underlying mechanisms is
    still evolving, a topic that is beyond the scope of this brief review. It is worth
    mentioning some exceptions. (1) The discovery of reverse transcription, a process
    where RNA is converted back into DNA, challenged the unidirectionality of the
    central dogma. This process is facilitated by the enzyme reverse transcriptase
    and is common in retroviruses, such as HIV. (2) DNA can also be transcribed into
    RNA molecules other than mRNA, such as transfer RNA (tRNA), ribosomal RNA (rRNA),
    and other types of non-coding RNA, adding another level of complexity to the flow
    of genetic information. (3) Finally, there’s a growing body of evidence about
    the role of epigenetics by mechanisms such as DNA methylation and histone modification,
    and research into the extent to which epigenetic changes can be inherited.'
  prefs: []
  type: TYPE_NORMAL
- en: Variation in our DNA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every individual is biologically shaped by the complex interplay between their
    DNA and environmental influences throughout their lifetime, from conception to
    the present moment. Our DNA, coupled with the human female reproductive system,
    ensures we are born as humans rather than, for instance, chimpanzees, whose DNA
    is 98.8% identical to ours. Any two humans share more than 99.9% identical DNA.
    Yet, our DNA variants account for the heritability of all our traits, including
    the genetic contribution to health and disease.
  prefs: []
  type: TYPE_NORMAL
- en: '**Origins of DNA variants.** The primary mechanism for introducing DNA variants
    is through mutations between the genomes of two parents and the germline genomes
    that both parents contribute to the offspring’s genome. In humans, a child’s DNA
    contains approximately 50–100 mutations compared to the parents’ DNA; the majority
    of these are contributed by the father, with a correlation to the father’s age
    (Kong et al. 2012). Germline mutations primarily drive genetic variation, accounting
    for our differences from species such as chimpanzees and squirrels. Most of these
    new variants are benign, either having no effect on the phenotype or some impact
    that is neither advantageous nor disadvantageous. A smaller fraction can be deleterious,
    particularly if they damage a functional region, which could be protein-coding,
    regulatory, or even related to chromatin structure. An even smaller fraction might
    be beneficial, such as a variant that fortuitously improves a functional element.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Selection.** Deleterious variants, or harmful genetic alterations, often
    render an organism less “fit” in evolutionary terms, with fitness defined as the
    expected number of surviving offspring. Over time, harmful variants tend to be
    statistically eliminated from the population. Consequently, genetic variations
    common in humans — those found in at least 1% of the population — are either benign
    or contribute to diseases that manifest later in life, beyond the reach of natural
    selection. This is also why rare variants are generally more likely to be harmful
    than common ones.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Coalescence and DNA sequence conservation.** Across longer evolutionary times,
    such as those separating humans and chimpanzees or dogs, the effects of selection
    on DNA are highly informative. Take any two individuals today. For example, me
    and my dog, Murzik (a maltipoo). Take any DNA region that is shared, such as any
    of the majority of human genes, which we share with dogs. Take my maternal copy
    of that gene, and Murzik’s (say) paternal copy of that gene. They have a similarity
    of about 84%. Now if we trace this region back in history (my mother inherited
    it from her mother (say), she inherited it from her father, and so on; Murzik’s
    father inherited it from his mother (say), she inherited it from her mother, and
    so on), eventually the two regions **coalesce**: there is an ancestor mammalian
    individual that had two kids that both inherited the precise same DNA piece: one
    of these kids led to me, and another one led to Murzik. The 16% sequence differences
    reflect all the germline mutations that took place across millions of generations
    that separate us from this common ancestral great grandparent. Importantly, mutations
    that took place in important parts of the gene tended to make individuals less
    fit, and are less likely to have led to me or to Murzik today. Therefore, the
    more conserved parts of the DNA region are more likely to be functionally important,
    and the less conserved parts are more likely to be tolerant of mutations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/05e7c79b466d86a650c891d2d38fe12b.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 6\. Cost of human genome seequencing.** The plot does not include
    the last two years, where additional major drops in cost took place. The cost
    today with the newest instruments is as low as $200\. Wetterstrand KA. DNA Sequencing
    Costs: Data from the NHGRI Genome Sequencing Program (GSP) Available at: [**www.genome.gov/sequencingcostsdata**](https://www.genome.gov/sequencingcostsdata).
    Accessed 5/25/2023.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data generation.** Since the initiation of the Human Genome Project over
    30 years ago, numerous DNA sequencing technologies have been developed, allowing
    for the rapid and cost-effective generation of DNA data. Today, an entire human
    genome can be fully sequenced for as little as $200 (Figure 6). Remarkably, the
    same technology used for sequencing our whole genome can also generate data on
    a multitude of molecular functions, such as those implicated in the central dogma
    of molecular biology. For instance, by integrating DNA sequencing with single-cell
    microfluidic technologies, researchers can measure the transcription level of
    every gene in thousands of individual cells within a biological sample. Sequencing-based
    methods can expose the structure of chromatin, histone modifications, transcription
    factor binding to DNA, and other crucial molecular information. A comprehensive
    explanation of how this is achieved is beyond the scope of this text, but in brief,
    short DNA segments with a specific property of interest — such as binding a certain
    transcription factor or being part of the open, accessible chromatin — are isolated
    in an experiment and sequenced.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to DNA sequencing, other technologies like mass spectrometry (MS)
    and affinity-based proteomics can measure the levels of all proteins in a biological
    sample. X-ray crystallography, albeit lower in throughput, provides high-resolution
    3D structures of proteins.
  prefs: []
  type: TYPE_NORMAL
- en: Over the past 20–30 years, our capacity to measure molecular function has drastically
    exceeded the pace of Moore’s law, primarily because of advances in DNA sequencing
    technology that also enable a large variety of molecular readouts such as gene
    expression, chromatin accessibility and histone modifications to be performed
    by sequencing-based assays. This swift advancement in data generation allows scientists
    to measure most genetic aspects within biological samples, often with single-cell
    or spatial precision.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/efbf7a798fa5a52ab2c87ccc8917e7c5.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 7\. The Genome Wide Association Studies Catalog.** The latest version
    of the iconic diagram that depicts the summary of all the associations known to
    date between locations in the 23 chromosomes and phenotypes. Source: [https://www.ebi.ac.uk/gwas/](https://www.ebi.ac.uk/gwas/).
    The diagram can be browsed live, and the associations are publicly available.
    Diagram available under CC0: [https://www.ebi.ac.uk/gwas/docs/about](https://www.ebi.ac.uk/gwas/docs/about).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linking variation to function.** For over two decades, researchers have endeavored
    to elucidate gene functions and the molecular mechanisms of diseases by correlating
    genetic variants across numerous individuals’ genomes with specific phenotypes,
    such as the presence or absence of a particular disease. These investigations,
    termed genome-wide association studies (GWAS), identify statistically significant
    associations of certain genome locations, which could be genes or regulatory regions,
    with the phenotypes under study. The GWAS Catalog (https://www.ebi.ac.uk/gwas/),
    a public resource, currently contains more than 6,300 publications and 515,000
    such associations (Figure 7). When the measured phenotype isn’t binary but a quantifiable
    entity, such as height, a regression can be performed between the genomic variation
    and the phenotype, with the identified genetic loci termed quantitative trait
    loci. Besides macroscopic phenotypes like disease status, height, or hair color,
    genetic variation can be associated with molecular phenotypes such as gene expression
    levels (leading to expression quantitative trait loci, or eQTLs), protein abundance
    (resulting in protein quantitative trait loci, or pQTLs), and virtually every
    other molecular measurement. These analyses offer valuable insights into the molecular
    mechanisms governing cell function and human physiology. However, as we will discuss
    below, these traditional association analyses are likely to be surpassed by the
    application of LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: Language Models in Molecular Biology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Over the past few years, remarkable progress has been made in modeling each
    step of the central dogma of molecular biology. While we haven’t yet fully transformed
    molecular biology into a computational science or made medicine and human health
    into an engineering discipline, the current momentum suggests that only a wealth
    of additional data and some further development stand between us and this vision.
    This progress is somewhat distinct from other areas of AI application. Personally,
    I believe that artificial general intelligence (AGI), even at the level of a small
    mammal, is still beyond the horizon. Additionally, combinatorics, discrete algorithms
    and mathematical reasoning aren’t the strong suits of LLMs. This is expected by
    the fact that these models are feed-forward architectures that do not include
    loops, other than the implicit loop created by taking the text generated so far,
    and feeding it back to the LLM as input. As explained in Stephen Wolfram’s excellent
    overview, computational irreducibility guarantees that these models can’t do certain
    things (Wolfram 2023). (It is worth mentioning that sparks of such abilities are
    starting to emerge in systems such as GPT-4 as described by Bubek et al, 2023.)
    However, modeling molecular biology does not need AGI: it doesn’t require high-level
    planning, agency, or goals, and only has a limited need for combinatorics and
    algorithmic reasoning. Instead, modeling molecular biology requires what LLMs
    excel at: learning the statistical properties of intricate, noisy sequential data
    to best predict such data from lossy representations.'
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this point, let us examine a few recent deep learning breakthroughs
    in different stages of the central dogma of molecular biology.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: I am coauthor in some of the work below, in particular the SpliceAI
    and PrimateAI-3D methods. As such my exposition could be biased.*'
  prefs: []
  type: TYPE_NORMAL
- en: Predicting gene structure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: According to the fundamental dogma of molecular biology, the primary function
    of DNA is to encode genes that are transcribed and translated into proteins. The
    specific segments of each gene that are translated into proteins are determined
    by the splicing mechanism; these segments are well-annotated for the vast majority
    of genes in the genome. However, mutations can disrupt the precise boundaries
    of splicing, known as splice sites. Rare mutations that disrupt splicing can significantly
    impact the resulting protein function, as they usually produce a completely different
    protein sequence. Consequently, they account for about 10% of rare genetic diseases
    (Jaganathan et al. 2019). Predicting splice sites and deducing gene structure
    is therefore a fundamental computational task with implications for diagnosing
    genetic diseases. In fact, this was one of the first problems I explored during
    my PhD and have continued to publish on it throughout my career. The literature
    on splice site prediction is extensive. However, until around 2018, the problem
    remained a significant challenge, with the best methods achieving accuracy of
    about 30%, a level not sufficiently predictive for applications such as genetic
    diagnosis.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8a2fa5094c3830176064791591698c63.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 8\. SpliceAI model.** Image created by Kishore Jaganathan, included
    with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In 2019, SpliceAI was introduced by the Illumina AI laboratory (Jaganathan
    et al. 2019). SpliceAI doesn’t utilize transformer technology or function as an
    LLM; instead, it employs earlier techniques for language modeling, wherein the
    language is DNA sequences. It is a deep residual CNN, utilizing dilated convolutions
    to efficiently expand the window size it can handle. It accepts 10,000 nucleotide
    windows of the human genome as input and predicts the exact locations of the intron-exon
    boundaries, the so-called donor and acceptor sites — the exon-intron and intron-exon
    borders, respectively. In terms of precision-recall area under the curve (PR-AUC),
    SpliceAI achieved a score of 0.98 across the human genome, compared to the previous
    best of 0.23\. Importantly, SpliceAI is accurate enough to perform mutational
    analysis in silico: it can artificially alter any position of the DNA and determine
    whether this change introduces or eliminates a splice site within 10,000 nucleotides
    of the alteration. As a result, it can be utilized to aid genetic diagnosis: given
    a patient with a genetic disease, such as a young individual with a pediatric
    disorder, all variants in the individual not present in the parents can be compiled,
    and each variant can be input into SpliceAI to ask if it is likely to alter the
    splicing of nearby genes, thereby disrupting the function of a gene. To date,
    it has solved hundreds of previously unresolved cases of rare undiagnosed pediatric
    disease, in the context of the Genomics England 100,000 genomes project (Farh
    K, personal communication).'
  prefs: []
  type: TYPE_NORMAL
- en: 'How did SpliceAI achieve high accuracy? In brief, it learned complex biomolecular
    properties of the DNA sequence that reliably guide the splicing machinery to the
    splice sites. These properties were previously unknown or only imprecisely known;
    SpliceAI’s deep residual network has enough capacity to capture them accurately.
    This raises an interesting question about the interpretation of deep neural networks:
    how can we extract the biomolecular rules that SpliceAI learned, to gain insight
    into the underlying biomolecular mechanisms? Generally, neural networks are black
    boxes that don’t explain how they make a prediction. However, techniques exist
    for probing the network and extracting the features to which it pays attention.
    The SpliceAI team performed such analysis and describe a plethora of learned features
    (Jaganathan et al. 2019).'
  prefs: []
  type: TYPE_NORMAL
- en: Predicting protein structure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The central dogma of molecular biology tells the story of how information in
    our DNA gives rise to proteins, which are the fundamental building blocks of life.
    Protein sequences are directly translated from spliced mRNA sequences according
    to the genetic code, and then fold into functional 3D shapes — protein structures.
    Predicting protein structure from the protein sequence, known as the protein folding
    problem, has long been regarded as the Holy Grail of molecular biology, due to
    its immense importance and seemingly insurmountable difficulty. The gold standard
    for protein structures is experimental data from X-ray crystallography, which
    is challenging to obtain due to difficulties in producing high-quality protein
    crystals and the complex data processing required to derive the protein structure.
    Computational prediction has been a focus of research for decades, despite structure
    prediction methods not coming close to the accuracy of X-ray crystallography.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b1abd3c27ea781dc72d2242b08e31eb6.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 9\. Accuracy results in the CASP competition for protein structure
    prediction.** Each method is scored in a number of proteins whose structures are
    previously unknown, and experimentally determined by the time of the competition’s
    conclusion. The score reflects the percentage of amino acids that match the experimentally
    determined structure almost perfectly. Whereas for many years methods hovered
    around 40% or less, AlphaFold 2 has achieved accuracy of 89%, which comes close
    to experimental-level accuracy. Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: The biannual competition, CASP (Critical Assessment of protein Structure Prediction),
    has been tracking progress in this field. In the 2019 competition, the AlphaFold
    method by DeepMind made a huge leap in accuracy compared to previous benchmarks.
    In 2021, AlphaFold 2 (Jumper et al. 2021) made another significant leap, nearly
    matching the accuracy of X-ray crystallography. Subsequently, in collaboration
    with the European Molecular Biology Laboratory (EMBL), DeepMind released a comprehensive
    open-source database based on AlphaFold2, called the AlphaFold Protein Structure
    Database. The database provides high-accuracy structural predictions for various
    organisms, including human proteins, model organisms, and important pathogens.
    These predicted structures are expected to expedite research and provide valuable
    insights into biological processes, drug discovery, and disease understanding.
    As of today, there are 214,683,829 protein structures in the database. In essence,
    the once Holy Grail of molecular biology is now close to a solved problem thanks
    to deep learning. AlphaFold 2 represents a major scientific advance by any measure.
  prefs: []
  type: TYPE_NORMAL
- en: “[DeepMind’s protein-folding AI has solved a 50-year-old grand challenge of
    biology](https://www.technologyreview.com/2020/11/30/1012712/deepmind-protein-folding-ai-solved-biology-science-drugs-disease/)’’
    Will Heaven, Technology Review, 30 November 2020
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/fff8c57b6ef3c75099b724b3e500eee6.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 10\. Architecture of AlphaFold 2\. Image used by permission from the
    AlphaFold team (Jumper et al. 2021). (a)** Performance of AlphaFold on the CASP14
    dataset. **(b)** AlphaFold prediction of target T1049 (PDB 6Y4F, blue) compared
    with the experimental structure (green). **(c)** CASP14 target T1056 (PDB 6YJ1).
    An example of a well-predicted zinc-binding site. **(d)** CASP target T1044 (PDB
    6VR4), a 2,180-amino acid single chain, was predicted accurately. **(e)** Model
    architecture. Readers are referred to the original paper for detailed explanations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'How did AlphaFold achieve such remarkable accuracy? The methodologies are worth
    summarizing (Figure 10). The techniques used in the AlphaFold paper bear a resemblance
    to an earlier method developed by Jinbo Xu and colleagues (Wang et al 2017). This
    method combines a convolutional neural network operating on protein sequences
    with a pairwise co-evolution feature. This feature identifies pairs of sequence
    positions that co-vary across related protein sequences in different species,
    to predict 2D contact maps across a protein sequence. A contact map is a score
    for every pair of positions in the sequence, indicating the likelihood of these
    two positions being in close proximity in 3D. The AlphaFold 2 method builds on
    these algorithms and is expertly engineered and trained to provide a significant
    leap in structural prediction accuracy. AlphaFold2 introduced several additional
    novel improvements: (1) It is based on the transformer LLM architecture, which
    enhances its ability to capture long-range interactions between amino acids in
    the protein sequence. (2) A novel energy-based score, the Amber energy, was introduced
    to directly optimize the 3D protein structure, allowing for an end-to-end differentiable
    approach during the structure optimization step. (3) An improved utilization of
    coevolutionary features by incorporating multiple sequence alignment (MSA) data
    boosts the model’s ability to identify conserved structural features across homologous
    protein sequences. (4) A refinement stage fine-tunes the predicted protein structures
    using a second model trained on the output of the first model, leading to more
    accurate and consistent predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/471c9e8218e79cc12d625923c3819b82.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 11\. Predicted (blue) and experimentally determined (green) protein
    structures.** Image included with permission from the AlphaFold team. (Jumper
    et al. 2021, Varadi et al. 2021).'
  prefs: []
  type: TYPE_NORMAL
- en: Since the inception of AlphaFold, progress in the application of deep learning
    to protein structural prediction, modeling, and design has progressed at blinding
    speed. ESMFold (Lin et al. 2023) is an LLM for protein structural prediction that
    provides a speedup of up to 60x without loss of accuracy. ProteinGenerator (Lyayuga
    Lisanza et al. 2023) is a sequence-space diffusion model based on the RoseTTAfold
    (Baek et al. 2021) protein structural prediction method by the same lab. ProteinGenerator
    simultaneously generates protein sequences and their accompanying structures that
    satisfy any given sequence and structural properties, as the authors demonstrate
    experimentally. RosettaFold2 (Baek et al. 2023) combines features of AlphaFold2
    and RosettaFold to provide comparable accuracy with AlphaFold2 at improved computational
    efficiency. We are at the beginning of incredible innovation in protein design,
    with upcoming groundbreaking advances in drug design and bioengineering.
  prefs: []
  type: TYPE_NORMAL
- en: One key takeaway is that while decades of work on first principles, including
    protein structure energy minimization and protein kinetics modeling, failed to
    yield accurate structural prediction, the complex and intricate molecular information
    of how proteins actually fold is present in the data, and LLMs were capable of
    learning it.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting the impact of protein variants
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: More than 4 million positions in the genomes of any two individuals vary, with
    over 20,000 such variants located within the protein-coding regions. The majority
    of this genetic variation is benign and contributes significantly to the phenotypic
    diversity observed across humans. However, a small fraction of this genetic diversity
    is deleterious and contributes to genetic diseases. Understanding the impact of
    genetic variants and categorizing them as benign or deleterious has direct applications
    in the diagnosis of genetic diseases, identification of gene targets for drug
    development, and comprehension of the molecular mechanisms of diseases. Regrettably,
    the vast majority of variants are “variants of uncertain significance” (VUSs),
    and their impact on disease is unknown. Annotating such variants is a crucial
    unresolved problem in human genetics.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3e11f081207fba5f6fb164e8bda39698.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12\. Primate Phylogeny.** Human’s closest relatives are the great
    apes. We share a common ancestor with chimpanzees and bonobos around 5–7 million
    years ago, and with gorillas a bit longer ago. Our DNA similarity is 98.8% with
    chimpanzees, 98.4% with gorillas, and 97% with orangutans. Phylogeny trees like
    the one in the picture display the evolutionary history of extant species. For
    example, the great apes consist of humans, chimpanzees and bonobos that split
    from the human lineage about 5–7 million years ago, east and west gorillas that
    split about 8 million years ago, and orangutan species that split about 15–19
    million years ago. Our next closest relatives are African and Asian monkeys. When
    comparing DNA sequences across species, the functionally important positions in
    DNA, which are likely to cause genetic disease if mutated, are more likely to
    be conserved. Conversely, positions where we observe differences between today’s
    primates and our genome are more likely to be tolerant of mutations and to not
    cause genetic disease when mutated. Image generated by Lukas Kuderna and included
    with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One important clue in determining whether a given variant is benign, or at
    least not too deleterious, comes from comparing human genetics to the genetics
    of close relatives such as chimpanzees and other primates (Figure 12). Our genome
    closely resembles the genomes of other primates: it is 98.8% similar to the genome
    of chimpanzees, 98.4% similar to the genome of gorillas, and 97% similar to the
    genome of orangutans, for instance. Proteins, which are conserved by evolution,
    are even more similar on average. Our biology is also very similar, and when a
    mutation in a human protein is lethal or causes a serious genetic disease, the
    same mutation in the corresponding primate protein is likely to also be harmful.
    Conversely, protein variants that are observed in healthy primates are likely
    to be benign in humans as well. Therefore, the more primate genomes we can access,
    the more information we can gather about the human genome: we can compile a list
    of protein variants that are frequently observed in primates and deduce that these
    variants are likely benign in humans. Hence, the search for mutations that confer
    serious genetic disease should start from mutations not on this list.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Such a list of variants in primate proteins can never be enough to classify
    human mutations as benign or pathogenic. Simply put, there will be too many benign
    human mutations that have not had the opportunity to appear on the list of variants
    observed in primates. However, this list can be utilized in a more productive
    way: by observing the patterns within protein sequences and structures that tend
    to tolerate variants, and the patterns that tend not to tolerate variants. By
    learning to differentiate between these two classes of protein positions, we can
    gain the ability to annotate variants in proteins as likely benign and likely
    pathogenic.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Illumina AI lab headed by Kyle Farh, which developed the SpliceAI method,
    adopted this approach to annotate variants in human proteins (Gao et al. 2023).
    Initially, in collaboration with others, they collected primate blood samples
    and sequenced the genomes of as many primates as they could access, including
    809 individuals from 233 distinct primate species. This sequencing effort is an
    important conservation initiative: some primate species are endangered, and preserving
    the wealth of genetic information in these species is crucial for basic science
    as well as for informing human genetics.'
  prefs: []
  type: TYPE_NORMAL
- en: The team identified a catalog of 4.3 million common protein variants in primates,
    with the corresponding protein also being present in humans. Then, they constructed
    a transformer that learns to distinguish between benign and pathogenic variants
    in human proteins. This was accomplished by learning the patterns of protein positions
    where primate variants tend to be present, in contrast to protein positions where
    primate variants tend to be absent. The transformer, named PrimateAI-3D, is a
    new version of a previous deep learning tool, PrimateAI (Sundaram et al. 2018),
    developed by the same laboratory. PrimateAI-3D utilizes both protein sequence
    data, as well as protein 3D models that are either experimentally reconstructed
    or computationally predicted by tools like AlphaFold and HHpred, voxelized at
    2 Angstrom resolution (Figure 13).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8bdc4ef3999b70254f9b71cd8c429913.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 13\. Architecture of PrimateAI-3D.** Human protein structures are
    voxelized, and together with multiple sequence alignments are passed as input
    to a 3D convolutional neural network that predicts pathogenicity of all possible
    point mutations of a target residue. The network is trained using a loss function
    with three components: (1) a language model predicting a missing human or primate
    amino acid using the surrounding multiple alignment as input; (2) a 3D convolutional
    “fill-in-the-blank” model predicting a missing amino acid in the 3D structure;
    a language model score trained on classifying between observed variants and random
    variants with matching statistical properties. Figure created by Tobias Hemp and
    included with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: In the ClinVar data set of human-annotated variants and their effects, PrimateAI-3D
    achieved 87.3% recall and 80.2% precision, with an AUC of 0.843, which was best
    across state-of-the-art methods, even though unlike other methods, it was not
    trained on ClinVar. Moreover, examining corrections to ClinVar across its versions
    hints to some proportion of the variants where PrimateAI-3D and ClinVar disagree,
    could be correctly called by PrimateAI-3D.
  prefs: []
  type: TYPE_NORMAL
- en: 'PrimateAI-3D can be applied to diagnosis of rare disease, where it can prioritize
    variants that are likely deleterious, and filter out likely benign variants. Another
    application is the discovery of genes associated with complex diseases: in a cohort
    of patients of a given disease, one can look for variants that are likely deleterious
    according to PrimateAI-3D, and then look for an abundance of such variants within
    a specific gene across the cohort. Genes that exhibit this pattern of being hit
    by many likely deleterious variants in patients of a given disease, are said to
    have a genetic “burden” that is a signal of playing a role in the disease. Gao
    and colleagues from the PrimateAI-3D team studied several genetic diseases with
    this methodology and discovered many genes previously not known to be associated
    with these diseases. Using PrimateAI-3D, Fiziev et al (2023) developed improved
    rare variant polygenic risk score (PRS) models to identify individuals at high
    disease risk. They also integrated PrimateAI-3D into rare variant burden tests
    within UK Biobank and identified promising novel drug target candidates.'
  prefs: []
  type: TYPE_NORMAL
- en: Modeling gene regulation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As outlined earlier, the intricate process of gene regulation encompasses many
    interacting molecular components: the DNA chromatin structure, the chemical alterations
    within histones that DNA wraps around, the attachment of transcription factors
    to promoters and enhancers, the establishment of 3D DNA structure involving promoters,
    enhancers, bound transcription factors, and the recruitment of RNA polymerase.
    Theoretically, the precise DNA sequence in the vicinity of a gene carries all
    the information needed for this machinery to be triggered at the correct time,
    in the right amount, and in the appropriate cell type. In practice, predicting
    gene expression from the DNA sequence alone is a formidable task. Yet, language
    models have recently achieved significant progress in this area.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data generation informative of gene regulation.** Over the past two decades,
    genomic researchers have undertaken monumental efforts to produce the appropriate
    types of large-scale molecular data for understanding gene regulation. Hundreds
    of different assays have been developed that inform various aspects of the central
    dogma, too numerous to detail here. Here are some examples of the information
    obtained, always related to a human cell line or tissue type (the former often
    being immortalized cell lines, and the latter often sourced from deceased donors):
    (1) Identifying the precise locations across the entire genome that have open
    chromatin and those that have tightly packed chromatin. Two relevant assays for
    this are DNAse-seq and ATAC-seq. (2) Pinpointing all locations in the genome where
    a specific transcription factor is bound. (3) Identifying all locations in the
    genome where a specific histone chemical modification has occurred. (4) Determining
    the level of mRNA available for a given gene, i.e., the expression level of a
    particular gene. This type of data has been obtained for hundreds of human and
    mouse cell lines from numerous individuals. In total, several thousand such experiments
    have already been collected under multi-year international projects like ENCODE,
    modENCODE, Roadmap Epigenomics, Human Cell Atlas, and others. Each experiment,
    in turn, has tens to hundreds of thousands of data points across the entire human
    or model organism genome.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A lineage of language models, culminating in the transformer-based Enformer
    tool (Avsek et al. 2021), have been developed to accept the DNA sequence near
    a gene as input and output the cell type-specific expression level of this gene
    for any gene in the genome. Enformer is trained on the following task: given a
    genome region of 100,000 nucleotides and a specific cell type, it is trained to
    predict each of the available types of experimental data for this region, including
    the status of open or packed chromatin, the present histone modifications, the
    specific bound transcription factors, and the level of gene expression. A language
    model is ideal for this task: instead of masked language modeling, Enformer is
    trained in a supervised way, predicting all the tracks simultaneously from DNA
    sequence. By incorporating attention mechanisms, it can efficiently collate information
    from distant regions (up to 100,000 nucleotides away) to predict the status of
    a given location. In effect, Enformer learns all the intricate correlations between
    these diverse molecular entities.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f46e614156fc8f1636838d813414eb2a.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 14\. Predictions of Enformer and an earlier system, Basenji2, compared
    to experimental results.** Image included with permission from corresponding author,
    Ziga Avsec.'
  prefs: []
  type: TYPE_NORMAL
- en: Enformer performs reasonably well in predicting gene expression from sequence
    alone. If we measure gene expression across all genes in the same cell line using
    a specific experimental assay (for instance, the CAGE assay), two replicates of
    the same experiment typically correlate at an average of 0.94\. A computational
    method performing at this level could arguably reduce the need for collecting
    experimental data. Enformer doesn’t quite achieve this yet, correlating at a level
    of 0.85 with experimental data, which is about three times the error compared
    to two experimental replicates. However, this performance is expected to improve
    as more data are incorporated and enhancements are made to the model. Notably,
    Enformer can predict the changes in gene expression caused by mutations present
    in different individuals, as well as by mutations artificially introduced through
    CRISPR experiments. However, it still has its limitations, such as performing
    poorly in predicting the effects of distal enhancers — enhancers that are far
    from the gene start — (Karollus et al. 2023) and to correctly determine the direction
    of the effect of personal variants in gene expression (Sasse et al. 2023). Such
    shortcomings are likely due to insufficient training data. With data generation
    proceeding at an accelerated pace, it is not unreasonable to anticipate that in
    the foreseeable future we will have LLMs capable of predicting gene expression
    from sequence alone with experimental-level accuracy, and consequently models
    that accurately and comprehensively depict the complex molecular mechanisms involved
    in the central dogma of molecular biology.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed above, DNA within cells is arranged in complex, hierarchical 3D
    chromatin structure, which plays a role in gene regulation because only genes
    within open chromatin are expressed. Orca (Zhou 2022) is a recent language model,
    based on a convolutional encoder-decoder architecture, that predicts 3D genome
    structure from proximity data provided by Hi-C experiments. Those are datasets
    across the entire genomes of a cell line or tissue sample, in which pairs of genomic
    positions that are close to each other are revealed as DNA fragments that glue
    a piece of DNA from each region. The Orca model is a hierarchical multi-level
    convolutional encoder, and a multilevel decoder, which predict DNA structure at
    9 levels of resolution, from 4kb (kilo base pairs) to 1024kb, for input DNA sequences
    that are as long as the longest human chromosome.
  prefs: []
  type: TYPE_NORMAL
- en: Foundation Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Foundation models are large deep learning architectures, such as the transformer-based
    GPT models by OpenAI, that encode a vast amount of knowledge from diverse sources.
    Researchers and practitioners can fine-tune these pre-trained models for specific
    tasks, resulting in high-performance systems for a wide range of downstream applications.
    Several foundation models have begun to emerge in molecular biology. Here, we
    will briefly introduce two such models that just appeared as preprints in biorXiv.
    *(Because the papers have not been peer reviewed yet, we refrain from reporting
    on their performance compared to other state-of-the-art methods.)*
  prefs: []
  type: TYPE_NORMAL
- en: '**scGPT** is a foundation model designed for single-cell transcriptomics, chromatin
    accessibility, and protein abundance. This model is trained on single-cell data
    from 10 million human cells. Each cell contains expression values for a fraction
    of the approximately 20,000 human genes. The model learns embeddings of this large
    cell × gene matrix, which provide insights into the underlying cellular states
    and active biological pathways. The authors innovatively adapted the GPT methodology
    to this vastly different setting (Figure 15). Specifically, the ordering of genes
    in the genome, unlike the ordering of words in a sentence, is not as meaningful.
    Therefore, while GPT models are trained to predict the next word, the concept
    of the “next gene” is unclear in single-cell data. The authors solve this problem
    by training the model to generate data based on a gene prompt (a collection of
    known gene values) and a cell prompt. Starting from the known genes, the model
    predicts the remaining genes along with their confidence values. For K iterations,
    it divides those into K bins, and the top 1/K most confident genes are fixed as
    known genes for the next iteration. Once trained, scGPT is fine-tuned for numerous
    downstream tasks: batch correction, cell annotation (where the ground truth is
    annotated collections of different cell types), perturbation prediction (predicting
    the cell state after a given set of genes are experimentally perturbed), multiomics
    (where each layer, transcriptome, chromatin, proteome, is treated as a different
    language), prediction of biological pathways, and more.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/933e9001525047abf8f5bb5758517072.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 15\. Overview of scGPT.** A. Workflow of scGPT. The model is trained
    on a large number of cells from cell atlas, and is then fine tuned for downstream
    applications such as clustering, batch correction, cell annotation, perturbation
    prediction and gene network inference. B. Input embeddings. There are gene tokens,
    gene expression values, and condition tokens. C. The transformer layer. Image
    provided by Bo Wang.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Nucleotide Transformer** is a foundational model that focuses on raw DNA
    sequences. These sequences are tokenized into words of six characters each (k-mers
    of length 6) and trained using the BERT methodology. The training data consists
    of the reference human genome, 3200 additional diverse human genomes to capture
    variations across human genomics, and the genomes of 850 other species. The Nucleotide
    Transformer is then applied to 18 downstream tasks that encompass many of the
    previously discussed ones: promoter prediction, splice site donor and acceptor
    prediction, histone modifications, and more. Predictions are made either through
    probing, wherein embeddings at different layers are used as features for simple
    classifiers (such as logistic regression or perceptrons), or through light, computationally
    inexpensive fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: Looking Forward
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deciphering the biomolecular code that connects our genomes to the intricate
    biomolecular pathways in our body’s various cells, and subsequently to our physiology
    in combination with environmental interactions, doesn’t require AGI. While there
    are numerous AI tasks that may or may not be on the horizon, I argue that understanding
    molecular biology and linking it to human health isn’t one of them. LLMs are already
    proving adequate for this general aspiration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some tasks that we are not asking the AI to do. We aren’t asking it
    to generate new content; rather, we’re asking it to learn the complex statistical
    properties of existing biological systems. We aren’t requesting it to navigate
    intricate environments in a goal-oriented manner, maintain an internal state,
    form goals and subgoals, or learn through interaction with the environment. We
    aren’t asking it to solve mathematical problems or to develop deep counterfactual
    reasoning. We do, however, expect it to learn one-step causality relationships:
    if a certain mutation occurs, a specific gene malfunctions. If this gene is under-expressed,
    other genes in the cascade increase or decrease. Through simple one-step causal
    relationships, which can be learned from triangulating between correlations across
    modalities such as DNA variation, protein abundance and phenotype (a technique
    known as Mendelian randomization) and large-scale perturbation experiments that
    are becoming increasingly common, LLMs will effectively model cellular states.
    This connection extends from the genome at one end to the phenotype at the other.'
  prefs: []
  type: TYPE_NORMAL
- en: In summary, today’s LLMs are sufficiently advanced to model molecular biology.
    Further methodological improvements are always welcome. However, the barrier is
    no longer deep learning methodology; the more significant gatekeeper is data.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, data is becoming both cheaper and richer. Advances in DNA sequencing
    technology have reduced the cost of sequencing a human genome from $3Bn billion
    for the first genome, to roughly $1000 a few years back, and now to as low as
    $200 today. The same cost reductions apply to all molecular assays that use DNA
    sequencing as their primary readout. This includes assays for quantifying gene
    expression, chromatin structure, histone modifications, transcription factor binding,
    and hundreds of other ingenious assays developed over the past 10–20 years. Further
    innovations in single-cell technologies, as well as in proteomics, metabolomics,
    lipidomics, and other -omic assays, allow for increasingly detailed and efficient
    measurements of the various molecular layers between DNA and human physiology.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f332e5e1d608fd41a45ff7298759190f.png)![](../Images/32e26abcc26d7824c5d9f8bbb431d45c.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 16\. UK Biobank.** The UK Biobank is a large-scale biomedical database
    and research resource, containing in-depth genetic and health information from
    around 500,000 UK volunteers. The participants were all between the ages of 40–69
    years when they were recruited from 2006–2010\. The data collected includes blood,
    urine and saliva samples, detailed information about the participants’ backgrounds,
    lifestyle and health, and subsequent medical histories accessed through health
    records. For a subset of participants, imaging data (brain, heart, abdomen, bones
    and joints) have also been collected. The exomes of 470,000 individuals were released
    in June 2022, and the entire genomes of all individuals are coming up by the end
    of 2023\. Images provided by UK Biobank and included with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: So, how can all this be put together? A key type of data initiative is one that
    brings together a large group of volunteer participants for deep exploration of
    their -omic data, phenotypes, and health records. A leading example of this is
    the **UK Biobank Project (UKB)**, a large-scale biobank, biomedical database and
    research resource containing comprehensive genetic and health information from
    half a million UK participants (Figure 16). Participant biosamples have been collected
    with broad consent, and a wealth of data is continuously being generated. The
    exomes (protein-coding parts of the genome) of almost all participants have been
    released, with whole genomes to follow. In addition, various types of data are
    available including COVID-19 antibody data, metabolomic, telomere, imaging, genotype,
    clinical measurements, primary care, pain questionnaires, and more. Additional
    data types are continuously added. UKB data are available to anyone for research
    purposes. All Of Us is a similar initiative in the US, which to date has sequenced
    the genomes of 250,000 participants. FinnGen (Finnland Genomics) aims to create
    a similar biobank of 500,000 Finnish participants, which is incredibly valuable
    because genetic studies turn out to be much easier in a cohort that is genetically
    more homogeneous. deCODE Genetics leads a similar effort in Iceland, with more
    than two-thirds of the adult population in Iceland participating in the effort.
    Additional cohorts of sequenced participants exist, including millions of exomes
    sequenced by Regeneron Pharmaceuticals (a private initiative), and many national
    initiatives worldwide.
  prefs: []
  type: TYPE_NORMAL
- en: Cancer in particular is a disease of the genome, and many companies are building
    a wealth of genomic information on cancer patients and cancer samples, and additional
    clinical information. Covering this field is beyond the scope, but it is worth
    mentioning Tempus, an AI-based precision medicine company with a large and growing
    library of clinical and molecular data on cancer, Foundation Medicine, a molecular
    information company that offers comprehensive genomic profiling assays to identify
    the molecular alterations in a patient’s cancer and match them with relevant targeted
    therapies, immunotherapies, and clinical trials, and GRAIL and Guardant Helth,
    two pioneering diagnostic companies that focus on early tumor detection from “liquid
    biopsies” or analysis of the genomic content of patient blood samples, which often
    contain molecular shedding of cancer cells. Each of these companies has data on
    large and growing cohorts of patients.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to these cohort initiatives, there are numerous other large-scale
    data initiatives. Notably, the Human Cell Atlas project has already produced gene
    expression data for 42 million human cells from 6,300 donor individuals. The ENCODE
    Project, a vast functional genomic dataset on hundreds of human cell lines and
    various molecular quantities, has generated data on gene expression, chromatin
    accessibility, transcription factor binding, histone marks, DNA methylation, and
    more.
  prefs: []
  type: TYPE_NORMAL
- en: 'LLMs are perfectly suited to integrate these data. Looking to the future, we
    could envision a mammoth LLM integrating across all such datasets. So, what might
    the architecture and training of such a model look like? Let’s engage in a thought
    experiment and try to piece it together:'
  prefs: []
  type: TYPE_NORMAL
- en: Genes in the genome, including important variants like different isoforms of
    the resulting proteins, are tokenized.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different types of cells and tissues are tokenized.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Human phenotypes, such as disease states, clinical indications, and adherence
    to drug regimens, are also tokenized.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DNA sequences are tokenized at a fixed-length nucleotide level.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Positional information in the genome connects genes with nucleotide content.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Protein sequences are tokenized using the amino acid alphabet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data from the Human Cell Atlas and other single-cell datasets train the LLM
    in an autoregressive manner akin to GPT, or with masked language modeling akin
    to BERT, highlighting cell-type specific and cell-state specific gene pathways.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ENCODE and similar data teach the LLM to associate different molecular information
    layers like raw DNA sequence and its variants, gene expression, methylation, histone
    modifications, chromatin accessibility, etc., in a cell-type specific manner.
    Each layer is a distinct “language,” with varying richness and vocabulary, providing
    unique information. The LLM learns to translate between these languages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Projects like the PrimateAI-3D’s primate genomics initiative and other species
    sequencing efforts instruct the LLM about the potential benign or harmful effects
    of mutations in the human genome.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The entire proteomes including protein variants are enriched with protein 3D
    structural information that is either experimentally obtained or predicted by
    AlphaFold, RoseTTAfold and other structural prediction methods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Datasets from the UK Biobank (UKB) and other cohorts allow the LLM to associate
    genomic variant information and other molecular data with human health information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The LLM leverages the complete clinical records of participants to understand
    common practice and its effects, and connect this with other “languages” across
    all datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The LLM harnesses the vast existing literature on basic biology, genetics, molecular
    science, and clinical practice, including all known associations of genes and
    phenotypes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Developing such an LLM presents a significant challenge, which is of different
    kind than the GPT line of LLMs. It requires technical innovation to represent
    and integrate the various information layers, as well scaling up the number of
    tokens processed by the model. Potential applications of such an LLM are vast.
    To list a few:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Clinical diagnosis.** It could leverage all available patient information,
    including their genome, other measurements, entire clinical history, and family
    health information, aiding doctors in making precise diagnoses, even for rare
    conditions. It could be particularly useful in diagnosing rare diseases and subtyping
    cancers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Drug development.** The LLM could help identify promising gene and pathway
    targets for different clinical indications, individuals likely to respond to certain
    drugs, and those unlikely to benefit, thereby increasing the success of clinical
    trials. It could also assist in drug molecule development and drug repurposing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Basic molecular biology.** Each of the layers of molecular information will
    be connected to other layers in a manner similar to language translation, and
    the LLM will be probed for features that provide substantial predictive power.
    Whereas interpretation of deep learning models is a challenge, impressive advances
    are continuusly made by a research community that is eager to make AI interpretable.
    In the latest such advance by OpenAI4 , GPT-4 has just been deployed to explain
    the behavior of each of the neurons of GPT-2\. *(https://openai.com/research/language-models-can-explain-neurons-in-language-models)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Suggestions of additional experiments.** The model can be leveraged to identify
    the “gaps’’ in the training data, in the form of cell types, or molecular layers,
    or even individuals of specific genetic background or disease indications, which
    are predicted with poor confidence levels from other data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While developing these technologies, it’s essential to consider potential risks,
    including those related to **patient privacy** and clinical practice. Patient
    privacy remains a significant concern. This is especially true for LLMs, because
    depending on the capacity of the model, in principle the data of participants
    that were used to train the model is retrievable through a prompt that includes
    part of that data or other information that hones in to a specific patient. Therefore,
    it is especially important when training LLMs with participant data to have proper
    informed consent for the intended use of and access to these models.
  prefs: []
  type: TYPE_NORMAL
- en: However, many individuals, exemplified by the participants in the UK Biobank
    cohort, are motivated to share their data and biosamples generously, providing
    immense benefits for research and society. As for clinical practice, it’s unclear
    if LLMs can independently be used for diagnosis and treatment recommendations.
    The primary purpose of these models is not to replace, but to assist healthcare
    professionals, offering powerful tools that doctors can use to verify and audit
    medical information. To quote Isaac Kohane, “trust, but verify” (Lee, Goldberg,
    Kohane 2023).
  prefs: []
  type: TYPE_NORMAL
- en: 'So, what are the hurdles to fully implement an LLM to bridge genetics, molecular
    biology, and human health? The main obstacle is data availability. The production
    of functional genomic data, such as those from ENCODE and the Human Cell Atlas,
    needs to be accelerated. Fortunately, the cost of generating such data is rapidly
    decreasing. Simultaneously, multiomic cohort and clinical data must be produced
    and made publicly accessible. This process requires participants’ consent, taking
    into account legitimate privacy concerns. However, alongside the inalienable right
    to privacy, there’s an equally important right to participant data transparency:
    many people *want* to contribute by sharing their data. This is especially true
    for patients of rare genetic diseases and cancer, who want to help other patients
    by contributing to the study of the disease and development of treatments. The
    success of the UK Biobank is a testament to participants’ generosity in data sharing,
    aiming to make a positive impact on human health.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Molecular biology is not a set of neat concepts and clear principles, but a
    collection of trillions of little facts assembled over eons of trial and error.
    Human biologists excel in storytelling, putting these facts into descriptions
    and stories that help with intuition and experimental planning. However, making
    biology into a computational science requires a combination of massive data acquisition
    and computational models of the right capacity to distill the trillions of biological
    facts from data. With LLMs and the accelerating pace of data acquisition, we are
    indeed a few years away from having accurate in silico predictive models of the
    primary biomolecular information highway, to connect our DNA, cellular biology,
    and health. We can reasonably expect that over the next 5-10 years a wealth of
    biomedical diagnostic, drug discovery, and health span companies and initiatives
    will bring these models to application in human health and medicine, with enormous
    impact. We will also likely witness the development of open foundation models
    that integrate across data spanning from genomes all the way to medical information.
    Such models will vastly accelerate research and innovation, and foster precision
    medicine.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I thank Eric Schadt and Bo Wang for numerous suggestions and edits to the document.
    I thank Anshul Kundaje, Bo Wang and Kyle Farh for providing thoughts, comments
    and figures. I thank Lukas Kuderna for creating the Primate Phylogeny figure for
    this manuscript. I am an employee of Seer, Inc, however all opinions expressed
    here are my own.
  prefs: []
  type: TYPE_NORMAL
- en: '**References**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Avsek Z et al. Effective gene expression prediction from sequence by integrating
    long-range interactions. Nature Methods 2021.
  prefs: []
  type: TYPE_NORMAL
- en: Baek M et al. Accurate prediction of protein structures and interactions using
    a three-track neural network. Science 2021.
  prefs: []
  type: TYPE_NORMAL
- en: 'Baek M et al. Efficient and accurate prediction of protein structure using
    RoseTTAFold2\. biorXiv doi: [https://doi.org/10.1101/2023.05.24.542179](https://doi.org/10.1101/2023.05.24.542179),
    2023.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bubeck S et al. Sparks of Artificial General Intelligence: Early experiments
    with GPT-4\. [arXiv:2303.12712](https://arxiv.org/abs/2303.12712), 2023.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cui et al. scGPT: Towards Building a Foundation Model for Single-Cell Multi-omics
    Using Generative AI. biorXiv [https://doi.org/10.1101/2023.04.30.538439](https://doi.org/10.1101/2023.04.30.538439),
    2023.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dalla-Torre H et al. The Nucleotide Transformer: Building and Evaluating Robust
    Foundation Models for Human Genomics. biorXiv [https://doi.org/10.1101/2023.01.11.523679](https://doi.org/10.1101/2023.01.11.523679),
    2023.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Devlin J et al. BERT: Pre-training of Deep Bidirectional Transformers for Language
    Understanding. [arXiv:1810.04805](https://arxiv.org/abs/1810.04805), 2018.'
  prefs: []
  type: TYPE_NORMAL
- en: Fiziev P et al. Rare penetrant mutations confer severe risk of common diseases.
    Science 2023.
  prefs: []
  type: TYPE_NORMAL
- en: Gao et al. The landscape of tolerated genetic variation in humans and primates.
    Science 2023.
  prefs: []
  type: TYPE_NORMAL
- en: Jaganathan et al. Predicting splicing from primary sequence with deep learning.
    Cell 2019.
  prefs: []
  type: TYPE_NORMAL
- en: Jumper, J., Evans, R., Pritzel, A. *et al.* Highly accurate protein structure
    prediction with AlphaFold. *Nature* **596**, 583–589, 2021.
  prefs: []
  type: TYPE_NORMAL
- en: Karollus et al. Current sequence-based models capture gene expression determinants
    in promoters but mostly ignore distal enhancers. Genome Biology 2023.
  prefs: []
  type: TYPE_NORMAL
- en: Kong et al. Rate of de novo mutations and the importance of father’s age to
    disease risk. Nature 2012.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lee P, Goldberg C, Kohane I. The AI Revolution in Medicine: GPT-4 and Beyond.
    Pearson, 2023.'
  prefs: []
  type: TYPE_NORMAL
- en: Lin Z et al. Evolutionary-scale prediction of atomic-level protein structure
    with a language model. Science 2023.
  prefs: []
  type: TYPE_NORMAL
- en: Lyayuga Lisanza S et al. Joint generation of protein sequence and structure
    with RoseTTAFold sequence space diffusion. biorXiv [https://doi.org/10.1101/2023.05.08.539766](https://doi.org/10.1101/2023.05.08.539766),
    2023.
  prefs: []
  type: TYPE_NORMAL
- en: Sasse et al. How far are we from personalized gene expression prediction using
    sequence-to-expression deep neural networks? biorXiv [https://doi.org/10.1101/2023.03.16.532969](https://doi.org/10.1101/2023.03.16.532969),
    2023.
  prefs: []
  type: TYPE_NORMAL
- en: Sundaram et al. Predicting the clinical impact of human mutation with deep neural
    networks. Nature Genetics 2018.
  prefs: []
  type: TYPE_NORMAL
- en: 'Varadi M et al. AlphaFold Protein Structure Database: massively expanding the
    structural coverage of protein-sequence space with high-accuracy models. Nucleic
    Acids Research, 2021.'
  prefs: []
  type: TYPE_NORMAL
- en: Wang S et al. Accurate De Novo Prediction of Protein Contact Map by Ultra-Deep
    Learning Model. PLoS Computational Biology 2017.
  prefs: []
  type: TYPE_NORMAL
- en: Wolfram S. What is ChatGPT doing… and why does it work? Wolfram Media, Inc.
    2023.
  prefs: []
  type: TYPE_NORMAL
- en: Zhou J. Sequence-based modeling of three-dimensional genome architecture from
    kilobase to chromosome scale. Nature Genetics 2022.
  prefs: []
  type: TYPE_NORMAL
