- en: Utilizing PyArrow to Improve pandas and Dask Workflows
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/utilizing-pyarrow-to-improve-pandas-and-dask-workflows-2891d3d96d2b?source=collection_archive---------4-----------------------#2023-06-06](https://towardsdatascience.com/utilizing-pyarrow-to-improve-pandas-and-dask-workflows-2891d3d96d2b?source=collection_archive---------4-----------------------#2023-06-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Get the most out of PyArrow support in pandas and Dask right now*'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@patrick_hoefler?source=post_page-----2891d3d96d2b--------------------------------)[![Patrick
    Hoefler](../Images/35ca9ef1100d8c93dbadd374f0569fe1.png)](https://medium.com/@patrick_hoefler?source=post_page-----2891d3d96d2b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2891d3d96d2b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2891d3d96d2b--------------------------------)
    [Patrick Hoefler](https://medium.com/@patrick_hoefler?source=post_page-----2891d3d96d2b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F103b3417e0f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Futilizing-pyarrow-to-improve-pandas-and-dask-workflows-2891d3d96d2b&user=Patrick+Hoefler&userId=103b3417e0f5&source=post_page-103b3417e0f5----2891d3d96d2b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2891d3d96d2b--------------------------------)
    ·13 min read·Jun 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2891d3d96d2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Futilizing-pyarrow-to-improve-pandas-and-dask-workflows-2891d3d96d2b&user=Patrick+Hoefler&userId=103b3417e0f5&source=-----2891d3d96d2b---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2891d3d96d2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Futilizing-pyarrow-to-improve-pandas-and-dask-workflows-2891d3d96d2b&source=-----2891d3d96d2b---------------------bookmark_footer-----------)![](../Images/1fd9277102c552e00b0d50732a3c12cd.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: All images were created by the author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This post investigates where we can use PyArrow to improve our pandas and Dask
    workflows right now. General support for PyArrow dtypes was added with pandas
    2.0 to [pandas](https://pandas.pydata.org) and [Dask](https://www.dask.org/?utm_source=tds&utm_medium=pyarrow-in-pandas-and-dask).
    This solves a bunch of long-standing pains for users of both libraries. pandas
    users often complain to me that pandas does not support missing values in arbitrary
    dtypes or that non-standard dtypes are not very well supported. A particularly
    annoying problem for Dask users is running out of memory with large datasets.
    PyArrow backed string columns consume up to 70% less memory compared to NumPy
    object columns and thus have the potential to mitigate this problem as well as
    providing a huge performance improvement.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Support for PyArrow dtypes in pandas, and by extension Dask, is still relatively
    new. I would recommend caution when opting into the PyArrow `dtype_backend` until
    at least pandas 2.1 is released. Not every part of both APIs is optimized yet.
    You should be able to get a big improvement in certain workflows though. This
    post will go over a couple of examples where I’d recommend switching to PyArrow
    right away, because it already provides huge benefits.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Dask itself can benefit in various ways from PyArrow dtypes. We will investigate
    how PyArrow backed strings can easily mitigate the pain point of running out of
    memory on Dask clusters and how we can improve performance through utilizing PyArrow.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: I am part of the pandas core team and was heavily involved in implementing and
    improving PyArrow support in pandas. I’ve recently joined [Coiled](https://www.coiled.io/?utm_source=tds&utm_medium=pyarrow-in-pandas-and-dask)
    where I am working on Dask. One of my tasks is improving the PyArrow integration
    in Dask.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: General overview of PyArrow support
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PyArrow dtypes were initially introduced in pandas 1.5\. The implementation
    was experimental and I wouldn’t recommend using it on pandas 1.5.x. Support for
    them is still relatively new. pandas 2.0 provides a huge improvement, including
    making opting into PyArrow backed DataFrames easy. We are still working on supporting
    them properly everywhere, and thus they should be used with caution until at least
    pandas 2.1 is released. Both projects work continuously to improve support throughout
    Dask and pandas.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: We encourage users to try them out! This will help us to get a better idea of
    what is still lacking support or is not fast enough. Giving feedback helps us
    improve support and will drastically reduce the time that is necessary to create
    a smooth user experience.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Dataset
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use the taxi dataset from New York City that contains all Uber and
    Lyft rides. It has some interesting attributes like price, tips, driver pay and
    many more. The dataset can be found [here](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)
    (see [terms of service](https://www.nyc.gov/home/terms-of-use.page)) and is stored
    in parquet files. When analyzing Dask queries, we will use a publicly available
    S3 bucket to simplify our queries: `s3://coiled-datasets/uber-lyft-tlc/`. We will
    use the dataset from December 2022 for our pandas queries, since this is the maximum
    that fits comfortably into memory on my machine (24GB of RAM). We have to avoid
    stressing our RAM usage, since this might introduce side effects when analyzing
    performance.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: We will also investigate the performance of `read_csv`. We will use the *Crimes
    in Chicago* dataset that can be found [here](https://www.kaggle.com/datasets/utkarshx27/crimes-2001-to-present).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Dask cluster
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are various different options to set up a Dask cluster, see the [Dask
    documentation](https://docs.dask.org/en/stable/deploying.html?utm_source=tds&utm_medium=pyarrow-in-pandas-and-dask)
    for a non-exhaustive list of deployment options. I will use [Coiled](https://docs.coiled.io/user_guide/index.html?utm_source=tds&utm_medium=pyarrow-in-pandas-and-dask)
    to create a cluster on AWS with 30 machines through:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Coiled is connected to my AWS account. It creates the cluster within my account
    and manages all resources for me. 30 machines are enough to operate on our dataset
    comfortably. We will investigate how we can reduce the required number of workers
    to 15 through some small modifications.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: pandas `StringDtype` backed by PyArrow
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We begin with a feature that was originally introduced over 3 years ago in pandas
    1.0\. Setting the dtype in pandas or Dask to `string` returns an object with `StringDtype`.
    This feature is relatively mature and should provide a smooth user experience.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Historically, pandas represented string data through NumPy arrays with dtype
    `object`. NumPy object data is stored as an array of pointers pointing to the
    actual data in memory. This makes iterating over an array containing strings very
    slow. pandas 1.0 initially introduced said `StringDtype` that allowed easier and
    consistent operations on strings. This dtype was still backed by Python strings
    and thus, wasn’t very performant either. Rather, it provided a clear abstraction
    of string data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'pandas 1.3 finally introduced an enhancement to create an efficient string
    dtype. This datatype is backed by PyArrow arrays. [PyArrow](https://arrow.apache.org/docs/python/index.html)
    provides a data structure that enables performant and memory efficient string
    operations. Starting from that point on, users could use a string dtype that was
    contiguous in memory and thus very fast. This dtype can be requested through `string[pyarrow]`.
    Alternatively, we can request it by specifying `string` as the dtype and setting:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Since Dask builds on top of pandas, this string dtype is available here as well.
    On top of that, Dask offers a convenient option that automatically converts all
    string-data to `string[pyarrow]`.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Dask 构建在 pandas 之上，这里也可以使用这种字符串数据类型。除此之外，Dask 还提供了一个方便的选项，可以自动将所有字符串数据转换为
    `string[pyarrow]`。
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This is a convenient way of avoiding NumPy object dtype for string columns.
    Additionally, it has the advantage that it creates PyArrow arrays natively for
    I/O methods that operate with Arrow objects. On top of providing huge performance
    improvements, PyArrow strings consume significantly less memory. An average Dask
    DataFrame with PyArrow strings consumes around 33–50% of the original memory compared
    to NumPy object. This solves the biggest pain point for Dask users that is running
    out of memory when operating on large datasets. The option enables global testing
    in Dask’s test suite. This ensures that PyArrow backed strings are mature enough
    to provide a smooth user experience.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这是避免字符串列使用 NumPy 对象数据类型的方便方法。此外，它还具有创建原生 PyArrow 数组的优势，适用于处理 Arrow 对象的 I/O 方法。在提供显著性能提升的同时，PyArrow
    字符串消耗的内存显著减少。与 NumPy 对象相比，使用 PyArrow 字符串的平均 Dask DataFrame 的内存消耗约为原始内存的 33-50%。这解决了
    Dask 用户在处理大型数据集时内存不足的最大痛点。该选项启用了 Dask 测试套件中的全局测试。这确保了 PyArrow 支持的字符串足够成熟，以提供流畅的用户体验。
- en: Let’s look at a few operations that represent typical string operations. We
    will start with a couple of pandas examples before switching over to operations
    on our Dask cluster.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看几个典型的字符串操作。我们将从一些 pandas 示例开始，然后切换到在 Dask 集群上执行的操作。
- en: We will use `df.convert_dtypes` to convert our object columns to PyArrow string
    arrays. There are more efficient ways of getting PyArrow dtypes in pandas that
    we will explore later. We will use the Uber-Lyft dataset from December 2022, this
    file fits comfortably into memory on my machine.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `df.convert_dtypes` 将我们的对象列转换为 PyArrow 字符串数组。还有更高效的方法来获取 pandas 中的 PyArrow
    数据类型，我们会在后面探讨。我们将使用 2022 年 12 月的 Uber-Lyft 数据集，这个文件在我的机器上可以舒适地放入内存中。
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Our DataFrame has NumPy dtypes for all non-string columns in this example. Let’s
    start with filtering for all rides that were operated by Uber.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们的 DataFrame 对所有非字符串列都有 NumPy 数据类型。让我们开始筛选所有由 Uber 操作的乘车记录。
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This operation creates a mask with True/False values that specify whether Uber
    operated a ride. This doesn’t utilize any special string methods, but the equality
    comparison dispatches to PyArrow. Next, we will use the String accessor that is
    implemented in pandas and gives you access to all kinds of string operations on
    a per-element basis. We want to find all rides that were dispatched from a base
    starting with `"B028"`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个操作创建了一个 True/False 值的掩码，指定 Uber 是否进行了乘车。这不使用任何特殊的字符串方法，但等式比较会交给 PyArrow。接下来，我们将使用
    pandas 实现的 String 访问器，它允许你对每个元素执行各种字符串操作。我们想要找到所有从以 `"B028"` 开头的基地发出的乘车记录。
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`startswith` iterates over our array and checks whether every string starts
    with the specified substring. The advantage of PyArrow is easy to see. The data
    are contiguous in memory, which means that we can efficiently iterate over them.
    Additionally, these arrays have a second array with pointers that point to the
    first memory address of every string, which makes computing the starting sequence
    even faster.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`startswith` 会遍历我们的数组，检查每个字符串是否以指定的子字符串开头。PyArrow 的优势显而易见。数据在内存中是连续的，这意味着我们可以高效地进行遍历。此外，这些数组还具有一个第二个数组，其中包含指向每个字符串首个内存地址的指针，这使得计算起始序列更快。'
- en: Finally, we look at a `GroupBy` operation that groups over PyArrow string columns.
    The calculation of the groups can dispatch to PyArrow as well, which is more efficient
    than factorizing over a NumPy object array.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们查看一个 `GroupBy` 操作，它对 PyArrow 字符串列进行分组。分组的计算也可以交给 PyArrow，这比在 NumPy 对象数组上因子化要高效得多。
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Let’s look at how these operations stack up against DataFrames where string
    columns are represented by NumPy object dtype.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些操作与字符串列由 NumPy 对象数据类型表示的 DataFrames 的对比。
- en: '![](../Images/4194a8e1e1222a4c91881bdafb5fd7f2.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4194a8e1e1222a4c91881bdafb5fd7f2.png)'
- en: The results are more or less as we expected. The string based comparisons are
    significantly faster when performed on PyArrow strings. Most string accessors
    should provide a huge performance improvement. Another interesting observation
    is memory usage, it is reduced by roughly 50% compared to NumPy object dtype.
    We will take a closer look at this with Dask.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'Dask mirrors the pandas API and dispatches to pandas for most operations. Consequently,
    we can use the same API to access PyArrow strings. A convenient option to request
    these globally is the option mentioned above, which is what we will use here:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: One of the biggest benefits of this option during development is that it enables
    easy testing of PyArrow strings globally in Dask to make sure that everything
    works smoothly. We will utilize the Uber-Lyft dataset for our explorations. The
    dataset takes up around 240GB of memory on our cluster. Our initial cluster has
    30 machines, which is enough to perform our computations comfortably.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We persist the data in memory so that I/O performance does not influence our
    performance measurements. Our data is now available in memory, which makes access
    fast. We will perform computations that are similar to our pandas computations.
    One of the main goals is to show that the benefits from pandas will translate
    to computations in a distributed environment with Dask.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: One of the first observations is that the DataFrame with PyArrow backed string
    columns consumes only 130GB of memory, only half of what it consumed with NumPy
    object columns. We have only a few string columns in our DataFrame, which means
    that the memory savings for string columns are actually higher than around 50%
    when switching to PyArrow strings. Consequently, we will reduce the size of our
    cluster to 15 workers when performing our operations on PyArrow string columns.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We measure the performance of the mask-operation and one of the String accessors
    together through subsequent filtering of the DataFrame.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We see that we can use the same methods as in our previous example. This makes
    transitioning from pandas to Dask relatively easy.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we will again compute a `GroupBy` operation on our data. This
    is significantly harder in a distributed environment, which makes the results
    more interesting. The previous operations parallelize relatively easy onto a large
    cluster, while this is harder with `GroupBy`.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/a371858da38112b3610ddc9d4bf49499.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: We get nice improvements by factors of 2 and 3\. This is especially intriguing
    since we reduced the size of our cluster from 30 machines to 15, reducing the
    cost by 50%. Subsequently, we also reduced our computational resources by a factor
    of 2, which makes our performance improvement even more impressive. Thus, the
    performance improved by a factor of 4 and 6 respectively. We can perform the same
    computations on a smaller cluster, which saves money and is more efficient in
    general, and still get a performance boost out of it.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing, we saw that PyArrow string-columns are a huge improvement when
    comparing them to NumPy object columns in DataFrames. Switching to PyArrow strings
    is a relatively small change that might improve the performance and efficiency
    of an average workflow that depends on string data immensely. These improvements
    are equally visible in pandas and Dask!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Engine keyword in I/O methods
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will now take a look at I/O functions in pandas and Dask. Some functions
    have custom implementations, like `read_csv`, while others dispatch to another
    library, like `read_excel` to `openpyxl`. Some of these functions gained a new
    `engine` keyword that enables us to dispatch to `PyArrow`. The PyArrow parsers
    are multithreaded by default and hence, can provide a significant performance
    improvement.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This configuration will return the same results as the other engines. The only
    difference is that PyArrow is used to read the data. The same option is available
    for `read_json`. The PyArrow-engines were added to provide a faster way of reading
    data. The improved speed is only one of the advantages. The PyArrow parsers return
    the data as a [PyArrow Table](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html).
    A PyArrow Table provides built-in functionality to convert to a pandas `DataFrame`.
    Depending on the data, this might require a copy while casting to NumPy (string,
    integers with missing values, ...), which brings an unnecessary slowdown. This
    is where the PyArrow `dtype_backend` comes in. It is implemented as an `ArrowExtensionArray`
    class in pandas, which is backed by a [PyArrow ChunkedArray](https://arrow.apache.org/docs/python/generated/pyarrow.ChunkedArray.html).
    As a direct consequence, the conversion from a PyArrow Table to pandas is extremely
    cheap since it does not require any copies.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This returns a `DataFrame` that is backed by PyArrow arrays. pandas isn''t
    optimized everywhere yet, so this can give you a slowdown in follow-up operations.
    It might be worth it if the workload is particularly I/O heavy. Let''s look at
    a direct comparison:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/131457fefa9bd06c86d134bcffb68187.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: We can see that PyArrow-engine and PyArrow dtypes provide a 15x speedup compared
    to the C-engine.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: The same advantages apply to Dask. Dask wraps the pandas csv reader and hence,
    gets the same features for free.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: The comparison for Dask is a bit more complicated. Firstly, my example reads
    the data from my local machine while our Dask examples will read the data from
    a S3 bucket. Network speed will be a relevant component. Also, distributed computations
    have some overhead that we have to account for.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: We are purely looking for speed here, so we will read some timeseries data from
    a public S3 bucket.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We will execute this code-snippet for `engine="c"`, `engine="pyarrow"` and additionally
    `engine="pyarrow"` with `dtype_backend="pyarrow"`. Let's look at some performance
    comparisons. Both examples were executed with 30 machines on the cluster.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f558e50482cde8aa566482578a74e017.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f558e50482cde8aa566482578a74e017.png)'
- en: The PyArrow-engine runs around 2 times as fast as the C-engine. Both implementations
    used the same number of machines. The memory usage was reduced by 50% with the
    PyArrow `dtype_backend`. The same reduction is available if only object columns
    are converted to PyArrow strings, which gives a better experience in follow-up
    operations.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: PyArrow引擎运行速度约为C引擎的两倍。两种实现使用了相同数量的机器。使用PyArrow `dtype_backend`时内存使用量减少了50%。如果仅将对象列转换为PyArrow字符串，也可以实现相同的减少，这能在后续操作中提供更好的体验。
- en: We’ve seen that the Arrow-engines provide significant speedups over the custom
    C implementations. They don’t support all features of the custom implementations
    yet, but if your use-case is compatible with the supported options, you should
    get a significant speedup for free.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到Arrow引擎提供了显著的速度提升，超过了自定义C实现。它们尚未支持自定义实现的所有功能，但如果你的使用场景与支持的选项兼容，你应该能免费获得显著的速度提升。
- en: The case with the PyArrow `dtype_backend` is a bit more complicated. Not all
    areas of the API are optimized yet. If you spend a lot of time processing your
    data outside I/O functions, then this might not give you what you need. It will
    speed up your processing if your workflow spends a lot of time reading the data.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PyArrow `dtype_backend`的情况稍微复杂一些。并非所有API区域都经过优化。如果你在I/O函数之外花费大量时间处理数据，那么这可能无法满足你的需求。如果你的工作流程在读取数据时花费大量时间，它会加速你的处理。
- en: dtype_backend in PyArrow-native I/O readers
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyArrow原生I/O读取器中的dtype_backend
- en: Some other I/O methods have an engine keyword as well. `read_parquet` is the
    most popular example. The situation is a bit different here though. These I/O
    methods were already using the PyArrow engine by default. So the parsing is as
    efficient as possible. One other potential performance benefit is the usage of
    the `dtype_backend` keyword. Normally, PyArrow will return a PyArrow table which
    is then converted to a pandas DataFrame. The PyArrow dtypes are converted to their
    NumPy equivalent. Setting `dtype_backend="pyarrow"` avoids this conversion. This
    gives a decent performance improvement and saves a lot of memory.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其他一些I/O方法也有一个engine关键字。`read_parquet`是最常见的例子。不过这里的情况稍有不同。这些I/O方法默认已经使用PyArrow引擎。因此解析尽可能高效。另一个潜在的性能提升是使用`dtype_backend`关键字。通常，PyArrow会返回一个PyArrow表，然后转换为pandas
    DataFrame。PyArrow的数据类型会转换为其NumPy等效类型。设置`dtype_backend="pyarrow"`可以避免这种转换。这能显著提高性能并节省大量内存。
- en: Let’s look at one pandas performance comparison. We read the Uber-Lyft taxi
    data from December 2022.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看pandas性能的比较。我们从2022年12月读取了Uber-Lyft出租车数据。
- en: '[PRE15]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We read the data with and without `dtype_backend="pyarrow"`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们读取了带有和不带有`dtype_backend="pyarrow"`的数据。
- en: '![](../Images/ce5a2300466ab2ccfef7c65e92ea5079.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce5a2300466ab2ccfef7c65e92ea5079.png)'
- en: We can easily see that the most time is taken up by the conversion after the
    reading of the Parquet file was finished. The function runs 3 times as fast when
    avoiding the conversion to NumPy dtypes.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以很容易地看到，转换所花费的时间最多，读取Parquet文件后完成的转换。如果避免转换为NumPy数据类型，函数运行速度提高了三倍。
- en: Dask has a specialized implementation for `read_parquet` that has some advantages
    tailored to distributed workloads compared to the pandas implementation. The common
    denominator is that both functions dispatch to PyArrow to read the parquet file.
    Both have in common that the data are converted to NumPy dtypes after successfully
    reading the file. We are reading the whole Uber-Lyft dataset, which consumes around
    240GB of memory on our cluster.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Dask为`read_parquet`提供了一个专门的实现，相比于pandas实现，具有一些针对分布式工作负载的优势。共同点是这两个函数都调度到PyArrow来读取parquet文件。两者都共同点是数据在成功读取文件后会转换为NumPy数据类型。我们正在读取整个Uber-Lyft数据集，这在我们的集群上消耗了约240GB的内存。
- en: '[PRE16]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We read the dataset in 3 different configurations. First with the default NumPy
    dtypes, then with the PyArrow string option turned on:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在3种不同的配置下读取了数据集。首先是使用默认的NumPy数据类型，然后打开PyArrow字符串选项：
- en: '[PRE17]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'And lastly with `dtype_backend="pyarrow"`. Let''s look at what this means performance-wise:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最后是`dtype_backend="pyarrow"`。让我们看看这在性能方面意味着什么：
- en: '![](../Images/3a880dae2241fbb7eb36e94977d32ffb.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a880dae2241fbb7eb36e94977d32ffb.png)'
- en: Similar to our pandas example, we can see that converting to NumPy dtypes takes
    up a huge chunk of our runtime. The PyArrow dtypes give us a nice performance
    improvement. Both PyArrow configurations use half of the memory that the NumPy
    dtypes are using.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: PyArrow-strings are a lot more mature than the general PyArrow `dtype_backend`.
    Based on the performance chart we got, we get roughly the same performance improvement
    when using PyArrow strings and NumPy dtypes for all other dtypes. If a workflow
    does not work well enough on PyArrow dtypes yet, I'd recommend enabling PyArrow
    strings only.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have seen how we can leverage PyArrow in pandas in Dask right now. PyArrow
    backed string columns have the potential to impact most workflows in a positive
    way and provide a smooth user experience with pandas 2.0\. Dask has a convenient
    option to globally avoid NumPy object dtype when possible, which makes opting
    into PyArrow backed strings even easier. PyArrow also provides huge speedups in
    other areas where available. The PyArrow `dtype_backend` is still pretty new and
    has the potential to cut I/O times significantly right now. It is certainly worth
    exploring whether it can solve performance bottlenecks. There is a lot of work
    going on to improve support for general PyArrow dtypes with the potential to speed
    up an average workflow in the near future.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: There is a current proposal in pandas to start inferring strings as PyArrow
    backed strings by default starting from pandas 3.0\. Additionally, it includes
    many more areas where leaning more onto PyArrow makes a lot of sense (e.g. Decimals,
    structured data, …). You can read up on the proposal [here](https://github.com/pandas-dev/pandas/pull/52711).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading. Feel free to reach out in the comments to share your
    thoughts and feedback about PyArrow support in both libraries. I will write follow
    up posts focused on this topic and pandas in general. Follow me on Medium if you
    like to read more about pandas and Dask.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
