- en: 5 Helpful Extract & Load Practices for High-Quality Raw Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/5-helpful-extract-load-practices-for-high-quality-raw-data-65b9a59a8721?source=collection_archive---------8-----------------------#2023-04-04](https://towardsdatascience.com/5-helpful-extract-load-practices-for-high-quality-raw-data-65b9a59a8721?source=collection_archive---------8-----------------------#2023-04-04)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Immutable raw areas, no transformations, no flattening, and no dedups before
    finishing your excavations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://svenbalnojan.medium.com/?source=post_page-----65b9a59a8721--------------------------------)[![Sven
    Balnojan](../Images/3c8ba26bf656dec273cde0d93acf5576.png)](https://svenbalnojan.medium.com/?source=post_page-----65b9a59a8721--------------------------------)[](https://towardsdatascience.com/?source=post_page-----65b9a59a8721--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----65b9a59a8721--------------------------------)
    [Sven Balnojan](https://svenbalnojan.medium.com/?source=post_page-----65b9a59a8721--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F31ae15774b19&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F5-helpful-extract-load-practices-for-high-quality-raw-data-65b9a59a8721&user=Sven+Balnojan&userId=31ae15774b19&source=post_page-31ae15774b19----65b9a59a8721---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----65b9a59a8721--------------------------------)
    ·8 min read·Apr 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F65b9a59a8721&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F5-helpful-extract-load-practices-for-high-quality-raw-data-65b9a59a8721&user=Sven+Balnojan&userId=31ae15774b19&source=-----65b9a59a8721---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F65b9a59a8721&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F5-helpful-extract-load-practices-for-high-quality-raw-data-65b9a59a8721&source=-----65b9a59a8721---------------------bookmark_footer-----------)![](../Images/942b5c7466ce87920f0d5f97feff976f.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Excavator - photo by [Dmitriy Zub](https://unsplash.com/@dimitryzub?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/jibUsRaauLY?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText).
  prefs: []
  type: TYPE_NORMAL
- en: '*This post is an updated version of the original version of the* [*Meltano
    blog*](https://github.com/meltano/meltano)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'ELT is becoming the default choice for data architectures, yet many best practices
    focus primarily on “T”: the transformations.'
  prefs: []
  type: TYPE_NORMAL
- en: But data quality is determined for transformation and beyond in the extract
    and load phase. As the saying goes, “Garbage in, garbage out.”
  prefs: []
  type: TYPE_NORMAL
- en: Robust EL pipelines provide the foundation for delivering accurate, timely,
    and error-free data.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily we have a community full of data experts that have worked with Meltano,
    Stitch, Airbyte, Fivetran, and all the big extract and load tools on the market.
    So we asked them to provide their most crucial extract and load practices!
  prefs: []
  type: TYPE_NORMAL
- en: We distilled 5 data practices [used & loved by the community](https://meltano.slack.com/)
    that will drive up quality for all your data sets, no matter what tool you use.
  prefs: []
  type: TYPE_NORMAL
- en: '*But wait, why aren’t these “best practices”? Because we consider them to pick
    and choose. If you’re tackling a new project or don’t yet have many practices
    inside your extract and load processes, you implement all of them. If you already
    have some, complement them with the ones that make sense.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting the Stage: We need E&L practices because “copying raw data” is more
    complex than it sounds.'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The idea of ELT as a pattern sounds easy, “just copy over the source data first
    and then run your transformations over the raw data inside your own space.” However,
    " copy " and “raw data” are two words with hidden obstacles.
  prefs: []
  type: TYPE_NORMAL
- en: “Copying” sounds easy. But source data changes, and unless you know what changed,
    “copying” is more complicated than you think. Imagine a production table of 150
    million “orders” that does come with a “timestamp” but not with “modified” data.
    And yes, these exist all over the place. So, how do you know that orders got modified,
    and if so, which ones? For instance, how would you know which orders got “canceled,”
    an operation that usually takes place in the same data record and “modifies” it
    in place?
  prefs: []
  type: TYPE_NORMAL
- en: “Raw data” sounds clear. And yet the idea of extracting & loading implicitly
    means that usually, you copy between two different technical systems A and B,
    where you need to adjust the data to match system B. You ingest from REST APIs
    and put it into Snowflake or ingest from an Oracle database into Redshift. Every
    time you change systems, you will need to modify the “raw data” to adhere to the
    new system's rules. You need to do typecasting; you need to think about whether
    you want to “flatten JSON” or whether you want to add additional metadata to your
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Simply “copying raw data” will raise new questions every time you add a new
    data source or target to your list. Even if it is just a new table from the same
    production database, you’ve always been copying from.
  prefs: []
  type: TYPE_NORMAL
- en: These practices will guide you when you use a new source for ingesting data
    into your data system.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Make each EL run uniquely identifiable — timestamp everything
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We start with arguably the most important best practice: Make every bit of
    data you load into your data system identifiable and traceable back to the process
    that got it there.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Typical ways of doing this are to include metadata values that capture:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ingestion time: the timestamp indicating when the load process started.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ingestion process: a unique identifier representing the load process and its
    instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Source system: Metadata about where the data was extracted from.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/41b27b818ea3570f01f23d958c21a027.png)'
  prefs: []
  type: TYPE_IMG
- en: Picture by author.
  prefs: []
  type: TYPE_NORMAL
- en: Add any or all of these metadata to each row/entry of the data you ingest. We
    recommend you use the starting time of your ingestor as the “ingestion time” as
    it simplifies the process. The “identifier of your ingestion instance” should
    be clear. Don’t just provide the “Airflow-OracleDB-Ingester” as a process, but
    the “Airflow-OracleDB-Ingester-ID1244” where ID1244 identifies the specific run
    of ingestion.
  prefs: []
  type: TYPE_NORMAL
- en: One benefit of having the source system as metadata is that you can quickly
    debug problems in downstream dashboards and identify their source. This is also
    useful metadata for other use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you have a legacy and a new customer registration component. In that
    case, you can provide the source as a filter option inside dashboards, allowing
    users to filter for customers from just one system.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/644ef20394e1ff864642f8a183b4f584.png)'
  prefs: []
  type: TYPE_IMG
- en: Picture by author.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Deduplicate data at a level beyond the raw level
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are usually three cases of duplicate data hitting your data systems you
    will want to “deduplicate.” But no matter the case, don’t do it at the raw/landing
    level!
  prefs: []
  type: TYPE_NORMAL
- en: The first case is intentional duplicate data, where a source system contains
    something your end-users or you consider duplicated. For instance, your CRM system
    might have two entries for a certain customer that canceled and signed up again.
    If you deduplicate at the raw level, this means either merging the two or deleting
    one. Both of which will delete data that is present in the source system.
  prefs: []
  type: TYPE_NORMAL
- en: The second case is unintentional duplicate data, where the source system either
    deletes a record you still have in your data warehouse or the source system unintentionally
    produces duplicate data it will likely delete in the future. Even though this
    is an “error,” I don’t recommend deleting this data in your raw ingestion area
    but rather filter it further down the line, for instance, in the next stage of
    your modeling. Otherwise, you add logic to your ingestion that is hard to follow
    up on later.
  prefs: []
  type: TYPE_NORMAL
- en: The third case is duplication happening due to technical restrictions. It might
    be the case that your ingestion tooling prefers an “at least once delivery” strategy,
    or it might even be a bug in an ingestion process. With “at least once delivery”
    incremental load strategies, you’re ensuring to get all data rows but might duplicate
    some. Again we recommend keeping the duplicate data at the raw level and filtering
    it down at a later level.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ff199127920861992094eaa3637ff3f0.png)'
  prefs: []
  type: TYPE_IMG
- en: Picture by author.
  prefs: []
  type: TYPE_NORMAL
- en: No matter the case, don’t deduplicate on load. Load everything, and keep it
    there as it is. Do your deduplication later at the next stage.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Don’t flatten during EL, do it one stage later
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many source systems you ingest will return arrays, JSONs, or other nested objects
    with some hierarchy you want to break down for further processing. But not at
    the ingestion level. Take the raw data and dump it as it is in your data system,
    then have a process do your “flattening.”
  prefs: []
  type: TYPE_NORMAL
- en: A very typical example is JSON objects. Your source might contain a large JSON
    object, and you would like to have it processed into individual columns inside
    your Snowflake database. This practice suggests first having a raw table with
    just your metadata columns and one “JSON_blob” column containing the JSON object.
    In a second step, you can then process this data into columns.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/65ab7ad6d86159b77caaa8f6cbd89b2d.png)'
  prefs: []
  type: TYPE_IMG
- en: Picture by author.
  prefs: []
  type: TYPE_NORMAL
- en: The reason for this is that flattening involves business logic. It involves
    you knowing what properties are “always there.” If you flatten on ingestion, your
    ingestion process might break because one JSON object is NULL, or one JSON object
    doesn’t come with one expected value. It is always easier to take the already
    ingested data and rerun your flattener than to run your ingestion + flattening
    process together.
  prefs: []
  type: TYPE_NORMAL
- en: '*Additional tip: The same practice leads to avoiding type casting (if possible)
    on ingestion. We recommend doing typecasting after ingestion.*'
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Have an immutable raw level
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At some point, you will switch to incremental updates on your data. It’s time
    to remember to create an “immutable raw level,” an area where you never, I repeat,
    modify or delete data.
  prefs: []
  type: TYPE_NORMAL
- en: '“immutable raw level”: an area where you **never modify or delete data**.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Not deduplicating is one part of it (see rule 2), but there is more: In your
    immutable raw level, you do not remove records removed upstream or modify data
    modified upstream. You’re simply loading new data in; that is it.'
  prefs: []
  type: TYPE_NORMAL
- en: A great example I still painfully remember is a north star metric dashboard.
    It showed the current development of the north star metric I worked at based on
    customer behavior over the last couple of months. The dashboard and the numbers
    looked great, moving upwards. Product & management decisions were made on this
    basis. New records of the north star metric were broadcasted each week.
  prefs: []
  type: TYPE_NORMAL
- en: Then suddenly, one day, the dashboard looked different. Our north star metric
    shaved 10% of its value and 30% in a specific segment.
  prefs: []
  type: TYPE_NORMAL
- en: One huge customer left, and the records were completely wiped.
  prefs: []
  type: TYPE_NORMAL
- en: Since we were modifying our raw data, we completely messed up the north star
    metric irreversibly.
  prefs: []
  type: TYPE_NORMAL
- en: From that day on, we used [snapshots](https://docs.getdbt.com/docs/build/snapshots)
    of all the raw data that might be subject to change.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1fb22bc843ea507b419f0816c0eb7b90.png)'
  prefs: []
  type: TYPE_IMG
- en: Picture by author.
  prefs: []
  type: TYPE_NORMAL
- en: Do yourself a favor, and have your raw level immutable.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: immutable staging areas are effectively what you create with full table
    syncs if you do not delete data. Also, for GDPR and privacy concerns, you should
    make an exception here.*'
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Do not transform data on ingestion, not even slightly, unless you have to
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are good reasons to transform data on the fly on ingestion, but almost
    every case you will think of also works without. Legal and security are two good
    reasons for transforming data on the fly. For all other reasons, you should try
    first to ingest data and then run a small transformation on the ingested data.
  prefs: []
  type: TYPE_NORMAL
- en: If you opt to do an “on the fly” transformation while ingesting data, make sure
    you make it fail-proof. Try only to add data or subtract data, not modify it.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to do transformations by default, it is always better to first ingest
    it. Then you can, for instance, create a mapping table and do the join there.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e210cf087a2aa0289dc7d767e08bead3.png)'
  prefs: []
  type: TYPE_IMG
- en: Picture by author.
  prefs: []
  type: TYPE_NORMAL
- en: You can do so by using mechanisms like “[dbt seeds](https://docs.getdbt.com/docs/build/seeds)”
    or ingesting Google Sheets maintained by an external contributor.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We all know about “Garbage in, garbage out,” yet we often fail to recognize
    it in the extract and load world.
  prefs: []
  type: TYPE_NORMAL
- en: These practices focus on reducing garbage at the very beginning of our processes.
    They will help you to fix problems faster and increase your data quality in the
    long run.
  prefs: []
  type: TYPE_NORMAL
- en: Look at the graphic below if you want the short version of all the practices.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a3e3d5dd98f02bb8fda9cbb7ae21cc98.png)'
  prefs: []
  type: TYPE_IMG
- en: 5 Helpful extract and load practices, image by the author.
  prefs: []
  type: TYPE_NORMAL
