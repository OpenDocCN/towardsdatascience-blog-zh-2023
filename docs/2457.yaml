- en: How I Built A Cascading Data Pipeline Based on AWS (Part 1)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-i-built-a-cascading-data-pipeline-based-on-aws-997b212a84d2?source=collection_archive---------7-----------------------#2023-07-31](https://towardsdatascience.com/how-i-built-a-cascading-data-pipeline-based-on-aws-997b212a84d2?source=collection_archive---------7-----------------------#2023-07-31)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Automatic, scalable, and powerful
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://anzhemeng.medium.com/?source=post_page-----997b212a84d2--------------------------------)[![Memphis
    Meng](../Images/5a2b214eb5d5ab884b18224c471662c0.png)](https://anzhemeng.medium.com/?source=post_page-----997b212a84d2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----997b212a84d2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----997b212a84d2--------------------------------)
    [Memphis Meng](https://anzhemeng.medium.com/?source=post_page-----997b212a84d2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F85370dce2b14&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-built-a-cascading-data-pipeline-based-on-aws-997b212a84d2&user=Memphis+Meng&userId=85370dce2b14&source=post_page-85370dce2b14----997b212a84d2---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----997b212a84d2--------------------------------)
    ·14 min read·Jul 31, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F997b212a84d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-built-a-cascading-data-pipeline-based-on-aws-997b212a84d2&user=Memphis+Meng&userId=85370dce2b14&source=-----997b212a84d2---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F997b212a84d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-built-a-cascading-data-pipeline-based-on-aws-997b212a84d2&source=-----997b212a84d2---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: Today I’m going to share some experience of building a data engineering project
    that I always take pride in. You are going to learn the reasons behind why I used
    the tools and AWS components, and how I designed the architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4f95b8b2918bb62a2337706df55ff6e3.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '**Disclaimer**: The content of this text is inspired by my experience with
    an unnamed entity. However, certain critical commercial interests and details
    have intentionally been replaced with fictional data/codes or omitted, for the
    purpose of maintaining confidentiality and privacy. Therefore, the full and accurate
    extent of the actual commercial interests involved is reserved.'
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Knowledge of Python
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Understanding of AWS components, such as DynamoDB, Lambda serverless, SQS and
    CloudWatch
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Comfortable coding experience with [YAML](https://en.wikipedia.org/wiki/YAML)
    & [SAM CLI](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/install-sam-cli.html)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Background
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s say you are a data engineer and you need to constantly update the data
    in the warehouse. For example, you are responsible to sync up with the sales records
    of [Dunder Mifflin Paper Co.](https://dundermifflinpaper.com) on a regular basis.
    (I understand this is not a realistic scenario but have fun :) !) The data is
    sent to you via a vendor’s API and…
  prefs: []
  type: TYPE_NORMAL
