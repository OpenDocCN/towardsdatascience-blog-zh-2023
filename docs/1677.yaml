- en: The Notorious XGBoost
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 恶名昭著的XGBoost
- en: 原文：[https://towardsdatascience.com/the-notorious-xgboost-c7f7adc4c183?source=collection_archive---------3-----------------------#2023-05-20](https://towardsdatascience.com/the-notorious-xgboost-c7f7adc4c183?source=collection_archive---------3-----------------------#2023-05-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-notorious-xgboost-c7f7adc4c183?source=collection_archive---------3-----------------------#2023-05-20](https://towardsdatascience.com/the-notorious-xgboost-c7f7adc4c183?source=collection_archive---------3-----------------------#2023-05-20)
- en: Revisiting one of the most awarded machine learning algorithms
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回顾最受奖项青睐的机器学习算法之一
- en: '[](https://crodriguez1a.medium.com/?source=post_page-----c7f7adc4c183--------------------------------)[![Carlos
    Rodriguez (he/him)](../Images/f93397a05d50935e2f7eb83e79dbddc6.png)](https://crodriguez1a.medium.com/?source=post_page-----c7f7adc4c183--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c7f7adc4c183--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c7f7adc4c183--------------------------------)
    [Carlos Rodriguez (he/him)](https://crodriguez1a.medium.com/?source=post_page-----c7f7adc4c183--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://crodriguez1a.medium.com/?source=post_page-----c7f7adc4c183--------------------------------)[![卡洛斯·罗德里格斯
    (他/他)](../Images/f93397a05d50935e2f7eb83e79dbddc6.png)](https://crodriguez1a.medium.com/?source=post_page-----c7f7adc4c183--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c7f7adc4c183--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c7f7adc4c183--------------------------------)
    [卡洛斯·罗德里格斯 (他/他)](https://crodriguez1a.medium.com/?source=post_page-----c7f7adc4c183--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8b0823c53807&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-notorious-xgboost-c7f7adc4c183&user=Carlos+Rodriguez+%28he%2Fhim%29&userId=8b0823c53807&source=post_page-8b0823c53807----c7f7adc4c183---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c7f7adc4c183--------------------------------)
    ·8 min read·May 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc7f7adc4c183&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-notorious-xgboost-c7f7adc4c183&user=Carlos+Rodriguez+%28he%2Fhim%29&userId=8b0823c53807&source=-----c7f7adc4c183---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8b0823c53807&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-notorious-xgboost-c7f7adc4c183&user=Carlos+Rodriguez+%28he%2Fhim%29&userId=8b0823c53807&source=post_page-8b0823c53807----c7f7adc4c183---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c7f7adc4c183--------------------------------)
    ·8分钟阅读·2023年5月20日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc7f7adc4c183&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-notorious-xgboost-c7f7adc4c183&user=Carlos+Rodriguez+%28he%2Fhim%29&userId=8b0823c53807&source=-----c7f7adc4c183---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc7f7adc4c183&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-notorious-xgboost-c7f7adc4c183&source=-----c7f7adc4c183---------------------bookmark_footer-----------)![](../Images/ffb7cc6b025bf7a5d1087565bfee4993.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc7f7adc4c183&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-notorious-xgboost-c7f7adc4c183&source=-----c7f7adc4c183---------------------bookmark_footer-----------)![](../Images/ffb7cc6b025bf7a5d1087565bfee4993.png)'
- en: XGBoost Personified — Author + OpenJourney
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 的化身 — 作者 + OpenJourney
- en: If you’re a Data Scientist working on a supervised learning problem, you’ve
    likely seen XGBoost rise to the top of your leaderboard chart often. Its winningness
    can largely be attributed to the fact that the algorithm is notoriously adept
    at capturing elusive non-linear relationships with remarkable accuracy. Although
    it may be too early to tell whether XGBoost will survive the proliferation of
    the most sophisticated general-purpose models ever developed, XGBoost’s consistent
    reliability inspires a look back at the elegant math behind the magic.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是一名从事监督学习问题的数据科学家，你可能已经注意到XGBoost经常在你的排行榜上名列前茅。它的成功很大程度上归功于该算法在捕捉难以捉摸的非线性关系方面的卓越准确性。尽管现在还为时已晚，尚无法判断XGBoost是否会在各种最先进的通用模型涌现后幸存，但XGBoost的一贯可靠性促使我们回顾其背后的优雅数学。
- en: 'XGBoost (Extreme Gradient Boosting) is a powerful tree-based ensemble technique
    that is particularly good at accomplishing classification and regression tasks.
    It is based on the Gradient Boosting Machines (GBM) algorithm, which learns by
    combining multiple weak models (in this case, decision trees) to form a more robust
    model (Friedman, 2001). The learning process behind XGBoost can be deconstructed
    as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost（极端梯度提升）是一种强大的基于树的集成技术，特别擅长完成分类和回归任务。它基于梯度提升机（GBM）算法，通过结合多个弱模型（在这种情况下是决策树）来形成更强的模型（Friedman,
    2001）。XGBoost 的学习过程可以分解如下：
- en: Deconstruction
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分解
- en: Objective Function
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标函数
- en: The basic principle behind XGBoost is to minimize an objective function. The
    objective function, denoted by Objective(T), combines the training loss (evaluation
    of training data) and a regularization term (prevents overfitting).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 的基本原理是最小化目标函数。目标函数，表示为 Objective(T)，结合了训练损失（对训练数据的评估）和正则化项（防止过拟合）。
- en: '***Objective(T) = Loss + Regularization***'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '***Objective(T) = Loss + Regularization***'
- en: 'The objective function in XGBoost is given as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 的目标函数如下所示：
- en: '***Objective(T) = ∑l(yi, y_pred,i) + ∑Ω(f)***'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '***Objective(T) = ∑l(yi, y_pred,i) + ∑Ω(f)***'
- en: 'Where:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '- ***T*** representsthe ensemble of decision trees'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '- ***T*** 代表决策树的集成'
- en: '- ***l(y, y_pred)*** isa differentiable convex loss function that measures
    the — difference between the true output *(y)* and the predicted output *(y_pred)*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '- ***l(y, y_pred)*** 是一个可微分的凸损失函数，衡量真实输出 *(y)* 和预测输出 *(y_pred)* 之间的差异'
- en: '- ***yi*** is thetrue output, for instance *i*'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '- ***yi*** 是真实输出，例如 *i*'
- en: '- ***y_pred,i*** is thepredicted output for instance *i*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '- ***y_pred,i*** 是实例 *i* 的预测输出'
- en: '- ***Ω(f)*** is the regularization term applied to each tree *(f)* in the ensemble
    *(T)*'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '- ***Ω(f)*** 是对集成中的每棵树 *(f)* 应用的正则化项 *(T)*'
- en: Additive Training
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加法训练
- en: XGBoost learns the target function in an additive manner, creating an iterative
    ensemble of decision trees (weak learners) that gradually minimizes the objective
    function. In each iteration, a new tree is added to the ensemble, and the objective
    function is optimized.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 以加法方式学习目标函数，创建一个迭代的决策树集成（弱学习器），逐步最小化目标函数。在每次迭代中，向集成中添加一棵新树，并优化目标函数。
- en: 'To formalize this, let''s consider the following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了形式化这一点，我们考虑以下内容：
- en: '***Fm(x) = Fm-1(x) + fm(x)***'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '***Fm(x) = Fm-1(x) + fm(x)***'
- en: 'Where:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '- ***Fm(x)*** is theprediction after adding m trees'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '- ***Fm(x)*** 是添加了 m 棵树后的预测'
- en: '- ***Fm-1(x)*** is the prediction up to *m-1* trees'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '- ***Fm-1(x)*** 是预测值直到 *m-1* 棵树'
- en: '- ***fm(x)*** is thenew tree added in the *m-th* iteration'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '- ***fm(x)*** 是在 *m-th* 迭代中添加的新树'
- en: Gradient and Hessian Calculation
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度和Hessian计算
- en: To minimize the objective function, XGBoost uses gradient descent. In each iteration,
    the first and second-order derivatives (gradient and Hessian) of the loss function
    are calculated with respect to the predicted output *(y_pred)*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最小化目标函数，XGBoost 使用梯度下降。在每次迭代中，计算损失函数相对于预测输出 *(y_pred)* 的一阶和二阶导数（梯度和Hessian）。
- en: '***Gradient(g): ∇l(y, y_pred) = d(l) / dy_pred***'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '***Gradient(g): ∇l(y, y_pred) = d(l) / dy_pred***'
- en: '***Hessian(h): ∇²l(y, y_pred) = d²(l) / dy_pred²***'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '***Hessian(h): ∇²l(y, y_pred) = d²(l) / dy_pred²***'
- en: The derivatives are calculated for each instance *(i)* in the data, giving vectors
    *g* and *h* with gradient and Hessian values for each instance.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据中的每个实例 *(i)*，计算导数，得到梯度和Hessian值的向量 *g* 和 *h*。
- en: Tree Construction
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 树的构造
- en: 'In the *m-th* iteration, the best tree *(fm)* that minimizes the objective
    function is added using the calculated gradients and Hessians. XGBoost initializes
    with an empty tree, and then successively splits each leaf to minimize the following
    equation:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *m-th* 迭代中，使用计算得到的梯度和Hessian，添加最小化目标函数的最佳树 *(fm)*。XGBoost 从空树开始，然后依次分裂每个叶子以最小化以下方程：
- en: '***Gain = 1/2 * [Gl² / (Hl + λ) + Gr² / (Hr + λ) - G² / (H + λ)] - γ***'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '***Gain = 1/2 * [Gl² / (Hl + λ) + Gr² / (Hr + λ) - G² / (H + λ)] - γ***'
- en: Where,
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，
- en: '- ***Gl*** and ***Gr*** are the sums of gradients in the left and right regions
    of the split'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '- ***Gl*** 和 ***Gr*** 是分裂后左侧和右侧区域的梯度之和'
- en: '- ***Hl*** and ***Hr*** are the sums of Hessians in the left and right regions
    of the split'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '- ***Hl*** 和 ***Hr*** 是分裂后左侧和右侧区域的Hessian之和'
- en: '- ***G = Gl + Gr***, the sum of gradients for the entire node'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '- ***G = Gl + Gr***，整个节点的梯度之和'
- en: '- ***H = Hl + Hr***, the sum of Hessians for the entire node'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '- ***H = Hl + Hr***，整个节点的Hessian矩阵之和'
- en: '- ***λ***, the L2 regularization term'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '- ***λ***，L2 正则化项'
- en: '- ***γ***, the minimum loss reduction required for a split (another regularization
    term)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '- ***γ***，进行分裂所需的最小损失减少（另一个正则化项）'
- en: The Gain equation combines both loss reduction and regularization terms, which
    help prevent overfitting and make the optimal trade-off between complexity and
    predictive power.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 增益方程结合了损失减少和正则化项，这有助于防止过拟合并在复杂度和预测能力之间做出最佳权衡。
- en: Hyperparameters
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数
- en: '***learning_rate***: (eta) Controls the update step size during each iteration,
    shrinking the effect of each tree to prevent overfitting. It’s a weight factor
    for the newly added tree in the ensemble.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '***learning_rate***: (eta) 控制每次迭代时的更新步长，缩小每棵树的效果以防止过拟合。它是新添加到集成中的树的权重因子。'
- en: '***max_depth***: Maximum allowed depth of a tree. As the depth increases, the
    model becomes more complex and potentially overfits the data.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '***max_depth***: 树的最大允许深度。随着深度的增加，模型变得更加复杂，可能会过拟合数据。'
- en: '***min_child_weight***: (minimum sum of instance weight H) Minimum sum of hessian
    values for a split to occur in a tree. Increasing this value prevents overfitting
    by making the tree more conservative.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '***min_child_weight***: (最小实例权重和 H) 进行树的分裂所需的最小 Hessian 值和。增加此值可以通过使树更为保守来防止过拟合。'
- en: '***lambda***: L2 regularization term on weights, applied as a part of the Gain
    calculation. Helps control the model complexity.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '***lambda***: 权重的 L2 正则化项，作为增益计算的一部分应用。帮助控制模型复杂性。'
- en: '***gamma***: (min_split_loss) Minimum loss reduction required to partition
    a leaf node further. Controls the tree’s growth and complexity.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '***gamma***: (min_split_loss) 进行进一步分裂所需的最小损失减少。控制树的生长和复杂性。'
- en: '***subsample***: Proportion of the training set to sample for each boosting
    round. Randomly selecting a subset of the data reduces the likelihood of overfitting
    by introducing randomness into the ensemble process.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '***subsample***: 每次提升轮次中从训练集中采样的比例。随机选择数据子集通过引入随机性来减少过拟合的可能性，从而提高集成过程的鲁棒性。'
- en: '***colsample_bytree***: Proportion of the features to select for each boosting
    round. Randomly selecting columns (features) for each tree builds less correlated
    trees and prevents overfitting.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '***colsample_bytree***: 每次提升轮次中选择的特征比例。随机选择列（特征）来构建相关性较小的树，防止过拟合。'
- en: Effectively, these hyperparameters affect the tree construction and Gain calculation
    (such as *λ* and *γ*) or the process of selecting data and features for each iteration
    (like subsample and colsample_bytree). Adjusting them helps to balance model complexity
    and predictive ability, improving performance while mitigating overfitting.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这些超参数影响树的构建和增益计算（如 *λ* 和 *γ*），或者每次迭代中选择数据和特征的过程（如 subsample 和 colsample_bytree）。调整这些超参数有助于平衡模型复杂度和预测能力，提高性能，同时减轻过拟合。
- en: In short, XGBoost learns by building an ensemble of decision trees iteratively,
    minimizing an objective function composed of the training loss and regularization
    terms. It leverages gradient descent to find the optimal trees, employing first
    and second-order derivatives of the loss function. XGBoost utilizes hyperparameters
    such as maximum depth, regularization parameters, and subsampling strategies for
    features and instances to prevent overfitting and improve computational efficiency.
    It is worth noting that subsampling, in particular, introduces randomness and
    diversity to the model, reducing the chances of overfitting and speeding up the
    training process by processing fewer data points during each iteration.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，XGBoost 通过迭代地构建决策树的集成来学习，最小化由训练损失和正则化项组成的目标函数。它利用梯度下降来找到最佳树，采用损失函数的一阶和二阶导数。XGBoost
    利用超参数，如最大深度、正则化参数和特征及实例的子采样策略，以防止过拟合并提高计算效率。值得注意的是，特别是子采样，通过在每次迭代中处理较少的数据点，引入了随机性和多样性，从而减少过拟合的机会并加快训练过程。
- en: Unique Characteristics
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 独特特征
- en: 'Chen and Guestrin highlight several distinctive features that set XGBoost apart
    from other boosting algorithms and contribute to its enhanced performance (Chen
    and Guestrin, 2016). These features include:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 陈天奇和古斯特林强调了几个使 XGBoost 与其他提升算法不同的独特特征，并提升了其性能（陈天奇和古斯特林，2016）。这些特征包括：
- en: Sparse Awareness
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 稀疏感知
- en: XGBoost is designed to handle sparse data effectively, common in real-world
    datasets containing missing or zero values. XGBoost uses a sparsity-aware algorithm
    to find optimal splits for data points with missing values, improving its performance
    on sparse data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost旨在有效处理稀疏数据，这在包含缺失值或零值的现实世界数据集中很常见。XGBoost使用一种考虑稀疏性的算法来为缺失值的数据点找到最佳分裂，从而提高了对稀疏数据的性能。
- en: Specifically, XGBoost employs a default (missing value) direction during tree
    construction. When a feature value is missing, the algorithm automatically chooses
    the direction that yields the highest Gain and does not make an explicit split
    on the missing value. This sparse-aware approach makes XGBoost efficient and reduces
    the amount of required information for tree construction.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，XGBoost在树构建过程中采用了默认（缺失值）方向。当特征值缺失时，算法会自动选择产生最高增益的方向，而不会对缺失值进行显式分裂。这种考虑稀疏性的方式使XGBoost高效，并减少了树构建所需的信息量。
- en: 'Regularized Boosting:'
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化提升：
- en: As discussed, XGBoost incorporates regularization terms (L1 and L2) in the tree
    construction process, which helps control the complexity of the model and reduces
    overfitting. This is a key difference from the traditional GBM, which lacks regularization
    components.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，XGBoost在树构建过程中引入了正则化项（L1和L2），这有助于控制模型的复杂性并减少过拟合。这与缺乏正则化组件的传统GBM有一个关键区别。
- en: 'Column Block (Cache-aware) and Parallel Learning:'
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 列块（缓存感知）和并行学习：
- en: XGBoost supports parallelism in the tree construction process, enabling it to
    utilize multiple processor cores for faster learning. The algorithm sorts the
    data by column and stores it in compressed form in column blocks. XGBoost ensures
    efficient memory access during tree construction by using cache-aware algorithms
    to prefetch column blocks, making it suitable for handling large datasets.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost支持树构建过程中的并行处理，使其能够利用多个处理器核心以加快学习速度。该算法按列排序数据，并以压缩形式存储在列块中。XGBoost通过使用缓存感知算法预取列块，确保树构建过程中的高效内存访问，使其适合处理大数据集。
- en: Assumptions, Benefits, and Drawbacks
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 假设、优势和缺点
- en: Assumptions
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 假设
- en: Simply put, XGBoost assumes that weak learners (decision trees) can be combined
    to create a more potent, robust model. It also assumes that the objective function
    is continuous, differentiable, and convex.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，XGBoost假设弱学习器（决策树）可以组合成一个更强大、更稳健的模型。它还假设目标函数是连续的、可微的和凸的。
- en: Benefits
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优势
- en: '**High performance**: XGBoost consistently achieves state-of-the-art results
    in classification and regression tasks.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**高性能**：XGBoost在分类和回归任务中始终取得最先进的结果。'
- en: '**Scalability**: XGBoost efficiently uses memory and computation resources,
    making it suitable for large-scale problems.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**可扩展性**：XGBoost高效利用内存和计算资源，使其适合大规模问题。'
- en: '**Regularization**: Built-in regularization terms help prevent overfitting.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**正则化**：内置正则化项有助于防止过拟合。'
- en: '**Sparse awareness**: XGBoost is designed to handle sparse data effectively.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**稀疏意识**：XGBoost被设计为有效处理稀疏数据。'
- en: '**Parallelism**: Supports parallel and distributed computing for faster learning.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**并行性**：支持并行和分布式计算以加快学习速度。'
- en: Drawbacks
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺点
- en: '**Computational complexity**: Despite its efficiency, XGBoost can still be
    computationally expensive, especially for large datasets or large ensembles.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算复杂性**：尽管XGBoost高效，但对于大数据集或大规模集成方法，它仍可能计算开销较大。'
- en: '**Explainability**: Whereas parametric approaches like linear regression or
    single decision trees have intrinsic interpretability, non-parametric ensembles
    like XGBoost require supplemental explanation methods like Tree SHAP to interpret
    outcomes.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**可解释性**：尽管线性回归或单一决策树等参数化方法具有固有的可解释性，但像XGBoost这样的非参数集成方法需要额外的解释方法，如Tree SHAP，以解释结果。'
- en: '**Sensitivity to hyperparameters**: The performance of XGBoost is influenced
    by its hyperparameters, and fine-tuning is often required for optimal results.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**对超参数的敏感性**：XGBoost的性能受超参数的影响，通常需要微调以获得最佳结果。'
- en: Susceptibility to Societal Bias, Trustworthiness, and Security
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对社会偏见、可信度和安全性的敏感性
- en: Like any other machine learning algorithm, XGBoost’s performance highly depends
    on the input data quality. While the algorithm may not exhibit weaknesses related
    to algorithmic bias, trustworthiness, or security vulnerabilities, these concerns
    can emerge due to biased sampling, improper application, or unfair interpretation
    of the model results.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 像其他机器学习算法一样，XGBoost的性能高度依赖于输入数据的质量。虽然算法本身可能不会表现出与算法偏见、可信度或安全漏洞相关的弱点，但这些问题可能由于偏见采样、不当应用或不公平解释模型结果而出现。
- en: Societal Bias
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 社会偏见
- en: XGBoost might inadvertently propagate or even amplify existing societal biases
    evidenced in the data. When training data reflects underrepresentation, discrimination,
    or perpetuated stereotypes, the XGBoost model will inevitably learn these patterns,
    which could lead to harmful outcomes. Ensuring representation and addressing societal
    biases encountered in the data is critical for mitigating risks related to disparate
    impact.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost可能会无意中传播或甚至放大数据中存在的社会偏见。当训练数据反映出代表性不足、歧视或延续的刻板印象时，XGBoost模型将不可避免地学习这些模式，可能导致有害的结果。确保数据中的代表性并解决社会偏见对于减少相关风险至关重要。
- en: Trustworthiness
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可信度
- en: XGBoost is an ensemble of decision trees that can result in complex models that
    are difficult to explain. This lack of transparency can make it challenging for
    stakeholders to trust the model and understand the underlying decision-making
    process. Methods like Shapley Additive Explanations (SHAP) have helped reduce
    the "black-box" problem, but explainability remains a concern (Lundberg and Lee
    2017; Rudin 2019).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost是一个决策树的集成，可能会导致复杂的模型，难以解释。这种缺乏透明度可能使利益相关者难以信任模型并理解其决策过程。像Shapley加法解释（SHAP）这样的方法已帮助减少“黑箱”问题，但解释性仍然是一个关注点（Lundberg和Lee
    2017；Rudin 2019）。
- en: Security
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安全性
- en: Machine learning models, including XGBoost, may be vulnerable to adversarial
    attacks, data poisoning, or reverse engineering, which can reveal sensitive information
    (i.e., deanonymization) or compromise the model’s performance. Ensuring the security
    of the dataset and protecting the model from malicious attacks is essential to
    maintain the integrity and robustness of the system. Additionally, tampering or
    altering the provenance of input data may lead to misleading or incorrect predictions,
    which raises questions about the model’s trustworthiness.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型，包括XGBoost，可能会受到对抗性攻击、数据中毒或逆向工程的影响，这些可能暴露敏感信息（即去匿名化）或影响模型性能。确保数据集的安全性和保护模型免受恶意攻击对于维护系统的完整性和稳健性至关重要。此外，篡改或改变输入数据的来源可能导致误导性或不正确的预测，这引发了关于模型可信度的问题。
- en: Conclusion
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: XGBoost is a powerful and versatile machine-learning algorithm that has dominated
    leaderboards thanks to its superior performance, scalability, and efficiency.
    By leveraging ensemble learning, gradient descent, and regularization techniques,
    XGBoost overcomes many limitations of traditional boosting approaches while adapting
    to handle sparse data and optimizing computing resources.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost是一个强大且多功能的机器学习算法，由于其卓越的性能、可扩展性和效率，它在排行榜上占据了主导地位。通过利用集成学习、梯度下降和正则化技术，XGBoost克服了许多传统提升方法的局限，同时适应处理稀疏数据并优化计算资源。
- en: However, it is essential to acknowledge that the potential risks associated
    with any machine learning model, including XGBoost, rely on the algorithm’s responsible
    use. Specifically, careful preprocessing of the data, enhancing transparency through
    explainability techniques, and implementing robust security measures can help
    address these challenges and ensure that XGBoost models are both practical and
    ethically sound.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，必须承认，任何机器学习模型，包括XGBoost，相关的潜在风险依赖于算法的负责任使用。具体而言，通过仔细的数据预处理、通过解释性技术增强透明度和实施稳健的安全措施，可以帮助应对这些挑战，并确保XGBoost模型在实践和伦理上都符合要求。
- en: Embracing ethical principles and best practices allows us to continue leveraging
    the power of XGBoost and other machine learning techniques while fostering a future
    where these technologies drive equitable and beneficial outcomes for everyone.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 采纳伦理原则和最佳实践可以让我们继续利用XGBoost和其他机器学习技术的力量，同时推动这些技术在未来带来公平和有益的成果。
- en: References
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '1\. Chen T, Guestrin C. XGBoost: A Scalable Tree Boosting System. In: Proceedings
    of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data
    Mining (KDD ‘16); 2016 Aug 13–17; San Francisco, CA. New York, NY: Association
    for Computing Machinery; 2016\. p. 785–794\. DOI: [https://doi.org/10.1145/2939672.2939785](https://doi.org/10.1145/2939672.2939785).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '1\. Chen T, Guestrin C. XGBoost：一种可扩展的树提升系统。发表于：第22届ACM SIGKDD国际知识发现与数据挖掘会议（KDD
    ‘16）；2016年8月13–17日；加州旧金山。纽约：计算机协会；2016年。第785–794页。DOI: [https://doi.org/10.1145/2939672.2939785](https://doi.org/10.1145/2939672.2939785)。'
- en: '2\. Friedman JH. Greedy function approximation: A gradient boosting machine.
    Ann Stat. 2001;29(5):1189–1232.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. Friedman JH. 贪婪的函数近似：一种梯度提升机。统计年鉴。2001年；29(5)：1189–1232。
- en: '3\. Lundberg SM, Lee SI. A unified approach to interpreting model predictions.
    In: Advances in Neural Information Processing Systems (NIPS 2017); 2017 Dec 4–9;
    Long Beach, CA. 2017.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. Lundberg SM, Lee SI. 统一的模型预测解释方法。发表于：神经信息处理系统进展（NIPS 2017）；2017年12月4–9日；加州长滩。2017年。
- en: 4\. Rudin C. Please stop explaining black box models for high-stakes decisions.
    arXiv:1811.10154\. 2019.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. Rudin C. 请停止解释用于高风险决策的黑箱模型。arXiv:1811.10154。2019年。
