- en: The Notorious XGBoost
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-notorious-xgboost-c7f7adc4c183?source=collection_archive---------3-----------------------#2023-05-20](https://towardsdatascience.com/the-notorious-xgboost-c7f7adc4c183?source=collection_archive---------3-----------------------#2023-05-20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Revisiting one of the most awarded machine learning algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://crodriguez1a.medium.com/?source=post_page-----c7f7adc4c183--------------------------------)[![Carlos
    Rodriguez (he/him)](../Images/f93397a05d50935e2f7eb83e79dbddc6.png)](https://crodriguez1a.medium.com/?source=post_page-----c7f7adc4c183--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c7f7adc4c183--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c7f7adc4c183--------------------------------)
    [Carlos Rodriguez (he/him)](https://crodriguez1a.medium.com/?source=post_page-----c7f7adc4c183--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8b0823c53807&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-notorious-xgboost-c7f7adc4c183&user=Carlos+Rodriguez+%28he%2Fhim%29&userId=8b0823c53807&source=post_page-8b0823c53807----c7f7adc4c183---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c7f7adc4c183--------------------------------)
    ·8 min read·May 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc7f7adc4c183&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-notorious-xgboost-c7f7adc4c183&user=Carlos+Rodriguez+%28he%2Fhim%29&userId=8b0823c53807&source=-----c7f7adc4c183---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc7f7adc4c183&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-notorious-xgboost-c7f7adc4c183&source=-----c7f7adc4c183---------------------bookmark_footer-----------)![](../Images/ffb7cc6b025bf7a5d1087565bfee4993.png)'
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost Personified — Author + OpenJourney
  prefs: []
  type: TYPE_NORMAL
- en: If you’re a Data Scientist working on a supervised learning problem, you’ve
    likely seen XGBoost rise to the top of your leaderboard chart often. Its winningness
    can largely be attributed to the fact that the algorithm is notoriously adept
    at capturing elusive non-linear relationships with remarkable accuracy. Although
    it may be too early to tell whether XGBoost will survive the proliferation of
    the most sophisticated general-purpose models ever developed, XGBoost’s consistent
    reliability inspires a look back at the elegant math behind the magic.
  prefs: []
  type: TYPE_NORMAL
- en: 'XGBoost (Extreme Gradient Boosting) is a powerful tree-based ensemble technique
    that is particularly good at accomplishing classification and regression tasks.
    It is based on the Gradient Boosting Machines (GBM) algorithm, which learns by
    combining multiple weak models (in this case, decision trees) to form a more robust
    model (Friedman, 2001). The learning process behind XGBoost can be deconstructed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Deconstruction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Objective Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The basic principle behind XGBoost is to minimize an objective function. The
    objective function, denoted by Objective(T), combines the training loss (evaluation
    of training data) and a regularization term (prevents overfitting).
  prefs: []
  type: TYPE_NORMAL
- en: '***Objective(T) = Loss + Regularization***'
  prefs: []
  type: TYPE_NORMAL
- en: 'The objective function in XGBoost is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '***Objective(T) = ∑l(yi, y_pred,i) + ∑Ω(f)***'
  prefs: []
  type: TYPE_NORMAL
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '- ***T*** representsthe ensemble of decision trees'
  prefs: []
  type: TYPE_NORMAL
- en: '- ***l(y, y_pred)*** isa differentiable convex loss function that measures
    the — difference between the true output *(y)* and the predicted output *(y_pred)*'
  prefs: []
  type: TYPE_NORMAL
- en: '- ***yi*** is thetrue output, for instance *i*'
  prefs: []
  type: TYPE_NORMAL
- en: '- ***y_pred,i*** is thepredicted output for instance *i*'
  prefs: []
  type: TYPE_NORMAL
- en: '- ***Ω(f)*** is the regularization term applied to each tree *(f)* in the ensemble
    *(T)*'
  prefs: []
  type: TYPE_NORMAL
- en: Additive Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: XGBoost learns the target function in an additive manner, creating an iterative
    ensemble of decision trees (weak learners) that gradually minimizes the objective
    function. In each iteration, a new tree is added to the ensemble, and the objective
    function is optimized.
  prefs: []
  type: TYPE_NORMAL
- en: 'To formalize this, let''s consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '***Fm(x) = Fm-1(x) + fm(x)***'
  prefs: []
  type: TYPE_NORMAL
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '- ***Fm(x)*** is theprediction after adding m trees'
  prefs: []
  type: TYPE_NORMAL
- en: '- ***Fm-1(x)*** is the prediction up to *m-1* trees'
  prefs: []
  type: TYPE_NORMAL
- en: '- ***fm(x)*** is thenew tree added in the *m-th* iteration'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient and Hessian Calculation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To minimize the objective function, XGBoost uses gradient descent. In each iteration,
    the first and second-order derivatives (gradient and Hessian) of the loss function
    are calculated with respect to the predicted output *(y_pred)*.
  prefs: []
  type: TYPE_NORMAL
- en: '***Gradient(g): ∇l(y, y_pred) = d(l) / dy_pred***'
  prefs: []
  type: TYPE_NORMAL
- en: '***Hessian(h): ∇²l(y, y_pred) = d²(l) / dy_pred²***'
  prefs: []
  type: TYPE_NORMAL
- en: The derivatives are calculated for each instance *(i)* in the data, giving vectors
    *g* and *h* with gradient and Hessian values for each instance.
  prefs: []
  type: TYPE_NORMAL
- en: Tree Construction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the *m-th* iteration, the best tree *(fm)* that minimizes the objective
    function is added using the calculated gradients and Hessians. XGBoost initializes
    with an empty tree, and then successively splits each leaf to minimize the following
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '***Gain = 1/2 * [Gl² / (Hl + λ) + Gr² / (Hr + λ) - G² / (H + λ)] - γ***'
  prefs: []
  type: TYPE_NORMAL
- en: Where,
  prefs: []
  type: TYPE_NORMAL
- en: '- ***Gl*** and ***Gr*** are the sums of gradients in the left and right regions
    of the split'
  prefs: []
  type: TYPE_NORMAL
- en: '- ***Hl*** and ***Hr*** are the sums of Hessians in the left and right regions
    of the split'
  prefs: []
  type: TYPE_NORMAL
- en: '- ***G = Gl + Gr***, the sum of gradients for the entire node'
  prefs: []
  type: TYPE_NORMAL
- en: '- ***H = Hl + Hr***, the sum of Hessians for the entire node'
  prefs: []
  type: TYPE_NORMAL
- en: '- ***λ***, the L2 regularization term'
  prefs: []
  type: TYPE_NORMAL
- en: '- ***γ***, the minimum loss reduction required for a split (another regularization
    term)'
  prefs: []
  type: TYPE_NORMAL
- en: The Gain equation combines both loss reduction and regularization terms, which
    help prevent overfitting and make the optimal trade-off between complexity and
    predictive power.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '***learning_rate***: (eta) Controls the update step size during each iteration,
    shrinking the effect of each tree to prevent overfitting. It’s a weight factor
    for the newly added tree in the ensemble.'
  prefs: []
  type: TYPE_NORMAL
- en: '***max_depth***: Maximum allowed depth of a tree. As the depth increases, the
    model becomes more complex and potentially overfits the data.'
  prefs: []
  type: TYPE_NORMAL
- en: '***min_child_weight***: (minimum sum of instance weight H) Minimum sum of hessian
    values for a split to occur in a tree. Increasing this value prevents overfitting
    by making the tree more conservative.'
  prefs: []
  type: TYPE_NORMAL
- en: '***lambda***: L2 regularization term on weights, applied as a part of the Gain
    calculation. Helps control the model complexity.'
  prefs: []
  type: TYPE_NORMAL
- en: '***gamma***: (min_split_loss) Minimum loss reduction required to partition
    a leaf node further. Controls the tree’s growth and complexity.'
  prefs: []
  type: TYPE_NORMAL
- en: '***subsample***: Proportion of the training set to sample for each boosting
    round. Randomly selecting a subset of the data reduces the likelihood of overfitting
    by introducing randomness into the ensemble process.'
  prefs: []
  type: TYPE_NORMAL
- en: '***colsample_bytree***: Proportion of the features to select for each boosting
    round. Randomly selecting columns (features) for each tree builds less correlated
    trees and prevents overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: Effectively, these hyperparameters affect the tree construction and Gain calculation
    (such as *λ* and *γ*) or the process of selecting data and features for each iteration
    (like subsample and colsample_bytree). Adjusting them helps to balance model complexity
    and predictive ability, improving performance while mitigating overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: In short, XGBoost learns by building an ensemble of decision trees iteratively,
    minimizing an objective function composed of the training loss and regularization
    terms. It leverages gradient descent to find the optimal trees, employing first
    and second-order derivatives of the loss function. XGBoost utilizes hyperparameters
    such as maximum depth, regularization parameters, and subsampling strategies for
    features and instances to prevent overfitting and improve computational efficiency.
    It is worth noting that subsampling, in particular, introduces randomness and
    diversity to the model, reducing the chances of overfitting and speeding up the
    training process by processing fewer data points during each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Unique Characteristics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Chen and Guestrin highlight several distinctive features that set XGBoost apart
    from other boosting algorithms and contribute to its enhanced performance (Chen
    and Guestrin, 2016). These features include:'
  prefs: []
  type: TYPE_NORMAL
- en: Sparse Awareness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: XGBoost is designed to handle sparse data effectively, common in real-world
    datasets containing missing or zero values. XGBoost uses a sparsity-aware algorithm
    to find optimal splits for data points with missing values, improving its performance
    on sparse data.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, XGBoost employs a default (missing value) direction during tree
    construction. When a feature value is missing, the algorithm automatically chooses
    the direction that yields the highest Gain and does not make an explicit split
    on the missing value. This sparse-aware approach makes XGBoost efficient and reduces
    the amount of required information for tree construction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regularized Boosting:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As discussed, XGBoost incorporates regularization terms (L1 and L2) in the tree
    construction process, which helps control the complexity of the model and reduces
    overfitting. This is a key difference from the traditional GBM, which lacks regularization
    components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Column Block (Cache-aware) and Parallel Learning:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: XGBoost supports parallelism in the tree construction process, enabling it to
    utilize multiple processor cores for faster learning. The algorithm sorts the
    data by column and stores it in compressed form in column blocks. XGBoost ensures
    efficient memory access during tree construction by using cache-aware algorithms
    to prefetch column blocks, making it suitable for handling large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Assumptions, Benefits, and Drawbacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Assumptions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Simply put, XGBoost assumes that weak learners (decision trees) can be combined
    to create a more potent, robust model. It also assumes that the objective function
    is continuous, differentiable, and convex.
  prefs: []
  type: TYPE_NORMAL
- en: Benefits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**High performance**: XGBoost consistently achieves state-of-the-art results
    in classification and regression tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalability**: XGBoost efficiently uses memory and computation resources,
    making it suitable for large-scale problems.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Regularization**: Built-in regularization terms help prevent overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sparse awareness**: XGBoost is designed to handle sparse data effectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parallelism**: Supports parallel and distributed computing for faster learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Drawbacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Computational complexity**: Despite its efficiency, XGBoost can still be
    computationally expensive, especially for large datasets or large ensembles.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Explainability**: Whereas parametric approaches like linear regression or
    single decision trees have intrinsic interpretability, non-parametric ensembles
    like XGBoost require supplemental explanation methods like Tree SHAP to interpret
    outcomes.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sensitivity to hyperparameters**: The performance of XGBoost is influenced
    by its hyperparameters, and fine-tuning is often required for optimal results.'
  prefs: []
  type: TYPE_NORMAL
- en: Susceptibility to Societal Bias, Trustworthiness, and Security
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like any other machine learning algorithm, XGBoost’s performance highly depends
    on the input data quality. While the algorithm may not exhibit weaknesses related
    to algorithmic bias, trustworthiness, or security vulnerabilities, these concerns
    can emerge due to biased sampling, improper application, or unfair interpretation
    of the model results.
  prefs: []
  type: TYPE_NORMAL
- en: Societal Bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: XGBoost might inadvertently propagate or even amplify existing societal biases
    evidenced in the data. When training data reflects underrepresentation, discrimination,
    or perpetuated stereotypes, the XGBoost model will inevitably learn these patterns,
    which could lead to harmful outcomes. Ensuring representation and addressing societal
    biases encountered in the data is critical for mitigating risks related to disparate
    impact.
  prefs: []
  type: TYPE_NORMAL
- en: Trustworthiness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: XGBoost is an ensemble of decision trees that can result in complex models that
    are difficult to explain. This lack of transparency can make it challenging for
    stakeholders to trust the model and understand the underlying decision-making
    process. Methods like Shapley Additive Explanations (SHAP) have helped reduce
    the "black-box" problem, but explainability remains a concern (Lundberg and Lee
    2017; Rudin 2019).
  prefs: []
  type: TYPE_NORMAL
- en: Security
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machine learning models, including XGBoost, may be vulnerable to adversarial
    attacks, data poisoning, or reverse engineering, which can reveal sensitive information
    (i.e., deanonymization) or compromise the model’s performance. Ensuring the security
    of the dataset and protecting the model from malicious attacks is essential to
    maintain the integrity and robustness of the system. Additionally, tampering or
    altering the provenance of input data may lead to misleading or incorrect predictions,
    which raises questions about the model’s trustworthiness.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: XGBoost is a powerful and versatile machine-learning algorithm that has dominated
    leaderboards thanks to its superior performance, scalability, and efficiency.
    By leveraging ensemble learning, gradient descent, and regularization techniques,
    XGBoost overcomes many limitations of traditional boosting approaches while adapting
    to handle sparse data and optimizing computing resources.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is essential to acknowledge that the potential risks associated
    with any machine learning model, including XGBoost, rely on the algorithm’s responsible
    use. Specifically, careful preprocessing of the data, enhancing transparency through
    explainability techniques, and implementing robust security measures can help
    address these challenges and ensure that XGBoost models are both practical and
    ethically sound.
  prefs: []
  type: TYPE_NORMAL
- en: Embracing ethical principles and best practices allows us to continue leveraging
    the power of XGBoost and other machine learning techniques while fostering a future
    where these technologies drive equitable and beneficial outcomes for everyone.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '1\. Chen T, Guestrin C. XGBoost: A Scalable Tree Boosting System. In: Proceedings
    of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data
    Mining (KDD ‘16); 2016 Aug 13–17; San Francisco, CA. New York, NY: Association
    for Computing Machinery; 2016\. p. 785–794\. DOI: [https://doi.org/10.1145/2939672.2939785](https://doi.org/10.1145/2939672.2939785).'
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Friedman JH. Greedy function approximation: A gradient boosting machine.
    Ann Stat. 2001;29(5):1189–1232.'
  prefs: []
  type: TYPE_NORMAL
- en: '3\. Lundberg SM, Lee SI. A unified approach to interpreting model predictions.
    In: Advances in Neural Information Processing Systems (NIPS 2017); 2017 Dec 4–9;
    Long Beach, CA. 2017.'
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Rudin C. Please stop explaining black box models for high-stakes decisions.
    arXiv:1811.10154\. 2019.
  prefs: []
  type: TYPE_NORMAL
