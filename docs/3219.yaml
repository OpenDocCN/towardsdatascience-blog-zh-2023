- en: Entropy-Regularized Reinforcement Learning Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 熵正则化的强化学习解释
- en: 原文：[https://towardsdatascience.com/entropy-regularized-reinforcement-learning-explained-2ba959c92aad?source=collection_archive---------11-----------------------#2023-10-26](https://towardsdatascience.com/entropy-regularized-reinforcement-learning-explained-2ba959c92aad?source=collection_archive---------11-----------------------#2023-10-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/entropy-regularized-reinforcement-learning-explained-2ba959c92aad?source=collection_archive---------11-----------------------#2023-10-26](https://towardsdatascience.com/entropy-regularized-reinforcement-learning-explained-2ba959c92aad?source=collection_archive---------11-----------------------#2023-10-26)
- en: Learn more reliable, robust, and transferable policies by adding entropy bonuses
    to your algorithm
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过在算法中添加熵奖励，学习更可靠、稳健和可转移的策略
- en: '[](https://wvheeswijk.medium.com/?source=post_page-----2ba959c92aad--------------------------------)[![Wouter
    van Heeswijk, PhD](../Images/9c996bccd6fdfb6d9aa8b50b93338eb9.png)](https://wvheeswijk.medium.com/?source=post_page-----2ba959c92aad--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2ba959c92aad--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2ba959c92aad--------------------------------)
    [Wouter van Heeswijk, PhD](https://wvheeswijk.medium.com/?source=post_page-----2ba959c92aad--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wvheeswijk.medium.com/?source=post_page-----2ba959c92aad--------------------------------)[![Wouter
    van Heeswijk, PhD](../Images/9c996bccd6fdfb6d9aa8b50b93338eb9.png)](https://wvheeswijk.medium.com/?source=post_page-----2ba959c92aad--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2ba959c92aad--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2ba959c92aad--------------------------------)
    [Wouter van Heeswijk, PhD](https://wvheeswijk.medium.com/?source=post_page-----2ba959c92aad--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33f45c9ab481&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-regularized-reinforcement-learning-explained-2ba959c92aad&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=post_page-33f45c9ab481----2ba959c92aad---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2ba959c92aad--------------------------------)
    ·8 min read·Oct 26, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2ba959c92aad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-regularized-reinforcement-learning-explained-2ba959c92aad&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----2ba959c92aad---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33f45c9ab481&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-regularized-reinforcement-learning-explained-2ba959c92aad&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=post_page-33f45c9ab481----2ba959c92aad---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2ba959c92aad--------------------------------)
    · 8 min read · Oct 26, 2023 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2ba959c92aad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-regularized-reinforcement-learning-explained-2ba959c92aad&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----2ba959c92aad---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2ba959c92aad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-regularized-reinforcement-learning-explained-2ba959c92aad&source=-----2ba959c92aad---------------------bookmark_footer-----------)![](../Images/2fce76a87f64dcb27147a0868dc84b72.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2ba959c92aad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fentropy-regularized-reinforcement-learning-explained-2ba959c92aad&source=-----2ba959c92aad---------------------bookmark_footer-----------)![](../Images/2fce76a87f64dcb27147a0868dc84b72.png)'
- en: Photo by [Jeremy Thomas](https://unsplash.com/@jeremythomasphoto?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由 [Jeremy Thomas](https://unsplash.com/@jeremythomasphoto?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) 提供的照片
- en: '*Entropy* is a concept associated with a state of disorder, randomness, or
    uncertainty. It can be considered as a **measure of information for random variables**.
    Traditionally, it is associated with fields such as thermodynamics, but the term
    found its way to many other domains.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*熵* 是与混乱、随机或不确定状态相关的概念。它可以被视为**随机变量的信息度量**。传统上，它与热力学等领域相关，但这一术语已扩展到许多其他领域。'
- en: 'In 1948, Claude Shannon introduced the notion of entropy in information theory.
    In this context, an event is considered to offer more information if it has a
    lower probability of happening; the **information of an event is inversely correlated
    to its probability of occurrence**. Intuitively: we learn more from rare events.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 1948年，克劳德·香农在信息理论中引入了熵的概念。在这种情况下，如果一个事件发生的概率较低，则该事件被认为提供了更多的信息；**事件的信息与其发生概率呈反比关系**。直观地说：我们从稀有事件中学到的更多。
- en: 'The notion of entropy can be formalized as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 熵的概念可以形式化如下：
- en: In Reinforcement Learning (RL), the notion of entropy has been deployed as well,
    with the purpose of encouraging exploration. In this context, entropy is a **measure
    of predictability of actions** returned by a stochastic policy.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习（RL）中，熵的概念也被应用于鼓励探索。在这种情况下，熵是**由随机策略返回的动作的可预测性度量**。
- en: Concretely, **RL takes the entropy of the policy (i.e., probability distribution
    of actions) as a bonus and embeds it as a reward component**. This article addresses
    the basic case, but entropy bonuses are an integral part of many…
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，**RL将策略的熵（即动作的概率分布）作为奖励的一部分，并将其嵌入奖励组件中**。本文讨论了基本情况，但熵奖励是许多…
