- en: From Centralized to Federated Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从集中式学习到联邦学习
- en: 原文：[https://towardsdatascience.com/from-centralized-to-federated-learning-b0074793e9f?source=collection_archive---------9-----------------------#2023-03-16](https://towardsdatascience.com/from-centralized-to-federated-learning-b0074793e9f?source=collection_archive---------9-----------------------#2023-03-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/from-centralized-to-federated-learning-b0074793e9f?source=collection_archive---------9-----------------------#2023-03-16](https://towardsdatascience.com/from-centralized-to-federated-learning-b0074793e9f?source=collection_archive---------9-----------------------#2023-03-16)
- en: A summary of dataset distribution techniques for Federated Learning on the CIFAR
    benchmark dataset
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于在CIFAR基准数据集上进行联邦学习的数据集分布技术总结
- en: '[](https://medium.com/@neged.ng?source=post_page-----b0074793e9f--------------------------------)[![Gergely
    D. Németh](../Images/156b43b2d7be061e9795fb31d21f75a9.png)](https://medium.com/@neged.ng?source=post_page-----b0074793e9f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b0074793e9f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b0074793e9f--------------------------------)
    [Gergely D. Németh](https://medium.com/@neged.ng?source=post_page-----b0074793e9f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@neged.ng?source=post_page-----b0074793e9f--------------------------------)[![Gergely
    D. Németh](../Images/156b43b2d7be061e9795fb31d21f75a9.png)](https://medium.com/@neged.ng?source=post_page-----b0074793e9f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b0074793e9f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b0074793e9f--------------------------------)
    [Gergely D. Németh](https://medium.com/@neged.ng?source=post_page-----b0074793e9f--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F71eefc6da84b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-centralized-to-federated-learning-b0074793e9f&user=Gergely+D.+N%C3%A9meth&userId=71eefc6da84b&source=post_page-71eefc6da84b----b0074793e9f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b0074793e9f--------------------------------)
    ·9 min read·Mar 16, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb0074793e9f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-centralized-to-federated-learning-b0074793e9f&user=Gergely+D.+N%C3%A9meth&userId=71eefc6da84b&source=-----b0074793e9f---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F71eefc6da84b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-centralized-to-federated-learning-b0074793e9f&user=Gergely+D.+N%C3%A9meth&userId=71eefc6da84b&source=post_page-71eefc6da84b----b0074793e9f---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b0074793e9f--------------------------------)
    ·9 min read·Mar 16, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb0074793e9f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-centralized-to-federated-learning-b0074793e9f&user=Gergely+D.+N%C3%A9meth&userId=71eefc6da84b&source=-----b0074793e9f---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb0074793e9f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-centralized-to-federated-learning-b0074793e9f&source=-----b0074793e9f---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb0074793e9f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-centralized-to-federated-learning-b0074793e9f&source=-----b0074793e9f---------------------bookmark_footer-----------)'
- en: '[**Federated Learning** (FL)](https://federated.withgoogle.com/) is a method
    to train Machine Learning (ML) models in a distributed setting [1]. The idea is
    that clients (for example hospitals) want to cooperate without sharing their private
    and sensitive data. Each client holds their private data in FL and trains an ML
    model on it. Then a central server collects and aggregates the model parameters,
    thus building a global model based on information from all the data distribution.
    Ideally, this serves as **privacy protection by design**.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[**联邦学习** (FL)](https://federated.withgoogle.com/) 是一种在分布式环境下训练机器学习（ML）模型的方法
    [1]。其理念是客户端（例如医院）希望在不共享其私密和敏感数据的情况下合作。每个客户端在FL中保留其私有数据，并在这些数据上训练ML模型。然后，中央服务器收集并聚合模型参数，从而基于所有数据分布的信息构建一个全球模型。理想情况下，这作为**设计上的隐私保护**。'
- en: A long line of research has been done to understand FL's efficiency, privacy,
    and fairness. Here we will focus on the benchmark datasets used to evaluate *horizontal
    FL methods* where the clients share the same task and data type but they have
    their individual data samples.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 已经进行了大量研究以理解FL的效率、隐私和公平性。在这里，我们将重点关注用于评估*水平FL方法*的基准数据集，其中客户端共享相同的任务和数据类型，但他们拥有各自的数据样本。
- en: If you want to know more about Federated Learning and what I work on, visit
    our [research lab website](https://ellisalicante.org/federated-learning)!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于联邦学习及我所做的工作，请访问我们的[研究实验室网站](https://ellisalicante.org/federated-learning)!
- en: '![](../Images/71456a6c58d0c80ceca1a5b605996eab.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/71456a6c58d0c80ceca1a5b605996eab.png)'
- en: Photo by [JJ Ying](https://unsplash.com/@jjying?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/8bghKxNU1j0?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[JJ Ying](https://unsplash.com/@jjying?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)拍摄，来源于[Unsplash](https://unsplash.com/photos/8bghKxNU1j0?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: 'There are three types of datasets in the literature:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中有三种类型的数据集：
- en: '**Real FL scenario**: an application where FL is a needed method. It has natural
    distributions and sensitive data. However, given the nature of FL if you want
    to keep the data locally you won’t publish the dataset online for benchmarking.
    Therefore it is hard to find a dataset of this kind. [OpenMinded behind PySyft
    tries to organize an FL community](https://github.com/OpenMined/PySyft) of universities
    and research labs to host data in a more realistic scenario. Additionally, there
    are applications where the privacy-awareness has risen recently. So there might
    be publicly available data while the demand for FL exists. One application is
    smart electricity meters [2].'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**真实的FL场景**：FL是一种需要的方法的应用场景。它具有自然分布和敏感数据。然而，考虑到FL的性质，如果你想将数据保存在本地，你不会将数据集在线发布以进行基准测试。因此，很难找到这种类型的数据集。[OpenMined背后的PySyft试图组织一个FL社区](https://github.com/OpenMined/PySyft)，由大学和研究实验室主办数据，以更现实的场景进行数据托管。此外，最近隐私意识有所提高的应用场景也存在。因此，可能会有公开的数据，而FL的需求依然存在。一个应用场景是智能电表[2]。'
- en: '**FL benchmark datasets**: these datasets are designed to serve as FL benchmarks.
    The distribution is realistic, but the sensitivity of the data is questionable
    as they are built from publicly available origins. One example is creating an
    FL dataset from Reddit posts using the users as clients and distributing it to
    one user as one partition. The [LEAF project](https://leaf.cmu.edu/) proposed
    more datasets like this [3].'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**FL基准数据集**：这些数据集设计用于作为FL基准。分布是现实的，但数据的敏感性存在疑问，因为它们是从公开来源构建的。例如，从Reddit帖子中创建FL数据集，使用用户作为客户端，并将其分发给一个用户作为一个分区。[LEAF项目](https://leaf.cmu.edu/)提出了更多类似的数据集[3]。'
- en: '**Distributing standard datasets**: there are a couple of well-known datasets
    like CIFAR and ImageNet for images as an example used as a benchmark in many Machine
    Learning works. Here FL scientists define a distribution according to their research
    questions. It makes sense to use this method if the topic is well-studied on a
    standard ML scenario and one wants to compare their FL algorithm to centralized
    SOTA. However, this artificial distribution doesn’t reveal every problem with
    the distribution skew. For example, if the clients collect images with very different
    cameras or in different lighting conditions.'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分发标准数据集**：有一些众所周知的数据集，如CIFAR和ImageNet，作为图像的例子，在许多机器学习工作中用作基准。在这里，FL科学家根据他们的研究问题定义一个分布。如果主题在标准机器学习场景中研究得很透彻，并且希望将其FL算法与集中式SOTA进行比较，那么使用这种方法是有意义的。然而，这种人工分布并没有揭示分布偏斜的所有问题。例如，客户端收集的图像可能来自不同的相机或不同的光照条件。'
- en: As the last category is not distributed by design, there are several ways past
    research works split them. In the rest of this story, I will summarise distribution
    techniques used for the CIFAR dataset in a federated scenario.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于最后一种类别不是设计上分布的，过去的研究将其分为几种方式。在接下来的内容中，我将总结在联邦场景中用于CIFAR数据集的分发技术。
- en: CIFAR dataset
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CIFAR数据集
- en: The [CIFAR-10 and CIFAR-100 datasets](https://www.cs.toronto.edu/~kriz/cifar.html)
    contain 32x32 colored images labeled to mutually exclusive classes [4]. The CIFAR-10
    has 10 classes of 6000 images and the CIFAR-100 has 100 classes of 600 images.
    They are used in many image classification tasks and one can access dozens of
    models evaluated on them, even browsing them using a [leaderboard on PapersWithCode](https://paperswithcode.com/sota/image-classification-on-cifar-100).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[CIFAR-10 和 CIFAR-100 数据集](https://www.cs.toronto.edu/~kriz/cifar.html) 包含
    32x32 的彩色图像，并标记为互斥的类别 [4]。CIFAR-10 有 10 个类别，每个类别 6000 张图像，而 CIFAR-100 有 100 个类别，每个类别
    600 张图像。它们在许多图像分类任务中使用，可以访问到基于这些数据集评估的数十个模型，甚至可以通过 [PapersWithCode 的排行榜](https://paperswithcode.com/sota/image-classification-on-cifar-100)
    浏览它们。'
- en: Data partitioning in Federated Learning
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联邦学习中的数据划分
- en: '**Uniform distribution**'
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**均匀分布**'
- en: This is considered to be **identically and independently distributed** (IID)
    data. Data points are randomly allocated to clients.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这被认为是 **同分布且独立**（IID）数据。数据点随机分配给客户端。
- en: Single (n-) class clients
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单类（n-）客户端
- en: Data points allocated for a specific client come from the same class or classes.
    It can be recognized as an extreme non-IID setting. Examples of this distribution
    are in [1,5–8]. The work first naming the setting as Federated Learning [1] uses
    200 single-class sets and gives two sets to each client making them 2-class clients.
    [5–7] use 2-class clients.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 分配给特定客户端的数据点来自相同的类别或类别。这可以被认为是极端非独立同分布（non-IID）设置。这种分布的例子见 [1,5–8]。首先将这种设置命名为联邦学习
    [1] 的工作使用了 200 个单类数据集，并给每个客户端两个数据集，使其成为 2 类客户端。[5–7] 使用 2 类客户端。
- en: '[9] builds on the hierarchical classes in CIFAR-100: clients have data points
    from one subclass in each superclass. This way in the classification task for
    superclasses has clients with samples from each (super)class, yet a distribution
    skew is simulated as the data points are from different subclasses. For example,
    one client has access to lions while the other has tiger images, the superclass
    task is to categorize both as large carnivores.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] 基于 CIFAR-100 中的层级类：客户端的数据点来自每个超级类中的一个子类。这样，在超级类的分类任务中，客户端拥有来自每个（超级）类的样本，但由于数据点来自不同的子类，因此模拟了分布偏斜。例如，一个客户端访问狮子的图像，而另一个客户端访问老虎的图像，超级类任务是将两者归类为大型食肉动物。'
- en: Dominant class clients
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主导类别客户端
- en: '[5] also uses a mixture of uniform and 2-class clients, which means half of
    the data points come from the 2 dominant classes, and the rest are uniformly selected
    from the other classes. [10] uses an 80%-20% partition 80% selected from a single
    dominant class and the rest is uniformly selected from the other classes.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] 也使用了均匀和 2 类客户端的混合，这意味着一半的数据点来自 2 个主要类别，其余的数据点均匀地从其他类别中选择。[10] 使用 80%-20%
    的划分，80% 来自单一主导类别，其余的从其他类别中均匀选择。'
- en: Dirichlet distribution
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dirichlet 分布
- en: 'To understand the [Dirichlet distribution](https://en.wikipedia.org/wiki/Dirichlet_distribution#),
    I follow the example of [this blog post](https://blog.bogatron.net/blog/2014/02/02/visualizing-dirichlet-distributions/).
    Let’s say one wants to produce a dice, with θ=(1/6,1/6,1/6,1/6,1/6,1/6) probabilities
    for each number 1–6\. However, in reality, nothing can be perfect, so each die
    will be a bit skewed. 4 a bit more likely and 3 a bit less likely for example.
    The Dirichlet distribution describes this variety with a parameter vector **α=(**α₁,α₂,..,α₆**)**.
    Larger αᵢ strengthens the weight of that number and the larger overall sum of
    the αᵢ values ensures more similar sampled probabilities (dice). Turning back
    to the dice example, to have a fair die each αᵢ should be equal, and the larger
    the α value the better manufactured the dice are. As it is a multivariate generalization
    of the [beta distribution](https://en.wikipedia.org/wiki/Beta_distribution), let’s
    display some examples of the beta distribution (Dirichlet distribution with two
    dice):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解 [Dirichlet 分布](https://en.wikipedia.org/wiki/Dirichlet_distribution#)，我参考了
    [这篇博客文章](https://blog.bogatron.net/blog/2014/02/02/visualizing-dirichlet-distributions/)。假设有人想要制作一个骰子，θ=(1/6,1/6,1/6,1/6,1/6,1/6)
    表示每个数字 1–6 的概率。然而，实际上没有什么是完美的，所以每个骰子会有些偏斜。例如，4 出现的可能性稍高，而 3 的可能性稍低。Dirichlet 分布用参数向量
    **α=(**α₁,α₂,..,α₆**)** 描述这种多样性。较大的 αᵢ 增强了该数字的权重，而 αᵢ 值的整体总和较大则确保了更相似的抽样概率（骰子）。回到骰子例子，为了让骰子公平，每个
    αᵢ 应该相等，且 α 值越大，骰子的制造越好。由于这是 [beta 分布](https://en.wikipedia.org/wiki/Beta_distribution)
    的多变量推广，让我们展示一些 beta 分布的例子（Dirichlet 分布与两个骰子）：
- en: '![](../Images/2de43c9da07b6edee8de41b34c90c01e.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2de43c9da07b6edee8de41b34c90c01e.png)'
- en: Different beta distributions (Dirichlet distribution for 2 variables) — Figure
    by the author
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的 beta 分布（2 变量的 Dirichlet 分布） — 图由作者提供
- en: I reproduced the visualization in [11], using the same α value for αᵢ each.
    This is called a **symmetric Dirichlet distribution**. We can see that as the
    α value decreases it is more likely that there will be unbalanced dice. The figures
    below show the Dirichlet distribution for different α values. Here each row represents
    a class, each column is a client and the area of the circles is proportionate
    to the probabilities.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我重现了 [11] 中的可视化，使用相同的 α 值为每个 αᵢ。这被称为 **对称 Dirichlet 分布**。我们可以看到，随着 α 值的减少，更有可能出现不平衡的骰子。下面的图显示了不同
    α 值的 Dirichlet 分布。这里每一行代表一个类别，每一列代表一个客户端，圆圈的面积与概率成正比。
- en: '![](../Images/9d7b720d17e1f23b3a3cc237b23b6215.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d7b720d17e1f23b3a3cc237b23b6215.png)'
- en: '**Distribution over classes**: Sampling 20 clients for 10 classes using different
    Dirichlet distribution α values — Figure by the author'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**类别分布**：使用不同的 Dirichlet 分布 α 值对 10 个类别的 20 个客户端进行采样 — 图由作者提供'
- en: '**Distribution over classes**: The samples for each client are drawn independently
    with class distribution following the Dirichlet method. [11, 16] use this version
    of the Dirichlet distribution.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**类别分布**：每个客户端的样本是独立抽取的，类别分布遵循 Dirichlet 方法。[11, 16] 使用这种 Dirichlet 分布版本。'
- en: '![](../Images/5abdd77d6e8846582cf563fe4c4491b2.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5abdd77d6e8846582cf563fe4c4491b2.png)'
- en: '**Distribution over classes:** normalized sum of samples by class (10) and
    by client (20) — Figure by the author'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**类别分布**：按类别（10）和客户端（20）归一化的样本总和 — 图由作者提供'
- en: Each client has a predetermined number of samples, but the classes are chosen
    randomly, thus the final total class representation will be unbalanced. In the
    clients, α→∞ is the prior (uniform) distribution while α→0 means single-class
    clients.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 每个客户端有一个预定数量的样本，但类别是随机选择的，因此最终的类别表示会不平衡。在客户端中，α→∞ 是先验（均匀）分布，而 α→0 意味着单类别客户端。
- en: '![](../Images/ea91e00df9dc8a2edbf286338e39aca6.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea91e00df9dc8a2edbf286338e39aca6.png)'
- en: '**Distribution over clients**: Sampling 20 clients for 10 classes using different
    Dirichlet distribution α values — Figure by the author'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**客户端分布**：使用不同的 Dirichlet 分布 α 值对 10 个类别的 20 个客户端进行采样 — 图由作者提供'
- en: '**Distribution over clients**: if we know the total number of samples in a
    class and the number of clients, we can distribute the samples to the clients
    class by class. This will result in clients having a different number of samples
    (which is very typical in FL), while the global class distribution is balanced.
    [12] use this variation of the Dirichlet distribution.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**客户端分布**：如果我们知道一个类别中的样本总数和客户端的数量，我们可以按类别将样本分配给客户端。这将导致客户端拥有不同数量的样本（在 FL 中非常典型），而全局类别分布则是平衡的。[12]
    使用了这种 Dirichlet 分布的变体。'
- en: '![](../Images/2e435888deb289b0978a04c2f25891d6.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e435888deb289b0978a04c2f25891d6.png)'
- en: '**Distribution over clients:** normalized sum of samples by class (10) and
    by client (20) — Figure by the author'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**客户端分布**：按类别（10）和客户端（20）归一化的样本总和 — 图由作者提供'
- en: While works like [11–16] follow and cite each other using Dirichlet distribution,
    they use the two different methods. Furthermore, the different experiments use
    different α values which can result in very different performances. [11,12] uses
    α=0.1 and [13-15] uses α=0.5, [16] gives an overview of different α values. These
    design choices lose the original principle of using the same benchmark dataset
    to evaluate algorithms.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管像 [11–16] 这样的工作相互引用使用 Dirichlet 分布，但它们使用了两种不同的方法。此外，不同的实验使用了不同的 α 值，这可能导致非常不同的性能。[11,12]
    使用 α=0.1，[13-15] 使用 α=0.5，[16] 概述了不同的 α 值。这些设计选择丧失了使用相同基准数据集来评估算法的原始原则。
- en: '**Asymmetric Dirichlet distribution:** one can use different αᵢ values to simulate
    more resourceful clients. For example, the figure below is produced using 1/*i*
    for the *i*th client. It is not represented in the literature to my knowledge,
    instead, Zipf distribution is used in [17].'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**非对称 Dirichlet 分布**：可以使用不同的 αᵢ 值来模拟更具资源的客户端。例如，下面的图是使用 1/*i* 为第 *i* 个客户端生成的。据我所知，这在文献中没有表示，而是使用
    Zipf 分布 [17]。'
- en: '![](../Images/48c47aee44f15bc19ac9f6e15b330f56.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/48c47aee44f15bc19ac9f6e15b330f56.png)'
- en: '**Asymmetric Dirichlet distribution** with αᵢ=1/*i* — Figure by the author'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**非对称 Dirichlet 分布**，αᵢ=1/*i* — 图由作者提供'
- en: Zipf distribution
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Zipf 分布
- en: '[17] uses a combination of Zipf and Dirichlet distributions. It uses the Zipf
    distribution to determine the number of samples at each client and then selects
    the class distribution using the Dirichlet.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] 使用了Zipf分布和Dirichlet分布的组合。它使用Zipf分布来确定每个客户端的样本数量，然后使用Dirichlet分布选择类别分布。'
- en: '![](../Images/a2a495150ba551ab0bc5e9bec50c6d30.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2a495150ba551ab0bc5e9bec50c6d30.png)'
- en: Probability for rank *k* in the Zipf distribution where is the Riemann Zeta
    function
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Zipf分布中第*k*名的概率，其中是黎曼ζ函数
- en: In the Zipf (zeta) distribution the frequency of an item is inversely proportional
    to its rank in a frequency table. [Zipf’s law](https://en.wikipedia.org/wiki/Zipf%27s_law)
    can be observed in many real-world datasets, for example regarding the word frequency
    in language corpora [18].
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在Zipf（zeta）分布中，某个项目的频率与其在频率表中的排名成反比。[Zipf定律](https://en.wikipedia.org/wiki/Zipf%27s_law)可以在许多现实世界的数据集中观察到，例如语言语料库中的词频[18]。
- en: '![](../Images/5d486c9eef7e59b824ce03d2041af8b2.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d486c9eef7e59b824ce03d2041af8b2.png)'
- en: Sampling items using the Zipf distribution — Figure by the author following
    the [numpy documentation on Zipf](https://numpy.org/doc/stable/reference/random/generated/numpy.random.zipf.html)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Zipf分布抽样的项目 — 作者根据[numpy Zipf文档](https://numpy.org/doc/stable/reference/random/generated/numpy.random.zipf.html)绘制的图
- en: Conclusion
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Benchmarking federated learning methods is a challenging task. Ideally, one
    uses predefined real federated datasets. However, if a certain scenario has to
    be simulated without a good existing dataset to cover it, one can use data distribution
    techniques. Proper documentation for reproducibility and motivation of the design
    choice is important. Here I summarized the most common methods already in use
    for FL algorithm evaluation. Visit [this Colab notebook](https://colab.research.google.com/drive/1Bj14Quxo8afOTdAxP3s7aFdTK7RP9bGC?usp=sharing)
    for the codes used for this story!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试联邦学习方法是一项具有挑战性的任务。理想情况下，人们会使用预定义的真实联邦数据集。然而，如果某种场景必须在没有良好的现有数据集来覆盖的情况下进行模拟，可以使用数据分布技术。为可重复性和设计选择的动机提供适当的文档是重要的。在这里，我总结了用于FL算法评估的最常见方法。访问[这个Colab笔记本](https://colab.research.google.com/drive/1Bj14Quxo8afOTdAxP3s7aFdTK7RP9bGC?usp=sharing)获取用于此故事的代码！
- en: References
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] McMahan, B., Moore, E., Ramage, D., Hampson, S., & y Arcas, B. A. (2017,
    April). Communication-efficient learning of deep networks from decentralized data.
    In *Artificial intelligence and statistics* (pp. 1273–1282). PMLR.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] McMahan, B., Moore, E., Ramage, D., Hampson, S., & y Arcas, B. A. (2017年4月).
    从去中心化数据中高效学习深度网络. 载于*人工智能与统计学*（第1273–1282页）。PMLR。'
- en: '[2] Savi, M., & Olivadese, F. (2021). Short-term energy consumption forecasting
    at the edge: A federated learning approach. *IEEE Access*, *9*, 95949–95969.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Savi, M., & Olivadese, F. (2021). 边缘端的短期能源消耗预测：一种联邦学习方法. *IEEE Access*,
    *9*, 95949–95969。'
- en: '[3] Caldas, S., Duddu, S. M. K., Wu, P., Li, T., Konečný, J., McMahan, H. B.,
    … & Talwalkar, A. (2019). Leaf: A benchmark for federated settings. *Workshop
    on Federated Learning for Data Privacy and Confidentiality*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Caldas, S., Duddu, S. M. K., Wu, P., Li, T., Konečný, J., McMahan, H. B.,
    … & Talwalkar, A. (2019). Leaf: 联邦设置的基准. *联邦学习数据隐私与保密研讨会*'
- en: '[4] Krizhevsky, A. (2009). Learning Multiple Layers of Features from Tiny Images.
    *Master’s thesis, University of Tront*.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Krizhevsky, A. (2009). 从微小图像中学习多层特征. *硕士论文, 特隆赫大学*。'
- en: '[5] Liu, W., Chen, L., Chen, Y., & Zhang, W. (2020). Accelerating federated
    learning via momentum gradient descent. *IEEE Transactions on Parallel and Distributed
    Systems*, *31*(8), 1754–1766.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Liu, W., Chen, L., Chen, Y., & Zhang, W. (2020). 通过动量梯度下降加速联邦学习. *IEEE并行与分布式系统汇刊*,
    *31*(8), 1754–1766。'
- en: '[6] Zhang, L., Luo, Y., Bai, Y., Du, B., & Duan, L. Y. (2021). Federated learning
    for non-iid data via unified feature learning and optimization objective alignment.
    In *Proceedings of the IEEE/CVF international conference on computer vision* (pp.
    4420–4428).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Zhang, L., Luo, Y., Bai, Y., Du, B., & Duan, L. Y. (2021). 通过统一特征学习和优化目标对齐进行非独立同分布数据的联邦学习.
    载于*IEEE/CVF国际计算机视觉会议论文集*（第4420–4428页）。'
- en: '[7] Zhang, J., Guo, S., Ma, X., Wang, H., Xu, W., & Wu, F. (2021). Parameterized
    knowledge transfer for personalized federated learning. *Advances in Neural Information
    Processing Systems*, *34*, 10092–10104.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Zhang, J., Guo, S., Ma, X., Wang, H., Xu, W., & Wu, F. (2021). 个性化联邦学习的参数化知识转移.
    *神经信息处理系统进展*, *34*, 10092–10104。'
- en: '[8] Zhao, Y., Li, M., Lai, L., Suda, N., Civin, D., & Chandra, V. (2018). Federated
    learning with non-iid data. *arXiv preprint arXiv:1806.00582*.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Zhao, Y., Li, M., Lai, L., Suda, N., Civin, D., & Chandra, V. (2018). 带有非独立同分布数据的联邦学习.
    *arXiv预印本arXiv:1806.00582*。'
- en: '[9] Li, D., & Wang, J. (2019). Fedmd: Heterogenous federated learning via model
    distillation. *arXiv preprint arXiv:1910.03581*.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Li, D., & Wang, J. (2019)。Fedmd：通过模型蒸馏的异质联邦学习。*arXiv预印本 arXiv:1910.03581*。'
- en: '[10] Wang, H., Kaplan, Z., Niu, D., & Li, B. (2020, July). Optimizing federated
    learning on non-iid data with reinforcement learning. In *IEEE INFOCOM 2020-IEEE
    Conference on Computer Communications* (pp. 1698–1707). IEEE.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Wang, H., Kaplan, Z., Niu, D., & Li, B. (2020年7月)。使用强化学习优化非独立同分布数据上的联邦学习。载于*IEEE
    INFOCOM 2020-IEEE计算机通信会议*（第1698–1707页）。IEEE。'
- en: '[11] Lin, T., Kong, L., Stich, S. U., & Jaggi, M. (2020). Ensemble distillation
    for robust model fusion in federated learning. *Advances in Neural Information
    Processing Systems*, *33*, 2351–2363.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Lin, T., Kong, L., Stich, S. U., & Jaggi, M. (2020)。用于联邦学习中强健模型融合的集成蒸馏。*神经信息处理系统进展*，*33*，2351–2363。'
- en: '[12] Luo, M., Chen, F., Hu, D., Zhang, Y., Liang, J., & Feng, J. (2021). No
    fear of heterogeneity: Classifier calibration for federated learning with non-iid
    data. *Advances in Neural Information Processing Systems*, *34*, 5972–5984.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Luo, M., Chen, F., Hu, D., Zhang, Y., Liang, J., & Feng, J. (2021)。不惧异质性：用于非独立同分布数据的联邦学习分类器校准。*神经信息处理系统进展*，*34*，5972–5984。'
- en: '[13] Yurochkin, M., Agarwal, M., Ghosh, S., Greenewald, K., Hoang, N., & Khazaeni,
    Y. (2019, May). Bayesian nonparametric federated learning of neural networks.
    In *International conference on machine learning* (pp. 7252–7261). PMLR.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Yurochkin, M., Agarwal, M., Ghosh, S., Greenewald, K., Hoang, N., & Khazaeni,
    Y. (2019年5月)。贝叶斯非参数化联邦学习的神经网络。载于*国际机器学习会议*（第7252–7261页）。PMLR。'
- en: '[14] Wang, H., Yurochkin, M., Sun, Y., Papailiopoulos, D., & Khazaeni, Y. (2020)
    Federated Learning with Matched Averaging. In *International Conference on Learning
    Representations*.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Wang, H., Yurochkin, M., Sun, Y., Papailiopoulos, D., & Khazaeni, Y. (2020)
    使用匹配平均的联邦学习。载于*国际学习表征会议*。'
- en: '[15] Li, Q., He, B., & Song, D. (2021). Model-contrastive federated learning.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*
    (pp. 10713–10722).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Li, Q., He, B., & Song, D. (2021)。模型对比的联邦学习。载于*IEEE/CVF计算机视觉与模式识别会议论文集*（第10713–10722页）。'
- en: '[16] Hsu, T. M. H., Qi, H., & Brown, M. (2019). Measuring the effects of non-identical
    data distribution for federated visual classification. *arXiv preprint arXiv:1909.06335*.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Hsu, T. M. H., Qi, H., & Brown, M. (2019)。测量非相同数据分布对联邦视觉分类的影响。*arXiv预印本
    arXiv:1909.06335*。'
- en: '[17] Wadu, M. M., Samarakoon, S., & Bennis, M. (2021). Joint client scheduling
    and resource allocation under channel uncertainty in federated learning. *IEEE
    Transactions on Communications*, *69*(9), 5962–5974.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Wadu, M. M., Samarakoon, S., & Bennis, M. (2021)。在联邦学习中在频道不确定性下的联合客户端调度与资源分配。*IEEE通信交易*，*69*(9)，5962–5974。'
- en: '[18] Fagan, Stephen; Gençay, Ramazan (2010), “An introduction to textual econometrics”,
    in Ullah, Aman; Giles, David E. A. (eds.), *Handbook of Empirical Economics and
    Finance*, CRC Press, pp. 133–153'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Fagan, Stephen; Gençay, Ramazan (2010)，“文本计量经济学导论”，载于Ullah, Aman; Giles,
    David E. A.（编），*实证经济学与金融手册*，CRC Press，第133–153页'
