# æ²¡æœ‰å†·å¯åŠ¨é—®é¢˜çš„é«˜æ•ˆæ¨èç³»ç»Ÿ

> åŸæ–‡ï¼š[`towardsdatascience.com/a-performant-recommender-system-without-cold-start-problem-69bf2f0f0b9b`](https://towardsdatascience.com/a-performant-recommender-system-without-cold-start-problem-69bf2f0f0b9b)

## [æ¨èç³»ç»Ÿ](https://medium.com/tag/recommendation-system)

## å½“ååŒè¿‡æ»¤å’ŒåŸºäºå†…å®¹çš„æ¨èç³»ç»Ÿåˆå¹¶æ—¶

[](https://dr-robert-kuebler.medium.com/?source=post_page-----69bf2f0f0b9b--------------------------------)![Dr. Robert KÃ¼bler](https://dr-robert-kuebler.medium.com/?source=post_page-----69bf2f0f0b9b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----69bf2f0f0b9b--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page-----69bf2f0f0b9b--------------------------------) [Dr. Robert KÃ¼bler](https://dr-robert-kuebler.medium.com/?source=post_page-----69bf2f0f0b9b--------------------------------)

Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----69bf2f0f0b9b--------------------------------) Â·é˜…è¯»æ—¶é—´ 11 åˆ†é’ŸÂ·2023 å¹´ 1 æœˆ 31 æ—¥

--

![](img/c0e674a93dd06ef0ac51647be57bbd22.png)

å›¾ç‰‡ç”± [Ivan Aleksic](https://unsplash.com/@ivalex?utm_source=medium&utm_medium=referral) æä¾›ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

ä¹Ÿè®¸æœ€è‘—åçš„æ¨èç³»ç»Ÿæ˜¯æ‰€è°“çš„**çŸ©é˜µåˆ†è§£**ã€‚åœ¨è¿™ç§**ååŒè¿‡æ»¤**æ¨èç³»ç»Ÿä¸­ï¼Œç”¨æˆ·å’Œé¡¹ç›®éƒ½é€šè¿‡**åµŒå…¥**è¡¨ç¤ºï¼Œè¿™ä¸è¿‡æ˜¯ä¸€ä¸ªæ•°å­—å‘é‡ã€‚ç›´è§‚çš„ç†è§£æ˜¯ï¼Œç”¨æˆ·å’Œé¡¹ç›®åµŒå…¥çš„ç‚¹ç§¯åº”ç­‰äºç”¨æˆ·å¯¹è¯¥é¡¹ç›®çš„è¯„åˆ†ã€‚

å¦‚æœä½ å¯¹è¿™äº›æ¦‚å¿µè¿˜ä¸ç†Ÿæ‚‰ï¼Œæˆ‘å»ºè®®ï¼ˆğŸ˜‰ï¼‰åœ¨ç»§ç»­ä¹‹å‰é˜…è¯»æˆ‘çš„å¦ä¸€ç¯‡æ–‡ç« ï¼Œå› ä¸ºæˆ‘åœ¨å…¶ä¸­è§£é‡Šäº†è®¸å¤šæ¦‚å¿µå’Œä»£ç ç‰‡æ®µã€‚

[](/introduction-to-embedding-based-recommender-systems-956faceb1919?source=post_page-----69bf2f0f0b9b--------------------------------) ## åµŒå…¥å¼æ¨èç³»ç»Ÿç®€ä»‹

### å­¦ä¹ å¦‚ä½•åœ¨ TensorFlow ä¸­æ„å»ºä¸€ä¸ªç®€å•çš„æ¨èç³»ç»Ÿ

towardsdatascience.com

# å†·å¯åŠ¨é—®é¢˜

çº¯ç²¹çš„ååŒè¿‡æ»¤æ¨èç³»ç»Ÿï¼Œå¦‚çŸ©é˜µåˆ†è§£ï¼Œæœ‰ä¸€ä¸ªä¼˜ç‚¹ï¼Œå³ä½¿æ²¡æœ‰å¤ªå¤šå…³äºç”¨æˆ·å’Œä½ æƒ³æ¨èçš„ç”µå½±/æ–‡ç« /é¡¹ç›®çš„æ•°æ®ï¼Œä½ ä¹Ÿé€šå¸¸å¯ä»¥ç«‹å³æ„å»ºå®ƒä»¬ã€‚ä½ åªéœ€è¦çŸ¥é“è°å¯¹ä»€ä¹ˆè¿›è¡Œäº†è¯„åˆ†ä»¥åŠè¯„åˆ†æƒ…å†µï¼›ä¾‹å¦‚ï¼Œç”¨æˆ·*B*å¯¹ç”µå½±*Y*çš„è¯„åˆ†ä¸º 2 æ˜Ÿã€‚

![](img/1445f7bebc146490dd07c4960680fa87.png)

ä½œè€…æä¾›çš„å›¾ç‰‡ã€‚

ç„¶è€Œï¼Œå½“ä½ æœ‰**æ–°**çš„**ç”¨æˆ·æˆ–é¡¹ç›®**æ—¶ï¼Œå®ƒä»¬å°±ä¸å¤Ÿç”¨äº†ï¼Œå› ä¸ºæ¨¡å‹æ²¡æœ‰æœºä¼šå­¦ä¹ è¿™äº›å†…å®¹ï¼Œä»è€ŒåŸºæœ¬ä¸Šä¸ºè¿™äº›ç”¨æˆ·æˆ–é¡¹ç›®æä¾›äº†éšæœºæ¨èâ€”â€”è¿™å°±æ˜¯ä»¤äººå¤´ç–¼çš„**å†·å¯åŠ¨é—®é¢˜**ã€‚å‡è®¾å¦ä¸€ä¸ªç”¨æˆ·*E*æ³¨å†Œäº†ï¼Œæˆ‘ä»¬è¿˜åœ¨æ•°æ®åº“ä¸­æ·»åŠ äº†ä¸€ä¸ªæ–°ç”µå½±*W*ã€‚

![](img/75385b2e5f953f3aa8c5b5e144ba238e.png)

å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†å±•ç¤ºä¸€ç§é€šè¿‡ç»“åˆæ›´å¤šå…³äºç”¨æˆ·å’Œé¡¹ç›®çš„ç‰¹å¾æ¥ç¼“è§£å†·å¯åŠ¨é—®é¢˜çš„ç®€å•æ–¹æ³•â€”â€”è¿™å°±æ˜¯æˆ‘ä»¬å°†åŠ å…¥åˆ°æ¨¡å‹ä¸­çš„**åŸºäºå†…å®¹çš„**ç»„ä»¶ã€‚ä½¿ç”¨å®é™…å†…å®¹æ•°æ®ï¼Œå¦‚ç”¨æˆ·å¹´é¾„æˆ–ç”µå½±ç±»å‹ï¼Œç”Ÿæˆçš„æ¨¡å‹èƒ½æ›´å¥½åœ°å¤„ç†æ–°ç”¨æˆ·æˆ–æ–°ç”µå½±ã€‚

# å›åˆ° MovieLens

å’Œæˆ‘ä¸Šä¸€ç¯‡æ–‡ç« ä¸€æ ·ï¼Œæˆ‘å°†ä½¿ç”¨ [MovieLens](https://movielens.org/) æ•°æ®é›†ï¼Œå®ƒæä¾›äº†ç”¨æˆ·-ç”µå½±è¯„åˆ†ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜åŒ…å«äº†ä¸€äº›ç”¨æˆ·å’Œç”µå½±ç‰¹å¾ï¼Œå°½ç®¡æˆ‘ä»¬åœ¨ä¸Šä¸€ç¯‡æ–‡ç« ä¸­å¿½ç•¥äº†è¿™äº›ç‰¹å¾ï¼Œä½†ä»Šå¤©æˆ‘ä»¬å°†åˆ©ç”¨è¿™äº›ç‰¹å¾æ¥æ„å»ºæ›´å¥½çš„æ¨¡å‹ï¼

> *ä½ å¯ä»¥åœ¨* [*æˆ‘çš„ Github*](https://github.com/Garve/Towards-Data-Science---Notebooks/blob/main/TDS%20-%20A%20Performant%20Recommender%20System%20Without%20Cold%20Start%C2%A0Problem.ipynb)*æ‰¾åˆ°ä»£ç *ã€‚

è·Ÿéšä¸Šä¸€ç¯‡æ–‡ç« è®©æˆ‘ä»¬

1.  ä½¿ç”¨ tensorflow-datasets è·å–æ•°æ®

1.  å°†å…¶åˆ¶ä½œæˆæ•°æ®æ¡†å¹¶æ›´æ”¹ä¸€äº›åˆ—ç±»å‹ï¼Œç„¶å

1.  æŒ‰æ—¶é—´æ’åºä»¥è¿›è¡Œæ—¶é—´ä¸Šçš„è®­ç»ƒ-æµ‹è¯•åˆ†å‰²

```py
import tensorflow_datasets as tfds

data = tfds.load("movielens/1m-ratings")
df = tfds.as_dataframe(data["train"])

filtered_data = (
    df
    .sort_values("timestamp") # for a temporal train-eval-test split
    .astype(
        {
            "bucketized_user_age": int,
            "movie_id": int,
            "movie_title": str,
            "user_gender": int,
            "user_id": int,
            "user_occupation_label": int,
            "user_occupation_text": str,
            "user_rating": int,
            "user_zip_code": str,
        }
    )
    .drop(columns=["timestamp"])
)

# temporal train-eval-test split
train = filtered_data.iloc[:80000]
evaluation = filtered_data.iloc[80000:90000]
test = filtered_data.iloc[90000:]

X_train = train.drop(columns=["user_rating"])
y_train = train["user_rating"]
X_eval = evaluation.drop(columns=["user_rating"])
y_eval = evaluation["user_rating"]
X_test = test.drop(columns=["user_rating"])
y_test = test["user_rating"]
```

`filtered_data` æ•°æ®æ¡†

![](img/b1d25ca050a152a9e1bba81fdee26380.png)

å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚

å‘Šè¯‰æˆ‘ä»¬é™¤äº†**user_idã€movie_id**å’Œç›®æ ‡**user_rating**ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜æœ‰**ç”¨æˆ·ç‰¹å¾**

+   bucketized_user_age

+   user_gender

+   user_occupation_label

+   user_occupation_text

+   user_zip_code

ä»¥åŠ**ç”µå½±ç‰¹å¾**

+   movie_genres

+   movie_title

ä½¿ç”¨ä¸€äº›åˆ»æ¿å°è±¡ï¼š

> ä»ç›´è§‚ä¸Šçœ‹ï¼Œè¿™äº›ç‰¹å¾åº”è¯¥éå¸¸æœ‰å¸®åŠ©ï¼Œå› ä¸ºæ¨¡å‹å¯ä»¥å­¦ä¹ â€œå¥³æ€§å–œæ¬¢æˆå‰§â€æˆ–â€œå¹´è½»äººä¸å–œæ¬¢è€ç”µå½±â€ä¹‹ç±»çš„å†…å®¹ã€‚

æˆ‘ä»¬ç°åœ¨å°†äº†è§£å¦‚ä½•é€šè¿‡ä¸€ä¸ªç®€å•çš„ç½‘ç»œæ¶æ„**LightFM**æ¥ä½¿ç”¨æ‰€æœ‰è¿™äº›é¢å¤–ç‰¹å¾ã€‚è¿™ä¸ªåå­—æ˜¯ç”± [Maciej Kula](https://www.linkedin.com/in/maciej-kula-57283147/) åœ¨ä»–å†™å¾—å¾ˆå¥½çš„è®ºæ–‡ **Metadata Embeddings for User and Item Cold-start Recommendations** [1] ä¸­é€‰æ‹©çš„ã€‚è¯·é˜…è¯»ä¸€ä¸‹ï¼

> LightFM æ˜¯ä¸€ç§æ··åˆå‹æ¨èç³»ç»Ÿï¼Œå› ä¸ºå®ƒä½¿ç”¨è¯„åˆ†ä»¥åŠç”¨æˆ·å’Œé¡¹ç›®ç‰¹å¾ã€‚

# LightFM çš„ç®€å•æƒ³æ³•

è®©æˆ‘ä»¬é¦–å…ˆå›é¡¾ä¸€ä¸‹æˆ‘ä»¬ç®€å•çš„çŸ©é˜µåˆ†è§£æ˜¯æ€æ ·çš„ï¼Œçœç•¥åå·®ã€‚åœ¨æˆ‘ä»¬æ—§çš„æ¨èç³»ç»Ÿä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†**user_id**å’Œ**movie_id**ï¼Œå°†ä¸¤è€…è¿›è¡ŒåµŒå…¥ï¼Œç„¶åè®¡ç®—ç‚¹ç§¯æ¥è®¡ç®—è¯„åˆ†ã€‚

![](img/8e6ee1d11d43d46b20f23f4f6241ff41.png)

çŸ©é˜µåˆ†è§£æ¶æ„ï¼Œå›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚

å¯¹äº**LightFM**ï¼Œå®ƒæ˜¯è¿™æ ·å·¥ä½œçš„ï¼š

1.  æˆ‘ä»¬**åµŒå…¥æ‰€æœ‰ç‰¹å¾**ï¼ŒåŒ…æ‹¬ç”¨æˆ·ç‰¹å¾å’Œç”µå½±ç‰¹å¾ã€‚

1.  ç”¨æˆ·ï¼ˆç”µå½±ï¼‰åµŒå…¥æ˜¯**æ‰€æœ‰è¿™äº›ç”¨æˆ·ï¼ˆç”µå½±ï¼‰ç‰¹æ€§åµŒå…¥çš„æ€»å’Œ**ã€‚

å°±è¿™æ ·ï¼å¯¹äºæŸäº›ç‰¹å®šçš„ç‰¹æ€§ï¼Œç½‘ç»œæ¶æ„å¯èƒ½çœ‹èµ·æ¥åƒè¿™æ ·ï¼š

![](img/1026153ed5733b5d7693965251cdd499.png)

LightFM æ¶æ„ï¼Œå›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚

## ä¼˜åŠ¿

è¿™ä¸ªæ–¹æ³•çš„å¥½å¤„æ˜¯ï¼Œå³ä½¿ä½ å°†æ–°ç”¨æˆ·æˆ–ç”µå½±æ·»åŠ åˆ°æ•°æ®åº“ä¸­ï¼Œåªè¦çŸ¥é“å®ƒä»¬çš„ç‰¹æ€§ï¼ˆ*å†…å®¹*ï¼‰ï¼Œä½ ä¹Ÿå¯ä»¥åˆ›å»ºæœ‰æ„ä¹‰çš„åµŒå…¥ã€‚ä½ ä¸ä¼šçŸ¥é“ ID çš„åµŒå…¥â€”â€”è¿™æ˜¯çŸ©é˜µåˆ†è§£æ–¹æ³•ä¸­çš„ä¸»è¦é—®é¢˜â€”â€”ä½†æˆ‘ä»¬å¸Œæœ›å…¶ä»–åµŒå…¥å¯ä»¥å¼¥è¡¥è¿™ä¸€ç‚¹ã€‚åœ¨å†·å¯åŠ¨è®¾ç½®ä¸­ï¼Œ**user_id** æˆ– **movie_id** æ˜¯æœªçŸ¥çš„ï¼Œä½†æˆ‘ä»¬ä»ç„¶å¯ä»¥ç»™å®ƒä»¬ä¸€äº›é»˜è®¤çš„åµŒå…¥ã€‚

# åœ¨ TensorFlow ä¸­çš„å®ç°

ä»…ç”¨ä¸¤ä¸ª ID ä½œä¸ºè¾“å…¥ï¼Œæ˜ç¡®æŒ‡å®šè¾“å…¥ã€ç¼–ç ã€åµŒå…¥å’Œåå·®å°±è¶³å¤Ÿäº†ã€‚ç„¶è€Œï¼Œå¯¹äºæˆ‘ä»¬çš„ç‰¹æ€§æ•°é‡ï¼Œå…ˆå®šä¹‰ä¸€äº›é…ç½®ç„¶åä½¿ç”¨å¾ªç¯æ˜¯æœ‰æ„ä¹‰çš„ã€‚

> ***æ³¨æ„ï¼š*** *æˆ‘ä»¬å°†çœç•¥* ***movie_title*** *å’Œ* ***movie_genres*** *ï¼Œå› ä¸ºæˆ‘ä»¬å¿…é¡»ä»¥ä¸åŒäºå…¶ä»–ç‰¹æ€§çš„æ–¹å¼å¤„ç†å®ƒä»¬ã€‚ç„¶è€Œï¼Œæˆ‘ä¼šå‘Šè¯‰ä½ å¦‚ä½•å°†è¿™äº›ç‰¹æ€§ä¹Ÿçº³å…¥å…¶ä¸­ã€‚*

```py
features_config = {
    "user_id": {"entity": "user", "dtype": tf.int64},
    "bucketized_user_age": {"entity": "user", "dtype": tf.int64},
    "user_gender": {"entity": "user", "dtype": tf.int64},
    "user_occupation_label": {"entity": "user", "dtype": tf.int64},
    "movie_id": {"entity": "movie", "dtype": tf.int64},
    "user_zip_code": {"entity": "user", "dtype": tf.string},
    "user_occupation_text": {"entity": "user", "dtype": tf.string},
}

for name, config in features_config.items():
    if config["dtype"] == tf.int64:
        config["encoding_layer_class"] = tf.keras.layers.IntegerLookup
    elif config["dtype"] == tf.string:
        config["encoding_layer_class"] = tf.keras.layers.StringLookup
    else:
        raise Exception

    config["vocab"] = train[name].unique()
```

æˆ‘ä»¬ç°åœ¨æœ‰ä¸€ä¸ªè¯¦ç»†çš„é…ç½®å­—å…¸ï¼Œå®ƒå‘Šè¯‰æˆ‘ä»¬æ¯ä¸ªç‰¹æ€§çš„ä¿¡æ¯ã€‚

+   è¾“å…¥å±‚éœ€è¦çš„*dtype*ï¼Œ

+   ç‰¹æ€§æ˜¯å¦å±äºç”µå½±æˆ–ç”¨æˆ·ç‰¹æ€§ï¼Œ

+   éœ€è¦å“ªä¸ªæŸ¥æ‰¾å±‚ï¼Œå³`IntegerLookup`ç”¨äºæ•´æ•°ç‰¹æ€§ï¼Œ`StringLookup`ç”¨äºå­—ç¬¦ä¸²ç‰¹æ€§ï¼Œ

+   ä»¥åŠè¯æ±‡è¡¨ï¼Œå³æ¯ä¸ªç‰¹æ€§çš„å”¯ä¸€ç±»ã€‚

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ª TensorFlow æ¨¡å‹æ¥å®ç°æˆ‘ä»¬åœ¨ LightFM æ¶æ„å›¾ä¸­çœ‹åˆ°çš„å†…å®¹ï¼š

```py
# define input layers for each feature
inputs = {
    name: tf.keras.layers.Input(shape=(1,), name=name, dtype=config["dtype"])
    for name, config in features_config.items()
}

# encode all features as integers via the lookup layers
inputs_encoded = {
    name: config"encoding_layer_class"(inputs[name])
    for name, config in features_config.items()
}

# create embeddings for all features
embeddings = {
    name: tf.keras.layers.Embedding(
        input_dim=len(config["vocab"]) + 1,
        output_dim=32,
    )(inputs_encoded[name])
    for name, config in features_config.items()
}

# create embeddings for all features
biases = {
    name: tf.keras.layers.Embedding(input_dim=len(config["vocab"]) + 1, output_dim=1)(
        inputs_encoded[name]
    )
    for name, config in features_config.items()
}

# compute the user embedding as the sum of all user feature embeddings
user_embedding = tf.keras.layers.Add()(
    [
        embeddings[name]
        for name, config in features_config.items()
        if config["entity"] == "user"
    ]
)

# compute the movie embedding as the sum of all movie feature embeddings
movie_embedding = tf.keras.layers.Add()(
    [
        embeddings[name]
        for name, config in features_config.items()
        if config["entity"] == "movie"
    ]
)

# compute the user bias as the sum of all user feature biases
user_bias = tf.keras.layers.Add()(
    [
        biases[name]
        for name, config in features_config.items()
        if config["entity"] == "user"
    ]
)

# compute the movie bias as the sum of all movie feature biases
movie_bias = tf.keras.layers.Add()(
    [
        biases[name]
        for name, config in features_config.items()
        if config["entity"] == "movie"
    ]
)

# do the exact same thing as in matrix factorization, 
# i.e. compute the dot product of the user and movie embedding,
# add the user and movie bias, and squash the result into the range [1, 5]
dot = tf.keras.layers.Dot(axes=2)([user_embedding, movie_embedding])
add = tf.keras.layers.Add()([dot, user_bias, movie_bias])
flatten = tf.keras.layers.Flatten()(add)
squash = tf.keras.layers.Lambda(lambda x: 4 * tf.nn.sigmoid(x) + 1)(flatten)

model = tf.keras.Model(
    inputs=[inputs[name] for name in features_config.keys()], outputs=squash
)

model.compile(loss="mse", metrics=[tf.keras.metrics.MeanAbsoluteError()])
```

æˆ‘çŸ¥é“è¿™å¾ˆç¹é‡ã€‚ä½†å¦‚æœä½ ä¹Ÿé˜…è¯»äº†æˆ‘å…³äºåŸºäºåµŒå…¥çš„æ¨èç³»ç»Ÿçš„å…¶ä»–æ–‡ç« ï¼Œåº”è¯¥æ²¡æœ‰å¤§æƒŠå°æ€ªçš„åœ°æ–¹ã€‚æˆ‘ä»¬å‡†å¤‡å¥½è®­ç»ƒæ¨¡å‹äº†ï¼

```py
model.fit(
    x={name: X_train[name].values for name in features_config.keys()},
    y=y_train.values,
    batch_size=256,
    epochs=100,
    validation_data=(
        {name: X_eval[name].values for name in features_config.keys()},
        y_eval.values,
    ),
    callbacks=[tf.keras.callbacks.EarlyStopping(patience=1, restore_best_weights=True)],
)

# Output:
# [...]
# Epoch 6/100
# 313/313 [==============================] - 1s 3ms/step - loss: 0.7626 - mean_absolute_error: 0.6836 - val_loss: 0.9836 - val_mean_absolute_error: 0.7985
```

æµ‹è¯•é›†ä¸Šçš„è¡¨ç°ï¼š

```py
model.evaluate(
    x={name: X_test[name].values for name in features_config.keys()},
    y=y_test.values,
    batch_size=1_000_000,
)

# Output:
# 1/1 [==============================] - 1s 667ms/step - loss: 1.0153 - mean_absolute_error: 0.8135
```

ä½ ä¹Ÿå¯ä»¥å°è¯•åœ¨è¿™ç§è®¾ç½®ä¸­è¿›è¡ŒçŸ©é˜µåˆ†è§£ï¼›ä½ åªéœ€å°†`features_config`å­—å…¸æ›´æ”¹ä¸º

```py
features_config = {
    "user_id": {"entity": "user", "dtype": tf.int64},
    "movie_id": {"entity": "movie", "dtype": tf.int64},
}
```

é€šè¿‡åˆ é™¤ä¸€äº›è¡Œï¼Œç„¶åæ‰§è¡Œå‰©ä½™çš„ä»£ç ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç»“æœæ˜¯æµ‹è¯• MSE ä¸º 1.322 å’Œ MAE ä¸º 0.953ï¼Œè¿™æ¯” LightFM ç»“æœå·®å¾—å¤šã€‚è¿™çœ‹èµ·æ¥å¾ˆæ£’ï¼

## å¤„ç†ç”µå½±ç±»å‹

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å¿½ç•¥äº†å¯èƒ½æå…·ä¿¡æ¯é‡çš„åˆ—**movie_genres**ï¼Œå› ä¸ºå®ƒæ¯”åˆ†ç±»å˜é‡æ›´éš¾å¤„ç†ï¼Œå› ä¸ºè¿™é‡Œæˆ‘ä»¬æœ‰ä¸€ä¸ªæ•´æ•°åˆ—è¡¨è€Œä¸æ˜¯å•ä¸ªæ•´æ•°ã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬å¿…é¡»åˆ¶å®šä¸€äº›é€»è¾‘æ¥å¤„ç†è¿™ä¸ªé—®é¢˜ã€‚

![](img/c13e6726a3bb9b84eab7d7aa1cf3504c.png)

å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚

å¤„ç†è¿™ä¸ªæœ€ç®€å•çš„æ–¹æ³•æ˜¯ä¸ºæ¯ä¸ªç±»å‹åˆ›å»ºä¸€ä¸ªåµŒå…¥ï¼Œç„¶åå–å®ƒä»¬çš„å‡å€¼ã€‚ä½ å¯ä»¥ä½¿ç”¨`GlobalAveragePooling1D`å±‚æ¥å®ç°è¿™ä¸€ç‚¹ã€‚

ä¸ºäº†åœ¨ä»£ç ä¸­å®ç°è¿™ä¸ªæƒ³æ³•ï¼Œè¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

```py
# new movie embeddings
all_movie_genres = train["movie_genres"].explode().unique().astype(int) # get all different genres
movie_genres_input = tf.keras.layers.Input(shape=(None,), name="movie_genres")
movie_genres_as_integer = tf.keras.layers.IntegerLookup(vocabulary=all_movie_genres)(movie_genres_input)
movie_genres_embeddings = tf.keras.layers.Embedding(input_dim=len(all_movie_genres) + 1, output_dim=32)(movie_genres_as_integer)
movie_genres_biases = tf.keras.layers.Embedding(input_dim=len(all_movie_genres) + 1, output_dim=1)(movie_genres_as_integer)
movie_genres_embedding = tf.keras.layers.GlobalAveragePooling1D(keepdims=True)(movie_genres_embeddings)
movie_genres_bias = tf.keras.layers.GlobalAveragePooling1D(keepdims=True)(movie_genres_biases)

movie_embedding = tf.keras.layers.Add()(
    [
        embeddings[name]
        for name, config in features_config.items()
        if config["entity"] == "movie"
    ] + [movie_genres_embedding] # add the movie genres embedding here as well
)

# new movie bias
movie_bias = tf.keras.layers.Add()(
    [
        biases[name]
        for name, config in features_config.items()
        if config["entity"] == "movie"
    ] + [movie_genres_bias] # add the movie genres bias here as well
)

# add the movie inut to the inputs
model = tf.keras.Model(
    inputs=[inputs[name] for name in features_config.keys()] + [movie_genres_input], outputs=squash
)
```

å…¶ä½™éƒ¨åˆ†ä¿æŒä¸å˜ã€‚åœ¨æ‹Ÿåˆã€è¯„ä¼°å’Œæµ‹è¯•æ¨¡å‹æ—¶ï¼Œä½ åªéœ€å°†ç‰¹å¾**movie_genres**æ·»åŠ åˆ°æ¨¡å‹ä¸­ã€‚ç”±äºç±»å‹çš„å½¢çŠ¶æœ‰ç‚¹å›°éš¾ï¼Œå› ä¸ºåˆ—è¡¨çš„é•¿åº¦ä¸åŒï¼Œæ‰€ä»¥ TensorFlow åœ¨å°†å…¶è½¬æ¢ä¸ºå¸¸è§„å¼ é‡æ—¶é‡åˆ°é—®é¢˜ã€‚å¹¸è¿çš„æ˜¯ï¼ŒTensorFlow é€šè¿‡æä¾›**ragged tensors**ï¼ˆé€šè¿‡`tf.ragged.constant`ï¼‰æ¥å¤„ç†è¿™äº›å¯å˜å¤§å°çš„å¼ é‡ã€‚

```py
model.fit(
    x={
        **{name: X_train[name].values for name in features_config.keys()},
        "movie_genres": tf.ragged.constant(X_train["movie_genres"].values)
    },

# [...]
```

åœ¨æµ‹è¯•é›†ä¸Šæ‹Ÿåˆå’Œè¯„ä¼°æ¨¡å‹æ˜¾ç¤ºäº†å¦ä¸€ä¸ªæ”¹è¿›ï¼Œå°½ç®¡å°äºé¢„æœŸã€‚MSE çº¦ä¸º 1.0ï¼ŒMAE çº¦ä¸º 0.807ã€‚

![](img/60f1b2ab0d69716a5356a9344c469ac5.png)

æ‰€æœ‰ç»“æœçš„ç»¼åˆã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚

## å¤„ç†ç”µå½±æ ‡é¢˜

å¦ä¸€ä¸ªæœ‰è¶£çš„ç‰¹å¾æ˜¯æˆ‘ä»¬è¿„ä»Šä¸ºæ­¢å¿½ç•¥çš„ï¼Œå› ä¸ºå®ƒåŒ…å«å…³äºç”µå½±ç³»åˆ—çš„ä¿¡æ¯ã€‚æœ‰äº†è¿™äº›ä¿¡æ¯ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿æ¨¡å‹æ›´å®¹æ˜“å­¦ä¹ ä¸€äº›ç”¨æˆ·éå¸¸å–œæ¬¢æ‰€æœ‰çš„è™è ä¾ ç”µå½±ã€‚ä¾‹å¦‚ï¼Œ*ç¼–ç è¿™ä¸ªæ˜¯ä½ çš„ä½œä¸š*ã€‚ä¸€ç§æ–¹æ³•æ˜¯å°†æ ‡é¢˜å­—ç¬¦ä¸²åˆ†å‰²æˆå•è¯åˆ—è¡¨ï¼Œç„¶åæŒ‰ç…§æˆ‘ä»¬å¯¹å¾…ç±»å‹çš„æ–¹æ³•è¿›è¡Œå¤„ç†ã€‚ä½ ç”šè‡³å¯ä»¥ä½¿ç”¨å¥å­ç¼–ç å™¨ã€ç±»ä¼¼å˜æ¢å™¨çš„æ¶æ„ã€LSTM æˆ–å…¶ä»–ä»»ä½•æ–¹æ³•å°†æ–‡æœ¬è½¬æ¢ä¸ºåµŒå…¥ã€‚

## è¿›è¡Œé¢„æµ‹

ä½ å¯ä»¥é€šè¿‡æä¾›æ‰€æœ‰å¿…è¦çš„ç‰¹å¾æ¥è¿›è¡Œé¢„æµ‹ï¼Œæ–¹æ³•å¦‚ä¸‹ï¼š

```py
query = {
    "user_id": tf.constant([-1]), # unknown user!
    "bucketized_user_age": tf.constant([18]),
    "user_gender": tf.constant([0]),
    "user_occupation_label": tf.constant([12]),
    "movie_id": tf.constant([1]),
    "user_zip_code": tf.constant(["b'65712'"]),
    "user_occupation_text": tf.constant(["b'writer'"]),
    "movie_genres": tf.ragged.constant([[1, 2, 3]])
}

model.predict(query)

# Output:
# array([[4.0875683]], dtype=float32)
```

åœ¨è¿™é‡Œï¼Œä½ å¯ä»¥çœ‹åˆ°ä¸€ä¸ªå¹´è½»çš„æœªçŸ¥ç”¨æˆ·ï¼Œè¯¥ç”¨æˆ·æ€§åˆ«ä¸º 0ï¼ŒèŒä¸šæ ‡ç­¾ä¸º 12ï¼Œå±…ä½åœ¨ 65712 é‚®æ”¿ç¼–ç åŒºåŸŸï¼Œä¸”æ˜¯ä¸€åä½œå®¶ï¼Œä»–å¯èƒ½ä¼šå–œæ¬¢ ID ä¸º 1 çš„ç”µå½±ï¼Œè¯¥ç”µå½±å±äº 1ã€2 å’Œ 3 è¿™äº›ç±»å‹ã€‚

# æ¥è‡ªè®ºæ–‡çš„æ›´å¤šæœ‰è¶£è§è§£

æˆ‘çš„ä¸€ä¸ªå°å®éªŒè¡¨æ˜ï¼ŒLightFM å¯ä»¥æé«˜æ¨¡å‹æ€§èƒ½ï¼Œæ­£å¦‚[1]ä¸­æ‰€è¿°ã€‚è¿™å¾ˆæ£’ï¼Œå°½ç®¡è¿™å¯èƒ½æ˜¯ä½ å·²ç»é¢„æ–™åˆ°çš„ï¼Œå› ä¸º**LightFM æ˜¯çŸ©é˜µåˆ†è§£çš„ä¸€ä¸ªæ¨å¹¿ç‰ˆæœ¬**ã€‚

åœ¨è¿™æ–¹é¢ï¼Œè®ºæ–‡ä½œè€…å†™é“ï¼š

1.  â€œåœ¨å†·å¯åŠ¨å’Œä½å¯†åº¦åœºæ™¯ä¸‹ï¼ŒLightFM çš„è¡¨ç°è‡³å°‘ä¸çº¯å†…å®¹æ¨¡å‹ä¸€æ ·å¥½ï¼Œå½“ï¼ˆ1ï¼‰è®­ç»ƒé›†ä¸­æœ‰ååŒä¿¡æ¯æˆ–ï¼ˆ2ï¼‰æ¨¡å‹ä¸­åŒ…å«ç”¨æˆ·ç‰¹å¾æ—¶ï¼Œå®ƒçš„è¡¨ç°ä¼šå¤§å¹…è¶…è¶Šè¿™äº›æ¨¡å‹ã€‚â€

1.  â€œå½“ååŒæ•°æ®ä¸°å¯Œï¼ˆçƒ­å¯åŠ¨ï¼Œå¯†é›†çš„ç”¨æˆ·-é¡¹ç›®çŸ©é˜µï¼‰æ—¶ï¼ŒLightFM çš„è¡¨ç°è‡³å°‘ä¸ MF æ¨¡å‹ä¸€æ ·å¥½ã€‚â€

1.  â€œLightFM ç”Ÿæˆçš„åµŒå…¥ç¼–ç äº†å…³äºç‰¹å¾çš„é‡è¦è¯­ä¹‰ä¿¡æ¯ï¼Œå¯ç”¨äºç›¸å…³çš„æ¨èä»»åŠ¡ï¼Œå¦‚æ ‡ç­¾æ¨èã€‚â€

è¿™äº›é™ˆè¿°æ²¡æœ‰è¯æ˜ï¼Œä½†ä»–é€šè¿‡åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šæµ‹è¯•å¾—å‡ºäº†è¿™ä¸ªç»“è®ºã€‚è¿™ä¸¤ä¸ªæ•°æ®é›†éƒ½æœ‰**äºŒè¿›åˆ¶æ ‡ç­¾ï¼Œ** æ„å‘³ç€è¯¥é¡¹ç›®å¯¹ç”¨æˆ·æœ‰ç”¨æˆ–æ— ç”¨ã€‚å¯¹äºäºŒè¿›åˆ¶æ ‡ç­¾ï¼Œä»–é€‰æ‹©äº† AUC ä½œä¸ºè¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶åœ¨æ­¤è¡¨æ ¼ä¸­æ€»ç»“äº†ä»–çš„å‘ç°ï¼š

![](img/c105fc4e74e6fa3f59a2c3392a8dfa3c.png)

è®ºæ–‡ä¸­ï¼ŒMF = çŸ©é˜µåˆ†è§£ã€‚æ•°å­—è¶Šé«˜è¶Šå¥½ã€‚

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è¿˜å¯ä»¥çœ‹åˆ° LightFM åœ¨å†·å¯åŠ¨ç”šè‡³çƒ­å¯åŠ¨è®¾ç½®ä¸­è¶…è¶Šäº†å…¶ä»–æ–¹æ³•ã€‚å¾ˆé«˜å…´çœ‹åˆ° LightFM åœ¨çƒ­å¯åŠ¨è®¾ç½®ä¸­ä¸æ¯” MF å·®ï¼Œä½†ä¸»è¦çš„å–ç‚¹æ˜¯**LightFM åœ¨å†·å¯åŠ¨è®¾ç½®ä¸­å®Œå…¨å‡»è´¥äº† MF**ã€‚

> ***è®°ä½ï¼š*** *AUC ä¸º 0.5 æ„å‘³ç€éšæœºçŒœæµ‹ï¼Œå³æŸç”¨æˆ·éšæœºæŒ‘é€‰çš„* ***ç›¸å…³é¡¹ç›®*** *çš„è¯„åˆ†é«˜äºè¯¥ç”¨æˆ·çš„* ***éšæœºé€‰æ‹©çš„éç›¸å…³é¡¹ç›®*** *çš„æ¦‚ç‡ä¸º* ***50%****ã€‚*

# ç»“è®º

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†çº¯ååŒæ¨èç³»ç»Ÿï¼ˆä¾‹å¦‚çŸ©é˜µåˆ†è§£ï¼‰åœ¨é¢å¯¹æ–°ç”¨æˆ·æˆ–æ–°é¡¹ç›®æ—¶é‡åˆ°çš„é—®é¢˜ï¼Œè¿™è¢«ç§°ä¸ºå†·å¯åŠ¨é—®é¢˜ã€‚

ä¸€æ—¦æˆ‘ä»¬è·å¾—æ›´å¤šå…³äºç”¨æˆ·å’Œé¡¹ç›®çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬å¯ä»¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œå› ä¸ºæ¨¡å‹å¯ä»¥å­¦ä¹ ä¸€äº›ä¸€èˆ¬æ€§çš„æ¨¡å¼ï¼Œä¾‹å¦‚å¹´è½»äººä¸å–œæ¬¢è€ç”µå½±ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ªæ–°ç”¨æˆ·ï¼Œå¹¶ä¸”çŸ¥é“ä»–ä»¬å¹´è½»ï¼Œå¥½çš„æ¨¡å‹åº”è¯¥å°†è€ç”µå½±çš„è¯„åˆ†ä½äºæ–°ç”µå½±ã€‚

ä¸è¿‡ï¼Œå¦‚æœè¿™ä¸ªæ–°ç”¨æˆ·ç»§ç»­è¯„åˆ†ï¼Œæ¨¡å‹å¯ä»¥è°ƒæ•´å¹¶å­¦ä¼šæ˜¾ç¤ºåƒ [è¯ºæ–¯è´¹æ‹‰å›¾](https://en.wikipedia.org/wiki/Nosferatu) è¿™æ ·çš„è€ç”µå½±ï¼Œå¦‚æœç”¨æˆ·çš„è¡Œä¸ºè¡¨æ˜è¿™å¯èƒ½æ˜¯ä¸€ä¸ªåˆé€‚çš„é€‰æ‹©ã€‚

å…·æœ‰è¿™äº›ç†æƒ³ç‰¹æ€§çš„æ¨¡å‹å¯¹æˆ‘æ¥è¯´æ„Ÿè§‰æœ‰ç‚¹*è´å¶æ–¯*ï¼š

> ç”¨æˆ·å’Œé¡¹ç›®ç‰¹å¾åµŒå…¥ä½œä¸ºä¸€ç§å…ˆéªŒï¼Œå¯¹é¢„æµ‹å…·æœ‰å¾ˆå¤§çš„å½±å“ï¼Œåªè¦æˆ‘ä»¬æ²¡æœ‰äº’åŠ¨æ•°æ®ã€‚éšç€äº’åŠ¨æ•°æ®çš„åˆ°æ¥ï¼Œè¿™ç§å…ˆéªŒä¼šå‘ç”Ÿå˜åŒ–ã€‚

ç„¶è€Œï¼Œä¸€ä¸ªæœ‰è¶£çš„é—®é¢˜æ˜¯ï¼Œä¸€æ—¦æˆ‘ä»¬æ‹¥æœ‰**å¯†é›†è¯„åˆ†çŸ©é˜µ**ï¼Œä¾‹å¦‚æ¯ä¸ªç”¨æˆ·è¯„åˆ†äº† 95%çš„æ‰€æœ‰ç”µå½±ï¼Œç”¨æˆ·å’Œé¡¹ç›®ç‰¹å¾æ˜¯å¦å¤±å»ç›¸å…³æ€§ã€‚

ä¸ç®¡æ€æ ·ï¼ŒLightFM æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å€™é€‰æ¨¡å‹ï¼Œè¿™ä¸€ç‚¹ä»æˆ‘å’Œè®ºæ–‡ä½œè€…çš„å®éªŒä¸­å¯ä»¥çœ‹å‡ºã€‚LightFM åœ¨æˆ‘ä»¬çš„é€‰å®šæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äº MFï¼Œç‰¹åˆ«æ˜¯åœ¨å†·å¯åŠ¨è®¾ç½®ä¸­ã€‚å¦‚æœå†·å¯åŠ¨ä¸æ˜¯é—®é¢˜ï¼Œæ”¹è¿›æ˜¯å¾®å°çš„ï¼Œç”šè‡³å¯èƒ½åªæ˜¯ç»Ÿè®¡å™ªå£°ã€‚

**ä½ ä¹Ÿå¯ä»¥å°è¯•è®ºæ–‡** [**ä½œè€…å¯¹ LightFM çš„å®ç°**](https://github.com/lyst/lightfm)**ã€‚**

# å‚è€ƒæ–‡çŒ®

[1] M. Kula, [ç”¨æˆ·å’Œé¡¹ç›®å†·å¯åŠ¨æ¨èçš„å…ƒæ•°æ®åµŒå…¥](https://arxiv.org/abs/1507.08439) (2015)ï¼ŒarXiv é¢„å°æœ¬ arXiv:1507.08439

æˆ‘å¸Œæœ›ä½ ä»Šå¤©å­¦åˆ°äº†ä¸€äº›æ–°çš„ã€æœ‰è¶£çš„å’Œæœ‰ç”¨çš„ä¸œè¥¿ã€‚æ„Ÿè°¢é˜…è¯»ï¼

> *å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜ï¼Œå¯ä»¥åœ¨* [*LinkedIn*](https://www.linkedin.com/in/dr-robert-k%C3%BCbler-983859150/)*ä¸Šè”ç³»æˆ‘ï¼*

å¦‚æœä½ æƒ³æ·±å…¥äº†è§£ç®—æ³•çš„ä¸–ç•Œï¼Œå¯ä»¥å°è¯•æˆ‘çš„æ–°å‡ºç‰ˆç‰©**å…³äºç®—æ³•çš„ä¸€åˆ‡**ï¼æˆ‘ä»åœ¨å¯»æ‰¾ä½œè€…ï¼

[](https://medium.com/all-about-algorithms?source=post_page-----69bf2f0f0b9b--------------------------------) [## å…³äºç®—æ³•çš„ä¸€åˆ‡

### ä»ç›´è§‚è§£é‡Šåˆ°æ·±å…¥åˆ†æï¼Œç®—æ³•é€šè¿‡ç¤ºä¾‹ã€ä»£ç å’Œç²¾å½©çš„æ–¹å¼å±•ç°å‡ºæ´»åŠ›â€¦â€¦

medium.com](https://medium.com/all-about-algorithms?source=post_page-----69bf2f0f0b9b--------------------------------)
