- en: 'RAG: How to Talk to Your Data'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAG：如何与您的数据交流
- en: 原文：[https://towardsdatascience.com/rag-how-to-talk-to-your-data-eaf5469b83b0?source=collection_archive---------0-----------------------#2023-11-11](https://towardsdatascience.com/rag-how-to-talk-to-your-data-eaf5469b83b0?source=collection_archive---------0-----------------------#2023-11-11)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/rag-how-to-talk-to-your-data-eaf5469b83b0?source=collection_archive---------0-----------------------#2023-11-11](https://towardsdatascience.com/rag-how-to-talk-to-your-data-eaf5469b83b0?source=collection_archive---------0-----------------------#2023-11-11)
- en: Comprehensive guide on how to analyse customer feedback using ChatGPT
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 详细指南：如何使用 ChatGPT 分析客户反馈
- en: '[](https://miptgirl.medium.com/?source=post_page-----eaf5469b83b0--------------------------------)[![Mariya
    Mansurova](../Images/b1dd377b0a1887db900cc5108bca8ea8.png)](https://miptgirl.medium.com/?source=post_page-----eaf5469b83b0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----eaf5469b83b0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----eaf5469b83b0--------------------------------)
    [Mariya Mansurova](https://miptgirl.medium.com/?source=post_page-----eaf5469b83b0--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://miptgirl.medium.com/?source=post_page-----eaf5469b83b0--------------------------------)[![Mariya
    Mansurova](../Images/b1dd377b0a1887db900cc5108bca8ea8.png)](https://miptgirl.medium.com/?source=post_page-----eaf5469b83b0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----eaf5469b83b0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----eaf5469b83b0--------------------------------)
    [Mariya Mansurova](https://miptgirl.medium.com/?source=post_page-----eaf5469b83b0--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F15a29a4fc6ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frag-how-to-talk-to-your-data-eaf5469b83b0&user=Mariya+Mansurova&userId=15a29a4fc6ad&source=post_page-15a29a4fc6ad----eaf5469b83b0---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----eaf5469b83b0--------------------------------)
    ·21 min read·Nov 11, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Feaf5469b83b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frag-how-to-talk-to-your-data-eaf5469b83b0&user=Mariya+Mansurova&userId=15a29a4fc6ad&source=-----eaf5469b83b0---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F15a29a4fc6ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frag-how-to-talk-to-your-data-eaf5469b83b0&user=Mariya+Mansurova&userId=15a29a4fc6ad&source=post_page-15a29a4fc6ad----eaf5469b83b0---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----eaf5469b83b0--------------------------------)
    ·21 分钟阅读·2023 年 11 月 11 日'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feaf5469b83b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frag-how-to-talk-to-your-data-eaf5469b83b0&source=-----eaf5469b83b0---------------------bookmark_footer-----------)![](../Images/07ab6bd0a42c6e657b2f5212e94c83c0.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feaf5469b83b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frag-how-to-talk-to-your-data-eaf5469b83b0&source=-----eaf5469b83b0---------------------bookmark_footer-----------)![](../Images/07ab6bd0a42c6e657b2f5212e94c83c0.png)'
- en: Image by DALL-E 3
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由 DALL-E 3 提供的图像
- en: In my previous articles, we discussed how to do Topic Modelling using ChatGPT.
    Our task was to analyse customer comments for different hotel chains and identify
    the main topics mentioned for each hotel.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的以前的文章中，我们讨论了如何使用 ChatGPT 进行主题建模。我们的任务是分析不同酒店连锁的客户评论，并确定每家酒店提到的主要主题。
- en: As a result of such Topic Modelling, we know topics for each customer review
    and can easily filter by them and dive deeper. However, in real life, it’s impossible
    to have such an exhaustive set of topics that could cover all your possible use
    cases.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样的主题建模，我们知道每个客户评论的主题，可以轻松按主题筛选并深入了解。然而，在现实生活中，拥有能够涵盖所有可能用例的详尽主题集是不可能的。
- en: For example, here’s the list of topics we identified from customer feedback
    earlier.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这是我们从客户反馈中先前识别出的主题列表。
- en: '![](../Images/617d605242d40f7a4640d3b393394cf5.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/617d605242d40f7a4640d3b393394cf5.png)'
- en: These topics can help us get a high-level overview of the customer feedback
    and do initial pre-filtering. But suppose we want to understand what customers
    think about the gym or beverages for breakfast. In that case, we will need to
    go through quite a lot of customer feedback ourselves from “Hotel facilities”
    and “Breakfast” topics.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这些主题可以帮助我们获得客户反馈的高层次概述，并进行初步预筛选。但是，假设我们想了解客户对健身房或早餐饮品的看法。在这种情况下，我们将需要自己从“酒店设施”和“早餐”主题中浏览相当多的客户反馈。
- en: Luckily, LLMs could help us with this analysis and save many hours of going
    through customers’ reviews (even though it still might be helpful to listen to
    the customer’s voice yourself). In this article we will discuss such approaches.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，LLMs 可以帮助我们进行这种分析，节省大量浏览客户评论的时间（尽管自己倾听客户的声音仍可能是有帮助的）。在本文中，我们将讨论这些方法。
- en: We will continue using LangChain (one of the most popular frameworks for LLM
    applications). You can find a basic overview of LangChain in [my previous article](https://medium.com/towards-data-science/topic-modelling-in-production-e3b3e99e4fca).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用 LangChain（最流行的 LLM 应用框架之一）。你可以在[我之前的文章](https://medium.com/towards-data-science/topic-modelling-in-production-e3b3e99e4fca)中找到
    LangChain 的基本概述。
- en: Naive approaches
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 幼稚的方法
- en: The most straightforward way to get comments related to a specific topic is
    just to look for some particular words in the texts, like “gym” or “drink”. I’ve
    been using this approach many times when ChatGPT didn’t exist.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 获取与特定主题相关的评论最直接的方法就是在文本中寻找一些特定的词汇，比如“健身房”或“饮料”。在 ChatGPT 出现之前，我曾多次使用这种方法。
- en: 'The problems with this approach are pretty obvious:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的问题是相当明显的：
- en: You might get quite a lot of not relevant comments about gymnasia nearby or
    alcoholic drinks in the hotel restaurant. Such filters are not specific enough
    and can’t take context into account so that you will have a lot of false positives.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可能会得到很多不相关的关于附近健身房或酒店餐厅酒精饮料的评论。这种过滤器不够具体，不能考虑上下文，因此你会有很多假阳性。
- en: On the other hand, you might not have good enough coverage as well. People tend
    to use slightly different words for the same things (for example, drinks, refreshments,
    beverages, juices, etc). There might be typos. And this task might become even
    more convoluted if your customers speak different languages.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，你可能也无法获得足够好的覆盖范围。人们往往对相同的事物使用略微不同的词汇（例如，饮料、茶点、饮品、果汁等）。可能还会有拼写错误。如果你的客户说不同的语言，这个任务可能会变得更加复杂。
- en: So, this approach has problems both with precision and recall. It will give
    you a rough understanding of the question, but its capabilities are limited.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这种方法在精准度和召回率方面都有问题。它会给你对问题的粗略理解，但能力有限。
- en: 'The other potential solution is to use the same approach as with Topic Modelling:
    send all customer comments to LLM and ask the model to define whether they are
    related to our topic of interest (beverages at breakfast or gym). We can even
    ask the model to sum up all customer feedback and provide a conclusion.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种潜在的解决方案是使用与主题建模相同的方法：将所有客户评论发送给 LLM，并让模型确定它们是否与我们的兴趣主题相关（早餐饮品或健身房）。我们甚至可以要求模型总结所有客户反馈并提供结论。
- en: 'This approach is likely to work pretty well. However, it has its limitations
    too: you will need to send all the documents you have to LLM each time you want
    to dive deeper into a particular topic. Even with high-level filtering based on
    topics we defined, it might be quite a lot of data to pass to LLM, and it will
    be rather costly.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可能会工作得很好。然而，它也有其局限性：每次你想深入探讨一个特定话题时，你需要将所有文档发送给 LLM。即使基于我们定义的主题进行高水平过滤，传递给
    LLM 的数据量也可能相当大，而且成本也会相当高。
- en: Luckily, there is another way to solve this task, and it’s called RAG.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，还有另一种解决这个任务的方法，它被称为 RAG。
- en: Retrieval-augmented generation
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检索增强生成
- en: 'We have a set of documents (customer reviews), and we want to ask questions
    related to the content of these documents (for example, “What do customers like
    about breakfast?”). As we discussed before, we don’t want to send all customer
    reviews to LLM, so we need to have a way to define only the most relevant ones.
    Then, the task will be pretty straightforward: pass the user question and these
    documents as the context to LLM, and that’s it.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一组文档（客户评论），我们希望提出与这些文档内容相关的问题（例如，“客户喜欢早餐的哪些方面？”）。正如我们之前讨论的，我们不想将所有客户评论都发送给
    LLM，因此我们需要一种方法来定义最相关的评论。然后，任务将变得非常简单：将用户问题和这些文档作为上下文传递给 LLM，就可以了。
- en: Such an approach is called [Retrieval-augmented generation](https://python.langchain.com/docs/use_cases/question_answering/)
    or RAG.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法称为[检索增强生成](https://python.langchain.com/docs/use_cases/question_answering/)或RAG。
- en: '![](../Images/eeacde37f8694c0b34865d586c1ff75b.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eeacde37f8694c0b34865d586c1ff75b.png)'
- en: Scheme by author
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的方案
- en: 'The pipeline for RAG consists of the following stages:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: RAG的流水线包括以下几个阶段：
- en: '**Loading documents** from the data sources we have.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加载文档**从我们拥有的数据源。'
- en: '**Splitting documents** into chunks that are easy to use further.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将文档分割**为更容易进一步使用的块。'
- en: '**Storage:** vector stores are often used for this use case to process data
    effectively.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储：** 向量存储通常用于此用例，以有效处理数据。'
- en: '**Retrieval** of relevant to the question documents.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检索**与问题相关的文档。'
- en: '**Generation** is passing a question and relevant documents to LLM and getting
    the final answer**.**'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成**是将问题和相关文档传递给LLM并获得最终答案**。**'
- en: You might have heard that OpenAI launched [Assistant API](https://platform.openai.com/docs/assistants/tools/function-calling)
    this week, which could do all these steps for you. However, I believe it’s worth
    going through the whole process to understand how it works and its peculiarities.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经听说OpenAI本周推出了[助理API](https://platform.openai.com/docs/assistants/tools/function-calling)，它可以为您完成所有这些步骤。但我认为值得通过整个过程来理解它的工作原理及其特殊性。
- en: So, let’s go through all these stages step-by-step.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们逐步了解所有这些阶段。
- en: Loading documents
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载文档
- en: The first step is to load our documents. LangChain supports different document
    types, for example, [CSV](https://python.langchain.com/docs/modules/data_connection/document_loaders/csv)
    or [JSON](https://python.langchain.com/docs/modules/data_connection/document_loaders/json).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是加载我们的文档。LangChain支持不同类型的文档，例如[CSV](https://python.langchain.com/docs/modules/data_connection/document_loaders/csv)或[JSON](https://python.langchain.com/docs/modules/data_connection/document_loaders/json)。
- en: You might wonder what is the benefit of using LangChain for such basic data
    types. It goes without saying that you can parse CSV or JSON files using standard
    Python libraries. However, I recommend using LangChain data loaders API since
    it returns Document objects containing content and metadata. It will be easier
    for you to use LangChain Documents later on.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会想知道使用LangChain处理这些基本数据类型的好处是什么。毫无疑问，您可以使用标准Python库解析CSV或JSON文件。但我建议使用LangChain数据加载器API，因为它返回包含内容和元数据的文档对象。稍后使用LangChain文档会更容易。
- en: Let’s look at a bit more complex examples of data types.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些更复杂的数据类型的例子。
- en: We often have tasks to analyse web page content, so we have to work with HTML.
    Even if you’ve already mastered the [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
    library, you might find [BSHTMLLoader](https://python.langchain.com/docs/modules/data_connection/document_loaders/html)
    helpful.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常需要分析网页内容，因此必须处理HTML。即使您已经掌握了[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)库，您可能会发现[BSHTMLLoader](https://python.langchain.com/docs/modules/data_connection/document_loaders/html)也很有帮助。
- en: What’s interesting about HTML related to LLM applications is that, most likely,
    you will need to preprocess it a lot. If you look at any website using Browser
    Inspector, you will notice much more text than you see on the site. It’s used
    to specify the layout, formatting, styles, etc.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 与LLM应用相关的HTML的有趣之处在于，很可能您需要对其进行大量预处理。如果您使用浏览器检查工具查看任何网站，您会注意到比网站上看到的文本要多得多。它用于指定布局、格式和样式等。
- en: '![](../Images/0dd75068e94af13d284fab15e72ad923.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0dd75068e94af13d284fab15e72ad923.png)'
- en: Image by author, [LangChain documentation](https://python.langchain.com/docs/modules/data_connection/document_loaders/html)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片，[LangChain文档](https://python.langchain.com/docs/modules/data_connection/document_loaders/html)
- en: In most real-life cases, we won’t need to pass all this data to LLM. The whole
    HTML for a site could easily exceed 200K tokens (and only ~10–20% of it will be
    text you see as a user), so it would be challenging to fit it into a context size.
    More than that, this technical info might make the model’s job a bit harder.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数实际情况下，我们不需要将所有这些数据传递给LLM。一个站点的整个HTML很容易超过200K标记（只有用户看到的文本约为10-20%），因此将其适应上下文大小将是一项挑战。而且，这些技术信息可能会让模型的工作变得更加困难。
- en: So, it’s pretty standard to extract only text from HTML and use it for further
    analysis. To do it, you could use the command below. As a result, you will get
    a Document object where text from the web page is in the `page_content` parameter.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，从HTML中提取文本并将其用于进一步分析是相当标准的做法。要做到这一点，你可以使用下面的命令。结果，你将得到一个文档对象，其中网页内容在`page_content`参数中。
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The other commonly used data type is PDF. We can parse PDFs, for example, using
    the PyPDF library. Let’s load text from DALL-E 3 paper.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常用的数据类型是PDF。我们可以解析PDF，例如使用PyPDF库。让我们从DALL-E 3论文中加载文本。
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the output, you will get a set of Documents — one for each page. In metadata,
    both `source` and `page` fields will be populated.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出中，你会得到一组文档 — 每页一个文档。在元数据中，`source`和`page`字段都会被填充。
- en: So, as you can see, LangChain allows you to work with an extensive range of
    different document types.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，正如你所见，LangChain允许你处理广泛的不同文档类型。
- en: Let’s return to our initial task. In our dataset, we have a separate .txt file
    with customer comments for each hotel. We need to parse all files in the directory
    and put them together. We can use `DirectoryLoader` for it.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们最初的任务。在我们的数据集中，每个酒店都有一个单独的.txt文件，其中包含顾客的评论。我们需要解析目录中的所有文件并将它们整合在一起。我们可以使用`DirectoryLoader`来完成这个任务。
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'I’ve also used `’autodetect_encoding’: True` since our texts are encoded not
    in standard UTF-8.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的文本不是标准的UTF-8编码，所以我还使用了`''autodetect_encoding'': True`。'
- en: As a result, we got the list of documents — one document for each text file.
    We know that each document consists of individual customer reviews. It will be
    more effective for us to work with smaller chunks rather than with all customer
    comments for a hotel. So, we need to split our documents. Let’s move on to the
    next stage and discuss document splitting in detail.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 结果，我们得到了文档列表 — 每个文本文件一个文档。我们知道每个文档由独立的客户评论组成。与其处理酒店所有顾客评论的大文本，我们更有效地使用较小的块来处理。因此，我们需要分割我们的文档。让我们继续下一阶段，详细讨论文档分割。
- en: Splitting documents
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文档分割
- en: The next step is to split documents. You might wonder why we need to do this.
    Documents are often long and cover multiple topics, for example, Confluence pages
    or documentation. If we pass such lengthy texts to LLMs, we might face issues
    that either LLM is distracted by irrelevant information or texts don’t fit the
    context size.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是分割文档。也许你会想为什么我们需要这样做。文档通常很长，涵盖多个主题，例如Confluence页面或文档。如果我们将这样的长文本传递给LLMs，我们可能会面临以下问题：要么LLM被无关信息分散注意力，要么文本不适合上下文大小。
- en: So, to work effectively with LLMs, it’s worth defining the most relevant information
    from our knowledge base (set of documents) and passing only this info to the model.
    That’s why we need to split our documents into smaller chunks.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了有效地处理LLMs，值得从我们的知识库（文档集合）中定义最相关的信息，并仅将此信息传递给模型。这就是为什么我们需要将文档分割成较小块的原因。
- en: The most commonly used technique for general texts is [recursive split by character](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter).
    In LangChain, it’s implemented in `RecursiveCharacterTextSplitter` class.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 通常用于一般文本的最常见技术是[递归字符分割](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter)。在LangChain中，它是由`RecursiveCharacterTextSplitter`类实现的。
- en: Let’s try to understand how it works. First, you define a prioritised list of
    characters for the splitter (by default, it’s `["\n\n", "\n", " ", ""]`). Then,
    the splitter goes through this list and tries to split the document by characters
    one by one until it gets small enough chunks. It means that this approach tries
    to keep semantically close parts together (paragraphs, sentences, words) until
    we need to split them to achieve the desired chunk size.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试理解它是如何工作的。首先，你需要定义一个优先级列表用于分割器（默认为`["\n\n", "\n", " ", ""]`）。然后，分割器会逐个字符地遍历这个列表，并尝试将文档分割成足够小的块。这意味着该方法试图保持语义上紧密相关的部分在一起（段落、句子、单词），直到我们需要分割它们以达到期望的块大小。
- en: Let’s use [the Zen of Python](https://peps.python.org/pep-0020/#easter-egg)
    to see how it works. There are 824 characters, 139 words and 21 paragraphs in
    this text.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用[Python之禅](https://peps.python.org/pep-0020/#easter-egg)看看它是如何工作的。这段文字有824个字符，139个单词和21个段落。
- en: You can see the Zen of Python if you execute `import this`.
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你执行`import this`，你可以看到Python之禅。
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Let’s use `RecursiveCharacterTextSplitter` and start with a relatively big chunk
    size equal to 300.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`RecursiveCharacterTextSplitter`，并从相对较大的块大小开始，设为300。
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We will get three chunks: 264, 293 and 263 characters. We could see that all
    sentences are held together.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到三个块：264、293和263个字符。我们可以看到所有的句子都保持在一起。
- en: All images below are made by author.
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 以下所有图像均由作者制作。
- en: '![](../Images/80a3551e4ca8caa9f264b0c457177aac.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80a3551e4ca8caa9f264b0c457177aac.png)'
- en: You might notice a `chunk_overlap` parameter that could allow you to split with
    overlap. It’s important because we will be passing to LLM some chunks with our
    questions, and it’s crucial to have enough context to make decisions based only
    on the information provided in each chunk.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到一个`chunk_overlap`参数，它允许你进行重叠分割。这很重要，因为我们将把一些块和问题一起传递给LLM，而拥有足够的上下文来仅根据每个块中提供的信息做出决策是至关重要的。
- en: '![](../Images/1ea599b673f69b36e3a2e164519cecfb.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ea599b673f69b36e3a2e164519cecfb.png)'
- en: Scheme by author
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 作者方案
- en: Let’s try to add `chunk_overlap`.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试添加`chunk_overlap`。
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now, we have four splits with 264, 232, 297 and 263 characters, and we can see
    that our chunks overlap.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有四个分割块，字符数分别为264、232、297和263，我们可以看到我们的块有重叠。
- en: '![](../Images/efdab3a9394850a6aa0ed1b8be657a27.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/efdab3a9394850a6aa0ed1b8be657a27.png)'
- en: Let’s make the chunk size a bit smaller.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把块的大小稍微调小一点。
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, we even had to split some longer sentences. That’s how recursive split
    works: since after splitting by paragraphs (`"\n"`), chunks are still not small
    enough, the splitter proceeded to `" "`.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们甚至不得不分割一些较长的句子。这就是递归分割的工作原理：由于按段落（`"\n"`）分割后，块仍然不够小，因此分割器继续按`" "`分割。
- en: '![](../Images/a4799d99b63f977376473a7390690c76.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4799d99b63f977376473a7390690c76.png)'
- en: 'You can customise the split even further. For example, you could specify `length_function
    = lambda x: len(x.split("\n"))` to use the number of paragraphs as the chunk length
    instead of the number of characters. It’s also quite common to [split by tokens](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/split_by_token)
    because LLMs have limited context sizes based on the number of tokens.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '你可以进一步自定义分割。例如，你可以指定`length_function = lambda x: len(x.split("\n"))`来使用段落的数量作为块的长度，而不是字符的数量。按[标记分割](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/split_by_token)也很常见，因为LLM的上下文大小基于标记的数量。'
- en: The other potential customisation is to use other `separators` to prefer to
    split by `","` instead of `" "` . Let’s try to use it with a couple of sentences.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种潜在的自定义方式是使用其他`separators`，而不是用`","`而是用`" "`来分隔。让我们尝试用几句话来使用它。
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: It works, but commas are not in the right places.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 它有效，但逗号的位置不对。
- en: '![](../Images/93224fc16ae8c68ae604642a25f39b4c.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93224fc16ae8c68ae604642a25f39b4c.png)'
- en: To fix this issue, we could use regexp with lookback as a separator.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们可以使用带回顾的正则表达式作为分隔符。
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now it’s fixed.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在已修复。
- en: '![](../Images/b0d44768c377383b9f49024919ac2fab.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b0d44768c377383b9f49024919ac2fab.png)'
- en: Also, LangChain provides [tools for working with code](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/code_splitter)
    so that your texts are split based on separators specific to programming languages.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，LangChain 提供了[处理代码的工具](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/code_splitter)，可以根据编程语言特定的分隔符来分割文本。
- en: However, in our case, the situation is more straightforward. We know we have
    individual independent comments delimited by `"\n"` in each file, and we just
    need to split by it. Unfortunately, LangChain doesn’t support such a basic use
    case, so we need to do a bit of hacking to make it work as we want to.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们的情况下，情况更简单。我们知道每个文件中有用`"\n"`分隔的独立评论，我们只需按此分隔即可。不幸的是，LangChain 不支持这种基本用例，因此我们需要进行一些黑客操作以使其按我们想要的方式工作。
- en: '[PRE9]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You can find more details on why we need a hack here in [my previous article
    about LangChain](https://medium.com/towards-data-science/topic-modelling-in-production-e3b3e99e4fca).
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可以在[我之前关于 LangChain 的文章](https://medium.com/towards-data-science/topic-modelling-in-production-e3b3e99e4fca)中找到更多关于我们为什么需要这个
    hack 的详细信息。
- en: The significant part of the documents is metadata since it can give more context
    about where this chunk came from. In our case, LangChain automatically populated
    the `source` parameter for metadata so that we know which hotel each comment is
    related to.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 文档的重要部分是元数据，因为它可以提供有关该块来源的更多上下文。在我们的例子中，LangChain 自动填充了元数据的`source`参数，因此我们知道每条评论涉及哪个酒店。
- en: '![](../Images/ca49a0b04359615bfb5dc8939c97fcdc.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ca49a0b04359615bfb5dc8939c97fcdc.png)'
- en: There are some other approaches (i.e. for [HTML](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/HTML_header_metadata)
    or [Markdown](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/markdown_header_metadata))
    that add titles to metadata while splitting documents. These methods could be
    quite helpful if you’re working with such data types.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他方法（例如用于[HTML](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/HTML_header_metadata)或[Markdown](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/markdown_header_metadata)的方法），它们在拆分文档时添加标题到元数据。如果您正在处理这些数据类型，这些方法可能非常有帮助。
- en: Vector stores
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量存储
- en: Now we have comment texts and next step is to learn how to store them effectively
    so that we could get relevant documents for our questions.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有评论文本，下一步是学习如何有效地存储它们，以便我们可以获得相关的文档来回答我们的问题。
- en: We could store comments as strings, but it won’t help us to solve this task
    — we won’t be able to filter customer reviews relevant to the question.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将评论存储为字符串，但这对我们解决这个任务没有帮助——我们无法过滤与问题相关的客户评论。
- en: A much more functional solution is to store documents’ embeddings.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 更加功能强大的解决方案是存储文档的嵌入。
- en: Embeddings are high-dimensional vectors. Embeddings capture semantical meanings
    and relationships between words and phrases so that semantically close texts will
    have a smaller distance between them.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是高维向量。嵌入捕捉单词和短语之间的语义含义和关系，因此语义上接近的文本之间的距离较小。
- en: 'We will be using [OpenAI Embeddings](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings)
    since they are pretty popular. OpenAI advises using the `text-embedding-ada-002`
    model since it has better performance, more extended context and lower price.
    As usual, it has [its risks and limitations](https://platform.openai.com/docs/guides/embeddings/limitations-risks):
    potential social bias and limited knowledge about recent events.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用[OpenAI嵌入](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings)，因为它们非常流行。OpenAI建议使用`text-embedding-ada-002`模型，因为它具有更好的性能、更广泛的上下文和更低的价格。像往常一样，它有[其风险和限制](https://platform.openai.com/docs/guides/embeddings/limitations-risks)：潜在的社会偏见和对最近事件的有限了解。
- en: Let’s try to use Embeddings on toy examples to see how it works.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试在玩具示例上使用嵌入来看看它的工作原理。
- en: '[PRE10]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We can use `*np.dot*` as cosine similarity because OpenAI embeddings are already
    normed.
  id: totrans-105
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们可以使用`*np.dot*`作为余弦相似度，因为OpenAI嵌入已经被归一化。
- en: We can see that the first and the third vectors are close to each other, while
    the second one differs. The first and third sentences have similar semantical
    meanings (they are both about the room size), while the second sentence is not
    close, talking about the weather. So, distances between embeddings actually reflect
    the semantical similarity between texts.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到第一和第三个向量彼此接近，而第二个向量不同。第一和第三个句子有类似的语义含义（它们都是关于房间大小），而第二个句子不接近，讨论天气。因此，嵌入之间的距离实际上反映了文本之间的语义相似性。
- en: '![](../Images/42b71fa8f8b25c311ff5de86ee2ec70e.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/42b71fa8f8b25c311ff5de86ee2ec70e.png)'
- en: Now, we know how to convert comments into numeric vectors. The next question
    is how we should store it so that this data is easily accessible.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们知道如何将评论转换为数值向量。下一个问题是如何存储这些数据，以便轻松访问。
- en: 'Let’s think about our use case. Our flow will be:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一下我们的用例。我们的流程将是：
- en: get a question,
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取一个问题，
- en: calculate its embedding,
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算其嵌入，
- en: find the most relevant document chunks related to this question (the ones with
    the smallest distance to this embedding),
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到与此问题相关的最相关的文档块（与此嵌入距离最小的文档块），
- en: finally, pass found chunks to LLM as a context along with the initial question.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，将找到的块作为上下文与初始问题一起传递给LLM。
- en: The regular task for the data storage will be to find K nearest vectors (K most
    relevant documents). So, we will need to calculate the distance (in our case,
    [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity)) between
    our question’s embedding and all the vectors we have.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 数据存储的常规任务是找到K个最近的向量（K个最相关的文档）。因此，我们需要计算我们问题的嵌入与我们拥有的所有向量之间的距离（在我们的情况下，[余弦相似度](https://zh.wikipedia.org/wiki/%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E5%BA%A6)）。
- en: Generic databases (like Snowflake or Postgres) will perform poorly for such
    a task. But there are databases optimised, especially for this use case — vector
    databases.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 通用数据库（如Snowflake或Postgres）在这样的任务中表现不佳。但是有些数据库被优化，特别适合这种用例——向量数据库。
- en: We will be using an open-source embedding database, [Chroma](https://www.trychroma.com/).
    Chroma is a lightweight in-memory DB, so it’s ideal for prototyping. You can find
    much more options for vector stores [here](https://python.langchain.com/docs/integrations/vectorstores/).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个开源嵌入数据库，[Chroma](https://www.trychroma.com/)。Chroma 是一个轻量级的内存数据库，非常适合原型设计。你可以在[这里](https://python.langchain.com/docs/integrations/vectorstores/)找到更多的向量存储选项。
- en: First, we need to install Chroma using pip.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要使用pip安装Chroma。
- en: '[PRE11]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We will use `persist_directory` to store our data locally and reload it from
    disk.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`persist_directory`来将数据本地存储并从磁盘重新加载。
- en: '[PRE12]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: To be able to load data from disk when you need it next time, execute the following
    command.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在下次需要时能够从磁盘加载数据，请执行以下命令。
- en: '[PRE13]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The database initialisation might take a couple of minutes since Chroma needs
    to load all documents and get their embeddings using OpenAI API.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库初始化可能需要几分钟时间，因为Chroma需要加载所有文档并使用OpenAI API获取它们的嵌入。
- en: We can see that all documents have been loaded.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到所有文档已经加载完毕。
- en: '[PRE14]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now, we could use a similarity search to find top customer comments about staff
    politeness.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用相似性搜索来查找关于员工礼貌的顶级客户评论。
- en: '[PRE15]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Documents look pretty relevant to the question.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 文档看起来与问题非常相关。
- en: '![](../Images/a7a65fa196ff784ba87b1a0ba09450a5.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a7a65fa196ff784ba87b1a0ba09450a5.png)'
- en: We have stored our customer comments in an accessible way, and it’s time to
    discuss retrieval in more detail.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经以可访问的方式存储了客户评论，现在是时候更详细地讨论检索了。
- en: Retrieval
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检索
- en: 'We’ve already used `vectordb.similarity_search` to retrieve the most related
    chunks to the question. In most cases, such an approach will work for you, but
    there could be some nuances:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使用了`vectordb.similarity_search`来检索与问题最相关的块。在大多数情况下，这种方法将对你有效，但可能会有一些细节：
- en: '**Lack of diversity** — The model might return extremely close texts (even
    duplicates), which won’t add much new information to LLM.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多样性缺乏** — 模型可能会返回极其相似的文本（甚至重复），这不会给LLM带来多少新信息。'
- en: '**Not taking into account metadata** —`similarity_search` doesn’t take into
    account the metadata information we have. For example, if I query the top-5 comments
    for the question “breakfast in Travelodge Farringdon”, only three comments in
    the result will have the source equal to `uk_england_london_travelodge_london_farringdon`.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**未考虑元数据** — `similarity_search`不会考虑我们拥有的元数据。例如，如果我查询问题“Travelodge Farringdon的早餐”的前五条评论，结果中只有三条评论的来源等于`uk_england_london_travelodge_london_farringdon`。'
- en: '**Context size limitation** — as usual, we have limited LLM context size and
    need to fit our documents into it.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文大小限制** — 和往常一样，我们有有限的LLM上下文大小，需要将文档适配到其中。'
- en: Let’s discuss techniques that could help us to solve these problems.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一下可以帮助我们解决这些问题的技术。
- en: '**Addressing Diversity — MMR (Maximum Marginal Relevance)**'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决多样性问题 — MMR（最大边际相关性）**'
- en: Similarity search returns the most close responses to your question. But to
    provide the complete information to the model, you might want not to focus on
    the most similar texts. For example, for the question “breakfast in Travelodge
    Farringdon”, the top five customer reviews might be about coffee. If we look only
    at them, we will miss other comments mentioning eggs or staff behaviour and get
    somewhat limited view on the customer feedback.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 相似性搜索返回与你的问题最接近的响应。但为了向模型提供完整的信息，你可能不想只关注最相似的文本。例如，对于问题“Travelodge Farringdon的早餐”，前五条客户评论可能都关于咖啡。如果我们仅查看这些评论，就会错过其他提到鸡蛋或员工行为的评论，从而对客户反馈有一定的局限性。
- en: 'We could use the MMR (Maximum Marginal Relevance) approach to increase the
    diversity of customer comments. It works pretty straightforward:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用MMR（最大边际相关性）方法来增加客户评论的多样性。它的工作原理非常简单：
- en: First, we get `fetch_k` the most similar docs to the question using `similarity_search`
    .
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们使用`similarity_search`获取`fetch_k`与问题最相似的文档。
- en: Then, we picked up `k` the most diverse among them.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们选择了`k`中最具多样性的那些。
- en: '![](../Images/8caa8c4cf29380abbba2ec74ada30a1f.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8caa8c4cf29380abbba2ec74ada30a1f.png)'
- en: Scheme by author
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 作者方案
- en: If we want to use MMR, we should use `max_marginal_relevance_search` instead
    of `similarity_search` and specify `fetch_k` number. It’s worth keeping `fetch_k`
    relatively small so that you don’t have irrelevant answers in the output. That’s
    it.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想使用MMR，我们应该使用`max_marginal_relevance_search`而不是`similarity_search`，并指定`fetch_k`数量。值得保持`fetch_k`相对较小，以便输出中不会有不相关的答案。就这些。
- en: '[PRE16]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Let’s look at the examples for the same query. We got more diverse feedback
    this time. There’s even a comment with negative sentiment.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下相同查询的示例。这次我们收到了更多样化的反馈，甚至还有带有负面情绪的评论。
- en: '![](../Images/2eac3daac518b2051659eea0af9bbbd3.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2eac3daac518b2051659eea0af9bbbd3.png)'
- en: '**Addressing specificity — LLM-aided retrieval**'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决特异性问题 — LLM辅助检索**'
- en: 'The other problem is that we don’t take into account the metadata while retrieving
    documents. To solve it, we can ask LLM to split the initial question into two
    parts:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是我们在检索文档时没有考虑元数据。为了解决这个问题，我们可以让LLM将初始问题拆分为两部分：
- en: semantical filter based on document texts,
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于文档文本的语义过滤器，
- en: filter based on metadata we have.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于我们拥有的元数据进行过滤，
- en: This approach is called [“Self querying”](https://python.langchain.com/docs/modules/data_connection/retrievers/self_query).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法被称为[“自查询”](https://python.langchain.com/docs/modules/data_connection/retrievers/self_query)。
- en: First, let’s add a manual filter specifying a `source` parameter with the filename
    related to Travelodge Farringdon hotel.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们添加一个手动过滤器，指定与Travelodge Farringdon酒店相关的`source`参数的文件名。
- en: '[PRE17]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now, let’s try to use LLM to come up with such a filter automatically. We need
    to describe all our metadata parameters in detail and then use `SelfQueryRetriever`.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试使用LLM自动生成这样的过滤器。我们需要详细描述所有元数据参数，然后使用`SelfQueryRetriever`。
- en: '[PRE18]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Our case is tricky since the `source` parameter in the metadata consists of
    multiple fields: country, city, hotel chain and location. It’s worth splitting
    such complex parameters into more granular ones in such situations so that the
    model can easily understand how to use metadata filters.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的情况很棘手，因为元数据中的`source`参数包含多个字段：国家、城市、酒店连锁和位置。在这种情况下，将如此复杂的参数拆分为更详细的子参数是值得的，以便模型可以更容易地理解如何使用元数据过滤器。
- en: However, with a detailed prompt, it worked and returned only documents related
    to Travelodge Farringdon. But I must confess, it took me several iterations to
    achieve this result.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，通过详细的提示，它确实有效，并仅返回了与Travelodge Farringdon相关的文档。但我必须承认，这花了我几个迭代才达到这个结果。
- en: Let’s switch on debug and see how it works. To enter debug mode, you just need
    to execute the code below.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开启调试模式看看它的工作情况。要进入调试模式，只需执行下面的代码。
- en: '[PRE19]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The complete prompt is pretty long, so let’s look at the main parts of it. Here’s
    the prompt’s start, which gives the model an overview of what we expect and the
    main criteria for the result.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的提示非常长，所以让我们看看它的主要部分。这是提示的开头，给模型一个我们期望的概述和结果的主要标准。
- en: '![](../Images/dde2c1f718e302800303ff0a749009de.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dde2c1f718e302800303ff0a749009de.png)'
- en: Then, the few-shot prompting technique is used, and the model is provided with
    two examples of input and expected output. Here’s one of the examples.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用少量示例提示技术，模型提供了两个输入和期望输出的示例。这是其中一个示例。
- en: '![](../Images/9a18e1d83aa3cba03202f7f72f2e30c0.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9a18e1d83aa3cba03202f7f72f2e30c0.png)'
- en: We are not using a chat model like ChatGPT but general LLM (not fine-tuned on
    instructions). It’s trained just to predict the following tokens for the text.
    That’s why we finished our prompt with our question and the string `Structured
    output:` expecting the model to provide the answer.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并没有使用像ChatGPT这样的聊天模型，而是使用通用的LLM（没有针对指令进行微调）。它只是训练来预测文本的后续标记。这就是为什么我们以问题和字符串`Structured
    output:`结束提示，期待模型提供答案的原因。
- en: '![](../Images/5282f1b7a1d3110c851092eba1c448e3.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5282f1b7a1d3110c851092eba1c448e3.png)'
- en: 'As a result, we got from the model the initial question split into two parts:
    semantic one (`breakfast`) and metadata filters (`source = hotels/london/uk_england_london_travelodge_london_farringdon`)'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 结果，我们从模型那里得到的初始问题被拆分为两部分：语义部分（`breakfast`）和元数据过滤器（`source = hotels/london/uk_england_london_travelodge_london_farringdon`）
- en: '![](../Images/5fcd1ac6561fb29d2dd90b4421eb65f2.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5fcd1ac6561fb29d2dd90b4421eb65f2.png)'
- en: Then, we used this logic to retrieve documents from our vector store and got
    only documents we need.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用了这种逻辑从我们的向量存储中检索文档，并仅获取了我们需要的文档。
- en: '**Addressing size limitations — Compression**'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决大小限制 — 压缩**'
- en: The other technique for retrieval that might be handy is compression. Even though
    GPT 4 Turbo has a context size of 128K tokens, it’s still limited. That’s why
    we might want to preprocess documents and extract only relevant parts.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种可能有用的检索技术是压缩。尽管GPT 4 Turbo的上下文大小为128K标记，但它仍然有限。因此，我们可能需要预处理文档并仅提取相关部分。
- en: 'The main advantages are:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 主要优势有：
- en: You will be able to fit more documents and information into the final prompt
    since they will be condensed.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您将能够将更多文档和信息整合到最终提示中，因为它们将被压缩。
- en: You will get better, more focused results because the non-relevant context will
    be cleaned during preprocessing.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您将会得到更好、更集中的结果，因为在预处理期间将清除非相关的上下文。
- en: These benefits come with the cost — you will have more calls to LLM for compression,
    which means lower speed and higher price.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这些好处是有代价的 — 您将需要更多的LLM调用来进行压缩，这意味着更慢的速度和更高的价格。
- en: You can find more info about this technique in [the docs](https://python.langchain.com/docs/modules/data_connection/retrievers/contextual_compression/).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[文档](https://python.langchain.com/docs/modules/data_connection/retrievers/contextual_compression/)中找到有关此技术的更多信息。
- en: '![](../Images/158eac2e3af993c03b952a02ff8b848b.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/158eac2e3af993c03b952a02ff8b848b.png)'
- en: Scheme by author
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提出的方案
- en: Actually, we can even combine techniques and use MMR here. We used `ContextualCompressionRetriever`
    to get results. Also, we specified that we want just three documents in return.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们甚至可以结合技术并在这里使用MMR。我们使用`ContextualCompressionRetriever`来获取结果。此外，我们指定了我们只想要三个文档作为返回结果。
- en: '[PRE20]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As usual, understanding how it works under the hood is the most exciting part.
    If we look at actual calls, there are three calls to LLM to extract only relevant
    information from the text. Here’s an example.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，了解其内部运作方式是最有趣的部分。如果我们看实际调用，可以看到有三次调用LLM来从文本中提取仅相关信息的情况。这里有一个例子。
- en: '![](../Images/81e83d6794873f70fbdf0c74d5390dbd.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81e83d6794873f70fbdf0c74d5390dbd.png)'
- en: In the output, we got only part of the sentence related to breakfast, so compression
    helps.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出中，我们只得到了与早餐相关的部分句子，所以压缩有所帮助。
- en: '![](../Images/4568d36d28b10db7323cd8bd5bb86c55.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4568d36d28b10db7323cd8bd5bb86c55.png)'
- en: 'There are many more beneficial approaches for retrieval, for example, techniques
    from classic NLP: [SVM](https://python.langchain.com/docs/integrations/retrievers/svm)
    or [TF-IDF](https://python.langchain.com/docs/integrations/retrievers/tf_idf).
    Different retrievers might be helpful in different situations, so I recommend
    you compare different versions for your task and select the most suitable one
    for your use case.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多更有利的检索方法，例如经典自然语言处理技术：[支持向量机（SVM）](https://python.langchain.com/docs/integrations/retrievers/svm)或者[TF-IDF](https://python.langchain.com/docs/integrations/retrievers/tf_idf)。不同的检索器可能在不同情况下有所帮助，因此我建议您为您的任务比较不同版本，并选择最适合您使用情况的版本。
- en: Generation
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成
- en: 'Finally, we got to the last stage: we will combine everything and generate
    the final answer.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们来到了最后阶段：我们将所有内容合并并生成最终答案。
- en: 'Here’s a scheme on how it all will work:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是它们将如何运作的一个方案：
- en: we get a question from a user,
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们收到了用户的一个问题，
- en: we retrieve relevant documents for this question from the vector store using
    embeddings,
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们从向量存储中使用嵌入检索了此问题的相关文档，
- en: we pass the initial question along with retrieved documents to the LLM and get
    the final answer.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将初始问题与从嵌入中检索到的相关文档一起传递给LLM，并获得最终答案。
- en: '![](../Images/5e3e0cfc54ba45e0085e7fbca218baec.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e3e0cfc54ba45e0085e7fbca218baec.png)'
- en: Scheme by author
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提出的方案
- en: In LangChain, we could use `RetrievalQA` chain to implement this flow quickly.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在LangChain中，我们可以使用`RetrievalQA`链快速实现这一流程。
- en: '[PRE21]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Let’s look at the call to ChatGPT. As you can see, we passed retrieved documents
    along with the user query.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下对ChatGPT的调用。正如您所见，我们将检索到的文档与用户查询一起传递。
- en: '![](../Images/123f5083f0c9c4e4bfedae15e9d50a1a.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/123f5083f0c9c4e4bfedae15e9d50a1a.png)'
- en: Here’s an output from the model.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是模型的输出。
- en: '![](../Images/f1b887953657f01038cc79a9bc1101c2.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1b887953657f01038cc79a9bc1101c2.png)'
- en: We can tweak the model’s behaviour, customising prompt. For example, we could
    ask the model to be more concise.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以调整模型的行为，定制提示。例如，我们可以要求模型更加简洁。
- en: '[PRE22]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We got a much shorter answer this time. Also, since we specified `return_source_documents=True`,
    we got a set of documents in return. It could be helpful for debugging.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这次我们得到了一个更短的答案。此外，由于我们指定了`return_source_documents=True`，我们得到了一组返回的文档。这对于调试可能有帮助。
- en: '![](../Images/ac6a6d4dd1e70ed16df741ceacce5658.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac6a6d4dd1e70ed16df741ceacce5658.png)'
- en: As we’ve seen, all retrieved documents are combined in one prompt by default.
    This approach is excellent and straightforward since it invokes only one call
    to LLM. The only limitation is that your documents must fit the context size.
    If they don’t, you need to apply more complex techniques.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，所有检索到的文档默认都合并在一个提示中。这种方法既优秀又简单，因为只需要一个调用来执行语言模型。唯一的限制是您的文档必须符合上下文大小。如果不符合，您需要应用更复杂的技术。
- en: Let’s look at different chain types that could allow us to work with any number
    of documents. The first one is MapReduce.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看不同的链类型，它们可以让我们处理任意数量的文档。第一个是MapReduce。
- en: 'This approach is similar to classical [MapReduce](https://en.wikipedia.org/wiki/MapReduce):
    we generate answers based on each retrieved document (map stage) and then combine
    these answers into the final one (reduce stage).'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法类似于经典的[MapReduce](https://en.wikipedia.org/wiki/MapReduce)：我们根据每个检索到的文档生成答案（map阶段），然后将这些答案合并成最终答案（reduce阶段）。
- en: '![](../Images/94d4aa804326b3615e79dddf6a55fbe0.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94d4aa804326b3615e79dddf6a55fbe0.png)'
- en: Scheme by author
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 作者的方案
- en: The limitations of all such approaches are cost and speed. Instead of one call
    to LLM, you need to do a call for each retrieved document.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些方法的局限性在于成本和速度。你需要为每个检索到的文档进行一次调用，而不是一次调用LLM。
- en: Regarding code, we just need to specify `chain_type="map_reduce"` to change
    behaviour.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 关于代码，我们只需要指定`chain_type="map_reduce"`以改变行为。
- en: '[PRE23]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In the result, we got the following output.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 结果，我们得到了以下输出。
- en: '![](../Images/035d0587917428467a88ed5dff0bfdd5.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/035d0587917428467a88ed5dff0bfdd5.png)'
- en: Let’s see how it works using debug mode. Since it’s a MapReduce, we first sent
    each document to LLM and got the answer based on this chunk. Here’s an example
    of prompt for one of the chunks.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在调试模式下看看它是如何工作的。由于这是MapReduce，我们首先将每个文档发送到LLM，并根据这个块得到答案。以下是其中一个块的提示示例。
- en: '![](../Images/284f539a3a8f40a15ee11cd9fb855b3a.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/284f539a3a8f40a15ee11cd9fb855b3a.png)'
- en: Then, we combine all the results and ask LLM to come up with the final answer.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将所有结果结合起来，并要求LLM给出最终答案。
- en: '![](../Images/5a3a248a9afc2448760caf5dad93b6fd.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a3a248a9afc2448760caf5dad93b6fd.png)'
- en: That’s it.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。
- en: There is another drawback specific to the MapReduce approach. The model sees
    each document separately and doesn’t have them all in the same context, which
    might lead to worse results.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce方法还有另一个特定的缺点。模型分别看到每个文档，而不是将它们全部放在同一上下文中，这可能导致更差的结果。
- en: We can overcome this drawback with the Refine chain type. Then, we will look
    at documents sequentially and allow the model to refine the answer on each iteration.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过Refine链类型克服这个缺点。然后，我们将按顺序查看文档，并允许模型在每次迭代时细化答案。
- en: '![](../Images/f1e06fac8143b3a7f59e5395d3b27cbb.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1e06fac8143b3a7f59e5395d3b27cbb.png)'
- en: Scheme by author
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 作者的方案
- en: Again, we just need to change `chain_type` to test another approach.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们只需要更改`chain_type`以测试另一种方法。
- en: '[PRE24]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: With the Refine chain, we got a bit more wordy and complete answer.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Refine链，我们得到了一个更详细和完整的答案。
- en: '![](../Images/6335d9a32845b1e132d6419e17a4e4c2.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6335d9a32845b1e132d6419e17a4e4c2.png)'
- en: Let’s see how it works using debug. For the first chunk, we are starting from
    scratch.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用调试模式看看它是如何工作的。对于第一个块，我们从头开始。
- en: '![](../Images/0929294c4835fcf65dcf04f5540fb070.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0929294c4835fcf65dcf04f5540fb070.png)'
- en: Then, we pass the current answer and a new chunk and give the model a chance
    to refine its answer.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们传递当前的答案和一个新的块，并给模型一个机会来细化其答案。
- en: '![](../Images/b191a4c12c728851bbe8ca57f4840d9c.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b191a4c12c728851bbe8ca57f4840d9c.png)'
- en: Then, we repeat the refining prompt for each remaining retrieved document and
    get the final result.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们对每个剩余的检索文档重复细化提示，最终得到结果。
- en: That’s all that I wanted to tell you today. Let’s do a quick recap.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 今天我想告诉你的就这些。让我们快速回顾一下。
- en: Summary
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this article, we went through the whole process of Retrieval-augmented generation:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们详细介绍了检索增强生成的整个过程：
- en: We’ve looked at different data loaders.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们已经查看了不同的数据加载器。
- en: We’ve discussed possible approaches to data splitting and their potential nuances.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们讨论了数据拆分的可能方法及其潜在的细微差别。
- en: We’ve learned what embeddings are and set up a vector store to access data effectively.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们了解了什么是嵌入，并建立了一个向量存储库以有效地访问数据。
- en: We’ve found different solutions for retrieval issues and learned how to increase
    diversity, to overcome context size limitations and to use metadata.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们找到了检索问题的不同解决方案，并学习了如何增加多样性、克服上下文大小限制以及使用元数据。
- en: Finally, we’ve used the `RetrievalQA` chain to generate the answer based on
    our data and compared different chain types.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们使用了`RetrievalQA`链来生成基于我们数据的答案，并比较了不同的链类型。
- en: This knowledge should be enough for start building something similar with your
    data.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这些知识应该足够你开始使用自己的数据构建类似的东西。
- en: Thank you a lot for reading this article. I hope it was insightful to you. If
    you have any follow-up questions or comments, please leave them in the comments
    section.
  id: totrans-241
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 非常感谢您阅读本文。希望本文对您有所启发。如果您有任何后续问题或评论，请在评论部分留言。
- en: Dataset
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集
- en: '*Ganesan, Kavita and Zhai, ChengXiang. (2011). OpinRank Review Dataset.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '*Ganesan, Kavita 和 Zhai, ChengXiang. (2011). OpinRank 评论数据集.'
- en: UCI Machine Learning Repository (CC BY 4.0).* [*https://doi.org/10.24432/C5QW4W*](https://doi.org/10.24432/C5QW4W.)
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: UCI 机器学习库 (CC BY 4.0).* [*https://doi.org/10.24432/C5QW4W*](https://doi.org/10.24432/C5QW4W.)
- en: Reference
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: 'This article is based on information from the courses:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 本文基于以下课程信息：
- en: '[“LangChain for LLM Application Development”](https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/)
    by DeepLearning.AI and LangChain,'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“LLM 应用开发的 LangChain”](https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/)由
    DeepLearning.AI 和 LangChain 提供，'
- en: '[“LangChain: Chat with your data”](https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/)
    by DeepLearning.AI and LangChain.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“LangChain：与您的数据聊天”](https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/)由
    DeepLearning.AI 和 LangChain 提供。'
