- en: 'RAG: How to Talk to Your Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/rag-how-to-talk-to-your-data-eaf5469b83b0?source=collection_archive---------0-----------------------#2023-11-11](https://towardsdatascience.com/rag-how-to-talk-to-your-data-eaf5469b83b0?source=collection_archive---------0-----------------------#2023-11-11)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Comprehensive guide on how to analyse customer feedback using ChatGPT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://miptgirl.medium.com/?source=post_page-----eaf5469b83b0--------------------------------)[![Mariya
    Mansurova](../Images/b1dd377b0a1887db900cc5108bca8ea8.png)](https://miptgirl.medium.com/?source=post_page-----eaf5469b83b0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----eaf5469b83b0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----eaf5469b83b0--------------------------------)
    [Mariya Mansurova](https://miptgirl.medium.com/?source=post_page-----eaf5469b83b0--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F15a29a4fc6ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frag-how-to-talk-to-your-data-eaf5469b83b0&user=Mariya+Mansurova&userId=15a29a4fc6ad&source=post_page-15a29a4fc6ad----eaf5469b83b0---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----eaf5469b83b0--------------------------------)
    ·21 min read·Nov 11, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Feaf5469b83b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frag-how-to-talk-to-your-data-eaf5469b83b0&user=Mariya+Mansurova&userId=15a29a4fc6ad&source=-----eaf5469b83b0---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feaf5469b83b0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frag-how-to-talk-to-your-data-eaf5469b83b0&source=-----eaf5469b83b0---------------------bookmark_footer-----------)![](../Images/07ab6bd0a42c6e657b2f5212e94c83c0.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image by DALL-E 3
  prefs: []
  type: TYPE_NORMAL
- en: In my previous articles, we discussed how to do Topic Modelling using ChatGPT.
    Our task was to analyse customer comments for different hotel chains and identify
    the main topics mentioned for each hotel.
  prefs: []
  type: TYPE_NORMAL
- en: As a result of such Topic Modelling, we know topics for each customer review
    and can easily filter by them and dive deeper. However, in real life, it’s impossible
    to have such an exhaustive set of topics that could cover all your possible use
    cases.
  prefs: []
  type: TYPE_NORMAL
- en: For example, here’s the list of topics we identified from customer feedback
    earlier.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/617d605242d40f7a4640d3b393394cf5.png)'
  prefs: []
  type: TYPE_IMG
- en: These topics can help us get a high-level overview of the customer feedback
    and do initial pre-filtering. But suppose we want to understand what customers
    think about the gym or beverages for breakfast. In that case, we will need to
    go through quite a lot of customer feedback ourselves from “Hotel facilities”
    and “Breakfast” topics.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, LLMs could help us with this analysis and save many hours of going
    through customers’ reviews (even though it still might be helpful to listen to
    the customer’s voice yourself). In this article we will discuss such approaches.
  prefs: []
  type: TYPE_NORMAL
- en: We will continue using LangChain (one of the most popular frameworks for LLM
    applications). You can find a basic overview of LangChain in [my previous article](https://medium.com/towards-data-science/topic-modelling-in-production-e3b3e99e4fca).
  prefs: []
  type: TYPE_NORMAL
- en: Naive approaches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most straightforward way to get comments related to a specific topic is
    just to look for some particular words in the texts, like “gym” or “drink”. I’ve
    been using this approach many times when ChatGPT didn’t exist.
  prefs: []
  type: TYPE_NORMAL
- en: 'The problems with this approach are pretty obvious:'
  prefs: []
  type: TYPE_NORMAL
- en: You might get quite a lot of not relevant comments about gymnasia nearby or
    alcoholic drinks in the hotel restaurant. Such filters are not specific enough
    and can’t take context into account so that you will have a lot of false positives.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, you might not have good enough coverage as well. People tend
    to use slightly different words for the same things (for example, drinks, refreshments,
    beverages, juices, etc). There might be typos. And this task might become even
    more convoluted if your customers speak different languages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, this approach has problems both with precision and recall. It will give
    you a rough understanding of the question, but its capabilities are limited.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other potential solution is to use the same approach as with Topic Modelling:
    send all customer comments to LLM and ask the model to define whether they are
    related to our topic of interest (beverages at breakfast or gym). We can even
    ask the model to sum up all customer feedback and provide a conclusion.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach is likely to work pretty well. However, it has its limitations
    too: you will need to send all the documents you have to LLM each time you want
    to dive deeper into a particular topic. Even with high-level filtering based on
    topics we defined, it might be quite a lot of data to pass to LLM, and it will
    be rather costly.'
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, there is another way to solve this task, and it’s called RAG.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval-augmented generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have a set of documents (customer reviews), and we want to ask questions
    related to the content of these documents (for example, “What do customers like
    about breakfast?”). As we discussed before, we don’t want to send all customer
    reviews to LLM, so we need to have a way to define only the most relevant ones.
    Then, the task will be pretty straightforward: pass the user question and these
    documents as the context to LLM, and that’s it.'
  prefs: []
  type: TYPE_NORMAL
- en: Such an approach is called [Retrieval-augmented generation](https://python.langchain.com/docs/use_cases/question_answering/)
    or RAG.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eeacde37f8694c0b34865d586c1ff75b.png)'
  prefs: []
  type: TYPE_IMG
- en: Scheme by author
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline for RAG consists of the following stages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Loading documents** from the data sources we have.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Splitting documents** into chunks that are easy to use further.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Storage:** vector stores are often used for this use case to process data
    effectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retrieval** of relevant to the question documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generation** is passing a question and relevant documents to LLM and getting
    the final answer**.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You might have heard that OpenAI launched [Assistant API](https://platform.openai.com/docs/assistants/tools/function-calling)
    this week, which could do all these steps for you. However, I believe it’s worth
    going through the whole process to understand how it works and its peculiarities.
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s go through all these stages step-by-step.
  prefs: []
  type: TYPE_NORMAL
- en: Loading documents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step is to load our documents. LangChain supports different document
    types, for example, [CSV](https://python.langchain.com/docs/modules/data_connection/document_loaders/csv)
    or [JSON](https://python.langchain.com/docs/modules/data_connection/document_loaders/json).
  prefs: []
  type: TYPE_NORMAL
- en: You might wonder what is the benefit of using LangChain for such basic data
    types. It goes without saying that you can parse CSV or JSON files using standard
    Python libraries. However, I recommend using LangChain data loaders API since
    it returns Document objects containing content and metadata. It will be easier
    for you to use LangChain Documents later on.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a bit more complex examples of data types.
  prefs: []
  type: TYPE_NORMAL
- en: We often have tasks to analyse web page content, so we have to work with HTML.
    Even if you’ve already mastered the [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
    library, you might find [BSHTMLLoader](https://python.langchain.com/docs/modules/data_connection/document_loaders/html)
    helpful.
  prefs: []
  type: TYPE_NORMAL
- en: What’s interesting about HTML related to LLM applications is that, most likely,
    you will need to preprocess it a lot. If you look at any website using Browser
    Inspector, you will notice much more text than you see on the site. It’s used
    to specify the layout, formatting, styles, etc.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0dd75068e94af13d284fab15e72ad923.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author, [LangChain documentation](https://python.langchain.com/docs/modules/data_connection/document_loaders/html)
  prefs: []
  type: TYPE_NORMAL
- en: In most real-life cases, we won’t need to pass all this data to LLM. The whole
    HTML for a site could easily exceed 200K tokens (and only ~10–20% of it will be
    text you see as a user), so it would be challenging to fit it into a context size.
    More than that, this technical info might make the model’s job a bit harder.
  prefs: []
  type: TYPE_NORMAL
- en: So, it’s pretty standard to extract only text from HTML and use it for further
    analysis. To do it, you could use the command below. As a result, you will get
    a Document object where text from the web page is in the `page_content` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The other commonly used data type is PDF. We can parse PDFs, for example, using
    the PyPDF library. Let’s load text from DALL-E 3 paper.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the output, you will get a set of Documents — one for each page. In metadata,
    both `source` and `page` fields will be populated.
  prefs: []
  type: TYPE_NORMAL
- en: So, as you can see, LangChain allows you to work with an extensive range of
    different document types.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s return to our initial task. In our dataset, we have a separate .txt file
    with customer comments for each hotel. We need to parse all files in the directory
    and put them together. We can use `DirectoryLoader` for it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'I’ve also used `’autodetect_encoding’: True` since our texts are encoded not
    in standard UTF-8.'
  prefs: []
  type: TYPE_NORMAL
- en: As a result, we got the list of documents — one document for each text file.
    We know that each document consists of individual customer reviews. It will be
    more effective for us to work with smaller chunks rather than with all customer
    comments for a hotel. So, we need to split our documents. Let’s move on to the
    next stage and discuss document splitting in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting documents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next step is to split documents. You might wonder why we need to do this.
    Documents are often long and cover multiple topics, for example, Confluence pages
    or documentation. If we pass such lengthy texts to LLMs, we might face issues
    that either LLM is distracted by irrelevant information or texts don’t fit the
    context size.
  prefs: []
  type: TYPE_NORMAL
- en: So, to work effectively with LLMs, it’s worth defining the most relevant information
    from our knowledge base (set of documents) and passing only this info to the model.
    That’s why we need to split our documents into smaller chunks.
  prefs: []
  type: TYPE_NORMAL
- en: The most commonly used technique for general texts is [recursive split by character](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter).
    In LangChain, it’s implemented in `RecursiveCharacterTextSplitter` class.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try to understand how it works. First, you define a prioritised list of
    characters for the splitter (by default, it’s `["\n\n", "\n", " ", ""]`). Then,
    the splitter goes through this list and tries to split the document by characters
    one by one until it gets small enough chunks. It means that this approach tries
    to keep semantically close parts together (paragraphs, sentences, words) until
    we need to split them to achieve the desired chunk size.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use [the Zen of Python](https://peps.python.org/pep-0020/#easter-egg)
    to see how it works. There are 824 characters, 139 words and 21 paragraphs in
    this text.
  prefs: []
  type: TYPE_NORMAL
- en: You can see the Zen of Python if you execute `import this`.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Let’s use `RecursiveCharacterTextSplitter` and start with a relatively big chunk
    size equal to 300.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We will get three chunks: 264, 293 and 263 characters. We could see that all
    sentences are held together.'
  prefs: []
  type: TYPE_NORMAL
- en: All images below are made by author.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/80a3551e4ca8caa9f264b0c457177aac.png)'
  prefs: []
  type: TYPE_IMG
- en: You might notice a `chunk_overlap` parameter that could allow you to split with
    overlap. It’s important because we will be passing to LLM some chunks with our
    questions, and it’s crucial to have enough context to make decisions based only
    on the information provided in each chunk.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1ea599b673f69b36e3a2e164519cecfb.png)'
  prefs: []
  type: TYPE_IMG
- en: Scheme by author
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try to add `chunk_overlap`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now, we have four splits with 264, 232, 297 and 263 characters, and we can see
    that our chunks overlap.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/efdab3a9394850a6aa0ed1b8be657a27.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s make the chunk size a bit smaller.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we even had to split some longer sentences. That’s how recursive split
    works: since after splitting by paragraphs (`"\n"`), chunks are still not small
    enough, the splitter proceeded to `" "`.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a4799d99b63f977376473a7390690c76.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can customise the split even further. For example, you could specify `length_function
    = lambda x: len(x.split("\n"))` to use the number of paragraphs as the chunk length
    instead of the number of characters. It’s also quite common to [split by tokens](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/split_by_token)
    because LLMs have limited context sizes based on the number of tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: The other potential customisation is to use other `separators` to prefer to
    split by `","` instead of `" "` . Let’s try to use it with a couple of sentences.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: It works, but commas are not in the right places.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/93224fc16ae8c68ae604642a25f39b4c.png)'
  prefs: []
  type: TYPE_IMG
- en: To fix this issue, we could use regexp with lookback as a separator.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now it’s fixed.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b0d44768c377383b9f49024919ac2fab.png)'
  prefs: []
  type: TYPE_IMG
- en: Also, LangChain provides [tools for working with code](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/code_splitter)
    so that your texts are split based on separators specific to programming languages.
  prefs: []
  type: TYPE_NORMAL
- en: However, in our case, the situation is more straightforward. We know we have
    individual independent comments delimited by `"\n"` in each file, and we just
    need to split by it. Unfortunately, LangChain doesn’t support such a basic use
    case, so we need to do a bit of hacking to make it work as we want to.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You can find more details on why we need a hack here in [my previous article
    about LangChain](https://medium.com/towards-data-science/topic-modelling-in-production-e3b3e99e4fca).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The significant part of the documents is metadata since it can give more context
    about where this chunk came from. In our case, LangChain automatically populated
    the `source` parameter for metadata so that we know which hotel each comment is
    related to.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ca49a0b04359615bfb5dc8939c97fcdc.png)'
  prefs: []
  type: TYPE_IMG
- en: There are some other approaches (i.e. for [HTML](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/HTML_header_metadata)
    or [Markdown](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/markdown_header_metadata))
    that add titles to metadata while splitting documents. These methods could be
    quite helpful if you’re working with such data types.
  prefs: []
  type: TYPE_NORMAL
- en: Vector stores
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we have comment texts and next step is to learn how to store them effectively
    so that we could get relevant documents for our questions.
  prefs: []
  type: TYPE_NORMAL
- en: We could store comments as strings, but it won’t help us to solve this task
    — we won’t be able to filter customer reviews relevant to the question.
  prefs: []
  type: TYPE_NORMAL
- en: A much more functional solution is to store documents’ embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings are high-dimensional vectors. Embeddings capture semantical meanings
    and relationships between words and phrases so that semantically close texts will
    have a smaller distance between them.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be using [OpenAI Embeddings](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings)
    since they are pretty popular. OpenAI advises using the `text-embedding-ada-002`
    model since it has better performance, more extended context and lower price.
    As usual, it has [its risks and limitations](https://platform.openai.com/docs/guides/embeddings/limitations-risks):
    potential social bias and limited knowledge about recent events.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try to use Embeddings on toy examples to see how it works.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We can use `*np.dot*` as cosine similarity because OpenAI embeddings are already
    normed.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We can see that the first and the third vectors are close to each other, while
    the second one differs. The first and third sentences have similar semantical
    meanings (they are both about the room size), while the second sentence is not
    close, talking about the weather. So, distances between embeddings actually reflect
    the semantical similarity between texts.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/42b71fa8f8b25c311ff5de86ee2ec70e.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, we know how to convert comments into numeric vectors. The next question
    is how we should store it so that this data is easily accessible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s think about our use case. Our flow will be:'
  prefs: []
  type: TYPE_NORMAL
- en: get a question,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: calculate its embedding,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: find the most relevant document chunks related to this question (the ones with
    the smallest distance to this embedding),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: finally, pass found chunks to LLM as a context along with the initial question.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The regular task for the data storage will be to find K nearest vectors (K most
    relevant documents). So, we will need to calculate the distance (in our case,
    [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity)) between
    our question’s embedding and all the vectors we have.
  prefs: []
  type: TYPE_NORMAL
- en: Generic databases (like Snowflake or Postgres) will perform poorly for such
    a task. But there are databases optimised, especially for this use case — vector
    databases.
  prefs: []
  type: TYPE_NORMAL
- en: We will be using an open-source embedding database, [Chroma](https://www.trychroma.com/).
    Chroma is a lightweight in-memory DB, so it’s ideal for prototyping. You can find
    much more options for vector stores [here](https://python.langchain.com/docs/integrations/vectorstores/).
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to install Chroma using pip.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We will use `persist_directory` to store our data locally and reload it from
    disk.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: To be able to load data from disk when you need it next time, execute the following
    command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The database initialisation might take a couple of minutes since Chroma needs
    to load all documents and get their embeddings using OpenAI API.
  prefs: []
  type: TYPE_NORMAL
- en: We can see that all documents have been loaded.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Now, we could use a similarity search to find top customer comments about staff
    politeness.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Documents look pretty relevant to the question.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a7a65fa196ff784ba87b1a0ba09450a5.png)'
  prefs: []
  type: TYPE_IMG
- en: We have stored our customer comments in an accessible way, and it’s time to
    discuss retrieval in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ve already used `vectordb.similarity_search` to retrieve the most related
    chunks to the question. In most cases, such an approach will work for you, but
    there could be some nuances:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lack of diversity** — The model might return extremely close texts (even
    duplicates), which won’t add much new information to LLM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Not taking into account metadata** —`similarity_search` doesn’t take into
    account the metadata information we have. For example, if I query the top-5 comments
    for the question “breakfast in Travelodge Farringdon”, only three comments in
    the result will have the source equal to `uk_england_london_travelodge_london_farringdon`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context size limitation** — as usual, we have limited LLM context size and
    need to fit our documents into it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s discuss techniques that could help us to solve these problems.
  prefs: []
  type: TYPE_NORMAL
- en: '**Addressing Diversity — MMR (Maximum Marginal Relevance)**'
  prefs: []
  type: TYPE_NORMAL
- en: Similarity search returns the most close responses to your question. But to
    provide the complete information to the model, you might want not to focus on
    the most similar texts. For example, for the question “breakfast in Travelodge
    Farringdon”, the top five customer reviews might be about coffee. If we look only
    at them, we will miss other comments mentioning eggs or staff behaviour and get
    somewhat limited view on the customer feedback.
  prefs: []
  type: TYPE_NORMAL
- en: 'We could use the MMR (Maximum Marginal Relevance) approach to increase the
    diversity of customer comments. It works pretty straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we get `fetch_k` the most similar docs to the question using `similarity_search`
    .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, we picked up `k` the most diverse among them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/8caa8c4cf29380abbba2ec74ada30a1f.png)'
  prefs: []
  type: TYPE_IMG
- en: Scheme by author
  prefs: []
  type: TYPE_NORMAL
- en: If we want to use MMR, we should use `max_marginal_relevance_search` instead
    of `similarity_search` and specify `fetch_k` number. It’s worth keeping `fetch_k`
    relatively small so that you don’t have irrelevant answers in the output. That’s
    it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Let’s look at the examples for the same query. We got more diverse feedback
    this time. There’s even a comment with negative sentiment.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2eac3daac518b2051659eea0af9bbbd3.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Addressing specificity — LLM-aided retrieval**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The other problem is that we don’t take into account the metadata while retrieving
    documents. To solve it, we can ask LLM to split the initial question into two
    parts:'
  prefs: []
  type: TYPE_NORMAL
- en: semantical filter based on document texts,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: filter based on metadata we have.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This approach is called [“Self querying”](https://python.langchain.com/docs/modules/data_connection/retrievers/self_query).
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s add a manual filter specifying a `source` parameter with the filename
    related to Travelodge Farringdon hotel.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s try to use LLM to come up with such a filter automatically. We need
    to describe all our metadata parameters in detail and then use `SelfQueryRetriever`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Our case is tricky since the `source` parameter in the metadata consists of
    multiple fields: country, city, hotel chain and location. It’s worth splitting
    such complex parameters into more granular ones in such situations so that the
    model can easily understand how to use metadata filters.'
  prefs: []
  type: TYPE_NORMAL
- en: However, with a detailed prompt, it worked and returned only documents related
    to Travelodge Farringdon. But I must confess, it took me several iterations to
    achieve this result.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s switch on debug and see how it works. To enter debug mode, you just need
    to execute the code below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The complete prompt is pretty long, so let’s look at the main parts of it. Here’s
    the prompt’s start, which gives the model an overview of what we expect and the
    main criteria for the result.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dde2c1f718e302800303ff0a749009de.png)'
  prefs: []
  type: TYPE_IMG
- en: Then, the few-shot prompting technique is used, and the model is provided with
    two examples of input and expected output. Here’s one of the examples.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9a18e1d83aa3cba03202f7f72f2e30c0.png)'
  prefs: []
  type: TYPE_IMG
- en: We are not using a chat model like ChatGPT but general LLM (not fine-tuned on
    instructions). It’s trained just to predict the following tokens for the text.
    That’s why we finished our prompt with our question and the string `Structured
    output:` expecting the model to provide the answer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5282f1b7a1d3110c851092eba1c448e3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As a result, we got from the model the initial question split into two parts:
    semantic one (`breakfast`) and metadata filters (`source = hotels/london/uk_england_london_travelodge_london_farringdon`)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5fcd1ac6561fb29d2dd90b4421eb65f2.png)'
  prefs: []
  type: TYPE_IMG
- en: Then, we used this logic to retrieve documents from our vector store and got
    only documents we need.
  prefs: []
  type: TYPE_NORMAL
- en: '**Addressing size limitations — Compression**'
  prefs: []
  type: TYPE_NORMAL
- en: The other technique for retrieval that might be handy is compression. Even though
    GPT 4 Turbo has a context size of 128K tokens, it’s still limited. That’s why
    we might want to preprocess documents and extract only relevant parts.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main advantages are:'
  prefs: []
  type: TYPE_NORMAL
- en: You will be able to fit more documents and information into the final prompt
    since they will be condensed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will get better, more focused results because the non-relevant context will
    be cleaned during preprocessing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These benefits come with the cost — you will have more calls to LLM for compression,
    which means lower speed and higher price.
  prefs: []
  type: TYPE_NORMAL
- en: You can find more info about this technique in [the docs](https://python.langchain.com/docs/modules/data_connection/retrievers/contextual_compression/).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/158eac2e3af993c03b952a02ff8b848b.png)'
  prefs: []
  type: TYPE_IMG
- en: Scheme by author
  prefs: []
  type: TYPE_NORMAL
- en: Actually, we can even combine techniques and use MMR here. We used `ContextualCompressionRetriever`
    to get results. Also, we specified that we want just three documents in return.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: As usual, understanding how it works under the hood is the most exciting part.
    If we look at actual calls, there are three calls to LLM to extract only relevant
    information from the text. Here’s an example.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/81e83d6794873f70fbdf0c74d5390dbd.png)'
  prefs: []
  type: TYPE_IMG
- en: In the output, we got only part of the sentence related to breakfast, so compression
    helps.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4568d36d28b10db7323cd8bd5bb86c55.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are many more beneficial approaches for retrieval, for example, techniques
    from classic NLP: [SVM](https://python.langchain.com/docs/integrations/retrievers/svm)
    or [TF-IDF](https://python.langchain.com/docs/integrations/retrievers/tf_idf).
    Different retrievers might be helpful in different situations, so I recommend
    you compare different versions for your task and select the most suitable one
    for your use case.'
  prefs: []
  type: TYPE_NORMAL
- en: Generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Finally, we got to the last stage: we will combine everything and generate
    the final answer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a scheme on how it all will work:'
  prefs: []
  type: TYPE_NORMAL
- en: we get a question from a user,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we retrieve relevant documents for this question from the vector store using
    embeddings,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we pass the initial question along with retrieved documents to the LLM and get
    the final answer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/5e3e0cfc54ba45e0085e7fbca218baec.png)'
  prefs: []
  type: TYPE_IMG
- en: Scheme by author
  prefs: []
  type: TYPE_NORMAL
- en: In LangChain, we could use `RetrievalQA` chain to implement this flow quickly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Let’s look at the call to ChatGPT. As you can see, we passed retrieved documents
    along with the user query.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/123f5083f0c9c4e4bfedae15e9d50a1a.png)'
  prefs: []
  type: TYPE_IMG
- en: Here’s an output from the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f1b887953657f01038cc79a9bc1101c2.png)'
  prefs: []
  type: TYPE_IMG
- en: We can tweak the model’s behaviour, customising prompt. For example, we could
    ask the model to be more concise.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We got a much shorter answer this time. Also, since we specified `return_source_documents=True`,
    we got a set of documents in return. It could be helpful for debugging.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ac6a6d4dd1e70ed16df741ceacce5658.png)'
  prefs: []
  type: TYPE_IMG
- en: As we’ve seen, all retrieved documents are combined in one prompt by default.
    This approach is excellent and straightforward since it invokes only one call
    to LLM. The only limitation is that your documents must fit the context size.
    If they don’t, you need to apply more complex techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at different chain types that could allow us to work with any number
    of documents. The first one is MapReduce.
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach is similar to classical [MapReduce](https://en.wikipedia.org/wiki/MapReduce):
    we generate answers based on each retrieved document (map stage) and then combine
    these answers into the final one (reduce stage).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/94d4aa804326b3615e79dddf6a55fbe0.png)'
  prefs: []
  type: TYPE_IMG
- en: Scheme by author
  prefs: []
  type: TYPE_NORMAL
- en: The limitations of all such approaches are cost and speed. Instead of one call
    to LLM, you need to do a call for each retrieved document.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding code, we just need to specify `chain_type="map_reduce"` to change
    behaviour.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: In the result, we got the following output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/035d0587917428467a88ed5dff0bfdd5.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s see how it works using debug mode. Since it’s a MapReduce, we first sent
    each document to LLM and got the answer based on this chunk. Here’s an example
    of prompt for one of the chunks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/284f539a3a8f40a15ee11cd9fb855b3a.png)'
  prefs: []
  type: TYPE_IMG
- en: Then, we combine all the results and ask LLM to come up with the final answer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5a3a248a9afc2448760caf5dad93b6fd.png)'
  prefs: []
  type: TYPE_IMG
- en: That’s it.
  prefs: []
  type: TYPE_NORMAL
- en: There is another drawback specific to the MapReduce approach. The model sees
    each document separately and doesn’t have them all in the same context, which
    might lead to worse results.
  prefs: []
  type: TYPE_NORMAL
- en: We can overcome this drawback with the Refine chain type. Then, we will look
    at documents sequentially and allow the model to refine the answer on each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f1e06fac8143b3a7f59e5395d3b27cbb.png)'
  prefs: []
  type: TYPE_IMG
- en: Scheme by author
  prefs: []
  type: TYPE_NORMAL
- en: Again, we just need to change `chain_type` to test another approach.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: With the Refine chain, we got a bit more wordy and complete answer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6335d9a32845b1e132d6419e17a4e4c2.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s see how it works using debug. For the first chunk, we are starting from
    scratch.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0929294c4835fcf65dcf04f5540fb070.png)'
  prefs: []
  type: TYPE_IMG
- en: Then, we pass the current answer and a new chunk and give the model a chance
    to refine its answer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b191a4c12c728851bbe8ca57f4840d9c.png)'
  prefs: []
  type: TYPE_IMG
- en: Then, we repeat the refining prompt for each remaining retrieved document and
    get the final result.
  prefs: []
  type: TYPE_NORMAL
- en: That’s all that I wanted to tell you today. Let’s do a quick recap.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this article, we went through the whole process of Retrieval-augmented generation:'
  prefs: []
  type: TYPE_NORMAL
- en: We’ve looked at different data loaders.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ve discussed possible approaches to data splitting and their potential nuances.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ve learned what embeddings are and set up a vector store to access data effectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ve found different solutions for retrieval issues and learned how to increase
    diversity, to overcome context size limitations and to use metadata.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we’ve used the `RetrievalQA` chain to generate the answer based on
    our data and compared different chain types.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This knowledge should be enough for start building something similar with your
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you a lot for reading this article. I hope it was insightful to you. If
    you have any follow-up questions or comments, please leave them in the comments
    section.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Ganesan, Kavita and Zhai, ChengXiang. (2011). OpinRank Review Dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: UCI Machine Learning Repository (CC BY 4.0).* [*https://doi.org/10.24432/C5QW4W*](https://doi.org/10.24432/C5QW4W.)
  prefs: []
  type: TYPE_NORMAL
- en: Reference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This article is based on information from the courses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[“LangChain for LLM Application Development”](https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/)
    by DeepLearning.AI and LangChain,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[“LangChain: Chat with your data”](https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/)
    by DeepLearning.AI and LangChain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
