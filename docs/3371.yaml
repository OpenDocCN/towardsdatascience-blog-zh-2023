- en: Testing the Consistency of Reported Machine Learning Performance Scores by the
    mlscorecheck Package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/testing-the-consistency-of-reported-machine-learning-performance-scores-by-the-mlscorecheck-package-e1baaba57610?source=collection_archive---------6-----------------------#2023-11-12](https://towardsdatascience.com/testing-the-consistency-of-reported-machine-learning-performance-scores-by-the-mlscorecheck-package-e1baaba57610?source=collection_archive---------6-----------------------#2023-11-12)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/fa56cf93c52f06cc70ff8f8ba26f6c98.png)'
  prefs: []
  type: TYPE_IMG
- en: AI (Dall-E) generated depiction of the topic
  prefs: []
  type: TYPE_NORMAL
- en: A small step towards the big leap of reproducible machine learning science
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@gyuriofkovacs?source=post_page-----e1baaba57610--------------------------------)[![Gyorgy
    Kovacs](../Images/aa5d1fcc59d738acc1056de3f0cbe7ca.png)](https://medium.com/@gyuriofkovacs?source=post_page-----e1baaba57610--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e1baaba57610--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e1baaba57610--------------------------------)
    [Gyorgy Kovacs](https://medium.com/@gyuriofkovacs?source=post_page-----e1baaba57610--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4563dd81810c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftesting-the-consistency-of-reported-machine-learning-performance-scores-by-the-mlscorecheck-package-e1baaba57610&user=Gyorgy+Kovacs&userId=4563dd81810c&source=post_page-4563dd81810c----e1baaba57610---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e1baaba57610--------------------------------)
    ·11 min read·Nov 12, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe1baaba57610&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftesting-the-consistency-of-reported-machine-learning-performance-scores-by-the-mlscorecheck-package-e1baaba57610&user=Gyorgy+Kovacs&userId=4563dd81810c&source=-----e1baaba57610---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe1baaba57610&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftesting-the-consistency-of-reported-machine-learning-performance-scores-by-the-mlscorecheck-package-e1baaba57610&source=-----e1baaba57610---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we explore how the Python package [mlscorecheck](https://github.com/FalseNegativeLab/mlscorecheck)
    can be used for testing the consistency between reported machine learning performance
    scores and the accompanying descriptions of experimental setups.
  prefs: []
  type: TYPE_NORMAL
- en: '*Disclaimer: the author of this post is the author of the mlscorecheck package.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**What is the consistency testing of performance scores?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Assume you come across accuracy (0.9494), sensitivity (0.8523), and specificity
    (0.9765) scores reported for a binary classification problem with a testset consisting
    of 100 positive and 1000 negative samples. Can you trust these scores? How can
    you check if they could truly be the outcome of the claimed experiment? This is
    where the `mlscorecheck` package can help you by providing such consistency testing
    capabilities. In this particular example, one can exploit
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: and the `'insconsistency’` flag of the result being `False` indicates that the
    scores could be yielded from the experiment. (Which is true, since the scores
    correspond to 81 true positive and 850 true negative samples.) What if the accuracy
    score 0.8474 was reported due to an accidental typo?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Testing the adjusted setup, the result signals inconsistency: the scores could
    not be the outcome of the experiment. Either the scores or the assumed experimental
    setup is incorrect.'
  prefs: []
  type: TYPE_NORMAL
- en: In the rest of the post, we take a closer look on the main features and use
    cases of the [mlscorecheck](https://github.com/FalseNegativeLab/mlscorecheck)
    package.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In both research and applications, supervised learning approaches are routinely
    ranked by performance scores calculated in some experiments ([binary classification](https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers),
    [multiclass classification](/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f),
    [regression](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics)).
    Due to typos in the publications, [improperly used statistics](https://doi.org/10.1055/s-0033-1359421),
    [data leakage](https://www.cell.com/patterns/abstract/S2666-3899(23)00159-9),
    and [cosmetics](https://doi.org/10.1371/journal.pone.0005738), in many cases the
    reported performance scores are unreliable. Beyond contributing to the [reproducibility
    crisis](https://en.wikipedia.org/wiki/Replication_crisis) in machine learning
    and artificial intelligence, the effect of unrealistically high performance scores
    is usually further amplified by the [publication bias](https://en.wikipedia.org/wiki/Publication_bias),
    eventually [skewing entire fields](https://doi.org/10.1016/j.artmed.2020.101987)
    of research.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of the [mlscorecheck](https://github.com/FalseNegativeLab/mlscorecheck)
    package is to provide numerical techniques **to test if a set of reported performance
    scores could be the outcome of an assumed experimental setup**.
  prefs: []
  type: TYPE_NORMAL
- en: '**The operation of consistency tests**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The idea behind consistency testing is that in a given experimental setup,
    performance scores cannot take any values independently:'
  prefs: []
  type: TYPE_NORMAL
- en: For example, if there are 100 positive samples in a binary classification testset,
    the [sensitivity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) score
    can only take the values 0.0, 0.01, 0.02, …, 1.0, but it cannot be 0.8543.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When multiple performance scores are reported, they need to be consistent with
    each other. For example, [accuracy](https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers)
    is the weighted average of [sensitivity and specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity),
    hence, in a binary classification problem with a testset of 100 positive and 100
    negative samples, the scores acc = 0.96, sens = 0.91, spec = 0.97 cannot be yielded.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In more complex experimental setups (involving [k-fold cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)),
    the aggregation of the scores across multiple folds/datasets, etc.), the constraints
    become more advanced, but they still exist. The [mlscorecheck](https://github.com/FalseNegativeLab/mlscorecheck)
    package implements numerical tests to check if the scores assumed to be yielded
    from an experiment satisfy the corresponding constraints.
  prefs: []
  type: TYPE_NORMAL
- en: '**The tests are numerical, inconsistencies are identified conclusively, with
    certainty.** Drawing an analogy with statistical hypothesis testing, the null-hypothesis
    is that there are no inconsistencies, and whenever some inconsistency is identified,
    it provides evidence against the null-hypothesis, but being a numerical test,
    this evidence is indisputable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Various experimental setups impose various constraints on the performance scores
    that need dedicated solutions. The tests implemented in the package are based
    on three principles: exhaustive enumeration expedited by interval computing; linear
    integer programming; analytical relations between the scores. The sensitivity
    of the tests highly depends on the experimental setup and the numerical uncertainty:
    large datasets, large numerical uncertainty and a small number of reported scores
    reduce the ability of the tests to recognize deviations from the assumed evaluation
    protocols. Nevertheless, as we see later on, the tests are still applicable in
    many real life scenarios. For further details on the mathematical background of
    the tests, refer to the [preprint](https://www.researchgate.net/publication/374845553_Testing_the_Consistency_of_Performance_Scores_Reported_for_Binary_Classification_Problems)
    and the [documentation](https://mlscorecheck.readthedocs.io/en/latest/).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Use cases**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we explore some examples illustrating the use of the package, but first,
    we discuss the general requirements of testing and some terms used to describe
    the experiments.
  prefs: []
  type: TYPE_NORMAL
- en: '**The requirements**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Consistency testing has three requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: the collection of **reported performance scores**;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the **estimated numerical uncertainty** of the scores (when the scores are truncated
    to *4* decimal places, one can assume that the real values are within the range
    of 0.0001 from the reported values, and this is the numerical uncertainty of the
    scores) — this is usually the *eps* parameter of the tests which is simply inferred
    by inspecting the scores;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the **details of the experiment** (the statistics of the dataset(s) involved,
    the cross-validation scheme, the mode of aggregation).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Glossary**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The terms used in the specifications of the experiments:'
  prefs: []
  type: TYPE_NORMAL
- en: '*mean of scores* (MoS): the scores are calculated for each fold/dataset, and
    then averaged to gain the reported ones;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*score of means* (SoM): the fold/dataset level raw figures (e.g. confusion
    matrices) are averaged first, and the scores are calculated from the average figures;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*micro-average*: the evaluation of a multiclass problem is carried out by measuring
    the performance on each class against all other (as a binary classification),
    and the class-level results are aggregated in the *score of means* fashion;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*macro-average*: the same as the micro-average, but the class level scores
    are aggregated in the *mean of scores* fashion;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*fold configuration*: when k-fold cross-validation is used, the tests usually
    rely on linear integer programming. Knowing the number of samples of classes in
    the folds can be utilized in the formation of the linear program. These fold level
    class sample counts are referred to as the fold configuration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Binary classification**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the beginning, we already illustrated the use of the package when binary
    classification scores calculated on a single testset are to be tested. Now, we
    look into some more advanced examples.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the two examples we investigate in detail, the package supports
    altogether 10 experimental setups for binary classification, the list of which
    can be found in the [documentation](https://mlscorecheck.readthedocs.io/en/latest/)
    with further examples in the sample [notebooks](https://github.com/FalseNegativeLab/mlscorecheck/tree/main/notebooks/illustration).
  prefs: []
  type: TYPE_NORMAL
- en: '**N testsets, score-of-means aggregation**'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we assume that there are N testsets, k-folding is not involved,
    but the scores are aggregated in the score-of-means fashion, that is, the raw
    true positive and true negative figures are determined for each testset and the
    performance scores are calculated from the total (or average) number of true positive
    and true negative figures. The available scores are assumed to be the [accuracy](https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers),
    [negative predictive value](https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers)
    and the [F1-score](https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers).
  prefs: []
  type: TYPE_NORMAL
- en: For example, in practice, the evaluation of an image segmentation technique
    on N test images stored in one tensor usually leads to this scenario.
  prefs: []
  type: TYPE_NORMAL
- en: 'The design of the package is such that the details of the experimental setup
    are encoded in the names of the test functions, in this way guiding the user to
    take care of all available details of the experiment when choosing the suitable
    tests. In this case, the suitable test is the function `check_n_testsets_som_no_kfold`
    in the `mlscorecheck.check.binary` module, the token `''som’` referring to the
    mode of aggregation (score of means):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The result indicates that the scores could be the outcome of the experiment.
    No wonder, the scores are prepared by sampling true positive and true negative
    counts for the testsets and calculating them in the specified manner. However,
    if one of the scores is slightly changed, for example F1 is modified to 0.3191,
    the configuration becomes inconsistent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Further details of the analysis, for example, the evidence for feasibility can
    be extracted from the dictionaries returned by the test functions. For the structure
    of the outputs, again, see the [documentation](https://mlscorecheck.readthedocs.io/en/latest/).
  prefs: []
  type: TYPE_NORMAL
- en: '**1 dataset, k-fold cross-validation, mean of scores aggregation**'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we assume that there is a dataset on which a binary classifier
    is evaluated in a stratified repeated k-fold cross-validation manner (2 folds,
    3 repetitions), and the mean of the scores yielded on the folds is reported.
  prefs: []
  type: TYPE_NORMAL
- en: This experimental setup is possibly the most commonly used in supervised machine
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: We highlight the distinction between **knowing** and **not knowing** the *fold
    configuration*. Typically, MoS tests rely on linear integer programming and the
    fold configuration is required to formulate the linear integer program. The fold
    configuration might be specified by listing the statistics of the folds, or one
    can refer to a folding strategy leading to deterministic fold statistics, such
    as *stratification*. Later on, we show that testing can be carried in the lack
    of knowing the fold configuration, as well, however, in that case all possible
    fold configurations are tested, which might lead to enormous computational demands.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, the first step is to select the suitable test to be used. In this case,
    the correct test is the `check_1_dataset_known_folds_mos` function, where the
    token `mos` refers to the mode of aggregation, and `known_folds` indicates that
    the *fold configuration* is known (due to stratification). The test is executed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly to the previous examples, there is no inconsistency, since the performance
    scores are prepared to constitute a consistent configuration. However, if one
    of the scores is slightly changed, the test detects the inconsistency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous examples, we supposed that the fold configuration is known.
    However, in many cases, the exact fold configuration is not known and stratification
    is not specified. In these cases one can rely on tests that systematically test
    all possible fold configurations, as shown in the below example. This time, the
    suitable test has the `''unknown_folds''` token in its name, indicating that all
    potential fold configurations are to be tested:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As before, the test correctly identifies that there is no inconsistency: during
    the process of evaluating all possible fold configurations, it got to the point
    of testing the actual stratified configuration which shows consistency, and with
    this evidence, stopped the testing of the remaining one.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, prior to launching a test with unknown folds, it is advised to
    make an estimation on the number of possible fold configurations to be tested:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In worst case, solving 4096 small linear integer programming problems is still
    feasible with regular computing equipment, however, with larger datasets the number
    of potential fold configurations can quickly grow intractable.
  prefs: []
  type: TYPE_NORMAL
- en: '**Multiclass classification**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Testing multiclass classification scenarios is analogous to that of the binary
    case, therefore, we do not get into as much details as in the binary case.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the 6 experimental setups supported by the package we picked a commonly
    used one for illustration: we assume there is a multiclass dataset (4 classes),
    and repeated stratified k-fold cross-validation was carried out with 4 folds and
    2 repetitions. We also know that the scores were aggregated in the *macro-average*
    fashion, that is, in each fold, the performance on each class was evaluated against
    all other classes in a binary classification manner, and the scores were averaged
    across the classes and then across the folds.'
  prefs: []
  type: TYPE_NORMAL
- en: Again, the first step is chosing the suitable test function, which in this case
    becomes `check_1_dataset_known_folds_mos_macro` from the `mlscorecheck.check.multiclass`
    module. Again, the tokens `'mos’` and `'macro’` in the name of the test refer
    to the aggregations used in the experiment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Similarly to the previous cases, with the hand-crafted set of consistent scores,
    the test detects no inconsistency. However, a small change, for example, accuracy
    modified to 0.656 renders the configuration infeasible.
  prefs: []
  type: TYPE_NORMAL
- en: '**Regression**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last supervised learning task supported by the [mlscorecheck](https://github.com/FalseNegativeLab/mlscorecheck)
    package is regression. The testing of regression problems is the most difficult
    since the predictions on the testsets can take any values, consequently, any score
    values could be yielded an experiment. The only thing regression tests can rely
    on is the mathematical relation between the currently supported *mean average
    error (mae)*, *mean squared error (mse)* and *r-squared* (r2).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we assume that the *mae* and *r2* scores are reported
    for a testset, and we know its main statistics (the number of samples and the
    variance). Then, the consistency test can be executed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Again, the test correctly shows that there is no inconsistency (the scores are
    prepared by a real evaluation). However, if the *r2* score is slightly changed,
    for example, to 0.9997, the configuration becomes infeasible.
  prefs: []
  type: TYPE_NORMAL
- en: '**Test bundles**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To make the consistency testing of scores reported for popular, widely researched
    problems more accessible, the [mlscorecheck](https://github.com/FalseNegativeLab/mlscorecheck)
    package includes specifications for numerous experimental setups that are considered
    standards in certain problems.
  prefs: []
  type: TYPE_NORMAL
- en: '**Retinal vessel segmentation on the DRIVE dataset**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the field of retinal image analysis, an [ambiguity](https://www.researchgate.net/publication/350236730_A_new_baseline_for_retinal_vessel_segmentation_Numerical_identification_and_correction_of_methodological_inconsistencies_affecting_100_papers)
    exists in the evaluation of various segmentation techniques: authors have the
    freedom to account for pixels outside the circular field of view area, and this
    choice is rarely indicated in publications. This ambiguity can result in the ranking
    of algorithms based on incomparable performance scores. The functionalities implemented
    in the [mlscorecheck](https://github.com/FalseNegativeLab/mlscorecheck) package
    are suitable to identify if the authors used pixels outside the field of view
    for evaluation or not.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most widely researched problems is the segmentation of vessels based
    on the [DRIVE](https://ieeexplore.ieee.org/document/1282003) dataset. To prevent
    the cumbersome task of looking up the statistics of the images and constructing
    the experimental setups, the package contains the statistics of the dataset and
    provides two high-level functions to test the ambiguity of image-level and aggregated
    scores. For example, having a triplet of image level accuracy, sensitivity and
    specificity scores for the test image ‘03’ of the [DRIVE](https://ieeexplore.ieee.org/document/1282003)
    dataset, one can exploit the package as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The result indicates that the scores for this image must have been obtained
    by using only the field of view (*fov*) pixels for evaluation, since the scores
    are not inconsistent with this hypothesis, but they inconsistent with the alternative
    hypothesis of using *all* pixels for evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Further test bundles**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The list of all popular research problems and corresponding publicly available
    datasets supported by test bundles in the [mlscorecheck](https://github.com/FalseNegativeLab/mlscorecheck)
    package reads as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Retinal image analysis:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '- Vessel segmentation: [DRIVE](https://ieeexplore.ieee.org/document/1282003),
    [STARE](https://pubmed.ncbi.nlm.nih.gov/9929355/), [HRF](https://pubmed.ncbi.nlm.nih.gov/24416040/),
    [CHASE_DB1](https://ieeexplore.ieee.org/document/6224174);'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- Retinopathy recognition: [DIARETDB0](https://www.semanticscholar.org/paper/DIARETDB-0-%3A-Evaluation-Database-and-Methodology-Kauppi-Kalesnykiene/bd7d2380e76fb9dfd367d669e311d4913f67f7d2),
    [DIARETDB1](https://www.researchgate.net/publication/221259835_DIARETDB1_diabetic_retinopathy_database_and_evaluation_protocol);'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- Optic disk/cup segmentation: [DRISHTI_GS](https://ieeexplore.ieee.org/document/6867807);'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- Exudate segmentation: [DIARETDB1](https://www.researchgate.net/publication/221259835_DIARETDB1_diabetic_retinopathy_database_and_evaluation_protocol);'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Skin lesion classification: [ISIC2016](https://arxiv.org/abs/1605.01397), [ISIC2017](https://arxiv.org/abs/1710.05006);'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Term-preterm delivery prediction from electrohysterogram signals: [TPEHG](https://pubmed.ncbi.nlm.nih.gov/18437439/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Call for contribution**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Experts from any fields are welcome to submit further test bundles to facilitate
    the validation of machine learning performance scores in various areas of research!
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusions**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The meta-analysis of machine learning research doesn’t encompass many techniques
    beyond thorough paper assessments and potential attempts at re-implementing proposed
    methods to validate claimed results. The functionalities provided by the [mlscorecheck](https://github.com/FalseNegativeLab/mlscorecheck)
    package enable a more concise, numerical approach to the meta-analysis of machine
    learning research, contributing to maintaining the integrity of various research
    fields.
  prefs: []
  type: TYPE_NORMAL
- en: '**Further reading**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For further information, we recommend checking:'
  prefs: []
  type: TYPE_NORMAL
- en: the README of the [mlscorecheck](https://github.com/FalseNegativeLab/mlscorecheck)
    package,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the illustrative [notebooks](https://github.com/FalseNegativeLab/mlscorecheck/tree/main/notebooks/illustration)
    provided in the package,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the detailed [documentation](https://mlscorecheck.readthedocs.io/en/latest/),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the [preprint](https://www.researchgate.net/publication/374845553_Testing_the_Consistency_of_Performance_Scores_Reported_for_Binary_Classification_Problems)
    that describes the numerical methods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
