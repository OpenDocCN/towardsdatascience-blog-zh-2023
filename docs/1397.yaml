- en: A Gentle Intro to Chaining LLMs, Agents, and utils via LangChain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/a-gentle-intro-to-chaining-llms-agents-and-utils-via-langchain-16cd385fca81?source=collection_archive---------0-----------------------#2023-04-21](https://towardsdatascience.com/a-gentle-intro-to-chaining-llms-agents-and-utils-via-langchain-16cd385fca81?source=collection_archive---------0-----------------------#2023-04-21)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*#LLM for beginners*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Understand the basics of agents, tools, and prompts and some learnings along
    the way
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://varshitasher.medium.com/?source=post_page-----16cd385fca81--------------------------------)[![Dr.
    Varshita Sher](../Images/a3f2e9bf1dc1d8cbe018e54f9341f608.png)](https://varshitasher.medium.com/?source=post_page-----16cd385fca81--------------------------------)[](https://towardsdatascience.com/?source=post_page-----16cd385fca81--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----16cd385fca81--------------------------------)
    [Dr. Varshita Sher](https://varshitasher.medium.com/?source=post_page-----16cd385fca81--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff8ca36def59&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-intro-to-chaining-llms-agents-and-utils-via-langchain-16cd385fca81&user=Dr.+Varshita+Sher&userId=f8ca36def59&source=post_page-f8ca36def59----16cd385fca81---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----16cd385fca81--------------------------------)
    ¬∑20 min read¬∑Apr 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F16cd385fca81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-intro-to-chaining-llms-agents-and-utils-via-langchain-16cd385fca81&user=Dr.+Varshita+Sher&userId=f8ca36def59&source=-----16cd385fca81---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F16cd385fca81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-intro-to-chaining-llms-agents-and-utils-via-langchain-16cd385fca81&source=-----16cd385fca81---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Audience: For those feeling overwhelmed with the giant (yet brilliant) library‚Ä¶'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/4c6806a457e762ae481cc1f29a02d4bd.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by Author using [DALL.E 2](https://openai.com/product/dall-e-2)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I‚Äôd be lying if I said I have got the entire LangChain library covered ‚Äî in
    fact, I am far from it. But the buzz surrounding it was enough to shake me out
    of my writing hiatus and give it a go üöÄ.
  prefs: []
  type: TYPE_NORMAL
- en: The initial motivation was to see what was it that LangChain was adding (on
    a practical level) that set it apart from the chatbot I built last month using
    the `ChatCompletion.create()` function from the `openai` package. Whilst doing
    so, I realized I needed to understand the building blocks for LangChain first
    before moving on to the more complex parts.
  prefs: []
  type: TYPE_NORMAL
- en: This is what this article does. Heads-up though, there will be more parts coming
    as I am truly fascinated by the library and will continue to explore to see what
    all can be built through it.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs begin by understanding the fundamental building blocks of LangChain ‚Äî
    i.e. Chains. If you‚Äôd like to follow along, here‚Äôs the [GitHub repo](https://github.com/V-Sher/LangChain-Tutorial).
  prefs: []
  type: TYPE_NORMAL
- en: What are chains in LangChain?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Chains are what you get by connecting one or more large language models (LLMs)
    in a logical way. (Chains can be built of entities other than LLMs but for now,
    let‚Äôs stick with this definition for simplicity).
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI is a type of LLM (provider) that you can use but there are others like
    Cohere, Bloom, Huggingface, etc.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: Pretty much most of these LLM providers will need you to request an
    API key in order to use them. So make sure you do that before proceeding with
    the remainder of this blog. For example:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*P.S. I am going to use OpenAI for this tutorial because I have a key with
    credits that expire in a month‚Äôs time, but feel free to replace it with any other
    LLM. The concepts covered here will be useful regardless.*'
  prefs: []
  type: TYPE_NORMAL
- en: Chains can be simple (i.e. Generic) or specialized (i.e. Utility).
  prefs: []
  type: TYPE_NORMAL
- en: 'Generic ‚Äî A single LLM is the simplest chain. It takes an input prompt and
    the name of the LLM and then uses the LLM for text generation (i.e. output for
    the prompt). Here‚Äôs an example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let‚Äôs build a basic chain ‚Äî create a prompt and get a prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prompt creation (using `PromptTemplate`) is a bit fancy in Lanchain but this
    is probably because there are quite a few different ways prompts can be created
    depending on the use case (we will cover `AIMessagePromptTemplate`,
  prefs: []
  type: TYPE_NORMAL
- en: '`HumanMessagePromptTemplate` etc. in the next blog post). Here‚Äôs a simple one
    for now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '*Note: If you require multiple* `*input_variables*`*, for instance:* `*input_variables=["product",
    "audience"]*` *for a template such as* `*‚ÄúWhat is a good name for a company that
    makes {product} for {audience}‚Äù*`*, you need to do* `print(prompt.format(product="podcast
    player", audience="children‚Äù)` to get the updated prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: Once you have built a prompt, we can call the desired LLM with it. To do so,
    we create an `LLMChain` instance (in our case, we use `OpenAI`'s large language
    model `text-davinci-003`). To get the prediction (i.e. AI-generated text), we
    use `run` function with the name of the `product`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If you had more than one input_variables, then you won‚Äôt be able to use `run`.
    Instead, you‚Äôll have to pass all the variables as a `dict`. For example, `llmchain({‚Äúproduct‚Äù:
    ‚Äúpodcast player‚Äù, ‚Äúaudience‚Äù: ‚Äúchildren‚Äù})`.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Note 1: According to* [*OpenAI*](https://openai.com/blog/introducing-chatgpt-and-whisper-apis),`*davinci*`
    *text-generation models are 10x more expensive than their chat counterparts i.e*
    `*gpt-3.5-turbo*`*, so I tried to switch from a text model to a chat model (i.e.
    from* `*OpenAI*` *to* `*ChatOpenAI*`*) and the results are pretty much the same.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Note 2: You might see some tutorials using* `*OpenAIChat*`*instead of* `*ChatOpenAI*`*.
    The former is* [*deprecated*](https://github.com/hwchase17/langchain/issues/1556#issuecomment-1463224442)
    *and will no longer be supported and we are supposed to use* `*ChatOpenAI*`*.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This concludes our section on simple chains. It is important to note that we
    rarely use generic chains as standalone chains. More often they are used as building
    blocks for Utility chains (as we will see next).
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Utility ‚Äî These are specialized chains, comprised of many LLMs to help solve
    a specific task. For example, LangChain supports some end-to-end chains (such
    as `[AnalyzeDocumentChain](https://python.langchain.com/docs/use_cases/question_answering/how_to/analyze_document)`
    for summarization, QnA, etc) and some specific ones (such as `[GraphQnAChain](https://python.langchain.com/en/latest/modules/chains/index_examples/graph_qa.html#querying-the-graph)`
    for creating, querying, and saving graphs). We will look at one specific chain
    called `PalChain` in this tutorial for digging deeper.
  prefs: []
  type: TYPE_NORMAL
- en: PAL stands for [Programme Aided Language Model](https://arxiv.org/pdf/2211.10435.pdf).
    `PALChain` reads complex math problems (described in natural language) and generates
    programs (for solving the math problem) as the intermediate reasoning steps, but
    offloads the solution step to a runtime such as a Python interpreter.
  prefs: []
  type: TYPE_NORMAL
- en: 'To confirm this is in fact true, we can inspect the `_call()` in the base code
    [here](https://github.com/hwchase17/langchain/blob/master/langchain/chains/pal/base.py).
    Under the hood, we can see this chain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[first uses a generic](https://github.com/hwchase17/langchain/blob/master/langchain/chains/pal/base.py#L58)
    `[LLMChain](https://github.com/hwchase17/langchain/blob/master/langchain/chains/pal/base.py#L58)`
    [to understand the query](https://github.com/hwchase17/langchain/blob/master/langchain/chains/pal/base.py#L58)
    we pass to it and get a prediction. Thus, this chain requires passing an LLM at
    the time of initializing (we are going to use the same OpenAI LLM as before).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: s[econd, it uses Python REPL](https://github.com/hwchase17/langchain/blob/master/langchain/chains/pal/base.py#L63-L64)
    to solve the function/program outputted by the LLM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P.S. It is a good practice to inspect* `*_call()*` *in* `*base.py*` *for any
    of the chains in LangChain to see how things are working under the hood.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '*Note1:* `*verbose*` *can be set to* `*False*` *if you do not need to see the
    intermediate step.*'
  prefs: []
  type: TYPE_NORMAL
- en: Now some of you may be wondering ‚Äî *but what about the prompt? We certainly
    didn‚Äôt pass one as we did for the generic* `*llmchain*` *we built.* The fact is,
    it is automatically loaded when using `.from_math_prompt()`. You can check the
    default prompt using `palchain.prompt.template` or you can directly inspect the
    prompt file [here](https://github.com/hwchase17/langchain/blob/master/langchain/chains/pal/math_prompt.py).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '*Note: Most of the utility chains will have their prompts pre-defined as part
    of the library (check them out* [*here*](https://github.com/hwchase17/langchain/tree/master/langchain/chains)*).
    They are, at times, quite detailed (read: lots of tokens) so there is definitely
    a trade-off between cost and the quality of response from the LLM.*'
  prefs: []
  type: TYPE_NORMAL
- en: Are there any Chains that don‚Äôt need LLMs and prompts?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Even though PalChain requires an LLM (and a corresponding prompt) to parse
    the user‚Äôs question written in natural language, there are some chains in LangChain
    that don‚Äôt need one. These are mainly transformation chains that preprocess the
    prompt, such as removing extra spaces, before inputting it into the LLM. You can
    see another example* [*here*](https://python.langchain.com/en/latest/modules/chains/generic/transformation.html)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: Can we get to the good part and start creating chains?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Of course, we can! We have all the basic building blocks we need to start chaining
    together LLMs logically such that input from one can be fed to the next. To do
    so, we will use `SimpleSequentialChain`.
  prefs: []
  type: TYPE_NORMAL
- en: The documentation has some great examples on this, for example, you can see
    [here](https://python.langchain.com/en/latest/modules/chains/generic/transformation.html)
    how to have two chains combined where chain#1 is used to clean the prompt (remove
    extra whitespaces, shorten prompt, etc) and chain#2 is used to call an LLM with
    this clean prompt. Here‚Äôs [another one](https://js.langchain.com/docs/modules/chains/foundational/sequential_chains/#simplesequentialchain)
    where chain#1 is used to generate a synopsis for a play and chain#2 is used to
    write a review based on this synopsis.
  prefs: []
  type: TYPE_NORMAL
- en: While these are excellent examples, I want to focus on something else. If you
    remember before, I mentioned that chains can be composed of entities other than
    LLMs. More specifically, I am interested in chaining agents and LLMs together.
    *But first, what are agents?*
  prefs: []
  type: TYPE_NORMAL
- en: Using agents for dynamically calling LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It will be much easier to explain what an agent does vs. what it is.
  prefs: []
  type: TYPE_NORMAL
- en: Say, we want to know the weather forecast for tomorrow. If were to use the simple
    ChatGPT API and give it a prompt `Show me the weather for tomorrow in London`,
    it won‚Äôt know the answer because it does not have access to real-time data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a379c0d1313089f343dc25f6510660a3.png)'
  prefs: []
  type: TYPE_IMG
- en: Wouldn‚Äôt it be useful if we had an arrangement where we could utilize an LLM
    for understanding our query (i.e prompt) in natural language and then call the
    weather API on our behalf to fetch the data needed? This is exactly what an agent
    does (amongst other things, of course).
  prefs: []
  type: TYPE_NORMAL
- en: An agent has access to an LLM and a suite of tools for example Google Search,
    Python REPL, math calculator, weather APIs, etc.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There are quite a few agents that LangChain supports ‚Äî see [here](https://python.langchain.com/docs/modules/agents/agent_types/)
    for the complete list, but quite frankly the most common one I came across in
    tutorials and YT videos was `zero-shot-react-description`. This agent uses [ReAct](https://arxiv.org/abs/2210.03629)
    (Reason + Act) framework to pick the most usable tool (from a list of tools),
    based on what the input query is.
  prefs: []
  type: TYPE_NORMAL
- en: '*P.S.:* [*Here‚Äôs*](https://tsmatz.wordpress.com/2023/03/07/react-with-openai-gpt-and-langchain/)
    *a nice article that goes in-depth into the ReAct framework.*'
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs initialize an agent using `initialize_agent` and pass it the `tools` and
    `LLM` it needs. There‚Äôs a long list of tools available [here](https://python.langchain.com/docs/integrations/tools/)
    that an agent can use to interact with the outside world. For our example, we
    are using the same math-solving tool as above, called `pal-math`. This one requires
    an LLM at the time of initialization, so we pass to it the same OpenAI LLM instance
    as before.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let‚Äôs test it out on the same example as above:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '*Note 1: At each step, you‚Äôll notice that an agent does one of three things
    ‚Äî it either has an* `*observation*`*, a* `*thought*`*, or it takes an* `*action*`*.
    This is mainly due to the ReAct framework and the associated prompt that the agent
    is using:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '*Note2: You might be wondering what‚Äôs the point of getting an agent to do the
    same thing that an LLM can do. Some applications will require not just a predetermined
    chain of calls to LLMs/other tools, but potentially an unknown chain that depends
    on the user‚Äôs input [*[*Source*](https://python.langchain.com/en/latest/modules/agents.html#agents)*].
    In these types of chains, there is an ‚Äúagent‚Äù which has access to a suite of tools.'
  prefs: []
  type: TYPE_NORMAL
- en: For instance,* [*here‚Äôs*](https://python.langchain.com/en/latest/modules/agents/agent_executors/examples/agent_vectorstore.html#create-the-agent)
    *an example of an agent that can fetch the correct documents (from the vectorstores)
    for* `*RetrievalQAChain*` *depending on whether the question refers to document
    A or document B.*
  prefs: []
  type: TYPE_NORMAL
- en: For fun, I tried making the input question more complex (using Demi Moore‚Äôs
    age as a placeholder for Dad‚Äôs actual age).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately, the answer was slightly off as the agent was not using the latest
    age for Demi Moore (since Open AI models were trained on data until 2020). This
    can be easily fixed by including another tool ‚Äî
  prefs: []
  type: TYPE_NORMAL
- en: '`tools = load_tools([‚Äúpal-math‚Äù, **"serpapi"**], llm=llm)`. `serpapi` is useful
    for answering questions about current events.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: It is important to add as many tools as you think may be relevant to
    the user query. The problem with using a single tool is that the agent keeps trying
    to use the same tool even if it‚Äôs not the most relevant for a particular observation/action
    step.*'
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs another example of a tool you can use ‚Äî `podcast-api`. You need to [get
    your own API key](https://www.listennotes.com/api/pricing/) and plug it into the
    code below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '*Note1: There is a* [*known error*](https://github.com/hwchase17/langchain/pull/1833)
    *with using this API where you might see,* `*openai.error.InvalidRequestError:
    This model‚Äôs maximum context length is 4097 tokens, however you requested XXX
    tokens (XX in your prompt; XX for the completion). Please reduce your prompt;
    or completion length.*` *This happens when the response returned by the API might
    be too big. To work around this, the documentation suggests returning fewer search
    results, for example, by updating the question to* `"Show me episodes for money
    saving tips, return only 1 result"`.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Note2: While tinkering around with this tool, I noticed some inconsistencies.
    The responses aren‚Äôt always complete the first time around, for instance here
    are the input and responses from two consecutive runs:*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Input: ‚ÄúPodcasts for getting better at French‚Äù*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Response 1: ‚ÄúThe best podcast for learning French is the one with the highest
    review score.‚Äù'
  prefs: []
  type: TYPE_NORMAL
- en: 'Response 2: ‚ÄòThe best podcast for learning French is ‚ÄúFrenchPod101‚Äù.*'
  prefs: []
  type: TYPE_NORMAL
- en: Under the hood, the tool is first using an LLMChain for [building the API URL](https://github.com/hwchase17/langchain/blob/master/langchain/chains/api/base.py#L115)
    based on our input instructions (something along the lines of `[https://listen-api.listennotes.com/api/v2/search?q=french&type=podcast&page_size=3](https://listen-api.listennotes.com/api/v2/search?q=french&type=podcast&page_size=3%29)`[)](https://listen-api.listennotes.com/api/v2/search?q=french&type=podcast&page_size=3%29)
    and [making the API call](https://github.com/hwchase17/langchain/blob/master/langchain/chains/api/base.py#L116).
    Upon receiving the response, it uses another LLMChain that [summarizes the response](https://github.com/hwchase17/langchain/blob/master/langchain/chains/api/base.py#L117)
    to get the answer to our original question. You can check out the prompts [here](https://github.com/hwchase17/langchain/blob/master/langchain/chains/api/prompt.py)
    for both LLMchains which describe the process in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: I am inclined to guess the inconsistent results seen above are resulting from
    the summarization step because I have separately debugged and tested the API URL
    (created by LLMChain#1) via Postman and received the right response. To further
    confirm my doubts, I also stress-tested the summarization chain as a standalone
    chain with an empty API URL hoping it would throw an error but got the response
    *‚ÄúInvesting‚Äô podcasts were found, containing 3 results in total.‚Äù* ü§∑‚Äç‚ôÄ I‚Äôd be
    curious to see if others had better luck than me with this tool!
  prefs: []
  type: TYPE_NORMAL
- en: 'Use Case 2: Combine chains to create an age-appropriate gift generator'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let‚Äôs put our knowledge of agents and sequential chaining to good use and create
    our own sequential chain. We will combine:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chain #1 ‚Äî The `agent` we just created that can solve [age problems](https://www.cliffsnotes.com/study-guides/algebra/algebra-i/word-problems/age-problems)
    in math.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chain #2 ‚Äî An LLM that takes the age of a person and suggests an appropriate
    gift for them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have both chains ready we can combine them using `SimpleSequentialChain`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'A couple of things to note:'
  prefs: []
  type: TYPE_NORMAL
- en: We need not explicitly pass `input_variables` and `output_variables` for `SimpleSequentialChain`
    as the underlying assumption is that the output from chain 1 is passed as input
    to chain 2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, we can run it with the same math problem as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: There might be times when you need to pass along some additional context to
    the second chain, in addition to what it is receiving from the first chain. For
    instance, I want to set a budget for the gift, depending on the age of the person
    that is returned by the first chain. We can do so using `SimpleMemory`.
  prefs: []
  type: TYPE_NORMAL
- en: First, let‚Äôs update the prompt for `chain_two` and pass to it a second variable
    called `budget` inside `input_variables`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'If you compare the `template` we had for `SimpleSequentialChain` with the one
    above, you‚Äôll notice that I have also updated the first input‚Äôs variable name
    from `age` ‚Üí `output`. This is a crucial step, failing which an error would be
    raised at the time of [chain validation](https://github.com/hwchase17/langchain/blob/master/langchain/chains/sequential.py#L41)
    ‚Äî `*Missing required input keys: {age}, only had {input, output, budget}*`.'
  prefs: []
  type: TYPE_NORMAL
- en: This is because the output from the first entity in the chain (i.e. `agent`)
    will be the input for the second entity in the chain (i.e. `chain_two`) and therefore
    the variable names must match**.** Upon inspecting `agent`‚Äôs output keys, we see
    that the output variable is called `output`, hence the update.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Next, let‚Äôs update the kind of chain we are making. We can no longer work with
    `SimpleSequentialChain` because it only works in cases where this is a single
    input and single output. Since `chain_two` is now taking two `input_variables`,
    we need to use `SequentialChain` which is tailored to handle multiple inputs and
    outputs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'A couple of things to note:'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike `SimpleSequentialChain`, passing `input_variables` parameter is mandatory
    for `SequentialChain`. It is a list containing the name of the input variables
    that the first entity in the chain (i.e. `agent` in our case) expects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now some of you may be wondering how to know the exact name used in the input
    prompt that the `agent` is going to use. We certainly did not write the prompt
    for this agent (as we did for `chain_two`)! It's actually pretty straightforward
    to find it out by inspecting the prompt template of the `llm_chain` that the agent
    is made up of.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: As you can see toward the end of the prompt, the questions being asked by the
    end-user is stored in an input variable by the name `input`. If for some reason
    you had to manipulate this name in the prompt, make sure you are also updating
    the `input_variables` at the time of the creation of `SequentialChain`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, you could have found out the same information without going through
    the whole prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '`[SimpleMemory](https://github.com/hwchase17/langchain/blob/master/langchain/memory/simple.py#L6)`
    is an easy way to store context or other bits of information that shouldn‚Äôt ever
    change between prompts. It requires one parameter at the time of initialization
    ‚Äî `memories`. You can pass elements to it in `dict` form. For instance, `SimpleMemory(memories={‚Äúbudget‚Äù:
    ‚Äú100 GBP‚Äù})`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, let‚Äôs run the new chain with the same prompt as before. You will notice,
    the final output has some luxury gift recommendations such as weekend getaways
    in accordance with the higher budget in our updated prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hopefully, the learnings I have shared through this post have made you more
    comfortable in taking a deep dive into the library. This article just scratched
    the surface, there is so much more to cover. For instance, how to build a QnA
    chatbot over your own datasets, and how to optimize memory for these chatbots
    so that you can cherry-pick/summarize conversations to send in the prompt rather
    than sending all previous chat history as part of the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: As always if there‚Äôs an easier way to do/explain some of the things mentioned
    in this article, do let me know. In general, refrain from unsolicited destructive/trash/hostile
    comments!
  prefs: []
  type: TYPE_NORMAL
- en: Until next time ‚ú®
  prefs: []
  type: TYPE_NORMAL
- en: '*I enjoy writing step-by-step beginner‚Äôs guides, how-to tutorials, decoding
    terminology used in ML/AI, etc. If you want full access to all my articles (and
    others on Medium), then you can sign up using* [***my link***](https://varshitasher.medium.com/membership)*here****.***'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/step-by-step-guide-to-explaining-your-ml-project-during-a-data-science-interview-81dfaaa408bf?source=post_page-----16cd385fca81--------------------------------)
    [## Step by step guide to explaining your ML project during a data science interview.'
  prefs: []
  type: TYPE_NORMAL
- en: With a bonus sample script at the end that lets you show off your tech skills
    discreetly!
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/step-by-step-guide-to-explaining-your-ml-project-during-a-data-science-interview-81dfaaa408bf?source=post_page-----16cd385fca81--------------------------------)
    [](/time-series-modeling-using-scikit-pandas-and-numpy-682e3b8db8d1?source=post_page-----16cd385fca81--------------------------------)
    [## Time Series Modeling using Scikit, Pandas, and Numpy
  prefs: []
  type: TYPE_NORMAL
- en: Intuitive use of seasonality to improve model accuracy.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/time-series-modeling-using-scikit-pandas-and-numpy-682e3b8db8d1?source=post_page-----16cd385fca81--------------------------------)
    [](/hands-on-introduction-to-github-actions-for-data-scientists-f422631c9ea7?source=post_page-----16cd385fca81--------------------------------)
    [## Hands-On Introduction to Github Actions for Data Scientists
  prefs: []
  type: TYPE_NORMAL
- en: Learn how to automate experiment tracking with Weights & Biases, unit testing,
    artifact creation, and lots more‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/hands-on-introduction-to-github-actions-for-data-scientists-f422631c9ea7?source=post_page-----16cd385fca81--------------------------------)
    [](/deploying-an-end-to-end-deep-learning-project-with-few-clicks-part-2-89009cff6f16?source=post_page-----16cd385fca81--------------------------------)
    [## Deploying an End to End Deep Learning Project with few clicks: Part 2'
  prefs: []
  type: TYPE_NORMAL
- en: Taking model from Jupyter notebook to Flask app, testing API endpoint using
    Postman, and Heroku deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/deploying-an-end-to-end-deep-learning-project-with-few-clicks-part-2-89009cff6f16?source=post_page-----16cd385fca81--------------------------------)
  prefs: []
  type: TYPE_NORMAL
