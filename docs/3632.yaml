- en: Can LLMs Replace Data Analysts? Building An LLM-Powered Analyst
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/can-llms-replace-data-analysts-building-an-llm-powered-analyst-851578fa10ce?source=collection_archive---------0-----------------------#2023-12-11](https://towardsdatascience.com/can-llms-replace-data-analysts-building-an-llm-powered-analyst-851578fa10ce?source=collection_archive---------0-----------------------#2023-12-11)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Part 1: empowering ChatGPT with tools'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://miptgirl.medium.com/?source=post_page-----851578fa10ce--------------------------------)[![Mariya
    Mansurova](../Images/b1dd377b0a1887db900cc5108bca8ea8.png)](https://miptgirl.medium.com/?source=post_page-----851578fa10ce--------------------------------)[](https://towardsdatascience.com/?source=post_page-----851578fa10ce--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----851578fa10ce--------------------------------)
    [Mariya Mansurova](https://miptgirl.medium.com/?source=post_page-----851578fa10ce--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F15a29a4fc6ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-llms-replace-data-analysts-building-an-llm-powered-analyst-851578fa10ce&user=Mariya+Mansurova&userId=15a29a4fc6ad&source=post_page-15a29a4fc6ad----851578fa10ce---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----851578fa10ce--------------------------------)
    ·19 min read·Dec 11, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F851578fa10ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-llms-replace-data-analysts-building-an-llm-powered-analyst-851578fa10ce&user=Mariya+Mansurova&userId=15a29a4fc6ad&source=-----851578fa10ce---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F851578fa10ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-llms-replace-data-analysts-building-an-llm-powered-analyst-851578fa10ce&source=-----851578fa10ce---------------------bookmark_footer-----------)![](../Images/b088e6e4add866f67e47f8714dd3bae5.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image by DALL-E 3
  prefs: []
  type: TYPE_NORMAL
- en: I think each of us has wondered at least once over the past year if (or rather
    when) ChatGPT will be able to replace your role. I’m no exception here.
  prefs: []
  type: TYPE_NORMAL
- en: We have a somewhat consensus that the recent breakthroughs in Generative AI
    will highly affect our personal lives and work. However, there is no clear view
    yet of how our roles will change over time.
  prefs: []
  type: TYPE_NORMAL
- en: Spending lots of time thinking about different possible future scenarios and
    their probabilities might be captivating, but I suggest an absolutely different
    approach — to try to build your prototype yourself. First, it’s rather challenging
    and fun. Second, it will help us to look at our work in a more structured way.
    Third, it will give us an opportunity to try in practice one of the most cutting-edge
    approaches — LLM agents.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will start simple and learn how LLMs can leverage tools
    and do straightforward tasks. But in the following articles, we will dive deeper
    into different approaches and best practices for LLM agents.
  prefs: []
  type: TYPE_NORMAL
- en: So, let the journey begin.
  prefs: []
  type: TYPE_NORMAL
- en: What is data analytics?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before moving on to the LLMs, let’s try defining what analytics is and what
    tasks we do as analysts.
  prefs: []
  type: TYPE_NORMAL
- en: My motto is that the goal of the analytical team is to help the product teams
    make the right decisions based on data in the available time. It’s a good mission,
    but to define the scope of the LLM-powered analyst, we should decompose the analytical
    work further.
  prefs: []
  type: TYPE_NORMAL
- en: 'I like [the framework](https://www.gartner.com/en/topics/data-and-analytics)
    proposed by Gartner. It identifies four different Data and Analytics techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Descriptive analytics** answers questions like “What happened?”. For example,
    what was the revenue in December? This approach includes reporting tasks and working
    with BI tools.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Diagnostic analytics** goes a bit further and asks questions like “Why did
    something happen?”. For example, why revenue decreased by 10% compared to the
    previous year? This technique requires more drill-down and slicing & dicing of
    your data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predictive analytics** allows us to get answers to questions like “What will
    happen?”. The two cornerstones of this approach are forecasting (predicting the
    future for business-as-usual situations) and simulation (modelling different possible
    outcomes).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prescriptive analytics** impacts the final decisions. The common questions
    are “What should we focus on?” or “How could we increase volume by 10%?”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Usually, companies go through all these stages step by step. It’s almost impossible
    to start looking at forecasts and different scenario analyses if your company
    hasn’t mastered descriptive analytics yet (you don’t have a data warehouse, BI
    tools, or metrics definitions). So, this framework can also show the company’s
    data maturity.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, when an analyst grows from junior to senior level, she will likely
    go through all these stages, starting from well-defined reporting tasks and progressing
    to vague strategic questions. So, this framework is relevant on an individual
    level as well.
  prefs: []
  type: TYPE_NORMAL
- en: If we return to our LLM-powered analyst, we should focus on descriptive analytics
    and reporting tasks. It’s better to start from the basics. So, we will focus on
    learning LLM to understand the basic questions about data.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve defined our focus for the first prototype. So, we are ready to move on
    to the technical questions and discuss the concept of LLM agents and tools.
  prefs: []
  type: TYPE_NORMAL
- en: LLM agents and tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we were using LLMs before (for example, to do topic modelling [here](/topic-modelling-in-production-e3b3e99e4fca)),
    we described the exact steps ourselves in the code. For example, let’s look at
    the chain below. Firstly, we asked the model to determine the sentiment for a
    customer review. Then, depending on the sentiment, extract from the review either
    the advantages or disadvantages mentioned in the text.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/351d036bc9e9dda4689b9cad8da4841b.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration by author
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we clearly defined the LLM’s behaviour, and the LLM solved
    this task pretty well. However, this approach won’t work if we build something
    more high-level and vague, like an LLM-powered analyst.
  prefs: []
  type: TYPE_NORMAL
- en: If you’ve ever worked as or with an analyst for at least one day, you would
    know that analysts are getting a vast range of different questions and asks, starting
    from basic questions (like “How many customers did we have on our site yesterday?”
    or “Could you make a graph for our Board meeting tomorrow?”) to very high-level
    ones (for example, “What are the main customer pain points?” or “What market should
    we launch next?”). It goes without saying it’s not feasible to describe all possible
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there’s an approach that could help us — agents. The core idea of
    the agents is to use LLMs as a reasoning engine that could choose what to do next
    and when it’s time to return the final answer to the customer. It sounds pretty
    close to our behaviour: we get a task, define needed tools, use them, and then
    come back with the final answer when ready.'
  prefs: []
  type: TYPE_NORMAL
- en: The essential concept related to agents (that I’ve already mentioned above)
    is tools. Tools are functions that LLM could invoke to get missing information
    (for example, execute SQL, use a calculator or call a search engine). Tools are
    crucial because they allow you to bring LLMs to the next level and interact with
    the world. In this article, we will primarily focus on OpenAI functions as tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI has fine-tuned models to be able to work with functions so that:'
  prefs: []
  type: TYPE_NORMAL
- en: You can pass to the model the list of functions with descriptions;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If it’s relevant to your query, the model will return you a function call —
    function name and input parameters to call it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find more info and the up-to-date list of models that support functions
    in [the documentation](https://platform.openai.com/docs/guides/function-calling).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two prominent use cases to use functions with LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: Tagging & extraction — in these cases, functions are used to ensure the output
    format of the model. Instead of the usual output with content, you will get a
    structured function call.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tools & routing — this is a more exciting use case that allows you to create
    an agent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start with the more straightforward use case of extraction to learn how
    to use OpenAI functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use Case #1: Tagging & Extraction'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You might wonder what is the difference between tagging and extraction. These
    terms are pretty close. The only difference is whether the model extracts info
    presented in the text or labels the text providing new information (i.e. defines
    language or sentiment).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a5448906e9ce962ab3d0a17a25e1e470.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we’ve decided to focus on descriptive analytics and reporting tasks,
    let’s use this approach to structure incoming data requests and pull the following
    components: metrics, dimensions, filters, period and desired output.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1ebc1a0b667eb225b3d60c210bc71d94.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration by author
  prefs: []
  type: TYPE_NORMAL
- en: It will be an example of extraction since we only need information present in
    the text.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI Completion API basic example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we need to define the function. OpenAI expects a function description
    as a JSON. This JSON will be passed to LLM, so we need to tell it all the context:
    what this function does and how to use it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of a function JSON. We’ve specified:'
  prefs: []
  type: TYPE_NORMAL
- en: '`name` and `description` for the function itself,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`type` and `description` for each argument,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the list of required input parameters for the function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: There’s no need to implement the function itself in this use case because we
    won’t be using it. We only get LLM responses in a structured way as function calls.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we could use the standard OpenAI Chat Completion API to call the function.
    We passed to the API call:'
  prefs: []
  type: TYPE_NORMAL
- en: model — I’ve used the latest ChatGPT 3.5 Turbo that can work with functions,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: list of messages — one system message to set up the context and a user request,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: list of functions we’ve defined earlier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As a result, we got the following JSON.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Remember that functions and function calls will be counted into the tokens limits
    and be billed.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The model returned a function call instead of a common response: we can see
    that the `content` is empty and `finish_reason` is equal to `function_call`. In
    the response, there are also the input parameters for the function call:'
  prefs: []
  type: TYPE_NORMAL
- en: '`metric = "number of users"`,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filters = "platform = ''iOS''"`,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dimensions = "date"`,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`period_start = "2021-01-01"`,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`period_start = "2021-12-31"`,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_type = "visualisation"`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model did a pretty good job. The only problem is that it presumed the period
    out of nowhere. We can fix it by adding more explicit guidance to the system message,
    for example, `"Extract the relevant information from the provided request. Extract
    ONLY the information presented in the initial request; don't add anything else.
    Return partial information if something is missing."`
  prefs: []
  type: TYPE_NORMAL
- en: By default, models decide whether to use functions independently (`function_call
    = 'auto'`). We can require it to return a specific function call every time or
    not to use functions at all.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We’ve got the first working program that uses LLM functions. That’s awesome.
    However, it’s not very convenient to describe functions in a JSON. Let’s discuss
    how to do it easier.
  prefs: []
  type: TYPE_NORMAL
- en: Using Pydantic to define functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To define functions more conveniently, we can leverage [Pydantic](https://docs.pydantic.dev/latest/).
    Pydantic is the most popular Python library for data validation.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve already [used](https://medium.com/towards-data-science/topic-modelling-in-production-e3b3e99e4fca)
    Pydantic to define LangChain Output Parser.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: First, we need to create a class inheriting from the `BaseModel` class and define
    all the fields (arguments of our function).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Then, we can use LangChain to convert the Pydantic class into the OpenAI function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: LangChain validates the class we provided. For example, it ensures that the
    function description is specified since LLM needs it to be able to use this tool.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, we got the same JSON to pass to LLM, but now we express it as a
    Pydantic class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now, we could use it in our call to OpenAI. Let’s switch from OpenAI API to
    LangChain to make our API calls more modular.
  prefs: []
  type: TYPE_NORMAL
- en: Defining LangChain chain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s define a chain to extract needed information from the requests. We will
    use LangChain since it’s the most popular framework for LLMs. If you haven’t worked
    with it before, I recommend you learn some basics in [one of my previous articles](https://medium.com/towards-data-science/topic-modelling-in-production-e3b3e99e4fca).
  prefs: []
  type: TYPE_NORMAL
- en: Our chain is simple. It consists of an Open AI model and prompt with one variable
    `request` (a user message).
  prefs: []
  type: TYPE_NORMAL
- en: We’ve also used the `bind` function to pass `functions` argument to the model.
    The `bind` function [allows](https://python.langchain.com/docs/expression_language/how_to/binding)
    us to specify constant arguments for our models that are not part of the input
    (for example, `functions` or `temperature`).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now it’s time to try our function. We need to use the invoke method and pass
    a `request`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In the output, we got `AIMessage` without any content but with a function call.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: So, we’ve learned how to use OpenAI functions in LangChain to get structured
    output. Now, let’s move on to the more interesting use case — tools and routing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use Case #2: Tools & Routing'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s time to use tools and empower our model with external capabilities. Models
    in this approach are reasoning engines, and they can decide what tools to use
    and when (it’s called routing).
  prefs: []
  type: TYPE_NORMAL
- en: LangChain has a concept of [tools](https://python.langchain.com/docs/modules/agents/tools/)
    — interfaces that agents can use to interact with the world. Tools can be functions,
    LangChain chains or even other agents.
  prefs: []
  type: TYPE_NORMAL
- en: We can easily convert tools into OpenAI functions using `format_tool_to_openai_function`
    and keep passing the `functions` argument to LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a custom tool
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s teach our LLM-powered analyst to calculate the difference between two
    metrics. We know that LLMs might make mistakes in math, so we would like to ask
    a model to use a calculator instead of counting on its own.
  prefs: []
  type: TYPE_NORMAL
- en: To define a tool, we need to create a function and use a `@tool` decorator.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now, this function has `name` and `description` parameters that will be passed
    to LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: These parameters will be used to create an OpenAI function specification. Let’s
    convert our tool to an OpenAI function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We got the following JSON as the result. It outlines the structure, but field
    descriptions are missing.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We can use Pydantic to specify a schema for the arguments.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Now, if we convert a new version to the OpenAI function specification, it will
    include argument descriptions. It’s much better since we could share all the needed
    context with the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: So, we’ve defined the tool that LLM will be able to use. Let’s try it in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Using a tool in practice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s define a chain and pass our tool to the function. Then, we could test
    it on a user request.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We got a function call with the correct arguments, so it’s working.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: To have a more convenient way to work with the output, we can use`OpenAIFunctionsAgentOutputParser`.
    Let’s add it to our chain.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Now, we got output in a more structured way, and we could easily retrieve arguments
    for our tool as `result.tool_input` .
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: So, we could execute the function as the LLM requested like this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: If we want to get the final answer from the model, we need to pass the function
    execution result back. To do it, we need to define a message list to pass to the
    model observations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Then, we need to add the observation to our `observations` variable. We could
    use `format_to_openai_functions` function to format our results in an expected
    way for the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: As a result, we got such a message that the LLM can understand.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Let’s invoke our chain one more time, passing the function execution result
    as an observation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Now, we got the final result from the model, which sounds reasonable.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: If we were working with vanilla OpenAI Chat Completion API, we could just add
    another message with role = tool . You can find a detailed example [here](https://platform.openai.com/docs/guides/function-calling).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If we switch on debug, we can see the exact prompt that was passed to OpenAI
    API.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: To switch on LangChain debug, execute the following code and invoke your chain
    to see what is going on under the hood.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: We’ve tried to work with one tool, but let’s extend our toolkit and see how
    LLM could handle it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Routing: using multiple tools'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s add a couple more tools to our analyst’s toolkit:'
  prefs: []
  type: TYPE_NORMAL
- en: get monthly active users
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: using Wikipedia.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First, let’s define a dummy function to calculate the audience with filters
    by month and city. We will again use Pydantic to specify the input arguments for
    our function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Then, let’s use [the wikipedia](https://pypi.org/project/wikipedia/) Python
    package to allow model query Wikipedia.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Let’s define a dictionary with all the functions our model knows now. This dictionary
    will help us to do routing later.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'I’ve made a couple of changes to our previous setup:'
  prefs: []
  type: TYPE_NORMAL
- en: I tweaked the system prompt a bit to force LLM to consult with Wikipedia if
    it needs some basic knowledge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I’ve changed the model to GPT 4 because it’s better for handling tasks requiring
    reasoning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: We can invoke our chain with all the functions. Let’s start with a pretty straightforward
    query.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We got in the result function call for `get_monthly_active_users` with input
    parameters — `{''month'': ''2023–04–01'', ''city'': ''Berlin''}` , which looks
    correct. The model was able to find the right tool and solve the task.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try to make task a bit more complex.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s pause for a minute and think how we would like the model to reason. It’s
    evident that there’s not enough information for the model to answer straight away,
    so it needs to make a bunch of function calls:'
  prefs: []
  type: TYPE_NORMAL
- en: call Wikipedia to get the capital of Germany
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: call the `get_monthly_active_users` function twice to get MAU for April and
    May
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: call `percentage_difference` to calculate the difference between metrics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It looks pretty complex. Let’s see whether ChatGPT would be able to handle this
    question.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the first call, LLM returned back a function call to Wikipedia with the
    following params — `{''term'': ''capital of Germany''}`. So far, it’s following
    our plan.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s provide the observation and see what the next steps will be.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The model wants to execute `get_monthly_active_users` with arguments `{''month'':
    ''2023–04–01'', ''city'': ''Berlin''}`. Let’s do it and return the information
    to the model once again.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the model requests to call `get_monthly_active_users` again with arguments
    `{''month'': ''2023–05–01'', ''city'': ''Berlin''}`. So far, it’s doing an excellent
    job. Let’s follow its logic.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The subsequent result is a function call for percentage_difference with the
    following arguments `{''metric1'': 168, ''metric2'': 1046}`. Let’s calculate observation
    and invoke our chain one more time. Hopefully, it will be the last step.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'In the end, we got the following response from the model: `The number of users
    from Berlin, the capital of Germany, increased by approximately 523.27% between
    April and May 2023.`'
  prefs: []
  type: TYPE_NORMAL
- en: Here’s the complete scheme of the LLM calls for this question.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/227c76d1bfd7066c2b81cb64329fb0e6.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration by author
  prefs: []
  type: TYPE_NORMAL
- en: In the above example, we triggered subsequent calls one by one manually, but
    it can be easily automated.
  prefs: []
  type: TYPE_NORMAL
- en: It’s a fantastic result, and we were able to see how LLMs can do reasoning and
    utilize multiple tools. It took model 5 steps to achieve the result, but it followed
    the plan we outlined initially, so it was a pretty logical path. However, if you
    plan to use LLMs in production, keep in mind that it might make mistakes and introduce
    evaluation and quality assurance processes.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the full code on [GitHub](https://github.com/miptgirl/miptgirl_medium/blob/main/analyst_agent/functions_how_to.ipynb).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This article taught us how to empower LLMs with external tools using OpenAI
    functions. We’ve examined two use cases: extraction to get structured output and
    routing to use external information for questions. The final result inspires me
    since LLM could answer pretty complex questions using three different tools.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s return to the initial question of whether LLMs can replace data analysts.
    Our current prototype is basic and far from the junior analysts’ capabilities,
    but it’s only the beginning. Stay tuned! We will dive deeper into the different
    approaches to LLM agents. Next time, we will try to create an agent that can access
    the database and answer basic questions.
  prefs: []
  type: TYPE_NORMAL
- en: Reference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article is inspired by the [“Functions, Tools and Agents with LangChain”](https://www.deeplearning.ai/short-courses/functions-tools-agents-langchain/)
    course from DeepLearning.AI
  prefs: []
  type: TYPE_NORMAL
