- en: Optimizing Neural Networks For Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/optimisation-algorithms-neural-networks-101-256e16a88412?source=collection_archive---------4-----------------------#2023-11-24](https://towardsdatascience.com/optimisation-algorithms-neural-networks-101-256e16a88412?source=collection_archive---------4-----------------------#2023-11-24)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to improve training beyond the “vanilla” gradient descent algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@egorhowell?source=post_page-----256e16a88412--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----256e16a88412--------------------------------)[](https://towardsdatascience.com/?source=post_page-----256e16a88412--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----256e16a88412--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----256e16a88412--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1cac491223b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimisation-algorithms-neural-networks-101-256e16a88412&user=Egor+Howell&userId=1cac491223b2&source=post_page-1cac491223b2----256e16a88412---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----256e16a88412--------------------------------)
    ·8 min read·Nov 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F256e16a88412&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimisation-algorithms-neural-networks-101-256e16a88412&user=Egor+Howell&userId=1cac491223b2&source=-----256e16a88412---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F256e16a88412&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimisation-algorithms-neural-networks-101-256e16a88412&source=-----256e16a88412---------------------bookmark_footer-----------)![](../Images/f4296e3c0bd744ca991919ad968f9e98.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network).neural
    network icons. Neural network icons created by andinur — Flaticon.'
  prefs: []
  type: TYPE_NORMAL
- en: Background
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In my last post, we discussed how you can improve the performance of neural
    networks through [***hyperparameter tuning***](/optimise-your-hyperparameter-tuning-with-hyperopt-861573239eb5):'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/hyperparameter-tuning-neural-networks-101-ca1102891b27?source=post_page-----256e16a88412--------------------------------)
    [## Hyperparameter Tuning: Neural Networks 101'
  prefs: []
  type: TYPE_NORMAL
- en: How you can improve the “learning” and “training” of neural networks through
    tuning hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/hyperparameter-tuning-neural-networks-101-ca1102891b27?source=post_page-----256e16a88412--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: This is a process whereby the best hyperparameters such as learning rate and
    number of hidden layers are “tuned” to find the most optimal ones for our network
    to boost its performance.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, this tuning process for large deep neural networks ([***deep
    learning***](https://en.wikipedia.org/wiki/Deep_learning)) is painstakingly slow.
    One way to improve upon this is to use *faster optimisers* than the traditional
    “vanilla” gradient descent method. In this post, we will dive into the most popular
    optimisers and variants of [***gradient descent***](/why-gradient-descent-is-so-common-in-data-science-def3e6515c5c)
    that can enhance the speed of training and also convergence and compare them in
    PyTorch!
  prefs: []
  type: TYPE_NORMAL
- en: 'Recap: Gradient Descent'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before diving in, let’s quickly brush up on our knowledge of gradient descent
    and the theory behind it.
  prefs: []
  type: TYPE_NORMAL
