- en: Optimizing Neural Networks For Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习中的神经网络优化
- en: 原文：[https://towardsdatascience.com/optimisation-algorithms-neural-networks-101-256e16a88412?source=collection_archive---------4-----------------------#2023-11-24](https://towardsdatascience.com/optimisation-algorithms-neural-networks-101-256e16a88412?source=collection_archive---------4-----------------------#2023-11-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/optimisation-algorithms-neural-networks-101-256e16a88412?source=collection_archive---------4-----------------------#2023-11-24](https://towardsdatascience.com/optimisation-algorithms-neural-networks-101-256e16a88412?source=collection_archive---------4-----------------------#2023-11-24)
- en: How to improve training beyond the “vanilla” gradient descent algorithm
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何在“基础”梯度下降算法之外提升训练效果
- en: '[](https://medium.com/@egorhowell?source=post_page-----256e16a88412--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----256e16a88412--------------------------------)[](https://towardsdatascience.com/?source=post_page-----256e16a88412--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----256e16a88412--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----256e16a88412--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@egorhowell?source=post_page-----256e16a88412--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----256e16a88412--------------------------------)[](https://towardsdatascience.com/?source=post_page-----256e16a88412--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----256e16a88412--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----256e16a88412--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1cac491223b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimisation-algorithms-neural-networks-101-256e16a88412&user=Egor+Howell&userId=1cac491223b2&source=post_page-1cac491223b2----256e16a88412---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----256e16a88412--------------------------------)
    ·8 min read·Nov 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F256e16a88412&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimisation-algorithms-neural-networks-101-256e16a88412&user=Egor+Howell&userId=1cac491223b2&source=-----256e16a88412---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1cac491223b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimisation-algorithms-neural-networks-101-256e16a88412&user=Egor+Howell&userId=1cac491223b2&source=post_page-1cac491223b2----256e16a88412---------------------post_header-----------)
    发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----256e16a88412--------------------------------)
    ·8分钟阅读·2023年11月24日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F256e16a88412&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimisation-algorithms-neural-networks-101-256e16a88412&user=Egor+Howell&userId=1cac491223b2&source=-----256e16a88412---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F256e16a88412&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimisation-algorithms-neural-networks-101-256e16a88412&source=-----256e16a88412---------------------bookmark_footer-----------)![](../Images/f4296e3c0bd744ca991919ad968f9e98.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F256e16a88412&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimisation-algorithms-neural-networks-101-256e16a88412&source=-----256e16a88412---------------------bookmark_footer-----------)![](../Images/f4296e3c0bd744ca991919ad968f9e98.png)'
- en: '[https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network).neural
    network icons. Neural network icons created by andinur — Flaticon.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network).神经网络图标。神经网络图标由andinur创建
    — Flaticon.'
- en: Background
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: 'In my last post, we discussed how you can improve the performance of neural
    networks through [***hyperparameter tuning***](/optimise-your-hyperparameter-tuning-with-hyperopt-861573239eb5):'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在我上一篇文章中，我们讨论了如何通过[***超参数调整***](/optimise-your-hyperparameter-tuning-with-hyperopt-861573239eb5)提高神经网络的性能：
- en: '[](/hyperparameter-tuning-neural-networks-101-ca1102891b27?source=post_page-----256e16a88412--------------------------------)
    [## Hyperparameter Tuning: Neural Networks 101'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/hyperparameter-tuning-neural-networks-101-ca1102891b27?source=post_page-----256e16a88412--------------------------------)
    [## 超参数调整：神经网络基础'
- en: How you can improve the “learning” and “training” of neural networks through
    tuning hyperparameters
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何通过调整超参数来提升神经网络的“学习”和“训练”
- en: towardsdatascience.com](/hyperparameter-tuning-neural-networks-101-ca1102891b27?source=post_page-----256e16a88412--------------------------------)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/hyperparameter-tuning-neural-networks-101-ca1102891b27?source=post_page-----256e16a88412--------------------------------)
- en: This is a process whereby the best hyperparameters such as learning rate and
    number of hidden layers are “tuned” to find the most optimal ones for our network
    to boost its performance.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个过程，通过该过程可以“调优”最佳超参数，如学习率和隐藏层数量，以找到最适合我们网络的超参数，从而提升其性能。
- en: Unfortunately, this tuning process for large deep neural networks ([***deep
    learning***](https://en.wikipedia.org/wiki/Deep_learning)) is painstakingly slow.
    One way to improve upon this is to use *faster optimisers* than the traditional
    “vanilla” gradient descent method. In this post, we will dive into the most popular
    optimisers and variants of [***gradient descent***](/why-gradient-descent-is-so-common-in-data-science-def3e6515c5c)
    that can enhance the speed of training and also convergence and compare them in
    PyTorch!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，对于大型深度神经网络（[***深度学习***](https://en.wikipedia.org/wiki/Deep_learning)）的调优过程非常缓慢。改进的方法之一是使用比传统的“原生”梯度下降方法更*快速的优化器*。在这篇文章中，我们将深入探讨最流行的优化器和[***梯度下降***](/why-gradient-descent-is-so-common-in-data-science-def3e6515c5c)的变体，这些可以提高训练速度以及收敛速度，并在PyTorch中进行比较！
- en: 'Recap: Gradient Descent'
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回顾：梯度下降
- en: Before diving in, let’s quickly brush up on our knowledge of gradient descent
    and the theory behind it.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入之前，我们快速复习一下梯度下降及其背后的理论。
