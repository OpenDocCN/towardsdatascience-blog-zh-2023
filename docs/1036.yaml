- en: Multimodal 3D Brain Tumor Segmentation with Azure ML and MONAI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/multimodal-3d-brain-tumor-segmentation-with-azure-ml-and-monai-4a721e42f5f7?source=collection_archive---------6-----------------------#2023-03-21](https://towardsdatascience.com/multimodal-3d-brain-tumor-segmentation-with-azure-ml-and-monai-4a721e42f5f7?source=collection_archive---------6-----------------------#2023-03-21)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Running the medical imaging framework at scale on an enterprise ML platform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@andreaskopp_89294?source=post_page-----4a721e42f5f7--------------------------------)[![Andreas
    Kopp](../Images/a3184ebc20f577c933a0e9a74eb0c291.png)](https://medium.com/@andreaskopp_89294?source=post_page-----4a721e42f5f7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4a721e42f5f7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4a721e42f5f7--------------------------------)
    [Andreas Kopp](https://medium.com/@andreaskopp_89294?source=post_page-----4a721e42f5f7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe712cdda5a0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultimodal-3d-brain-tumor-segmentation-with-azure-ml-and-monai-4a721e42f5f7&user=Andreas+Kopp&userId=e712cdda5a0c&source=post_page-e712cdda5a0c----4a721e42f5f7---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4a721e42f5f7--------------------------------)
    ·14 min read·Mar 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4a721e42f5f7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultimodal-3d-brain-tumor-segmentation-with-azure-ml-and-monai-4a721e42f5f7&user=Andreas+Kopp&userId=e712cdda5a0c&source=-----4a721e42f5f7---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4a721e42f5f7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultimodal-3d-brain-tumor-segmentation-with-azure-ml-and-monai-4a721e42f5f7&source=-----4a721e42f5f7---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: By [Harmke Alkemade](https://www.linkedin.com/in/harmke-alkemade/) and [Andreas
    Kopp](https://www.linkedin.com/in/andreas-kopp-1947183/)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be12bbfcd17e35b5f16c263d1eb3b757.png)'
  prefs: []
  type: TYPE_IMG
- en: 3D Brain Tumor Segmentation (*Image via Shutterstock under license to Andreas
    Kopp)*
  prefs: []
  type: TYPE_NORMAL
- en: We like to thank Brad Genereaux, Prerna Dogra, Kristopher Kersten, Ahmed Harouni,
    and Wenqi Li from NVIDIA and the MONAI team for their active support in the development
    of this asset.
  prefs: []
  type: TYPE_NORMAL
- en: Since December 2021, we have released several examples to support [Medical Imaging
    with Azure Machine Learning](https://github.com/Azure/medical-imaging), and the
    response we received was overwhelming. The interest and inquiries reinforce how
    AI is quickly becoming a crucial aspect of modern medical practice.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d06607cd34f626de5c5ed94ed3287380.png)'
  prefs: []
  type: TYPE_IMG
- en: Selected Medical Imaging Repository Use Cases (*Images 5,6 via Shutterstock
    under license to Andreas Kopp)*
  prefs: []
  type: TYPE_NORMAL
- en: 3D Brain tumor segmentation use case
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Today, we introduce a new medical imaging asset for [3D brain tumor segmentation](https://github.com/Azure/medical-imaging/tree/main/3d-brain-tumor-segmentation),
    a solution that tackles a challenging use case in the field of oncology. Our solution
    leverages volumetric visual input from multiple MRI modalities and different 3D
    glioma tumor segmentations to produce accurate predictions of tumor boundaries
    and sub-regions. To handle the high amount of image data involved in this task,
    our asset employs parallel training using scalable GPU resources on Azure ML.
    Furthermore, we leverage the Medical Open Network for Artificial Intelligence
    (MONAI), a domain-specific framework that provides state-of-the-art tools and
    methods for medical image analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Combining Azure Machine Learning and MONAI provides valuable synergies for
    scalable ML development and operations with specific innovations for medical imaging:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Azure Machine Learning**](https://azure.microsoft.com/en-us/products/machine-learning)
    is a cloud-based platform that provides a collaborative environment for data scientists
    and ML engineers to develop and deploy machine learning models at scale. It offers
    a range of authoring options, from no-code/low-code Automated ML to code-first
    using popular tools like VSCode. Users have access to scalable compute resources
    for training and deployment, making it ideal for handling large datasets and complex
    models. Additionally, it includes MLOps capabilities that enable the management
    and maintenance and versioning of ML assets, and allows for automated training
    and deployment pipelines. The platform includes responsible AI tools that can
    help explain models and mitigate potential bias, making it an ideal choice for
    businesses looking to develop and deploy ethical AI solutions.'
  prefs: []
  type: TYPE_NORMAL
- en: '[**The Medical Open Network for Artificial Intelligence (MONAI)**](https://monai.io/)
    is an open-source PyTorch-based project designed for medical imaging. It provides
    a comprehensive set of tools for building and deploying AI models for healthcare
    imaging:'
  prefs: []
  type: TYPE_NORMAL
- en: '**MONAI Label** for AI-assisted labeling medical imaging data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MONAI Core** for training AI models with domain-specific capabilities'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MONAI Deploy** for packaging, testing, and deploying medical AI applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MONAI Core is the focus of the solution described in this article. It offers
    native support for commonly used medical imaging formats such as Nifty and DICOM.
    It also includes features such as dictionary transforms that ensure consistency
    between images and segmentations in complex transformation pipelines. Moreover,
    MONAI Core provides a portfolio of network architectures, including state-of-the-art
    transformer-based 3D segmentation algorithms like UNETR.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to explore our demo and use it as a template for your own 3D segmentation
    use cases in (or beyond) medical domains.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are using the 2021 version of the [Brain Tumor Segmentation (BraTS) Challenge](http://braintumorsegmentation.org/)
    dataset. It is a collection of expert-annotated multimodal 3D magnetic resonance
    imaging (MRI) scans of brain tumors from different institutions. MRI is a medical
    imaging technique that uses a combination of magnetic fields and radio waves to
    visualize the internal structures of the body. Different MRI modalities can be
    generated by altering the imaging parameters, which results in varying tissue
    contrasts, making certain features more prominent in the images. The BraTS dataset
    uses the following modalities:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Native T1 (**T1**): Can be used to differentiate between various tissue types
    and pathological conditions. T1 native is often used to extract quantitative information
    about tissue properties.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'T1-weighted with contrast agent Gadolinium (**T1Gd**): This modality can be
    used for delineating tumor boundaries and identifying areas of active tumor growth.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'T2-weighted (**T2**): These images are useful for detecting edema (fluid accumulation),
    inflammation, and other changes in brain tissue that may be associated with tumors.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'T2-weighted Fluid-Attenuated Inversion Recovery (**T2-FLAIR**): T2-FLAIR images
    are useful for identifying infiltrative tumor margins and non-enhancing tumor
    components.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following illustration provides an example of the expert-annotated tumor
    segmentations available with the BraTS dataset: Tumor core, whole tumor, and enhancing
    structure. The image in the upper left corner combines the three segmentations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a00f76d7a1dc145e7f92caf8ee1b43e3.png)'
  prefs: []
  type: TYPE_IMG
- en: Brain tumor segmentations derived from the BraTS 2021 dataset (image by the
    authors)
  prefs: []
  type: TYPE_NORMAL
- en: Being able to distinguish between these structures is critical for diagnosis,
    prognosis, and treatment planning. In the BraTS dataset, the whole tumor refers
    to the complete volume of the tumor, including the core and any surrounding edema
    or swelling. The core is the region within the tumor that contains the most aggressive
    cancer cells, known as the necrotic and active tumor. The enhancing structure
    refers to the region of the tumor that appears bright and enhances with contrast
    on MRI scans. This region is associated with the most active and invasive cancer
    cells.
  prefs: []
  type: TYPE_NORMAL
- en: Bringing it to action
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, that we have gained an overview about the use case, let’s see how we are
    using Azure Machine Learning and MONAI to train and deploy a machine learning
    model for this interesting task.
  prefs: []
  type: TYPE_NORMAL
- en: 'The end-to-end workflow of this use case is implemented in our [orchestration
    notebook](https://github.com/Azure/medical-imaging/blob/main/3d-brain-tumor-segmentation/3d-brain-tumor-seg-BRATS2021.ipynb).
    These are the main steps and their associated outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0ade5cb0596b1028c7d46d2e2f823dcc.png)'
  prefs: []
  type: TYPE_IMG
- en: Brain tumor segmentation workflow and outputs (image by the authors)
  prefs: []
  type: TYPE_NORMAL
- en: The first steps involve downloading and visualizing the data, followed by submitting
    a training job to an Azure ML Compute Cluster. Once the model is trained and registered,
    it is deployed as an Azure ML Managed Endpoint. The notebook concludes with visualizing
    model predictions on an image volume from the validation set. The training and
    inferencing scripts are stored separately in the repository.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/04290d9732eb031470492ec0e52b98a0.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualizing the tumor core in the itkwidgets 3D viewer (image by the authors)
  prefs: []
  type: TYPE_NORMAL
- en: To manage machine learning assets like datasets, compute resources, experiments
    and the resulting model in our workflow, we use the Azure ML SDK to create a connection
    to the Azure ML Workspace. This connection can be established from any Python
    environment and does not require an Azure ML Compute Instance to run the code.
    By managing the assets with Azure ML, they can be found in the Azure ML Studio
    for future reference. This facilitates easy comparison between different experiments
    and creates a lineage for the resulting model by referencing the associated assets
    (dataset version, training code, environment etc.).
  prefs: []
  type: TYPE_NORMAL
- en: The data is downloaded from Kaggle using the Kaggle API and registered as an
    Azure ML managed dataset. This helps with versioning the training data, which
    is especially helpful in an iterative model development scenario where the training
    data may change over time. For instance, the BraTS dataset has been updated continuously
    since 2012\. Linking the data version to a model ensures good governance.
  prefs: []
  type: TYPE_NORMAL
- en: The [training script](https://github.com/Azure/medical-imaging/blob/main/3d-brain-tumor-segmentation/src/train-brats21.py)
    uses the MONAI framework along with native PyTorch classes. The data is pre-processed
    using several transforms provided by the MONAI framework, mainly using the dictionary-based
    wrappers. The different MRI modalities of the training images are stored as separate
    files and there is one file for the segmentation masks. The dictionary-based wrappers
    enable using a single pre-processing pipeline for all training data, where every
    transformation in the pipeline can be specified to which part of the data it should
    be applied.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example illustrates the benefit of MONAI dictionary transforms
    to ensure consistency between the image and the associated segmentation labels.
    Imagine we use a pipeline of random transforms for data augmentation that affect
    the shape or position of the images (resize, flip, rotation, adjustment of perspective
    etc.). We have to make sure that the identical transformations are applied to
    the segmentation images to ensure consistency between images and labels for the
    downstream training process. MONAI dictionary transforms are a handy tool in this
    scenario: We simply need to specify if a transform should be applied to the image,
    the label or both. In this example, we will apply the transformations to both
    objects. MONAI also ensures consistency in the case of random transformations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c905e76650b5865e7727d15d95bf05d8.png)'
  prefs: []
  type: TYPE_IMG
- en: Dictionary transform example for random horizontal flip (image by the authors)
  prefs: []
  type: TYPE_NORMAL
- en: MONAI provides a portfolio of PyTorch based network architectures. We have chosen
    SegResNet (a variant of the ResNet architecture adapted for segmentation tasks)
    for our task. It is based on an encoder-decoder framework, where the encoder extracts
    features from the input image and the decoder reconstructs the output segmentation
    mask.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The number of encoder blocks for downsampling is specified by the `blocks_down`
    parameter. Accordingly, the upsampling blocks of the decoder part are defined
    with the `blocks_up` parameter. 16 filters are used in the first convolutional
    layer. The number is typically doubled as the network goes deeper, allowing it
    to learn more complex features. The network accepts input with 4 channels, specified
    by `in_channels=4`. This represents our multi-modal MRI data, where each channel
    corresponds to a different MRI modality: T1, T1Gd, T2 and T2-FLAIR. The output
    of the network has 3 channels, as specified by `out_channels=3`. This corresponds
    to our segmentation task classes of whole tumor, tumor core and enhancing structure.'
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch native features are used to include support for Distributed Data Parallel
    (DDP) training with multiple GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: For the loss function, we leverage the Dice loss from MONAI. It is a common
    loss function used to measure the similarity between the predicted segmentation
    and the ground truth. It is particularly useful for problems with imbalanced class
    distributions, such as medical image segmentation, where the target region (e.g.,
    tumor) is usually much smaller than the background.
  prefs: []
  type: TYPE_NORMAL
- en: The Adam optimizer is used in conjunction with a cosine annealing learning rate
    scheduler (CosineAnnealingLR). The scheduler oscillates the learning rate between
    an initial value and the specified minimum rate. CosineAnnealingLR can help improve
    model performance by allowing it to escape local minima during the initial stages
    of training and enabling precise adjustments in the later stages.
  prefs: []
  type: TYPE_NORMAL
- en: 'The training script is submitted to an Azure ML Compute Cluster via a Command
    Job, using the definition as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: When we submit the training job, a container is spun up in the Compute Cluster
    using the environment “monai-multi-gpu” that we defined earlier in the notebook.
    This environment is built from a conda specification file which contains the requirements
    for the training script. The resulting Docker image is cached in the Azure Container
    Registry that is linked to the Azure ML Workspace.
  prefs: []
  type: TYPE_NORMAL
- en: By using the `distribution` parameter, we initiate the number of processes as
    part of the distributed data parallel training. Since we used a machine with four
    GPUs for this experiment, we initiate four processes, each with access to their
    own GPU instance. Additionally, we can parallelize the training run over different
    nodes of the Compute Cluster by using the `resources` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding GPU utilization is crucial for optimizing resource usage and preventing
    the dreaded Out of Memory (OOM) errors that can occur during lengthy PyTorch training
    runs. Azure ML offers a range of metrics to track network and disk I/O, as well
    as CPU and GPU memory and processor utilization for active jobs. The following
    example demonstrates GPU memory and energy consumption at the start of a training
    run on a multi-GPU cluster node of the Standard_NC24rs_v3 type. In this scenario,
    we can see that the 16 GB of memory available on each of the four GPUs is being
    effectively utilized. There is only small headroom remaining, which makes clear
    that increasing the batch size could heighten the risk of encountering OOM errors.
    Additionally, Azure ML allows us to monitor the energy consumption of each GPU,
    measured in kilojoules.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/26e1fcb5788b50ab98cf8494bc6005a9.png)'
  prefs: []
  type: TYPE_IMG
- en: Monitoring multi-GPU resource consumption during training (image by the authors)
  prefs: []
  type: TYPE_NORMAL
- en: Training with extensive datasets can require a significant amount of time, potentially
    taking several hours or even days to complete. The benefit of using a cloud-based
    data science and machine learning platform like Azure ML lies in the ability to
    access powerful resources on-demand as needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our final model, we utilized NC96ads_A100_v4 cluster nodes with 96 cores,
    880 GB of CPU memory, and 80 GB of memory for each of the four NVIDIA A100 GPUs
    to train over 150 epochs. The primary advantage of this setup is the on-demand
    availability of powerful resources: once training is complete, the cluster automatically
    shuts down, eliminating any further costs.'
  prefs: []
  type: TYPE_NORMAL
- en: In our case, we opted for a “low priority” SKU, which offers a more cost-effective
    solution compared to dedicated compute resources. However, this choice comes with
    the risk of a running training job being preempted if the compute resources are
    required for other tasks.
  prefs: []
  type: TYPE_NORMAL
- en: While the training job is running, JupyterLab, VSCode, and Tensorboard are executed
    in the container, which makes it accessible for monitoring and debugging purposes.
    This is specified in the `services` parameter. We use Tensorboard and MLFlow logging
    in the training script to track training metrics, and MLFlow to register the final
    model. Both the metrics and model can be found as part of the experiment in the
    Azure ML Studio due to the native MLFlow support in Azure ML. By accessing the
    Tensorboard instance running on the container, we can review training and validation
    metrics while our model is trained.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a6777327e55b4160b10d16f897962e11.png)'
  prefs: []
  type: TYPE_IMG
- en: Monitoring training performance using interactive jobs with Tensorboard (image
    by the authors)
  prefs: []
  type: TYPE_NORMAL
- en: 'The Dice coefficient is used to evaluate model performance, which is a commonly
    used metric for object segmentation models. It measures the similarity between
    the predicted segmentation mask and the ground truth mask. A Dice coefficient
    of 1.0 indicates a perfect overlap between the predicted and the ground truth
    masks. Separate Dice metrics are tracked for the different classes in the dataset:
    Tumor core (*val_dice_tc*), whole tumor (*val_dice_wt*) and enhancing structure
    (*val_dice_et*). The average Dice metric across the different classes is also
    tracked (*val_mean_dice*). Running an experiment with 150 epochs results in the
    following metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/14a154808c996fae0e64eda9ef596375.png)'
  prefs: []
  type: TYPE_IMG
- en: Validation DICE metrics during 150 epoch training in Azure ML Studio (image
    by the authors)
  prefs: []
  type: TYPE_NORMAL
- en: After registering the model, we move on to the deployment, which is a way to
    make the model accessible for predicting segmentation masks on new MRI images
    in a production setting. A possible deployment scenario for brain tumor segmentation
    is to integrate it with an MRI image viewer for diagnostic assistance. In the
    notebook, Azure ML Managed Endpoints are leveraged for deployment. First, a Managed
    Online Endpoint is created, which provides an interface for sending requests and
    receiving the inference output of a model. It also provides authentication and
    monitoring capabilities. Then, we create a deployment by defining the model, scoring
    script, environment, and target compute type. We reference the latest registered
    BraTS model in the Workspace for this deployment.
  prefs: []
  type: TYPE_NORMAL
- en: After sending a binary encoded version of the four modalities for a stack of
    MRI images to our endpoint, we receive predicted segmentations as part of the
    JSON response. The final part of the orchestration notebook contains code to visualize
    these predictions. Our image slider shows the predicted segmentations for each
    slice side by side with the ground truth to compare them.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9de27c8554fd29c9ed43f12a41d92d44.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparing expert-annotated (ground truth) with predicted tumor structures (image
    by the authors)
  prefs: []
  type: TYPE_NORMAL
- en: Responsible use of AI in Healthcare
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Incorporating machine learning into clinical practices, such as tumor detection
    and analysis, offers numerous benefits. For instance, algorithms can assist radiologists
    by flagging suspicious areas in the myriad of images generated daily. Another
    application involves the consistent monitoring of tumor progression to evaluate
    the effectiveness of cancer treatments. Manually calculating tumor volume from
    a collection of over a hundred slices per patient study can be incredibly time-consuming,
    but AI-assisted measurements can significantly expedite the process.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the substantial advancements in AI-assisted image analysis, errors can
    still occur. Undetected tumors pose a considerable concern, and rare tumors or
    underrepresented patient characteristics in the training data can contribute to
    such false negatives. Consequently, it is crucial to address these limitations
    by incorporating diverse datasets and continuously refining the algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Physicians must continue to hold responsibility for diagnoses and treatment
    decisions. Machine learning predictions should be viewed as valuable and supportive
    tools, but not as infallible replacements for human expertise. The physician remains
    at the forefront of the entire process, ensuring that patient care is informed
    by both technology and professional judgment.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this article, we explored the use of Azure Machine Learning and MONAI for
    3D brain tumor segmentation, a challenging use case in the field of oncology.
    We discussed the various modalities of MRI images and the importance of distinguishing
    between tumor core, whole tumor, and enhancing structure for diagnosis, prognosis,
    and treatment planning. We also outlined the end-to-end workflow of this use case,
    including data visualization, training and registration of the model, deployment,
    and visualization of model predictions. The use of Azure Machine Learning and
    MONAI allowed us to employ parallel training using scalable GPU resources and
    provided domain-specific capabilities for training AI models for healthcare imaging.
  prefs: []
  type: TYPE_NORMAL
- en: We hope that [our asset](https://github.com/Azure/medical-imaging/tree/main/3d-brain-tumor-segmentation)
    can serve as a template for future 3D segmentation use cases in medical domains
    and contribute to the growing importance of AI in modern medical practice.
  prefs: []
  type: TYPE_NORMAL
- en: The training code of this asset is based on an earlier version of the [MONAI
    tutorial for 3D Brain Tumor Segmentation](https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/swin_unetr_brats21_segmentation_3d.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1]: Hatamizadeh, A., Nath, V., Tang, Y., Yang, D., Roth, H. and Xu, D., 2022\.
    Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI
    Images. arXiv preprint arXiv:2201.01266.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2]: Tang, Y., Yang, D., Li, W., Roth, H.R., Landman, B., Xu, D., Nath, V.
    and Hatamizadeh, A., 2022\. Self-supervised pre-training of swin transformers
    for 3d medical image analysis. In Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (pp. 20730–20740).'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] U.Baid, et al., The RSNA-ASNR-MICCAI BraTS 2021 Benchmark on Brain Tumor
    Segmentation and Radiogenomic Classification, arXiv:2107.02314, 2021.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby,
    et al. “The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)”, IEEE
    Transactions on Medical Imaging 34(10), 1993–2024 (2015) DOI: 10.1109/TMI.2014.2377694'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J.S. Kirby, et
    al., “Advancing The Cancer Genome Atlas glioma MRI collections with expert segmentation
    labels and radiomic features”, Nature Scientific Data, 4:170117 (2017) DOI: 10.1038/sdata.2017.117'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J. Kirby, et al.,
    “Segmentation Labels and Radiomic Features for the Pre-operative Scans of the
    TCGA-GBM collection”, The Cancer Imaging Archive, 2017\. DOI: 10.7937/K9/TCIA.2017.KLXWJJ1Q'
  prefs: []
  type: TYPE_NORMAL
