- en: Running Falcon Inference on a CPU with Hugging Face Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/running-falcon-on-a-cpu-with-hugging-face-pipelines-b60b3b8a32a3?source=collection_archive---------2-----------------------#2023-06-06](https://towardsdatascience.com/running-falcon-on-a-cpu-with-hugging-face-pipelines-b60b3b8a32a3?source=collection_archive---------2-----------------------#2023-06-06)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/344b0c93c3824c6cec55f3db8be1c23a.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Image Source](https://www.freepik.com/free-photo/wild-eagle-standing-majestic-scene-generative-ai_40949970.htm#query=falcon&position=0&from_view=search&track=sph)'
  prefs: []
  type: TYPE_NORMAL
- en: Learn how to run inference with 7-billion and 40-billion Falcon on a 4th Gen
    Xeon CPU with Hugging Face Pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://eduand-alvarez.medium.com/?source=post_page-----b60b3b8a32a3--------------------------------)[![Eduardo
    Alvarez](../Images/afa0ad855c8ec2e977ebbe60dc3e77a4.png)](https://eduand-alvarez.medium.com/?source=post_page-----b60b3b8a32a3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b60b3b8a32a3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b60b3b8a32a3--------------------------------)
    [Eduardo Alvarez](https://eduand-alvarez.medium.com/?source=post_page-----b60b3b8a32a3--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe49cc416a8ef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frunning-falcon-on-a-cpu-with-hugging-face-pipelines-b60b3b8a32a3&user=Eduardo+Alvarez&userId=e49cc416a8ef&source=post_page-e49cc416a8ef----b60b3b8a32a3---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b60b3b8a32a3--------------------------------)
    ·5 min read·Jun 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb60b3b8a32a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frunning-falcon-on-a-cpu-with-hugging-face-pipelines-b60b3b8a32a3&user=Eduardo+Alvarez&userId=e49cc416a8ef&source=-----b60b3b8a32a3---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb60b3b8a32a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frunning-falcon-on-a-cpu-with-hugging-face-pipelines-b60b3b8a32a3&source=-----b60b3b8a32a3---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: It’s easy to assume that the only way that we can perform inference with LLMs
    that are made up of billions of parameters is with a GPU. While it’s true that
    GPUs provide significant accelerations over CPUs in deep learning, the hardware
    should always be selected based on the use case. For example, suppose your end
    users only need a response every 30 secs. In that case, there’s a diminishing
    return if you’re struggling (financially and logistically) to reserve accelerators
    that give you answers in < 30 secs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/11d61d846e9add1d0566bd508e52052a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. Working backward from end-users to hardware and software stack —
    thinking like a “Compute Aware AI Developer” — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: This all comes back to a fundamental principle, being a “Compute Aware AI Developer”
    — working backward from the goals of your application to the right software and
    hardware to use. Imagine starting a home project like hanging a new shelf and
    going straight for the sledgehammer without even considering that a smaller and
    more precise hammer would be the right tool for the project.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will perform inference with [Falcon-7b](https://huggingface.co/tiiuae/falcon-7b)
    and [Falcon-40b](https://huggingface.co/tiiuae/falcon-40b) on a [4th Generation
    Xeon CPU](https://www.intel.com/content/www/us/en/newsroom/news/4th-gen-xeon-scalable-processors-max-series-cpus-gpus.html#gs.zlhfe8)
    using Hugging Face [Pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines).
    Falcon-40b is a 40-billion parameter decoder-only model developed by the Technology
    Innovation Institute (TII) in Abu Dhabi. It outperforms several models like LLaMA,
    StableLM, RedPajama, and MPT, utilizing the FlashAttention method to achieve faster
    and optimized inference, resulting in significant speed improvements across different
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Environment Setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you have accessed your Xeon compute instance, you must secure enough storage
    to download the checkpoints and model shards for Falcon. We recommend securing
    at least 150 GB of storage if you want to test both the 7-billion and 40-billion
    Falcon versions. You must also provide enough RAM to load the model into memory
    and cores to run the workload efficiently. We successfully ran the 7-Billion and
    40-Billion Falcon versions on a 32-core 64GB RAM VM (4th Gen Xeon) on the [Intel
    Developer Cloud](https://bit.ly/3Fewcto). However, this is one of many valid compute
    specifications, and further testing would likely improve performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Install miniconda. You can find the latest version on their website: [https://docs.conda.io/en/latest/miniconda.html](https://docs.conda.io/en/latest/miniconda.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a conda environment `conda create -n falcon python==3.8.10`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install dependencies `pip install -r requirements.txt` . You can find the contents
    requirements.txt file below.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 4\. Activate your conda environment `conda activate falcon`
  prefs: []
  type: TYPE_NORMAL
- en: Running Falcon with Hugging Face Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Hugging Face](https://www.intel.com/content/www/us/en/developer/partner/hugging-face.html)
    pipelines provide a simple and high-level interface for applying pre-trained models
    to various natural language processing (NLP) tasks, such as text classification,
    named entity recognition, text generation, and more. These pipelines abstract
    away the complexities of model loading, tokenization, and inference, allowing
    users to quickly utilize state-of-the-art models for NLP tasks with just a few
    lines of code.'
  prefs: []
  type: TYPE_NORMAL
- en: Below is a convenient script you can run in the cmd/terminal to experiment with
    the raw pre-trained Falcon models.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To run the script (falcon-demo.py) you must provide the script and various
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`python falcon-demo.py --falcon_version "7b" --max_length 25 --top_k 5`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The script has 3 optional parameters to help control the execution of the Hugging
    Face pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '**falcon_version:** allows you to select from Falcon’s 7 billion or 40 billion
    parameter versions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**max_length:** used to control the maximum length of the generated text in
    text generation tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**top_k:** specifies the number of highest probability tokens to consider at
    each step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can hack the script to add/remove/edit the parameters. What is important
    is that you now have access to one of the most powerful open-source models ever
    released!
  prefs: []
  type: TYPE_NORMAL
- en: Playing with Raw Falcon
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Raw Falcon is not tuned for any particular purpose, so it will likely spew nonsense
    (Figure 2). Still, this doesn’t stop us from asking a few questions to test it
    out. When the script is done downloading the model and creating the pipeline,
    you will be prompted to provide input to the model. When you’re ready to stop,
    type “stop”.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8825f4b73d5939d9eca8153326a8ffd3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. Command line interface inference test of 7 Billion Parameter Falcon
    Model on Intel 4th Gen Xeon with default script parameters — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The script prints the inference time to give you an idea of how long the model
    takes to respond based on the current parameters provided to the pipeline and
    the compute you have made available to this workload.
  prefs: []
  type: TYPE_NORMAL
- en: '*Tip: You can significantly alter the inference time by adjusting the max_length
    parameter.*'
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial is designed to share how to get Falcon running on a CPU with Hugging
    Face Transformers but does not explore options for further optimizations on Intel
    CPUs. Libraries like the [Intel Extension for Transformers](https://github.com/intel/intel-extension-for-transformers)
    offer capabilities to accelerate Transformer-based models through techniques like
    quantization, distillation, and pruning. Quantization is a widely-used model compression
    technique that can reduce the model size and improve inference latency — this
    would be a valuable next step to explore enhancing the performance of this workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Summary and Discussion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Foundational LLMs create opportunities for developers to build exciting AI applications.
    However, half the battle is usually finding a model with the correct license that
    allows for commercial derivatives. Falcon presents a rare opportunity because
    it intersects performance and license flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: Although Falcon is fairly democratized from an open-source perspective, its
    size creates new challenges for engineers/enthusiasts. This tutorial helped address
    this by combining Falcon’s “truly open” license, Hugging Face Pipelines, and the
    availability/accessibility of CPUs to give developers more access to this powerful
    model.
  prefs: []
  type: TYPE_NORMAL
- en: '**A few exciting things to try would be:**'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tune Falcon to a specific task by leveraging the [Intel Extension for PyTorch](https://www.intel.com/content/www/us/en/developer/tools/oneapi/optimization-for-pytorch.html#gs.0ugs0o)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use model compression tools available in [Intel Neural Compressor (INC)](https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html#gs.zm5zqx)
    and [Intel Extension for Transformers](https://github.com/intel/intel-extension-for-transformers)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Play with the parameters of Hugging Face pipelines to optimize performance for
    your particular use case.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Don’t forget to follow*** [***my profile for more articles***](https://eduand-alvarez.medium.com/)
    ***like this!***'
  prefs: []
  type: TYPE_NORMAL
