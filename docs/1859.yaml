- en: Running Falcon Inference on a CPU with Hugging Face Pipelines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在CPU上使用Hugging Face Pipelines运行Falcon推断
- en: 原文：[https://towardsdatascience.com/running-falcon-on-a-cpu-with-hugging-face-pipelines-b60b3b8a32a3?source=collection_archive---------2-----------------------#2023-06-06](https://towardsdatascience.com/running-falcon-on-a-cpu-with-hugging-face-pipelines-b60b3b8a32a3?source=collection_archive---------2-----------------------#2023-06-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/running-falcon-on-a-cpu-with-hugging-face-pipelines-b60b3b8a32a3?source=collection_archive---------2-----------------------#2023-06-06](https://towardsdatascience.com/running-falcon-on-a-cpu-with-hugging-face-pipelines-b60b3b8a32a3?source=collection_archive---------2-----------------------#2023-06-06)
- en: '![](../Images/344b0c93c3824c6cec55f3db8be1c23a.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/344b0c93c3824c6cec55f3db8be1c23a.png)'
- en: '[Image Source](https://www.freepik.com/free-photo/wild-eagle-standing-majestic-scene-generative-ai_40949970.htm#query=falcon&position=0&from_view=search&track=sph)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[图片来源](https://www.freepik.com/free-photo/wild-eagle-standing-majestic-scene-generative-ai_40949970.htm#query=falcon&position=0&from_view=search&track=sph)'
- en: Learn how to run inference with 7-billion and 40-billion Falcon on a 4th Gen
    Xeon CPU with Hugging Face Pipelines
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解如何在第4代Xeon CPU上使用Hugging Face Pipelines运行7亿和40亿Falcon模型
- en: '[](https://eduand-alvarez.medium.com/?source=post_page-----b60b3b8a32a3--------------------------------)[![Eduardo
    Alvarez](../Images/afa0ad855c8ec2e977ebbe60dc3e77a4.png)](https://eduand-alvarez.medium.com/?source=post_page-----b60b3b8a32a3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b60b3b8a32a3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b60b3b8a32a3--------------------------------)
    [Eduardo Alvarez](https://eduand-alvarez.medium.com/?source=post_page-----b60b3b8a32a3--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://eduand-alvarez.medium.com/?source=post_page-----b60b3b8a32a3--------------------------------)[![爱德华多·阿尔瓦雷斯](../Images/afa0ad855c8ec2e977ebbe60dc3e77a4.png)](https://eduand-alvarez.medium.com/?source=post_page-----b60b3b8a32a3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b60b3b8a32a3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b60b3b8a32a3--------------------------------)
    [爱德华多·阿尔瓦雷斯](https://eduand-alvarez.medium.com/?source=post_page-----b60b3b8a32a3--------------------------------)'
- en: ·
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe49cc416a8ef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frunning-falcon-on-a-cpu-with-hugging-face-pipelines-b60b3b8a32a3&user=Eduardo+Alvarez&userId=e49cc416a8ef&source=post_page-e49cc416a8ef----b60b3b8a32a3---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b60b3b8a32a3--------------------------------)
    ·5 min read·Jun 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb60b3b8a32a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frunning-falcon-on-a-cpu-with-hugging-face-pipelines-b60b3b8a32a3&user=Eduardo+Alvarez&userId=e49cc416a8ef&source=-----b60b3b8a32a3---------------------clap_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe49cc416a8ef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frunning-falcon-on-a-cpu-with-hugging-face-pipelines-b60b3b8a32a3&user=Eduardo+Alvarez&userId=e49cc416a8ef&source=post_page-e49cc416a8ef----b60b3b8a32a3---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b60b3b8a32a3--------------------------------)
    ·5分钟阅读·2023年6月6日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb60b3b8a32a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frunning-falcon-on-a-cpu-with-hugging-face-pipelines-b60b3b8a32a3&user=Eduardo+Alvarez&userId=e49cc416a8ef&source=-----b60b3b8a32a3---------------------clap_footer-----------)'
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb60b3b8a32a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frunning-falcon-on-a-cpu-with-hugging-face-pipelines-b60b3b8a32a3&source=-----b60b3b8a32a3---------------------bookmark_footer-----------)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb60b3b8a32a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frunning-falcon-on-a-cpu-with-hugging-face-pipelines-b60b3b8a32a3&source=-----b60b3b8a32a3---------------------bookmark_footer-----------)'
- en: It’s easy to assume that the only way that we can perform inference with LLMs
    that are made up of billions of parameters is with a GPU. While it’s true that
    GPUs provide significant accelerations over CPUs in deep learning, the hardware
    should always be selected based on the use case. For example, suppose your end
    users only need a response every 30 secs. In that case, there’s a diminishing
    return if you’re struggling (financially and logistically) to reserve accelerators
    that give you answers in < 30 secs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易认为我们只能通过 GPU 执行由数十亿参数构成的 LLM 推理。虽然 GPU 在深度学习中相较于 CPU 提供了显著的加速，但硬件的选择应始终基于具体的使用案例。例如，假设你的终端用户只需要每
    30 秒得到一次响应。如果你在经济和后勤上都很困难去预留能在 < 30 秒内给出答案的加速器，那么你可能会遇到收益递减的问题。
- en: '![](../Images/11d61d846e9add1d0566bd508e52052a.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11d61d846e9add1d0566bd508e52052a.png)'
- en: Figure 1\. Working backward from end-users to hardware and software stack —
    thinking like a “Compute Aware AI Developer” — Image by Author
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 从终端用户到硬件和软件堆栈的反向思考——像“计算意识的 AI 开发者”一样思考——图像由作者提供
- en: This all comes back to a fundamental principle, being a “Compute Aware AI Developer”
    — working backward from the goals of your application to the right software and
    hardware to use. Imagine starting a home project like hanging a new shelf and
    going straight for the sledgehammer without even considering that a smaller and
    more precise hammer would be the right tool for the project.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都回到一个基本原则，即成为一个“计算意识的 AI 开发者”——从应用程序的目标反向推导出使用的正确软件和硬件。想象一下开始一个家庭项目，比如挂一个新架子，却直接使用大锤，而没有考虑到一个更小、更精确的锤子可能更适合这个项目。
- en: In this article, we will perform inference with [Falcon-7b](https://huggingface.co/tiiuae/falcon-7b)
    and [Falcon-40b](https://huggingface.co/tiiuae/falcon-40b) on a [4th Generation
    Xeon CPU](https://www.intel.com/content/www/us/en/newsroom/news/4th-gen-xeon-scalable-processors-max-series-cpus-gpus.html#gs.zlhfe8)
    using Hugging Face [Pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines).
    Falcon-40b is a 40-billion parameter decoder-only model developed by the Technology
    Innovation Institute (TII) in Abu Dhabi. It outperforms several models like LLaMA,
    StableLM, RedPajama, and MPT, utilizing the FlashAttention method to achieve faster
    and optimized inference, resulting in significant speed improvements across different
    tasks.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将使用 Hugging Face [Pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines)
    在 [4th Generation Xeon CPU](https://www.intel.com/content/www/us/en/newsroom/news/4th-gen-xeon-scalable-processors-max-series-cpus-gpus.html#gs.zlhfe8)
    上对 [Falcon-7b](https://huggingface.co/tiiuae/falcon-7b) 和 [Falcon-40b](https://huggingface.co/tiiuae/falcon-40b)
    进行推理。Falcon-40b 是由阿布扎比技术创新研究院 (TII) 开发的一个 40 亿参数的解码器模型。它在多个模型如 LLaMA、StableLM、RedPajama
    和 MPT 上表现优越，利用 FlashAttention 方法实现更快和优化的推理，显著提高了在不同任务中的速度。
- en: Environment Setup
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 环境设置
- en: Once you have accessed your Xeon compute instance, you must secure enough storage
    to download the checkpoints and model shards for Falcon. We recommend securing
    at least 150 GB of storage if you want to test both the 7-billion and 40-billion
    Falcon versions. You must also provide enough RAM to load the model into memory
    and cores to run the workload efficiently. We successfully ran the 7-Billion and
    40-Billion Falcon versions on a 32-core 64GB RAM VM (4th Gen Xeon) on the [Intel
    Developer Cloud](https://bit.ly/3Fewcto). However, this is one of many valid compute
    specifications, and further testing would likely improve performance.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦访问到你的 Xeon 计算实例，你必须确保有足够的存储空间来下载 Falcon 的检查点和模型碎片。如果你想同时测试 7 亿和 40 亿的 Falcon
    版本，我们建议至少确保 150 GB 的存储空间。你还必须提供足够的 RAM 以将模型加载到内存中，并提供足够的核心以高效运行工作负载。我们成功地在 [Intel
    Developer Cloud](https://bit.ly/3Fewcto) 的 32 核心 64GB RAM 虚拟机（第 4 代 Xeon）上运行了
    7 亿和 40 亿 Falcon 版本。然而，这只是众多有效计算规格中的一种，进一步的测试可能会提高性能。
- en: 'Install miniconda. You can find the latest version on their website: [https://docs.conda.io/en/latest/miniconda.html](https://docs.conda.io/en/latest/miniconda.html)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 miniconda。你可以在他们的网站找到最新版本：[https://docs.conda.io/en/latest/miniconda.html](https://docs.conda.io/en/latest/miniconda.html)
- en: Create a conda environment `conda create -n falcon python==3.8.10`
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 conda 环境 `conda create -n falcon python==3.8.10`
- en: Install dependencies `pip install -r requirements.txt` . You can find the contents
    requirements.txt file below.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装依赖项 `pip install -r requirements.txt`。你可以在下面找到 requirements.txt 文件的内容。
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 4\. Activate your conda environment `conda activate falcon`
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 激活你的 conda 环境 `conda activate falcon`
- en: Running Falcon with Hugging Face Pipelines
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Hugging Face Pipelines 运行 Falcon
- en: '[Hugging Face](https://www.intel.com/content/www/us/en/developer/partner/hugging-face.html)
    pipelines provide a simple and high-level interface for applying pre-trained models
    to various natural language processing (NLP) tasks, such as text classification,
    named entity recognition, text generation, and more. These pipelines abstract
    away the complexities of model loading, tokenization, and inference, allowing
    users to quickly utilize state-of-the-art models for NLP tasks with just a few
    lines of code.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[Hugging Face](https://www.intel.com/content/www/us/en/developer/partner/hugging-face.html)
    管道提供了一个简单而高级的接口，用于将预训练模型应用于各种自然语言处理（NLP）任务，如文本分类、命名实体识别、文本生成等。这些管道抽象了模型加载、分词和推理的复杂性，使用户能够仅用几行代码快速利用最先进的NLP模型。'
- en: Below is a convenient script you can run in the cmd/terminal to experiment with
    the raw pre-trained Falcon models.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个方便的脚本，你可以在 cmd/terminal 中运行它来试验原始的预训练 Falcon 模型。
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To run the script (falcon-demo.py) you must provide the script and various
    parameters:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行脚本（falcon-demo.py），你必须提供脚本和各种参数：
- en: '`python falcon-demo.py --falcon_version "7b" --max_length 25 --top_k 5`'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`python falcon-demo.py --falcon_version "7b" --max_length 25 --top_k 5`'
- en: 'The script has 3 optional parameters to help control the execution of the Hugging
    Face pipeline:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本有 3 个可选参数，帮助控制 Hugging Face 管道的执行：
- en: '**falcon_version:** allows you to select from Falcon’s 7 billion or 40 billion
    parameter versions.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**falcon_version:** 允许你从 Falcon 的 70 亿或 400 亿参数版本中进行选择。'
- en: '**max_length:** used to control the maximum length of the generated text in
    text generation tasks.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**max_length:** 用于控制文本生成任务中生成文本的最大长度。'
- en: '**top_k:** specifies the number of highest probability tokens to consider at
    each step.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**top_k:** 指定在每一步中考虑的最高概率标记数量。'
- en: You can hack the script to add/remove/edit the parameters. What is important
    is that you now have access to one of the most powerful open-source models ever
    released!
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以修改脚本以添加/删除/编辑参数。重要的是，你现在可以访问到有史以来最强大的开源模型之一！
- en: Playing with Raw Falcon
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 玩转原始 Falcon
- en: Raw Falcon is not tuned for any particular purpose, so it will likely spew nonsense
    (Figure 2). Still, this doesn’t stop us from asking a few questions to test it
    out. When the script is done downloading the model and creating the pipeline,
    you will be prompted to provide input to the model. When you’re ready to stop,
    type “stop”.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 原始 Falcon 并未针对任何特定目的进行调整，因此可能会输出无意义的内容（图 2）。不过，这并不妨碍我们提出一些问题来进行测试。当脚本完成模型下载和创建管道后，你将被提示向模型提供输入。当你准备停止时，键入“stop”。
- en: '![](../Images/8825f4b73d5939d9eca8153326a8ffd3.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8825f4b73d5939d9eca8153326a8ffd3.png)'
- en: Figure 2\. Command line interface inference test of 7 Billion Parameter Falcon
    Model on Intel 4th Gen Xeon with default script parameters — Image by Author
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 在 Intel 第四代 Xeon 上使用默认脚本参数对 70 亿参数 Falcon 模型进行命令行接口推理测试 — 图片由作者提供
- en: The script prints the inference time to give you an idea of how long the model
    takes to respond based on the current parameters provided to the pipeline and
    the compute you have made available to this workload.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本会打印推理时间，让你了解模型在当前参数设置和你为该工作负载提供的计算资源下响应的时间。
- en: '*Tip: You can significantly alter the inference time by adjusting the max_length
    parameter.*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*提示：通过调整 max_length 参数，你可以显著改变推理时间。*'
- en: This tutorial is designed to share how to get Falcon running on a CPU with Hugging
    Face Transformers but does not explore options for further optimizations on Intel
    CPUs. Libraries like the [Intel Extension for Transformers](https://github.com/intel/intel-extension-for-transformers)
    offer capabilities to accelerate Transformer-based models through techniques like
    quantization, distillation, and pruning. Quantization is a widely-used model compression
    technique that can reduce the model size and improve inference latency — this
    would be a valuable next step to explore enhancing the performance of this workflow.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程旨在分享如何在 CPU 上通过 Hugging Face Transformers 运行 Falcon，但不探索 Intel CPUs 上进一步优化的选项。像
    [Intel Extension for Transformers](https://github.com/intel/intel-extension-for-transformers)
    这样的库提供了通过量化、蒸馏和剪枝等技术加速基于 Transformer 的模型的能力。量化是一种广泛使用的模型压缩技术，可以减少模型大小并提高推理延迟 —
    这将是探索提升此工作流性能的宝贵下一步。
- en: Summary and Discussion
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结与讨论
- en: Foundational LLMs create opportunities for developers to build exciting AI applications.
    However, half the battle is usually finding a model with the correct license that
    allows for commercial derivatives. Falcon presents a rare opportunity because
    it intersects performance and license flexibility.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 基础的 LLMs 为开发者创造了构建令人兴奋的 AI 应用的机会。然而，通常一半的挑战是找到一个允许商业衍生的正确许可的模型。Falcon 提供了一个难得的机会，因为它兼具性能和许可证灵活性。
- en: Although Falcon is fairly democratized from an open-source perspective, its
    size creates new challenges for engineers/enthusiasts. This tutorial helped address
    this by combining Falcon’s “truly open” license, Hugging Face Pipelines, and the
    availability/accessibility of CPUs to give developers more access to this powerful
    model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Falcon 从开源的角度来看相当民主化，但其规模给工程师/爱好者带来了新的挑战。本教程通过结合 Falcon 的“真正开放”许可证、Hugging
    Face Pipelines 和 CPU 的可用性/可及性，帮助解决了这些问题，使开发者可以更多地访问这个强大的模型。
- en: '**A few exciting things to try would be:**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**尝试一些令人兴奋的事情包括：**'
- en: Fine-tune Falcon to a specific task by leveraging the [Intel Extension for PyTorch](https://www.intel.com/content/www/us/en/developer/tools/oneapi/optimization-for-pytorch.html#gs.0ugs0o)
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过利用 [Intel Extension for PyTorch](https://www.intel.com/content/www/us/en/developer/tools/oneapi/optimization-for-pytorch.html#gs.0ugs0o)
    将 Falcon 微调到特定任务。
- en: Use model compression tools available in [Intel Neural Compressor (INC)](https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html#gs.zm5zqx)
    and [Intel Extension for Transformers](https://github.com/intel/intel-extension-for-transformers)
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 [Intel Neural Compressor (INC)](https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html#gs.zm5zqx)
    和 [Intel Extension for Transformers](https://github.com/intel/intel-extension-for-transformers)
    中提供的模型压缩工具。
- en: Play with the parameters of Hugging Face pipelines to optimize performance for
    your particular use case.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整 Hugging Face 管道的参数，以优化性能以适应你的特定用例。
- en: '***Don’t forget to follow*** [***my profile for more articles***](https://eduand-alvarez.medium.com/)
    ***like this!***'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '***别忘了关注*** [***我的个人资料以获取更多类似的文章***](https://eduand-alvarez.medium.com/) ***！***'
