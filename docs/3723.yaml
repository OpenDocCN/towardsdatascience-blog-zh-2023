- en: 'Machine Learning is Not All You Need: A Case Study on Signature Detection'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/machine-learning-is-not-all-you-need-a-case-study-on-signature-detection-9551f2e5d0e7?source=collection_archive---------7-----------------------#2023-12-21](https://towardsdatascience.com/machine-learning-is-not-all-you-need-a-case-study-on-signature-detection-9551f2e5d0e7?source=collection_archive---------7-----------------------#2023-12-21)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Machine learning should not be your go-to solution for every task. Consider
    the KISS principle like I did for signature detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://toon-beerten.medium.com/?source=post_page-----9551f2e5d0e7--------------------------------)[![Toon
    Beerten](../Images/f169eaa8cefa00f17176955596972d57.png)](https://toon-beerten.medium.com/?source=post_page-----9551f2e5d0e7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9551f2e5d0e7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9551f2e5d0e7--------------------------------)
    [Toon Beerten](https://toon-beerten.medium.com/?source=post_page-----9551f2e5d0e7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3aef462e13b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-is-not-all-you-need-a-case-study-on-signature-detection-9551f2e5d0e7&user=Toon+Beerten&userId=3aef462e13b5&source=post_page-3aef462e13b5----9551f2e5d0e7---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9551f2e5d0e7--------------------------------)
    ·6 min read·Dec 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9551f2e5d0e7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-is-not-all-you-need-a-case-study-on-signature-detection-9551f2e5d0e7&user=Toon+Beerten&userId=3aef462e13b5&source=-----9551f2e5d0e7---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9551f2e5d0e7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-is-not-all-you-need-a-case-study-on-signature-detection-9551f2e5d0e7&source=-----9551f2e5d0e7---------------------bookmark_footer-----------)![](../Images/51960a70a543b67e7494729412514b04.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I present a case study demonstrating that machine learning
    should not be your go-to solution for every task. Simpler techniques could give
    good results as well and are easier to implement.
  prefs: []
  type: TYPE_NORMAL
- en: '**Case Study: Signature Detection**'
  prefs: []
  type: TYPE_NORMAL
- en: Imagine we have a pile of contracts and we need to know whether they are signed
    or not. This scenario involves signature detection — reliably identifying whether
    a signature appears in a specific location or not — assuming you already know
    the rough location where a signature should be (e.g. south-east). In ancient times
    this task was done by binarizing the image and counting the black pixels in an
    area. If a signature is present, the black pixel count would surpass a threshold.
    But in 2023, how could we do this differently?
  prefs: []
  type: TYPE_NORMAL
- en: '**The Machine Learning Approach**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We would use [GroundingDino,](https://github.com/IDEA-Research/GroundingDINO)
    which is a state-of-the-art zero-shot object detection model. The input to the
    model is an image combined with a prompt, while the output consists of rectangles
    indicating potential locations with associated confidence scores. While this may
    seem like an ideal solution at first glance, there are certain limitations worth
    considering. Let’s try it out with three different prompts: ‘signature’, ‘handwriting’
    and ‘scribble’.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/517494875622219dee3da066b698cf19.png)![](../Images/8eec019f5630393c4e58d164c1ac0286.png)![](../Images/d85ca8777f6198937fe373f7dc64eac9.png)'
  prefs: []
  type: TYPE_IMG
- en: Prompt results with ‘signature’, ‘handwriting’ and ‘scribble’ respectively.
    Images by author.
  prefs: []
  type: TYPE_NORMAL
- en: You can see that the results heavily depend on the prompt, not to mention that
    it takes about 30 seconds on CPU before the results show up. That is because this
    is a foundational model, trained extensively on thousands of other categories
    beyond signatures alone. What can we do to get it to be more accurate and fast?
    We could use Autodistill ([tutorial](https://roboflow.com/train/grounding-dino-and-yolov8))
    which uses Grounding DINO to train a YOLOv8 model. Effectively using a foundation
    model to train a lighter, supervised model. The workflow would be to collect a
    sizeable dataset of signed documents, then find a good prompt to get labelled
    data and ultimately train a YOLOv8 model on it.
  prefs: []
  type: TYPE_NORMAL
- en: You can imagine this takes some serious time and effort. But is there another
    way?
  prefs: []
  type: TYPE_NORMAL
- en: '**Alternative Approach: OpenCV**'
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV is an open-source computer vision library that provides a wide range
    of functionalities for real-time image and video processing, analysis, and understanding,
    using optimized algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: The `connectedComponentsWithStats` function in OpenCV is used to label and analyze
    image regions (connected components) based on their pixel connectivity, and additionally
    calculates various statistics such as area and bounding box dimensions for each
    labeled region.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make it more comprehensible, i created this image. It is a cutout of the
    area with the signature. Each island of connected pixels has a color which represents
    a single connected component (or: label).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4e37330f93bc3588b263cb4c15997962.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Knowing the above, let’s dive into the intuition behind this computer vision
    approach. The key idea here is: can we identify which label(s) make up the signature?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Running this function on a normal, average document would produce hundreds
    if not thousands of unique labels for:'
  prefs: []
  type: TYPE_NORMAL
- en: each letter (because it is not connected to other letters)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: bigger things like signatures and logos
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: smaller things like tiny specks of noise and dots
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To filter out irrelevant labels, we can take the median area of all labels,
    which would be the size of a single character (supposing the image contains more
    letters than noise), as the minimum threshold. Any region below this threshold
    can be filtered out. We can also set a maximum threshold by assuming that a signature
    will not take up more than x times the area of a letter. What we are left with
    are actual candidates for our signature. But what about logos? They may have the
    same size as signatures, however a signature typically has lots of whitespace
    in between. With a black pixel ratio filter I can filter those out. The labels
    that are left should be actual signatures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Turning the above into code results in this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: I only spent a fraction of the time I would have needed to implement the machine
    learning approach. Besides the time save, it works remarkably well. It handles
    resolutions from both high and low DPI scans. Other advantages of this approach
    is that it is easily integrated into existing C++ or Python code and it is blazingly
    fast. Surely the parameters can be tweaked further, for this I invite you to open
    my [shared colab notebook](https://github.com/Toon-nooT/notebooks/blob/main/Signature_Detection_OpenCV.ipynb)
    and tinker yourself. If you rather try it out online, have a go at my [demo on
    huggingface](https://huggingface.co/spaces/to-be/signature_detection_opencv).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dde0e5a5805b630f6aee3ab883f7aa2f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion**'
  prefs: []
  type: TYPE_NORMAL
- en: When confronted with a technical challenge, don’t go immediately full ‘machine
    learning mode’, be open for other, simpler techniques. While machine learning
    is exciting and opens a lot of new possibilities, it is not always necessary for
    every task. It is important to consider factors like development time, ease of
    deployment, accuracy trade-offs, and processing speed when choosing an appropriate
    approach for your challenge.
  prefs: []
  type: TYPE_NORMAL
