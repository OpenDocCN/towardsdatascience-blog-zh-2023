- en: Is It Compression That You Need?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你需要的是压缩吗？
- en: 原文：[https://towardsdatascience.com/is-it-compression-that-you-need-7aa544a328b1?source=collection_archive---------4-----------------------#2023-07-22](https://towardsdatascience.com/is-it-compression-that-you-need-7aa544a328b1?source=collection_archive---------4-----------------------#2023-07-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/is-it-compression-that-you-need-7aa544a328b1?source=collection_archive---------4-----------------------#2023-07-22](https://towardsdatascience.com/is-it-compression-that-you-need-7aa544a328b1?source=collection_archive---------4-----------------------#2023-07-22)
- en: A more efficient implementation of compression-based topic classification
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更高效的基于压缩的主题分类实现
- en: '[](https://medium.com/@mtths.mndr?source=post_page-----7aa544a328b1--------------------------------)[![Matthias
    Minder](../Images/f4d6c603bc53deeac3a515db4bdfbeee.png)](https://medium.com/@mtths.mndr?source=post_page-----7aa544a328b1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7aa544a328b1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7aa544a328b1--------------------------------)
    [Matthias Minder](https://medium.com/@mtths.mndr?source=post_page-----7aa544a328b1--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mtths.mndr?source=post_page-----7aa544a328b1--------------------------------)[![Matthias
    Minder](../Images/f4d6c603bc53deeac3a515db4bdfbeee.png)](https://medium.com/@mtths.mndr?source=post_page-----7aa544a328b1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7aa544a328b1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7aa544a328b1--------------------------------)
    [Matthias Minder](https://medium.com/@mtths.mndr?source=post_page-----7aa544a328b1--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6a3d24668b34&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-it-compression-that-you-need-7aa544a328b1&user=Matthias+Minder&userId=6a3d24668b34&source=post_page-6a3d24668b34----7aa544a328b1---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7aa544a328b1--------------------------------)
    ·6 min read·Jul 22, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7aa544a328b1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-it-compression-that-you-need-7aa544a328b1&user=Matthias+Minder&userId=6a3d24668b34&source=-----7aa544a328b1---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6a3d24668b34&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-it-compression-that-you-need-7aa544a328b1&user=Matthias+Minder&userId=6a3d24668b34&source=post_page-6a3d24668b34----7aa544a328b1---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7aa544a328b1--------------------------------)
    ·6 min read·2023年7月22日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7aa544a328b1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-it-compression-that-you-need-7aa544a328b1&user=Matthias+Minder&userId=6a3d24668b34&source=-----7aa544a328b1---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7aa544a328b1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-it-compression-that-you-need-7aa544a328b1&source=-----7aa544a328b1---------------------bookmark_footer-----------)![](../Images/75dfff5b5a21605b44529d810116dce6.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7aa544a328b1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-it-compression-that-you-need-7aa544a328b1&source=-----7aa544a328b1---------------------bookmark_footer-----------)![](../Images/75dfff5b5a21605b44529d810116dce6.png)'
- en: Photo by [Tomas Sobek](https://unsplash.com/es/@tomas_nz?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Tomas Sobek](https://unsplash.com/es/@tomas_nz?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'The recently published paper with the title *“Low-Resource” Text Classification:
    A Parameter-Free Classification Method with Compressors* [1] has received quite
    some public attention in the recent days. Their key finding is that they can in
    some cases outperform large language models like BERT using the simple idea that
    two text documents are more similar if they can be compressed to a smaller file
    size (although there is some controversy about the validity of their results,
    see [this blog post](https://kenschutte.com/gzip-knn-paper/) and [this discussion
    including the author’s reply](https://github.com/bazingagin/npc_gzip/issues/3#issuecomment-1641679866)).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最近发布的论文标题为*“低资源”文本分类：一种无参数分类方法与压缩器* [1]，最近引起了相当多的公众关注。他们的关键发现是，在某些情况下，他们可以通过简单的想法来超越大型语言模型如BERT，即如果两个文本文件可以被压缩到更小的文件大小，它们就更相似（尽管关于他们结果的有效性存在一些争议，见[这篇博客文章](https://kenschutte.com/gzip-knn-paper/)和[这次讨论，包括作者的回复](https://github.com/bazingagin/npc_gzip/issues/3#issuecomment-1641679866)）。
- en: The main idea behind their approach is that the “information distance” between
    two documents, as defined by Bennet et. al [2], is a good distance metric to use
    during text classification. Since the information distance itself isn’t computable,
    they approximate it using the Normalized Compression Distance (NCD) [3], which
    estimates the information distance using “real-life” data compressors like gzip.
    NCD has the property that better compressors (i.e. compressors with a better compression
    ratio) will allow for a better estimation of the true information distance.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 他们方法的主要思想是，Bennet等人定义的“信息距离”在文本分类中是一个好的距离度量。由于信息距离本身不可计算，他们使用归一化压缩距离（NCD）[3]来近似，它通过像gzip这样的“现实生活”数据压缩器来估计信息距离。NCD具有更好的压缩器（即压缩比更好的压缩器）可以更好地估计真实信息距离的特性。
- en: 'It seems natural then to expect that better compressor will achieve superior
    performance in classification. But they cannot validate this experimentally; the
    best compressor considered in the paper, bz2, performs worse in terms of accuracy
    than gzip. They explain this as follows: “[…] the Burrows-Wheeler algorithm used
    by bz2 dismisses the information of character order by permuting characters during
    compression” [1, p.6817]. This implies that compression alone doesn’t explain
    their findings, but that it has something to do with character order as well.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，自然期望更好的压缩器会在分类中实现更好的性能。但他们无法通过实验验证这一点；论文中考虑的最佳压缩器bz2在准确性方面表现不如gzip。他们解释道：“[…]
    bz2使用的Burrows-Wheeler算法通过在压缩过程中排列字符，忽略了字符顺序的信息” [1, p.6817]。这意味着仅凭压缩无法解释他们的发现，但与字符顺序也有关系。
- en: 'This made me wonder: How much of their results are due to compression, and
    how much is due to the comparison of strings between two documents?'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这让我思考：他们的结果有多少是由于压缩，多少是由于两个文档之间的字符串比较？
- en: 'To investigate this question, I compare their results with two alternatives:
    (1) a simple compressor that relies solely on replacing recurring strings, and
    (2) an algorithm that explicitly does substring matching between a query document
    and all documents belonging to some topic.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调查这个问题，我将他们的结果与两种替代方案进行比较：（1）一个仅依赖于替换重复字符串的简单压缩器，以及（2）一个在查询文档和属于某些主题的所有文档之间显式进行子字符串匹配的算法。
- en: '**A first ablation: Will LZ4 do the job?** The compression algorithm gzip is
    based on [DEFLATE](https://en.wikipedia.org/wiki/Deflate), which in term uses
    [LZ77](https://en.wikipedia.org/wiki/LZ77_and_LZ78) and [Huffman coding](https://en.wikipedia.org/wiki/Huffman_coding)
    to compress the data. Let’s look at these two algorithms in more detail and think
    about what they imply when applied to our use-case.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**第一次消融：LZ4能否完成任务？** 压缩算法gzip基于[DEFLATE](https://en.wikipedia.org/wiki/Deflate)，它使用[LZ77](https://en.wikipedia.org/wiki/LZ77_and_LZ78)和[霍夫曼编码](https://en.wikipedia.org/wiki/Huffman_coding)来压缩数据。让我们更详细地了解这两种算法，并思考它们在我们使用场景中的含义。'
- en: During compression, LZ77 uses a sliding window over previously seen data. If
    a string of characters is repeated, a reference to the first occurrence of the
    character string is stored, instead of the string itself. Hence, if we take the
    length of two concatenated documents as a distance metric, documents will be closer
    together if they have more overlapping substrings within the size of the sliding
    window (typically 32KB).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在压缩过程中，LZ77使用一个滑动窗口来处理先前看到的数据。如果字符串重复，则存储字符串第一次出现的引用，而不是字符串本身。因此，如果我们将两个连接的文档的长度作为距离度量，文档会更接近，如果它们在滑动窗口大小（通常为32KB）内有更多重叠的子串。
- en: 'The Huffman coding compresses the resulting text even further: Instead of using
    8 bits for every character, it represents those letters that occur frequently
    with less bits and those letters that occur rarely with more bits. If we apply
    the Huffman coding to the concatenated documents, two documents would be smaller
    after compression if they use the characters with similar frequency.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 哈夫曼编码进一步压缩结果文本：它不是对每个字符使用8位，而是用更少的位表示频繁出现的字母，用更多的位表示不常出现的字母。如果我们将哈夫曼编码应用于连接的文档，那么如果两个文档使用频率相似的字符，压缩后的文档将会更小。
- en: One would expect matching substrings to be much more important to topic classification
    than similar character frequencies. I therefore do an ablation study by looking
    at the performance when using the [LZ4](https://en.wikipedia.org/wiki/LZ4_(compression_algorithm))
    algorithm [4] for compression (basically LZ77 but with a [fast implementation
    available in python](https://pypi.org/project/lz4/)). Since LZ4 has a much lower
    compression ratio than gzip, their explanation would suggest that performance
    of LZ4 is worse than gzip. If however the main thing going on is substring matching,
    LZ4 will perform just as well as gzip.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 可以预期，匹配的子串在主题分类中比相似的字符频率更重要。因此，我通过观察使用[LZ4](https://en.wikipedia.org/wiki/LZ4_(compression_algorithm))算法[4]进行压缩时的性能，进行了一项消融研究（基本上是LZ77，但有一个[在python中可用的快速实现](https://pypi.org/project/lz4/)）。由于LZ4的压缩比远低于gzip，他们的解释表明LZ4的性能不如gzip。然而，如果主要的工作是子串匹配，LZ4将表现得与gzip一样好。
- en: '**A more explicit algorithm** To go even further, I implement a simple algorithm
    that explicitly does the substring matching: It assigns documents to the topic
    with the most similar substrings (substrings being character-level n-grams here).
    It works as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**更明确的算法** 为了进一步深入，我实现了一个简单的算法，明确地进行子串匹配：它将文档分配给具有最相似子串的主题（这里的子串是字符级n-gram）。其工作原理如下：'
- en: 'Text encoding:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 文本编码：
- en: 1\. Extract all character n-grams within the text with 5 ≤ n ≤ 50.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 提取文本中的所有字符n-gram，范围为5 ≤ n ≤ 50。
- en: 2\. For the extracted n-grams, calculate an 8-digit hash code using `hash(n_gram)
    % int(10e8)` in python (since I want to control how many different things to keep
    track of).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 对提取的n-gram，使用`hash(n_gram) % int(10e8)`在python中计算8位哈希代码（因为我想控制要跟踪的不同事物的数量）。
- en: 3\. Keep track of this in sets (thus losing the information about how many times
    a given code occurs).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 将其记录在集合中（因此丢失了有关某个代码出现次数的信息）。
- en: 'Training:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 训练：
- en: 1\. Calculate the set of hash codes for every text of a given topic.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 计算给定主题的每个文本的哈希代码集合。
- en: 2\. Take the set union to obtain a set of hash codes that occur in the topic.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 进行集合并，以获得在主题中出现的哈希代码集合。
- en: 'Inferencing:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 推断：
- en: 1\. For some query text, calculate the set of its hashed n-grams.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 对某些查询文本，计算其哈希n-gram的集合。
- en: 2\. For every topic encountered during training, calculate the size of the intersection
    between the topic sets and the query set.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 对于训练中遇到的每个主题，计算该主题集与查询集之间的交集大小。
- en: 3\. Assign the query text to the topic with the largest intersection.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 将查询文本分配给交集最大的主题。
- en: '**Experiment and Results** I compare the results of gzip, lz4 and the hashed
    n-gram algorithm in the 100-shot setting with 5 runs, as described in their paper.
    For all three methods I stick to their experimental setup in order to reproduce
    their reported results (again, leaving us with potentially inflated accuracy measures).
    The code can be found on [github](https://github.com/mattminder/npc_gzip).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**实验和结果** 我在100-shot设置中对gzip、lz4和哈希n-gram算法进行了比较，共进行了5次实验，具体如他们的论文所述。对于这三种方法，我坚持他们的实验设置，以便重现他们报告的结果（再次说明，这可能导致准确性度量的夸大）。代码可以在[github](https://github.com/mattminder/npc_gzip)上找到。'
- en: 'You can see the performance on three data sets from [torchtext](https://pytorch.org/text/stable/datasets.html)
    ([AG_NEWS](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html) [5],
    [DBpedia](http://wikidata.dbpedia.org/develop/datasets) [6] and [YahooAnswers](https://github.com/LC-John/Yahoo-Answers-Topic-Classification-Dataset/tree/master/dataset)
    [5]) in the following table:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从[torchtext](https://pytorch.org/text/stable/datasets.html) ([AG_NEWS](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html)
    [5]，[DBpedia](http://wikidata.dbpedia.org/develop/datasets) [6] 和 [YahooAnswers](https://github.com/LC-John/Yahoo-Answers-Topic-Classification-Dataset/tree/master/dataset)
    [5])中的三个数据集上查看性能，见下表：
- en: '![](../Images/a39a190e7ead963fef2e66f8bd2aa8b7.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a39a190e7ead963fef2e66f8bd2aa8b7.png)'
- en: We see that lz4 and the hashed n-grams outperform gzip on all three considered
    data sets, with the hashed n-gram algorithm being best in 2 out of 3 data. But
    it still doesn’t compete with BERT, which has a performance of 0.82 on AG_NEWS
    and close to 1 on DBpedia according to their paper in the 100-shot setting.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到lz4和哈希n-gram在所有三个考虑的数据集中都优于gzip，其中哈希n-gram算法在3个数据集中的2个数据集中表现最好。但它仍然无法与BERT竞争，根据他们在100-shot设置中的论文，BERT在AG_NEWS上的性能为0.82，在DBpedia上接近1。
- en: 'These results have important practical implications: In our experiment, the
    lz4-based algorithm runs approximately 10x faster than the gzip-based algorithm.
    And even more importantly, the hashed n-gram algorithm even improves the computational
    complexity at inference time: Instead of having to compare a query document with
    every other document in the text corpus, you just have to do a comparison with
    every topic set.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果具有重要的实际意义：在我们的实验中，基于lz4的算法运行速度比基于gzip的算法快大约10倍。更重要的是，哈希n-gram算法甚至在推理时改善了计算复杂度：你只需与每个主题集进行比较，而不是与文本语料库中的每个文档进行比较。
- en: '**What do we learn from this?** My results suggest that the driving force between
    the impressive reported performance of gzip can be attributed to the fact that
    their method implicitly compares character-level n-grams between documents. This
    finding allows for the use of faster compressors like lz4 without any performance
    loss. Furthermore, one can even rewrite their algorithm to have constant complexity
    with respect to dataset size at inference time, bringing their method a big step
    closer to practical use on big data sets. If you do want to use it in practice,
    I have started an implementation following the scikit-learn API of my proposed
    algorithm [here](http://github.com/mattminder/long-tail-text-classifier).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们从中学到了什么？** 我的结果表明，gzip的显著性能可以归因于他们的方法隐式比较了字符级n-gram。这一发现使得可以使用像lz4这样的更快的压缩器而不会有任何性能损失。此外，甚至可以重写他们的算法，使其在推理时具有与数据集大小无关的恒定复杂度，使他们的方法更接近于在大数据集上的实际应用。如果你想在实践中使用它，我已经开始按照我提出的算法的scikit-learn
    API进行实现，[请见此处](http://github.com/mattminder/long-tail-text-classifier)。'
- en: The one question that remains is, why does this outperform the [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)
    approach that also compares words between documents?
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的一个问题是，为什么这种方法比[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)方法表现更好，即使两者都在比较文档中的词汇？
- en: 'Maybe considering character-level n-grams does a better job for some tasks
    than splitting the text into individual words. But probably more importantly,
    the method here gives equal weight to all n-grams, regardless of their occurrence
    count. This means that it gives a lot of importance to so-called long-tail (read:
    rare) information, which apparently is relevant for some text tasks such as topic
    detection. Note that transformer networks don’t do too good a job on modelling
    such long-tail information (for evidence, see e.g. [5]), which is why these very
    simple approaches are a very interesting benchmark to measure your million-parameter
    model against.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 也许考虑字符级n-gram在某些任务中比将文本拆分成单个词汇更有效。但更重要的是，这里使用的方法对所有n-gram赋予相等的权重，无论它们的出现次数。这意味着它给予所谓的长尾（即稀有）信息很多重要性，这显然对某些文本任务如主题检测很重要。注意，变换器网络在建模这种长尾信息上表现不佳（有关证据，请参见例如[5]），这也是这些非常简单的方法成为衡量你百万参数模型的一个非常有趣的基准的原因。
- en: '**References** [1] Z. Jiang, M. Yang, M. Tsirlin, R. Tang, Y. Dai, J. Lin.
    “Low-Resource” Text Classification: A Parameter-Free Classification Method with
    Compressors (2023), ACL 2023'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献** [1] Z. Jiang, M. Yang, M. Tsirlin, R. Tang, Y. Dai, J. Lin. “低资源”文本分类：一种无参数分类方法与压缩器
    (2023), ACL 2023'
- en: '[2] C. H. Bennett, P. Gács, M. Li, P. MB Vitányi, and W. H. Zurek, Information
    distance (1998), IEEE Transactions on information theory'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] C. H. Bennett, P. Gács, M. Li, P. MB Vitányi 和 W. H. Zurek, 信息距离 (1998),
    IEEE 信息理论交易'
- en: '[3] M. Li, X. Chen, X. Li, B. Ma, and P. Vitányi, The similarity metric (2004),
    IEEE transactions on Information Theory'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] M. Li, X. Chen, X. Li, B. Ma 和 P. Vitányi, 相似性度量 (2004), IEEE 信息理论交易'
- en: '[4] N. Kandpal, H. Deng, A. Roberts, E. Wallace, C. Raffel, Large Language
    Models Struggle to Learn Long-Tail Knowledge (2022), arxiv.org/abs/2211.08411'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] N. Kandpal, H. Deng, A. Roberts, E. Wallace, C. Raffel, 大型语言模型在学习长尾知识方面的困难
    (2022), arxiv.org/abs/2211.08411'
- en: Thanks to Joel Niklaus and Marcel Gygli for our discussions and your feedback.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢 Joel Niklaus 和 Marcel Gygli 的讨论和反馈。
