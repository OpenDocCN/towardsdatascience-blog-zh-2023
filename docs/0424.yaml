- en: 'Reinforcement Learning for Inventory Optimization Series III: Sim-to-Real Transfer
    for the RL Model'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/reinforcement-learning-for-inventory-optimization-series-iii-sim-to-real-transfer-for-the-rl-model-d260c3b8277d?source=collection_archive---------10-----------------------#2023-01-30](https://towardsdatascience.com/reinforcement-learning-for-inventory-optimization-series-iii-sim-to-real-transfer-for-the-rl-model-d260c3b8277d?source=collection_archive---------10-----------------------#2023-01-30)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Bridge the gap between the simulator and real world
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@guanx92?source=post_page-----d260c3b8277d--------------------------------)[![Guangrui
    Xie](../Images/def9aa637424a88d75a6a3bb103350bc.png)](https://medium.com/@guanx92?source=post_page-----d260c3b8277d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d260c3b8277d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d260c3b8277d--------------------------------)
    [Guangrui Xie](https://medium.com/@guanx92?source=post_page-----d260c3b8277d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F495b92f0c66d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-for-inventory-optimization-series-iii-sim-to-real-transfer-for-the-rl-model-d260c3b8277d&user=Guangrui+Xie&userId=495b92f0c66d&source=post_page-495b92f0c66d----d260c3b8277d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d260c3b8277d--------------------------------)
    ·8 min read·Jan 30, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd260c3b8277d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-for-inventory-optimization-series-iii-sim-to-real-transfer-for-the-rl-model-d260c3b8277d&user=Guangrui+Xie&userId=495b92f0c66d&source=-----d260c3b8277d---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd260c3b8277d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-for-inventory-optimization-series-iii-sim-to-real-transfer-for-the-rl-model-d260c3b8277d&source=-----d260c3b8277d---------------------bookmark_footer-----------)![](../Images/57bb3512be81d81faddf8f3fd7b5317c.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Suad Kamardeen](https://unsplash.com/@suadkamardeen?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'Update: this article is the third article in my blog series *Reinforcement
    Learning for Inventory Optimization*.Below are the links to the other articles
    in the same series. Please check them out if you are interested.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Reinforcement Learning for Inventory Optimization Series I: An RL Model for
    Single Retailers*](https://medium.com/towards-data-science/a-reinforcement-learning-based-inventory-control-policy-for-retailers-ac35bc592278)'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Reinforcement Learning for Inventory Optimization Series II: An RL Model
    for A Multi-Echelon Network*](https://medium.com/towards-data-science/reinforcement-learning-for-inventory-optimization-series-ii-an-rl-model-for-a-multi-echelon-921857acdb00)'
  prefs: []
  type: TYPE_NORMAL
- en: In previous articles, I built two RL models for a single retailer and a multi-echelon
    supply chain network. In these two articles, I trained the RL model using a simulator
    of the inventory system based on a historical demand dataset following a mixture
    of different normal distributions whose parameters depend on days of week. Then
    I tested the RL models on a test set in which the demand data follow the same
    distribution as the historical training data. This is essentially assuming the
    simulator perfectly matches the real world where we would apply the RL models
    to, and historical demand data perfectly represent the future demand pattern.
    However, this assumption is rarely true in real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: How to deal with the gap between the distributions in the training set and test
    set is a well-known problem in the general context of machine learning. The essence
    of this problem is the bias vs variance trade-off. If we rely too much on the
    training set to train our model, then the model is prone to overfitting, meaning
    it will only well on the training set but not on the test set. The same issue
    applies here in the RL field.
  prefs: []
  type: TYPE_NORMAL
- en: Performance Deterioration due to the Gap between Simulator and Real World
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Taking the RL models I built previously as an example, the models were trained
    through a large number of episodes on the same historical demand data, hence it
    is highly likely that the inventory policies learned from the trained RL models
    are overfitted to the demand pattern represented by the historical demand data.
    The inventory policies will perform well if the same demand pattern continues
    in the future, but the performance will deteriorate if the future demand pattern
    deviates from the historical one.
  prefs: []
  type: TYPE_NORMAL
- en: As a numerical illustration, let’s assume that the future demand distribution
    in the real world deviates from the historical demand data in the simulator used
    for training the RL models. I took the RL model trained in [my first article](https://medium.com/towards-data-science/a-reinforcement-learning-based-inventory-control-policy-for-retailers-ac35bc592278)
    and applied it to two future demand scenarios in real world. In one of the scenarios,
    we assume the means of the mixture of the normal distributions will increase by
    1, while in the other scenario, the means will decrease by 1\. The demand distribution
    structure is given by the table below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ae64463d93bb6ab33b286a83fb4967d3.png)'
  prefs: []
  type: TYPE_IMG
- en: Two future/real-world demand distribution scenarios (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: For both of the demand increase scenario and demand decrease scenario (second
    and third columns in the table above), I generated 100 demand datasets which consist
    of 52 weeks of data using their own demand distribution setting. Then I applied
    the DQN policy trained on the historical/simulator demand data (first column in
    the table above) to both scenarios, the corresponding average profits obtained
    are given in the table below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/875d73b680def5abba7790abdb6faf55.png)'
  prefs: []
  type: TYPE_IMG
- en: Average profits obtained by applying the DQN policy trained on historical/simulator
    data to each demand scenario (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: To assess how well the trained DQN policy performs, for each of the demand scenarios,
    let’s assume we know what the future/real-world demand distribution will deviate
    ahead of time, and generate a training set consisting of 52 weeks of demand data
    using its own future/real-world demand distribution setting. Then we train a brand
    new RL model using its own training set for each scenario and apply the new RL
    model to each scenario. The corresponding average profits obtained are given in
    the table below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/46b9f9db2d6275890dd36142ebd0b90a.png)'
  prefs: []
  type: TYPE_IMG
- en: Average profits obtained by applying the DQN policy trained on each scenario’s
    training set to each demand scenario (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: As we can see from the comparison in the two tables above ($25077.61 vs $27399.79
    and $13890.99 vs $14707.44), there is a performance deterioration in the RL model
    if there is a gap between the simulator and real world.
  prefs: []
  type: TYPE_NORMAL
- en: Bridging the Gap Using Domain Randomization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To bridge the gap between the simulator and real world, in real applications,
    one may choose to select a shorter period of most recent historical demand data,
    and assume this shorter period represents the future demand pattern or trend better.
    Then we frequently retrain the RL model using the recently updated shorter period
    of historical demand data. However, we can also try to address this problem during
    the training process in the first place using the concept of domain randomization.
  prefs: []
  type: TYPE_NORMAL
- en: Domain randomization is a technique typically used for sim-to-real transfer
    when applying RL to robotics. The applications of RL in robotics also face the
    issue of reality gap, since the gap between the simulation and real world degrades
    the performance of the policies once the RL models are transferred into real robots[1].
    In robotics context, the core idea of domain randomization is by randomizing the
    the physical parameters of the simulated environment (e.g., friction coefficients
    and vision properties such as objects’ appearance) when training the RL model,
    the RL model will experience situations more like the real environment, hence
    the learned policies will generalize better to the real environment. The figure
    below illustrates the intuition behind domain randomization.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7c7a3d1976c0b74a9a3e3e6a0e95d41b.png)'
  prefs: []
  type: TYPE_IMG
- en: Intuition behind domain randomization (Image from reference [1])
  prefs: []
  type: TYPE_NORMAL
- en: In the context of inventory optimization, to implement domain randomization,
    we can add randomization to the historical demand data. To be specific, we extend
    the historical demand dataset by adding normally distributed noise to the demand
    observations. The implementation of this idea is given in the code block below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here I sort of created 10 historical demand scenario by adding a random noise
    to the demand observation in each scenario. Then I appended the scenarios together
    as the entire historical demand data used for training, pretending that we have
    a 10 years’ of demand data. Note that the essence of this idea also aligns with
    the data augmentation technique in computer vision tasks where we manipulate the
    images to enrich the training set to avoid overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: I tried different values for the mean of the normal distribution used for generating
    the random noise. Initially I thought it makes sense to use a 0 mean, since it
    will more likely generate equal numbers of demand increase and decrease scenarios.
    Interestingly, after trying out different values, I found making the mean above
    0 (more demand increase scenarios) gave better test results for this particular
    example. It might be due to the fact that the demand data are truncated at 0,
    hence although we generate noise with mean 0, there is not too much room for the
    demand distribution to shift left. So generating noise with a mean of a positive
    number will let the RL model learn more useful knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Now we train the DQN model using the new 10 years’ historical demand data obtained
    by domain randomization and then apply the learned DQN policy to the demand increase
    and decrease scenarios (detailed setting in the first table) described in the
    previous section. The following table shows the average profits obtained in each
    scenario.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/360f435930f658b9dada5c27e199242f.png)'
  prefs: []
  type: TYPE_IMG
- en: Average profits obtained in each scenario using the DQN policy trained after
    domain randomization (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: We see that there is a clear improvement as compared to the results without
    domain randomization ($26432.61 vs $25077.61 in increase scenario, $14311.52 vs
    $13890.99 in decrease scenario).
  prefs: []
  type: TYPE_NORMAL
- en: To show the performance improvement is not obtained by chance, I further created
    more randomized test demand scenarios by adding a random noise to the means and
    variances of the mixture of demand distributions. See the code below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The DQN policy before domain randomization gives an average profit of $20163.84
    on this test set, and the DQN policy after domain randomization gives an average
    profit of $21887.77, which still shows a performance improvement.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, I focus on pointing out and addressing the gap between training/simulation
    and test/real world environments for the previous RL model I built for inventory
    optimization. It’s good to see that techniques such as domain randomization in
    the robotics field can also be effective in the inventory optimization area. Experimental
    results suggest that it should be a good practice to manipulate historical demand
    data using techniques like domain randomization to enrich the training set, so
    that the RL model trained in a lab can better generalize to the real world.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] [Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey](https://arxiv.org/abs/2009.13303)'
  prefs: []
  type: TYPE_NORMAL
