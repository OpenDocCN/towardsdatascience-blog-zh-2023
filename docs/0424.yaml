- en: 'Reinforcement Learning for Inventory Optimization Series III: Sim-to-Real Transfer
    for the RL Model'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/reinforcement-learning-for-inventory-optimization-series-iii-sim-to-real-transfer-for-the-rl-model-d260c3b8277d?source=collection_archive---------10-----------------------#2023-01-30](https://towardsdatascience.com/reinforcement-learning-for-inventory-optimization-series-iii-sim-to-real-transfer-for-the-rl-model-d260c3b8277d?source=collection_archive---------10-----------------------#2023-01-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Bridge the gap between the simulator and real world
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@guanx92?source=post_page-----d260c3b8277d--------------------------------)[![Guangrui
    Xie](../Images/def9aa637424a88d75a6a3bb103350bc.png)](https://medium.com/@guanx92?source=post_page-----d260c3b8277d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d260c3b8277d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d260c3b8277d--------------------------------)
    [Guangrui Xie](https://medium.com/@guanx92?source=post_page-----d260c3b8277d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F495b92f0c66d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-for-inventory-optimization-series-iii-sim-to-real-transfer-for-the-rl-model-d260c3b8277d&user=Guangrui+Xie&userId=495b92f0c66d&source=post_page-495b92f0c66d----d260c3b8277d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d260c3b8277d--------------------------------)
    ·8 min read·Jan 30, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd260c3b8277d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-for-inventory-optimization-series-iii-sim-to-real-transfer-for-the-rl-model-d260c3b8277d&user=Guangrui+Xie&userId=495b92f0c66d&source=-----d260c3b8277d---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd260c3b8277d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-for-inventory-optimization-series-iii-sim-to-real-transfer-for-the-rl-model-d260c3b8277d&source=-----d260c3b8277d---------------------bookmark_footer-----------)![](../Images/57bb3512be81d81faddf8f3fd7b5317c.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Suad Kamardeen](https://unsplash.com/@suadkamardeen?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 'Update: this article is the third article in my blog series *Reinforcement
    Learning for Inventory Optimization*.Below are the links to the other articles
    in the same series. Please check them out if you are interested.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 更新：这篇文章是我博客系列*库存优化中的强化学习*的第三篇文章。以下是同一系列其他文章的链接。如果您感兴趣，请查看。
- en: '[*Reinforcement Learning for Inventory Optimization Series I: An RL Model for
    Single Retailers*](https://medium.com/towards-data-science/a-reinforcement-learning-based-inventory-control-policy-for-retailers-ac35bc592278)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[*库存优化中的强化学习系列 I：单一零售商的 RL 模型*](https://medium.com/towards-data-science/a-reinforcement-learning-based-inventory-control-policy-for-retailers-ac35bc592278)'
- en: '[*Reinforcement Learning for Inventory Optimization Series II: An RL Model
    for A Multi-Echelon Network*](https://medium.com/towards-data-science/reinforcement-learning-for-inventory-optimization-series-ii-an-rl-model-for-a-multi-echelon-921857acdb00)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[*库存优化中的强化学习系列 II：多级网络的 RL 模型*](https://medium.com/towards-data-science/reinforcement-learning-for-inventory-optimization-series-ii-an-rl-model-for-a-multi-echelon-921857acdb00)'
- en: In previous articles, I built two RL models for a single retailer and a multi-echelon
    supply chain network. In these two articles, I trained the RL model using a simulator
    of the inventory system based on a historical demand dataset following a mixture
    of different normal distributions whose parameters depend on days of week. Then
    I tested the RL models on a test set in which the demand data follow the same
    distribution as the historical training data. This is essentially assuming the
    simulator perfectly matches the real world where we would apply the RL models
    to, and historical demand data perfectly represent the future demand pattern.
    However, this assumption is rarely true in real-world applications.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的文章中，我为单一零售商和多级供应链网络构建了两个 RL 模型。在这两篇文章中，我使用了基于历史需求数据的库存系统模拟器来训练 RL 模型，这些数据遵循不同正态分布的混合，其参数依赖于星期几。然后，我在一个测试集上测试了
    RL 模型，其中的需求数据与历史训练数据的分布相同。这实际上假设模拟器与我们应用 RL 模型的现实世界完全匹配，且历史需求数据完全代表未来需求模式。然而，这一假设在实际应用中很少成立。
- en: How to deal with the gap between the distributions in the training set and test
    set is a well-known problem in the general context of machine learning. The essence
    of this problem is the bias vs variance trade-off. If we rely too much on the
    training set to train our model, then the model is prone to overfitting, meaning
    it will only well on the training set but not on the test set. The same issue
    applies here in the RL field.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如何处理训练集与测试集之间分布的差距是机器学习领域中的一个著名问题。这个问题的核心是偏差与方差的权衡。如果我们过于依赖训练集来训练模型，那么模型容易过拟合，意味着它只会在训练集上表现良好，而在测试集上表现不佳。在
    RL 领域也是如此。
- en: Performance Deterioration due to the Gap between Simulator and Real World
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 由于模拟器与现实世界之间的差距导致的性能退化
- en: Taking the RL models I built previously as an example, the models were trained
    through a large number of episodes on the same historical demand data, hence it
    is highly likely that the inventory policies learned from the trained RL models
    are overfitted to the demand pattern represented by the historical demand data.
    The inventory policies will perform well if the same demand pattern continues
    in the future, but the performance will deteriorate if the future demand pattern
    deviates from the historical one.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以我之前构建的 RL 模型为例，这些模型通过大量的历史需求数据进行训练，因此很可能从训练的 RL 模型中学习到的库存政策过度拟合了历史需求数据所代表的需求模式。如果未来的需求模式继续保持相同，库存政策将表现良好，但如果未来的需求模式偏离历史模式，性能将会退化。
- en: As a numerical illustration, let’s assume that the future demand distribution
    in the real world deviates from the historical demand data in the simulator used
    for training the RL models. I took the RL model trained in [my first article](https://medium.com/towards-data-science/a-reinforcement-learning-based-inventory-control-policy-for-retailers-ac35bc592278)
    and applied it to two future demand scenarios in real world. In one of the scenarios,
    we assume the means of the mixture of the normal distributions will increase by
    1, while in the other scenario, the means will decrease by 1\. The demand distribution
    structure is given by the table below.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ae64463d93bb6ab33b286a83fb4967d3.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: Two future/real-world demand distribution scenarios (Image by author)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: For both of the demand increase scenario and demand decrease scenario (second
    and third columns in the table above), I generated 100 demand datasets which consist
    of 52 weeks of data using their own demand distribution setting. Then I applied
    the DQN policy trained on the historical/simulator demand data (first column in
    the table above) to both scenarios, the corresponding average profits obtained
    are given in the table below.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/875d73b680def5abba7790abdb6faf55.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: Average profits obtained by applying the DQN policy trained on historical/simulator
    data to each demand scenario (Image by author)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: To assess how well the trained DQN policy performs, for each of the demand scenarios,
    let’s assume we know what the future/real-world demand distribution will deviate
    ahead of time, and generate a training set consisting of 52 weeks of demand data
    using its own future/real-world demand distribution setting. Then we train a brand
    new RL model using its own training set for each scenario and apply the new RL
    model to each scenario. The corresponding average profits obtained are given in
    the table below.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/46b9f9db2d6275890dd36142ebd0b90a.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: Average profits obtained by applying the DQN policy trained on each scenario’s
    training set to each demand scenario (Image by author)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: As we can see from the comparison in the two tables above ($25077.61 vs $27399.79
    and $13890.99 vs $14707.44), there is a performance deterioration in the RL model
    if there is a gap between the simulator and real world.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Bridging the Gap Using Domain Randomization
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To bridge the gap between the simulator and real world, in real applications,
    one may choose to select a shorter period of most recent historical demand data,
    and assume this shorter period represents the future demand pattern or trend better.
    Then we frequently retrain the RL model using the recently updated shorter period
    of historical demand data. However, we can also try to address this problem during
    the training process in the first place using the concept of domain randomization.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Domain randomization is a technique typically used for sim-to-real transfer
    when applying RL to robotics. The applications of RL in robotics also face the
    issue of reality gap, since the gap between the simulation and real world degrades
    the performance of the policies once the RL models are transferred into real robots[1].
    In robotics context, the core idea of domain randomization is by randomizing the
    the physical parameters of the simulated environment (e.g., friction coefficients
    and vision properties such as objects’ appearance) when training the RL model,
    the RL model will experience situations more like the real environment, hence
    the learned policies will generalize better to the real environment. The figure
    below illustrates the intuition behind domain randomization.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7c7a3d1976c0b74a9a3e3e6a0e95d41b.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
- en: Intuition behind domain randomization (Image from reference [1])
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: In the context of inventory optimization, to implement domain randomization,
    we can add randomization to the historical demand data. To be specific, we extend
    the historical demand dataset by adding normally distributed noise to the demand
    observations. The implementation of this idea is given in the code block below.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here I sort of created 10 historical demand scenario by adding a random noise
    to the demand observation in each scenario. Then I appended the scenarios together
    as the entire historical demand data used for training, pretending that we have
    a 10 years’ of demand data. Note that the essence of this idea also aligns with
    the data augmentation technique in computer vision tasks where we manipulate the
    images to enrich the training set to avoid overfitting.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: I tried different values for the mean of the normal distribution used for generating
    the random noise. Initially I thought it makes sense to use a 0 mean, since it
    will more likely generate equal numbers of demand increase and decrease scenarios.
    Interestingly, after trying out different values, I found making the mean above
    0 (more demand increase scenarios) gave better test results for this particular
    example. It might be due to the fact that the demand data are truncated at 0,
    hence although we generate noise with mean 0, there is not too much room for the
    demand distribution to shift left. So generating noise with a mean of a positive
    number will let the RL model learn more useful knowledge.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Now we train the DQN model using the new 10 years’ historical demand data obtained
    by domain randomization and then apply the learned DQN policy to the demand increase
    and decrease scenarios (detailed setting in the first table) described in the
    previous section. The following table shows the average profits obtained in each
    scenario.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/360f435930f658b9dada5c27e199242f.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
- en: Average profits obtained in each scenario using the DQN policy trained after
    domain randomization (Image by author)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: We see that there is a clear improvement as compared to the results without
    domain randomization ($26432.61 vs $25077.61 in increase scenario, $14311.52 vs
    $13890.99 in decrease scenario).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，与没有领域随机化的结果相比，明显有所改善（增加场景中为 $26432.61 对 $25077.61，减少场景中为 $14311.52 对 $13890.99）。
- en: To show the performance improvement is not obtained by chance, I further created
    more randomized test demand scenarios by adding a random noise to the means and
    variances of the mixture of demand distributions. See the code below.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了表明性能提升不是偶然获得的，我进一步创建了更多随机化的测试需求场景，通过对需求分布的均值和方差添加随机噪声。见下面的`代码`。
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The DQN policy before domain randomization gives an average profit of $20163.84
    on this test set, and the DQN policy after domain randomization gives an average
    profit of $21887.77, which still shows a performance improvement.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在领域随机化之前，DQN 策略在该测试集上的平均利润为 $20163.84，而领域随机化之后的 DQN 策略平均利润为 $21887.77，这仍然显示出性能的提升。
- en: '**Conclusion**'
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**结论**'
- en: In this article, I focus on pointing out and addressing the gap between training/simulation
    and test/real world environments for the previous RL model I built for inventory
    optimization. It’s good to see that techniques such as domain randomization in
    the robotics field can also be effective in the inventory optimization area. Experimental
    results suggest that it should be a good practice to manipulate historical demand
    data using techniques like domain randomization to enrich the training set, so
    that the RL model trained in a lab can better generalize to the real world.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我重点指出并解决了我为库存优化构建的 RL 模型在训练/仿真和测试/现实世界环境之间的差距。很高兴看到，像领域随机化这样的技术在机器人领域中也能在库存优化领域发挥作用。实验结果表明，使用领域随机化等技术来操控历史需求数据以丰富训练集应该是一种良好的实践，这样实验室中训练的
    RL 模型可以更好地推广到现实世界。
- en: References
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] [Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey](https://arxiv.org/abs/2009.13303)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [深度强化学习在机器人领域的仿真到现实迁移：综述](https://arxiv.org/abs/2009.13303)'
