- en: Creating a Dutch question-answering machine learning model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/creating-a-dutch-question-answering-machine-learning-model-3b666a115be3?source=collection_archive---------3-----------------------#2023-01-29](https://towardsdatascience.com/creating-a-dutch-question-answering-machine-learning-model-3b666a115be3?source=collection_archive---------3-----------------------#2023-01-29)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: NLP Tutorial
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Creating a new dataset by using NLP translation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@ErwinVanCrasbeek?source=post_page-----3b666a115be3--------------------------------)[![Erwin
    van Crasbeek](../Images/7eab3fc949f86b80ccd8e99f17a1ed7f.png)](https://medium.com/@ErwinVanCrasbeek?source=post_page-----3b666a115be3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3b666a115be3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3b666a115be3--------------------------------)
    [Erwin van Crasbeek](https://medium.com/@ErwinVanCrasbeek?source=post_page-----3b666a115be3--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Feeab190a1f50&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-a-dutch-question-answering-machine-learning-model-3b666a115be3&user=Erwin+van+Crasbeek&userId=eeab190a1f50&source=post_page-eeab190a1f50----3b666a115be3---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3b666a115be3--------------------------------)
    ·20 min read·Jan 29, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3b666a115be3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-a-dutch-question-answering-machine-learning-model-3b666a115be3&user=Erwin+van+Crasbeek&userId=eeab190a1f50&source=-----3b666a115be3---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3b666a115be3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-a-dutch-question-answering-machine-learning-model-3b666a115be3&source=-----3b666a115be3---------------------bookmark_footer-----------)![](../Images/c188bc671e83f8a4d731dd53628bac15.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline for the creation of a Dutch question-answering model
  prefs: []
  type: TYPE_NORMAL
- en: Natural language processing models are currently a hot topic. The release of
    ‘Attention Is All You Need’ by Google [1] has spurred the development of many
    Transformer models like BERT, GPT-3, and ChatGPT which have received a lot of
    attention all over the world. While many language models are trained on English
    or multiple languages, models and datasets for specific languages can be difficult
    to find or of questionable quality.
  prefs: []
  type: TYPE_NORMAL
- en: NLP has a vast amount of applications including but not limited to translation,
    information extraction, summarization and question answering, the latter of which
    is something I’ve personally been working on. As an Applied Artificial Intelligence
    student, I have been working on question answering NLP models and have found it
    challenging to find a useful Dutch dataset for training purposes. To address this
    issue, I have developed a translation solution that can be applied to various
    NLP problems and pretty much all languages, which may be of interest to other
    students. I feel like this also has a great value for the AI development and research
    community. There are basically no Dutch datasets available especially for specific
    tasks like question answering. By translating a large and well-known dataset,
    I have been able to create a Dutch question answering model with relatively low
    effort.
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in learning more about my process, the challenges I faced,
    and the potential applications of this solution, please continue reading. This
    article is aimed at students with a basic NLP background. However, I’ve also included
    a refresher and introductions to various concepts for those who are not yet familiar
    in the field or simply need a refresher.
  prefs: []
  type: TYPE_NORMAL
- en: 'To properly explain my solution for using translated datasets, I have divided
    this article into two main sections, the translation of a dataset and the training
    of a question answering model. I’ve written this article in a way that intends
    to show you my progress towards the solution but also as a step-by-step guide.
    The article consists of the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Refresher on NLP and a brief history of NLP](#6ae5)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[The problem, the dataset and question answering](#4fab)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Translating the dataset](#137f)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Building a question answering model](#5e71)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[What has been achieved and what has not been achieved?](#df96)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Future plans](#a6ce)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Sources](#c683)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Refresher on NLP and a brief history of NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To get a better understanding of the various elements of this solution, I want
    to start with refresher on NLP and its recent history. The languages we know can
    be split in two groups, formal and natural. Formal language refers to languages
    that have specifically been designed for specific tasks like math and programming.
    A natural or ordinary language is a language that has naturally been developed
    and evolved by humans without any form of planning ahead. This can take multiple
    forms like the different kinds of human speech we know or even sign language [2].
  prefs: []
  type: TYPE_NORMAL
- en: NLP in its broadest form is the application of computational methods to natural
    languages. By combining rule-based modelling of language with AI models, we have
    been able to get computers to ‘understand’ our human language in a way that allows
    it to process it both in text and voice form [3]. The way this understanding works
    — if it can even be called understanding — is up for debate. But recent developments
    like ChatGPT have shown that we humans do often feel like the output of these
    models makes it feel sentient and like has a high level of understanding [4].
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this understanding didn’t come out of nowhere. NLP has a vast history
    dating back to the 1940s after World War II [5]. During this period, people realized
    the importance of translation and hoped to create a machine that could do so automatically.
    However, this proved to be quite the challenge. Around 1960, NLP research split
    into rule-based and stochastic. Rule-based, or symbolic covered mainly formal
    languages and the generation of syntax. Many of the linguistic researchers and
    computer scientists in this group saw this as the beginning of artificial intelligence
    research. Stochastic research focused more on statistics and problems like pattern
    recognition between texts.
  prefs: []
  type: TYPE_NORMAL
- en: Since then, many more developments on NLP have been made and many more areas
    of research have emerged. However, the actual text resulting from NLP models has
    always been quite limited and didn’t have many real-world applications. That is,
    until the early 2000s. After this period developments in NLP made big leaps every
    few years which led to where we are now.
  prefs: []
  type: TYPE_NORMAL
- en: The problem, the dataset and question answering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that I’ve given a short refresher on NLP it is time to introduce the actual
    problem that I have been working on. In short, my goal was to train a Dutch question
    answering machine learning model. However, the lack of suitable datasets made
    this quite difficult which is I created my own by using translation. In this article
    I will go through the creation of a dataset and the training of the machine learning
    model step by step so you can follow along and either replicate the entire solution
    or select the parts that are of importance to you.
  prefs: []
  type: TYPE_NORMAL
- en: This article can be split into two main components. The first one being the
    creation of a Dutch dataset and the second being the training of a question answering
    machine learning model. In this chapter I will give some background information
    on them, introduce my solutions and explain my choices.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we want to find a useful Dutch dataset it is important to look at what is
    exactly needed to train a question answering model. There are two main approaches
    to the generation of answers to questions. The first one being extractive and
    the second one being abstractive.
  prefs: []
  type: TYPE_NORMAL
- en: · **Extractive** question answering models are trained to extract an answer
    from the context (the source text) [7]. Older approaches used to do this by training
    a model to output a start and end index of the location of the answer in the context.
    However, the introduction of Transformers has made this approach obsolete.
  prefs: []
  type: TYPE_NORMAL
- en: · **Abstractive** question answering models are trained to generate new text
    based on the context and the question [8].
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1 shows an example of the output extractive and abstractive models might
    give.
  prefs: []
  type: TYPE_NORMAL
- en: Although different approaches are possible, nowadays both extractive and abstractive
    question answering models are often based on Transformers like BERT [8], [9].
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0682a79c2ea32064801f8556b954126a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. An example of an answer generated in an extractive versus abstractive
    way.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the information about extractive and abstractive models, we now know
    that we need a dataset with contexts, questions, answers and, optionally, start
    and end indices of the location of the answer in the context. I have explored
    the following options in order to find a suitable dataset.
  prefs: []
  type: TYPE_NORMAL
- en: I have used A 2020 paper by Cambazoglu *et al*. [10] to get a clear image of
    what datasets are available for question answering. Their research has resulted
    in a table with the most prominent question answering datasets. Unfortunately,
    none of these big datasets are in the Dutch language.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another option was Huggingface which hosts a large collection of datasets [11].
    At first glance, there are a few question answering datasets available for the
    Dutch language. However, further inspection shows that these datasets are often
    incomplete, include website domains instead of contexts or are a mix of various
    languages. These are completely unusable or too incomplete to be used for our
    goal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concluding from these observations, there are practically no public datasets
    that can be used to train a Dutch question answering model. Creating our own dataset
    manually would take far too much time so what other options do we have? Firstly
    we could simply use an English model, translate the input from Dutch to English
    and then translate the output back to Dutch. However, a quick test with Google
    Translate and this approach has shown that the results are far from desirable
    and almost feel passive aggressive. Perhaps too much information and context got
    lost during the double translation step? That leads to the second option, translating
    the entire dataset and training on it. During my research I have come across a
    few instances where this was mentioned. For example a post by Zoumana Keita on
    Towardsdatascience [16] uses translation for data augmentation. Chapter three
    will dive into my execution of the translation of a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly we need to select what dataset to use for our translation approach. Since
    we decided to translate the entire dataset, it does not matter what language the
    original dataset is in. The [Stanford Question Answering Dataset](https://rajpurkar.github.io/SQuAD-explorer/)
    (SQuAD) [12] seems to be quite popular and is used by Paperswithcode for the question
    answering benchmark [13]. It also contains a large amount (100.000+) of questions
    with answers and upon closer inspection does not seem to have any unexpected data.
    This is the dataset we will be working with.
  prefs: []
  type: TYPE_NORMAL
- en: The machine learning model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we have determined how we are going to get a dataset; we need to decide
    what kind of machine learning model will be suitable for the goal of answering
    questions. In the previous chapter we have established that we can choose between
    an extractive model and an abstractive model. In my research I have used an abstractive
    model because it is based on a newer technology and gives more interesting results.
    However, just in case anyone wants to take this approach for an extractive model
    I will cover that as well. This is also in line with the selection of the dataset
    since it contains the start indices of answers.
  prefs: []
  type: TYPE_NORMAL
- en: Training a Transformer from scratch would be, to say the least, inefficient.
    The book transfer Learning for Natural Language Processing by P. Azunre [14] goes
    in-depth on why transfer learning is done and shows a number of examples on how
    to do it. A large number of big NLP models are hosted on Huggingface [15] and
    are available for transfer learning. I have chosen the t5-v1_1-base model because
    it is multi-task trained on multiple languages. Chapter 4 will cover the transfer
    learning of this model.
  prefs: []
  type: TYPE_NORMAL
- en: Translating the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter I will be showing how I have translated the dataset by giving
    snippets of code and explaining them. The code resulting from these code blocks
    in succession is the entire dataset translation script I’ve written. Feel free
    to follow along or take specific parts that are of use to you.
  prefs: []
  type: TYPE_NORMAL
- en: '*Imports*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The solution uses a few modules. First of all, we need to translate text in
    a way that is as fast as possible. In my research I have tried using various translation
    AI models from Huggingface but by far the fastest translator was the Googletrans
    module which makes use of the Google Translate API. The solution also uses Timeout
    from httpx to define a timeout for the translations, json for SQuAD dataset parsing,
    Pandas for dataframes and Time to measure how long everything is taking.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Initialization**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First of all we should define a few constants that will be used throughout the
    script. For ease-of-access I have added the source language and translation language
    here.
  prefs: []
  type: TYPE_NORMAL
- en: The Googletrans module provides us with a Translator that can have a custom
    timout defined. I have used a relatively long timeout because translations kept
    timing out during my tests. I will provide a bit more information on this issue
    further along in the guide.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Reading the SQuAD dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following code extracts contexts, questions and answers from the train and
    validation json files. This is done by reading the files as json format and looping
    through the data inside in a way that extracts the three lists. For each question
    and answer, the context is copied and added to the contexts list. This way we
    can easily access a question with its relevant context and answer by using an
    index.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Timing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following code provides us with a very rough estimation of how long each
    translation takes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Translating
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Remember how I mentioned translations timing out? During my research I kept
    bumping into the issue where translations were timing out and the resulting dataset
    got corrupted. It turns out that the Googletrans module is not 100% reliable since
    it uses the Google Translate API. The way I have found around this is to create
    a small wrapper function that keeps trying to translate until it has been successful.
    After doing this I no longer experienced the timing out problem.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Because of the way we have extracted contexts from the dataset, they have been
    duplicated for each question and answer pair. Simply translating all contexts
    would be redundant and very slow so the following translation function compares
    the previous context to the current one first. If they match, the previous translation
    is used.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Translating the questions and answers is pretty straightforward since we just
    need to loop through the lists to translate all of them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now we can use the functions we have defined to translate all parts of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Exporting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All that is left is exporting the translations for later use. We can do this
    by converting the lists to dataframes and then using the to_csv function. One
    thing to keep in mind is that the Googletrans module outputs translations with
    characters that are not included in utf-8 encoding. That is why we use utf-16
    encoding here. It would make sense to convert it to utf-8 at some point since
    that might be more useful in an AI model. However, since we are just working on
    the dataset here we can decide to leave that step for later when we are doing
    the data preprocessing for training our model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Building a question answering model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Figuring out how to train a question answering model turned out to be a bit
    of a challenge. However, by taking inspiration from a Notebook by P. Suraj [17],
    I was able to create a Transformer based model that can be trained on question
    answering. In line with the Notebook I have used Torch to create the model.
  prefs: []
  type: TYPE_NORMAL
- en: Imports
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Starting with the imports, the following modules are used. We also define some
    variables that define the max in- and output length of the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Loading data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we can load the dataset that we have previously created. Since we used Pandas
    to export a csv we can now easily load it and convert it to an array. I have also
    defined a function that will be used later on to convert any training or input
    data to utf-8 which is the format we will train the model on.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now we can actually load the data. For the training of the model I only used
    the train data and split this with a test size of 0.2.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Preparing data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like I mentioned before, it is possible to train an extractive model and an
    abstractive model. During my research I developed both an extractive and an abstractive
    model. In this article I just cover the abstractive version but, for anyone interested,
    I will also explain how I preprocessed my data for the extractive model. This
    was necessary to create the start- and endindices of the answers in contexts.
  prefs: []
  type: TYPE_NORMAL
- en: '**Abstractive**'
  prefs: []
  type: TYPE_NORMAL
- en: The dataset does not need much preprocessing in order to train an abstractive
    model. We simply convert all train data to utf-8\. The last three lines can be
    uncommented to decrease the size of the trainset, this will improve training time
    and might help with debugging.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**Extractive**'
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, extractive models need start and end indices of the answer in
    the context. However, since we translated our dataset using a Transformer a few
    issues can occur. For example, answers might be worded differently than in the
    context or the position or length of the answer might have changed. To solve this,
    we can try to find the answer in the context and, if the answer is found, add
    it to the cleaned answers. Because of this, we also have information about the
    start index and the end index is simply the start index plus the length of the
    answer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Tokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next step is tokenizing, since we are using t5-v1_1-base, we can simply
    import the tokenizer from Huggingface. Then we will tokenize the contexts with
    the questions so that the tokenizer will add them together with end of string
    tokens. We also specify the previously defined max_text_length. Lastly the tokenized
    answers are added to the encodings as the target.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Dataloader
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll use a Dataloader to train the PyTorch model as follows. In here the batch
    size can also be specified. The server I trained on had limited memory so I had
    to use a batch size of 2\. If possible, a bigger batch size would be preferable.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Training the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The model we use is T5ForConditionalGeneration based on T5-v1_1-base. If CUDA
    is installed on the PC or server that is used for training, we can try to utilize
    it to significantly increase training speed. We also tell the model that we are
    going to train it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The optimizer we use is AdamW with a learning rate of 1e-4\. This is based
    on the T5 documentation [18] which mentions that it is a good value to use in
    our situation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Typically, 1e-4 and 3e-4 work well for most problems (classification, summarization,
    translation, question answering, question generation).*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Lastly we define a function that saves our model for later usage after it is
    done training.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The actual training of the model will be done in three epochs, the Notebook
    I have used [17] and the T5 documentation both state that this is a good amount
    of epochs to train on. On my PC which has a RTX 3090 this would take about 24
    hours per epoch. The server I have used took advantage of an Nvidia Tesla T4 and
    took about 6 hours per epoch.
  prefs: []
  type: TYPE_NORMAL
- en: The Tqdm module is used for visual feedback on the training state. It provides
    us with data about the elapsed time and the estimated time training will take.
    The steps between the two commented arrows are important for our goal of question
    answering, this is where we define what input to give the model. The other steps
    taken in this code block are pretty straightforward for the training of a PyTorch
    model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you have followed along, congratulations! You have created your own Dutch
    dataset and trained a Dutch question answering model! If you are like me, you
    probably can’t wait to try the model to see what results it gives. You can use
    the following code to evaluate the model. Interestingly enough, you might find
    that the model is not only capable of answering Dutch questions! It is also somewhat
    capable of answering questions in different (mostly Germanic) languages. This
    is most likely due to the fact that the original T5-v1_1-base model has been trained
    on four different languages.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are some example contexts and questions together with the answers that
    have been generated by the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Context** We zijn met de klas van de master Applied Artificial Intelligence
    naar keulen geweest.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Question** Waar is de klas heen geweest?'
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer** Keulen'
  prefs: []
  type: TYPE_NORMAL
- en: '**Context** De grote bruine vos springt over de luie hond heen.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Question** Waar springt de vos overheen?'
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer** Luie hond'
  prefs: []
  type: TYPE_NORMAL
- en: '**Context** The big brown fox jumps over the lazy dog.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Question** What does the fox do?'
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer** Jumps over the lazy dog'
  prefs: []
  type: TYPE_NORMAL
- en: '**Context** Twee maal twee is tien.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Question** Wat is twee maal twee?'
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer** Tien'
  prefs: []
  type: TYPE_NORMAL
- en: What has been achieved and what has not been achieved?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, to summarize, we have selected an English dataset for question answering,
    translated it to Dutch using the Google Translate API and we have trained a PyTorch
    encoder-decoder model based on T5-v1_1-base. What exactly have we achieved with
    this and can this be used in real-life situations?
  prefs: []
  type: TYPE_NORMAL
- en: First of all, it is important to realize that we have not properly evaluated
    the model as that was not part of the scope of this article. However, to be able
    to properly interpret our results and to be able to say something about its usability,
    I suggest looking into metrics like Rouge [19] or a human evaluation. The approach
    I have taken is a human evaluation. Table 2 shows the average rating between one
    and five that five people have given the generated answers of various context
    sources and questions. The average score is 2.96\. This number alone does not
    tell us much but we can conclude from the table that the model we created can
    in some cases generate near perfect answers. However, it does also quite often
    generate answers that the panel of human evaluators consider to be complete nonsense.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f55d4a916a6b078f6daa3f1443a294b5.png)'
  prefs: []
  type: TYPE_IMG
- en: Table 2\. Human evaluation scores (1–5) of various articles, papers and theses.
  prefs: []
  type: TYPE_NORMAL
- en: It is also important to note that, by translating a dataset, we have most likely
    introduced a bias. The AI behind Google Translate has been trained on a dataset
    which, since it is based on natural language, naturally contains a bias. By translating
    our data with it, this bias will be passed on to any model that’s trained with
    the dataset. Before a dataset created like this can be used in a real-life situation,
    it should be evaluated thoroughly to indicate what biases there are and how they
    impact the results.
  prefs: []
  type: TYPE_NORMAL
- en: However, this solution can be very interesting to people who are experimenting
    with AI, developing a new kind of machine learning model or are simply learning
    about NLP. It is a very accessible way to get a big dataset in any language for
    almost any NLP problem. Many students do not have access to big datasets because
    they are only accessible to big companies or are too expensive. With an approach
    like this, any big English dataset can be transformed into a dataset in a specific
    language.
  prefs: []
  type: TYPE_NORMAL
- en: Future plans
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Personally I am interested in seeing where I can take this approach. I am currently
    working on a question generation model that is using exactly the same approach
    and dataset. I would like to investigate the usage of these two models combined
    so I can learn more about potential biases or errors that have been introduced.
    This is in line with chapter 5 in which I talked about the need for evaluation.
    I have created a human evaluation by asking five people to rate the results of
    the created model. However, I intend to learn more about different metrics which
    can hopefully tell me more about how the model works, why it generates certain
    results and what biases it contains.
  prefs: []
  type: TYPE_NORMAL
- en: I have also learned that version 2.0 of the Stanford Question and Answer dataset
    includes questions that cannot be answered. Even though it is not directly related
    to the solution offered in this article, I am curious about the differences in
    results when I apply the solution of this article to the full SQuAD 2.0 dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] A. Vaswani *et al.*, “Attention Is All You Need,” 2017.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] D. Khurana, A. Koli, K. Khatter, and S. Singh, “Natural language processing:
    state of the art, current trends and challenges,” *Multimedia Tools and Applications*,
    Jul. 2022, doi: 10.1007/s11042–022–13428–4.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] “What is Natural Language Processing? | IBM,” [*www.ibm.com*.](http://www.ibm.com.)
    [https://www.ibm.com/topics/natural-language-processing](https://www.ibm.com/topics/natural-language-processing)
    (accessed Jan. 11, 2023).'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] E. Holloway, “Yes, ChatGPT Is Sentient — Because It’s Really Humans in
    the Loop,” *Mind Matters*, Dec. 26, 2022\. [https://mindmatters.ai/2022/12/yes-chatgpt-is-sentient-because-its-really-humans-in-the-loop/](https://mindmatters.ai/2022/12/yes-chatgpt-is-sentient-because-its-really-humans-in-the-loop/)
    (accessed Jan. 18, 2023).'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] “NLP — overview,” *cs.stanford.edu*. [https://cs.stanford.edu/people/eroberts/courses/soco/projects/2004-05/nlp/overview_history.html](https://cs.stanford.edu/people/eroberts/courses/soco/projects/2004-05/nlp/overview_history.html)
    (accessed Jan. 18, 2023).'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] S. Ruder, “A Review of the Recent History of Natural Language Processing,”
    *Sebastian Ruder*, Oct. 01, 2018\. [https://ruder.io/a-review-of-the-recent-history-of-nlp/](https://ruder.io/a-review-of-the-recent-history-of-nlp/)
    (accessed Jan. 18, 2023).'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] S. Varanasi, S. Amin, and G. Neumann, “AutoEQA: Auto-Encoding Questions
    for Extractive Question Answering,” *Findings of the Association for Computational
    Linguistics: EMNLP 2021*, 2021.'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] “What is Question Answering? — Hugging Face,” *huggingface.co*. [https://huggingface.co/tasks/question-answering](https://huggingface.co/tasks/question-answering)
    (accessed Jan. 18, 2023).'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] R. E. López Condori and T. A. Salgueiro Pardo, “Opinion summarization methods:
    Comparing and extending extractive and abstractive approaches,” *Expert Systems
    with Applications*, vol. 78, pp. 124–134, Jul. 2017, doi: 10.1016/j.eswa.2017.02.006.'
  prefs: []
  type: TYPE_NORMAL
- en: '[10] B. B. Cambazoglu, M. Sanderson, F. Scholer, and B. Croft, “A review of
    public datasets in question answering research,” *ACM SIGIR Forum*, vol. 54, no.
    2, pp. 1–23, Dec. 2020, doi: 10.1145/3483382.3483389.'
  prefs: []
  type: TYPE_NORMAL
- en: '[11] “Hugging Face — The AI community building the future.,” *huggingface.co*.
    [https://huggingface.co/datasets?language=language:nl&task_categories=task_categories:question-answering&sort=downloads](https://huggingface.co/datasets?language=language%3Anl&task_categories=task_categories%3Aquestion-answering&sort=downloads)
    (accessed Jan. 18, 2023).'
  prefs: []
  type: TYPE_NORMAL
- en: '[12] “The Stanford Question Answering Dataset,” *rajpurkar.github.io*. [https://rajpurkar.github.io/SQuAD-explorer/](https://rajpurkar.github.io/SQuAD-explorer/)
    (accessed Jan. 18, 2023).'
  prefs: []
  type: TYPE_NORMAL
- en: '[13] “Papers with Code — Question Answering,” *paperswithcode.com*. [https://paperswithcode.com/task/question-answering](https://paperswithcode.com/task/question-answering)
    (accessed Jan. 18, 2023).'
  prefs: []
  type: TYPE_NORMAL
- en: '[14] P. Azunre, *Transfer Learning for Natural Language Processing*. Simon
    and Schuster, 2021.'
  prefs: []
  type: TYPE_NORMAL
- en: '[15] “Hugging Face — On a mission to solve NLP, one commit at a time.,” *huggingface.co*.
    [https://huggingface.co/models](https://huggingface.co/models) (accessed Jan.
    18, 2023).'
  prefs: []
  type: TYPE_NORMAL
- en: '[16] Z. Keita, “Data Augmentation in NLP Using Back Translation With MarianMT,”
    *Medium*, Nov. 05, 2022\. [https://towardsdatascience.com/data-augmentation-in-nlp-using-back-translation-with-marianmt-a8939dfea50a](/data-augmentation-in-nlp-using-back-translation-with-marianmt-a8939dfea50a)
    (accessed Jan. 18, 2023).'
  prefs: []
  type: TYPE_NORMAL
- en: '[17] P. Suraj, “Google Colaboratory,” *colab.research.google.com*. [https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb)
    (accessed Jan. 25, 2023).'
  prefs: []
  type: TYPE_NORMAL
- en: '[18] “T5,” *huggingface.co*. [https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Model](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Model)
    (accessed Jan. 25, 2023).'
  prefs: []
  type: TYPE_NORMAL
- en: '[19] “ROUGE — a Hugging Face Space by evaluate-metric,” *huggingface.co*. [https://huggingface.co/spaces/evaluate-metric/rouge](https://huggingface.co/spaces/evaluate-metric/rouge)
    (accessed Jan. 25, 2023).'
  prefs: []
  type: TYPE_NORMAL
- en: All images unless otherwise noted are by the author.
  prefs: []
  type: TYPE_NORMAL
