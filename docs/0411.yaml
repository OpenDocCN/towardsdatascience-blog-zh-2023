- en: Creating a Dutch question-answering machine learning model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建荷兰语问答机器学习模型
- en: 原文：[https://towardsdatascience.com/creating-a-dutch-question-answering-machine-learning-model-3b666a115be3?source=collection_archive---------3-----------------------#2023-01-29](https://towardsdatascience.com/creating-a-dutch-question-answering-machine-learning-model-3b666a115be3?source=collection_archive---------3-----------------------#2023-01-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/creating-a-dutch-question-answering-machine-learning-model-3b666a115be3?source=collection_archive---------3-----------------------#2023-01-29](https://towardsdatascience.com/creating-a-dutch-question-answering-machine-learning-model-3b666a115be3?source=collection_archive---------3-----------------------#2023-01-29)
- en: NLP Tutorial
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然语言处理教程
- en: Creating a new dataset by using NLP translation
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用自然语言处理翻译创建新的数据集
- en: '[](https://medium.com/@ErwinVanCrasbeek?source=post_page-----3b666a115be3--------------------------------)[![Erwin
    van Crasbeek](../Images/7eab3fc949f86b80ccd8e99f17a1ed7f.png)](https://medium.com/@ErwinVanCrasbeek?source=post_page-----3b666a115be3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3b666a115be3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3b666a115be3--------------------------------)
    [Erwin van Crasbeek](https://medium.com/@ErwinVanCrasbeek?source=post_page-----3b666a115be3--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ErwinVanCrasbeek?source=post_page-----3b666a115be3--------------------------------)[![Erwin
    van Crasbeek](../Images/7eab3fc949f86b80ccd8e99f17a1ed7f.png)](https://medium.com/@ErwinVanCrasbeek?source=post_page-----3b666a115be3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3b666a115be3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3b666a115be3--------------------------------)
    [Erwin van Crasbeek](https://medium.com/@ErwinVanCrasbeek?source=post_page-----3b666a115be3--------------------------------)'
- en: ·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Feeab190a1f50&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-a-dutch-question-answering-machine-learning-model-3b666a115be3&user=Erwin+van+Crasbeek&userId=eeab190a1f50&source=post_page-eeab190a1f50----3b666a115be3---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3b666a115be3--------------------------------)
    ·20 min read·Jan 29, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3b666a115be3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-a-dutch-question-answering-machine-learning-model-3b666a115be3&user=Erwin+van+Crasbeek&userId=eeab190a1f50&source=-----3b666a115be3---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Feeab190a1f50&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-a-dutch-question-answering-machine-learning-model-3b666a115be3&user=Erwin+van+Crasbeek&userId=eeab190a1f50&source=post_page-eeab190a1f50----3b666a115be3---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3b666a115be3--------------------------------)
    ·20 min阅读·2023年1月29日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3b666a115be3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-a-dutch-question-answering-machine-learning-model-3b666a115be3&user=Erwin+van+Crasbeek&userId=eeab190a1f50&source=-----3b666a115be3---------------------clap_footer-----------)'
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3b666a115be3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-a-dutch-question-answering-machine-learning-model-3b666a115be3&source=-----3b666a115be3---------------------bookmark_footer-----------)![](../Images/c188bc671e83f8a4d731dd53628bac15.png)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3b666a115be3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-a-dutch-question-answering-machine-learning-model-3b666a115be3&source=-----3b666a115be3---------------------bookmark_footer-----------)![](../Images/c188bc671e83f8a4d731dd53628bac15.png)'
- en: Pipeline for the creation of a Dutch question-answering model
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 荷兰语问答模型创建流程
- en: Natural language processing models are currently a hot topic. The release of
    ‘Attention Is All You Need’ by Google [1] has spurred the development of many
    Transformer models like BERT, GPT-3, and ChatGPT which have received a lot of
    attention all over the world. While many language models are trained on English
    or multiple languages, models and datasets for specific languages can be difficult
    to find or of questionable quality.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理模型目前是一个热门话题。谷歌发布的《Attention Is All You Need》[1] 推动了许多像 BERT、GPT-3 和 ChatGPT
    这样的 Transformer 模型的发展，这些模型受到了全球的广泛关注。虽然许多语言模型是在英语或多语言上进行训练的，但针对特定语言的模型和数据集可能难以找到或质量堪忧。
- en: NLP has a vast amount of applications including but not limited to translation,
    information extraction, summarization and question answering, the latter of which
    is something I’ve personally been working on. As an Applied Artificial Intelligence
    student, I have been working on question answering NLP models and have found it
    challenging to find a useful Dutch dataset for training purposes. To address this
    issue, I have developed a translation solution that can be applied to various
    NLP problems and pretty much all languages, which may be of interest to other
    students. I feel like this also has a great value for the AI development and research
    community. There are basically no Dutch datasets available especially for specific
    tasks like question answering. By translating a large and well-known dataset,
    I have been able to create a Dutch question answering model with relatively low
    effort.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: NLP有广泛的应用，包括但不限于翻译、信息提取、摘要和问答，而后者是我个人一直在从事的工作。作为应用人工智能的学生，我一直在研究问答NLP模型，并且发现很难找到有用的荷兰语数据集用于训练。为了解决这个问题，我开发了一个翻译解决方案，可以应用于各种NLP问题和几乎所有语言，这可能对其他学生有兴趣。我认为这对人工智能开发和研究社区也具有很大的价值。特别是对于像问答这样的特定任务，几乎没有荷兰语数据集。通过翻译一个大型且知名的数据集，我能够以相对较低的努力创建一个荷兰语问答模型。
- en: If you are interested in learning more about my process, the challenges I faced,
    and the potential applications of this solution, please continue reading. This
    article is aimed at students with a basic NLP background. However, I’ve also included
    a refresher and introductions to various concepts for those who are not yet familiar
    in the field or simply need a refresher.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣了解更多关于我的过程、我面临的挑战以及此解决方案的潜在应用，请继续阅读。本文旨在为具有基本NLP背景的学生提供。然而，我还为那些尚未熟悉该领域或仅需复习的人士提供了复习材料和各种概念的介绍。
- en: 'To properly explain my solution for using translated datasets, I have divided
    this article into two main sections, the translation of a dataset and the training
    of a question answering model. I’ve written this article in a way that intends
    to show you my progress towards the solution but also as a step-by-step guide.
    The article consists of the following chapters:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确解释我使用翻译数据集的解决方案，我将本文分为两个主要部分：数据集的翻译和问答模型的训练。我撰写本文的方式旨在展示我在解决方案方面的进展，同时也作为一个逐步指南。文章包括以下章节：
- en: '[Refresher on NLP and a brief history of NLP](#6ae5)'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[关于NLP的复习和NLP的简要历史](#6ae5)'
- en: '[The problem, the dataset and question answering](#4fab)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[问题、数据集和问答](#4fab)'
- en: '[Translating the dataset](#137f)'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[翻译数据集](#137f)'
- en: '[Building a question answering model](#5e71)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[构建一个问答模型](#5e71)'
- en: '[What has been achieved and what has not been achieved?](#df96)'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[已取得的成就与未取得的成就？](#df96)'
- en: '[Future plans](#a6ce)'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[未来计划](#a6ce)'
- en: '[Sources](#c683)'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[来源](#c683)'
- en: Refresher on NLP and a brief history of NLP
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于NLP的复习和NLP的简要历史
- en: To get a better understanding of the various elements of this solution, I want
    to start with refresher on NLP and its recent history. The languages we know can
    be split in two groups, formal and natural. Formal language refers to languages
    that have specifically been designed for specific tasks like math and programming.
    A natural or ordinary language is a language that has naturally been developed
    and evolved by humans without any form of planning ahead. This can take multiple
    forms like the different kinds of human speech we know or even sign language [2].
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这个解决方案的各个元素，我想从对NLP及其近期历史的复习开始。我们所知道的语言可以分为两组，形式语言和自然语言。形式语言指的是专门为特定任务如数学和编程设计的语言。自然语言或普通语言是指由人类自然发展和演变的语言，没有任何形式的预先规划。这可以表现为我们所知道的各种人类言语形式，甚至是手语[2]。
- en: NLP in its broadest form is the application of computational methods to natural
    languages. By combining rule-based modelling of language with AI models, we have
    been able to get computers to ‘understand’ our human language in a way that allows
    it to process it both in text and voice form [3]. The way this understanding works
    — if it can even be called understanding — is up for debate. But recent developments
    like ChatGPT have shown that we humans do often feel like the output of these
    models makes it feel sentient and like has a high level of understanding [4].
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: NLP在其最广泛的形式上是将计算方法应用于自然语言。通过将基于规则的语言建模与人工智能模型相结合，我们已经能够使计算机以一种能够处理文本和语音形式的方式“理解”我们的自然语言[3]。这种理解的方式——如果它真的可以称为理解的话——仍然存在争议。然而，像ChatGPT这样的最新发展表明，我们人类确实常常觉得这些模型的输出让人感到它有自我意识，并且具有较高的理解水平[4]。
- en: Of course, this understanding didn’t come out of nowhere. NLP has a vast history
    dating back to the 1940s after World War II [5]. During this period, people realized
    the importance of translation and hoped to create a machine that could do so automatically.
    However, this proved to be quite the challenge. Around 1960, NLP research split
    into rule-based and stochastic. Rule-based, or symbolic covered mainly formal
    languages and the generation of syntax. Many of the linguistic researchers and
    computer scientists in this group saw this as the beginning of artificial intelligence
    research. Stochastic research focused more on statistics and problems like pattern
    recognition between texts.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这种理解并非凭空而来。NLP有着广泛的历史，可以追溯到二战后的1940年代[5]。在这个时期，人们意识到了翻译的重要性，并希望创造一种能够自动完成翻译的机器。然而，这证明是相当具有挑战性的。大约在1960年左右，NLP研究分为基于规则的和随机的两大类。基于规则的或符号化的主要涉及形式语言和语法生成。这个领域的许多语言学研究者和计算机科学家认为这是人工智能研究的开始。随机研究则更多关注统计学和文本间的模式识别等问题。
- en: Since then, many more developments on NLP have been made and many more areas
    of research have emerged. However, the actual text resulting from NLP models has
    always been quite limited and didn’t have many real-world applications. That is,
    until the early 2000s. After this period developments in NLP made big leaps every
    few years which led to where we are now.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 自那时起，NLP（自然语言处理）领域取得了许多进展，研究领域也不断扩展。然而，NLP模型生成的实际文本一直相当有限，且缺乏许多现实世界的应用。直到2000年代初期，NLP的发展才迎来了每隔几年便有显著突破的阶段，这才导致了我们现在的情况。
- en: The problem, the dataset and question answering
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题、数据集和问答
- en: Now that I’ve given a short refresher on NLP it is time to introduce the actual
    problem that I have been working on. In short, my goal was to train a Dutch question
    answering machine learning model. However, the lack of suitable datasets made
    this quite difficult which is I created my own by using translation. In this article
    I will go through the creation of a dataset and the training of the machine learning
    model step by step so you can follow along and either replicate the entire solution
    or select the parts that are of importance to you.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我已经简要回顾了NLP的背景，是时候介绍我一直在研究的实际问题了。简而言之，我的目标是训练一个荷兰语问答的机器学习模型。然而，由于缺乏合适的数据集，这变得相当困难，因此我通过翻译创建了自己的数据集。在本文中，我将逐步讲解数据集的创建和机器学习模型的训练，以便你可以跟随并复制整个解决方案，或选择对你来说重要的部分。
- en: This article can be split into two main components. The first one being the
    creation of a Dutch dataset and the second being the training of a question answering
    machine learning model. In this chapter I will give some background information
    on them, introduce my solutions and explain my choices.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本文可以分为两个主要部分。第一个是荷兰语数据集的创建，第二个是问答机器学习模型的训练。在这一章中，我将提供一些背景信息，介绍我的解决方案并解释我的选择。
- en: The dataset
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: If we want to find a useful Dutch dataset it is important to look at what is
    exactly needed to train a question answering model. There are two main approaches
    to the generation of answers to questions. The first one being extractive and
    the second one being abstractive.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想找到一个有用的荷兰语数据集，那么了解训练一个问答模型所需的具体内容是很重要的。生成答案的主要有两种方法：第一种是抽取式，第二种是生成式。
- en: · **Extractive** question answering models are trained to extract an answer
    from the context (the source text) [7]. Older approaches used to do this by training
    a model to output a start and end index of the location of the answer in the context.
    However, the introduction of Transformers has made this approach obsolete.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: · **抽取式**问答模型被训练以从上下文（源文本）中提取答案[7]。较早的方法通过训练一个模型来输出答案在上下文中的起始和结束索引来实现这一点。然而，Transformer的引入使这种方法已经过时。
- en: · **Abstractive** question answering models are trained to generate new text
    based on the context and the question [8].
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: · **生成式**问答模型被训练以根据上下文和问题生成新文本[8]。
- en: Figure 1 shows an example of the output extractive and abstractive models might
    give.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图1展示了抽取式和生成式模型可能产生的输出示例。
- en: Although different approaches are possible, nowadays both extractive and abstractive
    question answering models are often based on Transformers like BERT [8], [9].
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有不同的方法，但如今抽取式和生成式问答模型通常都基于像BERT这样的Transformer[8]，[9]。
- en: '![](../Images/0682a79c2ea32064801f8556b954126a.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0682a79c2ea32064801f8556b954126a.png)'
- en: Figure 1\. An example of an answer generated in an extractive versus abstractive
    way.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 抽取式与生成式方式生成的答案示例。
- en: Based on the information about extractive and abstractive models, we now know
    that we need a dataset with contexts, questions, answers and, optionally, start
    and end indices of the location of the answer in the context. I have explored
    the following options in order to find a suitable dataset.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 基于关于抽取式和生成式模型的信息，我们现在知道我们需要一个包含上下文、问题、答案以及（可选的）答案在上下文中的起始和结束索引的数据集。我已经探索了以下选项，以寻找合适的数据集。
- en: I have used A 2020 paper by Cambazoglu *et al*. [10] to get a clear image of
    what datasets are available for question answering. Their research has resulted
    in a table with the most prominent question answering datasets. Unfortunately,
    none of these big datasets are in the Dutch language.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我使用了Cambazoglu *et al* 的2020年论文[10]，以获得有关问答数据集的清晰图像。他们的研究结果提供了一张包含最显著问答数据集的表格。不幸的是，这些大型数据集中没有荷兰语的数据集。
- en: Another option was Huggingface which hosts a large collection of datasets [11].
    At first glance, there are a few question answering datasets available for the
    Dutch language. However, further inspection shows that these datasets are often
    incomplete, include website domains instead of contexts or are a mix of various
    languages. These are completely unusable or too incomplete to be used for our
    goal.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个选择是Huggingface，它托管了大量的数据集[11]。乍一看，有一些荷兰语的问答数据集。然而，进一步检查显示，这些数据集往往不完整，包含网站域名而不是上下文，或者是各种语言的混合。这些数据集完全无法使用，或者不够完整，无法用于我们的目标。
- en: Concluding from these observations, there are practically no public datasets
    that can be used to train a Dutch question answering model. Creating our own dataset
    manually would take far too much time so what other options do we have? Firstly
    we could simply use an English model, translate the input from Dutch to English
    and then translate the output back to Dutch. However, a quick test with Google
    Translate and this approach has shown that the results are far from desirable
    and almost feel passive aggressive. Perhaps too much information and context got
    lost during the double translation step? That leads to the second option, translating
    the entire dataset and training on it. During my research I have come across a
    few instances where this was mentioned. For example a post by Zoumana Keita on
    Towardsdatascience [16] uses translation for data augmentation. Chapter three
    will dive into my execution of the translation of a dataset.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些观察结果来看，几乎没有公共数据集可以用来训练荷兰语问答模型。手动创建我们自己的数据集将花费太多时间，那么我们还有什么其他选项？首先，我们可以简单地使用一个英语模型，将荷兰语输入翻译成英语，然后将输出再翻译回荷兰语。然而，通过Google翻译进行的快速测试表明，这种方法的结果远非理想，几乎感觉有些消极攻击。也许在双重翻译步骤中丢失了太多信息和上下文？这就引出了第二个选项，即翻译整个数据集并在其上进行训练。在我的研究中，我遇到了一些提到这一点的实例。例如，Zoumana
    Keita在Towardsdatascience上的一篇文章[16]使用翻译进行数据增强。第三章将深入探讨我如何执行数据集的翻译。
- en: Lastly we need to select what dataset to use for our translation approach. Since
    we decided to translate the entire dataset, it does not matter what language the
    original dataset is in. The [Stanford Question Answering Dataset](https://rajpurkar.github.io/SQuAD-explorer/)
    (SQuAD) [12] seems to be quite popular and is used by Paperswithcode for the question
    answering benchmark [13]. It also contains a large amount (100.000+) of questions
    with answers and upon closer inspection does not seem to have any unexpected data.
    This is the dataset we will be working with.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要选择用于翻译的方法的数据集。既然我们决定翻译整个数据集，那么原始数据集使用什么语言就不重要了。[斯坦福问答数据集](https://rajpurkar.github.io/SQuAD-explorer/)（SQuAD）[12]
    似乎相当受欢迎，并被 Paperswithcode 用于问答基准测试[13]。它还包含大量（100,000+）的问答，并且经仔细检查后似乎没有任何意外数据。这就是我们将要使用的数据集。
- en: The machine learning model
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习模型
- en: Now we have determined how we are going to get a dataset; we need to decide
    what kind of machine learning model will be suitable for the goal of answering
    questions. In the previous chapter we have established that we can choose between
    an extractive model and an abstractive model. In my research I have used an abstractive
    model because it is based on a newer technology and gives more interesting results.
    However, just in case anyone wants to take this approach for an extractive model
    I will cover that as well. This is also in line with the selection of the dataset
    since it contains the start indices of answers.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确定了如何获取数据集；我们需要决定哪种机器学习模型适合回答问题的目标。在前一章中，我们已经确定可以选择抽取式模型和生成式模型。在我的研究中，我使用了生成式模型，因为它基于较新的技术，并且给出了更有趣的结果。然而，以防有人希望采用抽取式模型，我也会对此进行介绍。这也与数据集的选择一致，因为它包含了答案的起始索引。
- en: Training a Transformer from scratch would be, to say the least, inefficient.
    The book transfer Learning for Natural Language Processing by P. Azunre [14] goes
    in-depth on why transfer learning is done and shows a number of examples on how
    to do it. A large number of big NLP models are hosted on Huggingface [15] and
    are available for transfer learning. I have chosen the t5-v1_1-base model because
    it is multi-task trained on multiple languages. Chapter 4 will cover the transfer
    learning of this model.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从头开始训练一个 Transformer 模型，至少可以说是低效的。P. Azunre 的《自然语言处理中的迁移学习》一书[14]深入探讨了为什么进行迁移学习，并展示了如何进行迁移学习的多个示例。大量大型
    NLP 模型托管在 Huggingface[15]上，并可用于迁移学习。我选择了 t5-v1_1-base 模型，因为它经过多语言的多任务训练。第 4 章将介绍该模型的迁移学习。
- en: Translating the dataset
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 翻译数据集
- en: In this chapter I will be showing how I have translated the dataset by giving
    snippets of code and explaining them. The code resulting from these code blocks
    in succession is the entire dataset translation script I’ve written. Feel free
    to follow along or take specific parts that are of use to you.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将展示如何通过提供代码片段并对其进行解释来翻译数据集。这些代码块连续生成的代码就是我编写的整个数据集翻译脚本。欢迎跟随或取用对你有用的特定部分。
- en: '*Imports*'
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*导入*'
- en: The solution uses a few modules. First of all, we need to translate text in
    a way that is as fast as possible. In my research I have tried using various translation
    AI models from Huggingface but by far the fastest translator was the Googletrans
    module which makes use of the Google Translate API. The solution also uses Timeout
    from httpx to define a timeout for the translations, json for SQuAD dataset parsing,
    Pandas for dataframes and Time to measure how long everything is taking.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案使用了几个模块。首先，我们需要以尽可能快的速度翻译文本。在我的研究中，我尝试使用来自 Huggingface 的各种翻译 AI 模型，但迄今为止，最快的翻译器是
    Googletrans 模块，它使用了 Google Translate API。该解决方案还使用了 httpx 的 Timeout 来定义翻译的超时时间，使用
    json 解析 SQuAD 数据集，使用 Pandas 处理数据框，以及使用 Time 来测量所有操作所需的时间。
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Initialization**'
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**初始化**'
- en: First of all we should define a few constants that will be used throughout the
    script. For ease-of-access I have added the source language and translation language
    here.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们应该定义几个在脚本中会用到的常量。为了方便访问，我在这里添加了源语言和翻译语言。
- en: The Googletrans module provides us with a Translator that can have a custom
    timout defined. I have used a relatively long timeout because translations kept
    timing out during my tests. I will provide a bit more information on this issue
    further along in the guide.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Googletrans 模块为我们提供了一个可以自定义超时时间的翻译器。我使用了相对较长的超时时间，因为在测试期间翻译经常超时。我将在本指南后面的部分提供更多有关这个问题的信息。
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Reading the SQuAD dataset
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 阅读 SQuAD 数据集
- en: The following code extracts contexts, questions and answers from the train and
    validation json files. This is done by reading the files as json format and looping
    through the data inside in a way that extracts the three lists. For each question
    and answer, the context is copied and added to the contexts list. This way we
    can easily access a question with its relevant context and answer by using an
    index.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码从训练和验证 json 文件中提取上下文、问题和答案。这是通过将文件以 json 格式读取，并以一种提取三种列表的方式遍历数据来完成的。对于每个问题和答案，上下文被复制并添加到上下文列表中。这样我们可以通过使用索引轻松访问带有相关上下文和答案的问题。
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Timing
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间
- en: The following code provides us with a very rough estimation of how long each
    translation takes.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码为我们提供了每个翻译所需时间的大致估计。
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Translating
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 翻译
- en: Remember how I mentioned translations timing out? During my research I kept
    bumping into the issue where translations were timing out and the resulting dataset
    got corrupted. It turns out that the Googletrans module is not 100% reliable since
    it uses the Google Translate API. The way I have found around this is to create
    a small wrapper function that keeps trying to translate until it has been successful.
    After doing this I no longer experienced the timing out problem.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 记得我提到过翻译超时的问题吗？在我的研究过程中，我不断遇到翻译超时的问题，导致结果数据集被损坏。事实证明，Googletrans 模块并不是 100%
    可靠的，因为它使用了 Google Translate API。我找到的解决办法是创建一个小的包装函数，该函数会不断尝试翻译，直到成功为止。经过这样处理后，我不再遇到超时问题。
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Because of the way we have extracted contexts from the dataset, they have been
    duplicated for each question and answer pair. Simply translating all contexts
    would be redundant and very slow so the following translation function compares
    the previous context to the current one first. If they match, the previous translation
    is used.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们从数据集中提取上下文的方式，每个问题和答案对都有重复的上下文。直接翻译所有上下文会显得冗余且非常缓慢，因此以下翻译函数首先会将前一个上下文与当前上下文进行比较。如果它们匹配，则使用之前的翻译。
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Translating the questions and answers is pretty straightforward since we just
    need to loop through the lists to translate all of them.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 翻译问题和答案非常简单，因为我们只需循环遍历列表来翻译所有内容。
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now we can use the functions we have defined to translate all parts of the dataset.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用我们定义的函数来翻译数据集的所有部分。
- en: '[PRE7]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Exporting
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导出
- en: All that is left is exporting the translations for later use. We can do this
    by converting the lists to dataframes and then using the to_csv function. One
    thing to keep in mind is that the Googletrans module outputs translations with
    characters that are not included in utf-8 encoding. That is why we use utf-16
    encoding here. It would make sense to convert it to utf-8 at some point since
    that might be more useful in an AI model. However, since we are just working on
    the dataset here we can decide to leave that step for later when we are doing
    the data preprocessing for training our model.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 只剩下将翻译导出以供以后使用。我们可以通过将列表转换为数据框，然后使用 to_csv 函数来完成这一点。需要注意的是，Googletrans 模块输出的翻译包含
    utf-8 编码中不包含的字符。这就是我们在这里使用 utf-16 编码的原因。将其转换为 utf-8 可能在某些时候更有用，因为这可能对 AI 模型更有帮助。然而，由于我们这里只是在处理数据集，所以我们可以决定将这一步骤留到后续数据预处理阶段。
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Building a question answering model
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建问答模型
- en: Figuring out how to train a question answering model turned out to be a bit
    of a challenge. However, by taking inspiration from a Notebook by P. Suraj [17],
    I was able to create a Transformer based model that can be trained on question
    answering. In line with the Notebook I have used Torch to create the model.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 发现如何训练一个问答模型的过程有点挑战。然而，通过借鉴 P. Suraj [17] 的 Notebook，我能够创建一个基于 Transformer 的模型，该模型可以用于问答训练。按照
    Notebook 的指导，我使用了 Torch 来创建模型。
- en: Imports
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导入
- en: Starting with the imports, the following modules are used. We also define some
    variables that define the max in- and output length of the model.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 从导入开始，使用了以下模块。我们还定义了一些变量，这些变量定义了模型的最大输入和输出长度。
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Loading data
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载数据
- en: Now we can load the dataset that we have previously created. Since we used Pandas
    to export a csv we can now easily load it and convert it to an array. I have also
    defined a function that will be used later on to convert any training or input
    data to utf-8 which is the format we will train the model on.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以加载之前创建的数据集。由于我们使用 Pandas 导出了 csv，因此现在可以轻松加载并将其转换为数组。我还定义了一个函数，该函数将在后续将任何训练或输入数据转换为
    utf-8，这是我们将用于训练模型的格式。
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now we can actually load the data. For the training of the model I only used
    the train data and split this with a test size of 0.2.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以实际加载数据。在模型训练中，我只使用了训练数据，并将其拆分为测试数据，测试数据大小为 0.2。
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Preparing data
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备数据
- en: Like I mentioned before, it is possible to train an extractive model and an
    abstractive model. During my research I developed both an extractive and an abstractive
    model. In this article I just cover the abstractive version but, for anyone interested,
    I will also explain how I preprocessed my data for the extractive model. This
    was necessary to create the start- and endindices of the answers in contexts.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如我之前提到的，我们可以训练一个抽取式模型和一个抽象生成模型。在我的研究中，我开发了这两种模型。在这篇文章中，我只介绍抽象生成版本，但对于感兴趣的读者，我还会解释如何为抽取式模型预处理数据。这是为了创建上下文中答案的起始和结束索引。
- en: '**Abstractive**'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**抽象生成**'
- en: The dataset does not need much preprocessing in order to train an abstractive
    model. We simply convert all train data to utf-8\. The last three lines can be
    uncommented to decrease the size of the trainset, this will improve training time
    and might help with debugging.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集不需要过多预处理就可以训练抽象生成模型。我们只需将所有训练数据转换为 utf-8。可以取消注释最后三行，以减少训练集的大小，这将改善训练时间并有助于调试。
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Extractive**'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**抽取式**'
- en: In many cases, extractive models need start and end indices of the answer in
    the context. However, since we translated our dataset using a Transformer a few
    issues can occur. For example, answers might be worded differently than in the
    context or the position or length of the answer might have changed. To solve this,
    we can try to find the answer in the context and, if the answer is found, add
    it to the cleaned answers. Because of this, we also have information about the
    start index and the end index is simply the start index plus the length of the
    answer.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，抽取式模型需要上下文中答案的起始和结束索引。然而，由于我们使用 Transformer 翻译了数据集，可能会出现一些问题。例如，答案可能与上下文中的措辞不同，或者答案的位置或长度可能已经改变。为了解决这个问题，我们可以尝试在上下文中找到答案，如果找到答案，则将其添加到清理后的答案中。因此，我们也获得了关于起始索引的信息，结束索引简单地是起始索引加上答案的长度。
- en: '[PRE13]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Tokenizer
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分词器
- en: The next step is tokenizing, since we are using t5-v1_1-base, we can simply
    import the tokenizer from Huggingface. Then we will tokenize the contexts with
    the questions so that the tokenizer will add them together with end of string
    tokens. We also specify the previously defined max_text_length. Lastly the tokenized
    answers are added to the encodings as the target.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是分词，因为我们使用的是 t5-v1_1-base，我们可以直接从 Huggingface 导入分词器。然后，我们将使用问题对上下文进行分词，以便分词器将它们与结束字符串标记一起添加。我们还指定了之前定义的
    max_text_length。最后，分词后的答案被添加到编码中作为目标。
- en: '[PRE14]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Dataloader
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据加载器
- en: We’ll use a Dataloader to train the PyTorch model as follows. In here the batch
    size can also be specified. The server I trained on had limited memory so I had
    to use a batch size of 2\. If possible, a bigger batch size would be preferable.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用数据加载器来训练 PyTorch 模型。这里还可以指定批量大小。我训练时的服务器内存有限，所以我不得不使用批量大小为 2。如果可能的话，使用更大的批量大小会更好。
- en: '[PRE15]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Training the model
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: The model we use is T5ForConditionalGeneration based on T5-v1_1-base. If CUDA
    is installed on the PC or server that is used for training, we can try to utilize
    it to significantly increase training speed. We also tell the model that we are
    going to train it.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的模型是基于 T5-v1_1-base 的 T5ForConditionalGeneration。如果用于训练的 PC 或服务器上安装了 CUDA，我们可以尝试利用它来显著提高训练速度。我们还告诉模型我们将对其进行训练。
- en: 'The optimizer we use is AdamW with a learning rate of 1e-4\. This is based
    on the T5 documentation [18] which mentions that it is a good value to use in
    our situation:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的优化器是 AdamW，学习率为 1e-4。这个选择基于 T5 文档[18]，文档中提到在我们的情况下这是一个合适的值：
- en: '*Typically, 1e-4 and 3e-4 work well for most problems (classification, summarization,
    translation, question answering, question generation).*'
  id: totrans-99
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*通常，1e-4 和 3e-4 对于大多数问题（分类、摘要、翻译、问答、问题生成）效果很好。*'
- en: Lastly we define a function that saves our model for later usage after it is
    done training.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义一个函数，在模型训练完成后将其保存以供以后使用。
- en: '[PRE16]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The actual training of the model will be done in three epochs, the Notebook
    I have used [17] and the T5 documentation both state that this is a good amount
    of epochs to train on. On my PC which has a RTX 3090 this would take about 24
    hours per epoch. The server I have used took advantage of an Nvidia Tesla T4 and
    took about 6 hours per epoch.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的实际训练将在三个时期内完成，我使用的Notebook [17] 和T5文档都表明这是一个不错的训练周期数。在我配备RTX 3090的PC上，这大约需要每个周期24小时。我使用的服务器利用了Nvidia
    Tesla T4，每个周期大约需要6小时。
- en: The Tqdm module is used for visual feedback on the training state. It provides
    us with data about the elapsed time and the estimated time training will take.
    The steps between the two commented arrows are important for our goal of question
    answering, this is where we define what input to give the model. The other steps
    taken in this code block are pretty straightforward for the training of a PyTorch
    model.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Tqdm模块用于对训练状态进行可视化反馈。它提供了关于已过时间和估计训练时间的数据。两个注释箭头之间的步骤对于我们的问答目标很重要，这里我们定义了给模型的输入。该代码块中的其他步骤对于PyTorch模型的训练相当直接。
- en: '[PRE17]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Results
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: If you have followed along, congratulations! You have created your own Dutch
    dataset and trained a Dutch question answering model! If you are like me, you
    probably can’t wait to try the model to see what results it gives. You can use
    the following code to evaluate the model. Interestingly enough, you might find
    that the model is not only capable of answering Dutch questions! It is also somewhat
    capable of answering questions in different (mostly Germanic) languages. This
    is most likely due to the fact that the original T5-v1_1-base model has been trained
    on four different languages.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你跟随完成了，恭喜你！你已经创建了自己的荷兰数据集并训练了一个荷兰问答模型！如果你和我一样，可能迫不及待想尝试一下模型的结果。你可以使用以下代码来评估模型。有趣的是，你可能会发现模型不仅能够回答荷兰语问题！它也有能力回答不同（主要是日耳曼语）的语言的问题。这很可能是因为原始T5-v1_1-base模型已经在四种不同语言上进行了训练。
- en: '[PRE18]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here are some example contexts and questions together with the answers that
    have been generated by the model:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些示例背景和问题以及模型生成的答案：
- en: '**Context** We zijn met de klas van de master Applied Artificial Intelligence
    naar keulen geweest.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**背景** 我们和应用人工智能硕士班的同学们去过科隆。'
- en: '**Question** Waar is de klas heen geweest?'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题** 班级去过哪里？'
- en: '**Answer** Keulen'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案** 科隆'
- en: '**Context** De grote bruine vos springt over de luie hond heen.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**背景** 大棕色狐狸跳过了懒狗。'
- en: '**Question** Waar springt de vos overheen?'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题** 狐狸跳过了什么？'
- en: '**Answer** Luie hond'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案** 懒狗'
- en: '**Context** The big brown fox jumps over the lazy dog.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**背景** 大棕色狐狸跳过了懒狗。'
- en: '**Question** What does the fox do?'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题** 狐狸做了什么？'
- en: '**Answer** Jumps over the lazy dog'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案** 跳过懒狗'
- en: '**Context** Twee maal twee is tien.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**背景** 两乘二是十。'
- en: '**Question** Wat is twee maal twee?'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题** 两乘二是多少？'
- en: '**Answer** Tien'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案** 十'
- en: What has been achieved and what has not been achieved?
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 已经实现了什么，未实现什么？
- en: So, to summarize, we have selected an English dataset for question answering,
    translated it to Dutch using the Google Translate API and we have trained a PyTorch
    encoder-decoder model based on T5-v1_1-base. What exactly have we achieved with
    this and can this be used in real-life situations?
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们选择了一个用于问答的英文数据集，通过Google Translate API将其翻译成荷兰语，并训练了一个基于T5-v1_1-base的PyTorch编码器-解码器模型。我们究竟实现了什么，这在实际情况中是否能使用？
- en: First of all, it is important to realize that we have not properly evaluated
    the model as that was not part of the scope of this article. However, to be able
    to properly interpret our results and to be able to say something about its usability,
    I suggest looking into metrics like Rouge [19] or a human evaluation. The approach
    I have taken is a human evaluation. Table 2 shows the average rating between one
    and five that five people have given the generated answers of various context
    sources and questions. The average score is 2.96\. This number alone does not
    tell us much but we can conclude from the table that the model we created can
    in some cases generate near perfect answers. However, it does also quite often
    generate answers that the panel of human evaluators consider to be complete nonsense.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，重要的是要认识到我们没有对模型进行适当评估，因为这不是本文的范围。然而，为了能够正确解释我们的结果，并能够谈论其可用性，我建议查看如 Rouge
    [19] 等度量标准或进行人类评估。我采取的方法是人类评估。表 2 显示了五个人对各种上下文来源和问题生成答案的平均评分，评分范围从 1 到 5。平均分为
    2.96\。这个数字本身并没有告诉我们很多信息，但我们可以从表中得出结论，我们创建的模型在某些情况下可以生成接近完美的答案。然而，它也经常生成评估小组认为完全无意义的答案。
- en: '![](../Images/f55d4a916a6b078f6daa3f1443a294b5.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f55d4a916a6b078f6daa3f1443a294b5.png)'
- en: Table 2\. Human evaluation scores (1–5) of various articles, papers and theses.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. 各种文章、论文和学位论文的人类评估评分（1–5）。
- en: It is also important to note that, by translating a dataset, we have most likely
    introduced a bias. The AI behind Google Translate has been trained on a dataset
    which, since it is based on natural language, naturally contains a bias. By translating
    our data with it, this bias will be passed on to any model that’s trained with
    the dataset. Before a dataset created like this can be used in a real-life situation,
    it should be evaluated thoroughly to indicate what biases there are and how they
    impact the results.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要注意的是，通过翻译数据集，我们很可能引入了偏差。谷歌翻译背后的 AI 已经在一个数据集上进行训练，由于其基于自然语言，因此自然包含了偏差。通过翻译我们的数据，这种偏差将传递到任何使用该数据集训练的模型中。在像这样的数据集可以在实际情况下使用之前，应彻底评估，以指出其中的偏差以及这些偏差如何影响结果。
- en: However, this solution can be very interesting to people who are experimenting
    with AI, developing a new kind of machine learning model or are simply learning
    about NLP. It is a very accessible way to get a big dataset in any language for
    almost any NLP problem. Many students do not have access to big datasets because
    they are only accessible to big companies or are too expensive. With an approach
    like this, any big English dataset can be transformed into a dataset in a specific
    language.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种解决方案对于那些在实验 AI、开发新型机器学习模型或仅仅学习 NLP 的人来说可能非常有趣。这是一种非常便捷的方式，可以为几乎任何 NLP 问题获取大规模的数据集。许多学生无法获得大数据集，因为这些数据集通常只对大型公司开放或费用过高。通过这样的方式，任何大型英语数据集都可以转换为特定语言的数据集。
- en: Future plans
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 未来计划
- en: Personally I am interested in seeing where I can take this approach. I am currently
    working on a question generation model that is using exactly the same approach
    and dataset. I would like to investigate the usage of these two models combined
    so I can learn more about potential biases or errors that have been introduced.
    This is in line with chapter 5 in which I talked about the need for evaluation.
    I have created a human evaluation by asking five people to rate the results of
    the created model. However, I intend to learn more about different metrics which
    can hopefully tell me more about how the model works, why it generates certain
    results and what biases it contains.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我个人对这个方法的应用前景非常感兴趣。我目前正在研究一个使用完全相同方法和数据集的问题生成模型。我希望调查这两种模型结合使用的效果，以便更多地了解潜在的偏差或错误。这与第五章讨论的评估需求是一致的。我已经通过请五个人对创建的模型的结果进行评分来创建了一个人类评估。然而，我打算进一步了解不同的度量标准，这些标准可以更好地告诉我模型的工作原理、生成某些结果的原因以及其中包含的偏差。
- en: I have also learned that version 2.0 of the Stanford Question and Answer dataset
    includes questions that cannot be answered. Even though it is not directly related
    to the solution offered in this article, I am curious about the differences in
    results when I apply the solution of this article to the full SQuAD 2.0 dataset.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我还了解到，斯坦福问题与回答数据集的 2.0 版本包含一些无法回答的问题。虽然这与本文提供的解决方案没有直接关系，但我对将本文的解决方案应用于完整的 SQuAD
    2.0 数据集后的结果差异感到好奇。
- en: Sources
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 来源
- en: '[1] A. Vaswani *et al.*, “Attention Is All You Need,” 2017.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] A. Vaswani *et al.*，“注意力机制是你所需要的一切，” 2017 年。'
- en: '[2] D. Khurana, A. Koli, K. Khatter, and S. Singh, “Natural language processing:
    state of the art, current trends and challenges,” *Multimedia Tools and Applications*,
    Jul. 2022, doi: 10.1007/s11042–022–13428–4.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] D. Khurana, A. Koli, K. Khatter, 和 S. Singh，“自然语言处理：最新进展、当前趋势和挑战，” *Multimedia
    Tools and Applications*，2022 年 7 月，doi: 10.1007/s11042–022–13428–4。'
- en: '[3] “What is Natural Language Processing? | IBM,” [*www.ibm.com*.](http://www.ibm.com.)
    [https://www.ibm.com/topics/natural-language-processing](https://www.ibm.com/topics/natural-language-processing)
    (accessed Jan. 11, 2023).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] “什么是自然语言处理？| IBM，” [*www.ibm.com*](http://www.ibm.com.)。[https://www.ibm.com/topics/natural-language-processing](https://www.ibm.com/topics/natural-language-processing)（访问日期：2023
    年 1 月 11 日）。'
- en: '[4] E. Holloway, “Yes, ChatGPT Is Sentient — Because It’s Really Humans in
    the Loop,” *Mind Matters*, Dec. 26, 2022\. [https://mindmatters.ai/2022/12/yes-chatgpt-is-sentient-because-its-really-humans-in-the-loop/](https://mindmatters.ai/2022/12/yes-chatgpt-is-sentient-because-its-really-humans-in-the-loop/)
    (accessed Jan. 18, 2023).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] E. Holloway，“是的，ChatGPT 是有意识的 — 因为实际上是人类在其中，” *Mind Matters*，2022 年 12
    月 26 日。[https://mindmatters.ai/2022/12/yes-chatgpt-is-sentient-because-its-really-humans-in-the-loop/](https://mindmatters.ai/2022/12/yes-chatgpt-is-sentient-because-its-really-humans-in-the-loop/)（访问日期：2023
    年 1 月 18 日）。'
- en: '[5] “NLP — overview,” *cs.stanford.edu*. [https://cs.stanford.edu/people/eroberts/courses/soco/projects/2004-05/nlp/overview_history.html](https://cs.stanford.edu/people/eroberts/courses/soco/projects/2004-05/nlp/overview_history.html)
    (accessed Jan. 18, 2023).'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] “NLP — 概述，” *cs.stanford.edu*。[https://cs.stanford.edu/people/eroberts/courses/soco/projects/2004-05/nlp/overview_history.html](https://cs.stanford.edu/people/eroberts/courses/soco/projects/2004-05/nlp/overview_history.html)（访问日期：2023
    年 1 月 18 日）。'
- en: '[6] S. Ruder, “A Review of the Recent History of Natural Language Processing,”
    *Sebastian Ruder*, Oct. 01, 2018\. [https://ruder.io/a-review-of-the-recent-history-of-nlp/](https://ruder.io/a-review-of-the-recent-history-of-nlp/)
    (accessed Jan. 18, 2023).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] S. Ruder，“自然语言处理最近历史的回顾，” *Sebastian Ruder*，2018 年 10 月 1 日。[https://ruder.io/a-review-of-the-recent-history-of-nlp/](https://ruder.io/a-review-of-the-recent-history-of-nlp/)（访问日期：2023
    年 1 月 18 日）。'
- en: '[7] S. Varanasi, S. Amin, and G. Neumann, “AutoEQA: Auto-Encoding Questions
    for Extractive Question Answering,” *Findings of the Association for Computational
    Linguistics: EMNLP 2021*, 2021.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] S. Varanasi, S. Amin, 和 G. Neumann，“AutoEQA：用于提取式问答的自动编码问题，” *计算语言学协会年会论文集：EMNLP
    2021*，2021 年。'
- en: '[8] “What is Question Answering? — Hugging Face,” *huggingface.co*. [https://huggingface.co/tasks/question-answering](https://huggingface.co/tasks/question-answering)
    (accessed Jan. 18, 2023).'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] “什么是问答？ — Hugging Face，” *huggingface.co*。[https://huggingface.co/tasks/question-answering](https://huggingface.co/tasks/question-answering)（访问日期：2023
    年 1 月 18 日）。'
- en: '[9] R. E. López Condori and T. A. Salgueiro Pardo, “Opinion summarization methods:
    Comparing and extending extractive and abstractive approaches,” *Expert Systems
    with Applications*, vol. 78, pp. 124–134, Jul. 2017, doi: 10.1016/j.eswa.2017.02.006.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] R. E. López Condori 和 T. A. Salgueiro Pardo，“观点总结方法：比较和扩展提取式和抽象式方法，” *专家系统应用*，第
    78 卷，第 124–134 页，2017 年 7 月，doi: 10.1016/j.eswa.2017.02.006。'
- en: '[10] B. B. Cambazoglu, M. Sanderson, F. Scholer, and B. Croft, “A review of
    public datasets in question answering research,” *ACM SIGIR Forum*, vol. 54, no.
    2, pp. 1–23, Dec. 2020, doi: 10.1145/3483382.3483389.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] B. B. Cambazoglu, M. Sanderson, F. Scholer, 和 B. Croft，“关于问答研究的公共数据集综述，”
    *ACM SIGIR Forum*，第 54 卷，第 2 期，第 1–23 页，2020 年 12 月，doi: 10.1145/3483382.3483389。'
- en: '[11] “Hugging Face — The AI community building the future.,” *huggingface.co*.
    [https://huggingface.co/datasets?language=language:nl&task_categories=task_categories:question-answering&sort=downloads](https://huggingface.co/datasets?language=language%3Anl&task_categories=task_categories%3Aquestion-answering&sort=downloads)
    (accessed Jan. 18, 2023).'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] “Hugging Face — 建设未来的人工智能社区，” *huggingface.co*。[https://huggingface.co/datasets?language=language:nl&task_categories=task_categories:question-answering&sort=downloads](https://huggingface.co/datasets?language=language%3Anl&task_categories=task_categories%3Aquestion-answering&sort=downloads)（访问日期：2023
    年 1 月 18 日）。'
- en: '[12] “The Stanford Question Answering Dataset,” *rajpurkar.github.io*. [https://rajpurkar.github.io/SQuAD-explorer/](https://rajpurkar.github.io/SQuAD-explorer/)
    (accessed Jan. 18, 2023).'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] “斯坦福问答数据集，” *rajpurkar.github.io*。[https://rajpurkar.github.io/SQuAD-explorer/](https://rajpurkar.github.io/SQuAD-explorer/)（访问日期：2023
    年 1 月 18 日）。'
- en: '[13] “Papers with Code — Question Answering,” *paperswithcode.com*. [https://paperswithcode.com/task/question-answering](https://paperswithcode.com/task/question-answering)
    (accessed Jan. 18, 2023).'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] “Papers with Code — 问答，” *paperswithcode.com*。[https://paperswithcode.com/task/question-answering](https://paperswithcode.com/task/question-answering)（访问日期：2023
    年 1 月 18 日）。'
- en: '[14] P. Azunre, *Transfer Learning for Natural Language Processing*. Simon
    and Schuster, 2021.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] P. Azunre，*自然语言处理中的迁移学习*。Simon and Schuster，2021年。'
- en: '[15] “Hugging Face — On a mission to solve NLP, one commit at a time.,” *huggingface.co*.
    [https://huggingface.co/models](https://huggingface.co/models) (accessed Jan.
    18, 2023).'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] “Hugging Face — 一次提交解决NLP问题的使命。” *huggingface.co*。 [https://huggingface.co/models](https://huggingface.co/models)（访问日期：2023年1月18日）。'
- en: '[16] Z. Keita, “Data Augmentation in NLP Using Back Translation With MarianMT,”
    *Medium*, Nov. 05, 2022\. [https://towardsdatascience.com/data-augmentation-in-nlp-using-back-translation-with-marianmt-a8939dfea50a](/data-augmentation-in-nlp-using-back-translation-with-marianmt-a8939dfea50a)
    (accessed Jan. 18, 2023).'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Z. Keita，“使用MarianMT进行NLP中的数据增强，” *Medium*，2022年11月5日。 [https://towardsdatascience.com/data-augmentation-in-nlp-using-back-translation-with-marianmt-a8939dfea50a](/data-augmentation-in-nlp-using-back-translation-with-marianmt-a8939dfea50a)（访问日期：2023年1月18日）。'
- en: '[17] P. Suraj, “Google Colaboratory,” *colab.research.google.com*. [https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb)
    (accessed Jan. 25, 2023).'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] P. Suraj，“Google Colaboratory，” *colab.research.google.com*。 [https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb)（访问日期：2023年1月25日）。'
- en: '[18] “T5,” *huggingface.co*. [https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Model](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Model)
    (accessed Jan. 25, 2023).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] “T5，” *huggingface.co*。 [https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Model](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Model)（访问日期：2023年1月25日）。'
- en: '[19] “ROUGE — a Hugging Face Space by evaluate-metric,” *huggingface.co*. [https://huggingface.co/spaces/evaluate-metric/rouge](https://huggingface.co/spaces/evaluate-metric/rouge)
    (accessed Jan. 25, 2023).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] “ROUGE — evaluate-metric提供的Hugging Face空间，” *huggingface.co*。 [https://huggingface.co/spaces/evaluate-metric/rouge](https://huggingface.co/spaces/evaluate-metric/rouge)（访问日期：2023年1月25日）。'
- en: All images unless otherwise noted are by the author.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图片均为作者所摄。
