- en: 'Learning Transformers Code First: Part 1 — The Setup'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变压器代码学习第一部分：设置
- en: 原文：[https://towardsdatascience.com/nanogpt-learning-transformers-code-first-part-1-f2044cf5bca0?source=collection_archive---------7-----------------------#2023-07-07](https://towardsdatascience.com/nanogpt-learning-transformers-code-first-part-1-f2044cf5bca0?source=collection_archive---------7-----------------------#2023-07-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/nanogpt-learning-transformers-code-first-part-1-f2044cf5bca0?source=collection_archive---------7-----------------------#2023-07-07](https://towardsdatascience.com/nanogpt-learning-transformers-code-first-part-1-f2044cf5bca0?source=collection_archive---------7-----------------------#2023-07-07)
- en: A 4 Part Exploration of Transformers Using nanoGPT as a Starting Point
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 nanoGPT 作为起点的变压器四部分探索
- en: '[](https://medium.com/@oaguy1?source=post_page-----f2044cf5bca0--------------------------------)[![Lily
    Hughes-Robinson](../Images/b610721a40e274e7fb81418395314ae3.png)](https://medium.com/@oaguy1?source=post_page-----f2044cf5bca0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f2044cf5bca0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f2044cf5bca0--------------------------------)
    [Lily Hughes-Robinson](https://medium.com/@oaguy1?source=post_page-----f2044cf5bca0--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@oaguy1?source=post_page-----f2044cf5bca0--------------------------------)[![Lily
    Hughes-Robinson](../Images/b610721a40e274e7fb81418395314ae3.png)](https://medium.com/@oaguy1?source=post_page-----f2044cf5bca0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f2044cf5bca0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f2044cf5bca0--------------------------------)
    [Lily Hughes-Robinson](https://medium.com/@oaguy1?source=post_page-----f2044cf5bca0--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5389e25ca1bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnanogpt-learning-transformers-code-first-part-1-f2044cf5bca0&user=Lily+Hughes-Robinson&userId=5389e25ca1bb&source=post_page-5389e25ca1bb----f2044cf5bca0---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f2044cf5bca0--------------------------------)
    ·8 min read·Jul 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff2044cf5bca0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnanogpt-learning-transformers-code-first-part-1-f2044cf5bca0&user=Lily+Hughes-Robinson&userId=5389e25ca1bb&source=-----f2044cf5bca0---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5389e25ca1bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnanogpt-learning-transformers-code-first-part-1-f2044cf5bca0&user=Lily+Hughes-Robinson&userId=5389e25ca1bb&source=post_page-5389e25ca1bb----f2044cf5bca0---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f2044cf5bca0--------------------------------)
    ·8 分钟阅读·2023年7月7日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff2044cf5bca0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnanogpt-learning-transformers-code-first-part-1-f2044cf5bca0&user=Lily+Hughes-Robinson&userId=5389e25ca1bb&source=-----f2044cf5bca0---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff2044cf5bca0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnanogpt-learning-transformers-code-first-part-1-f2044cf5bca0&source=-----f2044cf5bca0---------------------bookmark_footer-----------)![](../Images/dfca366786f1d736d9f6aebbeedc4b65.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff2044cf5bca0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnanogpt-learning-transformers-code-first-part-1-f2044cf5bca0&source=-----f2044cf5bca0---------------------bookmark_footer-----------)![](../Images/dfca366786f1d736d9f6aebbeedc4b65.png)'
- en: Photo by [Josh Riemer](https://unsplash.com/@joshriemer?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Josh Riemer](https://unsplash.com/@joshriemer?utm_source=medium&utm_medium=referral)
    提供，来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: I don’t know about you, but sometime looking at code is easier than reading
    papers. When I was working on [AdventureGPT](https://github.com/oaguy1/AdventureGPT),
    I started by reading the source code to [BabyAGI](https://github.com/yoheinakajima/babyagi),
    an implementation of the [ReAct paper](https://arxiv.org/abs/2210.03629) in around
    600 lines of python.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我不知道你怎样，但有时候看代码比读论文更容易。当我在研究 [AdventureGPT](https://github.com/oaguy1/AdventureGPT)
    时，我从阅读 [BabyAGI](https://github.com/yoheinakajima/babyagi) 的源代码开始，它是对 [ReAct 论文](https://arxiv.org/abs/2210.03629)
    的一个实现，约 600 行 Python 代码。
- en: Recently, I became aware of a recent paper called [TinyStories](https://arxiv.org/abs/2305.07759)
    through [episode 33](https://www.cognitiverevolution.ai/e33-the-tiny-model-revolution-with-ronen-eldan-and-yuanzhi-li-of-microsoft-research/)
    of the excellent [Cognitive Revolution Podcast](https://www.cognitiverevolution.ai/).
    TinyStories attempts to show that models trained on millions (not billions) of
    parameters can be effective with high-enough quality data. In the case of the
    Microsoft researchers in the paper, they utilized synthetic data generated from
    GPT-3.5 and GPT-4 that would have cost around $10k retail to generate. The dataset
    and models are available from the author’s [HuggingFace repo](https://huggingface.co/roneneldan).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，我通过优秀的 [Cognitive Revolution Podcast](https://www.cognitiverevolution.ai/)
    的 [第33期](https://www.cognitiverevolution.ai/e33-the-tiny-model-revolution-with-ronen-eldan-and-yuanzhi-li-of-microsoft-research/)
    了解到了一篇名为 [TinyStories](https://arxiv.org/abs/2305.07759) 的最新论文。TinyStories 尝试展示在参数达到百万级（而非亿级）的模型也可以在足够高质量的数据上表现有效。在论文中的微软研究人员的案例中，他们使用了由
    GPT-3.5 和 GPT-4 生成的合成数据，生成这些数据的零售价大约为 $10k。数据集和模型可以从作者的 [HuggingFace 仓库](https://huggingface.co/roneneldan)
    获取。
- en: I was captivated to hear that a model could be trained on 30M and fewer parameters.
    For reference, I am running all my model training and inference on a Lenovo Legion
    5 laptop with a GTX 1660 Ti. Even just for inference, most models with over 3B
    parameters are too large to run on my machine. I know there are cloud compute
    resources available for a price, but I am learning all this in my spare time and
    can really only afford the modest OpenAI bill I rack up…
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 听说一个模型可以在 30M 和更少的参数上进行训练，我感到非常着迷。作为参考，我在搭载 GTX 1660 Ti 的 Lenovo Legion 5 笔记本上进行所有模型训练和推理。即使仅仅是推理，大多数超过
    3B 参数的模型也太大，无法在我的机器上运行。我知道有云计算资源可以购买，但我在闲暇时间学习这些内容，真的只能负担得起我累积的 modest OpenAI
    账单……
