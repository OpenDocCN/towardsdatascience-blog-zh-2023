- en: 'Learning Transformers Code First: Part 1 — The Setup'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/nanogpt-learning-transformers-code-first-part-1-f2044cf5bca0?source=collection_archive---------7-----------------------#2023-07-07](https://towardsdatascience.com/nanogpt-learning-transformers-code-first-part-1-f2044cf5bca0?source=collection_archive---------7-----------------------#2023-07-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A 4 Part Exploration of Transformers Using nanoGPT as a Starting Point
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@oaguy1?source=post_page-----f2044cf5bca0--------------------------------)[![Lily
    Hughes-Robinson](../Images/b610721a40e274e7fb81418395314ae3.png)](https://medium.com/@oaguy1?source=post_page-----f2044cf5bca0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f2044cf5bca0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f2044cf5bca0--------------------------------)
    [Lily Hughes-Robinson](https://medium.com/@oaguy1?source=post_page-----f2044cf5bca0--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5389e25ca1bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnanogpt-learning-transformers-code-first-part-1-f2044cf5bca0&user=Lily+Hughes-Robinson&userId=5389e25ca1bb&source=post_page-5389e25ca1bb----f2044cf5bca0---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f2044cf5bca0--------------------------------)
    ·8 min read·Jul 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff2044cf5bca0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnanogpt-learning-transformers-code-first-part-1-f2044cf5bca0&user=Lily+Hughes-Robinson&userId=5389e25ca1bb&source=-----f2044cf5bca0---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff2044cf5bca0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnanogpt-learning-transformers-code-first-part-1-f2044cf5bca0&source=-----f2044cf5bca0---------------------bookmark_footer-----------)![](../Images/dfca366786f1d736d9f6aebbeedc4b65.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Josh Riemer](https://unsplash.com/@joshriemer?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: I don’t know about you, but sometime looking at code is easier than reading
    papers. When I was working on [AdventureGPT](https://github.com/oaguy1/AdventureGPT),
    I started by reading the source code to [BabyAGI](https://github.com/yoheinakajima/babyagi),
    an implementation of the [ReAct paper](https://arxiv.org/abs/2210.03629) in around
    600 lines of python.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, I became aware of a recent paper called [TinyStories](https://arxiv.org/abs/2305.07759)
    through [episode 33](https://www.cognitiverevolution.ai/e33-the-tiny-model-revolution-with-ronen-eldan-and-yuanzhi-li-of-microsoft-research/)
    of the excellent [Cognitive Revolution Podcast](https://www.cognitiverevolution.ai/).
    TinyStories attempts to show that models trained on millions (not billions) of
    parameters can be effective with high-enough quality data. In the case of the
    Microsoft researchers in the paper, they utilized synthetic data generated from
    GPT-3.5 and GPT-4 that would have cost around $10k retail to generate. The dataset
    and models are available from the author’s [HuggingFace repo](https://huggingface.co/roneneldan).
  prefs: []
  type: TYPE_NORMAL
- en: I was captivated to hear that a model could be trained on 30M and fewer parameters.
    For reference, I am running all my model training and inference on a Lenovo Legion
    5 laptop with a GTX 1660 Ti. Even just for inference, most models with over 3B
    parameters are too large to run on my machine. I know there are cloud compute
    resources available for a price, but I am learning all this in my spare time and
    can really only afford the modest OpenAI bill I rack up…
  prefs: []
  type: TYPE_NORMAL
