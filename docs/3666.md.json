["```py\npip install tensorflow\n```", "```py\npip3 install tensorflow\n```", "```py\nfrom environment import Environment\nfrom agent import Agent\nfrom experience_replay import ExperienceReplay\nimport time\n\nif __name__ == '__main__':\n\n    grid_size = 5\n\n    environment = Environment(grid_size=grid_size, render_on=True)\n    agent = Agent(grid_size=grid_size, epsilon=1, epsilon_decay=0.998, epsilon_end=0.01)\n    # agent.load(f'models/model_{grid_size}.h5')\n\n    experience_replay = ExperienceReplay(capacity=10000, batch_size=32)\n\n    # Number of episodes to run before training stops\n    episodes = 5000\n    # Max number of steps in each episode\n    max_steps = 200\n\n    for episode in range(episodes):\n\n        # Get the initial state of the environment and set done to False\n        state = environment.reset()\n\n        # Loop until the episode finishes\n        for step in range(max_steps):\n            print('Episode:', episode)\n            print('Step:', step)\n            print('Epsilon:', agent.epsilon)\n\n            # Get the action choice from the agents policy\n            action = agent.get_action(state)\n\n            # Take a step in the environment and save the experience\n            reward, next_state, done = environment.step(action)\n            experience_replay.add_experience(state, action, reward, next_state, done)\n\n            # If the experience replay has enough memory to provide a sample, train the agent\n            if experience_replay.can_provide_sample():\n                experiences = experience_replay.sample_batch()\n                agent.learn(experiences)\n\n            # Set the state to the next_state\n            state = next_state\n\n            if done:\n                break\n            # time.sleep(0.5)\n\n        agent.save(f'models/model_{grid_size}.h5')\n```", "```py\n[0, 1, 0, 0, 0]\n[0, 0, 0, 0, 0]\n[0, 0, 0, 0, 0]\n[0, 0, 0, -1, 0]\n[0, 0, 0, 0, 0]\n```", "```py\nimport numpy as np\n\nclass Environment:\n    def __init__(self, grid_size):\n        self.grid_size = grid_size\n        self.grid = []\n\n    def reset(self):\n        # Initialize the empty grid as a 2d list of 0s\n        self.grid = np.zeros((self.grid_size, self.grid_size))\n```", "```py\nimport random\n\ndef add_agent(self):\n    # Choose a random location\n    location = (random.randint(0, self.grid_size - 1), random.randint(0, self.grid_size - 1))\n\n    # Agent is represented by a 1\n    self.grid[location[0]][location[1]] = 1\n\n    return location\n\ndef add_goal(self):\n    # Choose a random location\n    location = (random.randint(0, self.grid_size - 1), random.randint(0, self.grid_size - 1))\n\n    # Get a random location until it is not occupied\n    while self.grid[location[0]][location[1]] == 1:\n        location = (random.randint(0, self.grid_size - 1), random.randint(0, self.grid_size - 1))\n\n    # Goal is represented by a -1\n    self.grid[location[0]][location[1]] = -1\n\n    return location\n```", "```py\ndef render(self):\n        # Convert to a list of ints to improve formatting\n        grid = self.grid.astype(int).tolist()\n\n        for row in grid:\n            print(row)\n        print('') # To add some space between renders for each step\n```", "```py\nimport numpy as np\nimport random\n\nclass Environment:\n    def __init__(self, grid_size):\n        self.grid_size = grid_size\n        self.grid = []\n\n    def reset(self):\n        # Initialize the empty grid as a 2d array of 0s\n        self.grid = np.zeros((self.grid_size, self.grid_size))\n\n    def add_agent(self):\n        # Choose a random location\n        location = (random.randint(0, self.grid_size - 1), random.randint(0, self.grid_size - 1))\n\n        # Agent is represented by a 1\n        self.grid[location[0]][location[1]] = 1\n\n        return location\n\n    def add_goal(self):\n        # Choose a random location\n        location = (random.randint(0, self.grid_size - 1), random.randint(0, self.grid_size - 1))\n\n        # Get a random location until it is not occupied\n        while self.grid[location[0]][location[1]] == 1:\n            location = (random.randint(0, self.grid_size - 1), random.randint(0, self.grid_size - 1))\n\n        # Goal is represented by a -1\n        self.grid[location[0]][location[1]] = -1\n\n        return location\n\n    def render(self):\n        # Convert to a list of ints to improve formatting\n        grid = self.grid.astype(int).tolist()\n\n        for row in grid:\n            print(row)\n        print('') # To add some space between renders for each step\n\n# Test Environment\nenv = Environment(5)\nenv.reset()\nagent_location = env.add_agent()\ngoal_location = env.add_goal()\nenv.render()\n\nprint(f'Agent Location: {agent_location}')\nprint(f'Goal Location: {goal_location}')\n```", "```py\n>>>\n[0, 0, 0, 0, 0]\n[0, 0, -1, 0, 0]\n[0, 0, 0, 0, 0]\n[0, 0, 0, 1, 0]\n[0, 0, 0, 0, 0]\n\nAgent Location: (3, 3)\nGoal Location: (1, 2)\n```", "```py\nclass Environment:\n    def __init__(self, grid_size, render_on=False):\n        self.grid_size = grid_size\n        self.grid = []\n        # Make sure to add the new attributes\n        self.render_on = render_on\n        self.agent_location = None\n        self.goal_location = None\n\n    def reset(self):\n        # Initialize the empty grid as a 2d array of 0s\n        self.grid = np.zeros((self.grid_size, self.grid_size))\n\n        # Add the agent and the goal to the grid\n        self.agent_location = self.add_agent()\n        self.goal_location = self.add_goal()\n\n        if self.render_on:\n            self.render()\n```", "```py\n...\n\n# Test Environment\nenv = Environment(5, render_on=True)\nenv.reset()\n\n# Now to access agent and goal location you can use Environment's attributes\nprint(f'Agent Location: {env.agent_location}')\nprint(f'Goal Location: {env.goal_location}')\n```", "```py\n>>>\n[0, 0, 0, 0, 0]\n[0, 0, 0, 0, 0]\n[0, 0, 0, 0, 0]\n[0, 0, 0, 0, -1]\n[1, 0, 0, 0, 0]\n\nAgent Location: (4, 0)\nGoal Location: (3, 4)\n```", "```py\ndef get_state(self):\n    # Flatten the grid from 2d to 1d\n    state = self.grid.flatten()\n    return state\n```", "```py\n[0, 0, 0, 0, 0]\n[0, 0, 0, 1, 0]\n[0, 0, 0, 0, 0]\n[0, 0, 0, 0, -1]\n[0, 0, 0, 0, 0]\n```", "```py\n[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0]\n```", "```py\ndef reset(self):\n    ...\n\n    # Return the initial state of the grid\n    return self.get_state()\n```", "```py\nimport random\n\nclass Environment:\n    def __init__(self, grid_size, render_on=False):\n        self.grid_size = grid_size\n        self.grid = []\n        self.render_on = render_on\n        self.agent_location = None\n        self.goal_location = None\n\n    def reset(self):\n        # Initialize the empty grid as a 2d array of 0s\n        self.grid = np.zeros((self.grid_size, self.grid_size))\n\n        # Add the agent and the goal to the grid\n        self.agent_location = self.add_agent()\n        self.goal_location = self.add_goal()\n\n        if self.render_on:\n            self.render()\n\n        # Return the initial state of the grid\n        return self.get_state()\n\n    def add_agent(self):\n        # Choose a random location\n        location = (random.randint(0, self.grid_size - 1), random.randint(0, self.grid_size - 1))\n\n        # Agent is represented by a 1\n        self.grid[location[0]][location[1]] = 1\n\n        return location\n\n    def add_goal(self):\n        # Choose a random location\n        location = (random.randint(0, self.grid_size - 1), random.randint(0, self.grid_size - 1))\n\n        # Get a random location until it is not occupied\n        while self.grid[location[0]][location[1]] == 1:\n            location = (random.randint(0, self.grid_size - 1), random.randint(0, self.grid_size - 1))\n\n        # Goal is represented by a -1\n        self.grid[location[0]][location[1]] = -1\n\n        return location\n\n    def render(self):\n        # Convert to a list of ints to improve formatting\n        grid = self.grid.astype(int).tolist()\n\n        for row in grid:\n            print(row)\n        print('') # To add some space between renders for each step\n\n    def get_state(self):\n        # Flatten the grid from 2d to 1d\n        state = self.grid.flatten()\n        return state\n```", "```py\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\n\nclass Agent:\n    def __init__(self, grid_size):\n        self.grid_size = grid_size\n        self.model = self.build_model()\n\n    def build_model(self):\n        # Create a sequential model with 3 layers\n        model = Sequential([\n            # Input layer expects a flattened grid, hence the input shape is grid_size squared\n            Dense(128, activation='relu', input_shape=(self.grid_size**2,)),\n            Dense(64, activation='relu'),\n            # Output layer with 4 units for the possible actions (up, down, left, right)\n            Dense(4, activation='linear')\n        ])\n\n        model.compile(optimizer='adam', loss='mse')\n\n        return model\n```", "```py\nimport numpy as np   \n\ndef get_action(self, state):\n    # Add an extra dimension to the state to create a batch with one instance\n    state = np.expand_dims(state, axis=0)\n\n    # Use the model to predict the Q-values (action values) for the given state\n    q_values = self.model.predict(state, verbose=0)\n\n    # Select and return the action with the highest Q-value\n    action = np.argmax(q_values[0]) # Take the action from the first (and only) entry\n\n    return action\n```", "```py\nstate = np.expand_dims(state, axis=0)\n```", "```py\n[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0]\n```", "```py\n[[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0]]\n```", "```py\n[[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0],\n [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n [0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n ...\n [0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n```", "```py\n...\n\n# Use the model to predict the Q-values (action values) for the given state\nq_values = self.model.predict(state, verbose=0)\n\n# Select and return the action with the highest Q-value\naction = np.argmax(q_values[0]) # Take the action from the first (and only) entry\n\n...\n```", "```py\nInitialize Values:\nepsilon = 1\nepsilon_decay = 0.998\n\n-----------------\n\nStep 1:\nepsilon = 1\n\nepsilon = 1 * 0.998 = 0.998\n\n-----------------\n\nStep 2:\nepsilon = 0.998\n\nepsilon = 0.998 * 0.998 = 0.996\n\n-----------------\n\nStep 3:\nepsilon = 0.996\n\nepsilon = 0.996 * 0.998 = 0.994\n\n-----------------\n\nStep 4:\nepsilon = 0.994\n\nepsilon = 0.994 * 0.998 = 0.992\n\n-----------------\n\n...\n\n-----------------\n\nStep 1000:\nepsilon = 1 * (0.998)^1000 = 0.135\n\n-----------------\n\n...and so on\n```", "```py\nimport numpy as np\n\nclass Agent:\n    def __init__(self, grid_size, epsilon=1, epsilon_decay=0.998, epsilon_end=0.01):\n        self.grid_size = grid_size\n        self.epsilon = epsilon\n        self.epsilon_decay = epsilon_decay\n        self.epsilon_end = epsilon_end\n        ...\n\n    ...\n\n    def get_action(self, state):\n\n        # rand() returns a random value between 0 and 1\n        if np.random.rand() <= self.epsilon:\n            # Exploration: random action\n            action = np.random.randint(0, 4)\n        else:\n            # Add an extra dimension to the state to create a batch with one instance\n            state = np.expand_dims(state, axis=0)\n\n            # Use the model to predict the Q-values (action values) for the given state\n            q_values = self.model.predict(state, verbose=0)\n\n            # Select and return the action with the highest Q-value\n            action = np.argmax(q_values[0]) # Take the action from the first (and only) entry\n\n        # Decay the epsilon value to reduce the exploration over time\n        if self.epsilon > self.epsilon_end:\n            self.epsilon *= self.epsilon_decay\n\n        return action\n```", "```py\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\nimport numpy as np\n\nclass Agent:\n    def __init__(self, grid_size, epsilon=1, epsilon_decay=0.998, epsilon_end=0.01):\n        self.grid_size = grid_size\n        self.epsilon = epsilon\n        self.epsilon_decay = epsilon_decay\n        self.epsilon_end = epsilon_end\n        self.model = self.build_model()\n\n    def build_model(self):\n        # Create a sequential model with 3 layers\n        model = Sequential([\n            # Input layer expects a flattened grid, hence the input shape is grid_size squared\n            Dense(128, activation='relu', input_shape=(self.grid_size**2,)),\n            Dense(64, activation='relu'),\n            # Output layer with 4 units for the possible actions (up, down, left, right)\n            Dense(4, activation='linear')\n        ])\n\n        model.compile(optimizer='adam', loss='mse')\n\n        return model\n\n    def get_action(self, state):\n\n        # rand() returns a random value between 0 and 1\n        if np.random.rand() <= self.epsilon:\n            # Exploration: random action\n            action = np.random.randint(0, 4)\n        else:\n            # Add an extra dimension to the state to create a batch with one instance\n            state = np.expand_dims(state, axis=0)\n\n            # Use the model to predict the Q-values (action values) for the given state\n            q_values = self.model.predict(state, verbose=0)\n\n            # Select and return the action with the highest Q-value\n            action = np.argmax(q_values[0]) # Take the action from the first (and only) entry\n\n        # Decay the epsilon value to reduce the exploration over time\n        if self.epsilon > self.epsilon_end:\n            self.epsilon *= self.epsilon_decay\n\n        return action\n```", "```py\n...\n\ndef move_agent(self, action):\n    # Map agent action to the correct movement\n    moves = {\n        0: (-1, 0), # Up\n        1: (1, 0),  # Down\n        2: (0, -1), # Left\n        3: (0, 1)   # Right\n    }\n\n    previous_location = self.agent_location\n\n    # Determine the new location after applying the action\n    move = moves[action]\n    new_location = (previous_location[0] + move[0], previous_location[1] + move[1])\n\n    # Check for a valid move\n    if self.is_valid_location(new_location):\n        # Remove agent from old location\n        self.grid[previous_location[0]][previous_location[1]] = 0\n\n        # Add agent to new location\n        self.grid[new_location[0]][new_location[1]] = 1\n\n        # Update agent's location\n        self.agent_location = new_location\n\ndef is_valid_location(self, location):\n    # Check if the location is within the boundaries of the grid\n    if (0 <= location[0] < self.grid_size) and (0 <= location[1] < self.grid_size):\n        return True\n    else:\n        return False\n```", "```py\n...\n\ndef move_agent(self, action):\n    ...\n    done = False  # The episode is not done by default\n\n    # Check for a valid move\n    if self.is_valid_location(new_location):\n        # Remove agent from old location\n        self.grid[previous_location[0]][previous_location[1]] = 0\n\n        # Add agent to new location\n        self.grid[new_location[0]][new_location[1]] = 1\n\n        # Update agent's location\n        self.agent_location = new_location\n\n        # Check if the new location is the reward location\n        if self.agent_location == self.goal_location:\n            # Episode is complete\n            done = True\n\n    return done\n\n...\n```", "```py\n...\n\ndef move_agent(self, action):\n    ...\n    done = False # The episode is not done by default\n    reward = 0   # Initialize reward\n\n    # Check for a valid move\n    if self.is_valid_location(new_location):\n        # Remove agent from old location\n        self.grid[previous_location[0]][previous_location[1]] = 0\n\n        # Add agent to new location\n        self.grid[new_location[0]][new_location[1]] = 1\n\n        # Update agent's location\n        self.agent_location = new_location\n\n        # Check if the new location is the reward location\n        if self.agent_location == self.goal_location:\n            # Reward for getting the goal\n            reward = 100\n\n            # Episode is complete\n            done = True\n        else:\n            # Small punishment for valid move that did not get the goal\n            reward = -1\n    else:\n        # Slightly larger punishment for an invalid move\n        reward = -3\n\n    return reward, done\n\n...\n```", "```py\nreward = -(np.abs(self.goal_location[0] - new_location[0]) + \\\n           np.abs(self.goal_location[1] - new_location[1]))\n```", "```py\n...\n\ndef move_agent(self, action):\n    ...\n        if self.agent_location == self.goal_location:\n            ...\n        else:\n            # Calculate the distance before the move\n            previous_distance = np.abs(self.goal_location[0] - previous_location[0]) + \\\n                                np.abs(self.goal_location[1] - previous_location[1])\n\n            # Calculate the distance after the move\n            new_distance = np.abs(self.goal_location[0] - new_location[0]) + \\\n                           np.abs(self.goal_location[1] - new_location[1])\n\n            # If new_location is closer to the goal, reward = 1, if further, reward = -1\n            reward = (previous_distance - new_distance)\n    ...\n```", "```py\ndef move_agent(self, action):\n    # Map agent action to the correct movement\n    moves = {\n        0: (-1, 0), # Up\n        1: (1, 0),  # Down\n        2: (0, -1), # Left\n        3: (0, 1)   # Right\n    }\n\n    previous_location = self.agent_location\n\n    # Determine the new location after applying the action\n    move = moves[action]\n    new_location = (previous_location[0] + move[0], previous_location[1] + move[1])\n\n    done = False # The episode is not done by default\n    reward = 0   # Initialize reward\n\n    # Check for a valid move\n    if self.is_valid_location(new_location):\n        # Remove agent from old location\n        self.grid[previous_location[0]][previous_location[1]] = 0\n\n        # Add agent to new location\n        self.grid[new_location[0]][new_location[1]] = 1\n\n        # Update agent's location\n        self.agent_location = new_location\n\n        # Check if the new location is the reward location\n        if self.agent_location == self.goal_location:\n            # Reward for getting the goal\n            reward = 100\n\n            # Episode is complete\n            done = True\n        else:\n            # Calculate the distance before the move\n            previous_distance = np.abs(self.goal_location[0] - previous_location[0]) + \\\n                                np.abs(self.goal_location[1] - previous_location[1])\n\n            # Calculate the distance after the move\n            new_distance = np.abs(self.goal_location[0] - new_location[0]) + \\\n                           np.abs(self.goal_location[1] - new_location[1])\n\n            # If new_location is closer to the goal, reward = 1, if further, reward = -1\n            reward = (previous_distance - new_distance)\n    else:\n        # Slightly larger punishment for an invalid move\n        reward = -3\n\n    return reward, done\n```", "```py\n...\n\n# If new_location is closer to the goal, reward = 0.9, if further, reward = -1.1\nreward = (previous_distance - new_distance) - 0.1\n\n...\n```", "```py\ndef step(self, action):\n    # Apply the action to the environment, record the observations\n    reward, done = self.move_agent(action)\n    next_state = self.get_state()\n\n    # Render the grid at each step\n    if self.render_on:\n        self.render()\n\n    return reward, next_state, done\n```", "```py\nimport random\nimport numpy as np\n\nclass Environment:\n    def __init__(self, grid_size, render_on=False):\n        self.grid_size = grid_size\n        self.render_on = render_on\n        self.grid = []\n        self.agent_location = None\n        self.goal_location = None\n\n    def reset(self):\n        # Initialize the empty grid as a 2d array of 0s\n        self.grid = np.zeros((self.grid_size, self.grid_size))\n\n        # Add the agent and the goal to the grid\n        self.agent_location = self.add_agent()\n        self.goal_location = self.add_goal()\n\n        # Render the initial grid\n        if self.render_on:\n            self.render()\n\n        # Return the initial state\n        return self.get_state()\n\n    def add_agent(self):\n        # Choose a random location\n        location = (random.randint(0, self.grid_size - 1), random.randint(0, self.grid_size - 1))\n\n        # Agent is represented by a 1\n        self.grid[location[0]][location[1]] = 1\n        return location\n\n    def add_goal(self):\n        # Choose a random location\n        location = (random.randint(0, self.grid_size - 1), random.randint(0, self.grid_size - 1))\n\n        # Get a random location until it is not occupied\n        while self.grid[location[0]][location[1]] == 1:\n            location = (random.randint(0, self.grid_size - 1), random.randint(0, self.grid_size - 1))\n\n        # Goal is represented by a -1\n        self.grid[location[0]][location[1]] = -1\n\n        return location\n\n    def move_agent(self, action):\n        # Map agent action to the correct movement\n        moves = {\n            0: (-1, 0), # Up\n            1: (1, 0),  # Down\n            2: (0, -1), # Left\n            3: (0, 1)   # Right\n        }\n\n        previous_location = self.agent_location\n\n        # Determine the new location after applying the action\n        move = moves[action]\n        new_location = (previous_location[0] + move[0], previous_location[1] + move[1])\n\n        done = False  # The episode is not done by default\n        reward = 0   # Initialize reward\n\n        # Check for a valid move\n        if self.is_valid_location(new_location):\n            # Remove agent from old location\n            self.grid[previous_location[0]][previous_location[1]] = 0\n\n            # Add agent to new location\n            self.grid[new_location[0]][new_location[1]] = 1\n\n            # Update agent's location\n            self.agent_location = new_location\n\n            # Check if the new location is the reward location\n            if self.agent_location == self.goal_location:\n                # Reward for getting the goal\n                reward = 100\n\n                # Episode is complete\n                done = True\n            else:\n                # Calculate the distance before the move\n                previous_distance = np.abs(self.goal_location[0] - previous_location[0]) + \\\n                                    np.abs(self.goal_location[1] - previous_location[1])\n\n                # Calculate the distance after the move\n                new_distance = np.abs(self.goal_location[0] - new_location[0]) + \\\n                               np.abs(self.goal_location[1] - new_location[1])\n\n                # If new_location is closer to the goal, reward = 0.9, if further, reward = -1.1\n                reward = (previous_distance - new_distance) - 0.1\n        else:\n            # Slightly larger punishment for an invalid move\n            reward = -3\n\n        return reward, done\n\n    def is_valid_location(self, location):\n        # Check if the location is within the boundaries of the grid\n        if (0 <= location[0] < self.grid_size) and (0 <= location[1] < self.grid_size):\n            return True\n        else:\n            return False\n\n    def get_state(self):\n        # Flatten the grid from 2d to 1d\n        state = self.grid.flatten()\n        return state\n\n    def render(self):\n        # Convert to a list of ints to improve formatting\n        grid = self.grid.astype(int).tolist()\n        for row in grid:\n            print(row)\n        print('') # To add some space between renders for each step\n\n    def step(self, action):\n        # Apply the action to the environment, record the observations\n        reward, done = self.move_agent(action)\n        next_state = self.get_state()\n\n        # Render the grid at each step\n        if self.render_on:\n            self.render()\n\n        return reward, next_state, done\n```", "```py\nfrom collections import deque, namedtuple\n\nclass ExperienceReplay:\n    def __init__(self, capacity, batch_size):\n        # Memory stores the experiences in a deque, so if capacity is exceeded it removes\n        # the oldest item efficiently\n        self.memory = deque(maxlen=capacity)\n\n        # Batch size specifices the amount of experiences that will be sampled at once\n        self.batch_size = batch_size\n\n        # Experience is a namedtuple that stores the relevant information for training\n        self.Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])\n```", "```py\nimport random\n\ndef add_experience(self, state, action, reward, next_state, done):\n    # Create a new experience and store it in memory\n    experience = self.Experience(state, action, reward, next_state, done)\n    self.memory.append(experience)\n\ndef sample_batch(self):\n    # Batch will be a random sample of experiences from memory of size batch_size\n    batch = random.sample(self.memory, self.batch_size)\n    return batch\n```", "```py\ndef can_provide_sample(self):\n    # Determines if the length of memory has exceeded batch_size\n    return len(self.memory) >= self.batch_size\n```", "```py\nimport random\nfrom collections import deque, namedtuple\n\nclass ExperienceReplay:\n    def __init__(self, capacity, batch_size):\n        # Memory stores the experiences in a deque, so if capacity is exceeded it removes\n        # the oldest item efficiently\n        self.memory = deque(maxlen=capacity)\n\n        # Batch size specifices the amount of experiences that will be sampled at once\n        self.batch_size = batch_size\n\n        # Experience is a namedtuple that stores the relevant information for training\n        self.Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])\n\n    def add_experience(self, state, action, reward, next_state, done):\n        # Create a new experience and store it in memory\n        experience = self.Experience(state, action, reward, next_state, done)\n        self.memory.append(experience)\n\n    def sample_batch(self):\n        # Batch will be a random sample of experiences from memory of size batch_size\n        batch = random.sample(self.memory, self.batch_size)\n        return batch\n\n    def can_provide_sample(self):\n        # Determines if the length of memory has exceeded batch_size\n        return len(self.memory) >= self.batch_size\n```", "```py\nimport numpy as np\n\ndef learn(self, experiences):\n    states = np.array([experience.state for experience in experiences])\n    actions = np.array([experience.action for experience in experiences])\n    rewards = np.array([experience.reward for experience in experiences])\n    next_states = np.array([experience.next_state for experience in experiences])\n    dones = np.array([experience.done for experience in experiences])\n\n    # Predict the Q-values (action values) for the given state batch\n    current_q_values = self.model.predict(states, verbose=0)\n\n    # Predict the Q-values for the next_state batch\n    next_q_values = self.model.predict(next_states, verbose=0)\n    ...\n```", "```py\nimport numpy as np\n...\n\nclass Agent:\n    def __init__(self, grid_size, epsilon=1, epsilon_decay=0.995, epsilon_end=0.01, gamma=0.99):\n        ...\n        self.gamma = gamma\n        ...\n    ...\n\n    def learn(self, experiences):\n        ...\n\n        # Initialize the target Q-values as the current Q-values\n        target_q_values = current_q_values.copy()\n\n        # Loop through each experience in the batch\n        for i in range(len(experiences)):\n            if dones[i]:\n                # If the episode is done, there is no next Q-value\n                # [i, actions[i]] is the numpy equivalent of [i][actions[i]]\n                target_q_values[i, actions[i]] = rewards[i]\n            else:\n                # The updated Q-value is the reward plus the discounted max Q-value for the next state\n                # [i, actions[i]] is the numpy equivalent of [i][actions[i]]\n                target_q_values[i, actions[i]] = rewards[i] + self.gamma * np.max(next_q_values[i])\n        ...\n```", "```py\ntarget_q_values[i, actions[i]] = rewards[i]\n```", "```py\ntarget_q_values[i, actions[i]] = rewards[i] + self.gamma * np.max(next_q_values[i])\n```", "```py\n Experience in batch   Reward from environment\n                v                    v\ntarget_q_values[i, actions[i]] = rewards[i]\n                       ^\n           Index of the action chosen\n```", "```py\ngamma = 0.99\nactions = [1, 2, 2]  # (down, left, left)\nrewards = [1, -1, 100] # Rewards given by the environment for the action\ndones = [False, False, True] # Indicating whether the episode is complete\n\ncurrent_q_values = [\n    [2, 5, -2, -3],  # In this state, action 2 (index 1) is best so far\n    [1, 3, 4, -1],   # Here, action 3 (index 2) is currently favored\n    [-3, 2, 6, 1]    # Action 3 (index 2) has the highest Q-value in this state\n]\n\nnext_q_values = [\n    [1, 4, -1, -2],  # Future Q-values after taking each action from the first state\n    [2, 2, 5, 0],    # Future Q-values from the second state\n    [-2, 3, 7, 2]    # Future Q-values from the third state\n]\n```", "```py\ntarget_q_values = current_q_values\n```", "```py\ni = 0 # This is the first entry in the batch (first loop)\n\n# First entries of associated values\nactions[i] = 1\nrewards[i] = 1\ndones[i] = False\ntarget_q_values[i] = [2, 5, -2, -3]\nnext_q_values[i] = [1, 4, -1, -2]\n```", "```py\ntarget_q_values[i, actions[i]] = rewards[i] + 0.99 * max(next_q_values[i])\n```", "```py\n# Updated target_q_values[i]\ntarget_q_values[i] = [2, 4.96, -2, -3]\n                ^          ^\n              i = 0    action[i] = 1\n```", "```py\ni = 1 # This is the second entry in the batch\n\n# Second entries of associated values\nactions[i] = 2\nrewards[i] = -1\ndones[i] = False\ntarget_q_values[i] = [1, 3, 4, -1]\nnext_q_values[i] = [2, 2, 5, 0]\n```", "```py\ntarget_q_values[i, actions[i]] = rewards[i] + 0.99 * max(next_q_values[i])\n```", "```py\n# Updated target_q_values[i]\ntarget_q_values[i] = [1, 3, 3.95, -1]\n                ^             ^\n              i = 1      action[i] = 2\n```", "```py\ni = 2 # This is the third and final entry in the batch\n\n# Second entries of associated values\nactions[i] = 2\nrewards[i] = 100\ndones[i] = True\ntarget_q_values[i] = [-3, 2, 6, 1]\nnext_q_values[i] = [-2, 3, 7, 2]\n```", "```py\ntarget_q_values[i, actions[i]] = rewards[i]\n```", "```py\n# Updated target_q_values[i]\ntarget_q_values[i] = [-3, 2, 100, 1]\n                ^             ^\n              i = 2       action[i] = 2\n```", "```py\n...\n\ndef learn(self, experiences):\n    states = np.array([experience.state for experience in experiences])\n    actions = np.array([experience.action for experience in experiences])\n    rewards = np.array([experience.reward for experience in experiences])\n    next_states = np.array([experience.next_state for experience in experiences])\n    dones = np.array([experience.done for experience in experiences])\n\n    # Predict the Q-values (action values) for the given state batch\n    current_q_values = self.model.predict(states, verbose=0)\n\n    # Predict the Q-values for the next_state batch\n    next_q_values = self.model.predict(next_states, verbose=0)\n\n    # Initialize the target Q-values as the current Q-values\n    target_q_values = current_q_values.copy()\n\n    # Loop through each experience in the batch\n    for i in range(len(experiences)):\n        if dones[i]:\n            # If the episode is done, there is no next Q-value\n            target_q_values[i, actions[i]] = rewards[i]\n        else:\n            # The updated Q-value is the reward plus the discounted max Q-value for the next state\n            # [i, actions[i]] is the numpy equivalent of [i][actions[i]]\n            target_q_values[i, actions[i]] = rewards[i] + self.gamma * np.max(next_q_values[i])\n\n    # Train the model\n    self.model.fit(states, target_q_values, epochs=1, verbose=0)\n```", "```py\nfrom tensorflow.keras.models import load_model\n\ndef load(self, file_path):\n    self.model = load_model(file_path)\n\ndef save(self, file_path):\n    self.model.save(file_path)\n```", "```py\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential, load_model\nimport numpy as np\n\nclass Agent:\n    def __init__(self, grid_size, epsilon=1, epsilon_decay=0.998, epsilon_end=0.01, gamma=0.99):\n        self.grid_size = grid_size\n        self.epsilon = epsilon\n        self.epsilon_decay = epsilon_decay\n        self.epsilon_end = epsilon_end\n        self.gamma = gamma\n\n    def build_model(self):\n        # Create a sequential model with 3 layers\n        model = Sequential([\n            # Input layer expects a flattened grid, hence the input shape is grid_size squared\n            Dense(128, activation='relu', input_shape=(self.grid_size**2,)),\n            Dense(64, activation='relu'),\n            # Output layer with 4 units for the possible actions (up, down, left, right)\n            Dense(4, activation='linear')\n        ])\n\n        model.compile(optimizer='adam', loss='mse')\n\n        return model\n\n    def get_action(self, state):\n\n        # rand() returns a random value between 0 and 1\n        if np.random.rand() <= self.epsilon:\n            # Exploration: random action\n            action = np.random.randint(0, 4)\n        else:\n            # Add an extra dimension to the state to create a batch with one instance\n            state = np.expand_dims(state, axis=0)\n\n            # Use the model to predict the Q-values (action values) for the given state\n            q_values = self.model.predict(state, verbose=0)\n\n            # Select and return the action with the highest Q-value\n            action = np.argmax(q_values[0]) # Take the action from the first (and only) entry\n\n        # Decay the epsilon value to reduce the exploration over time\n        if self.epsilon > self.epsilon_end:\n            self.epsilon *= self.epsilon_decay\n\n        return action\n\n    def learn(self, experiences):\n        states = np.array([experience.state for experience in experiences])\n        actions = np.array([experience.action for experience in experiences])\n        rewards = np.array([experience.reward for experience in experiences])\n        next_states = np.array([experience.next_state for experience in experiences])\n        dones = np.array([experience.done for experience in experiences])\n\n        # Predict the Q-values (action values) for the given state batch\n        current_q_values = self.model.predict(states, verbose=0)\n\n        # Predict the Q-values for the next_state batch\n        next_q_values = self.model.predict(next_states, verbose=0)\n\n        # Initialize the target Q-values as the current Q-values\n        target_q_values = current_q_values.copy()\n\n        # Loop through each experience in the batch\n        for i in range(len(experiences)):\n            if dones[i]:\n                # If the episode is done, there is no next Q-value\n                target_q_values[i, actions[i]] = rewards[i]\n            else:\n                # The updated Q-value is the reward plus the discounted max Q-value for the next state\n                # [i, actions[i]] is the numpy equivalent of [i][actions[i]]\n                target_q_values[i, actions[i]] = rewards[i] + self.gamma * np.max(next_q_values[i])\n\n        # Train the model\n        self.model.fit(states, target_q_values, epochs=1, verbose=0)\n\n    def load(self, file_path):\n        self.model = load_model(file_path)\n\n    def save(self, file_path):\n        self.model.save(file_path)\n```", "```py\nfrom environment import Environment\nfrom agent import Agent\nfrom experience_replay import ExperienceReplay\n\nif __name__ == '__main__':\n\n    grid_size = 5\n\n    environment = Environment(grid_size=grid_size, render_on=True)\n    agent = Agent(grid_size=grid_size, epsilon=1, epsilon_decay=0.998, epsilon_end=0.01)\n    experience_replay = ExperienceReplay(capacity=10000, batch_size=32)\n    ...\n```", "```py\nfrom environment import Environment\nfrom agent import Agent\nfrom experience_replay import ExperienceReplay\n\nif __name__ == '__main__':\n\n    grid_size = 5\n\n    environment = Environment(grid_size=grid_size, render_on=True)\n    agent = Agent(grid_size=grid_size, epsilon=1, epsilon_decay=0.998, epsilon_end=0.01)\n    experience_replay = ExperienceReplay(capacity=10000, batch_size=32)\n\n    # Number of episodes to run before training stops\n    episodes = 5000\n    # Max number of steps in each episode\n    max_steps = 200\n    ...\n```", "```py\nfrom environment import Environment\nfrom agent import Agent\nfrom experience_replay import ExperienceReplay\n\nif __name__ == '__main__':\n\n    grid_size = 5\n\n    environment = Environment(grid_size=grid_size, render_on=True)\n    agent = Agent(grid_size=grid_size, epsilon=1, epsilon_decay=0.998, epsilon_end=0.01)\n    experience_replay = ExperienceReplay(capacity=10000, batch_size=32)\n\n    # Number of episodes to run before training stops\n    episodes = 5000\n    # Max number of steps in each episode\n    max_steps = 200\n\n    for episode in range(episodes):\n        # Get the initial state of the environment and set done to False\n        state = environment.reset()\n\n        # Loop until the episode finishes\n        for step in range(max_steps):\n            # Logic for each step\n            ...\n            if done:\n                break\n\n        agent.save(f'models/model_{grid_size}.h5')\n```", "```py\nfrom environment import Environment\nfrom agent import Agent\nfrom experience_replay import ExperienceReplay\n\nif __name__ == '__main__':\n\n    grid_size = 5\n\n    environment = Environment(grid_size=grid_size, render_on=True)\n    agent = Agent(grid_size=grid_size, epsilon=1, epsilon_decay=0.998, epsilon_end=0.01)\n    experience_replay = ExperienceReplay(capacity=10000, batch_size=32)\n\n    # Number of episodes to run before training stops\n    episodes = 5000\n    # Max number of steps in each episode\n    max_steps = 200\n\n    for episode in range(episodes):\n        # Get the initial state of the environment and set done to False\n        state = environment.reset()\n\n        # Loop until the episode finishes\n        for step in range(max_steps):\n            print('Episode:', episode)\n            print('Step:', step)\n            print('Epsilon:', agent.epsilon)\n\n            # Get the action choice from the agents policy\n            action = agent.get_action(state)\n\n            # Take a step in the environment and save the experience\n            reward, next_state, done = environment.step(action)\n            experience_replay.add_experience(state, action, reward, next_state, done)\n\n            # If the experience replay has enough memory to provide a sample, train the agent\n            if experience_replay.can_provide_sample():\n                experiences = experience_replay.sample_batch()\n                agent.learn(experiences)\n\n            # Set the state to the next_state\n            state = next_state\n\n            if done:\n                break\n\n        agent.save(f'models/model_{grid_size}.h5')\n```", "```py\nfrom environment import Environment\nfrom agent import Agent\nfrom experience_replay import ExperienceReplay\nimport time\n\nif __name__ == '__main__':\n\n    grid_size = 5\n\n    environment = Environment(grid_size=grid_size, render_on=True)\n    agent = Agent(grid_size=grid_size, epsilon=1, epsilon_decay=0.998, epsilon_end=0.01)\n    # agent.load(f'models/model_{grid_size}.h5')\n\n    experience_replay = ExperienceReplay(capacity=10000, batch_size=32)\n\n    # Number of episodes to run before training stops\n    episodes = 5000\n    # Max number of steps in each episode\n    max_steps = 200\n\n    for episode in range(episodes):\n\n        # Get the initial state of the environment and set done to False\n        state = environment.reset()\n\n        # Loop until the episode finishes\n        for step in range(max_steps):\n            print('Episode:', episode)\n            print('Step:', step)\n            print('Epsilon:', agent.epsilon)\n\n            # Get the action choice from the agents policy\n            action = agent.get_action(state)\n\n            # Take a step in the environment and save the experience\n            reward, next_state, done = environment.step(action)\n            experience_replay.add_experience(state, action, reward, next_state, done)\n\n            # If the experience replay has enough memory to provide a sample, train the agent\n            if experience_replay.can_provide_sample():\n                experiences = experience_replay.sample_batch()\n                agent.learn(experiences)\n\n            # Set the state to the next_state\n            state = next_state\n\n            if done:\n                break\n\n            # Optionally, pause for half a second to evaluate the model\n            # time.sleep(0.5)\n\n        agent.save(f'models/model_{grid_size}.h5')\n```", "```py\n[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0]\n```", "```py\n[-2, -1]\n```", "```py\ndef get_state(self):\n    # Calculate row distance and column distance\n    relative_distance = (self.agent_location[0] - self.goal_location[0],\n                         self.agent_location[1] - self.goal_location[1])\n\n    # Unpack tuple into numpy array\n    state = np.array([*relative_distance])\n    return state\n```", "```py\ndef move_agent(self, action):\n    ...\n    else:\n        # Same punishment for an invalid move\n        reward = -1.1\n\n    return reward, done\n```", "```py\nclass Agent:\n    def __init__(self, grid_size, ...):\n        self.grid_size = grid_size\n        ...\n        self.model = self.build_model()\n\n    def build_model(self):\n        # Create a sequential model with 3 layers\n        model = Sequential([\n            # Input layer expects a flattened grid, hence the input shape is grid_size squared\n            Dense(128, activation='relu', input_shape=(self.grid_size**2,)),\n            Dense(64, activation='relu'),\n            # Output layer with 4 units for the possible actions (up, down, left, right)\n            Dense(4, activation='linear')\n        ])\n\n        model.compile(optimizer='adam', loss='mse')\n\n        return model\n    ...\n```", "```py\nclass Agent:\n    def __init__(self, ...):\n        ...\n        self.model = self.build_model()\n\n    def build_model(self):\n        # Create a sequential model with 3 layers\n        model = Sequential([\n            # Input layer expects a flattened grid, hence the input shape is grid_size squared\n            Dense(64, activation='relu', input_shape=(2,)),\n            Dense(32, activation='relu'),\n            # Output layer with 4 units for the possible actions (up, down, left, right)\n            Dense(4, activation='linear')\n        ])\n\n        model.compile(optimizer='adam', loss='mse')\n\n        return model\n    ...\n```", "```py\n[-2, 0]\n```", "```py\n[[-2],\n [0]]\n```", "```py\nfrom environment import Environment\nfrom agent import Agent\nfrom experience_replay import ExperienceReplay\nimport time\n\nif __name__ == '__main__':\n\n    grid_size = 5\n\n    environment = Environment(grid_size=grid_size, render_on=True)\n    agent = Agent(epsilon=1, epsilon_decay=0.998, epsilon_end=0.01)\n    # agent.load(f'models/model.h5')\n\n    experience_replay = ExperienceReplay(capacity=10000, batch_size=32)\n\n    # Number of episodes to run before training stops\n    episodes = 5000\n    # Max number of steps in each episode\n    max_steps = 200\n\n    for episode in range(episodes):\n\n        # Get the initial state of the environment and set done to False\n        state = environment.reset()\n\n        # Loop until the episode finishes\n        for step in range(max_steps):\n            print('Episode:', episode)\n            print('Step:', step)\n            print('Epsilon:', agent.epsilon)\n\n            # Get the action choice from the agents policy\n            action = agent.get_action(state)\n\n            # Take a step in the environment and save the experience\n            reward, next_state, done = environment.step(action)\n            experience_replay.add_experience(state, action, reward, next_state, done)\n\n            # If the experience replay has enough memory to provide a sample, train the agent\n            if experience_replay.can_provide_sample():\n                experiences = experience_replay.sample_batch()\n                agent.learn(experiences)\n\n            # Set the state to the next_state\n            state = next_state\n\n            if done:\n                break\n\n            # Optionally, pause for half a second to evaluate the model\n            # time.sleep(0.5)\n\n        agent.save(f'models/model.h5')\n```", "```py\nStep 10\n[[ 0.29763165 0.28393078 -0.01633328 -0.45749056]]\n\nStep 50\n[[ 7.173178 6.3558702 -0.48632553 -3.1968129 ]]\n\nStep 100\n[[ 33.015953 32.89661 33.11674 -14.883122]]\n\nStep 200\n[[573.52844 590.95685 592.3647 531.27576]]\n\n...\n\nStep 5000\n[[37862352\\. 34156752\\. 35527612\\. 37821140.]]\n```", "```py\nfrom tensorflow.keras.optimizers import Adam\n\ndef build_model(self):\n    # Create a sequential model with 3 layers\n    model = Sequential([\n        # Input layer expects a flattened grid, hence the input shape is grid_size squared\n        Dense(64, activation='relu', input_shape=(2,)),\n        Dense(32, activation='relu'),\n        # Output layer with 4 units for the possible actions (up, down, left, right)\n        Dense(4, activation='linear')\n    ])\n\n    # Update learning rate\n    optimizer = Adam(learning_rate=0.00001)\n\n    # Compile the model with the custom optimizer\n    model.compile(optimizer=optimizer, loss='mse')\n\n    return model\n```", "```py\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tensorflow.keras.models import load_model\n\ndef generate_heatmap(episode, grid_size, model_path):\n    # Load the model\n    model = load_model(model_path)\n\n    goal_location = (grid_size // 2, grid_size // 2)  # Center of the grid\n\n    # Initialize an array to store the color intensities\n    heatmap_data = np.zeros((grid_size, grid_size, 3))\n\n    # Define colors for each action\n    colors = {\n        0: np.array([0, 0, 1]),  # Blue for up\n        1: np.array([1, 0, 0]),  # Red for down\n        2: np.array([0, 1, 0]),  # Green for left\n        3: np.array([1, 1, 0])   # Yellow for right\n    }\n\n    # Calculate Q-values for each state and determine the color intensity\n    for x in range(grid_size):\n        for y in range(grid_size):\n            relative_distance = (x - goal_location[0], y - goal_location[1])\n            state = np.array([*relative_distance]).reshape(1, -1)\n            q_values = model.predict(state)\n            best_action = np.argmax(q_values)\n            if (x, y) == goal_location:\n                heatmap_data[x, y] = np.array([1, 1, 1])\n            else:\n                heatmap_data[x, y] = colors[best_action]\n\n    # Plotting the heatmap\n    plt.imshow(heatmap_data, interpolation='nearest')\n    plt.xlabel(f'Episode: {episode}')\n    plt.axis('off')\n    plt.tight_layout(pad=0)\n    plt.savefig(f'./figures/heatmap_{grid_size}_{episode}', bbox_inches='tight')\n```"]