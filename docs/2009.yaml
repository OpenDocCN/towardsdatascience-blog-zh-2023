- en: Word2Vec, GloVe, and FastText, Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/word2vec-glove-and-fasttext-explained-215a5cd4c06f?source=collection_archive---------3-----------------------#2023-06-20](https://towardsdatascience.com/word2vec-glove-and-fasttext-explained-215a5cd4c06f?source=collection_archive---------3-----------------------#2023-06-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How computers understand words
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@dataemporium?source=post_page-----215a5cd4c06f--------------------------------)[![Ajay
    Halthor](../Images/1be821c8d8ed336b9ecedcf94f960ede.png)](https://medium.com/@dataemporium?source=post_page-----215a5cd4c06f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----215a5cd4c06f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----215a5cd4c06f--------------------------------)
    [Ajay Halthor](https://medium.com/@dataemporium?source=post_page-----215a5cd4c06f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb0a3e7e495ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword2vec-glove-and-fasttext-explained-215a5cd4c06f&user=Ajay+Halthor&userId=b0a3e7e495ca&source=post_page-b0a3e7e495ca----215a5cd4c06f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----215a5cd4c06f--------------------------------)
    ·10 min read·Jun 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F215a5cd4c06f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword2vec-glove-and-fasttext-explained-215a5cd4c06f&user=Ajay+Halthor&userId=b0a3e7e495ca&source=-----215a5cd4c06f---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F215a5cd4c06f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword2vec-glove-and-fasttext-explained-215a5cd4c06f&source=-----215a5cd4c06f---------------------bookmark_footer-----------)![](../Images/9c0e7688b42e45235291764ce07ad479.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Growtika](https://unsplash.com/@growtika?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Computers don’t understand words like we do. They prefer to work with numbers.
    So, to help computers understand words and their meanings, we use something called
    embeddings. These embeddings numerically represent words as mathematical vectors.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: The cool thing about these embeddings is that if we learn them properly, words
    that have similar meanings will have similar numeric values. In other words, their
    numbers will be closer to each other. This allows computers to grasp the connections
    and similarities between different words based on their numeric representations.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: One prominent method for learning word embeddings is Word2Vec. In this article,
    we will delve into the intricacies of Word2Vec and explore its various architectures
    and variants.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一种学习词嵌入的显著方法是Word2Vec。在本文中，我们将深入探讨Word2Vec的复杂性，并探讨其各种架构和变体。
- en: Word2Vec
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Word2Vec
- en: '![](../Images/729bbfa94374af6bfcda2475c9a71f4f.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/729bbfa94374af6bfcda2475c9a71f4f.png)'
- en: 'Figure 1: Word2Vec architectures ([Source](https://arxiv.org/abs/1301.3781))'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：Word2Vec架构（[来源](https://arxiv.org/abs/1301.3781)）
- en: In the early days, sentences were represented with n-gram vectors. These vectors
    aimed to capture the essence of a sentence by considering sequences of words.
    However, they had some limitations. N-gram vectors were often large and sparse,
    which made them computationally challenging to create. This created…
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期，句子通过n-gram向量表示。这些向量旨在通过考虑单词序列来捕捉句子的本质。然而，它们存在一些局限性。n-gram向量通常很大且稀疏，这使得它们在计算上难以创建。这就产生了…
