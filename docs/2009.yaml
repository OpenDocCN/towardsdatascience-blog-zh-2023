- en: Word2Vec, GloVe, and FastText, Explained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/word2vec-glove-and-fasttext-explained-215a5cd4c06f?source=collection_archive---------3-----------------------#2023-06-20](https://towardsdatascience.com/word2vec-glove-and-fasttext-explained-215a5cd4c06f?source=collection_archive---------3-----------------------#2023-06-20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How computers understand words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@dataemporium?source=post_page-----215a5cd4c06f--------------------------------)[![Ajay
    Halthor](../Images/1be821c8d8ed336b9ecedcf94f960ede.png)](https://medium.com/@dataemporium?source=post_page-----215a5cd4c06f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----215a5cd4c06f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----215a5cd4c06f--------------------------------)
    [Ajay Halthor](https://medium.com/@dataemporium?source=post_page-----215a5cd4c06f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb0a3e7e495ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword2vec-glove-and-fasttext-explained-215a5cd4c06f&user=Ajay+Halthor&userId=b0a3e7e495ca&source=post_page-b0a3e7e495ca----215a5cd4c06f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----215a5cd4c06f--------------------------------)
    ·10 min read·Jun 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F215a5cd4c06f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword2vec-glove-and-fasttext-explained-215a5cd4c06f&user=Ajay+Halthor&userId=b0a3e7e495ca&source=-----215a5cd4c06f---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F215a5cd4c06f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword2vec-glove-and-fasttext-explained-215a5cd4c06f&source=-----215a5cd4c06f---------------------bookmark_footer-----------)![](../Images/9c0e7688b42e45235291764ce07ad479.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Growtika](https://unsplash.com/@growtika?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Computers don’t understand words like we do. They prefer to work with numbers.
    So, to help computers understand words and their meanings, we use something called
    embeddings. These embeddings numerically represent words as mathematical vectors.
  prefs: []
  type: TYPE_NORMAL
- en: The cool thing about these embeddings is that if we learn them properly, words
    that have similar meanings will have similar numeric values. In other words, their
    numbers will be closer to each other. This allows computers to grasp the connections
    and similarities between different words based on their numeric representations.
  prefs: []
  type: TYPE_NORMAL
- en: One prominent method for learning word embeddings is Word2Vec. In this article,
    we will delve into the intricacies of Word2Vec and explore its various architectures
    and variants.
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/729bbfa94374af6bfcda2475c9a71f4f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Word2Vec architectures ([Source](https://arxiv.org/abs/1301.3781))'
  prefs: []
  type: TYPE_NORMAL
- en: In the early days, sentences were represented with n-gram vectors. These vectors
    aimed to capture the essence of a sentence by considering sequences of words.
    However, they had some limitations. N-gram vectors were often large and sparse,
    which made them computationally challenging to create. This created…
  prefs: []
  type: TYPE_NORMAL
