- en: Image Classification with Vision Transformer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/image-classification-with-vision-transformer-8bfde8e541d4?source=collection_archive---------0-----------------------#2023-04-13](https://towardsdatascience.com/image-classification-with-vision-transformer-8bfde8e541d4?source=collection_archive---------0-----------------------#2023-04-13)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to classify images with the help of Transformer-based model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@marcellusruben?source=post_page-----8bfde8e541d4--------------------------------)[![Ruben
    Winastwan](../Images/15ad0dd03bf5892510abdf166a1e91e1.png)](https://medium.com/@marcellusruben?source=post_page-----8bfde8e541d4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8bfde8e541d4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8bfde8e541d4--------------------------------)
    [Ruben Winastwan](https://medium.com/@marcellusruben?source=post_page-----8bfde8e541d4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5dae9da73c9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimage-classification-with-vision-transformer-8bfde8e541d4&user=Ruben+Winastwan&userId=5dae9da73c9b&source=post_page-5dae9da73c9b----8bfde8e541d4---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8bfde8e541d4--------------------------------)
    ·13 min read·Apr 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8bfde8e541d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimage-classification-with-vision-transformer-8bfde8e541d4&user=Ruben+Winastwan&userId=5dae9da73c9b&source=-----8bfde8e541d4---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8bfde8e541d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimage-classification-with-vision-transformer-8bfde8e541d4&source=-----8bfde8e541d4---------------------bookmark_footer-----------)![](../Images/a404a96d42476c650eab290a3eb6734f.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [drmakete lab](https://unsplash.com/@drmakete?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/hsg538WrP0Y?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: Since its introduction in 2017, Transformer has been widely recognized as a
    powerful encoder-decoder model to solve pretty much any language modeling task.
  prefs: []
  type: TYPE_NORMAL
- en: BERT, RoBERTa, and XLM-RoBERTa are a few examples of state-of-the-art models
    in language processing that use a stack of Transformer encoders as the backbone
    in their architecture. ChatGPT and the GPT family also use the decoder part of
    Transformer to generate texts. It’s safe to say that almost any state-of-the-art
    model in natural language processing incorporate Transformer in its architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Transformer performance is so good that it seems wasteful not to use it for
    tasks beyond natural language processing, like computer vision for example. However,
    the big question is: can we actually use it for computer vision tasks?'
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that Transformer also has a good potential to be applied to computer
    vision tasks. In 2020, Google Brain team introduced a Transformer-based model
    that can be used to solve an image classification task called Vision Transformer
    (ViT). Its performance is very competitive in comparison with conventional CNNs
    on several image classification benchmarks.
  prefs: []
  type: TYPE_NORMAL
