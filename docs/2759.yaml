- en: 'GLIP: Introducing Language-Image Pre-Training to Object Detection'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'GLIP: 将语言-图像预训练引入目标检测'
- en: 原文：[https://towardsdatascience.com/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=collection_archive---------1-----------------------#2023-09-01](https://towardsdatascience.com/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=collection_archive---------1-----------------------#2023-09-01)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=collection_archive---------1-----------------------#2023-09-01](https://towardsdatascience.com/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=collection_archive---------1-----------------------#2023-09-01)
- en: '[🚀Sascha’s Paper Club](https://towardsdatascience.com/tagged/saschas-paper-club)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[🚀Sascha’s Paper Club](https://towardsdatascience.com/tagged/saschas-paper-club)'
- en: Grounded Language-Image Pre-training by L. H. Li et. al.
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Grounded Language-Image Pre-training by L. H. Li et. al.
- en: '[](https://medium.com/@SaschaKirch?source=post_page-----5ddb601873aa--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----5ddb601873aa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5ddb601873aa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5ddb601873aa--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----5ddb601873aa--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@SaschaKirch?source=post_page-----5ddb601873aa--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----5ddb601873aa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5ddb601873aa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5ddb601873aa--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----5ddb601873aa--------------------------------)'
- en: ·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5c38dace9d5e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fglip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa&user=Sascha+Kirch&userId=5c38dace9d5e&source=post_page-5c38dace9d5e----5ddb601873aa---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5ddb601873aa--------------------------------)
    ·9 min read·Sep 1, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5ddb601873aa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fglip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa&user=Sascha+Kirch&userId=5c38dace9d5e&source=-----5ddb601873aa---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5c38dace9d5e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fglip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa&user=Sascha+Kirch&userId=5c38dace9d5e&source=post_page-5c38dace9d5e----5ddb601873aa---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5ddb601873aa--------------------------------)
    ·9 min read·Sep 1, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5ddb601873aa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fglip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa&user=Sascha+Kirch&userId=5c38dace9d5e&source=-----5ddb601873aa---------------------clap_footer-----------)'
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5ddb601873aa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fglip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa&source=-----5ddb601873aa---------------------bookmark_footer-----------)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5ddb601873aa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fglip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa&source=-----5ddb601873aa---------------------bookmark_footer-----------)'
- en: 'Today we will dive into a paper that builds upon the great success of [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    in language-image pre-training and extends it to the task of object detection:
    GLIP — **G**rounded **L**anguage-**I**mage **P**re-training. We will cover the
    key concepts and findings of the paper and make them easy to understand by providing
    further context and adding annotations to images and experiment results. Let’s
    go!'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 今天我们将深入探讨一篇建立在[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)在语言-图像预训练领域巨大成功的基础上，并将其扩展到目标检测任务的论文：GLIP——**G**rounded
    **L**anguage-**I**mage **P**re-training。我们将介绍论文的关键概念和发现，并通过提供更多的背景信息和对图像及实验结果的注释，使其易于理解。让我们开始吧！
- en: '![](../Images/d7ef9f457ada5a99c0936ce420e24274.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d7ef9f457ada5a99c0936ce420e24274.png)'
- en: Image created from [publication](https://arxiv.org/abs/2112.03857) by [Sascha
    Kirch](https://medium.com/@SaschaKirch)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [出版物](https://arxiv.org/abs/2112.03857) 由 [Sascha Kirch](https://medium.com/@SaschaKirch)
    创作
- en: '**Paper:** [Grounded Language-Image Pre-training](https://arxiv.org/abs/2112.03857)
    by'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**论文：** [Grounded Language-Image Pre-training](https://arxiv.org/abs/2112.03857)
    作者'
- en: Liunian Harold Li et. al., 7\. Dec. 2021
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Liunian Harold Li 等，2021 年 12 月 7 日
- en: ''
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Resources:** [GitHub](https://github.com/microsoft/GLIP)'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**资源：** [GitHub](https://github.com/microsoft/GLIP)'
- en: ''
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Category:** representation learning, object detection, phrase-grounding,
    foundation models'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**类别：** 表示学习、物体检测、短语定位、基础模型'
- en: ''
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[**Other Walkthroughs**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**:**'
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[**其他教程**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**：**'
- en: '[[BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)]
    — [[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)]
    — [[Depth Anything](https://medium.com/towards-data-science/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?sk=fc6197edd68e6137c3396c83e50f65cb)]
    — [[Segment Anything](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    — [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    — [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[[BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)]
    — [[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)]
    — [[Depth Anything](https://medium.com/towards-data-science/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?sk=fc6197edd68e6137c3396c83e50f65cb)]
    — [[Segment Anything](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    — [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    — [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
- en: Outline
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大纲
- en: Context & Background
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 背景与上下文
- en: Claimed Contributions
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 声称的贡献
- en: Method
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 方法
- en: Experiments
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实验
- en: Further Readings & Resources
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进一步阅读与资源
- en: Context & Background
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景与上下文
