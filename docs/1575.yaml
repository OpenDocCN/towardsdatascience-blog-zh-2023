- en: 'Similarity Search, Part 2: Product Quantization'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相似性搜索，第二部分：产品量化
- en: 原文：[https://towardsdatascience.com/similarity-search-product-quantization-b2a1a6397701?source=collection_archive---------1-----------------------#2023-05-10](https://towardsdatascience.com/similarity-search-product-quantization-b2a1a6397701?source=collection_archive---------1-----------------------#2023-05-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/similarity-search-product-quantization-b2a1a6397701?source=collection_archive---------1-----------------------#2023-05-10](https://towardsdatascience.com/similarity-search-product-quantization-b2a1a6397701?source=collection_archive---------1-----------------------#2023-05-10)
- en: Learn a powerful technique to effectively compress large data
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习一种强大的技术来有效地压缩大量数据
- en: '[](https://medium.com/@slavahead?source=post_page-----b2a1a6397701--------------------------------)[![Vyacheslav
    Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----b2a1a6397701--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b2a1a6397701--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b2a1a6397701--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----b2a1a6397701--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@slavahead?source=post_page-----b2a1a6397701--------------------------------)[![Vyacheslav
    Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----b2a1a6397701--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b2a1a6397701--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b2a1a6397701--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----b2a1a6397701--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimilarity-search-product-quantization-b2a1a6397701&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----b2a1a6397701---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b2a1a6397701--------------------------------)
    ·9 min read·May 10, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb2a1a6397701&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimilarity-search-product-quantization-b2a1a6397701&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----b2a1a6397701---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimilarity-search-product-quantization-b2a1a6397701&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----b2a1a6397701---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b2a1a6397701--------------------------------)
    ·9分钟阅读·2023年5月10日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb2a1a6397701&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimilarity-search-product-quantization-b2a1a6397701&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----b2a1a6397701---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb2a1a6397701&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimilarity-search-product-quantization-b2a1a6397701&source=-----b2a1a6397701---------------------bookmark_footer-----------)![](../Images/cb55243dbdb7fd2bccdc8007780f6427.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb2a1a6397701&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimilarity-search-product-quantization-b2a1a6397701&source=-----b2a1a6397701---------------------bookmark_footer-----------)![](../Images/cb55243dbdb7fd2bccdc8007780f6427.png)'
- en: S**imilarity search** is a problem where given a query the goal is to find the
    most similar documents to it among all the database documents.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**相似性搜索**是一个问题，其中给定一个查询，目标是找到与其最相似的数据库文档。'
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In data science, similarity search often appears in the NLP domain, search engines
    or recommender systems where the most relevant documents or items need to be retrieved
    for a query. There exists a large variety of different ways to improve search
    performance in massive volumes of data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学中，相似性搜索常出现在自然语言处理（NLP）领域、搜索引擎或推荐系统中，需要为查询检索最相关的文档或项。存在多种方法来提升在大量数据中的搜索性能。
- en: In the [first part](https://medium.com/@slavahead/similarity-search-part-1-7cab80cc0e79)
    of this article series, we looked at kNN and inverted file index structure for
    performing similarity search. As we learned, kNN is the most straightforward approach
    while inverted file index acts on top of it suggesting a trade-off between speed
    acceleration and accuracy. Nevertheless, both methods do not use data compression
    techniques which might lead to memory issues, especially in cases of large datasets
    and limited RAM. In this article, we will try to address this issue by looking
    at another method called **Product Quantization**.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章系列的[第一部分](https://medium.com/@slavahead/similarity-search-part-1-7cab80cc0e79)中，我们研究了
    kNN 和倒排文件索引结构来执行相似性搜索。正如我们所了解的，kNN 是最直接的方法，而倒排文件索引则在其之上运行，建议在速度加速和准确性之间进行权衡。然而，这两种方法都不使用数据压缩技术，这可能会导致内存问题，特别是在大数据集和有限
    RAM 的情况下。在本文中，我们将尝试通过另一种方法来解决这个问题，这种方法被称为 **产品量化**。
- en: '[](/similarity-search-knn-inverted-file-index-7cab80cc0e79?source=post_page-----b2a1a6397701--------------------------------)
    [## Similarity Search, Part 1: kNN & Inverted File Index'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/similarity-search-knn-inverted-file-index-7cab80cc0e79?source=post_page-----b2a1a6397701--------------------------------)
    [## 相似性搜索，第 1 部分：kNN 和倒排文件索引'
- en: Similarity search is a popular problem where given a query Q we need to find
    the most similar documents to it among all…
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相似性搜索是一个流行的问题，其中给定查询 Q，我们需要在所有文档中找到最相似的文档…
- en: towardsdatascience.com](/similarity-search-knn-inverted-file-index-7cab80cc0e79?source=post_page-----b2a1a6397701--------------------------------)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/similarity-search-knn-inverted-file-index-7cab80cc0e79?source=post_page-----b2a1a6397701--------------------------------)'
- en: Definition
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义
- en: '**Product quantization** is the process where each dataset vector is converted
    into a short memory-efficient representation (called **PQ code**). Instead of
    fully keeping all the vectors, their short representations are stored. At the
    same time, product quantization is a lossy-compression method which results in
    lower prediction accuracy but in practice, this algorithm works very well.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**产品量化** 是一个过程，其中每个数据集向量被转换成一种简短的内存高效表示（称为 **PQ 代码**）。与其完全保存所有向量，不如存储它们的简短表示。同时，产品量化是一种有损压缩方法，会导致预测准确性降低，但在实际应用中，该算法效果很好。'
- en: In general, quantization is the process of mapping infinite values to discrete
    ones.
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一般来说，量化是将无限值映射到离散值的过程。
- en: Training
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练
- en: Firstly, the algorithm divides each vector into several equal parts — **subvectors**.
    Each of the respective parts of all dataset vectors form independent **subspaces**
    and is processed separately. Then a clustering algorithm is executed for each
    subspace of vectors. By doing so, several centroids in each subspace are created.
    Each subvector is encoded with the ID of the centroid that it belongs to. Additionally,
    the coordinates of all centroids are stored for later use.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，算法将每个向量分成几个相等的部分 —— **子向量**。所有数据集向量的各个部分形成独立的 **子空间** 并分别处理。然后，对每个子空间的向量执行聚类算法。这样，每个子空间中会生成多个质心。每个子向量使用它所属质心的
    ID 进行编码。此外，所有质心的坐标也会被存储以供后续使用。
- en: Subspace centroids are also called **quantized vectors**.
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 子空间质心也称为 **量化向量**。
- en: ''
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In product quantization, a cluster ID is often referred to as a **reproduction
    value**.
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在产品量化中，集群 ID 通常被称为 **重现值**。
- en: '*Note.* In the figures below a rectangle represents a vector containing several
    values while a square indicates a single number.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意。* 在下面的图示中，矩形代表包含多个值的向量，而正方形表示单个数字。'
- en: '![](../Images/12f7d9a6c6937706fb94959292ab8b55.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12f7d9a6c6937706fb94959292ab8b55.png)'
- en: Encoding using quantization
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用量化进行编码
- en: As a result, if an original vector is divided into *n* parts, then it can be
    encoded by *n* numbers — IDs of respective centroids for each of its subvectors.
    Typically, the number of created centroids *k* is usually chosen as a power of
    2 for more efficient memory usage. This way, the memory required to store an encoded
    vector is *n * log(k)* bits.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果原始向量被分成 *n* 部分，那么它可以由 *n* 个数字 —— 每个子向量的相应质心的 ID 来编码。通常，创建的质心数量 *k* 通常选择为
    2 的幂，以便更有效地使用内存。这样，存储一个编码向量所需的内存是 *n * log(k)* 位。
- en: The collection of all centroids inside a subspace is called a **codebook**.
    Running n clustering algorithms for all subspaces produces n separate codebooks.
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 每个子空间内所有质心的集合称为 **代码本**。对所有子空间运行 n 个聚类算法会生成 n 个独立的代码本。
- en: Compression example
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 压缩示例
- en: 'Imagine an original vector of size 1024 which stores floats (32 bits) was divided
    into *n = 8* subvectors where each subvector is encoded by one of *k = 256* clusters.
    Therefore, encoding the ID of a single cluster would require *log(256) = 8* bits.
    Let us compare the memory sizes for the vector representation in both cases:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个原始向量大小为1024，存储浮点数（32位），被划分为*n = 8*个子向量，每个子向量由*k = 256*个聚类中的一个进行编码。因此，编码单个聚类的ID需要*log(256)
    = 8*位。让我们比较这两种情况下向量表示的内存大小：
- en: 'Original vector: 1024 * 32 bits = 4096 bytes.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始向量：1024 * 32位 = 4096字节。
- en: 'Encoded vector: 8 * 8 bits = 8 bytes.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码向量：8 * 8位 = 8字节。
- en: The final compression is 512 times! This is the real power of product quantization.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最终压缩比为512倍！这就是产品量化的真正力量。
- en: '![](../Images/b07e898ad69ea53472c6bf06834cc760.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b07e898ad69ea53472c6bf06834cc760.png)'
- en: Quantization example. Numbers in vectors show how many numbers it stores.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 量化示例。向量中的数字显示了它存储了多少数字。
- en: 'Here are some important notes:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些重要的备注：
- en: 'The algorithm can be trained on one subset of vectors (e.g., to create clusters)
    and be used for another one: once the algorithm is trained, another dataset of
    vectors is passed where new vectors are encoded by using already constructed centroids
    for each subspace.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该算法可以在一个向量子集上进行训练（例如，创建聚类），并用于另一个子集：一旦算法被训练，另一个向量数据集会传递过来，其中新向量使用已经构建的每个子空间的质心进行编码。
- en: Typically, k-means is chosen as a clustering algorithm. One of its advantages
    is that the number of clusters *k* is a hyperparameter that can be manually defined,
    according to memory usage requirements.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常，k-means被选择作为聚类算法。它的一个优点是，聚类数*k*是一个超参数，可以根据内存使用要求手动定义。
- en: Inference
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推断
- en: To get a better understanding, let us first have a look at several naive approaches
    and find out their downsides. This will also help us realize why they should not
    be normally used.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解，让我们首先看几个天真的方法并找出它们的缺点。这也将帮助我们理解为什么它们通常不应该被使用。
- en: Naive approaches
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 天真的方法
- en: The first naive approach consists of decompressing all vectors by concatenating
    the corresponding centroids of each vector. After that, the *L2* distance (or
    another metric) can be calculated from a query vector to all the dataset vectors.
    Obviously, this method works but it is very time-consuming because the brute-force
    search is performed and the distance calculation is performed on high-dimensional
    decompressed vectors.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种天真的方法包括通过连接每个向量的相应质心来解压所有向量。之后，可以从查询向量到所有数据集向量计算*L2*距离（或其他度量）。显然，这种方法是可行的，但非常耗时，因为进行的是暴力搜索，并且距离计算是在高维解压向量上进行的。
- en: Another possible way is to split a query vector into subvectors and compute
    a sum of distances from each query subvector to respective quantized vectors of
    a database vector, based on its PQ code. As a consequence, the brute-search technique
    is used again and the distance calculation here still requires a linear time of
    the original vectors’ dimensionality, as in the previous case.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种可能的方法是将查询向量拆分为子向量，并计算每个查询子向量与基于其PQ代码的数据库向量的相应量化向量的距离之和。因此，暴力搜索技术再次被使用，并且这里的距离计算仍然需要原始向量维度的线性时间，与前一种情况相同。
- en: '![](../Images/99a6b8413b311ed9fdfe1a395b43dc71.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99a6b8413b311ed9fdfe1a395b43dc71.png)'
- en: Calculating approximate distance using naive approach. The example is shown
    for euclidean distance as a metric.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用天真的方法计算近似距离。示例显示了作为度量的欧几里得距离。
- en: Another possible method is to encode the query vector into a PQ code. Then this
    PQ code is directly utilized to calculate distances to all other PQ codes. The
    dataset vector with the corresponding PQ code which has the shortest distance
    is then considered as the nearest neighbour to the query. This approach is faster
    than the previous two because the distance is always computed between low-dimensional
    PQ codes. However, PQ codes are composed by cluster IDs which do not have a lot
    of semantic meaning and can be considered as a categorical variable explicitly
    used as a real variable. Clearly, this is a bad practice and this method can lead
    to poor prediction quality.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种可能的方法是将查询向量编码为PQ代码。然后直接利用这个PQ代码计算与所有其他PQ代码的距离。具有最短距离的对应PQ代码的数据集向量被认为是查询的最近邻。这种方法比前两种方法更快，因为距离总是在低维PQ代码之间计算。然而，PQ代码由聚类ID组成，这些ID没有太多语义意义，可以被明确视为实际变量的类别变量。显然，这是一种不好的做法，这种方法可能导致预测质量差。
- en: Optimized approach
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化方法
- en: A query vector is divided into subvectors. For each of its subvectors, distances
    to all the centroids of the corresponding subspace are computed. Ultimately, this
    information is stored in table *d*.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 查询向量被分为子向量。对于每个子向量，计算其到相应子空间中所有质心的距离。最终，这些信息存储在表格 *d* 中。
- en: '![](../Images/a5e93d2c4449b27fd168741abe6ab6b7.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a5e93d2c4449b27fd168741abe6ab6b7.png)'
- en: Obtaining a table d storing partial query subvector-to-centroid distances
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 获取一个表格 *d* 存储部分查询子向量到质心的距离
- en: Calculated subvector-to-centroid distances are often referred to as **partial
    distances**.
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 计算出的子向量到质心的距离通常被称为 **部分距离**。
- en: 'By using this subvector-to-centroid distance table *d*, the approximate distance
    from the query to any database vector can be easily obtained by its PQ codes:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用这个子向量到质心的距离表 *d*，可以通过其 PQ 代码轻松获得查询到任何数据库向量的近似距离：
- en: For each of subvectors of a database vector, the closest centroid *j* is found
    (by using mapping values from PQ codes) and the partial distance *d[i][j]* from
    that centroid to the query subvector *i* (by using the calculated matrix *d*)
    is taken.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于数据库向量的每个子向量，找到最接近的质心 *j*（通过使用 PQ 代码中的映射值），并从质心到查询子向量 *i* 的部分距离 *d[i][j]*（通过使用计算矩阵
    *d*）被取用。
- en: All the partial distances are squared and summed up. By taking the square root
    of this value, the approximate euclidean distance is obtained. If you want to
    know how to get approximate results for other metrics as well, navigate to the
    section below *“Approximation of other distance metrics”*.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有的部分距离被平方并求和。通过对该值开方，可以获得近似的欧几里得距离。如果你想了解如何获得其他度量的近似结果，请导航到 *“其他距离度量的近似”* 部分。
- en: '![](../Images/56ccf26207115c19fa0f848988c326e4.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56ccf26207115c19fa0f848988c326e4.png)'
- en: Computing distance from a query to database vector by using PQ code and distance
    table
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 PQ 代码和距离表计算查询向量到数据库向量的距离
- en: Using this method for calculating approximate distances assumes that partial
    distances **d** are very close to actual distances **a**between query and database
    subvectors.
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用这种方法计算近似距离假设部分距离 **d** 非常接近查询与数据库子向量之间的实际距离 **a**。
- en: Nevertheless, this condition may not be satisfied, especially when the distance
    *c* between the database subvector and its centroid is large. In such cases, calculations
    result in lower accuracy.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种条件可能无法满足，特别是当数据库子向量与其质心之间的距离 *c* 较大时。在这种情况下，计算结果的准确性较低。
- en: '![](../Images/3ef376fc19518ee9221a7c32c4845ac5.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ef376fc19518ee9221a7c32c4845ac5.png)'
- en: Example on the left shows a good case of approximation when the actual distance
    is very close to the partial distance (c is small). On the right side, we can
    observe a bad scenario because the partial distance is much longer than the actual
    distance (c is large).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的示例展示了一个良好的近似情况，当实际距离非常接近部分距离（*c* 较小）。右侧则展示了一个不良情况，因为部分距离远大于实际距离（*c* 较大）。
- en: After we have obtained approximate distances for all database rows, we search
    for vectors with the smallest values. Those vectors will be the nearest neighbours
    to the query.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在获取所有数据库行的近似距离后，我们搜索具有最小值的向量。这些向量将是查询的最近邻。
- en: Approximation of other distance metrics
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他距离度量的近似
- en: So far have looked at how to approximate euclidean distance by using partial
    distances. Let us generalize the rule for other metrics as well.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解了如何通过部分距离来近似欧几里得距离。让我们将规则推广到其他度量。
- en: 'Imagine we would like to calculate a distance metric between a pair of vectors.
    If we know the metrics’ formula, we can directly apply it to get the result. But
    sometimes we can do it by parts in the following manner:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要计算一对向量之间的距离度量。如果我们知道度量的公式，我们可以直接应用它来得到结果。但有时我们可以按以下方式分步进行：
- en: Both vectors are divided into *n* subvectors.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个向量都被分成 *n* 个子向量。
- en: For each pair of respective subvectors, the distance metric is calculated.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每一对对应的子向量，计算距离度量。
- en: Calculated *n* metrics are then combined to produce the actual distance between
    the original vectors.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算出的 *n* 个度量然后被组合以生成原始向量之间的实际距离。
- en: '![](../Images/8de585683199aca7b1de05a38e58d375.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8de585683199aca7b1de05a38e58d375.png)'
- en: The figure shows two ways of calculating a metric. On the left, the metric formula
    is directly applied to both vectors. On the right, partial distances are calculated
    for each pair of respective subvectors. Then they are combined by using aggregation
    functions h, g and f.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图中显示了计算度量的两种方法。左侧，度量公式直接应用于两个向量。右侧，为每对对应的子向量计算部分距离，然后通过使用聚合函数h、g和f进行组合。
- en: Euclidean distance is an example of a metric which can be calculated by parts.
    Based on the figure above, we can choose the aggregation functions to be *h(z)
    = z²* , *g(z₀, z₁, …, zₙ) = sum(z₀, z₁, …, zₙ)* and *f(z) = √z*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离是可以按部分计算的度量的一个例子。根据上图，我们可以选择聚合函数为 *h(z) = z²*，*g(z₀, z₁, …, zₙ) = sum(z₀,
    z₁, …, zₙ)* 和 *f(z) = √z*。
- en: '![](../Images/d9b2df072d03fd046dadb5505fe76fb5.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d9b2df072d03fd046dadb5505fe76fb5.png)'
- en: Euclidean distance can be calculated by parts
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离可以按部分计算
- en: Inner product is another example of such metric with aggregation functions *h(z)
    = z, g(z₀, z₁, …, zₙ) = sum(z₀, z₁, …, zₙ) and f(z) = z*.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 内积是另一种度量的例子，具有聚合函数 *h(z) = z, g(z₀, z₁, …, zₙ) = sum(z₀, z₁, …, zₙ) 和 f(z) =
    z*。
- en: In the context of product quantization, this is a very important property because
    during inference the algorithm calculates distances by parts. This means that
    it would be much more problematic to use metrics for product quantization that
    do not have this property. Cosine distance is an example of such metric.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在产品量化的背景下，这是一个非常重要的属性，因为在推断过程中，算法按部分计算距离。这意味着使用没有此属性的度量来进行产品量化会更加困难。余弦距离就是这种度量的一个例子。
- en: If there is still a need to use a metric without this property, then additional
    heuristics need to be applied to aggregate partial distances with some error.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果仍然需要使用没有此属性的度量，则需要应用额外的启发式方法来聚合带有一定误差的部分距离。
- en: Performance
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能
- en: The main advantage of the product quantization is a massive compression of database
    vectors which are stored as short PQ codes. For some applications, such compression
    rate may be even higher than 95%! However, apart from PQ codes, the matrix *d*
    of size *k* x *n* containing quantized vectors of each subspace needs to be stored.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 产品量化的主要优势是数据库向量的巨大压缩，这些向量作为简短的PQ代码存储。对于某些应用，这种压缩率甚至可能超过95%! 然而，除了PQ代码外，还需要存储大小为
    *k* x *n* 的矩阵 *d*，其中包含每个子空间的量化向量。
- en: Product quantization is a lossy-compression method, so the higher the compression
    is, the more likely that the prediction accuracy will decrease.
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 产品量化是一种有损压缩方法，因此压缩越高，预测准确性下降的可能性就越大。
- en: Building a system for efficient representation requires training several cluster
    algorithms. Apart from it, during inference, *k * n* partial distances need to
    be calculated in a brute-force manner and summed up for each of the database vectors
    which may take some time.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 建立一个高效表示的系统需要训练多个聚类算法。除此之外，在推断过程中，*k * n* 部分距离需要以暴力方式计算并为每个数据库向量求和，这可能需要一些时间。
- en: '![](../Images/9b0d9230996b8ee80f0ce38d117df577.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b0d9230996b8ee80f0ce38d117df577.png)'
- en: Product Quantization performance
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 产品量化性能
- en: Faiss implementation
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Faiss实现
- en: '[**Faiss**](https://github.com/facebookresearch/faiss) (Facebook AI Search
    Similarity) is a Python library written in C++ used for optimised similarity search.
    This library presents different types of indexes which are data structures used
    to efficiently store the data and perform queries.'
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[**Faiss**](https://github.com/facebookresearch/faiss)（Facebook AI搜索相似性）是一个用C++编写的Python库，用于优化相似性搜索。该库展示了不同类型的索引，这些索引是用于高效存储数据和执行查询的数据结构。'
- en: Based on the information from the [Faiss documentation](https://faiss.ai), we
    will see how product quantization is utilized.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[Faiss文档](https://faiss.ai)的信息，我们将看到产品量化是如何被利用的。
- en: 'Product quantization is implemented in the *IndexPQ* class. For initialisation,
    we need to provide it 3 parameters:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 产品量化在*IndexPQ*类中实现。初始化时，我们需要提供3个参数：
- en: '**d**: number of dimensions in data.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**d**：数据中的维度数量。'
- en: '**M**: number of splits for each vector (the same parameter as *n* used above).'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**M**：每个向量的拆分数量（与上文中使用的*n*相同的参数）。'
- en: '**nbits**: number of bits it takes to encode a single cluster ID. This means
    that the number of total clusters in a single subspace will be equal to *k = 2^nbits*.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**nbits**：编码单个聚类ID所需的位数。这意味着单个子空间中的总聚类数将等于 *k = 2^nbits*。'
- en: For equal subspace dimensions splitting, the parameter *dim* must be divisible
    by *M*.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于相等子空间维度的拆分，参数 *dim* 必须可以被 *M* 整除。
- en: 'The total number of bytes required to store a single vector is equal to:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 存储单个向量所需的总字节数等于：
- en: '![](../Images/9ee1277db1e10ce70bdf5a1c71816b2a.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ee1277db1e10ce70bdf5a1c71816b2a.png)'
- en: As we can see in the formula above, for more efficient memory usage the value
    of M * nbits should be divisible by *8*.
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如上公式所示，为了更有效地利用内存，M * nbits 的值应能被 *8* 整除。
- en: '![](../Images/dd96a42e85819a4d310f507facbe232f.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd96a42e85819a4d310f507facbe232f.png)'
- en: Faiss implementation of IndexPQ
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Faiss 对 IndexPQ 的实现
- en: Conclusion
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: We have looked through a very popular algorithm in information retrieval systems
    that efficiently compresses large volumes of data. Its principal downside is a
    slow inference speed. Despite this fact, the algorithm is widely used in modern
    Big data applications, especially in combination with other similarity search
    techniques.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探讨了信息检索系统中一种非常流行的算法，该算法能有效地压缩大量数据。其主要缺点是推理速度较慢。尽管如此，该算法在现代大数据应用中被广泛使用，特别是与其他相似性搜索技术结合时。
- en: In the first part of the article series, we described the workflow of the inverted
    file index. In fact, we can merge these two algorithms into a more efficient one
    which will possess the advantages of both! This is what exactly we are going to
    do in the next part of this series.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在文章系列的第一部分中，我们描述了倒排文件索引的工作流程。实际上，我们可以将这两种算法合并成一个更高效的算法，兼具两者的优势！这正是我们将在本系列的下一部分中要做的。
- en: '[](https://medium.com/@slavahead/similarity-search-blending-inverted-file-index-and-product-quantization-a8e508c765fa?source=post_page-----b2a1a6397701--------------------------------)
    [## Similarity Search, Part 3: Blending Inverted File Index and Product Quantization'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 相似性搜索，第3部分：融合倒排文件索引和产品量化](https://medium.com/@slavahead/similarity-search-blending-inverted-file-index-and-product-quantization-a8e508c765fa?source=post_page-----b2a1a6397701--------------------------------)'
- en: 'In the first two parts of this series we have discussed two fundamental algorithms
    in information retrieval: inverted…'
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在本系列的前两部分中，我们讨论了信息检索中的两个基本算法：倒排索引……
- en: medium.com](https://medium.com/@slavahead/similarity-search-blending-inverted-file-index-and-product-quantization-a8e508c765fa?source=post_page-----b2a1a6397701--------------------------------)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/@slavahead/similarity-search-blending-inverted-file-index-and-product-quantization-a8e508c765fa?source=post_page-----b2a1a6397701--------------------------------)'
- en: Resources
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源
- en: '[Product quantization for nearest neighbour search](https://lear.inrialpes.fr/pubs/2011/JDS11/jegou_searching_with_quantization.pdf)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[最近邻搜索的产品量化](https://lear.inrialpes.fr/pubs/2011/JDS11/jegou_searching_with_quantization.pdf)'
- en: '[Faiss documentation](https://faiss.ai)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Faiss 文档](https://faiss.ai)'
- en: '[Faiss repository](https://github.com/facebookresearch/faiss)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Faiss 仓库](https://github.com/facebookresearch/faiss)'
- en: '[Summary of Faiss indexes](https://github.com/facebookresearch/faiss/wiki/Faiss-indexes)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Faiss 索引总结](https://github.com/facebookresearch/faiss/wiki/Faiss-indexes)'
- en: '*All images unless otherwise noted are by the author.*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，否则所有图片均由作者提供。*'
