- en: 'Similarity Search, Part 2: Product Quantization'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/similarity-search-product-quantization-b2a1a6397701?source=collection_archive---------1-----------------------#2023-05-10](https://towardsdatascience.com/similarity-search-product-quantization-b2a1a6397701?source=collection_archive---------1-----------------------#2023-05-10)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn a powerful technique to effectively compress large data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@slavahead?source=post_page-----b2a1a6397701--------------------------------)[![Vyacheslav
    Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----b2a1a6397701--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b2a1a6397701--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b2a1a6397701--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----b2a1a6397701--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimilarity-search-product-quantization-b2a1a6397701&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----b2a1a6397701---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b2a1a6397701--------------------------------)
    ·9 min read·May 10, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb2a1a6397701&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimilarity-search-product-quantization-b2a1a6397701&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----b2a1a6397701---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb2a1a6397701&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimilarity-search-product-quantization-b2a1a6397701&source=-----b2a1a6397701---------------------bookmark_footer-----------)![](../Images/cb55243dbdb7fd2bccdc8007780f6427.png)'
  prefs: []
  type: TYPE_NORMAL
- en: S**imilarity search** is a problem where given a query the goal is to find the
    most similar documents to it among all the database documents.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In data science, similarity search often appears in the NLP domain, search engines
    or recommender systems where the most relevant documents or items need to be retrieved
    for a query. There exists a large variety of different ways to improve search
    performance in massive volumes of data.
  prefs: []
  type: TYPE_NORMAL
- en: In the [first part](https://medium.com/@slavahead/similarity-search-part-1-7cab80cc0e79)
    of this article series, we looked at kNN and inverted file index structure for
    performing similarity search. As we learned, kNN is the most straightforward approach
    while inverted file index acts on top of it suggesting a trade-off between speed
    acceleration and accuracy. Nevertheless, both methods do not use data compression
    techniques which might lead to memory issues, especially in cases of large datasets
    and limited RAM. In this article, we will try to address this issue by looking
    at another method called **Product Quantization**.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/similarity-search-knn-inverted-file-index-7cab80cc0e79?source=post_page-----b2a1a6397701--------------------------------)
    [## Similarity Search, Part 1: kNN & Inverted File Index'
  prefs: []
  type: TYPE_NORMAL
- en: Similarity search is a popular problem where given a query Q we need to find
    the most similar documents to it among all…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/similarity-search-knn-inverted-file-index-7cab80cc0e79?source=post_page-----b2a1a6397701--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Product quantization** is the process where each dataset vector is converted
    into a short memory-efficient representation (called **PQ code**). Instead of
    fully keeping all the vectors, their short representations are stored. At the
    same time, product quantization is a lossy-compression method which results in
    lower prediction accuracy but in practice, this algorithm works very well.'
  prefs: []
  type: TYPE_NORMAL
- en: In general, quantization is the process of mapping infinite values to discrete
    ones.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Firstly, the algorithm divides each vector into several equal parts — **subvectors**.
    Each of the respective parts of all dataset vectors form independent **subspaces**
    and is processed separately. Then a clustering algorithm is executed for each
    subspace of vectors. By doing so, several centroids in each subspace are created.
    Each subvector is encoded with the ID of the centroid that it belongs to. Additionally,
    the coordinates of all centroids are stored for later use.
  prefs: []
  type: TYPE_NORMAL
- en: Subspace centroids are also called **quantized vectors**.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In product quantization, a cluster ID is often referred to as a **reproduction
    value**.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Note.* In the figures below a rectangle represents a vector containing several
    values while a square indicates a single number.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/12f7d9a6c6937706fb94959292ab8b55.png)'
  prefs: []
  type: TYPE_IMG
- en: Encoding using quantization
  prefs: []
  type: TYPE_NORMAL
- en: As a result, if an original vector is divided into *n* parts, then it can be
    encoded by *n* numbers — IDs of respective centroids for each of its subvectors.
    Typically, the number of created centroids *k* is usually chosen as a power of
    2 for more efficient memory usage. This way, the memory required to store an encoded
    vector is *n * log(k)* bits.
  prefs: []
  type: TYPE_NORMAL
- en: The collection of all centroids inside a subspace is called a **codebook**.
    Running n clustering algorithms for all subspaces produces n separate codebooks.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Compression example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Imagine an original vector of size 1024 which stores floats (32 bits) was divided
    into *n = 8* subvectors where each subvector is encoded by one of *k = 256* clusters.
    Therefore, encoding the ID of a single cluster would require *log(256) = 8* bits.
    Let us compare the memory sizes for the vector representation in both cases:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Original vector: 1024 * 32 bits = 4096 bytes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Encoded vector: 8 * 8 bits = 8 bytes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final compression is 512 times! This is the real power of product quantization.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b07e898ad69ea53472c6bf06834cc760.png)'
  prefs: []
  type: TYPE_IMG
- en: Quantization example. Numbers in vectors show how many numbers it stores.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some important notes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm can be trained on one subset of vectors (e.g., to create clusters)
    and be used for another one: once the algorithm is trained, another dataset of
    vectors is passed where new vectors are encoded by using already constructed centroids
    for each subspace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typically, k-means is chosen as a clustering algorithm. One of its advantages
    is that the number of clusters *k* is a hyperparameter that can be manually defined,
    according to memory usage requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To get a better understanding, let us first have a look at several naive approaches
    and find out their downsides. This will also help us realize why they should not
    be normally used.
  prefs: []
  type: TYPE_NORMAL
- en: Naive approaches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first naive approach consists of decompressing all vectors by concatenating
    the corresponding centroids of each vector. After that, the *L2* distance (or
    another metric) can be calculated from a query vector to all the dataset vectors.
    Obviously, this method works but it is very time-consuming because the brute-force
    search is performed and the distance calculation is performed on high-dimensional
    decompressed vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Another possible way is to split a query vector into subvectors and compute
    a sum of distances from each query subvector to respective quantized vectors of
    a database vector, based on its PQ code. As a consequence, the brute-search technique
    is used again and the distance calculation here still requires a linear time of
    the original vectors’ dimensionality, as in the previous case.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/99a6b8413b311ed9fdfe1a395b43dc71.png)'
  prefs: []
  type: TYPE_IMG
- en: Calculating approximate distance using naive approach. The example is shown
    for euclidean distance as a metric.
  prefs: []
  type: TYPE_NORMAL
- en: Another possible method is to encode the query vector into a PQ code. Then this
    PQ code is directly utilized to calculate distances to all other PQ codes. The
    dataset vector with the corresponding PQ code which has the shortest distance
    is then considered as the nearest neighbour to the query. This approach is faster
    than the previous two because the distance is always computed between low-dimensional
    PQ codes. However, PQ codes are composed by cluster IDs which do not have a lot
    of semantic meaning and can be considered as a categorical variable explicitly
    used as a real variable. Clearly, this is a bad practice and this method can lead
    to poor prediction quality.
  prefs: []
  type: TYPE_NORMAL
- en: Optimized approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A query vector is divided into subvectors. For each of its subvectors, distances
    to all the centroids of the corresponding subspace are computed. Ultimately, this
    information is stored in table *d*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a5e93d2c4449b27fd168741abe6ab6b7.png)'
  prefs: []
  type: TYPE_IMG
- en: Obtaining a table d storing partial query subvector-to-centroid distances
  prefs: []
  type: TYPE_NORMAL
- en: Calculated subvector-to-centroid distances are often referred to as **partial
    distances**.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'By using this subvector-to-centroid distance table *d*, the approximate distance
    from the query to any database vector can be easily obtained by its PQ codes:'
  prefs: []
  type: TYPE_NORMAL
- en: For each of subvectors of a database vector, the closest centroid *j* is found
    (by using mapping values from PQ codes) and the partial distance *d[i][j]* from
    that centroid to the query subvector *i* (by using the calculated matrix *d*)
    is taken.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All the partial distances are squared and summed up. By taking the square root
    of this value, the approximate euclidean distance is obtained. If you want to
    know how to get approximate results for other metrics as well, navigate to the
    section below *“Approximation of other distance metrics”*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/56ccf26207115c19fa0f848988c326e4.png)'
  prefs: []
  type: TYPE_IMG
- en: Computing distance from a query to database vector by using PQ code and distance
    table
  prefs: []
  type: TYPE_NORMAL
- en: Using this method for calculating approximate distances assumes that partial
    distances **d** are very close to actual distances **a**between query and database
    subvectors.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Nevertheless, this condition may not be satisfied, especially when the distance
    *c* between the database subvector and its centroid is large. In such cases, calculations
    result in lower accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3ef376fc19518ee9221a7c32c4845ac5.png)'
  prefs: []
  type: TYPE_IMG
- en: Example on the left shows a good case of approximation when the actual distance
    is very close to the partial distance (c is small). On the right side, we can
    observe a bad scenario because the partial distance is much longer than the actual
    distance (c is large).
  prefs: []
  type: TYPE_NORMAL
- en: After we have obtained approximate distances for all database rows, we search
    for vectors with the smallest values. Those vectors will be the nearest neighbours
    to the query.
  prefs: []
  type: TYPE_NORMAL
- en: Approximation of other distance metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far have looked at how to approximate euclidean distance by using partial
    distances. Let us generalize the rule for other metrics as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine we would like to calculate a distance metric between a pair of vectors.
    If we know the metrics’ formula, we can directly apply it to get the result. But
    sometimes we can do it by parts in the following manner:'
  prefs: []
  type: TYPE_NORMAL
- en: Both vectors are divided into *n* subvectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each pair of respective subvectors, the distance metric is calculated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculated *n* metrics are then combined to produce the actual distance between
    the original vectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/8de585683199aca7b1de05a38e58d375.png)'
  prefs: []
  type: TYPE_IMG
- en: The figure shows two ways of calculating a metric. On the left, the metric formula
    is directly applied to both vectors. On the right, partial distances are calculated
    for each pair of respective subvectors. Then they are combined by using aggregation
    functions h, g and f.
  prefs: []
  type: TYPE_NORMAL
- en: Euclidean distance is an example of a metric which can be calculated by parts.
    Based on the figure above, we can choose the aggregation functions to be *h(z)
    = z²* , *g(z₀, z₁, …, zₙ) = sum(z₀, z₁, …, zₙ)* and *f(z) = √z*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d9b2df072d03fd046dadb5505fe76fb5.png)'
  prefs: []
  type: TYPE_IMG
- en: Euclidean distance can be calculated by parts
  prefs: []
  type: TYPE_NORMAL
- en: Inner product is another example of such metric with aggregation functions *h(z)
    = z, g(z₀, z₁, …, zₙ) = sum(z₀, z₁, …, zₙ) and f(z) = z*.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of product quantization, this is a very important property because
    during inference the algorithm calculates distances by parts. This means that
    it would be much more problematic to use metrics for product quantization that
    do not have this property. Cosine distance is an example of such metric.
  prefs: []
  type: TYPE_NORMAL
- en: If there is still a need to use a metric without this property, then additional
    heuristics need to be applied to aggregate partial distances with some error.
  prefs: []
  type: TYPE_NORMAL
- en: Performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main advantage of the product quantization is a massive compression of database
    vectors which are stored as short PQ codes. For some applications, such compression
    rate may be even higher than 95%! However, apart from PQ codes, the matrix *d*
    of size *k* x *n* containing quantized vectors of each subspace needs to be stored.
  prefs: []
  type: TYPE_NORMAL
- en: Product quantization is a lossy-compression method, so the higher the compression
    is, the more likely that the prediction accuracy will decrease.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Building a system for efficient representation requires training several cluster
    algorithms. Apart from it, during inference, *k * n* partial distances need to
    be calculated in a brute-force manner and summed up for each of the database vectors
    which may take some time.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9b0d9230996b8ee80f0ce38d117df577.png)'
  prefs: []
  type: TYPE_IMG
- en: Product Quantization performance
  prefs: []
  type: TYPE_NORMAL
- en: Faiss implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[**Faiss**](https://github.com/facebookresearch/faiss) (Facebook AI Search
    Similarity) is a Python library written in C++ used for optimised similarity search.
    This library presents different types of indexes which are data structures used
    to efficiently store the data and perform queries.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Based on the information from the [Faiss documentation](https://faiss.ai), we
    will see how product quantization is utilized.
  prefs: []
  type: TYPE_NORMAL
- en: 'Product quantization is implemented in the *IndexPQ* class. For initialisation,
    we need to provide it 3 parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**d**: number of dimensions in data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**M**: number of splits for each vector (the same parameter as *n* used above).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**nbits**: number of bits it takes to encode a single cluster ID. This means
    that the number of total clusters in a single subspace will be equal to *k = 2^nbits*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For equal subspace dimensions splitting, the parameter *dim* must be divisible
    by *M*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The total number of bytes required to store a single vector is equal to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9ee1277db1e10ce70bdf5a1c71816b2a.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see in the formula above, for more efficient memory usage the value
    of M * nbits should be divisible by *8*.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/dd96a42e85819a4d310f507facbe232f.png)'
  prefs: []
  type: TYPE_IMG
- en: Faiss implementation of IndexPQ
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have looked through a very popular algorithm in information retrieval systems
    that efficiently compresses large volumes of data. Its principal downside is a
    slow inference speed. Despite this fact, the algorithm is widely used in modern
    Big data applications, especially in combination with other similarity search
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: In the first part of the article series, we described the workflow of the inverted
    file index. In fact, we can merge these two algorithms into a more efficient one
    which will possess the advantages of both! This is what exactly we are going to
    do in the next part of this series.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@slavahead/similarity-search-blending-inverted-file-index-and-product-quantization-a8e508c765fa?source=post_page-----b2a1a6397701--------------------------------)
    [## Similarity Search, Part 3: Blending Inverted File Index and Product Quantization'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first two parts of this series we have discussed two fundamental algorithms
    in information retrieval: inverted…'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@slavahead/similarity-search-blending-inverted-file-index-and-product-quantization-a8e508c765fa?source=post_page-----b2a1a6397701--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Product quantization for nearest neighbour search](https://lear.inrialpes.fr/pubs/2011/JDS11/jegou_searching_with_quantization.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Faiss documentation](https://faiss.ai)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Faiss repository](https://github.com/facebookresearch/faiss)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Summary of Faiss indexes](https://github.com/facebookresearch/faiss/wiki/Faiss-indexes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*All images unless otherwise noted are by the author.*'
  prefs: []
  type: TYPE_NORMAL
