- en: Solving Reinforcement Learning Racetrack Exercise with Off-policy Monte Carlo
    Control
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用离策略蒙特卡洛控制解决强化学习赛道练习
- en: 原文：[https://towardsdatascience.com/solving-reinforcement-learning-racetrack-exercise-building-the-environment-33712602de0c?source=collection_archive---------0-----------------------#2023-07-23](https://towardsdatascience.com/solving-reinforcement-learning-racetrack-exercise-building-the-environment-33712602de0c?source=collection_archive---------0-----------------------#2023-07-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/solving-reinforcement-learning-racetrack-exercise-building-the-environment-33712602de0c?source=collection_archive---------0-----------------------#2023-07-23](https://towardsdatascience.com/solving-reinforcement-learning-racetrack-exercise-building-the-environment-33712602de0c?source=collection_archive---------0-----------------------#2023-07-23)
- en: '[](https://medium.com/@outerrencedl?source=post_page-----33712602de0c--------------------------------)[![Tingsong
    Ou](../Images/459edc4bbd2353895acfb0f57eeddaa3.png)](https://medium.com/@outerrencedl?source=post_page-----33712602de0c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----33712602de0c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----33712602de0c--------------------------------)
    [Tingsong Ou](https://medium.com/@outerrencedl?source=post_page-----33712602de0c--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@outerrencedl?source=post_page-----33712602de0c--------------------------------)[![Tingsong
    Ou](../Images/459edc4bbd2353895acfb0f57eeddaa3.png)](https://medium.com/@outerrencedl?source=post_page-----33712602de0c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----33712602de0c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----33712602de0c--------------------------------)
    [Tingsong Ou](https://medium.com/@outerrencedl?source=post_page-----33712602de0c--------------------------------)'
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa7aefc686327&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-reinforcement-learning-racetrack-exercise-building-the-environment-33712602de0c&user=Tingsong+Ou&userId=a7aefc686327&source=post_page-a7aefc686327----33712602de0c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----33712602de0c--------------------------------)
    ·10 min read·Jul 23, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F33712602de0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-reinforcement-learning-racetrack-exercise-building-the-environment-33712602de0c&user=Tingsong+Ou&userId=a7aefc686327&source=-----33712602de0c---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa7aefc686327&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-reinforcement-learning-racetrack-exercise-building-the-environment-33712602de0c&user=Tingsong+Ou&userId=a7aefc686327&source=post_page-a7aefc686327----33712602de0c---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----33712602de0c--------------------------------)
    ·10 min read·2023年7月23日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F33712602de0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-reinforcement-learning-racetrack-exercise-building-the-environment-33712602de0c&user=Tingsong+Ou&userId=a7aefc686327&source=-----33712602de0c---------------------clap_footer-----------)'
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F33712602de0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-reinforcement-learning-racetrack-exercise-building-the-environment-33712602de0c&source=-----33712602de0c---------------------bookmark_footer-----------)![](../Images/9a54dfbfe19b0a23290550dc0cad6d9e.png)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F33712602de0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-reinforcement-learning-racetrack-exercise-building-the-environment-33712602de0c&source=-----33712602de0c---------------------bookmark_footer-----------)![](../Images/9a54dfbfe19b0a23290550dc0cad6d9e.png)'
- en: Image generated by Midjourney with a paid subscription, which complies general
    commercial terms [1].
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 Midjourney 使用付费订阅生成，符合一般商业条款 [1]。
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'In the section *Off-policy Monte Carlo Control* of the book *Reinforcement
    Learning: An Introduction 2nd Edition (page 112)*, the author left us with an
    interesting exercise: using the weighted importance sampling off-policy Monte
    Carlo method to find the fastest way driving on both tracks. This exercise is
    comprehensive that asks us to consider and build almost every component of a reinforcement
    learning task, like the environment, agent, reward, actions, conditions of termination,
    and the algorithm. Solving this exercise is fun and helps us build a solid understanding
    of the interaction between algorithm and environment, the importance of a correct
    episodic task definition, and how the value initialization affects the training
    outcome. Through this post, I hope to share my understanding and solution to this
    exercise with everyone interested in reinforcement learning.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在书籍 *强化学习：导论 第二版（第112页）* 的 *离策略蒙特卡洛控制* 一节中，作者给我们留下了一个有趣的练习：使用加权重要性采样离策略蒙特卡洛方法找到在两条赛道上最快的行驶方式。这个练习非常全面，要求我们考虑和构建强化学习任务的几乎所有组件，如环境、代理、奖励、动作、终止条件和算法。解决这个练习很有趣，并帮助我们建立对算法与环境之间互动的扎实理解，正确的
    episodic 任务定义的重要性，以及价值初始化如何影响训练结果。通过这篇文章，我希望与所有对强化学习感兴趣的人分享我对这个练习的理解和解决方案。
- en: Exercise requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习要求
- en: 'As mentioned above, this exercise asks us to find a policy that makes a race
    car drive from the starting line to the finishing line as fast as possible without
    running into gravel or off the track. After carefully reading the exercise descriptions,
    I listed some key points that are essential to complete this task:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，这个练习要求我们找到一种策略，使赛车从起始线尽可能快速地驶向终点线，而不撞到碎石或驶出赛道。在仔细阅读练习描述后，我列出了完成这项任务的关键点：
- en: '**Map representation**: maps in this context are actually 2D matrices with
    (row_index, column_index) as coordinates. The value of each cell represents the
    state of that cell; for instance, we can use 0 to describe gravel, 1 for the track
    surface, 0.4 for the starting region, and 0.8 for the finishing line. Any row
    and column index outside the matrix can be considered as out-of-boundary.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**地图表示**：在此背景下，地图实际上是以 (row_index, column_index) 为坐标的二维矩阵。每个单元格的值表示该单元格的状态；例如，我们可以用
    0 来描述碎石，用 1 来表示赛道表面，用 0.4 表示起始区域，用 0.8 表示终点线。任何超出矩阵的行列索引都可以视为越界。'
- en: '**Car representation**: we can directly use the matrix’s coordinates to represent
    the car’s position;'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**汽车表示**：我们可以直接使用矩阵的坐标来表示汽车的位置；'
- en: '**Velocity and control**: the velocity space is discrete and consists of horizontal
    and vertical speeds that can be represented as a tuple (row_speed, col_speed).
    The speed limit on both axes is (-5, 5) and incremented by +1, 0, and -1 on each
    axis in each step; therefore, there are a total of 9 possible actions in each
    step. Literally, the speed cannot be both zero except at the starting line, and
    the vertical velocity, or row speed, cannot be negative as we don’t want our car
    to drive back to the starting line.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速度和控制**：速度空间是离散的，包括水平和垂直速度，可以表示为一个元组 (row_speed, col_speed)。两个轴上的速度限制为 (-5,
    5)，并且每个轴在每一步递增 +1、0 或 -1；因此，每一步有总共 9 种可能的动作。字面上，除了起始线外，速度不能同时为零，垂直速度（或行速度）不能为负数，因为我们不希望汽车回到起始线。'
- en: '**Reward and episode**: the reward for each step before crossing the finishing
    line is -1\. When the car runs out of the track, it’ll be reset to one of the
    starting cells. The episode ends **ONLY** when the car successfully crosses the
    finishing line.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励和回合**：在穿越终点线之前的每一步奖励为 -1。当汽车驶出赛道时，它会被重置到起始单元格之一。回合 **仅在** 汽车成功穿越终点线时结束。'
- en: '**Starting states**: we randomly choose starting cell for the car from the
    starting line; the car’s initial speed is (0, 0) according to the exercise’s description.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**起始状态**：我们从起始线随机选择起始单元格；根据练习的描述，汽车的初始速度为 (0, 0)。'
- en: '**Zero-acceleration challenge**: the author proposes a small *zero-acceleration
    challenge* that, at each time step, with 0.1 probability, the action will not
    take effect and the car will remain at its previous speed. We can implement this
    challenge in training instead of adding the feature to the environment.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**零加速度挑战**：作者提出了一个小的*零加速度挑战*，即在每个时间步骤中，以 0.1 的概率，动作将不起作用，汽车将保持之前的速度。我们可以在训练中实现这个挑战，而不是将其添加到环境中。'
- en: 'The solution to the exercise is separated into two posts; in this post, we’ll
    focus on building a racetrack environment. The file structure of this exercise
    is as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 练习的解决方案分为两篇文章；在这篇文章中，我们将重点讨论如何构建赛道环境。此练习的文件结构如下：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'And the libraries used in this implementation are as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现中使用的库如下：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Building track maps
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建赛道地图
- en: We can represent track maps as 2D matrices with different values indicating
    track states. I want to be loyal to the exercise, so I’m trying to build the same
    maps shown in the book by assigning matrix values manually. The maps will be saved
    as separate *.npy* files so that the environment can read the file in training
    instead of generating them in the runtime.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将赛道地图表示为2D矩阵，不同的值表示赛道状态。我希望忠于练习，所以我正尝试通过手动分配矩阵值来构建书中展示的相同地图。这些地图将作为单独的*.npy*文件保存，以便环境在训练时可以读取文件，而不是在运行时生成它们。
- en: The two maps look as follows; the light cells are gravel, the dark cells are
    track surfaces, the green-ish cells represent the finishing line, and the light-blue-ish
    cells represent the starting line.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个地图如下所示；浅色单元格是碎石，深色单元格是赛道表面，绿色单元格表示终点线，浅蓝色单元格表示起始线。
- en: '![](../Images/a014b029de744b7f65c750a5ad033e42.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a014b029de744b7f65c750a5ad033e42.png)'
- en: 'Figure 1 track maps with the 2D matrix representation. Source: Image by the
    author'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图1 2D矩阵表示的赛道地图。来源：作者提供的图像
- en: Building Gym-like environment
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建类似Gym的环境
- en: 'With the track maps ready, we now proceed to create a gym-like environment
    with which the algorithm can interact. The [gymnasium](https://gymnasium.farama.org/),
    previously the OpenAI gym now belonging to the *Farama Foundation*, provides a
    simple interface for testing RL algorithms; we will use the *gymnasium* package
    to create the racetrack environment. Our environment will include the following
    components/features:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 赛道地图准备好后，我们将创建一个类似于gym的环境，算法可以与之互动。 [gymnasium](https://gymnasium.farama.org/)，之前是OpenAI
    gym，现在属于*Farama Foundation*，提供了一个用于测试RL算法的简单接口；我们将使用*gymnasium*包来创建赛道环境。我们的环境将包括以下组件/功能：
- en: '**env.nS**: the shape of the observation space, which is (num_rows, num_cols,
    row_speed, col_speed) in this example. The number of rows and columns varies between
    maps, but the speed space are consistent across tracks. For the row speed, as
    we don’t want the car to go back to the starting line, the row speed observations
    consist of [-4, -3, -2, -1, 0] (negative value because we expect the car to go
    upwards in the map), five spaces in total; the column speed has no such limit,
    so the observations range from -4 to 4, nine spaces in total. Therefore, the shape
    of the observation space in the racetrack example is (num_rows, num_cols, 5, 9).'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**env.nS**：观察空间的形状，在这个例子中为(num_rows, num_cols, row_speed, col_speed)。行数和列数在地图之间有所不同，但速度空间在所有赛道中是一致的。对于行速度，由于我们不希望汽车回到起点，行速度的观察值为[-4,
    -3, -2, -1, 0]（负值因为我们期望汽车在地图上向上移动），总共有五个空间；列速度没有这样的限制，所以观察值范围从-4到4，总共有九个空间。因此，赛道示例中的观察空间形状为(num_rows,
    num_cols, 5, 9)。'
- en: '**env.nA**: the number of possible actions. There are 9 possible actions in
    our implementation; we will also create a dictionary in the environment class
    to map the integer action to the (row_speed, col_speed) tuple representation to
    control the agent.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**env.nA**：可能的行动数量。在我们的实现中有9种可能的行动；我们还将在环境类中创建一个字典，将整数行动映射到(row_speed, col_speed)元组表示，以控制智能体。'
- en: '**env.reset()**: the function that takes the car back to one of the starting
    cells when the episode finishes or the car runs out of the track;'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**env.reset()**：当回合结束或汽车驶出赛道时，该函数将汽车带回起始单元格之一；'
- en: '**env.step(action)**: the step function enables the algorithm to interact with
    the environment by taking an action and returning a tuple of (next_state, reward,
    is_terminated, is_truncated).'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**env.step(action)**：步进函数使算法通过采取行动与环境进行交互，并返回一个包含(next_state, reward, is_terminated,
    is_truncated)的元组。'
- en: 'State-checking functions: there’re two private functions that help to check
    if the car left the track or crossed the finishing line;'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态检查函数：有两个私有函数帮助检查汽车是否离开赛道或越过终点线；
- en: 'Rendering functions: rendering function is essential to the customized environment
    from my point of view; I always check if the environment has properly been built
    by rendering out the game space and agent’s behaviors, which is more straightforward
    than just staring at logging information.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 渲染函数：在我看来，渲染函数对定制环境至关重要；我总是通过渲染游戏空间和智能体的行为来检查环境是否已正确构建，这比单纯查看日志信息更直接。
- en: With these features in mind, we can start building the racetrack environment.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 牢记这些特点后，我们可以开始构建赛车道环境。
- en: Checking the environment
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查环境
- en: So far, with everything needed for the racetrack environment ready, we can test/verify
    our environment setup.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，所有赛车道环境所需的准备工作都已就绪，我们可以测试/验证我们的环境设置。
- en: 'First, we render out the game-playing to check if every component is working
    smoothly:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们渲染游戏以检查每个组件是否正常工作：
- en: '![](../Images/a9a2a779ce71a61e4ee17c2ed7f0eecc.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9a2a779ce71a61e4ee17c2ed7f0eecc.png)'
- en: 'Figure 2 Agents driving on both tracks with random policy. Source: Gif by the
    author'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图2 采用随机策略在两个轨道上行驶的智能体。来源：作者制作的动图
- en: 'Then we turn off the render option to make the environment run background to
    check if the trajectory terminates properly:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们关闭渲染选项，让环境在后台运行，以检查轨迹是否正确终止：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Off-policy Monte Carlo Control Algorithm
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 离策略蒙特卡罗控制算法
- en: 'With the environment ready, we can prepare to implement the off-policy MC control
    algorithm with weighted importance sampling algorithm, as show below:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在环境准备好后，我们可以准备实施带加权重要性采样的离策略MC控制算法，如下所示：
- en: '![](../Images/6b96023d481220e61c1b1e7f0710b1c3.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b96023d481220e61c1b1e7f0710b1c3.png)'
- en: 'Figure 3 Off-policy Comte Carlo Control Algorithm. Source: Algorithm written
    in [latex](https://gist.github.com/terrence-ou/67aad132f41975c1cb3aad460a03ac88)
    by the author.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图3 离策略蒙特卡罗控制算法。来源：[latex](https://gist.github.com/terrence-ou/67aad132f41975c1cb3aad460a03ac88)
    由作者编写。
- en: The Monte Carlo methods solve the reinforcement learning problem by averaging
    sample returns. In training, the method generates a trajectory based on a given
    policy and learns from the tail of each episode. The difference between on-policy
    and off-policy learning is that the on-policy methods use one policy to make decisions
    and improvements, whereas the off-policy methods use separate policies for generating
    data and policy improvement. According to the author of the book, almost all off-policy
    methods utilize importance sampling to estimate expected values under one distribution
    given samples from another. [2]
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡罗方法通过对样本回报进行平均来解决强化学习问题。在训练中，该方法基于给定策略生成轨迹，并从每个回合的尾部学习。在线策略和离策略学习之间的区别在于，在线策略方法使用一种策略来做决策和改进，而离策略方法使用不同的策略来生成数据和策略改进。根据书籍作者的说法，几乎所有的离策略方法都利用重要性采样来估计在一个分布下的期望值，给定来自另一个分布的样本。[2]
- en: The following several sections will explain tricks of soft policy and weighted
    importance sampling appearing in the algorithm above and how essential a proper
    value initialization is to this task.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的几个部分将解释上面算法中出现的软策略和加权重要性采样技巧，以及适当的值初始化对该任务的关键性。
- en: Target policy and behavior policy
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标策略和行为策略
- en: 'The off-policy method of this algorithm uses two policies:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 本算法的离策略方法使用了两种策略：
- en: '**Target policy**: the policy being learned about. In this algorithm, the target
    policy is greedy and deterministic, which means the probability of the greedy
    action *A* at time *t* is 1.0, with all other actions’ probability equal to 0.0\.
    The target policy follows the Generalized Policy Iteration (GPI) that improves
    after every step.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标策略**：正在学习的策略。在本算法中，目标策略是贪婪的和确定性的，这意味着在时间*t*，贪婪动作*A*的概率是1.0，其它所有动作的概率为0.0。目标策略遵循每步改进的广义策略迭代（GPI）。'
- en: '**Behavior policy**: the policy used to generate behavior. The behavior policy
    in this algorithm is defined as *soft policy*, meaning that all actions in a given
    state have a non-zero probability. We’ll use the ε-greedy policy as our behavior
    policy here.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**行为策略**：用于生成行为的策略。本算法中的行为策略被定义为*软策略*，意味着在给定状态下所有动作都有非零概率。我们将在这里使用ε-贪婪策略作为我们的行为策略。'
- en: 'In soft policy, the probability of an action is:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在软策略中，某个动作的概率是：
- en: '![](../Images/ea7d0d6e471fc982b4b4730148a83983.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea7d0d6e471fc982b4b4730148a83983.png)'
- en: 'We should also return this probability when generating actions for the importance
    sampling purpose. The code for the behavior policy looks as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在为重要性采样生成动作时也应返回这个概率。行为策略的代码如下所示：
- en: Weighted importance sampling
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加权重要性采样
- en: 'We estimate the relative probability of the trajectory generated by the target
    policy under the trajectory from the behavior policy, and the equation for such
    probability is:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们估计由目标策略生成的轨迹在行为策略下的相对概率，这样的概率方程是：
- en: '![](../Images/bd966b843cf9d67c29ecb0ebd67d78ce.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd966b843cf9d67c29ecb0ebd67d78ce.png)'
- en: 'The value function for the weighted importance sampling is:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 加权重要性采样的值函数是：
- en: '![](../Images/1803ba37915eb930a53a4fce6d1ef363.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1803ba37915eb930a53a4fce6d1ef363.png)'
- en: 'We can reframe the equation to fit it to the episodic task with the form of
    incremental updates:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将方程重新调整，以适应增量更新形式的情景任务：
- en: '![](../Images/8ea43a0e47e279128a1b6e4c489e6043.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8ea43a0e47e279128a1b6e4c489e6043.png)'
- en: 'There are a lot of excellent examples of how to derivate the above equation,
    so we don’t spend time deriving it here again. But I do also want to explain the
    interesting tricks about the last several lines of the algorithm:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多优秀的例子展示了如何推导上述方程，因此我们在这里不再推导。但我也想解释算法最后几行的有趣技巧：
- en: '![](../Images/1c3e3d57e2be46a0aeecf06b0587173e.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c3e3d57e2be46a0aeecf06b0587173e.png)'
- en: 'Figure 4 The trick in the off-policy MC control algorithm. Source: Image by
    the author'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图4 离策略蒙特卡罗控制算法中的技巧。来源：作者提供的图片
- en: The trick is based on the setting that the target policy here is deterministic.
    If the action taken at a time step differs from that comes from the target policy,
    then the probability of that action w.r.t the target policy is 0.0; thus, all
    the proceeding steps no longer update to the state-action value function of the
    trajectory. On the contrary, if the action at that time step is the same as the
    target policy, then the probability is 1.0, and the action-value update continues.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这个技巧基于目标策略在这里是确定性的设置。如果某个时间步采取的动作与目标策略的动作不同，那么该动作相对于目标策略的概率为0.0；因此，所有后续步骤将不再更新轨迹的状态-动作值函数。相反，如果那个时间步的动作与目标策略相同，则概率为1.0，动作值更新继续进行。
- en: Weight initialization
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 权重初始化
- en: 'Proper weight initialization plays an important role in successfully solving
    this exercise. Let’s first take a look at the rewards on both tracks from a random
    policy:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 合适的权重初始化在成功解决此练习中发挥着重要作用。我们首先来看随机策略在两个轨道上的奖励：
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The reward at each time step before crossing the finishing line is -1\. At the
    early stage of training, the reward will be large in negative values; if we initialize
    a state-action value to 0 or random values from a normal distribution, say, with
    a mean of 0 and variance of 1, the value then could be considered too optimistic
    that might take a very long time for the program to find an optimal policy.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在穿越终点线之前，每一步的奖励是-1。训练的早期阶段，奖励将是较大的负值；如果我们将状态-动作值初始化为0或从均值为0、方差为1的正态分布中随机值，则该值可能被认为过于乐观，这可能需要很长时间程序才能找到最佳策略。
- en: With several tests, I found a normal distribution with a mean of -500 and a
    variance of 1 could be effective values for a faster convergence; you are encouraged
    to try other values and can definitely find a better initial value than this.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 通过几次测试，我发现均值为-500、方差为1的正态分布可能是更快收敛的有效值；你可以尝试其他值，绝对能找到比这更好的初始值。
- en: Solving the Problem
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决问题
- en: With all of the thoughts above in mind, we can program out the Off-policy Monte
    Carlo control function and use it to solve the racetrack exercise. We’ll also
    add the zero-acceleration challenge that is mentioned in the exercise description.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 牢记上述所有思想，我们可以编写离策略蒙特卡罗控制函数，并用它来解决赛道练习。我们还将添加练习描述中提到的零加速度挑战。
- en: 'Then we train the policies for 1,000,000 episodes without/with zero acceleration
    challenge on both tracks and evaluate them with the following code:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们在两个轨道上训练政策1,000,000个回合，包含/不包含零加速度挑战，并用以下代码进行评估：
- en: By plotting out the training record, we find that the policy converges around
    the 10,000th episode on both tracks without considering zero acceleration; adding
    zero acceleration makes the convergence slower and unstable before the 100,000th
    episode but still converges afterward.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 通过绘制训练记录，我们发现策略在不考虑零加速度的情况下在两个轨道上都在第10,000集左右收敛；添加零加速度使得收敛变得更慢，并且在第100,000集之前不稳定，但之后仍然收敛。
- en: '![](../Images/6918a1b931aaa5fcb033677405aef3c0.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6918a1b931aaa5fcb033677405aef3c0.png)'
- en: 'Figure 5 Training reward history of track A. Source: Image by author'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图5 轨道A的训练奖励历史。来源：作者提供的图像
- en: '![](../Images/3ca458fa857f9d38822990255cd46b5a.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ca458fa857f9d38822990255cd46b5a.png)'
- en: 'Figure 6 Training reward history of track B. Source: Image by author'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图6 轨道B的训练奖励历史。来源：作者提供的图像
- en: Results
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: Finally, we can ask the agents to play the game with trained policies, and we
    also plot out the possible trajectories to further check the correctness of the
    policies.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以让智能体使用训练后的策略进行游戏，同时绘制出可能的轨迹以进一步检查策略的正确性。
- en: '![](../Images/e1ade388e7ec2b717df36aeb436acef9.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1ade388e7ec2b717df36aeb436acef9.png)'
- en: 'Figure 7 Agents driving on both tracks based on trained policies. Source: Gif
    by the author'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图7 智能体基于训练后的策略在两个轨道上行驶。来源：作者提供的GIF
- en: 'Sample trajectories for track a:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 轨道a的样本轨迹：
- en: '![](../Images/a65edebe8370063de691a667a4a6651b.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a65edebe8370063de691a667a4a6651b.png)'
- en: 'Figure 8 Sample optimal trajectories for track A. Source: Image by the author'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图8 轨道A的样本最优轨迹。来源：作者提供的图像
- en: 'Sample trajectories for track b:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 轨道b的样本轨迹：
- en: '![](../Images/4771002e1893aaab88bbe7483a4eec01.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4771002e1893aaab88bbe7483a4eec01.png)'
- en: 'Figure 9 Sample optimal trajectories for track B. Source: Image by the author'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图9 轨道B的样本最优轨迹。来源：作者提供的图像
- en: So far, we’ve solved the racetrack exercise. This implementation could still
    have some problems, and you’re very welcome to point them out and discuss a better
    solution in the comment. Thanks for reading!
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经解决了赛道练习。这种实现可能仍然存在一些问题，非常欢迎您指出这些问题并在评论中讨论更好的解决方案。感谢阅读！
- en: Reference
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考
- en: '[1] Midjourney Terms of Service: [https://docs.midjourney.com/docs/terms-of-service](https://docs.midjourney.com/docs/terms-of-service)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Midjourney 服务条款: [https://docs.midjourney.com/docs/terms-of-service](https://docs.midjourney.com/docs/terms-of-service)'
- en: '[2] Sutton, Richard S., and Andrew G. Barto. *Reinforcement learning: An introduction*.
    MIT Press, 2018.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Sutton, Richard S., 和 Andrew G. Barto. *强化学习：导论*。麻省理工学院出版社，2018年。'
- en: 'My GitHub repo of this exercise: [[Link]](https://github.com/terrence-ou/Reinforcement-Learning-2nd-Edition-Notes-Codes/tree/main/chapter_05_monte_carlo_methods);
    please check the “exercise section”.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '本练习的GitHub仓库: [[链接]](https://github.com/terrence-ou/Reinforcement-Learning-2nd-Edition-Notes-Codes/tree/main/chapter_05_monte_carlo_methods)；请查看“练习部分”。'
