- en: Solving Reinforcement Learning Racetrack Exercise with Off-policy Monte Carlo
    Control
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/solving-reinforcement-learning-racetrack-exercise-building-the-environment-33712602de0c?source=collection_archive---------0-----------------------#2023-07-23](https://towardsdatascience.com/solving-reinforcement-learning-racetrack-exercise-building-the-environment-33712602de0c?source=collection_archive---------0-----------------------#2023-07-23)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@outerrencedl?source=post_page-----33712602de0c--------------------------------)[![Tingsong
    Ou](../Images/459edc4bbd2353895acfb0f57eeddaa3.png)](https://medium.com/@outerrencedl?source=post_page-----33712602de0c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----33712602de0c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----33712602de0c--------------------------------)
    [Tingsong Ou](https://medium.com/@outerrencedl?source=post_page-----33712602de0c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa7aefc686327&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-reinforcement-learning-racetrack-exercise-building-the-environment-33712602de0c&user=Tingsong+Ou&userId=a7aefc686327&source=post_page-a7aefc686327----33712602de0c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----33712602de0c--------------------------------)
    ·10 min read·Jul 23, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F33712602de0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-reinforcement-learning-racetrack-exercise-building-the-environment-33712602de0c&user=Tingsong+Ou&userId=a7aefc686327&source=-----33712602de0c---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F33712602de0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-reinforcement-learning-racetrack-exercise-building-the-environment-33712602de0c&source=-----33712602de0c---------------------bookmark_footer-----------)![](../Images/9a54dfbfe19b0a23290550dc0cad6d9e.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image generated by Midjourney with a paid subscription, which complies general
    commercial terms [1].
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the section *Off-policy Monte Carlo Control* of the book *Reinforcement
    Learning: An Introduction 2nd Edition (page 112)*, the author left us with an
    interesting exercise: using the weighted importance sampling off-policy Monte
    Carlo method to find the fastest way driving on both tracks. This exercise is
    comprehensive that asks us to consider and build almost every component of a reinforcement
    learning task, like the environment, agent, reward, actions, conditions of termination,
    and the algorithm. Solving this exercise is fun and helps us build a solid understanding
    of the interaction between algorithm and environment, the importance of a correct
    episodic task definition, and how the value initialization affects the training
    outcome. Through this post, I hope to share my understanding and solution to this
    exercise with everyone interested in reinforcement learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Exercise requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned above, this exercise asks us to find a policy that makes a race
    car drive from the starting line to the finishing line as fast as possible without
    running into gravel or off the track. After carefully reading the exercise descriptions,
    I listed some key points that are essential to complete this task:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Map representation**: maps in this context are actually 2D matrices with
    (row_index, column_index) as coordinates. The value of each cell represents the
    state of that cell; for instance, we can use 0 to describe gravel, 1 for the track
    surface, 0.4 for the starting region, and 0.8 for the finishing line. Any row
    and column index outside the matrix can be considered as out-of-boundary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Car representation**: we can directly use the matrix’s coordinates to represent
    the car’s position;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Velocity and control**: the velocity space is discrete and consists of horizontal
    and vertical speeds that can be represented as a tuple (row_speed, col_speed).
    The speed limit on both axes is (-5, 5) and incremented by +1, 0, and -1 on each
    axis in each step; therefore, there are a total of 9 possible actions in each
    step. Literally, the speed cannot be both zero except at the starting line, and
    the vertical velocity, or row speed, cannot be negative as we don’t want our car
    to drive back to the starting line.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reward and episode**: the reward for each step before crossing the finishing
    line is -1\. When the car runs out of the track, it’ll be reset to one of the
    starting cells. The episode ends **ONLY** when the car successfully crosses the
    finishing line.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Starting states**: we randomly choose starting cell for the car from the
    starting line; the car’s initial speed is (0, 0) according to the exercise’s description.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Zero-acceleration challenge**: the author proposes a small *zero-acceleration
    challenge* that, at each time step, with 0.1 probability, the action will not
    take effect and the car will remain at its previous speed. We can implement this
    challenge in training instead of adding the feature to the environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The solution to the exercise is separated into two posts; in this post, we’ll
    focus on building a racetrack environment. The file structure of this exercise
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'And the libraries used in this implementation are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Building track maps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can represent track maps as 2D matrices with different values indicating
    track states. I want to be loyal to the exercise, so I’m trying to build the same
    maps shown in the book by assigning matrix values manually. The maps will be saved
    as separate *.npy* files so that the environment can read the file in training
    instead of generating them in the runtime.
  prefs: []
  type: TYPE_NORMAL
- en: The two maps look as follows; the light cells are gravel, the dark cells are
    track surfaces, the green-ish cells represent the finishing line, and the light-blue-ish
    cells represent the starting line.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a014b029de744b7f65c750a5ad033e42.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1 track maps with the 2D matrix representation. Source: Image by the
    author'
  prefs: []
  type: TYPE_NORMAL
- en: Building Gym-like environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With the track maps ready, we now proceed to create a gym-like environment
    with which the algorithm can interact. The [gymnasium](https://gymnasium.farama.org/),
    previously the OpenAI gym now belonging to the *Farama Foundation*, provides a
    simple interface for testing RL algorithms; we will use the *gymnasium* package
    to create the racetrack environment. Our environment will include the following
    components/features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**env.nS**: the shape of the observation space, which is (num_rows, num_cols,
    row_speed, col_speed) in this example. The number of rows and columns varies between
    maps, but the speed space are consistent across tracks. For the row speed, as
    we don’t want the car to go back to the starting line, the row speed observations
    consist of [-4, -3, -2, -1, 0] (negative value because we expect the car to go
    upwards in the map), five spaces in total; the column speed has no such limit,
    so the observations range from -4 to 4, nine spaces in total. Therefore, the shape
    of the observation space in the racetrack example is (num_rows, num_cols, 5, 9).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**env.nA**: the number of possible actions. There are 9 possible actions in
    our implementation; we will also create a dictionary in the environment class
    to map the integer action to the (row_speed, col_speed) tuple representation to
    control the agent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**env.reset()**: the function that takes the car back to one of the starting
    cells when the episode finishes or the car runs out of the track;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**env.step(action)**: the step function enables the algorithm to interact with
    the environment by taking an action and returning a tuple of (next_state, reward,
    is_terminated, is_truncated).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'State-checking functions: there’re two private functions that help to check
    if the car left the track or crossed the finishing line;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rendering functions: rendering function is essential to the customized environment
    from my point of view; I always check if the environment has properly been built
    by rendering out the game space and agent’s behaviors, which is more straightforward
    than just staring at logging information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these features in mind, we can start building the racetrack environment.
  prefs: []
  type: TYPE_NORMAL
- en: Checking the environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, with everything needed for the racetrack environment ready, we can test/verify
    our environment setup.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we render out the game-playing to check if every component is working
    smoothly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a9a2a779ce71a61e4ee17c2ed7f0eecc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2 Agents driving on both tracks with random policy. Source: Gif by the
    author'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we turn off the render option to make the environment run background to
    check if the trajectory terminates properly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Off-policy Monte Carlo Control Algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With the environment ready, we can prepare to implement the off-policy MC control
    algorithm with weighted importance sampling algorithm, as show below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6b96023d481220e61c1b1e7f0710b1c3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3 Off-policy Comte Carlo Control Algorithm. Source: Algorithm written
    in [latex](https://gist.github.com/terrence-ou/67aad132f41975c1cb3aad460a03ac88)
    by the author.'
  prefs: []
  type: TYPE_NORMAL
- en: The Monte Carlo methods solve the reinforcement learning problem by averaging
    sample returns. In training, the method generates a trajectory based on a given
    policy and learns from the tail of each episode. The difference between on-policy
    and off-policy learning is that the on-policy methods use one policy to make decisions
    and improvements, whereas the off-policy methods use separate policies for generating
    data and policy improvement. According to the author of the book, almost all off-policy
    methods utilize importance sampling to estimate expected values under one distribution
    given samples from another. [2]
  prefs: []
  type: TYPE_NORMAL
- en: The following several sections will explain tricks of soft policy and weighted
    importance sampling appearing in the algorithm above and how essential a proper
    value initialization is to this task.
  prefs: []
  type: TYPE_NORMAL
- en: Target policy and behavior policy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The off-policy method of this algorithm uses two policies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Target policy**: the policy being learned about. In this algorithm, the target
    policy is greedy and deterministic, which means the probability of the greedy
    action *A* at time *t* is 1.0, with all other actions’ probability equal to 0.0\.
    The target policy follows the Generalized Policy Iteration (GPI) that improves
    after every step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Behavior policy**: the policy used to generate behavior. The behavior policy
    in this algorithm is defined as *soft policy*, meaning that all actions in a given
    state have a non-zero probability. We’ll use the ε-greedy policy as our behavior
    policy here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In soft policy, the probability of an action is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ea7d0d6e471fc982b4b4730148a83983.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We should also return this probability when generating actions for the importance
    sampling purpose. The code for the behavior policy looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Weighted importance sampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We estimate the relative probability of the trajectory generated by the target
    policy under the trajectory from the behavior policy, and the equation for such
    probability is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bd966b843cf9d67c29ecb0ebd67d78ce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The value function for the weighted importance sampling is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1803ba37915eb930a53a4fce6d1ef363.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can reframe the equation to fit it to the episodic task with the form of
    incremental updates:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8ea43a0e47e279128a1b6e4c489e6043.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are a lot of excellent examples of how to derivate the above equation,
    so we don’t spend time deriving it here again. But I do also want to explain the
    interesting tricks about the last several lines of the algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c3e3d57e2be46a0aeecf06b0587173e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4 The trick in the off-policy MC control algorithm. Source: Image by
    the author'
  prefs: []
  type: TYPE_NORMAL
- en: The trick is based on the setting that the target policy here is deterministic.
    If the action taken at a time step differs from that comes from the target policy,
    then the probability of that action w.r.t the target policy is 0.0; thus, all
    the proceeding steps no longer update to the state-action value function of the
    trajectory. On the contrary, if the action at that time step is the same as the
    target policy, then the probability is 1.0, and the action-value update continues.
  prefs: []
  type: TYPE_NORMAL
- en: Weight initialization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Proper weight initialization plays an important role in successfully solving
    this exercise. Let’s first take a look at the rewards on both tracks from a random
    policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The reward at each time step before crossing the finishing line is -1\. At the
    early stage of training, the reward will be large in negative values; if we initialize
    a state-action value to 0 or random values from a normal distribution, say, with
    a mean of 0 and variance of 1, the value then could be considered too optimistic
    that might take a very long time for the program to find an optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: With several tests, I found a normal distribution with a mean of -500 and a
    variance of 1 could be effective values for a faster convergence; you are encouraged
    to try other values and can definitely find a better initial value than this.
  prefs: []
  type: TYPE_NORMAL
- en: Solving the Problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With all of the thoughts above in mind, we can program out the Off-policy Monte
    Carlo control function and use it to solve the racetrack exercise. We’ll also
    add the zero-acceleration challenge that is mentioned in the exercise description.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we train the policies for 1,000,000 episodes without/with zero acceleration
    challenge on both tracks and evaluate them with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: By plotting out the training record, we find that the policy converges around
    the 10,000th episode on both tracks without considering zero acceleration; adding
    zero acceleration makes the convergence slower and unstable before the 100,000th
    episode but still converges afterward.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6918a1b931aaa5fcb033677405aef3c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5 Training reward history of track A. Source: Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3ca458fa857f9d38822990255cd46b5a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6 Training reward history of track B. Source: Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, we can ask the agents to play the game with trained policies, and we
    also plot out the possible trajectories to further check the correctness of the
    policies.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e1ade388e7ec2b717df36aeb436acef9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7 Agents driving on both tracks based on trained policies. Source: Gif
    by the author'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sample trajectories for track a:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a65edebe8370063de691a667a4a6651b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8 Sample optimal trajectories for track A. Source: Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sample trajectories for track b:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4771002e1893aaab88bbe7483a4eec01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9 Sample optimal trajectories for track B. Source: Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: So far, we’ve solved the racetrack exercise. This implementation could still
    have some problems, and you’re very welcome to point them out and discuss a better
    solution in the comment. Thanks for reading!
  prefs: []
  type: TYPE_NORMAL
- en: Reference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Midjourney Terms of Service: [https://docs.midjourney.com/docs/terms-of-service](https://docs.midjourney.com/docs/terms-of-service)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Sutton, Richard S., and Andrew G. Barto. *Reinforcement learning: An introduction*.
    MIT Press, 2018.'
  prefs: []
  type: TYPE_NORMAL
- en: 'My GitHub repo of this exercise: [[Link]](https://github.com/terrence-ou/Reinforcement-Learning-2nd-Edition-Notes-Codes/tree/main/chapter_05_monte_carlo_methods);
    please check the “exercise section”.'
  prefs: []
  type: TYPE_NORMAL
