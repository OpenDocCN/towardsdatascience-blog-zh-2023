- en: Solving Bottlenecks on the Data Input Pipeline with PyTorch Profiler and TensorBoard
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PyTorch Profiler 和 TensorBoard 解决数据输入管道瓶颈
- en: 原文：[https://towardsdatascience.com/solving-bottlenecks-on-the-data-input-pipeline-with-pytorch-profiler-and-tensorboard-5dced134dbe9?source=collection_archive---------2-----------------------#2023-08-26](https://towardsdatascience.com/solving-bottlenecks-on-the-data-input-pipeline-with-pytorch-profiler-and-tensorboard-5dced134dbe9?source=collection_archive---------2-----------------------#2023-08-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/solving-bottlenecks-on-the-data-input-pipeline-with-pytorch-profiler-and-tensorboard-5dced134dbe9?source=collection_archive---------2-----------------------#2023-08-26](https://towardsdatascience.com/solving-bottlenecks-on-the-data-input-pipeline-with-pytorch-profiler-and-tensorboard-5dced134dbe9?source=collection_archive---------2-----------------------#2023-08-26)
- en: PyTorch Model Performance Analysis and Optimization — Part 4
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch 模型性能分析与优化——第四部分
- en: '[](https://chaimrand.medium.com/?source=post_page-----5dced134dbe9--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page-----5dced134dbe9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5dced134dbe9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5dced134dbe9--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page-----5dced134dbe9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://chaimrand.medium.com/?source=post_page-----5dced134dbe9--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page-----5dced134dbe9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5dced134dbe9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5dced134dbe9--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page-----5dced134dbe9--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9440b37e27fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-bottlenecks-on-the-data-input-pipeline-with-pytorch-profiler-and-tensorboard-5dced134dbe9&user=Chaim+Rand&userId=9440b37e27fe&source=post_page-9440b37e27fe----5dced134dbe9---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5dced134dbe9--------------------------------)
    ·11 min read·Aug 26, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5dced134dbe9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-bottlenecks-on-the-data-input-pipeline-with-pytorch-profiler-and-tensorboard-5dced134dbe9&user=Chaim+Rand&userId=9440b37e27fe&source=-----5dced134dbe9---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9440b37e27fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-bottlenecks-on-the-data-input-pipeline-with-pytorch-profiler-and-tensorboard-5dced134dbe9&user=Chaim+Rand&userId=9440b37e27fe&source=post_page-9440b37e27fe----5dced134dbe9---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5dced134dbe9--------------------------------)
    ·11分钟阅读·2023年8月26日'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5dced134dbe9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-bottlenecks-on-the-data-input-pipeline-with-pytorch-profiler-and-tensorboard-5dced134dbe9&source=-----5dced134dbe9---------------------bookmark_footer-----------)![](../Images/6d1d0873c895da273752a2327c322b7c.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5dced134dbe9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-bottlenecks-on-the-data-input-pipeline-with-pytorch-profiler-and-tensorboard-5dced134dbe9&source=-----5dced134dbe9---------------------bookmark_footer-----------)![](../Images/6d1d0873c895da273752a2327c322b7c.png)'
- en: Photo by [Alexander Grey](https://unsplash.com/@sharonmccutcheon?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Alexander Grey](https://unsplash.com/@sharonmccutcheon?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'This is the fourth post in our series of posts on the topic of performance
    analysis and optimization of GPU-based PyTorch workloads. Our focus in this post
    will be on the training **data input pipeline**. In a typical training application,
    the host’s CPUs load, pre-process, and collate data before feeding it into the
    GPU for training. Bottlenecks in the input pipeline occur when the host is not
    able to keep up with the speed of the GPU. This results in the GPU — the *most
    expensive* resource in the training setup — remaining idle for periods of time
    while it waits for data input from the *overly tasked* host. In previous posts
    (e.g., [here](/overcoming-data-preprocessing-bottlenecks-with-tensorflow-data-service-nvidia-dali-and-other-d6321917f851))
    we have discussed input pipeline bottlenecks in detail and reviewed different
    ways of addressing them, such as:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们关于 GPU 基于 PyTorch 工作负载的性能分析和优化系列文章中的第四篇。我们将在这篇文章中重点关注训练**数据输入管道**。在典型的训练应用中，主机的
    CPU 加载、预处理和整理数据，然后将其送入 GPU 进行训练。当主机无法跟上 GPU 的速度时，输入管道中会出现瓶颈。这会导致 GPU —— 训练设置中*最昂贵*的资源——在等待来自*过度负荷*主机的数据输入时闲置。在之前的文章中（例如，[这里](/overcoming-data-preprocessing-bottlenecks-with-tensorflow-data-service-nvidia-dali-and-other-d6321917f851)），我们详细讨论了输入管道瓶颈，并回顾了应对这些瓶颈的不同方法，例如：
- en: Choosing a training instance with a CPU to GPU compute ratio that is more suited
    to your workload (e.g., see our [previous post](/instance-selection-for-deep-learning-7463d774cff0)
    on tips for choosing the best instance type for your ML workload),
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个 CPU 到 GPU 计算比率更适合你的工作负载的训练实例（例如，参见我们 [之前的帖子](/instance-selection-for-deep-learning-7463d774cff0)
    关于选择最适合你的 ML 工作负载的实例类型的提示），
- en: Improving the workload balance between the CPU and GPU by moving some of the
    CPU pre-processing activity to the GPU, and
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将一些 CPU 预处理活动转移到 GPU 来改善 CPU 和 GPU 之间的工作负载平衡，并
- en: Offloading some of the CPU computation to auxiliary CPU-worker devices (e.g.,
    see [here](/overcoming-ml-data-preprocessing-bottlenecks-with-grpc-ca30fdc01bee)).
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将部分 CPU 计算任务转移到辅助 CPU 工作设备（例如，参见 [这里](/overcoming-ml-data-preprocessing-bottlenecks-with-grpc-ca30fdc01bee)）。
- en: Of course, **the first step to addressing a performance bottleneck in the data-input
    pipeline is to identify and understand it**. In this post we will demonstrate
    how this can be done using [PyTorch Profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html)
    and its associated [TensorBoard plugin](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，**解决数据输入管道中的性能瓶颈的第一步是识别并理解它**。在这篇文章中，我们将演示如何使用 [PyTorch Profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html)
    及其相关的 [TensorBoard 插件](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html)
    来实现这一点。
- en: As in our previous posts, we will define a toy PyTorch model and then *iteratively*
    profile its performance, identify bottlenecks, and attempt to fix them. We will
    run our experiments on an [Amazon EC2 g5.2xlarge](https://aws.amazon.com/ec2/instance-types/g5/)
    instance (containing an NVIDIA A10G GPU and 8 vCPUs) and using the official [AWS
    PyTorch 2.0 Docker image](https://github.com/aws/deep-learning-containers). Keep
    in mind that some of the behaviors we describe may vary between versions of PyTorch.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前的帖子中所述，我们将定义一个玩具 PyTorch 模型，然后*迭代地*分析其性能，识别瓶颈，并尝试解决它们。我们将在一个 [Amazon EC2
    g5.2xlarge](https://aws.amazon.com/ec2/instance-types/g5/) 实例上运行我们的实验（该实例包含 NVIDIA
    A10G GPU 和 8 个 vCPUs），并使用官方的 [AWS PyTorch 2.0 Docker 镜像](https://github.com/aws/deep-learning-containers)。请记住，我们描述的某些行为可能因
    PyTorch 版本不同而有所变化。
- en: Many thanks to [Yitzhak Levi](https://www.linkedin.com/in/yitzhak-levi-49a217201/)
    for his contributions to this post.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢 [Yitzhak Levi](https://www.linkedin.com/in/yitzhak-levi-49a217201/) 对这篇文章的贡献。
- en: Toy Model
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 玩具模型
- en: In the following blocks we introduce the toy example we will use for our demonstration.
    We start by defining a simple image classification model. The input to the model
    is a batch of *256*x*256* YUV images and the output is its associated batch of
    semantic class predictions.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们介绍了我们将用于演示的玩具示例。我们首先定义了一个简单的图像分类模型。模型的输入是一个*256*x*256*的 YUV 图像批次，输出是其相关的语义类别预测批次。
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The code block below contains our dataset definition. Our dataset contains ten
    thousand JPEG-image file-paths and their associated (randomly generated) semantic
    labels. To simplify our demonstration, we will assume that all of the JPEG-file
    paths point to the same image — the picture of the colorful “bottlenecks” at the
    top of this post.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码块包含我们的数据集定义。我们的数据集包含一万张 JPEG 图像的文件路径及其相关（随机生成的）语义标签。为了简化演示，我们将假设所有 JPEG
    文件路径指向相同的图像——本文顶部的多彩“瓶颈”图片。
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that we have wrapped the file reader with a [torch.profiler.record_function](https://pytorch.org/tutorials/beginner/profiler.html#performance-debugging-using-profiler)
    context manager.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们已经使用 [torch.profiler.record_function](https://pytorch.org/tutorials/beginner/profiler.html#performance-debugging-using-profiler)
    上下文管理器包装了文件读取器。
- en: 'Our input data pipeline includes the following transformations on the image:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的输入数据管道包括对图像进行以下转换：
- en: '[PILToTensor](https://pytorch.org/vision/main/generated/torchvision.transforms.PILToTensor.html)
    converts the PIL image to a PyTorch Tensor.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[PILToTensor](https://pytorch.org/vision/main/generated/torchvision.transforms.PILToTensor.html)
    将 PIL 图像转换为 PyTorch Tensor。'
- en: '[RandomCrop](https://pytorch.org/vision/0.8/transforms.html#torchvision.transforms.RandomCrop)
    returns a *256*x*256* crop at a random offset in the image.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[RandomCrop](https://pytorch.org/vision/0.8/transforms.html#torchvision.transforms.RandomCrop)
    在图像的随机偏移处返回一个 *256*x*256* 的裁剪区域。'
- en: RandomMask is a custom transform that creates a random *256*x*256* boolean mask
    and applies it to the image. The transform includes a four-neighbor dilation operation
    on the mask.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RandomMask 是一种自定义转换，创建一个随机的 *256*x*256* 布尔掩码并将其应用于图像。该转换包括对掩码的四邻域膨胀操作。
- en: ConvertColor is a custom transformation that converts the image format from
    RGB to YUV.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ConvertColor 是一种自定义转换，将图像格式从 RGB 转换为 YUV。
- en: Scale is a custom transformation that scales the pixels to the range [0,1].
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Scale 是一种自定义转换，将像素缩放到 [0,1] 范围内。
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We chain the transformations using the [Compose](https://pytorch.org/vision/0.8/transforms.html#torchvision.transforms.Compose)
    class which we have modified slightly to include a [torch.profiler.record_function](https://pytorch.org/tutorials/beginner/profiler.html#performance-debugging-using-profiler)
    context manager around each of the transform invocations.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 [Compose](https://pytorch.org/vision/0.8/transforms.html#torchvision.transforms.Compose)
    类链接这些转换，我们稍微修改了它，以在每个转换调用周围包含一个 [torch.profiler.record_function](https://pytorch.org/tutorials/beginner/profiler.html#performance-debugging-using-profiler)
    上下文管理器。
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the code block below we define the dataset and dataloader. We configure the
    [DataLoader](https://pytorch.org/docs/stable/data.html) to use a [custom collate
    function](https://pytorch.org/docs/stable/data.html#working-with-collate-fn) in
    which we wrap the [default collate function](https://pytorch.org/docs/stable/data.html#torch.utils.data.default_collate)
    with a [torch.profiler.record_function](https://pytorch.org/tutorials/beginner/profiler.html#performance-debugging-using-profiler)
    context manager.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码块中，我们定义了数据集和数据加载器。我们将 [DataLoader](https://pytorch.org/docs/stable/data.html)
    配置为使用 [自定义 collate 函数](https://pytorch.org/docs/stable/data.html#working-with-collate-fn)，在其中我们使用
    [torch.profiler.record_function](https://pytorch.org/tutorials/beginner/profiler.html#performance-debugging-using-profiler)
    上下文管理器包装 [默认 collate 函数](https://pytorch.org/docs/stable/data.html#torch.utils.data.default_collate)。
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Lastly, we define the model, loss function, optimizer, and the training loop,
    which we wrap with a [profiler context manager](https://pytorch.org/docs/stable/profiler.html#torch.profiler.profile).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义了模型、损失函数、优化器和训练循环，并用 [profiler 上下文管理器](https://pytorch.org/docs/stable/profiler.html#torch.profiler.profile)
    包装。
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the following sections we will use [PyTorch Profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html)
    and its associated [TensorBoard plugin](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html)
    in order to assess the performance of our model. Our focus will be on the *Trace
    View* of the profiler report. Please see the [first post](/pytorch-model-performance-analysis-and-optimization-10c3c5822869)
    in our series for a demonstration of how to use the other sections of the report.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将使用 [PyTorch Profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html)
    及其相关的 [TensorBoard 插件](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html)
    来评估我们模型的性能。我们的重点将是分析器报告的 *Trace View*。有关如何使用报告的其他部分，请参阅我们系列中的 [第一篇文章](/pytorch-model-performance-analysis-and-optimization-10c3c5822869)。
- en: Initial Performance Results
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初步性能结果
- en: 'The average step time reported by the script we defined is 1.3 seconds and
    the average GPU utilization is a very low 18.21%. In the image below we capture
    the performance results as displayed in the [TensorBoard plugin](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html)
    *Trace View*:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义的脚本报告的平均步长时间为 1.3 秒，平均 GPU 利用率非常低，为 18.21%。在下面的图像中，我们捕捉了在 [TensorBoard 插件](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html)
    *跟踪视图* 中显示的性能结果。
- en: '![](../Images/d88379983c412418b10a9b62114465e9.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d88379983c412418b10a9b62114465e9.png)'
- en: Trace View of Baseline Model (Captured by Author)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 基线模型的跟踪视图（作者拍摄）
- en: We can see that every fourth training step includes a long (~5.5 second) period
    of data-loading during which the GPU is completely idle. The reason that this
    occurs on every fourth step is directly related to the number of DataLoader workers
    we chose — four. Every fourth step we find all of the workers busy producing the
    samples for the next batch while the GPU waits. This is a clear indication of
    a bottleneck in the data input pipeline. The question is how do we analyze it?
    Complicating matters is the fact that the many [record_function](https://pytorch.org/tutorials/beginner/profiler.html#performance-debugging-using-profiler)
    markers that we inserted into the code are nowhere to be found in the profile
    trace.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到每隔四步训练就会出现一个较长的 (~5.5 秒) 数据加载期间，在此期间GPU完全空闲。这种现象发生在每隔四步的原因与我们选择的DataLoader工作线程数量—四个—直接相关。每隔四步，我们发现所有工作线程都在忙于生成下一批次的样本，而GPU则在等待。这清楚地表明数据输入管道中存在瓶颈。问题是我们该如何分析它？使问题复杂化的是，我们插入代码中的许多
    [record_function](https://pytorch.org/tutorials/beginner/profiler.html#performance-debugging-using-profiler)
    标记在性能分析轨迹中找不到。
- en: The use of multiple workers in the DataLoader is **critical** for optimizing
    performance. Unfortunately, it also makes the profiling process more difficult.
    Although there exist profilers that support multi-process analysis (e.g., check
    out [VizTracer](https://viztracer.readthedocs.io/en/latest/concurrency.html)),
    the approach we will take in this post is to run, analyze, and optimize our model
    **in single-process mode** (i.e., with zero DataLoader workers) and then apply
    the optimizations to the **multi-worker mode**. Admittedly, optimizing the speed
    of a standalone function does not guarantee that multiple (parallel) invocations
    of the same function will also benefit. However, as we will see in this post,
    this strategy will enable us to identify and address some core issues that we
    were *not* able to identify otherwise, and, at least with regards to the issues
    discussed here, we *will* find a strong correlation between the performance impacts
    on the two modes. But just before we apply this strategy, let us tune our choice
    of the number of workers.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在DataLoader中使用多个工作线程对优化性能**至关重要**。不幸的是，这也使得性能分析过程变得更加困难。虽然存在支持多进程分析的性能分析工具（例如，查看
    [VizTracer](https://viztracer.readthedocs.io/en/latest/concurrency.html)），但我们在本文中采用的方法是以**单进程模式**（即，零DataLoader工作线程）运行、分析和优化我们的模型，然后将优化应用到**多工作线程模式**。诚然，优化一个独立函数的速度并不能保证多个（并行）调用相同函数也会有同样的效果。然而，正如我们在本文中将看到的，这一策略将帮助我们识别并解决一些我们*无法*通过其他方式识别的核心问题，并且，至少在这里讨论的问题方面，我们*将*发现这两种模式的性能影响之间有很强的关联。但在应用这一策略之前，让我们调整工作线程的数量选择。
- en: 'Optimization 1: Tune the multi-processing strategy'
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化 1：调整多进程策略
- en: Determining the optimal number of threads or processes in a multi-process/multi-threaded
    application, such as ours, can be tricky. On the one hand, if we choose a number
    that is too low, we might end up under-utilizing the CPU resources. On the other
    hand, if we go too high, we run the risk of *thrashing*, an undesired situation
    in which the operating system spends most of its time managing the multiple threading/processing
    rather than running our code. In the case of a PyTorch training workload, it is
    recommended to test out different choices for the DataLoader *num_workers* setting.
    A good place to start is to set the number based on the number of CPUs on the
    host, (e.g., *num_workers*:=*num_cpus/num_gpus*). In our case the [Amazon EC2
    g5.2xlarge](https://aws.amazon.com/ec2/instance-types/g5/) has eight vCPUs and,
    in fact, increasing the number of DataLoader workers to eight results in a slightly
    better average step time of 1.17 seconds (an 11% boost).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 确定多进程/多线程应用程序（例如我们的应用程序）中的最佳线程或进程数量可能会很棘手。一方面，如果我们选择的数量太少，可能会导致CPU资源未被充分利用。另一方面，如果数量设置得过高，我们就有可能面临*thrashing*的风险，这是一个不希望出现的情况，操作系统花费大部分时间来管理多个线程/进程，而不是运行我们的代码。在PyTorch训练工作负载的情况下，建议尝试不同的DataLoader
    *num_workers*设置。一个好的起点是根据主机上的CPU数量来设置这个数字（例如，*num_workers*:=*num_cpus/num_gpus*）。在我们的案例中，[Amazon
    EC2 g5.2xlarge](https://aws.amazon.com/ec2/instance-types/g5/)有八个vCPUs，实际上，将DataLoader的工作线程数量增加到八个会导致稍微更好的平均步骤时间为1.17秒（提升11%）。
- en: Importantly, look out for other, less obvious, configuration settings that might
    impact the number of threads or processes being used by the data-input pipeline.
    For example, [opencv-python](https://pypi.org/project/opencv-python/), a library
    commonly used for image pre-processing in computer vision workloads, includes
    the [cv2.setNumThreads(int)](https://medium.com/@rachittayal7/a-note-on-opencv-threads-performance-in-prod-d10180716fba)
    function for controlling the number of threads.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，要留意其他不太明显的配置设置，这些设置可能会影响数据输入管道使用的线程或进程数量。例如，[opencv-python](https://pypi.org/project/opencv-python/)，这是一个常用于计算机视觉工作负载中的图像预处理的库，其中包括用于控制线程数量的[cv2.setNumThreads(int)](https://medium.com/@rachittayal7/a-note-on-opencv-threads-performance-in-prod-d10180716fba)函数。
- en: In the image below we capture a portion of the *Trace View* when running the
    script with *num_workers* set to **zero**.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图像中，我们捕捉到在将*num_workers*设置为**零**时运行脚本的*Trace View*的一部分。
- en: '![](../Images/535011e9c86417421f028e452be9a593.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/535011e9c86417421f028e452be9a593.png)'
- en: Trace View of Baseline Model in Single-process Mode (Captured by Author)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 单进程模式下基线模型的跟踪视图（作者拍摄）
- en: Running the script in this manner enables us to see the *record_function* labels
    we set and to identify the *RandomMask* transform, or more specifically our *dilation*
    function, as the most time-consuming operation in the retrieval of each individual
    sample.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式运行脚本使我们能够看到我们设置的*record_function*标签，并识别出*RandomMask*变换，或者更具体地说，我们的*dilation*函数，作为检索每个单独样本时最耗时的操作。
- en: 'Optimization 2: Optimize the dilation function'
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化2：优化膨胀函数
- en: Our current implementation of the dilation function uses a 2D convolution, typically
    implemented using matrix multiplication and not known to run especially fast on
    CPU. One option would be to run the dilation on the GPU (as described in [this
    post](/overcoming-data-preprocessing-bottlenecks-with-tensorflow-data-service-nvidia-dali-and-other-d6321917f851)).
    However, the overhead involved in the host-device transaction would likely outweigh
    the potential performance gains from this type of solution, not to mention that
    we prefer not to increase the load on the GPU.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当前的膨胀函数实现使用2D卷积，通常使用矩阵乘法实现，并且在CPU上运行速度不是特别快。一个选项是将膨胀操作在GPU上运行（如[这篇文章](/overcoming-data-preprocessing-bottlenecks-with-tensorflow-data-service-nvidia-dali-and-other-d6321917f851)中所述）。然而，主机与设备之间的事务开销可能会超过这种解决方案带来的潜在性能提升，更不用说我们不愿增加GPU的负担。
- en: 'In the code block below we propose an alternative, more CPU-friendly, implementation
    of the dilation function that uses boolean operations instead of a convolution:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码块中，我们提出了一种更为CPU友好的膨胀函数实现，它使用布尔操作而不是卷积：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Following this modification, our step time drops to 0.78 seconds, which amounts
    to an additional 50% improvement. The update single-process *Trace-View* is displayed
    below:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 经过这种修改后，我们的步骤时间降至0.78秒，相当于额外提高了50%。更新后的单进程*Trace-View*如下所示：
- en: '![](../Images/5ec0eeb29fc63b3c03de5b1c9fd86dd4.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5ec0eeb29fc63b3c03de5b1c9fd86dd4.png)'
- en: Trace View Following Dilation Optimization in Single-process Mode (Captured
    by Author)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the dilation operation has shrunk significantly and that the
    most time-consuming operation is now the [PILToTensor](https://pytorch.org/vision/main/generated/torchvision.transforms.PILToTensor.html)
    transform.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'A closer look at the [PILToTensor](https://pytorch.org/vision/main/generated/torchvision.transforms.PILToTensor.html)
    function (see [here](https://pytorch.org/vision/stable/_modules/torchvision/transforms/functional.html#pil_to_tensor))
    reveals three underlying operations:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Loading the PIL image — due to lazy loading property of [Image.open](https://pillow.readthedocs.io/en/latest/reference/Image.html#PIL.Image.open),
    the image is loaded here.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The PIL image is converted to a numpy array.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The numpy array is converted to a PyTorch Tensor.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Although the image loading takes the majority of time, we note the extreme wastefulness
    of applying the subsequent operations to the full-size image only to crop it immediately
    afterwards. This leads us to our next optimization.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 'Optimization 3: Reorder transformations'
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Fortunately, the [RandomCrop](https://pytorch.org/vision/0.8/transforms.html#torchvision.transforms.RandomCrop)
    transformation can be applied directly to the PIL image enabling us to apply the
    image-size reduction as the first operation on our pipeline:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Following this optimization our step time drops to 0.72 seconds, an additional
    8% optimization. The *Trace View* capture below shows that the RandomCrop transformation
    is now the dominant operation:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f327927acfd400caac441940988a887a.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: Trace View Following Transformation Reordering in Single-process Mode (Captured
    by Author)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: In reality, as before, it is actually the PIL image (lazy) loading that causes
    the bottleneck, not the random crop.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, we would be able to optimize this further by limiting the read operation
    to only the crop in which we are interested. Unfortunately, as of the time of
    this writing, [torchvision](https://pytorch.org/vision/stable/index.html) does
    not support this option. In a future post we will demonstrate how we can overcome
    this shortcoming by implementing our own [custom](https://pytorch.org/tutorials/advanced/cpp_extension.html)
    *decode_and_crop* PyTorch operator.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'Optimization 4: Apply batch transformations'
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In our current implementation, each of the image transformations are applied
    on each image individually. However, some transformations may run more optimally
    when applied on the entire batch at once. In the code block below we modify our
    pipeline so that the ColorTransformation and Scale transforms are applied on image
    **batches** within our [custom collate function](https://pytorch.org/docs/stable/data.html#working-with-collate-fn):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The result of this change is actually a slight increase in the step time, to
    0.75 seconds. Although unhelpful in the case of our toy model, the ability to
    apply certain operations as batch transforms rather than per-sample transforms
    carries the potential to optimize certain workloads.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Results
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The successive optimizations we have applied in this post resulted in an 80%
    improvement in runtime performance. However, although less severe, there still
    remains a bottleneck in the input pipeline and the GPU remains highly under-utilized
    (~30%). Please revisit our previous posts (e.g., [here](/overcoming-data-preprocessing-bottlenecks-with-tensorflow-data-service-nvidia-dali-and-other-d6321917f851))
    for additional methods of addressing such issues.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this post we have focused on performance issues in the training-data input
    pipeline. As in our previous posts in this series we have chosen PyTorch Profiler
    and its associated TensorBoard plugin as our weapons of choice and demonstrated
    their use in accelerating the speed of training. In particular, we showed how
    running the DataLoader with zero workers increases our ability to identify, analyze,
    and optimize bottlenecks on the data-input pipeline.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: As in our previous posts, we emphasize that the path to successful optimization
    will vary greatly based on the details of the training project, including the
    model architecture and training environment. In practice, reaching your goals
    may be more difficult than in the example we presented here. Some of the techniques
    we described may have little impact on your performance or might even make it
    worse. We also note that the precise optimizations that we chose, and the order
    in which we chose to apply them, was somewhat arbitrary. You are highly encouraged
    to develop your own tools and techniques for reaching your optimization goals
    based on the specific details of your project.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
