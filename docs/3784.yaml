- en: An Intuition for How Models like ChatGPT Work
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于模型如ChatGPT的工作原理的直观理解
- en: 原文：[https://towardsdatascience.com/an-intuition-for-how-models-like-chatgpt-work-c7f01616bd6d?source=collection_archive---------1-----------------------#2023-12-30](https://towardsdatascience.com/an-intuition-for-how-models-like-chatgpt-work-c7f01616bd6d?source=collection_archive---------1-----------------------#2023-12-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/an-intuition-for-how-models-like-chatgpt-work-c7f01616bd6d?source=collection_archive---------1-----------------------#2023-12-30](https://towardsdatascience.com/an-intuition-for-how-models-like-chatgpt-work-c7f01616bd6d?source=collection_archive---------1-----------------------#2023-12-30)
- en: Providing an intuition on the ideas behind popular transformer models like ChatGPT
    and other large language models (LLMs)
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提供对流行的变换器模型如ChatGPT和其他大型语言模型（LLMs）背后思想的直观理解
- en: '[](https://dkhundley.medium.com/?source=post_page-----c7f01616bd6d--------------------------------)[![David
    Hundley](../Images/1779ef96ec3d338f8fe4a9567ba7b194.png)](https://dkhundley.medium.com/?source=post_page-----c7f01616bd6d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c7f01616bd6d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c7f01616bd6d--------------------------------)
    [David Hundley](https://dkhundley.medium.com/?source=post_page-----c7f01616bd6d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://dkhundley.medium.com/?source=post_page-----c7f01616bd6d--------------------------------)[![David
    Hundley](../Images/1779ef96ec3d338f8fe4a9567ba7b194.png)](https://dkhundley.medium.com/?source=post_page-----c7f01616bd6d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c7f01616bd6d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c7f01616bd6d--------------------------------)
    [David Hundley](https://dkhundley.medium.com/?source=post_page-----c7f01616bd6d--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F82498630db6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-intuition-for-how-models-like-chatgpt-work-c7f01616bd6d&user=David+Hundley&userId=82498630db6&source=post_page-82498630db6----c7f01616bd6d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c7f01616bd6d--------------------------------)
    ·10 min read·Dec 30, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc7f01616bd6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-intuition-for-how-models-like-chatgpt-work-c7f01616bd6d&user=David+Hundley&userId=82498630db6&source=-----c7f01616bd6d---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F82498630db6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-intuition-for-how-models-like-chatgpt-work-c7f01616bd6d&user=David+Hundley&userId=82498630db6&source=post_page-82498630db6----c7f01616bd6d---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c7f01616bd6d--------------------------------)
    ·10分钟阅读·2023年12月30日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc7f01616bd6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-intuition-for-how-models-like-chatgpt-work-c7f01616bd6d&user=David+Hundley&userId=82498630db6&source=-----c7f01616bd6d---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc7f01616bd6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-intuition-for-how-models-like-chatgpt-work-c7f01616bd6d&source=-----c7f01616bd6d---------------------bookmark_footer-----------)![](../Images/6298bdd024c30f5750f9870f35f92f5a.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc7f01616bd6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-intuition-for-how-models-like-chatgpt-work-c7f01616bd6d&source=-----c7f01616bd6d---------------------bookmark_footer-----------)![](../Images/6298bdd024c30f5750f9870f35f92f5a.png)'
- en: Title card created by the author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作者制作的标题卡
- en: As we wind down 2023, it’s incredible to think about how much Generative AI
    has already impacted our daily lives. Starting with ChatGPT’s release in November
    2022, this space has evolved so quickly that it’s hard to believe that it’s been
    just one year in which all these advancements have come out.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着2023年的结束，令人难以置信的是生成式AI已经对我们的日常生活产生了多大的影响。从2022年11月ChatGPT发布开始，这一领域的发展如此之快，以至于很难相信在短短一年内所有这些进展都出现了。
- en: While the results are quite amazing, the underlying complexity has led a lot
    of people to publicly speculate on how these large language models (LLMs) work.
    Some people have speculated that these models are pulling from a preformulated
    database of responses, and some have gone as far to speculate that these LLMs
    have gained a human-level of sentience. These are extreme stances, and as you
    might guess, both are incorrect.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管结果相当惊人，但其潜在的复杂性使得很多人公开猜测这些大型语言模型（LLMs）是如何工作的。有些人猜测这些模型是从预设的响应数据库中提取内容的，有些人甚至猜测这些LLMs已经达到了人类级别的意识。这些都是极端的观点，正如你可能猜到的，两者都不正确。
- en: You may have heard that these **LLMs are next-word predictors, meaning that
    they use probability to determine the next word that should come in a sentence**.
    This understanding is technically correct, but it’s a little too high level to
    sufficiently understand these models. In order to build a stronger intuition,
    we need to go deeper. **The intention of this post is to provide business leaders
    with a deep enough**…
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能听说过这些**大型语言模型（LLMs）是下一个词预测器，意思是它们使用概率来确定句子中应该出现的下一个词**。这个理解从技术上来说是正确的，但对于充分理解这些模型来说，略显高层次。为了建立更强的直觉，我们需要深入探讨。**这篇文章的意图是为商业领袖提供足够深入的理解**…
