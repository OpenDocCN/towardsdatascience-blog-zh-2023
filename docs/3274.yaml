- en: Cracking the Code LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/cracking-the-code-llms-354505c53295?source=collection_archive---------4-----------------------#2023-11-03](https://towardsdatascience.com/cracking-the-code-llms-354505c53295?source=collection_archive---------4-----------------------#2023-11-03)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How code LLMs progressed from RNNs to Transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@agarwal.shubham166?source=post_page-----354505c53295--------------------------------)[![Shubham
    Agarwal](../Images/4a41b8e70829943cd91d1104424f9ce5.png)](https://medium.com/@agarwal.shubham166?source=post_page-----354505c53295--------------------------------)[](https://towardsdatascience.com/?source=post_page-----354505c53295--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----354505c53295--------------------------------)
    [Shubham Agarwal](https://medium.com/@agarwal.shubham166?source=post_page-----354505c53295--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdd67f0fc7318&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcracking-the-code-llms-354505c53295&user=Shubham+Agarwal&userId=dd67f0fc7318&source=post_page-dd67f0fc7318----354505c53295---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----354505c53295--------------------------------)
    ·7 min read·Nov 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F354505c53295&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcracking-the-code-llms-354505c53295&user=Shubham+Agarwal&userId=dd67f0fc7318&source=-----354505c53295---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F354505c53295&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcracking-the-code-llms-354505c53295&source=-----354505c53295---------------------bookmark_footer-----------)![](../Images/95d03e0d607d3c3e4f897b86c3482843.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Markus Spiske](https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recent years have seen remarkable evolution of language models with the introduction
    of Transformers, which has revolutionized the way we perform our daily tasks like
    writing emails, creating documentations, searching the web and even the way we
    code. With researchers applying Large Language Models in code intelligence tasks,
    a new field of **Neural Code Intelligence** has emerged. This domain aims at improving
    programming efficiency and minimizing human errors in the software industry by
    solving tasks like code summarization, generation and translation.
  prefs: []
  type: TYPE_NORMAL
- en: With the latest release of Code Llama, the state of art model by Meta AI for
    code generation and understanding, this article looks back at the evolution of
    Large Language Models (LLMs) for Code, from RNNs to Transformers.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7cf34459d21a523b15464a4fd87d17ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig-1: A timeline for Large Language Models For Code. Image by Author.'
  prefs: []
  type: TYPE_NORMAL
- en: Code2Vec, 2018
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This was one of the first attempts for language models to understand code. [Code2Vec](https://arxiv.org/pdf/1803.09473.pdf)
    aimed at representing code snippets into embeddings. These embeddings capture
    semantic and structural information from the code, making them useful for various
    software engineering tasks such as code classification, retrieval, and understanding.
  prefs: []
  type: TYPE_NORMAL
- en: Model tries to predict the method name from the code-snippet, by encoding well-named
    tokens and AST (Abstract Syntax Tree) paths, and applying neural attention for
    aggregation into fixed length vector representation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1df35ea57f0c7130e459d84b7c3dd9d5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig-2: Code2Vec Model Architecture: Program is first decomposed into Bag of
    Context which includes tokens and AST paths, then through fully connected layer
    and attention layer to generate code-vector.Image inspired from the original paper
    by Uri Alon et. al from [Code2Vec](https://arxiv.org/pdf/1803.09473.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training Set:** 14M Java Program Examples'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Architecture**: RNN + Feed-Forward Network'
  prefs: []
  type: TYPE_NORMAL
- en: '**Novelty**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Path-based Attention Mode**l- The authors propose a novel neural network
    architecture that uses syntactic paths in the Abstract Syntax Tree (AST) of a
    code snippet as input features. The model learns to assign different attention
    weights to each path, and to aggregate them into a single code vector. The code
    vector can then be used to predict the label distribution for the snippet, or
    to measure similarity and analogy between snippets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can play with the model [here](https://code2vec.org/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: CodeBERT, 2020
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[CodeBERT](https://arxiv.org/pdf/2002.08155.pdf), developed by Microsoft Research
    team, represents a significant advancement in the realm of Large Language Models
    (LLMs) for code by introducing multimodal data pre-training, combining Natural
    Language and Programming Language (NL + PL) on the Transformer based [BERT model](/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270).
    The model is trained on a diverse dataset comprising both bimodal data points
    pair and unimodal data points for [Masked Language Modeling (MLM)](/masked-language-modelling-with-bert-7d49793e5d2c)
    and [Replaced Token Detection (RTD)](https://arxiv.org/pdf/2003.10555.pdf) tasks.
    CodeBERT demonstrated exceptional performance in a variety of domains, excelling
    notably in natural language code search and code to documentation generation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f83b83f3dbdba19b780af10b88fa345a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig-3: CodeBERT model pre-training using Replace Token Detection (RTD) task.
    Natural Language Generation and Code Generator replacing tokens with a different
    token, and the CodeBERT model is trained to classify each token as replaced or
    original. Image from Feng et. al, [CodeBERT](https://arxiv.org/pdf/2002.08155.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training Dataset:** [Codesearch Net Dataset](https://paperswithcode.com/dataset/codesearchnet)-
    2.1M bimodal Data points (NL + PL), 6.4M Unimodal Data Points (6 languages — Python,
    Java, Javascript, PHP, Ruby, Go)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parameter Size:** 125M'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Architecture:** RoBERTa-base'
  prefs: []
  type: TYPE_NORMAL
- en: '**Novelty:**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bimodal Training**: CodeBERT introduces an innovative training approach that
    encompasses both Natural Language and Programming Language tokens. This bimodal
    training technique enhances the model’s ability to understand and generate code
    by considering the intricate interplay between human-readable descriptions and
    programming language elements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Replace Token Detection (RTD) Task for code**: CodeBERT pre-training used
    Replace Token Detection (RTD) instead of Next Sentence Prediction(NSP) which showed
    superior performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Codex, 2021
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Codex](https://arxiv.org/abs/2107.03374) was one of the first successful Code
    LLM to generate code from doc-string or Natural language prompts with high accuracy,
    and predecessor of widely used [Github Copilot](https://github.com/features/copilot).
    Developed by the OpenAI team, Codex uses [GPT3](/understanding-gpt-3-in-5-minutes-7fe35c3a1e52)
    architecture & tokenizer, and pre-trains on a large corpus of Github code. This
    Large Language model has 12B parameters, and was a state-of-art model in 2021,
    which showed best performance on [human-eval dataset](https://github.com/openai/human-eval)
    by solving 28.8% of the problems at first pass.'
  prefs: []
  type: TYPE_NORMAL
- en: Further fine-tuning of the model on standalone python functions (rather than
    whole code which include configs, class implementations etc.), showed significant
    improvement, and was able to solve **37.7% of the human-eval dataset** problem.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/57281f6d8acb2cde891466393c1f6354.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig-4: A decoder only Transformer architecture used for Codex GPT model. Image
    inspired from original [Transformer paper](https://arxiv.org/abs/1706.03762) by
    Vaswani et. al.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training Dataset:** 159GB of python files from 54M Github Repositories.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parameter Size:** 12B (Codex- 12B)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Architecture:** GPT3'
  prefs: []
  type: TYPE_NORMAL
- en: '**Novelty:**'
  prefs: []
  type: TYPE_NORMAL
- en: One of the first successful models which excelled in code-writing capabilities
    from Natural language prompts. This trains GPT-3 models on a large corpus of Github
    repositories.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authors of this model also created a new dataset, **“**[**HumanEval**](https://github.com/openai/human-eval)**”**
    to benchmark models for code-generation tasks. This dataset consists of 164 hand-written
    programming problems with unit tests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try Codex Model at OpenAI Playground [here](https://platform.openai.com/playground)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: CodeT5, 2021
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Code-T5](https://arxiv.org/pdf/2109.00859.pdf) is an **encoder-decoder model**
    based on the T5 architecture, distinct from both CodeBERT (encoder-only) and Codex
    (decoder-only) models. It introduces a unique identifier-aware denoising pre-training
    task which helps the model distinguish and recover identifiers in code, enhancing
    its understanding of structure.'
  prefs: []
  type: TYPE_NORMAL
- en: Code-T5 excels in various tasks such as Code Defect Detection, Clone Detection,
    Code Translation, and Refinement, through multi-task learning, requiring less
    data for quicker fine-tuning. However, it uses CodeBleu scores for evaluation
    rather than benchmarking against the HumanEval dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/25c86d7d4052e417c982092de6b81ab7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig-5: Illustration to show how CodeT5 excels in various code understanding
    and generation tasks. Image taken from Paper by Wang et al, [CodeT5](https://arxiv.org/pdf/2109.00859.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training Dataset**: [Codesearch Net Dataset](https://paperswithcode.com/dataset/codesearchnet)
    (Same as CodeBERT)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parameter Size**: 220M'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Architecture**: [T5](https://jmlr.org/papers/volume21/20-074/20-074.pdf)
    (Encoder-Decoder Architecture)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Novelty:**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Encoder-Decoder Mode**l: One of the first Encoder-Decoder Code LLM to support
    both code-understanding and code-generation tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proposes a novel pre-training objective **identifier-aware denoising,** which
    learns token-type information and structure of the code. This approach trains
    models to differentiate between identifiers (variable names, function names) from
    PL keywords (like if, while etc.), and also recovers them when they are masked.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-Task Learning in Fine Tuning stage**: Fine-tunes on various Code related
    tasks simultaneously like Code Defect Detection, Clone Detection, Code Translation,
    Refinement etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PLBart, 2021
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PLBART, or Program and Language BART](https://arxiv.org/pdf/2103.06333.pdf),
    model leverages the BART model architecture to automate a range of software engineering
    tasks, encompassing code summarization, generation, and translation under the
    umbrella of PLUG (Program and Language Understanding and Generation).'
  prefs: []
  type: TYPE_NORMAL
- en: It introduces a denoising sequence-to-sequence modeling approach for enhanced
    Program and Language understanding, strategically combining the strengths of BERT
    and GPT models. This is achieved by combining a bidirectional encoder with an
    autoregressive decoder, allowing for a more comprehensive grasp of context and
    a versatile generation process. The model employs three denoising strategies,
    including **token masking, token deletion, and token infilling**, to train and
    fine-tune its capabilities effectively.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7cf67533f432400c1dc4966c5d73d9a4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig-6: Illustration to visualize the BART model (used in PLBART too) architecture
    which has bidirectional encoder and autoregressive decoder. Image from original
    [BART paper](https://arxiv.org/pdf/1910.13461.pdf) by Lewis et. al.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training Dataset**: 2M Java and Python Functions and their Natural Language
    descriptions collected from Github, Stackoverflow ([code](https://huggingface.co/docs/transformers/model_doc/plbart)).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parameter Size**: 140M (6 encoder layer + 6 decoder layer + additional norm
    layer on encoder and decoder)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Architecture**: [BART](https://arxiv.org/abs/1910.13461)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Novelty**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Denoising Auto-encoder Approach**: Employs a denoising auto-encoder approach,
    which enhances code understanding and generation by effectively utilizing the
    bidirectional and auto-regressive properties of both the encoder and decoder,
    combining the strengths of BERT and GPT models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Diverse Noising Strategies**: Proposes multiple denoising strategies, such
    as token masking, token deletion, and token infilling. This diversity in noising
    techniques enhances the model’s robustness and effectiveness in learning from
    noisy data, contributing to improved code understanding and generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not all models use the same benchmark for evaluating the performance. PLBART
    authors don’t evaluate model performance on HumanEval, dataset used by majority
    of other models for benchmarking.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Code Llama, 2023
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Code Llama](https://arxiv.org/abs/2308.12950) is the latest Code LLM, released
    by Meta, which beats all the existing open-source models in several benchmark
    datasets. It scores 53% on [HumanEval Dataset](https://github.com/openai/human-eval)
    and 55% on MBPP dataset (only GPT-4 has better performance). These gains can be
    attributed to longer context length of 16K (4x of Llama2) and training pre-trained
    Llama 2 on extra 500B tokes from Program and Natural Language.'
  prefs: []
  type: TYPE_NORMAL
- en: This model is suited best for Code Generation and Infilling tasks, and can act
    as best copilot during IDE based Software Development. Code Llama models family
    has 3 types of models-
  prefs: []
  type: TYPE_NORMAL
- en: Code Llama
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Llama Python
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Llama-Instruct
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: each of them coming in 3 sizes — **7B, 13B and 34B**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/939f1d241185f9018c4f390209bdd46a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig-7: Code Llama training and fine-tuning pipeline taking pre-trained Llama-2
    model as input. Image from original [Code Llama paper](https://arxiv.org/abs/2308.12950)
    by Rozière et. al.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training Dataset**: 500B tokens + additional 100B tokens for Code llama Python
    on publicly available code'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Architecture**: Llama 2'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parameter Size:** Available in 3 sizes — 7B, 13B and 34B.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Novelty**:'
  prefs: []
  type: TYPE_NORMAL
- en: Proposed a fine-tuning step to handle long sequences called **Long Context Fine-Tuning**,
    which increases context length to 16,384 (4x from Llama 2 context length i.e.
    4096)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Instruction Fine Tuning & Self-Instruct**: One of the few models that performs
    instruction fine-tuning, which uses explicit instruction or prompts during the
    fine-tuning process. Instead of creating human feedback data which is expensive,
    authors propose a novel execution feedback approach to construct a self-instruction
    dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Andrej Karapathy, one of the founders of Open AI, recently called Transformers
    [the best idea in AI](https://www.youtube.com/watch?v=9uw3F6rndnA). He added that
    the transformer is like a general purpose differentiable computer which is simultaneously
    — expressive, optimizable and efficient ([X post](https://twitter.com/karpathy/status/1582807367988654081)).
    As evident with the transformation it has brought in the last 3–4 years, the Transformer
    model has vast potential to further change the landscape of how we code as a software
    engineer, and I think this is just the beginning.
  prefs: []
  type: TYPE_NORMAL
- en: Follow me more!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I am a Staff ML Engineer @ LinkedIn. You can follow me at [LinkedIn](https://www.linkedin.com/in/shubham166/)
    or [Twitter](https://twitter.com/ShubhhamAgarwal). You can reach out to me for
    quick chat at [Topmate.io](https://topmate.io/shubham_agarwal166)
  prefs: []
  type: TYPE_NORMAL
