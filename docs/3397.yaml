- en: Bayesian AB Testing with Pyro
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/bayesian-ab-testing-with-pyro-cd4daff686e1?source=collection_archive---------8-----------------------#2023-11-15](https://towardsdatascience.com/bayesian-ab-testing-with-pyro-cd4daff686e1?source=collection_archive---------8-----------------------#2023-11-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A primer in Bayesian thinking and AB testing using Pyro
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@fraserdbrown99?source=post_page-----cd4daff686e1--------------------------------)[![Fraser
    Brown](../Images/d5f99897c571a61ed8832e5542751973.png)](https://medium.com/@fraserdbrown99?source=post_page-----cd4daff686e1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cd4daff686e1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cd4daff686e1--------------------------------)
    [Fraser Brown](https://medium.com/@fraserdbrown99?source=post_page-----cd4daff686e1--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1843b94382f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-ab-testing-with-pyro-cd4daff686e1&user=Fraser+Brown&userId=1843b94382f&source=post_page-1843b94382f----cd4daff686e1---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cd4daff686e1--------------------------------)
    ·13 min read·Nov 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcd4daff686e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-ab-testing-with-pyro-cd4daff686e1&user=Fraser+Brown&userId=1843b94382f&source=-----cd4daff686e1---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcd4daff686e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-ab-testing-with-pyro-cd4daff686e1&source=-----cd4daff686e1---------------------bookmark_footer-----------)![](../Images/b9404c0cdb80e4403e9849c9bfe08141.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'Credit: [Free-Photos](https://pixabay.com/fr/users/free-photos-242387/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1030852)
    on [Pixabay](https://pixabay.com/)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: This article is an introduction to AB testing using the Python probability programming
    language (PPL) Pyro, an alternative to PyMC. The motivation for writing this article
    was to further my understanding of Bayesian statistical inference using the Pyro
    framework and to help others in the process. As such, feedback is welcomed and
    encouraged.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '**Introduction**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: My previous experience with Bayesian modelling in Python was with PyMC. However,
    I have found it troublesome to work with some of the latest versions. My research
    into other probabilistic programming languages led me to discover Pyro, a universal
    PPL built by Uber and supported by PyTorch on the backend. The documentation for
    both Pyro and PyTorch is linked below.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://pyro.ai/?source=post_page-----cd4daff686e1--------------------------------)
    [## Pyro'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Deep Universal Probabilistic Programming
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pyro.ai](https://pyro.ai/?source=post_page-----cd4daff686e1--------------------------------)  [##
    PyTorch documentation - PyTorch 2.1 documentation
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.
    Features described in this documentation…
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pytorch.org](https://pytorch.org/docs/stable/index.html?source=post_page-----cd4daff686e1--------------------------------)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Whilst exploring Pyro, I found it difficult to find end-to-end tutorials for
    AB testing using the package. This article aims to fill that gap.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: This article has five sections. In the first section, I will give a primer on
    Bayesian thinking, a background in the philosophy that the methodology speaks
    for. I will give a brief technical background on Pyro and the Bayesian methods
    used to make our statistical inferences. Next, I will perform an AB test using
    Pyro and discuss the results. I will then explain the case for Bayesian AB testing
    in a business setting. The final section will summarise.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '**A Primer in Bayesian Thinking**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: The Bayesian process is relatively simple at a high level. First, you have a
    variable that you are interested in. We state our current understanding of that
    variable, which we express as a probability distribution (everything in Bayes’
    world is a probability distribution). This is called a *prior*. Probability in
    Bayesian inference is epistemic, meaning it arises through our degree of knowledge.
    Thus, the probability distribution we state as our prior is as much of a statement
    of *uncertainty*, as it is understanding. We then observe events from the real
    world. These observations are then used to *update* our understanding of the variable
    we are interested in. The new state of our understanding is called the *posterior*.
    We also characterise the posterior with a probability distribution, the probability
    of the variable we are interested in, given the data we have observed. This process
    is illustrated in the flow diagram below, where theta is the variable we are interested
    in and data is the outcome of the events we have observed.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3225ead89cc91c8851582f819f38898f.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
- en: Bayesian Process
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: The Bayesian process is actually cyclical, because we then repeat this process,
    starting again but using our new-found knowledge after observing the world. Our
    old posterior becomes our new prior, we observe new events and once again update
    our understanding, becoming ‘less and less and less wrong’ as Nate Silver put
    it in his book *The Signal and the Noise.* We continuouslyreduce our uncertainty
    concerning the variable, converging towards a state of certainty (but we can never
    be certain). Mathematically, this way of thinking is encapsulated by Bayes’ theorem.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be8935f2bf3df8177a7ffb19dfe05953.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: Bayes’ Theorem
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: In practice, Bayes’ theorem quickly becomes computationally intractable for
    large models across higher dimensions. There are numerous approaches to solve
    this issue, but the method we will use today is called Markov Chain Monte Carlo,
    or MCMC. MCMC is a class of algorithms (we will be using one called Hamiltonian
    Monte Carlo), which draws samples from a probability distribution intelligently.
    It is beyond the scope of this article to dive deep into this, but I will provide
    some helpful references at the end for further reading. For now, it is sufficient
    to know that Bayes’ theorem is too complex to be able to calculate, so we leverage
    the power of advanced algorithms, such as MCMC to help. This article will focus
    on MCMC using Pyro. However, Pyro does emphasise the use of Stochastic Variational
    Inference (SVI), an approximation-based approach, which will not be covered here.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '**AB Testing Using Pyro**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Consider a company that has designed a new website landing page and wants to
    understand the impact this will have on conversion, i.e. do visitors continue
    their web session on the website after landing on the page? In test group A, website
    visitors will be shown the current landing page. In test group B, website visitors
    will be shown the new landing page. In the rest of the article, I will refer to
    test group A as the control group, and group B as the treatment group. The business
    is sceptical about the change and has opted for an 80/20 split in session traffic.
    The total number of visitors and the total number of page conversions for each
    test group are summarised below.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a67d81d5f4a39335be11b2a99633af50.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
- en: Test Observations
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: The null hypothesis of the AB test is that there will be no change in page conversion
    for the two test groups. Under the frequentist framework, this would be expressed
    as the following for a two-sided test, where r_c and r_t are the page conversion
    rates in the control and treatment groups, respectively.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c854b401ebce5ba39d5ad28d56cd6c80.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: Null and Alternative Hypotheses
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: A significance test would then seek to either reject or fail to reject the null
    hypothesis. Under the Bayesian framework, we express the null hypothesis slightly
    differently by asserting the same *prior* for each of the test groups.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s pause and outline exactly what is happening during our test. The variable
    we are interested in is the page conversion rate. This is simply calculated by
    taking the number of distinct converted visitors over the total number of visitors.
    The event that generates this rate is whether the visitor clicks through the page.
    There are only two possible outcomes here for each visitor, either the visitor
    clicks through the page and converts, or does not. Some of you might recognise
    that for each distinct visitor, this is an example of a Bernoulli trial; there
    is one trial and two possible outcomes. Now, when we collect a set of these Bernoulli
    trials, we have a binomial distribution. When the random variable X has a binomial
    distribution, we give it the following notation:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1365c7af26417ec922f6c552cda0f2fe.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
- en: Binomial Distribution Notation
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Where n is the number of visitors (or the number of Bernoulli trials), and p
    is the probability of the event on each trial. p is what we are interested in
    here, we want to understand what the probability of a visitor converting on the
    page is in each test group. We have observed some data, but as mentioned in the
    previous section, we first need to define our prior. As always in Bayesian statistics,
    we need to define this prior as a probability distribution. As mentioned before,
    this probability distribution is a characterisation of our uncertainty. Beta distributions
    are commonly used for modelling probabilities, as it is defined between the intervals
    of [0,1]. Furthermore, using a beta distribution as our prior for a binomial likelihood
    function gives us the helpful property of conjugacy, which means our posterior
    will be generated from the same distribution as our prior. We say that the beta
    distribution is a *conjugate* prior. A beta distribution is defined by two parameters,
    alpha, and confusingly, beta.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2ff329a5c166acf1124b8dcf35c3a483.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: Beta Distribution Notation
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: With access to historical data, we can assert an informed prior. We do not necessarily
    need historical data, we could use our intuition to inform our understanding,
    but for now let’s assume we have neither (later in this tutorial we will use informed
    priors, but to demonstrate the impact, I will start with the uninformed). Let’s
    assume we have no understanding of the conversion rate on the company’s site,
    and therefore define our prior as Beta(1,1). This is called a flat prior. The
    probability distribution of this function looks like the graph below, the same
    as a uniform distribution defined between the intervals [0,1]. By asserting a
    Beta(1,1) prior, we say that all possible values of the page conversion rate are
    equally probable.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7b897406a4832fde19684cc17ed692af.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: 'Credit: Author'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: We now have all the information we need, the priors, and the data. Let’s jump
    into the code. The code provided herein will provide a framework to get started
    with AB testing using Pyro; it therefore neglects some features of the package.
    To help optimise your code further and take full advantage of Pyro’s capabilities,
    I recommend referring to the official documentation.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们拥有了所需的所有信息，包括先验和数据。让我们跳入代码中。此处提供的代码将为使用 Pyro 进行 AB 测试提供一个框架；因此，它忽略了该软件包的一些功能。为了进一步优化您的代码并充分利用
    Pyro 的能力，我建议参考官方文档。
- en: First, we need to import our packages. The final line is good practice, particularly
    when working in notebooks, clearing the store of parameters we have built up.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要导入我们的包。最后一行是良好的实践，特别是在处理笔记本时，用于清除我们已建立的参数存储。
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Models in Pyro are defined as regular Python functions. This is helpful as it
    makes it intuitive to follow.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Pyro 中的模型被定义为常规的 Python 函数。这一点很有帮助，因为它使得跟随过程变得直观。
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: A few things to break down and explain here. First, we have a function wrapped
    inside an outer function, the outer function returns the partial function of the
    inner function. This allows us to change our priors, without needing to change
    the code. I have referred to the variables defined in the inner function as primitives,
    think of primitives as variables in the model. We have two types of primitives
    in the model, stochastic and observed stochastic. In Pyro, we do not have to explicitly
    define the difference, we simply add the obs argument to the sample method when
    it is an observed primitive and Pyro interprets it accordingly. Observed primitives
    are contained within the context manager pyro.plate(), which is best practice
    and makes our code look cleaner. Our stochastic primitives are our two priors,
    characterised by Beta distributions, governed by the alpha and beta parameters
    that we pass in from the outer function. As previously mentioned, we assert the
    null hypothesis by defining these as equal. We then stack these two primitives
    together using tensor.stack(), which performs an operation akin to concatenating
    a Numpy array. This will return a tensor, the data structure required for inference
    in Pyro. We have defined our model, now let’s move onto the inference stage.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些需要分解和解释的内容。首先，我们有一个包裹在外层函数中的函数，外层函数返回内层函数的部分函数。这使我们可以更改先验，而无需更改代码。我将内层函数中定义的变量称为原始变量，可以将原始变量视为模型中的变量。模型中有两种类型的原始变量，即随机和观察随机。在
    Pyro 中，我们不需要显式定义差异，只需在 sample 方法中添加 obs 参数，当它是观察原始变量时，Pyro 会相应地解释它。观察原始变量包含在上下文管理器
    pyro.plate() 中，这是最佳实践，使我们的代码看起来更清晰。我们的随机原始变量是两个由 Beta 分布表征的先验，由我们从外层函数传入的 alpha
    和 beta 参数控制。如前所述，我们通过将它们定义为相等来断言零假设。然后，我们使用 tensor.stack() 将这两个原始变量堆叠在一起，该操作类似于连接一个
    Numpy 数组。这将返回一个张量，这是 Pyro 中推断所需的数据结构。我们已经定义了模型，现在让我们进入推断阶段。
- en: As previously mentioned, this tutorial will use MCMC. The function below will
    take the model that we have defined above and the number of samples we wish to
    use to generate our posterior distribution as a parameter. We also pass our data
    into the function, as we did for the model.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，本教程将使用 MCMC。下面的函数将使用我们上面定义的模型和我们希望用于生成后验分布的样本数量作为参数。我们还将数据传递给函数，正如我们为模型所做的那样。
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The first line inside this function defines our kernel. We use the NUTS class
    to define our kernel, which stands for No-U-Turn Sampler, an autotuning version
    of Hamiltonian Monte Carlo. This tells Pyro how to sample from the posterior probability
    space. Again, it is beyond the scope of this article to dive deeper into this
    topic, but for now, it is sufficient to know that NUTS allows us to sample from
    the probability space intelligently. The kernel is then used to initialise the
    MCMC class on the second line, specifying it to use NUTS. We pass the number_of_samples
    argument in the MCMC class which is the number of samples used to generate the
    posterior distribution. We assign the initialised MCMC class to the mcmc variable
    and call the run() method, passing our data as parameters. The function returns
    the mcmc variable.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数中的第一行定义了我们的内核。我们使用NUTS类定义我们的内核，NUTS代表No-U-Turn Sampler，是Hamiltonian Monte
    Carlo的自调版本。这告诉Pyro如何从后验概率空间进行采样。同样，深入探讨这个主题超出了本文的范围，但目前知道NUTS允许我们智能地从概率空间中进行采样就足够了。然后在第二行中使用该内核初始化MCMC类，指定使用NUTS。我们将number_of_samples参数传递给MCMC类，该参数是用于生成后验分布的样本数量。我们将初始化的MCMC类分配给mcmc变量，并调用run()方法，将我们的数据作为参数传递。该函数返回mcmc变量。
- en: This is all we need; the following code defines our data and calls the functions
    we have just made using the Beta(1,1) prior.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们需要的一切；以下代码定义了我们的数据并调用了我们刚刚使用Beta(1,1)先验创建的函数。
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The first element of the traffic and conversions tensors are the counts for
    the control group, and the second element in each tensor is the counts for the
    treatment group. We pass the model function, with the parameters to govern our
    prior distribution, alongside the tensors we have defined. Running this code will
    generate our posterior samples. We run the following code to extract the posterior
    samples and pass them to a Pandas dataframe.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 交通和转换张量的第一个元素是对照组的计数，而每个张量中的第二个元素是处理组的计数。我们将模型函数、控制我们先验分布的参数以及我们定义的张量一起传入。运行此代码将生成我们的后验样本。我们运行以下代码以提取后验样本并将它们传递到一个Pandas数据框中。
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Notice the column names of this dataframe are the strings we passed when we
    defined our primitives in the model function. Each row in our dataframe contains
    samples drawn from the posterior distribution, and each of these samples represents
    an estimate of the page conversion rate, the probability value p that governs
    our Binomial distribution. Now we have returned the samples, we can plot our posterior
    distributions.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意该数据框的列名是我们在定义模型函数时传递的字符串。数据框中的每一行包含从后验分布中抽取的样本，每个样本代表页面转换率的估计值，即控制我们的二项分布的概率值p。现在我们已经返回了样本，我们可以绘制我们的后验分布。
- en: '**Results**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**结果**'
- en: An insightful way to visualise the results of the AB test with two test groups
    is by a joint kernel density plot. It allows us to visualise the density of samples
    in the probability space across both distributions. The graph below can be produced
    from the dataframe we have just built.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一种有洞察力的方式来可视化两个测试组的AB测试结果是通过联合核密度图。它允许我们可视化两个分布中概率空间的样本密度。下面的图可以从我们刚刚构建的数据框中生成。
- en: '![](../Images/b2df9ebb9cab17eea7484b968bc80bf8.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2df9ebb9cab17eea7484b968bc80bf8.png)'
- en: 'Credit: Author'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 信誉：作者
- en: The probability space contained in the graph above can be divided across its
    diagonal, anything above the line would indicate regions where the estimation
    of the conversion rate is higher in the treatment group than the control and vice
    versa. As illustrated in the plot, the samples drawn from the posterior are densely
    populated in the region which would indicate the conversion rate is higher in
    the treatment group. It is important to highlight that the posterior distribution
    for the treatment group is wider than the control group, reflecting a higher degree
    of uncertainty. This is a result of observing less data in the treatment group.
    Nevertheless, the plot strongly indicates that the treatment group has outperformed
    the control group. By collecting an array of samples from the posterior and taking
    the element-wise difference, we can say that the probability that the treatment
    group outperforms the control group is 90.4%. This figure suggests that 90.4%
    of the samples drawn from the posterior will be populated above the diagonal in
    the joint density plot above.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: These results were achieved by using a flat (uninformed) prior. The use of an
    informed prior may help improve the model, particularly when the availability
    of observed data is limited. A helpful exercise is to explore the effects of using
    different priors. The plot below shows the Beta(2,2) probability density function
    and the joint plot it produces when we rerun the model. We can see that using
    the Beta(2,2) prior produces a very similar posterior distribution for both test
    groups.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/734d781ef99d44e895478e86c720d88c.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
- en: 'Credit: Author'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: The samples drawn from the posterior suggest there is a 91.5% probability that
    the treatment group performs better than the control. Therefore, we do believe
    with a higher degree of certainty that the treatment group is better than the
    control versus using a flat prior. However, in this example the difference is
    negligible.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: There is one other thing I would like to highlight about these results. When
    we ran the inference, we told Pyro to generate 1000 samples from the posterior.
    This is an arbitrary number, selecting a different number of samples can change
    the results. To highlight the effect of increasing the number of samples, I ran
    an AB test where the observations from the control and treatment groups were the
    same, each with an overall conversion rate of 50%. Using a Beta(2,2) prior generates
    the following posterior distributions as we incrementally increase the number
    of samples.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/09acb2be339d0b6ce5713d6a385e3eb9.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: 'Credit: Author'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: When we run our inference with just 10 samples, the posterior distribution for
    the control and treatment groups are relatively wide and adopt different shapes.
    As the number of samples that we draw increases, the distributions converge*,*
    eventually generating nearly identical distributions. Furthermore, we observe
    two properties of statistical distributions, the central limit theorem and the
    law of large numbers. The central limit theorem states that the distribution of
    sample means converges towards a normal distribution as the number of samples
    increases, and we can see that in the plot above. Additionally, the law of large
    numbers states that as the sample size grows, the sample mean converges towards
    the population mean. We can see that the mean of the distributions in the bottom
    right tile is approximately 0.5, the conversion rate observed in each of the test
    samples.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '**The Business Case for Bayesian AB Testing**'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian AB testing can help elevate a business’s test and learn culture. Bayesian
    statistical inference allows for small uplifts in the test to be detected quickly
    as it is not reliant on long-run probabilities to draw conclusions. Test conclusions
    can be reached faster and therefore the rate of learning increases. Bayesian AB
    testing also allows for early stoppage of a test, if results gained through ‘peeking’
    indicate test groups are performing significantly worse than the control then
    the test can be stopped. Therefore, the opportunity cost of testing can be significantly
    reduced. This is a major benefit of Bayesian AB testing; results can be constantly
    monitored, and our posteriors constantly updated. Conversely, the early detection
    of an uplift in the test subject can help businesses implement changes faster,
    reducing the latency of implementing revenue-improving changes. Customer-facing
    businesses must be able to quickly implement and analyse test results, which is
    facilitated by the Bayesian AB testing framework.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary**'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: This article has given a brief background on Bayesian thinking and explored
    the results of Bayesian AB testing using Pyro. I hope you have found this article
    insightful. As mentioned in the introduction, feedback is welcomed and encouraged.
    As promised, I have linked some further reading material below.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '**Recommended Material**'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 'The following books provide good insight into Bayesian Inference:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'The Signal and the Noise: The Art and Science of Prediction — Nate Silver'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The Book of Why: The New Science of Cause and Effect* — Judea Pearl and Dana
    Mackenzie. Although this book largely focuses on causality, chapter 3 is a worthwhile
    read on Bayes’.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Bayesian Methods for Hackers: Probabilistic Programming and Bayesian Inference
    —* Cameron Davidson-Pilon. This book is also available on Git, I have linked it
    below.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers?source=post_page-----cd4daff686e1--------------------------------)
    [## GitHub — CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers:
    aka “Bayesian…'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'aka “Bayesian Methods for Hackers”: An introduction to Bayesian methods + probabilistic
    programming with a…'
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers?source=post_page-----cd4daff686e1--------------------------------)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: These Medium articles provide detailed explanations of MCMC.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[](/monte-carlo-markov-chain-mcmc-explained-94e3a6c8de11?source=post_page-----cd4daff686e1--------------------------------)
    [## Monte Carlo Markov Chain (MCMC) explained'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: MCMC methods are a family of algorithms that uses Markov Chains to perform Monte-Carlo
    estimate.
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/monte-carlo-markov-chain-mcmc-explained-94e3a6c8de11?source=post_page-----cd4daff686e1--------------------------------)
    [](/bayesian-inference-problem-mcmc-and-variational-inference-25a8aa9bce29?source=post_page-----cd4daff686e1--------------------------------)
    [## Bayesian inference problem, MCMC and variational inference
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the Bayesian inference problem in statistics.
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/bayesian-inference-problem-mcmc-and-variational-inference-25a8aa9bce29?source=post_page-----cd4daff686e1--------------------------------)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
