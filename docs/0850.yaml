- en: Geometric Deep Learning on Groups
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/geometric-deep-learning-on-groups-cec82eb9366?source=collection_archive---------17-----------------------#2023-03-06](https://towardsdatascience.com/geometric-deep-learning-on-groups-cec82eb9366?source=collection_archive---------17-----------------------#2023-03-06)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Continuous vs discrete approaches on the sphere
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://jasonmcewen.medium.com/?source=post_page-----cec82eb9366--------------------------------)[![Jason
    McEwen](../Images/794e7e6546ed049860dab5e294535880.png)](https://jasonmcewen.medium.com/?source=post_page-----cec82eb9366--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cec82eb9366--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cec82eb9366--------------------------------)
    [Jason McEwen](https://jasonmcewen.medium.com/?source=post_page-----cec82eb9366--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fea87e920245&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgeometric-deep-learning-on-groups-cec82eb9366&user=Jason+McEwen&userId=ea87e920245&source=post_page-ea87e920245----cec82eb9366---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cec82eb9366--------------------------------)
    ¬∑6 min read¬∑Mar 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcec82eb9366&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgeometric-deep-learning-on-groups-cec82eb9366&user=Jason+McEwen&userId=ea87e920245&source=-----cec82eb9366---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcec82eb9366&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgeometric-deep-learning-on-groups-cec82eb9366&source=-----cec82eb9366---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Ideally geometric deep learning techniques on groups would encode equivariance
    to group transformations, to provide well-behaved representation spaces and excellent
    performance, while also being computationally efficient. However, no single approach
    provides both of these desirable properties. Continuous approaches offer excellent
    equivariance but with a very large computational cost. Discrete approaches are
    typically relatively computationally efficient but sacrifice equivariance. We
    point towards future techniques that achieve the best of both worlds.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b7a2db73eea2c07103c581afc144a2ff.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Serg Antonov](https://unsplash.com/@antonov?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning on groups is a rapidly growing area of geometric deep learning
    (see our recent TDS article on [*A Brief Introduction to Geometric Deep Learning*](/a-brief-introduction-to-geometric-deep-learning-dae114923ddb)).
    [Groups](https://en.wikipedia.org/wiki/Group_theory) include homogenous spaces
    with global symmetries, with the archetypical example being the sphere.
  prefs: []
  type: TYPE_NORMAL
- en: Practical applications on geometric deep learning on groups are prevalent, particularly
    for the sphere. For example, spherical data arise in myrad applications, not only
    when data is acquired directly on the sphere (such as over the Earth or by 360¬∞
    cameras that capture panoramic photos and videos), but also when considering spherical
    symmetries (such as in molecular chemistry or magnetic resonance imaging).
  prefs: []
  type: TYPE_NORMAL
- en: We need deep learning techniques on groups that are both highly effective and
    scalable to huge datasets of high-resolution data. In general this problem remains
    unsolved.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/3875a12177434d39615895de8cbdece3.png)'
  prefs: []
  type: TYPE_IMG
- en: An example of spherical data. [Photo by [NASA](https://unsplash.com/@nasa?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)]
  prefs: []
  type: TYPE_NORMAL
- en: Goals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the reasons deep learning techniques have been so effective is due to
    the inductive biases encoded in modern architectures.
  prefs: []
  type: TYPE_NORMAL
- en: One particularly powerful inductive bias is to encode symmetries that the data
    are known to satisfy (as elaborated in our TDS article [*What Einstein Can Teach
    Us About Machine Learning*](/what-einstein-can-teach-us-about-machine-learning-1661e26bef2c)*).*
    Convolutional neural networks (CNNs), for example, encode translational symmetry
    or, more precisely, translational equivariance, as illustrated in the diagram
    below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dfaac1eb43106dd1ca19d77ccbeeb762.png)'
  prefs: []
  type: TYPE_IMG
- en: llustration of translational equivariance. Given an image (top left), applying
    a convolutional kernel (ùíú) to obtain a feature map (top right) and then translating
    (ùíØ) the feature map (bottom right) is equivalent to first translating the image
    (bottom left) and then applying the convolution kernel (bottom right). [Original
    figure created by authors.]
  prefs: []
  type: TYPE_NORMAL
- en: Encoding equivariance in deep learning architectures results in well-behaved
    feature spaces where learning can be performed very effectively.
  prefs: []
  type: TYPE_NORMAL
- en: For geometric deep learning on groups we would therefore like to encode equivariance
    to various group transformations, which typically results in very good performance.
    However, in the general group setting this becomes highly computational demanding
    ‚Äî prohibitively so in many cases.
  prefs: []
  type: TYPE_NORMAL
- en: How to encode equivariance in deep learning architectures on groups in a computationally
    scalable manner is an active area of research.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Group convolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The notion of convolution, which is responsible for the huge success of CNN
    architectures for planar images, naturally encodes equivariance and can be generalised
    to the group setting.
  prefs: []
  type: TYPE_NORMAL
- en: The group convolution of a signal (i.e. data, feature map) *f* defined over
    the group, with a filter *ùù≠*, is given by
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/709f954b8e725411af8150a6d441f80d.png)'
  prefs: []
  type: TYPE_IMG
- en: where *g* is an element of the group *G* and d*¬µ(u)* is the (Haar) measure of
    integration. The above expression is entirely analogous to convolution in the
    more common planar setting. We apply a transformation to the filter (a translation
    for planar CNNs), take the product with the signal of interest, and then sum,
    i.e. integrate.
  prefs: []
  type: TYPE_NORMAL
- en: On the sphere we consider transformations given by 3D rotations and so the convolution
    of a signal on the sphere reads
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/34b7b3b493003ea983d69ea818088ff6.png)'
  prefs: []
  type: TYPE_IMG
- en: where *R* denotes a rotation and *œâ* spherical coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: Once a convolution on the group is defined, we can then construct a CNN on the
    group in a manner analogous to standard planar CNNs. That is, by composing convolutions
    and pointwise non-linear activations (also with pooling and normalisation layers,
    appropriately constructed on the group).
  prefs: []
  type: TYPE_NORMAL
- en: 'The question then remains: how do we compute the group convolution in practice?'
  prefs: []
  type: TYPE_NORMAL
- en: On one hand, we‚Äôd like the implementation to accurately capture the equivariance
    properties on the convolution. While on the other hand, we‚Äôd like the implementation
    to be highly computationally efficient. As we will see, existing approaches typically
    capture one of these requirements but not both simultaneously.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Discrete spherical CNN approaches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Existing approaches can be broadly categorised in discrete and continuous approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Discrete approaches work with a discrete version of the data, typically either
    pixels or a graph representation, which can often be highly computationally efficient.
    However, in general regular discretizations do not exist.
  prefs: []
  type: TYPE_NORMAL
- en: Taking the sphere as an example, it is well known that a regular discretization
    of the sphere does not exist. Consequently, there is no way to discretise the
    sphere in a manner that is invariant to rotations, as illustrated in the diagram
    below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c0fcfa6c76dcce8dd5b460409d7b84a4.png)'
  prefs: []
  type: TYPE_IMG
- en: Rotating a set of pixels on the sphere results in a set on pixels that cannot
    be overlayed with the existing set. This is true for all samplings of the sphere.
    [Original figure created by authors.]
  prefs: []
  type: TYPE_NORMAL
- en: Capturing strict equivariance with operations defined directly on the discretized
    space is simply not possible.
  prefs: []
  type: TYPE_NORMAL
- en: Discrete approaches therefore over favourable computational performance but
    at the cost of equivariance.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Continuous spherical CNN approaches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As an alternative to the discrete approaches discussed above, a continuous representation
    of the underlying signal can also be considered.
  prefs: []
  type: TYPE_NORMAL
- en: Functions on the sphere can be represented by an expansion in terms of spherical
    harmonics (illustrated below). For a bandlimited signal, it is posible to capture
    all of the signal‚Äôs information content in a finite set of samples, from which
    spherical harmonic coefficients can be computed exactly [1]. This is the analog
    of the well-known [Nyquist-Shannon sampling theorem](https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem)
    extended to the sphere.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d363eb6335ceb277a928b5ee435d5ad8.png)'
  prefs: []
  type: TYPE_IMG
- en: Spherical harmoinc functions. [Image sourced from [Wikimedia Commons](https://en.wikipedia.org/wiki/Spherical_harmonics#/media/File:Spherical_Harmonics.png).]
  prefs: []
  type: TYPE_NORMAL
- en: Since the sphere is a compact manifold, its harmonic space is discrete. By working
    with a finite spherical harmonic space representation it is therefore possible
    to access the underlying continuous signal.
  prefs: []
  type: TYPE_NORMAL
- en: Various spherical CNN architecutres have been constructed where convolutions
    are computed through their harmonic representation [2‚Äì6]. By accessing the underlying
    continuous signal, these approaches achieve execellent equivariance properties.
    However, they involve repeatedly performing spherical harmonic transforms, which
    is computationally costly.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous appraoches capture rotational equivariance acurately but are computationally
    demanding.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Dichotomy: discrete vs continuous approaches'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have seen above, a dichotomy exists between discrete and continous approaches,
    as illustrated in the diagram below. Ideally we‚Äôd like techniques that are both
    equivariant and computationally scalable.
  prefs: []
  type: TYPE_NORMAL
- en: However, continuous approaches offer equivariance but with a large computational
    cost. Discrete approaches, on the other hand, are typically relatively computationally
    efficient but sacrifice equivariance.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/191b1b697f3daf4718c598e3bb5585cc.png)'
  prefs: []
  type: TYPE_IMG
- en: Dichotomy between continuous and discrete geometric deep learning techniques
    on groups. [Original figure created by authors.]
  prefs: []
  type: TYPE_NORMAL
- en: Breaking the dichotomy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We desire geometric deep learning techniques on groups that provide equivariance
    (which typically translates to well-behaved representation spaces and excellent
    performance) and are also computationally scalable.
  prefs: []
  type: TYPE_NORMAL
- en: In our next post we will describe a new hyrbid discrete-continuous (DISCO) approach,
    recently accepted for ICLR, that achieves precisely these goals [7]. By keeping
    some components of the representation continuous we achieve excellent equivariance
    properties, while other components are discretized to provide highly efficient
    scalable computation.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] McEwen & Wiaux, *A novel sampling theorem on the sphere*, IEEE TSP (2012),
    [arXiv:1110.6298](https://arxiv.org/abs/1110.6298)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Cohen, Geiger, Koehler, Welling, *Spherical CNNs*, ICLR (2018), [arxiv:1801.10130](https://arxiv.org/abs/1801.10130).'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Esteves, Allen-Blanchette, Makadia, Daniilidis, *Learning SO(3) Equivariant
    Representations with Spherical CNNs*, ECCV (2018), [arXiv:1711.06721](https://arxiv.org/abs/1711.06721).'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Kondor, Lin, Trivedi, *Clebsch-Gordan Nets: a Fully Fourier Space Spherical
    Convolutional Neural Network*, NeurIPS (2018), [arXiv:1806.09231](https://arxiv.org/abs/1806.09231)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Cobb, Wallis, Mavor-Parker, Marignier, Price, d‚ÄôAvezac, McEwen, *Efficient
    Generalised Spherical CNNs*, ICLR (2021), [arXiv:2010.11661](https://arxiv.org/abs/2010.11661#)'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] McEwen, Wallis, Mavor-Parker*, Scattering Networks on the Sphere for Scalable
    and Rotationally Equivariant Spherical CNNs*, ICLR (2022), [arXiv:2102.02828](https://arxiv.org/abs/2102.02828)'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] Ocampo, Price, McEwen, *Scalable and equivariant spherical CNNs by discrete-continuous
    (DISCO) convolutions*, ICLR (2023), [arXiv:2209.13603](https://arxiv.org/abs/2209.13603)'
  prefs: []
  type: TYPE_NORMAL
