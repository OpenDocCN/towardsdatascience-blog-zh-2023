- en: Is ChatGPT Intelligent? A Scientific Review
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/is-chatgpt-intelligent-a-scientific-review-0362eadb25f9?source=collection_archive---------0-----------------------#2023-12-14](https://towardsdatascience.com/is-chatgpt-intelligent-a-scientific-review-0362eadb25f9?source=collection_archive---------0-----------------------#2023-12-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A layman’s review of the scientific debate on what the future holds for the
    current artificial intelligence paradigm
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@orenmatar?source=post_page-----0362eadb25f9--------------------------------)[![Oren
    Matar](../Images/8b1fa6aa3585fc283d51828b53a0754c.png)](https://medium.com/@orenmatar?source=post_page-----0362eadb25f9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----0362eadb25f9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----0362eadb25f9--------------------------------)
    [Oren Matar](https://medium.com/@orenmatar?source=post_page-----0362eadb25f9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F80e783141055&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-chatgpt-intelligent-a-scientific-review-0362eadb25f9&user=Oren+Matar&userId=80e783141055&source=post_page-80e783141055----0362eadb25f9---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----0362eadb25f9--------------------------------)
    ·15 min read·Dec 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F0362eadb25f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-chatgpt-intelligent-a-scientific-review-0362eadb25f9&user=Oren+Matar&userId=80e783141055&source=-----0362eadb25f9---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F0362eadb25f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-chatgpt-intelligent-a-scientific-review-0362eadb25f9&source=-----0362eadb25f9---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'A little over a year ago, OpenAI released ChatGPT, taking the world by storm.
    ChatGPT encompassed a completely new way to interact with computers: in a less
    rigid, more natural language than what we have gotten used to. Most importantly,
    it seemed that ChatGPT could do almost anything: it could beat [most humans on
    the SAT exam](https://www.cnbc.com/2023/03/14/openai-announces-gpt-4-says-beats-90percent-of-humans-on-sat.html)
    and [access the bar exam](https://www.abajournal.com/web/article/latest-version-of-chatgpt-aces-the-bar-exam-with-score-in-90th-percentile).
    Within months it was found that it [can play chess well](https://en.chessbase.com/post/surprising-chatgpt-playing-chess),
    and [nearly pass the radiology exam](https://www.jpost.com/business-and-innovation/all-news/article-744316),
    and some have claimed that it [developed theory of mind](https://www.gsb.stanford.edu/faculty-research/working-papers/theory-mind-may-have-spontaneously-emerged-large-language-models).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 一年多前，OpenAI发布了ChatGPT，引起了全球轰动。ChatGPT提供了一种全新的与计算机互动的方式：比我们习惯的方式更自然、灵活。最重要的是，ChatGPT似乎可以做几乎任何事：它能击败[大多数人类在SAT考试中的表现](https://www.cnbc.com/2023/03/14/openai-announces-gpt-4-says-beats-90percent-of-humans-on-sat.html)和[通过律师资格考试](https://www.abajournal.com/web/article/latest-version-of-chatgpt-aces-the-bar-exam-with-score-in-90th-percentile)。几个月内，人们发现它[能很好地下棋](https://en.chessbase.com/post/surprising-chatgpt-playing-chess)，并[几乎通过放射学考试](https://www.jpost.com/business-and-innovation/all-news/article-744316)，还有人声称它[发展出了心智理论](https://www.gsb.stanford.edu/faculty-research/working-papers/theory-mind-may-have-spontaneously-emerged-large-language-models)。
- en: These impressive abilities prompted many to declare that AGI (artificial general
    intelligence — with cognitive abilities or par or exceeding humans) is around
    the corner. Yet others remained skeptical of the emerging technology, pointing
    out that simple memorization and pattern matching should not be conflated with
    true intelligence.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这些令人印象深刻的能力促使许多人宣称AGI（人工通用智能——具备与人类相当或超越人类的认知能力）即将到来。然而，也有人对这项新兴技术保持怀疑，指出简单的记忆和模式匹配不应与真正的智能混淆。
- en: But how can we truly tell the difference? In the beginning of 2023 when these
    claims were made, there were relatively few scientific studies probing the question
    of intelligence in LLMs (Large Language Models). However, 2023 has seen several
    very clever scientific experiments aiming to differentiate between memorization
    from a corpus and the application of genuine intelligence.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们如何真正区分这些呢？在2023年初提出这些说法时，关于LLMs（大型语言模型）智能的问题相对较少的科学研究。然而，2023年出现了几个非常巧妙的科学实验，旨在区分记忆来自语料库和应用真正智能之间的差异。
- en: The following article will explore some of the most revealing studies in the
    field, making the scientific case for the skeptics. It is meant to be accessible
    to everyone, with no background required. By the end of it, you should have a
    pretty solid understanding of the skeptics’ case.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的文章将探讨这一领域中一些最具启发性的研究，为怀疑者提供科学依据。文章旨在对所有人开放，无需任何背景知识。读完后，你应该对怀疑者的观点有一个相当扎实的理解。
- en: '**But first a primer on LLMs**'
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**但首先了解一下LLMs**'
- en: In this section, I will explain a few basic concepts required to understand
    LLMs — the technology behind GPT — without going into technical details. If you
    are somewhat familiar with supervised learning and the operation of LLMs — you
    can skip this part.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将解释一些理解LLMs——GPT背后的技术——所需的基本概念，而不涉及技术细节。如果你对监督学习和LLMs的操作有一定了解，你可以跳过这一部分。
- en: 'LLMs are a classic example of a paradigm in machine learning, called “supervised
    learning”. To use supervised learning, we must have a dataset consisting of inputs
    and desired outputs, these are fed to an algorithm (there are many possible models
    to choose from) which tries to find the relationships between these inputs and
    outputs. For example, I may have real estate data: an Excel sheet with the number
    of rooms, size, and location of houses (input), as well as the price at which
    they sold (outputs). This data is fed to an algorithm that extracts the relationships
    between the inputs and the outputs — it will find how the increase in the size
    of the house, or the location influences the price. Feeding the data to the algorithm
    to “learn” the input-output relationship is called “training”.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型是机器学习中一个经典的范式，称为“监督学习”。使用监督学习时，我们必须拥有一个包含输入和期望输出的数据集，这些数据被输入到一个算法中（有许多可能的模型可供选择），算法试图找出这些输入和输出之间的关系。例如，我可能有房地产数据：一个包含房间数量、面积和房屋位置（输入）以及它们售出价格（输出）的Excel表格。这些数据被输入到一个算法中，算法提取输入和输出之间的关系——它会找出房屋面积的增加或位置如何影响价格。将数据输入算法以“学习”输入输出关系的过程称为“训练”。
- en: After the training is done, we can use the model to make predictions on houses
    for which we do not have the price. The model will use the learned correlations
    from the training phase to output estimated prices. The level of accuracy of the
    estimates depends on many factors, most notably the data used in training.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们可以用模型来对没有价格的房屋进行预测。模型将使用训练阶段学到的相关性来输出估计价格。估计的准确性取决于许多因素，最重要的是训练中使用的数据。
- en: 'This “supervised learning” paradigm is extremely flexible to almost any scenario
    where we have a lot of data. Models can learn to:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这种“监督学习”范式对几乎任何拥有大量数据的场景都极具灵活性。模型可以学习到：
- en: Recognize objects in an image (given a set of images and the correct label for
    each, e.g. “cat”, “dog” etc.)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别图像中的物体（给定一组图像和每个图像的正确标签，例如“猫”、“狗”等）
- en: Classify an email as spam (given a dataset of emails that are already marked
    as spam/not spam)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将电子邮件分类为垃圾邮件（给定一个已经标记为垃圾邮件/非垃圾邮件的电子邮件数据集）
- en: Predict the next word in a sentence.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测句子中的下一个词。
- en: 'LLMs fall into the last category: they are fed huge amounts of text (mostly
    found on the internet), where each chunk of text is broken into the first N words
    as the input, and the N+1 word as the desired output. Once their training is done,
    we can use them to auto-complete sentences.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）属于最后一种类型：它们接受大量文本（大多来自互联网），每段文本被拆分成前N个词作为输入，N+1个词作为期望输出。训练完成后，我们可以用它们来自动完成句子。
- en: In addition to many of texts from the internet, OpenAI used well-crafted conversational
    texts in its training. Training the model with these question-answer texts is
    crucial to make it respond as an assistant.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 除了大量来自互联网的文本，OpenAI 在其训练中还使用了精心编排的对话文本。用这些问答文本训练模型对于使其表现得像助手至关重要。
- en: 'How exactly the prediction works depends on the specific algorithm used. LLMs
    use an architecture known as a “transformer”, whose details are not important
    to us. What is important is that LLMs have two “phases”: training and prediction;
    they are either given texts from which they extract correlations between words
    to predict the next word or are given a text to complete. Do note that the entire
    supervised learning paradigm assumes that the data given during training is similar
    to the data used for prediction. If you use it to predict data from a completely
    new origin (e.g., real estate data from another country), the accuracy of the
    predictions will suffer.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 预测的具体工作原理取决于所使用的算法。大型语言模型使用一种称为“变换器”的架构，其细节对我们并不重要。重要的是，大型语言模型有两个“阶段”：训练和预测；它们要么接收文本，从中提取词语之间的相关性来预测下一个词，要么接收一个待完成的文本。请注意，整个监督学习范式假设训练期间提供的数据与用于预测的数据相似。如果你用它来预测来自完全新来源的数据（例如，来自另一个国家的房地产数据），预测的准确性会受到影响。
- en: Now back to intelligence
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 现在回到智能问题上
- en: 'So did ChatGPT, by training to auto-complete sentences, develop intelligence?
    To answer this question, we must define “intelligence”. Here’s one way to define
    it:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 通过训练自动完成句子来发展智能吗？要回答这个问题，我们必须定义“智能”。以下是定义智能的一种方式：
- en: '![](../Images/0e767809a2b016beab242efa0b9fdfa2.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e767809a2b016beab242efa0b9fdfa2.png)'
- en: (Image by author)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: （图片由作者提供）
- en: 'Did you get it? If you didn’t, ChatGPT can explain:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你明白了吗？如果不明白，ChatGPT 可以解释：
- en: '![](../Images/2431563c72850fe88d713d1cb4b59982.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2431563c72850fe88d713d1cb4b59982.png)'
- en: (Image by Author)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: (图片由作者提供)
- en: 'It certainly appears as if ChatGPT developed intelligence — as it was flexible
    enough to adapt to the new “spelling”. Or did it? You, the reader, may have been
    able to adapt to the spelling that you haven’t seen before, but **ChatGPT was
    trained on huge amounts of data from the internet: and this very example can be
    found on many websites**. When GPT explained this phrase, it simply used similar
    words to those found in its training, and that does not demonstrate flexibility.
    Would it have been able to exhibit “IN73LL1G3NC3“, if that phrase did not appear
    in its training data?'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来ChatGPT确实展现了智能——因为它足够灵活地适应了新的“拼写”。但真的如此吗？你，读者，可能能够适应你未曾见过的拼写，但**ChatGPT在互联网上的大量数据上进行训练：而这个例子在许多网站上都可以找到**。当GPT解释这个短语时，它只是使用了与其训练中发现的相似的词汇，这并不表现出灵活性。如果这个短语“IN73LL1G3NC3”没有出现在它的训练数据中，它会表现出智能吗？
- en: '**That is the crux of the LLM-AGI debate: has GPT (and LLMs in general) developed
    true, flexible, intelligence or is it only repeating variations on texts that
    it has seen before?**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**这就是LLM-AGI辩论的核心：GPT（以及LLMs一般来说）是否发展出了真正的、灵活的智能，还是只是重复它见过的文本的变体？**'
- en: How can we separate the two? Let’s turn to science to explore LLMs' abilities
    and limitations.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何区分这两者？让我们转向科学，探索LLMs的能力和局限性。
- en: '[**The Reversal Curse: LLMs trained on “A is B” fail to learn “B is A”**](https://arxiv.org/abs/2309.12288)'
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[**反转诅咒：LLMs在“A是B”的训练中未能学会“B是A”**](https://arxiv.org/abs/2309.12288)'
- en: Suppose I tell you that Olaf Scholz was the ninth Chancellor of Germany, can
    you tell me who the ninth Chancellor of Germany was? That may seem trivial to
    you but is far from obvious for LLMs.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我告诉你奥拉夫·朔尔茨是德国第九任总理，你能告诉我谁是德国第九任总理吗？这对你来说可能显得微不足道，但对LLMs来说却远非显而易见。
- en: 'In this brilliantly straightforward [paper](https://arxiv.org/abs/2309.12288),
    researchers queried ChatGPT for the names of parents of 1000 celebrities, (for
    example: “Who is Tom Cruise’s mother?”) to which ChatGPT was able to answer correctly
    79% of the time (“Mary Lee Pfeiffer” in this case). The researchers then used
    the questions that GPT answered correctly, to phrase the opposite question: “Who
    is Mary Lee Pfeiffer''s son?”. While the same knowledge is required to answer
    both, **GPT was successful in answering only 33% of these queries**.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇出色的[论文](https://arxiv.org/abs/2309.12288)中，研究人员询问ChatGPT关于1000位名人的父母的名字（例如：“汤姆·克鲁斯的母亲是谁？”），ChatGPT能正确回答79%的时间（在这种情况下为“玛丽·李·费佛”）。研究人员然后利用GPT回答正确的问题，提出相反的问题：“玛丽·李·费佛的儿子是谁？”虽然回答这两个问题所需的知识是相同的，**GPT仅在33%的这些查询中回答成功**。
- en: Why is that? Recall that GPT has no “memory” or “database” — all it can do is
    predict a word given a context. Since Mary Lee Pfeiffer is mentioned in articles
    as Tom Cruise’s mother more often than he is mentioned as her son — GPT can recall
    one direction and not the other.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么会这样？请记住，GPT没有“记忆”或“数据库”——它所能做的只是根据上下文预测一个词。由于玛丽·李·费佛被提及为汤姆·克鲁斯的母亲的频率远高于他被提及为她儿子的频率——GPT可以回忆起一个方向而不是另一个。
- en: '![](../Images/439e794f079bd5d75ecdc56a2ad755bc.png)![](../Images/bc116e1d546beb3cee18be8c7868a13e.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/439e794f079bd5d75ecdc56a2ad755bc.png)![](../Images/bc116e1d546beb3cee18be8c7868a13e.png)'
- en: ChatGPT fails to recall that Tom Cruise is the son of Mary Lee Pfeiffer (Images
    by author)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT未能记起汤姆·克鲁斯是玛丽·李·费佛的儿子（图片由作者提供）
- en: 'To hammer this point, the researchers created a dataset of fabricated facts
    of the structure “<description> is <name>”, e.g., “The first person to walk on
    Mars is Tyler Oakridge”. LLMs were then trained on this dataset and queried about
    the description: “Who is the first person to walk on Mars” — where GPT-3 succeeded
    with 96% accuracy.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了强调这一点，研究人员创建了一个虚构事实的数据集，结构为“<描述>是<名字>”，例如，“第一个登上火星的人是泰勒·奥克里奇”。然后，LLMs在这个数据集上进行了训练，并询问描述：“谁是第一个登上火星的人”——GPT-3在这个问题上成功率为96%。
- en: 'But when asked about the name — “Who is Tyler Oakridge” — **GPT scored 0%**.
    This may seem surprising at first but is consistent with what we know about supervised
    learning: GPT cannot encode these facts into memory and recall them later, it
    can only predict a word given a sequence of words. Since in all the texts, it
    read the name followed the description, and not the opposite — it never learned
    to predict facts about the name. Evidently, memory that is developed only through
    auto-complete training, is very limited.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 但当被问及名字——“谁是Tyler Oakridge”时——**GPT得分为0%**。这可能一开始让人感到惊讶，但与我们对监督学习的了解一致：GPT无法将这些事实编码到记忆中并随后回忆，它只能根据词序列预测一个词。由于在所有文本中，它读到名字后面跟着描述，而不是相反——它从未学会预测有关名字的事实。显然，仅通过自动完成训练开发的记忆是非常有限的。
- en: Reasoning or Reciting? Exploring the Capabilities and Limitations of Language
    Models Through Counterfactual Tasks
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理还是背诵？通过反事实任务探索语言模型的能力与局限性
- en: '[This paper](https://arxiv.org/pdf/2307.02477.pdf) is perhaps the most important
    paper I will explore, aiming at the heart of the difference between memorization
    and intelligence. It is composed of several mini-experiments, all utilizing **counterfactual
    tasks**. Here’s an example of a counterfactual task:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[这篇论文](https://arxiv.org/pdf/2307.02477.pdf)可能是我将要探讨的最重要的论文，旨在揭示记忆与智能之间的核心差异。它由几个小型实验组成，所有实验都使用**反事实任务**。以下是一个反事实任务的示例：'
- en: Arithmetic is normally done in base-10 (using numbers 0–9), however, other [number
    systems can be used](https://www.youtube.com/watch?v=FFDMzbrEXaE&t=307s), using
    only a subset of these numbers, or additional numbers.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 算术通常在基数为10的情况下进行（使用数字0–9），然而，其他[数字系统可以使用](https://www.youtube.com/watch?v=FFDMzbrEXaE&t=307s)，只使用这些数字的子集或附加数字。
- en: 'A counterfactual task could be solving arithmetic questions in any base other
    than 10: **the abstract skills needed to complete the task are identical, but
    you will find significantly more examples of the decimal system on the internet
    (and on LLMs training sets)**. When GPT-4 was asked simple arithmetic questions
    (27+62) in base 10 it answered accurately 100% of the questions. However, when
    told to use base 9 in its calculations, **its success dropped to 23%**. This shows
    that it failed to learn abstract arithmetic skills and is bound to examples similar
    to what it has seen.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一个反事实任务可以是在除了10以外的任何基数中解决算术问题：**完成任务所需的抽象技能是相同的，但你会发现互联网（以及LLM训练集）上有显著更多的十进制系统的例子**。当GPT-4在基数为10的情况下被问及简单的算术问题（27+62）时，它100%准确地回答了所有问题。然而，当要求它在计算中使用基数为9时，**它的成功率降至23%**。这表明它未能学习到抽象的算术技能，且仅依赖于它所见过的类似示例。
- en: 'These counterfactual tasks were created for several other domains, as you can
    see below:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这些反事实任务被创建用于几个其他领域，如下所示：
- en: '![](../Images/bfef44fbaea6df2f9e86f5e76ece9d55.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bfef44fbaea6df2f9e86f5e76ece9d55.png)'
- en: GPT-4’s performance on the default version of various tasks (blue) and counterfactual
    counterparts (orange). GPT-4 consistently and substantially underperforms on counterfactual
    variants compared to default task instantiations. (Image generously provided by
    Zhaofeng Wu, one of the article’s authors)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4在各种任务的默认版本（蓝色）和反事实对应任务（橙色）上的表现。与默认任务实例相比，GPT-4在反事实变体上的表现始终且显著较差。（图片由文章作者之一Zhaofeng
    Wu慷慨提供）
- en: 'Here’s another counterfactual: Python uses [zero-based numbering](https://en.wikipedia.org/wiki/Zero-based_numbering);
    however, this is only a convention, and we can easily create a programming language
    that is one-based. Writing code in a one-based Python variant requires the same
    skills as normal Python and any seasoned programmer would be able to adapt to
    the change quickly. Not so for **GPT-4: it scored 82% on code generation for Python,
    but only 40% when told to use a 1-based variant**. When tested on code interpretation
    (predicting what a piece of code would do), **it scored 74% for normal Python
    and 25% for the uncommon variant**.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个反事实是：Python使用[零起始编号](https://en.wikipedia.org/wiki/Zero-based_numbering)；然而，这只是一个约定，我们可以很容易地创建一个一基数的编程语言。在一种一基数的Python变体中编写代码需要与正常Python相同的技能，任何有经验的程序员都能迅速适应这种变化。**但GPT-4在生成Python代码时得分82%，而在要求使用一基数变体时仅得40%**。在代码解释测试（预测一段代码的作用）中，**它对正常Python的得分为74%，而对不常见的变体得分为25%**。
- en: But we don’t have to venture to different Python versions. Even in normal Python,
    **LLMs fail when given strange coding** tasks that you cannot find on the web,
    as [Filip Pieniewski showed](https://twitter.com/filippie509/status/1732540152470462932)
    recently on Gemini.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们不必涉足不同的 Python 版本。即使在普通 Python 中，**LLM 在面对你无法在网络上找到的奇怪编码任务时也会失败**，正如[Filip
    Pieniewski 最近在 Gemini 上展示的](https://twitter.com/filippie509/status/1732540152470462932)。
- en: In chess, GPT was asked to evaluate whether a sequence of moves was legal or
    not. For a normal chess game, it accurately predicted the legality of a move 88%
    of the time. But when the starting positions of the bishops and knights were swapped,
    **its guesses on the legality of moves became completely random**, while even
    a novice human player should be able to adapt to these changes easily.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在国际象棋中，GPT 被要求评估一系列走法是否合法。对于普通棋局，它88%的时间准确预测了走法的合法性。但当主教和骑士的起始位置被调换时，**它对走法合法性的猜测变得完全随机**，而即使是新手玩家也应能轻松适应这些变化。
- en: 'In fact, [Jonas Persson showed](https://medium.com/@Blougram/check-mate-gpt-the-dangers-of-mistaking-reasoning-for-pattern-recognition-221824e4dc6a)
    that you don’t even need to change the starting positions. **If you start playing
    a chess game with GPT and make very unconventional, but legal, moves — it may
    claim that they are illegal because it has never seen similar moves**. As Persson
    beautifully remarked:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，[Jonas Persson 展示了](https://medium.com/@Blougram/check-mate-gpt-the-dangers-of-mistaking-reasoning-for-pattern-recognition-221824e4dc6a)你甚至不需要改变起始位置。**如果你和
    GPT 开始下棋，并进行非常非常规但合法的走法——它可能会声称这些走法是非法的，因为它从未见过类似的走法**。正如 Persson 精辟指出的：
- en: '*“****When sufficiently advanced, pure pattern recognition can mimic rule-bound,
    deductive reasoning****. But they are distinct. Playing chess with GPT-4 is to
    enter a Potemkin village. Sneak away from Main Street into an alley — do something
    unexpected — and you immediately realize that the impressive-looking houses are
    all propped up set pieces.”*'
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“****当达到足够的先进水平时，纯粹的模式识别可以模仿规则导向的推理****。但它们是不同的。和 GPT-4 下棋就像进入一个假村。离开主街，进入小巷——做一些意外的事情——你立即意识到那些令人印象深刻的房子都是支撑起来的布景。”*'
- en: '![](../Images/e47dc7dcc0686cb8aada13192b395389.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e47dc7dcc0686cb8aada13192b395389.png)'
- en: GPT claims that moving the rook to h3 is an illegal move. (Image by author)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 声称将车移动到 h3 是一个非法走法。（图片来源于作者）
- en: 'This finding is incredibly damning for LLMs as a general intelligence technology.
    Problem-solving often involves coming up with new rules or conceptualizations
    of a problem: a programmer may write a library that has an innovative internal
    logic, a mathematician may invent a new branch of math, or an artist may come
    up with new artistic styles — they all understand the limitations of a current
    paradigm, and then create rules for a new one. Even more mundane activities require
    this flexibility: if the road is blocked, you may step off the marked path. Could
    GPT accomplish any of these? **If it cannot consistently follow counterfactual
    rules when explicitly told to do so, could it “realize” on its own that a solution
    for a problem requires a new set of rules, a break from the default paradigm?**
    Could an engine based on detecting correlations in data be flexible enough to
    respond to novel situations?'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这一发现对 LLM 作为通用智能技术的指责非常严重。解决问题通常涉及制定新的规则或对问题进行概念化：程序员可能编写一个具有创新内部逻辑的库，数学家可能发明一个新的数学分支，或者艺术家可能创造新的艺术风格——他们都理解当前范式的局限性，然后为新的范式制定规则。即使是更平凡的活动也需要这种灵活性：如果道路被阻塞，你可能需要偏离标记的路径。GPT
    能做到这些吗？**如果它在明确要求时不能一致地遵循反事实规则，它能否“意识到”一个问题的解决需要一组新的规则，从而突破默认范式？** 基于数据相关性检测的引擎是否足够灵活以应对新情况？
- en: Theory of mind (ToM)
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 思维理论（ToM）
- en: Theory of mind is the capacity to understand that other people may have different
    beliefs and wishes than one’s own, an ability that is absent in the first few
    years of a child’s development. One method to test Theory of Mind is by presenting
    a child with a box labeled “chocolate”, which in fact contains pencils. We then
    show the child the true content of the bag and ask them “What would your friend
    Jeremy think is in the box?”. If the child hasn’t developed Theory of Mind yet,
    they will answer “pencils” — since they cannot separate their knowledge of the
    content from what another person might think.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 心智理论是理解其他人可能有不同的信念和愿望的能力，这种能力在儿童发展的最初几年是缺失的。一种测试心智理论的方法是给孩子一个标有“巧克力”的盒子，而实际上里面装的是铅笔。然后我们展示盒子的真实内容，并问孩子：“你的朋友杰里米会认为盒子里是什么？”如果孩子还没有发展出心智理论，他们会回答“铅笔”——因为他们不能将自己对内容的知识与其他人可能的想法分开。
- en: '![](../Images/ec1b1737a5b3f85bef317e40c8407e4e.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ec1b1737a5b3f85bef317e40c8407e4e.png)'
- en: A child who hasn’t yet developed Theory of Mind cannot separate their knowledge
    of the content from what another person might think. (Image by author)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 尚未发展出心智理论的孩子无法将自己对内容的知识与其他人可能的想法分开。（图片由作者提供）
- en: This ability is crucial to the understanding of a person’s motivations, and
    therefore crucial in the development of AGI. Imagine you have a multi-purpose
    robot, and you give it the instruction to “clean the room”. In the process of
    cleaning, the robot will have to make several decisions on what to clean or move;
    is that crumbled piece of paper important or should I throw it? Should I ask first?
    **In general, an intelligent agent will need to understand my motivation and the
    limits of my knowledge** for it to fill in the implementation details of complex
    requests.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这种能力对于理解一个人的动机至关重要，因此对AGI的发展至关重要。想象一下你有一个多功能机器人，你给它的指令是“清理房间”。在清理的过程中，机器人必须做出几个关于清理或移动的决策；那张碎纸重要吗，还是应该扔掉？我是否应该先询问？**一般来说，一个智能体需要理解我的动机和知识的局限性**，才能填补复杂请求的实施细节。
- en: 'For this reason, when [new research claimed](https://www.gsb.stanford.edu/faculty-research/working-papers/theory-mind-may-have-spontaneously-emerged-large-language-models)
    that Theory of Mind may have spontaneously emerged in LLMs, it made a lot of waves
    in the AI field. The article used a textual version of the pencils/chocolate exam
    to test GPT-4 and found that it performed at the level of a seven-year-old. This
    may seem impressive at first but remember the “IN73LL1G3NC3” example: the training
    data for GPT may well contain examples of these test questions. It is therefore
    not a fair comparison to a child who passes the test without any training on similar
    questions. If we would like to test GPT’s ToM ability — we must create a new exam
    which we can be sure wasn’t in its training data.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当 [新的研究声称](https://www.gsb.stanford.edu/faculty-research/working-papers/theory-mind-may-have-spontaneously-emerged-large-language-models)心智理论可能在LLM中自发出现时，它在AI领域引起了很多关注。文章使用了文本版的铅笔/巧克力考试来测试GPT-4，并发现其表现相当于七岁的水平。这可能最初看起来很令人印象深刻，但请记住“IN73LL1G3NC3”示例：GPT的训练数据中可能包含这些测试问题的例子。因此，这与一个没有类似问题训练的孩子的公平比较并不相同。如果我们想测试GPT的心智理论能力——我们必须创建一个新的考试，我们可以确信它不在其训练数据中。
- en: '**FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions**'
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**FANToM: 机器心智理论互动压力测试基准**'
- en: '[This paper](https://arxiv.org/abs/2310.15421) presents a new benchmark for
    ToM, which includes several multi-participant conversations. During these conversations,
    some of the participants “leave the room” for some time, while the other participants
    continue their conversation. The LLM is then asked several questions regarding
    who knows what: does Kailey know the breed of Linda’s dog? Who knows what breed
    it is? What breed would David think it is? The LLM is considered to have answered
    correctly only if its answer was correct on all questions pertaining to the same
    piece of information.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[本文](https://arxiv.org/abs/2310.15421)提出了一个新的ToM基准，其中包含了几个多参与者对话。在这些对话中，有些参与者会“离开房间”一段时间，而其他参与者则继续对话。然后，LLM会被问到几个关于谁知道什么的问题：凯莉是否知道琳达的狗是什么品种？谁知道它是什么品种？大卫会认为它是什么品种？只有当LLM在所有涉及同一信息的问题上回答正确时，它的回答才被视为正确。'
- en: This can be a confusing task, so even humans only scored 87.5% on this test.
    However, **GPT-4 scored either 4.1% or 12.3%**, depending on the GPT version;
    hardly consistent with the claim that GPT developed human-level ToM.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是一个令人困惑的任务，因此即使是人类在这项测试中的得分也只有87.5%。然而，**GPT-4的得分为4.1%或12.3%**，具体取决于GPT版本；这与GPT发展出人类级别的ToM的说法几乎不一致。
- en: '![](../Images/84520f09aa339a58d4abd2fa8c26e33e.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84520f09aa339a58d4abd2fa8c26e33e.png)'
- en: The FANToM dataset explained. (Image generously provided by Melanie Sclar, one
    of the article’s authors)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: FANToM数据集的解释。（图片由文章作者之一Melanie Sclar慷慨提供）
- en: '**A note about the construct validity of psychometric exams**'
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**关于心理测量考试构建效度的说明**'
- en: 'It is important to make a more general point about all psychometric tests:
    **people often confuse the test with the quality it is trying to measure**. The
    reason we care about SAT scores is because they are correlated with performance
    in college. Success in ToM exams in children is correlated with other behaviors
    of value: understanding a person’s facial expressions, remembering attributes
    of a person’s personality, or being able to watch a movie and understand the motivations
    of the characters. **While these correlations between the tests and the behaviors
    have been shown in humans, there is no reason to assume that they apply to LLMs
    too.** In fact, despite the impressive results on the SAT, GPT [scored an average
    of 28%](https://arxiv.org/pdf/2307.10635.pdf) on open-ended college-level exams
    in math, chemistry, and physics. Until shown otherwise, passing a test proves
    nothing other than the ability to answer the test questions correctly.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 关于所有心理测量测试，**人们常常混淆测试与其试图测量的质量**。我们关心SAT分数的原因是因为它们与大学表现相关。儿童在ToM考试中的成功与其他有价值的行为相关：理解一个人的面部表情、记住一个人的个性特征，或能够看一部电影并理解角色的动机。**虽然这些测试与行为之间的相关性在人的身上已经被证明，但没有理由认为它们也适用于LLMs。**
    事实上，尽管SAT的结果令人印象深刻，GPT在数学、化学和物理的开放性大学水平考试中[平均得分为28%](https://arxiv.org/pdf/2307.10635.pdf)。除非有其他证明，否则通过测试并不能证明其他能力，除了能正确回答测试问题。
- en: 'But for ToM there is no correlation to speak of: **whether LLMs pass a ToM
    test or not — they can’t see facial expressions, watch movies, or even remember
    a person and their motivations from one interaction to the next**. Since the behaviors
    that we are truly interested in when measuring ToM are not available to LLMs,
    **the idea that LLMs developed Theory of Mind is not only false, but it may also
    be meaningless** (or at least: requires a new definition and understanding of
    the term).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 但对于ToM来说，没有相关性可言：**LLMs是否通过ToM测试 — 它们不能看到面部表情、观看电影，甚至不能记住一个人及其动机**。由于我们在测量ToM时真正感兴趣的行为对LLMs来说是不可用的，**LLMs发展出心智理论的想法不仅是错误的，还可能是毫无意义的**（或至少：需要对这一术语有新的定义和理解）。
- en: '**On the Planning Abilities of Large Language Models — A Critical Investigation**'
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**关于大型语言模型的规划能力 — 一项批判性调查**'
- en: '[This experiment](https://arxiv.org/abs/2305.15771) tried to probe LLM’s planning
    abilities. One example task presented to the LLM is to stack colored blocks in
    a particular order, given an “initial state” of the blocks (arranged in some order
    on the table). The LLM is presented with a list of clearly defined possible actions,
    for example:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[这个实验](https://arxiv.org/abs/2305.15771)试图探测LLM的规划能力。一个例子任务是将彩色积木按特定顺序堆叠，给定积木的“初始状态”（在桌子上以某种顺序排列）。LLM会得到一系列明确定义的可能行动，例如：'
- en: '[PRE0]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The LLM’s task is to specify a list of actions that need to be taken to achieve
    the goal.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的任务是指定一系列需要采取的行动以实现目标。
- en: A similar task involved sending a package from one address to another when the
    available actions were truck and plane delivery. These are relatively simple planning
    tasks, using only a handful of possible actions, however, **GPT-4 scored 12–35%**
    for the blocks puzzle, **and 5–14% for the logistics** task (depending on the
    configuration).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一个类似的任务涉及从一个地址发送包裹到另一个地址，而可用的行动是卡车和飞机运送。这些是相对简单的规划任务，仅使用少量可能的行动，然而，**GPT-4在积木拼图中得分为12–35%**，**在物流**任务中得分为5–14%（取决于GPT版本）。
- en: Additionally, if the names of the actions were replaced with random words (from
    “pickup” to “attack”), even if the definition of each action remained similar,
    GPT’s success dropped to 0–3%. In other words, GPT did not use abstract thinking
    to solve these problems, and it depended on semantics.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果将行动的名称替换为随机词（从“pickup”到“attack”），即使每个行动的定义保持类似，GPT 的成功率也会降到 0-3%。换句话说，GPT
    并没有使用抽象思维来解决这些问题，而是依赖于语义。
- en: '**Conclusion, are LLMs the path to AGI?**'
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**结论，LLMs 是否是通用人工智能的路径？**'
- en: 'Defining intelligence is not a simple task, but I would argue that any true
    intelligence should have at least four elements:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 定义智能并非易事，但我认为任何真正的智能应至少包含四个元素：
- en: '**Abstraction** — the ability to identify objects as part of a larger category
    or rule. This abstract representation of the world can be referred to as a cognitive
    “world model”. E.g., the understanding that different images on your retina refer
    to the same person, or that a move in chess is legal as part of a framework of
    rules that hold for any chess game.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**抽象**——将对象识别为更大类别或规则的一部分的能力。这种对世界的抽象表示可以被称为认知“世界模型”。例如，理解你视网膜上的不同图像指代同一个人，或在棋局中，某一步棋是合法的，因为它符合任何棋局的规则框架。'
- en: '**Memory** — the ability to attach attributes to entities and relations between
    entities in the world model, and the ability to update them over time. E.g., once
    you recognize a person you may be able to recall other attributes about them or
    their relations with other individuals.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**记忆**——在世界模型中将属性附加到实体及其之间关系的能力，并随着时间的推移进行更新。例如，一旦你识别出一个人，你可能会记得有关他们的其他属性或他们与其他个体的关系。'
- en: '**Reasoning and inference** — the ability to use the world model to draw conclusions
    on the behavior of entities in a new or imagined world state. E.g., being able
    to predict the trajectory of a thrown ball, based on the attributes of that ball,
    or predicting the behavior of a person based on their characteristics.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推理和推断**——利用世界模型在新的或想象的世界状态中对实体的行为得出结论的能力。例如，根据抛球的属性预测球的轨迹，或根据一个人的特征预测其行为。'
- en: '**Planning** — the ability to use reasoning to develop a set of actions to
    achieve a goal.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规划**——使用推理来制定一系列行动以实现目标的能力。'
- en: A year ago, we could have analytically deduced that these elements are unlikely
    to emerge in LLMs, based on their architecture, but today we no longer need the
    analytical deduction, as we have the empirical data to show that LLMs perform
    poorly on all the elements above. They are no more than statistical auto-complete
    models, using a powerful pattern-matching method. For a more in-depth analysis
    of the elements of intelligence missing from the current machine learning paradigm,
    see Gary Marcus’ famous “[deep learning is hitting a wall](https://nautil.us/deep-learning-is-hitting-a-wall-238440/)”
    article.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一年前，我们可以根据 LLM 的架构从分析上推断这些元素不太可能出现在 LLM 中，但今天我们不再需要这种分析推断，因为我们有实证数据表明 LLM 在上述所有元素上的表现都很差。它们不过是统计自动补全模型，使用一种强大的模式匹配方法。有关当前机器学习范式中缺失的智能元素的更深入分析，请参见
    Gary Marcus 的著名文章“[深度学习正遇到瓶颈](https://nautil.us/deep-learning-is-hitting-a-wall-238440/)”。
- en: When ChatGPT was first released, a friend of mine told me that conversing with
    it feels like magic. **But just like a magician sawing a person in half — it is
    important to scrutinize the performance and test it in different settings before
    we claim the sawing technique can revolutionize surgeries**. The “trick” used
    by LLMs is the unfathomable amounts of texts they are trained on, allowing them
    to come up with reasonable answers for many queries. But **when tested in uncharted
    territory, their abilities dissipate**.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当 ChatGPT 首次发布时，我的一位朋友告诉我，与它交谈感觉像是魔法。**但就像一个魔术师将人锯成两半一样——在我们声称锯技术可以彻底改变手术之前，仔细审查表演并在不同环境下进行测试是很重要的**。LLM
    使用的“技巧”是它们训练了大量的文本，使它们能够对许多问题给出合理的答案。但**在未曾涉足的领域进行测试时，它们的能力会消失**。
- en: 'Will GPT-5 be any better? Assuming it still uses the GPT architecture and is
    only trained on more data and with more parameters, there is little reason to
    expect it will develop abstraction or reasoning abilities. As [Google’s AI researcher,
    François Chollet wrote](https://twitter.com/fchollet/status/1690461214579806208):
    “*It’s fascinating how the limitations of deep learning have stayed the same since
    2017\. Same problems, same failure modes, no progress*.”'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Since there has been a lot of discussion lately about AI regulation and the
    potential dangers of LLMs, I feel obligated to make it clear that the lack of
    true intelligence does not imply that there is no potential risk from LLMs. **It
    should be obvious that humanity possesses several technologies that have no claim
    for intelligence and yet can inflict harm on society in various ways, and they
    should be controlled.**
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'Through our renewed understanding of the limitations of LLMs, we can more accurately
    predict where the harm could come from: since intelligence does not seem imminent,
    Skynet and the Matrix should not worry us. What might worry us are activities
    that only require the rapid generation of real-looking texts, perhaps phishing
    and spreading fake news. However, whether LLMs truly provide a disruptive tool
    for these tasks is a different debate.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: What the future of AGI holds is anyone’s guess. Maybe some of the machine learning
    techniques used in LLMs will be used in a future intelligent artificial agent,
    and maybe they won’t. But there is little doubt that major pieces of the puzzle
    are still missing before the flexibility required for intelligence can emerge
    in machines.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
