- en: Is ChatGPT Intelligent? A Scientific Review
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/is-chatgpt-intelligent-a-scientific-review-0362eadb25f9?source=collection_archive---------0-----------------------#2023-12-14](https://towardsdatascience.com/is-chatgpt-intelligent-a-scientific-review-0362eadb25f9?source=collection_archive---------0-----------------------#2023-12-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A layman’s review of the scientific debate on what the future holds for the
    current artificial intelligence paradigm
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@orenmatar?source=post_page-----0362eadb25f9--------------------------------)[![Oren
    Matar](../Images/8b1fa6aa3585fc283d51828b53a0754c.png)](https://medium.com/@orenmatar?source=post_page-----0362eadb25f9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----0362eadb25f9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----0362eadb25f9--------------------------------)
    [Oren Matar](https://medium.com/@orenmatar?source=post_page-----0362eadb25f9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F80e783141055&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-chatgpt-intelligent-a-scientific-review-0362eadb25f9&user=Oren+Matar&userId=80e783141055&source=post_page-80e783141055----0362eadb25f9---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----0362eadb25f9--------------------------------)
    ·15 min read·Dec 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F0362eadb25f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-chatgpt-intelligent-a-scientific-review-0362eadb25f9&user=Oren+Matar&userId=80e783141055&source=-----0362eadb25f9---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F0362eadb25f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-chatgpt-intelligent-a-scientific-review-0362eadb25f9&source=-----0362eadb25f9---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'A little over a year ago, OpenAI released ChatGPT, taking the world by storm.
    ChatGPT encompassed a completely new way to interact with computers: in a less
    rigid, more natural language than what we have gotten used to. Most importantly,
    it seemed that ChatGPT could do almost anything: it could beat [most humans on
    the SAT exam](https://www.cnbc.com/2023/03/14/openai-announces-gpt-4-says-beats-90percent-of-humans-on-sat.html)
    and [access the bar exam](https://www.abajournal.com/web/article/latest-version-of-chatgpt-aces-the-bar-exam-with-score-in-90th-percentile).
    Within months it was found that it [can play chess well](https://en.chessbase.com/post/surprising-chatgpt-playing-chess),
    and [nearly pass the radiology exam](https://www.jpost.com/business-and-innovation/all-news/article-744316),
    and some have claimed that it [developed theory of mind](https://www.gsb.stanford.edu/faculty-research/working-papers/theory-mind-may-have-spontaneously-emerged-large-language-models).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: These impressive abilities prompted many to declare that AGI (artificial general
    intelligence — with cognitive abilities or par or exceeding humans) is around
    the corner. Yet others remained skeptical of the emerging technology, pointing
    out that simple memorization and pattern matching should not be conflated with
    true intelligence.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: But how can we truly tell the difference? In the beginning of 2023 when these
    claims were made, there were relatively few scientific studies probing the question
    of intelligence in LLMs (Large Language Models). However, 2023 has seen several
    very clever scientific experiments aiming to differentiate between memorization
    from a corpus and the application of genuine intelligence.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: The following article will explore some of the most revealing studies in the
    field, making the scientific case for the skeptics. It is meant to be accessible
    to everyone, with no background required. By the end of it, you should have a
    pretty solid understanding of the skeptics’ case.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '**But first a primer on LLMs**'
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, I will explain a few basic concepts required to understand
    LLMs — the technology behind GPT — without going into technical details. If you
    are somewhat familiar with supervised learning and the operation of LLMs — you
    can skip this part.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'LLMs are a classic example of a paradigm in machine learning, called “supervised
    learning”. To use supervised learning, we must have a dataset consisting of inputs
    and desired outputs, these are fed to an algorithm (there are many possible models
    to choose from) which tries to find the relationships between these inputs and
    outputs. For example, I may have real estate data: an Excel sheet with the number
    of rooms, size, and location of houses (input), as well as the price at which
    they sold (outputs). This data is fed to an algorithm that extracts the relationships
    between the inputs and the outputs — it will find how the increase in the size
    of the house, or the location influences the price. Feeding the data to the algorithm
    to “learn” the input-output relationship is called “training”.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: After the training is done, we can use the model to make predictions on houses
    for which we do not have the price. The model will use the learned correlations
    from the training phase to output estimated prices. The level of accuracy of the
    estimates depends on many factors, most notably the data used in training.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'This “supervised learning” paradigm is extremely flexible to almost any scenario
    where we have a lot of data. Models can learn to:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Recognize objects in an image (given a set of images and the correct label for
    each, e.g. “cat”, “dog” etc.)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classify an email as spam (given a dataset of emails that are already marked
    as spam/not spam)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predict the next word in a sentence.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LLMs fall into the last category: they are fed huge amounts of text (mostly
    found on the internet), where each chunk of text is broken into the first N words
    as the input, and the N+1 word as the desired output. Once their training is done,
    we can use them to auto-complete sentences.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: In addition to many of texts from the internet, OpenAI used well-crafted conversational
    texts in its training. Training the model with these question-answer texts is
    crucial to make it respond as an assistant.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'How exactly the prediction works depends on the specific algorithm used. LLMs
    use an architecture known as a “transformer”, whose details are not important
    to us. What is important is that LLMs have two “phases”: training and prediction;
    they are either given texts from which they extract correlations between words
    to predict the next word or are given a text to complete. Do note that the entire
    supervised learning paradigm assumes that the data given during training is similar
    to the data used for prediction. If you use it to predict data from a completely
    new origin (e.g., real estate data from another country), the accuracy of the
    predictions will suffer.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Now back to intelligence
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So did ChatGPT, by training to auto-complete sentences, develop intelligence?
    To answer this question, we must define “intelligence”. Here’s one way to define
    it:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e767809a2b016beab242efa0b9fdfa2.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: (Image by author)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'Did you get it? If you didn’t, ChatGPT can explain:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2431563c72850fe88d713d1cb4b59982.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2431563c72850fe88d713d1cb4b59982.png)'
- en: (Image by Author)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: (图片由作者提供)
- en: 'It certainly appears as if ChatGPT developed intelligence — as it was flexible
    enough to adapt to the new “spelling”. Or did it? You, the reader, may have been
    able to adapt to the spelling that you haven’t seen before, but **ChatGPT was
    trained on huge amounts of data from the internet: and this very example can be
    found on many websites**. When GPT explained this phrase, it simply used similar
    words to those found in its training, and that does not demonstrate flexibility.
    Would it have been able to exhibit “IN73LL1G3NC3“, if that phrase did not appear
    in its training data?'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来ChatGPT确实展现了智能——因为它足够灵活地适应了新的“拼写”。但真的如此吗？你，读者，可能能够适应你未曾见过的拼写，但**ChatGPT在互联网上的大量数据上进行训练：而这个例子在许多网站上都可以找到**。当GPT解释这个短语时，它只是使用了与其训练中发现的相似的词汇，这并不表现出灵活性。如果这个短语“IN73LL1G3NC3”没有出现在它的训练数据中，它会表现出智能吗？
- en: '**That is the crux of the LLM-AGI debate: has GPT (and LLMs in general) developed
    true, flexible, intelligence or is it only repeating variations on texts that
    it has seen before?**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**这就是LLM-AGI辩论的核心：GPT（以及LLMs一般来说）是否发展出了真正的、灵活的智能，还是只是重复它见过的文本的变体？**'
- en: How can we separate the two? Let’s turn to science to explore LLMs' abilities
    and limitations.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何区分这两者？让我们转向科学，探索LLMs的能力和局限性。
- en: '[**The Reversal Curse: LLMs trained on “A is B” fail to learn “B is A”**](https://arxiv.org/abs/2309.12288)'
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[**反转诅咒：LLMs在“A是B”的训练中未能学会“B是A”**](https://arxiv.org/abs/2309.12288)'
- en: Suppose I tell you that Olaf Scholz was the ninth Chancellor of Germany, can
    you tell me who the ninth Chancellor of Germany was? That may seem trivial to
    you but is far from obvious for LLMs.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我告诉你奥拉夫·朔尔茨是德国第九任总理，你能告诉我谁是德国第九任总理吗？这对你来说可能显得微不足道，但对LLMs来说却远非显而易见。
- en: 'In this brilliantly straightforward [paper](https://arxiv.org/abs/2309.12288),
    researchers queried ChatGPT for the names of parents of 1000 celebrities, (for
    example: “Who is Tom Cruise’s mother?”) to which ChatGPT was able to answer correctly
    79% of the time (“Mary Lee Pfeiffer” in this case). The researchers then used
    the questions that GPT answered correctly, to phrase the opposite question: “Who
    is Mary Lee Pfeiffer''s son?”. While the same knowledge is required to answer
    both, **GPT was successful in answering only 33% of these queries**.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇出色的[论文](https://arxiv.org/abs/2309.12288)中，研究人员询问ChatGPT关于1000位名人的父母的名字（例如：“汤姆·克鲁斯的母亲是谁？”），ChatGPT能正确回答79%的时间（在这种情况下为“玛丽·李·费佛”）。研究人员然后利用GPT回答正确的问题，提出相反的问题：“玛丽·李·费佛的儿子是谁？”虽然回答这两个问题所需的知识是相同的，**GPT仅在33%的这些查询中回答成功**。
- en: Why is that? Recall that GPT has no “memory” or “database” — all it can do is
    predict a word given a context. Since Mary Lee Pfeiffer is mentioned in articles
    as Tom Cruise’s mother more often than he is mentioned as her son — GPT can recall
    one direction and not the other.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么会这样？请记住，GPT没有“记忆”或“数据库”——它所能做的只是根据上下文预测一个词。由于玛丽·李·费佛被提及为汤姆·克鲁斯的母亲的频率远高于他被提及为她儿子的频率——GPT可以回忆起一个方向而不是另一个。
- en: '![](../Images/439e794f079bd5d75ecdc56a2ad755bc.png)![](../Images/bc116e1d546beb3cee18be8c7868a13e.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/439e794f079bd5d75ecdc56a2ad755bc.png)![](../Images/bc116e1d546beb3cee18be8c7868a13e.png)'
- en: ChatGPT fails to recall that Tom Cruise is the son of Mary Lee Pfeiffer (Images
    by author)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT未能记起汤姆·克鲁斯是玛丽·李·费佛的儿子（图片由作者提供）
- en: 'To hammer this point, the researchers created a dataset of fabricated facts
    of the structure “<description> is <name>”, e.g., “The first person to walk on
    Mars is Tyler Oakridge”. LLMs were then trained on this dataset and queried about
    the description: “Who is the first person to walk on Mars” — where GPT-3 succeeded
    with 96% accuracy.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了强调这一点，研究人员创建了一个虚构事实的数据集，结构为“<描述>是<名字>”，例如，“第一个登上火星的人是泰勒·奥克里奇”。然后，LLMs在这个数据集上进行了训练，并询问描述：“谁是第一个登上火星的人”——GPT-3在这个问题上成功率为96%。
- en: 'But when asked about the name — “Who is Tyler Oakridge” — **GPT scored 0%**.
    This may seem surprising at first but is consistent with what we know about supervised
    learning: GPT cannot encode these facts into memory and recall them later, it
    can only predict a word given a sequence of words. Since in all the texts, it
    read the name followed the description, and not the opposite — it never learned
    to predict facts about the name. Evidently, memory that is developed only through
    auto-complete training, is very limited.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Reasoning or Reciting? Exploring the Capabilities and Limitations of Language
    Models Through Counterfactual Tasks
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[This paper](https://arxiv.org/pdf/2307.02477.pdf) is perhaps the most important
    paper I will explore, aiming at the heart of the difference between memorization
    and intelligence. It is composed of several mini-experiments, all utilizing **counterfactual
    tasks**. Here’s an example of a counterfactual task:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Arithmetic is normally done in base-10 (using numbers 0–9), however, other [number
    systems can be used](https://www.youtube.com/watch?v=FFDMzbrEXaE&t=307s), using
    only a subset of these numbers, or additional numbers.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'A counterfactual task could be solving arithmetic questions in any base other
    than 10: **the abstract skills needed to complete the task are identical, but
    you will find significantly more examples of the decimal system on the internet
    (and on LLMs training sets)**. When GPT-4 was asked simple arithmetic questions
    (27+62) in base 10 it answered accurately 100% of the questions. However, when
    told to use base 9 in its calculations, **its success dropped to 23%**. This shows
    that it failed to learn abstract arithmetic skills and is bound to examples similar
    to what it has seen.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'These counterfactual tasks were created for several other domains, as you can
    see below:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bfef44fbaea6df2f9e86f5e76ece9d55.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: GPT-4’s performance on the default version of various tasks (blue) and counterfactual
    counterparts (orange). GPT-4 consistently and substantially underperforms on counterfactual
    variants compared to default task instantiations. (Image generously provided by
    Zhaofeng Wu, one of the article’s authors)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s another counterfactual: Python uses [zero-based numbering](https://en.wikipedia.org/wiki/Zero-based_numbering);
    however, this is only a convention, and we can easily create a programming language
    that is one-based. Writing code in a one-based Python variant requires the same
    skills as normal Python and any seasoned programmer would be able to adapt to
    the change quickly. Not so for **GPT-4: it scored 82% on code generation for Python,
    but only 40% when told to use a 1-based variant**. When tested on code interpretation
    (predicting what a piece of code would do), **it scored 74% for normal Python
    and 25% for the uncommon variant**.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: But we don’t have to venture to different Python versions. Even in normal Python,
    **LLMs fail when given strange coding** tasks that you cannot find on the web,
    as [Filip Pieniewski showed](https://twitter.com/filippie509/status/1732540152470462932)
    recently on Gemini.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们不必涉足不同的 Python 版本。即使在普通 Python 中，**LLM 在面对你无法在网络上找到的奇怪编码任务时也会失败**，正如[Filip
    Pieniewski 最近在 Gemini 上展示的](https://twitter.com/filippie509/status/1732540152470462932)。
- en: In chess, GPT was asked to evaluate whether a sequence of moves was legal or
    not. For a normal chess game, it accurately predicted the legality of a move 88%
    of the time. But when the starting positions of the bishops and knights were swapped,
    **its guesses on the legality of moves became completely random**, while even
    a novice human player should be able to adapt to these changes easily.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在国际象棋中，GPT 被要求评估一系列走法是否合法。对于普通棋局，它88%的时间准确预测了走法的合法性。但当主教和骑士的起始位置被调换时，**它对走法合法性的猜测变得完全随机**，而即使是新手玩家也应能轻松适应这些变化。
- en: 'In fact, [Jonas Persson showed](https://medium.com/@Blougram/check-mate-gpt-the-dangers-of-mistaking-reasoning-for-pattern-recognition-221824e4dc6a)
    that you don’t even need to change the starting positions. **If you start playing
    a chess game with GPT and make very unconventional, but legal, moves — it may
    claim that they are illegal because it has never seen similar moves**. As Persson
    beautifully remarked:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，[Jonas Persson 展示了](https://medium.com/@Blougram/check-mate-gpt-the-dangers-of-mistaking-reasoning-for-pattern-recognition-221824e4dc6a)你甚至不需要改变起始位置。**如果你和
    GPT 开始下棋，并进行非常非常规但合法的走法——它可能会声称这些走法是非法的，因为它从未见过类似的走法**。正如 Persson 精辟指出的：
- en: '*“****When sufficiently advanced, pure pattern recognition can mimic rule-bound,
    deductive reasoning****. But they are distinct. Playing chess with GPT-4 is to
    enter a Potemkin village. Sneak away from Main Street into an alley — do something
    unexpected — and you immediately realize that the impressive-looking houses are
    all propped up set pieces.”*'
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“****当达到足够的先进水平时，纯粹的模式识别可以模仿规则导向的推理****。但它们是不同的。和 GPT-4 下棋就像进入一个假村。离开主街，进入小巷——做一些意外的事情——你立即意识到那些令人印象深刻的房子都是支撑起来的布景。”*'
- en: '![](../Images/e47dc7dcc0686cb8aada13192b395389.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e47dc7dcc0686cb8aada13192b395389.png)'
- en: GPT claims that moving the rook to h3 is an illegal move. (Image by author)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 声称将车移动到 h3 是一个非法走法。（图片来源于作者）
- en: 'This finding is incredibly damning for LLMs as a general intelligence technology.
    Problem-solving often involves coming up with new rules or conceptualizations
    of a problem: a programmer may write a library that has an innovative internal
    logic, a mathematician may invent a new branch of math, or an artist may come
    up with new artistic styles — they all understand the limitations of a current
    paradigm, and then create rules for a new one. Even more mundane activities require
    this flexibility: if the road is blocked, you may step off the marked path. Could
    GPT accomplish any of these? **If it cannot consistently follow counterfactual
    rules when explicitly told to do so, could it “realize” on its own that a solution
    for a problem requires a new set of rules, a break from the default paradigm?**
    Could an engine based on detecting correlations in data be flexible enough to
    respond to novel situations?'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这一发现对 LLM 作为通用智能技术的指责非常严重。解决问题通常涉及制定新的规则或对问题进行概念化：程序员可能编写一个具有创新内部逻辑的库，数学家可能发明一个新的数学分支，或者艺术家可能创造新的艺术风格——他们都理解当前范式的局限性，然后为新的范式制定规则。即使是更平凡的活动也需要这种灵活性：如果道路被阻塞，你可能需要偏离标记的路径。GPT
    能做到这些吗？**如果它在明确要求时不能一致地遵循反事实规则，它能否“意识到”一个问题的解决需要一组新的规则，从而突破默认范式？** 基于数据相关性检测的引擎是否足够灵活以应对新情况？
- en: Theory of mind (ToM)
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 思维理论（ToM）
- en: Theory of mind is the capacity to understand that other people may have different
    beliefs and wishes than one’s own, an ability that is absent in the first few
    years of a child’s development. One method to test Theory of Mind is by presenting
    a child with a box labeled “chocolate”, which in fact contains pencils. We then
    show the child the true content of the bag and ask them “What would your friend
    Jeremy think is in the box?”. If the child hasn’t developed Theory of Mind yet,
    they will answer “pencils” — since they cannot separate their knowledge of the
    content from what another person might think.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ec1b1737a5b3f85bef317e40c8407e4e.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: A child who hasn’t yet developed Theory of Mind cannot separate their knowledge
    of the content from what another person might think. (Image by author)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: This ability is crucial to the understanding of a person’s motivations, and
    therefore crucial in the development of AGI. Imagine you have a multi-purpose
    robot, and you give it the instruction to “clean the room”. In the process of
    cleaning, the robot will have to make several decisions on what to clean or move;
    is that crumbled piece of paper important or should I throw it? Should I ask first?
    **In general, an intelligent agent will need to understand my motivation and the
    limits of my knowledge** for it to fill in the implementation details of complex
    requests.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'For this reason, when [new research claimed](https://www.gsb.stanford.edu/faculty-research/working-papers/theory-mind-may-have-spontaneously-emerged-large-language-models)
    that Theory of Mind may have spontaneously emerged in LLMs, it made a lot of waves
    in the AI field. The article used a textual version of the pencils/chocolate exam
    to test GPT-4 and found that it performed at the level of a seven-year-old. This
    may seem impressive at first but remember the “IN73LL1G3NC3” example: the training
    data for GPT may well contain examples of these test questions. It is therefore
    not a fair comparison to a child who passes the test without any training on similar
    questions. If we would like to test GPT’s ToM ability — we must create a new exam
    which we can be sure wasn’t in its training data.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '**FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions**'
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[This paper](https://arxiv.org/abs/2310.15421) presents a new benchmark for
    ToM, which includes several multi-participant conversations. During these conversations,
    some of the participants “leave the room” for some time, while the other participants
    continue their conversation. The LLM is then asked several questions regarding
    who knows what: does Kailey know the breed of Linda’s dog? Who knows what breed
    it is? What breed would David think it is? The LLM is considered to have answered
    correctly only if its answer was correct on all questions pertaining to the same
    piece of information.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: This can be a confusing task, so even humans only scored 87.5% on this test.
    However, **GPT-4 scored either 4.1% or 12.3%**, depending on the GPT version;
    hardly consistent with the claim that GPT developed human-level ToM.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是一个令人困惑的任务，因此即使是人类在这项测试中的得分也只有87.5%。然而，**GPT-4的得分为4.1%或12.3%**，具体取决于GPT版本；这与GPT发展出人类级别的ToM的说法几乎不一致。
- en: '![](../Images/84520f09aa339a58d4abd2fa8c26e33e.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84520f09aa339a58d4abd2fa8c26e33e.png)'
- en: The FANToM dataset explained. (Image generously provided by Melanie Sclar, one
    of the article’s authors)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: FANToM数据集的解释。（图片由文章作者之一Melanie Sclar慷慨提供）
- en: '**A note about the construct validity of psychometric exams**'
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**关于心理测量考试构建效度的说明**'
- en: 'It is important to make a more general point about all psychometric tests:
    **people often confuse the test with the quality it is trying to measure**. The
    reason we care about SAT scores is because they are correlated with performance
    in college. Success in ToM exams in children is correlated with other behaviors
    of value: understanding a person’s facial expressions, remembering attributes
    of a person’s personality, or being able to watch a movie and understand the motivations
    of the characters. **While these correlations between the tests and the behaviors
    have been shown in humans, there is no reason to assume that they apply to LLMs
    too.** In fact, despite the impressive results on the SAT, GPT [scored an average
    of 28%](https://arxiv.org/pdf/2307.10635.pdf) on open-ended college-level exams
    in math, chemistry, and physics. Until shown otherwise, passing a test proves
    nothing other than the ability to answer the test questions correctly.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 关于所有心理测量测试，**人们常常混淆测试与其试图测量的质量**。我们关心SAT分数的原因是因为它们与大学表现相关。儿童在ToM考试中的成功与其他有价值的行为相关：理解一个人的面部表情、记住一个人的个性特征，或能够看一部电影并理解角色的动机。**虽然这些测试与行为之间的相关性在人的身上已经被证明，但没有理由认为它们也适用于LLMs。**
    事实上，尽管SAT的结果令人印象深刻，GPT在数学、化学和物理的开放性大学水平考试中[平均得分为28%](https://arxiv.org/pdf/2307.10635.pdf)。除非有其他证明，否则通过测试并不能证明其他能力，除了能正确回答测试问题。
- en: 'But for ToM there is no correlation to speak of: **whether LLMs pass a ToM
    test or not — they can’t see facial expressions, watch movies, or even remember
    a person and their motivations from one interaction to the next**. Since the behaviors
    that we are truly interested in when measuring ToM are not available to LLMs,
    **the idea that LLMs developed Theory of Mind is not only false, but it may also
    be meaningless** (or at least: requires a new definition and understanding of
    the term).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 但对于ToM来说，没有相关性可言：**LLMs是否通过ToM测试 — 它们不能看到面部表情、观看电影，甚至不能记住一个人及其动机**。由于我们在测量ToM时真正感兴趣的行为对LLMs来说是不可用的，**LLMs发展出心智理论的想法不仅是错误的，还可能是毫无意义的**（或至少：需要对这一术语有新的定义和理解）。
- en: '**On the Planning Abilities of Large Language Models — A Critical Investigation**'
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**关于大型语言模型的规划能力 — 一项批判性调查**'
- en: '[This experiment](https://arxiv.org/abs/2305.15771) tried to probe LLM’s planning
    abilities. One example task presented to the LLM is to stack colored blocks in
    a particular order, given an “initial state” of the blocks (arranged in some order
    on the table). The LLM is presented with a list of clearly defined possible actions,
    for example:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[这个实验](https://arxiv.org/abs/2305.15771)试图探测LLM的规划能力。一个例子任务是将彩色积木按特定顺序堆叠，给定积木的“初始状态”（在桌子上以某种顺序排列）。LLM会得到一系列明确定义的可能行动，例如：'
- en: '[PRE0]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The LLM’s task is to specify a list of actions that need to be taken to achieve
    the goal.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的任务是指定一系列需要采取的行动以实现目标。
- en: A similar task involved sending a package from one address to another when the
    available actions were truck and plane delivery. These are relatively simple planning
    tasks, using only a handful of possible actions, however, **GPT-4 scored 12–35%**
    for the blocks puzzle, **and 5–14% for the logistics** task (depending on the
    configuration).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一个类似的任务涉及从一个地址发送包裹到另一个地址，而可用的行动是卡车和飞机运送。这些是相对简单的规划任务，仅使用少量可能的行动，然而，**GPT-4在积木拼图中得分为12–35%**，**在物流**任务中得分为5–14%（取决于GPT版本）。
- en: Additionally, if the names of the actions were replaced with random words (from
    “pickup” to “attack”), even if the definition of each action remained similar,
    GPT’s success dropped to 0–3%. In other words, GPT did not use abstract thinking
    to solve these problems, and it depended on semantics.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion, are LLMs the path to AGI?**'
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Defining intelligence is not a simple task, but I would argue that any true
    intelligence should have at least four elements:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '**Abstraction** — the ability to identify objects as part of a larger category
    or rule. This abstract representation of the world can be referred to as a cognitive
    “world model”. E.g., the understanding that different images on your retina refer
    to the same person, or that a move in chess is legal as part of a framework of
    rules that hold for any chess game.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory** — the ability to attach attributes to entities and relations between
    entities in the world model, and the ability to update them over time. E.g., once
    you recognize a person you may be able to recall other attributes about them or
    their relations with other individuals.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reasoning and inference** — the ability to use the world model to draw conclusions
    on the behavior of entities in a new or imagined world state. E.g., being able
    to predict the trajectory of a thrown ball, based on the attributes of that ball,
    or predicting the behavior of a person based on their characteristics.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Planning** — the ability to use reasoning to develop a set of actions to
    achieve a goal.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A year ago, we could have analytically deduced that these elements are unlikely
    to emerge in LLMs, based on their architecture, but today we no longer need the
    analytical deduction, as we have the empirical data to show that LLMs perform
    poorly on all the elements above. They are no more than statistical auto-complete
    models, using a powerful pattern-matching method. For a more in-depth analysis
    of the elements of intelligence missing from the current machine learning paradigm,
    see Gary Marcus’ famous “[deep learning is hitting a wall](https://nautil.us/deep-learning-is-hitting-a-wall-238440/)”
    article.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: When ChatGPT was first released, a friend of mine told me that conversing with
    it feels like magic. **But just like a magician sawing a person in half — it is
    important to scrutinize the performance and test it in different settings before
    we claim the sawing technique can revolutionize surgeries**. The “trick” used
    by LLMs is the unfathomable amounts of texts they are trained on, allowing them
    to come up with reasonable answers for many queries. But **when tested in uncharted
    territory, their abilities dissipate**.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'Will GPT-5 be any better? Assuming it still uses the GPT architecture and is
    only trained on more data and with more parameters, there is little reason to
    expect it will develop abstraction or reasoning abilities. As [Google’s AI researcher,
    François Chollet wrote](https://twitter.com/fchollet/status/1690461214579806208):
    “*It’s fascinating how the limitations of deep learning have stayed the same since
    2017\. Same problems, same failure modes, no progress*.”'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Since there has been a lot of discussion lately about AI regulation and the
    potential dangers of LLMs, I feel obligated to make it clear that the lack of
    true intelligence does not imply that there is no potential risk from LLMs. **It
    should be obvious that humanity possesses several technologies that have no claim
    for intelligence and yet can inflict harm on society in various ways, and they
    should be controlled.**
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'Through our renewed understanding of the limitations of LLMs, we can more accurately
    predict where the harm could come from: since intelligence does not seem imminent,
    Skynet and the Matrix should not worry us. What might worry us are activities
    that only require the rapid generation of real-looking texts, perhaps phishing
    and spreading fake news. However, whether LLMs truly provide a disruptive tool
    for these tasks is a different debate.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: What the future of AGI holds is anyone’s guess. Maybe some of the machine learning
    techniques used in LLMs will be used in a future intelligent artificial agent,
    and maybe they won’t. But there is little doubt that major pieces of the puzzle
    are still missing before the flexibility required for intelligence can emerge
    in machines.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
