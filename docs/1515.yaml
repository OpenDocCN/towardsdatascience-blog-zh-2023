- en: 'Integrating Neural Net: Deriving the Normal Distribution CDF'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/integrating-neural-net-deriving-the-normal-distribution-cdf-ea14574548ea?source=collection_archive---------7-----------------------#2023-05-03](https://towardsdatascience.com/integrating-neural-net-deriving-the-normal-distribution-cdf-ea14574548ea?source=collection_archive---------7-----------------------#2023-05-03)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Integrating a function using a neural network (with code)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@john_morrow?source=post_page-----ea14574548ea--------------------------------)[![John
    Morrow](../Images/4a8ce62a0b4e1eb1cf77ecaba6b7ddcc.png)](https://medium.com/@john_morrow?source=post_page-----ea14574548ea--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ea14574548ea--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ea14574548ea--------------------------------)
    [John Morrow](https://medium.com/@john_morrow?source=post_page-----ea14574548ea--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb4bcd051bb38&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintegrating-neural-net-deriving-the-normal-distribution-cdf-ea14574548ea&user=John+Morrow&userId=b4bcd051bb38&source=post_page-b4bcd051bb38----ea14574548ea---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ea14574548ea--------------------------------)
    ¬∑6 min read¬∑May 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fea14574548ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintegrating-neural-net-deriving-the-normal-distribution-cdf-ea14574548ea&user=John+Morrow&userId=b4bcd051bb38&source=-----ea14574548ea---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fea14574548ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintegrating-neural-net-deriving-the-normal-distribution-cdf-ea14574548ea&source=-----ea14574548ea---------------------bookmark_footer-----------)![](../Images/a4ae015d22dc9fef5466d38897886747.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Jack Anstey](https://unsplash.com/@jack_anstey?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Introduction**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article presents a method for training a neural network to derive the integral
    of a function. The technique works not only with analytically-solvable integrals
    but also with integrals that do not have a closed-form solution and are typically
    solved by numerical methods. An example is the normal distribution‚Äôs cumulative
    density function (CDF). Equation 1 is this distribution‚Äôs probability density
    function (PDF), and Equation 2 is its CDF, the integral of the PDF. Figure 1 is
    a plot of these functions. After being trained, the resulting network can serve
    as a stand-alone function generator that delivers points on the CDF curve given
    points from the domain of the distribution‚Äôs PDF.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/94fc44bd8cc33bdf637f916c2bdb174d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Equation 1: **PDF (with u=0, ùúé=1)**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/edc8ecd01cc6d361ff26ba2a908dc473.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Equation2: **CDF**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/20af856a48703278cd9f0085b2662f4c.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. **PDF and CDF of the normal distribution**
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Integrating neural network**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An integrating neural network is trained to produce the integral of a function
    *y = f(x)*. Expressed in terms of the network‚Äôs input and output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/72fa511f99bc616132acfde0459a19f5.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 3
  prefs: []
  type: TYPE_NORMAL
- en: where *h* and *x* are the network‚Äôs output and input, respectively. For the
    normal distribution, *f(x)* is given by Equation 1, the PDF of the distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Integration of the function is accomplished by training the neural network such
    that the *derivative of the network‚Äôs output is equal to the function‚Äôs output,
    resulting in the network‚Äôs output becoming the integral of the function*.
  prefs: []
  type: TYPE_NORMAL
- en: '**2.1 Neural network training**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Following are the steps of the training procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Apply a training point, *x·µ¢* , to the function *y = f(x)*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/0d50d763a59f6dbf18e42ec1cc21d51b.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 4
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Also apply *x·µ¢* to the input of the neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: (The neural network model comprises a single input, *x*, two hidden layers,
    and a single output, *h*, and is represented by *h(x) = nn_model(x)*.)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4e9d3afa34ec48afde0aaaf37d792e6a.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 5
  prefs: []
  type: TYPE_NORMAL
- en: '3\. Take the derivative of h*·µ¢* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/440d0ba5cc93ff27f319bfccece30c4a.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6
  prefs: []
  type: TYPE_NORMAL
- en: (Differentiation is provided in TensorFlow and PyTorch via their automatic differentiation
    function. In this article, the neural network is developed with TensorFlow [GradientTape](https://www.tensorflow.org/guide/autodiff).)
  prefs: []
  type: TYPE_NORMAL
- en: '4\. Train the neural network with a loss function (loss 2 in Section 2.2) that
    forces the following relationship:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c1707bbc428483113966cec2fcf1ff66.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 7
  prefs: []
  type: TYPE_NORMAL
- en: 'Then after the neural network is trained, since *g = y*, and *s*ubstituting
    *g* and *y* from Equation 6 and Equation 4, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/befd286c620dd7aa2a595614937418da.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 8
  prefs: []
  type: TYPE_NORMAL
- en: 'Integrating both sides of Equation 8 confirms that the neural network‚Äôs output
    is the integral of the function *f(x)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c55fbd86a0f38112beb5b2c47d8fccf9.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 9
  prefs: []
  type: TYPE_NORMAL
- en: where *C* is the constant of integration.
  prefs: []
  type: TYPE_NORMAL
- en: '**2.2 Neural network loss function**'
  prefs: []
  type: TYPE_NORMAL
- en: Typically, a neural network is trained with pairs of known input and output
    data. The training input data is presented to the neural network, and the resulting
    output is compared to the training output data using a loss function. The loss
    returned by this function is used via backpropagation to adjust the network‚Äôs
    weights to reduce the loss. An integrating neural network uses a custom loss function
    to constrain the neural network to produce an output that complies with the output
    of the integrated function.
  prefs: []
  type: TYPE_NORMAL
- en: The loss function for the integrating neural network, Figure 2, has three components.
    Loss 2, described in the training procedure above (Section 2.1), forces the output
    of the neural network to comply with the integral of *f(x)*.
  prefs: []
  type: TYPE_NORMAL
- en: Loss 3 forces the neural network to comply with the initial condition *h(x_init2)
    = h_init2*. For the CDF model, this condition is *h(‚àí10) = 0*, which sets *C =
    0* (Equation 9). For the purpose of this model, the responses of the PDF and CDF
    at *x = ‚àí10* approximate the responses at *x = ‚àí‚àû*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting the initial condition in Loss 3 to *h(‚àí‚àû) = 0* also simplifies the
    CDF calculation. Expanding the definite integral of Equation 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7851d45aed1c315196b1004b80825ecd.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 10
  prefs: []
  type: TYPE_NORMAL
- en: 'The initial condition, *h(‚àí‚àû) = 0*, means that the second term equals zero,
    and the output of the trained neural network is the value of the CDF for the corresponding
    *x* input:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/93fb2602d50c605008d27656cac01573.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 11
  prefs: []
  type: TYPE_NORMAL
- en: Loss 1, with condition *h(10) = 1*, stabilizes the training process for points
    near the right tail of the distribution. For the purpose of this model, the responses
    of the PDF and CDF at *x = 10* approximate the responses at
  prefs: []
  type: TYPE_NORMAL
- en: x = ‚àû.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/843aebca813b98102b1a60a320978e94.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. **Loss function**
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. Integrating neural network implementation**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Following is the Python code for the integrating neural network implementation
    of the normal distribution CDF. The complete code is available [here](https://github.com/jmorrow1000/integrating-nn).
  prefs: []
  type: TYPE_NORMAL
- en: '**3.1 Neural network model definition**'
  prefs: []
  type: TYPE_NORMAL
- en: The neural network has two fully-connected hidden layers, each with 512 neurons.
    There is a single input for domain points and a single output for the corresponding
    integral values.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 1\. **TensorFlow neural network model**
  prefs: []
  type: TYPE_NORMAL
- en: '**3.2 Initialization**'
  prefs: []
  type: TYPE_NORMAL
- en: The *x·µ¢* training points from Section 2.1 for loss 2 are defined on line 9,
    below. The order of these points is randomly shuffled on line 11 to promote stable
    training of the neural network. On line 12 the points are applied to the PDF as
    described in Equation 4.
  prefs: []
  type: TYPE_NORMAL
- en: The initial conditions for loss 1 and loss 3 are defined in lines 15-16 and
    lines 19-20, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2\. **Initialization**
  prefs: []
  type: TYPE_NORMAL
- en: '**3.3 Batch training step**'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3 is the training step function applied to each batch of training points.
    The total loss (the sum of loss 1, loss 2, and loss 3) in line 24 is used to update
    the neural network‚Äôs weights via gradient descent (lines 26 -30). Each training
    epoch includes multiple batches, which collectively use all the training points
    in model updates.
  prefs: []
  type: TYPE_NORMAL
- en: Line 9 produces the neural network‚Äôs response to the *x_init* initial condition.
    The response is compared to the corresponding initial condition, *h_init*, producing
    loss 1 (line 10).
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, Line 13 produces the network‚Äôs response to the *x_init2* initial
    condition. The response is compared to the corresponding initial condition, *h_init2*,
    producing loss 3 (line 14).
  prefs: []
  type: TYPE_NORMAL
- en: Line 17 produces the network‚Äôs response to training point *x·µ¢* (Equation 5).
    Line 18 extracts the gradient of the response (Equation 6), and lines 19‚Äì20 compare
    the gradient to *f(x·µ¢)* (Equation 7), producing loss 2.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3\. **Batch training step**
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Figure 3 shows the CDF response (red trace) from the output of the trained neural
    network. As verification of the accuracy of the results, the CDF response from
    the *norm.cdf* function in the Python [SciPy](https://scipy.org) library is included
    (green dots).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9168084258ef4ca60bc87b5f9ac90b84.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. **Trained CDF neural network output**
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4 is the loss from the total loss function v.s. epoch logged during the
    training process.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6ef33914bc3c560f3192416eff377c17.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. **Training loss**
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article demonstrates a method for training a neural network to integrate
    a function by using a custom loss function and automatic differentiation. Specifically,
    a neural network is trained to successfully integrate the PDF of the normal distribution
    to produce the CDF.
  prefs: []
  type: TYPE_NORMAL
- en: An upcoming article will present a method for training a neural network to invert
    a function. The inverting network will be used to invert the output of the CDF-trained
    network from this article, then produce samples from the normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '**A pdf of this article is available for download** [**here**](https://github.com/jmorrow1000/integrating-nn)**.**'
  prefs: []
  type: TYPE_NORMAL
- en: '*All images, unless otherwise noted, are by the author.*'
  prefs: []
  type: TYPE_NORMAL
