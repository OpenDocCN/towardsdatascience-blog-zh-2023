- en: Solving Unity Environment with Deep Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/solving-unity-environment-with-deep-reinforcement-learning-836dc181ee3b?source=collection_archive---------10-----------------------#2023-02-20](https://towardsdatascience.com/solving-unity-environment-with-deep-reinforcement-learning-836dc181ee3b?source=collection_archive---------10-----------------------#2023-02-20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: End to End Project with code of a PyTorch implementation of Deep Reinforcement
    Learning Agent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://gabrielcassimiro17.medium.com/?source=post_page-----836dc181ee3b--------------------------------)[![Gabriel
    Cassimiro](../Images/2cf8a09a706236059c46c7f0f20d4365.png)](https://gabrielcassimiro17.medium.com/?source=post_page-----836dc181ee3b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----836dc181ee3b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----836dc181ee3b--------------------------------)
    [Gabriel Cassimiro](https://gabrielcassimiro17.medium.com/?source=post_page-----836dc181ee3b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3692fb93d7e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-unity-environment-with-deep-reinforcement-learning-836dc181ee3b&user=Gabriel+Cassimiro&userId=3692fb93d7e5&source=post_page-3692fb93d7e5----836dc181ee3b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----836dc181ee3b--------------------------------)
    ·6 min read·Feb 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F836dc181ee3b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-unity-environment-with-deep-reinforcement-learning-836dc181ee3b&user=Gabriel+Cassimiro&userId=3692fb93d7e5&source=-----836dc181ee3b---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F836dc181ee3b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-unity-environment-with-deep-reinforcement-learning-836dc181ee3b&source=-----836dc181ee3b---------------------bookmark_footer-----------)![](../Images/407363663dbe1f188dd99ddb58cb70e2.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image by [Arseny Togulev](https://unsplash.com/@tetrakiss?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/pt-br/s/fotografias/robot?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: Unity is a popular game development engine that allows developers to create
    games with stunning graphics and immersive gameplay. It is widely used for developing
    games across various platforms, including mobile, PC, and consoles. However, creating
    intelligent and challenging game environments is a challenging task for game developers.
    This is where Deep Reinforcement Learning (DRL) comes into play.
  prefs: []
  type: TYPE_NORMAL
- en: DRL is a subset of machine learning that combines deep learning and reinforcement
    learning. It is a powerful technique that has been used to solve complex tasks
    in various domains, including robotics, finance, and gaming. In recent years,
    DRL has become a popular approach to building intelligent game agents that can
    learn from experience and adapt to new situations.
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we will explore how DRL can be used to solve Unity game environments.
    We will go through an implementation of DRL in the Unity environment to collect
    Bananas. We will also explore some of the challenges associated with using DRL
    in game development and how these challenges can be overcome.
  prefs: []
  type: TYPE_NORMAL
- en: This was a project for the Deep Reinforcement Learning specialization from Udacity.
    The full project and code can be found on this [Github repo](https://github.com/gabrielcassimiro17/rl-dqn-collect-bananas).
  prefs: []
  type: TYPE_NORMAL
- en: Objective
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This project has the objective to train an Agent using Deep Q Learning. The
    agent will be trained to collect yellow bananas while avoiding blue bananas from
    Unity’s Banana Collector environment.
  prefs: []
  type: TYPE_NORMAL
- en: More information about the Unix environment can be found [here](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Examples.md#banana-collector).
    The agent was trained using a Deep Q Learning algorithm and was able to solve
    the environment in 775 episodes.
  prefs: []
  type: TYPE_NORMAL
- en: Enviroment & Task
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The environment consists in a square world with yellow and blue bananas. The
    agent has the objective to collect as many yellow bananas as possible while avoiding
    the blue ones. The agent has 4 possible actions: move forward, move backward,
    turn left and turn right.'
  prefs: []
  type: TYPE_NORMAL
- en: The state space has 37 dimensions and contains the agent’s velocity, along with
    ray-based perception of objects around the agent’s forward direction. A reward
    of +1 is provided for collecting a yellow banana, and a reward of -1 is provided
    for collecting a blue banana.
  prefs: []
  type: TYPE_NORMAL
- en: The task is episodic, and in order to solve the environment, the agent must
    get an average score of +13 over 100 consecutive episodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The env looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/03b8b6f99cd8f3437e3a8c7c5fc4a448.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '**The Agent**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To solve the problem given by the environment it was implemented a Deep Q Learning
    algorithm. The algorithm is based on the paper [Human-level control through deep
    reinforcement learning](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)
    by DeepMind.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm works by using a neural network to approximate the Q-Function.
    The neural network receives the state as input and outputs the Q-Value for each
    action. Then it uses the Q-Value to select the best action to be taken by the
    agent. The algorithm learns by using the Q-Learning algorithm to train the neural
    network. There are also two problems with a simple implementation of the algorithm:
    correlated experiences and correlated targets. The algorithm uses two techniques
    to solve these problems: Experience Replay and Fixed Q-Targets.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Correlated experiences**'
  prefs: []
  type: TYPE_NORMAL
- en: Correlated experiences refer to a situation where the experiences (or transitions)
    of an agent are correlated with each other, meaning they are not independent and
    identically distributed. This can lead to an overestimation of the expected reward
    of a particular state or action, resulting in poor performance or convergence
    to suboptimal policies.
  prefs: []
  type: TYPE_NORMAL
- en: To solve this problem it is used a technique called **Experience Replay**. The
    technique consists in storing the experiences of the agent in a replay buffer
    and sampling randomly from it to train the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: '**Correlated targets**'
  prefs: []
  type: TYPE_NORMAL
- en: Correlated targets refer to a situation where the target values used to update
    the policy are not independent of each other, leading to correlation in the learning
    signal. This can slow down or prevent convergence to the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve this problem it is used a technique called **Fixed Q-Targets**. The
    technique consists in using two neural networks: the local network and the target
    network. The local network is used to select the best action to be taken by the
    agent while the target network is used to calculate the target value for the Q-Learning
    algorithm. The target network is updated every 4 steps with the weights of the
    local network.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the implementation in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Neural network architecture**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The neural network architecture used in the algorithm is a simple fully connected
    neural network with 2 hidden layers. The input layer has 37 neurons, the output
    layer has 4 neurons and the hidden layers have 64 neurons each. The activation
    function used in the hidden layers is ReLU and the activation function used in
    the output layer is the identity function.
  prefs: []
  type: TYPE_NORMAL
- en: The optimizer used for this implementation is Adam with a learning rate of 0.0005.
  prefs: []
  type: TYPE_NORMAL
- en: The library used to implement the neural network was PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: 'This was the implementation of the neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: Training Task
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To train the agent we used a loop to interact with the environment, collect
    and learn from the experiences. One of the hyperparameters used in the training
    task was the number of episodes. This first hyperparameter was tuned manually
    trying to optimize the training time and the performance of the agent. The number
    of episodes used in the final implementation was 1200 however the env was solved
    in 775.
  prefs: []
  type: TYPE_NORMAL
- en: The second hyperparameter used in the training task was the number of steps
    per episode. This hyperparameter was also tuned manually trying to optimize the
    training time and the performance of the agent. The bigger the number of steps
    the more the agent can explore the environment but it increases a lot the training
    time. The number of steps per episode used in the final implementation was 1000.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some other hyperparameters used:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Replay buffer size: 1000'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Batch size: 32'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Update every: 4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gamma: 0.99'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tau: 1e-3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Learning rate: 0.0005'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With this, we were able to solve the environment in 775 episodes. The plot below
    shows the progress of the agent in obtaining higher rewards.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0c871cf37146a7d3bd8546b3624108df.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Here we can see the rewards increase as the agent improves. The tradeoff between
    **exploration and exploitation** is also visible in the plot, where the agent
    explores more in the first 200 episodes and then starts to exploit the environment
    and get higher rewards.
  prefs: []
  type: TYPE_NORMAL
- en: The full implementation can be found on this [GitHub repo](https://github.com/gabrielcassimiro17/rl-dqn-collect-bananas).
  prefs: []
  type: TYPE_NORMAL
- en: While we were able to solve the environment, there are a few improvements that
    can be applied to speed up the solution.
  prefs: []
  type: TYPE_NORMAL
- en: '**Future improvements**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The algorithm can be improved by using the following techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: Dueling DQN — [paper](https://arxiv.org/pdf/1511.06581.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prioritized Experience Replay — [paper](https://arxiv.org/pdf/1511.05952.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another possible improvement is to work with pixel data from the environment.
    These improvements will likely be a topic for a new article, and I intend on going
    deeper into the core concepts and implementing the NN with TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for Reading!
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few other articles you might be interested:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/object-detection-with-tensorflow-model-and-opencv-d839f3e42849?source=post_page-----836dc181ee3b--------------------------------)
    [## Object detection with Tensorflow model and OpenCV'
  prefs: []
  type: TYPE_NORMAL
- en: Using a trained model to identify objects on static images and live video
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/object-detection-with-tensorflow-model-and-opencv-d839f3e42849?source=post_page-----836dc181ee3b--------------------------------)
    [](/how-to-prepare-for-the-gcp-professional-machine-learning-engineer-exam-b1c59967355f?source=post_page-----836dc181ee3b--------------------------------)
    [## How to prepare for the GCP Professional Machine Learning Engineer exam
  prefs: []
  type: TYPE_NORMAL
- en: Courses review, study tips, and how I did it
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-to-prepare-for-the-gcp-professional-machine-learning-engineer-exam-b1c59967355f?source=post_page-----836dc181ee3b--------------------------------)
  prefs: []
  type: TYPE_NORMAL
