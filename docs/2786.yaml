- en: Quantize Llama models with GGUF and llama.cpp
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 GGUF 和 llama.cpp 对 Llama 模型进行量化
- en: 原文：[https://towardsdatascience.com/quantize-llama-models-with-ggml-and-llama-cpp-3612dfbcc172?source=collection_archive---------0-----------------------#2023-09-04](https://towardsdatascience.com/quantize-llama-models-with-ggml-and-llama-cpp-3612dfbcc172?source=collection_archive---------0-----------------------#2023-09-04)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/quantize-llama-models-with-ggml-and-llama-cpp-3612dfbcc172?source=collection_archive---------0-----------------------#2023-09-04](https://towardsdatascience.com/quantize-llama-models-with-ggml-and-llama-cpp-3612dfbcc172?source=collection_archive---------0-----------------------#2023-09-04)
- en: GGML vs. GPTQ vs. NF4
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GGML 与 GPTQ 与 NF4
- en: '[](https://medium.com/@mlabonne?source=post_page-----3612dfbcc172--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----3612dfbcc172--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3612dfbcc172--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3612dfbcc172--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----3612dfbcc172--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne?source=post_page-----3612dfbcc172--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----3612dfbcc172--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3612dfbcc172--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3612dfbcc172--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----3612dfbcc172--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc89da634938&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantize-llama-models-with-ggml-and-llama-cpp-3612dfbcc172&user=Maxime+Labonne&userId=dc89da634938&source=post_page-dc89da634938----3612dfbcc172---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3612dfbcc172--------------------------------)
    ·9 min read·Sep 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3612dfbcc172&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantize-llama-models-with-ggml-and-llama-cpp-3612dfbcc172&user=Maxime+Labonne&userId=dc89da634938&source=-----3612dfbcc172---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc89da634938&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantize-llama-models-with-ggml-and-llama-cpp-3612dfbcc172&user=Maxime+Labonne&userId=dc89da634938&source=post_page-dc89da634938----3612dfbcc172---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3612dfbcc172--------------------------------)
    · 9 分钟阅读 · 2023年9月4日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3612dfbcc172&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantize-llama-models-with-ggml-and-llama-cpp-3612dfbcc172&user=Maxime+Labonne&userId=dc89da634938&source=-----3612dfbcc172---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3612dfbcc172&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantize-llama-models-with-ggml-and-llama-cpp-3612dfbcc172&source=-----3612dfbcc172---------------------bookmark_footer-----------)![](../Images/ccb4e06ca80fad8db1f16998deaf34be.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3612dfbcc172&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantize-llama-models-with-ggml-and-llama-cpp-3612dfbcc172&source=-----3612dfbcc172---------------------bookmark_footer-----------)![](../Images/ccb4e06ca80fad8db1f16998deaf34be.png)'
- en: Image by author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Due to the massive size of Large Language Models (LLMs), quantization has become
    an essential technique to run them efficiently. By reducing the precision of their
    weights, you can save memory and speed up inference while preserving most of the
    model’s performance. Recently, 8-bit and 4-bit quantization unlocked the possibility
    of **running LLMs on consumer hardware**. Coupled with the release of Llama models
    and parameter-efficient techniques to fine-tune them (LoRA, QLoRA), this created
    a rich ecosystem of local LLMs that are now competing with OpenAI’s GPT-3.5 and
    GPT-4.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大型语言模型（LLMs）的庞大规模，量化已成为高效运行这些模型的关键技术。通过降低权重的精度，可以节省内存并加速推理，同时保持模型的绝大部分性能。近期，8
    位和 4 位量化技术使**在消费级硬件上运行 LLMs**成为可能。加上 Llama 模型的发布及其参数高效微调技术（如 LoRA、QLoRA），这形成了一个丰富的本地
    LLM 生态系统，与 OpenAI 的 GPT-3.5 和 GPT-4 竞争。
- en: 'Besides the naive approach covered [in this article](https://medium.com/towards-data-science/introduction-to-weight-quantization-2494701b9c0c),
    there are three main quantization techniques: NF4, GPTQ, and GGML. [NF4](https://huggingface.co/blog/4bit-transformers-bitsandbytes)
    is a static method used by QLoRA to load a model in 4-bit precision to perform
    fine-tuning. [In a previous article](https://medium.com/towards-data-science/4-bit-quantization-with-gptq-36b0f4f02c34),
    we explored the GPTQ method and quantized our own model to run it on a consumer
    GPU. In this article, we will introduce the GGML technique, see how to quantize
    Llama models, and provide tips and tricks to achieve the best results.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在[这篇文章](https://medium.com/towards-data-science/introduction-to-weight-quantization-2494701b9c0c)中讨论的天真方法外，还有三种主要的量化技术：NF4、GPTQ和GGML。[NF4](https://huggingface.co/blog/4bit-transformers-bitsandbytes)是一种静态方法，QLoRA使用它以4位精度加载模型进行微调。[在上一篇文章中](https://medium.com/towards-data-science/4-bit-quantization-with-gptq-36b0f4f02c34)，我们探讨了GPTQ方法，并将自己的模型量化以在消费级GPU上运行。在这篇文章中，我们将介绍GGML技术，看看如何量化Llama模型，并提供实现最佳结果的技巧和窍门。
- en: You can find the code on [Google Colab](https://colab.research.google.com/drive/1pL8k7m04mgE5jo2NrjGi8atB0j_37aDD?usp=sharing)
    and [GitHub](https://github.com/mlabonne/llm-course).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[Google Colab](https://colab.research.google.com/drive/1pL8k7m04mgE5jo2NrjGi8atB0j_37aDD?usp=sharing)和[GitHub](https://github.com/mlabonne/llm-course)上找到代码。
- en: What is GGML?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是GGML？
- en: GGML is a C library focused on machine learning. It was created by Georgi Gerganov,
    which is what the initials “GG” stand for. This library not only provides foundational
    elements for machine learning, such as tensors, but also a **unique binary format**
    to distribute LLMs.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: GGML是一个专注于机器学习的C语言库。它由Georgi Gerganov创建，"GG"就是这个名字的缩写。这个库不仅提供了机器学习的基础元素，如张量，还提供了**独特的二进制格式**以分发LLM。
- en: This format recently changed to **GGUF**. This new format is designed to be
    extensible, so that new features shouldn’t break compatibility with existing models.
    It also centralizes all the metadata in one file, such as special tokens, RoPE
    scaling parameters, etc. In short, it answers a few historical pain points and
    should be future-proof. For more information, you can read the specification [at
    this address](https://github.com/philpax/ggml/blob/gguf-spec/docs/gguf.md). In
    the rest of the article, we will call “GGML models” all models that either use
    GGUF or previous formats.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这个格式最近更改为**GGUF**。这个新格式旨在可扩展，以便新功能不会破坏与现有模型的兼容性。它还将所有元数据集中在一个文件中，如特殊令牌、RoPE缩放参数等。简而言之，它解决了一些历史痛点，并且应该具有未来保障。有关更多信息，你可以阅读[这个地址](https://github.com/philpax/ggml/blob/gguf-spec/docs/gguf.md)上的规范。在本文的其余部分，我们将称所有使用GGUF或以前格式的模型为“GGML模型”。
- en: GGML was designed to be used in conjunction with the [llama.cpp](https://github.com/ggerganov/llama.cpp)
    library, also created by Georgi Gerganov. The library is written in C/C++ for
    efficient inference of Llama models. It can load GGML models and **run them on
    a CPU**. Originally, this was the main difference with GPTQ models, which are
    loaded and run on a GPU. However, you can now offload some layers of your LLM
    to the GPU with llama.cpp. To give you an example, there are 35 layers for a 7b
    parameter model. This drastically speeds up inference and allows you to run LLMs
    that don’t fit in your VRAM.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: GGML旨在与[llama.cpp](https://github.com/ggerganov/llama.cpp)库一起使用，该库也由Georgi Gerganov创建。这个库是用C/C++编写的，以便高效推断Llama模型。它可以加载GGML模型并**在CPU上运行**。最初，这与GPTQ模型的主要区别在于GPTQ模型是在GPU上加载和运行的。然而，现在你可以通过llama.cpp将一些LLM的层卸载到GPU上。举个例子，7b参数模型有35层。这大大加快了推断速度，并允许你运行那些无法完全装入VRAM的LLM。
- en: '![](../Images/1839d4c1f226033fd5cef2957dfb7170.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1839d4c1f226033fd5cef2957dfb7170.png)'
- en: Image by author
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: If command-line tools are your thing, llama.cpp and GGUF support have been integrated
    into many GUIs, like oobabooga’s [text-generation-web-ui](https://github.com/oobabooga/text-generation-webui),
    [koboldcpp](https://github.com/LostRuins/koboldcpp), [LM Studio](https://lmstudio.ai/),
    or [ctransformers](https://github.com/marella/ctransformers). You can simply load
    your GGML models with these tools and interact with them in a ChatGPT-like way.
    Fortunately, many quantized models are directly available on the [Hugging Face
    Hub](https://huggingface.co/models?search=gg). You’ll quickly notice that most
    of them are quantized by [TheBloke](https://huggingface.co/TheBloke), a popular
    figure in the LLM community.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果命令行工具是你的强项，llama.cpp 和 GGUF 支持已集成到许多 GUI 中，如 oobabooga 的 [text-generation-web-ui](https://github.com/oobabooga/text-generation-webui)、[koboldcpp](https://github.com/LostRuins/koboldcpp)、[LM
    Studio](https://lmstudio.ai/) 或 [ctransformers](https://github.com/marella/ctransformers)。你可以使用这些工具加载你的
    GGML 模型，并以类似 ChatGPT 的方式与其互动。幸运的是，许多量化模型直接可以在 [Hugging Face Hub](https://huggingface.co/models?search=gg)
    上获得。你会很快注意到，大多数模型都是由 LLM 社区中的知名人物 [TheBloke](https://huggingface.co/TheBloke)
    进行量化的。
- en: In the next section, we will see how to quantize our own models and run them
    on a consumer GPU.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将看到如何量化我们自己的模型并在消费级 GPU 上运行它们。
- en: How to quantize LLMs with GGML?
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何用 GGML 对 LLM 进行量化？
- en: 'Let’s look at the files inside of [TheBloke/Llama-2–13B-chat-GGML](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/tree/main)
    repo. We can see **14 different GGML models**, corresponding to different types
    of quantization. They follow a particular naming convention: “q” + the number
    of bits used to store the weights (precision) + a particular variant. Here is
    a list of all the possible quant methods and their corresponding use cases, based
    on model cards made by TheBloke:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看看 [TheBloke/Llama-2–13B-chat-GGML](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/tree/main)
    仓库中的文件。我们可以看到**14 种不同的 GGML 模型**，对应不同的量化类型。它们遵循特定的命名规则：“q” + 存储权重的位数（精度）+ 特定变体。以下是所有可能的量化方法及其对应的使用场景，基于
    TheBloke 制作的模型卡：
- en: '`q2_k`: Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for
    the other tensors.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`q2_k`：对 attention.vw 和 feed_forward.w2 张量使用 Q4_K，对其他张量使用 Q2_K。'
- en: '`q3_k_l`: Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2
    tensors, else Q3_K'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`q3_k_l`：对 attention.wv、attention.wo 和 feed_forward.w2 张量使用 Q5_K，其余张量使用 Q3_K'
- en: '`q3_k_m`: Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2
    tensors, else Q3_K'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`q3_k_m`：对 attention.wv、attention.wo 和 feed_forward.w2 张量使用 Q4_K，其余张量使用 Q3_K'
- en: '`q3_k_s`: Uses Q3_K for all tensors'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`q3_k_s`：对所有张量使用 Q3_K'
- en: '`q4_0`: Original quant method, 4-bit.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`q4_0`：原始量化方法，4 位。'
- en: '`q4_1`: Higher accuracy than q4_0 but not as high as q5_0\. However has quicker
    inference than q5 models.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`q4_1`：比 q4_0 精度更高，但不如 q5_0 高。不过推理速度比 q5 模型快。'
- en: '`q4_k_m`: Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors,
    else Q4_K'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`q4_k_m`：对一半的 attention.wv 和 feed_forward.w2 张量使用 Q6_K，其余张量使用 Q4_K'
- en: '`q4_k_s`: Uses Q4_K for all tensors'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`q4_k_s`：对所有张量使用 Q4_K'
- en: '`q5_0`: Higher accuracy, higher resource usage and slower inference.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`q5_0`：更高的精度，更高的资源使用和较慢的推理。'
- en: '`q5_1`: Even higher accuracy, resource usage and slower inference.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`q5_1`：更高的精度，更高的资源使用和更慢的推理。'
- en: '`q5_k_m`: Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors,
    else Q5_K'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`q5_k_m`：对一半的 attention.wv 和 feed_forward.w2 张量使用 Q6_K，其余张量使用 Q5_K'
- en: '`q5_k_s`: Uses Q5_K for all tensors'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`q5_k_s`：对所有张量使用 Q5_K'
- en: '`q6_k`: Uses Q8_K for all tensors'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`q6_k`：对所有张量使用 Q8_K'
- en: '`q8_0`: Almost indistinguishable from float16\. High resource use and slow.
    Not recommended for most users.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`q8_0`：几乎无法与 float16 区分。高资源使用和较慢。不推荐给大多数用户。'
- en: As a rule of thumb, **I recommend using Q5_K_M** as it preserves most of the
    model’s performance. Alternatively, you can use Q4_K_M if you want to save some
    memory. In general, K_M versions are better than K_S versions. I cannot recommend
    Q2 or Q3 versions, as they drastically decrease model performance.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 作为经验法则，**我推荐使用 Q5_K_M**，因为它保留了大部分模型的性能。或者，如果你想节省一些内存，可以使用 Q4_K_M。一般来说，K_M 版本比
    K_S 版本更好。我不推荐 Q2 或 Q3 版本，因为它们会大幅降低模型性能。
- en: Now that we know more about the quantization types available, let’s see how
    to use them on a real model. You can execute the following code on a **free T4
    GPU** on [Google Colab](https://colab.research.google.com/drive/1pL8k7m04mgE5jo2NrjGi8atB0j_37aDD?usp=sharing).
    The first step consists of compiling llama.cpp and installing the required libraries
    in our Python environment.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们对可用的量化类型有了更多了解，接下来我们来看看如何在真实模型中使用它们。你可以在[Google Colab](https://colab.research.google.com/drive/1pL8k7m04mgE5jo2NrjGi8atB0j_37aDD?usp=sharing)上的**免费
    T4 GPU**上执行以下代码。第一步是编译 llama.cpp 并在我们的 Python 环境中安装所需的库。
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now we can download our model. We will use the model we fine-tuned [in the previous
    article](https://medium.com/towards-data-science/a-beginners-guide-to-llm-fine-tuning-4bae7d4da672),
    `[mlabonne/EvolCodeLlama-7b](https://huggingface.co/mlabonne/EvolCodeLlama-7b)`.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以下载我们的模型了。我们将使用我们在[上一篇文章](https://medium.com/towards-data-science/a-beginners-guide-to-llm-fine-tuning-4bae7d4da672)中微调过的模型，`[mlabonne/EvolCodeLlama-7b](https://huggingface.co/mlabonne/EvolCodeLlama-7b)`。
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This step can take a while. Once it’s done, we need to convert our weight to
    GGML FP16 format.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步可能需要一些时间。一旦完成，我们需要将权重转换为 GGML FP16 格式。
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Finally, we can quantize the model using one or several methods. In this case,
    we will use the Q4_K_M and Q5_K_M methods I recommended earlier. This is the only
    step that actually requires a GPU.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用一种或几种方法来量化模型。在这种情况下，我们将使用我之前推荐的 Q4_K_M 和 Q5_K_M 方法。这是唯一实际上需要 GPU 的步骤。
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Our two quantized models are now **ready for inference**. We can check the size
    of the bin files to see how much we compressed them. The FP16 model takes up 13.5
    GB, while the Q4_K_M model takes up 4.08 GB (3.3 times smaller) and the Q5_K_M
    model takes up 4.78 GB (2.8 times smaller).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的两个量化模型现在**已准备好进行推断**。我们可以检查 bin 文件的大小，以查看我们压缩了多少。FP16 模型占用 13.5 GB，而 Q4_K_M
    模型占用 4.08 GB（小了 3.3 倍），Q5_K_M 模型占用 4.78 GB（小了 2.8 倍）。
- en: Let’s use llama.cpp to efficiently run them. Since we’re using a GPU with 16
    GB of VRAM, we can offload every layer to the GPU. In this case, it represents
    35 layers (7b parameter model), so we’ll use the `-ngl 35` parameter. In the following
    code block, we'll also input a prompt and the quantization method we want to use.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 llama.cpp 来高效运行它们。由于我们使用的是具有 16 GB VRAM 的 GPU，我们可以将每一层都卸载到 GPU 上。在这种情况下，它代表
    35 层（7b 参数模型），因此我们将使用 `-ngl 35` 参数。在下面的代码块中，我们还将输入一个提示和我们想使用的量化方法。
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let’s ask the model “Write a Python function to print the nth Fibonacci numbers”
    using the Q5_K_M method. If we look at the logs, we can confirm that we successfully
    offloaded our layers thanks to the line “llm_load_tensors: offloaded 35/35 layers
    to GPU”. Here is the code the model generated:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们使用 Q5_K_M 方法向模型提出“编写一个 Python 函数来打印第 n 个斐波那契数”的请求。如果我们查看日志，我们可以确认我们成功将层卸载了，感谢“llm_load_tensors:
    offloaded 35/35 layers to GPU”这一行。这里是模型生成的代码：'
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This wasn’t a very complex prompt, but it successfully produced a working piece
    of code in no time. With llama.cpp, you can use your local LLM as an assistant
    in a terminal using the interactive mode (`-i` flag). Note that this also works
    on Macbooks with Apple's Metal Performance Shaders (MPS), which is an excellent
    option to run LLMs.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个非常复杂的提示，但它很快生成了一个有效的代码片段。使用 llama.cpp，你可以在终端中使用交互模式（`-i` 标志）将本地 LLM 作为助手。注意，这也适用于配备苹果
    Metal 性能着色器（MPS）的 Macbook，这是一种运行 LLM 的优秀选择。
- en: Finally, we can push our quantized model to a new repo on the Hugging Face Hub
    with the “-GGUF” suffix. First, let’s log in and modify the following code block
    to match your username.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将我们的量化模型推送到 Hugging Face Hub 上的新仓库，并使用“ -GGUF”后缀。首先，让我们登录并修改以下代码块以匹配你的用户名。
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Finally, we can push our quantized model to a new repo on the Hugging Face Hub
    with the "-GGUF" suffix. First, let's log in and modify the following code block
    to match your username. You can enter your Hugging Face token ([https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens))
    in Google Colab's "Secrets" tab. We use the `allow_patterns` parameter to only
    upload GGUF models and not the entirety of the directory.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将量化后的模型推送到 Hugging Face Hub 上的新仓库，并使用 "-GGUF" 后缀。首先，让我们登录并修改以下代码块以匹配你的用户名。你可以在
    Google Colab 的“Secrets”选项卡中输入你的 Hugging Face 令牌（[https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)）。我们使用
    `allow_patterns` 参数仅上传 GGUF 模型，而不是整个目录。
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We have successfully quantized, run, and pushed GGML models to the Hugging Face
    Hub! In the next section, we will explore how GGML actually quantize these models.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经成功量化、运行并推送了 GGML 模型到 Hugging Face Hub！在下一部分，我们将深入探讨 GGML 如何实际量化这些模型。
- en: Quantization with GGML
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 GGML 进行量化
- en: The way GGML quantizes weights is not as sophisticated as GPTQ’s. Basically,
    it groups blocks of values and rounds them to a lower precision. Some techniques,
    like Q4_K_M and Q5_K_M, implement a **higher precision for critical layers**.
    In this case, every weight is stored in 4-bit precision, with the exception of
    half of the attention.wv and feed_forward.w2 tensors. Experimentally, this mixed
    precision proves to be a good tradeoff between accuracy and resource usage.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: GGML 量化权重的方式不如 GPTQ 那么复杂。基本上，它将值块分组并将它们舍入到较低的精度。一些技术，如 Q4_K_M 和 Q5_K_M，实现了**对关键层的更高精度**。在这种情况下，每个权重都以
    4 位精度存储，除了注意力.wv 和 feed_forward.w2 张量的一半。实验结果表明，这种混合精度在准确性和资源使用之间提供了一个良好的折衷。
- en: 'If we look into the [ggml.c file](https://github.com/ggerganov/ggml/blob/master/src/ggml.c),
    we can see how the blocks are defined. For example, the `block_q4_0` structure
    is defined as:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看[ggml.c 文件](https://github.com/ggerganov/ggml/blob/master/src/ggml.c)，可以看到块是如何定义的。例如，`block_q4_0`
    结构定义如下：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In GGML, weights are processed in blocks, each consisting of 32 values. For
    each block, a scale factor (delta) is derived from the largest weight value. All
    weights in the block are then scaled, quantized, and packed efficiently for storage
    (nibbles). This approach significantly reduces the storage requirements while
    allowing for a relatively simple and deterministic conversion between the original
    and quantized weights.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GGML 中，权重按块处理，每块包含 32 个值。对于每块，从最大权重值派生出一个缩放因子（delta）。然后对块中的所有权重进行缩放、量化，并高效打包以进行存储（nibbles）。这种方法显著减少了存储需求，同时允许在原始权重和量化权重之间进行相对简单且确定的转换。
- en: Now that we know more about the quantization process, we can compare the results
    with NF4 and GPTQ.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对量化过程有了更多了解，我们可以将结果与 NF4 和 GPTQ 进行比较。
- en: NF4 vs. GGML vs. GPTQ
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NF4 vs. GGML vs. GPTQ
- en: 'Which technique is better for 4-bit quantization? To answer this question,
    we need to introduce the different backends that run these quantized LLMs. For
    GGML models, llama.cpp with Q4_K_M models is the way to go. For GPTQ models, we
    have two options: [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ) or [ExLlama](https://github.com/turboderp/exllama).
    Finally, NF4 models can directly be run in transformers with the `--load-in-4bit`
    flag.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 哪种技术更适合 4 位量化？为了回答这个问题，我们需要介绍运行这些量化 LLM 的不同后端。对于 GGML 模型，使用 Q4_K_M 模型的 llama.cpp
    是最佳选择。对于 GPTQ 模型，我们有两个选项：[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ) 或 [ExLlama](https://github.com/turboderp/exllama)。最后，NF4
    模型可以直接在 transformers 中通过 `--load-in-4bit` 标志运行。
- en: 'Oobabooga ran multiple experiments in an excellent [blog post](https://oobabooga.github.io/blog/posts/perplexities/)
    that compare different models in terms of perplexity (lower is better):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Oobabooga 在一篇出色的[博客文章](https://oobabooga.github.io/blog/posts/perplexities/)中进行了多个实验，比较了不同模型在困惑度方面的表现（困惑度越低越好）：
- en: '![](../Images/b5148511b4f0101b11c4ccb8546403ad.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5148511b4f0101b11c4ccb8546403ad.png)'
- en: 'Based on these results, we can say that GGML models have a slight advantage
    in terms of perplexity. The difference is not particularly significant, which
    is why it is better to focus on the generation speed in terms of tokens/second.
    The best technique depends on your GPU: if you have enough VRAM to fit the entire
    quantized model, **GPTQ with ExLlama** will be the fastest. If that’s not the
    case, you can offload some layers and use **GGML models with llama.cpp** to run
    your LLM.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些结果，我们可以说 GGML 模型在困惑度方面略有优势。差异并不特别显著，这也是为什么在生成速度（以 token/秒计）方面更值得关注。最佳技术取决于你的
    GPU：如果你有足够的 VRAM 可以容纳整个量化模型，**GPTQ 和 ExLlama** 将是最快的。如果不是这样，你可以卸载一些层并使用**GGML
    模型和 llama.cpp** 来运行你的 LLM。
- en: Conclusion
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we introduced the GGML library and the new GGUF format to efficiently
    store these quantized models. We used it to **quantize our own Llama model** in
    different formats (Q4_K_M and Q5_K_M). We then ran the GGML model and pushed our
    bin files to the Hugging Face Hub. Finally, we delved deeper into GGML’s code
    to understand how it actually quantizes the weights and compared it to NF4 and
    GPTQ.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们介绍了 GGML 库和新的 GGUF 格式，以高效地存储这些量化模型。我们使用它来**量化我们自己的 Llama 模型**为不同格式（Q4_K_M
    和 Q5_K_M）。随后，我们运行了 GGML 模型，并将我们的二进制文件推送到了 Hugging Face Hub。最后，我们深入研究了 GGML 的代码，以理解它是如何实际量化权重的，并将其与
    NF4 和 GPTQ 进行了比较。
- en: Quantization is a formidable vector to democratize LLMs by lowering the cost
    of running them. In the future, mixed precision and other techniques will keep
    improving the performance we can achieve with quantized weights. Until then, I
    hope you enjoyed reading this article and learned something new.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是使大型语言模型（LLMs）变得更加普及的强大手段，通过降低运行成本来实现。在未来，混合精度和其他技术将不断提升我们使用量化权重时可以达到的性能。在此之前，希望你喜欢阅读这篇文章，并从中学到了新知识。
- en: If you’re interested in more technical content around LLMs, [follow me on Medium](https://medium.com/@mlabonne).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对 LLMs 的更多技术内容感兴趣，[请在 Medium 上关注我](https://medium.com/@mlabonne)。
- en: Articles about quantization
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于量化的文章
- en: '[](/introduction-to-weight-quantization-2494701b9c0c?source=post_page-----3612dfbcc172--------------------------------)
    [## Part 1: Introduction to Weight Quantization'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/introduction-to-weight-quantization-2494701b9c0c?source=post_page-----3612dfbcc172--------------------------------)
    [## 第 1 部分：权重量化简介'
- en: Reducing the size of Large Language Models with 8-bit quantization
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 8 位量化减少大型语言模型的体积
- en: 'towardsdatascience.com](/introduction-to-weight-quantization-2494701b9c0c?source=post_page-----3612dfbcc172--------------------------------)
    [](/4-bit-quantization-with-gptq-36b0f4f02c34?source=post_page-----3612dfbcc172--------------------------------)
    [## Part 2: 4-bit Quantization with GPTQ'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/introduction-to-weight-quantization-2494701b9c0c?source=post_page-----3612dfbcc172--------------------------------)
    [](/4-bit-quantization-with-gptq-36b0f4f02c34?source=post_page-----3612dfbcc172--------------------------------)
    [## 第 2 部分：使用 GPTQ 的 4 位量化
- en: Quantize your own LLMs using AutoGPTQ
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 AutoGPTQ 量化你自己的 LLMs
- en: towardsdatascience.com](/4-bit-quantization-with-gptq-36b0f4f02c34?source=post_page-----3612dfbcc172--------------------------------)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/4-bit-quantization-with-gptq-36b0f4f02c34?source=post_page-----3612dfbcc172--------------------------------)
- en: '*Learn more about machine learning and support my work with one click — become
    a Medium member here:*'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*通过一次点击了解更多关于机器学习的信息并支持我的工作 — 立即成为 Medium 会员：*'
- en: '[](https://medium.com/@mlabonne/membership?source=post_page-----3612dfbcc172--------------------------------)
    [## Join Medium with my referral link — Maxime Labonne'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne/membership?source=post_page-----3612dfbcc172--------------------------------)
    [## 通过我的推荐链接加入 Medium — Maxime Labonne'
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story…
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 作为 Medium 会员，你的一部分会员费用将用于支持你阅读的作者，同时你可以完全访问每一个故事…
- en: medium.com](https://medium.com/@mlabonne/membership?source=post_page-----3612dfbcc172--------------------------------)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@mlabonne/membership?source=post_page-----3612dfbcc172--------------------------------)
