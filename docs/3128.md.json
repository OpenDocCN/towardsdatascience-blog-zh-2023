["```py\n # Collect the papers with subject \"math\"\ndef extract_entries_with_math(filename: str) -> List[str]:\n    \"\"\"\n    Function to extract the entries that contain the string 'math' in the 'id'.\n    \"\"\"\n\n    # Initialize an empty list to store the extracted entries.\n    entries_with_math = []\n\n    with open(filename, 'r') as f:\n        for line in f:\n            try:\n                # Load the JSON object from the line\n                data = json.loads(line)\n                # Check if the \"id\" key exists and if it contains \"math\"\n                if \"id\" in data and \"math\" in data[\"id\"]:\n                    entries_with_math.append(data)\n\n            except json.JSONDecodeError:\n                # Print an error message if this line isn't valid JSON\n                print(f\"Couldn't parse: {line}\")\n\n    return entries_with_math\n\n# Extract the mathematics papers\nentries = extract_entries_with_math(arxiv_full_dataset)\n\n# Save the dataset as a JSON object\narxiv_dataset_math = file_path + \"/data/arxiv_math_dataset.json\"\n\nwith open(arxiv_dataset_math, 'w') as fout:\n    json.dump(entries, fout)\n```", "```py\nparsed_titles = []\n\nfor i,a in df.iterrows():\n    \"\"\"\n    Function to replace LaTeX script with ISO code.\n    \"\"\"\n    try:\n        parsed_titles.append(LatexNodes2Text().latex_to_text(a['title']).replace('\\\\n', ' ').strip()) \n    except:\n        parsed_titles.append(a['title'].replace('\\\\n', ' ').strip())\n\n# Create a new column with the parsed titles\ndf['parsed_title'] = parsed_titles\n```", "```py\n# Extract the parsed titles as a list\ntrain_sentences = df.parsed_title.to_list()\n```", "```py\n# Add noise to the data\ntrain_dataset = datasets.DenoisingAutoEncoderDataset(train_sentences)\n```", "```py\nprint(train_dataset[2010])\n\ninitial text: \"On solutions of Bethe equations for the XXZ model\"\ncorrupted text: \"On solutions of for the XXZ\"\n```", "```py\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, \n                              shuffle=True, drop_last=True)\n```", "```py\nmodel_name = 'bert-base-uncased'\nword_embedding_model = models.Transformer(model_name)\n```", "```py\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n                               \"cls\")\t\t\t\t\t\t\t\t\t\t\t 'cls')\n```", "```py\nmodel = SentenceTransformer(modules=[word_embedding_model,\n                            pooling_model])\t\t\t\t\t\t\t\t\t\t\t\t\tpooling_model])\n```", "```py\ntrain_loss = losses.DenoisingAutoEncoderLoss(model,\n                                             decoder_name_or_path=model_name,\n                                             tie_encoder_decoder=True)\n```", "```py\nmodel.fit(\n    train_objectives=[(train_dataloader, train_loss)],\n    epochs=1,\n    weight_decay=0,\n    scheduler='constantlr',\n    optimizer_params={'lr': 3e-5},\n    show_progress_bar=True,\n    use_amp=True # set to False if GPU does not support FP16 cores\n)\n\npretrained_model_save_path = 'output/tsdae-bert-uncased-math'\nmodel.save(pretrained_model_save_path)\n```", "```py\nnli_dataset_path = 'data/AllNLI.tsv.gz'\n\nif not os.path.exists(nli_dataset_path):\n    util.http_get('<https://sbert.net/datasets/AllNLI.tsv.gz>', \n                  nli_dataset_path)\n```", "```py\ndef add_to_samples(sent1, sent2, label):\n    if sent1 not in train_data:\n        train_data[sent1] = {'contradiction': set(),\n                             'entailment': set(), \n                             'neutral': set()}\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t 'entailment': set\t\t\t\t\t\t\t\t\t\t\t\t 'neutral': set()}\n    train_data[sent1][label].add(sent2)\n\ntrain_data = {}\nwith gzip.open(nli_dataset_path, 'rt', encoding='utf8') as fIn:\n    reader = csv.DictReader(fIn, delimiter='\\\\t', \n                            quoting=csv.QUOTE_NONE)\n    for row in reader:\n        if row['split'] == 'train':\n            sent1 = row['sentence1'].strip()\n            sent2 = row['sentence2'].strip()\n\n            add_to_samples(sent1, sent2, row['label'])\n            add_to_samples(sent2, sent1, row['label'])  # Also add the opposite\n\ntrain_samples = []\nfor sent1, others in train_data.items():\n    if len(others['entailment']) > 0 and len(others['contradiction']) > 0:\n        train_samples.append(InputExample(texts=[sent1, \n                     random.choice(list(others['entailment'])), \n                     random.choice(list(others['contradiction']))]))\n        train_samples.append(InputExample(texts=[random.choice(list(others['entailment'])), \n                     sent1, \n                     random.choice(list(others['contradiction']))]))\t\t\t\t\t\t\t\t\t\t\t\t\t\t\trandom.choice(list(others['contradiction']))]))\n```", "```py\ntrain_dataloader = datasets.NoDuplicatesDataLoader(train_samples,\n                                                   batch_size=32)\n```", "```py\n# Set the model parameters\nmodel_name = 'output/tsdae-bert-uncased-math'\ntrain_batch_size = 32 \nmax_seq_length = 75\nnum_epochs = 1\n\n# Load the pre-trained model\nlocal_model = SentenceTransformer(model_name)\n# Choose the loss function\ntrain_loss = losses.MultipleNegativesRankingLoss(local_model)\n# Use 10% of the train data for warm-up\nwarmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1)\n\n# Train the model\nlocal_model.fit(train_objectives=[(train_dataloader, train_loss)],\n          #evaluator=dev_evaluator,\n          epochs=num_epochs,\n          #evaluation_steps=int(len(train_dataloader)*0.1),\n          warmup_steps=warmup_steps,\n          output_path=model_save_path,\n          use_amp=True  # Set True, if your GPU supports FP16 operations\n          )\n\n# Save the model\nfinetuned_model_save_path = 'output/finetuned-bert-uncased-math'\nlocal_model.save(finetuned_model_save_path)\n```", "```py\nimport datasets as dts\nfrom datasets import load_dataset\n\n# Import the STS benchmark dataset from HuggingFace\nsts = dts.load_dataset('glue', 'stsb', split='validation')\n```", "```py\nDataset({\n    features: ['sentence1', 'sentence2', 'label', 'idx'],\n    num_rows: 1379\n})\n```", "```py\n# Take a peek at one of the entries\nsts['idx'][100], sts['sentence1'][100], sts['sentence2'][100], sts['label'][100]\n\n>>>(100,\n 'A woman is riding on a horse.',\n 'A man is turning over tables in anger.',\n 0.0)\n```", "```py\n# Normalize the [0, 5] range to [0, 1]\nsts = sts.map(lambda x: {'label': x['label'] / 5.0})\n```", "```py\n# Create a list to store the parsed data\nsamples = []\n\nfor sample in sts:\n    # Reformat to use InputExample class\n    samples.append(InputExample(\n        texts=[sample['sentence1'], sample['sentence2']],\n        label=sample['label']\n    ))\n```", "```py\n# Instantiate the evaluation module\nevaluator = EmbeddingSimilarityEvaluator.from_input_examples(samples)\n```"]