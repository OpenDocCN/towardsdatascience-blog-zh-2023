- en: 'How ChatGPT Works: The Model Behind The Bot'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-chatgpt-works-the-models-behind-the-bot-1ce5fca96286?source=collection_archive---------0-----------------------#2023-01-30](https://towardsdatascience.com/how-chatgpt-works-the-models-behind-the-bot-1ce5fca96286?source=collection_archive---------0-----------------------#2023-01-30)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A brief introduction to the intuition and methodology behind the chat bot you
    can’t stop hearing about.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@molly.ruby?source=post_page-----1ce5fca96286--------------------------------)[![Molly
    Ruby](../Images/2a493bd01057722138857a90035347cd.png)](https://medium.com/@molly.ruby?source=post_page-----1ce5fca96286--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1ce5fca96286--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1ce5fca96286--------------------------------)
    [Molly Ruby](https://medium.com/@molly.ruby?source=post_page-----1ce5fca96286--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7a38f8e9fb80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-chatgpt-works-the-models-behind-the-bot-1ce5fca96286&user=Molly+Ruby&userId=7a38f8e9fb80&source=post_page-7a38f8e9fb80----1ce5fca96286---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1ce5fca96286--------------------------------)
    ·9 min read·Jan 30, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1ce5fca96286&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-chatgpt-works-the-models-behind-the-bot-1ce5fca96286&user=Molly+Ruby&userId=7a38f8e9fb80&source=-----1ce5fca96286---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1ce5fca96286&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-chatgpt-works-the-models-behind-the-bot-1ce5fca96286&source=-----1ce5fca96286---------------------bookmark_footer-----------)![](../Images/0e4bc81c17a6711eda750f7e522f100e.png)'
  prefs: []
  type: TYPE_NORMAL
- en: This gentle introduction to the machine learning models that power ChatGPT,
    will start at the introduction of Large Language Models, dive into the revolutionary
    self-attention mechanism that enabled GPT-3 to be trained, and then burrow into
    Reinforcement Learning From Human Feedback, the novel technique that made ChatGPT
    exceptional.
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ChatGPT is an extrapolation of a class of machine learning Natural Language
    Processing models known as Large Language Model (LLMs). LLMs digest huge quantities
    of text data and infer relationships between words within the text. These models
    have grown over the last few years as we’ve seen advancements in computational
    power. LLMs increase their capability as the size of their input datasets and
    parameter space increase.
  prefs: []
  type: TYPE_NORMAL
- en: The most basic training of language models involves predicting a word in a sequence
    of words. Most commonly, this is observed as either next-token-prediction and
    masked-language-modeling.
  prefs: []
  type: TYPE_NORMAL
