["```py\nkernel void matrix_multiply_naive(\n  device const MatrixParams *params,\n  constant float *A,\n  constant float *B,\n  device float *C,\n  // Indicates the thread's unique position within the entire grid of \n  // threads being executed. The uint2 type is a 2D coordinate, with \n  // fields x and y representing its indices on each axis.\n  // This parameter is not directly provided from the calling code, \n  // but provided by the Metal framework\n  uint2 gid [[thread_position_in_grid]]\n) {\n  if (gid.x >= params->a_rows || gid.y >= params->b_cols) {\n    return; // This thread is out of matrix dimensionality range, do nothing\n  }\n\n  float sum = 0.0;\n  int k;\n\n  // Loop unrolling; improves performance by a notable margin\n  for (k = 0; k <= params->a_cols - 4; k += 4) {\n    sum += A[gid.x * params->a_cols + k] \n       * B[k * params->b_cols + gid.y];\n    sum += A[gid.x * params->a_cols + k + 1] \n       * B[(k + 1) * params->b_cols + gid.y];\n    sum += A[gid.x * params->a_cols + k + 2] \n       * B[(k + 2) * params->b_cols + gid.y];\n    sum += A[gid.x * params->a_cols + k + 3] \n       * B[(k + 3) * params->b_cols + gid.y];\n  }\n\n  // Handle any remaining elements\n  for (; k < params->a_cols; ++k) {\n    sum += A[gid.x * params->a_cols + k] * B[k * params->b_cols + gid.y];\n  }\n\n  C[gid.x * params->b_cols + gid.y] = sum;\n}\n```", "```py\n// Loop unrolling; improves performance by a notable margin\nfor (k = 0; k <= params->a_cols - 4; k += 4) {\n  sum += A[gid.x * params->a_cols + k]     \n    * B[gid.y * params->b_cols + k]; // Note this is gid.y * cols plus k\n  sum += A[gid.x * params->a_cols + k + 1] \n    * B[gid.y * params->b_cols + k + 1];\n  sum += A[gid.x * params->a_cols + k + 2] \n    * B[gid.y * params->b_cols + k + 2];\n  sum += A[gid.x * params->a_cols + k + 3] \n    * B[gid.y * params->b_cols + k + 3];\n}\n\n// Handle any remaining elements\nfor (; k < params->a_cols; ++k) {\n  sum += A[gid.x * params->a_cols + k] * B[gid.y * params->b_cols + k];\n}\n```", "```py\nid<MTLDevice> device = MTLCreateSystemDefaultDevice();\n\n// Compile and initialize a new library located at the provided source path.\nMTLCompileOptions *compileOptions = [MTLCompileOptions new];\ncompileOptions.languageVersion = MTLLanguageVersion3_0;\n\n// Wrap input source path string\nNSString *ss = [NSString stringWithUTF8String:source_path];\n\n// Initialize new library containing compiled shader functions\nid<MTLLibrary> lib = [device newLibraryWithSource:ss\n  options:compileOptions\n  error:&error];\n\n// Create a representation of the naive multiplication public shader function in \n// the Metal library created above\nid<MTLFunction> naiveFunction =\n    [lib newFunctionWithName:@\"matrix_multiply_naive\"];\n\n// Create the new compute pipeline state\nid<MTLComputePipelineState> pipelineStateNaive = [device newComputePipelineStateWithFunction:naiveFunction\n  error:&error];\n```", "```py\n[computeEncoder setComputePipelineState:pipelineStateNaive];\n\nMTLSize threadsPerGrid = MTLSizeMake(params->a_cols, params->a_rows, 1);\n\n// Calculate a threadgroup size.\n// https://developer.apple.com/documentation/metal/calculating_threadgroup_and_grid_sizes?language=objc\nNSUInteger w = pipelineStateNaive.threadExecutionWidth;\nNSUInteger h = pipelineStateNaive.maxTotalThreadsPerThreadgroup / w;\nMTLSize threadsPerThreadgroup = MTLSizeMake(w, h, 1);\n\n// Encode kernel function inputs\n[computeEncoder setBytes:params length:16 atIndex:0];\n[computeEncoder setBuffer:bufferA offset:0 atIndex:1];\n[computeEncoder setBuffer:bufferB offset:0 atIndex:2];\n[computeEncoder setBuffer:bufferC offset:0 atIndex:3];\n\n// Encode the compute command.\n[computeEncoder dispatchThreads:threadsPerGrid \n  threadsPerThreadgroup:threadsPerThreadgroup];\n\n// End the compute pass.\n[computeEncoder endEncoding];\n\n// Execute the command.\n[commandBuffer commit];\n```", "```py\n// Define Matrix \"descriptions\", accounting for matrix dimensionality and byte size\nMPSMatrixDescriptor *descriptorA = [MPSMatrixDescriptor matrixDescriptorWithDimensions:a_rows\n  columns:a_cols\n  rowBytes:a_cols * sizeof(float)\n  dataType:MPSDataTypeFloat32];\n\nMPSMatrixDescriptor *descriptorB = [MPSMatrixDescriptor matrixDescriptorWithDimensions:b_rows\n  columns:b_cols\n  rowBytes:b_cols * sizeof(float)\n  dataType:MPSDataTypeFloat32];\n\n// Output matrix\nMPSMatrixDescriptor *descriptorC = [MPSMatrixDescriptor matrixDescriptorWithDimensions:a_rows\n  columns:b_cols\n  rowBytes:b_cols * sizeof(float)\n  dataType:MPSDataTypeFloat32];\n\n// Initialize matrix representations using above descriptions and matrix buffers\nMPSMatrix *matrixA = [[MPSMatrix alloc] initWithBuffer:bufferA descriptor:descriptorA];\nMPSMatrix *matrixB = [[MPSMatrix alloc] initWithBuffer:bufferB descriptor:descriptorB];\nMPSMatrix *matrixC = [[MPSMatrix alloc] initWithBuffer:bufferC descriptor:descriptorC];\n\n// Creates the multiplication instance\nMPSMatrixMultiplication *matrixMultiplication = [[MPSMatrixMultiplication alloc] initWithDevice:device\n  resultRows:a_rows\n  resultColumns:b_cols\n  interiorColumns:a_cols];\n\n// Encodes the multiplication command into the command buffer for the GPU\nid<MTLCommandBuffer> commandBuffer = [commandQueue commandBuffer];\n[matrixMultiplication encodeToCommandBuffer:commandBuffer\n  leftMatrix:matrixA\n  rightMatrix:matrixB\n  resultMatrix:matrixC];\n```", "```py\n/*\n#cgo LDFLAGS: -framework Foundation -framework CoreGraphics -framework Metal -framework MetalPerformanceShaders -L/opt/homebrew/opt/openblas/lib -lopenblas\n#include <stdlib.h>\n#include \"metal.h\"\n*/\nimport \"C\"\n```", "```py\n//go:embed mm.metal\nvar source string\n\n// Compile the shader source code and initialize pipelines. The metalSource\n// param contains the contents of an embedded Metal Shading Language file.\nfunc Compile (metalSource string) {\n // Wrap string in a C string\n src := C.CString(metalSource)\n\n // Free the above string after command queue is initialized\n defer C.free(unsafe.Pointer(src))\n\n // Compile the source, initialize pipelines and command queue\n C.initializePipelineAndCommandQueue(src)\n}\n```", "```py\n// Calls initializeMTLBuffers from Obj-C bindings\nC.initializeMTLBuffers(\n a_data,                  // Input opaque pointer for A\n b_data,                  // Input opaque pointer for B\n C.int(4),                // Converts 4 into C integer type\n C.int(a.Size()),         \n C.int(b.Size()),         \n C.int(a.Rows * b.Cols))\n\nparams := MatrixParams{\n a_rows: int32(a.Rows),\n a_cols: int32(a.Cols),\n b_rows: int32(b.Rows),\n b_cols: int32(b.Cols),\n}\n\n// Return an unsafe pointer to this MatrixParams struct, cast to \n// the native C representation defined in the shared header file\nreturn (*C.MatrixParams)(unsafe.Pointer(&params));\n```", "```py\nfunc (a Matrix[T]) TransposeMultParallel(b *Matrix[T]) *Matrix[T] {\n if a.Cols != b.Rows {\n  panic(\"matrices are the wrong size for multiplication\")\n }\n\n c_data := make([]T, a.Rows*b.Cols)\n t := b.Transpose()\n\n var wg sync.WaitGroup\n\n for i := 0; i < a.Rows; i++ {\n  wg.Add(1) // Add a count to the WaitGroup for the new goroutine\n  go func(i int) { // Kick off goroutine\n   defer wg.Done() // Decrease the count when the goroutine completes\n   ptr := i * b.Cols\n   for j := 0; j < b.Cols; j++ {\n    var sum T = 0.0\n    for k := 0; k < a.Cols; k++ {\n     sum += a.At(i, k) * t.At(j, k)\n    }\n    c_data[ptr+j] = sum\n   }\n  }(i)\n }\n\n wg.Wait() // Wait for all goroutines to complete\n return InitMatrixWithData(a.Rows, b.Cols, c_data)\n}\n```", "```py\n// Convert primitive arrays into gonum dense matrix types\ngonum_a := mat.NewDense(a_rows, a_cols, a64_data)\ngonum_b := mat.NewDense(b_rows, b_cols, b64_data)\ngonum_c := mat.NewDense(a_rows, b_cols, nil)\ngonum_d := mat.NewDense(a_rows, b_cols, nil)\n\n// Configure Gonum to use Gonum-default Go implementation\nblas64.Use(gonum.Implementation{})\n\n// Run a multiplication using Gonum BLAS impl\nstart = time.Now()\ngonum_c.Mul(gonum_a, gonum_b)\nbdata.TimeGonumNative(start)\n\n// Configure Gonum to use Netlib which forwards operations to a \n// native C-code BLAS implementation (OpenBLAS in our case)\nblas64.Use(netlib.Implementation{})\n\n// Run a multiplication using OpenBLAS impl through Gonum API\nstart = time.Now()\ngonum_d.Mul(gonum_a, gonum_b)\nbdata.TimeGonumOpenBLAS(start)\n```", "```py\n- Naive multiplication, in Go\n- Transposed naive multiplication, in Go\n- Goroutine-parallelized transposed naive multiplication, in Go\n- Gonum pure Go-based BLAS multiplication\n- Gonum-wrapped OpenBLAS multiplication, written in C\n- Hand-implemented naive multiplication, in MSL, on GPU\n- Hand-implemented transposed naive multiplication, in MSL, on GPU\n- Metal Performance Shaders framework, called from Objective-C, on GPU\n```", "```py\n2023-12-01 11:12:51.644 go-mm[75818:22427382] Using default device Apple M2\nelements naive transpose transpose_parallel metal_naive metal_transpose mps gonum openblas\n160000 196.00 201.00 42.00 8.00 9.67 0.33 4.67 6.00\n250000 381.33 387.67 80.67 11.00 11.67 0.00 8.33 21.00\n360000 801.00 789.33 159.33 19.00 16.33 0.00 14.33 4.67\n490000 1228.00 1075.00 411.00 23.67 24.33 1.00 26.67 16.33\n...\n```"]