- en: 'From Decision Trees to Transformers: Comparing Sentiment Analysis Models for
    Macedonian Restaurant Reviews'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/from-decision-trees-to-transformers-comparing-sentiment-analysis-models-for-macedonian-restaurant-4c2d931ec021?source=collection_archive---------4-----------------------#2023-03-03](https://towardsdatascience.com/from-decision-trees-to-transformers-comparing-sentiment-analysis-models-for-macedonian-restaurant-4c2d931ec021?source=collection_archive---------4-----------------------#2023-03-03)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ML Techniques for Analysing Macedonian Restaurant Reviews
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@danilo.najkov?source=post_page-----4c2d931ec021--------------------------------)[![Danilo
    Najkov](../Images/e0e8976f1f9f78ae58ba7efcc90a4f00.png)](https://medium.com/@danilo.najkov?source=post_page-----4c2d931ec021--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4c2d931ec021--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4c2d931ec021--------------------------------)
    [Danilo Najkov](https://medium.com/@danilo.najkov?source=post_page-----4c2d931ec021--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F19802d0e7d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-decision-trees-to-transformers-comparing-sentiment-analysis-models-for-macedonian-restaurant-4c2d931ec021&user=Danilo+Najkov&userId=19802d0e7d&source=post_page-19802d0e7d----4c2d931ec021---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4c2d931ec021--------------------------------)
    ·10 min read·Mar 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4c2d931ec021&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-decision-trees-to-transformers-comparing-sentiment-analysis-models-for-macedonian-restaurant-4c2d931ec021&user=Danilo+Najkov&userId=19802d0e7d&source=-----4c2d931ec021---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4c2d931ec021&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-decision-trees-to-transformers-comparing-sentiment-analysis-models-for-macedonian-restaurant-4c2d931ec021&source=-----4c2d931ec021---------------------bookmark_footer-----------)![](../Images/af0e8f1d709eb059137751a9703ae73d.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Graphic by author
  prefs: []
  type: TYPE_NORMAL
- en: While machine learning models for natural language processing have traditionally
    focused on popular languages such as English and Spanish, less commonly spoken
    languages have seen much less development. However, with the recent rise in e-commerce
    due to the COVID-19 pandemic, even less commonly spoken languages like Macedonian
    are generating large amounts of data through online reviews. This has opened an
    opportunity to develop and train machine learning models for sentiment analysis
    on Macedonian restaurant reviews, which can help businesses better understand
    customer sentiment and improve their services. In this study, we tackle the challenges
    that arise from this problem and explore and compare various sentiment analysis
    models for analyzing sentiment in Macedonian restaurant reviews, ranging from
    the classical random forests to modern deep learning techniques and transformers.
  prefs: []
  type: TYPE_NORMAL
- en: '**Contents**'
  prefs: []
  type: TYPE_NORMAL
- en: Challenges and preprocessing the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating vector embeddings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '- LASER embeddings'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- Multilingual universal text encoder'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- OpenAI Ada v2'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Machine learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '- Random forest'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- XGBoost'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- Support vector machines'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- Deep learning'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- Transformers'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Results and Discussion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Future work
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocessing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Language is a uniquely human communication tool, and computers cannot interpret
    it without appropriate processing techniques. To allow machines to analyse and
    understand language, we need to represent the complex semantic and lexical information
    in a way that can be processed computationally. One popular method for achieving
    this is through the use of vector representations. In recent years, additional
    to language specific representation models, multilingual models emerged. These
    models can capture the semantic context of text on a large number of languages.
  prefs: []
  type: TYPE_NORMAL
- en: However, for languages with Cyrillic script, an additional challenge arises
    as users on the internet often express themselves using Latin script, resulting
    in mixed data consisting of both Latin and Cyrillic text. To address this challenge,
    I used a dataset from a local restaurant of approximately 500 reviews, containing
    both Latin and Cyrillic script. The dataset also includes a small subset of english
    reviews, which will help to assess the performance on mixed data. Additionally,
    online texts can contain symbols such as emojis that need to be removed. Therefore
    preprocessing is a crucial step before any text embedding can be performed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The dataset contains positive and negative classes with nearly equal distribution.
    For removing emojis, I used the python library `emoji`, which can easily remove
    emojis and other symbols.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: For the problem of Cyrillic and Latin text, i converted all texts into one or
    the other, so the machine learning models can be tested on both to compare the
    performance. I used the “cyrtranslit” library for this task. It supports most
    of the Cyrillic alphabets like Macedonian, Bulgarian, Ukrainian and others.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/df4ff53d87f8ed5066a9fc78eb489af6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. Output from conversion
  prefs: []
  type: TYPE_NORMAL
- en: For embedding models that I used, it’s generally not necessary to remove punctuation,
    stop-words and do other text cleaning. These models are designed to process natural
    language text, including punctuation marks, and are often able to capture the
    meaning of sentences more accurately when they are left intact. With that the
    preprocesing of the text is finished.
  prefs: []
  type: TYPE_NORMAL
- en: Vector embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Currently, there are no large-scale Macedonian representation models available.
    However, we can use multilingual models trained on Macedonian text. There are
    several such models available, but for this task, I have found that LASER and
    Multilingual Universal Sentence Encoder would be the most suitable options.
  prefs: []
  type: TYPE_NORMAL
- en: LASER
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LASER (Language-Agnostic SEntence Representations) is a language-agnostic approach
    for generating high-quality multilingual sentence embeddings. The LASER model
    is based on a two-stage process, where the first stage is preprocessing the text,
    including tokenization, lowercasing, and applying sentencepiece. This part is
    language specific. The second stage involves mapping the preprocessed input text
    to a fixed-length embedding using a multi-layer bidirectional LSTM.
  prefs: []
  type: TYPE_NORMAL
- en: LASER has been shown to outperform other popular sentence embedding methods,
    such as fastText and InferSent, on a range of benchmark datasets. Additionally,
    the LASER model is open-source and freely available, making it easily accessible
    to everyone.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating the embeddings with LASER is a straightforward process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Multilingual Universal Sentence Encoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multilingual Universal Sentence Encoder (MUSE) is a pre-trained model for generating
    sentence embeddings, developed by Facebook. MUSE is designed to encode sentences
    in multiple languages into a common space.
  prefs: []
  type: TYPE_NORMAL
- en: The model is based on a deep neural network that uses an encoder-decoder architecture
    to learn a mapping between a sentence and its corresponding embedding vector in
    a high-dimensional space. MUSE is trained on a large-scale multilingual corpus,
    which includes texts from Wikipedia, news articles, and web pages.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: OpenAI Ada v2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Towards the end of 2022, OpenAI announced their brand new state-of-the-art embedding
    model [text-embedding-ada-002](https://openai.com/blog/new-and-improved-embedding-model/).
    As this model is built on GPT-3, it has multilingual processing capabilities.
    To compare the results between the Cyrillic and Latin reviews, I ran the model
    on both datasets
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Machine learning models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section explores the various machine learning models utilized to predict
    sentiment in Macedonian restaurant reviews. From traditional machine learning
    models to deep learning techniques, we’ll look into the strengths and weaknesses
    of each model and compare their performance on the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Before running any models, the data should be split for training and testing
    for every embedding type. This can be easily done with with the `sklearn` library.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Random Forests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/f3c3e1b81c5fa1eb8e8f42e7fb391761.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. Simplified representation of the random forest classification. 100
    decision trees are constructed and the result is calculated as a majority vote
    between the results of each decision tree individually. (figure by author)
  prefs: []
  type: TYPE_NORMAL
- en: Random Forests are a widely-used machine learning algorithm that uses an ensemble
    of decision trees to classify data points. The algorithm works by training each
    decision tree on a subset of the full dataset and a random subset of the features.
    During inference, each decision tree generates a prediction of the sentiment,
    and the final output is obtained by taking a majority vote of all the trees. This
    approach helps to prevent overfitting and can lead to more robust and accurate
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: XGBoost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/069088713baa3757823045c72cc6597c.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. Sequential process of boosting based algorithms. Every next decision
    tree is trained on the residuals (errors) of the previous one. (figure by author)
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost (eXtreme Gradient Boosting) is a powerful ensemble method, mainly used
    in tabular data. Like Random Forest, XGBoost also uses decision trees to classify
    data points, but with a different approach. Instead of training all trees at once,
    XGBoost trains each tree in a sequential manner, learning from the errors made
    by the previous tree. This process is called boosting, which means combining weak
    models to form a stronger one. Although XGBoost primarly produces great results
    with tabular data, it would be interesting to test it with vector embeddings as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Support Vector Machines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/a7582c92959dbf18690e24b78a04b7ac.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. Simplified representation of support vector classification. In the
    case of this sentiment analysis with 1024 input features, the hyperplane would
    be 1023 dimensional. (figure by author)
  prefs: []
  type: TYPE_NORMAL
- en: Support Vector Machines (SVM) is a popular and powerful machine learning algorithm
    for classification and regression tasks. It works by finding the optimal hyperplane
    that separates the data into different classes, while maximizing the margin between
    the classes. SVM is particularly useful for high-dimensional data and can handle
    non-linear boundaries using kernel functions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Deep Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/2357db5eea9f7f39d928d7476ad3857d.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5\. Simplified representation of the neural network used in this problem.
    (figure by author)
  prefs: []
  type: TYPE_NORMAL
- en: Deep Learning is an advanced machine learning method that utilizes artificial
    neural networks consisting of multiple layers and neurons. Deep learning networks
    demonstrate great performance with text and image data. Implementing these networks
    is a straightforward process using the library Keras.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Here, a neural network with two hidden layers and a rectified linear unit (ReLU)
    activation function was used. The output layer contains a single neuron with a
    sigmoid activation function, enabling the network to make binary predictions for
    positive or negative sentiment. The binary cross-entropy loss function is paired
    with the sigmoid activation to train the model. Additionally, Dropout was used
    to help prevent overfitting and improve the generalization of the model. I tested
    with various different hyper-parameters and found that this configuration works
    best for this problem.
  prefs: []
  type: TYPE_NORMAL
- en: With the following function we can visualize the training of the models.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/8848ddc6401ccfdb6b39638a2638dfc6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6\. Example training output
  prefs: []
  type: TYPE_NORMAL
- en: Transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/4f481ee30e9e5081d647526efa18634f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7\. Pre-training and fine-tuning process of the BERT large language
    model. (source: [original BERT paper](https://arxiv.org/pdf/1810.04805v2.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning transformers is a popular technique in natural language processing
    that involves adjusting pre-trained transformer models to suit specific tasks.
    Transformers, such as BERT, GPT-2, and RoBERTa, are pre-trained on large amounts
    of text data and are capable of learning complex patterns and relationships in
    language. However, in order to perform well on specific tasks, such as sentiment
    analysis or text classification, these models need to be fine-tuned on task-specific
    data.
  prefs: []
  type: TYPE_NORMAL
- en: For these types of models, the vector representations we created earlier are
    not needed, as they directly process the tokens (extracted directly from the text).
    For this task of sentiment analysis in Macedonian I worked with`bert-base-multilingual-uncased`
    which is the multilingual version of the BERT model.
  prefs: []
  type: TYPE_NORMAL
- en: '[HuggingFace](https://huggingface.co/) has made fine-tuning the transformers
    a very simple task. Firstly the data needs to be loaded into a transformers dataset.
    Then the text is tokenized and finally the model is trained.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: With that we have successfully fine-tuned BERT for sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Results and discussion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/9db813559915c7d0a87f0d2eadf8bfcf.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8\. Results obtained with all the models
  prefs: []
  type: TYPE_NORMAL
- en: The results of the sentiment analysis on Macedonian restaurant reviews are promising,
    with several models achieving high accuracy and F1 scores. The experiments show
    that deep learning models and transformers, outperform traditional machine learning
    models like Random Forests and Support Vector Machines, although not by much.
    Transformers and deep neural networks using the new OpenAI embedding managed to
    break the 0.9 accuracy barrier.
  prefs: []
  type: TYPE_NORMAL
- en: The OpenAI embedding model `textembedding-ada-002` managed to considerably boost
    the results obtained even from the classical ML models, especially on the Support
    Vector Machines. The best result in this study was achieved with this embedding
    on the Cyrillic text on a deep learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Generally the Latin texts performed worse compared to the Cyrillic texts. Although
    I initially hypothesized that the performance of these models would be better,
    given the prevalence of similar words in Latin among other Slavic languages and
    the fact that the embedding models were trained on such data, the findings did
    not support this hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: Future work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In future work, it would be valuable to collect more data to further train and
    test the models, especially with a larger diversity of review topics and sources.
    Additionally, trying to incorporate more features such as metadata (e.g., reviewer’s
    age, gender, location) or temporal information (e.g., time of review) into the
    models might improve their accuracy. Finally, it would be interesting to extend
    the analysis to other less commonly spoken languages and compare the performance
    of the models with these trained on the Macedonian reviews.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In conclusion, this post has demonstrated the effectiveness of various machine
    learning models and embedding techniques for sentiment analysis of Macedonian
    restaurant reviews. Several classic machine learning models are explored and compared,
    such as Random Forests and SVM, as well as modern deep learning techniques including
    neural networks and transformers. The results have shown that fine-tuned transformer
    models and deep learning models with the newest OpenAI embedding outperform other
    methods, with a validation accuracy of up to 90%.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading!
  prefs: []
  type: TYPE_NORMAL
