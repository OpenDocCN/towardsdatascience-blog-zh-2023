- en: Thoughts on Stateful ML, Online Learning, and Intelligent ML Model Retraining
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于状态保持机器学习、在线学习和智能机器学习模型再训练的思考
- en: 原文：[https://towardsdatascience.com/thoughts-on-stateful-ml-online-learning-and-intelligent-ml-model-retraining-4e583728e8a1?source=collection_archive---------14-----------------------#2023-04-05](https://towardsdatascience.com/thoughts-on-stateful-ml-online-learning-and-intelligent-ml-model-retraining-4e583728e8a1?source=collection_archive---------14-----------------------#2023-04-05)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/thoughts-on-stateful-ml-online-learning-and-intelligent-ml-model-retraining-4e583728e8a1?source=collection_archive---------14-----------------------#2023-04-05](https://towardsdatascience.com/thoughts-on-stateful-ml-online-learning-and-intelligent-ml-model-retraining-4e583728e8a1?source=collection_archive---------14-----------------------#2023-04-05)
- en: Designing scalable architecture for online and offline continuous learning systems
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计可扩展的在线和离线持续学习系统架构
- en: '[](https://medium.com/@kylegallatin?source=post_page-----4e583728e8a1--------------------------------)[![Kyle
    Gallatin](../Images/ee2796ba575412e9caf6034a65d741e5.png)](https://medium.com/@kylegallatin?source=post_page-----4e583728e8a1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4e583728e8a1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4e583728e8a1--------------------------------)
    [Kyle Gallatin](https://medium.com/@kylegallatin?source=post_page-----4e583728e8a1--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@kylegallatin?source=post_page-----4e583728e8a1--------------------------------)[![Kyle
    Gallatin](../Images/ee2796ba575412e9caf6034a65d741e5.png)](https://medium.com/@kylegallatin?source=post_page-----4e583728e8a1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4e583728e8a1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4e583728e8a1--------------------------------)
    [Kyle Gallatin](https://medium.com/@kylegallatin?source=post_page-----4e583728e8a1--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F51ff4b76ebf4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthoughts-on-stateful-ml-online-learning-and-intelligent-ml-model-retraining-4e583728e8a1&user=Kyle+Gallatin&userId=51ff4b76ebf4&source=post_page-51ff4b76ebf4----4e583728e8a1---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4e583728e8a1--------------------------------)
    ·6 min read·Apr 5, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4e583728e8a1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthoughts-on-stateful-ml-online-learning-and-intelligent-ml-model-retraining-4e583728e8a1&user=Kyle+Gallatin&userId=51ff4b76ebf4&source=-----4e583728e8a1---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F51ff4b76ebf4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthoughts-on-stateful-ml-online-learning-and-intelligent-ml-model-retraining-4e583728e8a1&user=Kyle+Gallatin&userId=51ff4b76ebf4&source=post_page-51ff4b76ebf4----4e583728e8a1---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4e583728e8a1--------------------------------)
    ·6 min read·2023年4月5日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4e583728e8a1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthoughts-on-stateful-ml-online-learning-and-intelligent-ml-model-retraining-4e583728e8a1&user=Kyle+Gallatin&userId=51ff4b76ebf4&source=-----4e583728e8a1---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e583728e8a1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthoughts-on-stateful-ml-online-learning-and-intelligent-ml-model-retraining-4e583728e8a1&source=-----4e583728e8a1---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e583728e8a1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthoughts-on-stateful-ml-online-learning-and-intelligent-ml-model-retraining-4e583728e8a1&source=-----4e583728e8a1---------------------bookmark_footer-----------)'
- en: 'Ever since I read Chip Huyen’s [*Real-time machine learning: challenges and
    solutions*](https://huyenchip.com/2022/01/02/real-time-machine-learning-challenges-and-solutions.html),
    I’ve been thinking about the future of machine learning in production. Short feedback
    loops, real-time features, and stateful ML model deployments capable of learning
    online merit a very different sort of systems architecture that many of the stateless
    ML model deployments I work with today.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 自从我读了Chip Huyen的[*实时机器学习：挑战与解决方案*](https://huyenchip.com/2022/01/02/real-time-machine-learning-challenges-and-solutions.html)后，我一直在思考生产环境中机器学习的未来。短反馈循环、实时特性以及能够在线学习的状态保持机器学习模型部署需要一种与我今天所用的无状态机器学习模型部署截然不同的系统架构。
- en: '![](../Images/cb765555b528f3298223d7073c4cf1b4.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb765555b528f3298223d7073c4cf1b4.png)'
- en: Me thinking ‘bout stateful ML in Cozumel, MX — Image by Author
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我在墨西哥科苏梅尔思考有状态的机器学习——图片来自作者
- en: For the past few months, I’ve been conducting informal user research, white-boarding,
    and doing ad-hoc development to get to the core of what a real stateful ML system
    might look like. For the most part, this post outlines the story of my thought
    process and I continue to dive into this space and uncover interesting and unique
    architectural challenges.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 过去几个月，我一直在进行非正式用户研究、白板讨论和临时开发，以探讨真正有状态的机器学习系统可能是什么样的。大部分内容都概述了我的思维过程，并且我继续深入这个领域，发现有趣且独特的架构挑战。
- en: Definitions
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义
- en: '**Stateful (or continuous) learning** involves *updating* model parameters
    instead of retraining from scratch in order to:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**有状态（或连续）学习** 涉及到*更新*模型参数，而不是从头开始再训练，以便于：'
- en: Decrease training time
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缩短训练时间
- en: Save cost
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节省成本
- en: Update models more frequently
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更频繁地更新模型
- en: '![](../Images/32dadd54d68b8ab84e7e751df71e43a8.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/32dadd54d68b8ab84e7e751df71e43a8.png)'
- en: Stateless versus stateful retraining — from [Chip Huyen](https://huyenchip.com/2022/01/02/real-time-machine-learning-challenges-and-solutions.html)
    with permission
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 无状态与有状态的再训练——来自[Chip Huyen](https://huyenchip.com/2022/01/02/real-time-machine-learning-challenges-and-solutions.html)的许可
- en: '**Online learning** involves learning from ground truth examples in real-time
    in order to:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**在线学习** 涉及实时从真实样本中学习，以便于：'
- en: Increase model performance and reactivity
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升模型性能和反应性
- en: Mitigate performance issues that would result from drift/staleness
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓解因漂移/过时导致的性能问题
- en: Right now, most learning in the industry is done *offline* in batch.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，大多数行业中的学习都是*离线*进行的。
- en: '**Intelligent model retraining** typically refers to automatically retraining
    models using some performance metric as opposed to on a set schedule in order
    to:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**智能模型再训练** 通常指的是使用某些性能指标自动再训练模型，而不是按照固定的时间表进行，以便于：'
- en: Reduce cost without sacrificing performance
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低成本而不牺牲性能
- en: Right now, most models across industries are retrained on a schedule using DAGs.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，大多数行业中的模型都是使用DAG按照时间表再训练的。
- en: '![](../Images/5da4771588b8116ac934298acf5fe48e.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5da4771588b8116ac934298acf5fe48e.png)'
- en: Intelligent retraining architecture from [A Guide To Automated Model Retraining](https://arize.com/resource/a-guide-to-optimizing-automated-model-retraining/?utm_campaign=Newsletter+-+DRIFT&utm_medium=email&_hsmi=252645972&_hsenc=p2ANqtz-_--qVbD_z1d-GRI1rSslPJDb1J6jevmfkOcQTOkQ3IwCJME58-WrfDPvra_rpfXHrDv_BIXyJmcTrtSiIRQvVn4DKr7Q&utm_content=252645972&utm_source=hs_email)
    — by Arize AI with permission
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[自动模型再训练指南](https://arize.com/resource/a-guide-to-optimizing-automated-model-retraining/?utm_campaign=Newsletter+-+DRIFT&utm_medium=email&_hsmi=252645972&_hsenc=p2ANqtz-_--qVbD_z1d-GRI1rSslPJDb1J6jevmfkOcQTOkQ3IwCJME58-WrfDPvra_rpfXHrDv_BIXyJmcTrtSiIRQvVn4DKr7Q&utm_content=252645972&utm_source=hs_email)的智能再训练架构——由Arize
    AI授权
- en: Designing an MVP for online learning
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为在线学习设计MVP
- en: In a [previous article](/building-a-lil-stateful-ml-application-for-online-learning-66624d62afae?sk=cd01b0115ea189cf7abdc295c35f4d43),
    I’d tried to use foundational engineering principles in order to create a dead
    simple online learning architecture. My first thought — to model stateful, online
    learning architecture after stateful web applications. by treating the “model”
    as the DB (where predictions are reads and incremental training sessions are writes),
    I thought I might simplify the design process.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在[上一篇文章](/building-a-lil-stateful-ml-application-for-online-learning-66624d62afae?sk=cd01b0115ea189cf7abdc295c35f4d43)中，我尝试运用基础工程原理来创建一个极其简单的在线学习架构。我的第一个想法是——将有状态的在线学习架构建模为有状态的网页应用程序。通过将“模型”视作数据库（其中预测为读取，增量训练会话为写入），我认为可以简化设计过程。
- en: '![](../Images/8b4f9d6bf80e4313dad2071275aa7c3b.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b4f9d6bf80e4313dad2071275aa7c3b.png)'
- en: Image by Author
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自作者
- en: To a degree, I actually did! By using the online learning library [River](https://riverml.xyz/0.15.0/),
    I [built a small, stateful online learning application](https://github.com/kylegallatin/stateful-ml-app)
    that allowed me to update a model *and* serve predictions in real-time.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在某种程度上，我确实做到了！通过使用在线学习库[River](https://riverml.xyz/0.15.0/)，我[构建了一个小型有状态的在线学习应用](https://github.com/kylegallatin/stateful-ml-app)，这让我能够*实时*更新模型并提供预测。
- en: '![](../Images/8aa2c88281674d66b569ccfba1569df2.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8aa2c88281674d66b569ccfba1569df2.png)'
- en: Flask app that shares a model in memory across multiple workers — Image by Author
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Flask应用程序在多个工作进程之间共享内存中的模型——图片来自作者
- en: 'This approach was cool and fun to code — but has some fundamental issues at
    scale:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法在编码时很酷很有趣——但在规模上存在一些根本性的问题：
- en: '**Doesn’t scale horizontally:** We can easily share a model in the memory of
    a single application — but this approach doesn’t scale approach multiple pods
    in orchestration engines like Kubernetes'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**无法横向扩展：** 我们可以轻松地在单个应用程序的内存中共享一个模型——但这种方法在像Kubernetes这样的编排引擎中无法扩展到多个pod。'
- en: '**Mixes application responsibilities:** I don’t know (and don’t want to be
    the one to find out) about the caveats of trying to support a deployment that
    mixes training and serving'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**混合应用职责：** 我不知道（也不想成为第一个知道）关于尝试支持一个混合训练和服务的部署的注意事项。'
- en: '**Preemptively introduces complexity:** Online learning is the most proactive
    type of machine learning possible, but we haven’t even validated we need it in
    the first place. There has to be a better place to start…'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预先引入复杂性：** 在线学习是最积极主动的机器学习类型，但我们甚至还没有验证我们是否需要它。必须有一个更好的起点……'
- en: Designing something that scales
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计一种可扩展的架构
- en: Let’s start from an existing standard — distributed model training. It’s fairly
    common practice to use something like a parameter server as a centralized store
    while multiple workers calculate a partial/distributed gradient…or something…and
    reconcile the parameters after the fact.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 从现有标准开始——分布式模型训练。使用类似参数服务器的东西作为集中存储，同时多个工作者计算部分/分布式梯度……或其他什么……并在之后调整参数是相当常见的做法。
- en: So — I thought I’d try to this about this in the context of real-time model
    serving deployments, and came up with the dumbest architecture possible.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 所以——我想尝试在实时模型服务部署的背景下思考这个问题，结果想出了最傻的架构。
- en: '![](../Images/d812ed7402ef09badbf527beb4e9bba6.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d812ed7402ef09badbf527beb4e9bba6.png)'
- en: An architecture that makes no sense — Image by Author
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一种没有意义的架构——作者提供的图片
- en: Distributed model training is mean to speed up the training process. However,
    in this instance there’s no real need to be both training *and* serving in a distributed
    fashion — keeping the training decentralized introduces complexity and serves
    no purpose in an online training system. It makes way more sense to separate training
    entirely.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式模型训练旨在加快训练过程。然而，在这种情况下，没有必要以分布式方式进行训练*和*服务——保持训练去中心化会引入复杂性，并且在在线训练系统中没有实际用途。完全分开训练更有意义。
- en: '![](../Images/5970ff10b7470e23e06e1428d352d154.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5970ff10b7470e23e06e1428d352d154.png)'
- en: An architecture that makes slightly more sense — Image by Author
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一种稍微有意义的架构——作者提供的图片
- en: 'Great! Sort of. At this point I had to take a step back, as I was making quite
    a few assumptions and probably getting a bit ahead of myself:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 很好！有点。此时我不得不退后一步，因为我做了很多假设，可能有些过于前瞻性：
- en: We may not be able to get ground truth in near-real time
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可能无法在接近实时的情况下获得真实情况。
- en: Continuous *online* training may not provide a net benefit over continuous training
    *offline* and is a premature optimization
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 持续*在线*训练可能不会比持续*离线*训练提供净收益，而且是一种过早的优化。
- en: Offline/online learning may also not be binary — and there are scenarios where
    we’d want/need both!
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 离线/在线学习也可能不是二元的——有些场景中我们可能需要/想要两者都具备！
- en: Some sensible architectures for intelligent retraining, continuous learning,
    and online learning
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一些合理的智能再训练、持续学习和在线学习架构
- en: Let’s start from a simpler offline scenario — I want to use some sort of ML
    observability system to automatically retrain a model based on performance metric
    degradation. In a scenario where I’m doing continuous training (and model weights
    don’t take long to update) this is feasible to do without significant business
    impact.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个更简单的离线场景开始——我想使用某种机器学习可观察性系统，根据性能指标的退化自动再训练模型。在进行持续训练（且模型权重更新不会花费太长时间）的场景下，这在不显著影响业务的情况下是可行的。
- en: '![](../Images/69c2c535406ca8e5372514fe2decfad2.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69c2c535406ca8e5372514fe2decfad2.png)'
- en: Intelligent retraining and continuous o*nline* learning — Image by Author
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 智能再训练和持续*在线*学习——作者提供的图片
- en: Amazing — the first reasonable thing I’ve drawn all day! This system likely
    has a lower cost overhead than a *stateless* training architecture, and is reactive
    to changes in the model/data. We save lots of $ by only retraining as needed,
    and overall it’s pretty simple!
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 真棒——这是我今天画的第一个合理的东西！这个系统可能比*无状态*训练架构有更低的成本开销，并且对模型/数据的变化反应迅速。通过仅在需要时进行再训练，我们节省了大量的$，总体上也非常简单！
- en: This architecture has a big problem though….it’s not nearly as fun! What might
    a system look like that has all the reactivity of online learning with the cost
    savings of continuous learning and the resilience of online learning?! Hopefully,
    something like this…
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种架构有一个大问题……它远没有那么有趣！什么样的系统能兼具在线学习的所有反应性、连续学习的成本节约和在线学习的弹性呢？希望，像这样……
- en: '![](../Images/559693798798de7e56dc5406c2beef80.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/559693798798de7e56dc5406c2beef80.png)'
- en: Continuous, online learning — Image by Author
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 连续的在线学习 — 作者提供的图像
- en: Though there are details I still haven’t flushed out, there are a lot of benefits
    to this architecture. It allows for mixed online and offline learning (just as
    feature stores allow access to both streaming features and features computed offline),
    is highly robust to changes in data distribution or even individual user preferences
    for personalized systems (recsys), and still allows us to integrate ML observability
    (O11y) tooling to constantly measure data distributions and performance.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然还有很多细节我还没完全弄清楚，但这种架构有很多好处。它允许混合在线和离线学习（正如特征存储允许访问流式特征和离线计算的特征一样），对数据分布的变化或个别用户偏好（个性化系统（recsys））非常稳健，并且仍然允许我们集成
    ML 可观测性（O11y）工具来不断测量数据分布和性能。
- en: 'However, though this *might* be the most sensible thing diagram I’ve created
    yet, it still leaves a lot of open questions:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管这可能是我迄今为止创建的最合理的图表，它仍然留有很多未解的问题：
- en: How/when do we evaluate the model and with what data in an online system? If
    the data distribution is subject large shifts, we’ll need to to create new data-driven
    methodologies and best practices for designing a held-out evaluation set that
    includes both old data and the most recent data.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在在线系统中，我们如何/何时评估模型，以及使用什么数据？如果数据分布发生大幅变化，我们需要创建新的数据驱动方法和最佳实践，设计一个包含旧数据和最新数据的保留评估集。
- en: How do we reconcile an ML model that splits training processes into batch/offline
    and online? We’ll need to experiment with new techniques and system architectures
    to allow for complex, computational operations that involve large ML models in
    a system like this.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何调和将训练过程分为批处理/离线和在线的 ML 模型？我们需要尝试新的技术和系统架构，以允许在像这样的系统中涉及大型 ML 模型的复杂计算操作。
- en: How do we pull/push the model weights? On a cadence? During some event or subject
    to some change in metric? Each of this architectural decisions could have a significant
    impact on the performance of our system — and without online A/B testing or other
    research, it’ll be difficult to validate these choices.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何拉取/推送模型权重？按照一定的节奏？在某些事件中或根据某些指标的变化？这些架构决策中的每一个都可能对我们系统的性能产生重大影响——而且没有在线
    A/B 测试或其他研究，将很难验证这些选择。
- en: Next steps
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下一步
- en: Of course, one of my next steps is simply to start building some of this stuff
    and see what happens. However, I would appreciate insight, ideas and engagement
    from any and all folks in the industry to think about what some paths forward
    might be!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我的下一步是开始构建这些东西，看看会发生什么。然而，我希望能从业内的任何人那里获得见解、想法和参与，以考虑一些前进的路径！
- en: Please reach out on [twitter](https://twitter.com/kylegallatin), [LinkedIn](https://www.linkedin.com/in/kylegallatin/),
    or sign-up for the next sessions of my course on [Designing Production ML Systems
    this May](https://nycdatascience.com/courses/designing-and-implementing-production-machine-learning-systems-mlops/)!
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 请通过 [twitter](https://twitter.com/kylegallatin)、[LinkedIn](https://www.linkedin.com/in/kylegallatin/)
    联系我，或报名参加我课程的下一期：[Designing Production ML Systems this May](https://nycdatascience.com/courses/designing-and-implementing-production-machine-learning-systems-mlops/)！
