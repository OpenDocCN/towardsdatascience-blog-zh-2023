["```py\nclass CustomLoss(nn.Module):\n    def __init__(self):\n        super(CustomLoss, self).__init__()\n\n    def forward(self, output, target):\n        target = torch.LongTensor(target)\n        criterion = nn.CrossEntropyLoss()\n        loss = criterion(output, target)\n        mask = target == 9\n        high_cost = (loss * mask.float()).mean()\n        return loss + high_cost\n```", "```py\nimport torch.nn as nn\nimport torch\nfrom torchvision import datasets, transforms\nfrom torch import nn, optim\nimport torch.nn.functional as F\nimport torchvision\nimport os\n\nclass CustomLoss(nn.Module):\n    def __init__(self):\n        super(CustomLoss, self).__init__()\n\n    def forward(self, output, target):\n        target = torch.LongTensor(target)\n        criterion = nn.CrossEntropyLoss()\n        loss = criterion(output, target)\n        mask = target == 9\n        high_cost = (loss * mask.float()).mean()\n        return loss + high_cost\n\n# Load the MNIST dataset\ntrain_loader = torch.utils.data.DataLoader(\n  torchvision.datasets.MNIST('/files/', train=True, download=True,\n                             transform=torchvision.transforms.Compose([\n                               torchvision.transforms.ToTensor(),\n                               torchvision.transforms.Normalize(\n                                 (0.1307,), (0.3081,))\n                             ])),\n  batch_size=32, shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(\n  torchvision.datasets.MNIST('/files/', train=False, download=True,\n                             transform=torchvision.transforms.Compose([\n                               torchvision.transforms.ToTensor(),\n                               torchvision.transforms.Normalize(\n                                 (0.1307,), (0.3081,))\n                             ])),\n  batch_size=32, shuffle=True)\n\n# Define the model, loss function and optimizer\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x)\n\nnetwork = Net()\noptimizer = optim.SGD(network.parameters(), lr=0.01,\n                      momentum=0.5)\ncriterion = CustomLoss()\n\n# Training loop\nn_epochs = 10\n\ntrain_losses = []\ntrain_counter = []\ntest_losses = []\ntest_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n\nif os.path.exists('results'):\n  os.system('rm -r results')\n\nos.mkdir('results')\n\ndef train(epoch):\n  network.train()\n  for batch_idx, (data, target) in enumerate(train_loader):\n    optimizer.zero_grad()\n    output = network(data)\n    loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n    if batch_idx % 1000 == 0:\n      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n        epoch, batch_idx * len(data), len(train_loader.dataset),\n        100\\. * batch_idx / len(train_loader), loss.item()))\n      train_losses.append(loss.item())\n      train_counter.append(\n        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n      torch.save(network.state_dict(), 'results/model.pth')\n      torch.save(optimizer.state_dict(), 'results/optimizer.pth')\n\ndef test():\n  network.eval()\n  test_loss = 0\n  correct = 0\n  with torch.no_grad():\n    for data, target in test_loader:\n      output = network(data)\n      test_loss += criterion(output, target).item()\n      pred = output.data.max(1, keepdim=True)[1]\n      correct += pred.eq(target.data.view_as(pred)).sum()\n  test_loss /= len(test_loader.dataset)\n  test_losses.append(test_loss)\n  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n    test_loss, correct, len(test_loader.dataset),\n    100\\. * correct / len(test_loader.dataset)))\n\ntest()\nfor epoch in range(1, n_epochs + 1):\n  train(epoch)\n  test()\n```", "```py\nimport matplotlib.pyplot as plt\n\nfig = plt.figure()\nplt.plot(train_counter, train_losses, color='blue')\nplt.scatter(test_counter, test_losses, color='red')\nplt.legend(['Train Loss', 'Test Loss'], loc='upper right')\nplt.xlabel('number of training examples seen')\nplt.ylabel('negative log likelihood loss')\nplt.show()\n```", "```py\nexamples = enumerate(test_loader)\nbatch_idx, (example_data, example_targets) = next(examples)\nwith torch.no_grad():\n  output = network(example_data)\nfig = plt.figure()\nfor i in range(6):\n  plt.subplot(2,3,i+1)\n  plt.tight_layout()\n  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n  plt.title(\"Prediction: {}\".format(\n    output.data.max(1, keepdim=True)[1][i].item()))\n  plt.xticks([])\n  plt.yticks([])\n\nplt.show()\n```"]