- en: 'Reinforcement Learning Basics: Understanding Stochastic Theory Underlying a
    Markov Decision Process'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/reinforcement-learning-basics-1-understanding-stochastic-theory-underlying-an-mdp-e770be0665cf?source=collection_archive---------5-----------------------#2023-02-16](https://towardsdatascience.com/reinforcement-learning-basics-1-understanding-stochastic-theory-underlying-an-mdp-e770be0665cf?source=collection_archive---------5-----------------------#2023-02-16)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Part 1: On the Markov Decision Model which forms the theoretical foundation
    of reinforcement learning problems'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@shaileydash?source=post_page-----e770be0665cf--------------------------------)[![Shailey
    Dash](../Images/b5a44aafce4c310f60398e714d515b76.png)](https://medium.com/@shaileydash?source=post_page-----e770be0665cf--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e770be0665cf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e770be0665cf--------------------------------)
    [Shailey Dash](https://medium.com/@shaileydash?source=post_page-----e770be0665cf--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc4fe44a8e55d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-basics-1-understanding-stochastic-theory-underlying-an-mdp-e770be0665cf&user=Shailey+Dash&userId=c4fe44a8e55d&source=post_page-c4fe44a8e55d----e770be0665cf---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e770be0665cf--------------------------------)
    ¬∑28 min read¬∑Feb 16, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe770be0665cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-basics-1-understanding-stochastic-theory-underlying-an-mdp-e770be0665cf&user=Shailey+Dash&userId=c4fe44a8e55d&source=-----e770be0665cf---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe770be0665cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-basics-1-understanding-stochastic-theory-underlying-an-mdp-e770be0665cf&source=-----e770be0665cf---------------------bookmark_footer-----------)![](../Images/7ffb68ec9c76b1f9339fd860f362d1a1.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example of a simple MDP with three states (green circles) and two actions (orange
    circles), with two rewards (Image source: [Wikipedia](https://en.wikipedia.org/wiki/Markov_decision_process#/media/File:Markov_Decision_Process.svg))'
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning (RL) is a type of machine learning that enables an agent
    to learn to achieve a goal in an uncertain environment by taking actions. An important
    aspect of reinforcement learning is that it evaluates the actions taken rather
    than instructs by giving correct actions. Each action has a reward associated
    with it which signals to the agent the success of the action in progressing towards
    the goal. The agent navigates the environment repetitively learning how to optimize
    to reach its goal.
  prefs: []
  type: TYPE_NORMAL
- en: Till very recently most of reinforcement learning‚Äôs successes where related
    to game playing agents such as Alpha Go. However increasingly reinforcement learning
    is being applied in real world applications such as self-driving cars, and robotic
    automation. The recently launched ChatGPT also has a RL component in its architecture
    for finetuning of answers. Given this, it becomes important to understand RL which
    has been acknowledged to have a steep learning curve both from the theoretical
    perspective and also the practical implementation.
  prefs: []
  type: TYPE_NORMAL
- en: This article focuses on the Markov Decision model (MDP) which is often used
    to formalize the reinforcement agent‚Äôs problem. However, a neglected aspect of
    this conceptualization of the RL problem is that the MDP framework is an important
    application of Stochastic theory.
  prefs: []
  type: TYPE_NORMAL
- en: There is a non-trivial amount of stochastic and probability theory that underlies
    the basic Markov Decision Process equations. Without understanding these equations
    and how they are derived, it is difficult to make progress with RL. This article
    aims to clarify the stochastic and probability concepts that underlie MDPs with
    a focus on understanding equations presented in Sutton(2017) Ch 3.
  prefs: []
  type: TYPE_NORMAL
- en: The basic MDP framework as presented in Sutton and Barto (2017), Ch3, actually
    appears to be quite simple and intuitive ‚Äî a sleight of hand achieved by a master!
    However, there is a huge backlog of statistical theory behind this framework.
    We don‚Äôt need to know the depths of Stochastic Processes, because, believe me,
    it‚Äôs a lot! But it is important to be aware of some of the subtleties underlying
    some of the concepts used in MDPs. Hence it is important to understand what are
    stochastic processes as the theory of these lies at the core of MDPs.
  prefs: []
  type: TYPE_NORMAL
- en: This article is the first in a sequence of articles that will aim to make the
    fundamentals of RL clearer both at the theoretical and practical application level.
  prefs: []
  type: TYPE_NORMAL
- en: '**Who is the intended audience?**'
  prefs: []
  type: TYPE_NORMAL
- en: Well, of course, it looks like it is for beginners, but I would say that this
    post is actually more useful to people who have some knowledge of RL and then
    try to understand the equations presented in Sutton (2017) Ch 3 more intuitively.
    This article is the result of my learning journey into RL. I started quite happily
    with Sutton and Barto (2017) and thought I had understood most of the concepts
    in Ch 3 for the MDP framework. However, once I tried to actually derive some of
    the equations presented in ch3, I found myself unable to figure them out. So,
    it became clear to me that possibly more was going on than I seemed to know about.
    There is no real resource available that I could find that relates MDPs to their
    underlying statistical underpinnings. I had to use a variety of material ranging
    from Wikipedia to various university level Stochastic theory courses. Given how
    scattered the underlying material is, I decided to document it in this post.
  prefs: []
  type: TYPE_NORMAL
- en: '**Approach ‚Äî Decoding Sutton and Barto (2017)**'
  prefs: []
  type: TYPE_NORMAL
- en: The other thing is the format of this post. Sutton (2017) represents the bible
    of RL theory. The current edition is an updated version available online. But,
    much of theoretical material was actually published 20 years ago in the first
    edition of the book. Also, the approach taken in Ch3 of the book is the approach
    found across the internet. Given this, my approach in this post, is a bit like
    a ‚Äòkey‚Äô where I present material from the book and then explain it using the lens
    of stochastic theory.
  prefs: []
  type: TYPE_NORMAL
- en: A slight digression in defense of ‚Äòkeys‚Äô. Legions of students, especially in
    school, use them extensively; whereas teachers actually frown on them for spoon
    feeding. However, when you are learning online and typically on your own, then
    a detailed guide becomes important, as you can‚Äôt just put up your hand and ask
    a question.
  prefs: []
  type: TYPE_NORMAL
- en: Enough on the preamble! Let‚Äôs move to the actual topic‚Ä¶.
  prefs: []
  type: TYPE_NORMAL
- en: '**Role of MDP**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Understanding how the reinforcement learning problem works in the theoretical
    MDP environment is critical to getting the fundamentals of RL. Many important
    RL algorithms either are based on Markov based equations or models or represent
    some key departures from the Markov model. MDPs are a mathematically idealized
    form of the reinforcement learning process and provide a theoretical framework
    for an idealized type of reinforcement learning problem. Hence, no getting away
    from it: understanding MDPs is foundational to understanding the RL problem and
    how it is being addressed.'
  prefs: []
  type: TYPE_NORMAL
- en: Be warned, Stochastic Theory is a particularly tough branch of probability theory
    and at times the notation used can be very intimidating. It shall be my attempt
    to bring the underlying stochastic concepts and ideas underlying MDPs more explicitly.
    Hopefully, the end result brings to the front the complexities that underlie the
    Markov model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article I split up the content in 3 sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Section 1: Key Components of an MDP**'
  prefs: []
  type: TYPE_NORMAL
- en: '- RL problem'
  prefs: []
  type: TYPE_NORMAL
- en: '- State, agent, environment, reward, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Section 2: Probability and Stochastic Concepts Used in MDPs**'
  prefs: []
  type: TYPE_NORMAL
- en: '- Random variables, state or sample space, stochastic processes, realization
    or trajectory'
  prefs: []
  type: TYPE_NORMAL
- en: '- Probability distributions, joint distributions, marginal distributions, conditional
    distributions'
  prefs: []
  type: TYPE_NORMAL
- en: '- Deriving the Markov property'
  prefs: []
  type: TYPE_NORMAL
- en: '- Probability transition Matrix'
  prefs: []
  type: TYPE_NORMAL
- en: '**Section 3: Understanding Sutton Ch3 equations Using Probability and Stochastic
    concepts**'
  prefs: []
  type: TYPE_NORMAL
- en: '- The MDP Model'
  prefs: []
  type: TYPE_NORMAL
- en: '- Formal presentation of MDP model'
  prefs: []
  type: TYPE_NORMAL
- en: '- Equations 3.2, 3.3, 3.4, 3.5, 3.6 from Ch3, Sutton (2017)'
  prefs: []
  type: TYPE_NORMAL
- en: 'This probably seems like just the introductory part of an MDP. You may well
    ask: ‚Äòwhat about value functions, policy, Q value function, Bellman equations?‚Äô.
    Well, that comes in the next article, as once I have gone through with the above
    contents, you will agree that this by itself it is a lot.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Section 1: Key Components of an MDP**'
  prefs: []
  type: TYPE_NORMAL
- en: First, to set context about what we are about to explain, I shall provide a
    brief summary of the basic MDP model and definitions used by [Sutton (2017), Ch3](http://www.incompleteideas.net/book/RLbook2020.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: '***Why MDP?***'
  prefs: []
  type: TYPE_NORMAL
- en: The major reason for the popularity of the MDP framework is due to the Markov
    property where future states depend only on the current state and not the history
    of states. Or, to put it another way, the current state encapsulates all the relevant
    information required for making decisions about the future states. By the way,
    this seemingly innocuous assumption actually has a wealth of probability theory
    behind it and hence also implications.
  prefs: []
  type: TYPE_NORMAL
- en: Because of this property, an MDP represents a tractable way to formalize sequential
    decision making. It provides a probabilistic framework for modeling decision making
    in situations where outcomes are partly random and partly under the control of
    a decision maker. In this scenario, actions don‚Äôt just have immediate payoffs
    in terms of rewards, but also affect subsequent time periods or states, and through
    them future rewards. Of course, the way reinforcement learning is done in practice
    is different from the MDP.
  prefs: []
  type: TYPE_NORMAL
- en: '***Some Definitions to start off with***'
  prefs: []
  type: TYPE_NORMAL
- en: '**Agent:** This is the learner or decision maker who makes sequential decisions
    to achieve an end goal in an environment.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Environment:** This is defined as everything that is outside the agent‚Äôs
    locus of control. Hence the environment can be the external environment for a
    robot, it can be roads and pedestrians for a self -driving car, etc. In a video
    game, the environment is the game. The following points are important for the
    environment:'
  prefs: []
  type: TYPE_NORMAL
- en: Observations are what the agent receives as input ‚Äî for example a self-driving
    car can only receive inputs from the environment in its vicinity and will not
    be aware of a road block significantly ahead
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The agent only has full knowledge of the environment if it knows the model of
    the environment, example the structure of rules of a video game which govern the
    environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The environment is changed by agent‚Äôs actions (sudden breaking causes a small
    jam), but can also change on its own (a road diversion)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***What is the RL problem?***'
  prefs: []
  type: TYPE_NORMAL
- en: The typical RL problem involves a learning agent interacting over time with
    its environment to achieve a goal. To do this, a learning agent must be able to
    sense the state of its environment to some extent and must be able to take actions
    that affect the state. The fundamental problem is presented in this diagram in
    below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6586539f97d6dff700ae8f57dbde2125.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Agent environment interaction in a Markov decision process (Image source: Created
    by author, inspired by Sutton (2017) Ch3)'
  prefs: []
  type: TYPE_NORMAL
- en: '***The measurement of time***'
  prefs: []
  type: TYPE_NORMAL
- en: 'Time is typically measured in discrete steps: t=0,1,2,..T. Where, T represents
    the final or terminal state. Hence it is a *finite discrete time* problem. There
    are also continuous time problems and also infinite horizon problems, but these
    are a lot tougher to handle mathematically. As a result the focus is on the finite
    discrete time model.'
  prefs: []
  type: TYPE_NORMAL
- en: '***State***'
  prefs: []
  type: TYPE_NORMAL
- en: This represents a snapshot of the environment and contains all the information
    pertaining to the environment. This includes information on rewards, how the environment
    will change with respect to actions, etc. In the case of a self-driving car, a
    state may represent a region around the car. Similarly, in the case of a robot
    moving forward, it would represent a snapshot of the environment once an action
    such as stepping forward has been taken.
  prefs: []
  type: TYPE_NORMAL
- en: 'For Markov purposes, the agent and environment interact in a series of discrete
    time steps: t=0,1,2‚Ä¶T. Where, T represents the final or terminal state. At each
    time step, t, the agent receives some representation of the environment encapsulated
    as S‚Çú ‚àà **S**,where **S** is the set of all states. In the context of MDPs, State
    is a discrete random variable.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Rewards***'
  prefs: []
  type: TYPE_NORMAL
- en: The agent receives a signal or reward R‚Çú‚Çä‚ÇÅ from the environment in the next
    time period which is associated with how well the action is doing in terms of
    achieving the overall goal. The reward is typically defined as a scalar where
    R‚Çú ‚àà ***R*** ‚äÇ‚Ñù is just the set of real numbers (example of complicated notation
    for something simple! But that‚Äôs the way things happen in statsworldüòä).
  prefs: []
  type: TYPE_NORMAL
- en: Rewards can be both deterministic or a random variable. Sutton (2017) takes
    R‚Çú to be a random variable, though most of the examples take a deterministic reward.
  prefs: []
  type: TYPE_NORMAL
- en: Now, R‚Çú depends on the current state of the world, the action just taken, and
    the next state of the world, i.e., R‚Çú = R(S‚Çú, A‚Çú, S‚Çú‚Çä‚ÇÅ). However, its frequently
    simplified to depend on the current state and action pair, R‚Çú = R(S‚Çú, A‚Çú).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the basic components of the MDP in place, i.e., State, Reward,
    Agent, Environment, we need to better understand the nature of the MDP variables
    State, Reward, Agent, Environment in the context of probability and stochastic
    theory.
  prefs: []
  type: TYPE_NORMAL
- en: So, let‚Äôs now jump to Probability and Stochastic theory‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: '**Section 2: Probability and Stochastic Basics**'
  prefs: []
  type: TYPE_NORMAL
- en: To define stochastic processes, we need to first understand what are random
    variables, probability distributions, joint distributions, marginal distribution,
    conditional distributions and the chain rule of probability.
  prefs: []
  type: TYPE_NORMAL
- en: Let us now spend some time understanding the above concepts. Once we understand
    these, we are going to come back to MDPs and understand how it works using these
    concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Since stats and probability is vast, I would like to keep the focus on the applicability
    of these concepts to MDPs. So, I present the statistical concepts related to an
    MDP concept in *italics* and then explain its relevance to the MDP. Hopefully,
    this will make the MDP concept clearer.
  prefs: []
  type: TYPE_NORMAL
- en: '***Random variable***'
  prefs: []
  type: TYPE_NORMAL
- en: '*In probability, a random variable is a way of capturing the fact that a variable
    can take multiple values or occurrences. It represents a mapping or function that
    captures the fact that the variable can be mapped to many values. For example
    a toss of a coin has outcomes: {H,T} and we can map this to a number lying between
    0 and 1 signifying probability. See this* [*Wikpedia*](https://en.wikipedia.org/wiki/Random_variable)
    *page for a more formal definition. As an aside, I have found the Wikipedia pages
    for probability and statistics to be an excellent, if slightly advanced, resource.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*When the range of the random variable is countable, it is termed discrete
    and its distribution is a discrete probability mass function ( a concept that
    I will define later as it will be used a lot). If the variable is continuous,
    then its distribution can be defined by a probability distribution function.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*The definition of a random variable is traditionally defined for real valued
    cases, denoted by* **R***. The definition can also be extended to any measurable
    set E which contains random Boolean values or categorical values, vectors, matrices
    or functions.*'
  prefs: []
  type: TYPE_NORMAL
- en: This becomes important to note because in MDPs we have real valued random variables
    as well as categorical types. Also stochastic processes (to be defined a little
    later) are also a random function of time.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of the MDP defined above, we have State and Reward as random
    variables. Both variables are discrete and have a finite range.
  prefs: []
  type: TYPE_NORMAL
- en: '***State space or sample space:***'
  prefs: []
  type: TYPE_NORMAL
- en: '*When we talk of a random number, then the no. of finite number of states that
    a random variable can take is termed as the sample state or state space in the
    case of an MDP. The simplest example of a sample space is that for a throw of
    coin. It can take one of two values: heads or tails.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Sutton ch3, this is denoted by the set ***S*** which contains different
    states, where each state can be representative of a different situation. For example,
    we can have a stochastic process for going to office. The list of states that
    ***S*** could potentially contain:'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. s‚Äô: meets a jam on the way to commuting to office'
  prefs: []
  type: TYPE_NORMAL
- en: '2\. s: Meets clear roads to office'
  prefs: []
  type: TYPE_NORMAL
- en: '3\. s‚Äù: Meets a road block due to an accident'
  prefs: []
  type: TYPE_NORMAL
- en: These states are what constitute the sample space or range of a random variable,
    i.e., the values it can take.
  prefs: []
  type: TYPE_NORMAL
- en: '***Stochastic Process*:**'
  prefs: []
  type: TYPE_NORMAL
- en: '*A stochastic process is a collection or ensemble of random variables indexed
    by a variable t, usually representing time. So, to be clear, a stochastic process
    comprises the same random variable at multiple points of time. The stochastic
    process is typically indexed by the time variable (it can also be vector space,
    but let‚Äôs not confuse things). Thus, each index or point of time is associated
    by a specific random variable. Read more about stochastic processes on this* [*Wikipedia*](https://en.wikipedia.org/wiki/Stochastic_process)
    *page.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*This means that at every point of time, the stochastic random variable can
    realize one of the values in* ***S****.**A stochastic process can also be written
    as: S(t,i), t*‚ààT. *Where the State is a function of both, time as indexed by t,
    and i representing the specific state from the state space* ***S.***'
  prefs: []
  type: TYPE_NORMAL
- en: The state space is defined using elements that reflect the different values
    that the stochastic process can take. So, in the case of MDPs, a state, S‚Çú , is
    a stochastic random variable over time, indexed by t, where for each t, S‚Çú can
    take values in the finite set ***S =***{s,s‚Äô,s‚Äô‚Äô‚Äô}.
  prefs: []
  type: TYPE_NORMAL
- en: 'The point of confusion regarding stochastic processes is that often the second
    index is skipped to avoid messy presentations. However, without it, the clarity
    that there are actually 2 things going on is lost:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Movement overtime indexed by t as the variable evolves
  prefs: []
  type: TYPE_NORMAL
- en: 2\. At each point of time a choice of state from the set of states
  prefs: []
  type: TYPE_NORMAL
- en: Let us continue with our example of the road to illustrate the distinction between
    the stochastic process evolving over time and different state spaces. There are
    a finite number of state spaces which the stochastic variable can take at a point
    of time. So, for example, when we head out to office we can potentially meet with
    clear roads or a jam on the way for the first 15 minutes. In the next 15 minutes,
    the situation may change. Suppose that it requires 45 minutes to get to office,
    then we can break it up into 3 different time slots of 15 minutes. So, our time
    periods can be t=1,2,3.
  prefs: []
  type: TYPE_NORMAL
- en: There is a probability p of clear road, q of meeting a jam and r of meeting
    a road block in the first 15 minutes. In the next 15 minutes, one of these 3 outcomes
    can again occur and similarly for the last 15 minutes. This is evolution over
    time. This means that, at each observation at a certain time, there is a certain
    probability to get a certain outcome. This can be illustrated as below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e72ab0a6813c0777f742c6e210871e7a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Office drive MDP (Image source: author)'
  prefs: []
  type: TYPE_NORMAL
- en: The image shows a sample trajectory. At time t=1, i.e., first 15 minutes, the
    road can be clear, jammed or blocked with respective probabilities. Similarly
    for t=2 or second 15 minutes and so on. At a point of time, say, first 15 minutes,
    we can see the road state can vary randomly across 3 possible states. This represents
    the randomness in state at a point of time. However, if we look across time t,
    then we have a trajectory. So, if we have 3 time points, we will also have 3 random
    variables, one associated with each point of time.
  prefs: []
  type: TYPE_NORMAL
- en: We can also understand the reward set in this context. The reward set **R**
    in this case would be the time taken to cover the specific segment of the road
    at a particular time. There is a defined distribution of time (for simplicity
    we can keep it as discrete minutes) for each state. What does this mean? Basically,
    each state of the road would be associated with a distribution of time. It‚Äôs like
    sometimes we do the clear road (s) in 15 minutes and sometimes in 17 minutes due
    to some random variations and, at others in 18 minutes. So, we can define R(s)={15,16,17,18}.
    There is a probability distribution defined for the time taken to complete the
    segment with clear road conditions. This is an intuitive example of how the reward
    can also be random ‚Äî something which most of the examples in Sutton (2017), Ch3
    do not cover. Each state of the road ‚Äî for example, the jammed state ‚Äî would have
    its own distribution over the ETA.
  prefs: []
  type: TYPE_NORMAL
- en: '***Realization or trajectory of a stochastic process***'
  prefs: []
  type: TYPE_NORMAL
- en: '*A stochastic process can have many* [*outcomes*](https://en.wikipedia.org/wiki/Outcome_(probability))*,
    due to its randomness. A single outcome of a stochastic process is called variously
    as a* [***realization***](https://en.wikipedia.org/wiki/Stochastic_process)***,
    episode or a trajectory****. It is formed by taking a single possible value of
    the random variable at each time point of the stochastic process as it evolves
    over time.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, to continue with our traffic example: a possible trajectory is shown in
    the image by the arrows. In the first 15 minutes, the motorist meets a clear road,
    in the next 15 minutes hits a jam and in the final 15 minutes hits a blocked road.
    So, a potential sample path would be:(s,s‚Äô,s‚Äô‚Äô). This is just one possible realization.
    The motorist traverses this path every day and is likely to meet a slightly different
    set of states every day; another day it can be (s‚Äô‚Äô,s‚Äô, s).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Stochastic processes are widely used for probabilistic representations of
    systems and phenomena that appear to vary in a random manner. Or, to put it more
    simply, stochastic processes or models are used to estimate the probability of
    various outcomes of a random variable which also changes overtime. Examples include
    the growth of a* [*bacterial*](https://en.wikipedia.org/wiki/Bacteria) *population,
    or the movement of a* [*gas*](https://en.wikipedia.org/wiki/Gas)[*molecule*](https://en.wikipedia.org/wiki/Molecule)*.
    They are also used extensively in financial analysis where stochastic models can
    be used to estimate situations involving uncertainty, such as investment returns,
    volatile markets, or inflation rates*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Further there are many different types of stochastic processes of which the
    Markov process is one type. Others, for example, include random walks, Gaussian
    processes, etc. Each stochastic process has its own specific assumptions and features
    which it is important to understand.*'
  prefs: []
  type: TYPE_NORMAL
- en: '***Probability Distribution***'
  prefs: []
  type: TYPE_NORMAL
- en: Now we turn to the question of how do we get the probabilities of different
    states? Recall, in above example, we have defined the probabilities of meeting
    a jam and having clear road as q and p. Now, let us just elaborate a bit about
    what this means in terms of probability distributions and how they come about.
  prefs: []
  type: TYPE_NORMAL
- en: The probability of a random outcome (road is jammed) is assessed as the proportion
    of times the outcome would occur in a very long series of repetitions. So, this
    is how we would assess p, q, r by noting the state of road variable at the specific
    time intervals specified across multiple days.
  prefs: []
  type: TYPE_NORMAL
- en: '*Recording all these probabilities of outputs of a random variable,* S‚Çú , *gives
    us the probability distribution of the random variable. In the discrete case it
    is termed the probability mass distribution.* *This is a function that provides
    the probability that a discrete random variable equals a particular value. Read
    more of this* [*here*](https://en.wikipedia.org/wiki/Probability_distribution)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'To continue with our office commute example: the State variable, can therefore
    take any of these values with a specific probability. For example if our state
    set for a road consists of 3 possible states: **S**= { s = clear, s‚Äô=jammed, s‚Äô‚Äô=blocked}.'
  prefs: []
  type: TYPE_NORMAL
- en: The road can have any of these 3 states at a specific point of time (say morning)
    with a probability. For example, it can be free =.5, jammed =.4, and blocked =.1(blocking
    is relatively rarer state!). The set of probabilities is termed the probability
    distribution for the variable which is the state of the road.
  prefs: []
  type: TYPE_NORMAL
- en: '***Joint Distribution vs Marginal Distribution vs Conditional Distribution***'
  prefs: []
  type: TYPE_NORMAL
- en: '*In case of stochastic processes, since the random variable is the same but
    measured over different points of time, it is likely that state in t,* S‚Çú , *is
    correlated to states in previous periods,* S‚Çú‚Çã‚ÇÅ, S‚Çú‚Çã‚ÇÇ. *In this case, then, if
    we want to understand the probability of* S‚Çú *, it is the joint probability mass
    distribution (pmf) that is relevant. The joint pmf allows us to compute probabilities
    of events involving multiple random variables taking into account the relationship
    between the variables.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Given two* [*random variables*](https://en.wikipedia.org/wiki/Random_variable)
    *that are defined on the same* [*probability space*](https://en.wikipedia.org/wiki/Probability_space)
    *the* [***joint probability distribution***](https://en.wikipedia.org/wiki/Joint_probability_distribution)
    *is the corresponding* [*probability distribution*](https://en.wikipedia.org/wiki/Probability_distribution)
    *on all possible pairs of outputs.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*This can be defined more generally in Markov context as:*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Suppose we consider a discrete time 2 period stochastic process,* S‚Çô *, where
    n=1,2\. So,* S‚ÇÅ and S‚ÇÇ *are the two discrete stochastic random variables which
    can take values from the set i* ‚àà S = {1,2}. *The joint distribution of* S‚Çô is
    given for *every n, i.e., time periods and finite sequence of states:(*i‚ÇÅ,i‚ÇÇ)
    by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3ee11bfd7563334600b682307bfac7e8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let us look at our simple road example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For simplicity assume that there are only 2 time periods ‚Äî i.e., S‚Çú and S‚Çú‚Çã‚ÇÅ.
    The state space is still 3: {clear( C ), jammed(J), blocked(B)}.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The joint probability table would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b744db17148155938ea05c72804862fe.png)'
  prefs: []
  type: TYPE_IMG
- en: 2 period office drive joint probability distribution ( source:author)
  prefs: []
  type: TYPE_NORMAL
- en: The joint probability is the probability of two events occurring together. The
    table above provides the different combinations of instances of both events. Notice,
    since this is a stochastic process, time t-1 always comes first and then t.
  prefs: []
  type: TYPE_NORMAL
- en: '***Marginal probability distribution***'
  prefs: []
  type: TYPE_NORMAL
- en: '*The* [*marginal distribution*](https://en.wikipedia.org/wiki/Joint_probability_distribution)
    *of a variable gives the probabilities of various values of the variables in the
    subset without reference to the values of the other variables. The marginal mass
    function for* S·µ¢ is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7ed0b4f566946e349449eeb2156a2566.png)'
  prefs: []
  type: TYPE_IMG
- en: The marginal probability is the probability of a single event occurring. Essentially
    the marginal distribution is the column wise sum of probabilities. The equation
    for this requires you to sum over the column for j. The joint probability for
    any combination is given by the cell probabilities. In our example‚Äôs context,
    it is the probability of meeting a jam in S‚Çú irrespective of what road conditions
    were in S‚Çú‚Çã‚ÇÅ, i.e., we sum the probabilities across the col J.
  prefs: []
  type: TYPE_NORMAL
- en: '***Conditional probability***'
  prefs: []
  type: TYPE_NORMAL
- en: '*A* [*conditional probability*](https://en.wikipedia.org/wiki/Conditional_probability_distribution)
    *is that a specific event occurs given that another specific event has already
    occurred. This means the probability of a jam in* S‚Çú *given that the road was
    clear in* S‚Çú‚Çã‚ÇÅ.'
  prefs: []
  type: TYPE_NORMAL
- en: Now in our case, because S‚Çú is a stochastic variable, and we are looking at
    a two period example, the joint and conditional probabilities are the same.
  prefs: []
  type: TYPE_NORMAL
- en: '*Mathematically, the conditional distribution of a variable given another variable
    is the joint distribution of both variables divided by the marginal distribution
    of the other variable.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Defining this generally it is:*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dde8178697faf0a7231e62d4bf85bd21.png)'
  prefs: []
  type: TYPE_IMG
- en: '*The generalization of this to case where we have n random variables,* X‚ÇÅ,..,
    X‚Çô.'
  prefs: []
  type: TYPE_NORMAL
- en: '*The joint probability distribution of these n variables is:*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2d293c3740f6f14ecbe1a7a3ee91fb29.png)'
  prefs: []
  type: TYPE_IMG
- en: '*This can be written as the conditional function times the marginal function.
    This is based on the* [*chain rule of probability*](https://en.wikipedia.org/wiki/Chain_rule_(probability))*.
    In terms of notation,* P‚Çì *(*x‚ÇÅ) = P(X=x‚ÇÅ). *For the 2 random variables,* X‚ÇÅ,
    X‚ÇÇ *the joint distribution can be written as:*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/831a66b22750e3ca7f6ad95e7465229c.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Stated in words: the joint probability distribution of 2 variables* X‚ÇÅ, X‚ÇÇ
    *taking specific values* x‚ÇÅ, x‚ÇÇ *can be given by the conditional probability of*
    X‚ÇÇ=x‚ÇÇ *given that* X‚ÇÅ=x‚ÇÅ, *times the marginal probability of getting* X‚ÇÅ=x‚ÇÅ.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Why do we need these distributions ‚Äî deriving the need for the Markov Property***'
  prefs: []
  type: TYPE_NORMAL
- en: Joint, conditional and marginal probability distribution definitions become
    important in the context of MDPs when we want to determine the probabilities of
    the State variable taking specific values as it transitions across different time
    periods. Our example is simple and consists of 3 time periods. But, what if the
    time period was 100, and we wanted to understand the probability of the state
    taking a specific value in the 100th time period? This is the underlying probability
    problem. Let‚Äôs look at how the general case is solved and then what the Markov
    property is
  prefs: []
  type: TYPE_NORMAL
- en: '*For the more general case, to get the joint probability of* X‚ÇÅ,.., X‚Çô *random
    variables taking specific values,* x‚ÇÅ,.., x‚Çô *can be found by the conditional
    probability of* X‚Çô *given the entire history of previous random variables taking
    specific values, times the joint probability distribution of the n-1th distribution.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/32c29c24137d4886665587e377268d89.png)'
  prefs: []
  type: TYPE_IMG
- en: '*We can open out the expression for joint probability of P(*X‚Çô‚Çã‚ÇÅ=x‚Çô‚Çã‚ÇÅ‚Ä¶,X‚ÇÅ=x‚ÇÅ)
    *again into an expression for conditional probability of* X‚Çô‚Çã‚ÇÅ *given the history
    of (*X‚Çô‚Çã‚ÇÇ,‚Ä¶,X‚ÇÅ) *and the joint probability of (*X‚ÇÅ,..,X‚Çô‚Çã‚ÇÇ).'
  prefs: []
  type: TYPE_NORMAL
- en: '*This can be further unrolled backwards. We will finally get the expression
    for a sequence of conditional probabilities and the marginal probability of the
    first variable P(*X‚ÇÅ=x‚ÇÅ)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/91af8fb4cd7f3e847a7d8ded58ae7481.png)'
  prefs: []
  type: TYPE_IMG
- en: Now let us translate this to the MDP context. So far, this has been couched
    in terms of any sequence of random variables. However, this result translates
    easily to stochastic variables and Markov processes. We just need to replace X·µ¢
    by S‚Çú and now the indexing would be in terms of time, t=1,2,..n instead of X·µ¢
    = x·µ¢.
  prefs: []
  type: TYPE_NORMAL
- en: Why am I belaboring the obvious? Because there is a lot of material out there
    that uses different variables and indices for Markov processes and other sequences
    of random variables and this can get hugely confusing. I tried to unravel this
    for myself and hence am writing it down, so that it can help other non-statisticians
    who are trying to figure out the probability theory behind Markov results.
  prefs: []
  type: TYPE_NORMAL
- en: For analyzing the MDP, t=1,..,n and states x·µ¢ ‚àà S. Just to be clear x‚Çô is a
    specific value of a state at time period t=n.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs write the joint probability of getting S‚ÇÅ,..,S‚Çô as the state variable
    evolves over t=1..n. Substituting in the general case discussed above we get the
    joint probability as a sequence of conditional probabilities and the marginal
    distribution of S‚ÇÅ.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e219768fe3b276680ffb69ef9a7f5725.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To put it in words: the probability of getting states S‚ÇÅ,..,S‚Çô with specific
    values is equal to the conditional probability of the state at time t=n having
    a value x‚Çô given the historical values of previous states times the joint distribution
    of the n-1th period. This unrolls into a sequence of conditional probabilities
    and the marginal or initial distribution of state at S‚ÇÅ.'
  prefs: []
  type: TYPE_NORMAL
- en: Now this is just a stochastic process where the value of a state in time period
    n depends on the past history of state values. This is clearly intractable to
    compute, hence the Markov property
  prefs: []
  type: TYPE_NORMAL
- en: '***Markov Property:***'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Markov property makes a state S‚Çô depend on only the immediately preceding
    state, S‚Çô‚Çã‚ÇÅ, rather than the entire historical backlog of states. In this case
    our conditional probability for state S‚Çô reduces to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fa4686a7d953c456ec5748e0a61e7e2e.png)'
  prefs: []
  type: TYPE_IMG
- en: However, we still have n-1 one step conditional probabilities to be estimated
    along with the initial probability distribution, *P(*X‚ÇÅ=x‚ÇÅ). Still somewhat complex,
    as there are a large number of conditional probabilities to be calculated. A further
    assumption that is made to simplify and make the calculations more tractable is
    time homogeneity of transition probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '***Time homogeneity of transition probabilities***'
  prefs: []
  type: TYPE_NORMAL
- en: '*This basically says that the probability of transitioning from state i in
    time period n to state j in period n+1 is the same irrespective of the time periods:*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7252b0a97eedde66e56d3017a4a6eaa1.png)'
  prefs: []
  type: TYPE_IMG
- en: '*What this means is that the probabilities for transitioning from one state
    to the next is fixed, irrespective of time.*'
  prefs: []
  type: TYPE_NORMAL
- en: To put it in context of our road example, the probability of going from clear
    road to jammed is fixed irrespective of whether we meet these 2 states in the
    first 2 time periods or the last 2 time periods. So, the probability of transition
    reduces to the following 9 cases irrespective of time. Where C= clear, j=jammed
    and B=blockled.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/607a0cf49440b45aa8727f0ec37ec842.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, if the number of states is less than the time period, as is
    more likely, this will significantly reduce the probabilities to be calculated.
  prefs: []
  type: TYPE_NORMAL
- en: '***Transition probabilities***'
  prefs: []
  type: TYPE_NORMAL
- en: '*The state* [*transition probability matrix*](https://en.wikipedia.org/wiki/Discrete-time_Markov_chain)
    *of a Markov chain gives the probabilities of transitioning from one state to
    another in a single time step.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*For t=1,2 states case this reduces the dimensions of the problem to*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/17300716c49a49e0bad004b4fc071bb3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This breaks up into the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The conditional probability:*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8174f8ebfb168c1ee0e42311baec22df.png)'
  prefs: []
  type: TYPE_IMG
- en: '*The initial distribution or marginal distribution: P(*S‚ÇÅ=x‚ÇÅ)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our road example, this was the initial probabilities for the 3 states: p,q,r.'
  prefs: []
  type: TYPE_NORMAL
- en: So, all we need is the one step conditional probabilities and the initial distribution
    of states to get the joint probability of transitioning to a state.
  prefs: []
  type: TYPE_NORMAL
- en: '*The transition matrix can be represented as below:*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a184600677e45bc59986b9fad4c3c4bc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [Wikipedia Stochastic Matrix](https://en.wikipedia.org/wiki/Stochastic_matrix#:~:text=In%20mathematics%2C%20a%20stochastic%20matrix,substitution%20matrix%2C%20or%20Markov%20matrix.)'
  prefs: []
  type: TYPE_NORMAL
- en: '*In the transition matrix the rows represent the current state and the columns
    the future state. Thus* p‚ÇÅ‚ÇÇ *is the probability of transitioning from state 1
    in time t to state 2 in t+1.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Also note the sum of all branch probabilities from a state has to be one.
    This is logical as you have to transition from a state to another one. Mathematically
    this is written as:*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a8e31f02b2165753041bbf20c42822b1.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Why do we sum over ‚Äòj‚Äô? Because this represents the possible states that you
    can transition to from, ‚Äòi‚Äô.*'
  prefs: []
  type: TYPE_NORMAL
- en: '***Markov Chain dynamics***'
  prefs: []
  type: TYPE_NORMAL
- en: For a Markov chain, we use the word dynamics to describe the variation of the
    state over a short time interval starting from a given initial state. The evolution
    of a Markov chain is a random process and so we cannot say exactly what sequence
    of states will follow the initial state. In reinforcement learning we wish predict
    the future states given the current state or initial state. A prediction of the
    future state, S‚Çô given S‚ÇÅ can be calculated easily once we have the probability
    transition matrix and the initial distribution. The n -step transition probability
    matrix can be found by multiplying the single-step probability matrix by itself
    n times.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8f6bd37c2c6ba38902a8477428014508.png)'
  prefs: []
  type: TYPE_IMG
- en: This is the beauty of the Markov system and why it is used in reinforcement
    learning. Once we have a tractable set of transition probabilities, we can calculate
    the probability of transitioning from any initial state to state j in n steps.
  prefs: []
  type: TYPE_NORMAL
- en: Now after this very long digression into the stochastic and probability behind
    MDPs, I will briefly review the MDP equations in Sutton (2017), ch 3.
  prefs: []
  type: TYPE_NORMAL
- en: '**Section 3: Understanding Sutton Ch3 MDP Equations Using Probability and Stochastic
    Concepts**'
  prefs: []
  type: TYPE_NORMAL
- en: '***The MDP Model***'
  prefs: []
  type: TYPE_NORMAL
- en: In this section I briefly define the MDP giving what is essentially the textbook
    definition. In the next section we will look at the statistical theory underlying
    MDPs.
  prefs: []
  type: TYPE_NORMAL
- en: '*The MDP framework is a considerable abstraction of the problem of goal-directed
    learning from interaction. It proposes that whatever the details of the sensory,
    memory, and control apparatus, and whatever objective one is trying to achieve,
    any problem of learning goal-directed behavior can be reduced to three signals
    passing back and forth between an agent and its environment: one signal to represent
    the choices made by the agent (the actions), one signal to represent the basis
    on which the choices are made (the states), and one signal to define the agent‚Äôs
    goal (the rewards). This framework may not be sufficient to represent all decision-learning
    problems usefully, but it has proved to be widely useful and applicable.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Sutton ch2, p50*'
  prefs: []
  type: TYPE_NORMAL
- en: Most of the MDP framework can actually be understood at a high level without
    detailed knowledge of probability and stochastic theory. However, now that we
    have exposure to those concepts we can actually appreciate what is going on and
    its complexity. Let us now look at some of the equations related to the Markov
    process that are presented in Sutton (2017), ch3.
  prefs: []
  type: TYPE_NORMAL
- en: '***Markov Property***'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is essentially the idea that the future is independent of the past, given
    the present. It is conceptualized probabilistically as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e8419e256fd14c81e1b587212d392bd8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This has already been discussed above at length and enables us to understand
    how the basic Markov problem can be made more tractable. Stochastic processes
    that satisfy the Markov property are typically much simpler to analyze than incorporation
    of the historical process in models. In practice, it is important to consider
    whether the Markov property will hold. For many scenarios, for example, the position
    of a car, the present location of the car will determine where it goes in future
    states. But, in some instances, history maybe important: for example, a trading
    agent may have to consider past trends in a share as well as the present price
    to determine whether to buy or sell.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A Markov process is a stochastic process with the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: (a.) The number of possible outcomes or states is finite
  prefs: []
  type: TYPE_NORMAL
- en: This is there for analytical convenience. Infinite processes can also be handled
    with some minor manipulation. However, for time dependent processes, we assume
    time is finite t=0,1,2‚Ä¶,T.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (b.) The outcome at any stage depends only on the outcome of the previous stage
    ‚Äî given by Markov property discussed above.
  prefs: []
  type: TYPE_NORMAL
- en: (c.) The transition probabilities are constant over time.
  prefs: []
  type: TYPE_NORMAL
- en: (d.) The system is stationary ‚Äî this is not actually called out, but property
    c) actually implies it.
  prefs: []
  type: TYPE_NORMAL
- en: '***Formal representation of MDP***'
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally the MDP can be represented as follows. Given, the Markov property,
    an MDP is defined as follows: a tuple which typically contains 5 elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/060e9627a0855438bb93867e50909331.png)'
  prefs: []
  type: TYPE_IMG
- en: '**S**: Set of states in the environment: *this is actually a stochastic process*'
  prefs: []
  type: TYPE_NORMAL
- en: '**A**: Set of actions possible in each state'
  prefs: []
  type: TYPE_NORMAL
- en: '**R:** Reward function in response to actions: *This is normally deterministic,
    but sometimes can be stochastic as well*'
  prefs: []
  type: TYPE_NORMAL
- en: '**p :** Probability matrix of transitioning from one state to the next ‚Äî This
    is a critical part of an MDP. It is defined by the joint and conditional distribution
    of the state variable.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Œ≥ :** Discount factor to be applied for rewards in the distant future'
  prefs: []
  type: TYPE_NORMAL
- en: 'The MDP and agent together thereby give rise to a sequence or trajectory that
    begins like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c3dfa5301b2d5039c76ef0268da018a2.png)'
  prefs: []
  type: TYPE_IMG
- en: This process is a stochastic process and is finite, i.e., has T time steps with
    a clear terminal state.
  prefs: []
  type: TYPE_NORMAL
- en: '***Understanding equations***'
  prefs: []
  type: TYPE_NORMAL
- en: '*Equation 3.2*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dda8c3e2e3878d545cc3ee992cfd1ba7.png)'
  prefs: []
  type: TYPE_IMG
- en: This equation tells us that R‚Çú and S‚Çú are random variables with discrete probability
    mass distributions which depend on the previous state and action. Equation 3.2
    defines the dynamics function p. This is a function that takes in 4 arguments.
    It also specifies a conditional distribution for each choice of state and action.
    For a specific value of state s‚Äô and reward r, we can estimate the probability
    of these values occurring at time t, given the preceding state and action values
    at t-1.
  prefs: []
  type: TYPE_NORMAL
- en: '*Equation 3.3*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/35cffa20d2bc48f2491305f3080fb469.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 3.3 basically tells us that for each value of ‚Äòs‚Äô and ‚Äòa‚Äô, there is
    a joint distribution of states and rewards. This is basically a joint conditional
    distribution, hence it must sum to 1\. The joint distribution will have the probabilities
    of different combinations of s‚Äô and r. The sum of all probabilities of all combinations
    must sum to 1 ([refer to Wikipedia](https://en.wikipedia.org/wiki/Joint_probability_distribution)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs go back to our road example. At each state of the road, we have two actions:
    drive or stop. Then, if we are in state ‚Äòjammed‚Äô and we choose to drive. Then
    conditional on these two, there is a joint distribution of states and rewards
    for the next state.'
  prefs: []
  type: TYPE_NORMAL
- en: '*State transition probabilities ‚Äî Equation 3.4*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f20656f25b96ae18ef0a61e75f836266.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The state transition probabilities are given by summing over reward probabilities
    (note for the probability challenged: like a marginal probability distribution
    of states- [refer to Wikipedia](https://en.wikipedia.org/wiki/Joint_probability_distribution)).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Expected Rewards ‚Äî Equation 3.5*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d16279d0fa3436d03788c23e817da7c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here we are again working with equation 3.2\. This time we sum over the probability
    of states (note for the probability challenged: like a marginal probability distribution
    of rewards ‚Äî [refer to Wikipedia](https://en.wikipedia.org/wiki/Joint_probability_distribution)).
    This gives us the marginal probability of rewards distribution (expression on
    extreme RHS of 3.5). We then multiply this probability by the actual rewards,
    r to get the expected rewards.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Expected rewards for state‚Äìaction‚Äìnext-state triples ‚Äî Equation 3.6*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a79bbafe62dcd3eaceb9c8f08eea3eb3.png)'
  prefs: []
  type: TYPE_IMG
- en: This one is a little trickier. Probability of reward and state joint probability
    conditional on ‚Äòs‚Äô and ‚Äòa‚Äô divided by state transition probability . This is an
    application of chain rule of joint distributions (refer [Wikipedia](https://en.wikipedia.org/wiki/Chain_rule_(probability))).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/403725b86d34581c1cfbaf9ef844b799.png)'
  prefs: []
  type: TYPE_IMG
- en: Let‚Äôs substitute our terms.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c03c5419d93ff42848065fc7eff84bb1.png)![](../Images/8accbbba3b261f8518cac930ab112523.png)'
  prefs: []
  type: TYPE_IMG
- en: We can now take expectations over R‚Çú
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/01fa099b84e858904ef3bb2a6430d147.png)'
  prefs: []
  type: TYPE_IMG
- en: So, this completes the basic equations derived for the set up of the MDP. It
    is important to understand each of 3.2‚Äì3.6 as Sutton(2017) mention that each of
    these is a way to represent the MDP and would be used again in subsequent chapters.
  prefs: []
  type: TYPE_NORMAL
- en: '**To Conclude..**'
  prefs: []
  type: TYPE_NORMAL
- en: Well friends, this has been quite a journey on probability and stochastic theory.
    However, I hope what you have realized is the complexity and beauty of the underlying
    Markov system. Without understanding these, it is difficult to figure out the
    equations that underlie the basic MDP equations. Without understanding these equations
    and how they are derived, it is likely that one will become totally lost when
    it comes to policy, value functions and Q values. These will be picked up in the
    next installment of this tutorial.
  prefs: []
  type: TYPE_NORMAL
- en: 'I hope you liked my writing. Please consider following me for more such articles
    as I proceed with reinforcement learning: [https://medium.com/@shaileydash](https://medium.com/@shaileydash)'
  prefs: []
  type: TYPE_NORMAL
- en: Also do let me know your inputs and comments in the comments which will help
    me revise this.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can follow me on Linkedin for more corporate oriented articles on AI and
    DS: [www.linkedin.com/in/shailey-dash-phd-8b4a286](http://www.linkedin.com/in/shailey-dash-phd-8b4a286)'
  prefs: []
  type: TYPE_NORMAL
- en: '**References**'
  prefs: []
  type: TYPE_NORMAL
- en: I am listing quite a long list of stats and probability theory resources. They
    are all excellent, if difficult.
  prefs: []
  type: TYPE_NORMAL
- en: 'Richard S. Sutton and Andrew G. Barto. [Reinforcement Learning: An Introduction;
    2nd Edition](http://incompleteideas.net/book/bookdraft2017nov5.pdf). 2017.'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Markov_decision_process](https://en.wikipedia.org/wiki/Markov_decision_process)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Stochastic_process](https://en.wikipedia.org/wiki/Stochastic_process)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Joint_probability_distribution](https://en.wikipedia.org/wiki/Joint_probability_distribution)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Conditional_probability_distribution](https://en.wikipedia.org/wiki/Conditional_probability_distribution)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.kent.ac.uk/smsas/personal/lb209/files/notes1.pdf](https://www.kent.ac.uk/smsas/personal/lb209/files/notes1.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Lawler, G.F. (2006). Introduction to Stochastic Processes (2nd ed.). Chapman
    and Hall/CRC. [https://doi.org/10.1201/9781315273600](https://doi.org/10.1201/9781315273600)
  prefs: []
  type: TYPE_NORMAL
- en: 'Yates, R.D. and Goodman, D.J., 2014\. *Probability and stochastic processes:
    a friendly introduction for electrical and computer engineers*. John Wiley & Sons.'
  prefs: []
  type: TYPE_NORMAL
- en: Unnikrishna Pillai, Probability and Stochastic Processes, Lecture 8, NYU, Tandon
    school of Engineering, [https://www.youtube.com/watch?v=nFF6eRQT2-c&t=455s](https://www.youtube.com/watch?v=nFF6eRQT2-c&t=455s)
  prefs: []
  type: TYPE_NORMAL
