- en: 'Reinforcement Learning Basics: Understanding Stochastic Theory Underlying a
    Markov Decision Process'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ åŸºç¡€ï¼šç†è§£é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹èƒŒåçš„éšæœºç†è®º
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/reinforcement-learning-basics-1-understanding-stochastic-theory-underlying-an-mdp-e770be0665cf?source=collection_archive---------5-----------------------#2023-02-16](https://towardsdatascience.com/reinforcement-learning-basics-1-understanding-stochastic-theory-underlying-an-mdp-e770be0665cf?source=collection_archive---------5-----------------------#2023-02-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/reinforcement-learning-basics-1-understanding-stochastic-theory-underlying-an-mdp-e770be0665cf?source=collection_archive---------5-----------------------#2023-02-16](https://towardsdatascience.com/reinforcement-learning-basics-1-understanding-stochastic-theory-underlying-an-mdp-e770be0665cf?source=collection_archive---------5-----------------------#2023-02-16)
- en: 'Part 1: On the Markov Decision Model which forms the theoretical foundation
    of reinforcement learning problems'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€éƒ¨åˆ†ï¼šé©¬å°”ç§‘å¤«å†³ç­–æ¨¡å‹çš„ç†è®ºåŸºç¡€ï¼Œè¿™ä¸ºå¼ºåŒ–å­¦ä¹ é—®é¢˜æä¾›äº†åŸºç¡€
- en: '[](https://medium.com/@shaileydash?source=post_page-----e770be0665cf--------------------------------)[![Shailey
    Dash](../Images/b5a44aafce4c310f60398e714d515b76.png)](https://medium.com/@shaileydash?source=post_page-----e770be0665cf--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e770be0665cf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e770be0665cf--------------------------------)
    [Shailey Dash](https://medium.com/@shaileydash?source=post_page-----e770be0665cf--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@shaileydash?source=post_page-----e770be0665cf--------------------------------)[![Shailey
    Dash](../Images/b5a44aafce4c310f60398e714d515b76.png)](https://medium.com/@shaileydash?source=post_page-----e770be0665cf--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e770be0665cf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e770be0665cf--------------------------------)
    [Shailey Dash](https://medium.com/@shaileydash?source=post_page-----e770be0665cf--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc4fe44a8e55d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-basics-1-understanding-stochastic-theory-underlying-an-mdp-e770be0665cf&user=Shailey+Dash&userId=c4fe44a8e55d&source=post_page-c4fe44a8e55d----e770be0665cf---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e770be0665cf--------------------------------)
    Â·28 min readÂ·Feb 16, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe770be0665cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-basics-1-understanding-stochastic-theory-underlying-an-mdp-e770be0665cf&user=Shailey+Dash&userId=c4fe44a8e55d&source=-----e770be0665cf---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc4fe44a8e55d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-basics-1-understanding-stochastic-theory-underlying-an-mdp-e770be0665cf&user=Shailey+Dash&userId=c4fe44a8e55d&source=post_page-c4fe44a8e55d----e770be0665cf---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e770be0665cf--------------------------------)
    Â·28åˆ†é’Ÿé˜…è¯»Â·2023å¹´2æœˆ16æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe770be0665cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-basics-1-understanding-stochastic-theory-underlying-an-mdp-e770be0665cf&user=Shailey+Dash&userId=c4fe44a8e55d&source=-----e770be0665cf---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe770be0665cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-basics-1-understanding-stochastic-theory-underlying-an-mdp-e770be0665cf&source=-----e770be0665cf---------------------bookmark_footer-----------)![](../Images/7ffb68ec9c76b1f9339fd860f362d1a1.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe770be0665cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-basics-1-understanding-stochastic-theory-underlying-an-mdp-e770be0665cf&source=-----e770be0665cf---------------------bookmark_footer-----------)![](../Images/7ffb68ec9c76b1f9339fd860f362d1a1.png)'
- en: 'Example of a simple MDP with three states (green circles) and two actions (orange
    circles), with two rewards (Image source: [Wikipedia](https://en.wikipedia.org/wiki/Markov_decision_process#/media/File:Markov_Decision_Process.svg))'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªç®€å•çš„MDPç¤ºä¾‹ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªçŠ¶æ€ï¼ˆç»¿è‰²åœ†åœˆï¼‰å’Œä¸¤ä¸ªåŠ¨ä½œï¼ˆæ©™è‰²åœ†åœˆï¼‰ï¼Œä»¥åŠä¸¤ä¸ªå¥–åŠ±ï¼ˆå›¾ç‰‡æ¥æºï¼š[ç»´åŸºç™¾ç§‘](https://en.wikipedia.org/wiki/Markov_decision_process#/media/File:Markov_Decision_Process.svg)ï¼‰
- en: Reinforcement learning (RL) is a type of machine learning that enables an agent
    to learn to achieve a goal in an uncertain environment by taking actions. An important
    aspect of reinforcement learning is that it evaluates the actions taken rather
    than instructs by giving correct actions. Each action has a reward associated
    with it which signals to the agent the success of the action in progressing towards
    the goal. The agent navigates the environment repetitively learning how to optimize
    to reach its goal.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ ç±»å‹ï¼Œä½¿ä»£ç†èƒ½å¤Ÿé€šè¿‡é‡‡å–è¡ŒåŠ¨åœ¨ä¸ç¡®å®šçš„ç¯å¢ƒä¸­å­¦ä¹ å®ç°ç›®æ ‡ã€‚å¼ºåŒ–å­¦ä¹ çš„ä¸€ä¸ªé‡è¦æ–¹é¢æ˜¯å®ƒè¯„ä¼°æ‰€é‡‡å–çš„è¡ŒåŠ¨ï¼Œè€Œä¸æ˜¯é€šè¿‡æä¾›æ­£ç¡®çš„è¡ŒåŠ¨æ¥æŒ‡å¯¼ã€‚æ¯ä¸ªè¡ŒåŠ¨éƒ½æœ‰ä¸€ä¸ªç›¸å…³çš„å¥–åŠ±ï¼Œå‘ä»£ç†å‘å‡ºè¯¥è¡ŒåŠ¨åœ¨æœç€ç›®æ ‡å‰è¿›ä¸­æˆåŠŸçš„ä¿¡å·ã€‚ä»£ç†åœ¨ç¯å¢ƒä¸­é‡å¤å¯¼èˆªï¼Œå­¦ä¹ å¦‚ä½•ä¼˜åŒ–ä»¥è¾¾åˆ°å…¶ç›®æ ‡ã€‚
- en: Till very recently most of reinforcement learningâ€™s successes where related
    to game playing agents such as Alpha Go. However increasingly reinforcement learning
    is being applied in real world applications such as self-driving cars, and robotic
    automation. The recently launched ChatGPT also has a RL component in its architecture
    for finetuning of answers. Given this, it becomes important to understand RL which
    has been acknowledged to have a steep learning curve both from the theoretical
    perspective and also the practical implementation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´åˆ°æœ€è¿‘ï¼Œå¤§å¤šæ•°å¼ºåŒ–å­¦ä¹ çš„æˆåŠŸä¸æ¸¸æˆä»£ç†ç›¸å…³ï¼Œå¦‚ Alpha Goã€‚ç„¶è€Œï¼Œå¼ºåŒ–å­¦ä¹ è¶Šæ¥è¶Šå¤šåœ°åº”ç”¨äºç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ï¼Œå¦‚è‡ªåŠ¨é©¾é©¶æ±½è½¦å’Œæœºå™¨äººè‡ªåŠ¨åŒ–ã€‚æœ€è¿‘æ¨å‡ºçš„
    ChatGPT ä¹Ÿåœ¨å…¶æ¶æ„ä¸­åŒ…å«äº†ä¸€ä¸ªç”¨äºç­”æ¡ˆå¾®è°ƒçš„ RL ç»„ä»¶ã€‚é‰´äºæ­¤ï¼Œç†è§£ RL å˜å¾—é‡è¦ï¼Œå› ä¸ºå®ƒè¢«è®¤ä¸ºå…·æœ‰é™¡å³­çš„å­¦ä¹ æ›²çº¿ï¼Œæ—¢åŒ…æ‹¬ç†è®ºæ–¹é¢ï¼Œä¹ŸåŒ…æ‹¬å®é™…åº”ç”¨ã€‚
- en: This article focuses on the Markov Decision model (MDP) which is often used
    to formalize the reinforcement agentâ€™s problem. However, a neglected aspect of
    this conceptualization of the RL problem is that the MDP framework is an important
    application of Stochastic theory.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡é‡ç‚¹è®¨è®ºäº†é©¬å°”å¯å¤«å†³ç­–æ¨¡å‹ï¼ˆMDPï¼‰ï¼Œå®ƒé€šå¸¸ç”¨äºå½¢å¼åŒ–å¼ºåŒ–å­¦ä¹ ä»£ç†çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™ç§å¯¹å¼ºåŒ–å­¦ä¹ é—®é¢˜çš„æ¦‚å¿µåŒ–ä¸€ä¸ªè¢«å¿½è§†çš„æ–¹é¢æ˜¯ MDP æ¡†æ¶æ˜¯éšæœºç†è®ºçš„é‡è¦åº”ç”¨ã€‚
- en: There is a non-trivial amount of stochastic and probability theory that underlies
    the basic Markov Decision Process equations. Without understanding these equations
    and how they are derived, it is difficult to make progress with RL. This article
    aims to clarify the stochastic and probability concepts that underlie MDPs with
    a focus on understanding equations presented in Sutton(2017) Ch 3.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºæœ¬çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹æ–¹ç¨‹èƒŒåæœ‰ç›¸å½“å¤æ‚çš„éšæœºå’Œæ¦‚ç‡ç†è®ºã€‚æ²¡æœ‰ç†è§£è¿™äº›æ–¹ç¨‹åŠå…¶æ¨å¯¼è¿‡ç¨‹ï¼Œå°±å¾ˆéš¾åœ¨ RL é¢†åŸŸå–å¾—è¿›å±•ã€‚æœ¬æ–‡æ—¨åœ¨æ¾„æ¸… MDP èƒŒåçš„éšæœºå’Œæ¦‚ç‡æ¦‚å¿µï¼Œé‡ç‚¹ç†è§£
    Suttonï¼ˆ2017ï¼‰ç¬¬3ç« ä¸­å‘ˆç°çš„æ–¹ç¨‹ã€‚
- en: The basic MDP framework as presented in Sutton and Barto (2017), Ch3, actually
    appears to be quite simple and intuitive â€” a sleight of hand achieved by a master!
    However, there is a huge backlog of statistical theory behind this framework.
    We donâ€™t need to know the depths of Stochastic Processes, because, believe me,
    itâ€™s a lot! But it is important to be aware of some of the subtleties underlying
    some of the concepts used in MDPs. Hence it is important to understand what are
    stochastic processes as the theory of these lies at the core of MDPs.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºæœ¬çš„ MDP æ¡†æ¶å¦‚ Sutton å’Œ Bartoï¼ˆ2017ï¼‰ï¼Œç¬¬3ç« ä¸­æ‰€å‘ˆç°çš„ï¼Œå®é™…ä¸Šçœ‹èµ·æ¥ç›¸å½“ç®€å•å’Œç›´è§‚â€”â€”è¿™æ˜¯ä¸€ä¸ªå¤§å¸ˆçº§çš„å·§å¦™æ‰‹æ³•ï¼ç„¶è€Œï¼Œè¿™ä¸ªæ¡†æ¶èƒŒåæœ‰å¤§é‡çš„ç»Ÿè®¡ç†è®ºã€‚æˆ‘ä»¬ä¸éœ€è¦äº†è§£éšæœºè¿‡ç¨‹çš„æ·±åº¦ï¼Œå› ä¸ºï¼Œç›¸ä¿¡æˆ‘ï¼ŒçœŸçš„å¾ˆå¤šï¼ä½†äº†è§£ä¸€äº›
    MDP ä¸­ä½¿ç”¨çš„æ¦‚å¿µèƒŒåçš„ç»†å¾®ä¹‹å¤„æ˜¯å¾ˆé‡è¦çš„ã€‚å› æ­¤ï¼Œç†è§£ä»€ä¹ˆæ˜¯éšæœºè¿‡ç¨‹å¾ˆé‡è¦ï¼Œå› ä¸ºè¿™äº›ç†è®ºæ˜¯ MDP çš„æ ¸å¿ƒã€‚
- en: This article is the first in a sequence of articles that will aim to make the
    fundamentals of RL clearer both at the theoretical and practical application level.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡æ˜¯ä¸€ä¸ªç³»åˆ—æ–‡ç« ä¸­çš„ç¬¬ä¸€ç¯‡ï¼Œæ—¨åœ¨ä½¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„åŸºç¡€çŸ¥è¯†åœ¨ç†è®ºå’Œå®é™…åº”ç”¨å±‚é¢ä¸Šæ›´åŠ æ¸…æ™°ã€‚
- en: '**Who is the intended audience?**'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç›®æ ‡å—ä¼—æ˜¯è°ï¼Ÿ**'
- en: Well, of course, it looks like it is for beginners, but I would say that this
    post is actually more useful to people who have some knowledge of RL and then
    try to understand the equations presented in Sutton (2017) Ch 3 more intuitively.
    This article is the result of my learning journey into RL. I started quite happily
    with Sutton and Barto (2017) and thought I had understood most of the concepts
    in Ch 3 for the MDP framework. However, once I tried to actually derive some of
    the equations presented in ch3, I found myself unable to figure them out. So,
    it became clear to me that possibly more was going on than I seemed to know about.
    There is no real resource available that I could find that relates MDPs to their
    underlying statistical underpinnings. I had to use a variety of material ranging
    from Wikipedia to various university level Stochastic theory courses. Given how
    scattered the underlying material is, I decided to document it in this post.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '**Approach â€” Decoding Sutton and Barto (2017)**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: The other thing is the format of this post. Sutton (2017) represents the bible
    of RL theory. The current edition is an updated version available online. But,
    much of theoretical material was actually published 20 years ago in the first
    edition of the book. Also, the approach taken in Ch3 of the book is the approach
    found across the internet. Given this, my approach in this post, is a bit like
    a â€˜keyâ€™ where I present material from the book and then explain it using the lens
    of stochastic theory.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: A slight digression in defense of â€˜keysâ€™. Legions of students, especially in
    school, use them extensively; whereas teachers actually frown on them for spoon
    feeding. However, when you are learning online and typically on your own, then
    a detailed guide becomes important, as you canâ€™t just put up your hand and ask
    a question.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Enough on the preamble! Letâ€™s move to the actual topicâ€¦.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '**Role of MDP**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'Understanding how the reinforcement learning problem works in the theoretical
    MDP environment is critical to getting the fundamentals of RL. Many important
    RL algorithms either are based on Markov based equations or models or represent
    some key departures from the Markov model. MDPs are a mathematically idealized
    form of the reinforcement learning process and provide a theoretical framework
    for an idealized type of reinforcement learning problem. Hence, no getting away
    from it: understanding MDPs is foundational to understanding the RL problem and
    how it is being addressed.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Be warned, Stochastic Theory is a particularly tough branch of probability theory
    and at times the notation used can be very intimidating. It shall be my attempt
    to bring the underlying stochastic concepts and ideas underlying MDPs more explicitly.
    Hopefully, the end result brings to the front the complexities that underlie the
    Markov model.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article I split up the content in 3 sections:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '**Section 1: Key Components of an MDP**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '- RL problem'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '- State, agent, environment, reward, etc.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '**Section 2: Probability and Stochastic Concepts Used in MDPs**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¬¬2èŠ‚ï¼šMDPä¸­ä½¿ç”¨çš„æ¦‚ç‡å’Œéšæœºæ¦‚å¿µ**'
- en: '- Random variables, state or sample space, stochastic processes, realization
    or trajectory'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '- éšæœºå˜é‡ã€çŠ¶æ€æˆ–æ ·æœ¬ç©ºé—´ã€éšæœºè¿‡ç¨‹ã€å®ç°æˆ–è½¨è¿¹'
- en: '- Probability distributions, joint distributions, marginal distributions, conditional
    distributions'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '- æ¦‚ç‡åˆ†å¸ƒã€è”åˆåˆ†å¸ƒã€è¾¹é™…åˆ†å¸ƒã€æ¡ä»¶åˆ†å¸ƒ'
- en: '- Deriving the Markov property'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '- æ¨å¯¼é©¬å°”å¯å¤«æ€§è´¨'
- en: '- Probability transition Matrix'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '- æ¦‚ç‡è½¬ç§»çŸ©é˜µ'
- en: '**Section 3: Understanding Sutton Ch3 equations Using Probability and Stochastic
    concepts**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¬¬3èŠ‚ï¼šä½¿ç”¨æ¦‚ç‡å’Œéšæœºæ¦‚å¿µç†è§£Sutton Ch3æ–¹ç¨‹**'
- en: '- The MDP Model'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '- MDPæ¨¡å‹'
- en: '- Formal presentation of MDP model'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '- MDPæ¨¡å‹çš„å½¢å¼åŒ–è¡¨ç¤º'
- en: '- Equations 3.2, 3.3, 3.4, 3.5, 3.6 from Ch3, Sutton (2017)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '- Sutton (2017) Ch3ä¸­çš„æ–¹ç¨‹3.2, 3.3, 3.4, 3.5, 3.6'
- en: 'This probably seems like just the introductory part of an MDP. You may well
    ask: â€˜what about value functions, policy, Q value function, Bellman equations?â€™.
    Well, that comes in the next article, as once I have gone through with the above
    contents, you will agree that this by itself it is a lot.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯èƒ½åªæ˜¯MDPçš„ä»‹ç»éƒ¨åˆ†ã€‚ä½ å¯èƒ½ä¼šé—®ï¼šâ€œé‚£ä»·å€¼å‡½æ•°ã€ç­–ç•¥ã€Qå€¼å‡½æ•°ã€è´å°”æ›¼æ–¹ç¨‹å‘¢ï¼Ÿâ€è¿™äº›å†…å®¹å°†åœ¨ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­ä»‹ç»ï¼Œå› ä¸ºä¸€æ—¦æˆ‘è®²è§£äº†ä¸Šè¿°å†…å®¹ï¼Œä½ ä¼šåŒæ„è¿™äº›å†…å®¹æœ¬èº«å°±å·²ç»å¾ˆå¤šäº†ã€‚
- en: '**Section 1: Key Components of an MDP**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¬¬1èŠ‚ï¼šMDPçš„å…³é”®ç»„æˆéƒ¨åˆ†**'
- en: First, to set context about what we are about to explain, I shall provide a
    brief summary of the basic MDP model and definitions used by [Sutton (2017), Ch3](http://www.incompleteideas.net/book/RLbook2020.pdf).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œä¸ºäº†è®¾ç½®æˆ‘ä»¬å³å°†è§£é‡Šçš„å†…å®¹çš„èƒŒæ™¯ï¼Œæˆ‘å°†ç®€è¦æ€»ç»“[Sutton (2017), Ch3](http://www.incompleteideas.net/book/RLbook2020.pdf)ä½¿ç”¨çš„åŸºæœ¬MDPæ¨¡å‹å’Œå®šä¹‰ã€‚
- en: '***Why MDP?***'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '***ä¸ºä»€ä¹ˆé€‰æ‹©MDPï¼Ÿ***'
- en: The major reason for the popularity of the MDP framework is due to the Markov
    property where future states depend only on the current state and not the history
    of states. Or, to put it another way, the current state encapsulates all the relevant
    information required for making decisions about the future states. By the way,
    this seemingly innocuous assumption actually has a wealth of probability theory
    behind it and hence also implications.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: MDPæ¡†æ¶å—æ¬¢è¿çš„ä¸»è¦åŸå› æ˜¯ç”±äºé©¬å°”å¯å¤«æ€§è´¨ï¼Œå…¶ä¸­æœªæ¥çŠ¶æ€ä»…ä¾èµ–äºå½“å‰çŠ¶æ€ï¼Œè€Œä¸ä¾èµ–äºçŠ¶æ€çš„å†å²ã€‚æ¢å¥è¯è¯´ï¼Œå½“å‰çŠ¶æ€å°è£…äº†åšå‡ºå…³äºæœªæ¥çŠ¶æ€å†³ç­–æ‰€éœ€çš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚é¡ºä¾¿è¯´ä¸€å¥ï¼Œè¿™ç§çœ‹ä¼¼æ— å®³çš„å‡è®¾å®é™…ä¸ŠèƒŒåæœ‰ä¸°å¯Œçš„æ¦‚ç‡ç†è®ºï¼Œå¹¶ä¸”ä¹Ÿæœ‰ç›¸åº”çš„å½±å“ã€‚
- en: Because of this property, an MDP represents a tractable way to formalize sequential
    decision making. It provides a probabilistic framework for modeling decision making
    in situations where outcomes are partly random and partly under the control of
    a decision maker. In this scenario, actions donâ€™t just have immediate payoffs
    in terms of rewards, but also affect subsequent time periods or states, and through
    them future rewards. Of course, the way reinforcement learning is done in practice
    is different from the MDP.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºè¿™ä¸ªç‰¹æ€§ï¼ŒMDPä»£è¡¨äº†ä¸€ç§å¯å¤„ç†çš„æ–¹å¼æ¥å½¢å¼åŒ–é¡ºåºå†³ç­–ã€‚å®ƒæä¾›äº†ä¸€ä¸ªæ¦‚ç‡æ¡†æ¶ï¼Œç”¨äºå»ºæ¨¡åœ¨ç»“æœéƒ¨åˆ†éšæœºå’Œéƒ¨åˆ†ç”±å†³ç­–è€…æ§åˆ¶çš„æƒ…å†µä¸‹çš„å†³ç­–ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¡ŒåŠ¨ä¸ä»…åœ¨å¥–åŠ±æ–¹é¢æœ‰ç›´æ¥çš„å›æŠ¥ï¼Œè¿˜ä¼šå½±å“åç»­æ—¶é—´æ®µæˆ–çŠ¶æ€ï¼Œä»è€Œå½±å“æœªæ¥çš„å¥–åŠ±ã€‚å½“ç„¶ï¼Œå¼ºåŒ–å­¦ä¹ åœ¨å®è·µä¸­çš„æ–¹å¼ä¸MDPæœ‰æ‰€ä¸åŒã€‚
- en: '***Some Definitions to start off with***'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '***ä¸€äº›å®šä¹‰ä»¥å¼€å§‹***'
- en: '**Agent:** This is the learner or decision maker who makes sequential decisions
    to achieve an end goal in an environment.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç†ï¼š**è¿™æ˜¯å­¦ä¹ è€…æˆ–å†³ç­–è€…ï¼Œåœ¨ç¯å¢ƒä¸­åšå‡ºé¡ºåºå†³ç­–ä»¥å®ç°æœ€ç»ˆç›®æ ‡ã€‚'
- en: '**Environment:** This is defined as everything that is outside the agentâ€™s
    locus of control. Hence the environment can be the external environment for a
    robot, it can be roads and pedestrians for a self -driving car, etc. In a video
    game, the environment is the game. The following points are important for the
    environment:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¯å¢ƒï¼š**è¿™è¢«å®šä¹‰ä¸ºä»£ç†æ§åˆ¶èŒƒå›´ä¹‹å¤–çš„æ‰€æœ‰äº‹ç‰©ã€‚å› æ­¤ï¼Œç¯å¢ƒå¯ä»¥æ˜¯æœºå™¨äººçš„å¤–éƒ¨ç¯å¢ƒï¼Œå¯ä»¥æ˜¯è‡ªé©¾è½¦çš„é“è·¯å’Œè¡Œäººç­‰ã€‚åœ¨è§†é¢‘æ¸¸æˆä¸­ï¼Œç¯å¢ƒå°±æ˜¯æ¸¸æˆã€‚ç¯å¢ƒçš„ä»¥ä¸‹å‡ ç‚¹æ˜¯é‡è¦çš„ï¼š'
- en: Observations are what the agent receives as input â€” for example a self-driving
    car can only receive inputs from the environment in its vicinity and will not
    be aware of a road block significantly ahead
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è§‚å¯Ÿæ˜¯ä»£ç†æ¥æ”¶åˆ°çš„è¾“å…¥â€”â€”ä¾‹å¦‚ï¼Œè‡ªé©¾è½¦åªèƒ½æ¥æ”¶æ¥è‡ªå…¶å‘¨å›´ç¯å¢ƒçš„è¾“å…¥ï¼Œè€Œä¸ä¼šæ„è¯†åˆ°å‰æ–¹å¾ˆè¿œå¤„çš„é“è·¯éšœç¢ã€‚
- en: The agent only has full knowledge of the environment if it knows the model of
    the environment, example the structure of rules of a video game which govern the
    environment
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åªæœ‰åœ¨çŸ¥é“ç¯å¢ƒæ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œä»£ç†æ‰èƒ½å®Œå…¨äº†è§£ç¯å¢ƒï¼Œä¾‹å¦‚æŒæ¡æ¸¸æˆè§„åˆ™çš„ç»“æ„ï¼Œè¿™äº›è§„åˆ™å†³å®šäº†ç¯å¢ƒã€‚
- en: The environment is changed by agentâ€™s actions (sudden breaking causes a small
    jam), but can also change on its own (a road diversion)
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¯å¢ƒä¼šå› ä»£ç†çš„è¡ŒåŠ¨è€Œæ”¹å˜ï¼ˆçªç„¶åˆ¹è½¦ä¼šé€ æˆå°å µå¡ï¼‰ï¼Œä½†ä¹Ÿå¯èƒ½è‡ªè¡Œå˜åŒ–ï¼ˆé“è·¯æ”¹é“ï¼‰ã€‚
- en: '***What is the RL problem?***'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '***ä»€ä¹ˆæ˜¯ RL é—®é¢˜ï¼Ÿ***'
- en: The typical RL problem involves a learning agent interacting over time with
    its environment to achieve a goal. To do this, a learning agent must be able to
    sense the state of its environment to some extent and must be able to take actions
    that affect the state. The fundamental problem is presented in this diagram in
    below.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å…¸å‹çš„ RL é—®é¢˜æ¶‰åŠä¸€ä¸ªå­¦ä¹ ä»£ç†åœ¨ä¸€æ®µæ—¶é—´å†…ä¸å…¶ç¯å¢ƒäº’åŠ¨ä»¥å®ç°ç›®æ ‡ã€‚ä¸ºæ­¤ï¼Œå­¦ä¹ ä»£ç†å¿…é¡»èƒ½å¤Ÿåœ¨ä¸€å®šç¨‹åº¦ä¸Šæ„ŸçŸ¥ç¯å¢ƒçš„çŠ¶æ€ï¼Œå¹¶ä¸”å¿…é¡»èƒ½å¤Ÿé‡‡å–å½±å“çŠ¶æ€çš„è¡ŒåŠ¨ã€‚è¿™ä¸ªåŸºæœ¬é—®é¢˜åœ¨ä¸‹é¢çš„å›¾ä¸­å±•ç¤ºã€‚
- en: '![](../Images/6586539f97d6dff700ae8f57dbde2125.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6586539f97d6dff700ae8f57dbde2125.png)'
- en: 'Agent environment interaction in a Markov decision process (Image source: Created
    by author, inspired by Sutton (2017) Ch3)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä¸­ï¼Œä»£ç†å’Œç¯å¢ƒçš„äº¤äº’ï¼ˆå›¾åƒæ¥æºï¼šä½œè€…åˆ›ä½œï¼Œçµæ„Ÿæ¥æºäº Sutton (2017) ç¬¬ä¸‰ç« ï¼‰
- en: '***The measurement of time***'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '***æ—¶é—´çš„æµ‹é‡***'
- en: 'Time is typically measured in discrete steps: t=0,1,2,..T. Where, T represents
    the final or terminal state. Hence it is a *finite discrete time* problem. There
    are also continuous time problems and also infinite horizon problems, but these
    are a lot tougher to handle mathematically. As a result the focus is on the finite
    discrete time model.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¶é—´é€šå¸¸ä»¥ç¦»æ•£æ­¥éª¤æ¥æµ‹é‡ï¼št=0,1,2,..Tã€‚å…¶ä¸­ï¼ŒT ä»£è¡¨æœ€ç»ˆæˆ–ç»ˆç«¯çŠ¶æ€ã€‚å› æ­¤ï¼Œè¿™æ˜¯ä¸€ä¸ª*æœ‰é™ç¦»æ•£æ—¶é—´*é—®é¢˜ã€‚è¿˜æœ‰è¿ç»­æ—¶é—´é—®é¢˜ä»¥åŠæ— é™è§†ç•Œé—®é¢˜ï¼Œä½†è¿™äº›åœ¨æ•°å­¦ä¸Šå¤„ç†èµ·æ¥æ›´ä¸ºå¤æ‚ã€‚å› æ­¤ï¼Œé‡ç‚¹åœ¨äºæœ‰é™ç¦»æ•£æ—¶é—´æ¨¡å‹ã€‚
- en: '***State***'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '***çŠ¶æ€***'
- en: This represents a snapshot of the environment and contains all the information
    pertaining to the environment. This includes information on rewards, how the environment
    will change with respect to actions, etc. In the case of a self-driving car, a
    state may represent a region around the car. Similarly, in the case of a robot
    moving forward, it would represent a snapshot of the environment once an action
    such as stepping forward has been taken.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä»£è¡¨äº†ç¯å¢ƒçš„ä¸€ä¸ªå¿«ç…§ï¼ŒåŒ…å«äº†æ‰€æœ‰ä¸ç¯å¢ƒç›¸å…³çš„ä¿¡æ¯ã€‚è¿™åŒ…æ‹¬å¥–åŠ±ä¿¡æ¯ã€ç¯å¢ƒå¦‚ä½•æ ¹æ®è¡ŒåŠ¨å‘ç”Ÿå˜åŒ–ç­‰ã€‚ä»¥è‡ªåŠ¨é©¾é©¶æ±½è½¦ä¸ºä¾‹ï¼ŒçŠ¶æ€å¯èƒ½ä»£è¡¨æ±½è½¦å‘¨å›´çš„ä¸€ä¸ªåŒºåŸŸã€‚ç±»ä¼¼åœ°ï¼Œä»¥å‰è¿›ä¸­çš„æœºå™¨äººä¸ºä¾‹ï¼Œå®ƒå°†ä»£è¡¨åœ¨é‡‡å–å¦‚å‰è¿›ç­‰è¡ŒåŠ¨åç¯å¢ƒçš„å¿«ç…§ã€‚
- en: 'For Markov purposes, the agent and environment interact in a series of discrete
    time steps: t=0,1,2â€¦T. Where, T represents the final or terminal state. At each
    time step, t, the agent receives some representation of the environment encapsulated
    as Sâ‚œ âˆˆ **S**,where **S** is the set of all states. In the context of MDPs, State
    is a discrete random variable.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºé©¬å°”å¯å¤«ç›®çš„ï¼Œä»£ç†å’Œç¯å¢ƒåœ¨ä¸€ç³»åˆ—ç¦»æ•£æ—¶é—´æ­¥éª¤ä¸­äº¤äº’ï¼št=0,1,2â€¦Tã€‚å…¶ä¸­ï¼ŒT ä»£è¡¨æœ€ç»ˆæˆ–ç»ˆç«¯çŠ¶æ€ã€‚åœ¨æ¯ä¸ªæ—¶é—´æ­¥éª¤ tï¼Œä»£ç†æ¥æ”¶ç¯å¢ƒçš„æŸç§è¡¨ç¤ºï¼Œå°è£…ä¸º
    Sâ‚œ âˆˆ **S**ï¼Œå…¶ä¸­ **S** æ˜¯æ‰€æœ‰çŠ¶æ€çš„é›†åˆã€‚åœ¨ MDP çš„èƒŒæ™¯ä¸‹ï¼ŒçŠ¶æ€æ˜¯ä¸€ä¸ªç¦»æ•£éšæœºå˜é‡ã€‚
- en: '***Rewards***'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '***å¥–åŠ±***'
- en: The agent receives a signal or reward Râ‚œâ‚Šâ‚ from the environment in the next
    time period which is associated with how well the action is doing in terms of
    achieving the overall goal. The reward is typically defined as a scalar where
    Râ‚œ âˆˆ ***R*** âŠ‚â„ is just the set of real numbers (example of complicated notation
    for something simple! But thatâ€™s the way things happen in statsworldğŸ˜Š).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£ç†åœ¨ä¸‹ä¸€æ—¶é—´å‘¨æœŸä»ç¯å¢ƒä¸­æ¥æ”¶åˆ°ä¸€ä¸ªä¿¡å·æˆ–å¥–åŠ± Râ‚œâ‚Šâ‚ï¼Œè¿™ä¸è¡ŒåŠ¨åœ¨å®ç°æ•´ä½“ç›®æ ‡æ–¹é¢çš„è¡¨ç°ç›¸å…³ã€‚å¥–åŠ±é€šå¸¸å®šä¹‰ä¸ºä¸€ä¸ªæ ‡é‡ï¼Œå…¶ä¸­ Râ‚œ âˆˆ ***R***
    âŠ‚â„ åªæ˜¯å®æ•°é›†åˆçš„ä¸€ä¸ªä¾‹å­ï¼ˆå¤æ‚çš„ç¬¦å·è¡¨ç¤ºç®€å•çš„ä¸œè¥¿ï¼ä½†åœ¨ç»Ÿè®¡ä¸–ç•Œä¸­å°±æ˜¯è¿™æ ·ğŸ˜Šï¼‰ã€‚
- en: Rewards can be both deterministic or a random variable. Sutton (2017) takes
    Râ‚œ to be a random variable, though most of the examples take a deterministic reward.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å¥–åŠ±å¯ä»¥æ˜¯ç¡®å®šæ€§çš„ï¼Œä¹Ÿå¯ä»¥æ˜¯éšæœºå˜é‡ã€‚Suttonï¼ˆ2017ï¼‰å°† Râ‚œ è§†ä¸ºéšæœºå˜é‡ï¼Œå°½ç®¡å¤§å¤šæ•°ç¤ºä¾‹é‡‡ç”¨ç¡®å®šæ€§å¥–åŠ±ã€‚
- en: Now, Râ‚œ depends on the current state of the world, the action just taken, and
    the next state of the world, i.e., Râ‚œ = R(Sâ‚œ, Aâ‚œ, Sâ‚œâ‚Šâ‚). However, its frequently
    simplified to depend on the current state and action pair, Râ‚œ = R(Sâ‚œ, Aâ‚œ).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼ŒRâ‚œ ä¾èµ–äºä¸–ç•Œçš„å½“å‰çŠ¶æ€ã€åˆšåˆšé‡‡å–çš„è¡ŒåŠ¨å’Œä¸–ç•Œçš„ä¸‹ä¸€ä¸ªçŠ¶æ€ï¼Œå³ Râ‚œ = R(Sâ‚œ, Aâ‚œ, Sâ‚œâ‚Šâ‚)ã€‚ç„¶è€Œï¼Œå®ƒé€šå¸¸è¢«ç®€åŒ–ä¸ºä»…ä¾èµ–äºå½“å‰çŠ¶æ€å’Œè¡ŒåŠ¨å¯¹ï¼Œå³
    Râ‚œ = R(Sâ‚œ, Aâ‚œ)ã€‚
- en: Now that we have the basic components of the MDP in place, i.e., State, Reward,
    Agent, Environment, we need to better understand the nature of the MDP variables
    State, Reward, Agent, Environment in the context of probability and stochastic
    theory.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»æœ‰äº† MDP çš„åŸºæœ¬ç»„æˆéƒ¨åˆ†ï¼Œå³çŠ¶æ€ã€å¥–åŠ±ã€ä»£ç†ã€ç¯å¢ƒï¼Œæˆ‘ä»¬éœ€è¦æ›´å¥½åœ°ç†è§£åœ¨æ¦‚ç‡å’Œéšæœºç†è®ºèƒŒæ™¯ä¸‹ï¼ŒMDP å˜é‡çŠ¶æ€ã€å¥–åŠ±ã€ä»£ç†å’Œç¯å¢ƒçš„æ€§è´¨ã€‚
- en: So, letâ€™s now jump to Probability and Stochastic theoryâ€¦
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬è·³åˆ°æ¦‚ç‡å’Œéšæœºç†è®ºâ€¦â€¦
- en: '**Section 2: Probability and Stochastic Basics**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: To define stochastic processes, we need to first understand what are random
    variables, probability distributions, joint distributions, marginal distribution,
    conditional distributions and the chain rule of probability.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Let us now spend some time understanding the above concepts. Once we understand
    these, we are going to come back to MDPs and understand how it works using these
    concepts.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Since stats and probability is vast, I would like to keep the focus on the applicability
    of these concepts to MDPs. So, I present the statistical concepts related to an
    MDP concept in *italics* and then explain its relevance to the MDP. Hopefully,
    this will make the MDP concept clearer.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '***Random variable***'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '*In probability, a random variable is a way of capturing the fact that a variable
    can take multiple values or occurrences. It represents a mapping or function that
    captures the fact that the variable can be mapped to many values. For example
    a toss of a coin has outcomes: {H,T} and we can map this to a number lying between
    0 and 1 signifying probability. See this* [*Wikpedia*](https://en.wikipedia.org/wiki/Random_variable)
    *page for a more formal definition. As an aside, I have found the Wikipedia pages
    for probability and statistics to be an excellent, if slightly advanced, resource.*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '*When the range of the random variable is countable, it is termed discrete
    and its distribution is a discrete probability mass function ( a concept that
    I will define later as it will be used a lot). If the variable is continuous,
    then its distribution can be defined by a probability distribution function.*'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '*The definition of a random variable is traditionally defined for real valued
    cases, denoted by* **R***. The definition can also be extended to any measurable
    set E which contains random Boolean values or categorical values, vectors, matrices
    or functions.*'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: This becomes important to note because in MDPs we have real valued random variables
    as well as categorical types. Also stochastic processes (to be defined a little
    later) are also a random function of time.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: In the context of the MDP defined above, we have State and Reward as random
    variables. Both variables are discrete and have a finite range.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '***State space or sample space:***'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '*When we talk of a random number, then the no. of finite number of states that
    a random variable can take is termed as the sample state or state space in the
    case of an MDP. The simplest example of a sample space is that for a throw of
    coin. It can take one of two values: heads or tails.*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 'In Sutton ch3, this is denoted by the set ***S*** which contains different
    states, where each state can be representative of a different situation. For example,
    we can have a stochastic process for going to office. The list of states that
    ***S*** could potentially contain:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '1\. sâ€™: meets a jam on the way to commuting to office'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '2\. s: Meets clear roads to office'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '3\. sâ€: Meets a road block due to an accident'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: These states are what constitute the sample space or range of a random variable,
    i.e., the values it can take.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›çŠ¶æ€æ„æˆäº†æ ·æœ¬ç©ºé—´æˆ–éšæœºå˜é‡çš„èŒƒå›´ï¼Œå³å®ƒå¯ä»¥å–çš„å€¼ã€‚
- en: '***Stochastic Process*:**'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '***éšæœºè¿‡ç¨‹***ï¼š'
- en: '*A stochastic process is a collection or ensemble of random variables indexed
    by a variable t, usually representing time. So, to be clear, a stochastic process
    comprises the same random variable at multiple points of time. The stochastic
    process is typically indexed by the time variable (it can also be vector space,
    but letâ€™s not confuse things). Thus, each index or point of time is associated
    by a specific random variable. Read more about stochastic processes on this* [*Wikipedia*](https://en.wikipedia.org/wiki/Stochastic_process)
    *page.*'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*éšæœºè¿‡ç¨‹æ˜¯ç”±å˜é‡ tï¼ˆé€šå¸¸ä»£è¡¨æ—¶é—´ï¼‰ç´¢å¼•çš„éšæœºå˜é‡çš„é›†åˆæˆ–é›†æˆã€‚æ˜ç¡®åœ°è¯´ï¼Œéšæœºè¿‡ç¨‹åŒ…æ‹¬åœ¨å¤šä¸ªæ—¶é—´ç‚¹ä¸Šçš„ç›¸åŒéšæœºå˜é‡ã€‚éšæœºè¿‡ç¨‹é€šå¸¸ç”±æ—¶é—´å˜é‡ç´¢å¼•ï¼ˆå®ƒä¹Ÿå¯ä»¥æ˜¯å‘é‡ç©ºé—´ï¼Œä½†ä¸ºäº†é¿å…æ··æ·†ï¼Œæˆ‘ä»¬æš‚ä¸”ä¸æï¼‰ã€‚å› æ­¤ï¼Œæ¯ä¸ªç´¢å¼•æˆ–æ—¶é—´ç‚¹éƒ½ä¸ç‰¹å®šçš„éšæœºå˜é‡ç›¸å…³è”ã€‚äº†è§£æ›´å¤šå…³äºéšæœºè¿‡ç¨‹çš„ä¿¡æ¯ï¼Œè¯·è®¿é—®è¿™ä¸ª*
    [*ç»´åŸºç™¾ç§‘*](https://en.wikipedia.org/wiki/Stochastic_process) *é¡µé¢ã€‚*'
- en: '*This means that at every point of time, the stochastic random variable can
    realize one of the values in* ***S****.**A stochastic process can also be written
    as: S(t,i), t*âˆˆT. *Where the State is a function of both, time as indexed by t,
    and i representing the specific state from the state space* ***S.***'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿™æ„å‘³ç€åœ¨æ¯ä¸ªæ—¶é—´ç‚¹ï¼Œéšæœºå˜é‡å¯ä»¥å®ç°* ***S****.**ä¸­çš„ä¸€ä¸ªå€¼ã€‚éšæœºè¿‡ç¨‹ä¹Ÿå¯ä»¥å†™ä½œï¼šS(t,i)ï¼Œt*âˆˆTã€‚*å…¶ä¸­çŠ¶æ€æ˜¯æ—¶é—´ï¼ˆç”± t ç´¢å¼•ï¼‰å’Œä»£è¡¨çŠ¶æ€ç©ºé—´***S***çš„ç‰¹å®šçŠ¶æ€
    i çš„å‡½æ•°ã€‚*'
- en: The state space is defined using elements that reflect the different values
    that the stochastic process can take. So, in the case of MDPs, a state, Sâ‚œ , is
    a stochastic random variable over time, indexed by t, where for each t, Sâ‚œ can
    take values in the finite set ***S =***{s,sâ€™,sâ€™â€™â€™}.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: çŠ¶æ€ç©ºé—´æ˜¯é€šè¿‡åæ˜ éšæœºè¿‡ç¨‹å¯ä»¥å–çš„ä¸åŒå€¼çš„å…ƒç´ æ¥å®šä¹‰çš„ã€‚å› æ­¤ï¼Œåœ¨ MDP çš„æƒ…å†µä¸‹ï¼ŒçŠ¶æ€ Sâ‚œ æ˜¯ä¸€ä¸ªéšæ—¶é—´å˜åŒ–çš„éšæœºå˜é‡ï¼Œç”± t ç´¢å¼•ï¼Œå…¶ä¸­å¯¹äºæ¯ä¸ª
    tï¼ŒSâ‚œ å¯ä»¥å–å€¼äºæœ‰é™é›†åˆ ***S =***{s,sâ€™,sâ€™â€™â€™}ã€‚
- en: 'The point of confusion regarding stochastic processes is that often the second
    index is skipped to avoid messy presentations. However, without it, the clarity
    that there are actually 2 things going on is lost:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºéšæœºè¿‡ç¨‹çš„å›°æƒ‘åœ¨äºï¼Œé€šå¸¸è·³è¿‡ç¬¬äºŒä¸ªç´¢å¼•ä»¥é¿å…æ··ä¹±çš„å±•ç¤ºã€‚ç„¶è€Œï¼Œæ²¡æœ‰å®ƒçš„è¯ï¼Œå®é™…æœ‰ 2 ä»¶äº‹æƒ…æ­£åœ¨å‘ç”Ÿçš„æ¸…æ™°åº¦ä¼šä¸§å¤±ï¼š
- en: 1\. Movement overtime indexed by t as the variable evolves
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. éšç€å˜é‡ t çš„æ¼”å˜ï¼Œéšæ—¶é—´æ¨ç§»çš„è¿åŠ¨
- en: 2\. At each point of time a choice of state from the set of states
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. åœ¨æ¯ä¸ªæ—¶é—´ç‚¹ä¸Šï¼Œä»çŠ¶æ€é›†åˆä¸­é€‰æ‹©ä¸€ä¸ªçŠ¶æ€
- en: Let us continue with our example of the road to illustrate the distinction between
    the stochastic process evolving over time and different state spaces. There are
    a finite number of state spaces which the stochastic variable can take at a point
    of time. So, for example, when we head out to office we can potentially meet with
    clear roads or a jam on the way for the first 15 minutes. In the next 15 minutes,
    the situation may change. Suppose that it requires 45 minutes to get to office,
    then we can break it up into 3 different time slots of 15 minutes. So, our time
    periods can be t=1,2,3.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç»§ç»­ä»¥é“è·¯ä¸ºä¾‹ï¼Œä»¥è¯´æ˜éšæœºè¿‡ç¨‹éšæ—¶é—´æ¼”å˜å’Œä¸åŒçŠ¶æ€ç©ºé—´ä¹‹é—´çš„åŒºåˆ«ã€‚åœ¨æŸä¸ªæ—¶é—´ç‚¹ä¸Šï¼Œéšæœºå˜é‡å¯ä»¥å–æœ‰é™æ•°é‡çš„çŠ¶æ€ç©ºé—´ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬å‰å¾€åŠå…¬å®¤æ—¶ï¼Œåœ¨å‰
    15 åˆ†é’Ÿå†…ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šé‡åˆ°æ¸…æ™°çš„é“è·¯æˆ–å µè½¦ã€‚æ¥ä¸‹æ¥çš„ 15 åˆ†é’Ÿï¼Œæƒ…å†µå¯èƒ½ä¼šå‘ç”Ÿå˜åŒ–ã€‚å‡è®¾åˆ°è¾¾åŠå…¬å®¤éœ€è¦ 45 åˆ†é’Ÿï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥å°†å…¶åˆ†ä¸º 3 ä¸ª 15
    åˆ†é’Ÿçš„æ—¶é—´æ®µã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ—¶é—´æ®µå¯ä»¥æ˜¯ t=1,2,3ã€‚
- en: There is a probability p of clear road, q of meeting a jam and r of meeting
    a road block in the first 15 minutes. In the next 15 minutes, one of these 3 outcomes
    can again occur and similarly for the last 15 minutes. This is evolution over
    time. This means that, at each observation at a certain time, there is a certain
    probability to get a certain outcome. This can be illustrated as below.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‰ 15 åˆ†é’Ÿå†…ï¼Œé“è·¯æ¸…æ™°çš„æ¦‚ç‡ä¸º pï¼Œé‡åˆ°å µè½¦çš„æ¦‚ç‡ä¸º qï¼Œé‡åˆ°é“è·¯å°é—­çš„æ¦‚ç‡ä¸º rã€‚åœ¨æ¥ä¸‹æ¥çš„ 15 åˆ†é’Ÿå†…ï¼Œè¿™ä¸‰ç§ç»“æœä¸­çš„ä¸€ç§å¯èƒ½å†æ¬¡å‡ºç°ï¼Œæœ€å
    15 åˆ†é’Ÿä¹Ÿæ˜¯å¦‚æ­¤ã€‚è¿™å°±æ˜¯éšæ—¶é—´æ¼”å˜ã€‚è¿™æ„å‘³ç€ï¼Œåœ¨æ¯ä¸ªæ—¶é—´ç‚¹ä¸Šï¼Œå­˜åœ¨è·å¾—æŸä¸ªç»“æœçš„ç‰¹å®šæ¦‚ç‡ã€‚å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è¿›è¡Œè¯´æ˜ã€‚
- en: '![](../Images/e72ab0a6813c0777f742c6e210871e7a.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e72ab0a6813c0777f742c6e210871e7a.png)'
- en: 'Office drive MDP (Image source: author)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: åŠå…¬å®¤é©±åŠ¨ MDPï¼ˆå›¾åƒæ¥æºï¼šä½œè€…ï¼‰
- en: The image shows a sample trajectory. At time t=1, i.e., first 15 minutes, the
    road can be clear, jammed or blocked with respective probabilities. Similarly
    for t=2 or second 15 minutes and so on. At a point of time, say, first 15 minutes,
    we can see the road state can vary randomly across 3 possible states. This represents
    the randomness in state at a point of time. However, if we look across time t,
    then we have a trajectory. So, if we have 3 time points, we will also have 3 random
    variables, one associated with each point of time.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒæ˜¾ç¤ºäº†ä¸€ä¸ªæ ·æœ¬è½¨è¿¹ã€‚åœ¨æ—¶é—´t=1ï¼Œå³å‰15åˆ†é’Ÿï¼Œé“è·¯å¯èƒ½æ˜¯ç•…é€šçš„ã€æ‹¥å µçš„æˆ–å°é—­çš„ï¼Œå„æœ‰ç›¸åº”çš„æ¦‚ç‡ã€‚ç±»ä¼¼åœ°ï¼Œå¯¹äºt=2æˆ–ç¬¬äºŒä¸ª15åˆ†é’Ÿï¼Œä»¥æ­¤ç±»æ¨ã€‚åœ¨æŸä¸€æ—¶åˆ»ï¼Œæ¯”å¦‚å‰15åˆ†é’Ÿï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°é“è·¯çŠ¶æ€å¯ä»¥éšæœºå˜åŒ–äº3ç§å¯èƒ½çŠ¶æ€ã€‚è¿™ä»£è¡¨äº†åœ¨æŸä¸€æ—¶åˆ»çŠ¶æ€çš„éšæœºæ€§ã€‚ç„¶è€Œï¼Œå¦‚æœæˆ‘ä»¬çœ‹æ—¶é—´tï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±æœ‰äº†ä¸€ä¸ªè½¨è¿¹ã€‚æ‰€ä»¥ï¼Œå¦‚æœæˆ‘ä»¬æœ‰3ä¸ªæ—¶é—´ç‚¹ï¼Œæˆ‘ä»¬å°†æœ‰3ä¸ªéšæœºå˜é‡ï¼Œæ¯ä¸ªæ—¶é—´ç‚¹å¯¹åº”ä¸€ä¸ªã€‚
- en: We can also understand the reward set in this context. The reward set **R**
    in this case would be the time taken to cover the specific segment of the road
    at a particular time. There is a defined distribution of time (for simplicity
    we can keep it as discrete minutes) for each state. What does this mean? Basically,
    each state of the road would be associated with a distribution of time. Itâ€™s like
    sometimes we do the clear road (s) in 15 minutes and sometimes in 17 minutes due
    to some random variations and, at others in 18 minutes. So, we can define R(s)={15,16,17,18}.
    There is a probability distribution defined for the time taken to complete the
    segment with clear road conditions. This is an intuitive example of how the reward
    can also be random â€” something which most of the examples in Sutton (2017), Ch3
    do not cover. Each state of the road â€” for example, the jammed state â€” would have
    its own distribution over the ETA.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å¯ä»¥åœ¨è¿™ç§æƒ…å†µä¸‹ç†è§£å¥–åŠ±é›†ã€‚å¥–åŠ±é›†**R**åœ¨è¿™ç§æƒ…å†µä¸‹å°±æ˜¯åœ¨ç‰¹å®šæ—¶é—´è¦†ç›–ç‰¹å®šé“è·¯æ®µæ‰€éœ€çš„æ—¶é—´ã€‚æ¯ç§çŠ¶æ€éƒ½æœ‰ä¸€ä¸ªå®šä¹‰çš„æ—¶é—´åˆ†å¸ƒï¼ˆä¸ºäº†ç®€åŒ–æˆ‘ä»¬å¯ä»¥ä»¥ç¦»æ•£åˆ†é’Ÿä¸ºå•ä½ï¼‰ã€‚è¿™æ„å‘³ç€ä»€ä¹ˆï¼ŸåŸºæœ¬ä¸Šï¼Œé“è·¯çš„æ¯ç§çŠ¶æ€éƒ½ä¸æ—¶é—´åˆ†å¸ƒç›¸å…³ã€‚å°±åƒæœ‰æ—¶æˆ‘ä»¬åœ¨15åˆ†é’Ÿå†…å®Œæˆæ¸…æ™°é“è·¯
    (s)ï¼Œæœ‰æ—¶ç”±äºä¸€äº›éšæœºå˜åŒ–åœ¨17åˆ†é’Ÿï¼Œæœ‰æ—¶åœ¨18åˆ†é’Ÿã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰R(s)={15,16,17,18}ã€‚å¯¹äºæ¸…æ™°é“è·¯æ¡ä»¶ä¸‹å®Œæˆè·¯æ®µæ‰€éœ€æ—¶é—´æœ‰ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒã€‚è¿™æ˜¯å¥–åŠ±ä¹Ÿå¯èƒ½æ˜¯éšæœºçš„ä¸€ä¸ªç›´è§‚ç¤ºä¾‹â€”â€”è¿™æ˜¯Sutton
    (2017)ï¼ŒCh3ä¸­çš„å¤§å¤šæ•°ç¤ºä¾‹æ²¡æœ‰æ¶µç›–çš„å†…å®¹ã€‚é“è·¯çš„æ¯ç§çŠ¶æ€â€”â€”ä¾‹å¦‚ï¼Œæ‹¥å µçŠ¶æ€â€”â€”å°†æœ‰å…¶è‡ªèº«çš„ETAåˆ†å¸ƒã€‚
- en: '***Realization or trajectory of a stochastic process***'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '***éšæœºè¿‡ç¨‹çš„å®ç°æˆ–è½¨è¿¹***'
- en: '*A stochastic process can have many* [*outcomes*](https://en.wikipedia.org/wiki/Outcome_(probability))*,
    due to its randomness. A single outcome of a stochastic process is called variously
    as a* [***realization***](https://en.wikipedia.org/wiki/Stochastic_process)***,
    episode or a trajectory****. It is formed by taking a single possible value of
    the random variable at each time point of the stochastic process as it evolves
    over time.*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*éšæœºè¿‡ç¨‹å¯ä»¥æœ‰è®¸å¤š* [*ç»“æœ*](https://en.wikipedia.org/wiki/Outcome_(probability))*, ç”±äºå…¶éšæœºæ€§ã€‚éšæœºè¿‡ç¨‹çš„å•ä¸ªç»“æœè¢«ç§°ä¸º*
    [***å®ç°***](https://en.wikipedia.org/wiki/Stochastic_process)***ã€æƒ…èŠ‚æˆ–è½¨è¿¹****ã€‚å®ƒæ˜¯é€šè¿‡åœ¨éšæœºè¿‡ç¨‹çš„æ¯ä¸ªæ—¶é—´ç‚¹ä¸Šå–éšæœºå˜é‡çš„ä¸€ä¸ªå¯èƒ½å€¼æ¥å½¢æˆçš„ã€‚*'
- en: 'So, to continue with our traffic example: a possible trajectory is shown in
    the image by the arrows. In the first 15 minutes, the motorist meets a clear road,
    in the next 15 minutes hits a jam and in the final 15 minutes hits a blocked road.
    So, a potential sample path would be:(s,sâ€™,sâ€™â€™). This is just one possible realization.
    The motorist traverses this path every day and is likely to meet a slightly different
    set of states every day; another day it can be (sâ€™â€™,sâ€™, s).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œç»§ç»­æˆ‘ä»¬çš„äº¤é€šç¤ºä¾‹ï¼šå›¾åƒä¸­çš„ç®­å¤´æ˜¾ç¤ºäº†ä¸€ä¸ªå¯èƒ½çš„è½¨è¿¹ã€‚åœ¨å‰15åˆ†é’Ÿï¼Œå¸æœºé‡åˆ°äº†ä¸€æ¡ç•…é€šçš„é“è·¯ï¼Œåœ¨æ¥ä¸‹æ¥çš„15åˆ†é’Ÿé‡åˆ°äº†æ‹¥å µï¼Œè€Œåœ¨æœ€å15åˆ†é’Ÿé‡åˆ°äº†å°é—­çš„é“è·¯ã€‚æ‰€ä»¥ï¼Œä¸€ä¸ªæ½œåœ¨çš„æ ·æœ¬è·¯å¾„å¯ä»¥æ˜¯ï¼š(s,sâ€™,sâ€™â€™)ã€‚è¿™åªæ˜¯ä¸€ä¸ªå¯èƒ½çš„å®ç°ã€‚å¸æœºæ¯å¤©éƒ½ç»å†è¿™æ¡è·¯å¾„ï¼Œå¹¶ä¸”å¾ˆå¯èƒ½æ¯å¤©ä¼šé‡åˆ°ç•¥å¾®ä¸åŒçš„çŠ¶æ€é›†ï¼›å¦ä¸€å¤©å¯èƒ½æ˜¯
    (sâ€™â€™,sâ€™,s)ã€‚
- en: '*Stochastic processes are widely used for probabilistic representations of
    systems and phenomena that appear to vary in a random manner. Or, to put it more
    simply, stochastic processes or models are used to estimate the probability of
    various outcomes of a random variable which also changes overtime. Examples include
    the growth of a* [*bacterial*](https://en.wikipedia.org/wiki/Bacteria) *population,
    or the movement of a* [*gas*](https://en.wikipedia.org/wiki/Gas)[*molecule*](https://en.wikipedia.org/wiki/Molecule)*.
    They are also used extensively in financial analysis where stochastic models can
    be used to estimate situations involving uncertainty, such as investment returns,
    volatile markets, or inflation rates*'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*éšæœºè¿‡ç¨‹è¢«å¹¿æ³›åº”ç”¨äºå¯¹ç³»ç»Ÿå’Œç°è±¡çš„æ¦‚ç‡è¡¨ç¤ºï¼Œè¿™äº›ç³»ç»Ÿå’Œç°è±¡è¡¨ç°ä¸ºéšæœºå˜åŒ–ã€‚ç®€å•æ¥è¯´ï¼Œéšæœºè¿‡ç¨‹æˆ–æ¨¡å‹ç”¨äºä¼°ç®—éšæ—¶é—´å˜åŒ–çš„éšæœºå˜é‡çš„å„ç§ç»“æœçš„æ¦‚ç‡ã€‚ä¾‹å­åŒ…æ‹¬*
    [*ç»†èŒ*](https://en.wikipedia.org/wiki/Bacteria) *ç¾¤ä½“çš„å¢é•¿ï¼Œæˆ–* [*æ°”ä½“*](https://en.wikipedia.org/wiki/Gas)[*åˆ†å­*](https://en.wikipedia.org/wiki/Molecule)*çš„è¿åŠ¨ã€‚å®ƒä»¬åœ¨é‡‘èåˆ†æä¸­ä¹Ÿè¢«å¹¿æ³›ä½¿ç”¨ï¼Œå…¶ä¸­éšæœºæ¨¡å‹å¯ç”¨äºä¼°ç®—æ¶‰åŠä¸ç¡®å®šæ€§çš„æƒ…å†µï¼Œå¦‚æŠ•èµ„å›æŠ¥ã€å¸‚åœºæ³¢åŠ¨æˆ–é€šè´§è†¨èƒ€ç‡*'
- en: '*Further there are many different types of stochastic processes of which the
    Markov process is one type. Others, for example, include random walks, Gaussian
    processes, etc. Each stochastic process has its own specific assumptions and features
    which it is important to understand.*'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ­¤å¤–ï¼Œè¿˜æœ‰è®¸å¤šä¸åŒç±»å‹çš„éšæœºè¿‡ç¨‹ï¼Œå…¶ä¸­ Markov è¿‡ç¨‹æ˜¯ä¸€ç§ã€‚å…¶ä»–ç±»å‹åŒ…æ‹¬éšæœºæ¸¸èµ°ã€é«˜æ–¯è¿‡ç¨‹ç­‰ã€‚æ¯ç§éšæœºè¿‡ç¨‹éƒ½æœ‰å…¶ç‰¹å®šçš„å‡è®¾å’Œç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾å¾ˆé‡è¦ã€‚*'
- en: '***Probability Distribution***'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '***æ¦‚ç‡åˆ†å¸ƒ***'
- en: Now we turn to the question of how do we get the probabilities of different
    states? Recall, in above example, we have defined the probabilities of meeting
    a jam and having clear road as q and p. Now, let us just elaborate a bit about
    what this means in terms of probability distributions and how they come about.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬è½¬å‘å¦‚ä½•è·å–ä¸åŒçŠ¶æ€çš„æ¦‚ç‡ï¼Ÿå›é¡¾ä¸Šé¢çš„ä¾‹å­ï¼Œæˆ‘ä»¬å·²ç»å®šä¹‰äº†é‡åˆ°æ‹¥å µå’Œæ¸…æ™°é“è·¯çš„æ¦‚ç‡åˆ†åˆ«ä¸º q å’Œ pã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬ç¨å¾®è¯¦ç»†è¯´æ˜ä¸€ä¸‹è¿™åœ¨æ¦‚ç‡åˆ†å¸ƒä¸­çš„å«ä¹‰ä»¥åŠå®ƒä»¬æ˜¯å¦‚ä½•äº§ç”Ÿçš„ã€‚
- en: The probability of a random outcome (road is jammed) is assessed as the proportion
    of times the outcome would occur in a very long series of repetitions. So, this
    is how we would assess p, q, r by noting the state of road variable at the specific
    time intervals specified across multiple days.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: éšæœºç»“æœï¼ˆé“è·¯æ‹¥å µï¼‰çš„æ¦‚ç‡è¢«è¯„ä¼°ä¸ºåœ¨éå¸¸é•¿çš„é‡å¤åºåˆ—ä¸­è¯¥ç»“æœå‘ç”Ÿçš„æ¯”ä¾‹ã€‚å› æ­¤ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬å¦‚ä½•é€šè¿‡è®°å½•å¤šä¸ªå¤©æ•°ä¸­çš„é“è·¯çŠ¶æ€å˜é‡åœ¨ç‰¹å®šæ—¶é—´é—´éš”çš„çŠ¶æ€æ¥è¯„ä¼°
    pã€qã€rã€‚
- en: '*Recording all these probabilities of outputs of a random variable,* Sâ‚œ , *gives
    us the probability distribution of the random variable. In the discrete case it
    is termed the probability mass distribution.* *This is a function that provides
    the probability that a discrete random variable equals a particular value. Read
    more of this* [*here*](https://en.wikipedia.org/wiki/Probability_distribution)*.*'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*è®°å½•éšæœºå˜é‡çš„æ‰€æœ‰è¿™äº›è¾“å‡ºæ¦‚ç‡*ï¼ŒSâ‚œï¼Œ*ç»™æˆ‘ä»¬æä¾›äº†éšæœºå˜é‡çš„æ¦‚ç‡åˆ†å¸ƒã€‚åœ¨ç¦»æ•£æƒ…å†µä¸‹ï¼Œè¿™è¢«ç§°ä¸ºæ¦‚ç‡è´¨é‡åˆ†å¸ƒã€‚* *è¿™æ˜¯ä¸€ä¸ªæä¾›ç¦»æ•£éšæœºå˜é‡ç­‰äºç‰¹å®šå€¼çš„æ¦‚ç‡çš„å‡½æ•°ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…*
    [*è¿™é‡Œ*](https://en.wikipedia.org/wiki/Probability_distribution)*ã€‚*'
- en: 'To continue with our office commute example: the State variable, can therefore
    take any of these values with a specific probability. For example if our state
    set for a road consists of 3 possible states: **S**= { s = clear, sâ€™=jammed, sâ€™â€™=blocked}.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ç»§ç»­æˆ‘ä»¬åŠå…¬å®¤é€šå‹¤çš„ä¾‹å­ï¼šçŠ¶æ€å˜é‡å› æ­¤å¯ä»¥ä»¥ç‰¹å®šçš„æ¦‚ç‡å–è¿™äº›å€¼ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬ä¸ºä¸€æ¡é“è·¯å®šä¹‰çš„çŠ¶æ€é›†åŒ…å« 3 ç§å¯èƒ½çš„çŠ¶æ€ï¼š**S**= { s = æ¸…æ™°,
    sâ€™= æ‹¥å µ, sâ€™â€™= é˜»å¡}ã€‚
- en: The road can have any of these 3 states at a specific point of time (say morning)
    with a probability. For example, it can be free =.5, jammed =.4, and blocked =.1(blocking
    is relatively rarer state!). The set of probabilities is termed the probability
    distribution for the variable which is the state of the road.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç‰¹å®šæ—¶é—´ç‚¹ï¼ˆæ¯”å¦‚æ—©æ™¨ï¼‰ï¼Œé“è·¯å¯ä»¥æœ‰è¿™ä¸‰ç§çŠ¶æ€ä¸­çš„ä»»ä½•ä¸€ç§ï¼Œä¸”å…·æœ‰ç›¸åº”çš„æ¦‚ç‡ã€‚ä¾‹å¦‚ï¼Œå®ƒå¯ä»¥æ˜¯è‡ªç”± =0.5ï¼Œæ‹¥å µ =0.4ï¼Œé˜»å¡ =0.1ï¼ˆé˜»å¡ç›¸å¯¹è¾ƒå°‘è§ï¼ï¼‰ã€‚è¿™äº›æ¦‚ç‡çš„é›†åˆè¢«ç§°ä¸ºè¯¥å˜é‡çš„æ¦‚ç‡åˆ†å¸ƒï¼Œå³é“è·¯çš„çŠ¶æ€ã€‚
- en: '***Joint Distribution vs Marginal Distribution vs Conditional Distribution***'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '***è”åˆåˆ†å¸ƒ vs è¾¹é™…åˆ†å¸ƒ vs æ¡ä»¶åˆ†å¸ƒ***'
- en: '*In case of stochastic processes, since the random variable is the same but
    measured over different points of time, it is likely that state in t,* Sâ‚œ , *is
    correlated to states in previous periods,* Sâ‚œâ‚‹â‚, Sâ‚œâ‚‹â‚‚. *In this case, then, if
    we want to understand the probability of* Sâ‚œ *, it is the joint probability mass
    distribution (pmf) that is relevant. The joint pmf allows us to compute probabilities
    of events involving multiple random variables taking into account the relationship
    between the variables.*'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '*åœ¨éšæœºè¿‡ç¨‹çš„æƒ…å†µä¸‹ï¼Œç”±äºéšæœºå˜é‡ç›¸åŒä½†åœ¨ä¸åŒæ—¶é—´ç‚¹æµ‹é‡ï¼Œå› æ­¤çŠ¶æ€åœ¨ t,* Sâ‚œ , *å¯èƒ½ä¸ä¹‹å‰çš„çŠ¶æ€æœ‰å…³ï¼Œå³* Sâ‚œâ‚‹â‚, Sâ‚œâ‚‹â‚‚ã€‚*åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¦‚æœæˆ‘ä»¬æƒ³äº†è§£*
    Sâ‚œ *çš„æ¦‚ç‡ï¼Œåˆ™ç›¸å…³çš„æ˜¯è”åˆæ¦‚ç‡è´¨é‡åˆ†å¸ƒ (pmf)ã€‚è”åˆ pmf å…è®¸æˆ‘ä»¬è®¡ç®—æ¶‰åŠå¤šä¸ªéšæœºå˜é‡çš„äº‹ä»¶çš„æ¦‚ç‡ï¼ŒåŒæ—¶è€ƒè™‘å˜é‡ä¹‹é—´çš„å…³ç³»ã€‚*'
- en: '*Given two* [*random variables*](https://en.wikipedia.org/wiki/Random_variable)
    *that are defined on the same* [*probability space*](https://en.wikipedia.org/wiki/Probability_space)
    *the* [***joint probability distribution***](https://en.wikipedia.org/wiki/Joint_probability_distribution)
    *is the corresponding* [*probability distribution*](https://en.wikipedia.org/wiki/Probability_distribution)
    *on all possible pairs of outputs.*'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '*ç»™å®šä¸¤ä¸ª* [*éšæœºå˜é‡*](https://en.wikipedia.org/wiki/Random_variable) *å®šä¹‰åœ¨ç›¸åŒçš„* [*æ¦‚ç‡ç©ºé—´*](https://en.wikipedia.org/wiki/Probability_space)
    *ä¸Šï¼Œ* [***è”åˆæ¦‚ç‡åˆ†å¸ƒ***](https://en.wikipedia.org/wiki/Joint_probability_distribution)
    *æ˜¯æ‰€æœ‰å¯èƒ½çš„è¾“å‡ºå¯¹çš„ç›¸åº”* [*æ¦‚ç‡åˆ†å¸ƒ*](https://en.wikipedia.org/wiki/Probability_distribution)
    *ã€‚*'
- en: '*This can be defined more generally in Markov context as:*'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿™å¯ä»¥æ›´ä¸€èˆ¬åœ°åœ¨é©¬å°”å¯å¤«èƒŒæ™¯ä¸‹å®šä¹‰ä¸ºï¼š*'
- en: '*Suppose we consider a discrete time 2 period stochastic process,* Sâ‚™ *, where
    n=1,2\. So,* Sâ‚ and Sâ‚‚ *are the two discrete stochastic random variables which
    can take values from the set i* âˆˆ S = {1,2}. *The joint distribution of* Sâ‚™ is
    given for *every n, i.e., time periods and finite sequence of states:(*iâ‚,iâ‚‚)
    by:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*å‡è®¾æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªç¦»æ•£æ—¶é—´ 2 æ—¶é—´æ®µéšæœºè¿‡ç¨‹ï¼Œ* Sâ‚™ *, å…¶ä¸­ n=1,2ã€‚*æ‰€ä»¥ï¼Œ* Sâ‚ å’Œ Sâ‚‚ *æ˜¯ä¸¤ä¸ªç¦»æ•£éšæœºå˜é‡ï¼Œå¯ä»¥ä»é›†åˆ i*
    âˆˆ S = {1,2} ä¸­å–å€¼ã€‚* Sâ‚™ çš„è”åˆåˆ†å¸ƒç»™å‡ºæ¯ä¸ª nï¼Œå³æ—¶é—´æ®µå’Œæœ‰é™çŠ¶æ€åºåˆ—çš„ï¼š(iâ‚,iâ‚‚) ä¸ºï¼š'
- en: '![](../Images/3ee11bfd7563334600b682307bfac7e8.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ee11bfd7563334600b682307bfac7e8.png)'
- en: 'Let us look at our simple road example:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬çš„ç®€å•é“è·¯ç¤ºä¾‹ï¼š
- en: 'For simplicity assume that there are only 2 time periods â€” i.e., Sâ‚œ and Sâ‚œâ‚‹â‚.
    The state space is still 3: {clear( C ), jammed(J), blocked(B)}.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºç®€å•èµ·è§ï¼Œå‡è®¾åªæœ‰ 2 ä¸ªæ—¶é—´æ®µï¼Œå³ Sâ‚œ å’Œ Sâ‚œâ‚‹â‚ã€‚çŠ¶æ€ç©ºé—´ä»ç„¶æ˜¯ 3ï¼š{clear(C)ï¼Œjammed(J)ï¼Œblocked(B)}ã€‚
- en: 'The joint probability table would be as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: è”åˆæ¦‚ç‡è¡¨å¦‚ä¸‹ï¼š
- en: '![](../Images/b744db17148155938ea05c72804862fe.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b744db17148155938ea05c72804862fe.png)'
- en: 2 period office drive joint probability distribution ( source:author)
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 2 æ—¶é—´æ®µåŠå…¬å®¤é©¾é©¶è”åˆæ¦‚ç‡åˆ†å¸ƒï¼ˆæ¥æºï¼šä½œè€…ï¼‰
- en: The joint probability is the probability of two events occurring together. The
    table above provides the different combinations of instances of both events. Notice,
    since this is a stochastic process, time t-1 always comes first and then t.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: è”åˆæ¦‚ç‡æ˜¯ä¸¤ä¸ªäº‹ä»¶åŒæ—¶å‘ç”Ÿçš„æ¦‚ç‡ã€‚ä¸Šè¡¨æä¾›äº†ä¸¤ä¸ªäº‹ä»¶çš„ä¸åŒå®ä¾‹ç»„åˆã€‚æ³¨æ„ï¼Œç”±äºè¿™æ˜¯ä¸€ä¸ªéšæœºè¿‡ç¨‹ï¼Œæ—¶é—´ t-1 æ€»æ˜¯å…ˆäº tã€‚
- en: '***Marginal probability distribution***'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '***è¾¹é™…æ¦‚ç‡åˆ†å¸ƒ***'
- en: '*The* [*marginal distribution*](https://en.wikipedia.org/wiki/Joint_probability_distribution)
    *of a variable gives the probabilities of various values of the variables in the
    subset without reference to the values of the other variables. The marginal mass
    function for* Sáµ¢ is:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*å˜é‡çš„* [*è¾¹é™…åˆ†å¸ƒ*](https://en.wikipedia.org/wiki/Joint_probability_distribution)
    *ç»™å‡ºäº†åœ¨å­é›†ä¸­å„ç§å€¼çš„æ¦‚ç‡ï¼Œè€Œä¸å‚è€ƒå…¶ä»–å˜é‡çš„å€¼ã€‚Sáµ¢ çš„è¾¹é™…è´¨é‡å‡½æ•°ä¸ºï¼š*'
- en: '![](../Images/7ed0b4f566946e349449eeb2156a2566.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ed0b4f566946e349449eeb2156a2566.png)'
- en: The marginal probability is the probability of a single event occurring. Essentially
    the marginal distribution is the column wise sum of probabilities. The equation
    for this requires you to sum over the column for j. The joint probability for
    any combination is given by the cell probabilities. In our exampleâ€™s context,
    it is the probability of meeting a jam in Sâ‚œ irrespective of what road conditions
    were in Sâ‚œâ‚‹â‚, i.e., we sum the probabilities across the col J.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: è¾¹é™…æ¦‚ç‡æ˜¯å•ä¸ªäº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡ã€‚æœ¬è´¨ä¸Šï¼Œè¾¹é™…åˆ†å¸ƒæ˜¯æŒ‰åˆ—çš„æ¦‚ç‡æ€»å’Œã€‚è¿™ä¸ªæ–¹ç¨‹è¦æ±‚ä½ å¯¹ j åˆ—è¿›è¡Œæ±‚å’Œã€‚ä»»ä½•ç»„åˆçš„è”åˆæ¦‚ç‡ç”±å•å…ƒæ ¼æ¦‚ç‡ç»™å‡ºã€‚åœ¨æˆ‘ä»¬ç¤ºä¾‹çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œå®ƒæ˜¯
    Sâ‚œ ä¸­é‡åˆ°äº¤é€šå µå¡çš„æ¦‚ç‡ï¼Œè€Œä¸è€ƒè™‘ Sâ‚œâ‚‹â‚ ä¸­çš„è·¯å†µï¼Œå³æˆ‘ä»¬å¯¹ J åˆ—çš„æ¦‚ç‡è¿›è¡Œæ±‚å’Œã€‚
- en: '***Conditional probability***'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '***æ¡ä»¶æ¦‚ç‡***'
- en: '*A* [*conditional probability*](https://en.wikipedia.org/wiki/Conditional_probability_distribution)
    *is that a specific event occurs given that another specific event has already
    occurred. This means the probability of a jam in* Sâ‚œ *given that the road was
    clear in* Sâ‚œâ‚‹â‚.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '*ä¸€ä¸ª* [*æ¡ä»¶æ¦‚ç‡*](https://en.wikipedia.org/wiki/Conditional_probability_distribution)
    *æ˜¯æŒ‡åœ¨å¦ä¸€ä¸ªç‰¹å®šäº‹ä»¶å·²ç»å‘ç”Ÿçš„æ¡ä»¶ä¸‹ï¼ŒæŸä¸€ç‰¹å®šäº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡ã€‚è¿™æ„å‘³ç€åœ¨* Sâ‚œ *çš„æƒ…å†µä¸‹ï¼Œç»™å®šé“è·¯åœ¨* Sâ‚œâ‚‹â‚ *æ—¶æ˜¯ç•…é€šçš„æƒ…å†µä¸‹å‘ç”Ÿå µå¡çš„æ¦‚ç‡ã€‚*'
- en: Now in our case, because Sâ‚œ is a stochastic variable, and we are looking at
    a two period example, the joint and conditional probabilities are the same.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œç”±äº Sâ‚œ æ˜¯ä¸€ä¸ªéšæœºå˜é‡ï¼Œå¹¶ä¸”æˆ‘ä»¬è€ƒè™‘çš„æ˜¯ä¸€ä¸ªä¸¤ä¸ªæ—¶æœŸçš„ä¾‹å­ï¼Œæ‰€ä»¥è”åˆæ¦‚ç‡å’Œæ¡ä»¶æ¦‚ç‡æ˜¯ç›¸åŒçš„ã€‚
- en: '*Mathematically, the conditional distribution of a variable given another variable
    is the joint distribution of both variables divided by the marginal distribution
    of the other variable.*'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*åœ¨æ•°å­¦ä¸Šï¼Œç»™å®šå¦ä¸€ä¸ªå˜é‡çš„æ¡ä»¶ä¸‹çš„å˜é‡åˆ†å¸ƒæ˜¯ä¸¤ä¸ªå˜é‡çš„è”åˆåˆ†å¸ƒé™¤ä»¥å¦ä¸€ä¸ªå˜é‡çš„è¾¹é™…åˆ†å¸ƒã€‚*'
- en: '*Defining this generally it is:*'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '*ä¸€èˆ¬è€Œè¨€ï¼Œå®šä¹‰ä¸ºï¼š*'
- en: '![](../Images/dde8178697faf0a7231e62d4bf85bd21.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dde8178697faf0a7231e62d4bf85bd21.png)'
- en: '*The generalization of this to case where we have n random variables,* Xâ‚,..,
    Xâ‚™.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*å°†å…¶æ¨å¹¿åˆ°æˆ‘ä»¬æœ‰ n ä¸ªéšæœºå˜é‡çš„æƒ…å†µï¼Œ* Xâ‚,.., Xâ‚™ã€‚'
- en: '*The joint probability distribution of these n variables is:*'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿™ n ä¸ªå˜é‡çš„è”åˆæ¦‚ç‡åˆ†å¸ƒæ˜¯ï¼š*'
- en: '![](../Images/2d293c3740f6f14ecbe1a7a3ee91fb29.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d293c3740f6f14ecbe1a7a3ee91fb29.png)'
- en: '*This can be written as the conditional function times the marginal function.
    This is based on the* [*chain rule of probability*](https://en.wikipedia.org/wiki/Chain_rule_(probability))*.
    In terms of notation,* Pâ‚“ *(*xâ‚) = P(X=xâ‚). *For the 2 random variables,* Xâ‚,
    Xâ‚‚ *the joint distribution can be written as:*'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿™å¯ä»¥å†™æˆæ¡ä»¶å‡½æ•°ä¹˜ä»¥è¾¹é™…å‡½æ•°ã€‚è¿™æ˜¯åŸºäº* [*æ¦‚ç‡é“¾å¼æ³•åˆ™*](https://en.wikipedia.org/wiki/Chain_rule_(probability))
    *çš„ã€‚åœ¨ç¬¦å·è¡¨ç¤ºä¸Šï¼Œ* Pâ‚“ *ï¼ˆ*xâ‚ï¼‰= P(X=xâ‚)ã€‚* å¯¹äºä¸¤ä¸ªéšæœºå˜é‡* Xâ‚, Xâ‚‚ *ï¼Œå…¶è”åˆåˆ†å¸ƒå¯ä»¥è¡¨ç¤ºä¸ºï¼š*'
- en: '![](../Images/831a66b22750e3ca7f6ad95e7465229c.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/831a66b22750e3ca7f6ad95e7465229c.png)'
- en: '*Stated in words: the joint probability distribution of 2 variables* Xâ‚, Xâ‚‚
    *taking specific values* xâ‚, xâ‚‚ *can be given by the conditional probability of*
    Xâ‚‚=xâ‚‚ *given that* Xâ‚=xâ‚, *times the marginal probability of getting* Xâ‚=xâ‚.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '*ç”¨è¨€è¯­è¡¨è¿°ï¼šä¸¤ä¸ªå˜é‡* Xâ‚, Xâ‚‚ *å–ç‰¹å®šå€¼* xâ‚, xâ‚‚ *çš„è”åˆæ¦‚ç‡åˆ†å¸ƒå¯ä»¥è¡¨ç¤ºä¸ºï¼š* Xâ‚‚=xâ‚‚ *åœ¨ç»™å®š* Xâ‚=xâ‚ *çš„æ¡ä»¶ä¸‹çš„æ¡ä»¶æ¦‚ç‡ï¼Œä¹˜ä»¥*
    Xâ‚=xâ‚ *çš„è¾¹é™…æ¦‚ç‡ã€‚*'
- en: '***Why do we need these distributions â€” deriving the need for the Markov Property***'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '***æˆ‘ä»¬ä¸ºä»€ä¹ˆéœ€è¦è¿™äº›åˆ†å¸ƒâ€”â€”æ¨å¯¼Markovæ€§è´¨çš„éœ€æ±‚***'
- en: Joint, conditional and marginal probability distribution definitions become
    important in the context of MDPs when we want to determine the probabilities of
    the State variable taking specific values as it transitions across different time
    periods. Our example is simple and consists of 3 time periods. But, what if the
    time period was 100, and we wanted to understand the probability of the state
    taking a specific value in the 100th time period? This is the underlying probability
    problem. Letâ€™s look at how the general case is solved and then what the Markov
    property is
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: è”åˆã€æ¡ä»¶å’Œè¾¹é™…æ¦‚ç‡åˆ†å¸ƒå®šä¹‰åœ¨MDPï¼ˆé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼‰èƒŒæ™¯ä¸‹å˜å¾—é‡è¦ï¼Œå½“æˆ‘ä»¬æƒ³ç¡®å®šçŠ¶æ€å˜é‡åœ¨ä¸åŒæ—¶é—´æ®µè¿‡æ¸¡æ—¶å–ç‰¹å®šå€¼çš„æ¦‚ç‡æ—¶ã€‚æˆ‘ä»¬çš„ä¾‹å­å¾ˆç®€å•ï¼Œç”±ä¸‰ä¸ªæ—¶é—´æ®µç»„æˆã€‚ä½†æ˜¯ï¼Œå¦‚æœæ—¶é—´æ®µæ˜¯100ï¼Œæˆ‘ä»¬æƒ³äº†è§£çŠ¶æ€åœ¨ç¬¬100ä¸ªæ—¶é—´æ®µå–ç‰¹å®šå€¼çš„æ¦‚ç‡æ€ä¹ˆåŠï¼Ÿè¿™å°±æ˜¯æ½œåœ¨çš„æ¦‚ç‡é—®é¢˜ã€‚è®©æˆ‘ä»¬çœ‹çœ‹ä¸€èˆ¬æƒ…å†µå¦‚ä½•è§£å†³ï¼Œç„¶åäº†è§£Markovæ€§è´¨ã€‚
- en: '*For the more general case, to get the joint probability of* Xâ‚,.., Xâ‚™ *random
    variables taking specific values,* xâ‚,.., xâ‚™ *can be found by the conditional
    probability of* Xâ‚™ *given the entire history of previous random variables taking
    specific values, times the joint probability distribution of the n-1th distribution.*'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¯¹äºæ›´ä¸€èˆ¬çš„æƒ…å†µï¼Œè¦è·å¾—* Xâ‚,.., Xâ‚™ *éšæœºå˜é‡å–ç‰¹å®šå€¼* xâ‚,.., xâ‚™ *çš„è”åˆæ¦‚ç‡ï¼Œå¯ä»¥é€šè¿‡* Xâ‚™ *ç»™å®šä¹‹å‰æ‰€æœ‰éšæœºå˜é‡å–ç‰¹å®šå€¼çš„æ¡ä»¶æ¦‚ç‡ï¼Œä¹˜ä»¥å‰
    n-1 æ¬¡åˆ†å¸ƒçš„è”åˆæ¦‚ç‡åˆ†å¸ƒæ¥æ‰¾åˆ°ã€‚*'
- en: '![](../Images/32c29c24137d4886665587e377268d89.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/32c29c24137d4886665587e377268d89.png)'
- en: '*We can open out the expression for joint probability of P(*Xâ‚™â‚‹â‚=xâ‚™â‚‹â‚â€¦,Xâ‚=xâ‚)
    *again into an expression for conditional probability of* Xâ‚™â‚‹â‚ *given the history
    of (*Xâ‚™â‚‹â‚‚,â€¦,Xâ‚) *and the joint probability of (*Xâ‚,..,Xâ‚™â‚‹â‚‚).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*æˆ‘ä»¬å¯ä»¥å°† P(*Xâ‚™â‚‹â‚=xâ‚™â‚‹â‚â€¦,Xâ‚=xâ‚)* çš„è”åˆæ¦‚ç‡è¡¨è¾¾å¼å†æ¬¡å±•å¼€ä¸º* Xâ‚™â‚‹â‚ *ç»™å®šï¼ˆ*Xâ‚™â‚‹â‚‚,â€¦,Xâ‚*ï¼‰çš„æ¡ä»¶æ¦‚ç‡å’Œï¼ˆ*Xâ‚,..,Xâ‚™â‚‹â‚‚*ï¼‰çš„è”åˆæ¦‚ç‡ã€‚*'
- en: '*This can be further unrolled backwards. We will finally get the expression
    for a sequence of conditional probabilities and the marginal probability of the
    first variable P(*Xâ‚=xâ‚)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿™å¯ä»¥è¿›ä¸€æ­¥å‘åå±•å¼€ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å°†å¾—åˆ°æ¡ä»¶æ¦‚ç‡åºåˆ—å’Œç¬¬ä¸€ä¸ªå˜é‡ P(*Xâ‚=xâ‚) çš„è¾¹é™…æ¦‚ç‡çš„è¡¨è¾¾å¼ã€‚*'
- en: '![](../Images/91af8fb4cd7f3e847a7d8ded58ae7481.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91af8fb4cd7f3e847a7d8ded58ae7481.png)'
- en: Now let us translate this to the MDP context. So far, this has been couched
    in terms of any sequence of random variables. However, this result translates
    easily to stochastic variables and Markov processes. We just need to replace Xáµ¢
    by Sâ‚œ and now the indexing would be in terms of time, t=1,2,..n instead of Xáµ¢
    = xáµ¢.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬å°†å…¶ç¿»è¯‘åˆ°MDPçš„èƒŒæ™¯ä¸­ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œè¿™å·²ç»ä»¥ä»»ä½•éšæœºå˜é‡åºåˆ—çš„å½¢å¼å‘ˆç°ã€‚ç„¶è€Œï¼Œè¿™ä¸ªç»“æœå¾ˆå®¹æ˜“è½¬åŒ–ä¸ºéšæœºå˜é‡å’Œé©¬å°”å¯å¤«è¿‡ç¨‹ã€‚æˆ‘ä»¬åªéœ€å°†Xáµ¢æ›¿æ¢ä¸ºSâ‚œï¼Œç°åœ¨ç´¢å¼•å°†ä»¥æ—¶é—´çš„å½¢å¼å‡ºç°ï¼Œå³t=1,2,..nï¼Œè€Œä¸æ˜¯Xáµ¢
    = xáµ¢ã€‚
- en: Why am I belaboring the obvious? Because there is a lot of material out there
    that uses different variables and indices for Markov processes and other sequences
    of random variables and this can get hugely confusing. I tried to unravel this
    for myself and hence am writing it down, so that it can help other non-statisticians
    who are trying to figure out the probability theory behind Markov results.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¸ºä»€ä¹ˆè¦é‡å¤è¿™äº›æ˜æ˜¾çš„å†…å®¹ï¼Ÿå› ä¸ºæœ‰å¾ˆå¤šææ–™ä½¿ç”¨ä¸åŒçš„å˜é‡å’Œç´¢å¼•æ¥æè¿°é©¬å°”å¯å¤«è¿‡ç¨‹å’Œå…¶ä»–éšæœºå˜é‡åºåˆ—ï¼Œè¿™å¯èƒ½ä¼šéå¸¸æ··ä¹±ã€‚æˆ‘å°è¯•ä¸ºè‡ªå·±è§£å¼€è¿™äº›å¤æ‚çš„æ¦‚å¿µï¼Œå› æ­¤å†™ä¸‹è¿™äº›å†…å®¹ï¼Œä»¥å¸®åŠ©é‚£äº›è¯•å›¾å¼„æ¸…æ¥šé©¬å°”å¯å¤«ç»“æœèƒŒåçš„æ¦‚ç‡ç†è®ºçš„éç»Ÿè®¡å­¦å®¶ã€‚
- en: For analyzing the MDP, t=1,..,n and states xáµ¢ âˆˆ S. Just to be clear xâ‚™ is a
    specific value of a state at time period t=n.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åˆ†æMDPæ—¶ï¼Œt=1,..,nï¼ŒçŠ¶æ€xáµ¢ âˆˆ Sã€‚ä¸ºäº†æ˜ç¡®ï¼Œxâ‚™æ˜¯æ—¶é—´æ®µt=næ—¶çš„ä¸€ä¸ªç‰¹å®šçŠ¶æ€å€¼ã€‚
- en: Letâ€™s write the joint probability of getting Sâ‚,..,Sâ‚™ as the state variable
    evolves over t=1..n. Substituting in the general case discussed above we get the
    joint probability as a sequence of conditional probabilities and the marginal
    distribution of Sâ‚.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å†™å‡ºçŠ¶æ€å˜é‡åœ¨t=1..nçš„æ¼”å˜è¿‡ç¨‹ä¸­ï¼Œè·å¾—Sâ‚,..,Sâ‚™çš„è”åˆæ¦‚ç‡ã€‚ä»£å…¥ä¸Šè¿°è®¨è®ºçš„ä¸€èˆ¬æƒ…å†µï¼Œæˆ‘ä»¬å¾—åˆ°çš„è”åˆæ¦‚ç‡æ˜¯æ¡ä»¶æ¦‚ç‡çš„åºåˆ—å’ŒSâ‚çš„è¾¹é™…åˆ†å¸ƒã€‚
- en: '![](../Images/e219768fe3b276680ffb69ef9a7f5725.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e219768fe3b276680ffb69ef9a7f5725.png)'
- en: 'To put it in words: the probability of getting states Sâ‚,..,Sâ‚™ with specific
    values is equal to the conditional probability of the state at time t=n having
    a value xâ‚™ given the historical values of previous states times the joint distribution
    of the n-1th period. This unrolls into a sequence of conditional probabilities
    and the marginal or initial distribution of state at Sâ‚.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨è¯­è¨€æè¿°å°±æ˜¯ï¼šè·å¾—çŠ¶æ€Sâ‚,..,Sâ‚™çš„æ¦‚ç‡ç­‰äºåœ¨æ—¶é—´t=næ—¶çŠ¶æ€çš„å€¼ä¸ºxâ‚™çš„æ¡ä»¶æ¦‚ç‡ï¼Œä¹˜ä»¥å‰n-1æœŸçš„è”åˆåˆ†å¸ƒã€‚è¿™å±•å¼€ä¸ºä¸€ç³»åˆ—æ¡ä»¶æ¦‚ç‡å’ŒçŠ¶æ€Sâ‚çš„è¾¹é™…æˆ–åˆå§‹åˆ†å¸ƒã€‚
- en: Now this is just a stochastic process where the value of a state in time period
    n depends on the past history of state values. This is clearly intractable to
    compute, hence the Markov property
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è¿™åªæ˜¯ä¸€ä¸ªéšæœºè¿‡ç¨‹ï¼Œå…¶ä¸­çŠ¶æ€åœ¨æ—¶é—´æ®µnçš„å€¼ä¾èµ–äºè¿‡å»çŠ¶æ€å€¼çš„å†å²ã€‚è¿™æ˜¾ç„¶æ˜¯éš¾ä»¥è®¡ç®—çš„ï¼Œå› æ­¤éœ€è¦é©¬å°”å¯å¤«æ€§è´¨ã€‚
- en: '***Markov Property:***'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '***é©¬å°”å¯å¤«æ€§è´¨:***'
- en: 'The Markov property makes a state Sâ‚™ depend on only the immediately preceding
    state, Sâ‚™â‚‹â‚, rather than the entire historical backlog of states. In this case
    our conditional probability for state Sâ‚™ reduces to:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: é©¬å°”å¯å¤«æ€§è´¨ä½¿å¾—çŠ¶æ€Sâ‚™åªä¾èµ–äºç´§æ¥å‰ä¸€ä¸ªçŠ¶æ€Sâ‚™â‚‹â‚ï¼Œè€Œä¸æ˜¯æ•´ä¸ªå†å²çŠ¶æ€çš„ç§¯ç´¯ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„çŠ¶æ€Sâ‚™çš„æ¡ä»¶æ¦‚ç‡å‡å°‘ä¸ºï¼š
- en: '![](../Images/fa4686a7d953c456ec5748e0a61e7e2e.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa4686a7d953c456ec5748e0a61e7e2e.png)'
- en: However, we still have n-1 one step conditional probabilities to be estimated
    along with the initial probability distribution, *P(*Xâ‚=xâ‚). Still somewhat complex,
    as there are a large number of conditional probabilities to be calculated. A further
    assumption that is made to simplify and make the calculations more tractable is
    time homogeneity of transition probabilities.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæˆ‘ä»¬ä»ç„¶éœ€è¦ä¼°è®¡n-1ä¸ªä¸€æ­¥æ¡ä»¶æ¦‚ç‡ä»¥åŠåˆå§‹æ¦‚ç‡åˆ†å¸ƒï¼Œ*P(*Xâ‚=xâ‚)ã€‚è¿™ä»ç„¶æ¯”è¾ƒå¤æ‚ï¼Œå› ä¸ºéœ€è¦è®¡ç®—å¤§é‡çš„æ¡ä»¶æ¦‚ç‡ã€‚ä¸ºäº†ç®€åŒ–å¹¶ä½¿è®¡ç®—æ›´å¯è¡Œï¼Œè¿›ä¸€æ­¥çš„å‡è®¾æ˜¯è¿‡æ¸¡æ¦‚ç‡çš„æ—¶é—´åŒè´¨æ€§ã€‚
- en: '***Time homogeneity of transition probabilities***'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '***è¿‡æ¸¡æ¦‚ç‡çš„æ—¶é—´åŒè´¨æ€§***'
- en: '*This basically says that the probability of transitioning from state i in
    time period n to state j in period n+1 is the same irrespective of the time periods:*'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿™åŸºæœ¬ä¸Šè¯´çš„æ˜¯ï¼šä»æ—¶é—´æ®µnçš„çŠ¶æ€iè¿‡æ¸¡åˆ°æ—¶é—´æ®µn+1çš„çŠ¶æ€jçš„æ¦‚ç‡æ˜¯ç›¸åŒçš„ï¼Œä¸å—æ—¶é—´æ®µçš„å½±å“ï¼š*'
- en: '![](../Images/7252b0a97eedde66e56d3017a4a6eaa1.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7252b0a97eedde66e56d3017a4a6eaa1.png)'
- en: '*What this means is that the probabilities for transitioning from one state
    to the next is fixed, irrespective of time.*'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿™æ„å‘³ç€ä»ä¸€ä¸ªçŠ¶æ€è¿‡æ¸¡åˆ°ä¸‹ä¸€ä¸ªçŠ¶æ€çš„æ¦‚ç‡æ˜¯å›ºå®šçš„ï¼Œä¸å—æ—¶é—´çš„å½±å“ã€‚*'
- en: To put it in context of our road example, the probability of going from clear
    road to jammed is fixed irrespective of whether we meet these 2 states in the
    first 2 time periods or the last 2 time periods. So, the probability of transition
    reduces to the following 9 cases irrespective of time. Where C= clear, j=jammed
    and B=blockled.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¾åˆ°æˆ‘ä»¬çš„é“è·¯ç¤ºä¾‹ä¸­ï¼Œä»æ¸…æ™°é“è·¯åˆ°æ‹¥å µé“è·¯çš„æ¦‚ç‡æ˜¯å›ºå®šçš„ï¼Œæ— è®ºæˆ‘ä»¬åœ¨å‰ 2 ä¸ªæ—¶é—´æ®µè¿˜æ˜¯æœ€å 2 ä¸ªæ—¶é—´æ®µé‡åˆ°è¿™ 2 ä¸ªçŠ¶æ€ã€‚å› æ­¤ï¼Œè½¬ç§»æ¦‚ç‡ç®€åŒ–ä¸ºä»¥ä¸‹
    9 ç§æƒ…å†µï¼Œä¸æ—¶é—´æ— å…³ã€‚å…¶ä¸­ C=æ¸…æ™°ï¼Œj=æ‹¥å µï¼ŒB=å°å µã€‚
- en: '![](../Images/607a0cf49440b45aa8727f0ec37ec842.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/607a0cf49440b45aa8727f0ec37ec842.png)'
- en: As you can see, if the number of states is less than the time period, as is
    more likely, this will significantly reduce the probabilities to be calculated.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œå¦‚æœçŠ¶æ€çš„æ•°é‡å°‘äºæ—¶é—´å‘¨æœŸï¼Œé€šå¸¸è¿™ç§æƒ…å†µä¼šæ˜¾è‘—å‡å°‘éœ€è¦è®¡ç®—çš„æ¦‚ç‡ã€‚
- en: '***Transition probabilities***'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '***è½¬ç§»æ¦‚ç‡***'
- en: '*The state* [*transition probability matrix*](https://en.wikipedia.org/wiki/Discrete-time_Markov_chain)
    *of a Markov chain gives the probabilities of transitioning from one state to
    another in a single time step.*'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '*çŠ¶æ€* [*è½¬ç§»æ¦‚ç‡çŸ©é˜µ*](https://en.wikipedia.org/wiki/Discrete-time_Markov_chain) *ç»™å‡ºäº†åœ¨å•ä¸€æ­¥éª¤ä¸­ä»ä¸€ä¸ªçŠ¶æ€è½¬ç§»åˆ°å¦ä¸€ä¸ªçŠ¶æ€çš„æ¦‚ç‡ã€‚*'
- en: '*For t=1,2 states case this reduces the dimensions of the problem to*'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¯¹äº t=1,2 çŠ¶æ€æƒ…å†µï¼Œè¿™å°†æŠŠé—®é¢˜çš„ç»´åº¦ç®€åŒ–ä¸º*'
- en: '![](../Images/17300716c49a49e0bad004b4fc071bb3.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17300716c49a49e0bad004b4fc071bb3.png)'
- en: 'This breaks up into the following components:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åˆ†è§£ä¸ºä»¥ä¸‹å‡ ä¸ªç»„æˆéƒ¨åˆ†ï¼š
- en: '*The conditional probability:*'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ¡ä»¶æ¦‚ç‡ï¼š*'
- en: '![](../Images/8174f8ebfb168c1ee0e42311baec22df.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8174f8ebfb168c1ee0e42311baec22df.png)'
- en: '*The initial distribution or marginal distribution: P(*Sâ‚=xâ‚)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '*åˆå§‹åˆ†å¸ƒæˆ–è¾¹é™…åˆ†å¸ƒï¼šP(*Sâ‚=xâ‚)*'
- en: 'In our road example, this was the initial probabilities for the 3 states: p,q,r.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„é“è·¯ç¤ºä¾‹ä¸­ï¼Œè¿™äº›æ˜¯ 3 ä¸ªçŠ¶æ€çš„åˆå§‹æ¦‚ç‡ï¼špã€qã€rã€‚
- en: So, all we need is the one step conditional probabilities and the initial distribution
    of states to get the joint probability of transitioning to a state.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œæˆ‘ä»¬æ‰€éœ€çš„ä»…ä»…æ˜¯ä¸€æ­¥æ¡ä»¶æ¦‚ç‡å’ŒçŠ¶æ€çš„åˆå§‹åˆ†å¸ƒï¼Œä»¥è·å¾—è½¬ç§»åˆ°æŸä¸ªçŠ¶æ€çš„è”åˆæ¦‚ç‡ã€‚
- en: '*The transition matrix can be represented as below:*'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '*è½¬ç§»çŸ©é˜µå¯ä»¥è¡¨ç¤ºå¦‚ä¸‹ï¼š*'
- en: '![](../Images/a184600677e45bc59986b9fad4c3c4bc.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a184600677e45bc59986b9fad4c3c4bc.png)'
- en: 'Source: [Wikipedia Stochastic Matrix](https://en.wikipedia.org/wiki/Stochastic_matrix#:~:text=In%20mathematics%2C%20a%20stochastic%20matrix,substitution%20matrix%2C%20or%20Markov%20matrix.)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[ç»´åŸºç™¾ç§‘ éšæœºçŸ©é˜µ](https://en.wikipedia.org/wiki/Stochastic_matrix#:~:text=In%20mathematics%2C%20a%20stochastic%20matrix,substitution%20matrix%2C%20or%20Markov%20matrix.)
- en: '*In the transition matrix the rows represent the current state and the columns
    the future state. Thus* pâ‚â‚‚ *is the probability of transitioning from state 1
    in time t to state 2 in t+1.*'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '*åœ¨è½¬ç§»çŸ©é˜µä¸­ï¼Œè¡Œä»£è¡¨å½“å‰çŠ¶æ€ï¼Œåˆ—ä»£è¡¨æœªæ¥çŠ¶æ€ã€‚å› æ­¤* pâ‚â‚‚ *æ˜¯ä»æ—¶é—´ t çš„çŠ¶æ€ 1 è½¬ç§»åˆ°æ—¶é—´ t+1 çš„çŠ¶æ€ 2 çš„æ¦‚ç‡ã€‚*'
- en: '*Also note the sum of all branch probabilities from a state has to be one.
    This is logical as you have to transition from a state to another one. Mathematically
    this is written as:*'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¦å¤–ï¼Œè¯·æ³¨æ„ä»ä¸€ä¸ªçŠ¶æ€å‡ºå‘çš„æ‰€æœ‰åˆ†æ”¯æ¦‚ç‡ä¹‹å’Œå¿…é¡»ä¸º 1ã€‚è¿™æ˜¯é€»è¾‘ä¸Šçš„ï¼Œå› ä¸ºä½ å¿…é¡»ä»ä¸€ä¸ªçŠ¶æ€è½¬ç§»åˆ°å¦ä¸€ä¸ªçŠ¶æ€ã€‚æ•°å­¦ä¸Šå¯ä»¥å†™ä½œï¼š*'
- en: '![](../Images/a8e31f02b2165753041bbf20c42822b1.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8e31f02b2165753041bbf20c42822b1.png)'
- en: '*Why do we sum over â€˜jâ€™? Because this represents the possible states that you
    can transition to from, â€˜iâ€™.*'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '*ä¸ºä»€ä¹ˆæˆ‘ä»¬å¯¹â€˜jâ€™è¿›è¡Œæ±‚å’Œï¼Ÿå› ä¸ºè¿™ä»£è¡¨äº†ä½ å¯ä»¥ä»â€˜iâ€™è½¬ç§»åˆ°çš„å¯èƒ½çŠ¶æ€ã€‚*'
- en: '***Markov Chain dynamics***'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '***é©¬å°”å¯å¤«é“¾åŠ¨æ€***'
- en: For a Markov chain, we use the word dynamics to describe the variation of the
    state over a short time interval starting from a given initial state. The evolution
    of a Markov chain is a random process and so we cannot say exactly what sequence
    of states will follow the initial state. In reinforcement learning we wish predict
    the future states given the current state or initial state. A prediction of the
    future state, Sâ‚™ given Sâ‚ can be calculated easily once we have the probability
    transition matrix and the initial distribution. The n -step transition probability
    matrix can be found by multiplying the single-step probability matrix by itself
    n times.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºé©¬å°”å¯å¤«é“¾ï¼Œæˆ‘ä»¬ä½¿ç”¨â€œåŠ¨æ€â€ä¸€è¯æ¥æè¿°åœ¨ç»™å®šåˆå§‹çŠ¶æ€ä¸‹ï¼ŒçŠ¶æ€åœ¨çŸ­æ—¶é—´é—´éš”å†…çš„å˜åŒ–ã€‚é©¬å°”å¯å¤«é“¾çš„æ¼”å˜æ˜¯ä¸€ä¸ªéšæœºè¿‡ç¨‹ï¼Œå› æ­¤æˆ‘ä»¬ä¸èƒ½ç¡®åˆ‡åœ°è¯´åˆå§‹çŠ¶æ€ä¹‹åä¼šè·Ÿéšä»€ä¹ˆçŠ¶æ€åºåˆ—ã€‚åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›æ ¹æ®å½“å‰çŠ¶æ€æˆ–åˆå§‹çŠ¶æ€é¢„æµ‹æœªæ¥çŠ¶æ€ã€‚ä¸€æ—¦æˆ‘ä»¬æœ‰äº†æ¦‚ç‡è½¬ç§»çŸ©é˜µå’Œåˆå§‹åˆ†å¸ƒï¼Œå¯ä»¥å¾ˆå®¹æ˜“åœ°è®¡ç®—å‡ºç»™å®š
    Sâ‚ çš„æœªæ¥çŠ¶æ€ Sâ‚™ çš„é¢„æµ‹ã€‚n æ­¥è½¬ç§»æ¦‚ç‡çŸ©é˜µå¯ä»¥é€šè¿‡å°†å•æ­¥æ¦‚ç‡çŸ©é˜µè‡ªä¹˜ n æ¬¡å¾—åˆ°ã€‚
- en: '![](../Images/8f6bd37c2c6ba38902a8477428014508.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f6bd37c2c6ba38902a8477428014508.png)'
- en: This is the beauty of the Markov system and why it is used in reinforcement
    learning. Once we have a tractable set of transition probabilities, we can calculate
    the probability of transitioning from any initial state to state j in n steps.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯é©¬å°”å¯å¤«ç³»ç»Ÿçš„ç¾å¦™ä¹‹å¤„ï¼Œä»¥åŠå®ƒä¸ºä½•åœ¨å¼ºåŒ–å­¦ä¹ ä¸­è¢«ä½¿ç”¨ã€‚ä¸€æ—¦æˆ‘ä»¬æ‹¥æœ‰ä¸€ä¸ªå¯å¤„ç†çš„è½¬ç§»æ¦‚ç‡é›†ï¼Œæˆ‘ä»¬å°±å¯ä»¥è®¡ç®—ä»ä»»ä½•åˆå§‹çŠ¶æ€åˆ°çŠ¶æ€jåœ¨næ­¥å†…çš„è½¬ç§»æ¦‚ç‡ã€‚
- en: Now after this very long digression into the stochastic and probability behind
    MDPs, I will briefly review the MDP equations in Sutton (2017), ch 3.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œåœ¨å¯¹MDPèƒŒåçš„éšæœºå’Œæ¦‚ç‡è¿›è¡Œå¦‚æ­¤é•¿æ—¶é—´çš„ç¦»é¢˜è®¨è®ºä¹‹åï¼Œæˆ‘å°†ç®€è¦å›é¡¾Suttonï¼ˆ2017ï¼‰ç¬¬3ç« ä¸­çš„MDPæ–¹ç¨‹ã€‚
- en: '**Section 3: Understanding Sutton Ch3 MDP Equations Using Probability and Stochastic
    Concepts**'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¬¬3èŠ‚ï¼šä½¿ç”¨æ¦‚ç‡å’Œéšæœºæ¦‚å¿µç†è§£Suttonç¬¬3ç« çš„MDPæ–¹ç¨‹**'
- en: '***The MDP Model***'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '***MDPæ¨¡å‹***'
- en: In this section I briefly define the MDP giving what is essentially the textbook
    definition. In the next section we will look at the statistical theory underlying
    MDPs.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€èŠ‚ä¸­ï¼Œæˆ‘ç®€è¦å®šä¹‰äº†MDPï¼Œç»™å‡ºæœ¬è´¨ä¸Šæ˜¯æ•™ç§‘ä¹¦ä¸­çš„å®šä¹‰ã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨MDPçš„ç»Ÿè®¡ç†è®ºã€‚
- en: '*The MDP framework is a considerable abstraction of the problem of goal-directed
    learning from interaction. It proposes that whatever the details of the sensory,
    memory, and control apparatus, and whatever objective one is trying to achieve,
    any problem of learning goal-directed behavior can be reduced to three signals
    passing back and forth between an agent and its environment: one signal to represent
    the choices made by the agent (the actions), one signal to represent the basis
    on which the choices are made (the states), and one signal to define the agentâ€™s
    goal (the rewards). This framework may not be sufficient to represent all decision-learning
    problems usefully, but it has proved to be widely useful and applicable.*'
  id: totrans-183
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*MDPæ¡†æ¶æ˜¯ä¸€ä¸ªå…³äºç›®æ ‡å¯¼å‘å­¦ä¹ ä»äº¤äº’ä¸­é—®é¢˜çš„æ˜¾è‘—æŠ½è±¡ã€‚å®ƒæå‡ºï¼Œæ— è®ºæ„ŸçŸ¥ã€è®°å¿†å’Œæ§åˆ¶è£…ç½®çš„ç»†èŠ‚å¦‚ä½•ï¼Œä»¥åŠç›®æ ‡æ˜¯ä»€ä¹ˆï¼Œä»»ä½•ç›®æ ‡å¯¼å‘è¡Œä¸ºçš„å­¦ä¹ é—®é¢˜éƒ½å¯ä»¥å½’ç»“ä¸ºåœ¨ä»£ç†å’Œç¯å¢ƒä¹‹é—´æ¥å›ä¼ é€’çš„ä¸‰ä¸ªä¿¡å·ï¼šä¸€ä¸ªä¿¡å·è¡¨ç¤ºä»£ç†æ‰€åšçš„é€‰æ‹©ï¼ˆè¡ŒåŠ¨ï¼‰ï¼Œä¸€ä¸ªä¿¡å·è¡¨ç¤ºé€‰æ‹©çš„ä¾æ®ï¼ˆçŠ¶æ€ï¼‰ï¼Œä¸€ä¸ªä¿¡å·å®šä¹‰ä»£ç†çš„ç›®æ ‡ï¼ˆå¥–åŠ±ï¼‰ã€‚è¿™ä¸ªæ¡†æ¶å¯èƒ½ä¸è¶³ä»¥æœ‰æ•ˆåœ°è¡¨ç¤ºæ‰€æœ‰å†³ç­–å­¦ä¹ é—®é¢˜ï¼Œä½†å®ƒè¢«è¯æ˜åœ¨å¹¿æ³›çš„åº”ç”¨ä¸­éå¸¸æœ‰ç”¨ã€‚*'
- en: '*Sutton ch2, p50*'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '*Suttonç¬¬2ç« ï¼Œç¬¬50é¡µ*'
- en: Most of the MDP framework can actually be understood at a high level without
    detailed knowledge of probability and stochastic theory. However, now that we
    have exposure to those concepts we can actually appreciate what is going on and
    its complexity. Let us now look at some of the equations related to the Markov
    process that are presented in Sutton (2017), ch3.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œå¤§éƒ¨åˆ†MDPæ¡†æ¶å¯ä»¥åœ¨æ²¡æœ‰è¯¦ç»†çš„æ¦‚ç‡å’Œéšæœºç†è®ºçŸ¥è¯†çš„æƒ…å†µä¸‹é«˜å±‚æ¬¡åœ°ç†è§£ã€‚ç„¶è€Œï¼Œç°åœ¨æˆ‘ä»¬å·²ç»æ¥è§¦åˆ°è¿™äº›æ¦‚å¿µï¼Œæˆ‘ä»¬å¯ä»¥çœŸæ­£æ¬£èµåˆ°å…¶ä¸­çš„å¤æ‚æ€§ã€‚è®©æˆ‘ä»¬ç°åœ¨çœ‹çœ‹Suttonï¼ˆ2017ï¼‰ç¬¬3ç« ä¸­ä¸é©¬å°”å¯å¤«è¿‡ç¨‹ç›¸å…³çš„ä¸€äº›æ–¹ç¨‹ã€‚
- en: '***Markov Property***'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '***é©¬å°”å¯å¤«æ€§è´¨***'
- en: 'This is essentially the idea that the future is independent of the past, given
    the present. It is conceptualized probabilistically as:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æœ¬è´¨ä¸Šæ˜¯æœªæ¥åœ¨ç»™å®šç°åœ¨çš„æƒ…å†µä¸‹ä¸è¿‡å»æ— å…³çš„è§‚ç‚¹ã€‚å®ƒåœ¨æ¦‚ç‡ä¸Šè¢«æ¦‚å¿µåŒ–ä¸ºï¼š
- en: '![](../Images/e8419e256fd14c81e1b587212d392bd8.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e8419e256fd14c81e1b587212d392bd8.png)'
- en: 'This has already been discussed above at length and enables us to understand
    how the basic Markov problem can be made more tractable. Stochastic processes
    that satisfy the Markov property are typically much simpler to analyze than incorporation
    of the historical process in models. In practice, it is important to consider
    whether the Markov property will hold. For many scenarios, for example, the position
    of a car, the present location of the car will determine where it goes in future
    states. But, in some instances, history maybe important: for example, a trading
    agent may have to consider past trends in a share as well as the present price
    to determine whether to buy or sell.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å·²ç»åœ¨ä¸Šæ–‡ä¸­è¯¦ç»†è®¨è®ºè¿‡ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿç†è§£åŸºæœ¬çš„é©¬å°”å¯å¤«é—®é¢˜å¦‚ä½•å˜å¾—æ›´å®¹æ˜“å¤„ç†ã€‚æ»¡è¶³é©¬å°”å¯å¤«æ€§è´¨çš„éšæœºè¿‡ç¨‹é€šå¸¸æ¯”å°†å†å²è¿‡ç¨‹çº³å…¥æ¨¡å‹ä¸­æ›´ç®€å•åˆ†æã€‚åœ¨å®è·µä¸­ï¼Œè€ƒè™‘é©¬å°”å¯å¤«æ€§è´¨æ˜¯å¦æˆç«‹æ˜¯å¾ˆé‡è¦çš„ã€‚ä¾‹å¦‚ï¼Œå¯¹äºæ±½è½¦çš„ä½ç½®æ¥è¯´ï¼Œæ±½è½¦çš„å½“å‰æ‰€åœ¨ä½ç½®å°†å†³å®šå®ƒæœªæ¥çš„çŠ¶æ€ã€‚ä½†åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå†å²å¯èƒ½å¾ˆé‡è¦ï¼šä¾‹å¦‚ï¼Œäº¤æ˜“ä»£ç†å¯èƒ½éœ€è¦è€ƒè™‘è‚¡ç¥¨çš„è¿‡å»è¶‹åŠ¿ä»¥åŠå½“å‰ä»·æ ¼ï¼Œä»¥å†³å®šæ˜¯å¦ä¹°å…¥æˆ–å–å‡ºã€‚
- en: 'A Markov process is a stochastic process with the following properties:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: é©¬å°”å¯å¤«è¿‡ç¨‹æ˜¯å…·æœ‰ä»¥ä¸‹æ€§è´¨çš„éšæœºè¿‡ç¨‹ï¼š
- en: (a.) The number of possible outcomes or states is finite
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: (a.) å¯èƒ½çš„ç»“æœæˆ–çŠ¶æ€çš„æ•°é‡æ˜¯æœ‰é™çš„
- en: This is there for analytical convenience. Infinite processes can also be handled
    with some minor manipulation. However, for time dependent processes, we assume
    time is finite t=0,1,2â€¦,T.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸ºäº†åˆ†æçš„ä¾¿åˆ©ã€‚æ— é™è¿‡ç¨‹ä¹Ÿå¯ä»¥é€šè¿‡ä¸€äº›å°çš„æ“ä½œæ¥å¤„ç†ã€‚ç„¶è€Œï¼Œå¯¹äºæ—¶é—´ä¾èµ–çš„è¿‡ç¨‹ï¼Œæˆ‘ä»¬å‡è®¾æ—¶é—´æ˜¯æœ‰é™çš„ t=0,1,2,â€¦,Tã€‚
- en: (b.) The outcome at any stage depends only on the outcome of the previous stage
    â€” given by Markov property discussed above.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: (b.) ä»»ä½•é˜¶æ®µçš„ç»“æœä»…ä¾èµ–äºå‰ä¸€é˜¶æ®µçš„ç»“æœ â€” ç”±ä¸Šè¿°è®¨è®ºçš„é©¬å°”å¯å¤«æ€§è´¨ç»™å‡ºã€‚
- en: (c.) The transition probabilities are constant over time.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: (c.) è½¬ç§»æ¦‚ç‡éšæ—¶é—´ä¿æŒä¸å˜ã€‚
- en: (d.) The system is stationary â€” this is not actually called out, but property
    c) actually implies it.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: (d.) ç³»ç»Ÿæ˜¯å¹³ç¨³çš„ â€” è¿™å®é™…ä¸Šæ²¡æœ‰æ˜ç¡®è¯´æ˜ï¼Œä½†å±æ€§ c) å®é™…ä¸Šéšå«äº†è¿™ä¸€ç‚¹ã€‚
- en: '***Formal representation of MDP***'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '***MDP çš„æ­£å¼è¡¨ç¤º***'
- en: 'Formally the MDP can be represented as follows. Given, the Markov property,
    an MDP is defined as follows: a tuple which typically contains 5 elements:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¼åœ°ï¼ŒMDP å¯ä»¥è¡¨ç¤ºå¦‚ä¸‹ã€‚é‰´äºé©¬å°”å¯å¤«æ€§è´¨ï¼ŒMDP å®šä¹‰å¦‚ä¸‹ï¼šä¸€ä¸ªé€šå¸¸åŒ…å« 5 ä¸ªå…ƒç´ çš„å…ƒç»„ï¼š
- en: '![](../Images/060e9627a0855438bb93867e50909331.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/060e9627a0855438bb93867e50909331.png)'
- en: '**S**: Set of states in the environment: *this is actually a stochastic process*'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '**S**ï¼šç¯å¢ƒä¸­çš„çŠ¶æ€é›†åˆï¼š*è¿™å®é™…ä¸Šæ˜¯ä¸€ä¸ªéšæœºè¿‡ç¨‹*'
- en: '**A**: Set of actions possible in each state'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**A**ï¼šæ¯ä¸ªçŠ¶æ€ä¸‹å¯èƒ½çš„åŠ¨ä½œé›†åˆ'
- en: '**R:** Reward function in response to actions: *This is normally deterministic,
    but sometimes can be stochastic as well*'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**R:** å¯¹åŠ¨ä½œçš„å¥–åŠ±å‡½æ•°ï¼š*è¿™é€šå¸¸æ˜¯ç¡®å®šæ€§çš„ï¼Œä½†æœ‰æ—¶ä¹Ÿå¯ä»¥æ˜¯éšæœºçš„*'
- en: '**p :** Probability matrix of transitioning from one state to the next â€” This
    is a critical part of an MDP. It is defined by the joint and conditional distribution
    of the state variable.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**p :** ä»ä¸€ä¸ªçŠ¶æ€è½¬ç§»åˆ°ä¸‹ä¸€ä¸ªçŠ¶æ€çš„æ¦‚ç‡çŸ©é˜µ â€” è¿™æ˜¯ MDP çš„å…³é”®éƒ¨åˆ†ã€‚å®ƒç”±çŠ¶æ€å˜é‡çš„è”åˆå’Œæ¡ä»¶åˆ†å¸ƒå®šä¹‰ã€‚'
- en: '**Î³ :** Discount factor to be applied for rewards in the distant future'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '**Î³ :** é€‚ç”¨äºè¿œæœŸå¥–åŠ±çš„æŠ˜æ‰£å› å­'
- en: 'The MDP and agent together thereby give rise to a sequence or trajectory that
    begins like this:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: MDP å’Œæ™ºèƒ½ä½“å…±åŒäº§ç”Ÿä¸€ä¸ªåºåˆ—æˆ–è½¨è¿¹ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](../Images/c3dfa5301b2d5039c76ef0268da018a2.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3dfa5301b2d5039c76ef0268da018a2.png)'
- en: This process is a stochastic process and is finite, i.e., has T time steps with
    a clear terminal state.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¿‡ç¨‹æ˜¯ä¸€ä¸ªéšæœºè¿‡ç¨‹ä¸”æ˜¯æœ‰é™çš„ï¼Œå³æœ‰ T ä¸ªæ—¶é—´æ­¥éª¤ï¼Œå¹¶ä¸”æœ‰ä¸€ä¸ªæ˜ç¡®çš„ç»ˆæ­¢çŠ¶æ€ã€‚
- en: '***Understanding equations***'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '***ç†è§£æ–¹ç¨‹***'
- en: '*Equation 3.2*'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ–¹ç¨‹ 3.2*'
- en: '![](../Images/dda8c3e2e3878d545cc3ee992cfd1ba7.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dda8c3e2e3878d545cc3ee992cfd1ba7.png)'
- en: This equation tells us that Râ‚œ and Sâ‚œ are random variables with discrete probability
    mass distributions which depend on the previous state and action. Equation 3.2
    defines the dynamics function p. This is a function that takes in 4 arguments.
    It also specifies a conditional distribution for each choice of state and action.
    For a specific value of state sâ€™ and reward r, we can estimate the probability
    of these values occurring at time t, given the preceding state and action values
    at t-1.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ–¹ç¨‹å‘Šè¯‰æˆ‘ä»¬ Râ‚œ å’Œ Sâ‚œ æ˜¯å…·æœ‰ç¦»æ•£æ¦‚ç‡è´¨é‡åˆ†å¸ƒçš„éšæœºå˜é‡ï¼Œè¿™äº›åˆ†å¸ƒä¾èµ–äºå…ˆå‰çš„çŠ¶æ€å’ŒåŠ¨ä½œã€‚æ–¹ç¨‹ 3.2 å®šä¹‰äº†åŠ¨æ€å‡½æ•° pã€‚è¿™æ˜¯ä¸€ä¸ªåŒ…å« 4 ä¸ªå‚æ•°çš„å‡½æ•°ã€‚å®ƒè¿˜ä¸ºæ¯ç§çŠ¶æ€å’ŒåŠ¨ä½œé€‰æ‹©æŒ‡å®šäº†æ¡ä»¶åˆ†å¸ƒã€‚å¯¹äºç‰¹å®šçš„çŠ¶æ€
    sâ€™ å’Œå¥–åŠ± rï¼Œæˆ‘ä»¬å¯ä»¥ä¼°è®¡åœ¨æ—¶é—´ t è¿™äº›å€¼å‘ç”Ÿçš„æ¦‚ç‡ï¼Œç»™å®šåœ¨ t-1 æ—¶åˆ»çš„å‰çŠ¶æ€å’ŒåŠ¨ä½œå€¼ã€‚
- en: '*Equation 3.3*'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ–¹ç¨‹ 3.3*'
- en: '![](../Images/35cffa20d2bc48f2491305f3080fb469.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/35cffa20d2bc48f2491305f3080fb469.png)'
- en: Equation 3.3 basically tells us that for each value of â€˜sâ€™ and â€˜aâ€™, there is
    a joint distribution of states and rewards. This is basically a joint conditional
    distribution, hence it must sum to 1\. The joint distribution will have the probabilities
    of different combinations of sâ€™ and r. The sum of all probabilities of all combinations
    must sum to 1 ([refer to Wikipedia](https://en.wikipedia.org/wiki/Joint_probability_distribution)).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ 3.3 åŸºæœ¬ä¸Šå‘Šè¯‰æˆ‘ä»¬ï¼Œå¯¹äºæ¯ä¸ªâ€œsâ€å’Œâ€œaâ€çš„å€¼ï¼Œéƒ½æœ‰ä¸€ä¸ªçŠ¶æ€å’Œå¥–åŠ±çš„è”åˆåˆ†å¸ƒã€‚è¿™åŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªè”åˆæ¡ä»¶åˆ†å¸ƒï¼Œå› æ­¤å®ƒçš„å’Œå¿…é¡»ä¸º 1ã€‚è”åˆåˆ†å¸ƒå°†å…·æœ‰ä¸åŒç»„åˆçš„
    sâ€™ å’Œ r çš„æ¦‚ç‡ã€‚æ‰€æœ‰ç»„åˆçš„æ¦‚ç‡ä¹‹å’Œå¿…é¡»ä¸º 1ï¼ˆ[å‚è§ç»´åŸºç™¾ç§‘](https://en.wikipedia.org/wiki/Joint_probability_distribution)ï¼‰ã€‚
- en: 'Letâ€™s go back to our road example. At each state of the road, we have two actions:
    drive or stop. Then, if we are in state â€˜jammedâ€™ and we choose to drive. Then
    conditional on these two, there is a joint distribution of states and rewards
    for the next state.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å›åˆ°ä¹‹å‰çš„é“è·¯ç¤ºä¾‹ã€‚åœ¨é“è·¯çš„æ¯ä¸ªçŠ¶æ€ä¸‹ï¼Œæˆ‘ä»¬æœ‰ä¸¤ä¸ªåŠ¨ä½œï¼šé©¾é©¶æˆ–åœæ­¢ã€‚å¦‚æœæˆ‘ä»¬å¤„äºâ€œæ‹¥å µâ€çŠ¶æ€å¹¶é€‰æ‹©é©¾é©¶ï¼Œé‚£ä¹ˆåœ¨è¿™ä¸¤è€…çš„æ¡ä»¶ä¸‹ï¼Œå°†æœ‰ä¸€ä¸ªä¸‹ä¸€çŠ¶æ€çš„çŠ¶æ€å’Œå¥–åŠ±çš„è”åˆåˆ†å¸ƒã€‚
- en: '*State transition probabilities â€” Equation 3.4*'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '*çŠ¶æ€è½¬ç§»æ¦‚ç‡ â€” æ–¹ç¨‹ 3.4*'
- en: '![](../Images/f20656f25b96ae18ef0a61e75f836266.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f20656f25b96ae18ef0a61e75f836266.png)'
- en: 'The state transition probabilities are given by summing over reward probabilities
    (note for the probability challenged: like a marginal probability distribution
    of states- [refer to Wikipedia](https://en.wikipedia.org/wiki/Joint_probability_distribution)).'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: çŠ¶æ€è½¬ç§»æ¦‚ç‡é€šè¿‡å¯¹å¥–åŠ±æ¦‚ç‡è¿›è¡Œæ±‚å’Œæ¥ç»™å‡ºï¼ˆè¯·æ³¨æ„ï¼Œå¯¹äºæ¦‚ç‡æŒ‘æˆ˜ï¼šç±»ä¼¼äºçŠ¶æ€çš„è¾¹é™…æ¦‚ç‡åˆ†å¸ƒ - [å‚è§ç»´åŸºç™¾ç§‘](https://en.wikipedia.org/wiki/Joint_probability_distribution)ï¼‰ã€‚
- en: '*Expected Rewards â€” Equation 3.5*'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '*æœŸæœ›å¥–åŠ± â€” æ–¹ç¨‹3.5*'
- en: '![](../Images/3d16279d0fa3436d03788c23e817da7c.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d16279d0fa3436d03788c23e817da7c.png)'
- en: 'Here we are again working with equation 3.2\. This time we sum over the probability
    of states (note for the probability challenged: like a marginal probability distribution
    of rewards â€” [refer to Wikipedia](https://en.wikipedia.org/wiki/Joint_probability_distribution)).
    This gives us the marginal probability of rewards distribution (expression on
    extreme RHS of 3.5). We then multiply this probability by the actual rewards,
    r to get the expected rewards.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å†æ¬¡å¤„ç†æ–¹ç¨‹3.2ã€‚è¿™æ¬¡æˆ‘ä»¬å¯¹çŠ¶æ€çš„æ¦‚ç‡è¿›è¡Œæ±‚å’Œï¼ˆå¯¹äºæ¦‚ç‡ä¸å¤ªæ¸…æ¥šçš„äººï¼šç±»ä¼¼äºå¥–åŠ±çš„è¾¹é™…æ¦‚ç‡åˆ†å¸ƒ â€” [å‚è§Wikipedia](https://en.wikipedia.org/wiki/Joint_probability_distribution)ï¼‰ã€‚è¿™ç»™å‡ºäº†å¥–åŠ±åˆ†å¸ƒçš„è¾¹é™…æ¦‚ç‡ï¼ˆ3.5çš„æç«¯å³ä¾§è¡¨è¾¾å¼ï¼‰ã€‚ç„¶åæˆ‘ä»¬å°†è¿™ä¸ªæ¦‚ç‡ä¹˜ä»¥å®é™…å¥–åŠ±rï¼Œä»¥è·å¾—æœŸæœ›å¥–åŠ±ã€‚
- en: '*Expected rewards for stateâ€“actionâ€“next-state triples â€” Equation 3.6*'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '*æœŸæœ›å¥–åŠ±çš„çŠ¶æ€-åŠ¨ä½œ-ä¸‹ä¸€çŠ¶æ€ä¸‰å…ƒç»„ â€” æ–¹ç¨‹3.6*'
- en: '![](../Images/a79bbafe62dcd3eaceb9c8f08eea3eb3.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a79bbafe62dcd3eaceb9c8f08eea3eb3.png)'
- en: This one is a little trickier. Probability of reward and state joint probability
    conditional on â€˜sâ€™ and â€˜aâ€™ divided by state transition probability . This is an
    application of chain rule of joint distributions (refer [Wikipedia](https://en.wikipedia.org/wiki/Chain_rule_(probability))).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæœ‰ç‚¹æ£˜æ‰‹ã€‚å¥–åŠ±å’ŒçŠ¶æ€çš„è”åˆæ¦‚ç‡æ¡ä»¶åœ¨â€˜sâ€™å’Œâ€˜aâ€™ä¸‹é™¤ä»¥çŠ¶æ€è½¬ç§»æ¦‚ç‡ã€‚è¿™æ˜¯è”åˆåˆ†å¸ƒé“¾å¼æ³•åˆ™çš„ä¸€ä¸ªåº”ç”¨ï¼ˆå‚è§ [Wikipedia](https://en.wikipedia.org/wiki/Chain_rule_(probability))ï¼‰ã€‚
- en: '![](../Images/403725b86d34581c1cfbaf9ef844b799.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/403725b86d34581c1cfbaf9ef844b799.png)'
- en: Letâ€™s substitute our terms.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»£å…¥æˆ‘ä»¬çš„æœ¯è¯­ã€‚
- en: '![](../Images/c03c5419d93ff42848065fc7eff84bb1.png)![](../Images/8accbbba3b261f8518cac930ab112523.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c03c5419d93ff42848065fc7eff84bb1.png)![](../Images/8accbbba3b261f8518cac930ab112523.png)'
- en: We can now take expectations over Râ‚œ
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥å¯¹Râ‚œå–æœŸæœ›å€¼
- en: '![](../Images/01fa099b84e858904ef3bb2a6430d147.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01fa099b84e858904ef3bb2a6430d147.png)'
- en: So, this completes the basic equations derived for the set up of the MDP. It
    is important to understand each of 3.2â€“3.6 as Sutton(2017) mention that each of
    these is a way to represent the MDP and would be used again in subsequent chapters.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œè¿™å®Œæˆäº†MDPè®¾ç½®çš„åŸºæœ¬æ–¹ç¨‹çš„æ¨å¯¼ã€‚ç†è§£3.2åˆ°3.6æ¯ä¸€ä¸ªæ–¹ç¨‹éƒ½å¾ˆé‡è¦ï¼Œå› ä¸ºSuttonï¼ˆ2017å¹´ï¼‰æåˆ°è¿™äº›æ–¹ç¨‹éƒ½æ˜¯è¡¨ç¤ºMDPçš„ä¸€ç§æ–¹å¼ï¼Œå¹¶ä¸”ä¼šåœ¨åç»­ç« èŠ‚ä¸­å†æ¬¡ä½¿ç”¨ã€‚
- en: '**To Conclude..**'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ€»ç»“..**'
- en: Well friends, this has been quite a journey on probability and stochastic theory.
    However, I hope what you have realized is the complexity and beauty of the underlying
    Markov system. Without understanding these, it is difficult to figure out the
    equations that underlie the basic MDP equations. Without understanding these equations
    and how they are derived, it is likely that one will become totally lost when
    it comes to policy, value functions and Q values. These will be picked up in the
    next installment of this tutorial.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½æœ‹å‹ä»¬ï¼Œè¿™ä¸€æ®µå…³äºæ¦‚ç‡å’Œéšæœºç†è®ºçš„æ—…ç¨‹å·²ç»ç›¸å½“é•¿äº†ã€‚ç„¶è€Œï¼Œæˆ‘å¸Œæœ›ä½ ä»¬æ„è¯†åˆ°çš„æ˜¯åŸºç¡€Markovç³»ç»Ÿçš„å¤æ‚æ€§å’Œç¾ä¸½ã€‚å¦‚æœä¸ç†è§£è¿™äº›å†…å®¹ï¼Œå¾ˆéš¾å¼„æ¸…æ¥šåŸºæœ¬MDPæ–¹ç¨‹çš„åº•å±‚æ–¹ç¨‹ã€‚å¦‚æœä¸ç†è§£è¿™äº›æ–¹ç¨‹åŠå…¶æ¨å¯¼æ–¹å¼ï¼Œå½“æ¶‰åŠåˆ°ç­–ç•¥ã€ä»·å€¼å‡½æ•°å’ŒQå€¼æ—¶ï¼Œå¾ˆå¯èƒ½ä¼šå®Œå…¨è¿·å¤±ã€‚ä¸‹ä¸€éƒ¨åˆ†æ•™ç¨‹ä¸­ä¼šæ¶‰åŠåˆ°è¿™äº›å†…å®¹ã€‚
- en: 'I hope you liked my writing. Please consider following me for more such articles
    as I proceed with reinforcement learning: [https://medium.com/@shaileydash](https://medium.com/@shaileydash)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: å¸Œæœ›ä½ å–œæ¬¢æˆ‘çš„å†™ä½œã€‚è¯·è€ƒè™‘å…³æ³¨æˆ‘ï¼Œä»¥ä¾¿è·å–æ›´å¤šç±»ä¼¼çš„æ–‡ç« ï¼Œæˆ‘ä¼šç»§ç»­æ’°å†™å…³äºå¼ºåŒ–å­¦ä¹ çš„å†…å®¹ï¼š[https://medium.com/@shaileydash](https://medium.com/@shaileydash)
- en: Also do let me know your inputs and comments in the comments which will help
    me revise this.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿè¯·åœ¨è¯„è®ºä¸­å‘Šè¯‰æˆ‘ä½ çš„æ„è§å’Œå»ºè®®ï¼Œè¿™å°†å¸®åŠ©æˆ‘ä¿®æ”¹å†…å®¹ã€‚
- en: 'You can follow me on Linkedin for more corporate oriented articles on AI and
    DS: [www.linkedin.com/in/shailey-dash-phd-8b4a286](http://www.linkedin.com/in/shailey-dash-phd-8b4a286)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨Linkedinä¸Šå…³æ³¨æˆ‘ï¼Œè·å–æ›´å¤šå…³äºAIå’Œæ•°æ®ç§‘å­¦çš„ä¼ä¸šå¯¼å‘æ–‡ç« ï¼š[www.linkedin.com/in/shailey-dash-phd-8b4a286](http://www.linkedin.com/in/shailey-dash-phd-8b4a286)
- en: '**References**'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**å‚è€ƒæ–‡çŒ®**'
- en: I am listing quite a long list of stats and probability theory resources. They
    are all excellent, if difficult.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åˆ—å‡ºäº†ä¸€ä»½ç›¸å½“é•¿çš„ç»Ÿè®¡å­¦å’Œæ¦‚ç‡ç†è®ºèµ„æºåˆ—è¡¨ã€‚è¿™äº›èµ„æºéƒ½éå¸¸ä¼˜ç§€ï¼Œä½†ä¹Ÿç›¸å½“å›°éš¾ã€‚
- en: 'Richard S. Sutton and Andrew G. Barto. [Reinforcement Learning: An Introduction;
    2nd Edition](http://incompleteideas.net/book/bookdraft2017nov5.pdf). 2017.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: Richard S. Sutton å’Œ Andrew G. Barto. [ã€Šå¼ºåŒ–å­¦ä¹ ï¼šå¯¼è®ºï¼›ç¬¬2ç‰ˆã€‹](http://incompleteideas.net/book/bookdraft2017nov5.pdf)ã€‚2017å¹´ã€‚
- en: '[https://en.wikipedia.org/wiki/Markov_decision_process](https://en.wikipedia.org/wiki/Markov_decision_process)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Markov_decision_process](https://en.wikipedia.org/wiki/Markov_decision_process)'
- en: '[https://en.wikipedia.org/wiki/Stochastic_process](https://en.wikipedia.org/wiki/Stochastic_process)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Stochastic_process](https://en.wikipedia.org/wiki/Stochastic_process)'
- en: '[https://en.wikipedia.org/wiki/Joint_probability_distribution](https://en.wikipedia.org/wiki/Joint_probability_distribution)'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Joint_probability_distribution](https://en.wikipedia.org/wiki/Joint_probability_distribution)'
- en: '[https://en.wikipedia.org/wiki/Conditional_probability_distribution](https://en.wikipedia.org/wiki/Conditional_probability_distribution)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Conditional_probability_distribution](https://en.wikipedia.org/wiki/Conditional_probability_distribution)'
- en: '[https://www.kent.ac.uk/smsas/personal/lb209/files/notes1.pdf](https://www.kent.ac.uk/smsas/personal/lb209/files/notes1.pdf)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.kent.ac.uk/smsas/personal/lb209/files/notes1.pdf](https://www.kent.ac.uk/smsas/personal/lb209/files/notes1.pdf)'
- en: Lawler, G.F. (2006). Introduction to Stochastic Processes (2nd ed.). Chapman
    and Hall/CRC. [https://doi.org/10.1201/9781315273600](https://doi.org/10.1201/9781315273600)
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: Lawler, G.F. (2006). éšæœºè¿‡ç¨‹å¯¼è®ºï¼ˆç¬¬2ç‰ˆï¼‰ã€‚Chapman and Hall/CRCã€‚ [https://doi.org/10.1201/9781315273600](https://doi.org/10.1201/9781315273600)
- en: 'Yates, R.D. and Goodman, D.J., 2014\. *Probability and stochastic processes:
    a friendly introduction for electrical and computer engineers*. John Wiley & Sons.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: Yates, R.D. å’Œ Goodman, D.J., 2014\. *æ¦‚ç‡ä¸éšæœºè¿‡ç¨‹ï¼šç”µæ°”ä¸è®¡ç®—æœºå·¥ç¨‹å¸ˆçš„å‹å¥½ä»‹ç»*ã€‚John Wiley & Sons.
- en: Unnikrishna Pillai, Probability and Stochastic Processes, Lecture 8, NYU, Tandon
    school of Engineering, [https://www.youtube.com/watch?v=nFF6eRQT2-c&t=455s](https://www.youtube.com/watch?v=nFF6eRQT2-c&t=455s)
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: Unnikrishna Pillai, æ¦‚ç‡ä¸éšæœºè¿‡ç¨‹ï¼Œç¬¬8è®²ï¼ŒNYU Tandon å·¥ç¨‹å­¦é™¢ï¼Œ[https://www.youtube.com/watch?v=nFF6eRQT2-c&t=455s](https://www.youtube.com/watch?v=nFF6eRQT2-c&t=455s)
