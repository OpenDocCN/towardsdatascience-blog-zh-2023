- en: 'Reinforcement Learning Basics: Understanding Stochastic Theory Underlying a
    Markov Decision Process'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习基础：理解马尔科夫决策过程背后的随机理论
- en: 原文：[https://towardsdatascience.com/reinforcement-learning-basics-1-understanding-stochastic-theory-underlying-an-mdp-e770be0665cf?source=collection_archive---------5-----------------------#2023-02-16](https://towardsdatascience.com/reinforcement-learning-basics-1-understanding-stochastic-theory-underlying-an-mdp-e770be0665cf?source=collection_archive---------5-----------------------#2023-02-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/reinforcement-learning-basics-1-understanding-stochastic-theory-underlying-an-mdp-e770be0665cf?source=collection_archive---------5-----------------------#2023-02-16](https://towardsdatascience.com/reinforcement-learning-basics-1-understanding-stochastic-theory-underlying-an-mdp-e770be0665cf?source=collection_archive---------5-----------------------#2023-02-16)
- en: 'Part 1: On the Markov Decision Model which forms the theoretical foundation
    of reinforcement learning problems'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一部分：马尔科夫决策模型的理论基础，这为强化学习问题提供了基础
- en: '[](https://medium.com/@shaileydash?source=post_page-----e770be0665cf--------------------------------)[![Shailey
    Dash](../Images/b5a44aafce4c310f60398e714d515b76.png)](https://medium.com/@shaileydash?source=post_page-----e770be0665cf--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e770be0665cf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e770be0665cf--------------------------------)
    [Shailey Dash](https://medium.com/@shaileydash?source=post_page-----e770be0665cf--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@shaileydash?source=post_page-----e770be0665cf--------------------------------)[![Shailey
    Dash](../Images/b5a44aafce4c310f60398e714d515b76.png)](https://medium.com/@shaileydash?source=post_page-----e770be0665cf--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e770be0665cf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e770be0665cf--------------------------------)
    [Shailey Dash](https://medium.com/@shaileydash?source=post_page-----e770be0665cf--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc4fe44a8e55d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-basics-1-understanding-stochastic-theory-underlying-an-mdp-e770be0665cf&user=Shailey+Dash&userId=c4fe44a8e55d&source=post_page-c4fe44a8e55d----e770be0665cf---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e770be0665cf--------------------------------)
    ·28 min read·Feb 16, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe770be0665cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-basics-1-understanding-stochastic-theory-underlying-an-mdp-e770be0665cf&user=Shailey+Dash&userId=c4fe44a8e55d&source=-----e770be0665cf---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc4fe44a8e55d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-basics-1-understanding-stochastic-theory-underlying-an-mdp-e770be0665cf&user=Shailey+Dash&userId=c4fe44a8e55d&source=post_page-c4fe44a8e55d----e770be0665cf---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e770be0665cf--------------------------------)
    ·28分钟阅读·2023年2月16日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe770be0665cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-basics-1-understanding-stochastic-theory-underlying-an-mdp-e770be0665cf&user=Shailey+Dash&userId=c4fe44a8e55d&source=-----e770be0665cf---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe770be0665cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-basics-1-understanding-stochastic-theory-underlying-an-mdp-e770be0665cf&source=-----e770be0665cf---------------------bookmark_footer-----------)![](../Images/7ffb68ec9c76b1f9339fd860f362d1a1.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe770be0665cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-basics-1-understanding-stochastic-theory-underlying-an-mdp-e770be0665cf&source=-----e770be0665cf---------------------bookmark_footer-----------)![](../Images/7ffb68ec9c76b1f9339fd860f362d1a1.png)'
- en: 'Example of a simple MDP with three states (green circles) and two actions (orange
    circles), with two rewards (Image source: [Wikipedia](https://en.wikipedia.org/wiki/Markov_decision_process#/media/File:Markov_Decision_Process.svg))'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的MDP示例，包括三个状态（绿色圆圈）和两个动作（橙色圆圈），以及两个奖励（图片来源：[维基百科](https://en.wikipedia.org/wiki/Markov_decision_process#/media/File:Markov_Decision_Process.svg)）
- en: Reinforcement learning (RL) is a type of machine learning that enables an agent
    to learn to achieve a goal in an uncertain environment by taking actions. An important
    aspect of reinforcement learning is that it evaluates the actions taken rather
    than instructs by giving correct actions. Each action has a reward associated
    with it which signals to the agent the success of the action in progressing towards
    the goal. The agent navigates the environment repetitively learning how to optimize
    to reach its goal.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）是一种机器学习类型，使代理能够通过采取行动在不确定的环境中学习实现目标。强化学习的一个重要方面是它评估所采取的行动，而不是通过提供正确的行动来指导。每个行动都有一个相关的奖励，向代理发出该行动在朝着目标前进中成功的信号。代理在环境中重复导航，学习如何优化以达到其目标。
- en: Till very recently most of reinforcement learning’s successes where related
    to game playing agents such as Alpha Go. However increasingly reinforcement learning
    is being applied in real world applications such as self-driving cars, and robotic
    automation. The recently launched ChatGPT also has a RL component in its architecture
    for finetuning of answers. Given this, it becomes important to understand RL which
    has been acknowledged to have a steep learning curve both from the theoretical
    perspective and also the practical implementation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 直到最近，大多数强化学习的成功与游戏代理相关，如 Alpha Go。然而，强化学习越来越多地应用于现实世界中的应用，如自动驾驶汽车和机器人自动化。最近推出的
    ChatGPT 也在其架构中包含了一个用于答案微调的 RL 组件。鉴于此，理解 RL 变得重要，因为它被认为具有陡峭的学习曲线，既包括理论方面，也包括实际应用。
- en: This article focuses on the Markov Decision model (MDP) which is often used
    to formalize the reinforcement agent’s problem. However, a neglected aspect of
    this conceptualization of the RL problem is that the MDP framework is an important
    application of Stochastic theory.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文重点讨论了马尔可夫决策模型（MDP），它通常用于形式化强化学习代理的问题。然而，这种对强化学习问题的概念化一个被忽视的方面是 MDP 框架是随机理论的重要应用。
- en: There is a non-trivial amount of stochastic and probability theory that underlies
    the basic Markov Decision Process equations. Without understanding these equations
    and how they are derived, it is difficult to make progress with RL. This article
    aims to clarify the stochastic and probability concepts that underlie MDPs with
    a focus on understanding equations presented in Sutton(2017) Ch 3.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的马尔可夫决策过程方程背后有相当复杂的随机和概率理论。没有理解这些方程及其推导过程，就很难在 RL 领域取得进展。本文旨在澄清 MDP 背后的随机和概率概念，重点理解
    Sutton（2017）第3章中呈现的方程。
- en: The basic MDP framework as presented in Sutton and Barto (2017), Ch3, actually
    appears to be quite simple and intuitive — a sleight of hand achieved by a master!
    However, there is a huge backlog of statistical theory behind this framework.
    We don’t need to know the depths of Stochastic Processes, because, believe me,
    it’s a lot! But it is important to be aware of some of the subtleties underlying
    some of the concepts used in MDPs. Hence it is important to understand what are
    stochastic processes as the theory of these lies at the core of MDPs.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的 MDP 框架如 Sutton 和 Barto（2017），第3章中所呈现的，实际上看起来相当简单和直观——这是一个大师级的巧妙手法！然而，这个框架背后有大量的统计理论。我们不需要了解随机过程的深度，因为，相信我，真的很多！但了解一些
    MDP 中使用的概念背后的细微之处是很重要的。因此，理解什么是随机过程很重要，因为这些理论是 MDP 的核心。
- en: This article is the first in a sequence of articles that will aim to make the
    fundamentals of RL clearer both at the theoretical and practical application level.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本文是一个系列文章中的第一篇，旨在使强化学习（RL）的基础知识在理论和实际应用层面上更加清晰。
- en: '**Who is the intended audience?**'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**目标受众是谁？**'
- en: Well, of course, it looks like it is for beginners, but I would say that this
    post is actually more useful to people who have some knowledge of RL and then
    try to understand the equations presented in Sutton (2017) Ch 3 more intuitively.
    This article is the result of my learning journey into RL. I started quite happily
    with Sutton and Barto (2017) and thought I had understood most of the concepts
    in Ch 3 for the MDP framework. However, once I tried to actually derive some of
    the equations presented in ch3, I found myself unable to figure them out. So,
    it became clear to me that possibly more was going on than I seemed to know about.
    There is no real resource available that I could find that relates MDPs to their
    underlying statistical underpinnings. I had to use a variety of material ranging
    from Wikipedia to various university level Stochastic theory courses. Given how
    scattered the underlying material is, I decided to document it in this post.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '**Approach — Decoding Sutton and Barto (2017)**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: The other thing is the format of this post. Sutton (2017) represents the bible
    of RL theory. The current edition is an updated version available online. But,
    much of theoretical material was actually published 20 years ago in the first
    edition of the book. Also, the approach taken in Ch3 of the book is the approach
    found across the internet. Given this, my approach in this post, is a bit like
    a ‘key’ where I present material from the book and then explain it using the lens
    of stochastic theory.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: A slight digression in defense of ‘keys’. Legions of students, especially in
    school, use them extensively; whereas teachers actually frown on them for spoon
    feeding. However, when you are learning online and typically on your own, then
    a detailed guide becomes important, as you can’t just put up your hand and ask
    a question.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Enough on the preamble! Let’s move to the actual topic….
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '**Role of MDP**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'Understanding how the reinforcement learning problem works in the theoretical
    MDP environment is critical to getting the fundamentals of RL. Many important
    RL algorithms either are based on Markov based equations or models or represent
    some key departures from the Markov model. MDPs are a mathematically idealized
    form of the reinforcement learning process and provide a theoretical framework
    for an idealized type of reinforcement learning problem. Hence, no getting away
    from it: understanding MDPs is foundational to understanding the RL problem and
    how it is being addressed.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Be warned, Stochastic Theory is a particularly tough branch of probability theory
    and at times the notation used can be very intimidating. It shall be my attempt
    to bring the underlying stochastic concepts and ideas underlying MDPs more explicitly.
    Hopefully, the end result brings to the front the complexities that underlie the
    Markov model.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article I split up the content in 3 sections:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '**Section 1: Key Components of an MDP**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '- RL problem'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '- State, agent, environment, reward, etc.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '**Section 2: Probability and Stochastic Concepts Used in MDPs**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**第2节：MDP中使用的概率和随机概念**'
- en: '- Random variables, state or sample space, stochastic processes, realization
    or trajectory'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '- 随机变量、状态或样本空间、随机过程、实现或轨迹'
- en: '- Probability distributions, joint distributions, marginal distributions, conditional
    distributions'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '- 概率分布、联合分布、边际分布、条件分布'
- en: '- Deriving the Markov property'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '- 推导马尔可夫性质'
- en: '- Probability transition Matrix'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '- 概率转移矩阵'
- en: '**Section 3: Understanding Sutton Ch3 equations Using Probability and Stochastic
    concepts**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**第3节：使用概率和随机概念理解Sutton Ch3方程**'
- en: '- The MDP Model'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '- MDP模型'
- en: '- Formal presentation of MDP model'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '- MDP模型的形式化表示'
- en: '- Equations 3.2, 3.3, 3.4, 3.5, 3.6 from Ch3, Sutton (2017)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '- Sutton (2017) Ch3中的方程3.2, 3.3, 3.4, 3.5, 3.6'
- en: 'This probably seems like just the introductory part of an MDP. You may well
    ask: ‘what about value functions, policy, Q value function, Bellman equations?’.
    Well, that comes in the next article, as once I have gone through with the above
    contents, you will agree that this by itself it is a lot.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能只是MDP的介绍部分。你可能会问：“那价值函数、策略、Q值函数、贝尔曼方程呢？”这些内容将在下一篇文章中介绍，因为一旦我讲解了上述内容，你会同意这些内容本身就已经很多了。
- en: '**Section 1: Key Components of an MDP**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**第1节：MDP的关键组成部分**'
- en: First, to set context about what we are about to explain, I shall provide a
    brief summary of the basic MDP model and definitions used by [Sutton (2017), Ch3](http://www.incompleteideas.net/book/RLbook2020.pdf).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，为了设置我们即将解释的内容的背景，我将简要总结[Sutton (2017), Ch3](http://www.incompleteideas.net/book/RLbook2020.pdf)使用的基本MDP模型和定义。
- en: '***Why MDP?***'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '***为什么选择MDP？***'
- en: The major reason for the popularity of the MDP framework is due to the Markov
    property where future states depend only on the current state and not the history
    of states. Or, to put it another way, the current state encapsulates all the relevant
    information required for making decisions about the future states. By the way,
    this seemingly innocuous assumption actually has a wealth of probability theory
    behind it and hence also implications.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: MDP框架受欢迎的主要原因是由于马尔可夫性质，其中未来状态仅依赖于当前状态，而不依赖于状态的历史。换句话说，当前状态封装了做出关于未来状态决策所需的所有相关信息。顺便说一句，这种看似无害的假设实际上背后有丰富的概率理论，并且也有相应的影响。
- en: Because of this property, an MDP represents a tractable way to formalize sequential
    decision making. It provides a probabilistic framework for modeling decision making
    in situations where outcomes are partly random and partly under the control of
    a decision maker. In this scenario, actions don’t just have immediate payoffs
    in terms of rewards, but also affect subsequent time periods or states, and through
    them future rewards. Of course, the way reinforcement learning is done in practice
    is different from the MDP.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个特性，MDP代表了一种可处理的方式来形式化顺序决策。它提供了一个概率框架，用于建模在结果部分随机和部分由决策者控制的情况下的决策。在这种情况下，行动不仅在奖励方面有直接的回报，还会影响后续时间段或状态，从而影响未来的奖励。当然，强化学习在实践中的方式与MDP有所不同。
- en: '***Some Definitions to start off with***'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '***一些定义以开始***'
- en: '**Agent:** This is the learner or decision maker who makes sequential decisions
    to achieve an end goal in an environment.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**代理：**这是学习者或决策者，在环境中做出顺序决策以实现最终目标。'
- en: '**Environment:** This is defined as everything that is outside the agent’s
    locus of control. Hence the environment can be the external environment for a
    robot, it can be roads and pedestrians for a self -driving car, etc. In a video
    game, the environment is the game. The following points are important for the
    environment:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**环境：**这被定义为代理控制范围之外的所有事物。因此，环境可以是机器人的外部环境，可以是自驾车的道路和行人等。在视频游戏中，环境就是游戏。环境的以下几点是重要的：'
- en: Observations are what the agent receives as input — for example a self-driving
    car can only receive inputs from the environment in its vicinity and will not
    be aware of a road block significantly ahead
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察是代理接收到的输入——例如，自驾车只能接收来自其周围环境的输入，而不会意识到前方很远处的道路障碍。
- en: The agent only has full knowledge of the environment if it knows the model of
    the environment, example the structure of rules of a video game which govern the
    environment
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有在知道环境模型的情况下，代理才能完全了解环境，例如掌握游戏规则的结构，这些规则决定了环境。
- en: The environment is changed by agent’s actions (sudden breaking causes a small
    jam), but can also change on its own (a road diversion)
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境会因代理的行动而改变（突然刹车会造成小堵塞），但也可能自行变化（道路改道）。
- en: '***What is the RL problem?***'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '***什么是 RL 问题？***'
- en: The typical RL problem involves a learning agent interacting over time with
    its environment to achieve a goal. To do this, a learning agent must be able to
    sense the state of its environment to some extent and must be able to take actions
    that affect the state. The fundamental problem is presented in this diagram in
    below.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的 RL 问题涉及一个学习代理在一段时间内与其环境互动以实现目标。为此，学习代理必须能够在一定程度上感知环境的状态，并且必须能够采取影响状态的行动。这个基本问题在下面的图中展示。
- en: '![](../Images/6586539f97d6dff700ae8f57dbde2125.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6586539f97d6dff700ae8f57dbde2125.png)'
- en: 'Agent environment interaction in a Markov decision process (Image source: Created
    by author, inspired by Sutton (2017) Ch3)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在马尔可夫决策过程中，代理和环境的交互（图像来源：作者创作，灵感来源于 Sutton (2017) 第三章）
- en: '***The measurement of time***'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '***时间的测量***'
- en: 'Time is typically measured in discrete steps: t=0,1,2,..T. Where, T represents
    the final or terminal state. Hence it is a *finite discrete time* problem. There
    are also continuous time problems and also infinite horizon problems, but these
    are a lot tougher to handle mathematically. As a result the focus is on the finite
    discrete time model.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 时间通常以离散步骤来测量：t=0,1,2,..T。其中，T 代表最终或终端状态。因此，这是一个*有限离散时间*问题。还有连续时间问题以及无限视界问题，但这些在数学上处理起来更为复杂。因此，重点在于有限离散时间模型。
- en: '***State***'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '***状态***'
- en: This represents a snapshot of the environment and contains all the information
    pertaining to the environment. This includes information on rewards, how the environment
    will change with respect to actions, etc. In the case of a self-driving car, a
    state may represent a region around the car. Similarly, in the case of a robot
    moving forward, it would represent a snapshot of the environment once an action
    such as stepping forward has been taken.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这代表了环境的一个快照，包含了所有与环境相关的信息。这包括奖励信息、环境如何根据行动发生变化等。以自动驾驶汽车为例，状态可能代表汽车周围的一个区域。类似地，以前进中的机器人为例，它将代表在采取如前进等行动后环境的快照。
- en: 'For Markov purposes, the agent and environment interact in a series of discrete
    time steps: t=0,1,2…T. Where, T represents the final or terminal state. At each
    time step, t, the agent receives some representation of the environment encapsulated
    as Sₜ ∈ **S**,where **S** is the set of all states. In the context of MDPs, State
    is a discrete random variable.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于马尔可夫目的，代理和环境在一系列离散时间步骤中交互：t=0,1,2…T。其中，T 代表最终或终端状态。在每个时间步骤 t，代理接收环境的某种表示，封装为
    Sₜ ∈ **S**，其中 **S** 是所有状态的集合。在 MDP 的背景下，状态是一个离散随机变量。
- en: '***Rewards***'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '***奖励***'
- en: The agent receives a signal or reward Rₜ₊₁ from the environment in the next
    time period which is associated with how well the action is doing in terms of
    achieving the overall goal. The reward is typically defined as a scalar where
    Rₜ ∈ ***R*** ⊂ℝ is just the set of real numbers (example of complicated notation
    for something simple! But that’s the way things happen in statsworld😊).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 代理在下一时间周期从环境中接收到一个信号或奖励 Rₜ₊₁，这与行动在实现整体目标方面的表现相关。奖励通常定义为一个标量，其中 Rₜ ∈ ***R***
    ⊂ℝ 只是实数集合的一个例子（复杂的符号表示简单的东西！但在统计世界中就是这样😊）。
- en: Rewards can be both deterministic or a random variable. Sutton (2017) takes
    Rₜ to be a random variable, though most of the examples take a deterministic reward.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励可以是确定性的，也可以是随机变量。Sutton（2017）将 Rₜ 视为随机变量，尽管大多数示例采用确定性奖励。
- en: Now, Rₜ depends on the current state of the world, the action just taken, and
    the next state of the world, i.e., Rₜ = R(Sₜ, Aₜ, Sₜ₊₁). However, its frequently
    simplified to depend on the current state and action pair, Rₜ = R(Sₜ, Aₜ).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Rₜ 依赖于世界的当前状态、刚刚采取的行动和世界的下一个状态，即 Rₜ = R(Sₜ, Aₜ, Sₜ₊₁)。然而，它通常被简化为仅依赖于当前状态和行动对，即
    Rₜ = R(Sₜ, Aₜ)。
- en: Now that we have the basic components of the MDP in place, i.e., State, Reward,
    Agent, Environment, we need to better understand the nature of the MDP variables
    State, Reward, Agent, Environment in the context of probability and stochastic
    theory.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了 MDP 的基本组成部分，即状态、奖励、代理、环境，我们需要更好地理解在概率和随机理论背景下，MDP 变量状态、奖励、代理和环境的性质。
- en: So, let’s now jump to Probability and Stochastic theory…
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们跳到概率和随机理论……
- en: '**Section 2: Probability and Stochastic Basics**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: To define stochastic processes, we need to first understand what are random
    variables, probability distributions, joint distributions, marginal distribution,
    conditional distributions and the chain rule of probability.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Let us now spend some time understanding the above concepts. Once we understand
    these, we are going to come back to MDPs and understand how it works using these
    concepts.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Since stats and probability is vast, I would like to keep the focus on the applicability
    of these concepts to MDPs. So, I present the statistical concepts related to an
    MDP concept in *italics* and then explain its relevance to the MDP. Hopefully,
    this will make the MDP concept clearer.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '***Random variable***'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '*In probability, a random variable is a way of capturing the fact that a variable
    can take multiple values or occurrences. It represents a mapping or function that
    captures the fact that the variable can be mapped to many values. For example
    a toss of a coin has outcomes: {H,T} and we can map this to a number lying between
    0 and 1 signifying probability. See this* [*Wikpedia*](https://en.wikipedia.org/wiki/Random_variable)
    *page for a more formal definition. As an aside, I have found the Wikipedia pages
    for probability and statistics to be an excellent, if slightly advanced, resource.*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '*When the range of the random variable is countable, it is termed discrete
    and its distribution is a discrete probability mass function ( a concept that
    I will define later as it will be used a lot). If the variable is continuous,
    then its distribution can be defined by a probability distribution function.*'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '*The definition of a random variable is traditionally defined for real valued
    cases, denoted by* **R***. The definition can also be extended to any measurable
    set E which contains random Boolean values or categorical values, vectors, matrices
    or functions.*'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: This becomes important to note because in MDPs we have real valued random variables
    as well as categorical types. Also stochastic processes (to be defined a little
    later) are also a random function of time.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: In the context of the MDP defined above, we have State and Reward as random
    variables. Both variables are discrete and have a finite range.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '***State space or sample space:***'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '*When we talk of a random number, then the no. of finite number of states that
    a random variable can take is termed as the sample state or state space in the
    case of an MDP. The simplest example of a sample space is that for a throw of
    coin. It can take one of two values: heads or tails.*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 'In Sutton ch3, this is denoted by the set ***S*** which contains different
    states, where each state can be representative of a different situation. For example,
    we can have a stochastic process for going to office. The list of states that
    ***S*** could potentially contain:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '1\. s’: meets a jam on the way to commuting to office'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '2\. s: Meets clear roads to office'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '3\. s”: Meets a road block due to an accident'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: These states are what constitute the sample space or range of a random variable,
    i.e., the values it can take.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这些状态构成了样本空间或随机变量的范围，即它可以取的值。
- en: '***Stochastic Process*:**'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '***随机过程***：'
- en: '*A stochastic process is a collection or ensemble of random variables indexed
    by a variable t, usually representing time. So, to be clear, a stochastic process
    comprises the same random variable at multiple points of time. The stochastic
    process is typically indexed by the time variable (it can also be vector space,
    but let’s not confuse things). Thus, each index or point of time is associated
    by a specific random variable. Read more about stochastic processes on this* [*Wikipedia*](https://en.wikipedia.org/wiki/Stochastic_process)
    *page.*'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*随机过程是由变量 t（通常代表时间）索引的随机变量的集合或集成。明确地说，随机过程包括在多个时间点上的相同随机变量。随机过程通常由时间变量索引（它也可以是向量空间，但为了避免混淆，我们暂且不提）。因此，每个索引或时间点都与特定的随机变量相关联。了解更多关于随机过程的信息，请访问这个*
    [*维基百科*](https://en.wikipedia.org/wiki/Stochastic_process) *页面。*'
- en: '*This means that at every point of time, the stochastic random variable can
    realize one of the values in* ***S****.**A stochastic process can also be written
    as: S(t,i), t*∈T. *Where the State is a function of both, time as indexed by t,
    and i representing the specific state from the state space* ***S.***'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*这意味着在每个时间点，随机变量可以实现* ***S****.**中的一个值。随机过程也可以写作：S(t,i)，t*∈T。*其中状态是时间（由 t 索引）和代表状态空间***S***的特定状态
    i 的函数。*'
- en: The state space is defined using elements that reflect the different values
    that the stochastic process can take. So, in the case of MDPs, a state, Sₜ , is
    a stochastic random variable over time, indexed by t, where for each t, Sₜ can
    take values in the finite set ***S =***{s,s’,s’’’}.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 状态空间是通过反映随机过程可以取的不同值的元素来定义的。因此，在 MDP 的情况下，状态 Sₜ 是一个随时间变化的随机变量，由 t 索引，其中对于每个
    t，Sₜ 可以取值于有限集合 ***S =***{s,s’,s’’’}。
- en: 'The point of confusion regarding stochastic processes is that often the second
    index is skipped to avoid messy presentations. However, without it, the clarity
    that there are actually 2 things going on is lost:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 关于随机过程的困惑在于，通常跳过第二个索引以避免混乱的展示。然而，没有它的话，实际有 2 件事情正在发生的清晰度会丧失：
- en: 1\. Movement overtime indexed by t as the variable evolves
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 随着变量 t 的演变，随时间推移的运动
- en: 2\. At each point of time a choice of state from the set of states
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 在每个时间点上，从状态集合中选择一个状态
- en: Let us continue with our example of the road to illustrate the distinction between
    the stochastic process evolving over time and different state spaces. There are
    a finite number of state spaces which the stochastic variable can take at a point
    of time. So, for example, when we head out to office we can potentially meet with
    clear roads or a jam on the way for the first 15 minutes. In the next 15 minutes,
    the situation may change. Suppose that it requires 45 minutes to get to office,
    then we can break it up into 3 different time slots of 15 minutes. So, our time
    periods can be t=1,2,3.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续以道路为例，以说明随机过程随时间演变和不同状态空间之间的区别。在某个时间点上，随机变量可以取有限数量的状态空间。例如，当我们前往办公室时，在前
    15 分钟内，我们可能会遇到清晰的道路或堵车。接下来的 15 分钟，情况可能会发生变化。假设到达办公室需要 45 分钟，那么我们可以将其分为 3 个 15
    分钟的时间段。因此，我们的时间段可以是 t=1,2,3。
- en: There is a probability p of clear road, q of meeting a jam and r of meeting
    a road block in the first 15 minutes. In the next 15 minutes, one of these 3 outcomes
    can again occur and similarly for the last 15 minutes. This is evolution over
    time. This means that, at each observation at a certain time, there is a certain
    probability to get a certain outcome. This can be illustrated as below.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在前 15 分钟内，道路清晰的概率为 p，遇到堵车的概率为 q，遇到道路封闭的概率为 r。在接下来的 15 分钟内，这三种结果中的一种可能再次出现，最后
    15 分钟也是如此。这就是随时间演变。这意味着，在每个时间点上，存在获得某个结果的特定概率。可以通过以下方式进行说明。
- en: '![](../Images/e72ab0a6813c0777f742c6e210871e7a.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e72ab0a6813c0777f742c6e210871e7a.png)'
- en: 'Office drive MDP (Image source: author)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 办公室驱动 MDP（图像来源：作者）
- en: The image shows a sample trajectory. At time t=1, i.e., first 15 minutes, the
    road can be clear, jammed or blocked with respective probabilities. Similarly
    for t=2 or second 15 minutes and so on. At a point of time, say, first 15 minutes,
    we can see the road state can vary randomly across 3 possible states. This represents
    the randomness in state at a point of time. However, if we look across time t,
    then we have a trajectory. So, if we have 3 time points, we will also have 3 random
    variables, one associated with each point of time.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图像显示了一个样本轨迹。在时间t=1，即前15分钟，道路可能是畅通的、拥堵的或封闭的，各有相应的概率。类似地，对于t=2或第二个15分钟，以此类推。在某一时刻，比如前15分钟，我们可以看到道路状态可以随机变化于3种可能状态。这代表了在某一时刻状态的随机性。然而，如果我们看时间t，那么我们就有了一个轨迹。所以，如果我们有3个时间点，我们将有3个随机变量，每个时间点对应一个。
- en: We can also understand the reward set in this context. The reward set **R**
    in this case would be the time taken to cover the specific segment of the road
    at a particular time. There is a defined distribution of time (for simplicity
    we can keep it as discrete minutes) for each state. What does this mean? Basically,
    each state of the road would be associated with a distribution of time. It’s like
    sometimes we do the clear road (s) in 15 minutes and sometimes in 17 minutes due
    to some random variations and, at others in 18 minutes. So, we can define R(s)={15,16,17,18}.
    There is a probability distribution defined for the time taken to complete the
    segment with clear road conditions. This is an intuitive example of how the reward
    can also be random — something which most of the examples in Sutton (2017), Ch3
    do not cover. Each state of the road — for example, the jammed state — would have
    its own distribution over the ETA.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以在这种情况下理解奖励集。奖励集**R**在这种情况下就是在特定时间覆盖特定道路段所需的时间。每种状态都有一个定义的时间分布（为了简化我们可以以离散分钟为单位）。这意味着什么？基本上，道路的每种状态都与时间分布相关。就像有时我们在15分钟内完成清晰道路
    (s)，有时由于一些随机变化在17分钟，有时在18分钟。因此，我们可以定义R(s)={15,16,17,18}。对于清晰道路条件下完成路段所需时间有一个概率分布。这是奖励也可能是随机的一个直观示例——这是Sutton
    (2017)，Ch3中的大多数示例没有涵盖的内容。道路的每种状态——例如，拥堵状态——将有其自身的ETA分布。
- en: '***Realization or trajectory of a stochastic process***'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '***随机过程的实现或轨迹***'
- en: '*A stochastic process can have many* [*outcomes*](https://en.wikipedia.org/wiki/Outcome_(probability))*,
    due to its randomness. A single outcome of a stochastic process is called variously
    as a* [***realization***](https://en.wikipedia.org/wiki/Stochastic_process)***,
    episode or a trajectory****. It is formed by taking a single possible value of
    the random variable at each time point of the stochastic process as it evolves
    over time.*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*随机过程可以有许多* [*结果*](https://en.wikipedia.org/wiki/Outcome_(probability))*, 由于其随机性。随机过程的单个结果被称为*
    [***实现***](https://en.wikipedia.org/wiki/Stochastic_process)***、情节或轨迹****。它是通过在随机过程的每个时间点上取随机变量的一个可能值来形成的。*'
- en: 'So, to continue with our traffic example: a possible trajectory is shown in
    the image by the arrows. In the first 15 minutes, the motorist meets a clear road,
    in the next 15 minutes hits a jam and in the final 15 minutes hits a blocked road.
    So, a potential sample path would be:(s,s’,s’’). This is just one possible realization.
    The motorist traverses this path every day and is likely to meet a slightly different
    set of states every day; another day it can be (s’’,s’, s).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，继续我们的交通示例：图像中的箭头显示了一个可能的轨迹。在前15分钟，司机遇到了一条畅通的道路，在接下来的15分钟遇到了拥堵，而在最后15分钟遇到了封闭的道路。所以，一个潜在的样本路径可以是：(s,s’,s’’)。这只是一个可能的实现。司机每天都经历这条路径，并且很可能每天会遇到略微不同的状态集；另一天可能是
    (s’’,s’,s)。
- en: '*Stochastic processes are widely used for probabilistic representations of
    systems and phenomena that appear to vary in a random manner. Or, to put it more
    simply, stochastic processes or models are used to estimate the probability of
    various outcomes of a random variable which also changes overtime. Examples include
    the growth of a* [*bacterial*](https://en.wikipedia.org/wiki/Bacteria) *population,
    or the movement of a* [*gas*](https://en.wikipedia.org/wiki/Gas)[*molecule*](https://en.wikipedia.org/wiki/Molecule)*.
    They are also used extensively in financial analysis where stochastic models can
    be used to estimate situations involving uncertainty, such as investment returns,
    volatile markets, or inflation rates*'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*随机过程被广泛应用于对系统和现象的概率表示，这些系统和现象表现为随机变化。简单来说，随机过程或模型用于估算随时间变化的随机变量的各种结果的概率。例子包括*
    [*细菌*](https://en.wikipedia.org/wiki/Bacteria) *群体的增长，或* [*气体*](https://en.wikipedia.org/wiki/Gas)[*分子*](https://en.wikipedia.org/wiki/Molecule)*的运动。它们在金融分析中也被广泛使用，其中随机模型可用于估算涉及不确定性的情况，如投资回报、市场波动或通货膨胀率*'
- en: '*Further there are many different types of stochastic processes of which the
    Markov process is one type. Others, for example, include random walks, Gaussian
    processes, etc. Each stochastic process has its own specific assumptions and features
    which it is important to understand.*'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*此外，还有许多不同类型的随机过程，其中 Markov 过程是一种。其他类型包括随机游走、高斯过程等。每种随机过程都有其特定的假设和特征，这些特征很重要。*'
- en: '***Probability Distribution***'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '***概率分布***'
- en: Now we turn to the question of how do we get the probabilities of different
    states? Recall, in above example, we have defined the probabilities of meeting
    a jam and having clear road as q and p. Now, let us just elaborate a bit about
    what this means in terms of probability distributions and how they come about.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们转向如何获取不同状态的概率？回顾上面的例子，我们已经定义了遇到拥堵和清晰道路的概率分别为 q 和 p。现在，让我们稍微详细说明一下这在概率分布中的含义以及它们是如何产生的。
- en: The probability of a random outcome (road is jammed) is assessed as the proportion
    of times the outcome would occur in a very long series of repetitions. So, this
    is how we would assess p, q, r by noting the state of road variable at the specific
    time intervals specified across multiple days.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 随机结果（道路拥堵）的概率被评估为在非常长的重复序列中该结果发生的比例。因此，这就是我们如何通过记录多个天数中的道路状态变量在特定时间间隔的状态来评估
    p、q、r。
- en: '*Recording all these probabilities of outputs of a random variable,* Sₜ , *gives
    us the probability distribution of the random variable. In the discrete case it
    is termed the probability mass distribution.* *This is a function that provides
    the probability that a discrete random variable equals a particular value. Read
    more of this* [*here*](https://en.wikipedia.org/wiki/Probability_distribution)*.*'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*记录随机变量的所有这些输出概率*，Sₜ，*给我们提供了随机变量的概率分布。在离散情况下，这被称为概率质量分布。* *这是一个提供离散随机变量等于特定值的概率的函数。有关更多信息，请参阅*
    [*这里*](https://en.wikipedia.org/wiki/Probability_distribution)*。*'
- en: 'To continue with our office commute example: the State variable, can therefore
    take any of these values with a specific probability. For example if our state
    set for a road consists of 3 possible states: **S**= { s = clear, s’=jammed, s’’=blocked}.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 继续我们办公室通勤的例子：状态变量因此可以以特定的概率取这些值。例如，如果我们为一条道路定义的状态集包含 3 种可能的状态：**S**= { s = 清晰,
    s’= 拥堵, s’’= 阻塞}。
- en: The road can have any of these 3 states at a specific point of time (say morning)
    with a probability. For example, it can be free =.5, jammed =.4, and blocked =.1(blocking
    is relatively rarer state!). The set of probabilities is termed the probability
    distribution for the variable which is the state of the road.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在特定时间点（比如早晨），道路可以有这三种状态中的任何一种，且具有相应的概率。例如，它可以是自由 =0.5，拥堵 =0.4，阻塞 =0.1（阻塞相对较少见！）。这些概率的集合被称为该变量的概率分布，即道路的状态。
- en: '***Joint Distribution vs Marginal Distribution vs Conditional Distribution***'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '***联合分布 vs 边际分布 vs 条件分布***'
- en: '*In case of stochastic processes, since the random variable is the same but
    measured over different points of time, it is likely that state in t,* Sₜ , *is
    correlated to states in previous periods,* Sₜ₋₁, Sₜ₋₂. *In this case, then, if
    we want to understand the probability of* Sₜ *, it is the joint probability mass
    distribution (pmf) that is relevant. The joint pmf allows us to compute probabilities
    of events involving multiple random variables taking into account the relationship
    between the variables.*'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '*在随机过程的情况下，由于随机变量相同但在不同时间点测量，因此状态在 t,* Sₜ , *可能与之前的状态有关，即* Sₜ₋₁, Sₜ₋₂。*在这种情况下，如果我们想了解*
    Sₜ *的概率，则相关的是联合概率质量分布 (pmf)。联合 pmf 允许我们计算涉及多个随机变量的事件的概率，同时考虑变量之间的关系。*'
- en: '*Given two* [*random variables*](https://en.wikipedia.org/wiki/Random_variable)
    *that are defined on the same* [*probability space*](https://en.wikipedia.org/wiki/Probability_space)
    *the* [***joint probability distribution***](https://en.wikipedia.org/wiki/Joint_probability_distribution)
    *is the corresponding* [*probability distribution*](https://en.wikipedia.org/wiki/Probability_distribution)
    *on all possible pairs of outputs.*'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '*给定两个* [*随机变量*](https://en.wikipedia.org/wiki/Random_variable) *定义在相同的* [*概率空间*](https://en.wikipedia.org/wiki/Probability_space)
    *上，* [***联合概率分布***](https://en.wikipedia.org/wiki/Joint_probability_distribution)
    *是所有可能的输出对的相应* [*概率分布*](https://en.wikipedia.org/wiki/Probability_distribution)
    *。*'
- en: '*This can be defined more generally in Markov context as:*'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*这可以更一般地在马尔可夫背景下定义为：*'
- en: '*Suppose we consider a discrete time 2 period stochastic process,* Sₙ *, where
    n=1,2\. So,* S₁ and S₂ *are the two discrete stochastic random variables which
    can take values from the set i* ∈ S = {1,2}. *The joint distribution of* Sₙ is
    given for *every n, i.e., time periods and finite sequence of states:(*i₁,i₂)
    by:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*假设我们考虑一个离散时间 2 时间段随机过程，* Sₙ *, 其中 n=1,2。*所以，* S₁ 和 S₂ *是两个离散随机变量，可以从集合 i*
    ∈ S = {1,2} 中取值。* Sₙ 的联合分布给出每个 n，即时间段和有限状态序列的：(i₁,i₂) 为：'
- en: '![](../Images/3ee11bfd7563334600b682307bfac7e8.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ee11bfd7563334600b682307bfac7e8.png)'
- en: 'Let us look at our simple road example:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的简单道路示例：
- en: 'For simplicity assume that there are only 2 time periods — i.e., Sₜ and Sₜ₋₁.
    The state space is still 3: {clear( C ), jammed(J), blocked(B)}.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为简单起见，假设只有 2 个时间段，即 Sₜ 和 Sₜ₋₁。状态空间仍然是 3：{clear(C)，jammed(J)，blocked(B)}。
- en: 'The joint probability table would be as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 联合概率表如下：
- en: '![](../Images/b744db17148155938ea05c72804862fe.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b744db17148155938ea05c72804862fe.png)'
- en: 2 period office drive joint probability distribution ( source:author)
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 2 时间段办公室驾驶联合概率分布（来源：作者）
- en: The joint probability is the probability of two events occurring together. The
    table above provides the different combinations of instances of both events. Notice,
    since this is a stochastic process, time t-1 always comes first and then t.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 联合概率是两个事件同时发生的概率。上表提供了两个事件的不同实例组合。注意，由于这是一个随机过程，时间 t-1 总是先于 t。
- en: '***Marginal probability distribution***'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '***边际概率分布***'
- en: '*The* [*marginal distribution*](https://en.wikipedia.org/wiki/Joint_probability_distribution)
    *of a variable gives the probabilities of various values of the variables in the
    subset without reference to the values of the other variables. The marginal mass
    function for* Sᵢ is:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*变量的* [*边际分布*](https://en.wikipedia.org/wiki/Joint_probability_distribution)
    *给出了在子集中各种值的概率，而不参考其他变量的值。Sᵢ 的边际质量函数为：*'
- en: '![](../Images/7ed0b4f566946e349449eeb2156a2566.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ed0b4f566946e349449eeb2156a2566.png)'
- en: The marginal probability is the probability of a single event occurring. Essentially
    the marginal distribution is the column wise sum of probabilities. The equation
    for this requires you to sum over the column for j. The joint probability for
    any combination is given by the cell probabilities. In our example’s context,
    it is the probability of meeting a jam in Sₜ irrespective of what road conditions
    were in Sₜ₋₁, i.e., we sum the probabilities across the col J.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 边际概率是单个事件发生的概率。本质上，边际分布是按列的概率总和。这个方程要求你对 j 列进行求和。任何组合的联合概率由单元格概率给出。在我们示例的上下文中，它是
    Sₜ 中遇到交通堵塞的概率，而不考虑 Sₜ₋₁ 中的路况，即我们对 J 列的概率进行求和。
- en: '***Conditional probability***'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '***条件概率***'
- en: '*A* [*conditional probability*](https://en.wikipedia.org/wiki/Conditional_probability_distribution)
    *is that a specific event occurs given that another specific event has already
    occurred. This means the probability of a jam in* Sₜ *given that the road was
    clear in* Sₜ₋₁.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '*一个* [*条件概率*](https://en.wikipedia.org/wiki/Conditional_probability_distribution)
    *是指在另一个特定事件已经发生的条件下，某一特定事件发生的概率。这意味着在* Sₜ *的情况下，给定道路在* Sₜ₋₁ *时是畅通的情况下发生堵塞的概率。*'
- en: Now in our case, because Sₜ is a stochastic variable, and we are looking at
    a two period example, the joint and conditional probabilities are the same.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在我们的例子中，由于 Sₜ 是一个随机变量，并且我们考虑的是一个两个时期的例子，所以联合概率和条件概率是相同的。
- en: '*Mathematically, the conditional distribution of a variable given another variable
    is the joint distribution of both variables divided by the marginal distribution
    of the other variable.*'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*在数学上，给定另一个变量的条件下的变量分布是两个变量的联合分布除以另一个变量的边际分布。*'
- en: '*Defining this generally it is:*'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '*一般而言，定义为：*'
- en: '![](../Images/dde8178697faf0a7231e62d4bf85bd21.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dde8178697faf0a7231e62d4bf85bd21.png)'
- en: '*The generalization of this to case where we have n random variables,* X₁,..,
    Xₙ.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*将其推广到我们有 n 个随机变量的情况，* X₁,.., Xₙ。'
- en: '*The joint probability distribution of these n variables is:*'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*这 n 个变量的联合概率分布是：*'
- en: '![](../Images/2d293c3740f6f14ecbe1a7a3ee91fb29.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d293c3740f6f14ecbe1a7a3ee91fb29.png)'
- en: '*This can be written as the conditional function times the marginal function.
    This is based on the* [*chain rule of probability*](https://en.wikipedia.org/wiki/Chain_rule_(probability))*.
    In terms of notation,* Pₓ *(*x₁) = P(X=x₁). *For the 2 random variables,* X₁,
    X₂ *the joint distribution can be written as:*'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '*这可以写成条件函数乘以边际函数。这是基于* [*概率链式法则*](https://en.wikipedia.org/wiki/Chain_rule_(probability))
    *的。在符号表示上，* Pₓ *（*x₁）= P(X=x₁)。* 对于两个随机变量* X₁, X₂ *，其联合分布可以表示为：*'
- en: '![](../Images/831a66b22750e3ca7f6ad95e7465229c.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/831a66b22750e3ca7f6ad95e7465229c.png)'
- en: '*Stated in words: the joint probability distribution of 2 variables* X₁, X₂
    *taking specific values* x₁, x₂ *can be given by the conditional probability of*
    X₂=x₂ *given that* X₁=x₁, *times the marginal probability of getting* X₁=x₁.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '*用言语表述：两个变量* X₁, X₂ *取特定值* x₁, x₂ *的联合概率分布可以表示为：* X₂=x₂ *在给定* X₁=x₁ *的条件下的条件概率，乘以*
    X₁=x₁ *的边际概率。*'
- en: '***Why do we need these distributions — deriving the need for the Markov Property***'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '***我们为什么需要这些分布——推导Markov性质的需求***'
- en: Joint, conditional and marginal probability distribution definitions become
    important in the context of MDPs when we want to determine the probabilities of
    the State variable taking specific values as it transitions across different time
    periods. Our example is simple and consists of 3 time periods. But, what if the
    time period was 100, and we wanted to understand the probability of the state
    taking a specific value in the 100th time period? This is the underlying probability
    problem. Let’s look at how the general case is solved and then what the Markov
    property is
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 联合、条件和边际概率分布定义在MDP（马尔可夫决策过程）背景下变得重要，当我们想确定状态变量在不同时间段过渡时取特定值的概率时。我们的例子很简单，由三个时间段组成。但是，如果时间段是100，我们想了解状态在第100个时间段取特定值的概率怎么办？这就是潜在的概率问题。让我们看看一般情况如何解决，然后了解Markov性质。
- en: '*For the more general case, to get the joint probability of* X₁,.., Xₙ *random
    variables taking specific values,* x₁,.., xₙ *can be found by the conditional
    probability of* Xₙ *given the entire history of previous random variables taking
    specific values, times the joint probability distribution of the n-1th distribution.*'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*对于更一般的情况，要获得* X₁,.., Xₙ *随机变量取特定值* x₁,.., xₙ *的联合概率，可以通过* Xₙ *给定之前所有随机变量取特定值的条件概率，乘以前
    n-1 次分布的联合概率分布来找到。*'
- en: '![](../Images/32c29c24137d4886665587e377268d89.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/32c29c24137d4886665587e377268d89.png)'
- en: '*We can open out the expression for joint probability of P(*Xₙ₋₁=xₙ₋₁…,X₁=x₁)
    *again into an expression for conditional probability of* Xₙ₋₁ *given the history
    of (*Xₙ₋₂,…,X₁) *and the joint probability of (*X₁,..,Xₙ₋₂).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们可以将 P(*Xₙ₋₁=xₙ₋₁…,X₁=x₁)* 的联合概率表达式再次展开为* Xₙ₋₁ *给定（*Xₙ₋₂,…,X₁*）的条件概率和（*X₁,..,Xₙ₋₂*）的联合概率。*'
- en: '*This can be further unrolled backwards. We will finally get the expression
    for a sequence of conditional probabilities and the marginal probability of the
    first variable P(*X₁=x₁)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '*这可以进一步向后展开。最终，我们将得到条件概率序列和第一个变量 P(*X₁=x₁) 的边际概率的表达式。*'
- en: '![](../Images/91af8fb4cd7f3e847a7d8ded58ae7481.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91af8fb4cd7f3e847a7d8ded58ae7481.png)'
- en: Now let us translate this to the MDP context. So far, this has been couched
    in terms of any sequence of random variables. However, this result translates
    easily to stochastic variables and Markov processes. We just need to replace Xᵢ
    by Sₜ and now the indexing would be in terms of time, t=1,2,..n instead of Xᵢ
    = xᵢ.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将其翻译到MDP的背景中。到目前为止，这已经以任何随机变量序列的形式呈现。然而，这个结果很容易转化为随机变量和马尔可夫过程。我们只需将Xᵢ替换为Sₜ，现在索引将以时间的形式出现，即t=1,2,..n，而不是Xᵢ
    = xᵢ。
- en: Why am I belaboring the obvious? Because there is a lot of material out there
    that uses different variables and indices for Markov processes and other sequences
    of random variables and this can get hugely confusing. I tried to unravel this
    for myself and hence am writing it down, so that it can help other non-statisticians
    who are trying to figure out the probability theory behind Markov results.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我为什么要重复这些明显的内容？因为有很多材料使用不同的变量和索引来描述马尔可夫过程和其他随机变量序列，这可能会非常混乱。我尝试为自己解开这些复杂的概念，因此写下这些内容，以帮助那些试图弄清楚马尔可夫结果背后的概率理论的非统计学家。
- en: For analyzing the MDP, t=1,..,n and states xᵢ ∈ S. Just to be clear xₙ is a
    specific value of a state at time period t=n.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析MDP时，t=1,..,n，状态xᵢ ∈ S。为了明确，xₙ是时间段t=n时的一个特定状态值。
- en: Let’s write the joint probability of getting S₁,..,Sₙ as the state variable
    evolves over t=1..n. Substituting in the general case discussed above we get the
    joint probability as a sequence of conditional probabilities and the marginal
    distribution of S₁.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们写出状态变量在t=1..n的演变过程中，获得S₁,..,Sₙ的联合概率。代入上述讨论的一般情况，我们得到的联合概率是条件概率的序列和S₁的边际分布。
- en: '![](../Images/e219768fe3b276680ffb69ef9a7f5725.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e219768fe3b276680ffb69ef9a7f5725.png)'
- en: 'To put it in words: the probability of getting states S₁,..,Sₙ with specific
    values is equal to the conditional probability of the state at time t=n having
    a value xₙ given the historical values of previous states times the joint distribution
    of the n-1th period. This unrolls into a sequence of conditional probabilities
    and the marginal or initial distribution of state at S₁.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 用语言描述就是：获得状态S₁,..,Sₙ的概率等于在时间t=n时状态的值为xₙ的条件概率，乘以前n-1期的联合分布。这展开为一系列条件概率和状态S₁的边际或初始分布。
- en: Now this is just a stochastic process where the value of a state in time period
    n depends on the past history of state values. This is clearly intractable to
    compute, hence the Markov property
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这只是一个随机过程，其中状态在时间段n的值依赖于过去状态值的历史。这显然是难以计算的，因此需要马尔可夫性质。
- en: '***Markov Property:***'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '***马尔可夫性质:***'
- en: 'The Markov property makes a state Sₙ depend on only the immediately preceding
    state, Sₙ₋₁, rather than the entire historical backlog of states. In this case
    our conditional probability for state Sₙ reduces to:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫性质使得状态Sₙ只依赖于紧接前一个状态Sₙ₋₁，而不是整个历史状态的积累。在这种情况下，我们的状态Sₙ的条件概率减少为：
- en: '![](../Images/fa4686a7d953c456ec5748e0a61e7e2e.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa4686a7d953c456ec5748e0a61e7e2e.png)'
- en: However, we still have n-1 one step conditional probabilities to be estimated
    along with the initial probability distribution, *P(*X₁=x₁). Still somewhat complex,
    as there are a large number of conditional probabilities to be calculated. A further
    assumption that is made to simplify and make the calculations more tractable is
    time homogeneity of transition probabilities.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们仍然需要估计n-1个一步条件概率以及初始概率分布，*P(*X₁=x₁)。这仍然比较复杂，因为需要计算大量的条件概率。为了简化并使计算更可行，进一步的假设是过渡概率的时间同质性。
- en: '***Time homogeneity of transition probabilities***'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '***过渡概率的时间同质性***'
- en: '*This basically says that the probability of transitioning from state i in
    time period n to state j in period n+1 is the same irrespective of the time periods:*'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '*这基本上说的是：从时间段n的状态i过渡到时间段n+1的状态j的概率是相同的，不受时间段的影响：*'
- en: '![](../Images/7252b0a97eedde66e56d3017a4a6eaa1.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7252b0a97eedde66e56d3017a4a6eaa1.png)'
- en: '*What this means is that the probabilities for transitioning from one state
    to the next is fixed, irrespective of time.*'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*这意味着从一个状态过渡到下一个状态的概率是固定的，不受时间的影响。*'
- en: To put it in context of our road example, the probability of going from clear
    road to jammed is fixed irrespective of whether we meet these 2 states in the
    first 2 time periods or the last 2 time periods. So, the probability of transition
    reduces to the following 9 cases irrespective of time. Where C= clear, j=jammed
    and B=blockled.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 放到我们的道路示例中，从清晰道路到拥堵道路的概率是固定的，无论我们在前 2 个时间段还是最后 2 个时间段遇到这 2 个状态。因此，转移概率简化为以下
    9 种情况，与时间无关。其中 C=清晰，j=拥堵，B=封堵。
- en: '![](../Images/607a0cf49440b45aa8727f0ec37ec842.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/607a0cf49440b45aa8727f0ec37ec842.png)'
- en: As you can see, if the number of states is less than the time period, as is
    more likely, this will significantly reduce the probabilities to be calculated.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，如果状态的数量少于时间周期，通常这种情况会显著减少需要计算的概率。
- en: '***Transition probabilities***'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '***转移概率***'
- en: '*The state* [*transition probability matrix*](https://en.wikipedia.org/wiki/Discrete-time_Markov_chain)
    *of a Markov chain gives the probabilities of transitioning from one state to
    another in a single time step.*'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '*状态* [*转移概率矩阵*](https://en.wikipedia.org/wiki/Discrete-time_Markov_chain) *给出了在单一步骤中从一个状态转移到另一个状态的概率。*'
- en: '*For t=1,2 states case this reduces the dimensions of the problem to*'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '*对于 t=1,2 状态情况，这将把问题的维度简化为*'
- en: '![](../Images/17300716c49a49e0bad004b4fc071bb3.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17300716c49a49e0bad004b4fc071bb3.png)'
- en: 'This breaks up into the following components:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这分解为以下几个组成部分：
- en: '*The conditional probability:*'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '*条件概率：*'
- en: '![](../Images/8174f8ebfb168c1ee0e42311baec22df.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8174f8ebfb168c1ee0e42311baec22df.png)'
- en: '*The initial distribution or marginal distribution: P(*S₁=x₁)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '*初始分布或边际分布：P(*S₁=x₁)*'
- en: 'In our road example, this was the initial probabilities for the 3 states: p,q,r.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的道路示例中，这些是 3 个状态的初始概率：p、q、r。
- en: So, all we need is the one step conditional probabilities and the initial distribution
    of states to get the joint probability of transitioning to a state.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们所需的仅仅是一步条件概率和状态的初始分布，以获得转移到某个状态的联合概率。
- en: '*The transition matrix can be represented as below:*'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '*转移矩阵可以表示如下：*'
- en: '![](../Images/a184600677e45bc59986b9fad4c3c4bc.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a184600677e45bc59986b9fad4c3c4bc.png)'
- en: 'Source: [Wikipedia Stochastic Matrix](https://en.wikipedia.org/wiki/Stochastic_matrix#:~:text=In%20mathematics%2C%20a%20stochastic%20matrix,substitution%20matrix%2C%20or%20Markov%20matrix.)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[维基百科 随机矩阵](https://en.wikipedia.org/wiki/Stochastic_matrix#:~:text=In%20mathematics%2C%20a%20stochastic%20matrix,substitution%20matrix%2C%20or%20Markov%20matrix.)
- en: '*In the transition matrix the rows represent the current state and the columns
    the future state. Thus* p₁₂ *is the probability of transitioning from state 1
    in time t to state 2 in t+1.*'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '*在转移矩阵中，行代表当前状态，列代表未来状态。因此* p₁₂ *是从时间 t 的状态 1 转移到时间 t+1 的状态 2 的概率。*'
- en: '*Also note the sum of all branch probabilities from a state has to be one.
    This is logical as you have to transition from a state to another one. Mathematically
    this is written as:*'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '*另外，请注意从一个状态出发的所有分支概率之和必须为 1。这是逻辑上的，因为你必须从一个状态转移到另一个状态。数学上可以写作：*'
- en: '![](../Images/a8e31f02b2165753041bbf20c42822b1.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8e31f02b2165753041bbf20c42822b1.png)'
- en: '*Why do we sum over ‘j’? Because this represents the possible states that you
    can transition to from, ‘i’.*'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '*为什么我们对‘j’进行求和？因为这代表了你可以从‘i’转移到的可能状态。*'
- en: '***Markov Chain dynamics***'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '***马尔可夫链动态***'
- en: For a Markov chain, we use the word dynamics to describe the variation of the
    state over a short time interval starting from a given initial state. The evolution
    of a Markov chain is a random process and so we cannot say exactly what sequence
    of states will follow the initial state. In reinforcement learning we wish predict
    the future states given the current state or initial state. A prediction of the
    future state, Sₙ given S₁ can be calculated easily once we have the probability
    transition matrix and the initial distribution. The n -step transition probability
    matrix can be found by multiplying the single-step probability matrix by itself
    n times.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 对于马尔可夫链，我们使用“动态”一词来描述在给定初始状态下，状态在短时间间隔内的变化。马尔可夫链的演变是一个随机过程，因此我们不能确切地说初始状态之后会跟随什么状态序列。在强化学习中，我们希望根据当前状态或初始状态预测未来状态。一旦我们有了概率转移矩阵和初始分布，可以很容易地计算出给定
    S₁ 的未来状态 Sₙ 的预测。n 步转移概率矩阵可以通过将单步概率矩阵自乘 n 次得到。
- en: '![](../Images/8f6bd37c2c6ba38902a8477428014508.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f6bd37c2c6ba38902a8477428014508.png)'
- en: This is the beauty of the Markov system and why it is used in reinforcement
    learning. Once we have a tractable set of transition probabilities, we can calculate
    the probability of transitioning from any initial state to state j in n steps.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是马尔可夫系统的美妙之处，以及它为何在强化学习中被使用。一旦我们拥有一个可处理的转移概率集，我们就可以计算从任何初始状态到状态j在n步内的转移概率。
- en: Now after this very long digression into the stochastic and probability behind
    MDPs, I will briefly review the MDP equations in Sutton (2017), ch 3.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在对MDP背后的随机和概率进行如此长时间的离题讨论之后，我将简要回顾Sutton（2017）第3章中的MDP方程。
- en: '**Section 3: Understanding Sutton Ch3 MDP Equations Using Probability and Stochastic
    Concepts**'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**第3节：使用概率和随机概念理解Sutton第3章的MDP方程**'
- en: '***The MDP Model***'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '***MDP模型***'
- en: In this section I briefly define the MDP giving what is essentially the textbook
    definition. In the next section we will look at the statistical theory underlying
    MDPs.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我简要定义了MDP，给出本质上是教科书中的定义。在下一节中，我们将探讨MDP的统计理论。
- en: '*The MDP framework is a considerable abstraction of the problem of goal-directed
    learning from interaction. It proposes that whatever the details of the sensory,
    memory, and control apparatus, and whatever objective one is trying to achieve,
    any problem of learning goal-directed behavior can be reduced to three signals
    passing back and forth between an agent and its environment: one signal to represent
    the choices made by the agent (the actions), one signal to represent the basis
    on which the choices are made (the states), and one signal to define the agent’s
    goal (the rewards). This framework may not be sufficient to represent all decision-learning
    problems usefully, but it has proved to be widely useful and applicable.*'
  id: totrans-183
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*MDP框架是一个关于目标导向学习从交互中问题的显著抽象。它提出，无论感知、记忆和控制装置的细节如何，以及目标是什么，任何目标导向行为的学习问题都可以归结为在代理和环境之间来回传递的三个信号：一个信号表示代理所做的选择（行动），一个信号表示选择的依据（状态），一个信号定义代理的目标（奖励）。这个框架可能不足以有效地表示所有决策学习问题，但它被证明在广泛的应用中非常有用。*'
- en: '*Sutton ch2, p50*'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '*Sutton第2章，第50页*'
- en: Most of the MDP framework can actually be understood at a high level without
    detailed knowledge of probability and stochastic theory. However, now that we
    have exposure to those concepts we can actually appreciate what is going on and
    its complexity. Let us now look at some of the equations related to the Markov
    process that are presented in Sutton (2017), ch3.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，大部分MDP框架可以在没有详细的概率和随机理论知识的情况下高层次地理解。然而，现在我们已经接触到这些概念，我们可以真正欣赏到其中的复杂性。让我们现在看看Sutton（2017）第3章中与马尔可夫过程相关的一些方程。
- en: '***Markov Property***'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '***马尔可夫性质***'
- en: 'This is essentially the idea that the future is independent of the past, given
    the present. It is conceptualized probabilistically as:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这本质上是未来在给定现在的情况下与过去无关的观点。它在概率上被概念化为：
- en: '![](../Images/e8419e256fd14c81e1b587212d392bd8.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e8419e256fd14c81e1b587212d392bd8.png)'
- en: 'This has already been discussed above at length and enables us to understand
    how the basic Markov problem can be made more tractable. Stochastic processes
    that satisfy the Markov property are typically much simpler to analyze than incorporation
    of the historical process in models. In practice, it is important to consider
    whether the Markov property will hold. For many scenarios, for example, the position
    of a car, the present location of the car will determine where it goes in future
    states. But, in some instances, history maybe important: for example, a trading
    agent may have to consider past trends in a share as well as the present price
    to determine whether to buy or sell.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这已经在上文中详细讨论过，使我们能够理解基本的马尔可夫问题如何变得更容易处理。满足马尔可夫性质的随机过程通常比将历史过程纳入模型中更简单分析。在实践中，考虑马尔可夫性质是否成立是很重要的。例如，对于汽车的位置来说，汽车的当前所在位置将决定它未来的状态。但在某些情况下，历史可能很重要：例如，交易代理可能需要考虑股票的过去趋势以及当前价格，以决定是否买入或卖出。
- en: 'A Markov process is a stochastic process with the following properties:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫过程是具有以下性质的随机过程：
- en: (a.) The number of possible outcomes or states is finite
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: (a.) 可能的结果或状态的数量是有限的
- en: This is there for analytical convenience. Infinite processes can also be handled
    with some minor manipulation. However, for time dependent processes, we assume
    time is finite t=0,1,2…,T.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是为了分析的便利。无限过程也可以通过一些小的操作来处理。然而，对于时间依赖的过程，我们假设时间是有限的 t=0,1,2,…,T。
- en: (b.) The outcome at any stage depends only on the outcome of the previous stage
    — given by Markov property discussed above.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: (b.) 任何阶段的结果仅依赖于前一阶段的结果 — 由上述讨论的马尔可夫性质给出。
- en: (c.) The transition probabilities are constant over time.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: (c.) 转移概率随时间保持不变。
- en: (d.) The system is stationary — this is not actually called out, but property
    c) actually implies it.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: (d.) 系统是平稳的 — 这实际上没有明确说明，但属性 c) 实际上隐含了这一点。
- en: '***Formal representation of MDP***'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '***MDP 的正式表示***'
- en: 'Formally the MDP can be represented as follows. Given, the Markov property,
    an MDP is defined as follows: a tuple which typically contains 5 elements:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 正式地，MDP 可以表示如下。鉴于马尔可夫性质，MDP 定义如下：一个通常包含 5 个元素的元组：
- en: '![](../Images/060e9627a0855438bb93867e50909331.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/060e9627a0855438bb93867e50909331.png)'
- en: '**S**: Set of states in the environment: *this is actually a stochastic process*'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '**S**：环境中的状态集合：*这实际上是一个随机过程*'
- en: '**A**: Set of actions possible in each state'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**A**：每个状态下可能的动作集合'
- en: '**R:** Reward function in response to actions: *This is normally deterministic,
    but sometimes can be stochastic as well*'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**R:** 对动作的奖励函数：*这通常是确定性的，但有时也可以是随机的*'
- en: '**p :** Probability matrix of transitioning from one state to the next — This
    is a critical part of an MDP. It is defined by the joint and conditional distribution
    of the state variable.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**p :** 从一个状态转移到下一个状态的概率矩阵 — 这是 MDP 的关键部分。它由状态变量的联合和条件分布定义。'
- en: '**γ :** Discount factor to be applied for rewards in the distant future'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '**γ :** 适用于远期奖励的折扣因子'
- en: 'The MDP and agent together thereby give rise to a sequence or trajectory that
    begins like this:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: MDP 和智能体共同产生一个序列或轨迹，如下所示：
- en: '![](../Images/c3dfa5301b2d5039c76ef0268da018a2.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3dfa5301b2d5039c76ef0268da018a2.png)'
- en: This process is a stochastic process and is finite, i.e., has T time steps with
    a clear terminal state.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程是一个随机过程且是有限的，即有 T 个时间步骤，并且有一个明确的终止状态。
- en: '***Understanding equations***'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '***理解方程***'
- en: '*Equation 3.2*'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '*方程 3.2*'
- en: '![](../Images/dda8c3e2e3878d545cc3ee992cfd1ba7.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dda8c3e2e3878d545cc3ee992cfd1ba7.png)'
- en: This equation tells us that Rₜ and Sₜ are random variables with discrete probability
    mass distributions which depend on the previous state and action. Equation 3.2
    defines the dynamics function p. This is a function that takes in 4 arguments.
    It also specifies a conditional distribution for each choice of state and action.
    For a specific value of state s’ and reward r, we can estimate the probability
    of these values occurring at time t, given the preceding state and action values
    at t-1.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 该方程告诉我们 Rₜ 和 Sₜ 是具有离散概率质量分布的随机变量，这些分布依赖于先前的状态和动作。方程 3.2 定义了动态函数 p。这是一个包含 4 个参数的函数。它还为每种状态和动作选择指定了条件分布。对于特定的状态
    s’ 和奖励 r，我们可以估计在时间 t 这些值发生的概率，给定在 t-1 时刻的前状态和动作值。
- en: '*Equation 3.3*'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '*方程 3.3*'
- en: '![](../Images/35cffa20d2bc48f2491305f3080fb469.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/35cffa20d2bc48f2491305f3080fb469.png)'
- en: Equation 3.3 basically tells us that for each value of ‘s’ and ‘a’, there is
    a joint distribution of states and rewards. This is basically a joint conditional
    distribution, hence it must sum to 1\. The joint distribution will have the probabilities
    of different combinations of s’ and r. The sum of all probabilities of all combinations
    must sum to 1 ([refer to Wikipedia](https://en.wikipedia.org/wiki/Joint_probability_distribution)).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 3.3 基本上告诉我们，对于每个“s”和“a”的值，都有一个状态和奖励的联合分布。这基本上是一个联合条件分布，因此它的和必须为 1。联合分布将具有不同组合的
    s’ 和 r 的概率。所有组合的概率之和必须为 1（[参见维基百科](https://en.wikipedia.org/wiki/Joint_probability_distribution)）。
- en: 'Let’s go back to our road example. At each state of the road, we have two actions:
    drive or stop. Then, if we are in state ‘jammed’ and we choose to drive. Then
    conditional on these two, there is a joint distribution of states and rewards
    for the next state.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到之前的道路示例。在道路的每个状态下，我们有两个动作：驾驶或停止。如果我们处于“拥堵”状态并选择驾驶，那么在这两者的条件下，将有一个下一状态的状态和奖励的联合分布。
- en: '*State transition probabilities — Equation 3.4*'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '*状态转移概率 — 方程 3.4*'
- en: '![](../Images/f20656f25b96ae18ef0a61e75f836266.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f20656f25b96ae18ef0a61e75f836266.png)'
- en: 'The state transition probabilities are given by summing over reward probabilities
    (note for the probability challenged: like a marginal probability distribution
    of states- [refer to Wikipedia](https://en.wikipedia.org/wiki/Joint_probability_distribution)).'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 状态转移概率通过对奖励概率进行求和来给出（请注意，对于概率挑战：类似于状态的边际概率分布 - [参见维基百科](https://en.wikipedia.org/wiki/Joint_probability_distribution)）。
- en: '*Expected Rewards — Equation 3.5*'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '*期望奖励 — 方程3.5*'
- en: '![](../Images/3d16279d0fa3436d03788c23e817da7c.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d16279d0fa3436d03788c23e817da7c.png)'
- en: 'Here we are again working with equation 3.2\. This time we sum over the probability
    of states (note for the probability challenged: like a marginal probability distribution
    of rewards — [refer to Wikipedia](https://en.wikipedia.org/wiki/Joint_probability_distribution)).
    This gives us the marginal probability of rewards distribution (expression on
    extreme RHS of 3.5). We then multiply this probability by the actual rewards,
    r to get the expected rewards.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们再次处理方程3.2。这次我们对状态的概率进行求和（对于概率不太清楚的人：类似于奖励的边际概率分布 — [参见Wikipedia](https://en.wikipedia.org/wiki/Joint_probability_distribution)）。这给出了奖励分布的边际概率（3.5的极端右侧表达式）。然后我们将这个概率乘以实际奖励r，以获得期望奖励。
- en: '*Expected rewards for state–action–next-state triples — Equation 3.6*'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '*期望奖励的状态-动作-下一状态三元组 — 方程3.6*'
- en: '![](../Images/a79bbafe62dcd3eaceb9c8f08eea3eb3.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a79bbafe62dcd3eaceb9c8f08eea3eb3.png)'
- en: This one is a little trickier. Probability of reward and state joint probability
    conditional on ‘s’ and ‘a’ divided by state transition probability . This is an
    application of chain rule of joint distributions (refer [Wikipedia](https://en.wikipedia.org/wiki/Chain_rule_(probability))).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这个有点棘手。奖励和状态的联合概率条件在‘s’和‘a’下除以状态转移概率。这是联合分布链式法则的一个应用（参见 [Wikipedia](https://en.wikipedia.org/wiki/Chain_rule_(probability))）。
- en: '![](../Images/403725b86d34581c1cfbaf9ef844b799.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/403725b86d34581c1cfbaf9ef844b799.png)'
- en: Let’s substitute our terms.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们代入我们的术语。
- en: '![](../Images/c03c5419d93ff42848065fc7eff84bb1.png)![](../Images/8accbbba3b261f8518cac930ab112523.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c03c5419d93ff42848065fc7eff84bb1.png)![](../Images/8accbbba3b261f8518cac930ab112523.png)'
- en: We can now take expectations over Rₜ
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以对Rₜ取期望值
- en: '![](../Images/01fa099b84e858904ef3bb2a6430d147.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01fa099b84e858904ef3bb2a6430d147.png)'
- en: So, this completes the basic equations derived for the set up of the MDP. It
    is important to understand each of 3.2–3.6 as Sutton(2017) mention that each of
    these is a way to represent the MDP and would be used again in subsequent chapters.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这完成了MDP设置的基本方程的推导。理解3.2到3.6每一个方程都很重要，因为Sutton（2017年）提到这些方程都是表示MDP的一种方式，并且会在后续章节中再次使用。
- en: '**To Conclude..**'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结..**'
- en: Well friends, this has been quite a journey on probability and stochastic theory.
    However, I hope what you have realized is the complexity and beauty of the underlying
    Markov system. Without understanding these, it is difficult to figure out the
    equations that underlie the basic MDP equations. Without understanding these equations
    and how they are derived, it is likely that one will become totally lost when
    it comes to policy, value functions and Q values. These will be picked up in the
    next installment of this tutorial.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 好朋友们，这一段关于概率和随机理论的旅程已经相当长了。然而，我希望你们意识到的是基础Markov系统的复杂性和美丽。如果不理解这些内容，很难弄清楚基本MDP方程的底层方程。如果不理解这些方程及其推导方式，当涉及到策略、价值函数和Q值时，很可能会完全迷失。下一部分教程中会涉及到这些内容。
- en: 'I hope you liked my writing. Please consider following me for more such articles
    as I proceed with reinforcement learning: [https://medium.com/@shaileydash](https://medium.com/@shaileydash)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你喜欢我的写作。请考虑关注我，以便获取更多类似的文章，我会继续撰写关于强化学习的内容：[https://medium.com/@shaileydash](https://medium.com/@shaileydash)
- en: Also do let me know your inputs and comments in the comments which will help
    me revise this.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 也请在评论中告诉我你的意见和建议，这将帮助我修改内容。
- en: 'You can follow me on Linkedin for more corporate oriented articles on AI and
    DS: [www.linkedin.com/in/shailey-dash-phd-8b4a286](http://www.linkedin.com/in/shailey-dash-phd-8b4a286)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在Linkedin上关注我，获取更多关于AI和数据科学的企业导向文章：[www.linkedin.com/in/shailey-dash-phd-8b4a286](http://www.linkedin.com/in/shailey-dash-phd-8b4a286)
- en: '**References**'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: I am listing quite a long list of stats and probability theory resources. They
    are all excellent, if difficult.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我列出了一份相当长的统计学和概率理论资源列表。这些资源都非常优秀，但也相当困难。
- en: 'Richard S. Sutton and Andrew G. Barto. [Reinforcement Learning: An Introduction;
    2nd Edition](http://incompleteideas.net/book/bookdraft2017nov5.pdf). 2017.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: Richard S. Sutton 和 Andrew G. Barto. [《强化学习：导论；第2版》](http://incompleteideas.net/book/bookdraft2017nov5.pdf)。2017年。
- en: '[https://en.wikipedia.org/wiki/Markov_decision_process](https://en.wikipedia.org/wiki/Markov_decision_process)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Markov_decision_process](https://en.wikipedia.org/wiki/Markov_decision_process)'
- en: '[https://en.wikipedia.org/wiki/Stochastic_process](https://en.wikipedia.org/wiki/Stochastic_process)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Stochastic_process](https://en.wikipedia.org/wiki/Stochastic_process)'
- en: '[https://en.wikipedia.org/wiki/Joint_probability_distribution](https://en.wikipedia.org/wiki/Joint_probability_distribution)'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Joint_probability_distribution](https://en.wikipedia.org/wiki/Joint_probability_distribution)'
- en: '[https://en.wikipedia.org/wiki/Conditional_probability_distribution](https://en.wikipedia.org/wiki/Conditional_probability_distribution)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Conditional_probability_distribution](https://en.wikipedia.org/wiki/Conditional_probability_distribution)'
- en: '[https://www.kent.ac.uk/smsas/personal/lb209/files/notes1.pdf](https://www.kent.ac.uk/smsas/personal/lb209/files/notes1.pdf)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.kent.ac.uk/smsas/personal/lb209/files/notes1.pdf](https://www.kent.ac.uk/smsas/personal/lb209/files/notes1.pdf)'
- en: Lawler, G.F. (2006). Introduction to Stochastic Processes (2nd ed.). Chapman
    and Hall/CRC. [https://doi.org/10.1201/9781315273600](https://doi.org/10.1201/9781315273600)
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: Lawler, G.F. (2006). 随机过程导论（第2版）。Chapman and Hall/CRC。 [https://doi.org/10.1201/9781315273600](https://doi.org/10.1201/9781315273600)
- en: 'Yates, R.D. and Goodman, D.J., 2014\. *Probability and stochastic processes:
    a friendly introduction for electrical and computer engineers*. John Wiley & Sons.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: Yates, R.D. 和 Goodman, D.J., 2014\. *概率与随机过程：电气与计算机工程师的友好介绍*。John Wiley & Sons.
- en: Unnikrishna Pillai, Probability and Stochastic Processes, Lecture 8, NYU, Tandon
    school of Engineering, [https://www.youtube.com/watch?v=nFF6eRQT2-c&t=455s](https://www.youtube.com/watch?v=nFF6eRQT2-c&t=455s)
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: Unnikrishna Pillai, 概率与随机过程，第8讲，NYU Tandon 工程学院，[https://www.youtube.com/watch?v=nFF6eRQT2-c&t=455s](https://www.youtube.com/watch?v=nFF6eRQT2-c&t=455s)
