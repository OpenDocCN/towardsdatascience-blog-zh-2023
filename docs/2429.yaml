- en: 3 Use-Cases for Gaussian Mixture Models (GMM)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://towardsdatascience.com/3-use-cases-for-gaussian-mixture-model-gmm-72951fcf8363?source=collection_archive---------1-----------------------#2023-07-27](https://towardsdatascience.com/3-use-cases-for-gaussian-mixture-model-gmm-72951fcf8363?source=collection_archive---------1-----------------------#2023-07-27)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Feature engineering, unsupervised classification, and anomaly detection with
    the versatility of the GMM algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://viyaleta.medium.com/?source=post_page-----72951fcf8363--------------------------------)[![Viyaleta
    Apgar](../Images/8d8fd8e4817bc4d1dbeb16a2ec1ae1f1.png)](https://viyaleta.medium.com/?source=post_page-----72951fcf8363--------------------------------)[](https://towardsdatascience.com/?source=post_page-----72951fcf8363--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----72951fcf8363--------------------------------)
    [Viyaleta Apgar](https://viyaleta.medium.com/?source=post_page-----72951fcf8363--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Â·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fccae8864d5a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-use-cases-for-gaussian-mixture-model-gmm-72951fcf8363&user=Viyaleta+Apgar&userId=ccae8864d5a4&source=post_page-ccae8864d5a4----72951fcf8363---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----72951fcf8363--------------------------------)
    Â·10 min readÂ·Jul 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F72951fcf8363&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-use-cases-for-gaussian-mixture-model-gmm-72951fcf8363&user=Viyaleta+Apgar&userId=ccae8864d5a4&source=-----72951fcf8363---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F72951fcf8363&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-use-cases-for-gaussian-mixture-model-gmm-72951fcf8363&source=-----72951fcf8363---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian Mixture Model (GMM) is a simple, yet powerful unsupervised classification
    algorithm which builds upon K-means instructions in order to predict the probability
    of classification for each instance. This property of GMM makes it versatile for
    many applications. In this article, I will discuss how GMM can be used in feature
    engineering, unsupervised classification, and anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/668cbd936d2f6728d0a0fef57658e670.png)'
  prefs: []
  type: TYPE_IMG
- en: What are Gaussian Mixture Models (GMM)?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model Description
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the Gaussian distribution of a single or multiple variables of a dataset
    attempts to represent the entire population probabilistically, GMM makes an assumption
    that there exist subpopulations in the dataset and each follows its own normal
    distribution. In an unsupervised fashion, GMM attempts to learn the subpopulations
    within the data and its probabilistic representation of each data point [1]. This
    property of GMM allows us to use the model to find points that have low probability
    of belonging to any subpopulation and, therefore, categorize such points as outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'GMM essentially extends the multivariate Gaussian distribution to fit the subpopulation
    case by utilizing components to represent these subpopulations and alters the
    multivariate probability distribution function to fit the components. As a gentle
    reminder, the probability density function of the multivariate Gaussian looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4cd3b3619fa973e582fa30b30c9a6578.png)'
  prefs: []
  type: TYPE_IMG
- en: Multivariate Gaussian distribution probability density function
  prefs: []
  type: TYPE_NORMAL
- en: 'In GMM, the probability of each instance is modified to be the sum of probabilities
    across all components and component weights are parameterized as ðœ™. GMM requires
    that the sum of all components weights is 1 so it can treat each component as
    a ratio of the whole. GMM also incorporates feature means and variances for each
    component. The model looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dfcb4ede0d9a1f6b290d56d28e38f85e.png)'
  prefs: []
  type: TYPE_IMG
- en: GMM model formulation
  prefs: []
  type: TYPE_NORMAL
- en: Notice the parallels between multivariate distribution and GMM. In essence,
    the GMM algorithm finds the correct weight for each component is represented as
    a multivariate Gaussian distribution. In [his post](/gaussian-mixture-models-explained-6986aaf5a95),
    [Oscar Contreras Carrasco](https://medium.com/u/91a848e356c8?source=post_page-----72951fcf8363--------------------------------)
    does a fantastic derivation of GMM [2].
  prefs: []
  type: TYPE_NORMAL
- en: The parameters of the model can be initiated randomly or by using a specific
    strategy and the component weights ðœ™ of the model are determined using repeated
    Expectation Maximization (EM) steps [1].
  prefs: []
  type: TYPE_NORMAL
- en: Model Algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first part of implementation of GMM is **initialization of the components**.
    GMM implementation consists of initialization step followed by iterative **Expectation
    Maximization** (EM) process which repeats until convergence:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1:** In the ***initialization******step***, model parameters are initialized:
    *K* values from the dataset are randomly assigned as component means; component
    variances are calculated based on the randomly assigned means; all component weights
    are assigned a value of 1/*K.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2:** In the ***expectation******step***, we calculate the probability
    that each data point is generated by each component. The expectation for each
    datapoint-component pair is the weight of this specific component times the probability
    that our datapoint belongs to this component (given the component mean and variance)
    as a fraction of all other component probabilities, parameterized with their respective
    component weights. Essentially, expectation attempts to find the likelihood that
    each point belongs to each component and will use this value to slowly adjust
    model parameters until convergence.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c1305a6cae954acf60a4a26af782f866.png)'
  prefs: []
  type: TYPE_IMG
- en: Formula for the expectation step
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3:** In the ***maximization step***, we reset the component weights
    and means and recalculate the variances based on the Î³ value from expectation
    step. The new component weights are set as sum of expectation values across all
    datapoints for this component. The new mean value for each component is the average
    of all data points, weighted by the expectation value.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fac778a796f4fa457553840394b4112f.png)'
  prefs: []
  type: TYPE_IMG
- en: Formulas for the maximization step
  prefs: []
  type: TYPE_NORMAL
- en: Just like in a k-means algorithm, it is appropriate to guess the number of components
    *K*, if the number of components is not previously available.
  prefs: []
  type: TYPE_NORMAL
- en: Below is a visual example of GMM convergence. Here, GMM demonstrates convergence
    on a two-dimensional dataset with two clusters. The algorithm behaves similarly
    to k-means but differs in that it estimates the probability density (as opposed
    to pure categorization of sample data points).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f00ea4a65606926a97d1de7570082347.png)'
  prefs: []
  type: TYPE_IMG
- en: EM Clustering of Old Faithful data [[3](https://commons.wikimedia.org/wiki/File:EM_Clustering_of_Old_Faithful_data.gif)]
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s see how this algorithm can be applied to our 3 use-cases.
  prefs: []
  type: TYPE_NORMAL
- en: GMM for Feature Engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While some machine learning models (like the infamous XGBoost) can learn a variety
    of input feature distributions, others are more strict in their requirements.
    Linear and logistic regression, linear discriminant analysis (LDA), and multivariate
    Gaussian typically expect features to be normally distributed and may not function
    so well if the data is multimodal. There are other analytical and visual reasons
    we may want to deal with multimodality and GMM can help us do just that.
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s take a look at an example of bimodal feature from a fictional bookstore
    dataset. I pulled this data from a Kaggle repository ([found here](https://www.kaggle.com/datasets/sbonelondhlazi/bookstore-dataset))
    which contains data scraped from [books.toscrape.com](http://books.toscrape.com/)
    [7]. This dataset features typical bookstore information like book title, genre,
    price, and rating. It also contains book quantity, which determines the total
    number of books the fictional book store has in stock. As luck would have it,
    book has a bimodal distribution. Letâ€™s see if we can use GMM as a feature engineering
    technique in order to create two separate features from the book quantity data.
  prefs: []
  type: TYPE_NORMAL
- en: 'I used the following code to implement this task in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s take a look at the results. The left hand-side chart shows the original
    distribution of book quantity. The right hand-side chart shows the distribution
    of each predicted component, after the GMM transformation. Notice that the shape
    of the complete distribution is exactly the same but the two components split
    the original distribution at a specific point, creating two (mostly) normal histograms.
    If I was not satisfied with the point which segments two components, I can use
    the GMM predicted probability to adjust where component 1 ends and component 2
    begins.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0edf34e4c22611332b3030d577ca1c7c.png)'
  prefs: []
  type: TYPE_IMG
- en: Distribution of Quantity before and after GMM [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
  prefs: []
  type: TYPE_NORMAL
- en: GMM for Unsupervised Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another use-case for GMM is unsupervised classification. In this sense, GMM
    works similarly to K-means algorithm but allows probabilistic determination of
    class belonging (unlike the K-means, where the output is a binary metric). This
    can be especially beneficial to use-cases that require custom thresholds for categorization
    or simply call for a probabilistic output.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, I downloaded the penguins dataset ([available on Kaggle](https://www.kaggle.com/datasets/parulpandey/palmer-archipelago-antarctica-penguin-data))
    and selected two features for the purpose of visual demonstration: penguin culmen
    length and depth (culmen is the top ridge of penguinâ€™s bill) [[8](https://www.kaggle.com/datasets/parulpandey/palmer-archipelago-antarctica-penguin-data)].
    I removed null data points and created a scatterplot to depict the data.'
  prefs: []
  type: TYPE_NORMAL
- en: When we take a look at the scatterplot, 3 potential groups stand out. If we
    color the ground truth categories, we see that, in fact, the three species of
    penguin do align with the 3 groups of data points. This example is quite rudimentary
    since in real world, we usually work with multidimensional data and there is no
    easy way to determine how many subpopulations exist in our dataset. Nonetheless,
    letâ€™s see how GMM can help us segment our data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4243f106ece15ba8650d32dda59a0820.png)'
  prefs: []
  type: TYPE_IMG
- en: Scatterplot depicting penguin culmen length vs. depth [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
  prefs: []
  type: TYPE_NORMAL
- en: In the Python code snippet below, I downloaded the dataset, removed null values,
    selected the two features of interest, and fitted the GMM model to them. `sklearn`
    offers two options for predictions â€” predicting the class and predicting the probability
    of class belonging. Sum of probabilities for each data point is equal to 1 (per
    GMM algorithm constraint).
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s take a look at the probability of each data point belonging to each component.
    The three scatterplots below show probabilities of instances per component. Points
    with higher transparency have lower probabilities and points depicted in brighter
    color have higher probability. In the chart below, we can see that GMM extrapolated
    greater uncertainty for points located between different components, as expected.
    We can use the `gmm.predict_proba()` function to retain control of class attribution.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b4c0098171b267e3e92e23a7a4217bb2.png)'
  prefs: []
  type: TYPE_IMG
- en: Scatterplots depicting probability of class belonging for each predicted class
    [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
  prefs: []
  type: TYPE_NORMAL
- en: GMM for Anomaly Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In my [previous story](/the-basics-of-anomaly-detection-65aff59949b7), I shared
    the basics of anomaly detection and an application of a statistical approach to
    detect anomalies [6]. Namely, I used the mutlivariate Gaussian distribution to
    identify data points that have low probability of following the Normal distribution.
    The problem with this approach, however, is that our data is often more complex.
    Gaussian Mixture Model (GMM) attempts to solve the multimodality problem and is
    useful in cases where features form subpopulations of normally distributed feature
    relationships.
  prefs: []
  type: TYPE_NORMAL
- en: For this last example, I will use a wine dataset from the [ODDS library](http://odds.cs.stonybrook.edu/wine-dataset/)
    [10]. This is the same dataset I used in [my post](/the-basics-of-anomaly-detection-65aff59949b7)
    where I applied multivariate Gaussian distribution to detect outliers. Letâ€™s see
    if I can improve on my [previous result](/the-basics-of-anomaly-detection-65aff59949b7),
    where 41 instances where misclassified, 40 of which were falsely identified anomalies.
    The first advantage over the multivariate Gaussian distribution method is that
    we can use all of our features and we are not restricted to only normally distributed
    features. The second advantage is that we can account for data subpopulations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perhaps the most prominent advantage of GMM is that this model can help in
    finding two types of anomalous data: anomalies which are outliers of a population
    (e.g. mistake in data entry) and anomalies which form their own group (e.g. credit
    card fraud behavior). The positive instances in the wine dataset are constructed
    as samples of two types of wine while anomalies in the dataset are constructed
    as a subsample of a third type of wine [10]. Therefore, we will likely find that
    our outliers make up their own group in this instance.'
  prefs: []
  type: TYPE_NORMAL
- en: Since we have 13 features in our dataset, letâ€™s summarize the data into first
    two PCA components and plot them. In the chart below, we will see that the scatterplot
    features one dense area and about two dozen points scattered throughout. No distinct
    subpopulations appear in the data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b05535bb4405c809eb09efc549e88069.png)'
  prefs: []
  type: TYPE_IMG
- en: Scatterplot of first 2 PCA components, showing ground truth anomalies [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
  prefs: []
  type: TYPE_NORMAL
- en: The code snippet below fits the GMM model with 2 components and standard input
    fields from `sklearn` library to our dataset. Hopefully, one component will identify
    positive instances while the other component identifies anomalies.
  prefs: []
  type: TYPE_NORMAL
- en: The predicted components are numbered randomly so I did some background magic
    to relabel the predictions. You can check out my [Jupyter notebook](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)
    for more guidance on how to do that.
  prefs: []
  type: TYPE_NORMAL
- en: The results look good! We have 0 false negatives and we have 11 falsely identified
    anomalies. If this was the real world, we would only have to manually check some
    15% of data after running this model. This is already a significant improvement
    upon the multivariate Gaussian distribution approach.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0a0149bc5a3f51631c72088af7d686eb.png)'
  prefs: []
  type: TYPE_IMG
- en: Confusion matrix of GMM results [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
  prefs: []
  type: TYPE_NORMAL
- en: If we plot our results, we can see that the falsely identified anomalies happen
    to be closer to the most dense area. Again, we are using the first two components
    of PCA for visualization purposes only and GMM is doing a lot more to categorize
    our data points.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/706cf41f59a0847713126bce99a11db9.png)'
  prefs: []
  type: TYPE_IMG
- en: Scatterplot of first 2 PCA components, showing predicted anomalies [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
  prefs: []
  type: TYPE_NORMAL
- en: We can probably improve upon this result by analyzing our predicted anomaly
    probabilities. Below, I have charted those probabilities for falsely identified
    anomaly instances and true anomaly instances (there were no predicted false negatives).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/04a51ec35f2cbfcf0c692cf1e5b2ad2f.png)'
  prefs: []
  type: TYPE_IMG
- en: Bar chart showing probabilities of anomaly class belonging among falsely identified
    anomalies and true anomalies [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that most of our falsely identified anomaly probabilities are lower
    than 0.9999\. So, instead of using default classification, we can use the predicted
    anomaly probability and set a new threshold. If we set the threshold anomaly detection
    to be greater 0.9999, we will only have 3 falsely identified anomalies. This can
    be done with a single line of code: `components = np.where(proba>0.9999, 1, 0)`
    . In some instances, changing the threshold may add more false negatives to our
    final results. Lucky us, this is not the case.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4747a4b5145bbd2020e95a3fa57ed7dd.png)'
  prefs: []
  type: TYPE_IMG
- en: Confusion matrix of GMM results, after decreasing increasing probability threshold
    [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
  prefs: []
  type: TYPE_NORMAL
- en: In the following PCA scatterplot, we can see that our model still predicts 3
    falsely identified anomalies but with the new improvement, we would only need
    to manually check 10% of data instances for presence of anomalies. Given that
    8% of data are actually anomalous, this is great!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cdc939fc37c9e0326251483df54cac8d.png)'
  prefs: []
  type: TYPE_IMG
- en: Scatterplot of first 2 PCA components, after increasing anomaly probability
    threshold [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
  prefs: []
  type: TYPE_NORMAL
- en: The power to predict class probabilities makes GMM is powerful and versatile
    tool in data science. Although it has the same limitations as its K-means cousin,
    GMM is helpful in feature engineering, more flexible unsupervised classification,
    and anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: I had a lot of fun writing this article and I had just as much fun reading it.
    I am always open to feedback and questions so make sure to make use of that comment
    section. If you would prefer to provide direct feedback, you can always [find
    me on LinkedIn](https://www.linkedin.com/in/viyaleta/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Sources:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[https://brilliant.org/wiki/gaussian-mixture-model/](https://brilliant.org/wiki/gaussian-mixture-model/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://towardsdatascience.com/gaussian-mixture-models-explained-6986aaf5a95](/gaussian-mixture-models-explained-6986aaf5a95)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://commons.wikimedia.org/wiki/File:EM_Clustering_of_Old_Faithful_data.gif](https://commons.wikimedia.org/wiki/File:EM_Clustering_of_Old_Faithful_data.gif)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://towardsdatascience.com/understanding-anomaly-detection-in-python-using-gaussian-mixture-model-e26e5d06094b](/understanding-anomaly-detection-in-python-using-gaussian-mixture-model-e26e5d06094b)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture.fit](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture.fit)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://towardsdatascience.com/the-basics-of-anomaly-detection-65aff59949b7](/the-basics-of-anomaly-detection-65aff59949b7)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/datasets/sbonelondhlazi/bookstore-dataset](https://www.kaggle.com/datasets/sbonelondhlazi/bookstore-dataset)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://medium.com/r/?url=https%3A%2F%2Fwww.kaggle.com%2Fdatasets%2Fparulpandey%2Fpalmer-archipelago-antarctica-penguin-data](https://www.kaggle.com/datasets/parulpandey/palmer-archipelago-antarctica-penguin-data)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Saket Sathe and Charu C. Aggarwal. LODES: Local Density meets Spectral Outlier
    Detection. SIAM Conference on Data Mining, 2016\. [http://odds.cs.stonybrook.edu/wine-dataset/](http://odds.cs.stonybrook.edu/wine-dataset/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
