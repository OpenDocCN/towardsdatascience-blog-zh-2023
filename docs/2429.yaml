- en: 3 Use-Cases for Gaussian Mixture Models (GMM)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高斯混合模型（GMM）的3个应用场景
- en: 原文：[https://towardsdatascience.com/3-use-cases-for-gaussian-mixture-model-gmm-72951fcf8363?source=collection_archive---------1-----------------------#2023-07-27](https://towardsdatascience.com/3-use-cases-for-gaussian-mixture-model-gmm-72951fcf8363?source=collection_archive---------1-----------------------#2023-07-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/3-use-cases-for-gaussian-mixture-model-gmm-72951fcf8363?source=collection_archive---------1-----------------------#2023-07-27](https://towardsdatascience.com/3-use-cases-for-gaussian-mixture-model-gmm-72951fcf8363?source=collection_archive---------1-----------------------#2023-07-27)
- en: Feature engineering, unsupervised classification, and anomaly detection with
    the versatility of the GMM algorithm
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征工程、无监督分类以及使用GMM算法的异常检测
- en: '[](https://viyaleta.medium.com/?source=post_page-----72951fcf8363--------------------------------)[![Viyaleta
    Apgar](../Images/8d8fd8e4817bc4d1dbeb16a2ec1ae1f1.png)](https://viyaleta.medium.com/?source=post_page-----72951fcf8363--------------------------------)[](https://towardsdatascience.com/?source=post_page-----72951fcf8363--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----72951fcf8363--------------------------------)
    [Viyaleta Apgar](https://viyaleta.medium.com/?source=post_page-----72951fcf8363--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://viyaleta.medium.com/?source=post_page-----72951fcf8363--------------------------------)[![Viyaleta
    Apgar](../Images/8d8fd8e4817bc4d1dbeb16a2ec1ae1f1.png)](https://viyaleta.medium.com/?source=post_page-----72951fcf8363--------------------------------)[](https://towardsdatascience.com/?source=post_page-----72951fcf8363--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----72951fcf8363--------------------------------)
    [Viyaleta Apgar](https://viyaleta.medium.com/?source=post_page-----72951fcf8363--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fccae8864d5a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-use-cases-for-gaussian-mixture-model-gmm-72951fcf8363&user=Viyaleta+Apgar&userId=ccae8864d5a4&source=post_page-ccae8864d5a4----72951fcf8363---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----72951fcf8363--------------------------------)
    ·10 min read·Jul 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F72951fcf8363&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-use-cases-for-gaussian-mixture-model-gmm-72951fcf8363&user=Viyaleta+Apgar&userId=ccae8864d5a4&source=-----72951fcf8363---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fccae8864d5a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-use-cases-for-gaussian-mixture-model-gmm-72951fcf8363&user=Viyaleta+Apgar&userId=ccae8864d5a4&source=post_page-ccae8864d5a4----72951fcf8363---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----72951fcf8363--------------------------------)
    ·10分钟阅读·2023年7月27日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F72951fcf8363&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-use-cases-for-gaussian-mixture-model-gmm-72951fcf8363&user=Viyaleta+Apgar&userId=ccae8864d5a4&source=-----72951fcf8363---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F72951fcf8363&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-use-cases-for-gaussian-mixture-model-gmm-72951fcf8363&source=-----72951fcf8363---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F72951fcf8363&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-use-cases-for-gaussian-mixture-model-gmm-72951fcf8363&source=-----72951fcf8363---------------------bookmark_footer-----------)'
- en: Gaussian Mixture Model (GMM) is a simple, yet powerful unsupervised classification
    algorithm which builds upon K-means instructions in order to predict the probability
    of classification for each instance. This property of GMM makes it versatile for
    many applications. In this article, I will discuss how GMM can be used in feature
    engineering, unsupervised classification, and anomaly detection.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯混合模型（GMM）是一种简单而强大的无监督分类算法，基于K-means算法来预测每个实例的分类概率。GMM的这一特性使其在许多应用中都具有很大的灵活性。在本文中，我将讨论GMM如何用于特征工程、无监督分类和异常检测。
- en: '![](../Images/668cbd936d2f6728d0a0fef57658e670.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/668cbd936d2f6728d0a0fef57658e670.png)'
- en: What are Gaussian Mixture Models (GMM)?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高斯混合模型（GMM）是什么？
- en: Model Description
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型描述
- en: While the Gaussian distribution of a single or multiple variables of a dataset
    attempts to represent the entire population probabilistically, GMM makes an assumption
    that there exist subpopulations in the dataset and each follows its own normal
    distribution. In an unsupervised fashion, GMM attempts to learn the subpopulations
    within the data and its probabilistic representation of each data point [1]. This
    property of GMM allows us to use the model to find points that have low probability
    of belonging to any subpopulation and, therefore, categorize such points as outliers.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管单一或多个变量的数据集的高斯分布试图以概率方式表示整个数据集，GMM假设数据集中存在子群体，并且每个子群体遵循其自己的正态分布。以无监督的方式，GMM试图学习数据中的子群体及其对每个数据点的概率表示[1]。GMM的这一特性使我们能够使用模型找到属于任何子群体的概率较低的点，从而将这些点分类为异常值。
- en: 'GMM essentially extends the multivariate Gaussian distribution to fit the subpopulation
    case by utilizing components to represent these subpopulations and alters the
    multivariate probability distribution function to fit the components. As a gentle
    reminder, the probability density function of the multivariate Gaussian looks
    like this:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: GMM本质上是通过利用组件来表示这些子群体，并修改多变量概率分布函数以适应组件，从而扩展了多变量高斯分布以适应子群体情况。温馨提醒，多变量高斯分布的概率密度函数如下：
- en: '![](../Images/4cd3b3619fa973e582fa30b30c9a6578.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4cd3b3619fa973e582fa30b30c9a6578.png)'
- en: Multivariate Gaussian distribution probability density function
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 多变量高斯分布的概率密度函数
- en: 'In GMM, the probability of each instance is modified to be the sum of probabilities
    across all components and component weights are parameterized as 𝜙. GMM requires
    that the sum of all components weights is 1 so it can treat each component as
    a ratio of the whole. GMM also incorporates feature means and variances for each
    component. The model looks like this:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在GMM中，每个实例的概率被修改为所有组件的概率和，组件权重被参数化为𝜙。GMM要求所有组件权重的总和为1，以便将每个组件视为整体的一个比率。GMM还结合了每个组件的特征均值和方差。模型如下：
- en: '![](../Images/dfcb4ede0d9a1f6b290d56d28e38f85e.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dfcb4ede0d9a1f6b290d56d28e38f85e.png)'
- en: GMM model formulation
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: GMM模型的公式
- en: Notice the parallels between multivariate distribution and GMM. In essence,
    the GMM algorithm finds the correct weight for each component is represented as
    a multivariate Gaussian distribution. In [his post](/gaussian-mixture-models-explained-6986aaf5a95),
    [Oscar Contreras Carrasco](https://medium.com/u/91a848e356c8?source=post_page-----72951fcf8363--------------------------------)
    does a fantastic derivation of GMM [2].
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 注意多变量分布与GMM之间的相似性。实质上，GMM算法为每个组件找到正确的权重，这些组件被表示为多变量高斯分布。在[他的文章](/gaussian-mixture-models-explained-6986aaf5a95)中，[Oscar
    Contreras Carrasco](https://medium.com/u/91a848e356c8?source=post_page-----72951fcf8363--------------------------------)对GMM做了精彩的推导[2]。
- en: The parameters of the model can be initiated randomly or by using a specific
    strategy and the component weights 𝜙 of the model are determined using repeated
    Expectation Maximization (EM) steps [1].
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的参数可以通过随机初始化或使用特定策略进行初始化，模型的组件权重𝜙通过重复的期望最大化（EM）步骤来确定[1]。
- en: Model Algorithm
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型算法
- en: 'The first part of implementation of GMM is **initialization of the components**.
    GMM implementation consists of initialization step followed by iterative **Expectation
    Maximization** (EM) process which repeats until convergence:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: GMM的实施的第一部分是**组件的初始化**。GMM的实施包括初始化步骤，随后是迭代的**期望最大化**（EM）过程，直到收敛：
- en: '**Step 1:** In the ***initialization******step***, model parameters are initialized:
    *K* values from the dataset are randomly assigned as component means; component
    variances are calculated based on the randomly assigned means; all component weights
    are assigned a value of 1/*K.*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**第1步：** 在***初始化***步骤中，模型参数被初始化：*K*值从数据集中随机分配为组件均值；组件方差根据随机分配的均值计算；所有组件权重被赋值为1/*K*。'
- en: '**Step 2:** In the ***expectation******step***, we calculate the probability
    that each data point is generated by each component. The expectation for each
    datapoint-component pair is the weight of this specific component times the probability
    that our datapoint belongs to this component (given the component mean and variance)
    as a fraction of all other component probabilities, parameterized with their respective
    component weights. Essentially, expectation attempts to find the likelihood that
    each point belongs to each component and will use this value to slowly adjust
    model parameters until convergence.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2：** 在***期望步骤***中，我们计算每个数据点由每个组件生成的概率。每个数据点-组件对的期望是该特定组件的权重乘以我们数据点属于该组件的概率（给定组件均值和方差），作为所有其他组件概率的一个分数，参数化为各自的组件权重。基本上，期望步骤尝试找出每个点属于每个组件的可能性，并利用这个值来逐步调整模型参数直到收敛。'
- en: '![](../Images/c1305a6cae954acf60a4a26af782f866.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c1305a6cae954acf60a4a26af782f866.png)'
- en: Formula for the expectation step
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 期望步骤的公式
- en: '**Step 3:** In the ***maximization step***, we reset the component weights
    and means and recalculate the variances based on the γ value from expectation
    step. The new component weights are set as sum of expectation values across all
    datapoints for this component. The new mean value for each component is the average
    of all data points, weighted by the expectation value.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 3：** 在***最大化步骤***中，我们重置组件的权重和均值，并根据期望步骤中的γ值重新计算方差。新的组件权重设置为该组件所有数据点期望值的总和。每个组件的新均值是所有数据点的平均值，加权由期望值决定。'
- en: '![](../Images/fac778a796f4fa457553840394b4112f.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fac778a796f4fa457553840394b4112f.png)'
- en: Formulas for the maximization step
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最大化步骤的公式
- en: Just like in a k-means algorithm, it is appropriate to guess the number of components
    *K*, if the number of components is not previously available.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在k-means算法中一样，如果组件的数量事先不可用，那么猜测组件的数量*K*是合适的。
- en: Below is a visual example of GMM convergence. Here, GMM demonstrates convergence
    on a two-dimensional dataset with two clusters. The algorithm behaves similarly
    to k-means but differs in that it estimates the probability density (as opposed
    to pure categorization of sample data points).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是GMM收敛的视觉示例。在这里，GMM展示了在具有两个簇的二维数据集上的收敛。该算法的行为类似于k-means，但不同之处在于它估计概率密度（而不是纯粹对样本数据点的分类）。
- en: '![](../Images/f00ea4a65606926a97d1de7570082347.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f00ea4a65606926a97d1de7570082347.png)'
- en: EM Clustering of Old Faithful data [[3](https://commons.wikimedia.org/wiki/File:EM_Clustering_of_Old_Faithful_data.gif)]
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 旧忠实数据的EM聚类 [[3](https://commons.wikimedia.org/wiki/File:EM_Clustering_of_Old_Faithful_data.gif)]
- en: Let’s see how this algorithm can be applied to our 3 use-cases.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个算法如何应用于我们的三个用例。
- en: GMM for Feature Engineering
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程中的GMM
- en: While some machine learning models (like the infamous XGBoost) can learn a variety
    of input feature distributions, others are more strict in their requirements.
    Linear and logistic regression, linear discriminant analysis (LDA), and multivariate
    Gaussian typically expect features to be normally distributed and may not function
    so well if the data is multimodal. There are other analytical and visual reasons
    we may want to deal with multimodality and GMM can help us do just that.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管一些机器学习模型（如臭名昭著的XGBoost）可以学习各种输入特征分布，但其他模型对其要求更为严格。线性回归、逻辑回归、线性判别分析（LDA）和多变量高斯通常期望特征呈正态分布，如果数据是多模态的，可能效果不佳。我们可能还有其他分析和视觉上的原因需要处理多模态，而GMM可以帮助我们实现这一点。
- en: Let’s take a look at an example of bimodal feature from a fictional bookstore
    dataset. I pulled this data from a Kaggle repository ([found here](https://www.kaggle.com/datasets/sbonelondhlazi/bookstore-dataset))
    which contains data scraped from [books.toscrape.com](http://books.toscrape.com/)
    [7]. This dataset features typical bookstore information like book title, genre,
    price, and rating. It also contains book quantity, which determines the total
    number of books the fictional book store has in stock. As luck would have it,
    book has a bimodal distribution. Let’s see if we can use GMM as a feature engineering
    technique in order to create two separate features from the book quantity data.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看一个虚构书店数据集中双峰特征的示例。我从 Kaggle 数据库中提取了这些数据（[链接在此](https://www.kaggle.com/datasets/sbonelondhlazi/bookstore-dataset)），其中包含从
    [books.toscrape.com](http://books.toscrape.com/) [7] 爬取的数据。该数据集包含典型的书店信息，如书名、类别、价格和评分。它还包含书籍数量，这决定了虚构书店库存中的书籍总数。巧合的是，这些书籍具有双峰分布。我们来看看是否可以使用
    GMM 作为特征工程技术，从书籍数量数据中创建两个独立的特征。
- en: 'I used the following code to implement this task in Python:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用以下代码在 Python 中实现了这一任务：
- en: Let’s take a look at the results. The left hand-side chart shows the original
    distribution of book quantity. The right hand-side chart shows the distribution
    of each predicted component, after the GMM transformation. Notice that the shape
    of the complete distribution is exactly the same but the two components split
    the original distribution at a specific point, creating two (mostly) normal histograms.
    If I was not satisfied with the point which segments two components, I can use
    the GMM predicted probability to adjust where component 1 ends and component 2
    begins.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看结果。左侧的图表显示了书籍数量的原始分布。右侧的图表显示了每个预测组件的分布，在 GMM 转换之后。请注意，完整分布的形状完全相同，但两个组件在特定点处拆分了原始分布，创建了两个（大多）正常的直方图。如果我对分割两个组件的点不满意，我可以使用
    GMM 预测的概率来调整组件 1 结束和组件 2 开始的位置。
- en: '![](../Images/0edf34e4c22611332b3030d577ca1c7c.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0edf34e4c22611332b3030d577ca1c7c.png)'
- en: Distribution of Quantity before and after GMM [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 数量在 GMM 之前和之后的分布 [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
- en: GMM for Unsupervised Classification
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GMM 用于无监督分类
- en: Another use-case for GMM is unsupervised classification. In this sense, GMM
    works similarly to K-means algorithm but allows probabilistic determination of
    class belonging (unlike the K-means, where the output is a binary metric). This
    can be especially beneficial to use-cases that require custom thresholds for categorization
    or simply call for a probabilistic output.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: GMM 的另一个应用场景是无监督分类。在这方面，GMM 的工作方式类似于 K-means 算法，但允许对类别归属进行概率性判断（不同于 K-means，其中输出是一个二元度量）。这对于需要自定义分类阈值或简单要求概率输出的用例特别有益。
- en: 'For this example, I downloaded the penguins dataset ([available on Kaggle](https://www.kaggle.com/datasets/parulpandey/palmer-archipelago-antarctica-penguin-data))
    and selected two features for the purpose of visual demonstration: penguin culmen
    length and depth (culmen is the top ridge of penguin’s bill) [[8](https://www.kaggle.com/datasets/parulpandey/palmer-archipelago-antarctica-penguin-data)].
    I removed null data points and created a scatterplot to depict the data.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我下载了企鹅数据集（[在 Kaggle 上提供](https://www.kaggle.com/datasets/parulpandey/palmer-archipelago-antarctica-penguin-data)），并选择了两个特征以进行可视化演示：企鹅的喙长度和深度（喙是企鹅喙的顶部脊）[[8](https://www.kaggle.com/datasets/parulpandey/palmer-archipelago-antarctica-penguin-data)]。我删除了空值数据点，并创建了散点图以描绘数据。
- en: When we take a look at the scatterplot, 3 potential groups stand out. If we
    color the ground truth categories, we see that, in fact, the three species of
    penguin do align with the 3 groups of data points. This example is quite rudimentary
    since in real world, we usually work with multidimensional data and there is no
    easy way to determine how many subpopulations exist in our dataset. Nonetheless,
    let’s see how GMM can help us segment our data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们查看散点图时，3 个潜在的组脱颖而出。如果我们为实际类别上色，我们会发现，事实上，三种企鹅与三组数据点对齐。这个例子非常基础，因为在现实世界中，我们通常处理的是多维数据，且没有简单的方法来确定数据集中存在多少个子群体。尽管如此，我们还是来看看
    GMM 如何帮助我们对数据进行分割。
- en: '![](../Images/4243f106ece15ba8650d32dda59a0820.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4243f106ece15ba8650d32dda59a0820.png)'
- en: Scatterplot depicting penguin culmen length vs. depth [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 描述企鹅喙长与深度的散点图 [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
- en: In the Python code snippet below, I downloaded the dataset, removed null values,
    selected the two features of interest, and fitted the GMM model to them. `sklearn`
    offers two options for predictions — predicting the class and predicting the probability
    of class belonging. Sum of probabilities for each data point is equal to 1 (per
    GMM algorithm constraint).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的Python代码片段中，我下载了数据集，删除了空值，选择了两个感兴趣的特征，并将GMM模型拟合到它们上面。`sklearn`提供了两种预测选项——预测类别和预测类别归属的概率。每个数据点的概率总和等于1（根据GMM算法约束）。
- en: Let’s take a look at the probability of each data point belonging to each component.
    The three scatterplots below show probabilities of instances per component. Points
    with higher transparency have lower probabilities and points depicted in brighter
    color have higher probability. In the chart below, we can see that GMM extrapolated
    greater uncertainty for points located between different components, as expected.
    We can use the `gmm.predict_proba()` function to retain control of class attribution.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看每个数据点属于每个组件的概率。下面的三个散点图显示了每个组件实例的概率。透明度较高的点具有较低的概率，而颜色较亮的点具有较高的概率。在下图中，我们可以看到，GMM对位于不同组件之间的点预测了更大的不确定性，这在预期之中。我们可以使用`gmm.predict_proba()`函数来控制类别归属。
- en: '![](../Images/b4c0098171b267e3e92e23a7a4217bb2.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4c0098171b267e3e92e23a7a4217bb2.png)'
- en: Scatterplots depicting probability of class belonging for each predicted class
    [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 描述每个预测类别的类归属概率的散点图 [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
- en: GMM for Anomaly Detection
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于异常检测的GMM
- en: In my [previous story](/the-basics-of-anomaly-detection-65aff59949b7), I shared
    the basics of anomaly detection and an application of a statistical approach to
    detect anomalies [6]. Namely, I used the mutlivariate Gaussian distribution to
    identify data points that have low probability of following the Normal distribution.
    The problem with this approach, however, is that our data is often more complex.
    Gaussian Mixture Model (GMM) attempts to solve the multimodality problem and is
    useful in cases where features form subpopulations of normally distributed feature
    relationships.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的[之前的故事](/the-basics-of-anomaly-detection-65aff59949b7)中，我分享了异常检测的基础知识以及统计方法在检测异常中的应用[6]。也就是说，我使用多变量高斯分布来识别低概率符合正态分布的数据点。然而，这种方法的问题是我们的数据往往更复杂。高斯混合模型（GMM）尝试解决多模态问题，并且在特征形成正态分布特征关系的子群体的情况下非常有用。
- en: For this last example, I will use a wine dataset from the [ODDS library](http://odds.cs.stonybrook.edu/wine-dataset/)
    [10]. This is the same dataset I used in [my post](/the-basics-of-anomaly-detection-65aff59949b7)
    where I applied multivariate Gaussian distribution to detect outliers. Let’s see
    if I can improve on my [previous result](/the-basics-of-anomaly-detection-65aff59949b7),
    where 41 instances where misclassified, 40 of which were falsely identified anomalies.
    The first advantage over the multivariate Gaussian distribution method is that
    we can use all of our features and we are not restricted to only normally distributed
    features. The second advantage is that we can account for data subpopulations.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个最后的例子中，我将使用来自[ODDS library](http://odds.cs.stonybrook.edu/wine-dataset/)
    [10]的葡萄酒数据集。这是我在[我的帖子](/the-basics-of-anomaly-detection-65aff59949b7)中使用的数据集，在那里我应用了多变量高斯分布来检测离群点。让我们看看我是否可以改进[之前的结果](/the-basics-of-anomaly-detection-65aff59949b7)，在那个结果中，41个实例被错误分类，其中40个被错误识别为异常点。相比多变量高斯分布方法，第一个优点是我们可以使用所有特征，而不限于仅正态分布的特征。第二个优点是我们可以考虑数据子群体。
- en: 'Perhaps the most prominent advantage of GMM is that this model can help in
    finding two types of anomalous data: anomalies which are outliers of a population
    (e.g. mistake in data entry) and anomalies which form their own group (e.g. credit
    card fraud behavior). The positive instances in the wine dataset are constructed
    as samples of two types of wine while anomalies in the dataset are constructed
    as a subsample of a third type of wine [10]. Therefore, we will likely find that
    our outliers make up their own group in this instance.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: GMM最显著的优势可能是该模型可以帮助发现两种类型的异常数据：一种是人口中的离群点（例如数据录入错误），另一种是形成自己组的异常（例如信用卡欺诈行为）。在葡萄酒数据集中，正实例被构建为两种类型的葡萄酒样本，而数据集中的异常被构建为第三种葡萄酒的子样本
    [10]。因此，我们很可能会发现我们的离群点在这个实例中构成了自己的组。
- en: Since we have 13 features in our dataset, let’s summarize the data into first
    two PCA components and plot them. In the chart below, we will see that the scatterplot
    features one dense area and about two dozen points scattered throughout. No distinct
    subpopulations appear in the data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的数据集中有13个特征，让我们将数据汇总到前两个PCA组件中并绘制它们。在下面的图表中，我们将看到散点图包含一个密集区域和大约二十个分散的点。数据中没有明显的子群体。
- en: '![](../Images/b05535bb4405c809eb09efc549e88069.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b05535bb4405c809eb09efc549e88069.png)'
- en: Scatterplot of first 2 PCA components, showing ground truth anomalies [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个PCA组件的散点图，显示真实的异常情况 [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
- en: The code snippet below fits the GMM model with 2 components and standard input
    fields from `sklearn` library to our dataset. Hopefully, one component will identify
    positive instances while the other component identifies anomalies.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码片段使用`sklearn`库中的2个组件和标准输入字段来拟合GMM模型到我们的数据集。希望一个组件能够识别正实例，而另一个组件能够识别异常。
- en: The predicted components are numbered randomly so I did some background magic
    to relabel the predictions. You can check out my [Jupyter notebook](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)
    for more guidance on how to do that.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 预测组件的编号是随机的，因此我做了一些背景处理来重新标记预测。你可以查看我的 [Jupyter notebook](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)
    以获取更多关于如何做到这一点的指导。
- en: The results look good! We have 0 false negatives and we have 11 falsely identified
    anomalies. If this was the real world, we would only have to manually check some
    15% of data after running this model. This is already a significant improvement
    upon the multivariate Gaussian distribution approach.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 结果看起来很好！我们没有假阴性，并且有11个错误识别的异常。如果这是现实世界，我们在运行此模型后只需人工检查大约15%的数据。这已经是对多变量高斯分布方法的显著改进。
- en: '![](../Images/0a0149bc5a3f51631c72088af7d686eb.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a0149bc5a3f51631c72088af7d686eb.png)'
- en: Confusion matrix of GMM results [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: GMM结果的混淆矩阵 [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
- en: If we plot our results, we can see that the falsely identified anomalies happen
    to be closer to the most dense area. Again, we are using the first two components
    of PCA for visualization purposes only and GMM is doing a lot more to categorize
    our data points.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们绘制结果，可以看到被错误识别的异常情况实际上更接近最密集的区域。再次强调，我们仅使用前两个PCA组件进行可视化，GMM在对数据点进行分类时做了更多工作。
- en: '![](../Images/706cf41f59a0847713126bce99a11db9.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/706cf41f59a0847713126bce99a11db9.png)'
- en: Scatterplot of first 2 PCA components, showing predicted anomalies [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个PCA组件的散点图，显示预测的异常情况 [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
- en: We can probably improve upon this result by analyzing our predicted anomaly
    probabilities. Below, I have charted those probabilities for falsely identified
    anomaly instances and true anomaly instances (there were no predicted false negatives).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能通过分析预测的异常概率来改进这个结果。下面，我已经为错误识别的异常实例和真实异常实例绘制了这些概率（没有预测的假阴性）。
- en: '![](../Images/04a51ec35f2cbfcf0c692cf1e5b2ad2f.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04a51ec35f2cbfcf0c692cf1e5b2ad2f.png)'
- en: Bar chart showing probabilities of anomaly class belonging among falsely identified
    anomalies and true anomalies [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 条形图显示了在错误识别的异常和真实异常之间的异常类别归属概率 [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
- en: 'We can see that most of our falsely identified anomaly probabilities are lower
    than 0.9999\. So, instead of using default classification, we can use the predicted
    anomaly probability and set a new threshold. If we set the threshold anomaly detection
    to be greater 0.9999, we will only have 3 falsely identified anomalies. This can
    be done with a single line of code: `components = np.where(proba>0.9999, 1, 0)`
    . In some instances, changing the threshold may add more false negatives to our
    final results. Lucky us, this is not the case.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到大多数错误识别的异常概率低于 0.9999。因此，我们可以使用预测的异常概率并设置新的阈值，而不是使用默认分类。如果我们将阈值设置为大于 0.9999
    的异常检测，我们将只有 3 个错误识别的异常。这可以用一行代码完成：`components = np.where(proba>0.9999, 1, 0)`。在某些情况下，改变阈值可能会增加我们最终结果中的假阴性。幸运的是，这种情况并不存在。
- en: '![](../Images/4747a4b5145bbd2020e95a3fa57ed7dd.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4747a4b5145bbd2020e95a3fa57ed7dd.png)'
- en: Confusion matrix of GMM results, after decreasing increasing probability threshold
    [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: GMM 结果的混淆矩阵，在降低提高概率阈值之后 [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
- en: In the following PCA scatterplot, we can see that our model still predicts 3
    falsely identified anomalies but with the new improvement, we would only need
    to manually check 10% of data instances for presence of anomalies. Given that
    8% of data are actually anomalous, this is great!
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下 PCA 散点图中，我们可以看到我们的模型仍然预测了 3 个错误识别的异常，但通过新的改进，我们只需手动检查 10% 的数据实例以检测异常。考虑到
    8% 的数据实际上是异常的，这真是太棒了！
- en: '![](../Images/cdc939fc37c9e0326251483df54cac8d.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cdc939fc37c9e0326251483df54cac8d.png)'
- en: Scatterplot of first 2 PCA components, after increasing anomaly probability
    threshold [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 提高异常概率阈值后的前 2 个 PCA 组件的散点图 [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
- en: The power to predict class probabilities makes GMM is powerful and versatile
    tool in data science. Although it has the same limitations as its K-means cousin,
    GMM is helpful in feature engineering, more flexible unsupervised classification,
    and anomaly detection.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 预测类别概率的能力使得 GMM 成为数据科学中一个强大而多用途的工具。尽管它有与 K-means 同类的相同限制，GMM 在特征工程、更灵活的无监督分类和异常检测中非常有用。
- en: I had a lot of fun writing this article and I had just as much fun reading it.
    I am always open to feedback and questions so make sure to make use of that comment
    section. If you would prefer to provide direct feedback, you can always [find
    me on LinkedIn](https://www.linkedin.com/in/viyaleta/).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我在写这篇文章时非常开心，读它的时候也同样开心。我总是欢迎反馈和问题，所以一定要利用评论区。如果你希望直接反馈，可以随时[在 LinkedIn 上找到我](https://www.linkedin.com/in/viyaleta/)。
- en: 'Sources:'
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 来源：
- en: '[https://brilliant.org/wiki/gaussian-mixture-model/](https://brilliant.org/wiki/gaussian-mixture-model/)'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://brilliant.org/wiki/gaussian-mixture-model/](https://brilliant.org/wiki/gaussian-mixture-model/)'
- en: '[https://towardsdatascience.com/gaussian-mixture-models-explained-6986aaf5a95](/gaussian-mixture-models-explained-6986aaf5a95)'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://towardsdatascience.com/gaussian-mixture-models-explained-6986aaf5a95](/gaussian-mixture-models-explained-6986aaf5a95)'
- en: '[https://commons.wikimedia.org/wiki/File:EM_Clustering_of_Old_Faithful_data.gif](https://commons.wikimedia.org/wiki/File:EM_Clustering_of_Old_Faithful_data.gif)'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://commons.wikimedia.org/wiki/File:EM_Clustering_of_Old_Faithful_data.gif](https://commons.wikimedia.org/wiki/File:EM_Clustering_of_Old_Faithful_data.gif)'
- en: '[https://towardsdatascience.com/understanding-anomaly-detection-in-python-using-gaussian-mixture-model-e26e5d06094b](/understanding-anomaly-detection-in-python-using-gaussian-mixture-model-e26e5d06094b)'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://towardsdatascience.com/understanding-anomaly-detection-in-python-using-gaussian-mixture-model-e26e5d06094b](/understanding-anomaly-detection-in-python-using-gaussian-mixture-model-e26e5d06094b)'
- en: '[https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture.fit](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture.fit)'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture.fit](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture.fit)'
- en: '[https://towardsdatascience.com/the-basics-of-anomaly-detection-65aff59949b7](/the-basics-of-anomaly-detection-65aff59949b7)'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://towardsdatascience.com/the-basics-of-anomaly-detection-65aff59949b7](/the-basics-of-anomaly-detection-65aff59949b7)'
- en: '[https://www.kaggle.com/datasets/sbonelondhlazi/bookstore-dataset](https://www.kaggle.com/datasets/sbonelondhlazi/bookstore-dataset)'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/datasets/sbonelondhlazi/bookstore-dataset](https://www.kaggle.com/datasets/sbonelondhlazi/bookstore-dataset)'
- en: '[https://medium.com/r/?url=https%3A%2F%2Fwww.kaggle.com%2Fdatasets%2Fparulpandey%2Fpalmer-archipelago-antarctica-penguin-data](https://www.kaggle.com/datasets/parulpandey/palmer-archipelago-antarctica-penguin-data)'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://medium.com/r/?url=https%3A%2F%2Fwww.kaggle.com%2Fdatasets%2Fparulpandey%2Fpalmer-archipelago-antarctica-penguin-data](https://www.kaggle.com/datasets/parulpandey/palmer-archipelago-antarctica-penguin-data)'
- en: '[https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)'
- en: 'Saket Sathe and Charu C. Aggarwal. LODES: Local Density meets Spectral Outlier
    Detection. SIAM Conference on Data Mining, 2016\. [http://odds.cs.stonybrook.edu/wine-dataset/](http://odds.cs.stonybrook.edu/wine-dataset/)'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Saket Sathe 和 Charu C. Aggarwal. LODES: 局部密度与谱异常检测的结合。SIAM 数据挖掘会议，2016\. [http://odds.cs.stonybrook.edu/wine-dataset/](http://odds.cs.stonybrook.edu/wine-dataset/)'
