- en: Padding Large Language Models — Examples with Llama 2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/padding-large-language-models-examples-with-llama-2-199fb10df8ff?source=collection_archive---------2-----------------------#2023-08-11](https://towardsdatascience.com/padding-large-language-models-examples-with-llama-2-199fb10df8ff?source=collection_archive---------2-----------------------#2023-08-11)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Best practices to pad training examples for causal LLMs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----199fb10df8ff--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----199fb10df8ff--------------------------------)[](https://towardsdatascience.com/?source=post_page-----199fb10df8ff--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----199fb10df8ff--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----199fb10df8ff--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpadding-large-language-models-examples-with-llama-2-199fb10df8ff&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----199fb10df8ff---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----199fb10df8ff--------------------------------)
    ·10 min read·Aug 11, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F199fb10df8ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpadding-large-language-models-examples-with-llama-2-199fb10df8ff&user=Benjamin+Marie&userId=ad2a414578b3&source=-----199fb10df8ff---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F199fb10df8ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpadding-large-language-models-examples-with-llama-2-199fb10df8ff&source=-----199fb10df8ff---------------------bookmark_footer-----------)![](../Images/5dee339a0ba5a939a758499baeec9333.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Image by the author — Based on an image from [Pixabay](https://pixabay.com/vectors/llama-alpaca-animal-mammal-zoo-297668/)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Padding is one of the most under-documented aspects of large language models
    (LLMs). Why? Simply because LLMs are usually pre-trained without padding.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'Nonetheless, for fine-tuning LLMs on custom datasets, padding is necessary.
    Failing to correctly pad training examples may result in different kinds of unexpected
    behaviors: Null loss or infinity loss during training, over-generation, or empty
    output during inference, are all symptoms of incorrect padding.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I first explain what is padding and why it is necessary. Then,
    I show how you can find the correct padding strategy for an LLM pre-trained without
    padding. I propose two different solutions to add padding support to LLMs using
    Hugging Face’s Transformers.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Toward the end of the article, I also provide examples showing how to pad your
    training examples for Llama 2.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: After reading this article, you should be able to figure out by yourself how
    to pad training examples for LLMs without reading their documentation or tutorials.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Pad and batch
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is padding and why do we pad?
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s take one example that we wish to use for fine-tuning an LLM.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We have to turn this example into a sequence of tokens. Libraries, such as
    Transformers, usually tokenize following these steps:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'Segment the example into subwords according to a given vocabulary:'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Replace words by their index from the vocabulary to obtain a sequence of integers:'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Add special tokens to the sequence: BOS token, EOS token, UNK token, PAD token,
    etc.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*Note: For this example, I use Llama 2’s tokenizer. We will see below in detail
    how to do it.*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: In this example, only the BOS (begin of sequence) special token has been added.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: An attention mask is also generated for each training example. This mask tells
    the transformer whether it should give attention to a token (1) or not (0). The
    attention mask for this example is simple since all the tokens should be considered.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The next step is to wrap everything into tensors with Pytorch. This wrapping
    is necessary to apply the matrix operations for which CUDA and GPUs are optimized.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, let’s say that we have not one but two training examples. For the sake
    of simplicity, I’ll just duplicate the one I already have. The new tensors have
    one more row:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Both examples have the same length (of course, since they are identical). Both
    tensors have the same dimensions 2x8 (N x M).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Examples are put in the tensors to create batches so that the neural network
    can update its values after seeing N examples. Batching is critical for computing
    efficiency and model performance.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s introduce a third example that is shorter:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'After tokenization, we obtain:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'If you try to add it to our list of examples and create tensors, you will get
    an error. But imagine that we don’t have any errors, we would obtain:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '*Can you see the problem here and why it is not possible to create such tensors?*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: We have one row of a different length. We can’t apply matrix operations on this.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: In most datasets, examples don’t have the same length. We have to modify them
    to make sure that examples in the same batch have the same length.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: This is why we need “padding”.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Padding token and padding side
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can see padding as extending a sequence up to a given length by repeating
    a dummy token.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: This dummy token is a “pad token”.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: For example, our first example above has a length of 8 tokens (including the
    BOS token). Let’s say that in our batch we won’t have sequences longer than 8
    tokens. All the sequences must be 8 tokens long.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Our second example contains only 5 tokens. So we must add 3 pad tokens.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In practice, we don’t manually add “[PAD]” tokens to the sequences. Most tokenizers
    would split “[PAD]” into subwords. The pad token is usually a special token defined
    inside the tokenizer and automatically added, if necessary, along with the other
    special tokens to the sequence.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'If the pad token has the ID 32000 in the vocabulary, we would obtain:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, we have a sequence with the expected length. But one problem remains:
    We also need to modify the attention mask.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Remember, the pad tokens are dummy tokens, we don’t want the LLM to give any
    attention to them. We only introduced these tokens to fill sequences and create
    correct tensors.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: To indicate it to the model, we simply put “0” in the attention mask so that
    the model will ignore them.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, we can create correct tensors with the padded examples:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '*Note: Padding is performed when a sequence is too short given the maximum
    length. But in some cases, a sequence can be too long. In this situation, we have
    to truncate the sequence so that its size matches the maximum length.*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Another important parameter of padding is the padding side. In the example above,
    I padded right. If the model has an EOS token, the pad tokens will be added after
    it.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also pad left. In this situation, the tensors look like this:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Pad tokens are added before the BOS token.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Which side to choose mainly depends on the LLM you want to use and your downstream
    tasks. That’s why it’s important to study the model and its tokenizer before taking
    any decision. Below, we will see how to make this decision for Llama 2.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Adding padding support for causal LLM
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw, padding is (almost) always necessary for fine-tuning. Yet, many LLMs
    don’t support padding by default. It means that they don’t have a special pad
    token in their vocabulary.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Here, I present two solutions to add a pad token.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: The simple solution
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This solution is the one that you will find in most tutorials.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'It simply assigns an existing token to the pad token. For instance, you can
    declare that your pad token will be the EOS token. We would obtain tensors like
    this (right-padded, and where “2” is the ID of the EOS token):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The problem with this solution is that the LLM is now confused: Most of the
    time, the EOS token will have “0” in the attention mask. It encourages the LLM
    to ignore the original EOS token. This is not ideal since the EOS token signals
    the LLM to stop generating.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Also with this solution, we have to pad right. If you pad left, you would have
    sequences beginning with an EOS token thus early stopping the generation.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: I read several tutorials for fine-tuning Llama 2 that use the EOS token
    for left padding. If you do that, you will have a 0.0 loss and the training will
    diverge. Try it! It’s interesting to observe it.*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: In my opinion, a better alternative is to use the UNK token as the pad token.
    This token is rarely used. It appears in sequences only when a token is not in
    the vocabulary. Using it for another purpose shouldn’t have a significant impact.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Meta in its “[Llama recipes](https://github.com/facebookresearch/llama-recipes/blob/main/utils/train_utils.py)”
    uses this alternative. With this solution, the padding side doesn’t matter much.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'The alternative solution: Create a pad token from scratch'
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The UNK token already has a role in the model. Ideally, we want a pad token
    that is used only for padding.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: We have to create from scratch a pad token in the vocabulary if it doesn’t exist.
    [This is the solution recommended by Hugging Face for Llama 2](https://huggingface.co/docs/transformers/main/model_doc/llama2).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: With libraries such as transformers, it’s easy to extend a vocabulary.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to create a pad token, you have to follow these steps:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: add the pad token as a special token in the vocabulary of the LLM
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: resize the token embeddings
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: retrain the token embeddings (optional)
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are on a budget and use LoRa for fine-tuning, you may want to skip the
    last step since the token embeddings can weigh several 100 million parameters.
    Moreover, in my experiments, retraining the embeddings always led to worse results,
    indicating that my fine-tuning dataset is not large enough or that good hyperparameters
    are difficult to find.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'Case study: padding Llama 2 with Hugging Face’s Transformers'
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will enable padding for Llama 2\. To replicate each step,
    you will need access to Llama 2 on Hugging Face. I explained [how to get Llama
    2 in this article](https://kaitchup.substack.com/p/fine-tune-llama-2-on-your-computer).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: I share* [*a notebook (#8) replicating all these steps on The Kaitchup,
    my substack newsletter.*](https://kaitchup.substack.com/publish/post/134749542)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install the Transformers library:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, we import transformers and load the tokenizer. Make sure you put your
    Hugging Face’s access token there:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We define two training examples:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'If we put in the same batch the prompt1 twice, everything goes well:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Output:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'But if you add prompt2, you will get an error as expected:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Output:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: It’s clear that the tokenizer didn’t pad the examples.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'We can solve this problem by simply using the UNK token as a pad token, as
    follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In this example, I asked the tokenizer to pad up to max_length. I set max_length
    to 20\. If your example contains 10 tokens, the tokenizer will add 10 pad tokens.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The alternative is to create a pad token from scratch. With Hugging Face’s transformers,
    we can do this with the method “add_special_tokens”.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Output:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Don’t forget to resize the token embeddings of Llama 2 after you added the
    pad token to its vocabulary. I explained how to do it in this article:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 添加了填充标记到Llama 2的词汇表后，不要忘记调整Llama 2的令牌嵌入。我在这篇文章中解释了如何做到这一点：
- en: '[](https://kaitchup.substack.com/p/fine-tune-llama-2-on-your-computer?source=post_page-----199fb10df8ff--------------------------------)
    [## Fine-tune Llama 2 on Your Computer with QLoRa and TRL'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 在你的计算机上使用QLoRa和TRL对Llama 2进行微调](https://kaitchup.substack.com/p/fine-tune-llama-2-on-your-computer?source=post_page-----199fb10df8ff--------------------------------)'
- en: On Guanaco and with the correct padding
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Guanaco和正确的填充设置下
- en: kaitchup.substack.com](https://kaitchup.substack.com/p/fine-tune-llama-2-on-your-computer?source=post_page-----199fb10df8ff--------------------------------)
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[kaitchup.substack.com](https://kaitchup.substack.com/p/fine-tune-llama-2-on-your-computer?source=post_page-----199fb10df8ff--------------------------------)'
- en: Conclusion
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Once you understand it, padding is very straightforward.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你理解了，填充就非常简单。
- en: Using the UNK token for padding, or creating a pad token from scratch, are very
    safe solutions that will work for almost all causal LLMs. But you should always
    have a look at how the tokenizer works. At least you should be aware of the special
    tokens it already supports. For instance, not all LLMs have a UNK token, some
    LLMs have a pad token that is not explicitly defined as a pad token in the vocabulary,
    etc.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 使用UNK标记进行填充，或者从头创建填充标记，是非常安全的解决方案，几乎适用于所有因果语言模型。但你应该始终查看分词器的工作方式。至少你应该了解它已经支持的特殊标记。例如，并非所有语言模型都有UNK标记，有些语言模型的填充标记在词汇表中没有明确定义为填充标记，等等。
- en: As usual, if you have any questions, please leave drop a comment. I’ll try to
    answer it.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，如果你有任何问题，请留下评论。我会尽量回答。
