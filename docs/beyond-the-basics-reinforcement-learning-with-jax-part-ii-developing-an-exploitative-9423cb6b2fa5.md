# 超越基础知识：使用 Jax 的强化学习 —— 第二部分：开发一种替代 A/B 测试的利用性方法

> 原文：[https://towardsdatascience.com/beyond-the-basics-reinforcement-learning-with-jax-part-ii-developing-an-exploitative-9423cb6b2fa5?source=collection_archive---------4-----------------------#2023-06-02](https://towardsdatascience.com/beyond-the-basics-reinforcement-learning-with-jax-part-ii-developing-an-exploitative-9423cb6b2fa5?source=collection_archive---------4-----------------------#2023-06-02)

## 一种关于多臂赌博机和探索-利用困境的实用介绍

[](https://medium.com/@Lando-L?source=post_page-----9423cb6b2fa5--------------------------------)[![Lando L](../Images/9675667c061daf1d9ac527b49b7a8a9f.png)](https://medium.com/@Lando-L?source=post_page-----9423cb6b2fa5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9423cb6b2fa5--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9423cb6b2fa5--------------------------------) [Lando L](https://medium.com/@Lando-L?source=post_page-----9423cb6b2fa5--------------------------------)

·

[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F395c5e41bd1e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-the-basics-reinforcement-learning-with-jax-part-ii-developing-an-exploitative-9423cb6b2fa5&user=Lando+L&userId=395c5e41bd1e&source=post_page-395c5e41bd1e----9423cb6b2fa5---------------------post_header-----------) 发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9423cb6b2fa5--------------------------------) ·19 分钟阅读·2023年6月2日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9423cb6b2fa5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-the-basics-reinforcement-learning-with-jax-part-ii-developing-an-exploitative-9423cb6b2fa5&user=Lando+L&userId=395c5e41bd1e&source=-----9423cb6b2fa5---------------------clap_footer-----------)

--

[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9423cb6b2fa5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-the-basics-reinforcement-learning-with-jax-part-ii-developing-an-exploitative-9423cb6b2fa5&source=-----9423cb6b2fa5---------------------bookmark_footer-----------)![](../Images/696f574091a931a758adfa8f5a628807.png)

照片由 [Carl Raw](https://unsplash.com/@carltraw?utm_source=medium&utm_medium=referral) 提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

在我们上一篇博客文章中，我们探讨了强化学习范式，*深入研究*了其有限马尔可夫决策过程、策略和价值函数的核心概念。现在，我们准备应用新获得的知识，通过多臂老虎机发现传统 A/B 测试的替代方法。

# 课程链接

+   [第一部分：介绍和核心概念](https://medium.com/@Lando-L/beyond-the-basics-reinforcement-learning-with-jax-part-i-introduction-and-core-concepts-4f85f3478f5)

+   **第二部分：开发一种替代 A/B 测试的利用性方法**

# A/B 测试是否被过分高估了？

回想一下你上次浏览视频流媒体服务寻找新电影的情景。如果你之前不知道该找什么，你可能会受到*选择悖论*¹的影响。拥有如此多的潜在好电影可能会使做出明智的决定变得困难。相反，我们常常依赖于几乎可以立即做出的简单决定。因此，我们往往根据视频的缩略图和标题进行比较。了解到这一效果，视频制作和流媒体公司努力优化缩略图和标题，以提高电影的点击率。如果没有观众偏好的先前知识，制作吸引人的缩略图和标题的过程变成了*试错*。 （我希望这现在让你有所领悟！）

## 传统方法

让我们设想自己是决策者，负责为流媒体网站上新发布的电影选择合适的缩略图和标题。除了原始的电影缩略图和标题外，我们还获得了一组建议，包括两个缩略图和两个标题。在这种情况下，传统的数据驱动方法是进行 A/B 测试，将原始版本的点击率与建议版本的点击率进行比较。虽然 A/B 测试的优点在于，当样本量足够大时，可以确定点击率的差异是否具有统计显著性，但它限制我们只能同时比较两个变体，并且通常需要大量样本来确定结果。此外，如果其中一个变体的表现远远优于另一个，我们仍然被迫将表现较差的变体展示给一半的客户，直到实验结束，这可能导致金钱损失。

## 强化学习方法

或者，我们可以设置一个*多臂赌博机*（MAB）实验。MABs是强化学习（RL）问题的简化，因为它们可以完全由一组动作和一个奖励函数定义。这使得它们实际上类似于只有一个状态的有限*马尔可夫决策过程*（MDP）。与MDP不同的是，MABs中的动作是相互独立的，这意味着在每个时间步骤，相同的动作集会提供相同的奖励分布。因此，如果我们知道可用动作的确切奖励分布，我们可以简单地使用贪婪方法选择奖励最高的动作。相反，MDP有时需要我们采取‘次优’动作以达到高奖励状态。然而，MABs可以建模的问题比MDP少。例如，在[上一篇文章](https://medium.com/@Lando-L/beyond-the-basics-reinforcement-learning-with-jax-part-i-introduction-and-core-concepts-4f85f3478f5)中的音乐练习环境中，需要引入状态来建模我们朋友的情绪，而这在MABs中是做不到的。

我们可以通过定义如下的动作和奖励函数来建模我们视频流平台的挑战：

+   每次用户访问网站时，我们必须选择显示哪个版本。我们的选择包括展示原版电影，或是由两个缩略图和两个标题组合而成的四种变体之一。

+   一旦我们选择了要展示的变体，用户可以选择观看或不观看我们的电影。因此，我们的奖励函数是二元的，如果用户选择不观看电影则奖励为零，如果他们决定观看电影则奖励为一。

与传统的A/B测试相比，MAB方法有许多优势；它允许同时测试无限数量的变体，动态降低表现较差的变体的频率，并且需要更少的样本量来收敛——这些都能带来成本节省。缺点是它不提供传统A/B测试的统计显著性；然而，当实验的影响不涉及人们的福祉（如大多数情况下），统计显著性并不是严格必要的。

# 让我看看一些代码

最后，我们将开始编写代码。对于本课程的代码示例，我们将使用 Python 作为我们的编程语言，并且主要使用 [Jax](https://jax.readthedocs.io/en/latest/index.html) 作为我们的机器学习（ML）框架。Jax 是由 Google 开发的 Python 库，专门为 ML 研究设计。与 Tensorflow 和 PyTorch 不同，它采用了函数式编程（FP）范式，使其高度可组合，并推动了纯函数的概念，即没有副作用。这意味着所有的状态变化，如参数更新或随机生成器的拆分，必须显式地进行。虽然这可能比它们的面向对象编程（OOP）等效方法需要更多的代码行，但它使开发人员对状态变化有了完全的控制，从而增加了对代码的理解，减少了意外情况。

（完整的代码可以在 [GitHub](https://github.com/Lando-L/reinforcement-learning-with-jax/blob/91404625fd22fe62cb3e644c0ee8d51dd6bf2f24/2-developing-an-exploitative-alternative-to-a-b-testing.ipynb) 和 [Kaggle](https://www.kaggle.com/code/landol/exploitative-alternative-to-a-b-testing) 上以 Jupyter notebook 形式获得。）

## 实现环境

第一步，我们实现了视频流平台挑战的环境。从高层次看，每当有人访问平台以浏览可用的电影选项时，环境需要询问代理显示哪些缩略图和标题，然后通知代理访客选择了哪部电影。这项任务的复杂性将取决于平台架构的成熟度，可能从简单地更改几行代码到开发一个全新的服务。

本课程的目的是通过实现一个三步过程来保持环境简单：

1.  询问代理希望展示五个变体中的哪一个。

1.  模拟访客的选择。

1.  通知代理其决策的奖励。

由于我们使用 Jax 作为主要 ML 框架，我们需要导入 Jax 和三个模块 *numpy,* *lax* 和 *random*²：

```py
import jax

# Numpy API with hardware acceleration and automatic differentiation
from jax import numpy as jnp

# Low level operators
from jax import lax

# API for working with pseudorandom number generators
from jax import random
```

接下来，我们设置环境的常量，包括随机种子以确保可重复性、我们想要模拟的访客数量和预期的点击率。需要注意的是，在现实世界中，点击率被认为是未知的。由于我们没有进行实际实验，我们必须模拟访客的点击行为。为此，我们为五个变体定义不同的点击率，以模仿访客的偏好，其中原始变体的点击率为 4.2%，四个变体的点击率分别为 3%、3.5%、3.8% 和 4.5%。

```py
# Random seed to make our experiment replicable 
SEED = 42

# Number of visitors we want to simulate
NUM_VISITS = 10000

# Expected click rates for the five variants with the
CLICK_RATES = [0.042, 0.03, 0.035, 0.038, 0.045]
```

最后，我们定义了一个通用函数，用于模拟用户访问或环境中的单一步骤。该函数包括三个步骤，接近我们之前设定的高层次实现：

1.  执行代理的策略以确定显示给用户的变体。

1.  随机模拟用户是否点击了电影。

1.  根据显示的变体和相关的奖励更新代理的参数。

```py
def visit(state, timestep, click_rates, policy_fn, update_fn):
    """
    Simulates a user visit.
    """

    # Unpacking the environment state into
    # the agent's parameters and the random number generator
    params, rng = state

    # Splitting the random number generator
    next_rng, policy_rng, user_rng = random.split(rng, num=3)

    # Selecting the variant to show the user, based on
    # the given policy, the agent's parameters, and the current timestep
    variant = policy_fn(params, timestep, policy_rng)

    # Randomly simulating the user click, based on
    # the variant's click rate
    clicked = random.uniform(user_rng) < click_rates[variant]

    # Calculating the agent's updated parameters, based on
    # the current parameters, the selected variant,
    # and whether or not the user clicked
    next_params = update_fn(params, variant, clicked)

    # Returning the updated experiment state (params and rng) and
    # whether or not the user clicked
    return (next_params, next_rng), clicked
```

在继续之前，让我们讨论一下函数签名。如果我们固定参数*click_rates*、*policy_fn*和*update_fn*，*visit*函数接受环境状态和当前时间步作为其参数，并返回一个包含下一个环境状态和一个编码二元奖励的布尔值的元组。在Haskell表示法中，函数签名看起来像这样：

```py
state -> timestep -> (state, Bool)
```

因此，为了在我们的环境中模拟第*n*步，我们将函数传递给第*n*状态和第*n*时间步，*并接收第*(n+1)*状态和第*n*奖励。对于第*(n+1)*步，我们使用相同的函数，传递给它上一个函数调用返回的第*(n+1)*状态和时间步*n+1*。

```py
# Initialising the state
s0 = ...

# Simulating the time steps 0, 1, and 2
s1, r0 = visit(s0, 0)
s2, r1 = visit(s1, 1)
s3, r2 = visit(s2, 2)
```

对于习惯于面向对象编程（OOP）的人来说，每次调用*visit*函数时都必须传递状态和时间步参数可能会显得繁琐。然而，在这个例子中，使用纯函数实现环境相比于面向对象的方法有几个优势。首先，它明确了环境依赖于哪些参数，从而消除了可能影响结果的隐藏全局变量。其次，它使得测试环境与各种状态和时间步变得更加容易，而不必设置和读取环境的内部状态。最后，我们将发现来自Jax库的一个有用函数，它提供了状态管理，从而大幅减少了调用端所需的代码。

## 实现策略

环境就绪后，我们现在可以实现我们的决策过程或策略。我们已经确定了MABs的最佳策略。考虑到动作-价值分布，采取的最佳行动是预期收益最高的那个，这被称为*exploitation*。然而，由于我们不知道实际的动作-价值分布，我们必须尝试所有可用的选项至少一次，以估计分布，这个过程通常被称为*exploration*。探索与利用之间的微妙平衡是强化学习中的一个反复出现的主题，并将在整个课程中详细讨论。

我们将在这篇博客文章中介绍的三种策略是*epsilon-greedy*策略、*Boltzmann*策略和*upper-confidence-bound*策略。它们都是*action-value methods*，即它们明确估计所有动作的值，并基于这些估计做出决策。最后，我们将介绍一种基于*Thompson sampling*启发式方法的奖励策略，它被认为是一种*bayesian method*。

## 动作-价值方法

代理估计动作价值的最简单方法是平均它迄今为止收到的每个变体的奖励。在这里，*Q* 表示代理在时间步 *t* 对变体 *a* 的动作价值估计。

![](../Images/408d5f320d66872fc42a31e65dd673e4.png)

与其每轮都重新计算平均值，不如以增量方式实现动作价值估计。在这里，*Q* 存储每个变体 *a* 当前的动作价值估计，*N* 计数 *a* 出现的频次，*R* 表示在时间步 *t* 收到的奖励。

![](../Images/d129c762cd14f25aa335c1c61ac7dfc5.png)

让我们实现两个函数来处理动作价值估计。第一个函数初始化每个变体 *a* 的查找表 *Q* 和 *N*，将所有变体的估计值设置为乐观的初始值 1（或点击率 100%）。第二个函数根据上面描述的增量更新定义更新 *Q* 和 *N*。

```py
def action_value_init(num_variants):
    """
    Returns the initial action values
    """

    return {
        'n': jnp.ones(num_variants, dtype=jnp.int32),
        'q': jnp.ones(num_variants, dtype=jnp.float32)
    }

def action_value_update(params, variant, clicked):
    """
    Calculates the updated action values
    """

    # Reading n and q parameters of the selected variant
    n, q = params['n'][variant], params['q'][variant]

    # Converting the boolean clicked variable to a float value
    r = clicked.astype(jnp.float32)

    return {
        # Incrementing the counter of the taken action by one
        'n': params['n'].at[variant].add(1),

        # Incrementally updating the action-value estimate
        'q': params['q'].at[variant].add((r - q) / n)
    }
```

我们选择使用函数式方法来实现动作价值估计的初始化和更新，这类似于环境函数的实现。与 numpy 数组不同，Jax 数组是不可变的，因此不能就地更新；相反，每次更新会返回一个包含所做更改的新副本，而原始数组保持不变。

## Epsilon-greedy 策略

epsilon-greedy 策略定义了一种随机方法来平衡探索和利用的权衡。根据超参数 *ε*，它随机决定是选择具有最高动作价值 *Q* 的变体 *a*，还是选择一个均匀随机的变体。

![](../Images/ba8de89e9dd50665f3b42e008be29b3e.png)

在 Jax 中，我们可以使用 *cond* 函数定义条件策略。它接受一个谓词、两个函数和一个可变数量的参数。根据谓词的结果，*cond* 采用两个函数中的一个，并传递给它给定的参数。

```py
def epsilon_greedy_policy(params, timestep, rng, epsilon):
    """
    Randomly selects either the variant with highest action-value,
    or an arbitrary variant.
    """

    # Selecting a random variant
    def explore(q, rng):
        return random.choice(rng, jnp.arange(len(q)))

    # Selecting the variant with the highest action-value estimate
    def exploit(q, rng):
        return jnp.argmax(q)

    # Splitting the random number generator 
    uniform_rng, choice_rng = random.split(rng)

    # Deciding randomly whether to explore or to exploit
    return lax.cond(
        random.uniform(uniform_rng) < epsilon,
        explore,
        exploit,
        params['q'],
        choice_rng
    )
```

## Boltzmann 策略

Boltzmann 或 softmax 策略类似于 epsilon-greedy 策略，因为它是一种基于动作价值估计的随机策略。这种方法从应用 softmax 函数到动作价值估计 *Q* 后得到的概率分布中随机抽取一个变体 *a*。探索与利用的权衡可以通过温度超参数 *τ* 来控制，其中较低的温度有利于利用，较高的温度促进探索。每个变体被选择的概率 *P* 定义如下：

![](../Images/f61eeaa02bfa7055380975cc6a880f67.png)

在 Jax 中，这可以通过利用来自随机模块的 *choice* 函数来轻松实现，该函数以应用于动作价值估计的 softmax 函数作为参数。

```py
def boltzmann_policy(params, timestep, rng, tau):
    """
    Randomly selects a variant proportional to the current action-values
    """

    return random.choice(
        rng,
        jnp.arange(len(params['q'])),
        # Turning the action-value estimates into a probability distribution
        # by applying the softmax function controlled by tau
        p=jax.nn.softmax(params['q'] / tau)
    )
```

## Upper-Confidence-Bound 策略

我们现在将讨论一种具有确定性方法的策略来平衡探索与利用。与前面讨论的策略类似，它通过优先选择具有高动作值估计的变体来鼓励利用。然而，它并不依赖随机性来进行探索，而是利用一种启发式方法来鼓励选择选择次数较少的变体。

这个启发式方法通过*在面对不确定性时保持乐观*来实现。这意味着，每个变体都被赋予了比我们当前的动作值估计更好的怀疑余地。在实验过程中，每次选择一个变体并观察到实际奖励时，我们对我们的动作值估计会变得更有信心，并减少该变体的怀疑余地。

从形式上讲，我们将这种乐观猜测定义为变体的*上置信界*（UCB），它由置信超参数*c*缩放，并加到当前的动作值估计上。最后，我们选择总和最高的变体。

![](../Images/c3220146cde79e66ec374fcf4aadc4e0.png)

UCB策略是我们发现的第一个同时奖励探索和利用的策略：

+   对于两个具有相同动作值估计*Q*的变体，我们将选择选择次数较少的变体*N*。

+   对于两个具有相同选择次数*N*的变体，我们将选择具有更高动作值估计*Q*的变体。

为了确保所有策略的函数定义一致，UCB策略需要一个随机数生成器参数，即使它是一个确定性算法。

```py
def upper_confidence_bound_policy(params, timestep, rng, confidence):
    """
    Selects the variant with highest action-value plus upper confidence bound
    """

    # Read n and q parameters
    n, q = params['n'], params['q']

    # Calculating each variant's upper confidence bound
    # and selecting the variant with the highest value
    return jnp.argmax(q + confidence * jnp.sqrt(jnp.log(timestep) / n))
```

## 贝叶斯方法

讨论的动作值方法对我们五个变体的未知点击率进行点估计。然而，我们现在采用一种更贝叶斯的方法，将变体的点击率视为一组独立的随机变量。具体而言，我们通过将其建模为遵循*贝塔*分布的随机变量²来定义我们对变体点击率*C*的当前信念。

![](../Images/5bdde6a5e07ca711a8a6786940822d5e.png)

贝塔分布由两个参数*a*和*b*特征化，这些参数可以解释为变体*i*在展示时被点击的次数与未被点击的次数。比较贝叶斯方法与动作值方法时，我们可以使用随机变量*C*的期望值*E*来定义我们的最佳猜测，这可以通过将变体被点击的次数除以展示的次数来确定：

![](../Images/4fe83c7070c7a2e2397fd35be18f8c5b.png)

我们定义了两个函数来处理贝塔分布，类似于动作值方法。第一个函数初始化每个变体的均匀贝塔先验，而第二个函数通过将*a*或*b*参数增加一来计算后验贝塔分布。

```py
def beta_init(num_variants):
    """
    Returns the initial hyperparameters of the beta distribution
    """

    return {
        'a': jnp.ones(num_variants, dtype=jnp.int32),
        'b': jnp.ones(num_variants, dtype=jnp.int32)
    }

def beta_update(params, variant, clicked):
    """
    Calculates the updated hyperparameters of the beta distribution
    """

    # Incrementing alpha by one
    def increment_alpha(a, b):
        return {'a': a.at[variant].add(1), 'b': b}

    # Incrementing beta by one
    def increment_beta(a, b):
        return {'b': b.at[variant].add(1), 'a': a}

    # Incrementing either alpha or beta
    # depending on whether or not the user clicked
    return lax.cond(
        clicked,
        increment_alpha,
        increment_beta,
        params['a'],
        params['b']
    )
```

## 汤普森采样策略

TS策略基于两步启发式方法，首先从我们的贝塔分布中抽取随机点击率样本，然后选择点击率样本最高的变体。我们收到的反馈会立即被纳入该变体的贝塔分布，从而使分布更接近实际点击率。

与UCB策略类似，这种方法同时奖励探索和利用：

+   给定两个均值相同的变体，方差更大的变体有更高的被选择几率，因为它具有更广泛的分布，采样时更容易得到更高的行动值。

+   给定两个方差相同的变体，均值更高的变体更常被选择，因为它更有可能采样到更高的行动值。

对于TS策略的实现，我们使用Jax的`*random*`模块来根据变体的贝塔分布采样随机点击率，然后选择点击率样本最高的变体。

```py
def thompson_policy(params, timestep, rng):
    """
    Randomly sampling click rates for all variants
    and selecting the variant with the highest sample
    """

    return jnp.argmax(random.beta(rng, params['a'], params['b']))
```

## 实现评估

环境和策略准备好后，我们终于可以进行实验并比较结果。在继续之前，我想强调，这个实验旨在演示算法的工作原理，而不是实证评估它们的性能。为了执行以下实现，我们需要从Python的`*functools*`库中导入`*partial*`函数，并从`*matplotlib*`中导入`*pyplot*`：

```py
from functools import partial
from matplotlib import pyplot as plt
```

`*evaluate*`函数负责执行`*visit*`函数，这一过程由常量集合、参数初始化和更新函数以及策略指导。评估的输出是最终环境状态，包括策略的最终参数和最终随机数生成器状态，以及点击历史。我们利用Jax的`*scan*`函数来确保实验状态得以传递，用户点击数被累计。此外，`*just-in-time*`（JIT）编译用于优化性能，而`*partial*`则用于固定`*click_rate*`、`*policy_fn*`和`*update_fn*`参数，以确保它们与预期签名匹配。

```py
def evaluate(policy_fn, init_fn, update_fn):
    """
    Simulating the environment for NUM_VISITS users
    while accumulating the click history
    """

    return lax.scan(
        # Compiling the visit function using just-in-time (JIT) compilation
        # for better performance
        jax.jit(
            # Partially applying the visit function by fixing
            # the click_rates, policy_fn, and update_fn parameters 
            partial(
                visit,
                click_rates=jnp.array(CLICK_RATES),
                policy_fn=jax.jit(policy_fn),
                update_fn=jax.jit(update_fn)
            )
        ),

        # Initialising the experiment state using
        # init_fn and a new PRNG key
        (init_fn(len(CLICK_RATES)), random.PRNGKey(SEED)),

        # Setting the number of steps in our environment
        jnp.arange(1, NUM_VISITS + 1)
    )
```

`*regret*`函数是我们评估的最后一个组件。在强化学习术语中，`regret`定义为我们因采取次优行动而错失的奖励量，只有在知道最佳行动的情况下才能计算。根据点击历史，我们的`*regret*`函数计算环境中每一步采取的行动的遗憾值。

```py
def regret(history):
    """
    Calculates the regret for every action in the environment history
    """

    # Calculating regret with regard to picking the optimal (0.045) variant
    def fn(acc, reward):
        n, v = acc[0] + 1, acc[1] + reward
        return (n, v), 0.045 - (v / n)

    # Calculating regret values over entire history
    _, result = lax.scan(
        jax.jit(fn),
        (jnp.array(0), jnp.array(0)),
        history
    )

    return result
```

接下来，我们对所有四种策略进行评估并可视化遗憾值。请注意，策略的超参数尚未进行微调，而是设置为适合各种MAB问题的通用默认值。

```py
# Epsilon greedy policy
(epsilon_greedy_params, _), epsilon_greedy_history = evaluate(
    policy_fn=partial(epsilon_greedy_policy, epsilon=0.1),
    init_fn=action_value_init,
    update_fn=action_value_update
)

# Boltzmann policy
(boltzmann_params, _), boltzmann_history = evaluate(
    policy_fn=partial(boltzmann_policy, tau=1.0),
    init_fn=action_value_init,
    update_fn=action_value_update
)

# Upper confidence bound policy
(ucb_params, _), ucb_history = evaluate(
    policy_fn=partial(upper_confidence_bound_policy, confidence=2),
    init_fn=action_value_init,
    update_fn=action_value_update
)

# Thompson sampling policy
(ts_params, _), ts_history = evaluate(
    policy_fn=thompson_policy,
    init_fn=beta_init,
    update_fn=beta_update
)

# Visualisation
fig, ax = plt.subplots(figsize=(16, 8))

x = jnp.arange(1, NUM_VISITS + 1)

ax.set_xlabel('Number of visits')
ax.set_ylabel('Regret')

ax.plot(x, jnp.repeat(jnp.mean(jnp.array(CLICK_RATES)), NUM_VISITS), label='A/B Testing')
ax.plot(x, regret(epsilon_greedy_history), label='Espilon Greedy Policy')
ax.plot(x, regret(boltzmann_history), label='Boltzmann Policy')
ax.plot(x, regret(ucb_history), label='UCB Policy')
ax.plot(x, regret(ts_history), label='TS Policy')

plt.legend()
plt.show()
```

结果图如下面所示，绘制了我们的策略在访问次数上的遗憾。我们可以清楚地看到，所有策略在遗憾方面都优于假设的 A/B 测试场景。在这个特定场景中，epsilon-贪婪和 TS 策略的表现似乎略优于 Boltzmann 和 UCB 策略。

![](../Images/b991caf84e166570ab27028e50317295.png)

作者创建的可视化图。

# 超越遗憾

在进行机器学习实验时，通常建议定义一组指标来衡量除目标函数外的定性性能。然而，对于强化学习实验，确定合适的指标往往并不那么简单。在我们的案例中，我选择深入研究我们策略的最终参数，并将动作值估计与实际点击率以及不同策略下每个变体的选择频率进行比较。

## 动作值估计

我们通过计算与实际点击率的 *均方根误差*（RMSE）来比较策略的动作值估计的准确性，如下表所示。

```py
|      Name      | Original |   V1   |   V2   |   V3   |   V4   |  RMSE  |
|----------------|----------|--------|--------|--------|--------|--------|
| Ground truth   |   0.0420 | 0.0300 | 0.0350 | 0.0380 | 0.0450 | 0.0000 |
| Epsilon greedy |   0.0420 | 0.0367 | 0.0378 | 0.0256 | 0.0375 | 0.0072 |
| Boltzmann      |   0.0397 | 0.0291 | 0.0383 | 0.0346 | 0.0449 | 0.0024 |
| UCB            |   0.0399 | 0.0259 | 0.0247 | 0.0390 | 0.0518 | 0.0060 |
| TS             |   0.0390 | 0.0425 | 0.0370 | 0.0393 | 0.0441 | 0.0059 |
```

+   出乎意料的是，尽管整体表现良好，但事实证明 epsilon-贪婪策略过早地专注于剥削第二优的变体，而其他策略正确地识别了最后一个变体为最佳变体。

+   Boltzmann 策略在预测变体的点击率方面表现最佳。

+   UCB 策略和 TS 策略表现相当。虽然 UCB 似乎高估了 V4 的价值并低估了 V2，但 TS 似乎高估了 V1 并低估了原始版本。

## 变体计数器

我想讨论的第二个特征是不同策略下每个变体的选择频率。在这里，我们仅仅比较了实验中 10,000 名访客的绝对变体计数。

```py
|      Name      | Original |  V1  |  V2  |  V3  |  V4  |
|----------------|----------|------|------|------|------|
| Epsilon greedy |     8334 |  409 |  291 |  352 |  614 |
| Boltzmann      |     1991 | 1998 | 2012 | 2024 | 1984 |
| UCB            |     2079 | 1701 | 1661 | 2051 | 2508 |
| TS             |     1901 |  963 | 1324 |  735 | 5077 |
```

+   发现 epsilon-贪婪算法过早地进行剥削，因为它选择原始版本的频率超过了 83%。

+   Boltzmann 策略在预测变体的点击率方面表现出色，这可能是因为它对所有变体进行了均匀的探索。

+   TS 策略的遗憾值低于 UCB 策略，这可能是因为它更广泛地剥削了最佳变体。

## 结论

在进一步检查我们的观察结果后，我们发现当前实验设置中有几个改进的地方。

+   epsilon-贪婪策略似乎过于依赖剥削，因此我们应该增加 epsilon 超参数，以鼓励更全面的探索。

+   Boltzmann 策略似乎进行了更多的探索而非剥削，导致准确的点击率预测，但错过了一些奖励。这可能表明其温度超参数应该增加。

+   UCB 和 TS 策略表现良好，但仍有改进的空间。我们可以分别调整置信度超参数和初始 beta 先验，以进一步优化它们的性能。

除了建议的超参数更改外，我鼓励感兴趣的读者作为练习探索更复杂的改进：

+   利用多种随机种子并平均结果，以减少实验结果中的偏差。

+   实施动态调度器，逐渐减少探索行为，例如在 epsilon-贪婪和 UCB 策略中降低 epsilon 或置信度参数，在 Boltzmann 策略中增加温度参数。

# **接下来是什么？**

MAB 是一种极其多用途的工具，用于解决各种问题。在本文中，我们讨论了四种 MAB 策略，这些策略可以用来解决视频流媒体平台场景中的挑战。其他 MAB 使用案例包括在线广告的广告选择、搜索引擎查询结果的优先级排序，以及不同项目的资源分配。

此外，MAB 还可以进一步扩展以包含额外的信息。在我们的例子中，如果我们能够获取更多的上下文数据，例如个体用户的偏好和每个变体的特征向量，那么决策过程可能会得到改善。这类问题被称为上下文赌博问题，我们将在后续的文章中进一步探讨近似解决方法。

[1] Schwartz, Barry, and Barry Schwartz. “选择的悖论：为什么更多就是更少。” 纽约：Ecco，2004。

[2] 我们选择 Beta 分布来建模点击率，因为它是伯努利分布的共轭先验，而伯努利分布是我们用来描述访客点击或不点击展示变体的似然函数。为了更好地理解概率分布和贝叶斯推断，我推荐参考 Christopher M. Bishop 和 Nasser M. Nasrabadi 的《模式识别与机器学习》一书。
