- en: Running Llama 2 on CPU Inference Locally for Document Q&A
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/running-llama-2-on-cpu-inference-for-document-q-a-3d636037a3d8?source=collection_archive---------0-----------------------#2023-07-18](https://towardsdatascience.com/running-llama-2-on-cpu-inference-for-document-q-a-3d636037a3d8?source=collection_archive---------0-----------------------#2023-07-18)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Clearly explained guide for running quantized open-source LLM applications on
    CPUs using Llama 2, C Transformers, GGML, and LangChain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://kennethleungty.medium.com/?source=post_page-----3d636037a3d8--------------------------------)[![Kenneth
    Leung](../Images/2514dffb34529d6d757c0c4ec5f98334.png)](https://kennethleungty.medium.com/?source=post_page-----3d636037a3d8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3d636037a3d8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3d636037a3d8--------------------------------)
    [Kenneth Leung](https://kennethleungty.medium.com/?source=post_page-----3d636037a3d8--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdcd08e36f2d0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frunning-llama-2-on-cpu-inference-for-document-q-a-3d636037a3d8&user=Kenneth+Leung&userId=dcd08e36f2d0&source=post_page-dcd08e36f2d0----3d636037a3d8---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3d636037a3d8--------------------------------)
    ·11 min read·Jul 18, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3d636037a3d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frunning-llama-2-on-cpu-inference-for-document-q-a-3d636037a3d8&user=Kenneth+Leung&userId=dcd08e36f2d0&source=-----3d636037a3d8---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3d636037a3d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frunning-llama-2-on-cpu-inference-for-document-q-a-3d636037a3d8&source=-----3d636037a3d8---------------------bookmark_footer-----------)![](../Images/887641ba170cdfdd4548d8d2553f96b1.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [NOAA](https://unsplash.com/@noaa?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/computing-cloud?orientation=landscape&license=free&utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: Third-party commercial large language model (LLM) providers like OpenAI’s GPT4
    have democratized LLM use via simple API calls. However, teams may still require
    self-managed or private deployment for model inference within enterprise perimeters
    due to various reasons around data privacy and compliance.
  prefs: []
  type: TYPE_NORMAL
- en: The proliferation of open-source LLMs has fortunately opened up a vast range
    of options for us, thus reducing our reliance on these third-party providers.
  prefs: []
  type: TYPE_NORMAL
- en: When we host open-source models locally on-premise or in the cloud, the dedicated
    compute capacity becomes a key consideration. While GPU instances may seem the
    most convenient choice, the costs can easily spiral out of control.
  prefs: []
  type: TYPE_NORMAL
- en: In this easy-to-follow guide, we will discover how to run quantized versions
    of open-source LLMs on local CPU inference for retrieval-augmented generation
    (aka document Q&A) in Python. In particular, we will leverage the latest, highly-performant
    [Llama 2](https://ai.meta.com/llama/) chat model in this project.
  prefs: []
  type: TYPE_NORMAL
- en: Contents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '***(1)*** [*Quick Primer on Quantization*](#afd1)…'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
