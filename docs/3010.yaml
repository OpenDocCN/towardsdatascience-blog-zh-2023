- en: 'Optical Flow with RAFT: Part 1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Optical Flow with RAFT: 第1部分'
- en: 原文：[https://towardsdatascience.com/optical-flow-with-raft-part-1-f984b4a33993?source=collection_archive---------1-----------------------#2023-10-03](https://towardsdatascience.com/optical-flow-with-raft-part-1-f984b4a33993?source=collection_archive---------1-----------------------#2023-10-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/optical-flow-with-raft-part-1-f984b4a33993?source=collection_archive---------1-----------------------#2023-10-03](https://towardsdatascience.com/optical-flow-with-raft-part-1-f984b4a33993?source=collection_archive---------1-----------------------#2023-10-03)
- en: Dive into Deep Learning for Optical Flow
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入探讨光流的深度学习
- en: '[](https://medium.com/@itberrios6?source=post_page-----f984b4a33993--------------------------------)[![Isaac
    Berrios](../Images/e36cdbfc91b71ceb91c6e4191e1e0833.png)](https://medium.com/@itberrios6?source=post_page-----f984b4a33993--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f984b4a33993--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f984b4a33993--------------------------------)
    [Isaac Berrios](https://medium.com/@itberrios6?source=post_page-----f984b4a33993--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@itberrios6?source=post_page-----f984b4a33993--------------------------------)[![Isaac
    Berrios](../Images/e36cdbfc91b71ceb91c6e4191e1e0833.png)](https://medium.com/@itberrios6?source=post_page-----f984b4a33993--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f984b4a33993--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f984b4a33993--------------------------------)
    [Isaac Berrios](https://medium.com/@itberrios6?source=post_page-----f984b4a33993--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffbadc8e8ee44&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptical-flow-with-raft-part-1-f984b4a33993&user=Isaac+Berrios&userId=fbadc8e8ee44&source=post_page-fbadc8e8ee44----f984b4a33993---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f984b4a33993--------------------------------)
    ·14 min read·Oct 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff984b4a33993&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptical-flow-with-raft-part-1-f984b4a33993&user=Isaac+Berrios&userId=fbadc8e8ee44&source=-----f984b4a33993---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffbadc8e8ee44&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptical-flow-with-raft-part-1-f984b4a33993&user=Isaac+Berrios&userId=fbadc8e8ee44&source=post_page-fbadc8e8ee44----f984b4a33993---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f984b4a33993--------------------------------)
    ·14分钟阅读·2023年10月3日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff984b4a33993&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptical-flow-with-raft-part-1-f984b4a33993&user=Isaac+Berrios&userId=fbadc8e8ee44&source=-----f984b4a33993---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff984b4a33993&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptical-flow-with-raft-part-1-f984b4a33993&source=-----f984b4a33993---------------------bookmark_footer-----------)![](../Images/b52b23d6214d4730b1c80371e0d0ab43.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff984b4a33993&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptical-flow-with-raft-part-1-f984b4a33993&source=-----f984b4a33993---------------------bookmark_footer-----------)![](../Images/b52b23d6214d4730b1c80371e0d0ab43.png)'
- en: Photo by [Zdeněk Macháček](https://unsplash.com/@zmachacek?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Zdeněk Macháček](https://unsplash.com/@zmachacek?utm_source=medium&utm_medium=referral)
    提供，发布于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'In this post we will learn about a flagship deep learning approach to Optical
    Flow that won the 2020 [ECCV](https://en.wikipedia.org/wiki/European_Conference_on_Computer_Vision)
    best paper award and has been cited over 1000 times. It is also the basis for
    many top performing models on the [KITTI benchmark](https://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow).
    This model is called [RAFT: Recurrent All-Pairs Field Transforms for Optical Flow](https://arxiv.org/pdf/2003.12039.pdf)
    and is readily available in [PyTorch](https://pytorch.org/vision/stable/auto_examples/plot_optical_flow.html)
    or on [GitHub](https://github.com/princeton-vl/RAFT/tree/master). The implementations
    make it highly accessible, but the model is complex and understanding it can be
    confusing. In this post we will break down RAFT into its basic components and
    learn about each of them in detail. Then we will learn how to use it in Python
    to estimate optical flow. In [part 2](https://medium.com/towards-data-science/optical-flow-with-raft-part-2-f0376a972c25)
    we will explore the obscure details and visualize the different blocks so we can
    gain deeper intuition for how they work.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '在这篇文章中，我们将学习一种旗舰级深度学习方法，这种方法在2020年获得了[ECCV](https://en.wikipedia.org/wiki/European_Conference_on_Computer_Vision)最佳论文奖，并被引用超过1000次。它也是许多顶级模型在[KITTI基准测试](https://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow)中的基础。这个模型叫做[RAFT:
    Recurrent All-Pairs Field Transforms for Optical Flow](https://arxiv.org/pdf/2003.12039.pdf)，可以在[PyTorch](https://pytorch.org/vision/stable/auto_examples/plot_optical_flow.html)或[GitHub](https://github.com/princeton-vl/RAFT/tree/master)上轻松获取。实现使其非常易于获取，但模型复杂，理解起来可能会令人困惑。在这篇文章中，我们将把RAFT分解为其基本组成部分，并详细了解它们。然后，我们将学习如何在Python中使用它来估计光流。在[第2部分](https://medium.com/towards-data-science/optical-flow-with-raft-part-2-f0376a972c25)中，我们将探索隐秘的细节并可视化不同的模块，以便深入理解它们的工作原理。'
- en: '**Introduction**'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**介绍**'
- en: '**Foundations of RAFT**'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RAFT的基础**'
- en: '**Visual Similarity**'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**视觉相似性**'
- en: '**Iterative Updates**'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迭代更新**'
- en: '**How to use RAFT**'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**如何使用RAFT**'
- en: '**Conclusion**'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结论**'
- en: Introduction
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Optical Flow
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 光流
- en: Optical flow is the apparent motion of pixels in a sequence of images. In order
    for optical flow to be estimated, the movement of an object in a scene must have
    a corresponding brightness displacement. This means that a moving red ball in
    one image should have the same brightness and color in the next image, this enables
    us to determine how much it has moved in terms of pixels. Figure 1 shows an example
    of Optical Flow where a ceiling fan rotating counter-clock wise is captured by
    a sequence of images.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 光流是图像序列中像素的表观运动。为了估计光流，场景中物体的运动必须有相应的亮度位移。这意味着图像中的一个移动的红球在下一张图像中应具有相同的亮度和颜色，这使我们能够确定它在像素上的移动量。图1展示了一个光流的例子，其中一个逆时针旋转的天花板风扇被一系列图像捕捉到。
- en: '![](../Images/40ef9d1595608dfdaad13c7ac9a44c7f.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40ef9d1595608dfdaad13c7ac9a44c7f.png)'
- en: 'Figure 1\. Optical Flow Estimation for a sequence of images. Frame 1, Frame
    2, Computed Optical Flow between frame 1 and frame 2\. Source: Author.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 图像序列的光流估计。帧1，帧2，帧1和帧2之间计算的光流。来源：作者。
- en: The color image on the far right contains the apparent motion of every pixel
    from frame 1 to frame 2, it is color coded such that different colors indicate
    different horizontal and vertical directions of pixel motion. This is an example
    of Dense Optical Flow estimation.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最右边的彩色图像包含了从帧1到帧2的每个像素的表观运动，它的颜色编码方式使得不同的颜色表示像素运动的不同水平和垂直方向。这是一个密集光流估计的例子。
- en: An estimation of ***Dense Optical Flow*** assigns each pixel a 2D flow vector
    describing it’s horizontal and vertical displacement over a time interval. In
    ***Sparse Optical Flow*** this vector is only assigned to pixels that correspond
    to strong features such as corners and edges. *In order for a flow vector to exist,
    the pixel must have the same intensity at time t as it does at time t+1, this
    is known as the* ***brightness consistency assumption****.* The image intensity
    or brightness at location *(x,y)* at time *t* is given by *I(x,y,t).* Let’s visualize
    this with an example of known pixel displacement below in figure 2, where *dx*
    and *dy* are the horizontal and vertical image displacements and *dt* is the time
    difference between frames.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对***稠密光流***的估计为每个像素分配一个二维流向量，描述其在时间间隔内的水平和垂直位移。在***稀疏光流***中，这个向量仅分配给对应于强特征（如角点和边缘）的像素。*为了使流向量存在，该像素在时间
    t 的亮度必须与时间 t+1 时相同，这被称为* ***亮度一致性假设***。*位置*(x,y)* 在时间*t* 的图像强度或亮度由 *I(x,y,t)*
    给出。下面的图2展示了已知像素位移的示例，其中 *dx* 和 *dy* 是水平和垂直图像位移，*dt* 是帧之间的时间差。
- en: '![](../Images/1b11bdebf2f3cde61abdbfb10cf8e0fc.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b11bdebf2f3cde61abdbfb10cf8e0fc.png)'
- en: 'Figure 2\. Displacement of a single pixel from time t to t+dt. The brightness
    consistency assumption implies that this pixel is the same color and intensity
    in both frames. Source: Author.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图2。像素从时间 t 到 t+dt 的位移。亮度一致性假设意味着该像素在两个帧中具有相同的颜色和强度。来源：作者。
- en: 'The brightness consistency assumption implies that a pixel at (x,y,t) will
    have the same intensity at (x+dx, y+dy, t+dy). Therefore: **I(x, y, t) = I(x+dx,
    y+dy, t+dt)**.'
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 亮度一致性假设意味着在 (x,y,t) 处的像素在 (x+dx, y+dy, t+dy) 处将具有相同的强度。因此：**I(x, y, t) = I(x+dx,
    y+dy, t+dt)**。
- en: From the brightness consistency assumption we can derive the Optical Flow equation
    by expanding the right hand side with a 1ˢᵗ Order Taylor Approximation about *(x,
    y, t)* [[1](http://image.diku.dk/imagecanon/material/HornSchunckOptical_Flow.pdf)].
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从亮度一致性假设出发，我们可以通过在*(x, y, t)* 周围展开右侧的1ˢᵗ阶泰勒近似来推导光流方程[[1](http://image.diku.dk/imagecanon/material/HornSchunckOptical_Flow.pdf)]。
- en: '![](../Images/e890a55fafb97aeca71370e6f27a3772.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e890a55fafb97aeca71370e6f27a3772.png)'
- en: 'Optical Flow equation derivation. Source: Author.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 光流方程的推导。来源：作者。
- en: The horizontal and vertical gradients, *Iₓ* and *Iᵧ*, can be approximated with
    the [Sobel Operator](https://en.wikipedia.org/wiki/Sobel_operator) and the time
    gradient *Iₜ* is known since we have images at time *t* and *t+1*. The flow equation
    has two unknowns *u* and *v* which are the horizontal and vertical displacements
    over time *dt*. The two unknowns in a single equation make this is an [underdetermined](https://en.wikipedia.org/wiki/Underdetermined_system)
    problem and many attempts have been made to solve for *u* and *v*. RAFT is a deep
    learning approach to estimating *u* and *v*, but it is actually more involved
    than just predicting flow based on two frames. It was meticulously designed to
    accurately estimate optical flow fields , in the next section we will dive into
    it’s intricate details.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 水平和垂直梯度 *Iₓ* 和 *Iᵧ* 可以通过 [Sobel算子](https://en.wikipedia.org/wiki/Sobel_operator)
    进行近似，时间梯度 *Iₜ* 是已知的，因为我们有时间 *t* 和 *t+1* 的图像。流方程有两个未知数 *u* 和 *v*，分别是时间 *dt* 内的水平和垂直位移。一个方程中的两个未知数使这个问题成为一个
    [欠定](https://en.wikipedia.org/wiki/Underdetermined_system) 问题，许多尝试都旨在解决 *u* 和
    *v*。RAFT 是一种深度学习方法，用于估计 *u* 和 *v*，但实际上，它比仅仅基于两帧预测光流要复杂得多。它经过精心设计，以准确估计光流场，下一节我们将深入探讨它的复杂细节。
- en: Foundations of RAFT
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAFT的基础
- en: RAFT is a Deep Neural Network that is able to estimate the Dense Optical Flow
    given a pair of sequential images *I₁* and *I₂*. It estimates a flow displacement
    field *(****f¹****,* ***f²****)* that maps each pixel *(u, v)* in *I₁* to its
    corresponding pixel *(u’, v’)* in *I₂*, where *(u’, v’) = (u +* ***f¹****(u),
    v +* ***f²****(v))*. It works by extracting features, finding their correlations,
    and then iteratively updates the flow in a manner that mimics an optimization
    algorithm. The initial flow is either initialized as all 0’s or it can be initialized
    with the forward projected previous flow estimate which is known as a warm start.
    The overall architecture is shown below.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: RAFT是一个深度神经网络，能够估计给定一对连续图像的密集光流*I₁*和*I₂*。它估计一个流位移场*(****f¹****,* ***f²****)*，将每个像素*(u,
    v)*在*I₁*中映射到*I₂*中对应的像素*(u’, v’)*，其中*(u’, v’) = (u +* ***f¹****(u), v +* ***f²****(v))*。它通过提取特征、寻找其相关性，然后以模拟优化算法的方式迭代更新流。初始流要么初始化为全0，要么可以使用向前投影的先前流估计来初始化，这被称为温启动。整体架构如下所示。
- en: '![](../Images/ba7083f8d850dff02dfa034d097a49cb.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba7083f8d850dff02dfa034d097a49cb.png)'
- en: Figure 3\. Architecture of RAFT. Modified from [Source](https://arxiv.org/pdf/2003.12039.pdf).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. RAFT的架构。修改自[源](https://arxiv.org/pdf/2003.12039.pdf)。
- en: 'Notice how it has three main blocks: Feature Encoder Block, Visual Similarity
    Block, and an Iterative Update Block. The RAFT architecture comes in two sizes
    a large with 4.8 million parameters and a small with 1 million parameters, in
    this post we will focus on the large architecture, but understanding the small
    architecture will be of little account once we understand the large one.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，它包含三个主要模块：特征编码器模块、视觉相似性模块和迭代更新模块。RAFT架构有两个版本，一个大版本有480万参数，一个小版本有100万参数，在这篇文章中我们将重点关注大版本，但理解小版本在理解大版本后意义不大。
- en: Feature Extraction
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征提取
- en: RAFT performs feature extraction on both input images using a Convolutional
    Neural Network (CNN) that consists of six residual blocks and downsamples each
    image to 1/8 resolution with D feature maps.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: RAFT通过一个包含六个残差块的卷积神经网络（CNN）对两个输入图像进行特征提取，并将每个图像下采样到1/8分辨率，具有D个特征图。
- en: '![](../Images/69464580dc15f8b6ed9f9bdd811bf39b.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69464580dc15f8b6ed9f9bdd811bf39b.png)'
- en: Figure 4\. Encoding Block of RAFT. Modified from [Source](https://arxiv.org/pdf/2003.12039.pdf).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. RAFT的编码块。修改自[源](https://arxiv.org/pdf/2003.12039.pdf)。
- en: The feature encoder network *g* operates on both images with shared weights,
    while the context encoder network *f* only operates on *I₁* and extracts features
    that serve as a primary reference for the flow estimation. Aside from minor differences,
    the overall architecture of both networks is nearly the same. The context network
    uses batch normalization while the feature network uses instance normalization,
    and the context network extracts *C = c + h* feature maps, where *c* is the number
    of context feature maps and *h* is the number of hidden feature maps that will
    initialize the hidden state of the Iterative Update Block.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 特征编码器网络*g*在两个图像上使用共享权重进行操作，而上下文编码器网络*f*仅在*I₁*上操作，并提取作为流估计的主要参考的特征。除了细微的差异外，两个网络的整体架构几乎相同。上下文网络使用批归一化，而特征网络使用实例归一化，上下文网络提取*C
    = c + h*特征图，其中*c*是上下文特征图的数量，*h*是将初始化迭代更新模块隐藏状态的隐藏特征图数量。
- en: '![](../Images/b271f2ab7872d61354a8e84e538bbcbe.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b271f2ab7872d61354a8e84e538bbcbe.png)'
- en: 'Function mappings for the feature network f and the context network g. Source:
    Author.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 特征网络f和上下文网络g的函数映射。来源：作者。
- en: 'NOTE: The original paper constantly refers to feature map sizes H/8xW/8 with
    the shorthand notation: HxW. This can be confusing, so we will follow the convention
    of H’ = H/8 such that a feature map size is H’xW’. We also refer to the feature
    map tensor extracted from I₁ *a*s g*¹*, likewise for I₂.'
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：原始论文中经常使用特征图大小H/8xW/8的简写符号：HxW。这可能会令人困惑，因此我们将遵循H’ = H/8的约定，使特征图大小为H’xW’。我们也将提及从I₁
    *a*s g*¹*中提取的特征图张量，I₂亦然。
- en: Visual Similarity
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视觉相似性
- en: Correlation Volume
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关体积
- en: The Visual Similarity is a 4D H’xW’xH’xW’ All-Pairs Correlation Volume **C**
    that is computed by taking the dot product of the feature maps.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉相似性是一个4D H’xW’xH’xW’的全对关联体积**C**，通过计算特征图的点积得到。
- en: '![](../Images/91d93dbdffa89e68ad293f325fc7db97.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91d93dbdffa89e68ad293f325fc7db97.png)'
- en: Computation of the 4D correlation Volume. Modified from [Source](https://arxiv.org/pdf/2003.12039.pdf).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 4D相关体积的计算。修改自[源](https://arxiv.org/pdf/2003.12039.pdf)。
- en: 'In the correlation volume, each pixel from feature map *g¹* has a computed
    correlation with every pixel from feature map *g²*, we call each of these correlations
    a ***2D response map*** *(see figure 5)*. It can be challenging to think in 4D,
    so imagine flattening the first two dimensions of the volume: *(H’xW’)xH’xW’*,
    we now have a 3D volume where each pixel from *g¹* has it’s own 2D response map
    that shows it’s correlation to each pixel location of *g²*.Since features are
    derived from images, the response maps actually indicate how much a given pixel
    of *I₁* is correlated to each pixel of *I₂*.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在相关体积中，来自特征图*g¹*的每个像素与特征图*g²*中的每个像素都有一个计算得到的相关性，我们称这些相关性中的每一个为***2D响应映射*** *(见图5)*。想象在4D空间中可能有些挑战，所以可以将体积的前两个维度展平：*(H’xW’)xH’xW’*，现在我们得到一个3D体积，其中*g¹*的每个像素都有自己的2D响应映射，显示其与*g²*的每个像素位置的相关性。由于特征是从图像中提取的，响应映射实际上指示了*I₁*的给定像素与*I₂*的每个像素的相关程度。
- en: The **Visual Similarity** is an all-pairs Correlation Volume that relates the
    pixels of I₁ to every single pixel of I₂ by computing the correlation of every
    feature map at each pixel location
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**视觉相似性**是一种全对Correlation Volume，通过计算每个像素位置处的每个特征图的相关性，将*I₁*的像素与*I₂*的每个单一像素联系起来'
- en: Correlation Pyramid
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关金字塔
- en: The correlation volume effectively provides information for small pixel displacements,
    but will likely struggle to capture larger displacements. In order to capture
    both large and small pixel displacements, multiple levels of correlation are needed.
    To solve this, we construct a **Correlation Pyramid** which contains multiple
    levels of correlation volumes where different levels of correlation volumes are
    produced by average pooling the last two dimensions of the correlation volume.
    The average pooling operation produces coarse *I₂* correlation features in the
    last two dimensions of the volume, this enables the fine features of *I₁* to be
    correlated with the progressively coarse features of *I₂.* Each pyramid level
    contains smaller and smaller 2D response maps.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 相关体积有效地提供了关于小像素位移的信息，但可能难以捕捉较大的位移。为了捕捉大和小的像素位移，需要多个级别的相关性。为此，我们构建了一个包含多个相关体积级别的**相关金字塔**，其中不同级别的相关体积通过对相关体积的最后两个维度进行平均池化来生成。平均池化操作在体积的最后两个维度产生了*I₂*的粗略相关特征，这使得*I₁*的精细特征能够与*I₂*的逐渐粗略的特征相关联。每个金字塔级别包含越来越小的2D响应映射。
- en: '![](../Images/4a6d33f0bc89d459238bf26e1345bbe9.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4a6d33f0bc89d459238bf26e1345bbe9.png)'
- en: 'Figure 5\. Left: Relationship of a single pixel in *I₁* to all pixels of *I₂.*
    Right: 2D response maps of various correlation volumes in the correlation pyramid.
    [Source](https://arxiv.org/pdf/2003.12039.pdf).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图5。左：*I₁*中单个像素与*I₂*所有像素的关系。右：相关金字塔中各种相关体积的2D响应映射。[来源](https://arxiv.org/pdf/2003.12039.pdf)。
- en: 'Figure 5 shows the different 2D response maps for different levels of average
    pooling. The dimensions of the corresponding correlation volumes are stacked together
    into a 5D Correlation Pyramid which contains four levels with kernel sizes: 1,
    2, 4, and 8\. The pyramid provides robust information about both large and small
    displacements while maintaining a high resolution with respect to *I₁*.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图5显示了不同平均池化级别的不同2D响应映射。相应的相关体积的尺寸被堆叠到一个5D相关金字塔中，其中包含四个级别的核大小：1、2、4和8。该金字塔提供了关于大和小位移的强大信息，同时保持对*I₁*的高分辨率。
- en: Correlation Lookup
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关查找
- en: 'The **Correlation Lookup Operator** *L꜀* generates new feature maps by indexing
    features from the correlation pyramid at each level. Given the current Optical
    Flow estimate *(****f¹****,* ***f²****)*, each pixel of *I₁*: *x = (u, v)* is
    mapped to it’s estimated **correspondence** in *I₂*: *x’ = (u + f¹(u) + v + f²(v))*.
    We define a local neighborhood around *x’*:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关查找运算符** *L꜀* 通过在每个级别的相关金字塔中索引特征来生成新的特征图。给定当前的光流估计*(****f¹****,* ***f²****)*，*I₁*的每个像素：*x
    = (u, v)*映射到其在*I₂*中估计的**对应关系**：*x’ = (u + f¹(u) + v + f²(v))*。我们定义了*x’*周围的局部邻域：'
- en: '![](../Images/176fed846a5ff12125e441040bbbb905.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/176fed846a5ff12125e441040bbbb905.png)'
- en: 'Neighborhood of radius r around pixel x’ = (u’, v’). Source: Author.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 以像素*x’ = (u’, v’)*为中心的半径*r*的邻域。来源：作者。
- en: '**Correspondence** is the new location of a pixel in I₂ based on it’s flow
    estimate'
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**对应关系**是基于其流估计的*I₂*像素的新位置'
- en: A constant radius across all pyramid levels means that a larger context will
    be incorporated across the lower levels. *i.e. a radius of 4 corresponds to 256
    pixels at the original resolution.*
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 所有金字塔层级上的常量半径意味着更大的上下文将被纳入到较低层级中。 *即，半径为4对应于原始分辨率下的256像素。*
- en: In practice this neighborhood is a square grid centered around each fine resolution
    pixel, with r = 4 we get a 9x9 grid around each pixel, where each dimension is
    of length *(2r + 1)*. We obtain new feature maps by [bilinearly resampling](https://en.wikipedia.org/wiki/Bilinear_interpolation#Application_in_image_processing)
    the correlation features around each pixel at locations defined by the grid (edge
    locations are zero padded). Due to the flow offsets and average pooling, the neighborhood
    grid values will likely be floating points, the bilinear resampling readily handles
    this by taking weighted average of the 2x2 sub-neighborhood of nearby pixels.
    In other words, the resampling will give us [subpixel](https://dsp.stackexchange.com/questions/34103/subpixel-what-is-it)
    accuracy. We resample at all pixel locations in each layer of the pyramid, this
    can be efficiently done with [F.grid_sample()](https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html)
    from PyTorch. These resampled features are known as the **Correlation Features**
    and they are input into the Update Block.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这个邻域是围绕每个细分分辨率像素中心的正方形网格，r = 4时，我们在每个像素周围得到一个9x9的网格，其中每个维度的长度为 *(2r + 1)*。我们通过
    [双线性重采样](https://en.wikipedia.org/wiki/Bilinear_interpolation#Application_in_image_processing)
    在网格定义的位置周围对每个像素的相关性特征进行重采样（边缘位置使用零填充）。由于流偏移和平均池化，邻域网格值可能是浮点数，双线性重采样通过对附近像素的2x2子邻域进行加权平均来处理这一点。换句话说，重采样将提供
    [亚像素](https://dsp.stackexchange.com/questions/34103/subpixel-what-is-it) 精度。我们在金字塔的每一层的所有像素位置进行重采样，这可以通过
    PyTorch 的 [F.grid_sample()](https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html)
    高效完成。这些重采样后的特征被称为 **相关性特征**，并输入到更新块中。
- en: Efficient Correlation Lookup (Optional)
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高效的相关性查找（可选）
- en: The correlation lookup scales with *O(N²)* where *N* is the number of pixels,
    this could be a bottleneck for large images but there is an equivalent operation
    that scales with *O(NM)* where *M* is the number of pyramid levels. This operations
    combines the correlation pyramid with the lookup and operation exploits the linearity
    of the inner product and average pooling. The average correlation response Cᵐ
    (pyramid level m) over a *2ᵐx2ᵐ* grid is shown below.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 相关性查找的复杂度为 *O(N²)*，其中 *N* 是像素数量，这可能会成为大图像的瓶颈，但存在一种等效操作，其复杂度为 *O(NM)*，其中 *M*
    是金字塔层数。该操作将相关性金字塔与查找相结合，利用了内积和平均池化的线性特性。下图显示了在 *2ᵐx2ᵐ* 网格上的平均相关性响应 Cᵐ（金字塔层级 m）。
- en: '![](../Images/f1ebe42dd4af2d672fd335b0c78c3d24.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1ebe42dd4af2d672fd335b0c78c3d24.png)'
- en: Equivalent Correlation implementation. [Source](https://arxiv.org/pdf/2003.12039.pdf).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 等效的相关性实现。[来源](https://arxiv.org/pdf/2003.12039.pdf)。
- en: For a given pyramid level *m* we don’t need to sum over the feature map *g¹*,
    this means that the correlation can be computed by taking the inner product of
    feature map *g¹* with the average pooled feature map *g²*, this has a complexity
    of *O(N)*. Since this is only valid for a single Pyramid level *m*, we must compute
    this inner product for each level, making it scale by *O(M)* for a total complexity
    of *O(NM)*. Instead of precomputing the correlations for the pyramid, we only
    precompute the pooled feature maps and compute the correlation values on demand
    when the lookup occurs.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的金字塔层级 *m*，我们不需要对特征图 *g¹* 进行求和，这意味着可以通过将特征图 *g¹* 与平均池化后的特征图 *g²* 进行内积来计算相关性，这具有
    *O(N)* 的复杂度。由于这仅适用于单个金字塔层级 *m*，我们必须为每一层计算这个内积，使其复杂度为 *O(M)*，总复杂度为 *O(NM)*。我们不是预计算金字塔的相关性，而是只预计算池化特征图，并在查找发生时按需计算相关性值。
- en: Iterative Updates
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迭代更新
- en: 'The update operator estimates a series of flows: ***{f₀***, ***f*₁ *,…, f*ₙ*}***
    from an initial starting point ***f₀***, which can either be all 0’s or the forward
    projected previous flow estimation (warm start). In each iteration *k* it produces
    a flow update direction *Δf* which is added to the current estimate: *fₖ₊₁ = fₖ
    + Δfₖ*. The update operator mimics an optimization algorithm and is trained to
    provide updates such that the estimated flow sequence converges to a fixed point:
    ***fₖ → f****.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 更新操作符估计一系列光流：***{f₀***, ***f*₁ *,…, f*ₙ*}*** 从初始起点 ***f₀*** 开始，该起点可以是全 0 或向前投影的先前光流估计（热启动）。在每次迭代
    *k* 中，它产生一个光流更新方向 *Δf*，该方向被加到当前估计中：*fₖ₊₁ = fₖ + Δfₖ*。更新操作符模仿优化算法，并经过训练以提供更新，使得估计的光流序列收敛到一个固定点：***fₖ
    → f****。
- en: Update Block
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更新块
- en: 'The update block takes as inputs: the correlation features, current flow estimate,
    context features, and the hidden features. Its architecture with highlighted sub-blocks
    is displayed below.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 更新块的输入包括：相关特征、当前光流估计、上下文特征和隐藏特征。其结构及突出显示的子块如下所示。
- en: '![](../Images/f89fbbfb27859689f7fbeed8893d40fa.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f89fbbfb27859689f7fbeed8893d40fa.png)'
- en: Figure 6\. RAFT Update Block for large architecture with different sub-blocks
    highlighted. Blue-Feature extraction block, Red — Recurrent Update Block, Green
    — Flow Head. Modified from [Source](https://arxiv.org/pdf/2003.12039.pdf).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6\. 大型架构的 RAFT 更新块，不同子块突出显示。蓝色-特征提取块，红色 — 递归更新块，绿色 — 光流头。改编自 [来源](https://arxiv.org/pdf/2003.12039.pdf)。
- en: 'The sub-blocks inside the Update Block are:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 更新块中的子块包括：
- en: '***Feature Extraction Block*** — extracts motion features from the correlation,
    flow, and *I₁* (context network).'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***特征提取块*** — 从相关性、光流和 *I₁*（上下文网络）中提取运动特征。'
- en: '***Recurrent Update Block*** — Recurrently computes flow updates'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***递归更新块*** — 递归计算光流更新'
- en: '***Flow Head*** — Final convolutional layers that re-size flow estimate to
    H/8 x W/8 x 2'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***光流头*** — 最终卷积层，将光流估计重新调整为 H/8 x W/8 x 2'
- en: As seen in figure 6, the input to the Recurrent Update Block is the concatenation
    of the flow, correlation, and context features. The latent hidden state is initialized
    with the hidden features from the context network. (The context network extracts
    a stack of 2D feature maps that is then separated into the context and the hidden
    feature maps). The Recurrent Update Block consists of 2 separable ConvGRUs that
    enable an increased receptive field without significantly increasing the network
    size. On each update, the hidden state from the Recurrent Update Block is passed
    to the Flow Head to obtain a flow estimate of size H/8 x W/8 x 2\. This estimate
    is then upsampled using Convex Upsampling.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 6 所示，递归更新块的输入是光流、相关性和上下文特征的连接。潜在的隐藏状态由上下文网络中的隐藏特征初始化。（上下文网络提取了一堆 2D 特征图，然后将其分离为上下文特征图和隐藏特征图）。递归更新块由
    2 个可分离的 ConvGRU 组成，这些 ConvGRU 可以在不显著增加网络规模的情况下增加感受野。在每次更新时，递归更新块中的隐藏状态被传递到光流头，以获得尺寸为
    H/8 x W/8 x 2 的光流估计。该估计随后使用凸上采样进行上采样。
- en: Convex Upsampling
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 凸上采样
- en: The authors of RAFT experimented with both [bilinear](https://en.wikipedia.org/wiki/Bilinear_interpolation)
    and convex upsampling and found that convex upsampling provides a significant
    performance boost.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: RAFT 的作者实验了 [双线性](https://en.wikipedia.org/wiki/Bilinear_interpolation) 和凸上采样，并发现凸上采样提供了显著的性能提升。
- en: '![](../Images/57c264bf7c8dcc7194f2022d101fcecb.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57c264bf7c8dcc7194f2022d101fcecb.png)'
- en: Figure 7\. Comparison of Bilinear VS Convex Upsampling. [Source](https://arxiv.org/pdf/2003.12039.pdf).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. 双线性 VS 凸上采样的比较。 [来源](https://arxiv.org/pdf/2003.12039.pdf)。
- en: '**Convex Upsampling** estimates each fine pixel as the [convex combination](https://www.math.ucla.edu/~baker/149/handouts/cc_convex/node4.html)
    of it’s neighboring 3x3 grid of coarse pixels'
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**凸上采样** 估计每个细像素为其相邻 3x3 粗像素的 [凸组合](https://www.math.ucla.edu/~baker/149/handouts/cc_convex/node4.html)'
- en: Let’s break down how Convex Upsampling works, figure 8 below provides a nice
    visual.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解一下凸上采样的工作原理，下面的图 8 提供了一个很好的视觉示例。
- en: '![](../Images/46913b3e7e604edf7cdffd5628d2c20c.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/46913b3e7e604edf7cdffd5628d2c20c.png)'
- en: Figure 8\. Convex Upsampling example at a single full res pixel (purple). [Source](https://arxiv.org/pdf/2003.12039.pdf).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 单个全分辨率像素（紫色）的凸上采样示例。 [来源](https://arxiv.org/pdf/2003.12039.pdf)。
- en: 'First, we assume that a fine resolution pixel is the [convex combination](https://www.math.ucla.edu/~baker/149/handouts/cc_convex/node4.html)
    of a 3x3 grid of it’s nearest coarse neighbors. This assumption implies that the
    weighed sum of the coarse pixels must equal the true fine resolution pixel, with
    the constraint that the weights sum to one and are non-negative. Since we are
    upsampling by a factor of eight, each coarse pixel must be broken down into 64
    (8x8) fine pixels (the visual in figure 8 is not to scale). We also notice that
    each of the 64 pixels in the center of the 3x3 grid will need it’s own set of
    weights, making the total number of weights required: (H/8 x W/8 x (8x8x9)).'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们假设一个细分分辨率的像素是其最近的粗分辨率邻居的[凸组合](https://www.math.ucla.edu/~baker/149/handouts/cc_convex/node4.html)。这一假设意味着粗分辨率像素的加权和必须等于真实的细分分辨率像素，且权重之和为一且非负。由于我们是按八倍因子上采样的，每个粗分辨率像素必须分解成64个(8x8)的细分像素（图8中的视觉效果不按比例）。我们还注意到3x3网格中心的每个64个像素都需要自己的权重集，总共需要的权重数为：(H/8
    x W/8 x (8x8x9))。
- en: In practice, the weights are parameterized with a neural network, the convex
    upsampling block uses two convolutional layers to predict a (H/8 x W/8 x (8x8x9))
    mask and then takes a softmax over the weights of the nine neighbors leaving a
    mask of shape (H/8 x W/8 x (8x8)). We then use this mask to obtain a weighted
    combination over the neighborhood and reshape to get a HxWx2 flow field.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，权重由神经网络参数化，凸上采样块使用两个卷积层来预测一个(H/8 x W/8 x (8x8x9))的掩码，然后对九个邻居的权重进行softmax，得到形状为(H/8
    x W/8 x (8x8))的掩码。然后我们使用这个掩码来获得邻域的加权组合，并重新调整以得到HxWx2的流场。
- en: Training
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: The objective function for RAFT is able to capture all iterative flow predictions.
    Formally, it is the sum of weighted *l1* distances between the flow predictions
    and ground truth, with exponentially increasing weights.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: RAFT的目标函数能够捕捉所有迭代的流预测。形式上，它是流预测和真实值之间加权的*l1*距离的总和，权重以指数形式增长。
- en: '![](../Images/9e5ca1819b206ff5e6e507a204495f60.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e5ca1819b206ff5e6e507a204495f60.png)'
- en: Loss for RAFT, γ = 0.8\. [Source](https://arxiv.org/pdf/2003.12039.pdf).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: RAFT的损失，γ = 0.8。 [来源](https://arxiv.org/pdf/2003.12039.pdf)。
- en: How to use RAFT
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用RAFT
- en: We can use RAFT to estimate Dense Optical Flow on our own images. First we will
    need to clone the [GitHub repository](https://github.com/princeton-vl/RAFT/tree/master)
    and download the models. Code for this tutorial is on [GitHub](https://github.com/itberrios/CV_projects/blob/main/RAFT/RAFT_exploration.ipynb).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用RAFT来估计我们自己图像上的密集光流。首先，我们需要克隆[GitHub仓库](https://github.com/princeton-vl/RAFT/tree/master)并下载模型。此教程的代码在[GitHub](https://github.com/itberrios/CV_projects/blob/main/RAFT/RAFT_exploration.ipynb)上。
- en: '[PRE0]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The pre-trained RAFT models come in a few different flavors, according to the
    [authors](https://github.com/princeton-vl/RAFT/issues/67) they are:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的RAFT模型有几种不同的版本，根据[作者](https://github.com/princeton-vl/RAFT/issues/67)，它们是：
- en: '**raft-chairs** — trained on FlyingChairs'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**raft-chairs** — 在FlyingChairs上训练'
- en: '**raft-things** — trained on FlyingChairs + FlyingThings'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**raft-things** — 在FlyingChairs + FlyingThings上训练'
- en: '**raft-sintel** — trained on FlyingChairs + FlyingThings + Sintel + KITTI (model
    used for submission)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**raft-sintel** — 在FlyingChairs + FlyingThings + Sintel + KITTI上训练（用于提交的模型）'
- en: '**raft-kitti** — raft-sintel finetuned on only KITTI'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**raft-kitti** — raft-sintel在仅KITTI上微调'
- en: '**raft-small** — trained on FlyingChairs + FlyingThings'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**raft-small** — 在FlyingChairs + FlyingThings上训练'
- en: Next we add the core of RAFT to the path
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将RAFT的核心添加到路径中
- en: '[PRE1]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, we need some helper functions to interface with the RAFT class. NOTE:
    these helpers are written for CUDA only, but you can easily access a GPU with
    [Colab](https://colab.research.google.com/).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要一些辅助函数来与RAFT类接口。注意：这些辅助函数仅为CUDA编写，但你可以通过[Colab](https://colab.research.google.com/)轻松访问GPU。
- en: '[PRE2]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Notice the input padding in *inference()*, we will need to ensure that all images
    are divisible by 8\. The raft.py code can easily be accessed from the command
    line, but if we want to interface with it, we will need to rewrite some of it,
    or we can make a special class to pass arguments to it.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到*inference()*中的输入填充，我们需要确保所有图像的尺寸都能被8整除。raft.py代码可以从命令行轻松访问，但如果我们想要与之接口，我们需要重写部分代码，或者可以创建一个特殊的类来传递参数。
- en: '[PRE3]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The default initialization of the Args class will interface directly with any
    of the large RAFT models. To demonstrate RAFT, we will use frames from a video
    of a slowly rotating ceiling fan. Now we can load a model and estimate the optical
    flow.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Args 类的默认初始化将直接与任何大型 RAFT 模型进行接口。为了演示 RAFT，我们将使用一个慢速旋转的天花板风扇视频的帧。现在我们可以加载一个模型并估算光流。
- en: '[PRE4]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Test mode will return both the 1/8 res flow along with the Convex Upsampled
    flow.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 测试模式将返回 1/8 分辨率光流以及凸性上采样光流。
- en: '![](../Images/b362e8cf8ba922ffb202806e319f0865.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b362e8cf8ba922ffb202806e319f0865.png)'
- en: 'Figure 9\. Top: Input image sequence for RAFT. Bottom: 1/8 res and upsampled
    Optical Flow estimates. Images are from the Author. Source: Author.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9\. 上：RAFT 的输入图像序列。下：1/8 分辨率和上采样的光流估计。图片来源于作者。来源：作者。
- en: Once again, we can see the significant benefit of convex upsampling, now let’s
    view the advantages of extra iterations. Figure 10 shows a gif of 20 iterations
    on the ceiling fan images.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们可以看到凸性上采样的显著优势，现在让我们来看看额外迭代的优势。图 10 展示了一个风扇图像上的 20 次迭代的 GIF 动画。
- en: '[PRE5]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/690479d7bd0f0b87a20143ca96fdb25c.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/690479d7bd0f0b87a20143ca96fdb25c.png)'
- en: 'Figure 10\. Progressive Iterations of Optical Flow Estimation. Source: Author.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10\. 光流估计的渐进迭代。来源：作者。
- en: We can see a clear benefit from the first few iterations, in this case the model
    is able to converge in about 10 iterations. Now we will experiment with a warm
    start, to use a warm initialization, we pass the previous flow estimate at 1/8
    resolution to the inference function.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到前几次迭代的明显好处，在这种情况下，模型能够在约 10 次迭代中收敛。现在我们将尝试使用温初始化，将前一个 1/8 分辨率的流估计传递给推理函数。
- en: '[PRE6]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/4ac2489e9683d92e3e1243b55588c28a.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ac2489e9683d92e3e1243b55588c28a.png)'
- en: 'Figure 11\. Optical flow estimation with 0 VS warm initialization between frames
    2 and 3\. Source: Author.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11\. 在第 2 和第 3 帧之间进行 0 VS 温初始化的光流估计。来源：作者。
- en: In this case we don’t see any improvement, the warm start on the right actually
    looks worse than the 0 initialized flow. The benefits of warm start seem minimal
    for this video sequence, but it could be useful for different environments.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们并未看到任何改善，右侧的温初始化实际上看起来比初始化为 0 的流还要糟糕。对于这个视频序列来说，温启动的好处似乎微乎其微，但在不同的环境中可能会有用。
- en: Conclusion
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this post we learned about RAFT, an advanced model capable of estimating
    accurate flow fields. RAFT is able to capture the relationship between all pixels
    by computing the all-pairs correlation volume from extracted feature maps. The
    correlation pyramid is constructed to capture both large and small pixel displacements.
    The look up operator extracts new correlation features from the correlation pyramid
    based on the current flow estimate. The update block uses the correlation features
    and the current flow estimate to provide iterative updates that converge to a
    final flow estimate which is upsampled with convex upsampling. In [part 2](https://medium.com/towards-data-science/optical-flow-with-raft-part-2-f0376a972c25),
    we will unpack the network and learn how some of the key blocks work.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们了解了 RAFT，一个能够估算准确流场的先进模型。RAFT 能够通过计算从提取的特征图中的所有像素的全对相关体积来捕捉所有像素之间的关系。建立相关金字塔以捕捉大和小的像素位移。查找运算符基于当前流估计从相关金字塔中提取新的相关特征。更新块使用相关特征和当前流估计提供迭代更新，收敛到最终的流估计，然后使用凸性上采样进行上采样。在[第
    2 部分](https://medium.com/towards-data-science/optical-flow-with-raft-part-2-f0376a972c25)，我们将详细探讨网络并了解一些关键块的工作方式。
- en: References
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: '[1] Horn, B. K. P., & Schunck, B. G. (1981). Determining optical flow. *Artificial
    Intelligence*, *17*(1–3), 185–203\. [https://doi.org/10.1016/0004-3702(81)90024-2](https://doi.org/10.1016/0004-3702(81)90024-2)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Horn, B. K. P., & Schunck, B. G. (1981). 确定光流。*人工智能*, *17*(1–3), 185–203\.
    [https://doi.org/10.1016/0004-3702(81)90024-2](https://doi.org/10.1016/0004-3702(81)90024-2)'
- en: '[2] Teed, Z., & Deng, J. (2020). Raft: Recurrent all-pairs field transforms
    for optical flow. *Computer Vision — ECCV 2020*, 402–419\. [https://doi.org/10.1007/978-3-030-58536-5_24](https://doi.org/10.1007/978-3-030-58536-5_24)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Teed, Z., & Deng, J. (2020). Raft: 循环全对场变换用于光流。*计算机视觉 — ECCV 2020*, 402–419\.
    [https://doi.org/10.1007/978-3-030-58536-5_24](https://doi.org/10.1007/978-3-030-58536-5_24)'
