- en: 'An imPULSE to Action: A Practical Solution for Positive-Unlabeled Classification'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/an-impulse-to-action-a-practical-solution-for-positive-unlabelled-classification-cd5895128e45?source=collection_archive---------12-----------------------#2023-04-06](https://towardsdatascience.com/an-impulse-to-action-a-practical-solution-for-positive-unlabelled-classification-cd5895128e45?source=collection_archive---------12-----------------------#2023-04-06)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We introduce an approach called ImPULSE Classifier with improved performance
    on balanced and imbalanced PU data compared to other methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wldmrgml.medium.com/?source=post_page-----cd5895128e45--------------------------------)[![Volodymyr
    Holomb](../Images/ff4a34f4dc4ee397d4d30512aa8f177c.png)](https://wldmrgml.medium.com/?source=post_page-----cd5895128e45--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cd5895128e45--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cd5895128e45--------------------------------)
    [Volodymyr Holomb](https://wldmrgml.medium.com/?source=post_page-----cd5895128e45--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F95923fba037b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-impulse-to-action-a-practical-solution-for-positive-unlabelled-classification-cd5895128e45&user=Volodymyr+Holomb&userId=95923fba037b&source=post_page-95923fba037b----cd5895128e45---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cd5895128e45--------------------------------)
    ·5 min read·Apr 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcd5895128e45&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-impulse-to-action-a-practical-solution-for-positive-unlabelled-classification-cd5895128e45&user=Volodymyr+Holomb&userId=95923fba037b&source=-----cd5895128e45---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcd5895128e45&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-impulse-to-action-a-practical-solution-for-positive-unlabelled-classification-cd5895128e45&source=-----cd5895128e45---------------------bookmark_footer-----------)![](../Images/344834119473633b29719a471ec76bee.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Made by DALL-E-2 according to the author’s description
  prefs: []
  type: TYPE_NORMAL
- en: In data science, datasets with positive and unlabeled data, known as positive-unlabeled
    (PU) datasets, present a common yet challenging problem. PU datasets are characterised
    by the presence of unlabeled data, where the positive instances are not explicitly
    identified as positive. PU datasets are often encountered in business scenarios
    where only a small proportion of data is labelled. The remaining unlabelled samples
    could belong to either the positive or negative class. Such scenarios include
    fraud detection, product recommendation, cross-selling, or customer retention.
    The goal is to accurately classify the unlabelled data as positive or negative,
    leveraging the available labelled data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a5ef4cb61b34b96594b8ded20e60fdad.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Naive approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When dealing with this task, the first approach to binary classification that
    comes to mind is the naive one. It involves **treating the unlabelled data as
    samples of the negative class**. However, this approach has a limited performance
    when dealing with imbalanced datasets, i.e. datasets with a large number of unknown
    positive samples. Furthermore, it relies on the assumption that the proportion
    of positive samples in the unlabelled data is small enough to not significantly
    impact the model’s performance. Despite being simple to implement, the naive approach
    may not be suitable for all scenarios and often results in suboptimal performance
    in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Elkan and Noto’s approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A much more sophisticated method for positive-unlabelled classification is Elkan
    and Noto’s (E&N) approach. Within this method, we usually train a classifier to
    predict the probability that a sample is labelled and further use the model to
    estimate the probability that a positive sample is labelled. Then the probability
    that an unlabeled sample is labelled is divided by the probability that a positive
    sample is labelled to get the actual probability that the sample is positive.
    You can find a perfect explanation of how it works [here](/semi-supervised-classification-of-unlabeled-data-pu-learning-81f96e96f7cb).
  prefs: []
  type: TYPE_NORMAL
- en: While the E&N approach is effective in practice, **it requires a significant
    amount of labelled data** to accurately estimate the likelihood of a sample being
    positive, which may be impractical in real-world scenarios where marked data is
    scarce.
  prefs: []
  type: TYPE_NORMAL
- en: Custom self-training approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: However, our team has developed a custom ImPULSE (stands for **Imbalanced Positive-Unlabelled
    Learning with Self-Enrichment)** solution that addresses this issue and has shown
    improved performance compared to the E&N approach on both balanced and imbalanced
    datasets, even with a high proportion of unknown positive labels.
  prefs: []
  type: TYPE_NORMAL
- en: ImPULSE Classifier is a modification of any (potentially) supervised classification
    method that can work in a semi-supervised manner. We use LightGBM (as a base estimator)
    and a modified workflow that incorporates all predicted probabilities as weights
    for each sample in the training set. This allows us to not only add new pseudo-labels
    for the next iteration of the model’s training but also make it pay attention
    to the most promising (valid) negative samples, allowing the model to be retrained
    on a larger and more diverse set of data more confidently. Additionally, we provide
    adjusted class weights at each iteration to deal with imbalanced data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7d7e29f5379a9a275233aac8fe40915b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Our approach generally involves training a classifier on available data and
    using it to make predictions on unlabelled data (in a [self-training manner](https://www.altexsoft.com/blog/semi-supervised-learning/)).
    We start by splitting the data into a training set and a hold-out evaluation set.
    Then, we train the model while monitoring its performance on the evaluation set
    to prevent overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we use the trained model to predict the labels of the unlabelled data
    and select the samples with high-confidence predictions as positive examples.
    **We use these positive examples to update the labelled data and retrain the model
    iteratively**. To prevent overfitting, we have implemented custom early stopping
    based on metrics for average precision.
  prefs: []
  type: TYPE_NORMAL
- en: To speed up convergence, we increase the learning rate at each iteration, allowing
    the model to learn more quickly from the updated set of labelled data and make
    better predictions on the unlabelled data. This iterative process continues until
    a stopping criterion is met. Overall, our approach focuses on updating the labelled
    data with informative positive examples and effectively preventing overfitting
    to improve the model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: Experimental runs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Further, we would like to explain how we evaluated the performance of all three
    mentioned approaches for classifying positive-unlabeled data. To achieve this,
    we developed an experiment pipeline that utilised the LightGBM classifier, a standalone
    naive estimator, the Python implementation of the E&N Noto method, and our custom
    self-training algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: To create a controlled environment, we generated a synthetic dataset using Scikit-learn’s
    *make_classification* function, which we then split into training and testing
    sets. We defined the number of iterations we wanted to perform, corresponding
    to the proportion of positive labels to be hidden, and then iteratively modify
    the training set by removing the corresponding labels at each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: At each iteration, we fit all the models on the (modified) training data and
    get predictions on the testing set. We calculated the F1-score, a metric that
    combines precision and recall, using the predicted and test labels. By doing so,
    we were able to more rigorously evaluate the performance of each approach and
    compare them under different conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the result of all experimental runs:'
  prefs: []
  type: TYPE_NORMAL
- en: You can find the corresponding [demo notebook on Jovian](https://jovian.com/wldmrgml/impulse-demo-git)
    and the full code in the [GitHub repo](https://github.com/woldemarg/self_training_pu.git).
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thus, we have developed a custom solution for positive-unlabeled classification
    that has shown improved performance compared to the E&N approach on both balanced
    and imbalanced datasets. Our approach involves training a classifier on available
    data and using it to make predictions on unlabelled data in a self-training manner.
    This solution is utilised as a core classification algorithm in a bunch of our
    applied analytical tools for cross-selling and customer churn. Despite not being
    as robust and well-generalised (yet) as the [known Python implementation of the
    E&N approach](https://pulearn.github.io/pulearn/doc/pulearn/), our method may
    be considered a practical choice for the classification of PU data in real business
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Reference Reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Bekker Jessa, and Davis Jesse. “Learning from Positive and Unlabeled Data:
    a Survey.”, 2018](https://doi.org/10.48550/arXiv.1811.04820)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Kiyomaru Hirokazu, a collection of notebooks with algorithms introduced in
    “Learning from Positive and Unlabeled Data: a Survey.”, 2020](https://github.com/hkiyomaru/pu-learning)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Dobilas Saul. “Self-Training Classifier: How to Make Any Algorithm Behave
    Like a Semi-Supervised One.”, 2021](/self-training-classifier-how-to-make-any-algorithm-behave-like-a-semi-supervised-one-2958e7b54ab7)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Dorigatti, Emilio, et al. “Robust and Efficient Imbalanced Positive-Unlabeled
    Learning with Self-Supervision.”, 2022](https://doi.org/10.48550/arXiv.2209.02459)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[JointEntropy. “Awesome ML Positive Unlabeled Learning.”, 2022](https://github.com/JointEntropy/awesome-ml-pu-learning)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Agmon Alon. “Semi-Supervised Classification of Unlabeled Data (PU Learning).”,
    2022](/semi-supervised-classification-of-unlabeled-data-pu-learning-81f96e96f7cb)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Holomb, Volodymyr. “A Practical Approach to Evaluating Positive-Unlabeled
    (PU) Classifiers in Business Analytics.”, 2023](/a-practical-approach-to-evaluating-positive-unlabeled-pu-classifiers-in-real-world-business-66e074bb192f)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
