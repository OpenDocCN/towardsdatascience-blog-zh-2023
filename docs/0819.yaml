- en: Combating Overfitting with Dropout Regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/combating-overfitting-with-dropout-regularization-f721e8712fbe?source=collection_archive---------2-----------------------#2023-03-03](https://towardsdatascience.com/combating-overfitting-with-dropout-regularization-f721e8712fbe?source=collection_archive---------2-----------------------#2023-03-03)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Discover the Process of Implementing Dropout in Your Own Machine Learning Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@rohankvij?source=post_page-----f721e8712fbe--------------------------------)[![Rohan
    Vij](../Images/6ef53fffb4749e1665360555bf18275f.png)](https://medium.com/@rohankvij?source=post_page-----f721e8712fbe--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f721e8712fbe--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f721e8712fbe--------------------------------)
    [Rohan Vij](https://medium.com/@rohankvij?source=post_page-----f721e8712fbe--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe44b36765084&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombating-overfitting-with-dropout-regularization-f721e8712fbe&user=Rohan+Vij&userId=e44b36765084&source=post_page-e44b36765084----f721e8712fbe---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f721e8712fbe--------------------------------)
    ·7 min read·Mar 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff721e8712fbe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombating-overfitting-with-dropout-regularization-f721e8712fbe&user=Rohan+Vij&userId=e44b36765084&source=-----f721e8712fbe---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff721e8712fbe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombating-overfitting-with-dropout-regularization-f721e8712fbe&source=-----f721e8712fbe---------------------bookmark_footer-----------)![](../Images/2cb3dea34c130db7298a48db8ba2f7d2.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Pierre Bamin](https://unsplash.com/@bamin?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting is a common challenge that most of us have incurred or will eventually
    incur when training and utilizing a machine learning model. Ever since the dawn
    of machine learning, researchers have been trying to combat overfitting. One such
    technique they came up with was dropout regularization, in which neurons in the
    model are removed at random. In this article, we will explore how dropout regularization
    works, how you can implement it in your own model, as well as its benefits and
    disadvantages when compared to other methods.
  prefs: []
  type: TYPE_NORMAL
- en: I. Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is Overfitting?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Overfitting is when a model is overtrained on its training data, leading it
    to perform poorly on new data. Essentially, in the model’s strive to be as accurate
    as possible, it focuses too much on fine details and noise within its training
    dataset. These attributes are often not present in real-world data, so the model
    tends to not perform well. Overfitting can occur when a model has too many parameters
    relative to the amount of data. This can lead the model to hyper-focus on smaller
    details that are not relevant to the general patterns the model must develop.
    For example, suppose a complex model (many parameters) is trained to identify
    whether a horse is present in a picture or not. In that case, it might start focusing
    on details about the sky or environment rather than the horse itself. This can
    happen when:'
  prefs: []
  type: TYPE_NORMAL
- en: The model is too complex (has too many parameters) for its own good.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model is trained for too long.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The dataset the model was trained on is too small.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model is trained and tested on the same data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The dataset the model is trained on has repetitive features that make it prone
    to overfitting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is Overfitting Important?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Overfitting is more than a simple annoyance — it can destroy entire models.
    It gives the illusion that a model is performing well, even though it will have
    failed to make proper generalizations about the data provided.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting can have extremely serious consequences, especially in fields such
    as healthcare, where AI is becoming more and more proliferated. An AI that was
    not properly trained nor tested due to overfitting can lead to incorrect diagnoses.
  prefs: []
  type: TYPE_NORMAL
- en: II. What is Dropout?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dropout as a Regularization Technique
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ideally, the best way to combat overfitting would be to train a plethora of
    models of different architecture all on the same dataset and then average their
    outputs. The problem with this approach is that it is incredibly resource and
    time intensive. While it might be affordable with relatively small models, large
    models that might take large amounts of time to train could easily overwhelm anyone’s
    resources.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout works by essentially “dropping” a neuron from the input or hidden layers.
    Multiple neurons are removed from the network, meaning they practically do not
    exist — their incoming and outcoming connections are also destroyed. This artificially
    creates a multitude of smaller, less complex networks. This forces the model to
    not become solely dependent on one neuron, meaning it has to diversify its approach
    and develop a multitude of methods to achieve the same result. For instance, going
    back to the horse example, if one neuron is primarily responsible for the tree
    part of the horse, its being dropped will force the model to focus more on other
    features of the image. Dropout can also be applied directly to the input neurons,
    meaning that entire features go missing from the model.
  prefs: []
  type: TYPE_NORMAL
- en: Applying Dropout to a Neural Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dropout is applied to a neural network by randomly dropping neurons in every
    layer (including the input layer). A pre-defined dropout rate determines the chance
    of each neuron being dropped. For example, a dropout rate of 0.25 means that there
    is a 25% chance of a neuron being dropped. Dropout is applied during every epoch
    during the training of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that there is no ideal dropout value — it heavily depends on the
    hyperparameters and end goal of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout and Sexual Reproduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Think back to your freshman biology class — you probably covered meiosis, or
    sexual reproduction. During the process of meiosis, random genes mutation occur.
    This means that the resulting offspring might have traits that both parents do
    not have present in their genes. This randomness, over time, allows populations
    of organisms to become more suited to their environment. This process is called
    evolution, and without it, we would not exist today.
  prefs: []
  type: TYPE_NORMAL
- en: Both dropout and sexual reproduction seek to increase diversity and stop a system
    from becoming reliant on one set of parameters, with no room for improvement.
  prefs: []
  type: TYPE_NORMAL
- en: III. Apply Dropout to Your Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start with a dataset that might be prone to overfitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This data ties back to our example of the horse and its environment. We have
    abstracted the qualities of the image into a simple format it is easy to interpret.
    As can be clearly seen, the data is not ideal as images with horses in them also
    happen to contain trees, green grass, or a blue sky — they might be in the same
    picture, but one does not influence the other.
  prefs: []
  type: TYPE_NORMAL
- en: The MLP Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s quickly create a simple MLP using Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: I highly recommend using Python notebooks such as Jupyter Notebook to organize
    your code so you can quickly rerun cells without having to retrain the model.
    Split the code along each comment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s further analyze the data we are testing the model with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Essentially, we have an image with all the attributes of a horse, but without
    any of the environmental factors we included in the data (green grass, blue sky,
    tree, etc). The model outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Ouch! Even though the model has a face and a tail — what we are using to identify
    the horse — it is only 2.7% sure that the image is a horse image.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Dropout in an MLP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Keras makes implementing dropout, among other methods to prevent overfitting,
    shockingly simple. We just have to go back to the list containing the layers of
    the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: And add some dropout layers!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the model outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: It is 99% sure that the horse image, even though it does not contain the environmental
    variables, is a horse!
  prefs: []
  type: TYPE_NORMAL
- en: The `Dropout(0.5)` line indicates that any of the neurons in the layer above
    have a 50% chance of being “dropped,” or removed from existence, in reference
    tod the following layers. By implementing dropout, we have essentially trained
    the MLP on hundreds of models in a resource-efficient manner.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a Dropout Rate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The best way to find the ideal dropout rate for your model is through trial
    and error — there is no one-size-fits-all. Start with a low dropout rate, around
    0.1 or 0.2, and slowly increase it until you reach your desired accuracy. Using
    our horse MLP, a dropout of 0.05 results in the model being 16.5% confident the
    image is that of a horse. On the other hand, a dropout of 0.95 simply drops out
    too many neurons for the model to function — but still, a confidence of 54.1%
    is achieved. These values are not appropriate for this model, but that does mean
    they might be the right fit for others.
  prefs: []
  type: TYPE_NORMAL
- en: IV. Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s recap — dropout is a powerful technique used in machine learning to prevent
    overfitting and overall improve model performance. It does this by randomly “dropping”
    neurons from the model in the input and hidden layers. This allows the classifier
    to train on hundreds to thousands of unique models in one training session, preventing
    it from hyper-focusing on certain features.
  prefs: []
  type: TYPE_NORMAL
- en: In the coming articles, we will discover new techniques used in the field of
    machine learning as an alternative or addition to dropout. Stay tuned for more!
  prefs: []
  type: TYPE_NORMAL
