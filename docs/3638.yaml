- en: 'Courage to Learn ML: Demystifying L1 & L2 Regularization (part 4)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/courage-to-learn-ml-demystifying-l1-l2-regularization-part-4-27c13dc250f9?source=collection_archive---------6-----------------------#2023-12-11](https://towardsdatascience.com/courage-to-learn-ml-demystifying-l1-l2-regularization-part-4-27c13dc250f9?source=collection_archive---------6-----------------------#2023-12-11)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Explore L1 & L2 Regularization as Bayesian Priors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://amyma101.medium.com/?source=post_page-----27c13dc250f9--------------------------------)[![Amy
    Ma](../Images/2edf55456a1f92724535a1441fa2bef5.png)](https://amyma101.medium.com/?source=post_page-----27c13dc250f9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----27c13dc250f9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----27c13dc250f9--------------------------------)
    [Amy Ma](https://amyma101.medium.com/?source=post_page-----27c13dc250f9--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd6d8df787b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcourage-to-learn-ml-demystifying-l1-l2-regularization-part-4-27c13dc250f9&user=Amy+Ma&userId=d6d8df787b&source=post_page-d6d8df787b----27c13dc250f9---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----27c13dc250f9--------------------------------)
    ·8 min read·Dec 11, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F27c13dc250f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcourage-to-learn-ml-demystifying-l1-l2-regularization-part-4-27c13dc250f9&user=Amy+Ma&userId=d6d8df787b&source=-----27c13dc250f9---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F27c13dc250f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcourage-to-learn-ml-demystifying-l1-l2-regularization-part-4-27c13dc250f9&source=-----27c13dc250f9---------------------bookmark_footer-----------)![](../Images/22bdc3089e02a827c9315e4de1e8cd0f.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Dominik Jirovský](https://unsplash.com/@dominik_jirovsky?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'Welcome back to ‘[Courage to Learn ML](/towardsdatascience.com/tagged/courage-to-learn-ml):
    Unraveling L1 & L2 Regularization,’ in its fourth post. Last time, our mentor-learner
    pair explored [the properties of L1 and L2 regularization through the lens of
    Lagrange Multipliers](https://medium.com/p/ee27cd4b557a).'
  prefs: []
  type: TYPE_NORMAL
- en: In this concluding segment on L1 and L2 regularization, the duo will delve into
    these topics from a fresh angle — [Bayesian priors](https://medium.com/p/65218b2c2b99).
    We’ll also summarize how L1 and L2 regularizations are applied across different
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we’ll address several intriguing questions. If any of these
    topics spark your curiosity, you’ve come to the right place!
  prefs: []
  type: TYPE_NORMAL
- en: How MAP priors relate to L1 and L2 regularizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An intuitive breakdown of using Laplace and normal distributions as priors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the sparsity induced by L1 regularization with a Laplace prior
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithms that are compatible with L1 and L2 regularization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why L2 regularization is often referred to as ‘weight decay’ in neural network
    training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reasons behind the less frequent use of L1 norm in neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**So, we’ve talked about how**…'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
