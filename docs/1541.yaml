- en: Unboxing DINOv2, Meta’s new all-purpose computer vision backbone
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拆解 DINOv2，Meta 的新型全能计算机视觉骨干网
- en: 原文：[https://towardsdatascience.com/unboxing-dinov2-metas-new-all-purpose-computer-vision-backbone-d8e22c059040?source=collection_archive---------2-----------------------#2023-05-07](https://towardsdatascience.com/unboxing-dinov2-metas-new-all-purpose-computer-vision-backbone-d8e22c059040?source=collection_archive---------2-----------------------#2023-05-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/unboxing-dinov2-metas-new-all-purpose-computer-vision-backbone-d8e22c059040?source=collection_archive---------2-----------------------#2023-05-07](https://towardsdatascience.com/unboxing-dinov2-metas-new-all-purpose-computer-vision-backbone-d8e22c059040?source=collection_archive---------2-----------------------#2023-05-07)
- en: Artificial Intelligence
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人工智能
- en: Are vision foundational models catching up with LLMs?
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视觉基础模型是否正在追赶大型语言模型？
- en: '[](https://michaloleszak.medium.com/?source=post_page-----d8e22c059040--------------------------------)[![Michał
    Oleszak](../Images/61b32e70cec4ba54612a8ca22e977176.png)](https://michaloleszak.medium.com/?source=post_page-----d8e22c059040--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d8e22c059040--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d8e22c059040--------------------------------)
    [Michał Oleszak](https://michaloleszak.medium.com/?source=post_page-----d8e22c059040--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://michaloleszak.medium.com/?source=post_page-----d8e22c059040--------------------------------)[![Michał
    Oleszak](../Images/61b32e70cec4ba54612a8ca22e977176.png)](https://michaloleszak.medium.com/?source=post_page-----d8e22c059040--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d8e22c059040--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d8e22c059040--------------------------------)
    [Michał Oleszak](https://michaloleszak.medium.com/?source=post_page-----d8e22c059040--------------------------------)'
- en: ·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc58320fab2a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funboxing-dinov2-metas-new-all-purpose-computer-vision-backbone-d8e22c059040&user=Micha%C5%82+Oleszak&userId=c58320fab2a8&source=post_page-c58320fab2a8----d8e22c059040---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d8e22c059040--------------------------------)
    ·8 min read·May 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd8e22c059040&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funboxing-dinov2-metas-new-all-purpose-computer-vision-backbone-d8e22c059040&user=Micha%C5%82+Oleszak&userId=c58320fab2a8&source=-----d8e22c059040---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc58320fab2a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funboxing-dinov2-metas-new-all-purpose-computer-vision-backbone-d8e22c059040&user=Micha%C5%82+Oleszak&userId=c58320fab2a8&source=post_page-c58320fab2a8----d8e22c059040---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d8e22c059040--------------------------------)
    · 8分钟阅读 · 2023年5月7日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd8e22c059040&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funboxing-dinov2-metas-new-all-purpose-computer-vision-backbone-d8e22c059040&user=Micha%C5%82+Oleszak&userId=c58320fab2a8&source=-----d8e22c059040---------------------clap_footer-----------)'
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd8e22c059040&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funboxing-dinov2-metas-new-all-purpose-computer-vision-backbone-d8e22c059040&source=-----d8e22c059040---------------------bookmark_footer-----------)![](../Images/a82bce14167223d4394f5857aa29d74f.png)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd8e22c059040&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funboxing-dinov2-metas-new-all-purpose-computer-vision-backbone-d8e22c059040&source=-----d8e22c059040---------------------bookmark_footer-----------)![](../Images/a82bce14167223d4394f5857aa29d74f.png)'
- en: Self-supervised training methods continue to deliver breakthrough after breakthrough.
    Last week, Meta AI released the second version of their self-DIstillation with
    NO labels or DINO model. The model can supposedly be used as a backbone to solve
    virtually any computer vision task without fine-tuning! Have the foundational
    models in computer vision caught up to the level of versatility that Large Language
    Models have held for some time? Let’s take DINO for a walk to see what it can
    do!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督训练方法不断取得突破。上周，Meta AI 发布了第二版的自我蒸馏模型，无需标签或 DINO 模型。该模型据说可以作为解决几乎任何计算机视觉任务的主干骨架，而无需微调！计算机视觉中的基础模型是否已经赶上了大型语言模型所持有的多功能性水平？让我们来探索一下
    DINO 的能力吧！
- en: '*If you’re mainly interested in playing with the new DINO, feel free to scroll
    down to the “Testing DINOv2” section. Before that, we look in more detail at the
    model’s architecture and training routine.*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你主要对玩新的DINO感兴趣，可以随意滚动到“测试DINOv2”部分。在此之前，我们将更详细地查看模型的架构和训练过程。*'
- en: 🦖 Self-supervised learning in computer vision
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🦖 计算机视觉中的自监督学习
- en: 'Self-supervision has been gaining popularity in computer vision applications
    for a couple of years now. And to no surprise: the possibility to train models
    without labeled examples allows for using a much larger pool of training data,
    and in some [applications where labels are hard or expensive to get](/self-supervised-learning-in-computer-vision-fd43719b1625),
    it may even enable training where it was previously…'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习在计算机视觉应用中已经越来越受欢迎，这并不令人惊讶：没有标注示例的训练模型的可能性允许使用更大范围的训练数据，在一些[标签难以获取或成本高昂的应用中](/self-supervised-learning-in-computer-vision-fd43719b1625)，它甚至可能使得之前无法进行的训练成为可能……
