- en: Fine-Tune Your Own Open-Source LLM Using the Latest Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-efficiently-fine-tune-your-own-open-source-llm-using-novel-techniques-code-provided-03a4e67d1b48?source=collection_archive---------4-----------------------#2023-12-15](https://towardsdatascience.com/how-to-efficiently-fine-tune-your-own-open-source-llm-using-novel-techniques-code-provided-03a4e67d1b48?source=collection_archive---------4-----------------------#2023-12-15)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this article, I tune a base LLama2 LLM to output SQL code. I use Parameter
    Efficient Fine-Tuning techniques to optimise the process.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@christopher_karg?source=post_page-----03a4e67d1b48--------------------------------)[![Christopher
    Karg](../Images/9d163d59e0c3167732f55d497caf9db2.png)](https://medium.com/@christopher_karg?source=post_page-----03a4e67d1b48--------------------------------)[](https://towardsdatascience.com/?source=post_page-----03a4e67d1b48--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----03a4e67d1b48--------------------------------)
    [Christopher Karg](https://medium.com/@christopher_karg?source=post_page-----03a4e67d1b48--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5fbda6d16c39&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-efficiently-fine-tune-your-own-open-source-llm-using-novel-techniques-code-provided-03a4e67d1b48&user=Christopher+Karg&userId=5fbda6d16c39&source=post_page-5fbda6d16c39----03a4e67d1b48---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----03a4e67d1b48--------------------------------)
    ·13 min read·Dec 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F03a4e67d1b48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-efficiently-fine-tune-your-own-open-source-llm-using-novel-techniques-code-provided-03a4e67d1b48&user=Christopher+Karg&userId=5fbda6d16c39&source=-----03a4e67d1b48---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F03a4e67d1b48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-efficiently-fine-tune-your-own-open-source-llm-using-novel-techniques-code-provided-03a4e67d1b48&source=-----03a4e67d1b48---------------------bookmark_footer-----------)![](../Images/389987bb764f9aceba43b821ecf0128e.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Source: [https://www.pexels.com/photo/calm-body-of-lake-between-mountains-346529/](https://www.pexels.com/photo/calm-body-of-lake-between-mountains-346529/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In [a previous article](https://medium.com/towards-data-science/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb),
    I began to make a case for why you would consider training your own LLM. I also
    provided a brief introduction to the hardware requirements, as well as methods
    for optimising the training and inference. In this article, I will cover exactly
    how to fine-tune an open-source LLM and provide code snippets for you to follow
    along and reproduce the results. We will tune a Llama2–7B model to provide us
    with SQL output based on natural language input — in other words, the model will
    convert a question we ask in natural language:'
  prefs: []
  type: TYPE_NORMAL
- en: '*“How many customers decided to buy eggs in the month of November?”*'
  prefs: []
  type: TYPE_NORMAL
- en: 'To a SQL query that fetches the corresponding result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In each case, the schema of the database (DB) will be provided as the context
    for the LLM to work with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
