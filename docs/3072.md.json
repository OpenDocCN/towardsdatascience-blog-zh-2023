["```py\ndf.isna().sum().sum()\n\n0\n```", "```py\ncategories = {}  \ncat_columns = ['type']\nfor p in cat_columns:\n    df[p] = pd.Categorical(df[p])\n\n    categories[p] = df[p].cat.categories\n\ndf[cat_columns] = df[cat_columns].apply(lambda x: x.cat.codes)\nprint(categories)\n\n{'type': Index(['red', 'white'], dtype='object')}\n```", "```py\nimport sklearn.model_selection\n\ntrain_df, val_df = sklearn.model_selection.train_test_split(df, \n    test_size=0.2)\n\ntrain_X, train_y = train_df.drop(['quality'], axis = 1), train_df.quality\nval_X, val_y = val_df.drop(['quality'], axis = 1), val_df.quality\n\nprint(train_X.shape, val_X.shape)\n\n(5197, 12) (1300, 12)\n```", "```py\nimport sklearn.tree\nimport graphviz\n\nmodel = sklearn.tree.DecisionTreeRegressor(max_depth=3)\n# I've limited max_depth mostly for visualisation purposes\n\nmodel.fit(train_X, train_y)\n```", "```py\n dot_data = sklearn.tree.export_graphviz(model, out_file=None,\n                                       feature_names = train_X.columns,\n                                       filled = True)\n\ngraph = graphviz.Source(dot_data)\n\n# saving tree to png file\npng_bytes = graph.pipe(format='png')\nwith open('decision_tree.png','wb') as f:\n    f.write(png_bytes)\n```", "```py\ndef get_binary_split_for_param(param, X, y):\n    uniq_vals = list(sorted(X[param].unique()))\n\n    tmp_data = []\n\n    for i in range(1, len(uniq_vals)):\n        threshold = 0.5 * (uniq_vals[i-1] + uniq_vals[i])\n\n        # split dataset by threshold\n        split_left = y[X[param] <= threshold]\n        split_right = y[X[param] > threshold]\n\n        # calculate predicted values for each split\n        pred_left = split_left.mean()\n        pred_right = split_right.mean()\n\n        num_left = split_left.shape[0]\n        num_right = split_right.shape[0]\n\n        mse_left = ((split_left - pred_left) * (split_left - pred_left)).mean()\n        mse_right = ((split_right - pred_right) * (split_right - pred_right)).mean()\n        mse = mse_left * num_left / (num_left + num_right) \\\n            + mse_right * num_right / (num_left + num_right)\n\n        tmp_data.append(\n            {\n                'param': param,\n                'threshold': threshold,\n                'mse': mse\n            }\n        )\n\n    return pd.DataFrame(tmp_data).sort_values('mse')\n\nget_binary_split_for_param('sulphates', train_X, train_y).head(5)\n\n| param     |   threshold |      mse |\n|:----------|------------:|---------:|\n| sulphates |       0.685 | 0.758495 |\n| sulphates |       0.675 | 0.758794 |\n| sulphates |       0.705 | 0.759065 |\n| sulphates |       0.715 | 0.759071 |\n| sulphates |       0.635 | 0.759495 |\n```", "```py\ndef get_binary_split(X, y):\n    tmp_dfs = []\n    for param in X.columns:\n        tmp_dfs.append(get_binary_split_for_param(param, X, y))\n\n    return pd.concat(tmp_dfs).sort_values('mse')\n\nget_binary_split(train_X, train_y).head(5)\n\n| param   |   threshold |      mse |\n|:--------|------------:|---------:|\n| alcohol |      10.625 | 0.640368 |\n| alcohol |      10.675 | 0.640681 |\n| alcohol |      10.85  | 0.641541 |\n| alcohol |      10.725 | 0.641576 |\n| alcohol |      10.775 | 0.641604 |\n```", "```py\nmodel = sklearn.tree.DecisionTreeRegressor(min_samples_leaf = 420)\n```", "```py\nimport sklearn.metrics\nprint(sklearn.metrics.mean_absolute_error(model.predict(val_X), val_y))\n0.5890557338155006\n```", "```py\nimport sklearn.ensemble\nimport sklearn.metrics\n\nmodel = sklearn.ensemble.RandomForestRegressor(100, min_samples_leaf=100)\nmodel.fit(train_X, train_y)\n\nprint(sklearn.metrics.mean_absolute_error(model.predict(val_X), val_y))\n0.5592536196736408\n```", "```py\n# we need to specify oob_score = True to be able to calculate OOB error\nmodel = sklearn.ensemble.RandomForestRegressor(100, min_samples_leaf=100, \n     oob_score=True)\n\nmodel.fit(train_X, train_y)\n\n# error for validation set\nprint(sklearn.metrics.mean_absolute_error(model.predict(val_X), val_y))\n0.5592536196736408\n\n# error for training set\nprint(sklearn.metrics.mean_absolute_error(model.predict(train_X), train_y))\n0.5430398596179975\n\n# out-of-bag error\nprint(sklearn.metrics.mean_absolute_error(model.oob_prediction_, train_y))\n0.5571191870008492\n```", "```py\ndef plot_feature_importance(model, names, threshold = None):\n    feature_importance_df = pd.DataFrame.from_dict({'feature_importance': model.feature_importances_,\n                                                    'feature': names})\\\n            .set_index('feature').sort_values('feature_importance', ascending = False)\n\n    if threshold is not None:\n        feature_importance_df = feature_importance_df[feature_importance_df.feature_importance > threshold]\n\n    fig = px.bar(\n        feature_importance_df,\n        text_auto = '.2f',\n        labels = {'value': 'feature importance'},\n        title = 'Feature importances'\n    )\n\n    fig.update_layout(showlegend = False)\n    fig.show()\n\nplot_feature_importance(model, train_X.columns)\n```", "```py\nsklearn.inspection.PartialDependenceDisplay.from_estimator(clf, train_X, \n    range(12))\n```", "```py\nsklearn.inspection.PartialDependenceDisplay.from_estimator(clf, train_X, \n    [(1, 10)])\n```", "```py\nval_df['predictions_mean'] = np.stack([dt.predict(val_X.values) \n  for dt in model.estimators_]).mean(axis = 0)\nval_df['predictions_std'] = np.stack([dt.predict(val_X.values) \n  for dt in model.estimators_]).std(axis = 0)\n\nax = val_df.predictions_std.hist(bins = 10)\nax.set_title('Distribution of predictions std')\n```", "```py\nfrom treeinterpreter import treeinterpreter\nfrom waterfall_chart import plot as waterfall\n\nrow = val_X.iloc[[7]]\nprediction, bias, contributions = treeinterpreter.predict(model, row.values)\n\nwaterfall(val_X.columns, contributions[0], threshold=0.03, \n          rotation_value=45, formatting='{:,.3f}');\n```", "```py\nlow_impact_features = feature_importance_df[feature_importance_df.feature_importance <= 0.01].index.values\n\ntrain_X_imp = train_X.drop(low_impact_features, axis = 1)\nval_X_imp = val_X.drop(low_impact_features, axis = 1)\n\nmodel_imp = sklearn.ensemble.RandomForestRegressor(100, min_samples_leaf=100)\nmodel_imp.fit(train_X_sm, train_y)\n```", "```py\nimport fastbook\nfastbook.cluster_columns(train_X_imp)\n```", "```py\nnon_uniq_features = ['self_reference_max_shares', 'kw_min_max', \n  'n_unique_tokens']\ntrain_X_imp_uniq = train_X_imp.drop(non_uniq_features, axis = 1)\nval_X_imp_uniq = val_X_imp.drop(non_uniq_features, axis = 1)\n\nmodel_imp_uniq = sklearn.ensemble.RandomForestRegressor(100, \n  min_samples_leaf=100)\nmodel_imp_uniq.fit(train_X_imp_uniq, train_y)\nsklearn.metrics.mean_absolute_error(model_imp_uniq.predict(val_X_imp_uniq), \n  val_y)\n2974.853274034488\n```"]