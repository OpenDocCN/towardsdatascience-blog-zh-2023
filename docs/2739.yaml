- en: Regression and Bayesian Methods in Modern Preference Elicitation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现代偏好引导中的回归与贝叶斯方法
- en: 原文：[https://towardsdatascience.com/regression-and-bayesian-methods-in-modern-preference-elicitation-39a21435898d?source=collection_archive---------5-----------------------#2023-08-29](https://towardsdatascience.com/regression-and-bayesian-methods-in-modern-preference-elicitation-39a21435898d?source=collection_archive---------5-----------------------#2023-08-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/regression-and-bayesian-methods-in-modern-preference-elicitation-39a21435898d?source=collection_archive---------5-----------------------#2023-08-29](https://towardsdatascience.com/regression-and-bayesian-methods-in-modern-preference-elicitation-39a21435898d?source=collection_archive---------5-----------------------#2023-08-29)
- en: Application to simple smoothie-making
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单果昔制作的应用
- en: '[](https://ouaguenouni-hachemi.medium.com/?source=post_page-----39a21435898d--------------------------------)[![Ouaguenouni
    Mohamed](../Images/6822685015770dab0431b3a1d5966d6c.png)](https://ouaguenouni-hachemi.medium.com/?source=post_page-----39a21435898d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----39a21435898d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----39a21435898d--------------------------------)
    [Ouaguenouni Mohamed](https://ouaguenouni-hachemi.medium.com/?source=post_page-----39a21435898d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ouaguenouni-hachemi.medium.com/?source=post_page-----39a21435898d--------------------------------)[![Ouaguenouni
    Mohamed](../Images/6822685015770dab0431b3a1d5966d6c.png)](https://ouaguenouni-hachemi.medium.com/?source=post_page-----39a21435898d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----39a21435898d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----39a21435898d--------------------------------)
    [Ouaguenouni Mohamed](https://ouaguenouni-hachemi.medium.com/?source=post_page-----39a21435898d--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6c5dbf6956c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregression-and-bayesian-methods-in-modern-preference-elicitation-39a21435898d&user=Ouaguenouni+Mohamed&userId=6c5dbf6956c8&source=post_page-6c5dbf6956c8----39a21435898d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----39a21435898d--------------------------------)
    ·14 min read·Aug 29, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F39a21435898d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregression-and-bayesian-methods-in-modern-preference-elicitation-39a21435898d&user=Ouaguenouni+Mohamed&userId=6c5dbf6956c8&source=-----39a21435898d---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6c5dbf6956c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregression-and-bayesian-methods-in-modern-preference-elicitation-39a21435898d&user=Ouaguenouni+Mohamed&userId=6c5dbf6956c8&source=post_page-6c5dbf6956c8----39a21435898d---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----39a21435898d--------------------------------)
    ·14 min read·2023年8月29日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F39a21435898d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregression-and-bayesian-methods-in-modern-preference-elicitation-39a21435898d&user=Ouaguenouni+Mohamed&userId=6c5dbf6956c8&source=-----39a21435898d---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F39a21435898d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregression-and-bayesian-methods-in-modern-preference-elicitation-39a21435898d&source=-----39a21435898d---------------------bookmark_footer-----------)![](../Images/8639022815e07b35f4283dd0b0b115b8.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F39a21435898d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregression-and-bayesian-methods-in-modern-preference-elicitation-39a21435898d&source=-----39a21435898d---------------------bookmark_footer-----------)![](../Images/8639022815e07b35f4283dd0b0b115b8.png)'
- en: Photo by [Denis Tuksar](https://unsplash.com/@dtuksar?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 摄影：[Denis Tuksar](https://unsplash.com/@dtuksar?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Linear regression is often considered the workhorse of predictive modeling,
    yet its application extends beyond straightforward predictive tasks. This article
    seeks to enrich the dialogue around regression techniques by introducing Probit
    Linear Regression as an effective tool for modeling preferences. Furthermore,
    we employ a Bayesian framework to transition from classical to Bayesian Linear
    Regression, elucidating the intrinsic relationship between cost-based optimization
    — specifically Binary Cross-Entropy (BCE) loss minimization — and maximum likelihood
    estimation.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归通常被视为预测建模的主力，但其应用超越了简单的预测任务。本文旨在通过引入Probit线性回归作为建模偏好的有效工具来丰富关于回归技术的对话。此外，我们使用贝叶斯框架从经典线性回归过渡到贝叶斯线性回归，阐明了基于成本的优化——特别是二元交叉熵（BCE）损失最小化——与最大似然估计之间的内在关系。
- en: In doing so, we aim to demonstrate that regularization can be considered a form
    of Bayesian prior selection, thereby bridging cost function approaches with probabilistic
    reasoning.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样做，我们旨在展示正则化可以被视为一种贝叶斯先验选择形式，从而将成本函数方法与概率推理连接起来。
- en: Finally, we will discuss how Bayesian Linear Regression allows not only for
    point estimates but also provides a distribution over these predictions, offering
    a richer, uncertainty-aware perspective.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将讨论贝叶斯线性回归如何不仅提供点估计，还提供这些预测的分布，提供一种更丰富、考虑不确定性的视角。
- en: The Bayesian Framework
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯框架
- en: 'The Bayes Framework identifies two principal components: the data *D* and the
    model *w*. By specifying the likelihood *P*(*D*∣*w*) and a prior over the model
    *P*(*w*), we aim to find the model that maximizes the posterior *P*(*w*∣*D*),
    derived via Bayes’ theorem as:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯框架识别出两个主要组成部分：数据 *D* 和模型 *w*。通过指定似然 *P*(*D*∣*w*) 和模型的先验 *P*(*w*)，我们旨在找到最大化后验
    *P*(*w*∣*D*) 的模型，该后验通过贝叶斯定理得出，如下：
- en: '![](../Images/5f43fcae2575abf52b8ad9bf08e67ebc.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5f43fcae2575abf52b8ad9bf08e67ebc.png)'
- en: In preference learning, having a distribution over *w* offers the advantage
    of capturing the uncertainty inherent in human preferences, thereby providing
    not just a single ‘best guess’ but a range of plausible models.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在偏好学习中，对 *w* 的分布提供了捕捉人类偏好固有不确定性的优势，从而不仅提供一个‘最佳猜测’，而是一个合理模型的范围。
- en: The Preference Elicitation Problem
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏好引导问题
- en: 'Preference Elicitation is a key component in decision theory, aimed at identifying
    a decision-maker’s choices based on available data. In this study, we tackle the
    Preference Elicitation Problem by fitting a model to a partial set of preferences.
    In our case, preferences are expressed in their most straightforward form: pairwise
    comparisons. To illustrate this concept, consider a set of fruits, denoted by
    *F*, including apple, banana, orange, litchi, and mango.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 偏好引导是决策理论中的关键组成部分，旨在根据可用数据识别决策者的选择。在这项研究中，我们通过拟合一个模型到部分偏好集来处理偏好引导问题。在我们的案例中，偏好以最简单的形式表达：成对比较。为了说明这一概念，考虑一组水果，记作
    *F*，包括苹果、香蕉、橙子、荔枝和芒果。
- en: '![](../Images/74260e1eca490358d745b0aa582aee8c.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/74260e1eca490358d745b0aa582aee8c.png)'
- en: In our context, the alternative set *A* consists of all possible smoothies that
    can be created using one or multiple ingredients from set *F*.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的背景下，备选集 *A* 包含所有可以使用来自集合 *F* 的一个或多个成分制作的可能的冰沙。
- en: '![](../Images/1c0579c5f141ddaf41b9a25c24a6fcb6.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c0579c5f141ddaf41b9a25c24a6fcb6.png)'
- en: The user articulates their preferences through a set of ordered pairs (*A*,*B*),
    where *A* is strictly preferred over *B*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 用户通过一组有序对（*A*，*B*）表达他们的偏好，其中 *A* 被严格偏好于 *B*。
- en: '![](../Images/b57a0e52cff1b056c31178b57c96cac0.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b57a0e52cff1b056c31178b57c96cac0.png)'
- en: 'The subsequent section of this article will introduce the family of functions
    specifically chosen to capture user preferences: additive functions. These mathematical
    constructs provide a straightforward yet robust framework for understanding how
    different factors contribute to an individual’s preferences, thereby enabling
    effective modeling of the choices expressed through pairwise comparisons.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的下一部分将介绍专门选择的用于捕捉用户偏好的函数家族：加性函数。这些数学构造提供了一个直接而强大的框架，用于理解不同因素如何影响个人偏好，从而有效建模通过成对比较表达的选择。
- en: Additive Models
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加性模型
- en: The Linear Additive Model is the most straightforward model that could be used
    to capture the preferences of the user.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 线性加性模型是最简单的模型，可以用来捕捉用户的偏好。
- en: The additive linear model
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加性线性模型
- en: An additive utility model is one that assigns a specific weight to each individual
    ingredient in our set. The overall utility or ‘likability’ of a smoothie is then
    calculated by summing up the weights of its constituent ingredients. Formally,
    given a vector of weights
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 加性效用模型是一个将特定权重分配给我们集合中每个成分的模型。果昔的整体效用或“受欢迎程度”通过汇总其组成成分的权重来计算。正式来说，给定一个权重向量
- en: '![](../Images/21fc22fe58a5d138c9d123bd46f0983d.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21fc22fe58a5d138c9d123bd46f0983d.png)'
- en: 'The utility of a smoothie made from a subset A of ingredients is:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从成分子集A制成的果昔的效用是：
- en: '![](../Images/e18598ce3522be9380edfceb69962c02.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e18598ce3522be9380edfceb69962c02.png)'
- en: Where I is the identity function that tests whether I am in A or not.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其中I是测试是否在A中的单位函数。
- en: The additive model with binary interactions
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 带有二元交互的加性模型
- en: 'The 2-additive model builds upon the 1-additive model by introducing an additional
    layer of complexity. Not only does the weight vector contain a weight for each
    individual ingredient, but it also includes weights for every possible pair of
    ingredients. This allows the model to capture synergies between pairs of fruits,
    effectively recognizing how the combination of two ingredients can influence the
    overall utility. Formally, the weight vector **w** is extended to include weights
    ​ for each pair (*i*,*j*) in addition to the singletons:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 2-additive模型在1-additive模型的基础上引入了额外的复杂度。权重向量不仅包含每个单独成分的权重，还包括每对成分的权重。这使得模型能够捕捉成分对之间的协同效应，有效识别两个成分的组合如何影响整体效用。正式来说，权重向量**w**被扩展为包括每对(*i*,*j*)的权重，除了单一成分：
- en: '![](../Images/ce533d540397c61288f99ba8dedce633.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce533d540397c61288f99ba8dedce633.png)'
- en: 'And with the 2-additive linear model, the utility of a smoothie is given by:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用2-additive线性模型，果昔的效用由以下公式给出：
- en: '![](../Images/9b83e08e3fe84756efb6d279b39bcae8.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b83e08e3fe84756efb6d279b39bcae8.png)'
- en: Where F² is the set of singletons and pairs.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 其中F²是单一成分和成对的集合。
- en: The n-additive model
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: n-additive模型
- en: Extending the concept even further, the *n*-additive model offers a highly flexible
    utility framework. In this model, the weight vector not only accounts for individual
    ingredients and pairs but also extends to include weights for any subset of up
    to *n* ingredients. This generalization allows the model to capture intricate
    relationships and synergies among multiple ingredients simultaneously.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步扩展概念，*n*-additive模型提供了一个高度灵活的效用框架。在这个模型中，权重向量不仅考虑单个成分和成对，还扩展到包括任何最多*n*成分的子集的权重。这种概括使得模型能够同时捕捉多个成分之间的复杂关系和协同效应。
- en: 'Formally, the weight vector **w** is expanded to include weights for all possible
    combinations of up to *n* ingredients:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 正式来说，权重向量**w**被扩展为包括所有可能的最多*n*成分的组合的权重：
- en: '![](../Images/305e7329be95fe3b1f639b3688a6400a.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/305e7329be95fe3b1f639b3688a6400a.png)'
- en: This *n*-additive model can capture the full range of interactions among ingredients,
    making it an extremely powerful tool for understanding complex preference structures.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这个*n*-additive模型可以捕捉成分之间的全部交互，成为理解复杂偏好结构的强大工具。
- en: For the purposes of this analysis, we will restrict ourselves to 2-additive
    models, as we believe that the complexity of the preference relationships among
    ingredients is unlikely to exceed pairwise interactions.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了这项分析，我们将限制在2-additive模型上，因为我们认为成分之间的偏好关系复杂性不太可能超过成对交互。
- en: Learning the preferences by solving a Probit Regression problem
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过解决Probit回归问题来学习偏好
- en: While traditional regression models output real-valued predictions, our goal
    is to predict a binary preference relation.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然传统回归模型输出实值预测，但我们的目标是预测二元偏好关系。
- en: To achieve this, we modify our regression model to output the probability that
    option *A* is preferred over option *B*. We then derive an appropriate cost function
    to effectively fit this probabilistic model to our data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们修改回归模型以输出选项*A*优于选项*B*的概率。然后我们推导出一个合适的成本函数，以有效地将这个概率模型拟合到我们的数据中。
- en: One classic way to squash a value between 0 and 1 is to use the Probit function.
    The Probit function is defined as follows
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一种经典的将值压缩到0和1之间的方法是使用Probit函数。Probit函数定义如下
- en: '![](../Images/fcae2b379eefb68d8fea18dcc54d0664.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fcae2b379eefb68d8fea18dcc54d0664.png)'
- en: The following figure illustrates its shape
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了其形状
- en: '![](../Images/a13fa996771b732d3bc51385e2cdc94e.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a13fa996771b732d3bc51385e2cdc94e.png)'
- en: Shape of the probit function (by me)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 概率单位函数的形状（由我绘制）
- en: By applying this function to the difference between *f*(*A*) and *f*(*B*), our
    model will yield a probability approaching 1 if *f*(*A*) significantly exceeds
    *f*(*B*). Conversely, it will produce a probability near 0.5 if *f*(*A*) is approximately
    equal to *f*(*B*).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将此函数应用于 *f*(*A*) 和 *f*(*B*) 之间的差异，我们的模型将产生一个接近 1 的概率，如果 *f*(*A*) 显著超过 *f*(*B*)。相反，如果
    *f*(*A*) 大约等于 *f*(*B*)，它将产生一个接近 0.5 的概率。
- en: 'Thus, the preference elicitation problem can be rephrased as the search for
    an optimal weight vector **w** such that:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，偏好引导问题可以重新表述为寻找一个最优的权重向量 **w**，使得：
- en: '![](../Images/0dfd4428cdcfb41d29681cd670834fc5.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0dfd4428cdcfb41d29681cd670834fc5.png)'
- en: Binary Cross-Entropy (BCE) loss
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 二元交叉熵 (BCE) 损失
- en: 'The Binary Cross-Entropy (BCE) loss, also known as log loss, serves as a performance
    metric for classification models that output probabilities ranging from 0 to 1,
    typically used in binary classification tasks. Mathematically, given the true
    labels *y* (either 0 or 1) and the predicted probabilities *p*, the BCE is defined
    as:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 二元交叉熵 (BCE) 损失，也称为对数损失，是一种用于输出范围为 0 到 1 概率的分类模型的性能指标，通常用于二元分类任务。数学上，给定真实标签 *y*（0
    或 1）和预测概率 *p*，BCE 定义为：
- en: '![](../Images/027d438930246ef9cf3446ce9917c43e.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/027d438930246ef9cf3446ce9917c43e.png)'
- en: Toy Data Generation
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 玩具数据生成
- en: To validate our methods, we introduce a protocol for generating synthetic data.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证我们的方法，我们引入了一个生成合成数据的协议。
- en: The process begins by randomly sampling a weight vector **w**. We then set some
    of its parameters to zero, To introduce a layer of realism and simplicity.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程首先通过随机采样一个权重向量 **w** 开始。然后我们将其某些参数设置为零，以引入现实性和简单性。
- en: Operating under the assumption that the user’s preferences align with this sampled
    function, we can employ it as a benchmark to assess the accuracy of our predictive
    model.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在假设用户的偏好与这个采样函数一致的前提下，我们可以将其作为基准来评估我们预测模型的准确性。
- en: '[PRE0]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Then we encode each alternative with a binary vector where the components are
    in the same order as in the model parameters using the following function
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用以下函数将每个备选方案编码为一个二进制向量，其中组件的顺序与模型参数中的顺序相同
- en: '[PRE1]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Then to evaluate a particular smoothie we use a product
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了评估特定的平滑度，我们使用乘积
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To construct our dataset, we begin by sampling a weight vector **w**. Next,
    we generate a set of smoothies and evaluate each based on the sampled weights.
    For every pair of smoothies *A* and *B* where *f*(*A*)>*f*(*B*), we add a corresponding
    preference to our dataset. Each preference between *A* and *B* is captured in
    a vector, defined as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建我们的数据集，我们首先采样一个权重向量 **w**。接下来，我们生成一组平滑度，并根据采样的权重对每个平滑度进行评估。对于每对平滑度 *A* 和
    *B*，其中 *f*(*A*) > *f*(*B*)，我们将相应的偏好添加到我们的数据集中。每个 *A* 和 *B* 之间的偏好都被捕捉在一个向量中，定义如下：
- en: '![](../Images/8e8d20cd5c9aecac92c1c327ee473704.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8e8d20cd5c9aecac92c1c327ee473704.png)'
- en: For each pair A,B where f(A) > f(B) we add two rows v(A,B) and v(B,A) the first
    labelled with the class 1 and the second with the class 0.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每对 *A* 和 *B*，其中 *f*(*A*) > *f*(*B*)，我们添加了两行 v(*A*,*B*) 和 v(*B*,*A*)，第一行标记为类别
    1，第二行标记为类别 0。
- en: The following code gives us a dataset on n smoothies.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码为我们提供了一个关于 n 个平滑度的数据集。
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The cost-based resolution
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于成本的分辨率
- en: One way of solving the problem is by using the convexity of the BCE loss and
    a library such as Torch.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种方法是利用 BCE 损失的凸性以及类似 Torch 的库。
- en: We start by wrapping the generated data into the proper Dataset Loaders provided
    by PyTorch.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将生成的数据包装到 PyTorch 提供的适当数据集加载器中。
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, we create a simple linear model
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们创建一个简单的线性模型
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: And we train it using the autograd functionalities of PyTorch.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 并使用 PyTorch 的自动梯度功能对其进行训练。
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Then we test the obtained model using the test dataset
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用测试数据集来测试获得的模型
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: With 20% of the data used for the training, we obtained about 98.32% of accuracy
    which is not bad at all.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 20% 的数据进行训练，我们获得了约 98.32% 的准确率，这已经相当不错了。
- en: The Maximum Likelihood Estimation (MLE)
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大似然估计 (MLE)
- en: An alternative method for addressing the probit regression challenge involves
    explicitly formulating the likelihood of the data given a weight vector **w**.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 解决 probit 回归挑战的另一种方法是明确地制定给定权重向量 **w** 的数据的似然函数。
- en: 'We begin by assuming that the model produces a probability *p* indicating that
    *A* is preferred over *B*. The predictive distribution for this scenario is expressed
    as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先假设模型产生一个概率 *p*，表示 *A* 相对于 *B* 的偏好。该情景的预测分布表示如下：
- en: '![](../Images/23ccb46867f2bc8314928beb7fd9b1a9.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23ccb46867f2bc8314928beb7fd9b1a9.png)'
- en: 'The likelihood of a pair (x,y) given a vector of weights is then expressed
    as:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一对（x, y）的权重向量的可能性可表示为：
- en: '![](../Images/e924f0a3ab9ac63c7d930e15ae326bd6.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e924f0a3ab9ac63c7d930e15ae326bd6.png)'
- en: The probability of the dataset is
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的概率为
- en: '![](../Images/804943e3f83db3d19887710ae7eec446.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/804943e3f83db3d19887710ae7eec446.png)'
- en: Likelihood values can be extremely small, significant when multiplying many
    probabilities together. This can lead to numerical underflow (where very small
    floating-point numbers are rounded to zero). Taking the logarithm of these values
    turns them into more manageable numbers, which are typically negative and of a
    larger magnitude.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 似然值可能非常小，这在将许多概率相乘时尤为重要。这可能导致数值下溢（非常小的浮点数被四舍五入为零）。对这些值取对数会将其转化为更可管理的数字，这些数字通常为负值且具有更大的量级。
- en: The log-likelihood is thus given by
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对数似然因此给出为
- en: '![](../Images/52ce1997d47fa44457be3f2dcd6dad75.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52ce1997d47fa44457be3f2dcd6dad75.png)'
- en: You will probably notice that this loss is the negative of the BCE loss, and
    this is why **maximizing the likelihood is equivalent to minimizing the BCE loss**.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到，这个损失是BCE损失的负值，这就是**最大化似然等同于最小化BCE损失**的原因。
- en: '![](../Images/45e183df60348ae43c630c5651400e9f.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/45e183df60348ae43c630c5651400e9f.png)'
- en: Regularization Techniques
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化技术
- en: Regularization is a key technique in machine learning to combat overfitting,
    where a model excessively adapts to training data, including its noise, impairing
    its performance on new data. It works by adding penalty terms to the loss to limit
    the complexity of model parameters. This promotes simpler models, striking a balance
    between fitting the training data and maintaining model simplicity.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化是机器学习中的一项关键技术，用以对抗过拟合，即模型过度适应训练数据，包括其噪声，从而影响在新数据上的表现。它通过在损失函数中添加惩罚项来限制模型参数的复杂性。这促进了更简单的模型，平衡了对训练数据的拟合与保持模型简洁性之间的关系。
- en: L1 (Lasso) and L2 (Ridge) are common regularization forms, each introducing
    unique penalty terms to the model’s objective.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: L1（Lasso）和L2（Ridge）是常见的正则化形式，每种形式都向模型的目标函数中引入了独特的惩罚项。
- en: L1 adds a penalty based on the absolute value of parameters, leading to sparse
    models with some weights being zero.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: L1通过基于参数的绝对值添加惩罚，导致稀疏模型，其中一些权重为零。
- en: '![](../Images/76763393d01134971157fc90f5212933.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/76763393d01134971157fc90f5212933.png)'
- en: In contrast, L2 penalizes the square magnitude of parameters, shrinking weights
    without making them zero.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，L2惩罚参数的平方大小，缩小权重但不将其设为零。
- en: '![](../Images/b476b58e4cca61245fcdc0709798d67b.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b476b58e4cca61245fcdc0709798d67b.png)'
- en: L1 (Lasso) and L2 (Ridge) regularization techniques differentiate in how they
    penalize model parameters. L1 applies a penalty proportional to the absolute values,
    leading to some weights being entirely zero, facilitating feature selection. In
    contrast, L2 penalizes the squared magnitudes of the weights, ensuring they remain
    small but generally non-zero, preserving all features with reduced impact.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: L1（Lasso）和L2（Ridge）正则化技术在惩罚模型参数的方式上有所不同。L1应用一个与绝对值成比例的惩罚，导致一些权重完全为零，从而便于特征选择。相反，L2惩罚权重的平方大小，确保权重保持较小但通常不为零，从而保留所有特征并减少其影响。
- en: Maximum a Posteriori
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大后验
- en: As previously mentioned, Bayes’ Theorem allows us to estimate the posterior
    distribution of model parameters, denoted as *P*(*w*∣*X*,*y*), by leveraging the
    likelihood function and a chosen prior distribution *P*(*w*) for the parameters.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，贝叶斯定理允许我们通过利用似然函数和选定的参数先验分布*P*(*w*)来估计模型参数的后验分布*P*(*w*∣*X*,*y*)。
- en: In essence, the prior encapsulates our initial beliefs or assumptions about
    the parameters before observing any data, while the likelihood quantifies how
    well the parameters explain the observed data. Bayes’ Theorem combines these elements
    to produce a posterior distribution that represents our updated belief about the
    parameters, given both the prior and the data.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，先验包含了我们在观察到任何数据之前对参数的初步信念或假设，而似然量化了参数解释观察到的数据的效果。贝叶斯定理将这些元素结合起来，生成一个后验分布，代表了在先验和数据的基础上我们对参数的更新信念。
- en: Two very known priors are the **Laplace** and the **Gaussian** priors.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 两个非常著名的先验是**拉普拉斯**先验和**高斯**先验。
- en: The Laplace prior operates under the assumption that the weights *w* are drawn
    from a Laplace distribution with location parameter *μ*=0 and scale parameter
    b.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 拉普拉斯先验假设权重 *w* 服从位置参数 *μ*=0 和尺度参数 b 的拉普拉斯分布。
- en: In other words, it presumes that the distribution of the weights centres around
    zero and decays exponentially as values deviate from this central point, reflecting
    a preference for sparser models in which many weights may be set to zero.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，它假设权重的分布围绕零中心，并且随着值偏离这一中心点而指数衰减，反映了对稀疏模型的偏好，在这些模型中，许多权重可能被设为零。
- en: '![](../Images/d632ecc933faf53bb432d57995966040.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d632ecc933faf53bb432d57995966040.png)'
- en: The Gaussian prior operates under the assumption that the weights *w* are drawn
    from a Gaussian (or Normal) distribution with mean *μ*=0 and variance *σ*.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯先验假设权重 *w* 服从均值 *μ*=0 和方差 *σ* 的高斯（或正态）分布。
- en: In essence, it supposes that the distribution of the weights is symmetrically
    centred around zero, with a bell-shaped profile indicating that weights are most
    likely to be close to the mean, tapering off less likely values as you move further
    away. This leads to a preference for models where weights are smoothly regularized,
    ensuring they remain small in magnitude without necessarily driving any to be
    exactly zero.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，它假设权重的分布对称地围绕零中心，具有钟形曲线轮廓，表明权重最有可能接近均值，而随着远离均值的距离增加，可能性逐渐降低。这导致对平滑正则化的模型有偏好，确保权重保持较小的幅度，而不必精确地设为零。
- en: '![](../Images/f2c691ea0da96e69368763c9486776d6.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f2c691ea0da96e69368763c9486776d6.png)'
- en: The log-posterior is given by
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 对数后验分布由以下公式给出
- en: '![](../Images/db75cc9ca9533ce3a5c3318adfd7c51d.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db75cc9ca9533ce3a5c3318adfd7c51d.png)'
- en: By optimizing our model, we find that maximizing the log posterior is fundamentally
    equivalent to minimizing a specific regularized loss.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 通过优化我们的模型，我们发现最大化对数后验分布在本质上等同于最小化特定的正则化损失。
- en: Notably, the distinction between L1 and L2 regularization rests upon the chosen
    form of prior distribution considered.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，L1 和 L2 正则化的区别在于所考虑的先验分布形式。
- en: Using the posterior in an MCMC method
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 MCMC 方法中的后验分布
- en: In a Bayesian framework, everything is treated probabilistically. So, instead
    of estimating fixed values for regression coefficients as in classical linear
    regression, Bayesian Linear Regression estimates a distribution over possible
    coefficient values.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯框架中，一切都以概率的方式对待。因此，与经典线性回归中估计回归系数的固定值不同，贝叶斯线性回归估计的是可能系数值的分布。
- en: One way of using the posterior distribution is by sampling a set of weights
    from the distribution P(w|X,y).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 使用后验分布的一种方法是从分布 P(w|X,y) 中抽取一组权重。
- en: A simple way to do so is by using an MCMC method, the starting point to understand
    an MCMC method is the Metropolis-Hasting Approach.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的方法是使用 MCMC 方法，理解 MCMC 方法的起点是 Metropolis-Hasting 方法。
- en: Metropolis Hasting Approach
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Metropolis-Hasting 方法
- en: The Metropolis-Hastings (M-H) algorithm is a method in Bayesian statistics to
    sample from complex probability distributions.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Metropolis-Hastings (M-H) 算法是贝叶斯统计中用于从复杂概率分布中采样的一种方法。
- en: It uses a simpler “proposal distribution” to explore a target distribution,
    accepting or rejecting samples based on a calculated probability. Notably, the
    M-H algorithm doesn’t require knowledge of the exact target distribution; having
    a distribution proportional to it is sufficient.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 它使用一个更简单的“提议分布”来探索目标分布，根据计算出的概率接受或拒绝样本。值得注意的是，M-H 算法不需要确切的目标分布知识；只需具有与之成比例的分布即可。
- en: We will not use it because other approaches are more reliable and efficient
    but we will still briefly explain how it works because M-H algorithm is a foundational
    MCMC method.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会使用它，因为其他方法更可靠、更高效，但我们仍将简要说明它的工作原理，因为 M-H 算法是一个基础的 MCMC 方法。
- en: Choose an initial guess
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个初始猜测
- en: Set a proposal distribution, typically a Gaussian centred at the current value
    w.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设定一个提议分布，通常是一个以当前值 w 为中心的高斯分布。
- en: '![](../Images/9265da9a9f6ac979a19e30e9f9ecb60f.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9265da9a9f6ac979a19e30e9f9ecb60f.png)'
- en: 'Then for each iteration, we proceed as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 然后对于每次迭代，我们按如下步骤进行：
- en: Sample a new w’ from the proposal distribution P(w’|w).
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从提议分布 P(w’|w) 中抽取一个新的 w’。
- en: Compute the acceptance probability
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算接受概率
- en: '![](../Images/9ab0d6bd9ab8ca5f1c614708db06bece.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ab0d6bd9ab8ca5f1c614708db06bece.png)'
- en: Draw a random number u from a uniform distribution over [0,1]. If u ≤ α, accept
    w’ as the new sample; otherwise, retain w.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从[0,1]的均匀分布中抽取一个随机数u。如果u ≤ α，则接受w'作为新样本；否则，保留w。
- en: NUTS Sampler and pyMC3
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NUTS 采样器和 PyMC3
- en: The Metropolis-Hastings approach involves proposing a new point in the parameter
    space, and then deciding whether to accept this new point based on a comparison
    of its likelihood to the current point’s likelihood. Its efficiency depends heavily
    on the choice of proposal distribution, and it can suffer from random-walk behaviour
    in high-dimensional spaces, leading to slow convergence.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Metropolis-Hastings 方法涉及在参数空间中提议一个新点，然后根据新点的似然与当前点的似然进行比较来决定是否接受这个新点。其效率在很大程度上依赖于提议分布的选择，并且在高维空间中可能会出现随机游走行为，从而导致收敛速度缓慢。
- en: NUTS (No-U-Turn Sampler) is an extension of the Hamiltonian Monte Carlo (HMC)
    method. Instead of a random walk, NUTS utilizes gradient information from the
    target distribution to propose leapfrog steps, allowing it to traverse the distribution
    more efficiently. One of its main advantages is that it automatically determines
    the optimal number of leapfrog steps, thus avoiding the random walk problem and
    the tedious task of tuning this manually.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: NUTS（无转弯采样器）是哈密顿蒙特卡罗（HMC）方法的扩展。NUTS 不依赖随机游走，而是利用目标分布的梯度信息来提议跃迁步伐，从而使得其能够更有效地遍历分布。它的一个主要优点是自动确定跃迁步伐的最优数量，从而避免了随机游走问题以及手动调整的繁琐任务。
- en: PyMC3 is a popular probabilistic programming framework that seamlessly integrates
    both these methods (and others), enabling users to fit complex Bayesian models
    with ease, without getting bogged down in the intricacies of the underlying algorithms.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: PyMC3 是一个流行的概率编程框架，它无缝集成了这些方法（以及其他方法），使用户能够轻松拟合复杂的贝叶斯模型，而不会被底层算法的复杂性所困扰。
- en: In our case, this code will sample a sequence of weights from the posterior
    distribution P(w|X,y).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，这段代码将从后验分布 P(w|X,y) 中采样一系列权重。
- en: '[PRE8]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We can plot the different distributions of each weight.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以绘制每个权重的不同分布。
- en: '![](../Images/c37c83543b2bfc866c8893597140c185.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c37c83543b2bfc866c8893597140c185.png)'
- en: Distributions of the weights and their convergence plot (by me)
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 权重的分布及其收敛图（由我提供）
- en: We see that each weight converges to a Gaussian distribution. And so now each
    prediction could be made probabilistically and the distribution of the predictions
    will also be a Gaussian.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到每个权重都收敛到高斯分布。因此，现在每个预测都可以以概率的方式进行，并且预测的分布也将是高斯分布。
- en: For instance, the preferences of our fictive decider for an Orange smoothie,
    for an Orange-Apple smoothie, and for a Banana-Apple smoothie are given by the
    following Gaussians.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们虚拟决策者对橙子冰沙、橙子-苹果冰沙和香蕉-苹果冰沙的偏好分别由以下高斯分布给出。
- en: '![](../Images/8e57ba69da7383b5f92e77e0e167f108.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8e57ba69da7383b5f92e77e0e167f108.png)'
- en: Using the model that generated the data we can see that the ground truth utility
    of the three smoothies are respectively -0.66, -0.24 and 0.79 so the Gaussian
    actually reflects the preferences and the gap between them pretty well.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 使用生成数据的模型，我们可以看到三种冰沙的真实效用分别为 -0.66、-0.24 和 0.79，因此高斯分布实际上很好地反映了偏好及其之间的差距。
- en: Conclusion
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this blog post, we have journeyed from the intricacies of preference elicitation
    to the complexities of Bayesian linear regression models. Our discussion began
    with an exploration of 2-additive models, which serve as a realistic yet computationally
    tractable means of capturing user preferences. By transitioning from basic linear
    regression to more advanced probit models, we offered a new lens through which
    to understand preference data.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我们从偏好引导的复杂性到贝叶斯线性回归模型的复杂性进行了探讨。我们的讨论从2-additive模型的探索开始，这些模型作为捕捉用户偏好的现实但计算上可处理的方式。通过从基础线性回归过渡到更高级的probit模型，我们提供了一个理解偏好数据的新视角。
- en: We also dove into the equivalence between a cost-based perspective and a probabilistic
    one, shedding light on how minimizing Binary Cross-Entropy loss is analogous to
    maximizing likelihood, and how regularization serves as the implicit selection
    of a prior.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还探讨了基于成本的视角与概率视角之间的等价性，阐明了最小化二元交叉熵损失如何类似于最大化似然，以及正则化如何作为对先验的隐式选择。
- en: Lastly, we discussed the utility of a Bayesian framework in generating not just
    point estimates but entire predictive distributions. This approach lends a higher
    level of confidence and interpretability to our models, particularly useful in
    the nuanced task of preference learning.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们讨论了贝叶斯框架在生成不仅仅是点估计而是整个预测分布的实用性。这种方法为我们的模型提供了更高的信心和可解释性，特别是在偏好学习这一微妙任务中十分有用。
- en: With this groundwork laid, future research can delve deeper into the application
    of these sophisticated models to increasingly complex and large-scale preference
    data.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在奠定了这些基础后，未来的研究可以更深入地探讨将这些复杂模型应用于越来越复杂的大规模偏好数据中。
- en: Acknowledgments
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: Special thank to [Anissa Hacene](https://medium.com/u/7bb6c80751ad?source=post_page-----39a21435898d--------------------------------)
    my coworker/friend for her contribution in this work and to the TDS team for their
    prompt review and insightful remarks.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 特别感谢我的同事/朋友[Anissa Hacene](https://medium.com/u/7bb6c80751ad?source=post_page-----39a21435898d--------------------------------)在这项工作中的贡献，以及TDS团队的及时审查和有见地的评论。
