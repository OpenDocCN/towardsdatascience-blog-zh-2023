- en: Multivariate Process Control by Principal Component Analysis Using T² and Q
    errors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/multivariate-process-control-by-principal-component-analysis-using-t%C2%B2-and-q-errors-c94908d14b04?source=collection_archive---------8-----------------------#2023-04-26](https://towardsdatascience.com/multivariate-process-control-by-principal-component-analysis-using-t%C2%B2-and-q-errors-c94908d14b04?source=collection_archive---------8-----------------------#2023-04-26)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Using and interpreting Hotelling’s T² and Squared Prediction Error Q in anomaly
    detection systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@davide.massidda?source=post_page-----c94908d14b04--------------------------------)[![Davide
    Massidda](../Images/b5cf1dc0201041ff5cad76ee13ee2df1.png)](https://medium.com/@davide.massidda?source=post_page-----c94908d14b04--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c94908d14b04--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c94908d14b04--------------------------------)
    [Davide Massidda](https://medium.com/@davide.massidda?source=post_page-----c94908d14b04--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F433251ece79d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultivariate-process-control-by-principal-component-analysis-using-t%C2%B2-and-q-errors-c94908d14b04&user=Davide+Massidda&userId=433251ece79d&source=post_page-433251ece79d----c94908d14b04---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c94908d14b04--------------------------------)
    ·12 min read·Apr 26, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc94908d14b04&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultivariate-process-control-by-principal-component-analysis-using-t%25C2%25B2-and-q-errors-c94908d14b04&user=Davide+Massidda&userId=433251ece79d&source=-----c94908d14b04---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc94908d14b04&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultivariate-process-control-by-principal-component-analysis-using-t%25C2%25B2-and-q-errors-c94908d14b04&source=-----c94908d14b04---------------------bookmark_footer-----------)![](../Images/054e0f77d6a401cd71794ce2aa09c550.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image from [geralt](https://pixabay.com/users/geralt-9301/) on [Pixabay](https://pixabay.com/illustrations/geometry-mathematics-volume-surface-1044090/)
  prefs: []
  type: TYPE_NORMAL
- en: A substantial part of my job as a data scientist consists in building anomaly
    detection systems for process control in manufacturing, and the Principal Component
    Analysis (from now PCA) has an essential role in my toolbox.
  prefs: []
  type: TYPE_NORMAL
- en: 'The scientific literature suggests two measures to trace anomalies through
    the PCA: **Hotelling’s T²** and Squared Prediction Error, aka **Q error**. Although
    their calculation is not complicated, the base software packages often ignore
    them, and their meaning remains quite enigmatic.'
  prefs: []
  type: TYPE_NORMAL
- en: There are hundreds of good tutorials about PCA on the web. However, despite
    this large amount of information, some questions about the usage of PCA in production
    environments still need to be adequately addressed. In this tutorial, I’ll avoid
    repeating what hundreds of people said before me, going straight to the point.
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After a quick introduction to PCA — just enough to emphasize some points —
    in this tutorial, we’ll deep dive into the usage of T² and Q errors, disassembling
    their formulas to understand what they can tell us. Here you will not find the
    whole story about PCA and its usage in anomaly detection, but I will focus on
    the meaning of T² and Q measures: from where they rise and how to interpret them.'
  prefs: []
  type: TYPE_NORMAL
- en: I will use basic Python code, directly leveraging the SVD algorithm implemented
    in `numpy`. I will propose only the necessary code to obtain the described results,
    cutting the one to generate the graphs (you can find it [here](https://github.com/DavideMassidda/MPC-by-PCA)).
  prefs: []
  type: TYPE_NORMAL
- en: Read on if you have at least a shallow knowledge of PCA as a data reduction
    technique. If you don’t know what a principal component is, I suggest you move
    to a general dissertation about PCA, then next, return here.
  prefs: []
  type: TYPE_NORMAL
- en: Table of contents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Process control data**'
  prefs: []
  type: TYPE_NORMAL
- en: – *Centering and scaling data* **Process control by PCA**
  prefs: []
  type: TYPE_NORMAL
- en: – *An encoding-decoding system*
  prefs: []
  type: TYPE_NORMAL
- en: – *Distillation* **Anomaly detection by PCA**
  prefs: []
  type: TYPE_NORMAL
- en: – *Squared Prediction Error Q*
  prefs: []
  type: TYPE_NORMAL
- en: – *Hotelling’s T²* **Anomaly detection in practice** – *From data to PCs… and
    back*
  prefs: []
  type: TYPE_NORMAL
- en: – *Q contributions*
  prefs: []
  type: TYPE_NORMAL
- en: – *T² contributions* **Concluding remarks**
  prefs: []
  type: TYPE_NORMAL
- en: Process control data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this tutorial, I generated some ad-hoc data, simulating a straightforward
    process control in an industrial plant. In [this repository](https://github.com/DavideMassidda/MPC-by-PCA),
    you can find the data and a notebook, including the complete code used to run
    the analyses.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start reading the data and briefly exploring it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The table includes a column “group” splitting observations in a *train* set
    (100 records) and a *test* set (30 records). Five variables are recorded.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Each variable is monitored over time.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The correlation matrix shows good relations between the whole set of variables.
    No “isolated communities” of variables or redundant measures appear.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Centering and scaling data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since data have different scales, we need to center and scale values before
    running the PCA, obtaining a training set where each variable has a mean of 0
    and a standard deviation of 1.
  prefs: []
  type: TYPE_NORMAL
- en: To rescale future data, I train a “scaler” object storing the center and scale
    parameters, so I transform the training data. I call “X” the raw observed data
    and “Z” the scaled data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Finally, let’s see the time series of records for each autoscaled variable.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9bb9203266640708a70d3e8322ed59eb.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. Times series of autoscaled variables — Image from the author, license
    CC0.
  prefs: []
  type: TYPE_NORMAL
- en: Process control by PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A way to view PCA is as an encoding-decoding system trained on a data set representing
    a process in control. During the training, the system learns the rules for relating
    the variables to monitor. Subsequently, laying on these rules, the system can
    evaluate new data, establishing whether a process is in control.
  prefs: []
  type: TYPE_NORMAL
- en: 'PCA takes the observed variables as an ensemble and **redistributes their variability**,
    building new **orthogonal** variables: the Principal Components (PCs). Starting
    from *k* variables, PCA allows obtaining *k* PCs. The algorithm estimates the
    coefficients (*loadings*) to multiply with observed variables (Z) to get PCs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cf94358b68cc321cd5bb4a72ef0f8e7d.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. Relations between the data space and the components space in PCA
    — Image from the author, license CC0.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, PCA finds the rules (i.e., the loadings) to **project** data from
    the space of observed data to the space of principal components, obtaining the
    PC scores. This projection can be reverted by multiplying the PC scores by the
    transposed loadings matrix (see Figure 2).
  prefs: []
  type: TYPE_NORMAL
- en: I use the Singular Value Decomposition algorithm in the following code block
    to estimate loadings for our plant data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The loadings array reports variables on the rows and PCs on the columns. This
    is the usual orientation of the matrix, but software packages can differ in their
    disposition.
  prefs: []
  type: TYPE_NORMAL
- en: We can obtain the PC scores by multiplying the observed data matrix by the loadings
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: An encoding-decoding system
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A key concept of PCA is that each PC is fed by all the variables (although with
    varying degrees depending on the loading value). Then, when we inspect the scores
    of a single PC, we inspect all the variables at once (yes, it is!).
  prefs: []
  type: TYPE_NORMAL
- en: However, the overall data variability isn’t equally distributed between PCs
    because PCs are ranked from the one accounting for the greatest variability (PC_1)
    to the one accounting for the lesser variability (PC_k). Realistically, the first
    PCs account for the “signal” present in data, while the latest for the noise.
    So, we can drop the PCs not conveying a significant amount of variability, separating
    the signal from the noise.
  prefs: []
  type: TYPE_NORMAL
- en: The figure below shows the time series PC scores on our data. It’s evident the
    decrement in variability when increasing the rank of the component (in particular,
    jumping from the first to the second component).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1128f44e5a1a16488cf6c14495a3e9c8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. Time series of principal component scores — Image from the author,
    license CC0.
  prefs: []
  type: TYPE_NORMAL
- en: Distillation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the image below, I exploded Figure 2\. I represented all the retained PCs
    in the gray node named *distilled data*, while with the other gray node, I represented
    all the dropped PCs. I used the term “distilled” because each component is an
    amalgamated essence of many variables.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b190cb219fc2564ac841b2d74d18d42.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. PCA as an encoding-decoding system — Image from the author, license
    CC0.
  prefs: []
  type: TYPE_NORMAL
- en: We can calculate the amount of variance assimilated by each component by squaring
    the singular values from SVD and dividing them by the degrees of freedom.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Let’s look at the results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The first component accounts for 69% of the total variance of data. Adding up
    the second component, we can explain the 80%. Two components can be enough to
    resume the whole data if we assume the remaining 20% is noise.
  prefs: []
  type: TYPE_NORMAL
- en: Using only two of five components is like performing a data compression with
    losing information. However, since the lost part theoretically is noise, we are
    gaining an advantage by dropping the last components.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s isolate the loadings and variances on our system.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: When new data arrive at a trained PCA, they are projected into the component
    space (encoding). Afterward, they are rebuilt to revert the process, using just
    the firsts PCs (decoding).
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection by PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we deep dive into detecting anomalies by Q and T² error measures. First
    of all, let’s understand what they represent. We will see the formulas directly
    applied to the data later.
  prefs: []
  type: TYPE_NORMAL
- en: Squared Prediction Error Q
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since we dropped the noise in the encoding step, the rebuilt data cannot be
    exactly equal to the observed data. This difference generates the error Q.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the process control system returns an anomaly of type Q, **something has
    broken the correlation structure**: one or more variables are no longer variating
    in harmony with the others (where the concept of “in harmony” is defined by the
    correlation matrix).'
  prefs: []
  type: TYPE_NORMAL
- en: We can distinguish two extreme scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: A variable takes an unexpected value (not necessarily out of range) and is no
    longer “predictable” from the others. If a certain type of correlation is expected
    from two variables, and suddenly the direction of one changes, their expected
    relation isn’t observed more, generating an alarm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The variable has a normal behavior, but all the others deviate from expectations.
    If previous information says that when the other variables deviate, our variable
    should deviate too, and it does not, the process has an anomaly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second scenario can appear paradoxical. For this reason, we need to consider
    another type of error: Hotelling’s T².'
  prefs: []
  type: TYPE_NORMAL
- en: Hotelling’s T²
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unlike Q, T² is more **linked to the metric of the observed data**: values
    out of the expected range generate extremes T².'
  prefs: []
  type: TYPE_NORMAL
- en: The T² statistic is strictly related to the [Mahalanobis distance](https://en.wikipedia.org/wiki/Mahalanobis_distance).
    We can conceive T² as a multivariate version of an anomaly detection system based
    on thresholds imposed on observed data but with some relevant differences.
  prefs: []
  type: TYPE_NORMAL
- en: Observed data are denoised from the encoding system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Anomalies are searched at the PC level, and only later they will be decoded
    at the observed data level.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second point is interesting because PCs synthesize the entire group of variables.
    Using T², we do not search anomalies variable-by-variable in a univariate perspective,
    but we analyze data all at once.
  prefs: []
  type: TYPE_NORMAL
- en: During the training process, we stored the variances of the PCs. Now, we can
    use this information to compare the observed variance of PCs against the expected
    one, calculating an “anomaly score” for each PC. Afterward, we can decode this
    anomaly score to return to the data space, obtaining the contribution of each
    variable to the variation of the PCs.
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection in practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Previously we built our PCA system. Now, let’s start to use it, projecting new
    data in the PC space. We start centering and scaling data, next visualizing the
    time series of variables.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In the image below, the time series of each variable for the new data is represented.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5b264beb537ff0a6617c08b3fd51ea5b.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5\. Time series of centered and scaled test data — Image from the author,
    license CC0.
  prefs: []
  type: TYPE_NORMAL
- en: Insane data, isn’t it? As you can see, **I simulated a going crazy process**.
    Four variables strongly drift from the expected mean to high values, and a fifth
    variable, after an initial drift, gradually returns to normal values.
  prefs: []
  type: TYPE_NORMAL
- en: From data to PCs… and back
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can **encode** data calculating the PC scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Thereafter, we can **decode** the PCs calculating the expected data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s visualize the time series of rebuilt data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/46a1faf68a46450b5ad31ffc1f4a9e06.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6\. Times series of expected data built using a subset of PCs — Image
    from the author, license CC0.
  prefs: []
  type: TYPE_NORMAL
- en: Well, here we finally reveal a fundamental issue of multivariate process control.
    If you look closely at the chart above, you’ll notice that the rebuilt data show
    two main defects compared to the observed data (Figure 5).
  prefs: []
  type: TYPE_NORMAL
- en: The observed data of the first four variables peak and settle on a plateau.
    Differently, their expected values, after peaking, gradually decrease.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The observed data of the fifth variable climb to the peak and quickly decrease.
    Differently, its expected values show a slight decrease after peaking. Essentially,
    its trend is similar to one of the other variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The reconstruction of the first four variables has been influenced by what happens
    in the fifth, while the reconstruction of the fifth variable has been influenced
    by what happens in the first four. Then, each rebuilt variable is a mix of the
    entire dataset.
  prefs: []
  type: TYPE_NORMAL
- en: This inaccuracy is happening **because we are decoding by dropping the last
    PCs**. According to our PCA, the “madness” of new data is noise, so it’s absorbed
    from the last PCs, which absorb the “singularities” of new data absent in training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In production environments, Q and T² errors can be used to monitor these anomalies
    and understand their origin. For both statistics, we can calculate the following
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: the contribution of each variable to each measure of error;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the squared cumulated contributions for each measure of error, generating the
    Q and T² properly said.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find details with good math explanations [here](https://wiki.eigenvector.com/index.php?title=T-Squared_Q_residuals_and_Contributions).
    In this post, I’ll focus on the contributions since the main interest in data
    monitoring is locating the source of the anomaly.
  prefs: []
  type: TYPE_NORMAL
- en: Q contributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The contribution of each variable to the Q error is the distance between observed
    scaled data and their expected (rebuilt) values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In the image below, you can visualize the time series of Q contributions of
    new data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d59e0bca5f013d27591efda947055e22.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7\. Times series of Q contributions — Image from the author, license
    CC0.
  prefs: []
  type: TYPE_NORMAL
- en: Roughly speaking, Q contributions are near zero for all the variables since
    half of the monitoring period. In the second part of the period, Q contributions
    drift from zero for all the variables, especially for the fifth.
  prefs: []
  type: TYPE_NORMAL
- en: This shouldn’t surprise us. As we have seen, up to the middle of the period,
    the variables are anomalous but anomalous **in a synchronous way** (coherently
    with correlations observed in the training set). In the second part of the period,
    the fifth variable returns towards the mean, going to “normal” values (near the
    ones of the training set) but **breaking** the correlation structure.
  prefs: []
  type: TYPE_NORMAL
- en: T² contributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The formula to obtain T² contributions is similar to the formula to decode
    PCs to the data level but with an important difference: the **scaling factor**.'
  prefs: []
  type: TYPE_NORMAL
- en: PCs are all mean-centered, so the expected value for all the PCs is zero. However,
    since each PC absorbs a different amount of data variance (see Figure 3), each
    PC has its scale. The same score may represent a small variation on one PC but
    a large variation on another!
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, before decoding, each PC is scaled by dividing it by its standard
    deviation. In this way, PCs are put on the same scale before decoding.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The image below shows the time series of T² contributions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5f6918975916ca544256ea9ac21057ce.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8\. Time series of T² contributions — Image from the author, license
    CC0.
  prefs: []
  type: TYPE_NORMAL
- en: Concluding remarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The process control using PCA considers the monitored variables as a system
    that should move in unison, expecting its elements to vary consistently (Q statistic)
    and within certain ranges (T² statistic).
  prefs: []
  type: TYPE_NORMAL
- en: 'The main difference between Q and T² anomalies is where these originate: the
    first at the **data level** and the second at the **PC level**.'
  prefs: []
  type: TYPE_NORMAL
- en: Q warns us that the encoding-decoding system isn’t working as expected on new
    data because of the observed relations between the variables. So, the Q contributions
    allow us to identify the unpredictable variables given the observed data.
  prefs: []
  type: TYPE_NORMAL
- en: T² warns us that the encoding system is getting PC scores too distant from the
    center of the whole data. The T² contributions decode the errors, “back-propagating”
    them to the data level, allowing us to identify the variables with anomalous observations.
  prefs: []
  type: TYPE_NORMAL
- en: The core idea is that each data record consists of a signal and a noise. The
    PCA removes the noise and evaluates the signal, alarming if the signal of a variable
    is too distant from as expected (T²) and if it’s overwhelmed by the noise (Q).
    One does not necessarily imply the other, although they can occur together.
  prefs: []
  type: TYPE_NORMAL
