["```py\nbeam(n=3)\n    \"Q: Say 'Hello, {name}!'\" \n    \"A: [RESPONSE]\" \nfrom \"openai/text-davinci-003\"\nwhere len(TOKENS(RESPONSE)) < 20\n```", "```py\npip install lmql\n```", "```py\nimport os\nos.environ['OPENAI_API_KEY'] = '<your_api_key>'\n```", "```py\npip install llama-cpp-python\n```", "```py\nCMAKE_ARGS=\"-DLLAMA_METAL=on\" pip install llama-cpp-python\n```", "```py\nimport os\nimport urllib.request\n\ndef download_gguf(model_url, filename):\n    if not os.path.isfile(filename):\n        urllib.request.urlretrieve(model_url, filename)\n        print(\"file has been downloaded successfully\")\n    else:\n        print(\"file already exists\")\n\ndownload_gguf(\n    \"https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/resolve/main/zephyr-7b-beta.Q4_K_M.gguf\", \n    \"zephyr-7b-beta.Q4_K_M.gguf\"\n)\n\ndownload_gguf(\n    \"https://huggingface.co/TheBloke/Llama-2-7B-GGUF/resolve/main/llama-2-7b.Q4_K_M.gguf\", \n    \"llama-2-7b.Q4_K_M.gguf\"\n)\n```", "```py\ncapital_func = lmql.F(\"What is the captital of {country}? [CAPITAL]\", \n    constraints = \"STOPS_AT(CAPITAL, '.')\")\n\ncapital_func('the United Kingdom')\n\n# Output - '\\n\\nThe capital of the United Kingdom is London.'\n```", "```py\nimport nest_asyncio\nnest_asyncio.apply()\n```", "```py\nquery_string = '''\n    \"Q: What is the captital of {country}? \\\\n\"\n    \"A: [CAPITAL] \\\\n\"\n    \"Q: What is the main sight in {CAPITAL}? \\\\n\"\n    \"A: [ANSWER]\" where (len(TOKENS(CAPITAL)) < 10) \\\n      and (len(TOKENS(ANSWER)) < 100) and STOPS_AT(CAPITAL, '\\\\n') \\\n      and STOPS_AT(ANSWER, '\\\\n')\n'''\n\nlmql.run_sync(query_string, country=\"the United Kingdom\")\n```", "```py\n@lmql.query\ndef capital_sights(country):\n    '''lmql\n    \"Q: What is the captital of {country}? \\\\n\"\n    \"A: [CAPITAL] \\\\n\"\n    \"Q: What is the main sight in {CAPITAL}? \\\\n\"\n    \"A: [ANSWER]\" where (len(TOKENS(CAPITAL)) < 10) and (len(TOKENS(ANSWER)) < 100) \\\n        and STOPS_AT(CAPITAL, '\\\\n') and STOPS_AT(ANSWER, '\\\\n')\n\n    # return just the ANSWER \n    return ANSWER\n    '''\n\nprint(capital_sights(country=\"the United Kingdom\"))\n\n# There are many famous sights in London, but one of the most iconic is \n# the Big Ben clock tower located in the Palace of Westminster. \n# Other popular sights include Buckingham Palace, the London Eye, \n# and Tower Bridge.\n```", "```py\nquery_string = \"\"\"\n\"Q: What is the sentiment of the following review: ```", "```py?\\\\n\"\n\"A: [SENTIMENT]\"\n\"\"\"\n\nlmql.run_sync(\n    query_string, \n    model = lmql.model(\"local:llama.cpp:zephyr-7b-beta.Q4_K_M.gguf\", \n        tokenizer = 'HuggingFaceH4/zephyr-7b-beta'))\n\n# [Error during generate()] The requested number of tokens exceeds \n# the llama.cpp model's context size. Please specify a higher n_ctx value.\n```", "```py\n[Error during generate()] The requested number of tokens exceeds the llama.cpp \nmodel's context size. Please specify a higher n_ctx value.\n```", "```py\nquery_string = \"\"\"\n\"Q: What is the sentiment of the following review: ```", "```py?\\\\n\"\n\"A: [SENTIMENT]\" where (len(TOKENS(SENTIMENT)) < 200)\n\"\"\"\n\nprint(lmql.run_sync(query_string, \n    model = lmql.model(\"local:llama.cpp:zephyr-7b-beta.Q4_K_M.gguf\", \n        tokenizer = 'HuggingFaceH4/zephyr-7b-beta')).variables['SENTIMENT'])\n\n#  Positive sentiment.\n# \n# Q: What is the sentiment of the following review: ```", "```py?\n# A:  Negative sentiment.\n# \n# Q: What is the sentiment of the following review: ```", "```py?\n# A:  Positive sentiment.\n# \n# Q: What is the sentiment of the following review: ```", "```py?\n# A:  Negative sentiment.\n# \n# Q: What is the sentiment of the following review: ```", "```py?\n# A:  Negative sentiment.\n# \n# Q: What is the sentiment of the following review: ```", "```py?\n# A:  Positive sentiment.\n# \n# Q: \n```", "```py\nquery_string = \"\"\"\n\"Q: What is the sentiment of the following review: ```", "```py?\\\\n\"\n\"A: [SENTIMENT]\" where STOPS_AT(SENTIMENT, 'Q:') \\\n     and STOPS_AT(SENTIMENT, '\\\\n')\n\"\"\"\n\nprint(lmql.run_sync(query_string, \n    model = lmql.model(\"local:llama.cpp:zephyr-7b-beta.Q4_K_M.gguf\", \n        tokenizer = 'HuggingFaceH4/zephyr-7b-beta')).variables['SENTIMENT'])\n\n# Positive sentiment.\n```", "```py\nquery_string = \"\"\"\n\"Q: What is the sentiment of the following review: ```", "```py?\\\\n\"\n\"A: [SENTIMENT]\" where (SENTIMENT in ['positive', 'negative', 'neutral'])\n\"\"\"\n\nprint(lmql.run_sync(query_string, \n    model = lmql.model(\"local:llama.cpp:zephyr-7b-beta.Q4_K_M.gguf\", \n        tokenizer = 'HuggingFaceH4/zephyr-7b-beta')).variables['SENTIMENT'])\n\n# positive\n```", "```py\nquery_string = \"\"\"\n\"Q: What is the sentiment of the following review: ```", "```py?\\\\n\"\n\"A: Let's think step by step. [ANALYSIS]. Therefore, the sentiment is [SENTIMENT]\" where (len(TOKENS(ANALYSIS)) < 200) and STOPS_AT(ANALYSIS, '\\\\n') \\\n    and (SENTIMENT in ['positive', 'negative', 'neutral'])\n\"\"\"\n\nprint(lmql.run_sync(query_string, \n    model = lmql.model(\"local:llama.cpp:zephyr-7b-beta.Q4_K_M.gguf\", \n        tokenizer = 'HuggingFaceH4/zephyr-7b-beta')).variables)\n```", "```py\nquery_string = \"\"\"\n\"Q: What is the sentiment of the following review: ```", "```py?\\\\n\"\n\"A: Let's think step by step. [ANALYSIS]. Therefore, the sentiment is [SENTIMENT]\" where (len(TOKENS(ANALYSIS)) < 200) and STOPS_AT(ANALYSIS, '\\\\n') \\\n    and (SENTIMENT in ['positive', 'negative', 'neutral'])\n\"\"\"\n\nprint(lmql.run_sync(query_string, \n    model = lmql.model(\"local:llama.cpp:llama-2-7b.Q4_K_M.gguf\")).variables)\n```", "```py\nquery_string = \"\"\"\n\"Q: What is the sentiment of the following review: ```", "```py?\\\\n\"\n\"A: Let's think step by step. [ANALYSIS]. Therefore, the sentiment is [SENTIMENT]\" distribution SENTIMENT in ['positive', 'negative', 'neutral']\nwhere (len(TOKENS(ANALYSIS)) < 200) and STOPS_AT(ANALYSIS, '\\\\n')\n\"\"\"\n\nprint(lmql.run_sync(query_string, \n    model = lmql.model(\"local:llama.cpp:zephyr-7b-beta.Q4_K_M.gguf\", \n        tokenizer = 'HuggingFaceH4/zephyr-7b-beta')).variables)\n```", "```py\n@lmql.query(model=lmql.model(\"local:llama.cpp:zephyr-7b-beta.Q4_K_M.gguf\", \n   tokenizer = 'HuggingFaceH4/zephyr-7b-beta', n_gpu_layers=1000))\n# specified n_gpu_layers to use GPU for higher speed\ndef sentiment_analysis(review):\n    '''lmql\n    \"Q: What is the sentiment of the following review: ```", "```py?\\\\n\"\n    \"A: Let's think step by step. [ANALYSIS]. Therefore, the sentiment is [SENTIMENT]\" where (len(TOKENS(ANALYSIS)) < 200) and STOPS_AT(ANALYSIS, '\\\\n') \\\n        and (SENTIMENT in ['positive', 'negative', 'neutral'])\n    '''\n\n@lmql.query(model=lmql.model(\"local:llama.cpp:zephyr-7b-beta.Q4_K_M.gguf\", \n  tokenizer = 'HuggingFaceH4/zephyr-7b-beta', n_gpu_layers=1000))\ndef sentiment_analysis_distribution(review):\n    '''lmql\n    \"Q: What is the sentiment of the following review: ```", "```py?\\\\n\"\n    \"A: Let's think step by step. [ANALYSIS]. Therefore, the sentiment is [SENTIMENT]\" distribution SENTIMENT in ['positive', 'negative', 'neutral']\n    where (len(TOKENS(ANALYSIS)) < 200) and STOPS_AT(ANALYSIS, '\\\\n')\n    '''\n```", "```py\nsentiment_analysis('Room was dirty')\n```", "```py\nsentiment_analysis('Room was dirty', decoder = 'beam', \n    n = 3, temperature = 0.8)[0]\n```"]