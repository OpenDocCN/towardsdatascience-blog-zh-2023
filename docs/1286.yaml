- en: Accelerating Block Sparse Matrix Multiplication with Graphcore IPU and the PopSparse
    library
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/accelerating-block-sparse-matrix-multiplication-with-graphcore-ipu-and-the-popsparse-library-3791284a2127?source=collection_archive---------10-----------------------#2023-04-12](https://towardsdatascience.com/accelerating-block-sparse-matrix-multiplication-with-graphcore-ipu-and-the-popsparse-library-3791284a2127?source=collection_archive---------10-----------------------#2023-04-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Faster sparse matrix multiplication with the new library PopSparse**'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@dominic.masters.gc?source=post_page-----3791284a2127--------------------------------)[![Dominic
    Masters](../Images/a4bb210cb0a19c81941c50a1e07a11b6.png)](https://medium.com/@dominic.masters.gc?source=post_page-----3791284a2127--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3791284a2127--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3791284a2127--------------------------------)
    [Dominic Masters](https://medium.com/@dominic.masters.gc?source=post_page-----3791284a2127--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb23ad1d05ffe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faccelerating-block-sparse-matrix-multiplication-with-graphcore-ipu-and-the-popsparse-library-3791284a2127&user=Dominic+Masters&userId=b23ad1d05ffe&source=post_page-b23ad1d05ffe----3791284a2127---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3791284a2127--------------------------------)
    ·10 min read·Apr 12, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3791284a2127&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faccelerating-block-sparse-matrix-multiplication-with-graphcore-ipu-and-the-popsparse-library-3791284a2127&user=Dominic+Masters&userId=b23ad1d05ffe&source=-----3791284a2127---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3791284a2127&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faccelerating-block-sparse-matrix-multiplication-with-graphcore-ipu-and-the-popsparse-library-3791284a2127&source=-----3791284a2127---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'Authors: **Zhiyi Li**, AI Engineer, **Douglas Orr**, Research Scientist, **Valeriu
    Ohan**, Software Engineer, and [**Dominic Masters**](https://medium.com/u/b23ad1d05ffe?source=post_page-----3791284a2127--------------------------------),
    Research Scientist'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5b70b53b06a68360136d4264b945137d.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
- en: Image by author.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: In light of the significant growth in deep learning model size as well as the
    increased uptake of artificial intelligence across a broad range of applications,
    the incentives to reduce the computational cost of running these models have never
    been greater.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于深度学习模型规模的显著增长以及人工智能在广泛应用中的普及，减少运行这些模型的计算成本的激励比以往任何时候都更大。
- en: One method that has shown significant promise to reduce these costs is to use
    *sparsity*. Sparsity typically refers to the method of inducing as many zeros
    as possible in the model weights through a process of *pruning*, then skipping
    these values when doing the computation itself.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一种显示出显著前景的减少这些成本的方法是使用*稀疏性*。稀疏性通常指通过*剪枝*过程在模型权重中引入尽可能多的零，然后在实际计算时跳过这些值。
- en: While much success has been achieved in using sparsity to reduce theoretical
    measures of cost like FLOPs and parameter count, achieving actual speed improvements
    while maintaining acceptable task performance has been much more difficult, particularly
    on general-purpose accelerators such as GPUs using low-precision number formats.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在利用稀疏性减少理论成本指标（如FLOPs和参数计数）方面取得了很大成功，但在保持可接受任务性能的同时实现实际速度提升要困难得多，特别是在使用低精度数字格式的通用加速器（如GPU）上。
- en: 'To help realise these theoretical benefits in practice and enable further investigation
    of sparse techniques on Graphcore IPUs, we introduce PopSparse, a library that
    enables fast sparse operations by leveraging both the unique hardware characteristics
    of IPUs as well as any block structure defined in the data. We target two different
    types of sparsity: static, where the sparsity pattern is fixed at compile-time;
    and dynamic, where it can change each time the model is run. We benchmarked sparse
    dense matrix multiplication for both modes on IPU. Results indicate that the PopSparse
    implementations are faster than dense matrix multiplications on IPU at a range
    of sparsity levels with large matrix size and block size. Furthermore, static
    sparsity in general outperforms dynamic sparsity. While previous work on GPUs
    has shown speedups only for very high sparsity (typically 99% and above), we demonstrate
    that Graphcore’s static sparse implementation outperforms equivalent dense calculations
    in FP16 at lower sparsity (around 90%).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在实践中实现这些理论上的好处，并进一步研究在Graphcore IPUs上的稀疏技术，我们推出了PopSparse，这是一种通过利用IPUs的独特硬件特性以及数据中定义的任何块结构来实现快速稀疏操作的库。我们针对两种不同类型的稀疏性：静态稀疏性，即稀疏模式在编译时固定；和动态稀疏性，即每次运行模型时可以改变。我们在IPU上对这两种模式的稀疏稠密矩阵乘法进行了基准测试。结果表明，PopSparse的实现比IPU上的稠密矩阵乘法在各种稀疏水平、大矩阵尺寸和块尺寸下都更快。此外，静态稀疏性通常优于动态稀疏性。虽然以前在GPU上的研究仅在非常高的稀疏性（通常为99%及以上）下显示出加速效果，但我们展示了Graphcore的静态稀疏实现比在较低稀疏性（约90%）下的等效稠密计算表现更好。
- en: These results were published online on [arXiv](https://arxiv.org/abs/2303.16999)
    and accepted to the [Sparse Neural Network workshop at ICLR 2023](https://www.sparseneural.net/?).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果已在线发布在[arXiv](https://arxiv.org/abs/2303.16999)上，并被[2023年ICLR Sparse Neural
    Network工作坊](https://www.sparseneural.net/?)接受。
- en: '**Graphcore IPU for sparse dense matrix multiplication in deep neural network**'
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**Graphcore IPU用于深度神经网络中的稀疏稠密矩阵乘法**'
- en: The notion of sparsity in deep learning most commonly refers to the idea of
    sparsifying the model weights with the aim of reducing the associated storage
    and compute costs. This is typically done through a single pruning step at the
    end of training (Zhu & Gupta, 2017) or potentially through some sparse training
    regime (Evci et al., 2019). These approaches have typically achieved between 90–99%
    reduction in model weights while maintaining and acceptable level of accuracy
    (Hoefler et al., 2021).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中的稀疏性概念最常指的是通过稀疏化模型权重来减少相关的存储和计算成本。通常，这通过在训练结束时的单次剪枝步骤（Zhu & Gupta, 2017）或通过某些稀疏训练机制（Evci
    et al., 2019）来实现。这些方法通常在保持可接受的准确度水平的同时，实现了模型权重减少90–99%的效果（Hoefler et al., 2021）。
- en: While the benefits to model size can be easily realised, increased computational
    efficiency can often be more difficult to achieve as the resulting unstructured
    sparsity patterns do not align well with the vectorised instruction sets in highly
    parallel deep learning accelerators (Qin et al., 2022). Therefore, one approach
    to improving sparse compute efficiency is to enforce some degree of structure
    on the sparsity pattern. This has motivated a wide range of structured pruning
    techniques such as neuron (Ebrahimi & Klabjan, 2021), channel-wise (He et al.,
    2017), block (Gray et al., 2017), 2:4 (Mishra et al., 2021). However, these methods
    generally incur a task performance penalty compared to equivalent unstructured
    methods.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管模型大小的好处很容易实现，但提高计算效率通常更为困难，因为产生的无结构稀疏模式与高度并行的深度学习加速器中的矢量化指令集不太匹配（Qin 等人，2022）。因此，提高稀疏计算效率的一种方法是对稀疏模式施加一定程度的结构。这促使了广泛的结构化剪枝技术，如神经元（Ebrahimi
    & Klabjan，2021）、通道级（He 等人，2017）、块（Gray 等人，2017）、2:4（Mishra 等人，2021）。然而，这些方法通常会比等效的无结构方法带来性能惩罚。
- en: 'Here we present PopSparse, a library for the effective acceleration of sparse
    dense matrix multiplication (SpMM) on Graphcore IPUs. IPUs have multiple architectural
    features that help accelerate sparse operations:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们介绍 PopSparse，这是一个用于 Graphcore IPU 上稀疏稠密矩阵乘法（SpMM）有效加速的库。IPU 拥有多个架构特性，有助于加速稀疏操作：
- en: Large, high bandwidth on-chip SRAM that improves performance for communication
    heavy, low arithmetic efficiency operations.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型、高带宽的片上 SRAM 可提高通信密集型、低算术效率操作的性能。
- en: Fine-grained MIMD processing model that partitions work over 1472 independent
    compute units (tiles) that enables high degrees of parallelism.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 细粒度 MIMD 处理模型将工作划分到 1472 个独立计算单元（瓦片）上，支持高度的并行性。
- en: Ahead-of-time compilation model that allows further optimisations to be employed
    when sparsity structure is known at compile time.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提前编译模型，当稀疏性结构在编译时已知时，可以采用进一步优化。
- en: '**SpMM in PopSparse library on IPU**'
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**PopSparse 库中的 SpMM 在 IPU 上**'
- en: 'The SpMM can be written as:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: SpMM 可以写作：
- en: '![](../Images/803733b50cd0ffffee604b7f742cf57b.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/803733b50cd0ffffee604b7f742cf57b.png)'
- en: where ⊙ denotes elementwise multiplication, * for inner product, *Y ∈ Rᵐ* ˣ*ⁿ*,
    *X ∈ Rᵏ* ˣ*ⁿ* are dense output and input matrices respectively. The sparse weight
    matrix (*M*⊙*W*) is defined via *M ∈ Bᵐ* ˣ*ᵏ (B = {*0,1*})*, a mask that represents
    the sparsity pattern, itself derived from *M̂* ∈ *B*⁽ᵐᐟᵇ⁾ˣ⁽ᵏᐟᵇ⁾, a block mask
    and *W ∈ Rᵐ* ˣ*ᵏ* defines weight values.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ⊙ 表示元素逐一相乘，* 表示内积，*Y ∈ Rᵐ* ˣ*ⁿ*，*X ∈ Rᵏ* ˣ*ⁿ* 分别是稠密的输出和输入矩阵。稀疏权重矩阵 (*M*⊙*W*)
    通过 *M ∈ Bᵐ* ˣ*ᵏ (B = {*0,1*})* 定义，这是一种表示稀疏性模式的掩码，来自 *M̂* ∈ *B*⁽ᵐᐟᵇ⁾ˣ⁽ᵏᐟᵇ⁾，块掩码和
    *W ∈ Rᵐ* ˣ*ᵏ* 定义权重值。
- en: In this formulation, (*M*⊙*W*) has a block-sparse structure, where contiguous
    square blocks of weights of shape *b* × *b* contain all the non-zero elements,
    given block-size *b*. The dimensions *m* and *k* are referred to as output and
    input feature size and n is referred to as batch size, corresponding to their
    typical role in weight-sparse neural network computation.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种表述中，(*M*⊙*W*) 具有块稀疏结构，其中形状为 *b* × *b* 的连续方形权重块包含所有非零元素，给定块大小 *b*。维度 *m* 和
    *k* 分别称为输出和输入特征大小，而 *n* 被称为批量大小，对应于其在权重稀疏神经网络计算中的典型角色。
- en: 'We define density, *d* = ∑ᵢⱼMᵢⱼ/(*m⋅k*), where the standard term sparsity corresponds
    to *(*1 *— d)*. We use floating point operation (FLOP) count to quantify the amount
    of useful arithmetic work in an SpMM as: 2*⋅m⋅k⋅n⋅d*. Note that this only counts
    operations performed on non-zero values and does not depend on block size *b*.
    The PopSparse library supports unstructured (block size *b* = 1) or structured
    sparsity (*b* = 4, 8, 16).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义密度，*d* = ∑ᵢⱼMᵢⱼ/(*m⋅k*)，其中标准术语稀疏度对应于 *(*1 *— d)*。我们使用浮点运算（FLOP）计数来量化 SpMM
    中有用的算术工作量为：2*⋅m⋅k⋅n⋅d*。注意，这只计算非零值上的操作，并不依赖于块大小 *b*。PopSparse 库支持无结构（块大小 *b* =
    1）或有结构的稀疏性（*b* = 4, 8, 16）。
- en: Static sparsity
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 静态稀疏性
- en: With static sparsity, during compile time, the sparsity pattern and matrix shapes
    are known to the partitioner, which splits the non-zero elements (specific values
    not known yet) of the sparse matrix across *k* dimension into *qₖ* partitions
    and the dense matrix across n dimension into *qₙ* partitions. Figure 1a illustrates
    the distribution, computation and reduction of the sparse matrix in the static
    sparse case. Splits over the k dimension do not have to be evenly sized and are
    chosen to ensure a balanced distribution of the non-zero elements (denoted with
    coloured small blocks in the figure). Once the model is compiled, during execution,
    the specific non-zero values of *W* are provided. As the sparsity pattern is known
    in advance, no extra exchange of data is needed. Local dot product computation
    with the dense matrix is then executed and there is a final reduction across tiles
    to give the dense output matrix.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic sparsity
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With dynamic sparsity, the maximum number of non-zero elements for the sparse
    operand is fixed, while the sparsity pattern is updated during execution. To minimise
    compute cycles, we employ a planner during compile time to optimise the partitioning
    while accommodating for the full range of sparsity patterns that can be defined
    during execution. As the sparsity pattern could change for each run, compared
    to static sparsity, there is an additional step to distribute sparse matrix indices
    and non-zero values by the partitioner. This is demonstrated in Figure 1b. Furthermore,
    as fixed size partitions over m and k dimensions are required, the number of non-zero
    elements in each partition may not be balanced. In Figure 1b the *B* elements
    on the second partition of *k* dimension do not fit on Tile1\. Three of the *B*
    elements overflow to Tile2\. However, the Tile2 does not have all the partition
    information (slices of *m, k, n*) corresponding to B elements to compute the result.
    Consequently, extra steps of propagation are induced to finish the computation.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, our dynamic sparse implementation has several additional costs compared
    to the static case:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Additional control flow after compilation, which incurs some cost overhead.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We must compile for the largest inter-tile communication volume possible and
    thus are less efficient on average.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When data is unbalanced, additional propagation and compute phases are required
    to rebalance the data. The number of steps required is dependent on the degree
    of the imbalance.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/9d7f8e5ea83338795d555ffeee22d8ca.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Illustration of static and dynamic sparse partitioning of the input
    feature dimension, *k*. Image by author.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '**Benchmarks**'
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our micro benchmarking experiments explore a single sparse-dense matmul SpMM:
    *Y=(M*⊙*W)***X* on IPU and GPU. The APIs used for IPU experiments and GPU baselines
    are shown in Table 1\. The range of parameters that were benchmarked is detailed
    in Table 2.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b827a018386682c4aacf6b98a025ea8.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: 'Table 1: APIs used when benchmarking IPU and GPU.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '**IPU**: The operation is executed a single time on a single Bow IPU in a Bow-2000
    chassis, with a randomly generated sparsity pattern and values. We extract cycle
    count information and convert these cycle counts into TFLOP/s values given a constant
    clock of 1.85 GHz. Host transfers are excluded from measured time.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '**GPU**: Baseline sparse implementations on GPU are provided by the cuSPARSE
    library v11.6.2 (NVIDIA, 2022). These are divided according to the sparse format
    used, with no distinction between static and dynamic sparsity patterns. We consider
    compressed sparse row (CSR) and block sparse row (BSR) for unstructured and block
    sparsity, respectively. We generate a random sparsity pattern and values, copy
    to device, execute 25 iterations of the operation and copy results to host for
    validation. We start timing after 5 iterations and measure wall clock time using
    *cudaEventRecord*. All experiments were performed on a single A100 GPU running
    in a 4 x A100-SXM4–40G chassis.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d9b4f3d6e2d0f3bcbf2bb005fec005e.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: 'Table 2: Ranges of parameters swept for benchmarks. Image by author.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: The key results of this work are shown in Figure 2\. These plots show the compute
    FLOP/s (in log scale) vs density with fixed feature size, with each line presenting
    different implementations of the same operation. By construction, the dense implementation
    shows linear scaling with density — since zeros are not counted in the FLOP/s
    calculation. While on the other hand, for a sparse implementation, perfect scaling
    would predict constant FLOP/s as density varies.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/efcdb9f5f8a9a9edd15a098060cf823b.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: IPU FP16 and GPU block-sparse matmul performance as density varies
    for various block size b, square feature size m = k = 4096, best over batch size
    n. Image by author.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: For the dense baselines, while we broadly see chip-for-chip parity between IPU
    and GPU in FP16, it's important to highlight that the IPUs used are approximately
    [half the price in the cloud](https://docs.paperspace.com/gradient/machines/),
    highlighting their cost efficiency for these basic operations. Furthermore, for
    the sparse operations, we see even larger benefits from using IPUs.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, for IPUs (Figure 2a), we see that using static sparse operations
    and block sparsity both individually improve performance. For example, the block
    size 16, dynamic case shows benefits over the dense baseline across all densities
    considered and unstructured (b = 1) static sparsity outperforming dense in the
    low-density regime (<5%). In addition, when both static and block sparsity methods
    are used, we see benefits of as much as 5x in throughput over the dense baseline.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: For the GPU (Figure 2b), we see much less favourable performance with the sparse
    operations, with no case tested outperforming the FP16 dense baseline. However,
    we did find that the FP32 block sparse (BSR) case scaled very well as density
    decreased. It looks possible that for very low densities, this may outperform
    the dense baseline.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the two platforms, we can therefore see that IPUs achieve as much
    as a 5x throughput benefit over the dense GPU baseline and as much as a 25x throughput
    improvement when only sparse operations are considered. With the additional price
    benefits, we believe this presents a compelling case for investigating sparse
    applications on IPUs.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'It should however be noted that even on IPU, the sparse operations only outperform
    the dense baselines under certain conditions. More extensive result can be found
    in [our paper](https://arxiv.org/abs/2303.16999), but as an approximate guide,
    sparsity typically brings speedup over the dense baseline on IPU (FP16, large
    batch size for both) when:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Static sparsity, block size b = 1, features (m = k) ≥ 4096, density d ≤ 1/32.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Static sparsity, block size b ≥ 4: features (m = k) ≥ 4096, density d ≤ 1/8.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic sparsity, block size b ≥ 8, features (m = k) ≥ 4096, density d ≤ 1/32.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Importantly, these ranges are somewhat challenging to use today in the context
    of weight sparse training and inference. This is primarily because finding block
    sparse patterns that do not degrade performance is a hard problem and that, on
    theoretical efficiency metrics (per non-zero FLOP), unstructured sparsity will
    always appear as good or better than block sparsity. We hope that this work shows
    that block sparsity is a promising method for achieving practical acceleration
    of sparse models and will motivate further investigation into effective block
    sparse pruning algorithms in the research community.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Come and [try our PopSparse operations](https://console.paperspace.com/github/graphcore-research/notebooks?container=graphcore%2Fpytorch-jupyter%3A3.2.0-ubuntu-20.04&machine=Free-IPU-POD4&file=%2Fsparsity_benchmarks%2FSpMM.ipynb)
    for yourself on IPUs on Paperspace and then why not try integrating them into
    your own models? If you are actively pursuing research in sparsity that could
    benefit from IPU support, please consider signing up for our [Academic program](https://www.graphcore.ai/academics)
    where we support researchers to achieve great things using IPUs.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Try it yourself
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To try SpMM on IPU — [run on Paperspace](https://console.paperspace.com/github/graphcore-research/notebooks?container=graphcore%2Fpytorch-jupyter%3A3.2.0-ubuntu-20.04&machine=Free-IPU-POD4&file=%2Fsparsity_benchmarks%2FSpMM.ipynb).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '**References**'
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Michael Zhu and Suyog Gupta. To prune, or not to prune: exploring the efficacy
    of pruning for model compression, 2017\. URL [https://arxiv.org/abs/1710.01878.](https://arxiv.org/abs/1710.01878.)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen.
    Rigging the lottery: Making all tickets winners, 2019\. URL [https://arxiv.org/abs/1911.11134.](https://arxiv.org/abs/1911.11134.)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro 和 Erich Elsen.
    改变彩票：让所有票据都成为赢家，2019\. URL [https://arxiv.org/abs/1911.11134.](https://arxiv.org/abs/1911.11134.)'
- en: '[3] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra
    Peste. Sparsity in deep learning: Pruning and growth for efficient inference and
    training in neural networks, 2021\. URL [https://arxiv.org/abs/2102.00554](https://arxiv.org/abs/2102.00554).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden 和 Alexandra Peste.
    深度学习中的稀疏性：神经网络中的剪枝与增长以实现高效推理和训练，2021\. URL [https://arxiv.org/abs/2102.00554](https://arxiv.org/abs/2102.00554).'
- en: '[4] Eric Qin, Raveesh Garg, Abhimanyu Bambhaniya, Michael Pellauer, Angshuman
    Parashar, Sivasankaran Rajamanickam, Cong Hao, and Tushar Krishna. Enabling flexibility
    for sparse tensor acceleration via heterogeneity, 2022\. URL [https://arxiv.org/abs/2201.08916.](https://arxiv.org/abs/2201.08916.)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Eric Qin, Raveesh Garg, Abhimanyu Bambhaniya, Michael Pellauer, Angshuman
    Parashar, Sivasankaran Rajamanickam, Cong Hao 和 Tushar Krishna. 通过异质性实现稀疏张量加速的灵活性，2022\.
    URL [https://arxiv.org/abs/2201.08916.](https://arxiv.org/abs/2201.08916.)'
- en: '[5] Abdolghani Ebrahimi and Diego Klabjan. Neuron-based pruning of deep neural
    networks with better generalization using kronecker factored curvature approximation,
    2021\. URL [https://arxiv.org/abs/2111.08577.](https://arxiv.org/abs/2111.08577.)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Abdolghani Ebrahimi 和 Diego Klabjan. 基于神经元的深度神经网络剪枝，通过克罗内克因子曲率近似实现更好的泛化，2021\.
    URL [https://arxiv.org/abs/2111.08577.](https://arxiv.org/abs/2111.08577.)'
- en: '[6] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating
    very deep neural networks, 2017\. URL [https://arxiv.org/abs/1707.06168.](https://arxiv.org/abs/1707.06168.)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Yihui He, Xiangyu Zhang 和 Jian Sun. 通过通道剪枝加速非常深的神经网络，2017\. URL [https://arxiv.org/abs/1707.06168.](https://arxiv.org/abs/1707.06168.)'
- en: '[7] Scott Gray, Alec Radford, and Diederik P. Kingma. GPU kernels for block-sparse
    weights. 2017.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Scott Gray, Alec Radford 和 Diederik P. Kingma. 用于块稀疏权重的GPU内核。2017.'
- en: '[8] Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic,
    Ganesh Venkatesh, Chong Yu, and Paulius Micikevicius. Accelerating sparse deep
    neural networks, 2021\. URL [https://arxiv.org/abs/2104.08378](https://arxiv.org/abs/2104.08378).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic,
    Ganesh Venkatesh, Chong Yu 和 Paulius Micikevicius. 加速稀疏深度神经网络，2021\. URL [https://arxiv.org/abs/2104.08378](https://arxiv.org/abs/2104.08378).'
- en: '[9] NVIDIA. API reference guide for cuSPARSE, 2022\. URL [https://docs.nvidia.com/cuda/cusparse/](https://docs.nvidia.com/cuda/cusparse/).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] NVIDIA. cuSPARSE的API参考指南，2022\. URL [https://docs.nvidia.com/cuda/cusparse/](https://docs.nvidia.com/cuda/cusparse/).'
