- en: 'Vectorisation: What is it and how does it work?'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量化：是什么以及它是如何工作的？
- en: 原文：[https://towardsdatascience.com/vectorisation-what-is-it-and-how-does-it-work-1dd9cef48407?source=collection_archive---------6-----------------------#2023-04-13](https://towardsdatascience.com/vectorisation-what-is-it-and-how-does-it-work-1dd9cef48407?source=collection_archive---------6-----------------------#2023-04-13)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/vectorisation-what-is-it-and-how-does-it-work-1dd9cef48407?source=collection_archive---------6-----------------------#2023-04-13](https://towardsdatascience.com/vectorisation-what-is-it-and-how-does-it-work-1dd9cef48407?source=collection_archive---------6-----------------------#2023-04-13)
- en: '![](../Images/6640ccc3787bc8c5c4bce0fd8d6a2bd4.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6640ccc3787bc8c5c4bce0fd8d6a2bd4.png)'
- en: Photo by [Mariana Beltrán](https://unsplash.com/ja/@ostranenie?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/greek-columns?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Mariana Beltrán](https://unsplash.com/ja/@ostranenie?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)提供，来自[Unsplash](https://unsplash.com/s/photos/greek-columns?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: O(n) is faster than O(1), cache lines, Pandas 2.0 and the consistent rise of
    the column
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: O(n)比O(1)更快，缓存行，Pandas 2.0和列的持续增长
- en: '[](https://markjamison03.medium.com/?source=post_page-----1dd9cef48407--------------------------------)[![Mark
    Jamison](../Images/11b16af9d84585cff964b6e2b95089cd.png)](https://markjamison03.medium.com/?source=post_page-----1dd9cef48407--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1dd9cef48407--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1dd9cef48407--------------------------------)
    [Mark Jamison](https://markjamison03.medium.com/?source=post_page-----1dd9cef48407--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://markjamison03.medium.com/?source=post_page-----1dd9cef48407--------------------------------)[![Mark
    Jamison](../Images/11b16af9d84585cff964b6e2b95089cd.png)](https://markjamison03.medium.com/?source=post_page-----1dd9cef48407--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1dd9cef48407--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1dd9cef48407--------------------------------)
    [Mark Jamison](https://markjamison03.medium.com/?source=post_page-----1dd9cef48407--------------------------------)'
- en: ·
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F19b7fdef1d09&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvectorisation-what-is-it-and-how-does-it-work-1dd9cef48407&user=Mark+Jamison&userId=19b7fdef1d09&source=post_page-19b7fdef1d09----1dd9cef48407---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1dd9cef48407--------------------------------)
    ·10 min read·Apr 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1dd9cef48407&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvectorisation-what-is-it-and-how-does-it-work-1dd9cef48407&user=Mark+Jamison&userId=19b7fdef1d09&source=-----1dd9cef48407---------------------clap_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F19b7fdef1d09&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvectorisation-what-is-it-and-how-does-it-work-1dd9cef48407&user=Mark+Jamison&userId=19b7fdef1d09&source=post_page-19b7fdef1d09----1dd9cef48407---------------------post_header-----------)
    发布在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1dd9cef48407--------------------------------)
    ·10 min read·Apr 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1dd9cef48407&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvectorisation-what-is-it-and-how-does-it-work-1dd9cef48407&user=Mark+Jamison&userId=19b7fdef1d09&source=-----1dd9cef48407---------------------clap_footer-----------)'
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1dd9cef48407&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvectorisation-what-is-it-and-how-does-it-work-1dd9cef48407&source=-----1dd9cef48407---------------------bookmark_footer-----------)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1dd9cef48407&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvectorisation-what-is-it-and-how-does-it-work-1dd9cef48407&source=-----1dd9cef48407---------------------bookmark_footer-----------)'
- en: This is the 2nd iteration of this article. After finishing the 1st iteration,
    I left it to stew and edit as the headlines didn’t look good — a 13 min ramble
    about vectorisation with some loose links to database theory and historical trends.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这是本文的第二次迭代。在完成第一次迭代后，我让它静置一段时间进行编辑，因为标题看起来不太好——一个关于向量化的13分钟长篇，其中包含与数据库理论和历史趋势的松散联系。
- en: 'While waiting to re-draft, I came across several [performance comparisons](/measuring-the-speed-of-new-pandas-2-0-against-polars-and-datatable-still-not-good-enough-e44dc78f6585)
    of the new much hyped pandas 2.0 release — especially comparing it to Polars.
    At this point I must come clean — Pandas is Ground Zero for me and I’m yet to
    even `pip install` Polars for testing. I''m always hesitant to swap out a well
    known and supported package for anything new or in vogue until either:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在等待重新草拟时，我发现了几个关于新版本 Pandas 2.0 的[性能比较](/measuring-the-speed-of-new-pandas-2-0-against-polars-and-datatable-still-not-good-enough-e44dc78f6585)——尤其是与
    Polars 的比较。此时我必须坦白——Pandas 对我而言是起点，我甚至还没有 `pip install` Polars 进行测试。我总是犹豫在尚未得到广泛支持的新工具和流行工具之间进行替换，直到：
- en: the existing tool is starting to fail (I’m not using large enough data post
    SQL-aggregation)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现有工具开始失败（我在 SQL 聚合后使用的数据不够大）。
- en: there is some other clear compelling evidence for stomaching the adoption
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一些其他明显有力的证据支持采用。
- en: However the relatively below par performance of the new Pandas 2.0 release did
    have me wondering — if Polars is so fast for in-memory operation, how is it achieving
    this?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，新版 Pandas 2.0 性能相对较差确实让我产生了疑问——如果 Polars 在内存操作中如此快速，它是如何实现的？
- en: The Polars author wrote my original article (but better)
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Polars 的作者写了我原来的文章（但更好）。
- en: Vectorisation. Polars is fast because the original author designed the whole
    thing to be about vectorisation. In [this ‘hello world’ post](https://www.pola.rs/posts/i-wrote-one-of-the-fastest-dataframe-libraries/)
    from Polars author Ritchie Vink he explains using a combination of clear concise
    sentences and simple visuals how Polars achieves what it achieves — because it
    is not just built with ideas of vectorisation in mind, but built *completely*
    around those principles.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 向量化。Polars 之所以快速，是因为原作者设计了整个系统以向量化为核心。在[这篇‘hello world’文章](https://www.pola.rs/posts/i-wrote-one-of-the-fastest-dataframe-libraries/)中，Polars
    作者 Ritchie Vink 通过清晰简洁的语言和简单的视觉效果解释了 Polars 如何实现其目标——因为它不仅仅是以向量化为理念进行构建，而是*完全*围绕这些原则构建的。
- en: The aim for the rest of this article isn’t necessarily to rehash what has been
    said there, but instead to work back through some ideas and historical context
    to shed light on how we have come to column (or array) based computing being more
    ‘mainstream’ and how that has started to perforate the modern day data science
    tool kit in python.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章的其余部分并非仅仅是重述那些内容，而是通过回顾一些想法和历史背景，阐明我们如何将基于列（或数组）的计算变得更加‘主流’，以及这如何开始渗透到现代数据科学工具包中。
- en: “I don’t care what your fancy data structure is but I know that an array will
    beat it”
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: “我不在乎你的花哨数据结构是什么，但我知道数组会胜过它。”
- en: The above is from [this talk](https://www.youtube.com/watch?v=WDIkqP4JbkE) by
    Scott Meyers where he attributes the above quote to the CTO of an algorithmic
    trading company championing an array. It’s also eluded to in the Polars article
    but the idea is the same — when it comes to the actual practicalities, sometimes
    you need to chuck your basic [time complexity analysis](https://en.wikipedia.org/wiki/Time_complexity)
    out the window because an O(n) algorithm can trump an O(1) algorithm.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 上述内容出自[这次讲座](https://www.youtube.com/watch?v=WDIkqP4JbkE)由 Scott Meyers 主讲，他将上述引言归于一位支持数组的算法交易公司
    CTO。这个想法在 Polars 文章中也有所提及，但概念相同——在实际应用中，有时你需要抛弃基本的[时间复杂度分析](https://en.wikipedia.org/wiki/Time_complexity)，因为
    O(n) 算法可以超越 O(1) 算法。
- en: 'I come from a non computer science background, but the wealth of material that
    universities (particularly American) make available online means I’ve been able
    to take some of the basic ‘Algorithms’ and ‘Data Structures’ courses. From what
    I can see, the joint aim could be (probably badly) summarised as:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我来自非计算机科学背景，但大学（特别是美国大学）提供的大量在线材料使我能够学习一些基础的‘算法’和‘数据结构’课程。根据我所见，联合目标可以（可能总结得很糟糕）如下：
- en: use logic to come up with a process that involves the least amount of steps
    (algorithms)
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用逻辑来提出一个涉及最少步骤（算法）的过程。
- en: organise your data to enable the selection of an algorithm with minimum steps
    (data structures)
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组织你的数据，以便选择具有最小步骤的算法（数据结构）。
- en: 'The importance of understanding the core concepts of both is underscored by
    the legions of companies out there barraging their hopefuls with reams of [Leetcode](https://leetcode.com/)
    questions on Dynamic Programming or Binary Search Trees or any other generally
    day-to-day irrelevant topic. One such example is usually this — when it comes
    to looking stuff up use a hash map rather than an array. This is because:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 理解两者核心概念的重要性在于众多公司用大量的[Leetcode](https://leetcode.com/)动态规划或二叉搜索树等与日常无关的题目来考察他们的候选人。一个典型的例子是——在查找东西时使用哈希映射而不是数组。这是因为：
- en: a hash map is O(1) to lookup
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哈希映射的查找是O(1)
- en: an (unsorted) array is O(n) as you potentially need to traverse the whole thing
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （未排序的）数组是O(n)，因为你可能需要遍历整个数组
- en: Why doesn’t the above always hold? Because the statement *“an O(1) algorithm
    is faster than an O(n) algorithm”* is incomplete. The true statement should be
    more like *“an O(1) algorithm is faster than an O(n) algorithm,* ***conditional
    on the same starting point****”*.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么上述情况并不总是成立？因为*“O(1)算法比O(n)算法更快”*是不完整的。真正的说法应该是*“O(1)算法比O(n)算法更快，***前提是起始点相同****”*。
- en: Driving a Ferrari in New York
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在纽约驾驶法拉利
- en: Modern day CPUs have been described as similar to ‘driving a Ferrari in New
    York’. Needless to say the analogy has clearly come from a computer scientist
    who believes the only reason to drive a car is to get from A to B, but the point
    remains. Why do you need such a fast car if all you’ll be doing is continuously
    stopping and starting (although stopping and starting *really really* fast)?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现代CPU被描述为类似于在纽约驾驶一辆法拉利。毋庸置疑，这个比喻显然来自于一个认为开车唯一目的就是从A点到B点的计算机科学家，但这一点依然成立。如果你只是不断地停车和启动（尽管停车和启动*非常非常*快），为什么需要这样一辆快车呢？
- en: 'Substitute ‘car’ for ‘processor’ and we are looking at modern day CPUs. This
    has come about due **to the relative speed improvements that have been and are
    continuing to be made (although at a much slower rate these days) in processor
    speeds vs memory speeds.** This is illustrated beautifully in this [C++ orientated
    talk](https://www.youtube.com/watch?v=L7zSU9HI-6I) by Herb Sutter (start at 12:00
    for around 20mins if you want to just strip the overview) and the below chart
    shows it well:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 将‘车’替换为‘处理器’，我们就看到了现代CPU。这是因为**处理器速度与内存速度之间的相对速度改进（尽管近年来速度提升变得较慢）**。这一点在Herb
    Sutter的[C++导向讲座](https://www.youtube.com/watch?v=L7zSU9HI-6I)中得到了很好的阐述（如果你只想了解概述，可以从12:00开始观看约20分钟），下面的图表也很好地展示了这一点：
- en: '![](../Images/db368b943bd050b6f26d24c03505fa1a.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db368b943bd050b6f26d24c03505fa1a.png)'
- en: 'Opportunities and choice in a new vector era — Scientific Figure on ResearchGate.
    Available from: [https://www.researchgate.net/figure/CPU-memory-performance-gap-Modelled-after-Computer-Architecture-Hennessy-John-L_fig1_273029990](https://www.researchgate.net/figure/CPU-memory-performance-gap-Modelled-after-Computer-Architecture-Hennessy-John-L_fig1_273029990)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 新的向量时代的机会和选择——[ResearchGate上的科学图](https://www.researchgate.net/figure/CPU-memory-performance-gap-Modelled-after-Computer-Architecture-Hennessy-John-L_fig1_273029990)
- en: In simple terms we have a “[Moore’s Law](https://en.wikipedia.org/wiki/Moore's_law)
    induced” widening gap between how fast our processor can operate on stuff and
    how fast we can get that stuff to our processor. **There’s no need to have such
    a fast processor if it’s sitting idle most of the time waiting for new data to
    operate on**.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，我们有一个“[摩尔定律](https://en.wikipedia.org/wiki/Moore's_law)引发的”处理器操作速度和将数据送到处理器的速度之间的差距。**如果处理器大部分时间都在空闲等待新数据进行操作，那么就没有必要拥有如此快速的处理器**。
- en: How do we circumvent this? Caching, caching and more caching
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们如何规避这个问题？缓存、缓存和更多缓存
- en: 'Below is the standard image that many people have of a computer’s architecture
    ([skip to 22:30 of Herb Sutter’s talk for a comedic example of this](https://www.youtube.com/watch?v=L7zSU9HI-6I)):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是许多人对计算机架构的标准印象（[跳到Herb Sutter讲座的22:30，观看这个有趣的例子](https://www.youtube.com/watch?v=L7zSU9HI-6I)）：
- en: '![](../Images/885295d81bde4bb243ac8c198ef4fe31.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/885295d81bde4bb243ac8c198ef4fe31.png)'
- en: Image by author
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'In other words — we have:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说——我们有：
- en: our CPU that does stuff
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的CPU执行任务
- en: our memory (or RAM) that is ‘quick’ to access
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的内存（或RAM）‘快速’访问
- en: our disk which is slow to access
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的磁盘访问速度慢
- en: 'Now as the above graph showed, memory might be fast *relative* to retrieving
    data form disk — but retrieval certainly isn’t fast relative to the data we can
    process with our current quantity and speed of processors. To solve this, hardware
    developers decided to put memory *onto* the CPU (or ‘die’). **These are known
    as caches and there are several per each processing core.** Each core consists
    of:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，内存可能相对于从硬盘检索数据是快的——但相对于我们当前处理器的数量和速度，检索速度确实不快。为了解决这个问题，硬件开发者决定将内存*放置到*CPU（或‘芯片’）上。**这些被称为缓存，每个处理核心都有多个。**
    每个核心包括：
- en: 'an L1 cache: this is the fastest and is split into an instruction cache (storing
    the instructions that your code boils down to) and a data cache (stores variables
    that you are operating on)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L1缓存：这是最快的，并分为指令缓存（存储你代码转化成的指令）和数据缓存（存储你操作的变量）
- en: 'an L2 cache: larger than L1 but slower'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L2缓存：比L1大，但较慢
- en: and then all cores on a machine share an L3 cache — again larger than both L1
    and L2 but slower again.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，机器上的所有核心共享一个L3缓存——同样比L1和L2大，但再次较慢。
- en: 'The below image, based on a blown up photo from an Intel white paper ([The
    Basics of Intel Architecture (v.1, Jan. 2014)](https://www.intel.co.uk/content/www/uk/en/intelligent-systems/embedded-systems-training/ia-introduction-basics-paper.html?wapkw=white+paper+introduction+to+intel+architecture)),
    shows how a 4 core CPU is laid out for Intel’s i7 processor — L1 and L2 caches
    are *within* each ‘Core’ section:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 下图基于来自Intel白皮书的放大照片（[Intel架构基础知识（v.1, Jan. 2014）](https://www.intel.co.uk/content/www/uk/en/intelligent-systems/embedded-systems-training/ia-introduction-basics-paper.html?wapkw=white+paper+introduction+to+intel+architecture)），展示了Intel
    i7处理器的4核心CPU布局——L1和L2缓存*在*每个‘核心’部分内：
- en: '![](../Images/0f2b8d6263ae9dc32c2145d118bfb975.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f2b8d6263ae9dc32c2145d118bfb975.png)'
- en: 'Image by author — original image in this document: [The Basics of Intel Architecture
    (v.1, Jan. 2014)](https://www.intel.co.uk/content/www/uk/en/intelligent-systems/embedded-systems-training/ia-introduction-basics-paper.html?wapkw=white+paper+introduction+to+intel+architecture)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供——原图见此文档：[Intel架构基础知识（v.1, Jan. 2014）](https://www.intel.co.uk/content/www/uk/en/intelligent-systems/embedded-systems-training/ia-introduction-basics-paper.html?wapkw=white+paper+introduction+to+intel+architecture)
- en: But the same applies for more up to date PCs. I’m writing this on a Macbook
    Air M2 and a [quick lookup on Wikipedia](https://en.wikipedia.org/wiki/Apple_M2)
    indicates that the M2 chips contain L1, L2 and a shared ‘last level cache’ (L3).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 但这同样适用于更新的PC。我在Macbook Air M2上写这篇文章， [在维基百科上快速查阅](https://en.wikipedia.org/wiki/Apple_M2)表明M2芯片包含L1、L2和一个共享的‘最后一级缓存’（L3）。
- en: So how much faster is retrieving data from cache than memory?
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 那么，从缓存中检索数据比从内存中快多少呢？
- en: 'For this — a picture is worth a thousand words — or more specifically, an animation.
    The following link from gaming software optimisation company Overbyte shows just
    how drastic the relative performance is: [http://www.overbyte.com.au/misc/Lesson3/CacheFun.html](http://www.overbyte.com.au/misc/Lesson3/CacheFun.html).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对此——一图胜千言——或者更具体地说，是一个动画。来自游戏软件优化公司Overbyte的以下链接展示了相对性能的极端差异：[http://www.overbyte.com.au/misc/Lesson3/CacheFun.html](http://www.overbyte.com.au/misc/Lesson3/CacheFun.html)。
- en: 'Speeds vary between machines but — measuring time in the standard [‘clock cycle’](https://en.wikipedia.org/wiki/Clock_rate)
    unit we have roughly:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 速度在机器之间有所不同，但——以标准的[‘时钟周期’](https://en.wikipedia.org/wiki/Clock_rate)单位来衡量，我们大致有：
- en: 'L1: ~1–3x clock cycles'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L1：~1–3x时钟周期
- en: 'L2: ~10x clock cycles'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L2：~10x时钟周期
- en: 'L3: ~40x clock cycles'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L3：~40x时钟周期
- en: 'Main memory: ~100–300x clock cycles'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主内存：~100–300x时钟周期
- en: 'In other words: if our data is in L1 memory our computer will process it between
    30–300x faster than if we fetch it from main memory. Main memory being RAM, not
    disk. When your CPU can’t find the data in the L1 cache, it searches L2, then
    L3 and then it goes to main memory **with each of these failed searches labelled
    a ‘cache miss’.** The fewer cache misses you have, the faster your code.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说：如果我们的数据在L1内存中，我们的计算机会比从主内存中提取数据快30–300倍。主内存指的是RAM，而非硬盘。当你的CPU在L1缓存中找不到数据时，它会搜索L2，然后是L3，最后到主内存**每次这些搜索失败都会标记为‘缓存未命中’。**
    缓存未命中越少，你的代码执行越快。
- en: So how do I get my data into the caches? Cache Lines
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 那么，我如何将数据加载到缓存中？缓存行
- en: Your CPU does this for you. Based on your code which is translated into the
    instruction set (lowest level of commands — even assembly is assembled into [instructions](https://en.wikipedia.org/wiki/Instruction_set_architecture)),
    your CPU loads data into the caches. It then operates on it and stores the result
    in the cache.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 你的 CPU 为你完成了这一切。基于你的代码，这些代码被翻译成指令集（最低级的命令——即使汇编也被汇编成[指令](https://en.wikipedia.org/wiki/Instruction_set_architecture)），你的
    CPU 将数据加载到缓存中。然后它对数据进行操作并将结果存储在缓存中。
- en: 'In particular, if you define the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，如果你定义以下内容：
- en: '[PRE0]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: your CPU doesn’t just load into the caches the integers `x`and `y` — **it actually
    loads ‘cache lines’.** [Cache lines](https://en.algorithmica.org/hpc/cpu-cache/cache-lines/)
    are the lowest level of data unit that a CPU works with. A cache line will include
    the data you need, but also the data *around* that in memory that make up the
    rest of the cache line — generally a 64 byte contiguous block of memory.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你的 CPU 不仅仅是将整数 `x` 和 `y` 加载到缓存中——**它实际上加载的是‘缓存行’。** [缓存行](https://en.algorithmica.org/hpc/cpu-cache/cache-lines/)
    是 CPU 处理数据的最底层单位。一个缓存行将包含你需要的数据，但也包括内存中*周围*的其他数据，这些数据构成了整个缓存行——通常是一个 64 字节的连续内存块。
- en: 'Not only that, but CPUs are built to do clever things to optimise this data
    fetching from memory. Why? Because it’s slow — so the earlier we can load this
    data into the cache to be operated on the sooner the bottleneck is fixed between:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅如此，CPU 还被设计来进行聪明的数据优化提取。为什么？因为这很慢——所以我们越早将数据加载到缓存中进行操作，就越早解决数据存储便宜而处理器快速之间的瓶颈。
- en: the speed the processor can operate on the data
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理器对数据进行操作的速度
- en: the time it takes to make that data available to the processor — in the caches
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据提供给处理器所需的时间——在缓存中
- en: To do this CPUs implement things like [pre-fetching](https://en.wikipedia.org/wiki/Cache_prefetching)
    — which means identifying patterns in your program’s memory access and predicting
    which memory you will use next.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，CPU 实现了类似于[预取](https://en.wikipedia.org/wiki/Cache_prefetching)的功能——这意味着识别程序内存访问中的模式，并预测你将会使用哪些内存。
- en: Recap
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: Probably the best way to recap is to answer (at last) our original question
    — what is vectorisation?
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 可能最好的总结方式是回答（最后）我们最初的问题——什么是向量化？
- en: vectorisation is getting the most out of your super fast processor
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量化是充分利用你超快处理器的手段
- en: it does this by organising your data into predictable contiguous chunks (arrays)
    that can be loaded together (cache lines) and pre-fetched
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过将数据组织成可预测的连续块（数组）来完成这一点，这些数据块可以一起加载（缓存行）并进行预取
- en: this prevents your CPU twiddling its thumbs while you wait to load data from
    L2/L3 cache [or worse](https://www.youtube.com/watch?v=HQ47glxcxr0) — main memory
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这可以防止你的 CPU 在等待从 L2/L3 缓存[或更糟](https://www.youtube.com/watch?v=HQ47glxcxr0)——主内存中加载数据时无所事事
- en: But why now? If this seems so obvious, why wasn’t this the foundation always?
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 但为什么是现在？如果这看起来如此明显，为什么这不是一直以来的基础？
- en: What’s changed? Relative historical advancement
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发生了什么变化？相对的历史进展
- en: 'It’s not that the previous incarnation of computer scientists were stupid (quite
    the opposite) and missed this glaringly obvious idea. But instead that we are
    now developing the tool kit that best matches both:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的计算机科学家并不是愚蠢的（正好相反）而错过了这个显而易见的想法。相反，我们现在正在开发最适合的工具包：
- en: our current hardware landscape
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们当前的硬件格局
- en: our current use cases
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们当前的使用案例
- en: 'Due to historical advances (the above Moore’s Law chart) we are in a situation
    where:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 由于历史进展（上面的摩尔定律图表），我们正处于这样的情况：
- en: data storage is cheap and processors are rapid
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据存储便宜而处理器迅速
- en: we have widespread use cases for data analytics to drive decision making (either
    automated or not)
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有广泛的数据分析使用案例来推动决策制定（无论是自动化还是非自动化）
- en: As a result, due to *relative* advances we now have a situation where there
    is a bottleneck — getting all that data into our super fast processors. And for
    clarity, this problem isn’t new. As the above graph shows it’s been a widening
    problem for years but has been one that we’ve applied the sticking plasters of
    L1/L2 caches to; because that meant most people didn’t have to care about this.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，由于*相对*的进步，我们现在面临一个瓶颈——将所有这些数据输入到我们超快的处理器中。而且为了清楚起见，这个问题并不新鲜。如上图所示，这已经是一个多年来逐渐扩大的问题，但我们应用了
    L1/L2 缓存的权宜之计；因为这意味着大多数人不必担心这个问题。
- en: However now the gap is *so* wide and the data sizes are growing that the problem
    needs solved closer to source. **In other words, if you want your data to be operated
    on at lightning speed, then don’t shove the work down to the CPU designers but
    do it yourself and store your data in arrays.**
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 但是现在差距 *如此* 巨大，数据规模也在不断增长，以至于问题需要在更接近源头的地方解决。**换句话说，如果你想让你的数据以闪电般的速度被操作，那么不要把工作推给
    CPU 设计师，而是自己动手，并将数据存储在数组中。**
- en: Kdb+, Dremel & BigQuery, PyArrow
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kdb+、Dremel 和 BigQuery，PyArrow
- en: 'In general, when we work with data we work with it in 2 flavours:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，当我们处理数据时，通常有两种方式：
- en: in memory
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在内存中
- en: on disk
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在磁盘上
- en: For me that just means whether or not it’s generally in a data *base* or in
    a data *frame*. Polars, and more recently Pandas 2.0, aren’t necessarily doing
    something completely new, but more redesigning our in-memory representations of
    data to be closer to how they are generally stored on disk.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对我来说，这仅仅意味着数据是否一般存储在一个数据 *库* 或者数据 *框架* 中。Polars，以及最近的 Pandas 2.0，未必做了完全新的事情，而是更多地重新设计了我们内存中数据的表示方式，使其更接近于在磁盘上通常的存储方式。
- en: Why? Because we’ve made significant advancements in how to store large swathes
    of data in ways we can rapidly filter and aggregate on disk — so why not represent
    our data in memory in such a way we can leverage those advancements? We might
    as well take the ideas that have driven technologies like [Kdb+](https://code.kx.com/q/)
    and implement them in the way we store our in memory data.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么？因为我们在如何以可以快速过滤和汇总的数据存储方式上取得了显著进展——那么为什么不在内存中以能够利用这些进展的方式来表示数据呢？我们完全可以采纳像
    [Kdb+](https://code.kx.com/q/) 这样的技术驱动思想，并将其实现于内存数据存储方式中。
- en: Moving toward a consistent approach
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迈向一致的方法
- en: Polars is based on PyArrow — a python implementation of the [Apache Arrow in-memory
    columnar data format](https://arrow.apache.org/). So too is the [new Pandas 2.0](https://pandas.pydata.org/docs/user_guide/pyarrow.html#pyarrow-functionality).
    PyArrow works especially well with loading in data from disk in [Apache Parquet](https://parquet.apache.org/)
    format. Parquet is basically the column format described in the original [Dremel
    paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36632.pdf).
    [Dremel](http://www.goldsborough.me/distributed-systems/2019/05/18/21-09-00-a_look_at_dremel/)
    is the analysis engine that underlies Google’s Big Query.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Polars 基于 PyArrow——这是 [Apache Arrow 内存列式数据格式](https://arrow.apache.org/) 的 Python
    实现。新版本的 [Pandas 2.0](https://pandas.pydata.org/docs/user_guide/pyarrow.html#pyarrow-functionality)
    也是如此。PyArrow 在从磁盘加载 [Apache Parquet](https://parquet.apache.org/) 格式的数据时表现特别好。Parquet
    基本上是原始 [Dremel 论文](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36632.pdf)
    中描述的列格式。[Dremel](http://www.goldsborough.me/distributed-systems/2019/05/18/21-09-00-a_look_at_dremel/)
    是支撑 Google Big Query 的分析引擎。
- en: 'The point being: these are all interlinked concepts with the latest iteration
    of in-memory data science tools not necessarily a step change, but more a move
    toward an increasingly consistent idea that underlies our data analysis tools.
    We store data for numerical computation in arrays/columns — **both on disk *and*
    in memory**.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 要点是：这些都是相互关联的概念，最新的内存数据科学工具并不一定是一个彻底的变化，而是朝着一个日益一致的理念迈进，这个理念支撑着我们的数据分析工具。我们将数据存储在数组/列中——**既在磁盘上
    *也* 在内存中**。
- en: Why? Because that’s the best way to do things given the hardware landscape that
    has developed due to relative speed improvements between processing, cost of data
    storage and speed of data retrieval from memory.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么？因为这是考虑到处理硬件、数据存储成本和内存数据检索速度之间的相对速度改进后，做事的最佳方式。
- en: 'Conclusion: Wes McKinney’s CV'
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论：Wes McKinney 的简历
- en: It might seem like an odd topic to conclude with, but the above trajectory toward
    a language agnostic consistent approach to data manipulation and modelling can
    be viewed through the lens of the Pandas creator’s career. He started off at [AQR
    Capital Management](https://www.aqr.com/) wrangling with data in spreadsheets,
    then created and popularised Pandas (utilising NumPy’s vector-friendly [ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html))
    and now he is [heavily involved with Apache Arrow](https://www.youtube.com/watch?v=Hqi_Bw_0y8Q).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来是一个奇怪的结论话题，但通过 Pandas 创始人的职业生涯可以看出上述向语言无关、一致的数据操作和建模方法的轨迹。他最初在 [AQR Capital
    Management](https://www.aqr.com/) 使用电子表格处理数据，然后创建并推广了 Pandas（利用 NumPy 的向量友好型 [ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html)），现在他
    [深度参与 Apache Arrow](https://www.youtube.com/watch?v=Hqi_Bw_0y8Q)。
- en: His career has moved hand in hand with how the modern day ‘data science stack’
    (at least in python) has started to move toward a more column orientated in memory
    data representation. It seems like the championing of the array is set to continue
    and I personally don’t see it slowing down. Pardon the pun.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 他的职业生涯与现代“数据科学堆栈”（至少在Python中）朝着更加面向列的内存数据表示方式的演变紧密相连。看起来数组的推广势不可挡，并且我个人认为这种趋势不会放缓。请原谅这句双关语。
