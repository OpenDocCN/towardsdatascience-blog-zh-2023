["```py\nclass Graph:\n    def __init__(\n        self, molecule_smiles: str,\n        node_vec_len: int,\n        max_atoms: int = None\n      ):\n        # Store properties\n        self.smiles = molecule_smiles\n        self.node_vec_len = node_vec_len\n        self.max_atoms = max_atoms\n\n        # Call helper function to convert SMILES to RDKit mol\n        self.smiles_to_mol()\n\n        # If valid mol is created, generate a graph of the mol\n        if self.mol is not None:\n            self.smiles_to_graph()\n\n    def smiles_to_mol(self):\n        # Use MolFromSmiles from RDKit to get molecule object\n        mol = Chem.MolFromSmiles(self.smiles)\n\n        # If a valid mol is not returned, set mol as None and exit\n        if mol is None:\n            self.mol = None\n            return\n\n        # Add hydrogens to molecule\n        self.mol = Chem.AddHs(mol)\n\n    def smiles_to_graph(self):\n        # Get list of atoms in molecule\n        atoms = self.mol.GetAtoms()\n\n        # If max_atoms is not provided, max_atoms is equal to maximum number \n        # of atoms in this molecule. \n        if self.max_atoms is None:\n            n_atoms = len(list(atoms))\n        else:\n            n_atoms = self.max_atoms\n\n        # Create empty node matrix\n        node_mat = np.zeros((n_atoms, self.node_vec_len))\n\n        # Iterate over atoms and add to node matrix\n        for atom in atoms:\n            # Get atom index and atomic number\n            atom_index = atom.GetIdx()\n            atom_no = atom.GetAtomicNum()\n\n            # Assign to node matrix\n            node_mat[atom_index, atom_no] = 1\n\n        # Get adjacency matrix using RDKit\n        adj_mat = rdmolops.GetAdjacencyMatrix(self.mol)\n        self.std_adj_mat = np.copy(adj_mat)\n\n        # Get distance matrix using RDKit\n        dist_mat = molDG.GetMoleculeBoundsMatrix(self.mol)\n        dist_mat[dist_mat == 0.] = 1\n\n        # Get modified adjacency matrix with inverse bond lengths\n        adj_mat = adj_mat * (1 / dist_mat)\n\n        # Pad the adjacency matrix with 0s\n        dim_add = n_atoms - adj_mat.shape[0]\n        adj_mat = np.pad(\n            adj_mat, pad_width=((0, dim_add), (0, dim_add)), mode=\"constant\"\n        )\n\n        # Add an identity matrix to adjacency matrix\n        # This will make an atom its own neighbor\n        adj_mat = adj_mat + np.eye(n_atoms)\n\n        # Save both matrices\n        self.node_mat = node_mat\n        self.adj_mat = adj_mat\n```", "```py\nclass GraphData(Dataset):\n    def __init__(self, dataset_path: str, node_vec_len: int, max_atoms: int):\n        # Save attributes\n        self.node_vec_len = node_vec_len\n        self.max_atoms = max_atoms\n\n        # Open dataset file\n        df = pd.read_csv(dataset_path)\n\n        # Create lists\n        self.indices = df.index.to_list()\n        self.smiles = df[\"smiles\"].to_list()\n        self.outputs = df[\"measured log solubility in mols per litre\"].to_list()\n\n    def __len__(self):\n        return len(self.indices)\n\n    def __getitem__(self, i: int):\n        # Get smile\n        smile = self.smiles[i]\n\n        # Create MolGraph object using the Graph abstraction\n        mol = Graph(smile, self.node_vec_len, self.max_atoms)\n\n        # Get node and adjacency matrices\n        node_mat = torch.Tensor(mol.node_mat)\n        adj_mat = torch.Tensor(mol.adj_mat)\n\n        # Get output\n        output = torch.Tensor([self.outputs[i]])\n\n        return (node_mat, adj_mat), output, smile\n```", "```py\ndef collate_graph_dataset(dataset: Dataset):\n    # Create empty lists of node and adjacency matrices, outputs, and smiles\n    node_mats = []\n    adj_mats = []\n    outputs = []\n    smiles = []\n\n    # Iterate over list and assign each component to the correct list\n    for i in range(len(dataset)):\n        (node_mat,adj_mat), output, smile = dataset[i]\n        node_mats.append(node_mat)\n        adj_mats.append(adj_mat)\n        outputs.append(output)\n        smiles.append(smile)\n\n    # Create tensors\n    node_mats_tensor = torch.cat(node_mats, dim=0)\n    adj_mats_tensor = torch.cat(adj_mats, dim=0)\n    outputs_tensor = torch.stack(outputs, dim=0)\n\n    # Return tensors\n    return (node_mats_tensor, adj_mats_tensor), outputs_tensor, smiles\n```", "```py\nclass ConvolutionLayer(nn.Module):\n    def __init__(self, node_in_len: int, node_out_len: int):\n        # Call constructor of base class\n        super().__init__()\n\n        # Create linear layer for node matrix\n        self.conv_linear = nn.Linear(node_in_len, node_out_len)\n\n        # Create activation function\n        self.conv_activation = nn.LeakyReLU()\n\n    def forward(self, node_mat, adj_mat):\n        # Calculate number of neighbors\n        n_neighbors = adj_mat.sum(dim=-1, keepdims=True)\n        # Create identity tensor\n        self.idx_mat = torch.eye(\n            adj_mat.shape[-2], adj_mat.shape[-1], device=n_neighbors.device\n        )\n        # Add new (batch) dimension and expand\n        idx_mat = self.idx_mat.unsqueeze(0).expand(*adj_mat.shape)\n        # Get inverse degree matrix\n        inv_degree_mat = torch.mul(idx_mat, 1 / n_neighbors)\n\n        # Perform matrix multiplication: D^(-1)AN\n        node_fea = torch.bmm(inv_degree_mat, adj_mat)\n        node_fea = torch.bmm(node_fea, node_mat)\n\n        # Perform linear transformation to node features \n        # (multiplication with W)\n        node_fea = self.conv_linear(node_fea)\n\n        # Apply activation\n        node_fea = self.conv_activation(node_fea)\n\n        return node_fea\n```", "```py\nclass PoolingLayer(nn.Module):\n    def __init__(self):\n        # Call constructor of base class\n        super().__init__()\n\n    def forward(self, node_fea):\n        # Pool the node matrix\n        pooled_node_fea = node_fea.mean(dim=1)\n        return pooled_node_fea\n```", "```py\nclass ChemGCN(nn.Module):\n    def __init__(\n        self,\n        node_vec_len: int,\n        node_fea_len: int,\n        hidden_fea_len: int,\n        n_conv: int,\n        n_hidden: int,\n        n_outputs: int,\n        p_dropout: float = 0.0,\n    ):\n        # Call constructor of base class\n        super().__init__()\n\n        # Define layers\n        # Initial transformation from node matrix to node features\n        self.init_transform = nn.Linear(node_vec_len, node_fea_len)\n\n        # Convolution layers\n        self.conv_layers = nn.ModuleList(\n            [\n                ConvolutionLayer(\n                    node_in_len=node_fea_len,\n                    node_out_len=node_fea_len,\n                )\n                for i in range(n_conv)\n            ]\n        )\n\n        # Pool convolution outputs\n        self.pooling = PoolingLayer()\n        pooled_node_fea_len = node_fea_len\n\n        # Pooling activation\n        self.pooling_activation = nn.LeakyReLU()\n\n        # From pooled vector to hidden layers\n        self.pooled_to_hidden = nn.Linear(pooled_node_fea_len, hidden_fea_len)\n\n        # Hidden layer\n        self.hidden_layer = nn.Linear(hidden_fea_len, hidden_fea_len)\n\n        # Hidden layer activation function\n        self.hidden_activation = nn.LeakyReLU()\n\n        # Hidden layer dropout\n        self.dropout = nn.Dropout(p=p_dropout)\n\n        # If hidden layers more than 1, add more hidden layers\n        self.n_hidden = n_hidden\n        if self.n_hidden > 1:\n            self.hidden_layers = nn.ModuleList(\n                [self.hidden_layer for _ in range(n_hidden - 1)]\n            )\n            self.hidden_activation_layers = nn.ModuleList(\n                [self.hidden_activation for _ in range(n_hidden - 1)]\n            )\n            self.hidden_dropout_layers = nn.ModuleList(\n                [self.dropout for _ in range(n_hidden - 1)]\n            )\n\n        # Final layer going to the output\n        self.hidden_to_output = nn.Linear(hidden_fea_len, n_outputs)\n\n    def forward(self, node_mat, adj_mat):\n        # Perform initial transform on node_mat\n        node_fea = self.init_transform(node_mat)\n\n        # Perform convolutions\n        for conv in self.conv_layers:\n            node_fea = conv(node_fea, adj_mat)\n\n        # Perform pooling\n        pooled_node_fea = self.pooling(node_fea)\n        pooled_node_fea = self.pooling_activation(pooled_node_fea)\n\n        # First hidden layer\n        hidden_node_fea = self.pooled_to_hidden(pooled_node_fea)\n        hidden_node_fea = self.hidden_activation(hidden_node_fea)\n        hidden_node_fea = self.dropout(hidden_node_fea)\n\n        # Subsequent hidden layers\n        if self.n_hidden > 1:\n            for i in range(self.n_hidden - 1):\n                hidden_node_fea = self.hidden_layers[i](hidden_node_fea)\n                hidden_node_fea = self.hidden_activation_layers[i](hidden_node_fea)\n                hidden_node_fea = self.hidden_dropout_layers[i](hidden_node_fea)\n\n        # Output\n        out = self.hidden_to_output(hidden_node_fea)\n\n        return out\n```", "```py\nclass Standardizer:\n    def __init__(self, X):\n        self.mean = torch.mean(X)\n        self.std = torch.std(X)\n\n    def standardize(self, X):\n        Z = (X - self.mean) / (self.std)\n        return Z\n\n    def restore(self, Z):\n        X = self.mean + Z * self.std\n        return X\n\n    def state(self):\n        return {\"mean\": self.mean, \"std\": self.std}\n\n    def load(self, state):\n        self.mean = state[\"mean\"]\n        self.std = state[\"std\"]\n```", "```py\ndef train_model(\n    epoch,\n    model,\n    training_dataloader,\n    optimizer,\n    loss_fn,\n    standardizer,\n    use_GPU,\n    max_atoms,\n    node_vec_len,\n):\n    # Create variables to store losses and error\n    avg_loss = 0\n    avg_mae = 0\n    count = 0\n\n    # Switch model to train mode\n    model.train()\n\n    # Go over each batch in the dataloader\n    for i, dataset in enumerate(training_dataloader):\n        # Unpack data\n        node_mat = dataset[0][0]\n        adj_mat = dataset[0][1]\n        output = dataset[1]\n\n        # Reshape inputs\n        first_dim = int((torch.numel(node_mat)) / (max_atoms * node_vec_len))\n        node_mat = node_mat.reshape(first_dim, max_atoms, node_vec_len)\n        adj_mat = adj_mat.reshape(first_dim, max_atoms, max_atoms)\n\n        # Standardize output\n        output_std = standardizer.standardize(output)\n\n        # Package inputs and outputs; check if GPU is enabled\n        if use_GPU:\n            nn_input = (node_mat.cuda(), adj_mat.cuda())\n            nn_output = output_std.cuda()\n        else:\n            nn_input = (node_mat, adj_mat)\n            nn_output = output_std\n\n        # Compute output from network\n        nn_prediction = model(*nn_input)\n\n        # Calculate loss\n        loss = loss_fn(nn_output, nn_prediction)\n        avg_loss += loss\n\n        # Calculate MAE\n        prediction = standardizer.restore(nn_prediction.detach().cpu())\n        mae = mean_absolute_error(output, prediction)\n        avg_mae += mae\n\n        # Set zero gradients for all tensors\n        optimizer.zero_grad()\n\n        # Do backward prop\n        loss.backward()\n\n        # Update optimizer parameters\n        optimizer.step()\n\n        # Increase count\n        count += 1\n\n    # Calculate avg loss and MAE\n    avg_loss = avg_loss / count\n    avg_mae = avg_mae / count\n\n    # Print stats\n    print(\n        \"Epoch: [{0}]\\tTraining Loss: [{1:.2f}]\\tTraining MAE: [{2:.2f}]\"\\\n           .format(\n                    epoch, avg_loss, avg_mae\n           )\n    )\n\n    # Return loss and MAE\n    return avg_loss, avg_mae\n```", "```py\n#### Fix seeds\nnp.random.seed(0)\ntorch.manual_seed(0)\nuse_GPU = torch.cuda.is_available()\n\n#### Inputs\nmax_atoms = 200\nnode_vec_len = 60\ntrain_size = 0.7\nbatch_size = 32\nhidden_nodes = 60\nn_conv_layers = 4\nn_hidden_layers = 2\nlearning_rate = 0.01\nn_epochs = 50\n\n#### Start by creating dataset\nmain_path = Path(__file__).resolve().parent\ndata_path = main_path / \"data\" / \"solubility_data.csv\"\ndataset = GraphData(dataset_path=data_path, max_atoms=max_atoms, \n                        node_vec_len=node_vec_len)\n\n#### Split data into training and test sets\n# Get train and test sizes\ndataset_indices = np.arange(0, len(dataset), 1)\ntrain_size = int(np.round(train_size * len(dataset)))\ntest_size = len(dataset) - train_size\n\n# Randomly sample train and test indices\ntrain_indices = np.random.choice(dataset_indices, size=train_size, \n                                                            replace=False)\ntest_indices = np.array(list(set(dataset_indices) - set(train_indices)))\n\n# Create dataoaders\ntrain_sampler = SubsetRandomSampler(train_indices)\ntest_sampler = SubsetRandomSampler(test_indices)\ntrain_loader = DataLoader(dataset, batch_size=batch_size, \n                          sampler=train_sampler, \n                          collate_fn=collate_graph_dataset)\ntest_loader = DataLoader(dataset, batch_size=batch_size, \n                         sampler=test_sampler,\n                         collate_fn=collate_graph_dataset)\n\n#### Initialize model, standardizer, optimizer, and loss function\n# Model\nmodel = ChemGCN(node_vec_len=node_vec_len, node_fea_len=hidden_nodes,\n                hidden_fea_len=hidden_nodes, n_conv=n_conv_layers, \n                n_hidden=n_hidden_layers, n_outputs=1, p_dropout=0.1)\n# Transfer to GPU if needed\nif use_GPU:\n    model.cuda()\n\n# Standardizer\noutputs = [dataset[i][1] for i in range(len(dataset))]\nstandardizer = Standardizer(torch.Tensor(outputs))\n\n# Optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# Loss function\nloss_fn = torch.nn.MSELoss()\n\n#### Train the model\nloss = []\nmae = []\nepoch = []\nfor i in range(n_epochs):\n    epoch_loss, epoch_mae = train_model(\n        i,\n        model,\n        train_loader,\n        optimizer,\n        loss_fn,\n        standardizer,\n        use_GPU,\n        max_atoms,\n        node_vec_len,\n    )\n    loss.append(epoch_loss)\n    mae.append(epoch_mae)\n    epoch.append(i)\n\n#### Test the model\n# Call test model function\ntest_loss, test_mae = test_model(model, test_loader, loss_fn, standardizer,\n                                 use_GPU, max_atoms, node_vec_len)\n\n#### Print final results\nprint(f\"Training Loss: {loss[-1]:.2f}\")\nprint(f\"Training MAE: {mae[-1]:.2f}\")\nprint(f\"Test Loss: {test_loss:.2f}\")\nprint(f\"Test MAE: {test_mae:.2f}\")\n```"]