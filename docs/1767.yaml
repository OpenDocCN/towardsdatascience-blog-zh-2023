- en: Summarising Best Practices for Prompt Engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/summarising-best-practices-for-prompt-engineering-c5e86c483af4?source=collection_archive---------0-----------------------#2023-05-29](https://towardsdatascience.com/summarising-best-practices-for-prompt-engineering-c5e86c483af4?source=collection_archive---------0-----------------------#2023-05-29)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to build your own LLM-based application using OpenAI API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@andimid?source=post_page-----c5e86c483af4--------------------------------)[![Dmytro
    Nikolaiev (Dimid)](../Images/4121156b9c08ed20e7aa620712a391d9.png)](https://medium.com/@andimid?source=post_page-----c5e86c483af4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c5e86c483af4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c5e86c483af4--------------------------------)
    [Dmytro Nikolaiev (Dimid)](https://medium.com/@andimid?source=post_page-----c5e86c483af4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F97b5279dad26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsummarising-best-practices-for-prompt-engineering-c5e86c483af4&user=Dmytro+Nikolaiev+%28Dimid%29&userId=97b5279dad26&source=post_page-97b5279dad26----c5e86c483af4---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c5e86c483af4--------------------------------)
    ·13 min read·May 29, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc5e86c483af4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsummarising-best-practices-for-prompt-engineering-c5e86c483af4&user=Dmytro+Nikolaiev+%28Dimid%29&userId=97b5279dad26&source=-----c5e86c483af4---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc5e86c483af4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsummarising-best-practices-for-prompt-engineering-c5e86c483af4&source=-----c5e86c483af4---------------------bookmark_footer-----------)![](../Images/29f7df4f533997ab03fde5e13ffcb7b5.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Glenn Carstens-Peters](https://unsplash.com/@glenncarstenspeters?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/RLw-UC03Gwc?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering refers to the process of creating instructions called *prompts*
    for Large Language Models (LLMs), such as OpenAI’s ChatGPT. With the immense potential
    of LLMs to solve a wide range of tasks, leveraging prompt engineering can empower
    us to save significant time and facilitate the development of impressive applications.
    It holds the key to **unleashing the full capabilities** of these huge models,
    transforming how we interact and benefit from them.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, I tried to summarize the best practices of prompt engineering
    to help you build LLM-based applications faster. While the field is developing
    very rapidly, the following “time-tested” :) techniques tend to work well and
    allow you to achieve fantastic results. In particular, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: The concept of **iterative prompt development**, using separators and structural
    output;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chain-of-Thoughts** reasoning;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Few-shot learning**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Together with intuitive explanations, I’ll share both hands-on examples and
    resources for future investigation.
  prefs: []
  type: TYPE_NORMAL
- en: Then we’ll explore how you can build a simple LLM-based application for local
    use using [OpenAI API](https://platform.openai.com/docs/introduction) for free.
    We will use Python to describe the logic and [Streamlit library](https://streamlit.io/)
    to build the web interface.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start!
  prefs: []
  type: TYPE_NORMAL
- en: Best Practices for Prompt Engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, I will interact with ChatGPT through both the web interface
    and API. The `gpt-3.5-turbo` model I’ll use is the one behind ChatGPT, so you
    can experiment with your prompts right in the browser.
  prefs: []
  type: TYPE_NORMAL
- en: One important thing to note here is that ChatGPT is not only LLM; as you probably
    know, it’s also a SFT (Supervised Fine-Tuning) model that was further finetuned
    with [Reinforcement Learning from Human Feedback (RLHF)](https://huyenchip.com/2023/05/02/rlhf.html).
    While many developers currently utilize OpenAI’s models for experimental projects
    and personal exploration, there are other models that may be more appropriate
    for deployment in production settings within large corporations due to privacy
    and other reasons.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you want to know why base models (such as [GPT-3](https://en.wikipedia.org/wiki/GPT-3),
    [Chinchilla](https://arxiv.org/abs/2203.15556), [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/))
    do not function in the same way as fine-tuned and RLHF-trained assistants (e.g.,
    [ChatGPT](https://openai.com/blog/chatgpt), [Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/),
    [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)), there is [a talk
    by Andrej Karpathy about training and using GPT-like models](https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2).
    I highly recommend checking it out for a deeper understanding, and it is just
    40 minutes long. For a summary, take a look at [this Twitter thread](https://threadreaderapp.com/thread/1661236778458832896.html).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Now let’s dive into best practices for prompting instruction-tuned LLMs!
  prefs: []
  type: TYPE_NORMAL
- en: Iterative Prompt Development
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just as any machine learning model is built through an *iterative process*,
    effective prompts are also constructed through a similar iterative approach. Even
    the most talented developer may not create the perfect prompt on their first attempt,
    so be prepared for the reality that you may need *dozens* of attempts to achieve
    the desired goal.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4083d7c372cb4ac15b64bd8af2107112.png)'
  prefs: []
  type: TYPE_IMG
- en: Building data-based applications is always an iterative process. [Public domain](https://commons.wikimedia.org/wiki/File:CRISP-DM_Process_Diagram.png)
  prefs: []
  type: TYPE_NORMAL
- en: It’s always better to understand things through examples. Let’s start building
    a **system to extract information from a job description**. The sample job description
    we’ll be using in the examples is the following Machine Learning Engineer job
    posting from LinkedIn.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2d4285bb3456098a7f92e0604ba754c5.png)'
  prefs: []
  type: TYPE_IMG
- en: Sample job description. Screenshot of the [LinkedIn job page](https://www.linkedin.com/jobs/)
  prefs: []
  type: TYPE_NORMAL
- en: The initial prompt can be as simple as asking the model to extract specific
    information. Additionally, I’ll use delimiters (you can learn more about it in
    the [ChatGPT Prompt Engineering for Developers course](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)
    I will mention later). While it’s unlikely that a local application would be susceptible
    to [prompt injection attacks](https://learnprompting.org/docs/prompt_hacking/injection),
    it’s just good practice.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3f70008e054894ce2493fe290b9c3ad0.png)'
  prefs: []
  type: TYPE_IMG
- en: Output for a prompt v1\. Image by Author created using [ChatGPT](https://chat.openai.com/chat)
  prefs: []
  type: TYPE_NORMAL
- en: Hmm, that’s not very helpful. We need to be more specific about what we want
    from the model. Let’s ask it to extract the job title, company name, key skills
    required, and a summarized job description.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember that this is just an example, and you can design your prompts to extract
    as much information as you want: degree, years of experience required, location,
    etc.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/f9638fb4a5327f81e5a5cf32ff957ffb.png)'
  prefs: []
  type: TYPE_IMG
- en: Output for a prompt v2\. Image by Author created using [ChatGPT](https://chat.openai.com/chat)
  prefs: []
  type: TYPE_NORMAL
- en: Looking better! To make the output more compact and concise, let’s ask the model
    to output skills as a list and a more brief summary of the job description.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c77323df87ea345febfaa32f5f4bf160.png)'
  prefs: []
  type: TYPE_IMG
- en: Output for a prompt v3\. Image by Author created using [ChatGPT](https://chat.openai.com/chat)
  prefs: []
  type: TYPE_NORMAL
- en: It’s a significant improvement from our initial attempt, isn’t it? And we accomplish
    this in just two iterations! So, don’t lose hope when things don’t go smoothly
    initially. Keep guiding the model, experiment, and you’ll certainly achieve success.
  prefs: []
  type: TYPE_NORMAL
- en: Asking for a Structural Output
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The second point I would like to discuss is asking the model to output results
    in some expected structural format. While it may not be as critical for interacting
    with LLM through the web interface (e.g. as we do with ChatGPT), it becomes **extremely
    useful for LLM-based applications** since the process of parsing the results is
    much easier.
  prefs: []
  type: TYPE_NORMAL
- en: One common practice is to use formats like JSON or XML and define specific keys
    to organize output data. Let’s modify our prompt to show the model expected JSON
    structure.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7ec32d518783fe76a5e156fe8e630909.png)'
  prefs: []
  type: TYPE_IMG
- en: Output for a prompt v4, asking for JSON output. Image by Author created using
    [ChatGPT](https://chat.openai.com/chat)
  prefs: []
  type: TYPE_NORMAL
- en: Such output is much easier to parse and process in the following logic of your
    application.
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth saying a few words about the development of this direction. Some
    tools are aiming to strictly fit the model’s output into a given format, which
    can be extremely useful for some tasks. Just look at the examples below!
  prefs: []
  type: TYPE_NORMAL
- en: One of the possible applications is the generation of a large amount of content
    in a specific format (e.g. game characters information using [guidance](https://github.com/microsoft/guidance)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/947eb4216ade29b99b26d82cb16f0ca2.png)'
  prefs: []
  type: TYPE_IMG
- en: Generating game character info in JSON. Gif from [guidance GitHub repo](https://github.com/microsoft/guidance)
  prefs: []
  type: TYPE_NORMAL
- en: Languages like [LMQL](https://lmql.ai/) bring a programming-like approach to
    prompting language models. As these tools continue to evolve and improve, they
    have the potential to revolutionize how we interact with LLMs, resulting in more
    accurate and structured responses.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5fe7d232aafae7011bd6de1799076779.png)'
  prefs: []
  type: TYPE_IMG
- en: LMQL query example. Screenshot of the [LMQL webpage, see more examples here](https://lmql.ai/)
  prefs: []
  type: TYPE_NORMAL
- en: Chain-of-Thought Reasoning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Chain-of-Thought (CoT) reasoning was discovered to be very helpful for tasks
    that require… well, reasoning. So if you have the opportunity to solve the task
    by breaking it into multiple simpler steps that can be a great approach for LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Take a look at the example from the original paper. By splitting the problem
    into smaller steps and providing explicit instructions, we can assist the model
    in producing correct outputs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8ed7855cdb779bf6d4e88425bf2cbcfe.png)'
  prefs: []
  type: TYPE_IMG
- en: Introducing CoT prompting. Figure 1 from the [Chain-of-Thought Prompting Elicits
    Reasoning in LLMs paper](https://arxiv.org/pdf/2201.11903.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, it emerges later that appending a straightforward and *magic*
    **“let’s think step by step”** at the end of a prompt can improve results — this
    technique is known as *zero-shot CoT*. So, compose prompts that allow the model
    to “think out loud” since it does not have any other ability to express thoughts
    other than generate tokens.
  prefs: []
  type: TYPE_NORMAL
- en: The best zero-shot CoT prompt so far is “**Let’s work this out in a step by
    step way to be sure we have the right answer**”.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d9ad914d665075aee6752b6276e50124.png)'
  prefs: []
  type: TYPE_IMG
- en: Best zero-shot prompts. Table 7 from the [LLMs Are Human-Level Prompt Engineers
    paper](https://arxiv.org/pdf/2211.01910.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: 'More sophisticated approaches to solving even more complex tasks are now being
    actively developed. While they significantly outperform in some scenarios, their
    practical usage remains somewhat limited. I will mention two such techniques:
    *self-consistency* and the *Tree of Thoughts*.'
  prefs: []
  type: TYPE_NORMAL
- en: The authors of the [self-consistency paper](https://arxiv.org/pdf/2203.11171.pdf)
    offered the following approach. Instead of just relying on the initial model output,
    they suggested sampling multiple times and aggregating the results through majority
    voting. By relying on both intuition and the success of [ensembles](https://en.wikipedia.org/wiki/Ensemble_learning)
    in classical machine learning, this technique enhances the model’s robustness.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c7c6be883fb384c6a33edc479af020de.png)'
  prefs: []
  type: TYPE_IMG
- en: Self-consistency. Figure 1 from the [Self-Consistency Improves CoT Reasoning
    in Language Models paper](https://arxiv.org/pdf/2203.11171.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: You can also apply self-consistency without implementing the aggregation step.
    For tasks with short outputs ask the model to **suggest several options** and
    choose the best one.
  prefs: []
  type: TYPE_NORMAL
- en: Tree of Thoughts (ToT) takes this concept a stride further. It puts forward
    the idea of applying tree-search algorithms for the model’s “reasoning thoughts”,
    essentially backtracking when it stumbles upon poor assumptions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a0fb05671b6dee06c9e97bfb2c687e18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Tree of Thoughts. Figure 1 from the [Tree of Thoughts: Deliberate Problem Solving
    with LLMs paper](https://arxiv.org/pdf/2305.10601.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested, check out [Yannic Kilcher’s video with a ToT paper review](https://www.youtube.com/watch?v=ut5kp56wW_4).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For our particular scenario, utilizing Chain-of-Thought reasoning is not necessary,
    yet we can prompt the model to tackle the summarization task in two phases. Initially,
    it can condense the entire job description, and then summarize the derived summary
    with a focus on job responsibilities.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8c95872561a59e8ab6fde59c6ae006c2.png)'
  prefs: []
  type: TYPE_IMG
- en: Output for a prompt v5, containing step-by-step instructions. Image by Author
    created using [ChatGPT](https://chat.openai.com/chat)
  prefs: []
  type: TYPE_NORMAL
- en: In this particular example, the results did not show significant changes, but
    this approach works very well for most tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Few-shot Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last technique we will cover is called *few-shot learning*, also known as
    *in-context learning*. It’s as simple as incorporating several examples into your
    prompt to provide the model with a clearer picture of your task.
  prefs: []
  type: TYPE_NORMAL
- en: These examples should not only be **relevant** to your task but also **diverse**
    to encapsulate the variety in your data. “Labeling” data for few-shot learning
    might be a bit more challenging when you’re using CoT, particularly if your pipeline
    has many steps or your inputs are long. However, typically, the results make it
    worth the effort. Also, keep in mind that labeling a few examples is far less
    expensive than labeling an entire training/testing set as in traditional ML model
    development.
  prefs: []
  type: TYPE_NORMAL
- en: If we add an example to our prompt, it will understand the requirements even
    better. For instance, if we demonstrate that we’d prefer the final summary in
    bullet-point format, the model will mirror our template.
  prefs: []
  type: TYPE_NORMAL
- en: 'This prompt is quite overwhelming, but don’t be afraid: it is just a previous
    prompt (v5) and one labeled example with another job description in the `For example:
    ''input description'' -> ''output JSON''` format.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2be54202a408a5d5282c61effc8f4fe8.png)'
  prefs: []
  type: TYPE_IMG
- en: Output for a prompt v6, containing an example. Image by Author created using
    [ChatGPT](https://chat.openai.com/chat)
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing Best Practices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To summarize the best practices for prompt engineering, consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Don’t be afraid to experiment**. Try different approaches and iterate gradually,
    correcting the model and taking small steps at a time;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use separators** in input (e.g. <>) and ask for a **structured output** (e.g.
    JSON);'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Provide a list of actions to complete the task**. Whenever feasible, offer
    the model a set of actions and let it output its “internal thoughts”;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In case of short outputs **ask for multiple suggestions**;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Provide examples**. If possible, show the model several diverse examples
    that represent your data with the desired output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I would say that this framework offers a sufficient basis for automating a wide
    range of day-to-day tasks, like information extraction, summarization, text generation
    such as emails, etc. However, in a production environment, it is still possible
    to further optimize models by [fine-tuning them](https://magazine.sebastianraschka.com/p/finetuning-large-language-models)
    on specific datasets to further enhance performance. Additionally, there is rapid
    development in the [plugins](https://openai.com/blog/chatgpt-plugins) and [agents](https://www.pinecone.io/learn/langchain-agents/),
    but that’s a whole different story altogether.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Engineering Course by DeepLearning.AI and OpenAI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Along with the earlier-mentioned [talk by Andrej Karpathy](https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2),
    this blog post draws its inspiration from the [ChatGPT Prompt Engineering for
    Developers course by DeepLearning.AI and OpenAI](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/).
    It’s absolutely free, takes just a couple of hours to complete, and, my personal
    favorite, it enables you to experiment with the OpenAI API without even signing
    up!
  prefs: []
  type: TYPE_NORMAL
- en: That’s a great playground for experimenting, so definitely check it out.
  prefs: []
  type: TYPE_NORMAL
- en: Building the LLM-based Application with OpenAI API and Streamlit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Wow, we covered quite a lot of information! Now, let’s move forward and start
    building the application using the knowledge we have gained.
  prefs: []
  type: TYPE_NORMAL
- en: Generating OpenAI Key
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To get started, you’ll need to register an OpenAI account and create your API
    key. [OpenAI currently offers **$5 of free credit for 3 months**](https://openai.com/pricing#:~:text=Simple%20and%20flexible-,Start%20for%20free,-Start%20experimenting%20with)
    to every individual. Follow the [introduction to the OpenAI API](https://platform.openai.com/docs/api-reference/introduction)
    page to register your account and [generate your API key](https://platform.openai.com/docs/api-reference/introduction).
  prefs: []
  type: TYPE_NORMAL
- en: Once you have a key, create an `OPENAI_API_KEY` [environment variable](https://www.twilio.com/blog/how-to-set-environment-variables-html)
    to access it in the code with `os.getenv('OPENAI_API_KEY')`.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating the Costs with Tokenizer Playground
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At this stage, you might be curious about how much you can do with just a free
    trial and what options are available after the initial three months. It’s a pretty
    good question to ask, especially when you consider that [LLMs cost millions of
    dollars](https://medium.com/towards-data-science/behind-the-millions-estimating-the-scale-of-large-language-models-97bd7287fb6b)!
  prefs: []
  type: TYPE_NORMAL
- en: Of course, these millions are about training. It turns out that the inference
    requests are quite affordable. While GPT-4 may be perceived as expensive (although
    the price is likely to decrease), `gpt-3.5-turbo` (the model behind default ChatGPT)
    is still sufficient for the majority of tasks. In fact, OpenAI has done an incredible
    engineering job, given how inexpensive and fast these models are now, considering
    their original size in billions of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The `gpt-3.5-turbo` model comes at a cost of $0.002 per 1,000 tokens.
  prefs: []
  type: TYPE_NORMAL
- en: But how much is it? Let’s see. First, we need to know what is a token. In simple
    terms, a token refers to a part of a word. In the context of the English language,
    you can expect **around 14 tokens for every 10 words**.
  prefs: []
  type: TYPE_NORMAL
- en: To get a more accurate estimation of the number of tokens for your specific
    task and prompt, the best approach is to give it a try! Luckily, OpenAI provides
    a [tokenizer playground](https://platform.openai.com/tokenizer) that can help
    you with this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Side note: Tokenization for Different Languages'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Due to the widespread use of English on the Internet, this language benefits
    from the most optimal tokenization. As highlighted in the [“All languages are
    not tokenized equal”](https://blog.yenniejun.com/p/all-languages-are-not-created-tokenized)
    blog post, tokenization is not a uniform process across languages, and certain
    languages may require a greater number of tokens for representation. Keep this
    in mind if you want to build an application that involves prompts in multiple
    languages, e.g. for translation.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this point, let’s take a look at the tokenization of [pangrams](https://clagnut.com/blog/2380)
    in different languages. In this toy example, English required 9 tokens, French
    — 12, Bulgarian — 59, Japanese — 72, and Russian — 73.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ff25f16bfa9d4e539d0b727755bced5c.png)'
  prefs: []
  type: TYPE_IMG
- en: Tokenization for different languages. Screenshot of the [OpenAI tokenizer playground](https://platform.openai.com/tokenizer)
  prefs: []
  type: TYPE_NORMAL
- en: Cost vs Performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you may have noticed, prompts can become quite lengthy, especially when incorporating
    examples. By increasing the length of the prompt, we potentially enhance the quality,
    but the cost grows at the same time as we use more tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Our latest prompt (v6) consists of approximately 1.5k tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0d86ff60b17ea35cb57608a7b0e76e0a.png)'
  prefs: []
  type: TYPE_IMG
- en: Tokenization of the prompt v6\. Screenshot of the [OpenAI tokenizer playground](https://platform.openai.com/tokenizer)
  prefs: []
  type: TYPE_NORMAL
- en: Considering that the output length is typically the same range as the input
    length, we can estimate an average of around 3k tokens per request (*input tokens
    + output tokens*). By multiplying this number by the initial cost, we find that
    **each request is about $0.006 or 0.6 cents**, which is quite affordable.
  prefs: []
  type: TYPE_NORMAL
- en: Even if we consider a slightly higher cost of 1 cent per request (equivalent
    to roughly 5k tokens), you would still be able to make **100 requests for just
    $1**. Additionally, OpenAI offers the flexibility [to set both soft and hard limits](https://openai.com/pricing#:~:text=How%20can%20I%20manage%20my%20spending%3F).
    With soft limits, you receive notifications when you approach your defined limit,
    while hard limits restrict you from exceeding the specified threshold.
  prefs: []
  type: TYPE_NORMAL
- en: For local use of your LLM application, you can comfortably configure a hard
    limit of $1 per month, ensuring that you remain within budget while enjoying the
    benefits of the model.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Streamlit App Template
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, let’s build a web interface to interact with the model programmatically
    eliminating the need to manually copy prompts each time. We will do this with
    [Streamlit](https://streamlit.io/).
  prefs: []
  type: TYPE_NORMAL
- en: Streamlit is a Python library that allows you to create simple web interfaces
    without the need for HTML, CSS, and JavaScript. It is beginner-friendly and enables
    the creation of browser-based applications using minimal Python knowledge. Let’s
    now create a simple template for our LLM-based application.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, we need the logic that will handle the communication with the OpenAI
    API. In the example below, I consider `generate_prompt()`function to be defined
    and return the prompt for a given input text (e.g. similar to what you saw before).
  prefs: []
  type: TYPE_NORMAL
- en: And that’s it! Know more about different parameters in [OpenAI’s documentation](https://platform.openai.com/docs/api-reference/chat/create#:~:text=given%20chat%20conversation.-,Request%20body,-model),
    but things work well just out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: Having this code, we can design a simple web app. We need a field to enter some
    text, a button to process it, and a couple of output widgets. I prefer to have
    access to both the full model prompt and output for debugging and exploring reasons.
  prefs: []
  type: TYPE_NORMAL
- en: The code for the entire application will look something like this and can be
    found in [this GitHub repository](https://github.com/Winston-503/streamlit_app_template).
    I have added a placeholder function called `toy_ask_chatgpt()` since sharing the
    OpenAI key is not a good idea. Currently, this application simply copies the prompt
    into the output.
  prefs: []
  type: TYPE_NORMAL
- en: Without defining functions and placeholders, it is only about 50 lines of code!
  prefs: []
  type: TYPE_NORMAL
- en: And thanks to a [recent update in Streamlit it now allows embed it](https://docs.streamlit.io/streamlit-community-cloud/get-started/embed-your-app)
    right in this article! So you should be able to see it right below.
  prefs: []
  type: TYPE_NORMAL
- en: Now you see how easy it is. If you wish, you can [deploy your app with Streamlit
    Cloud](https://docs.streamlit.io/streamlit-community-cloud/get-started/deploy-an-app).
    But be careful, since every request costs you money if you put your API key there!
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this blog post, I listed several best practices for prompt engineering. We
    discussed iterative prompt development, the use of separators, requesting structural
    output, Chain-of-Thought reasoning, and few-shot learning. I also provided you
    with a template to build a simple web app using Streamlit in under 100 lines of
    code. Now, it’s your turn to come up with an exciting project idea and turn it
    into reality!
  prefs: []
  type: TYPE_NORMAL
- en: It’s truly amazing how modern tools allow us to create complex applications
    in just a few hours. Even without extensive programming knowledge, proficiency
    in Python, or a deep understanding of machine learning, you can quickly build
    something useful and automate some tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t hesitate to ask me questions if you’re a beginner and want to create a
    similar project. I’ll be more than happy to assist you and respond as soon as
    possible. Best of luck with your projects!
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are my other articles about LLMs that may be useful to you. I have already
    covered:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Estimating the Scale of Large Language Models](/behind-the-millions-estimating-the-scale-of-large-language-models-97bd7287fb6b):
    what are LLMs, how are they trained, and how much data and compute do they need;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Using ChatGPT for Debugging](/using-chatgpt-for-efficient-debugging-fc9e065b7856#da94-27cac6b3f550):
    how to use LLMs for debugging and code generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can be also interested in:'
  prefs: []
  type: TYPE_NORMAL
- en: Free [Learn Prompting course](https://learnprompting.org/) to gain a deeper
    understanding of prompting and various techniques associated with it;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recently released [short courses by DeepLearning.AI](https://www.deeplearning.ai/short-courses/)
    to build applications with OpenAI API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thank you for reading!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I hope these materials were useful to you. [Follow me on Medium](https://medium.com/@andimid)
    to get more articles like this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you have any questions or comments, I will be glad to get any feedback. Ask
    me in the comments, or connect via [LinkedIn](https://www.linkedin.com/in/andimid/)
    or [Twitter](https://twitter.com/dimid_ml).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To support me as a writer and to get access to thousands of other Medium articles,
    get Medium membership using [my referral link](https://medium.com/@andimid/membership)
    (no extra charge for you).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
