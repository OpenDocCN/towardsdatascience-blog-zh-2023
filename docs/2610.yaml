- en: A Modest Introduction to Analytical Stream Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对分析流处理的简要介绍
- en: 原文：[https://towardsdatascience.com/a-modest-introduction-to-analytical-stream-processing-db58b3694263?source=collection_archive---------9-----------------------#2023-08-15](https://towardsdatascience.com/a-modest-introduction-to-analytical-stream-processing-db58b3694263?source=collection_archive---------9-----------------------#2023-08-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-modest-introduction-to-analytical-stream-processing-db58b3694263?source=collection_archive---------9-----------------------#2023-08-15](https://towardsdatascience.com/a-modest-introduction-to-analytical-stream-processing-db58b3694263?source=collection_archive---------9-----------------------#2023-08-15)
- en: Architectural Foundations for Building Reliable Distributed Systems.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建可靠分布式系统的架构基础。
- en: '[](https://newfrontcreative.medium.com/?source=post_page-----db58b3694263--------------------------------)[![Scott
    Haines](../Images/b53c166b64314b4a5fe41abbe1839716.png)](https://newfrontcreative.medium.com/?source=post_page-----db58b3694263--------------------------------)[](https://towardsdatascience.com/?source=post_page-----db58b3694263--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----db58b3694263--------------------------------)
    [Scott Haines](https://newfrontcreative.medium.com/?source=post_page-----db58b3694263--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://newfrontcreative.medium.com/?source=post_page-----db58b3694263--------------------------------)[![Scott
    Haines](../Images/b53c166b64314b4a5fe41abbe1839716.png)](https://newfrontcreative.medium.com/?source=post_page-----db58b3694263--------------------------------)[](https://towardsdatascience.com/?source=post_page-----db58b3694263--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----db58b3694263--------------------------------)
    [Scott Haines](https://newfrontcreative.medium.com/?source=post_page-----db58b3694263--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3b4cab6af83e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-modest-introduction-to-analytical-stream-processing-db58b3694263&user=Scott+Haines&userId=3b4cab6af83e&source=post_page-3b4cab6af83e----db58b3694263---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----db58b3694263--------------------------------)
    ·23 min read·Aug 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdb58b3694263&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-modest-introduction-to-analytical-stream-processing-db58b3694263&user=Scott+Haines&userId=3b4cab6af83e&source=-----db58b3694263---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3b4cab6af83e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-modest-introduction-to-analytical-stream-processing-db58b3694263&user=Scott+Haines&userId=3b4cab6af83e&source=post_page-3b4cab6af83e----db58b3694263---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----db58b3694263--------------------------------)
    ·23分钟阅读·2023年8月15日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdb58b3694263&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-modest-introduction-to-analytical-stream-processing-db58b3694263&user=Scott+Haines&userId=3b4cab6af83e&source=-----db58b3694263---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdb58b3694263&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-modest-introduction-to-analytical-stream-processing-db58b3694263&source=-----db58b3694263---------------------bookmark_footer-----------)![](../Images/1fba9140fab3628fef09399e2a81e5a8.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdb58b3694263&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-modest-introduction-to-analytical-stream-processing-db58b3694263&source=-----db58b3694263---------------------bookmark_footer-----------)![](../Images/1fba9140fab3628fef09399e2a81e5a8.png)'
- en: Distributed Streaming Data Networks are Unbounded and Growing at Incredible
    Rates. Image Created via [Author’s MidJourney](https://www.midjourney.com/app/jobs/3cf0ebab-3ec4-487d-a0ea-24ef01387eae/)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式流数据网络是无限的，并且以惊人的速度增长。图片由 [作者的 MidJourney](https://www.midjourney.com/app/jobs/3cf0ebab-3ec4-487d-a0ea-24ef01387eae/)
    创建
- en: Foundations of Stream Processing
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流处理的基础
- en: Foundations are the unshakable, unbreakable base upon which structures are placed.
    When it comes to building a successful data architecture, the data is the core
    central tenant of the entire system and the principal component of that foundation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 基础是结构建立的坚固、牢不可破的基础。在构建成功的数据架构时，数据是整个系统的核心要素和基础的主要组成部分。
- en: Given the most common way data flows into our data platforms is through stream
    processing platforms like [Apache Kafka](https://kafka.apache.org/) and [Apache
    Pulsar](https://pulsar.apache.org/) this post covers this topic area.
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 鉴于数据流入我们的数据平台的最常见方式是通过像[Apache Kafka](https://kafka.apache.org/)和[Apache Pulsar](https://pulsar.apache.org/)这样的流处理平台，本文涵盖了这一主题领域。
- en: Therefore it becomes critical to ensure we (as software engineers) provide hygienic
    capabilities and frictionless guardrails to reduce the problem space related to
    data quality “after” data has entered into these fast-flowing data networks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，确保我们（作为软件工程师）提供清洁的能力和无摩擦的保护措施，以减少数据进入这些快速流动的数据网络后的数据质量问题，是至关重要的。
- en: This means establishing api-level contracts surrounding our data’s *schema (types,
    and structure)*, field-level *availability (nullable, etc)*, and field-type *validity
    (expected ranges, etc)* become the critical underpinnings of our data foundation
    especially given the decentralized, distributed streaming nature of today’s modern
    data systems.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着建立围绕我们数据的*架构（类型和结构）*、字段级*可用性（可为空等）*和字段类型*有效性（预期范围等）*的API级合同，成为我们数据基础的重要支撑，尤其是在当今现代数据系统的去中心化、分布式流性质下。
- en: However, to get to the point where we can even begin to establish blind-faith
    — or high-trust data networks — we must first establish intelligent system-level
    design patterns.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了达到建立盲目信任——或高信任数据网络的程度，我们必须首先建立智能系统级设计模式。
- en: Building Reliable Streaming Data Systems
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建可靠的流数据系统
- en: As software and data engineers, building reliable data systems is literally
    our job, and this means data downtime should be measured like any other component
    of the business. You’ve probably heard of the terms *SLAs*, *SLOs* and *SLIs*
    at one point or another. In a nutshell, these acronyms are associated to the **contracts**,
    **promises**, and **actual measures** in which we grade our end-to-end systems.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作为软件和数据工程师，构建可靠的数据系统实际上是我们的工作，这意味着数据停机时间应像业务的其他组件一样进行测量。你可能听说过*SLAs*、*SLOs*和*SLIs*这些术语。这些缩写词与**合同**、**承诺**和**实际度量**相关，帮助我们评估端到端系统的表现。
- en: As service owners, we will be held *accountable* for our own successes and failures,
    but with a little upfront effort, standard metadata, and common standards and
    best practices can ensure things are running smooth across the board.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 作为服务负责人，我们将对自己的成功与失败负责，但通过一些前期的努力，标准化的元数据、共同的标准和最佳实践可以确保各方面的运行顺利。
- en: Additionally, the same metadata can also provide valuable insights into the
    quality and trust of our data-in-flight, along its journey until it finds its
    terminal area to rest. The lineage tells a story all on its own.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，相同的元数据还可以提供关于我们数据在传输过程中质量和可信度的宝贵见解，直到数据找到其终端区域休息。数据的血统本身就讲述了一个故事。
- en: Adopting the Owners Mindset
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 采用拥有者心态
- en: For example, *Service Level Agreements* (SLAs) between your team, or organization,
    and your customers (both internal and external) are used to create a binding contract
    with respect to the service you are providing. For data teams, this means identifying
    and capturing metrics (KPMs — key performance metrics) based on your *Service
    Level Objectives* (SLOs). The SLOs are the promises you intend to keep based on
    your SLAs, this can be anything from a promise of near perfect (99.999%) service
    uptime (API or JDBC), or something as simple as a promise of 90-day data retention
    for a particular dataset. Lastly, your *Service Level Indicators* (SLIs) are the
    proof that you are operating in accordance with the service level contracts and
    are typically presented in the form of operational analytics (dashboards) or reports.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你的团队或组织与客户（包括内部和外部客户）之间的*服务水平协议*（SLAs）用于创建一个关于你所提供服务的有约束力的合同。对于数据团队来说，这意味着根据你的*服务水平目标*（SLOs）识别和捕捉指标（KPMs
    — 关键绩效指标）。SLOs是你基于SLAs打算遵守的承诺，这可以是从接近完美（99.999%）的服务正常运行时间（API或JDBC）的承诺，到简单的承诺，例如某特定数据集的90天数据保留。最后，你的*服务水平指标*（SLIs）是你按照服务水平合同运营的证明，通常以操作分析（仪表板）或报告的形式呈现。
- en: Knowing where we want to go can help establish the plan to get there. This journey
    begins at the inset (or ingest point), and with the data. Specifically, with the
    formal structure and identity of each data point. Considering the observation
    that “more and more data is making its way into the data platform through stream
    processing platforms like Apache Kafka” it helps to have *compile time guarantees*,
    *backwards compatibility*, and *fast binary serialization* of the data being emitted
    into these data streams. Data accountability can be a challenge in and of itself.
    Let’s look at why.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 知道我们想要去哪里可以帮助制定到达那里的计划。这段旅程从数据的插入（或摄取点）开始，特别是每个数据点的正式结构和身份。考虑到“越来越多的数据通过像Apache
    Kafka这样的流处理平台进入数据平台”的观察，*编译时保证*、*向后兼容性*和*快速的二进制序列化*都对这些数据流中的数据至关重要。数据责任本身就是一个挑战。让我们看看原因。
- en: Managing Streaming Data Accountability
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管理流数据责任
- en: Streaming systems operate 24 hours a day, 7 days a week, and 365 days of the
    year. This can complicate things if the right up front effort isn’t applied to
    the problem, and one of the problems that tends to rear its head from time to
    time is that of corrupt data, aka *data problems in flight*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理系统全天候运行，每周7天，每年365天。如果没有在前期投入适当的工作，这可能会使问题复杂化，其中一个时常出现的问题是数据损坏，即*飞行中的数据问题*。
- en: Dealing with Data Problems in Flight
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理飞行中的数据问题
- en: There are two common ways to reduce data problems in flight. First, you can
    introduce gatekeepers at the edge of your data network that negotiate and validate
    data using traditional *Application Programming Interfaces* (APIs), or as a second
    option, you can create and compile helper libraries, or Software Development Kits
    (SDKs), to enforce the data protocols and enable distributed writers (data producers)
    into your streaming data infrastructure, you can even use both strategies in tandem.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种常见的方法可以减少飞行中的数据问题。首先，你可以在数据网络的边缘引入门卫，使用传统的*应用程序编程接口*（APIs）来协商和验证数据；或者作为第二种选择，你可以创建和编译辅助库或软件开发工具包（SDKs），以强制执行数据协议并使分布式写入者（数据生产者）进入你的流处理数据基础设施，你甚至可以同时使用这两种策略。
- en: Data Gatekeepers
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据门卫
- en: The benefit of adding gateway APIs at the edge (in-front) of your data network
    is that you can enforce *authentication* (can this system access this API?), *authorization*
    (can this system publish data to a specific data stream?), and *validation* (is
    this data acceptable or valid?) at the point of data production. The diagram in
    Figure 1–1 below shows the flow of the data gateway.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据网络的边缘（前端）添加网关API的好处在于，你可以在数据生产点执行*认证*（这个系统能访问这个API吗？），*授权*（这个系统能否将数据发布到特定的数据流中？），以及*验证*（这些数据是否可接受或有效？）。下图1–1展示了数据网关的流动情况。
- en: '![](../Images/76491b63816c94a91e888be33d526e48.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/76491b63816c94a91e888be33d526e48.png)'
- en: '**Figure 1–1**: A Distributed Systems Architecture showing authentication and
    authorization layers at a Data Intake Gateway. Flowing from left to right, approved
    data is published to Apache Kafka for downstream processing. Image Credit by [Scott
    Haines](https://medium.com/u/3b4cab6af83e?source=post_page-----db58b3694263--------------------------------)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**图1–1**：一个分布式系统架构图，显示了数据摄取网关的认证和授权层。从左到右，批准的数据被发布到Apache Kafka中进行下游处理。图像来源
    [Scott Haines](https://medium.com/u/3b4cab6af83e?source=post_page-----db58b3694263--------------------------------)'
- en: The **data gateway service** acts as the digital gatekeeper (bouncer) to your
    protected (internal) data network. With the main role of controlling , limiting,
    and even restricting unauthenticated access at the edge (see APIs/Services in
    figure 1–1 above), by authorizing which upstream services (or users) are allowed
    to publish data (commonly handled through the use of service [ACLs](https://docs.confluent.io/platform/current/kafka/authorization.html))
    coupled with a provided identity (think service identity and access [IAM](https://spiffe.io/),
    web identity and access [JWT](https://jwt.io/), and our old friend OAUTH).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据网关服务**充当你受保护（内部）数据网络的数字门卫（保镖）。其主要角色是控制、限制甚至阻止未经认证的访问（见上图1–1中的API/服务），通过授权哪些上游服务（或用户）可以发布数据（通常通过使用服务[ACLs](https://docs.confluent.io/platform/current/kafka/authorization.html)来处理），以及提供的身份（比如服务身份和访问[IAM](https://spiffe.io/)，网络身份和访问[JWT](https://jwt.io/)，以及我们老朋友OAUTH）。'
- en: 'The core responsibility of the gateway service is to validate inbound data
    before publishing potentially corrupt, or generally bad data. If the gateway is
    doing its job correctly, only “good” data will make its way along and into the
    data network which is the conduit of event and operational data to be digested
    via Stream Processing, in other words:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 网关服务的核心责任是验证传入的数据，在发布潜在的损坏或一般性差的数据之前。如果网关正确地执行其职责，只有“良好”的数据才会通过并进入数据网络，这个网络是事件和操作数据的传输通道，通过流处理进行消化，换句话说：
- en: “This means that the upstream system producing data can *fail fast* when producing
    data. This stops corrupt data from entering the streaming or stationary data pipelines
    at the edge of the data network and is a means of establishing a conversation
    with the producers regarding exactly why, and how things went wrong in a more
    automatic way via error codes and helpful messaging.”
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “这意味着产生数据的上游系统可以在生成数据时*快速失败*。这可以阻止损坏的数据进入流数据或静态数据管道，并通过错误代码和有用的消息以更自动的方式与生产者建立对话，了解具体原因和问题。”
- en: Using Error Messages to Provide Self-Service Solutions
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用错误消息提供自助解决方案
- en: The difference between a good and bad experience come down to how much effort
    is required to pivot from bad to good. We’ve all probably worked with, or on,
    or heard of, services that just fail with no rhyme or reason (null pointer exception
    throws random 500).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 好与坏体验的差异在于从坏到好的转变所需的努力程度。我们都可能曾经与或工作过，或听说过，服务出现无缘无故的故障（空指针异常抛出随机500）。
- en: For establishing basic trust, a little bit goes a long way. For example, getting
    back a HTTP 400 from an API endpoint with the following message body (seen below)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了建立基本的信任，一点点就足够了。例如，从API端点获取HTTP 400，并附带以下消息体（见下文）
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: provides a reason for the 400, and empowers engineers sending data to us (as
    the service owners) to fix a problem without setting up a meeting, blowing up
    the pager, or hitting up everyone on slack. When you can, remember that everyone
    is human, and we love closed loop systems!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 提供了400的原因，并赋予向我们（作为服务拥有者）发送数据的工程师修复问题的能力，无需安排会议、响起警报或在Slack上联系每个人。当你可以时，请记住，每个人都是人，我们喜欢闭环系统！
- en: Pros and Cons of the API for Data
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据API的利弊
- en: This API approach has its pros and cons.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这种API方法有其优缺点。
- en: The pros are that most programming languages work out of box with HTTP (or HTTP/2)
    transport protocols — or with the addition of a tiny library — and JSON data is
    just about as universal of a data exchange format these days.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 优点是大多数编程语言与HTTP（或HTTP/2）传输协议即开即用——或者通过添加一个小库——并且JSON数据现如今几乎是最通用的数据交换格式。
- en: On the flip side (cons), one can argue that for any new data domain, there is
    yet another service to write and manage, and without some form of API automation,
    or adherence to an open specification like [OpenAPI](https://spec.openapis.org/oas/latest.html#format),
    each new API route (endpoint) ends up taking more time than necessary.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 从另一方面（缺点）看，可以说对于任何新的数据领域，还有另一个服务需要编写和管理，如果没有某种形式的API自动化，或遵循像[OpenAPI](https://spec.openapis.org/oas/latest.html#format)这样的开放规范，每个新的API路由（端点）最终都会花费更多的时间。
- en: In many cases, failure to provide updates to data ingestion APIs in a “timely”
    fashion, or compounding issues with scaling and/or api downtime, random failures,
    or just people not communicating provides the necessary rational for folks to
    bypass the “stupid” API, and instead attempt to directly publish event data to
    Kafka. While APIs can feel like they are getting in the way, there is a strong
    argument for keeping a common gatekeeper, especially after data quality problems
    like corrupt events, or accidentally mixed events, begin to destabilize the streaming
    dream.
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在许多情况下，未能以“及时”的方式更新数据摄取API，或者扩展和/或API停机、随机故障，或只是人员沟通不畅，为人们绕过“愚蠢”API提供了必要的理由，进而尝试直接将事件数据发布到Kafka。虽然API可能会觉得阻碍，但在数据质量问题如事件损坏或意外混合事件开始破坏流处理梦想后，保持一个公共的门卫有强烈的论据。
- en: To flip this problem on its head (and remove it almost entirely), good documentation,
    change management (CI/CD), and general software development hygiene including
    actual unit and integration testing — enable fast feature and iteration cycles
    that don’t reduce trust.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 要彻底解决这个问题（并几乎完全消除它），良好的文档、变更管理（CI/CD）以及包括实际单元测试和集成测试在内的一般软件开发卫生——能够实现快速的功能和迭代周期，而不会降低信任度。
- en: Ideally, the data itself (schema / format) could dictate the rules of their
    own data level contract by enabling field level validation (predicates), producing
    helpful error messages, and acting in its own self-interest. Hey, with a little
    route or data level metadata, and some creative thinking, the API could automatically
    generate self-defining routes and behavior.
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 理想情况下，数据本身（模式/格式）可以通过启用字段级验证（谓词）、生成有用的错误信息并维护自身利益，来制定其自身数据层合同的规则。嘿，凭借一些路由或数据级元数据和一些创造性思维，API
    可以自动生成自定义的路由和行为。
- en: Lastly, gateway APIs can be seen as centralized troublemakers as each failure
    by an upstream system to emit valid data (eg. blocked by the gatekeeper) causes
    valuable information (event data, metrics) to be dropped on the floor. *The problem
    of blame here also tends to go both ways*, as a bad deployment of the gatekeeper
    can blind an upstream system that isn’t setup to handle retries in the event of
    gateway downtime (if even for a few seconds).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，网关 API 可以被视为集中式的麻烦制造者，因为每次上游系统未能发出有效数据（例如，被门卫阻挡）都会导致有价值的信息（事件数据、指标）被丢失。*这里的责任问题也往往是双向的*，因为门卫部署不当可能使一个未设置为处理网关停机（即使是几秒钟）的重试的上游系统失明。
- en: Putting aside all the pros and cons, using a gateway API to stop the propagation
    of corrupt data before it enters the data platform means that when a problem occurs
    (cause they always do), the surface area of the problem is reduced to a given
    service. This sure beat debugging a distributed network of data pipelines, services,
    and the myriad final data destinations and upstream systems to figure out that
    bad data is being directly published by “someone” at the company.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 抛开所有优缺点不谈，使用网关 API 来阻止腐败数据在进入数据平台之前的传播意味着，当出现问题时（因为问题总是会出现），问题的表面面积将减少到某个特定服务。这肯定比调试分布式的数据管道、服务，以及无数的最终数据目标和上游系统要好，因为那样你要弄清楚坏数据是由公司中的“某人”直接发布的。
- en: If we were to cut out the middle man (gateway service) then the capabilities
    to govern the transmission of “expected” data falls into the lap of “libraries”
    in the form of specialized SDKS.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们去掉中间人（网关服务），那么治理“预期”数据传输的能力将落在“库”这一形式的专用 SDKs 上。
- en: Software Development Kits (SDKs)
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 软件开发工具包（SDKs）
- en: SDKs are libraries (or micro-frameworks) that are imported into a codebase to
    streamline an action, activity, or otherwise complex operation. They are also
    known by another name, *clients*. Take the example from earlier about using good
    error messages and error codes. This process is necessary in order *to inform
    a client* that their prior action was invalid, however it can be advantageous
    to add appropriate guard rails directly into an SDK to reduce the surface area
    of any potential problems. For example, let’s say we have an API setup to track
    customer’s coffee related behavior through event tracking.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: SDKs 是导入到代码库中的库（或微框架），用于简化某个操作、活动或其他复杂操作。它们也被称为 *客户端*。以之前提到的使用良好错误信息和错误代码为例。这个过程是必要的，用于
    *通知客户端* 他们之前的操作是无效的，但直接将适当的保护措施添加到 SDK 中可以减少潜在问题的表面面积。例如，假设我们有一个 API 设置来通过事件跟踪跟踪客户的咖啡相关行为。
- en: Reducing User Error with SDK Guardrails
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过 SDK 保护措施减少用户错误
- en: A client SDK can theoretically include *all the tools necessary* to manage the
    interactions with the API server, including authentication, authorization, and
    as for validation, if the SDK does its job, the validation issues would go out
    the door. The following code snippet shows an example SDK that could be used to
    reliably track customer events.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，客户端 SDK 可以包含 *管理与 API 服务器交互所需的所有工具*，包括身份验证、授权，而对于验证来说，如果 SDK 发挥作用，验证问题将迎刃而解。以下代码片段展示了一个可以可靠地跟踪客户事件的
    SDK 示例。
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: With some additional work (aka the client SDK), the problem of data validation
    or event corruption can just about go away entirely. Additional problems can be
    managed within the SDK itself like for example how to retry sending a request
    in the case of the server being offline. Rather than having all requests retry
    immediately, or in some loop that floods a gateway load balancer indefinitely,
    the SDK can take smarter actions like employing exponential backoff. See “The
    Thundering Herd Problem” for a dive into what goes wrong when things go, well,
    wrong!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一些额外的工作（即客户端 SDK），数据验证或事件损坏的问题几乎可以完全消除。额外的问题可以在 SDK 内部处理，例如当服务器离线时如何重试发送请求。与其让所有请求立即重试，或在某个循环中无限制地淹没网关负载均衡器，SDK
    可以采取更智能的措施，比如采用指数回退。有关当事情出错时会发生什么的深入探讨，请参见“**雷鸣之 Herd 问题**”！
- en: '**The Thundering Herd Problem**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**雷鸣之 Herd 问题**'
- en: Let’s say we have a single gateway API server. You’ve written a fantastic API
    and many teams across the company are sending event data to this API. Things are
    going well until one day a new internal team starts to send invalid data to the
    server (and instead of respecting your http status codes, they treat all non-200
    http codes as a reason to retry. But wait, they forgot to add any kind of retry
    heuristics like exponential backoff, so all requests just retry indefinitely —
    across an ever increasing retry queue). Mind you, before this new team came on
    board there was never a reason to run more than one instance of the API server,
    and there was never a need to use any sort of service level rate limiter either,
    because everything was running smoothly within the agreed upon SLAs.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 比如说，我们有一个单一的网关 API 服务器。你编写了一个很棒的 API，公司的许多团队都在向这个 API 发送事件数据。事情进展顺利，直到有一天，一个新的内部团队开始向服务器发送无效数据（他们没有尊重你的
    http 状态码，而是将所有非 200 的 http 代码视为重试的理由。不过，他们忘记添加任何重试启发式算法，如指数回退，因此所有请求都会无限重试——在一个不断增加的重试队列中）。请注意，在这个新团队加入之前，根本没有理由运行多个
    API 服务器实例，也没有必要使用任何服务级别的速率限制器，因为一切都在商定的 SLA 内顺利运行。
- en: '![](../Images/fe55a743c4f71ff3c3ccfce69fa0ad7f.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fe55a743c4f71ff3c3ccfce69fa0ad7f.png)'
- en: The Not-So-Fail-Whale. What can happen when you restore problems and get back
    out of the hot water again. Image via [Midjourney via the Author.](https://www.midjourney.com/app/jobs/fd36ca2e-848f-4916-8125-2d0105da8fb4/)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**非失败鲸**。当你解决问题并重新摆脱困境时可能发生的情况。图片来源 [Midjourney 通过作者](https://www.midjourney.com/app/jobs/fd36ca2e-848f-4916-8125-2d0105da8fb4/)'
- en: Well, that was before today. Now your service is offline. Data is backing up,
    upstream services are filling their queues, and people are upset because their
    services are now starting to run into issues because of your single point of failure…
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，那是今天之前的事了。现在你的服务离线了。数据在备份，上游服务正在填充他们的队列，人们很不满，因为他们的服务现在因为你的单点故障而开始出现问题……
- en: These problems all stem from a form of resource starvation coined “The Thundering
    Herd Problem”. This problem occurs when many processes are awaiting an event,
    like system resources being available, or in this example, the API server coming
    back online. Now there is a scramble as all of the processes compete to attempt
    to gain resources, and in many cases the load on the single process (api server)
    is enough to take the service back offline again. Unfortunately, starting the
    cycle of resource starvation over again. This is of course unless you can calm
    the herd or distribute the load over a larger number of working processes which
    decreases the load across the network to the point where the resources have room
    to breathe again.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题都源于一种资源枯竭形式，称为“**雷鸣之 Herd 问题**”。当许多进程在等待某个事件时，比如系统资源变得可用，或者在这个例子中，API 服务器重新上线时，就会发生这个问题。现在，所有进程争先恐后地竞争资源，在许多情况下，单个进程（API
    服务器）上的负载足以使服务再次离线。不幸的是，这会导致资源枯竭的循环重新开始。除非你能平息“ Herd”现象或将负载分布到更多的工作进程上，从而减少网络负载，使资源再次有空间呼吸。
- en: While the initial example above is more of an unintentional [distributed denial
    of service attack](https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack/)
    (DDoS), these kinds of problems can be solved at the client (with exponential
    backoff or self-throttling) and at the API edge via load balancing and rate limiting.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然上面的初始示例更多的是一种无意的 [分布式拒绝服务攻击](https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack/)（DDoS），这些问题可以在客户端（通过指数回退或自我限流）和
    API 边缘通过负载均衡和速率限制来解决。
- en: Ultimately, without the right set of eyes and ears, enabled by operational metrics,
    monitors and system level ([SLAs/SLIs/SLOs](https://cloud.google.com/blog/products/devops-sre/sre-fundamentals-slis-slas-and-slos))
    alerting, data can play the disappearing act, and this can be a challenge to resolve.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Whether you decide to add a *data gateway API* to the edge of your data network,
    employ a *custom SDK* for upstream consistency and accountability, or decide to
    take an alternative approach when it comes to dealing with getting data into your
    data platform it is still good to know what your options are. Regardless of the
    path in which data is emitted into your data streams this introduction to streaming
    data wouldn’t be complete without a proper discussion of data formats, protocols,
    and the topic of binary serializable data. Who knows we may just uncover a better
    approach to handling our data accountability problem!
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the Right Data Protocol for The Job
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you think of structured data the first thing to come to mind might be JSON
    data. JSON data has structure, is a standard web-based data protocol, and if nothing
    else it is super easy to work with. These are all benefits in terms of getting
    started quickly, but over time, and without the appropriate safeguards in place,
    you could face problems when it comes to standardizing on JSON for your streaming
    systems.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: The Love / Hate Relationship with JSON
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first problem is that JSON data is mutable. This means as a data structure
    it is flexible and therefore fragile. Data must be consistent to be accountable,
    and in the case of transferring data across a network (on-the-wire) the serialized
    format (binary representation) should be highly compactable. With JSON data, you
    must send the keys (for all fields) for each object represented across the payload.
    Inevitably this means that you’ll typically be sending a large amount of additional
    weight for each additional record (after the first) in a series of objects.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, this is not a new problem, and it just so happens that there are best
    practices for these kinds of things, and multiple schools of thought regarding
    what is the best strategy for optimally serializing data. This is not to say that
    JSON doesn’t have its merits. Just when it comes to laying a solid data foundation
    the more structure the better and the higher level of compaction the better as
    long as it doesn’t burn up a lot of CPU cycles.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Serializable Structured Data
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When it comes to efficiently encoding and transferring binary data two serialization
    frameworks tend to always come up: [Apache Avro](https://avro.apache.org/) and
    Google [Protocol Buffers](https://developers.google.com/protocol-buffers) (protobuf).
    Both libraries provide CPU efficient techniques for serializing row-based data
    structures, and in addition to both technologies also provide their own *remote
    procedure call* (RPC) frameworks and capabilities. Let’s look at *avro*, then
    *protobuf*, and we will wrap up looking at *remote procedure calls*.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在高效编码和传输二进制数据时，两种序列化框架常常被提及：[Apache Avro](https://avro.apache.org/)和Google [Protocol
    Buffers](https://developers.google.com/protocol-buffers)（protobuf）。这两个库提供了CPU高效的行数据结构序列化技术，并且这两种技术还提供了各自的*远程过程调用*（RPC）框架和功能。让我们先来看*avro*，然后是*protobuf*，最后再看看*远程过程调用*。
- en: Avro Message Format
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Avro消息格式
- en: With [*avro*](https://avro.apache.org/), you define declarative schemas for
    your structured data using the concept of records. These records are simply JSON
    formatted data definitions files (schemas) stored with the file type *avsc*. The
    following example shows a Coffee schema in the avro descriptor format.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[*avro*](https://avro.apache.org/)时，你可以通过记录的概念定义结构化数据的声明式模式。这些记录只是JSON格式的数据定义文件（模式），存储为文件类型*avsc*。以下示例展示了avro描述符格式中的Coffee模式。
- en: '[PRE2]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Working with avro data can take two paths that diverge relating to how you want
    to work at runtime. You can take the compile time approach, or the figure things
    out on-demand at runtime. This enables a flexibility that can enhance an interactive
    data discovery session. For example, avro was originally created as an efficient
    data serialization protocol for storing large collections of data, as partitioned
    files, long-term within the Hadoop file system. Given data was typically read
    from one location, and written to another, within HDFS, avro could store the schema
    (used at write time) once per file.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 处理avro数据可以有两种路径，取决于你在运行时如何工作。你可以采取编译时的方法，或者在运行时按需解决问题。这种灵活性可以增强交互式数据发现会话。例如，avro最初被创建为一种高效的数据序列化协议，用于在Hadoop文件系统中长期存储大型数据集合，以分区文件的形式存储。由于数据通常是从一个位置读取，写入到HDFS中的另一个位置，avro可以每个文件存储一次模式（用于写入时）。
- en: Avro Binary Format
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Avro二进制格式
- en: When you write a collection of avro records to disk the process encodes the
    schema of the avro data directly into the file itself (once). There is a similar
    process when it comes to Parquet file encoding, where the schema is compressed
    and written as a binary file footer. We saw this process firsthand, at the end
    of chapter 4, when we went through the process of adding StructField level documentation
    to our *StructType*. This schema was used to encode our DataFrame, and when we
    wrote to disk it preserved our inline documentation on the next read.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将一组avro记录写入磁盘时，该过程会将avro数据的模式直接编码到文件中（仅一次）。在Parquet文件编码中也有类似的过程，模式会被压缩并作为二进制文件尾部写入。我们在第4章结束时，经历了将StructField级文档添加到我们的*StructType*的过程。这个模式用于编码我们的DataFrame，并且在写入磁盘时，它在下一次读取时保留了我们的内联文档。
- en: Enabling Backwards Compatibility and Preventing Data Corruption
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启用向后兼容性并防止数据损坏
- en: In the case of reading multiple files, as a single collection, problems can
    arise in the case of schema changes between records. Avro encodes binary records
    as byte arrays and applies a schema to the data at the time of deserialization
    (conversation back from a byte array into an object).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在读取多个文件作为单一集合的情况下，当记录之间的模式发生变化时，可能会出现问题。Avro将二进制记录编码为字节数组，并在反序列化时（将字节数组转换回对象）应用模式。
- en: This means you taking extra precaution to preserve backwards compatibility,
    otherwise you’ll find yourself running into issues with *ArrayIndexOutOfBounds*
    exceptions.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着你需要额外注意以保持向后兼容，否则你可能会遇到*ArrayIndexOutOfBounds*异常。
- en: Broken schema promises can happen in other subtle ways too. For example, say
    you need to change an integer value to a long value for a specific field in your
    schema. Don’t. This will break backwards compatibility due to the increase in
    byte size from an int to a long. This is due to the use of the schema definition
    for defining the starting and ending position in the byte array for each field
    of a record. To maintain backwards compatibility, you’ll need to deprecate the
    use of the integer field moving forwards (while preserving it in your avro definition)
    and add (append) a new field to the schema to use moving forwards.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Best Practices for Streaming Avro Data
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Moving from static avro files, with their useful embedded schemas, to an unbounded
    stream of well *binary data*, the main differentiator is that you need to *bring
    your own schema to the party*. This means that you’ll need to support backwards
    compatibility (in the case that you need to rewind and reprocess data before and
    after a schema change), as well as forward compatibility, in the case that you
    have existing readers already consuming from a stream.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: The challenge here is support both forms of compatibility given that avro doesn’t
    have the ability to ignore unknown fields, which is a requirement for supporting
    forward compatibility. In order to support these challenges with avro, the folks
    at Confluence open-sourced their [schema registry](https://docs.confluent.io/platform/current/schema-registry/index.html)
    (for use with Kafka) which enables schema versioning at the Kafka topic (data
    stream) level.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: When supporting avro without a schema registry, you’ll have to ensure you’ve
    updated any active readers (spark applications or otherwise) to use the new version
    of the schema prior to updating the schema library version on your writers. The
    moment you flip the switch otherwise, you could find yourself at the start of
    an incident.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Protobuf Message Format
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With protobuf, you define your structured data definitions using the concept
    of messages. Messages are written in a format that feels more like defining a
    struct in C. These message files are written into files with the proto filename
    extension. Protocol Buffers have the advantage of using *imports*. This means
    you can define common message types and enumerations, that can be used within
    a large project, or even imported into external projects enabling wide scale reuse.
    A simple example of creating the Coffee record (message type) using protobuf.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: With protobuf you define your messages once, and then compile down for your
    programming language of choice. For example, we can generate code for Scala using
    the coffee.proto file using the standalone compiler from the [ScalaPB](https://scalapb.github.io/)
    project (*created and maintained by* [*Nadav Samet*](https://www.linkedin.com/in/nadav-samet/)),
    or utilize the brilliance of [Buf](https://buf.build/), which created an invaluable
    set of tools and utilities around protobuf and grpc.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Code Generation
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Compiling protobuf enables simple code generation. The following example is
    taken from the /ch-09/data/protobuf directory. The directions in the chapter READMEj
    covers how to install ScalaPB and includes the steps to set the correct environment
    variables to execute the command.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This process saves time in the long run by freeing you up from having to write
    additional code to serialize and deserialize your data objects (across language
    boundaries or within different code bases).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Protobuf Binary Format
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The serialized (binary wire format) is [encoded](https://developers.google.com/protocol-buffers/docs/encoding)
    using the concept of binary field level separators. These separators are used
    as markers that identify the data types encapsulated within a serialized protobuf
    message. In the example, coffee.proto, you probably noticed that there was an
    indexed marker next to each field type (string id = 1;), this is used to assist
    with encoding / decoding of messages on / off the wire. This means there is a
    little additional overhead compared to the avro binary, but if you read over the
    [encoding specification](https://developers.google.com/protocol-buffers/docs/encoding),
    you’ll see that other efficiencies more than make up for any additional bytes
    (such as bit packing, efficient handling of numeric data types, and special encoding
    of the first 15 indices for each message). With respect to using protobuf as your
    binary protocol of choice for streaming data the pros far outweigh the cons in
    the grand scheme of things. One of the ways in which it more than makes up for
    itself is with support for both backwards and forwards compatibility.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Enabling Backwards Compatibility and Preventing Data Corruption
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are similar rules to keep in mind when it comes to modifying your protobuf
    schemas like we discussed with avro. As a rule of thumb, you can change the name
    of a field, but you never change the type or change the position (index) unless
    you want to break backwards compatibility. These rules can be overlooked when
    it comes to supporting any kind of data in the long term and can be especially
    difficult as teams become more proficient with their use of protobuf. There is
    this need to rearrange, and optimize, that can come back to bite you if you are
    not careful. (See the Tip below called *Maintaining Data Quality Over Time* for
    more context).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Best Practices for Streaming Protobuf Data
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given protobuf supports both **backwards* and **forwards* compatibility, this
    means that you can deploy new writers without having to worry about updating your
    readers first, and the same is true of your readers, you can update them with
    newer versions of your protobuf definitions without worrying about a complex deploy
    of all your writers. Protobuf supports forward compatibility using the notion
    of unknown fields. This is an additional concept that doesn’t exist within the
    avro specification, and it is used to track the indices and associated bytes it
    was unable to parse due to the divergence between the local version of the protobuf
    and the version it is currently reading. The beneficial thing here is that you
    can also *opt-in*, at any point, to newer changes in the protobuf definitions.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于 protobuf 支持 **向后** 和 **向前** 兼容，这意味着你可以在不担心首先更新读者的情况下部署新的写入器，读者也是如此，你可以用更新版本的
    protobuf 定义来更新它们，而不必担心所有写入器的复杂部署。Protobuf 通过未知字段的概念支持向前兼容。这是 avro 规范中不存在的附加概念，用于跟踪由于
    protobuf 本地版本与当前读取版本之间的差异而无法解析的索引和相关字节。这里的好处是你可以在任何时候 *选择* 更新 protobuf 定义中的新更改。
- en: For example, say you have two streaming applications (a) and (b). Application
    (a) is processing streaming data from an upstream Kafka topic (x), enhancing each
    record with additional information, and then writing it out to a new Kafka topic
    (y). Now, application (b) reads from (y) and does its thing. Say there is a newer
    version of the protobuf definition, and application (a) has yet to be updated
    to the newest version, while the upstream Kafka topic (x) and application (b)
    are already updated and expecting to use some new fields available from the upgrade.
    The amazing thing is that it is still possible to pass the unknown fields through
    application (a) and onto application (b) without even knowing they exist.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，假设你有两个流应用程序 (a) 和 (b)。应用程序 (a) 正在处理来自上游 Kafka 主题 (x) 的流数据，为每条记录增加额外的信息，然后将其写入一个新的
    Kafka 主题 (y)。现在，应用程序 (b) 从 (y) 中读取数据并执行其操作。假设有一个更新的 protobuf 定义版本，而应用程序 (a) 尚未更新到最新版本，而上游
    Kafka 主题 (x) 和应用程序 (b) 已经更新，并期望使用从升级中获得的新字段。令人惊讶的是，依然可以将未知字段通过应用程序 (a) 传递到应用程序
    (b)，甚至不需要知道它们的存在。
- en: See *“Tips for maintaining good data quality over time”* for an additional deep
    dive.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 参见 *“维护数据质量的技巧”* 以获取更多深入的探讨。
- en: '**Tip: Maintaining Data Quality over Time**'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示：维护数据质量的技巧**'
- en: When working with either *avro* or *protobuf*, you should treat the schemas
    no different than you would code you want to push to production. This means creating
    a project that can be committed to your companies *github* (or whatever version
    control system you are using), and it also means you *should* write unit tests
    for your schemas. Not only does this provides living examples of how to use each
    message type, but the important reason for testing your data formats is to ensure
    that changes to the schema don’t break backwards compatibility. The icing on the
    cake is that in order to unit test the schemas you’ll need to first compile the
    (.avsc or .proto) files and use the respective library code generation. This makes
    it easier to create releasable library code, and you can also use release versioning
    (version 1.0.0) to catalog each change to the schemas.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 *avro* 或 *protobuf* 时，你应该像对待需要推送到生产环境的代码一样对待这些模式。这意味着要创建一个可以提交到你公司 *github*（或你使用的任何版本控制系统）的项目，这也意味着你
    *应该* 为你的模式编写单元测试。这不仅提供了如何使用每种消息类型的实际示例，而且测试数据格式的一个重要原因是确保模式的更改不会破坏向后兼容性。更进一步的是，为了单元测试模式，你需要首先编译
    (.avsc 或 .proto) 文件，并使用相应的库代码生成。这使得创建可发布的库代码变得更加容易，你还可以使用发布版本（版本 1.0.0）来记录每次对模式的更改。
- en: One simple method to enable this process is by serializing and storing a binary
    copy of each message, across all schema changes, as part of the project lifecycle.
    I have found success adding this step directly into the unit tests themselves,
    using the test suite to create, read and write these records directl into the
    project test resources directory. This way each binary version, across all schema
    changes, is available within the code base itself.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的方法是通过在项目生命周期内序列化并存储每个消息的二进制副本，来使这一过程得以实现。我发现将这一步骤直接添加到单元测试中很有效，利用测试套件创建、读取和写入这些记录到项目测试资源目录中。这样，每个二进制版本在所有模式更改中都可以在代码库中找到。
- en: With a little extra upfront effort, you can save yourself a lot of pain in the
    grand scheme of things, and rest easy at night knowing your data is safe (at least
    on the producing and consuming side of the table)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一点额外的前期努力，你可以在整体方案中节省大量麻烦，并且可以安心入睡，知道你的数据是安全的（至少在生产和消费方面）。
- en: Using Buf Tooling and Protobuf in Spark
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Buf 工具和 Protobuf 在 Spark 中
- en: Since writing this chapter back in 2021, **Buf Build** ([https://buf.build/](https://buf.build/))
    has materialized into the *all-things-protobuf* company. Their tooling is simple
    to use, free-and-open-source, and appeared at just the right time to power a few
    initatives in the Spark community. The [Apache Spark](https://spark.apache.org/)
    project introduced full native support for [Protocol Buffers in Spark 3.4](https://github.com/apache/spark/tree/v3.4.1/connector/protobuf/src/main/scala/org/apache/spark/sql/protobuf)
    in order to support the [spark-connect](https://spark.apache.org/docs/latest/spark-connect-overview.html),
    and are using Buf for compiling GRPC services and messages. Spark Connect is after
    all a GRPC native connector for embedding Spark applications outside of the JVM.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 自从 2021 年撰写本章以来，**Buf Build** ([https://buf.build/](https://buf.build/)) 已经发展成了*一切
    protobuf* 公司。他们的工具简单易用，免费且开源，并在合适的时机出现，为 Spark 社区的一些倡议提供了支持。[Apache Spark](https://spark.apache.org/)
    项目引入了对 [Protocol Buffers in Spark 3.4](https://github.com/apache/spark/tree/v3.4.1/connector/protobuf/src/main/scala/org/apache/spark/sql/protobuf)
    的全面原生支持，以支持 [spark-connect](https://spark.apache.org/docs/latest/spark-connect-overview.html)，并使用
    Buf 编译 GRPC 服务和消息。毕竟，Spark Connect 是一个 GRPC 原生连接器，用于嵌入 JVM 外的 Spark 应用程序。
- en: Traditional Apache Spark application must run as a driver application somewhere,
    and in the past this meant using **pyspark** or native spark, which in both cases
    still run on top of a JVM process.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的 Apache Spark 应用必须在某处作为驱动程序应用运行，过去这意味着使用**pyspark** 或原生 spark，这两者仍然运行在 JVM
    进程之上。
- en: '![](../Images/cec7730d94bae772f26b9508103edc50.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cec7730d94bae772f26b9508103edc50.png)'
- en: '[Directory structure](https://github.com/apache/spark/tree/v3.4.1/connector/connect/common/src/main)
    via Spark Connect. Shows the protobuf definitions, along with buf.gen.yaml and
    buf.work.yaml which help with code generation.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[目录结构](https://github.com/apache/spark/tree/v3.4.1/connector/connect/common/src/main)
    通过 Spark Connect 显示了 protobuf 定义，以及 buf.gen.yaml 和 buf.work.yaml，这些文件有助于代码生成。'
- en: 'At the end of the day, Buf Build enables peace of mind in the build process.
    In order to generate the code, one must run a simple command: `buf generate` .
    For simple linting and consistent formatting, `buf lint && buf format -w` . The
    icing on the cake however is the breaking change detection. `buf breaking --against
    .git#branch=origin/main` is all it takes to ensure that new changes to your message
    definitions won’t negatively affect anything that is currently running in production.
    *In the future, I will do a write up on using **buf** for enterprise analytics,
    but for now, it is time to conclude this chapter.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 到头来，Buf Build 为构建过程带来了安心。为了生成代码，只需运行一个简单的命令：`buf generate`。对于简单的 lint 检查和一致的格式化，可以使用
    `buf lint && buf format -w`。不过，最棒的是变更检测。`buf breaking --against .git#branch=origin/main`
    就可以确保对消息定义的新更改不会对当前在生产环境中运行的内容产生负面影响。*未来，我会写一篇关于使用**buf**进行企业分析的文章，但现在，是时候结束这一章了。*
- en: So where were we. You now know that there are benefits to using avro or protobuf
    when it comes to your long-term data accountability strategy. By using these language
    agnostic, row-based, structured data formats you reduce the problem of long-term
    language lock-in, leaving the doors open to whatever the programing language is
    later down the line. Cause honestly it can be a thankless task to be supporting
    legacy libraries and code bases. Additionally, the serialized formats help to
    reduce the network bandwidth costs and congestion associated with sending and
    receiving large amounts of data. This helps as well to reduce the storage overhead
    costs for retaining your data long-term.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们刚才讲到哪里了？你现在知道，在长期数据管理策略中使用 avro 或 protobuf 有其好处。通过使用这些与语言无关的、基于行的结构化数据格式，你减少了长期语言锁定的问题，为将来的任何编程语言打开了大门。说实话，支持遗留库和代码库是一项难以回报的任务。此外，序列化格式有助于减少与发送和接收大量数据相关的网络带宽成本和拥堵。这也有助于降低长期存储数据的开销。
- en: Lastly, let’s look at how these structured data protocols enable additional
    efficiencies when it comes to sending and receiving data across the network using
    remote procedure calls.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看这些结构化数据协议如何在通过远程过程调用发送和接收数据时实现额外的效率。
- en: Remote Procedure Calls
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 远程过程调用
- en: '*RPC* frameworks, in a nutshell, enable *client* applications to transparently
    call *remote* (server-side) methods (procedures) via local function calls by passing
    serialized messages back and forth. The *client* and *server-side* implementations
    use the same *public interface* definition to define the functional *RPC* methods
    and services available. The Interface Definition Language (IDL) defines the protocol
    and message definitions and acts as a contract between the client and server-side.
    Let’s see this in action looking at the popular open-source RPC framework [gRPC](https://grpc.io/).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*RPC* 框架，总的来说，使得 *客户端* 应用程序可以通过本地函数调用透明地调用 *远程*（服务器端）方法（过程），通过来回传递序列化的消息。*客户端*
    和 *服务器端* 实现使用相同的 *公共接口* 定义来定义可用的功能性 *RPC* 方法和服务。接口定义语言（IDL）定义协议和消息定义，并作为客户端和服务器端之间的契约。让我们通过流行的开源
    RPC 框架 [gRPC](https://grpc.io/) 来看看实际情况。'
- en: gRPC
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: gRPC
- en: First conceptualized and created at Google, [gRPC](https://grpc.io/) which stands
    for “generic” remote procedure call, is a robust open-source framework being used
    for high performance services ranging from distributed database coordination,
    as seen with [CockroachDB](https://www.cockroachlabs.com/docs/stable/architecture/distribution-layer.html),
    to real-time analytics, as seen with [Microsofts Azure Video Analytics](https://docs.microsoft.com/en-us/azure/media-services/live-video-analytics-edge/grpc-extension-protocol).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 首先由 Google 设想并创建的，[gRPC](https://grpc.io/) 即“通用”远程过程调用，是一个强大的开源框架，用于高性能服务，从分布式数据库协调（如
    [CockroachDB](https://www.cockroachlabs.com/docs/stable/architecture/distribution-layer.html)）到实时分析（如
    [微软 Azure 视频分析](https://docs.microsoft.com/en-us/azure/media-services/live-video-analytics-edge/grpc-extension-protocol)）。
- en: '![](../Images/bb0e956f0dcd4a9d8154939f7d47bad4.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bb0e956f0dcd4a9d8154939f7d47bad4.png)'
- en: '**Figure 1–2**. RPC (in this example gRPC) works by passing serializing messages
    to and from a client and server. The client implements the same Interface Definition
    Language (IDL) interface and this acts as an API contract between the client and
    server. (photo credit: [https://grpc.io/docs/what-is-grpc/introduction/)](https://grpc.io/docs/what-is-grpc/introduction/))'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1–2**。RPC（在此示例中是 gRPC）通过在客户端和服务器之间传递序列化消息来工作。客户端实现相同的接口定义语言（IDL）接口，这作为客户端和服务器之间的
    API 合同。（图片来源: [https://grpc.io/docs/what-is-grpc/introduction/)](https://grpc.io/docs/what-is-grpc/introduction/))'
- en: The diagram shown in Figure 9–3 shows an example of gRPC at work. The server-side
    code is written in C++ for speed, while clients written in both ruby and java
    can interoperate with the service using protobuf messages as their means of communicating.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9–3 显示了 gRPC 工作的示例。服务器端代码使用 C++ 编写以提高速度，而用 ruby 和 java 编写的客户端可以使用 protobuf
    消息作为通信手段与服务进行互操作。
- en: Using protocol buffers for message definitions, serialization, as well as the
    declaration and definition of services, gRPC can simplify how you capture data
    and build services. For example, let’s say we wanted to continue the exercise
    of creating a tracking API for customer coffee orders. The API contract could
    be defined in a simple services file, and from there the server-side implementation
    and any number of client-side implementations could be built using the same service
    definition and message types.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 使用协议缓冲区进行消息定义、序列化以及服务的声明和定义，gRPC 可以简化你捕获数据和构建服务的方式。例如，假设我们想继续创建一个用于跟踪客户咖啡订单的
    API。API 合同可以在一个简单的服务文件中定义，从而可以使用相同的服务定义和消息类型构建服务器端实现和任意数量的客户端实现。
- en: Defining a gRPC Service
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义 gRPC 服务
- en: You can define a service interface, the request and response objects, as well
    as the message types that need to be passed between the client and server as easily
    as 1–2–3.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以像 1–2–3 一样轻松定义服务接口、请求和响应对象以及需要在客户端和服务器之间传递的消息类型。
- en: '[PRE5]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: With the addition of gRPC, it can be much easier to implement, and maintain
    both the server-side and client-side code used within your data infrastructure.
    Given that protobuf supports backwards and forwards compatibility, this means
    that older gRPC clients can still send valid messages to newer gRPC services without
    running into common problems and pain points (discussed earlier under “Data Problems
    in Flight”).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 通过添加 gRPC，实现和维护数据基础设施中使用的服务器端和客户端代码将变得更容易。由于 protobuf 支持向后和向前兼容，这意味着旧版 gRPC
    客户端仍可以向新版 gRPC 服务发送有效消息，而不会遇到常见问题和痛点（如之前在“飞行中的数据问题”中讨论的）。
- en: gRPC speaks HTTP/2
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: gRPC 使用 HTTP/2
- en: As a bonus, with respect to modern service stacks, gRPC is able to use HTTP/2
    for its transport layer. This also means you can take advantage of modern data
    meshes (like [Envoy](https://www.envoyproxy.io/)) for proxy support, routing and
    service level [authentication](https://www.envoyproxy.io/docs/envoy/v1.17.2/api-v2/config/filter/http/ext_authz/v2/ext_authz.proto#envoy-api-file-envoy-config-filter-http-ext-authz-v2-ext-authz-proto),
    all while also reduce the problems of TCP packet congestion seen with standard
    HTTP over TCP.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 作为额外的好处，对于现代服务栈，gRPC 能够使用 HTTP/2 作为其传输层。这也意味着你可以利用现代数据网格（如 [Envoy](https://www.envoyproxy.io/)）进行代理支持、路由和服务级别的
    [认证](https://www.envoyproxy.io/docs/envoy/v1.17.2/api-v2/config/filter/http/ext_authz/v2/ext_authz.proto#envoy-api-file-envoy-config-filter-http-ext-authz-v2-ext-authz-proto)，同时减少标准
    HTTP over TCP 中的 TCP 数据包拥塞问题。
- en: Mitigating data problems in flight and achieving success when it comes to data
    accountability starts with the data and fans outwards from that central point.
    Putting processes in place when it comes to how data can enter into your data
    network should be considered a prerequisite to check off before diving into the
    torrent of streaming data.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 减轻飞行中的数据问题并在数据责任方面取得成功，从数据开始，并从那个中心点向外扩展。对于如何将数据引入你的数据网络，建立相应的流程应该被视为在投入流数据之前需要完成的前提条件。
- en: Summary
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: The goal of this post is to present the moving parts, concepts, and background
    information required to arm ourselves before blindly leaping from a more traditional
    (stationary) batch-based mindset to one that understandings the risks and rewards
    of working with real-time streaming data.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的目标是呈现所需的移动部件、概念和背景信息，以便在盲目从传统的（静态）批处理思维方式跳跃到理解实时流数据风险和回报的方式之前，武装好自己。
- en: Harnessing data in real-time can lead to fast, actionable insights, and open
    the doors to state-of-the-art machine learning and artificial intelligence.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 实时利用数据可以带来快速、可操作的洞察，并打开通往最先进的机器学习和人工智能的大门。
- en: However, distributed data management can also become a data crisis if the right
    steps aren’t taken into consideration ahead of time. Remember that without a strong,
    solid data foundation, built on top of valid (trustworthy) data, that the road
    to real-time will not be a simple endeavor, but one has its fair share of bumps
    and detours along the way.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，分布式数据管理如果未考虑正确的步骤，也可能成为数据危机。请记住，如果没有建立在有效（值得信赖）数据之上的强大、坚实的数据基础，实时数据的道路将不是一条简单的道路，而是充满了颠簸和绕行。
- en: I hope you enjoyed the second half of Chapter 9\. To read the first part of
    this series, head on over to [A Gentle Introduction to Analytical Stream Processing](https://medium.com/towards-data-science/a-gentle-introduction-to-stream-processing-f47912a2a2ea).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你喜欢第 9 章的下半部分。要阅读本系列的第一部分，请前往 [对分析流处理的温和介绍](https://medium.com/towards-data-science/a-gentle-introduction-to-stream-processing-f47912a2a2ea)。
- en: '[](/a-gentle-introduction-to-stream-processing-f47912a2a2ea?source=post_page-----db58b3694263--------------------------------)
    [## A Gentle Introduction to Analytical Stream Processing'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/a-gentle-introduction-to-stream-processing-f47912a2a2ea?source=post_page-----db58b3694263--------------------------------)
    [## 对分析流处理的温和介绍'
- en: Building a Mental Model for Engineers and Anyone in Between
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为工程师及其他人构建心理模型
- en: towardsdatascience.com](/a-gentle-introduction-to-stream-processing-f47912a2a2ea?source=post_page-----db58b3694263--------------------------------)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/a-gentle-introduction-to-stream-processing-f47912a2a2ea?source=post_page-----db58b3694263--------------------------------)
- en: — — — — — — — — — — — — — — — — — — — — — — — —
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '— — — — — — — — — — — — — — — — — — — — — — — — '
- en: If you want to find dig in even deeper, please check out my book, or support
    me with a high five.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想深入了解，请查看我的书，或给我一个击掌支持。
- en: '[](https://www.amazon.com/Modern-Engineering-Apache-Spark-Hands/dp/1484274512?source=post_page-----db58b3694263--------------------------------)
    [## Modern Data Engineering with Apache Spark: A Hands-On Guide for Building Mission-Critical
    Streaming…'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'Amazon.com: Modern Data Engineering with Apache Spark: A Hands-On Guide for
    Building Mission-Critical Streaming…'
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.amazon.com](https://www.amazon.com/Modern-Engineering-Apache-Spark-Hands/dp/1484274512?source=post_page-----db58b3694263--------------------------------)
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: If you have access to [O’Reilly Media](https://medium.com/u/fbfa235a954c?source=post_page-----db58b3694263--------------------------------)
    then you can also read the book entirely for free (good for you, not so good for
    me), but please find the book for free somewhere if you have the opportunity,
    or get an ebook to save on shipping cost (or needing to find a place for a 600+
    page book).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://learning.oreilly.com/library/view/modern-data-engineering/9781484274521/?source=post_page-----db58b3694263--------------------------------)
    [## Modern Data Engineering with Apache Spark: A Hands-On Guide for Building Mission-Critical
    Streaming…'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Leverage Apache Spark within a modern data engineering ecosystem. This hands-on
    guide will teach you how to write fully…
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: learning.oreilly.com](https://learning.oreilly.com/library/view/modern-data-engineering/9781484274521/?source=post_page-----db58b3694263--------------------------------)
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
