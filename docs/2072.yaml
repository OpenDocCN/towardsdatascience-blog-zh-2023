- en: 'Efficient Image Segmentation Using PyTorch: Part 1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PyTorch 的高效图像分割：第 1 部分
- en: 原文：[https://towardsdatascience.com/efficient-image-segmentation-using-pytorch-part-1-89e8297a0923?source=collection_archive---------1-----------------------#2023-06-27](https://towardsdatascience.com/efficient-image-segmentation-using-pytorch-part-1-89e8297a0923?source=collection_archive---------1-----------------------#2023-06-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/efficient-image-segmentation-using-pytorch-part-1-89e8297a0923?source=collection_archive---------1-----------------------#2023-06-27](https://towardsdatascience.com/efficient-image-segmentation-using-pytorch-part-1-89e8297a0923?source=collection_archive---------1-----------------------#2023-06-27)
- en: Concepts and Ideas
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概念与想法
- en: '[](https://medium.com/@dhruvbird?source=post_page-----89e8297a0923--------------------------------)[![Dhruv
    Matani](../Images/d63bf7776c28a29c02b985b1f64abdd3.png)](https://medium.com/@dhruvbird?source=post_page-----89e8297a0923--------------------------------)[](https://towardsdatascience.com/?source=post_page-----89e8297a0923--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----89e8297a0923--------------------------------)
    [Dhruv Matani](https://medium.com/@dhruvbird?source=post_page-----89e8297a0923--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@dhruvbird?source=post_page-----89e8297a0923--------------------------------)[![Dhruv
    Matani](../Images/d63bf7776c28a29c02b985b1f64abdd3.png)](https://medium.com/@dhruvbird?source=post_page-----89e8297a0923--------------------------------)[](https://towardsdatascience.com/?source=post_page-----89e8297a0923--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----89e8297a0923--------------------------------)
    [Dhruv Matani](https://medium.com/@dhruvbird?source=post_page-----89e8297a0923--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F63f5d5495279&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficient-image-segmentation-using-pytorch-part-1-89e8297a0923&user=Dhruv+Matani&userId=63f5d5495279&source=post_page-63f5d5495279----89e8297a0923---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----89e8297a0923--------------------------------)
    ·18 min read·Jun 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F89e8297a0923&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficient-image-segmentation-using-pytorch-part-1-89e8297a0923&user=Dhruv+Matani&userId=63f5d5495279&source=-----89e8297a0923---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F63f5d5495279&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficient-image-segmentation-using-pytorch-part-1-89e8297a0923&user=Dhruv+Matani&userId=63f5d5495279&source=post_page-63f5d5495279----89e8297a0923---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----89e8297a0923--------------------------------)
    ·18分钟阅读·2023年6月27日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F89e8297a0923&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficient-image-segmentation-using-pytorch-part-1-89e8297a0923&user=Dhruv+Matani&userId=63f5d5495279&source=-----89e8297a0923---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F89e8297a0923&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficient-image-segmentation-using-pytorch-part-1-89e8297a0923&source=-----89e8297a0923---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F89e8297a0923&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficient-image-segmentation-using-pytorch-part-1-89e8297a0923&source=-----89e8297a0923---------------------bookmark_footer-----------)'
- en: In this 4-part series, we’ll implement image segmentation step by step from
    scratch using deep learning techniques in PyTorch. We’ll start the series with
    the basic concepts and ideas needed for image segmentation in this article.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个四部分系列中，我们将一步步地使用 PyTorch 的深度学习技术从零开始实现图像分割。我们将从本文开始介绍图像分割所需的基本概念与想法。
- en: '![](../Images/bbca4847fd8e7d2bf8725818e1f76773.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bbca4847fd8e7d2bf8725818e1f76773.png)'
- en: 'Figure 1: Pet images and their segmentation masks (Source: [The Oxford-IIIT
    Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/))'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：宠物图像及其分割掩膜（来源：[The Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/))
- en: '**Co-authored with** [**Naresh Singh**](https://medium.com/@brocolishbroxoli)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**与** [**Naresh Singh**](https://medium.com/@brocolishbroxoli) **合作撰写**'
- en: '[Image Segmentation](https://en.wikipedia.org/wiki/Image_segmentation) is a
    technique to isolate pixels belonging to specific objects in an image. Isolation
    of object pixels opens doors to interesting applications. For example, in Figure
    1, the images on the right are the masks corresponding to the pet images on the
    left where the yellow pixels belong to the pet. Once the pixels are identified,
    we could easily make the pet bigger or change the image backgrounds. This technique
    is widely used in face filter features in several social media applications.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[图像分割](https://en.wikipedia.org/wiki/Image_segmentation)是一种将图像中属于特定对象的像素隔离的技术。隔离对象像素开辟了有趣的应用。例如，在图
    1 中，右侧的图像是对应左侧宠物图像的掩码，其中黄色像素属于宠物。一旦像素被识别，我们可以轻松地放大宠物或更改图像背景。这种技术在几个社交媒体应用中的面部滤镜功能中被广泛使用。'
- en: Our goal at the end of this article series is to give the reader a sense of
    all the steps it takes to build a vision AI model and run experiments with different
    settings using PyTorch.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本系列文章结束时的目标是让读者了解构建视觉 AI 模型并使用 PyTorch 进行不同设置实验的所有步骤。
- en: Articles in this series
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本系列文章
- en: 'This series is for readers at all experience levels with deep learning. If
    you want to learn about the practice of deep learning and vision AI along with
    some solid theory and hands-on experience, you’ve come to the right place! This
    is expected to be a 4-part series with the following articles:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本系列适用于所有深度学习经验水平的读者。如果你想了解深度学习和视觉 AI 的实践，以及一些扎实的理论和实践经验，你来对地方了！预计这是一个四部分的系列，包括以下文章：
- en: '**Concepts and Ideas (this article)**'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**概念和思路（本文）**'
- en: '[A CNN-based model](https://medium.com/p/bed68cadd7c7/)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[基于 CNN 的模型](https://medium.com/p/bed68cadd7c7/)'
- en: '[Depthwise separable convolutions](https://medium.com/p/3534cf04fb89/)'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[深度可分离卷积](https://medium.com/p/3534cf04fb89/)'
- en: '[A Vision Transformer-based model](https://medium.com/p/6c86da083432/)'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[基于视觉变换器的模型](https://medium.com/p/6c86da083432/)'
- en: Introduction to Image Segmentation
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像分割简介
- en: '[Image segmentation](https://en.wikipedia.org/wiki/Image_segmentation) partitions
    or segments an image into regions that correspond to objects, backgrounds, and
    boundaries. Take a look at Figure 2 which shows a city scene. It marks regions
    corresponding to cars, motorcycles, trees, buildings, sidewalks, and other interesting
    objects with different color masks. These regions are identified through image
    segmentation techniques.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[图像分割](https://en.wikipedia.org/wiki/Image_segmentation)将图像划分或分割成对应于对象、背景和边界的区域。请查看图
    2，它展示了一个城市场景。它用不同的颜色掩码标记了对应于汽车、摩托车、树木、建筑物、人行道和其他有趣对象的区域。这些区域是通过图像分割技术识别的。'
- en: Historically, we have used [specialized image processing tools and pipelines](https://www.v7labs.com/blog/image-segmentation-guide#h4)
    to break down an image into regions. However, due to the incredible growth of
    visual data in the last two decades, deep learning has emerged as a go-to solution
    to the image segmentation task. It significantly reduces the reliance on an expert
    to build a domain-specific image segmentation strategy as was done in the past.
    A deep learning practitioner can train an image segmentation model if enough training
    data is available for the task.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 历史上，我们使用了[专用图像处理工具和流程](https://www.v7labs.com/blog/image-segmentation-guide#h4)来将图像分解为不同区域。然而，由于过去二十年来视觉数据的惊人增长，深度学习已成为图像分割任务的首选解决方案。它显著减少了对专家的依赖，以构建特定领域的图像分割策略，这在过去是必需的。只要有足够的训练数据，深度学习从业者可以训练图像分割模型。
- en: '![](../Images/3a397ef2a4ba2c4df9565de6196fe194.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a397ef2a4ba2c4df9565de6196fe194.png)'
- en: 'Figure 2: A segmented scene from the [a2d2 dataset (CC BY-ND 4.0)](https://aev-autonomous-driving-dataset.s3.eu-central-1.amazonaws.com/LICENSE.txt)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：来自[a2d2 数据集 (CC BY-ND 4.0)](https://aev-autonomous-driving-dataset.s3.eu-central-1.amazonaws.com/LICENSE.txt)的分割场景
- en: What are the applications of image segmentation?
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像分割的应用有哪些？
- en: 'Image Segmentation has applications in diverse fields such as communication,
    agriculture, transportation, healthcare, and more. Moreover, its applications
    are growing with the growth of visual data. Here are a few examples:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分割在通信、农业、交通、医疗保健等多个领域都有应用。此外，随着视觉数据的增长，它的应用也在不断增长。以下是一些例子：
- en: In **self-driving cars**, a deep learning model constantly processes the video
    feed from the car cameras to segment the scene into objects such as cars, pedestrians,
    and traffic lights which is essential for the car to operate safely.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**自动驾驶汽车**中，深度学习模型不断处理来自汽车摄像头的视频流，将场景分割成汽车、行人和交通信号灯等对象，这对于汽车安全操作至关重要。
- en: In **medical imaging**, image segmentation assists doctors to identify areas
    in the medical scans that correspond to tumors, lesions, and other abnormalities.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**医学成像**中，图像分割帮助医生识别医学扫描中的肿瘤、病变和其他异常区域。
- en: In **Zoom video calls**, it is used to preserve an individual’s privacy by replacing
    the background with virtual scenes.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**Zoom视频通话**中，利用虚拟场景替换背景以保护个人隐私。
- en: In **agriculture**, the information about weed and crop regions identified using
    image segmentation is used to [maintain healthy crop yields](https://www.sciencedirect.com/science/article/pii/S2214317323000112).
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**农业**中，通过图像分割识别的杂草和作物区域的信息被用来[保持健康的作物产量](https://www.sciencedirect.com/science/article/pii/S2214317323000112)。
- en: You can read more details about the practical applications of image segmentation
    on this [page by v7labs](https://www.v7labs.com/blog/image-segmentation-guide#h6).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[v7labs的这一页面](https://www.v7labs.com/blog/image-segmentation-guide#h6)上阅读有关图像分割实际应用的更多细节。
- en: What are the different types of image segmentation tasks?
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像分割任务的不同类型有哪些？
- en: 'There are many different types of image segmentation tasks, each with its advantages
    and disadvantages. The 2 most common types of image segmentation tasks are:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分割任务有很多不同类型，每种类型都有其优缺点。最常见的两种图像分割任务是：
- en: '**Class or Semantic segmentation**: Class Segmentation assigns a semantic class
    such as *background*, *road*, *car,* or *person* to each image pixel. If the image
    has 2 cars in it, then the pixels corresponding to both cars will be tagged as
    car pixels. It is often used for tasks such as autonomous driving and [scene understanding](https://ps.is.mpg.de/research_fields/semantic-scene-understanding).'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类别或语义分割**：类别分割为每个图像像素分配一个语义类别，如*背景*、*道路*、*汽车*或*行人*。如果图像中有2辆车，那么与两辆车对应的像素将标记为汽车像素。这通常用于自主驾驶和[场景理解](https://ps.is.mpg.de/research_fields/semantic-scene-understanding)等任务。'
- en: '**Object or Instance segmentation**: Object segmentation identifies objects
    and assigns a mask to each unique object in an image. If the image has 2 cars
    in it, then the pixels corresponding to each car will be identified as belonging
    to separate objects. Object segmentation is often used for tracking individual
    objects, such as a self-driving car programmed to follow a specific car ahead
    of it'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对象或实例分割**：对象分割识别图像中的对象，并为每个独特对象分配一个掩膜。如果图像中有2辆车，那么与每辆车对应的像素将被识别为属于不同的对象。对象分割通常用于跟踪单个对象，例如编程为跟随前方特定汽车的自动驾驶汽车。'
- en: '![](../Images/9cd15779082c7c510647f283d7f03af8.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9cd15779082c7c510647f283d7f03af8.png)'
- en: 'Figure 3: Object and class segmentations (Source: [MS Coco — Creative Commons
    Attribution license](https://cocodataset.org/#home))'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：对象和类别分割（来源：[MS Coco — 创作共享署名许可](https://cocodataset.org/#home)）
- en: In this series, we’ll focus on class segmentation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一系列中，我们将重点关注类别分割。
- en: Decisions needed to implement efficient image segmentation
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施高效图像分割所需的决策
- en: 'Efficiently training your model for speed and accuracy involves making numerous
    important decisions during the lifecycle of your project. This includes (but is
    not limited to):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 高效训练模型以提高速度和准确性涉及在项目生命周期内做出许多重要决策。这包括（但不限于）：
- en: Choice of your deep learning framework
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择你的深度学习框架
- en: Choosing a good model architecture
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个好的模型架构
- en: Selecting an effective loss function that optimizes the aspect you care about
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个有效的损失函数来优化你关心的方面
- en: Avoiding overfitting and underfitting
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 避免过拟合和欠拟合
- en: Evaluating the model’s accuracy
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型的准确性
- en: In the rest of this article, we’ll dive deeper into each of the aspects mentioned
    above and provide numerous links to articles that discuss each topic in a lot
    more detail that can be covered here.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文的其余部分，我们将深入探讨上述每一个方面，并提供大量链接，以便进一步了解每个主题。
- en: PyTorch for efficient image segmentation
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高效图像分割的PyTorch
- en: What is PyTorch?
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是PyTorch？
- en: '*“*[*PyTorch*](https://pytorch.org/) *is an open source deep learning framework
    built to be flexible and modular for research, with the stability and support
    needed for production deployment. PyTorch provides a Python package for high-level
    features like tensor computation (like NumPy) with strong GPU acceleration and
    TorchScript for an easy transition between eager mode and graph mode. With the
    latest release of PyTorch, the framework provides graph-based execution, distributed
    training, mobile deployment, and quantization.”* (source: [Meta AI page on PyTorch](https://ai.facebook.com/tools/pytorch/))'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*“*[*PyTorch*](https://pytorch.org/) *是一个开源深度学习框架，旨在灵活和模块化以便于研究，同时具备生产部署所需的稳定性和支持。PyTorch
    提供了一个 Python 包，用于高层次的特性，如张量计算（类似于 NumPy），并具有强大的 GPU 加速和 TorchScript，实现了在急切模式和图模式之间的轻松过渡。最新版本的
    PyTorch 框架提供了基于图的执行、分布式训练、移动部署和量化。”*（来源：[Meta AI 页面的 PyTorch](https://ai.facebook.com/tools/pytorch/)）'
- en: PyTorch is written in Python and C++, which makes it easy to use and learn as
    well as efficient to run. It supports a wide range of hardware platforms, including
    (server and mobile) CPUs, GPUs, and TPUs.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 是用 Python 和 C++ 编写的，这使得它既易于使用和学习，又高效运行。它支持多种硬件平台，包括（服务器和移动设备）CPU、GPU
    和 TPU。
- en: Why is PyTorch a good choice for image segmentation?
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么 PyTorch 是图像分割的好选择？
- en: 'PyTorch is a popular choice for deep learning research and development, as
    it provides a flexible and powerful environment for creating and training neural
    networks. It is a great choice of framework for implementing deep learning-based
    image segmentation due to the following features:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 是深度学习研究和开发的热门选择，因为它提供了一个灵活且强大的环境来创建和训练神经网络。它是实现基于深度学习的图像分割的绝佳框架，具有以下特点：
- en: '**Flexibility**: PyTorch is a flexible framework that allows you to create
    and train neural networks in a variety of ways. You can use pre-trained models,
    or you can create your own from scratch very easily'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活性**：PyTorch 是一个灵活的框架，允许你以多种方式创建和训练神经网络。你可以使用预训练模型，也可以非常轻松地从头开始创建自己的模型。'
- en: '**Backend support**: PyTorch supports multiple backends such as GPU/TPU hardware'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**后端支持**：PyTorch 支持多种后端，如 GPU/TPU 硬件。'
- en: '**Domain libraries**: PyTorch has a rich set of domain libraries that make
    working with specific data verticals very easy. For example, for vision (image/video)
    related AI, PyTorch provides a library called [torchvision](https://pytorch.org/vision/main/index.html)
    that we’ll use extensively throughout this series'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**领域库**：PyTorch 具有丰富的领域库，使得处理特定数据垂直领域变得非常容易。例如，对于与视觉（图像/视频）相关的 AI，PyTorch 提供了一个库
    [torchvision](https://pytorch.org/vision/main/index.html)，我们将在本系列中广泛使用。'
- en: '**Ease of use and community adoption**: PyTorch is an easy-to-use framework
    that is well-documented and has a large [community](https://dev-discuss.pytorch.org/)
    of users and developers. Many researchers use PyTorch for their experiments, and
    the results in their published papers have an implementation of the model in PyTorch
    freely available'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**易用性和社区接受度**：PyTorch 是一个易于使用的框架，文档齐全，并拥有一个大型的 [社区](https://dev-discuss.pytorch.org/)
    用户和开发者。许多研究人员在他们的实验中使用 PyTorch，他们发表的论文中有一个 PyTorch 实现的模型可以自由获取。'
- en: Choice of dataset
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集的选择
- en: 'We’re going to use the [Oxford IIIT Pet dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/)
    (licensed under CC BY-SA 4.0) for class segmentation. This dataset has 3680 images
    in the training set, and each image has a segmentation trimap associated with
    it. The trimap is one of 3 pixels classes:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 [Oxford IIIT Pet 数据集](https://www.robots.ox.ac.uk/~vgg/data/pets/)（许可协议：CC
    BY-SA 4.0）进行类别分割。这个数据集的训练集中有 3680 张图像，每张图像都有一个分割三值图。三值图分为 3 类像素：
- en: Pet
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 宠物
- en: Background
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 背景
- en: Border
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 边界
- en: We choose this dataset since it is sufficiently diverse to provide us with a
    non-trivial class segmentation task. Additionally, it’s not so complex that we
    end up spending time on things like dealing with class imbalance, etc… and lose
    track of the main problem we want to learn about and solve; namely class segmentation.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择这个数据集是因为它足够多样化，能够提供一个非平凡的类别分割任务。此外，它又不至于复杂到我们需要花时间处理类别不平衡等问题，从而失去对我们想要学习和解决的主要问题的关注；即类别分割。
- en: 'Other popular datasets used for image segmentation tasks include:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其他常用的图像分割任务数据集包括：
- en: '[Pascal VOC (visual object classes)](http://host.robots.ox.ac.uk/pascal/VOC/)'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Pascal VOC（视觉对象类别）](http://host.robots.ox.ac.uk/pascal/VOC/)'
- en: '[MS Coco](https://cocodataset.org/#home)'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[MS Coco](https://cocodataset.org/#home)'
- en: '[Cityscapes](https://www.cityscapes-dataset.com/examples/)'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Cityscapes](https://www.cityscapes-dataset.com/examples/)'
- en: Efficient image segmentation with PyTorch
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PyTorch 进行高效图像分割
- en: In this series, we’ll train multiple models for class segmentation from scratch.
    There are many considerations to account for when building and training a model
    from scratch. Below, we will look at some of the key decisions that you need to
    make when doing so.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在本系列中，我们将从头开始训练多个分类分割模型。构建和训练从头开始的模型时需要考虑许多因素。以下，我们将探讨在进行此操作时需要做出的一些关键决策。
- en: Choosing the right model for your task
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择适合你任务的模型
- en: 'There are many factors to consider when choosing the right deep-learning model
    for image segmentation. Some of the most important factors include:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 选择适合图像分割的深度学习模型时，有许多因素需要考虑。最重要的一些因素包括：
- en: '**The type of image segmentation task**: There are two main types of image
    segmentation tasks: class (semantic) segmentation and object (instance) segmentation.
    Since we’re focusing on the simpler class segmentation problem, we shall consider
    modeling our problem accordingly.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像分割任务的类型**：图像分割任务主要有两种类型：分类（语义）分割和对象（实例）分割。由于我们专注于较简单的分类分割问题，我们将根据这个问题来建模。'
- en: '**Size and complexity of the dataset**: The size and complexity of the dataset
    will affect the complexity of the model that we need to use. For example, if we
    are working with images with a small spatial dimension, we may use a simpler (or
    shallower) model, such as a fully convolutional network (FCN). If we are working
    with a large and complex dataset, we may use a more complex (or deeper) model
    such as a U-Net.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集的大小和复杂性**：数据集的大小和复杂性将影响我们需要使用的模型的复杂性。例如，如果我们处理的是空间维度较小的图像，我们可以使用较简单（或较浅）的模型，如全卷积网络（FCN）。如果我们处理的是大而复杂的数据集，我们可以使用更复杂（或更深）的模型，如
    U-Net。'
- en: '**Availability of pre-trained models**: There are many pre-trained models available
    for image segmentation. These models can be used as a starting point for our own
    model or we can use them directly. However, if we use a pre-trained model, we
    may be constrained by the spatial dimensions of the input image to the model.
    In this series, we shall focus on training a model from scratch.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预训练模型的可用性**：有许多预训练模型可用于图像分割。这些模型可以作为我们自己模型的起点，也可以直接使用。然而，如果我们使用预训练模型，可能会受到模型输入图像空间维度的限制。在本系列中，我们将重点关注从头开始训练模型。'
- en: '**Computational resources available**: Deep learning models can be computationally
    expensive to train. If we have limited computational resources, we may need to
    choose simpler models or more efficient model architectures.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可用的计算资源**：深度学习模型的训练可能计算开销较大。如果我们的计算资源有限，可能需要选择较简单的模型或更高效的模型架构。'
- en: In this series, we are going to work with the Oxford IIIT Pet dataset since
    it’s big enough for us to be able to train a medium size model and require the
    use of a GPU. We would highly recommend creating an account on [kaggle.com](https://www.kaggle.com/)
    or use [Google Colab](https://colab.research.google.com/)’s free GPU for running
    the notebooks and code referenced in this series.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一系列中，我们将使用 Oxford IIIT Pet 数据集，因为它足够大，可以训练中等规模的模型，并且需要使用 GPU。我们强烈建议在 [kaggle.com](https://www.kaggle.com/)
    上创建一个帐户，或使用 [Google Colab](https://colab.research.google.com/) 的免费 GPU 来运行本系列中提到的笔记本和代码。
- en: Model architectures
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型架构
- en: 'Here are some of the most popular deep learning model architectures for image
    segmentation:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些最受欢迎的深度学习模型架构，用于图像分割：
- en: '[**U-Net**](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/):
    The U-Net is a convolutional neural network that is commonly used for image segmentation
    tasks. It uses skip connections, which can help train the network faster and result
    in better overall accuracy. If you have to choose, U-Net is always an **excellent
    default choice**!'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**U-Net**](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/):
    U-Net 是一种常用于图像分割任务的卷积神经网络。它使用跳跃连接，这有助于加快网络训练速度并提高整体准确率。如果必须选择，U-Net 始终是一个**极佳的默认选择**！'
- en: '[**FCN**](/review-fcn-semantic-segmentation-eb8c9b50d2d1): The Fully Convolutional
    Network (FCN) is a fully convolutional network, but it is [not as deep as the
    U-Net](https://stackoverflow.com/questions/50239795/intuition-behind-u-net-vs-fcn-for-semantic-segmentation).
    The lack of depth is mainly due to the fact that at higher network depths, the
    accuracy drops. This makes it faster to train, but it may not be as accurate as
    the U-Net.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**FCN**](/review-fcn-semantic-segmentation-eb8c9b50d2d1)：全卷积网络（FCN）是一个完全卷积的网络，但它[不如
    U-Net 深](https://stackoverflow.com/questions/50239795/intuition-behind-u-net-vs-fcn-for-semantic-segmentation)。缺乏深度主要是因为在较高的网络深度下，准确率会下降。这使得它训练更快，但可能不如
    U-Net 准确。'
- en: '[**SegNet**](https://arxiv.org/abs/1511.00561): SegNet is a popular model architecture
    similar to U-Net, and uses lesser activation memory than U-Net. We shall use SegNet
    in this series.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**SegNet**](https://arxiv.org/abs/1511.00561)：SegNet 是一种类似于 U-Net 的流行模型架构，并且比
    U-Net 使用更少的激活内存。我们在这一系列中也将使用 SegNet。'
- en: '[**Vision Transformer (ViT)**](https://arxiv.org/abs/2010.11929): Vision Transformers
    have recently gained popularity due to their simple structure and applicability
    of the attention mechanism to text, vision, and other domains. Vision Transformers
    can be more efficient (compared to CNNs) for both training and inference, but
    historically have needed more data to train compared to convolutional neural networks.
    We shall also use ViT in this series.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**视觉 Transformer (ViT)**](https://arxiv.org/abs/2010.11929)：视觉 Transformer
    最近因其简单结构和将注意力机制应用于文本、视觉等领域的能力而受到欢迎。与 CNN 相比，视觉 Transformer 在训练和推理时可能更高效，但历史上需要更多数据来训练。我们在这一系列中也将使用
    ViT。'
- en: '![](../Images/616bcf8bc926426b47371e10a626122f.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/616bcf8bc926426b47371e10a626122f.png)'
- en: 'Figure 4: The U-Net model architecture. Source: [The University of Freiburg,
    original author(s) of U-Net](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：U-Net 模型架构。来源：[弗莱堡大学，U-Net 的原作者](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/)。
- en: These are just a few of the many deep learning models that can be used for image
    segmentation. The best model for your specific task will depend on the factors
    mentioned earlier, on the specific task, and your own experiments.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是众多可以用于图像分割的深度学习模型中的一部分。适合你特定任务的最佳模型将取决于之前提到的因素、具体任务和你自己的实验。
- en: Choosing the right loss function
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择正确的损失函数
- en: 'The choice of loss function for image segmentation tasks is an important one,
    as it can have a significant impact on the performance of the model. There are
    many different loss functions available, each with its own advantages and disadvantages.
    The most popular loss functions for image segmentation are:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分割任务的损失函数选择非常重要，因为它对模型性能有显著影响。可用的损失函数有很多，每种都有其优缺点。图像分割中最流行的损失函数包括：
- en: '[**Cross-entropy loss**](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html):
    Cross-entropy loss is a measure of the difference between the predicted probability
    distribution and the ground truth probability distribution'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**交叉熵损失**](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)：交叉熵损失是预测概率分布与真实概率分布之间差异的度量。'
- en: '**IoU loss**: IoU loss measures the amount of overlap between the predicted
    mask and ground-truth mask per class. IoU loss penalizes cases where either the
    prediction or recall would suffer. IoU as defined is not differentiable, so we
    need to slightly tweak it to use it as a loss function'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**IoU 损失**：IoU 损失衡量每个类别中预测掩膜与真实掩膜之间的重叠量。IoU 损失惩罚预测或召回性能差的情况。由于定义的 IoU 不是可微分的，因此我们需要稍微调整它以用作损失函数。'
- en: '[**Dice loss**](https://torchmetrics.readthedocs.io/en/stable/classification/dice.html):
    Dice loss is also a measure of the overlap between the predicted mask and the
    ground truth mask.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Dice 损失**](https://torchmetrics.readthedocs.io/en/stable/classification/dice.html)：Dice
    损失也是衡量预测掩膜与真实掩膜之间重叠量的一种方法。'
- en: '[**Tversky loss**](https://arxiv.org/abs/1706.05721): Tversky loss is proposed
    as a robust loss function that can be used to handle imbalanced datasets.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Tversky 损失**](https://arxiv.org/abs/1706.05721)：Tversky 损失被提出作为一种稳健的损失函数，可以用于处理不平衡的数据集。'
- en: '[**Focal loss**](https://pytorch.org/vision/main/generated/torchvision.ops.sigmoid_focal_loss.html#torchvision.ops.sigmoid_focal_loss):
    Focal loss is designed to focus on hard examples, which are examples that are
    difficult to classify. This can be helpful for improving the performance of the
    model on challenging datasets.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Focal 损失**](https://pytorch.org/vision/main/generated/torchvision.ops.sigmoid_focal_loss.html#torchvision.ops.sigmoid_focal_loss)：Focal
    损失旨在关注难以分类的样本。这对于提高模型在具有挑战性数据集上的表现可能很有帮助。'
- en: The best loss function for a particular task will depend on the specific requirements
    of the task. For example, if accuracy is more important, then IoU loss or Dice
    loss may be better choices. If the task is imbalanced, then Tversky loss or Focal
    loss may be good choices. The specific loss function used may impact the rate
    of convergence of your model when training it.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特定任务，最佳损失函数将取决于任务的具体要求。例如，如果准确性更重要，则 IoU 损失或 Dice 损失可能是更好的选择。如果任务存在不平衡，则 Tversky
    损失或 Focal 损失可能是较好的选择。使用的具体损失函数可能会影响模型训练时的收敛速度。
- en: The loss function is a hyperparameter of your model, and using a different loss
    based on the results we see can allow us to reduce the loss faster and improve
    the model’s accuracy.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数是模型的一个超参数，根据我们观察到的结果使用不同的损失函数可以让我们更快地减少损失，并提高模型的准确性。
- en: '**Default**: In this series, we shall use cross entropy loss, since it’s always
    a good *default* to choose when the results are not known.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**默认**：在这一系列中，我们将使用交叉熵损失，因为在结果未知时，选择它通常是一个很好的*默认*选择。'
- en: You can use the following resources to learn more about loss functions.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下资源来了解更多关于损失函数的内容。
- en: '[PyTorch Loss Functions: The Ultimate Guide](https://neptune.ai/blog/pytorch-loss-functions)'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[PyTorch 损失函数：终极指南](https://neptune.ai/blog/pytorch-loss-functions)'
- en: '[Torchvision — Losses](https://pytorch.org/vision/main/ops.html#losses)'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Torchvision — 损失](https://pytorch.org/vision/main/ops.html#losses)'
- en: '[Torchmetrics](https://torchmetrics.readthedocs.io/en/stable/pages/overview.html#metrics-and-differentiability)'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Torchmetrics](https://torchmetrics.readthedocs.io/en/stable/pages/overview.html#metrics-and-differentiability)'
- en: Let’s take a detailed look at the IoU Loss we define below as a robust alternative
    to the Cross Entropy Loss for segmentation tasks.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细查看下面定义的 IoU 损失，作为分割任务中交叉熵损失的一个强健替代方案。
- en: The Custom IoU Loss
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义 IoU 损失
- en: '[IoU](/intersection-over-union-iou-calculation-for-evaluating-an-image-segmentation-model-8b22e2e84686)
    is defined as intersection over union. For image segmentation tasks, we can compute
    this by computing (for each class), the intersection of pixels in that class as
    predicted by the model and in the ground truth segmentation mask.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[IoU](/intersection-over-union-iou-calculation-for-evaluating-an-image-segmentation-model-8b22e2e84686)
    被定义为交集与并集之比。对于图像分割任务，我们可以通过计算每个类别的像素交集来计算 IoU，这些像素是由模型预测的，并且在实际分割掩码中。'
- en: 'For example, if we have 2 classes:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们有 2 个类别：
- en: Background
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 背景
- en: Person
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 人物
- en: Then we can determine which pixels were classified as a person, and compare
    that with the ground truth pixels for a person, and compute the IoU for the person
    class. Similarly, we can compute the IoU for the background class.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以确定哪些像素被分类为人物，并将其与实际人物像素进行比较，从而计算人物类别的 IoU。同样，我们可以计算背景类别的 IoU。
- en: Once we have these class-specific IoU metrics, we can choose to average them
    unweighted or weigh them before averaging them to account for any sort of class
    imbalance as we saw in the example earlier.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了这些特定类别的 IoU 度量，我们可以选择对它们进行无权平均，或在平均之前加权，以考虑之前示例中的任何类别不平衡。
- en: The IoU metric as defined requires us to compute hard labels for each metric.
    This requires the use of the argmax() function, which isn’t differentiable, so
    we can’t use this metric as a loss function. Hence, instead of using hard labels,
    we apply softmax() and use the predicted probabilities as soft labels to compute
    the IoU metric. This results in a differentiable metric that we can then compute
    the loss from. Hence, sometimes, the IoU metric is also known as the soft-IoU-metric
    when used in the context of a loss function.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 按定义的 IoU 度量要求我们为每个度量计算硬标签。这需要使用 argmax() 函数，而该函数不可微分，因此我们不能将此度量用作损失函数。因此，我们用
    softmax() 替代硬标签，并使用预测的概率作为软标签来计算 IoU 度量。这将得到一个可微分的度量，我们可以基于此计算损失。因此，有时，在作为损失函数时，IoU
    度量也被称为软 IoU 度量。
- en: 'If we have a metric (M) that takes values between 0.0 and 1.0, we can compute
    the loss (L) as:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个在 0.0 和 1.0 之间取值的度量（M），我们可以计算损失（L）如下：
- en: '*L = 1 — M*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*L = 1 — M*'
- en: 'However, here’s another trick one can use to convert a metric into a loss if
    your metric has the value between 0.0 and 1.0\. Compute:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，如果你的指标值在0.0和1.0之间，可以使用另一种技巧将指标转换为损失。计算：
- en: '*L = -log(M)*'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*L = -log(M)*'
- en: I.e. compute the negative log of the metric. This is meaningfully different
    from the previous formulation, and you can read about it [here](/why-we-care-about-the-log-loss-50c00c8e777c)
    and [here](/intuition-behind-log-loss-score-4e0c9979680a). Basically, it results
    in better learning for your model.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 即计算指标的负对数。这与之前的公式有意义的不同，你可以[在这里]( /why-we-care-about-the-log-loss-50c00c8e777c)和[这里](
    /intuition-behind-log-loss-score-4e0c9979680a)了解更多。基本上，这将带来更好的模型学习效果。
- en: '![](../Images/02aaeb2051bb8124a8b565eccf1c584f.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02aaeb2051bb8124a8b565eccf1c584f.png)'
- en: 'Figure 6: Comparing the loss resulting from 1-P(x) with -log(P(x)). Source:
    Author(s).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：比较1-P(x)与-log(P(x))产生的损失。来源：作者。
- en: Using IoU as our loss also brings the loss function closer to capturing what
    we really care about. There are pros and cons of using an evaluation metric as
    the loss function. If you’re interested in exploring this space more, you can
    start with [this discussion on stackexchange](https://stats.stackexchange.com/questions/577556/why-not-use-evaluation-metrics-as-the-loss-function).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 使用IoU作为损失函数也使得损失函数更接近于捕捉我们真正关心的内容。使用评估指标作为损失函数有利有弊。如果你对深入探讨这个领域感兴趣，可以从[这个讨论](https://stats.stackexchange.com/questions/577556/why-not-use-evaluation-metrics-as-the-loss-function)开始。
- en: Data Augmentation
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据增强
- en: To train your model efficiently and effectively for good accuracy, one needs
    to be mindful of the amount and kind of training data used to train the model.
    The choice of training data used will significantly impact the final model’s accuracy,
    so if there’s one thing you wish to take away from this article series then this
    should be it!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了高效且有效地训练你的模型以获得良好的准确率，需要注意训练数据的数量和种类。所使用的训练数据的选择会显著影响最终模型的准确率，所以如果你希望从这篇文章系列中学到一件事，那就是这一点！
- en: Typically, we’d split our data into 3 parts with the parts being roughly in
    the proportions mentioned below.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，我们会将数据分成3部分，部分之间的大致比例如下所示。
- en: Training (80%)
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练 (80%)
- en: Validation (10%)
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证 (10%)
- en: Test (10%)
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试 (10%)
- en: You’d train your model on the training set, evaluate accuracy on the validation
    set, and repeat the process till you’re happy with the reported metrics. Only
    then would you evaluate the model on the test set, and then report the numbers.
    This is done to prevent any sort of bias from creeping into your model’s architecture
    and hyperparameters used during training and evaluation. In general, the more
    you tweak your setup based on the outcomes you see with the test data, the less
    reliable your results will get. Hence, we must limit our decision making to only
    the results we see on the training and validation datasets.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 你会在训练集上训练你的模型，在验证集上评估准确率，然后重复这个过程，直到你对报告的指标感到满意。只有在那时你才会在测试集上评估模型，并报告数字。这样做是为了防止任何偏差渗入到模型的架构和训练及评估过程中使用的超参数中。一般来说，你越是根据测试数据的结果来调整设置，你的结果就会越不可靠。因此，我们必须将决策限制在仅基于训练和验证数据集上看到的结果。
- en: In this series, we shall not use a test dataset. Instead, we’ll use our test
    dataset as the validation dataset, and apply [data augmentation](https://en.wikipedia.org/wiki/Data_augmentation)
    on the test dataset so that we’re always validating our models on data that’s
    slightly different. This kind of prevents us from overfitting our decisions on
    the validation dataset. This is a bit of a hack, and we’re doing this just for
    expediency and as a short-cut. For production model development, you should try
    to stick with the standard recipe mentioned above.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一系列中，我们不会使用测试数据集。相反，我们将使用我们的测试数据集作为验证数据集，并对测试数据集应用[数据增强](https://en.wikipedia.org/wiki/Data_augmentation)，以便我们总是在稍有不同的数据上验证我们的模型。这种做法有助于防止我们在验证数据集上过拟合决策。这有点像是一个变通方法，我们这样做只是为了方便和作为一种捷径。对于生产模型开发，你应该尽量坚持上述标准方法。
- en: The dataset we’re going to use in this series has 3680 images in the training
    set. While this may seem like a large number of images, we want to make sure that
    our model doesn’t overfit on these images since we’ll be training the model for
    numerous epochs.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一系列中我们将使用的数据集包含3680张图像作为训练集。虽然这可能看起来图像数量很多，但我们希望确保我们的模型不会在这些图像上过拟合，因为我们将对模型进行多个轮次的训练。
- en: In a single training epoch, we train the model on the entire training dataset,
    and we’d typically train models in production for 60 or more epochs. In this series,
    we shall train the model only for 20 epochs for faster iteration times. To **prevent
    overfitting**, we’ll employ a technique called [data augmentation](https://efficientdlbook.com/#table-of-contents)
    that is used to generate new input data from existing input data. The basic idea
    behind data augmentation for image inputs is that if you change the image slightly,
    it feels like a new image to the model, but one can reason about whether the expected
    outputs would be the same. Here are some examples of data augmentations that we’ll
    apply in this series.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个训练周期中，我们会在整个训练数据集上训练模型，通常我们会在生产环境中训练模型 60 个周期或更多。在本系列中，我们将只训练模型 20 个周期以加快迭代速度。为了**防止过拟合**，我们将采用一种叫做
    [数据增强](https://efficientdlbook.com/#table-of-contents) 的技术，用于从现有输入数据生成新的输入数据。数据增强的基本理念是，如果你稍微更改图像，它对模型来说就像是一张新图像，但可以推测期望的输出是否相同。以下是我们将在本系列中应用的一些数据增强示例。
- en: '[Random horizontal flip](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomHorizontalFlip.html)'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[随机水平翻转](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomHorizontalFlip.html)'
- en: '[Random Color jitter](https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html)'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[随机颜色抖动](https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html)'
- en: While we’re going to use the Torchvision library for applying the data augmentations
    above, we’d encourage you to evaluate the Albumentations data augmentation library
    for vision tasks as well. Both libraries have a rich set of transformations available
    for use with image data. We personally continue to use Torchvision simply because
    it’s what we started with. [Albumentations](https://albumentations.ai/) supports
    richer primitives for data augmentation that can make changes to the input image
    as well as the ground truth labels or masks at the same time. For example, if
    you were to resize or flip an image, you’d need to make the same change to the
    ground truth segmentation mask. Albumentations can do this for you out of the
    box.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们将使用 Torchvision 库来应用上述数据增强，但我们也鼓励你评估 Albumentations 数据增强库在视觉任务中的应用。两个库都提供了丰富的图像数据变换选项。我们个人继续使用
    Torchvision，因为这是我们最初选择的库。[Albumentations](https://albumentations.ai/) 支持更丰富的数据增强原语，可以同时对输入图像及其真实标签或掩码进行更改。例如，如果你需要调整图像大小或翻转图像，你也需要对真实分割掩码进行相应的更改。Albumentations
    可以直接为你完成这些操作。
- en: Broadly speaking, both libraries support transformations that are applied to
    the image either at the pixel-level or change the spatial dimensions of the image.
    The pixel-level transforms are called color transforms by torchvision, and the
    spatial transforms are called Geometric transforms by torchvision.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 广义而言，这两个库都支持对图像应用的变换，这些变换可以是像素级别的，也可以改变图像的空间维度。像素级变换被 torchvision 称为颜色变换，而空间变换被称为几何变换。
- en: Below, we shall see some examples of both pixel-level as well as geometric transforms
    applied by the Torchvision and Albumentations libraries.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看到 Torchvision 和 Albumentations 库对像素级和几何变换的应用示例。
- en: '![](../Images/61329a6abccd37e7631dcc683568d326.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61329a6abccd37e7631dcc683568d326.png)'
- en: 'Figure 7: Examples of pixel level data augmentations applied to images using
    Albumentations. Source: [Albumentations](https://albumentations.ai/)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：使用 Albumentations 对图像应用的像素级数据增强示例。来源：[Albumentations](https://albumentations.ai/)
- en: '![](../Images/5c490cff06d9218c578606c1cb6e3a7e.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5c490cff06d9218c578606c1cb6e3a7e.png)'
- en: 'Figure 8: Examples of data augmentations applied to images using Torchvision
    transforms. Source: Author(s) ([notebook](https://www.kaggle.com/code/dhruv4930/starter-for-oxford-iiit-pet-using-torchvision/notebook?scriptVersionId=129839935))'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：使用 Torchvision 变换对图像应用的数据增强示例。来源：作者 ([notebook](https://www.kaggle.com/code/dhruv4930/starter-for-oxford-iiit-pet-using-torchvision/notebook?scriptVersionId=129839935))
- en: '![](../Images/7e9741d85b50584037b6a25aa71e91e0.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e9741d85b50584037b6a25aa71e91e0.png)'
- en: 'Figure 9: Examples of spatial level transforms applied using Albumentations.
    Source: Author(s) ([notebook](https://www.kaggle.com/code/dhruv4930/starter-for-oxford-iiit-pet-using-torchvision/notebook?scriptVersionId=129839935))'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：使用 Albumentations 应用的空间级变换示例。来源：作者 ([notebook](https://www.kaggle.com/code/dhruv4930/starter-for-oxford-iiit-pet-using-torchvision/notebook?scriptVersionId=129839935))
- en: Evaluating your model’s performance
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估模型性能
- en: When evaluating your model’s performance, you’d want to know how it performs
    on a metric that is representative of the quality of the model’s performance on
    real world data. For example, for the image segmentation task, we’d want to know
    how accurately a model is able to predict the correct class for a pixel. Hence,
    we say that [Pixel Accuracy](https://torchmetrics.readthedocs.io/en/stable/classification/accuracy.html)
    is the validation metric for this model.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估模型的性能时，你会想要了解它在代表模型在真实数据上的表现质量的指标上的表现。例如，对于图像分割任务，我们想知道模型在预测像素的正确类别方面的准确度。因此，我们称[像素准确率](https://torchmetrics.readthedocs.io/en/stable/classification/accuracy.html)为该模型的验证指标。
- en: You could use your evaluation metric as the loss function (why not optimize
    what you really care about!) except that this [may not always be possible](https://jonathan-sands.com/metric/loss/2021/05/13/Metric-vs-Loss.html#what-is-a-loss-function).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将评估指标用作损失函数（为什么不优化你真正关心的东西！），只不过这[可能并不总是可行的](https://jonathan-sands.com/metric/loss/2021/05/13/Metric-vs-Loss.html#what-is-a-loss-function)。
- en: In addition to [Accuracy](https://torchmetrics.readthedocs.io/en/stable/classification/accuracy.html),
    we’ll also track the IoU metric (also called [Jaccard Index](https://torchmetrics.readthedocs.io/en/stable/classification/jaccard_index.html)),
    and the Custom IoU metric we defined above.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 除了[准确率](https://torchmetrics.readthedocs.io/en/stable/classification/accuracy.html)，我们还将跟踪IoU指标（也称为[Jaccard指数](https://torchmetrics.readthedocs.io/en/stable/classification/jaccard_index.html)），以及我们上面定义的自定义IoU指标。
- en: 'To read more about various accuracy metrics applicable to image segmentation
    tasks, please see:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多适用于图像分割任务的各种准确率指标，请参见：
- en: '[All the segmentation metrics — Kaggle](https://www.kaggle.com/code/yassinealouini/all-the-segmentation-metrics)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[所有分割指标 — Kaggle](https://www.kaggle.com/code/yassinealouini/all-the-segmentation-metrics)'
- en: '[How to evaluate image segmentation models](/how-accurate-is-image-segmentation-dd448f896388)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何评估图像分割模型](/how-accurate-is-image-segmentation-dd448f896388)'
- en: '[Evaluating image segmentation models](/evaluating-image-segmentation-models-1e9bb89a001b)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[评估图像分割模型](/evaluating-image-segmentation-models-1e9bb89a001b)'
- en: The downside of using pixel accuracy as a performance metric
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用像素准确率作为性能指标的缺点
- en: While the accuracy metric may be a good default choice to measure the performance
    of image segmentation tasks, it does have its own drawbacks, which may be significant
    based on your specific situation.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管准确率指标可能是衡量图像分割任务表现的一个良好默认选择，但它确实存在自身的缺陷，这些缺陷可能在特定情况下非常重要。
- en: 'For example, consider an image segmentation task to identify a person’s eyes
    in a picture, and mark those pixels accordingly. The model will hence classify
    each pixels as either one of:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个图像分割任务，以识别图片中的人的眼睛，并相应地标记这些像素。因此，模型将把每个像素分类为以下之一：
- en: Background
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 背景
- en: Eye
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 眼睛
- en: Assume that there’s just 1 person in each image, and 98% of the pixels don’t
    correspond to an eye. In this case, the model can simply learn to predict every
    pixel as being a background pixel and achieve 98% pixel accuracy on the segmentation
    task. Wow!
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 假设每张图像中只有1个人，并且98%的像素不对应于眼睛。在这种情况下，模型可以简单地学会将每个像素预测为背景像素，从而在分割任务中实现98%的像素准确率。哇！
- en: '![](../Images/01e74d83606404a77323179cb7419753.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01e74d83606404a77323179cb7419753.png)'
- en: 'Figure 10: An image of a person’s face and the corresponding segmentation mask
    for their eyes. You can see that the eyes occupy a very small fraction of the
    overall image. Source: Adapted from [Unsplash](https://unsplash.com/photos/iFgRcqHznqg)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：一个人的面部图像及其眼睛的分割掩码。你可以看到眼睛只占整个图像的很小一部分。来源：改编自[Unsplash](https://unsplash.com/photos/iFgRcqHznqg)
- en: In such cases, using the IoU or Dice metric may be a much better idea, since
    IoU would capture how much of the prediction was correct, and wouldn’t necessarily
    be biased by the region that each class or category occupies in the original image.
    You could even consider using the IoU or Dice coefficient per class as a metric.
    This may better capture the performance of your model for the task at hand.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，使用IoU或Dice指标可能是一个更好的选择，因为IoU会捕捉到预测的正确部分，并且不会受到每个类别或类别在原始图像中所占区域的偏见。你甚至可以考虑将IoU或Dice系数按类别作为指标。这可能更好地反映模型在当前任务中的表现。
- en: When considering pixel accuracy alone, the [precision and recall](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall)
    of the object we’re looking to compute the segmentation mask for (eyes in the
    example above) can capture the details we’re looking for.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 当仅考虑像素准确性时，[精确度和召回率](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall)对于我们计算分割掩膜的对象（如上述示例中的眼睛）可以捕捉我们所寻找的细节。
- en: Now that we have covered a large part of the theoretical underpinnings of image
    segmentation, let’s take a detour into considerations related to inference and
    deployment of image segmentation for real-world workloads.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了图像分割理论基础的大部分内容，让我们绕道讨论一下与实际工作负载的图像分割推理和部署相关的考虑。
- en: Model size and inference latency
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型大小和推理延迟
- en: Last but not least, we’d want to ensure that our model has a reasonable number
    of parameters but not too many, since we want a small and efficient model. We
    shall look into this aspect in greater detail in a future post related to reducing
    model size using [efficient model architectures](https://efficientdlbook.com/#table-of-contents).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，我们希望确保我们的模型参数数量合理而不多，因为我们需要一个小而高效的模型。我们将在未来的帖子中详细探讨这个方面，讨论如何使用[高效模型架构](https://efficientdlbook.com/#table-of-contents)来减少模型大小。
- en: As far as inference latency is concerned, what matters is the number of mathematical
    operations (mult-adds) our model executes. Both the model size and mult-adds can
    be displayed using the [torchinfo](https://pypi.org/project/torchinfo/) package.
    While mult-adds is a great proxy for determining the model’s latency, there can
    be a large variation in latency across various backends. The only real way to
    determine the performance of your model on a specific backend or device is to
    profile and benchmark it on that specific device with the set of inputs you expect
    to see in production settings.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 就推理延迟而言，重要的是模型执行的数学运算（加减运算）的数量。模型大小和加减运算可以通过[torchinfo](https://pypi.org/project/torchinfo/)包显示。虽然加减运算是确定模型延迟的一个很好的代理，但在不同的后端之间，延迟可能会有很大差异。唯一真正确定模型在特定后端或设备上性能的方法是对该设备进行性能分析和基准测试，并使用你预计在生产环境中看到的输入。
- en: '[PRE0]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Output:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE1]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Further Reading
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: The following articles provide additional information regarding the basics of
    image segmentation. If you’re the kind of person who likes reading different perspectives
    on the same subject, please consider reading them.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 以下文章提供了有关图像分割基础知识的额外信息。如果你喜欢阅读不同观点的内容，请考虑阅读这些文章。
- en: '[Guide to Image Segmentation in Computer Vision: Best Practices](https://encord.com/blog/image-segmentation-for-computer-vision-best-practice-guide/)'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[计算机视觉中的图像分割指南：最佳实践](https://encord.com/blog/image-segmentation-for-computer-vision-best-practice-guide/)'
- en: '[An Introduction to Image Segmentation: Deep Learning vs. Traditional [+Examples]](https://www.v7labs.com/blog/image-segmentation-guide#h5)'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[图像分割简介：深度学习与传统方法 [+示例]](https://www.v7labs.com/blog/image-segmentation-guide#h5)'
- en: '[Image Segmentation: The Basics and 5 Key Techniques](https://datagen.tech/guides/image-annotation/image-segmentation/)'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[图像分割：基础知识和5种关键技术](https://datagen.tech/guides/image-annotation/image-segmentation/)'
- en: If you wish to get your hands dirty with the Oxford IIIT Pet dataset and use
    torchvision and Albumentations to perform image augmentations, we have provided
    a [starter notebook on Kaggle](https://www.kaggle.com/code/dhruv4930/starter-for-oxford-iiit-pet-using-torchvision/notebook?scriptVersionId=129839935)
    that you can clone and play around with. Many of the images in this article were
    generated by that notebook!
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望亲自动手处理Oxford IIIT Pet数据集，并使用torchvision和Albumentations进行图像增强，我们提供了一个[在Kaggle上的起始笔记本](https://www.kaggle.com/code/dhruv4930/starter-for-oxford-iiit-pet-using-torchvision/notebook?scriptVersionId=129839935)供你克隆和尝试。本文中的许多图像都是由该笔记本生成的！
- en: Article recap
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文章总结
- en: Here’s a quick recap of what we discussed so far.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们迄今为止讨论内容的快速回顾。
- en: 'Image segmentation is a technique to partition an image into multiple segments
    (source: [Wikipedia](https://en.wikipedia.org/wiki/Image_segmentation))'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像分割是一种将图像划分为多个部分的技术（来源：[维基百科](https://en.wikipedia.org/wiki/Image_segmentation)）
- en: 'There are two main types of image segmentation tasks: class (semantic) segmentation
    and object (instance) segmentation. Class segmentation assigns each pixel in an
    image to a semantic class. Object segmentation identifies each individual object
    in an image and assigns a mask to each unique object'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像分割任务主要有两种类型：类别（语义）分割和对象（实例）分割。类别分割将图像中的每个像素分配给一个语义类别。对象分割则识别图像中的每个独立对象，并为每个唯一对象分配一个掩膜。
- en: We shall use PyTorch as the deep learning framework and the Oxford IIIT Pet
    dataset in this series of efficient image segmentation
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本系列高效图像分割中，我们将使用PyTorch作为深度学习框架，并使用Oxford IIIT Pet数据集。
- en: There are many factors to consider when choosing the right deep learning model
    for image segmentation, including (but not limited to) the type of image segmentation
    task, the size and complexity of the dataset, the availability of pre-trained
    models, and the computational resources available. Some of the most popular deep
    learning model architectures for image segmentation include U-Net, FCN, SegNet,
    and Vision Transformer (ViT)
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择合适的深度学习模型进行图像分割时需要考虑许多因素，包括（但不限于）图像分割任务的类型、数据集的大小和复杂性、预训练模型的可用性以及计算资源的情况。一些最受欢迎的图像分割深度学习模型架构包括U-Net、FCN、SegNet和Vision
    Transformer（ViT）。
- en: The choice of loss function for image segmentation tasks is an important one,
    as it can have a significant impact on the performance of the model and on training
    efficiency. For image segmentation tasks, we can use cross-entropy loss, IoU Loss,
    Dice loss, or Focal loss (among others)
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像分割任务中损失函数的选择非常重要，因为它可能对模型的性能和训练效率产生重大影响。对于图像分割任务，我们可以使用交叉熵损失、IoU损失、Dice损失或Focal损失（以及其他一些）。
- en: Data augmentation is a valuable technique that is used to prevent overfitting
    as well as deal with insufficient training data
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据增强是一种宝贵的技术，用于防止过拟合以及处理训练数据不足的问题。
- en: Evaluating your model’s performance is important for the task at hand and one
    must choose this metric carefully
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估模型性能对当前任务至关重要，必须仔细选择这一指标。
- en: Model size and inference latency are vital metrics to consider when developing
    a model, especially if you intend it to be used in real-time applications such
    as face segmentation or background noise removal
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的大小和推理延迟是开发模型时需要考虑的重要指标，尤其是当你打算将其用于实时应用程序，如面部分割或背景噪声去除时。
- en: In the [next post](https://medium.com/p/bed68cadd7c7/), we shall look at a Convolutional
    Neural Network (CNN) built from scratch using PyTorch to perform image segmentation
    on the Oxford IIIT Pet dataset.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在[下一篇文章](https://medium.com/p/bed68cadd7c7/)中，我们将讨论如何使用PyTorch从零开始构建一个卷积神经网络（CNN）来对Oxford
    IIIT Pet数据集进行图像分割。
