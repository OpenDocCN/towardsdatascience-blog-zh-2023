- en: 'Simplifying Transformers: State of the Art NLP Using Words You Understand —
    part 2— Input'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简化 Transformers：使用你理解的词语的最前沿 NLP — 第二部分 — 输入
- en: 原文：[https://towardsdatascience.com/transformers-part-2-input-2a8c3a141c7d?source=collection_archive---------2-----------------------#2023-07-26](https://towardsdatascience.com/transformers-part-2-input-2a8c3a141c7d?source=collection_archive---------2-----------------------#2023-07-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/transformers-part-2-input-2a8c3a141c7d?source=collection_archive---------2-----------------------#2023-07-26](https://towardsdatascience.com/transformers-part-2-input-2a8c3a141c7d?source=collection_archive---------2-----------------------#2023-07-26)
- en: Deep dive into how transformers’ inputs are constructed
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入探讨 Transformer 输入的构建方式
- en: '[](https://medium.com/@chenmargalit?source=post_page-----2a8c3a141c7d--------------------------------)[![Chen
    Margalit](../Images/fb37720654b3d1068b448d4d9ad624d5.png)](https://medium.com/@chenmargalit?source=post_page-----2a8c3a141c7d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2a8c3a141c7d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2a8c3a141c7d--------------------------------)
    [Chen Margalit](https://medium.com/@chenmargalit?source=post_page-----2a8c3a141c7d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@chenmargalit?source=post_page-----2a8c3a141c7d--------------------------------)[![Chen
    Margalit](../Images/fb37720654b3d1068b448d4d9ad624d5.png)](https://medium.com/@chenmargalit?source=post_page-----2a8c3a141c7d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2a8c3a141c7d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2a8c3a141c7d--------------------------------)
    [Chen Margalit](https://medium.com/@chenmargalit?source=post_page-----2a8c3a141c7d--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff8e6113b0479&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-part-2-input-2a8c3a141c7d&user=Chen+Margalit&userId=f8e6113b0479&source=post_page-f8e6113b0479----2a8c3a141c7d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2a8c3a141c7d--------------------------------)
    ·10 min read·Jul 26, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2a8c3a141c7d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-part-2-input-2a8c3a141c7d&user=Chen+Margalit&userId=f8e6113b0479&source=-----2a8c3a141c7d---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff8e6113b0479&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-part-2-input-2a8c3a141c7d&user=Chen+Margalit&userId=f8e6113b0479&source=post_page-f8e6113b0479----2a8c3a141c7d---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2a8c3a141c7d--------------------------------)
    ·10分钟阅读·2023年7月26日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2a8c3a141c7d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-part-2-input-2a8c3a141c7d&user=Chen+Margalit&userId=f8e6113b0479&source=-----2a8c3a141c7d---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2a8c3a141c7d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-part-2-input-2a8c3a141c7d&source=-----2a8c3a141c7d---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2a8c3a141c7d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-part-2-input-2a8c3a141c7d&source=-----2a8c3a141c7d---------------------bookmark_footer-----------)'
- en: Inputs
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入
- en: Dragon hatches from eggs, babies spring out from bellies, AI-generated text
    starts from inputs. We all have to start somewhere.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 龙从蛋中孵化，婴儿从母腹中诞生，AI 生成的文本从输入开始。我们都必须从某个地方开始。
- en: What kind of inputs? it depends on the task at hand. If you’re building a language
    model, a software that knows how to generate relevant text (the Transformers architecture
    is useful in diverse scenarios) the input is text. Nonetheless, can a computer
    receive any kind of input (text, image, sound) and magically know how to process
    it? it doesn’t.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 什么类型的输入？这取决于手头的任务。如果你正在构建一个语言模型，一个能够生成相关文本的程序（Transformers 架构在各种场景中都很有用），输入就是文本。然而，计算机能否接收任何类型的输入（文本、图像、声音）并神奇地知道如何处理它？它并不能。
- en: I’m sure you know people who aren’t very good with words but are great with
    numbers. The computer is something like that. It cannot process text directly
    in the CPU/GPU (where the calculations happen), but it can certainly work with
    numbers! As you will soon see, the way to represent these words as numbers is
    a crucial ingredient in the secret sauce.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我相信你认识一些不擅长文字但擅长数字的人。计算机有点类似于这样。它不能直接在CPU/GPU（进行计算的地方）处理文本，但它可以处理数字！正如你将很快看到的，将这些单词表示为数字的方式是秘密配方中的关键成分。
- en: '![](../Images/f4213bd23253b425f574bfe74a6638cb.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4213bd23253b425f574bfe74a6638cb.png)'
- en: Image from the [original paper](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
    by Vaswani, A. et al.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自Vaswani, A.等人的[原始论文](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
- en: Tokenizer
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标记化器
- en: 'Tokenization is the process of transforming the corpus (all the text you’ve
    got) into smaller parts that the machine can make better use of. Say we have a
    dataset of 10,000 Wikipedia articles. We take each character and we transform
    (tokenize) it. There are many ways to tokenize text, let''s see how [OpenAi’s
    tokenizer](https://platform.openai.com/tokenizer) does it with the following text:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 标记化是将语料库（你拥有的所有文本）转换为机器可以更好利用的更小部分的过程。假设我们有一个10,000篇维基百科文章的数据集。我们将每个字符进行转换（标记化）。有很多种标记化文本的方法，让我们看看[OpenAI的标记化器](https://platform.openai.com/tokenizer)如何处理以下文本：
- en: '“*Many words map to one token, but some don’t: indivisible.*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: “*许多单词映射到一个标记，但有些则不是：不可分割的。*
- en: '*Unicode characters like emojis may be split into many tokens containing the
    underlying bytes: 🤚🏾*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*像表情符号这样的Unicode字符可能会被拆分成包含底层字节的多个标记：🤚🏾*'
- en: '*Sequences of characters commonly found next to each other may be grouped together:
    1234567890*"'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*常见的字符序列可能会被组合在一起：1234567890*'
- en: 'This is the tokenization result:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这是标记化的结果：
- en: '![](../Images/8dd4cdd6e1ff1009368b0e4488218986.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8dd4cdd6e1ff1009368b0e4488218986.png)'
- en: Image by OpenAi, taken from [here](https://platform.openai.com/tokenizer)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自OpenAI，来源于[这里](https://platform.openai.com/tokenizer)
- en: As you can see, there are around 40 words (depending on how you count (punctuation
    signs). Out of these 40 words, 64 tokens were generated. Sometimes the token is
    the entire word, as with “Many, words, map” and sometimes it's a part of a word,
    as with “Unicode”. Why do we break entire words into smaller parts? why even divide
    sentences? we could’ve held them contact. In the end, they are converted to numbers
    anyway so what’s the difference in the computer’s point of view if the token is
    3 characters long or 30?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这里大约有40个单词（取决于如何计算（标点符号））。在这40个单词中，生成了64个标记。有时候标记是整个单词，比如“Many, words,
    map”，有时候则是单词的一部分，比如“Unicode”。我们为什么要将整个单词拆分成更小的部分？甚至拆分句子又有何意义？最终，它们无论如何都会被转换为数字，那么从计算机的角度来看，标记是3个字符长还是30个字符长有何区别？
- en: 'Tokens help the model learn because as text is our data, they are the data’s
    features. Different ways of engineering those features will lead to variations
    in performance. For example, in the sentence: “Get out!!!!!!!”, we need to decide
    if multiple “!” are different than just one, or if it has the same meaning. Technically
    we could’ve kept the sentences as a whole, but Imagine looking at a crowd vs.
    at each person individually, in which scenario will you get better insights?'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 标记帮助模型学习，因为文本是我们的数据，它们是数据的特征。工程化这些特征的不同方式会导致性能的变化。例如，在句子：“Get out!!!!!!!”中，我们需要决定多个“！”是否与一个“！”不同，还是它们有相同的意义。从技术上讲，我们可以将句子保持为整体，但想象一下看一个人群与单独看每个人的场景，你会在什么场景中获得更好的见解？
- en: 'Now that we have tokens we can build a lookup dictionary that will allow us
    to get rid of words and use indexes (numbers) instead. For example, if our whole
    dataset is the sentence: “Where is god”. We might build this kind of vocabulary,
    which is just a key:value pair of the words and a single number representing them.
    We won''t have to use the entire word every time, we can use the number. For example:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了标记，我们可以建立一个查找字典，这将允许我们摆脱单词而使用索引（数字）。例如，如果我们的整个数据集是句子：“Where is god”。我们可能会建立这样的词汇表，它只是单词和一个代表它们的数字的键值对。我们不必每次都使用整个单词，我们可以使用数字。例如：
- en: '{Where: 0, is: 1, god: 2}. Whenever we encounter the word “is”, we replace
    it with 1\. For more examples of tokenizers, you can check the one [Google](https://github.com/google/sentencepiece)
    developed or play some more with OpenAI’s [TikToken](https://github.com/openai/tiktoken).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '{Where: 0, is: 1, god: 2}。每当我们遇到单词“is”时，我们用1来替换它。想了解更多关于分词器的例子，你可以查看[Google](https://github.com/google/sentencepiece)开发的分词器，或者多玩一些
    OpenAI 的[TikToken](https://github.com/openai/tiktoken)。'
- en: Word to Vector
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Word to Vector
- en: '***Intuition*** We are making great progress in our journey to represent words
    as numbers. The next step will be to generate numeric, **semantic** representations
    from those tokens. To do so, we can use an algorithm called [Word2Vec](https://arxiv.org/abs/1301.3781).
    The details aren''t very important at the moment, but the main idea is that you
    take a vector (we’ll simplify for now, think of a regular list) of numbers in
    any size you want (the [paper’s](https://arxiv.org/abs/1706.03762) authors used
    512) and this list of numbers should represent the semantic meaning of a word.
    Imagine a list of numbers like [-2, 4,-3.7, 41…-0.98] which actually holds the
    semantic representation of a word. It should be created in such a way, that if
    we plot these vectors on a 2D graph, similar terms will be closer than dissimilar
    terms.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '***直觉*** 我们在将词表示为数字的旅程中取得了很大进展。下一步将是从这些标记生成数值的**语义**表示。为此，我们可以使用一种称为[Word2Vec](https://arxiv.org/abs/1301.3781)的算法。细节目前不重要，但主要思想是你拿一个向量（我们暂时简化一下，想象成一个常规列表）包含任意大小的数字（[论文](https://arxiv.org/abs/1706.03762)的作者使用了512），这个数字列表应该代表一个词的语义含义。想象一个数字列表，例如[-2,
    4, -3.7, 41…-0.98]，它实际上包含了一个词的语义表示。它应该以这样的方式创建，以便如果我们将这些向量绘制在二维图上，相似的词会比不相似的词更靠近。'
- en: As You can see in the picture (taken from [here](https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial)),
    “Baby” is close to “Aww” and “Asleep” whereas “Citizen”/“State”/“America’s” are
    also somewhat grouped together.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在图片中所见（图片来源于[这里](https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial)），“Baby”接近于“Aww”和“Asleep”，而“Citizen”/“State”/“America’s”也有些被分组在一起。
- en: '*2D word vectors (a.k.a a list with 2 numbers) will not be able to hold any
    accurate meaning even for one word, as mentioned the authors used 512 numbers.
    Since we can''t plot anything with 512 dimensions, we use a method called [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)
    to reduce the number of dimensions to two, hopefully preserving much of the original
    meaning. In the [3rd part of this series](https://medium.com/@chenmargalit/transformers-part-3-attention-7b95881714df)
    we deep our toes a bit into how that happens.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*二维词向量（即一个包含2个数字的列表）即使对于一个词也无法承载任何准确的意义，如前所述，作者使用了512个数字。由于我们无法用512维绘制任何内容，我们使用一种称为[PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)的方法来将维度减少到两个，希望保留大部分原始意义。在[本系列的第3部分](https://medium.com/@chenmargalit/transformers-part-3-attention-7b95881714df)中，我们会深入探讨这一过程。'
- en: '![](../Images/3f3e1a460426ae91de692d590b91f3c1.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f3e1a460426ae91de692d590b91f3c1.png)'
- en: Word2Vec 2D presentation — image by Piere Mergret from [here](https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 2D 演示 — 图片由 Piere Mergret 提供，来源于[这里](https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial)
- en: It works! You can actually train a model that will be able to produce lists
    of numbers that hold semantic meanings. The computer doesn't know a baby is a
    screaming, sleep depriving (super sweet) small human but it knows it usually sees
    that baby word around “aww”, more often than “State” and “Government”. I’ll be
    writing some more on exactly how that happens, but until then if you’re interested,
    [this](https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial) might
    be a good place to check out.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 它有效！你实际上可以训练一个模型，使其能够生成具有语义意义的数字列表。计算机不知道婴儿是一个尖叫的、缺乏睡眠的（超级可爱）小人类，但它知道它通常会在“aww”附近看到这个婴儿词，而不是“State”和“Government”。我会详细说明这一过程，但如果你感兴趣的话，[这个](https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial)可能是一个不错的地方。
- en: These “lists of numbers” are pretty important, so they get their own name in
    the ML terminology which is Embeddings. Why embeddings? because we are performing
    an embedding (so creative) which is the process of mapping (translating) a term
    from one form (words) to another (list of numbers). These are a lot of ().
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这些“数字列表”相当重要，因此在机器学习术语中有了它们自己的名称——嵌入（Embeddings）。为什么叫嵌入？因为我们在执行嵌入（很有创意），这是将一个术语从一种形式（词）映射（翻译）到另一种形式（数字列表）的过程。这些有很多（）。
- en: From here on we will call words, embeddings, which as explained are lists of
    numbers that hold the semantic meaning of any word it's trained to represent.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，我们将把词汇称为嵌入，正如之前解释的，它们是包含任何词语语义意义的数字列表。
- en: Creating Embeddings with Pytorch
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Pytorch创建嵌入
- en: 'We first calculate the number of unique tokens we have, for simplicity let’s
    say 2\. The creation of the embeddings layer, which is the first part of the Transformer
    architecture, will be as simple as writing this code:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先计算我们有多少个唯一的标记，为了简化起见，假设为2。嵌入层的创建，即Transformer架构的第一部分，将像这样简单：
- en: '*General code remark — don’t take this code and its conventions as good coding
    style, it’s written particularly to make it easy to understand.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*一般代码说明——不要把这段代码及其约定当作良好的编码风格，它的编写特别是为了易于理解。*'
- en: '***Code***'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '***代码***'
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We now have an embedding matrix which in this case is a 2 by 2 matrix, generated
    with random numbers derived from the normal distribution N(0,1) (e.g. a distribution
    with mean 0 and variance 1).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个嵌入矩阵，在这种情况下，它是一个2乘2的矩阵，生成于从正态分布N(0,1)（例如，均值为0，方差为1的分布）中得到的随机数。
- en: Note the requires_grad=True, this is Pytorch language for saying these 4 numbers
    are learnable weights. They can and will be customized in the learning process
    to better represent the data the model receives.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意requires_grad=True，这是Pytorch语言，用于表示这4个数字是可学习的权重。它们可以并且会在学习过程中被定制，以更好地表示模型接收到的数据。
- en: In a more realistic scenario, we can expect something closer to a 10k by 512
    matrix which represents our entire dataset in numbers.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在更现实的场景中，我们可以期望接近于一个10k乘512的矩阵，它以数字的形式表示我们的整个数据集。
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Fun fact (we can think of things that are more fun), you sometimes hear language
    models use billions of parameters. This initial, not too crazy layer, holds 10_000
    by 512 parameters which are 5 million parameters. This LLM (Large Language Model)
    is difficult stuff, it needs a lot of calculations.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*有趣的事实（我们可以想一些更有趣的事情），你有时会听到语言模型使用数十亿的参数。这个初步的、不过于疯狂的层，包含10_000乘512的参数，即500万参数。这个LLM（大语言模型）是复杂的，需要大量的计算。*'
- en: Parameters here is a fancy word for those numbers (-1.525 etc.) just that they
    are subject to change and will change during training.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的参数是这些数字（-1.525等）的花哨词汇，只不过它们会发生变化，并且在训练过程中会变化。
- en: These numbers are the learning of the machine, this is what the machine is learning.
    Later when we give it input, we multiply the input with those numbers, and we
    hopefully get a good result. What do you know, numbers matter. When you’re important,
    you get your own name, so those aren’t just numbers, those are parameters.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字是机器学习的内容，这就是机器正在学习的东西。稍后，当我们给它输入时，我们将输入与这些数字相乘，并希望得到一个好的结果。你知道吗，数字很重要。当你重要时，你会得到自己的名字，所以这些不仅仅是数字，它们是参数。
- en: Why use as many as 512 and not 5? because more numbers mean we can probably
    generate more accurate meaning. Great, stop thinking small, let's use a million
    then! why not? because more numbers mean more calculations, more computing power,
    more expensive to train, etc. 512 has been found to be a good place in the middle.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要用多达512个参数而不是5个？因为更多的数字意味着我们可以生成更准确的意义。很好，别再小看了，让我们用一百万个参数吧！为什么不呢？因为更多的数字意味着更多的计算，更多的计算能力，更昂贵的训练费用等等。512被发现是一个很好的折中点。
- en: Sequence Length
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 序列长度
- en: When training the model we are going to put a whole bunch of words together.
    It's more computationally efficient and it helps the model learn as it gets more
    context together. As mentioned every word will be represented by a 512-dimensional
    vector (list with 512 numbers) and each time we pass inputs to the model (a.k.a
    forward pass), we will send a bunch of sentences, not only one. For example, we
    decided to support a 50-word sequence. This means we are going to take the x number
    of words in a sentence, if x > 50 we split it and only take the first 50, if x
    < 50, we still need the size to be the exact same (I’ll soon explain why). To
    solve this we add padding which are special dummy strings, to the rest of the
    sentence. For example, if we support a 7-word sentence, and we have the sentence
    “Where is god”. We add 4 paddings, so the input to the model will be “Where is
    god <PAD> <PAD> <PAD> <PAD>”. Actually, we usually add at least 2 more special
    paddings so the model knows where the sentence starts and where it ends, so it
    will actually be something like “<StartOfSentence> Where is god <PAD> <PAD> <EndOfSentence>”.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '* Why must all input vectors be of the same size? because software has “expectations”,
    and matrices have even stricter expectations. You can’t do any “mathy” calculation
    you want, it has to adhere to certain rules, and one of those rules is adequate
    vector sizes.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Positional encodings
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '***Intuition*** We now have a way to represent (and learn) words in our vocabulary.
    Let’s make it even better by encoding the position of the words. Why is this important?
    because if we take these two sentences:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 1\. The man played with my cat
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 2\. The cat played with my man
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: We can represent the two sentences using the exact same embeddings, but the
    sentences have different meanings. We can think of such data in which order does
    not matter. If I’m calculating a sum of something, it doesn’t matter where we
    start. In Language — order usually matters. The embeddings contain semantic meanings,
    but no exact order meaning. They do hold order in a way because these embeddings
    were originally created according to some linguistic logic (baby appears closer
    to sleep, not to state), but the same word can have more than one meaning in itself,
    and more importantly, different meaning when it's in a different context.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'Representing words as text without order is not good enough, we can improve
    this. The authors suggest we add positional encoding to the embeddings. We do
    this by calculating a position vector for every word and adding it (summing) the
    two vectors. The positional encoding vectors must be of the same size so they
    can be added. The formula for positional encoding uses two functions: sine for
    even positions (e.g. 0th word, 2d word, 4th, 6th, etc.) and cosine for odd positions
    (e.g. 1st, 3rd, 5th, etc.).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '***Visualization*** By looking at these functions (sin in red, cosine in blue)
    you can perhaps imagine why these two functions specifically were chosen. There
    is some symmetry between the functions, as there is between a word and the word
    that came before it, which helps model (represent) these related positions. Also,
    they output values from -1 to 1, which are very stable numbers to work with (they
    don’t get super big or super small).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '***可视化*** 通过观察这些函数（红色的正弦函数，蓝色的余弦函数），你也许可以想象为什么特别选择了这两个函数。函数之间存在一些对称性，就像单词与它之前的单词之间的对称性，这有助于模型（表示）这些相关的位置。此外，它们输出的值范围从
    -1 到 1，这些数字非常稳定（不会变得过大或过小）。'
- en: '![](../Images/8b18820bfa435ba932b01a95329315dc.png)![](../Images/fe240d159361255b14a25d37e3634d24.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b18820bfa435ba932b01a95329315dc.png)![](../Images/fe240d159361255b14a25d37e3634d24.png)'
- en: Formula image from the [original paper](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
    by Vaswani, A. et al.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 公式图像来自Vaswani, A.等人的[原始论文](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)。
- en: In the formula above, the upper row represents even numbers starting from 0
    (i = 0) and continues to be even-numbered (2*1, 2*2, 2*3). The second row represents
    odd numbers in the same way.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述公式中，第一行表示从0开始的偶数（i = 0），并继续为偶数（2*1，2*2，2*3）。第二行以相同的方式表示奇数。
- en: Every positional vector is a number_of_dimensions (512 in our case) vector with
    numbers from 0 to 1.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 每个位置向量是一个具有维度数（在我们的例子中为512）的向量，数字范围从0到1。
- en: '***Code***'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '***代码***'
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If we print the first word, we see we only get 0 and 1 interchangeably.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们打印第一个单词，我们会发现只得到0和1交替出现。
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The second number is already much more diverse.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个数字已经多样化得多。
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*Code inspiration is from [here](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*代码灵感来自[这里](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/)*。'
- en: We have seen that different positions result in different representations. In
    order to finalize the section input as a whole (squared in red in the picture
    below) we add the numbers in the position matrix to our input embeddings matrix.
    We end up getting a matrix of the same size as the embedding, only this time the
    numbers contain semantic meaning + order.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到不同的位置导致不同的表示。为了最终确定作为整体的部分输入（在下图中用红色标出），我们将位置矩阵中的数字添加到我们的输入嵌入矩阵中。最终得到的矩阵与嵌入的矩阵大小相同，只不过这次数字包含了语义意义+顺序。
- en: '![](../Images/c538dd4cc49a64d18d3ff1415f645712.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c538dd4cc49a64d18d3ff1415f645712.png)'
- en: Image from the [original paper](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
    by Vaswani, A. et al.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来自Vaswani, A.等人的[原始论文](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)。
- en: '***Summary*** This concludes our first part of the series (Rectangled in red).
    We talked about the model gets its inputs. We saw how to break down text to its
    features (tokens), represent them as numbers (embeddings) and a smart way to add
    positional encoding to these numbers.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '***总结*** 这总结了我们系列的第一部分（用红色标出）。我们讨论了模型如何获取输入。我们查看了如何将文本拆分为特征（标记），将它们表示为数字（嵌入），以及将位置编码添加到这些数字中的智能方法。'
- en: '[The next part](https://medium.com/@chenmargalit/transformers-part-3-attention-7b95881714df)
    will focus on the different mechanics of the Encoder block (the first gray rectangle),
    with each section describing a different coloured rectangle (e.g. Multi head attention,
    Add & Norm, etc.)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[下一部分](https://medium.com/@chenmargalit/transformers-part-3-attention-7b95881714df)将重点讨论编码器块（第一个灰色矩形）的不同机制，每个部分描述不同颜色的矩形（例如，多头注意力、添加和归一化等）。'
