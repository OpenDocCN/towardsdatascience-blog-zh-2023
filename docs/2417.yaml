- en: 'Simplifying Transformers: State of the Art NLP Using Words You Understand ‚Äî
    part 2‚Äî Input'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/transformers-part-2-input-2a8c3a141c7d?source=collection_archive---------2-----------------------#2023-07-26](https://towardsdatascience.com/transformers-part-2-input-2a8c3a141c7d?source=collection_archive---------2-----------------------#2023-07-26)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Deep dive into how transformers‚Äô inputs are constructed
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@chenmargalit?source=post_page-----2a8c3a141c7d--------------------------------)[![Chen
    Margalit](../Images/fb37720654b3d1068b448d4d9ad624d5.png)](https://medium.com/@chenmargalit?source=post_page-----2a8c3a141c7d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2a8c3a141c7d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2a8c3a141c7d--------------------------------)
    [Chen Margalit](https://medium.com/@chenmargalit?source=post_page-----2a8c3a141c7d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff8e6113b0479&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-part-2-input-2a8c3a141c7d&user=Chen+Margalit&userId=f8e6113b0479&source=post_page-f8e6113b0479----2a8c3a141c7d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2a8c3a141c7d--------------------------------)
    ¬∑10 min read¬∑Jul 26, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2a8c3a141c7d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-part-2-input-2a8c3a141c7d&user=Chen+Margalit&userId=f8e6113b0479&source=-----2a8c3a141c7d---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2a8c3a141c7d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-part-2-input-2a8c3a141c7d&source=-----2a8c3a141c7d---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: Inputs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dragon hatches from eggs, babies spring out from bellies, AI-generated text
    starts from inputs. We all have to start somewhere.
  prefs: []
  type: TYPE_NORMAL
- en: What kind of inputs? it depends on the task at hand. If you‚Äôre building a language
    model, a software that knows how to generate relevant text (the Transformers architecture
    is useful in diverse scenarios) the input is text. Nonetheless, can a computer
    receive any kind of input (text, image, sound) and magically know how to process
    it? it doesn‚Äôt.
  prefs: []
  type: TYPE_NORMAL
- en: I‚Äôm sure you know people who aren‚Äôt very good with words but are great with
    numbers. The computer is something like that. It cannot process text directly
    in the CPU/GPU (where the calculations happen), but it can certainly work with
    numbers! As you will soon see, the way to represent these words as numbers is
    a crucial ingredient in the secret sauce.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f4213bd23253b425f574bfe74a6638cb.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from the [original paper](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
    by Vaswani, A. et al.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Tokenization is the process of transforming the corpus (all the text you‚Äôve
    got) into smaller parts that the machine can make better use of. Say we have a
    dataset of 10,000 Wikipedia articles. We take each character and we transform
    (tokenize) it. There are many ways to tokenize text, let''s see how [OpenAi‚Äôs
    tokenizer](https://platform.openai.com/tokenizer) does it with the following text:'
  prefs: []
  type: TYPE_NORMAL
- en: '‚Äú*Many words map to one token, but some don‚Äôt: indivisible.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Unicode characters like emojis may be split into many tokens containing the
    underlying bytes: ü§öüèæ*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Sequences of characters commonly found next to each other may be grouped together:
    1234567890*"'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the tokenization result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8dd4cdd6e1ff1009368b0e4488218986.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by OpenAi, taken from [here](https://platform.openai.com/tokenizer)
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, there are around 40 words (depending on how you count (punctuation
    signs). Out of these 40 words, 64 tokens were generated. Sometimes the token is
    the entire word, as with ‚ÄúMany, words, map‚Äù and sometimes it's a part of a word,
    as with ‚ÄúUnicode‚Äù. Why do we break entire words into smaller parts? why even divide
    sentences? we could‚Äôve held them contact. In the end, they are converted to numbers
    anyway so what‚Äôs the difference in the computer‚Äôs point of view if the token is
    3 characters long or 30?
  prefs: []
  type: TYPE_NORMAL
- en: 'Tokens help the model learn because as text is our data, they are the data‚Äôs
    features. Different ways of engineering those features will lead to variations
    in performance. For example, in the sentence: ‚ÄúGet out!!!!!!!‚Äù, we need to decide
    if multiple ‚Äú!‚Äù are different than just one, or if it has the same meaning. Technically
    we could‚Äôve kept the sentences as a whole, but Imagine looking at a crowd vs.
    at each person individually, in which scenario will you get better insights?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have tokens we can build a lookup dictionary that will allow us
    to get rid of words and use indexes (numbers) instead. For example, if our whole
    dataset is the sentence: ‚ÄúWhere is god‚Äù. We might build this kind of vocabulary,
    which is just a key:value pair of the words and a single number representing them.
    We won''t have to use the entire word every time, we can use the number. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '{Where: 0, is: 1, god: 2}. Whenever we encounter the word ‚Äúis‚Äù, we replace
    it with 1\. For more examples of tokenizers, you can check the one [Google](https://github.com/google/sentencepiece)
    developed or play some more with OpenAI‚Äôs [TikToken](https://github.com/openai/tiktoken).'
  prefs: []
  type: TYPE_NORMAL
- en: Word to Vector
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '***Intuition*** We are making great progress in our journey to represent words
    as numbers. The next step will be to generate numeric, **semantic** representations
    from those tokens. To do so, we can use an algorithm called [Word2Vec](https://arxiv.org/abs/1301.3781).
    The details aren''t very important at the moment, but the main idea is that you
    take a vector (we‚Äôll simplify for now, think of a regular list) of numbers in
    any size you want (the [paper‚Äôs](https://arxiv.org/abs/1706.03762) authors used
    512) and this list of numbers should represent the semantic meaning of a word.
    Imagine a list of numbers like [-2, 4,-3.7, 41‚Ä¶-0.98] which actually holds the
    semantic representation of a word. It should be created in such a way, that if
    we plot these vectors on a 2D graph, similar terms will be closer than dissimilar
    terms.'
  prefs: []
  type: TYPE_NORMAL
- en: As You can see in the picture (taken from [here](https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial)),
    ‚ÄúBaby‚Äù is close to ‚ÄúAww‚Äù and ‚ÄúAsleep‚Äù whereas ‚ÄúCitizen‚Äù/‚ÄúState‚Äù/‚ÄúAmerica‚Äôs‚Äù are
    also somewhat grouped together.
  prefs: []
  type: TYPE_NORMAL
- en: '*2D word vectors (a.k.a a list with 2 numbers) will not be able to hold any
    accurate meaning even for one word, as mentioned the authors used 512 numbers.
    Since we can''t plot anything with 512 dimensions, we use a method called [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)
    to reduce the number of dimensions to two, hopefully preserving much of the original
    meaning. In the [3rd part of this series](https://medium.com/@chenmargalit/transformers-part-3-attention-7b95881714df)
    we deep our toes a bit into how that happens.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3f3e1a460426ae91de692d590b91f3c1.png)'
  prefs: []
  type: TYPE_IMG
- en: Word2Vec 2D presentation ‚Äî image by Piere Mergret from [here](https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial)
  prefs: []
  type: TYPE_NORMAL
- en: It works! You can actually train a model that will be able to produce lists
    of numbers that hold semantic meanings. The computer doesn't know a baby is a
    screaming, sleep depriving (super sweet) small human but it knows it usually sees
    that baby word around ‚Äúaww‚Äù, more often than ‚ÄúState‚Äù and ‚ÄúGovernment‚Äù. I‚Äôll be
    writing some more on exactly how that happens, but until then if you‚Äôre interested,
    [this](https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial) might
    be a good place to check out.
  prefs: []
  type: TYPE_NORMAL
- en: These ‚Äúlists of numbers‚Äù are pretty important, so they get their own name in
    the ML terminology which is Embeddings. Why embeddings? because we are performing
    an embedding (so creative) which is the process of mapping (translating) a term
    from one form (words) to another (list of numbers). These are a lot of ().
  prefs: []
  type: TYPE_NORMAL
- en: From here on we will call words, embeddings, which as explained are lists of
    numbers that hold the semantic meaning of any word it's trained to represent.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Embeddings with Pytorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We first calculate the number of unique tokens we have, for simplicity let‚Äôs
    say 2\. The creation of the embeddings layer, which is the first part of the Transformer
    architecture, will be as simple as writing this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '*General code remark ‚Äî don‚Äôt take this code and its conventions as good coding
    style, it‚Äôs written particularly to make it easy to understand.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Code***'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We now have an embedding matrix which in this case is a 2 by 2 matrix, generated
    with random numbers derived from the normal distribution N(0,1) (e.g. a distribution
    with mean 0 and variance 1).
  prefs: []
  type: TYPE_NORMAL
- en: Note the requires_grad=True, this is Pytorch language for saying these 4 numbers
    are learnable weights. They can and will be customized in the learning process
    to better represent the data the model receives.
  prefs: []
  type: TYPE_NORMAL
- en: In a more realistic scenario, we can expect something closer to a 10k by 512
    matrix which represents our entire dataset in numbers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '*Fun fact (we can think of things that are more fun), you sometimes hear language
    models use billions of parameters. This initial, not too crazy layer, holds 10_000
    by 512 parameters which are 5 million parameters. This LLM (Large Language Model)
    is difficult stuff, it needs a lot of calculations.'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters here is a fancy word for those numbers (-1.525 etc.) just that they
    are subject to change and will change during training.
  prefs: []
  type: TYPE_NORMAL
- en: These numbers are the learning of the machine, this is what the machine is learning.
    Later when we give it input, we multiply the input with those numbers, and we
    hopefully get a good result. What do you know, numbers matter. When you‚Äôre important,
    you get your own name, so those aren‚Äôt just numbers, those are parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Why use as many as 512 and not 5? because more numbers mean we can probably
    generate more accurate meaning. Great, stop thinking small, let's use a million
    then! why not? because more numbers mean more calculations, more computing power,
    more expensive to train, etc. 512 has been found to be a good place in the middle.
  prefs: []
  type: TYPE_NORMAL
- en: Sequence Length
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When training the model we are going to put a whole bunch of words together.
    It's more computationally efficient and it helps the model learn as it gets more
    context together. As mentioned every word will be represented by a 512-dimensional
    vector (list with 512 numbers) and each time we pass inputs to the model (a.k.a
    forward pass), we will send a bunch of sentences, not only one. For example, we
    decided to support a 50-word sequence. This means we are going to take the x number
    of words in a sentence, if x > 50 we split it and only take the first 50, if x
    < 50, we still need the size to be the exact same (I‚Äôll soon explain why). To
    solve this we add padding which are special dummy strings, to the rest of the
    sentence. For example, if we support a 7-word sentence, and we have the sentence
    ‚ÄúWhere is god‚Äù. We add 4 paddings, so the input to the model will be ‚ÄúWhere is
    god <PAD> <PAD> <PAD> <PAD>‚Äù. Actually, we usually add at least 2 more special
    paddings so the model knows where the sentence starts and where it ends, so it
    will actually be something like ‚Äú<StartOfSentence> Where is god <PAD> <PAD> <EndOfSentence>‚Äù.
  prefs: []
  type: TYPE_NORMAL
- en: '* Why must all input vectors be of the same size? because software has ‚Äúexpectations‚Äù,
    and matrices have even stricter expectations. You can‚Äôt do any ‚Äúmathy‚Äù calculation
    you want, it has to adhere to certain rules, and one of those rules is adequate
    vector sizes.'
  prefs: []
  type: TYPE_NORMAL
- en: Positional encodings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '***Intuition*** We now have a way to represent (and learn) words in our vocabulary.
    Let‚Äôs make it even better by encoding the position of the words. Why is this important?
    because if we take these two sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. The man played with my cat
  prefs: []
  type: TYPE_NORMAL
- en: 2\. The cat played with my man
  prefs: []
  type: TYPE_NORMAL
- en: We can represent the two sentences using the exact same embeddings, but the
    sentences have different meanings. We can think of such data in which order does
    not matter. If I‚Äôm calculating a sum of something, it doesn‚Äôt matter where we
    start. In Language ‚Äî order usually matters. The embeddings contain semantic meanings,
    but no exact order meaning. They do hold order in a way because these embeddings
    were originally created according to some linguistic logic (baby appears closer
    to sleep, not to state), but the same word can have more than one meaning in itself,
    and more importantly, different meaning when it's in a different context.
  prefs: []
  type: TYPE_NORMAL
- en: 'Representing words as text without order is not good enough, we can improve
    this. The authors suggest we add positional encoding to the embeddings. We do
    this by calculating a position vector for every word and adding it (summing) the
    two vectors. The positional encoding vectors must be of the same size so they
    can be added. The formula for positional encoding uses two functions: sine for
    even positions (e.g. 0th word, 2d word, 4th, 6th, etc.) and cosine for odd positions
    (e.g. 1st, 3rd, 5th, etc.).'
  prefs: []
  type: TYPE_NORMAL
- en: '***Visualization*** By looking at these functions (sin in red, cosine in blue)
    you can perhaps imagine why these two functions specifically were chosen. There
    is some symmetry between the functions, as there is between a word and the word
    that came before it, which helps model (represent) these related positions. Also,
    they output values from -1 to 1, which are very stable numbers to work with (they
    don‚Äôt get super big or super small).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8b18820bfa435ba932b01a95329315dc.png)![](../Images/fe240d159361255b14a25d37e3634d24.png)'
  prefs: []
  type: TYPE_IMG
- en: Formula image from the [original paper](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
    by Vaswani, A. et al.
  prefs: []
  type: TYPE_NORMAL
- en: In the formula above, the upper row represents even numbers starting from 0
    (i = 0) and continues to be even-numbered (2*1, 2*2, 2*3). The second row represents
    odd numbers in the same way.
  prefs: []
  type: TYPE_NORMAL
- en: Every positional vector is a number_of_dimensions (512 in our case) vector with
    numbers from 0 to 1.
  prefs: []
  type: TYPE_NORMAL
- en: '***Code***'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If we print the first word, we see we only get 0 and 1 interchangeably.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The second number is already much more diverse.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '*Code inspiration is from [here](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/).'
  prefs: []
  type: TYPE_NORMAL
- en: We have seen that different positions result in different representations. In
    order to finalize the section input as a whole (squared in red in the picture
    below) we add the numbers in the position matrix to our input embeddings matrix.
    We end up getting a matrix of the same size as the embedding, only this time the
    numbers contain semantic meaning + order.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c538dd4cc49a64d18d3ff1415f645712.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from the [original paper](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
    by Vaswani, A. et al.
  prefs: []
  type: TYPE_NORMAL
- en: '***Summary*** This concludes our first part of the series (Rectangled in red).
    We talked about the model gets its inputs. We saw how to break down text to its
    features (tokens), represent them as numbers (embeddings) and a smart way to add
    positional encoding to these numbers.'
  prefs: []
  type: TYPE_NORMAL
- en: '[The next part](https://medium.com/@chenmargalit/transformers-part-3-attention-7b95881714df)
    will focus on the different mechanics of the Encoder block (the first gray rectangle),
    with each section describing a different coloured rectangle (e.g. Multi head attention,
    Add & Norm, etc.)'
  prefs: []
  type: TYPE_NORMAL
