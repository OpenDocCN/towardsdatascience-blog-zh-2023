- en: 'Simplifying Transformers: State of the Art NLP Using Words You Understand â€”
    part 2â€” Input'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç®€åŒ– Transformersï¼šä½¿ç”¨ä½ ç†è§£çš„è¯è¯­çš„æœ€å‰æ²¿ NLP â€” ç¬¬äºŒéƒ¨åˆ† â€” è¾“å…¥
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/transformers-part-2-input-2a8c3a141c7d?source=collection_archive---------2-----------------------#2023-07-26](https://towardsdatascience.com/transformers-part-2-input-2a8c3a141c7d?source=collection_archive---------2-----------------------#2023-07-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/transformers-part-2-input-2a8c3a141c7d?source=collection_archive---------2-----------------------#2023-07-26](https://towardsdatascience.com/transformers-part-2-input-2a8c3a141c7d?source=collection_archive---------2-----------------------#2023-07-26)
- en: Deep dive into how transformersâ€™ inputs are constructed
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ·±å…¥æ¢è®¨ Transformer è¾“å…¥çš„æ„å»ºæ–¹å¼
- en: '[](https://medium.com/@chenmargalit?source=post_page-----2a8c3a141c7d--------------------------------)[![Chen
    Margalit](../Images/fb37720654b3d1068b448d4d9ad624d5.png)](https://medium.com/@chenmargalit?source=post_page-----2a8c3a141c7d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2a8c3a141c7d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2a8c3a141c7d--------------------------------)
    [Chen Margalit](https://medium.com/@chenmargalit?source=post_page-----2a8c3a141c7d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@chenmargalit?source=post_page-----2a8c3a141c7d--------------------------------)[![Chen
    Margalit](../Images/fb37720654b3d1068b448d4d9ad624d5.png)](https://medium.com/@chenmargalit?source=post_page-----2a8c3a141c7d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2a8c3a141c7d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2a8c3a141c7d--------------------------------)
    [Chen Margalit](https://medium.com/@chenmargalit?source=post_page-----2a8c3a141c7d--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff8e6113b0479&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-part-2-input-2a8c3a141c7d&user=Chen+Margalit&userId=f8e6113b0479&source=post_page-f8e6113b0479----2a8c3a141c7d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2a8c3a141c7d--------------------------------)
    Â·10 min readÂ·Jul 26, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2a8c3a141c7d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-part-2-input-2a8c3a141c7d&user=Chen+Margalit&userId=f8e6113b0479&source=-----2a8c3a141c7d---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff8e6113b0479&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-part-2-input-2a8c3a141c7d&user=Chen+Margalit&userId=f8e6113b0479&source=post_page-f8e6113b0479----2a8c3a141c7d---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2a8c3a141c7d--------------------------------)
    Â·10åˆ†é’Ÿé˜…è¯»Â·2023å¹´7æœˆ26æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2a8c3a141c7d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-part-2-input-2a8c3a141c7d&user=Chen+Margalit&userId=f8e6113b0479&source=-----2a8c3a141c7d---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2a8c3a141c7d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-part-2-input-2a8c3a141c7d&source=-----2a8c3a141c7d---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2a8c3a141c7d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-part-2-input-2a8c3a141c7d&source=-----2a8c3a141c7d---------------------bookmark_footer-----------)'
- en: Inputs
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¾“å…¥
- en: Dragon hatches from eggs, babies spring out from bellies, AI-generated text
    starts from inputs. We all have to start somewhere.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: é¾™ä»è›‹ä¸­å­µåŒ–ï¼Œå©´å„¿ä»æ¯è…¹ä¸­è¯ç”Ÿï¼ŒAI ç”Ÿæˆçš„æ–‡æœ¬ä»è¾“å…¥å¼€å§‹ã€‚æˆ‘ä»¬éƒ½å¿…é¡»ä»æŸä¸ªåœ°æ–¹å¼€å§‹ã€‚
- en: What kind of inputs? it depends on the task at hand. If youâ€™re building a language
    model, a software that knows how to generate relevant text (the Transformers architecture
    is useful in diverse scenarios) the input is text. Nonetheless, can a computer
    receive any kind of input (text, image, sound) and magically know how to process
    it? it doesnâ€™t.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆç±»å‹çš„è¾“å…¥ï¼Ÿè¿™å–å†³äºæ‰‹å¤´çš„ä»»åŠ¡ã€‚å¦‚æœä½ æ­£åœ¨æ„å»ºä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼Œä¸€ä¸ªèƒ½å¤Ÿç”Ÿæˆç›¸å…³æ–‡æœ¬çš„ç¨‹åºï¼ˆTransformers æ¶æ„åœ¨å„ç§åœºæ™¯ä¸­éƒ½å¾ˆæœ‰ç”¨ï¼‰ï¼Œè¾“å…¥å°±æ˜¯æ–‡æœ¬ã€‚ç„¶è€Œï¼Œè®¡ç®—æœºèƒ½å¦æ¥æ”¶ä»»ä½•ç±»å‹çš„è¾“å…¥ï¼ˆæ–‡æœ¬ã€å›¾åƒã€å£°éŸ³ï¼‰å¹¶ç¥å¥‡åœ°çŸ¥é“å¦‚ä½•å¤„ç†å®ƒï¼Ÿå®ƒå¹¶ä¸èƒ½ã€‚
- en: Iâ€™m sure you know people who arenâ€™t very good with words but are great with
    numbers. The computer is something like that. It cannot process text directly
    in the CPU/GPU (where the calculations happen), but it can certainly work with
    numbers! As you will soon see, the way to represent these words as numbers is
    a crucial ingredient in the secret sauce.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ç›¸ä¿¡ä½ è®¤è¯†ä¸€äº›ä¸æ“…é•¿æ–‡å­—ä½†æ“…é•¿æ•°å­—çš„äººã€‚è®¡ç®—æœºæœ‰ç‚¹ç±»ä¼¼äºè¿™æ ·ã€‚å®ƒä¸èƒ½ç›´æ¥åœ¨CPU/GPUï¼ˆè¿›è¡Œè®¡ç®—çš„åœ°æ–¹ï¼‰å¤„ç†æ–‡æœ¬ï¼Œä½†å®ƒå¯ä»¥å¤„ç†æ•°å­—ï¼æ­£å¦‚ä½ å°†å¾ˆå¿«çœ‹åˆ°çš„ï¼Œå°†è¿™äº›å•è¯è¡¨ç¤ºä¸ºæ•°å­—çš„æ–¹å¼æ˜¯ç§˜å¯†é…æ–¹ä¸­çš„å…³é”®æˆåˆ†ã€‚
- en: '![](../Images/f4213bd23253b425f574bfe74a6638cb.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4213bd23253b425f574bfe74a6638cb.png)'
- en: Image from the [original paper](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
    by Vaswani, A. et al.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥è‡ªVaswani, A.ç­‰äººçš„[åŸå§‹è®ºæ–‡](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
- en: Tokenizer
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ ‡è®°åŒ–å™¨
- en: 'Tokenization is the process of transforming the corpus (all the text youâ€™ve
    got) into smaller parts that the machine can make better use of. Say we have a
    dataset of 10,000 Wikipedia articles. We take each character and we transform
    (tokenize) it. There are many ways to tokenize text, let''s see how [OpenAiâ€™s
    tokenizer](https://platform.openai.com/tokenizer) does it with the following text:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡è®°åŒ–æ˜¯å°†è¯­æ–™åº“ï¼ˆä½ æ‹¥æœ‰çš„æ‰€æœ‰æ–‡æœ¬ï¼‰è½¬æ¢ä¸ºæœºå™¨å¯ä»¥æ›´å¥½åˆ©ç”¨çš„æ›´å°éƒ¨åˆ†çš„è¿‡ç¨‹ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ª10,000ç¯‡ç»´åŸºç™¾ç§‘æ–‡ç« çš„æ•°æ®é›†ã€‚æˆ‘ä»¬å°†æ¯ä¸ªå­—ç¬¦è¿›è¡Œè½¬æ¢ï¼ˆæ ‡è®°åŒ–ï¼‰ã€‚æœ‰å¾ˆå¤šç§æ ‡è®°åŒ–æ–‡æœ¬çš„æ–¹æ³•ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹[OpenAIçš„æ ‡è®°åŒ–å™¨](https://platform.openai.com/tokenizer)å¦‚ä½•å¤„ç†ä»¥ä¸‹æ–‡æœ¬ï¼š
- en: 'â€œ*Many words map to one token, but some donâ€™t: indivisible.*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: â€œ*è®¸å¤šå•è¯æ˜ å°„åˆ°ä¸€ä¸ªæ ‡è®°ï¼Œä½†æœ‰äº›åˆ™ä¸æ˜¯ï¼šä¸å¯åˆ†å‰²çš„ã€‚*
- en: '*Unicode characters like emojis may be split into many tokens containing the
    underlying bytes: ğŸ¤šğŸ¾*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*åƒè¡¨æƒ…ç¬¦å·è¿™æ ·çš„Unicodeå­—ç¬¦å¯èƒ½ä¼šè¢«æ‹†åˆ†æˆåŒ…å«åº•å±‚å­—èŠ‚çš„å¤šä¸ªæ ‡è®°ï¼šğŸ¤šğŸ¾*'
- en: '*Sequences of characters commonly found next to each other may be grouped together:
    1234567890*"'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¸¸è§çš„å­—ç¬¦åºåˆ—å¯èƒ½ä¼šè¢«ç»„åˆåœ¨ä¸€èµ·ï¼š1234567890*'
- en: 'This is the tokenization result:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æ ‡è®°åŒ–çš„ç»“æœï¼š
- en: '![](../Images/8dd4cdd6e1ff1009368b0e4488218986.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8dd4cdd6e1ff1009368b0e4488218986.png)'
- en: Image by OpenAi, taken from [here](https://platform.openai.com/tokenizer)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥è‡ªOpenAIï¼Œæ¥æºäº[è¿™é‡Œ](https://platform.openai.com/tokenizer)
- en: As you can see, there are around 40 words (depending on how you count (punctuation
    signs). Out of these 40 words, 64 tokens were generated. Sometimes the token is
    the entire word, as with â€œMany, words, mapâ€ and sometimes it's a part of a word,
    as with â€œUnicodeâ€. Why do we break entire words into smaller parts? why even divide
    sentences? we couldâ€™ve held them contact. In the end, they are converted to numbers
    anyway so whatâ€™s the difference in the computerâ€™s point of view if the token is
    3 characters long or 30?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œè¿™é‡Œå¤§çº¦æœ‰40ä¸ªå•è¯ï¼ˆå–å†³äºå¦‚ä½•è®¡ç®—ï¼ˆæ ‡ç‚¹ç¬¦å·ï¼‰ï¼‰ã€‚åœ¨è¿™40ä¸ªå•è¯ä¸­ï¼Œç”Ÿæˆäº†64ä¸ªæ ‡è®°ã€‚æœ‰æ—¶å€™æ ‡è®°æ˜¯æ•´ä¸ªå•è¯ï¼Œæ¯”å¦‚â€œMany, words,
    mapâ€ï¼Œæœ‰æ—¶å€™åˆ™æ˜¯å•è¯çš„ä¸€éƒ¨åˆ†ï¼Œæ¯”å¦‚â€œUnicodeâ€ã€‚æˆ‘ä»¬ä¸ºä»€ä¹ˆè¦å°†æ•´ä¸ªå•è¯æ‹†åˆ†æˆæ›´å°çš„éƒ¨åˆ†ï¼Ÿç”šè‡³æ‹†åˆ†å¥å­åˆæœ‰ä½•æ„ä¹‰ï¼Ÿæœ€ç»ˆï¼Œå®ƒä»¬æ— è®ºå¦‚ä½•éƒ½ä¼šè¢«è½¬æ¢ä¸ºæ•°å­—ï¼Œé‚£ä¹ˆä»è®¡ç®—æœºçš„è§’åº¦æ¥çœ‹ï¼Œæ ‡è®°æ˜¯3ä¸ªå­—ç¬¦é•¿è¿˜æ˜¯30ä¸ªå­—ç¬¦é•¿æœ‰ä½•åŒºåˆ«ï¼Ÿ
- en: 'Tokens help the model learn because as text is our data, they are the dataâ€™s
    features. Different ways of engineering those features will lead to variations
    in performance. For example, in the sentence: â€œGet out!!!!!!!â€, we need to decide
    if multiple â€œ!â€ are different than just one, or if it has the same meaning. Technically
    we couldâ€™ve kept the sentences as a whole, but Imagine looking at a crowd vs.
    at each person individually, in which scenario will you get better insights?'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡è®°å¸®åŠ©æ¨¡å‹å­¦ä¹ ï¼Œå› ä¸ºæ–‡æœ¬æ˜¯æˆ‘ä»¬çš„æ•°æ®ï¼Œå®ƒä»¬æ˜¯æ•°æ®çš„ç‰¹å¾ã€‚å·¥ç¨‹åŒ–è¿™äº›ç‰¹å¾çš„ä¸åŒæ–¹å¼ä¼šå¯¼è‡´æ€§èƒ½çš„å˜åŒ–ã€‚ä¾‹å¦‚ï¼Œåœ¨å¥å­ï¼šâ€œGet out!!!!!!!â€ä¸­ï¼Œæˆ‘ä»¬éœ€è¦å†³å®šå¤šä¸ªâ€œï¼â€æ˜¯å¦ä¸ä¸€ä¸ªâ€œï¼â€ä¸åŒï¼Œè¿˜æ˜¯å®ƒä»¬æœ‰ç›¸åŒçš„æ„ä¹‰ã€‚ä»æŠ€æœ¯ä¸Šè®²ï¼Œæˆ‘ä»¬å¯ä»¥å°†å¥å­ä¿æŒä¸ºæ•´ä½“ï¼Œä½†æƒ³è±¡ä¸€ä¸‹çœ‹ä¸€ä¸ªäººç¾¤ä¸å•ç‹¬çœ‹æ¯ä¸ªäººçš„åœºæ™¯ï¼Œä½ ä¼šåœ¨ä»€ä¹ˆåœºæ™¯ä¸­è·å¾—æ›´å¥½çš„è§è§£ï¼Ÿ
- en: 'Now that we have tokens we can build a lookup dictionary that will allow us
    to get rid of words and use indexes (numbers) instead. For example, if our whole
    dataset is the sentence: â€œWhere is godâ€. We might build this kind of vocabulary,
    which is just a key:value pair of the words and a single number representing them.
    We won''t have to use the entire word every time, we can use the number. For example:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æœ‰äº†æ ‡è®°ï¼Œæˆ‘ä»¬å¯ä»¥å»ºç«‹ä¸€ä¸ªæŸ¥æ‰¾å­—å…¸ï¼Œè¿™å°†å…è®¸æˆ‘ä»¬æ‘†è„±å•è¯è€Œä½¿ç”¨ç´¢å¼•ï¼ˆæ•°å­—ï¼‰ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬çš„æ•´ä¸ªæ•°æ®é›†æ˜¯å¥å­ï¼šâ€œWhere is godâ€ã€‚æˆ‘ä»¬å¯èƒ½ä¼šå»ºç«‹è¿™æ ·çš„è¯æ±‡è¡¨ï¼Œå®ƒåªæ˜¯å•è¯å’Œä¸€ä¸ªä»£è¡¨å®ƒä»¬çš„æ•°å­—çš„é”®å€¼å¯¹ã€‚æˆ‘ä»¬ä¸å¿…æ¯æ¬¡éƒ½ä½¿ç”¨æ•´ä¸ªå•è¯ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ•°å­—ã€‚ä¾‹å¦‚ï¼š
- en: '{Where: 0, is: 1, god: 2}. Whenever we encounter the word â€œisâ€, we replace
    it with 1\. For more examples of tokenizers, you can check the one [Google](https://github.com/google/sentencepiece)
    developed or play some more with OpenAIâ€™s [TikToken](https://github.com/openai/tiktoken).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '{Where: 0, is: 1, god: 2}ã€‚æ¯å½“æˆ‘ä»¬é‡åˆ°å•è¯â€œisâ€æ—¶ï¼Œæˆ‘ä»¬ç”¨1æ¥æ›¿æ¢å®ƒã€‚æƒ³äº†è§£æ›´å¤šå…³äºåˆ†è¯å™¨çš„ä¾‹å­ï¼Œä½ å¯ä»¥æŸ¥çœ‹[Google](https://github.com/google/sentencepiece)å¼€å‘çš„åˆ†è¯å™¨ï¼Œæˆ–è€…å¤šç©ä¸€äº›
    OpenAI çš„[TikToken](https://github.com/openai/tiktoken)ã€‚'
- en: Word to Vector
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Word to Vector
- en: '***Intuition*** We are making great progress in our journey to represent words
    as numbers. The next step will be to generate numeric, **semantic** representations
    from those tokens. To do so, we can use an algorithm called [Word2Vec](https://arxiv.org/abs/1301.3781).
    The details aren''t very important at the moment, but the main idea is that you
    take a vector (weâ€™ll simplify for now, think of a regular list) of numbers in
    any size you want (the [paperâ€™s](https://arxiv.org/abs/1706.03762) authors used
    512) and this list of numbers should represent the semantic meaning of a word.
    Imagine a list of numbers like [-2, 4,-3.7, 41â€¦-0.98] which actually holds the
    semantic representation of a word. It should be created in such a way, that if
    we plot these vectors on a 2D graph, similar terms will be closer than dissimilar
    terms.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '***ç›´è§‰*** æˆ‘ä»¬åœ¨å°†è¯è¡¨ç¤ºä¸ºæ•°å­—çš„æ—…ç¨‹ä¸­å–å¾—äº†å¾ˆå¤§è¿›å±•ã€‚ä¸‹ä¸€æ­¥å°†æ˜¯ä»è¿™äº›æ ‡è®°ç”Ÿæˆæ•°å€¼çš„**è¯­ä¹‰**è¡¨ç¤ºã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ç§ç§°ä¸º[Word2Vec](https://arxiv.org/abs/1301.3781)çš„ç®—æ³•ã€‚ç»†èŠ‚ç›®å‰ä¸é‡è¦ï¼Œä½†ä¸»è¦æ€æƒ³æ˜¯ä½ æ‹¿ä¸€ä¸ªå‘é‡ï¼ˆæˆ‘ä»¬æš‚æ—¶ç®€åŒ–ä¸€ä¸‹ï¼Œæƒ³è±¡æˆä¸€ä¸ªå¸¸è§„åˆ—è¡¨ï¼‰åŒ…å«ä»»æ„å¤§å°çš„æ•°å­—ï¼ˆ[è®ºæ–‡](https://arxiv.org/abs/1706.03762)çš„ä½œè€…ä½¿ç”¨äº†512ï¼‰ï¼Œè¿™ä¸ªæ•°å­—åˆ—è¡¨åº”è¯¥ä»£è¡¨ä¸€ä¸ªè¯çš„è¯­ä¹‰å«ä¹‰ã€‚æƒ³è±¡ä¸€ä¸ªæ•°å­—åˆ—è¡¨ï¼Œä¾‹å¦‚[-2,
    4, -3.7, 41â€¦-0.98]ï¼Œå®ƒå®é™…ä¸ŠåŒ…å«äº†ä¸€ä¸ªè¯çš„è¯­ä¹‰è¡¨ç¤ºã€‚å®ƒåº”è¯¥ä»¥è¿™æ ·çš„æ–¹å¼åˆ›å»ºï¼Œä»¥ä¾¿å¦‚æœæˆ‘ä»¬å°†è¿™äº›å‘é‡ç»˜åˆ¶åœ¨äºŒç»´å›¾ä¸Šï¼Œç›¸ä¼¼çš„è¯ä¼šæ¯”ä¸ç›¸ä¼¼çš„è¯æ›´é è¿‘ã€‚'
- en: As You can see in the picture (taken from [here](https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial)),
    â€œBabyâ€ is close to â€œAwwâ€ and â€œAsleepâ€ whereas â€œCitizenâ€/â€œStateâ€/â€œAmericaâ€™sâ€ are
    also somewhat grouped together.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ åœ¨å›¾ç‰‡ä¸­æ‰€è§ï¼ˆå›¾ç‰‡æ¥æºäº[è¿™é‡Œ](https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial)ï¼‰ï¼Œâ€œBabyâ€æ¥è¿‘äºâ€œAwwâ€å’Œâ€œAsleepâ€ï¼Œè€Œâ€œCitizenâ€/â€œStateâ€/â€œAmericaâ€™sâ€ä¹Ÿæœ‰äº›è¢«åˆ†ç»„åœ¨ä¸€èµ·ã€‚
- en: '*2D word vectors (a.k.a a list with 2 numbers) will not be able to hold any
    accurate meaning even for one word, as mentioned the authors used 512 numbers.
    Since we can''t plot anything with 512 dimensions, we use a method called [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)
    to reduce the number of dimensions to two, hopefully preserving much of the original
    meaning. In the [3rd part of this series](https://medium.com/@chenmargalit/transformers-part-3-attention-7b95881714df)
    we deep our toes a bit into how that happens.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*äºŒç»´è¯å‘é‡ï¼ˆå³ä¸€ä¸ªåŒ…å«2ä¸ªæ•°å­—çš„åˆ—è¡¨ï¼‰å³ä½¿å¯¹äºä¸€ä¸ªè¯ä¹Ÿæ— æ³•æ‰¿è½½ä»»ä½•å‡†ç¡®çš„æ„ä¹‰ï¼Œå¦‚å‰æ‰€è¿°ï¼Œä½œè€…ä½¿ç”¨äº†512ä¸ªæ•°å­—ã€‚ç”±äºæˆ‘ä»¬æ— æ³•ç”¨512ç»´ç»˜åˆ¶ä»»ä½•å†…å®¹ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ç§ç§°ä¸º[PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)çš„æ–¹æ³•æ¥å°†ç»´åº¦å‡å°‘åˆ°ä¸¤ä¸ªï¼Œå¸Œæœ›ä¿ç•™å¤§éƒ¨åˆ†åŸå§‹æ„ä¹‰ã€‚åœ¨[æœ¬ç³»åˆ—çš„ç¬¬3éƒ¨åˆ†](https://medium.com/@chenmargalit/transformers-part-3-attention-7b95881714df)ä¸­ï¼Œæˆ‘ä»¬ä¼šæ·±å…¥æ¢è®¨è¿™ä¸€è¿‡ç¨‹ã€‚'
- en: '![](../Images/3f3e1a460426ae91de692d590b91f3c1.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f3e1a460426ae91de692d590b91f3c1.png)'
- en: Word2Vec 2D presentation â€” image by Piere Mergret from [here](https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 2D æ¼”ç¤º â€” å›¾ç‰‡ç”± Piere Mergret æä¾›ï¼Œæ¥æºäº[è¿™é‡Œ](https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial)
- en: It works! You can actually train a model that will be able to produce lists
    of numbers that hold semantic meanings. The computer doesn't know a baby is a
    screaming, sleep depriving (super sweet) small human but it knows it usually sees
    that baby word around â€œawwâ€, more often than â€œStateâ€ and â€œGovernmentâ€. Iâ€™ll be
    writing some more on exactly how that happens, but until then if youâ€™re interested,
    [this](https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial) might
    be a good place to check out.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒæœ‰æ•ˆï¼ä½ å®é™…ä¸Šå¯ä»¥è®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆå…·æœ‰è¯­ä¹‰æ„ä¹‰çš„æ•°å­—åˆ—è¡¨ã€‚è®¡ç®—æœºä¸çŸ¥é“å©´å„¿æ˜¯ä¸€ä¸ªå°–å«çš„ã€ç¼ºä¹ç¡çœ çš„ï¼ˆè¶…çº§å¯çˆ±ï¼‰å°äººç±»ï¼Œä½†å®ƒçŸ¥é“å®ƒé€šå¸¸ä¼šåœ¨â€œawwâ€é™„è¿‘çœ‹åˆ°è¿™ä¸ªå©´å„¿è¯ï¼Œè€Œä¸æ˜¯â€œStateâ€å’Œâ€œGovernmentâ€ã€‚æˆ‘ä¼šè¯¦ç»†è¯´æ˜è¿™ä¸€è¿‡ç¨‹ï¼Œä½†å¦‚æœä½ æ„Ÿå…´è¶£çš„è¯ï¼Œ[è¿™ä¸ª](https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial)å¯èƒ½æ˜¯ä¸€ä¸ªä¸é”™çš„åœ°æ–¹ã€‚
- en: These â€œlists of numbersâ€ are pretty important, so they get their own name in
    the ML terminology which is Embeddings. Why embeddings? because we are performing
    an embedding (so creative) which is the process of mapping (translating) a term
    from one form (words) to another (list of numbers). These are a lot of ().
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›â€œæ•°å­—åˆ—è¡¨â€ç›¸å½“é‡è¦ï¼Œå› æ­¤åœ¨æœºå™¨å­¦ä¹ æœ¯è¯­ä¸­æœ‰äº†å®ƒä»¬è‡ªå·±çš„åç§°â€”â€”åµŒå…¥ï¼ˆEmbeddingsï¼‰ã€‚ä¸ºä»€ä¹ˆå«åµŒå…¥ï¼Ÿå› ä¸ºæˆ‘ä»¬åœ¨æ‰§è¡ŒåµŒå…¥ï¼ˆå¾ˆæœ‰åˆ›æ„ï¼‰ï¼Œè¿™æ˜¯å°†ä¸€ä¸ªæœ¯è¯­ä»ä¸€ç§å½¢å¼ï¼ˆè¯ï¼‰æ˜ å°„ï¼ˆç¿»è¯‘ï¼‰åˆ°å¦ä¸€ç§å½¢å¼ï¼ˆæ•°å­—åˆ—è¡¨ï¼‰çš„è¿‡ç¨‹ã€‚è¿™äº›æœ‰å¾ˆå¤šï¼ˆï¼‰ã€‚
- en: From here on we will call words, embeddings, which as explained are lists of
    numbers that hold the semantic meaning of any word it's trained to represent.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ç°åœ¨å¼€å§‹ï¼Œæˆ‘ä»¬å°†æŠŠè¯æ±‡ç§°ä¸ºåµŒå…¥ï¼Œæ­£å¦‚ä¹‹å‰è§£é‡Šçš„ï¼Œå®ƒä»¬æ˜¯åŒ…å«ä»»ä½•è¯è¯­è¯­ä¹‰æ„ä¹‰çš„æ•°å­—åˆ—è¡¨ã€‚
- en: Creating Embeddings with Pytorch
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Pytorchåˆ›å»ºåµŒå…¥
- en: 'We first calculate the number of unique tokens we have, for simplicity letâ€™s
    say 2\. The creation of the embeddings layer, which is the first part of the Transformer
    architecture, will be as simple as writing this code:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆè®¡ç®—æˆ‘ä»¬æœ‰å¤šå°‘ä¸ªå”¯ä¸€çš„æ ‡è®°ï¼Œä¸ºäº†ç®€åŒ–èµ·è§ï¼Œå‡è®¾ä¸º2ã€‚åµŒå…¥å±‚çš„åˆ›å»ºï¼Œå³Transformeræ¶æ„çš„ç¬¬ä¸€éƒ¨åˆ†ï¼Œå°†åƒè¿™æ ·ç®€å•ï¼š
- en: '*General code remark â€” donâ€™t take this code and its conventions as good coding
    style, itâ€™s written particularly to make it easy to understand.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*ä¸€èˆ¬ä»£ç è¯´æ˜â€”â€”ä¸è¦æŠŠè¿™æ®µä»£ç åŠå…¶çº¦å®šå½“ä½œè‰¯å¥½çš„ç¼–ç é£æ ¼ï¼Œå®ƒçš„ç¼–å†™ç‰¹åˆ«æ˜¯ä¸ºäº†æ˜“äºç†è§£ã€‚*'
- en: '***Code***'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '***ä»£ç ***'
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We now have an embedding matrix which in this case is a 2 by 2 matrix, generated
    with random numbers derived from the normal distribution N(0,1) (e.g. a distribution
    with mean 0 and variance 1).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨æœ‰ä¸€ä¸ªåµŒå…¥çŸ©é˜µï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒæ˜¯ä¸€ä¸ª2ä¹˜2çš„çŸ©é˜µï¼Œç”Ÿæˆäºä»æ­£æ€åˆ†å¸ƒN(0,1)ï¼ˆä¾‹å¦‚ï¼Œå‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1çš„åˆ†å¸ƒï¼‰ä¸­å¾—åˆ°çš„éšæœºæ•°ã€‚
- en: Note the requires_grad=True, this is Pytorch language for saying these 4 numbers
    are learnable weights. They can and will be customized in the learning process
    to better represent the data the model receives.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„requires_grad=Trueï¼Œè¿™æ˜¯Pytorchè¯­è¨€ï¼Œç”¨äºè¡¨ç¤ºè¿™4ä¸ªæ•°å­—æ˜¯å¯å­¦ä¹ çš„æƒé‡ã€‚å®ƒä»¬å¯ä»¥å¹¶ä¸”ä¼šåœ¨å­¦ä¹ è¿‡ç¨‹ä¸­è¢«å®šåˆ¶ï¼Œä»¥æ›´å¥½åœ°è¡¨ç¤ºæ¨¡å‹æ¥æ”¶åˆ°çš„æ•°æ®ã€‚
- en: In a more realistic scenario, we can expect something closer to a 10k by 512
    matrix which represents our entire dataset in numbers.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ›´ç°å®çš„åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥æœŸæœ›æ¥è¿‘äºä¸€ä¸ª10kä¹˜512çš„çŸ©é˜µï¼Œå®ƒä»¥æ•°å­—çš„å½¢å¼è¡¨ç¤ºæˆ‘ä»¬çš„æ•´ä¸ªæ•°æ®é›†ã€‚
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Fun fact (we can think of things that are more fun), you sometimes hear language
    models use billions of parameters. This initial, not too crazy layer, holds 10_000
    by 512 parameters which are 5 million parameters. This LLM (Large Language Model)
    is difficult stuff, it needs a lot of calculations.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*æœ‰è¶£çš„äº‹å®ï¼ˆæˆ‘ä»¬å¯ä»¥æƒ³ä¸€äº›æ›´æœ‰è¶£çš„äº‹æƒ…ï¼‰ï¼Œä½ æœ‰æ—¶ä¼šå¬åˆ°è¯­è¨€æ¨¡å‹ä½¿ç”¨æ•°åäº¿çš„å‚æ•°ã€‚è¿™ä¸ªåˆæ­¥çš„ã€ä¸è¿‡äºç–¯ç‹‚çš„å±‚ï¼ŒåŒ…å«10_000ä¹˜512çš„å‚æ•°ï¼Œå³500ä¸‡å‚æ•°ã€‚è¿™ä¸ªLLMï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰æ˜¯å¤æ‚çš„ï¼Œéœ€è¦å¤§é‡çš„è®¡ç®—ã€‚*'
- en: Parameters here is a fancy word for those numbers (-1.525 etc.) just that they
    are subject to change and will change during training.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„å‚æ•°æ˜¯è¿™äº›æ•°å­—ï¼ˆ-1.525ç­‰ï¼‰çš„èŠ±å“¨è¯æ±‡ï¼Œåªä¸è¿‡å®ƒä»¬ä¼šå‘ç”Ÿå˜åŒ–ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šå˜åŒ–ã€‚
- en: These numbers are the learning of the machine, this is what the machine is learning.
    Later when we give it input, we multiply the input with those numbers, and we
    hopefully get a good result. What do you know, numbers matter. When youâ€™re important,
    you get your own name, so those arenâ€™t just numbers, those are parameters.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ•°å­—æ˜¯æœºå™¨å­¦ä¹ çš„å†…å®¹ï¼Œè¿™å°±æ˜¯æœºå™¨æ­£åœ¨å­¦ä¹ çš„ä¸œè¥¿ã€‚ç¨åï¼Œå½“æˆ‘ä»¬ç»™å®ƒè¾“å…¥æ—¶ï¼Œæˆ‘ä»¬å°†è¾“å…¥ä¸è¿™äº›æ•°å­—ç›¸ä¹˜ï¼Œå¹¶å¸Œæœ›å¾—åˆ°ä¸€ä¸ªå¥½çš„ç»“æœã€‚ä½ çŸ¥é“å—ï¼Œæ•°å­—å¾ˆé‡è¦ã€‚å½“ä½ é‡è¦æ—¶ï¼Œä½ ä¼šå¾—åˆ°è‡ªå·±çš„åå­—ï¼Œæ‰€ä»¥è¿™äº›ä¸ä»…ä»…æ˜¯æ•°å­—ï¼Œå®ƒä»¬æ˜¯å‚æ•°ã€‚
- en: Why use as many as 512 and not 5? because more numbers mean we can probably
    generate more accurate meaning. Great, stop thinking small, let's use a million
    then! why not? because more numbers mean more calculations, more computing power,
    more expensive to train, etc. 512 has been found to be a good place in the middle.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆè¦ç”¨å¤šè¾¾512ä¸ªå‚æ•°è€Œä¸æ˜¯5ä¸ªï¼Ÿå› ä¸ºæ›´å¤šçš„æ•°å­—æ„å‘³ç€æˆ‘ä»¬å¯ä»¥ç”Ÿæˆæ›´å‡†ç¡®çš„æ„ä¹‰ã€‚å¾ˆå¥½ï¼Œåˆ«å†å°çœ‹äº†ï¼Œè®©æˆ‘ä»¬ç”¨ä¸€ç™¾ä¸‡ä¸ªå‚æ•°å§ï¼ä¸ºä»€ä¹ˆä¸å‘¢ï¼Ÿå› ä¸ºæ›´å¤šçš„æ•°å­—æ„å‘³ç€æ›´å¤šçš„è®¡ç®—ï¼Œæ›´å¤šçš„è®¡ç®—èƒ½åŠ›ï¼Œæ›´æ˜‚è´µçš„è®­ç»ƒè´¹ç”¨ç­‰ç­‰ã€‚512è¢«å‘ç°æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æŠ˜ä¸­ç‚¹ã€‚
- en: Sequence Length
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åºåˆ—é•¿åº¦
- en: When training the model we are going to put a whole bunch of words together.
    It's more computationally efficient and it helps the model learn as it gets more
    context together. As mentioned every word will be represented by a 512-dimensional
    vector (list with 512 numbers) and each time we pass inputs to the model (a.k.a
    forward pass), we will send a bunch of sentences, not only one. For example, we
    decided to support a 50-word sequence. This means we are going to take the x number
    of words in a sentence, if x > 50 we split it and only take the first 50, if x
    < 50, we still need the size to be the exact same (Iâ€™ll soon explain why). To
    solve this we add padding which are special dummy strings, to the rest of the
    sentence. For example, if we support a 7-word sentence, and we have the sentence
    â€œWhere is godâ€. We add 4 paddings, so the input to the model will be â€œWhere is
    god <PAD> <PAD> <PAD> <PAD>â€. Actually, we usually add at least 2 more special
    paddings so the model knows where the sentence starts and where it ends, so it
    will actually be something like â€œ<StartOfSentence> Where is god <PAD> <PAD> <EndOfSentence>â€.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '* Why must all input vectors be of the same size? because software has â€œexpectationsâ€,
    and matrices have even stricter expectations. You canâ€™t do any â€œmathyâ€ calculation
    you want, it has to adhere to certain rules, and one of those rules is adequate
    vector sizes.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Positional encodings
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '***Intuition*** We now have a way to represent (and learn) words in our vocabulary.
    Letâ€™s make it even better by encoding the position of the words. Why is this important?
    because if we take these two sentences:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 1\. The man played with my cat
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 2\. The cat played with my man
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: We can represent the two sentences using the exact same embeddings, but the
    sentences have different meanings. We can think of such data in which order does
    not matter. If Iâ€™m calculating a sum of something, it doesnâ€™t matter where we
    start. In Language â€” order usually matters. The embeddings contain semantic meanings,
    but no exact order meaning. They do hold order in a way because these embeddings
    were originally created according to some linguistic logic (baby appears closer
    to sleep, not to state), but the same word can have more than one meaning in itself,
    and more importantly, different meaning when it's in a different context.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'Representing words as text without order is not good enough, we can improve
    this. The authors suggest we add positional encoding to the embeddings. We do
    this by calculating a position vector for every word and adding it (summing) the
    two vectors. The positional encoding vectors must be of the same size so they
    can be added. The formula for positional encoding uses two functions: sine for
    even positions (e.g. 0th word, 2d word, 4th, 6th, etc.) and cosine for odd positions
    (e.g. 1st, 3rd, 5th, etc.).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '***Visualization*** By looking at these functions (sin in red, cosine in blue)
    you can perhaps imagine why these two functions specifically were chosen. There
    is some symmetry between the functions, as there is between a word and the word
    that came before it, which helps model (represent) these related positions. Also,
    they output values from -1 to 1, which are very stable numbers to work with (they
    donâ€™t get super big or super small).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '***å¯è§†åŒ–*** é€šè¿‡è§‚å¯Ÿè¿™äº›å‡½æ•°ï¼ˆçº¢è‰²çš„æ­£å¼¦å‡½æ•°ï¼Œè“è‰²çš„ä½™å¼¦å‡½æ•°ï¼‰ï¼Œä½ ä¹Ÿè®¸å¯ä»¥æƒ³è±¡ä¸ºä»€ä¹ˆç‰¹åˆ«é€‰æ‹©äº†è¿™ä¸¤ä¸ªå‡½æ•°ã€‚å‡½æ•°ä¹‹é—´å­˜åœ¨ä¸€äº›å¯¹ç§°æ€§ï¼Œå°±åƒå•è¯ä¸å®ƒä¹‹å‰çš„å•è¯ä¹‹é—´çš„å¯¹ç§°æ€§ï¼Œè¿™æœ‰åŠ©äºæ¨¡å‹ï¼ˆè¡¨ç¤ºï¼‰è¿™äº›ç›¸å…³çš„ä½ç½®ã€‚æ­¤å¤–ï¼Œå®ƒä»¬è¾“å‡ºçš„å€¼èŒƒå›´ä»
    -1 åˆ° 1ï¼Œè¿™äº›æ•°å­—éå¸¸ç¨³å®šï¼ˆä¸ä¼šå˜å¾—è¿‡å¤§æˆ–è¿‡å°ï¼‰ã€‚'
- en: '![](../Images/8b18820bfa435ba932b01a95329315dc.png)![](../Images/fe240d159361255b14a25d37e3634d24.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b18820bfa435ba932b01a95329315dc.png)![](../Images/fe240d159361255b14a25d37e3634d24.png)'
- en: Formula image from the [original paper](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
    by Vaswani, A. et al.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å…¬å¼å›¾åƒæ¥è‡ªVaswani, A.ç­‰äººçš„[åŸå§‹è®ºæ–‡](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)ã€‚
- en: In the formula above, the upper row represents even numbers starting from 0
    (i = 0) and continues to be even-numbered (2*1, 2*2, 2*3). The second row represents
    odd numbers in the same way.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šè¿°å…¬å¼ä¸­ï¼Œç¬¬ä¸€è¡Œè¡¨ç¤ºä»0å¼€å§‹çš„å¶æ•°ï¼ˆi = 0ï¼‰ï¼Œå¹¶ç»§ç»­ä¸ºå¶æ•°ï¼ˆ2*1ï¼Œ2*2ï¼Œ2*3ï¼‰ã€‚ç¬¬äºŒè¡Œä»¥ç›¸åŒçš„æ–¹å¼è¡¨ç¤ºå¥‡æ•°ã€‚
- en: Every positional vector is a number_of_dimensions (512 in our case) vector with
    numbers from 0 to 1.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªä½ç½®å‘é‡æ˜¯ä¸€ä¸ªå…·æœ‰ç»´åº¦æ•°ï¼ˆåœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ä¸º512ï¼‰çš„å‘é‡ï¼Œæ•°å­—èŒƒå›´ä»0åˆ°1ã€‚
- en: '***Code***'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '***ä»£ç ***'
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If we print the first word, we see we only get 0 and 1 interchangeably.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æ‰“å°ç¬¬ä¸€ä¸ªå•è¯ï¼Œæˆ‘ä»¬ä¼šå‘ç°åªå¾—åˆ°0å’Œ1äº¤æ›¿å‡ºç°ã€‚
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The second number is already much more diverse.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªæ•°å­—å·²ç»å¤šæ ·åŒ–å¾—å¤šã€‚
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*Code inspiration is from [here](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*ä»£ç çµæ„Ÿæ¥è‡ª[è¿™é‡Œ](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/)*ã€‚'
- en: We have seen that different positions result in different representations. In
    order to finalize the section input as a whole (squared in red in the picture
    below) we add the numbers in the position matrix to our input embeddings matrix.
    We end up getting a matrix of the same size as the embedding, only this time the
    numbers contain semantic meaning + order.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»çœ‹åˆ°ä¸åŒçš„ä½ç½®å¯¼è‡´ä¸åŒçš„è¡¨ç¤ºã€‚ä¸ºäº†æœ€ç»ˆç¡®å®šä½œä¸ºæ•´ä½“çš„éƒ¨åˆ†è¾“å…¥ï¼ˆåœ¨ä¸‹å›¾ä¸­ç”¨çº¢è‰²æ ‡å‡ºï¼‰ï¼Œæˆ‘ä»¬å°†ä½ç½®çŸ©é˜µä¸­çš„æ•°å­—æ·»åŠ åˆ°æˆ‘ä»¬çš„è¾“å…¥åµŒå…¥çŸ©é˜µä¸­ã€‚æœ€ç»ˆå¾—åˆ°çš„çŸ©é˜µä¸åµŒå…¥çš„çŸ©é˜µå¤§å°ç›¸åŒï¼Œåªä¸è¿‡è¿™æ¬¡æ•°å­—åŒ…å«äº†è¯­ä¹‰æ„ä¹‰+é¡ºåºã€‚
- en: '![](../Images/c538dd4cc49a64d18d3ff1415f645712.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c538dd4cc49a64d18d3ff1415f645712.png)'
- en: Image from the [original paper](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
    by Vaswani, A. et al.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒæ¥è‡ªVaswani, A.ç­‰äººçš„[åŸå§‹è®ºæ–‡](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)ã€‚
- en: '***Summary*** This concludes our first part of the series (Rectangled in red).
    We talked about the model gets its inputs. We saw how to break down text to its
    features (tokens), represent them as numbers (embeddings) and a smart way to add
    positional encoding to these numbers.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '***æ€»ç»“*** è¿™æ€»ç»“äº†æˆ‘ä»¬ç³»åˆ—çš„ç¬¬ä¸€éƒ¨åˆ†ï¼ˆç”¨çº¢è‰²æ ‡å‡ºï¼‰ã€‚æˆ‘ä»¬è®¨è®ºäº†æ¨¡å‹å¦‚ä½•è·å–è¾“å…¥ã€‚æˆ‘ä»¬æŸ¥çœ‹äº†å¦‚ä½•å°†æ–‡æœ¬æ‹†åˆ†ä¸ºç‰¹å¾ï¼ˆæ ‡è®°ï¼‰ï¼Œå°†å®ƒä»¬è¡¨ç¤ºä¸ºæ•°å­—ï¼ˆåµŒå…¥ï¼‰ï¼Œä»¥åŠå°†ä½ç½®ç¼–ç æ·»åŠ åˆ°è¿™äº›æ•°å­—ä¸­çš„æ™ºèƒ½æ–¹æ³•ã€‚'
- en: '[The next part](https://medium.com/@chenmargalit/transformers-part-3-attention-7b95881714df)
    will focus on the different mechanics of the Encoder block (the first gray rectangle),
    with each section describing a different coloured rectangle (e.g. Multi head attention,
    Add & Norm, etc.)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[ä¸‹ä¸€éƒ¨åˆ†](https://medium.com/@chenmargalit/transformers-part-3-attention-7b95881714df)å°†é‡ç‚¹è®¨è®ºç¼–ç å™¨å—ï¼ˆç¬¬ä¸€ä¸ªç°è‰²çŸ©å½¢ï¼‰çš„ä¸åŒæœºåˆ¶ï¼Œæ¯ä¸ªéƒ¨åˆ†æè¿°ä¸åŒé¢œè‰²çš„çŸ©å½¢ï¼ˆä¾‹å¦‚ï¼Œå¤šå¤´æ³¨æ„åŠ›ã€æ·»åŠ å’Œå½’ä¸€åŒ–ç­‰ï¼‰ã€‚'
