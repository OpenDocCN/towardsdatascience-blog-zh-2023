- en: 'Understanding Deep Learning Optimizers: Momentum, AdaGrad, RMSProp & Adam'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/understanding-deep-learning-optimizers-momentum-adagrad-rmsprop-adam-e311e377e9c2?source=collection_archive---------0-----------------------#2023-12-30](https://towardsdatascience.com/understanding-deep-learning-optimizers-momentum-adagrad-rmsprop-adam-e311e377e9c2?source=collection_archive---------0-----------------------#2023-12-30)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Gain intuition behind acceleration training techniques in neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@slavahead?source=post_page-----e311e377e9c2--------------------------------)[![Vyacheslav
    Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----e311e377e9c2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e311e377e9c2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e311e377e9c2--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----e311e377e9c2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-deep-learning-optimizers-momentum-adagrad-rmsprop-adam-e311e377e9c2&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----e311e377e9c2---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e311e377e9c2--------------------------------)
    ·8 min read·Dec 30, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe311e377e9c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-deep-learning-optimizers-momentum-adagrad-rmsprop-adam-e311e377e9c2&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----e311e377e9c2---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe311e377e9c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-deep-learning-optimizers-momentum-adagrad-rmsprop-adam-e311e377e9c2&source=-----e311e377e9c2---------------------bookmark_footer-----------)![](../Images/665f9f91827ad58ea605c3ef0704e2e8.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'D**eep learning** made a gigantic step in the world of artificial intelligence.
    At the current moment, neural networks outperform other types of algorithms on
    non-tabular data: images, videos, audio, etc. Deep learning models usually have
    a strong complexity and come up with millions or even billions of trainable parameters.
    That is why it is essential in the modern era to use acceleration techniques to
    reduce training time.'
  prefs: []
  type: TYPE_NORMAL
- en: One of the most common algorithms performed during training is **backpropagation**
    consisting of changing weights of a neural network in respect to a given loss
    function. Backpropagation is usually performed via **gradient descent** which
    tries to converge loss function to a local minimum step by step.
  prefs: []
  type: TYPE_NORMAL
- en: As it turns out, naive gradient descent is not usually a preferable choice for
    training a deep network because of its slow convergence rate. This became a motivation
    for researchers to develop optimization algorithms which accelerate gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: Before reading this article, it is highly recommended that you are familiar
    with the **exponentially moving average** concept which is used in optimization
    algorithms. If not, you can refer to the article below.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](/intuitive-explanation-of-exponential-moving-average-2eb9693ea4dc?source=post_page-----e311e377e9c2--------------------------------)
    [## Intuitive Explanation of Exponential Moving Average'
  prefs: []
  type: TYPE_NORMAL
- en: Understand the logic behind the fundamental algorithm used inside the gradient
    descent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/intuitive-explanation-of-exponential-moving-average-2eb9693ea4dc?source=post_page-----e311e377e9c2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Gradient descent is the simplest optimization algorithm which computes gradients
    of loss function with respect to model weights and updates them by using the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d32e845ebbb9169b66dfac2195651c04.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradient descent equation. w is the weight vector, dw is the gradient of w,
    α is the learning rate, t is the iteration number
  prefs: []
  type: TYPE_NORMAL
- en: To understand why gradient descent converges slowly, let us look at the example
    below of a **ravine** where a given function of two variables should be minimised.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/45fe2806f8f0dbaf9ec84762d1c2f2cf.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of an optimization problem with gradient descent in a ravine area. The
    starting point is depicted in blue and the local minimum is shown in black.
  prefs: []
  type: TYPE_NORMAL
- en: '**A ravine** is an area where the surface is much more steep in one dimension
    than in another'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: From the image, we can see that the starting point and the local minima have
    different horizontal coordinates and are almost equal vertical coordinates. Using
    gradient descent to find the local minima will likely make the loss function slowly
    oscillate towards vertical axes. These bounces occur because gradient descent
    does not store any history about its previous gradients making gradient steps
    more undeterministic on each iteration. This example can be generalized to a higher
    number of dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: As a consequence, it would be risky to use a large learning rate as it could
    lead to disconvergence.
  prefs: []
  type: TYPE_NORMAL
- en: Momentum
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Based on the example above, it would be desirable to make a loss function performing
    larger steps in the horizontal direction and smaller steps in the vertical. This
    way, the convergence would be much faster. This effect is exactly achieved by
    Momentum.
  prefs: []
  type: TYPE_NORMAL
- en: 'Momentum uses a pair of equations at each iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1deb4e4c81fb9ccb3870e1987c5e3597.png)'
  prefs: []
  type: TYPE_IMG
- en: Momentum equations
  prefs: []
  type: TYPE_NORMAL
- en: The first formula uses an exponentially moving average for gradient values *dw*.
    Basically, it is done to store trend information about a set of previous gradient
    values. The second equation performs the normal gradient descent update using
    the computed moving average value on the current iteration. α is the learning
    rate of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Momentum can be particularly useful for cases like the above. Imagine we have
    computed gradients on every iteration like in the picture above. Instead of simply
    using them for updating weights, we take several past values and literaturally
    perform update in the averaged direction.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/027d92c3a7351463a50ea5572741f48d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Sebastian Ruder concisely describes the effect of Momentum in his [paper](https://arxiv.org/pdf/1609.04747.pdf):
    “The momentum term increases for dimensions whose gradients point in the same
    directions and reduces updates for dimensions whose gradients change directions.
    As a result, we gain faster convergence and reduced oscillation”.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As a result, updates performed by Momentum might look like in the figure below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fa9de8bee2c37be9dba9294007916497.png)'
  prefs: []
  type: TYPE_IMG
- en: Optimization with Momentum
  prefs: []
  type: TYPE_NORMAL
- en: In practice, Momentum usually converges much faster than gradient descent. With
    Momentum, there are also fewer risks in using larger learning rates, thus accelerating
    the training process.
  prefs: []
  type: TYPE_NORMAL
- en: In Momentum, it is recommended to choose β close to 0.9.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: AdaGrad (Adaptive Gradient Algorithm)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AdaGrad is another optimizer with the motivation to adapt the learning rate
    to computed gradient values. There might occur situations when during training,
    one component of the weight vector has very large gradient values while another
    one has extremely small. **This happens especially in cases when an infrequent
    model parameter appears to have a low influence on predictions**. It is worth
    noting that with frequent parameters such problems do not usually occur as, for
    their update, the model uses a lot of prediction signals. Since lots of information
    from signals is taken into account for gradient computation, gradients are usually
    adequate and represent a correct direction towards the local minimum. However,
    this is not the case for rare parameters which can lead to extremely large and
    unstable gradients. The same problem can occur with sparse data where there is
    too little information about certain features.
  prefs: []
  type: TYPE_NORMAL
- en: AdaGrad deals with the aforementioned problem **by independently adapting the
    learning rate for each weight component**. If gradients corresponding to a certain
    weight vector component are large, then the respective learning rate will be small.
    Inversely, for smaller gradients, the learning rate will be bigger. This way,
    Adagrad deals with vanishing and exploding gradient problems.
  prefs: []
  type: TYPE_NORMAL
- en: Under the hood, Adagrad accumulates element-wise squares *dw²* of gradients
    from all previous iterations. During weight update, instead of using normal learning
    rate α, AdaGrad scales it by dividing α by the square root of the accumulated
    gradients *√vₜ*. Additionally, a small positive term ε is added to the denominator
    to prevent potential division by zero.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8e0f659992ad29fb50e113b0daf3571d.png)'
  prefs: []
  type: TYPE_IMG
- en: AdaGrad equations
  prefs: []
  type: TYPE_NORMAL
- en: 'The greatest advantage of AdaGrad is that there is no longer a need to manually
    adjust the learning rate as it adapts itself during training. Nevertheless, there
    is a negative side of AdaGrad: the learning rate constantly decays with the increase
    of iterations (the learning rate is always divided by a positive cumulative number).
    Therefore, the algorithm tends to converge slowly during the last iterations where
    it becomes very low.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b0fc1e109cdcd4288a12b6abe3d8f08e.png)'
  prefs: []
  type: TYPE_IMG
- en: Optimization with AdaGrad
  prefs: []
  type: TYPE_NORMAL
- en: RMSProp (Root Mean Square Propagation)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RMSProp was elaborated as an improvement over AdaGrad which tackles the issue
    of learning rate decay. Similarly to AdaGrad, RMSProp uses a pair of equations
    for which the weight update is absolutely the same.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bdd2b8950a31350fc6aeb1330e1aaf2d.png)'
  prefs: []
  type: TYPE_IMG
- en: RMSProp equations
  prefs: []
  type: TYPE_NORMAL
- en: However, instead of storing a cumulated sum of squared gradients *dw²* for vₜ,
    the exponentially moving average is calculated for squared gradients *dw²*. Experiments
    show that RMSProp generally converges faster than AdaGrad because, with the exponentially
    moving average, it puts more emphasis on recent gradient values rather than equally
    distributing importance between all gradients by simply accumulating them from
    the first iteration. Furthermore, compared to AdaGrad, the learning rate in RMSProp
    does not always decay with the increase of iterations making it possible to adapt
    better in particular situations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9d0dd019424624f624dd0940c1a21e0a.png)'
  prefs: []
  type: TYPE_IMG
- en: Optimization with RMSProp
  prefs: []
  type: TYPE_NORMAL
- en: In RMSProp, it is recommended to choose β close to 1.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Why not to simply use a squared gradient for v**ₜ **instead of the exponentially
    moving average?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is known that the exponentially moving average distributes higher weights
    to recent gradient values. This is one of the reasons why RMSProp adapts quickly.
    But would not it be better if instead of the moving average we only took into
    account the last square gradient at every iteration (vₜ *= dw²*)? As it turns
    out, the update equation would transform in the following manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/921431c4faa6b96309222b853c12f650.png)'
  prefs: []
  type: TYPE_IMG
- en: Transformation of RMSProp equations when using a squared gradient instead of
    the exponentially moving average
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, the resulting formula looks very similar to the one used in
    the gradient descent. However, instead of using a normal gradient value for the
    update, we are now using the sign of the gradient:'
  prefs: []
  type: TYPE_NORMAL
- en: If *dw > 0*, then a weight *w* is decreased by α.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If *dw < 0*, then a weight *w* is increased by α.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To sum it up, if vₜ *= dw²*, then model weights can only be changed by ±α. Though
    this approach works sometimes, it is still not flexible the algorithm becomes
    extremely sensitive to the choice of α and absolute magnitudes of gradient are
    ignored which can make the method tremendously slow to converge. A positive aspect
    about this algorithm is the fact only a single bit is required to store signs
    of gradietns which can be handy in distributed computations with strict memory
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Adam (Adaptive Moment Estimation)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the moment, Adam is the most famous optimization algorithm in deep learning.
    At a high level, Adam combines Momentum and RMSProp algorithms. To achieve it,
    it simply keeps track of the exponentially moving averages for computed gradients
    and squared gradients respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/36f3227cbddd28917346c2d951b40e27.png)'
  prefs: []
  type: TYPE_IMG
- en: Adam equations
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, it is possible to use bias correction for moving averages for a
    more precise approximation of gradient trend during the first several iterations.
    The experiments show that Adam adapts well to almost any type of neural network
    architecture taking the advantages of both Momentum and RMSProp.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/820a86fef66786198d018b7f3bd3b4de.png)'
  prefs: []
  type: TYPE_IMG
- en: Optimization with Adam
  prefs: []
  type: TYPE_NORMAL
- en: According to the [Adam paper](https://arxiv.org/pdf/1412.6980.pdf), good default
    values for hyperparameters are β₁ = 0.9, β₂ = 0.999, ε = 1e-8.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have looked at different optimization algorithms in neural networks. Considered
    as a combination of Momentum and RMSProp, Adam is the most superior of them which
    robustly adapts to large datasets and deep networks. Moreover, it has a straightforward
    implementation and little memory requirements making it a preferable choice in
    the majority of situations.
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[An overview of gradient descent optimization algorithms](https://arxiv.org/pdf/1609.04747.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Adam: A Method For Stochastic Optimization](https://arxiv.org/pdf/1412.6980.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*All images unless otherwise noted are by the author*'
  prefs: []
  type: TYPE_NORMAL
