# å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰

> åŸæ–‡ï¼š[`towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91`](https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91)

## ä¸€ä¸ªå¸¦æœ‰ç¤ºä¾‹ Python ä»£ç çš„æ¦‚å¿µæ¦‚è¿°

[](https://shawhin.medium.com/?source=post_page-----23473d763b91--------------------------------)![Shaw Talebi](https://shawhin.medium.com/?source=post_page-----23473d763b91--------------------------------)[](https://towardsdatascience.com/?source=post_page-----23473d763b91--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page-----23473d763b91--------------------------------) [Shaw Talebi](https://shawhin.medium.com/?source=post_page-----23473d763b91--------------------------------)

Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----23473d763b91--------------------------------) Â·14 åˆ†é’Ÿé˜…è¯»Â·2023 å¹´ 9 æœˆ 11 æ—¥

--

è¿™æ˜¯å…³äº[ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹](https://medium.com/towards-data-science/a-practical-introduction-to-llms-65194dda1148)ï¼ˆLLMsï¼‰å®è·µçš„ç¬¬ 5 ç¯‡æ–‡ç« ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºå¦‚ä½•å¯¹é¢„è®­ç»ƒçš„ LLM è¿›è¡Œå¾®è°ƒï¼ˆFTï¼‰ã€‚æˆ‘ä»¬å°†é¦–å…ˆä»‹ç»å…³é”®çš„ FT æ¦‚å¿µå’ŒæŠ€æœ¯ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªå…·ä½“çš„ç¤ºä¾‹ï¼Œæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ Python å’Œ Hugging Face çš„è½¯ä»¶ç”Ÿæ€ç³»ç»Ÿåœ¨æœ¬åœ°å¾®è°ƒæ¨¡å‹ã€‚

![](img/d74b7565b127d69e37ddf51e16125896.png)

è°ƒæ•´è¯­è¨€æ¨¡å‹ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚

åœ¨[æœ¬ç³»åˆ—çš„ä¸Šä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f)ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°å¦‚ä½•é€šè¿‡å°†æç¤ºå·¥ç¨‹æ•´åˆåˆ° Python ä»£ç ä¸­æ¥æ„å»ºå®ç”¨çš„ LLM é©±åŠ¨åº”ç”¨ç¨‹åºã€‚å¯¹äºç»å¤§å¤šæ•° LLM ä½¿ç”¨æ¡ˆä¾‹ï¼Œè¿™æ˜¯æˆ‘æ¨èçš„åˆæ­¥æ–¹æ³•ï¼Œå› ä¸ºå®ƒæ¯”å…¶ä»–æ–¹æ³•éœ€è¦çš„èµ„æºå’ŒæŠ€æœ¯ä¸“é•¿å°‘å¾—å¤šï¼ŒåŒæ—¶ä»ç„¶èƒ½æä¾›è®¸å¤šå¥½å¤„ã€‚

ç„¶è€Œï¼Œå­˜åœ¨ä¸€äº›æƒ…å†µï¼Œå…¶ä¸­ç›´æ¥æç¤ºç°æœ‰çš„ LLM å¹¶ä¸å¤Ÿæœ‰æ•ˆï¼Œéœ€è¦æ›´å¤æ‚çš„è§£å†³æ–¹æ¡ˆã€‚è¿™å°±æ˜¯æ¨¡å‹å¾®è°ƒå¯ä»¥å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚

é™„åŠ è§†é¢‘ã€‚

# **ä»€ä¹ˆæ˜¯å¾®è°ƒï¼Ÿ**

**å¾®è°ƒ**æ˜¯å¯¹ä¸€ä¸ªé¢„è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œ**è‡³å°‘ä¸€ä¸ªå†…éƒ¨æ¨¡å‹å‚æ•°çš„è®­ç»ƒ**ï¼ˆå³æƒé‡ï¼‰ã€‚åœ¨ LLMs çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œè¿™é€šå¸¸æ˜¯å°†ä¸€ä¸ªé€šç”¨çš„åŸºç¡€æ¨¡å‹ï¼ˆä¾‹å¦‚ GPT-3ï¼‰è½¬å˜ä¸ºä¸€ä¸ªé’ˆå¯¹ç‰¹å®šç”¨ä¾‹çš„ä¸“ä¸šæ¨¡å‹ï¼ˆä¾‹å¦‚ ChatGPTï¼‰[1]ã€‚

è¿™ç§æ–¹æ³•çš„**ä¸»è¦ä¼˜åŠ¿**åœ¨äºï¼Œä¸ä»…ä¾èµ–ç›‘ç£è®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”ï¼Œæ¨¡å‹å¯ä»¥åœ¨éœ€è¦ï¼ˆè¿œï¼‰å°‘å¾—å¤šçš„äººå·¥æ ‡è®°ç¤ºä¾‹çš„æƒ…å†µä¸‹å®ç°æ›´å¥½çš„æ€§èƒ½ã€‚

è™½ç„¶ä¸¥æ ¼çš„è‡ªç›‘ç£åŸºç¡€æ¨¡å‹åœ¨å€ŸåŠ©æç¤ºå·¥ç¨‹[2]çš„å¸®åŠ©ä¸‹å¯ä»¥åœ¨å„ç§ä»»åŠ¡ä¸Šå±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„è¡¨ç°ï¼Œä½†å®ƒä»¬ä»ç„¶æ˜¯è¯é¢„æµ‹å™¨ï¼Œå¯èƒ½ç”Ÿæˆçš„å®Œæˆç»“æœå¹¶ä¸å®Œå…¨æœ‰ç”¨æˆ–å‡†ç¡®ã€‚ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬æ¯”è¾ƒ davinciï¼ˆåŸºç¡€ GPT-3 æ¨¡å‹ï¼‰å’Œ text-davinci-003ï¼ˆä¸€ä¸ªç»è¿‡å¾®è°ƒçš„æ¨¡å‹ï¼‰çš„å®Œæˆç»“æœã€‚

![](img/b3618c95fe5ccf99162d28d4b1bb1344.png)

davinciï¼ˆåŸºç¡€ GPT-3 æ¨¡å‹ï¼‰å’Œ text-davinci-003ï¼ˆä¸€ä¸ªç»è¿‡å¾®è°ƒçš„æ¨¡å‹ï¼‰çš„å®Œæˆç»“æœæ¯”è¾ƒã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚

æ³¨æ„ï¼ŒåŸºç¡€æ¨¡å‹ä»…ä»…æ˜¯é€šè¿‡åˆ—å‡ºä¸€ç³»åˆ—é—®é¢˜æ¥å®Œæˆæ–‡æœ¬ï¼Œå°±åƒè°·æ­Œæœç´¢æˆ–ä½œä¸šä»»åŠ¡ä¸€æ ·ï¼Œè€Œ**å¾®è°ƒæ¨¡å‹æä¾›äº†æ›´æœ‰å¸®åŠ©çš„å“åº”**ã€‚text-davinci-003 æ‰€ä½¿ç”¨çš„å¾®è°ƒæ–¹æ³•æ˜¯**å¯¹é½å¾®è°ƒ**ï¼Œå…¶ç›®çš„æ˜¯ä½¿å¤§å‹è¯­è¨€æ¨¡å‹çš„å“åº”æ›´æœ‰å¸®åŠ©ã€æ›´è¯šå®å’Œæ— å®³ï¼Œä½†ç¨åä¼šè¯¦ç»†ä»‹ç»[3,4]ã€‚

# **ä¸ºä»€ä¹ˆè¦å¾®è°ƒ**

å¾®è°ƒä¸ä»…èƒ½æé«˜åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ï¼Œè€Œä¸”**è¾ƒå°çš„ï¼ˆå¾®è°ƒçš„ï¼‰æ¨¡å‹åœ¨è®­ç»ƒçš„ä»»åŠ¡é›†ä¸Šå¾€å¾€èƒ½è¶…è¶Šæ›´å¤§çš„ï¼ˆæ›´æ˜‚è´µçš„ï¼‰æ¨¡å‹**[4]ã€‚OpenAI é€šè¿‡å…¶ç¬¬ä¸€ä»£â€œInstructGPTâ€æ¨¡å‹è¯æ˜äº†è¿™ä¸€ç‚¹ï¼Œå…¶ä¸­ 1.3B å‚æ•°çš„ InstructGPT æ¨¡å‹åœ¨å®Œæˆç»“æœä¸Šä¼˜äº 175B å‚æ•°çš„ GPT-3 åŸºç¡€æ¨¡å‹ï¼Œå°½ç®¡å®ƒå°äº† 100 å€[4]ã€‚

è™½ç„¶æˆ‘ä»¬ä»Šå¤©å¯èƒ½ä¸ä¹‹äº’åŠ¨çš„å¤§å¤šæ•°å¤§å‹è¯­è¨€æ¨¡å‹å¹¶éåƒ GPT-3 é‚£æ ·ä¸¥æ ¼çš„è‡ªç›‘ç£æ¨¡å‹ï¼Œä½†å¯¹ç°æœ‰çš„å¾®è°ƒæ¨¡å‹è¿›è¡Œç‰¹å®šç”¨ä¾‹çš„æç¤ºä»ç„¶å­˜åœ¨ç¼ºé™·ã€‚

ä¸€ä¸ªé‡è¦çš„é—®é¢˜æ˜¯ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹æœ‰ä¸€ä¸ªæœ‰é™çš„ä¸Šä¸‹æ–‡çª—å£ã€‚å› æ­¤ï¼Œæ¨¡å‹å¯èƒ½åœ¨éœ€è¦å¤§é‡çŸ¥è¯†åŸºç¡€æˆ–ç‰¹å®šé¢†åŸŸä¿¡æ¯çš„ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³[1]ã€‚å¾®è°ƒæ¨¡å‹å¯ä»¥é€šè¿‡åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­â€œå­¦ä¹ â€è¿™äº›ä¿¡æ¯æ¥é¿å…è¿™ä¸ªé—®é¢˜ã€‚è¿™ä¹Ÿé¿å…äº†åœ¨æç¤ºä¸­å¡å…¥é¢å¤–çš„ä¸Šä¸‹æ–‡ï¼Œä»è€Œå¯ä»¥é™ä½æ¨ç†æˆæœ¬ã€‚

# **å¾®è°ƒçš„ä¸‰ç§æ–¹æ³•**

æœ‰**ä¸‰ç§é€šç”¨çš„æ¨¡å‹å¾®è°ƒæ–¹æ³•**ï¼šè‡ªç›‘ç£ã€ç›‘ç£å’Œå¼ºåŒ–å­¦ä¹ ã€‚è¿™äº›æ–¹æ³•å¹¶éç›¸äº’æ’æ–¥ï¼Œå¯ä»¥æŒ‰é¡ºåºç»„åˆä½¿ç”¨è¿™ä¸‰ç§æ–¹æ³•æ¥å¾®è°ƒä¸€ä¸ªæ¨¡å‹ã€‚

## **è‡ªç›‘ç£å­¦ä¹ **

**è‡ªç›‘ç£å­¦ä¹ **åŒ…æ‹¬**åŸºäºè®­ç»ƒæ•°æ®çš„å›ºæœ‰ç»“æ„æ¥è®­ç»ƒæ¨¡å‹**ã€‚åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒŒæ™¯ä¸‹ï¼Œè¿™é€šå¸¸è¡¨ç°ä¸ºç»™å®šä¸€ä¸ªè¯ï¼ˆæˆ–æ›´å‡†ç¡®åœ°è¯´æ˜¯ä»¤ç‰Œï¼‰çš„åºåˆ—ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼ˆä»¤ç‰Œï¼‰ã€‚

å°½ç®¡è¿™æ˜¯è®¸å¤šç°ä»Šé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„å¼€å‘æ–¹å¼ï¼Œä½†å®ƒä¹Ÿå¯ä»¥ç”¨äºæ¨¡å‹å¾®è°ƒã€‚ä¸€ä¸ªæ½œåœ¨çš„åº”ç”¨åœºæ™¯æ˜¯å¼€å‘ä¸€ä¸ªèƒ½å¤Ÿæ¨¡æ‹ŸæŸäººå†™ä½œé£æ ¼çš„æ¨¡å‹ï¼Œç»™å®šä¸€ç»„ç¤ºä¾‹æ–‡æœ¬ã€‚

## **ç›‘ç£å­¦ä¹ **

ä¸‹ä¸€ç§ï¼Œä¹Ÿè®¸æ˜¯æœ€æµè¡Œçš„ï¼Œå¾®è°ƒæ¨¡å‹çš„æ–¹æ³•æ˜¯é€šè¿‡**ç›‘ç£å­¦ä¹ **ã€‚è¿™æ¶‰åŠåˆ°**å¯¹ç‰¹å®šä»»åŠ¡çš„è¾“å…¥-è¾“å‡ºå¯¹è¿›è¡Œæ¨¡å‹è®­ç»ƒ**ã€‚ä¸€ä¸ªä¾‹å­æ˜¯**æŒ‡ä»¤è°ƒæ•´**ï¼Œå…¶ç›®çš„æ˜¯æå‡æ¨¡å‹åœ¨å›ç­”é—®é¢˜æˆ–å›åº”ç”¨æˆ·æç¤ºæ—¶çš„è¡¨ç°[1,3]ã€‚

ç›‘ç£å­¦ä¹ ä¸­çš„**å…³é”®æ­¥éª¤**æ˜¯**ç­–åˆ’è®­ç»ƒæ•°æ®é›†**ã€‚ä¸€ç§ç®€å•çš„æ–¹æ³•æ˜¯åˆ›å»ºé—®ç­”å¯¹å¹¶å°†å…¶æ•´åˆåˆ°æç¤ºæ¨¡æ¿ä¸­[1,3]ã€‚ä¾‹å¦‚ï¼Œé—®ç­”å¯¹ï¼š*è°æ˜¯ç¾å›½ç¬¬ 35 ä»»æ€»ç»Ÿï¼Ÿâ€”â€” çº¦ç¿°Â·FÂ·è‚¯å°¼è¿ª* å¯ä»¥ç²˜è´´åˆ°ä¸‹é¢çš„æç¤ºæ¨¡æ¿ä¸­ã€‚æ›´å¤šç¤ºä¾‹æç¤ºæ¨¡æ¿å¯ä»¥åœ¨å‚è€ƒæ–‡çŒ®[4]çš„ A.2.1 èŠ‚ä¸­æ‰¾åˆ°ã€‚

```py
"""Please answer the following question.

Q: {Question}

A: {Answer}"""
```

ä½¿ç”¨æç¤ºæ¨¡æ¿å¾ˆé‡è¦ï¼Œå› ä¸ºåƒ GPT-3 è¿™æ ·çš„åŸºç¡€æ¨¡å‹æœ¬è´¨ä¸Šæ˜¯â€œæ–‡æ¡£è¡¥å…¨å™¨â€ã€‚è¿™æ„å‘³ç€ï¼Œç»™å®šä¸€äº›æ–‡æœ¬ï¼Œæ¨¡å‹ä¼šç”Ÿæˆåœ¨è¯¥ä¸Šä¸‹æ–‡ä¸­ï¼ˆç»Ÿè®¡ä¸Šï¼‰æœ‰æ„ä¹‰çš„æ›´å¤šæ–‡æœ¬ã€‚è¿™å›åˆ°æœ¬ç³»åˆ—çš„[ä¸Šä¸€ç¯‡åšå®¢](https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f)å’Œé€šè¿‡æç¤ºå·¥ç¨‹â€œæ¬ºéª—â€è¯­è¨€æ¨¡å‹æ¥è§£å†³é—®é¢˜çš„æƒ³æ³•ã€‚

[](/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f?source=post_page-----23473d763b91--------------------------------) ## æç¤ºå·¥ç¨‹ â€” å¦‚ä½•â€œæ¬ºéª—â€AI æ¥è§£å†³ä½ çš„é—®é¢˜

### 7 ä¸ªæç¤ºæŠ€å·§ã€Langchain å’Œ Python ç¤ºä¾‹ä»£ç 

towardsdatascience.com

## **å¼ºåŒ–å­¦ä¹ **

æœ€åï¼Œå¯ä»¥ä½¿ç”¨**å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰**æ¥å¾®è°ƒæ¨¡å‹ã€‚RL**ä½¿ç”¨å¥–åŠ±æ¨¡å‹æ¥æŒ‡å¯¼åŸºç¡€æ¨¡å‹çš„è®­ç»ƒ**ã€‚è¿™å¯ä»¥é‡‡å–å¤šç§å½¢å¼ï¼Œä½†åŸºæœ¬æ€æƒ³æ˜¯è®­ç»ƒå¥–åŠ±æ¨¡å‹ä»¥å¯¹è¯­è¨€æ¨¡å‹çš„å®Œæˆæƒ…å†µè¿›è¡Œè¯„åˆ†ï¼Œä½¿å…¶åæ˜ äººç±»æ ‡æ³¨è€…çš„åå¥½[3,4]ã€‚ç„¶åï¼Œå¥–åŠ±æ¨¡å‹å¯ä»¥ä¸å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆä¾‹å¦‚ï¼Œè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ï¼‰ç»“åˆä½¿ç”¨ï¼Œä»¥å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚

OpenAI çš„ InstructGPT æ¨¡å‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œæ¨¡å‹å¾®è°ƒï¼Œè¿™äº›æ¨¡å‹æ˜¯é€šè¿‡**3 ä¸ªå…³é”®æ­¥éª¤**å¼€å‘çš„[4]ã€‚

1.  ç”Ÿæˆé«˜è´¨é‡çš„æç¤º-å“åº”å¯¹ï¼Œå¹¶ä½¿ç”¨ç›‘ç£å­¦ä¹ å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚ï¼ˆ~13k è®­ç»ƒæç¤ºï¼‰ *æ³¨æ„ï¼šå¯ä»¥ï¼ˆå¯é€‰åœ°ï¼‰è·³åˆ°æ­¥éª¤ 2ï¼Œä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹[3]ã€‚*

1.  ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹ç”Ÿæˆå®Œæˆï¼Œå¹¶è®©äººå·¥æ ‡æ³¨è€…æ ¹æ®å…¶åå¥½å¯¹å“åº”è¿›è¡Œæ’åã€‚ä½¿ç”¨è¿™äº›åå¥½æ¥è®­ç»ƒå¥–åŠ±æ¨¡å‹ã€‚ï¼ˆ~33k è®­ç»ƒæç¤ºï¼‰

1.  ä½¿ç”¨å¥–åŠ±æ¨¡å‹å’Œå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆä¾‹å¦‚ PPOï¼‰è¿›ä¸€æ­¥å¾®è°ƒæ¨¡å‹ã€‚ï¼ˆ~31k è®­ç»ƒæç¤ºï¼‰

å°½ç®¡ä¸Šè¿°ç­–ç•¥é€šå¸¸ä¼šå¯¼è‡´ LLM å®Œæˆåº¦æ˜¾è‘—ä¼˜äºåŸºç¡€æ¨¡å‹ï¼Œä½†è¿™ä¹Ÿå¯èƒ½å¯¼è‡´åœ¨æŸäº›ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸‹é™ã€‚è¿™ç§æ€§èƒ½ä¸‹é™ä¹Ÿè¢«ç§°ä¸º**å¯¹é½æˆæœ¬**[3,4]ã€‚

# **ç›‘ç£å¾®è°ƒæ­¥éª¤ï¼ˆé«˜çº§ï¼‰**

å¦‚ä¸Šæ‰€è¿°ï¼Œå¯ä»¥é€šè¿‡è®¸å¤šæ–¹æ³•å¾®è°ƒç°æœ‰è¯­è¨€æ¨¡å‹ã€‚ç„¶è€Œï¼Œæœ¬æ–‡å‰©ä½™éƒ¨åˆ†å°†ä¸“æ³¨äºé€šè¿‡ç›‘ç£å­¦ä¹ è¿›è¡Œå¾®è°ƒã€‚ä»¥ä¸‹æ˜¯ç›‘ç£æ¨¡å‹å¾®è°ƒçš„é«˜çº§ç¨‹åº[1]ã€‚

1.  **é€‰æ‹©å¾®è°ƒä»»åŠ¡**ï¼ˆä¾‹å¦‚æ€»ç»“ã€é—®ç­”ã€æ–‡æœ¬åˆ†ç±»ï¼‰

1.  **å‡†å¤‡è®­ç»ƒæ•°æ®é›†**å³åˆ›å»ºï¼ˆ100â€“10kï¼‰è¾“å…¥è¾“å‡ºå¯¹å¹¶é¢„å¤„ç†æ•°æ®ï¼ˆå³æ ‡è®°åŒ–ã€æˆªæ–­å’Œå¡«å……æ–‡æœ¬ï¼‰ã€‚

1.  **é€‰æ‹©åŸºç¡€æ¨¡å‹**ï¼ˆå°è¯•ä¸åŒæ¨¡å‹å¹¶é€‰æ‹©åœ¨æ‰€éœ€ä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³çš„ä¸€ä¸ªï¼‰ã€‚

1.  **é€šè¿‡ç›‘ç£å­¦ä¹ å¾®è°ƒæ¨¡å‹**

1.  **è¯„ä¼°æ¨¡å‹æ€§èƒ½**

è™½ç„¶è¿™äº›æ­¥éª¤ä¸­çš„æ¯ä¸€ä¸ªéƒ½å¯ä»¥æˆä¸ºä¸€ç¯‡æ–‡ç« ï¼Œä½†æˆ‘æƒ³ä¸“æ³¨äº**ç¬¬ 4 æ­¥**å¹¶è®¨è®ºå¦‚ä½•è®­ç»ƒå¾®è°ƒåçš„æ¨¡å‹ã€‚

# **å‚æ•°è®­ç»ƒçš„ 3 ç§é€‰é¡¹**

åœ¨å¾®è°ƒæ‹¥æœ‰~100M-100B å‚æ•°çš„æ¨¡å‹æ—¶ï¼Œéœ€è¦è€ƒè™‘è®¡ç®—æˆæœ¬ã€‚åœ¨è¿™æ–¹é¢ï¼Œä¸€ä¸ªé‡è¦çš„é—®é¢˜æ˜¯â€”â€”*æˆ‘ä»¬ï¼ˆé‡æ–°ï¼‰è®­ç»ƒå“ªäº›å‚æ•°ï¼Ÿ*

åœ¨å¤„ç†å¤§é‡å‚æ•°æ—¶ï¼Œæˆ‘ä»¬æœ‰æ— æ•°çš„é€‰æ‹©æ¥å†³å®šè®­ç»ƒå“ªäº›å‚æ•°ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘å°†ä¸“æ³¨äº**ä¸‰ç§é€šç”¨é€‰é¡¹**ã€‚

## **é€‰é¡¹ 1ï¼šé‡æ–°è®­ç»ƒæ‰€æœ‰å‚æ•°**

ç¬¬ä¸€ä¸ªé€‰é¡¹æ˜¯**è®­ç»ƒæ‰€æœ‰å†…éƒ¨æ¨¡å‹å‚æ•°**ï¼ˆç§°ä¸º**å®Œå…¨å‚æ•°è°ƒä¼˜**ï¼‰[3]ã€‚è™½ç„¶è¿™ä¸ªé€‰é¡¹åœ¨æ¦‚å¿µä¸Šå¾ˆç®€å•ï¼Œä½†å®ƒæ˜¯æœ€è€—è´¹è®¡ç®—èµ„æºçš„ã€‚æ­¤å¤–ï¼Œå®Œå…¨å‚æ•°è°ƒä¼˜çš„ä¸€ä¸ªå·²çŸ¥é—®é¢˜æ˜¯ç¾éš¾æ€§é—å¿˜ç°è±¡ã€‚è¿™æ˜¯æ¨¡å‹â€œé—å¿˜â€äº†åœ¨åˆå§‹è®­ç»ƒä¸­â€œå­¦åˆ°â€çš„æœ‰ç”¨ä¿¡æ¯[3]ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡å†»ç»“æ¨¡å‹å‚æ•°çš„å¤§éƒ¨åˆ†æ¥ç¼“è§£é€‰é¡¹ 1 çš„ç¼ºç‚¹ï¼Œè¿™å°±å¼•å‡ºäº†é€‰é¡¹ 2ã€‚

## **é€‰é¡¹ 2ï¼šè¿ç§»å­¦ä¹ **

**è¿ç§»å­¦ä¹ ï¼ˆTLï¼‰**çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä¿ç•™æ¨¡å‹ä»è¿‡å»è®­ç»ƒä¸­å­¦åˆ°çš„æœ‰ç”¨è¡¨ç¤º/ç‰¹å¾ï¼Œå¹¶å°†æ¨¡å‹åº”ç”¨äºæ–°ä»»åŠ¡æ—¶ä½¿ç”¨ã€‚è¿™é€šå¸¸åŒ…æ‹¬**â€œå»æ‰ç¥ç»ç½‘ç»œï¼ˆNNï¼‰çš„å¤´éƒ¨å¹¶ç”¨æ–°çš„å¤´éƒ¨æ›¿æ¢å®ƒâ€**ï¼ˆä¾‹å¦‚ï¼Œæ·»åŠ å…·æœ‰éšæœºæƒé‡çš„æ–°å±‚ï¼‰ã€‚*æ³¨æ„ï¼šç¥ç»ç½‘ç»œçš„å¤´éƒ¨åŒ…æ‹¬å…¶æœ€ç»ˆå±‚ï¼Œè¿™äº›å±‚å°†æ¨¡å‹çš„å†…éƒ¨è¡¨ç¤ºè½¬æ¢ä¸ºè¾“å‡ºå€¼ã€‚*

è™½ç„¶ä¿æŒå¤§éƒ¨åˆ†å‚æ•°ä¸å˜å¯ä»¥å‡è½»è®­ç»ƒ LLM çš„å·¨å¤§è®¡ç®—æˆæœ¬ï¼Œä½† TL å¯èƒ½æ— æ³•è§£å†³ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚ä¸ºäº†æ›´å¥½åœ°å¤„ç†è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥è½¬å‘ä¸€ç»„ä¸åŒçš„æ–¹æ³•ã€‚

## **é€‰é¡¹ 3ï¼šå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰**

**PEFT** æ¶‰åŠ **ç”¨ç›¸å¯¹å°‘é‡çš„å¯è®­ç»ƒå‚æ•°å¢å¼ºåŸºç¡€æ¨¡å‹**ã€‚å…¶å…³é”®ç»“æœæ˜¯ä¸€ç§å¾®è°ƒæ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æä½çš„è®¡ç®—å’Œå­˜å‚¨æˆæœ¬ä¸‹å±•ç¤ºå‡ºä¸å®Œå…¨å‚æ•°è°ƒæ•´ç›¸å½“çš„æ€§èƒ½ [5]ã€‚

PEFT å°è£…äº†ä¸€ç³»åˆ—æŠ€æœ¯ï¼Œå…¶ä¸­ä¹‹ä¸€æ˜¯æµè¡Œçš„ **LoRA (ä½ç§©é€‚é…)** æ–¹æ³• [6]ã€‚LoRA çš„åŸºæœ¬æ€æƒ³æ˜¯é€‰æ‹©ç°æœ‰æ¨¡å‹ä¸­çš„ä¸€éƒ¨åˆ†å±‚ï¼Œå¹¶æ ¹æ®ä»¥ä¸‹æ–¹ç¨‹å¼ä¿®æ”¹å®ƒä»¬çš„æƒé‡ã€‚

![](img/446dc4369edc5bc4655f4986a426a513.png)

æ–¹ç¨‹å¼æ˜¾ç¤ºäº†å¦‚ä½•ä½¿ç”¨ LoRA ä¿®æ”¹æƒé‡çŸ©é˜µä»¥è¿›è¡Œå¾®è°ƒ [6]ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚

å…¶ä¸­ *h()* = ä¸€ä¸ªå°†è¢«è°ƒæ•´çš„éšè—å±‚ï¼Œ*x* = è¾“å…¥åˆ° *h()* çš„æ•°æ®ï¼Œ*Wâ‚€* = *h* çš„åŸå§‹æƒé‡çŸ©é˜µï¼Œè€Œ *Î”W* = æ³¨å…¥åˆ° *h* çš„å¯è®­ç»ƒå‚æ•°çŸ©é˜µã€‚*Î”W* æ ¹æ® *Î”W*=*BA* è¢«åˆ†è§£ï¼Œå…¶ä¸­ *Î”W* æ˜¯ä¸€ä¸ª d ä¹˜ k çš„çŸ©é˜µï¼Œ*B* æ˜¯ d ä¹˜ r çš„çŸ©é˜µï¼Œè€Œ *A* æ˜¯ r ä¹˜ k çš„çŸ©é˜µã€‚r æ˜¯ *Î”W* çš„å‡å®šâ€œå†…åœ¨ç§©â€ï¼ˆå¯ä»¥å°åˆ° 1 æˆ– 2ï¼‰[6]ã€‚

å¯¹ä¸èµ·æœ‰è¿™ä¹ˆå¤šæ•°å­¦å…¬å¼ï¼Œä½†**å…³é”®ç‚¹æ˜¯ *Wâ‚€* ä¸­çš„ (d * k) æƒé‡æ˜¯å†»ç»“çš„ï¼Œå› æ­¤ä¸åŒ…æ‹¬åœ¨ä¼˜åŒ–ä¸­**ã€‚ç›¸åï¼Œæ„æˆçŸ©é˜µ *B* å’Œ *A* çš„ ((d * r) + (r * k)) æƒé‡æ˜¯å”¯ä¸€ç»è¿‡è®­ç»ƒçš„ã€‚

é€šè¿‡å°†ä¸€äº›è™šæ„çš„æ•°å­—ä»£å…¥ d=100, k=100 å’Œ r=2 æ¥æ„Ÿå—æ•ˆç‡æå‡ï¼Œ**å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ä» 10,000 é™åˆ° 400**ã€‚å®é™…ä¸Šï¼ŒLoRA è®ºæ–‡çš„ä½œè€…æåˆ°ï¼Œä½¿ç”¨ LoRA å¾®è°ƒ GPT-3 ç›¸æ¯”å®Œå…¨å‚æ•°è°ƒæ•´ï¼Œ**å‚æ•°æ£€æŸ¥ç‚¹å¤§å°å‡å°‘äº† 10,000 å€** [6]ã€‚

ä¸ºäº†ä½¿è¿™æ›´åŠ å…·ä½“ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä½¿ç”¨ LoRA é«˜æ•ˆåœ°å¾®è°ƒè¯­è¨€æ¨¡å‹ï¼Œä»¥ä¾¿åœ¨ä¸ªäººè®¡ç®—æœºä¸Šè¿è¡Œã€‚

# **ç¤ºä¾‹ä»£ç ï¼šä½¿ç”¨ LoRA å¾®è°ƒ LLM**

åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Hugging Face ç”Ÿæ€ç³»ç»Ÿæ¥å¾®è°ƒè¯­è¨€æ¨¡å‹ï¼Œä»¥å°†æ–‡æœ¬åˆ†ç±»ä¸ºâ€œæ­£é¢â€æˆ–â€œè´Ÿé¢â€ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¾®è°ƒ [*distilbert-base-uncased*](https://huggingface.co/distilbert-base-uncased)ï¼Œä¸€ä¸ªåŸºäº [BERT](https://arxiv.org/pdf/1810.04805.pdf) çš„çº¦ 70M å‚æ•°æ¨¡å‹ã€‚ç”±äºè¿™ä¸ªåŸºç¡€æ¨¡å‹æ˜¯ä¸ºäº†è¯­è¨€å»ºæ¨¡è€Œä¸æ˜¯åˆ†ç±»è®­ç»ƒçš„ï¼Œæˆ‘ä»¬é‡‡ç”¨ **è¿ç§»å­¦ä¹ ** å°†åŸºç¡€æ¨¡å‹å¤´éƒ¨æ›¿æ¢ä¸ºåˆ†ç±»å¤´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨ **LoRA** é«˜æ•ˆåœ°å¾®è°ƒæ¨¡å‹ï¼Œä½¿å…¶å¯ä»¥åœ¨æˆ‘çš„ Mac Miniï¼ˆM1 èŠ¯ç‰‡ï¼Œ16GB å†…å­˜ï¼‰ä¸Šåœ¨åˆç†æ—¶é—´å†…ï¼ˆçº¦ 20 åˆ†é’Ÿï¼‰è¿è¡Œã€‚

ä»£ç ä»¥åŠ conda ç¯å¢ƒæ–‡ä»¶å¯ä»¥åœ¨ [GitHub ä»“åº“](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/fine-tuning) ä¸­æ‰¾åˆ°ã€‚æœ€ç»ˆæ¨¡å‹å’Œ [æ•°æ®é›†](https://huggingface.co/datasets/shawhin/imdb-truncated) [7] å¯åœ¨ Hugging Face ä¸Šè·å¾—ã€‚

[](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/fine-tuning?source=post_page-----23473d763b91--------------------------------) [## YouTube-Blog/LLMs/fine-tuning at main Â· ShawhinT/YouTube-Blog

### ä»£ç ä»¥è¡¥å…… YouTube è§†é¢‘å’Œ Medium ä¸Šçš„åšå®¢æ–‡ç« ã€‚ - YouTube-Blog/LLMs/fine-tuning at main Â·â€¦

github.com](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/fine-tuning?source=post_page-----23473d763b91--------------------------------)

## å¯¼å…¥

æˆ‘ä»¬ä»å¯¼å…¥æœ‰ç”¨çš„åº“å’Œæ¨¡å—å¼€å§‹ã€‚[Datasets](https://huggingface.co/docs/datasets/index)ã€[transformers](https://huggingface.co/docs/transformers/index)ã€[peft](https://huggingface.co/docs/peft/index) å’Œ [evaluate](https://huggingface.co/docs/evaluate/index) éƒ½æ˜¯æ¥è‡ª[Hugging Face](https://huggingface.co/) (HF) çš„åº“ã€‚

```py
from datasets import load_dataset, DatasetDict, Dataset

from transformers import (
    AutoTokenizer,
    AutoConfig, 
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer)

from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig
import evaluate
import torch
import numpy as np
```

## åŸºç¡€æ¨¡å‹

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åŠ è½½åŸºç¡€æ¨¡å‹ã€‚è¿™é‡Œçš„åŸºç¡€æ¨¡å‹ç›¸å¯¹è¾ƒå°ï¼Œä½†æˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨å…¶ä»–å‡ ä¸ªï¼ˆæ›´å¤§çš„ï¼‰æ¨¡å‹ï¼ˆä¾‹å¦‚ roberta-baseã€llama2ã€gpt2ï¼‰ã€‚å®Œæ•´åˆ—è¡¨è¯·å‚è§[è¿™é‡Œ](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForSequenceClassification)ã€‚

```py
model_checkpoint = 'distilbert-base-uncased'

# define label maps
id2label = {0: "Negative", 1: "Positive"}
label2id = {"Negative":0, "Positive":1}

# generate classification model from model_checkpoint
model = AutoModelForSequenceClassification.from_pretrained(
    model_checkpoint, num_labels=2, id2label=id2label, label2id=label2id)
```

## åŠ è½½æ•°æ®

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ä» HF çš„æ•°æ®é›†åº“ä¸­åŠ è½½æˆ‘ä»¬çš„[è®­ç»ƒå’ŒéªŒè¯æ•°æ®](https://huggingface.co/datasets/shawhin/imdb-truncated)ã€‚è¿™æ˜¯ä¸€ä¸ªåŒ…å« 2000 æ¡ç”µå½±è¯„è®ºï¼ˆ1000 æ¡ç”¨äºè®­ç»ƒï¼Œ1000 æ¡ç”¨äºéªŒè¯ï¼‰çš„æ•°æ®é›†ï¼Œå…¶ä¸­çš„äºŒå…ƒæ ‡ç­¾è¡¨ç¤ºè¯„è®ºæ˜¯ç§¯æçš„ï¼ˆè¿˜æ˜¯æ¶ˆæçš„ï¼‰ã€‚

```py
# load dataset
dataset = load_dataset("shawhin/imdb-truncated")
dataset

# dataset = 
# DatasetDict({
#     train: Dataset({
#         features: ['label', 'text'],
#         num_rows: 1000
#     })
#     validation: Dataset({
#         features: ['label', 'text'],
#         num_rows: 1000
#     })
# }) 
```

## é¢„å¤„ç†æ•°æ®

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦é¢„å¤„ç†æ•°æ®ï¼Œä»¥ä¾¿ç”¨äºè®­ç»ƒã€‚è¿™åŒ…æ‹¬ä½¿ç”¨åˆ†è¯å™¨å°†æ–‡æœ¬è½¬æ¢ä¸ºåŸºç¡€æ¨¡å‹å¯ä»¥ç†è§£çš„æ•´æ•°è¡¨ç¤ºã€‚

```py
# create tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)
```

è¦å°†åˆ†è¯å™¨åº”ç”¨äºæ•°æ®é›†ï¼Œæˆ‘ä»¬ä½¿ç”¨.*map()*æ–¹æ³•ã€‚è¿™éœ€è¦ä¸€ä¸ªè‡ªå®šä¹‰å‡½æ•°æ¥æŒ‡å®šæ–‡æœ¬åº”å¦‚ä½•é¢„å¤„ç†ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¯¥å‡½æ•°ç§°ä¸º*tokenize_function()*ã€‚é™¤äº†å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•´æ•°ä¹‹å¤–ï¼Œæ­¤å‡½æ•°è¿˜ä¼šæˆªæ–­æ•´æ•°åºåˆ—ï¼Œä½¿å…¶ä¸è¶…è¿‡ 512 ä¸ªæ•°å­—ï¼Œä»¥ç¬¦åˆåŸºç¡€æ¨¡å‹çš„æœ€å¤§è¾“å…¥é•¿åº¦ã€‚

```py
# create tokenize function
def tokenize_function(examples):
    # extract text
    text = examples["text"]

    #tokenize and truncate text
    tokenizer.truncation_side = "left"
    tokenized_inputs = tokenizer(
        text,
        return_tensors="np",
        truncation=True,
        max_length=512
    )

    return tokenized_inputs

# add pad token if none exists
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    model.resize_token_embeddings(len(tokenizer))

# tokenize training and validation datasets
tokenized_dataset = dataset.map(tokenize_function, batched=True)
tokenized_dataset

# tokenized_dataset = 
# DatasetDict({
#     train: Dataset({
#        features: ['label', 'text', 'input_ids', 'attention_mask'],
#         num_rows: 1000
#     })
#     validation: Dataset({
#         features: ['label', 'text', 'input_ids', 'attention_mask'],
#         num_rows: 1000
#     })
# })
```

æ­¤æ—¶ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥åˆ›å»ºä¸€ä¸ªæ•°æ®æ•´ç†å™¨ï¼Œå®ƒå°†åœ¨è®­ç»ƒæœŸé—´åŠ¨æ€å¡«å……æ¯ä¸ªæ‰¹æ¬¡ä¸­çš„ç¤ºä¾‹ï¼Œä»¥ä½¿å®ƒä»¬éƒ½å…·æœ‰ç›¸åŒçš„é•¿åº¦ã€‚è¿™æ¯”å°†æ‰€æœ‰ç¤ºä¾‹å¡«å……åˆ°æ•´ä¸ªæ•°æ®é›†ä¸­çš„ç›¸åŒé•¿åº¦æ›´å…·è®¡ç®—æ•ˆç‡ã€‚

```py
# create data collator
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

## è¯„ä¼°æŒ‡æ ‡

æˆ‘ä»¬å¯ä»¥é€šè¿‡è‡ªå®šä¹‰å‡½æ•°å®šä¹‰å¦‚ä½•è¯„ä¼°æˆ‘ä»¬å¾®è°ƒåçš„æ¨¡å‹ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å®šä¹‰äº†*compute_metrics()*å‡½æ•°æ¥è®¡ç®—æ¨¡å‹çš„å‡†ç¡®ç‡ã€‚

```py
# import accuracy evaluation metric
accuracy = evaluate.load("accuracy")

# define an evaluation function to pass into trainer later
def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=1)

    return {"accuracy": accuracy.compute(predictions=predictions, 
                                          references=labels)}
```

## æœªè®­ç»ƒæ¨¡å‹çš„æ€§èƒ½

åœ¨è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬å¯ä»¥è¯„ä¼°åŸºç¡€æ¨¡å‹åœ¨éšæœºåˆå§‹åŒ–åˆ†ç±»å¤´ä¸Šçš„ä¸€äº›ç¤ºä¾‹è¾“å…¥çš„è¡¨ç°ã€‚

```py
# define list of examples
text_list = ["It was good.", "Not a fan, don't recommed.", 
"Better than the first one.", "This is not worth watching even once.", 
"This one is a pass."]

print("Untrained model predictions:")
print("----------------------------")
for text in text_list:
    # tokenize text
    inputs = tokenizer.encode(text, return_tensors="pt")
    # compute logits
    logits = model(inputs).logits
    # convert logits to label
    predictions = torch.argmax(logits)

    print(text + " - " + id2label[predictions.tolist()])

# Output:
# Untrained model predictions:
# ----------------------------
# It was good. - Negative
# Not a fan, don't recommed. - Negative
# Better than the first one. - Negative
# This is not worth watching even once. - Negative
# This one is a pass. - Negative
```

æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œæ¨¡å‹çš„æ€§èƒ½ç­‰åŒäºéšæœºçŒœæµ‹ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•é€šè¿‡å¾®è°ƒæ¥æ”¹å–„è¿™ä¸€ç‚¹ã€‚

## ä½¿ç”¨ LoRA å¾®è°ƒ

è¦ä½¿ç”¨ LoRA è¿›è¡Œå¾®è°ƒï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦ä¸€ä¸ªé…ç½®æ–‡ä»¶ã€‚è¿™è®¾ç½®äº† LoRA ç®—æ³•çš„æ‰€æœ‰å‚æ•°ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ä»£ç å—ä¸­çš„æ³¨é‡Šã€‚

```py
peft_config = LoraConfig(task_type="SEQ_CLS", # sequence classification
                        r=4, # intrinsic rank of trainable weight matrix
                        lora_alpha=32, # this is like a learning rate
                        lora_dropout=0.01, # probablity of dropout
                        target_modules = ['q_lin']) # we apply lora to query layer only
```

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªæ–°çš„æ¨¡å‹ç‰ˆæœ¬ï¼Œè¯¥æ¨¡å‹å¯ä»¥é€šè¿‡ PEFT è¿›è¡Œè®­ç»ƒã€‚æ³¨æ„åˆ°å¯è®­ç»ƒå‚æ•°çš„è§„æ¨¡å‡å°‘äº†å¤§çº¦ 100 å€ã€‚

```py
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

# trainable params: 1,221,124 || all params: 67,584,004 || trainable%: 1.8068239934408148
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰æ¨¡å‹è®­ç»ƒçš„è¶…å‚æ•°ã€‚

```py
# hyperparameters
lr = 1e-3 # size of optimization step 
batch_size = 4 # number of examples processed per optimziation step
num_epochs = 10 # number of times model runs through training data

# define training arguments
training_args = TrainingArguments(
    output_dir= model_checkpoint + "-lora-text-classification",
    learning_rate=lr,
    per_device_train_batch_size=batch_size, 
    per_device_eval_batch_size=batch_size,
    num_train_epochs=num_epochs,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
)
```

æœ€åï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ª trainer()å¯¹è±¡å¹¶å¾®è°ƒæ¨¡å‹ï¼

```py
# creater trainer object
trainer = Trainer(
    model=model, # our peft model
    args=training_args, # hyperparameters
    train_dataset=tokenized_dataset["train"], # training data
    eval_dataset=tokenized_dataset["validation"], # validation data
    tokenizer=tokenizer, # define tokenizer
    data_collator=data_collator, # this will dynamically pad examples in each batch to be equal length
    compute_metrics=compute_metrics, # evaluates model using compute_metrics() function from before
)

# train model
trainer.train()
```

ä¸Šè¿°ä»£ç å°†åœ¨è®­ç»ƒæœŸé—´ç”Ÿæˆä»¥ä¸‹æŒ‡æ ‡è¡¨ã€‚

![](img/75c8df11198f7db7e4d932cd45924488.png)

æ¨¡å‹è®­ç»ƒæŒ‡æ ‡ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚

## è®­ç»ƒæ¨¡å‹æ€§èƒ½

ä¸ºäº†æŸ¥çœ‹æ¨¡å‹æ€§èƒ½çš„æå‡ï¼Œæˆ‘ä»¬æ¥å°†å…¶åº”ç”¨äºä¹‹å‰çš„ç›¸åŒ 5 ä¸ªç¤ºä¾‹ã€‚

```py
model.to('mps') # moving to mps for Mac (can alternatively do 'cpu')

print("Trained model predictions:")
print("--------------------------")
for text in text_list:
    inputs = tokenizer.encode(text, return_tensors="pt").to("mps") # moving to mps for Mac (can alternatively do 'cpu')

    logits = model(inputs).logits
    predictions = torch.max(logits,1).indices

    print(text + " - " + id2label[predictions.tolist()[0]])

# Output:
# Trained model predictions:
# ----------------------------
# It was good. - Positive
# Not a fan, don't recommed. - Negative
# Better than the first one. - Positive
# This is not worth watching even once. - Negative
# This one is a pass. - Positive # this one is tricky
```

å¾®è°ƒåçš„æ¨¡å‹ç›¸è¾ƒäºä¹‹å‰çš„éšæœºçŒœæµ‹æœ‰äº†æ˜¾è‘—æ”¹å–„ï¼Œæ­£ç¡®åˆ†ç±»äº†ä¸Šè¿°ä»£ç ä¸­çš„æ‰€æœ‰ç¤ºä¾‹ï¼Œé™¤äº†ä¸€ä¸ªã€‚è¿™ä¸æˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çœ‹åˆ°çš„çº¦ 90%å‡†ç¡®ç‡æŒ‡æ ‡ç›¸ç¬¦ã€‚

é“¾æ¥: [ä»£ç åº“](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/fine-tuning) | [æ¨¡å‹](https://huggingface.co/shawhin/distilbert-base-uncased-lora-text-classification) | [æ•°æ®é›†](https://huggingface.co/datasets/shawhin/imdb-truncated)

# ç»“è®º

è™½ç„¶å¾®è°ƒç°æœ‰æ¨¡å‹æ¯”ä½¿ç”¨å¼€ç®±å³ç”¨çš„æ¨¡å‹éœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºå’ŒæŠ€æœ¯ä¸“é•¿ï¼Œï¼ˆè¾ƒå°çš„ï¼‰å¾®è°ƒæ¨¡å‹å¯ä»¥åœ¨ç‰¹å®šç”¨ä¾‹ä¸­è¶…è¿‡ï¼ˆè¾ƒå¤§çš„ï¼‰é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼Œå³ä½¿åœ¨ä½¿ç”¨å·§å¦™çš„æç¤ºå·¥ç¨‹ç­–ç•¥æ—¶ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æ­¤å¤–ï¼Œå€ŸåŠ©æ‰€æœ‰å¯ç”¨çš„å¼€æº LLM èµ„æºï¼Œå¾®è°ƒæ¨¡å‹ä»¥æ»¡è¶³è‡ªå®šä¹‰åº”ç”¨ä»æœªå¦‚æ­¤ç®€å•ã€‚

æœ¬ç³»åˆ—çš„ä¸‹ä¸€ç¯‡æ–‡ç« å°†è¶…è¶Šæ¨¡å‹å¾®è°ƒï¼Œè®¨è®ºå¦‚ä½•ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªè¯­è¨€æ¨¡å‹ã€‚

ğŸ‘‰ **æ›´å¤šå…³äº LLMs**: ä»‹ç» | [OpenAI API](https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971) | [Hugging Face Transformers](https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161) | [æç¤ºå·¥ç¨‹](https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f) | æ„å»º LLM | QLoRA | [RAG](https://medium.com/towards-data-science/how-to-improve-llms-with-rag-abdc132f76ac) | æ–‡æœ¬åµŒå…¥

![Shaw Talebi](img/02eefb458c6eeff7cd29d40c212e3b22.png)

[Shaw Talebi](https://shawhin.medium.com/?source=post_page-----23473d763b91--------------------------------)

## å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰

[æŸ¥çœ‹åˆ—è¡¨](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c?source=post_page-----23473d763b91--------------------------------)13 ä¸ªæ•…äº‹![](img/82e865594c68f5307e75665842d197bb.png)![](img/b9436354721f807e0390b5e301be2119.png)![](img/59c8db581de77a908457dec8981f3c37.png)

# èµ„æº

**è”ç³»**: [æˆ‘çš„ç½‘ç«™](https://shawhintalebi.com/) | [é¢„çº¦é€šè¯](https://calendly.com/shawhintalebi) | [é—®æˆ‘ä»»ä½•é—®é¢˜](https://shawhintalebi.com/contact/)

**ç¤¾äº¤åª’ä½“**: [YouTube ğŸ¥](https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA) | [LinkedIn](https://www.linkedin.com/in/shawhintalebi/) | [Twitter](https://twitter.com/ShawhinT)

**æ”¯æŒ**: [è¯·æˆ‘å–å’–å•¡](https://www.buymeacoffee.com/shawhint) â˜•ï¸

[](https://shawhin.medium.com/subscribe?source=post_page-----23473d763b91--------------------------------) [## å…è´¹è·å–æˆ‘å†™çš„æ¯ä¸ªæ–°æ•…äº‹

### å…è´¹è·å–æˆ‘å†™çš„æ¯ä¸ªæ–°æ•…äº‹ P.S. æˆ‘ä¸ä¼šå°†æ‚¨çš„é‚®ä»¶åˆ†äº«ç»™ä»»ä½•äºº æ³¨å†Œå³åˆ›å»ºä¸€ä¸ªâ€¦

shawhin.medium.com](https://shawhin.medium.com/subscribe?source=post_page-----23473d763b91--------------------------------)

[1] Deeplearning.ai å¤§å‹è¯­è¨€æ¨¡å‹å¾®è°ƒçŸ­è¯¾ç¨‹: [`www.deeplearning.ai/short-courses/finetuning-large-language-models/`](https://www.deeplearning.ai/short-courses/finetuning-large-language-models/)

[2] [arXiv:2005.14165](https://arxiv.org/abs/2005.14165) **[cs.CL] (**GPT-3 è®ºæ–‡)

[3] [arXiv:2303.18223](https://arxiv.org/abs/2303.18223) **[cs.CL] (**LLMs ç»¼è¿°)

[4] [arXiv:2203.02155](https://arxiv.org/abs/2203.02155) **[cs.CL] (**InstructGPT è®ºæ–‡)

[5] ğŸ¤— PEFT: åœ¨ä½èµ„æºç¡¬ä»¶ä¸Šå¯¹äº¿çº§è§„æ¨¡æ¨¡å‹è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒ: [`huggingface.co/blog/peft`](https://huggingface.co/blog/peft)

[6] [arXiv:2106.09685](https://arxiv.org/abs/2106.09685) **[cs.CL]** (LoRA è®ºæ–‡)

[7] åŸå§‹æ•°æ®é›†æ¥æº â€” Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, å’Œ Christopher Potts. 2011\. [å­¦ä¹ è¯å‘é‡è¿›è¡Œæƒ…æ„Ÿåˆ†æ](https://aclanthology.org/P11-1015)ã€‚åœ¨*ç¬¬ 49 å±Šè®¡ç®—è¯­è¨€å­¦åä¼šå¹´ä¼šï¼šäººç±»è¯­è¨€æŠ€æœ¯ä¼šè®®è®ºæ–‡é›†*ï¼Œç¬¬ 142â€“150 é¡µï¼Œç¾å›½ä¿„å‹’å†ˆå·æ³¢ç‰¹å…°å¸‚ã€‚è®¡ç®—è¯­è¨€å­¦åä¼šã€‚
