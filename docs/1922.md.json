["```py\nspark\n  .readStream\n  .format(\"iceberg\")\n  .option(\"stream-from-timestamp\", ts)\n  .option(\"streaming-skip-delete-snapshots\", True)\n  .option(\"streaming-skip-overwrite-snapshots\", True)\n  .load(constants.process_telemetry_table)\n  .createOrReplaceTempView(\"process_telemetry_view\")\n```", "```py\n+-------------------+---+---------+---------------------+                       \n|timestamp          |id |parent_id|Commandline          |\n+-------------------+---+---------+---------------------+\n|2022-12-25 00:00:01|11 |0        |                     |\n|2022-12-25 00:00:02|2  |0        |c:\\win\\notepad.exe   |\n|2022-12-25 00:00:03|12 |11       |                     |\n|2022-12-25 00:00:08|201|200      |cmdline and args     |\n|2022-12-25 00:00:09|202|201      |                     |\n|2022-12-25 00:00:10|203|202      |c:\\test.exe          |\n+-------------------+---+---------+---------------------+\n```", "```py\nselect\n *,\n -- regroup each rule's tags in a map (ruleName -> Tags)\n map(\n  'rule0',\n  map(\n      'selection1', (CommandLine LIKE '%rundll32.exe%'),\n      'selection2', (CommandLine LIKE '%.sys,%' OR CommandLine LIKE '%.sys %'),\n  )\n ) as sigma\nfrom\n    process_telemetry_view\n```", "```py\ndf = spark.sql(render_file(\"pattern_match.sql\"))\ndf.createOrReplaceTempView(\"pattern_match_view\")\n```", "```py\n+---+---------+---------------------+----------------------------------+\n|id |parent_id|Commandline          |sigma\n+---+---------+---------------------+----------------------------------+\n|11 |0        |                     |{rule0 -> {\n                                        selection1 -> false, \n                                        selection2 -> false\n                                        }, \n                                     }\n```", "```py\n select\n    *,\n    map_keys( -- only keep the rule names of rules that evaluted to true\n    map_filter( -- filter map entries keeping only rules that evaluated to true\n    map( -- store the result of the condition of each rule in a map\n        'rule0', \n        -- rule 0 -> condition: all of selection*\n        sigma.rule0.selection1 AND sigma.rule0.selection2)\n    )\n    , (k,v) -> v = TRUE)) as sigma_final\nfrom\n    pattern_match_view\n```", "```py\ndf = spark.sql(render_file(\"eval_final_condition.sql\"))\n```", "```py\n+---+---------+-------------------------------------+-------------+\n|id |parent_id|sigma                                | sigma_final |\n+---+---------+-------------------------------------+-------------+\n|11 |0        |{rule0 -> {                          | []          |\n                  selection1 -> false, \n                  selection2 -> false\n                  }\n               }\n```", "```py\nstreaming_query = (\n    df\n    .writeStream\n    .queryName(\"detections\")\n    .trigger(processingTime=f\"{trigger} seconds\")\n    .option(\"checkpointLocation\", get_checkpoint_location(constants.tagged_telemetry_table) )\n    .foreachBatch(foreach_batch_function)\n    .start()\n  )\n\nstreaming_query.awaitTermination() \n```", "```py\ndef foreach_batch_function(batchdf, epoch_id):\n    # Transform and write batchDF\n    batchdf.persist()\n    batchdf.createOrReplaceGlobalTempView(\"eval_condition_view\")\n    run(\"insert_into_tagged_telemetry\")\n    run(\"publish_suspected_anomalies\")\n    spark.catalog.clearCache()\n```", "```py\nflux_update_spec = read_flux_update_spec()\nbloom_capacity = 200000\n# reference the scala code\nflux_stateful_function = spark._sc._jvm.cccs.fluxcapacitor.FluxCapacitor.invoke\n# group logs by host_id\njdf = flux_stateful_function(\n          pattern_match_df._jdf, \n          \"host_id\", \n          bloom_capacity, \n          flux_update_spec)\noutput_df = DataFrame(jdf, spark)\n```", "```py\nrules:\n    - rulename: rule1\n      description: proc_creation_win_run_executable_invalid_extension\n      action: parent\n      tags:\n        - name: filter_iexplorer\n        - name: filter_edge_update\n        - name: filter_msiexec_system32\n      parent: parent_id\n      child: id\n```", "```py\nspec = \"\"\"\n    rules:\n        - rulename: rule2\n          action: ancestor\n          child: pid\n          parent: parent_pid\n          tags:\n            - name: pf\n    \"\"\"\n\ndf_input = spark.sql(\"\"\"\n    select\n        *\n    from\n    values\n    (TIMESTAMP '2022-12-30 00:00:05', 'host1', 'pid500', '', map('rule1', map('pf', true, 'cf', false))),\n    (TIMESTAMP '2022-12-30 00:00:06', 'host1', 'pid600', 'pid500', map('rule1', map('pf', false, 'cf', false))),\n    (TIMESTAMP '2022-12-30 00:00:07', 'host1', 'pid700', 'pid600', map('rule1', map('pf', false, 'cf', true)))\n    t(timestamp, host_id, pid, parent_pid, sigma)\n    \"\"\")\n```", "```py\n+-------------------+------+----------+--------------+-------------------------------------+\n|timestamp          |pid   |parent_pid|human_readable|sigma                                |\n+-------------------+------+----------+--------------+-------------------------------------+\n|2022-12-30 00:00:05|pid500|          |[pf]          |{rule2 -> {pf -> true, cf -> false}} |\n|2022-12-30 00:00:06|pid600|pid500    |[]            |{rule2 -> {pf -> false, cf -> false}}|\n|2022-12-30 00:00:07|pid700|pid600    |[cf]          |{rule2 -> {pf -> false, cf -> true}} |\n+-------------------+------+----------+--------------+-------------------------------------+\n```", "```py\njdf = flux_stateful_function(df_input._jdf, \"host_id\", bloom_capacity, spec, True)\ndf_output = DataFrame(jdf, spark)\n```", "```py\n+-------------------+------+----------+--------------+------------------------------------+\n|timestamp          |pid   |parent_pid|human_readable|sigma                               |\n+-------------------+------+----------+--------------+------------------------------------+\n|2022-12-30 00:00:05|pid500|          |[pf]          |{rule2 -> {pf -> true, cf -> false}}|\n|2022-12-30 00:00:06|pid600|pid500    |[pf]          |{rule2 -> {pf -> true, cf -> false}}|\n|2022-12-30 00:00:07|pid700|pid600    |[pf, cf]      |{rule2 -> {pf -> true, cf -> true}} |\n+-------------------+------+----------+--------------+------------------------------------+\n```", "```py\nCALL catalog.system.rewrite_data_files(\n            table => 'catalog.jc_sched.tagged_telemetry_table',\n            strategy => 'sort',\n            sort_order => 'host_id, has_temporal_proximity_tags',\n            options => map('min-input-files', '100',\n                        'max-concurrent-file-group-rewrites', '30',\n                        'partial-progress.enabled', 'true'),\n            where => 'timestamp >= TIMESTAMP \\'2023-05-06 00:00:00\\' '\n        )\n```", "```py\nCALL catalog.system.rewrite_data_files(\n            table => 'catalog.jc_sched.tagged_telemetry_table',\n            strategy => 'sort',\n            sort_order => 'host_id, has_temporal_proximity_tags',\n            options => map('min-input-files', '100',\n                        'max-concurrent-file-group-rewrites', '30',\n                        'partial-progress.enabled', 'true',\n                        'rewrite-all', 'true'),\n            where => 'timestamp >= TIMESTAMP \\'2023-05-05 00:00:00\\' AND timestamp < TIMESTAMP \\'2023-05-06 00:00:00\\' '\n        )\n```", "```py\ndelete from catalog.jc_sched.process_telemetry_table\nwhere\n    timestamp < current_timestamp() - interval 7 days\n```"]