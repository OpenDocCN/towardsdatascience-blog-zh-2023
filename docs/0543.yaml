- en: Beyond Transformers with PyNeuraLogic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/beyond-transformers-with-pyneuralogic-10b70cdc5e45?source=collection_archive---------6-----------------------#2023-02-07](https://towardsdatascience.com/beyond-transformers-with-pyneuralogic-10b70cdc5e45?source=collection_archive---------6-----------------------#2023-02-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: TOWARDS [DEEP RELATIONAL LEARNING](https://medium.com/tag/deep-relational-learning)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Demonstrating the power of neuro-symbolic programming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@lukaszahradnik?source=post_page-----10b70cdc5e45--------------------------------)[![Lukáš
    Zahradník](../Images/fe12d5cebab663f1df4372203636a6e4.png)](https://medium.com/@lukaszahradnik?source=post_page-----10b70cdc5e45--------------------------------)[](https://towardsdatascience.com/?source=post_page-----10b70cdc5e45--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----10b70cdc5e45--------------------------------)
    [Lukáš Zahradník](https://medium.com/@lukaszahradnik?source=post_page-----10b70cdc5e45--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffcee2c9fb760&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-transformers-with-pyneuralogic-10b70cdc5e45&user=Luk%C3%A1%C5%A1+Zahradn%C3%ADk&userId=fcee2c9fb760&source=post_page-fcee2c9fb760----10b70cdc5e45---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----10b70cdc5e45--------------------------------)
    ·8 min read·Feb 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F10b70cdc5e45&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-transformers-with-pyneuralogic-10b70cdc5e45&user=Luk%C3%A1%C5%A1+Zahradn%C3%ADk&userId=fcee2c9fb760&source=-----10b70cdc5e45---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F10b70cdc5e45&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-transformers-with-pyneuralogic-10b70cdc5e45&source=-----10b70cdc5e45---------------------bookmark_footer-----------)![](../Images/10b9f75778c05a2122e8e1b51d3285a0.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Visualization of the attention computation graph from the perspective of one
    token, with visible relationships between tokens. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: In the last few years, we have seen a rise of Transformer¹ based models with
    successful applications in many fields, such as Natural Language Processing or
    Computer Vision. In this article, we will explore a concise, explainable, and
    extendable way to express deep learning models, specifically transformers, as
    a hybrid architecture, i.e., via marrying deep learning with symbolic artificial
    intelligence. To do so, we will implement models in a Python neuro-symbolic framework
    called [PyNeuraLogic](https://github.com/LukasZahradnik/PyNeuraLogic/) *(the author
    is a co-author of the framework)*.
  prefs: []
  type: TYPE_NORMAL
- en: “We cannot construct rich cognitive models in an adequate, automated way without
    the triumvirate of hybrid architecture, rich prior knowledge, and sophisticated
    techniques for reasoning.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Gary Marcus²
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Combining symbolic representation with deep learning fills the gaps in the current
    deep learning models, such as out-of-the-box explainability or missing techniques
    for reasoning. Maybe, raising the number of parameters is not the soundest approach
    to achieving these desired results, just like increasing the number of camera
    megapixels does not necessarily yield better photos.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/680dc9c97005d1c4701ef2d0c54e5636.png)'
  prefs: []
  type: TYPE_IMG
- en: High-level visualization of the neuro-symbolic concept Lifted Relational Neural
    Networks³ (LRNN), which (Py)NeuraLogic implements. Here we show a simple template
    (logic program) with one linear layer followed by a sum aggregation. For each
    (input) sample a unique neural network is constructed. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: The PyNeuraLogic framework is based on logic programming with a twist — logic
    programs hold differentiable parameters. The framework is well-suited for smaller
    structured data, such as molecules, and complex models, such as Transformers and
    Graph Neural Networks. On the other hand, PyNeuraLogic is not the best choice
    for non-relational and large tensor data.
  prefs: []
  type: TYPE_NORMAL
- en: The key component of the framework is a differentiable logic program that we
    refer to as a template. A template consists of logic rules that define the structure
    of neural networks in an abstract way — we can think of a template as a blueprint
    of the model’s architecture. The template is then applied to each input data instance
    to produce (via grounding and neuralization) a neural network unique to the input
    sample. This process is entirely different from other frameworks with predefined
    architectures that cannot adjust themselves to different input samples. For a
    bit closer introduction to the framework, you can see, e.g., a [previous article](/beyond-graph-neural-networks-with-pyneuralogic-c1e6502c46f7)
    on PyNeuralogic from the perspective of Graph Neural Networks.
  prefs: []
  type: TYPE_NORMAL
- en: Symbolic Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/bd0bd53131df7c150f1d66c176f9ed6d.png)'
  prefs: []
  type: TYPE_IMG
- en: The Transformer architecture consists of two blocks — encoder (left) and decoder
    (right). Both blocks share similarities — the decoder is an extended encoder;
    therefore, we will focus only on the encoder, as the decoder implementation is
    analogous. Image by the author, inspired by [1].
  prefs: []
  type: TYPE_NORMAL
- en: We generally tend to implement deep learning models as tensor operations over
    input tokens batched into one large tensor. This makes sense because deep learning
    frameworks and hardware (e.g., GPUs) are typically optimized for processing larger
    tensors instead of multiple ones of diverse shapes and sizes. Transformers are
    no exception, and it is common to batch individual token vector representations
    into one large matrix and represent the model as operations over such matrices.
    Nevertheless, such implementations hide how individual input tokens relate to
    each other, as can be demonstrated in Transformer’s attention mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: The Attention Mechanism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The attention mechanism forms the very core of all the Transformer models. Specifically,
    its classic version makes use of a so-called multi-head scaled dot-product attention.
    Let us decompose the scaled dot-product attention with one head (for clarity)
    into a simple logic program.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/75505fc4973eb9f9d900b57bcf55a17d.png)'
  prefs: []
  type: TYPE_IMG
- en: The scaled dot product attention equation
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of the attention is to decide what parts of the input the network
    should focus on. The attention does that by computing a weighted sum of the values
    *V*, where the weights represent the compatibility of the input keys *K* and queries
    *Q*. In this specific version, the weights are computed by the softmax function
    of the dot product of queries *Q* and keys *K*, divided by the square root of
    the input feature vector dimensionality *d_k*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In PyNeuraLogic, we can fully capture the attention mechanism with the above
    logical rules. The first rule expresses the computation of the weights — it calculates
    the product of the inverse square root of dimensionality with a transposed *j*-th
    key vector and *i*-th query vector. Then we aggregate all the results for a given
    *i* and all possible *j*’s with softmax.
  prefs: []
  type: TYPE_NORMAL
- en: The second rule then calculates a product between this weight vector and the
    corresponding *j*-th value vector and sums up the results across different *j*’s
    for each respective *i*-th token.
  prefs: []
  type: TYPE_NORMAL
- en: Attention Masking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During the training and evaluation, we usually limit what input tokens can attend
    to. For example, we want to restrict tokens from looking ahead and attending to
    upcoming words. Popular frameworks, such as PyTorch, implement this via masking,
    that is, by setting a subset of elements of the scaled dot-product result to some
    very low negative number. Those numbers enforce the softmax function to assign
    zero as the weight for the corresponding token pair.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: With our symbolic representation, we can implement this by simply adding one
    body relation serving as a constraint. When calculating the weights, we restrict
    the *j* index to be less than or equal to the *i* index. In contrast to the masking,
    we compute only the needed scaled dot products.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2fc97073fac58ff837b52cea92fc9e62.png)'
  prefs: []
  type: TYPE_IMG
- en: Regular deep learning frameworks constrain the attention via masking (on the
    left). First, the whole QK^T matrix is calculated, then the values are masked
    by overriding with low values (white crossed cells) to simulate attending only
    to the relevant tokens (blue cells). In PyNeuraLogic, we compute only needed scalar
    values by applying a symbolic constraint (on the right) — hence there are no redundant
    calculations. This benefit is even more significant in the following attention
    versions. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond standard Attention aggregation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Of course, the symbolic “masking” can be completely arbitrary. Most of us heard
    of the *GPT-3⁴* (or its applications, such as *ChatGPT*), based on Sparse Transformers.⁵
    The Sparse Transformer’s attention (the strided version) has two types of attention
    heads:'
  prefs: []
  type: TYPE_NORMAL
- en: One that attends only to previous *n* tokens *(0* ≤ *i* − *j* ≤ *n)*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One that attends only to every *n*-th previous token (*(i* − *j) % n = 0*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The implementations of both types of heads require again only minor changes
    (e.g., for *n = 5*).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/eafd5be4a52a8c3619b31c01893e4720.png)'
  prefs: []
  type: TYPE_IMG
- en: The Relational Attention equations
  prefs: []
  type: TYPE_NORMAL
- en: We can go even further and generalize the attention for graph-like (relational)
    inputs, just like in Relational Attention.⁶ This type of attention operates on
    graphs, where nodes attend only to their neighbors (nodes connected by an edge).
    Queries *Q*, keys *K*, and values *V* are then edge embeddings summed with node
    vector embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This type of attention is, in our case, again almost the same as the previously
    shown scaled dot-product attention. The only difference is the addition of extra
    terms to capture the edges. Feeding a graph as input into the attention mechanism
    seems quite natural, which is not entirely surprising, considering that the Transformer
    is a type of [Graph Neural Network](https://medium.com/towards-data-science/beyond-graph-neural-networks-with-pyneuralogic-c1e6502c46f7),
    acting on fully-connected graphs (when no masking is applied). In the traditional
    tensor representation, this is not that obvious.
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer Encoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, when we showcased the implementation of the Attention mechanism, the missing
    pieces to construct an entire transformer encoder block are relatively straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have already seen in the Relational Attention how one can implement embeddings.
    For the traditional Transformer, the embeddings will be pretty similar. We project
    the input vector into three embedding vectors — keys, queries, and values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Skip connections, Normalization, and Feed-forward Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Query embeddings are summed with the attention’s output via a skip connection.
    The resulting vector is then normalized and passed into a multilayer perceptron
    (MLP).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: For the MLP, we will implement a fully connected neural network with two hidden
    layers, which can be elegantly expressed as one logic rule.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The last skip connection with normalization is then identical to the previous
    one.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Putting it all together
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have built all the necessary parts to construct a Transformer encoder. The
    decoder utilizes the same components; therefore, its implementation would be analogous.
    Let us combine all the blocks into one differentiable logic program that can be
    embedded into a Python script and compiled into Neural Networks with PyNeuraLogic.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we analysed the Transformer architecture and demonstrated its
    implementation in a neuro-symbolic framework called [PyNeuraLogic](https://github.com/LukasZahradnik/PyNeuraLogic/).
    Via this approach, we were able to implement various types of Transformers with
    only minor changes in the code, illustrating how everyone can quickly pivot and
    develop novel Transformer architectures. It also points out the unmistakable resemblance
    of various versions of Transformers, and of Transformers with GNNs.
  prefs: []
  type: TYPE_NORMAL
- en: '[1]: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,
    A., Kaiser, L., & Polosukhin, I.. (2017). Attention Is All You Need.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2]: Marcus, G.. (2020). The Next Decade in AI: Four Steps Towards Robust Artificial
    Intelligence.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3]: Gustav Šourek, Filip Železný, & Ondřej Kuželka (2021). Beyond graph neural
    networks with lifted relational neural networks*. Machine Learning, 110(7), 1695–1738.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[4]: Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
    Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss,
    A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter,
    C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J.,
    Berner, C., McCandlish, S., Radford, A., Sutskever, I., & Amodei, D.. (2020).
    Language Models are Few-Shot Learners.'
  prefs: []
  type: TYPE_NORMAL
- en: '[5]: Child, R., Gray, S., Radford, A., & Sutskever, I.. (2019). Generating
    Long Sequences with Sparse Transformers.'
  prefs: []
  type: TYPE_NORMAL
- en: '[6]: Diao, C., & Loynd, R.. (2022). Relational Attention: Generalizing Transformers
    for Graph-Structured Tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '*The author would like to thank* [*Gustav Šír*](https://medium.com/@sir.gustav)
    *for proofreading this article and giving valuable feedback. If you want to learn
    more about combining logic with deep learning, head to* [*Gustav’s article series*](https://medium.com/tag/deep-relational-learning)*.*'
  prefs: []
  type: TYPE_NORMAL
