- en: 'Courage to Learn ML: Demystifying L1 & L2 Regularization (part 3)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 勇敢学习机器学习：揭开 L1 和 L2 正则化的面纱（第3部分）
- en: 原文：[https://towardsdatascience.com/courage-to-learn-ml-demystifying-l1-l2-regularization-part-3-ee27cd4b557a?source=collection_archive---------3-----------------------#2023-11-29](https://towardsdatascience.com/courage-to-learn-ml-demystifying-l1-l2-regularization-part-3-ee27cd4b557a?source=collection_archive---------3-----------------------#2023-11-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/courage-to-learn-ml-demystifying-l1-l2-regularization-part-3-ee27cd4b557a?source=collection_archive---------3-----------------------#2023-11-29](https://towardsdatascience.com/courage-to-learn-ml-demystifying-l1-l2-regularization-part-3-ee27cd4b557a?source=collection_archive---------3-----------------------#2023-11-29)
- en: Why L0.5, L3, and L4 Regularizations Are Uncommon
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么 L0.5、L3 和 L4 正则化不常见
- en: '[](https://amyma101.medium.com/?source=post_page-----ee27cd4b557a--------------------------------)[![Amy
    Ma](../Images/2edf55456a1f92724535a1441fa2bef5.png)](https://amyma101.medium.com/?source=post_page-----ee27cd4b557a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ee27cd4b557a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ee27cd4b557a--------------------------------)
    [Amy Ma](https://amyma101.medium.com/?source=post_page-----ee27cd4b557a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://amyma101.medium.com/?source=post_page-----ee27cd4b557a--------------------------------)[![Amy
    Ma](../Images/2edf55456a1f92724535a1441fa2bef5.png)](https://amyma101.medium.com/?source=post_page-----ee27cd4b557a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ee27cd4b557a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ee27cd4b557a--------------------------------)
    [Amy Ma](https://amyma101.medium.com/?source=post_page-----ee27cd4b557a--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd6d8df787b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcourage-to-learn-ml-demystifying-l1-l2-regularization-part-3-ee27cd4b557a&user=Amy+Ma&userId=d6d8df787b&source=post_page-d6d8df787b----ee27cd4b557a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ee27cd4b557a--------------------------------)
    ·6 min read·Nov 29, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fee27cd4b557a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcourage-to-learn-ml-demystifying-l1-l2-regularization-part-3-ee27cd4b557a&user=Amy+Ma&userId=d6d8df787b&source=-----ee27cd4b557a---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd6d8df787b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcourage-to-learn-ml-demystifying-l1-l2-regularization-part-3-ee27cd4b557a&user=Amy+Ma&userId=d6d8df787b&source=post_page-d6d8df787b----ee27cd4b557a---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ee27cd4b557a--------------------------------)
    · 6分钟阅读·2023年11月29日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fee27cd4b557a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcourage-to-learn-ml-demystifying-l1-l2-regularization-part-3-ee27cd4b557a&user=Amy+Ma&userId=d6d8df787b&source=-----ee27cd4b557a---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fee27cd4b557a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcourage-to-learn-ml-demystifying-l1-l2-regularization-part-3-ee27cd4b557a&source=-----ee27cd4b557a---------------------bookmark_footer-----------)![](../Images/9f24921d1e575e80b12bb684d5cd6e86.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fee27cd4b557a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcourage-to-learn-ml-demystifying-l1-l2-regularization-part-3-ee27cd4b557a&source=-----ee27cd4b557a---------------------bookmark_footer-----------)![](../Images/9f24921d1e575e80b12bb684d5cd6e86.png)'
- en: Photo by [Kelvin Han](https://unsplash.com/@kelvinhan?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Kelvin Han](https://unsplash.com/@kelvinhan?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'Welcome back to the third installment of ‘[Courage to Learn ML](/towardsdatascience.com/tagged/courage-to-learn-ml):
    Demystifying L1 & L2 Regularization’ Previously, we delved into [the purpose of
    regularization](https://yujing-ma45.medium.com/understanding-l1-l2-regularization-part-1-9c7affe6f920)
    and [decoded L1 and L2 methods through the lens of Lagrange Multipliers.](/courage-to-learn-ml-unraveling-l1-l2-regularization-part-2-1bb171e43b35)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎回到《[勇敢学习机器学习](/towardsdatascience.com/tagged/courage-to-learn-ml)：揭开 L1 和
    L2 正则化的面纱》系列的第三篇文章。在之前的文章中，我们探讨了 [正则化的目的](https://yujing-ma45.medium.com/understanding-l1-l2-regularization-part-1-9c7affe6f920)
    和 [通过拉格朗日乘子解读 L1 和 L2 方法](/courage-to-learn-ml-unraveling-l1-l2-regularization-part-2-1bb171e43b35)
- en: Continuing our journey, our mentor-learner duo will further explore L1 and L2
    regularization using Lagrange Multipliers.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 继续我们的旅程，我们的导师-学习者二人组将进一步探讨使用拉格朗日乘数的 L1 和 L2 正则化。
- en: 'In this article, we’ll tackle some intriguing questions that might have crossed
    your mind. If you’re puzzled about these topics, you’re in the right place:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将探讨一些可能引起你兴趣的问题。如果你对这些话题感到困惑，你来对地方了：
- en: What’s the reason behind not having an L0.5 regularization?
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么没有 L0.5 正则化？
- en: Why do we care about whether a problem is a non-convex problem given most deep
    learning problems are non-convex?
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 既然大多数深度学习问题都是非凸的，我们为什么关心一个问题是否是非凸问题？
- en: Why are norms like L3 and L4 not commonly used?
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么像 L3 和 L4 这样的范数不常用？
- en: Can L1 and L2 regularization be combined? And what are the advantages and disadvantages
    of doing this?
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L1 和 L2 正则化可以结合使用吗？这样做有什么优点和缺点？
- en: I have a question based on our last discussion, I checked that for Lp norm,
    the value of p can be any number larger than 0\. Why don’t use p between 0 and
    1? What’s the reason behind not having an L0.5 regularization?
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 根据我们上次讨论的问题，我查阅了 Lp 范数，发现 p 的值可以是大于 0 的任何数字。为什么不使用 0 和 1 之间的 p 值？为什么没有 L0.5
    正则化？
- en: I’m glad you brought up this question. To get straight to the point, we typically
    avoid *p* values less than 1 because they lead to non-convex optimization problems.
    Let me illustrate this with an image showing the shape of Lp norms for different
    *p* values. Take a close look at **when p=0.5; you’ll notice that the shape is
    decidedly non-convex.**
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我很高兴你提了这个问题。直接说，我们通常避免 *p* 小于 1 的值，因为它们会导致非凸优化问题。让我通过一张展示不同 *p* 值的 Lp 范数形状的图像来说明这一点。仔细看看
    **当 p=0.5 时，你会注意到形状明显是非凸的**。
- en: '![](../Images/1f95c9fa59773bbedbaa68470fff96b9.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f95c9fa59773bbedbaa68470fff96b9.png)'
- en: 'The shape of Lp norms for different p value. Source: [https://lh5.googleusercontent.com/EoX3sngY7YnzCGY9CyMX0tEaNuKD3_ZiF4Fp3HQqbyqPtXks2TAbpTj5e4tiDv-U9PT0MAarRrPv6ClJ06C0HXQZKHeK40ZpVgRKke8-Ac0TAqdI7vWFdCXjK4taR40bdSdhGkWB](https://lh5.googleusercontent.com/EoX3sngY7YnzCGY9CyMX0tEaNuKD3_ZiF4Fp3HQqbyqPtXks2TAbpTj5e4tiDv-U9PT0MAarRrPv6ClJ06C0HXQZKHeK40ZpVgRKke8-Ac0TAqdI7vWFdCXjK4taR40bdSdhGkWB)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '不同 p 值的 Lp 范数形状。来源: [https://lh5.googleusercontent.com/EoX3sngY7YnzCGY9CyMX0tEaNuKD3_ZiF4Fp3HQqbyqPtXks2TAbpTj5e4tiDv-U9PT0MAarRrPv6ClJ06C0HXQZKHeK40ZpVgRKke8-Ac0TAqdI7vWFdCXjK4taR40bdSdhGkWB](https://lh5.googleusercontent.com/EoX3sngY7YnzCGY9CyMX0tEaNuKD3_ZiF4Fp3HQqbyqPtXks2TAbpTj5e4tiDv-U9PT0MAarRrPv6ClJ06C0HXQZKHeK40ZpVgRKke8-Ac0TAqdI7vWFdCXjK4taR40bdSdhGkWB)'
- en: This becomes even clearer when we look at a 3D representation, assuming we’re
    optimizing three weights. In this case, it’s evident that the problem isn’t convex,
    with numerous local minima appearing along the boundaries.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们查看 3D 表示时，情况变得更加清晰，假设我们正在优化三个权重。在这种情况下，很明显问题不是凸的，边界上出现了许多局部最小值。
- en: '![](../Images/f98ae3934a0118d6a306ca45c264efb9.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f98ae3934a0118d6a306ca45c264efb9.png)'
- en: 'Source: [https://ekamperi.github.io/images/lp_norms_3d.png](https://ekamperi.github.io/images/lp_norms_3d.png)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '来源: [https://ekamperi.github.io/images/lp_norms_3d.png](https://ekamperi.github.io/images/lp_norms_3d.png)'
- en: The reason why we typically avoid non-convex problems in machine learning is
    their complexity. With a convex problem, you’re guaranteed a global minimum —
    this makes it generally easier to solve. On the other hand, non-convex problems
    often come with multiple local minima and can be computationally intensive and
    unpredictable. It’s exactly these kinds of challenges we aim to sidestep in ML.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常避免在机器学习中使用非凸问题的原因是它们的复杂性。对于凸问题，你可以保证得到全局最小值——这使得解决起来一般更容易。另一方面，非凸问题通常有多个局部最小值，并且可能计算密集且不可预测。正是这些挑战我们在
    ML 中试图避免。
- en: When we use techniques like Lagrange multipliers to optimize a function with
    certain constraints, **it’s crucial that these constraints are convex functions.
    This ensures that adding them to the original problem doesn’t alter its fundamental
    properties, making it more difficult to solve.** This aspect is critical; otherwise,
    adding constraints could add more difficulties to the original problem.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用拉格朗日乘数等技术来优化具有特定约束的函数时，**这些约束必须是凸函数。这确保了将它们添加到原始问题中不会改变其基本性质，从而使问题更难解决。**
    这一点至关重要；否则，添加约束可能会使原始问题更加困难。
- en: Why do we care about whether a problem or a constraint is a non-convex problem
    here? Aren’t most deep learning problems non-convex?
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么我们在这里关注一个问题或约束是否是非凸问题？难道大多数深度学习问题不是非凸的吗？
- en: 'You questions touches an interesting aspect of deep learning. While it’s not
    that we prefer non-convex problems, it’s more accurate to say that **we often
    encounter and have to deal with them in the field of deep learning**. Here’s why:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你的问题涉及了深度学习中的一个有趣方面。虽然我们并不偏爱非凸问题，但更准确地说是**我们经常在深度学习领域遇到并必须处理这些问题**。原因如下：
- en: '**Nature of Deep Learning Models leads to a non-convex loss surface**: Most
    deep learning models, particularly neural networks with hidden layers, inherently
    have non-convex loss functions. This is due to the complex, non-linear transformations
    that occur within these models. The combination of these non-linearities and the
    high dimensionality of the parameter space typically results in a loss surface
    that is non-convex.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**深度学习模型的性质导致非凸损失面**：大多数深度学习模型，特别是具有隐藏层的神经网络，固有地具有非凸损失函数。这是由于这些模型内部发生的复杂非线性变换。非线性变换与参数空间的高维度的结合通常导致一个非凸的损失面。'
- en: '**Local Minima are no longer a problem in deep learning**: In high-dimensional
    spaces, which are typical in deep learning, local minima are not as problematic
    as they might be in lower-dimensional spaces. Research suggests that many of the
    local minima in deep learning are close in value to the global minimum. Moreover,
    saddle points — points where the gradient is zero but are neither maxima nor minima
    — are more common in such spaces and are a bigger challenge.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**局部最小值在深度学习中不再是问题**：在深度学习中典型的高维空间中，局部最小值不像在低维空间中那样成问题。研究表明，深度学习中的许多局部最小值与全局最小值的值接近。此外，鞍点——梯度为零但既不是极大值也不是极小值的点——在这样的空间中更为常见，并且是更大的挑战。'
- en: '**Advanced optimization techniques exist that are more effective in dealing
    with non-convex spaces.** Advanced optimization techniques, such as stochastic
    gradient descent (SGD) and its variants, have been particularly effective in finding
    good solutions in these non-convex spaces. While these solutions might not be
    global minima, they often are good enough to achieve high performance on practical
    tasks.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**存在更有效的高级优化技术来处理非凸空间。** 高级优化技术，如随机梯度下降（SGD）及其变体，在这些非凸空间中特别有效地找到良好的解决方案。虽然这些解决方案可能不是全局最小值，但它们通常足够好，可以在实际任务中实现高性能。'
- en: Even though deep learning models are non-convex, they excel at capturing complex
    patterns and relationships in large datasets. Additionally, research into non-convex
    functions is continually progressing, enhancing our understanding. Looking ahead,
    there’s potential for us to handle non-convex problems more efficiently, with
    fewer concerns.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习模型是非凸的，但它们在捕捉大型数据集中的复杂模式和关系方面表现出色。此外，对非凸函数的研究持续推进，增强了我们的理解。展望未来，我们有可能以更高效的方式处理非凸问题，减少担忧。
- en: Why don’t we consider using higher norms, like L3 and L4, for regularization?
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么我们不考虑使用更高的范数，如L3和L4，进行正则化？
- en: Recall the image we discussed earlier showing the shapes of Lp norms for various
    values of *p*. As *p* increases, the Lp norm’s shape evolves. For example, at
    *p = 3*, it resembles a square with rounded corners, and as *p* nears infinity,
    it forms a perfect square.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 回忆一下我们之前讨论的图像，该图像展示了不同*p*值下Lp范数的形状。随着*p*的增加，Lp范数的形状会发生变化。例如，在*p = 3*时，它类似于一个圆角矩形，而当*p*接近无穷大时，它形成一个完美的正方形。
- en: '![](../Images/1f95c9fa59773bbedbaa68470fff96b9.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f95c9fa59773bbedbaa68470fff96b9.png)'
- en: 'The shape of Lp norms for different p value. Source: [https://lh5.googleusercontent.com/EoX3sngY7YnzCGY9CyMX0tEaNuKD3_ZiF4Fp3HQqbyqPtXks2TAbpTj5e4tiDv-U9PT0MAarRrPv6ClJ06C0HXQZKHeK40ZpVgRKke8-Ac0TAqdI7vWFdCXjK4taR40bdSdhGkWB](https://lh5.googleusercontent.com/EoX3sngY7YnzCGY9CyMX0tEaNuKD3_ZiF4Fp3HQqbyqPtXks2TAbpTj5e4tiDv-U9PT0MAarRrPv6ClJ06C0HXQZKHeK40ZpVgRKke8-Ac0TAqdI7vWFdCXjK4taR40bdSdhGkWB)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 不同p值下的Lp范数的形状。来源：[https://lh5.googleusercontent.com/EoX3sngY7YnzCGY9CyMX0tEaNuKD3_ZiF4Fp3HQqbyqPtXks2TAbpTj5e4tiDv-U9PT0MAarRrPv6ClJ06C0HXQZKHeK40ZpVgRKke8-Ac0TAqdI7vWFdCXjK4taR40bdSdhGkWB](https://lh5.googleusercontent.com/EoX3sngY7YnzCGY9CyMX0tEaNuKD3_ZiF4Fp3HQqbyqPtXks2TAbpTj5e4tiDv-U9PT0MAarRrPv6ClJ06C0HXQZKHeK40ZpVgRKke8-Ac0TAqdI7vWFdCXjK4taR40bdSdhGkWB)
- en: 'In our optimization problem’s context, consider higher norms like L3 or L4\.
    Similar to L2 regularization, where the loss function and constraint contours
    intersect at rounded edges, these higher norms would encourage weights to approximate
    zero, just like L2 regularization. (If this part isn’t clear, feel free to revisit
    [Part 2](https://yujing-ma45.medium.com/courage-to-learn-ml-unraveling-l1-l2-regularization-part-2-1bb171e43b35)
    for a more detailed explanation.) Based on this statement, we can talk about the
    two crucial reasons why L3 and L4 norms aren’t commonly used:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的优化问题背景下，考虑更高的范数如 L3 或 L4。类似于 L2 正则化，其中损失函数和约束轮廓在圆角处交叉，这些更高的范数将鼓励权重接近零，就像
    L2 正则化一样。（如果这部分不清楚，请随时回顾 [第 2 部分](https://yujing-ma45.medium.com/courage-to-learn-ml-unraveling-l1-l2-regularization-part-2-1bb171e43b35)
    以获取更详细的解释。）基于这一声明，我们可以讨论 L3 和 L4 范数不常用的两个关键原因：
- en: '**L3 and L4 norms demonstrate similar effects as L2, without offering significant
    new advantages (make weights close to 0).** L1 regularization, in contrast, zeroes
    out weights and introduces sparsity, useful for feature selection.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**L3 和 L4 范数表现出与 L2 相似的效果，而没有提供显著的新优势（使权重接近 0）。** 相比之下，L1 正则化会将权重归零并引入稀疏性，有利于特征选择。'
- en: '**Computational complexity is another vital aspect.** Regularization affects
    the optimization process’s complexity. L3 and L4 norms are computationally heavier
    than L2, making them less feasible for most machine learning applications.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**计算复杂性是另一个重要方面。** 正则化会影响优化过程的复杂性。L3 和 L4 范数比 L2 的计算负担更重，使它们在大多数机器学习应用中不够可行。'
- en: To sum up, while L3 and L4 norms could be used in theory, they don’t provide
    unique benefits over L1 or L2 regularization, and their computational inefficiency
    makes them less practical choice.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，虽然理论上可以使用 L3 和 L4 范数，但它们并没有比 L1 或 L2 正则化提供独特的好处，而且它们的计算低效使得它们不太实用。
- en: '**Is it possible to combine L1 and L2 regularization?**'
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**是否可以结合 L1 和 L2 正则化？**'
- en: Yes, it is indeed possible to combine L1 and L2 regularization, a technique
    often referred to as Elastic Net regularization. This approach blends the properties
    of both L1 (lasso) and L2 (ridge) regularization together and can be useful while
    challenging.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，确实可以结合 L1 和 L2 正则化，这种技术通常称为弹性网正则化。这种方法将 L1（lasso）和 L2（ridge）正则化的特性融合在一起，既有用又有挑战。
- en: Elastic Net regularization is a linear combination of the L1 and L2 regularization
    terms. It adds both the L1 and L2 norm to the loss function. So it has two parameters
    to be tuned, lambda1 and lambda2
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 弹性网正则化是 L1 和 L2 正则化项的线性组合。它将 L1 和 L2 范数都添加到损失函数中。因此，它有两个需要调整的参数，lambda1 和 lambda2。
- en: '![](../Images/cacda52edd14e0bab2c5385e73d83ee8.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cacda52edd14e0bab2c5385e73d83ee8.png)'
- en: 'Elastic Net regularization. Source: [https://wikimedia.org/api/rest_v1/media/math/render/svg/a66c7bfcf201d515eb71dd0aed5c8553ce990b6e](https://wikimedia.org/api/rest_v1/media/math/render/svg/a66c7bfcf201d515eb71dd0aed5c8553ce990b6e)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 弹性网正则化。来源：[https://wikimedia.org/api/rest_v1/media/math/render/svg/a66c7bfcf201d515eb71dd0aed5c8553ce990b6e](https://wikimedia.org/api/rest_v1/media/math/render/svg/a66c7bfcf201d515eb71dd0aed5c8553ce990b6e)
- en: What is the benefit of using Elastic Net regularization? If so, why don’t we
    use it more often?
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用弹性网正则化的好处是什么？如果有的话，为什么我们不更频繁地使用它？
- en: By combining both regularization techniques, Elastic Net can improve the generalization
    capability of the model, reducing the risk of overfitting more effectively than
    using either L1 or L2 alone.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合这两种正则化技术，弹性网可以提高模型的泛化能力，比单独使用 L1 或 L2 更有效地减少过拟合的风险。
- en: 'Let’s break it down its advantages:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解一下它的优点：
- en: '**Elastic Net provides more stability than L1\.** L1 regularization can lead
    to sparse models, which is useful for feature selection. But it can also be unstable
    in certain situations. For example, L1 regularization can select features arbitrarily
    among highly correlated variables (while make others’ coefficients become 0).
    While Elastic Net can distribute the weights more evenly among those variables.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**弹性网提供的稳定性比 L1 更强。** L1 正则化可能导致稀疏模型，这对特征选择有用。但在某些情况下它也可能不稳定。例如，L1 正则化可能在高度相关的变量中任意选择特征（同时使其他系数变为
    0）。而弹性网可以在这些变量之间更均匀地分配权重。'
- en: '**L2 can be more stable than L1 regularization, but it doesn’t encourage sparsity.**
    Elastic Net aims to balance these two aspects, potentially leading to more robust
    models.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**L2 比 L1 正则化更稳定，但不鼓励稀疏性。** 弹性网旨在平衡这两个方面，可能导致更稳健的模型。'
- en: However, **Elastic Net regularization introduces an extra hyperparameter that
    demands meticulous tuning**. Achieving the right balance between L1 and L2 regularization
    and optimal model performance involves **increased computational effort**. This
    added complexity is why it’s not frequently used.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，**弹性网正则化引入了一个额外的超参数，需要细致的调整**。在L1和L2正则化之间取得适当的平衡，并实现最佳模型性能需要**增加计算工作量**。这种额外的复杂性是它不常用的原因。
- en: In our next session, we’ll explore L1 and L2 regularization from an entirely
    new angle, delving into the realm of *Bayesian* prior beliefs to deepen our understanding.
    Let’s pause here for now — looking forward to our next discussion!
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一次会议中，我们将从全新的角度探索L1和L2正则化，*深入探讨*贝叶斯先验信念以加深理解。暂时先停在这里——期待我们的下一次讨论！
- en: 'Other posts in this series:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 本系列中的其他帖子：
- en: '[Courage to Learn ML: Demystifying L1 & L2 Regularization (part 1)](/understanding-l1-l2-regularization-part-1-9c7affe6f920)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[勇敢学习机器学习：揭开L1和L2正则化的神秘面纱（第1部分）](/understanding-l1-l2-regularization-part-1-9c7affe6f920)'
- en: '[Courage to Learn ML: Demystifying L1 & L2 Regularization (part 2)](/courage-to-learn-ml-unraveling-l1-l2-regularization-part-2-1bb171e43b35)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[勇敢学习机器学习：揭开L1和L2正则化的神秘面纱（第2部分）](/courage-to-learn-ml-unraveling-l1-l2-regularization-part-2-1bb171e43b35)'
- en: '***If you liked the article, you can find me on*** [***LinkedIn***](https://www.linkedin.com/in/amyma101/)***.***'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***如果你喜欢这篇文章，可以在*** [***LinkedIn***](https://www.linkedin.com/in/amyma101/)***上找到我。***'
- en: '**Reference:**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考：**'
- en: '[Elastic Net Regularization via Iterative Soft Thresholding](https://web.mit.edu/lrosasco/www/contents/code/ENcode.html#descr)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[通过迭代软阈值的弹性网正则化](https://web.mit.edu/lrosasco/www/contents/code/ENcode.html#descr)'
- en: '[](https://kevinbinz.com/2019/06/09/regularization/?source=post_page-----ee27cd4b557a--------------------------------)
    [## Intro to Regularization'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[链接3](https://kevinbinz.com/2019/06/09/regularization/?source=post_page-----ee27cd4b557a--------------------------------)
    [## 正则化介绍'
- en: 'Part Of: Machine Learning sequenceFollowup To: Bias vs Variance, Gradient Descent
    Content Summary: 1100 words, 11 min…'
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部分内容：机器学习序列 后续内容：偏差与方差，梯度下降 内容摘要：1100字，11分钟…
- en: kevinbinz.com](https://kevinbinz.com/2019/06/09/regularization/?source=post_page-----ee27cd4b557a--------------------------------)
    [](https://ekamperi.github.io/machine%20learning/2019/10/19/norms-in-machine-learning.html?source=post_page-----ee27cd4b557a--------------------------------)
    [## Norms and machine learning
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[链接1](https://kevinbinz.com/2019/06/09/regularization/?source=post_page-----ee27cd4b557a--------------------------------)
    [链接2](https://ekamperi.github.io/machine%20learning/2019/10/19/norms-in-machine-learning.html?source=post_page-----ee27cd4b557a--------------------------------)
    [## 规范和机器学习'
- en: An introduction to norms in machine learning and optimization in general, emphasizing
    LASSO and ridge regression.
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 介绍了机器学习和优化中的规范，强调LASSO和岭回归。
- en: ekamperi.github.io](https://ekamperi.github.io/machine%20learning/2019/10/19/norms-in-machine-learning.html?source=post_page-----ee27cd4b557a--------------------------------)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[链接4](https://ekamperi.github.io/machine%20learning/2019/10/19/norms-in-machine-learning.html?source=post_page-----ee27cd4b557a--------------------------------)'
