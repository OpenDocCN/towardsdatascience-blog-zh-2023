["```py\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\n\n# 1\\. Load your documents:\ndocuments = SimpleDirectoryReader(\"YOUR_DATA\").load_data()\n\n# 2\\. Convert them to vectors:\nindex = VectorStoreIndex.from_documents(documents)\n\n# 3\\. Ask the question:\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"When's my boss's birthday?\")\nprint(response)\n```", "```py\n# 1\\. Download the model:\npython scripts/download.py --repo_id meta-llama/Llama-2-7b\n\n# 2\\. Convert the checkpoint to the lit-gpt format:\npython scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/llama\n\n# 3\\. Generate an instruction tuning dataset:\npython scripts/prepare_alpaca.py  # it should be your dataset\n\n# 4\\. Run the finetuning script\npython finetune/lora.py \\\n    --checkpoint_dir checkpoints/llama/\n    --data_dir your_data_folder/\n    --out_dir my_finetuned_model/ \n```", "```py\n# 1\\. Create a folder where your model will be stored:\nmkdir data\n\n# 2\\. Run Docker container (launch RestAPI service):\ndocker run --gpus all --shm-size 1g -p 8080:80 \\\n    -v $volume:/data \\\n    ghcr.io/huggingface/text-generation-inference:1.1.0\n    --model-id meta-llama/Llama-2-7b\n\n# 3\\. And now you can make requests:\ncurl 127.0.0.1:8080/generate \\\n    -X POST \\\n    -d '{\"inputs\":\"Tell me a joke!\",\"parameters\":{\"max_new_tokens\":20}}' \\\n    -H 'Content-Type: application/json'\n```"]