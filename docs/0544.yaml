- en: Demystifying the Random Forest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/demystifying-the-random-forest-8a46f4fd416f?source=collection_archive---------7-----------------------#2023-02-07](https://towardsdatascience.com/demystifying-the-random-forest-8a46f4fd416f?source=collection_archive---------7-----------------------#2023-02-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Deconstructing and Understanding this Beautiful Algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@siddarth.ramesh?source=post_page-----8a46f4fd416f--------------------------------)[![Siddarth
    Ramesh](../Images/645d2850a35ef0175a2fb1eeee9472d5.png)](https://medium.com/@siddarth.ramesh?source=post_page-----8a46f4fd416f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8a46f4fd416f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8a46f4fd416f--------------------------------)
    [Siddarth Ramesh](https://medium.com/@siddarth.ramesh?source=post_page-----8a46f4fd416f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcf4e627f4995&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-the-random-forest-8a46f4fd416f&user=Siddarth+Ramesh&userId=cf4e627f4995&source=post_page-cf4e627f4995----8a46f4fd416f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8a46f4fd416f--------------------------------)
    ·15 min read·Feb 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8a46f4fd416f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-the-random-forest-8a46f4fd416f&user=Siddarth+Ramesh&userId=cf4e627f4995&source=-----8a46f4fd416f---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8a46f4fd416f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-the-random-forest-8a46f4fd416f&source=-----8a46f4fd416f---------------------bookmark_footer-----------)![](../Images/bd5376dded404c13d44399eb9e2aa93f.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Inggrid Koe](https://unsplash.com/@inggridkoe?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/kbKEuU-YEIw?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: In classical Machine Learning, Random Forests have been a silver bullet type
    of model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model is great for a few reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Requires less preprocessing of data compared to many other algorithms, which
    makes it easy to set up
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Acts as either a classification or regression model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Less prone to overfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easily can compute feature importance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this post, I want to better understand the components that make up a Random
    Forest. To accomplish this, I am going to deconstruct the Random Forest into its
    most basic components and explain what is going on in each level of computation.
    By the end, we will have attained a much deeper understanding of how Random Forests
    work and how to work with them with more intuition. The examples we will use will
    be focused on **classification**, but many of the principles apply to the regression
    scenarios as well.
  prefs: []
  type: TYPE_NORMAL
- en: Running a Random Forest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start by invoking a classic Random Forest pattern. This is the highest
    level, and what many people do when training Random Forests in python.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c6e2edf8aedaeb3086ca8411e9f80d70.png)'
  prefs: []
  type: TYPE_IMG
- en: Simulated data. Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: If I wanted to run a Random Forest to predict my `target` column, I would just
    do the following
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Running the Random Forest classifier was super simple. I just defined the `n_estimators`
    parameter and set the `random_state` to 0\. I can tell you from personal experience
    that a lot of people would just look at that `.93` , be happy, and deploy that
    thing in the wild. We won’t be doing that today.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s re-examine this innocuous line
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: A random state is a feature of most Data Science models that ensures that someone
    else can reproduce your work. We won’t worry too much about that parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look deeply at `n_estimators`. If we look at the `scikit-learn` [docs](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html),
    the definition states:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of trees in the forest.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Investigating the Number of Trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At this point, let’s define a Random Forest a little more specifically. A **Random
    Forest** is an **ensemble** model that is a consensus of many **Decision Trees.**
    The definition is probably incomplete, but we will come back to it.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/91cfa0ae2df0113cc4757ec75d3e0fc8.png)'
  prefs: []
  type: TYPE_IMG
- en: Many trees talk to each other and arrive at a consensus. Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'That *might* make you think that if you broke it down into something like the
    following, you might arrive at a Random Forest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In that above example, we trained 3 decision trees on `X_train`, which means
    that `n_estimators = 3`. After training the 3 trees, we predicted each tree on
    the same test set and then finally take the prediction that 2 out of 3 trees pick.
  prefs: []
  type: TYPE_NORMAL
- en: That sort of makes sense, but this doesn’t look totally right. If all the decision
    trees were trained on the same data, wouldn’t they mostly reach the same conclusion,
    thereby negating the advantage of an ensemble?
  prefs: []
  type: TYPE_NORMAL
- en: Demystifying Sampling With Replacement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s add a word to our definition from earlier: A Random Forestis an ensemblemodel
    that is a consensus of many ***uncorrelated***Decision Trees**.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Decision Trees can become uncorrelated in 2 ways:'
  prefs: []
  type: TYPE_NORMAL
- en: You have a large enough dataset size where you can sample unique parts of your
    data to each decision tree. This is not as popular and often requires a huge amount
    of data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can leverage a technique called **sampling with replacement.** Sampling
    with replacement means that a sample that is drawn from a population is returned
    to the population before the next sample is drawn.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To explain sampling with replacement, let’s say I had 5 marbles with 3 colors,
    so my population looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`blue, blue, red, green, red`'
  prefs: []
  type: TYPE_NORMAL
- en: 'If I wanted to sample some marbles, I’d normally pluck out a couple and perhaps
    end up with:'
  prefs: []
  type: TYPE_NORMAL
- en: '`blue, red`'
  prefs: []
  type: TYPE_NORMAL
- en: This is because once I picked up red, I didn’t return it to my original stack
    of marbles.
  prefs: []
  type: TYPE_NORMAL
- en: However, if I was doing sampling with replacement, I can actually pick up any
    of my marbles twice. Because red was returned to my stack, I had a chance of picking
    it up again.
  prefs: []
  type: TYPE_NORMAL
- en: '`red, red`'
  prefs: []
  type: TYPE_NORMAL
- en: In Random Forests, the default is to build samples that are about 2/3 of the
    original population size. If my original train data was 1000 rows, then the train
    data samples I feed to my trees might be around 670 rows. That being said, trying
    different sampling ratios would be a great parameter to tune as you construct
    your Random Forest.
  prefs: []
  type: TYPE_NORMAL
- en: The below code, unlike the last snippet, is much closer to that of a Random
    Forest where `n_estimators = 3`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/30d1e6ff15b476f306ff621175bf2f9f.png)'
  prefs: []
  type: TYPE_IMG
- en: We sample with replacement, feed those samples to trees, produce results, and
    achieve consensus. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging Classifiers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/3ef504d907cfa554a6767b1917c186f7.png)'
  prefs: []
  type: TYPE_IMG
- en: The earlier architecture was actually a Bagging Classifier. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: We’re going to introduce a *new* algorithm at this point called **Bootstrap
    Aggregation**, also known as Bagging, but rest assured that this will tie back
    to Random Forests. The reason we are introducing this new concept is because as
    we can see in the below figure, everything we just did up to now is actually what
    a `BaggingClassifier` does!
  prefs: []
  type: TYPE_NORMAL
- en: In the code below, the `BaggingClassifier` has a parameter called `bootstrap`
    which actually executes the sampling-with-replacement step we just manually did.
    This same parameter exists for the `sklearn` Random Forest implementation. If
    bootstrapping was false, we would use the entire population for each classifier.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`BaggingClassifiers` are awesome because you can use them with estimators not
    named Decision Trees! You can plug in many algorithms, and Bagging turns it into
    an ensemble solution. A Random Forest Algorithm actually *extends* the Bagging
    Algorithm (if `bootstrapping = true`) because it partially leverages the bagging
    to form uncorrelated decision trees.'
  prefs: []
  type: TYPE_NORMAL
- en: However even if `bootstrapping = false`, Random Forests go one step extra to
    *really* make sure the trees are not correlated — feature sampling.
  prefs: []
  type: TYPE_NORMAL
- en: Demystifying Feature Sampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Feature sampling means that not only are the rowssampled, but the columns too.
    Unlike the rows, the columns of the Random Forest are sampled *without* replacement,
    which means we won’t have duplicate columns training 1 tree.
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to sample the features. You can specify a fixed max number
    of features to sample, take the square-root of the total number of features, or
    try using logs. Each of these approaches has tradeoffs and will depend on your
    data and use-case.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ebba61e751cb491987d4eed373570aba.png)'
  prefs: []
  type: TYPE_IMG
- en: Bagging gets extended with Feature Sampling. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: The below snippet samples the columns using the `sqrt` technique, samples the
    rows, trains 3 Decision trees, and uses majority-rules to make the prediction.
    We first perform sampling-with-replacement, them sample the columns, train our
    individual trees, let our trees predict on our test data, then take the majority
    rules consensus.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: When I ran this code, I found that my decision trees started predicting different
    things, which indicated that we had removed a lot of the correlations between
    the trees.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b87df62d480d87d06d536fc11c9cdff3.png)'
  prefs: []
  type: TYPE_IMG
- en: My trees no longer are agreeing with each other all the time! Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: Decision Tree Basics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Up to this point, we’ve deconstructed how data is fed into a multitude of Decision
    Trees. In the previous code examples, we used the `DecisionTreeClassifier()` to
    train Decision Trees, but we will need to deconstruct Decision Trees in order
    to completely understand Random Forests.
  prefs: []
  type: TYPE_NORMAL
- en: A Decision Tree, true to its name, looks like an upside-down tree. At a high
    level, the algorithm tries to ask questions to split data into different nodes.
    The below image shows an example of what a decision tree looks like.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/691a0400401b95fbc6a92344d3266917.png)'
  prefs: []
  type: TYPE_IMG
- en: Example Tree. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: A decision tree asks a series of questions based on the answer of the previous
    question. For every question it asks, there might be multiple answers, and we
    visualize that as **splitting the node**. The answer of the previous question
    will determine the next question the tree will ask. At some point after asking
    a series of questions, you arrive at the answer.
  prefs: []
  type: TYPE_NORMAL
- en: But how do you know your answer is accurate, or that you have asked the right
    questions? You can actually evaluate your decision trees in a couple different
    ways, and we will of course break down those approaches as well.
  prefs: []
  type: TYPE_NORMAL
- en: Entropy and Information Gain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At this point, we need to discuss a new term called **entropy**. At a high level,
    entropy is one way to measure the level of *impurity* or *randomness* in a node.
    As an aside, there is another popular to way measure the impurity of a node called
    **Gini impurity**, but we won’t deconstruct that method in this post since it
    overlaps with a lot of the concepts around entropy albeit with a slightly different
    computation. The general idea is that higher the entropy or Gini impurity, the
    more variance there is in the node, and our goal is to reduce that uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees try to minimize entropy by splitting the node they ask about
    into smaller more homogenous nodes. The actual formula for entropy is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fce8f09b156d8ccd46d1559dcf681163.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To break down entropy, let’s go back to that marble example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say I have 10 marbles. 5 of them are blue and 5 of them are green. The
    entropy of my collective dataset would be1.0,and the code to calculate entropy
    is below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If the `data` was completely filled with green marbles, the entropy would have
    been 0, and the entropy would increase the closer we got to that 50% split.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every time we reduce entropy, we gain some information about the dataset since
    we have reduced the randomness. **Information gain** tells us which feature relatively
    allowed us best to minimize our entropy. The way you calculate information gain
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '`entropy(parent) — [weighted_average_of_entropy(children)]`'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the parent is the original node and the children are the results
    of splitting the node.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9cdb3546196da9296c15c4dbe3cc344b.png)'
  prefs: []
  type: TYPE_IMG
- en: Splitting a node. Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate the information gain, we perform the following operations:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the entropy of the parent node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Split the parent node into child nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create the `weight` for each child node. This is measured by `number_of_samples_in_child_node
    / number_of_samples_in_parent_node`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the entropy of each child node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create `[weighted_average_of_entropy(children)]` by calculating `weight*entropy_of_child1
    + weight*entropy_of_child2`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subtract this weighted entropy from the entropy of the parent node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The below code implements a simple information gain for a parent node being
    split into 2 child nodes
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Deconstructing the Decision Tree
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With these concepts in mind, we are ready to implement a small decision tree!
  prefs: []
  type: TYPE_NORMAL
- en: Without any guidance, a decision tree will keep splitting nodes until all the
    final leaf nodes are pure. The idea of controlling the complexity of the tree
    is called **pruning,** and we can either prune the tree after it is completely
    built, or pre-prune the tree before the growing phase with certain parameter.
    Some ways to pre-prunetree complexity are to control the number of splits, limit
    the **max depth** (longest distance from root node to leaf node), or set an information
    gain .
  prefs: []
  type: TYPE_NORMAL
- en: The below code ties all these concepts back together
  prefs: []
  type: TYPE_NORMAL
- en: Start with an dataset where you have a target variable to predict
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute entropy (or Gini impurity) of the original dataset (root node)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Look at each feature and compute the information gain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose the best feature that had the best information gain, which is the same
    as which feature caused the entropy to decrease the most
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep growing until our stopping conditions are met — in this case it is our
    max depth limit and nodes having an entropy of 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: To predict on this tree means to traverse your grown tree with your new data
    until it arrives at the leaf node. The final leaf node is the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Some Interesting Things About Random Forests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Everything we discussed in the last section was how an individual decision tree
    makes its decisions. The below image connects those concepts with the earlier
    concepts we discussed around sampling for Random Forests.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/718efa8483662a21fc33d3bef4c6c0fc.png)'
  prefs: []
  type: TYPE_IMG
- en: The Random Forest Architecture with a Deconstructed Decision Tree. Image by
    Author
  prefs: []
  type: TYPE_NORMAL
- en: Because a Decision Tree literally checks the information gain of each feature,
    you can calculate **feature importance** in a Random Forest. The calculation of
    a feature importance is usually seen as the average decrease in impurity across
    *all* the trees. Random Forests are not as interpretable as models like Logistic
    Regressions, so feature importance provide us with a little insight into how our
    trees grew.
  prefs: []
  type: TYPE_NORMAL
- en: And finally, there are a couple ways you can test your trained Random Forest.
    You can always do the classic Machine Learning approach and use a test set to
    measure how well your model generalizes to unseen data. However, that usually
    requires a second computation. Random Forest has this unique property called **out-of-bag
    error**, or **OOB error**. Remember how we only sample a part of our dataset to
    build each tree? You can actually use the rest of the samples to do a validation
    *at the time of training*, and this is really only possible because of the ensemble
    nature of the algorithm. This means that in one shot, we can understand how well
    our model generalizes to unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion and Final Thoughts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To summarize what we learned:'
  prefs: []
  type: TYPE_NORMAL
- en: Random Forests are actually an ensemble of un-correlated **Decision Trees**
    making predictions and reaching a consensus. This consensus is an average of scores
    for regression problems and a majority rules for classification problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random Forests mitigate correlation by leveraging **bagging** and **feature
    sampling**. By leveraging both these techniques, the individual decision trees
    are viewing a particular dimension of our set and making predictions based on
    different factors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision Trees are grown by splitting data on the feature that produces the
    highest **information gain**. Information gain is measure as the highest decrease
    in impurity. Impurity is usually measured via **entropy** or **Gini impurity**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random Forests are able to achieve a limited level of interpretability via **feature
    importance**, which is a measure of average information gain of a feature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random Forests also have the ability to a form of cross-validation at training
    time, a unique technique known as **OOB** error. This is possible because of the
    way the algorithm samples data upstream.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: When looking at the original code to train a Random Forest, it’s amazing to
    me just how many different computations and evaluations happen in these few lines
    of code. There are a multitude of considerations taken to prevent overfitting,
    evaluate both on a tree and forest level, and achieve some basic level of interpretability
    — plus it is very easy to set up thanks to all the frameworks out there.
  prefs: []
  type: TYPE_NORMAL
- en: I hope next time you train a Random Forest model, you are able to check out
    the `[scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)`
    [documentation page](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)
    on Random Forests and better understand all the options you have. While there
    are some intuitive defaults set, it should be clear just how many different tweaks
    you can make and how many of these techniques can extend to other models as well.
  prefs: []
  type: TYPE_NORMAL
- en: I had a lot of fun writing this post, and personally learned a ton about how
    this beautiful algorithm works. I hope that you take something away too!
  prefs: []
  type: TYPE_NORMAL
