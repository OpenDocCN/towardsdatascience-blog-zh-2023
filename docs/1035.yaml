- en: Your Vision-Language Model Might Be a Bag of Words
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你的视觉-语言模型可能只是一个词袋
- en: 原文：[https://towardsdatascience.com/your-vision-language-model-might-be-a-bag-of-words-30b1beaef7f8?source=collection_archive---------5-----------------------#2023-03-21](https://towardsdatascience.com/your-vision-language-model-might-be-a-bag-of-words-30b1beaef7f8?source=collection_archive---------5-----------------------#2023-03-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/your-vision-language-model-might-be-a-bag-of-words-30b1beaef7f8?source=collection_archive---------5-----------------------#2023-03-21](https://towardsdatascience.com/your-vision-language-model-might-be-a-bag-of-words-30b1beaef7f8?source=collection_archive---------5-----------------------#2023-03-21)
- en: We explore the limits of what vision-language models get about language in our
    Oral Paper at ICLR 2023
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们在 ICLR 2023 的口头报告中探讨了视觉-语言模型对语言的理解极限
- en: '[](https://fede-bianchi.medium.com/?source=post_page-----30b1beaef7f8--------------------------------)[![Federico
    Bianchi](../Images/fa38ff2051af04df7803af7d84c5cd4d.png)](https://fede-bianchi.medium.com/?source=post_page-----30b1beaef7f8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----30b1beaef7f8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----30b1beaef7f8--------------------------------)
    [Federico Bianchi](https://fede-bianchi.medium.com/?source=post_page-----30b1beaef7f8--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://fede-bianchi.medium.com/?source=post_page-----30b1beaef7f8--------------------------------)[![Federico
    Bianchi](../Images/fa38ff2051af04df7803af7d84c5cd4d.png)](https://fede-bianchi.medium.com/?source=post_page-----30b1beaef7f8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----30b1beaef7f8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----30b1beaef7f8--------------------------------)
    [Federico Bianchi](https://fede-bianchi.medium.com/?source=post_page-----30b1beaef7f8--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2aff872fe60e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fyour-vision-language-model-might-be-a-bag-of-words-30b1beaef7f8&user=Federico+Bianchi&userId=2aff872fe60e&source=post_page-2aff872fe60e----30b1beaef7f8---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----30b1beaef7f8--------------------------------)
    ·8 min read·Mar 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F30b1beaef7f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fyour-vision-language-model-might-be-a-bag-of-words-30b1beaef7f8&user=Federico+Bianchi&userId=2aff872fe60e&source=-----30b1beaef7f8---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2aff872fe60e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fyour-vision-language-model-might-be-a-bag-of-words-30b1beaef7f8&user=Federico+Bianchi&userId=2aff872fe60e&source=post_page-2aff872fe60e----30b1beaef7f8---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----30b1beaef7f8--------------------------------)
    · 8 分钟阅读 · 2023 年 3 月 21 日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F30b1beaef7f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fyour-vision-language-model-might-be-a-bag-of-words-30b1beaef7f8&user=Federico+Bianchi&userId=2aff872fe60e&source=-----30b1beaef7f8---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F30b1beaef7f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fyour-vision-language-model-might-be-a-bag-of-words-30b1beaef7f8&source=-----30b1beaef7f8---------------------bookmark_footer-----------)![](../Images/727a10ca8f3a50cfa828295de0c20824.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F30b1beaef7f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fyour-vision-language-model-might-be-a-bag-of-words-30b1beaef7f8&source=-----30b1beaef7f8---------------------bookmark_footer-----------)![](../Images/727a10ca8f3a50cfa828295de0c20824.png)'
- en: Photo by [ThisIsEngineering](https://www.pexels.com/photo/code-projected-over-woman-3861969/).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [ThisIsEngineering](https://www.pexels.com/photo/code-projected-over-woman-3861969/)
    提供。
- en: Multimodal AI is the talk of the town. With the recent release of GPT-4, we
    are seeing myriads of new possible applications and future technologies that were
    unthinkable six months ago. Indeed, vision-language models in general are very
    useful for many different tasks. For example, with [CLIP](https://github.com/openai/CLIP)
    you can do zero-shot image classification on unseen datasets, often getting reliable
    performance without the need of training anything.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态人工智能是当前的热门话题。随着 GPT-4 的发布，我们看到了许多过去六个月无法想象的新应用和未来技术。事实上，视觉-语言模型在许多不同的任务中非常有用。例如，通过
    [CLIP](https://github.com/openai/CLIP) 你可以对未见过的数据集进行零样本图像分类，通常能够在不需要训练的情况下获得可靠的表现。
- en: At the same time, vision-language models are also not perfect. Here, we explore
    the limits of these models, highlighting where and why they might fail. This blog
    post is a short/high-level description of our recent paper that will be presented
    as an [ICLR 2023 oral](https://openreview.net/forum?id=KRLUvxh8uaX). If you want
    to take a look at the code, just click [here](https://github.com/mertyg/vision-language-models-are-bows).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，视觉-语言模型也并非完美。这里，我们探索这些模型的局限性，强调它们可能失败的地方和原因。这篇博客文章是我们最近论文的简要/高层次描述，该论文将在
    [ICLR 2023 口头报告](https://openreview.net/forum?id=KRLUvxh8uaX) 中介绍。如果你想查看代码，只需点击
    [这里](https://github.com/mertyg/vision-language-models-are-bows)。
- en: Yuksekgonul, M., Bianchi, F., Kalluri, P., Jurafsky, D., & Zou, J. (2023). When
    and why vision-language models behave like bags-of-words, and what to do about
    it?. ICLR.
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Yuksekgonul, M., Bianchi, F., Kalluri, P., Jurafsky, D., & Zou, J. (2023). 视觉-语言模型何时以及为何表现得像词袋模型，以及该如何应对？ICLR。
- en: Introduction
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: '**What’s a vision-language model?**'
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**什么是视觉-语言模型？**'
- en: Vision-Language models have revolutionized the field by leveraging the synergy
    between visual and linguistic data to perform various tasks. While many vision-language
    models have been introduced in the literature, CLIP is the most well-known and
    widely adopted model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉-语言模型通过利用视觉和语言数据之间的协同效应来执行各种任务，从而彻底改变了该领域。尽管文献中介绍了许多视觉-语言模型，但 CLIP 是最著名和广泛采用的模型。
- en: Through the embedding of images and captions in the same vector space, CLIP
    allows cross-modal reasoning, enabling users to perform tasks such as zero-shot
    image classification, and text-to-image retrieval with good accuracy. CLIP uses
    a contrastive learning approach to learn embeddings for images and captions.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将图像和字幕嵌入到同一向量空间中，CLIP 实现了跨模态推理，使用户能够执行如零-shot 图像分类和文本到图像检索等任务，并且精度良好。CLIP
    采用对比学习方法来学习图像和字幕的嵌入表示。
- en: A short introduction to contrastive learning
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对比学习的简要介绍
- en: Contrastive learning makes it so that CLIP can learn to associate images with
    their corresponding captions by minimizing the distance between them in a shared
    vector space. This approach has proven highly effective, as demonstrated by the
    impressive results achieved by CLIP and other contrastive-based models.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对比学习使 CLIP 能够通过最小化共享向量空间中图像与其对应字幕之间的距离来学习将图像与其字幕关联。这种方法已经被证明非常有效，CLIP 和其他基于对比的模型所取得的令人印象深刻的结果也证实了这一点。
- en: Contrastive loss is used to compare pairs of images and captions in a batch
    and optimizes the model to maximize the similarity between embeddings of matching
    image-text pairs and to decrease the similarity between other pairs in the batch.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对比损失用于比较批次中图像和字幕的配对，并优化模型以最大化匹配图像-文本配对的嵌入之间的相似度，并减少批次中其他配对的相似度。
- en: 'An example of a possible batch and training step is seen in the image below:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图像展示了一个可能的批次和训练步骤的示例：
- en: '**Purple squares** contain embeddings for all the captions, and **green squares**
    contain embeddings for all the images.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**紫色方块**包含所有字幕的嵌入，**绿色方块**包含所有图像的嵌入。'
- en: The squares of the matrix contain the dot product (read as “cosine similarity”,
    since the embeddings are normalized) of all the image embeddings in the batch
    and all the text embeddings.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵的方块包含批次中所有图像嵌入和所有文本嵌入的点积（读取为“余弦相似度”，因为嵌入已经归一化）。
- en: '**Blue squares** contain the dot product between the pairs for which the model
    has to maximize the similarity, the other **white squares** are similarities we
    want to minimize (because each one of those squares contains the similarity of
    a non-matching image-text pair e.g., the image of a cat and the description “my
    vintage chair”).'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**蓝色方块**包含模型需要最大化相似性的配对，其他**白色方块**是我们希望最小化的相似度（因为这些方块包含了不匹配的图像-文本配对的相似度，例如，一张猫的图片和描述“我的古董椅子”）。'
- en: '![](../Images/8dd883eea0defe3843fcbc04009cc470.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8dd883eea0defe3843fcbc04009cc470.png)'
- en: Contrastive pre-training in CLIP. The blue squares are the pairs for which we
    want to optimize the similarity. Image derived from [https://github.com/openai/CLIP](https://github.com/openai/CLIP)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP 中的对比预训练。蓝色方块表示我们希望优化相似性的配对。图片来源于 [https://github.com/openai/CLIP](https://github.com/openai/CLIP)
- en: After training, you should have a meaningful vector space in which you can encode
    images and captions. Once you have embeddings for each image and each text you
    can do many tasks, such as looking at which images are more similar to a caption
    (e.g., finding “dogs on the beach” in your 2017 summer vacation album) or finding
    which label of text is more similar to an image (e.g., you have a large collection
    of images of your dog and your cat and you want to be able to identify which is
    which).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，你应该拥有一个有意义的向量空间，在其中你可以对图像和描述进行编码。一旦你为每个图像和每个文本获取了嵌入，你可以执行许多任务，例如查看哪些图像与描述更相似（例如，在你2017年夏季度假专辑中查找“海滩上的狗”），或找出哪个文本标签与图像更相似（例如，你有一大堆你家狗和猫的图片，你希望能够识别它们）。
- en: Vision-language models such as CLIP have emerged as powerful tools for solving
    complex AI tasks by integrating visual and linguistic information. Their ability
    to embed both types of data in a shared vector space has led to unprecedented
    levels of accuracy and performance in a wide range of applications.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉-语言模型，如CLIP，已成为通过整合视觉和语言信息解决复杂AI任务的强大工具。它们将这两种数据嵌入到共享向量空间中的能力，使其在各种应用中达到前所未有的准确性和性能水平。
- en: '**Do vision-language models understand language?**'
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**视觉-语言模型理解语言吗？**'
- en: Our work tries to take some steps forward to answer this question. There is
    significant debate around whether or how much deep models understand language.
    Here, our goal is to investigate vision-language models and their compositional
    abilities. We first, propose a new dataset, to test compositional understanding;
    this new benchmark is called **ARO** (Attribution, Relations, and Order). We then
    explore why contrastive loss might be limited in this context. Finally, we propose
    a simple but promising solution to this problem.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作试图在回答这个问题上迈出一些步伐。关于深度模型是否或在多大程度上理解语言存在重大争议。在这里，我们的目标是调查视觉-语言模型及其组合能力。首先，我们提出了一个新的数据集，用于测试组合理解；这个新的基准叫做**ARO**（属性、关系和顺序）。然后，我们探讨了对比损失在这种情况下可能受到的限制。最后，我们提出了一个简单但有前景的解决方案。
- en: '**New Benchmark: Attribution, Relations, and Order**'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**新的基准：属性、关系和顺序**'
- en: How well do models like CLIP (and BLIP, a more recent model from Salesforce)
    fare on understanding language?
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 像CLIP（以及来自Salesforce的更新模型BLIP）这样的模型在理解语言方面表现如何？
- en: 'We collect a set of attribute-based compositional captions (e.g., “the red
    door and the standing man”) and relation-based compositional captions (e.g., “the
    horse is eating the grass”) with the respective matching images. We then generate
    alternative false captions like “the grass is eating the horse”. **Can the models
    find the correct caption?** We also explore the effect of shuffling words: **do
    models prefer non-shuffled captions to shuffled ones?**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们收集了一组基于属性的组合描述（例如，“红色的门和站着的人”）和基于关系的组合描述（例如，“马在吃草”），以及相应的匹配图片。然后，我们生成了替代的错误描述，如“草在吃马”。**模型能找到正确的描述吗？**
    我们还探索了打乱词语的效果：**模型是否更喜欢未打乱的描述而非打乱的描述？**
- en: 'The four datasets we create for Attribution, Relation, and Order (**ARO**)
    are illustrated in the following image (Note that Order contains two datasets):'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为属性、关系和顺序（**ARO**）创建的四个数据集在下图中有所说明（注意顺序包含两个数据集）：
- en: '![](../Images/65a7f62ba01da19787ddb9bd7af4ab6c.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/65a7f62ba01da19787ddb9bd7af4ab6c.png)'
- en: The different datasets we created are Relation, Attribution, and Order. For
    each dataset, we show one example of an image and the different captions. Only
    one caption is correct, and the model has to identify the correct one. Image by
    Author.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的不同数据集是关系、属性和顺序。对于每个数据集，我们展示一个图片示例和不同的描述。只有一个描述是正确的，模型必须识别正确的描述。图片由作者提供。
- en: 'Attribution tests understanding of attributes: *“the* ***paved*** *road and
    the* ***white*** *house”* vs *“the* ***white*** *road and the* ***paved*** *house”*.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 属性测试理解属性：*“铺装的* ***道路*** *和* ***白色*** *房子”* 与 *“白色* ***道路*** *和* ***铺装的*** *房子”*。
- en: 'Relation tests understanding of relationships: *“the horse* ***is eating***
    *the grass”* and *“the grass* ***is eating*** *the horse”.*'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关系测试理解关系：*“马* ***在吃*** *草”* 和 *“草* ***在吃*** *马”。*
- en: 'Finally, Order tests the models’ resilience to order shuffling: we randomly
    shuffle captions of standard datasets (e.g., MSCOCO).'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，顺序测试模型对顺序打乱的弹性：我们随机打乱标准数据集的描述（例如，MSCOCO）。
- en: Can vision-language models find the correct caption that matches the image?
    The task seems easy, we expect a model to understand the difference between “the
    horse is eating the grass” and “the grass is eating the horse”, right? I mean,
    who did ever see grass eating something?
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉语言模型能找到匹配图像的正确标题吗？这个任务看似简单，我们期望模型能理解“马在吃草”和“草在吃马”之间的区别，对吧？我意思是，谁见过草吃东西？
- en: 'Well, probably BLIP, since it is not able to understand the difference between
    “the horse is eating the grass” and “the grass is eating the horse”:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，可能是 BLIP，因为它不能理解“马在吃草”和“草在吃马”之间的区别：
- en: '![](../Images/27c4eeaf79fe4a4aac79a00d4447b008.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/27c4eeaf79fe4a4aac79a00d4447b008.png)'
- en: BLIP does not understand the difference between “the grass is eating the horse”
    and “the horse is eating the grass”. Image by Author with elements from the Visual
    Genome dataset. Image by Author.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: BLIP 不理解“草在吃马”和“马在吃草”之间的区别。图像由作者提供，包含来自 Visual Genome 数据集的元素。图像由作者提供。
- en: Let’s now see some results. Few models go largely above chance on relation understanding
    (e.g., eating). CLIP is marginally above chance in Attribution and Relation. This
    actually suggests that there is a problem with vision-language models.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看一些结果。少数模型在关系理解（例如，吃）上显著高于偶然水平。CLIP 在归因和关系上仅略高于偶然水平。这实际上表明视觉语言模型存在问题。
- en: '![](../Images/94a31fb70a8209dd389c6ceb5138452d.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94a31fb70a8209dd389c6ceb5138452d.png)'
- en: The performance of different models on the Attribution, Relation, and Order
    (for Flick30k) benchmark. You can see CLIP, BLIP, and other SoTA models. Image
    by Author.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 不同模型在归因、关系和顺序（针对 Flick30k）基准上的表现。你可以看到 CLIP、BLIP 和其他最先进的模型。图像由作者提供。
- en: A Critique of Retrieval and Contrastive Loss
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检索和对比损失的批评
- en: One of the main results of this work is that probably we need something more
    than the standard contrastive loss to learn **language**; but why?
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作的主要结果之一是我们可能需要超越标准对比损失来学习**语言**；但为什么呢？
- en: 'Let’s start from the top: vision-language models are often evaluated on retrieval
    tasks: take a caption and find the image it maps to. If you look at the datasets
    used to evaluate these models (e.g., MSCOCO, Flickr30K), you will see that they
    generally contain images that are described with captions that require compositional
    capabilities to be understood (e.g., “the orange cat is on the red table”). So
    why is it that the models do not learn compositional understanding if the captions
    are complex?'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 从最上面开始：视觉语言模型通常在检索任务上进行评估：获取一个标题并找到它对应的图像。如果你查看用于评估这些模型的数据集（例如，MSCOCO、Flickr30K），你会发现它们通常包含需要组成能力才能理解的标题描述的图像（例如，“橙色的猫在红色的桌子上”）。那么，为什么模型在标题复杂的情况下却没有学会组成性理解呢？
- en: '**Spoiler**: you don’t necessarily need compositional understanding to perform
    retrieval on these datasets.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**剧透**：你不一定需要组成性理解来在这些数据集上进行检索。'
- en: We tried to better understand the problem and we tested the performance of the
    models on retrieval when we shuffle the order of words in the caption. Can we
    find the correct image of the caption “books the looking at people are”? If the
    answer is yes, it means that order information is not required to find the correct
    image.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们试图更好地理解这个问题，并测试了模型在重新排列标题中的单词顺序后的表现。我们能找到标题为“books the looking at people are”的正确图像吗？如果答案是肯定的，这意味着查找正确图像时不需要顺序信息。
- en: '![](../Images/151199634d6caba0c53a41c8676f74b6.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/151199634d6caba0c53a41c8676f74b6.png)'
- en: The task we tested models on is retrieval with shuffled captions. Even if we
    shuffle the captions, the models can correctly find the respective images (and
    vice-versa). This suggests that the retrieval task might be too easy. Image by
    Author.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们测试的任务是用打乱的标题进行检索。即使我们打乱标题，模型仍能正确找到相应的图像（反之亦然）。这表明检索任务可能太简单。图像由作者提供。
- en: 'We tested the different shuffling procedures and the answer is yes: even with
    different shuffling techniques, retrieval performance is largely **not** impacted.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们测试了不同的打乱程序，答案是肯定的：即使使用不同的打乱技术，检索性能基本上**不**受影响。
- en: 'Let’s say this one more time: vision-language models achieve high performance
    on retrieval — on these datasets — even when order information is inaccessible.
    These models might behave like a bag of words, where order doesn’t matter: If
    models do not need to understand **word** **order** to perform well on retrieval,
    what are we actually measuring with retrieval?'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 再说一遍：视觉-语言模型在检索任务中表现优异——在这些数据集上——即使顺序信息不可得。这些模型可能像词袋一样运作，其中顺序并不重要：如果模型在检索中表现良好而不需要理解**词**
    **顺序**，那么我们到底在检索中测量什么呢？
- en: '**What to do about it?**'
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**该怎么办？**'
- en: 'Well, now that we know that there is a problem, we might want to look for a
    solution. The easiest one is the following: make CLIP understand that “the cat
    is on the table” is different from “the table is on the cat”.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们知道存在问题，我们可能需要寻找解决方案。最简单的方案如下：让 CLIP 理解“猫在桌子上”与“桌子在猫身上”是不同的。
- en: 'Indeed, one thing we propose is to improve CLIP training by adding hard negatives
    that are specifically made to tackle this issue. This is a very easy and effective
    solution: it requires a very minor edit on the original CLIP loss that does not
    compromise the general performance (with some caveats you can read in the paper).
    We called this version of CLIP, NegCLIP.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，我们提议通过增加专门用于解决此问题的困难负样本来改进 CLIP 的训练。这是一个非常简单有效的解决方案：它只需对原始 CLIP 损失做非常小的编辑，不会影响整体性能（具体细节可以在论文中阅读）。我们称这种
    CLIP 版本为 NegCLIP。
- en: '![](../Images/dd9b786195ee8d59a7fea6bec276aea3.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd9b786195ee8d59a7fea6bec276aea3.png)'
- en: Introducing hard negatives in CLIP. We add both image and text hard negatives.
    Image by Author.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CLIP 中引入困难负样本。我们添加了图像和文本的困难负样本。图片来源于作者。
- en: Basically, we are asking NegCLIP to put the image of the black cat close to
    the sentence “a black cat sitting on a desk” but far from the sentence “a black
    desk sitting on a cat”. The latter is automatically generated through the use
    of POS tagging.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们要求 NegCLIP 将黑猫的图像与“桌子上坐着一只黑猫”这句话靠近，但与“猫上坐着一张黑桌子”这句话远离。后者是通过使用 POS 标记自动生成的。
- en: The effect of this fix is that it can actually increase the performance on the
    ARO benchmark without hurting retrieval performance or performance over downstream
    tasks like retrieval and classification. See the following figure for the results
    on different benchmarks (details about this are in the paper).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这个修复的效果是，它实际上可以提高 ARO 基准上的性能，而不会影响检索性能或其他下游任务如检索和分类的表现。有关不同基准的结果，请参见下图（详细信息请见论文）。
- en: '![](../Images/38f67f6f2c47d4c3f8e1a3453dc4d163.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38f67f6f2c47d4c3f8e1a3453dc4d163.png)'
- en: NegCLIP vs CLIP on different benchmarks. Blue benchmarks are the ones we introduced,
    and green benchmarks are from the literature. Image by Author.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: NegCLIP 与 CLIP 在不同基准上的比较。蓝色基准是我们介绍的，绿色基准来自文献。图片来源于作者。
- en: You can see there is a huge improvement over ARO benchmarks and marginal improvement
    or similar performance on the other downstream tasks.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，在 ARO 基准测试中有了显著的改进，而在其他下游任务中则有边际改进或类似性能。
- en: Code!
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代码！
- en: '[Mert](https://mertyg.github.io/) (the main author of the paper) has done a
    wonderful job in [creating a small library](https://github.com/mertyg/vision-language-models-are-bows)
    to test vision-language models. You can use his code to replicate our results
    or to run experiments with new models.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[Mert](https://mertyg.github.io/)（论文的主要作者）在 [创建一个小型库](https://github.com/mertyg/vision-language-models-are-bows)
    来测试视觉-语言模型方面做了出色的工作。你可以使用他的代码来复制我们的结果或进行新模型的实验。'
- en: A couple of lines of python are all you need to download the datasets and start
    running!
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 只需几行 Python 代码，你就可以下载数据集并开始运行！
- en: '[PRE0]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We also release the implementation of NegCLIP (which is actually a fork of OpenCLIP).
    See the code [here](https://github.com/vinid/neg_clip).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还发布了 NegCLIP 的实现（实际上是 OpenCLIP 的一个分支）。查看代码 [这里](https://github.com/vinid/neg_clip)。
- en: Farewell
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 告别
- en: Thanks for reading! I hope this was interesting. Vision-Language models can
    already do many things and we can’t wait to see what future models, such as GPT4
    can do!
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！希望这篇文章有趣。视觉-语言模型已经能做很多事情，我们迫不及待想看看未来的模型，例如 GPT4 能做些什么！
- en: Acknowledgements
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: Thanks to [Mert](https://mertyg.github.io/) for all the suggestions!
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢 [Mert](https://mertyg.github.io/) 提供的所有建议！
- en: Related Things
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关内容
- en: If you want to know more about CLIP, I have written a blog post that goes into
    a bit more in detail.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于 CLIP 的信息，我写了一篇博客文章，详细介绍了相关内容。
- en: '[](/how-to-train-your-clip-45a451dcd303?source=post_page-----30b1beaef7f8--------------------------------)
    [## How to Train your CLIP'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 如何训练你的 CLIP]'
- en: Introduction to CLIP and to how we fine-tuned it for the Italian Language during
    the HuggingFace Community Week.
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CLIP 介绍以及我们在 HuggingFace 社区周期间如何为意大利语言进行微调。
- en: towardsdatascience.com](/how-to-train-your-clip-45a451dcd303?source=post_page-----30b1beaef7f8--------------------------------)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/how-to-train-your-clip-45a451dcd303?source=post_page-----30b1beaef7f8--------------------------------)'
- en: I have also fine-tuned CLIP on fashion data. Here’s a blog post you might be
    interested in!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我还对 CLIP 进行了时尚数据的微调。这里有一篇你可能感兴趣的博客文章！
- en: '[](/teaching-clip-some-fashion-3005ac3fdcc3?source=post_page-----30b1beaef7f8--------------------------------)
    [## Teaching CLIP Some Fashion'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 教授 CLIP 一些时尚]'
- en: Training FashionCLIP, a domain-specific CLIP model for Fashion
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练 FashionCLIP，一种针对时尚领域的 CLIP 模型
- en: towardsdatascience.com](/teaching-clip-some-fashion-3005ac3fdcc3?source=post_page-----30b1beaef7f8--------------------------------)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/teaching-clip-some-fashion-3005ac3fdcc3?source=post_page-----30b1beaef7f8--------------------------------)'
