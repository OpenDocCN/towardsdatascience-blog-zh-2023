# SDXL 1.0 的到来

> 原文：[https://towardsdatascience.com/the-arrival-of-sdxl-1-0-4e739d5cc6c7?source=collection_archive---------3-----------------------#2023-08-02](https://towardsdatascience.com/the-arrival-of-sdxl-1-0-4e739d5cc6c7?source=collection_archive---------3-----------------------#2023-08-02)

## 介绍 SDXL 1.0：理解扩散模型

[](https://ertugruldemir.medium.com/?source=post_page-----4e739d5cc6c7--------------------------------)[![Ertuğrul Demir](../Images/bc35d3fcfade72c8a4e72d1fd952a73c.png)](https://ertugruldemir.medium.com/?source=post_page-----4e739d5cc6c7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4e739d5cc6c7--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4e739d5cc6c7--------------------------------) [Ertuğrul Demir](https://ertugruldemir.medium.com/?source=post_page-----4e739d5cc6c7--------------------------------)

·

[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc2ba54d15a24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-arrival-of-sdxl-1-0-4e739d5cc6c7&user=Ertu%C4%9Frul+Demir&userId=c2ba54d15a24&source=post_page-c2ba54d15a24----4e739d5cc6c7---------------------post_header-----------) 发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4e739d5cc6c7--------------------------------) ·12分钟阅读·2023年8月2日

--

[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e739d5cc6c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-arrival-of-sdxl-1-0-4e739d5cc6c7&source=-----4e739d5cc6c7---------------------bookmark_footer-----------)![](../Images/213e95221db2cd6cab8cee362df023f1.png)

一个可爱的小机器人学习如何绘画——使用 SDXL 1.0 创建

在快速发展的机器学习领域，新的模型和技术几乎每天都在不断涌现，保持更新并做出明智的选择变得异常艰难。今天，我们将注意力转向 SDXL 1.0，这是一种文本到图像生成模型，无疑在该领域引起了相当大的兴趣。

SDXL 1.0，全称为“稳定扩散 XL”，被誉为一种潜在的文本到图像扩散模型，声称在多个方面超过了其前身。在接下来的章节中，我们将深入探讨这些说法，并仔细审视新的改进。

最值得注意的是，SDXL是一个开源模型，解决了生成模型领域的一个主要问题。虽然黑箱模型因其先进性而获得认可，但其架构的不透明性阻碍了对其性能的全面评估和验证，限制了更广泛的社区参与。

在这篇文章中，我们将详细探索这个有前景的模型，检查其能力、构建模块，并与之前的稳定扩散模型进行有益的比较。我的目标是提供清晰的理解，而不深入技术复杂性，使这篇文章对所有人都具有吸引力和可读性。让我们开始吧！

# 理解稳定扩散：揭示文本到图像生成的魔力

> 如果你对稳定扩散的工作原理感到自信，或者不涉及技术部分，可以跳过本章。

稳定扩散，这一突破性的深度学习文本到图像模型，于2022年发布时在AI界引起了震动，利用了前沿的扩散技术。

这一重要发展代表了AI图像生成的显著进步，有可能扩大高性能模型对更广泛受众的可及性。将普通文本描述转换为复杂视觉输出的吸引人能力，引起了体验过的人们的关注。稳定扩散展现了生成高质量图像的能力，同时也表现出显著的速度和效率，从而提高了AI生成艺术创作的可及性。

稳定扩散的训练涉及了像LAION-5B这样的大型公开数据集，利用广泛的带注释图像来提升其艺术能力。然而，推动其进展的一个关键方面在于社区的积极参与，提供的宝贵反馈推动了模型的持续发展，并随着时间的推移增强了其能力。

## **它是如何工作的？**

让我们从稳定扩散模型的主要构建模块开始，了解如何分别训练和进行预测。

**U-Net与扩散过程的本质：**

为了使用计算机视觉模型生成图像，我们超越了依赖标记数据（如分类、检测或分割）的传统方法。在稳定扩散的领域，目标是使模型能够学习图像本身的复杂细节，以一种被称为“扩散”的创新方法捕捉复杂的上下文。

扩散过程分为两个不同的阶段：

+   在第一部分，我们获取一张图像并引入一定量的随机噪声。这一步被称为**正向扩散**。

+   在第二部分，我们的目标是去噪图像并重建原始内容。这个过程被称为**反向扩散**。

![](../Images/4df88aec58451e651b93b19350e58556.png)

按步骤添加噪声。来源: [通过去噪扩散GAN解决生成学习三难问题](https://arxiv.org/abs/2112.07804)

第一部分涉及在每个时间步**t**向输入图像添加高斯噪声，较为直接。然而，第二阶段则存在挑战，因为直接计算原始图像是不可行的。为了克服这一障碍，我们使用了神经网络，这就是巧妙的**U-Net**发挥作用的地方。

利用**U-Net**，我们训练我们的模型来预测在时间步t的给定随机噪声图像中的噪声，并计算预测噪声与实际噪声之间的损失。通过使用足够大的数据集和多个噪声步骤，模型获得了对噪声模式进行有根据预测的能力。这个训练过的U-Net模型对于从给定噪声生成图像的近似重建也证明是非常宝贵的。

![](../Images/68fec94627024f8f0757b8d9a6a88a69.png)

反向扩散。来源: [去噪扩散概率模型](https://arxiv.org/abs/2006.11239)

如果你对基本概率和计算机视觉模型熟悉，这个过程是相对直接的。然而，还有一个问题值得注意。训练数百万张添加了噪声的图像并重建它们将非常耗时，并消耗计算能力。为了应对这一挑战，研究人员重新审视了一种知名架构：**自编码器**。由于我们已经采用了类似的方法，如使用U-Net结合转置卷积和残差块，这些元素在自编码器中也将发挥重要作用。

使用自编码器，可以将数据“**编码**”到一个更小的“**潜在**”空间中，然后将其“**解码**”回原始空间。实际上，这就是为什么原始稳定扩散论文被称为**潜在扩散**。这使我们能够有效地将大图像压缩到较低的维度。

![](../Images/3e9a2c8f30dc10b0825be665c2a3b99f.png)

简单的自编码器表示。图像由作者提供。

正向和反向扩散操作现在将在显著较小的潜在空间中进行，从而减少了内存需求并显著加快了处理速度。

我们几乎完成了著名的“稳定扩散”架构；唯一剩下的部分是**条件化**。通常，这个方面是通过**文本编码器**实现的，尽管也存在使用图像作为条件化的方法，如**ControlNet**，但这超出了本文的范围。**文本条件化**在基于文本提示生成图像中扮演着至关重要的角色，这也是稳定扩散模型的真正魔力所在。

为此，我们可以训练像 **BERT** 或 **CLIP** 这样的文本嵌入模型，使用带有标题的图像，并将标记嵌入向量作为条件输入。通过采用 **交叉注意力** 机制（查询、键和值），我们可以将条件文本嵌入映射到 U-Net 残差块中。因此，我们可以在训练过程中将图像标题与图像本身一起纳入，并有效地基于提供的文本调整图像生成。

![](../Images/aafb3b7d7cfa3cff75c2e3e76a6c368f.png)

潜在扩散模型 — 来源：[论文](https://arxiv.org/pdf/2112.10752.pdf)

现在你已经了解了 Stable Diffusion 模型的构建块，凭借这些知识，我们可以更好地比较之前的 Stable Diffusion 模型，并对它们的优缺点做出更为全面的评估。

# SDXL 有什么新变化？

现在我们已经掌握了 SD 模型的基础知识，让我们深入研究 SDXL 论文，揭示这一新模型引入的变革性变化。总的来说，SDXL 提出了以下改进：

+   **增加的 U-Net 参数数量：** SDXL 通过引入更多的 U-Net 参数来增强模型能力，从而实现更复杂的图像生成。

+   **变换器块的异质分布：** 与之前模型中的均匀分布（[1,1,1,1]）不同，SDXL 采用了异质分布（[0,2,4]），引入了优化和改进的学习能力。

+   **增强的文本条件编码器：** SDXL 利用更大的文本条件编码器 OpenCLIP ViT-bigG，有效地将文本信息融入图像生成过程中。

+   **附加文本编码器：** 该模型采用了附加文本编码器 CLIP ViT-L，连接其输出，丰富了条件处理过程中的文本特征。

+   **引入“尺寸条件”：** 一种名为“尺寸条件”的新型调节器将原始训练图像的宽度和高度作为条件输入，使模型能够根据与尺寸相关的提示调整图像生成。

+   **“裁剪条件”参数：** SDXL 引入了“裁剪条件”参数，将图像裁剪坐标作为条件输入。

+   **“多方面条件”参数：** 通过纳入条件的桶大小，“多方面条件”参数使 SDXL 能够适应不同的纵横比。

+   **专用精炼器模型：** SDXL 引入了第二个专门处理高质量、高分辨率数据的 SD 模型；本质上，它是一个 img2img 模型，能够有效捕捉复杂的局部细节。

现在，让我们更详细地查看这些新增功能与之前稳定扩散模型的比较。

**Stability.ai 官方比较：**

让我们首先查看稳定性.ai的官方比较，由作者呈现。这一比较提供了有关SDXL与Stable Diffusion之间用户偏好的有价值的见解。然而，我们必须谨慎对待这些发现……

![](../Images/7fb2471545e027cb6e0423d8ad515bc8.png)

比较SDXL与之前模型的用户偏好。来源：[论文](https://arxiv.org/abs/2307.01952)

这项研究表明，参与者选择了SDXL模型而非之前的SD 1.5和2.1模型。特别是，添加了Refiner的SDXL模型实现了48.44%的胜率。需要注意的是，尽管这一结果在统计上具有显著性，但我们还必须考虑由人为因素引入的固有偏差以及生成模型的随机性。

**与最先进黑箱模型的性能比较：**

![](../Images/82839287979c47c2c1dce5bfa0e41e9a.png)

来源：[论文](https://arxiv.org/abs/2307.01952)

目前，Midjourney在用户中非常受欢迎，有些人认为它目前是最先进的解决方案。根据官方调查，SDXL在“食品和饮料”和“动物”等类别中表现出较高的偏好率。然而，在“插图”和“抽象”等其他类别中，用户仍然偏好Midjourney V5.1。

![](../Images/e728916713a1086a5405b53311283ef8.png)

来源：SDXL: [提升潜在扩散模型以实现高分辨率图像合成](https://arxiv.org/abs/2307.01952)

再次观察复杂提示的情况，我们看到类似的模式。论文声明它们在10个复杂主题中的7个中受到青睐。然而，如果不了解具体的提示内容，就很难得出结论。此外，关于Midjourney中提示编码器的信息缺乏进一步复杂化了问题，只有时间才能揭示真正的偏好。

**U-Net参数数量**

如前所述，U-Net模型在Stable Diffusion中发挥着至关重要的作用，帮助从给定的噪声中重建图像。在SDXL中，作者通过引入一个比之前版本SD大得多的U-Net模型做出了显著改进，总计2.6B的U-Net参数，相较于其前身的~860M。

![](../Images/873ccf66cd22308b7aecfe4f177c87fc.png)

图片由作者提供。

尽管拥有更多参数可能初看起来很有前景，但必须考虑复杂性与质量之间的权衡。随着参数数量的增加，系统在训练和生成时的需求也会增加。虽然对于产品质量的最终结论还为时尚早，但复杂性与质量之间的权衡已经显而易见。

**文本编码器参数数量**

确实，当审视文本编码器的总参数数量时，我们观察到SDXL 1.0相比于其前身有显著增加。SDXL中引入了两个文本条件器，而之前版本中只有一个，这使得文本编码器的参数数量大幅增长。这一扩展使得SDXL能够利用更大容量的文本信息。

SDXL使用具有6.947亿参数的OpenCLIP G/14文本编码器，相比之下CLIP L/14具有1.2365亿参数，总计超过8亿参数。这代表了与其前身相比的重大飞跃。

![](../Images/046b34d74d2f5620fcc5117322be9f75.png)

图片由作者提供。

再次，使用更大和多个文本编码器最初可能看起来很有吸引力，但它引入的额外复杂性可能会产生不利影响。考虑到一个场景，其中你正在用自己的数据对SDXL模型进行微调。在这种情况下，确定最佳参数比以前的SD模型更具挑战性，因为找到两个编码器的超参数“甜蜜点”变得更加难以捉摸。

**工作流程**

确实，SDXL中的“XL”一词表明了其相比于之前的SD模型在规模和复杂性上的扩展。SDXL在多个方面超越了其前身，拥有更多的参数，包括两个文本编码器和两个U-Net模型——基础模型和细化模型，后者本质上作为图像到图像模型使用。自然，这种SDXL管道的复杂性增加：

![](../Images/5753c3a3f33cd87e5a94f1fe4705ba86.png)

通常的SD 1.5生成管道。图片由作者提供

![](../Images/4548ea3d34848d640c7881576aa6d8b8.png)

SDXL生成管道。图片由作者提供

## **SDXL实践**

SDXL的模型权重已经正式发布，并可以作为Python脚本自由使用，感谢Hugging Face的diffusers库。此外，还有一个用户友好的GUI选项，称为ComfyUI。这个GUI提供了一个高度可定制的基于节点的界面，允许用户直观地放置Stable Diffusion模型的构建块并进行可视化连接。

使用这些现成的实现，用户可以将SDXL无缝集成到他们的项目中，从而能够利用这一前沿的潜在文本到图像扩散模型的强大功能。

Diffusers使用的基本示例

![](../Images/8ea16dcf10d62e35673e580dcb27c5fa.png)

ComfyUI工作流程管道示例。图片由作者提供。

# **SDXL的当前状态和个人体验**

尽管SDXL中的新功能和附加功能看起来很有前景，但一些经过微调的SD 1.5模型仍然提供更好的结果。这一结果主要归因于蓬勃发展的社区的巨大支持——这是开放源代码方法带来的优势。

在其初始模型阶段，SDXL 相比于 1.5 有了改进，我相信在持续的社区支持下，其性能未来只会更强。然而，需要认识到，随着模型的复杂性增加，使用和微调它们在计算上需要更多资源。但目前还不需要过于担忧……

**LoRA**（局部秩自适应分解）在**大型语言模型的微调**中越来越受欢迎。这种方法涉及将秩分解权重矩阵对添加到现有权重中，并仅训练这些新添加的权重。因此，训练变得更快且计算上更高效。LoRA 的引入预计将为社区创造出更好的定制版本铺平道路。值得注意的是，SDXL 已经完全支持 LoRA。

尽管有积极的发展，但值得注意的是，SDXL 仍然面临一些常见的 Stable Diffusion 缺陷，正如作者们正式承认的那样：

+   该模型未能达到完美的照片现实主义。

+   该模型无法渲染清晰的文本。

+   该模型在处理涉及组合性的更复杂任务时表现不佳，例如渲染对应于“一个红色立方体在一个蓝色球体上方”的图像。

+   脸部和一般人物可能无法正确生成。

+   模型的自动编码部分是有损的。

**个人观察**

就个人而言，在实验 SDXL 模型时，我仍然发现自己在某些情况下更倾向于使用之前的 SD 1.5 社区检查点。在社区支持的几个月后，找到适合特定需求的微调模型（如照片现实主义或更卡通风格）相对容易。然而，由于对计算能力的高需求，目前难以找到针对 SDXL 的特定微调模型。尽管如此，SDXL 的基础模型在图像质量和分辨率方面似乎优于 SD 1.5 或 2.1 的基础模型，随着进一步的优化和时间，这种情况在不久的将来可能会发生变化。

值得注意的是，随着模型规模的增大，一些用户报告在日常笔记本电脑或 PC 上运行模型时遇到困难，这令人遗憾。我希望大型语言模型中常用的量化技术也能在这一领域找到应用。

此外，由于文本编码器的变化，我的常用提示在 SDXL 上不再产生令人满意的结果。尽管 SDXL 的开发者声称在 SDXL 上提示更容易，但我自己尚未找到合适的方法。适应新的提示风格可能需要一些时间，尤其是对于那些来自早期版本的人。

# **结论**

-   在我们的文章中，我们发现了 Stable Diffusion XL 的能力，这一模型能够将普通文本描述转化为复杂的视觉表现。我们发现 SDXL 的开源特性及其解决黑箱模型相关问题的方法为其广泛吸引力做出了贡献，使其能够接触到更广泛的受众。

-   SDXL 通过增加参数数量和额外功能，证明了自己是一个“XL”模型，与前辈相比，具备了更高的复杂性。

-   实践中实施 SDXL 已变得更加容易，官方发布的模型权重可以作为 Python 脚本从 huggingface 自由获取，并提供了用户友好的 ComfyUI GUI 选项。

-   尽管 SDXL 展示了巨大的潜力，但通向完美的道路仍在继续。一些精细调整过的 SD 1.5 模型在某些情况下仍优于 SDXL，这得益于活跃的社区支持。然而，凭借积极的参与和支持，我相信 SDXL 将继续随着时间的推移不断演进和改进。

-   然而，需要承认的是，像其前辈一样，SDXL 确实存在一些局限性。实现完美的照片现实主义、渲染清晰的文本、处理组合挑战以及准确生成面孔和人物是模型需要改进的领域。

-   总结来说，SDXL 1.0 代表了文本到图像生成的重大飞跃，释放了 AI 的创作潜力，并推动了可能性的边界。随着 AI 社区的持续合作和创新，我们可以期待在迷人的 SD 模型及其未来的发展中见证更多令人惊叹的进展。

# 参考文献：

+   [去噪扩散概率模型 — Jonathan Ho, Ajay Jain, Pieter Abbeel, 2020](https://arxiv.org/abs/2006.11239)

+   [高分辨率图像合成与潜在扩散模型 — Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer, 2021](https://arxiv.org/abs/2112.10752)

+   [SDXL: 改进潜在扩散模型以实现高分辨率图像合成 — Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, Robin Rombach, 2023](https://arxiv.org/abs/2307.01952)
