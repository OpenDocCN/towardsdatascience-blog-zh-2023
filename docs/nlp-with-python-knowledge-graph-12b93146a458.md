# ä½¿ç”¨ Python è¿›è¡Œ NLPï¼šçŸ¥è¯†å›¾è°±

> åŸæ–‡ï¼š[`towardsdatascience.com/nlp-with-python-knowledge-graph-12b93146a458`](https://towardsdatascience.com/nlp-with-python-knowledge-graph-12b93146a458)

![](img/bf7a59d41b19f964ce71b34753c515bf.png)

ä½œè€…æä¾›çš„å›¾ç‰‡

## SpaCyã€å¥å­åˆ†å‰²ã€è¯æ€§æ ‡æ³¨ã€D**ependency parsing**ã€å‘½åå®ä½“è¯†åˆ«ç­‰â€¦â€¦

[](https://maurodp.medium.com/?source=post_page-----12b93146a458--------------------------------)![Mauro Di Pietro](https://maurodp.medium.com/?source=post_page-----12b93146a458--------------------------------)[](https://towardsdatascience.com/?source=post_page-----12b93146a458--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page-----12b93146a458--------------------------------) [Mauro Di Pietro](https://maurodp.medium.com/?source=post_page-----12b93146a458--------------------------------)

Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----12b93146a458--------------------------------) Â·14 åˆ†é’Ÿé˜…è¯»Â·2023 å¹´ 4 æœˆ 19 æ—¥

--

## æ‘˜è¦

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ Python å’Œè‡ªç„¶è¯­è¨€å¤„ç†æ„å»ºçŸ¥è¯†å›¾è°±ã€‚

![](img/321c5d15bba2bcd22a3328c17f2827ac.png)

ç…§ç‰‡ç”±[Moritz Kindler](https://unsplash.com/@moritz_photography?utm_source=medium&utm_medium=referral)æä¾›ï¼Œ[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

[**ç½‘ç»œå›¾**](https://en.wikipedia.org/wiki/Graph_theory)æ˜¯ä¸€ç§æ•°å­¦ç»“æ„ï¼Œç”¨äºå±•ç¤ºç‚¹ä¹‹é—´çš„å…³ç³»ï¼Œå¯ä»¥é€šè¿‡æ— å‘å›¾/æœ‰å‘å›¾ç»“æ„è¿›è¡Œå¯è§†åŒ–ã€‚è¿™æ˜¯ä¸€ç§æ˜ å°„è¿æ¥èŠ‚ç‚¹çš„æ•°æ®åº“å½¢å¼ã€‚

[**çŸ¥è¯†åº“**](https://en.wikipedia.org/wiki/Knowledge_base)æ˜¯æ¥è‡ªä¸åŒæ¥æºçš„ä¿¡æ¯çš„ç»Ÿä¸€å­˜å‚¨åº“ï¼Œå¦‚*ç»´åŸºç™¾ç§‘*ã€‚

[**çŸ¥è¯†å›¾è°±**](https://en.wikipedia.org/wiki/Knowledge_graph)æ˜¯ä¸€ç§ä½¿ç”¨å›¾ç»“æ„æ•°æ®æ¨¡å‹çš„çŸ¥è¯†åº“ã€‚ç®€å•æ¥è¯´ï¼Œå®ƒæ˜¯ä¸€ç§ç½‘ç»œå›¾ï¼Œå±•ç¤ºäº†ç°å®ä¸–ç•Œå®ä½“ã€äº‹å®ã€æ¦‚å¿µå’Œäº‹ä»¶ä¹‹é—´çš„å®šæ€§å…³ç³»ã€‚â€œçŸ¥è¯†å›¾è°±â€ä¸€è¯é¦–æ¬¡ç”±*è°·æ­Œ*åœ¨ 2012 å¹´ä½¿ç”¨ï¼Œä»¥ä»‹ç»[ä»–ä»¬çš„æ¨¡å‹](https://en.wikipedia.org/wiki/Google_Knowledge_Graph)ã€‚

![](img/1c6b4397f08ef1f66cf0e7624bae5507.png)

ä½œè€…æä¾›çš„å›¾ç‰‡

ç›®å‰ï¼Œå¤§å¤šæ•°å…¬å¸æ­£åœ¨æ„å»º[æ•°æ®æ¹–](https://en.wikipedia.org/wiki/Data_lake)ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸­å¤®æ•°æ®åº“ï¼Œç”¨äºå­˜å‚¨ä»ä¸åŒæ¥æºè·å–çš„å„ç§åŸå§‹æ•°æ®ï¼ˆå³ç»“æ„åŒ–å’Œéç»“æ„åŒ–æ•°æ®ï¼‰ã€‚å› æ­¤ï¼Œäººä»¬éœ€è¦å·¥å…·æ¥ç†è§£è¿™äº›ä¸åŒä¿¡æ¯ç‰‡æ®µã€‚çŸ¥è¯†å›¾è°±å˜å¾—è¶Šæ¥è¶Šæµè¡Œï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥ç®€åŒ–å¯¹å¤§å‹æ•°æ®é›†çš„æ¢ç´¢å’Œæ´å¯Ÿå‘ç°ã€‚æ¢å¥è¯è¯´ï¼ŒçŸ¥è¯†å›¾è°±å°†æ•°æ®å’Œç›¸å…³çš„å…ƒæ•°æ®è¿æ¥èµ·æ¥ï¼Œå› æ­¤å¯ä»¥ç”¨æ¥æ„å»ºç»„ç»‡ä¿¡æ¯èµ„äº§çš„å…¨é¢è¡¨ç¤ºã€‚ä¾‹å¦‚ï¼ŒçŸ¥è¯†å›¾è°±å¯ä»¥æ›¿ä»£ä½ éœ€è¦æµè§ˆçš„æ‰€æœ‰æ–‡æ¡£å †ï¼Œä»¥ä¾¿æ‰¾åˆ°æŸä¸€ç‰¹å®šä¿¡æ¯ã€‚

çŸ¥è¯†å›¾è°±è¢«è®¤ä¸ºæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„ä¸€éƒ¨åˆ†ï¼Œå› ä¸ºä¸ºäº†æ„å»ºâ€œçŸ¥è¯†â€ï¼Œå¿…é¡»ç»è¿‡ä¸€ä¸ªå«åšâ€œ**è¯­ä¹‰ä¸°å¯ŒåŒ–**â€çš„è¿‡ç¨‹ã€‚ç”±äºæ²¡æœ‰äººæ„¿æ„æ‰‹åŠ¨æ‰§è¡Œè¿™ä¸ªè¿‡ç¨‹ï¼Œæˆ‘ä»¬éœ€è¦æœºå™¨å’Œ NLP ç®—æ³•æ¥ä¸ºæˆ‘ä»¬å®Œæˆè¿™é¡¹ä»»åŠ¡ã€‚

æˆ‘å°†å±•ç¤ºä¸€äº›æœ‰ç”¨çš„ Python ä»£ç ï¼Œè¿™äº›ä»£ç å¯ä»¥è½»æ¾åœ°åº”ç”¨äºå…¶ä»–ç±»ä¼¼çš„æƒ…å†µï¼ˆåªéœ€å¤åˆ¶ã€ç²˜è´´ã€è¿è¡Œï¼‰ï¼Œå¹¶é€è¡Œè®²è§£ä»£ç çš„æ³¨é‡Šï¼Œä»¥ä¾¿ä½ å¯ä»¥å¤åˆ¶è¿™ä¸ªç¤ºä¾‹ï¼ˆå®Œæ•´ä»£ç çš„é“¾æ¥å¦‚ä¸‹ï¼‰ã€‚

[## DataScience_ArtificialIntelligence_Utils/example_knowledge_graph.ipynb at master Â·â€¦](https://github.com/mdipietro09/DataScience_ArtificialIntelligence_Utils/blob/master/natural_language_processing/example_knowledge_graph.ipynb?source=post_page-----12b93146a458--------------------------------)

### ä½ ç°åœ¨æ— æ³•æ‰§è¡Œè¯¥æ“ä½œã€‚ä½ åœ¨å¦ä¸€ä¸ªæ ‡ç­¾é¡µæˆ–çª—å£ä¸­ç™»å½•äº†ã€‚åœ¨å¦ä¸€ä¸ªæ ‡ç­¾é¡µä¸­æ³¨é”€äº†â€¦

[github.com](https://github.com/mdipietro09/DataScience_ArtificialIntelligence_Utils/blob/master/natural_language_processing/example_knowledge_graph.ipynb?source=post_page-----12b93146a458--------------------------------)

æˆ‘å°†è§£æç»´åŸºç™¾ç§‘å¹¶æå–ä¸€ä¸ªé¡µé¢ï¼Œè¯¥é¡µé¢å°†ç”¨ä½œæœ¬æ•™ç¨‹çš„æ•°æ®é›†ï¼ˆé“¾æ¥å¦‚ä¸‹ï¼‰ã€‚

[## ä¿„ä¹Œæˆ˜äº‰ - ç»´åŸºç™¾ç§‘](https://en.wikipedia.org/wiki/Russo-Ukrainian_War?source=post_page-----12b93146a458--------------------------------)

### ä¿„ä¹Œæˆ˜äº‰æ˜¯ä¿„ç½—æ–¯åŠå…¶æ”¯æŒçš„åˆ†è£‚ä¸»ä¹‰è€…ä¹‹é—´æ­£åœ¨è¿›è¡Œçš„å›½é™…å†²çªâ€¦

[ç»´åŸºç™¾ç§‘](https://en.wikipedia.org/wiki/Russo-Ukrainian_War?source=post_page-----12b93146a458--------------------------------)

å…·ä½“æ¥è¯´ï¼Œæˆ‘å°†ä»‹ç»ï¼š

+   è®¾ç½®ï¼šé€šè¿‡[*Wikipedia-API*](https://pypi.org/project/Wikipedia-API/)è¯»å–æ•°æ®å’Œæ•°æ®åŒ…ã€‚

+   ä½¿ç”¨[*SpaCy*](https://spacy.io/usage/linguistic-features)è¿›è¡Œ NLPï¼šå¥å­åˆ†å‰²ã€è¯æ€§æ ‡æ³¨ã€ä¾å­˜å¥æ³•åˆ†æã€å‘½åå®ä½“è¯†åˆ«ã€‚

+   ä½¿ç”¨[*Textacy*](https://spacy.io/universe/project/textacy)æå–å®ä½“åŠå…¶å…³ç³»ã€‚

+   å¸¦æœ‰[*NetworkX*](https://networkx.org/documentation/stable/auto_examples/index.html)çš„ç½‘ç»œå›¾æ„å»ºã€‚

+   å¸¦æœ‰[*DateParser*](https://dateparser.readthedocs.io/en/latest/)çš„æ—¶é—´çº¿å›¾ã€‚

## è®¾ç½®

é¦–å…ˆï¼Œæˆ‘éœ€è¦å¯¼å…¥ä»¥ä¸‹åº“ï¼š

```py
## for data
import pandas as pd  #1.1.5
import numpy as np  #1.21.0

## for plotting
import matplotlib.pyplot as plt  #3.3.2

## for text
import wikipediaapi  #0.5.8
import nltk  #3.8.1
import re   

## for nlp
import spacy  #3.5.0
from spacy import displacy
import textacy  #0.12.0

## for graph
import networkx as nx  #3.0 (also pygraphviz==1.10)

## for timeline
import dateparser #1.1.7
```

*Wikipedia-api* æ˜¯ä¸€ä¸ª Python åŒ…è£…å™¨ï¼Œå¯ä»¥è½»æ¾è§£æ Wikipedia é¡µé¢ã€‚æˆ‘å°†æå–æˆ‘éœ€è¦çš„é¡µé¢ï¼Œæ’é™¤é¡µé¢åº•éƒ¨çš„æ‰€æœ‰â€œæ³¨é‡Šâ€å’Œâ€œå‚è€ƒä¹¦ç›®â€ï¼š

![](img/98801f2ea84a245c028d4c26538a7239.png)

æ¥æºäº Wikipedia

æˆ‘ä»¬å¯ä»¥ç®€å•åœ°å†™å‡ºé¡µé¢çš„åç§°ï¼š

```py
topic = "Russo-Ukrainian War"

wiki = wikipediaapi.Wikipedia('en')
page = wiki.page(topic)
txt = page.text[:page.text.find("See also")]
txt[0:500] + " ..."
```

![](img/94afa2a5e25e3dbda1692246d27596bf.png)

åœ¨è¿™ä¸ªç”¨ä¾‹ä¸­ï¼Œæˆ‘å°†å°è¯•é€šè¿‡è¯†åˆ«å’Œæå–æ–‡æœ¬ä¸­çš„ä¸»é¢˜-åŠ¨ä½œ-å¯¹è±¡ï¼ˆå› æ­¤åŠ¨ä½œå³ä¸ºå…³ç³»ï¼‰æ¥æ˜ å°„å†å²äº‹ä»¶ã€‚

## NLP

ä¸ºäº†æ„å»ºçŸ¥è¯†å›¾è°±ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦è¯†åˆ«å®ä½“åŠå…¶å…³ç³»ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ NLP æŠ€æœ¯å¤„ç†æ–‡æœ¬æ•°æ®é›†ã€‚

å½“å‰ï¼Œç”¨äºæ­¤ç±»ä»»åŠ¡çš„æœ€å¸¸ç”¨åº“æ˜¯ *SpaCy*ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºé«˜çº§ NLP çš„å¼€æºè½¯ä»¶ï¼Œåˆ©ç”¨ *Cython* (C+Python)ã€‚*SpaCy* ä½¿ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹å°†æ–‡æœ¬åˆ†è¯ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºä¸€ä¸ªé€šå¸¸ç§°ä¸º â€œ[document](https://spacy.io/api/doc)â€ çš„å¯¹è±¡ï¼ŒåŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªåŒ…å«æ¨¡å‹é¢„æµ‹çš„æ‰€æœ‰æ³¨é‡Šçš„ç±»ã€‚

```py
#python -m spacy download en_core_web_sm

nlp = spacy.load("en_core_web_sm")
doc = nlp(txt)
```

NLP æ¨¡å‹çš„ç¬¬ä¸€ä¸ªè¾“å‡ºæ˜¯ [**å¥å­åˆ†å‰²**](https://spacy.io/usage/linguistic-features#sbd)ï¼šç¡®å®šä¸€ä¸ªå¥å­å¼€å§‹å’Œç»“æŸçš„ä½ç½®çš„é—®é¢˜ã€‚é€šå¸¸ï¼Œé€šè¿‡åŸºäºæ ‡ç‚¹ç¬¦å·æ‹†åˆ†æ®µè½æ¥å®Œæˆã€‚è®©æˆ‘ä»¬çœ‹çœ‹ *SpaCy* å°†æ–‡æœ¬æ‹†åˆ†æˆäº†å¤šå°‘ä¸ªå¥å­ï¼š

```py
# from text to a list of sentences
lst_docs = [sent for sent in doc.sents]
print("tot sentences:", len(lst_docs))
```

![](img/3ec6987c0ef8065f19a9f2d4b1b6e9ea.png)

ä½œè€…æä¾›çš„å›¾ç‰‡

ç°åœ¨ï¼Œå¯¹äºæ¯ä¸ªå¥å­ï¼Œæˆ‘ä»¬å°†æå–å®ä½“åŠå…¶å…³ç³»ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦ç†è§£ [**è¯æ€§æ ‡æ³¨ (POS tagging)**](https://spacy.io/usage/linguistic-features#pos-tagging)**:** å°†å¥å­ä¸­çš„æ¯ä¸ªå•è¯æ ‡è®°ä¸Šé€‚å½“çš„è¯­æ³•æ ‡ç­¾çš„è¿‡ç¨‹ã€‚ä»¥ä¸‹æ˜¯å¯èƒ½çš„æ ‡ç­¾çš„å®Œæ•´åˆ—è¡¨ï¼ˆæˆªè‡³ä»Šå¤©ï¼‰ï¼š

> - ***ADJ***: å½¢å®¹è¯ï¼Œä¾‹å¦‚ big, old, green, incomprehensible, first
> 
> - ***ADP***: ä»‹è¯ï¼ˆå‰ç½®è¯/åç½®è¯ï¼‰ä¾‹å¦‚ in, to, during
> 
> - ***ADV***: å‰¯è¯ï¼Œä¾‹å¦‚ very, tomorrow, down, where, there
> 
> - ***AUX***: åŠ©åŠ¨è¯ï¼Œä¾‹å¦‚ is, has (done), will (do), should (do)
> 
> - ***CONJ***: è¿è¯ï¼Œä¾‹å¦‚ and, or, but
> 
> - ***CCONJ***: å¹¶åˆ—è¿è¯ï¼Œä¾‹å¦‚ and, or, but
> 
> - ***DET***: é™å®šè¯ï¼Œä¾‹å¦‚ a, an, the
> 
> - ***INTJ***: æ„Ÿå¹è¯ï¼Œä¾‹å¦‚ psst, ouch, bravo, hello
> 
> - ***NOUN***: åè¯ï¼Œä¾‹å¦‚ girl, cat, tree, air, beauty
> 
> - ***NUM***: æ•°å­—ï¼Œä¾‹å¦‚ 1, 2017, one, seventy-seven, IV, MMXIV
> 
> - ***PART***: è¯­æ°”è¯ï¼Œä¾‹å¦‚ â€˜s, not
> 
> - ***PRON***: ä»£è¯ï¼Œä¾‹å¦‚ I, you, he, she, myself, themselves, somebody
> 
> - ***PROPN***: ä¸“æœ‰åè¯ï¼Œä¾‹å¦‚ Mary, John, London, NATO, HBO
> 
> - ***PUNCT***: æ ‡ç‚¹ç¬¦å·ï¼Œä¾‹å¦‚ ., (, ), ?
> 
> - ***SCONJ***: ä»å±è¿è¯ï¼Œä¾‹å¦‚ if, while, that
> 
> - ***SYM***: ç¬¦å·ï¼Œä¾‹å¦‚ $, %, Â§, Â©, +, âˆ’, Ã—, Ã·, =, :), è¡¨æƒ…ç¬¦å·
> 
> - ***VERB***: åŠ¨è¯ï¼Œä¾‹å¦‚ run, runs, running, eat, ate, eating
> 
> - ***X***: å…¶ä»–ï¼Œä¾‹å¦‚ sfpksdpsxmsa
> 
> - ***SPACE***: ç©ºæ ¼ï¼Œä¾‹å¦‚

ä»…ä»…è¿›è¡Œè¯æ€§æ ‡æ³¨æ˜¯ä¸å¤Ÿçš„ï¼Œæ¨¡å‹è¿˜è¯•å›¾ç†è§£è¯å¯¹ä¹‹é—´çš„å…³ç³»ã€‚è¿™é¡¹ä»»åŠ¡ç§°ä¸º [**ä¾å­˜å¥æ³•åˆ†æ**](https://spacy.io/usage/linguistic-features#dependency-parse)ã€‚ä»¥ä¸‹æ˜¯æ‰€æœ‰å¯èƒ½çš„æ ‡è®°åˆ—è¡¨ï¼ˆæˆªè‡³ä»Šå¤©ï¼‰ï¼š

> - ***ä»å¥ä¿®é¥°åè¯ï¼š*** clausal modifier of noun
> 
> - ***å½¢å®¹è¯è¡¥è¶³è¯­ï¼š*** adjectival complement
> 
> - ***å‰¯è¯ä»å¥ä¿®é¥°è¯­ï¼š*** adverbial clause modifier
> 
> - ***å‰¯è¯ä¿®é¥°è¯­ï¼š*** adverbial modifier
> 
> - ***æ–½äº‹è€…ï¼š*** agent
> 
> - ***å½¢å®¹è¯ä¿®é¥°è¯­ï¼š*** adjectival modifier
> 
> - ***åŒä½ä¿®é¥°è¯­ï¼š*** appositional modifier
> 
> - ***å±æ€§ï¼š*** attribute
> 
> - ***è¾…åŠ©è¯ï¼š*** auxiliary
> 
> - ***è¾…åŠ©åŠ¨è¯ï¼ˆè¢«åŠ¨ï¼‰ï¼š*** auxiliary (passive)
> 
> - ***æ ¼æ ‡è®°ï¼š*** case marker
> 
> - ***å¹¶åˆ—è¿è¯ï¼š*** coordinating conjunction
> 
> - ***ä»å¥è¡¥è¶³è¯­ï¼š*** clausal complement
> 
> - ***å¤åˆä¿®é¥°è¯­ï¼š*** compound modifier
> 
> - ***è¿æ¥è¯ï¼š*** conjunct
> 
> - ***ä»å¥ä¸»è¯­ï¼š*** clausal subject
> 
> - ***ä»å¥ä¸»è¯­ï¼ˆè¢«åŠ¨ï¼‰ï¼š*** clausal subject (passive)
> 
> - ***ä¸æ ¼ï¼š*** dative
> 
> - ***ä¾å­˜è¯ï¼š*** unclassified dependent
> 
> - ***é™å®šè¯ï¼š*** determiner
> 
> - ***ç›´æ¥å®¾è¯­ï¼š*** direct object
> 
> - ***è™šè¯ï¼š*** expletive
> 
> - ***æ„Ÿå¹è¯ï¼š*** interjection
> 
> - ***æ ‡è®°ï¼š*** marker
> 
> - ***å…ƒä¿®é¥°è¯­ï¼š*** meta modifier
> 
> - ***å¦å®šä¿®é¥°è¯­ï¼š*** negation modifier
> 
> - ***åè¯ä¿®é¥°è¯­ï¼š*** modifier of nominal
> 
> - ***åè¯çŸ­è¯­ä½œä¸ºå‰¯è¯ä¿®é¥°è¯­ï¼š*** noun phrase as adverbial modifier
> 
> - ***åè¯ä¸»è¯­ï¼š*** nominal subject
> 
> - ***åè¯ä¸»è¯­ï¼ˆè¢«åŠ¨ï¼‰ï¼š*** nominal subject (passive)
> 
> - ***æ•°é‡ä¿®é¥°è¯­ï¼š*** number modifier
> 
> - ***å¯¹è±¡è°“è¯ï¼š*** object predicate
> 
> - ***å¹¶åˆ—è¯­æ³•ï¼š*** parataxis
> 
> - ***ä»‹è¯è¡¥è¶³è¯­ï¼š*** complement of preposition
> 
> - ***ä»‹è¯å®¾è¯­ï¼š*** object of preposition
> 
> - ***æ‰€æœ‰æ ¼ä¿®é¥°è¯­ï¼š*** possession modifier
> 
> - ***å‰å…³è”è¿è¯ï¼š*** pre-correlative conjunction
> 
> - ***å‰é™å®šè¯ï¼š*** pre-determiner
> 
> - ***ä»‹è¯ä¿®é¥°è¯­ï¼š*** prepositional modifier
> 
> - ***è¯ç´ ï¼š*** particle
> 
> - ***æ ‡ç‚¹ï¼š*** punctuation
> 
> - ***æ•°é‡è¯ä¿®é¥°è¯­ï¼š*** modifier of quantifier
> 
> - ***å…³ç³»ä»å¥ä¿®é¥°è¯­ï¼š*** relative clause modifier
> 
> - ***æ ¹ï¼š*** root
> 
> - ***å¼€æ”¾ä»å¥è¡¥è¶³è¯­ï¼š*** open clausal complement

è®©æˆ‘ä»¬ä¸¾ä¸ªä¾‹å­æ¥ç†è§£è¯æ€§æ ‡æ³¨å’Œä¾å­˜å¥æ³•åˆ†æï¼š

```py
# take a sentence
i = 3
lst_docs[i]
```

![](img/4ca9e2e7ea244d3a1bd20faf05a22517.png)

è®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹ NLP æ¨¡å‹é¢„æµ‹çš„è¯æ€§å’Œä¾å­˜æ ‡è®°ï¼š

```py
for token in lst_docs[i]:
    print(token.text, "-->", "pos: "+token.pos_, "|", "dep: "+token.dep_, "")
```

![](img/1410ef9f475e206da6e054efb8c69c85.png)

ä½œè€…æä¾›çš„å›¾ç‰‡

*SpaCy* è¿˜æä¾›äº†ä¸€ä¸ª [å›¾å½¢å·¥å…·](https://spacy.io/usage/visualizers) æ¥å¯è§†åŒ–è¿™äº›æ ‡æ³¨ï¼š

```py
from spacy import displacy

displacy.render(lst_docs[i], style="dep", options={"distance":100})
```

![](img/b8d9a5d99388ba97c4c4eaa3144f8dd5.png)

ä½œè€…æä¾›çš„å›¾ç‰‡

æœ€é‡è¦çš„è¯ç´ æ˜¯åŠ¨è¯ï¼ˆ*POS=VERB*ï¼‰ï¼Œå› ä¸ºå®ƒæ˜¯å¥å­æ„ä¹‰çš„æ ¹æºï¼ˆ*DEP=ROOT*ï¼‰ã€‚

![](img/64e0fa11ce6e368669c43d8859a55538.png)

ä½œè€…æä¾›çš„å›¾ç‰‡

è¾…åŠ©ç²’å­ï¼Œä¾‹å¦‚å‰¯è¯å’Œä»‹è¯ï¼ˆ*POS=ADV/ADP*ï¼‰ï¼Œé€šå¸¸ä½œä¸ºä¿®é¥°è¯­ä¸åŠ¨è¯ç›¸è¿ï¼ˆ*DEP=*mod*ï¼‰ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥ä¿®é¥°åŠ¨è¯çš„æ„ä¹‰ã€‚ä¾‹å¦‚ï¼Œâ€œ*travel to*â€ å’Œ â€œ*travel from*â€ å°½ç®¡æ ¹è¯ç›¸åŒï¼ˆâ€œ*travel*â€ï¼‰ï¼Œå´æœ‰ä¸åŒçš„å«ä¹‰ã€‚

![](img/f3287c63bbc5adc3e8332d44c1928d30.png)

ä½œè€…æä¾›çš„å›¾ç‰‡

åœ¨ä¸åŠ¨è¯ç›¸å…³è”çš„è¯ä¸­ï¼Œå¿…é¡»æœ‰ä¸€äº›åè¯ï¼ˆ*POS=PROPN/NOUN*ï¼‰ï¼Œå®ƒä»¬å……å½“å¥å­çš„ä¸»è¯­å’Œå®¾è¯­ï¼ˆ*DEP=nsubj/*obj*ï¼‰ã€‚

![](img/a8a6d2c219de36e650bce6151b5bf3a4.png)

ä½œè€…æä¾›çš„å›¾ç‰‡

åè¯é€šå¸¸é è¿‘ä¸€ä¸ªå½¢å®¹è¯ï¼ˆ*POS=ADJ*ï¼‰ï¼Œè¿™ä¸ªå½¢å®¹è¯å……å½“å®ƒä»¬æ„ä¹‰çš„ä¿®é¥°è¯ï¼ˆ*DEP=amod*ï¼‰ã€‚ä¾‹å¦‚ï¼Œåœ¨â€œ*å¥½äºº*â€å’Œâ€œ*åäºº*â€ä¸­ï¼Œå½¢å®¹è¯ç»™åè¯*â€œäººâ€*å¸¦æ¥äº†ç›¸åçš„å«ä¹‰ã€‚

![](img/5e6293965ae065dafede1c255e1a0564.png)

ä½œè€…æä¾›çš„å›¾ç‰‡

*SpaCy* æ‰§è¡Œçš„å¦ä¸€ä¸ªé…·ç‚«ä»»åŠ¡æ˜¯ [**å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰**](https://spacy.io/usage/linguistic-features#named-entities)ã€‚å‘½åå®ä½“æ˜¯ä¸€ä¸ªâ€œç°å®ä¸–ç•Œçš„å¯¹è±¡â€ï¼ˆå³äººã€å›½å®¶ã€äº§å“ã€æ—¥æœŸï¼‰ï¼Œæ¨¡å‹å¯ä»¥è¯†åˆ«æ–‡æ¡£ä¸­çš„å„ç§ç±»å‹ã€‚ä»¥ä¸‹æ˜¯å¯èƒ½æ ‡ç­¾çš„å®Œæ•´åˆ—è¡¨ï¼ˆæˆªè‡³ä»Šæ—¥ï¼‰ï¼š

> - ***äººç‰©:*** äººï¼ŒåŒ…æ‹¬è™šæ„çš„ã€‚
> 
> - ***å›½å®¶/å®—æ•™/æ”¿æ²»ç»„ç»‡:*** å›½ç±ã€å®—æ•™æˆ–æ”¿æ²»å›¢ä½“ã€‚
> 
> - ***è®¾æ–½:*** å»ºç­‘ç‰©ã€æœºåœºã€å…¬è·¯ã€æ¡¥æ¢ç­‰ã€‚
> 
> - ***ç»„ç»‡:*** å…¬å¸ã€æœºæ„ã€ç»„ç»‡ç­‰ã€‚
> 
> - ***GPE:*** å›½å®¶ã€åŸå¸‚ã€å·ã€‚
> 
> - ***åœ°ç‚¹:*** é GPE ä½ç½®ã€å±±è„‰ã€æ°´ä½“ã€‚
> 
> - ***äº§å“:*** ç‰©å“ã€è½¦è¾†ã€é£Ÿå“ç­‰ã€‚ï¼ˆä¸æ˜¯æœåŠ¡ã€‚ï¼‰
> 
> - ***äº‹ä»¶:*** å‘½åçš„é£“é£ã€æˆ˜å½¹ã€æˆ˜äº‰ã€ä½“è‚²èµ›äº‹ç­‰ã€‚
> 
> - ***è‰ºæœ¯ä½œå“:*** ä¹¦ç±ã€æ­Œæ›²ç­‰çš„æ ‡é¢˜ã€‚
> 
> - ***æ³•å¾‹:*** æˆä¸ºæ³•å¾‹çš„å‘½åæ–‡æ¡£ã€‚
> 
> - ***è¯­è¨€:*** ä»»ä½•å‘½åçš„è¯­è¨€ã€‚
> 
> - ***æ—¥æœŸ:*** ç»å¯¹æˆ–ç›¸å¯¹çš„æ—¥æœŸæˆ–æ—¶æœŸã€‚
> 
> - ***æ—¶é—´:*** å°äºä¸€å¤©çš„æ—¶é—´ã€‚
> 
> - ***ç™¾åˆ†æ¯”:*** åŒ…æ‹¬â€œ%â€çš„ç™¾åˆ†æ•°ã€‚
> 
> - ***è´§å¸:*** åŒ…æ‹¬å•ä½çš„è´§å¸å€¼ã€‚
> 
> - ***æ•°é‡:*** ä»¥é‡é‡æˆ–è·ç¦»ä¸ºå•ä½çš„æµ‹é‡å€¼ã€‚
> 
> - ***åºæ•°:*** â€œç¬¬ä¸€â€ã€â€œç¬¬äºŒâ€ç­‰ã€‚
> 
> - ***åŸºæ•°:*** ä¸å±äºå…¶ä»–ç±»å‹çš„æ•°å­—ã€‚

è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬çš„ä¾‹å­ï¼š

```py
for tag in lst_docs[i].ents:
    print(tag.text, f"({tag.label_})") 
```

![](img/848b13980abc501d3ac140d9e5acc8d8.png)

ä½œè€…æä¾›çš„å›¾ç‰‡

æˆ–è€…ä½¿ç”¨ *SpaCy* çš„å›¾å½¢å·¥å…·ï¼š

```py
displacy.render(lst_docs[i], style="ent")
```

![](img/b9006404b0c49d328aca53ab28813899.png)

ä½œè€…æä¾›çš„å›¾ç‰‡

è¿™åœ¨æˆ‘ä»¬æƒ³è¦å‘çŸ¥è¯†å›¾è°±ä¸­æ·»åŠ å‡ ä¸ªå±æ€§æ—¶éå¸¸æœ‰ç”¨ã€‚

ç»§ç»­ï¼Œä½¿ç”¨ NLP æ¨¡å‹é¢„æµ‹çš„æ ‡ç­¾ï¼Œæˆ‘ä»¬å¯ä»¥æå–å®ä½“åŠå…¶å…³ç³»ã€‚

## **å®ä½“ä¸å…³ç³»æå–**

è¿™ä¸ªæƒ³æ³•éå¸¸ç®€å•ï¼Œä½†å®ç°èµ·æ¥å¯èƒ½ä¼šå¾ˆæ£˜æ‰‹ã€‚å¯¹äºæ¯ä¸ªå¥å­ï¼Œæˆ‘ä»¬å°†æå–ä¸»è¯­å’Œå®¾è¯­åŠå…¶ä¿®é¥°è¯ã€å¤åˆè¯å’Œå®ƒä»¬ä¹‹é—´çš„æ ‡ç‚¹ç¬¦å·ã€‚

è¿™å¯ä»¥é€šè¿‡ä¸¤ç§æ–¹å¼å®Œæˆï¼š

1.  **æ‰‹åŠ¨**åœ°ï¼Œä½ å¯ä»¥ä»åŸºçº¿ä»£ç å¼€å§‹ï¼Œè¿™äº›ä»£ç å¯èƒ½éœ€è¦ç¨å¾®ä¿®æ”¹å’Œé€‚åº”ä½ çš„ç‰¹å®šæ•°æ®é›†/ç”¨ä¾‹ã€‚

```py
def extract_entities(doc):
    a, b, prev_dep, prev_txt, prefix, modifier = "", "", "", "", "", ""
    for token in doc:
        if token.dep_ != "punct":
            ## prexif --> prev_compound + compound
            if token.dep_ == "compound":
                prefix = prev_txt +" "+ token.text if prev_dep == "compound" else token.text

            ## modifier --> prev_compound + %mod
            if token.dep_.endswith("mod") == True:
                modifier = prev_txt +" "+ token.text if prev_dep == "compound" else token.text

            ## subject --> modifier + prefix + %subj
            if token.dep_.find("subj") == True:
                a = modifier +" "+ prefix + " "+ token.text
                prefix, modifier, prev_dep, prev_txt = "", "", "", ""

            ## if object --> modifier + prefix + %obj
            if token.dep_.find("obj") == True:
                b = modifier +" "+ prefix +" "+ token.text

            prev_dep, prev_txt = token.dep_, token.text

    # clean
    a = " ".join([i for i in a.split()])
    b = " ".join([i for i in b.split()])
    return (a.strip(), b.strip())

# The relation extraction requires the rule-based matching tool, 
# an improved version of regular expressions on raw text.
def extract_relation(doc, nlp):
    matcher = spacy.matcher.Matcher(nlp.vocab)
    p1 = [{'DEP':'ROOT'}, 
          {'DEP':'prep', 'OP':"?"},
          {'DEP':'agent', 'OP':"?"},
          {'POS':'ADJ', 'OP':"?"}] 
    matcher.add(key="matching_1", patterns=[p1]) 
    matches = matcher(doc)
    k = len(matches) - 1
    span = doc[matches[k][1]:matches[k][2]] 
    return span.text
```

è®©æˆ‘ä»¬åœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šè¯•è¯•ï¼Œå¹¶æŸ¥çœ‹å¸¸è§çš„ä¾‹å­ï¼š

```py
## extract entities
lst_entities = [extract_entities(i) for i in lst_docs]

## example
lst_entities[i]
```

![](img/31084e80201ba04ce021dd92e2a13cec.png)

```py
## extract relations
lst_relations = [extract_relation(i,nlp) for i in lst_docs]

## example
lst_relations[i]
```

![](img/67878e59e8bd4936a43a10c30a1bce21.png)

```py
## extract attributes (NER)
lst_attr = []
for x in lst_docs:
    attr = ""
    for tag in x.ents:
        attr = attr+tag.text if tag.label_=="DATE" else attr+""
    lst_attr.append(attr)

## example
lst_attr[i]
```

![](img/ab08673c6872b979eb3c506abcb60b0c.png)

2\. ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ ***Textacy***ï¼Œè¿™æ˜¯ä¸€ä¸ªå»ºç«‹åœ¨ *SpaCy* ä¹‹ä¸Šçš„åº“ï¼Œç”¨äºæ‰©å±•å…¶æ ¸å¿ƒåŠŸèƒ½ã€‚è¿™æ›´å‹å¥½ä¸”é€šå¸¸æ›´å‡†ç¡®ã€‚

```py
## extract entities and relations
dic = {"id":[], "text":[], "entity":[], "relation":[], "object":[]}

for n,sentence in enumerate(lst_docs):
    lst_generators = list(textacy.extract.subject_verb_object_triples(sentence))  
    for sent in lst_generators:
        subj = "_".join(map(str, sent.subject))
        obj  = "_".join(map(str, sent.object))
        relation = "_".join(map(str, sent.verb))
        dic["id"].append(n)
        dic["text"].append(sentence.text)
        dic["entity"].append(subj)
        dic["object"].append(obj)
        dic["relation"].append(relation)

## create dataframe
dtf = pd.DataFrame(dic)

## example
dtf[dtf["id"]==i]
```

![](img/2c430480ba4a981daa8c9c2822bf85af.png)

ä½œè€…æä¾›çš„å›¾ç‰‡

è®©æˆ‘ä»¬ä¹Ÿä½¿ç”¨ NER æ ‡ç­¾ï¼ˆå³æ—¥æœŸï¼‰æå–å±æ€§ï¼š

```py
## extract attributes
attribute = "DATE"
dic = {"id":[], "text":[], attribute:[]}

for n,sentence in enumerate(lst_docs):
    lst = list(textacy.extract.entities(sentence, include_types={attribute}))
    if len(lst) > 0:
        for attr in lst:
            dic["id"].append(n)
            dic["text"].append(sentence.text)
            dic[attribute].append(str(attr))
    else:
        dic["id"].append(n)
        dic["text"].append(sentence.text)
        dic[attribute].append(np.nan)

dtf_att = pd.DataFrame(dic)
dtf_att = dtf_att[~dtf_att[attribute].isna()]

## example
dtf_att[dtf_att["id"]==i]
```

![](img/d61096259f6f1b6ed84bc617a5fd3795.png)

ä½œè€…æä¾›çš„å›¾ç‰‡

ç°åœ¨æˆ‘ä»¬å·²ç»æå–äº†â€œçŸ¥è¯†â€ï¼Œå¯ä»¥æ„å»ºå›¾è°±ã€‚

## ç½‘ç»œå›¾

æ ‡å‡†çš„ Python åº“ç”¨äºåˆ›å»ºå’Œæ“ä½œå›¾ç½‘ç»œæ˜¯***NetworkX***ã€‚æˆ‘ä»¬å¯ä»¥ä»æ•´ä¸ªæ•°æ®é›†å¼€å§‹åˆ›å»ºå›¾ï¼Œä½†å¦‚æœèŠ‚ç‚¹å¤ªå¤šï¼Œè§†è§‰æ•ˆæœä¼šå˜å¾—æ··ä¹±ï¼š

```py
## create full graph
G = nx.from_pandas_edgelist(dtf, source="entity", target="object", 
                            edge_attr="relation", 
                            create_using=nx.DiGraph())

## plot
plt.figure(figsize=(15,10))

pos = nx.spring_layout(G, k=1)
node_color = "skyblue"
edge_color = "black"

nx.draw(G, pos=pos, with_labels=True, node_color=node_color, 
        edge_color=edge_color, cmap=plt.cm.Dark2, 
        node_size=2000, connectionstyle='arc3,rad=0.1')

nx.draw_networkx_edge_labels(G, pos=pos, label_pos=0.5, 
                         edge_labels=nx.get_edge_attributes(G,'relation'),
                         font_size=12, font_color='black', alpha=0.6)
plt.show()
```

![](img/4bfc74c9eb2a4af92f33d79e754fed87.png)

å›¾ç‰‡ç”±ä½œè€…æä¾›

çŸ¥è¯†å›¾è°±ä½¿å¾—ä»å¤§å±€ä¸Šçœ‹åˆ°äº‹ç‰©ä¹‹é—´çš„å…³ç³»æˆä¸ºå¯èƒ½ï¼Œä½†åƒè¿™æ ·æ˜¯ç›¸å½“æ— ç”¨çš„â€¦â€¦æ‰€ä»¥æœ€å¥½æ˜¯æ ¹æ®æˆ‘ä»¬è¦å¯»æ‰¾çš„ä¿¡æ¯åº”ç”¨ä¸€äº›è¿‡æ»¤å™¨ã€‚å¯¹äºè¿™ä¸ªä¾‹å­ï¼Œæˆ‘å°†åªå–æ¶‰åŠæœ€é¢‘ç¹å®ä½“ï¼ˆåŸºæœ¬ä¸Šæ˜¯æœ€è¿æ¥çš„èŠ‚ç‚¹ï¼‰çš„å›¾çš„ä¸€éƒ¨åˆ†ï¼š

```py
dtf["entity"].value_counts().head()
```

![](img/c2ff0f03744dec80e73556d39245abb2.png)

å›¾ç‰‡ç”±ä½œè€…æä¾›

```py
## filter
f = "Russia"
tmp = dtf[(dtf["entity"]==f) | (dtf["object"]==f)]

## create small graph
G = nx.from_pandas_edgelist(tmp, source="entity", target="object", 
                            edge_attr="relation", 
                            create_using=nx.DiGraph())

## plot
plt.figure(figsize=(15,10))

pos = nx.nx_agraph.graphviz_layout(G, prog="neato")
node_color = ["red" if node==f else "skyblue" for node in G.nodes]
edge_color = ["red" if edge[0]==f else "black" for edge in G.edges]

nx.draw(G, pos=pos, with_labels=True, node_color=node_color, 
        edge_color=edge_color, cmap=plt.cm.Dark2, 
        node_size=2000, node_shape="o", connectionstyle='arc3,rad=0.1')

nx.draw_networkx_edge_labels(G, pos=pos, label_pos=0.5, 
                        edge_labels=nx.get_edge_attributes(G,'relation'),
                        font_size=12, font_color='black', alpha=0.6)
plt.show()
```

![](img/62f9aba479b0b678fadcd734acc63dc8.png)

å›¾ç‰‡ç”±ä½œè€…æä¾›

è¿™æ ·æ›´å¥½ã€‚å¦‚æœä½ æƒ³å°†å…¶åšæˆ 3Dï¼Œä½¿ç”¨ä»¥ä¸‹ä»£ç ï¼š

```py
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(15,10))
ax = fig.add_subplot(111, projection="3d")
pos = nx.spring_layout(G, k=2.5, dim=3)

nodes = np.array([pos[v] for v in sorted(G) if v!=f])
center_node = np.array([pos[v] for v in sorted(G) if v==f])

edges = np.array([(pos[u],pos[v]) for u,v in G.edges() if v!=f])
center_edges = np.array([(pos[u],pos[v]) for u,v in G.edges() if v==f])

ax.scatter(*nodes.T, s=200, ec="w", c="skyblue", alpha=0.5)
ax.scatter(*center_node.T, s=200, c="red", alpha=0.5)

for link in edges:
    ax.plot(*link.T, color="grey", lw=0.5)
for link in center_edges:
    ax.plot(*link.T, color="red", lw=0.5)

for v in sorted(G):
    ax.text(*pos[v].T, s=v)
for u,v in G.edges():
    attr = nx.get_edge_attributes(G, "relation")[(u,v)]
    ax.text(*((pos[u]+pos[v])/2).T, s=attr)

ax.set(xlabel=None, ylabel=None, zlabel=None, 
       xticklabels=[], yticklabels=[], zticklabels=[])
ax.grid(False)
for dim in (ax.xaxis, ax.yaxis, ax.zaxis):
    dim.set_ticks([])
plt.show()
```

![](img/c92571d1d1e576e2b06ffc77a5922888.png)

å›¾ç‰‡ç”±ä½œè€…æä¾›

è¯·æ³¨æ„ï¼Œå›¾è¡¨å¯èƒ½å¾ˆæœ‰ç”¨ä¸”å¯è§†åŒ–æ•ˆæœå¾ˆå¥½ï¼Œä½†è¿™ä¸æ˜¯æœ¬æ•™ç¨‹çš„ä¸»è¦ç„¦ç‚¹ã€‚çŸ¥è¯†å›¾è°±æœ€é‡è¦çš„éƒ¨åˆ†æ˜¯â€œçŸ¥è¯†â€ï¼ˆæ–‡æœ¬å¤„ç†ï¼‰ï¼Œç„¶åç»“æœå¯ä»¥ä»¥æ•°æ®æ¡†ã€å›¾å½¢æˆ–å…¶ä»–å›¾è¡¨çš„å½¢å¼å±•ç¤ºã€‚ä¾‹å¦‚ï¼Œæˆ‘å¯ä»¥ä½¿ç”¨ NER è¯†åˆ«çš„æ—¥æœŸæ¥æ„å»ºä¸€ä¸ªæ—¶é—´è½´å›¾ã€‚

## æ—¶é—´è½´å›¾

é¦–å…ˆï¼Œæˆ‘å¿…é¡»å°†è¢«è¯†åˆ«ä¸ºâ€œæ—¥æœŸâ€çš„å­—ç¬¦ä¸²è½¬æ¢ä¸º datetime æ ¼å¼ã€‚åº“***DateParser***å¯ä»¥è§£æå‡ ä¹æ‰€æœ‰åœ¨ç½‘é¡µä¸Šå¸¸è§çš„å­—ç¬¦ä¸²æ ¼å¼çš„æ—¥æœŸã€‚

```py
def utils_parsetime(txt):
    x = re.match(r'.*([1-3][0-9]{3})', txt) #<--check if there is a year
    if x is not None:
        try:
            dt = dateparser.parse(txt)
        except:
            dt = np.nan
    else:
        dt = np.nan
    return dt
```

è®©æˆ‘ä»¬å°†å…¶åº”ç”¨äºå±æ€§çš„æ•°æ®æ¡†ï¼š

```py
dtf_att["dt"] = dtf_att["date"].apply(lambda x: utils_parsetime(x))

## example
dtf_att[dtf_att["id"]==i]
```

![](img/70777e12da6d469ee726b17e5717e20d.png)

å›¾ç‰‡ç”±ä½œè€…æä¾›

ç°åœ¨ï¼Œæˆ‘å°†å…¶ä¸ä¸»è¦çš„å®ä½“-å…³ç³»æ•°æ®æ¡†åˆå¹¶ï¼š

```py
tmp = dtf.copy()
tmp["y"] = tmp["entity"]+" "+tmp["relation"]+" "+tmp["object"]

dtf_att = dtf_att.merge(tmp[["id","y"]], how="left", on="id")
dtf_att = dtf_att[~dtf_att["y"].isna()].sort_values("dt", 
                 ascending=True).drop_duplicates("y", keep='first')
dtf_att.head()
```

![](img/4295cf217f9e8c39cd2e529feb4b21d1.png)

å›¾ç‰‡ç”±ä½œè€…æä¾›

æœ€åï¼Œæˆ‘å¯ä»¥ç»˜åˆ¶æ—¶é—´è½´ã€‚æ­£å¦‚æˆ‘ä»¬å·²ç»çŸ¥é“çš„ï¼Œå®Œæ•´çš„å›¾è¡¨å¯èƒ½ä¸ä¼šå¾ˆæœ‰ç”¨ï¼š

```py
dates = dtf_att["dt"].values
names = dtf_att["y"].values
l = [10,-10, 8,-8, 6,-6, 4,-4, 2,-2]
levels = np.tile(l, int(np.ceil(len(dates)/len(l))))[:len(dates)]

fig, ax = plt.subplots(figsize=(20,10))
ax.set(title=topic, yticks=[], yticklabels=[])

ax.vlines(dates, ymin=0, ymax=levels, color="tab:red")
ax.plot(dates, np.zeros_like(dates), "-o", color="k", markerfacecolor="w")

for d,l,r in zip(dates,levels,names):
    ax.annotate(r, xy=(d,l), xytext=(-3, np.sign(l)*3), 
                textcoords="offset points",
                horizontalalignment="center",
                verticalalignment="bottom" if l>0 else "top")

plt.xticks(rotation=90) 
plt.show()
```

![](img/cd5408bb3670cdc5ca8e046212b647cb.png)

å›¾ç‰‡ç”±ä½œè€…æä¾›

æ‰€ä»¥æœ€å¥½æ˜¯è¿‡æ»¤å‡ºç‰¹å®šæ—¶é—´æ®µï¼š

```py
yyyy = "2022"
dates = dtf_att[dtf_att["dt"]>yyyy]["dt"].values
names = dtf_att[dtf_att["dt"]>yyyy]["y"].values
l = [10,-10, 8,-8, 6,-6, 4,-4, 2,-2]
levels = np.tile(l, int(np.ceil(len(dates)/len(l))))[:len(dates)]

fig, ax = plt.subplots(figsize=(20,10))
ax.set(title=topic, yticks=[], yticklabels=[])

ax.vlines(dates, ymin=0, ymax=levels, color="tab:red")
ax.plot(dates, np.zeros_like(dates), "-o", color="k", markerfacecolor="w")

for d,l,r in zip(dates,levels,names):
    ax.annotate(r, xy=(d,l), xytext=(-3, np.sign(l)*3), 
                textcoords="offset points",
                horizontalalignment="center",
                verticalalignment="bottom" if l>0 else "top")

plt.xticks(rotation=90) 
plt.show()
```

![](img/1b71d3d0ba0a63a3ba9c68789e0a79aa.png)

å›¾ç‰‡ç”±ä½œè€…æä¾›

æ­£å¦‚ä½ æ‰€è§ï¼Œä¸€æ—¦â€œçŸ¥è¯†â€è¢«æå–å‡ºæ¥ï¼Œä½ å¯ä»¥ç”¨ä»»ä½•æ–¹å¼ç»˜åˆ¶å®ƒã€‚

## ç»“è®º

æœ¬æ–‡æ˜¯å…³äº**å¦‚ä½•ç”¨ Python æ„å»ºçŸ¥è¯†å›¾è°±**çš„æ•™ç¨‹ã€‚æˆ‘ä½¿ç”¨äº†å‡ ç§ NLP æŠ€æœ¯å¤„ç†ä»ç»´åŸºç™¾ç§‘è§£æçš„æ•°æ®ï¼Œä»¥æå–â€œçŸ¥è¯†â€ï¼ˆå³å®ä½“å’Œå…³ç³»ï¼‰ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨ä¸€ä¸ªç½‘ç»œå›¾å¯¹è±¡ä¸­ã€‚

ç°åœ¨ä½ æ˜ç™½äº†ä¸ºä»€ä¹ˆå…¬å¸æ­£åœ¨åˆ©ç”¨ NLP å’ŒçŸ¥è¯†å›¾è°±æ¥æ˜ å°„æ¥è‡ªå¤šä¸ªæ¥æºçš„ç›¸å…³æ•°æ®ï¼Œå¹¶æ‰¾åˆ°å¯¹ä¸šåŠ¡æœ‰ç”¨çš„æ´å¯Ÿã€‚è¯•æƒ³ä¸€ä¸‹ï¼Œé€šè¿‡åœ¨ä¸å•ä¸€å®ä½“ï¼ˆä¾‹å¦‚ Apple Incï¼‰ç›¸å…³çš„æ‰€æœ‰æ–‡æ¡£ï¼ˆä¾‹å¦‚è´¢æŠ¥ã€æ–°é—»ã€æ¨æ–‡ï¼‰ä¸Šåº”ç”¨è¿™ç§æ¨¡å‹ï¼Œå¯ä»¥æå–å‡ºå¤šå°‘ä»·å€¼ã€‚ä½ å¯ä»¥å¿«é€Ÿäº†è§£æ‰€æœ‰ä¸è¯¥å®ä½“ç›´æ¥ç›¸å…³çš„äº‹å®ã€äººç‰©å’Œå…¬å¸ã€‚ç„¶åï¼Œé€šè¿‡æ‰©å±•ç½‘ç»œï¼Œç”šè‡³å¯ä»¥è·å–ä¸èµ·å§‹å®ä½“ï¼ˆA â€” > B â€” > Cï¼‰ä¸ç›´æ¥ç›¸å…³çš„ä¿¡æ¯ã€‚

å¸Œæœ›ä½ å–œæ¬¢è¿™ä¸ªï¼å¦‚æœ‰ä»»ä½•é—®é¢˜æˆ–åé¦ˆï¼Œæˆ–æƒ³åˆ†äº«ä½ çš„æœ‰è¶£é¡¹ç›®ï¼Œæ¬¢è¿éšæ—¶è”ç³»æˆ‘ã€‚

> ğŸ‘‰ [è®©æˆ‘ä»¬è”ç³»](https://linktr.ee/maurodp) ğŸ‘ˆ
> 
> æœ¬æ–‡æ˜¯**ä½¿ç”¨ Python è¿›è¡Œ NLP**ç³»åˆ—çš„ä¸€éƒ¨åˆ†ï¼Œè¿˜å¯ä»¥æŸ¥çœ‹ï¼š

[](/text-summarization-with-nlp-textrank-vs-seq2seq-vs-bart-474943efeb09?source=post_page-----12b93146a458--------------------------------) ## ä½¿ç”¨ NLP è¿›è¡Œæ–‡æœ¬æ€»ç»“ï¼šTextRank ä¸ Seq2Seq ä¸ BART

### ä½¿ç”¨ Python è¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†ã€Gensimã€Tensorflowã€Transformers

[towardsdatascience.com [](/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794?source=post_page-----12b93146a458--------------------------------) ## NLP çš„æ–‡æœ¬åˆ†ç±»ï¼šTf-Idf ä¸ Word2Vec ä¸ BERT

### é¢„å¤„ç†ã€æ¨¡å‹è®¾è®¡ã€è¯„ä¼°ã€Bag-of-Wordsã€è¯åµŒå…¥ã€è¯­è¨€æ¨¡å‹çš„å¯è§£é‡Šæ€§

[towardsdatascience.com [](/text-analysis-feature-engineering-with-nlp-502d6ea9225d?source=post_page-----12b93146a458--------------------------------) ## NLP çš„æ–‡æœ¬åˆ†æä¸ç‰¹å¾å·¥ç¨‹

### è¯­è¨€æ£€æµ‹ã€æ–‡æœ¬æ¸…ç†ã€é•¿åº¦ã€æƒ…æ„Ÿã€å‘½åå®ä½“è¯†åˆ«ã€N-gram é¢‘ç‡ã€è¯å‘é‡ã€ä¸»é¢˜â€¦â€¦

[towardsdatascience.com [](/text-classification-with-no-model-training-935fe0e42180?source=post_page-----12b93146a458--------------------------------) ## BERT ç”¨äºæ— æ¨¡å‹è®­ç»ƒçš„æ–‡æœ¬åˆ†ç±»

### å½“ä½ æ²¡æœ‰æ ‡è®°è®­ç»ƒé›†æ—¶ï¼Œä½¿ç”¨ BERTã€è¯åµŒå…¥å’Œå‘é‡ç›¸ä¼¼æ€§

[towardsdatascience.com [](/ai-chatbot-with-nlp-speech-recognition-transformers-583716a299e9?source=post_page-----12b93146a458--------------------------------) ## ä½¿ç”¨ NLP æ„å»º AI èŠå¤©æœºå™¨äººï¼šè¯­éŸ³è¯†åˆ«+Transformers

### ä½¿ç”¨ Python æ„å»ºä¸€ä¸ªä¼šè¯èŠå¤©æœºå™¨äººï¼Œä¸ AI è¿›è¡Œå¯¹è¯

[towardsdatascience.com
