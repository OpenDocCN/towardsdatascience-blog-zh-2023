- en: Stereo Vision System for 3D Tracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/stereo-vision-system-for-3d-tracking-ce8eaca3a40a?source=collection_archive---------1-----------------------#2023-01-08](https://towardsdatascience.com/stereo-vision-system-for-3d-tracking-ce8eaca3a40a?source=collection_archive---------1-----------------------#2023-01-08)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Two eyes are all you need
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://sebastiengilbert.medium.com/?source=post_page-----ce8eaca3a40a--------------------------------)[![Sébastien
    Gilbert](../Images/380f6588c3ef718947bcf82061f190eb.png)](https://sebastiengilbert.medium.com/?source=post_page-----ce8eaca3a40a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ce8eaca3a40a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ce8eaca3a40a--------------------------------)
    [Sébastien Gilbert](https://sebastiengilbert.medium.com/?source=post_page-----ce8eaca3a40a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F975aef8c496a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstereo-vision-system-for-3d-tracking-ce8eaca3a40a&user=S%C3%A9bastien+Gilbert&userId=975aef8c496a&source=post_page-975aef8c496a----ce8eaca3a40a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ce8eaca3a40a--------------------------------)
    ·6 min read·Jan 8, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fce8eaca3a40a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstereo-vision-system-for-3d-tracking-ce8eaca3a40a&user=S%C3%A9bastien+Gilbert&userId=975aef8c496a&source=-----ce8eaca3a40a---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fce8eaca3a40a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstereo-vision-system-for-3d-tracking-ce8eaca3a40a&source=-----ce8eaca3a40a---------------------bookmark_footer-----------)![](../Images/729c84682441f3dacaf348fbf88094b2.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Adriano Pinto](https://unsplash.com/@adrianopintofotografia?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Like the vast majority of sighted animals on this planet, we have two eyes.
    This marvelous feature of our evolution allows us to see the environment in three
    dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: To obtain 3D information from a scene, we can mimic binocular vision with at
    least two cameras working together. Such a setup is called a *stereo vision system*.
    When properly calibrated, each camera supplies a constraint on the 3D coordinates
    of a given feature point. With at least two calibrated cameras, it is possible
    to calculate the 3D coordinates of the feature point.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we’ll calibrate a pair of cameras, and use this calibration
    to compute the 3D coordinates of a feature point tracked in a series of images.
    You can find the corresponding code [in this repository](https://github.com/sebastiengilbert73/tutorial_calibrate_stereo_vision).
  prefs: []
  type: TYPE_NORMAL
- en: Stereo Vision System
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Figure 1 shows the stereo vision system that I have been using. As you can
    see, nothing fancy: a pair of webcams, rigidly held together with 3D printed parts,
    tie wraps, and hot glue.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b1399212d3f0a0942471c8ec2d90d44.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The stereo vision system. Image by the author.'
  prefs: []
  type: TYPE_NORMAL
- en: It just happened that I had two webcams of the same model lying around in a
    drawer, but it is not necessary to have identical cameras. Since the cameras get
    calibrated independently, they can have different intrinsic parameters and still
    play their role in a stereo vision system.
  prefs: []
  type: TYPE_NORMAL
- en: Calibration of the Cameras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Projection Matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The calibration of the cameras boils down to computing their *projection matrix*.
    The projection matrix of a pinhole camera model is a 3x4 matrix that allows one
    to calculate the pixel coordinates of a feature point from its 3D coordinates
    in a world reference frame.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/585d80834287ff068b7fcf88e8bf8f85.png)'
  prefs: []
  type: TYPE_IMG
- en: In equation (1), *i* refers to the camera index. In a system of two cameras,
    *i* belongs to {1, 2}.
  prefs: []
  type: TYPE_NORMAL
- en: Pᵢ is the 3x4 projection matrix of camera *i*.
  prefs: []
  type: TYPE_NORMAL
- en: (uᵢ, vᵢ) are the pixel coordinates of the feature point, as seen from camera
    *i*. (X, Y, Z) are the feature point 3D coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: The scalar λᵢ is the scaling factor that preserves the equation homogeneity
    (i.e. the last element of the vectors on both sides is 1). Its presence comes
    from the loss of information that happens when a 3D point is projected on a 2D
    plane (the camera sensor), as multiple 3D points map to the same 2D point.
  prefs: []
  type: TYPE_NORMAL
- en: Assuming we know the projection matrix of the cameras and we have the pixel
    coordinates of a feature point, our goal is to isolate (X, Y, Z).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9fa9e7c101aea9e4ace8db3af43626e6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Inserting (3) into the first two rows of (2):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/72b8b49fffbc2ddccd9b62922d9fc254.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation (6) shows that each camera view supplies us with two linear equations
    in 3 unknowns (X, Y, Z). If we have at least two camera views of the same 3D point,
    we can compute the 3D coordinates by solving an overdetermined system of linear
    equations.
  prefs: []
  type: TYPE_NORMAL
- en: Great! But how do we compute the projection matrices?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Computation of the Projection Matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To compute the projection matrix of a camera, we need a large number of known
    3D points and their corresponding pixel coordinates. We use a checkerboard calibration
    pattern placed as precisely as possible at measured distances from the stereo
    rig.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b8fe2ea5058b6c7391ff242ad89134ce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The checkerboard calibration pattern at a known location. Image by
    the author.'
  prefs: []
  type: TYPE_NORMAL
- en: The [repository includes the calibration pattern images](https://github.com/sebastiengilbert73/tutorial_calibrate_stereo_vision/tree/main/calibration_images),
    for both cameras.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/990076df5538be05eec67d75df4db8cc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Images of the checkerboard calibration pattern, at 60 cm from the
    cameras. Image by the author.'
  prefs: []
  type: TYPE_NORMAL
- en: You can run the whole calibration procedure with [this python program](https://github.com/sebastiengilbert73/tutorial_calibrate_stereo_vision/blob/main/calibrate_stereo.py).
  prefs: []
  type: TYPE_NORMAL
- en: The square intersections in each calibration image get detected with an instance
    of the class CheckerboardIntersections, which we introduced in [my previous article](https://medium.com/towards-data-science/camera-radial-distortion-compensation-with-gradient-descent-22728487acb1).
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we set the intersection detector parameters to be relatively sensitive,
    such that all the real intersections are detected, plus a manageable amount of
    false positives (in other words, perfect recall, reasonable precision). Since
    the calibration is a process that we’ll only do once, we can afford the work of
    manually removing the false positives. The [program](https://github.com/sebastiengilbert73/tutorial_calibrate_stereo_vision/blob/main/calibrate_stereo.py)
    iterates over the calibration pattern images and asks the user to select the false
    positives.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/86b556356698bc4af90e7ed329d0a251.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Found intersection points, before and after the manual removal of
    false positives. Image by the author.'
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in [my article about the compensation of camera radial distortion](https://medium.com/towards-data-science/camera-radial-distortion-compensation-with-gradient-descent-22728487acb1),
    the raw intersection coordinates must be undistorted. The radial distortion model
    was previously computed for both cameras, and [the corresponding files are already
    in the repository](https://github.com/sebastiengilbert73/tutorial_calibrate_stereo_vision/tree/main/radial_distortion).
    The undistorted coordinates are the ones that we’ll use to build the projection
    matrix of a pinhole camera model.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we have 7 (captures at different distances) x 6 x 6 (square intersection
    points) = 252 correspondences between a 3D point and pixel coordinates for each
    camera.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute the entries of the projection matrix, we’ll start again with equation
    (1), but this time assuming we know (u, v)ₖ and (X, Y, Z)ₖ, and we want to solve
    for the entries of P. The subscript k refers to the index of the pair (pixel_coords,
    XYZ_world). The scalars λₖ (one for each point) are also unknown. We can eliminate
    the λₖ from the system of linear equations with a bit of manipulation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ee0b9873ccf2aee89a90c47ef0184a24.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Equations (8) and (9) can get written in the form Ap = 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3ce5bc073f325cafb6148b39d12132a0.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation (10) shows that each correspondence supplies two homogeneous linear
    equations in 12 unknowns. At least 6 correspondences are necessary to solve for
    the entries of P. We also need our 3D points to be non-coplanar. With 252 correspondences
    from 7 planes, we are safe.
  prefs: []
  type: TYPE_NORMAL
- en: After execution of the calibration program, we can verify that the projection
    matrices correctly project the known 3D points back to their undistorted pixel
    coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f17937b96be652b55561e8d60969356e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Projection of the checkerboard 3D points in the images. Image by
    the author.'
  prefs: []
  type: TYPE_NORMAL
- en: In Figure 5, the blue dots are the points found by the intersection detector,
    after compensating for the radial distortion. The yellow circles are the projections
    of the 3D points in the image. We can see that the projection matrices of both
    cameras do a good job. Note that the excentric annotated points do not coincide
    with the checkerboard intersections, due to the radial distortion compensation.
  prefs: []
  type: TYPE_NORMAL
- en: 3D Tracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can now use our calibrated stereo system to compute the 3D location of a
    feature point. To demonstrate that, we’ll track an easy-to-detect feature point
    (the center of a red square) in a series of images.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the tracking program [here](https://github.com/sebastiengilbert73/tutorial_calibrate_stereo_vision/blob/main/track_red_square.py).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b33e6e53aba1d639ffe4249adee0a68f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Left: An image of the tracked red square. Right: The detected blob.
    Image by the author.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6 shows an example of the detection of the tracked red square. In a few
    words, the square is tracked by first identifying the regions of the images where
    the blue component is dominant since the area around the red square is blue. A
    region where the red component dominates, within the blue-dominated area, is then
    found. For the details, please refer to [the code](https://github.com/sebastiengilbert73/tutorial_calibrate_stereo_vision/blob/main/utilities/red_square.py).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6c6d694dc3406e8bb5a50b50e6b03fd6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The center of the red square gets tracked in 3D. The coordinates
    are in cm. Animation by the author.'
  prefs: []
  type: TYPE_NORMAL
- en: Using the undistorted pixel coordinates from both cameras, and their corresponding
    projection matrices, the 3D location of the feature point can be computed for
    each image in the series, as displayed in the animation above.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We built a simple stereo vision system with a pair of webcams. We calibrated
    both cameras by compensating for their radial distortion and computing their projection
    matrices. We could use our calibrated system to track the 3D location of a feature
    point in a series of images.
  prefs: []
  type: TYPE_NORMAL
- en: Please feel free to experiment with [the code](https://github.com/sebastiengilbert73/tutorial_calibrate_stereo_vision).
  prefs: []
  type: TYPE_NORMAL
- en: '**If you have an application of stereo vision in mind, let me know, I would
    be very interested to hear about it!**'
  prefs: []
  type: TYPE_NORMAL
