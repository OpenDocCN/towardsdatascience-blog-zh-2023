- en: Implementing Custom Loss Functions in PyTorch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 PyTorch 中实现自定义损失函数
- en: 原文：[https://towardsdatascience.com/implementing-custom-loss-functions-in-pytorch-50739f9e0ee1?source=collection_archive---------4-----------------------#2023-01-16](https://towardsdatascience.com/implementing-custom-loss-functions-in-pytorch-50739f9e0ee1?source=collection_archive---------4-----------------------#2023-01-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/implementing-custom-loss-functions-in-pytorch-50739f9e0ee1?source=collection_archive---------4-----------------------#2023-01-16](https://towardsdatascience.com/implementing-custom-loss-functions-in-pytorch-50739f9e0ee1?source=collection_archive---------4-----------------------#2023-01-16)
- en: Understanding the theory and implementation of custom loss functions in PyTorch
    using the MNIST dataset
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 MNIST 数据集理解 PyTorch 中自定义损失函数的理论和实现
- en: '[](https://marcosanguineti.medium.com/?source=post_page-----50739f9e0ee1--------------------------------)[![Marco
    Sanguineti](../Images/9c426e512b31b77734801912d81f51c1.png)](https://marcosanguineti.medium.com/?source=post_page-----50739f9e0ee1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----50739f9e0ee1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----50739f9e0ee1--------------------------------)
    [Marco Sanguineti](https://marcosanguineti.medium.com/?source=post_page-----50739f9e0ee1--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://marcosanguineti.medium.com/?source=post_page-----50739f9e0ee1--------------------------------)[![Marco
    Sanguineti](../Images/9c426e512b31b77734801912d81f51c1.png)](https://marcosanguineti.medium.com/?source=post_page-----50739f9e0ee1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----50739f9e0ee1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----50739f9e0ee1--------------------------------)
    [Marco Sanguineti](https://marcosanguineti.medium.com/?source=post_page-----50739f9e0ee1--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33141be0f14d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-custom-loss-functions-in-pytorch-50739f9e0ee1&user=Marco+Sanguineti&userId=33141be0f14d&source=post_page-33141be0f14d----50739f9e0ee1---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----50739f9e0ee1--------------------------------)
    ·12 min read·Jan 16, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F50739f9e0ee1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-custom-loss-functions-in-pytorch-50739f9e0ee1&user=Marco+Sanguineti&userId=33141be0f14d&source=-----50739f9e0ee1---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33141be0f14d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-custom-loss-functions-in-pytorch-50739f9e0ee1&user=Marco+Sanguineti&userId=33141be0f14d&source=post_page-33141be0f14d----50739f9e0ee1---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----50739f9e0ee1--------------------------------)
    ·12 min 阅读·2023年1月16日'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F50739f9e0ee1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-custom-loss-functions-in-pytorch-50739f9e0ee1&source=-----50739f9e0ee1---------------------bookmark_footer-----------)![](../Images/0b3072c7dfbedca6f5144d9f7eb88a7c.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F50739f9e0ee1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-custom-loss-functions-in-pytorch-50739f9e0ee1&source=-----50739f9e0ee1---------------------bookmark_footer-----------)![](../Images/0b3072c7dfbedca6f5144d9f7eb88a7c.png)'
- en: Photo by [Markus Winkler](https://unsplash.com/es/@markuswinkler?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Markus Winkler](https://unsplash.com/es/@markuswinkler?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) 提供
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: In machine learning, the loss function is a critical component that measures
    the difference between the predicted output and the actual output. It plays a
    vital role in the training of a model, as it guides the optimization process by
    indicating the direction in which the model should improve. The choice of loss
    function depends on the specific task and the data type. In this article, we will
    delve into the theory and implementation of custom loss functions in PyTorch,
    using the MNIST dataset for digit classification as an example.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，损失函数是一个关键组成部分，用于衡量预测输出与实际输出之间的差异。它在模型训练中起着至关重要的作用，因为它通过指示模型应改进的方向来指导优化过程。损失函数的选择依赖于具体任务和数据类型。本文将深入探讨PyTorch中自定义损失函数的理论和实现，以MNIST数据集的数字分类为例。
- en: The MNIST dataset is a widely used dataset for image classification tasks, it
    contains 70,000 images of handwritten digits, each with a resolution of 28x28
    pixels. The task is to classify these images into one of the 10 digits (0–9).
    This task aims to train a model that can accurately classify new images of handwritten
    digits, based on the training examples provided in the MNIST dataset.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST数据集是一个广泛使用的图像分类数据集，包含70,000张手写数字图像，每张图像的分辨率为28x28像素。任务是将这些图像分类为10个数字中的一个（0–9）。该任务旨在训练一个模型，使其能够准确地分类新的手写数字图像，基于MNIST数据集中提供的训练示例。
- en: '![](../Images/06d53b28de548088516452652c919f9c.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06d53b28de548088516452652c919f9c.png)'
- en: Photo by [Carlos Muza](https://unsplash.com/fr/@kmuza?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Carlos Muza](https://unsplash.com/fr/@kmuza?utm_source=medium&utm_medium=referral)
    拍摄，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: A typical approach for this task is to use a multi-class logistic regression
    model, which is a softmax classifier. The softmax function maps the output of
    the model to a probability distribution over the 10 classes. The cross-entropy
    loss is commonly used as the loss function for this type of model. The cross-entropy
    loss calculates the difference between the predicted probability distribution
    and the actual probability distribution.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这个任务的一个典型方法是使用多类逻辑回归模型，即软最大分类器。软最大函数将模型的输出映射到10个类别的概率分布上。交叉熵损失通常作为这种模型的损失函数。交叉熵损失计算预测概率分布与实际概率分布之间的差异。
- en: However, in some cases, the cross-entropy loss may not be the best choice for
    a particular task. For example, consider a scenario where the cost of misclassifying
    certain classes is much higher than others. In such cases, it is necessary to
    use a custom loss function that takes into account the relative importance of
    each class.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在某些情况下，交叉熵损失可能不是特定任务的最佳选择。例如，考虑一个场景，其中误分类某些类别的代价远高于其他类别。在这种情况下，有必要使用自定义损失函数来考虑每个类别的相对重要性。
- en: In this article, I will show you how to implement a custom loss function for
    MNIST dataset, where the cost of misclassifying the digit 9 is much higher than
    the other digits. We will use Pytorch as the framework, and we will start by discussing
    the theory behind the custom loss function, then we will show the implementation
    of the custom loss function using Pytorch. Finally, we will use the custom loss
    function to train a linear model on the MNIST dataset and we will evaluate the
    performance of the model.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将展示如何为MNIST数据集实现自定义损失函数，其中误分类数字9的代价远高于其他数字。我们将使用Pytorch作为框架，首先讨论自定义损失函数背后的理论，然后展示如何使用Pytorch实现自定义损失函数。最后，我们将使用自定义损失函数在MNIST数据集上训练一个线性模型，并评估模型的性能。
- en: 'Custom Loss function: why'
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义损失函数：原因
- en: 'Implementing custom loss functions is important for several reasons:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 实现自定义损失函数非常重要，原因有几个：
- en: '**Problem-specific**: The choice of loss function depends on the specific task
    and the type of data. Custom loss functions can be designed to better suit the
    characteristics of the problem at hand, resulting in improved model performance.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**特定问题**：损失函数的选择依赖于具体任务和数据类型。可以设计自定义损失函数，以更好地适应问题的特征，从而提高模型性能。'
- en: '**Class imbalance**: In many real-world datasets, the number of samples in
    each class can be very different. A custom loss function can be designed to take
    into account the class imbalance and assign different costs to different classes.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**类别不平衡**：在许多实际数据集中，每个类别的样本数量可能差异很大。可以设计自定义损失函数以考虑类别不平衡，并对不同类别分配不同的代价。'
- en: '**Cost-sensitive**: In some tasks, the cost of misclassifying certain classes
    may be much higher than others. A custom loss function can be designed to take
    into account the relative importance of each class, resulting in a more robust
    model.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**成本敏感**：在一些任务中，错误分类某些类别的成本可能远高于其他类别。可以设计自定义损失函数以考虑每个类别的相对重要性，从而得到一个更鲁棒的模型。'
- en: '**Multi-task learning**: Custom loss functions can be designed to handle multiple
    tasks simultaneously. This can be useful in cases where a single model is required
    to perform multiple related tasks.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**多任务学习**：自定义损失函数可以设计成同时处理多个任务。这在需要一个模型执行多个相关任务的情况下非常有用。'
- en: '**Regularization**: Custom loss functions can also be used for regularization,
    which helps to prevent overfitting and improve the generalization of the model.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**正则化**：自定义损失函数也可以用于正则化，这有助于防止过拟合并提高模型的泛化能力。'
- en: '**Adversarial training**: Custom loss functions can also be used to train models
    to be robust against adversarial attacks.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**对抗训练**：自定义损失函数还可以用于训练模型，使其对对抗攻击具有鲁棒性。'
- en: In summary, custom loss functions can provide a way to better optimize the model
    for a specific problem and can provide better performance and generalization.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，自定义损失函数可以提供一种更好地优化模型以适应特定问题的方法，并能提供更好的性能和泛化能力。
- en: Custom Loss function in PyTorch
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch中的自定义损失函数
- en: The MNIST dataset contains 70,000 images of handwritten digits, each with a
    resolution of 28x28 pixels. The task is to classify these images into one of the
    10 digits (0–9). The typical approach for this task is to use a multi-class logistic
    regression model, which is a softmax classifier. The softmax function maps the
    output of the model to a probability distribution over the 10 classes. The cross-entropy
    loss is commonly used as the loss function for this type of model.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 数据集包含70,000张手写数字图像，每张图像的分辨率为28x28像素。任务是将这些图像分类为10个数字中的一个（0–9）。这种任务的典型方法是使用多类逻辑回归模型，即softmax分类器。softmax函数将模型的输出映射到10个类别的概率分布上。交叉熵损失通常用作这种类型模型的损失函数。
- en: 'Cross-entropy loss calculates the difference between the predicted probability
    distribution and the actual probability distribution. The predicted probability
    distribution is obtained by applying the softmax function to the output of the
    model. The actual probability distribution is a one-hot vector, where the value
    of the element corresponding to the correct class is 1 and the values of the other
    elements are 0\. The cross-entropy loss is defined as:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵损失计算预测概率分布和实际概率分布之间的差异。预测概率分布是通过对模型的输出应用softmax函数获得的。实际概率分布是一个one-hot向量，其中对应正确类别的元素值为1，其他元素的值为0。交叉熵损失定义为：
- en: '**L = -∑(y_i * log(p_i))**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**L = -∑(y_i * log(p_i))**'
- en: where y_i is the actual probability of class i and p_i is the predicted probability
    of class i.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 其中y_i是类别i的实际概率，p_i是类别i的预测概率。
- en: However, in some cases, the cross-entropy loss may not be the best choice for
    a particular task. For example, consider a scenario where the cost of misclassifying
    certain classes is much higher than others. In such cases, it is necessary to
    use a custom loss function that takes into account the relative importance of
    each class.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在某些情况下，交叉熵损失可能不是特定任务的最佳选择。例如，考虑一种场景，其中错误分类某些类别的成本远高于其他类别。在这种情况下，有必要使用自定义损失函数来考虑每个类别的相对重要性。
- en: In PyTorch, custom loss functions can be implemented by creating a subclass
    of the nn.Module class and overriding the forward method. The forward method takes
    as input the predicted output and the actual output and returns the value of the
    loss.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中，自定义损失函数可以通过创建`nn.Module`类的子类并重写`forward`方法来实现。`forward`方法以预测输出和实际输出为输入，并返回损失的值。
- en: 'Here is an example of a custom loss function for the MNIST classification task,
    where the cost of misclassifying the digit 9 is much higher than the other digits:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个针对MNIST分类任务的自定义损失函数示例，其中错误分类数字9的成本远高于其他数字：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this example, we first calculate the cross-entropy loss using the nn.CrossEntropyLoss()
    function. Next, we create a mask that is 1 for samples that belong to class 9
    and 0 for the other samples. We then calculate the mean loss for the samples that
    belong to class 9\. Finally, we add this high cost loss to the original loss to
    obtain the final loss.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们首先使用 nn.CrossEntropyLoss() 函数计算交叉熵损失。接下来，我们创建一个掩码，对于属于 9 类的样本掩码值为 1，对于其他样本掩码值为
    0。然后，我们计算属于 9 类的样本的平均损失。最后，我们将这个高成本的损失添加到原始损失中，以获得最终损失。
- en: 'To use the custom loss function, we need to instantiate it and pass it as the
    argument to the criterion parameter of the optimizer in the training loop. Here
    is an example of how to use the custom loss function for training a model on the
    MNIST dataset:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用自定义损失函数，我们需要实例化它并将其作为参数传递给训练循环中的优化器的 criterion 参数。以下是如何使用自定义损失函数来训练 MNIST
    数据集模型的示例：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This code is an implementation of a custom loss function for the MNIST dataset
    in PyTorch. The MNIST dataset contains 70,000 images of handwritten digits, each
    with a resolution of 28x28 pixels. The task is to classify these images into one
    of the 10 digits (0–9).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码实现了一个自定义损失函数，用于 PyTorch 中的 MNIST 数据集。MNIST 数据集包含 70,000 张手写数字图像，每张图像的分辨率为
    28x28 像素。任务是将这些图像分类为 10 个数字（0–9）中的一个。
- en: The first block of code creates a custom loss function called “CustomLoss” by
    subclassing the PyTorch nn.Module. It has a forward method that takes in two inputs;
    the output of the model and the target label. The forward method first converts
    the target label to a tensor of long integers. Then it creates an instance of
    the built-in PyTorch cross-entropy loss function and uses it to calculate the
    loss between the model’s output and the target label. Next, it creates a mask
    that identifies the target label that is equal to 9, then it multiplies the loss
    by this mask and calculates the mean of the resulting tensor. Finally, it returns
    the sum of the original loss and the mean of the high-cost loss.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个代码块通过继承 PyTorch 的 nn.Module 创建了一个名为 “CustomLoss” 的自定义损失函数。它有一个 forward 方法，接受两个输入：模型的输出和目标标签。forward
    方法首先将目标标签转换为长整型张量。然后，它创建一个内置的 PyTorch 交叉熵损失函数的实例，并使用它计算模型输出与目标标签之间的损失。接下来，它创建一个掩码，以识别目标标签是否等于
    9，然后将损失乘以这个掩码，并计算结果张量的平均值。最后，它返回原始损失与高成本损失的平均值之和。
- en: The next block of code loads the MNIST dataset using PyTorch’s built-in data-loading
    utilities. The train_loader loads the training dataset and applies the specified
    transforms to the images, such as converting the images to tensors and normalizing
    the pixel values. The test_loader loads the test dataset and applies the same
    transforms.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个代码块使用 PyTorch 的内置数据加载工具加载 MNIST 数据集。train_loader 加载训练数据集并对图像应用指定的变换，例如将图像转换为张量和规范化像素值。test_loader
    加载测试数据集，并应用相同的变换。
- en: The following block of code defines a convolutional neural network (CNN) called
    “Net” by subclassing the PyTorch nn.Module. The CNN consists of 2 convolutional
    layers, 2 linear layers, and some dropout layers for regularization. The forward
    method of the Net class applies the convolutional and linear layers in sequence,
    passing the output through a ReLU activation function and max pooling layers.
    It also applies dropout layers to the output and returns the log-softmax of the
    final output.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块通过继承 PyTorch 的 nn.Module 定义了一个卷积神经网络（CNN），称为 “Net”。该 CNN 包含 2 个卷积层、2 个线性层，以及一些用于正则化的
    dropout 层。Net 类的 forward 方法按顺序应用卷积层和线性层，通过 ReLU 激活函数和最大池化层传递输出。它还对输出应用 dropout
    层，并返回最终输出的 log-softmax。
- en: The next block of code creates an instance of the Net class, an optimizer (stochastic
    gradient descent), and an instance of the custom loss function.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个代码块创建了一个 Net 类的实例，一个优化器（随机梯度下降），以及一个自定义损失函数的实例。
- en: The final block of code is the training loop, where the model is trained for
    10 epochs. In each epoch, the model iterates over the training dataset, passing
    the images through the network, calculating the loss using the custom loss function
    and backpropagating the gradients. Then it updates the model’s parameters using
    the optimizer. It also tracks the training loss and test loss and periodically
    prints the current loss to the console. Additionally, it creates a new directory
    called “results” to store the results and outputs of the training process.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一块代码是训练循环，其中模型训练了10个周期。在每个周期中，模型遍历训练数据集，将图像通过网络，使用自定义损失函数计算损失并反向传播梯度。然后，它使用优化器更新模型的参数。同时，它跟踪训练损失和测试损失，并定期将当前损失打印到控制台。此外，它还创建了一个名为“results”的新目录来存储训练过程的结果和输出。
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/93db5d18c905850d1b5ab433f420342b.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93db5d18c905850d1b5ab433f420342b.png)'
- en: Custom loss trend — Image by Author
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义损失趋势 — 图片由作者提供
- en: This code is creating a plot of the custom loss function for the MNIST dataset
    during the training process. The plot will display the custom loss for both the
    training set and the test set.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码正在为MNIST数据集创建自定义损失函数的图形。该图形将显示训练集和测试集的自定义损失。
- en: It starts by importing the Matplotlib library, which is a plotting library for
    Python. Then, it creates a figure object with a specified size using the `plt.figure()`
    function.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 它首先导入了Matplotlib库，这是一个用于Python的绘图库。然后，使用`plt.figure()`函数创建了一个指定大小的图形对象。
- en: The next line of code plots the custom loss for the training set using the `plt.plot()`
    function. It uses the `train_counter` and `train_losses` variables as the x and
    y-axis values, respectively. The color of the plot is set to blue using the `color`
    parameter.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 下一行代码使用`plt.plot()`函数绘制训练集的自定义损失。它使用`train_counter`和`train_losses`变量分别作为x轴和y轴的值。图的颜色通过`color`参数设置为蓝色。
- en: Then, it plots the custom loss for the test set using the `plt.scatter()` function.
    It uses the `test_counter` and `test_losses` variables as the x and y-axis values,
    respectively. The colour of the plot is set to red using the `color` parameter.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，它使用`plt.scatter()`函数绘制测试集的自定义损失。它使用`test_counter`和`test_losses`变量分别作为x轴和y轴的值。图的颜色通过`color`参数设置为红色。
- en: The `plt.legend()` function adds a legend to the plot, indicating which plot
    corresponds to the train loss and which corresponds to the test loss. The `loc`
    parameter is set to 'upper right' which means the legend will be located at the
    upper right corner of the plot.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`plt.legend()`函数为图形添加图例，指明哪个图代表训练损失，哪个图代表测试损失。`loc`参数设置为''upper right''，这意味着图例将位于图形的右上角。'
- en: The `plt.xlabel()` and `plt.ylabel()` functions add labels to the x and y-axis
    of the plot, respectively. The x-axis label is set to 'number of training examples
    seen' and the y-axis label is set to 'Custom loss'.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`plt.xlabel()`和`plt.ylabel()`函数分别为图形的x轴和y轴添加标签。x轴标签设置为''number of training
    examples seen''，y轴标签设置为''Custom loss''。'
- en: Finally, the `plt.show()` function is used to display the plot.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用`plt.show()`函数显示图形。
- en: This code will display a plot that shows the custom loss function over the training
    examples seen. The blue line represents the custom loss for the training set,
    and the red dots represent the custom loss for the test set. The plot will allow
    you to see how the custom loss function is behaving during the training process,
    and evaluate the performance of the model.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将显示一个图形，展示自定义损失函数在训练样本中的变化情况。蓝色线条代表训练集的自定义损失，红色点代表测试集的自定义损失。这个图形将帮助你观察自定义损失函数在训练过程中的表现，并评估模型的性能。
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/dff9371acd8f7041ea0389b7070f948c.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dff9371acd8f7041ea0389b7070f948c.png)'
- en: Test set samples and prediction — Image by Author
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集样本和预测 — 图片由作者提供
- en: This code is displaying a figure with 6 images from the test set and their corresponding
    predictions made by the trained network.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码显示了来自测试集的6张图像及其对应的训练网络的预测结果。
- en: It starts by using the `enumerate()` function to loop over the test_loader,
    which is an iterator that loads the test dataset in batches. The `next()` function
    is used to get the first batch of examples from the test set.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 它开始使用`enumerate()`函数循环遍历test_loader，这是一个按批次加载测试数据集的迭代器。使用`next()`函数获取测试集的第一个批次样本。
- en: The `example_data` variable contains the images and the `example_targets` variable
    contains the corresponding labels.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`example_data` 变量包含图像，`example_targets` 变量包含相应的标签。'
- en: Then it uses the Pytorch’s `torch.no_grad()` function, it is used to temporarily
    set the requires_grad flag to false. It will reduce memory usage and speed up
    computations but also it will not track the operations.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用 PyTorch 的 `torch.no_grad()` 函数，该函数用于暂时将 requires_grad 标志设置为 False。它将减少内存使用并加速计算，但也不会跟踪操作。
- en: The next block of code creates a new figure object using the `plt.figure()`
    function. Then, it uses a for loop to iterate over the first 6 examples in the
    test set. For each example, it creates a subplot in the current figure using the
    `plt.subplot()` function. The `plt.tight_layout()` function is used to adjust
    the spacing between the subplots.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的代码块使用 `plt.figure()` 函数创建一个新的图形对象。然后，使用 for 循环遍历测试集中的前 6 个示例。对于每个示例，它使用
    `plt.subplot()` 函数在当前图形中创建一个子图。`plt.tight_layout()` 函数用于调整子图之间的间距。
- en: Then it uses the `plt.imshow()` function to display the image in the current
    subplot. The `cmap` parameter is set to 'gray' to display the image in grayscale
    and the `interpolation` parameter is set to 'none' to display the image without
    any interpolation.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然后它使用 `plt.imshow()` 函数在当前子图中显示图像。`cmap` 参数设置为 'gray' 以灰度显示图像，`interpolation`
    参数设置为 'none' 以无插值显示图像。
- en: The `plt.title()` function is used to add a title to the current subplot. The
    title shows the prediction made by the network for the current example. The output
    of the network is passed through the `output.data.max(1, keepdim=True)[1]` which
    returns the index of the predicted class. The `[i].item()` extracts the integer
    value of the predicted class.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`plt.title()` 函数用于为当前子图添加标题。标题显示了网络对当前示例的预测结果。网络的输出通过 `output.data.max(1, keepdim=True)[1]`
    得到预测类别的索引。`[i].item()` 提取了预测类别的整数值。'
- en: The `plt.xticks()` and `plt.yticks()` functions are used to remove the x and
    y-axis ticks from the current subplot, respectively.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`plt.xticks()` 和 `plt.yticks()` 函数分别用于去除当前子图的 x 和 y 轴刻度。'
- en: Finally, the `plt.show()` function is used to display the figure. This code
    will display a figure with 6 images from the test set and their corresponding
    predictions made by the trained network. The images are displayed in grayscale
    and without any interpolation, and the predicted class is displayed as a title
    above each image. This can be a useful tool for visualizing the model’s performance
    on the test set and identifying any potential issues or misclassification.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用 `plt.show()` 函数来显示图像。此代码将显示一个包含 6 张来自测试集的图像及其由训练网络生成的预测结果的图形。图像以灰度显示且没有任何插值，预测的类别以标题形式显示在每张图像上方。这是一个有用的工具，可以用来可视化模型在测试集上的表现，识别潜在问题或误分类。
- en: Greetings
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问候
- en: In this article, we have discussed the theory and implementation of custom loss
    functions in PyTorch, using the MNIST dataset for digit classification as an example.
    We have shown how to create a custom loss function by subclassing the nn.Module
    class and overridding the forward method. We have also provided an example of
    how to use the custom loss function for training a model on the MNIST dataset.
    Custom loss functions can be useful in scenarios where the cost of misclassifying
    certain classes is much higher than others. It is important to note that care
    should be taken when implementing custom loss functions, as they can have a significant
    impact on the performance of the model.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们讨论了 PyTorch 中自定义损失函数的理论和实现，使用 MNIST 数据集进行数字分类作为例子。我们展示了如何通过子类化 nn.Module
    类并重写 forward 方法来创建自定义损失函数。我们还提供了一个如何使用自定义损失函数在 MNIST 数据集上训练模型的示例。在某些类别误分类的成本远高于其他类别的场景中，自定义损失函数可能非常有用。需要注意的是，实施自定义损失函数时应谨慎，因为它们可能对模型性能产生重大影响。
- en: Join Medium Membership
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入 Medium 会员
- en: If you enjoyed this article and want to keep learning more about this topic,
    I invite you to join Medium membership at this [link](https://marcosanguineti.medium.com/membership).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这篇文章并希望继续了解更多相关内容，我邀请你通过此[链接](https://marcosanguineti.medium.com/membership)加入
    Medium 会员。
- en: '[](https://marcosanguineti.medium.com/membership?source=post_page-----50739f9e0ee1--------------------------------)
    [## Join Medium with my referral link — Marco Sanguineti'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://marcosanguineti.medium.com/membership?source=post_page-----50739f9e0ee1--------------------------------)
    [## 通过我的推荐链接加入 Medium — Marco Sanguineti'
- en: Read every story from Marco Sanguineti (and thousands of other writers on Medium).
    Investment in culture is the best…
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阅读 Marco Sanguineti 的每一篇故事（以及 Medium 上成千上万其他作家的故事）。对文化的投资是最好的……
- en: marcosanguineti.medium.com](https://marcosanguineti.medium.com/membership?source=post_page-----50739f9e0ee1--------------------------------)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[marcosanguineti.medium.com](https://marcosanguineti.medium.com/membership?source=post_page-----50739f9e0ee1--------------------------------)'
- en: By becoming a member, you’ll have access to a wider variety of high-quality
    content, and exclusive access to member-only stories, and you’ll be supporting
    independent writers and creators like myself. Plus, as a member, you’ll be able
    to highlight your favourite passages, save stories for later, and get personalized
    reading recommendations. Sign up today and let’s continue exploring this topic
    and others together.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 成为会员后，你将能够访问更多种类的高质量内容，并获得专属会员故事的访问权限，同时你还将支持像我这样独立的作家和创作者。此外，作为会员，你可以高亮你喜欢的段落，保存故事以便稍后阅读，并获得个性化的阅读推荐。今天就注册，和我们一起继续探索这个话题和其他话题吧。
- en: Thank you for your support! Until next,
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你的支持！下次见，
- en: Marco
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔科
