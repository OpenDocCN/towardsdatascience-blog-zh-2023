- en: Implementing Custom Loss Functions in PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/implementing-custom-loss-functions-in-pytorch-50739f9e0ee1?source=collection_archive---------4-----------------------#2023-01-16](https://towardsdatascience.com/implementing-custom-loss-functions-in-pytorch-50739f9e0ee1?source=collection_archive---------4-----------------------#2023-01-16)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Understanding the theory and implementation of custom loss functions in PyTorch
    using the MNIST dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://marcosanguineti.medium.com/?source=post_page-----50739f9e0ee1--------------------------------)[![Marco
    Sanguineti](../Images/9c426e512b31b77734801912d81f51c1.png)](https://marcosanguineti.medium.com/?source=post_page-----50739f9e0ee1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----50739f9e0ee1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----50739f9e0ee1--------------------------------)
    [Marco Sanguineti](https://marcosanguineti.medium.com/?source=post_page-----50739f9e0ee1--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33141be0f14d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-custom-loss-functions-in-pytorch-50739f9e0ee1&user=Marco+Sanguineti&userId=33141be0f14d&source=post_page-33141be0f14d----50739f9e0ee1---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----50739f9e0ee1--------------------------------)
    ·12 min read·Jan 16, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F50739f9e0ee1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-custom-loss-functions-in-pytorch-50739f9e0ee1&user=Marco+Sanguineti&userId=33141be0f14d&source=-----50739f9e0ee1---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F50739f9e0ee1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-custom-loss-functions-in-pytorch-50739f9e0ee1&source=-----50739f9e0ee1---------------------bookmark_footer-----------)![](../Images/0b3072c7dfbedca6f5144d9f7eb88a7c.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Markus Winkler](https://unsplash.com/es/@markuswinkler?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In machine learning, the loss function is a critical component that measures
    the difference between the predicted output and the actual output. It plays a
    vital role in the training of a model, as it guides the optimization process by
    indicating the direction in which the model should improve. The choice of loss
    function depends on the specific task and the data type. In this article, we will
    delve into the theory and implementation of custom loss functions in PyTorch,
    using the MNIST dataset for digit classification as an example.
  prefs: []
  type: TYPE_NORMAL
- en: The MNIST dataset is a widely used dataset for image classification tasks, it
    contains 70,000 images of handwritten digits, each with a resolution of 28x28
    pixels. The task is to classify these images into one of the 10 digits (0–9).
    This task aims to train a model that can accurately classify new images of handwritten
    digits, based on the training examples provided in the MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/06d53b28de548088516452652c919f9c.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Carlos Muza](https://unsplash.com/fr/@kmuza?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: A typical approach for this task is to use a multi-class logistic regression
    model, which is a softmax classifier. The softmax function maps the output of
    the model to a probability distribution over the 10 classes. The cross-entropy
    loss is commonly used as the loss function for this type of model. The cross-entropy
    loss calculates the difference between the predicted probability distribution
    and the actual probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: However, in some cases, the cross-entropy loss may not be the best choice for
    a particular task. For example, consider a scenario where the cost of misclassifying
    certain classes is much higher than others. In such cases, it is necessary to
    use a custom loss function that takes into account the relative importance of
    each class.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I will show you how to implement a custom loss function for
    MNIST dataset, where the cost of misclassifying the digit 9 is much higher than
    the other digits. We will use Pytorch as the framework, and we will start by discussing
    the theory behind the custom loss function, then we will show the implementation
    of the custom loss function using Pytorch. Finally, we will use the custom loss
    function to train a linear model on the MNIST dataset and we will evaluate the
    performance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Custom Loss function: why'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Implementing custom loss functions is important for several reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Problem-specific**: The choice of loss function depends on the specific task
    and the type of data. Custom loss functions can be designed to better suit the
    characteristics of the problem at hand, resulting in improved model performance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Class imbalance**: In many real-world datasets, the number of samples in
    each class can be very different. A custom loss function can be designed to take
    into account the class imbalance and assign different costs to different classes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Cost-sensitive**: In some tasks, the cost of misclassifying certain classes
    may be much higher than others. A custom loss function can be designed to take
    into account the relative importance of each class, resulting in a more robust
    model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Multi-task learning**: Custom loss functions can be designed to handle multiple
    tasks simultaneously. This can be useful in cases where a single model is required
    to perform multiple related tasks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Regularization**: Custom loss functions can also be used for regularization,
    which helps to prevent overfitting and improve the generalization of the model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Adversarial training**: Custom loss functions can also be used to train models
    to be robust against adversarial attacks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In summary, custom loss functions can provide a way to better optimize the model
    for a specific problem and can provide better performance and generalization.
  prefs: []
  type: TYPE_NORMAL
- en: Custom Loss function in PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The MNIST dataset contains 70,000 images of handwritten digits, each with a
    resolution of 28x28 pixels. The task is to classify these images into one of the
    10 digits (0–9). The typical approach for this task is to use a multi-class logistic
    regression model, which is a softmax classifier. The softmax function maps the
    output of the model to a probability distribution over the 10 classes. The cross-entropy
    loss is commonly used as the loss function for this type of model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cross-entropy loss calculates the difference between the predicted probability
    distribution and the actual probability distribution. The predicted probability
    distribution is obtained by applying the softmax function to the output of the
    model. The actual probability distribution is a one-hot vector, where the value
    of the element corresponding to the correct class is 1 and the values of the other
    elements are 0\. The cross-entropy loss is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**L = -∑(y_i * log(p_i))**'
  prefs: []
  type: TYPE_NORMAL
- en: where y_i is the actual probability of class i and p_i is the predicted probability
    of class i.
  prefs: []
  type: TYPE_NORMAL
- en: However, in some cases, the cross-entropy loss may not be the best choice for
    a particular task. For example, consider a scenario where the cost of misclassifying
    certain classes is much higher than others. In such cases, it is necessary to
    use a custom loss function that takes into account the relative importance of
    each class.
  prefs: []
  type: TYPE_NORMAL
- en: In PyTorch, custom loss functions can be implemented by creating a subclass
    of the nn.Module class and overriding the forward method. The forward method takes
    as input the predicted output and the actual output and returns the value of the
    loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of a custom loss function for the MNIST classification task,
    where the cost of misclassifying the digit 9 is much higher than the other digits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we first calculate the cross-entropy loss using the nn.CrossEntropyLoss()
    function. Next, we create a mask that is 1 for samples that belong to class 9
    and 0 for the other samples. We then calculate the mean loss for the samples that
    belong to class 9\. Finally, we add this high cost loss to the original loss to
    obtain the final loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the custom loss function, we need to instantiate it and pass it as the
    argument to the criterion parameter of the optimizer in the training loop. Here
    is an example of how to use the custom loss function for training a model on the
    MNIST dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This code is an implementation of a custom loss function for the MNIST dataset
    in PyTorch. The MNIST dataset contains 70,000 images of handwritten digits, each
    with a resolution of 28x28 pixels. The task is to classify these images into one
    of the 10 digits (0–9).
  prefs: []
  type: TYPE_NORMAL
- en: The first block of code creates a custom loss function called “CustomLoss” by
    subclassing the PyTorch nn.Module. It has a forward method that takes in two inputs;
    the output of the model and the target label. The forward method first converts
    the target label to a tensor of long integers. Then it creates an instance of
    the built-in PyTorch cross-entropy loss function and uses it to calculate the
    loss between the model’s output and the target label. Next, it creates a mask
    that identifies the target label that is equal to 9, then it multiplies the loss
    by this mask and calculates the mean of the resulting tensor. Finally, it returns
    the sum of the original loss and the mean of the high-cost loss.
  prefs: []
  type: TYPE_NORMAL
- en: The next block of code loads the MNIST dataset using PyTorch’s built-in data-loading
    utilities. The train_loader loads the training dataset and applies the specified
    transforms to the images, such as converting the images to tensors and normalizing
    the pixel values. The test_loader loads the test dataset and applies the same
    transforms.
  prefs: []
  type: TYPE_NORMAL
- en: The following block of code defines a convolutional neural network (CNN) called
    “Net” by subclassing the PyTorch nn.Module. The CNN consists of 2 convolutional
    layers, 2 linear layers, and some dropout layers for regularization. The forward
    method of the Net class applies the convolutional and linear layers in sequence,
    passing the output through a ReLU activation function and max pooling layers.
    It also applies dropout layers to the output and returns the log-softmax of the
    final output.
  prefs: []
  type: TYPE_NORMAL
- en: The next block of code creates an instance of the Net class, an optimizer (stochastic
    gradient descent), and an instance of the custom loss function.
  prefs: []
  type: TYPE_NORMAL
- en: The final block of code is the training loop, where the model is trained for
    10 epochs. In each epoch, the model iterates over the training dataset, passing
    the images through the network, calculating the loss using the custom loss function
    and backpropagating the gradients. Then it updates the model’s parameters using
    the optimizer. It also tracks the training loss and test loss and periodically
    prints the current loss to the console. Additionally, it creates a new directory
    called “results” to store the results and outputs of the training process.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/93db5d18c905850d1b5ab433f420342b.png)'
  prefs: []
  type: TYPE_IMG
- en: Custom loss trend — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: This code is creating a plot of the custom loss function for the MNIST dataset
    during the training process. The plot will display the custom loss for both the
    training set and the test set.
  prefs: []
  type: TYPE_NORMAL
- en: It starts by importing the Matplotlib library, which is a plotting library for
    Python. Then, it creates a figure object with a specified size using the `plt.figure()`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: The next line of code plots the custom loss for the training set using the `plt.plot()`
    function. It uses the `train_counter` and `train_losses` variables as the x and
    y-axis values, respectively. The color of the plot is set to blue using the `color`
    parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Then, it plots the custom loss for the test set using the `plt.scatter()` function.
    It uses the `test_counter` and `test_losses` variables as the x and y-axis values,
    respectively. The colour of the plot is set to red using the `color` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: The `plt.legend()` function adds a legend to the plot, indicating which plot
    corresponds to the train loss and which corresponds to the test loss. The `loc`
    parameter is set to 'upper right' which means the legend will be located at the
    upper right corner of the plot.
  prefs: []
  type: TYPE_NORMAL
- en: The `plt.xlabel()` and `plt.ylabel()` functions add labels to the x and y-axis
    of the plot, respectively. The x-axis label is set to 'number of training examples
    seen' and the y-axis label is set to 'Custom loss'.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the `plt.show()` function is used to display the plot.
  prefs: []
  type: TYPE_NORMAL
- en: This code will display a plot that shows the custom loss function over the training
    examples seen. The blue line represents the custom loss for the training set,
    and the red dots represent the custom loss for the test set. The plot will allow
    you to see how the custom loss function is behaving during the training process,
    and evaluate the performance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/dff9371acd8f7041ea0389b7070f948c.png)'
  prefs: []
  type: TYPE_IMG
- en: Test set samples and prediction — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: This code is displaying a figure with 6 images from the test set and their corresponding
    predictions made by the trained network.
  prefs: []
  type: TYPE_NORMAL
- en: It starts by using the `enumerate()` function to loop over the test_loader,
    which is an iterator that loads the test dataset in batches. The `next()` function
    is used to get the first batch of examples from the test set.
  prefs: []
  type: TYPE_NORMAL
- en: The `example_data` variable contains the images and the `example_targets` variable
    contains the corresponding labels.
  prefs: []
  type: TYPE_NORMAL
- en: Then it uses the Pytorch’s `torch.no_grad()` function, it is used to temporarily
    set the requires_grad flag to false. It will reduce memory usage and speed up
    computations but also it will not track the operations.
  prefs: []
  type: TYPE_NORMAL
- en: The next block of code creates a new figure object using the `plt.figure()`
    function. Then, it uses a for loop to iterate over the first 6 examples in the
    test set. For each example, it creates a subplot in the current figure using the
    `plt.subplot()` function. The `plt.tight_layout()` function is used to adjust
    the spacing between the subplots.
  prefs: []
  type: TYPE_NORMAL
- en: Then it uses the `plt.imshow()` function to display the image in the current
    subplot. The `cmap` parameter is set to 'gray' to display the image in grayscale
    and the `interpolation` parameter is set to 'none' to display the image without
    any interpolation.
  prefs: []
  type: TYPE_NORMAL
- en: The `plt.title()` function is used to add a title to the current subplot. The
    title shows the prediction made by the network for the current example. The output
    of the network is passed through the `output.data.max(1, keepdim=True)[1]` which
    returns the index of the predicted class. The `[i].item()` extracts the integer
    value of the predicted class.
  prefs: []
  type: TYPE_NORMAL
- en: The `plt.xticks()` and `plt.yticks()` functions are used to remove the x and
    y-axis ticks from the current subplot, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the `plt.show()` function is used to display the figure. This code
    will display a figure with 6 images from the test set and their corresponding
    predictions made by the trained network. The images are displayed in grayscale
    and without any interpolation, and the predicted class is displayed as a title
    above each image. This can be a useful tool for visualizing the model’s performance
    on the test set and identifying any potential issues or misclassification.
  prefs: []
  type: TYPE_NORMAL
- en: Greetings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this article, we have discussed the theory and implementation of custom loss
    functions in PyTorch, using the MNIST dataset for digit classification as an example.
    We have shown how to create a custom loss function by subclassing the nn.Module
    class and overridding the forward method. We have also provided an example of
    how to use the custom loss function for training a model on the MNIST dataset.
    Custom loss functions can be useful in scenarios where the cost of misclassifying
    certain classes is much higher than others. It is important to note that care
    should be taken when implementing custom loss functions, as they can have a significant
    impact on the performance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Join Medium Membership
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you enjoyed this article and want to keep learning more about this topic,
    I invite you to join Medium membership at this [link](https://marcosanguineti.medium.com/membership).
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://marcosanguineti.medium.com/membership?source=post_page-----50739f9e0ee1--------------------------------)
    [## Join Medium with my referral link — Marco Sanguineti'
  prefs: []
  type: TYPE_NORMAL
- en: Read every story from Marco Sanguineti (and thousands of other writers on Medium).
    Investment in culture is the best…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: marcosanguineti.medium.com](https://marcosanguineti.medium.com/membership?source=post_page-----50739f9e0ee1--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: By becoming a member, you’ll have access to a wider variety of high-quality
    content, and exclusive access to member-only stories, and you’ll be supporting
    independent writers and creators like myself. Plus, as a member, you’ll be able
    to highlight your favourite passages, save stories for later, and get personalized
    reading recommendations. Sign up today and let’s continue exploring this topic
    and others together.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for your support! Until next,
  prefs: []
  type: TYPE_NORMAL
- en: Marco
  prefs: []
  type: TYPE_NORMAL
