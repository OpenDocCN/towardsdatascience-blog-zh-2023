# 从零开始训练 BERT 的终极指南：最终篇

> 译文：[`towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-final-act-eab78b0657bb`](https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-final-act-eab78b0657bb)

## 最终前沿：构建和训练你的 BERT 模型

[](https://dpoulopoulos.medium.com/?source=post_page-----eab78b0657bb--------------------------------)![Dimitris Poulopoulos](https://dpoulopoulos.medium.com/?source=post_page-----eab78b0657bb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----eab78b0657bb--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page-----eab78b0657bb--------------------------------) [Dimitris Poulopoulos](https://dpoulopoulos.medium.com/?source=post_page-----eab78b0657bb--------------------------------)

·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----eab78b0657bb--------------------------------)·阅读时间 7 分钟·2023 年 12 月 18 日

--

![](img/548c617f9d3d11635cc30ccc45e1d106.png)

图片来源：[Rob Laughter](https://unsplash.com/@roblaughter?utm_source=medium&utm_medium=referral)在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

> 本博客文章总结了我们关于从零开始训练 BERT 的系列内容。有关背景和完整理解，请参阅系列的第一部分、第二部分和第三部分。

当 BERT 在 2018 年登场时，它在自然语言处理（NLP）领域掀起了一场巨浪。许多人将其视为 NLP 的 ImageNet 时刻，类似于 2012 年深度神经网络对计算机视觉及更广泛的机器学习领域带来的变化。

五年过去了，预言依然准确。基于 Transformer 的大型语言模型（LLMs）不仅仅是新玩具；它们正在重塑整个格局。从改变我们的工作方式到革命性地获取信息，这些模型是无数新兴初创公司背后的核心技术，旨在挖掘它们尚未被充分利用的潜力。

这就是我决定撰写这一系列博客文章的原因，深入探讨 BERT 的世界以及如何从零开始训练自己的模型。重点不仅仅是完成任务——毕竟，你可以轻松找到在 Hugging Face Hub 上预训练的 BERT 模型。真正的魔力在于理解这个突破性模型的内部工作原理，并将这些知识应用于当前环境。

第一篇文章作为你的入门票，引入了 BERT 的核心概念、目标和潜在应用。我们甚至一起完成了微调过程，创建了一个问答系统：

[BERT 训练的终极指南：介绍](https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f?source=post_page-----eab78b0657bb--------------------------------) [## BERT 训练的终极指南：介绍

### 解密 BERT：改变 NLP 领域的模型的定义和各种应用。

[BERT 训练的终极指南：介绍](https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f?source=post_page-----eab78b0657bb--------------------------------)

第二部分作为你了解分词器这一经常被忽视领域的内部指南——解释它们的作用，展示它们如何将单词转换为数值，并指导你训练自己的分词器：

[BERT 训练的终极指南：分词器](https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822?source=post_page-----eab78b0657bb--------------------------------) [## BERT 训练的终极指南：分词器

### 从文本到标记：你的 BERT 分词指南

[BERT 训练的终极指南：分词器](https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822?source=post_page-----eab78b0657bb--------------------------------)

在第三章中，我们讨论了我认为整个训练流程中最困难的步骤：数据集准备。我们*深入探讨*了为 BERT 的专用 Masked ML 和 NSP 任务准备数据集的细节，这些任务是它用来理解语言和上下文的代理。

[BERT 训练的终极指南：准备数据集](https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-prepare-the-dataset-beaae6febfd5?source=post_page-----eab78b0657bb--------------------------------) [## BERT 训练的终极指南：准备数据集

### 数据准备：*深入探讨*，优化流程，并发现如何应对最关键的步骤

[BERT 训练的终极指南：准备数据集](https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-prepare-the-dataset-beaae6febfd5?source=post_page-----eab78b0657bb--------------------------------)

现在，最终的序幕拉开并落下：训练。在这篇文章中，我们将*挽起袖子*，训练一个新的 BERT 模型——你可以稍后针对特定的 NLP 需求进行微调。如果你已经涉足深度学习模型训练，这篇文章应该能让你毫无问题地跟上。所以，事不宜迟，让我们*深入探讨*吧！

> [学习速率](https://www.dimpo.me/newsletter?utm_source=medium&utm_medium=article&utm_campaign=bert-training)是一个面向对 ML 和 MLOps 世界感兴趣的人的通讯。如果你想了解更多类似的主题，请[点击这里](https://www.dimpo.me/newsletter?utm_source=medium&utm_medium=article&utm_campaign=bert-training)订阅。每个月的最后一个星期天，我会与你分享最新的 MLOps 新闻和文章的更新和看法！

# 训练循环

现代框架如 PyTorch 和 TensorFlow 简化了构建和训练深度神经网络的过程。训练循环就像编排，将数据集特征传递通过模型以获得预测，将模型的预测与标签（真实值）进行比较以计算损失，最后使用损失通过模型的层反向传播误差以更新模型的权重。

这就是在 PyTorch 中流程的样子：

```py
for epoch in range(epochs):
    # forward pass
    predictions = model(features)
    # calculate the loss
    loss = criterion(predictions, labels)
    # propagate the error backward to calculate the gradients
    loss.backward()
    # update the model's weights
    optimizer.step()

    # xero gradients
    optimizer.zero_grad()
```

理论上，这看起来很简单，不是吗？实际上，它变得更简单而且更强大。我们将使用 Hugging Face `transformers` 库来训练我们的 BERT 编码器。`Trainer` 对象为我们处理循环实现，并添加了许多其他功能。因此，我们可以将注意力转向代码的其他部分。现在，让我们首先定义我们的指标。

在纸面上，这个过程看起来真的很简单，你不觉得吗？现实更好：它不仅简单，而且非常强大，这要归功于像 Hugging Face `transformers` 库这样的工具。

他们的 `Trainer` 对象承担了繁重的工作，同时增加了许多额外的功能。因此，我们可以将注意力转向代码的其他部分。

所以，接下来让我们定义我们的指标。

## 准确率

为了评估我们模型在训练期间的表现，我们需要关注两个关键数字：损失和对我们有意义的指标。损失是像梯度下降这样的算法在反向传播期间用来改进模型性能的，而指标则是更易于理解的东西。

本着简洁的精神，我们将选择一个直接的衡量标准：模型在掩码语言建模（MLM）和下一句预测（NSP）任务上的准确率。实质上，我们关心的是两个方面：我们的模型在填充那些掩码词时的准确度，以及它在识别两句逻辑上是否相互衔接方面的表现。

然而，我们必须说这不是验证我们模型性能的最佳方式。例如，考虑以下句子：

> 我在数学方面是 [MASK]。

你会如何填充掩码词？我是*擅长*数学，还是我是*糟糕*的数学？这两种预测在语境上似乎都有效，但它们完全改变了句子的意思。

因此，用只有一个正确答案的验证数据集来评估模型不是最好的主意。这可能并不能真正测试模型理解语言的能力，而是记忆能力。为了解决这个问题，我们可以计算另一个指标，比如训练后模型的困惑度。

现在，让我们通过从 `transformers` 库中加载相应模块来评估模型的准确性：

```py
import evaluate

metric = evaluate.load("accuracy")

def preprocess_logits_for_metrics(logits, labels):
    mlm_logits = logits[0]
    nsp_logits = logits[1]
    return mlm_logits.argmax(-1), nsp_logits.argmax(-1)

def compute_metrics(eval_preds):
    preds, labels = eval_preds

    mlm_preds = preds[0]
    nsp_preds = preds[1]

    mlm_labels = labels[0]
    nsp_labels = labels[1]

    mask = mlm_labels != -100
    mlm_labels = mlm_labels[mask]
    mlm_preds = mlm_preds[mask]

    mlm_accuracy =  metric.compute(
        predictions=mlm_preds, references=mlm_labels)["accuracy"]
    nsp_accuracy = metric.compute(
        predictions=nsp_preds, references=nsp_labels)["accuracy"]

    return {"Masked ML Accuracy": mlm_accuracy, "NSP Accuracy": nsp_accuracy}
```

## 训练

假设你在整个教程系列中一直跟随我，现在你正处于一个关键时刻，你已经准备好了数据集。数据科学家中有一句常见的说法：一旦你的数据集为模型雕琢完成，你的大部分工作就差不多完成了。由于深度学习框架的进步，接下来的步骤往往感觉像是简单的模板代码。

所以，带着期待，让我们开始实例化模型吧。

```py
from transformers import BertForPreTraining, BertConfig

model_config = BertConfig()
model = BertForPreTraining(model_config)
```

现在迎来了高潮：启动训练脚本。首先，我们将定义训练参数，然后，在一切准备就绪后，我们将启动训练循环：

```py
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    "bert-pretrained-wikitext-2-raw-v1",
    logging_first_step=True,
    evaluation_strategy="epoch",
    learning_rate=5e-5,
    weight_decay=0.01,
    push_to_hub=True,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    fp16=True,
    save_strategy="epoch",
    num_train_epochs=20
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=nsp_train_dataset,
    eval_dataset=nsp_validation_dataset,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    preprocess_logits_for_metrics=preprocess_logits_for_metrics
)
```

两小时和 20 个周期后，在有限的 Colab Notebook 环境中，我在 NSP 任务上达到了 80%的准确率，在 Masked ML 任务上达到了 15%的准确率。对于一个从零开始训练的模型来说，这些数字还不错。

![](img/686ea6f168d6cb72d37ecbb6983d1ae2.png)

训练结果 — 作者提供的图片

然而，值得注意的是，在大多数实际场景中，你的起点并不是从零开始。通常，你会从一个预训练的模型开始，这往往能带来更好的结果。

同时，还有大量的空间进行实验和提升性能。调整训练过程中的超参数，特别是学习率，可以显著提升你的结果，即使你是从头开始构建模型。

# 结论

我们已经达到了从零开始训练 BERT 模型的最终目的地。在这段旅程中，我们了解了为什么 BERT 被认为是 NLP 领域的开创性模型，并深入探讨了分词器的复杂性，不仅理解了它们的机制，还掌握了训练我们自己分词器的技巧。

接下来，我们的旅程带我们经历了为两个并行运行的任务——NSP 和 Masked ML 任务——准备数据集的细致过程。最终，我们来到了启动训练脚本的关键步骤，使用了`transformers`库。

这是一段漫长但充实的旅程，我希望你也喜欢它。你应该准备好将这些知识应用到自己的活动中，并扩展到更多领域。下次见，祝编程愉快！

# 关于作者

我的名字是 [Dimitris Poulopoulos](https://www.dimpo.me/?utm_source=medium&utm_medium=article&utm_campaign=bert-intro)，我是一个为 [HPE](https://www.hpe.com/us/en/home.html)工作的机器学习工程师。我为欧洲委员会、国际货币基金组织、欧洲中央银行、宜家、Roblox 等主要客户设计和实施了 AI 和软件解决方案。

如果你有兴趣阅读更多关于机器学习、深度学习、数据科学和数据运维的文章，可以在 [Medium](https://towardsdatascience.com/medium.com/@dpoulopoulos/follow)、[LinkedIn](https://www.linkedin.com/in/dpoulopoulos/) 或在 Twitter 上的 [@james2pl](https://twitter.com/james2pl) 关注我。

所表达的意见仅代表我个人，并不代表我的雇主的观点或意见。
