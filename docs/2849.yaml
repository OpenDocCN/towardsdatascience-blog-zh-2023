- en: 'Large Language Models: SBERT — Sentence-BERT'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/sbert-deb3d4aef8a4?source=collection_archive---------1-----------------------#2023-09-12](https://towardsdatascience.com/sbert-deb3d4aef8a4?source=collection_archive---------1-----------------------#2023-09-12)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn how siamese BERT networks accurately transform sentences into embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@slavahead?source=post_page-----deb3d4aef8a4--------------------------------)[![Vyacheslav
    Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----deb3d4aef8a4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----deb3d4aef8a4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----deb3d4aef8a4--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----deb3d4aef8a4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsbert-deb3d4aef8a4&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----deb3d4aef8a4---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----deb3d4aef8a4--------------------------------)
    ·8 min read·Sep 12, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdeb3d4aef8a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsbert-deb3d4aef8a4&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----deb3d4aef8a4---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdeb3d4aef8a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsbert-deb3d4aef8a4&source=-----deb3d4aef8a4---------------------bookmark_footer-----------)![](../Images/213e63865ea083fbbc857bd1cdbdd7a5.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is no secret that **transformers** made evolutionary progress in NLP. Based
    on transformers, many other machine learning models have evolved. One of them
    is **BERT** which primarily consists of several stacked transformer **encoders**.
    Apart from being used for a set of different problems like sentiment analysis
    or question answering, BERT became increasingly popular for constructing word
    **embeddings** — vectors of numbers representing semantic meanings of words.
  prefs: []
  type: TYPE_NORMAL
- en: Representing words in the form of embeddings gave a huge advantage as machine
    learning algorithms cannot work with raw texts but can operate on vectors of vectors.
    This allows comparing different words by their similarity by using a standard
    metric like Euclidean or cosine distance.
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that, in practice, we often need to construct embeddings not
    for single words but instead for whole sentences. However, the basic BERT version
    builds embeddings only on the word level. Due to this, several BERT-like approaches
    were later developed to solve this problem which will be discussed in this article.
    By progressively discussing them, we will then reach to the state-of-the-art model
    called **SBERT**.
  prefs: []
  type: TYPE_NORMAL
- en: For getting a deep understanding of how SBERT works under the hood, it is recommended
    that you are already familiar with BERT. If not, the previous part of this article
    series explains it in detail.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](/bert-3d1bf880386a?source=post_page-----deb3d4aef8a4--------------------------------)
    [## Large Language Models: BERT'
  prefs: []
  type: TYPE_NORMAL
- en: Understand how BERT constructs state-of-the-art embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/bert-3d1bf880386a?source=post_page-----deb3d4aef8a4--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: BERT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First of all, let us remind how BERT processes information. As an input, it
    takes a *[CLS]* token and two sentences separated by a special *[SEP]* token.
    Depending on the model configuration, this information is processed 12 or 24 times
    by multi-head attention blocks. The output is then aggregated and passed to a
    simple regression model to get the final label.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7cec45ad8f1df3ed285b4332b79b587d.png)'
  prefs: []
  type: TYPE_IMG
- en: BERT architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information on BERT inner workings, you can refer to the previous
    part of this article series:'
  prefs: []
  type: TYPE_NORMAL
- en: Cross-encoder architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is possible to use BERT for calculation of similarity between a pair of documents.
    Consider the objective of finding the most similar pair of sentences in a large
    collection. To solve this problem, each possible pair is put inside the BERT model.
    This leads to quadratic complexity during inference. For instance, dealing with
    *n = 10 000* sentences requires *n * (n — 1) / 2 = 49 995 000* inference BERT
    computations which is not really scalable.
  prefs: []
  type: TYPE_NORMAL
- en: Other approaches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Analysing the inefficiency of cross-encoder architecture, it seems logical to
    precompute embeddings independently for each sentence. After that, we can directly
    compute the chosen distance metric on all pairs of documents which is much faster
    than feeding a quadratic number of pairs of sentences to BERT.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, this approach is not possible with BERT: the core problem of
    BERT is that every time two sentences are passed and processed simultaneously
    making it difficult to get embeddings that would independently represent only
    a single sentence.'
  prefs: []
  type: TYPE_NORMAL
- en: Researchers tried to eliminate this issue by using the output of the *[CLS]*
    token embedding hoping that it would contain enough information to represent a
    sentence. However, the *[CLS]* turned out to not be useful at all for this task
    simply because it was initially pre-trained in BERT for next sentence prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Another approach was to pass a single sentence to BERT and then averaging the
    output token embeddings. However, the obtained results were even worse than simply
    averaging GLoVe embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Deriving independent sentence embeddings is one of the main problems of BERT.
    To alleviate this aspect, SBERT was developed.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: SBERT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**SBERT** introduces the **Siamese network** concept meaning that each time
    two sentences are *passed independently* through the same BERT model. Before discussing
    SBERT architecture, let us refer to a subtle note on Siamese networks:'
  prefs: []
  type: TYPE_NORMAL
- en: Most of the time in scientific papers, a siamese network architecture is depicted
    with several models receiving so many inputs. In reality, it can be thought of
    a single model with the same configuration and weights shared across several parallel
    inputs. Whenever model weights are updated for a single input, they are equally
    updated for other inputs as well.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/c26e287d0cd1471cdba937f37da6edc3.png)'
  prefs: []
  type: TYPE_IMG
- en: Non-Siamese (cross-encoder) architecture is shown on left, and the Siamese (bi-encoder)
    architecture is on the right. The principal difference is that on the left the
    model accepts both inputs at the same time. On the right, the model accepts both
    inputs in parallel, so both outputs are not dependent on each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Getting back to SBERT, after passing a sentence through BERT, a pooling layer
    is applied to BERT embeddings to get their lower dimensionality representation:
    initial 512 768-dimensional vectors are transformed to a single 768-dimensional
    vector. For the pooling layer, SBERT authors propose choosing a mean-pooling layer
    as a default one, though they also mention that is possible to use the max-pooling
    strategy or simply to take the output of the *[CLS]* token instead.'
  prefs: []
  type: TYPE_NORMAL
- en: When both sentences are passed through pooling layers, we have two 768-dimensional
    vectors *u* and *v*. By using these two vectors, authors propose three approaches
    for optimising different objectives which are going to be discussed below.
  prefs: []
  type: TYPE_NORMAL
- en: Classification objective function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of this problem is to correctly classify a given pair of sentences
    in one of several classes.
  prefs: []
  type: TYPE_NORMAL
- en: After the generation of embeddings *u* and *v*, the researchers found it useful
    to generate another vector derived from these two as the element-wise absolute
    difference *|u-v|*. They also tried other feature engineering techniques but this
    one showed the best results.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, three vectors *u*, *v* and *|u-v|* are concatenated, multiplied by
    a trainable weight matrix *W* and the multiplication result is fed into the softmax
    classifier which outputs normalised probabilities of sentences corresponding to
    different classes. The cross-entropy loss function is used to update the weights
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2f055f8250f01acf463b4258b35ae33d.png)'
  prefs: []
  type: TYPE_IMG
- en: SBERT architecture for classification objective. Parameter n stands for the
    dimensionality of embeddings (768 by default for BERT base) while k designates
    the number of labels.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most popular existing problems used to be solved with this objective
    is [**NLI** (Natural Language Inference)](https://paperswithcode.com/task/natural-language-inference)
    where for a given pair of sentences A and B which define hypothesis and premise
    it is necessary to predict whether the hypothesis is true (*entailment*), false
    (*contradiction*) or undetermined (*neutral*) given the premise. For this problem,
    the inference process is the same as for the training.
  prefs: []
  type: TYPE_NORMAL
- en: 'As stated in the [paper](https://arxiv.org/pdf/1908.10084.pdf), the SBERT model
    is originally trained on two datasets SNLI and MultiNLI which contain a million
    sentence pairs with corresponding labels *entailment*, *contradiction* or *neutral*.
    After that, the paper researchers mention details about SBERT tuning parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: “We fine-tune SBERT with a 3-way softmax-classifier objective function for one
    epoch. We used a batch-size of 16, Adam optimizer with learning rate 2e−5, and
    a linear learning rate warm-up over 10% of the training data. Our default pooling
    strategy is mean.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Regression objective function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this formulation, after getting vectors u and v, the similarity score between
    them is directly computed by a chosen similarity metric. The predicted similarity
    score is compared with the true value and the model is updated by using the MSE
    loss function. By default, authors choose cosine similarity as the similarity
    metric.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b381c23229d4443516e6efdc035f46ab.png)'
  prefs: []
  type: TYPE_IMG
- en: SBERT architecture for regression objective. Parameter n stands for the dimensionality
    of embeddings (768 by default for BERT base).
  prefs: []
  type: TYPE_NORMAL
- en: 'During inference, this architecture can be used in one of two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: By a given sentence pair, it is possible to calculate the similarity score.
    The inference workflow is absolutely the same as for the training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a given sentence, it is possible to extract its sentence embedding (right
    after applying the pooling layer) for some later use. This is particularly useful
    when we are given a large collection of sentences with the objective to calculate
    pairwise similarity scores between them. By running each sentence through BERT
    only once, we extract all the necessary sentence embeddings. After that, we can
    directly calculate the chosen similarity metric between all the vectors (without
    doubt, it still requires a quadratic number of comparisons but at the same time
    we avoid quadratic inference computations with BERT as it was before).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Triplet objective function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The triplet objective introduces a triplet loss which is calculated on three
    sentences usually named *anchor*, *positive* and *negative*. It is assumed that
    *anchor* and *positive* sentences are very close to each other while *anchor*
    and *negative* are very different. During the training process, the model evaluates
    how closer the pair *(anchor, positive)* is, compared to the pair *(anchor, negative)*.
    Mathematically, the following loss function is minimised:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/caed12398bb6eb3f194cf4667f551f76.png)'
  prefs: []
  type: TYPE_IMG
- en: The triplet loss function from the [original paper](https://arxiv.org/pdf/1908.10084.pdf).
    Variables sₐ, sₚ, sₙ represent anchor, positive and negative embeddings respectively.
    Symbol ||s|| is the norm of the vector s. Parameter ε is called margin.
  prefs: []
  type: TYPE_NORMAL
- en: Margin ε ensures that a *positive* sentence is closer to the anchor at least
    by ε than the *negative* sentence to the *anchor*. Otherwise, the loss becomes
    greater than 0\. By default, in this formula, the authors choose the Euclidean
    distance as the vector norm and the parameter ε is set to 1.
  prefs: []
  type: TYPE_NORMAL
- en: The triplet SBERT architecture differs from the previous two in a way that the
    model now accepts in parallel three input sentences (instead of two).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1805aee36bb9bf02b5d1a338870b5da2.png)'
  prefs: []
  type: TYPE_IMG
- en: SBERT architecture for regression objective. Parameter n stands for the dimensionality
    of embeddings (768 by default for BERT base).
  prefs: []
  type: TYPE_NORMAL
- en: Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[SentenceTransformers](https://www.sbert.net/index.html) is a state-of-the-art
    Python library for building sentence embeddings. It contains several [pretrained
    models](https://www.sbert.net/docs/pretrained_models.html) for different tasks.
    Building embeddings with SentenceTransformers is simple and an example is shown
    in the code snippet below.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c38b4d4eb9eaf3a226ab4b4efd9e05c7.png)'
  prefs: []
  type: TYPE_IMG
- en: Constructing embeddings with SentenceTransformers
  prefs: []
  type: TYPE_NORMAL
- en: Constructed embeddings can be then used for similarity comparison. Every model
    is trained for a certain task, so it is always important to choose an appropriate
    similarity metric for comparison by referring to the documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have walked through one of the advanced NLP models for obtaining sentence
    embeddings. By reducing a quadratic number of BERT inference executions to linear,
    SBERT achieves a massive growth in speed while maintaining high accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: To finally understand how significant this difference is, it is enough to refer
    to the example described in the paper where researchers tried to find the most
    similar pair among *n = 10000* sentences. On a modern V100 GPU, this procedure
    took about 65 hours with BERT and only 5 seconds with SBERT! This example demonstrates
    that SBERT is a huge advancement in NLP.
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/pdf/1908.10084.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SentenceTransformers Documentation | SBERT.net](https://www.sbert.net/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Natural Language Inference | Papers with code](https://paperswithcode.com/task/natural-language-inference)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*All images unless otherwise noted are by the author*'
  prefs: []
  type: TYPE_NORMAL
