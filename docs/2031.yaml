- en: Random Forests and Missing Values
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林与缺失值
- en: 原文：[https://towardsdatascience.com/random-forests-and-missing-values-3daaea103db0?source=collection_archive---------7-----------------------#2023-06-21](https://towardsdatascience.com/random-forests-and-missing-values-3daaea103db0?source=collection_archive---------7-----------------------#2023-06-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/random-forests-and-missing-values-3daaea103db0?source=collection_archive---------7-----------------------#2023-06-21](https://towardsdatascience.com/random-forests-and-missing-values-3daaea103db0?source=collection_archive---------7-----------------------#2023-06-21)
- en: There is a very Intriguing Practical Fix
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这是一个非常引人入胜的实际解决方案
- en: '[](https://medium.com/@jeffrey_85949?source=post_page-----3daaea103db0--------------------------------)[![Jeffrey
    Näf](../Images/0ce6db85501192cdebeeb910eb81a688.png)](https://medium.com/@jeffrey_85949?source=post_page-----3daaea103db0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3daaea103db0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3daaea103db0--------------------------------)
    [Jeffrey Näf](https://medium.com/@jeffrey_85949?source=post_page-----3daaea103db0--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jeffrey_85949?source=post_page-----3daaea103db0--------------------------------)[![Jeffrey
    Näf](../Images/0ce6db85501192cdebeeb910eb81a688.png)](https://medium.com/@jeffrey_85949?source=post_page-----3daaea103db0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3daaea103db0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3daaea103db0--------------------------------)
    [Jeffrey Näf](https://medium.com/@jeffrey_85949?source=post_page-----3daaea103db0--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fca780798011a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandom-forests-and-missing-values-3daaea103db0&user=Jeffrey+N%C3%A4f&userId=ca780798011a&source=post_page-ca780798011a----3daaea103db0---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3daaea103db0--------------------------------)
    ·9 min read·Jun 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3daaea103db0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandom-forests-and-missing-values-3daaea103db0&user=Jeffrey+N%C3%A4f&userId=ca780798011a&source=-----3daaea103db0---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fca780798011a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandom-forests-and-missing-values-3daaea103db0&user=Jeffrey+N%C3%A4f&userId=ca780798011a&source=post_page-ca780798011a----3daaea103db0---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3daaea103db0--------------------------------)
    ·9分钟阅读·2023年6月21日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3daaea103db0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandom-forests-and-missing-values-3daaea103db0&user=Jeffrey+N%C3%A4f&userId=ca780798011a&source=-----3daaea103db0---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3daaea103db0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandom-forests-and-missing-values-3daaea103db0&source=-----3daaea103db0---------------------bookmark_footer-----------)![](../Images/2e019db6f9d938ddee0933bf834feb92.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3daaea103db0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandom-forests-and-missing-values-3daaea103db0&source=-----3daaea103db0---------------------bookmark_footer-----------)![](../Images/2e019db6f9d938ddee0933bf834feb92.png)'
- en: 'Features of (Distributional) Random Forests. In this article: The ability to
    deal with missing values. Source: Author.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: (分布式)随机森林的特点。在这篇文章中：处理缺失值的能力。来源：作者。
- en: Outside of some excessively cleaned data sets that one finds online, missing
    values are everywhere. In fact, the more complex and large the dataset, the more
    likely it is that missing values are present. Missing values are a fascinating
    field of statistical research, but in practice they are often a nuisance.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在网上找到的一些过度清理的数据集之外，缺失值无处不在。实际上，数据集越复杂、规模越大，缺失值出现的可能性就越高。缺失值是统计研究中的一个有趣领域，但在实际应用中，它们常常是一种麻烦。
- en: If you deal with a prediction problem where you want to predict a variable *Y*
    from *p-*dimensional covariates ***X****=(X_1,…,X_p)* and you face missing values
    in ***X***, there is an interesting solution for tree-based methods. This method
    is actually rather old but appears to work remarkably well in a wide range of
    data sets. I am talking about the “missing incorporated in attributes criterion”
    (MIA; [1]). While there are many good articles about missing values (such as [this
    one](https://medium.com/@vinitasilaparasetty/guide-to-handling-missing-values-in-data-science-37d62edbfdc1)),
    this powerful approach seems somewhat underused. In particular, one does not need
    to impute, delete or predict your missing values in any way, but instead can just
    run your prediction as if you have fully observed data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你处理的是预测问题，希望从 *p-* 维协变量 ***X***=(X_1，…，X_p)* 中预测一个变量 *Y*，并且你在 ***X*** 中面临缺失值，对于基于树的方法有一个有趣的解决方案。这种方法实际上已经存在很久，但在广泛的数据集中表现得非常好。我所说的是“缺失值纳入属性标准”（MIA；[1]）。虽然关于缺失值有许多好的文章（如[这篇](https://medium.com/@vinitasilaparasetty/guide-to-handling-missing-values-in-data-science-37d62edbfdc1)），但这种强大的方法似乎在使用上有所欠缺。特别是，你不需要以任何方式填补、删除或预测缺失值，而是可以像处理完全观测到的数据一样进行预测。
- en: I will quickly explain how the method itself works, and then present an example
    with the distributional random forest (DRF) explained [here](https://medium.com/towards-data-science/drf-a-random-forest-for-almost-everything-625fa5c3bcb8).
    I chose DRF because it is a very general version of Random Forest (in particular,
    it can also be used to predict a random vector ***Y***) and because I am somewhat
    biased here. MIA is actually implemented for the generalized random forest ([GRF](https://grf-labs.github.io/grf/)),
    which covers a wide range of forest implementations. In particular, since the
    implementation of DRF on [CRAN](https://cran.r-project.org/web/packages/drf/index.html)
    is based on GRF, after a slight modification, it can use the MIA method as well.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我将快速解释方法本身的工作原理，然后展示一个用分布随机森林（DRF）进行的示例，详细说明见[这里](https://medium.com/towards-data-science/drf-a-random-forest-for-almost-everything-625fa5c3bcb8)。我选择
    DRF 是因为它是随机森林的一个非常通用的版本（特别是，它也可以用来预测随机向量***Y***），并且我在这里有些偏向。MIA 实际上是为广义随机森林（[GRF](https://grf-labs.github.io/grf/)）实现的，涵盖了广泛的森林实现。特别是，由于
    [CRAN](https://cran.r-project.org/web/packages/drf/index.html) 上的 DRF 实现基于 GRF，经过轻微修改后，它也可以使用
    MIA 方法。
- en: Of course, be aware that this is a quick fix that (as far as I know) has no
    theoretical guarantees. Depending on the missingness mechanism, it might heavily
    bias the analysis. On the other hand, most commonly used methods for dealing with
    missing values don’t have any theoretical guarantees or are outright known to
    bias the analysis and, at least empirically, MIA appears to work well and
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，要注意这是一个快速修复（据我所知）没有理论保障。根据缺失机制，这可能会严重偏倚分析。另一方面，大多数处理缺失值的常用方法没有任何理论保障，或者明确已知会偏倚分析，至少在经验上，MIA
    似乎效果很好。
- en: How it works
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的
- en: Recall that in a RF, splits are build of the form *X_j < S* or *X_j ≥ S*, for
    a dimension *j=1,…,p*. To find this split valule *S* it optimizes some kind of
    criterion on the *Y*’s, for example the CART criterion. Thus the observations
    are successively divided through decision rules that depend on ***X***.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在 RF 中，拆分的形式为 *X_j < S* 或 *X_j ≥ S*，对于维度 *j=1，…，p*。为了找到这个拆分值 *S*，它优化某种关于
    *Y* 的标准，例如 CART 标准。因此，观察值通过依赖于 ***X*** 的决策规则逐步划分。
- en: '![](../Images/08097166f4a96b74715d61318e9e392e.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08097166f4a96b74715d61318e9e392e.png)'
- en: Illustration of the splitting done in a RF. Image by author.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: RF 中拆分的示意图。图片作者提供。
- en: 'The original paper explains it a bit confusingly, but as far as I understand
    MIA works as follows: Let us consider a sample (*Y_1*, ***X****_1),…, (Y_n,* ***X****_n),
    with*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 原始论文解释得有些混乱，但根据我的理解，MIA 的工作原理如下：我们考虑一个样本（*Y_1*，***X***_1），……，（Y_n，* ***X***_n），其中*
- en: '***X****_i=(X_i1,…,X_ip)’.*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '***X***_i=(X_i1，…，X_ip)’。*'
- en: 'Splitting without missing values is just looking for the value *S* as above
    and then throwing all *Y_i* with *X_ij < S* in Node 1 and all *Y_i* with *X_ij
    ≥ S* in Node 2\. Calculating the target criterion such as CART for each value
    *S*, we can choose the best one. With missing values there are instead 3 options
    for every candidate split value *S* to consider:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 没有缺失值的拆分只是寻找如上所述的值 *S*，然后将所有 *Y_i*，其中 *X_ij < S*，丢入节点 1，将所有 *Y_i*，其中 *X_ij ≥
    S*，丢入节点 2。计算每个值 *S* 的目标标准，例如 CART，我们可以选择最佳的一个。有缺失值时，对于每个候选拆分值 *S*，需要考虑 3 种选项：
- en: Use the usual rule for all observations *i* such that *X_ij* is observed and
    send *i* to Node 1 if *X_ij* is missing.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于所有观察值*i*，如果*X_ij*被观察到，则使用通常的规则，如果*X_ij*缺失，则将*i*发送到节点1。
- en: Use the usual rule for all observations i such that *X_ij* is observed and send
    *i* to Node 2 if *X_ij* is missing.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于所有观察值i，如果*X_ij*被观察到，则使用通常的规则，如果*X_ij*缺失，则将*i*发送到节点2。
- en: Ignore the usual rule and just send i to Node 1 if *X_ij* is missing and to
    Node 2 if it is observed.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 忽略通常的规则，只要*X_ij*缺失就将i发送到节点1，若被观察到则发送到节点2。
- en: Which of these rules to follow is again decided according to the criterion on
    *Y_i* we use.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循这些规则的选择再次根据我们对*Y_i*的标准来决定。
- en: '![](../Images/d95dcadbfd118c2e6d7d8b92323f3beb.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d95dcadbfd118c2e6d7d8b92323f3beb.png)'
- en: Illustration of how I understand the MIA procedure. Given observations in the
    parent node, we are looking for the best split value S. For each split value we
    consider the 3 options and try until we find the minimum. The sets {} on the left
    indicate the observations i that get sent to the left or the right. Image by author.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 说明我如何理解MIA过程。给定父节点中的观察值，我们寻找最佳分裂值S。对于每个分裂值，我们考虑3个选项并进行尝试，直到找到最小值。左侧的{}表示被发送到左侧或右侧的观察值i。图片由作者提供。
- en: A Small Example
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个小例子
- en: 'It needs to be mentioned at this point that the drf package on [CRAN](https://cran.r-project.org/web/packages/drf/index.html)
    is not yet updated with the newest methodology. There will be a point in the future
    where all of this is implemented in one package on CRAN(!) However, for the moment,
    there are two versions:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 需要提到的是，[CRAN](https://cran.r-project.org/web/packages/drf/index.html)上的 drf
    包尚未更新到最新的方法。未来会有一个时间点，所有这些将会在CRAN上的一个包中实现！然而，目前有两个版本：
- en: If you want to use the **fast drf implementation** with missing values (**without**
    confidence intervals), you can use the “drfown” function attached at the end of
    this article. This code is adapted from
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想使用带有缺失值的**快速 drf 实现**（**不带**置信区间），可以使用附在这篇文章末尾的“drfown”函数。这段代码改编自
- en: '[lorismichel/drf: Distributional Random Forests (Cevid et al., 2020) (github.com)](https://github.com/lorismichel/drf)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[lorismichel/drf: Distributional Random Forests (Cevid et al., 2020) (github.com)](https://github.com/lorismichel/drf)'
- en: If on the other hand, you want **confidence intervals** with your parameters,
    use this (slower) code
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要使用带有**置信区间**的参数，请使用这段（较慢的）代码。
- en: '[drfinference/drf-foo.R at main · JeffNaef/drfinference (github.com)](https://github.com/JeffNaef/drfinference/blob/main/drf-foo.R)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[drfinference/drf-foo.R at main · JeffNaef/drfinference (github.com)](https://github.com/JeffNaef/drfinference/blob/main/drf-foo.R)'
- en: In particular, drf-foo.R contains all you need in the latter case.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，drf-foo.R 包含了在后一种情况下所需的一切。
- en: 'We will focus on the slower code with confidence intervals, as explained in
    [this article](https://medium.com/towards-data-science/inference-for-distributional-random-forests-64610bbb3927)
    and also consider the same example as in said article:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将关注带有置信区间的较慢代码，如在[这篇文章](https://medium.com/towards-data-science/inference-for-distributional-random-forests-64610bbb3927)中所解释，并考虑与该文章中相同的例子：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Note that this is a heteroskedastic linear model with *p=2* and with the variance
    of the error term depending on the *X_1* values. Now we also add missing values
    to *X_1* in a Missing at Random (MAR) fashion:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这是一个异方差线性模型，*p=2*，误差项的方差取决于*X_1*的值。现在我们还以“随机缺失”（MAR）的方式向*X_1*中添加缺失值：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This means that *X_1* is missing with a probability of 0.3, whenever *X_2* has
    a value smaller than -0.2\. Thus the probability of *X_1* being missing depends
    on *X_2*, which is what is referred to as “Missing at Random”. This is already
    a complex situation and there is information to be gained by looking at the pattern
    of missing values. That is, the missingness is not “Missing Completely at Random
    (MCAR)”, because the missingness of *X_1* depends on the value of *X_2\.* This
    in turn means that the distribution of *X_2* we draw from is different, conditional
    on whether *X_1* is missing or not. This in particular means that deleting the
    rows with missing values might severely bias the analysis.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着*X_1*在*X_2*的值小于-0.2时，以0.3的概率缺失。因此，*X_1*缺失的概率取决于*X_2*，这被称为“随机缺失”（Missing
    at Random）。这已经是一个复杂的情况，通过查看缺失值的模式可以获得信息。也就是说，缺失情况不是“完全随机缺失（MCAR）”，因为*X_1*的缺失取决于*X_2*的值。这反过来意味着我们从中抽取的*X_2*的分布在*X_1*缺失与否的条件下是不同的。这尤其意味着删除缺失值的行可能会严重偏倚分析结果。
- en: We now fix***x*** and estimate the conditional expectation and variance given
    ***X****=****x,*** exactly as in the [previous article](https://medium.com/towards-data-science/inference-for-distributional-random-forests-64610bbb3927).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们固定 ***x*** 并估计给定 ***X****=****x,*** 的条件期望值和方差，就像在[上一篇文章](https://medium.com/towards-data-science/inference-for-distributional-random-forests-64610bbb3927)中一样。
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We then also fit DRF and predict the weights for the test point ***x*** (which
    corresponds to predicting the conditional distribution of *Y|****X****=****x***):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还拟合了DRF，并预测测试点 ***x*** 的权重（这相当于预测 *Y|****X****=****x*** 的条件分布）：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Example 1: Conditional Expectation'
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例 1：条件期望
- en: We first estimate the conditional expectation of *Y|****X****=****x.***
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先估计条件期望值 *Y|****X****=****x.***
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Remarkably, the values obtained with NAs are very close to the ones from the
    first analysis without NAs in the [previous article](https://medium.com/towards-data-science/inference-for-distributional-random-forests-64610bbb3927)!
    This really is quite astounding to me, as this missing mechanism is not easy to
    deal with. Interestingly, the estimated variance of the estimator also doubles,
    from around 0.025 without missing values to roughly 0.06 with missing values.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，通过缺失数据（NAs）获得的结果与[上一篇文章](https://medium.com/towards-data-science/inference-for-distributional-random-forests-64610bbb3927)中首次分析得到的结果非常接近！这让我感到相当惊讶，因为处理缺失机制并不容易。有趣的是，估计量的估计方差也翻倍了，从没有缺失值时的大约0.025增加到有缺失值时的大约0.06。
- en: 'The truth is given as:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 真相为：
- en: '![](../Images/cd5e502fc70db9f959510cdc678eb651.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cd5e502fc70db9f959510cdc678eb651.png)'
- en: so we have a slight error, but the confidence intervals contain the truth, as
    they should.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们有一个轻微的误差，但置信区间包含了真相，正如应该的那样。
- en: 'The result looks similar for a more complex target, like the conditional variance:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更复杂的目标，例如条件方差，结果看起来相似：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here the difference in the estimated values is a bit larger. As the truth is
    given as
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，估计值的差异略大一些。真相为
- en: '![](../Images/a8d473ae073c8529a43b43b80c61f949.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8d473ae073c8529a43b43b80c61f949.png)'
- en: the estimate with NAs is even slightly more accurate (though of course this
    is likely just randomness). Again the variance estimate of the (variance) estimator
    increases with missing values, from 0.15 (no missing values) to 0.23.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 估计值在有缺失值时甚至略微更准确（虽然这当然可能只是随机性）。再次强调，估计方差（方差）在有缺失值时增加，从0.15（无缺失值）增加到0.23。
- en: Conclusion
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we discussed MIA, which is an adaptation of the splitting method
    in Random Forest to deal with missing values. Since it is implemented in GRF and
    DRF, it can be used broadly and the small example we looked at indicates that
    it works remarkably well.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们讨论了MIA，它是对随机森林中分割方法的适应，以处理缺失值。由于它在GRF和DRF中实现，因此可以广泛使用，我们观察到的小示例表明它表现得非常好。
- en: However, I’d like to note again that there is no theoretical guarantee for consistency
    or for the confidence intervals to make sense, even for a very large number of
    datapoints. The reason for missing values are numerous and one has to be very
    careful to not bias one’s analysis through a careless handling of this issue.
    The MIA method is by no means a well-understood fix for this problem. However,
    it seems to be a reasonable quick fix for the moment, that appears to be able
    to make some use of the pattern of missingness in the data. If somebody does/has
    a more extensive simulation analysis I would be curious about the results.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我想再次指出，即使对于大量数据点，也没有理论保证一致性或置信区间的合理性。缺失值的原因很多，必须非常小心，以免由于粗心处理此问题而使分析产生偏差。MIA方法绝不是解决此问题的一个充分理解的修复方法。然而，它似乎是一个合理的快速修复方法，能够在某种程度上利用数据中的缺失模式。如果有人做了/有更广泛的模拟分析，我会对结果感到好奇。
- en: Code
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '***Citations***'
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '***引文***'
- en: '[1] Twala, B. E. T. H., M. C. Jones, and David J. Hand. Good methods for coping
    with missing data in decision trees. *Pattern Recognition Letters 29*,2008.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Twala, B. E. T. H., M. C. Jones, 和 David J. Hand. 应对决策树中缺失数据的良好方法。*模式识别信函
    29*, 2008。'
