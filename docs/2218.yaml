- en: 'LLaMA: LLMs for Everyone!'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/llama-llms-for-everyone-724e737835be?source=collection_archive---------1-----------------------#2023-07-11](https://towardsdatascience.com/llama-llms-for-everyone-724e737835be?source=collection_archive---------1-----------------------#2023-07-11)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: High-performing language models that are open-source…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wolfecameron.medium.com/?source=post_page-----724e737835be--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----724e737835be--------------------------------)[](https://towardsdatascience.com/?source=post_page-----724e737835be--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----724e737835be--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----724e737835be--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllama-llms-for-everyone-724e737835be&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----724e737835be---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----724e737835be--------------------------------)
    ·15 min read·Jul 11, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F724e737835be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllama-llms-for-everyone-724e737835be&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----724e737835be---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F724e737835be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllama-llms-for-everyone-724e737835be&source=-----724e737835be---------------------bookmark_footer-----------)![](../Images/f673f8c76d03117a2c38c912c91cd911.png)'
  prefs: []
  type: TYPE_NORMAL
- en: (Photo by [Raspopova Marina](https://unsplash.com/@raspopovamarisha?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/llama?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  prefs: []
  type: TYPE_NORMAL
- en: For years, the deep learning community has embraced openness and transparency,
    leading to massive open-source projects like [HuggingFace](https://huggingface.co/).
    Many of the most profound ideas in deep learning (e.g., [transformers](https://newsletter.artofsaience.com/p/vision-transformers-from-idea-to)
    [2], [self-supervised learning](https://cameronrwolfe.substack.com/i/76273144/self-supervised-learning),
    etc.) are openly available online, either via [public code repositories](https://huggingface.co/docs/transformers/v4.27.2/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel)
    or [Arxiv](https://arxiv.org/list/cs.AI/recent). Although open-source has been
    the norm for quite some time, the popularity (and commercial applicability) of
    large language models (LLMs) has recently challenged this tendency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many of the most powerful LLMs available today can only be accessed via APIs
    (e.g., from [OpenAI](https://openai.com/blog/openai-api) or [Anthropic](https://console.anthropic.com/docs/api#accessing-the-api)),
    making the source code and model parameters inaccessible to researchers and developers.
    While it’s not my goal to spark a [moral discussion](https://futureoflife.org/open-letter/pause-giant-ai-experiments/)
    of current trends in the LLM landscape, this information is relevant to the topic
    of this post: openly-available LLMs. Interestingly, not all powerful language
    foundation models are hidden behind a paywall. Some models, such as LLaMA, are
    both openly available and incredibly high-performing, thus maintaining a sense
    of openness in the deep learning research community.'
  prefs: []
  type: TYPE_NORMAL
- en: '**What is LLaMA?** LLaMA is not a single model, but rather a suite of LLMs
    with sizes ranging from 7 billion to 65 billion parameters. Taking inspiration
    from…'
  prefs: []
  type: TYPE_NORMAL
