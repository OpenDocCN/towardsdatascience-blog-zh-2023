- en: What If We Could Easily Explain Overly Complex Models?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/what-if-we-could-easily-explain-overly-complex-models-54a8ff7ac917?source=collection_archive---------6-----------------------#2023-09-28](https://towardsdatascience.com/what-if-we-could-easily-explain-overly-complex-models-54a8ff7ac917?source=collection_archive---------6-----------------------#2023-09-28)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Generating counterfactual explanations got a lot easier with CFNOW, but what
    are counterfactual explanations, and how can I use them?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://mazzine.medium.com/?source=post_page-----54a8ff7ac917--------------------------------)[![Raphael
    Mazzine, Ph.D.](../Images/0b05145a98a5a531d55f5a88fada0af5.png)](https://mazzine.medium.com/?source=post_page-----54a8ff7ac917--------------------------------)[](https://towardsdatascience.com/?source=post_page-----54a8ff7ac917--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----54a8ff7ac917--------------------------------)
    [Raphael Mazzine, Ph.D.](https://mazzine.medium.com/?source=post_page-----54a8ff7ac917--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F10c8c0e5ab30&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-if-we-could-easily-explain-overly-complex-models-54a8ff7ac917&user=Raphael+Mazzine%2C+Ph.D.&userId=10c8c0e5ab30&source=post_page-10c8c0e5ab30----54a8ff7ac917---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----54a8ff7ac917--------------------------------)
    ·12 min read·Sep 28, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F54a8ff7ac917&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-if-we-could-easily-explain-overly-complex-models-54a8ff7ac917&user=Raphael+Mazzine%2C+Ph.D.&userId=10c8c0e5ab30&source=-----54a8ff7ac917---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F54a8ff7ac917&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-if-we-could-easily-explain-overly-complex-models-54a8ff7ac917&source=-----54a8ff7ac917---------------------bookmark_footer-----------)![](../Images/54b393e95beac8c94c9a0394e88e75dd.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image generated with Illusion Diffusion model with CFNOW text as illusion (try
    to squint your eyes and look from a certain distance) | Image by the author using
    [Stable Diffusion model (license)](https://huggingface.co/spaces/CompVis/stable-diffusion-license)
  prefs: []
  type: TYPE_NORMAL
- en: 'This article is based on the following article: [https://www.sciencedirect.com/science/article/abs/pii/S0377221723006598](https://www.sciencedirect.com/science/article/abs/pii/S0377221723006598)'
  prefs: []
  type: TYPE_NORMAL
- en: 'And here is the address for the CFNOW repository: [https://github.com/rmazzine/CFNOW](https://github.com/rmazzine/CFNOW)'
  prefs: []
  type: TYPE_NORMAL
- en: If you are reading this, you may know how pivotal Artificial Intelligence (AI)
    is becoming in our world today. However, it’s important to note that the seemingly
    effective, novel machine learning approaches, combined with their widespread popularity,
    can lead to unforeseen/undesirable consequences.
  prefs: []
  type: TYPE_NORMAL
- en: This brings us to why eXplainable Artificial Intelligence (XAI) is a crucial
    component in ensuring AI’s ethical and responsible development. This area shows
    that explaining models that consist of millions or even billions of parameters
    is not a trivial question. The answer to this is multifaceted, as there are numerous
    methods revealing different aspects of the model, with LIME [1] and SHAP [2] being
    popular examples.
  prefs: []
  type: TYPE_NORMAL
- en: However, the complexity of the explanations generated by these methods can result
    in intricate charts or analyses, that potentially can lead to misinterpretations
    by those other than well-informed experts. One possible way to circumvent this
    complexity is a simple and natural method to explain things called Counterfactual
    Explanations [3].
  prefs: []
  type: TYPE_NORMAL
- en: Counterfactual Explanations leverage a natural human behavior to explain things
    — creating “alternate worlds” where altering a few parameters can change the outcome.
    It’s a common practice, you probably already did something like that— “*if only
    I woke up a bit earlier, I wouldn’t miss the bus*”, this type of explanation highlights
    the main reasons for an outcome in a straightforward manner.
  prefs: []
  type: TYPE_NORMAL
- en: Delving deeper, **counterfactuals extend beyond just mere explanations**; they
    can serve as guidance for changes, assist in debugging anomalous behavior, and
    verify if some features can potentially modify predictions (while not being so
    impactful on scoring). This multifunctional nature emphasizes the importance of
    explaining your predictions. It’s not just a matter of responsible AI; it’s also
    a path to improving models and using them beyond the scope of predictions. A remarkable
    feature of counterfactual explanations is their decision-driven nature, making
    them directly correspond to a change in prediction [6], unlike LIME and SHAP which
    are more suited to explaining scores.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the evident benefits, one might wonder why counterfactuals aren’t more
    popular. It’s a valid question! The primary barriers to the widespread adoption
    of counterfactual explanations are threefold [4, 5]: (1) the absence of user-friendly
    and compatible counterfactual generation algorithms, (2) algorithm inefficiency
    in generating counterfactuals, (3) and the lack of comprehensive visual representation.'
  prefs: []
  type: TYPE_NORMAL
- en: But I have some good news for you! A new package, CFNOW (CounterFactuals NOW
    or CounterFactual Nearest Optimal Wololo), is stepping up to address these challenges.
    CFNOW is a versatile Python package capable of generating multiple counterfactuals
    for various data types such as tabular, image, and textual (embedding) inputs.
    It adopts a model-agnostic approach, requiring only minimal data — (1) the factual
    point (point to be explained) and (2) the prediction function.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, CFNOW is structured to allow the development and integration of new
    strategies for finding and fine-tuning counterfactuals based on custom logic.
    It also features CounterPlots, a novel strategy for visually representing counterfactual
    explanations.
  prefs: []
  type: TYPE_NORMAL
- en: Central to CFNOW is a framework that converts data to a single structure manageable
    by the CF generator. Following this, a two-step process locates and optimizes
    the found counterfactual. To prevent local minimums, the package implements Tabu
    Search, a matheuristics method, allowing it to explore for new regions where the
    objective function might be better optimized.
  prefs: []
  type: TYPE_NORMAL
- en: The subsequent sections of this text will focus on demonstrating how CFNOW can
    be proficiently utilized to generate explanations for tabular, image, and textual
    (embedding) classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Tabular Classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we show the usual stuff, you have tabular data with multiple types of
    data. In the example below, I will use a dataset that has numerical continuous,
    categorical binary, and categorical one-hot encoded data to showcase CFNOW in
    its full power.
  prefs: []
  type: TYPE_NORMAL
- en: 'First things first, you need to install the CFNOW package, the requirement
    is a Python version superior to 3.8:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '(here is the full code for this example: [https://colab.research.google.com/drive/1GUsVfcM3I6SpYCmsBAsKMsjVdm-a6iY6?usp=sharing](https://colab.research.google.com/drive/1GUsVfcM3I6SpYCmsBAsKMsjVdm-a6iY6?usp=sharing))'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In this first part, we will make a classifier with Adult Dataset. Then, there
    is not much news here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We import basic packages to make the classification model and, we also deactivate
    the warnings related to making predictions without the columns' names.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we proceed to write the classifier where class 1 represents an income
    lower or equal to 50k (<=50K) and class 0 represents high income.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: With the code above, we create a dataset, pre-process it, create a classification
    model, and make a prediction and evaluation over the test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s take one point (the first from the test set) and verify its prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now it is time to use CFNOW to calculate how we can change this prediction
    by minimally modifying the features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The code above we:'
  prefs: []
  type: TYPE_NORMAL
- en: '`factual`Add the factual instance as a pd.Series'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`feat_types`Specify the feature types (“num” for numerical continuous and “cat”
    for categorical)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`has_ohe`Indicate that we have OHE features (it automatically detects OHE features
    by aggregating those that have the same prefix followed by an underscore, e.g.,
    country_brazil, country_usa, country_ireland).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_predict_proba`Includes a prediction function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`limit_seconds` Defines a total time threshold for running, this is important
    because the fine-tuning step can keep going indefinitely (default is 120 seconds)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, after some time, we can first evaluate the class of the best counterfactual
    (first index of `cf_res.cfs`)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'And here comes some differences with CFNOW, since it also integrates CounterPlots,
    we can plot their charts and have more insightful information like the below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b72425b5446259d2270b6591d6e6df6.png)'
  prefs: []
  type: TYPE_IMG
- en: CounterShapley Chart for our CF | Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: The CounterShapley plot below shows the relative importance of each feature
    to generate the counterfactual prediction. Here, we have some interesting insights
    showing that marial_status (if combined) represents more than 50% of the contribution
    to the CF class.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e0f2e1dad7699245bdb965c889669f7e.png)'
  prefs: []
  type: TYPE_IMG
- en: Greedy Chart for our CF | Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: The Greedy chart shows something very similar to the CounterShapley, the main
    difference here is the sequence of changes. While the CounterShapley does not
    consider any specific sequence (calculating contributions using Shapley’s values),
    the Greedy chart uses the greediest strategy to modify the factual instance, each
    step changing the feature that most contributes to the CF class. This might be
    useful for situations where some guidance is given in a greedy way (each step
    choosing the best approach to achieve the objective).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/db0fa976f82a4cac6be00a0dcb10d107.png)'
  prefs: []
  type: TYPE_IMG
- en: Constellation Chart for our CF | Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have the most complex analysis, the Constellation chart. Despite
    its daunting look, it is actually pretty straightforward to interpret it. Each
    large red dot represents one single feature change (respective to the label),
    and the smaller dots represent the combination of two or more features. Finally,
    the big blue dot represents the CF score. Here, we can see the only way to obtain
    a CF with these features is by modifying all of them to their respective values
    (i.e., there is no subset that generates a CF). We can also deep dive and investigate
    the relationship between features and potentially find interesting patterns.
  prefs: []
  type: TYPE_NORMAL
- en: In this particular case, it was interesting to observe that a prediction of
    high income would change if the person were a Female, Divorced, and with an own
    child. This counterfactual can lead to further discussions of the economic impacts
    on different social groups.
  prefs: []
  type: TYPE_NORMAL
- en: Image Classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As already mentioned, CFNOW can work with diverse types of data, so it can also
    generate counterfactuals for Image data. However, what does it mean to have a
    counterfactual for an image dataset?
  prefs: []
  type: TYPE_NORMAL
- en: The response can vary because there are several ways in which you can generate
    counterfactuals. It can be replacing single pixels with random noise (a method
    used by adversarial attacks) or something more complex, involving advanced segmentation
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: CFNOW uses a segmentation method called quickshift, which is a reliable and
    fast method to detect “semantic” segments. However, it is possible to integrate
    (and I invite you to do so) other segmentation techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'Segment detection alone is not sufficient to generate counterfactual explanations.
    We also need to modify the segments, replacing them with modified versions. To
    this modification, CFNOW has four options defined in the parameter `replace_mode`,
    where we can have: (default) `blur` — that adds a blur filter to the replaced
    segments, `mean` which replaces the segments by the average color, `random` that
    replaces it with random noise, and `inpaint`, which reconstructs the image based
    on neighborhood pixels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want the whole code you can find here: [https://colab.research.google.com/drive/1M6bEP4x7ilSdh01Gs8xzgMMX7Uuum5jZ?usp=sharing](https://colab.research.google.com/drive/1M6bEP4x7ilSdh01Gs8xzgMMX7Uuum5jZ?usp=sharing)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Following, I will show the code implementation of CFNOW for this type of data:'
  prefs: []
  type: TYPE_NORMAL
- en: First, again, let’s install the CFNOW package if you have not done it yet.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s add some additional packages to load a pre-trained model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then let’s load the data, load the pre-trained model and create a prediction
    function that is compatible to the data format CFNOW must receive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Most of the code work is related to building the model, getting the data, and
    adjusting it, because to generate counterfactuals with CFNOW we just need to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In the example above, we used all default optional parameters, therefore, we
    used quickshift to segment the image and replace the segments with blurred images.
    As result, we have this factual prediction below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ac67aab285c70f8c692c90648dcf713d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Factual image classified as a “daisy” | Image title: Sunflower (Helianthus
    L). Słonecznik by [Pudelek](https://commons.wikimedia.org/wiki/User:Pudelek) (Edit
    by [Yzmo](https://commons.wikimedia.org/wiki/User:Yzmo) and [Vassil](https://commons.wikimedia.org/wiki/User:Vassil))
    from [Wikimedia](https://commons.wikimedia.org/wiki/File:Sunflower_from_Silesia_Edit_2.jpg)
    under [**GNU Free Documentation License**](https://en.wikipedia.org/wiki/en:GNU_Free_Documentation_License),
    Version 1.2'
  prefs: []
  type: TYPE_NORMAL
- en: 'To the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f19664b505a98eebc99f23c465ad9b9f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'CF image classified as a “bee” | Image title: Sunflower (Helianthus L). Słonecznik
    by [Pudelek](https://commons.wikimedia.org/wiki/User:Pudelek) (Edit by [Yzmo](https://commons.wikimedia.org/wiki/User:Yzmo)
    and [Vassil](https://commons.wikimedia.org/wiki/User:Vassil)) from [Wikimedia](https://commons.wikimedia.org/wiki/File:Sunflower_from_Silesia_Edit_2.jpg)
    under [**GNU Free Documentation License**](https://en.wikipedia.org/wiki/en:GNU_Free_Documentation_License),
    Version 1.2'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, what are the outcomes from this analysis? Actually, image counterfactuals
    can be extremely useful tools to detect how the model is making the classifications.
    This can be applied in cases where: (1) we want to verify why the model made correct
    classifications — ensuring it is using correct image features: in this case, although
    it misclassified the sunflower as a daisy, we can see that blurring the flower
    (and not a background feature) makes it to change the prediction. It also can
    (2) help to diagnose misclassified images, which can lead to better insights for
    image processing and/or data acquisition.'
  prefs: []
  type: TYPE_NORMAL
- en: Textual Classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, we have textual classifiers based on embeddings. Although simple textual
    classifiers (that use a data structure more like tabular data) can use the tabular
    counterfactual generator, textual classifiers based on embeddings, this is not
    as clear.
  prefs: []
  type: TYPE_NORMAL
- en: The justification is that embeddings have a variable number of inputs and words
    that can considerably affect the prediction score and classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'CFNOW solves that with two strategies: (1) by removing evidence or (2) by adding
    antonyms. The first strategy is straightforward, to measure the impact of each
    word on the text, we simply remove them and see which ones we must remove to flip
    the classification. While adding antonyms, we can possibly keep a semantic structure
    (because removing a word can severely harm it).'
  prefs: []
  type: TYPE_NORMAL
- en: Then, the code below shows how to use CFNOW in this context.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want the entire code, you can check it here: [https://colab.research.google.com/drive/1ZMbqJmJoBukqRJGqhUaPjFFRpWlujpsi?usp=sharing](https://colab.research.google.com/drive/1ZMbqJmJoBukqRJGqhUaPjFFRpWlujpsi?usp=sharing)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'First, install the CFNOW package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, install the necessary packages for the textual classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, as in the previous sections, first, we will build the classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'For this code, we will see our factual text has a NEGATIVE sentiment with a
    high confidence (≥0.9), then let’s try to generate the counterfactual for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: With the code above, just changing a single word (but) the classification changed
    from NEGATIVE to POSITIVE with high confidence. This showcases how counterfactuals
    can be useful, since this minimal modifications can have implications on understanding
    how the model predicts sentences and/or help debugging undesirable behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This was a (relatively) brief introduction to CFNOW and Counterfactual explanations.
    There is an extensive (and increasing) literature regarding counterfactuals that
    you definitely should take a look if you want to deep dive, this seminal article
    [3] written by (my Ph.D. advisor, Prof. David Martens) is a great way to have
    a better introduction to Counterfactual Explanations. Additionally, there are
    good reviews like this one written by Verma et al [7]. In summary, counterfactual
    explanations are an easy and convinient way to explain complex machine learning
    algorithms decisions, and can do much more than explanations if correctly applied.
    CFNOW can provide an easy, fast, and flexible way to generate counterfactual explanations,
    allowing practitioners not just to explain, but also to leverage as much as possible
    the potential from their data and model.
  prefs: []
  type: TYPE_NORMAL
- en: 'References:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] — [https://github.com/marcotcr/lime](https://github.com/marcotcr/lime)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] — [https://github.com/shap/shap](https://github.com/shap/shap)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] — [https://www.jstor.org/stable/26554869](https://www.jstor.org/stable/26554869)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] — [https://www.mdpi.com/2076-3417/11/16/7274](https://www.mdpi.com/2076-3417/11/16/7274)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] — [https://arxiv.org/pdf/2306.06506.pdf](https://arxiv.org/pdf/2306.06506.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] — [https://arxiv.org/abs/2001.07417](https://arxiv.org/abs/2001.07417)'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] — [https://arxiv.org/abs/2010.10596](https://arxiv.org/abs/2010.10596)'
  prefs: []
  type: TYPE_NORMAL
