- en: 'Improve your Gradient Descent: The Epic Quest for the Optimal Stride'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/improve-your-gradient-descent-the-epic-quest-for-the-optimal-stride-4711dc6f5dba?source=collection_archive---------8-----------------------#2023-05-23](https://towardsdatascience.com/improve-your-gradient-descent-the-epic-quest-for-the-optimal-stride-4711dc6f5dba?source=collection_archive---------8-----------------------#2023-05-23)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Techniques for Optimizing Step-Size/Learning Rate in Gradient Descent for Machine
    Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://namanagr03.medium.com/?source=post_page-----4711dc6f5dba--------------------------------)[![Naman
    Agrawal](../Images/6bb885397aec17f5029cfac7f01edad9.png)](https://namanagr03.medium.com/?source=post_page-----4711dc6f5dba--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4711dc6f5dba--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4711dc6f5dba--------------------------------)
    [Naman Agrawal](https://namanagr03.medium.com/?source=post_page-----4711dc6f5dba--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5bbb90aa727&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimprove-your-gradient-descent-the-epic-quest-for-the-optimal-stride-4711dc6f5dba&user=Naman+Agrawal&userId=5bbb90aa727&source=post_page-5bbb90aa727----4711dc6f5dba---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4711dc6f5dba--------------------------------)
    ·14 min read·May 23, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4711dc6f5dba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimprove-your-gradient-descent-the-epic-quest-for-the-optimal-stride-4711dc6f5dba&user=Naman+Agrawal&userId=5bbb90aa727&source=-----4711dc6f5dba---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4711dc6f5dba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimprove-your-gradient-descent-the-epic-quest-for-the-optimal-stride-4711dc6f5dba&source=-----4711dc6f5dba---------------------bookmark_footer-----------)![](../Images/bd19435e63da6ad9aff7a330dfde7870.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image by [stokpic](https://pixabay.com/users/stokpic-692575/?utm_source=link-attribution&amp%3Butm_medium=referral&amp%3Butm_campaign=image&amp%3Butm_content=600468)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&amp%3Butm_medium=referral&amp%3Butm_campaign=image&amp%3Butm_content=600468)
  prefs: []
  type: TYPE_NORMAL
- en: Contents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Method 1: Fixed Step Size'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Method 2: Exact Line Search'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Method 3: Backtracking Line Search'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When training any machine learning model, Gradient Descent is one of the most
    commonly used techniques to optimize for the parameters. Gradient descent offers
    an efficient way of minimizing the loss function for the incoming data, especially
    in cases, where they may not be a closed-form solution to the problem. In general,
    consider a machine learning problem defined by a convex and differentiable function
    f: Rᵈ → R (most loss functions follow these properties). The goal is to find x*
    ∈ Rᵈ that minimizes the loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b833bde18991e44329d6ac0003f683be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Gradient Descent provides an iterative approach to solving this problem. The
    update rule is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5ec9b5a054d2f5965ac5262b08c9c3de.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where x⁽ᵏ⁾ refers to the value of x in the kth iteration of the algorithm,
    and tₖ refers to the step size or the learning rate of the model in the kth iteration.
    The general workflow of the algorithm is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Determine the loss function f and compute its gradient ∇f.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start with a random choice of x ∈ Rᵈ, call it x⁽⁰⁾(the starting iterate).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Until you reach the stopping criteria (e.g., the error falls below a certain
    threshold), do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A) Determine the direction along which x must be reduced or increased. Under
    gradient descent, this is given by the direction opposite to the gradient of the
    loss function evaluated at the current iterate. vₖ = ∇ₓ f(x⁽ᵏ ⁻ ¹⁾)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'B) Determine the step size or the magnitude of the change: tₖ.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'C) Update the iterate: x⁽ᵏ⁾= x⁽ᵏ ⁻ ¹⁾ − tₖ∇ₓ f(x⁽ᵏ ⁻ ¹⁾)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'That’s the entire workflow in a nutshell: Take the current iterate, look for
    the direction in which it needs to be updated (vₖ), determine the magnitude of
    the update (tₖ), and update it.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/07b2355658dcb4ce63f35fd42aec31b7.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration of Gradient Descent [Image by Author]
  prefs: []
  type: TYPE_NORMAL
- en: 'So, what’s this article about? In this article, our focus will be on step 3B:
    finding the optimal step size or the magnitude of tₖ. When it comes to gradient
    descent, this is one of the most neglected aspects of optimizing your model. The
    size of the step can greatly affect how fast your algorithm converges to the solution
    as well as the accuracy of the solution it converges to. most often, data scientists
    simply set a fixed value for the step size during the entire learning process
    or may occasionally use validation techniques to train it. But, there are many
    more efficient ways to go about solving this problem. In this article, we will
    discuss 3 different ways to determine the step size tₖ:'
  prefs: []
  type: TYPE_NORMAL
- en: Fixed Step Size
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exact Line Search
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Backtracking Line Search (Armijo’s Rule)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each of these methods, we will discuss the theory and implement it to calculate
    the first few iterates for an example. In particular, we will consider the following
    loss function to evaluate the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a4d6f7cefd8b208d5a564e89f466958f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The 3D-Plot of the function is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9c8a8138be8a65264602f8bdbfde2eda.png)'
  prefs: []
  type: TYPE_IMG
- en: Loss Function (3D Plot) [Image by Author generated using [LibreTexts](https://c3d.libretexts.org/CalcPlot3D/index.html)]
  prefs: []
  type: TYPE_NORMAL
- en: 'From the figure, it’s evident that the global minima is x* = [0; 0]. Throughout
    this article, we will manually calculate the first few iterates and computationally
    determine the number of steps for convergence for each of these methods. We will
    also trace the pattern of the descent (aka the iterate trajectory) to understand
    how each of these techniques affects the [process of convergence. Usually, it’s
    easier to refer to the contour plot of the function (instead of its 3D plot) to
    better evaluate the different trajectories that follow. The contour plot of the
    function can be easily generated via the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e50b0e25faa9beb8212094ed043b5aec.png)'
  prefs: []
  type: TYPE_IMG
- en: Contour Plot of f [Image by Author generated using Python]
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: 'Method 1: Fixed Step Size'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This method is the simplest to use, and the one most commonly used for training
    the ML model. This involves setting:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a2c27db5d282e8e68d0ec89e4ea4eb66.png)'
  prefs: []
  type: TYPE_IMG
- en: One needs to be very careful while choosing the right t under this method. While
    a small value of t can lead to very accurate solutions, the convergence can become
    quite slow. On the other hand, a large t makes the algorithm faster, but at the
    cost of accuracy. Using this method requires the implementer to carefully balance
    the trade-off between the rate of convergence and the accuracy of the solution
    yielded.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, most data scientists use validation techniques such as hold-out
    validation or k-fold cross-validation to optimize for t. This technique involves
    creating a partition of the training data (called the validation data), which
    is used to optimize for the performance by running the algorithm on a discrete
    set of values that t can take. Let’s look at our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7c9f2143f112403654b8507fec02f9a1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The first step is to compute it’s gradient:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a746fe2830d6c4404efacde46c07ab8d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For all subsequent calculations, we will take the initialization to be x⁽⁰⁾=
    [1; 1]. Under this strategy, we set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3f4c3a94f893c7064c735c06c109e18f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The first two iterates are calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c8ec49eaf61f5cf74dfe1ed286c9cbfe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We compute the remaining iterates programmatically via the following Python
    Code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In the above code, we have defined the following convergence criteria (which
    will be consistently utilized for all methods):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fd8d411ce856dbac80a73d18e31bea1e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'On running the above code, we find that it takes around 26 steps to converge.
    The following plot shows the trajectory of the iterates during the gradient descent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/4310c3950c0829f09b148a44a2862a1b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Contour Plot: Fixed Step Size = 0.1 [Image by Author generated using Python]'
  prefs: []
  type: TYPE_NORMAL
- en: 'To have a better understanding of how critical it is to choose the right t
    in this method, let’s see gauge the effect of increasing or decreasing t. If we
    decrease the value of t from 0.1 to 0.01, the number of steps to converge increases
    drastically from 26 to 295\. The iterate trajectory for this case is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9504a487fc6e5ac71a5e7bdac1473d07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Contour Plot: Fixed Step Size = 0.01 [Image by Author generated using Python]'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, on increasing the t from 0.1 to 0.2, the number of steps to converge
    decreases from 26 to a mere 11, as shown by the following trajectory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/314498c4f2e42830524e51fbdd7a0a07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Contour Plot: Fixed Step Size = 0.2 [Image by Author generated using Python]'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, it is important to note that this does not always be the case. If
    the value of the step size is too large, it is possible that the iterates simply
    bounce away from the optimal solution and never converge. In fact, increasing
    t from 0.2 to just around 0.3 causes the iterate values to shoot up, making it
    impossible to converge. This is seen from the following contour plot (with t =
    0.3) for just the first 8 steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ddcfcec2dd06b6afb851b027dc0e092d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Contour Plot: Fixed Step Size = 0.3 [Image by Author generated using Python]'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, it is evident that finding the right value of t is extremely vital in
    this method and even a small increase or decrease can greatly affect the algorithm’s
    ability to converge. Now, let’s talk about the next method to determine t.
  prefs: []
  type: TYPE_NORMAL
- en: 'Method 2: Exact Line Search'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this method, we don’t assign a simple pre-fixed value of t at every iteration.
    Instead, we treat the problem of finding the optimal t itself as a 1D optimization
    problem. In other words, we are interested in finding the best update t, that
    minimizes the value of the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c6b7515d4864a734d9cbf7340e03e547.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Notice how cool this is! We have a multi-dimensional optimization problem (minimizing
    f) that we attempt to solve using gradient descent. We know the best direction
    to update our iterate (vₖ = − ∇ₓ f(x⁽ᵏ ⁻ ¹⁾)), but we need to find the optimal
    step size tₖ. In other words, the value of the function for the next iterate only
    depends on the value of tₖ that we chose to use. So, we treat this as another
    (but simpler!) optimization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4bef8d9aaa5ea116affcf9ab732eb870.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, we update x⁽ᵏ⁾ to be the iterate that best minimizes the loss function
    f. This really helps increase the rate of convergence. However, it also adds an
    additional time requirement: To compute the minimizer of g(t). Usually, this isn’t
    a problem since it’s a 1D function, but sometimes it could take longer than expected.
    Thus, while using this method, it’s important to balance the trade-off between
    the number of steps reduced to converge and the additional time requirement to
    compute the argmin. Let’s look at our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/88e51c2844dbb22ff81ac17c789e8711.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The first two iterates are calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d6f59641966f55e3207383065f4c07de.png)'
  prefs: []
  type: TYPE_IMG
- en: We compute the remaining iterates programmatically via the following Python
    Code
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Just as before, in the above code, we have defined the following convergence
    criteria (which will be consistently utilized for all methods):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fd8d411ce856dbac80a73d18e31bea1e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'On running the above code, we find that it takes only 10 steps to converge
    ( a major improvement from the fixed step size). The following plot shows the
    trajectory of the iterates during the gradient descent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/01a5b6d8820366f31f5fc0cab5273946.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Contour Plot: Exact Line Search [Image by Author generated using Python]'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s talk about the next method to determine t.
  prefs: []
  type: TYPE_NORMAL
- en: 'Method 3: Backtracking Line Search'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Backtracking is an adaptive method of choosing the optimal step size. In my
    experience, I have found this to be one of the most useful strategies for optimizing
    the step size. The convergence is usually much faster than fixed step size without
    the complications of maximizing a 1D function g(t) in an exact line search. This
    method involves starting out with a rather large step size (t¯ = 1) and continuing
    to decrease it until a required decrease in f(x) is observed. Let us first take
    a look at the algorithm and subsequently, we will be discussing the specifics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e4ee0a61ec5b9b0c4d8d7987dc8cd43c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Algorithm 1: Backtracking (Armijo–Goldstein condition) [Image by Author]'
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, we start with a large step size (which is important usually
    in the initial stages of the algorithm) and check if it helps us improve the current
    iterate by a given threshold. If the step size is found to be too large, we reduce
    it by multiplying it with a scalar constant β ∈ (0, 1). We repeat this process
    until a desired decrease in f is obtained. Specifically, we choose the largest
    t such that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/203a2bc30892cf22af6bd0f28f788373.png)'
  prefs: []
  type: TYPE_IMG
- en: 'i.e., the decrease is at least σt || ∇ₓ f(x⁽ᵏ ⁻ ¹⁾) ||². But, why this value?
    It can be mathematically shown (via Taylor’s first order expansion) that t ||
    ∇ₓ f(x⁽ᵏ ⁻ ¹⁾) ||² is the minimum decrease in f that we can expect through an
    improvement made during the current iteration. There is an additional factor of
    σ in the condition. This is to account for the fact, that even if we cannot achieve
    the theoretically guaranteed decrease t || ∇ₓ f(x⁽ᵏ ⁻ ¹⁾) ||², we at least hope
    to achieve a fraction of it scaled by σ. That is to say, we require that the achieved
    reduction if f be at least a fixed fraction σ of the reduction promised by the
    first-order Taylor approximation of f at x⁽ᵏ⁾. If the condition is not fulfilled,
    we scale down t to a smaller value via β. Let’s look at our example (setting t¯=
    1, σ = β = 0.5):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/88e51c2844dbb22ff81ac17c789e8711.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The first two iterates are calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2364678ddcf744731712fc9c5c0f9482.png)'
  prefs: []
  type: TYPE_IMG
- en: Likewise,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d4a875e91d02e832711496c5cd647c0c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We compute the remaining iterates programmatically via the following Python
    Code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Just as before, in the above code, we have defined the following convergence
    criteria (which will be consistently utilized for all methods):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fd8d411ce856dbac80a73d18e31bea1e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'On running the above code, we find that it takes only 10 steps to converge.
    The following plot shows the trajectory of the iterates during the gradient descent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/1e5cba8e49668b16b60db196d0898156.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Contour Plot: Backtracking [Image by Author generated using Python]'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this article, we familiarised ourselves with some of the useful techniques
    to optimize for the step size in the gradient descent algorithm. In particular,
    we covered 3 main techniques: Fixed Step Size, which involves maintaining the
    same step size or learning rate throughout the training process, Exact Line Search,
    which involves minimizing the loss as a function of t, and Armijo Backtracking
    involving a gradual reduction in the step size until a threshold is met. While
    these are some of the most fundamental techniques that you can use to tune your
    optimization, there exist a vast array of other methods (eg. setting t as a function
    of the number of iterations). These tools are generally used in more complex settings,
    such as Stochastic Gradient Descent. The purpose of this article was not only
    to introduce you to these techniques but also to make you aware of the intricacies
    that can affect your optimization algorithm. While most of these techniques are
    used in the context of Gradient descent, they can also be applied to other optimization
    algorithms (e.g., Newton-Raphson Method). Each of these techniques has its own
    merits and may be preferred over the others for specific applications and algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: Hope you enjoyed reading this article! In case you have any doubts or suggestions,
    do reply in the comment box. Please feel free to contact me via [mail](mailto:naman.agr03@gmail.com).
  prefs: []
  type: TYPE_NORMAL
- en: If you liked my article and want to read more of them, please follow me.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: All images have been made by the author.'
  prefs: []
  type: TYPE_NORMAL
