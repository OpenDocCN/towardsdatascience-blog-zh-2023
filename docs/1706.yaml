- en: 'Improve your Gradient Descent: The Epic Quest for the Optimal Stride'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改善你的梯度下降：寻找最优步幅的史诗之旅
- en: 原文：[https://towardsdatascience.com/improve-your-gradient-descent-the-epic-quest-for-the-optimal-stride-4711dc6f5dba?source=collection_archive---------8-----------------------#2023-05-23](https://towardsdatascience.com/improve-your-gradient-descent-the-epic-quest-for-the-optimal-stride-4711dc6f5dba?source=collection_archive---------8-----------------------#2023-05-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/improve-your-gradient-descent-the-epic-quest-for-the-optimal-stride-4711dc6f5dba?source=collection_archive---------8-----------------------#2023-05-23](https://towardsdatascience.com/improve-your-gradient-descent-the-epic-quest-for-the-optimal-stride-4711dc6f5dba?source=collection_archive---------8-----------------------#2023-05-23)
- en: Techniques for Optimizing Step-Size/Learning Rate in Gradient Descent for Machine
    Learning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化梯度下降步长/学习率的技术
- en: '[](https://namanagr03.medium.com/?source=post_page-----4711dc6f5dba--------------------------------)[![Naman
    Agrawal](../Images/6bb885397aec17f5029cfac7f01edad9.png)](https://namanagr03.medium.com/?source=post_page-----4711dc6f5dba--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4711dc6f5dba--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4711dc6f5dba--------------------------------)
    [Naman Agrawal](https://namanagr03.medium.com/?source=post_page-----4711dc6f5dba--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://namanagr03.medium.com/?source=post_page-----4711dc6f5dba--------------------------------)[![Naman
    Agrawal](../Images/6bb885397aec17f5029cfac7f01edad9.png)](https://namanagr03.medium.com/?source=post_page-----4711dc6f5dba--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4711dc6f5dba--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4711dc6f5dba--------------------------------)
    [Naman Agrawal](https://namanagr03.medium.com/?source=post_page-----4711dc6f5dba--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5bbb90aa727&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimprove-your-gradient-descent-the-epic-quest-for-the-optimal-stride-4711dc6f5dba&user=Naman+Agrawal&userId=5bbb90aa727&source=post_page-5bbb90aa727----4711dc6f5dba---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4711dc6f5dba--------------------------------)
    ·14 min read·May 23, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4711dc6f5dba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimprove-your-gradient-descent-the-epic-quest-for-the-optimal-stride-4711dc6f5dba&user=Naman+Agrawal&userId=5bbb90aa727&source=-----4711dc6f5dba---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5bbb90aa727&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimprove-your-gradient-descent-the-epic-quest-for-the-optimal-stride-4711dc6f5dba&user=Naman+Agrawal&userId=5bbb90aa727&source=post_page-5bbb90aa727----4711dc6f5dba---------------------post_header-----------)
    发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----4711dc6f5dba--------------------------------)
    ·14分钟阅读·2023年5月23日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4711dc6f5dba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimprove-your-gradient-descent-the-epic-quest-for-the-optimal-stride-4711dc6f5dba&user=Naman+Agrawal&userId=5bbb90aa727&source=-----4711dc6f5dba---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4711dc6f5dba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimprove-your-gradient-descent-the-epic-quest-for-the-optimal-stride-4711dc6f5dba&source=-----4711dc6f5dba---------------------bookmark_footer-----------)![](../Images/bd19435e63da6ad9aff7a330dfde7870.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4711dc6f5dba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimprove-your-gradient-descent-the-epic-quest-for-the-optimal-stride-4711dc6f5dba&source=-----4711dc6f5dba---------------------bookmark_footer-----------)![](../Images/bd19435e63da6ad9aff7a330dfde7870.png)'
- en: Image by [stokpic](https://pixabay.com/users/stokpic-692575/?utm_source=link-attribution&amp%3Butm_medium=referral&amp%3Butm_campaign=image&amp%3Butm_content=600468)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&amp%3Butm_medium=referral&amp%3Butm_campaign=image&amp%3Butm_content=600468)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[stokpic](https://pixabay.com/users/stokpic-692575/?utm_source=link-attribution&amp%3Butm_medium=referral&amp%3Butm_campaign=image&amp%3Butm_content=600468)提供，来自[Pixabay](https://pixabay.com//?utm_source=link-attribution&amp%3Butm_medium=referral&amp%3Butm_campaign=image&amp%3Butm_content=600468)
- en: Contents
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目录
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 介绍
- en: 'Method 1: Fixed Step Size'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 方法 1：固定步长
- en: 'Method 2: Exact Line Search'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 方法 2：精确线搜索
- en: 'Method 3: Backtracking Line Search'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 方法 3：回溯线搜索
- en: Conclusion
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结论
- en: Introduction
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'When training any machine learning model, Gradient Descent is one of the most
    commonly used techniques to optimize for the parameters. Gradient descent offers
    an efficient way of minimizing the loss function for the incoming data, especially
    in cases, where they may not be a closed-form solution to the problem. In general,
    consider a machine learning problem defined by a convex and differentiable function
    f: Rᵈ → R (most loss functions follow these properties). The goal is to find x*
    ∈ Rᵈ that minimizes the loss function:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '在训练任何机器学习模型时，梯度下降是最常用的优化参数的技术之一。梯度下降提供了一种高效的方式来最小化传入数据的损失函数，特别是在没有问题的封闭解的情况下。一般来说，考虑一个由凸函数和可微分函数f:
    Rᵈ → R定义的机器学习问题（大多数损失函数都具有这些属性）。目标是找到x* ∈ Rᵈ，使损失函数最小化：'
- en: '![](../Images/b833bde18991e44329d6ac0003f683be.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b833bde18991e44329d6ac0003f683be.png)'
- en: 'Gradient Descent provides an iterative approach to solving this problem. The
    update rule is given as:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降提供了一种迭代方法来解决这个问题。更新规则如下：
- en: '![](../Images/5ec9b5a054d2f5965ac5262b08c9c3de.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5ec9b5a054d2f5965ac5262b08c9c3de.png)'
- en: 'Where x⁽ᵏ⁾ refers to the value of x in the kth iteration of the algorithm,
    and tₖ refers to the step size or the learning rate of the model in the kth iteration.
    The general workflow of the algorithm is given as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 其中x⁽ᵏ⁾指算法第k次迭代中x的值，tₖ指第k次迭代中模型的步长或学习率。算法的一般工作流程如下：
- en: Determine the loss function f and compute its gradient ∇f.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定损失函数f并计算其梯度∇f。
- en: Start with a random choice of x ∈ Rᵈ, call it x⁽⁰⁾(the starting iterate).
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从随机选择一个x ∈ Rᵈ开始，称之为x⁽⁰⁾（起始迭代）。
- en: 'Until you reach the stopping criteria (e.g., the error falls below a certain
    threshold), do the following:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直到达到停止标准（例如，误差低于某个阈值），执行以下操作：
- en: A) Determine the direction along which x must be reduced or increased. Under
    gradient descent, this is given by the direction opposite to the gradient of the
    loss function evaluated at the current iterate. vₖ = ∇ₓ f(x⁽ᵏ ⁻ ¹⁾)
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: A) 确定x必须减少或增加的方向。在梯度下降中，这是由当前迭代点的损失函数梯度的相反方向给出的。vₖ = ∇ₓ f(x⁽ᵏ ⁻ ¹⁾)
- en: 'B) Determine the step size or the magnitude of the change: tₖ.'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: B) 确定步长或变化的幅度：tₖ。
- en: 'C) Update the iterate: x⁽ᵏ⁾= x⁽ᵏ ⁻ ¹⁾ − tₖ∇ₓ f(x⁽ᵏ ⁻ ¹⁾)'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: C) 更新迭代：x⁽ᵏ⁾= x⁽ᵏ ⁻ ¹⁾ − tₖ∇ₓ f(x⁽ᵏ ⁻ ¹⁾)
- en: 'That’s the entire workflow in a nutshell: Take the current iterate, look for
    the direction in which it needs to be updated (vₖ), determine the magnitude of
    the update (tₖ), and update it.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是整个工作流程的核心：获取当前迭代，寻找需要更新的方向（vₖ），确定更新的幅度（tₖ），并进行更新。
- en: '![](../Images/07b2355658dcb4ce63f35fd42aec31b7.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07b2355658dcb4ce63f35fd42aec31b7.png)'
- en: Illustration of Gradient Descent [Image by Author]
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降的示意图 [作者提供的图片]
- en: 'So, what’s this article about? In this article, our focus will be on step 3B:
    finding the optimal step size or the magnitude of tₖ. When it comes to gradient
    descent, this is one of the most neglected aspects of optimizing your model. The
    size of the step can greatly affect how fast your algorithm converges to the solution
    as well as the accuracy of the solution it converges to. most often, data scientists
    simply set a fixed value for the step size during the entire learning process
    or may occasionally use validation techniques to train it. But, there are many
    more efficient ways to go about solving this problem. In this article, we will
    discuss 3 different ways to determine the step size tₖ:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这篇文章的主题是什么？在这篇文章中，我们将重点关注第3B步：寻找最优步长或tₖ的幅度。对于梯度下降来说，这是优化模型时最容易被忽视的方面之一。步长的大小可以大大影响算法收敛到解决方案的速度以及收敛到的解决方案的准确性。大多数情况下，数据科学家仅在整个学习过程中设置一个固定值的步长，或者偶尔使用验证技术来训练它。但解决这个问题还有许多更高效的方法。在这篇文章中，我们将讨论确定步长tₖ的三种不同方法：
- en: Fixed Step Size
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 固定步长
- en: Exact Line Search
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 精确线搜索
- en: Backtracking Line Search (Armijo’s Rule)
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回溯线搜索（阿米乔规则）
- en: 'For each of these methods, we will discuss the theory and implement it to calculate
    the first few iterates for an example. In particular, we will consider the following
    loss function to evaluate the model:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些方法中的每一种，我们将讨论理论并实现它以计算示例的前几个迭代。特别是，我们将考虑以下损失函数来评估模型：
- en: '![](../Images/a4d6f7cefd8b208d5a564e89f466958f.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4d6f7cefd8b208d5a564e89f466958f.png)'
- en: 'The 3D-Plot of the function is shown below:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是该函数的3D图：
- en: '![](../Images/9c8a8138be8a65264602f8bdbfde2eda.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c8a8138be8a65264602f8bdbfde2eda.png)'
- en: Loss Function (3D Plot) [Image by Author generated using [LibreTexts](https://c3d.libretexts.org/CalcPlot3D/index.html)]
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数（3D 图） [由作者使用 [LibreTexts](https://c3d.libretexts.org/CalcPlot3D/index.html)
    生成的图片]
- en: 'From the figure, it’s evident that the global minima is x* = [0; 0]. Throughout
    this article, we will manually calculate the first few iterates and computationally
    determine the number of steps for convergence for each of these methods. We will
    also trace the pattern of the descent (aka the iterate trajectory) to understand
    how each of these techniques affects the [process of convergence. Usually, it’s
    easier to refer to the contour plot of the function (instead of its 3D plot) to
    better evaluate the different trajectories that follow. The contour plot of the
    function can be easily generated via the following code:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中可以明显看出，全球最小值为 x* = [0; 0]。在本文中，我们将手动计算前几次迭代，并计算每种方法的收敛步骤数。我们还将跟踪下降模式（即迭代轨迹）以了解这些技术如何影响[收敛过程。通常，参考函数的等高线图（而不是其
    3D 图）可以更好地评估不同的轨迹。函数的等高线图可以通过以下代码轻松生成：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/e50b0e25faa9beb8212094ed043b5aec.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e50b0e25faa9beb8212094ed043b5aec.png)'
- en: Contour Plot of f [Image by Author generated using Python]
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: f 的等高线图 [由作者使用 Python 生成的图片]
- en: Let’s get started!
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: 'Method 1: Fixed Step Size'
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法 1：固定步长
- en: 'This method is the simplest to use, and the one most commonly used for training
    the ML model. This involves setting:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法最简单易用，也是训练 ML 模型时最常用的方法。这涉及到设置：
- en: '![](../Images/a2c27db5d282e8e68d0ec89e4ea4eb66.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2c27db5d282e8e68d0ec89e4ea4eb66.png)'
- en: One needs to be very careful while choosing the right t under this method. While
    a small value of t can lead to very accurate solutions, the convergence can become
    quite slow. On the other hand, a large t makes the algorithm faster, but at the
    cost of accuracy. Using this method requires the implementer to carefully balance
    the trade-off between the rate of convergence and the accuracy of the solution
    yielded.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用这种方法时，选择合适的 t 值需要非常小心。虽然较小的 t 值可以导致非常准确的解决方案，但收敛速度可能会变得相当慢。另一方面，较大的 t 值使算法更快，但会牺牲准确性。使用这种方法要求实施者仔细平衡收敛速度和所获得解决方案的准确性之间的权衡。
- en: 'In practice, most data scientists use validation techniques such as hold-out
    validation or k-fold cross-validation to optimize for t. This technique involves
    creating a partition of the training data (called the validation data), which
    is used to optimize for the performance by running the algorithm on a discrete
    set of values that t can take. Let’s look at our example:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，大多数数据科学家使用验证技术，如保留验证或 k 折交叉验证，来优化 t。该技术涉及创建训练数据的一个分区（称为验证数据），该数据用于通过在 t
    可以取的离散值集上运行算法来优化性能。让我们来看一下我们的例子：
- en: '![](../Images/7c9f2143f112403654b8507fec02f9a1.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c9f2143f112403654b8507fec02f9a1.png)'
- en: 'The first step is to compute it’s gradient:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是计算它的梯度：
- en: '![](../Images/a746fe2830d6c4404efacde46c07ab8d.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a746fe2830d6c4404efacde46c07ab8d.png)'
- en: 'For all subsequent calculations, we will take the initialization to be x⁽⁰⁾=
    [1; 1]. Under this strategy, we set:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有后续计算，我们将初始化设为 x⁽⁰⁾= [1; 1]。在这种策略下，我们设置：
- en: '![](../Images/3f4c3a94f893c7064c735c06c109e18f.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f4c3a94f893c7064c735c06c109e18f.png)'
- en: 'The first two iterates are calculated as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 前两次迭代的计算如下：
- en: '![](../Images/c8ec49eaf61f5cf74dfe1ed286c9cbfe.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c8ec49eaf61f5cf74dfe1ed286c9cbfe.png)'
- en: 'We compute the remaining iterates programmatically via the following Python
    Code:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过以下 Python 代码程序化计算其余的迭代：
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In the above code, we have defined the following convergence criteria (which
    will be consistently utilized for all methods):'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们定义了以下收敛标准（将始终用于所有方法）：
- en: '![](../Images/fd8d411ce856dbac80a73d18e31bea1e.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd8d411ce856dbac80a73d18e31bea1e.png)'
- en: 'On running the above code, we find that it takes around 26 steps to converge.
    The following plot shows the trajectory of the iterates during the gradient descent:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，我们发现收敛大约需要26步。下面的图展示了在梯度下降过程中迭代轨迹：
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/4310c3950c0829f09b148a44a2862a1b.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4310c3950c0829f09b148a44a2862a1b.png)'
- en: 'Contour Plot: Fixed Step Size = 0.1 [Image by Author generated using Python]'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 等高线图：固定步长 = 0.1 [由作者使用 Python 生成的图片]
- en: 'To have a better understanding of how critical it is to choose the right t
    in this method, let’s see gauge the effect of increasing or decreasing t. If we
    decrease the value of t from 0.1 to 0.01, the number of steps to converge increases
    drastically from 26 to 295\. The iterate trajectory for this case is shown below:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解在这种方法中选择正确的t有多关键，我们来衡量一下增加或减少t的效果。如果我们将t的值从0.1减少到0.01，收敛的步数会从26骤增至295。该情况的迭代轨迹如下所示：
- en: '![](../Images/9504a487fc6e5ac71a5e7bdac1473d07.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9504a487fc6e5ac71a5e7bdac1473d07.png)'
- en: 'Contour Plot: Fixed Step Size = 0.01 [Image by Author generated using Python]'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓图：固定步长 = 0.01 [作者使用Python生成的图像]
- en: 'However, on increasing the t from 0.1 to 0.2, the number of steps to converge
    decreases from 26 to a mere 11, as shown by the following trajectory:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，将t从0.1增加到0.2时，收敛的步数从26减少到仅仅11，如下所示的轨迹所示：
- en: '![](../Images/314498c4f2e42830524e51fbdd7a0a07.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/314498c4f2e42830524e51fbdd7a0a07.png)'
- en: 'Contour Plot: Fixed Step Size = 0.2 [Image by Author generated using Python]'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓图：固定步长 = 0.2 [作者使用Python生成的图像]
- en: 'However, it is important to note that this does not always be the case. If
    the value of the step size is too large, it is possible that the iterates simply
    bounce away from the optimal solution and never converge. In fact, increasing
    t from 0.2 to just around 0.3 causes the iterate values to shoot up, making it
    impossible to converge. This is seen from the following contour plot (with t =
    0.3) for just the first 8 steps:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，需要注意的是，这并不总是如此。如果步长的值过大，可能导致迭代过程偏离最优解而无法收敛。实际上，将t从0.2增加到约0.3会导致迭代值急剧上升，使得无法收敛。这可以从以下的轮廓图（t
    = 0.3）看出，仅针对前8步：
- en: '![](../Images/ddcfcec2dd06b6afb851b027dc0e092d.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ddcfcec2dd06b6afb851b027dc0e092d.png)'
- en: 'Contour Plot: Fixed Step Size = 0.3 [Image by Author generated using Python]'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓图：固定步长 = 0.3 [作者使用Python生成的图像]
- en: Thus, it is evident that finding the right value of t is extremely vital in
    this method and even a small increase or decrease can greatly affect the algorithm’s
    ability to converge. Now, let’s talk about the next method to determine t.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，显而易见的是，在此方法中找到正确的t值极其重要，甚至小幅的增加或减少都可能极大地影响算法的收敛能力。现在，让我们谈谈确定t的下一种方法。
- en: 'Method 2: Exact Line Search'
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法2：精确线搜索
- en: 'In this method, we don’t assign a simple pre-fixed value of t at every iteration.
    Instead, we treat the problem of finding the optimal t itself as a 1D optimization
    problem. In other words, we are interested in finding the best update t, that
    minimizes the value of the function:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，我们不会在每次迭代时分配一个简单的预设t值。相反，我们将找到最优t的问题本身视为一个一维优化问题。换句话说，我们关注于找到最优的更新t，以最小化函数值：
- en: '![](../Images/c6b7515d4864a734d9cbf7340e03e547.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c6b7515d4864a734d9cbf7340e03e547.png)'
- en: 'Notice how cool this is! We have a multi-dimensional optimization problem (minimizing
    f) that we attempt to solve using gradient descent. We know the best direction
    to update our iterate (vₖ = − ∇ₓ f(x⁽ᵏ ⁻ ¹⁾)), but we need to find the optimal
    step size tₖ. In other words, the value of the function for the next iterate only
    depends on the value of tₖ that we chose to use. So, we treat this as another
    (but simpler!) optimization problem:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这有多酷！我们有一个多维优化问题（最小化f），我们尝试使用梯度下降来解决。我们知道更新迭代值的最佳方向（vₖ = − ∇ₓ f(x⁽ᵏ ⁻ ¹⁾）），但我们需要找到最优步长tₖ。换句话说，下一次迭代的函数值仅依赖于我们选择使用的tₖ值。因此，我们将其视为另一个（但更简单的！）优化问题：
- en: '![](../Images/4bef8d9aaa5ea116affcf9ab732eb870.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4bef8d9aaa5ea116affcf9ab732eb870.png)'
- en: 'So, we update x⁽ᵏ⁾ to be the iterate that best minimizes the loss function
    f. This really helps increase the rate of convergence. However, it also adds an
    additional time requirement: To compute the minimizer of g(t). Usually, this isn’t
    a problem since it’s a 1D function, but sometimes it could take longer than expected.
    Thus, while using this method, it’s important to balance the trade-off between
    the number of steps reduced to converge and the additional time requirement to
    compute the argmin. Let’s look at our example:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们更新x⁽ᵏ⁾，使其成为最小化损失函数f的最佳迭代。这确实有助于提高收敛速度。然而，这也增加了额外的时间要求：计算g(t)的最小化器。通常，这不是问题，因为它是一个一维函数，但有时可能会花费比预期更长的时间。因此，在使用这种方法时，平衡减少收敛步数与计算argmin的额外时间要求之间的权衡是很重要的。让我们看看我们的例子：
- en: '![](../Images/88e51c2844dbb22ff81ac17c789e8711.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/88e51c2844dbb22ff81ac17c789e8711.png)'
- en: 'The first two iterates are calculated as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个迭代值计算如下：
- en: '![](../Images/d6f59641966f55e3207383065f4c07de.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6f59641966f55e3207383065f4c07de.png)'
- en: We compute the remaining iterates programmatically via the following Python
    Code
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过以下Python代码以编程方式计算其余迭代
- en: '[PRE4]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Just as before, in the above code, we have defined the following convergence
    criteria (which will be consistently utilized for all methods):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 和以前一样，在上述代码中，我们定义了以下收敛标准（所有方法将始终使用）：
- en: '![](../Images/fd8d411ce856dbac80a73d18e31bea1e.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd8d411ce856dbac80a73d18e31bea1e.png)'
- en: 'On running the above code, we find that it takes only 10 steps to converge
    ( a major improvement from the fixed step size). The following plot shows the
    trajectory of the iterates during the gradient descent:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述`code`时，我们发现仅需10步即可收敛（这是相对于固定步长的重大改进）。以下图表显示了梯度下降过程中的迭代轨迹：
- en: '[PRE5]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/01a5b6d8820366f31f5fc0cab5273946.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01a5b6d8820366f31f5fc0cab5273946.png)'
- en: 'Contour Plot: Exact Line Search [Image by Author generated using Python]'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 等高线图：精确线搜索 [作者使用Python生成的图像]
- en: Now, let’s talk about the next method to determine t.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论确定t的下一种方法。
- en: 'Method 3: Backtracking Line Search'
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法3：回溯线搜索
- en: 'Backtracking is an adaptive method of choosing the optimal step size. In my
    experience, I have found this to be one of the most useful strategies for optimizing
    the step size. The convergence is usually much faster than fixed step size without
    the complications of maximizing a 1D function g(t) in an exact line search. This
    method involves starting out with a rather large step size (t¯ = 1) and continuing
    to decrease it until a required decrease in f(x) is observed. Let us first take
    a look at the algorithm and subsequently, we will be discussing the specifics:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 回溯法是一种选择最优步长的自适应方法。根据我的经验，这被认为是优化步长的最有用策略之一。其收敛速度通常比固定步长要快，而无需在精确线搜索中最大化一维函数g(t)的复杂性。该方法包括从一个较大的步长（t¯
    = 1）开始，并继续减小步长，直到观察到f(x)的所需减少。我们首先来看看算法，随后将讨论具体细节：
- en: '![](../Images/e4ee0a61ec5b9b0c4d8d7987dc8cd43c.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4ee0a61ec5b9b0c4d8d7987dc8cd43c.png)'
- en: 'Algorithm 1: Backtracking (Armijo–Goldstein condition) [Image by Author]'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 算法1：回溯（Armijo–Goldstein条件）[作者提供的图像]
- en: 'In other words, we start with a large step size (which is important usually
    in the initial stages of the algorithm) and check if it helps us improve the current
    iterate by a given threshold. If the step size is found to be too large, we reduce
    it by multiplying it with a scalar constant β ∈ (0, 1). We repeat this process
    until a desired decrease in f is obtained. Specifically, we choose the largest
    t such that:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们从一个较大的步长开始（这在算法的初始阶段通常很重要），并检查它是否有助于我们按给定阈值改善当前迭代。如果步长过大，我们通过将其与标量常数β
    ∈ (0, 1)相乘来减少它。我们重复这个过程，直到获得所需的f减少。具体来说，我们选择最大的t，使得：
- en: '![](../Images/203a2bc30892cf22af6bd0f28f788373.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/203a2bc30892cf22af6bd0f28f788373.png)'
- en: 'i.e., the decrease is at least σt || ∇ₓ f(x⁽ᵏ ⁻ ¹⁾) ||². But, why this value?
    It can be mathematically shown (via Taylor’s first order expansion) that t ||
    ∇ₓ f(x⁽ᵏ ⁻ ¹⁾) ||² is the minimum decrease in f that we can expect through an
    improvement made during the current iteration. There is an additional factor of
    σ in the condition. This is to account for the fact, that even if we cannot achieve
    the theoretically guaranteed decrease t || ∇ₓ f(x⁽ᵏ ⁻ ¹⁾) ||², we at least hope
    to achieve a fraction of it scaled by σ. That is to say, we require that the achieved
    reduction if f be at least a fixed fraction σ of the reduction promised by the
    first-order Taylor approximation of f at x⁽ᵏ⁾. If the condition is not fulfilled,
    we scale down t to a smaller value via β. Let’s look at our example (setting t¯=
    1, σ = β = 0.5):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 即，减少至少为σt || ∇ₓ f(x⁽ᵏ ⁻ ¹⁾) ||²。但是，为什么选择这个值？可以通过数学方法（通过泰勒一阶展开）证明t || ∇ₓ f(x⁽ᵏ
    ⁻ ¹⁾) ||²是我们可以期望在当前迭代中通过改进得到的f的最小减少。条件中有一个额外的σ因子。这是为了考虑即使我们不能实现理论上保证的减少t || ∇ₓ
    f(x⁽ᵏ ⁻ ¹⁾) ||²，我们至少希望达到由σ缩放的该减少的一部分。也就是说，我们要求减少的f至少是f在x⁽ᵏ⁾的泰勒一阶近似所承诺的减少的固定比例σ。如果条件不满足，我们通过β将t缩小到较小值。我们来看一下我们的例子（设置t¯=
    1, σ = β = 0.5）：
- en: '![](../Images/88e51c2844dbb22ff81ac17c789e8711.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/88e51c2844dbb22ff81ac17c789e8711.png)'
- en: 'The first two iterates are calculated as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个迭代计算如下：
- en: '![](../Images/2364678ddcf744731712fc9c5c0f9482.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2364678ddcf744731712fc9c5c0f9482.png)'
- en: Likewise,
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，
- en: '![](../Images/d4a875e91d02e832711496c5cd647c0c.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d4a875e91d02e832711496c5cd647c0c.png)'
- en: 'We compute the remaining iterates programmatically via the following Python
    Code:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过以下Python代码编程计算剩余迭代：
- en: '[PRE6]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Just as before, in the above code, we have defined the following convergence
    criteria (which will be consistently utilized for all methods):'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，在上述代码中，我们定义了以下收敛标准（将为所有方法一致使用）：
- en: '![](../Images/fd8d411ce856dbac80a73d18e31bea1e.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd8d411ce856dbac80a73d18e31bea1e.png)'
- en: 'On running the above code, we find that it takes only 10 steps to converge.
    The following plot shows the trajectory of the iterates during the gradient descent:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码时，我们发现仅需10步即可收敛。以下图示显示了梯度下降过程中的迭代轨迹：
- en: '[PRE7]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/1e5cba8e49668b16b60db196d0898156.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e5cba8e49668b16b60db196d0898156.png)'
- en: 'Contour Plot: Backtracking [Image by Author generated using Python]'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 等高线图：回溯 [图像由作者使用Python生成]
- en: Conclusion
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'In this article, we familiarised ourselves with some of the useful techniques
    to optimize for the step size in the gradient descent algorithm. In particular,
    we covered 3 main techniques: Fixed Step Size, which involves maintaining the
    same step size or learning rate throughout the training process, Exact Line Search,
    which involves minimizing the loss as a function of t, and Armijo Backtracking
    involving a gradual reduction in the step size until a threshold is met. While
    these are some of the most fundamental techniques that you can use to tune your
    optimization, there exist a vast array of other methods (eg. setting t as a function
    of the number of iterations). These tools are generally used in more complex settings,
    such as Stochastic Gradient Descent. The purpose of this article was not only
    to introduce you to these techniques but also to make you aware of the intricacies
    that can affect your optimization algorithm. While most of these techniques are
    used in the context of Gradient descent, they can also be applied to other optimization
    algorithms (e.g., Newton-Raphson Method). Each of these techniques has its own
    merits and may be preferred over the others for specific applications and algorithms.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们了解了一些优化梯度下降算法步长的有用技巧。特别是，我们介绍了三种主要技巧：固定步长，即在训练过程中保持相同的步长或学习率，精确线搜索，即将损失函数最小化为t的函数，以及Armijo回溯，即逐渐减少步长直到满足阈值。虽然这些是调优优化的最基本技巧，但还有大量其他方法（例如，将t设置为迭代次数的函数）。这些工具通常用于更复杂的环境，如随机梯度下降。本文的目的不仅是介绍这些技巧，还使你意识到可能影响优化算法的复杂性。虽然大多数技巧用于梯度下降，但它们也可以应用于其他优化算法（例如牛顿-拉夫森方法）。每种技巧都有其优点，并可能在特定应用和算法中优于其他技巧。
- en: Hope you enjoyed reading this article! In case you have any doubts or suggestions,
    do reply in the comment box. Please feel free to contact me via [mail](mailto:naman.agr03@gmail.com).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你喜欢阅读这篇文章！如有任何疑问或建议，请在评论框中回复。欢迎通过[邮件](mailto:naman.agr03@gmail.com)与我联系。
- en: If you liked my article and want to read more of them, please follow me.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢我的文章并想阅读更多，请关注我。
- en: 'Note: All images have been made by the author.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：所有图像均由作者制作。
