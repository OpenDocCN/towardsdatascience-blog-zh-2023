- en: 'Improve your Gradient Descent: The Epic Quest for the Optimal Stride'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改善你的梯度下降：寻找最优步幅的史诗之旅
- en: 原文：[https://towardsdatascience.com/improve-your-gradient-descent-the-epic-quest-for-the-optimal-stride-4711dc6f5dba?source=collection_archive---------8-----------------------#2023-05-23](https://towardsdatascience.com/improve-your-gradient-descent-the-epic-quest-for-the-optimal-stride-4711dc6f5dba?source=collection_archive---------8-----------------------#2023-05-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/improve-your-gradient-descent-the-epic-quest-for-the-optimal-stride-4711dc6f5dba?source=collection_archive---------8-----------------------#2023-05-23](https://towardsdatascience.com/improve-your-gradient-descent-the-epic-quest-for-the-optimal-stride-4711dc6f5dba?source=collection_archive---------8-----------------------#2023-05-23)
- en: Techniques for Optimizing Step-Size/Learning Rate in Gradient Descent for Machine
    Learning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化梯度下降步长/学习率的技术
- en: '[](https://namanagr03.medium.com/?source=post_page-----4711dc6f5dba--------------------------------)[![Naman
    Agrawal](../Images/6bb885397aec17f5029cfac7f01edad9.png)](https://namanagr03.medium.com/?source=post_page-----4711dc6f5dba--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4711dc6f5dba--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4711dc6f5dba--------------------------------)
    [Naman Agrawal](https://namanagr03.medium.com/?source=post_page-----4711dc6f5dba--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://namanagr03.medium.com/?source=post_page-----4711dc6f5dba--------------------------------)[![Naman
    Agrawal](../Images/6bb885397aec17f5029cfac7f01edad9.png)](https://namanagr03.medium.com/?source=post_page-----4711dc6f5dba--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4711dc6f5dba--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4711dc6f5dba--------------------------------)
    [Naman Agrawal](https://namanagr03.medium.com/?source=post_page-----4711dc6f5dba--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5bbb90aa727&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimprove-your-gradient-descent-the-epic-quest-for-the-optimal-stride-4711dc6f5dba&user=Naman+Agrawal&userId=5bbb90aa727&source=post_page-5bbb90aa727----4711dc6f5dba---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4711dc6f5dba--------------------------------)
    ·14 min read·May 23, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4711dc6f5dba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimprove-your-gradient-descent-the-epic-quest-for-the-optimal-stride-4711dc6f5dba&user=Naman+Agrawal&userId=5bbb90aa727&source=-----4711dc6f5dba---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5bbb90aa727&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimprove-your-gradient-descent-the-epic-quest-for-the-optimal-stride-4711dc6f5dba&user=Naman+Agrawal&userId=5bbb90aa727&source=post_page-5bbb90aa727----4711dc6f5dba---------------------post_header-----------)
    发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----4711dc6f5dba--------------------------------)
    ·14分钟阅读·2023年5月23日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4711dc6f5dba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimprove-your-gradient-descent-the-epic-quest-for-the-optimal-stride-4711dc6f5dba&user=Naman+Agrawal&userId=5bbb90aa727&source=-----4711dc6f5dba---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4711dc6f5dba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimprove-your-gradient-descent-the-epic-quest-for-the-optimal-stride-4711dc6f5dba&source=-----4711dc6f5dba---------------------bookmark_footer-----------)![](../Images/bd19435e63da6ad9aff7a330dfde7870.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4711dc6f5dba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimprove-your-gradient-descent-the-epic-quest-for-the-optimal-stride-4711dc6f5dba&source=-----4711dc6f5dba---------------------bookmark_footer-----------)![](../Images/bd19435e63da6ad9aff7a330dfde7870.png)'
- en: Image by [stokpic](https://pixabay.com/users/stokpic-692575/?utm_source=link-attribution&amp%3Butm_medium=referral&amp%3Butm_campaign=image&amp%3Butm_content=600468)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&amp%3Butm_medium=referral&amp%3Butm_campaign=image&amp%3Butm_content=600468)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[stokpic](https://pixabay.com/users/stokpic-692575/?utm_source=link-attribution&amp%3Butm_medium=referral&amp%3Butm_campaign=image&amp%3Butm_content=600468)提供，来自[Pixabay](https://pixabay.com//?utm_source=link-attribution&amp%3Butm_medium=referral&amp%3Butm_campaign=image&amp%3Butm_content=600468)
- en: Contents
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目录
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 介绍
- en: 'Method 1: Fixed Step Size'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 方法 1：固定步长
- en: 'Method 2: Exact Line Search'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 方法 2：精确线搜索
- en: 'Method 3: Backtracking Line Search'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 方法 3：回溯线搜索
- en: Conclusion
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结论
- en: Introduction
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'When training any machine learning model, Gradient Descent is one of the most
    commonly used techniques to optimize for the parameters. Gradient descent offers
    an efficient way of minimizing the loss function for the incoming data, especially
    in cases, where they may not be a closed-form solution to the problem. In general,
    consider a machine learning problem defined by a convex and differentiable function
    f: Rᵈ → R (most loss functions follow these properties). The goal is to find x*
    ∈ Rᵈ that minimizes the loss function:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b833bde18991e44329d6ac0003f683be.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: 'Gradient Descent provides an iterative approach to solving this problem. The
    update rule is given as:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5ec9b5a054d2f5965ac5262b08c9c3de.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: 'Where x⁽ᵏ⁾ refers to the value of x in the kth iteration of the algorithm,
    and tₖ refers to the step size or the learning rate of the model in the kth iteration.
    The general workflow of the algorithm is given as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Determine the loss function f and compute its gradient ∇f.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start with a random choice of x ∈ Rᵈ, call it x⁽⁰⁾(the starting iterate).
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Until you reach the stopping criteria (e.g., the error falls below a certain
    threshold), do the following:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A) Determine the direction along which x must be reduced or increased. Under
    gradient descent, this is given by the direction opposite to the gradient of the
    loss function evaluated at the current iterate. vₖ = ∇ₓ f(x⁽ᵏ ⁻ ¹⁾)
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'B) Determine the step size or the magnitude of the change: tₖ.'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'C) Update the iterate: x⁽ᵏ⁾= x⁽ᵏ ⁻ ¹⁾ − tₖ∇ₓ f(x⁽ᵏ ⁻ ¹⁾)'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'That’s the entire workflow in a nutshell: Take the current iterate, look for
    the direction in which it needs to be updated (vₖ), determine the magnitude of
    the update (tₖ), and update it.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/07b2355658dcb4ce63f35fd42aec31b7.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: Illustration of Gradient Descent [Image by Author]
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'So, what’s this article about? In this article, our focus will be on step 3B:
    finding the optimal step size or the magnitude of tₖ. When it comes to gradient
    descent, this is one of the most neglected aspects of optimizing your model. The
    size of the step can greatly affect how fast your algorithm converges to the solution
    as well as the accuracy of the solution it converges to. most often, data scientists
    simply set a fixed value for the step size during the entire learning process
    or may occasionally use validation techniques to train it. But, there are many
    more efficient ways to go about solving this problem. In this article, we will
    discuss 3 different ways to determine the step size tₖ:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Fixed Step Size
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exact Line Search
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Backtracking Line Search (Armijo’s Rule)
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each of these methods, we will discuss the theory and implement it to calculate
    the first few iterates for an example. In particular, we will consider the following
    loss function to evaluate the model:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a4d6f7cefd8b208d5a564e89f466958f.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: 'The 3D-Plot of the function is shown below:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9c8a8138be8a65264602f8bdbfde2eda.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: Loss Function (3D Plot) [Image by Author generated using [LibreTexts](https://c3d.libretexts.org/CalcPlot3D/index.html)]
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'From the figure, it’s evident that the global minima is x* = [0; 0]. Throughout
    this article, we will manually calculate the first few iterates and computationally
    determine the number of steps for convergence for each of these methods. We will
    also trace the pattern of the descent (aka the iterate trajectory) to understand
    how each of these techniques affects the [process of convergence. Usually, it’s
    easier to refer to the contour plot of the function (instead of its 3D plot) to
    better evaluate the different trajectories that follow. The contour plot of the
    function can be easily generated via the following code:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/e50b0e25faa9beb8212094ed043b5aec.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: Contour Plot of f [Image by Author generated using Python]
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started!
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'Method 1: Fixed Step Size'
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This method is the simplest to use, and the one most commonly used for training
    the ML model. This involves setting:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a2c27db5d282e8e68d0ec89e4ea4eb66.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
- en: One needs to be very careful while choosing the right t under this method. While
    a small value of t can lead to very accurate solutions, the convergence can become
    quite slow. On the other hand, a large t makes the algorithm faster, but at the
    cost of accuracy. Using this method requires the implementer to carefully balance
    the trade-off between the rate of convergence and the accuracy of the solution
    yielded.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, most data scientists use validation techniques such as hold-out
    validation or k-fold cross-validation to optimize for t. This technique involves
    creating a partition of the training data (called the validation data), which
    is used to optimize for the performance by running the algorithm on a discrete
    set of values that t can take. Let’s look at our example:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7c9f2143f112403654b8507fec02f9a1.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: 'The first step is to compute it’s gradient:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a746fe2830d6c4404efacde46c07ab8d.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: 'For all subsequent calculations, we will take the initialization to be x⁽⁰⁾=
    [1; 1]. Under this strategy, we set:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3f4c3a94f893c7064c735c06c109e18f.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: 'The first two iterates are calculated as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c8ec49eaf61f5cf74dfe1ed286c9cbfe.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: 'We compute the remaining iterates programmatically via the following Python
    Code:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In the above code, we have defined the following convergence criteria (which
    will be consistently utilized for all methods):'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fd8d411ce856dbac80a73d18e31bea1e.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
- en: 'On running the above code, we find that it takes around 26 steps to converge.
    The following plot shows the trajectory of the iterates during the gradient descent:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/4310c3950c0829f09b148a44a2862a1b.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
- en: 'Contour Plot: Fixed Step Size = 0.1 [Image by Author generated using Python]'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'To have a better understanding of how critical it is to choose the right t
    in this method, let’s see gauge the effect of increasing or decreasing t. If we
    decrease the value of t from 0.1 to 0.01, the number of steps to converge increases
    drastically from 26 to 295\. The iterate trajectory for this case is shown below:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解在这种方法中选择正确的t有多关键，我们来衡量一下增加或减少t的效果。如果我们将t的值从0.1减少到0.01，收敛的步数会从26骤增至295。该情况的迭代轨迹如下所示：
- en: '![](../Images/9504a487fc6e5ac71a5e7bdac1473d07.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9504a487fc6e5ac71a5e7bdac1473d07.png)'
- en: 'Contour Plot: Fixed Step Size = 0.01 [Image by Author generated using Python]'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓图：固定步长 = 0.01 [作者使用Python生成的图像]
- en: 'However, on increasing the t from 0.1 to 0.2, the number of steps to converge
    decreases from 26 to a mere 11, as shown by the following trajectory:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，将t从0.1增加到0.2时，收敛的步数从26减少到仅仅11，如下所示的轨迹所示：
- en: '![](../Images/314498c4f2e42830524e51fbdd7a0a07.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/314498c4f2e42830524e51fbdd7a0a07.png)'
- en: 'Contour Plot: Fixed Step Size = 0.2 [Image by Author generated using Python]'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓图：固定步长 = 0.2 [作者使用Python生成的图像]
- en: 'However, it is important to note that this does not always be the case. If
    the value of the step size is too large, it is possible that the iterates simply
    bounce away from the optimal solution and never converge. In fact, increasing
    t from 0.2 to just around 0.3 causes the iterate values to shoot up, making it
    impossible to converge. This is seen from the following contour plot (with t =
    0.3) for just the first 8 steps:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，需要注意的是，这并不总是如此。如果步长的值过大，可能导致迭代过程偏离最优解而无法收敛。实际上，将t从0.2增加到约0.3会导致迭代值急剧上升，使得无法收敛。这可以从以下的轮廓图（t
    = 0.3）看出，仅针对前8步：
- en: '![](../Images/ddcfcec2dd06b6afb851b027dc0e092d.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ddcfcec2dd06b6afb851b027dc0e092d.png)'
- en: 'Contour Plot: Fixed Step Size = 0.3 [Image by Author generated using Python]'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓图：固定步长 = 0.3 [作者使用Python生成的图像]
- en: Thus, it is evident that finding the right value of t is extremely vital in
    this method and even a small increase or decrease can greatly affect the algorithm’s
    ability to converge. Now, let’s talk about the next method to determine t.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，显而易见的是，在此方法中找到正确的t值极其重要，甚至小幅的增加或减少都可能极大地影响算法的收敛能力。现在，让我们谈谈确定t的下一种方法。
- en: 'Method 2: Exact Line Search'
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法2：精确线搜索
- en: 'In this method, we don’t assign a simple pre-fixed value of t at every iteration.
    Instead, we treat the problem of finding the optimal t itself as a 1D optimization
    problem. In other words, we are interested in finding the best update t, that
    minimizes the value of the function:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，我们不会在每次迭代时分配一个简单的预设t值。相反，我们将找到最优t的问题本身视为一个一维优化问题。换句话说，我们关注于找到最优的更新t，以最小化函数值：
- en: '![](../Images/c6b7515d4864a734d9cbf7340e03e547.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c6b7515d4864a734d9cbf7340e03e547.png)'
- en: 'Notice how cool this is! We have a multi-dimensional optimization problem (minimizing
    f) that we attempt to solve using gradient descent. We know the best direction
    to update our iterate (vₖ = − ∇ₓ f(x⁽ᵏ ⁻ ¹⁾)), but we need to find the optimal
    step size tₖ. In other words, the value of the function for the next iterate only
    depends on the value of tₖ that we chose to use. So, we treat this as another
    (but simpler!) optimization problem:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这有多酷！我们有一个多维优化问题（最小化f），我们尝试使用梯度下降来解决。我们知道更新迭代值的最佳方向（vₖ = − ∇ₓ f(x⁽ᵏ ⁻ ¹⁾）），但我们需要找到最优步长tₖ。换句话说，下一次迭代的函数值仅依赖于我们选择使用的tₖ值。因此，我们将其视为另一个（但更简单的！）优化问题：
- en: '![](../Images/4bef8d9aaa5ea116affcf9ab732eb870.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4bef8d9aaa5ea116affcf9ab732eb870.png)'
- en: 'So, we update x⁽ᵏ⁾ to be the iterate that best minimizes the loss function
    f. This really helps increase the rate of convergence. However, it also adds an
    additional time requirement: To compute the minimizer of g(t). Usually, this isn’t
    a problem since it’s a 1D function, but sometimes it could take longer than expected.
    Thus, while using this method, it’s important to balance the trade-off between
    the number of steps reduced to converge and the additional time requirement to
    compute the argmin. Let’s look at our example:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们更新x⁽ᵏ⁾，使其成为最小化损失函数f的最佳迭代。这确实有助于提高收敛速度。然而，这也增加了额外的时间要求：计算g(t)的最小化器。通常，这不是问题，因为它是一个一维函数，但有时可能会花费比预期更长的时间。因此，在使用这种方法时，平衡减少收敛步数与计算argmin的额外时间要求之间的权衡是很重要的。让我们看看我们的例子：
- en: '![](../Images/88e51c2844dbb22ff81ac17c789e8711.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/88e51c2844dbb22ff81ac17c789e8711.png)'
- en: 'The first two iterates are calculated as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个迭代值计算如下：
- en: '![](../Images/d6f59641966f55e3207383065f4c07de.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
- en: We compute the remaining iterates programmatically via the following Python
    Code
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Just as before, in the above code, we have defined the following convergence
    criteria (which will be consistently utilized for all methods):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fd8d411ce856dbac80a73d18e31bea1e.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
- en: 'On running the above code, we find that it takes only 10 steps to converge
    ( a major improvement from the fixed step size). The following plot shows the
    trajectory of the iterates during the gradient descent:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/01a5b6d8820366f31f5fc0cab5273946.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
- en: 'Contour Plot: Exact Line Search [Image by Author generated using Python]'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s talk about the next method to determine t.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'Method 3: Backtracking Line Search'
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Backtracking is an adaptive method of choosing the optimal step size. In my
    experience, I have found this to be one of the most useful strategies for optimizing
    the step size. The convergence is usually much faster than fixed step size without
    the complications of maximizing a 1D function g(t) in an exact line search. This
    method involves starting out with a rather large step size (t¯ = 1) and continuing
    to decrease it until a required decrease in f(x) is observed. Let us first take
    a look at the algorithm and subsequently, we will be discussing the specifics:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e4ee0a61ec5b9b0c4d8d7987dc8cd43c.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
- en: 'Algorithm 1: Backtracking (Armijo–Goldstein condition) [Image by Author]'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, we start with a large step size (which is important usually
    in the initial stages of the algorithm) and check if it helps us improve the current
    iterate by a given threshold. If the step size is found to be too large, we reduce
    it by multiplying it with a scalar constant β ∈ (0, 1). We repeat this process
    until a desired decrease in f is obtained. Specifically, we choose the largest
    t such that:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/203a2bc30892cf22af6bd0f28f788373.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
- en: 'i.e., the decrease is at least σt || ∇ₓ f(x⁽ᵏ ⁻ ¹⁾) ||². But, why this value?
    It can be mathematically shown (via Taylor’s first order expansion) that t ||
    ∇ₓ f(x⁽ᵏ ⁻ ¹⁾) ||² is the minimum decrease in f that we can expect through an
    improvement made during the current iteration. There is an additional factor of
    σ in the condition. This is to account for the fact, that even if we cannot achieve
    the theoretically guaranteed decrease t || ∇ₓ f(x⁽ᵏ ⁻ ¹⁾) ||², we at least hope
    to achieve a fraction of it scaled by σ. That is to say, we require that the achieved
    reduction if f be at least a fixed fraction σ of the reduction promised by the
    first-order Taylor approximation of f at x⁽ᵏ⁾. If the condition is not fulfilled,
    we scale down t to a smaller value via β. Let’s look at our example (setting t¯=
    1, σ = β = 0.5):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/88e51c2844dbb22ff81ac17c789e8711.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
- en: 'The first two iterates are calculated as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2364678ddcf744731712fc9c5c0f9482.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
- en: Likewise,
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d4a875e91d02e832711496c5cd647c0c.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
- en: 'We compute the remaining iterates programmatically via the following Python
    Code:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Just as before, in the above code, we have defined the following convergence
    criteria (which will be consistently utilized for all methods):'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fd8d411ce856dbac80a73d18e31bea1e.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
- en: 'On running the above code, we find that it takes only 10 steps to converge.
    The following plot shows the trajectory of the iterates during the gradient descent:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/1e5cba8e49668b16b60db196d0898156.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
- en: 'Contour Plot: Backtracking [Image by Author generated using Python]'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this article, we familiarised ourselves with some of the useful techniques
    to optimize for the step size in the gradient descent algorithm. In particular,
    we covered 3 main techniques: Fixed Step Size, which involves maintaining the
    same step size or learning rate throughout the training process, Exact Line Search,
    which involves minimizing the loss as a function of t, and Armijo Backtracking
    involving a gradual reduction in the step size until a threshold is met. While
    these are some of the most fundamental techniques that you can use to tune your
    optimization, there exist a vast array of other methods (eg. setting t as a function
    of the number of iterations). These tools are generally used in more complex settings,
    such as Stochastic Gradient Descent. The purpose of this article was not only
    to introduce you to these techniques but also to make you aware of the intricacies
    that can affect your optimization algorithm. While most of these techniques are
    used in the context of Gradient descent, they can also be applied to other optimization
    algorithms (e.g., Newton-Raphson Method). Each of these techniques has its own
    merits and may be preferred over the others for specific applications and algorithms.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Hope you enjoyed reading this article! In case you have any doubts or suggestions,
    do reply in the comment box. Please feel free to contact me via [mail](mailto:naman.agr03@gmail.com).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: If you liked my article and want to read more of them, please follow me.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: All images have been made by the author.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
