- en: AI Telephone — A Battle of Multimodal Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/ai-telephone-a-battle-of-multimodal-models-282b01daf044?source=collection_archive---------5-----------------------#2023-06-15](https://towardsdatascience.com/ai-telephone-a-battle-of-multimodal-models-282b01daf044?source=collection_archive---------5-----------------------#2023-06-15)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: DALL-E2, Stable Diffusion, BLIP, and more!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@jacob_marks?source=post_page-----282b01daf044--------------------------------)[![Jacob
    Marks, Ph.D.](../Images/94d9832b8706d1044e3195386613bfab.png)](https://medium.com/@jacob_marks?source=post_page-----282b01daf044--------------------------------)[](https://towardsdatascience.com/?source=post_page-----282b01daf044--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----282b01daf044--------------------------------)
    [Jacob Marks, Ph.D.](https://medium.com/@jacob_marks?source=post_page-----282b01daf044--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff7dc0c0eae92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-telephone-a-battle-of-multimodal-models-282b01daf044&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=post_page-f7dc0c0eae92----282b01daf044---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----282b01daf044--------------------------------)
    ·14 min read·Jun 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F282b01daf044&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-telephone-a-battle-of-multimodal-models-282b01daf044&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----282b01daf044---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F282b01daf044&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-telephone-a-battle-of-multimodal-models-282b01daf044&source=-----282b01daf044---------------------bookmark_footer-----------)![](../Images/c69c273d766460fcee87fbfc328b6a01.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Artistic rendering of a game of AI Telephone. Image generated by the author
    using DALL-E2.*'
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI is on fire right now. The past few months especially have seen
    an explosion in multimodal machine learning — AI that connects concepts across
    different “modalities” such as text, images, and audio. As an example, [Midjourney](https://www.midjourney.com/)
    is a multimodal text-to-image model, because it takes in natural language, and
    outputs images. The magnum opus for this recent renaissance in multimodal synergy
    was Meta AI’s ImageBind, which can take inputs of 6(!) varieties and represent
    them in the same “space”.
  prefs: []
  type: TYPE_NORMAL
- en: 'With all of this excitement, I wanted to put multimodal models to the test
    and see how good they *actually* are. In particular, I wanted to answer three
    questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Which text-to-image model is the best?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which image-to-text model is the best?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is more important — image-to-text, or text-to-image?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Of course, each model brings its own biases to the table, from training data
    to model architecture, so there isn’t really ever one BEST model. But we can still
    put models to the test in a general context!*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To answer these questions, I decided to play a game of AI Telephone, inspired
    by the board game [Telestrations](https://en.wikipedia.org/wiki/Telestrations),
    which my family and I love to play together.
  prefs: []
  type: TYPE_NORMAL
- en: 'Telestrations is much like the [game of telephone](https://www.wikihow.com/Play-the-Telephone-Game):
    players go around in a circle, taking in communication from the person on one
    side, and in turn communicating their interpretation to the person on their other
    side. As the game ensues, the original message is invariably altered, if not lost
    entirely. Telestrations differs, however, by adding bimodal communication: players
    alternate between drawing (or *illustrating*) a description, and describing (in
    text) a description.'
  prefs: []
  type: TYPE_NORMAL
- en: Given that I was more interested in *comparing* models, I adapted the game to
    suit this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how the game of AI Telephone works:'
  prefs: []
  type: TYPE_NORMAL
- en: Each “game” will pair up an image-to-text (I2T) model with a text-to-image (T2I)
    model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Given an initial prompt, we use the T2I model to generate an image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then pass this image into the I2T model to generate a description.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We repeat steps 2 and 3 a fixed number of times `n` (in our case `n=10`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we quantify the difference between the original prompt and the final
    description.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this post, I will walk you through this entire process, so that you can play
    AI Telephone too! At the end, I’ll answer the three motivating questions.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: This game of AI Telephone is intimately connected with the notion of*
    [*cycle consistency*](https://openaccess.thecvf.com/content_ECCVW_2018/papers/11132/Cornia_Towards_Cycle-Consistent_Models_for_Text_and_Image_Retrieval_ECCVW_2018_paper.pdf)*.
    By incorporating a cycle consistency term in the loss function during training,
    models can be incentivized to, effectively, minimize degradation over a game of
    telephone. To my knowledge, none of the models considered in this experiment were
    trained with cycle consistency as a consideration.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The post is structured as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Choosing the Multimodal Models](#6fb8)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Generating the Prompts](#cc8c)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Creating Telephone Lines](#50e9)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Carrying out the Conversations](#5551)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Visualizing and Analyzing the Results](#715e)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All of the code to run this experiment and play AI Telephone can be found [here](https://github.com/voxel51/fiftyone-examples/blob/master/examples/ai_telephone.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: To run this code, you will need to install the [FiftyOne open source library](https://github.com/voxel51/fiftyone)
    for dataset curation, the [OpenAI Python Library](https://github.com/openai/openai-python),
    and the [Replicate Python client](https://replicate.com/docs/get-started/python).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Progression of images in a game of AI Telephone between DALL-E2 and BLIP.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the Competitors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The space of multimodal models is massive: at the time of writing, Hugging
    Face alone has 4,425 T2I models and 155 I2T models. Playing AI Telephone with
    all of these models — or even a non-negligible fraction of them — would be completely
    infeasible. My first task was to pare down this space of potential candidates
    to a more manageable set of competitors.'
  prefs: []
  type: TYPE_NORMAL
- en: Opting for APIs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To start this project, I knew that I would be working with many models. Some
    of the prospective models were quite large, and many required their own environments,
    with a unique set of requirements. Given that I planned to pair up each T2I model
    with each I2T model, installing these models locally to play games of AI Telephone
    presented a potential dependency purgatory — especially because I work on a MacBook
    Pro M1!
  prefs: []
  type: TYPE_NORMAL
- en: To circumvent this problem, I decided to stick to models that were accessible
    via APIs. In particular, I chose to primarily use [Replicate](https://replicate.com/),
    whose simple interface allowed me to work with T2I and I2T models in plug-and-play
    fashion. Almost every model that I used is open source, so if you are braver than
    I, you can run these models locally and avoid the charges. That being said, in
    total this experiment cost < $15 USD.
  prefs: []
  type: TYPE_NORMAL
- en: Text-to-Image Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When selecting T2I models, I chose from the models in Replicate’s [Text to image](https://replicate.com/collections/text-to-image)
    collection. My selection criteria were that the model needed to be cheap, fast,
    and relatively popular (judged by the number of “runs” of the model on Replicate).
    Additionally, the model needed to be *general purpose*, meaning that I wasn’t
    going to consider outpainting, logo generation, or anime styling models. You are
    more than welcome to try playing AI Telephone with these types of models if you’d
    like!
  prefs: []
  type: TYPE_NORMAL
- en: Given these requirements, I chose [Stable Diffusion](https://replicate.com/stability-ai/stable-diffusion)
    and [Feed forward VQGAN CLIP](https://replicate.com/mehdidc/feed_forward_vqgan_clip).
    Initially, I also worked with [DALL-E Mini](https://replicate.com/kuprel/min-dalle),
    but in early tests I was disappointed by the model’s performance, so I swapped
    the model out for OpenAI’s DALL-E2, which I accessed through OpenAI’s [image generations
    endpoint](https://platform.openai.com/docs/guides/images/usage).
  prefs: []
  type: TYPE_NORMAL
- en: As a side note, restricting my attention to API-accessible models meant that
    I did not consider [Midjourney](https://www.midjourney.com/). There is no official
    API, and I did not want to use an unofficial API, nor did I want to enter prompts
    into Discord one by one and download the generated images one at a time.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make this process as plug-and-play as possible, I took an object oriented
    approach. I defined a base `Text2Image` class, which exposes a method `generate_image(text)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'For Replicate models, all that is needed is then setting the `model_name` attribute,
    identifying the model on Replicate. For Stable Diffusion, for instance, the class
    definition looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'For other models, such as DALL-E2, the `generate_image(text)` method can be
    overloaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Each of these T2I models returns the URL of the generated image, which we can
    then pass directly to our I2T models.
  prefs: []
  type: TYPE_NORMAL
- en: Image-to-Text Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I followed a similar process to determine the I2T competitors, evaluating candidates
    in Replicate’s [Image to text](https://replicate.com/collections/image-to-text)
    collection. After looking at the examples for all of the models in the collection,
    six models stood out: [BLIP](https://replicate.com/salesforce/blip), [BLIP-2](https://replicate.com/andreasjansson/blip-2),
    [CLIP prefix captioning](https://replicate.com/rmokady/clip_prefix_caption), [Fine-grained
    Image Captioning with CLIP Reward](https://replicate.com/j-min/clip-caption-reward),
    [mPLUG-Owl](https://replicate.com/joehoover/mplug-owl), and [MiniGPT-4](https://replicate.com/daanelson/minigpt-4).
    Other models were enticing, such as [CLIP Interrogator](https://replicate.com/pharmapsychotic/clip-interrogator),
    which tries to reverse engineer a prompt you can then use to generate a similar
    image. But this felt a bit like cheating as far as AI Telephone was concerned!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Playing around with the six I2T candidates, I was able to quickly eliminate
    two models from contention: BLIP-2 generated responses that were consistently
    too short to be useful, and the CLIP Caption Reward model generated responses
    which were often incoherent.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In direct analogy with the T2I models, I defined a base class `Image2Text`
    class exposing a `generate_text(image_url)` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'I then created subclasses for each model. Here is what the BLIP subclass looks
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: All of the models are instantiated with the same task description — to “write
    a detailed description of this image”.
  prefs: []
  type: TYPE_NORMAL
- en: Progression of images in a game of AI Telephone between DALL-E2 and mPLUG-Owl.
  prefs: []
  type: TYPE_NORMAL
- en: Prompts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To be as “scientific” as possible, I thought it best to not generate the initial
    prompts myself. Instead, (and just for fun) I outsourced the task to ChatGPT.
    I asked:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: I’m playing a game of telephone using text-to-image and image-to-text AI models.
    I want to evaluate these models based on their ability to retain complex semantic
    information over the course of long conversations. Your job is to give me 10 text
    prompts that I can use to run these games of telephone. You must give me one 3
    easy, 3 medium, 3 hard, and 1 ultra-hard (“impossible”) prompt
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of the prompts ChatGPT generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '*A more rigorous scientific approach would be far more intentional with the
    prompts used, as well as their categorization.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'I then took the text prompts generated by ChatGPT and constructed `Prompt`
    objects, which contained the text for the prompt, and the “level” of difficulty
    assigned by ChatGPT:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Progression of images in a game of AI Telephone between VQGAN-CLIP and MiniGPT-4.
  prefs: []
  type: TYPE_NORMAL
- en: The Telephone Line
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last component to playing AI Telephone was the “telephone line” itself.
    I created a `TelephoneLine` class to encapsulate the connection between a T2I
    model and an I2T model. Given a single telephone line, a “game” of telephone is
    played by calling the `play(prompt, nturns=10)`, where the conversation evolves
    from `prompt`, and runs for `nturns` back-and-forth turns.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: For each game played, the conversation is logged with a unique name, generated
    by hashing the T2I model name, I2T model name, and the prompt text (`get_conversation_name()`
    method).
  prefs: []
  type: TYPE_NORMAL
- en: 'I also equipped the class with a `save_conversations_to_dataset()` method,
    which saves the images and descriptions from all games played on the telephone
    line to a FiftyOne `Dataset`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Progression of images in a game of AI Telephone between Stable Diffusion and
    CLIP Prefix Captioning.
  prefs: []
  type: TYPE_NORMAL
- en: Carrying out the Conversations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With all of the building blocks in place, playing AI Telephone is child’s play!
  prefs: []
  type: TYPE_NORMAL
- en: 'We can instantiate T2I and I2T models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'And then create a telephone line for each pair:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We then load in our prompts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'And create a FiftyOne `Dataset` which we will use to store the generated images
    and all relevant information from the conversations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then run all 120 games of telephone:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In the FiftyOne App, click on the splitting icon in the menu bar to group images
    by conversation, select `conversation_name` from the dropdown, then toggle the
    selector to `ordered` and select `step_number`.
  prefs: []
  type: TYPE_NORMAL
- en: Results and Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To assess the quality of a conversation — purely in terms of how closely the
    meaning of the final description approximated the meaning of the initial prompt,
    I decided to generate embeddings for the prompts and descriptions, and compute
    the [cosine distance](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html)
    (in `[0, 2]`) between the two.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'For an embedding model, I wanted a model that could embed both text and images,
    given the multimodal nature of the exercise. I ended up choosing to use ImageBind
    for three reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Other popular joint image-text embedding models like CLIP and BLIP are related
    to some of the models I used in the experiment (BLIP and CLIP prefix captioning),
    and I wanted to avoid any possible biases from using the same types of models
    for evaluation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Many text embedding models have a small `max_token_count` — the maximum number
    of tokens allowed in a text to be embedded. CLIP, for instance, has `max_token_count=77`.
    Some of our descriptions are significantly longer than this. Fortunately, ImageBind
    has a much longer maximum token count.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I’d been meaning to try ImageBind, and this was a great opportunity!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'I wrapped Replicate’s ImageBind API in a function `embed_text(text)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'To avoid redundant computations, I hashed the prompts and stored the prompt
    embeddings in a dictionary. This way, instead of embedding each prompt for each
    of the 12 telephone lines, we only need to embed each once:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then group samples by conversation name, iterate through these groups,
    compute the text embedding for each step, and record the cosine distance (smaller
    is better!) between the text embedding and the initial prompt embedding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: I then computed the average scores for each T2I-I2T pair across all prompts
    at a certain level of difficulty and plotted the results. In each of the videos,
    the I2T and T2I models are printed on the generated images, as well as the text
    used to generate that image (red), and the description generated from that image
    (green).
  prefs: []
  type: TYPE_NORMAL
- en: Easy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0f19f4db954511a26ab1c95cb13526c9.png)'
  prefs: []
  type: TYPE_IMG
- en: For easy prompts, performance tends to depend most strongly on the text-to-image
    model. DALL-E2 and Stable Diffusion dramatically outperform VQGAN-CLIP. MiniGPT-4
    is a member of both of the top-performing pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples for the easy prompt introduced above:'
  prefs: []
  type: TYPE_NORMAL
- en: AI Telephone for an easy prompt, with pairs of text-to-image and image-to-text
    models.
  prefs: []
  type: TYPE_NORMAL
- en: In the games with MiniGPT-4 (and to a slightly lesser extent BLIP), the apple
    remains front and center, whereas for games involving CLIP Prefix, the apple gets
    phased out over time.
  prefs: []
  type: TYPE_NORMAL
- en: Medium
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/cc664b641e6ec5bbbc202e6fa1dbd1e7.png)'
  prefs: []
  type: TYPE_IMG
- en: When the prompts become a bit more difficult, the situation starts to change.
  prefs: []
  type: TYPE_NORMAL
- en: AI Telephone for a medium difficulty prompt, with pairs of text-to-image and
    image-to-text models.
  prefs: []
  type: TYPE_NORMAL
- en: For nearly all of the games, the subject changes somewhere around the fourth
    or fifth step. Early on, MiniGPT-4 holds an advantage. But by the end of the game,
    that advantage seems to have been entirely lost.
  prefs: []
  type: TYPE_NORMAL
- en: Hard
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/9e02437c072b5f6388b7e36483df716b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By the time the prompts become challenging, we start to see something interesting:
    for early steps, the image-to-text model is most important (MiniGPT-4 is best,
    and CLIP Prefix is for the most part the worst). By later stages, however, the
    text-to-image model becomes most important. And to complicate the situation further,
    VQGAN-CLIP is best here!'
  prefs: []
  type: TYPE_NORMAL
- en: One might worry that “better” could just mean that consistency is maintained,
    without accurately representing the original concept. However, when we look at
    examples, we can see that this is not the case.
  prefs: []
  type: TYPE_NORMAL
- en: AI Telephone for a hard prompt, with pairs of text-to-image and image-to-text
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Take the example highlighted in the video, where the initial prompt is the “hard”
    prompt introduced above concerning a “bustling marketplace”. While the images
    generated by VQGAN-CLIP are without a doubt grainy, the subject can still be made
    out, and matches the original prompt fairly closely.
  prefs: []
  type: TYPE_NORMAL
- en: Impossible
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/00ae40dcc7622c335c9b871b46224e71.png)'
  prefs: []
  type: TYPE_IMG
- en: Unsurprisingly, none of our competitors do terribly well here. One might argue
    that VQGAN-CLIP is the winner. But for the most part, this is all just noise.
    In the video, even for games involving VQGAN-CLIP, the subject is effectively
    unrecognizable.
  prefs: []
  type: TYPE_NORMAL
- en: AI Telephone for an “impossible” prompt, with pairs of text-to-image and image-to-text
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Takeaways
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This exploration was far from scientific; I only looked at ten prompts, without
    true validation of their difficulty level. I only ran the conversations out to
    ten back-and-forth steps; and I only evaluated performance on one metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is clear that which T2I and I2T models fare best depends in large part on
    the complexity of the prompt, and how long you want to keep the models talking.
    Nevertheless, it is worth noting a few key observations:'
  prefs: []
  type: TYPE_NORMAL
- en: VQGAN-CLIP may fare better for more challenging prompts, but this doesn’t mean
    it is a *better* T2I model. The images produced by VQGAN-CLIP are often far less
    coherent and globally consistent than those produced by Stable Diffusion or DALL-E2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The analysis above is all about semantic similarity — it does not take *style*
    into account. The style of these images can change a ton over the course of a
    game of AI Telephone. Anecdotally, I found that the style is much more consistent
    for I2T models like mPLUG-Owl, which give long descriptions, than for models like
    BLIP, whose descriptions are more subject focused.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By around five or six iterations, the games had mostly converged to stable equilibria.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Even though the embedding model, ImageBind, was multimodal, the distance between
    consecutive image embeddings and text embeddings were far greater than the distance
    between consecutive images or consecutive descriptions. In general, they followed
    the same trends, but in less pronounced fashion, which is why I didn’t include
    these in the plots.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I hope this inspires you to run your own experiments with generative AI — whether
    you’re playing AI Telephone, or doing something else entirely!
  prefs: []
  type: TYPE_NORMAL
- en: If you try out a variation of this and get interesting results, comment on this
    post!
  prefs: []
  type: TYPE_NORMAL
