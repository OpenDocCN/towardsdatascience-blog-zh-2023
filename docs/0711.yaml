- en: 'CLIP: Creating Image Classifiers Without Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/clip-creating-image-classifiers-without-data-b21c72b741fa?source=collection_archive---------1-----------------------#2023-02-22](https://towardsdatascience.com/clip-creating-image-classifiers-without-data-b21c72b741fa?source=collection_archive---------1-----------------------#2023-02-22)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A hands-on tutorial explaining how to generate a custom Zero-Shot image classifier
    without training, using a pre-trained CLIP model. Full code included.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@lihigurarie?source=post_page-----b21c72b741fa--------------------------------)[![Lihi
    Gur Arie, PhD](../Images/7a1eb30725a95159401c3672fa5f43ab.png)](https://medium.com/@lihigurarie?source=post_page-----b21c72b741fa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b21c72b741fa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b21c72b741fa--------------------------------)
    [Lihi Gur Arie, PhD](https://medium.com/@lihigurarie?source=post_page-----b21c72b741fa--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F418175cbf131&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclip-creating-image-classifiers-without-data-b21c72b741fa&user=Lihi+Gur+Arie%2C+PhD&userId=418175cbf131&source=post_page-418175cbf131----b21c72b741fa---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b21c72b741fa--------------------------------)
    ·7 min read·Feb 22, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb21c72b741fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclip-creating-image-classifiers-without-data-b21c72b741fa&user=Lihi+Gur+Arie%2C+PhD&userId=418175cbf131&source=-----b21c72b741fa---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb21c72b741fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclip-creating-image-classifiers-without-data-b21c72b741fa&source=-----b21c72b741fa---------------------bookmark_footer-----------)![](../Images/0f4fc943b125ed58be2a7564ff5da20c.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image generated by the author with Midjourney
  prefs: []
  type: TYPE_NORMAL
- en: '**Introduction**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine you need to classify whether people wear glasses, but you have no data
    or resources to train a custom model. In this tutorial, you will learn how to
    use a pre-trained CLIP model to create a custom classifier without any training
    required. This approach is known as **Zero-Shot** image classification, and it
    enables classifying images of classes that were not explicitly seen during the
    training of the original CLIP model. An easy-to-use Jupyter notebook with the
    full code is provided below for your convenience.
  prefs: []
  type: TYPE_NORMAL
- en: 'CLIP: Theoretical Background'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The CLIP (Contrastive Language-Image Pre-training) model, developed by OpenAI,
    is a multi-modal vision and language model. It maps images and text descriptions
    to the same latent space, allowing it to determine whether an image and description
    match. CLIP was trained in a **contrastive** way to predict which captions correspond
    to which images in a dataset of over 400 million image-text pairs from the internet
    [1]. Incredibly, classifiers generated by the pre-trained CLIP were shown to achieve
    competitive results with supervised models baseline, and in this tutorial we will
    utilize this pre-trained model to generate a glasses detector.
  prefs: []
  type: TYPE_NORMAL
- en: '***CLIP contrastive training***'
  prefs: []
  type: TYPE_NORMAL
- en: CLIP model consists of an Image Encoder and a Text Encoder (Figure 1). During
    training, a batch of images is processed through the Image Encoder (ResNet variant
    or ViT) to obtain an image representation tensor (embeddings). In parallel, their
    corresponding descriptions are processed through the Text Encoder (Transformer),
    to obtain text embeddings. The CLIP model was trained to predict which image embedding
    belongs to which text embedding in a batch. This is achieved by jointly training
    the Image Encoder and Text Encoder to maximize the cosine similarity [2] between
    the image and text embeddings of real pairs in the batch (Figure 1, blue squares
    on the diagonal axis) while minimizing the cosine similarity between the embeddings
    of incorrect pairings (Figure 1, white squares). The optimization is performed
    using a symmetric cross-entropy loss over these similarity scores.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/73a32d9c8f11303d6607eedf21b25b46.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1 — illustration of the CLIP training process in a mini-batch. T1 is
    the embedding vector of class1, I1 is the embedding vector of image1, etc. | Image
    is taken from Radford et al., 2021 [1]
  prefs: []
  type: TYPE_NORMAL
- en: '***Creating a Custom Classifier***'
  prefs: []
  type: TYPE_NORMAL
- en: To create a custom classifier using CLIP, the names of the classes are transformed
    into a text embedding vector by the pre-trained Text Encoder, while the image
    is embedded using the pre-trained Image Encoder (Figure 2). The cosine similarity
    between the image embedding and each of the text embeddings is then computed,
    and the image is assigned to the class with the highest cosine similarity score.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aff008258be6fe18bd9efae5cda75b55.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2 — Zero-shot classification with CLIP | Image from Radford et al., 2021
    [1], edited by the author. The face image is taken from the ‘Glasses or No Glasses’
    dataset on Kaggle [3].
  prefs: []
  type: TYPE_NORMAL
- en: '**Code Implementation**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '***Dataset***'
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, we will create an image classifier that detects whether people
    wear eyeglasses, and use the ‘Glasses or No Glasses’ dataset from Kaggle [3] to
    evaluate the performance of our classifier. Although the dataset contains 5000
    images, we will only utilize the first 100 to expedite the demonstaration. The
    dataset consists of a folder with all the images, and a CSV file with the labels.
    To facilitate the loading of images paths and labels, we will customize the Pytorch
    `Dataset` class to create the `CustomDataset()` class. You can find the code for
    this in the provided notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bb29b5a302ddcb5f1827689d1f02a49f.png)'
  prefs: []
  type: TYPE_IMG
- en: Random images from ‘Glasses or No Glasses’ dataset on Kaggle [3]
  prefs: []
  type: TYPE_NORMAL
- en: '***Loading CLIP model***'
  prefs: []
  type: TYPE_NORMAL
- en: After installing and importing CLIP and related libraries, we load the model
    and the torchvision transformation pipeline that are required by the specified
    model. The text encoder is a Transformer, and the image encoder can be either
    a Vision Transformer (ViT) or a ResNet variant such as ResNet50\. To see the available
    image encoders, you can use the command `clip.available_models()`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '***Extracting text embeddings***'
  prefs: []
  type: TYPE_NORMAL
- en: The text labels are first processed by a text tokenizer (`clip.tokenize()`),
    which converts the label words into numerical values. This produces a padded tensor
    of size N x 77 (N is the number of classes, 2 x 77 in binary classification),
    which serves as input to the Text Encoder. Text Encoder then transforms the tensor
    to an N x 512 tensor of text embeddings, where each class is represented by a
    single vector. To encode the text and retrieve embedding, you can use the `model.encode_text()`method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '***Extracting image embeddings***'
  prefs: []
  type: TYPE_NORMAL
- en: Before being fed into the Image Encoder, each image undergoes preprocessing,
    including center-cropping, normalization, and resizing, to meet the requirements
    of the image encoder. Once preprocessed, the image is passed to the Image Encoder,
    which generates a 1 x 512 image embedding tensor as output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '***Similarity results***'
  prefs: []
  type: TYPE_NORMAL
- en: To measure the similarity between the image encoding and each text label encoding,
    we’ll use the cosine similarity distance metric. The `model()` takes the preprocessed
    image and text inputs, passes them through the image and text encoders, and computes
    the cosine similarities between the corresponding image and text features, multiplied
    by 100 (`image_logits`). Softmax is then used to normalize the logits into a list
    of probability distributions for each class. Since we are not training the model,
    we will disable the gradient calculations using `torch.no_grad()`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The class with the highest probability is set as the predicted class, and its
    index, probability, and corresponding token are extracted.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '***Wrapping the code***'
  prefs: []
  type: TYPE_NORMAL
- en: We can create a Python class called CustomClassifier to wrap this code. Upon
    initialization, the pre-trained CLIP model is loaded, and the embedded text representation
    vector is produced for each label. We’ll define a `classify()` method that takes
    an image path as input and returns the predicted label with its probability score
    (stored in a DataFrame called`df_results`). To evaluate the model’s performance,
    we’ll define a `validate()` method that uses a PyTorch Dataset instance (`CustomDataset()`)
    to retrieve images and labels, then predicts results by calling the `classify()`
    method and evaluates the model’s performance. This method returns a DataFrame
    with the predicted labels and probability scores for all the images. The`max_images`
    argument is used to restrict the number of images to 100.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'A single image can be classified with the`classify()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The classifier’s performance can be evaluated by the `validate()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Notably, using the original [‘no glasses’, ‘glasses’] classes labels, we achieved
    a decent accuracy of 0.82 without training any model, and we can improve our results
    even further through prompt engineering.
  prefs: []
  type: TYPE_NORMAL
- en: '***Prompt Engineering***'
  prefs: []
  type: TYPE_NORMAL
- en: The CLIP classifier encodes text labels, known as prompts, into a learned latent
    space, and compares their similarity to the image latent space. Modifying the
    wording of the prompts can result in a different text embedding, which can impact
    the performance of the classifier. To improve prediction accuracy, we’ll explore
    multiple prompts through trial and error, selecting the one that yields the best
    results. For example, using the prompts ‘photo of a man with no glasses’ and ‘photo
    of a man with glasses’ resulted in an accuracy of 0.94.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Analyzing multiple prompts produced the following outcomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[ ‘no glasses’, ‘glasses’,] — 0.82 accuracy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[‘face without glasses’, ‘face with glasses’] — 0.89 accuracy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[‘photo of a man with no glasses’, ‘photo of a man with glasses’] — 0.94 accuracy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we can see, adjusting the wording can significantly enhance performance.
    By analyzing multiple prompts, we improved accuracy performances from the 0.82
    baseline to 0.94\. However, it’s important to avoid overfitting the prompts to
    the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**Concluding Remarks**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The CLIP model is an incredibly powerful tool for developing zero-shot classifiers
    across a wide variety of tasks. With CLIP, I was able to effortlessly generate
    on-the-fly classifiers with highly satisfactory accuracy on my projects. However,
    CLIP might struggle with tasks like fine-grained classification, abstract or systematic
    tasks such as counting objects, and predicting truly out-of-distribution images
    that were not covered in its pre-training dataset. Therefore, its performance
    on a new assignment should be evaluated beforehand.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Jupyter notebook provided below, you can easily create your own custom
    classifier. Just follow the instructions, add your data, and you’ll have a personalized
    classifier up and running in no time.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Want to learn more?**'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Explore**](https://medium.com/@lihigurarie) additional articles I’ve written'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Subscribe**](https://medium.com/@lihigurarie/subscribe)to get notified when
    I publish articles'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Follow me on [**Linkedin**](https://www.linkedin.com/in/lihi-gur-arie/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Full Jupyter Notebook Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The full code for the tutorial is provided on the first reference [0].
  prefs: []
  type: TYPE_NORMAL
- en: '***References***'
  prefs: []
  type: TYPE_NORMAL
- en: '[0] Code: [https://gist.github.com/Lihi-Gur-Arie/844a4c3e98a7561d4e0ddb95879f8c11](https://gist.github.com/Lihi-Gur-Arie/9356a2018c3420a01e3033875405f605)'
  prefs: []
  type: TYPE_NORMAL
- en: '[1] CLIP article: [https://arxiv.org/pdf/2103.00020v1.pdf](https://arxiv.org/pdf/2103.00020v1.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Cosine similarity review: [https://towardsdatascience.com/understanding-cosine-similarity-and-its-application-fd42f585296a](/understanding-cosine-similarity-and-its-application-fd42f585296a)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] ‘Glasses or No Glasses’ dataset from Kaggle, license [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/):
    [https://www.kaggle.com/datasets/jeffheaton/glasses-or-no-glasses](https://www.kaggle.com/datasets/jeffheaton/glasses-or-no-glasses)'
  prefs: []
  type: TYPE_NORMAL
