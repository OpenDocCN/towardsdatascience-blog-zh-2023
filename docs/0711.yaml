- en: 'CLIP: Creating Image Classifiers Without Data'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CLIP：无需数据即可创建图像分类器
- en: 原文：[https://towardsdatascience.com/clip-creating-image-classifiers-without-data-b21c72b741fa?source=collection_archive---------1-----------------------#2023-02-22](https://towardsdatascience.com/clip-creating-image-classifiers-without-data-b21c72b741fa?source=collection_archive---------1-----------------------#2023-02-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/clip-creating-image-classifiers-without-data-b21c72b741fa?source=collection_archive---------1-----------------------#2023-02-22](https://towardsdatascience.com/clip-creating-image-classifiers-without-data-b21c72b741fa?source=collection_archive---------1-----------------------#2023-02-22)
- en: A hands-on tutorial explaining how to generate a custom Zero-Shot image classifier
    without training, using a pre-trained CLIP model. Full code included.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这是一个实践教程，解释如何使用预训练的 CLIP 模型生成自定义的 Zero-Shot 图像分类器，而无需进行训练。完整代码包含在内。
- en: '[](https://medium.com/@lihigurarie?source=post_page-----b21c72b741fa--------------------------------)[![Lihi
    Gur Arie, PhD](../Images/7a1eb30725a95159401c3672fa5f43ab.png)](https://medium.com/@lihigurarie?source=post_page-----b21c72b741fa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b21c72b741fa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b21c72b741fa--------------------------------)
    [Lihi Gur Arie, PhD](https://medium.com/@lihigurarie?source=post_page-----b21c72b741fa--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@lihigurarie?source=post_page-----b21c72b741fa--------------------------------)[![Lihi
    Gur Arie, 博士](../Images/7a1eb30725a95159401c3672fa5f43ab.png)](https://medium.com/@lihigurarie?source=post_page-----b21c72b741fa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b21c72b741fa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b21c72b741fa--------------------------------)
    [Lihi Gur Arie, 博士](https://medium.com/@lihigurarie?source=post_page-----b21c72b741fa--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F418175cbf131&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclip-creating-image-classifiers-without-data-b21c72b741fa&user=Lihi+Gur+Arie%2C+PhD&userId=418175cbf131&source=post_page-418175cbf131----b21c72b741fa---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b21c72b741fa--------------------------------)
    ·7 min read·Feb 22, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb21c72b741fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclip-creating-image-classifiers-without-data-b21c72b741fa&user=Lihi+Gur+Arie%2C+PhD&userId=418175cbf131&source=-----b21c72b741fa---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F418175cbf131&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclip-creating-image-classifiers-without-data-b21c72b741fa&user=Lihi+Gur+Arie%2C+PhD&userId=418175cbf131&source=post_page-418175cbf131----b21c72b741fa---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b21c72b741fa--------------------------------)
    ·7 分钟阅读·2023年2月22日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb21c72b741fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclip-creating-image-classifiers-without-data-b21c72b741fa&user=Lihi+Gur+Arie%2C+PhD&userId=418175cbf131&source=-----b21c72b741fa---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb21c72b741fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclip-creating-image-classifiers-without-data-b21c72b741fa&source=-----b21c72b741fa---------------------bookmark_footer-----------)![](../Images/0f4fc943b125ed58be2a7564ff5da20c.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb21c72b741fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclip-creating-image-classifiers-without-data-b21c72b741fa&source=-----b21c72b741fa---------------------bookmark_footer-----------)![](../Images/0f4fc943b125ed58be2a7564ff5da20c.png)'
- en: Image generated by the author with Midjourney
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由作者使用 Midjourney 生成
- en: '**Introduction**'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**介绍**'
- en: Imagine you need to classify whether people wear glasses, but you have no data
    or resources to train a custom model. In this tutorial, you will learn how to
    use a pre-trained CLIP model to create a custom classifier without any training
    required. This approach is known as **Zero-Shot** image classification, and it
    enables classifying images of classes that were not explicitly seen during the
    training of the original CLIP model. An easy-to-use Jupyter notebook with the
    full code is provided below for your convenience.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'CLIP: Theoretical Background'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The CLIP (Contrastive Language-Image Pre-training) model, developed by OpenAI,
    is a multi-modal vision and language model. It maps images and text descriptions
    to the same latent space, allowing it to determine whether an image and description
    match. CLIP was trained in a **contrastive** way to predict which captions correspond
    to which images in a dataset of over 400 million image-text pairs from the internet
    [1]. Incredibly, classifiers generated by the pre-trained CLIP were shown to achieve
    competitive results with supervised models baseline, and in this tutorial we will
    utilize this pre-trained model to generate a glasses detector.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '***CLIP contrastive training***'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: CLIP model consists of an Image Encoder and a Text Encoder (Figure 1). During
    training, a batch of images is processed through the Image Encoder (ResNet variant
    or ViT) to obtain an image representation tensor (embeddings). In parallel, their
    corresponding descriptions are processed through the Text Encoder (Transformer),
    to obtain text embeddings. The CLIP model was trained to predict which image embedding
    belongs to which text embedding in a batch. This is achieved by jointly training
    the Image Encoder and Text Encoder to maximize the cosine similarity [2] between
    the image and text embeddings of real pairs in the batch (Figure 1, blue squares
    on the diagonal axis) while minimizing the cosine similarity between the embeddings
    of incorrect pairings (Figure 1, white squares). The optimization is performed
    using a symmetric cross-entropy loss over these similarity scores.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/73a32d9c8f11303d6607eedf21b25b46.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
- en: Figure 1 — illustration of the CLIP training process in a mini-batch. T1 is
    the embedding vector of class1, I1 is the embedding vector of image1, etc. | Image
    is taken from Radford et al., 2021 [1]
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '***Creating a Custom Classifier***'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: To create a custom classifier using CLIP, the names of the classes are transformed
    into a text embedding vector by the pre-trained Text Encoder, while the image
    is embedded using the pre-trained Image Encoder (Figure 2). The cosine similarity
    between the image embedding and each of the text embeddings is then computed,
    and the image is assigned to the class with the highest cosine similarity score.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aff008258be6fe18bd9efae5cda75b55.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: Figure 2 — Zero-shot classification with CLIP | Image from Radford et al., 2021
    [1], edited by the author. The face image is taken from the ‘Glasses or No Glasses’
    dataset on Kaggle [3].
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2 — 使用 CLIP 的零样本分类 | 图像来自 Radford 等人，2021 [1]，由作者编辑。人脸图像取自 Kaggle 上的‘有眼镜还是没有眼镜’数据集
    [3]。
- en: '**Code Implementation**'
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**代码实现**'
- en: '***Dataset***'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '***数据集***'
- en: In this tutorial, we will create an image classifier that detects whether people
    wear eyeglasses, and use the ‘Glasses or No Glasses’ dataset from Kaggle [3] to
    evaluate the performance of our classifier. Although the dataset contains 5000
    images, we will only utilize the first 100 to expedite the demonstaration. The
    dataset consists of a folder with all the images, and a CSV file with the labels.
    To facilitate the loading of images paths and labels, we will customize the Pytorch
    `Dataset` class to create the `CustomDataset()` class. You can find the code for
    this in the provided notebook.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将创建一个图像分类器，用于检测人们是否戴眼镜，并使用 Kaggle 上的‘有眼镜还是没有眼镜’数据集 [3] 来评估我们分类器的性能。尽管数据集包含
    5000 张图像，但我们仅使用前 100 张以加快演示。数据集包含一个包含所有图像的文件夹和一个包含标签的 CSV 文件。为了方便加载图像路径和标签，我们将自定义
    Pytorch `Dataset` 类来创建 `CustomDataset()` 类。您可以在提供的 notebook 中找到相应的代码。
- en: '![](../Images/bb29b5a302ddcb5f1827689d1f02a49f.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bb29b5a302ddcb5f1827689d1f02a49f.png)'
- en: Random images from ‘Glasses or No Glasses’ dataset on Kaggle [3]
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 Kaggle 上‘有眼镜还是没有眼镜’数据集的随机图像 [3]
- en: '***Loading CLIP model***'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '***加载 CLIP 模型***'
- en: After installing and importing CLIP and related libraries, we load the model
    and the torchvision transformation pipeline that are required by the specified
    model. The text encoder is a Transformer, and the image encoder can be either
    a Vision Transformer (ViT) or a ResNet variant such as ResNet50\. To see the available
    image encoders, you can use the command `clip.available_models()`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装和导入 CLIP 及相关库之后，我们加载模型和指定模型所需的 torchvision 转换管道。文本编码器是 Transformer，而图像编码器可以是
    Vision Transformer (ViT) 或 ResNet 变体，如 ResNet50。要查看可用的图像编码器，可以使用命令 `clip.available_models()`。
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '***Extracting text embeddings***'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '***提取文本嵌入***'
- en: The text labels are first processed by a text tokenizer (`clip.tokenize()`),
    which converts the label words into numerical values. This produces a padded tensor
    of size N x 77 (N is the number of classes, 2 x 77 in binary classification),
    which serves as input to the Text Encoder. Text Encoder then transforms the tensor
    to an N x 512 tensor of text embeddings, where each class is represented by a
    single vector. To encode the text and retrieve embedding, you can use the `model.encode_text()`method.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 文本标签首先由文本分词器 (`clip.tokenize()`) 处理，将标签词转换为数值。这生成一个大小为 N x 77 的填充张量（N 是类别数，二分类中为
    2 x 77），作为文本编码器的输入。文本编码器将张量转换为 N x 512 的文本嵌入张量，其中每个类别由一个向量表示。要编码文本并检索嵌入，您可以使用
    `model.encode_text()` 方法。
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '***Extracting image embeddings***'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '***提取图像嵌入***'
- en: Before being fed into the Image Encoder, each image undergoes preprocessing,
    including center-cropping, normalization, and resizing, to meet the requirements
    of the image encoder. Once preprocessed, the image is passed to the Image Encoder,
    which generates a 1 x 512 image embedding tensor as output.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入到图像编码器之前，每张图像会经过预处理，包括中心裁剪、归一化和调整大小，以满足图像编码器的要求。一旦预处理完成，图像将传递给图像编码器，生成 1
    x 512 的图像嵌入张量作为输出。
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '***Similarity results***'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '***相似性结果***'
- en: To measure the similarity between the image encoding and each text label encoding,
    we’ll use the cosine similarity distance metric. The `model()` takes the preprocessed
    image and text inputs, passes them through the image and text encoders, and computes
    the cosine similarities between the corresponding image and text features, multiplied
    by 100 (`image_logits`). Softmax is then used to normalize the logits into a list
    of probability distributions for each class. Since we are not training the model,
    we will disable the gradient calculations using `torch.no_grad()`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了衡量图像编码与每个文本标签编码之间的相似性，我们将使用余弦相似度距离度量。`model()`接收预处理后的图像和文本输入，将它们通过图像和文本编码器，并计算对应图像和文本特征之间的余弦相似度，乘以
    100（`image_logits`）。然后使用 Softmax 将 logits 归一化为每个类别的概率分布列表。由于我们不训练模型，我们将使用`torch.no_grad()`禁用梯度计算。
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The class with the highest probability is set as the predicted class, and its
    index, probability, and corresponding token are extracted.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 具有最高概率的类别被设置为预测类别，并提取其索引、概率和对应的标记。
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '***Wrapping the code***'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '***封装代码***'
- en: We can create a Python class called CustomClassifier to wrap this code. Upon
    initialization, the pre-trained CLIP model is loaded, and the embedded text representation
    vector is produced for each label. We’ll define a `classify()` method that takes
    an image path as input and returns the predicted label with its probability score
    (stored in a DataFrame called`df_results`). To evaluate the model’s performance,
    we’ll define a `validate()` method that uses a PyTorch Dataset instance (`CustomDataset()`)
    to retrieve images and labels, then predicts results by calling the `classify()`
    method and evaluates the model’s performance. This method returns a DataFrame
    with the predicted labels and probability scores for all the images. The`max_images`
    argument is used to restrict the number of images to 100.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建一个名为 `CustomClassifier` 的 Python 类来封装这些代码。初始化时，预训练的 CLIP 模型会被加载，并为每个标签生成嵌入的文本表示向量。我们将定义一个
    `classify()` 方法，该方法接受一个图像路径作为输入，并返回预测的标签及其概率分数（存储在名为`df_results`的 DataFrame 中）。为了评估模型的性能，我们将定义一个
    `validate()` 方法，该方法使用 PyTorch 数据集实例（`CustomDataset()`）来检索图像和标签，然后通过调用 `classify()`
    方法来预测结果，并评估模型的性能。该方法返回一个包含所有图像预测标签和概率分数的 DataFrame。`max_images` 参数用于限制图像数量为 100。
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'A single image can be classified with the`classify()` method:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 `classify()` 方法对单张图像进行分类：
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The classifier’s performance can be evaluated by the `validate()` method:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器的性能可以通过 `validate()` 方法进行评估：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Notably, using the original [‘no glasses’, ‘glasses’] classes labels, we achieved
    a decent accuracy of 0.82 without training any model, and we can improve our results
    even further through prompt engineering.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，使用原始的 [‘无眼镜’，‘眼镜’] 类别标签，我们在没有训练任何模型的情况下取得了 0.82 的不错准确率，并且我们可以通过提示工程进一步提高结果。
- en: '***Prompt Engineering***'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '***提示工程***'
- en: The CLIP classifier encodes text labels, known as prompts, into a learned latent
    space, and compares their similarity to the image latent space. Modifying the
    wording of the prompts can result in a different text embedding, which can impact
    the performance of the classifier. To improve prediction accuracy, we’ll explore
    multiple prompts through trial and error, selecting the one that yields the best
    results. For example, using the prompts ‘photo of a man with no glasses’ and ‘photo
    of a man with glasses’ resulted in an accuracy of 0.94.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP 分类器将文本标签（称为提示）编码到一个学习到的潜在空间中，并将其与图像潜在空间进行比较。修改提示的措辞可能会导致不同的文本嵌入，这会影响分类器的性能。为了提高预测准确率，我们将通过反复试验来探索多个提示，选择结果最好的那个。例如，使用提示‘没有眼镜的男人的照片’和‘戴眼镜的男人的照片’的准确率为
    0.94。
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Analyzing multiple prompts produced the following outcomes:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 分析多个提示产生了以下结果：
- en: '[ ‘no glasses’, ‘glasses’,] — 0.82 accuracy'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[‘无眼镜’，‘眼镜’] — 0.82 的准确率'
- en: '[‘face without glasses’, ‘face with glasses’] — 0.89 accuracy'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[‘无眼镜的脸’，‘戴眼镜的脸’] — 0.89 的准确率'
- en: '[‘photo of a man with no glasses’, ‘photo of a man with glasses’] — 0.94 accuracy'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[‘没有眼镜的男人的照片’，‘戴眼镜的男人的照片’] — 0.94 的准确率'
- en: As we can see, adjusting the wording can significantly enhance performance.
    By analyzing multiple prompts, we improved accuracy performances from the 0.82
    baseline to 0.94\. However, it’s important to avoid overfitting the prompts to
    the dataset.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，调整措辞可以显著提升性能。通过分析多个提示，我们将准确率从 0.82 提升到了 0.94。然而，需要注意的是，避免对提示进行过拟合。
- en: '**Concluding Remarks**'
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**总结**'
- en: The CLIP model is an incredibly powerful tool for developing zero-shot classifiers
    across a wide variety of tasks. With CLIP, I was able to effortlessly generate
    on-the-fly classifiers with highly satisfactory accuracy on my projects. However,
    CLIP might struggle with tasks like fine-grained classification, abstract or systematic
    tasks such as counting objects, and predicting truly out-of-distribution images
    that were not covered in its pre-training dataset. Therefore, its performance
    on a new assignment should be evaluated beforehand.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP 模型是开发零样本分类器的一个非常强大的工具，适用于各种任务。使用 CLIP，我能够轻松地在我的项目中生成实时分类器，并取得了非常满意的准确率。然而，CLIP
    在细粒度分类、抽象或系统性任务（如计数物体）以及预测真正不在其预训练数据集中覆盖的图像时可能会遇到困难。因此，在进行新任务之前应对其性能进行评估。
- en: Using the Jupyter notebook provided below, you can easily create your own custom
    classifier. Just follow the instructions, add your data, and you’ll have a personalized
    classifier up and running in no time.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用下面提供的 Jupyter notebook，你可以轻松创建自己的自定义分类器。只需按照说明进行操作，添加你的数据，你就能快速启动并运行一个个性化的分类器。
- en: Thank you for reading!
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: '**Want to learn more?**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**想了解更多？**'
- en: '[**Explore**](https://medium.com/@lihigurarie) additional articles I’ve written'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**探索**](https://medium.com/@lihigurarie) 我所写的其他文章'
- en: '[**Subscribe**](https://medium.com/@lihigurarie/subscribe)to get notified when
    I publish articles'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**订阅**](https://medium.com/@lihigurarie/subscribe) 以便在我发布文章时收到通知'
- en: Follow me on [**Linkedin**](https://www.linkedin.com/in/lihi-gur-arie/)
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 [**Linkedin**](https://www.linkedin.com/in/lihi-gur-arie/) 上关注我
- en: Full Jupyter Notebook Code
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 完整的 Jupyter Notebook 代码
- en: The full code for the tutorial is provided on the first reference [0].
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 教程的完整代码可以在第一个参考文献 [0] 中找到。
- en: '***References***'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '***参考文献***'
- en: '[0] Code: [https://gist.github.com/Lihi-Gur-Arie/844a4c3e98a7561d4e0ddb95879f8c11](https://gist.github.com/Lihi-Gur-Arie/9356a2018c3420a01e3033875405f605)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[0] 代码: [https://gist.github.com/Lihi-Gur-Arie/844a4c3e98a7561d4e0ddb95879f8c11](https://gist.github.com/Lihi-Gur-Arie/9356a2018c3420a01e3033875405f605)'
- en: '[1] CLIP article: [https://arxiv.org/pdf/2103.00020v1.pdf](https://arxiv.org/pdf/2103.00020v1.pdf)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] CLIP 文章: [https://arxiv.org/pdf/2103.00020v1.pdf](https://arxiv.org/pdf/2103.00020v1.pdf)'
- en: '[2] Cosine similarity review: [https://towardsdatascience.com/understanding-cosine-similarity-and-its-application-fd42f585296a](/understanding-cosine-similarity-and-its-application-fd42f585296a)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] 余弦相似度回顾: [https://towardsdatascience.com/understanding-cosine-similarity-and-its-application-fd42f585296a](/understanding-cosine-similarity-and-its-application-fd42f585296a)'
- en: '[3] ‘Glasses or No Glasses’ dataset from Kaggle, license [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/):
    [https://www.kaggle.com/datasets/jeffheaton/glasses-or-no-glasses](https://www.kaggle.com/datasets/jeffheaton/glasses-or-no-glasses)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] 来自 Kaggle 的‘眼镜与非眼镜’数据集，许可证 [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/):
    [https://www.kaggle.com/datasets/jeffheaton/glasses-or-no-glasses](https://www.kaggle.com/datasets/jeffheaton/glasses-or-no-glasses)'
