["```py\npip install glovpy gensim scikit-learn\n```", "```py\nfrom gensim.utils import tokenize\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n\ndef clean_tokenize(text: str) -> list[str]:\n    \"\"\"This function tokenizes texts and removes stop words from them\"\"\"\n    tokens = tokenize(text, lower=True, deacc=True)\n    tokens = [token for token in tokens if token not in ENGLISH_STOP_WORDS]\n    return tokens\n\n# Loading the dataset\ndataset = fetch_20newsgroups(\n    remove=(\"headers\", \"footers\", \"quotes\"), categories=[\"sci.med\"]\n)\nnewsgroups = dataset.data\n\n# Tokenizing the dataset\ntokenized_corpus = [clean_tokenize(text) for text in newsgroups]\n```", "```py\nfrom glovpy import GloVe\n\n# Training word embeddings\nmodel = GloVe(vector_size=25)\nmodel.train(tokenized_corpus)\n```", "```py\nmodel.wv.most_similar(\"child\")\n==============================\n\n+------------+----------+\n| age        | 0.849304 |\n| consistent | 0.844267 |\n| adult      | 0.805101 |\n| range      | 0.800615 |\n| year       | 0.798799 |\n| hand       | 0.792965 |\n| children   | 0.792113 |\n| use        | 0.789804 |\n| restraint  | 0.773764 |\n| belt       | 0.77003  |\n+------------+----------+\n```", "```py\nfrom embedding_explorer import show_network_explorer\n\nvocabulary = model.wv.index_to_key\nembeddings = model.wv.vectors\nshow_network_explorer(vocabulary, embeddings=embeddings)\n```", "```py\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# First we train a model on the corpus that learns all 4-grams\n# We will only take the 4000 most frequent ones into account for now,\n# But you can freely experiment with this\nfeature_extractor = CountVectorizer(ngram_range=(4,4), max_features=4000)\nfeature_extractor.fit(newsgroups)\n# Then we get the vectorizer's vocabulary\nfour_grams = feature_extractor.get_feature_names_out()\n```", "```py\npip install embetter[text]\n```", "```py\nfrom embetter.text import SentenceEncoder\n\nencoder = SentenceEncoder(\"all-MiniLM-L6-v2\")\n```", "```py\nfrom embedding_explorer import show_network_explorer\n\nshow_network_explorer(four_grams, vectorizer=encoder)\n```", "```py\nimport pandas as pd\nimport numpy as np\n\n# Extracting text lengths in number of characters.\nlengths = [len(text) for text in corpus]\n\n# Extracting first 400 characters from each text.\ntext_starts = [text[:400] for text in corpus]\n\n# Extracting the group each text belongs to\n# Sklearn gives the labels back as integers, we have to map them back to\n# the actual textual label.\ngroup_labels = np.array(dataset.target_names)[dataset.target]\n\n# We build a dataframe with the available metadata\nmetadata = pd.DataFrame(dict(length=lengths, text=text_starts, group=group_labels))\n```", "```py\nfrom embedding_explorer import show_clustering\n\nshow_clustering(\n  newsgroups,\n  vectorizer=encoder,\n  metadata=metadata,\n  hover_name=\"group\", # Title of hover box is going to be the group\n  hover_data=[\"text\", \"length\"] # We would also like to see these on hover\n)\n```"]