- en: Too Many Features? Let’s Look at Principal Component Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征过多？让我们来看看主成分分析
- en: 原文：[https://towardsdatascience.com/too-many-features-lets-look-at-principal-component-analysis-62504b791ae9?source=collection_archive---------4-----------------------#2023-06-24](https://towardsdatascience.com/too-many-features-lets-look-at-principal-component-analysis-62504b791ae9?source=collection_archive---------4-----------------------#2023-06-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/too-many-features-lets-look-at-principal-component-analysis-62504b791ae9?source=collection_archive---------4-----------------------#2023-06-24](https://towardsdatascience.com/too-many-features-lets-look-at-principal-component-analysis-62504b791ae9?source=collection_archive---------4-----------------------#2023-06-24)
- en: Home-brewed machine learning model series
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自制机器学习模型系列
- en: '[](https://hectormrejia.medium.com/?source=post_page-----62504b791ae9--------------------------------)[![Hector
    Andres Mejia Vallejo](../Images/e794e545531eeea552986ce7ceb6162f.png)](https://hectormrejia.medium.com/?source=post_page-----62504b791ae9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----62504b791ae9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----62504b791ae9--------------------------------)
    [Hector Andres Mejia Vallejo](https://hectormrejia.medium.com/?source=post_page-----62504b791ae9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://hectormrejia.medium.com/?source=post_page-----62504b791ae9--------------------------------)[![Hector
    Andres Mejia Vallejo](../Images/e794e545531eeea552986ce7ceb6162f.png)](https://hectormrejia.medium.com/?source=post_page-----62504b791ae9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----62504b791ae9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----62504b791ae9--------------------------------)
    [Hector Andres Mejia Vallejo](https://hectormrejia.medium.com/?source=post_page-----62504b791ae9--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4f706205679a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-many-features-lets-look-at-principal-component-analysis-62504b791ae9&user=Hector+Andres+Mejia+Vallejo&userId=4f706205679a&source=post_page-4f706205679a----62504b791ae9---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----62504b791ae9--------------------------------)
    ·13 min read·Jun 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F62504b791ae9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-many-features-lets-look-at-principal-component-analysis-62504b791ae9&user=Hector+Andres+Mejia+Vallejo&userId=4f706205679a&source=-----62504b791ae9---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4f706205679a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-many-features-lets-look-at-principal-component-analysis-62504b791ae9&user=Hector+Andres+Mejia+Vallejo&userId=4f706205679a&source=post_page-4f706205679a----62504b791ae9---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----62504b791ae9--------------------------------)
    · 13分钟阅读 · 2023年6月24日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F62504b791ae9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-many-features-lets-look-at-principal-component-analysis-62504b791ae9&user=Hector+Andres+Mejia+Vallejo&userId=4f706205679a&source=-----62504b791ae9---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F62504b791ae9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-many-features-lets-look-at-principal-component-analysis-62504b791ae9&source=-----62504b791ae9---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F62504b791ae9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-many-features-lets-look-at-principal-component-analysis-62504b791ae9&source=-----62504b791ae9---------------------bookmark_footer-----------)'
- en: '*The companion repo is available* [*here*](https://github.com/hector6298/medium_posts/tree/main/pca)*!*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*该配套仓库可在* [*这里*](https://github.com/hector6298/medium_posts/tree/main/pca)*获取*！'
- en: The curse of dimensionality is one major problem in machine learning. As the
    number of features increases, so does the complexity of the model. Moreover, if
    there is not enough training data, it results in overfitting.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 维度诅咒是机器学习中的一个主要问题。随着特征数量的增加，模型的复杂性也随之增加。此外，如果训练数据不足，会导致过拟合。
- en: In this entry, Principal Component Analysis (PCA) will be introduced. First,
    I will explain why too many features are a problem. Then, the math behind PCA
    and why it works. Additionally, PCA will be broken down into steps, accompanied
    by visual examples and code snippets. Moreover, the advantages and disadvantages
    of PCA will be explained. Lastly, these steps will be encapsulated in a Python
    class for later use.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，将介绍主成分分析（PCA）。首先，我将解释为什么特征过多是个问题。接着，讲解PCA背后的数学原理及其为何有效。此外，将对PCA进行逐步解析，并附上视觉示例和代码片段。最后，将总结PCA的优缺点，并将这些步骤封装在一个Python类中以备后用。
- en: '![](../Images/17329967b1165e6dec65d96b810bfc39.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17329967b1165e6dec65d96b810bfc39.png)'
- en: Projecting a vector into a lower space is like casting a shadow in real life.
    PCA finds the direction from where to cast these shadows. Photo by [inbal marilli](https://unsplash.com/@inbalmarilli?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 将一个向量投影到一个低维空间就像现实生活中的投影一样。PCA 找到这些投影的方向。照片由 [inbal marilli](https://unsplash.com/@inbalmarilli?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '**Note for the reader:** If you are not interested in the math explanation
    and just want to see the practical examples and how PCA works, jump to the section
    **“**[**PCA in practice**](#ea59)**”**. If you are only interested in the Python
    class head to **“**[**Home-brewed PCA implementation**](#6814)**”**.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**读者注意：** 如果你对数学解释不感兴趣，只想查看实际示例以及PCA的工作原理，请跳至 **“**[**PCA的实践**](#ea59)**”**
    部分。如果你只对Python类感兴趣，请前往 **“**[**自制PCA实现**](#6814)**”** 部分。'
- en: The problem with too many features?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征过多的问题？
- en: Take a look at the feature space in Fig 1\. There are few examples to fill all
    the space, so a model of this data might not generalize well to new, unseen examples.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 查看图1中的特征空间。这里的示例较少，因此该数据的模型可能无法很好地推广到新的、未见过的示例。
- en: '![](../Images/9114d403b87c648ccd02026177041024.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9114d403b87c648ccd02026177041024.png)'
- en: Fig 1\. An example of 2-dimensional feature space. From my own authorship, inspired
    by [1].
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 二维特征空间的示例。由我创作，灵感来源于 [1]。
- en: What happens if we add another feature? Let’s have a look at the new feature
    space in Fig. 2\. You can see that there is even more empty space than in the
    previous example. As the number of features increases, the model will overfit
    the current data. That is why there are techniques to reduce the dimensionality
    of the data and alleviate this problem. [1]
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们添加另一个特征会发生什么？让我们查看图2中的新特征空间。你会发现这里比之前的例子有更多的空白区域。随着特征数量的增加，模型将过拟合当前数据。这就是为什么需要减少数据维度以缓解此问题的技术。[1]
- en: '![](../Images/9b2aa64da64c3633a4934004857e504f.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b2aa64da64c3633a4934004857e504f.png)'
- en: The same example with an additional feature. From my own authorship, inspired
    by [1].
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的例子，加上一个额外的特征。由我创作，灵感来源于 [1]。
- en: What is the goal of PCA?
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PCA的目标是什么？
- en: 'In a few words, the purpose of PCA is to extract new, uncorrelated features
    of lower dimensions that maximize the amount of information kept from the original
    data. The measure of information in this context is the variance. Let’s see why:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，PCA的目的是提取新的、不相关的低维特征，这些特征最大化保留原始数据中的信息。在这种情况下，信息的衡量标准是方差。让我们看看为什么：
- en: 'This technique is based on the assumption that our d-dimensional data point
    ***x*** can be represented by a linear combination of vectors of an [orthonormal
    basis](https://en.wikipedia.org/wiki/Orthonormal_basis) [1]:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 该技术基于这样的假设：我们的d维数据点***x***可以通过[正交基](https://en.wikipedia.org/wiki/Orthonormal_basis)
    [1]的向量的线性组合来表示。
- en: '![](../Images/1764faa46344ff82aacf17d3dc89c687.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1764faa46344ff82aacf17d3dc89c687.png)'
- en: 'Do not worry, I will explain where we get the vectors of said basis later.
    Moreover, we can extract a representation ***x̂*** using ***m*** out of the ***d***
    vectors in the combination (***m < d***):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 不用担心，我会稍后解释我们如何获得所述基的向量。此外，我们可以从这***d***个向量中提取一个表示***x̂***，使用***m***（***m <
    d***）：
- en: '![](../Images/767fe68f8992f5ff3f08f63892ca4df7.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/767fe68f8992f5ff3f08f63892ca4df7.png)'
- en: 'Of course, we are not getting an exact representation since there are fewer
    features, but at least we try to minimize the loss of information. Let us define
    the Mean Squared Error (MSE) between the original example ***x*** and the approximation
    ***x̂***:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，由于特征较少，我们无法获得精确的表示，但至少我们尽量减少信息的丢失。我们定义原始示例***x***与近似值***x̂***之间的均方误差（MSE）：
- en: '![](../Images/2504c6b592c4d34d8c61a5e60ff4cf0b.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2504c6b592c4d34d8c61a5e60ff4cf0b.png)'
- en: 'As the summations use the same variables with different cutoffs, then the difference
    is just the offset:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 由于求和使用了具有不同截止点的相同变量，因此差异仅是偏移量：
- en: '![](../Images/b82a5f4044c1574a6356e03e94ffd686.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b82a5f4044c1574a6356e03e94ffd686.png)'
- en: 'We know by our starting hypothesis that ***x*** is the sum of orthonormal vectors.
    Hence, the dot product of these vectors is zero, and each of their Euclidean norms
    is one. Thus:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从最初的假设知道***x***是正交归一向量的和。因此，这些向量的点积为零，它们的欧几里得范数为一。因此：
- en: '![](../Images/056c38c2beae05a575eff9a5c561e75d.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/056c38c2beae05a575eff9a5c561e75d.png)'
- en: Solving the importance value ***yi:***
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 求解重要性值***yi:***
- en: '![](../Images/63d9c9fa1b8d6ecd54ef51cef1277a11.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63d9c9fa1b8d6ecd54ef51cef1277a11.png)'
- en: 'Plugging that result into its expectation:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 将结果代入其期望值：
- en: '![](../Images/2a097f62ec444e57e2e93c7e17a8f8d5.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2a097f62ec444e57e2e93c7e17a8f8d5.png)'
- en: We can see that if ***xi* is centered (mean equal to zero), then the expectation
    turns out to be the covariance matrix of the whole data,** and this result is
    nothing more than the variance in the original space. By choosing the right vectors
    ***vi*** that maximize the variance, we will effectively minimize the representation
    error.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，如果***xi***被中心化（均值为零），那么期望值将变成整个数据的协方差矩阵，**这个结果只是原始空间中的方差**。通过选择能够最大化方差的正确向量***vi***，我们将有效地最小化表示误差。
- en: Where does this orthonormal basis come from?
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这个正交归一基是从哪里来的？
- en: 'As previously stated, we want to get ***m*** vectors that maximize the variance:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们希望获得***m***个最大化方差的向量：
- en: '![](../Images/3fae438f22129e25c968095b2aa84925.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3fae438f22129e25c968095b2aa84925.png)'
- en: If we take the whole data matrix, it can be seen that ***vi*** is a projection
    direction. The data is going to be projected in a space of lower dimensionality.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑整个数据矩阵，可以看出***vi***是一个投影方向。数据将会被投影到一个低维空间中。
- en: 'If we diagonalize the covariance matrix Σ using spectral decomposition we get:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用谱分解对协方差矩阵Σ进行对角化，我们得到：
- en: '![](../Images/7b348f4a8bea640534175f35b43ef2b5.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b348f4a8bea640534175f35b43ef2b5.png)'
- en: Where ***U*** is a matrix with the normalized eigenvectors ofΣ, and Λ is a diagonal
    matrix that contains the eigenvalues of Σ in descending order. This is possible
    since Σ is a real, symmetric matrix.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 其中***U***是一个包含Σ的归一化特征向量的矩阵，Λ是一个包含Σ特征值按降序排列的对角矩阵。这是可能的，因为Σ是一个实对称矩阵。
- en: 'Furthermore, as Λ only contains non-zero values on the diagonal, we can rewrite
    the above equation as:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于Λ仅在对角线上包含非零值，我们可以将上述方程重写为：
- en: '![](../Images/d5880b3851a323759b69615073bf1fe1.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d5880b3851a323759b69615073bf1fe1.png)'
- en: 'with:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 使用：
- en: '![](../Images/88c277359f9f809bdec013395bc58136.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/88c277359f9f809bdec013395bc58136.png)'
- en: 'Notice that the vectors in ***U*** and vector ***v*** are normalized***.***
    Thus, when performing the squared dot product of each v with a, we get a value
    between ***[0,1]*** and so ***w*** must also be a normalized vector:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 注意***U***中的向量和向量***v***都是归一化的***。*** 因此，当对每个v与a进行平方点积时，我们得到的值介于***[0,1]***之间，因此***w***也必须是一个归一化向量：
- en: '![](../Images/f1692e25298fd8ed0a62c32e1d730097.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1692e25298fd8ed0a62c32e1d730097.png)'
- en: From here, interesting properties arise.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，出现了一些有趣的性质。
- en: The first principal component
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一个主成分
- en: 'Recall the optimization problem. Since the eigenvalues are ordered and ***w***
    must be a normalized vector, our best option is to get the first eigenvector with
    ***w = (1,0,0,…)***. As a consequence, the upper bound is attained when:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾优化问题。由于特征值是有序的且***w***必须是一个归一化向量，我们的最佳选择是得到第一个特征向量，其中***w = (1,0,0,…)***。因此，上界在以下情况下达成：
- en: '![](../Images/964ece0473ac69de5d445f2a68380d4d.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/964ece0473ac69de5d445f2a68380d4d.png)'
- en: The projection direction that maximizes the variance turns out to be the **eigenvector**
    associated with the **largest eigenvalue**!
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最大化方差的投影方向恰好是与**最大特征值**相关联的**特征向量**！
- en: The rest of the components
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其余的成分
- en: 'Once the first principal component is set, a new restriction to the optimization
    problem is added:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦设定了第一个主成分，就会增加一个新的优化问题限制：
- en: '![](../Images/128e0ec1b2343655db9a6c2b5e267875.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/128e0ec1b2343655db9a6c2b5e267875.png)'
- en: It means that the new component ***v2*** must be orthogonal to the previous
    component, the eigenvector ***u1*** so that the information is not redundant.
    It can be proved that all the d components correspond to the d normalized eigenvectors
    from Σ associated with the eigenvalues, in descending order. Take a look at [these
    notes](https://verso.mat.uam.es/~joser.berrendero/blog/Componentes_principales.pdf)
    for formal proof of this claim [2].
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着新的成分***v2***必须与先前的成分特征向量***u1***正交，以避免信息冗余。可以证明，所有的d个成分对应于Σ中与特征值相关的d个归一化特征向量，按降序排列。请查看[这些笔记](https://verso.mat.uam.es/~joser.berrendero/blog/Componentes_principales.pdf)以获取正式证明[2]。
- en: PCA in practice
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践中的PCA
- en: 'From the theoretical description above the steps needed to get the principal
    components of a dataset can be described. Let the initial dataset be a random
    sample of the following 2D normal distribution:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述理论描述中，可以描述获取数据集主成分所需的步骤。假设初始数据集是以下二维正态分布的随机样本：
- en: '![](../Images/c7e6e739f0b39073b5e8bc0f726f3955.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7e6e739f0b39073b5e8bc0f726f3955.png)'
- en: '[PRE0]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/ad8e48a4a3fd0926f679852a702f589d.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad8e48a4a3fd0926f679852a702f589d.png)'
- en: Fig 3\. Original point cloud. Non-centered. From my own authorship.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 原始点云。非居中。作者原创。
- en: 1\. Centering the data
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 数据居中
- en: The first step is to move the cloud to the origin of the coordinate system so
    the data has zero mean. This step is performed by subtracting the sample mean
    from every point in the dataset.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是将数据点移动到坐标系的原点，使数据具有零均值。此步骤通过从数据集中的每个点中减去样本均值来完成。
- en: '![](../Images/924df51be6a24b5d02272354acc8d631.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/924df51be6a24b5d02272354acc8d631.png)'
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/9bf7e20ac75c23783b3938411fa9d203.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9bf7e20ac75c23783b3938411fa9d203.png)'
- en: Fig 4\. Centered point cloud. From my own authorship.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. 居中点云。作者原创。
- en: '**2\. Computing the covariance matrix**'
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2\. 计算协方差矩阵**'
- en: The variance defined above is the population covariance matrix Σ. In practice,
    that information is not available to us as we only have one sample. Therefore,
    we can approximate that parameter by using the sample covariance ***S***.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 上述定义的方差是总体协方差矩阵Σ。在实践中，我们没有这些信息，因为我们只有一个样本。因此，我们可以使用样本协方差***S***来近似该参数。
- en: '![](../Images/7008c4cf457e46a38dc01e78392221ab.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7008c4cf457e46a38dc01e78392221ab.png)'
- en: 'Recall that the data is already centered. Thus:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，数据已经被居中。因此：
- en: '![](../Images/65479fcb3ada247c13134ed263cc147f.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/65479fcb3ada247c13134ed263cc147f.png)'
- en: 'We can write this compactly using matrix multiplications. This can also help
    us vectorize computations:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用矩阵乘法将其简洁地表示。这也可以帮助我们向量化计算：
- en: '![](../Images/b0b8be7f69f5d2228c2670327d8cb2c2.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b0b8be7f69f5d2228c2670327d8cb2c2.png)'
- en: '[PRE2]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The reason for passing the transposed matrix as the first argument in the code
    is that in the mathematical formulation of the data matrix, the features are in
    the rows and the subjects in the columns. In the implementation, the opposite
    happens since in almost every system, the events, subjects, logs, and so forth,
    are stored in rows.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 代码中将转置矩阵作为第一个参数传递的原因是，在数据矩阵的数学公式中，特征在行中，主题在列中。在实现中，情况正好相反，因为在几乎每个系统中，事件、主题、日志等都是以行的形式存储的。
- en: 3\. Perform the eigendecomposition of the covariance matrix
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 执行协方差矩阵的特征分解
- en: 'The eigenvalues and eigenvectors a are computed using `eig()`from [scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.eig.html):'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 特征值和特征向量是使用来自[scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.eig.html)的`eig()`计算的：
- en: '[PRE3]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As it was explained earlier, the eigenvalues represent the variance of the
    principal components, and the eigenvectors are the projection directions:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面所解释的，特征值表示主成分的方差，而特征向量是投影方向：
- en: '![](../Images/4ab55a58280703a611fc2d5e071fb60c.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ab55a58280703a611fc2d5e071fb60c.png)'
- en: Fig 5\. Plot of the projection directions and the point cloud. From my own authorship.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 投影方向和点云的图示。作者原创。
- en: You can see that a new coordinate system is created using the directions of
    the principal components. Furthermore, the eigenvalues and eigenvectors must be
    stored to transform new data afterward.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，使用主成分的方向创建了一个新的坐标系统。此外，必须存储特征值和特征向量，以便在之后转换新数据。
- en: 4\. Enforce determinism
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 强制确定性
- en: The coefficients of the eigenvectors will be always the same, except for their
    sign. PCA can have multiple valid orientations. Thus, we need to enforce a deterministic
    result by taking the eigenvector matrix and for each of its columns, applying
    the sign of the largest absolute value within that column.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 特征向量的系数总是相同的，只是其符号不同。PCA 可以有多个有效的方向。因此，我们需要通过取特征向量矩阵并对每一列应用该列内最大绝对值的符号来强制确定结果。
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**5\. Extract the new features**'
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**5\. 提取新特征**'
- en: 'Each new feature (principal component) is extracted by performing the dot product
    between each point in the original feature space and the eigenvector:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 每个新特征（主成分）是通过计算原始特征空间中每个点与特征向量之间的点积来提取的：
- en: '![](../Images/cab4476710ddca3741d8316199bdf964.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cab4476710ddca3741d8316199bdf964.png)'
- en: '[PRE5]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'For this particular example, after computing the components, the new points
    in the space are depicted as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个特定示例，在计算主成分后，空间中的新点表示如下：
- en: '![](../Images/3d3812f2c2b87edc472180aebe1c7852.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d3812f2c2b87edc472180aebe1c7852.png)'
- en: Fig 6\. Plot of the uncorrelated point cloud, using the features from the PCs.
    Notice that it is just a rotation of the original point cloud. From my own authorship.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6\. 使用主成分中的特征绘制的不相关点云图。注意这只是原始点云的旋转。出自我自己的创作。
- en: Notice that this result is basically a rotation of the original cloud of points
    in a way that the attributes are uncorrelated.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个结果基本上是将原始点云旋转，使得属性不相关。
- en: 6\. Reduce dimensionality
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 降维
- en: So far, the principal components were computed in full to understand them, in
    a visual manner. What is left is to choose how many components are needed. We
    resort to the eigenvalues for this task, since they represent the variance of
    each principal component.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，主成分已经完全计算出来以进行视觉上的理解。剩下的任务是选择需要多少个主成分。我们利用特征值来完成这项任务，因为它们代表了每个主成分的方差。
- en: 'The ratio of the variance held by component ***i*** is given by:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 成分***i***所持方差的比例为：
- en: '![](../Images/fd0ce9ccf607931b83cfb40d532858ae.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd0ce9ccf607931b83cfb40d532858ae.png)'
- en: 'And the ratio of the variance preserved by choosing m components is given by:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 选择 m 个主成分所保留的方差比例为：
- en: '![](../Images/3049e97f942dcd6f40fc2bdd6c355584.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3049e97f942dcd6f40fc2bdd6c355584.png)'
- en: 'If we visualize the variance for each component in our example, we arrive at
    the following:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们可视化我们示例中每个成分的方差，我们会得到以下结果：
- en: '[PRE6]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/6133d15c87d477e0029c58506409a7d9.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6133d15c87d477e0029c58506409a7d9.png)'
- en: Fig 7\. Variance explained by each principal component. From my own authorship.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. 每个主成分解释的方差。出自我自己的创作。
- en: 'In this case, PC1 represents 80% of the variance of the original data, with
    the remaining 20% belonging to PC2\. Furthermore, we can choose to use only the
    first principal component, in which case the data would look like this:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，PC1 代表了原始数据方差的 80%，剩余的 20% 归属于 PC2。此外，我们可以选择仅使用第一个主成分，此时数据将如下所示：
- en: '![](../Images/57708ce6e3b3b01b737f82496af65fa5.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57708ce6e3b3b01b737f82496af65fa5.png)'
- en: Fig 8\. Projection of the data in the direction of the first eigenvector. From
    my own authorship.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 数据在第一个特征向量方向上的投影。出自我自己的创作。
- en: This is the projection of the data in the direction of the first eigenvector.
    It does not look very useful right now. **What if we instead choose data that
    belong to three classes? How would PCA look?**
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这是数据在第一个特征向量方向上的投影。现在看起来不是很有用。**如果我们选择属于三类的数据会怎样？PCA 会是什么样的？**
- en: PCA on multiclass data
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多类数据上的 PCA
- en: 'Let us create a dataset with three classes that can be linearly separable:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个具有三类且可以线性分开的数据集：
- en: '[PRE7]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/e0c35eadf0360239499ca2689ed87586.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e0c35eadf0360239499ca2689ed87586.png)'
- en: Fig 9\. Multi-class data. From my own authorship.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9\. 多类数据。出自我自己的创作。
- en: 'If we apply PCA to the data above, this would be the plot of the principal
    components:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将 PCA 应用于上述数据，这将是主成分的图示：
- en: '![](../Images/929c5cdaaa9692781531ccc3530afa25.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/929c5cdaaa9692781531ccc3530afa25.png)'
- en: Fig 10\. Multi-class uncorrelated data. From my own authorship.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10\. 多类不相关数据。出自我自己的创作。
- en: 'And this would be the plot of the first component (the projection of the data
    in the direction of the eigenvector corresponding to the largest eigenvalue):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是第一个主成分的图示（数据在对应于最大特征值的特征向量方向上的投影）：
- en: '![](../Images/ebee20490b7269ee9f07ef2242881138.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ebee20490b7269ee9f07ef2242881138.png)'
- en: Fig 11\. Projection of the data in the direction of the first eigenvector. This
    is the dimensionality reduction. From my own authorship.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11\. 数据在第一个特征向量方向的投影。这就是维度减少。来自我个人的创作。
- en: It works! The data still looks easily separable by a linear model.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 它有效！数据看起来仍然可以通过线性模型轻松分离。
- en: Advantages and disadvantages
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优缺点
- en: Like everything in science, there is no silver bullet. Here is a list of advantages
    and disadvantages that you should take into account before using PCA with real-world
    data.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 像科学中的一切一样，没有万能的解决方案。这里是一些优缺点的列表，你应该在使用 PCA 处理实际数据之前加以考虑。
- en: Advantages of PCA
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PCA 的优点
- en: '**Dimensionality Reduction:** PCA allows for the reduction of high-dimensional
    data into a lower-dimensional space while preserving most of the important information.
    This can be useful for data visualization, computational efficiency, and dealing
    with the curse of dimensionality.'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**维度减少：** PCA 允许将高维数据减少到低维空间，同时保留大部分重要信息。这对于数据可视化、计算效率和处理维度灾难非常有用。'
- en: '**Decorrelation:** PCA transforms the original variables into a new set of
    uncorrelated variables called principal components. This decorrelation simplifies
    the analysis and can improve the performance of downstream machine learning algorithms
    that assume independence of features.'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**去相关：** PCA 将原始变量转换为一组新的不相关变量，称为主成分。这种去相关简化了分析，并可以提高假设特征独立性的下游机器学习算法的性能。'
- en: '**Noise Reduction:** The lower-dimensional representation obtained through
    PCA tends to filter out noise and focus on the most significant variations in
    the data. This can enhance the signal-to-noise ratio and improve the robustness
    of subsequent analyses.'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**噪声减少：** 通过 PCA 获得的低维表示倾向于过滤掉噪声，关注数据中最重要的变化。这可以提高信噪比，并增强后续分析的鲁棒性。'
- en: Disadvantages of PCA
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PCA 的缺点
- en: '**Linearity Assumption:** PCA assumes that the underlying data relationships
    are linear. If the data has complex non-linear relationships, PCA may not capture
    the most meaningful variations and could provide suboptimal results.'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**线性假设：** PCA 假设数据关系是线性的。如果数据具有复杂的非线性关系，PCA 可能无法捕捉最有意义的变化，并可能提供次优结果。'
- en: '**Interpretability:** The principal components obtained from PCA are linear
    combinations of the original features. It can be difficult to relate the principal
    components back to the original variables and understand their exact meaning.'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**可解释性：** 从 PCA 获得的主成分是原始特征的线性组合。将主成分与原始变量联系起来并理解其确切含义可能比较困难。'
- en: '**Sensitivity to Scaling:** PCA is sensitive to the scaling of the input variables.
    If the variables have different scales, those with larger variances can dominate
    the analysis, potentially leading to biased results. Proper feature scaling is
    crucial for obtaining reliable results with PCA.'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**对尺度的敏感性：** PCA 对输入变量的尺度敏感。如果变量具有不同的尺度，那么具有更大方差的变量可能会主导分析，导致偏差结果。适当的特征缩放对于获得可靠的
    PCA 结果至关重要。'
- en: '**Outliers:** PCA is sensitive to outliers since it focuses on capturing the
    variance in the data. Outliers can significantly influence the principal components
    and distort the results.'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**异常值：** PCA 对异常值非常敏感，因为它关注数据的方差捕捉。异常值可能会显著影响主成分，并扭曲结果。'
- en: Home-brewed PCA implementation
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自制 PCA 实现
- en: Now that we have covered the details of Principal Component Analysis, all that
    remains is to create a class that encapsulates the aforementioned behavior and
    that could be reused in future problems.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了主成分分析的细节，剩下的就是创建一个封装上述行为的类，以便在未来的问题中重用。
- en: 'For this implementation, the scikit-learn interface will be used, which has
    the following methods:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个实现，将使用 scikit-learn 接口，其具有以下方法：
- en: '`fit()`'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fit()`'
- en: '`transform()`'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transform()`'
- en: '`fit_transform()`'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fit_transform()`'
- en: Constructor
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构造函数
- en: No complex logic is needed. The constructor will just define the number of components
    (features) that the transformed data will have.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要复杂的逻辑。构造函数只需定义变换数据将具有的组件（特征）数量。
- en: '[PRE8]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The fit method
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: fit 方法
- en: The fit method will apply steps 1–4 from the previous section.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: fit 方法将应用上一节中的步骤 1–4。
- en: Centering the data
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据中心化
- en: Computing the covariance matrix
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算协方差矩阵
- en: Computing eigenvalues, eigenvectors and sorting them
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算特征值、特征向量并对其进行排序
- en: Enforcing determinism by flipping the signs of the eigenvectors
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过翻转特征向量的符号来强制确定性
- en: It will also store the eigenvalues and vectors, as well as the sample mean,
    as object attributes to transform new data later.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 它还将存储特征值和特征向量，以及样本均值，作为对象属性，以便稍后转换新数据。
- en: '[PRE9]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The transform method
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换方法
- en: 'It will apply steps 1, 5, and 6:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 它将应用步骤1、5和6：
- en: Centering new data using the stored sample mean
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用存储的样本均值对新数据进行中心化
- en: Extracting the new PC features
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取新的主成分特征
- en: Reducing the dimensionality by picking `n_components` dimensions.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过选择`n_components`维度来降低维度。
- en: '[PRE10]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The fit_transform method
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: fit_transform方法
- en: For simplicity of implementation. This method will apply the `fit()` function
    first and `transform()` later. I am sure you can figure out a more clever definition.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化实现。该方法将首先应用`fit()`函数，然后应用`transform()`。我相信你可以找到更巧妙的定义。
- en: '[PRE11]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Helper functions
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 辅助函数
- en: These methods were defined as separate components, instead of applying all the
    steps in the `fit()`function to make the code more readable and maintainable.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法被定义为独立的组件，而不是将所有步骤应用于`fit()`函数，以使代码更具可读性和可维护性。
- en: '[PRE12]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Testing the class
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试类
- en: 'Let’s use the previous example with our `PCA` class:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用之前的示例与我们的`PCA`类：
- en: '[PRE13]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/ebee20490b7269ee9f07ef2242881138.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ebee20490b7269ee9f07ef2242881138.png)'
- en: Fig 12\. Results using the PCA class. From my own authorship.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图12\. 使用PCA类的结果。由我本人编著。
- en: Conclusion
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Having many features with small data can be harmful and will, most likely, result
    in overfitting. Principal Component Analysis is a tool that can help alleviate
    this problem. It is a dimensionality reduction technique that works by finding
    projection directions for the data in a way that the original variability is preserved
    as much as possible, and the resulting features are uncorrelated. Moreover, the
    variance explained by each new feature, or principal component, can be measured.
    Then, the user can choose how many principal components and how much variance
    is enough for the task. Finally, be sure to know your data first, as PCA works
    with samples that can be linearly separated and can be sensitive to outliers.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 特征过多而数据较少可能会有害，并且很可能导致过拟合。主成分分析（PCA）是一个可以帮助缓解这个问题的工具。它是一种降维技术，通过寻找数据的投影方向来尽可能多地保留原始变异性，同时使得结果特征无关。更重要的是，可以测量每个新特征或主成分解释的方差。然后，用户可以选择多少主成分和多少方差对任务足够。最后，确保首先了解你的数据，因为PCA适用于可以线性分离的样本，并且可能对异常值敏感。
- en: References
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Fernández, A. Dimensionality Reduction. Universidad Autónoma de Madrid.
    Madrid, Spain. 2022.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Fernández, A. Dimensionality Reduction. Universidad Autónoma de Madrid.
    Madrid, Spain. 2022.'
- en: '[2] Berrendero, J. R. Regresión lineal con datos de alta dimensión. Universidad
    Autónoma de Madrid. Madrid, Spain. 2022.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Berrendero, J. R. Regresión lineal con datos de alta dimensión. Universidad
    Autónoma de Madrid. Madrid, Spain. 2022.'
- en: Connect with me on [LinkedIn](https://www.linkedin.com/in/hectorandresmv/)!
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在[LinkedIn](https://www.linkedin.com/in/hectorandresmv/)上与我联系！
