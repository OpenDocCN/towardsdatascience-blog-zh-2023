- en: Too Many Features? Let’s Look at Principal Component Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/too-many-features-lets-look-at-principal-component-analysis-62504b791ae9?source=collection_archive---------4-----------------------#2023-06-24](https://towardsdatascience.com/too-many-features-lets-look-at-principal-component-analysis-62504b791ae9?source=collection_archive---------4-----------------------#2023-06-24)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Home-brewed machine learning model series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://hectormrejia.medium.com/?source=post_page-----62504b791ae9--------------------------------)[![Hector
    Andres Mejia Vallejo](../Images/e794e545531eeea552986ce7ceb6162f.png)](https://hectormrejia.medium.com/?source=post_page-----62504b791ae9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----62504b791ae9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----62504b791ae9--------------------------------)
    [Hector Andres Mejia Vallejo](https://hectormrejia.medium.com/?source=post_page-----62504b791ae9--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4f706205679a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-many-features-lets-look-at-principal-component-analysis-62504b791ae9&user=Hector+Andres+Mejia+Vallejo&userId=4f706205679a&source=post_page-4f706205679a----62504b791ae9---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----62504b791ae9--------------------------------)
    ·13 min read·Jun 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F62504b791ae9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-many-features-lets-look-at-principal-component-analysis-62504b791ae9&user=Hector+Andres+Mejia+Vallejo&userId=4f706205679a&source=-----62504b791ae9---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F62504b791ae9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftoo-many-features-lets-look-at-principal-component-analysis-62504b791ae9&source=-----62504b791ae9---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: '*The companion repo is available* [*here*](https://github.com/hector6298/medium_posts/tree/main/pca)*!*'
  prefs: []
  type: TYPE_NORMAL
- en: The curse of dimensionality is one major problem in machine learning. As the
    number of features increases, so does the complexity of the model. Moreover, if
    there is not enough training data, it results in overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: In this entry, Principal Component Analysis (PCA) will be introduced. First,
    I will explain why too many features are a problem. Then, the math behind PCA
    and why it works. Additionally, PCA will be broken down into steps, accompanied
    by visual examples and code snippets. Moreover, the advantages and disadvantages
    of PCA will be explained. Lastly, these steps will be encapsulated in a Python
    class for later use.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/17329967b1165e6dec65d96b810bfc39.png)'
  prefs: []
  type: TYPE_IMG
- en: Projecting a vector into a lower space is like casting a shadow in real life.
    PCA finds the direction from where to cast these shadows. Photo by [inbal marilli](https://unsplash.com/@inbalmarilli?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '**Note for the reader:** If you are not interested in the math explanation
    and just want to see the practical examples and how PCA works, jump to the section
    **“**[**PCA in practice**](#ea59)**”**. If you are only interested in the Python
    class head to **“**[**Home-brewed PCA implementation**](#6814)**”**.'
  prefs: []
  type: TYPE_NORMAL
- en: The problem with too many features?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Take a look at the feature space in Fig 1\. There are few examples to fill all
    the space, so a model of this data might not generalize well to new, unseen examples.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9114d403b87c648ccd02026177041024.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 1\. An example of 2-dimensional feature space. From my own authorship, inspired
    by [1].
  prefs: []
  type: TYPE_NORMAL
- en: What happens if we add another feature? Let’s have a look at the new feature
    space in Fig. 2\. You can see that there is even more empty space than in the
    previous example. As the number of features increases, the model will overfit
    the current data. That is why there are techniques to reduce the dimensionality
    of the data and alleviate this problem. [1]
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9b2aa64da64c3633a4934004857e504f.png)'
  prefs: []
  type: TYPE_IMG
- en: The same example with an additional feature. From my own authorship, inspired
    by [1].
  prefs: []
  type: TYPE_NORMAL
- en: What is the goal of PCA?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a few words, the purpose of PCA is to extract new, uncorrelated features
    of lower dimensions that maximize the amount of information kept from the original
    data. The measure of information in this context is the variance. Let’s see why:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This technique is based on the assumption that our d-dimensional data point
    ***x*** can be represented by a linear combination of vectors of an [orthonormal
    basis](https://en.wikipedia.org/wiki/Orthonormal_basis) [1]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1764faa46344ff82aacf17d3dc89c687.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Do not worry, I will explain where we get the vectors of said basis later.
    Moreover, we can extract a representation ***x̂*** using ***m*** out of the ***d***
    vectors in the combination (***m < d***):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/767fe68f8992f5ff3f08f63892ca4df7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Of course, we are not getting an exact representation since there are fewer
    features, but at least we try to minimize the loss of information. Let us define
    the Mean Squared Error (MSE) between the original example ***x*** and the approximation
    ***x̂***:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2504c6b592c4d34d8c61a5e60ff4cf0b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As the summations use the same variables with different cutoffs, then the difference
    is just the offset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b82a5f4044c1574a6356e03e94ffd686.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We know by our starting hypothesis that ***x*** is the sum of orthonormal vectors.
    Hence, the dot product of these vectors is zero, and each of their Euclidean norms
    is one. Thus:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/056c38c2beae05a575eff9a5c561e75d.png)'
  prefs: []
  type: TYPE_IMG
- en: Solving the importance value ***yi:***
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/63d9c9fa1b8d6ecd54ef51cef1277a11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Plugging that result into its expectation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2a097f62ec444e57e2e93c7e17a8f8d5.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that if ***xi* is centered (mean equal to zero), then the expectation
    turns out to be the covariance matrix of the whole data,** and this result is
    nothing more than the variance in the original space. By choosing the right vectors
    ***vi*** that maximize the variance, we will effectively minimize the representation
    error.
  prefs: []
  type: TYPE_NORMAL
- en: Where does this orthonormal basis come from?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As previously stated, we want to get ***m*** vectors that maximize the variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3fae438f22129e25c968095b2aa84925.png)'
  prefs: []
  type: TYPE_IMG
- en: If we take the whole data matrix, it can be seen that ***vi*** is a projection
    direction. The data is going to be projected in a space of lower dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we diagonalize the covariance matrix Σ using spectral decomposition we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7b348f4a8bea640534175f35b43ef2b5.png)'
  prefs: []
  type: TYPE_IMG
- en: Where ***U*** is a matrix with the normalized eigenvectors ofΣ, and Λ is a diagonal
    matrix that contains the eigenvalues of Σ in descending order. This is possible
    since Σ is a real, symmetric matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, as Λ only contains non-zero values on the diagonal, we can rewrite
    the above equation as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d5880b3851a323759b69615073bf1fe1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'with:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/88c277359f9f809bdec013395bc58136.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Notice that the vectors in ***U*** and vector ***v*** are normalized***.***
    Thus, when performing the squared dot product of each v with a, we get a value
    between ***[0,1]*** and so ***w*** must also be a normalized vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f1692e25298fd8ed0a62c32e1d730097.png)'
  prefs: []
  type: TYPE_IMG
- en: From here, interesting properties arise.
  prefs: []
  type: TYPE_NORMAL
- en: The first principal component
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recall the optimization problem. Since the eigenvalues are ordered and ***w***
    must be a normalized vector, our best option is to get the first eigenvector with
    ***w = (1,0,0,…)***. As a consequence, the upper bound is attained when:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/964ece0473ac69de5d445f2a68380d4d.png)'
  prefs: []
  type: TYPE_IMG
- en: The projection direction that maximizes the variance turns out to be the **eigenvector**
    associated with the **largest eigenvalue**!
  prefs: []
  type: TYPE_NORMAL
- en: The rest of the components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once the first principal component is set, a new restriction to the optimization
    problem is added:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/128e0ec1b2343655db9a6c2b5e267875.png)'
  prefs: []
  type: TYPE_IMG
- en: It means that the new component ***v2*** must be orthogonal to the previous
    component, the eigenvector ***u1*** so that the information is not redundant.
    It can be proved that all the d components correspond to the d normalized eigenvectors
    from Σ associated with the eigenvalues, in descending order. Take a look at [these
    notes](https://verso.mat.uam.es/~joser.berrendero/blog/Componentes_principales.pdf)
    for formal proof of this claim [2].
  prefs: []
  type: TYPE_NORMAL
- en: PCA in practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From the theoretical description above the steps needed to get the principal
    components of a dataset can be described. Let the initial dataset be a random
    sample of the following 2D normal distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c7e6e739f0b39073b5e8bc0f726f3955.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ad8e48a4a3fd0926f679852a702f589d.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 3\. Original point cloud. Non-centered. From my own authorship.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Centering the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step is to move the cloud to the origin of the coordinate system so
    the data has zero mean. This step is performed by subtracting the sample mean
    from every point in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/924df51be6a24b5d02272354acc8d631.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9bf7e20ac75c23783b3938411fa9d203.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 4\. Centered point cloud. From my own authorship.
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Computing the covariance matrix**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The variance defined above is the population covariance matrix Σ. In practice,
    that information is not available to us as we only have one sample. Therefore,
    we can approximate that parameter by using the sample covariance ***S***.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7008c4cf457e46a38dc01e78392221ab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Recall that the data is already centered. Thus:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/65479fcb3ada247c13134ed263cc147f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can write this compactly using matrix multiplications. This can also help
    us vectorize computations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b0b8be7f69f5d2228c2670327d8cb2c2.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The reason for passing the transposed matrix as the first argument in the code
    is that in the mathematical formulation of the data matrix, the features are in
    the rows and the subjects in the columns. In the implementation, the opposite
    happens since in almost every system, the events, subjects, logs, and so forth,
    are stored in rows.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Perform the eigendecomposition of the covariance matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The eigenvalues and eigenvectors a are computed using `eig()`from [scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.eig.html):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'As it was explained earlier, the eigenvalues represent the variance of the
    principal components, and the eigenvectors are the projection directions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4ab55a58280703a611fc2d5e071fb60c.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 5\. Plot of the projection directions and the point cloud. From my own authorship.
  prefs: []
  type: TYPE_NORMAL
- en: You can see that a new coordinate system is created using the directions of
    the principal components. Furthermore, the eigenvalues and eigenvectors must be
    stored to transform new data afterward.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Enforce determinism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The coefficients of the eigenvectors will be always the same, except for their
    sign. PCA can have multiple valid orientations. Thus, we need to enforce a deterministic
    result by taking the eigenvector matrix and for each of its columns, applying
    the sign of the largest absolute value within that column.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**5\. Extract the new features**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Each new feature (principal component) is extracted by performing the dot product
    between each point in the original feature space and the eigenvector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cab4476710ddca3741d8316199bdf964.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'For this particular example, after computing the components, the new points
    in the space are depicted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d3812f2c2b87edc472180aebe1c7852.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 6\. Plot of the uncorrelated point cloud, using the features from the PCs.
    Notice that it is just a rotation of the original point cloud. From my own authorship.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that this result is basically a rotation of the original cloud of points
    in a way that the attributes are uncorrelated.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Reduce dimensionality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, the principal components were computed in full to understand them, in
    a visual manner. What is left is to choose how many components are needed. We
    resort to the eigenvalues for this task, since they represent the variance of
    each principal component.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ratio of the variance held by component ***i*** is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fd0ce9ccf607931b83cfb40d532858ae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And the ratio of the variance preserved by choosing m components is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3049e97f942dcd6f40fc2bdd6c355584.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we visualize the variance for each component in our example, we arrive at
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/6133d15c87d477e0029c58506409a7d9.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 7\. Variance explained by each principal component. From my own authorship.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, PC1 represents 80% of the variance of the original data, with
    the remaining 20% belonging to PC2\. Furthermore, we can choose to use only the
    first principal component, in which case the data would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/57708ce6e3b3b01b737f82496af65fa5.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 8\. Projection of the data in the direction of the first eigenvector. From
    my own authorship.
  prefs: []
  type: TYPE_NORMAL
- en: This is the projection of the data in the direction of the first eigenvector.
    It does not look very useful right now. **What if we instead choose data that
    belong to three classes? How would PCA look?**
  prefs: []
  type: TYPE_NORMAL
- en: PCA on multiclass data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let us create a dataset with three classes that can be linearly separable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e0c35eadf0360239499ca2689ed87586.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 9\. Multi-class data. From my own authorship.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we apply PCA to the data above, this would be the plot of the principal
    components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/929c5cdaaa9692781531ccc3530afa25.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 10\. Multi-class uncorrelated data. From my own authorship.
  prefs: []
  type: TYPE_NORMAL
- en: 'And this would be the plot of the first component (the projection of the data
    in the direction of the eigenvector corresponding to the largest eigenvalue):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ebee20490b7269ee9f07ef2242881138.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 11\. Projection of the data in the direction of the first eigenvector. This
    is the dimensionality reduction. From my own authorship.
  prefs: []
  type: TYPE_NORMAL
- en: It works! The data still looks easily separable by a linear model.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and disadvantages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like everything in science, there is no silver bullet. Here is a list of advantages
    and disadvantages that you should take into account before using PCA with real-world
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of PCA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Dimensionality Reduction:** PCA allows for the reduction of high-dimensional
    data into a lower-dimensional space while preserving most of the important information.
    This can be useful for data visualization, computational efficiency, and dealing
    with the curse of dimensionality.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Decorrelation:** PCA transforms the original variables into a new set of
    uncorrelated variables called principal components. This decorrelation simplifies
    the analysis and can improve the performance of downstream machine learning algorithms
    that assume independence of features.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Noise Reduction:** The lower-dimensional representation obtained through
    PCA tends to filter out noise and focus on the most significant variations in
    the data. This can enhance the signal-to-noise ratio and improve the robustness
    of subsequent analyses.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Disadvantages of PCA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Linearity Assumption:** PCA assumes that the underlying data relationships
    are linear. If the data has complex non-linear relationships, PCA may not capture
    the most meaningful variations and could provide suboptimal results.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Interpretability:** The principal components obtained from PCA are linear
    combinations of the original features. It can be difficult to relate the principal
    components back to the original variables and understand their exact meaning.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Sensitivity to Scaling:** PCA is sensitive to the scaling of the input variables.
    If the variables have different scales, those with larger variances can dominate
    the analysis, potentially leading to biased results. Proper feature scaling is
    crucial for obtaining reliable results with PCA.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Outliers:** PCA is sensitive to outliers since it focuses on capturing the
    variance in the data. Outliers can significantly influence the principal components
    and distort the results.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Home-brewed PCA implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have covered the details of Principal Component Analysis, all that
    remains is to create a class that encapsulates the aforementioned behavior and
    that could be reused in future problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this implementation, the scikit-learn interface will be used, which has
    the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`fit()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transform()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fit_transform()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: No complex logic is needed. The constructor will just define the number of components
    (features) that the transformed data will have.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The fit method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The fit method will apply steps 1–4 from the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: Centering the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing the covariance matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing eigenvalues, eigenvectors and sorting them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enforcing determinism by flipping the signs of the eigenvectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It will also store the eigenvalues and vectors, as well as the sample mean,
    as object attributes to transform new data later.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The transform method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It will apply steps 1, 5, and 6:'
  prefs: []
  type: TYPE_NORMAL
- en: Centering new data using the stored sample mean
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting the new PC features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing the dimensionality by picking `n_components` dimensions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The fit_transform method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For simplicity of implementation. This method will apply the `fit()` function
    first and `transform()` later. I am sure you can figure out a more clever definition.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Helper functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These methods were defined as separate components, instead of applying all the
    steps in the `fit()`function to make the code more readable and maintainable.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Testing the class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s use the previous example with our `PCA` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ebee20490b7269ee9f07ef2242881138.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig 12\. Results using the PCA class. From my own authorship.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having many features with small data can be harmful and will, most likely, result
    in overfitting. Principal Component Analysis is a tool that can help alleviate
    this problem. It is a dimensionality reduction technique that works by finding
    projection directions for the data in a way that the original variability is preserved
    as much as possible, and the resulting features are uncorrelated. Moreover, the
    variance explained by each new feature, or principal component, can be measured.
    Then, the user can choose how many principal components and how much variance
    is enough for the task. Finally, be sure to know your data first, as PCA works
    with samples that can be linearly separated and can be sensitive to outliers.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Fernández, A. Dimensionality Reduction. Universidad Autónoma de Madrid.
    Madrid, Spain. 2022.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Berrendero, J. R. Regresión lineal con datos de alta dimensión. Universidad
    Autónoma de Madrid. Madrid, Spain. 2022.'
  prefs: []
  type: TYPE_NORMAL
- en: Connect with me on [LinkedIn](https://www.linkedin.com/in/hectorandresmv/)!
  prefs: []
  type: TYPE_NORMAL
