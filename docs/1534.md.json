["```py\nlatent_dim = 2 # dimension of latent space\n\nepochs = 200\nbatch_size = 32\nlearning_rate = 1e-4\n```", "```py\ndef get_default_loss(model, x):\n    with tf.device(device):\n        mean, logvar, z = model.encoder(x)\n        xhat = model.decoder(z)\n        rl = tf.reduce_mean(keras.losses.binary_crossentropy(x, xhat))*28*28\n        kl = tf.reduce_mean(1+logvar-tf.square(mean)-tf.exp(logvar)) * -0.5\n        return rl + kl\n\n'''Sampling layer'''\nclass Sampling(Layer):\n\n    def call(self, prob):\n        # uses reparameterization trick\n        mean, logvar = tf.split(prob, num_or_size_splits=2, axis=1)\n        e = random.normal(shape=(tf.shape(mean)[0], tf.shape(mean)[1]))\n        z = mean + e * tf.exp(logvar * 0.5)\n        return mean, logvar, z\n\n'''Basic Convolutional VAE'''\nclass VAE(Model):\n\n    def __init__(self, latent_dim, **kwargs):\n        super(VAE, self).__init__(**kwargs)\n        self.latent_dim = latent_dim\n        self.encoder = self.get_encoder()\n        self.decoder = self.get_decoder()\n\n    '''encoder + reparametrization (i.e., sampling) layer'''\n    def get_encoder(self):\n        # encoder \n        input_x = Input(shape=(28,28,1))\n        x = Conv2D(filters=64, kernel_size=3, strides=(2,2), activation='relu')(input_x)\n        x = Conv2D(filters=64, kernel_size=3, strides=(2,2), activation='relu')(x)\n        x = Flatten()(x)\n        x = Dense(self.latent_dim * 2)(x)\n        # sampling \n        (mean, logvar, z) = Sampling()(x)\n        return Model(input_x, [mean, logvar, z], name=\"encoder\")\n\n    '''decoder'''\n    def get_decoder(self):\n        input_z = Input(shape=(self.latent_dim,))\n        z = Dense(7*7*64, activation=\"relu\")(input_z)\n        z = Reshape((7, 7, 64))(z)\n        z = Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same', activation='relu')(z)\n        z = Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same', activation='relu')(z)\n        xhat = Conv2DTranspose(filters=1, kernel_size=3, strides=1, padding='same', activation='sigmoid')(z)\n        return  Model(input_z, xhat, name=\"decoder\")\n\n    '''train'''\n    def train_step(self, x):\n        with tf.device(device):\n            x = x[0] if isinstance(x, tuple) else x\n            with tf.GradientTape() as tape:\n                loss = get_default_loss(self, x)\n            gradient = tape.gradient(loss, self.trainable_weights)\n            self.optimizer.apply_gradients(zip(gradient, self.trainable_weights))\n            return {\"loss\": loss}\n\n    def call(self, inputs):\n       pass\n\nvae = VAE(latent_dim=latent_dim)\nvae.compile(optimizer=Adam(learning_rate=learning_rate))\nvae.build(input_shape=(28,28,1))\nvae.summary()\n```", "```py\nlatent_dim = 100\nepochs = 100\nbatch_size = 32\nlearning_rate = 1e-4\n```", "```py\ngenerator = Sequential([\n    # input\n    Dense(units=7*7*128, input_shape=(latent_dim,)),\n    Reshape((7,7,128)),\n    # conv 1\n    Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same'),\n    BatchNormalization(),\n    ReLU(max_value=0.2),\n    # conv 2\n    Conv2DTranspose(filters=64, kernel_size=3, strides=1, padding='same'),\n    BatchNormalization(),\n    ReLU(max_value=0.2),\n    # final tanh    \n    Conv2DTranspose(filters=1, kernel_size=3, strides=2, padding='same', activation='tanh')\n])\n\ndiscriminator = Sequential([\n    # conv 1\n    Conv2D(filters=64, kernel_size=3, strides=2, padding='same', input_shape=(28,28,1)),\n    LeakyReLU(0.2),\n    # conv 2\n    Conv2D(filters=64, kernel_size=3, strides=2, padding='same'),\n    # BatchNormalization(),\n    LeakyReLU(0.2),\n    # output\n    Flatten(),\n    Dense(1, activation='sigmoid')\n])\n\ndiscriminator.compile(optimizer=Adam(learning_rate=learning_rate),\n                      loss=BinaryCrossentropy(), \n                      metrics=[BinaryAccuracy()])\ndiscriminator.trainable = False\ngan = Sequential([\n    generator, \n    discriminator\n])\ngan.compile(optimizer=Adam(learning_rate=learning_rate), loss=BinaryCrossentropy(), metrics=[BinaryAccuracy()])\n```", "```py\ndef generate():\n  # generate image based on random noise z\n    with tf.device(device):\n        random_vector_z = np.random.normal(loc=0, scale=1, size=(16, latent_dim))\n        generated = generator(random_vector_z)\n        return generate\n\n# labels \nreal = np.ones(shape=(batch_size, 1))\nfake = np.zeros(shape=(batch_size, 1))\n\n# generator and discriminator losses\ng_losses, d_losses = [], []\n\nwith tf.device(device_name=device):\n    for epoch in range(epochs):\n        for real_x in x_train_digits:\n            '''discriminator'''\n            # train on real data\n            d_loss_real = discriminator.train_on_batch(x=real_x, y=real)\n\n            # train on fake data\n            z = np.random.normal(loc=0, scale=1, size=(batch_size, latent_dim))\n            fake_x = generator.predict_on_batch(x=z)\n            d_loss_fake = discriminator.train_on_batch(x=fake_x, y=fake)\n\n            # total loss\n            d_loss = np.mean(d_loss_real + d_loss_fake)\n\n            '''generator'''\n            g_loss = gan.train_on_batch(x=z, y=real)\n\n        g_losses.append(g_loss[-1])\n        d_losses.append(d_loss)\n```", "```py\n def call(self, x, time, training=True, **kwargs):\n        with tf.device(device):\n            # front conv\n            x = self.init_conv(x)\n\n            # time embedding\n            t = self.time_mlp(time)\n\n            # move down the encoder\n            h = []\n            for down_block1, down_block2, attention, downsample in self.downs:\n                x = down_block1(x, t)\n                x = down_block2(x, t)\n                x = attention(x)\n                h.append(x) # keep for skip connection!\n                x = downsample(x)\n\n            # bottleneck consists of \n            x = self.mid_block1(x, t) # ResNet block\n            x = self.mid_attn(x) # Attention layer\n            x = self.mid_block2(x, t) # ResNet block\n\n            # move up the decoder\n            for up_block1, up_block2, attention, upsample in self.ups:\n                x = tf.concat([x, h.pop()], axis=-1)\n                x = up_block1(x, t)\n                x = up_block2(x, t)\n                x = attention(x) \n                x = upsample(x)\n            x = tf.concat([x, h.pop()], axis=-1)\n\n            # back conv\n            x = self.final_conv(x)\n            return x\n```", "```py\nimage_size = (32, 32)\nnum_channel = 1\nbatch_size = 64\ntimesteps = 200 \nlearning_rate = 1e-4\nepochs = 10\n```", "```py\n# define forward pass\nbeta = np.linspace(0.0001, 0.02, timesteps) # variance schedule \nalpha = 1 - beta\na = np.concatenate((np.array([1.]), np.cumprod(alpha, 0)[:-1]), axis=0) # alpha bar\n\ndef forward(x_0, t):\n    # uses trick to sample from arbitrary timestep!\n    with tf.device(device):\n        noise_t = np.random.normal(size=x_0.shape)\n        sqrt_a_t = np.reshape(np.take(np.sqrt(a), t), (-1, 1, 1, 1))\n        sqrt_one_minus_a_t = np.reshape(np.take(np.sqrt(1-a), t), (-1, 1, 1, 1))\n        x_t = sqrt_a_t  * x_0 + sqrt_one_minus_a_t  * noise_t\n        return noise_t, x_t\n```", "```py\n unet = Unet() # instantiate model\n\ndef loss(noise, predicted):\n    # remember we just use MSE!\n    with tf.device(device):\n        return tf.math.reduce_mean((noise-predicted)**2)\n\noptimizer = keras.optimizers.Adam(learning_rate=learning_rate)\ndef train_step(train_images):\n    with tf.device(device):\n        # create a \"batch\" number of random timesteps (in our case 64)\n        timestep_values = tf.random.uniform(shape=[train_images.shape[0]], minval=0, maxval=timesteps, dtype=tf.int32)\n\n        # forward \n        noised_images, noise = forward(x_0=train_images, t=timestep_values)\n\n        # set the gradient and get the prediction\n        with tf.GradientTape() as tape:\n            predicted = unet(x=noised_images, time=timestep_values)\n            loss_value = loss(noise, predicted)\n\n        # optimize U-Net using ADAM\n        gradients = tape.gradient(loss_value, unet.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, unet.trainable_variables))\n\n    return loss_value\n\ndef train(epochs):\n    with tf.device(device):\n        for epoch in range(epochs):\n            losses = []\n            for i, batch_images in enumerate(iter(dataset)):\n                loss = train_step(batch_images)\n                losses.append(loss)\n```", "```py\ndef denoise_x(x_t, pred_noise, t):\n    with tf.device(device):\n        # obtain variables\n        alpha_t = np.take(alpha, t)\n        a_t = np.take(a, t)\n\n        # calculate denoised_x (i.e., x_{t-1})\n        beta_t = np.take(beta, t)\n        z = np.random.normal(size=x_t.shape)\n        denoised_x = (1/np.sqrt(alpha_t)) * (x_t - ((1-alpha_t)/np.sqrt(1-a_t))*pred_noise) + np.sqrt(beta_t) * z\n        return denoised_x\n\ndef backward(x, i):\n    with tf.device(device):\n        t = np.expand_dims(np.array(timesteps-i-1, np.int32), 0)\n        pred_noise = unet(x, t)\n        return denoise_x(x, pred_noise, t)\n```", "```py\ndef get_sample(x=None):\n    # generate noise\n    if x is None:\n        x = tf.random.normal(shape=(1,32,32,1))\n\n    # array to store images\n    imgs = [np.squeeze(np.squeeze(x, 0),-1)]\n\n    # backward process\n    for i in tqdm(range(timesteps-1)):\n        x = backward(x, i)\n        if i in [0,25,50,75,100,125,150,175,198]:\n            imgs.append(np.squeeze(np.squeeze(x, 0),-1))\n\n    return imgs if show_progress else imgs[-1]\n```"]