- en: 'Identification: The Key to Credible Causal Inference'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/identification-the-key-to-credible-causal-inference-c3023143349e?source=collection_archive---------5-----------------------#2023-02-22](https://towardsdatascience.com/identification-the-key-to-credible-causal-inference-c3023143349e?source=collection_archive---------5-----------------------#2023-02-22)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Improve your causal IQ and build trust in your causal inference by mastering
    identification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@murat.unal?source=post_page-----c3023143349e--------------------------------)[![Murat
    Unal](../Images/9f00db7597d7ece01213a6b0589c87d8.png)](https://medium.com/@murat.unal?source=post_page-----c3023143349e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c3023143349e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c3023143349e--------------------------------)
    [Murat Unal](https://medium.com/@murat.unal?source=post_page-----c3023143349e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F15a64c9fc55d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fidentification-the-key-to-credible-causal-inference-c3023143349e&user=Murat+Unal&userId=15a64c9fc55d&source=post_page-15a64c9fc55d----c3023143349e---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c3023143349e--------------------------------)
    ·8 min read·Feb 22, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc3023143349e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fidentification-the-key-to-credible-causal-inference-c3023143349e&user=Murat+Unal&userId=15a64c9fc55d&source=-----c3023143349e---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc3023143349e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fidentification-the-key-to-credible-causal-inference-c3023143349e&source=-----c3023143349e---------------------bookmark_footer-----------)![](../Images/a11824c6853cbc713984143488ab3d6b.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Paul Skorupskas](https://unsplash.com/@pawelskor?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'Causal inference is the process by which we use data to make claims about causal
    relationships, thus it is among the core tasks of data scientists. Behind this
    process are two distinct concepts: identification and estimation, and only by
    mastering both can we get better at establishing causality from associations observed
    in the data.'
  prefs: []
  type: TYPE_NORMAL
- en: However, as new estimation methods continue to emerge data scientists tend to
    equate method complexity with strength in causal inference. Unfortunately, without
    a well-defined identification, no amount of sophisticated modeling or estimation
    can help us in establishing causality from data. As such, in this article we will
    discuss in detail why identification has precedence over estimation and why causal
    inference will fail without it.
  prefs: []
  type: TYPE_NORMAL
- en: If you are new to causal inference and/or are more familiar with machine learning
    then you can think identification in causal inference as being the counterpart
    to some of the fundamental concepts in machine learning such as regularization
    and cross validation. Mastering these concepts is essential for succeeding in
    prediction tasks because any algorithm is effective insofar as these concepts
    are applied correctly during training. A simple regularized regression applied
    properly without data leakage can perform better on unseen cases then the state-of-the-art
    algorithm that suffers from overfitting and data leakage.
  prefs: []
  type: TYPE_NORMAL
- en: Another, albeit less obvious, reason for mastering these principles for machine
    learning is to gain trust in our work. When we want to deploy our prediction model,
    one of the first things we do is to convince our stakeholders that our model is
    not merely memorizing what it saw during training but can actually learn from
    data and therefore generalize. We describe in detail how we applied cross-validation,
    how we handled overfitting through regularization and why the test error is a
    reliable estimate of our model’s performance on unseen data. This builds trust
    in our model and we gain the support of our stakeholders.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, in causal inference, identification allows us to not only set the
    stage correctly for estimation but also build trust in our work. As such, to be
    able to conduct an identification analysis and to communicate it clearly is an
    underappreciated yet powerful skill that we must master if we want to improve
    our causal IQ as well as build trust in our causal inference.
  prefs: []
  type: TYPE_NORMAL
- en: Potential outcomes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To understand identification, it is useful to start with the potential outcomes
    framework. Let’s say we are interested in answering whether becoming a Prime member
    causes customers to spend more on Amazon’s online store. Because this is a simple
    case with two treatment conditions we can think about treatment, Prime member
    or not, described by a binary random variable, *Ti=[0,1]*. The outcome we are
    interested in is the value of purchases, say, made within 12 months of joining
    Prime, denoted by *Yi.*
  prefs: []
  type: TYPE_NORMAL
- en: 'To answer this question, we assume we can imagine what might have happened
    to someone who joined Prime if that person had not done so, and vice versa. Therefore,
    for each customer there are two potential outcomes, one if the customer is a member,
    and the other if not. A causal effect is the difference between the two potential
    outcomes, but only one of them is observed. Let *Yi1* denote the potential outcome
    for customer *i* if they are a member and *Yi0* denote the potential outcome for
    customer *i* if they are not*.* The causal effect of Prime membership for customer
    *i* is the difference in the potential outcomes and is defined by:'
  prefs: []
  type: TYPE_NORMAL
- en: Because we never observe *Yi1* and *Yi0* both at the same time, we are faced
    with the**fundamental problem of causal inference**, which simply states that
    causal inference at the individual level is **impossible** [1].
  prefs: []
  type: TYPE_NORMAL
- en: 'The observed outcome for customer *i*, *Yi*, in our data can be connected to
    the potential outcomes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In general we focus on the average treatment effect (ATE), which is the difference
    between the expected values:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Herein lies the challenge: we need the unconditional expectations *E[Yi1]*
    and *E[Yi0]* to obtain the ATE, which is the difference in expected outcomes if
    everyone in the **population** became a Prime member vs not. However, we only
    observe the conditional expectations *E[Yi1|Ti=1]* and *E[Yi0|Ti=0]*, which are
    the expected outcomes among the members and the nonmembers, respectively, we see
    in our data, which is just a **sample** of the population. So, unless we have
    a reason to believe that *E[Yi1|Ti=1]=E[Yi1]* and *E[Yi0|Ti=0]=E[Yi0]*, we can
    not obtain the ATE.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternative way to look at the challenge of causal inference is by decomposing
    the ATE as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fa48dec9e78b135e988ecac191f513d6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the ATE is a function of five quantities, and all we can estimate from
    the observed data are the following three: *P(Ti=1)* using the proportion assigned
    to the treatment condition, *E[Yi1|Ti=1]* using *E[Yi|Ti=1]*, the average outcome
    among members and *E[Yi0|Ti=0]* using *E[Yi|Ti=0]*, the average outcome among
    nonmembers. The other two quantities are *E[Yi0|Ti=1]*, the average outcome under
    control for those in the treatment condition, and *E[Yi1|Ti=0]*, the average outcome
    under treatment for those in the control condition. Notice these are **unobserbed
    counterfactuals**, and we have no way to estimate these two quantities from the
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: Identification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So, how do we proceed? How do we show the conditional expectations are equivalent
    to the unconditional ones and what we obtain by taking their difference is indeed
    the ATE? Or what do we do with the unobserved counterfactuals in the alternate
    expression? The answer is we make **untestable assumptions** and advocate for
    them.
  prefs: []
  type: TYPE_NORMAL
- en: This is exactly where identification comes into the picture. Essentially, identification
    means laying out the assumptions needed for a statistical estimate we obtain from
    the data to be given a causal interpretation. However, it does not stop there.
    It also means making a case for why the assumptions are plausible and therefore
    the association we find in the data **identifies** the causal estimand, i.e. the
    ATE, that we are after and can be trusted as a causal relationship. As such, identification
    forces us to not only make the assumptions needed for causality explicit but also
    defend them in our analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Now, I hope you won’t be disappointed if I told you that every approach for
    causal inference, including randomized experiments, require **untestable assumptions**
    to establish causality. Yup, that’s correct. Even the gold standard of causal
    inference can’t give us causality without making assumptions. The thing is that
    not all assumptions are equal. Some are more plausible than others and when we
    and our audience is clear about the assumptions guiding our causal inference we
    can look for ways to evaluate them.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that untestable assumptions is required for causal inference does not
    mean that it is impossible. It does mean, however, that it is accompanied with
    a high degree of uncertainty and, having a clear identification strategy goes
    a long way in reducing that uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: This also makes it clear why identification has precedence over estimation.
    Simply put, if identification fails, in other words if the assumptions behind
    our causal inference are not plausible, then no modeling or estimation approach
    can take us beyond association. On the other hand, if identification is valid,
    then we can seek to improve estimation by leveraging a variety of tools ranging
    from non-parametric to fully parametric methods.
  prefs: []
  type: TYPE_NORMAL
- en: Bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here is what I mean. Let’s say to find the effect of Prime membership on customer
    purchases at Amazon, I told you that I collected data for historic purchases of
    members and nonmembers and that I will be using it to estimate the ATE. Clearly,
    this means I’m assuming that I can obtain *E[Yi1]* using *E[Yi1|Ti=1]* and *E[Yi0]*
    using *E[Yi0|Ti=0],* whichis my identifying assumption. Now, before looking at
    the analysis or data, we should ask whether this is a plausible assumption.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make a judgment, let’s look at the following decomposition obtained from
    the conditional expectations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0eaf986824d3d5bdb5274c943cfecef1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Well, what we end up with this approach is not the ATE, but an association
    that is the combination of two things: the effect of Prime membership on the members,
    i.e. the average treatment effect on the treated (ATT), and a bias term. Simply
    put, the bias term tells us what the differences in purchases between members
    and nonmembers would have been if members had not joined Prime.'
  prefs: []
  type: TYPE_NORMAL
- en: As in many business contexts, we expect those who voluntarily subscribe for
    a service or product to have different purchase behavior than those who do not.
    In our example, we can think those who join Prime do so because they already use
    Amazon quite often and because they expect to continue to do so, joining Prime
    is a good deal for them. Essentially, members would have higher purchases than
    nonmembers even if they had not joined Prime, suggesting a positive bias, *E[Yi0|Ti=1]>E[Yi0|Ti=0].*
    This means the assumption that I made for causal inference is not valid and we
    can not identify the ATE.
  prefs: []
  type: TYPE_NORMAL
- en: Now, it does not matter how many observations we have in the data or whether
    we use a simple difference in means estimator or run a regression. Because our
    identification is not valid, in the end we will have an association and not a
    causal effect.
  prefs: []
  type: TYPE_NORMAL
- en: '**Identification Strategies**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Great causal works in social science and business are those that have a dedicated
    section where the identification strategy is explicitly described before discussing
    any modeling or estimation. By doing this the minds of these works not only communicate
    confidence in their findings but also convince their audience that their findings
    can be interpreted as causal under the maintained assumptions. Most credible causal
    inferences also don’t require heavy modeling and estimation methods. In fact,
    when a causal inference is credible, it is because most of the challenges have
    been addressed during identification and before the statistical analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, what are the main identification strategies then for causal inference?
    Based on their identifying assumptions they can be classified as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Randomized Experiments
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Natural Experiments
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instrumental Variables
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Regression Discontinuity Designs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Selection on Observables
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Selection on Observables with Temporal Data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Basically, every estimation method that we encounter ranging from a simple difference
    in means to the latest causal machine learning relies on one of these identification
    strategies [2]. For example, the selection on observables strategy encompasses
    everything from regression adjustment to double machine learning and everything
    in between, including every propensity score as well as matching algorithm. Hence,
    for practitioners who want to get better at causal inference, it makes more sense
    to first understand this strategy and its assumptions in detail before moving
    into studying the various estimation methods.
  prefs: []
  type: TYPE_NORMAL
- en: In the next series of articles, we will be doing exactly that starting from
    randomized experiments. We will look into each identification strategy in detail
    by discussing the identifying assumptions behind them.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s conclude this piece by reiterating that causal inference starts with identification
    and the most credible causal inferences are those that have a clear and convincing
    identification strategy, not those that have the most sophisticated estimation
    methods. Data science practitioners who want their causal inferences to be taken
    seriously, need to master identification.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading! I hope you felt it was worth your time.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I strive to write high quality and useful articles for practitioners on methods
    and applications in causal inference as well as marketing data science.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you’re interested in these areas consider following me, and feel invited
    to share your comments/suggestions.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] P. Holland, [Statistics and Causal Inference.](https://www.tandfonline.com/doi/abs/10.1080/01621459.1986.10478354)
    (1986)*, Journal of the American Statistical Association.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] L. Keele, [The Statistics of Causal Inference: A View from Political Methodology.](https://www.cambridge.org/core/journals/political-analysis/article/abs/statistics-of-causal-inference-a-view-from-political-methodology/314EFF877ECB1B90A1452D10D4E24BB3)
    (2015), *Political Analysis.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] A. Lewbel, [The Identification Zoo — Meanings of Identification in Econometrics.](https://www.aeaweb.org/articles?id=10.1257%2Fjel.20181361)
    (2019), *Journal of Economic Literature.*'
  prefs: []
  type: TYPE_NORMAL
