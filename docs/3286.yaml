- en: Forward Pass & Backpropagation In Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前向传播与神经网络中的反向传播
- en: 原文：[https://towardsdatascience.com/forward-pass-backpropagation-neural-networks-101-3a75996ada3b?source=collection_archive---------6-----------------------#2023-11-04](https://towardsdatascience.com/forward-pass-backpropagation-neural-networks-101-3a75996ada3b?source=collection_archive---------6-----------------------#2023-11-04)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/forward-pass-backpropagation-neural-networks-101-3a75996ada3b?source=collection_archive---------6-----------------------#2023-11-04](https://towardsdatascience.com/forward-pass-backpropagation-neural-networks-101-3a75996ada3b?source=collection_archive---------6-----------------------#2023-11-04)
- en: Explaining how neural networks “train” and “learn” patterns in data by hand
    and in code using PyTorch
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释神经网络如何通过手工和代码使用 PyTorch 进行“训练”和“学习”数据中的模式
- en: '[](https://medium.com/@egorhowell?source=post_page-----3a75996ada3b--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----3a75996ada3b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3a75996ada3b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3a75996ada3b--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----3a75996ada3b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@egorhowell?source=post_page-----3a75996ada3b--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----3a75996ada3b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3a75996ada3b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3a75996ada3b--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----3a75996ada3b--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1cac491223b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fforward-pass-backpropagation-neural-networks-101-3a75996ada3b&user=Egor+Howell&userId=1cac491223b2&source=post_page-1cac491223b2----3a75996ada3b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3a75996ada3b--------------------------------)
    ·10 min read·Nov 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3a75996ada3b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fforward-pass-backpropagation-neural-networks-101-3a75996ada3b&user=Egor+Howell&userId=1cac491223b2&source=-----3a75996ada3b---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1cac491223b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fforward-pass-backpropagation-neural-networks-101-3a75996ada3b&user=Egor+Howell&userId=1cac491223b2&source=post_page-1cac491223b2----3a75996ada3b---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3a75996ada3b--------------------------------)
    ·10分钟阅读·2023年11月4日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3a75996ada3b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fforward-pass-backpropagation-neural-networks-101-3a75996ada3b&user=Egor+Howell&userId=1cac491223b2&source=-----3a75996ada3b---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3a75996ada3b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fforward-pass-backpropagation-neural-networks-101-3a75996ada3b&source=-----3a75996ada3b---------------------bookmark_footer-----------)![](../Images/a894bd9e34501e8413c8bf96a918f1e5.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3a75996ada3b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fforward-pass-backpropagation-neural-networks-101-3a75996ada3b&source=-----3a75996ada3b---------------------bookmark_footer-----------)![](../Images/a894bd9e34501e8413c8bf96a918f1e5.png)'
- en: Neural network icons created by juicy_fish — Flaticon. [https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络图标由 juicy_fish 创建 — Flaticon。 [https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network).
- en: Background
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: 'In my past two articles, we dived into the origins of the neural network from
    a single [***perceptron***](https://en.wikipedia.org/wiki/Perceptron)to a large
    interconnected ([***multi-layer perceptron***](https://en.wikipedia.org/wiki/Multilayer_perceptron)
    ***(MLP)***) non-linear optimisation engine. I highly recommend you check my previous
    posts if you are unfamiliar with the perceptron, MLP, and activation functions
    as we will discuss quite a bit in this article:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在我之前的两篇文章中，我们深入探讨了神经网络的起源，从单一的 [***感知器***](https://en.wikipedia.org/wiki/Perceptron)
    到大型互联的 ([***多层感知器***](https://en.wikipedia.org/wiki/Multilayer_perceptron) ***(MLP)***
    ) 非线性优化引擎。如果你对感知器、MLP 和激活函数不太熟悉，我强烈推荐你查看我之前的帖子，因为我们将在这篇文章中讨论这些内容：
- en: '[](https://levelup.gitconnected.com/intro-perceptron-architecture-neural-networks-101-2a487062810c?source=post_page-----3a75996ada3b--------------------------------)
    [## Intro, Perceptron & Architecture: Neural Networks 101'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[***介绍、感知器及架构***](https://levelup.gitconnected.com/intro-perceptron-architecture-neural-networks-101-2a487062810c?source=post_page-----3a75996ada3b--------------------------------)：神经网络
    101'
- en: An introduction to Neural Networks and their building blocks
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络及其构建块的介绍
- en: 'levelup.gitconnected.com](https://levelup.gitconnected.com/intro-perceptron-architecture-neural-networks-101-2a487062810c?source=post_page-----3a75996ada3b--------------------------------)
    [](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=post_page-----3a75996ada3b--------------------------------)
    [## Activation Functions & Non-Linearity: Neural Networks 101'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[***激活函数***](https://levelup.gitconnected.com/intro-perceptron-architecture-neural-networks-101-2a487062810c?source=post_page-----3a75996ada3b--------------------------------)
    和非线性：神经网络 101'
- en: Explaining why neural networks can learn (nearly) anything and everything
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解释为什么神经网络可以学习（几乎）任何和所有的东西
- en: towardsdatascience.com](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=post_page-----3a75996ada3b--------------------------------)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[***前向传播***](https://theneuralblog.com/forward-pass-backpropagation-example/)
    和 [***反向传播***](https://en.wikipedia.org/wiki/Backpropagation) 是两个关键组成部分。让我们深入了解一下吧！'
- en: 'Now it’s time to understand how these neural networks get “trained” and “learn”
    the patterns in the data that you pass into it. There are two key components:
    [***forward pass***](https://theneuralblog.com/forward-pass-backpropagation-example/)
    and [***backpropagation***](https://en.wikipedia.org/wiki/Backpropagation). Let’s
    get into it!'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是了解这些神经网络如何“训练”和“学习”你传递给它的数据中的模式的时候了。
