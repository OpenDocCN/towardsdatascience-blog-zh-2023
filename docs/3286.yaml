- en: Forward Pass & Backpropagation In Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/forward-pass-backpropagation-neural-networks-101-3a75996ada3b?source=collection_archive---------6-----------------------#2023-11-04](https://towardsdatascience.com/forward-pass-backpropagation-neural-networks-101-3a75996ada3b?source=collection_archive---------6-----------------------#2023-11-04)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Explaining how neural networks “train” and “learn” patterns in data by hand
    and in code using PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@egorhowell?source=post_page-----3a75996ada3b--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----3a75996ada3b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3a75996ada3b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3a75996ada3b--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----3a75996ada3b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1cac491223b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fforward-pass-backpropagation-neural-networks-101-3a75996ada3b&user=Egor+Howell&userId=1cac491223b2&source=post_page-1cac491223b2----3a75996ada3b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3a75996ada3b--------------------------------)
    ·10 min read·Nov 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3a75996ada3b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fforward-pass-backpropagation-neural-networks-101-3a75996ada3b&user=Egor+Howell&userId=1cac491223b2&source=-----3a75996ada3b---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3a75996ada3b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fforward-pass-backpropagation-neural-networks-101-3a75996ada3b&source=-----3a75996ada3b---------------------bookmark_footer-----------)![](../Images/a894bd9e34501e8413c8bf96a918f1e5.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Neural network icons created by juicy_fish — Flaticon. [https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network).
  prefs: []
  type: TYPE_NORMAL
- en: Background
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In my past two articles, we dived into the origins of the neural network from
    a single [***perceptron***](https://en.wikipedia.org/wiki/Perceptron)to a large
    interconnected ([***multi-layer perceptron***](https://en.wikipedia.org/wiki/Multilayer_perceptron)
    ***(MLP)***) non-linear optimisation engine. I highly recommend you check my previous
    posts if you are unfamiliar with the perceptron, MLP, and activation functions
    as we will discuss quite a bit in this article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://levelup.gitconnected.com/intro-perceptron-architecture-neural-networks-101-2a487062810c?source=post_page-----3a75996ada3b--------------------------------)
    [## Intro, Perceptron & Architecture: Neural Networks 101'
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to Neural Networks and their building blocks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'levelup.gitconnected.com](https://levelup.gitconnected.com/intro-perceptron-architecture-neural-networks-101-2a487062810c?source=post_page-----3a75996ada3b--------------------------------)
    [](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=post_page-----3a75996ada3b--------------------------------)
    [## Activation Functions & Non-Linearity: Neural Networks 101'
  prefs: []
  type: TYPE_NORMAL
- en: Explaining why neural networks can learn (nearly) anything and everything
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=post_page-----3a75996ada3b--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it’s time to understand how these neural networks get “trained” and “learn”
    the patterns in the data that you pass into it. There are two key components:
    [***forward pass***](https://theneuralblog.com/forward-pass-backpropagation-example/)
    and [***backpropagation***](https://en.wikipedia.org/wiki/Backpropagation). Let’s
    get into it!'
  prefs: []
  type: TYPE_NORMAL
