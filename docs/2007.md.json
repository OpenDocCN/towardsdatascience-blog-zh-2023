["```py\nfrom llama_hub.wikipedia.base import WikipediaReader\n\nloader = WikipediaReader()\ndocuments = loader.load_data(pages=['Berlin', 'Rome', 'Tokyo', 'Canberra', 'Santiago'])\n```", "```py\nfrom llama_index import VectorStoreIndex\nindex = VectorStoreIndex.from_documents(docs)\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"Who is Paul Graham.\")\n```", "```py\n# Source: https://gpt-index.readthedocs.io/en/latest/how_to/index/document_management.html\nfrom llama_index import ListIndex, Document\n\nindex = ListIndex([])\ntext_chunks = ['text_chunk_1', 'text_chunk_2', 'text_chunk_3']\n\ndoc_chunks = []\nfor i, text in enumerate(text_chunks):\n    doc = Document(text, doc_id=f\"doc_id_{i}\")\n    doc_chunks.append(doc)\n\n# insert\nfor doc_chunk in doc_chunks:\n    index.insert(doc_chunk)\n```", "```py\n# Source: https://gpt-index.readthedocs.io/en/latest/examples/usecases/10q_sub_question.html\n\n# Load data\nmarch_2022 = SimpleDirectoryReader(input_files=[\"../data/10q/uber_10q_march_2022.pdf\"]).load_data()\njune_2022 = SimpleDirectoryReader(input_files=[\"../data/10q/uber_10q_june_2022.pdf\"]).load_data()\nsept_2022 = SimpleDirectoryReader(input_files=[\"../data/10q/uber_10q_sept_2022.pdf\"]).load_data()\n# Build indices\nmarch_index = VectorStoreIndex.from_documents(march_2022)\njune_index = VectorStoreIndex.from_documents(june_2022)\nsept_index = VectorStoreIndex.from_documents(sept_2022)\n# Build query engines\nmarch_engine = march_index.as_query_engine(similarity_top_k=3)\njune_engine = june_index.as_query_engine(similarity_top_k=3)\nsept_engine = sept_index.as_query_engine(similarity_top_k=3)\nquery_engine_tools = [\n    QueryEngineTool(\n        query_engine=sept_engine, \n        metadata=ToolMetadata(name='sept_22', description='Provides information about Uber quarterly financials ending September 2022')\n    ),\n    QueryEngineTool(\n        query_engine=june_engine, \n        metadata=ToolMetadata(name='june_22', description='Provides information about Uber quarterly financials ending June 2022')\n    ),\n    QueryEngineTool(\n        query_engine=march_engine, \n        metadata=ToolMetadata(name='march_22', description='Provides information about Uber quarterly financials ending March 2022')\n    ),\n]\n# Run queries \ns_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=query_engine_tools)\nRun queries\nresponse = s_engine.query('Analyze Uber revenue growth over the latest two quarter filings')\n```", "```py\n# Source: https://gpt-index.readthedocs.io/en/latest/use_cases/queries.html#routing-over-heterogeneous-data\nfrom llama_index import TreeIndex, VectorStoreIndex\nfrom llama_index.tools import QueryEngineTool\n# define sub-indices\nindex1 = VectorStoreIndex.from_documents(notion_docs)\nindex2 = VectorStoreIndex.from_documents(slack_docs)\n# define query engines and tools\ntool1 = QueryEngineTool.from_defaults(\nquery_engine=index1.as_query_engine(),\ndescription=\"Use this query engine to do…\",\n)\ntool2 = QueryEngineTool.from_defaults(\nquery_engine=index2.as_query_engine(),\ndescription=\"Use this query engine for something else…\",\n)\nfrom llama_index.query_engine import RouterQueryEngine\nquery_engine = RouterQueryEngine.from_defaults(\nquery_engine_tools=[tool1, tool2]\n)\nresponse = query_engine.query(\n\"In Notion, give me a summary of the product roadmap.\"\n)\n```", "```py\n# Source: https://gpt-index.readthedocs.io/en/latest/examples/query_transformations/HyDEQueryTransformDemo.html\n\n# load documents\ndocuments = SimpleDirectoryReader('llama_index/examples/paul_graham_essay/data').load_data()\nindex = VectorStoreIndex.from_documents(documents)\nquery_str = \"what did paul graham do after going to RISD\"\n\n#Now, we use HyDEQueryTransform to generate a hypothetical document and use it for embedding lookup.\nhyde = HyDEQueryTransform(include_original=True)\nhyde_query_engine = TransformQueryEngine(query_engine, hyde)\nresponse = hyde_query_engine.query(query_str)\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n#In this example, HyDE improves output quality significantly, by hallucinating accurately what Paul Graham did after RISD (see below), and thus improving the embedding quality, and final output.\nquery_bundle = hyde(query_str)\nhyde_doc = query_bundle.embedding_strs[0]\n```", "```py\n# source: https://github.com/jerryjliu/llama_index/blob/main/examples/langchain_demo/LangchainDemo.ipynb\n# Using LlamaIndex as a memory module\nfrom langchain import OpenAI\nfrom langchain.llms import OpenAIChat\nfrom langchain.agents import initialize_agent\n```", "```py\nfrom llama_index import ListIndex\nfrom llama_index.langchain_helpers.memory_wrapper import GPTIndexChatMemory\nindex = ListIndex([])memory = GPTIndexChatMemory(\n    index=index, \n    memory_key=\"chat_history\", \n    query_kwargs={\"response_mode\": \"compact\"},\n    # return_source returns source nodes instead of querying index\n    return_source=True,\n    # return_messages returns context in message format\n    return_messages=True\n)\nllm = OpenAIChat(temperature=0)\n# llm=OpenAI(temperature=0)\nagent_executor = initialize_agent([], llm, agent=\"conversational-react-description\", memory=memory)\n```"]