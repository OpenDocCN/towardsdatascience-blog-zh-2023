["```py\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_blobs\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n```", "```py\nn_classes = 3\n# Make train and test data\nX, y = make_blobs(n_samples=10000, n_features=2, centers=n_classes, cluster_std=3.75, random_state=42)\n\n# Reduce the size of the first class to create an imbalanced dataset\n\n# Set numpy random seed\nnp.random.seed(42)\n# Get index of when y is class 0\nclass_0_idx = np.where(y == 0)[0]\n# Get 30% of the class 0 indices\nclass_0_idx = np.random.choice(class_0_idx, int(len(class_0_idx) * 0.3), replace=False)\n# Get the index for all other classes\nrest_idx = np.where(y != 0)[0]\n# Combine the indices\nidx = np.concatenate([class_0_idx, rest_idx])\n# Shuffle the indices\nnp.random.shuffle(idx)\n# Split the data\nX = X[idx]\ny = y[idx]\n\n# Split off model training set\nX_train, X_rest, y_train, y_rest = train_test_split(X, y, test_size=0.5, random_state=42)\n# Split rest into calibration and test\nX_Cal, X_test, y_cal, y_test = train_test_split(X_rest, y_rest, test_size=0.5, random_state=42)\n\n# Set class labels\nclass_labels = ['blue', 'orange', 'green']\n```", "```py\n# Plot the data\nfig = plt.subplots(figsize=(5, 5))\nax = plt.subplot(111)\nfor i in range(n_classes):\n    ax.scatter(X_test[y_test == i, 0], X_test[y_test == i, 1], label=class_labels[i], alpha=0.5, s=10)\nlegend = ax.legend()\nlegend.set_title(\"Class\")\nax.set_xlabel(\"Feature 1\")\nax.set_ylabel(\"Feature 2\")\nplt.show()\n```", "```py\n# Build and train the classifier\nclassifier = LogisticRegression(random_state=42)\nclassifier.fit(X_train, y_train)\n\n# Test the classifier\ny_pred = classifier.predict(X_test)\naccuracy = np.mean(y_pred == y_test)\nprint(f\"Accuracy: {accuracy:0.3f}\")\n\n# Test recall for each class\nfor i in range(n_classes):\n    recall = np.mean(y_pred[y_test == i] == y_test[y_test == i])\n    print(f\"Recall for class {class_labels[i]}: {recall:0.3f}\")\n```", "```py\nAccuracy: 0.930\nRecall for class blue: 0.772\nRecall for class orange: 0.938\nRecall for class green: 0.969\n```", "```py\n# Get predictions for calibration set\ny_pred = classifier.predict(X_Cal)\ny_pred_proba = classifier.predict_proba(X_Cal)\n\n# Show first 5 instances\ny_pred_proba[0:5]\n```", "```py\narray([[4.65677826e-04, 1.29602253e-03, 9.98238300e-01],\n       [1.73428257e-03, 1.20718182e-02, 9.86193899e-01],\n       [2.51649788e-01, 7.48331668e-01, 1.85434981e-05],\n       [5.97545130e-04, 3.51642214e-04, 9.99050813e-01],\n       [4.54193815e-06, 9.99983628e-01, 1.18300819e-05]])\n```", "```py\nsi_scores = []\n# Loop through all calibration instances\nfor i, true_class in enumerate(y_cal):\n    # Get predicted probability for observed/true class\n    predicted_prob = y_pred_proba[i][true_class]\n    si_scores.append(1 - predicted_prob)    \n\n# Convert to NumPy array\nsi_scores = np.array(si_scores)\n\n# Show first 5 instances\nsi_scores[0:5]\n```", "```py\narray([1.76170035e-03, 1.38061008e-02, 2.51668332e-01, 9.49187344e-04,\n       1.63720201e-05])\n```", "```py\nnumber_of_samples = len(X_Cal)\nalpha = 0.05\nqlevel = (1 - alpha) * ((number_of_samples + 1) / number_of_samples)\nthreshold = np.percentile(si_scores, qlevel*100)\nprint(f'Threshold: {threshold:0.3f}')\n```", "```py\nThreshold: 0.598\n```", "```py\nx = np.arange(len(si_scores)) + 1\nsorted_si_scores = np.sort(si_scores)\nindex_of_95th_percentile = int(len(si_scores) * 0.95)\n\n# Color by cut-off\nconform = 'g' * index_of_95th_percentile\nnonconform = 'r' * (len(si_scores) - index_of_95th_percentile)\ncolor = list(conform + nonconform)\n\nfig = plt.figure(figsize=((6,4)))\nax = fig.add_subplot()\n\n# Add bars\nax.bar(x, sorted_si_scores, width=1.0, color = color)\n\n# Add lines for 95th percentile\nax.plot([0, index_of_95th_percentile],[threshold, threshold], \n        c='k', linestyle='--')\nax.plot([index_of_95th_percentile, index_of_95th_percentile], [threshold, 0],\n        c='k', linestyle='--')\n\n# Add text\ntxt = '95th percentile conformality threshold'\nax.text(5, threshold + 0.04, txt)\n\n# Add axis labels\nax.set_xlabel('Sample instance (sorted by $s_i$)')\nax.set_ylabel('$S_i$ (non-conformality)')\n\nplt.show()\n```", "```py\nprediction_sets = (1 - classifier.predict_proba(X_test) <= threshold)\n# Show first ten instances\nprediction_sets[0:10]\n```", "```py\narray([[ True, False, False],\n       [False, False,  True],\n       [ True, False, False],\n       [False, False,  True],\n       [False,  True, False],\n       [False,  True, False],\n       [False,  True, False],\n       [ True,  True, False],\n       [False,  True, False],\n       [False,  True, False]])\n```", "```py\n# Get standard predictions\ny_pred = classifier.predict(X_test)\n\n# Function to get set labels\ndef get_prediction_set_labels(prediction_set, class_labels):\n    # Get set of class labels for each instance in prediction sets\n    prediction_set_labels = [\n        set([class_labels[i] for i, x in enumerate(prediction_set) if x]) for prediction_set in \n        prediction_sets]\n    return prediction_set_labels\n\n# Collate predictions\nresults_sets = pd.DataFrame()\nresults_sets['observed'] = [class_labels[i] for i in y_test]\nresults_sets['labels'] = get_prediction_set_labels(prediction_sets, class_labels)\nresults_sets['classifications'] = [class_labels[i] for i in y_pred]\nresults_sets.head(10)\n```", "```py\n observed  labels           classifications\n0  blue      {blue}           blue\n1  green     {green}          green\n2  blue      {blue}           blue\n3  green     {green}          green\n4  orange    {orange}         orange\n5  orange    {orange}         orange\n6  orange    {orange}         orange\n7  orange    {blue, orange}   blue\n8  orange    {orange}         orange\n9  orange    {orange}         orange\n```", "```py\n# Plot the data\nfig = plt.subplots(figsize=(5, 5))\nax = plt.subplot(111)\nfor i in range(n_classes):\n    ax.scatter(X_test[y_test == i, 0], X_test[y_test == i, 1],\n               label=class_labels[i], alpha=0.5, s=10)\n# Add instance 7\nset_label = results_sets['labels'].iloc[7]\nax.scatter(X_test[7, 0], X_test[7, 1], color='k', s=100, marker='*', label=f'Instance 7')\nlegend = ax.legend()\nlegend.set_title(\"Class\")\nax.set_xlabel(\"Feature 1\")\nax.set_ylabel(\"Feature 2\")\ntxt = f\"Prediction set for instance 7: {set_label}\"\nax.text(-20, 18, txt)\nplt.show()\n```", "```py\n# Get class counts\ndef get_class_counts(y_test):\n    class_counts = []\n    for i in range(n_classes):\n        class_counts.append(np.sum(y_test == i))\n    return class_counts\n\n# Get coverage for each class\ndef get_coverage_by_class(prediction_sets, y_test):\n    coverage = []\n    for i in range(n_classes):\n        coverage.append(np.mean(prediction_sets[y_test == i, i]))\n    return coverage\n\n# Get average set size for each class\ndef get_average_set_size(prediction_sets, y_test):\n    average_set_size = []\n    for i in range(n_classes):\n        average_set_size.append(\n            np.mean(np.sum(prediction_sets[y_test == i], axis=1)))\n    return average_set_size     \n\n# Get weighted coverage (weighted by class size)\ndef get_weighted_coverage(coverage, class_counts):\n    total_counts = np.sum(class_counts)\n    weighted_coverage = np.sum((coverage * class_counts) / total_counts)\n    weighted_coverage = round(weighted_coverage, 3)\n    return weighted_coverage\n\n# Get weighted set_size (weighted by class size)\ndef get_weighted_set_size(set_size, class_counts):\n    total_counts = np.sum(class_counts)\n    weighted_set_size = np.sum((set_size * class_counts) / total_counts)\n    weighted_set_size = round(weighted_set_size, 3)\n    return weighted_set_size\n```", "```py\nresults = pd.DataFrame(index=class_labels)\nresults['Class counts'] = get_class_counts(y_test)\nresults['Coverage'] = get_coverage_by_class(prediction_sets, y_test)\nresults['Average set size'] = get_average_set_size(prediction_sets, y_test)\nresults\n```", "```py\n Class counts  Coverage   Average set size\nblue    241           0.817427   1.087137\norange  848           0.954009   1.037736\ngreen   828           0.977053   1.016908\n```", "```py\nweighted_coverage = get_weighted_coverage(\n    results['Coverage'], results['Class counts'])\n\nweighted_set_size = get_weighted_set_size(\n    results['Average set size'], results['Class counts'])\n\nprint (f'Overall coverage: {weighted_coverage}')\nprint (f'Average set size: {weighted_set_size}')\n```", "```py\nOverall coverage: 0.947\nAverage set size: 1.035\n```", "```py\n# Set alpha (1 - coverage)\nalpha = 0.05\nthresholds = []\n# Get predicted probabilities for calibration set\ny_cal_prob = classifier.predict_proba(X_Cal)\n# Get 95th percentile score for each class's s-scores\nfor class_label in range(n_classes):\n    mask = y_cal == class_label\n    y_cal_prob_class = y_cal_prob[mask][:, class_label]\n    s_scores = 1 - y_cal_prob_class\n    q = (1 - alpha) * 100\n    class_size = mask.sum()\n    correction = (class_size + 1) / class_size\n    q *= correction\n    threshold = np.percentile(s_scores, q)\n    thresholds.append(threshold)\n\nprint(thresholds)\n```", "```py\n[0.9030202125697161, 0.6317149025299887, 0.26033562285411]\n```", "```py\n# Get Si scores for test set\npredicted_proba = classifier.predict_proba(X_test)\nsi_scores = 1 - predicted_proba\n\n# For each class, check whether each instance is below the threshold\nprediction_sets = []\nfor i in range(n_classes):\n    prediction_sets.append(si_scores[:, i] <= thresholds[i])\nprediction_sets = np.array(prediction_sets).T\n\n# Get prediction set labels and show first 10\nprediction_set_labels = get_prediction_set_labels(prediction_sets, class_labels)\n\n# Get standard predictions\ny_pred = classifier.predict(X_test)\n\n# Collate predictions\nresults_sets = pd.DataFrame()\nresults_sets['observed'] = [class_labels[i] for i in y_test]\nresults_sets['labels'] = get_prediction_set_labels(prediction_sets, class_labels)\nresults_sets['classifications'] = [class_labels[i] for i in y_pred]\n\n# Show first 10 results\nresults_sets.head(10)\n```", "```py\n observed  labels           classifications\n0 blue     {blue}            blue\n1 green    {green}           green\n2 blue     {blue}            blue\n3 green    {green}           green\n4 orange   {orange}          orange\n5 orange   {orange}          orange\n6 orange   {orange}          orange\n7 orange   {blue, orange}    blue\n8 orange   {orange}          orange\n9 orange   {orange}          orange\n```", "```py\nresults = pd.DataFrame(index=class_labels)\nresults['Class counts'] = get_class_counts(y_test)\nresults['Coverage'] = get_coverage_by_class(prediction_sets, y_test)\nresults['Average set size'] = get_average_set_size(prediction_sets, y_test)\nresults\n```", "```py\n Class counts  Coverage   Average set size\nblue    241           0.954357   1.228216\norange  848           0.956368   1.139151\ngreen   828           0.942029   1.006039\n```", "```py\nweighted_coverage = get_weighted_coverage(\n    results['Coverage'], results['Class counts'])\n\nweighted_set_size = get_weighted_set_size(\n    results['Average set size'], results['Class counts'])\n\nprint (f'Overall coverage: {weighted_coverage}')\nprint (f'Average set size: {weighted_set_size}')\n```", "```py\nOverall coverage: 0.95\nAverage set size: 1.093\n```"]