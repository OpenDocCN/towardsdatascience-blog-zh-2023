- en: Say Once! Repeating Words Is Not Helping AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/say-once-repeating-words-is-not-helping-ai-58f38035f66e?source=collection_archive---------9-----------------------#2023-06-20](https://towardsdatascience.com/say-once-repeating-words-is-not-helping-ai-58f38035f66e?source=collection_archive---------9-----------------------#2023-06-20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '| ARTIFICIAL INTELLIGENCE | NLP | LLMs'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How and why is repeating tokens harming LLMs? Why is this a problem?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://salvatore-raieli.medium.com/?source=post_page-----58f38035f66e--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----58f38035f66e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----58f38035f66e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----58f38035f66e--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----58f38035f66e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff1a08d9452cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsay-once-repeating-words-is-not-helping-ai-58f38035f66e&user=Salvatore+Raieli&userId=f1a08d9452cd&source=post_page-f1a08d9452cd----58f38035f66e---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----58f38035f66e--------------------------------)
    ·14 min read·Jun 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F58f38035f66e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsay-once-repeating-words-is-not-helping-ai-58f38035f66e&user=Salvatore+Raieli&userId=f1a08d9452cd&source=-----58f38035f66e---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F58f38035f66e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsay-once-repeating-words-is-not-helping-ai-58f38035f66e&source=-----58f38035f66e---------------------bookmark_footer-----------)![](../Images/40a23dee61acd2fe912d9497b6a07d5f.png)'
  prefs: []
  type: TYPE_NORMAL
- en: image by [Kristina Flour](https://unsplash.com/it/@tinaflour) on Unsplash
  prefs: []
  type: TYPE_NORMAL
- en: '[Large Language Models (LLMs)](https://en.wikipedia.org/wiki/Large_language_model)
    have shown their capabilities and have taken the world by storm. Every big company
    now has a model with a fancy name. But, under the hood, they are all [transformers](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)).
    **Everyone dreams of the trillion parameters, but is there no limit?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we discuss that:'
  prefs: []
  type: TYPE_NORMAL
- en: Is it guaranteed that a bigger model has better performance than a small model?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we have the data for huge models?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What happens if instead of collecting new data you use the data again?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scaling over the sky: what is hurting the wing?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/4c30898f3c8683fc888bf2424f1a20f8.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Sean Pollock](https://unsplash.com/it/@seanpollock) on Unsplash
  prefs: []
  type: TYPE_NORMAL
- en: '[OpenAI has defined the scaling law](https://arxiv.org/abs/2001.08361), stating
    that model performance follows a power law according to how many parameters are
    used and the number of data points. This along with the search for emergent properties
    has created the parameter race: **the bigger the model, the better.**'
  prefs: []
  type: TYPE_NORMAL
