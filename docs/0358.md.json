["```py\nimport time, dateutil\nfrom typing import Tuple, List\nimport numpy as np\nimport pandas as pd\n# Import libraries for reading from partitioned parquet.\nimport pyarrow.parquet as pq\nimport pyarrow.dataset as pds\n# Import forecasting libraries.\nimport prophet\nfrom prophet import Prophet\n# Import Ray's multiprocessing library.\nimport ray\nfrom ray.util.multiprocessing import Pool \nimport tqdm\n```", "```py\n##########\n# STEP 1\\. Define a Python function to read and preprocess a file of data. \n##########\ndef preprocess_data(file_path: str) -> Tuple[pd.DataFrame, np.int32]:\n\n    # Read a single pyarrow parquet S3 file.\n    data = pq.read_table(file_path,\n           filters=[ (\"pickup_location_id\", \"=\", SAMPLE_UNIQUE_ID) ],\n           columns=[ \"pickup_at\", \"pickup_location_id\", TARGET ])\\\n          .to_pandas()\n\n    # Transform data.\n    data[\"ds\"] = data[\"pickup_at\"].dt.to_period(\"D\").dt.to_timestamp()\n    data.rename(columns={TARGET: \"y\"}, inplace=True)\n    data.rename(columns={\"pickup_location_id\": \"unique_id\"}, inplace=True)\n    data.drop(\"pickup_at\", inplace=True, axis=1)\n    unique_id = data[\"unique_id\"][0]\n    return data, unique_id\n\n##########\n# STEP 2\\. Define Python functions to train and evaluate a model on a file of data.\n##########\ndef train_model(file_path: str) -> Tuple[pd.DataFrame, pd.DataFrame, \n    'prophet.forecaster.Prophet', np.int32]:\n\n    # Prepare data from a single S3 file.\n    data, unique_id = preprocess_data(file_path)\n\n    # Split data into train, test.\n    train_end = data.ds.max() - relativedelta(days=FORECAST_LENGTH - 1)\n    train_df = data.loc[(data.ds <= train_end), :].copy()\n    test_df = data.iloc[-(FORECAST_LENGTH):, :].copy()\n\n    # Define Prophet model with 75% confidence interval.\n    model = Prophet(interval_width=0.75, seasonality_mode=\"multiplicative\")      \n\n    # Train and fit Prophet model.\n    model = model.fit(train_df[[\"ds\", \"y\"]])\n    return train_df, test_df, model, unique_id\n\ndef evaluate_model(model: 'prophet.forecaster.Prophet', train: pd.DataFrame, \n    valid: pd.DataFrame, input_value: np.int32) -> Tuple[float, pd.DataFrame]:\n\n    # Inference model using FORECAST_LENGTH.\n    future_dates = model.make_future_dataframe(\n        periods=FORECAST_LENGTH, freq=\"D\")\n    future = model.predict(future_dates)\n\n    # Merge in the actual y-values.\n    future = pd.merge(future, train[['ds', 'y']], on=['ds'], how='left')\n    future = pd.merge(future, valid[['ds', 'y']], on=['ds'], how='left')\n    future['y'] = future.y_x.combine_first(future.y_y)\n    future.drop(['y_x', 'y_y'], inplace=True, axis=1)\n    future['unique_id'] = input_value\n\n    # Calculate mean absolute forecast error.\n    temp = future.copy()\n    temp[\"forecast_error\"] = np.abs(temp[\"yhat\"] - temp[\"y\"])\n    temp.dropna(inplace=True)\n    error = np.mean(temp[\"forecast_error\"])\n    return error, future\n\n############\n# STEP 3\\.  Define a calling function which calls all the above functions,\n#          and will be called in parallel for every data file.\n############\ndef train__and_evaluate(file_path: str) -> Tuple[pd.DataFrame, \n    'prophet.forecaster.Prophet', pd.DataFrame, float, np.int16]:\n\n    # Read S3 file and train a Prophet model.\n    train_df, valid_df, model, unique_id = train_model(file_path)\n\n    # Inference model and evaluate error.\n    error, future = evaluate_model(model, train_df, valid_df, unique_id)\n    return valid_df, model, future, error, unique_id\n```", "```py\nstart = time.time()\n# Create a pool, where each worker is assigned 1 CPU by Ray.\npool = Pool(ray_remote_args={\"num_cpus\": 1})\n\n# Use the pool to run `train_model` on the data, in batches of 1.\niterator = pool.imap_unordered(train__and_evaluate, models_to_train, chunksize=1)\n\n# Track the progress using tqdm and retrieve the results into a list.\nresults = list(tqdm.tqdm(iterator, total=len(models_to_train)))\n\n# Print some training stats.\ntime_ray_multiprocessing = time.time() - start\nprint(f\"Total number of models: {len(results)}\")\nprint(f\"TOTAL TIME TAKEN: {time_ray_multiprocessing/60:.2f} minutes\")\nprint(type(results[0][0]), type(results[0][1]), type(results[0][2]), \n      type(results[0][3]), type(results[0][4]))\n```", "```py\nimport os\nnum_cpu = os.cpu_count()\n# Import another forecasting library.\nimport statsforecast\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import AutoARIMA\n# Import Ray AIR libraries.\nfrom ray import air, tune\nfrom ray.air import session, ScalingConfig\nfrom ray.air.checkpoint import Checkpoint\n\n##########\n# STEP 1\\. Define Python functions to read and prepare a segment of data.\n##########\ndef preprocess_per_uniqueid(\n    s3_files: List[str], sample_location_id: np.int32) -> pd.DataFrame:\n\n    # Load data.\n    df_list = [read_data(f, sample_location_id) for f in s3_files]\n    df_raw = pd.concat(df_list, ignore_index=True)\n\n    # Transform data.\n    df = transform_df(df_raw)\n    df.sort_values(by=\"ds\", inplace=True)\n    return df\n\n##########\n# STEP 2\\. Define Python functions to train and evaluate a model on a segment of data.\n##########\ndef train_prophet(s3_files: List[str], sample_unique_id: np.int32,\n    model_type: str) -> Tuple[pd.DataFrame, pd.DataFrame, \n                        'prophet.forecaster.Prophet', np.int32]:\n\n    # Prepare data from a list of S3 files.\n    data = preprocess_per_uniqueid(s3_files, sample_unique_id)\n\n    # Split data into train, test.\n    train_end = data.ds.max() - relativedelta(days=FORECAST_LENGTH - 1)\n    train_df = data.loc[(data.ds <= train_end), :].copy()\n    test_df = data.iloc[-(FORECAST_LENGTH):, :].copy()\n\n    # Define Prophet model with 75% confidence interval.\n    if model_type == \"prophet_additive\":\n        model = Prophet(interval_width=0.75, seasonality_mode=\"additive\")\n    elif model_type == \"prophet_multiplicative\":\n        model = Prophet(interval_width=0.75, seasonality_mode=\"multiplicative\")     \n\n    # Train and fit Prophet model.\n    model = model.fit(train_df[[\"ds\", \"y\"]])\n    return train_df, test_df, model\n\n# Train an ARIMA model. Full code not shown here.\ndef train_arima(): \n\n# Evaluate an ARIMA model. Full code not shown here.\ndef evaluate_arima(): \n\n############\n# STEP 3\\.  Define a calling function `train_models`, which calls all \n#          the above functions, and will be called in parallel for every \n#          permutation in the Tune search space.\n############\ndef train_models(config: dict) -> None:\n\n    # Get Tune parameters\n    file_list = config['params']['file_list']\n    model_type = config['params']['algorithm']\n    sample_unique_id = config['params']['location']\n\n    # Train model.\n    if model_type == \"arima\":\n        # Train and fit the Prophet model.\n        train_df, valid_df, model = \\\n            train_arima(file_list, sample_unique_id)\n        # Inference model and evaluate error.\n        error, future = \\\n            evaluate_arima(model, valid_df)\n    else:\n        # Train and fit the Prophet model.\n        train_df, valid_df, model = \\\n            train_prophet(file_list, sample_unique_id, model_type)\n        # Inference model and evaluate error.\n        error, future = evaluate_model(model, train_df, valid_df, sample_unique_id)\n\n    # Define a model checkpoint using AIR API.\n    checkpoint = ray.air.checkpoint.Checkpoint.from_dict({\n          \"model\": model,\n          \"valid_df\": valid_df,\n          \"forecast_df\": future,\n          \"location_id\": sample_unique_id,\n        })\n    metrics = dict(error=error)\n    session.report(metrics, checkpoint=checkpoint)\n\n############\n# STEP 4\\. Customize distributed compute scaling.\n############\nnum_training_workers = min(num_cpu - 2, 32)\nscaling_config = ScalingConfig(\n    # Number of distributed workers.\n    num_workers=num_training_workers,\n    # Turn on/off GPU.\n    use_gpu=False,\n    # Specify resources used for trainer.\n    trainer_resources={\"CPU\": 1},\n    # Try to schedule workers on different nodes.\n    placement_strategy=\"SPREAD\")\n\n############\n# STEP 5\\. Define a search space dict of all config parameters.\n############\nsearch_space = {\n    \"scaling_config\": scaling_config,\n    \"params\": {\n        \"file_list\": tune.grid_search([files_to_use]),\n        \"algorithm\": tune.grid_search(algorithms_to_use),\n        \"location\": tune.grid_search(models_to_train),\n    },\n}\n\n# Optional STEP 6\\. Specify the hyperparameter tuning search strategy.\n\n##########\n# STEP 7\\. Run the experiment with Ray AIR APIs.\n##########\n# Define a tuner object.\ntuner = tune.Tuner(\n        train_models,\n        param_space=search_space,\n        tune_config=tune.TuneConfig(\n            metric=\"error\",\n            mode=\"min\",\n        ),\n        run_config=air.RunConfig(\n            # Redirect logs to relative path instead of default ~/ray_results/.\n            local_dir=\"my_Tune_logs\",\n            # Specify name to make logs easier to find in log path.\n            name=\"tune_nyc\",\n        ),\n    )\n# Fit the tuner object.\nresults = tuner.fit()\n```", "```py\n# Import forecasting libraries.\nimport torch\nimport pytorch_lightning as pl\nimport pytorch_forecasting as ptf \nimport tensorboard as tb  \n# Import ray libraries.\nimport ray_lightning\nfrom ray_lightning import RayStrategy\nfrom ray_lightning.tune import get_tune_resources, TuneReportCheckpointCallback\nfrom ray import air, tune\nfrom ray.tune.schedulers import ASHAScheduler\n\n# Define a tuner object.\ntuner = tune.Tuner(\n        tune.with_resources(\n            train_with_parameters,\n            resources=get_tune_resources(num_workers=num_training_workers),\n        ),\n        tune_config=tune.TuneConfig(\n            metric=\"loss\",\n            mode=\"min\",\n            scheduler=scheduler,\n        ),\n        run_config=air.RunConfig(\n            # Redirect logs to relative path instead of default ~/ray_results/.\n            local_dir=\"my_Tune_logs\",\n            # Specify name to make logs easier to find in log path.\n            name=\"ptf_nyc\",\n        ),\n        param_space=FORECAST_CONFIG,\n    )\n\n# Fit the tuner object.\nresults = tuner.fit()\n\n# Get checkpoint for best model from results object, code not shown.\n\n# Plot inference forecasts for some unique_ids.\nsome_unique_ids = [25, 41, 14, 24, 4]\nfor idx in some_unique_ids:\n    best_model.plot_prediction(x, raw_predictions, idx=idx)\n```", "```py\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport pyarrow\nimport pyarrow.parquet as pq\n# Import forecasting libraries. \nimport torch\nimport pytorch_lightning as pl\nimport pytorch_forecasting as ptf\n# Import ray libraries.\nimport ray\nfrom ray import serve\n\n##########\n# STEP 1\\. Instantiate a batch predictor from checkpoint.\n##########\nbatch_predictor = ptf.models.TemporalFusionTransformer.load_from_checkpoint(model_path)\n\n##########\n# STEP 2\\. Create some test data. \n##########\n# Being lazy, pretend the last test data is our out-of-sample test data.\nmax_prediction_length = FORECAST_CONFIG['forecast_horizon']\nnew_prediction_data = df.copy()\nnew_prediction_data[\"time_idx\"] = new_prediction_data[\"time_idx\"] + max_prediction_length\n# Convert data from pandas to PyTorch tensors.\n_, _, test_loader = convert_pandas_pytorch_timeseriesdata(\n    new_prediction_data, FORECAST_CONFIG)\n\n##########\n# STEP 3\\. Define a Ray Serve deployment class.\n##########\n@serve.deployment\nclass ForecastPredictor:\n    def __init__(self, predictor, test_data):\n        self.predictor = predictor\n        self.test_data = test_data\n\n    def predict(self):\n        raw_predictions, x = \\\n          self.predictor.predict(self.test_data, mode=\"raw\", return_x=True)\n        return x, raw_predictions\n\n    def __call__(self):\n        x, raw_predictions = self.predict()\n        return [x, raw_predictions]\n\n##########\n# STEP 4\\. Deploy the predictor.\n##########\n# Bind arguments to the Class constructor.\nmy_first_deployment = ForecastPredictor.bind(\n    predictor=batch_predictor,\n    test_data=test_loader)\n\n##########\n# STEP 5\\. Query the deployment and get the result.\n##########\n# Get handle from serve.run().\nhandle = serve.run(my_first_deployment)\n\n# ray.get() the results from the handle.\nray_return = ray.get(handle.remote())\nnew_x = ray_return[0]\nnew_pred = ray_return[1]\n```"]