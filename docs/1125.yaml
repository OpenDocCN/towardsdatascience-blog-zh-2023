- en: Distributed Hyperparameter Tuning in Vertex AI Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/distributed-hyperparameter-tuning-in-vertex-ai-pipeline-2f3278a1eb64?source=collection_archive---------11-----------------------#2023-03-29](https://towardsdatascience.com/distributed-hyperparameter-tuning-in-vertex-ai-pipeline-2f3278a1eb64?source=collection_archive---------11-----------------------#2023-03-29)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A path to enable the distributed hyperparameter tuning in GCP Vertex AI pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@hangyu_5199?source=post_page-----2f3278a1eb64--------------------------------)[![Hang
    Yu](../Images/feb12ff14af31f9875ea2ad121d5a41e.png)](https://medium.com/@hangyu_5199?source=post_page-----2f3278a1eb64--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2f3278a1eb64--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2f3278a1eb64--------------------------------)
    [Hang Yu](https://medium.com/@hangyu_5199?source=post_page-----2f3278a1eb64--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2665192d75e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistributed-hyperparameter-tuning-in-vertex-ai-pipeline-2f3278a1eb64&user=Hang+Yu&userId=2665192d75e3&source=post_page-2665192d75e3----2f3278a1eb64---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2f3278a1eb64--------------------------------)
    ·10 min read·Mar 29, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2f3278a1eb64&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistributed-hyperparameter-tuning-in-vertex-ai-pipeline-2f3278a1eb64&user=Hang+Yu&userId=2665192d75e3&source=-----2f3278a1eb64---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2f3278a1eb64&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistributed-hyperparameter-tuning-in-vertex-ai-pipeline-2f3278a1eb64&source=-----2f3278a1eb64---------------------bookmark_footer-----------)![](../Images/8b0fddf4a86690a33d7255047da3c52b.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Marsha Reid](https://unsplash.com/@marsha_reid?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vertex AI pipelines offer a handy way to implement end-to-end ML workflows from
    data collection to endpoint monitoring with extremely low effort. For new users,
    the easiness of development and deployment is largely thanks to the [**V**ertex
    AI pipeline example](https://github.com/GoogleCloudPlatform/professional-services/tree/main/examples/vertex_pipeline)
    offered by GCP.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/GoogleCloudPlatform/professional-services/tree/main/examples/vertex_pipeline?source=post_page-----2f3278a1eb64--------------------------------)
    [## professional-services/examples/vertex_pipeline at main · GoogleCloudPlatform/professional-services'
  prefs: []
  type: TYPE_NORMAL
- en: This repository demonstrates end-to-end MLOps process using Vertex AI platform
    and Smart Analytics technology…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/GoogleCloudPlatform/professional-services/tree/main/examples/vertex_pipeline?source=post_page-----2f3278a1eb64--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite the comprehensive demonstration of the essential components, the official
    example also exposes the feasibility for users to customize and enhance the pipeline
    based on their own needs. Amongst all, one of the most exciting components is
    the distributed Hyperparameter Tuning (HPT) that is capable of exploring a huge
    search space to identify the best hyperparameters in a short time. For the time
    being, GCP recommends leveraging **cloudml-hypertune** and **google_cloud_pipeline_components**
    for this purpose and presents the corresponding tutorials:'
  prefs: []
  type: TYPE_NORMAL
- en: '[GCP HPT task tutorial](https://codelabs.developers.google.com/vertex-training-200#0)'
  prefs: []
  type: TYPE_NORMAL
- en: '[## Vertex AI: Distributed hyperparameter tuning | Google Codelabs'
  prefs: []
  type: TYPE_NORMAL
- en: In this lab, you'll learn how to use Vertex AI for hyperparameter tuning and
    distributed training. While this lab uses…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: codelabs.developers.google.com](https://codelabs.developers.google.com/vertex-training-200?source=post_page-----2f3278a1eb64--------------------------------#0)
  prefs: []
  type: TYPE_NORMAL
- en: '[HPT pipeline sample](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/cad623ef84882f410fcc0dc39527be25a5e5f584/notebooks/community/ml_ops/stage3/get_started_with_hpt_pipeline_components.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/cad623ef84882f410fcc0dc39527be25a5e5f584/notebooks/community/ml_ops/stage3/get_started_with_hpt_pipeline_components.ipynb?source=post_page-----2f3278a1eb64--------------------------------)
    [## vertex-ai-samples/get_started_with_hpt_pipeline_components.ipynb at…'
  prefs: []
  type: TYPE_NORMAL
- en: You can't perform that action at this time. You signed in with another tab or
    window. You signed out in another tab or…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/cad623ef84882f410fcc0dc39527be25a5e5f584/notebooks/community/ml_ops/stage3/get_started_with_hpt_pipeline_components.ipynb?source=post_page-----2f3278a1eb64--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: However, the limitation of the tutorials is that the distributed HPT is presented
    as a standalone HPT task/pipeline and it doesn’t explicitly present the approach
    to integrate into the existing Vertex AI pipeline shown in the [**V**ertex AI
    pipeline example](https://github.com/GoogleCloudPlatform/professional-services/tree/main/examples/vertex_pipeline),
    which motivates me to share my successful attempt that bridges the gap. I believe
    this will benefit many businesses who have built or will build their ML workflows
    based on Vertex AI pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '**The major contribution of this blog is the integration of the distributed
    HPT into the Vertex AI pipelines. Specifically, it demonstrates how to:**'
  prefs: []
  type: TYPE_NORMAL
- en: Chain data collection and preprocessing of a Vertex AI pipeline to the distributed
    HPT. In comparison, the [GCP HPT task tutorial](https://codelabs.developers.google.com/vertex-training-200#0)
    and [HPT pipeline sample](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/cad623ef84882f410fcc0dc39527be25a5e5f584/notebooks/community/ml_ops/stage3/get_started_with_hpt_pipeline_components.ipynb)
    simplify data collection and processing via loading a static tensorflow dataset
    in the training step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Optimize HPT results collection to avoid docker arg length limit. In the [HPT
    pipeline sample](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/cad623ef84882f410fcc0dc39527be25a5e5f584/notebooks/community/ml_ops/stage3/get_started_with_hpt_pipeline_components.ipynb),
    the complete HPT results of all trials are encoded as a string that’s passed to
    a docker task as an input argument for further processing. However, the risk is
    that the string might violate the length limit of docker input argument in case
    of a large search space. Thus, a simple solution that combines those two components
    is explored in this article.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the best parameters in firestore. In the [HPT pipeline sample](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/cad623ef84882f410fcc0dc39527be25a5e5f584/notebooks/community/ml_ops/stage3/get_started_with_hpt_pipeline_components.ipynb),
    HPT runs the trials, saves the models, and deploys the best one whereas it’s unclear
    how to visit the best parameters afterwards. This doesn’t suit the scenario that
    HPT and training is expected to be decoupled. Thus, the firestore option is explored
    to save the best hyperparameters for the later training jobs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chain the distributed HPT to the training component and train the model using
    the best parameters. Instead of saving the model of every HPT trial as shown in
    the [HPT pipeline sample](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/cad623ef84882f410fcc0dc39527be25a5e5f584/notebooks/community/ml_ops/stage3/get_started_with_hpt_pipeline_components.ipynb),
    an alternative that re-trains and only saves the best model is explored though
    it’s open to debate if this approach offers a better storage-compute tradeoff
    depending on the specific scenario.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Integrating distributed HPT into Vertex AI pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let’s go through the major steps mentioned above. It is worth noting that
    the ML pipeline demonstrated here is largely based on the [**V**ertex AI pipeline
    example](https://github.com/GoogleCloudPlatform/professional-services/tree/main/examples/vertex_pipeline)
    and only the minimum changes are made to enable HPT. For the purpose of demonstration,
    only two hyperparameters are tuned via grid search as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The jupyter notebook of this work, which contains the end-to-end process to
    deploy the distributed HPT, is hosted in the [repo](https://github.com/simon19891101/professional-services/blob/distributed-hpt-demo/examples/vertex_pipeline/notebook/hpt_pipeline_development.ipynb)
    below.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/simon19891101/professional-services/blob/distributed-hpt-demo/examples/vertex_pipeline/notebook/hpt_pipeline_development.ipynb?source=post_page-----2f3278a1eb64--------------------------------)
    [## professional-services/hpt_pipeline_development.ipynb at distributed-hpt-demo
    ·…'
  prefs: []
  type: TYPE_NORMAL
- en: Common solutions and tools developed by Google Cloud&#39;s Professional Services
    team …
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/simon19891101/professional-services/blob/distributed-hpt-demo/examples/vertex_pipeline/notebook/hpt_pipeline_development.ipynb?source=post_page-----2f3278a1eb64--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Chain data preprocessing to HPT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first challenge I encountered is that the [**vertex-ai-samples**](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/cad623ef84882f410fcc0dc39527be25a5e5f584/notebooks/community/ml_ops/stage3/get_started_with_hpt_pipeline_components.ipynb)tutorialhard
    coded the data collection in the HPT container image that is called by the HyperparameterTuningJobRunOp
    class of google_cloud_pipeline_components.v1.hyperparameter_tuning_job whereas
    in practice we may want to use the data collection and processing pipeline component
    in the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: However, currently the HyperparameterTuningJobRunOp doesn’t support input data
    as an argument, which motivates me to find an alternative way to pass the data
    source. Fortunately, it turns out that HyperparameterTuningJobRunOp consumes worker_pool_specs
    that contains the HPT container specification which supports the input args to
    the HPT container.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Intuitively, it implies the feasibility to pass the input data source as a part
    of the args of the container_spec and it’s validated to be a successful attempt.
  prefs: []
  type: TYPE_NORMAL
- en: The code example of such operation is shown below. Specifically, a new pipeline
    component called worker_pool_specs is created to receive the input_dataset from
    the data processing component and generate the worker_pool_specs that’s passed
    to HyperparameterTuningJobRunOp. In this way, data preprocessing and the core
    HPT module is associated as depicted in the screenshot below. It’s worth noting
    training_data_schema, label, and features are also passed as they are required
    by the training script of [**V**ertex AI pipeline example](https://github.com/GoogleCloudPlatform/professional-services/tree/main/examples/vertex_pipeline).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/98738cee422ae463f6258bdf299f0792.png)'
  prefs: []
  type: TYPE_IMG
- en: Chain preprocess to HPT via worker-pool-specs component
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Optimize HPT results collection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the original [HPT pipeline sample](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/cad623ef84882f410fcc0dc39527be25a5e5f584/notebooks/community/ml_ops/stage3/get_started_with_hpt_pipeline_components.ipynb),
    the output of the HPT module, which is the resource name of the HPT job, is passed
    to the GetTrialsOp module to retrieve all the hyperparameters and their scores
    so as to let the GetBestTrialOp module find the best, as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Currently, GetTrialsOp module encodes the results of all HPT trials into a string
    as presented below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ce5cb406081b5ed85f04d1b48f6af1db.png)'
  prefs: []
  type: TYPE_IMG
- en: Sample output of GetTrialsOp
  prefs: []
  type: TYPE_NORMAL
- en: When the search space is large, one risk observed in practice is that this long
    string may violate the input argument length of the following docker container
    that is GetBestTrialOp.
  prefs: []
  type: TYPE_NORMAL
- en: 'job_spec.worker_pool_specs[0].container_spec.args; Message: Container args
    should contain less than 100k characters'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To avoid this limitation, a hacky but effective approach is attempted though
    there may be some better options. Basically, the source code of GetTrialsOp (see
    [hyperparameter_tuning_job](https://github.com/kubeflow/pipelines/tree/master/components/google-cloud/google_cloud_pipeline_components/experimental/hyperparameter_tuning_job))
    is injected into that of GetBestTrialOp so that those two pipeline components
    merge into one to avoid passing the long string as a docker input.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/256f15b58f9ea020e2e64d578f62aba7.png)'
  prefs: []
  type: TYPE_IMG
- en: GetTrialsOp injected into GetBestTrialOp to be one component
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Save the best parameters in firestore
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the [HPT pipeline sample](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/cad623ef84882f410fcc0dc39527be25a5e5f584/notebooks/community/ml_ops/stage3/get_started_with_hpt_pipeline_components.ipynb),
    each HPT trial saves its trained model and the best one gets deployed later on.
    However, this approach, which couples HPT and model training, exposes some limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: The model deployed is trained during one of the HPT trials. However, not every
    training needs HPT in practice. One such example is the recommender system built
    using matrix factorization. The model needs to be trained frequently using the
    latest user-item interactions but HPT is not always needed. Therefore, the option
    that decouples training and HPT is expected.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploying the model of HPT directly could lead to a biased evaluation as HPT
    is based on the validation data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To this end, instead of saving the trained models, it is preferred to save the
    best HPT result to a database like firestore for later use. After storing the
    best hyperparameters, model training and HPT are decoupled. The best parameters
    can be re-used to train models until a new round of HPT is needed. Besides, model
    evaluation can be improved by adding a seperate test set when a model is trained.
  prefs: []
  type: TYPE_NORMAL
- en: The code below demonstrates how the best HPT result is saved to firestore. Specifically,
    a pipeline component called best_hpt_to_args is defined to consume the best hyperparameters
    found by the GetBestTrialOp step discussed previously. The storage structure of
    firestore is to be decided case by case. Here, the timestamp is used to label
    a HPT pipeline. Lastly, this function returns a string “true”, which is preferred
    by the pipeline condition, to kick off the conditional model training that’s discussed
    later on. The validation accuracy is also logged for the sake of observability
    but this is totally optional.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/33f13156abf2552447f9663849fffefb.png)'
  prefs: []
  type: TYPE_IMG
- en: Firestore example of the saved HPT result
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/186df62776fe928e597cb8a84919dd0f.png)'
  prefs: []
  type: TYPE_IMG
- en: Save best HPT result to firestore
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Train models using the best hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, the HPT is finished. The last improvement I made is appending a conditional
    training task so that the latest HPT so that the best hyperparameters are utilized
    to update the model in production immediately. This step is totally optional and
    subject to the specific use case. It’s worth noting that this condition is receiving
    hpt_op.output that’s a function that wraps all HPT components from worker_pool_specs
    to best_hpt_to_args so its output equals that of best_hpt_to_args. Please see
    the details in the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ba2fe1c2f095bb49182f915fe5ec6400.png)'
  prefs: []
  type: TYPE_IMG
- en: Conditional training
  prefs: []
  type: TYPE_NORMAL
- en: In the training script ([images/training/app.py](https://github.com/simon19891101/professional-services/blob/distributed-hpt-demo/examples/vertex_pipeline/images/training/app.py)),
    a function called get_best_param_values is implemented to collect the latest HPT
    result by querying firestore. Based on different ways to label the HPT pipelines,
    there might be different approaches to collect the HPT result of interest. The
    collected hyperparameters are in the form of a dictionary so they can easily get
    used to train the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Vertex AI pipelines on GCP provides a great platform to productionize ML solutions
    with high performance and flexibility. However, the existing tutorials show a
    limited coverage regarding how distributed HPT can be achieved. To fill up the
    gap, this article demonstrates the successful attempt to integrate the distributed
    GCP HPT module into the existing Vertex AI pipeline. Specifically, four limitations
    that are overlooked by the existing tutorials have been addressed:'
  prefs: []
  type: TYPE_NORMAL
- en: Data input. This would allow users to use the data preprocessed on the fly for
    the purpose of HPT.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: HPT results collection. The optimized result collection enables the capacity
    to explore a large search space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: HPT results storage. Saving HPT results in firestore means training and HPT
    could be decoupled.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model training using the best HPT result. Now we can train new models using
    the saved HPT result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It’s believed that the improvements discussed above would largely benefit the
    industrial use cases of Vertex AI pipelines for the scenarios that need to involve
    a fully automated distributed HPT to optimize the predictive power of the ML solutions
    running in production. For the detailed end-to-end implementation, please visit
    the notebook hosted in the [repo](https://github.com/simon19891101/professional-services/blob/distributed-hpt-demo/examples/vertex_pipeline/notebook/hpt_pipeline_development.ipynb)
    and feel free to get in touch with me on [LinkedIn](https://www.linkedin.com/in/hang-yu-0242ac120002/).
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading!
  prefs: []
  type: TYPE_NORMAL
- en: '*All images unless otherwise noted are by the author.*'
  prefs: []
  type: TYPE_NORMAL
