["```py\nimport gpboost as gpb\nimport pandas as pd\nimport numpy as np\n# Load data\ndata = pd.read_csv(\"https://raw.githubusercontent.com/fabsig/Compare_ML_HighCardinality_Categorical_Variables/master/data/wages.csv.gz\")\n# Partition into training and test data\nn = data.shape[0]\nnp.random.seed(n)\npermute_aux = np.random.permutation(n)\ntrain_idx = permute_aux[0:int(0.8 * n)]\ntest_idx = permute_aux[int(0.8 * n):n]\ndata_train = data.iloc[train_idx]\ndata_test = data.iloc[test_idx]\n# Define fixed effects predictor variables\npred_vars = [col for col in data.columns if col not in ['ln_wage', 'idcode', 't']]\n```", "```py\nlibrary(gpboost)\nlibrary(tidyverse)\n# Load data\ndata <- read_csv(\"https://raw.githubusercontent.com/fabsig/Compare_ML_HighCardinality_Categorical_Variables/master/data/wages.csv.gz\")\ndata <- as.matrix(data) # Convert to matrix since the boosting part does not support data.frames\n# Partition into training and test data\nn <- dim(data)[1]\nset.seed(n)\npermute_aux <- sample.int(n, n)\ntrain_idx <- permute_aux[1:as.integer(0.8*n)]\ntest_idx <- permute_aux[(as.integer(0.8*n)+1):n]\ndata_train <- data[train_idx,]\ndata_test <- data[test_idx,]\n# Define fixed effects predictor variables\npred_vars <- colnames(data)[-which(colnames(data) %in% c(\"ln_wage\", \"idcode\", \"t\"))]\n```", "```py\ngp_model = gpb.GPModel(group_data=data_train['idcode'], likelihood='gaussian')\ndata_bst = gpb.Dataset(data=data_train[pred_vars], label=data_train['ln_wage'])\nparams = {'learning_rate': 0.01, 'max_depth': 2, 'min_data_in_leaf': 10,\n          'lambda_l2': 10, 'num_leaves': 2**10, 'verbose': 0}\nnrounds = 391\ngpbst = gpb.train(params=params, train_set=data_bst, gp_model=gp_model,\n                  num_boost_round=nrounds)\n```", "```py\ngp_model <- GPModel(group_data = data_train[,\"idcode\"], likelihood = \"gaussian\")\ndata_bst <- gpb.Dataset(data = data_train[,pred_vars], label = data_train[,\"ln_wage\"])\nparams <- list(learning_rate = 0.01, max_depth = 2, num_leaves = 2^10,\n               min_data_in_leaf = 10, lambda_l2 = 10)\nnrounds <- 391\ngpbst <- gpb.train(data = data_bst, gp_model = gp_model, \n                   nrounds = nrounds, params = params, verbose = 0)\n```", "```py\ngp_model = gpb.GPModel(group_data=data_train[['idcode','t']], likelihood='gaussian')\ngp_model.set_optim_params(params={\"optimizer_cov\": \"nelder_mead\"})\n```", "```py\ngp_model <- GPModel(group_data = data_train[,c(\"idcode\",\"t\")], likelihood = \"gaussian\")\ngp_model$set_optim_params(params = list(optimizer_cov = \"nelder_mead\"))\n```", "```py\n# Partition training data into inner training data and validation data\nntrain = data_train.shape[0]\nnp.random.seed(ntrain)\npermute_aux = np.random.permutation(ntrain)\ntrain_tune_idx = permute_aux[0:int(0.8 * ntrain)]\nvalid_tune_idx = permute_aux[int(0.8 * ntrain):ntrain]\nfolds = [(train_tune_idx, valid_tune_idx)]\n# Specify parameter grid, gp_model, and gpb.Dataset\nparam_grid = {'learning_rate': [1,0.1,0.01], 'max_depth': [1,2,3,5,10],\n              'min_data_in_leaf': [10,100,1000], 'lambda_l2': [0,1,10]}\nother_params = {'num_leaves': 2**10, 'verbose': 0}\ngp_model = gpb.GPModel(group_data=data_train['idcode'], likelihood='gaussian')\ndata_bst = gpb.Dataset(data=data_train[pred_vars], label=data_train['ln_wage'])\n# Find optimal tuning parameters\nopt_params = gpb.grid_search_tune_parameters(param_grid=param_grid, params=other_params,\n                                             num_try_random=None, folds=folds, seed=1000,\n                                             train_set=data_bst, gp_model=gp_model,\n                                             num_boost_round=1000, early_stopping_rounds=10,\n                                             verbose_eval=1, metric='mse')\nopt_params\n# {'best_params': {'learning_rate': 0.01, 'max_depth': 2, 'min_data_in_leaf': 10, 'lambda_l2': 10}, 'best_iter': 391, 'best_score': 0.08507862825877585}\n```", "```py\n# Partition training data into inner training data and validation data\nntrain <- dim(data_train)[1]\nset.seed(ntrain)\nvalid_tune_idx <- sample.int(ntrain, as.integer(0.2*ntrain))\nfolds <- list(valid_tune_idx)\n# Specify parameter grid, gp_model, and gpb.Dataset\nparam_grid <- list(\"learning_rate\" = c(1,0.1,0.01), \"max_depth\" = c(1,2,3,5,10),\n                   \"min_data_in_leaf\" = c(10,100,1000), \"lambda_l2\" = c(0,1,10))\nother_params <- list(num_leaves = 2^10)\ngp_model <- GPModel(group_data = data_train[,\"idcode\"], likelihood = \"gaussian\")\ndata_bst <- gpb.Dataset(data = data_train[,pred_vars], label = data_train[,\"ln_wage\"])\n# Find optimal tuning parameters\nopt_params <- gpb.grid.search.tune.parameters(param_grid = param_grid, params = other_params,\n                                              num_try_random = NULL, folds = folds,\n                                              data = data_bst, gp_model = gp_model,\n                                              nrounds = 1000, early_stopping_rounds = 10,\n                                              verbose_eval = 1, metric = \"mse\")\nopt_params\n```", "```py\npred = gpbst.predict(data=data_test[pred_vars], group_data_pred=data_test['idcode'], \n                     predict_var=False, pred_latent=False)\ny_pred = pred['response_mean']\nnp.mean((data_test['ln_wage'] - y_pred)**2)\n```", "```py\n## 0.0866730945477676\n```", "```py\npred <- predict(gpbst, data = data_test[,pred_vars], group_data_pred = data_test[,\"idcode\"], \n                predict_var = FALSE, pred_latent = FALSE)\ny_pred <- pred[[\"response_mean\"]]\nmean((data_test[,\"ln_wage\"] - y_pred)^2)\n```", "```py\ngp_model.summary()\n```", "```py\n## =====================================================\n## Covariance parameters (random effects):\n##             Param.\n## Error_term  0.0703\n## idcode      0.0448\n## =====================================================\n```", "```py\nsummary(gp_model)\n```", "```py\nimport shap\nshap_values = shap.TreeExplainer(gpbst).shap_values(data_train[pred_vars])\nfeature_names = [ a + \": \" + str(b) for a,b in zip(pred_vars, np.abs(shap_values).mean(0).round(2)) ]\nshap.summary_plot(shap_values, data_train[pred_vars], max_display=10, feature_names=feature_names)\nshap.dependence_plot(\"ttl_exp\", shap_values, data_train[pred_vars], alpha=0.5)\n```", "```py\nlibrary(SHAPforxgboost)\nlibrary(ggplot2)\nshap.plot.summary.wrap1(gpbst, X = data_train[,pred_vars], top_n=10) + \n  ggtitle(\"SHAP values\")\nshap_long <- shap.prep(gpbst, X_train = data_train[,pred_vars])\nshap.plot.dependence(data_long = shap_long, x = \"ttl_exp\", \n                     color_feature = \"occ_code_7\", smooth = FALSE, size = 2) + \n  ggtitle(\"SHAP dependence plot for ttl_exp\")\n```", "```py\npred_vars_int = ['idcode'] + pred_vars\ngp_model = gpb.GPModel(group_data=data_train['idcode'], likelihood='gaussian')\ndata_bst = gpb.Dataset(data=data_train[pred_vars_int], categorical_feature=[0], \n                       label=data_train['ln_wage'])\nparams = {'learning_rate': 0.01, 'max_depth': 2, 'min_data_in_leaf': 10,\n          'lambda_l2': 1, 'num_leaves': 2**10, 'verbose': 0}\ngpbst = gpb.train(params=params, train_set=data_bst,  gp_model=gp_model,\n                  num_boost_round=482)\npred = gpbst.predict(data=data_test[pred_vars_int], group_data_pred=data_test['idcode'])\nnp.mean((data_test['ln_wage'] - pred['response_mean'])**2)\n```", "```py\n## 0.08616085739486774\n```", "```py\npred_vars_int = c(\"idcode\", pred_vars)\ngp_model <- GPModel(group_data = data_train[,\"idcode\"], likelihood = \"gaussian\")\ndata_bst <- gpb.Dataset(data = data_train[,pred_vars_int], categorical_feature = c(1), \n                        label = data_train[,\"ln_wage\"])\nparams <- list(learning_rate = 0.01, max_depth = 2, num_leaves = 2^10,\n               min_data_in_leaf = 10, lambda_l2 = 1)\ngpbst <- gpb.train(data = data_bst, gp_model = gp_model, \n                   nrounds = 482, params = params, verbose = 0)\npred <- predict(gpbst, data = data_test[,pred_vars_int], group_data_pred = data_test[,\"idcode\"])\nmean((data_test[,\"ln_wage\"] - pred[[\"response_mean\"]])^2)\n```", "```py\nX = data_train[pred_vars]\nX = X.assign(Intercept = 1)\ngp_model = gpb.GPModel(group_data=data_train['idcode'], likelihood=\"gaussian\")\ngp_model.fit(y=data_train['ln_wage'], X=X, params={\"std_dev\": True})\ngp_model.summary()\n```", "```py\n## =====================================================\n## Model summary:\n##  Log-lik      AIC     BIC\n## -6191.97 12493.95 12934.9\n## Nb. observations: 22410\n## Nb. groups: 4567 (idcode)\n## -----------------------------------------------------\n## Covariance parameters (random effects):\n##             Param.  Std. dev.\n## Error_term  0.0794     0.0008\n## idcode      0.0452     0.0014\n## -----------------------------------------------------\n## Linear regression coefficients (fixed effects):\n##              Param.  Std. dev.  z value  P(>|z|)\n## grade        0.0379     0.0025  14.8659   0.0000\n## age          0.0032     0.0013   2.4665   0.0136\n## ttl_exp      0.0310     0.0012  25.4221   0.0000\n## tenure       0.0107     0.0009  11.9256   0.0000\n## not_smsa    -0.1326     0.0079 -16.8758   0.0000\n## south       -0.0884     0.0072 -12.3259   0.0000\n...\n```", "```py\nX = cbind(Intercept = rep(1,dim(data_train)[1]), data_train[,pred_vars])\ngp_model <- fitGPModel(group_data = data_train[,\"idcode\"], X = X, \n                       y = data_train[,\"ln_wage\"], likelihood = \"gaussian\", \n                       params = list(std_dev=TRUE))\nsummary(gp_model)\n```"]