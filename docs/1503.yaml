- en: 'Beyond the Basics: Reinforcement Learning with Jax — Part I: Introduction and
    Core Concepts'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基础知识之外：使用 Jax 的强化学习 — 第一部分：介绍和核心概念
- en: 原文：[https://towardsdatascience.com/beyond-the-basics-reinforcement-learning-with-jax-part-i-introduction-and-core-concepts-4f85f3478f5?source=collection_archive---------11-----------------------#2023-05-02](https://towardsdatascience.com/beyond-the-basics-reinforcement-learning-with-jax-part-i-introduction-and-core-concepts-4f85f3478f5?source=collection_archive---------11-----------------------#2023-05-02)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/beyond-the-basics-reinforcement-learning-with-jax-part-i-introduction-and-core-concepts-4f85f3478f5?source=collection_archive---------11-----------------------#2023-05-02](https://towardsdatascience.com/beyond-the-basics-reinforcement-learning-with-jax-part-i-introduction-and-core-concepts-4f85f3478f5?source=collection_archive---------11-----------------------#2023-05-02)
- en: 'An overview of Reinforcement Learning fundamentals: Markov Decision Processes,
    Policies, and Value Functions'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习基础概述：马尔可夫决策过程、策略和价值函数
- en: '[](https://medium.com/@Lando-L?source=post_page-----4f85f3478f5--------------------------------)[![Lando
    L](../Images/9675667c061daf1d9ac527b49b7a8a9f.png)](https://medium.com/@Lando-L?source=post_page-----4f85f3478f5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4f85f3478f5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4f85f3478f5--------------------------------)
    [Lando L](https://medium.com/@Lando-L?source=post_page-----4f85f3478f5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@Lando-L?source=post_page-----4f85f3478f5--------------------------------)[![Lando
    L](../Images/9675667c061daf1d9ac527b49b7a8a9f.png)](https://medium.com/@Lando-L?source=post_page-----4f85f3478f5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4f85f3478f5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4f85f3478f5--------------------------------)
    [Lando L](https://medium.com/@Lando-L?source=post_page-----4f85f3478f5--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F395c5e41bd1e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-the-basics-reinforcement-learning-with-jax-part-i-introduction-and-core-concepts-4f85f3478f5&user=Lando+L&userId=395c5e41bd1e&source=post_page-395c5e41bd1e----4f85f3478f5---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4f85f3478f5--------------------------------)
    ·12 min read·May 2, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f85f3478f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-the-basics-reinforcement-learning-with-jax-part-i-introduction-and-core-concepts-4f85f3478f5&user=Lando+L&userId=395c5e41bd1e&source=-----4f85f3478f5---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F395c5e41bd1e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-the-basics-reinforcement-learning-with-jax-part-i-introduction-and-core-concepts-4f85f3478f5&user=Lando+L&userId=395c5e41bd1e&source=post_page-395c5e41bd1e----4f85f3478f5---------------------post_header-----------)
    发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4f85f3478f5--------------------------------)
    ·12 分钟阅读·2023年5月2日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f85f3478f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-the-basics-reinforcement-learning-with-jax-part-i-introduction-and-core-concepts-4f85f3478f5&user=Lando+L&userId=395c5e41bd1e&source=-----4f85f3478f5---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f85f3478f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-the-basics-reinforcement-learning-with-jax-part-i-introduction-and-core-concepts-4f85f3478f5&source=-----4f85f3478f5---------------------bookmark_footer-----------)![](../Images/37e98d1c74438a090d291747bfb68fff.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f85f3478f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-the-basics-reinforcement-learning-with-jax-part-i-introduction-and-core-concepts-4f85f3478f5&source=-----4f85f3478f5---------------------bookmark_footer-----------)![](../Images/37e98d1c74438a090d291747bfb68fff.png)'
- en: Diagram created by the author using Miro.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图表由作者使用 Miro 创建。
- en: In this series of blog posts, I am excited to share with you my passion for
    the Reinforcement Learning (RL) paradigm. My mission is to provide an in-depth
    exploration of RL, combining theoretical knowledge with practical examples that
    tackle real-world problems.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一系列博客文章中，我很高兴与您分享我对强化学习（RL）范式的热情。我的使命是提供对 RL 的深入探索，将理论知识与实际例子相结合，以解决现实世界中的问题。
- en: Course details
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 课程详情
- en: Who is this course intended for?
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这个课程适合谁？
- en: This course is for anyone who wishes to learn about RL. It is recommended that
    you have some prior coding experience in Python and be familiar with the use of
    Jupyter Notebooks. Furthermore, it would be beneficial to have a basic understanding
    of neural networks when tackling the chapters in the second part of this course.
    If you lack prior knowledge of neural networks, you can still use them and treat
    them as black box function approximators; however, I suggest taking the time to
    learn more about them before progressing through the latter chapters.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本课程适合任何希望学习RL的人。建议你具有一定的Python编程经验，并且熟悉Jupyter Notebooks的使用。此外，处理本课程第二部分的章节时，拥有基本的神经网络知识会更有帮助。如果你对神经网络没有先前知识，你仍然可以将其视为黑箱函数逼近器来使用；不过，我建议在继续学习后面的章节之前，花时间多了解一下神经网络。
- en: Why should I care about Reinforcement Learning?
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么我应该关心强化学习？
- en: RL is an incredibly exciting field that has seen exponential research growth
    in recent years. Despite the successes that have been achieved, bridging the gap
    between academia and industry is not as straightforward as is the case of other
    Machine Learning fields. RL is often viewed as too impractical for commercial
    applications, but I am here to dispel that notion and arm you with practical knowledge
    of RL methods to bolster your problem-solving toolbox.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: RL是一个极具激动人心的领域，近年来研究增长呈指数级。尽管取得了一些成功，但弥合学术界与工业界之间的差距并不像其他机器学习领域那样直接。RL常被视为商业应用中过于不切实际，但我在这里打破这种观念，并为你提供RL方法的实用知识，以增强你的问题解决工具箱。
- en: What will I learn?
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我会学到什么？
- en: 'This is the first in a series of blog posts where you will:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这是系列博客文章中的第一篇，你将会：
- en: Learn to identify use cases of RL and model your real-world scenarios to fit
    the RL paradigm
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学会识别强化学习（RL）的应用场景，并将你的现实世界情境建模以适应RL范式
- en: Become familiar with the various RL algorithms and understand their advantages
    and limitations
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熟悉各种RL算法，了解它们的优缺点
- en: Understand the principles of exploration and exploitation and how to balance
    them for optimal performance
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解探索与利用的原则，并了解如何平衡它们以实现最佳性能
- en: Analyse and interpret the results of RL experiments
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析和解读RL实验的结果
- en: 'Here is a sneak peak of the techniques we will cover:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们将要涵盖的技术的预览：
- en: Multi-arm bandits
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多臂老虎机问题
- en: Dynamic Programming
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态规划
- en: Monte Carlo Methods
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蒙特卡洛方法
- en: Temporal Difference Learning
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时序差分学习
- en: Deep Q-Networks
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度Q网络
- en: Policy Gradient Methods
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略梯度方法
- en: Actor-Critic Methods
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Actor-Critic方法
- en: Introduction
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'Some of the most significant breakthroughs in artificial intelligence are inspired
    by nature and the RL paradigm is no exception. This simple yet powerful concept
    is closest to how we humans learn and can be seen as an essential element of what
    we would expect from an artificial general intelligence: Learning through trial
    and error. This approach to learning teaches us about cause and effect, and how
    our actions impact our environment. It teaches us how our behaviour can either
    harm or benefit us, and enables us to develop strategies to achieve our long-term
    goals.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能领域的一些重大突破灵感来源于自然，而RL范式也不例外。这个简单却强大的概念最接近我们人类学习的方式，并且可以被看作是我们对人工通用智能所期待的一个重要组成部分：通过试错学习。这种学习方法教会我们因果关系以及我们的行为如何影响环境。它教会我们我们的行为如何对我们产生害处或益处，并使我们能够制定策略以实现我们的长期目标。
- en: What is RL?
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是RL？
- en: The RL paradigm is a powerful and versatile machine learning approach that enables
    decision makers to learn from their interactions with the environment. It draws
    from a wide range of ideas and methodologies for finding an optimal strategy to
    maximize a numerical reward. With a long history of connections to other scientific
    and engineering disciplines, research in RL is well-established. However, while
    there is a wealth of academic success, practical applications of RL in the commercial
    sphere remain rare. The most famous examples of RL in action are computers achieving
    super-human level performance on games such as chess and Go, as well as on titles
    like Atari and Starcraft. In recent years, however, we have seen a growing number
    of industries adopt RL methods.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: RL范式是一种强大而多用途的机器学习方法，能够让决策者通过与环境的互动来学习。它借鉴了广泛的思想和方法来寻找最大化数值奖励的最佳策略。RL具有与其他科学和工程学科的长期联系，研究已非常成熟。然而，尽管学术上取得了丰富的成功，RL在商业领域的实际应用仍然很少。最著名的RL应用例子是计算机在象棋、围棋等游戏以及Atari和Starcraft等游戏中实现超人水平的表现。然而，近年来我们看到越来越多的行业采用RL方法。
- en: How is it used today?
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 现在如何使用它？
- en: 'Despite the low level of commercial adoption of RL, there are some exciting
    applications in the field of:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管RL的商业应用水平较低，但在以下领域有一些令人兴奋的应用：
- en: 'Health: Dynamic treatment regime; automated diagnosis; drug discovery'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 健康：动态治疗方案；自动化诊断；药物发现
- en: 'Finance: Trading; dynamic pricing; risk management'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 财务：交易；动态定价；风险管理
- en: 'Transportation: Adaptive traffic control; autonomous driving'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交通：自适应交通控制；自动驾驶
- en: 'Recommendation: Web search; news recommendation; product recommendation'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐：网页搜索；新闻推荐；产品推荐
- en: 'Natural Language Processing: text summarization; question answering; machine
    translation; dialog generation'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理：文本摘要；问答系统；机器翻译；对话生成
- en: Sometimes less is more
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有时候，少即是多
- en: 'A good way to gain an understanding of RL use cases is to consider an example
    challenge. Let us imagine we are trying to help our friend learn to play a musical
    instrument. Each morning, our friend tells us how motivated they feel and how
    much they have learned during yesterday’s practice, and asks us how they should
    proceed. For reasons unknown to us, our friend has a limited set of studying choices:
    Taking a day off, practicing for one hour, or practicing for three hours.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 了解RL（强化学习）使用案例的好方法是考虑一个挑战示例。假设我们在帮助朋友学习一种乐器。每天早晨，我们的朋友告诉我们他们的动机如何，以及他们在昨天的练习中学到了多少，并询问我们该如何继续。由于我们不清楚的原因，我们的朋友只有有限的学习选择：休息一天、练习一小时或练习三小时。
- en: 'After observing our friend’s progress, we have noticed a few interesting characteristics:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在观察了我们朋友的进展后，我们注意到了一些有趣的特征：
- en: It appears that the progress our friend is making is directly correlated with
    the amount of hours they practice.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 似乎我们朋友的进展与他们练习的小时数直接相关。
- en: Consistent practice sessions make our friend progress faster.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一致的练习使我们的朋友进步更快。
- en: Our friend does not do well with long practicing sessions. Every time we instructed
    them to study for three hours, the next day they felt tired and unmotivated to
    continue.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的朋友不擅长长时间的练习。每当我们指导他们学习三小时，第二天他们总是感到疲倦，不愿继续。
- en: From our observations, we have created a graph modeling their learning progress
    using state machine notation.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的观察，我们使用状态机符号创建了一个图表来建模他们的学习进展。
- en: '![](../Images/dbc51d737a1771bfab31fcad7d871349.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dbc51d737a1771bfab31fcad7d871349.png)'
- en: Diagram created by the author using Miro.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图表由作者使用Miro创建。
- en: 'Let us discuss again our findings based on our model:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次讨论基于模型的发现：
- en: 'Our friend has three distinct emotional states: neutral, motivated, and demotivated.'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的朋友有三种不同的情绪状态：中性、激励和失去动力。
- en: On any given day, they can choose to practice for zero, one, or three hours,
    except when they are feeling demotivated — in which case studying for zero hours
    (or not studying) is their only available option.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在任何一天，他们可以选择练习零、一或三小时，除非他们感到失去动力——在这种情况下，零小时（或不学习）是唯一的选择。
- en: 'Our friend’s mood is predictive: In the neutral state, practicing for one hour,
    will make them feel motivated the following day, while practicing for three hours
    will leave them feeling demotivated und not practicing at all will keep them in
    a neutral state. Conversely, in the motivated state, one hour of practice will
    maintain our friend’s motivation, while three hours of practice will demotivate
    them and no practice at all will leave them feeling neutral. Lastly, in the demotivated
    state, our friend will refrain from studying altogether, resulting in them feeling
    neutral the next day.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们朋友的心情是具有预测性的：在中性状态下，练习一个小时会让他们在第二天感到有动力，而练习三个小时则会使他们感到失去动力，不练习则会保持在中性状态。相反，在有动力的状态下，一个小时的练习会维持我们朋友的动力，而三个小时的练习会使他们感到失去动力，不练习则会使他们感到中性。最后，在失去动力的状态下，我们的朋友会完全停止学习，结果是第二天他们会感到中性。
- en: 'Their progress is heavily influenced by their mood and the amount of practice
    they put in: the more motivated they are and the more hours they dedicate to practice,
    the faster they will learn and grow.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 他们的进展受他们的心情和练习时间的影响很大：他们越有动力，投入的时间越多，他们学习和成长的速度就越快。
- en: Why did we structure our findings like this? Because it helps us model our challenge
    using a mathematical framework called finite *Markov decision processes* (MDPs).
    This approach helps us gain a better understanding of the problem and how to best
    address it.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们要这样构建我们的发现？因为这有助于我们使用一个称为有限*马尔可夫决策过程*（MDP）的数学框架来建模我们的挑战。这种方法帮助我们更好地理解问题以及如何最好地解决它。
- en: Markov Decission Processes
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程
- en: 'Finite MDPs provide a useful framework to model RL problems, allowing us to
    abstract away from the specifics of a given problem and formulate it in a way
    that can be solved using RL algorithms. In doing so, we are able to transfer learnings
    from one problem to another, instead of having to theorise about each problem
    individually. This helps us to simplify the process of solving complex RL problems.
    Formally, a finite MDP is a control process defined by a four-tuple:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 有限MDP提供了一个有用的框架来建模强化学习问题，使我们能够抽象化具体问题，并将其表述为可以用强化学习算法解决的问题。通过这样做，我们能够将从一个问题中获得的知识转移到另一个问题，而不必对每个问题逐一进行理论分析。这有助于我们简化解决复杂强化学习问题的过程。正式地，有限MDP是由一个四元组定义的控制过程：
- en: '![](../Images/935296186b805287707a7ddd8fa813c7.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/935296186b805287707a7ddd8fa813c7.png)'
- en: The four-tuple (*S*, *A*, *P*, *R*) defines four distinct components, each of
    which describes a specific aspect of the system. *S* and *A* define the set of
    *states* and *actions* respectively. Whereas, *P* denotes the *transition function*
    and R denotes the *reward function*. In our example, we define our friend’s mood
    as our set of states *S* and their practice choices as our set of actions *A*.
    The transition function *P*, visualised by arrows in the graph, shows us how our
    friend’s mood will be altered depending on the amount of studying they do. Furthermore,
    the reward function *R* is used to measure the progress our friend has made, which
    is influenced by their mood and the practice choices they make.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 四元组（*S*，*A*，*P*，*R*）定义了四个不同的组件，每个组件描述了系统的一个特定方面。*S*和*A*分别定义了*状态*和*动作*的集合。而*P*表示*转移函数*，R表示*奖励函数*。在我们的例子中，我们将朋友的心情定义为状态集合*S*，他们的练习选择定义为动作集合*A*。转移函数*P*，通过图中的箭头可视化，展示了朋友的心情如何根据他们的学习量而变化。此外，奖励函数*R*用于衡量朋友的进步，这受到他们的心情和练习选择的影响。
- en: Policies and value functions
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略和价值函数
- en: 'Given the MDP, we can now develop strategies for our friend. Drawing on the
    wisdom of our favorite cooking podcast, we are reminded that to master the art
    of cooking one must develop a routine of practicing a little every day. Inspired
    by this idea, we develop a strategy for our friend that advocates for a consistent
    practice schedule: practice for one hour every day. In RL theory, strategies are
    referred to as *policies* or *policy functions*, and are defined as mappings from
    the set of states to the probabilities of each possible action in that state.
    Formally, a policy *π* is a probability distribution over actions *a* given state
    *s.*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 给定MDP，我们现在可以为我们的朋友制定策略。借鉴我们最喜欢的烹饪播客的智慧，我们被提醒要掌握烹饪艺术，必须建立每天练习一点的习惯。受到这一思想的启发，我们为朋友制定了一项策略，提倡坚持练习的计划：每天练习一个小时。在强化学习理论中，策略被称为*策略*或*策略函数*，定义为从状态集合到在该状态下每个可能行动的概率的映射。正式地，策略*π*是给定状态*s*的行动*a*的概率分布。
- en: '![](../Images/dd4ec9c01c2be4d54a70293707f71b1e.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd4ec9c01c2be4d54a70293707f71b1e.png)'
- en: To adhere to the “practice a little every day” mantra, we establish a policy
    with a 100% probability of practicing for one hour in both the neutral and motivated
    states. However, in the demotivated state, we skip practice 100% of the time,
    since it is the only available action. This example demonstrates that policies
    can be deterministic, instead of returning a full probability distribution over
    available actions, they return a degenerate distribution with a single action
    which is taken exclusively.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了遵循“每天练习一点点”的理念，我们建立了一个策略，在中立和有动力状态下都有100%的概率进行一小时的练习。然而，在沮丧状态下，我们100%跳过练习，因为这是唯一可用的动作。这个例子展示了策略可以是确定性的，而不是返回所有可用动作的完整概率分布，而是返回一个只有一个动作的退化分布，该动作被完全执行。
- en: '![](../Images/40749867c48e26c42795da9e81079938.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40749867c48e26c42795da9e81079938.png)'
- en: As much as we trust our favourite cooking podcast, we would like to find out
    how well our friend is doing by following our strategy. In RL lingo we speak of
    evaluating our policy, which is defined by the *value function*. To get a first
    impression, let us calculate how much knowledge our friend is gaining by following
    our strategy for ten days. Assuming they start the practice feeling neutral, they
    will gain one unit of knowledge on the first day and two units of knowledge thereafter,
    resulting in a total of 19 units. Conversely, if our friend had already been motivated
    on the first day, they would have gained 20 units of knowledge and if they had
    started feeling demotivated, they would have gained only 17 units.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们非常信任自己喜欢的烹饪播客，我们仍然希望通过跟随我们的策略来了解朋友的进展。在强化学习的术语中，我们称之为评估我们的策略，这由*价值函数*定义。为了获得初步印象，让我们计算一下朋友在跟随我们策略的十天内获得了多少知识。假设他们在开始练习时感到中立，他们将在第一天获得一个知识单元，之后每天增加两个知识单元，总计为19个单元。相反，如果我们的朋友在第一天已经有了动力，他们将获得20个知识单元；如果他们开始时感到沮丧，他们将仅获得17个知识单元。
- en: While this calculation may seem a little arbitrary at first, there are actually
    a few things we can learn from it. Firstly, we intuitively found a way to assign
    our policy a numerical value. Secondly, we observe that this value depends on
    the mood our friend starts in. That said, let us have a look at the formal definition
    of value functions. A value function *v* of state *s* is defined as the expected
    *discounted return* an agent receives starting in state *s* and following policy
    *π* thereafter. We refer to *v* as the *state-value function* for policy *π.*
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个计算最开始看起来有点随意，但实际上我们可以从中学到一些东西。首先，我们直观地找到了为我们的策略分配数值的方法。其次，我们观察到这个数值依赖于朋友开始时的情绪。也就是说，让我们看看价值函数的正式定义。状态
    *s* 的价值函数 *v* 被定义为代理在状态 *s* 开始并随后遵循策略 *π* 所获得的期望*折现回报*。我们将 *v* 称为策略 *π* 的*状态价值函数*。
- en: '![](../Images/09a1f63ff6a20d4515654c15496accf9.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09a1f63ff6a20d4515654c15496accf9.png)'
- en: 'Where we define the state-value function as the expected value *E* of the discounted
    return *G* when starting in state *s.* As it turns out, our first approach is
    in fact not far off the actual definition. The only difference is that we based
    our calculations on the sum of knowledge gains over a fixed number of days, as
    opposed to the more objective expected discounted return *G*. In RL theory, the
    discounted return is defined as the sum of discounted future rewards:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将状态价值函数定义为在状态 *s* 开始时折现回报 *G* 的期望值 *E*。事实证明，我们的第一个方法实际上与实际定义相差不远。唯一的区别在于，我们的计算基于在固定天数内的知识增益总和，而不是更为客观的期望折现回报
    *G*。在强化学习理论中，折现回报被定义为折现未来奖励的总和：
- en: '![](../Images/8eb810e2a60104bae0a7ef9e3f20fd40.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8eb810e2a60104bae0a7ef9e3f20fd40.png)'
- en: Where *R* denotes the reward at timestep *t* multiplied by the *discount rate*
    denoted by a lowercase gamma. The discount ratelies in the interval of zero to
    one and determines how much value we assign to future rewards. To better understand
    the implication of the discount rate on the sum of rewards let us consider the
    special cases of assigning gamma to zero or to one. By setting gamma to zero,
    we consider only immediate rewards and disregard any future rewards, meaning the
    discounted return would only equal the reward *R* at timestep *t+1*. Conversely,
    when gamma is set to one, we assign any future rewards their full value, thus
    the discounted return would equal the sum of all future rewards.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *R* 表示在时间步 *t* 的奖励，乘以用小写 gamma 表示的 *折扣率*。折扣率在零到一的区间内，决定了我们对未来奖励赋予多少价值。为了更好地理解折扣率对奖励总和的影响，我们可以考虑将
    gamma 设为零或一的特殊情况。将 gamma 设为零时，我们只考虑即时奖励，忽略任何未来奖励，这意味着折扣回报仅等于时间步 *t+1* 的奖励 *R*。相反，当
    gamma 设为一时，我们对未来奖励赋予其全部价值，因此折扣回报将等于所有未来奖励的总和。
- en: 'Equipped with the concept of value functions and discounted returns we can
    now properly evaluate our policy. Firstly, we need to decide on a suitable discount
    rate for our example. We must discard zero as a possible candidate, as it would
    not account for the long-term value of knowledge generation we are interested
    in. A discount rate of one should also be avoided, as our example does not have
    a natural notion of a final state; thus, any policy that includes regular practice
    of the instrument, no matter how ineffective, would yield an infinite amount of
    knowledge with enough time. Hence, chosing a discount rate of one, would make
    us indifferent between having our friend practice every day or once a year. After
    rejecting the special cases of zero and one, we have to choose a suitable discount
    rate between the two. The smaller the discount rate, the less value is assigned
    to future rewards and vice versa. For our example, we set the discount rate to
    0.9 and calculate the discounted returns for each of our friend’s moods. Let us
    start again with the motivated state. Instead of considering only the next ten
    days, we calculate the sum of all discounted future rewards, resulting in 20 units
    of knowledge. The calculation is as follows¹:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 具备了价值函数和折扣回报的概念后，我们现在可以正确评估我们的策略。首先，我们需要为我们的例子选择一个合适的折扣率。我们必须排除零作为可能的候选，因为它不会考虑我们感兴趣的知识生成的长期价值。折扣率为一也应避免，因为我们的例子没有自然的最终状态；因此，任何包含定期练习工具的策略，无论多么无效，只要时间足够，都将产生无限的知识。因此，选择折扣率为一将使我们对让朋友每天练习还是每年练习无动于衷。在排除了零和一的特殊情况后，我们必须选择介于两者之间的合适折扣率。折扣率越小，未来奖励的价值越低，反之亦然。对于我们的例子，我们将折扣率设为0.9，并计算每种情绪的折扣回报。我们再次从动机状态开始。我们不再只考虑接下来的十天，而是计算所有折扣未来奖励的总和，结果为20个知识单位。计算过程如下¹：
- en: '![](../Images/12d147f86d7d6e2ec13559bc20df7157.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12d147f86d7d6e2ec13559bc20df7157.png)'
- en: Note, by introducing a discount rate smaller than one, the sum of an infinite
    number of future rewards is still constant. The next state we wish to analyze
    is the neutral state. In this state, our agent choses to practice for one hour,
    gaining one unit of knowledge, and then transitions to the motivated state. This
    simplifies the calculation process tremendously, as we already know the value
    of the motivated state.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，通过引入小于一的折扣率，无限数量的未来奖励的总和仍然是常数。接下来我们希望分析的是中立状态。在这种状态下，我们的代理选择练习一个小时，获得一个知识单位，然后过渡到动机状态。这极大地简化了计算过程，因为我们已经知道动机状态的价值。
- en: '![](../Images/2ef4b747b82d1870aa680ffda3c53bd6.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ef4b747b82d1870aa680ffda3c53bd6.png)'
- en: As a final step, we can also calculate the value function of the demotivated
    state. The process is analogous to the neutral state, resulting in a value of
    a little over 17.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最终步骤，我们还可以计算失去动机状态的价值函数。这个过程类似于中立状态，结果为略高于17的值。
- en: '![](../Images/023893a2814a64745684f576f69e687f.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/023893a2814a64745684f576f69e687f.png)'
- en: By examining the state-value functions of our policy in all states, we can deduce
    that the motivated state is the most rewarding, which is why we should instruct
    our friend to reach it as quickly as possible and remain there.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查我们策略中所有状态的状态价值函数，我们可以推断出动机状态是最有奖励的，这就是为什么我们应该指示我们的朋友尽快到达这个状态并保持在那里。
- en: I encourage you to consider developing alternative policy functions and evaluating
    them using the state-value function. While some of them may be more successful
    in the short term, they will not generate as much units of knowledge as our proposed
    theory in the long-term². If you want to dig deeper into the math behind MDPs,
    policies and value functions, I highly recommend “Reinforcement Learning — An
    Introduction” by Richard S. Sutton and Andrew G. Barto. Alternatively, I suggest
    checking out the “RL Course by David Silver” on YouTube.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我鼓励你考虑开发替代的策略函数，并使用状态价值函数来评估它们。虽然其中一些可能在短期内更为成功，但从长期来看，它们生成的知识单元不会像我们提出的理论那样多²。如果你想深入了解
    MDP、策略和价值函数背后的数学，我强烈推荐理查德·S·萨顿和安德鲁·G·巴托的《强化学习——导论》。另外，我建议你查看 YouTube 上的“David
    Silver 的 RL 课程”。
- en: What’s next?
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接下来是什么？
- en: What if our friend was not into music, but instead asked us to help them build
    a self-driving car, or our supervisor instructed our team to develop an improved
    recommender system? Unfortunately, discovering the optimal policy for our example
    will not help us much with other RL problems. Therefore, we need to devise algorithms
    that are capable of solving any finite MDP².
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的朋友不对音乐感兴趣，而是请求我们帮助他们构建一辆自动驾驶汽车，或者我们的主管指示我们的团队开发一个改进的推荐系统怎么办？不幸的是，发现我们例子中的最优策略对其他强化学习问题帮助不大。因此，我们需要设计能够解决任何有限
    MDP² 的算法。
- en: In the following blog posts you will explore how to apply various RL algorithms
    to practical examples. We will start with tabular solution methods, which are
    the simplest form of RL algorithms and are suitable for solving MDPs with small
    state and action spaces, such as the one in our example. We will then delve into
    deep learning to tackle more intricate RL problems with arbitrarily large state
    and action spaces, where tabular methods are no longer feasible. These approximate
    solutions methods will be the focus of the second part of this course. Finally,
    to conclude the course, we will cover some of the most innovative papers in the
    field of RL, providing a comprehensive analysis of each one, along with practical
    examples to illustrate the theory.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的博客文章中，你将探索如何将各种强化学习算法应用于实际例子。我们将从表格解法方法开始，这些是最简单的强化学习算法形式，适用于解决具有小状态和动作空间的
    MDP，例如我们例子中的情况。然后我们将深入深度学习，以解决具有任意大状态和动作空间的更复杂的强化学习问题，其中表格方法不再适用。这些近似解法方法将成为本课程第二部分的重点。最后，为了结束课程，我们将介绍一些领域内最具创新性的论文，对每篇论文进行全面分析，并提供实际例子来说明理论。
- en: '[1] In our case, the state-value function is simply equal to the discounted
    return. However, if the environment or the policy were to be nondeterministic,
    then we would need to sum over all the discounted returns, each weighted by their
    likelihood.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 在我们的例子中，状态价值函数仅等于折扣回报。然而，如果环境或策略是非确定性的，那么我们需要对所有折扣回报进行求和，每个回报都按其可能性加权。'
- en: '[2] Proving that the proposed policy is truly optimal in the long run is beyond
    the scope of this blog post. However, I encourage those interested to seek out
    a proof as an exercise. I would start by defining a hypothetical optimal policy,
    then try to demonstrate that the value function of the proposed policy is greater
    than or equal to the value function of the optimal policy for every state of the
    environment.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] 证明所提出的策略在长期内确实是最优的超出了这篇博客文章的范围。然而，我鼓励有兴趣的人将其作为练习来寻找证明。我建议从定义一个假设的最优策略开始，然后尝试证明所提出的策略的价值函数在环境的每个状态下都大于或等于最优策略的价值函数。'
- en: '[3] Not all RL problems fit the mathematically idealised form of finite MDPs.
    Many interesting RL problems require a relaxed formulation. Nevertheless, algorithms
    developed for finite MDPs will often perform well on RL problems that do not fit
    the strict formulation.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] 并不是所有的强化学习问题都符合数学上理想化的有限马尔可夫决策过程（MDP）形式。许多有趣的强化学习问题需要一种放宽的表述。尽管如此，为有限 MDP
    开发的算法往往在不完全符合严格表述的强化学习问题中也能表现良好。'
