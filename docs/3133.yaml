- en: How to Train BERT for Masked Language Modeling Tasks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何训练 BERT 以进行掩码语言建模任务
- en: 原文：[https://towardsdatascience.com/how-to-train-bert-for-masked-language-modeling-tasks-3ccce07c6fdc?source=collection_archive---------0-----------------------#2023-10-17](https://towardsdatascience.com/how-to-train-bert-for-masked-language-modeling-tasks-3ccce07c6fdc?source=collection_archive---------0-----------------------#2023-10-17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-train-bert-for-masked-language-modeling-tasks-3ccce07c6fdc?source=collection_archive---------0-----------------------#2023-10-17](https://towardsdatascience.com/how-to-train-bert-for-masked-language-modeling-tasks-3ccce07c6fdc?source=collection_archive---------0-----------------------#2023-10-17)
- en: Hands-on guide to building language model for MLM tasks from scratch using Python
    and Transformers library
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从头开始使用 Python 和 Transformers 库构建语言模型以进行 MLM 任务的实践指南
- en: '[](https://ransakaravihara.medium.com/?source=post_page-----3ccce07c6fdc--------------------------------)[![Ransaka
    Ravihara](../Images/ac09746938c10ad8f157d46ea0de27ca.png)](https://ransakaravihara.medium.com/?source=post_page-----3ccce07c6fdc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3ccce07c6fdc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3ccce07c6fdc--------------------------------)
    [Ransaka Ravihara](https://ransakaravihara.medium.com/?source=post_page-----3ccce07c6fdc--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ransakaravihara.medium.com/?source=post_page-----3ccce07c6fdc--------------------------------)[![Ransaka
    Ravihara](../Images/ac09746938c10ad8f157d46ea0de27ca.png)](https://ransakaravihara.medium.com/?source=post_page-----3ccce07c6fdc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3ccce07c6fdc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3ccce07c6fdc--------------------------------)
    [Ransaka Ravihara](https://ransakaravihara.medium.com/?source=post_page-----3ccce07c6fdc--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F61b4d96de932&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-bert-for-masked-language-modeling-tasks-3ccce07c6fdc&user=Ransaka+Ravihara&userId=61b4d96de932&source=post_page-61b4d96de932----3ccce07c6fdc---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3ccce07c6fdc--------------------------------)
    ·7 min read·Oct 17, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3ccce07c6fdc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-bert-for-masked-language-modeling-tasks-3ccce07c6fdc&user=Ransaka+Ravihara&userId=61b4d96de932&source=-----3ccce07c6fdc---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F61b4d96de932&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-bert-for-masked-language-modeling-tasks-3ccce07c6fdc&user=Ransaka+Ravihara&userId=61b4d96de932&source=post_page-61b4d96de932----3ccce07c6fdc---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3ccce07c6fdc--------------------------------)
    · 7 分钟阅读 · 2023年10月17日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3ccce07c6fdc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-bert-for-masked-language-modeling-tasks-3ccce07c6fdc&user=Ransaka+Ravihara&userId=61b4d96de932&source=-----3ccce07c6fdc---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3ccce07c6fdc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-bert-for-masked-language-modeling-tasks-3ccce07c6fdc&source=-----3ccce07c6fdc---------------------bookmark_footer-----------)![](../Images/f2df84bc5e99738d8d9c3264785f2390.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3ccce07c6fdc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-bert-for-masked-language-modeling-tasks-3ccce07c6fdc&source=-----3ccce07c6fdc---------------------bookmark_footer-----------)![](../Images/f2df84bc5e99738d8d9c3264785f2390.png)'
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In recent years, large language models(LLMs) have taken all the attention from
    the machine learning community. Before LLMs came in, we had a crucial research
    phase on various language modeling techniques, including masked language modeling,
    causal language modeling, and sequence-to-sequence language modeling.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，大型语言模型（LLMs）吸引了机器学习社区的所有注意。在 LLMs 出现之前，我们在各种语言建模技术上经历了一个关键的研究阶段，包括掩码语言建模、因果语言建模和序列到序列语言建模。
- en: From the above list, masked language models such as BERT became more usable
    in downstream NLP tasks such as classification and clustering. Thanks to libraries
    such as Hugging Face Transformers, adapting these models for downstream tasks
    became more accessible and manageable. Also thanks to the open-source community,
    we have plenty of language models to choose from covering widely used languages
    and domains.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: GitHub repository for this tutorial
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/Ransaka/Train-BERT-for-Masked-Language-Modeling-Tasks?source=post_page-----3ccce07c6fdc--------------------------------)
    [## GitHub - Ransaka/Train-BERT-for-Masked-Language-Modeling-Tasks: GitHub repo
    for TDS article "How to…'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: GitHub repo for TDS article "How to Train BERT for Masked Language Modeling
    Tasks" - GitHub …
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/Ransaka/Train-BERT-for-Masked-Language-Modeling-Tasks?source=post_page-----3ccce07c6fdc--------------------------------)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tune or build one from scratch?
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
