- en: Explore Semantic Relations in Corpora with Embedding Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索语料库中的语义关系与嵌入模型
- en: 原文：[https://towardsdatascience.com/explore-semantic-relations-in-corpora-with-embedding-models-0a6d64c3ec7f?source=collection_archive---------2-----------------------#2023-11-24](https://towardsdatascience.com/explore-semantic-relations-in-corpora-with-embedding-models-0a6d64c3ec7f?source=collection_archive---------2-----------------------#2023-11-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/explore-semantic-relations-in-corpora-with-embedding-models-0a6d64c3ec7f?source=collection_archive---------2-----------------------#2023-11-24](https://towardsdatascience.com/explore-semantic-relations-in-corpora-with-embedding-models-0a6d64c3ec7f?source=collection_archive---------2-----------------------#2023-11-24)
- en: '[](https://medium.com/@power.up1163?source=post_page-----0a6d64c3ec7f--------------------------------)[![Márton
    Kardos](../Images/8c86c5ea10391a0031cdc18bb77b0736.png)](https://medium.com/@power.up1163?source=post_page-----0a6d64c3ec7f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----0a6d64c3ec7f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----0a6d64c3ec7f--------------------------------)
    [Márton Kardos](https://medium.com/@power.up1163?source=post_page-----0a6d64c3ec7f--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@power.up1163?source=post_page-----0a6d64c3ec7f--------------------------------)[![Márton
    Kardos](../Images/8c86c5ea10391a0031cdc18bb77b0736.png)](https://medium.com/@power.up1163?source=post_page-----0a6d64c3ec7f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----0a6d64c3ec7f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----0a6d64c3ec7f--------------------------------)
    [Márton Kardos](https://medium.com/@power.up1163?source=post_page-----0a6d64c3ec7f--------------------------------)'
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6e63b1795236&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplore-semantic-relations-in-corpora-with-embedding-models-0a6d64c3ec7f&user=M%C3%A1rton+Kardos&userId=6e63b1795236&source=post_page-6e63b1795236----0a6d64c3ec7f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----0a6d64c3ec7f--------------------------------)
    ·10 min read·Nov 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F0a6d64c3ec7f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplore-semantic-relations-in-corpora-with-embedding-models-0a6d64c3ec7f&user=M%C3%A1rton+Kardos&userId=6e63b1795236&source=-----0a6d64c3ec7f---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6e63b1795236&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplore-semantic-relations-in-corpora-with-embedding-models-0a6d64c3ec7f&user=M%C3%A1rton+Kardos&userId=6e63b1795236&source=post_page-6e63b1795236----0a6d64c3ec7f---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----0a6d64c3ec7f--------------------------------)
    ·10 min 阅读·2023年11月24日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F0a6d64c3ec7f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplore-semantic-relations-in-corpora-with-embedding-models-0a6d64c3ec7f&user=M%C3%A1rton+Kardos&userId=6e63b1795236&source=-----0a6d64c3ec7f---------------------clap_footer-----------)'
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F0a6d64c3ec7f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplore-semantic-relations-in-corpora-with-embedding-models-0a6d64c3ec7f&source=-----0a6d64c3ec7f---------------------bookmark_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F0a6d64c3ec7f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplore-semantic-relations-in-corpora-with-embedding-models-0a6d64c3ec7f&source=-----0a6d64c3ec7f---------------------bookmark_footer-----------)'
- en: Recently I have talked to a handful of fellow students and scholars who had
    research interests which involved the analysis of free-form text. Unfortunately
    to everyone, gaining meaningful insight to written natural language is not a trivial
    task by any measures. Close reading is of course an option, but you would ideally
    prefer to look at textual data through a more macro-analytical/quantitative lens
    as well. Not to mention that in the age of big data close reading is rarely a
    feasible option.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，我与一些同学和学者讨论了他们研究兴趣涉及自由形式文本分析的话题。遗憾的是，获得对书面自然语言的有意义的见解绝非易事。密切阅读当然是一个选择，但你理想中会希望通过更宏观的分析/量化视角来看待文本数据。更不用说在大数据时代，密切阅读往往不可行。
- en: By far my favorite way to conduct exploratory data analyses on corpora is with
    topic models, and I have written multiple articles about how to go about this
    [in the least painful way possible](https://x-tabdeveloping.github.io/topicwizard/).
    While topic models are awesome, they are not universally the best method for all
    things text.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我最喜欢在语料库上进行探索性数据分析的方法是主题模型，我已经写过多篇文章讨论如何以[尽可能少的痛苦方式](https://x-tabdeveloping.github.io/topicwizard/)进行这项工作。尽管主题模型非常棒，但它们并不是所有文本任务的最佳方法。
- en: Embeddings are numerical representations of textual data, and have become the
    canonical approach for semantic querying of text. In this article we will explore
    some of the ways in which we can explore textual data with the use of embeddings.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是文本数据的数值表示，已经成为文本语义查询的经典方法。在这篇文章中，我们将探讨如何使用嵌入来分析文本数据的一些方法。
- en: Capturing Relations between Concepts with Word Embeddings
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用词嵌入捕捉概念之间的关系
- en: Word embedding models are a set of approaches that learn latent vector representations
    of terms in an unsupervised fashion. When learning word embeddings from natural
    language, one essentially obtains a map of semantic relations in an embedding
    space.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入模型是一组以无监督方式学习术语潜在向量表示的方法。当从自然语言中学习词嵌入时，实际上是获得了一个嵌入空间中的语义关系图。
- en: Word embeddings are typically trained on large corpora so that they can capture
    general word-to-word relations in human language. This is useful, because one
    can infuse general knowledge about language into models for specific applications.
    This is also known as transfer learning, and has been a hot topic in machine learning
    for quite some time.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入通常在大型语料库上进行训练，以便捕捉人类语言中的一般词对词关系。这很有用，因为可以将关于语言的一般知识注入到特定应用的模型中。这也被称为迁移学习，并且在机器学习中一直是一个热门话题。
- en: What if, instead of wanting to transfer general knowledge into a specific model,
    we just want to get a mapping of the semantically specific aspects of a smaller
    corpus? Let’s say that we have a a corpus of comments from a forum and we want
    to explore what kinds of associative relations can be found in them.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不想将一般知识转移到特定模型中，而是希望得到一个较小语料库的语义特定方面的映射，该怎么办呢？假设我们有一个来自论坛的评论语料库，我们想探索其中可以发现哪些关联关系。
- en: One way we may achieve this is by training a word embedding model from scratch
    on this corpus instead of using one that has been pretrained for us. In this example
    I am going to use the 20Newsgroups dataset as the corpus, in which we will explore
    semantic relations.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法是从头开始训练一个词嵌入模型，而不是使用已经为我们预训练的模型。在这个例子中，我将使用20Newsgroups数据集作为语料库，我们将在其中探索语义关系。
- en: Train a Model
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: Now let’s start with a word embedding model. You might be familiar with Word2Vec,
    which is the method that popularized the use of static word embeddings in research
    and practice. On the other hand GloVe, developed by the folks over at Stanford
    seems to be a better method under most circumstances, and my anecdotal experience
    indicates that it gives much higher quality embeddings, especially on smaller
    corpora.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们从一个词嵌入模型开始。你可能对Word2Vec有所了解，它是普及静态词嵌入在研究和实践中的方法。另一方面，由斯坦福大学的团队开发的GloVe在大多数情况下似乎是一种更好的方法，我的经验表明，它提供了更高质量的嵌入，特别是在较小的语料库上。
- en: Unfortunately GloVe is not implemented in Gensim, but luckily I have made a
    fully Gensim compatible interface for the original GloVe code, we are going to
    use this for training the model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，GloVe在Gensim中没有实现，但幸运的是，我为原始GloVe代码制作了一个完全兼容Gensim的接口，我们将使用它来训练模型。
- en: 'Let’s install gensim, glovpy, scikit-learn, so we can fetch 20Newsgroups as
    well as embedding-explorer:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们安装gensim、glovpy和scikit-learn，以便我们可以获取20Newsgroups以及embedding-explorer：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We first have to load the dataset, and tokenize it, for this we will use gensim’s
    built in tokenizer. We are also going to filter out stop words, as they do not
    bear any meaningful information for the task at hand.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要加载数据集，并对其进行标记化，为此我们将使用gensim内置的标记化工具。我们还将过滤掉停用词，因为它们对当前任务没有任何有意义的信息。
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: After this we can easily train a GloVe model on the tokenized corpus.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们可以在标记化的语料库上轻松训练一个GloVe模型。
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We can already query this word embedding model, let’s check for example which
    ten words are closest to “child”.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经可以查询这个词嵌入模型了，举个例子，让我们检查一下哪些十个词最接近“child”。
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Investigate, Visualize!
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调查，可视化！
- en: Individually investigating every words’ relation to other words becomes tedious
    very quickly though. Ideally we would also like to visualize relations, maybe
    even get some networks.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，单独调查每个词与其他词的关系很快会变得乏味。理想情况下，我们也希望可视化关系，甚至可能得到一些网络。
- en: Luckily, the [embedding-explorer](https://centre-for-humanities-computing.github.io/embedding-explorer/#)
    package can help us here, which I have developed. Working in computational humanities
    we often make use of word embedding models and the semantic networks built from
    relations in those models, and embedding-explorer helps us explore these in an
    interactive and visual manner. The package contains multiple interactive web applications,
    we will first look at the “network explorer”.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，[embedding-explorer](https://centre-for-humanities-computing.github.io/embedding-explorer/#)
    包可以帮助我们，我也开发了这个包。在计算人文学科中，我们经常使用词嵌入模型及其建立的语义网络，而 embedding-explorer 帮助我们以互动和可视化的方式探索这些网络。该包包含多个互动网页应用，我们首先来看看“网络探索器”。
- en: The idea behind this app is that concepts in embedding models naturally form
    some sort of network structure. Words that are closely related have strong links,
    while others might not have any. In the app you can build concept graphs based
    on a set of seed words you specify and two levels of free association.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这个应用的想法是，嵌入模型中的概念自然形成某种网络结构。相关性强的词有强链接，而其他词可能没有。在应用中，您可以基于指定的一组种子词和两个自由联想级别构建概念图。
- en: At each level of association we take the five closest words in the embedding
    model to the words we already have and we add the to our network with a connection
    to the word it was associated to. The strength of the connection is determined
    by the cosine distance of concepts in embedding space. These kinds of networks
    have proven useful for multiple research projects me or my colleagues have worked
    on.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个关联级别，我们从嵌入模型中找出与已有词汇最接近的五个词，并将其添加到我们的网络中，与其关联的词相连。连接的强度由嵌入空间中概念的余弦距离决定。这类网络在我或我的同事进行的多个研究项目中都证明了其有用性。
- en: Let’s start up the app on our word embedding model.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们启动我们的词嵌入模型应用。
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This will open a browser window where you can freely explore the semantic relations
    in the corpus. Here is a screenshot of me looking at what networks arise around
    the words “jesus”, “science” and “religion”.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打开一个浏览器窗口，您可以自由探索语料库中的语义关系。这里是我查看围绕“jesus”，“science”和“religion”这些词汇形成的网络的截图。
- en: '![](../Images/4eaca92387bf13d4e0e9267ed4f07729.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4eaca92387bf13d4e0e9267ed4f07729.png)'
- en: Exploring Semantic Relations in our GloVe Model
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 探索我们 GloVe 模型中的语义关系
- en: We can for example see that the way people talk about these subjects online
    seems to suggest that religion and science seem to relate through politics, society
    and philosophy, which makes a lot of sense. It is also interesting to observe
    how education is somewhere mid-way between science and religion, but is clearly
    more connected to science. This would be interesting to explore in more detail.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以看到，人们在线讨论这些话题时，似乎暗示了宗教和科学通过政治、社会和哲学相关联，这非常有道理。观察到教育在科学和宗教之间，虽然明显更接近科学，这也很有趣。这将值得进一步探讨。
- en: Networks of N-grams with Sentence Transformers
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: N-grams 与句子变换器的网络
- en: Now what if we not only want to look at word level relations, but phrases or
    sentences?
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如果我们不仅想查看词级别的关系，还想查看短语或句子呢？
- en: 'My suggestion is to use N-grams. N-grams are essentially just N terms that
    follow each other in text. For example in the sentence “I love my little cute
    dog” we would have 4-grams: “I love my little”, “love my little cute” and “my
    little cute dog”. Now the question is, how do we learn good semantic representations
    of N-grams?'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我的建议是使用 N-grams。N-grams 本质上就是文本中连续的 N 个术语。例如，在句子“I love my little cute dog”中，我们将得到
    4-grams：“I love my little”，“love my little cute”和“my little cute dog”。现在的问题是，我们如何学习
    N-grams 的良好语义表示？
- en: You could technically still do this with GloVe by treating a phrase or sentence
    as a token, but there is a catch. Since the variety of N-grams increases drastically
    with N, a particular N-gram might only occur once or twice, and we might not be
    able to learn good representations of them.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 技术上，你仍然可以通过将短语或句子视为一个标记来使用 GloVe，但有一个问题。由于 N-grams 的多样性随着 N 的增加而急剧增加，某些 N-grams
    可能只出现一两次，我们可能无法学习到良好的表示。
- en: How about taking the mean of word embeddings in the phrase? Well this could
    go a long way, but the problem is that we completely lose all information about
    the importance of different words, their order in the sentence and all contextual
    information as well.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 取短语中词嵌入的平均值怎么样？这可能会有很大帮助，但问题是我们完全丧失了关于不同词的重要性、它们在句子中的顺序以及所有上下文信息。
- en: The solution to this issue is to use [sentence transformers](https://www.sbert.net/),
    deep neural language models that produce contextually sensitive representations
    of text. They have [outperformed all other approaches](https://huggingface.co/blog/mteb)
    for a few years now, and have become the industry standard for embedding text.
    Now training such a model takes a lot of data, that we do not have at hand, but
    luckily we can use a handful of good pretrained models.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 解决此问题的方案是使用[句子变换器](https://www.sbert.net/)，这些深度神经语言模型产生具有上下文敏感性的文本表示。它们已经[超越了所有其他方法](https://huggingface.co/blog/mteb)好几年，并成为了嵌入文本的行业标准。现在训练这样的模型需要大量的数据，我们手头没有，但幸运的是，我们可以使用一些优秀的预训练模型。
- en: N-gram Extraction
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: N-gram 提取
- en: Let us first extract N-grams from our corpus. I chose to go with four-grams,
    but you can choose any number you would like. We are going to use scikit-learn’s
    CountVectorizer for doing this.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们从语料库中提取 N-gram。我选择了四元组，但你可以选择任何你喜欢的数量。我们将使用 scikit-learn 的 CountVectorizer
    来完成这项工作。
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Embedding Model
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入模型
- en: We will need an embedding model for text representation. As I said earlier,
    we are going to use a pretrained model. I chose [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)
    as it is very stable, widely used and is quite small, so it will probably run
    smoothly even on your personal computer.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个嵌入模型来表示文本。正如我之前所说，我们将使用一个预训练模型。我选择了[all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)，因为它非常稳定、广泛使用且相当小巧，因此即使在你的个人电脑上也能顺畅运行。
- en: We will use yet another package, [embetter](https://github.com/koaning/embetter),
    so that we can use sentence transformers in a scikit-learn compatible manner.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用另一个包，[embetter](https://github.com/koaning/embetter)，以便可以以与 scikit-learn
    兼容的方式使用句子变换器。
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can load the model in Python like this:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像这样在 Python 中加载模型：
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Explore!
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索！
- en: We can then load the model and the n-grams into embedding-explorer.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将模型和 N-gram 加载到 embedding-explorer 中。
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note that this allows us to specify any arbitrary seed instead of just the ones
    that are in our vocabulary of four-grams. Here’s a screenshot of me putting in
    two sentences and seeing what kinds of networks are built from the four grams
    around them.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这允许我们指定任何任意种子，而不仅仅是我们四元组词汇表中的种子。这里是我输入两个句子的截图，并查看从它们周围的四元组构建了什么样的网络。
- en: '![](../Images/3db5a5c43e29284b149aa94c94740de6.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3db5a5c43e29284b149aa94c94740de6.png)'
- en: Exploring Phrases and Sentences in the Corpus
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 探索语料库中的短语和句子
- en: Interesting to observe yet again which phrases lie in the middle. It looks like
    law and history serve as sort of a connection between religion and science here.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是再次观察哪些短语处于中间位置。看起来法律和历史在这里充当了宗教和科学之间的某种连接。
- en: Investigating Corpus-Level Semantic Structure with Document Embeddings
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用文档嵌入研究语料库级语义结构
- en: We have now looked at our corpus on the word and phrase level, and seen the
    semantic structures that naturally arise in them.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经在单词和短语级别查看了我们的语料库，并观察了它们中自然出现的语义结构。
- en: What if we wanted to gain some information about what happens on the level of
    documents? What documents lie close to each other, and what kinds of groups show
    up?
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要了解在文档级别发生了什么呢？哪些文档彼此接近，出现了什么样的群体？
- en: Note that one natural solution to this problem is topic modeling, which you
    should have a look into if you haven’t done it yet. In this article we will explore
    other tangentially related conceptualizations of this task.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，一个自然的解决方案是主题建模，如果你还没有尝试过，应该看看。在本文中，我们将探索与此任务相关的其他概念化方法。
- en: Document Representations
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文档表示
- en: As before, we need to think about how we are going to represent individual documents
    so that their semantic content gets captured.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 和以前一样，我们需要考虑如何表示单个文档，以便捕捉它们的语义内容。
- en: More traditional machine learning practice would typically use Bag-of-Words
    representations or would train a Doc2Vec model. These are all good options (and
    you could and should experiment with them), but they again, lack contextual understanding
    of text. Since texts in our corpus are not too long, we can still use sentence
    transformers for embedding them. Let’s continue with the same embedding model
    we used for phrases.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 更传统的机器学习实践通常使用词袋模型表示或训练Doc2Vec模型。这些都是很好的选择（你可以并且应该尝试这些方法），但它们同样缺乏文本的上下文理解。由于我们语料库中的文本不算太长，我们仍然可以使用句子变换器进行嵌入。让我们继续使用我们为短语使用的相同嵌入模型。
- en: Projection and Clustering
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 投影与聚类
- en: A natural way to explore semantic representations of documents is to project
    them into lower dimensional spaces (usually 2D) and use these projections for
    visualizing the documents. We can also look at how documents get clustered given
    some clustering approach.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 探索文档的语义表示的一种自然方式是将它们投影到较低维度的空间（通常是2D），并使用这些投影来可视化文档。我们还可以查看文档在某些聚类方法下的聚类情况。
- en: 'Now this is all great, but the space of projection, dimensionality reduction
    and clustering approaches is so vast, that I constantly found myself wondering:
    *“Would this look substantially different if I had used something else?”* To counteract
    this issue I added another app to embedding-explorer, where you can freely and
    quickly explore what kinds of visualizations you would get out of all sorts of
    different methods.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这一切都很棒，但投影、降维和聚类方法的领域如此广泛，以至于我常常会想：*“如果我使用其他方法，结果会有实质性不同吗？”* 为了应对这个问题，我在嵌入探索中添加了另一个应用程序，你可以自由快速地探索不同方法下的可视化效果。
- en: 'Here’s our workflow:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的工作流程：
- en: 1\. We may or may not want to reduce the dimensionality of the embeddings before
    we proceed. You can choose from all sorts of dimensionality reduction methods,
    or you can turn it off.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 我们可能会在继续之前减少嵌入的维度。你可以选择各种降维方法，或者关闭它。
- en: 2\. We want to project our embeddings into 2D space so we can visualize them.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 我们想将嵌入投影到2D空间中，以便我们可以对其进行可视化。
- en: 3\. We might want to cluster the embeddings to see what kinds of documents get
    grouped together.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 我们可能想要对嵌入进行聚类，以查看哪些文档被归为一组。
- en: '![](../Images/46e31c7d58375bcd1946ed521a1779c7.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/46e31c7d58375bcd1946ed521a1779c7.png)'
- en: Clustering and Projection Workflow in embedding-explorer
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入探索中的聚类与投影工作流
- en: Now we also have to know some outside information about the documents when we
    do this (textual content, title, etc.) otherwise there isn’t much for us to interpret.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在进行此操作时，我们还需要了解一些关于文档的外部信息（文本内容、标题等），否则我们没有太多可解释的内容。
- en: 'Let’s create a data frame with columns that contain:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个包含以下列的数据框：
- en: 1\. The first 400 characters of each document, so we can get a feel for what
    the text is about.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 每个文档的前400个字符，以便我们可以了解文本的内容。
- en: 2\. The length of the text, so we can see which texts are long and which ones
    are short in the visualizations.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 文本的长度，以便我们可以在可视化中查看哪些文本较长，哪些较短。
- en: 3\. The group from which they come from in our data set.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 它们在数据集中来源的组。
- en: '[PRE9]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We can then start the application with the metadata passed along so we can hover
    and look at information about the documents.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以启动应用程序，传递元数据，以便我们可以悬停并查看有关文档的信息。
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'When the app launches first you’ll be presented with this screen:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用程序启动时，你会首先看到这个屏幕：
- en: '![](../Images/4abedd0f5b11143fec215a1b9e2bfcfa.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4abedd0f5b11143fec215a1b9e2bfcfa.png)'
- en: Options in the Clustering App
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类应用中的选项
- en: After running the clustering you will be able to look at a map of all documents
    colored by cluster. You can hover on points to see metadata about the document…
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 运行聚类后，你将能够查看按聚类着色的所有文档的地图。你可以悬停在点上以查看文档的元数据……
- en: '![](../Images/d7b03a98df2630141f73eaabf56bd9cd.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d7b03a98df2630141f73eaabf56bd9cd.png)'
- en: Clustering App Screenshot
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类应用截图
- en: and in the bottom you can even choose how the points should be colored, labelled
    and sized.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 并且在底部你甚至可以选择点的颜色、标签和大小。
- en: '![](../Images/bb8c364a297dc4cde2893155fa028f59.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bb8c364a297dc4cde2893155fa028f59.png)'
- en: Clusters with Document Sizes
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 带有文档大小的聚类
- en: Summary
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Exploratory analysis of textual data is difficult. We have looked at a handful
    of approaches for interactive investigation using state-of-the-art machine learning
    technology. I hope the methods discussed in this article and the embedding-explorer
    Python package will be useful for you in your future research/work.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对文本数据的探索性分析是困难的。我们已经研究了几种使用最先进的机器学习技术进行互动调查的方法。我希望本文讨论的方法以及 embedding-explorer
    Python 包对你未来的研究/工作有所帮助。
- en: Peace ✌️
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 和平 ✌️
- en: ((all images in the article were taken from embedding-explorer’s documentation,
    which was produced by the author))
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: （（本文中的所有图片均来自 embedding-explorer 的文档，由作者制作））
