- en: Explore Semantic Relations in Corpora with Embedding Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/explore-semantic-relations-in-corpora-with-embedding-models-0a6d64c3ec7f?source=collection_archive---------2-----------------------#2023-11-24](https://towardsdatascience.com/explore-semantic-relations-in-corpora-with-embedding-models-0a6d64c3ec7f?source=collection_archive---------2-----------------------#2023-11-24)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@power.up1163?source=post_page-----0a6d64c3ec7f--------------------------------)[![Márton
    Kardos](../Images/8c86c5ea10391a0031cdc18bb77b0736.png)](https://medium.com/@power.up1163?source=post_page-----0a6d64c3ec7f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----0a6d64c3ec7f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----0a6d64c3ec7f--------------------------------)
    [Márton Kardos](https://medium.com/@power.up1163?source=post_page-----0a6d64c3ec7f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6e63b1795236&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplore-semantic-relations-in-corpora-with-embedding-models-0a6d64c3ec7f&user=M%C3%A1rton+Kardos&userId=6e63b1795236&source=post_page-6e63b1795236----0a6d64c3ec7f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----0a6d64c3ec7f--------------------------------)
    ·10 min read·Nov 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F0a6d64c3ec7f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplore-semantic-relations-in-corpora-with-embedding-models-0a6d64c3ec7f&user=M%C3%A1rton+Kardos&userId=6e63b1795236&source=-----0a6d64c3ec7f---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F0a6d64c3ec7f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexplore-semantic-relations-in-corpora-with-embedding-models-0a6d64c3ec7f&source=-----0a6d64c3ec7f---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: Recently I have talked to a handful of fellow students and scholars who had
    research interests which involved the analysis of free-form text. Unfortunately
    to everyone, gaining meaningful insight to written natural language is not a trivial
    task by any measures. Close reading is of course an option, but you would ideally
    prefer to look at textual data through a more macro-analytical/quantitative lens
    as well. Not to mention that in the age of big data close reading is rarely a
    feasible option.
  prefs: []
  type: TYPE_NORMAL
- en: By far my favorite way to conduct exploratory data analyses on corpora is with
    topic models, and I have written multiple articles about how to go about this
    [in the least painful way possible](https://x-tabdeveloping.github.io/topicwizard/).
    While topic models are awesome, they are not universally the best method for all
    things text.
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings are numerical representations of textual data, and have become the
    canonical approach for semantic querying of text. In this article we will explore
    some of the ways in which we can explore textual data with the use of embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Capturing Relations between Concepts with Word Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Word embedding models are a set of approaches that learn latent vector representations
    of terms in an unsupervised fashion. When learning word embeddings from natural
    language, one essentially obtains a map of semantic relations in an embedding
    space.
  prefs: []
  type: TYPE_NORMAL
- en: Word embeddings are typically trained on large corpora so that they can capture
    general word-to-word relations in human language. This is useful, because one
    can infuse general knowledge about language into models for specific applications.
    This is also known as transfer learning, and has been a hot topic in machine learning
    for quite some time.
  prefs: []
  type: TYPE_NORMAL
- en: What if, instead of wanting to transfer general knowledge into a specific model,
    we just want to get a mapping of the semantically specific aspects of a smaller
    corpus? Let’s say that we have a a corpus of comments from a forum and we want
    to explore what kinds of associative relations can be found in them.
  prefs: []
  type: TYPE_NORMAL
- en: One way we may achieve this is by training a word embedding model from scratch
    on this corpus instead of using one that has been pretrained for us. In this example
    I am going to use the 20Newsgroups dataset as the corpus, in which we will explore
    semantic relations.
  prefs: []
  type: TYPE_NORMAL
- en: Train a Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now let’s start with a word embedding model. You might be familiar with Word2Vec,
    which is the method that popularized the use of static word embeddings in research
    and practice. On the other hand GloVe, developed by the folks over at Stanford
    seems to be a better method under most circumstances, and my anecdotal experience
    indicates that it gives much higher quality embeddings, especially on smaller
    corpora.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately GloVe is not implemented in Gensim, but luckily I have made a
    fully Gensim compatible interface for the original GloVe code, we are going to
    use this for training the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s install gensim, glovpy, scikit-learn, so we can fetch 20Newsgroups as
    well as embedding-explorer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We first have to load the dataset, and tokenize it, for this we will use gensim’s
    built in tokenizer. We are also going to filter out stop words, as they do not
    bear any meaningful information for the task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: After this we can easily train a GloVe model on the tokenized corpus.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We can already query this word embedding model, let’s check for example which
    ten words are closest to “child”.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Investigate, Visualize!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Individually investigating every words’ relation to other words becomes tedious
    very quickly though. Ideally we would also like to visualize relations, maybe
    even get some networks.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, the [embedding-explorer](https://centre-for-humanities-computing.github.io/embedding-explorer/#)
    package can help us here, which I have developed. Working in computational humanities
    we often make use of word embedding models and the semantic networks built from
    relations in those models, and embedding-explorer helps us explore these in an
    interactive and visual manner. The package contains multiple interactive web applications,
    we will first look at the “network explorer”.
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind this app is that concepts in embedding models naturally form
    some sort of network structure. Words that are closely related have strong links,
    while others might not have any. In the app you can build concept graphs based
    on a set of seed words you specify and two levels of free association.
  prefs: []
  type: TYPE_NORMAL
- en: At each level of association we take the five closest words in the embedding
    model to the words we already have and we add the to our network with a connection
    to the word it was associated to. The strength of the connection is determined
    by the cosine distance of concepts in embedding space. These kinds of networks
    have proven useful for multiple research projects me or my colleagues have worked
    on.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start up the app on our word embedding model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This will open a browser window where you can freely explore the semantic relations
    in the corpus. Here is a screenshot of me looking at what networks arise around
    the words “jesus”, “science” and “religion”.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4eaca92387bf13d4e0e9267ed4f07729.png)'
  prefs: []
  type: TYPE_IMG
- en: Exploring Semantic Relations in our GloVe Model
  prefs: []
  type: TYPE_NORMAL
- en: We can for example see that the way people talk about these subjects online
    seems to suggest that religion and science seem to relate through politics, society
    and philosophy, which makes a lot of sense. It is also interesting to observe
    how education is somewhere mid-way between science and religion, but is clearly
    more connected to science. This would be interesting to explore in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Networks of N-grams with Sentence Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now what if we not only want to look at word level relations, but phrases or
    sentences?
  prefs: []
  type: TYPE_NORMAL
- en: 'My suggestion is to use N-grams. N-grams are essentially just N terms that
    follow each other in text. For example in the sentence “I love my little cute
    dog” we would have 4-grams: “I love my little”, “love my little cute” and “my
    little cute dog”. Now the question is, how do we learn good semantic representations
    of N-grams?'
  prefs: []
  type: TYPE_NORMAL
- en: You could technically still do this with GloVe by treating a phrase or sentence
    as a token, but there is a catch. Since the variety of N-grams increases drastically
    with N, a particular N-gram might only occur once or twice, and we might not be
    able to learn good representations of them.
  prefs: []
  type: TYPE_NORMAL
- en: How about taking the mean of word embeddings in the phrase? Well this could
    go a long way, but the problem is that we completely lose all information about
    the importance of different words, their order in the sentence and all contextual
    information as well.
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this issue is to use [sentence transformers](https://www.sbert.net/),
    deep neural language models that produce contextually sensitive representations
    of text. They have [outperformed all other approaches](https://huggingface.co/blog/mteb)
    for a few years now, and have become the industry standard for embedding text.
    Now training such a model takes a lot of data, that we do not have at hand, but
    luckily we can use a handful of good pretrained models.
  prefs: []
  type: TYPE_NORMAL
- en: N-gram Extraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us first extract N-grams from our corpus. I chose to go with four-grams,
    but you can choose any number you would like. We are going to use scikit-learn’s
    CountVectorizer for doing this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Embedding Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will need an embedding model for text representation. As I said earlier,
    we are going to use a pretrained model. I chose [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)
    as it is very stable, widely used and is quite small, so it will probably run
    smoothly even on your personal computer.
  prefs: []
  type: TYPE_NORMAL
- en: We will use yet another package, [embetter](https://github.com/koaning/embetter),
    so that we can use sentence transformers in a scikit-learn compatible manner.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can load the model in Python like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Explore!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can then load the model and the n-grams into embedding-explorer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note that this allows us to specify any arbitrary seed instead of just the ones
    that are in our vocabulary of four-grams. Here’s a screenshot of me putting in
    two sentences and seeing what kinds of networks are built from the four grams
    around them.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3db5a5c43e29284b149aa94c94740de6.png)'
  prefs: []
  type: TYPE_IMG
- en: Exploring Phrases and Sentences in the Corpus
  prefs: []
  type: TYPE_NORMAL
- en: Interesting to observe yet again which phrases lie in the middle. It looks like
    law and history serve as sort of a connection between religion and science here.
  prefs: []
  type: TYPE_NORMAL
- en: Investigating Corpus-Level Semantic Structure with Document Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have now looked at our corpus on the word and phrase level, and seen the
    semantic structures that naturally arise in them.
  prefs: []
  type: TYPE_NORMAL
- en: What if we wanted to gain some information about what happens on the level of
    documents? What documents lie close to each other, and what kinds of groups show
    up?
  prefs: []
  type: TYPE_NORMAL
- en: Note that one natural solution to this problem is topic modeling, which you
    should have a look into if you haven’t done it yet. In this article we will explore
    other tangentially related conceptualizations of this task.
  prefs: []
  type: TYPE_NORMAL
- en: Document Representations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As before, we need to think about how we are going to represent individual documents
    so that their semantic content gets captured.
  prefs: []
  type: TYPE_NORMAL
- en: More traditional machine learning practice would typically use Bag-of-Words
    representations or would train a Doc2Vec model. These are all good options (and
    you could and should experiment with them), but they again, lack contextual understanding
    of text. Since texts in our corpus are not too long, we can still use sentence
    transformers for embedding them. Let’s continue with the same embedding model
    we used for phrases.
  prefs: []
  type: TYPE_NORMAL
- en: Projection and Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A natural way to explore semantic representations of documents is to project
    them into lower dimensional spaces (usually 2D) and use these projections for
    visualizing the documents. We can also look at how documents get clustered given
    some clustering approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now this is all great, but the space of projection, dimensionality reduction
    and clustering approaches is so vast, that I constantly found myself wondering:
    *“Would this look substantially different if I had used something else?”* To counteract
    this issue I added another app to embedding-explorer, where you can freely and
    quickly explore what kinds of visualizations you would get out of all sorts of
    different methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s our workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. We may or may not want to reduce the dimensionality of the embeddings before
    we proceed. You can choose from all sorts of dimensionality reduction methods,
    or you can turn it off.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. We want to project our embeddings into 2D space so we can visualize them.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. We might want to cluster the embeddings to see what kinds of documents get
    grouped together.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/46e31c7d58375bcd1946ed521a1779c7.png)'
  prefs: []
  type: TYPE_IMG
- en: Clustering and Projection Workflow in embedding-explorer
  prefs: []
  type: TYPE_NORMAL
- en: Now we also have to know some outside information about the documents when we
    do this (textual content, title, etc.) otherwise there isn’t much for us to interpret.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a data frame with columns that contain:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. The first 400 characters of each document, so we can get a feel for what
    the text is about.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. The length of the text, so we can see which texts are long and which ones
    are short in the visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. The group from which they come from in our data set.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We can then start the application with the metadata passed along so we can hover
    and look at information about the documents.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'When the app launches first you’ll be presented with this screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4abedd0f5b11143fec215a1b9e2bfcfa.png)'
  prefs: []
  type: TYPE_IMG
- en: Options in the Clustering App
  prefs: []
  type: TYPE_NORMAL
- en: After running the clustering you will be able to look at a map of all documents
    colored by cluster. You can hover on points to see metadata about the document…
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d7b03a98df2630141f73eaabf56bd9cd.png)'
  prefs: []
  type: TYPE_IMG
- en: Clustering App Screenshot
  prefs: []
  type: TYPE_NORMAL
- en: and in the bottom you can even choose how the points should be colored, labelled
    and sized.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bb8c364a297dc4cde2893155fa028f59.png)'
  prefs: []
  type: TYPE_IMG
- en: Clusters with Document Sizes
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploratory analysis of textual data is difficult. We have looked at a handful
    of approaches for interactive investigation using state-of-the-art machine learning
    technology. I hope the methods discussed in this article and the embedding-explorer
    Python package will be useful for you in your future research/work.
  prefs: []
  type: TYPE_NORMAL
- en: Peace ✌️
  prefs: []
  type: TYPE_NORMAL
- en: ((all images in the article were taken from embedding-explorer’s documentation,
    which was produced by the author))
  prefs: []
  type: TYPE_NORMAL
