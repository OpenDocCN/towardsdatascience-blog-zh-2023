- en: To Guarantee Impartial AI Decisions, Lady Justice Needs to Blink
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/to-guarantee-impartial-ai-decisions-lady-justice-needs-to-blink-2992167b2591?source=collection_archive---------15-----------------------#2023-02-10](https://towardsdatascience.com/to-guarantee-impartial-ai-decisions-lady-justice-needs-to-blink-2992167b2591?source=collection_archive---------15-----------------------#2023-02-10)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why is deleting the sensitive attributes not a simple solution to AI fairness?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@boris-ruf?source=post_page-----2992167b2591--------------------------------)[![Boris
    Ruf](../Images/96dc4fc2f32add89fef6911195590cd8.png)](https://medium.com/@boris-ruf?source=post_page-----2992167b2591--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2992167b2591--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2992167b2591--------------------------------)
    [Boris Ruf](https://medium.com/@boris-ruf?source=post_page-----2992167b2591--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fed341456850c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fto-guarantee-impartial-ai-decisions-lady-justice-needs-to-blink-2992167b2591&user=Boris+Ruf&userId=ed341456850c&source=post_page-ed341456850c----2992167b2591---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2992167b2591--------------------------------)
    ·6 min read·Feb 10, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2992167b2591&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fto-guarantee-impartial-ai-decisions-lady-justice-needs-to-blink-2992167b2591&user=Boris+Ruf&userId=ed341456850c&source=-----2992167b2591---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2992167b2591&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fto-guarantee-impartial-ai-decisions-lady-justice-needs-to-blink-2992167b2591&source=-----2992167b2591---------------------bookmark_footer-----------)![](../Images/4e137101cd1e0766b899e5d02573b435.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Pawel Czerwinski](https://unsplash.com/@pawel_czerwinski?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '*Unwanted bias has been identified as major risk to the wider adoption of artificial
    intelligence (AI). In* [*my previous article*](https://medium.com/towards-data-science/how-prejudice-creeps-into-ai-systems-3673646ae8e3)
    *I have discussed different potential sources of this bias. But what if we simply
    ignored the sensitive attributes altogether? In the following text, I will explain
    why so-called “fairness through unawareness” is not the right solution.*'
  prefs: []
  type: TYPE_NORMAL
- en: Systematic, unequal treatment of individuals based on their membership of a
    sensitive group is considered discrimination. For example, when people face unequal
    or disadvantageous treatment simply because they are male or female, this is considered
    [gender discrimination](https://en.wikipedia.org/wiki/Gender_inequality). When
    job applicants or employees are treated less favourably because of their age,
    we talk about [age discrimination](https://en.wikipedia.org/wiki/Ageism).
  prefs: []
  type: TYPE_NORMAL
- en: There is broad consensus in our society that it is unfair to make a distinction
    on the ground of a personal characteristic which is usually not a matter of choice.
    Therefore, most legal frameworks prohibit such actions. When it comes to non-discrimination
    in the EU, for example, the **Convention for the Protection of Human Rights and
    Fundamental Freedoms** defines the “Prohibition of discrimination” in [Article
    14](https://www.coe.int/en/web/conventions/full-list?module=treaty-detail&treatynum=005).
    This principle is further contained in the **Charter of Fundamental Rights of
    the European Union** which states in [Article 21](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&cd=&ved=2ahUKEwiRzLnMtO71AhXG4IUKHdo4Dx4QFnoECAgQAQ&url=https%3A%2F%2Feur-lex.europa.eu%2FLexUriServ%2FLexUriServ.do%3Furi%3DOJ%3AC%3A2010%3A083%3A0389%3A0403%3Aen%3APDF&usg=AOvVaw0BJqdnCuQHjC-VBG1oO1gL)
    that “Any discrimination based on any ground such as sex, race, colour, ethnic
    or social origin, genetic features, language, religion or belief, political or
    any other opinion, membership of a national minority, property, birth, disability,
    age or sexual orientation shall be prohibited.”
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/20dbb63f1efd8e3f28984dd0e640ef81.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Tingey Injury Law Firm](https://unsplash.com/@tingeyinjurylawfirm?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Fairness through unawareness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Computers are not biased like humans often are, so it seems simple for a machine
    to remove discrimination. Take gender-based discrimination for instance: if the
    machine does not have any information regarding the gender, then there can be
    no more bias, right? Unfortunately, things are a little more complicated.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The principle of simply excluding any sensitive attributes as features from
    the data in order to obtain “fairness through unawareness” is known as “anti-classification”
    among legal scholars. In EU law, this is enforced on the level of data protection:
    The [**General Data Protection Regulation (GDPR)**](https://eur-lex.europa.eu/eli/reg/2016/679/oj)
    regulates the collection and use of personal data, including sensitive personal
    data. For many use cases, it strictly prohibits the storing and processing of
    a list of attributes which were classified as protected, i.e. sensitive.'
  prefs: []
  type: TYPE_NORMAL
- en: Chatty proxies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For non-AI systems, when using conventional, deterministic algorithms with a
    manageable amount of data, the current approa
  prefs: []
  type: TYPE_NORMAL
- en: 'ch can provide a solution. However, it is important to point out that in the
    case of ill intention, anti-classification does not prevent discrimination per
    se, as the practice called “[redlining](https://en.wikipedia.org/wiki/Redlining)”
    has proven in the past: Sometimes, non-sensitive attributes may be strongly linked
    to sensitive attributes. Consequently, they can serve as substitutes or proxies.
    For example, the non-sensitive attribute zip code might be correlated with the
    sensitive attribute race when many people from the same ethnic background live
    in the same neighbourhood. Hence, already in the context of non-AI systems, seemingly
    unsuspicious attributes can be misused to produce discriminatory decisions with
    the purpose of explicitly excluding a specific sensitive subgroup. Without actual
    knowledge of the sensitive attributes, such actions are hard to detect and to
    prevent.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9dc254aab32ca96707fe7b15e5ed6f2f.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Mika Baumeister](https://unsplash.com/@mbaumi?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Enter Big Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When it comes to AI systems, the concept of removing sensitive attributes from
    data in order to prevent algorithms from being unfair has proven particularly
    insufficient: Such systems are usually backed by high-dimensional and strongly
    correlated datasets. This means that the decisions are based on hundreds or even
    thousands of attributes whose relevance is not obvious at first glance for the
    human eye. Further, some of those attributes usually contain strong links which
    again are difficult to spot for humans. Even after removing the sensitive attributes,
    such complex correlations in the data may continue to provide many unexpected
    links to protected information. In fact, heuristic methods exist to actively reconstruct
    missing sensitive attributes. For example, the [Bayesian Improved Surname Geocoding
    (BISG)](https://pubmed.ncbi.nlm.nih.gov/18479410/) method attempts to predict
    the race given the surname and a geolocation. While the reliability of this method
    is generally disputed, it demonstrates that prohibiting to collect sensitive attributes
    does not prevent any possible misuse just by technical design.'
  prefs: []
  type: TYPE_NORMAL
- en: Can you anonymise a resume?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'But even without any bad intent to discriminate, there is a danger of hidden
    indirect discrimination which is very difficult to detect in the results. To illustrate
    the problem, imagine an AI system which analyses resumes in order to propose starting
    salaries for newly hired staff. Let’s further assume that women were discriminated
    in the past because their salaries were systematically lower compared to those
    of their male colleagues. As explained above, historical bias of this kind cannot
    be overcome by excluding the sensitive attribute “gender” in the learning data
    since many links to non-sensitive attributes exist. For example, some sports are
    more popular among women or men. In languages with grammatical gender, the applicant’s
    gender may be revealed through gender inflections of nouns, pronouns or adjectives.
    And yet more complex, in a country with compulsory military service exclusively
    for men, the entry age at university could provide a hint to the gender, too.
    Even when trying to additionally adjust for all of those identified correlations
    manually, it remains impossible to establish a sufficient degree of “unawareness”
    which could guarantee discrimination-free decisions: Based on seemingly unsuspicious
    proxy variables, a machine learning algorithm would be quick to recognise the
    old pattern and continue to allocate lower salaries to women.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/232d8c87f5287e89b87bfd0f9a747cf2.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Marjan Blan | @marjanblan](https://unsplash.com/@marjan_blan?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Active fairness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The risk to privacy protection when storing sensitive attributes is obvious.
    For example, confidential information could get leaked to an untrusted environment
    and misused by third parties for fraud. Accordingly, the motivation behind the
    current rules to address such concerns is understandable: Where no personal data
    is stored, none can be lost. When it comes to Big Data, however, this assumption
    does not hold anymore due to the high degree of correlations in the data. For
    AI applications, the current practice of trying to ignore the existence of sensitive
    subgroups by omitting sensitive attributes actually may bare greater risk than
    any privacy concerns related to the data collection. New technical security mechanisms
    are needed to protect the sensitive attributes from misuse, but allow their active
    use to make sensitive subgroups visible and account for them with the purpose
    of verifiable fair results. They are required, for AI stakeholders but also for
    regulators, to apply statistical measures in order to test for imbalanced results
    and detect any type of discrimination. It is only such “active fairness” that
    can ensure that the standards of fairness and non-discrimination in AI systems
    are respected.'
  prefs: []
  type: TYPE_NORMAL
- en: So what
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ignoring the sensitive attributes to obtain impartial AI decisions is useless
    due to the many correlations and proxy variables in the data. Doing so is even
    counterproductive because it makes detecting unwanted biases very difficult. It
    is instead necessary to “see” the sensitive subgroups which ought to be protected
    to achieve AI fairness. This requires the active use of sensitive attributes.
  prefs: []
  type: TYPE_NORMAL
- en: '*Many thanks to Antoine Pietri for his valuable support in writing this post.
    In* [*my next article*](/so-how-fair-is-your-ai-exactly-83f8defcf449)*, I discuss
    the challenging but essential task of defining fairness.*'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: B. Ruf & M. Detyniecki (2020). [Active Fairness Instead of Unawareness](https://arxiv.org/abs/2009.06251).
    *ArXiv, abs/2009.06251*.
  prefs: []
  type: TYPE_NORMAL
- en: 'A. Bouverot, T. Delaporte, A. Amabile et al. (2020). [Algorithms: mind the
    bias!](https://www.institutmontaigne.org/en/publications/algorithms-please-mind-bias)
    Report of the Institut Montaigne. ISSN: 1771–6756.'
  prefs: []
  type: TYPE_NORMAL
- en: 'S. Corbett-Davies & S. Goel (2018). [The measure and mismeasure of fairness:
    A critical review of fair machine learning](https://arxiv.org/abs/1808.00023).
    *CoRR, abs/1808.00023*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A Gano (2017). Disparate impact and mortgage lending: A beginner’s guide. University
    of Colorado Law. Review 88, 1109–1166.'
  prefs: []
  type: TYPE_NORMAL
