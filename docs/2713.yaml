- en: The CLIP Foundation Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CLIP 基础模型
- en: 原文：[https://towardsdatascience.com/the-clip-foundation-model-7770858b487d?source=collection_archive---------3-----------------------#2023-08-26](https://towardsdatascience.com/the-clip-foundation-model-7770858b487d?source=collection_archive---------3-----------------------#2023-08-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-clip-foundation-model-7770858b487d?source=collection_archive---------3-----------------------#2023-08-26](https://towardsdatascience.com/the-clip-foundation-model-7770858b487d?source=collection_archive---------3-----------------------#2023-08-26)
- en: '[🚀Sascha’s Paper Club](https://towardsdatascience.com/tagged/saschas-paper-club)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[🚀Sascha的论文俱乐部](https://towardsdatascience.com/tagged/saschas-paper-club)'
- en: Learning Transferable Visual Models From Natural Language Supervision by A.
    Radford et. al.
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 《从自然语言监督中学习可转移的视觉模型》，作者 A. Radford 等。
- en: '[](https://medium.com/@SaschaKirch?source=post_page-----7770858b487d--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----7770858b487d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7770858b487d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7770858b487d--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----7770858b487d--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@SaschaKirch?source=post_page-----7770858b487d--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----7770858b487d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7770858b487d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7770858b487d--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----7770858b487d--------------------------------)'
- en: ·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5c38dace9d5e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-clip-foundation-model-7770858b487d&user=Sascha+Kirch&userId=5c38dace9d5e&source=post_page-5c38dace9d5e----7770858b487d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7770858b487d--------------------------------)
    ·8 min read·Aug 26, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7770858b487d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-clip-foundation-model-7770858b487d&user=Sascha+Kirch&userId=5c38dace9d5e&source=-----7770858b487d---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5c38dace9d5e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-clip-foundation-model-7770858b487d&user=Sascha+Kirch&userId=5c38dace9d5e&source=post_page-5c38dace9d5e----7770858b487d---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7770858b487d--------------------------------)
    ·8 分钟阅读·2023年8月26日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7770858b487d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-clip-foundation-model-7770858b487d&user=Sascha+Kirch&userId=5c38dace9d5e&source=-----7770858b487d---------------------clap_footer-----------)'
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7770858b487d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-clip-foundation-model-7770858b487d&source=-----7770858b487d---------------------bookmark_footer-----------)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7770858b487d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-clip-foundation-model-7770858b487d&source=-----7770858b487d---------------------bookmark_footer-----------)'
- en: In this article we are going through the paper behind CLIP (**C**ontrastive
    **L**anguage-**I**mage **P**re-Training). We will extract key concepts and break
    them down to make them easy to understand. Further, images and data graphs are
    annotated to clarify doubts.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将深入探讨 CLIP（**C**ontrastive **L**anguage-**I**mage **P**re-Training）背后的论文。我们将提取关键概念并将其拆解，以便于理解。此外，图像和数据图表也会进行注释，以澄清疑问。
- en: '![](../Images/60cfe45b459db912d2688dab7c9b3d31.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/60cfe45b459db912d2688dab7c9b3d31.png)'
- en: Image created from [publication](https://arxiv.org/abs/2103.00020) by [Sascha
    Kirch](https://medium.com/@SaschaKirch)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [出版物](https://arxiv.org/abs/2103.00020)，作者 [Sascha Kirch](https://medium.com/@SaschaKirch)
- en: '**Paper:** [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020),
    Alec Radford et. al., 26 Feb. 2021'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**论文：** [从自然语言监督中学习可转移的视觉模型](https://arxiv.org/abs/2103.00020)，Alec Radford
    等，2021年2月26日'
- en: ''
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Resources:** [GitHub](https://github.com/OpenAI/CLIP) — [Blog Post](https://openai.com/research/clip)
    — [Hugging Face](https://huggingface.co/docs/transformers/model_doc/clip)'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**资源：** [GitHub](https://github.com/OpenAI/CLIP) — [博客文章](https://openai.com/research/clip)
    — [Hugging Face](https://huggingface.co/docs/transformers/model_doc/clip)'
- en: ''
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Category:** multi-modal deep learning, computer vision, natural language
    processing, foundation models, representation learning'
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**类别：** 多模态深度学习、计算机视觉、自然语言处理、基础模型、表示学习'
- en: ''
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[**Other Walkthroughs**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**:**'
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[**其他讲解**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**：**'
- en: '[[BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)]
    — [[Depth Anything](https://medium.com/towards-data-science/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?sk=fc6197edd68e6137c3396c83e50f65cb)]
    — [[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)]
    — [[Segment Anything](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    — [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    — [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[[BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)]
    — [[深度任何东西](https://medium.com/towards-data-science/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?sk=fc6197edd68e6137c3396c83e50f65cb)]
    — [[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)]
    — [[随意分割](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    — [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    — [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
- en: Outline
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大纲
- en: Context & Background
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 背景与概述
- en: Method
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 方法
- en: Experiments
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实验
- en: Further Readings & Resources
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进一步阅读与资源
- en: Context & Background
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景与概述
- en: CLIP (**C**ontrastive **L**anguage-**I**mage **P**re-Training) is a multi-modal
    model that learns the correspondence between natural language and images. It is
    trained on 400 million text-images…
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP（**C**ontrastive **L**anguage-**I**mage **P**re-Training）是一个多模态模型，学习自然语言与图像之间的对应关系。它在4亿对文本-图像上进行训练…
