- en: 'LlamaIndex: the ultimate LLM framework for indexing and retrieval'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/llamaindex-the-ultimate-llm-framework-for-indexing-and-retrieval-fa588d8ca03e?source=collection_archive---------1-----------------------#2023-06-20](https://towardsdatascience.com/llamaindex-the-ultimate-llm-framework-for-indexing-and-retrieval-fa588d8ca03e?source=collection_archive---------1-----------------------#2023-06-20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An Introduction to LlamaIndex
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://sophiamyang.medium.com/?source=post_page-----fa588d8ca03e--------------------------------)[![Sophia
    Yang, Ph.D.](../Images/c133f918245ea4857dc46df3a07fc2b1.png)](https://sophiamyang.medium.com/?source=post_page-----fa588d8ca03e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fa588d8ca03e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fa588d8ca03e--------------------------------)
    [Sophia Yang, Ph.D.](https://sophiamyang.medium.com/?source=post_page-----fa588d8ca03e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fae9cae9cbcd2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllamaindex-the-ultimate-llm-framework-for-indexing-and-retrieval-fa588d8ca03e&user=Sophia+Yang%2C+Ph.D.&userId=ae9cae9cbcd2&source=post_page-ae9cae9cbcd2----fa588d8ca03e---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fa588d8ca03e--------------------------------)
    ·8 min read·Jun 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffa588d8ca03e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllamaindex-the-ultimate-llm-framework-for-indexing-and-retrieval-fa588d8ca03e&user=Sophia+Yang%2C+Ph.D.&userId=ae9cae9cbcd2&source=-----fa588d8ca03e---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffa588d8ca03e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllamaindex-the-ultimate-llm-framework-for-indexing-and-retrieval-fa588d8ca03e&source=-----fa588d8ca03e---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: 'LlamaIndex, previously known as the GPT Index, is a remarkable data framework
    aimed at helping you build applications with LLMs by providing essential tools
    that facilitate data ingestion, structuring, retrieval, and integration with various
    application frameworks. The capabilities offered by LlamaIndex are numerous and
    highly valuable:'
  prefs: []
  type: TYPE_NORMAL
- en: ✅ Ingest from different data sources and data formats using Data connectors
    (Llama Hub).
  prefs: []
  type: TYPE_NORMAL
- en: ✅ Enable document operations such as inserting, deleting, updating, and refreshing
    the document index.
  prefs: []
  type: TYPE_NORMAL
- en: ✅ Support synthesis over heterogeneous data and multiple documents.
  prefs: []
  type: TYPE_NORMAL
- en: ✅ Use “Router” to pick between different query engines.
  prefs: []
  type: TYPE_NORMAL
- en: ✅ Allow for the hypothetical document embeddings to enhance output quality
  prefs: []
  type: TYPE_NORMAL
- en: ✅ Offer a wide range of integrations with various vector stores, ChatGPT plugins,
    tracing tools, and LangChain, among others.
  prefs: []
  type: TYPE_NORMAL
- en: ✅ Support the brand new OpenAI function calling API.
  prefs: []
  type: TYPE_NORMAL
- en: These are just a few examples of the extensive capabilities provided by LlamaIndex.
    In this blog post, we will explore some of the functionalities that I find exceptionally
    useful with LlamaIndex.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data connectors (LlamaHub)**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When developing an LLM application, it’s essential to enable LLM to interact
    with external data sources effectively. How to ingest data is the key here. The
    Llama Hub offers a wide range of over 100 data sources and formats, allowing LlamaIndex
    or LangChain to ingest data in a consistent manner.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fe02f58ae92b5c4448beff7572f9307d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'LlamaHub. Source: [https://llama-hub-ui.vercel.app/](https://llama-hub-ui.vercel.app/).'
  prefs: []
  type: TYPE_NORMAL
- en: By default, you can `pip install llama-hub` and use it as a standalone package.
    You may also choose to use our `download_loader` method to individually download
    a data loader for use with LlamaIndex.
  prefs: []
  type: TYPE_NORMAL
- en: Here is an example where we load in a Wikipedia data loader from the `llama-hub`
    package. The consistent syntax is very nice.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/17289b051cc62fcc71c2393680a0415b.png)'
  prefs: []
  type: TYPE_IMG
- en: Llama Hub also supports multimodal documents. For example, the [ImageReader](https://llamahub.ai/l/file-image)
    loader uses pytesseract or the Donut transformer model to extract text from an
    image.
  prefs: []
  type: TYPE_NORMAL
- en: Basic query functionalities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Index, retriever, and query engine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Index, retriever, and query engine are three basic components for asking questions
    over your data or documents:'
  prefs: []
  type: TYPE_NORMAL
- en: Index is a data structure that allows us to retrieve relevant information quickly
    for a user query from external documents. Index works by parsing documents into
    text chunks, which are called “*Node*” objects, and then building index from the
    chunks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retriever is used for fetching and retrieving relevant information given user
    query.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Query engine is built on top of index and retriever providing a generic interface
    to ask questions about your data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the simplest way to ask questions about your document. You create an
    index from the document first, and then use a query engine as the interface for
    your question:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: There are various types of index, retriever methods, and query engines that
    you can future read on the LlamaIndex docs. In the remainder of this article,
    I’d like to cover some of the cool features I find useful next.
  prefs: []
  type: TYPE_NORMAL
- en: Handle document updates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Often times, once we create an index for our document, there might be a need
    to periodically update the document. This process can be costly if we were to
    recreate the embeddings for the entire document again. LlamaIndex index structure
    offers a solution by enabling efficient insertion, deletion, update, and refresh
    operations. For example, a new document can be inserted as additional nodes (text
    chunks) without the need to recreate nodes from previous documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Query multiple documents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With LlamaIndex, it’s easy to query multiple documents. This functionality is
    enabled through the `SubQuestionQueryEngine` class. When given a query, the query
    engine generates a “query plan” consisting of sub-queries against sub-documents,
    which are then synthesized to provide the final answer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you can see below, LlamaIndex decomposed a complex query into 2 subqueries
    and was able to compare the information from multiple documents to get the final
    answer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e19591b076d1d0b861c4955ca8f50ca9.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Use “Router” to pick between different query engines**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine you are building a bot to retrieve information from both Notion and
    Slack, how does the language model know which tool to use to search for information?
    LlamaIndex is like a clever helper that can find things for you, even if they
    are in different places. Specifically, LlamaIndex’s “Router” is a super simple
    abstraction that allows “picking” between different query engines.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we have two document indexes from Notion and Slack, and we
    create two query engines for each of them. After that, we put all the tools together
    and create a super tool called RouterQueryEngine, which picks which tool to use
    based on the description we gave to the individual tools. This way, when we ask
    a question about Notion, the router will automatically look for information from
    the Notion documents.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'There are many exciting use cases for this. Here is a complete example that
    uses the router to pick between SQL and a vector db: [https://gpt-index.readthedocs.io/en/latest/examples/query_engine/SQLRouterQueryEngine.html](https://gpt-index.readthedocs.io/en/latest/examples/query_engine/SQLRouterQueryEngine.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Hypothetical document embeddings (HyDE)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Typically, when we ask a question about an external document, what we normally
    do is that we use text embeddings to create vector representations for both the
    question and the document. Then we use semantic search to find the text chunks
    that are the most relevant to the question. However, the answer to the question
    may differ significantly from the question itself. What if we could generate hypothetical
    answers to our question first and then find the text chunks that are most relevant
    to the hypothetical answer? That’s where hypothetical document embeddings (HyDE)
    come into play and can potentially improve output quality.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Support OpenAI function calling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[OpenAI recently released](https://openai.com/blog/function-calling-and-other-api-updates)
    the function calling capabilities to more reliably connect GPT’s capabilities
    with external tools and APIs. Check out my previous video to see exactly how it
    works.'
  prefs: []
  type: TYPE_NORMAL
- en: LlamaIndex has quickly integrated this functionality and added a brand new `OpenAIAgent`.
    Check out this [notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/agent/openai_agent.ipynb)
    to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: What if there are way too many numbers of functions? Use the `RetrieverOpenAIAgent`!
    Check out this [notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/agent/openai_agent_retrieval.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Use LlamaIndex with LangChain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LlmaIndex offers a wide range of integrations with various vector stores, ChatGPT
    plugins, tracing tools, and LangChain.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/891c857e8348eec985fbfd411d777ab5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [https://imgflip.com/memegenerator](https://imgflip.com/memegenerator).'
  prefs: []
  type: TYPE_NORMAL
- en: How is LIamaIndex different from LangChain?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you have used LangChain, you may wonder how is LlamaIndex different from
    LangChain. If you are not familiar with LangChain, check out my previous [blog
    post](/the-easiest-way-to-interact-with-language-models-4da158cfb5c5?sk=271c9c82a16282f93ef3df37f034babe)
    and [video](https://www.youtube.com/watch?v=kmbS6FDQh7c). You will find striking
    similarities between LIamaIndex and LangChain in their functionalities including
    indexing, semantic search, retrieval, and vector databases. They both excel in
    tasks like question answering, document summarization, and building chatbots.
  prefs: []
  type: TYPE_NORMAL
- en: However, each of them has its unique areas of focus. LangChain, with its extensive
    list of features, casts a wider net, concentrating on the use of chains and agents
    to connect with external APIs. On the other hand, LlamaIndex has a narrower focus
    shining in the area of data indexing and document retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: How to use LlamaIndex with LangChain?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Interestingly, LIamaIndex and LangChain aren’t mutually exclusive. In fact,
    you can use both in your LLM applications. You can use both LlamaIndex’s data
    loader and query engine and LangChain’s agents. I know a lot of people actually
    use both of these tools in their projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example where we used LlamaIndex to keep the chat history when using
    a LangChain agent. When we ask “what’s my name?” in the second round of conversation,
    the language model knows that “I am Bob” from the first round of conversation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/4b644e445698a714a2618c1af1b0c061.png)'
  prefs: []
  type: TYPE_IMG
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In summary, LlamaIndex is an incredibly powerful tool for enhancing the capabilities
    of Large Language Models with your own data. Its array of data connectors, advanced
    query interfaces, and flexible integration make it a vital component in the development
    of applications with LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thank you Jerry Liu for the advice and feedback!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7f6ab4af852a267c4deacaef1b99aa63.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Danielle Barnes](https://unsplash.com/@ghost_cat?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/RUSijmFDm0M?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: . . .
  prefs: []
  type: TYPE_NORMAL
- en: By [Sophia Yang](https://www.linkedin.com/in/sophiamyang/) on June 19, 2023
  prefs: []
  type: TYPE_NORMAL
- en: Sophia Yang is a Senior Data Scientist. Connect with me on [LinkedIn](https://www.linkedin.com/in/sophiamyang/),
    [Twitter](https://twitter.com/sophiamyang), and [YouTube](https://www.youtube.com/SophiaYangDS)
    and join the DS/ML [Book Club](https://dsbookclub.github.io/) ❤️
  prefs: []
  type: TYPE_NORMAL
