# 相似性搜索，第二部分：产品量化

> 原文：[https://towardsdatascience.com/similarity-search-product-quantization-b2a1a6397701?source=collection_archive---------1-----------------------#2023-05-10](https://towardsdatascience.com/similarity-search-product-quantization-b2a1a6397701?source=collection_archive---------1-----------------------#2023-05-10)

## 学习一种强大的技术来有效地压缩大量数据

[](https://medium.com/@slavahead?source=post_page-----b2a1a6397701--------------------------------)[![Vyacheslav Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----b2a1a6397701--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b2a1a6397701--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b2a1a6397701--------------------------------) [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----b2a1a6397701--------------------------------)

·

[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimilarity-search-product-quantization-b2a1a6397701&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----b2a1a6397701---------------------post_header-----------) 发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b2a1a6397701--------------------------------) ·9分钟阅读·2023年5月10日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb2a1a6397701&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimilarity-search-product-quantization-b2a1a6397701&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----b2a1a6397701---------------------clap_footer-----------)

--

[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb2a1a6397701&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimilarity-search-product-quantization-b2a1a6397701&source=-----b2a1a6397701---------------------bookmark_footer-----------)![](../Images/cb55243dbdb7fd2bccdc8007780f6427.png)

**相似性搜索**是一个问题，其中给定一个查询，目标是找到与其最相似的数据库文档。

# 介绍

在数据科学中，相似性搜索常出现在自然语言处理（NLP）领域、搜索引擎或推荐系统中，需要为查询检索最相关的文档或项。存在多种方法来提升在大量数据中的搜索性能。

在这篇文章系列的[第一部分](https://medium.com/@slavahead/similarity-search-part-1-7cab80cc0e79)中，我们研究了 kNN 和倒排文件索引结构来执行相似性搜索。正如我们所了解的，kNN 是最直接的方法，而倒排文件索引则在其之上运行，建议在速度加速和准确性之间进行权衡。然而，这两种方法都不使用数据压缩技术，这可能会导致内存问题，特别是在大数据集和有限 RAM 的情况下。在本文中，我们将尝试通过另一种方法来解决这个问题，这种方法被称为 **产品量化**。

[](/similarity-search-knn-inverted-file-index-7cab80cc0e79?source=post_page-----b2a1a6397701--------------------------------) [## 相似性搜索，第 1 部分：kNN 和倒排文件索引

### 相似性搜索是一个流行的问题，其中给定查询 Q，我们需要在所有文档中找到最相似的文档…

[towardsdatascience.com](/similarity-search-knn-inverted-file-index-7cab80cc0e79?source=post_page-----b2a1a6397701--------------------------------)

# 定义

**产品量化** 是一个过程，其中每个数据集向量被转换成一种简短的内存高效表示（称为 **PQ 代码**）。与其完全保存所有向量，不如存储它们的简短表示。同时，产品量化是一种有损压缩方法，会导致预测准确性降低，但在实际应用中，该算法效果很好。

> 一般来说，量化是将无限值映射到离散值的过程。

# 训练

首先，算法将每个向量分成几个相等的部分 —— **子向量**。所有数据集向量的各个部分形成独立的 **子空间** 并分别处理。然后，对每个子空间的向量执行聚类算法。这样，每个子空间中会生成多个质心。每个子向量使用它所属质心的 ID 进行编码。此外，所有质心的坐标也会被存储以供后续使用。

> 子空间质心也称为 **量化向量**。
> 
> 在产品量化中，集群 ID 通常被称为 **重现值**。

*注意。* 在下面的图示中，矩形代表包含多个值的向量，而正方形表示单个数字。

![](../Images/12f7d9a6c6937706fb94959292ab8b55.png)

使用量化进行编码

因此，如果原始向量被分成 *n* 部分，那么它可以由 *n* 个数字 —— 每个子向量的相应质心的 ID 来编码。通常，创建的质心数量 *k* 通常选择为 2 的幂，以便更有效地使用内存。这样，存储一个编码向量所需的内存是 *n * log(k)* 位。

> 每个子空间内所有质心的集合称为 **代码本**。对所有子空间运行 n 个聚类算法会生成 n 个独立的代码本。

## 压缩示例

想象一个原始向量大小为1024，存储浮点数（32位），被划分为*n = 8*个子向量，每个子向量由*k = 256*个聚类中的一个进行编码。因此，编码单个聚类的ID需要*log(256) = 8*位。让我们比较这两种情况下向量表示的内存大小：

+   原始向量：1024 * 32位 = 4096字节。

+   编码向量：8 * 8位 = 8字节。

最终压缩比为512倍！这就是产品量化的真正力量。

![](../Images/b07e898ad69ea53472c6bf06834cc760.png)

量化示例。向量中的数字显示了它存储了多少数字。

以下是一些重要的备注：

+   该算法可以在一个向量子集上进行训练（例如，创建聚类），并用于另一个子集：一旦算法被训练，另一个向量数据集会传递过来，其中新向量使用已经构建的每个子空间的质心进行编码。

+   通常，k-means被选择作为聚类算法。它的一个优点是，聚类数*k*是一个超参数，可以根据内存使用要求手动定义。

# 推断

为了更好地理解，让我们首先看几个天真的方法并找出它们的缺点。这也将帮助我们理解为什么它们通常不应该被使用。

## 天真的方法

第一种天真的方法包括通过连接每个向量的相应质心来解压所有向量。之后，可以从查询向量到所有数据集向量计算*L2*距离（或其他度量）。显然，这种方法是可行的，但非常耗时，因为进行的是暴力搜索，并且距离计算是在高维解压向量上进行的。

另一种可能的方法是将查询向量拆分为子向量，并计算每个查询子向量与基于其PQ代码的数据库向量的相应量化向量的距离之和。因此，暴力搜索技术再次被使用，并且这里的距离计算仍然需要原始向量维度的线性时间，与前一种情况相同。

![](../Images/99a6b8413b311ed9fdfe1a395b43dc71.png)

使用天真的方法计算近似距离。示例显示了作为度量的欧几里得距离。

另一种可能的方法是将查询向量编码为PQ代码。然后直接利用这个PQ代码计算与所有其他PQ代码的距离。具有最短距离的对应PQ代码的数据集向量被认为是查询的最近邻。这种方法比前两种方法更快，因为距离总是在低维PQ代码之间计算。然而，PQ代码由聚类ID组成，这些ID没有太多语义意义，可以被明确视为实际变量的类别变量。显然，这是一种不好的做法，这种方法可能导致预测质量差。

## 优化方法

查询向量被分为子向量。对于每个子向量，计算其到相应子空间中所有质心的距离。最终，这些信息存储在表格 *d* 中。

![](../Images/a5e93d2c4449b27fd168741abe6ab6b7.png)

获取一个表格 *d* 存储部分查询子向量到质心的距离

> 计算出的子向量到质心的距离通常被称为 **部分距离**。

通过使用这个子向量到质心的距离表 *d*，可以通过其 PQ 代码轻松获得查询到任何数据库向量的近似距离：

1.  对于数据库向量的每个子向量，找到最接近的质心 *j*（通过使用 PQ 代码中的映射值），并从质心到查询子向量 *i* 的部分距离 *d[i][j]*（通过使用计算矩阵 *d*）被取用。

1.  所有的部分距离被平方并求和。通过对该值开方，可以获得近似的欧几里得距离。如果你想了解如何获得其他度量的近似结果，请导航到 *“其他距离度量的近似”* 部分。

![](../Images/56ccf26207115c19fa0f848988c326e4.png)

通过使用 PQ 代码和距离表计算查询向量到数据库向量的距离

> 使用这种方法计算近似距离假设部分距离 **d** 非常接近查询与数据库子向量之间的实际距离 **a**。

然而，这种条件可能无法满足，特别是当数据库子向量与其质心之间的距离 *c* 较大时。在这种情况下，计算结果的准确性较低。

![](../Images/3ef376fc19518ee9221a7c32c4845ac5.png)

左侧的示例展示了一个良好的近似情况，当实际距离非常接近部分距离（*c* 较小）。右侧则展示了一个不良情况，因为部分距离远大于实际距离（*c* 较大）。

在获取所有数据库行的近似距离后，我们搜索具有最小值的向量。这些向量将是查询的最近邻。

## 其他距离度量的近似

到目前为止，我们已经了解了如何通过部分距离来近似欧几里得距离。让我们将规则推广到其他度量。

假设我们想要计算一对向量之间的距离度量。如果我们知道度量的公式，我们可以直接应用它来得到结果。但有时我们可以按以下方式分步进行：

+   两个向量都被分成 *n* 个子向量。

+   对于每一对对应的子向量，计算距离度量。

+   计算出的 *n* 个度量然后被组合以生成原始向量之间的实际距离。

![](../Images/8de585683199aca7b1de05a38e58d375.png)

图中显示了计算度量的两种方法。左侧，度量公式直接应用于两个向量。右侧，为每对对应的子向量计算部分距离，然后通过使用聚合函数h、g和f进行组合。

欧几里得距离是可以按部分计算的度量的一个例子。根据上图，我们可以选择聚合函数为 *h(z) = z²*，*g(z₀, z₁, …, zₙ) = sum(z₀, z₁, …, zₙ)* 和 *f(z) = √z*。

![](../Images/d9b2df072d03fd046dadb5505fe76fb5.png)

欧几里得距离可以按部分计算

内积是另一种度量的例子，具有聚合函数 *h(z) = z, g(z₀, z₁, …, zₙ) = sum(z₀, z₁, …, zₙ) 和 f(z) = z*。

在产品量化的背景下，这是一个非常重要的属性，因为在推断过程中，算法按部分计算距离。这意味着使用没有此属性的度量来进行产品量化会更加困难。余弦距离就是这种度量的一个例子。

如果仍然需要使用没有此属性的度量，则需要应用额外的启发式方法来聚合带有一定误差的部分距离。

## 性能

产品量化的主要优势是数据库向量的巨大压缩，这些向量作为简短的PQ代码存储。对于某些应用，这种压缩率甚至可能超过95%! 然而，除了PQ代码外，还需要存储大小为 *k* x *n* 的矩阵 *d*，其中包含每个子空间的量化向量。

> 产品量化是一种有损压缩方法，因此压缩越高，预测准确性下降的可能性就越大。

建立一个高效表示的系统需要训练多个聚类算法。除此之外，在推断过程中，*k * n* 部分距离需要以暴力方式计算并为每个数据库向量求和，这可能需要一些时间。

![](../Images/9b0d9230996b8ee80f0ce38d117df577.png)

产品量化性能

# Faiss实现

> [**Faiss**](https://github.com/facebookresearch/faiss)（Facebook AI搜索相似性）是一个用C++编写的Python库，用于优化相似性搜索。该库展示了不同类型的索引，这些索引是用于高效存储数据和执行查询的数据结构。

根据[Faiss文档](https://faiss.ai)的信息，我们将看到产品量化是如何被利用的。

产品量化在*IndexPQ*类中实现。初始化时，我们需要提供3个参数：

+   **d**：数据中的维度数量。

+   **M**：每个向量的拆分数量（与上文中使用的*n*相同的参数）。

+   **nbits**：编码单个聚类ID所需的位数。这意味着单个子空间中的总聚类数将等于 *k = 2^nbits*。

对于相等子空间维度的拆分，参数 *dim* 必须可以被 *M* 整除。

存储单个向量所需的总字节数等于：

![](../Images/9ee1277db1e10ce70bdf5a1c71816b2a.png)

> 如上公式所示，为了更有效地利用内存，M * nbits 的值应能被 *8* 整除。

![](../Images/dd96a42e85819a4d310f507facbe232f.png)

Faiss 对 IndexPQ 的实现

# 结论

我们已经探讨了信息检索系统中一种非常流行的算法，该算法能有效地压缩大量数据。其主要缺点是推理速度较慢。尽管如此，该算法在现代大数据应用中被广泛使用，特别是与其他相似性搜索技术结合时。

在文章系列的第一部分中，我们描述了倒排文件索引的工作流程。实际上，我们可以将这两种算法合并成一个更高效的算法，兼具两者的优势！这正是我们将在本系列的下一部分中要做的。

[## 相似性搜索，第3部分：融合倒排文件索引和产品量化](https://medium.com/@slavahead/similarity-search-blending-inverted-file-index-and-product-quantization-a8e508c765fa?source=post_page-----b2a1a6397701--------------------------------)

### 在本系列的前两部分中，我们讨论了信息检索中的两个基本算法：倒排索引……

[medium.com](https://medium.com/@slavahead/similarity-search-blending-inverted-file-index-and-product-quantization-a8e508c765fa?source=post_page-----b2a1a6397701--------------------------------)

# 资源

+   [最近邻搜索的产品量化](https://lear.inrialpes.fr/pubs/2011/JDS11/jegou_searching_with_quantization.pdf)

+   [Faiss 文档](https://faiss.ai)

+   [Faiss 仓库](https://github.com/facebookresearch/faiss)

+   [Faiss 索引总结](https://github.com/facebookresearch/faiss/wiki/Faiss-indexes)

*除非另有说明，否则所有图片均由作者提供。*
