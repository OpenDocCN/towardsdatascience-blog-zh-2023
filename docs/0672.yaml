- en: Selecting the Right XGBoost Loss Function in SageMaker
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 SageMaker 中选择正确的 XGBoost 损失函数
- en: 原文：[https://towardsdatascience.com/selecting-the-right-xgboost-loss-function-in-sagemaker-60e545a75c47?source=collection_archive---------4-----------------------#2023-02-18](https://towardsdatascience.com/selecting-the-right-xgboost-loss-function-in-sagemaker-60e545a75c47?source=collection_archive---------4-----------------------#2023-02-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/selecting-the-right-xgboost-loss-function-in-sagemaker-60e545a75c47?source=collection_archive---------4-----------------------#2023-02-18](https://towardsdatascience.com/selecting-the-right-xgboost-loss-function-in-sagemaker-60e545a75c47?source=collection_archive---------4-----------------------#2023-02-18)
- en: When and why you should use absolute or squared error
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 何时以及为何应使用绝对误差或平方误差
- en: '[](https://medium.com/@andrewcharabin?source=post_page-----60e545a75c47--------------------------------)[![Andrew
    Charabin](../Images/8cfe2657a9cd16c3ce30b98e3c9e9945.png)](https://medium.com/@andrewcharabin?source=post_page-----60e545a75c47--------------------------------)[](https://towardsdatascience.com/?source=post_page-----60e545a75c47--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----60e545a75c47--------------------------------)
    [Andrew Charabin](https://medium.com/@andrewcharabin?source=post_page-----60e545a75c47--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@andrewcharabin?source=post_page-----60e545a75c47--------------------------------)[![Andrew
    Charabin](../Images/8cfe2657a9cd16c3ce30b98e3c9e9945.png)](https://medium.com/@andrewcharabin?source=post_page-----60e545a75c47--------------------------------)[](https://towardsdatascience.com/?source=post_page-----60e545a75c47--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----60e545a75c47--------------------------------)
    [Andrew Charabin](https://medium.com/@andrewcharabin?source=post_page-----60e545a75c47--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff282e085f18e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fselecting-the-right-xgboost-loss-function-in-sagemaker-60e545a75c47&user=Andrew+Charabin&userId=f282e085f18e&source=post_page-f282e085f18e----60e545a75c47---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----60e545a75c47--------------------------------)
    ·7 min read·Feb 18, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F60e545a75c47&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fselecting-the-right-xgboost-loss-function-in-sagemaker-60e545a75c47&user=Andrew+Charabin&userId=f282e085f18e&source=-----60e545a75c47---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff282e085f18e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fselecting-the-right-xgboost-loss-function-in-sagemaker-60e545a75c47&user=Andrew+Charabin&userId=f282e085f18e&source=post_page-f282e085f18e----60e545a75c47---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----60e545a75c47--------------------------------)
    ·7 min 阅读·2023年2月18日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F60e545a75c47&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fselecting-the-right-xgboost-loss-function-in-sagemaker-60e545a75c47&user=Andrew+Charabin&userId=f282e085f18e&source=-----60e545a75c47---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F60e545a75c47&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fselecting-the-right-xgboost-loss-function-in-sagemaker-60e545a75c47&source=-----60e545a75c47---------------------bookmark_footer-----------)![](../Images/2cec24a6f11dc5dcb108a41c490780fc.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F60e545a75c47&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fselecting-the-right-xgboost-loss-function-in-sagemaker-60e545a75c47&source=-----60e545a75c47---------------------bookmark_footer-----------)![](../Images/2cec24a6f11dc5dcb108a41c490780fc.png)'
- en: '*Image via VectorStock under license to Andrew Charabin*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*图片来源于 VectorStock，版权归 Andrew Charabin 所有*'
- en: '[XGBoost](https://xgboost.readthedocs.io/en/stable/) is an open source software
    library to apply the gradient boosting framework to supervised learning (SL) tasks.
    Because of its effectiveness, speed, and robustness in solving a wide variety
    of SL problems, a current saying is that the best type of model for your SL problem
    is either a deep learning algorithm or XGBoost. As a result of its well-deserved
    popularity, XGBoost is available as a built-in model in Amazon SageMaker, a cloud
    machine learning platform.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[XGBoost](https://xgboost.readthedocs.io/en/stable/) 是一个开源软件库，用于将梯度提升框架应用于监督学习（SL）任务。由于其在解决各种
    SL 问题中的有效性、速度和稳健性，目前的说法是，针对你的 SL 问题，最佳的模型类型要么是深度学习算法，要么是 XGBoost。由于其名副其实的流行，XGBoost
    已作为内置模型在 Amazon SageMaker 这款云机器学习平台中提供。'
- en: With minimal knowledge of the XGBoost framework, any data scientist can easily
    plug in their dataset and produce an XGBoost model in SageMaker. Since SageMaker
    offers baysian hyperparameter tuning, which will retrain and select the best model
    across different hyperparameters, users can get away without fully understanding
    key inputs like max_depth and eta. However, there is often little consideration
    of the loss function (objective) to use when training XGBoost, even though it
    may be the most important human decision in the process. This article aims to
    provide more insight into how to select the right loss function when training
    XGBoost or other SL models for regression tasks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 凭借对XGBoost框架的基本了解，任何数据科学家都可以轻松地将数据集插入到SageMaker中并生成一个XGBoost模型。由于SageMaker提供了贝叶斯超参数调优，它将重新训练并选择不同超参数下的最佳模型，用户可以无需完全理解诸如max_depth和eta等关键输入。然而，在训练XGBoost时，通常很少考虑要使用的损失函数（目标），尽管它可能是过程中的最重要的人为决策。本文旨在提供更多关于如何在训练XGBoost或其他回归任务的SL模型时选择正确损失函数的见解。
- en: Let’s start by explaining gradient boosting. A helpful start is to understand
    what isn’t gradient boosting —one example being the ensemble decision tree algorithm,
    random forest. Ensemble decision tree algorithms are those that aim to produce
    several ‘weak-learner’ trees that when used in a network produce a ‘strong learner’.
    Let’s say we have an ensemble of 100 trees, random forest conceptually produces
    each of these trees in parallel, using ‘bagging’ to select a subset of observations
    and dimensions to use to produce each tree, then takes a weighted average of the
    predictions of all 100 trees to get the final prediction for a regression problem.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从解释梯度提升算法开始。一个有用的起点是理解什么不是梯度提升——例如集成决策树算法随机森林。集成决策树算法的目标是生成多个“弱学习者”树，这些树在网络中组合起来时可以产生一个“强学习者”。假设我们有一个由100棵树组成的集成，随机森林从概念上讲是并行生成这些树，使用“自助采样”选择观察值和维度的子集来生成每棵树，然后对所有100棵树的预测结果进行加权平均，以获得回归问题的最终预测。
- en: The key difference with XGBoost is that individual trees can’t be produced in
    parallel, they must be produced in sequence. Specifically, each subsequent tree
    in an XGBoost model is used to predict the error of the preceding tree. By using
    an ensemble approach with both ‘bagging’ (pulling different observations and dimensions
    out of a bag for each tree), and ‘boosting’ (building trees in sequence), XGBoost
    framework is able to reduce model bias without overfitting. Furthermore, XGBoost
    and other tree-based models are robust to high-variance datasets with low-importance
    dimensions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 与XGBoost的关键区别在于，单棵树不能并行生成，必须按顺序生成。具体来说，XGBoost模型中的每棵后续树用于预测前一棵树的误差。通过结合“自助采样”（从一个袋子中提取不同的观察值和维度）和“提升”（按顺序构建树）的集成方法，XGBoost框架能够减少模型偏差而不出现过拟合。此外，XGBoost和其他基于树的模型对高方差数据集和低重要性维度具有鲁棒性。
- en: Now that we understand how XGBoost works from a high-level, let’s return the
    loss function. Loss functions take in model predictions for a set of observations
    and tell you how far they were from actual outcomes. For regression problems,
    the focus of this discussion, the simplest loss function that comes to the human
    mind is mean absolute error — we subtract the predictions from the actual values
    (or vice versa), take the absolute value, then take an average across all observations.
    In other words, missing by 200 is twice as bad as missing by 100.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们从高层次上理解了XGBoost的工作原理，让我们回到损失函数上。损失函数接受一组观察值的模型预测，并告诉你这些预测距离实际结果有多远。对于回归问题，本讨论的重点是人们脑海中最简单的损失函数是均绝对误差——我们从实际值中减去预测值（或反之），取绝对值，然后对所有观察值取平均。换句话说，错了200比错了100要糟糕两倍。
- en: But the standard loss function used for most XGBoost use cases is ‘reg:squarederror’,
    formerly called ‘reg:linear’ in SageMaker. The only difference with squared error
    and absolute error, as the name indicates, is that errors for each observation
    are squared before being averaged. Because squaring has a compounding effect as
    the size of errors increases, mean squared error penalizes large misses proportionally
    more than mean absolute error does. Missing by 200 is now 4X as bad as missing
    by 100.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 但大多数XGBoost用例使用的标准损失函数是‘reg:squarederror’，在SageMaker中以前称为‘reg:linear’。正如名字所示，平方误差和绝对误差的唯一区别在于，错误在被平均之前会被平方。由于平方效应在错误大小增加时具有累积效应，均方误差对大错误的惩罚程度比均绝对误差更大。错了200现在比错了100要糟糕4倍。
- en: But why would mean squared error be the standard, unquestioned, loss function
    used to train XGBoost and other SL models when it’s just not as simple or intuitively
    sound as mean absolute error?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 但为什么均方误差会成为训练XGBoost和其他SL模型时的标准、无可争议的损失函数，而它并不像均值绝对误差那样简单或直观？
- en: The first distinction to understand is that of prediction and estimation loss.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要理解的区别是预测损失和估计损失。
- en: '![](../Images/64404893c2b3adf07abbc891ed349f22.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/64404893c2b3adf07abbc891ed349f22.png)'
- en: (Chart by author)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: （图表由作者提供）
- en: Prediction loss is the loss incurred by the business, while estimation loss
    is how far the model is from the ‘ground truth’ model. The right estimation loss
    function isn’t always the one that minimizes prediction loss. As I’ll show later,
    you can reduce prediction loss by choosing a different estimation loss function.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 预测损失是业务所承担的损失，而估计损失是模型与“真实情况”模型的距离。正确的估计损失函数并不总是最小化预测损失的函数。正如我稍后所展示的，你可以通过选择不同的估计损失函数来减少预测损失。
- en: Given mean average error does well at quantifying the penalty of missing in
    many real world cases, why does mean squared error often win when estimating the
    best model parameters?
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 给定的均值平均误差在许多实际案例中很好地量化了遗漏的惩罚，为什么均方误差在估计最佳模型参数时常常胜出？
- en: It all comes down to the practical assumption of model errors being normally
    distributed.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一切归结为模型误差被假设为正态分布的实际假设。
- en: '![](../Images/75335081e3c89dbcd0d1b61439bf8be4.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75335081e3c89dbcd0d1b61439bf8be4.png)'
- en: (Chart by author)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: （图表由作者提供）
- en: Let’s assume that there are two components required to perfectly explain an
    outcome, Y. First, the signal that can be derived from model dimensions. In this
    case, let’s assume a linear model with a single dimension and a bias term is sufficient
    (mX + b below). And second, a random variable, Z, that follows a normal distribution
    with independent variance.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有两个组件需要完美地解释一个结果Y。首先，是可以从模型维度中得出的信号。在这种情况下，假设一个具有单一维度和偏置项的线性模型是足够的（下面的mX +
    b）。其次，是一个遵循独立方差正态分布的随机变量Z。
- en: The best estimation loss function in a sense is the one that when applied to
    a model maximizes the likelihood of having produced the observed data. In this
    case, it turns out to be MSE.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从某种意义上说，最佳的估计损失函数是当应用于模型时最大化产生观察到的数据的可能性。在这种情况下，它被证明是均方误差。
- en: '![](../Images/8c43ad8d247cbd12720835461b0714a6.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c43ad8d247cbd12720835461b0714a6.png)'
- en: The appearance of the squaring can be traced back to the probability density
    function of the the normal distribution, which is essential to producing the bell
    shape.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 平方项的出现可以追溯到正态分布的概率密度函数，这对产生钟形曲线是必不可少的。
- en: But what if errors and specifically the random component, Z, are not distributed
    normally? Another common prediction error distribution observed in regression
    problems is the Laplace distribution.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果误差，特别是随机成分Z，并不是正态分布呢？在回归问题中观察到的另一种常见预测误差分布是拉普拉斯分布。
- en: '![](../Images/cc9524beda462e22fd481dc6662f7a35.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc9524beda462e22fd481dc6662f7a35.png)'
- en: (Chart by author)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: （图表由作者提供）
- en: When observing the probability density function of the Laplace distribution,
    the absolute error formula immediately sticks out. The lack of squaring results
    in an spike in the distribution about 0 on the X axis.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当观察到拉普拉斯分布的概率密度函数时，绝对误差公式立即显现出来。缺乏平方导致分布在X轴上的0附近出现尖峰。
- en: '![](../Images/20bd1165b7a2da2efed3419f0a8faf74.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20bd1165b7a2da2efed3419f0a8faf74.png)'
- en: Furthermore, it turns out that mean absolute error is the loss function that
    maximizes likelihood of observing data when model errors follow the Laplace distribution.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，事实证明，当模型误差遵循拉普拉斯分布时，均值绝对误差是最大化观察到数据的可能性的损失函数。
- en: Normal and Laplace distributions are only a couple of the error distributions
    that can be observed in regression tasks, however. For problems with count-based
    data, errors may follow a Poisson distribution. So in other cases, additional
    research be needed to find the matching error distribution and associated loss
    function that will lead to the best parameter estimates.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正态分布和拉普拉斯分布只是回归任务中可以观察到的几种误差分布。对于基于计数的数据问题，误差可能遵循泊松分布。因此，在其他情况下，可能需要进一步研究以找到匹配的误差分布和相关的损失函数，以便获得最佳的参数估计。
- en: Without going any further, we already have an improved algorithm to select the
    the right loss function for an XGBoost model.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在不进一步讨论的情况下，我们已经有了一种改进的算法来选择适用于XGBoost模型的正确损失函数。
- en: Produce an arbitrary model.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个任意模型。
- en: Observe errors and whether they closely resemble a normal or Laplace distribution.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察误差，并检查它们是否与正态分布或拉普拉斯分布相似。
- en: If they do, select a loss function of mean squared or mean absolute error, respectively.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果相似，请分别选择均方误差或平均绝对误差作为损失函数。
- en: If not, research and test alternative distributions that match observed errors
    and are common for similar problems. Then, find the loss function that will lead
    to the most likely parameter estimates.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果不相似，请研究并测试与观察到的误差匹配且对类似问题常见的其他分布。然后，找到能够导致最可能的参数估计的损失函数。
- en: As a business case for the application of this simple algorithm, I will share
    a real SL regression case I encountered.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 作为这个简单算法应用的商业案例，我将分享一个我遇到的真实SL回归案例。
- en: An XGBoost model with a mean squared error loss was trained leading to the following
    prediction error distribution.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用均方误差损失训练的XGBoost模型产生了如下的预测误差分布。
- en: '![](../Images/26c35b44e2cc2a0119115c216a783196.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26c35b44e2cc2a0119115c216a783196.png)'
- en: (Chart by author)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: （图表由作者提供）
- en: 2\. Prediction errors closely resembled the Laplace distribution.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 预测误差与拉普拉斯分布非常相似。
- en: 3\. Mean average error should be used as the loss function.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 应该使用平均绝对误差作为损失函数。
- en: The only issue is that XGBoost and deep learning algorithms, our go-tos to solve
    the laundry list of SL problems, require a smooth loss function to perform gradient
    descent (or comparable optimization algorithm). So we need to find a smooth loss
    function that closely resembles mean absolute error. The best option available
    in SageMaker is pseudo-huber; the huber distribution is shown below which closely
    resembles absolute error at a delta of 1.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的问题是，XGBoost和深度学习算法，作为我们解决众多SL问题的首选，需要平滑的损失函数来进行梯度下降（或类似的优化算法）。因此，我们需要找到一个平滑的损失函数，其与平均绝对误差非常相似。在SageMaker中最好的选择是伪Hubbard分布；下面显示的Hubbard分布在delta为1时与绝对误差非常接近。
- en: '![](../Images/225d55ea111430b701f4191380c0829c.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/225d55ea111430b701f4191380c0829c.png)'
- en: (Chart by author)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: （图表由作者提供）
- en: So for my use case, what were the impacts of switching from a squared error
    loss function to pseudo-huber and mean absolute error for model training and hyperparameter
    tuning?
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，对于我的用例来说，从均方误差损失函数切换到伪Hubbard和平均绝对误差对模型训练和超参数调优有什么影响？
- en: '![](../Images/d04a2d929ca02a4916b55c6cf7af3b8b.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d04a2d929ca02a4916b55c6cf7af3b8b.png)'
- en: (Chart by author)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: （图表由作者提供）
- en: In the chart, you can see the mean absolute error (MAE) dropped by 30% when
    pseudo-huber loss was paired with MAE hyperparameter tuning. More interestingly,
    even if the most important objective to the business was root mean squared error
    (RMSE), using an estimation loss function of mean absolute error resulted in improved
    business outcomes.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在图表中，你可以看到，当伪Hubbard损失与MAE超参数调优配对时，平均绝对误差（MAE）下降了30%。更有趣的是，即使对业务最重要的目标是均方根误差（RMSE），使用均值绝对误差作为估计损失函数仍然能带来改善的业务成果。
- en: Closing remarks
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结束语
- en: While XGBoost and comparable ensemble decision tree models are making it easy
    for data scientists to plug and play with their different datasets and achieve
    good model performance with minimal effort, there is a price to pay for surface
    level understanding. Selecting the right loss function is one simple example which
    I showed can lead to a 30%+ model improvement.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然XGBoost和类似的集成决策树模型使数据科学家能够轻松地在不同的数据集上进行插件式操作，并以最小的努力获得良好的模型性能，但这种表面上的理解是有代价的。选择正确的损失函数是一个简单的例子，我展示了它可以带来30%以上的模型改进。
- en: On a deeper level, data science is all about asking why, pursuing the important
    details to illuminate the answer, and discarding the ones that are accessory.
    It requires intuition, pursuing your curiosity, digging in further when surprised,
    and breaking down a problem from the big picture to the fine print. This article
    outlines my journey to understand why mean squared error is considered the standard
    loss function for XGBoost regression tasks. The result is a simple system to prescribe
    the best loss function for a variety of cases. I hope this article was of use
    to you as you progress on your own pursuit to understand why and use what you
    uncover to help tackle important business challenges.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 从更深层次来看，数据科学就是不断问“为什么”，追求重要的细节以揭示答案，并舍弃那些附属的细节。它需要直觉，追求好奇心，遇到惊讶时深入探究，并从大局到细节拆解问题。本文概述了我理解为什么均方误差被认为是XGBoost回归任务的标准损失函数的过程。结果是一个简单的系统，可以为各种情况推荐最佳损失函数。希望这篇文章对你在理解为什么以及使用你发现的东西来帮助解决重要的商业挑战的过程中有所帮助。
- en: '*Thanks for reading! If you liked this article, follow me to get notified of
    my new posts. Also, feel free to share any comments/suggestions.*'
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*感谢阅读！如果你喜欢这篇文章，请关注我以获取我新帖子通知。同时，欢迎随时分享任何评论/建议。*'
