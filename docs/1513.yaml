- en: All Languages Are NOT Created (Tokenized) Equal
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/all-languages-are-not-created-tokenized-equal-cd87694a97c1?source=collection_archive---------5-----------------------#2023-05-03](https://towardsdatascience.com/all-languages-are-not-created-tokenized-equal-cd87694a97c1?source=collection_archive---------5-----------------------#2023-05-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Language models cost much more in some languages than others
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@artfish?source=post_page-----cd87694a97c1--------------------------------)[![Yennie
    Jun](../Images/b635e965f21c3d55833269e12e861322.png)](https://medium.com/@artfish?source=post_page-----cd87694a97c1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cd87694a97c1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cd87694a97c1--------------------------------)
    [Yennie Jun](https://medium.com/@artfish?source=post_page-----cd87694a97c1--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F12ca1ab81192&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-languages-are-not-created-tokenized-equal-cd87694a97c1&user=Yennie+Jun&userId=12ca1ab81192&source=post_page-12ca1ab81192----cd87694a97c1---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cd87694a97c1--------------------------------)
    ·12 min read·May 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcd87694a97c1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-languages-are-not-created-tokenized-equal-cd87694a97c1&user=Yennie+Jun&userId=12ca1ab81192&source=-----cd87694a97c1---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcd87694a97c1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-languages-are-not-created-tokenized-equal-cd87694a97c1&source=-----cd87694a97c1---------------------bookmark_footer-----------)![](../Images/d5d82c8ce5ba774594d82455e802916d.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: “hey” translated to 52 different languages. The size of the text is scaled to
    corresponds to the number of tokens needed to represent the message in the corresponding
    language. Image created by author.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '*Original article was posted on my* [*blog*](https://www.artfish.ai/p/all-languages-are-not-created-tokenized)*.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Large language models such as ChatGPT process and generate text sequences by
    first splitting the text into smaller units called **tokens**. In the image below,
    each colored block represents a unique token. Short or common words such as “you”,
    “say”, “loud”, and “always” are its own token, whereas longer or less common words
    such as “atrocious”, “precocious”, and “supercalifragilisticexpialidocious” are
    broken into smaller subwords.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型如ChatGPT通过首先将文本拆分成称为**标记**的较小单元来处理和生成文本序列。在下图中，每个彩色块代表一个独特的标记。像“you”、“say”、“loud”和“always”这样的短或常见词是自己的标记，而像“atrocious”、“precocious”和“supercalifragilisticexpialidocious”这样的较长或不常见的词则被拆分成更小的子词。
- en: '![](../Images/17d355ca37bbda6bda1137a1956d5f38.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17d355ca37bbda6bda1137a1956d5f38.png)'
- en: Visualization of tokenization of a short text using [OpenAI’s tokenizer website](https://platform.openai.com/tokenizer).
    Screenshot taken by author.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[OpenAI的标记器网站](https://platform.openai.com/tokenizer)可视化短文本的标记化。截图由作者提供。
- en: This process of **tokenization** is not uniform across languages, leading to
    disparities in the number of tokens produced for equivalent expressions in different
    languages. For example, **a sentence in Burmese or Amharic may require 10x more
    tokens than a similar message in English.**
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这种**标记化**过程在不同语言中并不统一，导致在不同语言中产生的标记数量存在差异。例如，**缅甸语或阿姆哈拉语中的一个句子可能需要比英语中的类似信息多出10倍的标记**。
- en: '![](../Images/77a9be17c8b6729603e33808832bd519.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77a9be17c8b6729603e33808832bd519.png)'
- en: An example of the same message translated into five languages and the corresponding
    number of tokens required to…
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 将同一信息翻译成五种语言的示例以及所需的相应标记数量…
