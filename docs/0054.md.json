["```py\nimport altair as alt\n\nbars = alt.Chart(df_raw).mark_bar().encode(\n    x='n_trails_interacted:Q',\n    y='userid:O')\n\ntext = bars.mark_text(\n    align='left',\n    baseline='middle',\n    dx=3  # Nudges text to right so it doesn't appear on top of the bar\n).encode(\n    text='n_trails_interacted:Q'\n)\n\n(bars + text).properties(height=400)\n```", "```py\ndef plot_variable(variable):\n    source = df_trail_sub\n    bar = alt.Chart(source).mark_bar().encode(\n        x='count():Q',\n        y=alt.Y(f'{variable}:N', sort='-x')\n    )\n    return bar\n```", "```py\ndef plot_points_bar():\n    brush = alt.selection(type='interval')\n    points = alt.Chart(df_trail_sub).mark_point().encode(\n    x='global_rank_score',\n    y='rating',\n    color=alt.condition(brush, 'difficulty_title', alt.value('lightgray'))\n).add_selection(\n    brush\n)\n\n    bars = alt.Chart(df_trail_sub).mark_bar().encode(\n        y='difficulty_title',\n        color='difficulty_title',\n        x='count(difficulty_title)'\n    ).transform_filter(\n        brush\n    )\n\n    return points & bars\n```", "```py\nalt.Chart(dt).mark_circle().encode(\n    x='difficulty_title:O',\n    y='physical_rating:O',\n    size= alt.Size('pct:Q',scale=alt.Scale(range=[10, 2000])),\n    color= 'physical_rating'\n).properties(width=400, height=300)\n```", "```py\nimport implicit\nimport scipy.sparse as sparse\n\nn_user = df_raw.userid.nunique()\nn_item = df_raw.trailid.nunique()\n\n# Prepare ALS training data\nsparse_user_item = sparse.csr_matrix((df_raw['n_times_interacted'].astype(float), \n                                      (df_raw['userid'], df_raw['trailid'])))\n\n# initialize a model: set random_state for reproducibility!\nmodel = implicit.als.AlternatingLeastSquares(factors=40, \n                                             regularization=0.1, \n                                             iterations=15, \n                                             random_state=10)\n# train the model on a sparse matrix of user/item/confidence weights\nmodel.fit(sparse_user_item)\n```", "```py\nimport requests\nfrom ipywidgets import Image\n\ndef get_recommendations(userid, n):\n    rec, relevance = model.recommend(userid, sparse_user_item[userid], \n                                     N=n, \n                                     filter_already_liked_items=True\n                                    )\n\n    df_rec = df_trail.loc[df_trail['trailid'].isin(rec)]\n    return df_rec\n\ndef show_photo(df_rec):\n    photo_url = df_rec['cover_photo_url'].iloc[0]\n    image = Image(value=requests.get(photo_url).content)\n    return image\n\ndef show_map(df_rec):\n    map_url = df_rec['static_map_url'].iloc[0]\n    image = Image(value=requests.get(map_url).content)\n    return image\n```", "```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import pairwise_distances\n\ndef model_tfidf_num(trail_data):\n\n    # scale numerical data (continuous) : global_rank_score missing values\n    trail_data_numerical = trail_data[['stat_climb', 'stat_descent',\n                                       'stat_distance','rating']]\n    scaler_num = StandardScaler().fit(trail_data_numerical)\n    df_num_scaled = scaler_num.transform(trail_data_numerical)\n\n    # vectorize text data\n    tfidf = TfidfVectorizer()\n    tfidf_matrix = tfidf.fit_transform(trail_data['comb_text_clean']).toarray()\n\n    # concatenate and get similarity\n    all_features = np.concatenate([tfidf_matrix,df_num_scaled],axis=1)\n    #cosine_sim = cosine_similarity(all_features, all_features)\n\n    return all_features\n\ndef get_similar(idx, n):\n\n#idx: target item's index\n\n    # 1\\. compute distance\n    target_feature = all_features[idx]\n    couple_dist = pairwise_distances(all_features,\n                                     target_feature, metric='cosine')\n    # 2\\. get similar dataframe: no need to filter out the first\n    # because the first won't be the unseen url\n    indices = list(\n        map(lambda x: x.item(), np.argsort(couple_dist.ravel())))\n    # similar_score\n    cosine_similarity = 1 - couple_dist[indices].ravel()\n\n    df_sim_all = pd.DataFrame(\n        {\"tfidf_index\": indices, \"similar_score\": cosine_similarity})\n\n    df_sim = df_sim_all[1:n+1]\n    df_out = df_sim.merge(df_mapper, on='tfidf_index')\n\n    return df_out\n```"]