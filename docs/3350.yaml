- en: Using LLMs to evaluate LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/using-llms-to-evaluate-llms-ce390ae575c6?source=collection_archive---------6-----------------------#2023-11-10](https://towardsdatascience.com/using-llms-to-evaluate-llms-ce390ae575c6?source=collection_archive---------6-----------------------#2023-11-10)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@petyak.mi?source=post_page-----ce390ae575c6--------------------------------)[![Maksym
    Petyak](../Images/0c2d4054352a58537c9e76b5911b8b2e.png)](https://medium.com/@petyak.mi?source=post_page-----ce390ae575c6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ce390ae575c6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ce390ae575c6--------------------------------)
    [Maksym Petyak](https://medium.com/@petyak.mi?source=post_page-----ce390ae575c6--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2ab7d66fcd36&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-llms-to-evaluate-llms-ce390ae575c6&user=Maksym+Petyak&userId=2ab7d66fcd36&source=post_page-2ab7d66fcd36----ce390ae575c6---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ce390ae575c6--------------------------------)
    ·7 min read·Nov 10, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fce390ae575c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-llms-to-evaluate-llms-ce390ae575c6&user=Maksym+Petyak&userId=2ab7d66fcd36&source=-----ce390ae575c6---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fce390ae575c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-llms-to-evaluate-llms-ce390ae575c6&source=-----ce390ae575c6---------------------bookmark_footer-----------)![](../Images/44441b26ad2c878ab0d5fbe3ea98a1b6.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image generated by OpenAI’s DALL-E 3.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can ask ChatGPT to act in a million different ways: as your nutritionist,
    language tutor, doctor, etc. It’s no surprise we see a lot of demos and products
    launching on top of the OpenAI API. But while it’s easy to make LLMs act a certain
    way, ensuring they perform well and accurately complete the given task is a completely
    different story.'
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that many criteria that we care about are extremely subjective.
    Are the answers accurate? Are the responses coherent? Was anything hallucinated?
    It’s hard to build quantifiable metrics for evaluation. Mostly, you need human
    judgment, but it’s expensive to have humans check a large number of LLM outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, LLMs have a lot of parameters that you can tune. Prompt, temperature,
    context, etc. You can fine-tune the models on a specific dataset to fit your use
    case. With prompt engineering, even asking a model to take a deep breath [1] or
    making your request more emotional [2] can change performance for the better.
    There is a lot of room to tweak and experiment, but after you change something,
    you need to be able to tell if the system overall got better or worse.
  prefs: []
  type: TYPE_NORMAL
- en: With human labour being slow and expensive, there is a strong incentive to find
    automatic metrics for these more subjective criteria. One interesting approach,
    which is gaining popularity, is using LLMs to evaluate the output from LLMs. After
    all, if ChatGPT can generate a good, coherent response to a question, can it also
    not say if a given text is coherent? This opens up a whole box of potential biases,
    techniques, and opportunities, so let’s dive into it.
  prefs: []
  type: TYPE_NORMAL
- en: LLM biases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have a negative gut reaction about building metrics and evaluators using
    LLMs, your concerns are well-founded. This could be a horrible way to just propagate
    the existing biases.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in the G-Eval paper, which we will discuss in more detail later,
    researchers showed that their LLM-based evaluation gives higher scores to GPT-3.5
    summaries than human-written summaries, even when human judges prefer human-written
    summaries.
  prefs: []
  type: TYPE_NORMAL
- en: Another study, titled [“Large Language Models are not Fair Evaluators”](https://arxiv.org/abs/2305.17926)
    [3], found that when asked to choose which of the two presented options is better,
    there is a significant bias in the order in which order you present the options.
    GPT-4, for example, often preferred the first given option, while ChatGPT the
    second one. You can just ask the same question with the order flipped and see
    how consistent LLMs are in their answers. They subsequently developed techniques
    to mitigate this bias by running the LLM multiple times with different orders
    of options.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the evaluators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the end of the day, we want to know if LLMs can perform as well as or similarly
    to human evaluators. We can still approach this as a scientific problem:'
  prefs: []
  type: TYPE_NORMAL
- en: Set up evaluation criteria.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ask humans and LLMs to evaluate according to the criteria.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the correlation between human and LLM evaluation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This way, we can get an idea of how closely LLMs resemble human evaluators.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, there are already several studies like this, showing that for certain
    tasks, LLMs do a much better job than more traditional evaluation metrics. And
    it’s worth noting that we don’t need a perfect correlation. If we evaluate over
    many examples, even if the evaluation isn’t perfect, we could still get some idea
    of whether the new system is performing better or worse. We could also use LLM
    evaluators to flag the worrying edge cases for human evaluators.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s have a look at some of the recently proposed metrics and evaluators that
    rely on LLMs at their core.
  prefs: []
  type: TYPE_NORMAL
- en: G-Eval
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[G-Eval](https://arxiv.org/abs/2303.16634) [4] works by first outlining the
    evaluation criteria and then simply asking the model to give the rating. It could
    be used for summarisation and dialogue generation tasks, for example.'
  prefs: []
  type: TYPE_NORMAL
- en: 'G-Eval has the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt.** Defines the evaluation task and its criteria.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Intermediate instructions.** Outlines the intermediate instructions for evaluation.
    They actually ask the LLM to generate these steps.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Scoring function.** Instead of taking the LLM score at face value, we look
    under the hood at the token probabilities to get the final score. So, if you ask
    to rate between 1 and 5, instead of just taking whatever number is given by the
    LLM (say “3”), we would look at the probability of each rank and calculate the
    weighted score. This is because researchers found that usually one digit dominates
    the evaluation (e.g. outputting mostly 3), and even when you ask the LLM to give
    a decimal value, it still tends to return integers.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/d17a2fd00441c0db75b8a279d9adb929.png)'
  prefs: []
  type: TYPE_IMG
- en: G-Eval prompt for calculating coherence on a scale 1–5\. You can find more examples
    in the [original paper](https://arxiv.org/pdf/2303.16634.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: G-Eval was found to significantly outperform traditional reference-based metrics,
    such as BLEU and ROUGE, which had a relatively low correlation with human judgments.
    On the surface, it looks pretty straightforward, as we just ask the LLM to perform
    the evaluation. We could also try to break down the tasks into smaller components.
  prefs: []
  type: TYPE_NORMAL
- en: FactScore
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[FactScore](https://arxiv.org/abs/2305.14251) (Factual precision in Atomicity
    Score) [5] is a metric for factual precision. The two key ideas there are to treat
    atomic facts as a unit and to base trustfulness on a particular knowledge source.'
  prefs: []
  type: TYPE_NORMAL
- en: For evaluation, you break down the generation into small “atomic“ facts (e.g.
    “He was born in New York”) and then check for each fact if it is supported by
    the given ground-truth knowledge source. The final score is calculated by dividing
    the number of supported facts by the total number of facts.
  prefs: []
  type: TYPE_NORMAL
- en: In the paper, the researchers asked LLMs to generate biographies of people and
    then used Wikipedia articles about them as the source of truth. The error rate
    for LLMs doing the same procedure as humans was less than 2%.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bef8489a9e71bd5102d9e0527dc96a75.png)'
  prefs: []
  type: TYPE_IMG
- en: FactScore for generating a biography of Bridget Moynahan. See also the [original
    paper](https://arxiv.org/abs/2305.14251).
  prefs: []
  type: TYPE_NORMAL
- en: RAGAS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let’s have a look at some metrics for retrieval-augmented generation (RAG).
    With RAG, you first retrieve the relevant context in an external knowledge base
    and then ask the LLM to answer the question based on those facts.
  prefs: []
  type: TYPE_NORMAL
- en: '[RAGAS](https://arxiv.org/abs/2309.15217v1) (Retrieval Augmented Generation
    Assessment) [6] is a new framework for evaluating RAGs. It’s not a single metric
    but rather a collection of them. The three proposed in the paper are faithfulness,
    answer relevance, and context relevance. These metrics perfectly illustrate how
    you can break down evaluation into simpler tasks for LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Faithfulness** measures how grounded the answers are in the given context.
    It’s very similar to FactScore, in that you first break down the generation into
    the set of statements and then ask the LLM if the statement is supported by the
    given context. The score is the number of supported statements divided by the
    number of all statements. For faithfulness, researchers found a very high correlation
    to human annotators.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer relevance** tries to capture the idea that the answer addresses the
    actual question. You start by asking the LLM to generate questions based on the
    answer. For each generated question, you can calculate the similarity (by creating
    an embedding and using cosine similarity) between the generated question and the
    original question. By doing this *n* times and averaging out the similarity scores,
    you get the final value for answer relevance.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Context relevance** refers to how relevant the provided context is. Meaning,
    the provided context contains only the information that is needed to answer the
    question. In the ideal case, we give the LLM the right information to answer the
    question and only that. Context relevance is calculated by asking the LLM to extract
    the sentences in the given context that were relevant to the answer. Then just
    divide the number of relevant sentences by the total number of sentences to get
    the final score.'
  prefs: []
  type: TYPE_NORMAL
- en: You can find further metrics and explanations (along with the open-sourced GitHub
    repo) [here](https://docs.ragas.io/en/latest/getstarted/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: The key point is that we can transform evaluation into a smaller subproblem.
    Instead of asking if the entire text is supported by the context, we ask only
    if a small specific fact is supported by the context. Instead of directly giving
    a number for if the answer is relevant, we ask LLM to think up a question for
    the given answer.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Evaluating LLMs is an extremely interesting research topic that will get more
    and more attention as more systems start reaching production and are also applied
    in more safety-critical settings.
  prefs: []
  type: TYPE_NORMAL
- en: We could also use these metrics to monitor the performance of LLMs in production
    to notice if the quality of outputs starts degrading. Especially for applications
    with high costs of mistakes, such as healthcare, it will be crucial to develop
    guardrails and systems to catch and reduce errors.
  prefs: []
  type: TYPE_NORMAL
- en: While there are definitely biases and problems with using LLMs as evaluators,
    we should still keep an open mind and approach it as a research problem. Of course,
    humans will still be involved in the evaluation process, but automatic metrics
    could help partially assess the performance in some settings.
  prefs: []
  type: TYPE_NORMAL
- en: These metrics don’t always have to be perfect; they just need to work well enough
    to guide the development of products in the right way.
  prefs: []
  type: TYPE_NORMAL
- en: '*Special thanks to Daniel Raff and Yevhen Petyak for their feedback and suggestions.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Originally published on* [*Medplexity substack*](https://medplexity.substack.com/p/using-llms-to-evaluate-llms)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: Yang, Chengrun, et al. [*Large Language Models as Optimizers*](https://arxiv.org/abs/2309.03409).
    arXiv, 6 Sept. 2023\. *arXiv.org*, [https://doi.org/10.48550/arXiv.2309.03409.](https://doi.org/10.48550/arXiv.2309.03409.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Li, Cheng, et al. *Large Language Models Understand and Can Be Enhanced by Emotional
    Stimuli*. arXiv, 5 Nov. 2023\. *arXiv.org*, [https://doi.org/10.48550/arXiv.2307.11760.](https://doi.org/10.48550/arXiv.2307.11760.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wang, Peiyi, et al. [*Large Language Models Are Not Fair Evaluators*](https://arxiv.org/abs/2305.17926).
    arXiv, 30 Aug. 2023\. *arXiv.org*, [https://doi.org/10.48550/arXiv.2305.17926.](https://doi.org/10.48550/arXiv.2305.17926.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Liu, Yang, et al. [*G-Eval: NLG Evaluation Using GPT-4 with Better Human Alignment*](https://arxiv.org/abs/2303.16634).
    arXiv, 23 May 2023\. *arXiv.org*, [https://doi.org/10.48550/arXiv.2303.16634.](https://doi.org/10.48550/arXiv.2303.16634.)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Min, Sewon, et al. [*FActScore: Fine-Grained Atomic Evaluation of Factual Precision
    in Long Form Text Generation*](https://arxiv.org/abs/2305.14251). arXiv, 11 Oct.
    2023\. *arXiv.org*, [https://doi.org/10.48550/arXiv.2305.14251.](https://doi.org/10.48550/arXiv.2305.14251.)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Es, Shahul, et al. [*RAGAS: Automated Evaluation of Retrieval Augmented Generation*](https://arxiv.org/abs/2309.15217v1).
    1, arXiv, 26 Sept. 2023\. *arXiv.org*, [https://doi.org/10.48550/arXiv.2309.15217.](https://doi.org/10.48550/arXiv.2309.15217.)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
