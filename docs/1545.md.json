["```py\nimport transformers\n\n# we use StableLM as an example, but these issues impact all models to varying degrees\ngenerator = transformers.pipeline('text-generation', model='stabilityai/stablelm-base-alpha-3b')\n\nraw_gen('The link is <a href=\"http:') # helper func to call the generator\n```", "```py\nraw_gen('The link is <a href=\"http')\n```", "```py\nprint_tokens(generator.tokenizer.encode('The link is <a href=\"http:'))\n```", "```py\nprint_tokens(generator.tokenizer.encode('The link is <a href=\"http://www.google.com/search?q'))\n```", "```py\nN = generator.tokenizer.vocab_size\ntokens = generator.tokenizer.convert_ids_to_tokens(range(N))\nprint_tokens([i for i,t in enumerate(tokens) if t.startswith(\":\")])\n```", "```py\nprint_tokens([i for i,t in enumerate(tokens) if t.startswith(\"http\")])\n```", "```py\n# Accidentally adding a space, will lead to weird generation\nraw_gen('I read a book about ')\n```", "```py\n# No space, works as expected\nraw_gen('I read a book about')\n```", "```py\nraw_gen('An example [\"like this\"] and another example [')\n```", "```py\n# note the Ġ is converted to a space by the tokenizer\nprint_tokens([i for i,t in enumerate(tokens) if t.startswith(\"Ġ[\")])\n```", "```py\nfrom guidance import models, gen\n\n# load StableLM from huggingface\nlm = models.Transformers(\"stabilityai/stablelm-base-alpha-3b\", device=0)\n\n# With token healing we generate valid URLs,\n# even when the prompt ends with a colon:\nlm + 'The link is <a href=\"http:' + gen(max_tokens=10)\n```", "```py\n# With token healing, we will sometimes generate https URLs,\n# even when the prompt ends with \"http\":\n[str(lm + 'The link is <a href=\"http' + gen(max_tokens=10, temperature=1)) for i in range(10)]\n```", "```py\n# Accidentally adding a space will not impact generation\nlm + 'I read a book about ' + gen(max_tokens=5)\n```", "```py\n# This will generate the same text as above \nlm + 'I read a book about' + gen(max_tokens=6)\n```", "```py\nlm + 'An example [\"like this\"] and another example [' + gen(max_tokens=10)\n```"]