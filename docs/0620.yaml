- en: Train YOLOv8 Instance Segmentation on Your Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/trian-yolov8-instance-segmentation-on-your-data-6ffa04b2debd?source=collection_archive---------0-----------------------#2023-02-15](https://towardsdatascience.com/trian-yolov8-instance-segmentation-on-your-data-6ffa04b2debd?source=collection_archive---------0-----------------------#2023-02-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to train an instance segmentation model based on the state-of-the-art YOLOv8
    model on your data
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://alon-lek.medium.com/?source=post_page-----6ffa04b2debd--------------------------------)[![Alon
    Lekhtman](../Images/1451bacf9a127f7fe596cf32249035be.png)](https://alon-lek.medium.com/?source=post_page-----6ffa04b2debd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6ffa04b2debd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6ffa04b2debd--------------------------------)
    [Alon Lekhtman](https://alon-lek.medium.com/?source=post_page-----6ffa04b2debd--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F931822b64e54&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrian-yolov8-instance-segmentation-on-your-data-6ffa04b2debd&user=Alon+Lekhtman&userId=931822b64e54&source=post_page-931822b64e54----6ffa04b2debd---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6ffa04b2debd--------------------------------)
    ·10 min read·Feb 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6ffa04b2debd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrian-yolov8-instance-segmentation-on-your-data-6ffa04b2debd&user=Alon+Lekhtman&userId=931822b64e54&source=-----6ffa04b2debd---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6ffa04b2debd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrian-yolov8-instance-segmentation-on-your-data-6ffa04b2debd&source=-----6ffa04b2debd---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: YOLOv8 was launched on January 10th, 2023\. And as of this moment, this is the
    state-of-the-art model for classification, detection, and segmentation tasks in
    the computer vision world. The model outperforms all known models both in terms
    of accuracy and execution time.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/846d796782d2742599ba66f04d7a424c.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
- en: A comparison between YOLOv8 and other YOLO models (from [ultralytics](https://github.com/ultralytics/ultralytics))
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: The ultralytics team did a really good job in making this model easier to use
    compared to all the previous YOLO models — you don’t even have to clone the git
    repository anymore!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Creating the Image Dataset
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this post, I created a very simple example of all you need to do to train
    YOLOv8 on your data, specifically for a segmentation task. The dataset is small
    and “easy to learn” for the model, on purpose, so that we would be able to get
    satisfying results after training for only a few seconds on a simple CPU.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: We will create a dataset of white circles with a black background. The circles
    will be of varying sizes. We will train a model that segments the circles inside
    the image.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what the dataset looks like:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7a79eee703fe9676934e9eef85f99867.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
- en: 'The dataset was generated using the following code:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Creating Labels
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have an image dataset, we need to create labels for the images.
    Usually, we would need to do some manual work for this, but because the dataset
    we created is very simple, it is pretty easy to create code that generates labels
    for us:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here is an example of a label file content:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The label corresponds to this image:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/41ddab79fd8e2339d49bf12eec44bba4.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: an image that corresponds to the label example
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: The label content is only a single text line. We have only one object (circle)
    in each image, and each object is represented by a line in the file. If you have
    more than one object in each image, you should create a line for each labeled
    object.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: The first 0 represents the class type of the label. Because we have only one
    class type (a circle) we always have 0\. If you have more than one class in your
    data, you should map each class to a number ( 0, 1, 2…) and use this number in
    the label file.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: All the other numbers represent the coordinates of the bounding polygon of the
    labeled object. The format is <**x1 y1 x2 y2 x3 y3…>** and the coordinates are
    relative to the size of the image —you should normalize the coordinates to a 1x1
    image size. For example, if there is a point (15, 75) and the image size is 120x120
    the normalized point is (15/120, 75/120) = (0.125, 0.625).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: It is always confusing when dealing with image libraries to get the correct
    directionality of the coordinates. So to make this clear, for YOLO, the X coordinate
    goes from left to right, and the Y coordinate goes from top to bottom.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: The YAML Configuration
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have the images and the labels. Now we need to create a YAML file with the
    dataset configuration:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Pay attention that if you have more object class types, you need to add them
    here in the names array, in the same order you ordered them in the label files.
    The first is 0, the second is 1, etc…
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Dataset File Structure
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see the file structure we created, using the Linux [**tree**](https://pimylifeup.com/tree-command-linux/)
    command:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Training The Model
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have the images and the labels, we can start training the model.
    So first of all let''s install the package:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The ultralytics library changes pretty fast and sometimes breaks the API, so
    I prefer to stick with one version. The below code depends on version 8.0.38 (the
    newest version at the time I write those words). If you upgrade to a newer version,
    maybe you will need to do some code adaptations to make it work.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'And start the training:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: For the simplicity of this post, I use the nano model (yolov8n-seg), I train
    it only on the CPU, with only 7 epochs. The training took just a few seconds on
    my laptop.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: For more information about the parameters that can be used to train the model,
    you can check [this](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/yolo/engine/trainer.py#L42).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Results
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After the training is done you will see a line, similar to this, at the end
    of the output:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s take a look at some of the results found here:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Validation labels
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/567dec0b45f36610ab6970aaa544e672.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: part of the validation set labels
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Here we can see the ground truth labels on part of the validation set. This
    should be almost perfectly aligned. In case you see those labels do not cover
    the objects well, it is highly likely that your labeling is incorrect.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '**Predicted validation labels**'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/2192471bc68c59dcf54fcdecafb638d0.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: validation set predictions
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Here we can see the predictions the trained model did on part of the validation
    set (the same part we saw above). This can give you a feeling of how well the
    model performs. Pay attention that in order to create this image a confidence
    threshold should be chosen, the threshold used here is **0.5**, which is not always
    the optimal one (we will discuss it later).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Precision curve
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To understand this and the next charts you need to be familiar with precision
    and recall concepts. [Here](https://medium.com/@shrutisaxena0617/precision-vs-recall-386cf9f89488)
    is a good explanation of how they work.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/6d5e98b3d6235facea1866bfba43ad9b.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
- en: precision/confidence threshold curve
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Every detected object by the model has some confidence, usually, if it is important
    to you to be as sure as possible when declaring “this is a circle” you will use
    only high confidence values (high confidence threshold). Off course, it comes
    with a tradeoff- you can miss some “circles”. On the other hand, if you want to
    “catch” as many “circles” as you can with a tradeoff that some of them are not
    really “circles” you would use both low and high confidence values (low confidence
    threshold).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: The above chart (and the chart below) helps you decide which confidence threshold
    to use. In our case, we can see that for a threshold higher than **0.128**, we
    get **100%** precision, which means all objects are correctly predicted.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Pay attention that because we actually doing a segmentation task, there is another
    important threshold we need to worry about — IoU ( intersection over union), if
    you are not familiar with it, you can read about it [here](https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/).
    For this chart, an IoU of **0.5** is used.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '**Recall curve**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/e9898f596c3f8de76b7f97ec15de9546.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
- en: recall/confidence threshold curve
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Here you can see the recall chart, as the confidence threshold values go up,
    the recall goes down. Which means you “catch” fewer “circles”.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Here you can see why using the **0.5** confidence threshold, in this case, is
    a bad idea. For a **0.5** threshold, you get about **90%** recall. However, in
    the precision curve, we saw that for a threshold above **0.128**, we get **100%**
    precision, so we don’t need to get to **0.5**, we can safely use a **0.128** threshold
    and get both **100%** precision and almost **100%** recall :)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Precision-Recall curve
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Here](https://medium.com/@douglaspsteen/precision-recall-curves-d32e5b290248)
    is a good explanation of the precision-recall curve'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/ae03968630d242ece8e4631990d2805a.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
- en: precision-recall curve
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: We can see here clearly the conclusion we made before, for this model, we can
    get to almost **100%** precision and **100%** recall.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: The disadvantage of this chart is that we can’t see what threshold we should
    use, this is why we still need the charts above.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '**Loss over time**'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](../Images/94602d3dd0c79a4c321ba0f545bbd9d7.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
- en: loss over time
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Here you can see how the different losses change during the training, and how
    they behave on the validation set after each epoch.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: There is a lot to say about the losses, and conclusions you can make from those
    charts, however, it is out of the scope of this post. I just wanted to state that
    you can find it here :)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Using the trained model
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another thing that can be found in the result directory is the model itself.
    Here’s how to use the model on new images:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The results list may have multiple values, one for each detected object. Because
    in this example we have only one object in each image, we take the first list
    item.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: You can see I passed here the best confidence threshold value we found before
    (**0.128**).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: There are two ways to get the actual placement of the detected object in the
    image. Choosing the right method depends on what you intend to do with the results.
    I will show both ways.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This returns the bounding polygon of the object, similar to the format we passed
    the labeled data.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'And the second way:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This returns a tensor with a shape (1, 128, 128) that represents all the pixels
    in the image. Pixels that are part of the object receive 1 and background pixels
    receive 0.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what the mask looks like:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](../Images/188f74a8e99b400d049abcf6fa0ffb6a.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
- en: predicted segmentation for image
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'This was the original image:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/787981982a3eb511bfc09f1f297c9c6e.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
- en: original image
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Not perfect, but good enough for many applications, and the IoU is definitely
    higher than **0.5**.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然不完美，但对于许多应用来说已经足够好，并且 IoU 确实高于**0.5**。
- en: 'In conclusion, the new ultralytics library is much easier to work with compared
    to the previous Yolo versions, especially for the segmentation task, which is
    now a first-class citizen. You can find Yolov5 also as part of the ultralytics
    new package, so if you don’t want to use the new Yolo version, which is still
    kind of new and experimental, you can just use the well-known yolov5:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，与之前的 Yolo 版本相比，新版的 ultralytics 库更容易使用，特别是在分割任务上，现在它已经成为一个一流的功能。你可以在 ultralytics
    新包中找到 Yolov5，因此如果你不想使用仍然有些新的和实验性的 Yolo 版本，你可以选择使用广为人知的 yolov5：
- en: '![](../Images/2d6bb0b746deda291cf6d79aa89155af.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d6bb0b746deda291cf6d79aa89155af.png)'
- en: '[google trends comparison of yolov8 and yolov5](https://trends.google.com/trends/explore?date=today+5-y&q=yolov5%2Cyolov8)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[yolov8 和 yolov5 的 Google 趋势比较](https://trends.google.com/trends/explore?date=today+5-y&q=yolov5%2Cyolov8)'
- en: There were some topics I didn’t cover, like the different loss functions used
    for the model, the architecture changes that were made to create the yolov8, and
    more. Feel free to comment on this post if you want more information about those
    topics. If there will be interest, I will maybe write another post about it.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 有些话题我没有涵盖，比如用于模型的不同损失函数、创建 yolov8 时进行的架构更改等等。如果你想了解更多这些话题，欢迎在这篇文章下评论。如果有兴趣，我可能会写另一篇关于这些内容的文章。
- en: Thanks for taking the time to read this and I hope it helped you understand
    the Yolov8 model training process.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你抽时间阅读这篇文章，希望它能帮助你理解 Yolov8 模型的训练过程。
- en: '*All images, unless otherwise noted, were created by the author.*'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，所有图像均由作者创建。*'
