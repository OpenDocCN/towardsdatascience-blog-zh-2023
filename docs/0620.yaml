- en: Train YOLOv8 Instance Segmentation on Your Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/trian-yolov8-instance-segmentation-on-your-data-6ffa04b2debd?source=collection_archive---------0-----------------------#2023-02-15](https://towardsdatascience.com/trian-yolov8-instance-segmentation-on-your-data-6ffa04b2debd?source=collection_archive---------0-----------------------#2023-02-15)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to train an instance segmentation model based on the state-of-the-art YOLOv8
    model on your data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://alon-lek.medium.com/?source=post_page-----6ffa04b2debd--------------------------------)[![Alon
    Lekhtman](../Images/1451bacf9a127f7fe596cf32249035be.png)](https://alon-lek.medium.com/?source=post_page-----6ffa04b2debd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6ffa04b2debd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6ffa04b2debd--------------------------------)
    [Alon Lekhtman](https://alon-lek.medium.com/?source=post_page-----6ffa04b2debd--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F931822b64e54&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrian-yolov8-instance-segmentation-on-your-data-6ffa04b2debd&user=Alon+Lekhtman&userId=931822b64e54&source=post_page-931822b64e54----6ffa04b2debd---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6ffa04b2debd--------------------------------)
    ·10 min read·Feb 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6ffa04b2debd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrian-yolov8-instance-segmentation-on-your-data-6ffa04b2debd&user=Alon+Lekhtman&userId=931822b64e54&source=-----6ffa04b2debd---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6ffa04b2debd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrian-yolov8-instance-segmentation-on-your-data-6ffa04b2debd&source=-----6ffa04b2debd---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: YOLOv8 was launched on January 10th, 2023\. And as of this moment, this is the
    state-of-the-art model for classification, detection, and segmentation tasks in
    the computer vision world. The model outperforms all known models both in terms
    of accuracy and execution time.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/846d796782d2742599ba66f04d7a424c.png)'
  prefs: []
  type: TYPE_IMG
- en: A comparison between YOLOv8 and other YOLO models (from [ultralytics](https://github.com/ultralytics/ultralytics))
  prefs: []
  type: TYPE_NORMAL
- en: The ultralytics team did a really good job in making this model easier to use
    compared to all the previous YOLO models — you don’t even have to clone the git
    repository anymore!
  prefs: []
  type: TYPE_NORMAL
- en: Creating the Image Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this post, I created a very simple example of all you need to do to train
    YOLOv8 on your data, specifically for a segmentation task. The dataset is small
    and “easy to learn” for the model, on purpose, so that we would be able to get
    satisfying results after training for only a few seconds on a simple CPU.
  prefs: []
  type: TYPE_NORMAL
- en: We will create a dataset of white circles with a black background. The circles
    will be of varying sizes. We will train a model that segments the circles inside
    the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what the dataset looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7a79eee703fe9676934e9eef85f99867.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The dataset was generated using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Creating Labels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have an image dataset, we need to create labels for the images.
    Usually, we would need to do some manual work for this, but because the dataset
    we created is very simple, it is pretty easy to create code that generates labels
    for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is an example of a label file content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The label corresponds to this image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/41ddab79fd8e2339d49bf12eec44bba4.png)'
  prefs: []
  type: TYPE_IMG
- en: an image that corresponds to the label example
  prefs: []
  type: TYPE_NORMAL
- en: The label content is only a single text line. We have only one object (circle)
    in each image, and each object is represented by a line in the file. If you have
    more than one object in each image, you should create a line for each labeled
    object.
  prefs: []
  type: TYPE_NORMAL
- en: The first 0 represents the class type of the label. Because we have only one
    class type (a circle) we always have 0\. If you have more than one class in your
    data, you should map each class to a number ( 0, 1, 2…) and use this number in
    the label file.
  prefs: []
  type: TYPE_NORMAL
- en: All the other numbers represent the coordinates of the bounding polygon of the
    labeled object. The format is <**x1 y1 x2 y2 x3 y3…>** and the coordinates are
    relative to the size of the image —you should normalize the coordinates to a 1x1
    image size. For example, if there is a point (15, 75) and the image size is 120x120
    the normalized point is (15/120, 75/120) = (0.125, 0.625).
  prefs: []
  type: TYPE_NORMAL
- en: It is always confusing when dealing with image libraries to get the correct
    directionality of the coordinates. So to make this clear, for YOLO, the X coordinate
    goes from left to right, and the Y coordinate goes from top to bottom.
  prefs: []
  type: TYPE_NORMAL
- en: The YAML Configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have the images and the labels. Now we need to create a YAML file with the
    dataset configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Pay attention that if you have more object class types, you need to add them
    here in the names array, in the same order you ordered them in the label files.
    The first is 0, the second is 1, etc…
  prefs: []
  type: TYPE_NORMAL
- en: Dataset File Structure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see the file structure we created, using the Linux [**tree**](https://pimylifeup.com/tree-command-linux/)
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Training The Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have the images and the labels, we can start training the model.
    So first of all let''s install the package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The ultralytics library changes pretty fast and sometimes breaks the API, so
    I prefer to stick with one version. The below code depends on version 8.0.38 (the
    newest version at the time I write those words). If you upgrade to a newer version,
    maybe you will need to do some code adaptations to make it work.
  prefs: []
  type: TYPE_NORMAL
- en: 'And start the training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: For the simplicity of this post, I use the nano model (yolov8n-seg), I train
    it only on the CPU, with only 7 epochs. The training took just a few seconds on
    my laptop.
  prefs: []
  type: TYPE_NORMAL
- en: For more information about the parameters that can be used to train the model,
    you can check [this](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/yolo/engine/trainer.py#L42).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After the training is done you will see a line, similar to this, at the end
    of the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s take a look at some of the results found here:'
  prefs: []
  type: TYPE_NORMAL
- en: Validation labels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/567dec0b45f36610ab6970aaa544e672.png)'
  prefs: []
  type: TYPE_IMG
- en: part of the validation set labels
  prefs: []
  type: TYPE_NORMAL
- en: Here we can see the ground truth labels on part of the validation set. This
    should be almost perfectly aligned. In case you see those labels do not cover
    the objects well, it is highly likely that your labeling is incorrect.
  prefs: []
  type: TYPE_NORMAL
- en: '**Predicted validation labels**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2192471bc68c59dcf54fcdecafb638d0.png)'
  prefs: []
  type: TYPE_IMG
- en: validation set predictions
  prefs: []
  type: TYPE_NORMAL
- en: Here we can see the predictions the trained model did on part of the validation
    set (the same part we saw above). This can give you a feeling of how well the
    model performs. Pay attention that in order to create this image a confidence
    threshold should be chosen, the threshold used here is **0.5**, which is not always
    the optimal one (we will discuss it later).
  prefs: []
  type: TYPE_NORMAL
- en: Precision curve
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To understand this and the next charts you need to be familiar with precision
    and recall concepts. [Here](https://medium.com/@shrutisaxena0617/precision-vs-recall-386cf9f89488)
    is a good explanation of how they work.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/6d5e98b3d6235facea1866bfba43ad9b.png)'
  prefs: []
  type: TYPE_IMG
- en: precision/confidence threshold curve
  prefs: []
  type: TYPE_NORMAL
- en: Every detected object by the model has some confidence, usually, if it is important
    to you to be as sure as possible when declaring “this is a circle” you will use
    only high confidence values (high confidence threshold). Off course, it comes
    with a tradeoff- you can miss some “circles”. On the other hand, if you want to
    “catch” as many “circles” as you can with a tradeoff that some of them are not
    really “circles” you would use both low and high confidence values (low confidence
    threshold).
  prefs: []
  type: TYPE_NORMAL
- en: The above chart (and the chart below) helps you decide which confidence threshold
    to use. In our case, we can see that for a threshold higher than **0.128**, we
    get **100%** precision, which means all objects are correctly predicted.
  prefs: []
  type: TYPE_NORMAL
- en: Pay attention that because we actually doing a segmentation task, there is another
    important threshold we need to worry about — IoU ( intersection over union), if
    you are not familiar with it, you can read about it [here](https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/).
    For this chart, an IoU of **0.5** is used.
  prefs: []
  type: TYPE_NORMAL
- en: '**Recall curve**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e9898f596c3f8de76b7f97ec15de9546.png)'
  prefs: []
  type: TYPE_IMG
- en: recall/confidence threshold curve
  prefs: []
  type: TYPE_NORMAL
- en: Here you can see the recall chart, as the confidence threshold values go up,
    the recall goes down. Which means you “catch” fewer “circles”.
  prefs: []
  type: TYPE_NORMAL
- en: Here you can see why using the **0.5** confidence threshold, in this case, is
    a bad idea. For a **0.5** threshold, you get about **90%** recall. However, in
    the precision curve, we saw that for a threshold above **0.128**, we get **100%**
    precision, so we don’t need to get to **0.5**, we can safely use a **0.128** threshold
    and get both **100%** precision and almost **100%** recall :)
  prefs: []
  type: TYPE_NORMAL
- en: Precision-Recall curve
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Here](https://medium.com/@douglaspsteen/precision-recall-curves-d32e5b290248)
    is a good explanation of the precision-recall curve'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ae03968630d242ece8e4631990d2805a.png)'
  prefs: []
  type: TYPE_IMG
- en: precision-recall curve
  prefs: []
  type: TYPE_NORMAL
- en: We can see here clearly the conclusion we made before, for this model, we can
    get to almost **100%** precision and **100%** recall.
  prefs: []
  type: TYPE_NORMAL
- en: The disadvantage of this chart is that we can’t see what threshold we should
    use, this is why we still need the charts above.
  prefs: []
  type: TYPE_NORMAL
- en: '**Loss over time**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/94602d3dd0c79a4c321ba0f545bbd9d7.png)'
  prefs: []
  type: TYPE_IMG
- en: loss over time
  prefs: []
  type: TYPE_NORMAL
- en: Here you can see how the different losses change during the training, and how
    they behave on the validation set after each epoch.
  prefs: []
  type: TYPE_NORMAL
- en: There is a lot to say about the losses, and conclusions you can make from those
    charts, however, it is out of the scope of this post. I just wanted to state that
    you can find it here :)
  prefs: []
  type: TYPE_NORMAL
- en: Using the trained model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another thing that can be found in the result directory is the model itself.
    Here’s how to use the model on new images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The results list may have multiple values, one for each detected object. Because
    in this example we have only one object in each image, we take the first list
    item.
  prefs: []
  type: TYPE_NORMAL
- en: You can see I passed here the best confidence threshold value we found before
    (**0.128**).
  prefs: []
  type: TYPE_NORMAL
- en: There are two ways to get the actual placement of the detected object in the
    image. Choosing the right method depends on what you intend to do with the results.
    I will show both ways.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This returns the bounding polygon of the object, similar to the format we passed
    the labeled data.
  prefs: []
  type: TYPE_NORMAL
- en: 'And the second way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This returns a tensor with a shape (1, 128, 128) that represents all the pixels
    in the image. Pixels that are part of the object receive 1 and background pixels
    receive 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what the mask looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/188f74a8e99b400d049abcf6fa0ffb6a.png)'
  prefs: []
  type: TYPE_IMG
- en: predicted segmentation for image
  prefs: []
  type: TYPE_NORMAL
- en: 'This was the original image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/787981982a3eb511bfc09f1f297c9c6e.png)'
  prefs: []
  type: TYPE_IMG
- en: original image
  prefs: []
  type: TYPE_NORMAL
- en: Not perfect, but good enough for many applications, and the IoU is definitely
    higher than **0.5**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In conclusion, the new ultralytics library is much easier to work with compared
    to the previous Yolo versions, especially for the segmentation task, which is
    now a first-class citizen. You can find Yolov5 also as part of the ultralytics
    new package, so if you don’t want to use the new Yolo version, which is still
    kind of new and experimental, you can just use the well-known yolov5:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2d6bb0b746deda291cf6d79aa89155af.png)'
  prefs: []
  type: TYPE_IMG
- en: '[google trends comparison of yolov8 and yolov5](https://trends.google.com/trends/explore?date=today+5-y&q=yolov5%2Cyolov8)'
  prefs: []
  type: TYPE_NORMAL
- en: There were some topics I didn’t cover, like the different loss functions used
    for the model, the architecture changes that were made to create the yolov8, and
    more. Feel free to comment on this post if you want more information about those
    topics. If there will be interest, I will maybe write another post about it.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for taking the time to read this and I hope it helped you understand
    the Yolov8 model training process.
  prefs: []
  type: TYPE_NORMAL
- en: '*All images, unless otherwise noted, were created by the author.*'
  prefs: []
  type: TYPE_NORMAL
