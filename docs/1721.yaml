- en: End to End ML with GPT-3.5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/end-to-end-ml-with-gpt-3-5-8334db3d78e2?source=collection_archive---------2-----------------------#2023-05-24](https://towardsdatascience.com/end-to-end-ml-with-gpt-3-5-8334db3d78e2?source=collection_archive---------2-----------------------#2023-05-24)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/9ec4de24c2c445ccedaec0940db9619a.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration generated by me + Midjourney
  prefs: []
  type: TYPE_NORMAL
- en: Learn how to use GPT-3.5 to do the heavy lifting for data acquisition, preprocessing,
    model training, and deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://georgealexadam.medium.com/?source=post_page-----8334db3d78e2--------------------------------)[![Alex
    Adam](../Images/a2c18f61e6ed2bd1e6955ffb6232c9be.png)](https://georgealexadam.medium.com/?source=post_page-----8334db3d78e2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8334db3d78e2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8334db3d78e2--------------------------------)
    [Alex Adam](https://georgealexadam.medium.com/?source=post_page-----8334db3d78e2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F29fd9b1a62ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fend-to-end-ml-with-gpt-3-5-8334db3d78e2&user=Alex+Adam&userId=29fd9b1a62ae&source=post_page-29fd9b1a62ae----8334db3d78e2---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8334db3d78e2--------------------------------)
    ·14 min read·May 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8334db3d78e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fend-to-end-ml-with-gpt-3-5-8334db3d78e2&user=Alex+Adam&userId=29fd9b1a62ae&source=-----8334db3d78e2---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8334db3d78e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fend-to-end-ml-with-gpt-3-5-8334db3d78e2&source=-----8334db3d78e2---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: A lot of repetitive boilerplate code exists in the model development phase of
    any machine learning application. Popular libraries such as [PyTorch Lightning](https://lightning.ai/pytorch-lightning)
    have been created to standardize the operations performed when training/evaluating
    neural networks, leading to much cleaner code. However, boilerplate extends far
    beyond training loops. Even the data acquisition phase of machine learning projects
    is full of steps that are necessary but time consuming. One way to deal with this
    challenge would be to create a library similar to PyTorch Lightning for the entire
    model development process. It would have to be general enough to work with a variety
    of model types beyond neural networks, and capable of integrating a variety of
    data sources.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code examples for extracting data, preprocessing, model training, and deployment
    is readily available on the internet, though gathering it, and integrating it
    into a project takes time. Since such code is on the internet, chances are it
    has been trained on by a large language model (LLM) and can be rearranged in a
    variety of useful ways through natural language commands. The goal of this post
    is to show how easy it is to automate many of the steps common to ML projects
    by using the GPT-3.5 API from OpenAI. I’ll show some failure cases along the way,
    and how to tune prompts to fix bugs when possible. Starting from scratch, without
    even so much as a dataset, we’ll end up with a model that is ready to be deployed
    on AWS SageMaker. If you’re following along, make sure to setup the OpenAI API
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, the following utility function is helpful for calling the GPT-3.5 API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Extract, transform, load (ETL)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/2665dfe67e5e8e270ff34942dadb03b9.png)'
  prefs: []
  type: TYPE_IMG
- en: ETL Illustration by me + Midjourney
  prefs: []
  type: TYPE_NORMAL
- en: 'This section is simplified since it only considers a single data source, but
    can in principle be extended so situations where data comes from multiple sources
    (csv files, databases, images, etc.). The first step is to **extract** some data.
    For the sake of simplicity, I’ll use the [Income Prediction](http://archive.ics.uci.edu/ml/datasets/adult)¹
    dataset where the goal is to predict if an individual earns more/less than $50k
    per year based on their education, job position, industry, etc. The function below
    will be used to generate the code that downloads our data. Note how the prompt
    template is designed to bias the API to generate python code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we ask GPT-3.5 to:'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieve the adult income prediction dataset from openml using the sklearn fetch_openml
    function. Make sure to retrieve the data as a single dataframe which includes
    the target in a column named “target”. Name the resulting dataframe “df”.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'which gives us the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This code is free of bugs and gives us exactly what we want. Had I used a simpler
    prompt by removing mentions of openml and the function to use for retrieval, we
    would get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This assumes that the data is locally available. What’s interesting about this
    result is that it has the correct column names as a list, even though we did not
    include them in the API call! These names nevertheless are all over the web, particularly
    in [this](/exploring-the-census-income-dataset-using-bubble-plot-cfa1b366313b)
    Medium post, except for the target column which is added by GPT. The next step
    is to **transform** the data into a format that is usable by machine learning
    models. We’ll have to use a more advanced prompt template here since GPT needs
    to be made aware of the column names and their types:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'I then make the following request:'
  prefs: []
  type: TYPE_NORMAL
- en: Preprocess the dataframe by converting all categorical columns to their one-hot
    encoded equivalents, and normalizing numerical columns. Drop rows which have an
    NA or NaN value in any column. Drop rows that have numeric column outliers as
    determined by their z score. A numeric column outlier is a value that is outside
    of the 1 to 99 inter-quantile range. The numerical columns should be normalized
    using StandardScaler from sklearn. The values in the target colummn should be
    converted to 0 or 1 and should be of type int.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'We now get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: which is almost perfect for our use if we exclude the part that tries to load
    in the data from a csv file. It’s quite difficult to convince GPT to exclude this,
    even if we explicitly tell it to assume that `df` exists and should not be loaded.
    Lastly, we need to **load** the data into a local database. This is overkill for
    such a simple use case, but is a good habit to develop.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'I use the following instruction:'
  prefs: []
  type: TYPE_NORMAL
- en: Connect to an sqlite database named “data”. Use pandas to insert data from a
    DataFrame named “df” into a table named “income”. Do not include the index column.
    Commit the changes before closing the connection.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'in order to get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: which is perfect, minus the unnecessary creation of `df`. With this processed
    data in hand, we are ready to train some models.
  prefs: []
  type: TYPE_NORMAL
- en: Model Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/433e1a6b80dba4d54d04b33977243886.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration of a loss function by me + Midjourney
  prefs: []
  type: TYPE_NORMAL
- en: 'Resources permitting, it is a good idea to try out a few different model types
    to identify the one with the right level of complexity for the given task. Therefore,
    we ask GPT-3.5 to try out a few different models. First, let’s set up the generic
    prompt template for model training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'and the prompt we’ll be using is:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Train a variety of classification models to predict the “target” column using
    all other columns. Do so using 5-fold cross validation to choose the best model
    and corresponding set of hyperparameters, and return the best overall model and
    corresponding hyperparameter settings. Choose the best model based on accuracy.
    Assume a dataframe named “df” exists which is to be used for training. Log the
    entire process using MLFlow. Start logging with mlflow before training any models
    so only a single run is stored. Make sure that the model is logged using the sklearn
    module of mlflow. Make sure that only the best overall model is logged, but log
    metrics for all model types. The mean value of the following metrics on all cross
    validation folds should be logged: accuracy, AUC, F1 score'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s have a look at the generated output and this time go deeper into why some
    of the specific instructions had to be provided.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'If we remove the loading of `df` and the section `# set up mlflow` , we end
    up with exactly what is desired. Namely, a loop over a 3 different model types,
    performing a grid search using 5-fold cross validation to identify the best hyperparmeters
    for the given model type, while keeping track of metrics. Without specifying “choose
    the best model based on accuracy”, the generated code will use `scoring=[“accuracy”,
    “roc_auc", “f1”]` for the grid search which will not work since there is ambiguity
    as to how to select the best model according to multiple metrics. Without “make
    sure that the model is logged using the sklearn module of mlflow”, we sometimes
    end up with `mlflow.log_model()` which is wrong. Also, “make sure that only the
    best overall model is logged” is necessary to avoid storing all models. Overall,
    this output is acceptable, but it’s unstable, and running it multiple times is
    likely to introduce different bugs. In order to have everything ready for the
    **serving** step, it is useful to add the model signature when saving the model.
    This signature is basically a collection of feature names and their corresponding
    types. It is a pain to get GPT-3.5 to add this, so some manual labor has to be
    done by first adding the import:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'and then modifying the line of code which logs the model via:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Model Serving
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/00f90a37907f52b68f7916a92f02cccb.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration of deployment by me + Midjourney
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we used MLflow to log the best model, we have a couple of options to
    serve the model. The simplest option is to host the model locally. Let’s first
    design the general prompt template for model serving:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'and the prompt will be:'
  prefs: []
  type: TYPE_NORMAL
- en: Serve the model using port number 1111, and use the local environment manager
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'By calling `serve_model("<model path here>", question)` we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Once we run this command in the shell, we are ready to make predictions by sending
    data encoded as JSON to the model. We’ll first generate the command to send data
    to the model, and then create the JSON payload to be inserted into the command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The following request will be inserted into the prompt template in `send_request()`:'
  prefs: []
  type: TYPE_NORMAL
- en: Use the “curl” command to send data “<data here>” to an mlflow model hosted
    at port 1111 on localhost. Make sure that the content type is “application/json”.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The output generated by GPT-3.5 is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: It is preferable to have the URL immediately after `curl` instead of being at
    the very end of the command, i.e.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Getting GPT-3.5 to do this is not easy. Both of the following requests fail
    to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: Use the “curl” command to send data “<data here>” to an mlflow model hosted
    at port 1111 on localhost. Place the URL immediately after “curl”. Make sure that
    the content type is “application/json”.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Use the “curl” command, with the URL placed before any argument, to send data
    “<data here>” to an mlflow model hosted at port 1111 on localhost. Make sure that
    the content type is “application/json”.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Maybe it’s possible to get the desired output if we have GPT-3.5 modify an
    existing command rather than generate one from scratch. Here is the generic template
    for modifying commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We will call this function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'which finally gives us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now time to create the payload:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The prompt for this part needed quite a bit of tuning to get the desired output
    format:'
  prefs: []
  type: TYPE_NORMAL
- en: Convert the DataFrame “df” to json format that can be received by a deployed
    MLFlow model. Wrap the resulting json in an object called “dataframe_split”. The
    resulting string should not have newlines, and it should not escape quotes. Also,
    “dataframe_split” should be surrounded by doubles quotes instead of single quotes.
    Do not include the “target” column. Use the split “orient” argument
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Without the explicit instruction to avoid newlines and escaping quotes, a call
    to `json.dumps()` is made which is not the format that the MLflow endpoint expects.
    The generated command is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Before replacing `<data here>` in the `curl` request with the value of `wrapped_data`,
    we probably want to send only a few rows of data for prediction, otherwise the
    resulting payload is too large. So we modify the above to be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Invoking the model gives:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: whereas the actual targets are [0, 0, 1, 1, 0].
  prefs: []
  type: TYPE_NORMAL
- en: There we have it. At the beginning of this post, we didn’t even have access
    to a dataset, yet we’ve managed to end up with a deployed model that was selected
    to be the best through cross-validation. Importantly, GPT-3.5 did all the heavy
    lifting, and only required minimal assistance along the way. I did however have
    to specify particular libraries to use and methods to call, but this was mainly
    required to resolve ambiguities. Had I specified “Log the entire process” instead
    of “Log the entire process using MLFlow”, GPT-3.5 would have too many libraries
    to choose from, and the resulting model format might not have been useful for
    serving with MLflow. Thus, some knowledge of the tools used to perform the various
    steps in the ML pipeline is required to have success using GPT-3.5, but it is
    minimal compared to the knowledge required to code from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another option for serving the model is to host it as a SageMaker endpoint
    on AWS. Despite how easy this may look on the MLflow [website](https://mlflow.org/docs/latest/models.html#deploy-a-python-function-model-on-amazon-sagemaker),
    I assure you that as with many examples on the web involving AWS, things will
    go wrong. First of all, Docker must be installed in order to generate the Docker
    Imager using the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Second, the Ptyhon library `boto3` used to communicate with AWS also requires
    installation. Beyond this, permissions must be properly setup such that SageMaker,
    ECR, and S3 services can communicate with each other on behalf of your account.
    Here are the commands I ended up having to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: along with some manual tinkering behind the scenes to get the S3 bucket to be
    in the correct region.
  prefs: []
  type: TYPE_NORMAL
- en: With the help of GPT-3.5 we went through the ML pipeline in a (mostly) painless
    way, though the last mile was a bit trickier. Note how I didn’t use GPT-3.5 to
    generate the commands for serving the model on AWS. It works poorly for this use
    case, and creates made up argument names. I can only speculate that switching
    to the GPT-4.0 API would help resolve some of the above bugs, and lead to an even
    easier model development experience.
  prefs: []
  type: TYPE_NORMAL
- en: While the ML pipeline can be fully automated using LLMs, it isn’t yet safe to
    have a non-expert be responsible for the process. The bugs in the above code were
    easily identified because the Python interpreter would throw errors, but there
    are more subtle bugs that can be harmful. For example, the elimination of outlier
    values in the preprocessing code could be wrong leading to excess or insufficient
    samples being discarded. In the worst case, it could inadvertently drop entire
    subgroups of people, exacerbating potential fairness issues.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the grid search over hyperparameters could have been done over
    a poorly chosen range, leading to overfitting or underfitting depending on the
    range. This would be quite tricky to identify for someone with little ML experience
    as the code otherwise seems correct, but an understanding of how regularization
    works in these models is required. Thus, it isn’t yet appropriate to have an unspecialized
    software engineer stand in for an ML engineer, but that time is fast approaching.
  prefs: []
  type: TYPE_NORMAL
- en: '[1] Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml].
    Irvine, CA: University of California, School of Information and Computer Science.
    (CC BY 4.0)'
  prefs: []
  type: TYPE_NORMAL
