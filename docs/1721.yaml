- en: End to End ML with GPT-3.5
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/end-to-end-ml-with-gpt-3-5-8334db3d78e2?source=collection_archive---------2-----------------------#2023-05-24](https://towardsdatascience.com/end-to-end-ml-with-gpt-3-5-8334db3d78e2?source=collection_archive---------2-----------------------#2023-05-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/9ec4de24c2c445ccedaec0940db9619a.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: Illustration generated by me + Midjourney
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Learn how to use GPT-3.5 to do the heavy lifting for data acquisition, preprocessing,
    model training, and deployment
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://georgealexadam.medium.com/?source=post_page-----8334db3d78e2--------------------------------)[![Alex
    Adam](../Images/a2c18f61e6ed2bd1e6955ffb6232c9be.png)](https://georgealexadam.medium.com/?source=post_page-----8334db3d78e2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8334db3d78e2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8334db3d78e2--------------------------------)
    [Alex Adam](https://georgealexadam.medium.com/?source=post_page-----8334db3d78e2--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F29fd9b1a62ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fend-to-end-ml-with-gpt-3-5-8334db3d78e2&user=Alex+Adam&userId=29fd9b1a62ae&source=post_page-29fd9b1a62ae----8334db3d78e2---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8334db3d78e2--------------------------------)
    ·14 min read·May 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8334db3d78e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fend-to-end-ml-with-gpt-3-5-8334db3d78e2&user=Alex+Adam&userId=29fd9b1a62ae&source=-----8334db3d78e2---------------------clap_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8334db3d78e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fend-to-end-ml-with-gpt-3-5-8334db3d78e2&source=-----8334db3d78e2---------------------bookmark_footer-----------)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: A lot of repetitive boilerplate code exists in the model development phase of
    any machine learning application. Popular libraries such as [PyTorch Lightning](https://lightning.ai/pytorch-lightning)
    have been created to standardize the operations performed when training/evaluating
    neural networks, leading to much cleaner code. However, boilerplate extends far
    beyond training loops. Even the data acquisition phase of machine learning projects
    is full of steps that are necessary but time consuming. One way to deal with this
    challenge would be to create a library similar to PyTorch Lightning for the entire
    model development process. It would have to be general enough to work with a variety
    of model types beyond neural networks, and capable of integrating a variety of
    data sources.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何机器学习应用程序的模型开发阶段，都存在大量重复的样板代码。像 [PyTorch Lightning](https://lightning.ai/pytorch-lightning)
    这样的流行库已被创建，用以标准化训练/评估神经网络时执行的操作，从而使代码更加简洁。然而，样板代码的范围远超训练循环。即使是机器学习项目的数据获取阶段也充满了必要但耗时的步骤。应对这一挑战的一种方式是创建一个类似于
    PyTorch Lightning 的库，涵盖整个模型开发过程。它必须足够通用，以适用于各种模型类型，并能够集成各种数据源。
- en: 'Code examples for extracting data, preprocessing, model training, and deployment
    is readily available on the internet, though gathering it, and integrating it
    into a project takes time. Since such code is on the internet, chances are it
    has been trained on by a large language model (LLM) and can be rearranged in a
    variety of useful ways through natural language commands. The goal of this post
    is to show how easy it is to automate many of the steps common to ML projects
    by using the GPT-3.5 API from OpenAI. I’ll show some failure cases along the way,
    and how to tune prompts to fix bugs when possible. Starting from scratch, without
    even so much as a dataset, we’ll end up with a model that is ready to be deployed
    on AWS SageMaker. If you’re following along, make sure to setup the OpenAI API
    as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 提取数据、预处理、模型训练和部署的代码示例在互联网上随处可见，尽管收集这些代码并将其集成到项目中需要时间。由于这些代码在互联网上，可能已经被大型语言模型（LLM）训练过，并且可以通过自然语言命令以各种有用的方式重新排列。本篇文章的目标是展示如何通过使用
    OpenAI 的 GPT-3.5 API 来自动化许多常见的 ML 项目的步骤。我会展示一些失败案例，并在可能的情况下如何调整提示以修复错误。从头开始，甚至没有数据集，我们将最终得到一个可以在
    AWS SageMaker 上部署的模型。如果你跟着做，请确保按照以下步骤设置 OpenAI API：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Also, the following utility function is helpful for calling the GPT-3.5 API:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，以下实用函数对调用 GPT-3.5 API 很有帮助：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Extract, transform, load (ETL)
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取、转换、加载（ETL）
- en: '![](../Images/2665dfe67e5e8e270ff34942dadb03b9.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2665dfe67e5e8e270ff34942dadb03b9.png)'
- en: ETL Illustration by me + Midjourney
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ETL 由我 + Midjourney 进行说明
- en: 'This section is simplified since it only considers a single data source, but
    can in principle be extended so situations where data comes from multiple sources
    (csv files, databases, images, etc.). The first step is to **extract** some data.
    For the sake of simplicity, I’ll use the [Income Prediction](http://archive.ics.uci.edu/ml/datasets/adult)¹
    dataset where the goal is to predict if an individual earns more/less than $50k
    per year based on their education, job position, industry, etc. The function below
    will be used to generate the code that downloads our data. Note how the prompt
    template is designed to bias the API to generate python code:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本节经过简化，仅考虑了单一数据源，但原则上可以扩展到数据来自多个来源的情况（csv 文件、数据库、图像等）。第一步是 **提取** 一些数据。为了简单起见，我将使用
    [Income Prediction](http://archive.ics.uci.edu/ml/datasets/adult)¹ 数据集，其目标是预测一个人是否年收入超过或低于
    50k 美元，基于他们的教育、职位、行业等。下面的函数将用于生成下载我们数据的代码。注意提示模板是如何设计的，以使 API 生成 Python 代码：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here we ask GPT-3.5 to:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们要求 GPT-3.5：
- en: Retrieve the adult income prediction dataset from openml using the sklearn fetch_openml
    function. Make sure to retrieve the data as a single dataframe which includes
    the target in a column named “target”. Name the resulting dataframe “df”.
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用 sklearn fetch_openml 函数从 openml 检索成人收入预测数据集。确保将数据作为单个数据框检索，其中包含一个名为“target”的列。将结果数据框命名为“df”。
- en: 'which gives us the output:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这会给我们输出：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This code is free of bugs and gives us exactly what we want. Had I used a simpler
    prompt by removing mentions of openml and the function to use for retrieval, we
    would get:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码没有错误，完全达到了我们想要的效果。如果我使用更简单的提示，去掉提及 openml 和用于检索的函数，我们会得到：
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This assumes that the data is locally available. What’s interesting about this
    result is that it has the correct column names as a list, even though we did not
    include them in the API call! These names nevertheless are all over the web, particularly
    in [this](/exploring-the-census-income-dataset-using-bubble-plot-cfa1b366313b)
    Medium post, except for the target column which is added by GPT. The next step
    is to **transform** the data into a format that is usable by machine learning
    models. We’ll have to use a more advanced prompt template here since GPT needs
    to be made aware of the column names and their types:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'I then make the following request:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Preprocess the dataframe by converting all categorical columns to their one-hot
    encoded equivalents, and normalizing numerical columns. Drop rows which have an
    NA or NaN value in any column. Drop rows that have numeric column outliers as
    determined by their z score. A numeric column outlier is a value that is outside
    of the 1 to 99 inter-quantile range. The numerical columns should be normalized
    using StandardScaler from sklearn. The values in the target colummn should be
    converted to 0 or 1 and should be of type int.
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'We now get:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: which is almost perfect for our use if we exclude the part that tries to load
    in the data from a csv file. It’s quite difficult to convince GPT to exclude this,
    even if we explicitly tell it to assume that `df` exists and should not be loaded.
    Lastly, we need to **load** the data into a local database. This is overkill for
    such a simple use case, but is a good habit to develop.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'I use the following instruction:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Connect to an sqlite database named “data”. Use pandas to insert data from a
    DataFrame named “df” into a table named “income”. Do not include the index column.
    Commit the changes before closing the connection.
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'in order to get:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: which is perfect, minus the unnecessary creation of `df`. With this processed
    data in hand, we are ready to train some models.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Model Training
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/433e1a6b80dba4d54d04b33977243886.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: Illustration of a loss function by me + Midjourney
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'Resources permitting, it is a good idea to try out a few different model types
    to identify the one with the right level of complexity for the given task. Therefore,
    we ask GPT-3.5 to try out a few different models. First, let’s set up the generic
    prompt template for model training:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'and the prompt we’ll be using is:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'Train a variety of classification models to predict the “target” column using
    all other columns. Do so using 5-fold cross validation to choose the best model
    and corresponding set of hyperparameters, and return the best overall model and
    corresponding hyperparameter settings. Choose the best model based on accuracy.
    Assume a dataframe named “df” exists which is to be used for training. Log the
    entire process using MLFlow. Start logging with mlflow before training any models
    so only a single run is stored. Make sure that the model is logged using the sklearn
    module of mlflow. Make sure that only the best overall model is logged, but log
    metrics for all model types. The mean value of the following metrics on all cross
    validation folds should be logged: accuracy, AUC, F1 score'
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s have a look at the generated output and this time go deeper into why some
    of the specific instructions had to be provided.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'If we remove the loading of `df` and the section `# set up mlflow` , we end
    up with exactly what is desired. Namely, a loop over a 3 different model types,
    performing a grid search using 5-fold cross validation to identify the best hyperparmeters
    for the given model type, while keeping track of metrics. Without specifying “choose
    the best model based on accuracy”, the generated code will use `scoring=[“accuracy”,
    “roc_auc", “f1”]` for the grid search which will not work since there is ambiguity
    as to how to select the best model according to multiple metrics. Without “make
    sure that the model is logged using the sklearn module of mlflow”, we sometimes
    end up with `mlflow.log_model()` which is wrong. Also, “make sure that only the
    best overall model is logged” is necessary to avoid storing all models. Overall,
    this output is acceptable, but it’s unstable, and running it multiple times is
    likely to introduce different bugs. In order to have everything ready for the
    **serving** step, it is useful to add the model signature when saving the model.
    This signature is basically a collection of feature names and their corresponding
    types. It is a pain to get GPT-3.5 to add this, so some manual labor has to be
    done by first adding the import:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'and then modifying the line of code which logs the model via:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Model Serving
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/00f90a37907f52b68f7916a92f02cccb.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: Illustration of deployment by me + Midjourney
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we used MLflow to log the best model, we have a couple of options to
    serve the model. The simplest option is to host the model locally. Let’s first
    design the general prompt template for model serving:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'and the prompt will be:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Serve the model using port number 1111, and use the local environment manager
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'By calling `serve_model("<model path here>", question)` we get:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Once we run this command in the shell, we are ready to make predictions by sending
    data encoded as JSON to the model. We’ll first generate the command to send data
    to the model, and then create the JSON payload to be inserted into the command.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The following request will be inserted into the prompt template in `send_request()`:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Use the “curl” command to send data “<data here>” to an mlflow model hosted
    at port 1111 on localhost. Make sure that the content type is “application/json”.
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The output generated by GPT-3.5 is:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: It is preferable to have the URL immediately after `curl` instead of being at
    the very end of the command, i.e.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Getting GPT-3.5 to do this is not easy. Both of the following requests fail
    to do so:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Use the “curl” command to send data “<data here>” to an mlflow model hosted
    at port 1111 on localhost. Place the URL immediately after “curl”. Make sure that
    the content type is “application/json”.
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Use the “curl” command, with the URL placed before any argument, to send data
    “<data here>” to an mlflow model hosted at port 1111 on localhost. Make sure that
    the content type is “application/json”.
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Maybe it’s possible to get the desired output if we have GPT-3.5 modify an
    existing command rather than generate one from scratch. Here is the generic template
    for modifying commands:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We will call this function as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'which finally gives us:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now time to create the payload:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The prompt for this part needed quite a bit of tuning to get the desired output
    format:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Convert the DataFrame “df” to json format that can be received by a deployed
    MLFlow model. Wrap the resulting json in an object called “dataframe_split”. The
    resulting string should not have newlines, and it should not escape quotes. Also,
    “dataframe_split” should be surrounded by doubles quotes instead of single quotes.
    Do not include the “target” column. Use the split “orient” argument
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Without the explicit instruction to avoid newlines and escaping quotes, a call
    to `json.dumps()` is made which is not the format that the MLflow endpoint expects.
    The generated command is:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Before replacing `<data here>` in the `curl` request with the value of `wrapped_data`,
    we probably want to send only a few rows of data for prediction, otherwise the
    resulting payload is too large. So we modify the above to be:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Invoking the model gives:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: whereas the actual targets are [0, 0, 1, 1, 0].
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: There we have it. At the beginning of this post, we didn’t even have access
    to a dataset, yet we’ve managed to end up with a deployed model that was selected
    to be the best through cross-validation. Importantly, GPT-3.5 did all the heavy
    lifting, and only required minimal assistance along the way. I did however have
    to specify particular libraries to use and methods to call, but this was mainly
    required to resolve ambiguities. Had I specified “Log the entire process” instead
    of “Log the entire process using MLFlow”, GPT-3.5 would have too many libraries
    to choose from, and the resulting model format might not have been useful for
    serving with MLflow. Thus, some knowledge of the tools used to perform the various
    steps in the ML pipeline is required to have success using GPT-3.5, but it is
    minimal compared to the knowledge required to code from scratch.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'Another option for serving the model is to host it as a SageMaker endpoint
    on AWS. Despite how easy this may look on the MLflow [website](https://mlflow.org/docs/latest/models.html#deploy-a-python-function-model-on-amazon-sagemaker),
    I assure you that as with many examples on the web involving AWS, things will
    go wrong. First of all, Docker must be installed in order to generate the Docker
    Imager using the command:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个服务模型的选项是将其作为 SageMaker 端点托管在 AWS 上。尽管这在 MLflow 的 [网站](https://mlflow.org/docs/latest/models.html#deploy-a-python-function-model-on-amazon-sagemaker)
    上看起来很简单，但我向你保证，就像很多涉及 AWS 的网页示例一样，事情会出错。首先，必须安装 Docker，以便使用以下命令生成 Docker 镜像：
- en: '[PRE25]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Second, the Ptyhon library `boto3` used to communicate with AWS also requires
    installation. Beyond this, permissions must be properly setup such that SageMaker,
    ECR, and S3 services can communicate with each other on behalf of your account.
    Here are the commands I ended up having to use:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，用于与 AWS 通信的 Python 库 `boto3` 也需要安装。此外，必须正确设置权限，以便 SageMaker、ECR 和 S3 服务能够代表你的账户进行通信。以下是我最终不得不用的命令：
- en: '[PRE26]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: along with some manual tinkering behind the scenes to get the S3 bucket to be
    in the correct region.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 以及在幕后进行一些手动调整，以确保 S3 存储桶位于正确的区域。
- en: With the help of GPT-3.5 we went through the ML pipeline in a (mostly) painless
    way, though the last mile was a bit trickier. Note how I didn’t use GPT-3.5 to
    generate the commands for serving the model on AWS. It works poorly for this use
    case, and creates made up argument names. I can only speculate that switching
    to the GPT-4.0 API would help resolve some of the above bugs, and lead to an even
    easier model development experience.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GPT-3.5 的帮助下，我们以（大部分）顺利的方式完成了 ML 流程，尽管最后的步骤有些棘手。注意我没有使用 GPT-3.5 生成在 AWS 上服务模型的命令。它在这个用例中效果较差，并且会生成虚构的参数名称。我只能推测，切换到
    GPT-4.0 API 可能有助于解决一些上述的错误，并带来更轻松的模型开发体验。
- en: While the ML pipeline can be fully automated using LLMs, it isn’t yet safe to
    have a non-expert be responsible for the process. The bugs in the above code were
    easily identified because the Python interpreter would throw errors, but there
    are more subtle bugs that can be harmful. For example, the elimination of outlier
    values in the preprocessing code could be wrong leading to excess or insufficient
    samples being discarded. In the worst case, it could inadvertently drop entire
    subgroups of people, exacerbating potential fairness issues.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 ML 流程可以通过 LLMs 完全自动化，但目前还不安全让非专家负责这个过程。上述代码中的错误容易被识别，因为 Python 解释器会抛出错误，但还有更微妙的错误可能会造成伤害。例如，预处理代码中删除异常值可能会导致过多或不足的样本被丢弃。最糟糕的情况下，它可能会无意中丢弃整个子群体，加剧潜在的公平性问题。
- en: Additionally, the grid search over hyperparameters could have been done over
    a poorly chosen range, leading to overfitting or underfitting depending on the
    range. This would be quite tricky to identify for someone with little ML experience
    as the code otherwise seems correct, but an understanding of how regularization
    works in these models is required. Thus, it isn’t yet appropriate to have an unspecialized
    software engineer stand in for an ML engineer, but that time is fast approaching.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，超参数的网格搜索可能是在选择不当的范围内进行的，这可能导致过拟合或欠拟合，具体取决于范围。对于没有多少 ML 经验的人来说，这将很难识别，因为代码看起来没有问题，但需要理解这些模型中正则化的工作原理。因此，目前还不适合让未专门从事
    ML 的软件工程师代替 ML 工程师，但这一时刻正在快速到来。
- en: '[1] Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml].
    Irvine, CA: University of California, School of Information and Computer Science.
    (CC BY 4.0)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Dua, D. 和 Graff, C. (2019)。UCI 机器学习库 [http://archive.ics.uci.edu/ml]。加利福尼亚州欧文市：加州大学信息与计算机科学学院。（CC
    BY 4.0）'
