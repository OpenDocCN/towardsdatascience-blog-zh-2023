- en: Towards Unbiased Evaluation of Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/towards-unbiased-evaluation-of-large-language-models-9a7315144389?source=collection_archive---------4-----------------------#2023-12-09](https://towardsdatascience.com/towards-unbiased-evaluation-of-large-language-models-9a7315144389?source=collection_archive---------4-----------------------#2023-12-09)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How benchmark leakage and data contamination undermine LLMs evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://donatoriccio.medium.com/?source=post_page-----9a7315144389--------------------------------)[![Donato
    Riccio](../Images/0af2a026e72a023db4635522cbca50eb.png)](https://donatoriccio.medium.com/?source=post_page-----9a7315144389--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9a7315144389--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9a7315144389--------------------------------)
    [Donato Riccio](https://donatoriccio.medium.com/?source=post_page-----9a7315144389--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe384fc71d292&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftowards-unbiased-evaluation-of-large-language-models-9a7315144389&user=Donato+Riccio&userId=e384fc71d292&source=post_page-e384fc71d292----9a7315144389---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9a7315144389--------------------------------)
    ·7 min read·Dec 9, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9a7315144389&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftowards-unbiased-evaluation-of-large-language-models-9a7315144389&user=Donato+Riccio&userId=e384fc71d292&source=-----9a7315144389---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9a7315144389&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftowards-unbiased-evaluation-of-large-language-models-9a7315144389&source=-----9a7315144389---------------------bookmark_footer-----------)![](../Images/dc6cb8fa100c8187fb0ad581ac1e56a3.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image by author. (AI-assisted)
  prefs: []
  type: TYPE_NORMAL
- en: “Our new LLM beats GPT in every benchmark!”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is becoming increasingly common to hear bold claims like this, as the hype
    around LLMs is huge. There are new models every week, and currently everyone is
    trying to compete with GPT-4, which is still the most powerful LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking is a critical part of evaluating progress in large language models.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarks like **MMLU** and **HellaSwag** are the standard for assessing language
    models on skills like reasoning and comprehension. The scores provide a snapshot
    of progress, with new state-of-the-art results heralded as breakthroughs. LLMs
    are usually evaluated in a zero-shot setting, without explicit training on the
    test set, to gauge their general abilities.
  prefs: []
  type: TYPE_NORMAL
- en: This article shows how easy it is to manipulate benchmark results and offers
    suggestions to maintain evaluation integrity.
  prefs: []
  type: TYPE_NORMAL
- en: The Trouble with Benchmarks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Often, benchmarks don’t reflect usefulness in real-life scenarios. Google’s
    newest model, Gemini Ultra, scores **90.04% on MMLU**. While this is an impressive
    score, taking a closer look at the evaluation methodology, it is ***CoT@32***…
  prefs: []
  type: TYPE_NORMAL
