- en: 'Machine Learning Algorithms Part 1: Linear Regression'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习算法第1部分：线性回归
- en: 原文：[https://towardsdatascience.com/machine-learning-algorithms-part-1-linear-regression-a7079238edc9?source=collection_archive---------13-----------------------#2023-01-06](https://towardsdatascience.com/machine-learning-algorithms-part-1-linear-regression-a7079238edc9?source=collection_archive---------13-----------------------#2023-01-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/machine-learning-algorithms-part-1-linear-regression-a7079238edc9?source=collection_archive---------13-----------------------#2023-01-06](https://towardsdatascience.com/machine-learning-algorithms-part-1-linear-regression-a7079238edc9?source=collection_archive---------13-----------------------#2023-01-06)
- en: Predict the Price of Diamonds Using Linear Regression
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用线性回归预测钻石价格
- en: '[](https://medium.com/@rohankvij?source=post_page-----a7079238edc9--------------------------------)[![Rohan
    Vij](../Images/6ef53fffb4749e1665360555bf18275f.png)](https://medium.com/@rohankvij?source=post_page-----a7079238edc9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a7079238edc9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a7079238edc9--------------------------------)
    [Rohan Vij](https://medium.com/@rohankvij?source=post_page-----a7079238edc9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@rohankvij?source=post_page-----a7079238edc9--------------------------------)[![Rohan
    Vij](../Images/6ef53fffb4749e1665360555bf18275f.png)](https://medium.com/@rohankvij?source=post_page-----a7079238edc9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a7079238edc9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a7079238edc9--------------------------------)
    [Rohan Vij](https://medium.com/@rohankvij?source=post_page-----a7079238edc9--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe44b36765084&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-part-1-linear-regression-a7079238edc9&user=Rohan+Vij&userId=e44b36765084&source=post_page-e44b36765084----a7079238edc9---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a7079238edc9--------------------------------)
    ·12 min read·Jan 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa7079238edc9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-part-1-linear-regression-a7079238edc9&user=Rohan+Vij&userId=e44b36765084&source=-----a7079238edc9---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe44b36765084&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-part-1-linear-regression-a7079238edc9&user=Rohan+Vij&userId=e44b36765084&source=post_page-e44b36765084----a7079238edc9---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a7079238edc9--------------------------------)
    · 12分钟阅读 · 2023年1月6日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa7079238edc9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-part-1-linear-regression-a7079238edc9&user=Rohan+Vij&userId=e44b36765084&source=-----a7079238edc9---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa7079238edc9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-part-1-linear-regression-a7079238edc9&source=-----a7079238edc9---------------------bookmark_footer-----------)![](../Images/1d414630e1c9e783288706a39dafce5d.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa7079238edc9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-part-1-linear-regression-a7079238edc9&source=-----a7079238edc9---------------------bookmark_footer-----------)![](../Images/1d414630e1c9e783288706a39dafce5d.png)'
- en: Photo by [Bas van den Eijkhof](https://unsplash.com/@basvde?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Bas van den Eijkhof](https://unsplash.com/@basvde?utm_source=medium&utm_medium=referral)
    提供，发布在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Linear regression is a powerful but relatively simple tool that can be used
    to understand the relationship between variables. This tutorial will explore the
    fundamentals of linear regression in a beginner-friendly way. By the end of this
    tutorial, you will have a solid understanding of linear regression and how to
    implement it using real-world data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归是一种强大但相对简单的工具，可以用来理解变量之间的关系。本教程将以初学者友好的方式探索线性回归的基础知识。在本教程结束时，你将对线性回归有一个扎实的理解，并知道如何使用实际数据实现它。
- en: '**What Is Linear Regression?**'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**什么是线性回归？**'
- en: 'Linear regression, a statistical method first used in 1877, predicts the value
    of a dependent from an independent variable. Essentially, it “fits” a linear line
    to most accurately match the relationship of the dependent and independent variable
    based upon a multitude of points provided to the model, similar to that of a scatter
    plot. It is easiest to observe with a graph:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归，作为一种统计方法，首次用于1877年，用于预测因变量的值。实质上，它通过对提供给模型的多个点进行“拟合”来最准确地匹配因变量和自变量之间的关系，这类似于散点图。通过图表最容易观察到这一点：
- en: '![](../Images/280f906bc3c040c6ceed5b0df0571182.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/280f906bc3c040c6ceed5b0df0571182.png)'
- en: 'Source: [Wikipedia](https://en.wikipedia.org/wiki/File:Linear_regression.svg).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[维基百科](https://en.wikipedia.org/wiki/File:Linear_regression.svg)。
- en: Linear regression works by creating a linear line (in the form `y=mx+b`) to
    most accurately predict the value of dependent variables by solving for values
    `m` (slope) and `b` (y-intercept).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归通过创建一条线性直线（形式为`y=mx+b`）来最准确地预测因变量的值，通过求解值`m`（斜率）和`b`（y截距）。
- en: Least Squares
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最小二乘法
- en: 'To do this, models use a method known as least squares in order to most accurately
    find the line of best fit. The goal of this method is to reduce the total squares
    of the deviation of specific data points to the most fitted line as much as possible.
    The most fitted line will have the lowest resultant value of the least squares
    function. We can calculate the deviations from each provided point to the most
    fitted line using the equation:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，模型使用了一种称为最小二乘法的方法，以最准确地找到最佳拟合线。该方法的目标是尽可能减少特定数据点到最拟合线的偏差平方总和。最拟合的直线将具有最小的最小二乘函数结果值。我们可以使用以下方程计算从每个提供的点到最拟合线的偏差：
- en: '![](../Images/e82432fc5136879968169e1e73d27967.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e82432fc5136879968169e1e73d27967.png)'
- en: Image by the author.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由作者提供。
- en: Essentially, the linear function’s output dependent value (y-value) is subtracted
    from a given data point’s dependent variable. This value can either be positive
    or negative depending on whether the value of the function is greater than or
    lower than that of the data point. However, whether the deviation is positive
    or negative is irrelevant — the number is squared regardless.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上，线性函数的输出因变量（y值）从给定数据点的因变量中减去。这个值可以是正数也可以是负数，取决于函数的值是否大于或小于数据点的值。然而，偏差是正还是负并不重要——无论如何，数值都会被平方。
- en: 'More simply, we can use a sum to find the total value of the squares of all
    the deviations:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 更简单地说，我们可以用一个总和来找到所有偏差平方的总值：
- en: '![](../Images/21a34f6070511f02dc3f3a13e3cb7841.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21a34f6070511f02dc3f3a13e3cb7841.png)'
- en: Image by the author.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由作者提供。
- en: The least squares method claims that the most accurate/fitted line to the data
    will have the smallest sum (`S`).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最小二乘法声称，最准确/拟合的数据线将具有最小的总和（`S`）。
- en: Examples
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例
- en: 'Using points : `(1,2), (3,5), (5,2)`.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用点：`(1,2), (3,5), (5,2)`。
- en: '![](../Images/633e85d36fb275b8317d9a6d78b3a24d.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/633e85d36fb275b8317d9a6d78b3a24d.png)'
- en: Image by the author.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由作者提供。
- en: Now is a good time to mention why the values in least squares are squared. First
    of all, as previously mentioned, it ensures all the deviations are positive. More
    importantly, however, it ensures that higher deviations are given more weight.
    This allows the fitted line to cater more towards outliers than it might normally.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是提到为什么最小二乘法中的值被平方的好时机。首先，如前所述，它确保所有偏差都是正值。然而，更重要的是，它确保较大的偏差被赋予更多的权重。这使得拟合线能够更多地关注异常值。
- en: Take, for instance, the leftmost graph. We can see that deviations from the
    line to the first two data points are either 0 or negligible. In contrast, the
    other 2 graphs’ lines on average are closer to the data points. When comparing
    the raw (not squared) deviation values, we can see that they all are fairly close
    to each other. When comparing the squared deviation values, we can see that the
    leftmost graph’s deviation is more than 600% greater than that of the right graph.
    This is because larger deviations are penalized more, meaning outliers have a
    greater impact on the final line.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以最左边的图为例。我们可以看到，从直线到前两个数据点的偏差要么是0，要么可以忽略不计。相比之下，其他两个图的直线在平均上更接近数据点。比较原始（未平方的）偏差值时，我们可以看到它们彼此相当接近。比较平方偏差值时，我们可以看到最左边的图的偏差比右边的图大600%以上。这是因为较大的偏差受到的惩罚更大，这意味着异常值对最终直线的影响更大。
- en: Using Least Squares
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用最小二乘法
- en: There are two ways in which least squares can be utilized. While using matrix
    operations is the most computationally efficient and widely used method, we will
    explore using gradient descent to go about finding the best line. Gradient descent
    is an optimization algorithm wherein we will calculate the derivative of the sum
    and then modify the coefficient values based on the direction in which the derivative
    indicates they should move in. This process will be iterated through until the
    most optimal solution is found. This is just a brief overview of gradient descent;
    stay tuned for an article explaining gradient descent for those who do not know
    calculus.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 最小二乘法可以通过两种方式实现。虽然使用矩阵运算是计算效率最高、最广泛使用的方法，我们将探讨使用梯度下降来寻找最佳直线。梯度下降是一种优化算法，我们将在其中计算和的导数，然后根据导数指示的方向调整系数值。这个过程会重复进行，直到找到最优解。这只是梯度下降的简要概述；请关注将为不懂微积分的人解释梯度下降的文章。
- en: MSE vs SSE
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MSE与SSE
- en: 'We will be using the MSE cost function (**M**ean of **S**quared **E**rrors)
    as our cost function. Essentially, we want to minimize the value of this cost
    function in order to output the most fitted line. Previously, we used SSE (Sum
    of Squared Errors) to determine what line was the most fitted to its points. MSE
    is pretty self-explanatory — it’s identical to SSE, but we divide the final sum
    by the number of data points summed:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用均方误差（**M**ean of **S**quared **E**rrors）作为我们的成本函数。基本上，我们想要最小化这个成本函数的值，以输出最拟合的直线。之前，我们使用SSE（平方误差和）来确定哪条直线最适合其数据点。MSE相当直观——它与SSE相同，但我们将最终的和除以数据点的数量：
- en: '![](../Images/0a198e4953b50fa40e05c494e0720ac2.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a198e4953b50fa40e05c494e0720ac2.png)'
- en: Image by the author.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片。
- en: MSE is more popular than SSE for a number of reasons.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: MSE比SSE更受欢迎，有很多原因。
- en: Firstly, MSE is less subject to outliers than SSE. As MSE normalizes the errors
    by the number of data points, outliers have less of an impact. Let’s assume a
    data set with the errors `1, 4, 1, 25`. The outlier (25) would only account for
    25% of the error calculated through MSE. As a result, the MSE would be `7.75`.
    The SSE would be `31`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，均方误差（MSE）比平方误差和（SSE）对离群点的敏感度更低。由于MSE通过数据点的数量来归一化误差，离群点的影响较小。假设数据集中的误差为`1,
    4, 1, 25`。离群点（25）只占通过MSE计算误差的25%。因此，MSE将是`7.75`。SSE将是`31`。
- en: Secondly, using MSE also allows for the fittedness of different lines to be
    compared to one another, even if they utilize a different number of data points.
    For instance, consider two models that use a different number of data points,
    Model A and Model B. If Model A uses 100 data points and Model B uses 50 data
    points, most of the time Model A will have a higher SSE. However, if the error
    is normalized by using MSE, the models can be directly compared regardless of
    how many data points they use.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，使用MSE也允许比较不同直线的拟合度，即使它们使用的数据点数量不同。例如，考虑使用不同数量数据点的两个模型，模型A和模型B。如果模型A使用100个数据点，模型B使用50个数据点，大多数情况下模型A将有更高的SSE。然而，如果通过使用MSE来归一化误差，无论模型使用多少数据点，这些模型都可以直接进行比较。
- en: The combined factors mentioned above mean that MSE is more interpretable than
    SSE. Outliers in a dataset might make one model seem significantly better than
    another if they are compared using SSE, even though that model might fit the majority
    of data points better.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 上述因素的结合意味着MSE比SSE更易于解释。数据集中的离群点可能会使一个模型看起来比另一个模型显著更好，如果它们使用SSE进行比较，即使该模型可能更好地拟合大多数数据点。
- en: Code Time!
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码时间！
- en: With all the knowledge we have gained about linear regression, we can now implement
    it ourselves using Python!
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 有了我们对线性回归的所有知识，我们现在可以使用Python自行实现它！
- en: Getting Started
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开始使用
- en: 'For this tutorial, you need:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本教程，你需要：
- en: Python (version 3.7 or above) — basic experience recommended
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python（版本3.7或更高）— 推荐有基础经验
- en: '(Installation tutorial: [https://www.tutorialspoint.com/how-to-install-python-in-windows](https://www.tutorialspoint.com/how-to-install-python-in-windows))'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: （安装教程：[https://www.tutorialspoint.com/how-to-install-python-in-windows](https://www.tutorialspoint.com/how-to-install-python-in-windows)）
- en: 'Then, install three packages using `pip` in your terminal:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在你的终端中使用`pip`安装三个包：
- en: '`pip install notebook`'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pip install notebook`'
- en: '`pip install numpy`'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pip install numpy`'
- en: '`pip install matplotlib`'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pip install matplotlib`'
- en: 'Run `jupyter notebook` in your terminal. Your default web browser should open
    a tab where you see a file explorer. Simply go to the directory you wish to create
    your program in and then create a Python 3 Notebook (select `new` in the upper
    right-hand corner). You should now see this screen:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在终端中运行`jupyter notebook`。你的默认网页浏览器会打开一个选项卡，在其中你会看到文件资源管理器。简单地进入你希望创建程序的目录，然后创建一个Python
    3 Notebook（在右上角选择`new`）。你现在应该会看到以下界面：
- en: '![](../Images/948a850f5402c3655843781eda5f6812.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/948a850f5402c3655843781eda5f6812.png)'
- en: Image by the author.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: You can rename the file by clicking on “Untitled.”
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过点击“Untitled”来重命名文件。
- en: Initial Data
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始数据
- en: We will start by importing the Python library NumPy, which is invaluable for
    its ability to easily perform mathematical operations on arrays of numbers. We
    will then define the NumPy array for our points, as well as initialize the slope
    and intercept variables to 0\. Looking at the points, it is easy to deduce that
    the most fitted line for these points will be `y=1x+0`. We are using such predictable
    values so we can benchmark the model.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始导入Python库NumPy，它在对数字数组进行数学操作时非常有用。然后，我们将定义点的NumPy数组，并将斜率和截距变量初始化为0。看这些点，很容易推断出这些点的最适合的线是`y=1x+0`。我们使用这样的可预测值，以便对模型进行基准测试。
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Press `alt` + `enter` to create a new cell.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 按`alt` + `enter`来创建一个新单元格。
- en: Linear Function
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性函数
- en: 'As we are performing linear regression, it is useful to have a function that
    can evaluate a linear function:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在执行线性回归，拥有一个可以评估线性函数的函数是有用的：
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Press `alt` + `enter` to create a new cell.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 按`alt` + `enter`来创建一个新单元格。
- en: Cost Function
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成本函数
- en: 'It would also help to include a cost function in order to measure the effectiveness
    of our regression:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 包含一个成本函数来衡量我们回归的有效性也会有帮助：
- en: '[PRE2]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Press `alt` + `enter` to create a new cell.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 按`alt` + `enter`来创建一个新单元格。
- en: Gradient Descent
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度下降
- en: 'We will start by defining our input and output values (x-coordinates and y-coordinates):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始定义我们的输入和输出值（x坐标和y坐标）：
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, we will implement the gradient descent:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将实现梯度下降：
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If this does not make much sense, don’t worry — gradient descent is too complicated
    to explain in depth within this article, so keep an eye out for an article explaining
    gradient descent for those who don’t know calculus!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这不太有意义，别担心——梯度下降过于复杂，无法在本文中深入解释，所以请留意一篇解释梯度下降的文章，特别是对于不懂微积分的读者！
- en: 'However, it is important that you understand epochs. Namely, this line:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 但你理解周期是很重要的。也就是说，这一行：
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: An epoch is essentially an iteration of the gradient descent program. In each
    iteration of the loop, the slope and intercept of the linear function are adjusted
    using the math from the gradient descent calculations. The more epochs we run,
    the more the values are adjusted and fine-tuned.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一个周期基本上是梯度下降程序的一次迭代。在每次迭代中，线性函数的斜率和截距会使用梯度下降计算中的数学公式进行调整。运行的周期越多，值调整和微调得越多。
- en: 'After running this program, you should get the following values for the slope
    and intercept:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此程序后，你应该得到以下斜率和截距值：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If we were to run the program for more epochs, these values would be even closer
    to 1 and 0, respectively. However, these values are close to the expected outcome
    for all intents and purposes.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们运行更多周期，这些值会更接近1和0。然而，这些值已经非常接近预期结果。
- en: 'Graphically, this is what the result looks like:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 从图形上看，结果如下：
- en: '![](../Images/d1c7de1a913aac45ab6c08b5107abe14.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1c7de1a913aac45ab6c08b5107abe14.png)'
- en: Image by the author.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: Analyzing The Results
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果分析
- en: 'The final MSE is `0.00015821003618271694` — an extremely low value. However,
    if we were to graph the MSE every epoch (or iteration), we would get the following
    graph:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的均方误差（MSE）是`0.00015821003618271694`——这是一个极低的值。然而，如果我们将MSE图形化显示每个周期（或迭代），我们将得到以下图表：
- en: '![](../Images/86371f300cd05143671fbe67d2ccd790.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/86371f300cd05143671fbe67d2ccd790.png)'
- en: Image by the author.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: 'These seem to be some very, very diminishing returns. In fact, after epoch
    25 or so, it seems as if the MSE does not change at all! Let’s look at this graph
    from a different perspective, omitting the first 50 epochs:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这些似乎是非常、非常小的收益。实际上，在第25个周期左右，MSE似乎完全没有变化！让我们从不同的角度看这个图，省略前50个周期：
- en: '![](../Images/33b58c297ebf565bc61907361eef184d.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/33b58c297ebf565bc61907361eef184d.png)'
- en: Image by the author.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: 'What looks like a straight line is not — the MSE gets almost 100 times smaller
    from epoch 50 to epoch 1000\. You might be asking yourself — isn''t an MSE of
    ~0.015 already low enough? Let’s try running the gradient descent again, but this
    time with only 50 epochs:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 看似直线的并非直线——从第50个epoch到第1000个epoch，MSE几乎减小了100倍。你可能会问——MSE约0.015不是已经够低了吗？让我们尝试再次运行梯度下降，但这次只用50个epochs：
- en: '[PRE7]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Close, but not close enough:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 接近，但还不够接近：
- en: '![](../Images/07e30fabf4a7e0bdda7dcce84cbd9a04.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07e30fabf4a7e0bdda7dcce84cbd9a04.png)'
- en: Image by the author.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片。
- en: 'On the contrary, let’s run the gradient descent with 100,000 epochs:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，让我们用100,000个epochs运行梯度下降：
- en: '![](../Images/329f8664397e8e2600f47dbdc6dc5032.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/329f8664397e8e2600f47dbdc6dc5032.png)'
- en: Image by the author.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片。
- en: Perfect! It seems that running the model with 100,000 epochs gives us a practically
    perfect result. While running more epochs with this linear regression does give
    us more accuracy with this model, it is important to consider the balance between
    accuracy and time. In general, you should aim to use enough epochs to fit the
    model’s data, but not so many that the model takes an unnecessary amount of time
    to train. A technique called early stopped is commonly used in models of all types,
    where a model is automatically stopped once it reaches a certain accuracy. This
    allows the model to be trained as fast as possible, but still ensures some level
    of accuracy.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 完美！似乎运行100,000个epochs的模型给出了几乎完美的结果。虽然用更多的epochs运行线性回归可以提高模型的准确性，但重要的是要考虑准确性和时间之间的平衡。一般来说，您应该使用足够的epochs来拟合模型的数据，但不要使用过多，以至于模型训练的时间不必要地延长。通常在各种模型中使用一种叫做早期停止的技术，当模型达到一定的准确性时会自动停止。这允许模型尽可能快地训练，但仍确保一定的准确性。
- en: Applying Linear Regression
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用线性回归
- en: Last, but certainly not least, it is time to apply our linear regression knowledge
    to some real data!
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，是时候将我们的线性回归知识应用到实际数据上了！
- en: Finding a Dataset
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查找数据集
- en: Let’s start with finding a dataset. [Kaggle](https://www.kaggle.com/) is a great
    resource to find free datasets that are high quality and varied in terms of their
    structure or topic. For this mini-project, I chose to use the [Data Analysis on
    Diamonds Dataset](https://www.kaggle.com/datasets/swatikhedekar/price-prediction-of-diamond),
    in order to develop a linear relation between the carat of a diamond (independent
    variable) and its price (dependent variable).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从寻找数据集开始。[Kaggle](https://www.kaggle.com/)是一个很好的资源，可以找到高质量且在结构或主题上各异的免费数据集。对于这个小项目，我选择使用[钻石数据分析数据集](https://www.kaggle.com/datasets/swatikhedekar/price-prediction-of-diamond)，以便开发钻石克拉（自变量）与其价格（因变量）之间的线性关系。
- en: Choosing a Dataset
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择数据集
- en: 'In general, when choosing or building a dataset to run linear regression upon,
    it is important to consider the following factors:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在选择或构建用于线性回归的数据集时，需要考虑以下因素：
- en: A strong linear correlation between the independent and dependent variables
    — if the variables seem to be not correlated at all or their correlation is non-linear,
    it might be better to choose a different regression method.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自变量和因变量之间的强线性相关性——如果这些变量似乎没有相关性，或其相关性是非线性的，可能需要选择不同的回归方法。
- en: Outliers — a good data set should be relatively free of outliers, as they can
    heavily influence the performance of the regression.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 异常值——一个好的数据集应该相对没有异常值，因为它们会严重影响回归的性能。
- en: Suitability — the dataset must be relevant to the problem you are trying to
    solve. For instance, if you want to predict the price of a house in New York City
    based on its square feet, training the model on data from farms in the Montana
    countryside would not be appropriate.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 适用性——数据集必须与您试图解决的问题相关。例如，如果您想根据房屋的平方英尺预测纽约市的房价，那么用蒙大拿乡村农场的数据来训练模型就不合适。
- en: Downloading the Dataset
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载数据集
- en: Downloading a dataset is quite intuitive in Kaggle. Press the black download
    button in the top right corner of the screen, and save the `.zip` file to your
    computer. You can then unzip the file and move the `.csv`file within it to the
    same directory as your Jupyter Notebook file.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kaggle上下载数据集非常直观。点击屏幕右上角的黑色下载按钮，将`.zip`文件保存到计算机上。然后解压缩文件，将其中的`.csv`文件移动到与Jupyter
    Notebook文件相同的目录中。
- en: Implementation
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施
- en: 'Now, all that’s left to do is use the data in the linear regression model.
    Again, in this instance, we will be predicting a diamond’s price based on its
    carat. Comment out the following lines:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，剩下的就是使用数据进行线性回归模型。再次地，在这个实例中，我们将根据钻石的克拉数预测价格。注释掉以下行：
- en: '[PRE8]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Replace them with the following:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 用以下内容替换它们：
- en: '[PRE9]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'After running the program (for 1000 epochs), the outputs are:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 运行程序（1000个周期）后的输出结果是：
- en: '[PRE10]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The final MSE is:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的MSE是：
- en: '[PRE11]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Woah! That is an extremely large number. Admittedly, the diamond dataset we
    used is flawed. The linear regression on a graph is:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！这是一个极其庞大的数字。诚然，我们使用的钻石数据集存在缺陷。图上的线性回归是：
- en: '![](../Images/7314bd92726ee179a7025e403ccb9180.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7314bd92726ee179a7025e403ccb9180.png)'
- en: Image by the author.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: Take, for example, the varying prices of a 1-carat diamond. It can range from
    ~$1000 to almost $20,000! This is a prime example of this dataset not having enough
    of a linear relationship between *two* variables. In this case, the cut, color,
    and clarity of the diamonds all heavily contribute to the price as well. It is
    also important to consider that with real-world data, an MSE of 0 is practically
    impossible. Real-world phenomena are influenced by a plethora of factors, and
    capturing all of them in a regression model is practically impossible. It is left
    as an exercise to the reader to explore more datasets on Kaggle where the linear
    correlation between two given variables is stronger.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 以1克拉钻石的不同价格为例，它的价格范围可以从 ~$1000 到近$20,000！这是一个典型的例子，说明这个数据集在*两个*变量之间的线性关系不足。在这种情况下，钻石的切工、颜色和清晰度也都对价格产生了重大影响。同时，需要考虑的是，现实世界的数据中，MSE为0几乎是不可能的。现实世界现象受到众多因素的影响，捕捉所有这些因素并在回归模型中反映出来几乎是不可能的。留给读者作为练习的是探索Kaggle上更强的两个给定变量之间的线性相关性数据集。
- en: Testing
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试
- en: 'Let’s put our model to the test. According to [CreditDonkey](https://www.creditdonkey.com/1-carat-diamond.html),
    the best value for a 1-carat diamond is anywhere from $4500—$6000\. Using the
    following code:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来测试一下我们的模型。根据[CreditDonkey](https://www.creditdonkey.com/1-carat-diamond.html)，1克拉钻石的最佳价值在$4500到$6000之间。使用以下`code`：
- en: '[PRE12]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The model outputs:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 模型输出结果：
- en: '[PRE13]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Success!
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 成功！
- en: Conclusion
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Let’s recap — linear regression is a statistical method used to comprehend the
    relationship between two variables that are correlated linearly. This is done
    by fitting a line in the form `y=mx+b`to the provided independent and dependent
    variables. The most fitted line can be found by using a method known as least
    squares, which minimizes the sum of the squared deviation from each point to its
    corresponding point on the line. Least squares can be implemented using both matrix
    operations and gradient descent, with this article focusing on the utilization
    of gradient descent. The MSE cost function (i.e the mean of squared errors) was
    used in order to determine the accuracy of the model. By minimizing the MSE, we
    can optimize the model and improve its accuracy.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下——线性回归是一种统计方法，用于理解两个线性相关变量之间的关系。这是通过将一条形式为`y=mx+b`的直线拟合到提供的自变量和因变量上来完成的。通过使用称为最小二乘法的方法，可以找到最适合的直线，该方法最小化每个点到其对应直线上的点的平方偏差之和。最小二乘法可以通过矩阵运算和梯度下降来实现，本文重点介绍了梯度下降的应用。使用MSE成本函数（即平方误差的均值）来确定模型的准确性。通过最小化MSE，我们可以优化模型并提高其准确性。
- en: 'I leave you with a satisfying GIF of the model slowly converging on the most
    fitted line:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我给你留下了一个令人满意的GIF，展示了模型逐渐收敛到最适合的直线：
- en: '![](../Images/38ec45e07fffd1c0c6cb2241eaca87b5.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38ec45e07fffd1c0c6cb2241eaca87b5.png)'
- en: Image by the author.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: Thank you for reading!
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 谢谢你的阅读！
