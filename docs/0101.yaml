- en: 'Machine Learning Algorithms Part 1: Linear Regression'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/machine-learning-algorithms-part-1-linear-regression-a7079238edc9?source=collection_archive---------13-----------------------#2023-01-06](https://towardsdatascience.com/machine-learning-algorithms-part-1-linear-regression-a7079238edc9?source=collection_archive---------13-----------------------#2023-01-06)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Predict the Price of Diamonds Using Linear Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@rohankvij?source=post_page-----a7079238edc9--------------------------------)[![Rohan
    Vij](../Images/6ef53fffb4749e1665360555bf18275f.png)](https://medium.com/@rohankvij?source=post_page-----a7079238edc9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a7079238edc9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a7079238edc9--------------------------------)
    [Rohan Vij](https://medium.com/@rohankvij?source=post_page-----a7079238edc9--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe44b36765084&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-part-1-linear-regression-a7079238edc9&user=Rohan+Vij&userId=e44b36765084&source=post_page-e44b36765084----a7079238edc9---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a7079238edc9--------------------------------)
    ·12 min read·Jan 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa7079238edc9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-part-1-linear-regression-a7079238edc9&user=Rohan+Vij&userId=e44b36765084&source=-----a7079238edc9---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa7079238edc9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-part-1-linear-regression-a7079238edc9&source=-----a7079238edc9---------------------bookmark_footer-----------)![](../Images/1d414630e1c9e783288706a39dafce5d.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Bas van den Eijkhof](https://unsplash.com/@basvde?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression is a powerful but relatively simple tool that can be used
    to understand the relationship between variables. This tutorial will explore the
    fundamentals of linear regression in a beginner-friendly way. By the end of this
    tutorial, you will have a solid understanding of linear regression and how to
    implement it using real-world data.
  prefs: []
  type: TYPE_NORMAL
- en: '**What Is Linear Regression?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Linear regression, a statistical method first used in 1877, predicts the value
    of a dependent from an independent variable. Essentially, it “fits” a linear line
    to most accurately match the relationship of the dependent and independent variable
    based upon a multitude of points provided to the model, similar to that of a scatter
    plot. It is easiest to observe with a graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/280f906bc3c040c6ceed5b0df0571182.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [Wikipedia](https://en.wikipedia.org/wiki/File:Linear_regression.svg).'
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression works by creating a linear line (in the form `y=mx+b`) to
    most accurately predict the value of dependent variables by solving for values
    `m` (slope) and `b` (y-intercept).
  prefs: []
  type: TYPE_NORMAL
- en: Least Squares
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To do this, models use a method known as least squares in order to most accurately
    find the line of best fit. The goal of this method is to reduce the total squares
    of the deviation of specific data points to the most fitted line as much as possible.
    The most fitted line will have the lowest resultant value of the least squares
    function. We can calculate the deviations from each provided point to the most
    fitted line using the equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e82432fc5136879968169e1e73d27967.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, the linear function’s output dependent value (y-value) is subtracted
    from a given data point’s dependent variable. This value can either be positive
    or negative depending on whether the value of the function is greater than or
    lower than that of the data point. However, whether the deviation is positive
    or negative is irrelevant — the number is squared regardless.
  prefs: []
  type: TYPE_NORMAL
- en: 'More simply, we can use a sum to find the total value of the squares of all
    the deviations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/21a34f6070511f02dc3f3a13e3cb7841.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: The least squares method claims that the most accurate/fitted line to the data
    will have the smallest sum (`S`).
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using points : `(1,2), (3,5), (5,2)`.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/633e85d36fb275b8317d9a6d78b3a24d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Now is a good time to mention why the values in least squares are squared. First
    of all, as previously mentioned, it ensures all the deviations are positive. More
    importantly, however, it ensures that higher deviations are given more weight.
    This allows the fitted line to cater more towards outliers than it might normally.
  prefs: []
  type: TYPE_NORMAL
- en: Take, for instance, the leftmost graph. We can see that deviations from the
    line to the first two data points are either 0 or negligible. In contrast, the
    other 2 graphs’ lines on average are closer to the data points. When comparing
    the raw (not squared) deviation values, we can see that they all are fairly close
    to each other. When comparing the squared deviation values, we can see that the
    leftmost graph’s deviation is more than 600% greater than that of the right graph.
    This is because larger deviations are penalized more, meaning outliers have a
    greater impact on the final line.
  prefs: []
  type: TYPE_NORMAL
- en: Using Least Squares
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are two ways in which least squares can be utilized. While using matrix
    operations is the most computationally efficient and widely used method, we will
    explore using gradient descent to go about finding the best line. Gradient descent
    is an optimization algorithm wherein we will calculate the derivative of the sum
    and then modify the coefficient values based on the direction in which the derivative
    indicates they should move in. This process will be iterated through until the
    most optimal solution is found. This is just a brief overview of gradient descent;
    stay tuned for an article explaining gradient descent for those who do not know
    calculus.
  prefs: []
  type: TYPE_NORMAL
- en: MSE vs SSE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will be using the MSE cost function (**M**ean of **S**quared **E**rrors)
    as our cost function. Essentially, we want to minimize the value of this cost
    function in order to output the most fitted line. Previously, we used SSE (Sum
    of Squared Errors) to determine what line was the most fitted to its points. MSE
    is pretty self-explanatory — it’s identical to SSE, but we divide the final sum
    by the number of data points summed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0a198e4953b50fa40e05c494e0720ac2.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: MSE is more popular than SSE for a number of reasons.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, MSE is less subject to outliers than SSE. As MSE normalizes the errors
    by the number of data points, outliers have less of an impact. Let’s assume a
    data set with the errors `1, 4, 1, 25`. The outlier (25) would only account for
    25% of the error calculated through MSE. As a result, the MSE would be `7.75`.
    The SSE would be `31`.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, using MSE also allows for the fittedness of different lines to be
    compared to one another, even if they utilize a different number of data points.
    For instance, consider two models that use a different number of data points,
    Model A and Model B. If Model A uses 100 data points and Model B uses 50 data
    points, most of the time Model A will have a higher SSE. However, if the error
    is normalized by using MSE, the models can be directly compared regardless of
    how many data points they use.
  prefs: []
  type: TYPE_NORMAL
- en: The combined factors mentioned above mean that MSE is more interpretable than
    SSE. Outliers in a dataset might make one model seem significantly better than
    another if they are compared using SSE, even though that model might fit the majority
    of data points better.
  prefs: []
  type: TYPE_NORMAL
- en: Code Time!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With all the knowledge we have gained about linear regression, we can now implement
    it ourselves using Python!
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this tutorial, you need:'
  prefs: []
  type: TYPE_NORMAL
- en: Python (version 3.7 or above) — basic experience recommended
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(Installation tutorial: [https://www.tutorialspoint.com/how-to-install-python-in-windows](https://www.tutorialspoint.com/how-to-install-python-in-windows))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, install three packages using `pip` in your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pip install notebook`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pip install numpy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pip install matplotlib`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Run `jupyter notebook` in your terminal. Your default web browser should open
    a tab where you see a file explorer. Simply go to the directory you wish to create
    your program in and then create a Python 3 Notebook (select `new` in the upper
    right-hand corner). You should now see this screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/948a850f5402c3655843781eda5f6812.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: You can rename the file by clicking on “Untitled.”
  prefs: []
  type: TYPE_NORMAL
- en: Initial Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will start by importing the Python library NumPy, which is invaluable for
    its ability to easily perform mathematical operations on arrays of numbers. We
    will then define the NumPy array for our points, as well as initialize the slope
    and intercept variables to 0\. Looking at the points, it is easy to deduce that
    the most fitted line for these points will be `y=1x+0`. We are using such predictable
    values so we can benchmark the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Press `alt` + `enter` to create a new cell.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we are performing linear regression, it is useful to have a function that
    can evaluate a linear function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Press `alt` + `enter` to create a new cell.
  prefs: []
  type: TYPE_NORMAL
- en: Cost Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It would also help to include a cost function in order to measure the effectiveness
    of our regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Press `alt` + `enter` to create a new cell.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will start by defining our input and output values (x-coordinates and y-coordinates):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will implement the gradient descent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If this does not make much sense, don’t worry — gradient descent is too complicated
    to explain in depth within this article, so keep an eye out for an article explaining
    gradient descent for those who don’t know calculus!
  prefs: []
  type: TYPE_NORMAL
- en: 'However, it is important that you understand epochs. Namely, this line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: An epoch is essentially an iteration of the gradient descent program. In each
    iteration of the loop, the slope and intercept of the linear function are adjusted
    using the math from the gradient descent calculations. The more epochs we run,
    the more the values are adjusted and fine-tuned.
  prefs: []
  type: TYPE_NORMAL
- en: 'After running this program, you should get the following values for the slope
    and intercept:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If we were to run the program for more epochs, these values would be even closer
    to 1 and 0, respectively. However, these values are close to the expected outcome
    for all intents and purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Graphically, this is what the result looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d1c7de1a913aac45ab6c08b5107abe14.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing The Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The final MSE is `0.00015821003618271694` — an extremely low value. However,
    if we were to graph the MSE every epoch (or iteration), we would get the following
    graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/86371f300cd05143671fbe67d2ccd790.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'These seem to be some very, very diminishing returns. In fact, after epoch
    25 or so, it seems as if the MSE does not change at all! Let’s look at this graph
    from a different perspective, omitting the first 50 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/33b58c297ebf565bc61907361eef184d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'What looks like a straight line is not — the MSE gets almost 100 times smaller
    from epoch 50 to epoch 1000\. You might be asking yourself — isn''t an MSE of
    ~0.015 already low enough? Let’s try running the gradient descent again, but this
    time with only 50 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Close, but not close enough:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/07e30fabf4a7e0bdda7dcce84cbd9a04.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the contrary, let’s run the gradient descent with 100,000 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/329f8664397e8e2600f47dbdc6dc5032.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Perfect! It seems that running the model with 100,000 epochs gives us a practically
    perfect result. While running more epochs with this linear regression does give
    us more accuracy with this model, it is important to consider the balance between
    accuracy and time. In general, you should aim to use enough epochs to fit the
    model’s data, but not so many that the model takes an unnecessary amount of time
    to train. A technique called early stopped is commonly used in models of all types,
    where a model is automatically stopped once it reaches a certain accuracy. This
    allows the model to be trained as fast as possible, but still ensures some level
    of accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Applying Linear Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Last, but certainly not least, it is time to apply our linear regression knowledge
    to some real data!
  prefs: []
  type: TYPE_NORMAL
- en: Finding a Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start with finding a dataset. [Kaggle](https://www.kaggle.com/) is a great
    resource to find free datasets that are high quality and varied in terms of their
    structure or topic. For this mini-project, I chose to use the [Data Analysis on
    Diamonds Dataset](https://www.kaggle.com/datasets/swatikhedekar/price-prediction-of-diamond),
    in order to develop a linear relation between the carat of a diamond (independent
    variable) and its price (dependent variable).
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In general, when choosing or building a dataset to run linear regression upon,
    it is important to consider the following factors:'
  prefs: []
  type: TYPE_NORMAL
- en: A strong linear correlation between the independent and dependent variables
    — if the variables seem to be not correlated at all or their correlation is non-linear,
    it might be better to choose a different regression method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Outliers — a good data set should be relatively free of outliers, as they can
    heavily influence the performance of the regression.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Suitability — the dataset must be relevant to the problem you are trying to
    solve. For instance, if you want to predict the price of a house in New York City
    based on its square feet, training the model on data from farms in the Montana
    countryside would not be appropriate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Downloading the Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Downloading a dataset is quite intuitive in Kaggle. Press the black download
    button in the top right corner of the screen, and save the `.zip` file to your
    computer. You can then unzip the file and move the `.csv`file within it to the
    same directory as your Jupyter Notebook file.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, all that’s left to do is use the data in the linear regression model.
    Again, in this instance, we will be predicting a diamond’s price based on its
    carat. Comment out the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Replace them with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the program (for 1000 epochs), the outputs are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The final MSE is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Woah! That is an extremely large number. Admittedly, the diamond dataset we
    used is flawed. The linear regression on a graph is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7314bd92726ee179a7025e403ccb9180.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Take, for example, the varying prices of a 1-carat diamond. It can range from
    ~$1000 to almost $20,000! This is a prime example of this dataset not having enough
    of a linear relationship between *two* variables. In this case, the cut, color,
    and clarity of the diamonds all heavily contribute to the price as well. It is
    also important to consider that with real-world data, an MSE of 0 is practically
    impossible. Real-world phenomena are influenced by a plethora of factors, and
    capturing all of them in a regression model is practically impossible. It is left
    as an exercise to the reader to explore more datasets on Kaggle where the linear
    correlation between two given variables is stronger.
  prefs: []
  type: TYPE_NORMAL
- en: Testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s put our model to the test. According to [CreditDonkey](https://www.creditdonkey.com/1-carat-diamond.html),
    the best value for a 1-carat diamond is anywhere from $4500—$6000\. Using the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The model outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Success!
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s recap — linear regression is a statistical method used to comprehend the
    relationship between two variables that are correlated linearly. This is done
    by fitting a line in the form `y=mx+b`to the provided independent and dependent
    variables. The most fitted line can be found by using a method known as least
    squares, which minimizes the sum of the squared deviation from each point to its
    corresponding point on the line. Least squares can be implemented using both matrix
    operations and gradient descent, with this article focusing on the utilization
    of gradient descent. The MSE cost function (i.e the mean of squared errors) was
    used in order to determine the accuracy of the model. By minimizing the MSE, we
    can optimize the model and improve its accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'I leave you with a satisfying GIF of the model slowly converging on the most
    fitted line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/38ec45e07fffd1c0c6cb2241eaca87b5.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading!
  prefs: []
  type: TYPE_NORMAL
