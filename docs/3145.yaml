- en: The Current State of Continual Learning in AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-current-state-of-continual-learning-in-ai-af4a05c42f3c?source=collection_archive---------1-----------------------#2023-10-18](https://towardsdatascience.com/the-current-state-of-continual-learning-in-ai-af4a05c42f3c?source=collection_archive---------1-----------------------#2023-10-18)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why is ChatGPT only trained up until 2021?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@jon.flynn2?source=post_page-----af4a05c42f3c--------------------------------)[![Jon
    Flynn](../Images/492cef280f4ea0b002e5d00ad2e083a5.png)](https://medium.com/@jon.flynn2?source=post_page-----af4a05c42f3c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----af4a05c42f3c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----af4a05c42f3c--------------------------------)
    [Jon Flynn](https://medium.com/@jon.flynn2?source=post_page-----af4a05c42f3c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa3ee742fae3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-current-state-of-continual-learning-in-ai-af4a05c42f3c&user=Jon+Flynn&userId=a3ee742fae3&source=post_page-a3ee742fae3----af4a05c42f3c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----af4a05c42f3c--------------------------------)
    ·23 min read·Oct 18, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Faf4a05c42f3c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-current-state-of-continual-learning-in-ai-af4a05c42f3c&user=Jon+Flynn&userId=a3ee742fae3&source=-----af4a05c42f3c---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faf4a05c42f3c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-current-state-of-continual-learning-in-ai-af4a05c42f3c&source=-----af4a05c42f3c---------------------bookmark_footer-----------)![](../Images/295e18f233f38577ab964460240114a5.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image generated by author using DALL-E 3
  prefs: []
  type: TYPE_NORMAL
- en: '**Knowledge prerequisites:**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A couple of years ago, I learned the basics of deep learning through StatQuest
    videos, Lena Voita’s NLP blogs, and books like “Deep Learning for Coders” and
    “Talking Nets.” I’m now wanting to understand the current state of continual learning
    in deep learning. I found that there is not much information available that summarises
    this topic in simpler terms, and it requires sifting through expert research papers.
    Therefore, this article is intended for readers who have a basic understanding
    of the topic but find the research difficult to read and may not be experts. It
    holds a focus on chatbots, so knowing the training stages of chatGPT is also helpful.
  prefs: []
  type: TYPE_NORMAL
- en: '**Intro**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/c1b8d3a88f40aa06733f43b29cfe4f48.png)'
  prefs: []
  type: TYPE_IMG
- en: ChatGPT telling the user it is only trained up until September 2021 (screenshot
    by author)
  prefs: []
  type: TYPE_NORMAL
- en: If large language models like ChatGPT could be continuously updated with new
    data, they would accelerate a wide range of tasks, from software development to
    legal processes to learning. It would also make articles like this one obsolete.
  prefs: []
  type: TYPE_NORMAL
- en: Continual learning is the ability to pause the model training process, save
    the model’s current state, and then later resume training on new data. The model
    should be able to generalise well to new data, while still maintaining its ability
    to generalise to old data. Refer to [this paper](https://arxiv.org/pdf/2302.00487.pdf)
    for a more formal definition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Presently, the trend in the industry to augment chatbots with more data is
    to use RAG, combining queried vectors with prompt engineering to answer questions,
    rather than continuing to train the LLM with new data. ChatGPT’s zero-shot learning
    capability, which allows it to answer questions about new, unseen data, makes
    this approach very appealing. For instance, you could teach it a new programming
    language and then ask it questions about that language, with just a few prompts,
    although performance does degrade a bit proportionally to the amount of tokens
    input. Continually training the model to answer questions based on a new topic
    like this requires significant computing resources and more importantly, a wide
    variety of data on the relevant topic. Furthermore, if a topic has very low prevalence
    in the training set, it will generalise poorly to it. E.g.: take an unpopular
    public repo and it will know little about it and may hallucinate, despite having
    seen it at some point during the training process. Context windows (the amount
    of tokens the model can take as input) are getting increasingly larger very quickly,
    making RAG even more attractive. Ideally though, do we not want one intelligent
    all-knowing model, without the need for any external database?'
  prefs: []
  type: TYPE_NORMAL
- en: Continual learning is an essential step towards AGI, and some doubt we will
    even be able to achieve it without significant changes in deep learning network
    architectures. Jeff Hawkins in his book, [“A Thousand Brains”](https://www.numenta.com/resources/books/a-thousand-brains-by-jeff-hawkins/),
    stated he does not think current ANN’s are capable of effective continual learning,
    and believes future models will probably need to be architected more similarly
    to the human brain using his theory on reference frames in the cortical columns
    of the neocortex.
  prefs: []
  type: TYPE_NORMAL
- en: Continual Learning in the pre-training vs fine-tuning stages of language models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Earlier this year, a research paper called [“LIMA: Less Is More for Alignment”](https://arxiv.org/abs/2305.11206)
    was published. It introduced a chatbot that was not trained using Reinforcement
    Learning from Human Feedback (RLHF), but was instead fine-tuned on just 1,000
    carefully annotated question-and-answer samples. Surprisingly, the researchers
    said that in 43% of cases, “the chatbot’s responses were on par with those of
    GPT-4”. I did not take an in-depth look at how these were evaluated, but nonetheless,
    it’s widely acknowledged that a substantial amount of the model’s knowledge and
    capability is acquired during the pre-training phase, and research like this further
    proves this.'
  prefs: []
  type: TYPE_NORMAL
- en: Models like ChatGPT and Llama-chat have undergone extensive fine-tuning to generate
    more aligned and effective responses. OpenAI currently offer an API to further
    [fine-tune a model](https://platform.openai.com/docs/guides/fine-tuning), which
    takes Q&A data as input to be used for further training. However, this should
    not be used to teach the model new data, but rather to [customise the tone and
    steerability](https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates).
    Fine-tuning a model in attempt to teach it new data can cause *catastrophic forgetting*,
    a problem where the model forgets what it has already learned. This article will
    go over some techniques that aim to mitigate this problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'This also leads us to a couple key questions about the feasibility and strategy
    of continual learning:'
  prefs: []
  type: TYPE_NORMAL
- en: At which stage of development is it most beneficial and easiest to introduce
    continual learning?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given that both fine-tuning and RLHF alter the entire model’s parameters, is
    it even possible to revert to the pre-training stage for further modification?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Note: I provide some PyTorch-like pseudocode for some of the papers discussed
    below. It has not been tested and may not work, it’s used to break the techniques
    down step by step and translate any confusing math notation to help the reader
    understand.*'
  prefs: []
  type: TYPE_NORMAL
- en: The 5 sub-categories of continual learning techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The [comprehensive overview of continual learning paper](https://arxiv.org/pdf/2302.00487.pdf)
    states training strategies for continual learning can be divided into 5 sub categories:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Regularisation-based approach: this approach adds constraints or penalties
    to the learning process during the training process.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Optimisation-based approach: this technique focuses on modifying the optimisation
    algorithm.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Representation-based approach: this aims to learn a shared feature representation
    across different tasks, helping the model generalise better to new but related
    tasks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Replay-based approach: this involves storing some data or learned features
    from previous tasks and replaying them during training on new tasks to maintain
    performance on earlier learned tasks. In other words, mixing both the old and
    new datasets when training on new tasks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Architecture-based approach: in this approach, the network architecture is
    dynamically adjusted, often by growing or partitioning, delegating different parts
    of the network to different tasks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1\. Regularisation-based approaches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Soft Masking of Parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following soft-masking techniques mask and adjust the gradients of each
    parameter during the training process. The *optimisation-based approaches* coming
    up also manipulate the gradients for continual learning. Remember the gradients
    aren’t just temporary numbers that appear and disappear during training; they’re
    signals that guide the evolution of the weights.
  prefs: []
  type: TYPE_NORMAL
- en: SPG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This [paper](https://arxiv.org/pdf/2306.14775.pdf) proposes a technique named
    SPG (Soft-masking of Parameter-level Gradient flow) which aims to:'
  prefs: []
  type: TYPE_NORMAL
- en: Train the model on each task until convergence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After training, calculate the “importance” of each parameter for the task.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Soft-mask parameters based on their accumulated importance, making important
    parameters less likely to change during the learning of new tasks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s break the approach down step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Training the First Task
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Train the model on the first task’s dataset as normal.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Calculate Parameter Importance for the First Task
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After the training of the first task is complete, we calculate the importance
    of each model parameter. The intuition here is simple, we use the gradients of
    each parameter to compute its importance. A larger gradient implies that a small
    change in that parameter will result in a larger change in the loss, meaning the
    model’s performance could vary more significantly, hence that parameter is important.
  prefs: []
  type: TYPE_NORMAL
- en: The gradients are also normalised, because gradients in the first layer could
    be small, while those in the last layer could be large. If you’re calculating
    importance based on these raw gradient values, parameters in the last layer would
    seem more important because of the scale of their gradients, not necessarily because
    they are genuinely more crucial for the task.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/315a4134afb5961eac6c44a9e83f2760.png)'
  prefs: []
  type: TYPE_IMG
- en: Equations for calculating the importance of the model parameters in SPG (section
    3.1 of [paper](https://arxiv.org/pdf/2306.14775.pdf))
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s translate this calculation to PyTorch-like pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 3\. Accumulating Importance Across Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The accumulated importance of each parameter across task is simply calculated
    by taking the max value at any stage.
  prefs: []
  type: TYPE_NORMAL
- en: '4\. Training Subsequent Tasks, combined loss and the soft-masking mechanism:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When training on new tasks, the researchers use a combined loss function consisting
    of two parts. One is the standard loss function which is used as normal on the
    new task and data, and the second is an additional loss function which involves
    putting the *new* data through the *old* model (the converged model checkpoint
    after the previous task) and summing up the logits produced. In classification
    networks the logits are usually the raw non normalised predictions generated by
    the model in one of the last layers before going through something like a softmax
    function. This sum of logits serves as a form of loss. The rationale is that if
    the summed logits are significantly affected when the model parameters change,
    those parameters are crucial for the performance of the previously learned task.
  prefs: []
  type: TYPE_NORMAL
- en: The gradients generated from this additional loss serve as a guide during backpropagation,
    nudging the shared parameters to change in a direction that is less likely to
    harm performance on the first task. It therefore acts as a sort of penalty term
    to enforce that any updates made to the model do not lead to a significant loss
    of information related to previous tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Train the model on the next task. Use a standard training loop, but modify
    the gradients during backpropagation based on their accumulated importance. This
    is the soft-masking mechanism:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 5\. Soft-Masking Special Cases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Feature Extractor: Gradients of parameters in the shared feature extractor
    are modified based on their specific accumulated importance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Classification Head: For the classification head, gradients are modified based
    on the average importance of the feature extractor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying this to LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bear in mind, this paper does not experiment this with a language model, but
    I assume in a language model you could think of the transformer layers as analogous
    to the “feature extractor,” and the final classification layer (which predicts
    the next word or token in the sequence) as the “classification head.”
  prefs: []
  type: TYPE_NORMAL
- en: Soft-masking applied to continual pre-training in a language model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next we’ll go into a paper which applies similar soft-masking to the pre-training
    stage in language modelling.
  prefs: []
  type: TYPE_NORMAL
- en: '[This paper](https://arxiv.org/pdf/2302.03241.pdf) introduces a technique called
    DAS (Continual DA-pre-training of LMs with Soft-masking) for continual learning
    in the pre-training stage of a large language model. It applies a soft-masking
    technique similar to the one just discussed along with a couple other techniques
    in attempt to continue pre-training of an LLM without running into catastrophic
    forgetting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s break it down step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: Initial Pre-training Phase
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pre-train the LLM like normal.
  prefs: []
  type: TYPE_NORMAL
- en: Further Pre-training on A New Domain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Prepare New Domain Data:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A new dataset from a different domain is prepared.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the importance of each neuron
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SPG used gradients to determine the importance of each parameter, and then applied
    the calculated importance value to mask the gradient adjustments of parameters
    during training. This paper tries to determine the importance of each unit/neuron,
    rather than parameter, and then uses this in the same way by masking the gradient
    during training.
  prefs: []
  type: TYPE_NORMAL
- en: This paper uses two different methods to calculate the importance of neurons,
    depending on the task at hand. One, a gradient-based importance detection method
    (originally outlined in [this paper](https://arxiv.org/pdf/1905.10650.pdf)), and
    two, a custom “proxy loss function”.
  prefs: []
  type: TYPE_NORMAL
- en: The first introduced is *not* used in the continual learning of the *first*
    new domain. Why? It needs data from the training dataset to work and the authors
    state that users “don’t have access to the massive original pre-training dataset”,
    which is a fair assumption. The proxy loss function is used instead for the first
    phase of continual learning and then for each subsequent phase the other method
    is used.
  prefs: []
  type: TYPE_NORMAL
- en: '**The proxy loss function (“Proxy KL-divergence loss”):**'
  prefs: []
  type: TYPE_NORMAL
- en: 'I found this term confusing at first, but it’s called this because the original
    gradient-based importance detection method is defined as a loss function itself,
    which you can then use to run the network’s outputs through to get the gradients
    of each neuron, which can then be used to derive importance, just like the SPG
    technique. It’s calculated by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Take a subset of the new domain we’re wanting to train on and feed it twice
    through the model to get two different representations. These representations
    will differ a bit due to the existing dropout masks in the Transformer architecture.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the KL-divergence between these two representations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modified Backpropagation Flow with Proxy and Combined Loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Forward Pass:** Data goes through a forward pass in the neural network.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Backpropagation:**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Apply Proxy Loss for Gradient Adjustment:** The proxy loss function’s unit-level
    importance is used to soft-mask the original gradients. This is expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Calculate Combined Loss (MLM + Contrastive Loss):** Compute the combined
    loss using both MLM and contrastive loss.'
  prefs: []
  type: TYPE_NORMAL
- en: Further Pre-training on More Domains
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Direct Importance Calculation:** For each new domain, the importance of each
    unit can now be directly calculated using the data from the new domain via the
    gradient-based method outlined in equation 3, eliminating the need for the proxy
    loss function which is only once used after the initial pre-training.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**The importance of neurons is updated incrementally as each new task is learned.**
    This update is done using element-wise max. “Element-wise maximum (EMax) operation”
    refers to comparing two vectors element by element, and taking the maximum value
    for each corresponding element to create a new vector. E.g.: if you have two vectors
    A and B of the same length, the element-wise maximum will result in a new vector
    C where each element *C*[*i*] is the maximum between *A*[*i*] and *B*[*i*].'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2\. Optimisation-based approaches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ll refer to the two techniques outlined in the [comprehensive survey paper](https://arxiv.org/pdf/2302.00487.pdf)
    in section 3.1
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Direction Preservation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The paper talks about manipulating the gradient-based optimisation process to
    make the gradient *directions* of new training samples close to those from old
    training samples. The formula
  prefs: []
  type: TYPE_NORMAL
- en: ⟨ ∇θ Lₖ(θ; Dₖ), ∇θ Lₖ(θ; Mₜ) ⟩ ≥ 0
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: enforces that learning the new task should not increase the loss for the old
    tasks. Essentially, the gradients of the new task and the old tasks are encouraged
    to align.
  prefs: []
  type: TYPE_NORMAL
- en: Breaking down the formula, we take the dot product of the gradient of the loss
    from the new task (∇θ Lₖ(θ; Dₖ)) and the gradient of the loss from the old task
    (∇θ Lₖ(θ; Mₜ)) should be non-negative. In this context, a positive dot product
    implies that the gradients for the old task and the new task are generally pointing
    in the same direction, with the angle between these two vectors is less than or
    equal to 90 degrees.
  prefs: []
  type: TYPE_NORMAL
- en: 'Forward/Backward Passes:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Forward Pass:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You would run your input data *Dₖ* for the new task and *Mₜ*​ for the old task
    through the same model to calculate the loss for each.
  prefs: []
  type: TYPE_NORMAL
- en: 'Backward Pass:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Compute the gradients of the loss with respect to the network parameters for
    both the old and new task.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Alignment Check: Compute the dot product of the two gradients. You’d then use
    this information to modify the gradients for the new task in such a way that the
    dot product is non-negative.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update Weights: Update the model parameters using these “aligned” gradients.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Gradient Direction Preservation without needing old training samples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The text also highlights that gradient projection can be performed even without
    storing old samples. NCL (Natural continual learning, [paper link](https://proceedings.neurips.cc/paper/2021/hash/ec5aa0b7846082a2415f0902f0da88f2-Abstract.html))
    is the technique summarised here. Note, this can be categorised as both a regularisation
    and optimisation based approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'Training process step by step:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Forward Pass:**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You would run your new data through the network and calculate the loss as usual.
  prefs: []
  type: TYPE_NORMAL
- en: '**Backward Pass:**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Objective:** The aim is to minimise the task-specific loss *ℓk(θ)* while
    adhering to a distance constraint *d*(*θ*,*θ*+*δ*)≤*r.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Algorithm step by step**:'
  prefs: []
  type: TYPE_NORMAL
- en: As normal, compute the gradient of the loss with respect to the model parameters
    ∇*θ*​ℓ*k*​(*θ*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The *δ* is calculated using the update rule. This gives you the “suggested”
    changes to the model parameters *θ* based on the new task’s requirements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, you plug this *δ* into the distance constraint formula: *d(θ,θ+δ)=squareroot(δ⊤Λ_k-1​δ)*​.
    The constraint acts like a boundary around the current parameters *θ*, defined
    by the distance metric *d*(*θ*,*θ*+*δ*) and the radius *r*. I struggled to see
    why they called it a “radius”, and not just “constraint number” or something.
    I think it’s because the researchers are visualising the gradients and training
    process in a high-dimensional space. When you apply a constraint based on the
    distance metric, you’re essentially defining a “sphere” around your current parameter
    values in that high-dimensional space. The “radius” *r* of this sphere sets a
    limit on how much the parameter can move while learning a new task.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the proposed *δ* would move *θ* too far according to this distance metric,
    i.e., beyond this boundary, you scale it down so that it stays within the allowable
    region defined by the radius *r*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s look at each bit more in-depth:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Update Rule:** The update rule provides a direction in which *θ* should move.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/54949a33247d881c077379249243497d.png)'
  prefs: []
  type: TYPE_IMG
- en: NCL update rule from section 3.1 in the [comprehensive overview of continual
    learning paper](https://arxiv.org/pdf/2302.00487.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: 'Breaking it down:'
  prefs: []
  type: TYPE_NORMAL
- en: '*∇θ ℓk(θ)* represents the gradients for all parameters (*θ)* calculated by
    the loss function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Parameter importance calculation (*Λ^(k-1)_(-1)*): This term represents a *precision
    matrix* and it is yet another way to calculate the importance of parameters in
    the network. *more details below*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Regularisation Term (*θ — μ_(k-1)*): This term pulls the updated parameters
    closer to the optimal parameters *μ_(k-1)*​ from the previous task. Like the before
    techniques, it acts as a regulariser to avoid deviation from what was already
    learned.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning Rate (*λ*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distance Constraint:** Before applying this update, you’d usually check whether
    this change *δ* would violate the distance constraint *d*(*θ*,*θ*+*δ*)≤*r*. If
    it does, you’d typically scale down *δ* so that it satisfies the constraint.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Precision matrix explanation:** before in the soft-masking methods we saw
    the calculation of importance via the output of all neurons or their gradients.
    In this method a precision matrix is used. This is a bit more complex so I’ll
    attempt to explain it:'
  prefs: []
  type: TYPE_NORMAL
- en: We first calculate the *covariance matrix* for the networks parameters. In the
    context of neural networks, the columns in the gradient matrix *G* correspond
    to the parameters (weights and biases) of the model. Each row in *G* represents
    the gradient vector for a single training example, with respect to all of those
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: So, if you have a neural network with *P* parameters (this includes all the
    weights and biases from all layers), then each gradient vector will have *P* elements,
    one for each parameter. Therefore, *G* will be a matrix of shape *N* × *P*, *N*
    representing each batch and therefore each row representing the average gradient
    vector across all the training examples in a given batch.
  prefs: []
  type: TYPE_NORMAL
- en: When you calculate the covariance matrix Σ from *G*, the resulting matrix will
    have dimensions *P* × *P*. The diagonal entries Σ*ii*​ will indicate the variance
    of the gradient with respect to the *ith* parameter, and the off-diagonal entries
    Σ*ij*​ will indicate the covariance between the gradients with respect to the
    *ith* and *jth* parameters. This gives you an idea of how these parameters interact
    or co-vary during the training process. The inverse of this matrix is the *precision
    matrix*, which is what we use to determine importance.
  prefs: []
  type: TYPE_NORMAL
- en: Why the *precision matrix* over the *covariance matrix*? While the covariance
    matrix Σ does capture how parameters interact with each other during training,
    it doesn’t specifically indicate how crucial each parameter is to the task at
    hand when all other parameters are considered. In contrast, the precision matrix
    allows us to assess the *conditional independence* (this is a concept in probability
    theory, look it up) of parameters. Large values in the precision matrix indicate
    that knowing one parameter is highly informative about another, given all the
    other parameters. I’m not going to go into examples of how this works so get ChatGPT
    to generate some examples using a very small neural network to see how the values
    can be interpreted.
  prefs: []
  type: TYPE_NORMAL
- en: Previous methods we saw that calculate importance focus on individual neurons
    or parameters, ignoring the relationships between them. The precision matrix,
    on the other hand, can capture these relationships. Like everything in deep learning,
    whether this is a better way to calculate the importance of a network, is going
    to be empirical and could differ depending on the task and scale of the network.
  prefs: []
  type: TYPE_NORMAL
- en: '**Algorithm step by step in PyTorch:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 3\. Representation-based approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Firstly, it’s important to note that the pre-training of LLM’s to be further
    fine-tuned on a downstream task is an example of continual learning in this sub-category.
    I think ChatGPT’s ability to reason about never-before-seen data is also an example
    of this approach. Although we technically call it zero-shot learning, and the
    term “continual learning” requires updating model parameters, it goes beyond anything
    we’ve seen before. As discussed in the introduction, prompt engineering could
    be the future of continual learning, instead of continually updating the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Below we’ll take a look at using knowledge distillation for continual learning.
    I’m not really sure which sub-category this falls under, but I’d guess it’s probably
    a mix between representation, architecture and replay approaches. Even though
    some of the techniques we’re reviewing may seem random and unproven at large scale,
    breakthroughs in this field are often unpredictable. Therefore, it’s important
    to maintain a broad perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge Distillation for continual learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can transfer (or “distill”) the knowledge of one network into another network,
    and the second network does a reasonable job of approximating the function learned
    by the original network.
  prefs: []
  type: TYPE_NORMAL
- en: 'The distilled model (the *student*), is trained to mimic the output of the
    larger network (the *teacher*), instead of training it on the raw data directly.
    For example, say you want to train a smaller student model to mimic a large pre-trained
    language model (the teacher). Run the original pre-training dataset through the
    teacher model to generate “soft targets.” These are probability distributions
    over potential outputs, i.e.: next-word predictions. For instance, for a next-word
    prediction task, instead of predicting “cat,” the teacher might provide probabilities
    like 90% for “cat”, 5% for “kitten”, 3% for “feline”, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: This is usually done to transfer knowledge to much smaller models, and it yields
    great results despite the smaller model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how some researchers [applied this with success](https://ojs.aaai.org/index.php/AAAI/article/view/17600)
    to a NER (named entity recognition) model. The training process is fairly straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: Training process step by step
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two primary methods outlined in the paper: AddNER and ExtendNER.'
  prefs: []
  type: TYPE_NORMAL
- en: AddNER Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Note, NER models work by taking a sequence of tokens (usually a sentence) as
    input and then output a probability distribution (for the different types of entities)
    for each token. IOB tagging is commonly used for NER models, each token can be
    labeled as ‘O’, or as the beginning (‘B-’) or inside (‘I-’) of an entity of type
    *X*. ‘O’ stands for ‘Outside’, it just means the current token doesn’t belong
    to any entity. Therefore, for *n* entity types, you will have 2*n* output neurons
    in the classification layer: *n* for the ‘B-’ tags (one for each entity type)
    and *n* for the ‘I-’ tags (again, one for each entity type). Add to this the ‘O’
    label, which signifies that a token doesn’t belong to any entity, and you end
    up with 2*n* + 1 possible labels for each token. The final dimensions can be written
    as *h* × (2*n* + 1), where *h* is the size of the hidden layer’s output. Bear
    in mind, this is only for models where tokens can only be one entity. E.g.: “Apple”
    could be tagged as both “FOOD” and “COMPANY”.'
  prefs: []
  type: TYPE_NORMAL
- en: Architecture and teacher-student setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The student model in this case is a copy of the teacher model, with an additional
    output classification layer for each new entity type that the model should learn.
    During training, the new output layer learns from the new annotated data, and
    the older layers are guided by the teacher model’s outputs to minimise forgetting.
  prefs: []
  type: TYPE_NORMAL
- en: After training, the old output layers are not discarded. It then uses the algorithm
    and heuristics described in the conflict resolver section *(end of section 3.3)*
    to combine these outputs into a single, final prediction for each token in the
    sequence.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/31710e332c9941fc063c2fed5783abe9.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram of the AddNER model from section 3.2 of the [paper](https://ojs.aaai.org/index.php/AAAI/article/view/17600)
  prefs: []
  type: TYPE_NORMAL
- en: Forward Pass
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Old Entity Types: The input sentence is passed through the teacher model to
    obtain probability distributions (the “soft targets” in this context) for the
    old entity types.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'New Entity Types: The same sentence is also passed through the new student
    model with additional output layers specific to the new entity types​.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Backward Pass
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Combined loss function:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'KD Loss: calculated by comparing how closely the output probabilities of the
    old entity types from the new model (student) match those from the old model (teacher).
    It uses KL-divergence to calculate this. It’s probably calculated token-by-token
    and then summed or averaged over all tokens in a sentence or batch, but I don’t
    think the paper goes into this.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Cross-Entropy Loss: This is the usual loss function that compares the model’s
    predictions for the new entity types against the actual labels from the new dataset.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Combining the two: these two losses are combined into a combined loss by taking
    a weighted sum of them both. The weights for combining these losses are set by
    the hyperparameters alpha and beta, which are adjusted like any other hyperparameter
    to better performance based on experiments.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: ExtendNER Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Architecture and teacher-student setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The ExtendNER model extends the output layer dimensions to accommodate new
    entity types, instead of adding new output layers. The paper explains quite simply
    how the dimensions are to be:'
  prefs: []
  type: TYPE_NORMAL
- en: “Assuming that *Mi* was able to recognize *n* entity types, its final layer
    can be considered as a matrix with dimension *h×(2n+1)*. The output layer of *Mi+1*
    will then be extended to be a matrix with dimension *h × (2n + 2m + 1)* in order
    to accommodate the new entity types.”
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e04816628dc4e50a569db1c30a59c8d1.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram of the ExtendNER model from section 3.4 of the [paper](https://ojs.aaai.org/index.php/AAAI/article/view/17600)
  prefs: []
  type: TYPE_NORMAL
- en: Forward Pass
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Same as in AddNER, but with extended dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Backward Pass
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The loss calculation uses *either* the KL-divergence loss or the cross-entropy
    loss, depending on the following:'
  prefs: []
  type: TYPE_NORMAL
- en: When the NER category label *y* is “O” (from the IOB tagging schema), the KL
    divergence loss is used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the category label *y* is NOT “O”, the Cross-Entropy loss is used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Final Prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Viterbi algorithm is applied to decode the final entity types.
  prefs: []
  type: TYPE_NORMAL
- en: Both AddNER and ExtendNER models performed well for continual learning and the
    results did not differ between them much
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Replay-based approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “Fine-tuned language models are continual learners”
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[paper link](https://arxiv.org/pdf/2205.12393.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: The model in the paper is not a generic, single-task model like GPT trained
    just for conversational response. Instead, it’s fine-tuned for a sequence of specialised
    tasks, ranging from text simplification to Haiku generation. Each of these tasks
    has unique requirements, evaluation metrics, and specialised training datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The researchers mix parts of the old dataset with the new dataset, and achieve
    great results by mixing in just 1% of the previous task’s dataset when fine-tuning
    on a new task. This is done sequentially for many tasks (8). The model also performs
    well in zero-shot learning settings, meaning it can generalise well to tasks it
    hasn’t been trained on. For instance, it can generate a Haiku with the correct
    syllable count when given an unseen topic, showing its ability to generalise.
    The researchers also mention that their approach is task-order invariant, meaning
    the sequence in which tasks are learned does not affect the model’s performance.
    The experiments find that the amount of the old dataset mixed in with the new
    one doesn’t significantly affect the main task’s performance. However, it does
    affect the zero-shot learning. At 0% rehearsal, the model tends to forget the
    zero-shot tasks, while at 1% rehearsal, the model maintains its performance in
    those tasks very well.
  prefs: []
  type: TYPE_NORMAL
- en: This all seems positive, the fact we can just add 1% of the old dataset and
    continual learning is solved, but of course, applying it to a chatbot like chatGPT,
    will be empirical and can be completely different. Even if, hypothetically, chatGPT
    could be continually trained in the fine-tuning and RLHF stages like this, it
    would require an immense amount of labeled conversation data.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Architecture-based approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I won’t go into any specific paper or implementation in detail here, but I will
    provide a brief overview of this approach and a couple different techniques. I
    recommend reading this section (4.5) of the [comprehensive survey paper](https://arxiv.org/pdf/2302.00487.pdf).
    It is also easier to read than the other sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameter Allocation: Here, a subset of the network parameters is dedicated
    to each task. This can be done either by masking out irrelevant neurons or by
    explicitly identifying important ones for the current task.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Modular Network: This involves using separate sub-networks or modules for each
    task.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sub-networks can be connected in various ways to form an ensemble or a more
    complex architecture. Below are a few common methods for connecting sub-networks:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Concatenation of Outputs:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this approach, the outputs of multiple sub-networks are concatenated into
    a single tensor, which can then be passed through additional layers to produce
    the final output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Voting Mechanism:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In some models, each sub-network casts a “vote” on the likely outcome, and the
    final decision is made by taking the majority vote or a weighted vote. This has
    biological inspiration as it’s similar to how different cortical columns in the
    neocortex cast votes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Skip Connections:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some architectures allow sub-networks to have skip connections to other parts
    of the model, allowing information to flow across modules.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sequential:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this case, the output of one sub-network serves as the input to the next.
  prefs: []
  type: TYPE_NORMAL
- en: Going back to talking about chatbots, what I find particularly interesting if
    it were possible to create such an architecture with two sub-networks. The first
    one is the pre-trained model which holds the general “knowledge”. The second holds
    knowledge for aligning the model. Once the model is aligned, it would no longer
    need labeled conversational data. Instead, it could be continually updated by
    training the pre-trained subnetwork in an unsupervised way.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In conclusion the subfield of continual learning in deep learning is challenging
    and mostly unknown. This is because we do not fully understand how the neurons
    in LLMs work, and as outlined in the intro, could also be that current network
    architectures, or deep learning in general, is just not suited for it.
  prefs: []
  type: TYPE_NORMAL
- en: I noticed last month that ChatGPT (GPT-4 only) had [been updated](https://stackdiary.com/chatgpts-cutoff-date-upgraded-to-january-2022/)
    as it now says “Since my training cutoff in January 2022”, so I wonder what the
    folks at OpenAI did to achieve this.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1746b4e381fc5d968f5a8d60d20a46a6.png)'
  prefs: []
  type: TYPE_IMG
- en: ChatGPT (GPT-4 variant) telling the user it is trained up until January 2022
    (screenshot by author)
  prefs: []
  type: TYPE_NORMAL
