- en: 'Towards Green AI: How to Make Deep Learning Models More Efficient in Production'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/towards-green-ai-how-to-make-deep-learning-models-more-efficient-in-production-3b1e7430a14?source=collection_archive---------6-----------------------#2023-08-08](https://towardsdatascience.com/towards-green-ai-how-to-make-deep-learning-models-more-efficient-in-production-3b1e7430a14?source=collection_archive---------6-----------------------#2023-08-08)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[The Kaggle Blueprints](/the-kaggle-blueprints-unlocking-winning-approaches-to-data-science-competitions-24d7416ef5fd)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'From Academia to Industry: Finding the best trade-off between predictive performance
    and inference runtime for sustainability in Machine Learning practices'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@iamleonie?source=post_page-----3b1e7430a14--------------------------------)[![Leonie
    Monigatti](../Images/4044b1685ada53a30160b03dc78f9626.png)](https://medium.com/@iamleonie?source=post_page-----3b1e7430a14--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3b1e7430a14--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3b1e7430a14--------------------------------)
    [Leonie Monigatti](https://medium.com/@iamleonie?source=post_page-----3b1e7430a14--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3a38da70d8dc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftowards-green-ai-how-to-make-deep-learning-models-more-efficient-in-production-3b1e7430a14&user=Leonie+Monigatti&userId=3a38da70d8dc&source=post_page-3a38da70d8dc----3b1e7430a14---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3b1e7430a14--------------------------------)
    ·14 min read·Aug 8, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3b1e7430a14&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftowards-green-ai-how-to-make-deep-learning-models-more-efficient-in-production-3b1e7430a14&user=Leonie+Monigatti&userId=3a38da70d8dc&source=-----3b1e7430a14---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3b1e7430a14&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftowards-green-ai-how-to-make-deep-learning-models-more-efficient-in-production-3b1e7430a14&source=-----3b1e7430a14---------------------bookmark_footer-----------)![](../Images/2c688f3626d020d10afe04975cc8d466.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Making s’mEARTHs at the GPU bonfire (Image hand-drawn by the author)
  prefs: []
  type: TYPE_NORMAL
- en: '*This article was* [*originally published on Kaggle*](https://www.kaggle.com/code/iamleonie/towards-green-ai)
    *as an entry to the* [*“2023 Kaggle AI Report” competition*](https://www.kaggle.com/competitions/2023-kaggle-ai-report)
    *on July 5th, 2023, in which* [*it won 1st place in the category “Kaggle competitions”*](https://www.kaggle.com/competitions/2023-kaggle-ai-report/discussion/429989)*.
    As it reviews Kaggle competition writeups, it is a special edition of “*[*The
    Kaggle Blueprints*](/the-kaggle-blueprints-unlocking-winning-approaches-to-data-science-competitions-24d7416ef5fd)*”
    series.*'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “I think we’re at the end of the era where it’s going to be these, like, giant,
    giant models. […] We’ll make them better in other ways.”, [said Sam Altman](https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/),
    CEO of OpenAI, shortly after their release of GPT-4\. This statement surprised
    many, as GPT-4 is estimated to be ten times larger ([1.76 trillion parameters](https://wandb.ai/byyoung3/ml-news/reports/AI-Expert-Speculates-on-GPT-4-Architecture---Vmlldzo0NzA0Nzg4))
    than its predecessor, GPT-3 ([175 billion parameters](https://arxiv.org/abs/2005.14165)).
  prefs: []
  type: TYPE_NORMAL
- en: “I think we’re at the end of the era where it’s going to be these, like, giant,
    giant models. […] We’ll make them better in other ways.” — [Sam Altman](https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
