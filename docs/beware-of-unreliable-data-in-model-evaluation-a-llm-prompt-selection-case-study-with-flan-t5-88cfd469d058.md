# 注意在模型评估中数据的不可靠性：一个关于Flan-T5的LLM提示选择案例研究

> 原文：[https://towardsdatascience.com/beware-of-unreliable-data-in-model-evaluation-a-llm-prompt-selection-case-study-with-flan-t5-88cfd469d058?source=collection_archive---------9-----------------------#2023-06-16](https://towardsdatascience.com/beware-of-unreliable-data-in-model-evaluation-a-llm-prompt-selection-case-study-with-flan-t5-88cfd469d058?source=collection_archive---------9-----------------------#2023-06-16)

## 除非你清理测试数据，否则你可能会为你的LLM选择次优提示（或通过模型评估做出其他次优选择）。

[](https://medium.com/@chrismauck10?source=post_page-----88cfd469d058--------------------------------)[![Chris Mauck](../Images/d0aeb4d0458544afdfdd59915a962b18.png)](https://medium.com/@chrismauck10?source=post_page-----88cfd469d058--------------------------------)[](https://towardsdatascience.com/?source=post_page-----88cfd469d058--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----88cfd469d058--------------------------------) [Chris Mauck](https://medium.com/@chrismauck10?source=post_page-----88cfd469d058--------------------------------)

·

[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F96a38f7ac238&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeware-of-unreliable-data-in-model-evaluation-a-llm-prompt-selection-case-study-with-flan-t5-88cfd469d058&user=Chris+Mauck&userId=96a38f7ac238&source=post_page-96a38f7ac238----88cfd469d058---------------------post_header-----------) 发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----88cfd469d058--------------------------------) ·10 min阅读·2023年6月16日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F88cfd469d058&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeware-of-unreliable-data-in-model-evaluation-a-llm-prompt-selection-case-study-with-flan-t5-88cfd469d058&user=Chris+Mauck&userId=96a38f7ac238&source=-----88cfd469d058---------------------clap_footer-----------)

--

[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F88cfd469d058&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeware-of-unreliable-data-in-model-evaluation-a-llm-prompt-selection-case-study-with-flan-t5-88cfd469d058&source=-----88cfd469d058---------------------bookmark_footer-----------)![](../Images/1fba4029952f6fbbdc9ad048d189e4e1.png)

版权：Arthur Osipyan, Unsplash

*作者：Chris Mauck, Jonas Mueller*

可靠的**模型评估**是MLops和LLMops的核心，指导关键决策，例如选择哪个模型或提示（以及是否部署）。在本文中，我们使用[Google Research](https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html)的FLAN-T5 LLM对各种提示进行测试，以将文本分类为礼貌或不礼貌。在提示候选中，我们发现基于观察到的测试准确性表现最好的提示往往*实际上*比其他提示候选*更差*。对测试数据的仔细审查揭示了这是由于不可靠的注释所致。**在实际应用中，除非你清理你的测试数据以确保其可靠性，否则你可能会为你的LLM选择次优提示（或做出其他由模型评估指导的次优选择）。**

![](../Images/37a5fbd75a640b7e870f173c88982f6a.png)

选择优秀的提示对于确保大型语言模型的准确响应至关重要。

虽然噪声注释在训练数据中的危害已经得到了充分的描述，但本文展示了它们在测试数据中常被忽视的后果。

我目前是[Cleanlab](https://cleanlab.ai/)的一名数据科学家，我很高兴分享高质量测试数据的重要性（以及如何确保高质量测试数据），以确保最佳的LLM提示选择。

# 概述

你可以[在这里](https://s.cleanlab.ai/stanford-politeness-prompt-selection.csv)下载数据。

本文研究了[斯坦福礼貌数据集](https://convokit.cornell.edu/documentation/wiki_politeness.html)的二分类变体（使用[CC BY 4.0许可证](https://creativecommons.org/licenses/by/4.0/)），该数据集中的文本短语被标记为*礼貌*或*不礼貌*。我们使用包含700个短语的固定测试数据集来评估模型。

![](../Images/1ade1f412d93083e70b3a78a8403338e.png)

数据集快照，显示了文本和真实的礼貌标签。

评估分类模型“好坏”的标准做法是通过测量其对未在训练期间见过的样本的预测准确性来进行，这些样本通常被称为“测试”、“评估”或“验证”数据。这提供了一个数值指标来衡量模型A相对于模型B的优劣——如果模型A表现出更高的测试准确率，我们估计它是更好的模型，并会选择部署它而不是模型B。除了模型选择之外，类似的决策框架也可以应用于其他选择，例如使用超参数设置A或B、提示A或B、特征集A或B等。

[常见问题](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/f2217062e9a397a1dca429e7d70bc6ca-Paper-round1.pdf)在现实世界的测试数据中，某些示例由于人为注释错误、数据处理错误、传感器噪声等原因具有错误标签。在这种情况下，测试准确度成为评估模型A和模型B之间**相对性能**不太可靠的指标。让我们举一个非常简单的例子来说明这一点。想象一下，你的测试数据集中有两个*不礼貌*文本示例，但你不知道它们被错误地标记为*礼貌*。例如，在我们的斯坦福礼貌数据集中，我们看到一个实际的人类注释者错误地将这段文本“*你在这里疯了吗？到底发生了什么？*”标记为*礼貌*，而语言显然是激动的。现在你的工作是选择最好的模型来分类这些示例。模型A说这两个示例都是*不礼貌*，而模型B说这两个示例都是*礼貌*。基于这些（错误的）标签，模型A得分为0%，而模型B得分为100% — 你选择了模型B来部署！但等等，哪个模型才*实际上*更强大？

虽然这些含义微不足道，许多人知道现实世界数据充满标记错误，但人们往往只关注其训练数据中的嘈杂标签，却忘记仔细筛选其测试数据，尽管后者指导了重要决策。通过真实数据，本文说明了高质量测试数据对指导LLM提示选择的重要性，并演示了通过算法技术轻松提高数据质量的一种方式。

# 观察到的测试准确度与清洁测试准确度

这里我们考虑由同一组文本示例构成的两个可能的测试集，只有在某些示例（约30%）的标签上有所不同。代表您用来评估准确性的典型数据，一个版本的标签来源于每个示例的单个注释（人类评分者），我们将计算在该版本上模型预测的准确率作为*观察到的测试准确度*。同一测试集的第二个*更清洁*版本具有通过多个人类评分者达成一致共识而建立的高质量标签。我们将在更干净的版本上测量的准确性称为*清洁测试准确度*。因此，*清洁测试准确度*更接近您关心的内容（实际模型部署性能），但*观察到的测试准确度*是大多数应用中您只能观察到的 —— 除非您先清理您的测试数据！

下面是两个测试示例，单个人类注释者误标示例，但许多人类注释者组同意正确标记。

![](../Images/f46afa1e51716d912df4d0b3c5c74ae9.png)

从单个注释者收集的橙色注释成本更低，但常常是错误的。从多个注释者收集的蓝色注释成本更高，但通常更准确。

在现实项目中，您通常无法获得这样的“干净”标签，因此您只能测量*观察测试准确性*。如果您根据这个度量来做出诸如使用哪个LLM或提示的关键决策，请务必首先验证标签的质量。否则，我们发现您可能会做出错误的决定，如在选择礼貌分类提示时观察到的那样。

# 噪声评估数据的影响

作为分类文本礼貌的预测模型，自然而然地使用预训练的大型语言模型（LLM）是合理的。在这里，我们特别使用数据科学家最喜爱的开源FLAN-T5模型。要让这个LLM准确预测文本的礼貌，我们必须提供恰到好处的提示。提示工程可能非常敏感，小的改变会极大地影响准确性！

下面显示的 Prompt A 和 Prompt B（高亮文本）是两个不同的*逻辑链式*提示示例，可以附加在任何**文本样本**前面，以使LLM对其礼貌进行分类。这些提示结合了*少样本*和*指令*提示（稍后详细介绍），提供示例、正确的响应和鼓励LLM解释其推理的理由。这两个提示唯一的区别是实际上在引起LLM回应的高亮文本。少样本示例和推理保持不变。

![](../Images/313afddfe3d4b4c04410243235b488ea.png)

逻辑链式提示为模型提供了每个文本示例为什么正确的推理。

决定哪个提示更好的自然方法是基于它们的*观察测试准确性*。当用于提示 FLAN-T5 LLM 时，我们可以看到 Prompt A 生成的分类在原始测试集上比 Prompt B 的*观察测试准确性*更高。所以显然我们应该部署我们的LLM与 Prompt A，*对吗*？不要那么快！

当我们评估每个提示的*干净测试准确性*时，我们发现 Prompt B 实际上比 Prompt A **好得多**（高出4.5个百分点）。由于*干净测试准确性*更接近我们实际关心的真实表现，如果我们仅仅依赖于原始测试数据而不检查其标签质量，我们将会做出错误的决定！

![](../Images/595291e25668a8dcb82a4ad60c234202.png)

使用观察到的准确性，您会选择 Prompt A 更好。在干净的测试集上评估时，Prompt B 实际上是更好的提示。

# 这只是统计波动吗？

[McNemar检验](https://en.wikipedia.org/wiki/McNemar%27s_test)是一种评估ML准确性报告差异统计显著性的方法。当我们应用此检验评估Prompt A与B在700个文本示例中的*清洁测试准确性*的4.5%差异时，差异具有高度统计显著性（p值 = 0.007，*X²* = 7.086）。因此，所有证据表明Prompt B是一个意义深远的更好选择——我们不应未能通过仔细审计原始测试数据而未选择它！

# 这是一个偶然结果，还是仅仅发生在这两个提示上？

让我们看看其他类型的提示，以了解结果是否只是我们一对思维链提示的巧合。

# 指令提示

这种类型的提示仅向LLM提供一个*指令*，说明它需要对给定的文本示例做什么。考虑以下我们可能想要选择的这种提示对。

![](../Images/6bb28fb6b3a06ad2cdc804d9261be357.png)

# 少量样本提示

这种类型的提示使用两个指令，一个*前缀*和一个*后缀*，还包括两个（预先选择的）文本示例，以向LLM提供所需的输入-输出映射的清晰演示。考虑以下我们可能想要选择的这种提示对。

![](../Images/c9c06c3ea41b82767cdb0a012772d02f.png)

# 模板化提示

这种类型的提示使用两个指令，一个可选的*前缀*和一个*后缀*，以及多选格式，以便模型作为多项选择答案进行分类，而不是直接以预测的类别进行响应。考虑以下我们可能想要选择的这种提示对。

![](../Images/9cf3c405b4de34cea5b1a75e418216e8.png)

# 各种类型提示的结果

除了思维链外，我们还评估了相同FLAN-T5 LLM在这三种额外类型提示下的分类性能。绘制所有这些提示下的*观察测试准确性*与*清洁测试准确性*，我们看到许多提示对存在相同的上述问题，即依赖于*观察测试准确性*导致选择实际更差的提示。

![](../Images/be3a032c2c15f9de4ec3702dd35c282b.png)

作为一个提示工程师，使用可用的测试数据，你会选择左上角的灰色A提示（最高观察准确性），然而最佳提示实际上是右上角的灰色B（最高清洁准确性）。

基于*观察测试准确性*，你会倾向于选择每种类型提示中的“A”提示而不是“B”提示。然而，每种提示类型的更好提示实际上是提示B（具有更高的*清洁测试准确性*）。**这些提示对突显了验证测试数据质量的必要性，否则由于数据问题如噪声注释，你可能会做出次优决策。**

![](../Images/da676344e2573dad4730dae95fb885ca.png)

所有A提示看起来由于其更高的观测准确度而表现更好，但在基于真实数据测试时，所有B提示在客观上表现更好。

在这个图示中，你还可以看到所有A提示的观测准确度被圈了起来，这意味着它们的准确度比B提示更高。类似地，所有B提示的清理准确度也被圈了起来，这意味着它们的准确度比A提示更高。就像本文开头的简单示例一样，你可能倾向于选择所有A提示，但实际上B提示的表现要好得多。

# 改进可用测试数据以获得更可靠的评估

希望你能明白高质量评估数据的重要性。让我们来看几个修正可用测试数据的方法。

## 手动修正

确保测试数据质量的最简单方法是手动检查！确保逐一查看每个示例，验证其标签是否正确。根据测试集的大小，这可能可行也可能不可行。如果测试集相对较小（约100个示例），你可以查看它们并进行必要的修正。如果测试集较大（1000+示例），手动完成这一任务将过于耗时且心理负担过重。我们的测试集相当大，所以我们不会使用这种方法！

## 算法修正

评估你的可用（可能有噪声的）测试集的另一种方法是使用以数据为中心的AI算法来诊断可以修正的问题，以获得更可靠的同一数据集版本（无需收集许多额外的人类注释）。在这里，我们使用Confident Learning算法（通过开源的[cleanlab](https://github.com/cleanlab/cleanlab)包）来检查我们的测试数据，这些算法自动估计哪些示例可能被错误标记。然后我们仅检查这些自动检测的标签问题，并根据需要修正其标签，以产生更高质量的测试数据集版本。我们将对这个版本的测试数据集进行的模型准确度测量称为*CL测试准确度*。

![](../Images/442aa7374c7c2f78afb7370499bd246c.png)

所有B提示的CL测试准确度都更高。通过CL我们纠正了原始测试数据，现在可以信任我们的模型和提示决策。

使用这个新的CL纠正测试集进行模型评估，我们发现之前的所有B提示现在准确度正确地高于A提示。这意味着我们可以信任基于CL纠正测试集做出的决策，比基于原始噪声测试数据做出的决策更可靠。

当然，Confident Learning 并不能神奇地识别数据集中所有的错误。这个算法检测标签错误的效果将依赖于基线 ML 模型的合理预测，即便如此，某些系统性引入的错误仍然无法被检测到（例如，如果我们完全交换两个类别的定义）。有关 Confident Learning 在何种数学假设下被证明有效的详细列表，请参考 [Northcutt 等人原始论文](https://dl.acm.org/doi/10.1613/jair.1.12125)。对于许多现实世界的文本/图像/音频/表格数据集，这个算法至少提供了一种有效的方法，将有限的数据审查资源集中在大数据集中最可疑的例子上。

**你不总是需要花费时间/资源来创建一个“完美”的评估集——使用像 Confident Learning 这样的算法来诊断和修正你现有测试集中的潜在问题，可以提供高质量的数据，以确保最佳的提示和模型选择。**

*除非另有说明，否则所有图像均为作者提供。*
