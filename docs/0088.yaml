- en: ETL testing — How to test your data pipelines the right way
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/forget-about-the-new-data-trends-in-2023-d2756add3317?source=collection_archive---------0-----------------------#2023-01-06](https://towardsdatascience.com/forget-about-the-new-data-trends-in-2023-d2756add3317?source=collection_archive---------0-----------------------#2023-01-06)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Forget about the new data trends in 2023! This fundamental data engineering
    challenge is still not solved.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://vinodhini-sd.medium.com/?source=post_page-----d2756add3317--------------------------------)[![Vino
    Duraisamy](../Images/065b150b7518e0818ef37dbc423aacda.png)](https://vinodhini-sd.medium.com/?source=post_page-----d2756add3317--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d2756add3317--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d2756add3317--------------------------------)
    [Vino Duraisamy](https://vinodhini-sd.medium.com/?source=post_page-----d2756add3317--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fff5dbd5e34b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fforget-about-the-new-data-trends-in-2023-d2756add3317&user=Vino+Duraisamy&userId=ff5dbd5e34b8&source=post_page-ff5dbd5e34b8----d2756add3317---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d2756add3317--------------------------------)
    ·7 min read·Jan 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd2756add3317&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fforget-about-the-new-data-trends-in-2023-d2756add3317&user=Vino+Duraisamy&userId=ff5dbd5e34b8&source=-----d2756add3317---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd2756add3317&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fforget-about-the-new-data-trends-in-2023-d2756add3317&source=-----d2756add3317---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: It is **2023!** New data paradigms (or buzz words) like ELT, reverse ETL, EtLT,
    Data mesh, Data contracts, FinOps and modern data stack found their way into mainstream
    data conversations. Our data teams are still figuring out what is hype and what
    is not.
  prefs: []
  type: TYPE_NORMAL
- en: There may be 10 new paradigms tomorrow but some of the fundamental challenges
    in data engineering — like **data quality** — are still relevant and not solved
    completely (I don’t think we ever will solve this problem completely). The first
    step in improving data quality is to test changes to our data pipelines vigorously.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, let us review the challenges involved in testing data pipelines
    effectively and how to build a well-rounded testing strategy for your organization.
  prefs: []
  type: TYPE_NORMAL
- en: Why is achieving data quality hard?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In software application development world, improving the quality of software
    meant rigorous testing. Similarly in data engineering, we need a comprehensive
    testing strategy to achieve high quality data in production.
  prefs: []
  type: TYPE_NORMAL
- en: Most data teams are running against hard deadlines. So data engineering culture
    is such that we end up building pipelines that serve data by the end of the week
    instead of incorporating all the best practices that are valuable in the long
    run.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In ETL testing, we compare huge volumes of data (say millions of records) often
    from different source systems. We are comparing transformed data that are a result
    of complex SQL queries or Spark jobs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not all data engineers (and the data engineering leaders) are from software
    engineering background and are strong in SWE development principles and best practices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running automated suite of tests and automated deployment/release of data products
    is still not mainstream.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/97fba55bff6a7298497dba7caed923cb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Created by Author'
  prefs: []
  type: TYPE_NORMAL
- en: ETL testing is a data-centric testing process. To effectively test our pipelines
    we need production like data (in terms of volume, variety and velocity).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/45287cd2c724e6bbe0da82b26b116520.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Created by Author'
  prefs: []
  type: TYPE_NORMAL
- en: Getting access to production like data is hard. Here is how data teams in different
    companies tackle the problem of getting the right data to test the data pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Mock Data:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Pros:** This approach is prevalently used by all of us data engineers because
    of ease of mock data creation and availability of synthetic data generation tools
    (such as [Faker](https://fakerjs.dev/)).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cons:** Mock data does not reflect the production data in terms of volume,
    variety or velocity.'
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Sample Prod data to Test/Dev Env:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Pros:** Easy to copy fraction of production data than to copy huge swathes
    of prod data.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cons:** Should use the right sampling strategy to ensure the sample reflects
    real world prod data. Tests that run successfully on sample prod data might fail
    on actual prod data because volume and variety is not guaranteed.'
  prefs: []
  type: TYPE_NORMAL
- en: '3\. Copy all of Prod data to Test Env:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Pros:** Availability of real world production data for testing.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cons:** If prod contains PII data, it might lead to data privacy violations.
    If the prod data is constantly changing, then the copy of prod data in test/dev
    environment will become stale and needs to be constantly updated. Volume and variety
    guaranteed, but not velocity.'
  prefs: []
  type: TYPE_NORMAL
- en: '4\. Copy anonymized prod data to Test Env:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Pros:** Availability of real world production data for testing. Compliance
    to all data privacy regulations.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cons:** Again, a constantly changing prod data means the data in test env
    becomes stale and needs to be refreshed often. PII anonymization needs to be run
    every time you copy data out of prod. Manually running anonymization steps every
    time and maintaining a long-running test data environment is error-prone and resource
    intensive.'
  prefs: []
  type: TYPE_NORMAL
- en: '5\. Using a data versioning tool to mirror prod data to Dev/Test Env:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Pros**: Availability of real world production data. Automated short-lived
    test environments that are available through git-like API.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cons**: Add a new tool (such as [lakeFS](http://lakefs.io)) to your existing
    data stack.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: If you’re interested in exploring data versioning tools for ETL testing,
    here is a [quick guide](https://lakefs.io/blog/etl-testing/) to using lakeFS for
    your reference.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Video tutorial on how to use lakeFS to create different data environments for
    ETL testing. Source: Created by Author.'
  prefs: []
  type: TYPE_NORMAL
- en: Alright, suppose if you have the right data. What next?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A comprehensive list of tests need to be run to ensure the quality and reliability
    of data in the lake house or a ware house.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You could run them using a data quality testing tool (such as Great Expectations,
    soda.io, etc.,) or build an in-house test suite. Doesn’t matter!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on the project, data set and business, different type of tests need
    to be executed. However, here is a checklist of basic tests put together by [Irene](https://www.linkedin.com/in/irene-mikhailouskaya-makaranka-957a7388/)
    on different dimensions of data quality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/0b08da2ddc5d205ff1eaee90771cdf6e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [https://www.scnsoft.com/blog/guide-to-data-quality-management](https://www.scnsoft.com/blog/guide-to-data-quality-management)'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Consistency:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Expanding on the above cheatsheet, consistency checks should include comparing
    the source and target systems. In **some** industries, inconsistent data is permissible
    (YES!) and the variance of error should be below the specified threshold. Because
    there might be inconsistencies between different data systems if they are updated
    asynchronously (thanks to micro-services and message queues).
  prefs: []
  type: TYPE_NORMAL
- en: For example, when you compare number of customer orders in the last hour between
    source and target, the result may vary. But when you run aggregate tests (i.e.,
    number of orders over a month), the data will converge.
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Accuracy:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Your checks should include data validation and domain value checks. For example,
    ***DOB*** column cannot have a value older than 200 years.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on your domain, data should be in a specific range. Some business
    KPIs cannot have specific values. For example, a column named ***click-through
    ratio*** cannot have values greater than 1.0 and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '3\. Completeness:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It should have checks like record count, column count, percentage of missing
    values per column, percentage of null values, range checks (min_value, max_value)
    and basic statistics such as mean, median, variance and data distribution (frequency
    of values in a specific column) and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Again, depending on the business domain, you can have a threshold for error.
    In pharma and finance, the data teams strive for 100% data validation with 0%
    variance in error due to compliance requirements.
  prefs: []
  type: TYPE_NORMAL
- en: '4\. Data Auditability:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Comparing the audit logs of source and data systems and making sure each transformation
    or data movement step is captured and matches.
  prefs: []
  type: TYPE_NORMAL
- en: Data lineage tools support data audits.
  prefs: []
  type: TYPE_NORMAL
- en: '5\. Orderliness:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This involves tests around data and column format checks. Data types of columns,
    % of missing columns, % of columns with mismatched names, % of columns with mismatched
    data format (Date in one source maybe in *MMDDYYYY* format and in *DDMMYYYY* format
    in another source).
  prefs: []
  type: TYPE_NORMAL
- en: '6\. Uniqueness:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the name implies, check which columns should have unique values, % of duplicate
    entries and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '7\. Timeliness:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Late arriving data is a common challenge that we face. This happens if the data
    point generated at the source system arrives at the landing zone after a delay.
    For example, yesterday’s data might land in today’s date partition and get processed
    with today’s data points. This leads to unexpected data errors.
  prefs: []
  type: TYPE_NORMAL
- en: So, validating the ***created_timestamp*** column in the data with the ***date_partition***
    is crucial too.
  prefs: []
  type: TYPE_NORMAL
- en: '1\. White box testing:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The above seven types of tests fall under white box testing. One other type
    would be **unit testing** the complex data transformations. This is to assert
    the KPI definitions and other transformations are as expected.
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Black box testing:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A data pipeline has many moving parts — starting with multiple data sources,
    complex data transformations and concurrent data consumers downstream. So it is
    not sufficient to test only the transformations. End to end testing of ETL pipelines
    is needed to make sure the pipelines are working as expected.
  prefs: []
  type: TYPE_NORMAL
- en: '3\. Regression testing:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Suppose if there was a change in one part of the pipeline, it is mandatory to
    ensure there are no regression errors in other parts of the pipeline due to this
    change.
  prefs: []
  type: TYPE_NORMAL
- en: This is where automated ETL testing is important. That is, after every change,
    a suite of tests (sometimes also called continuous integration tests) need to
    be run. Only if the test suite runs successfully, the change can be pushed to
    production.
  prefs: []
  type: TYPE_NORMAL
- en: You can use a tool like [lakeFS to run automated tests and achieve CI/CD for
    your data lake](https://docs.lakefs.io/use_cases/cicd_for_data.html).
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://docs.lakefs.io/use_cases/cicd_for_data.html?source=post_page-----d2756add3317--------------------------------)
    [## CI/CD for data lakes'
  prefs: []
  type: TYPE_NORMAL
- en: Data pipelines feed processed data from data lakes to downstream consumers like
    business dashboards and machine…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: docs.lakefs.io](https://docs.lakefs.io/use_cases/cicd_for_data.html?source=post_page-----d2756add3317--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '4\. Performance Testing:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to ensuring the quality of data, load testing of ETL pipelines is
    necessary to improve the reliability of data product release.
  prefs: []
  type: TYPE_NORMAL
- en: So, analyze ETL task run times and the order of execution of the tasks to identify
    bottlenecks. Often times, when the data volume increases, it would slow down the
    pipeline. By monitoring the run time, optimizing the distributed compute job and
    by sizing the hardware requirements, this can be resolved.
  prefs: []
  type: TYPE_NORMAL
- en: These are the comprehensive checklist of tests one needs to run. However, most
    of us run a few of them at work. If you are a data engineer, **what is your team’s
    ETL testing strategy like?**
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for Reading!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you like my work and want to support me…
  prefs: []
  type: TYPE_NORMAL
- en: The BEST way to support me is by following me on [Medium](https://medium.com/@vinodhini-sd).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For data engineering best practices, and Python tips for beginners, follow me
    on [LinkedIn](https://www.linkedin.com/in/vinodhini-sd/).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feel free to give claps so I know how helpful this post was for you.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
