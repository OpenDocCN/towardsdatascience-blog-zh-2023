["```py\nDataLakeTokenCredential token = new DataLakeTokenCredential();\nString queueURL = Constants.hadoopConf.get(\"queue.location\");\nString queueName = Constants.hadoopConf.get(\"queue.name\");\nthis.queueClient = new QueueClientBuilder()\n                      .endpoint(queueURL)\n                      .credential(token)\n                      .queueName(queueName)\n                      .buildClient();\n```", "```py\nList<QueueMessageItem> messages = queueClient.receiveMessages(MAX_MESSAGES,\n                    Duration.ofSeconds(60), Duration.ofSeconds(10))\n                    .collect(Collectors.toList());\n```", "```py\n\"data\": {\n  \"api\": \"FlushWithClose\",\n  \"contentType\": \"application/x-compressed\",\n  \"contentLength\": 115739,\n  \"blobType\": \"BlockBlob\",\n  \"blobUrl\": \"https://<accountname>.blob.core.windows.net/users/iceberg/schema/data/file.parquet\",\n  \"url\": \"https://<accountname>.blob.core.windows.net/users/iceberg/schema/data/file.parquet\",\n},\n```", "```py\nList<DataFile> dataFiles = readMonikers();\nAppendFiles append = this.table.newAppend();\nfor (DataFile dataFile : dataFiles) {\n      append.appendFile(dataFile);\n}\nappend.commit();\n```", "```py\nInputFile in = this.table.io().newInputFile(f);\nMetrics metrics = ParquetUtil.fileMetrics(in, \n                      MetricsConfig.forTable(this.table));\n```", "```py\nAppendFiles append = this.table.newAppend();\nDataFile dataFile = DataFiles.builder(this.partitionSpec)\n                    .withPath(filePath)\n                    .withFileSizeInBytes(fileSize)\n                    .withFormat(\"PARQUET\")\n                    .withMetrics(metrics)\n                    .build();\nappend.appendFile(dataFile);\nappend.commit();\n```", "```py\nList<Callable<DataFile>> callableTasks = monikers.stream()\n         .map(m -> new MetricResolver(m, Constants.partitionSpec))\n         .collect(Collectors.toList());\n\nList<Future<DataFile>> futures = executorService.invokeAll(callableTasks);\n```"]