- en: 'Creating Custom Loss Functions in TensorFlow: Understanding the Theory and
    Practicalities'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/creating-custom-loss-functions-in-tensorflow-understanding-the-theory-and-practicalities-383a19e387d6?source=collection_archive---------7-----------------------#2023-01-12](https://towardsdatascience.com/creating-custom-loss-functions-in-tensorflow-understanding-the-theory-and-practicalities-383a19e387d6?source=collection_archive---------7-----------------------#2023-01-12)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Maximizing Model Performance with Custom Loss Functions in TensorFlow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://marcosanguineti.medium.com/?source=post_page-----383a19e387d6--------------------------------)[![Marco
    Sanguineti](../Images/9c426e512b31b77734801912d81f51c1.png)](https://marcosanguineti.medium.com/?source=post_page-----383a19e387d6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----383a19e387d6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----383a19e387d6--------------------------------)
    [Marco Sanguineti](https://marcosanguineti.medium.com/?source=post_page-----383a19e387d6--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33141be0f14d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-custom-loss-functions-in-tensorflow-understanding-the-theory-and-practicalities-383a19e387d6&user=Marco+Sanguineti&userId=33141be0f14d&source=post_page-33141be0f14d----383a19e387d6---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----383a19e387d6--------------------------------)
    ·7 min read·Jan 12, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F383a19e387d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-custom-loss-functions-in-tensorflow-understanding-the-theory-and-practicalities-383a19e387d6&user=Marco+Sanguineti&userId=33141be0f14d&source=-----383a19e387d6---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F383a19e387d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-custom-loss-functions-in-tensorflow-understanding-the-theory-and-practicalities-383a19e387d6&source=-----383a19e387d6---------------------bookmark_footer-----------)![](../Images/ca25b8e66b5b042aedf727f806276652.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Fotis Fotopoulos](https://unsplash.com/@ffstop?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In machine learning, the loss function is a crucial component of the training
    process. It measures the difference between the model’s predictions and the true
    output and is used to update the model’s parameters to minimize this difference.
    While many commonly used loss functions are built into TensorFlow, there may be
    situations where you need to define a custom loss function to better suit the
    specific requirements of your model.
  prefs: []
  type: TYPE_NORMAL
- en: '[## Module: tf.keras.losses | TensorFlow v2.11.0'
  prefs: []
  type: TYPE_NORMAL
- en: Built-in loss functions.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.tensorflow.org](https://www.tensorflow.org/api_docs/python/tf/keras/losses?source=post_page-----383a19e387d6--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: This can be useful for handling imbalanced datasets, incorporating domain knowledge,
    and other specific use cases. In this article, we will explore the theory behind
    custom loss functions, the benefits of using them, and the practicalities of creating
    them in TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Role of Loss Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In machine learning, the goal of training a model is to minimize the difference
    between its predictions and the true output. This difference is measured by a
    loss function, also known as a cost function. A loss function is a scalar function
    that compares the predicted output of a model with its true output. The most commonly
    used loss functions are mean squared error, mean absolute error, and cross-entropy.
  prefs: []
  type: TYPE_NORMAL
- en: '[## Loss function - Wikipedia'
  prefs: []
  type: TYPE_NORMAL
- en: In mathematical optimization and decision theory, a loss function or cost function
    (sometimes also called an error…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: en.wikipedia.org](https://en.wikipedia.org/wiki/Loss_function?source=post_page-----383a19e387d6--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: How do Loss functions drive the optimization?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The optimization algorithm uses the value of the loss function to adjust the
    parameters of the model so that the difference between the predicted and true
    output is minimized. During the training phase, the model is presented with a
    set of inputs and corresponding true outputs, and the parameters of the model
    are adjusted to minimize the loss. The process is iterative, and it stops when
    the loss function reaches a minimum value or when a maximum number of iterations
    is reached.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://en.wikipedia.org/wiki/Gradient_descent?source=post_page-----383a19e387d6--------------------------------)
    [## Gradient descent - Wikipedia'
  prefs: []
  type: TYPE_NORMAL
- en: In mathematics, gradient descent (also often called steepest descent) is a first-order
    iterative optimization algorithm…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: en.wikipedia.org](https://en.wikipedia.org/wiki/Gradient_descent?source=post_page-----383a19e387d6--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: The Need for Custom Loss Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the built-in loss functions provided by TensorFlow are sufficient for
    many cases, there may be situations where a custom loss function is needed. One
    of the most common reasons is handling imbalanced datasets. In such cases, the
    data may contain a large number of examples of one class and only a few examples
    of another class. This can lead to a model that is highly accurate but does not
    perform well for the minority class. In such cases, custom loss functions that
    optimize for recall or precision can be used to balance the trade-off between
    accuracy and performance on the minority class.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent (C1W2L04) — [Source](https://www.youtube.com/@Deeplearningai)
  prefs: []
  type: TYPE_NORMAL
- en: Another reason to use a custom loss function is to incorporate domain knowledge
    into the model. For example, in some cases, the predictions of a model need to
    satisfy specific constraints, such as being non-negative or having a specific
    range. Custom loss functions can be defined to enforce these constraints. Additionally,
    there are also some specific research areas such as object detection, and semantic
    segmentation has their own specific losses like cross-entropy with mask, dice
    loss, focal loss, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Custom Loss Functions in TensorFlow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TensorFlow provides several tools for creating custom loss functions, including
    the `tf.keras.losses` module. To create a custom loss function in TensorFlow,
    you can subclass the `tf.keras.losses.Loss` class and define a `call` method.
    The `call` the method should take in the predicted and true outputs and return
    the calculated loss. It's also possible to pass additional arguments to the custom
    loss function's constructor to use them in the loss calculation.
  prefs: []
  type: TYPE_NORMAL
- en: It is also possible to use functions from the TensorFlow library to create custom
    loss functions, such as using mathematical operations or using the `tf.nn` module.
    One important point to consider when defining custom loss functions is how the
    differentiable, as the optimizer is going to use the gradients of the loss w.r.t
    the model's parameters to update the model.
  prefs: []
  type: TYPE_NORMAL
- en: One common pitfall to avoid when creating custom loss functions is to forget
    to properly handle the case where the inputs are batched, this means that the
    inputs are matrices and not single values, so the mathematical operations should
    work on matrix dimensions as well.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also important to note that the custom loss function’s class should implement
    the `__init__`, `__call__` and `get_config` methods, which is the standard way
    to create a subclass in Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with an example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here’s an example of creating a custom loss function in TensorFlow for handling
    imbalanced datasets. The example is a binary classification problem where the
    goal is to classify data points as either class A or class B, where class A is
    the minority class.
  prefs: []
  type: TYPE_NORMAL
- en: The custom loss function we will create will be a weighted cross-entropy loss,
    which assigns a higher weight to the minority class to balance the trade-off between
    the accuracy and performance of the minority class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are subclassing the `tf.keras.losses.Loss` class and defining a `call`
    the method which takes in y_true and y_pred, the true and predicted labels respectively.
    The method calculates the weighted cross-entropy loss by using the weight variable
    passed during initialization and returning the mean of loss.
  prefs: []
  type: TYPE_NORMAL
- en: In order to use this custom loss function, you can pass an instance of it to
    the `compile` method of your model when defining the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here we passed the WeightedCrossEntropy object with weight=0.8 while compiling
    the model which will be used as the loss function during training.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that the weight parameter should be set based on the
    relative frequencies of the minority and majority classes in the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Please keep in mind that this is an example and you might need to adjust it
    to fit your specific use case, it should also be coupled with techniques such
    as over-sampling, under-sampling, or synthetic data generation in order to handle
    imbalanced data properly.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c979dae66ba8c762241c2df6dfc9548f.png)'
  prefs: []
  type: TYPE_IMG
- en: Some samples of the MNIST database of handwritten digits— [Source](https://www.tensorflow.org/datasets/catalog/mnist)
  prefs: []
  type: TYPE_NORMAL
- en: Here’s an example of how to use the custom loss function we created earlier
    in the context of a digit classification problem using the MNIST dataset. First,
    we will load the MNIST dataset using TensorFlow’s built-in `tf.keras.datasets`
    module and split it into training and testing sets. Next, we will define a simple
    model for classifying the digits in the MNIST dataset. Now, we will determine
    the weight for the custom loss function based on the relative frequencies of the
    minority and majority classes in the training dataset. In this case, the minority
    class is ‘0’ and the majority class is ‘1’ to ‘9’.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We then define the custom loss function and pass the weight parameter to it.
    Finally, we compile our model, specifying the custom loss function, optimizer
    and metrics and we train our model with the training data. It’s important to keep
    in mind that this is just one example of how to use a custom loss function in
    TensorFlow, and there are many other ways you could use it depending on the problem
    you’re trying to solve. Additionally, it’s important to evaluate the performance
    of the model on the test set, as well as interpret the results and compare them
    with other techniques to handle imbalanced datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Custom loss functions can be a powerful tool for improving the performance of
    machine learning models, particularly when dealing with imbalanced datasets or
    incorporating domain knowledge. While creating a custom loss function can seem
    daunting, TensorFlow provides several tools and libraries to make the process
    easier. By understanding the theory and practicalities of custom loss functions,
    you’ll be well-equipped to tackle any challenges that come your way.
  prefs: []
  type: TYPE_NORMAL
- en: Join Medium Membership
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you enjoyed this article and want to keep learning more about this topic,
    I invite you to join Medium membership at this [link](https://marcosanguineti.medium.com/membership).
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://marcosanguineti.medium.com/membership?source=post_page-----383a19e387d6--------------------------------)
    [## Join Medium with my referral link - Marco Sanguineti'
  prefs: []
  type: TYPE_NORMAL
- en: Read every story from Marco Sanguineti (and thousands of other writers on Medium).
    Investment in culture is the best…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: marcosanguineti.medium.com](https://marcosanguineti.medium.com/membership?source=post_page-----383a19e387d6--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: By becoming a member, you’ll have access to a wider variety of high-quality
    content, and exclusive access to member-only stories, and you’ll be supporting
    independent writers and creators like myself. Plus, as a member, you’ll be able
    to highlight your favourite passages, save stories for later, and get personalized
    reading recommendations. Sign up today and let’s continue exploring this topic
    and others together.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for your support! Until next,
  prefs: []
  type: TYPE_NORMAL
- en: Marco
  prefs: []
  type: TYPE_NORMAL
