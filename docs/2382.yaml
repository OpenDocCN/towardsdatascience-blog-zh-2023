- en: Multiple GPU training in PyTorch and Gradient Accumulation as an alternative
    to it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/multiple-gpu-training-in-pytorch-and-gradient-accumulation-as-an-alternative-to-it-e578b3fc5b91?source=collection_archive---------10-----------------------#2023-07-24](https://towardsdatascience.com/multiple-gpu-training-in-pytorch-and-gradient-accumulation-as-an-alternative-to-it-e578b3fc5b91?source=collection_archive---------10-----------------------#2023-07-24)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Code and Theory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@alexml0123?source=post_page-----e578b3fc5b91--------------------------------)[![Alexey
    Kravets](../Images/3b31f9b3c73c6c7ca709f845e6f70023.png)](https://medium.com/@alexml0123?source=post_page-----e578b3fc5b91--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e578b3fc5b91--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e578b3fc5b91--------------------------------)
    [Alexey Kravets](https://medium.com/@alexml0123?source=post_page-----e578b3fc5b91--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcf3e4a05b535&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-gpu-training-in-pytorch-and-gradient-accumulation-as-an-alternative-to-it-e578b3fc5b91&user=Alexey+Kravets&userId=cf3e4a05b535&source=post_page-cf3e4a05b535----e578b3fc5b91---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e578b3fc5b91--------------------------------)
    ·7 min read·Jul 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe578b3fc5b91&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-gpu-training-in-pytorch-and-gradient-accumulation-as-an-alternative-to-it-e578b3fc5b91&user=Alexey+Kravets&userId=cf3e4a05b535&source=-----e578b3fc5b91---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe578b3fc5b91&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-gpu-training-in-pytorch-and-gradient-accumulation-as-an-alternative-to-it-e578b3fc5b91&source=-----e578b3fc5b91---------------------bookmark_footer-----------)![](../Images/ed6fca5026469bec09b620c9620bc331.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://unsplash.com/photos/vBzJ0UFOA70](https://unsplash.com/photos/vBzJ0UFOA70)'
  prefs: []
  type: TYPE_NORMAL
- en: In this article we are going to first see the differences between Data Parallelism
    (DP) and Distributed Data Parallelism (DDP) algorithms, then we will explain what
    Gradient Accumulation (GA) is to finally show how DDP and GA are implemented in
    PyTorch and how they lead to the same result.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When training a deep neural network (DNN), one important hyperparameter is the
    batch size. Normally, the batch size should not be too big because the network
    would tend to overfit, but also not too small because it will result in slow convergence.
  prefs: []
  type: TYPE_NORMAL
- en: 'When working with images of high resolution or other types of data that occupy
    a lot of memory, assuming that today most training of big DNN models are done
    on GPUs, fitting small batch size can be problematic depending on the memory of
    the available GPU. Because, as we said, small batch sizes result in slow convergence,
    there are three main methods we can use to increase the effective batch size:'
  prefs: []
  type: TYPE_NORMAL
- en: Using multiple small GPUs running the model in parallel on mini-batches — DP
    or DDP algorithms
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using a larger GPU (expensive)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Accumulate the gradient over multiple steps
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
