["```py\nfrom pybaseball import (cache, batting_stats_bref, batting_stats, \n                        playerid_reverse_lookup)\nimport pandas as pd\n\ncache.enable()  # avoid unnecessary requests when re-running\n\nMIN_PLATE_APPEARANCES = 200\n\n# For readability and reasonable default sort order\ndf_bref = batting_stats_bref(2023).query(f\"PA >= {MIN_PLATE_APPEARANCES}\"\n                                        ).rename(columns={\"Lev\":\"League\",\n                                                          \"Tm\":\"Team\"}\n                                                )\ndf_bref[\"League\"] = \\\n  df_bref[\"League\"].str.replace(\"Maj-\",\"\").replace(\"AL,NL\",\"NL/AL\"\n                                                  ).astype('category')\n\ndf_fg = batting_stats(2023, qual=MIN_PLATE_APPEARANCES)\n\nkey_mapping = \\\n  playerid_reverse_lookup(df_bref[\"mlbID\"].to_list(), key_type='mlbam'\n                         )[[\"key_mlbam\",\"key_fangraphs\"]\n                          ].rename(columns={\"key_mlbam\":\"mlbID\",\n                                            \"key_fangraphs\":\"IDfg\"}\n                                  )\n\ndf = df_fg.drop(columns=\"Team\"\n               ).merge(key_mapping, how=\"inner\", on=\"IDfg\"\n                      ).merge(df_bref[[\"mlbID\",\"League\",\"Team\"]],\n                              how=\"inner\", on=\"mlbID\"\n                             ).sort_values([\"League\",\"Team\",\"Name\"])\n```", "```py\nprint(df[[\"OBP\",\"SLG\"]].describe().round(3))\n\nprint(f\"\\nCorrelation: {df[['OBP','SLG']].corr()['SLG']['OBP']:.3f}\")\n```", "```py\n OBP      SLG\ncount  362.000  362.000\nmean     0.320    0.415\nstd      0.034    0.068\nmin      0.234    0.227\n25%      0.300    0.367\n50%      0.318    0.414\n75%      0.340    0.460\nmax      0.416    0.654\n\nCorrelation: 0.630\n```", "```py\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ng = sns.JointGrid(data=df, x=\"OBP\", y=\"SLG\", height=5)\ng = g.plot_joint(func=sns.scatterplot, data=df, hue=\"League\",\n                 palette={\"AL\":\"blue\",\"NL\":\"maroon\",\"NL/AL\":\"green\"},\n                 alpha=0.6\n                )\ng.fig.suptitle(\"On-base percentage vs. Slugging\\n2023 season, min \"\n               f\"{MIN_PLATE_APPEARANCES} plate appearances\"\n              )\ng.figure.subplots_adjust(top=0.9)\nsns.kdeplot(x=df[\"OBP\"], color=\"orange\", ax=g.ax_marg_x, alpha=0.5)\nsns.kdeplot(y=df[\"SLG\"], color=\"orange\", ax=g.ax_marg_y, alpha=0.5)\nsns.kdeplot(data=df, x=\"OBP\", y=\"SLG\",\n            ax=g.ax_joint, color=\"orange\", alpha=0.5\n           )\ndf_extremes = df[ df[\"OBP\"].isin([df[\"OBP\"].min(),df[\"OBP\"].max()]) \n                 | df[\"OPS\"].isin([df[\"OPS\"].min(),df[\"OPS\"].max()])\n                ]\n\nfor _,row in df_extremes.iterrows():\n    g.ax_joint.annotate(row[\"Name\"], (row[\"OBP\"], row[\"SLG\"]),size=6,\n                      xycoords='data', xytext=(-3, 0),\n                        textcoords='offset points', ha=\"right\",\n                      alpha=0.7)\nplt.show()\n```", "```py\nimport numpy as np\n\nX = df[[\"OBP\",\"SLG\"]].to_numpy()\n\nGRID_RESOLUTION = 200\n\ndisp_x_range, disp_y_range = ( (.6*X[:,i].min(), 1.2*X[:,i].max()) \n                               for i in [0,1]\n                             )\nxx, yy = np.meshgrid(np.linspace(*disp_x_range, GRID_RESOLUTION), \n                     np.linspace(*disp_y_range, GRID_RESOLUTION)\n                    )\ngrid_shape = xx.shape\ngrid_unstacked = np.c_[xx.ravel(), yy.ravel()]\n```", "```py\nfrom sklearn.covariance import EllipticEnvelope\n\nell = EllipticEnvelope(random_state=17).fit(X)\ndf[\"outlier_score_ell\"] = ell.decision_function(X)\nZ_ell = ell.decision_function(grid_unstacked).reshape(grid_shape)\n```", "```py\nK = int(np.sqrt(X.shape[0]))\n\nprint(f\"Using K={K} nearest neighbors.\")\n```", "```py\nUsing K=19 nearest neighbors.\n```", "```py\nfrom scipy.spatial.distance import pdist, squareform\n\n# If we didn't have the elliptical envelope already,\n# we could calculate robust covariance:\n#   from sklearn.covariance import MinCovDet\n#   robust_cov = MinCovDet().fit(X).covariance_\n# But we can just re-use it from elliptical envelope:\nrobust_cov = ell.covariance_\n\nprint(f\"Robust covariance matrix:\\n{np.round(robust_cov,5)}\\n\")\n\ninv_robust_cov = np.linalg.inv(robust_cov)\n\nD_mahal = squareform(pdist(X, 'mahalanobis', VI=inv_robust_cov))\n\nprint(f\"Mahalanobis distance matrix of size {D_mahal.shape}, \"\n      f\"e.g.:\\n{np.round(D_mahal[:5,:5],3)}...\\n...\\n\")\n```", "```py\nRobust covariance matrix:\n[[0.00077 0.00095]\n [0.00095 0.00366]]\n\nMahalanobis distance matrix of size (362, 362), e.g.:\n[[0\\.    2.86  1.278 0.964 0.331]\n [2.86  0\\.    2.63  2.245 2.813]\n [1.278 2.63  0\\.    0.561 0.956]\n [0.964 2.245 0.561 0\\.    0.723]\n [0.331 2.813 0.956 0.723 0\\.   ]]...\n...\n```", "```py\nfrom sklearn.neighbors import LocalOutlierFactor\n\nlof = LocalOutlierFactor(n_neighbors=K, metric=\"precomputed\", novelty=True\n                        ).fit(D_mahal)\n\ndf[\"outlier_score_lof\"] = lof.negative_outlier_factor_\n```", "```py\nfrom scipy.spatial.distance import cdist\n\nD_mahal_grid = cdist(XA=grid_unstacked, XB=X, \n                     metric='mahalanobis', VI=inv_robust_cov\n                    )\nZ_lof = lof.decision_function(D_mahal_grid).reshape(grid_shape)\n```", "```py\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.kernel_approximation import Nystroem\nfrom sklearn.linear_model import SGDOneClassSVM\n\nsuv = make_pipeline(\n            RobustScaler(),\n            Nystroem(random_state=17),\n            SGDOneClassSVM(random_state=17)\n).fit(X)\n\ndf[\"outlier_score_svm\"] = suv.decision_function(X)\n\nZ_svm = suv.decision_function(grid_unstacked).reshape(grid_shape)\n```", "```py\nfrom sklearn.ensemble import IsolationForest\n\niso = IsolationForest(random_state=17).fit(X)\n\ndf[\"outlier_score_iso\"] = iso.score_samples(X)\n\nZ_iso = iso.decision_function(grid_unstacked).reshape(grid_shape)\n```", "```py\nfrom adjustText import adjust_text\nfrom sklearn.preprocessing import QuantileTransformer\n\nN_QUANTILES = 8 # This many color breaks per chart\nN_CALLOUTS=15  # Label this many top outliers per chart\n\nfig, axs = plt.subplots(2, 2, figsize=(12, 12), sharex=True, sharey=True)\n\nfig.suptitle(\"Comparison of Outlier Identification Algorithms\",size=20)\nfig.supxlabel(\"On-Base Percentage (OBP)\")\nfig.supylabel(\"Slugging (SLG)\")\n\nax_ell = axs[0,0]\nax_lof = axs[0,1]\nax_svm = axs[1,0]\nax_iso = axs[1,1]\n\nmodel_abbrs = [\"ell\",\"iso\",\"lof\",\"svm\"]\n\nqt = QuantileTransformer(n_quantiles=N_QUANTILES)\n\nfor ax, nm, abbr, zz in zip( [ax_ell,ax_iso,ax_lof,ax_svm], \n                            [\"Elliptic Envelope\",\"Isolation Forest\",\n                             \"Local Outlier Factor\",\"One-class SVM\"], \n                            model_abbrs,\n                            [Z_ell,Z_iso,Z_lof,Z_svm]\n                           ):\n    ax.title.set_text(nm)\n    outlier_score_var_nm = f\"outlier_score_{abbr}\"\n\n    qt.fit(np.sort(zz.reshape(-1,1)))\n    zz_qtl = qt.transform(zz.reshape(-1,1)).reshape(zz.shape)\n\n    cs = ax.contourf(xx, yy, zz_qtl, cmap=plt.cm.OrRd.reversed(), \n                     levels=np.linspace(0,1,N_QUANTILES)\n                    )\n    ax.scatter(X[:, 0], X[:, 1], s=20, c=\"b\", edgecolor=\"k\", alpha=0.5)\n\n    df_callouts = df.sort_values(outlier_score_var_nm).head(N_CALLOUTS)\n    texts = [ ax.text(row[\"OBP\"], row[\"SLG\"], row[\"Name\"], c=\"b\",\n                      size=9, alpha=1.0) \n             for _,row in df_callouts.iterrows()\n            ]\n    adjust_text(texts, \n                df_callouts[\"OBP\"].values, df_callouts[\"SLG\"].values, \n                arrowprops=dict(arrowstyle='->', color=\"b\", alpha=0.6), \n                ax=ax\n               )\n\nplt.tight_layout(pad=2)\nplt.show()\n\nfor var in [\"OBP\",\"SLG\"]:\n    df[f\"Pctl_{var}\"] = 100*(df[var].rank()/df[var].size).round(3)\n\nmodel_score_vars = [f\"outlier_score_{nm}\" for nm in model_abbrs]  \nmodel_rank_vars = [f\"Rank_{nm.upper()}\" for nm in model_abbrs]\n\ndf[model_rank_vars] = df[model_score_vars].rank(axis=0).astype(int)\n\n# Averaging the ranks is arbitrary; we just need a countdown order\ndf[\"Rank_avg\"] = df[model_rank_vars].mean(axis=1)\n\nprint(\"Counting down to the greatest outlier...\\n\")\nprint(\n    df.sort_values(\"Rank_avg\",ascending=False\n                  ).tail(N_CALLOUTS)[[\"Name\",\"AB\",\"PA\",\"H\",\"2B\",\"3B\",\n                                      \"HR\",\"BB\",\"HBP\",\"SO\",\"OBP\",\n                                      \"Pctl_OBP\",\"SLG\",\"Pctl_SLG\"\n                                     ] + \n                             [f\"Rank_{nm.upper()}\" for nm in model_abbrs]\n                            ].to_string(index=False)\n)\n```", "```py\nCounting down to the greatest outlier...\n\n            Name  AB  PA   H  2B  3B  HR  BB  HBP  SO   OBP  Pctl_OBP   SLG  Pctl_SLG  Rank_ELL  Rank_ISO  Rank_LOF  Rank_SVM\n   Austin Barnes 178 200  32   5   0   2  17    2  43 0.256       2.6 0.242       0.6        19         7        25        12\n   J.D. Martinez 432 479 117  27   2  33  34    2 149 0.321      52.8 0.572      98.1        15        18         5        15\n      Yandy Diaz 525 600 173  35   0  22  65    8  94 0.410      99.2 0.522      95.4        13        15        13        10\n       Jose Siri 338 364  75  13   2  25  20    2 130 0.267       5.5 0.494      88.4         8        14        15        13\n       Juan Soto 568 708 156  32   1  35 132    2 129 0.410      99.2 0.519      95.0        12        13        11        11\n    Mookie Betts 584 693 179  40   1  39  96    8 107 0.408      98.6 0.579      98.3         7        10        20         7\n   Rob Refsnyder 202 243  50   9   1   1  33    5  47 0.365      90.5 0.317       6.6         5        19         2        14\n  Yordan Alvarez 410 496 120  24   1  31  69   13  92 0.407      98.3 0.583      98.6         6         9        18         6\n Freddie Freeman 637 730 211  59   2  29  72   16 121 0.410      99.2 0.567      97.8         9        11         9         8\n      Matt Olson 608 720 172  27   3  54 104    4 167 0.389      96.5 0.604      99.2        11         6         7         9\n   Austin Hedges 185 212  34   5   0   1  11    2  47 0.234       0.3 0.227       0.3        10         1         4         3\n     Aaron Judge 367 458  98  16   0  37  88    0 130 0.406      98.1 0.613      99.4         3         5         6         4\nRonald Acuna Jr. 643 735 217  35   4  41  80    9  84 0.416     100.0 0.596      98.9         2         3        10         2\n    Corey Seager 477 536 156  42   0  33  49    4  88 0.390      97.0 0.623      99.7         4         4         3         5\n   Shohei Ohtani 497 599 151  26   8  44  91    3 143 0.412      99.7 0.654     100.0         1         2         1         1\n```"]