- en: Hands-On GenAI for Product & Engineering Leaders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/hands-on-genai-for-product-engineering-leaders-6ee6ad94e058?source=collection_archive---------10-----------------------#2023-11-28](https://towardsdatascience.com/hands-on-genai-for-product-engineering-leaders-6ee6ad94e058?source=collection_archive---------10-----------------------#2023-11-28)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Make better product decisions by taking a peek under the hood of LLM-based products
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@ninadsohoni?source=post_page-----6ee6ad94e058--------------------------------)[![Ninad
    Sohoni](../Images/8d6ec40665bb85fb7b4ece99e6a40913.png)](https://medium.com/@ninadsohoni?source=post_page-----6ee6ad94e058--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6ee6ad94e058--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6ee6ad94e058--------------------------------)
    [Ninad Sohoni](https://medium.com/@ninadsohoni?source=post_page-----6ee6ad94e058--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5ee93978501b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-genai-for-product-engineering-leaders-6ee6ad94e058&user=Ninad+Sohoni&userId=5ee93978501b&source=post_page-5ee93978501b----6ee6ad94e058---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6ee6ad94e058--------------------------------)
    ¬∑35 min read¬∑Nov 28, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6ee6ad94e058&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-genai-for-product-engineering-leaders-6ee6ad94e058&user=Ninad+Sohoni&userId=5ee93978501b&source=-----6ee6ad94e058---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6ee6ad94e058&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-genai-for-product-engineering-leaders-6ee6ad94e058&source=-----6ee6ad94e058---------------------bookmark_footer-----------)![](../Images/4b2f853b2bede6542039512f44e4c4c2.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image generated by [Bing Image Creator](https://www.bing.com/create) based on
    the prompt ‚Äúproduct owner for a machine learning powered application working on
    a prototype‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you‚Äôre a regular driver, the hood of your car could be full of cotton for
    all you care. However, if you are anywhere in the design and execution chain responsible
    for building a better car, knowing what the different parts are and how they work
    together will help you build a better car.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, as a product owner, business leader, or an engineer responsible for
    creating new Large Language Model (LLM) powered products, or for bringing LLMs
    / generative AI to existing products, an understanding of building blocks that
    go into an LLM-powered products will help you tackle strategic and tactical questions
    pertaining to technology, such as,
  prefs: []
  type: TYPE_NORMAL
- en: Is our use case a good fit for LLM-powered solutions? Perhaps traditional analytics,
    supervised machine learning, or another approach is a better fit?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If LLMs are the way to go, can our use case be addressed by an off-the-shelf
    product (say, ChatGPT Enterprise) now or in the near-future? Classic build-vs-buy
    decision.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the different building blocks of our LLM-powered product? Which of
    these are commoditized, and which are likely to need more time to build and test?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we measure the performance of our solution? What levers are available
    to improve the quality of outputs from our product?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is our data quality acceptable for the use case? Are we organizing our data
    correctly, and passing relevant data to the LLM?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can we be confident that the LLM‚Äôs responses will always be factually accurate.
    That is, will our solution ‚Äòhallucinate‚Äô when generating responses once in a while?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While these questions are answered later in the article, the objective with
    getting a little hands-on is to build an intuitive understanding of LLM-powered
    solutions, which should help you answer these questions on your own, or at least
    put you in a better position to research further.
  prefs: []
  type: TYPE_NORMAL
- en: In a [previous article](/how-genai-solutions-revolutionize-business-automation-57747b0f11ce),
    I delved into some foundational concepts associated with building LLM-powered
    products. But you can‚Äôt learn to drive just by reading blogs or watching videos
    ‚Äî it requires you to get behind the wheel. Well, thanks to the age we live in,
    we have free-to-us tools (which [cost millions of dollars to create](https://lambdalabs.com/blog/demystifying-gpt-3))
    at our fingertips to build our own LLM solution in under an hour! So, in this
    article, I propose we do just that. It‚Äôs a much easier undertaking than learning
    to drive üòù.
  prefs: []
  type: TYPE_NORMAL
- en: '**Build a Chatbot that allows you to ‚Äúchat‚Äù with websites**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Objective: Build a chatbot that answers questions based on information on a
    provided website, **to gain a better understanding of the building blocks** of
    popular GenAI solutions today'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We will create a question-answering chatbot that will answer questions based
    on information in a knowledge repository. This solution pattern, called Retrieval
    Augmented Generation (RAG), has become a go-to solution pattern in companies.
    One reason for the popularity of RAG is that rather than relying solely on the
    LLMs own knowledge, you can bring external information to the LLM in an automated
    manner. In real-world implementations, the external information can be from an
    organization‚Äôs own knowledge repository, holding proprietary information to enable
    the product to answer questions about the business, its products, business processes,
    etc. RAG also reduces LLM ‚Äòhallucinations‚Äô, in that the generated responses are
    grounded in the information provided to the LLM. According to a [recent talk](https://www.youtube.com/watch?v=xa7k9MUeIdk),
  prefs: []
  type: TYPE_NORMAL
- en: ‚ÄúRAG will the the default way enterprises use LLMs‚Äù
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: -Dr. Waleed Kadous, Chief Scientist, AnyScale
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For our hands-on exercise, we will let a user enter a website, which our solution
    will ‚Äúread‚Äù into its knowledge repository. The solution will then be able to answer
    questions based on the information on the website. The website is a placeholder
    ‚Äî in reality, this can be tweaked to consume text from any data source like PDFs,
    Excel, another product or internal system, etc. This approach works for other
    media ‚Äî such as images ‚Äî but they require a few different LLMs. For now, we will
    focus on text from websites.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our example, we will use a sample book list webpage created for this blog:
    [Books I‚Äôd Pick Up ‚Äî If There Were More Hours in the Day!](https://ninadsohoni.github.io/booklist/)
    You are welcome to use another website of your choice.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here‚Äôs what our result will look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/781fc874901946c9c92cb4c6ab0f0984.png)'
  prefs: []
  type: TYPE_IMG
- en: LLM-powered chatbot to intelligently answer questions based on information on
    a website. (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the steps we will go through to build our solution:'
  prefs: []
  type: TYPE_NORMAL
- en: 0\. Getting Set Up ‚Äî Google Colaboratory & OpenAI API Key
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Create knowledge repository
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Search question-relevant context
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Generate answer using LLM
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Add ‚Äúchat‚Äù capability (optional)
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Add a simple pre-coded UI (optional)
  prefs: []
  type: TYPE_NORMAL
- en: '**0.1\. Getting Set Up ‚Äî Google Colaboratory & OpenAI API Key**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To build a LLM solution, we need a place to write and run code, and an LLM to
    generate responses to questions. We will use Google Colab for the code environment,
    and the model behind ChatGPT as our LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs start with setting up [Google Colab](https://colab.google/), a free service
    by Google that enables running Python code in an easy-to-read format ‚Äî no need
    to install anything on your computer. I find it convenient to add Colab to Google
    Drive so that I can later find Colab notebooks easily.
  prefs: []
  type: TYPE_NORMAL
- en: To do so, navigate to **Google Drive** (using a browser) **> New > More > Connect
    More Apps >** Search **‚ÄúColaboratory‚Äù** in the Google Marketplace **> Install.**
  prefs: []
  type: TYPE_NORMAL
- en: To start using Colabobatory (‚ÄúColab‚Äù), you can select **New** > **More** > **Google
    Colaboratory.** This will create a new notebook in your Google Drive so you can
    go back to it.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ee514c154f891d1fd129b21195a698bd.png)'
  prefs: []
  type: TYPE_IMG
- en: Google Colaboratory accessible in Google Drive. (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: Next, let‚Äôs get access to an LLM. There are several open source and proprietary
    options available. While open source LLMs are free, powerful LLMs generally require
    powerful GPUs to process inputs and generate responses, and there is a nominal
    operate cost for GPUs. In our example, we will instead use OpenAI‚Äôs service to
    use the LLM used by ChatGPT. To do so, you will require an API key, which is like
    a username/password rolled into one to let OpenAI know who is trying to access
    the LLM. As of this writing, OpenAI offered a $5 credit for new users, which should
    be sufficient for this hands-on tutorial. Here are the steps to get the API key,
  prefs: []
  type: TYPE_NORMAL
- en: Go to [**OpenAI‚Äôs Platform website**](https://platform.openai.com/signup/)>
    **Get started > Sign up** with email & password or use Google or Microsoft account.
    You may also need a phone number to verification.
  prefs: []
  type: TYPE_NORMAL
- en: Once logged in, click on your profile icon in the top right corner > **View
    API keys** > **Create new secret key**. The key will look something like the following
    (fake key for informational purposes only). Save it for use later.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now, we are ready to build the solution.
  prefs: []
  type: TYPE_NORMAL
- en: 0.2\. Prepare Notebook for Building Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We need to install some packages in the Colab environment to facilitate our
    solution. Just type the following code in the text box (called a ‚Äúcell‚Äù) in Colab
    and press ‚ÄúShift + Return (enter)‚Äù. Alternatively, just click the ‚Äúplay‚Äù button
    on the left of the cell or use the ‚ÄúRun‚Äù menu at the top of the notebook. You
    may need to use the menu to insert new code cells for running subsequent code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Next, we should pull in code from the packages we installed so that the packages
    can be used in the code we write. You can use the new code cell and hit ‚ÄúShift
    + Return‚Äù again ‚Äî and continue in this manner for each subsequent code block.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Finally, add the OpenAI API key to a variable. Note that this key is like your
    password ‚Äî do not share it. Also, do not share your Colab notebook without removing
    the API key first.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we are ready to start building the solution. Here is a high-level view
    of the next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f4773785ae6fd1047c8a8c724bb381bf.png)'
  prefs: []
  type: TYPE_IMG
- en: Core steps to build the RAG solution (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: When coding, we will use LangChain, which has emerged as a popular framework
    to build solutions such as this one. It has packages for facilitating each of
    the steps from connecting to data sources to sending and receiving information
    from the LLM. LlamaIndex is another option to simplify building LLM-powered apps.
    While it‚Äôs not strictly required to use LangChain (or LlamaIndex), and in some
    cases the high-level abstraction may make has the risk of leaving teams oblivious
    to what‚Äôs happening under the hood, we will use LangChain but still look under
    the hood often.
  prefs: []
  type: TYPE_NORMAL
- en: Note that since the pace of innovation is so quick, it is likely that packages
    used in this code get updated, and some updates may cause the code to stop working
    unless updated accordingly. I do not intend to keep this code up-to-date. Nevertheless,
    the article is intended to serve as a demonstration, and the code could serve
    as reference, or a starting point that you may adapt to your needs.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Create Knowledge Repository
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**1.1\. Identify & Read-in Documents** Let‚Äôs access the book list and read
    the content into our Colab environment. The content is loaded as HTML originally,
    which is useful for web-browsers. However, we will convert it to a more human
    readable format using a HTML to Text convertor.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is what running the code generates on Google Colab:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/de6416c74c20602de4edbb11a39d23f5.png)'
  prefs: []
  type: TYPE_IMG
- en: Result of executing the code above . The website content is loaded into Colab
    environment. (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: '**1.2\. Break Documents into Smaller Excerpts** There is one more step before
    we load the blog‚Äôs information into our knowledge repository (which is essentially
    a database of our choice). The text should not be loaded into the database as-is.
    It should first be split into smaller chunks. This is for a few reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: If our text is too long, it cannot be sent to the LLM due to exceeding the text
    length threshold (known as *‚Äúcontext size‚Äù*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Longer text might have broad, loosely related information. We would be relying
    on the LLM to pick out the relevant portions ‚Äî and this might not always work
    out as expected. With smaller chunks, we could use retrieval mechanisms to identify
    only the relevant pieces of information to send to the LLM, as we will see later.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LLMs are prone to have stronger attention at the beginning and end of text,
    so longer chunks could lead the LLM to pay less attention to more content later
    (known as *‚Äúlost in the middle‚Äù*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The right chunk sizes for each use case will vary per the specifics of the use
    case, including the type of content, the LLM being used, and other factors. It
    is prudent to experiment with different chunk sizes and evaluate response quality
    before finalizing the solution. For this demonstration, let‚Äôs use context-aware
    splitting where each book recommendation from the list gets its own chunk,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ca21a3be9e3be635cf3f4fdb21f09bd4.png)'
  prefs: []
  type: TYPE_IMG
- en: One of many document chunks as a result of splitting the original content. (Image
    by the author)
  prefs: []
  type: TYPE_NORMAL
- en: Note that if the chunks created so far are still longer than desired, they can
    be split further using other text splitting algorithms, easily available via LangChain
    or LlamaIndex. For example, each book‚Äôs review could be split into paragraphs,
    if needed.
  prefs: []
  type: TYPE_NORMAL
- en: '**1.3\. Load Excerpts into Knowledge Repository** The text chunks are now ready
    to be loaded to the knowledge repository. These are first passed through an embedding
    model to convert the text to a series of numbers that capture the meaning of the
    text. Then the actual text along with the numerical representation (i.e., embeddings)
    will be loaded to the vector database ‚Äî our knowledge repository. Note that embeddings
    are also generated by LLMs, just a different kind than the chat LLM. If you wish
    to read more about embeddings, the [previous article](/how-genai-solutions-revolutionize-business-automation-57747b0f11ce)
    demonstrates the concept using examples.'
  prefs: []
  type: TYPE_NORMAL
- en: We will use a vector database to store all the information. This will manifest
    our knowledge repository. Vector databases are purpose-built to enable searching
    by embedding similarity. If we want to search something from the database, the
    search term is converted to a numerical representation by running it through the
    embedding model first, and then the question embeddings are compared to all of
    the embeddings in the database. Records (in our case, text chunks about each book
    on the list) that are closest to the question embeddings are returned as search
    results, as long as they clear a threshold.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/06255fabddafae0036919bdeaa6d2fdf.png)'
  prefs: []
  type: TYPE_IMG
- en: A view of the first few text chunks loaded to the vector DB along with numerical
    representations (i.e., embeddings). (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Search Question-Relevant Context
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We ultimately want our solution to pick out relevant information from our vector
    DB knowledge corpus and pass it along to the LLM along with the question we want
    the LLM to answer. Let‚Äôs try out the vector DB search, by asking the question
    ‚ÄúCan you recommend a few detective novels?‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/15bebbaf8b218ae61b888fe54953a1f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Top search results for the question ‚ÄúCan you recommend a few detective novels?‚Äù
    (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: We get top 4 results by default, unless we explicitly set the value to a different
    number. In this example, the top search, which is a Sherlock Holmes novel, mentions
    the term ‚Äòdetective‚Äô directly. The second result (*The Day of the Jackal*) does
    not have the term ‚Äòdetective‚Äô, but mentions ‚Äòpolice agencies‚Äô and ‚Äòuncover the
    plot‚Äô, which bear semantic association with ‚Äúdetective novels‚Äù. The third result
    (*The Undercover Economist*) mentions the term ‚Äòundercover‚Äô, though it is about
    economics. I believe the last result was fetched due to its association with novels
    / books rather than ‚Äúdetective novels‚Äù specifically, because four results were
    requested.
  prefs: []
  type: TYPE_NORMAL
- en: Also, it is not strictly necessary to use a vector DB. You could load embeddings
    and facilitate search in other forms of storage. ‚ÄúNormal‚Äù relational databases
    or even Excel can be used. But you would have to handle the ‚Äúsimilarity‚Äù calculation,
    which can be a dot product when using OpenAI embeddings, in your application logic.
    On the other hand, a vector DB does that for you.
  prefs: []
  type: TYPE_NORMAL
- en: Note that if we wanted to pre-filter some search results by metadata, we could
    do so. For our demonstration, let‚Äôs filter by the genre, which is under ‚ÄúHeader
    2‚Äù in the metadata we loaded from the book list.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3e7381e8da847e76b51f202714d7e319.png)'
  prefs: []
  type: TYPE_IMG
- en: Search results based on applying a metadata pre-filter, showing only key columns.
    (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: An interesting opportunity offered by LLMs is to use the LLM itself to inspect
    a user question, review available metadata, assess whether a metadata-based pre-filter
    is required and possible, and formulate the pre-filter query code, which can then
    be used on the vector DB to actually pre-filter data. See LangChain‚Äôs [self-query
    retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/)
    for more information about this.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Generate Answer using LLM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we will add instructions to the LLM that basically say ‚ÄúI am going to
    give you some information snippets, and a question. Please answer the question
    using the provided information snippets‚Äù. Then, we bundle these instructions,
    the search results from the vector DB, and our question into a packet and send
    it to the LLM to respond. All these steps are facilitated by the following code.
  prefs: []
  type: TYPE_NORMAL
- en: Note that LangChain offers the opportunity to abstract some of this code, so
    your code doesn‚Äôt have to be as verbose as the code that follows. However, the
    objective with the code below is to showcase the instructions sent to the language
    model. Here‚Äôs where you can customize them too ‚Äî like in this case, the default
    instructions are changed to request the LLM to keep responses as concise as possible.
    If the default works for your use case, your code can skip the question template
    part altogether and LangChain will use the default prompt from its own package
    when sending a request to the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now, let‚Äôs ask for detective novel recommendations again and see what we get
    as a response.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/546144d38d406b1c88712edd0333f838.png)'
  prefs: []
  type: TYPE_IMG
- en: Response from the solution recommending detective novels (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs confirm whether the model reviewed all four of our previous search results
    from the vector DB, or did it just get the two results noted in the response?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5f951dd421f7adbca2edefd17e925e95.png)'
  prefs: []
  type: TYPE_IMG
- en: What was passed in as context to the LLM along with the question, to facilitate
    the response. (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the LLM still had access to all four search results and reasoned
    that only the first two books were detective novels.
  prefs: []
  type: TYPE_NORMAL
- en: Be forewarned that the response from the LLM could change each time you ask
    a question, despite sending the same instructions, and the same information from
    the vector database. For example, on asking about fantasy book recommendations,
    the LLM sometimes gave three book recommendations, and sometimes more ‚Äî though
    all from the book list. In all cases, the top recommended book stayed the same.
    Note that these variations were despite configuring the consistency ‚Äî creativity
    spectrum ‚Äî the ‚Äòtemperature‚Äô parameter ‚Äî to 0 to minimize variance.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Add ‚Äúchat‚Äù capability (optional)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The solution now has the necessary core functionality ‚Äî it is able to read
    in information from a website and answer questions based on that information.
    But it currently does not offer a ‚Äúconversational‚Äù user experience. Thanks to
    ChatGPT, the ‚Äúchat interface‚Äù has become the dominant design: we now expect this
    to be the ‚Äúnatural‚Äù way to interact with generative AI, and LLMs in particular
    üòÖ. The first steps towards getting to the chat interface involves adding ‚Äúmemory‚Äù
    to the solution.'
  prefs: []
  type: TYPE_NORMAL
- en: ‚ÄúMemory‚Äù here is an illusion, in that the LLM is not actually remembering the
    conversation up to that point ‚Äî it needs to be shown the full conversation history
    in each turn. So, if a user asks the LLM a follow-up question, the solution will
    package the original question, the LLM‚Äôs original answer, and the follow-up question
    and send it to the LLM. The LLM reads the entire conversation and generates a
    meaningful response to continue the conversation.
  prefs: []
  type: TYPE_NORMAL
- en: In question-answering chatbots, like the one we‚Äôre building, this approach needs
    to be extended further because there is the interim step to reach out to the vector
    database and pull relevant information to formulate the response to a user‚Äôs follow-up
    question. The way ‚Äúmemory‚Äù is simulated in question-answering chatbots is,
  prefs: []
  type: TYPE_NORMAL
- en: Retain all questions and responses (in a variable) as ‚Äúchat history‚Äù
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the user asks a question, send the chat history and the new question to
    the LLM and ask it to generate a standalone question
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At this point, the chat history is no longer needed. Use the standalone question
    to run a new search on the vector DB
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pass the standalone question and search results, along with instructions to
    the LLM to get a final answer. This step is similar to what we implemented in
    the previous stage ‚ÄúGenerate Answer using LLM‚Äù
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While we can keep a track of chat history in simpler variables, we will use
    one of LangChain‚Äôs memory types. The particular memory object we will use offers
    the nice feature of automatically truncating older chat history when it reaches
    a size limit you specify, generally the size of the text that the selected LLM
    can accept. In our case, the LLM should be able to accept a little over 4,000
    ‚Äútokens‚Äù (which are word parts), which should roughly be 3,000 words or ~5 pages
    from a Word document. OpenAI offers a 16k variant of the same ChatGPT LLM, which
    can accept 4x the input. Hence, the need to configure the memory size.
  prefs: []
  type: TYPE_NORMAL
- en: Here is the code to achieve these steps. Again, LangChain provides a higher-level
    abstraction and the code does not have to be so explicit. This version is just
    to expose the underlying instructions sent to the LLM ‚Äî first to condense the
    chat history into a single standalone question, which will then be used for the
    vector DB search, and the second to generate a response to the generated standalone
    question based on vector DB search results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e472c9c36dc137b348e73db60cfafa78.png)'
  prefs: []
  type: TYPE_IMG
- en: Detective novel recommendations from the solution. Same response as the one
    received earlier using only ‚Äúquestion-answering‚Äù capability, without ‚Äúmemory‚Äù
    (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs ask a follow-up question and look at the response to validate the solution
    now has ‚Äúmemory‚Äù and can respond conversationally to follow-up questions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/fed2232a1253700a345d4e1bb406e967.png)'
  prefs: []
  type: TYPE_IMG
- en: Response to follow-up question asking more information about ‚Äúthe second book‚Äù.
    The solution responds back with more information about the same book as before
    (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs look at what is happening under the hood to validate that the solution
    does indeed go through the four steps outlined at the beginning of this section.
    Let‚Äôs start with the chat history to verify that the solution does indeed log
    the conversation so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5eb9194b5133cbb28fd5070319810bf1.png)'
  prefs: []
  type: TYPE_IMG
- en: Chat history after asking the second question. Note that the response is also
    included in the conversation at this point. (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs look at what else is the solution tracking besides the chat history:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/396886e2b0b33096284054126381fe35.png)'
  prefs: []
  type: TYPE_IMG
- en: Outputs, other than chat history, after asking the second question. (Image by
    the author)
  prefs: []
  type: TYPE_NORMAL
- en: The solution internally uses the LLM to first convert the question *‚ÄúTell me
    more about the second book‚Äù* to *‚ÄúWhat additional information can you provide
    about ‚ÄòThe Day of the Jackal‚Äô by Frederick Forsyth?‚Äù*. Armed with this question,
    the solution is able to search the vector DB for any relevant information and
    retrieves The Day of the Jackal chunk first this time. Though note that some other
    irrelevant search results about other books are also included.
  prefs: []
  type: TYPE_NORMAL
- en: Quick optional sidebar discussing potential issues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Potential Issue #1 ‚Äî Poor Standalone Question Generation:** In my tests,
    the chat solution wasn‚Äôt always successful in generating a good standalone question,
    until the question generator prompt was tweaked. For example, for a follow-up
    question, ‚ÄúTell me about the second book‚Äù, more often than not the generated follow-up
    question was ‚ÄúWhat can you tell me about the second book?‚Äù which is not particularly
    meaningful in itself and led to random search results and consequently a seemingly
    random generated response from the LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Potential Issue #2 ‚Äî Changing Search Results Between Original & Follow-up
    Questions:** It is noteworthy that even though the second generated question specifically
    names the book of interest, the returned results from the vector DB search include
    other book results, and more importantly, these search results are different than
    from those for the original question! In this example, this change in search results
    was desirable since the question changed from ‚Äúdetective novel recommendations‚Äù
    to a particular novel. However, when a user is asking follow-up questions intending
    to dig deeper into a topic, variations in question formulation or LLM-generated
    standalone question may lead to different search results or a different ranking
    of search results, which might *not* be desirable.'
  prefs: []
  type: TYPE_NORMAL
- en: This issue is possibly mitigated automatically, at least to a degree, by doing
    a broader initial search from the vector DB ‚Äî returning many results instead of
    just 4‚Äì5 as with our example ‚Äî and re-ranking them to ensure that the most relevant
    results bubble up to the top and are always sent to the LLM to generate the final
    response (see [Cohere‚Äôs ‚ÄòReranking‚Äô](https://docs.cohere.com/docs/reranking)).
    Besides, it should be relatively straightforward for an app to recognize that
    search results have changed. It might be possible to apply some heuristics around
    whether the degree of change in search results (measured by ranking and overlap
    metrics), and the degree of change in the question (measured by distance metrics
    such as cosine similarity) are at parity. At least in cases where there are unexpected
    swings in search results over chat turns, the end user could be alerted and brought
    into the loop for a closer inspection, depending on the use case criticality and
    training or sophistication of end users.
  prefs: []
  type: TYPE_NORMAL
- en: Another idea to control this behavior is to leverage the LLM to decide whether
    a follow-up question requires going to the vector DB again, or can the question
    be meaningfully answered with previously fetched results. Some use cases might
    want to generate two sets of search results and responses and let the LLM adjudicate
    between the answers, some others might be justified in passing the responsibility
    of controlling context to users by empowering them to freeze context (depending
    on the use case, user training or sophistication, and other considerations), and
    some others might simply be tolerant of changing search results over follow-up
    questions.
  prefs: []
  type: TYPE_NORMAL
- en: As you can probably tell, it is quite easy to get a basic solution working,
    but getting things just right ‚Äî that‚Äòs the hard part. The issues called out here
    is just scratching the surface. Alright, back to the main exercise ‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Add a pre-coded UI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, the chatbot‚Äôs functionality is ready. Now, we can add a nice user-interface
    to improve user experience. This is (somewhat) easily possible due to Python libraries
    such as Gradio and Streamlit, which build front-end widgets based on instructions
    written in Python. Here, we will go with Gradio to quickly create a user interface.
  prefs: []
  type: TYPE_NORMAL
- en: With dual objectives of catching anyone up in case they were not able to execute
    the code so far, and also to demonstrate some variations in getting to the same
    place, the following two blocks of code are self-contained and can be run in a
    completely new Colab notebook to generate the complete chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Before running the next set of code to render the chatbot UI, note that when
    rendered through Colab, the app becomes publicly accessible for 3 days for anyone
    with the link (the link is provided in the Colab notebook cell output). In theory,
    the app can be kept private by changing the last line in the code to *demo.launch(share=False)*,
    but I was not able to get the app to work at all then. Instead, I prefer running
    it in ‚Äòdebug‚Äô mode in Colab, so the Colab cell stays ‚Äúrunning‚Äù until stopped,
    which then terminates the chatbot. Alternatively, run the code shown below in
    a different Colab cell to terminate the chatbot and delete content loaded to the
    Chroma vector DB within Colab.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Below is the code to run the chatbot as an app. Most of this code reuses the
    code up to this point of the article, so should seem familiar. Note that there
    are some differences in the code below compared to the code earlier, including
    but not limited to there being no memory management using LangChain‚Äôs ‚Äòtoken‚Äô
    memory object that we used before. This means as the conversation continues for
    a while, the history will become too long to pass in to the language model‚Äôs context,
    and the app will need a restart.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'You can play around with the app by giving it a different URL to load content
    from. This goes without saying: this is not a production-grade app, and was only
    created to demonstrate building blocks of RAG-based GenAI solutions. This is an
    early prototype at best and if it were to be converted into a regular product,
    most of the software engineering work will lie ahead.'
  prefs: []
  type: TYPE_NORMAL
- en: Revisiting FAQs from the Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the context and knowledge of the chatbot we created, let‚Äôs revisit some
    of the questions posed in the *Introduction* and dive just a little bit deeper.
  prefs: []
  type: TYPE_NORMAL
- en: '*Is our use case a good fit for LLM-powered solutions? Perhaps traditional
    analytics, supervised machine learning, or another approach is a better fit?*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LLMs are good at ‚Äúunderstanding‚Äù language related tasks as well as following
    instructions. So, the early use cases for LLMs have been question-answering, summarization,
    generation (text in this case), enabling better meaning-based search, sentiment
    analysis, coding, etc. LLMs have also picked up the ability to problem-solve and
    reason. For example, LLMs can act as automated graders for students‚Äô assignments
    if you provide it with an answer key, or sometimes even without.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: On the other hand, predictions or classifications based on a large number of
    data points, multi-armed bandit experiments for marketing optimization, recommender
    systems, reinforcement learning systems (Roomba, Nest thermostat, optimizing power
    consumption or inventory levels, etc.) are the forte of other types of analytics
    or machine learning ‚Ä¶ at least for the time being. Hybrid approaches where traditional
    ML models feed information to LLMs and vice-versa should also be considered as
    a holistic solution to a core business problem.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*If LLMs are the way to go, can our use case be addressed by an off-the-shelf
    product (say, ChatGPT Enterprise) now or in the near-future? Classic build-vs-buy
    decision.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Services and products offered by OpenAI, AWS, and others are going to grow broader,
    better, and possibly cheaper. For example, ChatGPT let‚Äôs users upload their files
    for analysis, Bing Chat and Google‚Äôs Bard let you point to external websites for
    question answering, AWS Kendra brings semantic search to an enterprise‚Äôs information,
    Microsoft Copilot lets you bring LLMs to Word, Powerpoint, Excel, etc. For the
    same reason that companies do not build their own operating systems or their own
    databases, companies should think about whether they need to build AI solutions
    that might possibly get obsolete by current and future off-the-shelf products.
    On the other hand, if a company‚Äôs use cases are specific, or restrictive in some
    sense ‚Äî such as not being able to send their sensitive data to any vendor due
    to sensitivity, or due to regulatory guidance ‚Äî then, it might be required to
    build generative AI products within the company to address use cases. Products
    that use an LLM‚Äôs reasoning ability but undertake tasks or generate outputs too
    distinct than the vended solutions might warrant in-house development. For example,
    a system that is monitoring the factory floor, or a manufacturing processes, or
    inventory levels, etc. might warrant custom development, especially if there are
    no good domain-specific product offerings. Also, if the application requires specialized
    domain knowledge, then an LLM fine-tuned on domain-specific data is likely going
    to outperform a general-purpose LLM from OpenAI, and in-house development could
    be considered.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*What are the different building blocks of our LLM-powered product? Which of
    these are commoditized, and which are likely to need more time to build and test?*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The high-level building blocks for a RAG solution like we built are the data
    pipeline, vector database, retrieval, generation, and the LLM of course. There
    are lots of great choices for LLMs and for vector databases. The data pipelines,
    retrieval, prompt engineering for generation will require some good old-fashioned
    data-scienc*y* experimentation to optimize for a use case. Once an initial solution
    is in place, productionization will require a lot of work, which is true of any
    data science / machine learning pipeline. This talk offers hard-earned wisdom
    on the topic of productionization: [LLMs in Production: Learning from Experience,
    Dr. Waleed Kadous, Chief Scientist, AnyScale](https://youtu.be/xa7k9MUeIdk?si=LQizYwFt4m-XOYpk)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*How do we measure the performance of our solution? What levers are available
    to improve the quality of outputs from our product?* As with any technology (or
    non-technology ) solution, business impact should be measured using leading KPIs.
    Some direct metrics being difficult to measure get replaced by surrogate metrics
    such as average number of daily users (DAU) and other product metrics.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Business metrics should be complemented with technical metrics evaluating the
    performance of the RAG solution. The overall quality of the response ‚Äî how good
    is the system‚Äôs response compared to the best generated response from an expert
    human or a state of the art frontier model such as GPT-4 (currently) could be
    evaluated using a range of metrics that test for informativeness, factualness,
    relevance, toxicity, etc. It will help to delve deeper into performance of individual
    components to iterate and improve each: the quality of information that the solution
    will use as context, retrieval, and generation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: i. How good is the **data quality**? If data available to an organization and
    stored in the vector database doesn‚Äôt have the required information, no human
    or LLM can conjure a response based on it.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ii. How good is the **retrieval**? Assuming the information is available, how
    successful is the system in finding and fetching the relevant bits?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: iii. How good is the **generation (i.e. synthesis)**? Assuming the information
    is available, retrieved correctly, and passed on to the LLM to generate the final
    response, is the LLM using the information as expected?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Each of these areas could be evaluated separately and improved concurrently
    to improve the overall output.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Improve Data quality:** Companies need to work on data pipelines to feed
    good information into the system. If there is bad quality of information in the
    vector database, having great LLMs will not improve the outputs drastically. In
    addition to employing traditional data quality and governance frameworks, companies
    should also consider improve the quality of chunking (more on this in the next
    question‚Äôs response).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Improve Retrieval:** Retrieval could be improved through trying different
    retrieval algorithms, semantic re-ranking, hybrid search combining semantic search
    and keyword search, and fine-tuning embeddings. Improving instructions / prompt
    should also contribute to improving the quality of retrieval.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Improve Generation:** As LLMs improve , the synthesis step will improve,
    and possibly retrieval too due to improved embedding models. Another option assuming
    resource & time availability is fine-tuning, which can improve the quality of
    responses for specific domains and tasks. For example, a smaller fine-tuned model
    on diagnosing specific medical conditions might be better at the task than a general
    purpose model like GPT-4, while also being faster and cheaper.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Is our data quality acceptable for the use case? Are we organizing our data
    correctly, and passing relevant data to the LLM?*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data quality can be assessed with the traditional data quality & governance
    frameworks. Additionally for LLM-powered solutions, the information required by
    LLMs to answer user questions or carry out tasks should be available within the
    data available to the solution.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Assuming the data is available, the data should be chunked appropriately for
    the use case and LLM being used. Chunks shouldn‚Äôt be too broad to dilute coherence
    with respect to a specific topic or too narrow to not include all the necessary
    context. Data shouldn‚Äôt be split into chunks in a way that necessary context is
    split between chunks and meaningless when separated in this way. For example,
    if the two sentences below are split into two chunks,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*‚ÄúOpenAI‚Äôs GPT-3.5 is a powerful LLM. It can support context sizes up to 16K
    tokens.‚Äù*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A question such as ‚ÄúTell me about GPT 3.5 LLM‚Äù may not fetch the 2nd sentence
    as it doesn‚Äôt mention GPT 3.5 and that information might not be provided to a
    user, just by virtue of suboptimal chunking. More dangerously, the sentence might
    still be fetched when asked about a completely different LLM due to semantic association
    of context sizes and tokens with LLMs, and the response might be that other model
    in focus has context sizes up to 16K, which would be factually inaccurate. This
    is a simplified example unlikely to be encountered in production, but the idea
    holds.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: One possible approach to improve quality of chunks is to use context-aware text
    splitting, such as splitting by logical sections (as in our example of the book
    list). If any logical chunk is too big ‚Äî such as Wikipedia pages on particular
    topics would be quite lengthy, they could be split further by logical sections
    or by semantic units such as by paragraphs, with a meaningful overlap between
    chunks, as well as ensuring the overall metadata and chunk specific metadata is
    passed to the LLM.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Can we be confident that the LLM‚Äôs responses will always be factually accurate.
    That is, will our solution ‚Äòhallucinate‚Äô when generating responses once in a while?*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A key selling point of RAG is to drive factuality. GPT 3.5 and GPT-4 are good
    at following this instruction: ‚Äúrespond only from the provided context or say
    ‚Äòthe question cannot be answered based on the information provided‚Äô‚Äù. This is
    hypothesized to be due to a lot of reinforcement learning from human feedback
    (RLHF) conducted by OpenAI. As a corollary, other LLMs might not currently be
    as good at following instructions. For a production application, especially an
    external facing one, it will be prudent to conduct a lot of testing aimed at validating
    that the generated output is faithful to the available context retrieved from
    the vector database, even though the LLM believes it to be the case. Approaches
    range from manual tests on samples, to using a powerful model such as GPT-4 to
    test samples of retrieved context and generated responses by other models, to
    using services and products such as [Galileo](https://www.rungalileo.io/) which
    focus on detecting LLM hallucinations in real-time.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Had you known all of this 11 months ago, it would have justified a demonstration
    with the CEO of your company. Possibly even a TED talk to a wider audience. Today,
    this has become a part of AI literacy baseline, especially if you are involved
    in delivery of generative AI products. Hopefully, you‚Äôre fairly caught up due
    to this exercise! üëç
  prefs: []
  type: TYPE_NORMAL
- en: A few closing thoughts,
  prefs: []
  type: TYPE_NORMAL
- en: There is serious promise in the technology ‚Äî how many other technologies can
    ‚Äúthink‚Äù to this degree, and can be used as ‚Äúreasoning engines‚Äù (in the words of
    Dr. Andrew Ng [here](https://learn.deeplearning.ai/langchain/lesson/7/agents)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While frontier models (currently, GPT-4) will continue to advance, open source
    models and their domain-specific and task specific fine-tuned variants will be
    competitive on numerous tasks and will find many applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For better or worse, this cutting edge technology that took millions (hundreds
    of millions?) of dollars to develop is available for free ‚Äî you could fill a form
    and download Meta‚Äôs capable Llama2 model with a very permissive license. Nearly
    300,000 baseline LLMs or their fine-tuned variants are on HuggingFace‚Äôs model
    hub. Hardware is also commoditized.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI models are now capable of being aware of and using ‚Äútools‚Äù (functions,
    APIs, etc.), letting solutions interface with not just humans and databases, but
    with other programs. LangChain and other packages have already demonstrated using
    LLMs as the ‚Äúbrain‚Äù for autonomous agents that can accept input, decide what action
    to take, and follow through, repeating these steps until the agent reaches its
    goal. Our simple chatbot used two LLM calls in a deterministic sequence ‚Äî generate
    standalone question, and synthesize search results into a coherent natural language
    response. Imagine what hundreds of calls to rapidly evolving LLMs with agentic
    autonomy can achieve!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These rapid advancements are a result of tremendous momentum around GenAI, and
    it will proliferate enterprises, and day-to-day life through our devices. First
    in simpler ways, but later on in increasingly sophisticated applications that
    leverage the reasoning and decision-making capability of the technology, blending
    it with traditional AI.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, now is a great time to get involved as the playing field is fairly
    level, at least for applying this technology ‚Äî everyone is learning about this
    at more or less the same time since the ChatGPT boom in Dec 2022\. Things are
    of course different on the R&D side, with Big Tech companies that have spent years,
    and billions of dollars in developing this technology. Regardless, to build more
    sophisticated solutions later, it‚Äôs the perfect time to get started now!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**LangChain:** [Deeplearning.ai](http://deeplearning.ai/) course: [LangChain:
    Chat with Your Data](https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/)
    | LangChain [documentation](https://python.langchain.com/docs/get_started/introduction)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gradio:** [Deeplearning.ai](https://www.deeplearning.ai/) course - [Building
    Generative AI Applications with Gradio](https://www.deeplearning.ai/short-courses/building-generative-ai-applications-with-gradio/)
    | Gradio documentation and [guides](https://www.gradio.app/guides)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I have found [Shawhin Talebi](https://medium.com/u/f3998e1cd186?source=post_page-----6ee6ad94e058--------------------------------)‚Äôs
    articles very instructive. See [Cracking Open the OpenAI (Python) API](/cracking-open-the-openai-python-api-230e4cae7971),
    [Cracking Open the Hugging Face Transformers Library](/cracking-open-the-hugging-face-transformers-library-350aa0ef0161),
    and other recent articles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LLMs in Production: Learning from Experience, Dr. Waleed Kadous, Chief Scientist,
    AnyScale](https://youtu.be/xa7k9MUeIdk?si=LQizYwFt4m-XOYpk)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This talk by Jerry Liu, co-founfer of LlamaIndex outlines various approaches
    for output evaluation: [Practical Data Considerations for Building Production-Ready
    LLM Applications](https://youtu.be/xbeFAZl3uCk?si=XBpo6cWt0Z9P9v_w)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
