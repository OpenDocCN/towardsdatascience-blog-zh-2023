- en: 'From Python to Julia: Feature Engineering and ML'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/from-python-to-julia-feature-engineering-and-ml-d55e8321f888?source=collection_archive---------11-----------------------#2023-06-26](https://towardsdatascience.com/from-python-to-julia-feature-engineering-and-ml-d55e8321f888?source=collection_archive---------11-----------------------#2023-06-26)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/f7da00682658a8bdd533781a67403eaf.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [CardMapr.nl](https://unsplash.com/ja/@cardmapr?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/credit-cards?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: A Julia-based approach to building a fraud-detection model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@wangshenghao1993?source=post_page-----d55e8321f888--------------------------------)[![Wang
    Shenghao](../Images/c59ca7f4fc77ca81f6b670ea5435ac19.png)](https://medium.com/@wangshenghao1993?source=post_page-----d55e8321f888--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d55e8321f888--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d55e8321f888--------------------------------)
    [Wang Shenghao](https://medium.com/@wangshenghao1993?source=post_page-----d55e8321f888--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F75535ec0f14c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-python-to-julia-feature-engineering-and-ml-d55e8321f888&user=Wang+Shenghao&userId=75535ec0f14c&source=post_page-75535ec0f14c----d55e8321f888---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d55e8321f888--------------------------------)
    ·6 min read·Jun 26, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd55e8321f888&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-python-to-julia-feature-engineering-and-ml-d55e8321f888&user=Wang+Shenghao&userId=75535ec0f14c&source=-----d55e8321f888---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd55e8321f888&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-python-to-julia-feature-engineering-and-ml-d55e8321f888&source=-----d55e8321f888---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: This is part 2 in my two part series on getting started with Julia for applied
    data science. In [the first article](https://medium.com/towards-data-science/from-python-to-julia-basic-data-manipulation-and-eda-51171b34f685),
    we went through a few examples of simple data manipulation and conducting exploratory
    data analysis with Julia. In this blog, we will carry on the task of building
    a fraud detection model to identify fraudulent transactions.
  prefs: []
  type: TYPE_NORMAL
- en: To recap briefly, we used a [credit card fraud detection dataset](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)
    obtained from Kaggle. The dataset contains 30 features including transaction time,
    amount, and 28 principal component features obtained with PCA. Below is a screenshot
    of the first 5 instances of the dataset, loaded as a dataframe in Julia. Note
    that the transaction time feature records the elapsed time (in second) between
    the current transaction and the first transaction in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before training the fraud detection model, let’s prepare the data ready for
    the model to consume. Since the main purpose of this blog is to introduce Julia,
    we are not going to perform any feature selection or feature synthesis here.
  prefs: []
  type: TYPE_NORMAL
- en: Data splitting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When training a classification model, the data is typically split for training
    and test in a stratified manner. The main purpose is to maintain the distribution
    of the data with respect to the target class variable in both the training and
    test data. This is especially necessary when we are working with a dataset with
    extreme imbalance. The [MLDataUtils](https://mldatautilsjl.readthedocs.io/en/latest/index.html#)
    package in Julia provides a series of preprocessing functions including data splitting,
    label encoding, and feature normalisation. The following code shows how to perform
    stratified sampling using the `stratifiedobs` function from [MLDataUtils](https://mldatautilsjl.readthedocs.io/en/latest/index.html#).
    A random seed can be set so that the same data split can be reproduced.
  prefs: []
  type: TYPE_NORMAL
- en: Split data for training and test — Julia implementation
  prefs: []
  type: TYPE_NORMAL
- en: The usage of the stratifiedobs function is quite similar to the train_test_split
    function from the sklearn library in Python. Take note that the input features
    X need to go through twice of transpose to restore the original dimensions of
    the dataset. This can be confusing for a Julia novice like me. I’m not sure why
    the author of [MLDataUtils](https://mldatautilsjl.readthedocs.io/en/latest/index.html#)
    developed the function in this way.
  prefs: []
  type: TYPE_NORMAL
- en: The equivalent Python sklearn implementation is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3e9ea400baf6f37072dd3dbc1e28af74.png)'
  prefs: []
  type: TYPE_IMG
- en: Split data for training and test — Python implementation (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Feature scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a recommended practice in machine learning, feature scaling brings the features
    to the same or similar ranges of values or distribution. Feature scaling helps
    improve the speed of convergence when training neural networks, and also avoids
    the domination of any individual feature during training.
  prefs: []
  type: TYPE_NORMAL
- en: Although we are not training a neural network model in this work, I’d still
    like to find out how feature scaling can be performed in Julia. Unfortunately,
    I could not find a Julia library which provides both functions of fitting scaler
    and transforming features. The [feature normalization functions](https://mldatautilsjl.readthedocs.io/en/latest/data/feature.html)
    provided in the MLDataUtils package allow users to derive the mean and standard
    deviation of the features, but they cannot be easily applied on the training /
    test datasets to transform the features. Since the mean and standard deviation
    of the features can be easily calculated in Julia, we can implement the process
    of standard scaling manually.
  prefs: []
  type: TYPE_NORMAL
- en: The following code creates a copy of X_train and X_test, and calculates the
    mean and standard deviation of each feature in a loop.
  prefs: []
  type: TYPE_NORMAL
- en: Standadize features — Julia implementation
  prefs: []
  type: TYPE_NORMAL
- en: The transformed and original features are shown as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9608d482d04912956f856e6f1dc422d7.png)'
  prefs: []
  type: TYPE_IMG
- en: Scaled features vs. orginal features — Julia implementation (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: In Python, sklearn provides various options for feature scaling, including normalization
    and standardization. By declaring a feature scaler, the scaling can be done with
    two lines of code. The following code gives an example of using a [RobustScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/845227f504791224f4e127212246e518.png)'
  prefs: []
  type: TYPE_IMG
- en: Perform robust scaling to the features — Python implementation (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Oversampling (by PyCall)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A fraud detection dataset is typically severely imbalanced. For instance, the
    ratio of negative over positive examples of our dataset is above 500:1\. Since
    obtaining more data points is not possible, undersampling will result in a huge
    loss of data points from the majority class, oversampling becomes the best option
    in this case. Here I apply the popular SMOTE method to create synthetic examples
    for the positive class.
  prefs: []
  type: TYPE_NORMAL
- en: Currently, there is no working Julia library which provides implementation of
    SMOTE. The [ClassImbalance](https://github.com/bcbi/ClassImbalance.jl) package
    has not been maintained for two years, and cannot be used with the recent versions
    of Julia. Fortunately, Julia allows us to call the ready-to-use Python packages
    using a wrapper library called [PyCall](https://github.com/JuliaPy/PyCall.jl).
  prefs: []
  type: TYPE_NORMAL
- en: To import a Python library to Julia, we need to install PyCall and specify the
    PYTHONPATH as an environment variable. I tried create a Python virtual environment
    here but it did not work out. Due to some reason, Julia cannot recognize the python
    path of the virtual environment. This is why I have to specify the system default
    python path. After this, we can import the Python implementation of SMOTE, which
    is provided in the [imbalanced-learn](https://github.com/scikit-learn-contrib/imbalanced-learn)
    library. The `pyimport` function provided by PyCall can be used to import the
    Python libraries in Julia. The following code shows how to activate PyCall and
    ask for help from Python in a Julia kernel.
  prefs: []
  type: TYPE_NORMAL
- en: Upsample training data with SMOTE — Julia implementation
  prefs: []
  type: TYPE_NORMAL
- en: The equivalent Python implementation is as follows. We can see the fit_resample
    function is used in the same way in Julia.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9332c8f1e4c1ca50714575926017ca9d.png)'
  prefs: []
  type: TYPE_IMG
- en: Upsample training data with SMOTE — Python implementation (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Model Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we reach the stage of model training. We will be training a binary classifier,
    which can be done with a variety of ML algorithms, including logistic regression,
    decision tree, and neural networks. Currently, the resources for ML in Julia are
    distributed across multiple Julia libraries. Let me list down a few most popular
    options with their specialised set of models.
  prefs: []
  type: TYPE_NORMAL
- en: '[MLJ](https://github.com/alan-turing-institute/MLJ.jl): traditional ML algorithms'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ScikitLearn](https://github.com/cstjean/ScikitLearn.jl): traditional ML algorithms'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Mocha](https://github.com/pluskid/Mocha.jl): neural networks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Flux](https://github.com/FluxML/Flux.jl): neural networks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here I’m going to choose [XGBoost](https://github.com/dmlc/XGBoost.jl), considering
    its simplicity and superior performance over the traditional regression and classification
    problems. The process of training a XGBoost model in Julia is the same as that
    of Python, albeit there’s some minor difference in syntax.
  prefs: []
  type: TYPE_NORMAL
- en: Train a fraud detection model with XGBoost — Julia implementation
  prefs: []
  type: TYPE_NORMAL
- en: The equivalent Python implementation is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f71e95666783f20e286643e7ac84b985.png)'
  prefs: []
  type: TYPE_IMG
- en: Train a fraud detection model with XGBoost — Python implementation (Image by
    author)
  prefs: []
  type: TYPE_NORMAL
- en: Model Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, let’s look at how our model performs by looking at the precision, recall
    obtained on the test data, as well as the time spent on training the model. In
    Julia, the precision, recall metrics can be calculated using the [EvalMetrics](https://github.com/VaclavMacha/EvalMetrics.jl)
    library. An alternative package is [MLJBase](https://github.com/JuliaAI/MLJBase.jl)
    for the same purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Make prediction and calculate metrics — Julia implementation
  prefs: []
  type: TYPE_NORMAL
- en: In Python, we can employ sklearn to calculate the metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/82b89478a38a1dc21b83edcb0225f687.png)'
  prefs: []
  type: TYPE_IMG
- en: Make prediction and calculate metrics — Python implementation (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: So which is the winner between Julia and Python? To make a fair comparison,
    the two models were both trained with the default hyperparameters, and learning
    rate = 0.1, no. of estimators = 1000\. The performance metrics are summarised
    in the following table.
  prefs: []
  type: TYPE_NORMAL
- en: It can be observed that the Julia model achieves a better precision and recall
    with a slightly longer training time. Since the [XGBoost](https://github.com/dmlc/xgboost)
    library used for training the Python model is written in C++ under the hood, whereas
    the [Julia XGBoost](https://github.com/dmlc/XGBoost.jl) library is completely
    written in Julia, Julia does run as fast as C++, just as it claimed!
  prefs: []
  type: TYPE_NORMAL
- en: 'The hardware used for the aforementioned test: 11th Gen Intel® Core™ i7–1165G7
    @ 2.80GHz — 4 cores.'
  prefs: []
  type: TYPE_NORMAL
- en: Jupyter notebook can be found on [Github](https://github.com/shenghaowang/shenghao-blogs-work/tree/main/julia-traditional-ml).
  prefs: []
  type: TYPE_NORMAL
- en: Takeways
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I’d like to end this series with a summary of the mentioned Julia libraries
    for different data science tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the lack of community support, the usability of Julia cannot be compared
    to Python at the moment. Nonetheless, given its superior performance, Julia still
    has a great potential in future.
  prefs: []
  type: TYPE_NORMAL
- en: '**References**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Group of ULB (Université Libre de Bruxelles)](http://mlg.ulb.ac.be/).
    (no date). *Credit Card Fraud Detection* [Dataset]. [H](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)i
    i (Database Contents License (DbCL))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Akshay Gupta. May 13, 2021\. *Start Machine Learning With Julia: Top Julia
    Libraries for Machine Learning*. [https://www.analyticsvidhya.com/blog/2021/05/top-julia-machine-learning-libraries/](https://www.analyticsvidhya.com/blog/2021/05/top-julia-machine-learning-libraries/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
