- en: Topic Modeling with Llama 2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Llama 2进行主题建模
- en: 原文：[https://towardsdatascience.com/topic-modeling-with-llama-2-85177d01e174?source=collection_archive---------0-----------------------#2023-08-22](https://towardsdatascience.com/topic-modeling-with-llama-2-85177d01e174?source=collection_archive---------0-----------------------#2023-08-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/topic-modeling-with-llama-2-85177d01e174?source=collection_archive---------0-----------------------#2023-08-22](https://towardsdatascience.com/topic-modeling-with-llama-2-85177d01e174?source=collection_archive---------0-----------------------#2023-08-22)
- en: '![](../Images/31f5e2196198d26e5295ae70660d720c.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31f5e2196198d26e5295ae70660d720c.png)'
- en: Create easily interpretable topics with Large Language Models
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用大型语言模型创建易于解释的主题
- en: '[](https://medium.com/@maartengrootendorst?source=post_page-----85177d01e174--------------------------------)[![Maarten
    Grootendorst](../Images/58e24b9cf7e10ff1cd5ffd75a32d1a26.png)](https://medium.com/@maartengrootendorst?source=post_page-----85177d01e174--------------------------------)[](https://towardsdatascience.com/?source=post_page-----85177d01e174--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----85177d01e174--------------------------------)
    [Maarten Grootendorst](https://medium.com/@maartengrootendorst?source=post_page-----85177d01e174--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@maartengrootendorst?source=post_page-----85177d01e174--------------------------------)[![Maarten
    Grootendorst](../Images/58e24b9cf7e10ff1cd5ffd75a32d1a26.png)](https://medium.com/@maartengrootendorst?source=post_page-----85177d01e174--------------------------------)[](https://towardsdatascience.com/?source=post_page-----85177d01e174--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----85177d01e174--------------------------------)
    [Maarten Grootendorst](https://medium.com/@maartengrootendorst?source=post_page-----85177d01e174--------------------------------)'
- en: ·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F22405c3b2875&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-llama-2-85177d01e174&user=Maarten+Grootendorst&userId=22405c3b2875&source=post_page-22405c3b2875----85177d01e174---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----85177d01e174--------------------------------)
    ·12 min read·Aug 22, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F85177d01e174&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-llama-2-85177d01e174&user=Maarten+Grootendorst&userId=22405c3b2875&source=-----85177d01e174---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F22405c3b2875&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-llama-2-85177d01e174&user=Maarten+Grootendorst&userId=22405c3b2875&source=post_page-22405c3b2875----85177d01e174---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----85177d01e174--------------------------------)
    ·12分钟阅读·2023年8月22日'
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F85177d01e174&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-llama-2-85177d01e174&source=-----85177d01e174---------------------bookmark_footer-----------)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F85177d01e174&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-llama-2-85177d01e174&source=-----85177d01e174---------------------bookmark_footer-----------)'
- en: With the advent of **Llama 2**, running strong LLMs locally has become more
    and more a reality. Its accuracy approaches OpenAI’s GPT-3.5, which serves well
    for many use cases.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着**Llama 2**的出现，强大的LLM本地运行变得越来越现实。其准确性接近OpenAI的GPT-3.5，这在许多应用场景中表现良好。
- en: In this article, we will explore how we can use Llama2 for Topic Modeling without
    the need to pass every single document to the model. Instead, we are going to
    leverage [**BERTopic**](https://github.com/MaartenGr/BERTopic), a modular topic
    modeling technique that can use any LLM for fine-tuning topic representations.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将探讨如何使用**Llama2**进行主题建模，而无需将每一个文档都传递给模型。相反，我们将利用[**BERTopic**](https://github.com/MaartenGr/BERTopic)，这是一种模块化的主题建模技术，可以使用任何LLM来微调主题表示。
- en: 'BERTopic works rather straightforward. It consists of 5 sequential steps:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: BERTopic的工作流程相当简单。它包括5个连续步骤：
- en: Embedding documents
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 嵌入文档
- en: Reducing the dimensionality of embeddings
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 降低嵌入的维度
- en: Cluster reduced embeddings
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 聚类减少的嵌入
- en: Tokenize documents per cluster
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个聚类的文档进行标记化
- en: Extract best-representing words per cluster
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取每个聚类中最具代表性的词汇
- en: '![](../Images/63a385b329d173ae7fc70aa9fa1b4182.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63a385b329d173ae7fc70aa9fa1b4182.png)'
- en: The 5 main steps of BERTopic.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: BERTopic的5个主要步骤。
- en: However, with the rise of LLMs like **Llama 2**, we can do much better than
    a bunch of independent words per topic. It is computationally not feasible to
    pass all documents to Llama 2 directly and have it analyze them. We can employ
    vector databases for search…
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着像**Llama 2**这样的语言模型的兴起，我们可以比每个主题的一堆独立词汇做得更好。将所有文档直接传递给Llama 2并让它分析这些文档在计算上是不可行的。我们可以使用向量数据库进行搜索……
