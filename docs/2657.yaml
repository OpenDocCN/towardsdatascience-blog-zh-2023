- en: Topic Modeling with Llama 2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/topic-modeling-with-llama-2-85177d01e174?source=collection_archive---------0-----------------------#2023-08-22](https://towardsdatascience.com/topic-modeling-with-llama-2-85177d01e174?source=collection_archive---------0-----------------------#2023-08-22)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/31f5e2196198d26e5295ae70660d720c.png)'
  prefs: []
  type: TYPE_IMG
- en: Create easily interpretable topics with Large Language Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@maartengrootendorst?source=post_page-----85177d01e174--------------------------------)[![Maarten
    Grootendorst](../Images/58e24b9cf7e10ff1cd5ffd75a32d1a26.png)](https://medium.com/@maartengrootendorst?source=post_page-----85177d01e174--------------------------------)[](https://towardsdatascience.com/?source=post_page-----85177d01e174--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----85177d01e174--------------------------------)
    [Maarten Grootendorst](https://medium.com/@maartengrootendorst?source=post_page-----85177d01e174--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F22405c3b2875&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-llama-2-85177d01e174&user=Maarten+Grootendorst&userId=22405c3b2875&source=post_page-22405c3b2875----85177d01e174---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----85177d01e174--------------------------------)
    ·12 min read·Aug 22, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F85177d01e174&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-llama-2-85177d01e174&user=Maarten+Grootendorst&userId=22405c3b2875&source=-----85177d01e174---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F85177d01e174&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-llama-2-85177d01e174&source=-----85177d01e174---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: With the advent of **Llama 2**, running strong LLMs locally has become more
    and more a reality. Its accuracy approaches OpenAI’s GPT-3.5, which serves well
    for many use cases.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will explore how we can use Llama2 for Topic Modeling without
    the need to pass every single document to the model. Instead, we are going to
    leverage [**BERTopic**](https://github.com/MaartenGr/BERTopic), a modular topic
    modeling technique that can use any LLM for fine-tuning topic representations.
  prefs: []
  type: TYPE_NORMAL
- en: 'BERTopic works rather straightforward. It consists of 5 sequential steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Embedding documents
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reducing the dimensionality of embeddings
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cluster reduced embeddings
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tokenize documents per cluster
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract best-representing words per cluster
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/63a385b329d173ae7fc70aa9fa1b4182.png)'
  prefs: []
  type: TYPE_IMG
- en: The 5 main steps of BERTopic.
  prefs: []
  type: TYPE_NORMAL
- en: However, with the rise of LLMs like **Llama 2**, we can do much better than
    a bunch of independent words per topic. It is computationally not feasible to
    pass all documents to Llama 2 directly and have it analyze them. We can employ
    vector databases for search…
  prefs: []
  type: TYPE_NORMAL
