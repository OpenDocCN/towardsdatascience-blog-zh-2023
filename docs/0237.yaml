- en: 'Gradient Descent: Optimisation and Initialisation Explained'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/gradient-descent-optimisation-and-initialisation-explained-1f945e9943bd?source=collection_archive---------6-----------------------#2023-01-14](https://towardsdatascience.com/gradient-descent-optimisation-and-initialisation-explained-1f945e9943bd?source=collection_archive---------6-----------------------#2023-01-14)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A high-level introduction to optimisation in 7 minutes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://j-w-mcgowan18.medium.com/?source=post_page-----1f945e9943bd--------------------------------)[![Jamie
    McGowan](../Images/1150476f58297eb7e45cd3942a7a072b.png)](https://j-w-mcgowan18.medium.com/?source=post_page-----1f945e9943bd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1f945e9943bd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1f945e9943bd--------------------------------)
    [Jamie McGowan](https://j-w-mcgowan18.medium.com/?source=post_page-----1f945e9943bd--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F685229ed4b15&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-optimisation-and-initialisation-explained-1f945e9943bd&user=Jamie+McGowan&userId=685229ed4b15&source=post_page-685229ed4b15----1f945e9943bd---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1f945e9943bd--------------------------------)
    ·7 min read·Jan 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1f945e9943bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-optimisation-and-initialisation-explained-1f945e9943bd&user=Jamie+McGowan&userId=685229ed4b15&source=-----1f945e9943bd---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1f945e9943bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-optimisation-and-initialisation-explained-1f945e9943bd&source=-----1f945e9943bd---------------------bookmark_footer-----------)![](../Images/921f9e313e7fe2989178711be85d90c4.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [vackground.com](https://unsplash.com/@vackground?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Training a deep learning model involves taking a set of model **parameters**
    and shifting these towards some **optimum set of values**. The optimum set of
    values is defined as the point at which the model becomes the best version of
    itself with respect to performing some task.
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, this can be thought of as when we learn a new skill. For example,
    if you decide to take up a new sport, the likelihood is that you’ll be pretty
    bad the first time you play (disregarding any child prodigies of course).
  prefs: []
  type: TYPE_NORMAL
- en: However, over time you will improve and learn to shift your own parameters (in
    your brain) towards some optimum value, with respect to playing this sport.
  prefs: []
  type: TYPE_NORMAL
- en: How do these parameters move?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s imagine we have a **metric value** which defines how **bad** we are at
    a sport. The *higher* this value is, the *worse* we are and the *lower* the value
    is, the *better* we are. Sort of like a handicap in Golf.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b23180426f529c6d3cb08dd99e169a45.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Steven Shircliff](https://unsplash.com/@steveshirc?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: We can further imagine that shifting these parameters will have some effect
    on this metric i.e. as we move towards the optimum set of parameters, the metric
    gets **lower** (we get better at the task).
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully this makes sense… but if it doesn’t then don’t worry! We’ll take a
    look at a diagram to try and explain this situation.
  prefs: []
  type: TYPE_NORMAL
- en: Visualising Optimisation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/37afabde1f3252e67ee2fe059a57729a.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram showing a high-level description of optimisation for being better at
    Golf. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Take the diagram above, our golf handicap is at **point A** (it’s pretty bad
    — beginner level stuff), this is where we will begin our journey to Tiger Woods
    level!
  prefs: []
  type: TYPE_NORMAL
- en: Which way should we move to reach point B (the professional level of Golfing)?
  prefs: []
  type: TYPE_NORMAL
- en: To the left you say? **Correct!**
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically, this involves finding the **gradient** at point A and moving
    in the direction of *steepest descent*.
  prefs: []
  type: TYPE_NORMAL
- en: '**“Wait, wait… remind me what gradients are”**'
  prefs: []
  type: TYPE_NORMAL
- en: The gradient defines the steepest rate of change of a function.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dd65064ca61f3c296222041a51a48ee3.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram illustrating the locality of gradient estimations at different points
    along a function. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Since this gradient is only calculated **locally** i.e. the gradient at point
    A is only correct *at* point A, so we don’t really want to trust this too far
    away from this point. For example, in the picture, points X and Y have very **different**
    gradients.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore in practice, we multiply the gradient with a **learning rate** which
    tells us how far to move towards point B. (We will come back to this later!)
  prefs: []
  type: TYPE_NORMAL
- en: It is this **locality** argument that is one of the pillars of modern gradient
    descent optimisation algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '**Locality and Learning Rates**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine you are driving a car, you know where you want to end up, but you don’t
    know the way (and you don’t have a sat nav).
  prefs: []
  type: TYPE_NORMAL
- en: The best thing you can do is rely on sign posts to **guide** your way towards
    your goal.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/58bfecacad944134ea0eb750c6a1898e.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Brendan Church](https://unsplash.com/@bdchu614?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: However, these sign posts are only valid at the point they appear. For example,
    a *continue straight* instruction will not necessarily be the correct instruction
    2 miles later down the road.
  prefs: []
  type: TYPE_NORMAL
- en: These sign posts are a bit like our gradient calculations in optimisation algorithms.
    They contain local **information** about the direction of travel (or the shape
    of the function) at that specific point along the journey.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on how cautious (or adventurous) you are, you may prefer to have sign
    posts every 200 metres, or you may be happy to include them every two miles. It
    entirely depends on what the journey is like!
  prefs: []
  type: TYPE_NORMAL
- en: For example, if it is a long straight road, we can get away with very few sign
    posts. But if it is a complicated journey with many turns, we will likely need
    a lot **more** sign posts.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0cd1b20b69e9dfad8e5e1791704311b4.png)'
  prefs: []
  type: TYPE_IMG
- en: Two plots showing the differences between small and large learning rates for
    different types of functions. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: This is how we can think about **learning rates**. If we have a function like
    the one on the left, we may be able to use a *large* learning rate (similar to
    a long straight road).
  prefs: []
  type: TYPE_NORMAL
- en: However, if we have the one on the right, this will require a *smaller* learning
    rate as we may end up **overshooting** our destination (missing a turn).
  prefs: []
  type: TYPE_NORMAL
- en: It is also worth mentioning that it is very unlikely we can journey from A to
    B with a single direction at point A (unless we’re already very close). So in
    practice, **gradient descent** tends to be an **iterative** procedure where we
    receive directions at *waypoints* along the journey (*A to B* becomes *A to C
    to D to E to B*).
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, hopefully we can build some **intuition** about how closely related
    the learning rate and number of waypoints are.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OK, so hopefully we have a good idea of what **optimisation** is trying to achieve
    and some of the concepts we need to consider!
  prefs: []
  type: TYPE_NORMAL
- en: Using the above information, we can now define the gradient descent **algorithm**.
  prefs: []
  type: TYPE_NORMAL
- en: Going back to our picture from earlier, we will label the parameters at point
    A (⍬₀) and the final parameters at point B ⍬.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first iteration from point A to the first **waypoint** (point C) we
    can write down an equation to describe the parameter update. For this we will
    need to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: The gradient of a **performance** metric L at point A (with respect to the parameters)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A learning rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The initial parameters ⍬₀
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The updated parameters ⍬₁
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/a356b10c1024f1928dd0907e20138b44.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation for a single gradient descent update. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following parameter **updates** are calculated similarly and therefore
    we are able to write down the general formula as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/89debbb0d3303509a0a9ca24d26fdb9d.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation for a general gradient descent step. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Initialisation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OK, so the title promised some talk about initialisation.
  prefs: []
  type: TYPE_NORMAL
- en: For those of you absolutely fuming that nothing has been mentioned on this so
    far… sorry about that! But hopefully this section will satisfy you!
  prefs: []
  type: TYPE_NORMAL
- en: From all the descriptions above, it is fairly easy to think about how initialisation
    fits into the picture.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6689eba5989fecb3137b52a27183e9bc.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram illustrating the differences in values with respect to initialisation.
    Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Earlier I mentioned some child prodigy? Let’s call her Pam. In terms of the
    first image in this article, this would be somewhat equivalent to Pam having some
    **initial** parameters at point P, not at point A. By the way, Pam is the one
    with the crown and the smug smile — she knows she’s good!
  prefs: []
  type: TYPE_NORMAL
- en: A high level explanation of what **initialisation** is, is where you *start*
    your optimisation from.
  prefs: []
  type: TYPE_NORMAL
- en: A good initialisation can take a lot of **pressure** off optimisation algorithms
    and a good optimisation algorithm can do the **same** for initialisation. In practice,
    a good initialisation can save hundreds of hours of compute time when training
    a deep learning model with many parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Due to this, there are many different areas of research solely focussed on developing
    better **initialisation** **techniques**. The reason this is very difficult is
    that, in essence, it’s like trying to **predict** the future without knowing much
    about the environment in which we are in.
  prefs: []
  type: TYPE_NORMAL
- en: Another reason initialisation is important to consider is related to where we
    might end up *after* optimisation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dca968c48504464e93fa2b749f8a45ba.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram showing the different outcomes of optimisation by gradient descent for
    different initialisations. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Consider this new optimisation surface above. This has many **different** **minima
    —** some of which are better than others!
  prefs: []
  type: TYPE_NORMAL
- en: Clearly in this picture, our starting point will **heavily** affect where we
    end up. This is one of the reasons why it is so important for ML practitioners
    to experiment with **different** initialisations, as well as tuning hyperparameters
    (such as the learning rate) when attempting to find the best model for a specific
    task.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this article we have walked through some high-level **explanations** related
    to gradient descent, optimisation and initialisation. We have visualised the goals
    of optimisation and initialisation, investigated these **graphically**, introduced
    the concept of a learning rate, and even wrote down a formula for gradient descent!
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully this has helped build your **intuition** behind these important concepts
    and solidified your understanding of where the formula for gradient descent comes
    from!
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading and stay tuned for more articles related to **optimisation
    techniques**!
  prefs: []
  type: TYPE_NORMAL
- en: As always, let me know of any issues or comments!
  prefs: []
  type: TYPE_NORMAL
