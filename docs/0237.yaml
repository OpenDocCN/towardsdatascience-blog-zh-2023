- en: 'Gradient Descent: Optimisation and Initialisation Explained'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降：优化与初始化解释
- en: 原文：[https://towardsdatascience.com/gradient-descent-optimisation-and-initialisation-explained-1f945e9943bd?source=collection_archive---------6-----------------------#2023-01-14](https://towardsdatascience.com/gradient-descent-optimisation-and-initialisation-explained-1f945e9943bd?source=collection_archive---------6-----------------------#2023-01-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/gradient-descent-optimisation-and-initialisation-explained-1f945e9943bd?source=collection_archive---------6-----------------------#2023-01-14](https://towardsdatascience.com/gradient-descent-optimisation-and-initialisation-explained-1f945e9943bd?source=collection_archive---------6-----------------------#2023-01-14)
- en: A high-level introduction to optimisation in 7 minutes
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高层次的优化介绍，7分钟阅读
- en: '[](https://j-w-mcgowan18.medium.com/?source=post_page-----1f945e9943bd--------------------------------)[![Jamie
    McGowan](../Images/1150476f58297eb7e45cd3942a7a072b.png)](https://j-w-mcgowan18.medium.com/?source=post_page-----1f945e9943bd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1f945e9943bd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1f945e9943bd--------------------------------)
    [Jamie McGowan](https://j-w-mcgowan18.medium.com/?source=post_page-----1f945e9943bd--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://j-w-mcgowan18.medium.com/?source=post_page-----1f945e9943bd--------------------------------)[![Jamie
    McGowan](../Images/1150476f58297eb7e45cd3942a7a072b.png)](https://j-w-mcgowan18.medium.com/?source=post_page-----1f945e9943bd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1f945e9943bd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1f945e9943bd--------------------------------)
    [Jamie McGowan](https://j-w-mcgowan18.medium.com/?source=post_page-----1f945e9943bd--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F685229ed4b15&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-optimisation-and-initialisation-explained-1f945e9943bd&user=Jamie+McGowan&userId=685229ed4b15&source=post_page-685229ed4b15----1f945e9943bd---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1f945e9943bd--------------------------------)
    ·7 min read·Jan 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1f945e9943bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-optimisation-and-initialisation-explained-1f945e9943bd&user=Jamie+McGowan&userId=685229ed4b15&source=-----1f945e9943bd---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F685229ed4b15&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-optimisation-and-initialisation-explained-1f945e9943bd&user=Jamie+McGowan&userId=685229ed4b15&source=post_page-685229ed4b15----1f945e9943bd---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1f945e9943bd--------------------------------)
    ·7 分钟阅读·2023年1月14日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1f945e9943bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-optimisation-and-initialisation-explained-1f945e9943bd&user=Jamie+McGowan&userId=685229ed4b15&source=-----1f945e9943bd---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1f945e9943bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-optimisation-and-initialisation-explained-1f945e9943bd&source=-----1f945e9943bd---------------------bookmark_footer-----------)![](../Images/921f9e313e7fe2989178711be85d90c4.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1f945e9943bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-optimisation-and-initialisation-explained-1f945e9943bd&source=-----1f945e9943bd---------------------bookmark_footer-----------)![](../Images/921f9e313e7fe2989178711be85d90c4.png)'
- en: Photo by [vackground.com](https://unsplash.com/@vackground?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [vackground.com](https://unsplash.com/@vackground?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Training a deep learning model involves taking a set of model **parameters**
    and shifting these towards some **optimum set of values**. The optimum set of
    values is defined as the point at which the model becomes the best version of
    itself with respect to performing some task.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个深度学习模型涉及一组模型**参数**的调整，将这些参数逐步逼近某个**最优值集合**。最优值集合被定义为模型在执行某个任务时达到最佳表现的点。
- en: Intuitively, this can be thought of as when we learn a new skill. For example,
    if you decide to take up a new sport, the likelihood is that you’ll be pretty
    bad the first time you play (disregarding any child prodigies of course).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，这可以被认为是我们学习新技能时的情况。例如，如果你决定尝试一项新运动，你很可能在第一次玩的时候表现得相当差劲（当然忽略任何天才儿童）。
- en: However, over time you will improve and learn to shift your own parameters (in
    your brain) towards some optimum value, with respect to playing this sport.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着时间的推移，你会提高并学习如何将你自己的参数（在大脑中）向某个最佳值转变，以改善这项运动。
- en: How do these parameters move?
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这些参数如何移动？
- en: Let’s imagine we have a **metric value** which defines how **bad** we are at
    a sport. The *higher* this value is, the *worse* we are and the *lower* the value
    is, the *better* we are. Sort of like a handicap in Golf.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 设想我们有一个**度量值**，它定义了我们在某项运动中的**差劲**程度。这个值*越高*，我们就*越差*；值*越低*，我们就*越好*。有点像高尔夫中的差点。
- en: '![](../Images/b23180426f529c6d3cb08dd99e169a45.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b23180426f529c6d3cb08dd99e169a45.png)'
- en: Photo by [Steven Shircliff](https://unsplash.com/@steveshirc?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由[Steven Shircliff](https://unsplash.com/@steveshirc?utm_source=medium&utm_medium=referral)提供，[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)上的图片
- en: We can further imagine that shifting these parameters will have some effect
    on this metric i.e. as we move towards the optimum set of parameters, the metric
    gets **lower** (we get better at the task).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进一步设想，调整这些参数将对该度量值产生一些影响，即随着我们向最佳参数集合移动，度量值会**降低**（我们在任务中表现得更好）。
- en: Hopefully this makes sense… but if it doesn’t then don’t worry! We’ll take a
    look at a diagram to try and explain this situation.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这能让你明白……但如果没有也没关系！我们会查看一个图示，试图解释这种情况。
- en: Visualising Optimisation
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化优化
- en: '![](../Images/37afabde1f3252e67ee2fe059a57729a.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/37afabde1f3252e67ee2fe059a57729a.png)'
- en: Diagram showing a high-level description of optimisation for being better at
    Golf. Image by the author.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图示展示了提高高尔夫水平的优化高级描述。图像由作者提供。
- en: Take the diagram above, our golf handicap is at **point A** (it’s pretty bad
    — beginner level stuff), this is where we will begin our journey to Tiger Woods
    level!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 以上图为例，我们的高尔夫差点在**A点**（情况相当糟糕——初学者水平），这就是我们开始向泰格·伍兹水平进发的地方！
- en: Which way should we move to reach point B (the professional level of Golfing)?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该朝哪个方向移动以达到B点（高尔夫的职业水平）？
- en: To the left you say? **Correct!**
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你说左边？**正确！**
- en: Mathematically, this involves finding the **gradient** at point A and moving
    in the direction of *steepest descent*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，这涉及到在A点找到**梯度**并沿着*最陡下降*的方向移动。
- en: '**“Wait, wait… remind me what gradients are”**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**“等等，等等……提醒我一下梯度是什么”**'
- en: The gradient defines the steepest rate of change of a function.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度定义了函数变化的最陡速率。
- en: '![](../Images/dd65064ca61f3c296222041a51a48ee3.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd65064ca61f3c296222041a51a48ee3.png)'
- en: Diagram illustrating the locality of gradient estimations at different points
    along a function. Image by the author.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 该图展示了在函数不同点上梯度估计的局部性。图像由作者提供。
- en: Since this gradient is only calculated **locally** i.e. the gradient at point
    A is only correct *at* point A, so we don’t really want to trust this too far
    away from this point. For example, in the picture, points X and Y have very **different**
    gradients.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个梯度仅在**局部**计算，即A点的梯度仅在A点*正确*，所以我们不希望在距离此点太远的地方信任这个梯度。例如，在图片中，点X和Y的梯度非常**不同**。
- en: Therefore in practice, we multiply the gradient with a **learning rate** which
    tells us how far to move towards point B. (We will come back to this later!)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在实际操作中，我们将梯度乘以**学习率**，它告诉我们向B点移动的距离。（我们稍后会再回来讨论这个！）
- en: It is this **locality** argument that is one of the pillars of modern gradient
    descent optimisation algorithms.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这是**局部性**论点，它是现代梯度下降优化算法的支柱之一。
- en: '**Locality and Learning Rates**'
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**局部性与学习率**'
- en: Imagine you are driving a car, you know where you want to end up, but you don’t
    know the way (and you don’t have a sat nav).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你正在开车，你知道你想到达哪里，但不知道怎么走（也没有导航）。
- en: The best thing you can do is rely on sign posts to **guide** your way towards
    your goal.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以做的最好的事情是依靠指示标志来**指导**你朝着目标前进。
- en: '![](../Images/58bfecacad944134ea0eb750c6a1898e.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/58bfecacad944134ea0eb750c6a1898e.png)'
- en: Photo by [Brendan Church](https://unsplash.com/@bdchu614?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Brendan Church](https://unsplash.com/@bdchu614?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: However, these sign posts are only valid at the point they appear. For example,
    a *continue straight* instruction will not necessarily be the correct instruction
    2 miles later down the road.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些指示牌只在出现的地方有效。例如，*继续直行*的指示不一定在2英里后仍然有效。
- en: These sign posts are a bit like our gradient calculations in optimisation algorithms.
    They contain local **information** about the direction of travel (or the shape
    of the function) at that specific point along the journey.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指示牌有点像我们在优化算法中的梯度计算。它们包含关于旅行方向（或函数形状）在特定点的局部**信息**。
- en: Depending on how cautious (or adventurous) you are, you may prefer to have sign
    posts every 200 metres, or you may be happy to include them every two miles. It
    entirely depends on what the journey is like!
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你多么谨慎（或冒险），你可能更喜欢每200米设置一个指示牌，或者你可能满意每两英里设置一个。完全取决于旅程的情况！
- en: For example, if it is a long straight road, we can get away with very few sign
    posts. But if it is a complicated journey with many turns, we will likely need
    a lot **more** sign posts.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果是一条长而直的道路，我们可以用很少的指示牌。但如果是一个复杂的旅程，充满了许多转弯，我们很可能需要更多**的**指示牌。
- en: '![](../Images/0cd1b20b69e9dfad8e5e1791704311b4.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0cd1b20b69e9dfad8e5e1791704311b4.png)'
- en: Two plots showing the differences between small and large learning rates for
    different types of functions. Image by the author.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 显示不同类型函数的小学习率和大学习率之间差异的两个图。图片由作者提供。
- en: This is how we can think about **learning rates**. If we have a function like
    the one on the left, we may be able to use a *large* learning rate (similar to
    a long straight road).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们如何看待**学习率**。如果我们有一个像左边的那样的函数，我们可能能够使用*较大的*学习率（类似于一条长直的道路）。
- en: However, if we have the one on the right, this will require a *smaller* learning
    rate as we may end up **overshooting** our destination (missing a turn).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们有右边的这个，这将需要一个*更小*的学习率，因为我们可能会**超越**我们的目标（错过一个转弯）。
- en: It is also worth mentioning that it is very unlikely we can journey from A to
    B with a single direction at point A (unless we’re already very close). So in
    practice, **gradient descent** tends to be an **iterative** procedure where we
    receive directions at *waypoints* along the journey (*A to B* becomes *A to C
    to D to E to B*).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 还值得一提的是，我们很可能无法仅通过一个方向从A点到B点（除非我们已经非常接近）。因此，实际上，**梯度下降**通常是一个**迭代**过程，我们在旅程中的*路标*上接收指示（*A到B*
    变成 *A到C到D到E到B*）。
- en: Therefore, hopefully we can build some **intuition** about how closely related
    the learning rate and number of waypoints are.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，希望我们能够对学习率和路标数量之间的关系建立一些**直觉**。
- en: Putting it all together…
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将所有内容整合起来…
- en: OK, so hopefully we have a good idea of what **optimisation** is trying to achieve
    and some of the concepts we need to consider!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，希望我们已经对**优化**的目标和一些需要考虑的概念有了一个清晰的了解！
- en: Using the above information, we can now define the gradient descent **algorithm**.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述信息，我们现在可以定义梯度下降**算法**。
- en: Going back to our picture from earlier, we will label the parameters at point
    A (⍬₀) and the final parameters at point B ⍬.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们之前的图片，我们将点A的参数标记为（⍬₀），最终参数标记为点B ⍬。
- en: 'In the first iteration from point A to the first **waypoint** (point C) we
    can write down an equation to describe the parameter update. For this we will
    need to consider:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在从点A到第一个**路标**（点C）的第一次迭代中，我们可以写出一个方程来描述参数更新。为此我们需要考虑：
- en: The gradient of a **performance** metric L at point A (with respect to the parameters)
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在点A的**性能**指标L的梯度（相对于参数）
- en: A learning rate
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个学习率
- en: The initial parameters ⍬₀
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始参数 ⍬₀
- en: The updated parameters ⍬₁
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新的参数 ⍬₁
- en: '![](../Images/a356b10c1024f1928dd0907e20138b44.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a356b10c1024f1928dd0907e20138b44.png)'
- en: Equation for a single gradient descent update. Image by the author.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 单次梯度下降更新的方程。图片由作者提供。
- en: 'The following parameter **updates** are calculated similarly and therefore
    we are able to write down the general formula as:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 以下参数**更新**是类似地计算的，因此我们能够写出通用公式为：
- en: '![](../Images/89debbb0d3303509a0a9ca24d26fdb9d.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89debbb0d3303509a0a9ca24d26fdb9d.png)'
- en: Equation for a general gradient descent step. Image by the author.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一般梯度下降步骤的方程。图片由作者提供。
- en: Initialisation
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OK, so the title promised some talk about initialisation.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: For those of you absolutely fuming that nothing has been mentioned on this so
    far… sorry about that! But hopefully this section will satisfy you!
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: From all the descriptions above, it is fairly easy to think about how initialisation
    fits into the picture.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6689eba5989fecb3137b52a27183e9bc.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
- en: Diagram illustrating the differences in values with respect to initialisation.
    Image by the author.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Earlier I mentioned some child prodigy? Let’s call her Pam. In terms of the
    first image in this article, this would be somewhat equivalent to Pam having some
    **initial** parameters at point P, not at point A. By the way, Pam is the one
    with the crown and the smug smile — she knows she’s good!
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: A high level explanation of what **initialisation** is, is where you *start*
    your optimisation from.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: A good initialisation can take a lot of **pressure** off optimisation algorithms
    and a good optimisation algorithm can do the **same** for initialisation. In practice,
    a good initialisation can save hundreds of hours of compute time when training
    a deep learning model with many parameters.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Due to this, there are many different areas of research solely focussed on developing
    better **initialisation** **techniques**. The reason this is very difficult is
    that, in essence, it’s like trying to **predict** the future without knowing much
    about the environment in which we are in.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Another reason initialisation is important to consider is related to where we
    might end up *after* optimisation.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dca968c48504464e93fa2b749f8a45ba.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: Diagram showing the different outcomes of optimisation by gradient descent for
    different initialisations. Image by the author.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Consider this new optimisation surface above. This has many **different** **minima
    —** some of which are better than others!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Clearly in this picture, our starting point will **heavily** affect where we
    end up. This is one of the reasons why it is so important for ML practitioners
    to experiment with **different** initialisations, as well as tuning hyperparameters
    (such as the learning rate) when attempting to find the best model for a specific
    task.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this article we have walked through some high-level **explanations** related
    to gradient descent, optimisation and initialisation. We have visualised the goals
    of optimisation and initialisation, investigated these **graphically**, introduced
    the concept of a learning rate, and even wrote down a formula for gradient descent!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully this has helped build your **intuition** behind these important concepts
    and solidified your understanding of where the formula for gradient descent comes
    from!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading and stay tuned for more articles related to **optimisation
    techniques**!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: As always, let me know of any issues or comments!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
