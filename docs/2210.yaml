- en: 'Harnessing the Power of Knowledge Graphs: Enriching an LLM with Structured
    Data'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/harnessing-the-power-of-knowledge-graphs-enriching-an-llm-with-structured-data-997fabc62386?source=collection_archive---------2-----------------------#2023-07-10](https://towardsdatascience.com/harnessing-the-power-of-knowledge-graphs-enriching-an-llm-with-structured-data-997fabc62386?source=collection_archive---------2-----------------------#2023-07-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/e32186b4eaa2e36dee4c8c50e5110b8f.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: A step-by-step guide to creating a knowledge graph and exploring its potential
    to enhance an LLM
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://stevehedden.medium.com/?source=post_page-----997fabc62386--------------------------------)[![Steve
    Hedden](../Images/af7bec4a191ab857eccd885dd89e88b4.png)](https://stevehedden.medium.com/?source=post_page-----997fabc62386--------------------------------)[](https://towardsdatascience.com/?source=post_page-----997fabc62386--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----997fabc62386--------------------------------)
    [Steve Hedden](https://stevehedden.medium.com/?source=post_page-----997fabc62386--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2c634ce75a74&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fharnessing-the-power-of-knowledge-graphs-enriching-an-llm-with-structured-data-997fabc62386&user=Steve+Hedden&userId=2c634ce75a74&source=post_page-2c634ce75a74----997fabc62386---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----997fabc62386--------------------------------)
    ·20 min read·Jul 10, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F997fabc62386&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fharnessing-the-power-of-knowledge-graphs-enriching-an-llm-with-structured-data-997fabc62386&user=Steve+Hedden&userId=2c634ce75a74&source=-----997fabc62386---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F997fabc62386&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fharnessing-the-power-of-knowledge-graphs-enriching-an-llm-with-structured-data-997fabc62386&source=-----997fabc62386---------------------bookmark_footer-----------)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '*For accompanying code, see notebook* [*here.*](https://github.com/SteveHedden/kg_llm/blob/main/SDKG.ipynb)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, [large language models](https://snorkel.ai/large-language-models-llms/)
    (LLMs), have become ubiquitous. Perhaps the most famous LLM is ChatGPT, which
    was released by OpenAI in November 2022\. ChatGPT is able to [generate ideas](https://www.linkedin.com/pulse/generate-100-content-ideas-chat-gpt-mfon-akpan/),
    [give personalized recommendations](https://bootcamp.uxdesign.cc/how-to-use-chatgpt-for-personalized-recommendations-840e01dcad89),
    [understand complicated topics](https://medium.com/101-innovation-hacks/using-chatgpt-to-explain-complex-concepts-2ea6aba97cf3),
    [act as a writing assistant](https://chatgptwriter.ai/), or [help you build a
    model to predict the Academy Awards.](https://medium.com/design-bootcamp/using-chatgpt-to-predict-the-oscars-c6d8cdb6b3a0)
    Meta has announced their own LLM called [LLaMA](https://ai.meta.com/blog/large-language-model-llama-meta-ai/),
    Google has [LaMDA](https://blog.google/technology/ai/lamda/), and there is even
    an open-source alternative, [BLOOM.](https://huggingface.co/bigscience/bloom)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，[大型语言模型](https://snorkel.ai/large-language-models-llms/)（LLMs）已经变得无处不在。也许最著名的LLM是ChatGPT，它由OpenAI于2022年11月发布。ChatGPT能够[生成创意](https://www.linkedin.com/pulse/generate-100-content-ideas-chat-gpt-mfon-akpan/)，[提供个性化推荐](https://bootcamp.uxdesign.cc/how-to-use-chatgpt-for-personalized-recommendations-840e01dcad89)，[理解复杂主题](https://medium.com/101-innovation-hacks/using-chatgpt-to-explain-complex-concepts-2ea6aba97cf3)，[充当写作助手](https://chatgptwriter.ai/)，或者[帮助你建立一个预测奥斯卡奖的模型。](https://medium.com/design-bootcamp/using-chatgpt-to-predict-the-oscars-c6d8cdb6b3a0)
    Meta宣布了他们自己的LLM，叫做[LLaMA](https://ai.meta.com/blog/large-language-model-llama-meta-ai/)，Google则有[LaMDA](https://blog.google/technology/ai/lamda/)，甚至还有一个开源替代品，[BLOOM。](https://huggingface.co/bigscience/bloom)
- en: 'LLMs have excelled in natural language processing (NLP) tasks like the ones
    listed above because LLMs have historically focused on [unstructured data](https://en.wikipedia.org/wiki/Unstructured_data)
    — data that does not have a pre-defined structure, and is usually text-heavy.
    I asked ChatGPT, “why have LLMs historically focused on unstructured data?” The
    reply was:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs在自然语言处理（NLP）任务中表现出色，例如上述任务，因为LLMs历来关注于[非结构化数据](https://en.wikipedia.org/wiki/Unstructured_data)—即没有预定义结构的数据，通常以文本为主。我问ChatGPT，“为什么LLMs历来关注于非结构化数据？”回答是：
- en: “LLMs have historically focused on unstructured data due to its abundance, availability,
    and the challenges it poses. Unstructured data provides a vast source for training
    language models, allowing them to learn patterns, context, and semantics. LLMs
    excel in handling…
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “LLMs历来关注于非结构化数据，这主要是由于其丰富性、可获得性以及所带来的挑战。非结构化数据提供了大量的训练语言模型的资源，使它们能够学习模式、上下文和语义。LLMs在处理…
