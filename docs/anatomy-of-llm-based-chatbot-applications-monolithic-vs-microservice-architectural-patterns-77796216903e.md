# åŸºäº LLM çš„èŠå¤©æœºå™¨äººåº”ç”¨ç¨‹åºçš„ç»“æ„ï¼šå•ä½“æ¶æ„ä¸å¾®æœåŠ¡æ¶æ„æ¨¡å¼

> åŸæ–‡ï¼š[`towardsdatascience.com/anatomy-of-llm-based-chatbot-applications-monolithic-vs-microservice-architectural-patterns-77796216903e`](https://towardsdatascience.com/anatomy-of-llm-based-chatbot-applications-monolithic-vs-microservice-architectural-patterns-77796216903e)

## å®ç”¨æŒ‡å—ï¼šä½¿ç”¨ Streamlitã€Huggingface å’Œ FastAPI æ„å»ºå•ä½“å’Œå¾®æœåŠ¡èŠå¤©æœºå™¨äººåº”ç”¨ç¨‹åº

[](https://stephen-leo.medium.com/?source=post_page-----77796216903e--------------------------------)![Marie Stephen Leo](https://stephen-leo.medium.com/?source=post_page-----77796216903e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----77796216903e--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page-----77796216903e--------------------------------) [Marie Stephen Leo](https://stephen-leo.medium.com/?source=post_page-----77796216903e--------------------------------)

Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----77796216903e--------------------------------) Â·9 åˆ†é’Ÿé˜…è¯»Â·2023 å¹´ 5 æœˆ 8 æ—¥

--

![](img/5b4d2660c860f9c8391e33823a5f1824.png)

å›¾ç‰‡ç”±ä½œè€…ä½¿ç”¨ Midjourney V5.1 ç”Ÿæˆï¼Œæç¤ºè¯ä¸ºï¼šâ€œç­‰è·çš„é«˜ç°å®æ„Ÿç¬”è®°æœ¬ç”µè„‘è§†å›¾ï¼Œå±å¹•ä¸Šæ˜¾ç¤ºä¸€ä¸ªæ˜äº®çš„ã€å¤šå½©çš„é­”æ–¹ï¼Œå†…éƒ¨è¢«ç…§äº®ï¼Œæ˜äº®ã€æ¸©æš–ã€æ„‰å¿«çš„å…‰çº¿ã€‚8kï¼Œhdrï¼Œè™šå¹»å¼•æ“â€

éšç€ OpenAI çš„ ChatGPT çš„å‡ºç°ï¼ŒèŠå¤©æœºå™¨äººæ­£åœ¨è¿…é€Ÿæµè¡Œï¼æ¯ä¸ªä¼ä¸šéƒ½åœ¨å¯»æ±‚å°† ChatGPT èå…¥å…¶é¢å‘å®¢æˆ·å’Œå†…éƒ¨çš„åº”ç”¨ç¨‹åºä¸­ã€‚æ­¤å¤–ï¼Œç”±äºå¼€æºèŠå¤©æœºå™¨äººè¿›å±•å¦‚æ­¤è¿…é€Ÿï¼Œä»¥è‡³äºå³ä½¿[è°·æ­Œå·¥ç¨‹å¸ˆä¼¼ä¹ä¹Ÿå¾—å‡ºç»“è®ºè®¤ä¸ºä»–ä»¬å’Œ OpenAIâ€œæ²¡æœ‰æŠ¤åŸæ²³â€ï¼Œ](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither) ç°åœ¨æ­£æ˜¯è¿›å…¥ AI è¡Œä¸šçš„æœ€ä½³æ—¶æœºï¼

ä½œä¸ºæ„å»ºè¿™ç§åº”ç”¨ç¨‹åºçš„æ•°æ®ç§‘å­¦å®¶ï¼Œå…³é”®çš„å†³ç­–ä¹‹ä¸€æ˜¯é€‰æ‹©å•ä½“æ¶æ„è¿˜æ˜¯å¾®æœåŠ¡æ¶æ„ã€‚è¿™ä¸¤ç§æ¶æ„å„æœ‰ä¼˜ç¼ºç‚¹ï¼›æœ€ç»ˆçš„é€‰æ‹©å–å†³äºä¸šåŠ¡çš„éœ€æ±‚ï¼Œä¾‹å¦‚å¯æ‰©å±•æ€§å’Œä¸ç°æœ‰ç³»ç»Ÿçš„é›†æˆæ–¹ä¾¿æ€§ã€‚åœ¨è¿™ç¯‡åšå®¢æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Streamlitã€Huggingface å’Œ FastAPI çš„å®æ—¶ä»£ç ç¤ºä¾‹æ¥æ¢è®¨è¿™ä¸¤ç§æ¶æ„ä¹‹é—´çš„åŒºåˆ«ï¼

é¦–å…ˆï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„ conda ç¯å¢ƒå¹¶å®‰è£…å¿…è¦çš„åº“ã€‚

```py
# Create and activate a conda environment
conda create -n hf_llm_chatbot python=3.9
conda activate hf_llm_chatbot

# Install the necessary libraries
pip install streamlit streamlit-chat "fastapi[all]" "transformers[torch]"
```

# å•ä½“æ¶æ„

![](img/3c56f65e305be824fdbe283044fd1416.png)

åœ¨å•ä½“åº”ç”¨ç¨‹åºä¸­ï¼Œä¸åº”ç”¨ç¨‹åºç›¸å…³çš„æ‰€æœ‰ä»£ç éƒ½ç´§å¯†è€¦åˆåœ¨ä¸€ä¸ªç‹¬ç«‹çš„å•å…ƒä¸­ã€‚ å›¾ç‰‡ç”±ä½œè€…æä¾›

å•ä½“æ¶æ„æ˜¯ä¸€ç§å°†æ•´ä¸ªåº”ç”¨ç¨‹åºæ„å»ºä¸ºä¸€ä¸ªè‡ªåŒ…å«çš„å•å…ƒçš„æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•ç®€å•ä¸”æ˜“äºå¼€å‘ï¼Œä½†éšç€åº”ç”¨ç¨‹åºçš„å¢é•¿ï¼Œå¯èƒ½ä¼šå˜å¾—å¤æ‚ã€‚åœ¨å•ä½“æ¶æ„ä¸­ï¼ŒåŒ…æ‹¬ç”¨æˆ·ç•Œé¢ã€ä¸šåŠ¡é€»è¾‘å’Œæ•°æ®å­˜å‚¨åœ¨å†…çš„æ‰€æœ‰åº”ç”¨ç¨‹åºç»„ä»¶éƒ½æ˜¯ç´§å¯†è€¦åˆçš„ã€‚å¯¹åº”ç”¨ç¨‹åºæŸä¸€éƒ¨åˆ†æ‰€åšçš„ä»»ä½•æ›´æ”¹éƒ½å¯èƒ½å¯¹æ•´ä¸ªåº”ç”¨ç¨‹åºäº§ç”Ÿè¿é”ååº”ã€‚

ä¸‹é¢æˆ‘ä»¬ä½¿ç”¨ Huggingface å’Œ Streamlit æ¥æ„å»ºä¸€ä¸ªå•ä½“èŠå¤©æœºå™¨äººåº”ç”¨ç¨‹åºã€‚æˆ‘ä»¬å°†ä½¿ç”¨ Streamlit æ¥æ„å»ºå‰ç«¯ç”¨æˆ·ç•Œé¢ï¼Œè€Œ Huggingface æä¾›äº†ä¸€ç§éå¸¸æ˜“äºä½¿ç”¨çš„é«˜å±‚æŠ½è±¡ï¼Œç§°ä¸º [pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines#natural-language-processing)ï¼Œå®ƒå¯ä»¥è®¿é—®å„ç§å¼€æº LLM æ¨¡å‹ã€‚

é¦–å…ˆï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåŒ…å«ä¸‰ä¸ªåœ¨å•ä½“å’Œå¾®æœåŠ¡æ¶æ„çš„å‰ç«¯ä¸­å¸¸ç”¨çš„åŠ©æ‰‹å‡½æ•°çš„æ–‡ä»¶ utils.pyã€‚

1.  `clear_conversation()`: æ­¤å‡½æ•°åˆ é™¤ Streamlit å‰ç«¯ä¸­æ‰€æœ‰å­˜å‚¨çš„ session_state å˜é‡ã€‚æˆ‘ä»¬ç”¨å®ƒæ¥æ¸…é™¤æ•´ä¸ªèŠå¤©è®°å½•å¹¶å¼€å§‹ä¸€ä¸ªæ–°çš„èŠå¤©çº¿ç¨‹ã€‚

1.  `display_conversation()`: æ­¤å‡½æ•°ä½¿ç”¨ streamlit_chat åº“åˆ›å»ºä¸€ä¸ªæ¼‚äº®çš„èŠå¤©ç•Œé¢å‰ç«¯ï¼Œå°†æˆ‘ä»¬çš„æ•´ä¸ªèŠå¤©çº¿ç¨‹ä»æœ€æ–°åˆ°æœ€æ—§çš„æ¶ˆæ¯æ˜¾ç¤ºåœ¨å±å¹•ä¸Šã€‚ç”±äº Huggingface pipelines API å°† user_inputs å’Œ generate_responses å­˜å‚¨åœ¨ä¸åŒçš„åˆ—è¡¨ä¸­ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨æ­¤å‡½æ•°åˆ›å»ºä¸€ä¸ªåŒ…å«æ•´ä¸ªèŠå¤©çº¿ç¨‹çš„å•ä¸€ interleaved_conversation åˆ—è¡¨ï¼Œä»¥ä¾¿åœ¨éœ€è¦æ—¶å¯ä»¥ä¸‹è½½å®ƒã€‚

1.  `download_conversation()`: æ­¤å‡½æ•°å°†æ•´ä¸ªèŠå¤©çº¿ç¨‹è½¬æ¢ä¸º pandas dataframe å¹¶ä¸‹è½½ä¸º csv æ–‡ä»¶åˆ°æ‚¨çš„æœ¬åœ°è®¡ç®—æœºã€‚

```py
# %%writefile utils.py
from datetime import datetime

import pandas as pd
import streamlit as st
from streamlit_chat import message

def clear_conversation():
    """Clear the conversation history."""
    if (
        st.button("ğŸ§¹ Clear conversation", use_container_width=True)
        or "conversation_history" not in st.session_state
    ):
        st.session_state.conversation_history = {
            "past_user_inputs": [],
            "generated_responses": [],
        }
        st.session_state.user_input = ""
        st.session_state.interleaved_conversation = []

def display_conversation(conversation_history):
    """Display the conversation history in reverse chronology."""

    st.session_state.interleaved_conversation = []

    for idx, (human_text, ai_text) in enumerate(
        zip(
            reversed(conversation_history["past_user_inputs"]),
            reversed(conversation_history["generated_responses"]),
        )
    ):
        # Display the messages on the frontend
        message(ai_text, is_user=False, key=f"ai_{idx}")
        message(human_text, is_user=True, key=f"human_{idx}")

        # Store the messages in a list for download
        st.session_state.interleaved_conversation.append([False, ai_text])
        st.session_state.interleaved_conversation.append([True, human_text])

def download_conversation():
    """Download the conversation history as a CSV file."""
    conversation_df = pd.DataFrame(
        reversed(st.session_state.interleaved_conversation), columns=["is_user", "text"]
    )
    csv = conversation_df.to_csv(index=False)

    st.download_button(
        label="ğŸ’¾ Download conversation",
        data=csv,
        file_name=f"conversation_{datetime.now().strftime('%Y%m%d%H%M%S')}.csv",
        mime="text/csv",
        use_container_width=True,
    )
```

æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåŒ…å«æ•´ä¸ªå•ä½“åº”ç”¨ç¨‹åºçš„ monolith.py æ–‡ä»¶ã€‚

1.  OpenAI çš„ ChatGPT API å¯¹é—®é¢˜å’Œå›ç­”ä¸­çš„æ¯ä¸ªä»¤ç‰Œéƒ½æ”¶è´¹ã€‚å› æ­¤ï¼Œåœ¨è¿™ä¸ªå°æ¼”ç¤ºä¸­ï¼Œæˆ‘é€‰æ‹©ä½¿ç”¨ Huggingface çš„ä¸€ä¸ªå¼€æºæ¨¡å‹ï¼Œåä¸ºâ€œfacebook/blenderbot-400M-distillâ€ã€‚æ‚¨å¯ä»¥åœ¨ [Huggingface æ¨¡å‹ä¸­å¿ƒ](https://huggingface.co/models?pipeline_tag=conversational) æ‰¾åˆ°è¶…è¿‡ 2000 ä¸ªç»è¿‡å¯¹è¯ä»»åŠ¡è®­ç»ƒçš„å¼€æºæ¨¡å‹çš„å®Œæ•´åˆ—è¡¨ã€‚æœ‰å…³å¯¹è¯ä»»åŠ¡ pipeline çš„æ›´å¤šç»†èŠ‚ï¼Œè¯·å‚è€ƒ [Huggingface å®˜æ–¹æ–‡æ¡£](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/pipelines#transformers.Conversation)ã€‚å½“å¼€æºæ¨¡å‹ä¸å¯é¿å…åœ°èµ¶ä¸Š OpenAI å’Œ Google çš„ä¸“æœ‰æ¨¡å‹æ—¶ï¼Œæˆ‘ç›¸ä¿¡ Huggingface å°†æˆä¸ºç ”ç©¶äººå‘˜åˆ†äº«è¿™äº›æ¨¡å‹çš„å¹³å°ï¼Œè€ƒè™‘åˆ°ä»–ä»¬åœ¨è¿‡å»å‡ å¹´ä¸­å¦‚ä½•å½»åº•æ”¹å˜äº† NLP é¢†åŸŸï¼

1.  `main()`: æ­¤å‡½æ•°ä½¿ç”¨ Streamlit æ„å»ºå‰ç«¯åº”ç”¨çš„å¸ƒå±€ã€‚æˆ‘ä»¬å°†æœ‰ä¸€ä¸ªæŒ‰é’®æ¥æ¸…é™¤å¯¹è¯ï¼Œå¦ä¸€ä¸ªæŒ‰é’®æ¥ä¸‹è½½ã€‚æˆ‘ä»¬è¿˜ä¼šæœ‰ä¸€ä¸ªæ–‡æœ¬æ¡†ï¼Œç”¨æˆ·å¯ä»¥åœ¨å…¶ä¸­è¾“å…¥é—®é¢˜ï¼ŒæŒ‰ä¸‹å›è½¦åï¼Œæˆ‘ä»¬å°†è°ƒç”¨ `monolith_llm_response` å‡½æ•°å¤„ç†ç”¨æˆ·çš„è¾“å…¥ã€‚æœ€åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ¥è‡ª `utils` çš„ `display_conversation` å‡½æ•°åœ¨å‰ç«¯æ˜¾ç¤ºæ•´ä¸ªå¯¹è¯ã€‚

1.  `monolith_llm_response()`: æ­¤å‡½æ•°è´Ÿè´£ä½¿ç”¨ Huggingface ç®¡é“å¤„ç†èŠå¤©æœºå™¨äººé€»è¾‘ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ–°çš„ `Conversation` å¯¹è±¡ï¼Œå¹¶å°†å…¶åˆå§‹åŒ–ä¸ºåˆ°ç›®å‰ä¸ºæ­¢çš„æ•´ä¸ªå¯¹è¯å†å²ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†æœ€æ–°çš„ `user_input` æ·»åŠ åˆ°è¯¥å¯¹è±¡ä¸­ï¼Œæœ€åï¼Œæˆ‘ä»¬å°†æ­¤å¯¹è¯å¯¹è±¡ä¼ é€’ç»™æˆ‘ä»¬ä¹‹å‰åˆ›å»ºçš„ Huggingface ç®¡é“ã€‚Huggingface ä¼šè‡ªåŠ¨å°†ç”¨æˆ·è¾“å…¥å’Œç”Ÿæˆçš„å“åº”æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­ï¼

```py
# %%writefile monolith.py
import streamlit as st
import utils
from transformers import Conversation, pipeline

# https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/pipelines#transformers.Conversation
chatbot = pipeline(
    "conversational", model="facebook/blenderbot-400M-distill", max_length=1000
)

@st.cache_data()
def monolith_llm_response(user_input):
    """Run the user input through the LLM and return the response."""
    # Step 1: Initialize the conversation history
    conversation = Conversation(**st.session_state.conversation_history)

    # Step 2: Add the latest user input
    conversation.add_user_input(user_input)

    # Step 3: Generate a response
    _ = chatbot(conversation)

    # User input and generated response are automatically added to the conversation history
    # print(st.session_state.conversation_history)

def main():
    st.title("Monolithic ChatBot App")

    col1, col2 = st.columns(2)
    with col1:
        utils.clear_conversation()

    # Get user input
    if user_input := st.text_input("Ask your question ğŸ‘‡", key="user_input"):
        monolith_llm_response(user_input)

    # Display the entire conversation on the frontend
    utils.display_conversation(st.session_state.conversation_history)

    # Download conversation code runs last to ensure the latest messages are captured
    with col2:
        utils.download_conversation()

if __name__ == "__main__":
    main()
```

å°±è¿™äº›äº†ï¼æˆ‘ä»¬å¯ä»¥é€šè¿‡è¿è¡Œ `streamlit run monolith.py` æ¥è¿è¡Œè¿™ä¸ªå•ä½“åº”ç”¨ï¼Œå¹¶åœ¨ç½‘é¡µæµè§ˆå™¨ä¸Šä¸åº”ç”¨è¿›è¡Œäº¤äº’ï¼æˆ‘ä»¬è¿˜å¯ä»¥å°†æ­¤åº”ç”¨å¿«é€Ÿéƒ¨ç½²åˆ°åƒ Google Cloud Run è¿™æ ·çš„äº‘æœåŠ¡ä¸­ï¼Œæ­£å¦‚ [æˆ‘ä¹‹å‰çš„åšå®¢æ–‡ç« ](https://medium.com/towards-artificial-intelligence/make-extra-money-on-the-side-with-data-science-984a623c53f5) ä¸­æ‰€æè¿°çš„é‚£æ ·ï¼Œå¹¶é€šè¿‡äº’è”ç½‘è¿›è¡Œäº¤äº’ï¼

![](img/a7252eb9c1b0038f1c6afa067858db11.png)

å•ä½“ Streamlit åº”ç”¨ç•Œé¢ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚

# å¾®æœåŠ¡æ¶æ„

![](img/66daf7b302656ef96086a711dee6949f.png)

åœ¨å¾®æœåŠ¡åº”ç”¨ä¸­ï¼Œæ¯ä¸ªç»„ä»¶è¢«æ‹†åˆ†ä¸ºè‡ªå·±çš„è¾ƒå°çš„ç‹¬ç«‹æœåŠ¡ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚

å¾®æœåŠ¡æ¶æ„æ˜¯ä¸€ç§å°†åº”ç”¨ç¨‹åºæ‹†åˆ†æˆæ›´å°çš„ç‹¬ç«‹æœåŠ¡çš„æ–¹æ³•ã€‚æ¯ä¸ªåº”ç”¨ç¨‹åºç»„ä»¶ï¼Œå¦‚ç”¨æˆ·ç•Œé¢ã€ä¸šåŠ¡é€»è¾‘å’Œæ•°æ®å­˜å‚¨ï¼Œéƒ½æ˜¯ç‹¬ç«‹å¼€å‘å’Œéƒ¨ç½²çš„ã€‚è¿™ç§æ–¹æ³•æä¾›äº†çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥æ¨¡å—åŒ–åœ°æ·»åŠ æ›´å¤šåŠŸèƒ½ï¼Œå¹¶é€šè¿‡æ·»åŠ æ›´å¤šå®ä¾‹æ¥æ°´å¹³æ‰©å±•æ¯ä¸ªæœåŠ¡ï¼Œè€Œä¸ä¼šå½±å“å…¶ä»–æœåŠ¡ã€‚

è®©æˆ‘ä»¬ä½¿ç”¨ FastAPI å°† Huggingface æ¨¡å‹æ¨ç†ä»æˆ‘ä»¬çš„å•ä½“åº”ç”¨ä¸­æ‹†åˆ†ä¸ºä¸€ä¸ªå•ç‹¬çš„å¾®æœåŠ¡ï¼Œå¹¶å°† Streamlit å‰ç«¯æ‹†åˆ†ä¸ºå¦ä¸€ä¸ªå¾®æœåŠ¡ã€‚ç”±äºè¿™ä¸ªæ¼”ç¤ºä¸­çš„åç«¯ä»…æœ‰ LLM æ¨¡å‹ï¼Œæˆ‘ä»¬çš„åç«¯ API æœåŠ¡å™¨ä¸ä¸Šå›¾ä¸­çš„ LLM æ¨¡å‹ç›¸åŒã€‚æˆ‘ä»¬å¯ä»¥ç›´æ¥åœ¨å‰ç«¯å¾®æœåŠ¡ä¸­é‡ç”¨æˆ‘ä»¬ä¹‹å‰åˆ›å»ºçš„ `utils.py` æ–‡ä»¶ï¼

é¦–å…ˆï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ª `backend.py` æ–‡ä»¶ï¼Œè¿™å°†ä½œä¸ºæˆ‘ä»¬çš„ FastAPI å¾®æœåŠ¡ï¼Œè¿è¡Œ Huggingface ç®¡é“æ¨ç†ã€‚

1.  æˆ‘ä»¬é¦–å…ˆä½¿ç”¨ä¹‹å‰é€‰æ‹©çš„ç›¸åŒæ¨¡å‹ï¼Œâ€œfacebook/blenderbot-400M-distillâ€åˆ›å»ºç®¡é“å¯¹è±¡ã€‚

1.  æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ª `ConversationHistory` Pydantic æ¨¡å‹ï¼Œä»¥ä¾¿å°†ç®¡é“æ‰€éœ€çš„è¾“å…¥ä½œä¸ºè´Ÿè½½ä¼ é€’ç»™ FastAPI æœåŠ¡ã€‚å¦‚éœ€æ›´å¤šæœ‰å…³ FastAPI è¯·æ±‚ä¸»ä½“çš„ä¿¡æ¯ï¼Œè¯·å‚è§ [FastAPI æ–‡æ¡£](https://fastapi.tiangolo.com/tutorial/body/)ã€‚

1.  ä¸º APIs ä¿ç•™æ ¹è·¯ç”±ç”¨äºå¥åº·æ£€æŸ¥æ˜¯ä¸€ä¸ªå¥½ä¹ æƒ¯ã€‚æ‰€ä»¥æˆ‘ä»¬é¦–å…ˆå®šä¹‰è¿™ä¸ªè·¯ç”±ã€‚

1.  æœ€åï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªåä¸º `/chat` çš„è·¯ç”±ï¼Œè¯¥è·¯ç”±æ¥å— API æœ‰æ•ˆè´Ÿè½½ä½œä¸º ConversationHistory å¯¹è±¡ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºå­—å…¸ã€‚ç„¶åæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ–°çš„ Conversation å¯¹è±¡ï¼Œå¹¶ç”¨æœ‰æ•ˆè´Ÿè½½ä¸­æ¥æ”¶åˆ°çš„å¯¹è¯å†å²è¿›è¡Œåˆå§‹åŒ–ã€‚æ¥ç€ï¼Œæˆ‘ä»¬å°†æœ€æ–°çš„ user_input æ·»åŠ åˆ°è¯¥å¯¹è±¡ä¸­ï¼Œå¹¶å°†è¿™ä¸ªå¯¹è¯å¯¹è±¡ä¼ é€’ç»™ Huggingface pipelineã€‚æœ€åï¼Œæˆ‘ä»¬å¿…é¡»å°†æœ€æ–°ç”Ÿæˆçš„å“åº”è¿”å›ç»™å‰ç«¯ã€‚

```py
# %%writefile backend.py
from typing import Optional

from fastapi import FastAPI
from pydantic import BaseModel, Field
from transformers import Conversation, pipeline

app = FastAPI()

# https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/pipelines#transformers.Conversation
chatbot = pipeline(
    "conversational", model="facebook/blenderbot-400M-distill", max_length=1000
)

class ConversationHistory(BaseModel):
    past_user_inputs: Optional[list[str]] = []
    generated_responses: Optional[list[str]] = []
    user_input: str = Field(example="Hello, how are you?")

@app.get("/")
async def health_check():
    return {"status": "OK!"}

@app.post("/chat")
async def llm_response(history: ConversationHistory) -> str:
    # Step 0: Receive the API payload as a dictionary
    history = history.dict()

    # Step 1: Initialize the conversation history
    conversation = Conversation(
        past_user_inputs=history["past_user_inputs"],
        generated_responses=history["generated_responses"],
    )

    # Step 2: Add the latest user input
    conversation.add_user_input(history["user_input"])

    # Step 3: Generate a response
    _ = chatbot(conversation)

    # Step 4: Return the last generated result to the frontend
    return conversation.generated_responses[-1]
```

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `uvicorn backend:app --reload` æœ¬åœ°è¿è¡Œè¿™ä¸ª FastAPI åº”ç”¨ï¼Œä¹Ÿå¯ä»¥å°†å…¶éƒ¨ç½²åˆ° Google Cloud Run ç­‰äº‘æœåŠ¡ä¸­ï¼Œå¦‚[æˆ‘ä¹‹å‰çš„åšå®¢æ–‡ç« ](https://medium.com/towards-artificial-intelligence/make-extra-money-on-the-side-with-data-science-984a623c53f5)ä¸­æ‰€è¿°ï¼Œå¹¶é€šè¿‡äº’è”ç½‘è¿›è¡Œäº¤äº’ï¼ä½ å¯ä»¥é€šè¿‡è®¿é—® [`127.0.0.1:8000/docs`](http://127.0.0.1:8000/docs) æ¥ä½¿ç”¨ FastAPI è‡ªåŠ¨ç”Ÿæˆçš„ `/docs` è·¯ç”±ä¸­çš„ API æ–‡æ¡£æµ‹è¯•åç«¯ã€‚

![](img/beb421a5268c38e1ed6301ea58c47aef.png)

åç«¯çš„ FastAPI æ–‡æ¡£ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›

æœ€åï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåŒ…å«å‰ç«¯ä»£ç çš„ `frontend.py` æ–‡ä»¶ã€‚

1.  `main()`ï¼šè¿™ä¸ªå‡½æ•°ä¸å•ä½“åº”ç”¨ä¸­çš„ `main()` å‡½æ•°éå¸¸ç›¸ä¼¼ï¼Œå”¯ä¸€çš„å˜åŒ–æ˜¯å½“ç”¨æˆ·è¾“å…¥ä»»ä½•å†…å®¹æ—¶ï¼Œæˆ‘ä»¬è°ƒç”¨ `microservice_llm_response()` å‡½æ•°ã€‚

1.  `microservice_llm_response()`ï¼šç”±äºæˆ‘ä»¬å°† LLM é€»è¾‘æ‹†åˆ†åˆ°ä¸€ä¸ªå•ç‹¬çš„ FastAPI å¾®æœåŠ¡ä¸­ï¼Œè¿™ä¸ªå‡½æ•°åˆ©ç”¨å­˜å‚¨åœ¨ session_state ä¸­çš„å¯¹è¯å†å²è®°å½•ï¼Œå‘åç«¯ FastAPI æœåŠ¡å‘é€è¯·æ±‚ï¼Œç„¶åå°†ç”¨æˆ·è¾“å…¥å’Œ FastAPI åç«¯çš„å“åº”éƒ½é™„åŠ åˆ°å¯¹è¯å†å²ä¸­ï¼Œä»¥ä¾¿ç»§ç»­è®°ä½æ•´ä¸ªèŠå¤©çº¿ç¨‹ã€‚

```py
# %%writefile frontend.py
import requests
import streamlit as st
import utils

# Replace with the URL of your backend
app_url = "http://127.0.0.1:8000/chat"

@st.cache_data()
def microservice_llm_response(user_input):
    """Send the user input to the LLM API and return the response."""
    payload = st.session_state.conversation_history
    payload["user_input"] = user_input

    response = requests.post(app_url, json=payload)

    # Manually add the user input and generated response to the conversation history
    st.session_state.conversation_history["past_user_inputs"].append(user_input)
    st.session_state.conversation_history["generated_responses"].append(response.json())

def main():
    st.title("Microservices ChatBot App")

    col1, col2 = st.columns(2)
    with col1:
        utils.clear_conversation()

    # Get user input
    if user_input := st.text_input("Ask your question ğŸ‘‡", key="user_input"):
        microservice_llm_response(user_input)

    # Display the entire conversation on the frontend
    utils.display_conversation(st.session_state.conversation_history)

    # Download conversation code runs last to ensure the latest messages are captured
    with col2:
        utils.download_conversation()

if __name__ == "__main__":
    main()
```

å°±è¿™æ ·ï¼æˆ‘ä»¬å¯ä»¥é€šè¿‡è¿è¡Œ `streamlit run frontend.py` æ¥å¯åŠ¨è¿™ä¸ªå‰ç«¯åº”ç”¨ï¼Œå¹¶åœ¨ç½‘é¡µæµè§ˆå™¨ä¸Šä¸åº”ç”¨è¿›è¡Œäº¤äº’ï¼æ­£å¦‚æˆ‘çš„[ä¸Šä¸€ç¯‡åšå®¢æ–‡ç« ](https://medium.com/towards-artificial-intelligence/make-extra-money-on-the-side-with-data-science-984a623c53f5)ä¸­æ‰€æè¿°çš„ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å¿«é€Ÿéƒ¨ç½²åˆ°äº‘æœåŠ¡å¦‚ Google Cloud Runï¼Œå¹¶é€šè¿‡äº’è”ç½‘ä¸ä¹‹äº’åŠ¨ï¼

# é€‰æ‹©å“ªä¸ªæ¶æ„ï¼Ÿ

ç­”æ¡ˆå–å†³äºä½ åº”ç”¨çš„éœ€æ±‚ã€‚å•ä½“æ¶æ„å¯ä»¥æ˜¯æ•°æ®ç§‘å­¦å®¶å¿«é€Ÿæ„å»ºåˆæ­¥æ¦‚å¿µéªŒè¯å¹¶å‘ä¸šåŠ¡å¹²ç³»äººå±•ç¤ºçš„ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ã€‚ä½†ä¸å¯é¿å…åœ°ï¼Œå¦‚æœä½ è®¡åˆ’å°†åº”ç”¨æŠ•å…¥ç”Ÿäº§ï¼Œå¾®æœåŠ¡æ¶æ„é€šå¸¸æ¯”å•ä½“æ¶æ„æ›´å¥½ï¼Œå› ä¸ºå®ƒæä¾›äº†æ›´å¤šçš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ï¼Œå¹¶å…è®¸ä¸åŒçš„ä¸“ä¸šå¼€å‘äººå‘˜ä¸“æ³¨äºæ„å»ºå„ç§ç»„ä»¶ã€‚ä¾‹å¦‚ï¼Œå‰ç«¯å¼€å‘äººå‘˜å¯èƒ½ä¼šä½¿ç”¨ React æ„å»ºå‰ç«¯ï¼Œæ•°æ®å·¥ç¨‹å¸ˆå¯èƒ½ä¼šä½¿ç”¨ Airflow ç¼–å†™æ•°æ®ç®¡é“ï¼Œè€Œ ML å·¥ç¨‹å¸ˆå¯èƒ½ä¼šä½¿ç”¨ [FastAPI](https://fastapi.tiangolo.com/) æˆ– [BentoML](https://github.com/bentoml/BentoML) æ¥éƒ¨ç½²å…·æœ‰è‡ªå®šä¹‰ä¸šåŠ¡é€»è¾‘çš„æ¨¡å‹æœåŠ¡ APIã€‚

æ­¤å¤–ï¼Œé€šè¿‡å¾®æœåŠ¡ï¼ŒèŠå¤©æœºå™¨äººå¼€å‘äººå‘˜å¯ä»¥è½»æ¾åœ°åŠ å…¥æ–°åŠŸèƒ½æˆ–æ›´æ”¹ç°æœ‰åŠŸèƒ½ï¼Œè€Œä¸ä¼šå½±å“æ•´ä¸ªåº”ç”¨ç¨‹åºã€‚è¿™ç§çµæ´»æ€§å’Œå¯æ‰©å±•æ€§å¯¹é‚£äº›å¸Œæœ›å°†èŠå¤©æœºå™¨äººé›†æˆåˆ°ç°æœ‰åº”ç”¨ä¸­çš„ä¼ä¸šè‡³å…³é‡è¦ã€‚ä¸“æ³¨çš„ UI/UXã€æ•°æ®å·¥ç¨‹å¸ˆã€æ•°æ®ç§‘å­¦å®¶å’Œ ML å·¥ç¨‹å¸ˆå¯ä»¥å„è‡ªä¸“æ³¨äºä»–ä»¬çš„ä¸“ä¸šé¢†åŸŸï¼Œä»¥äº¤ä»˜ä¸€ä¸ªç²¾è‡´çš„äº§å“ï¼

# ç»“è®º

æ€»ä¹‹ï¼Œå•ä½“æ¶æ„å’Œå¾®æœåŠ¡æ¶æ„å„æœ‰ä¼˜ç¼ºç‚¹ï¼Œä¸¤è€…ä¹‹é—´çš„é€‰æ‹©å–å†³äºä¸šåŠ¡çš„å…·ä½“éœ€æ±‚ã€‚ç„¶è€Œï¼Œæˆ‘æ›´å€¾å‘äºå¾®æœåŠ¡æ¶æ„ç”¨äºèŠå¤©æœºå™¨äººåº”ç”¨ï¼Œå› ä¸ºå®ƒçš„çµæ´»æ€§ã€å¯æ‰©å±•æ€§ï¼Œä»¥åŠæˆ‘å¯ä»¥å°†å‰ç«¯å¼€å‘å§”æ‰˜ç»™æ›´åˆæ ¼çš„ UI/UX ä¸“å®¶ ğŸ¤©ã€‚
