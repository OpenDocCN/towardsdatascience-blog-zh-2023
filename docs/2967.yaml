- en: Imitation Models and the Open-Source LLM Revolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/imitation-models-and-the-open-source-llm-revolution-431ce48d4bae?source=collection_archive---------2-----------------------#2023-09-27](https://towardsdatascience.com/imitation-models-and-the-open-source-llm-revolution-431ce48d4bae?source=collection_archive---------2-----------------------#2023-09-27)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Are proprietary LLMs like ChatGPT and GPT-4 actually easy to replicate?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wolfecameron.medium.com/?source=post_page-----431ce48d4bae--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----431ce48d4bae--------------------------------)[](https://towardsdatascience.com/?source=post_page-----431ce48d4bae--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----431ce48d4bae--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----431ce48d4bae--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimitation-models-and-the-open-source-llm-revolution-431ce48d4bae&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----431ce48d4bae---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----431ce48d4bae--------------------------------)
    ·15 min read·Sep 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F431ce48d4bae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimitation-models-and-the-open-source-llm-revolution-431ce48d4bae&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----431ce48d4bae---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F431ce48d4bae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimitation-models-and-the-open-source-llm-revolution-431ce48d4bae&source=-----431ce48d4bae---------------------bookmark_footer-----------)![](../Images/f8709e8cc037ca4e582e6581004111d2.png)'
  prefs: []
  type: TYPE_NORMAL
- en: (Photo by [Tanbir Mahmud](https://unsplash.com/@photo_tanbir?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/oyJKjAzAcbU?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  prefs: []
  type: TYPE_NORMAL
- en: The proposal of the LLaMA suite [2] of large language models (LLMs) led to a
    surge in publications on the topic of open-source LLMs. In many cases, the goal
    of these works was to cheaply produce smaller, opens-source LLMs (for research
    purposes) that have comparable quality to proprietary models like [ChatGPT](https://openai.com/blog/chatgpt)
    and [GPT-4](https://openai.com/research/gpt-4). These models adopt an imitation
    strategy, which fine-tunes a base LLM over synthetic dialogue data from a more
    powerful LLM. Despite being cheap to train, these models seemed to perform comparably
    to proprietary LLMs like ChatGPT. As a result, the deep learning research community
    quickly adopted the view that open-source LLMs will rule the future — *re-producing
    open-source variants of proprietary models was both easy and cost-effective*!
  prefs: []
  type: TYPE_NORMAL
- en: “Will the most powerful LLMs be closed-source or will they be freely distributed
    for anyone to use, modify, and extend?” *— from [1]*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Unfortunately, preliminary evaluations performed on these models, which relied
    upon ratings provided by other LLMs (e.g., GPT-4) or human crowd workers, were
    somewhat cursory. *Does the performance of imitation models actually match that
    of models like ChatGPT?* To answer this question more rigorously, we will…
  prefs: []
  type: TYPE_NORMAL
