- en: 'Beyond The VIF: Collinearity Analysis for Bias Mitigation and Predictive Accuracy'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/beyond-the-vif-collinearity-analysis-for-bias-mitigation-and-predictive-accuracy-18fbba3f7aa2?source=collection_archive---------5-----------------------#2023-07-31](https://towardsdatascience.com/beyond-the-vif-collinearity-analysis-for-bias-mitigation-and-predictive-accuracy-18fbba3f7aa2?source=collection_archive---------5-----------------------#2023-07-31)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://goodrobotsai.medium.com/?source=post_page-----18fbba3f7aa2--------------------------------)[![Good
    Robots](../Images/f2fe19ea6712bbe0e4ff20763eaf61b5.png)](https://goodrobotsai.medium.com/?source=post_page-----18fbba3f7aa2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----18fbba3f7aa2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----18fbba3f7aa2--------------------------------)
    [Good Robots](https://goodrobotsai.medium.com/?source=post_page-----18fbba3f7aa2--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3abbfbfa9c59&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-the-vif-collinearity-analysis-for-bias-mitigation-and-predictive-accuracy-18fbba3f7aa2&user=Good+Robots&userId=3abbfbfa9c59&source=post_page-3abbfbfa9c59----18fbba3f7aa2---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----18fbba3f7aa2--------------------------------)
    ·12 min read·Jul 31, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F18fbba3f7aa2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-the-vif-collinearity-analysis-for-bias-mitigation-and-predictive-accuracy-18fbba3f7aa2&user=Good+Robots&userId=3abbfbfa9c59&source=-----18fbba3f7aa2---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F18fbba3f7aa2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-the-vif-collinearity-analysis-for-bias-mitigation-and-predictive-accuracy-18fbba3f7aa2&source=-----18fbba3f7aa2---------------------bookmark_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, collinearity is a complex puzzle for both seasoned professionals
    and newbies alike. Machine learning (ML) algorithms are optimised for predictive
    accuracy not explainability of predictors on the target. Also, most solutions
    for addressing collinearity, such as the ‘[***Variance Inflation Score***](https://www.statisticshowto.com/variance-inflation-factor/)***’***,
    and ‘[***Pearson’s cross-correlation analysis***](http://pubs.sciepub.com/ajams/8/2/1/index.html)’
    potentially leads to massive information loss in pre-processing.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Most machine learning algorithms will select the best possible combination of
    features to optimize predictive accuracy. Therefore, even with collinearity, provided
    the correlations observed in training remain true in the real-world, collinearity
    is not a problem in machine learning. However, for a model’s explainability, the
    unchecked effects of collinearity are a potential source of bias.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ecdab118843501b78f93e4362fba284d.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Collinearity Overview in The Boston Housing Dataset'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '***Collinearity***, which refers to high correlation between independent variables
    (**IVs**) in a dataset, often presents unique challenges in the interpretation
    of regression models. Particularly, it interferes with determining the true reasons
    for the relationships in the data, which can lead to biased interpretations and
    unfair decisions. For example, in figure 1, the independent variables (TAX), (B)
    and (RAD) are collinear IVs and also good predictors of the dependent variable
    (MEDV). While ML algorithms will select the best combination of predictors, they
    may fail to account for the effect of adding another collinear variable (RM) to
    a model with any of these three variables.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: To encourage machine learners to take collinearity analysis seriously as a pre-processing
    step, there must be a way to balance the ***inflationary cost of keeping collinear
    variables*** and the ***predictive cost of dropping them***.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Collinearity
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To demonstrate how unchecked collinearity leads to unintended bias, let’s use
    the cautionary tale of “*How not to collect data*”: the [***Boston Housing Dataset***](https://github.com/Good-Robots/Collinearity-Analysis/blob/main/housing.csv)***.***
    This dataset has since been debunked and withdrawn from public use because it
    contains a [“non-invertible](https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8)”
    variable “B”. The Collinear relationship between independent variables “B”, “RM”
    and “TAX” is a perfect case-study of how spurious correlations can suppress true
    relationships between IVs. The ‘[non-invertible](https://www.sciencedirect.com/science/article/abs/pii/0095069678900062)’
    transformation on “B”, (*A binary IV masquerading as a continuous IV*) introduces
    a moderating bias which may not be picked up by ML algorithms.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3eb95cd84eabbd62eafda2dacbdd7cea.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: IVs — Boston Housing Dataset'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Consider the 13 ***independent variables (IVs)*** in the Boston housing dataset,
    with the median value of owner-occupied homes (MEDV) in a town as the dependent
    variable (***DV***). Certain features may appear to be strong predictors of the
    outcome, but this influence lies in their variance being largely explained by
    other predictors.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/16c5fdc0b0a3d4779938a257e4f8f625.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Cross Correlation Analysis — Boston Housing Dataset'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'In a bivariate relationship between an independent and a dependent variable,
    there is one of four possibilities when a new independent variable is introduced:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '**Spurious inflation**: The inclusion of the third IV significantly escalates
    the influence of the first IV on the DV.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Masking or suppression**: The new IV hides or suppresses the influence of
    the initial IV and on the dependent variable.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Moderates or alters**: The new variable changes the direction of the original
    relationship for all or some observations in the independent variable.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**No effect**: The third IV provides no new information and has no effect on
    the IV and the DV.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For machine learners, off-the-shelf solutions to collinearity often results
    in loss of predictive power, overfitted models, and bias. Hence, a solution that
    mitigates information loss is crucial.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating Collinearity
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If two or more independent variables are highly correlated (RAD and TAX), the
    intuition behind collinearity is that they potentially provide exactly the same
    information about the influence of some ‘latent’ concept (Big Suburb houses/City
    Apartments) on the dependent variable (Property Value). In the presence of ‘Property
    Tax’, accessibility to radial highways provides no new information to property
    value (or vice versa). When IVs are meaninglessly highly correlated, the coefficients
    of a regression model becomes large which in turn leads to over-estimated inferences
    about the effects of the some factors on an outcome.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Currently, there are two ways to deal with collinearity, none of which takes
    into account the dependent variable.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '**Pairwise Correlation**: How many IVs are ‘highly’ correlated with each other.
    A cutoff for the correlation coefficient of ‘highly correlated’ features is subjective.
    However, it is the [general consensus](https://onlinelibrary.wiley.com/doi/full/10.1111/j.1600-0587.2012.07348.x)
    that collinearity becomes a serious problem at a correlation coefficient of +/-
    0.7.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**2\. Variance Inflation**: While a correlation coefficient confirms a degree
    of corresponding change between two IVs, it tells us little about the ***IV’s
    importance***. This is because, IVs, in a multivariate relationship, are not truly
    ***independent in their influence*** on the dependent variable (see Figure 1)
    and the true ***significance of their influence*** is in the presence of a combination
    of other **IV**s.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'The variance inflation score is the size of influence added to the coefficients
    of independent variables due to their dependence on other IVs. The VIF uses a
    ***‘leave-one-out’*** approach on the IVs, by treating each ***‘leave-out’***
    as a dependent variable and all ***‘leave-ins’*** as independent variables. So
    all IVs become dependent variables and each model produces an (***R2***) value.
    This R2 value indicates the percentage of variance in the ***‘leave-out’*** IV
    explained the by the ***‘leave-in’*** IVs. The VIF score is estimated as:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/11aa5178c75d032ce66a4f3c6cf9961f.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Variable Inflation Estimation — Boston Housing Dataset'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Following the VIF estimations above, we would need to drop 11 of 13 IVs to fully
    address collinearity. Not only would this lead to massive information loss, but
    can potentially produce over-fitted or under-fitted models that perform poorly
    in the real-world.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: In Machine learning, pairwise multi-correlation and VIF scores should not be
    the sole criteria for discarding or retaining features.
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Certain features may still offer significant predictive value or contribute
    to the interpretation of the model despite high correlation and VIF scores.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Inflationary VS Predictive Costs to Collinear Feature Selection
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To mitigate information loss, we can compare two values to measure ***inflationary
    cost of keeping collinear features*** and the ***predictive cost of dropping them***.
    Note that the VIF analysis is done independently of the outcome variable, so does
    not fully account for independent influence of individual IVs on the dependent
    variable.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The first measure is the independent influence of an IV on the dependent variable
    i.e amount of variance in the dependent variable, ***independently*** explained
    by the IV (R_squared). For consistency, we will also estimate the VIF score from
    this R_squared value and call this VIF(IY) — Independent Importance. The second
    measure is the influence of the independent variable on the dependent variable
    in the presence of all IVs i.e the VIF estimated above. Lets call this VIF(IX)
    — Collective Importance.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e7392eabce2644ac817325271855a666.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Estimating Inflationary VS Predictive Cost of features to an ML Model'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can confidently estimate the true amount of ‘surprise’ you are letting
    go by dropping a collinear feature. In the graph below, the X-axis represents
    the variance in Y explained by each IV (a measure of potential ‘usefulness’ to
    prediction) while the Y axis represents the variance in the IV explained by other
    IVs (a measure of potential ‘bias’ to model). The bubble size in subplot 1 is
    the inflationary cost of keeping these variables in the model and in subplot 2
    is the predictive cost of dropping them.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Since this is a pre-processing step let’s use very liberal ‘cut-offs’ for the
    VIF(IX) factor of 20 (the redline) and VIF(IY) of 1.15 (the blue line). So that
    features under the redline are least explained by other IVs and features behind
    the blue line cannot independently predict (Y).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5c82c220afce449655caf835e2f1f50a.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Feature Inflationary VS Predictive Cost'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: This graph summarises the independent predictive power of an IV **VS** its potential
    for bias.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '***Quadrant 1 — Potentially True Collinears***: These features (NOX, PTRATIO)
    are explained by some linear combination of IVs in the model and cannot independently
    predict the dependent variable (they may rely on some other set of IVs to be useful).
    Furthermore, whatever predictive power they have maybe cancelled out by some linear
    combination of other IVs (subplot 2). Their influence is significantly suppressed
    by the addition of some other IV(s).'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***象限 1 — 潜在真实共线性变量***：这些特征 (NOX, PTRATIO) 是由模型中某些 IV 的线性组合解释的，不能独立预测因变量（它们可能依赖于其他一组
    IV 才能有效）。此外，它们可能的预测能力可能被其他 IV 的线性组合所抵消（子图 2）。它们的影响被添加的其他 IV(s) 显著抑制。'
- en: '***Quadrant 2 — Potential Biasers***: These are related with both the dependent
    and other independent variable. Features (RM, TAX, B) seem independently predictive
    of (Y) but their variance is also explained by some other combination of independent
    variables. Their so-called independent predictive power on (Y) cannot be considered
    in isolation of some other (IVs). They can be extremely powerful predictors of
    the outcome or a become a source of bias when interpreting their significance
    to the outcome.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***象限 2 — 潜在偏差者***：这些变量与因变量和其他独立变量有关。特征 (RM, TAX, B) 似乎能独立预测 (Y)，但它们的方差也由其他独立变量组合解释。它们对
    (Y) 的所谓独立预测能力不能孤立于其他 (IVs) 考虑。当解释它们对结果的重要性时，它们可能是极其强大的预测变量或成为偏差来源。'
- en: '***Quadrant 3 — Dependents***: Although they are not independently predictive
    of (Y), there is a high predictive cost to dropping some of them. This is because
    they have unique information that is not explained by any other IV. The usefulness
    of this ‘unique’ information to predicting (Y), can only be considered in combination
    of other IV(s).'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***象限 3 — 依赖变量***：虽然它们并不是对 (Y) 的独立预测，但丢弃其中一些变量的预测成本很高。这是因为它们包含的独特信息是其他 IV 所无法解释的。这种‘独特’信息对预测
    (Y) 的有用性只能与其他 IV(s) 结合考虑。'
- en: '***Quadrant 4 — True Independent Predictors***: These variables are independently
    predictive of (Y). These variables also have unique information that is not explained
    by any other IV (more than Quadrant 3). The usefulness of this ‘unique’ information
    to predicting (Y) is independent of other IV(s). However, a linear combination
    of other IVs may have higher predictive power than their independent predictive.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***象限 4 — 真实独立预测变量***：这些变量能独立预测 (Y)。这些变量还包含其他 IV 所无法解释的独特信息（比象限 3 更多）。这种‘独特’信息对预测
    (Y) 的有用性独立于其他 IV(s)。然而，其他 IV 的线性组合可能比它们的独立预测能力具有更高的预测能力。'
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The variables ***B*** , ***TAX*** , and RM significantly predict each other
    and also independently predict the outcome. This could be the linear combination
    of IVs that bests predicts the DV (***MEDV)***. Alternatively, the predictive
    relevance any two of these IVs could be inflated or suppressed due to the presence
    of the third IV. To investigate this, each variable should be sequentially removed
    from a baseline model comprising all independent variables.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 变量 ***B***、***TAX*** 和 RM 彼此之间具有显著的预测关系，同时也独立预测结果。这可能是最能预测 DV (***MEDV***) 的
    IV 线性组合。或者，这些 IV 中任意两个的预测相关性可能因第三个 IV 的存在而被夸大或抑制。为了调查这一点，应将每个变量依次从包含所有独立变量的基线模型中移除。
- en: '![](../Images/a316c8bf4766c61b9fa51229d88ab9c0.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a316c8bf4766c61b9fa51229d88ab9c0.png)'
- en: 'Figure 7: Effects of IV Drops on Baseline Model'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：IV 丢弃对基线模型的影响
- en: Subsequently, the corresponding change in the significance of the remaining
    variables on the outcome should be quantified in terms of percentage. This procedure
    will help expose IVs explained by other IVs, pretending to be important.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，剩余变量对结果的显著性变化应以百分比形式量化。这一过程将有助于揭示被其他 IV 解释的 IV，假装自己很重要。
- en: Above the redline, removing collinear bias
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 红线以上，去除共线性偏差
- en: There are three main effects (of concern) collinear variables above the red
    line may have on the relationship between other IV(s) and the dependent variable.
    They may mediate (suppress), confound (exaggerate) or moderate (change).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 三个主要的影响（关注点）是，位于红线以上的共线性变量可能对其他 IV(s) 和因变量之间的关系产生的影响。它们可能会中介（抑制）、混淆（夸大）或调节（改变）。
- en: The concepts of moderators, mediators and confounders is not really discussed
    in Machine Learning. These concepts are often left to the ‘social scientists’,
    after all, they are ones who ever need to ‘interprete’ their coefficients. However,
    these concepts explain how collinearity can introduce bias to ML models.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Note that these effects cannot be truly established without deeper causal analysis,
    but for a bias removal pre-processing step, we can use simple definitions of these
    concepts to filter these relationships.
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/4f423c333f8ccfdb99bb3d26268ce125.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: mediating Relationships in Boston Housing Dataset'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '***A mediator*** explains ‘how’ the IV and DV are related i.e the process by
    which they are related. A mediator must meet three criteria:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: a) Be significantly predictive of the first IV, b) be significantly predictive
    of the dependent variable and c) be significantly predictive of the dependent
    variable in the presence of the first IV.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: It ‘mediates’ because its inclusion does not change the direction of the relationship
    between the first IV and the dependent variable. If a mediator is removed from
    a model, the strength of the relationship between the first IV and dependent variable
    should become stronger because the mediator was truly accounting for part of that
    effect.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: For example, in the relationship between (RM), (TAX) and (MEDV), the number
    of rooms potentially explains how property tax is related to its property value.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '***Confounders*** are elusive as it is difficult to define them in terms of
    correlations and significance. A confounding variable is an external variable
    that correlates with both the dependent and independent variables, thus potentially
    distorting the perceived relationship between them. As opposed to mediators, the
    relationship between the first IV and the dependent variable is meaningless. There
    is also no guarantee that removing the confounder will weaken or strengthen the
    relationship between the first IV and the dependent variable.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: The number of rooms in a house can either mediate or confound the relationship
    between the proportion of black population and property value. Well, according
    to [this paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2819361/#:~:text=Mediation%20involves%20a%20distinctly%20causal,examine%20undistorted%20estimates%20of%20effects.),
    it depends on the relationship between (B) and (RM). If the relationship between
    (RM <-> MEDV) and (RM <-> B) are ***in the same direction***, removing (RM) should
    weaken the effect of (B) on (MEDV). However, if relationship between (RM <-> MEDV)
    and (RM <-> B) are ***in the opposite direction***, removing (RM) should strengthen
    (B).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: (RM <—> MEDV) and (RM <-> B) are in the same direction (subplot 3 of figure
    1), however, removing (RM) strengthens the effect of (B).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: But see the figure below, where we there is a good decision boundary for a third
    IV in the relationship between the first IV and DV. This indicates a different
    type of relationship between (RM) and (TAX) based on the value of (B).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b5182633423e496832bd3e96b7e44ad5.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Moderating Regression'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: With ***moderators***, the relationship between the first IV and the dependent
    variable is different based on the value of the moderator. What property tax can
    you expect to pay on a house that costs $100,00? Well, it depends on the proportion
    of black population in the town and the number of rooms in that house. Infact,
    there are a set of towns whose property tax stays consistent, regardless of the
    number of rooms, provided (B) remains below a certain threshold.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d5b1251f303feb908eae9b2c2bb77612.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Moderating Relationship Boston Housing Dataset'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Moderators are usually categorical features or groups in the data. Conventional
    pre-processing steps for groups create dummy variables for each group label. This
    [potentially addresses](https://www.researchgate.net/post/Do-you-need-to-dummy-code-sex-to-run-it-as-a-moderator-variable-in-moderated-multiple-regression)
    any moderating effect from that group on the dependent variable. However, ranked
    variables or continuous variables with low variance (B) can also be moderators.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In conclusion, while collinearity is a challenging issue in regression modelling,
    its careful evaluation and management can enhance the predictive power and reliability
    of machine learning models. The ability to account for information loss, provides
    an effective framework for feature selection, enabling the balance of explainability
    and predictive accuracy.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
