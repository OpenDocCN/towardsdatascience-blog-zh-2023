- en: 'vLLM: PagedAttention for 24x Faster LLM Inference'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: vLLM：PagedAttention 实现 24 倍更快的 LLM 推理
- en: 原文：[https://towardsdatascience.com/vllm-pagedattention-for-24x-faster-llm-inference-fdfb1b80f83?source=collection_archive---------3-----------------------#2023-06-24](https://towardsdatascience.com/vllm-pagedattention-for-24x-faster-llm-inference-fdfb1b80f83?source=collection_archive---------3-----------------------#2023-06-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/vllm-pagedattention-for-24x-faster-llm-inference-fdfb1b80f83?source=collection_archive---------3-----------------------#2023-06-24](https://towardsdatascience.com/vllm-pagedattention-for-24x-faster-llm-inference-fdfb1b80f83?source=collection_archive---------3-----------------------#2023-06-24)
- en: A more efficient way to compute Transformer’s attention during inference
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在推理过程中更高效的 Transformer 注意力计算方法
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----fdfb1b80f83--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----fdfb1b80f83--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fdfb1b80f83--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fdfb1b80f83--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----fdfb1b80f83--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie?source=post_page-----fdfb1b80f83--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----fdfb1b80f83--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fdfb1b80f83--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fdfb1b80f83--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----fdfb1b80f83--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvllm-pagedattention-for-24x-faster-llm-inference-fdfb1b80f83&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----fdfb1b80f83---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fdfb1b80f83--------------------------------)
    ·6 min read·Jun 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffdfb1b80f83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvllm-pagedattention-for-24x-faster-llm-inference-fdfb1b80f83&user=Benjamin+Marie&userId=ad2a414578b3&source=-----fdfb1b80f83---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvllm-pagedattention-for-24x-faster-llm-inference-fdfb1b80f83&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----fdfb1b80f83---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fdfb1b80f83--------------------------------)
    · 6 分钟阅读 · 2023 年 6 月 24 日'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffdfb1b80f83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvllm-pagedattention-for-24x-faster-llm-inference-fdfb1b80f83&source=-----fdfb1b80f83---------------------bookmark_footer-----------)![](../Images/9b79c5674eecc1de9d97ae46d9048bff.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffdfb1b80f83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvllm-pagedattention-for-24x-faster-llm-inference-fdfb1b80f83&source=-----fdfb1b80f83---------------------bookmark_footer-----------)![](../Images/9b79c5674eecc1de9d97ae46d9048bff.png)'
- en: PagedAttention for a prompt “the cat is sleeping in the kitchen and the dog
    is”. Key-Value pairs of tensors for attention computation are stored in virtual
    contiguous blocks mapped to non-contiguous blocks in the GPU memory. — Image by
    the author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: PagedAttention 用于一个提示“猫在厨房里睡觉，狗在”。用于注意力计算的 Key-Value 张量对被存储在虚拟连续块中，这些块映射到 GPU
    内存中的非连续块。 — 作者提供的图片
- en: Almost all the large language models (LLM) rely on the Transformer neural architecture.
    While this architecture is praised for its efficiency, it has some well-known
    computational bottlenecks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有的大型语言模型（LLM）都依赖于 Transformer 神经架构。尽管这种架构因其高效性而受到赞誉，但它也存在一些众所周知的计算瓶颈。
- en: During decoding, one of these bottlenecks is in the computation of the attention
    with pairs of key-value tensors for each token of the input. All these tensors
    must be stored in memory.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在解码过程中，其中一个瓶颈是计算每个输入 token 的 key-value 张量对的注意力。所有这些张量必须存储在内存中。
- en: '*Note: I won’t explain in this article what is the role of these key-value
    pairs. It’s one of the most complicated and interesting aspects of the Transformer
    architecture. If you don’t know about it, I strongly recommend reading* [*The
    Illustrated Transformer by Jay Alammar*](https://jalammar.github.io/illustrated-transformer/)*.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：我不会在本文中解释这些键值对的作用。这是Transformer架构中最复杂且最有趣的方面之一。如果你不太了解这个概念，我强烈推荐阅读* [*Jay
    Alammar的《图解Transformer》*](https://jalammar.github.io/illustrated-transformer/)*。*'
- en: As LLM accepts longer and longer inputs, e.g., the LLM Claude accepts 100k token-long
    inputs, the memory consumed by these tensors can become very large.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 随着LLM接受越来越长的输入，例如LLM Claude接受100k令牌长的输入，这些张量消耗的内存可能会非常大。
- en: Naively storing all these tensors in memory leads to memory over-reservation
    and fragmentation. This fragmentation can make memory access very inefficient,
    especially for long sequences of tokens. As for over-reservation, the system does
    it to make sure it has allocated enough memory for the tensors, even if it doesn’t
    consume all of it.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 天真地将所有这些张量存储在内存中会导致内存过度预留和碎片化。这种碎片化可能使内存访问非常低效，特别是对于长序列的令牌而言。至于过度预留，系统这样做是为了确保分配足够的内存给张量，即使它并没有全部使用。
