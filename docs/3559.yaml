- en: 'dbt Core, Snowflake, and GitHub Actions: pet project for Data Engineers'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/dbt-core-snowflake-and-github-actions-pet-project-for-data-engineers-815991a48b44?source=collection_archive---------7-----------------------#2023-12-01](https://towardsdatascience.com/dbt-core-snowflake-and-github-actions-pet-project-for-data-engineers-815991a48b44?source=collection_archive---------7-----------------------#2023-12-01)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Pet Project for Data/Analytics Engineers: Explore Modern Data Stack Tools —
    dbt Core, Snowflake, Fivetran, GitHub Actions.'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@kategera6?source=post_page-----815991a48b44--------------------------------)[![Kateryna
    Herashchenko](../Images/dd6018e0f3ffb6d4fecd8cb72100282c.png)](https://medium.com/@kategera6?source=post_page-----815991a48b44--------------------------------)[](https://towardsdatascience.com/?source=post_page-----815991a48b44--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----815991a48b44--------------------------------)
    [Kateryna Herashchenko](https://medium.com/@kategera6?source=post_page-----815991a48b44--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4fc94e2ed685&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdbt-core-snowflake-and-github-actions-pet-project-for-data-engineers-815991a48b44&user=Kateryna+Herashchenko&userId=4fc94e2ed685&source=post_page-4fc94e2ed685----815991a48b44---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----815991a48b44--------------------------------)
    ·6 min read·Dec 1, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F815991a48b44&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdbt-core-snowflake-and-github-actions-pet-project-for-data-engineers-815991a48b44&user=Kateryna+Herashchenko&userId=4fc94e2ed685&source=-----815991a48b44---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F815991a48b44&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdbt-core-snowflake-and-github-actions-pet-project-for-data-engineers-815991a48b44&source=-----815991a48b44---------------------bookmark_footer-----------)![](../Images/c550e2d6ed787a2bf6243b3e727599ff.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Gaining Visuals](https://unsplash.com/@gainingvisuals?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Here is a simple and fast pet project for Data/Analytics Engineers, who want
    to kick the tires on Modern Data Stack tools including dbt Core, Snowflake, Fivetran,
    and GitHub Actions. This hands-on experience will allow you to develop an end-to-end
    data lifecycle, from extracting data from your Google Calendar to presenting it
    in a Snowflake analytics dashboard. In this article, I’ll walk you through the
    project, sharing insights and tips along the way. [*See Github repo.*](https://github.com/KHerashchenko/SurfalyticsWorkshop/tree/master)
  prefs: []
  type: TYPE_NORMAL
- en: Technical Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The project architecture is depicted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Google Calendar** -> **Fivetran** -> **Snowflake** -> **dbt** -> **Snowflake
    Dashboard**, with **GitHub Actions** orchestrating the deployment.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8ff05646a8b0204831aa105f5ef64cda.png)'
  prefs: []
  type: TYPE_IMG
- en: Architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'Referencing Joe Reis’s “Fundamentals of Data Engineering,” let’s review our
    project in alignment with the defined stages of the data lifecycle:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f1733e478c42fe23b00c1fa9d7338888.png)'
  prefs: []
  type: TYPE_IMG
- en: Data Engineering Lifecycle [1]
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Generation — *Google Calendar, Fivetran*** If you’re a Google Calendar
    user, chances are you’ve accumulated a wealth of data there. Now you can easily
    retrieve it from your account by leveraging “data movement platforms” like Fivetran.
    This tool automates ELT (Extract, Load, Transform) process, integrating your data
    from the source system of Google Calendar to our Snowflake data warehouse.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As of now, Fivetran offers 14-day free trial — [link](https://fivetran.com/docs/getting-started/free-trials/account-trial).
    Registration is very straightforward.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Storage — *Snowflake*** Snowflake, a cloud-based data warehouse tailored
    for analytical needs, will serve as our data storage solution. The data volume
    we will deal with is small, so we will not try to overkill with data partitioning,
    time travel, Snowpark, and other Snowflake advanced capabilities. However, we
    will pay particular attention to Access Control (this will be used for dbt access).
    You need to [set up a trial account](https://signup.snowflake.com/) which provides
    you with 30 days of free usage and 400$ limit for the Enterprise edition.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ingestion — *Fivetran*** Data ingestion can be configured from both Fivetran
    and Snowflake using the Partner Connect feature. Choose your preferred method
    and set up the Google Calendar connector. After the initial sync, you can access
    your data from the Snowflake UI. You can visit the connector webpage to see the
    schema diagram [here](https://www.fivetran.com/connectors/google-calendar).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A new database will be created specifically for Fivetran sync, and the corresponding
    warehouse to run SQL workloads. As you should know, Snowflake is built with decoupled
    Storage and Compute, hence the costs are separated as well. As a best practice,
    you should use different warehouses for different workloads (ad-hoc, sync, BI
    analytics) or different environments (dev, prod).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/f431365d1580340e358dcf4cd9e149a9.png)'
  prefs: []
  type: TYPE_IMG
- en: Go to Partner Connect to connect Fivetran
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7a112d493f9447f09ce99c6536c95c72.png)'
  prefs: []
  type: TYPE_IMG
- en: Set up Google Calendar connector in Fivetran
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/38a2542d3dac0d8066fd9bdccd10fae0.png)'
  prefs: []
  type: TYPE_IMG
- en: Synced data appears in Snowflake
  prefs: []
  type: TYPE_NORMAL
- en: '**Transformation — *dbt Core*** With the data residing in Snowflake (and automatically
    syncing every 6 hours by default), we move to the Transformation stage using dbt
    Core. dbt (data build tool) facilitates modularization of SQL queries, enabling
    the reuse and version control of SQL workflows, just like software code is typically
    managed. There are two ways to access dbt: dbt Cloud and dbt Core. dbt Cloud is
    a paid cloud-based version of the service and dbt Core is a python package providing
    all functionality you can use for free.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Install dbt Core on your machine, initialize the project using **“dbt init”**
    command in CLI, and set up the Snowflake connection. See the example of my [profiles.yml
    file](https://github.com/KHerashchenko/SurfalyticsWorkshop/blob/master/dbt_hol/profiles.yml).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To be able to connect to Snowflake, we will also need to run a bunch of DCL
    (Data Control Language) SQL commands, find them [here](https://github.com/KHerashchenko/SurfalyticsWorkshop/blob/master/snowflake_setup.sql).
    Following the least privilege principle, we will create a separate user for dbt
    and give it access only to the source database (where Fivetran syncs data) and
    development, and production databases (where we will sink transformed data).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Following the [best practice structuring approach](https://docs.getdbt.com/best-practices/how-we-structure/1-guide-overview),
    you need to create three folders representing the *staging, intermediate, and
    marts layers* of your data transformations. Here you can experiment with your
    models, but also you can copy my [examples](https://github.com/KHerashchenko/SurfalyticsWorkshop/tree/master/dbt_hol/models)
    for Google Calendar data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the repo, you will find the “sources.yml” file listing all tables in the
    Google Calendar schema. There are 3 staging models created (event.sql,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: attendee.sql, recurrence.sql), 1 transform model (utc_event.sql), and 1 mart
    model (event_attendee_summary.sql).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The important features of dbt are [Jinja and macros](https://courses.getdbt.com/courses/jinja-macros-packages)
    that you can weave into SQL, enhancing its impact and reusability.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/875c9de7d5e6d260a7ea70df5ce0a13c.png)'
  prefs: []
  type: TYPE_IMG
- en: Choose different types of model materializations
  prefs: []
  type: TYPE_NORMAL
- en: Set data expectations using generic or singular tests in dbt to ensure data
    quality. Some data quality rules are placed in the “source.yml” file,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: as well as in the “/tests” folder. During the **“dbt build”** command these
    data quality checks will be run along with models build to prevent data corruption.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/4c204eaf6b005f942f81ca4c07541544.png)'
  prefs: []
  type: TYPE_IMG
- en: You can run tests using “dbt test”
  prefs: []
  type: TYPE_NORMAL
- en: Explore dbt’s Snapshot feature for change data capture and type-2 Slowly Changing
    Dimensions. In our [example](https://github.com/KHerashchenko/SurfalyticsWorkshop/blob/master/dbt_hol/snapshots/recurrence_snapshot.sql),
    we capture changes in the “recurrence” table.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/e0da53b1e6955a9e2b81b1e635462f0f.png)'
  prefs: []
  type: TYPE_IMG
- en: Store snapshots in a separate schema
  prefs: []
  type: TYPE_NORMAL
- en: Take a while to generate dbt documentation using the **“dbt docs generate”**
    command. You will see the data lineage graph and metadata which is automatically
    created from your project. You can further enhance it by adding descriptions to
    data entities in your .yml files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Good documentation provides better data discoverability and governance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/4f4e168ba3ead8b7e73fb3217d083382.png)'
  prefs: []
  type: TYPE_IMG
- en: run “dbt docs serve” to open it in browser
  prefs: []
  type: TYPE_NORMAL
- en: '**Serving — *Snowflake Dashboard*** Finally, visualize your transformed data
    using Snowflake Dashboards. Create a dashboard in the Snowflake UI and experiment
    with tiles (plots) based on your SQL queries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/266ccc344bbcbe911847257bee354a3a.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of the dashboard
  prefs: []
  type: TYPE_NORMAL
- en: '**Deploying — *GitHub Actions*** While dbt Cloud offers an easy deployment
    option, we’ll use GitHub Actions workflows for our dbt Core project. You need
    to create a [workflow .yml file](https://github.com/KHerashchenko/SurfalyticsWorkshop/blob/master/.github/workflows/dbt_prod.yaml)
    which will be triggered whenever changes are pushed to the GitHub dbt repo, running
    specified actions. In my example workflow, you can see a two-step deployment process:
    **dbt build** for the development environment and in case of success, also **dbt'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: build** for the production environment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Note: replace secrets like Snowflake account and password with GitHub secrets.
    For that, on your repo webpage, go to Settings -> Secrets and Variables -> Actions.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Every time “master” branch gets updated, you can see the workflow started in
    your Actions tab on the repo:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/129492b139aaeea10506b6b16fd85bf2.png)'
  prefs: []
  type: TYPE_IMG
- en: See Actions results
  prefs: []
  type: TYPE_NORMAL
- en: In this project, we’ve just scratched the surface of various technologies in
    the modern data engineering stack. It’s not only a practical accomplishment but
    also an excellent starting point for deeper exploration. Thank you for reading,
    and happy coding!
  prefs: []
  type: TYPE_NORMAL
- en: '**If you reached the end of the article, maybe you found it valuable and may
    want to connect with me on** [**LinkedIn**](https://www.linkedin.com/in/kategera6/)**.
    *I am open to opportunities!***'
  prefs: []
  type: TYPE_NORMAL
- en: '*Unless otherwise noted, all images are by the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'References:'
  prefs: []
  type: TYPE_NORMAL
- en: '[1] Reis, J. (2022). Fundamentals of Data Engineering: Plan and Build Robust
    Data Systems. O’Reilly Media.'
  prefs: []
  type: TYPE_NORMAL
