["```py\nSYS_PROMPT = (\n    \"You are a network graph maker who extracts terms and their relations from a given context. \"\n    \"You are provided with a context chunk (delimited by ```", "```py{input}```", "```py\n\nIf we pass our (not fit for) nursery rhyme with this prompt, here is the result.\n\n```", "```py\n\nNotice, that it even guessed ‘food’ as a concept, which was not explicitly mentioned in the text chunk. Isn’t this wonderful!\n\nIf we run this through every text chunk of our example article and convert the json into a Pandas data frame, here is what it looks like.\n\n![](../Images/b90d053228d8ea0b384e52a8e2ff819a.png)\n\nEvery row here represents a relation between a pair of concepts. Each row is an edge between two nodes in our graph, and there can be multiple edges or relationships between the same pair of concepts. The count in the above data frame is the weight that I arbitrarily set to 4.\n\n## **Contextual Proximity**\n\nI assume that the concepts that occur close to each other in the text corpus are related. Let’s call this relation ‘contextual proximity’.\n\nTo calculate the contextual proximity edges, we melt the dataframe so that node_1 and node_2 collapse into a single column. Then we create a self-join of this dataframe using the chunk_id as the key. So nodes that have the same chunk_id will pair with each other to form a row.\n\nBut this also means that each concept will also be paired with itself. This is called a self-loop, where an edge starts and ends on the same node. To remove these self-loops, we will drop every row where node_1 is the same as node_2 from the dataframe.\n\nIn the end, we get a dataframe very similar to our original dataframe.\n\n![](../Images/8898559a2fcb5732570bd12f60e205e2.png)\n\nThe count column here is the number of chunks where node_1 and node_2 occur together. The column chunk_id is a list of all these chunks.\n\nSo we now have two dataframes, one with the semantic relation, and another with the contextual proximity relation between concepts mentioned in the text. We can combine them to form our network graph dataframe.\n\nWe are done building a graph of concepts for our text. But to leave it at this point will be quite an ungratifying exercise. Our goal is to visualise the Graph just like the featured image at the beginning of this article, and we are not far from our goal.\n\n# **Creating a Network of Concepts**\n\nNetworkX is a Python library that makes dealing with graphs super easy. If you are not already familiar with the library, click their logo below to learn more\n\n [## NetworkX - NetworkX documentation\n\n### NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of…\n\nnetworkx.org](https://networkx.org/?source=post_page-----110844f22a1a--------------------------------) \n\nAdding our dataframe to a NetworkX graph is just a few lines of code.\n\n```", "```py\n\nThis is where we can start harnessing the power of Network Graph. NetworkX provides a plethora of network algorithms out of the box for us to use. Here is a link to the list of algorithms we can run on our Graph.\n\n [## Algorithms - NetworkX 3.2.1 documentation\n\n### Edit description\n\nnetworkx.org](https://networkx.org/documentation/stable/reference/algorithms/index.html?source=post_page-----110844f22a1a--------------------------------) \n\nHere, I use a community detection algorithm to add colours to the nodes. Communities are groups of nodes that are more tightly connected with each other, than with the rest of the graph. Communities of concepts can give us a good idea of broad themes discussed in the text.\n\nThe Girvan Newman algorithm detected 17 communities of concept in the Review Article we are working with. Here is one such community.\n\n```"]