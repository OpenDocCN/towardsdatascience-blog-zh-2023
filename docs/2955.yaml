- en: Creating a LLaMa 2 Agent Empowered with Wikipedia Knowledge
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个具备维基百科知识的 LLaMa 2 代理
- en: 原文：[https://towardsdatascience.com/creating-a-llama-2-agent-empowered-with-wikipedia-knowledge-c048c7c63ef8?source=collection_archive---------2-----------------------#2023-09-26](https://towardsdatascience.com/creating-a-llama-2-agent-empowered-with-wikipedia-knowledge-c048c7c63ef8?source=collection_archive---------2-----------------------#2023-09-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/creating-a-llama-2-agent-empowered-with-wikipedia-knowledge-c048c7c63ef8?source=collection_archive---------2-----------------------#2023-09-26](https://towardsdatascience.com/creating-a-llama-2-agent-empowered-with-wikipedia-knowledge-c048c7c63ef8?source=collection_archive---------2-----------------------#2023-09-26)
- en: Powering up LLaMa 2 with retrieval augmented generation to seek and use information
    from Wikipedia.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用检索增强生成技术为 LLaMa 2 提供支持，从维基百科中查找和使用信息。
- en: '[](https://medium.com/@gabrielesgroi94?source=post_page-----c048c7c63ef8--------------------------------)[![Gabriele
    Sgroi, PhD](../Images/b81978d35e6238d160457de2affc2b0e.png)](https://medium.com/@gabrielesgroi94?source=post_page-----c048c7c63ef8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c048c7c63ef8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c048c7c63ef8--------------------------------)
    [Gabriele Sgroi, PhD](https://medium.com/@gabrielesgroi94?source=post_page-----c048c7c63ef8--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@gabrielesgroi94?source=post_page-----c048c7c63ef8--------------------------------)[![Gabriele
    Sgroi, PhD](../Images/b81978d35e6238d160457de2affc2b0e.png)](https://medium.com/@gabrielesgroi94?source=post_page-----c048c7c63ef8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c048c7c63ef8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c048c7c63ef8--------------------------------)
    [Gabriele Sgroi, PhD](https://medium.com/@gabrielesgroi94?source=post_page-----c048c7c63ef8--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F97ea0c34751b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-a-llama-2-agent-empowered-with-wikipedia-knowledge-c048c7c63ef8&user=Gabriele+Sgroi%2C+PhD&userId=97ea0c34751b&source=post_page-97ea0c34751b----c048c7c63ef8---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c048c7c63ef8--------------------------------)
    ·11 min read·Sep 26, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc048c7c63ef8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-a-llama-2-agent-empowered-with-wikipedia-knowledge-c048c7c63ef8&user=Gabriele+Sgroi%2C+PhD&userId=97ea0c34751b&source=-----c048c7c63ef8---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F97ea0c34751b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-a-llama-2-agent-empowered-with-wikipedia-knowledge-c048c7c63ef8&user=Gabriele+Sgroi%2C+PhD&userId=97ea0c34751b&source=post_page-97ea0c34751b----c048c7c63ef8---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c048c7c63ef8--------------------------------)
    ·11 min 阅读·2023年9月26日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc048c7c63ef8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-a-llama-2-agent-empowered-with-wikipedia-knowledge-c048c7c63ef8&user=Gabriele+Sgroi%2C+PhD&userId=97ea0c34751b&source=-----c048c7c63ef8---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc048c7c63ef8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-a-llama-2-agent-empowered-with-wikipedia-knowledge-c048c7c63ef8&source=-----c048c7c63ef8---------------------bookmark_footer-----------)![](../Images/4fe5a16d530a8b089a1c6ed5cd5ef8ce.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc048c7c63ef8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-a-llama-2-agent-empowered-with-wikipedia-knowledge-c048c7c63ef8&source=-----c048c7c63ef8---------------------bookmark_footer-----------)![](../Images/4fe5a16d530a8b089a1c6ed5cd5ef8ce.png)'
- en: Photo by [Lysander Yuen](https://unsplash.com/@lysanderyuen?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Lysander Yuen](https://unsplash.com/@lysanderyuen?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) 提供
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: Large Language Models (LLMs) are one of the hottest trends in AI. They have
    demonstrated impressive text-generation capabilities, ranging from the ability
    to carry on conversations with human users to writing code. The rise of open-source
    LLMs such as LLama, Falcon, Stable Beluga, etc., has made their potential available
    to the wide AI community, thanks also to the focus on developing smaller and more
    efficient models that can be run on consumer-grade hardware.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型（LLMs）是人工智能领域的热门趋势之一。它们展示了令人印象深刻的文本生成能力，从与人类用户对话的能力到编写代码。开源LLMs的兴起，如LLama、Falcon、Stable
    Beluga等，使得它们的潜力对广泛的AI社区可用，同时也得益于开发更小、更高效模型的重点，这些模型可以在消费级硬件上运行。
- en: One of the key ingredients contributing to the success of LLMs is the famous
    transformer architecture introduced in the revolutionary paper [Attention Is All
    You Need](https://arxiv.org/abs/1706.03762). The impressive performance of state-of-the-art
    LLMs is achieved by scaling this architecture to billions of parameters and training
    on datasets comprising trillions of tokens. This pre-training yields powerful
    foundation models cable to understand human language that can be further fine-tuned
    to specific use cases.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 促成大语言模型（LLMs）成功的关键因素之一是革命性论文[Attention Is All You Need](https://arxiv.org/abs/1706.03762)中引入的著名变换器架构。最先进的LLMs之所以表现出色，是因为将这一架构扩展到数十亿参数，并在包含万亿个标记的数据集上进行训练。这种预训练产生了强大的基础模型，能够理解人类语言，并可以进一步调整以适应特定的应用场景。
- en: The pre-training of Large Language Models is performed in a self-supervised
    fashion. It requires a huge collection of text corpora but doesn’t need human
    labeling. This makes it possible to scale up the training to huge datasets that
    can be created in an automated manner. After transforming the input texts into
    a sequence of tokens, the pre-training is performed with the objective of predicting
    the probability of each token in the sequence conditioned on all the previous
    tokens. In this way, after training, the model is able to generate text autoregressively
    by sampling one token at a time conditioned on all the tokens sampled so far.
    Large Language Models have been shown to obtain impressive language capabilities
    with this pre-training alone. However, by sampling tokens according to the conditional
    probabilities learned from the training data, the generated text is in general
    not aligned with human preferences and LLMs struggle to follow specific instructions
    or human intentions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型的预训练是在自监督的方式下进行的。它需要大量的文本语料库，但不需要人工标注。这使得可以将训练扩展到以自动化方式创建的大规模数据集。将输入文本转化为标记序列后，预训练的目标是预测序列中每个标记的概率，条件是所有先前的标记。通过这种方式，训练完成后，模型能够通过自回归地生成文本，每次根据到目前为止采样的所有标记来采样一个标记。大语言模型仅凭借这种预训练已经展示了令人印象深刻的语言能力。然而，通过根据从训练数据中学到的条件概率采样标记，生成的文本通常与人类偏好不一致，LLMs在遵循特定指令或人类意图方面存在困难。
- en: A significant step forward in aligning the text generated by LLMs with human
    preferences was achieved with Reinforcement Learning from Human Feedback (RLHF).
    This technique is at the core of the state-of-the-art chat models such as ChatGPT.
    With RLHF, after the initial self-supervised pre-training phase, the Large Language
    Model is further fine-tuned with a reinforcement learning algorithm to maximize
    a reward calibrated on human preferences. To obtain the reward function, it is
    usual to train an auxiliary model to learn a scalar reward reflecting human preferences.
    In this way, the amount of actual human-labeled data needed for the reinforcement
    learning phase can be kept at a minimum. The training data for the reward models
    consists of generated texts that have been ranked by humans according to their
    preferences. The model aims to predict a higher reward for the text ranked higher.
    Training an LLM with the objective of maximizing a reward reflecting human preferences
    should result in generated texts that are more aligned with human intentions.
    In fact, Large Language Models fine-tuned with Reinforcement Learning from Human
    Feedback have been shown to follow user instructions better while also being less
    toxic and more truthful.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐大语言模型（LLMs）生成的文本与人类偏好的一个重要进展是通过人类反馈强化学习（RLHF）实现的。这项技术是最先进的聊天模型，如ChatGPT的核心。通过RLHF，在初始的自我监督预训练阶段之后，大语言模型会进一步通过强化学习算法进行微调，以最大化一个基于人类偏好的奖励。为了获得奖励函数，通常会训练一个辅助模型来学习一个反映人类偏好的标量奖励。这样，可以将实际需要的人类标注数据量保持在最低。奖励模型的训练数据包括由人类根据其偏好对生成的文本进行排名的文本。模型的目标是对排名较高的文本预测更高的奖励。通过最大化反映人类偏好的奖励来训练LLM应能生成更符合人类意图的文本。实际上，通过人类反馈强化学习微调的大语言模型已被证明能够更好地遵循用户指令，同时也更少有毒，更真实。
- en: Retrieval Augmented Generation
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检索增强生成
- en: One of the typical drawbacks of Large Language Models is that they are trained
    offline and thus do not have information on events that happened after the training
    data was collected. Similarly, they cannot use any specific knowledge that was
    not present in the training data. This can be problematic for specific domains
    as the data used to train LLMs usually comes from general-domain corpora. One
    way to circumvent these problems, without requiring expensive fine-tuning, is
    Retrieval Augmented Generation (RAG). RAG works by augmenting the prompts fed
    to an LLM with external textual information. This is usually retrieved from an
    external data source by relevance to the current prompt. In practice, as a first
    step, this involves transforming the prompt and the external texts into vector
    embeddings. The latter can be obtained by pooling the output of a transformer
    encoder model (such as BERT) trained to map texts with similar meanings to embeddings
    that are close to each other according to a suitable metric. In the case of long
    texts, they can be split into chunks that are embedded individually, leading to
    the retrieval of the most relevant passages. Next, the texts whose embeddings
    are closest to the prompt embedding are retrieved. The concatenation of the prompt
    and the retrieved text, after suitable formatting, is given as input to the language
    model.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型的一个典型缺点是它们离线训练，因此没有关于训练数据收集后发生的事件的信息。同样，它们不能使用训练数据中未包含的特定知识。这对于特定领域可能是个问题，因为用于训练LLMs的数据通常来自一般领域的语料库。绕过这些问题的一种方法是检索增强生成（RAG），而无需昂贵的微调。RAG通过将外部文本信息增强到输入的提示中来工作。这通常是通过与当前提示相关的外部数据源进行检索来实现的。在实践中，第一步涉及将提示和外部文本转换为向量嵌入。后者可以通过汇聚一个训练过的转换器编码器模型（如BERT）的输出获得，该模型将具有相似意义的文本映射到根据适当度量接近的嵌入中。在长文本的情况下，可以将其拆分为单独嵌入的块，从而检索出最相关的段落。接下来，检索到的文本的嵌入与提示嵌入最接近。经过适当格式化的提示和检索文本的连接将作为输入提供给语言模型。
- en: With Retrieval Augmented Generation the model can access information that was
    not available during training and will base its answers on a selected corpus of
    text. RAG also makes it possible to inspect the sources the model used to answer,
    allowing for a more straightforward evaluation of the model outputs by a human
    user.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检索增强生成（RAG），模型可以访问训练期间不可用的信息，并基于选定的文本语料库来回答问题。RAG还使得检查模型用于回答的来源成为可能，从而使人类用户对模型输出的评估更加直接。
- en: In this blog post, I will explain how to create a simple agent capable of basing
    its answers on content retrieved from Wikipedia to demonstrate the ability of
    LLMs to seek and use external information. Given a prompt by the user, the model
    will search for appropriate pages on Wikipedia and base its answers on their content.
    I made the full code available in [this GitHub repo](https://github.com/GabrieleSgroi/wiki_llama).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我将解释如何创建一个简单的代理，能够基于从维基百科检索到的内容来回答问题，以展示LLMs寻求和使用外部信息的能力。给定用户的提示，模型将搜索适当的维基百科页面，并基于其内容进行回答。我在[这个GitHub仓库](https://github.com/GabrieleSgroi/wiki_llama)中提供了完整的代码。
- en: A Llama 2 agent augmented with Wikipedia content
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 增强了维基百科内容的Llama 2代理
- en: In this section, I will describe the steps needed to create a simple Llama 2
    agent that answers questions based on information retrieved from Wikipedia. In
    particular, the agent will…
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将描述创建一个简单的Llama 2代理的步骤，该代理基于从维基百科检索到的信息来回答问题。具体来说，代理将…
- en: Create appropriate queries to search pages on Wikipedia that are relevant to
    the user’s question.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建适当的查询以搜索与用户问题相关的维基百科页面。
- en: Retrieve, from the pages found on Wikipedia, the one with the content most relevant
    to the user’s question.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从维基百科找到的页面中检索出内容与用户问题最相关的页面。
- en: Extract, from the retrieved page, the most relevant passages to the user’s prompt.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从检索到的页面中提取与用户提示最相关的段落。
- en: Answer the user’s question based on the extracts from the page.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据页面摘录回答用户的问题。
- en: Notice that, more generally, the model could receive a prompt augmented with
    the full content of the most relevant page or with multiple extracts coming from
    different top pages ranked by relevance to the user’s prompt. While this could
    improve the quality of the response from the model, it will increase the required
    memory as it will inevitably lead to longer prompts. For simplicity, and in order
    to make a minimal example running on free-tier Google Colab GPUs, I have restricted
    the agent to use only a few extracts from the most relevant article.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，更一般来说，模型可能会收到一个增强的提示，包含最相关页面的完整内容或来自不同顶级页面的多个摘录，这些页面按照与用户提示的相关性进行排名。虽然这可以提高模型响应的质量，但它会增加所需的内存，因为它将不可避免地导致更长的提示。为了简单起见，并且为了在免费的Google
    Colab GPU上运行一个最小的示例，我限制了代理只使用来自最相关文章的几个摘录。
- en: Let us now delve into the various steps in more detail. The first step the agent
    needs to perform is to create a suitable search query to retrieve content from
    Wikipedia that contains information to answer the user’s prompt. In order to do
    that, we will prompt a Llama 2 chat model asking it to return keywords that represent
    the user prompt. Before going into the specific prompt used, I will shortly recall
    the general prompt format for Llama 2 chat models.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入探讨各个步骤的详细信息。代理需要执行的第一步是创建一个合适的搜索查询，以从维基百科中检索包含回答用户提示信息的内容。为此，我们将提示一个Llama
    2聊天模型，要求它返回表示用户提示的关键词。在讨论具体的提示之前，我将简要回顾Llama 2聊天模型的通用提示格式。
- en: 'The template that was used during the training procedure for Llama 2 chat models
    has the following structure:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在Llama 2聊天模型的训练过程中使用的模板具有以下结构：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The {{ system prompt}} specifies the behavior of the chat model to subsequent
    prompts and can be useful to adapt the model response to different tasks. The
    {{user_message}} is the user’s prompt the model needs to answer.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '{{ system prompt}}指定了聊天模型对后续提示的行为，并可以用于将模型响应适应于不同任务。{{user_message}}是模型需要回答的用户提示。'
- en: 'Going back to the problem of obtaining search queries for Wikipedia, our agent
    will use a Llama 2 model with the following prompt:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 回到获取维基百科搜索查询的问题，我们的代理将使用带有以下提示的Llama 2模型：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '{prompt} will be replaced, before generation, by the user’s input. The example
    provided as part of the system prompt aims to leverage the in-context learning
    capabilities of the model. In-context learning refers to the model''s ability
    to solve new tasks based on a few demonstration examples provided as part of the
    prompt. In this way, the model can learn that we expect it to provide keywords
    relevant to the provided prompt separated by commas after the text *[query]*.
    The latter is used as a delimiter to distinguish the prompt from the answer in
    the example and it is also useful to extract the queries from the model output.
    It is already provided as part of the input so that the model will have to generate
    only what comes after it.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '{prompt} 在生成之前会被用户的输入替换。系统提示中提供的示例旨在利用模型的上下文学习能力。上下文学习指的是模型基于作为提示一部分提供的几个演示示例解决新任务的能力。通过这种方式，模型可以了解到我们希望它在文本
    *[query]* 后提供与提供的提示相关的关键词，并用逗号分隔。后者用于作为分隔符，以区分示例中的提示和答案，并且还用于从模型输出中提取查询。它已作为输入的一部分提供，因此模型只需要生成它之后的内容。'
- en: Once the queries are obtained from the model output, they are used to search
    Wikipedia and retrieve the metadata and text of the returned pages. In the code
    accompanying the post, I used the [wikipediapackage](https://pypi.org/project/wikipedia/),
    which is a simple Python package that wraps the [MediaWiki API](https://www.mediawiki.org/wiki/API),
    to search and retrieve the data from Wikipedia.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦从模型输出中获得查询，就可以用来搜索维基百科，检索返回页面的元数据和文本。在文章中附带的代码中，我使用了 [wikipediapackage](https://pypi.org/project/wikipedia/)，这是一个简单的
    Python 包，它封装了 [MediaWiki API](https://www.mediawiki.org/wiki/API)，用于从维基百科搜索和检索数据。
- en: After extracting the text from the search results, the most relevant page to
    the original user prompt is selected. This will re-align the retrieved information
    to the original user’s prompt, potentially eliminating divergences originating
    from the search queries generated by the model. In order to do so, both the user’s
    prompt and the summary of the pages from the search result are embedded and stored
    in a vector database. The article with the closest embedding to the user’s prompt
    is then retrieved. I used the sentence transformers *all-MiniLM-L6-v2* model as
    the embedding model and a FAISS vector database with the integration provided
    by the *langchain* package.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在从搜索结果中提取文本后，选择与原始用户提示最相关的页面。这将重新对齐检索到的信息与原始用户的提示，可能消除由模型生成的搜索查询引起的偏差。为此，用户的提示和搜索结果页面的摘要都会被嵌入并存储在向量数据库中。然后检索与用户提示嵌入最接近的文章。我使用了句子变换器
    *all-MiniLM-L6-v2* 模型作为嵌入模型，并使用了 *langchain* 包提供的 FAISS 向量数据库集成。
- en: Having found a relevant page from Wikipedia, since adding its whole text to
    the prompt could require a lot of memory (or surpass the model tokens limit for
    context length), our agent will find the most relevant extracts to augment the
    prompt. This is done by first splitting the page’s text into chunks, and then,
    as before, embedding them into a vector space and retrieving the ones closest
    to the prompt embedding. I used again the *all-MiniLM-L6-v2* model to embed the
    chunks and a FAISS vector database to store and retrieve them.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在找到相关的维基百科页面后，由于将其整个文本添加到提示中可能需要大量内存（或超过模型的上下文长度令牌限制），我们的代理将找到最相关的摘录以补充提示。这是通过首先将页面的文本拆分成块，然后像之前一样，将它们嵌入到向量空间中，并检索与提示嵌入最接近的块来完成的。我再次使用了
    *all-MiniLM-L6-v2* 模型来嵌入这些块，并使用 FAISS 向量数据库来存储和检索它们。
- en: Now that we obtained the retrieved passages from the article, we can combine
    them with the user’s prompt and feed them to the Llama 2 model to get an answer.
    The template used for the input is the following
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经从文章中获得了检索到的段落，我们可以将它们与用户的提示结合起来，输入到 Llama 2 模型中以获得答案。输入的模板如下：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Before generation, {prompt} is replaced by the user prompt, {title} by the title
    of the Wikipedia page, and {extracts} is replaced by the extracted passages. One
    could also provide a few examples to leverage again the in-context learning capabilities
    of the model, but it would make the prompt significantly longer increasing the
    memory requirements.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成之前，{prompt} 被替换为用户提示，{title} 被替换为维基百科页面的标题，{extracts} 被替换为提取的段落。也可以提供几个示例，以再次利用模型的上下文学习能力，但这会显著增加提示的长度，从而增加内存需求。
- en: Let us now check if the model is able to answer using up-to-date information
    that was not present during its training by extracting it from Wikipedia. The
    following example has been obtained using the smallest chat version of LLaMa 2
    with 7 billion parameters running on Google Colab free-tier GPUs.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们检查模型是否能够使用在训练过程中未包含的最新信息来回答问题，这些信息是从维基百科中提取的。以下示例是使用在Google Colab免费层GPU上运行的最小LLaMa
    2聊天版本（拥有70亿参数）获得的。
- en: 'I have given the model the following prompt:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我给模型提供了以下提示：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The model created the following query to search on Wikipedia:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 模型创建了以下查询以在维基百科上搜索：
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'After extracting the passages from the most relevant page, the model returned
    the following answer:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在提取了来自最相关页面的段落后，模型返回了以下回答：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The agent also returns the metadata and the extracts of the page it has used
    for its answer, allowing the user to check its correctness and go into more detail
    by reading the original page. Here is the metadata for the previous answer
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 该代理还返回了生成答案所用页面的元数据和摘录，允许用户检查其准确性，并通过阅读原始页面深入了解细节。以下是之前回答的元数据
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Conclusion
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this post, I explained how to create a simple agent that can respond to a
    user’s prompt by searching on Wikipedia and base its answer on the retrieved page.
    Despite its simplicity, the agent is able to provide up-to-date and accurate answers
    even with the smallest Llama 2 7B model. The agent also returns the extracts from
    the page it has used to generate its answer, allowing the user to check the correctness
    of the information provided by the model and to go into more detail by reading
    the full original page.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我解释了如何创建一个简单的代理，它可以通过在维基百科上搜索并基于检索到的页面来回应用户的提示。尽管它很简单，但即使是最小的Llama 2
    7B模型也能提供最新且准确的回答。该代理还返回了生成回答所用页面的摘录，允许用户检查模型提供信息的准确性，并通过阅读完整的原始页面深入了解细节。
- en: Wikipedia is an interesting playground to demonstrate the ability of an LLM
    agent to seek and use external information that was not present in the training
    data but the same approach can be applied in other settings where external knowledge
    is needed. This is the case, for example, for applications that require up-to-date
    answers, fields that need specific knowledge not present in the training data,
    or extraction of information from private documents. This approach also highlights
    the potential of collaboration between LLM and humans. The model can quickly return
    a meaningful answer searching for relevant information from a very large external
    knowledge base, while the human user can check the validity of the model answer
    and delve deeper into the matter by inspecting the original source.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 维基百科是展示LLM代理寻找和使用训练数据中不存在的外部信息能力的有趣场所，但同样的方法也可以应用于其他需要外部知识的场景。例如，这适用于需要最新答案的应用、需要训练数据中不存在的特定知识的领域，或从私有文档中提取信息的情况。这种方法还突显了LLM与人类合作的潜力。模型可以快速返回有意义的回答，通过从非常大的外部知识库中搜索相关信息，而人类用户可以检查模型答案的有效性，并通过检查原始来源深入了解问题。
- en: 'A straightforward improvement of the agent described in this post can be obtained
    by combining multiple extracts from different pages in order to provide a larger
    amount of information to the model. In fact, in case of complex prompts, it could
    be useful to extract information from more than one Wikipedia page. The resulting
    increase in memory requirements due to the longer contexts can be partially offset
    by implementing quantization techniques such as [GPTQ](https://arxiv.org/abs/2210.17323).
    The results could be further improved by giving the model the possibility to reason
    over the search results and the retrieved content before giving its final answer
    to the users, following for example the ReAct framework described in the paper
    [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629).
    That way, for example, it is possible to build a model that iteratively collects
    the most relevant passages from different pages, discarding the ones that are
    not needed and combining information from different topics.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '对本文中描述的智能体的直接改进可以通过结合来自不同页面的多个摘录来获得，以便向模型提供更多的信息。实际上，对于复杂的提示，提取多个维基百科页面的信息可能会很有用。由于上下文的增加导致内存需求的增加，可以通过实现量化技术来部分抵消这种增加，例如[GPTQ](https://arxiv.org/abs/2210.17323)。通过让模型在给出最终答案之前，能够在搜索结果和检索到的内容上进行推理，结果可能会进一步改善，例如可以遵循论文中描述的ReAct框架[ReAct:
    Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629)。这样，例如，可以构建一个模型，迭代地从不同页面中收集最相关的片段，丢弃不需要的片段，并结合来自不同主题的信息。'
- en: Thank you for reading!
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 谢谢阅读！
