- en: Which Quantization Method is Right for You? (GPTQ vs. GGUF vs. AWQ)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://towardsdatascience.com/which-quantization-method-is-right-for-you-gptq-vs-gguf-vs-awq-c4cd9d77d5be?source=collection_archive---------2-----------------------#2023-11-14](https://towardsdatascience.com/which-quantization-method-is-right-for-you-gptq-vs-gguf-vs-awq-c4cd9d77d5be?source=collection_archive---------2-----------------------#2023-11-14)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Exploring Pre-Quantized Large Language Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@maartengrootendorst?source=post_page-----c4cd9d77d5be--------------------------------)[![Maarten
    Grootendorst](../Images/58e24b9cf7e10ff1cd5ffd75a32d1a26.png)](https://medium.com/@maartengrootendorst?source=post_page-----c4cd9d77d5be--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c4cd9d77d5be--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c4cd9d77d5be--------------------------------)
    [Maarten Grootendorst](https://medium.com/@maartengrootendorst?source=post_page-----c4cd9d77d5be--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Â·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F22405c3b2875&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhich-quantization-method-is-right-for-you-gptq-vs-gguf-vs-awq-c4cd9d77d5be&user=Maarten+Grootendorst&userId=22405c3b2875&source=post_page-22405c3b2875----c4cd9d77d5be---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c4cd9d77d5be--------------------------------)
    Â·11 min readÂ·Nov 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc4cd9d77d5be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhich-quantization-method-is-right-for-you-gptq-vs-gguf-vs-awq-c4cd9d77d5be&user=Maarten+Grootendorst&userId=22405c3b2875&source=-----c4cd9d77d5be---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc4cd9d77d5be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhich-quantization-method-is-right-for-you-gptq-vs-gguf-vs-awq-c4cd9d77d5be&source=-----c4cd9d77d5be---------------------bookmark_footer-----------)![](../Images/c45770a07b8d0685c7ab123f15862789.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the last year, we have seen the Wild West of Large Language Models
    (LLMs). The pace at which new technology and models were released was astounding!
    As a result, we have many different standards and ways of working with LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will explore one such topic, namely loading your local LLM
    through several (quantization) standards. With sharding, quantization, and different
    saving and compression strategies, it is not easy to know which method is suitable
    for you.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the examples, we will use [Zephyr 7B](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta),
    a fine-tuned variant of Mistral 7B that was trained with [Direct Preference Optimization](https://arxiv.org/abs/2305.18290)
    (DPO).
  prefs: []
  type: TYPE_NORMAL
- en: 'ðŸ”¥ **TIP**: After each example of loading an LLM, it is advised to restart your
    notebook to prevent OutOfMemory errors. Loading multiple LLMs requires significant
    RAM/VRAM. You can reset memory by deleting the models and resetting your cache
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You can also follow along with the [**Google Colab Notebook**](https://colab.research.google.com/drive/1rt318Ew-5dDw21YZx2zK2vnxbsuDAchH?usp=sharing)
    to make sure everything works as intended.
  prefs: []
  type: TYPE_NORMAL
