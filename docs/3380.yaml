- en: Which Quantization Method is Right for You? (GPTQ vs. GGUF vs. AWQ)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å“ªç§é‡åŒ–æ–¹æ³•é€‚åˆä½ ï¼Ÿï¼ˆGPTQ vs. GGUF vs. AWQï¼‰
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/which-quantization-method-is-right-for-you-gptq-vs-gguf-vs-awq-c4cd9d77d5be?source=collection_archive---------2-----------------------#2023-11-14](https://towardsdatascience.com/which-quantization-method-is-right-for-you-gptq-vs-gguf-vs-awq-c4cd9d77d5be?source=collection_archive---------2-----------------------#2023-11-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/which-quantization-method-is-right-for-you-gptq-vs-gguf-vs-awq-c4cd9d77d5be?source=collection_archive---------2-----------------------#2023-11-14](https://towardsdatascience.com/which-quantization-method-is-right-for-you-gptq-vs-gguf-vs-awq-c4cd9d77d5be?source=collection_archive---------2-----------------------#2023-11-14)
- en: Exploring Pre-Quantized Large Language Models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¢ç´¢é¢„é‡åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹
- en: '[](https://medium.com/@maartengrootendorst?source=post_page-----c4cd9d77d5be--------------------------------)[![Maarten
    Grootendorst](../Images/58e24b9cf7e10ff1cd5ffd75a32d1a26.png)](https://medium.com/@maartengrootendorst?source=post_page-----c4cd9d77d5be--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c4cd9d77d5be--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c4cd9d77d5be--------------------------------)
    [Maarten Grootendorst](https://medium.com/@maartengrootendorst?source=post_page-----c4cd9d77d5be--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@maartengrootendorst?source=post_page-----c4cd9d77d5be--------------------------------)[![Maarten
    Grootendorst](../Images/58e24b9cf7e10ff1cd5ffd75a32d1a26.png)](https://medium.com/@maartengrootendorst?source=post_page-----c4cd9d77d5be--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c4cd9d77d5be--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c4cd9d77d5be--------------------------------)
    [Maarten Grootendorst](https://medium.com/@maartengrootendorst?source=post_page-----c4cd9d77d5be--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F22405c3b2875&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhich-quantization-method-is-right-for-you-gptq-vs-gguf-vs-awq-c4cd9d77d5be&user=Maarten+Grootendorst&userId=22405c3b2875&source=post_page-22405c3b2875----c4cd9d77d5be---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c4cd9d77d5be--------------------------------)
    Â·11 min readÂ·Nov 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc4cd9d77d5be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhich-quantization-method-is-right-for-you-gptq-vs-gguf-vs-awq-c4cd9d77d5be&user=Maarten+Grootendorst&userId=22405c3b2875&source=-----c4cd9d77d5be---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F22405c3b2875&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhich-quantization-method-is-right-for-you-gptq-vs-gguf-vs-awq-c4cd9d77d5be&user=Maarten+Grootendorst&userId=22405c3b2875&source=post_page-22405c3b2875----c4cd9d77d5be---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c4cd9d77d5be--------------------------------)
    Â·11åˆ†é’Ÿé˜…è¯»Â·2023å¹´11æœˆ14æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc4cd9d77d5be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhich-quantization-method-is-right-for-you-gptq-vs-gguf-vs-awq-c4cd9d77d5be&user=Maarten+Grootendorst&userId=22405c3b2875&source=-----c4cd9d77d5be---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc4cd9d77d5be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhich-quantization-method-is-right-for-you-gptq-vs-gguf-vs-awq-c4cd9d77d5be&source=-----c4cd9d77d5be---------------------bookmark_footer-----------)![](../Images/c45770a07b8d0685c7ab123f15862789.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc4cd9d77d5be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhich-quantization-method-is-right-for-you-gptq-vs-gguf-vs-awq-c4cd9d77d5be&source=-----c4cd9d77d5be---------------------bookmark_footer-----------)![](../Images/c45770a07b8d0685c7ab123f15862789.png)'
- en: Throughout the last year, we have seen the Wild West of Large Language Models
    (LLMs). The pace at which new technology and models were released was astounding!
    As a result, we have many different standards and ways of working with LLMs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿‡å»çš„ä¸€å¹´é‡Œï¼Œæˆ‘ä»¬è§è¯äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„â€œç‹‚é‡è¥¿éƒ¨â€ã€‚æ–°æŠ€æœ¯å’Œæ¨¡å‹å‘å¸ƒçš„é€Ÿåº¦ä»¤äººéœ‡æƒŠï¼å› æ­¤ï¼Œæˆ‘ä»¬æ‹¥æœ‰è®¸å¤šä¸åŒçš„æ ‡å‡†å’Œå¤„ç†LLMçš„æ–¹æ³•ã€‚
- en: In this article, we will explore one such topic, namely loading your local LLM
    through several (quantization) standards. With sharding, quantization, and different
    saving and compression strategies, it is not easy to know which method is suitable
    for you.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†**æ·±å…¥æ¢è®¨**ä¸€ä¸ªè¯é¢˜ï¼Œå³é€šè¿‡å‡ ç§ï¼ˆé‡åŒ–ï¼‰æ ‡å‡†æ¥åŠ è½½ä½ çš„æœ¬åœ°LLMã€‚ç”±äºåˆ†ç‰‡ã€é‡åŒ–ä»¥åŠä¸åŒçš„ä¿å­˜å’Œå‹ç¼©ç­–ç•¥ï¼Œä½¿å¾—é€‰æ‹©é€‚åˆä½ çš„æ–¹æ³•å˜å¾—ä¸é‚£ä¹ˆç®€å•ã€‚
- en: Throughout the examples, we will use [Zephyr 7B](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta),
    a fine-tuned variant of Mistral 7B that was trained with [Direct Preference Optimization](https://arxiv.org/abs/2305.18290)
    (DPO).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¤ºä¾‹è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨[Zephyr 7B](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)ï¼Œè¿™æ˜¯Mistral
    7Bçš„ä¸€ä¸ªç²¾è°ƒç‰ˆæœ¬ï¼Œç»è¿‡[Direct Preference Optimization](https://arxiv.org/abs/2305.18290)ï¼ˆDPOï¼‰è®­ç»ƒã€‚
- en: 'ğŸ”¥ **TIP**: After each example of loading an LLM, it is advised to restart your
    notebook to prevent OutOfMemory errors. Loading multiple LLMs requires significant
    RAM/VRAM. You can reset memory by deleting the models and resetting your cache
    like so:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”¥ **æç¤º**ï¼šåœ¨åŠ è½½æ¯ä¸ªLLMç¤ºä¾‹åï¼Œå»ºè®®é‡æ–°å¯åŠ¨ç¬”è®°æœ¬ï¼Œä»¥é˜²æ­¢å‡ºç°OutOfMemoryé”™è¯¯ã€‚åŠ è½½å¤šä¸ªLLMéœ€è¦å¤§é‡çš„RAM/VRAMã€‚ä½ å¯ä»¥é€šè¿‡åˆ é™¤æ¨¡å‹å¹¶é‡ç½®ç¼“å­˜æ¥é‡ç½®å†…å­˜ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You can also follow along with the [**Google Colab Notebook**](https://colab.research.google.com/drive/1rt318Ew-5dDw21YZx2zK2vnxbsuDAchH?usp=sharing)
    to make sure everything works as intended.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ è¿˜å¯ä»¥å‚è€ƒ[**Google Colab Notebook**](https://colab.research.google.com/drive/1rt318Ew-5dDw21YZx2zK2vnxbsuDAchH?usp=sharing)ä»¥ç¡®ä¿ä¸€åˆ‡æŒ‰é¢„æœŸå·¥ä½œã€‚
