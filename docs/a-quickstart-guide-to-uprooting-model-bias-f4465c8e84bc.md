# 《快速入门指南：如何根除模型偏见》

> 原文：[https://towardsdatascience.com/a-quickstart-guide-to-uprooting-model-bias-f4465c8e84bc?source=collection_archive---------12-----------------------#2023-01-19](https://towardsdatascience.com/a-quickstart-guide-to-uprooting-model-bias-f4465c8e84bc?source=collection_archive---------12-----------------------#2023-01-19)

![](../Images/af28ba3d159eb1a66dd25bd6ce62218b.png)

图片来源于作者

[](https://aparnadhinak.medium.com/?source=post_page-----f4465c8e84bc--------------------------------)[![Aparna Dhinakaran](../Images/e431ee69563ecb27c86f3428ba53574c.png)](https://aparnadhinak.medium.com/?source=post_page-----f4465c8e84bc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f4465c8e84bc--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f4465c8e84bc--------------------------------) [Aparna Dhinakaran](https://aparnadhinak.medium.com/?source=post_page-----f4465c8e84bc--------------------------------)

·

[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff32f85889f3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quickstart-guide-to-uprooting-model-bias-f4465c8e84bc&user=Aparna+Dhinakaran&userId=f32f85889f3a&source=post_page-f32f85889f3a----f4465c8e84bc---------------------post_header-----------) 发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f4465c8e84bc--------------------------------) ·12 分钟阅读·2023年1月19日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff4465c8e84bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quickstart-guide-to-uprooting-model-bias-f4465c8e84bc&user=Aparna+Dhinakaran&userId=f32f85889f3a&source=-----f4465c8e84bc---------------------clap_footer-----------)

--

[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff4465c8e84bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quickstart-guide-to-uprooting-model-bias-f4465c8e84bc&source=-----f4465c8e84bc---------------------bookmark_footer-----------)

*本文由 Arize AI 的机器学习工程师 Amber Roberts 共同撰写*

在今天的世界中，读到关于AI表现出歧视行为的新闻已是司空见惯。从反映[持续存在的住房歧视遗产](https://www.technologyreview.com/2020/10/20/1009452/ai-has-exacerbated-racial-bias-in-housing-could-it-help-eliminate-it-instead/)的房地产估值模型到在医疗保健中加剧[获取护理和健康结果不平等](https://www.hsph.harvard.edu/news/hsph-in-the-news/study-widely-used-health-care-algorithm-has-racial-bias/)的模型，不幸的是，例子很容易找到。随着机器学习（ML）模型变得更加复杂，这一问题的真实范围及其对边缘化群体的影响可能尚未完全了解。幸运的是，ML团队可以采取一些简单步骤，以根除ML生命周期中的有害模型偏差。

# 什么是模型偏差？

模型偏差是指机器学习模型在其预测中做出一致的、系统性的错误。模型往往倾向于系统地学习错误的信号，因为没有考虑数据中包含的所有信息。模型偏差可能导致算法错过数据输入（特征）和目标输出（预测）之间的相关关系。本质上，偏差发生在算法从数据集中学习适当信号的能力不足时。几十年来，机器学习中的偏差已被认为是一个潜在问题，但在将模型投入生产时，它仍然是机器学习研究人员和工程师面临的复杂而具挑战性的问题。

# 偏差如何进入模型？

偏差可能通过多种来源引入到机器学习过程中，并通过模型预测得到强化。在模型开发的各个阶段，数据不足、不一致的数据收集和不良的数据实践都可能导致模型决策中的偏差。虽然这些偏差通常是无意的，但它们的存在可能对机器学习系统产生重大影响，并导致灾难性结果——从[就业歧视](https://www.thomsonreuters.com/en-us/posts/legal/ai-enabled-anti-black-bias/)到医疗保健中的[误诊](https://www.who.int/publications/i/item/9789240029200)。如果你使用的机器学习管道包含固有的偏差，模型不仅会学习这些偏差，还可能加剧甚至放大它们。

识别、评估和解决可能影响结果的任何潜在偏差，是创建新的机器学习模型并在生产中维护它时的关键要求。作为机器学习从业者，我们有责任检查、监控、评估、调查和评估这些系统，以避免对模型决策有效性产生负面影响的偏差。

模型中的偏差原因涉及数据和模型本身。

## 表示偏差（数据）

+   *应用：* 由历史数据引入的偏差、历史偏差、样本失衡、受污染的例子

+   *示例：* 在新闻文章上训练的嵌入展现了社会中的性别刻板印象。

## 测量偏差（数据）

+   *应用：* 代理变量、样本量差异、特征有限

+   *示例：* 预测再犯可能性的代理测量导致黑人的刑罚比白人在同样罪行上的刑罚更严厉。

## 聚合偏差（模型）

+   *应用：* 为所有人群使用的单一聚合模型

+   *示例：* 如果你有一个单一的预测模型来预测特定疾病在少数群体中的发生概率，它可能会失败并表现出偏见。

## 评估偏差（模型）

+   *应用：* 用于评估的基准不代表一般人群。

+   *示例：* 如果你为加利福尼亚州的房价预测模型进行基准测试，然后尝试将其应用于南卡罗来纳州的房市，价格将会有偏差。

# 公平性前提

# 定义保护属性

当然，在定义谁是被保护的对象之前，你不能量化有害的模型偏差。

了解法律下的保护类别是一个好的第一步。大多数美国人可能知道1964年美国民权法案禁止基于种族和性别的歧视，但更少人知道其他属性——如遗传信息或国籍——也符合法律下的保护类别，并且当发生违规时可能会导致[数百万或数十亿美元](https://www.justice.gov/opa/pr/justice-department-reaches-settlement-wells-fargo-resulting-more-175-million-relief)的罚款。

法律合规只是一个起点。许多大型企业也[超越](https://purpose.businessroundtable.org/)这些法律要求，拥有额外的保护类别或公开承诺多样性和公平性。

![](../Images/faee444511a6ac4201789bca31f369ba.png)

图片来源：作者

# 定义公平性

一旦你清楚了所有相关司法管辖区的保护类别，下一步就是定义公平性是什么样的。虽然这是一个[复杂的话题](https://arxiv.org/pdf/1908.09635.pdf)，但一些基本原则可以帮助你。

![](../Images/dbb303dc1ceb8828d59d79a998492ee2.png)

图片来源：作者

一个主要的区别是群体（相等和成比例）公平性与个人公平性之间的差异。

+   **群体公平性** 的定义是保护属性获得相似的待遇或结果。

+   **个人公平性** 的定义是相似的个体获得相似的待遇或结果。

例如，假设一家银行正在评估100份抵押贷款申请。70份申请来自男性，30份来自女性。基于等比例的群体公平性，你会批准男性50%的申请（35份）和女性50%的申请（15份）。另一方面，基于等数量的群体公平性，50份批准会平均分配——男性25份，女性25份。如果某一组中贷款资格较高，这两种结果都可能被认为是不公平的。行业中一种主流的方法是确保每个群体的预测准确性相同——这是公平机会与准确性的衡量标准。

![](../Images/320e687084b7c1159388e585af72ea3a.png)

作者提供的图像

# 数据建模阶段哪些易受偏见影响？

模型公平性 [影响](https://www.oreilly.com/library/view/practical-fairness/9781492075721/) 数据建模管道的预处理、处理中和后处理阶段。公平干预，即采取措施确保模型不对某些群体产生偏见，应在该过程的每个阶段实施。

## 预处理

+   **这是什么？** 数据处理的最早阶段，此阶段将数据转换为机器学习模型的输入。

+   **为什么在此阶段对模型公平性进行干预？** 在机器学习生命周期的这一最早阶段进行干预，可以对数据建模过程及其后续指标产生重大影响。

+   **如何在此阶段实现公平干预？** 移除、遮蔽、模糊、重命名或替换敏感属性。

+   **示例：** 一家银行有意建立一个预测贷款违约的模型，可能会对数据进行抽样，以确保其包含来自不同种族、性别和地理位置的申请人的代表性混合，然后移除数据中的敏感变量，以防这些变量被用于模型中。

## 处理中

+   **这是什么？** 处理过程指的是任何改变机器学习模型训练过程的行动。

+   **为什么在此阶段对模型公平性进行干预？** 如果由于计算限制或对专有或许可数据的限制无法在最早阶段进行干预，那么在训练过程中进行干预是下一最佳选择。在此阶段干预可以让团队保持其训练数据集的原始状态，不被修改。

+   **如何在此阶段实现公平干预？** 通过向模型的损失函数中添加额外的项来进行模型正则化，以确保没有一个特征不公平地主导模型的决策。你还可以使用对抗模型来减少模型中的不公平或虚假信息。

+   **示例：** 一家医疗提供商训练一个预测患者结果的模型，可能会创建一个对抗模型，使用目标模型的输出预测患者的受保护类别。这是为了确保患者的个人信息（如收入、种族和性别）不是其医疗结果的预测因素。

## 后处理

+   **这是什么？** 发生在模型在处理数据上训练之后。

+   **为什么在这个阶段介入模型公平性？** 当团队从前一个团队继承一个模型却不了解该模型时，这可能是他们公平性干预的最早阶段。

+   **如何在这个阶段实现公平性干预？** 在决策被用户收到之前使决策公平；第三方审计工具和使用机器学习可观测性工具的偏见追踪可以提供帮助。

+   **示例：** 一家提供宽带服务的公司有一个预测客户流失的模型，希望确保在向客户提供折扣时不产生歧视。该公司在模型部署过程中实施了公平性检查，使用公平性特定指标。然后，偏见追踪监控模型在多样化客户群体上的表现，以确保它不对任何特定群体存在偏见，从而保持输出公平。如果存在算法偏见，决策会被平等化。

# 什么是主流模型公平性指标？

根据你的目标，有许多[模型公平性指标](http://www.datasciencepublicpolicy.org/our-work/tools-guides/aequitas/)是适用的。这里是主流指标的定义和建议，以及每种指标的使用场景。

在决定使用哪个[公平性指标](https://arize.com/blog-course/fairness-bias-metrics/)时，你必须考虑需要什么见解，以确保你的模型没有表现出歧视。关心公平性的团队——特别是那些在高度监管行业如健康、贷款、保险和金融服务领域工作的团队——通常希望看到他们的模型在敏感属性（如种族或性别）上是否公平和无偏。当模型存在偏见时，团队需要知道哪个群体经历了最多的偏见，以便采取措施。

为了理解你评估的时间段内的公平性指标值，许多公司使用四分之一规则。四分之一规则是[用于](https://www.eeoc.gov/laws/guidance/questions-and-answers-clarify-and-provide-common-interpretation-uniform-guidelines)的一个阈值，由像美国平等就业机会委员会这样的监管机构帮助识别对受保护类别的负面待遇。由于理想的公平性分数是1，当利用四分之一规则时，你通常会测量你的公平性指标分数是否落在0.8到1.25的范围之外。如果分数低于0.8或高于1.25，你的模型可能对所选的敏感群体存在算法偏见。

从上面的决策树可以看出，选择的公平性指标取决于你的模型是否解决了不同表现、相等数量或辅助行为的问题。

让我们看看这些指标的使用时间和场景（*注意：FP = 假阳性，TP = 真阳性，FN = 假阴性，TN = 真阴性*）。

## 召回平衡

+   **定义：** 衡量模型对于一个组与另一个组的“敏感性”，或模型正确预测真阳性的能力

+   **何时使用：** 如果子组中的召回率接近，则实现了召回平衡

+   **如何计算：** *召回平衡 = 召回_敏感组 / 召回_基准组* ; 召回 = TP / (TP + FN)

## 假阳性率平衡

+   **定义：** 衡量模型是否对敏感组的正类预测不准确，相对于基准组

+   **何时使用：** 如果子组中的假阳性率（假阳性数与总负数的比率）接近，则实现了假阳性率平衡

+   **如何计算：** *假阳性平衡 = FPR_受限组 / FPR_特权组*; 假阳性率 = FP / (FP + TN)

## 不平等影响

+   **定义：** 受保护类别的不利待遇的量化衡量

+   **何时使用：** 不平等影响，也称为比例平衡，用于检查不同组的结果比率是否与它们在总体中的比例相同

+   **如何计算：** 如果男性的工作机会率为50%，女性的工作机会率为25%，那么这两个率的比率为2，表示存在不平等影响

一旦通过咨询公平性树定义了业务问题背景中的公平性，你可以计算你的平衡得分，并使用五分之一规则来确定是否需要在模型开发流程的预处理、处理中或后处理阶段进行干预。有关要实现的平衡类型以展示算法中立性，请参见下文。

## 第一类平衡

+   **描述：** 在假发现率（FDR）平衡和假阳性率（FPR）平衡中的公平性

+   **计算：** *FDR = FP / (TP + FP)* ; *FPR = FP / (TN + FP)*

## 第二类平衡

+   **描述：** 在假遗漏率（FOR）平衡和假阴性率（FNR）平衡中的公平性

+   **计算：** *FOR = FN / (TN + FN)* ; *FNR = FN / (TP + FN)*

## 平均赔率

+   **描述：** 在假阳性率（FPR）平衡和真阳性率（TPR）平衡中的公平性

+   **计算：** *FPR = FP / (TN + FP)* ; *TPR = TP / (TP + FN)*

## 监督公平性

+   **描述：** 在第一类和第二类平衡中的公平性

+   **计算：** *见上文*

## 总体公平性

+   **描述：** 在混淆矩阵中使用的所有指标的公平性

+   **计算：** *FP, TP, FN, TN*

# 有哪些工具可以帮助应对模型偏见？

有多种工具被开发出来以帮助应对整个机器学习生命周期中的算法偏见。

# 模型构建与验证

大多数解决方案专注于处理模型开发的初始阶段，目的是在模型发布之前提供模型公平性检查。

工具示例：

+   Aequitas：一个开源偏差审计工具包，用于对机器学习模型进行歧视和偏差审计。

+   Arize AI (**完全披露**：我是Arize的联合创始人！)：提供模型公平性检查，比较训练基线和生产数据，并进行根本原因分析工作流。

+   IBM Fairness 360：一个开源工具包，帮助你通过审计检查、报告和缓解机器学习模型中的歧视和偏差。

+   Google的PAIR AI：提供多个针对特定用例的工具，包括一个用于缓解图像数据集公平性和偏差问题的工具，支持TensorFlow Datasets API。

尽管其中一些工具可以用于汇总公平性指标和事后解释（即模型解释性），这些对于审计很有用，但它们大多数并不适用于生产中的实时监控。

# 生产中的监控

在生产中监控公平性指标很重要，原因很简单：在部署的AI中，模型偏差发生是时间问题，而非是否发生的问题。[概念漂移](https://arize.com/model-drift/)、训练中未见的新模式、训练与服务偏差以及异常值挑战着即使是最先进的团队，这些团队在训练中表现完美且通过验证阶段的模型也难以避免。

以下是一些提供生产中实时公平性监控的平台：

+   Arize：提供自动监控和公平性检查，通过多维比较揭示模型特征和群体，帮助发现算法偏差。

+   DataRobot：监控如比例平衡的公平性指标，并通过工作流将生产数据与训练数据进行比较。

# 团队应如何解决模型偏差？

解决模型偏差的第一步是理解数据，确保团队拥有正确的工具，并确保组织治理到位以确保公平。团队需要了解数据建模管道中对偏差有价值的预处理、处理和后处理阶段，因此，需要在这些阶段之一（如果不是多个阶段）进行公平性干预。以下是组织在这些阶段实现公平性的一些步骤。

**步骤1：使受保护类别数据对模型构建者和维护生产模型的机器学习团队可用**

根据[最近的一项调查](https://arize.com/resource/survey-machine-learning-observability-results/)，79.7%的机器学习团队报告称，他们“缺乏根除偏差或伦理问题所需的受保护数据”，至少有时如此，近一半（42.1%）表示这至少有时是一个问题。这需要改变。正如一位研究人员[所说](https://fairmlbook.org/classification.html)，无视并不能实现公平。

**步骤 2：确保你拥有生产中公平性的可视化工具，理想情况下是在模型发布之前**

在模型构建阶段结合公平性检查与定期事后审计，在模型偏差可能导致现实世界伤害的情况下是远远不够的。持续监控和警报可以帮助揭示在现实世界中不可避免出现的盲点（未知的未知），并加快解决时间。当模型所有者和维护生产模型的机器学习工程师拥有优化的指导和工具时，良好的结果就会发生。

**步骤 3：成为内部变革推动者并迅速行动**

解决模型偏差不仅仅是机器学习的问题。许多挑战——例如公平性与业务结果之间的权衡或团队之间责任的分散——只能通过多个团队和高层的参与来解决。机器学习团队在构建一种多管齐下的方法方面处于良好的位置，该方法结合了专门构建的基础设施、治理和专门的工作组以确保问责。

# 结论

当然，这些步骤只是开始，行业在公平性方面还有很长的路要走。识别问题仅仅是战斗的一半；采取行动至关重要。在群体层面上，机器学习可观测性和快速追踪模型公平性问题的原因可以提供帮助，特别是在知道何时重新训练或恢复到以前的模型（或不使用模型）时。

## 联系我们

如果这篇博客引起了你的兴趣，并且你渴望了解更多关于[机器学习可观测性](https://arize.com/ml-observability/)和[模型监控](https://arize.com/model-monitoring/)的信息，请查看我们的其他[博客](https://arize.com/blog/)和[资源](https://arize.com/resource-hub/)! 随时[联系我们](https://arize.com/contact/)提出任何问题或意见，或[注册一个免费账户](https://app.arize.com/auth/join)，如果你有兴趣加入一个有趣的、明星般的工程团队，帮助模型在生产中取得成功，可以在[这里](https://arize.com/careers/)找到我们的开放职位！
