["```py\nfrom langchain.document_loaders import BSHTMLLoader\n\nloader = BSHTMLLoader(\"my_site.html\")\ndata = loader.load()\n```", "```py\nfrom langchain.document_loaders import PyPDFLoader\nloader = PyPDFLoader(\"https://cdn.openai.com/papers/DALL_E_3_System_Card.pdf\")\ndoc = loader.load()\n```", "```py\nfrom langchain.document_loaders import TextLoader, DirectoryLoader\n\ntext_loader_kwargs={'autodetect_encoding': True}\nloader = DirectoryLoader('./hotels/london', show_progress=True, \n    loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n\ndocs = loader.load()\nlen(docs)\n82\n```", "```py\nzen = '''\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one -- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!\n'''\n\nprint('Number of characters: %d' % len(zen))\nprint('Number of words: %d' % len(zen.replace('\\n', ' ').split(' ')))\nprint('Number of paragraphs: %d' % len(zen.split('\\n')))\n\n# Number of characters: 825\n# Number of words: 140\n# Number of paragraphs: 21\n```", "```py\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size = 300,\n    chunk_overlap  = 0,\n    length_function = len,\n    is_separator_regex = False,\n)\ntext_splitter.split_text(zen)\n```", "```py\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size = 300,\n    chunk_overlap  = 100,\n    length_function = len,\n    is_separator_regex = False,\n)\ntext_splitter.split_text(zen)\n```", "```py\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size = 50,\n    chunk_overlap  = 10,\n    length_function = len,\n    is_separator_regex = False,\n)\ntext_splitter.split_text(zen)\n```", "```py\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size = 50,\n    chunk_overlap  = 0,\n    length_function = len,\n    is_separator_regex = False,\n    separators=[\"\\n\\n\", \"\\n\", \", \", \" \", \"\"]\n)\ntext_splitter.split_text('''\\\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.''')\n```", "```py\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size = 50,\n    chunk_overlap  = 0,\n    length_function = len,\n    is_separator_regex = True,\n    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\, )\", \" \", \"\"]\n)\ntext_splitter.split_text('''\\\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.''')\n```", "```py\nfrom langchain.text_splitter import CharacterTextSplitter\n\ntext_splitter = CharacterTextSplitter(\n    separator = \"\\n\",\n    chunk_size = 1,\n    chunk_overlap  = 0,\n    length_function = lambda x: 1, # hack - usually len is used \n    is_separator_regex = False\n)\nsplit_docs = text_splitter.split_documents(docs)\nlen(split_docs) \n12890\n```", "```py\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nembedding = OpenAIEmbeddings()\n\ntext1 = 'Our room (standard one) was very clean and large.'\ntext2 = 'Weather in London was wonderful.'\ntext3 = 'The room I had was actually larger than those found in other hotels in the area, and was very well appointed.'\n\nemb1 = embedding.embed_query(text1)\nemb2 = embedding.embed_query(text2)\nemb3 = embedding.embed_query(text3)\n\nprint('''\nDistance 1 -> 2: %.2f\nDistance 1 -> 3: %.2f\nDistance 2-> 3: %.2f\n''' % (np.dot(emb1, emb2), np.dot(emb1, emb3), np.dot(emb2, emb3)))\n```", "```py\npip install chromadb\n```", "```py\nfrom langchain.vectorstores import Chroma\npersist_directory = 'vector_store'\n\nvectordb = Chroma.from_documents(\n    documents=split_docs,\n    embedding=embedding,\n    persist_directory=persist_directory\n)\n```", "```py\nembedding = OpenAIEmbeddings()\nvectordb = Chroma(\n    persist_directory=persist_directory,\n    embedding_function=embedding\n)\n```", "```py\nprint(vectordb._collection.count())\n12890\n```", "```py\nquery_docs = vectordb.similarity_search('politeness of staff', k=3)\n```", "```py\nquery_docs = vectordb.max_marginal_relevance_search('politeness of staff', \n    k = 3, fetch_k = 30)\n```", "```py\nquery_docs = vectordb.similarity_search('breakfast in Travelodge Farrigdon', \n  k=5,\n  filter = {'source': 'hotels/london/uk_england_london_travelodge_london_farringdon'}\n)\n```", "```py\nfrom langchain.llms import OpenAI\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever\nfrom langchain.chains.query_constructor.base import AttributeInfo\n\nmetadata_field_info = [\n    AttributeInfo(\n        name=\"source\",\n        description=\"All sources starts with 'hotels/london/uk_england_london_' \\\n          then goes hotel chain, constant 'london_' and location.\",\n        type=\"string\",\n    )\n]\n\ndocument_content_description = \"Customer reviews for hotels\"\nllm = OpenAI(temperature=0.1) # low temperature to make model more factual\n# by default 'text-davinci-003' is used\n\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectordb,\n    document_content_description,\n    metadata_field_info,\n    verbose=True\n)\n\nquestion = \"breakfast in Travelodge Farringdon\"\ndocs = retriever.get_relevant_documents(question, k = 5)\n```", "```py\nimport langchain \nlangchain.debug = True\n```", "```py\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import LLMChainExtractor\n\nllm = OpenAI(temperature=0)\ncompressor = LLMChainExtractor.from_llm(llm)\ncompression_retriever = ContextualCompressionRetriever(\n    base_compressor=compressor,\n    base_retriever=vectordb.as_retriever(search_type = \"mmr\",  \n      search_kwargs={\"k\": 3})\n)\n\nquestion = \"breakfast in Travelodge Farringdon\"\ncompressed_docs = compression_retriever.get_relevant_documents(question)\n```", "```py\nfrom langchain.chains import RetrievalQA\n\nfrom langchain.chat_models import ChatOpenAI\nllm = ChatOpenAI(model_name='gpt-4', temperature=0.1)\n\nqa_chain = RetrievalQA.from_chain_type(\n    llm,\n    retriever=vectordb.as_retriever(search_kwargs={\"k\": 3})\n)\n\nresult = qa_chain({\"query\": \"what customers like about staff in the hotel?\"})\n```", "```py\nfrom langchain.prompts import PromptTemplate\n\ntemplate = \"\"\"\nUse the following pieces of context to answer the question at the end. \nIf you don't know the answer, just say that you don't know, don't try \nto make up an answer. \nKeep the answer as concise as possible. Use 1 sentence to sum all points up.\n______________\n{context}\nQuestion: {question}\nHelpful Answer:\"\"\"\n\nQA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n\nqa_chain = RetrievalQA.from_chain_type(\n    llm,\n    retriever=vectordb.as_retriever(),\n    return_source_documents=True,\n    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n)\nresult = qa_chain({\"query\": \"what customers like about staff in the hotel?\"})\n```", "```py\nqa_chain_mr = RetrievalQA.from_chain_type(\n    llm,\n    retriever=vectordb.as_retriever(),\n    chain_type=\"map_reduce\"\n)\nresult = qa_chain_mr({\"query\": \"what customers like about staff in the hotel?\"})\n```", "```py\nqa_chain_refine = RetrievalQA.from_chain_type(\n    llm,\n    retriever=vectordb.as_retriever(),\n    chain_type=\"refine\"\n)\nresult = qa_chain_refine({\"query\": \"what customers like about staff in the hotel?\"})\n```"]