- en: 'The Research Agent: Addressing the Challenge of Answering Questions Based on
    a Large Text Corpus'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/the-research-agent-4ef8e6f1b741?source=collection_archive---------0-----------------------#2023-08-29](https://towardsdatascience.com/the-research-agent-4ef8e6f1b741?source=collection_archive---------0-----------------------#2023-08-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I made an **Autonomous AI Research Agent** that can answer difficult questions
    with deep multi-hop reasoning capabilities
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@rahul.nyk?source=post_page-----4ef8e6f1b741--------------------------------)[![Rahul
    Nayak](../Images/9f8aa2f9af4e02b31c114222756489e5.png)](https://medium.com/@rahul.nyk?source=post_page-----4ef8e6f1b741--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4ef8e6f1b741--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4ef8e6f1b741--------------------------------)
    [Rahul Nayak](https://medium.com/@rahul.nyk?source=post_page-----4ef8e6f1b741--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F473e87f4b733&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-research-agent-4ef8e6f1b741&user=Rahul+Nayak&userId=473e87f4b733&source=post_page-473e87f4b733----4ef8e6f1b741---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4ef8e6f1b741--------------------------------)
    Â·16 min readÂ·Aug 29, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4ef8e6f1b741&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-research-agent-4ef8e6f1b741&user=Rahul+Nayak&userId=473e87f4b733&source=-----4ef8e6f1b741---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4ef8e6f1b741&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-research-agent-4ef8e6f1b741&source=-----4ef8e6f1b741---------------------bookmark_footer-----------)![](../Images/91b348b741ea57a92d2e3b954a304f27.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Image by the Author (Generated using Photoshop Generative fill)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '**Introduction to the problem**'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 2021, I started working on the challenge of answering questions based on
    a large corpus of text. In the era before the pre-trained transformers, this problem
    was a tough one to crack.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: And to my frustration, I started my experiments with one of the most complex
    and intricate stories ever written, The Mahabharata. For those unfamiliar with
    the work, the Mahabharata is a collection of 18 books with a total of about 1.8
    million words. It is the largest poem ever written with about 90,000 verses. It
    is roughly ten times the length of the Iliad and the Odyssey combined. But it
    is not only the length but also the breadth of Mahabharata that is staggering.
    Highly nonlinear and complex in its causes and effects, it has thousands of characters
    spanning seven generations, and out of those, not a single one is completely good
    or evil. It has profound philosophical commentaries on Duty (Karma), Choices and
    Human existence, especially on the conflicts of duties and the choices between
    multiple wrongs. The Bhagavad Gita (Key philosophy of Hinduism) is also a part
    of the 6th book of the Mahabharata.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¤æˆ‘æ²®ä¸§çš„æ˜¯ï¼Œæˆ‘å¼€å§‹ç”¨ã€Šæ‘©è¯ƒå©†ç½—å¤šã€‹è¿™ä¸€æœ€å¤æ‚ã€æœ€ç²¾ç»†çš„æ•…äº‹ä¹‹ä¸€è¿›è¡Œå®éªŒã€‚å¯¹äºé‚£äº›ä¸ç†Ÿæ‚‰è¿™éƒ¨ä½œå“çš„äººæ¥è¯´ï¼Œã€Šæ‘©è¯ƒå©†ç½—å¤šã€‹æ˜¯ä¸€éƒ¨ç”±18æœ¬ä¹¦ç»„æˆçš„æ€»å­—æ•°çº¦ä¸º180ä¸‡çš„ä½œå“ã€‚å®ƒæ˜¯æœ‰å²ä»¥æ¥æœ€é•¿çš„è¯—æ­Œï¼Œå¤§çº¦æœ‰90,000èŠ‚ã€‚å®ƒçš„é•¿åº¦å¤§çº¦æ˜¯ã€Šä¼Šåˆ©äºšç‰¹ã€‹å’Œã€Šå¥¥å¾·èµ›ã€‹æ€»å’Œçš„åå€ã€‚ä½†ã€Šæ‘©è¯ƒå©†ç½—å¤šã€‹çš„éœ‡æ’¼ä¸ä»…åœ¨äºé•¿åº¦ï¼Œè¿˜æœ‰å…¶å¹¿åº¦ã€‚å®ƒåœ¨å› æœå…³ç³»ä¸Šé«˜åº¦éçº¿æ€§å’Œå¤æ‚ï¼Œæ‹¥æœ‰è·¨è¶Šä¸ƒä»£çš„æ•°åƒä¸ªè§’è‰²ï¼Œè€Œè¿™äº›è§’è‰²ä¸­æ²¡æœ‰ä¸€ä¸ªæ˜¯å®Œå…¨å–„è‰¯æˆ–é‚ªæ¶çš„ã€‚å®ƒå¯¹èŒè´£ï¼ˆä¸šï¼‰ã€é€‰æ‹©å’Œäººç±»å­˜åœ¨ï¼Œå°¤å…¶æ˜¯èŒè´£å†²çªå’Œå¤šé‡é”™è¯¯é€‰æ‹©çš„å†²çªè¿›è¡Œäº†æ·±åˆ»çš„å“²å­¦è¯„è®ºã€‚ã€Šåšä¼½æ¢µæ­Œã€‹ï¼ˆå°åº¦æ•™çš„æ ¸å¿ƒå“²å­¦ï¼‰ä¹Ÿæ˜¯ã€Šæ‘©è¯ƒå©†ç½—å¤šã€‹ç¬¬å…­æœ¬ä¹¦çš„ä¸€éƒ¨åˆ†ã€‚
- en: I compiled the [Mahabharata text data](https://github.com/rahulnyk/mahabharata.git)
    from multiple sources online into a clean data set. However, I could not find
    a method to implement meaningful QA on the text.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†æ¥è‡ªå¤šä¸ªåœ¨çº¿æ¥æºçš„[ã€Šæ‘©è¯ƒå©†ç½—å¤šã€‹æ–‡æœ¬æ•°æ®](https://github.com/rahulnyk/mahabharata.git)æ•´ç†æˆäº†ä¸€ä¸ªå¹²å‡€çš„æ•°æ®é›†ã€‚ç„¶è€Œï¼Œæˆ‘æ‰¾ä¸åˆ°ä¸€ç§æ–¹æ³•æ¥å¯¹è¿™äº›æ–‡æœ¬å®æ–½æœ‰æ„ä¹‰çš„é—®ç­”ã€‚
- en: In less than two years, all that changed.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸åˆ°ä¸¤å¹´çš„æ—¶é—´ï¼Œä¸€åˆ‡éƒ½å‘ç”Ÿäº†å˜åŒ–ã€‚
- en: The rapid advancements in AI and Large pre-trained transformers are changing
    the world of technology profoundly and fundamentally. And I am fascinated by it,
    much like most techies these days are.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: AI å’Œå¤§å‹é¢„è®­ç»ƒå˜æ¢å™¨çš„å¿«é€Ÿè¿›æ­¥æ­£åœ¨æ·±åˆ»è€Œæ ¹æœ¬åœ°æ”¹å˜æŠ€æœ¯ä¸–ç•Œã€‚æˆ‘å¯¹è¿™ä¸€ç‚¹æ„Ÿåˆ°ç€è¿·ï¼Œå°±åƒç°åœ¨å¤§å¤šæ•°æŠ€æœ¯äººå‘˜ä¸€æ ·ã€‚
- en: So, a few months ago, I returned to the problem with a naive knowledge of the
    newborn art of prompt engineering. But this time with a general idea of making
    an **Autonomous** **Research Agent** that can work with any complex knowledge
    base.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å‡ ä¸ªæœˆå‰ï¼Œæˆ‘å¸¦ç€å¯¹æ–°å…´çš„æç¤ºå·¥ç¨‹è‰ºæœ¯çš„åˆæ­¥äº†è§£é‡æ–°å®¡è§†äº†è¿™ä¸ªé—®é¢˜ã€‚ä½†è¿™æ¬¡æˆ‘æœ‰äº†ä¸€ä¸ªå¤§è‡´çš„æƒ³æ³•ï¼Œå³åˆ›å»ºä¸€ä¸ªå¯ä»¥å¤„ç†ä»»ä½•å¤æ‚çŸ¥è¯†åº“çš„**è‡ªä¸»** **ç ”ç©¶ä»£ç†**ã€‚
- en: The Mahabharata is one of the most complex use cases. However, in every domain
    of knowledge, Law, Scientific research, Education, Medical, etc., every project
    starts with deep research on the prior art. So the problem is worthy enough to
    solve.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ã€Šæ‘©è¯ƒå©†ç½—å¤šã€‹æ˜¯æœ€å¤æ‚çš„ç”¨ä¾‹ä¹‹ä¸€ã€‚ç„¶è€Œï¼Œåœ¨æ¯ä¸ªçŸ¥è¯†é¢†åŸŸï¼Œå¦‚æ³•å¾‹ã€ç§‘å­¦ç ”ç©¶ã€æ•™è‚²ã€åŒ»ç–—ç­‰ï¼Œæ¯ä¸ªé¡¹ç›®éƒ½ä»å¯¹å‰äººå·¥ä½œçš„æ·±å…¥ç ”ç©¶å¼€å§‹ã€‚å› æ­¤ï¼Œè¿™ä¸ªé—®é¢˜æ˜¯å€¼å¾—è§£å†³çš„ã€‚
- en: '**The Research Agent**'
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ç ”ç©¶ä»£ç†**'
- en: Here, I will discuss the design and implementation of an Autonomous **AI Research
    Agent** that can solve the problem of multi-hop KBQA with deep reasoning capability.
    I will share the git repo with an initial implementation of the research agent
    in a Python notebook. If you are interested only in that part, please feel free
    to skip to the Implementation section later in this article.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘å°†è®¨è®ºä¸€ä¸ªå¯ä»¥è§£å†³å¤šè·³ KBQA é—®é¢˜å¹¶å…·å¤‡æ·±åº¦æ¨ç†èƒ½åŠ›çš„**è‡ªä¸» AI ç ”ç©¶ä»£ç†**çš„è®¾è®¡å’Œå®æ–½ã€‚æˆ‘ä¼šåˆ†äº«ä¸€ä¸ªåŒ…å«ç ”ç©¶ä»£ç†åˆæ­¥å®ç°çš„ Python
    ç¬”è®°æœ¬çš„ git ä»“åº“ã€‚å¦‚æœä½ å¯¹è¿™éƒ¨åˆ†å†…å®¹æ„Ÿå…´è¶£ï¼Œè¯·éšæ—¶è·³åˆ°æœ¬æ–‡åé¢çš„å®æ–½éƒ¨åˆ†ã€‚
- en: If you are interested in knowing more about AI Agent, â€˜Knowledge-Based Question
    Answerâ€™ (KBQA), the â€˜Whyâ€™, the â€˜Whatâ€™, and the design evolution of the AI Research
    Agent, then please read along.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å¯¹äº†è§£æ›´å¤šå…³äº AI ä»£ç†ã€â€˜åŸºäºçŸ¥è¯†çš„é—®é¢˜å›ç­”â€™ï¼ˆKBQAï¼‰ã€â€˜ä¸ºä»€ä¹ˆâ€™ã€â€˜ä»€ä¹ˆâ€™ä»¥åŠ AI ç ”ç©¶ä»£ç†çš„è®¾è®¡æ¼”å˜æ„Ÿå…´è¶£ï¼Œè¯·ç»§ç»­é˜…è¯»ã€‚
- en: '**Why?**'
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ä¸ºä»€ä¹ˆï¼Ÿ**'
- en: The first question that one may ask is why not just use the ChatGPT interface
    and ask questions. It has been trained on a humungous volume of Internet data
    generated till 2021, so a text corpus like the Mahabharata is known to it.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å¯èƒ½æœ‰äººä¼šé—®ï¼Œä¸ºä»€ä¹ˆä¸ç›´æ¥ä½¿ç”¨ ChatGPT æ¥å£æé—®ã€‚å®ƒå·²ç»åœ¨2021å¹´ä¹‹å‰ç”Ÿæˆçš„å¤§é‡äº’è”ç½‘æ•°æ®ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œå› æ­¤åƒã€Šæ‘©è¯ƒå©†ç½—å¤šã€‹è¿™æ ·çš„æ–‡æœ¬è¯­æ–™åº“å¯¹å®ƒæ¥è¯´æ˜¯å·²çŸ¥çš„ã€‚
- en: That was my first approach. I asked the ChatGPT several questions about the
    Mahabharata. I got good answers to some questions. However, they lack the rigour
    for the most. And that is expected. The GPT is trained over general data sets.
    It can very well understand and interpret natural languages. It can also reason
    well enough. However, it is not an expert in any specific domain. So, while it
    might have some knowledge of The Mahabharata, it may not respond with deeply researched
    answers. At times the GPT may not have any answer at all. In those cases, it either
    humbly refuses to answer the question, or confidently makes them up (Hallucinations).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘çš„ç¬¬ä¸€ç§æ–¹æ³•ã€‚æˆ‘é—® ChatGPT å…³äºã€Šæ‘©è¯ƒå©†ç½—å¤šã€‹çš„å‡ ä¸ªé—®é¢˜ã€‚æˆ‘å¾—åˆ°äº†å¯¹ä¸€äº›é—®é¢˜çš„è‰¯å¥½å›ç­”ã€‚ç„¶è€Œï¼Œå¤§éƒ¨åˆ†ç­”æ¡ˆç¼ºä¹ä¸¥è°¨æ€§ã€‚è¿™æ˜¯å¯ä»¥é¢„æ–™çš„ã€‚GPT
    æ˜¯åœ¨é€šç”¨æ•°æ®é›†ä¸Šè®­ç»ƒçš„ã€‚å®ƒå¯ä»¥å¾ˆå¥½åœ°ç†è§£å’Œè§£é‡Šè‡ªç„¶è¯­è¨€ã€‚å®ƒçš„æ¨ç†èƒ½åŠ›ä¹Ÿè¶³å¤Ÿå¥½ã€‚ç„¶è€Œï¼Œå®ƒåœ¨ä»»ä½•ç‰¹å®šé¢†åŸŸéƒ½ä¸æ˜¯ä¸“å®¶ã€‚æ‰€ä»¥ï¼Œå°½ç®¡å®ƒå¯èƒ½å¯¹ã€Šæ‘©è¯ƒå©†ç½—å¤šã€‹æœ‰ä¸€äº›äº†è§£ï¼Œä½†å¯èƒ½ä¸ä¼šç»™å‡ºæ·±å…¥ç ”ç©¶çš„ç­”æ¡ˆã€‚æœ‰æ—¶
    GPT ç”šè‡³å¯èƒ½æ²¡æœ‰ä»»ä½•ç­”æ¡ˆã€‚åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œå®ƒè¦ä¹ˆè°¦è™šåœ°æ‹’ç»å›ç­”é—®é¢˜ï¼Œè¦ä¹ˆè‡ªä¿¡åœ°ç¼–é€ ç­”æ¡ˆï¼ˆå¹»è§‰ï¼‰ã€‚
- en: The second most obvious way to achieve KBQA is to use a Retrieval QA Prompt.
    Here is where LangChain starts being extremely useful.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å®ç° KBQA çš„ç¬¬äºŒç§æœ€æ˜æ˜¾çš„æ–¹æ³•æ˜¯ä½¿ç”¨æ£€ç´¢é—®ç­”æç¤ºã€‚è¿™æ—¶ï¼ŒLangChain æ˜¾å¾—æä¸ºæœ‰ç”¨ã€‚
- en: '**Retrieval QA**'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**æ£€ç´¢é—®ç­”**'
- en: For those unfamiliar with the LangChain library, It is one of the best ways
    to use LLMs like GPT in your code. Here is an implementation of KBQA using LangChain.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºé‚£äº›ä¸ç†Ÿæ‚‰ LangChain åº“çš„äººæ¥è¯´ï¼Œå®ƒæ˜¯å°† LLMsï¼ˆå¦‚ GPTï¼‰ç”¨äºä»£ç ä¸­çš„æœ€ä½³æ–¹å¼ä¹‹ä¸€ã€‚è¿™é‡Œæ˜¯ä½¿ç”¨ LangChain å®ç° KBQA
    çš„æ–¹æ³•ã€‚
- en: '[](https://python.langchain.com/docs/use_cases/question_answering/how_to/vector_db_qa?source=post_page-----4ef8e6f1b741--------------------------------)
    [## QA using a Retriever | ğŸ¦œï¸ğŸ”— Langchain'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[## ä½¿ç”¨æ£€ç´¢å™¨è¿›è¡Œé—®ç­” | ğŸ¦œï¸ğŸ”— Langchain](https://python.langchain.com/docs/use_cases/question_answering/how_to/vector_db_qa?source=post_page-----4ef8e6f1b741--------------------------------)'
- en: This example showcases question answering over an index.
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†åœ¨ç´¢å¼•ä¸Šè¿›è¡Œé—®ç­”ã€‚
- en: python.langchain.com](https://python.langchain.com/docs/use_cases/question_answering/how_to/vector_db_qa?source=post_page-----4ef8e6f1b741--------------------------------)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[python.langchain.com](https://python.langchain.com/docs/use_cases/question_answering/how_to/vector_db_qa?source=post_page-----4ef8e6f1b741--------------------------------)'
- en: To summarise, here are the steps to achieve KBQA on any body of documents
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“ä¸€ä¸‹ï¼Œå®ç°ä»»ä½•æ–‡æ¡£é›†åˆä¸Šçš„ KBQA çš„æ­¥éª¤å¦‚ä¸‹
- en: Split the knowledge base into text chunks.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†çŸ¥è¯†åº“æ‹†åˆ†æˆæ–‡æœ¬å—ã€‚
- en: Create a numerical representation (Embeddings) for each chunk and save them
    to a vector database.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºæ¯ä¸ªæ–‡æœ¬å—åˆ›å»ºæ•°å€¼è¡¨ç¤ºï¼ˆåµŒå…¥ï¼‰å¹¶å°†å…¶ä¿å­˜åˆ°å‘é‡æ•°æ®åº“ä¸­ã€‚
- en: '*If your data is static, Steps 1 and 2 are one-time efforts.*'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*å¦‚æœä½ çš„æ•°æ®æ˜¯é™æ€çš„ï¼Œç¬¬ 1 æ­¥å’Œç¬¬ 2 æ­¥æ˜¯ä¸€æ¬¡æ€§çš„å·¥ä½œã€‚*'
- en: Run a semantic search using the userâ€™s query on this database and fetch relevant
    text chunks.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ç”¨æˆ·çš„æŸ¥è¯¢åœ¨è¿™ä¸ªæ•°æ®åº“ä¸Šè¿›è¡Œè¯­ä¹‰æœç´¢ï¼Œå¹¶è·å–ç›¸å…³çš„æ–‡æœ¬å—ã€‚
- en: Send these text chunks to the LLM along with the userâ€™s questions and ask them
    to Answer.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†è¿™äº›æ–‡æœ¬å—ä¸ç”¨æˆ·çš„é—®é¢˜ä¸€èµ·å‘é€åˆ° LLMï¼Œå¹¶è¦æ±‚å…¶å›ç­”ã€‚
- en: Here is a graphical representation of this process.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯è¯¥è¿‡ç¨‹çš„å›¾ç¤ºè¡¨ç¤ºã€‚
- en: '![](../Images/63234bd045fe01ff4f4ca296ff725645.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63234bd045fe01ff4f4ca296ff725645.png)'
- en: Image by the Author created using draw.io
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…é€šè¿‡ draw.io åˆ›å»ºçš„å›¾ç‰‡
- en: So why go any further? It seems like a solved problem!
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œä¸ºä»€ä¹ˆè¦ç»§ç»­å‘¢ï¼Ÿè¿™ä¼¼ä¹æ˜¯ä¸€ä¸ªå·²è§£å†³çš„é—®é¢˜ï¼
- en: Not quite ğŸ™
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å¤ªå¯¹ ğŸ™
- en: This approach works well for simple questions on a simple and factual knowledge
    base. However, it does not work for a more complex knowledge base and more complicated
    questions that require deeper, Multi-hop, reasoning. Multi-hop reasoning refers
    to a process in which multiple steps of logical or contextual inference are taken
    to arrive at a conclusion or answer to a question.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•é€‚ç”¨äºç®€å•é—®é¢˜å’Œç®€å•çš„äº‹å®çŸ¥è¯†åº“ã€‚ç„¶è€Œï¼Œå¯¹äºéœ€è¦æ›´æ·±å±‚æ¬¡ã€å¤šè·³æ¨ç†çš„å¤æ‚çŸ¥è¯†åº“å’Œæ›´å¤æ‚çš„é—®é¢˜åˆ™ä¸é€‚ç”¨ã€‚å¤šè·³æ¨ç†æŒ‡çš„æ˜¯é€šè¿‡å¤šä¸ªé€»è¾‘æˆ–ä¸Šä¸‹æ–‡æ¨ç†æ­¥éª¤æ¥å¾—å‡ºç»“è®ºæˆ–å›ç­”é—®é¢˜çš„è¿‡ç¨‹ã€‚
- en: Moreover, the LLMs are limited in the length of text they can chew in one prompt.
    You can, of course, send the documents one at a time and then â€˜refineâ€™ or â€˜reduceâ€™
    the answer with every call. However, this approach does not allow for complex
    â€˜multi-hopâ€™ reasoning. In some cases, the results using the â€˜refineâ€™ or â€˜reduceâ€™
    approach are better than simply stuffing all the documents in a single prompt,
    but not by a high margin.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼ŒLLMs åœ¨ä¸€æ¬¡æç¤ºä¸­å¤„ç†æ–‡æœ¬çš„é•¿åº¦æ˜¯æœ‰é™çš„ã€‚ä½ å¯ä»¥å½“ç„¶é€ä¸ªå‘é€æ–‡æ¡£ï¼Œç„¶åé€šè¿‡æ¯æ¬¡è°ƒç”¨æ¥â€˜ç»†åŒ–â€™æˆ–â€˜ç¼©å‡â€™ç­”æ¡ˆã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä¸å…è®¸è¿›è¡Œå¤æ‚çš„â€˜å¤šè·³â€™æ¨ç†ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä½¿ç”¨â€˜ç»†åŒ–â€™æˆ–â€˜ç¼©å‡â€™æ–¹æ³•å¾—åˆ°çš„ç»“æœæ¯”ä»…ä»…å°†æ‰€æœ‰æ–‡æ¡£å¡è¿›ä¸€ä¸ªæç¤ºä¸­è¦å¥½ï¼Œä½†å·®è·ä¸å¤§ã€‚
- en: For a complex knowledge base, the usersâ€™ question by itself may not be enough
    to find all the relevant documents that can help the LLM arrive at an accurate
    answer.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Who was Arjuna?
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This is a simple question and can be answered with limited context. However,
    the following question:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Why did the Mahabharata war happen?
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Is a question that has its context spread all across the text corpus. The question
    itself has limited information about its context. To find the relevant chunks
    of text and then to reason based on that may not work.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: So what next?
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '**AI Agents**'
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is one of the coolest concepts that has emerged after the advent of AI.
    If you donâ€™t know the concept of an AI Agent, I canâ€™t wait to explain it to you,
    but I may still fail to convey its awesomeness. Let me use ChatGPT to explain
    it first.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: An AI agent, also known simply as an â€œagent,â€ refers to a software program or
    system that can autonomously perceive its environment, make decisions, and take
    actions to achieve specific goals. AI agents are designed to mimic human-like
    behaviour in problem-solving and decision-making tasks. They operate within a
    defined environment and interact with that environment to achieve desired outcomes.
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Simply speaking, an Agent is a program that takes a problem, decides how to
    solve it, and then solves it. The Agent is provided with a set of tools like Functions,
    methods, API calls, etc. It can use any of them if it chooses to do so in any
    sequence it deems fit. Contrast this to conventional software, where the sequence
    of steps needed to solve the problem is pre-programmed. This is, of course, a
    very vague definition. But you probably get the hang of it by now.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Here are the two different agents I tried for our KBQA use case.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '***React*** This Agent uses a [â€˜ReActâ€™ (Reason and Action) style of reasoning](https://www.promptingguide.ai/techniques/react)
    to decide which tool to use for the given problem.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the langChain implementation of a ReAct Agent:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://python.langchain.com/docs/modules/agents/agent_types/react?source=post_page-----4ef8e6f1b741--------------------------------)
    [## ReAct | ğŸ¦œï¸ğŸ”— Langchain'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: This walkthrough showcases using an agent to implement the ReAct logic.
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: python.langchain.com](https://python.langchain.com/docs/modules/agents/agent_types/react?source=post_page-----4ef8e6f1b741--------------------------------)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'I provided the Agent with the following tools to choose from:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval QA chain with a document store.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Character Glossary search (I created a glossary with Named Entity Recognition
    using a pre-trained model)
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wikipedia search.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The react agent did not give me good results and failed to converge to any answer
    most of the time. It does not work well with GPT 3.5\. It may work better with
    GPT 4, which is 20 -30 times more expensive than GPT 3.5, so that may not be an
    option yet.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Even when it converged, I could not get good results. Someone more knowledgeable
    in creating â€˜reactâ€™ prompts probably would have done better.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿å®ƒæ”¶æ•›äº†ï¼Œæˆ‘ä¹Ÿæ— æ³•è·å¾—å¥½çš„ç»“æœã€‚å¯èƒ½åœ¨åˆ›å»ºâ€œååº”â€æç¤ºæ–¹é¢æ›´æœ‰ç»éªŒçš„äººä¼šåšå¾—æ›´å¥½ã€‚
- en: '***Self-Ask Agent*** This agent asks follow-up questions based on the original
    question and then tries to find the intermediate answers. Using these intermediate
    answers, it finally arrives at a final answer. Here is an article explaining the
    Self-Ask Agent'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '***è‡ªæˆ‘æé—®ä»£ç†*** è¿™ä¸ªä»£ç†æ ¹æ®åŸå§‹é—®é¢˜æå‡ºåç»­é—®é¢˜ï¼Œç„¶åå°è¯•æ‰¾åˆ°ä¸­é—´ç­”æ¡ˆã€‚é€šè¿‡è¿™äº›ä¸­é—´ç­”æ¡ˆï¼Œå®ƒæœ€ç»ˆå¾—å‡ºä¸€ä¸ªæœ€ç»ˆç­”æ¡ˆã€‚è¿™é‡Œæ˜¯è§£é‡Šè‡ªæˆ‘æé—®ä»£ç†çš„æ–‡ç« '
- en: '[](https://cobusgreyling.medium.com/self-ask-prompting-d0805ea31faa?source=post_page-----4ef8e6f1b741--------------------------------)
    [## Self-Ask Prompting'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://cobusgreyling.medium.com/self-ask-prompting-d0805ea31faa?source=post_page-----4ef8e6f1b741--------------------------------)
    [## è‡ªæˆ‘æé—®æç¤º'
- en: Self-Ask Prompting is a progression from Chain Of Thought Prompting. Below are
    a few practical examples and anâ€¦
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è‡ªæˆ‘æé—®æç¤ºæ˜¯ä¸€ç§ä»é“¾å¼æ€ç»´æç¤ºæ¼”å˜è€Œæ¥çš„æ–¹æ³•ã€‚ä¸‹é¢æ˜¯ä¸€äº›å®é™…çš„ä¾‹å­å’Œâ€¦
- en: cobusgreyling.medium.com](https://cobusgreyling.medium.com/self-ask-prompting-d0805ea31faa?source=post_page-----4ef8e6f1b741--------------------------------)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[cobusgreyling.medium.com](https://cobusgreyling.medium.com/self-ask-prompting-d0805ea31faa?source=post_page-----4ef8e6f1b741--------------------------------)'
- en: This approach gave me some good results. It works well for a Single-hop reason.
    But even this fails for questions that require multiple hops.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•ç»™äº†æˆ‘ä¸€äº›ä¸é”™çš„ç»“æœã€‚å®ƒåœ¨å•è·³æ¨ç†çš„æƒ…å†µä¸‹æ•ˆæœå¾ˆå¥½ï¼Œä½†å³ä½¿è¿™æ ·ï¼Œå¯¹äºéœ€è¦å¤šè·³çš„é—®æä¹Ÿä¼šå¤±è´¥ã€‚
- en: 'For example, the questions:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œé—®é¢˜ï¼š
- en: Who killed Karna, and why?
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è°æ€äº†å¡å°”çº³ï¼Œä¸ºä»€ä¹ˆï¼Ÿ
- en: Is relatively easy to answer with this approach
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨è¿™ç§æ–¹æ³•ç›¸å¯¹å®¹æ˜“å›ç­”ã€‚
- en: The question
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜
- en: Why did Arjuna kill Karna, his half-brother?
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆé˜¿å‘¨é‚£æ€äº†ä»–çš„åŠå…„å¡å°”çº³ï¼Ÿ
- en: Is much more difficult to answer. It requires the LLM to know the fact that
    Arjuna did not know that Karna was his half-brother. The LLM canâ€™t know that it
    needs to know this fact, either by understanding the question or by asking further
    questions based on the original question.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å›ç­”è¿™ä¸ªé—®é¢˜è¦å›°éš¾å¾—å¤šã€‚å®ƒè¦æ±‚ LLM çŸ¥é“é˜¿å‘¨é‚£ä¸çŸ¥é“å¡å°”çº³æ˜¯ä»–çš„åŠå…„å¼Ÿçš„äº‹å®ã€‚LLM æ— æ³•çŸ¥é“å®ƒéœ€è¦çŸ¥é“è¿™ä¸ªäº‹å®ï¼Œæ— è®ºæ˜¯é€šè¿‡ç†è§£é—®é¢˜è¿˜æ˜¯é€šè¿‡åŸºäºåŸå§‹é—®é¢˜æå‡ºè¿›ä¸€æ­¥çš„é—®é¢˜ã€‚
- en: '**Human Research Process**'
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**äººç±»ç ”ç©¶è¿‡ç¨‹**'
- en: Quoting GPT again
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å†æ¬¡å¼•ç”¨ GPT
- en: AI agents are designed to mimic human-like behaviour in problem-solving and
    decision-making tasks
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: AI ä»£ç†è¢«è®¾è®¡æˆåœ¨è§£å†³é—®é¢˜å’Œå†³ç­–ä»»åŠ¡ä¸­æ¨¡ä»¿ç±»ä¼¼äººç±»çš„è¡Œä¸ºã€‚
- en: So, my next idea was to research how humans research, the meta-research if you
    like. I imagined myself sitting in a library (College nostalgia) with easy access
    to all the books relevant to the topic of my research. I took a notebook and a
    pen and started jotting down the process I follow when I research a topic.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œæˆ‘çš„ä¸‹ä¸€ä¸ªæƒ³æ³•æ˜¯ç ”ç©¶äººç±»å¦‚ä½•ç ”ç©¶ï¼Œä¹Ÿå°±æ˜¯å…ƒç ”ç©¶ã€‚æˆ‘æƒ³è±¡è‡ªå·±ååœ¨å›¾ä¹¦é¦†é‡Œï¼ˆå¤§å­¦çš„æ€€æ—§æƒ…æ€€ï¼‰ï¼Œèƒ½å¤Ÿè½»æ¾è·å–æ‰€æœ‰ä¸æˆ‘çš„ç ”ç©¶ä¸»é¢˜ç›¸å…³çš„ä¹¦ç±ã€‚æˆ‘æ‹¿å‡ºä¸€ä¸ªç¬”è®°æœ¬å’Œä¸€æ”¯ç¬”ï¼Œå¼€å§‹è®°å½•æˆ‘åœ¨ç ”ç©¶ä¸€ä¸ªä¸»é¢˜æ—¶éµå¾ªçš„è¿‡ç¨‹ã€‚
- en: Here is what I came up with.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘å¾—åˆ°çš„ç»“æœã€‚
- en: '**Research methodology:**'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç ”ç©¶æ–¹æ³•è®ºï¼š**'
- en: Note down the original query on a page.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸€é¡µä¸Šè®°ä¸‹åŸå§‹æŸ¥è¯¢ã€‚
- en: I try to answer the current question by reading a few books. In the process,
    I make a few notes and bookmark a few excerpts that I find most relevant to the
    current question.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘å°è¯•é€šè¿‡é˜…è¯»å‡ æœ¬ä¹¦æ¥å›ç­”å½“å‰çš„é—®é¢˜ã€‚åœ¨è¿‡ç¨‹ä¸­ï¼Œæˆ‘åšäº†äº›ç¬”è®°ï¼Œå¹¶æ ‡è®°äº†ä¸€äº›æˆ‘è®¤ä¸ºä¸å½“å‰é—®é¢˜æœ€ç›¸å…³çš„æ‘˜å½•ã€‚
- en: Invariably, I find many unknowns in these excerpts. I note these unknowns and
    write down a few more questions that can help me learn about these unknowns.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨è¿™äº›æ‘˜å½•ä¸­æ€»èƒ½å‘ç°è®¸å¤šæœªçŸ¥ä¹‹å¤„ã€‚æˆ‘è®°å½•ä¸‹è¿™äº›æœªçŸ¥ï¼Œå¹¶å†™ä¸‹æ›´å¤šé—®é¢˜ä»¥å¸®åŠ©æˆ‘äº†è§£è¿™äº›æœªçŸ¥ã€‚
- en: Out of these questions, I choose one question that is most pertinent to the
    original question.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»è¿™äº›é—®é¢˜ä¸­ï¼Œæˆ‘é€‰æ‹©ä¸€ä¸ªä¸åŸå§‹é—®é¢˜æœ€ç›¸å…³çš„é—®é¢˜ã€‚
- en: I go back to step 1
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘å›åˆ°ç¬¬ä¸€æ­¥
- en: After a few such iterations, I ask myself if I have enough information to answer
    the original question.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ç»è¿‡å‡ æ¬¡è¿™æ ·çš„è¿­ä»£ï¼Œæˆ‘é—®è‡ªå·±æ˜¯å¦æœ‰è¶³å¤Ÿçš„ä¿¡æ¯æ¥å›ç­”åŸå§‹é—®é¢˜ã€‚
- en: If yes, then job well done!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ˜¯ï¼Œé‚£ä¹ˆå¹²å¾—å¥½ï¼
- en: If no, then toil on.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä¸æ˜¯ï¼Œé‚£å°±ç»§ç»­åŠªåŠ›å§ã€‚
- en: Voila!
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹å•Šï¼
- en: Finally, I knew what to code. I hoped that, with some prompt engineering, this
    process could give me more profound answers than any of the other approaches I
    have tried previously.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆï¼Œæˆ‘çŸ¥é“è¯¥ç¼–å†™ä»€ä¹ˆä»£ç ã€‚æˆ‘å¸Œæœ›é€šè¿‡ä¸€äº›æç¤ºå·¥ç¨‹ï¼Œè¿™ä¸ªè¿‡ç¨‹èƒ½ç»™æˆ‘æ¯”ä¹‹å‰å°è¯•çš„å…¶ä»–æ–¹æ³•æ›´æ·±åˆ»çš„ç­”æ¡ˆã€‚
- en: Spoiler alertâ€¦ it did! :)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å‰§é€è­¦å‘Šâ€¦â€¦ç¡®å®å¦‚æ­¤ï¼:)
- en: Before sitting down to code, I searched the internet for similar ideas. And
    I discovered the BabyAGI. What a wonderful world!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹ç¼–ç ä¹‹å‰ï¼Œæˆ‘åœ¨äº’è”ç½‘ä¸Šæœç´¢äº†ç±»ä¼¼çš„æƒ³æ³•ã€‚æˆ‘å‘ç°äº† BabyAGIã€‚çœŸæ˜¯ä¸€ä¸ªå¥‡å¦™çš„ä¸–ç•Œï¼
- en: Here is a repo describing [the BabyAGI](https://github.com/yoheinakajima/babyagi/tree/main)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æè¿° [BabyAGI](https://github.com/yoheinakajima/babyagi/tree/main) çš„ä¸€ä¸ªä»“åº“
- en: I realised there were many similarities between BabyAGI and the above research
    process. So with gratitude, I took some inspiration from the prompts used in the
    BabyAGI implementation.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ„è¯†åˆ° BabyAGI ä¸ä¸Šè¿°ç ”ç©¶è¿‡ç¨‹ä¹‹é—´æœ‰è®¸å¤šç›¸ä¼¼ä¹‹å¤„ã€‚å› æ­¤ï¼Œæˆ‘æ€€ç€æ„Ÿæ¿€ä¹‹æƒ…ï¼Œä» BabyAGI å®ç°ä¸­ä½¿ç”¨çš„æç¤ºä¸­è·å¾—äº†ä¸€äº›çµæ„Ÿã€‚
- en: '**The Research Agent â€” Implementation**'
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ç ”ç©¶ä»£ç†â€” å®ç°**'
- en: Here is the same process converted to a flow chart using the amazing [draw.io](http://draw.io)
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä½¿ç”¨æƒŠäººçš„ [draw.io](http://draw.io) å°†ç›¸åŒçš„è¿‡ç¨‹è½¬æ¢ä¸ºæµç¨‹å›¾
- en: '![](../Images/e2834b738664984ebb9d6b3b0eb5d794.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2834b738664984ebb9d6b3b0eb5d794.png)'
- en: Image by the Author (Generated using draw.io)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒï¼ˆä½¿ç”¨ draw.io ç”Ÿæˆï¼‰
- en: Every blue box in this chart is a call to an LLM.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾è¡¨ä¸­çš„æ¯ä¸ªè“è‰²æ¡†éƒ½æ˜¯å¯¹ LLM çš„è°ƒç”¨ã€‚
- en: '**Components**'
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ç»„ä»¶**'
- en: '**QA Agent â€”** Search for answers and further context'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**QA ä»£ç†â€”** æœç´¢ç­”æ¡ˆå’Œè¿›ä¸€æ­¥çš„èƒŒæ™¯ä¿¡æ¯'
- en: This is a simple ***â€˜stuffâ€™*** Retrieval QA chain that uses a vector store.
    In the future, this can be an AI Agent that uses tools like vector stores, search
    APIs or Wikipedia APIs, Moderation APIs and previous research data. The prompt
    here is tuned to generate succinct answers based on the 1\. The context (documents),
    and 2\. the pertinence to the ***original question***.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªç®€å•çš„***â€˜stuffâ€™*** æ£€ç´¢ QA é“¾ï¼Œå®ƒä½¿ç”¨äº†ä¸€ä¸ªå‘é‡å­˜å‚¨ã€‚åœ¨æœªæ¥ï¼Œè¿™å¯ä»¥æ˜¯ä¸€ä¸ªä½¿ç”¨å‘é‡å­˜å‚¨ã€æœç´¢ API æˆ–ç»´åŸºç™¾ç§‘ APIã€å†…å®¹å®¡æ ¸
    API å’Œä»¥å¾€ç ”ç©¶æ•°æ®ç­‰å·¥å…·çš„ AI ä»£ç†ã€‚è¿™é‡Œçš„æç¤ºç»è¿‡è°ƒæ•´ï¼Œä»¥æ ¹æ® 1. ä¸Šä¸‹æ–‡ï¼ˆæ–‡æ¡£ï¼‰å’Œ 2. ä¸***åŸå§‹é—®é¢˜***çš„ç›¸å…³æ€§æ¥ç”Ÿæˆç®€æ´çš„ç­”æ¡ˆã€‚
- en: Except for the first loop, the ***current question*** is always an intermediate
    question generated in step 2 and chosen in step 3\. The Agent appends the intermediate
    answer to the notes and the latest excerpts (the documents used to answer the
    ***current question***) to the bookmarks. The most recent of these documents are
    utilised in step 2.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: é™¤äº†ç¬¬ä¸€è½®å¤–ï¼Œ***å½“å‰é—®é¢˜*** æ€»æ˜¯æ­¥éª¤ 2 ä¸­ç”Ÿæˆçš„ä¸­é—´é—®é¢˜ï¼Œå¹¶åœ¨æ­¥éª¤ 3 ä¸­é€‰æ‹©ã€‚ä»£ç†å°†ä¸­é—´ç­”æ¡ˆé™„åŠ åˆ°ç¬”è®°ä¸­ï¼Œå¹¶å°†æœ€æ–°çš„æ‘˜å½•ï¼ˆç”¨äºå›ç­”***å½“å‰é—®é¢˜***çš„æ–‡æ¡£ï¼‰é™„åŠ åˆ°ä¹¦ç­¾ä¸­ã€‚æœ€æ–°çš„è¿™äº›æ–‡æ¡£å°†åœ¨æ­¥éª¤
    2 ä¸­ä½¿ç”¨ã€‚
- en: '**Question Generator â€”** Ask more questions based on fresh notes'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**é—®é¢˜ç”Ÿæˆå™¨â€”** æ ¹æ®æ–°çš„ç¬”è®°æå‡ºæ›´å¤šé—®é¢˜'
- en: Here, the agent uses the most recent vector search results matching the ***current
    question*** and uses them to generate more questions pertinent to the ***original
    question***. It appends these questions to the list of ***unanswered questions***.
    The prompt here is tuned such that the newly generated questions do not overlap
    with the existing list of questions.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œä»£ç†ä½¿ç”¨ä¸***å½“å‰é—®é¢˜***åŒ¹é…çš„æœ€æ–°å‘é‡æœç´¢ç»“æœï¼Œå¹¶åˆ©ç”¨è¿™äº›ç»“æœç”Ÿæˆä¸***åŸå§‹é—®é¢˜***ç›¸å…³çš„æ›´å¤šé—®é¢˜ã€‚å®ƒå°†è¿™äº›é—®é¢˜é™„åŠ åˆ°***æœªå›ç­”çš„é—®é¢˜***åˆ—è¡¨ä¸­ã€‚è¿™é‡Œçš„æç¤ºç»è¿‡è°ƒæ•´ï¼Œä»¥ç¡®ä¿æ–°ç”Ÿæˆçš„é—®é¢˜ä¸ä¼šä¸ç°æœ‰çš„é—®é¢˜åˆ—è¡¨é‡å ã€‚
- en: '**Most Pertinent Question Picker â€”** Pick one question most pertinent to the
    original question'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æœ€ç›¸å…³é—®é¢˜æŒ‘é€‰å™¨â€”** æŒ‘é€‰å‡ºä¸åŸå§‹é—®é¢˜æœ€ç›¸å…³çš„ä¸€ä¸ªé—®é¢˜'
- en: This prompt picks one question from the list of unanswered questions that is
    the most pertinent to the ***original question***. This question is used as the
    ***current question*** for the next loop.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæç¤ºä»æœªå›ç­”çš„é—®é¢˜åˆ—è¡¨ä¸­æŒ‘é€‰å‡ºä¸€ä¸ªä¸***åŸå§‹é—®é¢˜***æœ€ç›¸å…³çš„é—®é¢˜ã€‚è¿™ä¸ªé—®é¢˜å°†ä½œä¸ºä¸‹ä¸€è½®çš„***å½“å‰é—®é¢˜***ã€‚
- en: In the next loop, the agent removes this question from the list of ***unanswered
    questions*** after generating a fresh set of questions.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€è½®ä¸­ï¼Œä»£ç†ä¼šåœ¨ç”Ÿæˆä¸€ç»„æ–°çš„é—®é¢˜åï¼Œä»***æœªå›ç­”çš„é—®é¢˜***åˆ—è¡¨ä¸­åˆ é™¤è¿™ä¸ªé—®é¢˜ã€‚
- en: '**Analyser â€”** Do I know enough?'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åˆ†æå™¨â€”** æˆ‘æ˜¯å¦çŸ¥é“è¶³å¤Ÿå¤šï¼Ÿ'
- en: I am using a ***max_iterations*** parameter to exit the loop. This works pretty
    well for now. However, it might be better to dynamically decide on the number
    of iterations or an exit strategy based on the evolving context. I will work on
    an â€˜analyserâ€™ that can do this autonomously in the future.
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æˆ‘ä½¿ç”¨äº†ä¸€ä¸ª***max_iterations*** å‚æ•°æ¥é€€å‡ºå¾ªç¯ã€‚ç°åœ¨è¿™æ•ˆæœä¸é”™ã€‚ç„¶è€Œï¼Œæ ¹æ®ä¸æ–­å˜åŒ–çš„ä¸Šä¸‹æ–‡åŠ¨æ€å†³å®šè¿­ä»£æ¬¡æ•°æˆ–é€€å‡ºç­–ç•¥å¯èƒ½ä¼šæ›´å¥½ã€‚æˆ‘å°†æ¥ä¼šè‡´åŠ›äºå¼€å‘ä¸€ä¸ªå¯ä»¥è‡ªä¸»å®Œæˆè¿™é¡¹ä»»åŠ¡çš„â€˜åˆ†æå™¨â€™ã€‚
- en: '**Research Compiler â€”** Compile the research'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ç ”ç©¶ç¼–è¯‘å™¨â€”** ç¼–è¯‘ç ”ç©¶'
- en: This is the final prompt. It uses the notes made during the research process
    to arrive at an elaborate `final answer` to the `original questions`.
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æœ€ç»ˆçš„æç¤ºã€‚å®ƒåˆ©ç”¨ç ”ç©¶è¿‡ç¨‹ä¸­è®°å½•çš„ç¬”è®°ï¼Œå¾—å‡ºå¯¹`åŸå§‹é—®é¢˜`çš„è¯¦ç»†`æœ€ç»ˆç­”æ¡ˆ`ã€‚
- en: '**Results**'
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ç»“æœ**'
- en: The Research Agent is a big improvement over all the previous approaches I have
    tried. It yields more detailed and accurate answers than any other approach. I
    have been playing with this for a few weeks now, and I am surprised by the richness
    of the answers I get.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ç ”ç©¶ä»£ç†åœ¨æˆ‘å°è¯•è¿‡çš„æ‰€æœ‰æ–¹æ³•ä¸­æ˜¯ä¸€ä¸ªé‡å¤§æ”¹è¿›ã€‚å®ƒæä¾›çš„ç­”æ¡ˆæ¯”ä»»ä½•å…¶ä»–æ–¹æ³•éƒ½æ›´è¯¦ç»†ã€å‡†ç¡®ã€‚æˆ‘ç©äº†å‡ ä¸ªæ˜ŸæœŸè¿™ä¸ªæ–¹æ³•ï¼Œå¯¹å¾—åˆ°çš„ç­”æ¡ˆçš„ä¸°å¯Œæ€§æ„Ÿåˆ°æƒŠè®¶ã€‚
- en: The Agent avoids the problem of hallucinations to a greater extent than any
    previous approach. It autocorrects the hallucinations and the factual errors it
    generates in the first few iterations, during later ones. The deeper it gets into
    a problem, the more accurately it yields the result.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥ä»£ç†åœ¨é¿å…å¹»è§‰é—®é¢˜æ–¹é¢æ¯”ä»»ä½•ä»¥å‰çš„æ–¹æ³•éƒ½æœ‰æ›´å¤§çš„è¿›å±•ã€‚å®ƒä¼šåœ¨åˆæœŸçš„å‡ æ¬¡è¿­ä»£ä¸­è‡ªåŠ¨çº æ­£äº§ç”Ÿçš„å¹»è§‰å’Œäº‹å®é”™è¯¯ã€‚å®ƒåœ¨é—®é¢˜çš„æ·±å…¥å¤„ç†è¿‡ç¨‹ä¸­ï¼Œç»“æœä¼šæ›´ä¸ºå‡†ç¡®ã€‚
- en: Here is an example run.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹è¿è¡Œã€‚
- en: 'Questions: *Why did the Pandavas have to go live in the forest for 12 years?*'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜ï¼š*ä¸ºä»€ä¹ˆæ½˜è¾¾ç“¦æ–¯å¿…é¡»åœ¨æ£®æ—ä¸­ç”Ÿæ´»12å¹´ï¼Ÿ*
- en: Output â€”
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡º â€”
- en: '![](../Images/b47104951fa815be1000dafbcc2a7428.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b47104951fa815be1000dafbcc2a7428.png)'
- en: Just in case you are curious, here is the final answer
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æ„Ÿå…´è¶£ï¼Œè¿™æ˜¯æœ€ç»ˆç­”æ¡ˆ
- en: '[PRE0]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This answer is quite an elaborate one. But the beauty of the agent is not just
    that it answered the original question accurately but that it went further and
    found out the story surrounding the question.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç­”æ¡ˆç›¸å½“è¯¦ç»†ã€‚ä½†ä»£ç†çš„ç¾å¦™ä¹‹å¤„ä¸ä»…åœ¨äºå®ƒå‡†ç¡®å›ç­”äº†åŸå§‹é—®é¢˜ï¼Œè¿˜åœ¨äºå®ƒè¿›ä¸€æ­¥æ¢ç´¢äº†ä¸é—®é¢˜ç›¸å…³çš„æ•…äº‹ã€‚
- en: In most cases, the answers I get are rich in such details. And every such answer
    inspires my curiosity for further enquiry.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæˆ‘å¾—åˆ°çš„ç­”æ¡ˆå……æ»¡äº†ç»†èŠ‚ã€‚æ¯ä¸ªè¿™æ ·çš„ç­”æ¡ˆéƒ½æ¿€å‘äº†æˆ‘è¿›ä¸€æ­¥æ¢ç©¶çš„å¥½å¥‡å¿ƒã€‚
- en: The Agent also yields a set of ***answered questions*** and **unanswered questions**
    that it notes down during the research process. So after every run, it leads me
    to many other questions that I can ask. In the past few weeks, I have learned
    more about the Mahabharata than I did in many years before that.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥ä»£ç†è¿˜ä¼šç”Ÿæˆä¸€ç»„***å·²å›ç­”çš„é—®é¢˜***å’Œ**æœªå›ç­”çš„é—®é¢˜**ï¼Œå¹¶åœ¨ç ”ç©¶è¿‡ç¨‹ä¸­è®°å½•ä¸‹æ¥ã€‚å› æ­¤ï¼Œæ¯æ¬¡è¿è¡Œåï¼Œå®ƒéƒ½ä¼šå¼•å¯¼æˆ‘æå‡ºæ›´å¤šçš„é—®é¢˜ã€‚åœ¨è¿‡å»çš„å‡ å‘¨é‡Œï¼Œæˆ‘å¯¹ã€Šæ‘©è¯ƒå©†ç½—å¤šã€‹çš„äº†è§£æ¯”ä»¥å‰çš„å¤šå¹´éƒ½è¦å¤šã€‚
- en: '[PRE1]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Imagine following the same process for other domains of knowledge, quite an
    exhilarating thought!
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ³è±¡ä¸€ä¸‹å°†ç›¸åŒçš„è¿‡ç¨‹åº”ç”¨äºå…¶ä»–çŸ¥è¯†é¢†åŸŸï¼ŒçœŸæ˜¯ä»¤äººå…´å¥‹çš„æƒ³æ³•ï¼
- en: '**The Code**'
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ä»£ç **'
- en: And here are the Python notebooks with the implementation of the Research Agent.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯å®ç°ç ”ç©¶ä»£ç†çš„ Python ç¬”è®°æœ¬ã€‚
- en: '[](https://github.com/rahulnyk/research_agent?source=post_page-----4ef8e6f1b741--------------------------------)
    [## GitHub - rahulnyk/research_agent'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/rahulnyk/research_agent?source=post_page-----4ef8e6f1b741--------------------------------)
    [## GitHub - rahulnyk/research_agent'
- en: Contribute to rahulnyk/research_agent development by creating an account on
    GitHub.
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é€šè¿‡åœ¨ GitHub ä¸Šåˆ›å»ºä¸€ä¸ªå¸æˆ·ï¼Œæ¥ä¸º rahulnyk/research_agent çš„å¼€å‘åšå‡ºè´¡çŒ®ã€‚
- en: github.com](https://github.com/rahulnyk/research_agent?source=post_page-----4ef8e6f1b741--------------------------------)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/rahulnyk/research_agent?source=post_page-----4ef8e6f1b741--------------------------------)
- en: The Git repository of the Mahabharata dataset
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ã€Šæ‘©è¯ƒå©†ç½—å¤šã€‹æ•°æ®é›†çš„ Git ä»“åº“
- en: '[](https://github.com/rahulnyk/mahabharata?source=post_page-----4ef8e6f1b741--------------------------------)
    [## GitHub - rahulnyk/mahabharata: Mahabharata text compiled from multiple sources,
    split into chunksâ€¦'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/rahulnyk/mahabharata?source=post_page-----4ef8e6f1b741--------------------------------)
    [## GitHub - rahulnyk/mahabharata: ä»å¤šä¸ªæ¥æºç¼–çº‚çš„ã€Šæ‘©è¯ƒå©†ç½—å¤šã€‹æ–‡æœ¬ï¼Œåˆ†å‰²æˆè‹¥å¹²éƒ¨åˆ†â€¦'
- en: Mahabharata text is compiled from multiple sources, split into chunks, and parsed
    into CSV files with metadata. Named entitiesâ€¦
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ã€Šæ‘©è¯ƒå©†ç½—å¤šã€‹æ–‡æœ¬æ˜¯ä»å¤šä¸ªæ¥æºç¼–çº‚çš„ï¼Œåˆ†å‰²æˆè‹¥å¹²éƒ¨åˆ†ï¼Œå¹¶è§£ææˆå¸¦æœ‰å…ƒæ•°æ®çš„ CSV æ–‡ä»¶ã€‚å‘½åå®ä½“â€¦
- en: github.com](https://github.com/rahulnyk/mahabharata?source=post_page-----4ef8e6f1b741--------------------------------)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/rahulnyk/mahabharata?source=post_page-----4ef8e6f1b741--------------------------------)
- en: '**What next?**'
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**æ¥ä¸‹æ¥åšä»€ä¹ˆï¼Ÿ**'
- en: The current implementation is a simple version of the idea of an autonomous
    AI Research Agent. I turned the research process several times throughout the
    implementation of the Agent. It has been an exciting journey, but the fun is not
    over yet. Here are some of the enhancements I am currently working on.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: å½“å‰å®ç°æ˜¯è‡ªä¸» AI ç ”ç©¶ä»£ç†æ¦‚å¿µçš„ä¸€ä¸ªç®€å•ç‰ˆæœ¬ã€‚åœ¨ä»£ç†çš„å®ç°è¿‡ç¨‹ä¸­ï¼Œæˆ‘å¯¹ç ”ç©¶è¿‡ç¨‹è¿›è¡Œäº†å‡ æ¬¡ä¿®æ”¹ã€‚è¿™æ˜¯ä¸€æ¬¡ä»¤äººå…´å¥‹çš„æ—…ç¨‹ï¼Œä½†ä¹è¶£è¿˜æœªç»“æŸã€‚ä»¥ä¸‹æ˜¯æˆ‘ç›®å‰æ­£åœ¨è¿›è¡Œçš„ä¸€äº›æ”¹è¿›ã€‚
- en: Deploy this agent on a public link and observe more usage patterns.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨å…¬å…±é“¾æ¥ä¸Šéƒ¨ç½²è¿™ä¸ªä»£ç†ï¼Œå¹¶è§‚å¯Ÿæ›´å¤šçš„ä½¿ç”¨æ¨¡å¼ã€‚
- en: Use the Agent with different source documents other than The Mahabharata.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”¨ä¸åŒäºã€Šæ‘©è¯ƒå©†ç½—å¤šã€‹çš„æºæ–‡æ¡£æ¥ä½¿ç”¨è¯¥ä»£ç†ã€‚
- en: Step 1 of the process is currently a simple â€˜stuffâ€™ QA chain that uses a vector
    store with the source text corpus. I am working to replace it with a â€˜ReActâ€™ Agent
    so that other tools like search APIs, Wikipedia, moderation APIs etc., can be
    used in the research process.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿‡ç¨‹çš„ç¬¬1æ­¥ç›®å‰æ˜¯ä¸€ä¸ªç®€å•çš„â€˜stuffâ€™ QA é“¾ï¼Œå®ƒä½¿ç”¨äº†ä¸€ä¸ªåŒ…å«æºæ–‡æœ¬è¯­æ–™åº“çš„å‘é‡å­˜å‚¨ã€‚æˆ‘æ­£åœ¨åŠªåŠ›å°†å…¶æ›¿æ¢ä¸ºâ€˜ReActâ€™ä»£ç†ï¼Œä»¥ä¾¿åœ¨ç ”ç©¶è¿‡ç¨‹ä¸­å¯ä»¥ä½¿ç”¨å…¶ä»–å·¥å…·ï¼Œå¦‚æœç´¢
    APIã€ç»´åŸºç™¾ç§‘ã€å®¡æ ¸ API ç­‰ã€‚
- en: I am saving the data and metadata generated during every run to a `runs` vector
    store. I am also saving the embeddings of the original question to the same store.
    This has helped me follow the reasoning trail of the Agent and observe several
    logical patterns that emerge out of it. This can help tune the QA agent to follow
    a tighter reasoning path.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '-   æˆ‘å°†æ¯æ¬¡è¿è¡Œç”Ÿæˆçš„æ•°æ®å’Œå…ƒæ•°æ®ä¿å­˜åˆ°ä¸€ä¸ª`runs`å‘é‡å­˜å‚¨ä¸­ã€‚æˆ‘è¿˜å°†åŸå§‹é—®é¢˜çš„åµŒå…¥ä¿å­˜åˆ°åŒä¸€å­˜å‚¨ä¸­ã€‚è¿™å¸®åŠ©æˆ‘è·Ÿè¸ªä»£ç†çš„æ¨ç†è¿‡ç¨‹ï¼Œå¹¶è§‚å¯Ÿåˆ°å…¶ä¸­å‡ºç°çš„å‡ ä¸ªé€»è¾‘æ¨¡å¼ã€‚è¿™æœ‰åŠ©äºè°ƒæ•´QAä»£ç†ï¼Œä»¥è·Ÿéšæ›´ç´§å¯†çš„æ¨ç†è·¯å¾„ã€‚'
- en: Currently, the Research Agent exist after a fixed set of iterations. This works
    quite well for most questions. However, it might be better to dynamically decide
    on the number of iterations or an exit strategy based on the evolving context.
    I will work on an â€˜analyserâ€™ that can do this autonomously.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '-   ç›®å‰ï¼Œç ”ç©¶ä»£ç†åœ¨ç»è¿‡å›ºå®šçš„è¿­ä»£æ¬¡æ•°åå­˜åœ¨ã€‚è¿™å¯¹å¤§å¤šæ•°é—®é¢˜æ¥è¯´æ•ˆæœå¾ˆå¥½ã€‚ç„¶è€Œï¼Œæ ¹æ®ä¸æ–­å˜åŒ–çš„èƒŒæ™¯ï¼ŒåŠ¨æ€å†³å®šè¿­ä»£æ¬¡æ•°æˆ–é€€å‡ºç­–ç•¥å¯èƒ½æ›´å¥½ã€‚æˆ‘ä¼šå¼€å‘ä¸€ä¸ªèƒ½å¤Ÿè‡ªä¸»å®Œæˆè¿™ä¸€ä»»åŠ¡çš„â€œåˆ†æå™¨â€ã€‚'
- en: The Agent works well for most types of questions, except for the meta-questions.
    For example, if I ask â€˜Describe what happens in book 5 chapter 3â€™, the Agent struggles
    to answer. In future versions, I will include a self-query retriever with the
    â€˜ReActâ€™ Agent to deal with such cases.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '-   è¿™ä¸ªä»£ç†å¯¹å¤§å¤šæ•°ç±»å‹çš„é—®é¢˜æ•ˆæœå¾ˆå¥½ï¼Œä½†å¯¹å…ƒé—®é¢˜é™¤å¤–ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘é—®â€œæè¿°ç¬¬äº”å·ç¬¬ä¸‰ç« ä¸­å‘ç”Ÿäº†ä»€ä¹ˆâ€ï¼Œä»£ç†ä¼šå¾ˆéš¾å›ç­”ã€‚åœ¨æœªæ¥çš„ç‰ˆæœ¬ä¸­ï¼Œæˆ‘å°†åŠ å…¥ä¸€ä¸ªå¸¦æœ‰â€˜ReActâ€™ä»£ç†çš„è‡ªæŸ¥è¯¢æ£€ç´¢å™¨ï¼Œä»¥å¤„ç†æ­¤ç±»æƒ…å†µã€‚'
- en: So far, I have tried the Agent only with the OpenAI GPT3.5 model. It costs me
    about $0.02 per run. I will soon try the Research Agent with a smaller model like
    Llama, that can be hosted locally.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '-   åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘åªå°è¯•äº† OpenAI GPT3.5 æ¨¡å‹çš„ä»£ç†ã€‚æ¯æ¬¡è¿è¡Œçš„è´¹ç”¨çº¦ä¸º $0.02ã€‚æˆ‘å¾ˆå¿«ä¼šå°è¯•ä½¿ç”¨åƒ Llama è¿™æ ·å¯ä»¥æœ¬åœ°æ‰˜ç®¡çš„è¾ƒå°æ¨¡å‹ã€‚'
- en: In the next article, I plan to write about my findings after implementing some
    of these updates. The larger idea is to create a top-notch Autonomous AI Research
    Agent that can excel at finding deeply researched answers to difficult questions.
    So, please feel free to suggest and if possible, partner with me to mature this
    further.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '-   åœ¨ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘è®¡åˆ’å†™ä¸‹åœ¨å®æ–½è¿™äº›æ›´æ–°åçš„å‘ç°ã€‚æ›´å¤§çš„æ„æƒ³æ˜¯åˆ›å»ºä¸€ä¸ªé¡¶å°–çš„**è‡ªä¸»äººå·¥æ™ºèƒ½ç ”ç©¶ä»£ç†**ï¼Œèƒ½å¤Ÿåœ¨å¯»æ‰¾æ·±åº¦ç ”ç©¶çš„é—®é¢˜çš„ç­”æ¡ˆæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æ‰€ä»¥ï¼Œè¯·éšæ—¶æå‡ºå»ºè®®ï¼Œå¦‚æœå¯èƒ½ï¼Œå’Œæˆ‘åˆä½œè¿›ä¸€æ­¥å®Œå–„ã€‚'
- en: I hope you find this article and the shared code helpful.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '-   å¸Œæœ›ä½ è§‰å¾—è¿™ç¯‡æ–‡ç« å’Œåˆ†äº«çš„ä»£ç å¯¹ä½ æœ‰å¸®åŠ©ã€‚'
- en: Thanks for reading.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '-   è°¢è°¢é˜…è¯»ã€‚'
- en: I hope you find the AI Research Agent exciting and useful.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '-   å¸Œæœ›ä½ è§‰å¾—è¿™ä¸ªäººå·¥æ™ºèƒ½ç ”ç©¶ä»£ç†æ—¢ä»¤äººå…´å¥‹åˆæœ‰ç”¨ã€‚'
- en: The notebook I shared is just a simple implementation of this larger idea of
    creating an **Autonomous AI Research Agent**. A lot more can be done to make this
    Agent a top-notch researcher.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '-   æˆ‘åˆ†äº«çš„ç¬”è®°æœ¬åªæ˜¯åˆ›å»ºä¸€ä¸ª**è‡ªä¸»äººå·¥æ™ºèƒ½ç ”ç©¶ä»£ç†**è¿™ä¸€æ›´å¤§æ„æƒ³çš„ç®€å•å®ç°ã€‚ä¸ºäº†ä½¿è¿™ä¸ªä»£ç†æˆä¸ºé¡¶å°–ç ”ç©¶è€…ï¼Œè¿˜æœ‰å¾ˆå¤šå·¥ä½œè¦åšã€‚'
- en: So, please feel free to suggest and if possible, partner with me to mature this
    further.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '-   æ‰€ä»¥ï¼Œè¯·éšæ—¶æå‡ºå»ºè®®ï¼Œå¦‚æœå¯èƒ½ï¼Œå’Œæˆ‘åˆä½œè¿›ä¸€æ­¥å®Œå–„ã€‚'
- en: Thanks for reading.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '-   è°¢è°¢é˜…è¯»ã€‚'
- en: Credits for the data sets I used in the above article, along with the licensing
    information.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '-   ä¸Šè¿°æ–‡ç« ä¸­ä½¿ç”¨çš„æ•°æ®é›†çš„è‡´è°¢ä»¥åŠè®¸å¯è¯ä¿¡æ¯ã€‚'
- en: '[Complete Translation by K. M. Ganguli](https://en.wikipedia.org/wiki/Kisari_Mohan_Ganguli):
    Available in the public domain.'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[K. M. Ganguli çš„å®Œæ•´ç¿»è¯‘](https://en.wikipedia.org/wiki/Kisari_Mohan_Ganguli)ï¼šå¯åœ¨å…¬å…±é¢†åŸŸè·å¾—ã€‚'
- en: '[Laura Gibbs Tiny Tales](https://microfables.blogspot.com/2020/11/tiny-tales-from-mahabharata.html):
    This is a retelling of the Mahabharata using two hundred episodes that are each
    100 words long. I am using her work here with her permission.'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '-   [Laura Gibbs Tiny Tales](https://microfables.blogspot.com/2020/11/tiny-tales-from-mahabharata.html)ï¼šè¿™æ˜¯å¯¹ã€Šæ‘©è¯ƒå©†ç½—å¤šã€‹çš„é‡è¿°ï¼Œä½¿ç”¨äº†ä¸¤ç™¾ä¸ªæ¯ä¸ª100å­—é•¿çš„æ•…äº‹ã€‚æˆ‘åœ¨è¿™é‡Œä½¿ç”¨å¥¹çš„ä½œå“æ˜¯ç»è¿‡å¥¹çš„è®¸å¯ã€‚'
- en: '[Kaggle data repo by Tilak](https://www.kaggle.com/datasets/tilakd/mahabharata):
    All 18 Parvas of Mahabharata in text format for NLP. Shared with public domain
    license by [Tilak](https://www.kaggle.com/tilakd)'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '-   [Tilak çš„ Kaggle æ•°æ®åº“](https://www.kaggle.com/datasets/tilakd/mahabharata)ï¼š18
    éƒ¨ã€Šæ‘©è¯ƒå©†ç½—å¤šã€‹çš„æ–‡æœ¬æ ¼å¼æ•°æ®ç”¨äº NLPã€‚ç”±[Tilak](https://www.kaggle.com/tilakd)ä»¥å…¬å…±é¢†åŸŸè®¸å¯è¯å…±äº«ã€‚'
