- en: Making Sense of the Promise (and Risks) of Large Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解大型语言模型的承诺（及风险）
- en: 原文：[https://towardsdatascience.com/making-sense-of-the-promise-and-risks-of-large-language-models-2a8664f4347?source=collection_archive---------7-----------------------#2023-04-27](https://towardsdatascience.com/making-sense-of-the-promise-and-risks-of-large-language-models-2a8664f4347?source=collection_archive---------7-----------------------#2023-04-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/making-sense-of-the-promise-and-risks-of-large-language-models-2a8664f4347?source=collection_archive---------7-----------------------#2023-04-27](https://towardsdatascience.com/making-sense-of-the-promise-and-risks-of-large-language-models-2a8664f4347?source=collection_archive---------7-----------------------#2023-04-27)
- en: '[](https://towardsdatascience.medium.com/?source=post_page-----2a8664f4347--------------------------------)[![TDS
    Editors](../Images/4b2d1beaf4f6dcf024ffa6535de3b794.png)](https://towardsdatascience.medium.com/?source=post_page-----2a8664f4347--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2a8664f4347--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2a8664f4347--------------------------------)
    [TDS Editors](https://towardsdatascience.medium.com/?source=post_page-----2a8664f4347--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://towardsdatascience.medium.com/?source=post_page-----2a8664f4347--------------------------------)[![TDS
    Editors](../Images/4b2d1beaf4f6dcf024ffa6535de3b794.png)](https://towardsdatascience.medium.com/?source=post_page-----2a8664f4347--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2a8664f4347--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2a8664f4347--------------------------------)
    [TDS Editors](https://towardsdatascience.medium.com/?source=post_page-----2a8664f4347--------------------------------)'
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7e12c71dfa81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-sense-of-the-promise-and-risks-of-large-language-models-2a8664f4347&user=TDS+Editors&userId=7e12c71dfa81&source=post_page-7e12c71dfa81----2a8664f4347---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2a8664f4347--------------------------------)
    ·Sent as a [Newsletter](/newsletter?source=post_page-----2a8664f4347--------------------------------)
    ·4 min read·Apr 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2a8664f4347&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-sense-of-the-promise-and-risks-of-large-language-models-2a8664f4347&user=TDS+Editors&userId=7e12c71dfa81&source=-----2a8664f4347---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7e12c71dfa81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-sense-of-the-promise-and-risks-of-large-language-models-2a8664f4347&user=TDS+Editors&userId=7e12c71dfa81&source=post_page-7e12c71dfa81----2a8664f4347---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2a8664f4347--------------------------------)
    ·发送至 [通讯](/newsletter?source=post_page-----2a8664f4347--------------------------------)
    ·4分钟阅读·2023年4月27日'
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2a8664f4347&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-sense-of-the-promise-and-risks-of-large-language-models-2a8664f4347&source=-----2a8664f4347---------------------bookmark_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2a8664f4347&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-sense-of-the-promise-and-risks-of-large-language-models-2a8664f4347&source=-----2a8664f4347---------------------bookmark_footer-----------)'
- en: While ChatGPT and similar tools have monopolized much of our collective attention
    in recent months, large language models (LLMs)—the infrastructure behind all those
    chat outputs we endlessly share, screenshot, and occasionally shake our heads
    at—have remained in (relative) obscurity.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管ChatGPT和类似工具在最近几个月中占据了我们大量的关注，但大型语言模型（LLMs）——这些支撑着我们无休止分享、截图，偶尔摇头的聊天输出的基础设施——却一直处于（相对）默默无闻的状态。
- en: 'On a certain level, it makes sense: when you watch a show on your laptop, you
    focus on the plot and the characters, not on the electric grid that makes those
    moving pixels possible. If you’re eating a cookie right now (like… some TDS editors,
    most likely), you’re probably thinking about the flavors and textures you’re enjoying
    rather than, say, about wheat farmers or the history of cocoa beans.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 从某种程度上说，这很有道理：当你在笔记本电脑上观看节目时，你专注于情节和角色，而不是使那些移动像素成为可能的电网。如果你现在正在吃饼干（就像…一些TDS编辑，可能如此），你可能在思考你正在享受的风味和口感，而不是，比如说，关于小麦农民或可可豆的历史。
- en: As data professionals, however, we can only benefit by developing a deeper and
    more nuanced understanding of LLMs. There’s a professional angle to this, of course,
    but also a more amorphous pleasure of knowing how something complex and magic-like
    actually functions behind the curtain. To give you a leg up, we’ve assembled a
    stellar lineup of recent articles that look into the past, present, and future
    of LLMs—and tackle both their incredible abilities and nontrivial limitations.
    Enjoy!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，作为数据专业人员，我们只有通过深入和细致地理解LLM才能受益。当然，这有一个职业角度，但也有一种更模糊的享受，即了解某些复杂而神秘的事物在幕后实际是如何运作的。为了给你提供帮助，我们汇集了一系列优秀的文章，探讨了LLM的过去、现在和未来——并讨论了它们的惊人能力和非同小可的局限性。请享用！
- en: '[**Machine learning research in a post-ChatGPT world**](/closed-ai-models-make-bad-baselines-4bf6e47c9e6a).
    The inner workings of some of the most ubiquitous LLMs (like OpenAI’s GPT models)
    remain tightly guarded by corporate entities. What does that mean for NLP researchers
    whose future projects are likely to hit a proprietary wall sooner or later? [Anna
    Rogers](https://medium.com/u/201bcd64e17?source=post_page-----2a8664f4347--------------------------------)
    proposes a powerful idea: “That which is not open and reasonably reproducible
    cannot be considered a requisite baseline.”'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**后ChatGPT时代的机器学习研究**](/closed-ai-models-make-bad-baselines-4bf6e47c9e6a)。一些最普遍的LLM（如OpenAI的GPT模型）的内部运作仍被企业严格保密。这对那些未来项目很可能会遇到专有壁垒的NLP研究人员意味着什么？[安娜·罗杰斯](https://medium.com/u/201bcd64e17?source=post_page-----2a8664f4347--------------------------------)提出了一个有力的观点：“那些不开放且不合理可重复的东西不能被视为必要的基线。”'
- en: '[**Assessing the impact of recent advances in conversational AI**](/have-machines-just-made-an-evolutionary-leap-to-speak-in-human-language-319237593aa4).
    The size and power of LLMs have made it possible for human-machine interaction
    to take a giant leap in mere months. [Gadi Singer](https://medium.com/u/51de1f48d0b?source=post_page-----2a8664f4347--------------------------------)
    reflects on how we got here—and on what is still missing before the machines start
    communicating in a human-like manner.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**评估最近在对话AI方面的进展的影响**](/have-machines-just-made-an-evolutionary-leap-to-speak-in-human-language-319237593aa4)。LLM的规模和力量使得人机互动在短短几个月内取得了巨大的飞跃。[加迪·辛格](https://medium.com/u/51de1f48d0b?source=post_page-----2a8664f4347--------------------------------)反思了我们如何走到这里——以及在机器开始以类人方式交流之前，还缺少什么。'
- en: '[**Welcome to a new ethical (and practical) quagmire**](/a-pathway-towards-responsible-ai-generated-content-6c915e8155f9).
    LLMs require massive amounts of training data, which has turned out to be a major
    vector of risk. These models are currently facing intense scrutiny for the potential
    presence of biased, private, or harmful text and the unauthorized inclusion of
    copyrighted material. What are users to do in the meantime? [Lingjuan Lyu](https://medium.com/u/ca2f89d83dfb?source=post_page-----2a8664f4347--------------------------------)’s
    debut TDS article presents a thoughtful overview of the stakes and dangers we
    should all be aware of if we’re to produce AI-generated content responsibly.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**欢迎来到一个新的伦理（和实际）困境**](/a-pathway-towards-responsible-ai-generated-content-6c915e8155f9)。LLM需要大量的训练数据，这已成为一个主要的风险因素。这些模型目前面临着对可能存在的偏见、私人或有害文本以及未经授权的版权材料的严密审查。在此期间，用户该如何做？[凌娟·吕](https://medium.com/u/ca2f89d83dfb?source=post_page-----2a8664f4347--------------------------------)的首篇TDS文章对我们在负责任地生产AI生成内容时应了解的风险和危险进行了深思熟虑的概述。'
- en: '![](../Images/c79b0c68fc328f976d2f61d1e14f7021.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c79b0c68fc328f976d2f61d1e14f7021.png)'
- en: Photo by [Libby Penner](https://unsplash.com/@libby_penner?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 由[利比·佩纳](https://unsplash.com/@libby_penner?utm_source=medium&utm_medium=referral)拍摄，照片来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '[**The wide horizon of LLM-powered tools**](/a-gentle-intro-to-chaining-llms-agents-and-utils-via-langchain-16cd385fca81).
    Just a few months ago, the LangChain library was flying under most practitioners’
    radars. Well, not anymore: it’s become a go-to resource for many tinkerers who
    want to leverage LLMs to build new apps. [Dr. Varshita Sher](https://medium.com/u/f8ca36def59?source=post_page-----2a8664f4347--------------------------------)’s
    latest deep dive is a helpful, hands-on introduction to the library’s core building
    blocks.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Identifying drift and detecting anomalies with LLMs**](/applying-large-language-models-to-tabular-data-to-identify-drift-54c9fa59255f).
    As the novelty of generating clunky poems with ChatGPT fades, fresh use cases
    continue to emerge — and many of them might end up streamlining data science workflows.
    Case in point: [Aparna Dhinakaran](https://medium.com/u/f32f85889f3a?source=post_page-----2a8664f4347--------------------------------),
    Jason Lopatecki, and Christopher Brown’s latest post, which outlines a promising
    approach for using using LLM embeddings for anomaly and drift detection.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Bonus read: go one level deeper**](/the-map-of-transformers-e14952226398).
    If LLMs make user-facing applications like ChatGPT possible, transformer neural
    networks are the architecture that made LLMs possible in the first place. To get
    your bearings around this crucial (and often complex) topic, explore [Soran Ghaderi](https://medium.com/u/d2b75b0bb761?source=post_page-----2a8664f4347--------------------------------)’s
    detailed “map” of past and current transformers research.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ready to continue your cookie-fueled reading binge? Here are a few more standout
    articles you should check out:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: We’re thrilled to share [Michael Bronstein](https://medium.com/u/7b1129ddd572?source=post_page-----2a8664f4347--------------------------------)
    and Emanuele Rossi’s [latest research, exploring the intersection of machine learning
    and game theory](/learning-network-games-29970aee44bb).
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Three years after her viral post announced the death of the dashboard, [Taylor
    Brownlow](https://medium.com/u/cdc63fa2a06e?source=post_page-----2a8664f4347--------------------------------)
    brings an updated and more nuanced perspective on these oft-maligned yet inescapable
    tools.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does the current business landscape affect data teams? For [Barr Moses](https://medium.com/u/2818bac48708?source=post_page-----2a8664f4347--------------------------------),
    it’s crucial that [data scientists close the gap between their work and core business
    needs](/the-next-big-crisis-for-data-teams-58ac2bd856e8).
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deep learning pros, this one’s for you: [Shashank Prasanna](https://medium.com/u/e0c596ca35b5?source=post_page-----2a8664f4347--------------------------------)’s
    latest is a [handy, patient primer on the compiler technologies](/how-pytorch-2-0-accelerates-deep-learning-with-operator-fusion-and-cpu-gpu-code-generation-35132a85bd26)
    that power PyTorch 2.0.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thank you for spending some time with us this week! If you enjoy the articles
    you read on TDS, consider [becoming a Medium member](https://bit.ly/tds-membership)
    — and if you’re a student in an eligible country, don’t miss a chance to [enjoy
    a substantial discount on a membership](https://blog.medium.com/new-student-discounts-cc10e964495b).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你本周花时间与我们相伴！如果你喜欢在 TDS 阅读的文章，可以考虑 [成为 Medium 会员](https://bit.ly/tds-membership)
    —— 如果你是符合条件国家的学生，不要错过 [享受会员大幅折扣的机会](https://blog.medium.com/new-student-discounts-cc10e964495b)。
- en: Until the next Variable,
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 直到下一个变量，
- en: TDS Editors
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: TDS 编辑部
