- en: Making Sense of the Promise (and Risks) of Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/making-sense-of-the-promise-and-risks-of-large-language-models-2a8664f4347?source=collection_archive---------7-----------------------#2023-04-27](https://towardsdatascience.com/making-sense-of-the-promise-and-risks-of-large-language-models-2a8664f4347?source=collection_archive---------7-----------------------#2023-04-27)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://towardsdatascience.medium.com/?source=post_page-----2a8664f4347--------------------------------)[![TDS
    Editors](../Images/4b2d1beaf4f6dcf024ffa6535de3b794.png)](https://towardsdatascience.medium.com/?source=post_page-----2a8664f4347--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2a8664f4347--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2a8664f4347--------------------------------)
    [TDS Editors](https://towardsdatascience.medium.com/?source=post_page-----2a8664f4347--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7e12c71dfa81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-sense-of-the-promise-and-risks-of-large-language-models-2a8664f4347&user=TDS+Editors&userId=7e12c71dfa81&source=post_page-7e12c71dfa81----2a8664f4347---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2a8664f4347--------------------------------)
    ·Sent as a [Newsletter](/newsletter?source=post_page-----2a8664f4347--------------------------------)
    ·4 min read·Apr 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2a8664f4347&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-sense-of-the-promise-and-risks-of-large-language-models-2a8664f4347&user=TDS+Editors&userId=7e12c71dfa81&source=-----2a8664f4347---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2a8664f4347&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-sense-of-the-promise-and-risks-of-large-language-models-2a8664f4347&source=-----2a8664f4347---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: While ChatGPT and similar tools have monopolized much of our collective attention
    in recent months, large language models (LLMs)—the infrastructure behind all those
    chat outputs we endlessly share, screenshot, and occasionally shake our heads
    at—have remained in (relative) obscurity.
  prefs: []
  type: TYPE_NORMAL
- en: 'On a certain level, it makes sense: when you watch a show on your laptop, you
    focus on the plot and the characters, not on the electric grid that makes those
    moving pixels possible. If you’re eating a cookie right now (like… some TDS editors,
    most likely), you’re probably thinking about the flavors and textures you’re enjoying
    rather than, say, about wheat farmers or the history of cocoa beans.'
  prefs: []
  type: TYPE_NORMAL
- en: As data professionals, however, we can only benefit by developing a deeper and
    more nuanced understanding of LLMs. There’s a professional angle to this, of course,
    but also a more amorphous pleasure of knowing how something complex and magic-like
    actually functions behind the curtain. To give you a leg up, we’ve assembled a
    stellar lineup of recent articles that look into the past, present, and future
    of LLMs—and tackle both their incredible abilities and nontrivial limitations.
    Enjoy!
  prefs: []
  type: TYPE_NORMAL
- en: '[**Machine learning research in a post-ChatGPT world**](/closed-ai-models-make-bad-baselines-4bf6e47c9e6a).
    The inner workings of some of the most ubiquitous LLMs (like OpenAI’s GPT models)
    remain tightly guarded by corporate entities. What does that mean for NLP researchers
    whose future projects are likely to hit a proprietary wall sooner or later? [Anna
    Rogers](https://medium.com/u/201bcd64e17?source=post_page-----2a8664f4347--------------------------------)
    proposes a powerful idea: “That which is not open and reasonably reproducible
    cannot be considered a requisite baseline.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Assessing the impact of recent advances in conversational AI**](/have-machines-just-made-an-evolutionary-leap-to-speak-in-human-language-319237593aa4).
    The size and power of LLMs have made it possible for human-machine interaction
    to take a giant leap in mere months. [Gadi Singer](https://medium.com/u/51de1f48d0b?source=post_page-----2a8664f4347--------------------------------)
    reflects on how we got here—and on what is still missing before the machines start
    communicating in a human-like manner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Welcome to a new ethical (and practical) quagmire**](/a-pathway-towards-responsible-ai-generated-content-6c915e8155f9).
    LLMs require massive amounts of training data, which has turned out to be a major
    vector of risk. These models are currently facing intense scrutiny for the potential
    presence of biased, private, or harmful text and the unauthorized inclusion of
    copyrighted material. What are users to do in the meantime? [Lingjuan Lyu](https://medium.com/u/ca2f89d83dfb?source=post_page-----2a8664f4347--------------------------------)’s
    debut TDS article presents a thoughtful overview of the stakes and dangers we
    should all be aware of if we’re to produce AI-generated content responsibly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/c79b0c68fc328f976d2f61d1e14f7021.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Libby Penner](https://unsplash.com/@libby_penner?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '[**The wide horizon of LLM-powered tools**](/a-gentle-intro-to-chaining-llms-agents-and-utils-via-langchain-16cd385fca81).
    Just a few months ago, the LangChain library was flying under most practitioners’
    radars. Well, not anymore: it’s become a go-to resource for many tinkerers who
    want to leverage LLMs to build new apps. [Dr. Varshita Sher](https://medium.com/u/f8ca36def59?source=post_page-----2a8664f4347--------------------------------)’s
    latest deep dive is a helpful, hands-on introduction to the library’s core building
    blocks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Identifying drift and detecting anomalies with LLMs**](/applying-large-language-models-to-tabular-data-to-identify-drift-54c9fa59255f).
    As the novelty of generating clunky poems with ChatGPT fades, fresh use cases
    continue to emerge — and many of them might end up streamlining data science workflows.
    Case in point: [Aparna Dhinakaran](https://medium.com/u/f32f85889f3a?source=post_page-----2a8664f4347--------------------------------),
    Jason Lopatecki, and Christopher Brown’s latest post, which outlines a promising
    approach for using using LLM embeddings for anomaly and drift detection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Bonus read: go one level deeper**](/the-map-of-transformers-e14952226398).
    If LLMs make user-facing applications like ChatGPT possible, transformer neural
    networks are the architecture that made LLMs possible in the first place. To get
    your bearings around this crucial (and often complex) topic, explore [Soran Ghaderi](https://medium.com/u/d2b75b0bb761?source=post_page-----2a8664f4347--------------------------------)’s
    detailed “map” of past and current transformers research.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ready to continue your cookie-fueled reading binge? Here are a few more standout
    articles you should check out:'
  prefs: []
  type: TYPE_NORMAL
- en: We’re thrilled to share [Michael Bronstein](https://medium.com/u/7b1129ddd572?source=post_page-----2a8664f4347--------------------------------)
    and Emanuele Rossi’s [latest research, exploring the intersection of machine learning
    and game theory](/learning-network-games-29970aee44bb).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Three years after her viral post announced the death of the dashboard, [Taylor
    Brownlow](https://medium.com/u/cdc63fa2a06e?source=post_page-----2a8664f4347--------------------------------)
    brings an updated and more nuanced perspective on these oft-maligned yet inescapable
    tools.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does the current business landscape affect data teams? For [Barr Moses](https://medium.com/u/2818bac48708?source=post_page-----2a8664f4347--------------------------------),
    it’s crucial that [data scientists close the gap between their work and core business
    needs](/the-next-big-crisis-for-data-teams-58ac2bd856e8).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deep learning pros, this one’s for you: [Shashank Prasanna](https://medium.com/u/e0c596ca35b5?source=post_page-----2a8664f4347--------------------------------)’s
    latest is a [handy, patient primer on the compiler technologies](/how-pytorch-2-0-accelerates-deep-learning-with-operator-fusion-and-cpu-gpu-code-generation-35132a85bd26)
    that power PyTorch 2.0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thank you for spending some time with us this week! If you enjoy the articles
    you read on TDS, consider [becoming a Medium member](https://bit.ly/tds-membership)
    — and if you’re a student in an eligible country, don’t miss a chance to [enjoy
    a substantial discount on a membership](https://blog.medium.com/new-student-discounts-cc10e964495b).
  prefs: []
  type: TYPE_NORMAL
- en: Until the next Variable,
  prefs: []
  type: TYPE_NORMAL
- en: TDS Editors
  prefs: []
  type: TYPE_NORMAL
