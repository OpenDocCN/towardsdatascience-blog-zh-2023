["```py\nimport torch\n\ndef absmax_quantize(X):\n    # Calculate scale\n    scale = 127 / torch.max(torch.abs(X))\n\n    # Quantize\n    X_quant = (scale * X).round()\n\n    # Dequantize\n    X_dequant = X_quant / scale\n\n    return X_quant.to(torch.int8), X_dequant\n```", "```py\ndef zeropoint_quantize(X):\n    # Calculate value range (denominator)\n    x_range = torch.max(X) - torch.min(X)\n    x_range = 1 if x_range == 0 else x_range\n\n    # Calculate scale\n    scale = 255 / x_range\n\n    # Shift by zero-point\n    zeropoint = (-scale * torch.min(X) - 128).round()\n\n    # Scale and round the inputs\n    X_quant = torch.clip((X * scale + zeropoint).round(), -128, 127)\n\n    # Dequantize\n    X_dequant = (X_quant - zeropoint) / scale\n\n    return X_quant.to(torch.int8), X_dequant\n```", "```py\n!pip install -q bitsandbytes>=0.39.0\n!pip install -q git+https://github.com/huggingface/accelerate.git\n!pip install -q git+https://github.com/huggingface/transformers.git\n```", "```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\ntorch.manual_seed(0)\n\n# Set device to CPU for now\ndevice = 'cpu'\n\n# Load model and tokenizer\nmodel_id = 'gpt2'\nmodel = AutoModelForCausalLM.from_pretrained(model_id).to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# Print model size\nprint(f\"Model size: {model.get_memory_footprint():,} bytes\")\n```", "```py\nModel size: 510,342,192 bytes\n```", "```py\n# Extract weights of the first layer\nweights = model.transformer.h[0].attn.c_attn.weight.data\nprint(\"Original weights:\")\nprint(weights)\n\n# Quantize layer using absmax quantization\nweights_abs_quant, _ = absmax_quantize(weights)\nprint(\"\\nAbsmax quantized weights:\")\nprint(weights_abs_quant)\n\n# Quantize layer using absmax quantization\nweights_zp_quant, _ = zeropoint_quantize(weights)\nprint(\"\\nZero-point quantized weights:\")\nprint(weights_zp_quant)\n```", "```py\nOriginal weights:\ntensor([[-0.4738, -0.2614, -0.0978,  ...,  0.0513, -0.0584,  0.0250],\n        [ 0.0874,  0.1473,  0.2387,  ..., -0.0525, -0.0113, -0.0156],\n        [ 0.0039,  0.0695,  0.3668,  ...,  0.1143,  0.0363, -0.0318],\n        ...,\n        [-0.2592, -0.0164,  0.1991,  ...,  0.0095, -0.0516,  0.0319],\n        [ 0.1517,  0.2170,  0.1043,  ...,  0.0293, -0.0429, -0.0475],\n        [-0.4100, -0.1924, -0.2400,  ..., -0.0046,  0.0070,  0.0198]])\n\nAbsmax quantized weights:\ntensor([[-21, -12,  -4,  ...,   2,  -3,   1],\n        [  4,   7,  11,  ...,  -2,  -1,  -1],\n        [  0,   3,  16,  ...,   5,   2,  -1],\n        ...,\n        [-12,  -1,   9,  ...,   0,  -2,   1],\n        [  7,  10,   5,  ...,   1,  -2,  -2],\n        [-18,  -9, -11,  ...,   0,   0,   1]], dtype=torch.int8)\n\nZero-point quantized weights:\ntensor([[-20, -11,  -3,  ...,   3,  -2,   2],\n        [  5,   8,  12,  ...,  -1,   0,   0],\n        [  1,   4,  18,  ...,   6,   3,   0],\n        ...,\n        [-11,   0,  10,  ...,   1,  -1,   2],\n        [  8,  11,   6,  ...,   2,  -1,  -1],\n        [-18,  -8, -10,  ...,   1,   1,   2]], dtype=torch.int8)\n```", "```py\nimport numpy as np\nfrom copy import deepcopy\n\n# Store original weights\nweights = [param.data.clone() for param in model.parameters()]\n\n# Create model to quantize\nmodel_abs = deepcopy(model)\n\n# Quantize all model weights\nweights_abs = []\nfor param in model_abs.parameters():\n    _, dequantized = absmax_quantize(param.data)\n    param.data = dequantized\n    weights_abs.append(dequantized)\n\n# Create model to quantize\nmodel_zp = deepcopy(model)\n\n# Quantize all model weights\nweights_zp = []\nfor param in model_zp.parameters():\n    _, dequantized = zeropoint_quantize(param.data)\n    param.data = dequantized\n    weights_zp.append(dequantized)\n```", "```py\ndef generate_text(model, input_text, max_length=50):\n    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n    output = model.generate(inputs=input_ids,\n                            max_length=max_length,\n                            do_sample=True,\n                            top_k=30,\n                            pad_token_id=tokenizer.eos_token_id,\n                            attention_mask=input_ids.new_ones(input_ids.shape))\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\n# Generate text with original and quantized models\noriginal_text = generate_text(model, \"I have a dream\")\nabsmax_text   = generate_text(model_abs, \"I have a dream\")\nzp_text       = generate_text(model_zp, \"I have a dream\")\n\nprint(f\"Original model:\\n{original_text}\")\nprint(\"-\" * 50)\nprint(f\"Absmax model:\\n{absmax_text}\")\nprint(\"-\" * 50)\nprint(f\"Zeropoint model:\\n{zp_text}\")\n```", "```py\nOriginal model:\nI have a dream, and it is a dream I believe I would get to live in my future. I love my mother, and there was that one time I had been told that my family wasn't even that strong. And then I got the\n--------------------------------------------------\nAbsmax model:\nI have a dream to find out the origin of her hair. She loves it. But there's no way you could be honest about how her hair is made. She must be crazy.\n\nWe found a photo of the hairstyle posted on\n--------------------------------------------------\nZeropoint model:\nI have a dream of creating two full-time jobs in America—one for people with mental health issues, and one for people who do not suffer from mental illness—or at least have an employment and family history of substance abuse, to work part\n```", "```py\ndef calculate_perplexity(model, text):\n    # Encode the text\n    encodings = tokenizer(text, return_tensors='pt').to(device)\n\n    # Define input_ids and target_ids\n    input_ids = encodings.input_ids\n    target_ids = input_ids.clone()\n\n    with torch.no_grad():\n        outputs = model(input_ids, labels=target_ids)\n\n    # Loss calculation\n    neg_log_likelihood = outputs.loss\n\n    # Perplexity calculation\n    ppl = torch.exp(neg_log_likelihood)\n\n    return ppl\n\nppl     = calculate_perplexity(model, original_text)\nppl_abs = calculate_perplexity(model_abs, absmax_text)\nppl_zp  = calculate_perplexity(model_zp, absmax_text)\n\nprint(f\"Original perplexity:  {ppl.item():.2f}\")\nprint(f\"Absmax perplexity:    {ppl_abs.item():.2f}\")\nprint(f\"Zeropoint perplexity: {ppl_zp.item():.2f}\")\n```", "```py\nOriginal perplexity:  15.53\nAbsmax perplexity:    17.92\nZeropoint perplexity: 17.97\n```", "```py\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel_int8 = AutoModelForCausalLM.from_pretrained(model_id,\n                                             device_map='auto',\n                                             load_in_8bit=True,\n                                             )\nprint(f\"Model size: {model_int8.get_memory_footprint():,} bytes\")\n```", "```py\nModel size: 176,527,896 bytes\n```", "```py\n# Generate text with quantized model\ntext_int8 = generate_text(model_int8, \"I have a dream\")\n\nprint(f\"Original model:\\n{original_text}\")\nprint(\"-\" * 50)\nprint(f\"LLM.int8() model:\\n{text_int8}\")\n```", "```py\nOriginal model:\nI have a dream, and it is a dream I believe I would get to live in my future. I love my mother, and there was that one time I had been told that my family wasn't even that strong. And then I got the\n--------------------------------------------------\nLLM.int8() model:\nI have a dream. I don't know what will come of it, but I am going to have to look for something that will be right. I haven't thought about it for a long time, but I have to try to get that thing\n```", "```py\nprint(f\"Perplexity (original):   {ppl.item():.2f}\")\n\nppl = calculate_perplexity(model_int8, text_int8)\nprint(f\"Perplexity (LLM.int8()): {ppl.item():.2f}\")\n```", "```py\nPerplexity (original):   15.53\nPerplexity (LLM.int8()): 7.93\n```"]