["```py\nversion: '3'\nservices:\n  spark:\n    image: bitnami/spark:3.3.1\n    environment:\n      - SPARK_MODE=master\n    ports:\n      - '8080:8080'\n      - '7077:7077'\n    volumes:\n      - ./data:/data\n      - ./src:/src\n  spark-worker:\n    image: bitnami/spark:3.3.1\n    environment:\n      - SPARK_MODE=worker\n      - SPARK_MASTER_URL=spark://spark:7077\n      - SPARK_WORKER_MEMORY=4G\n      - SPARK_EXECUTOR_MEMORY=4G\n      - SPARK_WORKER_CORES=4\n    ports:\n      - '8081:8081'\n    volumes:\n      - ./data:/data\n      - ./src:/src\n  jupyter:\n    image: jupyter/pyspark-notebook:spark-3.3.1\n    ports:\n      - '8890:8888'\n    volumes:\n      - ./data:/data\n```", "```py\n# Install the delta-spark package.\n!pip install delta-spark\n```", "```py\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructField, StructType, StringType, IntegerType, DoubleType\nimport pyspark.sql.functions as F\n\nfrom delta.pip_utils import configure_spark_with_delta_pip\n\nspark = (\n    SparkSession\n    .builder.master(\"spark://spark:7077\")\n    .appName(\"DeltaLakeFundamentals\")\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n)\n\nspark = configure_spark_with_delta_pip(spark).getOrCreate()\n```", "```py\nSCHEMA = StructType(\n    [\n        StructField('id', StringType(), True),          # ACCIDENT ID\n        StructField('data_inversa', StringType(), True),# DATE\n        StructField('dia_semana', StringType(), True),  # DAY OF WEEK\n        StructField('horario', StringType(), True),     # HOUR\n        StructField('uf', StringType(), True),          # BRAZILIAN STATE\n        StructField('br', StringType(), True),          # HIGHWAY\n        # AND OTHER FIELDS OMITTED TO MAKE THIS CODE BLOCK SMALL\n    ]\n)\n\ndf_acidentes = (\n    spark\n    .read.format(\"csv\")\n    .option(\"delimiter\", \";\")\n    .option(\"header\", \"true\")\n    .option(\"encoding\", \"ISO-8859-1\")\n    .schema(SCHEMA)\n    .load(\"/data/acidentes/datatran2020.csv\")\n)\n\ndf_acidentes.show(5)\n```", "```py\ndf_acidentes\\\n    .write.format(\"delta\")\\\n    .mode(\"overwrite\")\\\n    .save(\"/data/delta/acidentes/\")\n```", "```py\ndf_acidentes_delta = (\n    spark\n    .read.format(\"delta\")\n    .load(\"/data/delta/acidentes/\")\n)\n```", "```py\ndf_acidentes_delta.select([\"id\", \"data_inversa\", \"dia_semana\", \"horario\", \"uf\"]).show(5)\n```", "```py\ndf_acidentes_delta.count()\n\n>> Output: 63576\n```", "```py\n# READING THE 2019 DATA\ndf_acidentes_2019 = (\n    spark\n    .read.format(\"csv\")\n    .option(\"delimiter\", \";\")\n    .option(\"header\", \"true\")\n    .schema(SCHEMA)\n    .load(\"/data/acidentes/datatran2019.csv\")\n)\n```", "```py\ndf_acidentes_2019\\\n    .write.format(\"delta\")\\\n    .mode(\"append\")\\\n    .save(\"/data/delta/acidentes/\")\n```", "```py\ndf_acidentes_delta.count()\n\n>> Output: 131132\n```", "```py\nfrom delta.tables import DeltaTable\n\ndelta_table = DeltaTable.forPath(spark, \"/data/delta/acidentes/\")\ndelta_table.history().show()\n```", "```py\ndelta_table.history().select(\"version\", \"timestamp\", \"operation\", \"operationParameters\").show(10, False)\n```", "```py\ndf_acidentes_latest = (\n    spark\n    .read.format(\"delta\")\n    .load(\"/data/delta/acidentes/\")\n)\ndf_acidentes_latest.count()\n\n>> Output: 131132\n```", "```py\ndf_acidentes_version_0 = (\n    spark\n    .read.format(\"delta\")\n    .option(\"versionAsOf\", 0)\n    .load(\"/data/delta/acidentes/\")\n)\ndf_acidentes_version_0.count()\n\n>> Output: 63576\n```", "```py\ndelta_table.restoreToVersion(0)\n```", "```py\n# Counting the number of rows in the latest version\ndf_acidentes_latest.count()\n```", "```py\ndelta_table.history().select(\"version\", \"timestamp\", \"operation\", \"operationParameters\").show(10, False)\n```", "```py\ndelta_table.restoreToVersion(1)\n```", "```py\ndf_acidentes_2016 = (\n    spark\n    .read.format(\"csv\")\n    .option(\"delimiter\", \";\")\n    .option(\"header\", \"true\")\n    .option(\"encoding\", \"ISO-8859-1\")\n    .schema(SCHEMA)\n    .load(\"/data/acidentes/datatran2016.csv\")\n)\n\ndf_acidentes_2016.select(\"data_inversa\").show(5)\n```", "```py\ndf_acidentes_2016\\\n    .write.format(\"delta\")\\\n    .mode(\"append\")\\\n    .save(\"/data/delta/acidentes/\")\n\ndf_acidentes_latest.count()\n>> Output: 227495\n```", "```py\ndf_acidentes_latest.createOrReplaceTempView(\"acidentes_latest\")\n\nspark.sql(\n    \"\"\"\n    UPDATE acidentes_latest\n    SET data_inversa = CAST( TO_DATE(data_inversa, 'dd/MM/yy') AS STRING)\n    WHERE data_inversa LIKE '%/16'\n    \"\"\"\n)\n```", "```py\ndf_acidentes_latest.filter( F.col(\"data_inversa\").like(\"%/16\") ).count()\n>> Output: 0\n```", "```py\n# FULL DATA FROM 2018\ndf_acidentes_2018 = (\n    spark\n    .read.format(\"csv\")\n    .option(\"delimiter\", \";\")\n    .option(\"header\", \"true\")\n    .option(\"encoding\", \"ISO-8859-1\")\n    .schema(SCHEMA)\n    .load(\"/data/acidentes/datatran2018.csv\")\n)\n\n# SAMPLE WITH pessoas=0\ndf_acidentes_2018_zero = (\n  df_acidentes_2018\n  .withColumn(\"pessoas\", F.lit(0))\n  .limit(1000)\n)\n\ndf_acidentes_2018_zero\\\n    .write.format(\"delta\")\\\n    .mode(\"append\")\\\n    .save(\"/data/delta/acidentes/\")\n```", "```py\ndf_acidentes_latest.createOrReplaceTempView(\"acidentes_latest\")\ndf_acidentes_2018.createOrReplaceTempView(\"acidentes_2018_new_counts\")\n\nspark.sql(\n    \"\"\"\n    MERGE INTO acidentes_latest\n    USING acidentes_2018_new_counts\n\n    ON acidentes_latest.id = acidentes_2018_new_counts.id\n    AND acidentes_latest.data_inversa = acidentes_2018_new_counts.data_inversa\n\n    WHEN MATCHED THEN\n        UPDATE SET pessoas = acidentes_latest.pessoas + acidentes_2018_new_counts.pessoas\n\n    WHEN NOT MATCHED THEN\n        INSERT *\n    \"\"\"\n)\n```"]