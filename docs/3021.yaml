- en: Topic Modelling using ChatGPT API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/topic-modelling-using-chatgpt-api-8775b0891d16?source=collection_archive---------0-----------------------#2023-10-04](https://towardsdatascience.com/topic-modelling-using-chatgpt-api-8775b0891d16?source=collection_archive---------0-----------------------#2023-10-04)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Comprehensive guide to ChatGPT API for newbies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://miptgirl.medium.com/?source=post_page-----8775b0891d16--------------------------------)[![Mariya
    Mansurova](../Images/b1dd377b0a1887db900cc5108bca8ea8.png)](https://miptgirl.medium.com/?source=post_page-----8775b0891d16--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8775b0891d16--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8775b0891d16--------------------------------)
    [Mariya Mansurova](https://miptgirl.medium.com/?source=post_page-----8775b0891d16--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F15a29a4fc6ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modelling-using-chatgpt-api-8775b0891d16&user=Mariya+Mansurova&userId=15a29a4fc6ad&source=post_page-15a29a4fc6ad----8775b0891d16---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8775b0891d16--------------------------------)
    ·16 min read·Oct 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8775b0891d16&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modelling-using-chatgpt-api-8775b0891d16&user=Mariya+Mansurova&userId=15a29a4fc6ad&source=-----8775b0891d16---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8775b0891d16&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modelling-using-chatgpt-api-8775b0891d16&source=-----8775b0891d16---------------------bookmark_footer-----------)![](../Images/9b8c199c3e9d4af61dc5e63fdec0154d.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Mia Baker](https://unsplash.com/@miabaker?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: In [the previous article](/topics-per-class-using-bertopic-252314f2640), I used
    BERTopic for Topic Modelling. The task was to compare the main topics in reviews
    for various hotel chains. This approach with BERTopic worked out, and we got some
    insights from the data. For example, from reviews, we could see that Holiday Inn,
    Travelodge and Park Inn have more reasonable prices for value.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8765ecf920cb7225ac217142a3f0c668.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: However, the most cutting-edge technology to analyse texts nowadays is LLMs
    (Large Language Models).
  prefs: []
  type: TYPE_NORMAL
- en: LLMs disrupted the process of building ML applications. Before LLMs, if we wanted
    to do sentiment analysis or chatbot, we would first spend several months getting
    labelled data and training models. Then, we would deploy it in production (it
    would also take a couple of months at least). With LLMs, we can solve such problems
    within a few hours.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/363a3e61def4bbc987e1d238f9383b7d.png)'
  prefs: []
  type: TYPE_IMG
- en: Slide from the talk [“Opportunities in AI”](https://www.youtube.com/watch?v=5p248yoa3oE)
    by Andrew Ng
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see whether LLMs could help us solve our task: to define one or several
    topics for customer reviews.'
  prefs: []
  type: TYPE_NORMAL
- en: LLM basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before jumping into our task, let’s discuss the basics of LLMs and how they
    could be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Large Language Models are trained on enormous amounts of text to predict the
    next word for the sentence. It’s a straightforward supervised Machine Learning
    task: we have the set of the sentences’ beginnings and the following words for
    them.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ee392a485821eff34485232efa5746ec.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: You can play with a basic LLM, for example, `text-davinci-003`, on [nat.dev](https://nat.dev/).
  prefs: []
  type: TYPE_NORMAL
- en: In most business applications, we need not a generic model but one that can
    solve problems. Basic LLMs are not perfect for such tasks because they are trained
    to predict the most likely next word. But on the internet, there are a lot of
    texts where the next word is not a correct answer, for example, jokes or just
    a list of questions to prepare for the exam.
  prefs: []
  type: TYPE_NORMAL
- en: That’s why, nowadays, Instruction Tuned LLMs are very popular for business cases.
    These models are basic LLMs, fine-tuned on datasets with instructions and good
    answers (for example, [OpenOrca dataset](https://huggingface.co/datasets/Open-Orca/OpenOrca)).
    Also, RLHF (Reinforcement Learning with Human Feedback) approach is often used
    to train such models.
  prefs: []
  type: TYPE_NORMAL
- en: The other important feature of Instruction Tuned LLMs is that they are trying
    to be helpful, honest and harmless, which is crucial for the models that will
    communicate with customers (especially vulnerable ones).
  prefs: []
  type: TYPE_NORMAL
- en: What are the primary tasks for LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LLMs are primarily used for tasks with unstructured data (not the cases when
    you have a table with lots of numbers). Here is the list of the most common applications
    for texts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Summarisation** — giving a concise overview of the text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text analysis**, for example, sentiment analysis or extracting specific features
    (for example, labels mentioned in hotel reviews).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text transformations** include translating to different languages, changing
    tone, or formatting from HTML to JSON.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generation**, for example, to generate a story from a prompt, respond to
    customer questions or help to brainstorm about some problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It looks like our task of topic modelling is the one where LLMs could be rather
    beneficial. It’s an example of **Text analysis**.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Engineering 101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We give tasks to LLMs using instructions that are often called prompts. You
    can think of LLM as a very motivated and knowledgeable junior specialist who is
    ready to help but needs clear instructions to follow. So, a prompt is critical.
  prefs: []
  type: TYPE_NORMAL
- en: There are a few main principles that you should take into account while creating
    prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Principle #1: Be as clear and specific as possible'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use delimiters to split different sections of your prompt, for example, separating
    different steps in the instruction or framing user message. The common delimeters
    are `”””` , `---` , `###` , `<>` or XML tags.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the format for the output. For example, you could use JSON or HTML and
    even specify a list of possible values. It will make response parsing much easier
    for you.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Show a couple of input & output examples to the model so it can see what you
    expect as separate messages. Such an approach is called few-shot prompting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, it could be helpful to instruct the model to check assumptions and conditions.
    For example, to ensure that the output format is JSON and returned values are
    from the specified list.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Principle #2:** Push the model to think about the answer'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Daniel Kahneman’s famous book “Thinking Fast and Slow” shows that our mind consists
    of 2 systems. System 1 works instinctively and allows us to give answers extremely
    quickly and with minimal effort (this system helped our ancestors to survive after
    meeting tigers). System 2 requires more time and concentration to get an answer.
    We tend to use System 1 in as many situations as possible because it’s more effective
    for basic tasks. Surprisingly, LLMs do the same and often jump to conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: We can push the model to think before answering and increase the quality.
  prefs: []
  type: TYPE_NORMAL
- en: We can give a model step-by-step instructions to force it to go through all
    the steps and don’t rush to conclusions. This approach is called “Chain of thought”
    reasoning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The other approach is to split your complex task into smaller ones and use
    different prompts for each elementary step. Such an approach has multiple advantages:
    it’s easier to support this code (good analogy: spaghetti code vs. modular one);
    it may be less costly (you don’t need to write long instructions for all possible
    cases); you can augment external tools at specific points of the workflow or include
    human in the loop.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the above approaches, we don’t need to share all the reasoning with the
    end user. We can just keep it as an inner monologue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suppose we want the model to check some results (for example, from the other
    model or students). In that case, we can ask it to independently get the result
    first or evaluate it against the list of criteria before coming to conclusions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find an example of a helpful system prompt from Jeremy Howard that pushes
    the model to reason in [this jupyter notebook](https://github.com/fastai/lm-hackers/blob/main/lm-hackers.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: 'Principle #3: Beware hallucinations'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The well-known problem of LLMs is hallucinations. It’s when a model tells you
    information that looks plausible but not true.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you ask GPT to provide the most popular papers on DALL-E 3,
    two out of three URLs are invalid.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b0eb48faf52a7ce342bd161c3fa97fb0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The common sources of hallucinations:'
  prefs: []
  type: TYPE_NORMAL
- en: The model doesn’t see many URLs, and it doesn’t know much about it. So, it tends
    to create fake URLs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It doesn’t know about itself (because there was no info about GPT-4 when the
    model was pre-trained).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model doesn’t have real-time data and will likely tell you something random
    if you ask about recent events.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To reduce hallucinations, you can try the following approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: Ask the model to link the answer to the relevant information from the context,
    then answer the question based on the found data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the end, ask the model to validate the result based on provided factual information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember that Prompt Engineering is an iterative process. It’s unlikely that
    you will be able to solve your task ideally from the first attempt. It’s worth
    trying multiple prompts on a set of example inputs.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The other thought-provoking idea about LLM answers’ quality is that if the model
    starts to tell you absurd or non-relevant things, it’s likely to proceed. Because,
    on the internet, if you see a thread where nonsense is discussed, the following
    discussion will likely be of poor quality. So, if you’re using the model in a
    chat mode (passing the previous conversation as the context), it might be worth
    starting from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ChatGPT from OpenAI is one of the most popular LLMs now, so for this example,
    we will be using ChatGPT API.
  prefs: []
  type: TYPE_NORMAL
- en: For now, GPT-4 is the best-performing LLM we have (according to [fasteval](https://fasteval.github.io/FastEval/)).
    However, it may be enough for non-chat tasks to use the previous version, GPT-3.5.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up account
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To use ChatGPT API, you need to register on [platform.openai.com](https://platform.openai.com/).
    As usual, you can use authentication from Google. Keep in mind that ChatGPT API
    access is not related to the ChatGPT Plus subscription you might have.
  prefs: []
  type: TYPE_NORMAL
- en: 'After registration, you also need to top up your balance. Since you will pay
    for API calls as you go. You can do it at the “Billing” tab. The process is straightforward:
    you need to fill in your card details and the initial amount you are ready to
    pay.'
  prefs: []
  type: TYPE_NORMAL
- en: The last important step is to create an API Key (a secret key you will use to
    access API). You can do it at the “API Keys” tab. Ensure you save the key since
    you won’t be able to access it afterwards. However, you can create a new key if
    you’ve lost the previous one.
  prefs: []
  type: TYPE_NORMAL
- en: Pricing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As I mentioned, you will be paying for API calls, so understanding how it works
    is worth it. I advise you to look through [the Pricing documentation](https://openai.com/pricing)
    for the most up-to-date info.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, the price depends on the model and the number of tokens. The more
    complex model would cost you more: ChatGPT 4 is more expensive than ChatGPT 3.5,
    and ChatGPT 3.5 with 16K context is more costly than ChatGPT 3.5 with 4K context.
    You will also have slightly different prices for input tokens (your prompt) and
    output (model response).'
  prefs: []
  type: TYPE_NORMAL
- en: However, all prices are for 1K tokens, so one of the main factors is the size
    of your input and output.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s discuss what a token is. The model splits text into tokens (widely used
    words or parts of the word). For the English language, one token on average is
    around four characters, and each word is 1.33 tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how one of our hotels review will be split into tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/797d1d2accfd035b12420713b260b7c6.png)'
  prefs: []
  type: TYPE_IMG
- en: You can find the exact number of tokens for your model using `tiktoken` python
    library.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: ChatGPT API calls
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenAI provides a python package that could help you work with ChatGPT. Let’s
    start with a simple function that will get messages and return responses.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s discuss the meaning of the main parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_tokens` — limit on the number of tokens in the output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`temperature` here is the measure of entropy (or randomness in the model).
    So if you specify `temperature = 0`, you will always get the same result. Increasing
    `temperature` will let the model to deviate a bit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`messages` is a set of messages for which the model will create a response.
    Each message has `content` and `role`. There could be several roles for messages:
    `user`, `assistant` (model) and `system` (an initial message that sets assistant
    behaviour).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s look at the case of topic modelling with two stages. First, we will translate
    the review into English and then define the main topics.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/93f080b59d431989a6693f93e826f0fd.png)'
  prefs: []
  type: TYPE_IMG
- en: Since the model doesn’t keep a state for each question in the session, we need
    to pass the whole context. So, in this case, our `messages` argument should look
    like this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Also, OpenAI provides a Moderation API that could help you check whether your
    customer input or model output is good enough and doesn’t contain violence, hate,
    discrimination, etc. These calls are free.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As a result, we will get a dictionary with both flags for each category and
    raw weights. You can use lower thresholds if you need more strict moderation (for
    example, if you’re working on products for kids or vulnerable customers).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We won’t need the Moderation API for our task of topic modelling, but it could
    be useful if you are working on a chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: Another good piece of advice, if you’re working with customers’ input, is to
    eliminate the delimiter from the text to avoid prompt injections.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Model evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last crucial question to discuss is how to evaluate the results of LLM.
    There are two main cases.
  prefs: []
  type: TYPE_NORMAL
- en: '**There’s one correct answer (for example, a classification problem)**. In
    this case, you can use supervised learning approaches and look at standard metrics
    (like precision, recall, accuracy, etc.).'
  prefs: []
  type: TYPE_NORMAL
- en: '**There’s no correct answer (topic modelling or chat use case).**'
  prefs: []
  type: TYPE_NORMAL
- en: You can use another LLM to access the results of this model. It’s helpful to
    provide the model with a set of criteria to understand the answers’ quality. Also,
    it’s worth using a more complex model for evaluation. For example, you use ChatGPT-3.5
    in production since it’s cheaper and good enough for the use case, but for the
    offline assessment on a sample of cases, you can use ChatGPT-4 to ensure the quality
    of your model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The other approach is to compare with an “ideal” or expert answer. You can use
    [BLEU score](https://en.wikipedia.org/wiki/BLEU) or another LLM ([OpenAI evals
    project](https://github.com/openai/evals/blob/main/evals/registry/modelgraded/fact.yaml)
    has a lot of helpful prompts for it).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In our case, we don’t have one correct answer for customer review, so we will
    need to compare results with expert answers or use another prompt to assess the
    quality of results.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve quickly looked at the LLM basics and are now ready to move on to the initial
    topic modelling task.
  prefs: []
  type: TYPE_NORMAL
- en: Empowering BERTopic with ChatGPT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most logical enhancement of the previous approach is using LLM to define
    the topics we’ve already identified using BERTopic. We can use the OpenAI representation
    model with a summarisation prompt for this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Then, BERTopic makes a request to ChatGPT API for each topic, providing keywords
    and a set of representative documents. The response from ChatGPT API is used as
    a model representation.
  prefs: []
  type: TYPE_NORMAL
- en: You can find more details in the [BERTopic documentation](https://maartengr.github.io/BERTopic/getting_started/representation/llm.html).
  prefs: []
  type: TYPE_NORMAL
- en: It’s a reasonable approach, but still we rely entirely on BERTopic to cluster
    documents using embeddings and we can see that topics are a bit entangled. Could
    we get rid of it and use our initial texts as the source of truth?
  prefs: []
  type: TYPE_NORMAL
- en: Topic Modelling using ChatGPT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Actually, we can use ChatGPT for this task and split it into two steps: define
    a list of topics and then assign one or multiple topics for each customer review.
    Let’s try to do it.'
  prefs: []
  type: TYPE_NORMAL
- en: Defining a list of topics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First of all, we need to define the list of topics. Then, we could use it to
    classify reviews.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, we could send all texts to ChatGPT and ask it to define the main topics.
    However, it might be pretty costly and not so straightforward. There are more
    than 2.5M tokens in the whole dataset of hotels’ reviews. So we won’t be able
    to feed all comments into one dataset (because the ChatGPT-4 now has only 32K
    as a context).
  prefs: []
  type: TYPE_NORMAL
- en: To overcome this limitation, we can define a representative subset of documents
    that fit the context size. BERTopic returns a set of the most representative documents
    for each topic so we can fit a basic BERTopic model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can use these documents to define a list of relevant topics.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Let’s check the size of `user_message` to ensure that it fits the context.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: It exceeds 4K, so we need to use `gpt-3.5-turbo-16k` for this task.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As a result, we got a list of relevant topics, and it looks pretty reasonable.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/211eb530e63f2d15be600e108eff577a.png)'
  prefs: []
  type: TYPE_IMG
- en: Classifying reviews by topics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next step is to assign one or several topics for each customer review. Let’s
    compose a prompt for it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Such an approach gives pretty good results. It can handle even comments in other
    languages (like German in the example below).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a25d042dea8303f0ffaf0fbdf73eb812.png)'
  prefs: []
  type: TYPE_IMG
- en: The only mistake in this small data sample is the `Restaurant` topic for the
    first comment. There were no mentions of the hotel’s restaurant in the customer
    review, only the ones nearby. But let’s look at our prompt. We don’t tell the
    model that we are interested only in specific restaurants, so it’s plausible for
    it to assign such a topic to the comment.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s think about how we could solve this problem. If we change the prompt a
    bit and provide the model with not only topic names (for example, “*Restaurant*”)
    but also topic descriptions (for example, “*A few reviews mention the hotel’s
    restaurant, either positively or negatively.*”), the model will have enough info
    to fix this issue. With the new prompt, the model returns only relevant `Location`
    and `Room Size` topics for the first comment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this article, we’ve discussed the main questions related to LLM practical
    usage: how they work, their main applications, and how to use LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: We’ve built a prototype for Topic Modelling using ChatGPT API. Based on a small
    sample of examples, it works amazingly and gives results that can be easily interpreted.
  prefs: []
  type: TYPE_NORMAL
- en: The only drawback of the ChatGPT approach is its cost. It would cost more than
    75 USD to classify all the texts in our hotel reviews dataset (based on 2.5M tokens
    in the dataset and pricing for GPT-4). So, even though ChatGPT is the best-performing
    model now, it might be worth looking at open-source alternatives if you need to
    work with massive datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you a lot for reading this article. I hope it was insightful to you. If
    you have any follow-up questions or comments, please leave them in the comments
    section.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Ganesan, Kavita and Zhai, ChengXiang. (2011). OpinRank Review Dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: UCI Machine Learning Repository.* [*https://doi.org/10.24432/C5QW4W*](https://doi.org/10.24432/C5QW4W.)
  prefs: []
  type: TYPE_NORMAL
- en: Reference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This article is based on information from the following sources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[The Hacker’s Guide to Language Models](https://www.youtube.com/watch?v=jkrNMKz9pWU)
    by Jeremy Howard'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ChatGPT Prompt Engineering for Developers](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)
    by DeepLearning.AI'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building Systems with ChatGPT API](https://www.deeplearning.ai/short-courses/building-systems-with-chatgpt/)
    by DeepLearning.AI'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
