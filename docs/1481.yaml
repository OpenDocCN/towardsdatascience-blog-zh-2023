- en: Evolving a Data Pipeline Testing Plan
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/evolving-a-testing-plan-for-a-data-pipeline-3e1fc44998d9?source=collection_archive---------4-----------------------#2023-04-30](https://towardsdatascience.com/evolving-a-testing-plan-for-a-data-pipeline-3e1fc44998d9?source=collection_archive---------4-----------------------#2023-04-30)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**The Perils of Exhaustive Multi-Source Multi-Destination Test-Driven Development**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://farmi.medium.com/?source=post_page-----3e1fc44998d9--------------------------------)[![Moussa
    Taifi PhD](../Images/ba824e71aede3ce7e9d5d73505055403.png)](https://farmi.medium.com/?source=post_page-----3e1fc44998d9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3e1fc44998d9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3e1fc44998d9--------------------------------)
    [Moussa Taifi PhD](https://farmi.medium.com/?source=post_page-----3e1fc44998d9--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F215bedf6878d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevolving-a-testing-plan-for-a-data-pipeline-3e1fc44998d9&user=Moussa+Taifi+PhD&userId=215bedf6878d&source=post_page-215bedf6878d----3e1fc44998d9---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3e1fc44998d9--------------------------------)
    ·9 min read·Apr 30, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3e1fc44998d9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevolving-a-testing-plan-for-a-data-pipeline-3e1fc44998d9&user=Moussa+Taifi+PhD&userId=215bedf6878d&source=-----3e1fc44998d9---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3e1fc44998d9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevolving-a-testing-plan-for-a-data-pipeline-3e1fc44998d9&source=-----3e1fc44998d9---------------------bookmark_footer-----------)![](../Images/a3a35cf7b8442d748afe2e6beb93454c.png)'
  prefs: []
  type: TYPE_NORMAL
- en: (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: The first contact with the ideas of Test-Driven Development (TDD) leaves many
    beginner data engineers in shock at what TDD promises. Faster development, cleaner
    code, career advancement, and world domination, to name a few. Yet, the reality
    is quite different. The initial attempts at applying TDD to data engineering leave
    many data engineers demoralized. Extracting the value of TDD takes so much efforts.
    It requires a deep knowledge of testing techniques that are not in the beginner
    DE toolbelt. The process of learning “what” to test is hard. Learning the tradeoffs
    inherent to applying TDD to data pipelines is even harder.
  prefs: []
  type: TYPE_NORMAL
- en: In this article we look at how to evolve a data pipeline testing plan to avoid
    feeling the full pain that comes from over-specified testing.
  prefs: []
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are the perils of test-driven development? For all its benefits, TDD can
    be a dangerous thing for a new data engineer. The initial drive to test everything
    is strong, and it can lead to sub-optimal design choices. Too much of a good thing,
    as they say.
  prefs: []
  type: TYPE_NORMAL
- en: The drive to test every single part of the data pipeline is a tempting direction
    for engineering-minded folks. But in order to preserve one’s sanity, there must
    necessarily be some restraint. Otherwise, you’ll end up with a jungle of tests
    surrounded by a sea of red. And balls of mud are always not far behind.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, say we have the following data pipeline.:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/34fb6d28fafbb5804e5375331bc6bd61.png)'
  prefs: []
  type: TYPE_IMG
- en: (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: We have three data sources, six transformations, and two data destinations.
  prefs: []
  type: TYPE_NORMAL
- en: '**What would an inexperienced data engineer produce as a test plan?**'
  prefs: []
  type: TYPE_NORMAL
- en: We’ve all be there.
  prefs: []
  type: TYPE_NORMAL
- en: 'Solution #1: Exhaustive Multi-Source Multi-Destination Paths with Edge Length
    > 0'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using the classic 3 part testing framework we can safely assume that our data
    engineer will start with this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unit tests** ✅ : Sure take each transformation, generate some sample input
    data for each transformation, run the sample data through each step of the pipeline,
    capture the results, and use the output to validate the transformation logic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**E2E test** ✅ : We are gonna need to run the pipeline on full prod data anyways
    so let’s run the whole pipeline on a sample from the production data, capture
    the results and use that output to validate the end to end pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration tests** ❓❓❓: But what to do here? The first inclination is to
    build one test for each of the combinations of the transformation stage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/1375333f1dc53f627a14874b99f8a77b.png)'
  prefs: []
  type: TYPE_IMG
- en: (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: After some back of the envelope calculation our data engineer starts internalizing
    the fact that the combination of the 6 transformation steps grows rapidly. There
    must be a better way.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/494427dab35f7266f2efb324ae2305d6.png)'
  prefs: []
  type: TYPE_IMG
- en: (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Testing every single combination with edges of variable length is not gonna
    meet the deadline we promised to the customer. We should have budgeted for more
    time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Solution #2:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*“OK OK, but it can’t be that bad.”*'
  prefs: []
  type: TYPE_NORMAL
- en: Yes considering that integration tests are not going to be touching real sources
    and sinks, then so be it, let’s plug these 6 transformations together. We get
    the graph combinations below. We get approximately 10 integration tests.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d1a22164634f331b5a07cf73ac88edff.png)'
  prefs: []
  type: TYPE_IMG
- en: (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: But remember this is data engineering, which means that input data is out of
    our control, and changes over time. So we need to add the data-centric tests in
    there. (You surely know better but lets follow this argument).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0b707e2d18b68badfd88b9ea25f050c5.png)'
  prefs: []
  type: TYPE_IMG
- en: (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Solution #3:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*“Right but can’t we summarize this somehow, there must be a core set of data
    scenarios we need to absolutely support? Like a priority list of data validation
    things?”*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sure, but even if we delay the data validation checks we still get this picture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1385ef5c11a9dc543e248a6135b69dfb.png)'
  prefs: []
  type: TYPE_IMG
- en: (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: '*“Do we really need this much testing? Isn’t acceptance testing about testing
    what the users sees? Can’t we sacrifice the developer experience in order to deliver
    a good product, on time, that solves the customers problem?”*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sure, yes the next logical step is to only run the e2e tests and move one with
    our lives. However, there is a middle step that solves both problems of *“too
    many integration tests”* and *“solid data validation tests”*. Your probably used
    it before but didn’t have a name for it: **“Inline assertions”**. This is a quite
    useful trick from the defensive programming tradition.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Solution #4:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The core idea of these ”Inline Assertions” is that you build, when possible,
    the whole pipeline as a monolith that includes assertions about the code interfaces
    AND the data interfaces between the components of your monolith.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/95c83461c67b0365f2905b90586c92cd.png)'
  prefs: []
  type: TYPE_IMG
- en: (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: That’s it, you put that in the red, green, refactor development loop, and keep
    growing that list of assertions as random things happen.
  prefs: []
  type: TYPE_NORMAL
- en: We are running a little hot here, but notice that we are using production data
    sources, and production sinks. If you are in a bind, go for it. If you have some
    time, at least create dedicated testing sinks, and remember to put limits on the
    number of rows you get from the input data sources.
  prefs: []
  type: TYPE_NORMAL
- en: This might seem obvious to you, but we are all learning how to build data pipelines
    that solves the customers problem here :)
  prefs: []
  type: TYPE_NORMAL
- en: How would that look in the revered Testing Pyramid?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3b89b51ec06a237e419cf316492ed513.png)'
  prefs: []
  type: TYPE_IMG
- en: (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s call that **DEE2E++ Testing.**
  prefs: []
  type: TYPE_NORMAL
- en: Data Engineering End to End ++ Testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'There seem to be two flavors of DEE2E++ Testing:'
  prefs: []
  type: TYPE_NORMAL
- en: Ubiquitous Anti-Corruption Layers (U-ACL)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mostly-Warnings Anti-Corruption Layer (MW-ACL)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On the Ubiquitous side it looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6cb69e674c656c37de883a8ac33cfd9c.png)'
  prefs: []
  type: TYPE_IMG
- en: (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Each transformation gets one input anti-corruption layer that protects it from
    upstream changes, and one output anti-corruption layer that protects the downstream
    consumers from the current transformation’s internal changes. If something changes
    in the upstream data schema or contents, then the input ACL with stop the processing
    and report the error to the user. Then if we change something in the data schema
    or contents of the current transformation, then the output ACL will also catch
    the errors and stop the processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is quite a lot of work for a starting data engineer. Adding mandatory
    validation rules on each and every transformation will push the data eng to “batch
    work”. Instead of breaking the pipeline into multiple steps, they will say to
    themselves: *“If I have to add these ACLs for each transform, that’s gonna be
    2 times the number of transformations. I might as well add just two. One at the
    top of the pipeline and one at the bottom. I’ll deal with the internals of the
    transformations on my own.”* That isa valid initial approach where the focus is
    on 1) top level ingestion logic and 2) customer-visible data outputs. The issue
    with this strategy is that we lose the benefits of tests in regards to Localizing
    bugs. If there is a bug in transformation 4 of 6 then the ACL tests will only
    show that the final output is invalid, not that transformation 4 is the culprit.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, as the data sources evolve, what we are talking about here is 90%
    warnings and 10% blocking errors. Just because a new column showed up on the input
    data does not mean that the whole pipeline should fail. And just because the distribution
    mean of some column has shifted a bit does not mean that all of the data is invalid.
    Customers might still be interested in the freshest available data for making
    their business decisions and do reconciliation later if needed.
  prefs: []
  type: TYPE_NORMAL
- en: For that, you need the “Mostly-Warnings Anti-Corruption Layer”.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d89d19ebdef98e22c424c0419410d5aa.png)'
  prefs: []
  type: TYPE_IMG
- en: (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: It notifies the dev that there is something wrong, but does not stop the processing.
    It achieves the same role as the input and output ACLs but for each transformation.
    Also it is much more tolerant to change. This type of ACL emits warnings and metrics
    and the dev can prioritize the warnings later. If something is completely out
    of hand, the dev can backfill the data after fixing the data transformation.
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, YMMV. If the current data pipeline’s output has few established human
    users, then communicating with the consumers will help the new dev learn about
    the domain. On the other hand if this pipeline has a multitude of established
    automated data pipelines that consume the output, then this DEE2E++ Testing might
    not be sufficient. However, new data devs that are starting out are probably not
    assigned on day one to business critical data pipelines that impact hundreds of
    data consumers. So, instead of forcing the new data devs to be crushed by both
    foreign testing techniques, and by mission-critical domains, the DEE2E++ method
    can be good starting point for new data devs.
  prefs: []
  type: TYPE_NORMAL
- en: Here is the DEE2E++ diagram again.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3b89b51ec06a237e419cf316492ed513.png)'
  prefs: []
  type: TYPE_IMG
- en: (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: '*“Wait, Wait, Wait are you saying that each component is only gonna get a Warnings-Only
    Anti-Corruption Layer?”*'
  prefs: []
  type: TYPE_NORMAL
- en: Not “Warnings-only”, “*Mostly*-Warnings”. Some of the assertions will certainly
    stop the processing and fail the job. But yes, that’s the idea. If you do the
    “Ubiquitous Anti-Corruption Layer” strategy, then you will need more time. As
    the domain become clearer you can add stricter ACLs around critical pieces of
    the data pipeline. This domain understanding will help rank the transformations
    in terms of complexity. As you identify the complex ones that need extra care,
    you can move from the “MW-ACL”s to the “U-ACL”s to protect highly critical business
    logic for example.
  prefs: []
  type: TYPE_NORMAL
- en: '*“I mean yes, but then why bother with the unit tests then? Aren’t they covered
    in the inline tests?”*'
  prefs: []
  type: TYPE_NORMAL
- en: Sure, fine, let’s remove them.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ec86ef2a608672412f295ea83954bda2.png)'
  prefs: []
  type: TYPE_IMG
- en: (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: OK? I guess we can all go back to work now.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In short, the common principles of test-driven development can be quite overpowering
    for a new data engineer. It is important to remember that TDD is a design tool,
    not a law. Use it wisely, and it will serve you well. But use it too much, and
    you’ll find yourself in a world of hurt.
  prefs: []
  type: TYPE_NORMAL
- en: In this article we examined what can happen with over-specified testing. First,
    we took a seemingly simple data pipeline and saw what happens when we fall in
    the trap of :“Exhaustive Multi-Source Multi-Destination Paths”. Then we observed
    how integration tests are only the tip of the iceberg when compared to the data-centric
    tests. Finally, we found out that a good place to start for beginner data engineers
    is to focus on E2E tests with “Mostly-Warnings Anti-Corruption Layers” as inline
    tests. This DEE2E++ Testing strategy has two benefits. First, the fresh data engineer
    will not give up on testing from day 1\. Second, it gives the devs breathing room
    to learn about the domain and iterate their data pipeline design using their existing
    basic knowledge of data engineering. Instead of immediately getting lost in the
    micro-level of TDD, they can deliver working software to the stakeholders, and
    then build on the protection that DEE2E++ Testing provides to add more fine grained
    tests as the requirements evolve.
  prefs: []
  type: TYPE_NORMAL
- en: So there you have it. The perils of test-driven development. May you all avoid
    them, and may your data pipelines be ever green.
  prefs: []
  type: TYPE_NORMAL
- en: Want to learn more about modern data pipelines testing techniques?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Checkout my latest book on the subject. This book gives a visual guide to the
    most popular techniques for testing modern data pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: '2023 Book link:'
  prefs: []
  type: TYPE_NORMAL
- en: '***Book link:*** [***Modern Data Pipelines Testing Techniques***](https://leanpub.com/moderndatapipelinestestingtechniques/)*on
    leanpub.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See ya!
  prefs: []
  type: TYPE_NORMAL
- en: '**Disclaimer:** The views expressed on this post are mine and do not necessarily
    reflect the views of my current or past employers.'
  prefs: []
  type: TYPE_NORMAL
