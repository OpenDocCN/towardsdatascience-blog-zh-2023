["```py\nSEARCH_SPACE={\"num_boost_round\": [100, 200],\n             \"min_data_in_leaf\": [5, 10]}\n```", "```py\ndata, info = tfds.load(name='horses_or_humans', as_supervised=True, with_info=True)\n```", "```py\nworker_pool_specs = [\n    {\n        \"machine_spec\": {\n            \"machine_type\": \"n1-standard-4\",\n        },\n        \"replica_count\": 1,\n        \"container_spec\": {\"image_uri\": hpt_container_image_uri, \"args\": CMDARGS},\n    }\n    ]\n```", "```py\n@component\ndef worker_pool_specs(project_id: str,\n    data_region: str,\n    data_pipeline_root: str,\n    hpt_container_image_uri: str,\n    custom_job_service_account: str,\n    input_dataset: Input[Dataset],\n    input_data_schema: str) -> list:\n    \"\"\"\n    Pass the preprocessed data uri to hpt as a worker pool argument. The vanilla hpt API \n    doesn't support 'input data' so it's done this way.\n\n    data_preprocess -> dataset.uri -> CMDARGS -> worker_pool_specs -> hpt\n    \"\"\"\n\n    display_name = 'hpt-pipeline-template'\n    fields = [field.split(':')[0] for field in input_data_schema.split(';')]\n    label = fields[-1]\n    features = ','.join(fields[0:-1])\n    CMDARGS = [\n    \"--training_data_uri=\"+str(input_dataset.uri),\n    \"--training_data_schema=\"+input_data_schema,\n    \"--label=\"+label,\n    \"--features=\"+features\n]\n\n    # The spec of the worker pools including machine type and Docker image\n    worker_pool_specs = [\n    {\n        \"machine_spec\": {\n            \"machine_type\": \"n1-standard-4\",\n        },\n        \"replica_count\": 1,\n        \"container_spec\": {\"image_uri\": hpt_container_image_uri, \"args\": CMDARGS},\n    }\n    ]\n\n    return worker_pool_specs\n```", "```py\ntuning_op = HyperparameterTuningJobRunOp(\n    display_name=display_name,\n    project=project,\n    location=region,\n    worker_pool_specs=worker_pool_specs,\n    study_spec_metrics=study_spec_metrics,\n    study_spec_parameters=study_spec_parameters,\n    max_trial_count=max_trial_count,\n    parallel_trial_count=parallel_trial_count,\n    base_output_directory=base_output_directory,\n)\n\ntrials_op = hyperparameter_tuning_job.GetTrialsOp(\n    gcp_resources=tuning_op.outputs[\"gcp_resources\"]\n)\n\nbest_trial_op = hyperparameter_tuning_job.GetBestTrialOp(\n    trials=trials_op.output, study_spec_metrics=study_spec_metrics\n)\n```", "```py\n@component(\n    packages_to_install=['google-cloud-aiplatform', \n                         'google-cloud-pipeline-components',\n                         'protobuf'], base_image='python:3.7')\ndef GetBestTrialOp(gcp_resources: str, study_spec_metrics: list) -> str:\n\n    from google.cloud import aiplatform\n    from google_cloud_pipeline_components.proto.gcp_resources_pb2 import GcpResources\n    from google.protobuf.json_format import Parse\n    from google.cloud.aiplatform_v1.types import study\n\n    api_endpoint_suffix = '-aiplatform.googleapis.com'\n    gcp_resources_proto = Parse(gcp_resources, GcpResources())\n    gcp_resources_split = gcp_resources_proto.resources[0].resource_uri.partition(\n      'projects')\n    resource_name = gcp_resources_split[1] + gcp_resources_split[2]\n    prefix_str = gcp_resources_split[0]\n    prefix_str = prefix_str[:prefix_str.find(api_endpoint_suffix)]\n    api_endpoint = prefix_str[(prefix_str.rfind('//') + 2):] + api_endpoint_suffix\n\n    client_options = {'api_endpoint': api_endpoint}\n    job_client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n    response = job_client.get_hyperparameter_tuning_job(name=resource_name)\n\n    trials = [study.Trial.to_json(trial) for trial in response.trials]\n\n    if len(study_spec_metrics) > 1:\n        raise RuntimeError('Unable to determine best parameters for multi-objective'\n                       ' hyperparameter tuning.')\n    trials_list = [study.Trial.from_json(trial) for trial in trials]\n    best_trial = None\n    goal = study_spec_metrics[0]['goal']\n    best_fn = None\n    if goal == study.StudySpec.MetricSpec.GoalType.MAXIMIZE:\n        best_fn = max\n    elif goal == study.StudySpec.MetricSpec.GoalType.MINIMIZE:\n        best_fn = min\n    best_trial = best_fn(\n      trials_list, key=lambda trial: trial.final_measurement.metrics[0].value)\n\n    return study.Trial.to_json(best_trial)\n```", "```py\n@component(packages_to_install=['google-cloud-firestore==2.3'])\ndef best_hpt_to_args(hpt_best: str,\n                    project_id: str,\n                    solution_name: str) -> str:\n    \"\"\"\n    Write the best hpt params to firestore. \n    We keep the output to chain this component to the conditional training\n    \"\"\"\n\n    import json\n    from datetime import datetime\n    from google.cloud import firestore\n    hpt_best = json.loads(hpt_best.replace(\"'\", '\"'))\n\n    hpt_best_dict = {}\n\n    for i in hpt_best['parameters']:\n        hpt_best_dict.update({i['parameterId']: i['value']})\n\n    for i in hpt_best['finalMeasurement']['metrics']:\n        hpt_best_dict.update({i['metricId']: i['value']})\n\n    db = firestore.Client(project=project_id)\n    task_flag=datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    db.collection(solution_name).document(task_flag).set(hpt_best_dict,merge=True)\n\n    return \"true\"\n```", "```py\nwith dsl.Condition(\n        hpt_op.output==\"true\",\n        name=\"train_model\"\n    ):\n\n    train_task = train_op(\n      project_id=project_id,\n      data_region=data_region,\n      data_pipeline_root=data_pipeline_root,\n      input_data_schema=training_data_schema,\n      training_container_image_uri=training_container_image_uri,\n      train_additional_args=train_additional_args,\n      serving_container_image_uri=serving_container_image_uri,\n      custom_job_service_account=custom_job_service_account,\n      input_dataset=preprocess_task.outputs['output_dataset'],\n      machine_type=machine_type,\n      accelerator_count=accelerator_count,\n      accelerator_type=accelerator_type,\n      hptune_region=hptune_region,\n      hp_config_max_trials=hp_config_max_trials,\n      hp_config_suggestions_per_request=hp_config_suggestions_per_request,\n      vpc_network=vpc_network)\n```", "```py\ndef get_best_param_values(project_id, solution_name='hpt-pipeline-template'):\n    db = firestore.Client(project=project_id)\n\n    docs = db.collection(solution_name).list_documents()\n    doc_latest = max([doc.id for doc in docs])\n\n    params_latest = db.collection(solution_name).document(doc_latest).get().to_dict()\n\n    logging.info(f'Latest doc id {doc_latest}: {params_latest}')\n\n    return params_latest\n```", "```py\nbest_param_values = get_best_param_values(project_id=args.hp_config_gcp_project_id)\n```"]