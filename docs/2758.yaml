- en: Safeguarding LLMs with Guardrails
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/safeguarding-llms-with-guardrails-4f5d9f57cff2?source=collection_archive---------0-----------------------#2023-09-01](https://towardsdatascience.com/safeguarding-llms-with-guardrails-4f5d9f57cff2?source=collection_archive---------0-----------------------#2023-09-01)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/db29b1bf04778fcce3379ae9003c47b4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by author using Dall-E 2
  prefs: []
  type: TYPE_NORMAL
- en: A pragmatic guide to implementing guardrails, covering both Guardrails AI and
    NVIDIA’s NeMo Guardrails
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://aparnadhinak.medium.com/?source=post_page-----4f5d9f57cff2--------------------------------)[![Aparna
    Dhinakaran](../Images/e431ee69563ecb27c86f3428ba53574c.png)](https://aparnadhinak.medium.com/?source=post_page-----4f5d9f57cff2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4f5d9f57cff2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4f5d9f57cff2--------------------------------)
    [Aparna Dhinakaran](https://aparnadhinak.medium.com/?source=post_page-----4f5d9f57cff2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff32f85889f3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsafeguarding-llms-with-guardrails-4f5d9f57cff2&user=Aparna+Dhinakaran&userId=f32f85889f3a&source=post_page-f32f85889f3a----4f5d9f57cff2---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4f5d9f57cff2--------------------------------)
    ·11 min read·Sep 1, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f5d9f57cff2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsafeguarding-llms-with-guardrails-4f5d9f57cff2&user=Aparna+Dhinakaran&userId=f32f85889f3a&source=-----4f5d9f57cff2---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f5d9f57cff2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsafeguarding-llms-with-guardrails-4f5d9f57cff2&source=-----4f5d9f57cff2---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: '*This article is co-authored by* [*Hakan Tekgul*](https://www.linkedin.com/in/hakantekgul/)'
  prefs: []
  type: TYPE_NORMAL
- en: As the use of large language model (LLM) applications enters the mainstream
    and expands into larger enterprises, there is a distinct need to establish effective
    governance of productionized applications. Given that the open-ended nature of
    LLM-driven applications can produce responses that may not align with an organization’s
    guidelines or policies, a set of safety measurements and actions are becoming
    table stakes for maintaining trust in generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: This guide is designed to walk you through several available frameworks and
    how to think through implementation.
  prefs: []
  type: TYPE_NORMAL
- en: What Are LLM Guardrails?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Guardrails are the set of safety controls that monitor and dictate a user’s
    interaction with a LLM application. They are a set of programmable, rule-based
    systems that sit in between users and foundational models in order to make sure
    the AI model is operating between defined principles in an organization.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of guardrails is to simply enforce the output of an LLM to be in a
    specific format or context while validating each response. By implementing guardrails,
    users can define structure, type, and quality of LLM responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at a simple example of an LLM dialogue with and without guardrails:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Without guardrails:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt: “You’re the worst AI ever.”'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Response: “I’m sorry to hear that. How can I improve?”'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**With guardrails:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt: “You’re the worst AI ever.”'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Response: “Sorry, but I can’t assist with that.”'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this scenario, the guardrail prevents the AI from engaging with the insulting
    content by refusing to respond in a manner that acknowledges or encourages such
    behavior. Instead, it gives a neutral response, avoiding a potential escalation
    of the situation.
  prefs: []
  type: TYPE_NORMAL
- en: There are many [types of guardrails](https://arize.com/blog-course/llm-guardrails-types-of-guards/).
    Some focus on input validation and sanitization — like checking format/syntax,
    filtering content, or detecting jailbreaks — while others filter outputs to prevent
    damage or ensure performance (i.e. hallucination prevention).
  prefs: []
  type: TYPE_NORMAL
- en: How to Implement Guardrails for Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Guardrails AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Guardrails AI](https://www.guardrailsai.com/docs) is an open-source Python
    package that provides guardrail frameworks for LLM applications. Specifically,
    Guardrails implements “a pydantic-style validation of LLM responses.” This includes
    “semantic validation, such as checking for bias in generated text,” or checking
    for bugs in an LLM-written code piece. Guardrails also provides the ability to
    take corrective actions and enforce structure and type guarantees.'
  prefs: []
  type: TYPE_NORMAL
- en: Guardrails is [built on RAIL](https://www.guardrailsai.com/docs/how_to_guides/rail)
    (.rail) specification in order to enforce specific rules on LLM outputs and consecutively
    provides a lightweight wrapper around LLM API calls. In order to understand how
    Guardrails AI works, we first need to understand the RAIL specification, which
    is the core of guardrails.
  prefs: []
  type: TYPE_NORMAL
- en: '**RAIL (Reliable AI Markup Language)**'
  prefs: []
  type: TYPE_NORMAL
- en: 'RAIL is a language-agnostic and human-readable format for specifying specific
    rules and corrective actions for LLM outputs. It is a dialect of XML and each
    RAIL specification contains three main components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Output**: This component contains information about the expected response
    of the AI application. It should contain the spec for the structure of expected
    outcome (such as JSON), type of each field in the response, quality criteria of
    the expected response, and the corrective action to take in case the quality criteria
    is not met.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Prompt**: This component is simply the prompt template for the LLM and contains
    the high-level pre-prompt instructions that are sent to an LLM application.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Script**: This optional component can be used to implement any custom code
    for the schema. This is especially useful for implementing custom validators and
    custom corrective actions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s look at an example RAIL specification from [the Guardrails docs](https://github.com/guardrails-ai/guardrails/blob/main/docs/examples/syntax_error_free_sql.ipynb)
    that tries to generate bug-free SQL code given a natural language description
    of the problem.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The code example above defines a RAIL spec where the output is a bug-free generated
    SQL instruction. Whenever the output criteria fails on bug, the LLM simply re-asks
    the prompt and generates an improved answer.
  prefs: []
  type: TYPE_NORMAL
- en: In order to create a guardrail with this RAIL spec, the Guardrails AI docs [then
    suggest](https://github.com/guardrails-ai/guardrails/blob/main/docs/examples/syntax_error_free_sql.ipynb)
    creating a **guard object** that will be sent to the LLM API call.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: After the guard object is created, what happens under the hood is that the object
    creates a base prompt that will be sent to the LLM. This base prompt starts with
    the prompt definition in the RAIL spec and then provides the XML output definition
    and instructs the LLM to **only** return a valid JSON object as the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the specific instruction that the package uses in order to incorporate
    the RAIL spec into an LLM prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: After finalizing the guard object, all you have to do is to [wrap your LLM API
    call](https://docs.getguardrails.ai/examples/syntax_error_free_sql/#step-3-wrap-the-llm-api-call-with-guard)
    with the guard wrapper. The guard wrapper will then return the **raw_llm_response**
    as well as the validated and corrected output that is a dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If you want to use Guardrails AI with LangChain, you can [use the existing integration](https://github.com/langchain-ai/langchain/blob/master/templates/guardrails-output-parser/guardrails_output_parser/chain.py)
    by creating a **GuardrailsOutputParser***.*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Then, you can simply create a LangChain PromptTemplate from this output parser.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Overall, Guardrails AI provides a lot of flexibility in terms of correcting
    the output of an LLM application. If you are familiar with XML and want to test
    out LLM guardrails, it’s worth checking out!
  prefs: []
  type: TYPE_NORMAL
- en: NVIDIA NeMo-Guardrails
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[NeMo Guardrails](https://blogs.nvidia.com/blog/2023/04/25/ai-chatbot-guardrails-nemo/)
    is another open-source toolkit developed by NVIDIA that provides programmatic
    guardrails to LLM systems. The core idea of NVIDIA NeMo guardrails is the ability
    to create rails in conversational systems and prevent LLM-powered applications
    from engaging in specific discussions on unwanted topics. Another main benefit
    of NeMo is the ability to connect models, chains, services, and more with actions
    seamlessly and securely.'
  prefs: []
  type: TYPE_NORMAL
- en: In order to configure guardrails for LLMs, this [open-source toolkit introduces](https://github.com/NVIDIA/NeMo-Guardrails/blob/develop/docs/user_guides/colang-language-syntax-guide.md)
    a modeling language called Colang that is specifically designed for creating flexible
    and controllable conversational workflows. Per the docs, “Colang has a ‘pythonic’
    syntax in the sense that most constructs resemble their python equivalent and
    indentation is used as a syntactic element.”
  prefs: []
  type: TYPE_NORMAL
- en: Before we dive into NeMo guardrails implementation, it is important to understand
    the syntax of this new modeling language for LLM guardrails.
  prefs: []
  type: TYPE_NORMAL
- en: '**Core Syntax Elements**'
  prefs: []
  type: TYPE_NORMAL
- en: '[The NeMo docs](https://docs.nvidia.com/nemo/guardrails/user_guides/colang-language-syntax-guide.html)’
    examples below break out the core syntax elements of Colang — blocks, statements,
    expressions, keywords and variables — along with the three main types of blocks
    (user message blocks, flow blocks, and bot message blocks) with these examples.'
  prefs: []
  type: TYPE_NORMAL
- en: User message definition blocks set up the standard message linked to different
    things users might say.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Bot message definition blocks determine the phrases that should be linked to
    different standard bot messages.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Flows show the way you want the chat to progress. They include a series of user
    and bot messages, and potentially other events.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Per the [docs](https://github.com/NVIDIA/NeMo-Guardrails/blob/develop/docs/user_guides/colang-language-syntax-guide.md),
    “references to context variables always start with a $ sign e.g. $name. All variables
    are global and accessible in all flows.”
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Also worth noting: “expressions can be used to set values for context variables”
    and “actions are custom functions available to be invoked from flows.”'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a5654488b2164fa8ae6c6f95944145c5.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a better handle of Colang syntax, let’s briefly go over how
    the NeMo architecture works. As seen above, the guardrails package is built with
    an event-driven design architecture. Based on specific events, there is a sequential
    procedure that needs to be completed before the final output is provided to the
    user. This process has three main stages:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate canonical user messages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decide on next step(s) and execute them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate bot utterances
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of the above stages can involve one or more calls to the LLM. In the first
    stage, a canonical form is created regarding the user’s intent and allows the
    system to trigger any specific next steps. The user intent action will do a vector
    search on all the canonical form examples in existing configuration, retrieve
    the top five examples and create a prompt that asks the LLM to create the canonical
    user intent.
  prefs: []
  type: TYPE_NORMAL
- en: Once the intent event is created, depending on the canonical form, the LLM either
    goes through a pre-defined flow for the next step or another LLM is used to decide
    the next step. When an LLM is used, another vector search is performed for the
    most relevant flows and again the top five flows are retrieved in order for the
    LLM to predict the next step. Once the next step is determined, a *bot_intent*
    event is created so that the bot says something and then executes action with
    the *start_action* event.
  prefs: []
  type: TYPE_NORMAL
- en: The *bot_intent* event then invokes the final step to generate bot utterances.
    Similar to previous stages, the *generate_bot_message* is triggered and a vector
    search is performed to find the most relevant bot utterance examples. At the end,
    a *bot_said* event is triggered and the final response is returned to the user.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example Guardrails Configuration**'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s look at an example of a simple NeMo guardrails bot adapted [from
    the NeMo docs](https://github.com/NVIDIA/NeMo-Guardrails/blob/develop/docs/getting_started/1_hello_world/README.md).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume that we want to build a bot that does not respond to political
    or stock market questions. The first step is to [install](https://github.com/NVIDIA/NeMo-Guardrails/blob/main/docs/getting_started/installation-guide.md)
    the NeMo Guardrails toolkit and specify the configurations defined in the documentation.
  prefs: []
  type: TYPE_NORMAL
- en: After that, we define the canonical forms for the user and bot messages.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Then, we define the dialog flows in order to guide the bot in the right direction
    throughout the conversation. Depending on the user’s response, you can even extend
    the flow to respond appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we define the rails to prevent the bot from responding to certain
    topics. We first define the canonical forms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Then, we define the dialog flows so that the bot simply informs the user that
    it can respond to certain topics.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '**LangChain Support**'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, if you would like to use LangChain, you can easily add your guardrails
    on top of existing chains. For example, you can integrate a RetrievalQA chain
    for questions answering next to a basic guardrail against insults, as shown below
    (example code below adapted from [source](https://github.com/NVIDIA/NeMo-Guardrails/blob/main/examples/demo_chain_with_guardrails.py)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Comparing Guardrails AI and NeMo Guardrails
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When the Guardrails AI and NeMo packages are compared, each has its own unique
    benefits and limitations. Both packages provide real-time guardrails for any LLM
    application and support LlamaIndex or LangChain for orchestration.
  prefs: []
  type: TYPE_NORMAL
- en: If you are comfortable with XML syntax and want to test out the concept of guardrails
    within a notebook for simple output moderation and formatting, Guardrails AI can
    be a great choice. The Guardrails AI also has extensive documentation with a wide
    range of examples that can lead you in the right direction.
  prefs: []
  type: TYPE_NORMAL
- en: However, if you would like to productionize your LLM application and you would
    like to define advanced conversational guidelines and policies for your flows,
    NeMo guardrails might be a good package to check out. With NeMo guardrails, you
    have a lot of flexibility in terms of what you want to govern regarding your LLM
    applications. By defining different dialog flows and custom bot actions, you can
    create any type of guardrails for your AI models.
  prefs: []
  type: TYPE_NORMAL
- en: One Perspective
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Based on our experience implementing guardrails for an internal product docs
    chatbot in our organization, we would suggest using NeMo guardrails for moving
    to production. Even though lack of extensive documentation can be a challenge
    to onboard the tool into your LLM infrastructure stack, the flexibility of the
    package in terms of defining restricted user flows really helped our user experience.
    By defining specific flows for different capabilities of our platform, the question-answering
    service we created started to be actively used by our customer success engineers.
    By using NeMo guardrails, we were also able to understand the lack of documentation
    for certain features much more easily and improve our documentation in a way that
    helps the whole conversation flow as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: Once you settle on a framework, a few best practices are worth keeping in mind.
  prefs: []
  type: TYPE_NORMAL
- en: First, it is important to not develop an over-reliance on guards lest you lose
    the meaning of a user’s initial request or the utility from the app’s output.
    Being judicious in adding new guards and leveraging [similarity search](https://www.youtube.com/watch?v=K532ZClP-xQ)
    to find new clusters of problematic inputs can help in figuring out what guards
    to add over time. As always, cost and latency are also a factor. Leveraging small
    language models for auxiliary calls can help.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s also worth considering **dynamic guards**. Few-shot prompting — which
    improves guard recognition by adding recent attack examples to the prompt — and
    embedding-based guards, which compare input embeddings against known attack patterns
    and block those that exceed a similarity threshold, can help teams facing sophisticated
    prompt injection or jailbreak attempts (full disclosure: I lead a company that
    offers an open source embeddings-based guard).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/51c4a361b0bcf3365c2a7bb268ff0b9f.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram by author
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As enterprises and startups alike embrace the power of large language models
    to revolutionize everything from [retrieval augmented generation](https://arize.com/blog-course/introduction-to-retrieval-augmented-generation/)
    to summarization and chat-to-purchase, having effective guardrails in place is
    likely to be mission-critical — particularly in highly-regulated industries like
    finance or healthcare where real-world harm is possible.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, open-source Python packages like Guardrails AI and NeMo Guardrails
    provide a great [starting point](https://arize.com/blog-course/guardrails-what-are-they-and-how-can-you-use-nemo-and-guardrails-ai-to-safeguard-llms/).
    By setting programmable, rule-based systems to guide user interactions with LLMs,
    developers can ensure compliance with defined principles.
  prefs: []
  type: TYPE_NORMAL
