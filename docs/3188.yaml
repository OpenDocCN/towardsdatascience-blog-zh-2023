- en: 'Falcon: The Pinnacle of Open-Source LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/falcon-the-pinnacle-of-open-source-llms-600de69c333c?source=collection_archive---------1-----------------------#2023-10-24](https://towardsdatascience.com/falcon-the-pinnacle-of-open-source-llms-600de69c333c?source=collection_archive---------1-----------------------#2023-10-24)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The gap between open-source and proprietary LLMs continues to shrink…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wolfecameron.medium.com/?source=post_page-----600de69c333c--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----600de69c333c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----600de69c333c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----600de69c333c--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----600de69c333c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffalcon-the-pinnacle-of-open-source-llms-600de69c333c&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----600de69c333c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----600de69c333c--------------------------------)
    ·14 min read·Oct 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F600de69c333c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffalcon-the-pinnacle-of-open-source-llms-600de69c333c&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----600de69c333c---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F600de69c333c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffalcon-the-pinnacle-of-open-source-llms-600de69c333c&source=-----600de69c333c---------------------bookmark_footer-----------)![](../Images/4a5506c6e87e01bb8145fcb5b5d29b13.png)'
  prefs: []
  type: TYPE_NORMAL
- en: (Photo by [Alan Mersom](https://unsplash.com/@merse?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/aerial-photography-of-bird-NzkjR1pw0Lc?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
  prefs: []
  type: TYPE_NORMAL
- en: 'Recent research in open-source large language models (LLMs) has mostly focused
    upon two areas: imitation learning and pre-training open-source base models. Though
    both approaches are viable, the creation of high-quality, open-source base models
    is especially enticing, as these models can be further fine-tuned (at a lower
    cost) and used in a variety of different downstream applications. Initial attempts
    at creating these models failed. Although later models (e.g., LLaMA and MPT-7B)
    perform much better, these models have struggled to match the quality of their
    proprietary counterparts (e.g., GPT-3.5 or GPT-4) until recently.'
  prefs: []
  type: TYPE_NORMAL
- en: With the release of the Falcon-7B and Falcon-40B LLMs [1], we see — *for the
    first time* — open-source base LLMs that begin to rival the quality of the most
    popular paid models. Trained over a massive textual corpus obtained via a novel
    data pipeline, these models achieve (by a decent margin) new state-of-the-art
    performance among open-source LLMs and are free to use in commercial applications.
    To make things better, the Falcon models adopt several modifications to their
    underlying transformer architecture that significantly accelerate inference and
    can even improve the efficiency of pre-training.
  prefs: []
  type: TYPE_NORMAL
