- en: How to Build an LLM from Scratch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-build-an-llm-from-scratch-8c477768f1f9?source=collection_archive---------1-----------------------#2023-09-21](https://towardsdatascience.com/how-to-build-an-llm-from-scratch-8c477768f1f9?source=collection_archive---------1-----------------------#2023-09-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Data Curation, Transformers, Training at Scale, and Model Evaluation**'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://shawhin.medium.com/?source=post_page-----8c477768f1f9--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page-----8c477768f1f9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8c477768f1f9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8c477768f1f9--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page-----8c477768f1f9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff3998e1cd186&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-an-llm-from-scratch-8c477768f1f9&user=Shaw+Talebi&userId=f3998e1cd186&source=post_page-f3998e1cd186----8c477768f1f9---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8c477768f1f9--------------------------------)
    ·16 min read·Sep 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8c477768f1f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-an-llm-from-scratch-8c477768f1f9&user=Shaw+Talebi&userId=f3998e1cd186&source=-----8c477768f1f9---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8c477768f1f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-an-llm-from-scratch-8c477768f1f9&source=-----8c477768f1f9---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: This is the 6th article in a [series on using large language models](https://medium.com/towards-data-science/a-practical-introduction-to-llms-65194dda1148)
    (LLMs) in practice. Previous articles explored how to leverage pre-trained LLMs
    via [prompt engineering](https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f)
    and [fine-tuning](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91).
    While these approaches can handle the overwhelming majority of LLM use cases,
    it may make sense to build an LLM from scratch in some situations. In this article,
    we will review key aspects of developing a foundation LLM based on the development
    of models such as GPT-3, Llama, Falcon, and beyond.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9e8157ee792c3765304fb0a7c4d15f82.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e8157ee792c3765304fb0a7c4d15f82.png)'
- en: Photo by [Frames For Your Heart](https://unsplash.com/@framesforyourheart?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Frames For Your Heart](https://unsplash.com/@framesforyourheart?utm_source=medium&utm_medium=referral)在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)上拍摄。
- en: Historically (i.e. less than 1 year ago), training large-scale language models
    (10b+ parameters) was an esoteric activity reserved for AI researchers. However,
    with all the AI and LLM excitement post-ChatGPT, we now have an environment where
    businesses and other organizations have an interest in developing their own custom
    LLMs from scratch [1]. Although this is not necessary (IMO) for >99% of LLM applications,
    it is still beneficial to understand what it takes to develop these large-scale
    models and when it makes sense to build them.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 历史上（即不到1年前），训练大规模语言模型（10b+参数）是AI研究人员专属的神秘活动。然而，在ChatGPT后的所有AI和LLM热情中，现在我们有了一个环境，企业和其他组织有兴趣从头开始开发他们自己的定制LLM
    [1]。尽管对于超过99%的LLM应用来说这并不是必需的（依我看），但了解开发这些大规模模型的要求以及在何时构建它们仍然是有益的。
