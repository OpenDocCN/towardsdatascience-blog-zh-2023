- en: Implement interpretable neural models in PyTorch!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/implement-interpretable-neural-models-in-pytorch-6a5932bdb078?source=collection_archive---------4-----------------------#2023-05-29](https://towardsdatascience.com/implement-interpretable-neural-models-in-pytorch-6a5932bdb078?source=collection_archive---------4-----------------------#2023-05-29)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@pb737?source=post_page-----6a5932bdb078--------------------------------)[![Pietro
    Barbiero](../Images/28a6cfc89e32de0401ec3d732c085410.png)](https://medium.com/@pb737?source=post_page-----6a5932bdb078--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6a5932bdb078--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6a5932bdb078--------------------------------)
    [Pietro Barbiero](https://medium.com/@pb737?source=post_page-----6a5932bdb078--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc3b867b869ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplement-interpretable-neural-models-in-pytorch-6a5932bdb078&user=Pietro+Barbiero&userId=c3b867b869ca&source=post_page-c3b867b869ca----6a5932bdb078---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6a5932bdb078--------------------------------)
    ·11 min read·May 29, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6a5932bdb078&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplement-interpretable-neural-models-in-pytorch-6a5932bdb078&user=Pietro+Barbiero&userId=c3b867b869ca&source=-----6a5932bdb078---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6a5932bdb078&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplement-interpretable-neural-models-in-pytorch-6a5932bdb078&source=-----6a5932bdb078---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: '**TL;DR —** Experience the power of interpretability with “[PyTorch, Explain!](https://github.com/pietrobarbiero/pytorch_explain)”
    — a Python library that empowers you to implement state-of-the-art and interpretable
    concept-based models! [[GitHub](https://github.com/pietrobarbiero/pytorch_explain)]'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/424bbb13c1b438e116c9cc6b35e31bd1.png)'
  prefs: []
  type: TYPE_IMG
- en: Interpretable AI models make predictions for reasons humans can understand.
    Image by the authors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tutorials inspired by methods presented in:'
  prefs: []
  type: TYPE_NORMAL
- en: '***ICML*** 2020 paper “[Concept Bottleneck Models](https://arxiv.org/abs/2007.04612)”;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***NeurIPS*** 2022 paper “[Concept Embedding Models: Beyond the accuracy-explainability
    trade-off](https://arxiv.org/abs/2209.09056)”;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***ICML*** 2023 paper “[Interpretable Neural-Symbolic Concept Reasoning](https://arxiv.org/abs/2304.14068)”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Motivation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The lack of interpretability in deep learning systems poses a significant challenge
    to establishing human trust. The complexity of these models makes it nearly impossible
    for humans to understand the underlying reasons behind their decisions.
  prefs: []
  type: TYPE_NORMAL
- en: The lack of interpretability in deep learning systems hinders human trust.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To address this issue, researchers have been actively investigating novel solutions,
    leading to significant innovations such as concept-based models. These models
    not only enhance model transparency but also foster a renewed sense of trust in
    the system’s decision-making by incorporating high-level human-interpretable concepts
    (like “colour” or “shape”) in the training process. As a result, these models
    can provide simple and intuitive explanations for their predictions in terms of
    the learnt concepts, allowing humans to **check the reasoning behind their decisions**.
    And that’s not all! They even allow humans to interact with the learnt concepts,
    giving us **control over the final decisions**.
  prefs: []
  type: TYPE_NORMAL
- en: Concept-based models allow humans to **check the reasoning behind deep learning
    predictions** and give us back **control over the final decision.**
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this blog post, we will delve into these techniques and provide you with
    the tools to implement state-of-the-art concept-based models using simple PyTorch
    interfaces. Through hands-on experience, you will learn how to leverage these
    powerful models to enhance interpretability and ultimately calibrate human trust
    in your deep learning systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tutorial #1: Implement your first concept bottleneck model'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To showcase the power of PyTorch Explain, let’s dive into our first tutorial!
  prefs: []
  type: TYPE_NORMAL
- en: A primer on concept bottleneck models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this introductory session, we’ll dive into concept bottleneck models. These
    models, introduced in a paper [1] presented at the International Conference on
    Machine Learning in 2020, are designed to first learn and predict a set of concepts,
    such as “colour” or “shape,” and then utilize these concepts to solve a downstream
    classification task:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3266d868c1f5d009d12463ed3c44186d.png)'
  prefs: []
  type: TYPE_IMG
- en: Concept Bottleneck Models learn tasks (**Y**) as a function of concepts (**C**).
    Image by the authors.
  prefs: []
  type: TYPE_NORMAL
- en: By following this approach, we can trace predictions back to concepts providing
    explanations like “The input object is an {apple} because it is {spherical} and
    {red}.”
  prefs: []
  type: TYPE_NORMAL
- en: Concept bottleneck models first learn a set of concepts, such as “colour” or
    “shape,” and then utilize these concepts to solve a downstream classification
    task.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Hands-on concept bottlenecks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To illustrate concept bottleneck models, we will revisit the well-known XOR
    problem, but with a twist. Our input will consist of two continuous features.
    To capture the essence of these features, we will employ a concept encoder that
    maps them into two meaningful concepts, denoted as “A” and “B”. The objective
    of our task is to predict the exclusive OR (XOR) of “A” and “B”. By working through
    this example, you’ll gain a better understanding of how concept bottlenecks can
    be applied in practice and witness their effectiveness in tackling a concrete
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can start by importing the necessary libraries and loading this simple dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we instantiate a concept encoder to map the input features to the concept
    space and a task predictor to map concepts to task predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We then train the network by optimizing the cross-entropy loss on both concepts
    and tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'After training the model, we evaluate its performance on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now, after just a few epochs, we can observe that both the concept and the task
    accuracy are quite good on the test set (~98% accuracy)!
  prefs: []
  type: TYPE_NORMAL
- en: 'Thanks to this architecture we can provide explanations for a model prediction
    by looking at the response of the task predictor in terms of the input concepts,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: which yields e.g., `f([0,1])=1` and `f([1,1])=0` , as expected. This allows
    us to understand a bit more about the behaviour of the model and check that it
    behaves as expected for any relevant set of concepts e.g., for mutually exclusive
    input concepts `[0,1]`or `[1,0]` it returns a prediction of `y=1`.
  prefs: []
  type: TYPE_NORMAL
- en: Concept bottleneck models provide **intuitive** explanations by tracing predictions
    back to concepts.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Drowning in the accuracy-explainability trade-off
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the key advantages of concept bottleneck models is their ability to provide
    explanations for their predictions by revealing concept-prediction patterns allowing
    humans to assess whether the model’s reasoning aligns with their expectations.
  prefs: []
  type: TYPE_NORMAL
- en: However, the main issue with standard concept bottleneck models is that they
    struggle in solving complex problems! More generally, they **suffer from a well-known
    issue in explainable AI, referred to as the accuracy-explainability trade-off**.
    Practically, we desire models that not only achieve high task performance but
    also offer high-quality explanations. Unfortunately, in many cases, as we strive
    for higher accuracy, the explanations provided by the models tend to deteriorate
    in quality and faithfulness, and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: 'Visually, this trade-off can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ba3b17e1ca59b6c3cabc0f81395f1077.png)'
  prefs: []
  type: TYPE_IMG
- en: Visual representation of the accuracy-explainability trade-off. The picture
  prefs: []
  type: TYPE_NORMAL
- en: shows the difference between interpretable and “black-box” (non-interpretable)
    models
  prefs: []
  type: TYPE_NORMAL
- en: 'in terms of two axes: task performance and explanation quality. Image by the
    authors.'
  prefs: []
  type: TYPE_NORMAL
- en: where interpretable models excel at providing high-quality explanations but
    struggle with solving challenging tasks, while black-box models achieve high task
    accuracy at the expense of providing brittle and poor explanations.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate this trade-off in a concrete setting, let’s consider a concept
    bottleneck model applied to a slightly more demanding benchmark, the “trigonometry”
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Upon training the same network architecture on this dataset, we observe significantly
    diminished task accuracy, reaching only around 80%.
  prefs: []
  type: TYPE_NORMAL
- en: Concept bottleneck models fail to strike a balance between task accuracy and
    explanation quality.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This begs the question: are we perpetually forced to choose between accuracy
    and the quality of explanations, or is there a way to strike a better balance?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tutorial #2: Beyond the accuracy-explainability trade-off with concept embedding
    models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The answer is “yes!”, a solution does exist!
  prefs: []
  type: TYPE_NORMAL
- en: A primer on concept embedding models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A recent solution to address this challenge was introduced at the *Advances
    in Neural Information Processing Systems* conference in a paper called “Concept
    Embedding Models: Beyond the accuracy-explainability trade-off” [2] (I discuss
    this method more extensively in [this](/concept-embedding-models-beyond-the-accuracy-explainability-trade-off-f7ba02f28fad)
    blog post if you want to know more!). The key innovation of this paper was to
    design supervised high-dimensional concept representations. Unlike standard concept
    bottleneck models that represent each concept with a single neuron’s activation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/15b64307f135319265cae5970401a4c4.png)'
  prefs: []
  type: TYPE_IMG
- en: Concept Bottleneck Models learn tasks (**Y**) as a function of concepts (**C**).
    Image by the authors.
  prefs: []
  type: TYPE_NORMAL
- en: '… a concept embedding model represents each concept with a set of neurons,
    effectively overcoming the information bottleneck associated with the concept
    layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eec3e417a7fcbc25c6982010803deb6a.png)'
  prefs: []
  type: TYPE_IMG
- en: Concept Embedding Models represent each concept as a supervised vector. Image
    by the authors.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a result, concept embedding models enable us to achieve both high accuracy
    and high-quality explanations simultaneously:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8cc20b7417da10e603aa681a9e8d3cbe.png)'
  prefs: []
  type: TYPE_IMG
- en: Concept Embedding Models go beyond the accuracy-explainability trade-off in
    Concept Bottleneck Models with nearly optimal task accuracy and concept alignment.
    The optimal trade-off is represented by the red star (top-right). The task is
    to learn the sign (+/-) of the dot product between two vectors. Image by the authors.
  prefs: []
  type: TYPE_NORMAL
- en: Concept embedding models succeed in striking a balance between task accuracy
    and explanation quality.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Hands-on concept embedding models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Implementing these models in pytorch is as easy as it was with standard concept
    bottleneck models!
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by loading our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we instantiate a concept encoder to map the input features to the concept
    space and a task predictor to map concepts to task predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We then train the network by optimizing the cross-entropy loss on both concepts
    and tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'After training the model, we evaluate its performance on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now, after just a few epochs, we can observe that both the concept and the task
    accuracy are quite good on the test set (~96% accuracy), almost ~15% higher than
    with a standard concept bottleneck model!
  prefs: []
  type: TYPE_NORMAL
- en: Why interpretability > explainability?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Despite the simplicity and intuitiveness of the explanations provided by the
    techniques discussed thus far, there is still an inherent limitation: the precise
    logical reasoning behind the model’s predictions remains unclear.'
  prefs: []
  type: TYPE_NORMAL
- en: In fact, even if we were to employ a transparent machine learning model like
    a decision tree or logistic regression, it wouldn’t necessarily alleviate the
    issue when using concept embeddings. This is because the individual dimensions
    of concept vectors lack a clear semantic interpretation for humans. For instance,
    a logic sentence in a decision tree stating*“if {yellow[2]>0.3} and {yellow[3]<-1.9}
    and {round[1]>4.2} then {banana}”* does not hold much semantic meaning as terms
    like “*{yellow[2]>0.3}”* (referring to the second dimension of the concept vector
    “yellow” being greater than “0.3”) do not carry significant relevance to us.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c634d0f0dd1554ef247f98676266e0e7.png)'
  prefs: []
  type: TYPE_IMG
- en: Standard interpretable classifiers fail to provide interpretable predictions
    using concept embeddings as individual embedding dimensions lack a clear semantic
    meaning. Image by the authors.
  prefs: []
  type: TYPE_NORMAL
- en: Even transparent models cannot provide interpretable predictions when applied
    on concept embeddings.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How can we overcome this challenge this time?!
  prefs: []
  type: TYPE_NORMAL
- en: 'Step #3: Interpretability without compromises'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Again, a solution does exist!
  prefs: []
  type: TYPE_NORMAL
- en: A primer on deep concept reasoning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Deep Concept Reasoners [3] (a recent paper accepted at the 2023 *International
    Conference on Machine Learning*) address the limitations of concept embedding
    models by achieving full interpretability using concept embeddings. The key innovation
    of this method was to design a task predictor which processes concept embeddings
    and concept truth degrees separately. While a standard machine learning model
    would process concept embeddings and concept truth degrees simultaneously:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c634d0f0dd1554ef247f98676266e0e7.png)'
  prefs: []
  type: TYPE_IMG
- en: Standard interpretable classifiers fail to provide interpretable predictions
    using concept embeddings as individual embedding dimensions lack a clear semantic
    meaning. Image by the authors.
  prefs: []
  type: TYPE_NORMAL
- en: 'a deep concept reasoner generates (***interpretable***!) logic rules using
    concept embeddings and then executes rules symbolically assigning to concept symbols
    their corresponding truth value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/32b9923c40fcb6be424da5d743252162.png)'
  prefs: []
  type: TYPE_IMG
- en: A Deep Concept Reasoner generates fuzzy logic rules using neural models on concept
    embeddings, and then
  prefs: []
  type: TYPE_NORMAL
- en: executes the rule using the concept truth degrees to evaluate the rule symbolically.
    Image by the authors.
  prefs: []
  type: TYPE_NORMAL
- en: Deep concept reasoners provide interpretable predictions when applied on concept
    embeddings as each prediction is generated using a logic rule of concept truth
    degrees.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This unique technique allows us to implement models that are **perfectly interpretable,
    as they make predictions based on logic rules as a decision tree!** What sets
    them apart is their remarkable performance on challenging tasks, surpassing that
    of traditional interpretable models like decision trees or logistic regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3ea093fae88bfa1d3f630952ae078f92.png)'
  prefs: []
  type: TYPE_IMG
- en: Deep concept reasoners outperform interpretable concept-based models and match
    black-box models’ accuracy. CE stands for concept embeddings and CT for concept
    truth values. Image by the authors.
  prefs: []
  type: TYPE_NORMAL
- en: By leveraging deep concept reasoning, we can unlock the potential for highly
    interpretable models that offer superior performance on complex tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Deep concept reasoners provide interpretable predictions while outperforming
    interpretable models in terms of task accuracy.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Hands-on deep concept reasoning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Implementing deep concept reasoning is again quite easy using the `pytorch_explain`
    library!
  prefs: []
  type: TYPE_NORMAL
- en: 'As in previous examples, we instantiate a concept encoder to map the input
    features to the concept space and a deep concept reasoner to map concepts to task
    predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We then train the network by optimizing the cross-entropy loss on both concepts
    and tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'After training the model, we can evaluate its performance on the test set and
    check that it matches the accuracy of a concept embedding model (~99%):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'However, this time we can get the precise and exact reasoning behind each prediction
    by reading the corresponding logical rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'where each element in `local_explanations` has the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we can extract global explanations using the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'which returns the full set of rules found by the model, enabling humans to
    double check that the reasoning of the deep learning system matches the expected
    behaviour:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Key takeaways
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this post, we have explored the key features of the `pytorch_explain` library,
    highlighting state-of-the-art concept-based architectures and demonstrating their
    implementation with just a few lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a recap of what we covered:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Concept bottleneck models: These models provide intuitive explanations by tracing
    predictions back to a set of human-interpretable concepts;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Concept embedding models: By overcoming the information bottleneck associated
    with concepts, these models achieve high prediction accuracy without compromising
    the quality of explanations;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deep concept reasoners: The predictions of these models are fully interpretable
    as deep concept reasoners make predictions using logical rules composing concepts’
    truth values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By utilizing the powerful capabilities of the “*Pytorch, Explain!”* library
    and implementing the techniques discussed, you have the opportunity to significantly
    enhance the interpretability of your models while maintaining high prediction
    accuracy. This not only empowers you to gain deeper insights into the reasoning
    behind model predictions but also fosters and calibrates users’ trust in the system.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Koh, Pang Wei, et al. “Concept bottleneck models.” *International Conference
    on Machine Learning*. PMLR, 2020.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Zarlenga, Mateo Espinosa, et al. “Concept embedding models: Beyond the
    accuracy-explainability trade-off.” *Advances in Neural Information Processing
    Systems*. Vol. 35\. Curran Associates, Inc., 2022\. 21400–21413.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Barbiero, Pietro, et al. “Interpretable Neural-Symbolic Concept Reasoning.”
    *arXiv preprint arXiv:2304.14068* (2023).'
  prefs: []
  type: TYPE_NORMAL
