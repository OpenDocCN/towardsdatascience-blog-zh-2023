- en: What are Multimodal models?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/what-are-multimodal-models-fe118f3ef963?source=collection_archive---------2-----------------------#2023-10-16](https://towardsdatascience.com/what-are-multimodal-models-fe118f3ef963?source=collection_archive---------2-----------------------#2023-10-16)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Give LLMs the ability to see!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@omermx?source=post_page-----fe118f3ef963--------------------------------)[![Omer
    Mahmood](../Images/0c87da4134bea397c77bc4ba6640e34b.png)](https://medium.com/@omermx?source=post_page-----fe118f3ef963--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fe118f3ef963--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fe118f3ef963--------------------------------)
    [Omer Mahmood](https://medium.com/@omermx?source=post_page-----fe118f3ef963--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F62cd989987f6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-are-multimodal-models-fe118f3ef963&user=Omer+Mahmood&userId=62cd989987f6&source=post_page-62cd989987f6----fe118f3ef963---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fe118f3ef963--------------------------------)
    ¬∑6 min read¬∑Oct 16, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffe118f3ef963&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-are-multimodal-models-fe118f3ef963&user=Omer+Mahmood&userId=62cd989987f6&source=-----fe118f3ef963---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe118f3ef963&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-are-multimodal-models-fe118f3ef963&source=-----fe118f3ef963---------------------bookmark_footer-----------)![](../Images/4b7fb0fc89f35adbb6b9d33055d2f0d0.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Screenshot of [Mecari text & image embeddings demo](https://atlas.nomic.ai/map/vertex-mercari)
    running on Atlas by Nomic.
  prefs: []
  type: TYPE_NORMAL
- en: Who is this post for?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Reader Audience [üü¢‚ö™Ô∏è‚ö™Ô∏è]:** AI beginners, familiar with popular concepts,
    models and their applications'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Level [üü¢üü¢Ô∏è‚ö™Ô∏è]:** Intermediate topic'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complexity [üü¢‚ö™Ô∏è‚ö™Ô∏è]:** Easy to digest, no mathematical formulas or complex
    theory here'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ‚ùìWhy It Matters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Foundational large language models (LLMs), pre-trained on huge datasets are
    pretty efficient at handling generic, multi-tasking via prompts through zero-shot,
    few-shot or transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, examples of these models like [PaLM2](https://ai.google/discover/palm2/)
    and [GPT4](https://openai.com/research/gpt-4) have revolutionised the way we interact
    with computers **using text as an input**, but‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: What if there was a way to extend the intelligence of these models, by enabling
    them to use different modalities of input, such as photos, audio, and video? **Or
    in other words, make them Multimodal!**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It could greatly improve how we search for things on the web, or even understand
    the world around us for example in real world applications such as medicine and
    pathology.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a solution! Multimodal deep learning models can combine the embeddings
    from different types of input, enabling, for example, an LLM to ‚Äúsee‚Äù what‚Ä¶
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
