- en: Anomaly Detection using Sigma Rules (Part 2) Spark Stream-Stream Join
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Sigma规则的异常检测（第2部分） Spark流-流连接
- en: 原文：[https://towardsdatascience.com/anomaly-detection-using-sigma-rules-part-2-spark-stream-stream-join-6bb4734e912f?source=collection_archive---------14-----------------------#2023-02-02](https://towardsdatascience.com/anomaly-detection-using-sigma-rules-part-2-spark-stream-stream-join-6bb4734e912f?source=collection_archive---------14-----------------------#2023-02-02)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/anomaly-detection-using-sigma-rules-part-2-spark-stream-stream-join-6bb4734e912f?source=collection_archive---------14-----------------------#2023-02-02](https://towardsdatascience.com/anomaly-detection-using-sigma-rules-part-2-spark-stream-stream-join-6bb4734e912f?source=collection_archive---------14-----------------------#2023-02-02)
- en: A class of Sigma rules detect temporal correlations. We evaluate the scalability
    of Spark’s stateful symmetric stream-stream join to perform temporal correlations.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一类Sigma规则检测时间相关性。我们评估了Spark的有状态对称流-流连接在执行时间相关性时的可扩展性。
- en: '[](https://medium.com/@jean-claude.cote?source=post_page-----6bb4734e912f--------------------------------)[![Jean-Claude
    Cote](../Images/aea2df9c7b95fc85cc336f64d64b0a76.png)](https://medium.com/@jean-claude.cote?source=post_page-----6bb4734e912f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6bb4734e912f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6bb4734e912f--------------------------------)
    [Jean-Claude Cote](https://medium.com/@jean-claude.cote?source=post_page-----6bb4734e912f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jean-claude.cote?source=post_page-----6bb4734e912f--------------------------------)[![Jean-Claude
    Cote](../Images/aea2df9c7b95fc85cc336f64d64b0a76.png)](https://medium.com/@jean-claude.cote?source=post_page-----6bb4734e912f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6bb4734e912f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6bb4734e912f--------------------------------)
    [Jean-Claude Cote](https://medium.com/@jean-claude.cote?source=post_page-----6bb4734e912f--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F444ed0089012&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanomaly-detection-using-sigma-rules-part-2-spark-stream-stream-join-6bb4734e912f&user=Jean-Claude+Cote&userId=444ed0089012&source=post_page-444ed0089012----6bb4734e912f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6bb4734e912f--------------------------------)
    ·7 min read·Feb 2, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6bb4734e912f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanomaly-detection-using-sigma-rules-part-2-spark-stream-stream-join-6bb4734e912f&user=Jean-Claude+Cote&userId=444ed0089012&source=-----6bb4734e912f---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F444ed0089012&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanomaly-detection-using-sigma-rules-part-2-spark-stream-stream-join-6bb4734e912f&user=Jean-Claude+Cote&userId=444ed0089012&source=post_page-444ed0089012----6bb4734e912f---------------------post_header-----------)
    发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----6bb4734e912f--------------------------------)
    ·7分钟阅读·2023年2月2日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6bb4734e912f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanomaly-detection-using-sigma-rules-part-2-spark-stream-stream-join-6bb4734e912f&user=Jean-Claude+Cote&userId=444ed0089012&source=-----6bb4734e912f---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6bb4734e912f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanomaly-detection-using-sigma-rules-part-2-spark-stream-stream-join-6bb4734e912f&source=-----6bb4734e912f---------------------bookmark_footer-----------)![](../Images/0b89900becc6702163692d4aacdc9d3e.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6bb4734e912f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanomaly-detection-using-sigma-rules-part-2-spark-stream-stream-join-6bb4734e912f&source=-----6bb4734e912f---------------------bookmark_footer-----------)![](../Images/0b89900becc6702163692d4aacdc9d3e.png)'
- en: Photo by Naveen Kumar on Unsplash
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由Naveen Kumar拍摄，来自Unsplash
- en: Following up on our [previous article](https://medium.com/towards-data-science/anomaly-detection-using-sigma-rules-part-1-leveraging-spark-sql-streaming-246900e95457),
    we evaluate Sparks ability to join a start-process event with it’s parent start-process
    event.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 跟进我们[之前的文章](https://medium.com/towards-data-science/anomaly-detection-using-sigma-rules-part-1-leveraging-spark-sql-streaming-246900e95457)，我们评估了Spark将一个开始进程事件与其父开始进程事件进行连接的能力。
- en: In this article, we evaluated how Spark [stream-stream join](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#stream-stream-joins)
    can scale. Specifically, how many events can it hold in in the join window.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们评估了 Spark [流-流连接](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#stream-stream-joins)的扩展性。具体来说，它能在连接窗口中容纳多少事件。
- en: 'During our research, we evaluated a few approaches:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的研究中，我们评估了几种方法：
- en: Full join
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完全连接
- en: Doing a full stream-stream join requires caching all previous events on the
    right side of the join (parent start-process). All the past parent start-process
    details are not necessary since only a subset of these parent start-process events
    are of interest. For example, a Sigma rule might specify a parent CommandLine
    containing the string `.cpl`— all other events can be ignored.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的流-流连接需要缓存连接右侧的所有先前事件（父项启动过程）。由于只有这些父项启动过程事件的子集是感兴趣的，因此不需要所有过去的父项启动过程详细信息。例如，一个
    Sigma 规则可能会指定一个包含字符串 `.cpl` 的父项 CommandLine——所有其他事件可以被忽略。
- en: Join with Parent of Interest
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入感兴趣的父项
- en: The parents of interest is the result of applying filter conditions to the right
    side of the join. This can greatly reduce the number of parents to remember. After
    the join is performed, we apply the conditions on the current process and on the
    parent process.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 感兴趣的父项是将过滤条件应用于连接的右侧得到的结果。这可以大大减少需要记住的父项数量。连接完成后，我们对当前处理和父项处理应用条件。
- en: '![](../Images/9bbc30b515ae4de9767ee0fb2a40f177.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9bbc30b515ae4de9767ee0fb2a40f177.png)'
- en: Join with Features of Parent of Interest
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与感兴趣的父项特征连接
- en: 'A better solution is to store the conditions we evaluate on the right side
    and discard all other attributes — CommandLine, Image etc. This way we only keep
    a limited number of boolean flags rather than potentially long strings. In the
    diagram below, `Features` is a map of Sigma filter expression name and the value
    is the result of the test. For example:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的解决方案是存储我们在右侧评估的条件，并丢弃所有其他属性——CommandLine、Image 等。这样我们只保留有限数量的布尔标志，而不是可能很长的字符串。在下图中，`Features`
    是 Sigma 过滤器表达式名称的映射，值是测试结果。例如：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/e05fc711d048fe95e60ca80be7188baa.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e05fc711d048fe95e60ca80be7188baa.png)'
- en: 'During our research, we quickly came to the realization that reducing the amount
    of state Spark needs to store is paramount. Thus we chose to keep only parents
    of interest. These are the parents which have a feature we are looking for. We
    discard all other parents and keep only the minimal set of information about these
    parents: join key, timestamp and feature flags.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的研究中，我们很快意识到减少 Spark 需要存储的状态量至关重要。因此，我们选择只保留感兴趣的父项。这些是我们正在寻找的特征的父项。我们丢弃所有其他父项，只保留这些父项的最小信息集：连接键、时间戳和特征标志。
- en: Simulation Test Harness
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模拟测试框架
- en: In order to evaluate the performance of the Spark stream-stream join, we created
    a mock stream of `cause` and `effect` events. In our experiments, we disabled
    the late arrival support by setting a watermark of zero.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估 Spark 流-流连接的性能，我们创建了一个 `cause` 和 `effect` 事件的模拟流。在我们的实验中，我们通过设置水印为零来禁用迟到支持。
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'These `cause` and `effect` streaming events are joined using spark stream-stream:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这些`cause`和`effect`流事件是通过 Spark 流-流连接操作进行合并的：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Notice the join expression `effect_key = cause_key` and the windowing clauses
    stating that an effect time must be after the `cause` but not further back in
    time than `window` seconds.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 注意连接表达式 `effect_key = cause_key` 和窗口子句，表示效果时间必须在 `cause` 之后，但不能比 `window` 秒更久。
- en: Linxiao Ma nicely explains in his article, [Spark Structured Streaming Deep
    Dive (7) — Stream-Stream Join](https://dataninjago.com/2022/07/21/spark-structured-streaming-deep-dive-7-stream-stream-join/),
    that under these conditions, cause events will get cached in Spark’s stateful
    state store up to the `window` seconds. However, the `effect` events are not required
    to be stored. For every `effect` event passing in the stream-stream join, a lookup
    is made to find a corresponding `cause`. For every `effect` entering the join,
    a `cause+effect` row is written out.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Linxiao Ma 在他的文章 [Spark Structured Streaming Deep Dive (7) — Stream-Stream Join](https://dataninjago.com/2022/07/21/spark-structured-streaming-deep-dive-7-stream-stream-join/)
    中详细解释了，在这些条件下，原因事件会被缓存到 Spark 的有状态存储中，最多 `window` 秒。然而，`effect` 事件不需要被存储。对于每个在流-流连接中经过的
    `effect` 事件，都会进行查找以找到相应的 `cause`。对于每个进入连接的 `effect`，都会写入一行 `cause+effect`。
- en: Choosing the Right State Store
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择合适的状态存储
- en: Spark has two state store implementations. The original is named HDFSBackedStateStore
    and is a simple in-memory hash map backed by HDFS files. The newest state store
    is based on RocksDB. [RocksDB is an embeddable key-value persistent store](https://www.confluent.io/blog/how-to-tune-rocksdb-kafka-streams-state-stores-performance/)
    written in C++. The state of RocksDB is kept partly in memory and partly on local
    disk. At every checkpoint, Spark saves a copy of the changed files to a central
    location (datalake).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Spark有两个状态存储实现。最初的是名为HDFSBackedStateStore的，它是一个由HDFS文件支持的简单内存哈希表。最新的状态存储基于RocksDB。[RocksDB
    是一个可嵌入的键值持久存储](https://www.confluent.io/blog/how-to-tune-rocksdb-kafka-streams-state-stores-performance/)用C++编写。RocksDB的状态部分保存在内存中，部分保存在本地磁盘上。在每个检查点，Spark将更改的文件副本保存到中央位置（数据湖）。
- en: Spark recommends [RocksDB](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#rocksdb-state-store-implementation)
    when you have a large number of keys to store. According to [DataBricks](https://docs.databricks.com/structured-streaming/stateful-streaming.html),
    a large Spark worker node can cache up to 100 million keys.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当你需要存储大量键时，Spark推荐使用[RocksDB](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#rocksdb-state-store-implementation)。根据[DataBricks](https://docs.databricks.com/structured-streaming/stateful-streaming.html)，一个大型Spark工作节点可以缓存多达1亿个键。
- en: Since our stream-stream join will cache a lot of parents of interest rows, we
    decided to use the RocksDB state store in our evaluation.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的流流连接会缓存大量的感兴趣的父项行，我们决定在评估中使用RocksDB状态存储。
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: All of our experiments are performed on a single Spark worker with 48G of RAM
    and 16 CPU. We simulate logs coming from 50,000 hosts.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所有的实验都在一个具有48G RAM和16 CPU的单个Spark工作节点上进行。我们模拟了来自50,000个主机的日志。
- en: Our test harness is very flexible. It lets us alter many parameters, such as
    the events per seconds, size of each event, the time window, for the join, key
    sizes, distribution of events in the time window etc.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的测试框架非常灵活。它允许我们更改许多参数，例如每秒事件数、每个事件的大小、连接的时间窗口、键大小、时间窗口中事件的分布等。
- en: Effect of Spark Partitions
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark分区的影响
- en: In our first experiment, we joined effects and causes over a window of 10,000
    seconds (~2.77 hours). We simulated that each parent of interest would have 12
    boolean flags. We set an event rate of 10,000 per second. Here, we show the effect
    of varying the number of Spark partitions (individual tasks).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的第一个实验中，我们在10,000秒（约2.77小时）的窗口中连接效果和原因。我们模拟了每个感兴趣的父项会有12个布尔标志。我们设置了每秒10,000个事件的速率。在这里，我们展示了Spark分区（单个任务）数量变化的影响。
- en: '![](../Images/851031bbf6f8f6033f559af9c816aad5.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/851031bbf6f8f6033f559af9c816aad5.png)'
- en: Varying the number of partitions has no effect on performance. The time to execute
    a micro-batch is about 225 seconds. Remember we triggered at every 60 seconds
    `.trigger(processingTime="1 minutes")` . Spark will start the next micro-batch
    immediately. The event processing latency is thus a maximum of 225 seconds.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 更改分区数量对性能没有影响。执行一个微批次的时间大约是225秒。记住我们每60秒触发一次`.trigger(processingTime="1 minutes")`。Spark将立即开始下一个微批次。因此，事件处理的延迟最多为225秒。
- en: Effect of Window Size
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 窗口大小的影响
- en: In this second experiment, we varied the size (time) of the stream-stream join
    window. The job is not stable at a rate of 5,000 events per seconds. Each micro-batch
    takes longer and longer to execute. We are falling behind.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个实验中，我们调整了流流连接窗口的大小（时间）。在每秒5,000个事件的速率下，作业不稳定。每个微批次的执行时间越来越长。我们正在落后。
- en: '![](../Images/e9c348879a1d1a4a2579a89409b3c987.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e9c348879a1d1a4a2579a89409b3c987.png)'
- en: If we reduce the window to 18 hours and reduce the rate to 2,500 events per
    second, the job stabilizes and settles at about 300 seconds per micro-batch.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将窗口减少到18小时，并将速率降低到每秒2,500个事件，作业会稳定下来，并在每个微批次约300秒时达到稳定。
- en: '![](../Images/118519c60d87af0830a72b92b10559af.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/118519c60d87af0830a72b92b10559af.png)'
- en: 'However, in practice, we will not keep every parent event. We will only keep
    the “parents of interests”. These are the events which have one or more Sigma
    rule expressions that is true. What is important to measure is Spark’s ability
    to hold parent events. We can easily calculate this: 2,500 event/seconds x 64,000
    seconds. Spark can cache 160 million “parents of interests”. Our experimental
    result confirms [Databricks claim that the RocksDB StateStore](https://docs.databricks.com/structured-streaming/stateful-streaming.html)
    can handle 100 million keys per machine. If we suppose these events come from
    50,000 hosts, Spark can hold 3,200 “parents of interests” per host.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，实际上，我们不会保留每一个父事件。我们只会保留“感兴趣的父事件”。这些事件中有一个或多个为真的 Sigma 规则表达式。重要的是要衡量 Spark
    存储父事件的能力。我们可以轻松计算：2,500 事件/秒 x 64,000 秒。Spark 可以缓存 1.6 亿个“感兴趣的父事件”。我们的实验结果确认了
    [Databricks 关于 RocksDB StateStore 的声明](https://docs.databricks.com/structured-streaming/stateful-streaming.html)，即每台机器可以处理
    1 亿个键。如果我们假设这些事件来自 50,000 个主机，那么 Spark 可以每台主机保存 3,200 个“感兴趣的父事件”。
- en: It’s interesting to observe that Spark stores the feature flags and the keys.
    It needs to store the key of the `causes` in order to join them to the `effect`
    key.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，Spark 存储了特征标志和键。它需要存储 `causes` 的键，以便将其与 `effect` 键连接。
- en: The other interesting observation we can make is, what do we retrieve from this
    lookup by key? We retrieve an event (a row) that contains boolean flags. In practice,
    these flags are often mutually exclusive. That is, in a given row, only one of
    them might be true, while all others are false. Spark is storing the `cause` key
    and all the associated flags no matter if they are true or false.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做的另一个有趣的观察是，通过键查找我们检索到什么？我们检索到一个包含布尔标志的事件（即一行）。实际上，这些标志通常是互斥的。也就是说，在某一行中，只有一个标志可能为真，而所有其他标志都为假。Spark
    存储 `cause` 键和所有相关的标志，无论它们是真还是假。
- en: Bloom Filter Join
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Bloom 过滤器连接
- en: Is there a better way to keep track of feature flags? Yes, the answer is a bloom
    filter.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 是否有更好的方法来跟踪特征标志？是的，答案是 Bloom 过滤器。
- en: A bloom filter is a probabilistic data structure which can store a key and test
    for the presence of a key. Bloom filters hash the key and use the result of the
    hash to set a few bits in a bit array.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Bloom 过滤器是一种概率数据结构，可以存储一个键并测试键的存在性。Bloom 过滤器对键进行哈希，并利用哈希结果在位数组中设置一些位。
- en: Bloom filters are extremely compact. The price you pay for the space saving
    is of possible false positives. However, once a detection is made, the Sigma rule
    that triggered can be re-evaluated to confirm correctness.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Bloom 过滤器非常紧凑。为了节省空间，你可能会付出假阳性的代价。然而，一旦检测完成，触发的 Sigma 规则可以重新评估以确认正确性。
- en: We can use a bloom filter to perform the join from above. Let’s suppose we use
    a composite key (`parent_key + feature_id`), where the `feature_id` is the name
    given to a Sigma filter expression. Filter expression that apply to a parent process
    are stored in the bloom, but only if they are true. Testing for the presence of
    the composite key returns true if the key is in the bloom and false if it is not.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 Bloom 过滤器来执行上述连接。假设我们使用一个复合键（`parent_key + feature_id`），其中 `feature_id`
    是给定 Sigma 过滤器表达式的名称。应用于父进程的过滤器表达式存储在 Bloom 过滤器中，但仅在它们为真的情况下。测试复合键的存在性，如果键在 Bloom
    中，则返回真；如果不在，则返回假。
- en: A bloom can hold a certain amount of keys. Past that number, false positives
    increase drastically. By only storing features that are true, we prolong the usefulness
    of the bloom filter.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Bloom 可以存储一定数量的键。超出这个数量后，假阳性会急剧增加。通过只存储真实的特征，我们可以延长 Bloom 过滤器的有效性。
- en: The join is thus modeled as a lookup in a bloom filter.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，连接被建模为在 Bloom 过滤器中的查找。
- en: '![](../Images/3ba551a067487290b99a1ed4f66d168f.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ba551a067487290b99a1ed4f66d168f.png)'
- en: In our next article, we will build a custom Spark stateful join function that
    leverages a bloom filter.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的下一篇文章中，我们将构建一个自定义的 Spark 有状态连接函数，利用 Bloom 过滤器。
- en: All images unless otherwise noted are by the author
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 除非另有说明，否则所有图像均由作者提供
