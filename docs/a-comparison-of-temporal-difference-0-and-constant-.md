# 时间差(0)与常数-α蒙特卡洛方法在随机游走任务中的比较

> 原文：[https://towardsdatascience.com/a-comparison-of-temporal-difference-0-and-constant-%CE%B1-monte-carlo-methods-on-the-random-walk-task-bc6497eb7c92?source=collection_archive---------4-----------------------#2023-08-24](https://towardsdatascience.com/a-comparison-of-temporal-difference-0-and-constant-%CE%B1-monte-carlo-methods-on-the-random-walk-task-bc6497eb7c92?source=collection_archive---------4-----------------------#2023-08-24)

[![Tingsong Ou](../Images/459edc4bbd2353895acfb0f57eeddaa3.png)](https://medium.com/@outerrencedl?source=post_page-----bc6497eb7c92--------------------------------) [![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bc6497eb7c92--------------------------------) [Tingsong Ou](https://medium.com/@outerrencedl?source=post_page-----bc6497eb7c92--------------------------------)

·

[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa7aefc686327&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comparison-of-temporal-difference-0-and-constant-%CE%B1-monte-carlo-methods-on-the-random-walk-task-bc6497eb7c92&user=Tingsong+Ou&userId=a7aefc686327&source=post_page-a7aefc686327----bc6497eb7c92---------------------post_header-----------) 发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bc6497eb7c92--------------------------------) ·9分钟阅读·2023年8月24日

--

![](../Images/ab9d686103362e65da2658ebd5e1455b.png)

由 Midjourney 生成的图像，使用了付费订阅，符合一般商业条款[1]。

# 引言

蒙特卡洛（MC）方法和时序差分（TD）方法都是强化学习领域的基本技术；它们通过与环境互动的经验解决预测问题，而不是依赖于环境模型。然而，TD 方法是 MC 方法和动态规划（DP）的结合，因此在更新规则、引导和偏差/方差方面与 MC 方法有所不同。与 MC 方法相比，TD 方法在大多数情况下被证明具有更好的性能和更快的收敛速度。

在这篇文章中，我们将比较 TD 和 MC，或更具体地说，比较 TD(0) 和常数-α MC 方法，在一个简单的网格环境和一个更全面的随机游走 [2] 环境中。希望这篇文章能够帮助对强化学习感兴趣的读者更好地理解每种方法如何更新状态值函数，以及它们在相同测试环境中的性能差异。

我们将在 Python 中实现算法和比较，本帖中使用的库如下：

```py
python==3.9.16
numpy==1.24.3
matplotlib==3.7.1
```

# TD 与 MC 的差异

## TD(0) 和常数-α MC 的介绍

常数-α MC 方法是一种常数步长参数 α 的常规 MC 方法，这个常数参数有助于使价值估计对最近的经验更加敏感。在实践中，α 值的选择取决于稳定性和适应性之间的权衡。以下是 MC 方法在时间 t 更新状态值函数的方程：

![](../Images/13431a73d76db109b060dd58241e7e67.png)

TD(0) 是 TD(λ) 的一个特例，它只看一步前的状态，是最简单的 TD 学习形式。该方法使用 TD 误差更新状态值函数，TD 误差是指状态的估计值与奖励加上下一个状态的估计值之间的差异。一个常数步长参数 α 与上述 MC 方法中的作用相同。以下是 TD(0) 在时间 t 更新状态值函数的方程：

![](../Images/07e6c9dc15bef8c2e744d21213d111e3.png)

一般来说，MC 和 TD 方法之间的差异体现在三个方面：

1.  **更新规则**：MC 方法仅在回合结束后更新值；如果回合非常长，这可能会导致程序变慢，或者在没有回合的持续任务中，这可能会成为问题。相反，TD 方法在每个时间步更新价值估计；这是一种在线学习，特别适用于持续任务。

1.  **引导**：在强化学习中，“引导”一词指的是基于其他价值估计来更新价值估计。TD(0) 方法基于下一个状态的价值来进行更新，因此它是一种引导方法；相反，MC 不使用引导，因为它直接从回报（G）中更新价值。

1.  **偏差/方差**：MC 方法没有偏差，因为它们通过加权实际观察到的回报来估计值，而在过程中不进行估计；然而，MC 方法有较高的方差，尤其是在样本数量较少时。相反，TD 方法有偏差，因为它们使用了自助法，偏差可能会根据实际实现而有所不同；TD 方法方差较低，因为它使用了即时奖励加上对下一状态的估计，这平滑了因奖励和行动的随机性引起的波动。

## 在简单的网格世界设置中评估 TD(0) 和常数-α MC

为了使它们的差异更加直观，我们可以设置一个简单的网格世界测试环境，具有两个固定轨迹，运行这两种算法直到收敛，并检查它们如何不同地更新值。

首先，我们可以用以下代码设置测试环境：

![](../Images/035c80a603c34f01682cf5eda5591d09.png)

图 1 左：环境设置。右：预设路径。来源：作者绘制的图

上图左侧显示了一个简单的网格世界环境设置。所有彩色单元格表示终端状态；代理在进入红色单元格时获得 +1 奖励，但在进入蓝色单元格时获得 -1 奖励。网格上的所有其他步骤返回零奖励。上图右侧标记了两个预设路径：一条到达蓝色单元格，另一条停在红色单元格；路径的交点有助于最大化两种方法之间的值差异。

然后我们可以使用上一节中的方程来评估环境。我们不对回报或估计进行折扣，并将 α 设置为一个小值 1e-3。当值增量的绝对和低于 1e-3 的阈值时，我们认为值已收敛。

评估结果如下：

![](../Images/e8aa51108e8a85e174624eb62ccfa1c4.png)

图 2 TD(0) 和常数-α MC 评估结果。来源：作者绘制的图

上述图像中，两种算法在估计值的方式上的不同变得非常明显。MC 方法忠实于路径的回报，因此每条路径上的值直接表示其结束状态。然而，TD 方法提供了更好的预测，特别是在蓝色路径上——在交点之前的蓝色路径上的值也表示到达红色单元格的可能性。

以这个最小的案例为基础，我们准备转向一个更复杂的示例，尝试找出两种方法之间的性能差异。

# 随机游走任务

随机游走任务是Sutton等人提出的一个简单的*马尔可夫奖励过程*，用于TD和MC预测目的[2]，如下图所示。在此任务中，代理从中心节点C开始。代理在每个节点上以相等的概率向右或向左迈一步。链条的两端有两个终止状态。进入左端的奖励为0，进入右端的奖励为+1。在终止之前的所有步骤生成的奖励为0。

![](../Images/b9ba3d8ff93a94f91a9f551d52a30843.png)

图3 随机游走。来源：作者提供的图

我们可以使用以下代码来创建随机游走环境：

```py
=====Test: checking environment setup=====

Links:        None ← Node A → Node B
Reward:          0 ← Node A → 0

Links:      Node A ← Node B → Node C
Reward:          0 ← Node B → 0

Links:      Node B ← Node C → Node D
Reward:          0 ← Node C → 0

Links:      Node C ← Node D → Node E
Reward:          0 ← Node D → 0

Links:      Node D ← Node E → None
Reward:          0 ← Node E → 1
```

在随机策略下，环境中每个节点的真实值为[1/6, 2/6, 3/6, 4/6, 5/6]。该值通过使用贝尔曼方程的策略评估计算得出：

![](../Images/482b1eb6918db8f72ff9f6a2eec4812c.png)

我们的任务是找出两个算法估计的值与真实值的接近程度；我们可以任意假设算法产生的值函数离真实值函数更近，通过平均均方根误差（RMS）来衡量，表示性能更好。

# TD(0)和常数-a MC在随机游走中的表现

## 算法

环境准备好后，我们可以开始在随机游走环境中运行这两种方法，并比较它们的表现。首先，让我们看一下这两个算法：

![](../Images/8ea1e405f6d2f5574118c28586af4257.png)

来源：[latex](https://gist.github.com/terrence-ou/10dc3571be8fb9ae9dad92a7505633b6)中由作者编写的算法

![](../Images/6b0e1dd91eb93046dfaa8b54d3ca21d5.png)

来源：[latex](https://gist.github.com/terrence-ou/5ab148962748c371ccd60b35c4feec51)中由作者编写的算法

如前所述，MC方法应该等到回合结束后才能更新从轨迹尾部得到的值，而TD方法则是逐步更新值。这种差异带来了初始化状态值函数时的一个技巧：在MC中，状态值函数不包括终止状态，而在TD(0)中，该函数应包括终止状态，并且值为0，因为TD(0)方法总是提前一步看未来的状态，直到回合结束。

## 实现

在此实现中的α参数选择参考了书中[2]提出的参数；MC方法的参数为[0.01, 0.02, 0.03, 0.04]，而TD方法的参数为[0.05, 0.10, 0.15]。我曾经疑惑为何作者没有在两个算法中选择相同的参数集，直到我用TD参数运行MD方法：TD参数对MC方法来说太高，因此不能展现MC的最佳性能。因此，我们将坚持书中的参数设置。现在，让我们运行这两个算法，找出它们在随机游走设置下的表现。

## 结果

![](../Images/eac2e265f1bfe8545a65cb8f5dc1187b.png)

图4 算法比较结果。来源：作者提供的图

100 次比较后的结果如上图所示。TD 方法通常比 MC 方法提供更好的值估计，且α = 0.05 的 TD 方法可以非常接近真实值。图表还显示，MC 方法的方差比 TD 方法更高，因为兰花线的波动大于钢蓝线。

值得注意的是，对于这两种算法，当α值（相对）较高时，RMS 损失首先下降然后再上升。这种现象是由于值初始化和α值的共同作用。我们初始化了一个相对较高的 0.5，超过了节点 A 和 B 的真实值。由于随机策略使得有 50% 的机会选择“错误”步骤，从而使智能体远离正确的终端状态，因此更高的α值也会强调错误步骤，使结果偏离真实值。

现在让我们尝试将初始值降低到 0.1，并再次进行比较，看看问题是否得到缓解：

![](../Images/b1c80040596857301b3f6e5d2318af9c.png)

图 5 初始值为 0.1 的算法比较结果。来源：作者绘制的图

较低的初始值显然有助于缓解问题；没有明显的“下降然后上升”现象。然而，较低的初始值的副作用是学习效率较低，因为 RMS 损失在 150 轮后从未低于 0.05。因此，初始值、参数和算法性能之间存在权衡。

# 批量训练

在这篇文章中，我想提到的最后一点是对两种算法的批量训练比较。

考虑到我们面临以下情况：我们只在随机游走任务上积累了有限数量的经验，或者由于时间和计算限制，我们只能运行一定数量的轮次。批量更新 [2] 的想法是通过充分利用现有轨迹来应对这种情况。

批量训练的想法是反复更新一批轨迹上的值，直到值收敛到一个答案。只有在所有批次经验完全处理后，值才会被更新。让我们在随机游走环境中对这两种算法实施批量训练，看看 TD 方法是否仍优于 MC 方法。

## 结果

![](../Images/9fa0460989237ec36464dafe755bbc81.png)

图 6 批量训练结果。来源：作者绘制的图

批量训练结果显示，TD 方法在有限经验下仍优于 MC 方法，两种算法的性能差距十分明显。

# 结论

在这篇文章中，我们讨论了常数-α MC 方法和 TD(0) 方法之间的区别，并比较了它们在随机游走任务中的表现。TD 方法在本文所有测试中都优于 MC 方法，因此将 TD 作为强化学习任务的方法是一个更可取的选择。然而，这并不意味着 TD 总是优于 MC，因为后者有一个最明显的优势：无偏差。如果我们面对的是一个不能容忍偏差的任务，那么 MC 可能是更好的选择；否则，TD 更能处理一般情况。

## 参考文献

[1] Midjourney 服务条款: [https://docs.midjourney.com/docs/terms-of-service](https://docs.midjourney.com/docs/terms-of-service)

[2] Sutton, Richard S., 和 Andrew G. Barto. *强化学习：导论*。麻省理工学院出版社，2018。

本文的 GitHub 仓库：[链接](https://github.com/terrence-ou/Reinforcement-Learning-2nd-Edition-Notes-Codes/tree/main/chapter_06_temporal_difference_learning)。
