- en: Implementing Vision Transformer (ViT) from Scratch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零实现 Vision Transformer (ViT)
- en: 原文：[https://towardsdatascience.com/implementing-vision-transformer-vit-from-scratch-3e192c6155f0?source=collection_archive---------9-----------------------#2023-03-07](https://towardsdatascience.com/implementing-vision-transformer-vit-from-scratch-3e192c6155f0?source=collection_archive---------9-----------------------#2023-03-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/implementing-vision-transformer-vit-from-scratch-3e192c6155f0?source=collection_archive---------9-----------------------#2023-03-07](https://towardsdatascience.com/implementing-vision-transformer-vit-from-scratch-3e192c6155f0?source=collection_archive---------9-----------------------#2023-03-07)
- en: Understand how Vision Transformer (ViT) works by implementing it from scratch
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过从零实现 Vision Transformer (ViT) 了解其工作原理
- en: '[](https://medium.com/@tintn03?source=post_page-----3e192c6155f0--------------------------------)[![Tin
    Nguyen](../Images/f5a69125e3d42be7906c8cd51f827854.png)](https://medium.com/@tintn03?source=post_page-----3e192c6155f0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3e192c6155f0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3e192c6155f0--------------------------------)
    [Tin Nguyen](https://medium.com/@tintn03?source=post_page-----3e192c6155f0--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[Tin Nguyen](https://medium.com/@tintn03?source=post_page-----3e192c6155f0--------------------------------)
    [![Tin Nguyen](../Images/f5a69125e3d42be7906c8cd51f827854.png)](https://medium.com/@tintn03?source=post_page-----3e192c6155f0--------------------------------)
    [![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3e192c6155f0--------------------------------)
    [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3e192c6155f0--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F78d51d946a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-vision-transformer-vit-from-scratch-3e192c6155f0&user=Tin+Nguyen&userId=78d51d946a3&source=post_page-78d51d946a3----3e192c6155f0---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3e192c6155f0--------------------------------)
    ·10 min read·Mar 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3e192c6155f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-vision-transformer-vit-from-scratch-3e192c6155f0&user=Tin+Nguyen&userId=78d51d946a3&source=-----3e192c6155f0---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F78d51d946a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-vision-transformer-vit-from-scratch-3e192c6155f0&user=Tin+Nguyen&userId=78d51d946a3&source=post_page-78d51d946a3----3e192c6155f0---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3e192c6155f0--------------------------------)
    ·10 分钟阅读·2023年3月7日'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3e192c6155f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-vision-transformer-vit-from-scratch-3e192c6155f0&source=-----3e192c6155f0---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3e192c6155f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-vision-transformer-vit-from-scratch-3e192c6155f0&source=-----3e192c6155f0---------------------bookmark_footer-----------)'
- en: Vision Transformer (ViT) is an adaptation of Transformer models to computer
    vision tasks. It was proposed by Google researchers in 2020 and has since gained
    popularity due to its impressive performance on various image classification benchmarks.
    ViT has been shown to achieve state-of-the-art performance on several computer
    vision tasks and has sparked a lot of interest in the computer vision community.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Vision Transformer (ViT) 是将 Transformer 模型适配于计算机视觉任务的一种方法。它由 Google 研究人员在 2020
    年提出，并因其在各种图像分类基准测试中的卓越表现而获得广泛关注。ViT 已显示出在多个计算机视觉任务中实现了最先进的性能，并引起了计算机视觉社区的极大兴趣。
- en: In this post, we’re going to implement ViT from scratch for image classification
    using PyTorch. We will also train our model on the CIFAR-10 dataset, a popular
    benchmark for image classification. By the end of this post, you should have a
    good understanding of how ViT works and how to use it for your own computer vision
    projects.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将从头开始实现ViT用于图像分类，使用PyTorch。我们还将用CIFAR-10数据集训练我们的模型，这是一个流行的图像分类基准。通过这篇文章，你应该能很好地理解ViT的工作原理以及如何将其应用于自己的计算机视觉项目。
- en: The code for the implementation can be found in [this repo](https://github.com/tintn/vision-transformer-from-scratch).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 实现的代码可以在[这个仓库](https://github.com/tintn/vision-transformer-from-scratch)中找到。
- en: Overview of the ViT Architecture
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ViT架构概述
- en: '![](../Images/0d112d70a71929fa8a2a543c38d2e8fc.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0d112d70a71929fa8a2a543c38d2e8fc.png)'
- en: Adapted from [https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 改编自 [https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)
- en: ViT’s architecture is inspired by BERT, an encoder-only transformer model that
    is often used in NLP supervised learning tasks like text classification or named
    entity recognition. The main idea behind ViT is that an image can be seen as a
    series of patches, which can be treated as tokens in NLP tasks.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ViT的架构灵感来源于BERT，一个仅包含编码器的transformer模型，通常用于NLP监督学习任务，如文本分类或命名实体识别。ViT的主要思想是图像可以被视为一系列小块，这些小块可以在NLP任务中作为token处理。
- en: The input image is split into small patches, which are then flattened into sequences
    of vectors. These vectors are then processed by a transformer encoder, which allows
    the model to learn interactions between patches through self-attention mechanism.
    The output of the transformer encoder is then fed into a classification layer
    that outputs the predicted class of the input image.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 输入图像被分割成小块，然后将这些小块展平为向量序列。这些向量随后由一个transformer编码器处理，使得模型能够通过自注意力机制学习小块之间的交互。transformer编码器的输出被送入分类层，输出输入图像的预测类别。
- en: In the following sections, we will go through each component of the model and
    implement it using PyTorch. This will help us understand how ViT models work and
    how they can be applied to computer vision tasks.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将逐一实现模型的每个组件，并使用PyTorch进行实现。这将帮助我们理解ViT模型的工作原理及其在计算机视觉任务中的应用。
- en: Transform Images into Embeddings
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将图像转换为嵌入
- en: '![](../Images/175b1e3332f4ceb1ba3e7eaf3051caf2.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/175b1e3332f4ceb1ba3e7eaf3051caf2.png)'
- en: 'In order to feed input images to a Transformer model, we need to convert the
    images to a sequence of vectors. This is done by splitting the image into a grid
    of non-overlapping patches, which are then linearly projected to obtain a fixed-size
    embedding vector for each patch. We can use PyTorch’s `nn.Conv2d` layer for this
    purpose:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将输入图像馈送到Transformer模型中，我们需要将图像转换为向量序列。这通过将图像拆分成不重叠的小块，然后将这些小块线性投影以获得每个小块的固定大小嵌入向量来完成。我们可以使用PyTorch的`nn.Conv2d`层来实现这一点：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`kernel_size=self.patch_size` and `stride=self.patch_size` are to make sure
    the layer''s filter is applied to non-overlapping patches.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`kernel_size=self.patch_size` 和 `stride=self.patch_size` 是为了确保层的过滤器应用于不重叠的小块。'
- en: After the patches are converted to a sequence of embeddings, the [CLS] token
    is added to the beginning of the sequence, it will be used later in the classification
    layer to classify the image. The [CLS] token’s embedding is learned during training.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在小块被转换为嵌入序列后，[CLS] token被添加到序列的开头，它将在分类层中用于对图像进行分类。[CLS] token的嵌入在训练过程中学习。
- en: As patches from different positions may contribute differently to the final
    predictions, we also need a way to encode patch positions into the sequence. We’re
    going to use learnable position embeddings to add positional information to the
    embeddings. This is similar to how position embeddings are used in Transformer
    models for NLP tasks.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 由于来自不同位置的小块可能对最终预测的贡献不同，我们还需要一种方法将小块的位置编码到序列中。我们将使用可学习的位置嵌入将位置信息添加到嵌入中。这类似于在NLP任务的Transformer模型中使用位置嵌入的方式。
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: At this step, the input image is converted to a sequence of embeddings with
    positional information and ready to be fed into the transformer layer.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步，输入图像被转换为带有位置信息的嵌入序列，并准备好输入transformer层。
- en: Multi-head Attention
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多头注意力
- en: '![](../Images/d58fa5483fd06acc1e2a3ed1b025a6b1.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d58fa5483fd06acc1e2a3ed1b025a6b1.png)'
- en: Before going through the transformer encoder, we first explore the multi-head
    attention module, which is its core component. The multi-head attention is used
    to compute the interactions between different patches in the input image. The
    multi-head attention consists of multiple attention heads, each of which is a
    single attention layer.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入 transformer 编码器之前，我们首先探索多头注意力模块，这是其核心组件。多头注意力用于计算输入图像中不同块之间的交互。多头注意力由多个注意力头组成，每个头部是一个单一的注意力层。
- en: Let’s implement a head of the multi-head attention module. The module takes
    a sequence of embeddings as input and computes query, key, and value vectors for
    each embedding. The query and key vectors are then used to compute the attention
    weights for each token. The attention weights are then used to compute new embeddings
    using a weighted sum of the value vectors. We can think of this mechanism as a
    soft version of a database query, where the query vectors find the most relevant
    key vectors in the database, and the value vectors are retrieved to compute the
    query output.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现多头注意力模块的一个头部。该模块接受一个嵌入序列作为输入，并为每个嵌入计算查询、键和值向量。查询和键向量随后用于计算每个令牌的注意力权重。注意力权重被用于使用值向量的加权和计算新的嵌入。我们可以将这种机制视为数据库查询的软版本，其中查询向量在数据库中找到最相关的键向量，并检索值向量以计算查询输出。
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The outputs from all attention heads are then concatenated and linearly projected
    to obtain the final output of the multi-head attention module.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 所有注意力头的输出随后被拼接并线性映射，以获得多头注意力模块的最终输出。
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Transformer Encoder
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer 编码器
- en: '![](../Images/efd1e5da157ff66d35e382535c4e39fc.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/efd1e5da157ff66d35e382535c4e39fc.png)'
- en: The transformer encoder is made of a stack of transformer layers. Each transformer
    layer mainly consists of a multi-head attention module that we just implemented
    and a feed-forward network. To better scale the model and stabilize training,
    two Layer normalization layers and skip connections are added to the transformer
    layer.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: transformer 编码器由一系列 transformer 层组成。每个 transformer 层主要由我们刚刚实现的多头注意力模块和一个前馈网络组成。为了更好地扩展模型和稳定训练，transformer
    层中添加了两个层归一化层和跳过连接。
- en: Let’s implement a transformer layer (referred to as `Block` in the code as it's
    the building block for the transformer encoder). We'll begin with the feed-forward
    network, which is a simple two-layer MLP with GELU activation in between.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现一个 transformer 层（在代码中称为`Block`，因为它是 transformer 编码器的构建块）。我们将从前馈网络开始，它是一个简单的两层
    MLP，中间有 GELU 激活函数。
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We have implemented the multi-head attention and the MLP, we can combine them
    to create the transformer layer. The skip connections and layer normalization
    are applied to the input of each layer.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经实现了多头注意力和 MLP，可以将它们结合起来创建 transformer 层。跳过连接和层归一化被应用于每一层的输入。
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The transformer encoder stacks multiple transformer layers sequentially:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: transformer 编码器将多个 transformer 层按顺序堆叠在一起：
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ViT for image classification
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于图像分类的 ViT
- en: After inputting the image to the embedding layer and transformer encoder, we
    obtain new embeddings for both the image patches and the [CLS] token. At this
    point, the embeddings should have some useful signals for classification after
    being processed by the transformer encoder. Similar to BERT, we’ll use only the
    [CLS] token’s embedding to pass to the classification layer.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 将图像输入到嵌入层和 transformer 编码器后，我们获得了图像块和[CLS]标记的新嵌入。此时，嵌入经过 transformer 编码器处理后应该具有一些用于分类的有用信号。类似于
    BERT，我们将仅使用[CLS]标记的嵌入传递给分类层。
- en: 'The classification layer is a fully connected layer that takes the [CLS] embedding
    as input and outputs logits for each image. The following code implements the
    ViT model for image classification:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 分类层是一个全连接层，它接受[CLS]嵌入作为输入，并输出每张图像的 logits。以下代码实现了用于图像分类的 ViT 模型：
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: To train the model, you can follow the standard steps for training classification
    models. You can find the training script [here](https://github.com/tintn/vision-transformer-from-scratch/blob/main/train.py).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练模型，可以遵循训练分类模型的标准步骤。你可以在[这里](https://github.com/tintn/vision-transformer-from-scratch/blob/main/train.py)找到训练脚本。
- en: Results
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: 'As The goal is not to achieve state-of-the-art performance but to demonstrate
    how the model works, the model I trained is much smaller than the original ViT
    models described in the paper, which have at least 12 layers and a hidden size
    of 768\. The model config I used for the training is:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 由于目标不是实现最先进的性能，而是展示模型的工作原理，我训练的模型远小于论文中描述的原始 ViT 模型，这些模型至少有 12 层，隐藏层大小为 768。我用于训练的模型配置是：
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The model is trained on the CIFAR-10 dataset for 100 epochs, with a batch size
    of 256\. The learning rate was set to 0.01, and no learning rate schedule was
    used. The model is able to achieve 75.5% accuracy after 100 epochs of training.
    The following shows the training loss, test loss, and accuracy on the test set
    during training.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在 CIFAR-10 数据集上训练了 100 轮，批量大小为 256。学习率设置为 0.01，并且没有使用学习率调整。经过 100 轮训练后，模型达到了
    75.5% 的准确率。下图展示了训练期间的训练损失、测试损失和测试集上的准确率。
- en: '![](../Images/2c4978640e8c8022af2514760286c84f.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2c4978640e8c8022af2514760286c84f.png)'
- en: The plot below displays the model’s attention maps to some test images. You
    can see that the model is able to identify objects from different classes. It
    learned to focus on the objects and ignore the background.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了模型对一些测试图像的注意力图。你可以看到模型能够识别不同类别的对象。它学会了关注对象并忽略背景。
- en: '![](../Images/453dfa740f9d00871fd682243e39ef2f.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/453dfa740f9d00871fd682243e39ef2f.png)'
- en: Conclusion
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this post, we have learned how the Vision Transformer works, from the embedding
    layer to the transformer encoder and finally to the classification layer. We have
    also learned how to implement each component of the model using PyTorch.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们学习了 Vision Transformer 的工作原理，从嵌入层到变换器编码器，最后到分类层。我们还学习了如何使用 PyTorch
    实现模型的每个组件。
- en: Since this implementation is not intended for production use, I recommend using
    more mature libraries for transformers, such as [HuggingFace](https://github.com/huggingface/transformers),
    if you intend to train full-sized models or train them on large datasets.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 由于该实现不用于生产环境，如果你打算训练全尺寸模型或在大型数据集上训练，建议使用更成熟的变换器库，如 [HuggingFace](https://github.com/huggingface/transformers)。
- en: '*Originally published at* [*https://tintn.github.io*](https://tintn.github.io/Implementing-Vision-Transformer-from-Scratch/)
    *on March 7, 2023.*'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*最初发布于* [*https://tintn.github.io*](https://tintn.github.io/Implementing-Vision-Transformer-from-Scratch/)
    *2023年3月7日。*'
