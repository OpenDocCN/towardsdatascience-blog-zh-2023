- en: Implementing Vision Transformer (ViT) from Scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/implementing-vision-transformer-vit-from-scratch-3e192c6155f0?source=collection_archive---------9-----------------------#2023-03-07](https://towardsdatascience.com/implementing-vision-transformer-vit-from-scratch-3e192c6155f0?source=collection_archive---------9-----------------------#2023-03-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Understand how Vision Transformer (ViT) works by implementing it from scratch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@tintn03?source=post_page-----3e192c6155f0--------------------------------)[![Tin
    Nguyen](../Images/f5a69125e3d42be7906c8cd51f827854.png)](https://medium.com/@tintn03?source=post_page-----3e192c6155f0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3e192c6155f0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3e192c6155f0--------------------------------)
    [Tin Nguyen](https://medium.com/@tintn03?source=post_page-----3e192c6155f0--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F78d51d946a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-vision-transformer-vit-from-scratch-3e192c6155f0&user=Tin+Nguyen&userId=78d51d946a3&source=post_page-78d51d946a3----3e192c6155f0---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3e192c6155f0--------------------------------)
    ·10 min read·Mar 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3e192c6155f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-vision-transformer-vit-from-scratch-3e192c6155f0&user=Tin+Nguyen&userId=78d51d946a3&source=-----3e192c6155f0---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3e192c6155f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-vision-transformer-vit-from-scratch-3e192c6155f0&source=-----3e192c6155f0---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: Vision Transformer (ViT) is an adaptation of Transformer models to computer
    vision tasks. It was proposed by Google researchers in 2020 and has since gained
    popularity due to its impressive performance on various image classification benchmarks.
    ViT has been shown to achieve state-of-the-art performance on several computer
    vision tasks and has sparked a lot of interest in the computer vision community.
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we’re going to implement ViT from scratch for image classification
    using PyTorch. We will also train our model on the CIFAR-10 dataset, a popular
    benchmark for image classification. By the end of this post, you should have a
    good understanding of how ViT works and how to use it for your own computer vision
    projects.
  prefs: []
  type: TYPE_NORMAL
- en: The code for the implementation can be found in [this repo](https://github.com/tintn/vision-transformer-from-scratch).
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the ViT Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/0d112d70a71929fa8a2a543c38d2e8fc.png)'
  prefs: []
  type: TYPE_IMG
- en: Adapted from [https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)
  prefs: []
  type: TYPE_NORMAL
- en: ViT’s architecture is inspired by BERT, an encoder-only transformer model that
    is often used in NLP supervised learning tasks like text classification or named
    entity recognition. The main idea behind ViT is that an image can be seen as a
    series of patches, which can be treated as tokens in NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The input image is split into small patches, which are then flattened into sequences
    of vectors. These vectors are then processed by a transformer encoder, which allows
    the model to learn interactions between patches through self-attention mechanism.
    The output of the transformer encoder is then fed into a classification layer
    that outputs the predicted class of the input image.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will go through each component of the model and
    implement it using PyTorch. This will help us understand how ViT models work and
    how they can be applied to computer vision tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Transform Images into Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/175b1e3332f4ceb1ba3e7eaf3051caf2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to feed input images to a Transformer model, we need to convert the
    images to a sequence of vectors. This is done by splitting the image into a grid
    of non-overlapping patches, which are then linearly projected to obtain a fixed-size
    embedding vector for each patch. We can use PyTorch’s `nn.Conv2d` layer for this
    purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`kernel_size=self.patch_size` and `stride=self.patch_size` are to make sure
    the layer''s filter is applied to non-overlapping patches.'
  prefs: []
  type: TYPE_NORMAL
- en: After the patches are converted to a sequence of embeddings, the [CLS] token
    is added to the beginning of the sequence, it will be used later in the classification
    layer to classify the image. The [CLS] token’s embedding is learned during training.
  prefs: []
  type: TYPE_NORMAL
- en: As patches from different positions may contribute differently to the final
    predictions, we also need a way to encode patch positions into the sequence. We’re
    going to use learnable position embeddings to add positional information to the
    embeddings. This is similar to how position embeddings are used in Transformer
    models for NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: At this step, the input image is converted to a sequence of embeddings with
    positional information and ready to be fed into the transformer layer.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-head Attention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/d58fa5483fd06acc1e2a3ed1b025a6b1.png)'
  prefs: []
  type: TYPE_IMG
- en: Before going through the transformer encoder, we first explore the multi-head
    attention module, which is its core component. The multi-head attention is used
    to compute the interactions between different patches in the input image. The
    multi-head attention consists of multiple attention heads, each of which is a
    single attention layer.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s implement a head of the multi-head attention module. The module takes
    a sequence of embeddings as input and computes query, key, and value vectors for
    each embedding. The query and key vectors are then used to compute the attention
    weights for each token. The attention weights are then used to compute new embeddings
    using a weighted sum of the value vectors. We can think of this mechanism as a
    soft version of a database query, where the query vectors find the most relevant
    key vectors in the database, and the value vectors are retrieved to compute the
    query output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The outputs from all attention heads are then concatenated and linearly projected
    to obtain the final output of the multi-head attention module.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Transformer Encoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/efd1e5da157ff66d35e382535c4e39fc.png)'
  prefs: []
  type: TYPE_IMG
- en: The transformer encoder is made of a stack of transformer layers. Each transformer
    layer mainly consists of a multi-head attention module that we just implemented
    and a feed-forward network. To better scale the model and stabilize training,
    two Layer normalization layers and skip connections are added to the transformer
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s implement a transformer layer (referred to as `Block` in the code as it's
    the building block for the transformer encoder). We'll begin with the feed-forward
    network, which is a simple two-layer MLP with GELU activation in between.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We have implemented the multi-head attention and the MLP, we can combine them
    to create the transformer layer. The skip connections and layer normalization
    are applied to the input of each layer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The transformer encoder stacks multiple transformer layers sequentially:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: ViT for image classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After inputting the image to the embedding layer and transformer encoder, we
    obtain new embeddings for both the image patches and the [CLS] token. At this
    point, the embeddings should have some useful signals for classification after
    being processed by the transformer encoder. Similar to BERT, we’ll use only the
    [CLS] token’s embedding to pass to the classification layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The classification layer is a fully connected layer that takes the [CLS] embedding
    as input and outputs logits for each image. The following code implements the
    ViT model for image classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: To train the model, you can follow the standard steps for training classification
    models. You can find the training script [here](https://github.com/tintn/vision-transformer-from-scratch/blob/main/train.py).
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As The goal is not to achieve state-of-the-art performance but to demonstrate
    how the model works, the model I trained is much smaller than the original ViT
    models described in the paper, which have at least 12 layers and a hidden size
    of 768\. The model config I used for the training is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The model is trained on the CIFAR-10 dataset for 100 epochs, with a batch size
    of 256\. The learning rate was set to 0.01, and no learning rate schedule was
    used. The model is able to achieve 75.5% accuracy after 100 epochs of training.
    The following shows the training loss, test loss, and accuracy on the test set
    during training.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2c4978640e8c8022af2514760286c84f.png)'
  prefs: []
  type: TYPE_IMG
- en: The plot below displays the model’s attention maps to some test images. You
    can see that the model is able to identify objects from different classes. It
    learned to focus on the objects and ignore the background.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/453dfa740f9d00871fd682243e39ef2f.png)'
  prefs: []
  type: TYPE_IMG
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this post, we have learned how the Vision Transformer works, from the embedding
    layer to the transformer encoder and finally to the classification layer. We have
    also learned how to implement each component of the model using PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Since this implementation is not intended for production use, I recommend using
    more mature libraries for transformers, such as [HuggingFace](https://github.com/huggingface/transformers),
    if you intend to train full-sized models or train them on large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '*Originally published at* [*https://tintn.github.io*](https://tintn.github.io/Implementing-Vision-Transformer-from-Scratch/)
    *on March 7, 2023.*'
  prefs: []
  type: TYPE_NORMAL
