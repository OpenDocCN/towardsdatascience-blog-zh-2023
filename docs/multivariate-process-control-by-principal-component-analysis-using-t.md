# 主成分分析中的多变量过程控制，使用 T² 和 Q 误差

> 原文：[https://towardsdatascience.com/multivariate-process-control-by-principal-component-analysis-using-t%C2%B2-and-q-errors-c94908d14b04?source=collection_archive---------8-----------------------#2023-04-26](https://towardsdatascience.com/multivariate-process-control-by-principal-component-analysis-using-t%C2%B2-and-q-errors-c94908d14b04?source=collection_archive---------8-----------------------#2023-04-26)

## 使用和解读 Hotelling 的 T² 和平方预测误差 Q 在异常检测系统中的应用

[![Davide Massidda](../Images/b5cf1dc0201041ff5cad76ee13ee2df1.png)](https://medium.com/@davide.massidda?source=post_page-----c94908d14b04--------------------------------) [![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c94908d14b04--------------------------------) [Davide Massidda](https://medium.com/@davide.massidda?source=post_page-----c94908d14b04--------------------------------)

·

[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F433251ece79d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultivariate-process-control-by-principal-component-analysis-using-t%C2%B2-and-q-errors-c94908d14b04&user=Davide+Massidda&userId=433251ece79d&source=post_page-433251ece79d----c94908d14b04---------------------post_header-----------) 发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c94908d14b04--------------------------------) · 12分钟阅读 · 2023年4月26日

[]

![](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc94908d14b04&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultivariate-process-control-by-principal-component-analysis-using-t%25C2%25B2-and-q-errors-c94908d14b04&source=-----c94908d14b04---------------------bookmark_footer-----------)![](../Images/054e0f77d6a401cd71794ce2aa09c550.png)

图片来自 [geralt](https://pixabay.com/users/geralt-9301/) 在 [Pixabay](https://pixabay.com/illustrations/geometry-mathematics-volume-surface-1044090/)

作为数据科学家的工作中，构建制造过程控制的异常检测系统是一个重要部分，主成分分析（以下简称PCA）在我的工具箱中扮演着关键角色。

科学文献建议使用两种度量来追踪PCA中的异常：**Hotelling的T²**和平方预测误差，也称为**Q误差**。尽管它们的计算并不复杂，但基础软件包通常忽略它们，其含义仍然相当神秘。

网络上有数百个关于PCA的优秀教程。然而，尽管信息量很大，一些关于PCA在生产环境中使用的问题仍然需要得到充分解决。在本教程中，我将避免重复之前已有的内容，直截了当地切入主题。

## 本教程

在对PCA进行快速介绍后——仅仅强调一些要点——在本教程中，我们将深入探讨T²和Q误差的使用，拆解它们的公式以理解它们能告诉我们什么。这里你不会找到有关PCA及其在异常检测中使用的全部故事，但我将重点讲解T²和Q度量的含义：它们从何而来以及如何解读。

我将使用基础的Python代码，直接利用`numpy`中实现的SVD算法。我只会提供获得所描述结果所需的代码，省略生成图表的代码（你可以在 [这里](https://github.com/DavideMassidda/MPC-by-PCA) 找到）。

如果你至少对PCA有一个浅显的了解，请继续阅读。如果你不知道什么是主成分，建议你先阅读关于PCA的一般论文，然后再回来这里。

# 目录

**过程控制数据**

– *数据中心化和缩放* **通过PCA进行过程控制**

– *编码-解码系统*

– *蒸馏* **通过PCA进行异常检测**

– *平方预测误差 Q*

– *Hotelling的T²* **实际中的异常检测** – *从数据到主成分…再回来*

– *Q贡献*

– *T²贡献* **总结性意见**

# 过程控制数据

在本教程中，我生成了一些特定的数据，模拟了工业工厂中的一个简单过程控制。在 [这个仓库](https://github.com/DavideMassidda/MPC-by-PCA) 中，你可以找到数据和一个笔记本，其中包括用于运行分析的完整代码。

让我们开始读取数据并简要探索它。

```py
import pandas as pd

plant = pd.read_csv("plant_generated.csv")
plant.head()
```

```py
 group  time        var1       var2        var3       var4        var5
0  train     1   83.346705  28.115372  455.898265  12.808663  227.974152
1  train     2   90.594521  33.497319  462.503195  14.079053  228.173486
2  train     3  101.275664  30.396332  492.407791  16.832834  250.212025
3  train     4   90.898109  29.143537  472.162499  16.505277  234.079354
4  train     5   84.898605  29.459506  467.872180  12.801665  238.440786
```

表格包括一个“group”列，将观察分为*训练*集（100条记录）和*测试*集（30条记录）。记录了五个变量。

```py
variables = plant.columns[2:].tolist()
print(variables)
```

```py
['var1', 'var2', 'var3', 'var4', 'var5']
```

每个变量在时间上被监控。

```py
plant.groupby(["group"]).agg({"time": [len, min, max]})
```

```py
 time
       len  min  max
group   
 test   30    1   30
train  100    1  100
```

相关矩阵显示了变量集之间的良好关系。没有出现“孤立社区”或冗余测量。

```py
plant.loc[plant.group=="train", variables].corr()
```

```py
 var1      var2     var3       var4      var5
var1  1.000000  0.536242  0.773872  0.534615  0.727272
var2  0.536242  1.000000  0.535442  0.451099  0.632337
var3  0.773872  0.535442  1.000000  0.613662  0.709600
var4  0.534615  0.451099  0.613662  1.000000  0.571399
var5  0.727272  0.632337  0.709600  0.571399  1.000000
```

## 数据的中心化和缩放

由于数据具有不同的尺度，我们需要在运行PCA之前对值进行中心化和缩放，得到一个训练集，其中每个变量的均值为0，标准差为1。

为了重新缩放未来数据，我训练一个“缩放器”对象来存储中心和尺度参数，以便我可以转换训练数据。我将原始观察数据称为“X”，将缩放后的数据称为“Z”。

```py
import numpy as np
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_train = np.array(plant.loc[plant.group=="train", variables])
Z_train = scaler.fit_transform(X_train)
```

最后，看看每个自动缩放变量的时间序列记录。

![](../Images/9bb9203266640708a70d3e8322ed59eb.png)

图1\. 自动缩放变量的时间序列 — 图片来源于作者，许可证CC0。

# 通过PCA进行过程控制

观察PCA的一种方式是将其视为一个经过训练的编码-解码系统，该数据集表示一个受控过程。在训练过程中，系统学习监控变量之间的关系规则。随后，依赖于这些规则，系统可以评估新数据，确定过程是否在控制之中。

PCA将观察到的变量视作一个整体，并**重新分配它们的变异性**，构建新的**正交**变量：主成分（PCs）。从* k *个变量开始，PCA可以得到* k *个PCs。该算法估计系数（*loadings*），用于与观察变量（Z）相乘以获得PCs。

![](../Images/cf94358b68cc321cd5bb4a72ef0f8e7d.png)

图2\. PCA中数据空间与成分空间之间的关系 — 图片来源于作者，许可证CC0。

实际上，PCA找到规则（即系数）来**将**数据从观察数据空间投影到主成分空间，从而获得PC得分。这种投影可以通过将PC得分与转置的系数矩阵相乘来恢复（见图2）。

我在以下代码块中使用奇异值分解算法来估计我们植物数据的系数。

```py
u, s, vh = np.linalg.svd(Z_train)
loadings = np.transpose(vh)
print(loadings)
```

```py
array([[ 0.46831293, -0.07447163,  0.50970523,  0.1634115 , -0.69902377],
       [ 0.40387459,  0.76147727, -0.41265167,  0.29133908, -0.04333374],
       [ 0.47556921, -0.21952275,  0.30115426,  0.43988217,  0.66441965],
       [ 0.40644337, -0.58699292, -0.68019685, -0.01720318, -0.16516475],
       [ 0.47561121,  0.14783576,  0.12867607, -0.83344223,  0.20187888]])
```

系数数组报告变量在行上，PCs在列上。这是矩阵的常规方向，但软件包的排列方式可能会有所不同。

我们可以通过将观察数据矩阵乘以系数矩阵来获得PC得分。

```py
train_scores = Z_train @ loadings
```

## 编码-解码系统

PCA的一个关键概念是每个PC都由所有变量提供（尽管根据系数值的不同，程度有所不同）。因此，当我们检查单个PC的得分时，我们实际上是在一次性检查所有变量（是的，就是这样！）。

然而，总的数据变异性在PCs之间并不均等分配，因为PCs是从最大变异性（PC_1）到较少变异性（PC_k）进行排序的。实际上，前面的PCs代表数据中的“信号”，而后面的PCs代表噪声。因此，我们可以丢弃那些没有显著变异性的PCs，将信号与噪声分开。

下面的图展示了我们数据的时间序列PC得分。显然，随着成分排名的增加（特别是从第一个成分跳到第二个成分），变异性减少。

![](../Images/1128f44e5a1a16488cf6c14495a3e9c8.png)

图3\. 主成分得分的时间序列 — 图片来源于作者，许可证CC0。

## 蒸馏

在下面的图像中，我放大了图2\. 我用名为*提炼数据*的灰色节点表示了所有保留的主成分，而用另一个灰色节点表示了所有被舍弃的主成分。我使用“提炼”这个词是因为每个组件是许多变量的融合精华。

![](../Images/2b190cb219fc2564ac841b2d74d18d42.png)

图4\. PCA作为编码-解码系统 — 图片来源于作者，许可证 CC0。

我们可以通过对SVD的奇异值进行平方并除以自由度，计算每个组件吸收的方差量。

```py
n = Z_train.shape[0]
variances = s**2 / (n - 1)
```

让我们来看看结果。

```py
pd.DataFrame({
    "Component": [*range(1, len(s)+1)],
    "Variance": variances,
    "Proportion": variances/np.sum(variances),
    "Cumulative": np.cumsum(variances/np.sum(variances))
})
```

```py
 Component  Variance  Proportion  Cumulative
0          1  3.485770    0.690182    0.690182
1          2  0.573965    0.113645    0.803828
2          3  0.498244    0.098652    0.902480
3          4  0.276403    0.054728    0.957208
4          5  0.216122    0.042792    1.000000
```

第一个组件占数据总方差的69%。加上第二个组件，我们可以解释80%。如果我们假设剩下的20%是噪声，那么这两个组件就足以概括整个数据。

仅使用五个组件中的两个，就像在数据压缩中丢失信息。然而，由于丢失的部分理论上是噪声，我们通过舍弃最后几个组件获得了优势。

让我们隔离系统中的载荷和方差。

```py
n_comp = 2
pca_loadings = loadings[:,:n_comp]
pca_variances = variances[:n_comp]
```

当新数据到达训练好的PCA时，它们会被投影到组件空间中（编码）。之后，它们会被重建以逆转过程，仅使用最初的主成分（解码）。

# 通过PCA检测异常

现在，我们深入探讨通过Q和T²误差度量检测异常。首先，让我们了解它们代表什么。稍后我们将直接看到公式应用于数据。

## 平方预测误差 Q

由于我们在编码步骤中丢弃了噪声，重建的数据不能与观察到的数据完全一致。这种差异产生了误差Q。

当过程控制系统返回Q类型的异常时，**某些东西破坏了相关结构**：一个或多个变量不再与其他变量和谐变化（“和谐”这一概念由相关矩阵定义）。

我们可以区分两种极端情况。

+   一个变量出现了意外的值（不一定超出范围），并且不再能从其他变量中“预测”。如果期望两个变量有某种相关性，但其中一个的方向突然变化，就不再观察到它们的预期关系，从而发出警报。

+   变量表现正常，但其他所有变量都偏离了预期。如果之前的信息显示，当其他变量偏离时，我们的变量也应该偏离，但实际没有，这个过程就出现了异常。

第二种情况可能显得矛盾。因此，我们需要考虑另一种类型的误差：Hotelling的T²。

## Hotelling的T²

与Q不同，T²与**观察数据的度量更相关**：超出预期范围的值会产生极端的T²。

T² 统计量与[马哈拉诺比斯距离](https://en.wikipedia.org/wiki/Mahalanobis_distance)紧密相关。我们可以将 T² 视为基于观测数据阈值的多变量异常检测系统，但有一些相关的差异。

1.  观测数据已从编码系统中去噪。

1.  异常在 PC 级别被搜索，只有在此之后它们才会在观测数据级别进行解码。

第二点很有趣，因为 PCs 综合了整个变量组。使用 T²，我们不是逐个变量地从单变量角度寻找异常，而是一次性分析数据。

在训练过程中，我们存储了 PCs 的方差。现在，我们可以使用这些信息，将观测到的 PCs 方差与期望的方差进行比较，为每个 PC 计算“异常分数”。之后，我们可以解码这个异常分数以返回数据空间，从而获取每个变量对 PCs 变化的贡献。

# 实践中的异常检测

我们之前构建了 PCA 系统。现在，让我们开始使用它，将新数据投影到 PC 空间中。我们首先进行数据的居中和标准化，然后可视化变量的时间序列。

```py
X_test = np.array(plant.loc[plant.group=="test", variables])
Z_test = scaler.transform(X_test)
```

在下图中，表示了新数据中每个变量的时间序列。

![](../Images/5b264beb537ff0a6617c08b3fd51ea5b.png)

图 5\. 居中和标准化测试数据的时间序列 — 图片来自作者，许可证 CC0。

疯狂的数据，不是吗？正如你所见，**我模拟了一个疯狂的过程**。四个变量从期望均值剧烈偏移到高值，而第五个变量在初始偏移后逐渐恢复到正常值。

## 从数据到 PCs……再回到数据

我们可以**编码**数据，计算 PC 分数：

```py
test_scores = Z_train @ pca_loadings
```

随后，我们可以通过计算期望数据来**解码** PCs：

```py
test_expect = test_scores @ np.transpose(pca_loadings)
```

现在，让我们可视化重建数据的时间序列：

![](../Images/46a1faf68a46450b5ad31ffc1f4a9e06.png)

图 6\. 使用 PC 子集构建的期望数据时间序列 — 图片来自作者，许可证 CC0。

好的，这里我们终于揭示了多变量过程控制的一个根本问题。如果仔细查看上面的图表，你会注意到重建的数据与观测数据（图 5）相比显示了两个主要缺陷。

1.  前四个变量的观测数据在高峰处达到稳定状态。不同的是，它们的期望值在达到高峰后逐渐减少。

1.  第五个变量的观测数据在峰值处迅速增加并迅速减少。不同的是，其期望值在高峰后略微减少。从本质上讲，其趋势类似于其他变量之一。

前四个变量的重建受到了第五个变量情况的影响，而第五个变量的重建受到了前四个变量情况的影响。因此，每个重建的变量都是整个数据集的混合体。

这种不准确性发生**因为我们通过丢弃最后的PCs来解码**。根据我们的PCA，新数据的“疯狂”是噪声，因此它从最后的PC中被吸收，而这些PC吸收了训练数据中不存在的新数据的“奇异性”。

在生产环境中，Q和T²误差可以用来监控这些异常并理解其来源。对于这两种统计量，我们可以计算以下值：

+   每个变量对每种误差度量的贡献；

+   每种误差度量的平方累积贡献，生成Q和T²。

你可以在[这里](https://wiki.eigenvector.com/index.php?title=T-Squared_Q_residuals_and_Contributions)找到详细的数学解释。在这篇文章中，我将专注于贡献，因为数据监控的主要兴趣在于定位异常的来源。

## Q贡献

每个变量对Q误差的贡献是观察到的缩放数据与其预期（重建）值之间的距离。

```py
contrib_Q = Z_test - test_expect
```

在下面的图片中，你可以可视化新数据的Q贡献时间序列。

![](../Images/d59e0bca5f013d27591efda947055e22.png)

图7\. Q贡献的时间序列 — 图片来自作者，许可证CC0。

粗略来说，自监控周期的一半以来，Q贡献对所有变量的影响接近零。在周期的后半部分，Q贡献开始从零漂移，尤其是对第五个变量。

这不应让我们感到惊讶。正如我们所见，直到周期中期，变量是异常的，但异常**以同步的方式**（与训练集中观察到的相关性一致）。在周期的后半部分，第五个变量返回到均值附近，趋向于“正常”值（接近训练集中的值），但**打破**了相关性结构。

## T²贡献

获取T²贡献的公式类似于将PC解码到数据层面的公式，但有一个重要的区别：**缩放因子**。

PCs都以均值为中心，因此所有PC的期望值为零。然而，由于每个PC吸收的数据方差不同（见图3），每个PC具有其尺度。相同的分数可能在一个PC上代表小的变化，而在另一个PC上代表大的变化！

因此，在解码之前，每个PC都通过除以其标准差进行缩放。这样，PCs在解码前被置于相同的尺度上。

```py
pc_scaling = np.diag(1/np.sqrt(pca_variances))
contrib_T2 = test_scores @ pc_scaling @ np.transpose(pca_loadings)
```

下面的图片展示了T²贡献的时间序列。

![](../Images/5f6918975916ca544256ea9ac21057ce.png)

图8\. T²贡献的时间序列 — 图片来自作者，许可证CC0。

# 总结

使用PCA的过程控制将被监控变量视为一个应当一致移动的系统，期望其元素在一定范围内（Q统计量）和一致地变化（T²统计量）。

Q和T²异常之间的主要区别在于它们的来源：第一个是在**数据层面**，第二个是在**PC层面**。

Q 警告我们编码解码系统在新数据上由于变量间的观察关系未按预期工作。因此，Q 贡献使我们能够根据观察数据识别不可预测的变量。

T² 警告我们编码系统的 PC 分数与整个数据中心的距离过远。T² 贡献解码错误，“反向传播”到数据层级，允许我们识别具有异常观测值的变量。

核心思想是每条数据记录由信号和噪声组成。PCA 去除噪声并评估信号，如果变量的信号与预期（T²）过远或被噪声压倒（Q），则发出警报。尽管它们可以同时发生，但一个并不一定意味着另一个。
