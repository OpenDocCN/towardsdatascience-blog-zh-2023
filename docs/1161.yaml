- en: Zero-ETL, ChatGPT, And The Future of Data Engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=collection_archive---------0-----------------------#2023-04-03](https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=collection_archive---------0-----------------------#2023-04-03)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*The post-modern data stack is coming. Are we ready?*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://barrmoses.medium.com/?source=post_page-----71849642ad9c--------------------------------)[![Barr
    Moses](../Images/4c74558ee692a85196d5a55ac1920718.png)](https://barrmoses.medium.com/?source=post_page-----71849642ad9c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----71849642ad9c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----71849642ad9c--------------------------------)
    [Barr Moses](https://barrmoses.medium.com/?source=post_page-----71849642ad9c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2818bac48708&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=post_page-2818bac48708----71849642ad9c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----71849642ad9c--------------------------------)
    ·9 min read·Apr 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----71849642ad9c---------------------bookmark_footer-----------)![](../Images/ebb6da7687de8bfa3305f977a6b9af5b.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image courtesy of the authors.
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t like change, data engineering is not for you. Little in this space
    has escaped reinvention.
  prefs: []
  type: TYPE_NORMAL
- en: The most prominent, recent examples are Snowflake and Databricks disrupting
    the concept of the database and ushering in the modern data stack era.
  prefs: []
  type: TYPE_NORMAL
- en: As part of this movement, Fivetran and dbt fundamentally altered the data pipeline
    from ETL to ELT. Hightouch interrupted SaaS eating the world in an attempt to
    shift the center of gravity to the data warehouse. Monte Carlo joined the fray
    and said, “Maybe having engineers manually code unit tests isn’t the best way
    to ensure [data quality](https://www.montecarlodata.com/blog-what-is-data-observability/).”
  prefs: []
  type: TYPE_NORMAL
- en: Today, data engineers continue to stomp on hard coded pipelines and on-premises
    servers as they march up the modern data stack slope of enlightenment. The inevitable
    consolidation and trough of disillusionment appear at a safe distance on the horizon.
  prefs: []
  type: TYPE_NORMAL
- en: 'And so it almost seems unfair that new ideas are already springing up to disrupt
    the disruptors:'
  prefs: []
  type: TYPE_NORMAL
- en: Zero-ETL has data ingestion in its sights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI and Large Language Models could transform transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data product containers are eyeing the table’s thrown as the core building block
    of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are we going to have to rebuild everything (again)? The body of the Hadoop era
    isn’t even all that cold.
  prefs: []
  type: TYPE_NORMAL
- en: The answer is, yes of course we will have to rebuild our data systems. Probably
    several times throughout our careers. The real questions are the why, when, and
    the how (in that order).
  prefs: []
  type: TYPE_NORMAL
- en: I don’t profess to have all the answers or a crystal ball. But this article
    will closely examine some of the most prominent near(ish) future ideas that *may*
    become part of the post-modern data stack as well as their potential impact on
    data engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Practicalities and tradeoffs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/28d17f17d5963a293653541bc2ae4bd5.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Tingey Injury Law Firm](https://unsplash.com/pt-br/@tingeyinjurylawfirm?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/balancing-scales?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: The modern data stack didn’t arise because it did everything better than its
    predecessor. There are real trade-offs. Data is bigger and faster, but it’s also
    messier and less governed. The jury is still out on cost efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: The modern data stack reigns supreme because it supports use cases and unlocks
    value from data in ways that were previously, if not impossible, then certainly
    very difficult. Machine learning moved from buzz word to revenue generator. Analytics
    and experimentation can go deeper to support bigger decisions.
  prefs: []
  type: TYPE_NORMAL
- en: The same will be true for each of the trends below. There will be pros and cons,
    but what will drive adoption is how they, or the dark horse idea we haven’t yet
    discovered, unlock new ways to leverage data. Let’s look closer at each.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-ETL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/d489673d77e0fda119d06174852e3953.png)'
  prefs: []
  type: TYPE_IMG
- en: '**What it is**: A misnomer for one thing; the data pipeline still exists.'
  prefs: []
  type: TYPE_NORMAL
- en: Today, data is often generated by a service and written into a transactional
    database. An automatic pipeline is deployed which not only moves the raw data
    to the analytical data warehouse, but modifies it slightly along the way.
  prefs: []
  type: TYPE_NORMAL
- en: For example, APIs will export data in JSON format and the ingestion pipeline
    will need to not only transport the data but apply light transformation to ensure
    it is in a table format that can be loaded into the data warehouse. Other common
    light transformations done within the ingestion phase are data formatting and
    deduplication.
  prefs: []
  type: TYPE_NORMAL
- en: While you can do heavier transformations by hard coding pipelines in Python,
    and [some have advocated for doing just that](https://medium.com/towards-data-science/is-the-modern-data-warehouse-broken-1c9cbfddec3e)
    to deliver data pre-modeled to the warehouse, most data teams choose not to do
    so for expediency and visibility/quality reasons.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-ETL changes this ingestion process by having the transactional database
    do the data cleaning and normalization prior to automatically loading it into
    the data warehouse. It’s important to note the data is still in a relatively raw
    state.
  prefs: []
  type: TYPE_NORMAL
- en: At the moment, this tight integration is possible because most zero-ETL architectures
    require both the transactional database and data warehouse to be from the same
    cloud provider.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros**: Reduced latency. No duplicate data storage. One less source for failure.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cons**: Less ability to customize how the data is treated during the ingestion
    phase. Some vendor lock-in.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Who’s driving it**: AWS is the driver behind the buzzword ([Aurora to Redshift](https://aws.amazon.com/about-aws/whats-new/2022/11/amazon-aurora-zero-etl-integration-redshift/)),
    but GCP ([BigTable to BigQuery](https://www.infoq.com/news/2022/08/bigtable-bigquery-zero-etl/))
    and Snowflake ([Unistore](https://www.snowflake.com/en/data-cloud/workloads/unistore/))
    all offer similar capabilities. Snowflake ([Secure Data Sharing](https://docs.snowflake.com/en/user-guide/data-sharing-intro))
    and Databricks ([Delta Sharing](https://www.databricks.com/product/delta-sharing))
    are also pursuing what they call “no copy data sharing.” This process *actually*
    doesn’t involve ETL and instead provides expanded access to the data where it’s
    stored.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Practicality and value unlock potential**: On one hand, with the tech giants
    behind it and ready to go capabilities, zero-ETL seems like it’s only a matter
    of time. On the other, I’ve observed data teams decoupling rather than more tightly
    integrating their operational and analytical databases to prevent unexpected schema
    changes from crashing the entire operation.'
  prefs: []
  type: TYPE_NORMAL
- en: This innovation could further lower the visibility and accountability of software
    engineers toward the data their services produce. Why should they care about the
    schema when the data is already on its way to the warehouse shortly after the
    code is committed?
  prefs: []
  type: TYPE_NORMAL
- en: With data steaming and micro-batch approaches seeming to serve most demands
    for “real-time” data at the moment, I see the primary business driver for this
    type of innovation as infrastructure simplification. And while that’s nothing
    to scoff at, the possibility for no copy data sharing to remove obstacles to lengthy
    security reviews may result in greater adoption in the long-run (although to be
    clear it’s not an either/or).
  prefs: []
  type: TYPE_NORMAL
- en: One Big Table and Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/c839082df453d6f5221ee12002197faf.png)'
  prefs: []
  type: TYPE_IMG
- en: '**What it is**: Currently, business stakeholders need to express their requirements,
    metrics, and logic to data professionals who then translate it all into a SQL
    query and maybe even a dashboard. That process takes time, even when all the data
    already exists within the data warehouse. Not to mention on the data team’s list
    of favorite activities, ad-hoc data requests rank somewhere between root canal
    and documentation.'
  prefs: []
  type: TYPE_NORMAL
- en: There is a bevy of startups aiming to take the power of large language models
    like GPT-4 to automate that process by letting consumers “query” the data in their
    natural language in a slick interface.
  prefs: []
  type: TYPE_NORMAL
- en: At least until our new robot overlords make Binary the new official language.
  prefs: []
  type: TYPE_NORMAL
- en: This would radically simplify the self-service analytics process and further
    democratize data, but it will be difficult to solve beyond basic “metric fetching,”
    given the complexity of data pipelines for more advanced analytics.
  prefs: []
  type: TYPE_NORMAL
- en: But what if that complexity was simplified by stuffing all the raw data into
    one big table?
  prefs: []
  type: TYPE_NORMAL
- en: That [was the idea put forth](https://benn.substack.com/p/the-rapture-and-the-reckoning#footnote-anchor-12-99275606)
    by Benn Stancil, one of data’s best and forward thinking writer/founders. No one
    has [imagined](https://benn.substack.com/p/how-fivetran-fails) the [death](https://benn.substack.com/p/how-dbt-fails)
    of the modern data stack more.
  prefs: []
  type: TYPE_NORMAL
- en: As a concept, it’s not *that* far-fetched. Some data teams already leverage
    a one big table (OBT) strategy, which has both [proponents and detractors](https://twitter.com/pdrmnvd/status/1619463942392389632).
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging large language models would seem to overcome one of the biggest challenges
    of using the one big table, which is the difficulty of discovery, pattern recognition,
    and its complete lack of organization. It’s helpful for humans to have a table
    of contents and well marked chapters for their story, but AI doesn’t care.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros**: Perhaps, finally delivering on the promise of [self service data
    analytics](https://www.montecarlodata.com/blog-is-self-service-datas-biggest-lie/).
    Speed to insights. Enables the data team to spend more time unlocking data value
    and building, and less time responding to ad-hoc queries.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cons**: Is it too much freedom? Data professionals are familiar with the
    painful eccentricities of data ([time zones](https://www.explainxkcd.com/wiki/index.php/1883:_Supervillain_Plan)!
    What is an “account?”) to an extent most business stakeholders are not. Do we
    benefit from having a representational rather than direct data democracy?'
  prefs: []
  type: TYPE_NORMAL
- en: '**Who’s driving it**: Super early startups such as [Delphi](https://www.delphihq.com/)
    and [GetDot.AI](https://getdot.ai/). Startups such as [Narrator](https://www.narratordata.com/).
    More established players doing some version of this such as Amazon [QuickSight](https://docs.aws.amazon.com/managedservices/latest/userguide/quicksight.html),
    Tableau [Ask Data](https://help.tableau.com/current/pro/desktop/en-us/ask_data.htm),
    or ThoughtSpot.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Practicality and value unlock potential: Refreshingly, this is not a [technology
    in search of a use case](https://en.wikipedia.org/wiki/Blockchain). The value
    and efficiencies are evident–but so are the technical challenges. This vision
    is still being built and will need more time to develop. Perhaps the biggest obstacle
    to adoption will be the infrastructure disruption required, which will likely
    be too risky for more established organizations.'
  prefs: []
  type: TYPE_NORMAL
- en: Data product containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**What it is**: A data table is the building block of data from which data
    products are built. In fact, many data leaders consider [production tables to
    be their data products](https://www.linkedin.com/posts/shanemurray5_datamesh-dataengineering-dataquality-activity-7023310666983735296-4W3Y?utm_source=share&utm_medium=member_desktop).
    However, for a data table to be treated like a product a lot of functionality
    needs to be layered on including access management, discovery, and data reliability.'
  prefs: []
  type: TYPE_NORMAL
- en: Containerization has been integral to the microservices movement in software
    engineering. They enhance portability, infrastructure abstraction, and ultimately
    enable organizations to scale microservices. The data product container concept
    imagines a similar containerization of the data table.
  prefs: []
  type: TYPE_NORMAL
- en: Data product containers may prove to be an effective mechanism for making data
    much more reliable and governable, especially if they can better surface information
    such as the semantic definition, [data lineage](https://www.montecarlodata.com/blog-data-lineage/),
    and quality metrics associated with the underlying unit of data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros**: Data product containers look to be a way to better package and execute
    on the four [data mesh](https://www.montecarlodata.com/blog-what-is-a-data-mesh-and-how-not-to-mesh-it-up/)
    principles (federated governance, data self service, treating data like a product,
    domain first infrastructure).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cons**: Will this concept make it easier or more difficult for organizations
    to scale their data products? Another fundamental question, which could be asked
    of many of these futuristic data trends, is do the byproducts of data pipelines
    (code, data, metadata) contain value for data teams that is worth preserving?'
  prefs: []
  type: TYPE_NORMAL
- en: '**Who’s driving it**: [Nextdata](https://www.nextdata.com/), the startup founded
    by data mesh creator Zhamak Dehgahni. [Nexla](https://www.nexla.com/nexsets-modern-data-building-blocks/)
    has been playing in this space as well.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Practicality and value unlock potential: While Nextdata has only recently emerged
    from stealth and data product containers are still evolving, many data teams have
    seen proven results from data mesh implementations. The future of the data table
    will be dependent on the exact shape and execution of these containers.'
  prefs: []
  type: TYPE_NORMAL
- en: The endless reimagination of the data lifecycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/eb8fb02d6640429c8ab06c6d65e66465.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [zero take](https://unsplash.com/fr/@zerotake?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/seasons?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: To peer into the data future, we need to look over our shoulder at the data
    past and present. Past, present, future — data infrastructures are in a constant
    state of disruption and rebirth (although perhaps we need some [more chaos](https://medium.com/towards-data-science/the-chaos-data-engineering-manifesto-5dc09a182e85)).
  prefs: []
  type: TYPE_NORMAL
- en: The meaning of data warehouse has changed drastically from the term introduced
    by Bill Inmon in the 1990s. ETL pipelines are now ELT pipelines. The data lake
    is not as amorphous as it was just two years ago.
  prefs: []
  type: TYPE_NORMAL
- en: With these innovations ushered in by the modern data stack, data engineers have
    still played a central, technical role in dictating how data moves and how data
    consumers access it. But some changes are bigger and scarier than others.
  prefs: []
  type: TYPE_NORMAL
- en: The term zero-ETL seems threatening because it (inaccurately) suggests the death
    of pipelines, and without pipelines do we need data engineers?
  prefs: []
  type: TYPE_NORMAL
- en: For all the hype behind ChatGPT’s ability to generate code, this process is
    still very much in the hands of technical data engineers who still need to review
    and debug. The scary aspect about large language models is how they might fundamentally
    warp the data pipeline or our relationship with data consumers (and how data is
    served to them).
  prefs: []
  type: TYPE_NORMAL
- en: However this future, if and when it comes to pass, is still strongly reliant
    on data engineers.
  prefs: []
  type: TYPE_NORMAL
- en: What has endured since the dawn of time is the general lifecycle of data. It
    is emitted, it is shaped, it is used, and then it is archived (best to avoid dwelling
    on our own mortality here).
  prefs: []
  type: TYPE_NORMAL
- en: While the underlying infrastructure may change and automations will shift time
    and attention to the right or left, **human data engineers will continue to play
    a crucial role in extracting value from data for the foreseeable future**.
  prefs: []
  type: TYPE_NORMAL
- en: It’s not because future technologies and innovations can’t simplify the complex
    data infrastructures of today–it’s because our demand and uses for data will continue
    to increase in sophistication and scale.
  prefs: []
  type: TYPE_NORMAL
- en: Big data has and always will be a pendulum swinging back and forth. We make
    a leap forward in capacity and then we just as quickly figure out a way to reach
    those boundaries until the next leap is required. Take comfort in this cycle–it’s
    nice to be needed.
  prefs: []
  type: TYPE_NORMAL
- en: '[***Shane Murray***](https://medium.com/@shane.murray5) ***co-authored this
    article.*** [***Subscribe***](https://medium.com/subscribe/@shane.murray5) ***to
    get his stories delivered to your inbox.***'
  prefs: []
  type: TYPE_NORMAL
- en: '***Interested in the future of data quality? Reach out to the*** [***Monte
    Carlo Team***](https://www.montecarlodata.com/request-a-demo/)***!***'
  prefs: []
  type: TYPE_NORMAL
