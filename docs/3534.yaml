- en: 'The History of Open-Source LLMs: Imitation and Alignment (Part Three)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-history-of-open-source-llms-imitation-and-alignment-part-three-603d709c7aa5?source=collection_archive---------12-----------------------#2023-11-28](https://towardsdatascience.com/the-history-of-open-source-llms-imitation-and-alignment-part-three-603d709c7aa5?source=collection_archive---------12-----------------------#2023-11-28)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Open-source LLMs need alignment to become truly remarkable…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wolfecameron.medium.com/?source=post_page-----603d709c7aa5--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----603d709c7aa5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----603d709c7aa5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----603d709c7aa5--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----603d709c7aa5--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-history-of-open-source-llms-imitation-and-alignment-part-three-603d709c7aa5&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----603d709c7aa5---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----603d709c7aa5--------------------------------)
    ·20 min read·Nov 28, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F603d709c7aa5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-history-of-open-source-llms-imitation-and-alignment-part-three-603d709c7aa5&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----603d709c7aa5---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F603d709c7aa5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-history-of-open-source-llms-imitation-and-alignment-part-three-603d709c7aa5&source=-----603d709c7aa5---------------------bookmark_footer-----------)![](../Images/09529330477dcea0f682d4764a7fe0fc.png)'
  prefs: []
  type: TYPE_NORMAL
- en: (Photo by [Joanna Kosinska](https://unsplash.com/@joannakosinska?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/brown-paper-and-black-pen-B6yDtYs2IgY?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
  prefs: []
  type: TYPE_NORMAL
- en: A majority of prior research on open-source large language models (LLMs) focused
    heavily upon creating pre-trained base models. However, these models have not
    undergone any fine-tuning, so they fail to match the quality of top closed-source
    LLMs (e.g., ChatGPT or Claude) due to their lack of alignment. Paid models are
    aligned extensively using techniques like SFT and RLHF, which greatly enhances
    their usability. In comparison, open-source models are typically fine-tuned to
    a lesser extent using smaller, public datasets. Within this overview, however,
    we will take a look at recent research that aims to improve the quality of open-source
    LLMs via more extensive fine-tuning and alignment.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1a77e11ba9dd423a60dcfd943e2a772e.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [1, 2, 12])
  prefs: []
  type: TYPE_NORMAL
- en: This overview is the third (and final) part of my series on the history of open-source
    LLMs. In the [first part](/the-history-of-open-source-llms-early-days-part-one-d782bcd8f7e8)
    of the series, we looked at initial attempts at creating open-source language
    models. Although these initial pre-trained LLMs performed poorly, they were quickly
    followed up by much better open-source base models, which we covered in [part
    two](https://medium.com/towards-data-science/the-history-of-open-source-llms-better-base-models-part-two-6ca51ae74ebe)
    of…
  prefs: []
  type: TYPE_NORMAL
