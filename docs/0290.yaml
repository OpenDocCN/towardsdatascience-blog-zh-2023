- en: Predicting Metadata for Humanitarian Datasets Using GPT-3
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 GPT-3 预测人道主义数据集的元数据
- en: 原文：[https://towardsdatascience.com/predicting-metadata-for-humanitarian-datasets-using-gpt-3-b104be17716d?source=collection_archive---------4-----------------------#2023-01-18](https://towardsdatascience.com/predicting-metadata-for-humanitarian-datasets-using-gpt-3-b104be17716d?source=collection_archive---------4-----------------------#2023-01-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/predicting-metadata-for-humanitarian-datasets-using-gpt-3-b104be17716d?source=collection_archive---------4-----------------------#2023-01-18](https://towardsdatascience.com/predicting-metadata-for-humanitarian-datasets-using-gpt-3-b104be17716d?source=collection_archive---------4-----------------------#2023-01-18)
- en: '[](https://medium.com/@astrobagel?source=post_page-----b104be17716d--------------------------------)[![Matthew
    Harris](../Images/4fa3264bb8a028633cd8d37093c16214.png)](https://medium.com/@astrobagel?source=post_page-----b104be17716d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b104be17716d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b104be17716d--------------------------------)
    [Matthew Harris](https://medium.com/@astrobagel?source=post_page-----b104be17716d--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@astrobagel?source=post_page-----b104be17716d--------------------------------)[![Matthew
    Harris](../Images/4fa3264bb8a028633cd8d37093c16214.png)](https://medium.com/@astrobagel?source=post_page-----b104be17716d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b104be17716d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b104be17716d--------------------------------)
    [Matthew Harris](https://medium.com/@astrobagel?source=post_page-----b104be17716d--------------------------------)'
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4a2cd25b8ff9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-metadata-for-humanitarian-datasets-using-gpt-3-b104be17716d&user=Matthew+Harris&userId=4a2cd25b8ff9&source=post_page-4a2cd25b8ff9----b104be17716d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b104be17716d--------------------------------)
    ·19 min read·Jan 18, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb104be17716d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-metadata-for-humanitarian-datasets-using-gpt-3-b104be17716d&user=Matthew+Harris&userId=4a2cd25b8ff9&source=-----b104be17716d---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4a2cd25b8ff9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-metadata-for-humanitarian-datasets-using-gpt-3-b104be17716d&user=Matthew+Harris&userId=4a2cd25b8ff9&source=post_page-4a2cd25b8ff9----b104be17716d---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b104be17716d--------------------------------)
    ·19分钟阅读·2023年1月18日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb104be17716d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-metadata-for-humanitarian-datasets-using-gpt-3-b104be17716d&user=Matthew+Harris&userId=4a2cd25b8ff9&source=-----b104be17716d---------------------clap_footer-----------)'
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb104be17716d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-metadata-for-humanitarian-datasets-using-gpt-3-b104be17716d&source=-----b104be17716d---------------------bookmark_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb104be17716d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-metadata-for-humanitarian-datasets-using-gpt-3-b104be17716d&source=-----b104be17716d---------------------bookmark_footer-----------)'
- en: Responding to humanitarian disasters quickly, better still, anticipating them
    can [save lives](https://reliefweb.int/report/world/mark-lowcock-under-secretary-general-humanitarian-affairs-and-emergency-relief)
    [1]. Data is key to this, not just having *lots* of data, but [clean data which
    is well understood](https://www.devex.com/news/opinion-humanitarian-world-is-full-of-data-myths-here-are-the-most-popular-91959)
    [2] in order to create a clear view of the situation on the ground. In many cases
    this critical data is stored in hundreds of small spreadsheets, so piecing them
    altogether can be time consuming and difficult to maintain as new data comes in
    during a humanitarian incident. Automating the process of data discovery would
    potentially speed responses and improve outcomes for affected people.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 快速响应人道主义灾难，更好的是，能够预测这些灾难，可以[挽救生命](https://reliefweb.int/report/world/mark-lowcock-under-secretary-general-humanitarian-affairs-and-emergency-relief)[1]。数据是关键，不仅仅是拥有*大量*数据，而是[清洁的数据并且理解良好](https://www.devex.com/news/opinion-humanitarian-world-is-full-of-data-myths-here-are-the-most-popular-91959)[2]，以便创建对实际情况的清晰视图。在许多情况下，这些关键数据被存储在数百个小型电子表格中，因此在进行数据整合时可能非常耗时，并且在发生人道主义事件时，随着新数据的不断涌入，维护这些数据也很困难。自动化数据发现过程可能会加快响应速度并改善受影响人员的结果。
- en: One way to make discovery easier is to ensure that tabular data has metadata
    describing each column. This can help in linking together datasets, for example
    knowing that a column in a table of landmine locations specifies longitude and
    latitude, similar to a column in another table locating field hospitals. It’s
    not always obvious from column names what data they might contain, which can be
    in many languages adhering to varying standards. In an ideal world such metadata
    is provided with the data, but as we will see below this isn’t typically the case.
    Doing it manually can be a BIG job.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 简化发现的一个方法是确保表格数据有描述每列的元数据。这可以帮助将数据集链接在一起，例如知道一个地雷位置表中的列指定了经度和纬度，这与另一个表中定位野战医院的列类似。列名并不总能明显显示它们可能包含的数据，这些数据可能以多种语言呈现，并遵循不同的标准。在理想的情况下，这种元数据是与数据一起提供的，但正如我们下面将看到的那样，这通常不是情况。手动处理这项工作可能非常庞大。
- en: In this article I look at how we might help automate this process by using [OpenAI’s
    GPT-3](https://openai.com/blog/gpt-3-apps/) Large Language Model to predict metadata
    attributes of Humanitarian datasets, and improve on the performance of previous
    work.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将探讨我们如何通过使用[OpenAI的 GPT-3](https://openai.com/blog/gpt-3-apps/) 大型语言模型来预测人道主义数据集的元数据属性，从而帮助自动化这个过程，并改进以往工作的表现。
- en: The Humanitarian Data Exchange (HDX)
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人道主义数据交换（HDX）
- en: The [Humanitarian Data Exchange](https://data.humdata.org/) (HDX) is a fantastic
    platform which aims to address some of these issues by bringing humanitarian datasets
    together in a standardized way. As I write this there are 20,403 datasets globally,
    that cover a wide range of domains and file types. CSV and Excel files in these
    datasets result in about 148,000 distinct tables, lots of lovely data!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[人道主义数据交换](https://data.humdata.org/)（HDX）是一个极好的平台，旨在通过以标准化的方式将人道主义数据集合在一起，解决这些问题。截至我写这篇文章时，全球共有20,403个数据集，涵盖了广泛的领域和文件类型。这些数据集中的CSV和Excel文件产生了大约148,000个不同的表格，数据量非常庞大！'
- en: '![](../Images/4bf351851393834c447cc8b83c2844b5.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4bf351851393834c447cc8b83c2844b5.png)'
- en: Types of files on the [Humanitarian Data Exchange](https://data.humdata.org/)
    (HDX) Platform. See [this notebook](https://github.com/datakind/gpt-3-meta-data-discovery/blob/main/hdx_gpt-3_tag_prediction.ipynb)
    for how the data was collated.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[人道主义数据交换](https://data.humdata.org/)（HDX）平台上的文件类型。有关数据如何汇总的信息，请参见[这个笔记本](https://github.com/datakind/gpt-3-meta-data-discovery/blob/main/hdx_gpt-3_tag_prediction.ipynb)。'
- en: The Humanitarian Exchange Language (HXL)
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人道主义交换语言（HXL）
- en: One great thing about the HDX platform is that it encourages data owners to
    tag their data with tags in the [Humanitarian Exchange Language](https://hxlstandard.org/?_gl=1*p7ffaf*_ga*OTc2MzE0MTI5LjE2NzAxMDE2NjE.*_ga_E60ZNX2F68*MTY3MzQ5NDk2NS4xOC4xLjE2NzM0OTUzNjQuNjAuMC4w)
    (HXL) format. This metadata makes it easier to combine and use data in a meaningful
    way, speeding things up when time is important.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: HDX 平台的一个优点是它鼓励数据拥有者使用[人道主义交换语言](https://hxlstandard.org/?_gl=1*p7ffaf*_ga*OTc2MzE0MTI5LjE2NzAxMDE2NjE.*_ga_E60ZNX2F68*MTY3MzQ5NDk2NS4xOC4xLjE2NzM0OTUzNjQuNjAuMC4w)（HXL）格式来标记他们的数据。这些元数据使得将数据结合起来并以有意义的方式使用变得更容易，从而在时间紧迫时加快处理速度。
- en: 'HXL Tags come in two forms, those set at the dataset level, and field-level
    tags which apply to columns in tabular data. The latter look like this:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: HXL 标签有两种形式，一种是设置在数据集级别的标签，另一种是应用于表格数据中列的字段级别标签。后者看起来像这样：
- en: '![](../Images/ac56a776af2eb693400af0f63fb15ee0.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac56a776af2eb693400af0f63fb15ee0.png)'
- en: Example of a table with HXL tags on the second row [[#HXL Standards examples](https://hxlstandard.org/hxlexample/)
    ]
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 第二行有 HXL 标签的表格示例[[#HXL Standards examples](https://hxlstandard.org/hxlexample/)]
- en: Notice the second row just below the column headers, those are HXL tags. They
    consist of the tag which is prefixed with a ‘#’ (e.g ‘#adm1’) and in some cases
    attributes, eg ‘+name’.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 注意列标题下方的第二行，那些是 HXL 标签。它们由前缀为‘#’的标签（例如‘#adm1’）和某些情况下的属性（例如‘+name’）组成。
- en: The challenge is that these field-level tags are not always set on datasets
    in HDX, making it harder to use the amazing data there. Looking at CSV and Excel
    data for Kenya, most tables appear to be missing column HXL tags.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 挑战在于这些字段级标签并不总是设置在 HDX 数据集上，这使得使用那里的数据变得更加困难。查看肯尼亚的 CSV 和 Excel 数据，大多数表格似乎缺少列
    HXL 标签。
- en: '![](../Images/d797dbe8f75ae6c6827659970a062991.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d797dbe8f75ae6c6827659970a062991.png)'
- en: Analysis of data files for Kenya on the [Humanitarian Data Exchange](https://data.humdata.org/)
    (HDX) Platform, to see which have HXL column tags. See [this notebook](https://github.com/datakind/gpt-3-meta-data-discovery/blob/main/hdx_gpt-3_tag_prediction.ipynb)
    for how data was collated.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 分析[人道主义数据交换](https://data.humdata.org/) (HDX)平台上肯尼亚的数据文件，查看哪些文件有 HXL 列标签。有关如何整理数据的细节，请参见[这个笔记本](https://github.com/datakind/gpt-3-meta-data-discovery/blob/main/hdx_gpt-3_tag_prediction.ipynb)。
- en: Wouldn’t it be great if we could fill in those blanks and populate HXL tags
    for columns that don’t yet have them?
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能填补那些空白并为尚未拥有标签的列填充 HXL 标签，那不是很好吗？
- en: There has already been some really fantastic work by Microsoft on predicting
    HXL tags using [fastText](https://fasttext.cc/) embedding, see [this notebook](https://github.com/humanitarian-data-collaboration/hdx-python-model)
    and corresponding [paper](https://www.kdd.org/kdd2019/docs/Humanitarian_Data_tagging_KDD2019_SocialImpactTrack_HXLTagPrediction.pdf)
    [3]. The authors achieved 95% accuracy predicting tags and 92% predicting attributes,
    really great performance.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 微软已经在使用[fastText](https://fasttext.cc/) 嵌入预测 HXL 标签方面做了一些非常出色的工作，请参见[这个笔记本](https://github.com/humanitarian-data-collaboration/hdx-python-model)和相应的[论文](https://www.kdd.org/kdd2019/docs/Humanitarian_Data_tagging_KDD2019_SocialImpactTrack_HXLTagPrediction.pdf)
    [3]。作者在预测标签方面达到了 95% 的准确率，预测属性方面达到了 92%，表现非常出色。
- en: However, I wondered if we might use another technique now that there are some
    new kids on the block …
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，我想知道我们是否可以使用另一种技术，现在有了一些新技术……
- en: GPT-3
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT-3
- en: As I mentioned in [a previous article](https://medium.com/@astrobagel/2022-the-year-superstar-ai-models-stepped-onto-the-stage-90eefa95d9b6),
    there seems to be a real buzz around generative AI in the last year. One of the
    stars of this story has been Open AI’s [GPT-3](https://openai.com/blog/gpt-3-apps/)
    Large Language Model (LLM) which has some pretty amazing capabilities. Importantly
    it can be fine-tuned to learn patterns in special applications of language such
    [as computer code](https://openai.com/blog/openai-codex/).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我在[上一篇文章](https://medium.com/@astrobagel/2022-the-year-superstar-ai-models-stepped-onto-the-stage-90eefa95d9b6)中提到的，去年对生成性
    AI 似乎真的非常关注。这个故事的明星之一是 Open AI 的[GPT-3](https://openai.com/blog/gpt-3-apps/) 大型语言模型（LLM），它有一些非常惊人的能力。重要的是，它可以经过微调来学习语言的特殊应用模式，如[计算机代码](https://openai.com/blog/openai-codex/)。
- en: So it occurred to me that HXL tags are just another type of language ‘Special
    case’ and that it might be possible to fine-tune GPT-3 with some HXL tag examples,
    then see if it can predict them on new data.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我想到 HXL 标签只是另一种语言‘特殊情况’，可能可以通过一些 HXL 标签示例对 GPT-3 进行微调，然后查看它是否能对新数据进行预测。
- en: Getting some training data from HDX
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从 HDX 获取一些训练数据
- en: First, it’s worth clarifying some HDX hierarchy of datasets, resources and tables.
    A ‘Dataset’ can include a set of ‘Resources’ which are files. Datasets have their
    own page like [this one](https://data.humdata.org/dataset/mli-vegetation-indicators-dekad-admin2)
    which provide lots of useful information about history, who uploaded it and dataset-level
    tags.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，值得澄清一下 HDX 数据集、资源和表格的层次结构。‘数据集’可以包含一组‘资源’，这些资源是文件。数据集有自己的页面，比如[这个](https://data.humdata.org/dataset/mli-vegetation-indicators-dekad-admin2)，提供了很多关于历史、上传者和数据集级别标签的有用信息。
- en: '![](../Images/add9842a1f5e62a1255154ef3ee78352.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/add9842a1f5e62a1255154ef3ee78352.png)'
- en: Example of an HDX Dataset on the HDX platform
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: HDX 平台上的一个 HDX 数据集示例
- en: The example above has two CSV file resources, which if you select **More > Preview
    on HDX** display the HXL tags.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的示例有两个 CSV 文件资源，如果选择**更多 > 在 HDX 上预览**，可以显示 HXL 标签。
- en: '![](../Images/fe0c06003bbc3ef33a58457108499182.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fe0c06003bbc3ef33a58457108499182.png)'
- en: An [example resource](https://data.humdata.org/dataset/mli-vegetation-indicators-dekad-admin2/resource/be4ec19e-046f-4df2-8772-5615a06aef03)
    for a dataset on the HDX Platform
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: It’s a super cool platform!
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: We will be downloading resources like the one above for our analysis. HDX provides
    a [python library for interacting with their API](https://hdx-python-api.readthedocs.io/en/latest/),
    which can be installed with …
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You will then need to set up the connection. As we are only downloading open
    datasets we don’t need to set up an API key …
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: After some experimentation, I wrote a little wrapper to download resources (files)
    for each dataset. It supports CSV, TSV, XLS and XLSX file types, which should
    include enough tables for our model fine-tuning. It also saves the dataset and
    resource HDX JSON metadata along with each file.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The above is a bit long-winded because I wanted to be able to restart the download
    and have the process continue where it left off. Also, the API seemed to error
    from time to time, likely due to my internet connection, so there are some Try/Excepts
    in there. Not a fan of Try/Excepts usually but the aim is to create a training
    dataset, so I don’t mind some missing resources as long as I have a representative
    sample to train GPT-3.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Using the search HDX API we search for ‘HXL’ to find datasets which are likely
    to have HXL tags, then download files for those …
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This can take a while (a few hours) so get yourself a nice cup of tea!
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Column HXL tags are not listed in HDX resource metadata as far as I could tell,
    so to extract these we will have to analyze our downloaded files. After a bit
    of experimentation I wrote a few helper functions …
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now we can run it on our previously downloaded datafiles …
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This dataframe has 25,695 rows for each tabular dataset found when scanning
    CSV and Excel files for datasets on HDX found searching for ‘HXL’, along with
    a data preview, columns names and in some cases HXL tags.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: The Train/Test split
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Normally, I would simply use [Scikit learn’s train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)
    on the data to be used with the model. However, in doing this I noticed that often
    repeat resources (files) from the same dataset might occur in both training and
    test sets. For example, an organization might provide files for multiple airports,
    each being in exactly the same format with the same HXL tags. If we generate a
    prompts dataframe then split, airports from this dataset will appear in both the
    training and test set, which wouldn’t reflect very well our problem where we need
    to predict HXL tags for brand new datasets.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'To get around this I did the following:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Split the HDX ‘datasets’ into train/test (remember a dataset can have multiple
    resource files)
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using each I created dataframes of resources, one row per data file
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then using these train/test resources dataframes, I created train/test dataframes,
    one row per *column*. These are the GPT-3 prompts needed for fine-tuning
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，使用这些训练/测试资源数据框，我创建了训练/测试数据框，每列一个*行*。这些是 GPT-3 微调所需的提示。
- en: Creating GPT-3 fine-tuning prompts
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建 GPT-3 微调提示
- en: In order to [fine tune GPT-3](https://beta.openai.com/docs/guides/fine-tuning),
    we need to provide a prompt and response training file in JSONL format. For the
    prompt I decided to use (i) Column name; (ii) A sample of data from that column.
    The Completion will be the HXL tag and attributes.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了[微调 GPT-3](https://beta.openai.com/docs/guides/fine-tuning)，我们需要提供一个 JSONL
    格式的提示和响应训练文件。我决定使用（i）列名；（ii）该列的一个数据样本。补全将是 HXL 标签和属性。
- en: Here is an example …
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个例子……
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The format for GPT-3 is very particular and took a while to get right! Things
    like having a space at the start of completion are recommended by OpenAI.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 的格式非常特殊，花了一段时间才调整好！像在补全开始处添加空格这样的设置是 OpenAI 推荐的。
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You’ll notice in the above that I exclude any prompts where there are NaNs in
    the data. I figured we’d start with good data samples but this is something to
    be revisited in future.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到我在上面排除了数据中存在 NaNs 的提示。我认为我们应该从良好的数据样本开始，但这是需要在未来重新审视的内容。
- en: We can now generate a training dataset and save to a file for GPT-3 …
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以生成一个训练数据集，并将其保存为 GPT-3 的文件……
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This is what the training data looks like …
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是训练数据的样子……
- en: '[PRE9]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: There were 139,503 rows in this training dataset, one row per column as found
    in the tabular data we downloaded from HDX, specifically for cases where the column
    had HXL tags.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这个训练数据集中有 139,503 行，每列一行，来自我们从 HDX 下载的表格数据，专门用于那些列中有 HXL 标签的情况。
- en: Generating an OpenAI API Key
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成 OpenAI API 密钥
- en: Before we can do anything, you will need to [sign up for an OpenAI account](https://auth0.openai.com/u/signup/identifier?state=hKFo2SBHOUJWNE9YNnVfbnFxQWJTbXhESW5SNlpsNlFBNzFPaaFur3VuaXZlcnNhbC1sb2dpbqN0aWTZIFdJa0xtVTAxV21POU1WbWRVNmNGVmxvWkwyMDBGY1pVo2NpZNkgRFJpdnNubTJNdTQyVDNLT3BxZHR3QjNOWXZpSFl6d0Q).
    Once that’s done, you should have [$18 of free credits](https://openai.com/api/pricing/).
    If using a small amount of data, this should suffice, but for this analysis and
    a few model trainings I racked up a bill of $50, so you may need to attach a credit
    card to your account.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能做任何事情之前，你需要先[注册一个 OpenAI 账户](https://auth0.openai.com/u/signup/identifier?state=hKFo2SBHOUJWNE9YNnVfbnFxQWJTbXhESW5SNlpsNlFBNzFPaaFur3VuaXZlcnNhbC1sb2dpbqN0aWTZIFdJa0xtVTAxV21POU1WbWRVNmNGVmxvWkwyMDBGY1pVo2NpZNkgRFJpdnNubTJNdTQyVDNLT3BxZHR3QjNOWXZpSFl6d0Q)。完成后，你应该有[$18
    的免费积分](https://openai.com/api/pricing/)。如果使用少量数据，这应该足够，但在这次分析和几次模型训练中，我累计了 $50
    的账单，因此你可能需要将信用卡绑定到你的账户上。
- en: Once you have an account you can [generate an API key](https://beta.openai.com/account/api-keys).
    I opted to save this to a local file and referenced the file in code, but the
    [OpenAI Python library](https://github.com/openai/openai-python) support using
    environment variables also.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了账户，你可以[生成 API 密钥](https://beta.openai.com/account/api-keys)。我选择将其保存到本地文件并在代码中引用，但[OpenAI
    Python 库](https://github.com/openai/openai-python)也支持使用环境变量。
- en: Fine-tuning GPT-3
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调 GPT-3
- en: Right, here comes the exciting bit! With our nice training data, we can fine-tune
    GPT-3 as follows …
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，现在是令人兴奋的部分！有了我们精美的训练数据，我们可以按如下方式微调 GPT-3……
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In the above we submit the fine-tuning model run to OpenAI, we can then see
    status with …
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面，我们将微调模型提交给 OpenAI，然后可以查看状态……
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: I opted to keep things simple, but you can also submit to OpenAI and monitor
    status via a stream as shown [here](https://github.com/openai/openai-cookbook/blob/main/examples/azure/finetuning.ipynb).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我选择保持简单，但你也可以将其提交给 OpenAI，并通过[此处](https://github.com/openai/openai-cookbook/blob/main/examples/azure/finetuning.ipynb)显示的流来监控状态。
- en: Once the status is ‘succeeded’ you can now get a model ID to use for predictions
    (completions) …
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦状态显示为‘成功’，你现在可以获得一个模型 ID 用于预测（补全）……
- en: '[PRE12]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Predicting HXL tags with our fine-tuned GPT-3 model
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用我们微调过的 GPT-3 模型预测 HXL 标签
- en: We now have a model, let’s see what it can do!
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有了一个模型，来看看它能做什么！
- en: To call GPT-3 you can use the [Open AI Python library ‘create’ method](https://beta.openai.com/docs/api-reference/completions/create).
    It’s worth checking out the documentation to see what parameters you can tune.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 要调用 GPT-3，你可以使用[Open AI Python 库的 ‘create’ 方法](https://beta.openai.com/docs/api-reference/completions/create)。查看文档了解你可以调整的参数是值得的。
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Which we call with the following, limiting to 500 prompts …
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下方式调用，限制为 500 个提示……
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This yields the following results …
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下结果……
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Uhhhh!? That’s, well …. terrible. Predicting just the HXL tag worked really
    well, but predicting tag and attributes, not so much.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯！？那，嗯……糟糕。仅预测 HXL 标签效果非常好，但预测标签和属性的效果就差很多。
- en: Let’s look at some of the failed predictions …
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些失败的预测……
- en: '[PRE17]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Interesting. It seems the model completed and captured the correct tag and attributes
    almost perfectly, then added some extra attributes at the end. So for example
    …
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是。似乎模型几乎完美地完成并捕捉了正确的标签和属性，然后在末尾添加了一些额外的属性。例如……
- en: '[PRE18]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Let’s see how often the expected tags and attributes occurred in the first half
    of the predictions …
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看期望的标签和属性在预测的前半部分出现的频率……
- en: '[PRE19]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Out of 500 predictions, the expected tags and attributes were in the predicted
    tags and attributes **99%** of the time. Put another way, the expected values
    were the first part of most predictions.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在 500 次预测中，期望的标签和属性**99%**的时间都出现在预测的标签和属性中。换句话说，期望的值通常是大多数预测的首部分。
- en: So GPT-3 has great accuracy for predicting tags and attributes but adds extra
    attributes at the end.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 所以 GPT-3 在预测标签和属性方面具有很高的准确性，但在末尾添加了额外的属性。
- en: So, how to exclude those extra tokens?
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何排除那些额外的标记呢？
- en: Well, it turns out that the GPT-3 returns log probabilities for each token.
    As you will notice above, we also calculated a prediction assuming we stopped
    completing tokens if the log probability was above some cutoff value …
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，结果证明 GPT-3 返回了每个标记的对数概率。如上所述，我们还计算了一个预测，假设我们在对数概率高于某个截止值时停止完成标记……
- en: '[PRE20]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Let’s see how that performed, assuming a cutoff of -0.001 ..
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在假设截止值为 -0.001 的情况下表现如何……
- en: '[PRE21]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: That’s pretty good, 0.94 for tags and attributes. Since we know that the correct
    tags and attributes occur in the prediction 99% of the time, we should be able
    to do a little better with some tuning of the log probability cutoff and maybe
    with some post-processing.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这还不错，标签和属性的准确率为 0.94。既然我们知道正确的标签和属性在预测中出现的概率为 99%，我们应该通过调整对数概率截止值和进行一些后处理来做得更好。
- en: Conclusions and Future Work
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论与未来工作
- en: The above is a quick analysis to see how GPT-3 might be applied for predicting
    meta data, specifically HXL tags on humanitarian datasets. It performs really
    well on this task and has a lot of potential for similar metadata prediction tasks.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 以上是对 GPT-3 在预测元数据，特别是人道主义数据集上的 HXL 标签方面应用的快速分析。它在这一任务上的表现非常好，并且在类似的元数据预测任务中有很大的潜力。
- en: 'More work is needed to refine the approach of course, such as:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，还需要更多的工作来完善方法，例如：
- en: Trying other models (I used 'ada' above) to see if this improves performance
    (though it will cost more)
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试其他模型（我上面使用的是 'ada'）以查看是否能改善性能（尽管这会增加成本）。
- en: Model hyperparameter tuning. The log probability cutoff will likely be very
    important
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型超参数调整。对数概率截止值可能非常重要。
- en: More prompt engineering to perhaps include column list on the table might provide
    better context, we well as overlying columns on two-row header tables.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可能需要更多的提示工程，比如在表格中包含列列表，以提供更好的上下文，以及在两行标题表格上的覆盖列。
- en: More preprocessing. Not much was done for this article, blindly taking tables
    extracted from CSV files, so the data is can be a bit messy
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更多的预处理。这篇文章的处理不多，盲目地使用从 CSV 文件中提取的表格，因此数据可能有些混乱。
- en: That said, some great potential here I feel for using GPT-3 to predict metadata
    on datasets.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，我觉得使用 GPT-3 来预测数据集上的元数据有很大的潜力。
- en: More to follow soon!
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 敬请关注更多更新！
- en: References
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Mark Lowcock, Under-Secretary-General for Humanitarian Affairs and Emergency
    Relief Coordinator, [Anticipation saves lives: How data and innovative financing
    can help improve the world’s response to humanitarian crises](https://reliefweb.int/report/world/mark-lowcock-under-secretary-general-humanitarian-affairs-and-emergency-relief)
    (2019)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Mark Lowcock, 人道主义事务副秘书长和紧急救援协调员，[Anticipation saves lives: How data and
    innovative financing can help improve the world’s response to humanitarian crises](https://reliefweb.int/report/world/mark-lowcock-under-secretary-general-humanitarian-affairs-and-emergency-relief)
    (2019)'
- en: '[2] Sarah Telford, [Opinion: Humanitarian world is full of data myths. Here
    are the most popular](https://www.devex.com/news/opinion-humanitarian-world-is-full-of-data-myths-here-are-the-most-popular-91959)
    (2018)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Sarah Telford, [Opinion: Humanitarian world is full of data myths. Here
    are the most popular](https://www.devex.com/news/opinion-humanitarian-world-is-full-of-data-myths-here-are-the-most-popular-91959)
    (2018)'
- en: '[3] Vinitra Swamy et al, [Machine Learning for Humanitarian Data: Tag Prediction
    using the HXL Standard](https://www.kdd.org/kdd2019/docs/Humanitarian_Data_tagging_KDD2019_SocialImpactTrack_HXLTagPrediction.pdf)
    (2019)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Vinitra Swamy 等人，[人道主义数据的机器学习：使用 HXL 标准进行标签预测](https://www.kdd.org/kdd2019/docs/Humanitarian_Data_tagging_KDD2019_SocialImpactTrack_HXLTagPrediction.pdf)（2019年）'
- en: A notebook used for this analysis can be found [here](https://github.com/datakind/gpt-3-meta-data-discovery).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 用于此分析的笔记本可以在[这里](https://github.com/datakind/gpt-3-meta-data-discovery)找到。
