- en: How I Leveraged Open Source LLMs to Achieve Massive Savings on a Large Compute
    Project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-i-leveraged-open-source-llms-to-achieve-massive-savings-on-a-large-compute-project-bd8bb3c7267?source=collection_archive---------4-----------------------#2023-08-30](https://towardsdatascience.com/how-i-leveraged-open-source-llms-to-achieve-massive-savings-on-a-large-compute-project-bd8bb3c7267?source=collection_archive---------4-----------------------#2023-08-30)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Unlocking Cost-Efficiency in Large Compute Projects with Open Source LLMs and
    GPU Rentals.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@ryanshrott?source=post_page-----bd8bb3c7267--------------------------------)[![Ryan
    Shrott](../Images/186524066383b4b02c994692aebb3ea5.png)](https://medium.com/@ryanshrott?source=post_page-----bd8bb3c7267--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bd8bb3c7267--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bd8bb3c7267--------------------------------)
    [Ryan Shrott](https://medium.com/@ryanshrott?source=post_page-----bd8bb3c7267--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Faba7ffb1d8f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-leveraged-open-source-llms-to-achieve-massive-savings-on-a-large-compute-project-bd8bb3c7267&user=Ryan+Shrott&userId=aba7ffb1d8f5&source=post_page-aba7ffb1d8f5----bd8bb3c7267---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bd8bb3c7267--------------------------------)
    ·6 min read·Aug 30, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbd8bb3c7267&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-leveraged-open-source-llms-to-achieve-massive-savings-on-a-large-compute-project-bd8bb3c7267&user=Ryan+Shrott&userId=aba7ffb1d8f5&source=-----bd8bb3c7267---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbd8bb3c7267&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-leveraged-open-source-llms-to-achieve-massive-savings-on-a-large-compute-project-bd8bb3c7267&source=-----bd8bb3c7267---------------------bookmark_footer-----------)![](../Images/83382f63b21df0de2b0d47d33ad7212d.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Alexander Grey](https://unsplash.com/@sharonmccutcheon?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the world of large language models (LLMs), the cost of computation can be
    a significant barrier, especially for extensive projects. I recently embarked
    on a project that required running 4,000,000 prompts with an average input length
    of 1000 tokens and an average output length of 200 tokens. That’s nearly 5 billion
    tokens! The traditional approach of paying per token, as is common with models
    like GPT-3.5 and GPT-4, would have resulted in a hefty bill. However, I discovered
    that by leveraging open source LLMs, I could shift the pricing model to pay per
    hour of compute time, leading to substantial savings. This article will detail
    the approaches I took and compare and contrast each of them. Please note that
    while I share my experience with pricing, these are subject to change and may
    vary depending on your region and specific circumstances. The key takeaway here
    is the potential cost savings when leveraging open source LLMs and renting a GPU
    per hour, rather than the specific prices quoted.
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
