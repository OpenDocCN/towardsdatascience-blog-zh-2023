- en: The Matrix Algebra of Linear Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-matrix-algebra-of-linear-regression-6fb433f522d5?source=collection_archive---------3-----------------------#2023-05-03](https://towardsdatascience.com/the-matrix-algebra-of-linear-regression-6fb433f522d5?source=collection_archive---------3-----------------------#2023-05-03)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Looking under the hood at the matrix operations behind linear regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@dataforyou?source=post_page-----6fb433f522d5--------------------------------)[![Rob
    Taylor, PhD](../Images/5e4e86da7b77404ed42d00a60ea5eacf.png)](https://medium.com/@dataforyou?source=post_page-----6fb433f522d5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6fb433f522d5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6fb433f522d5--------------------------------)
    [Rob Taylor, PhD](https://medium.com/@dataforyou?source=post_page-----6fb433f522d5--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F98de080592fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-matrix-algebra-of-linear-regression-6fb433f522d5&user=Rob+Taylor%2C+PhD&userId=98de080592fc&source=post_page-98de080592fc----6fb433f522d5---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6fb433f522d5--------------------------------)
    ·11 min read·May 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6fb433f522d5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-matrix-algebra-of-linear-regression-6fb433f522d5&user=Rob+Taylor%2C+PhD&userId=98de080592fc&source=-----6fb433f522d5---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6fb433f522d5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-matrix-algebra-of-linear-regression-6fb433f522d5&source=-----6fb433f522d5---------------------bookmark_footer-----------)![](../Images/46a525be79cdfa66f016233756660562.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Mingwei Lim](https://unsplash.com/@cmzw?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For most, simple linear regression is a common starting point for understanding
    model-based estimation and model comparison. Regardless of whether you’re taking
    an introductory statistics or data science course, you can bet your bottom dollar
    that linear regression will crop up at some point. And there’s a good reason for
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Simple linear regression provides a natural extension of simple descriptive
    statistics by modeling the response variable as a linear combination of only a
    single predictor. This simplicity not only facilitates the interpretation of model
    parameters but also makes estimation via ordinary least squares (OLS) easier to
    grasp.
  prefs: []
  type: TYPE_NORMAL
- en: While most textbook introductions will provide a detailed mathematical treatment
    in addition to the broader conceptual elements, when it comes to actually implementing
    the model it’s almost never through first principles. No matter which language
    you’re using there’ll almost certainly be a convenience function that fits the
    model for you. And why wouldn’t you use it? You don’t have to do all those tedious
    calculations by hand! That’s definitely a plus; though I’m a firm believer that
    taking some time to familiarise yourself with a model’s statistical machinery
    is an essential part of becoming an effective analyst and data scientist.
  prefs: []
  type: TYPE_NORMAL
- en: In an earlier post, I provided a primer on linear algebra that touched on some
    fundamental principles and operations. In this post, we’re going to build on those
    concepts by looking under the hood and getting our hands dirty with the matrix
    operations that underpin linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: Simple Linear Regression in Matrix Form
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Most will be familiar with the standard regression formula that models a response
    variable *Y* as a linear combination of a single predictor *X*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/10fec3de1e6596b67bdfb8735cde4992.png)'
  prefs: []
  type: TYPE_IMG
- en: The linear regression equation (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: where here I’ve adopted the convention of assuming errors are normally distributed.
    From here we’ll build out the matrix representation by assigning the elements
    to vectors and matrices.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll put all the responses in an *n*-dimensional vector called the
    *response vector*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3e826f9d3b97b2f3360e9957ad25e81f.png)'
  prefs: []
  type: TYPE_IMG
- en: The response vector (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: I’ve been quite explicit here by including the size of the vector — which is
    actually represented as a *column matrix —* just so we can keep track of things.
    However, it’s perfectly reasonable to use a lowercase boldface **y** if you wanted
    to.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, the predictor *X* is placed into an *n* × *p* matrix called the design
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be8751ab79cbbcdc314506630441a5e9.png)'
  prefs: []
  type: TYPE_IMG
- en: The design matrix (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: where *p* denotes the number of columns and corresponds to the number of coefficients
    in the model. Note that the first column contains only ones — we’ll discuss this
    shortly — but this is to accommodate the intercept, which is a constant. Therefore,
    the number of columns in the design matrix is always one greater than the number
    of predictors you have. In the example above, we only have a single predictor
    meaning we need to estimate an intercept and a slope; so, *p* = 2 in this case.
  prefs: []
  type: TYPE_NORMAL
- en: 'The regression coefficients are also placed into a *p* × 1 vector called the
    *parameter vector*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7bc5dde3b39840066299f01844d16f71.png)'
  prefs: []
  type: TYPE_IMG
- en: The parameter vector (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: Again, *p* denotes the number of parameters, though here *p* denotes the number
    of rows, unlike the design matrix where *p* is the column dimension. This arrangement
    is important because we will need to do some *matrix multiplication* with these
    two objects to compute the linear predictor.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, before we do that, there is one last thing to do — place all the error
    terms into an *n* × 1 vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/37954d9e73d10f2bfc8dfbae410aaf50.png)'
  prefs: []
  type: TYPE_IMG
- en: The error vector (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: 'With all of these in place, we can now express the simple linear regression
    model using matrix notation like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e9cfd5d551172af9b4d6c7c13fe0eefb.png)'
  prefs: []
  type: TYPE_IMG
- en: The linear regression model in matrix form (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: The Linear Predictor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In words, the matrix formulation of the linear regression model is the product
    of two matrices *X* and *β* plus an error vector. The product of *X* and *β* is
    an *n* × 1 matrix called the *linear predictor,* which I’ll denote here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/99504be4d07eea0e14233832c9a8a358.png)'
  prefs: []
  type: TYPE_IMG
- en: The linear predictor vector (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: Now, matrix multiplication works a little differently than you might expect.
    I covered this in my primer on linear algebra — so if you haven’t checked it out
    you can find it [here](https://medium.com/towards-data-science/a-primer-on-linear-algebra-414111d195ca)
    — but I’ll quickly run through the details now.
  prefs: []
  type: TYPE_NORMAL
- en: If we have some *m* × *q* matrix *A* and another *q* × *r* matrix *B*, then
    the product is an *m* × *r* matrix *C* (note how the *q* dimension drops out from
    the result). The reason for this change in size is because the element located
    at row *i* and column *j* in *C* is the *dot product* of the *i*th row in *A*
    and the *j*th column in *B:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b65503f8cd63f53a462cbe47d08e9bb.png)'
  prefs: []
  type: TYPE_IMG
- en: The dot product (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: So, because the dot product takes the sum across the *q* dimension, this drops
    out from the resulting matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the simple linear regression case, the product is between an *n* × *p*
    matrix *X* and a *p* × 1 matrix *β,* which therefore results in an *n* × 1 matrix
    η. Following on from above, the (*i, j*) element in η is computed using the following
    dot product:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b4f0289122ae5c6f93db018f43db2b39.png)'
  prefs: []
  type: TYPE_IMG
- en: The linear predictor expanded (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: Here the sum is taken over *p*, which we know is the number of coefficients
    in the model, and so *p* = 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we then substitute the dot product into the linear predictor vector and
    substitute in the values from the first column of the design matrix, we get the
    following (because *j* = 1 I’m going to drop that subscript to declutter the notation
    a little):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ccf3987424c41599043fc8a7dc727f59.png)'
  prefs: []
  type: TYPE_IMG
- en: The linear predictor in full (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: 'All of this reduces to something very familiar indeed: each element in η is
    just our linear regression equation applied to each value of *X*! Hopefully, you
    can see why the column of ones is included in the design matrix. This ensures
    that the intercept is added to each observation.'
  prefs: []
  type: TYPE_NORMAL
- en: Model Errors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are three critical assumptions relating to the error terms — or *perturbations
    —* around the linear predictor that fall out of the *Gauss-Markov theorem*. The
    first is that the expected conditional mean of the errors is zero, implying that
    the average of the errors should not depend on any particular value of *X*. This
    is called the *zero conditional mean assumption:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/61bcb0ecb338b1a81a25c1190bd445df.png)'
  prefs: []
  type: TYPE_IMG
- en: The zero conditional mean assumption (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: 'Related to this is the *homoscedasticity* *assumption* which states that the
    variance of the errors should not be affected by the values of the independent
    variables. That is, the distribution of the errors should be entirely independent
    of any information contained within the design matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0ccadfdd56196652ea6342870ed3a276.png)'
  prefs: []
  type: TYPE_IMG
- en: The *homoscedasticity* assumption (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: 'The final assumption is the *no autocorrelation assumption* which requires
    that error terms are uncorrelated. This implies that knowledge about one error
    term does not confer any information about another and therefore does not co-vary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d035a65aa2873c1e03ca5b81515e4f96.png)'
  prefs: []
  type: TYPE_IMG
- en: No autocorrelation assumption (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: 'Under these assumptions, the covariance matrix for the error terms is a scalar
    matrix and the errors are considered to be *spherical*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b66335a14f27c56bcdaf148a2b4ed269.png)'
  prefs: []
  type: TYPE_IMG
- en: The covariance matrix under the Gauss-Markov assumptions (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: Just a note before moving on, while I adopted the convention of normally distributed
    error, the Gauss-Markov theorem does not require the error terms to be normal,
    nor does it require that errors are independent and identically distributed; only
    that the error terms are homoscedastic and uncorrelated. This implies that a variable
    *can* have dependencies, but so long as those dependencies are not *linear* —
    which is what correlation measures — then parameter estimation can proceed safely.
  prefs: []
  type: TYPE_NORMAL
- en: Parameter Estimation via Ordinary Least Squares
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When fitting a linear regression model to data the goal is to estimate the unknown
    model coefficients contained in *β*.The usual approach to finding the best values
    for these unknowns is to satisfy the least squares criterion, the goal of which
    is to minimize the total squared error between the linear predictor and the response.
    I’m going to step through how this is implemented for the simple linear regression
    model, though will move through this section fairly quickly.
  prefs: []
  type: TYPE_NORMAL
- en: 'In matrix form, the vector of errors, or *residuals*, is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ceb410bad892b2979fe89861fe91ec98.png)'
  prefs: []
  type: TYPE_IMG
- en: Model error, or residuals (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: 'where the hat on top of *β* denotes the estimated coefficients. The sum of
    the squared residuals can then be written as the dot product of the error vector
    with itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ec9eb1ab0d24bb7af6be47ff78ab0f82.png)'
  prefs: []
  type: TYPE_IMG
- en: The sum squared error (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: 'where *T* denotes the transpose operator¹. To derive the least squares criterion
    it’s convenient to write out the errors in full as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fcfe51eefc98430d52f0d8b096757fdd.png)'
  prefs: []
  type: TYPE_IMG
- en: Expanding the dot product of the error vector (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: The idea, then, is to find the parameters that minimize this value. To do so,
    we need to take the derivative of the above with respect to the vector *β* and
    set this to zero*:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc22aa5e4258bdb72358dd656cd5336f.png)'
  prefs: []
  type: TYPE_IMG
- en: The first derivative of the sum of squared error (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: 'from which the *normal equation* can be derived:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b1d44c78834574b4e7515cae3829549b.png)'
  prefs: []
  type: TYPE_IMG
- en: The normal equation (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: From here, we can find a solution for our unknown parameters by multiplying
    each side of the normal equation by the inverse of *X*ᵀ*X.* I cover matrix inversion
    in this [post](https://medium.com/p/eba53c564a90/edit), though if you haven’t
    read that, a matrix *A* is invertible if there exists a matrix *B* such that their
    product returns the identity matrix, *I*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a simple linear regression model, *X*ᵀ*X* is square with a size of 2 ×
    2, though more generally the matrix will be a *p* × *p* matrix*.* We then need
    to find another 2 × 2 matrix that is the multiplicative inverse of *X*ᵀ*X*. If
    such a matrix does not exist then the equation cannot be solved, though if *X*ᵀ*X*
    is indeed invertible, then we can obtain the parameter vector **b** as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a5feb27ce5e6986edffd826732751bc4.png)'
  prefs: []
  type: TYPE_IMG
- en: The estimation equation for linear regression (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: Here you can see why it is necessary that the design matrix is not rank deficient.
    If there are indeed linear dependencies between columns then *X*ᵀ*X* cannot be
    inverted and a unique solution cannot be found². This is particularly true if
    you have *perfect multicollinearity*.
  prefs: []
  type: TYPE_NORMAL
- en: A Closer Look at Parameter Estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is interesting to further consider the elements contained in each matrix.
    First, let's look at the cross-product of the design matrix *X*ᵀ*X:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b1682048cb8a51d79ef59ef220e99985.png)'
  prefs: []
  type: TYPE_IMG
- en: Taking the cross product of the design matrix (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: 'From here we can see that the matrix contains products of each column in the
    design matrix. But what we need is the inverse of this matrix. I won’t go through
    how to derive the inverse, but it looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/936f0f5ae04f7aeb3071c1cb3b4ece9c.png)'
  prefs: []
  type: TYPE_IMG
- en: The inverse of the design matrix cross product (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we also need the cross-product of the design matrix and the response
    vector Y, which produces the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/694d2c7c07ee8921b843d9a6cd10f55d.png)'
  prefs: []
  type: TYPE_IMG
- en: The cross product of the design matrix with the response vector (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: 'With the matrices written out in full, we can substitute them into the estimation
    question and work through the calculation like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ba7df46fbb63754e8a9c3e6c2e7a2d5e.png)'
  prefs: []
  type: TYPE_IMG
- en: Derivation of the parameter vector (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: 'To be fair, there is a bit going on in that derivation, but it’s really the
    last line that is most interesting. What this all boils down to is something quite
    convenient; we can estimate the slope coefficient using the sample covariance
    and variance like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/34ab197a6dc7deb317ffcd1787e9a765.png)'
  prefs: []
  type: TYPE_IMG
- en: Estimate for the slope coefficient (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have that, we can use this estimate, along with the mean of *y* and
    *x*, to derive the estimate for the intercept:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/80d70d35f595c980c8a328469c442ee2.png)'
  prefs: []
  type: TYPE_IMG
- en: Estimate for the intercept coefficient (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: Fitted Values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have now used some linear algebra to find the best-fitting parameters for
    a simple linear regression model. Now that we have these in hand the next step
    is to see how well the fitted values correspond with the values contained in the
    response vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'All we need to obtain the fitted values is the design matrix along with the
    parameter vector **b**. Multiplying together, the fitted values are computed like
    so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0c48389a0ad0af3b6fde2494f1b54920.png)'
  prefs: []
  type: TYPE_IMG
- en: Vector of fitted values (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: Note that our fitted values have a hat placed on top of them to indicate that
    they’re estimated values. For this reason, an alternative way to express the fitted
    values is as a combination of the response vector and the *hat matrix,* which
    gets its name from the fact that it puts a hat on *Y*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see this, let’s write out the equation for the fitted values by substituting
    **b** with the full equation for the parameter vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8630753e77222a816dcd02bd2bf1e099.png)'
  prefs: []
  type: TYPE_IMG
- en: Substituting in expanded version of parameter vector (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: 'Basically, every term that involves the design matrix *X* is lumped together
    to form the definition of the hat matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1491dcb685d8e42ca3c91af3b14d9a30.png)'
  prefs: []
  type: TYPE_IMG
- en: The ‘hat’ matrix (image by author).
  prefs: []
  type: TYPE_NORMAL
- en: Final Remarks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you’ve made it to this point, thanks for sticking at it. There was a lot
    to unpack, for sure. However, I hope you can see how the principles of matrix
    algebra are used to build simple linear regression models. Now, while I have focussed
    on simple linear regression throughout to keep the examples concise, everything
    discussed here works the same way for multiple regression, too. All that changes
    is that the dimensionality of the matrices and vectors increases.
  prefs: []
  type: TYPE_NORMAL
- en: Footnotes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] The notation I have used here is different from what I used in my primer
    article, but they mean exactly the same thing. The reason for the change is to
    keep in step with how the matrix formulations are typically described.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Strictly speaking, singular matrices *can* be inverted. The *generalised
    inverse* extends the idea of the inverse and can be applied to a broader class
    of matrices. The generalised inverse is particularly useful in finding least squares
    solutions in cases where no unique solution exists.'
  prefs: []
  type: TYPE_NORMAL
- en: Related Articles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[A Primer on Linear Algebra](https://medium.com/towards-data-science/a-primer-on-linear-algebra-414111d195ca)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Linear Digressions](https://medium.com/@dataforyou/linear-digressions-1427951c0257)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Multicollinearity: Problem, or Not?](https://medium.com/towards-data-science/multicollinearity-problem-or-not-d4bd7a9cfb91)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thanks for reading!
  prefs: []
  type: TYPE_NORMAL
- en: If you enjoyed this post and would like to stay up to date then please consider
    [following me on Medium.](https://medium.com/@dataforyou) This will ensure you
    don’t miss out on any new content.
  prefs: []
  type: TYPE_NORMAL
- en: To get unlimited access to all content consider signing up for a [Medium subscription](https://medium.com/membership).
  prefs: []
  type: TYPE_NORMAL
- en: You can also follow me on [Twitter](https://twitter.com/dataforyounz), [LinkedIn](https://www.linkedin.com/in/dataforyou/),
    or check out my [GitHub](https://github.com/dataforyounz) if that’s more your
    thing.
  prefs: []
  type: TYPE_NORMAL
