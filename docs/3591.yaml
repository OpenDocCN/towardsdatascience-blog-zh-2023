- en: 'DL Notes: Advanced Gradient Descent'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DL 笔记：高级梯度下降
- en: 原文：[https://towardsdatascience.com/dl-notes-advanced-gradient-descent-4407d84c2515?source=collection_archive---------7-----------------------#2023-12-05](https://towardsdatascience.com/dl-notes-advanced-gradient-descent-4407d84c2515?source=collection_archive---------7-----------------------#2023-12-05)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/dl-notes-advanced-gradient-descent-4407d84c2515?source=collection_archive---------7-----------------------#2023-12-05](https://towardsdatascience.com/dl-notes-advanced-gradient-descent-4407d84c2515?source=collection_archive---------7-----------------------#2023-12-05)
- en: The main optimization algorithms used for training neural networks, explained
    and implemented from scratch in Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主要的优化算法用于训练神经网络，从头开始在 Python 中解释和实现。
- en: '[](https://medium.com/@luisdamed?source=post_page-----4407d84c2515--------------------------------)[![Luis
    Medina](../Images/d83d326290ae3272f0618d0bd28bd875.png)](https://medium.com/@luisdamed?source=post_page-----4407d84c2515--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4407d84c2515--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4407d84c2515--------------------------------)
    [Luis Medina](https://medium.com/@luisdamed?source=post_page-----4407d84c2515--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[![Luis Medina](../Images/d83d326290ae3272f0618d0bd28bd875.png)](https://medium.com/@luisdamed?source=post_page-----4407d84c2515--------------------------------)
    [![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4407d84c2515--------------------------------)
    [Luis Medina](https://medium.com/@luisdamed?source=post_page-----4407d84c2515--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F562a027a34f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdl-notes-advanced-gradient-descent-4407d84c2515&user=Luis+Medina&userId=562a027a34f0&source=post_page-562a027a34f0----4407d84c2515---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4407d84c2515--------------------------------)
    ·17 min read·Dec 5, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4407d84c2515&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdl-notes-advanced-gradient-descent-4407d84c2515&user=Luis+Medina&userId=562a027a34f0&source=-----4407d84c2515---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F562a027a34f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdl-notes-advanced-gradient-descent-4407d84c2515&user=Luis+Medina&userId=562a027a34f0&source=post_page-562a027a34f0----4407d84c2515---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4407d84c2515--------------------------------)
    · 17 分钟阅读 · 2023年12月5日'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4407d84c2515&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdl-notes-advanced-gradient-descent-4407d84c2515&source=-----4407d84c2515---------------------bookmark_footer-----------)![](../Images/5de63561379eb2ad31ce53a3d33a8cbb.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/5de63561379eb2ad31ce53a3d33a8cbb.png)'
- en: Photo by [Jack Anstey](https://unsplash.com/@jack_anstey?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit)
    / [Unsplash](https://unsplash.com/?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Jack Anstey](https://unsplash.com/@jack_anstey?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit)
    / [Unsplash](https://unsplash.com/?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit)
- en: In my [previous article about gradient descent](https://medium.com/@luisdamed/gradient-descent-f09f19eb35fb),
    I explained the basic concepts behind it and summarized the main challenges of
    this kind of optimization.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在我[之前关于梯度下降的文章](https://medium.com/@luisdamed/gradient-descent-f09f19eb35fb)中，我解释了其基本概念，并总结了这种优化方法的主要挑战。
- en: However, I only covered Stochastic Gradient Descent (SGD) and the “batch” and
    “mini-batch” implementation of gradient descent.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我只涉及了随机梯度下降（SGD）以及“批处理”和“小批量”梯度下降的实现。
- en: Other algorithms offer advantages in terms of convergence speed, robustness
    to “landscape” features (the vanishing gradient problem), and less dependence
    on the choice of learning rate to achieve good performance.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 其他算法在收敛速度、对“景观”特征（梯度消失问题）的鲁棒性，以及对学习率选择的依赖程度等方面提供了优势。
- en: So today I’ll write about more advanced optimization algorithms, implementing
    them from scratch in Python and comparing them through animated visualizations.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 所以今天我将讨论更高级的优化算法，从零开始在 Python 中实现它们，并通过动画可视化进行比较。
- en: I’ve also listed the resources I used for learning about these algorithms. They
    are great for diving deeper into formal concepts.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我还列出了用于学习这些算法的资源。这些资源非常适合深入探讨正式的概念。
- en: Comparing the algorithms using a simple objective function
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较使用简单目标函数的算法
- en: '![](../Images/214af3276a66868c0dd1d65362456436.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/214af3276a66868c0dd1d65362456436.png)'
- en: Throughout this article, I’ll show how I implemented the different algorithms
    in Python.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将展示如何在 Python 中实现不同的算法。
- en: I’ve created a Jupyter Notebook that you can [**access on GitHub**](https://github.com/luisdamed/Gradient_Descent_Visualizations/blob/main/Advanced_Gradient_Descent_Trajectories.ipynb?ref=makerluis.com)
    or directly [**on Google Colab**](https://colab.research.google.com/github/luisdamed/Gradient_Descent_Visualizations/blob/main/Advanced_Gradient_Descent_Trajectories.ipynb?ref=makerluis.com)to
    see all the code used to create the figures shown here.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我创建了一个 Jupyter Notebook，你可以通过[**GitHub 访问**](https://github.com/luisdamed/Gradient_Descent_Visualizations/blob/main/Advanced_Gradient_Descent_Trajectories.ipynb?ref=makerluis.com)或直接[**在
    Google Colab 上查看**](https://colab.research.google.com/github/luisdamed/Gradient_Descent_Visualizations/blob/main/Advanced_Gradient_Descent_Trajectories.ipynb?ref=makerluis.com)以查看用于创建此处展示的图形的所有代码。
- en: To generate the animations, I used the approach shown in my previous post about
    [creating an animated gradient descent figure in Python](/creating-a-gradient-descent-animation-in-python-3c4dcd20ca51?ref=makerluis.com).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为生成动画，我使用了我之前的文章中展示的[在 Python 中创建动画梯度下降图形的方法](/creating-a-gradient-descent-animation-in-python-3c4dcd20ca51?ref=makerluis.com)。
- en: The function definitions assume that the following code has been already included,
    as they use `numpy` classes and methods and call both the function `f` and its
    gradient, `grad_f`.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 函数定义假设已经包含了以下代码，因为它们使用了`numpy`类和方法，并调用了函数`f`及其梯度`grad_f`。
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Momentum
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动量
- en: '![](../Images/c2cf61a22635cb112694929bb532b4b0.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2cf61a22635cb112694929bb532b4b0.png)'
- en: Photo by [Sharon Pittaway](https://unsplash.com/@sharonp?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Sharon Pittaway](https://unsplash.com/@sharonp?utm_source=medium&utm_medium=referral)提供，拍摄于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: We could compare the optimization algorithm with a ball rolling downhill.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将优化算法与球体滚下坡道进行比较。
- en: If the “ball” had momentum like it has in real life, it would be less likely
    to remain stuck in a local minimum after accelerating down the hill at full speed.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果“球体”像现实中那样具有动量，它在以全速加速下坡后更不容易停留在局部最小值中。
- en: That’s what people realized when dealing with the problem of gradient descent
    getting stuck in local minima.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是人们在处理梯度下降被困在局部最小值的问题时意识到的。
- en: 'From high school physics, we know that *translational* momentum is defined
    by the product of the mass of an object and its velocity:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 从高中物理中我们知道，*平动*动量定义为物体质量与其速度的乘积：
- en: '![](../Images/3d3ebc4b9ecd19513eee3a78b3f19a25.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d3ebc4b9ecd19513eee3a78b3f19a25.png)'
- en: Translational momentum.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 平动动量。
- en: 'We also know that the gravitational potential energy of an object of mass ***m***
    is *proportional* to the height ***h*** at which it is placed:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还知道，质量为***m***的物体的重力势能与其所处的高度***h***是*成正比*的：
- en: '![](../Images/0403422b98dca25d394344ebd5a5dd03.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0403422b98dca25d394344ebd5a5dd03.png)'
- en: Gravitational potential energy.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 重力势能。
- en: Moreover, there is a direct [relationship between the potential energy of an
    object and the force exerted on it](https://phys.libretexts.org/Under_Construction/Purgatory/2%3A_Applying_Models_to_Mechanical_Phenomena/2.5%3A_Force_and_Potential_Energy?ref=makerluis.com)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，物体的势能与施加在其上的力之间存在直接的[关系](https://phys.libretexts.org/Under_Construction/Purgatory/2%3A_Applying_Models_to_Mechanical_Phenomena/2.5%3A_Force_and_Potential_Energy?ref=makerluis.com)
- en: '![](../Images/eac99ef0f2de0d84a9b77378f042d61a.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eac99ef0f2de0d84a9b77378f042d61a.png)'
- en: Force is equal to the negative gradient of potential energy.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 力等于势能的负梯度。
- en: 'The relationship between **p** and ***U*** can be derived from Newton’s second
    law of motion:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**p**与***U***之间的关系可以从牛顿第二定律推导出来：'
- en: The change of motion of an object is proportional to the force impressed; and
    is made in the direction of the straight line in which the force is impressed.
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 物体的运动变化与施加的力成正比，并且沿着力的施加方向发生。
- en: '![](../Images/f58e180d6fece6b73243d282bba9189e.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f58e180d6fece6b73243d282bba9189e.png)'
- en: Newton’s second law of motion.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 牛顿第二定律。
- en: 💡 In reality, this physics analogy is too simplified to cover all the advantages
    and disadvantages of adding momentum to a gradient descent optimization. To get
    the whole picture, I recommend you to check [**Why Momentum Really Works?**](https://distill.pub/2017/momentum/?ref=makerluis.com)
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 💡 实际上，这个物理类比过于简化，无法涵盖将动量添加到梯度下降优化中的所有优缺点。要获取全面的信息，推荐查看[**为什么动量真的有效？**](https://distill.pub/2017/momentum/?ref=makerluis.com)
- en: How to add momentum?
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何添加动量？
- en: When we initialize the optimization algorithm, we place the “ball” at a height
    ***h***, giving it potential energy ***U***.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们初始化优化算法时，我们把“球”放在高度***h***处，赋予它势能***U***。
- en: The force exerted on the ball is proportional to the gradient of such potential
    energy. Like the gradient of the function we are optimizing (the surface over
    which we are moving).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 施加在球上的力与这种势能的梯度成正比，就像我们优化的函数的梯度一样（我们正在移动的表面）。
- en: The way momentum works for the optimization is by using the gradient to change
    the “velocity” of the particle, which in turn changes its position.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 动量在优化中的作用是利用梯度来改变粒子的“速度”，进而改变其位置。
- en: '![](../Images/eeacef283ed7c5f063589f3754773bb5.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eeacef283ed7c5f063589f3754773bb5.png)'
- en: Momentum update.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 动量更新。
- en: Because of the velocity term, the “particle” builds up speed in any direction
    that has a consistent gradient.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 由于速度项，“粒子”在具有一致梯度的任何方向上加速。
- en: 'I’ve implemented this as a Python function as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我将这实现为以下Python函数：
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The Momentum update both accelerates the optimization in directions of low curvature
    and smooths out (dampening effect) oscillations caused by “landscape” features
    or noisy data [3].
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 动量更新既加速了在低曲率方向上的优化，又平滑了（阻尼效应）由于“地形”特征或噪声数据造成的振荡[3]。
- en: Some argue that the Momentum update is in reality more consistent with the physical
    effect of the friction coefficient because it reduces the kinetic energy of the
    system [2].
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一些人认为动量更新实际上与摩擦系数的物理效果更一致，因为它减少了系统的动能[2]。
- en: Another way to interpret it is that it gives “short-term” memory to the optimization
    process.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种解释是，它为优化过程提供了“短期”记忆。
- en: 'Being smaller than 1, the momentum parameter acts like an exponentially weighted
    sum of the previous gradients, and the velocity update can be rewritten as [3][5]:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 由于小于1，动量参数像是之前梯度的指数加权和，速度更新可以重写为[3][5]：
- en: '![](../Images/da422a45d928db3a2feada48ebda9a0f.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da422a45d928db3a2feada48ebda9a0f.png)'
- en: Velocity term rewritten as a weighted sum.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 速度项被重写为加权和。
- en: where *g* is the instantaneous gradient, and *v* is the smoothed gradient estimator.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*g*是瞬时梯度，*v*是平滑的梯度估计器。
- en: The parameter **β** controls how much weight we give to new values of the instantaneous
    gradient over the previous ones.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 参数**β**控制我们对当前梯度与之前梯度之间新值的权重分配。
- en: It is usually equal to 0.9, but sometimes it is “scheduled”, i.e. increased
    from as low as 0.5 up to 0.99 as the iterations progress.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 通常它的值为0.9，但有时会“调度”，即在迭代过程中从0.5逐步增加到0.99。
- en: Nesterov’s Accelerated Gradient (NAG)
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Nesterov加速梯度（NAG）
- en: Proposed by Nesterov, Y. in 1983.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 由Nesterov于1983年提出。
- en: The Nesterov update implements a “lookahead” feature to improve the stability
    and convergence speed of Momentum for convex functions.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Nesterov更新实现了“前瞻”功能，以提高动量在凸函数上的稳定性和收敛速度。
- en: '![](../Images/5499819332220dbcf8777354761a3983.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5499819332220dbcf8777354761a3983.png)'
- en: NAG update.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: NAG更新。
- en: While Momentum uses the current position to update the gradient, NAG performs
    a partial update of the current position first, knowing that
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 动量使用当前位置来更新梯度，而NAG首先对当前位置进行部分更新，知道
- en: '![](../Images/c239638fff8b6cd73262d88f0b93ba0a.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c239638fff8b6cd73262d88f0b93ba0a.png)'
- en: Intuition for lookahead update.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对前瞻性更新的直觉。
- en: '![](../Images/15c24caf53126189dff5fd3fbae2b2c9.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15c24caf53126189dff5fd3fbae2b2c9.png)'
- en: Vector representation of Momentum and NAG updates.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 动量和NAG更新的矢量表示。
- en: 'To implement this as a Python function, I just made the following modification
    to the “velocity update” in the code I showed before:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将其实现为Python函数，我对之前展示的“速度更新”代码进行了以下修改：
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This partial update helps improve the accuracy of the optimization. In practical
    terms, it translates into fewer oscillations around a local minimum when compared
    to Momentum.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分更新有助于提高优化的准确性。实际上，这意味着与动量法相比，它在局部最小值附近的振荡更少。
- en: The difference is evident in the next figure.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 差异在下图中非常明显。
- en: '![](../Images/06e151df4c4996aca7b296f530e03d3c.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06e151df4c4996aca7b296f530e03d3c.png)'
- en: Comparing Momentum and NAG descent optimizations for a complex surface.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 比较动量法和NAG在复杂表面上的下降优化。
- en: Both optimizers are initialized at the same coordinates, and using the same
    momentum parameter (0.95, fixed).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 两个优化器在相同坐标上初始化，并使用相同的动量参数（0.95，固定）。
- en: The following animation also helps us see the intuition behind scheduling or
    “annealing” the momentum parameter.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 以下动画也帮助我们理解调度或“退火”动量参数的直观感受。
- en: '![](../Images/659217b9fdbdccb3a83429be0bb9cbf8.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/659217b9fdbdccb3a83429be0bb9cbf8.png)'
- en: Comparison of different optimization algorithms getting past a vanishing gradient
    region. Momentum-based methods perform better in this case.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 比较不同优化算法在穿越梯度消失区域时的表现。在这种情况下，基于动量的方法表现更好。
- en: In the beginning, a small amount of momentum can be beneficial to get past the
    vanishing gradient. When we approach the local minimum, having larger momentum
    values could dampen out the oscillations we see, improving the convergence speed.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一开始，少量动量有助于穿越梯度消失区域。当我们接近局部最小值时，较大的动量值可能会减小我们观察到的振荡，从而提高收敛速度。
- en: Adaptive methods
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自适应方法
- en: The other optimization algorithms shown in the animation above are adaptive
    methods, which I’ll describe in this section.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 上面动画中展示的其他优化算法是自适应方法，我将在本节中描述这些方法。
- en: Looking at this simple example with a local and a global minimum, Momentum and
    NAG may seem far superior to the other methods. However, adaptive algorithms are
    more robust. I’ll show this with a practical example in another article.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个简单的例子，动量法和NAG似乎比其他方法优越。然而，自适应算法更具鲁棒性。我将在另一篇文章中通过实际例子来展示这一点。
- en: Adaptive Gradient Algorithm (AdaGrad)
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自适应梯度算法（AdaGrad）
- en: AdaGrad is a family of [subgradient](https://web.stanford.edu/class/ee392o/subgrad_method.pdf?ref=makerluis.com)
    algorithms for stochastic optimization, presented by [John Duchi, Elad Hazan and
    Yoram Singer in 2011.](https://jmlr.org/papers/v12/duchi11a.html?ref=makerluis.com)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: AdaGrad 是一类用于随机优化的[次梯度](https://web.stanford.edu/class/ee392o/subgrad_method.pdf?ref=makerluis.com)算法，由[John
    Duchi、Elad Hazan 和 Yoram Singer于2011年](https://jmlr.org/papers/v12/duchi11a.html?ref=makerluis.com)提出。
- en: They proposed to improve gradient-based learning by incorporating the history
    of the gradients into each new update of the weights.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 他们提出通过将梯度的历史信息纳入每次新的权重更新中来改进基于梯度的学习。
- en: Instead of biasing the gradient itself, as momentum does, AdaGrad modifies the
    learning rate dynamically and separately for each parameter of the objective function.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 与动量法通过偏置梯度本身不同，AdaGrad 动态地为目标函数的每个参数单独修改学习率。
- en: This means we have different learning rates for each model weight. They are
    adjusted based on the consistency of the gradients.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们对每个模型权重使用不同的学习率。这些学习率基于梯度的一致性进行调整。
- en: 'To do this, the sequence of gradient estimates is stored as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，梯度估计的序列如下存储：
- en: '![](../Images/018b04875576d906b9444d027b4adc3f.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/018b04875576d906b9444d027b4adc3f.png)'
- en: Sum of squared gradients or outer product of gradient history.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度的平方和或梯度历史的外积。
- en: If we are optimizing a function with *n* coordinates or parameters, g**ₜ** will
    be a vector with *n* elements, and so will G**ₜ**.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们优化一个具有*n*个坐标或参数的函数，g**ₜ**将是一个具有*n*个元素的向量，G**ₜ**也是如此。
- en: 'Then, the update rule is given by:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，更新规则如下：
- en: '![](../Images/3bc65abe1897974a0676b8470522d9b0.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3bc65abe1897974a0676b8470522d9b0.png)'
- en: AdaGrad update.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: AdaGrad 更新。
- en: The parameter **ε** is used to avoid a division by zero and is usually set to
    a small value, like 1e-08.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 参数**ε**用于避免除零错误，通常设置为一个小值，如1e-08。
- en: Interestingly, the definition of G**ₜ** is similar to the un-centered (zero-mean)
    [variance](https://en.wikipedia.org/wiki/Variance?ref=makerluis.com) of the distribution
    of gradients.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，G**ₜ**的定义类似于梯度分布的非中心（零均值）[方差](https://en.wikipedia.org/wiki/Variance?ref=makerluis.com)。
- en: '![](../Images/77d7e86480587b0db5f7d8998e28bfd8.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77d7e86480587b0db5f7d8998e28bfd8.png)'
- en: Variance definition.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 方差定义。
- en: The variance is a measure of the dispersion energy of a distribution.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 方差是分布的离散能量的度量。
- en: Hence, for each parameter **θᵢ**, the learning rate is adapted in proportion
    to the *inverse* of the gradient’s variance for **θᵢ**.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于每个参数**θᵢ**，学习率是根据**θᵢ**的梯度方差的*倒数*来调整的。
- en: Considering this, we could say that parameters with more dispersion in their
    gradient distribution will scale down the learning rate by a larger factor, while
    those with more consistent gradients (lower variance) will have larger learning
    rates.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，我们可以说，梯度分布中离散程度较大的参数将按较大的比例缩小学习率，而梯度较一致（方差较低）的参数将具有较大的学习率。
- en: AdaGrad also implements learning rate decay automatically, based on the time
    (accumulation of previous gradients) and the curvature of the objective function
    (“areas” with lower gradient variance will be assigned smaller step sizes). This
    improves the convergence rate of the algorithm.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: AdaGrad还根据时间（之前梯度的累积）和目标函数的曲率（“区域”中的梯度方差较低将分配较小的步长）自动实现学习率衰减。这改善了算法的收敛速度。
- en: 'I’ve implemented AdaGrad as a Python function as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我已将AdaGrad实现为以下Python函数：
- en: '[PRE3]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: One drawback of AdaGrad is that this learning rate decay during training may
    be too aggressive, causing the learning to stop early when training ANNs. Each
    parameter update is robust, but the rate at which the changes move toward the
    optimal point can decrease too much.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: AdaGrad的一个缺点是，这种训练过程中学习率的衰减可能过于激进，导致在训练人工神经网络时学习过早停止。每次参数更新都很稳健，但变化接近最优点的速度可能会下降得太多。
- en: Another drawback is that, although the learning rates are self-adjusted during
    learning, AdaGrad can still be sensitive to the initial conditions. If the gradients
    are large at the start of the optimization, the learning rates will be low for
    the rest of the training.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个缺点是，虽然学习率在学习过程中会自我调整，但AdaGrad仍然对初始条件敏感。如果优化开始时梯度很大，那么训练过程中学习率会较低。
- en: We can see this in the animated figure. AdaGrad breaks off from the symmetry
    quickly, but the learning is very slow, compared to other algorithms.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在动画图中看到这一点。AdaGrad很快打破了对称性，但学习非常慢，与其他算法相比。
- en: To compensate for this, one may need to tune the learning rate to a higher value,
    which in part defeats the purpose of the self-adjusting characteristic.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弥补这一点，可能需要将学习率调整到更高的值，这在一定程度上削弱了自我调整特性的目的。
- en: Root Mean Square Propagation (RMSprop)
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 均方根传播（RMSprop）
- en: Unpublished method, but mentioned in the [slides for lecture 6](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf?ref=makerluis.com)
    of the course Neural Networks for Machine Learning, from Prof. [Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton?ref=makerluis.com).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 未发表的方法，但在课程《神经网络与机器学习》第6讲的[幻灯片](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf?ref=makerluis.com)中提到，由[Geoffrey
    Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton?ref=makerluis.com)教授提供。
- en: The concept of this algorithm is similar to momentum. It also incorporates the
    short-term history of the magnitude of the gradients to perform the weights update.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法的概念类似于动量。它还结合了梯度幅度的短期历史来执行权重更新。
- en: However, similarly to AdaGrad, RMSProp modifies the learning rate and not the
    gradient.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与AdaGrad类似，RMSProp修改的是学习率而不是梯度。
- en: To do this, the learning rate is divided by a running average of the magnitudes
    of recent gradients.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，学习率被除以最近梯度幅度的滑动平均。
- en: 'First, the algorithm computes a weighted sum of the squared cost values and
    the previous ones:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，算法计算平方成本值及其之前值的加权和。
- en: '![](../Images/701f7cc431162487e3d89e088ba1bfc1.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/701f7cc431162487e3d89e088ba1bfc1.png)'
- en: Exponentially weighted sum of squared costs.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 平方成本的指数加权和。
- en: This is like a short-term mean, where the parameter **β** adjusts how much weight
    is given to more recent cost values over older ones.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像是一个短期均值，其中参数**β**调整了给更近期的成本值相较于较旧的成本值的权重。
- en: It is analog to the re-written form of momentum that I mentioned before but
    applied to the squared costs, instead of the gradient.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 它类似于我之前提到的动量的重写形式，但应用于平方成本，而不是梯度。
- en: The next step is dividing the learning rate by the square root of this moving
    average.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将学习率除以这个移动平均的平方根。
- en: '![](../Images/6d74a625b221b9b4e476c5fb201a4368.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d74a625b221b9b4e476c5fb201a4368.png)'
- en: RMSProp update rule.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: RMSProp 更新规则。
- en: This way, the step size depends on the history of the gradient magnitudes (short-term
    memory).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，步长依赖于梯度幅度的历史（短期记忆）。
- en: Notice that computing the root of a weighted sum (or weighted average) of squared
    values is equivalent to computing the Root Mean Square (RMS) of those values.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，计算加权平方和的平方根（或加权平均）等同于计算这些值的均方根（RMS）。
- en: '![](../Images/752ab5645f800051560cda31b3e4e5da.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/752ab5645f800051560cda31b3e4e5da.png)'
- en: Definition of RMS.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: RMS 的定义。
- en: The RMS of a signal is a representation of its *total* energy (as opposed to
    the variance, which represents its *dispersion* energy)[1].
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 信号的 RMS 是其*总*能量的表示（与方差不同，方差表示的是*离散*能量）[1]。
- en: Therefore, with RMSProp, the learning rate is modulated according to the total
    energy of the gradient of the cost function and its previous values. This adjustment
    is done dynamically and for each direction or component of our loss function (each
    weight!).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用 RMSProp 时，学习率会根据成本函数的梯度及其先前值的总能量进行调节。这种调整是动态的，并且针对损失函数的每个方向或组件（每个权重！）。
- en: The goal is to reduce the volatility caused by large changes in the gradient
    by reducing the step size in those cases.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是通过在这些情况下减小步长来减少由于梯度大幅变化引起的波动性。
- en: This also helps with vanishing gradient problems because when there’s a trend
    of very small gradients we take larger steps.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这也有助于解决梯度消失问题，因为当梯度非常小时，我们会采取更大的步伐。
- en: 'This is how I coded it as a Python function:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我作为 Python 函数编码的方式：
- en: '[PRE4]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: RMSprop is robust to the initial choice of learning rate, and it also implements
    an automatic learning rate decay. However, since it is based on a short-term history
    of gradient values, the decay is less aggressive than AdaGrad.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: RMSprop 对学习率的初始选择非常稳健，并且还实现了自动学习率衰减。然而，由于它基于梯度值的短期历史，因此衰减比 AdaGrad 更加温和。
- en: '![](../Images/0c6c1edca99d72c5a323f8c2e692bcce.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c6c1edca99d72c5a323f8c2e692bcce.png)'
- en: Photo by [Gonzalo Kaplanski](https://unsplash.com/@gonzakap?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Gonzalo Kaplanski](https://unsplash.com/@gonzakap?utm_source=medium&utm_medium=referral)
    拍摄，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: AdaDelta
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AdaDelta
- en: Proposed by [Matthew Zeiler in 2012](https://arxiv.org/abs/1212.5701?ref=makerluis.com).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 由 [Matthew Zeiler 于 2012 年提出](https://arxiv.org/abs/1212.5701?ref=makerluis.com)。
- en: 'This method was developed to overcome the main limitations of AdaGrad: the
    continuous decay of the learning rate that causes early stopping and the need
    to tune the “global” learning rate manually.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法是为了克服 AdaGrad 的主要局限性而开发的：学习率的持续衰减导致提前停止，并且需要手动调整“全局”学习率。
- en: To overcome the continuous learning rate decay, the algorithm accumulates the
    history of past gradients over a window or fixed size.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服持续的学习率衰减，算法在一个窗口或固定大小内累积过去梯度的历史。
- en: 'In practice, this involves dividing the learning rate by the RMS of previous
    gradients over a window of fixed size, just like RMSprop does:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这涉及到将学习率除以固定窗口内先前梯度的 RMS，就像 RMSprop 所做的那样：
- en: '![](../Images/e3e257fde445b1c8167cd06aa71c2ba6.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e3e257fde445b1c8167cd06aa71c2ba6.png)'
- en: Learning rate scaling similar to RMSProp.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率缩放类似于 RMSProp。
- en: The next modification from AdaGrad is the correction of the units of the optimization
    updates.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: AdaGrad 的下一个改进是优化更新单元的修正。
- en: 'In AdaGrad (and all other optimization algorithms I’ve described so far), the
    units of the optimization steps don’t match the units of the parameters that we
    modify to optimize the cost function [9]:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AdaGrad（以及我迄今为止描述的所有其他优化算法）中，优化步骤的单位与我们为优化成本函数而修改的参数单位不匹配 [9]：
- en: '![](../Images/bfb23bf151501ad39ebfd3a3e9704540.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bfb23bf151501ad39ebfd3a3e9704540.png)'
- en: We know from school that we can’t add apples and oranges. But with these optimization
    algorithms, it’s like we’ve been adding “apples” (current parameter values, **θₜ**
    and some unknown quantity (the optimization step **Δθ** ) that mathematically
    can be added to them to obtain new apples (the updated parameters, **θₜ ₊₁**.
    It just works, but doesn’t make sense in real life.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在学校里知道不能将苹果和橙子相加。但使用这些优化算法时，就像我们在加“苹果”（当前参数值，**θₜ** 和一些未知量（优化步骤 **Δθ**），这些在数学上可以加到一起以获得新的苹果（更新后的参数，**θₜ
    ₊₁**）。这有效，但在现实生活中没有意义。
- en: 'Zeiler decided to correct the units, rearranging the update term from [Newton’s
    method](https://en.wikipedia.org/wiki/Newton%27s_method?ref=makerluis.com) and
    assuming the curvature of the loss function could be approximated by a diagonal
    [Hessian matrix](https://en.wikipedia.org/wiki/Hessian_matrix?ref=makerluis.com):'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Zeiler 决定纠正单位，将更新项从[牛顿法](https://en.wikipedia.org/wiki/Newton%27s_method?ref=makerluis.com)重新排列，并假设损失函数的曲率可以用对角[海森矩阵](https://en.wikipedia.org/wiki/Hessian_matrix?ref=makerluis.com)来近似：
- en: '![](../Images/d4cb70742f35ebddcf11f40b98d708ac.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d4cb70742f35ebddcf11f40b98d708ac.png)'
- en: Comparing this observation with the update rule similar to RMSProp, Zeiler determined
    the correct form of the update term to preserve the right units.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 将这一观察与类似于 RMSProp 的更新规则进行比较，Zeiler 确定了正确的更新项形式，以保持正确的单位。
- en: 'The intuition is better explained in the original publication but in practice,
    it resulted in adding the square root of an exponentially-weighted average of
    the previous update values to the numerator of the update term:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 直观的解释在原始出版物中更为详尽，但实际上，它导致在更新项的分子中添加了以前更新值的指数加权平均的平方根：
- en: '![](../Images/5d7d73b0fd9e37e9727fa810762f744a.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d7d73b0fd9e37e9727fa810762f744a.png)'
- en: AdaDelta step for parameter update.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: AdaDelta 参数更新步骤。
- en: This is basically assuming that the loss function is smooth (low curvature)
    within a small window of size *w*, so that **Δθₜ** can be approximated by the
    exponential RMS of the previous values.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上假设损失函数在小窗口大小 *w* 内是平滑的（曲率低），以便**Δθₜ** 可以通过以前值的指数 RMS 来近似。
- en: 'The algorithm looks like this if we implement it as a Python function:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将其实现为 Python 函数，算法看起来是这样的：
- en: '[PRE5]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: AdaDelta combines the advantages of the optimization methods it builds upon.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: AdaDelta 结合了其所基于的优化方法的优点。
- en: For instance, the short-term memory of previous parameter updates in the numerator
    is similar to Momentum and has the effect of accelerating the gradient descent.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，分子中前期参数更新的短期记忆类似于动量，并具有加速梯度下降的效果。
- en: The denominator provides the per-dimension accuracy of AdaGrad but without the
    excessive learning rate decay (just like RMSProp).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 分母提供了 AdaGrad 的每维度准确性，但没有过度的学习率衰减（就像 RMSProp 一样）。
- en: Additionally, AdaDelta is more robust to sudden gradient changes, and it is
    robust to the choice of initial learning rate (see a practical example in the
    last section of this article).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，AdaDelta 对突发的梯度变化更为稳健，对初始学习率的选择也很稳健（请参见本文最后一节中的实际示例）。
- en: Adam (Adaptive Momentum)
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Adam（自适应动量）
- en: This is one of the most popular algorithms today.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这是今天最流行的算法之一。
- en: It was introduced by [Diederik P. Kingma and Jimmy Lei Ba in 2014](https://arxiv.org/pdf/1412.6980.pdf?ref=makerluis.com),
    and has become very popular because of its computational efficiency and because
    it works very well for problems with large amounts of data and parameters.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 它由[迪德里克·P·金马和吉米·雷·巴于 2014 年提出](https://arxiv.org/pdf/1412.6980.pdf?ref=makerluis.com)，并因其计算效率高以及在处理大量数据和参数的问题上表现良好而变得非常流行。
- en: Adam is like a combination of Momentum and RMSprop because it dynamically changes
    both the gradient of the loss function and the learning rates used to scale such
    gradient to update the weights.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Adam 类似于动量（Momentum）和 RMSprop 的结合，因为它动态地改变了损失函数的梯度以及用于缩放这些梯度的学习率来更新权重。
- en: To do this, the algorithm includes the computation of two terms that will be
    familiar from previous sections of this article.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，算法包括计算两个在本文之前部分已经熟悉的术语。
- en: 'First, there’s a term from momentum, an exponentially weighted sum of the previous
    gradients of the cost function (this is like a weighted variance):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，动量项是成本函数前几个梯度的指数加权和（这类似于加权方差）：
- en: '![](../Images/4fcf8fc07a1a6b7e064c47aa116ae596.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4fcf8fc07a1a6b7e064c47aa116ae596.png)'
- en: Exponentially weighted average of cost gradients.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 成本梯度的指数加权平均。
- en: Then, there’s a term from RMSprop, an exponentially weighted moving average
    of the squared gradients.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，有一个来自 RMSprop 的术语，是平方梯度的指数加权移动平均。
- en: '![](../Images/fb2aaf1105d8aebc521a41d67f9b5ed9.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fb2aaf1105d8aebc521a41d67f9b5ed9.png)'
- en: Exponentially weighted average of squared cost gradients.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 成本梯度的指数加权平均。
- en: Combining both terms with the SGD algorithm, the information from past gradients
    is included in the update step. Their total energy over a short window (RMS) is
    used to scale the learning rate, and their dispersion (variance) helps adjust
    the current gradient value used for updating the weights.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 将两者与 SGD 算法结合，过去梯度的信息被纳入更新步骤。它们在短窗口内的总能量（RMS）用于缩放学习率，而它们的离散度（方差）有助于调整用于更新权重的当前梯度值。
- en: '![](../Images/e1ae4b44a6a1e7fb27694f23883149dc.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1ae4b44a6a1e7fb27694f23883149dc.png)'
- en: Adam’s update rule.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: Adam 的更新规则。
- en: 'The values with tilde (~) correspond to bias correction terms that are introduced
    to reduce the contribution from the initial values of m and v as learning progresses:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 带有波浪号 (~) 的值对应于引入的偏差校正项，以减少学习过程中 m 和 v 初始值的贡献：
- en: '![](../Images/d9d3e694982e67f5eb6551697682715d.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d9d3e694982e67f5eb6551697682715d.png)'
- en: Initialization bias correction terms for Adam.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Adam 的初始化偏差校正项。
- en: t = current training epoch.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: t = 当前训练轮次。
- en: Unlike AdaDelta, Adam does require tuning of some hyperparameters, but they
    are easy to interpret.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 与 AdaDelta 不同，Adam 确实需要调整一些超参数，但这些参数很容易解释。
- en: The terms **β₁** and **β₂** are the decay rates of the [exponential moving averages](https://en.wikipedia.org/wiki/Exponential_smoothing)
    of the gradients and the squared gradients, respectively.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**β₁** 和 **β₂** 是梯度和平方梯度的 [指数移动平均](https://en.wikipedia.org/wiki/Exponential_smoothing)
    的衰减率。'
- en: Larger values will assign more weight to the previous gradients, giving a smoother
    behavior, and less reactive to recent changes. Values closer to zero give more
    weight to recent changes in the gradients. Typical values are **β₁** = 0.9 and
    **β₂** = 0.999.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 较大的值会对先前的梯度赋予更多权重，表现更平滑，对最近的变化反应较少。接近零的值会赋予最近梯度变化更多权重。典型值为**β₁** = 0.9 和**β₂**
    = 0.999。
- en: '**ε** is, like in all previous cases, a constant added to avoid division by
    zero, usually set to 1e-8.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**ε** 与所有前述情况一样，是一个常数，添加以避免除以零，通常设为 1e-8。'
- en: 'Despite having a bunch of additional terms, and significant advantages, Adam
    is easy to implement:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有许多附加项和显著的优势，Adam 实现起来非常简单：
- en: '[PRE6]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Interestingly, the authors of the paper point out that the term
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，论文的作者指出了这个术语
- en: '![](../Images/e4792c1a703ee073bf8177f34fe9d998.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4792c1a703ee073bf8177f34fe9d998.png)'
- en: Adam’s learning rate scaling.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Adam 的学习率缩放。
- en: '*resembles* the definition of the [Signal-to-Noise Ratio (SNR)](https://en.wikipedia.org/wiki/Signal-to-noise_ratio#Definition):'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '*类似于* [信噪比 (SNR)](https://en.wikipedia.org/wiki/Signal-to-noise_ratio#Definition)
    的定义：'
- en: '![](../Images/33d9ff2497e564d37438da78b5661d8e.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/33d9ff2497e564d37438da78b5661d8e.png)'
- en: Signal-to-Noise ratio.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 信噪比。
- en: Then, we could say that for smaller SNR values, the parameter updates will be
    close to zero. This means that we won’t perform large updates whenever there’s
    too much uncertainty about whether we are moving in the direction of the true
    gradient.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以说，对于较小的 SNR 值，参数更新将接近零。这意味着当对是否朝着真实梯度方向移动存在太多不确定性时，我们不会进行大幅更新。
- en: Adam and its variants typically outperform the other algorithms when training
    DL models, especially when there are very noisy and sparse gradients.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Adam 及其变体在训练深度学习模型时通常优于其他算法，特别是在梯度非常嘈杂和稀疏的情况下。
- en: Performance for different learning rates
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不同学习率的性能表现
- en: I decided to compare how the different optimizers perform when initialized with
    different “global” learning rates.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我决定比较不同优化器在不同“全局”学习率下的表现。
- en: This is a rather simple example, but it gives an idea of how these methods are
    affected by the choice of learning rate.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当简单的例子，但它提供了这些方法如何受到学习率选择影响的一个概念。
- en: '![](../Images/d5413170b1b8d1f0cde2cb54ba44551c.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d5413170b1b8d1f0cde2cb54ba44551c.png)'
- en: Comparing the evolution of x and y coordinates during optimization with different
    algorithms. For Momentum and NAG, mu = 0.95\. For RMSProp and AdaDelta, decay
    parameter = 0.9.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 比较不同算法优化过程中 x 和 y 坐标的演变。对于 Momentum 和 NAG，mu = 0.95。对于 RMSProp 和 AdaDelta，衰减参数
    = 0.9。
- en: AdaDelta seems remarkably robust to the global learning rate setting, and it
    “descends” at the same rate for all three cases. We can also see how AdaGrad requires
    larger learning rates to achieve a performance comparable to AdaDelta in this
    case.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: AdaDelta 对全局学习率设置似乎非常鲁棒，在所有三种情况下“下降”速率相同。我们还可以看到，在这种情况下，AdaGrad 需要较大的学习率才能实现与
    AdaDelta 相当的性能。
- en: For small learning rates, it is clear that Adam and RMSProp are similar, and
    superior to Momentum and SGD.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 对于较小的学习率，Adam 和 RMSProp 显然相似，并且优于 Momentum 和 SGD。
- en: However, for larger learning rates, RMSProp shows consistent oscillations around
    the optimum x value (x = 0), while Adam stabilizes after the initial transient,
    thanks to the dampening effect of the momentum term in the numerator.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于较大的学习率，RMSProp 在最优 x 值（x = 0）周围表现出一致的振荡，而 Adam 在初始瞬态后稳定下来，这得益于分子中动量项的阻尼效应。
- en: The adaptive algorithms break off the symmetry earlier than SGD and Momentum
    methods, except for the case with a global learning rate of 0.1 for which Momentum
    and NAG outperform AdaDelta.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应算法比 SGD 和 Momentum 方法更早打破对称性，除了全局学习率为 0.1 的情况，此时 Momentum 和 NAG 优于 AdaDelta。
- en: Again, these observations only apply to this particular scenario.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，这些观察结果仅适用于特定的场景。
- en: Conclusions
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The advantages of these optimization algorithms are not fully evident when we
    apply them to a simple function, like the saddle point example above.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将这些优化算法应用于简单函数时，如上述的鞍点例子，它们的优点并不完全显现。
- en: For other scenarios with small-scale models or datasets, even SGD may work better,
    so it is important to understand where each type of optimizer works best.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他小规模模型或数据集的场景，即使是 SGD 也可能效果更好，因此理解每种优化器最佳工作条件是重要的。
- en: When training neural networks, we optimize the Loss function, and we don’t have
    an exact value of its gradient at any point, just an estimation of it. This is
    why methods like Adam and AdaDelta, which are robust to noise and sparsity in
    the gradients have been used widely in the Data Science community.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练神经网络时，我们优化损失函数，而在任何时刻我们没有其梯度的确切值，仅有对其的估计。这就是为什么像 Adam 和 AdaDelta 这样的对梯度中的噪声和稀疏性具有鲁棒性的算法在数据科学社区中被广泛使用。
- en: Also, we can deal with a large amount of model weights, instead of just x and
    y coordinates. In these scenarios, the ability to obtain per-parameter learning
    rates is beneficial.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以处理大量的模型权重，而不仅仅是 x 和 y 坐标。在这些情况下，获取每个参数的学习率的能力是有益的。
- en: In a future post, I’ll show a more realistic comparison of these methods in
    another article, using an artificial neural network.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来的文章中，我将展示这些方法在另一篇文章中的更现实的比较，使用人工神经网络。
- en: Further reading
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '[DL Notes: Gradient Descent](https://medium.com/@luisdamed/gradient-descent-f09f19eb35fb)'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DL Notes: 梯度下降](https://medium.com/@luisdamed/gradient-descent-f09f19eb35fb)'
- en: '[DL Notes: Feed Forwards Artificial Neural Networks](https://medium.com/@luisdamed/feedforward-artificial-neural-networks-52bcf96d6ac3)'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DL Notes: 前馈人工神经网络](https://medium.com/@luisdamed/feedforward-artificial-neural-networks-52bcf96d6ac3)'
- en: '[Creating a Gradient Descent Animation in Python (Towards Data Science)](/creating-a-gradient-descent-animation-in-python-3c4dcd20ca51?ref=makerluis.com)'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在 Python 中创建梯度下降动画（数据科学的前沿）](/creating-a-gradient-descent-animation-in-python-3c4dcd20ca51?ref=makerluis.com)'
- en: References
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '**All figures, unless otherwise noted are created by the Author.**'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '**所有图示，除非另有说明，均由作者创建。**'
- en: '[1] Online course [A Deep Understanding of Deep Learning](https://www.udemy.com/course/deeplearning_x/?ref=makerluis.com),
    by Mike X Cohen ( [sincxpress.com](https://sincxpress.com/?ref=makerluis.com))'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 在线课程 [深入理解深度学习](https://www.udemy.com/course/deeplearning_x/?ref=makerluis.com)，由
    Mike X Cohen 主讲（[sincxpress.com](https://sincxpress.com/?ref=makerluis.com)）'
- en: '[2] [Standford Online: CS231 Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/neural-networks-3?ref=makerluis.com)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [斯坦福在线：CS231 卷积神经网络进行视觉识别](https://cs231n.github.io/neural-networks-3?ref=makerluis.com)'
- en: '[3] Goh. “Why Momentum Really Works”, Distill, 2017\. [http://doi.org/10.23915/distill.00006](http://doi.org/10.23915/distill.00006?ref=makerluis.com)'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Goh. “为什么 Momentum 真正有效”，Distill，2017\. [http://doi.org/10.23915/distill.00006](http://doi.org/10.23915/distill.00006?ref=makerluis.com)'
- en: '[4] Villalarga, D. “ [AdaGrad](https://optimization.cbe.cornell.edu/index.php?title=AdaGrad&ref=makerluis.com)
    “. Published in Cornell University Computational Optimization Open Textbook —
    Optimization Wiki.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Villalarga, D. “[AdaGrad](https://optimization.cbe.cornell.edu/index.php?title=AdaGrad&ref=makerluis.com)”。发布于康奈尔大学计算优化开放教材
    — 优化维基。'
- en: '[5] Bengio, Yoshua. “Practical recommendations for gradient-based training
    of deep architectures.” *Neural Networks: Tricks of the Trade: Second Edition*.
    Berlin, Heidelberg: Springer Berlin Heidelberg, 437–478, 2012\. Online: [arXiv:1206.5533](https://arxiv.org/abs/1206.5533?ref=makerluis.com)
    [cs.LG]'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Bengio, Yoshua. “深度架构的梯度训练的实际建议。” *神经网络：实用技巧：第二版*。柏林，海德堡：Springer Berlin
    Heidelberg, 437–478, 2012\. 在线: [arXiv:1206.5533](https://arxiv.org/abs/1206.5533?ref=makerluis.com)
    [cs.LG]'
- en: '[6] Sutskever, I., Martens, J., Dahl, G. & Hinton, G. “On the importance of
    initialization and momentum in deep learning”. *Proceedings of Machine Learning
    Research,* 28(3):1139–1147, 2013\. Available from [https://proceedings.mlr.press/v28/sutskever13.html](https://proceedings.mlr.press/v28/sutskever13.html?ref=makerluis.com).'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Sutskever, I., Martens, J., Dahl, G. & Hinton, G. “在深度学习中初始化和动量的重要性”。*机器学习研究会议论文集,*
    28(3):1139–1147, 2013\. 网址: [https://proceedings.mlr.press/v28/sutskever13.html](https://proceedings.mlr.press/v28/sutskever13.html?ref=makerluis.com).'
- en: '[7] Duchi, J., Hazan, E., Singer, Y., “Adaptive Subgradient Methods for Online
    Learning and Stochastic Optimization”. *Journal of Machine Learning Research,*
    12(61):2121−2159, 2011\. Available from: [https://jmlr.org/papers/v12/duchi11a.html](https://jmlr.org/papers/v12/duchi11a.html?ref=makerluis.com)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Duchi, J., Hazan, E., Singer, Y., “在线学习和随机优化的自适应子梯度方法”。*机器学习研究杂志,* 12(61):2121−2159,
    2011\. 网址: [https://jmlr.org/papers/v12/duchi11a.html](https://jmlr.org/papers/v12/duchi11a.html?ref=makerluis.com)'
- en: '[8] Jason Brownlee, [Gradient Descent With AdaGrad From Scratch](https://machinelearningmastery.com/gradient-descent-with-adagrad-from-scratch/?ref=makerluis.com).
    2021'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Jason Brownlee, [从零开始的AdaGrad梯度下降](https://machinelearningmastery.com/gradient-descent-with-adagrad-from-scratch/?ref=makerluis.com)。2021'
- en: '[9] Zeiler, M. “ADADELTA: AN ADAPTIVE LEARNING RATE METHOD”, 2012\. [arXiv:1212.5701v1](https://arxiv.org/abs/1212.5701v1?ref=makerluis.com)
    [cs.LG]'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Zeiler, M. “ADADELTA: 一种自适应学习率方法”，2012\. [arXiv:1212.5701v1](https://arxiv.org/abs/1212.5701v1?ref=makerluis.com)
    [cs.LG]'
- en: '[10] Kingma, D., Ba, J. “Adam: A Method for Stochastic Optimization”, 2014\.
    [arXiv:1412.6980](https://arxiv.org/abs/1412.6980?ref=makerluis.com) [cs.LG]'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Kingma, D., Ba, J. “Adam: 一种随机优化方法”，2014\. [arXiv:1412.6980](https://arxiv.org/abs/1412.6980?ref=makerluis.com)
    [cs.LG]'
- en: '*Originally published at* [*https://www.makerluis.com*](https://www.makerluis.com/dl-notes-advanced-gradient-descent/)
    *on December 5, 2023.*'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '*最初发布于* [*https://www.makerluis.com*](https://www.makerluis.com/dl-notes-advanced-gradient-descent/)
    *2023年12月5日。*'
