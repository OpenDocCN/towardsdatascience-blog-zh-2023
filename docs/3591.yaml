- en: 'DL Notes: Advanced Gradient Descent'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/dl-notes-advanced-gradient-descent-4407d84c2515?source=collection_archive---------7-----------------------#2023-12-05](https://towardsdatascience.com/dl-notes-advanced-gradient-descent-4407d84c2515?source=collection_archive---------7-----------------------#2023-12-05)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The main optimization algorithms used for training neural networks, explained
    and implemented from scratch in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@luisdamed?source=post_page-----4407d84c2515--------------------------------)[![Luis
    Medina](../Images/d83d326290ae3272f0618d0bd28bd875.png)](https://medium.com/@luisdamed?source=post_page-----4407d84c2515--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4407d84c2515--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4407d84c2515--------------------------------)
    [Luis Medina](https://medium.com/@luisdamed?source=post_page-----4407d84c2515--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F562a027a34f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdl-notes-advanced-gradient-descent-4407d84c2515&user=Luis+Medina&userId=562a027a34f0&source=post_page-562a027a34f0----4407d84c2515---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4407d84c2515--------------------------------)
    ¬∑17 min read¬∑Dec 5, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4407d84c2515&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdl-notes-advanced-gradient-descent-4407d84c2515&user=Luis+Medina&userId=562a027a34f0&source=-----4407d84c2515---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4407d84c2515&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdl-notes-advanced-gradient-descent-4407d84c2515&source=-----4407d84c2515---------------------bookmark_footer-----------)![](../Images/5de63561379eb2ad31ce53a3d33a8cbb.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Jack Anstey](https://unsplash.com/@jack_anstey?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit)
    / [Unsplash](https://unsplash.com/?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit)
  prefs: []
  type: TYPE_NORMAL
- en: In my [previous article about gradient descent](https://medium.com/@luisdamed/gradient-descent-f09f19eb35fb),
    I explained the basic concepts behind it and summarized the main challenges of
    this kind of optimization.
  prefs: []
  type: TYPE_NORMAL
- en: However, I only covered Stochastic Gradient Descent (SGD) and the ‚Äúbatch‚Äù and
    ‚Äúmini-batch‚Äù implementation of gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: Other algorithms offer advantages in terms of convergence speed, robustness
    to ‚Äúlandscape‚Äù features (the vanishing gradient problem), and less dependence
    on the choice of learning rate to achieve good performance.
  prefs: []
  type: TYPE_NORMAL
- en: So today I‚Äôll write about more advanced optimization algorithms, implementing
    them from scratch in Python and comparing them through animated visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: I‚Äôve also listed the resources I used for learning about these algorithms. They
    are great for diving deeper into formal concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the algorithms using a simple objective function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/214af3276a66868c0dd1d65362456436.png)'
  prefs: []
  type: TYPE_IMG
- en: Throughout this article, I‚Äôll show how I implemented the different algorithms
    in Python.
  prefs: []
  type: TYPE_NORMAL
- en: I‚Äôve created a Jupyter Notebook that you can [**access on GitHub**](https://github.com/luisdamed/Gradient_Descent_Visualizations/blob/main/Advanced_Gradient_Descent_Trajectories.ipynb?ref=makerluis.com)
    or directly [**on Google Colab**](https://colab.research.google.com/github/luisdamed/Gradient_Descent_Visualizations/blob/main/Advanced_Gradient_Descent_Trajectories.ipynb?ref=makerluis.com)to
    see all the code used to create the figures shown here.
  prefs: []
  type: TYPE_NORMAL
- en: To generate the animations, I used the approach shown in my previous post about
    [creating an animated gradient descent figure in Python](/creating-a-gradient-descent-animation-in-python-3c4dcd20ca51?ref=makerluis.com).
  prefs: []
  type: TYPE_NORMAL
- en: The function definitions assume that the following code has been already included,
    as they use `numpy` classes and methods and call both the function `f` and its
    gradient, `grad_f`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Momentum
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/c2cf61a22635cb112694929bb532b4b0.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Sharon Pittaway](https://unsplash.com/@sharonp?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: We could compare the optimization algorithm with a ball rolling downhill.
  prefs: []
  type: TYPE_NORMAL
- en: If the ‚Äúball‚Äù had momentum like it has in real life, it would be less likely
    to remain stuck in a local minimum after accelerating down the hill at full speed.
  prefs: []
  type: TYPE_NORMAL
- en: That‚Äôs what people realized when dealing with the problem of gradient descent
    getting stuck in local minima.
  prefs: []
  type: TYPE_NORMAL
- en: 'From high school physics, we know that *translational* momentum is defined
    by the product of the mass of an object and its velocity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d3ebc4b9ecd19513eee3a78b3f19a25.png)'
  prefs: []
  type: TYPE_IMG
- en: Translational momentum.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also know that the gravitational potential energy of an object of mass ***m***
    is *proportional* to the height ***h*** at which it is placed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0403422b98dca25d394344ebd5a5dd03.png)'
  prefs: []
  type: TYPE_IMG
- en: Gravitational potential energy.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, there is a direct [relationship between the potential energy of an
    object and the force exerted on it](https://phys.libretexts.org/Under_Construction/Purgatory/2%3A_Applying_Models_to_Mechanical_Phenomena/2.5%3A_Force_and_Potential_Energy?ref=makerluis.com)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eac99ef0f2de0d84a9b77378f042d61a.png)'
  prefs: []
  type: TYPE_IMG
- en: Force is equal to the negative gradient of potential energy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The relationship between **p** and ***U*** can be derived from Newton‚Äôs second
    law of motion:'
  prefs: []
  type: TYPE_NORMAL
- en: The change of motion of an object is proportional to the force impressed; and
    is made in the direction of the straight line in which the force is impressed.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/f58e180d6fece6b73243d282bba9189e.png)'
  prefs: []
  type: TYPE_IMG
- en: Newton‚Äôs second law of motion.
  prefs: []
  type: TYPE_NORMAL
- en: üí° In reality, this physics analogy is too simplified to cover all the advantages
    and disadvantages of adding momentum to a gradient descent optimization. To get
    the whole picture, I recommend you to check [**Why Momentum Really Works?**](https://distill.pub/2017/momentum/?ref=makerluis.com)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to add momentum?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we initialize the optimization algorithm, we place the ‚Äúball‚Äù at a height
    ***h***, giving it potential energy ***U***.
  prefs: []
  type: TYPE_NORMAL
- en: The force exerted on the ball is proportional to the gradient of such potential
    energy. Like the gradient of the function we are optimizing (the surface over
    which we are moving).
  prefs: []
  type: TYPE_NORMAL
- en: The way momentum works for the optimization is by using the gradient to change
    the ‚Äúvelocity‚Äù of the particle, which in turn changes its position.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eeacef283ed7c5f063589f3754773bb5.png)'
  prefs: []
  type: TYPE_IMG
- en: Momentum update.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the velocity term, the ‚Äúparticle‚Äù builds up speed in any direction
    that has a consistent gradient.
  prefs: []
  type: TYPE_NORMAL
- en: 'I‚Äôve implemented this as a Python function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The Momentum update both accelerates the optimization in directions of low curvature
    and smooths out (dampening effect) oscillations caused by ‚Äúlandscape‚Äù features
    or noisy data [3].
  prefs: []
  type: TYPE_NORMAL
- en: Some argue that the Momentum update is in reality more consistent with the physical
    effect of the friction coefficient because it reduces the kinetic energy of the
    system [2].
  prefs: []
  type: TYPE_NORMAL
- en: Another way to interpret it is that it gives ‚Äúshort-term‚Äù memory to the optimization
    process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Being smaller than 1, the momentum parameter acts like an exponentially weighted
    sum of the previous gradients, and the velocity update can be rewritten as [3][5]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/da422a45d928db3a2feada48ebda9a0f.png)'
  prefs: []
  type: TYPE_IMG
- en: Velocity term rewritten as a weighted sum.
  prefs: []
  type: TYPE_NORMAL
- en: where *g* is the instantaneous gradient, and *v* is the smoothed gradient estimator.
  prefs: []
  type: TYPE_NORMAL
- en: The parameter **Œ≤** controls how much weight we give to new values of the instantaneous
    gradient over the previous ones.
  prefs: []
  type: TYPE_NORMAL
- en: It is usually equal to 0.9, but sometimes it is ‚Äúscheduled‚Äù, i.e. increased
    from as low as 0.5 up to 0.99 as the iterations progress.
  prefs: []
  type: TYPE_NORMAL
- en: Nesterov‚Äôs Accelerated Gradient (NAG)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Proposed by Nesterov, Y. in 1983.
  prefs: []
  type: TYPE_NORMAL
- en: The Nesterov update implements a ‚Äúlookahead‚Äù feature to improve the stability
    and convergence speed of Momentum for convex functions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5499819332220dbcf8777354761a3983.png)'
  prefs: []
  type: TYPE_IMG
- en: NAG update.
  prefs: []
  type: TYPE_NORMAL
- en: While Momentum uses the current position to update the gradient, NAG performs
    a partial update of the current position first, knowing that
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c239638fff8b6cd73262d88f0b93ba0a.png)'
  prefs: []
  type: TYPE_IMG
- en: Intuition for lookahead update.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/15c24caf53126189dff5fd3fbae2b2c9.png)'
  prefs: []
  type: TYPE_IMG
- en: Vector representation of Momentum and NAG updates.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement this as a Python function, I just made the following modification
    to the ‚Äúvelocity update‚Äù in the code I showed before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This partial update helps improve the accuracy of the optimization. In practical
    terms, it translates into fewer oscillations around a local minimum when compared
    to Momentum.
  prefs: []
  type: TYPE_NORMAL
- en: The difference is evident in the next figure.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/06e151df4c4996aca7b296f530e03d3c.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparing Momentum and NAG descent optimizations for a complex surface.
  prefs: []
  type: TYPE_NORMAL
- en: Both optimizers are initialized at the same coordinates, and using the same
    momentum parameter (0.95, fixed).
  prefs: []
  type: TYPE_NORMAL
- en: The following animation also helps us see the intuition behind scheduling or
    ‚Äúannealing‚Äù the momentum parameter.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/659217b9fdbdccb3a83429be0bb9cbf8.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparison of different optimization algorithms getting past a vanishing gradient
    region. Momentum-based methods perform better in this case.
  prefs: []
  type: TYPE_NORMAL
- en: In the beginning, a small amount of momentum can be beneficial to get past the
    vanishing gradient. When we approach the local minimum, having larger momentum
    values could dampen out the oscillations we see, improving the convergence speed.
  prefs: []
  type: TYPE_NORMAL
- en: Adaptive methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The other optimization algorithms shown in the animation above are adaptive
    methods, which I‚Äôll describe in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at this simple example with a local and a global minimum, Momentum and
    NAG may seem far superior to the other methods. However, adaptive algorithms are
    more robust. I‚Äôll show this with a practical example in another article.
  prefs: []
  type: TYPE_NORMAL
- en: Adaptive Gradient Algorithm (AdaGrad)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AdaGrad is a family of [subgradient](https://web.stanford.edu/class/ee392o/subgrad_method.pdf?ref=makerluis.com)
    algorithms for stochastic optimization, presented by [John Duchi, Elad Hazan and
    Yoram Singer in 2011.](https://jmlr.org/papers/v12/duchi11a.html?ref=makerluis.com)
  prefs: []
  type: TYPE_NORMAL
- en: They proposed to improve gradient-based learning by incorporating the history
    of the gradients into each new update of the weights.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of biasing the gradient itself, as momentum does, AdaGrad modifies the
    learning rate dynamically and separately for each parameter of the objective function.
  prefs: []
  type: TYPE_NORMAL
- en: This means we have different learning rates for each model weight. They are
    adjusted based on the consistency of the gradients.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, the sequence of gradient estimates is stored as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/018b04875576d906b9444d027b4adc3f.png)'
  prefs: []
  type: TYPE_IMG
- en: Sum of squared gradients or outer product of gradient history.
  prefs: []
  type: TYPE_NORMAL
- en: If we are optimizing a function with *n* coordinates or parameters, g**‚Çú** will
    be a vector with *n* elements, and so will G**‚Çú**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, the update rule is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3bc65abe1897974a0676b8470522d9b0.png)'
  prefs: []
  type: TYPE_IMG
- en: AdaGrad update.
  prefs: []
  type: TYPE_NORMAL
- en: The parameter **Œµ** is used to avoid a division by zero and is usually set to
    a small value, like 1e-08.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, the definition of G**‚Çú** is similar to the un-centered (zero-mean)
    [variance](https://en.wikipedia.org/wiki/Variance?ref=makerluis.com) of the distribution
    of gradients.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/77d7e86480587b0db5f7d8998e28bfd8.png)'
  prefs: []
  type: TYPE_IMG
- en: Variance definition.
  prefs: []
  type: TYPE_NORMAL
- en: The variance is a measure of the dispersion energy of a distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, for each parameter **Œ∏·µ¢**, the learning rate is adapted in proportion
    to the *inverse* of the gradient‚Äôs variance for **Œ∏·µ¢**.
  prefs: []
  type: TYPE_NORMAL
- en: Considering this, we could say that parameters with more dispersion in their
    gradient distribution will scale down the learning rate by a larger factor, while
    those with more consistent gradients (lower variance) will have larger learning
    rates.
  prefs: []
  type: TYPE_NORMAL
- en: AdaGrad also implements learning rate decay automatically, based on the time
    (accumulation of previous gradients) and the curvature of the objective function
    (‚Äúareas‚Äù with lower gradient variance will be assigned smaller step sizes). This
    improves the convergence rate of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'I‚Äôve implemented AdaGrad as a Python function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: One drawback of AdaGrad is that this learning rate decay during training may
    be too aggressive, causing the learning to stop early when training ANNs. Each
    parameter update is robust, but the rate at which the changes move toward the
    optimal point can decrease too much.
  prefs: []
  type: TYPE_NORMAL
- en: Another drawback is that, although the learning rates are self-adjusted during
    learning, AdaGrad can still be sensitive to the initial conditions. If the gradients
    are large at the start of the optimization, the learning rates will be low for
    the rest of the training.
  prefs: []
  type: TYPE_NORMAL
- en: We can see this in the animated figure. AdaGrad breaks off from the symmetry
    quickly, but the learning is very slow, compared to other algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: To compensate for this, one may need to tune the learning rate to a higher value,
    which in part defeats the purpose of the self-adjusting characteristic.
  prefs: []
  type: TYPE_NORMAL
- en: Root Mean Square Propagation (RMSprop)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unpublished method, but mentioned in the [slides for lecture 6](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf?ref=makerluis.com)
    of the course Neural Networks for Machine Learning, from Prof. [Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton?ref=makerluis.com).
  prefs: []
  type: TYPE_NORMAL
- en: The concept of this algorithm is similar to momentum. It also incorporates the
    short-term history of the magnitude of the gradients to perform the weights update.
  prefs: []
  type: TYPE_NORMAL
- en: However, similarly to AdaGrad, RMSProp modifies the learning rate and not the
    gradient.
  prefs: []
  type: TYPE_NORMAL
- en: To do this, the learning rate is divided by a running average of the magnitudes
    of recent gradients.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the algorithm computes a weighted sum of the squared cost values and
    the previous ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/701f7cc431162487e3d89e088ba1bfc1.png)'
  prefs: []
  type: TYPE_IMG
- en: Exponentially weighted sum of squared costs.
  prefs: []
  type: TYPE_NORMAL
- en: This is like a short-term mean, where the parameter **Œ≤** adjusts how much weight
    is given to more recent cost values over older ones.
  prefs: []
  type: TYPE_NORMAL
- en: It is analog to the re-written form of momentum that I mentioned before but
    applied to the squared costs, instead of the gradient.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is dividing the learning rate by the square root of this moving
    average.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6d74a625b221b9b4e476c5fb201a4368.png)'
  prefs: []
  type: TYPE_IMG
- en: RMSProp update rule.
  prefs: []
  type: TYPE_NORMAL
- en: This way, the step size depends on the history of the gradient magnitudes (short-term
    memory).
  prefs: []
  type: TYPE_NORMAL
- en: Notice that computing the root of a weighted sum (or weighted average) of squared
    values is equivalent to computing the Root Mean Square (RMS) of those values.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/752ab5645f800051560cda31b3e4e5da.png)'
  prefs: []
  type: TYPE_IMG
- en: Definition of RMS.
  prefs: []
  type: TYPE_NORMAL
- en: The RMS of a signal is a representation of its *total* energy (as opposed to
    the variance, which represents its *dispersion* energy)[1].
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, with RMSProp, the learning rate is modulated according to the total
    energy of the gradient of the cost function and its previous values. This adjustment
    is done dynamically and for each direction or component of our loss function (each
    weight!).
  prefs: []
  type: TYPE_NORMAL
- en: The goal is to reduce the volatility caused by large changes in the gradient
    by reducing the step size in those cases.
  prefs: []
  type: TYPE_NORMAL
- en: This also helps with vanishing gradient problems because when there‚Äôs a trend
    of very small gradients we take larger steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is how I coded it as a Python function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: RMSprop is robust to the initial choice of learning rate, and it also implements
    an automatic learning rate decay. However, since it is based on a short-term history
    of gradient values, the decay is less aggressive than AdaGrad.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0c6c1edca99d72c5a323f8c2e692bcce.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Gonzalo Kaplanski](https://unsplash.com/@gonzakap?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: AdaDelta
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Proposed by [Matthew Zeiler in 2012](https://arxiv.org/abs/1212.5701?ref=makerluis.com).
  prefs: []
  type: TYPE_NORMAL
- en: 'This method was developed to overcome the main limitations of AdaGrad: the
    continuous decay of the learning rate that causes early stopping and the need
    to tune the ‚Äúglobal‚Äù learning rate manually.'
  prefs: []
  type: TYPE_NORMAL
- en: To overcome the continuous learning rate decay, the algorithm accumulates the
    history of past gradients over a window or fixed size.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, this involves dividing the learning rate by the RMS of previous
    gradients over a window of fixed size, just like RMSprop does:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e3e257fde445b1c8167cd06aa71c2ba6.png)'
  prefs: []
  type: TYPE_IMG
- en: Learning rate scaling similar to RMSProp.
  prefs: []
  type: TYPE_NORMAL
- en: The next modification from AdaGrad is the correction of the units of the optimization
    updates.
  prefs: []
  type: TYPE_NORMAL
- en: 'In AdaGrad (and all other optimization algorithms I‚Äôve described so far), the
    units of the optimization steps don‚Äôt match the units of the parameters that we
    modify to optimize the cost function [9]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bfb23bf151501ad39ebfd3a3e9704540.png)'
  prefs: []
  type: TYPE_IMG
- en: We know from school that we can‚Äôt add apples and oranges. But with these optimization
    algorithms, it‚Äôs like we‚Äôve been adding ‚Äúapples‚Äù (current parameter values, **Œ∏‚Çú**
    and some unknown quantity (the optimization step **ŒîŒ∏** ) that mathematically
    can be added to them to obtain new apples (the updated parameters, **Œ∏‚Çú ‚Çä‚ÇÅ**.
    It just works, but doesn‚Äôt make sense in real life.
  prefs: []
  type: TYPE_NORMAL
- en: 'Zeiler decided to correct the units, rearranging the update term from [Newton‚Äôs
    method](https://en.wikipedia.org/wiki/Newton%27s_method?ref=makerluis.com) and
    assuming the curvature of the loss function could be approximated by a diagonal
    [Hessian matrix](https://en.wikipedia.org/wiki/Hessian_matrix?ref=makerluis.com):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d4cb70742f35ebddcf11f40b98d708ac.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparing this observation with the update rule similar to RMSProp, Zeiler determined
    the correct form of the update term to preserve the right units.
  prefs: []
  type: TYPE_NORMAL
- en: 'The intuition is better explained in the original publication but in practice,
    it resulted in adding the square root of an exponentially-weighted average of
    the previous update values to the numerator of the update term:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5d7d73b0fd9e37e9727fa810762f744a.png)'
  prefs: []
  type: TYPE_IMG
- en: AdaDelta step for parameter update.
  prefs: []
  type: TYPE_NORMAL
- en: This is basically assuming that the loss function is smooth (low curvature)
    within a small window of size *w*, so that **ŒîŒ∏‚Çú** can be approximated by the
    exponential RMS of the previous values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm looks like this if we implement it as a Python function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: AdaDelta combines the advantages of the optimization methods it builds upon.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, the short-term memory of previous parameter updates in the numerator
    is similar to Momentum and has the effect of accelerating the gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: The denominator provides the per-dimension accuracy of AdaGrad but without the
    excessive learning rate decay (just like RMSProp).
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, AdaDelta is more robust to sudden gradient changes, and it is
    robust to the choice of initial learning rate (see a practical example in the
    last section of this article).
  prefs: []
  type: TYPE_NORMAL
- en: Adam (Adaptive Momentum)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is one of the most popular algorithms today.
  prefs: []
  type: TYPE_NORMAL
- en: It was introduced by [Diederik P. Kingma and Jimmy Lei Ba in 2014](https://arxiv.org/pdf/1412.6980.pdf?ref=makerluis.com),
    and has become very popular because of its computational efficiency and because
    it works very well for problems with large amounts of data and parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Adam is like a combination of Momentum and RMSprop because it dynamically changes
    both the gradient of the loss function and the learning rates used to scale such
    gradient to update the weights.
  prefs: []
  type: TYPE_NORMAL
- en: To do this, the algorithm includes the computation of two terms that will be
    familiar from previous sections of this article.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, there‚Äôs a term from momentum, an exponentially weighted sum of the previous
    gradients of the cost function (this is like a weighted variance):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4fcf8fc07a1a6b7e064c47aa116ae596.png)'
  prefs: []
  type: TYPE_IMG
- en: Exponentially weighted average of cost gradients.
  prefs: []
  type: TYPE_NORMAL
- en: Then, there‚Äôs a term from RMSprop, an exponentially weighted moving average
    of the squared gradients.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fb2aaf1105d8aebc521a41d67f9b5ed9.png)'
  prefs: []
  type: TYPE_IMG
- en: Exponentially weighted average of squared cost gradients.
  prefs: []
  type: TYPE_NORMAL
- en: Combining both terms with the SGD algorithm, the information from past gradients
    is included in the update step. Their total energy over a short window (RMS) is
    used to scale the learning rate, and their dispersion (variance) helps adjust
    the current gradient value used for updating the weights.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e1ae4b44a6a1e7fb27694f23883149dc.png)'
  prefs: []
  type: TYPE_IMG
- en: Adam‚Äôs update rule.
  prefs: []
  type: TYPE_NORMAL
- en: 'The values with tilde (~) correspond to bias correction terms that are introduced
    to reduce the contribution from the initial values of m and v as learning progresses:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d9d3e694982e67f5eb6551697682715d.png)'
  prefs: []
  type: TYPE_IMG
- en: Initialization bias correction terms for Adam.
  prefs: []
  type: TYPE_NORMAL
- en: t = current training epoch.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike AdaDelta, Adam does require tuning of some hyperparameters, but they
    are easy to interpret.
  prefs: []
  type: TYPE_NORMAL
- en: The terms **Œ≤‚ÇÅ** and **Œ≤‚ÇÇ** are the decay rates of the [exponential moving averages](https://en.wikipedia.org/wiki/Exponential_smoothing)
    of the gradients and the squared gradients, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Larger values will assign more weight to the previous gradients, giving a smoother
    behavior, and less reactive to recent changes. Values closer to zero give more
    weight to recent changes in the gradients. Typical values are **Œ≤‚ÇÅ** = 0.9 and
    **Œ≤‚ÇÇ** = 0.999.
  prefs: []
  type: TYPE_NORMAL
- en: '**Œµ** is, like in all previous cases, a constant added to avoid division by
    zero, usually set to 1e-8.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite having a bunch of additional terms, and significant advantages, Adam
    is easy to implement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Interestingly, the authors of the paper point out that the term
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e4792c1a703ee073bf8177f34fe9d998.png)'
  prefs: []
  type: TYPE_IMG
- en: Adam‚Äôs learning rate scaling.
  prefs: []
  type: TYPE_NORMAL
- en: '*resembles* the definition of the [Signal-to-Noise Ratio (SNR)](https://en.wikipedia.org/wiki/Signal-to-noise_ratio#Definition):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/33d9ff2497e564d37438da78b5661d8e.png)'
  prefs: []
  type: TYPE_IMG
- en: Signal-to-Noise ratio.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we could say that for smaller SNR values, the parameter updates will be
    close to zero. This means that we won‚Äôt perform large updates whenever there‚Äôs
    too much uncertainty about whether we are moving in the direction of the true
    gradient.
  prefs: []
  type: TYPE_NORMAL
- en: Adam and its variants typically outperform the other algorithms when training
    DL models, especially when there are very noisy and sparse gradients.
  prefs: []
  type: TYPE_NORMAL
- en: Performance for different learning rates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I decided to compare how the different optimizers perform when initialized with
    different ‚Äúglobal‚Äù learning rates.
  prefs: []
  type: TYPE_NORMAL
- en: This is a rather simple example, but it gives an idea of how these methods are
    affected by the choice of learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d5413170b1b8d1f0cde2cb54ba44551c.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparing the evolution of x and y coordinates during optimization with different
    algorithms. For Momentum and NAG, mu = 0.95\. For RMSProp and AdaDelta, decay
    parameter = 0.9.
  prefs: []
  type: TYPE_NORMAL
- en: AdaDelta seems remarkably robust to the global learning rate setting, and it
    ‚Äúdescends‚Äù at the same rate for all three cases. We can also see how AdaGrad requires
    larger learning rates to achieve a performance comparable to AdaDelta in this
    case.
  prefs: []
  type: TYPE_NORMAL
- en: For small learning rates, it is clear that Adam and RMSProp are similar, and
    superior to Momentum and SGD.
  prefs: []
  type: TYPE_NORMAL
- en: However, for larger learning rates, RMSProp shows consistent oscillations around
    the optimum x value (x = 0), while Adam stabilizes after the initial transient,
    thanks to the dampening effect of the momentum term in the numerator.
  prefs: []
  type: TYPE_NORMAL
- en: The adaptive algorithms break off the symmetry earlier than SGD and Momentum
    methods, except for the case with a global learning rate of 0.1 for which Momentum
    and NAG outperform AdaDelta.
  prefs: []
  type: TYPE_NORMAL
- en: Again, these observations only apply to this particular scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The advantages of these optimization algorithms are not fully evident when we
    apply them to a simple function, like the saddle point example above.
  prefs: []
  type: TYPE_NORMAL
- en: For other scenarios with small-scale models or datasets, even SGD may work better,
    so it is important to understand where each type of optimizer works best.
  prefs: []
  type: TYPE_NORMAL
- en: When training neural networks, we optimize the Loss function, and we don‚Äôt have
    an exact value of its gradient at any point, just an estimation of it. This is
    why methods like Adam and AdaDelta, which are robust to noise and sparsity in
    the gradients have been used widely in the Data Science community.
  prefs: []
  type: TYPE_NORMAL
- en: Also, we can deal with a large amount of model weights, instead of just x and
    y coordinates. In these scenarios, the ability to obtain per-parameter learning
    rates is beneficial.
  prefs: []
  type: TYPE_NORMAL
- en: In a future post, I‚Äôll show a more realistic comparison of these methods in
    another article, using an artificial neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[DL Notes: Gradient Descent](https://medium.com/@luisdamed/gradient-descent-f09f19eb35fb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[DL Notes: Feed Forwards Artificial Neural Networks](https://medium.com/@luisdamed/feedforward-artificial-neural-networks-52bcf96d6ac3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Creating a Gradient Descent Animation in Python (Towards Data Science)](/creating-a-gradient-descent-animation-in-python-3c4dcd20ca51?ref=makerluis.com)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**All figures, unless otherwise noted are created by the Author.**'
  prefs: []
  type: TYPE_NORMAL
- en: '[1] Online course [A Deep Understanding of Deep Learning](https://www.udemy.com/course/deeplearning_x/?ref=makerluis.com),
    by Mike X Cohen ( [sincxpress.com](https://sincxpress.com/?ref=makerluis.com))'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [Standford Online: CS231 Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/neural-networks-3?ref=makerluis.com)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Goh. ‚ÄúWhy Momentum Really Works‚Äù, Distill, 2017\. [http://doi.org/10.23915/distill.00006](http://doi.org/10.23915/distill.00006?ref=makerluis.com)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Villalarga, D. ‚Äú [AdaGrad](https://optimization.cbe.cornell.edu/index.php?title=AdaGrad&ref=makerluis.com)
    ‚Äú. Published in Cornell University Computational Optimization Open Textbook ‚Äî
    Optimization Wiki.'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Bengio, Yoshua. ‚ÄúPractical recommendations for gradient-based training
    of deep architectures.‚Äù *Neural Networks: Tricks of the Trade: Second Edition*.
    Berlin, Heidelberg: Springer Berlin Heidelberg, 437‚Äì478, 2012\. Online: [arXiv:1206.5533](https://arxiv.org/abs/1206.5533?ref=makerluis.com)
    [cs.LG]'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Sutskever, I., Martens, J., Dahl, G. & Hinton, G. ‚ÄúOn the importance of
    initialization and momentum in deep learning‚Äù. *Proceedings of Machine Learning
    Research,* 28(3):1139‚Äì1147, 2013\. Available from [https://proceedings.mlr.press/v28/sutskever13.html](https://proceedings.mlr.press/v28/sutskever13.html?ref=makerluis.com).'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] Duchi, J., Hazan, E., Singer, Y., ‚ÄúAdaptive Subgradient Methods for Online
    Learning and Stochastic Optimization‚Äù. *Journal of Machine Learning Research,*
    12(61):2121‚àí2159, 2011\. Available from: [https://jmlr.org/papers/v12/duchi11a.html](https://jmlr.org/papers/v12/duchi11a.html?ref=makerluis.com)'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] Jason Brownlee, [Gradient Descent With AdaGrad From Scratch](https://machinelearningmastery.com/gradient-descent-with-adagrad-from-scratch/?ref=makerluis.com).
    2021'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] Zeiler, M. ‚ÄúADADELTA: AN ADAPTIVE LEARNING RATE METHOD‚Äù, 2012\. [arXiv:1212.5701v1](https://arxiv.org/abs/1212.5701v1?ref=makerluis.com)
    [cs.LG]'
  prefs: []
  type: TYPE_NORMAL
- en: '[10] Kingma, D., Ba, J. ‚ÄúAdam: A Method for Stochastic Optimization‚Äù, 2014\.
    [arXiv:1412.6980](https://arxiv.org/abs/1412.6980?ref=makerluis.com) [cs.LG]'
  prefs: []
  type: TYPE_NORMAL
- en: '*Originally published at* [*https://www.makerluis.com*](https://www.makerluis.com/dl-notes-advanced-gradient-descent/)
    *on December 5, 2023.*'
  prefs: []
  type: TYPE_NORMAL
