- en: 'Logistic Regression: Faceoff and Conceptual Understanding'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é€»è¾‘å›å½’ï¼šå¯¹å†³ä¸æ¦‚å¿µç†è§£
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/logistic-regression-faceoff-67560de4f492?source=collection_archive---------5-----------------------#2023-05-18](https://towardsdatascience.com/logistic-regression-faceoff-67560de4f492?source=collection_archive---------5-----------------------#2023-05-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/logistic-regression-faceoff-67560de4f492?source=collection_archive---------5-----------------------#2023-05-18](https://towardsdatascience.com/logistic-regression-faceoff-67560de4f492?source=collection_archive---------5-----------------------#2023-05-18)
- en: What do log-losses and perfectly separated data have to do with hockey sticks?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é€»è¾‘æŸå¤±å’Œå®Œç¾åˆ†ç¦»çš„æ•°æ®ä¸å†°çƒæ£’æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ
- en: '[](https://medium.com/@igor-s?source=post_page-----67560de4f492--------------------------------)[![Igor
    Å egota](../Images/17c592b71fef9526a0679d47937837f6.png)](https://medium.com/@igor-s?source=post_page-----67560de4f492--------------------------------)[](https://towardsdatascience.com/?source=post_page-----67560de4f492--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----67560de4f492--------------------------------)
    [Igor Å egota](https://medium.com/@igor-s?source=post_page-----67560de4f492--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@igor-s?source=post_page-----67560de4f492--------------------------------)[![Igor
    Å egota](../Images/17c592b71fef9526a0679d47937837f6.png)](https://medium.com/@igor-s?source=post_page-----67560de4f492--------------------------------)[](https://towardsdatascience.com/?source=post_page-----67560de4f492--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----67560de4f492--------------------------------)
    [Igor Å egota](https://medium.com/@igor-s?source=post_page-----67560de4f492--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe5f8ebca4ad8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-faceoff-67560de4f492&user=Igor+%C5%A0egota&userId=e5f8ebca4ad8&source=post_page-e5f8ebca4ad8----67560de4f492---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----67560de4f492--------------------------------)
    Â·7 min readÂ·May 18, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F67560de4f492&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-faceoff-67560de4f492&user=Igor+%C5%A0egota&userId=e5f8ebca4ad8&source=-----67560de4f492---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe5f8ebca4ad8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-faceoff-67560de4f492&user=Igor+%C5%A0egota&userId=e5f8ebca4ad8&source=post_page-e5f8ebca4ad8----67560de4f492---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----67560de4f492--------------------------------)
    Â·7åˆ†é’Ÿé˜…è¯»Â·2023å¹´5æœˆ18æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F67560de4f492&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-faceoff-67560de4f492&user=Igor+%C5%A0egota&userId=e5f8ebca4ad8&source=-----67560de4f492---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F67560de4f492&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-faceoff-67560de4f492&source=-----67560de4f492---------------------bookmark_footer-----------)![](../Images/91a7b0a99da3d1740a217fe617880118.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F67560de4f492&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-faceoff-67560de4f492&source=-----67560de4f492---------------------bookmark_footer-----------)![](../Images/91a7b0a99da3d1740a217fe617880118.png)'
- en: Photo by [Jerry Yu](https://unsplash.com/@jerryyu?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± [Jerry Yu](https://unsplash.com/@jerryyu?utm_source=medium&utm_medium=referral)
    æä¾›ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Who ordered this?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è°ä¸‹çš„è¿™ä¸ªè®¢å•ï¼Ÿ
- en: As of this writing, Google search for â€œlogistic regression tutorialâ€ shows about
    11.2M results. Why add another thing to this pile?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆªè‡³æ’°å†™æœ¬æ–‡æ—¶ï¼Œè°·æ­Œæœç´¢â€œé€»è¾‘å›å½’æ•™ç¨‹â€æ˜¾ç¤ºå¤§çº¦æœ‰1120ä¸‡æ¡ç»“æœã€‚ä¸ºä»€ä¹ˆè¿˜è¦åœ¨è¿™å †ä¿¡æ¯ä¸­å†æ·»åŠ ä¸€ä»½ï¼Ÿ
- en: After reading a good number of articles, books and guides, I realized that most
    lack clear and intuitive explanations of how logistic regression works. Instead,
    they usually strive to be either practical, by showing how to run models, or as
    mathematically complete as possible, and as a consequence, basic concepts get
    buried underneath a forest of matrix algebra.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: é˜…è¯»äº†å¤§é‡æ–‡ç« ã€ä¹¦ç±å’ŒæŒ‡å—åï¼Œæˆ‘æ„è¯†åˆ°å¤§å¤šæ•°ç¼ºä¹å¯¹é€»è¾‘å›å½’å·¥ä½œåŸç†çš„æ¸…æ™°ç›´è§‚è§£é‡Šã€‚ç›¸åï¼Œå®ƒä»¬é€šå¸¸è¦ä¹ˆè‡´åŠ›äºå±•ç¤ºå¦‚ä½•è¿è¡Œæ¨¡å‹çš„å®ç”¨æ€§ï¼Œè¦ä¹ˆå°½å¯èƒ½åœ°æ•°å­¦å…¨é¢ï¼Œå› æ­¤åŸºæœ¬æ¦‚å¿µè¢«åŸ‹åœ¨çŸ©é˜µä»£æ•°çš„æ£®æ—ä¸­ã€‚
- en: 'We will start by clearing up what seem to be common misconceptions. Logistic
    regression is **not**:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: linear regression but with sigmoid curve instead of a straight line
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: classification algorithm (but can be used for this)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sigmoid curve â€œfitâ€ of a decision boundary separating two classes of points
    in the x-y plane
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What *is* a logistic regression?
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Logistic regression is a regression model that returns a probability of a binary
    outcome (0 or 1), assuming that log of the odds is a linear combination of one
    or more inputs. Odds is a ratio between probability of outcome happening (*p*)
    and the probability of the outcome not happening (*1-p*). When we have one input
    or predictor, this starting assumption is mathematically expressed as:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/685c506d6b3786c9ed389a77a810331b.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: 'The goal behind logistic regression is to model cases when inputs are shifting
    the outcome probability progressively from 0 to 1\. The probability of the outcome
    being 1, *p*, can be derived from the previous equation and expressed as a function
    of inputs:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eca13451561745c8a572f7c35592f11c.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: In the last part we swapped from parameters *Î²â‚* and *Î²â‚€* to *k* and *xâ‚€*. Using
    *k* and *xâ‚€* will give us a clearer picture of the model as we go along. We will
    also stick to a single predictor variable *x*, as opposed to marching in with
    an army of matrices, so we can easily visualize logistic fits.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Logistic curve
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will begin by plotting the logistic curve, with parameters *xâ‚€ = 2.5* and
    *k = 3*, on an interval *x* between 0 and 5:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/d11aca277e68ed608888dac0fb31053d.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: Points (red for y=0 and teal for y = 1) and p(x) (black) for logistic regression.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'This logistic curve *p(x)* is described by two parameters:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '*xâ‚€* is the value of a predictor *x* for which the probability is 0.5 (mid-point):
    *p(x = xâ‚€) = 0.5*, so tells us about the location of the mid-point.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*k* is related to the slope of the probability at mid-point: *(dp/dx){x = xâ‚€}
    = k/4*, so tells us about the steepness of the curve at that mid-point. The larger
    the *k*, the steeper the curve in the middle.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we naively employed ordinary least squares to fit the curve *p(x)* to these
    points, we would find that all residuals would be less than 1 and most points
    on the â€œwrong sideâ€ of the mid-point would have residuals ~ 1\. It would make
    more sense to assign a much larger cost to points that are large outliers.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Log-loss fit
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Instead of trying to make ordinary least squares work to fit *p(x)* to the
    points, logistic regression proceeds differently:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: For teal points at *y = 1*, we will fit *-log p(x)* instead of *p(x)*. Negative
    logarithm makes *-log p(x)* progressively larger; as *p(x)* approaches zero.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the red points at *y = 0* we can do the same by using the probability that
    the outcome is zero, *-log[1-p(x)]*.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We call these â€œlog-lossesâ€. If we collapse all the points to *y = 0*, then
    for each point these two log-losses represent a *cost* (loss) of that point, for
    being some amount away from the log-loss curves. In order to utilize `numpy` vectorization,
    we will code these two together as a single log-loss function (this combo log-loss
    also goes by the name â€œcross entropyâ€):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'One way to think about logistic regression is a method that simultaneously
    fits: *-log p(x)* for *y = 1* and -*log[1-p(x)]* for *y = 0*.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: How do these two log-loss curves look?
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'To visualize them, we will plot the same data in the previous plot, but now
    with log-losses instead of probability:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/c076cb0e9f2619ade776975a758f6cb9.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: 'Here we collapsed all points to *y = 0*, but use the colors as *y* labels,
    since the values of log-losses on their own represent the cost. Red points (*y
    = 0*) are fit to the red hockey stick curve: *-log[1-p(x)]*. Teal points (*y =
    1*) are fit to the teal hockey stick curve: *-log p(x)*. Sum of the vertical dashed
    lines represents the total log-loss that needs to be minimized for various *k*
    and *xâ‚€.*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Unlike probability, log-loss curves have the property of penalizing big outliers
    proportionally more and they do not have residuals that cap out at 1.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Finding the minimal log-loss
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How does changing *k* and *xâ‚€* affect this fit? To answer this, we can run fits
    with various combinations of *k* and *xâ‚€.*
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Changing *xâ‚€* moves the intersection point sideways:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/679f7ce48de4018c7a378a456b070d2f.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: If *xâ‚€* is chosen away from the optimal point, the log-loss increases because
    increasing number of points gets fitted to the rising parts of the hockey sticks.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'Changing *k* affects the steepness of the log-loss curves (note the different
    y axes):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/fbd25b1c48f6adda8fd7b9973317a959.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: If *k* is too low (0.5), most points add small but significant amounts to the
    total log-loss. If *k* is too high (7.0), only the points on the â€œwrong sideâ€
    contribute a significant amount to the total log-loss. In this case, it is the
    two teal points on the left of mid-point at *xâ‚€ = 2.5*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'This brings up a question: what if there are no points on the â€œwrong sideâ€
    of the mid-point, such as when the data is perfectly separated?'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Perfectly separated data
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It turns out, the logistic model cannot fit data that is perfectly separated!
    ğŸ˜® We can apply what we learned earlier about fitting log-losses to understand
    why. We start by creating perfectly separated data (with *k = 3* and *xâ‚€ = 2.5*):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/7f62036d309316e52dcff76ef7a35e73.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
- en: Remember how changing *k* affects these hockey stick log-loss curves? When we
    increased *k*, the main contribution to the total log-loss was from the points
    in the â€œwrong sideâ€ of the mid-point. Now, all points are on the â€œcorrect sideâ€
    of the mid-point.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 'That means that we can create arbitrarily good fits by continuously increasing
    *k*. Here is how the fits look when we set *k = 13*:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/84fb7ceac71bc3223536de55616f4355.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
- en: Fitting perfectly separated data would require log-losses to have a 90-degree
    angle and fitted probability to have an infinite slope at the mid-point. Therefore,
    there is no parameter *k* for which total log-loss has a minimum. In practice,
    numerical algorithms stop after some number of steps and may either return the
    value of *k* in the last step or an error.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: To be continued
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This post covers the main part of what logistic regression conceptually does.
    In the next part, we will cover the somewhat unintuitive meaning of parameter
    k as a log-odds-ratio and show how to run, and break, logistic models in Python
    libraries `statsmodels` and `scikit-learn`:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '[](/logistic-regression-deceptively-flawed-2c3e7f77eac9?source=post_page-----67560de4f492--------------------------------)
    [## Logistic Regression: Deceptively Flawed'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: When can large odds ratios and perfectly separated data bite you?
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/logistic-regression-deceptively-flawed-2c3e7f77eac9?source=post_page-----67560de4f492--------------------------------)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
