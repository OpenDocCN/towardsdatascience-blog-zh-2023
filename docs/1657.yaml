- en: 'Logistic Regression: Faceoff and Conceptual Understanding'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/logistic-regression-faceoff-67560de4f492?source=collection_archive---------5-----------------------#2023-05-18](https://towardsdatascience.com/logistic-regression-faceoff-67560de4f492?source=collection_archive---------5-----------------------#2023-05-18)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What do log-losses and perfectly separated data have to do with hockey sticks?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@igor-s?source=post_page-----67560de4f492--------------------------------)[![Igor
    ≈†egota](../Images/17c592b71fef9526a0679d47937837f6.png)](https://medium.com/@igor-s?source=post_page-----67560de4f492--------------------------------)[](https://towardsdatascience.com/?source=post_page-----67560de4f492--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----67560de4f492--------------------------------)
    [Igor ≈†egota](https://medium.com/@igor-s?source=post_page-----67560de4f492--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe5f8ebca4ad8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-faceoff-67560de4f492&user=Igor+%C5%A0egota&userId=e5f8ebca4ad8&source=post_page-e5f8ebca4ad8----67560de4f492---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----67560de4f492--------------------------------)
    ¬∑7 min read¬∑May 18, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F67560de4f492&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-faceoff-67560de4f492&user=Igor+%C5%A0egota&userId=e5f8ebca4ad8&source=-----67560de4f492---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F67560de4f492&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-faceoff-67560de4f492&source=-----67560de4f492---------------------bookmark_footer-----------)![](../Images/91a7b0a99da3d1740a217fe617880118.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Jerry Yu](https://unsplash.com/@jerryyu?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Who ordered this?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As of this writing, Google search for ‚Äúlogistic regression tutorial‚Äù shows about
    11.2M results. Why add another thing to this pile?
  prefs: []
  type: TYPE_NORMAL
- en: After reading a good number of articles, books and guides, I realized that most
    lack clear and intuitive explanations of how logistic regression works. Instead,
    they usually strive to be either practical, by showing how to run models, or as
    mathematically complete as possible, and as a consequence, basic concepts get
    buried underneath a forest of matrix algebra.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by clearing up what seem to be common misconceptions. Logistic
    regression is **not**:'
  prefs: []
  type: TYPE_NORMAL
- en: linear regression but with sigmoid curve instead of a straight line
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: classification algorithm (but can be used for this)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sigmoid curve ‚Äúfit‚Äù of a decision boundary separating two classes of points
    in the x-y plane
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What *is* a logistic regression?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Logistic regression is a regression model that returns a probability of a binary
    outcome (0 or 1), assuming that log of the odds is a linear combination of one
    or more inputs. Odds is a ratio between probability of outcome happening (*p*)
    and the probability of the outcome not happening (*1-p*). When we have one input
    or predictor, this starting assumption is mathematically expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/685c506d6b3786c9ed389a77a810331b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The goal behind logistic regression is to model cases when inputs are shifting
    the outcome probability progressively from 0 to 1\. The probability of the outcome
    being 1, *p*, can be derived from the previous equation and expressed as a function
    of inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eca13451561745c8a572f7c35592f11c.png)'
  prefs: []
  type: TYPE_IMG
- en: In the last part we swapped from parameters *Œ≤‚ÇÅ* and *Œ≤‚ÇÄ* to *k* and *x‚ÇÄ*. Using
    *k* and *x‚ÇÄ* will give us a clearer picture of the model as we go along. We will
    also stick to a single predictor variable *x*, as opposed to marching in with
    an army of matrices, so we can easily visualize logistic fits.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic curve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will begin by plotting the logistic curve, with parameters *x‚ÇÄ = 2.5* and
    *k = 3*, on an interval *x* between 0 and 5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/d11aca277e68ed608888dac0fb31053d.png)'
  prefs: []
  type: TYPE_IMG
- en: Points (red for y=0 and teal for y = 1) and p(x) (black) for logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'This logistic curve *p(x)* is described by two parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '*x‚ÇÄ* is the value of a predictor *x* for which the probability is 0.5 (mid-point):
    *p(x = x‚ÇÄ) = 0.5*, so tells us about the location of the mid-point.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*k* is related to the slope of the probability at mid-point: *(dp/dx){x = x‚ÇÄ}
    = k/4*, so tells us about the steepness of the curve at that mid-point. The larger
    the *k*, the steeper the curve in the middle.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we naively employed ordinary least squares to fit the curve *p(x)* to these
    points, we would find that all residuals would be less than 1 and most points
    on the ‚Äúwrong side‚Äù of the mid-point would have residuals ~ 1\. It would make
    more sense to assign a much larger cost to points that are large outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Log-loss fit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Instead of trying to make ordinary least squares work to fit *p(x)* to the
    points, logistic regression proceeds differently:'
  prefs: []
  type: TYPE_NORMAL
- en: For teal points at *y = 1*, we will fit *-log p(x)* instead of *p(x)*. Negative
    logarithm makes *-log p(x)* progressively larger; as *p(x)* approaches zero.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the red points at *y = 0* we can do the same by using the probability that
    the outcome is zero, *-log[1-p(x)]*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We call these ‚Äúlog-losses‚Äù. If we collapse all the points to *y = 0*, then
    for each point these two log-losses represent a *cost* (loss) of that point, for
    being some amount away from the log-loss curves. In order to utilize `numpy` vectorization,
    we will code these two together as a single log-loss function (this combo log-loss
    also goes by the name ‚Äúcross entropy‚Äù):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'One way to think about logistic regression is a method that simultaneously
    fits: *-log p(x)* for *y = 1* and -*log[1-p(x)]* for *y = 0*.'
  prefs: []
  type: TYPE_NORMAL
- en: How do these two log-loss curves look?
  prefs: []
  type: TYPE_NORMAL
- en: 'To visualize them, we will plot the same data in the previous plot, but now
    with log-losses instead of probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c076cb0e9f2619ade776975a758f6cb9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here we collapsed all points to *y = 0*, but use the colors as *y* labels,
    since the values of log-losses on their own represent the cost. Red points (*y
    = 0*) are fit to the red hockey stick curve: *-log[1-p(x)]*. Teal points (*y =
    1*) are fit to the teal hockey stick curve: *-log p(x)*. Sum of the vertical dashed
    lines represents the total log-loss that needs to be minimized for various *k*
    and *x‚ÇÄ.*'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike probability, log-loss curves have the property of penalizing big outliers
    proportionally more and they do not have residuals that cap out at 1.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the minimal log-loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How does changing *k* and *x‚ÇÄ* affect this fit? To answer this, we can run fits
    with various combinations of *k* and *x‚ÇÄ.*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Changing *x‚ÇÄ* moves the intersection point sideways:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/679f7ce48de4018c7a378a456b070d2f.png)'
  prefs: []
  type: TYPE_IMG
- en: If *x‚ÇÄ* is chosen away from the optimal point, the log-loss increases because
    increasing number of points gets fitted to the rising parts of the hockey sticks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Changing *k* affects the steepness of the log-loss curves (note the different
    y axes):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/fbd25b1c48f6adda8fd7b9973317a959.png)'
  prefs: []
  type: TYPE_IMG
- en: If *k* is too low (0.5), most points add small but significant amounts to the
    total log-loss. If *k* is too high (7.0), only the points on the ‚Äúwrong side‚Äù
    contribute a significant amount to the total log-loss. In this case, it is the
    two teal points on the left of mid-point at *x‚ÇÄ = 2.5*.
  prefs: []
  type: TYPE_NORMAL
- en: 'This brings up a question: what if there are no points on the ‚Äúwrong side‚Äù
    of the mid-point, such as when the data is perfectly separated?'
  prefs: []
  type: TYPE_NORMAL
- en: Perfectly separated data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It turns out, the logistic model cannot fit data that is perfectly separated!
    üòÆ We can apply what we learned earlier about fitting log-losses to understand
    why. We start by creating perfectly separated data (with *k = 3* and *x‚ÇÄ = 2.5*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/7f62036d309316e52dcff76ef7a35e73.png)'
  prefs: []
  type: TYPE_IMG
- en: Remember how changing *k* affects these hockey stick log-loss curves? When we
    increased *k*, the main contribution to the total log-loss was from the points
    in the ‚Äúwrong side‚Äù of the mid-point. Now, all points are on the ‚Äúcorrect side‚Äù
    of the mid-point.
  prefs: []
  type: TYPE_NORMAL
- en: 'That means that we can create arbitrarily good fits by continuously increasing
    *k*. Here is how the fits look when we set *k = 13*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/84fb7ceac71bc3223536de55616f4355.png)'
  prefs: []
  type: TYPE_IMG
- en: Fitting perfectly separated data would require log-losses to have a 90-degree
    angle and fitted probability to have an infinite slope at the mid-point. Therefore,
    there is no parameter *k* for which total log-loss has a minimum. In practice,
    numerical algorithms stop after some number of steps and may either return the
    value of *k* in the last step or an error.
  prefs: []
  type: TYPE_NORMAL
- en: To be continued
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This post covers the main part of what logistic regression conceptually does.
    In the next part, we will cover the somewhat unintuitive meaning of parameter
    k as a log-odds-ratio and show how to run, and break, logistic models in Python
    libraries `statsmodels` and `scikit-learn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/logistic-regression-deceptively-flawed-2c3e7f77eac9?source=post_page-----67560de4f492--------------------------------)
    [## Logistic Regression: Deceptively Flawed'
  prefs: []
  type: TYPE_NORMAL
- en: When can large odds ratios and perfectly separated data bite you?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/logistic-regression-deceptively-flawed-2c3e7f77eac9?source=post_page-----67560de4f492--------------------------------)
  prefs: []
  type: TYPE_NORMAL
