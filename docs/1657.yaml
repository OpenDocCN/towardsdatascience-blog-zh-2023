- en: 'Logistic Regression: Faceoff and Conceptual Understanding'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归：对决与概念理解
- en: 原文：[https://towardsdatascience.com/logistic-regression-faceoff-67560de4f492?source=collection_archive---------5-----------------------#2023-05-18](https://towardsdatascience.com/logistic-regression-faceoff-67560de4f492?source=collection_archive---------5-----------------------#2023-05-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/logistic-regression-faceoff-67560de4f492?source=collection_archive---------5-----------------------#2023-05-18](https://towardsdatascience.com/logistic-regression-faceoff-67560de4f492?source=collection_archive---------5-----------------------#2023-05-18)
- en: What do log-losses and perfectly separated data have to do with hockey sticks?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑损失和完美分离的数据与冰球棒有什么关系？
- en: '[](https://medium.com/@igor-s?source=post_page-----67560de4f492--------------------------------)[![Igor
    Šegota](../Images/17c592b71fef9526a0679d47937837f6.png)](https://medium.com/@igor-s?source=post_page-----67560de4f492--------------------------------)[](https://towardsdatascience.com/?source=post_page-----67560de4f492--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----67560de4f492--------------------------------)
    [Igor Šegota](https://medium.com/@igor-s?source=post_page-----67560de4f492--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@igor-s?source=post_page-----67560de4f492--------------------------------)[![Igor
    Šegota](../Images/17c592b71fef9526a0679d47937837f6.png)](https://medium.com/@igor-s?source=post_page-----67560de4f492--------------------------------)[](https://towardsdatascience.com/?source=post_page-----67560de4f492--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----67560de4f492--------------------------------)
    [Igor Šegota](https://medium.com/@igor-s?source=post_page-----67560de4f492--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe5f8ebca4ad8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-faceoff-67560de4f492&user=Igor+%C5%A0egota&userId=e5f8ebca4ad8&source=post_page-e5f8ebca4ad8----67560de4f492---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----67560de4f492--------------------------------)
    ·7 min read·May 18, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F67560de4f492&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-faceoff-67560de4f492&user=Igor+%C5%A0egota&userId=e5f8ebca4ad8&source=-----67560de4f492---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe5f8ebca4ad8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-faceoff-67560de4f492&user=Igor+%C5%A0egota&userId=e5f8ebca4ad8&source=post_page-e5f8ebca4ad8----67560de4f492---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----67560de4f492--------------------------------)
    ·7分钟阅读·2023年5月18日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F67560de4f492&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-faceoff-67560de4f492&user=Igor+%C5%A0egota&userId=e5f8ebca4ad8&source=-----67560de4f492---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F67560de4f492&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-faceoff-67560de4f492&source=-----67560de4f492---------------------bookmark_footer-----------)![](../Images/91a7b0a99da3d1740a217fe617880118.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F67560de4f492&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-faceoff-67560de4f492&source=-----67560de4f492---------------------bookmark_footer-----------)![](../Images/91a7b0a99da3d1740a217fe617880118.png)'
- en: Photo by [Jerry Yu](https://unsplash.com/@jerryyu?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Jerry Yu](https://unsplash.com/@jerryyu?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Who ordered this?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谁下的这个订单？
- en: As of this writing, Google search for “logistic regression tutorial” shows about
    11.2M results. Why add another thing to this pile?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 截至撰写本文时，谷歌搜索“逻辑回归教程”显示大约有1120万条结果。为什么还要在这堆信息中再添加一份？
- en: After reading a good number of articles, books and guides, I realized that most
    lack clear and intuitive explanations of how logistic regression works. Instead,
    they usually strive to be either practical, by showing how to run models, or as
    mathematically complete as possible, and as a consequence, basic concepts get
    buried underneath a forest of matrix algebra.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读了大量文章、书籍和指南后，我意识到大多数缺乏对逻辑回归工作原理的清晰直观解释。相反，它们通常要么致力于展示如何运行模型的实用性，要么尽可能地数学全面，因此基本概念被埋在矩阵代数的森林中。
- en: 'We will start by clearing up what seem to be common misconceptions. Logistic
    regression is **not**:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: linear regression but with sigmoid curve instead of a straight line
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: classification algorithm (but can be used for this)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sigmoid curve “fit” of a decision boundary separating two classes of points
    in the x-y plane
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What *is* a logistic regression?
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Logistic regression is a regression model that returns a probability of a binary
    outcome (0 or 1), assuming that log of the odds is a linear combination of one
    or more inputs. Odds is a ratio between probability of outcome happening (*p*)
    and the probability of the outcome not happening (*1-p*). When we have one input
    or predictor, this starting assumption is mathematically expressed as:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/685c506d6b3786c9ed389a77a810331b.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: 'The goal behind logistic regression is to model cases when inputs are shifting
    the outcome probability progressively from 0 to 1\. The probability of the outcome
    being 1, *p*, can be derived from the previous equation and expressed as a function
    of inputs:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eca13451561745c8a572f7c35592f11c.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: In the last part we swapped from parameters *β₁* and *β₀* to *k* and *x₀*. Using
    *k* and *x₀* will give us a clearer picture of the model as we go along. We will
    also stick to a single predictor variable *x*, as opposed to marching in with
    an army of matrices, so we can easily visualize logistic fits.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Logistic curve
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will begin by plotting the logistic curve, with parameters *x₀ = 2.5* and
    *k = 3*, on an interval *x* between 0 and 5:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/d11aca277e68ed608888dac0fb31053d.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: Points (red for y=0 and teal for y = 1) and p(x) (black) for logistic regression.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'This logistic curve *p(x)* is described by two parameters:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '*x₀* is the value of a predictor *x* for which the probability is 0.5 (mid-point):
    *p(x = x₀) = 0.5*, so tells us about the location of the mid-point.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*k* is related to the slope of the probability at mid-point: *(dp/dx){x = x₀}
    = k/4*, so tells us about the steepness of the curve at that mid-point. The larger
    the *k*, the steeper the curve in the middle.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we naively employed ordinary least squares to fit the curve *p(x)* to these
    points, we would find that all residuals would be less than 1 and most points
    on the “wrong side” of the mid-point would have residuals ~ 1\. It would make
    more sense to assign a much larger cost to points that are large outliers.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Log-loss fit
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Instead of trying to make ordinary least squares work to fit *p(x)* to the
    points, logistic regression proceeds differently:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: For teal points at *y = 1*, we will fit *-log p(x)* instead of *p(x)*. Negative
    logarithm makes *-log p(x)* progressively larger; as *p(x)* approaches zero.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the red points at *y = 0* we can do the same by using the probability that
    the outcome is zero, *-log[1-p(x)]*.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We call these “log-losses”. If we collapse all the points to *y = 0*, then
    for each point these two log-losses represent a *cost* (loss) of that point, for
    being some amount away from the log-loss curves. In order to utilize `numpy` vectorization,
    we will code these two together as a single log-loss function (this combo log-loss
    also goes by the name “cross entropy”):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'One way to think about logistic regression is a method that simultaneously
    fits: *-log p(x)* for *y = 1* and -*log[1-p(x)]* for *y = 0*.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: How do these two log-loss curves look?
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'To visualize them, we will plot the same data in the previous plot, but now
    with log-losses instead of probability:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/c076cb0e9f2619ade776975a758f6cb9.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: 'Here we collapsed all points to *y = 0*, but use the colors as *y* labels,
    since the values of log-losses on their own represent the cost. Red points (*y
    = 0*) are fit to the red hockey stick curve: *-log[1-p(x)]*. Teal points (*y =
    1*) are fit to the teal hockey stick curve: *-log p(x)*. Sum of the vertical dashed
    lines represents the total log-loss that needs to be minimized for various *k*
    and *x₀.*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Unlike probability, log-loss curves have the property of penalizing big outliers
    proportionally more and they do not have residuals that cap out at 1.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Finding the minimal log-loss
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How does changing *k* and *x₀* affect this fit? To answer this, we can run fits
    with various combinations of *k* and *x₀.*
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Changing *x₀* moves the intersection point sideways:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/679f7ce48de4018c7a378a456b070d2f.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: If *x₀* is chosen away from the optimal point, the log-loss increases because
    increasing number of points gets fitted to the rising parts of the hockey sticks.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'Changing *k* affects the steepness of the log-loss curves (note the different
    y axes):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/fbd25b1c48f6adda8fd7b9973317a959.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: If *k* is too low (0.5), most points add small but significant amounts to the
    total log-loss. If *k* is too high (7.0), only the points on the “wrong side”
    contribute a significant amount to the total log-loss. In this case, it is the
    two teal points on the left of mid-point at *x₀ = 2.5*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'This brings up a question: what if there are no points on the “wrong side”
    of the mid-point, such as when the data is perfectly separated?'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Perfectly separated data
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It turns out, the logistic model cannot fit data that is perfectly separated!
    😮 We can apply what we learned earlier about fitting log-losses to understand
    why. We start by creating perfectly separated data (with *k = 3* and *x₀ = 2.5*):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/7f62036d309316e52dcff76ef7a35e73.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
- en: Remember how changing *k* affects these hockey stick log-loss curves? When we
    increased *k*, the main contribution to the total log-loss was from the points
    in the “wrong side” of the mid-point. Now, all points are on the “correct side”
    of the mid-point.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 'That means that we can create arbitrarily good fits by continuously increasing
    *k*. Here is how the fits look when we set *k = 13*:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/84fb7ceac71bc3223536de55616f4355.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
- en: Fitting perfectly separated data would require log-losses to have a 90-degree
    angle and fitted probability to have an infinite slope at the mid-point. Therefore,
    there is no parameter *k* for which total log-loss has a minimum. In practice,
    numerical algorithms stop after some number of steps and may either return the
    value of *k* in the last step or an error.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: To be continued
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This post covers the main part of what logistic regression conceptually does.
    In the next part, we will cover the somewhat unintuitive meaning of parameter
    k as a log-odds-ratio and show how to run, and break, logistic models in Python
    libraries `statsmodels` and `scikit-learn`:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '[](/logistic-regression-deceptively-flawed-2c3e7f77eac9?source=post_page-----67560de4f492--------------------------------)
    [## Logistic Regression: Deceptively Flawed'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: When can large odds ratios and perfectly separated data bite you?
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/logistic-regression-deceptively-flawed-2c3e7f77eac9?source=post_page-----67560de4f492--------------------------------)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
