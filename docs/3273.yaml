- en: Lagrange Multipliers, KKT Conditions, and Duality — Intuitively Explained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/lagrange-multipliers-kkt-conditions-duality-intuitively-explained-de09f645b068?source=collection_archive---------3-----------------------#2023-11-03](https://towardsdatascience.com/lagrange-multipliers-kkt-conditions-duality-intuitively-explained-de09f645b068?source=collection_archive---------3-----------------------#2023-11-03)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Your key to understanding SVMs, Regularization, PCA, and many other machine
    learning concepts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://essamwissam.medium.com/?source=post_page-----de09f645b068--------------------------------)[![Essam
    Wisam](../Images/6320ce88ba2e5d56d70ce3e0f97ceb1d.png)](https://essamwissam.medium.com/?source=post_page-----de09f645b068--------------------------------)[](https://towardsdatascience.com/?source=post_page-----de09f645b068--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----de09f645b068--------------------------------)
    [Essam Wisam](https://essamwissam.medium.com/?source=post_page-----de09f645b068--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fccb82b9f3b87&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flagrange-multipliers-kkt-conditions-duality-intuitively-explained-de09f645b068&user=Essam+Wisam&userId=ccb82b9f3b87&source=post_page-ccb82b9f3b87----de09f645b068---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----de09f645b068--------------------------------)
    ·13 min read·Nov 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fde09f645b068&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flagrange-multipliers-kkt-conditions-duality-intuitively-explained-de09f645b068&user=Essam+Wisam&userId=ccb82b9f3b87&source=-----de09f645b068---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fde09f645b068&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flagrange-multipliers-kkt-conditions-duality-intuitively-explained-de09f645b068&source=-----de09f645b068---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: In this story, we will explore a clear and insightful grasp of three related
    concepts in mathematical optimization. These concepts required substantial time
    and effort for me to fully grasp, so I’ve aimed to present them in an intuitive
    way for all readers. Our journey will commence with a refresher on unconstrained
    optimization, followed by a consideration for constrained optimization, where
    we’ll utilize Lagrange Multipliers and KKT conditions. We also delve into the
    interplay between these ideas and their connections to the concept of duality.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, by the end of this story you will understand how to solve constrained
    and unconstrained optimization problems as well as be able to intuitively derive
    why such methods work.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/85a9dfa76b0f8b0c73ad39b85daf60f0.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Filip Mroz](https://unsplash.com/@mroz?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Unconstrained Optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/71be8bc42c3c9753a19ba5f9874dc427.png)'
  prefs: []
  type: TYPE_IMG
- en: Plot of a Multivariable Function by Cdang on [Wikimedia](https://commons.wikimedia.org/wiki/File:Surface3D_sinFoisSin_python_matplotlib.svg)
    CC BY-SA 4.0.
  prefs: []
  type: TYPE_NORMAL
- en: In unconstrained optimization, we are given a multivariable function *f(u)*
    and we want to find the value for the vector *u** where the value of the function
    *f(u*)* is optimum (maximum or minimum).
  prefs: []
  type: TYPE_NORMAL
- en: 'A function in general can have multiple maxima and minima as shown above. In
    classical machine learning and throughout this story, we will be mostly interested
    in convex functions (that are also sufficiently smooth). Being [convex](https://en.wikipedia.org/wiki/Convex_function)
    implies that the function has at most one optimum value (which is one minimum
    when the function at hand is a loss function) as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/694e1583a1664b47a985863d7cd250ea.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph for a 3D Surface by [Andrebis](https://commons.wikimedia.org/wiki/User:Andrebis)
    on [Wikipedia](https://en.wikipedia.org/wiki/Lagrange_multiplier#/media/File:As_wiki_lgm_parab.svg)
    CC BY-SA 3.0.
  prefs: []
  type: TYPE_NORMAL
- en: It’s much easier to deal with convex functions since otherwise it can be really
    hard to tell whether the minimum found is the lowest of all (i.e., the global
    minimum) and not just some local minimum. In general, even when there is one minimum
    value there can be many points that satisfy it (e.g., if it’s flat) we will pretend
    that this case doesn’t happen to simplify the explanation; assuming that it happens
    won’t change anything we derive whatsoever.
  prefs: []
  type: TYPE_NORMAL
- en: 'Performing unconstrained optimization on a given multivariable function *f(u)*
    is possible by solving *∇ᵤf(u) = 0\.* If *f(u)* is a function in *n* variables
    *(u***₁***, u*₂*,…,u*ₙ*)* then this is a system of *n* equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7ce5c143b36a8b273112ca1909d33633.png)'
  prefs: []
  type: TYPE_IMG
- en: Which once solved returns the optimal solution *u*=(u****₁***,u**₂*,…,u**ₙ*)*
    which is where the optimal value (e.g., minimum) occurs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8aab1194bce59f3a1becb388975da237.png)'
  prefs: []
  type: TYPE_IMG
- en: Tangent Plane on a Surface by Mike Run on [Wikimedia](https://commons.wikimedia.org/wiki/File:Law-of-reflection-for-curved-surfaces.svg)
    CC BY-SA 4.0.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see where this comes from, recall that:'
  prefs: []
  type: TYPE_NORMAL
- en: The normal vector to the tangent plane at any point *u* takes the form *(∂f(u)/∂u₁,
    ∂f(u)/∂u*₂, …, *∂f(u)/∂u*ₙ, *f(u))*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tangent plane at any minimum or maximum is horizontal (visually obvious)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hence, whenever *∇ᵤf(u) = 0* holds, there is a horizontal tangent plane at that
    point and thus, must be the minimum we are looking for.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another way to rationalize this which will be useful shortly, is to observe
    that the gradient points towards the direction of greatest increase (AND opposite
    to the direction of greatest decrease). Thus, when *∇ᵤf(u) = 0* holds, it must
    either be not possible to (there is no direction that would) increase the function
    from that point (i.e., at maximum) or decrease the function from that point (i.e.,
    at a minimum).
  prefs: []
  type: TYPE_NORMAL
- en: '**Unconstrained Optimization Summary**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Given:** *f(u)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Wanted:** *u where f(u) is minimum*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Approach:** Solve *∇ᵤf(u)* = 0 since that holds at the minimum'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/501d8eb69a61133d8c17584aa8e7b288.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Jorge Reyna](https://unsplash.com/@jorgereyna?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Constrained Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this type of optimization, we are given an equality constraint of the form
    *g(u)=0* or g(u)≤0 (else we can put it in this form by rearranging terms or multiplying
    by a negative) and we want to optimize **over only all the points satisfying the
    constraint**.
  prefs: []
  type: TYPE_NORMAL
- en: We typically assume that equality constraints *g(u)=0* are affine (generalization
    of linear) and that inequality constraints g(u)≤0 involve convex functions so
    that the whole optimization problem is [convex](https://en.wikipedia.org/wiki/Convex_optimization)
    (otherwise, the fact that *f(u)* is convex alone may not be sufficient to guarantee
    one optimal value).
  prefs: []
  type: TYPE_NORMAL
- en: '**Constraint Optimization with Equality Constraints**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this, we are given a multivariable function *f(u)* and a constraint *g(u)=0*
    and we want to find the point *u** where *g(u*)=0* and *f(u*)* is minimum (i.e.,
    lowest possible point while satisfying the constraint).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5da79d58c0d542dd0b841c58611ef0f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Constrained Optimization by [Jacobmelgrad](https://commons.wikimedia.org/w/index.php?title=User%3AJacobmelgaard&action=edit&redlink=1)
    on [Wikipedia](https://en.wikipedia.org/wiki/Lagrange_multiplier#/media/File:Lagrange_very_simple.svg)
    CC BY-SA 3.0.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, in the example shown the objective function is *f(u₁,u*₂*) = u₁+
    u*₂ (3D plane) and the constraint is *u*²*₁+ u*²₂=1 (2D circle). The goal is to
    find the point (*u₁, u*₂) corresponding to the lowest point on the planewhere
    *u*²*₁+ u*²₂=1 holds *(*i.e., the (*u₁, u*₂) where the lowest point on the circle
    projected onto the plane occurs).
  prefs: []
  type: TYPE_NORMAL
- en: 'One approach to solve this type of constrained optimization problems is to
    use the method of Lagrange multipliers. In simple terms, the Lagrange multiplier
    theorem states that any solution *u** to an optimization problem of the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Minimize** *f(u)* **such that** *g(u)=0*'
  prefs: []
  type: TYPE_NORMAL
- en: must satisfy the equation*∇ᵤL(u*,λ)=0* forsome *λ*∈R *(*and trivially, *g(u*)=0)*
    where *L* is the Lagrangianfunction which is given by *L(u,λ)=f(u)+λg(u).* It
    assumes that *∇ᵤg(u*)≠0.*
  prefs: []
  type: TYPE_NORMAL
- en: It follows from this that we can solve constrained optimization problems with
    equality constraints as follows (assuming *∇ᵤg(u*)≠0):*
  prefs: []
  type: TYPE_NORMAL
- en: Write the Lagrangian function *L(u,λ)=f(u)+λg(u).*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solve *n+1* equations resulting from *∇ᵤL(u,λ)* = 0 (*n* equations) with *g(u)=0*
    to find the *n+1* unknowns *u****₁***,u**₂*,…,u**ₙ,*λ*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The solution is *u*=(u****₁***,u**₂*,…,u**ₙ*)*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*λ* is called a Lagrange multiplier. We only need to find it because it’s part
    of the system that yields the solution *u*.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can work out the example corresponding to the figure above [here](https://en.wikipedia.org/wiki/Lagrange_multiplier#Example_1).
    In this example, the problem is not convex and solving should yield any minimum
    or maximum present. Notice that steps (1) and (2) above are equivalent to performing
    unconstrained optimization on *L(u,λ)=f(u)+λg(u).* That is, setting:'
  prefs: []
  type: TYPE_NORMAL
- en: '*∇ L(u,λ)* =*(∂L(u)/∂u₁, ∂L(u)/∂u*₂, …, *∂L(u)/∂u*ₙ, *∂L(u)/∂λ)=* 0'
  prefs: []
  type: TYPE_NORMAL
- en: In this sense, this method of Lagrange multipliers is powerful in that it casts
    a constrained optimization problem into an unconstrained optimization problem
    which we can solve by simply setting the gradient as zero.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5da79d58c0d542dd0b841c58611ef0f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Constrained Optimization by [Jacobmelgrad](https://commons.wikimedia.org/w/index.php?title=User%3AJacobmelgaard&action=edit&redlink=1)
    on [Wikipedia](https://en.wikipedia.org/wiki/Lagrange_multiplier#/media/File:Lagrange_very_simple.svg)
    CC BY-SA 3.0.
  prefs: []
  type: TYPE_NORMAL
- en: '**Rationale**'
  prefs: []
  type: TYPE_NORMAL
- en: It’s not hard to derive with intuition why this works. The **feasible region**
    is the set of points that satisfy the problem’s constraints (e.g., those on the
    circle above); we want to find, among such points, the point where the objective
    function is optimum.
  prefs: []
  type: TYPE_NORMAL
- en: We know that *∇ f(u)* points opposite to the direction of the greatest decrease
    (along the direction of the greatest increase). However, in our case, we are only
    allowed to move in the feasible region (points satisfying the constraint); thus,
    to minimize the function under the constraint, we should want to move in the direction
    of greatest decrease *along* the constraint curve (so we never exit the feasible
    region and reach the min).
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose the direction of the tangent at point *u* on the constraint curve is
    given by *r(u)*, then recalling the formula for [vector projection](https://en.wikipedia.org/wiki/Vector_projection),
    we want to move opposite to the following direction (*∇ f(u)* projection on *r(u))*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b29b634b577eb5476a6c9008e40d3c1.png)'
  prefs: []
  type: TYPE_IMG
- en: Similar to the unconstrained case above, it should dawn on you that whenever
    this is 0, we can’t move in any direction along the constraint to further increase
    *f(u)* (if at a maximum) or decrease it (if at a minimum).
  prefs: []
  type: TYPE_NORMAL
- en: It’s obvious that for this to be zero, we need *r(u)≠0* (so the denominator
    isn’t zero) and *∇ f(u) ⋅ r(u)=0\.* For the latter,We know that the normal vector
    on the constraint curve*∇ g(u)* is perpendicular to the tangent *r(u).* Hence,
    all we need is*∇ f(u)* to be parallel to *∇ g(u).*
  prefs: []
  type: TYPE_NORMAL
- en: 'Thereby, it must hold at the optimal point *u** that:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The normal to the constraint is nonzero: *∇ g(u*)≠0* (so that *r(u*)≠0)*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The constraint is satisfied: *g(u*)=0* (trivial requirement)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*∇ f(u*)* ∥*∇ g(u*)*: there exists some real β where*∇ f(u*) =* β*∇ g(u*)*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Notice that by rearranging terms and renaming -β, (3) is equivalent to “there
    exists some real *λ* where *∇ f(u)+λ∇ g(u)=0”.* In other words, *∇ᵤL(u,λ)* = 0,
    and by that, we have intuitively derived the Lagrange multipliers theorem for
    one constraint (scroll up if needed).
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the first condition is called a constraint qualification. If it
    isn’t satisfied by the constraint at a point where (2) and (3) are satisfied,
    then there are no guarantees that this point is optimal as the projection is not
    defined there.
  prefs: []
  type: TYPE_NORMAL
- en: '**Multiple Equality Constraints**'
  prefs: []
  type: TYPE_NORMAL
- en: 'When multiple constraints g*₁(u), g*₂*(u),…,g*ₖ*(u)* are present, the method
    smoothly generalizes to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Write the Lagrangian *L(u,λ₁,λ*₂*,…,λ*ₖ*) = f(u) + λ₁*g*₁(u) + λ*₂g₂*(u) +…+λ*ₖgₖ*(u)*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solve *n+k* equations by setting*∇ᵤL(u,λ₁,λ*₂*,…,λ*ₖ*)* = 0 (*n* equations)
    with *g₁(u)=0, g*₂*(u)=0, …, g*ₖ*(u)=0* to find *n+k* unknowns *u****₁***,u**₂*,…,u**ₙ,
    *λ₁,λ*₂*,…,λ*ₖ
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The solution is *u*=(u****₁***,u**₂*,…,u**ₙ*)*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The assumption*∇ g(u*)*≠*0* generalizes to that *∇* g*₁(u), ∇ g*₂*(u),…,∇ g*ₖ*(u)*
    must be linearly independent. This is called LICQ (linear independence constraint
    qualification).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/32bacc9722a73cdd828171b12bfcfa6d.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Jairph](https://unsplash.com/@jairph?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Constraint Optimization with Inequality Constraints
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Matters don’t get much more complex when we are rather dealing with an inequality
    constraint of the form *g(u)≤0*. In this case, we want the optimum point of *f(u)*
    that satisfies *g(u)≤0.*
  prefs: []
  type: TYPE_NORMAL
- en: For the problem above, this means that the feasible region is not just the points
    on the circle, it’s also the points inside it. It should be obvious that for that
    particular problem (and not generally), this does not change the solution.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/26889ebcefeed62886e273e6efe884d3.png)'
  prefs: []
  type: TYPE_IMG
- en: Constrained Optimization by [Jacobmelgrad](https://commons.wikimedia.org/w/index.php?title=User%3AJacobmelgaard&action=edit&redlink=1)
    on [Wikipedia](https://en.wikipedia.org/wiki/Lagrange_multiplier#/media/File:Lagrange_very_simple.svg)
    CC BY-SA 3.0\. Modified by Shading.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of solving the two conditions of Lagrange multipliers (2, 3) we solve
    a set of four conditions called KKT conditions that generalize the Lagrange multipliers
    case. We can derive them as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6650ca453c20451a371f2b4e1249283e.png)'
  prefs: []
  type: TYPE_IMG
- en: Inequality constraint diagram for optimization problems by [Onmyphd](https://commons.wikimedia.org/w/index.php?title=User%3AOnmyphd&action=edit&redlink=1)
    on [Wikipedia](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions#/media/File:Inequality_constraint_diagram.svg)
    CC BY-SA 3.0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Observe that with an arbitrary hypersurface *f(u)* and constraint *g(u)≤0,*
    there are exactly two possibilities assuming a convex smooth function with one
    optimum:'
  prefs: []
  type: TYPE_NORMAL
- en: The optimum point *uᴾ* lies inside the feasible region.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this case, the solution to the optimization problem *u** must be *uᴾ* and
    *g(u*)<0* must hold (left image).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s impossible to find a more optimum point in the feasible region because
    *uᴾ* is the most optimal point (e.g., minimum) over the entire region (domain)
    of *f(u).*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2\. The optimum point *uᴾ* lies outside the feasible region.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, *f(u)* must be only decreasing in the feasible region if that
    point is a maximum (can’t increase again else creates another optimal point)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*f(u)* must be only increasing in the feasible region if that point is a minimum
    (can’t decrease again else creates another optimal point)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, the optimum point *u** must be at the edge of the feasible region as it
    never gets better inside (*g(u*) = 0* must hold)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the first case, it’s obvious that solving the optimization problem is equivalent
    to solving the unconstrained version of it.
  prefs: []
  type: TYPE_NORMAL
- en: '*∇ᵤf(u) = 0*'
  prefs: []
  type: TYPE_NORMAL
- en: We say that the constraint is “inactive because” it doesn’t make a difference
    in the optimization problem.
  prefs: []
  type: TYPE_NORMAL
- en: In the second case, it’s obvious that solving the optimization problem is equivalent
    to solving the equality constraint version of it (Lagrange multipliers).
  prefs: []
  type: TYPE_NORMAL
- en: The only catch for that case is that *λ* must be ≥ 0 for minimization and must
    be ≤ 0 for maximization. For minimization, this implies that *∇ᵤf(u)* and *∇ᵤg(u)*
    point in opposite directions (i.e., β in*∇ᵤ f(u) =* β*∇ ᵤg(u)* is ≤0) which has
    to hold because *∇ᵤg(u)* points towards the positive side of the constraint *g(u)≥0*
    (basic property)*;* meanwhile, *∇ᵤ f(u)* points to the negative side of the constraint
    because that’s where *f(u)* is increasing. A similar argument can be easily constructed
    for the case of maximization.
  prefs: []
  type: TYPE_NORMAL
- en: 'But we don’t know beforehand which of these two cases applies. We can merge
    their methods as follows (assuming minimization):'
  prefs: []
  type: TYPE_NORMAL
- en: Write the Lagrangian function *L(u,λ)=f(u)+λg(u)*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *∇ᵤL(u,λ)* = 0 (*n* equations) and *g(u)≤0*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Solve for the solution (*u****₁***,u**₂*,…,u**ₙ,*λ.*) where one of the cases
    above applies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*λ=0* and *g(u*)<0 (*first case as *λ=0* means that *∇ᵤL(u,λ)* = *∇ᵤf(u) =
    0* so steps 1,2 are equivalent to solving *∇ᵤf(u) = 0*)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*g(u*)=0* and *λ≥0 (*second case as *g(u)=0* means that applying Lagrange is
    correct and that’s what we did)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can summarize these two bullets in that *g(u*)≤0* and *λ≥0* must hold and
    that *λg(u*)=0* must hold (one of *λ* or *g(u*)* must be zero). This implies that
    given an optimization problem of the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Minimize** *f(u)* **such that** *g(u)≤0*'
  prefs: []
  type: TYPE_NORMAL
- en: We expect the following four conditions to be satisfied for the optimal point
    *u*:*
  prefs: []
  type: TYPE_NORMAL
- en: 'Stationarity: *∇ᵤL(u*,λ)* = 0'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Primal Feasibility: *g(u*)≤0*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Dual Feasibility: *λ≥0*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Complementary Slackness: *λg(u*)=0*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: and that solving these conditions together yields the optimal point u*. In reality
    for [convex problems](https://en.wikipedia.org/wiki/Convex_optimization), these
    conditions are sufficient but not necessary for optimality. That is,
  prefs: []
  type: TYPE_NORMAL
- en: If a point satisfies these conditions (e.g., found by solving them together)
    then that suffices to prove that the point is optimal (no need to look further
    for convex problems).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meanwhile, these conditions are not necessary for a point to be optimal. It
    is possible that solving the conditions gives no solution when in reality there
    is an optimal point that doesn’t satisfy them. For example, consider *f(x) = x*
    and the constraint *x² ≤ 0* (both [this and another KKT example](https://drive.google.com/drive/u/1/folders/1uQ9iiadmIY-hg2DnjQaYfE3xNHgMdWrN)
    are solved in this document)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once we enforce a constraint qualification such as LICQ (stated earlier), we
    can guarantee that KKT conditions are both sufficient and necessary. An alternative
    constraint qualification that is easier to check is Slater’s condition which guarantees
    that KKT is necessary and sufficient for convex problems.
  prefs: []
  type: TYPE_NORMAL
- en: Slater’s condition simply states that the feasible region must have an interior
    point. That is, for a constraint *g(u)≤0* the function must have point(s) in the
    domain of *f(u)* satisfying *g(u)<0\.* This is a basic condition that is almost
    always satisfied in real-life problems (but not the counterexample above), and
    this means that KKT will rarely ever miss finding an optimal solution.
  prefs: []
  type: TYPE_NORMAL
- en: '**Multiple Constraints**'
  prefs: []
  type: TYPE_NORMAL
- en: 'When multiple equality constraints h*₁(u), h*₂*(u),…,h*ₖ*(u)* are present along
    with multiple inequality constraints g*₁(u), g*₂*(u),…,g*ₚ*(u)*, the method smoothly
    generalizes by writing the full Lagrangian and checking the KKT conditions only
    for inequality constraints and their multipliers (which we will call α):'
  prefs: []
  type: TYPE_NORMAL
- en: 0\. Write the Lagrangian
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7529d8b5c057f5fd95356441a226b6f4.png)'
  prefs: []
  type: TYPE_IMG
- en: Set*∇ᵤL(u,λ₁,λ*₂*,…,λ*ₖ, α*₁*, α₂, …, αₚ*)* = 0 (n equations)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2\. Set h*₁(u)=0, h*₂*(u)=0, …, h*ₖ*(u)=0 (k* equations) and set
  prefs: []
  type: TYPE_NORMAL
- en: g*₁(u)≤0, g*₂*(u)≤0, …, g*ₚ*(u)≤0* (*p* inequalities)
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Set α*₁≥0*, α₂≥0, …, αₚ≥0 (*p* inequalities)
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Set α*₁*g*₁(u) =* α₂*g*₂*(u) =* αₚ*g*ₖ*(u) = 0* (*p* equations)
  prefs: []
  type: TYPE_NORMAL
- en: In total, you have *n+k+p* equations and *2p* inequalities that you will solve
    together to find *n+k+p* variables *(u****₁***,u**₂*,…,u**ₙ,*λ₁,λ*₂*,…,λ*ₖ,α*₁*,
    α₂, …, αₚ *)* which would yield the solution *u*=(u****₁***,u**₂*,…,u**ₙ*)* that
    minimizes the function while satisfying the *k+p* constraints.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f4abf591968faa51746f8c2305eb5988.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [SpaceX](https://unsplash.com/@spacex?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The Duality Principle
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The duality principle simply states that for any optimization problem, we can
    write a dual optimization problem that when solved either tells us something about
    the original problem (called primal) or solves it.
  prefs: []
  type: TYPE_NORMAL
- en: 'For any optimization problem of the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e18b1d046a18f6de01955a0da13db128.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The dual optimization problem takes the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a6e209be534cbc88f290fa80842e2d55.png)'
  prefs: []
  type: TYPE_IMG
- en: Yes, what’s minimized takes the same form as the Lagrangian
  prefs: []
  type: TYPE_NORMAL
- en: and vice versa if it is maximization.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example**'
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, the constrained optimization problem that we discussed earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/00f3d920a058f61dfc08cee96350b8ae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Has the corresponding dual problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c467f002fc40352291fe47889beb5986.png)'
  prefs: []
  type: TYPE_IMG
- en: As basic calculus suggests, to carry on the minimization first we do
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2cfd493b0d031b1d57d9e2d56a771614.png)'
  prefs: []
  type: TYPE_IMG
- en: which implies
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a26caebb7ae806f60f9abca6d8c2fa8c.png)'
  prefs: []
  type: TYPE_IMG
- en: and thereby, the optimization problem becomes
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/84309110d276e7589f29470011a95b29.png)'
  prefs: []
  type: TYPE_IMG
- en: and all it takes now is to differentiate this and equate to zero to get λ =
    1/√2 which implies that *(x*, y*)* = (−1/√2, −1/√2) and which is the same solution
    that we had by solving the primal problem via KKT.
  prefs: []
  type: TYPE_NORMAL
- en: '**Deriving the Dual**'
  prefs: []
  type: TYPE_NORMAL
- en: The primal (original) problem is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f01b58c1873fac972a959be472e5b2fd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Suppose we define a function that returns infinity whenever *u* is not in the
    feasible region (doesn’t satisfy the constraint), and zero otherwise:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/850a15a476043605c867247c376fb50b.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, the primal problem is equivalent to
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3907fd374957f9da31adb34a672209be.png)'
  prefs: []
  type: TYPE_IMG
- en: This should make sense because *f(u)+P(u)* sets anything outside of the feasible
    region to infinity and leaves the feasible region as is. The minimum of this sum
    must occur in the feasible region even though the constraint is enforced explicitly
    because infinity is greater than anything.
  prefs: []
  type: TYPE_NORMAL
- en: 'Observe that we can claim that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/112ce6363f6219b8ee7073e4ef6f1a6d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Because with this, if:'
  prefs: []
  type: TYPE_NORMAL
- en: '*g(u)<0* then *P(u)=0* as defined earlier since *λ=0* must hold to maximize
    the quantity *λg(u)* (else it’s negative due to *g(u)<0*)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*g(u)=0* then *P(u)=0* as defined earlier as *λg(u)* will be zero (*λ* can
    be anything)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*g(u)>0* then *P(u)=*∞ as defined earlier as *λ=*∞ is what would maximize *λg(u)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thereby, the primal problem is equivalent to
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0873677c905f0b22d8e443aa4709e91e.png)'
  prefs: []
  type: TYPE_IMG
- en: It’s okay to introduce f to the Max since it isn’t an explicit function in *λ*
  prefs: []
  type: TYPE_NORMAL
- en: The difference between this and the dial problem is that in the dual the Max
    and Min are swapped.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f8f418d86b425be55e95b7913751c435.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, because in general *MinMax(…) ≥ MaxMin(…)* a solution to the dual would
    be a lower bound to the solution of the primal. This is called weak duality.
  prefs: []
  type: TYPE_NORMAL
- en: A more interesting case is when *MinMax(…) = MaxMin(…)* where a solution to
    the dual would be exactly the solution to the primal as well (as in the example).
    This is called strong duality. You can moderatily easily [prove that](https://or.stackexchange.com/questions/3117/is-there-any-relationship-between-kkt-and-duality)
    the equality holds (and hence strong duality) when KKT is both necessary and sufficient.
    In other words, strong duality will hold for convex problems whenever Slater’s
    condition holds!
  prefs: []
  type: TYPE_NORMAL
- en: '**So What?**'
  prefs: []
  type: TYPE_NORMAL
- en: If you think about it, solving the dual problem is akin to only applying the
    stationarity and dual feasibility conditions of KKT on the primal problem. Instead
    of applying primal feasibility and complementary slackness you get to deal with
    an extra minimization over dual variables. In many cases, this is much easier
    than solving KKT on the primal problem. The extra minimization can be for instance
    tackled with linear or quadratic programming.
  prefs: []
  type: TYPE_NORMAL
- en: '**Multiple Constraints?**'
  prefs: []
  type: TYPE_NORMAL
- en: In generalizing to multiple constraints, the Lagrangian changes just as you
    would expect (similar to what we have seen) and we only add α≥0 conditions in
    maximization to multipliers associated with inequality constraints.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c17f9f792aabbe00b5ba338c1a4fcbdb.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Hyundai Motor Group](https://unsplash.com/@hyundaimotorgroup?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Hope this story has helped you truly understand unconstrained optimization,
    Lagrange Mulipliers, KKT and duality. Till next time, au revoir!
  prefs: []
  type: TYPE_NORMAL
