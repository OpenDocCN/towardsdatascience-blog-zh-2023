- en: 'Ensemble Learning: Bagging and Boosting'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/ensemble-learning-bagging-and-boosting-23f9336d3cb0?source=collection_archive---------0-----------------------#2023-02-23](https://towardsdatascience.com/ensemble-learning-bagging-and-boosting-23f9336d3cb0?source=collection_archive---------0-----------------------#2023-02-23)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@jonas_dieckmann?source=post_page-----23f9336d3cb0--------------------------------)[![Jonas
    Dieckmann](../Images/e1f2d236e6bda6ec1e14fd5eaa9d205e.png)](https://medium.com/@jonas_dieckmann?source=post_page-----23f9336d3cb0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----23f9336d3cb0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----23f9336d3cb0--------------------------------)
    [Jonas Dieckmann](https://medium.com/@jonas_dieckmann?source=post_page-----23f9336d3cb0--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1c8d1cf684f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-learning-bagging-and-boosting-23f9336d3cb0&user=Jonas+Dieckmann&userId=1c8d1cf684f2&source=post_page-1c8d1cf684f2----23f9336d3cb0---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----23f9336d3cb0--------------------------------)
    ·11 min read·Feb 23, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F23f9336d3cb0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-learning-bagging-and-boosting-23f9336d3cb0&user=Jonas+Dieckmann&userId=1c8d1cf684f2&source=-----23f9336d3cb0---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F23f9336d3cb0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-learning-bagging-and-boosting-23f9336d3cb0&source=-----23f9336d3cb0---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: Would you like to take your data science skills to the next level? Are you interested
    in improving the accuracy of your models and making more informed decisions based
    on your data? Then it’s time to explore the world of bagging and boosting. With
    these powerful techniques, you can improve the performance of your models, reduce
    errors and make more accurate predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4986fb00c9a65b94a926de13aad60077.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Unsplash](https://unsplash.com/)
  prefs: []
  type: TYPE_NORMAL
- en: 'Whether you are working on a classification problem, a regression analysis,
    or another data science project, bagging and boosting algorithms can play a crucial
    role. In this article, we #1 **summarize the main idea of ensemble learning,**
    introduce both,#2 **bagging** and #3 **boosting,** before we finally #4 **compare
    both methods** to highlight similarities and differences.'
  prefs: []
  type: TYPE_NORMAL
- en: So let’s get ready for bagging and boosting to succeed!
  prefs: []
  type: TYPE_NORMAL
- en: '#1: Introduction and main idea: ensemble learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So when should we use it? Cleary, when we see overfitting or underfitting of
    our models. Let’s begin with the key concept of bagging and boosting, which both
    belong to the family of ensemble learning techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/62fe400c7e15d494a8434ff17c29e088.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: The main idea behind ensemble learning is the usage of multiple algorithms and
    models that are used together for the same task. While single models use only
    one algorithm to create prediction models, bagging and boosting methods aim to
    combine several of those to achieve better prediction with higher consistency
    compared to individual learnings.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example: Image classification**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The essential concept is encapsulated by means of a didactic illustration involving
    image classification. Supposing a collection of images, each accompanied by a
    categorical label corresponding to the kind of animal, is available for the purpose
    of training a model. In a traditional modeling approach, we would try several
    techniques and calculate the accuracy to choose one over the other. Imagine we
    used logistic regression, decision tree, and support vector machines here that
    perform differently on the given data set.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3930d9d6beb52266303d80bf4d46440e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: In the above example, it was observed that a specific record was predicted as
    a dog by the logistic regression and decision tree models, while a support vector
    machine identified it as a cat. As various models have their distinct advantages
    and disadvantages for particular records, it is the key idea of ensemble learning
    to combine all three models instead of selecting only one approach that showed
    the highest accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: The procedure is called *aggregation* or *voting* and combines the predictions
    of all underlying models, to come up with one prediction that is assumed to be
    more precise than any sub-model that would stay alone.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bias-Variance tradeoff**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next chart might be familiar to some of you, but it represents quite well
    the relationship and the tradeoff between bias and variance on the test error
    rate.
  prefs: []
  type: TYPE_NORMAL
- en: You might be familiar with the following concept, but I posit that it effectively
    illustrates the correlation and compromise between bias and variance with respect
    to the testing error rate.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d665a9b49ef57d63a4d3e054d49e93b0.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: The relationship between the variance and bias of a model is such that a reduction
    in variance results in an increase in bias, and vice versa. To achieve optimal
    performance, the model must be positioned at an equilibrium point, where the test
    error rate is minimized, and the variance and bias are appropriately balanced.
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble learning can help to balance both extreme cases to a more stable prediction.
    One method is called bagging and the other is called boosting.
  prefs: []
  type: TYPE_NORMAL
- en: '#2: Bagging (bootstrap aggregation)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/3f8382bcb505d60bb288f60b84f52178.png)'
  prefs: []
  type: TYPE_IMG
- en: A random bag. Image by [Unsplash](https://unsplash.com/)
  prefs: []
  type: TYPE_NORMAL
- en: Let us focus first on the Bagging technique called bootstrap aggregation. Bootstrap
    aggregation aims to solve the right extreme of the previous chart by **reducing
    the variance** of the model to avoid overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/00958b27e492116b8530c18c220c752a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: With this purpose, the idea is to have multiple models of the same learning
    algorithm that are trained by random subsets of the original training data. Those
    random subsets are called bags and can contain any combination of the data. Each
    of those datasets is then used to fit an individual model which produces individual
    predictions for the given data. Those predictions are then aggregated into one
    final classifier. The idea of this method is really close to our initial toy example
    with the cats and dogs.
  prefs: []
  type: TYPE_NORMAL
- en: Using random subsets of data, the risk of overfitting is reduced and flattened
    by averaging the results of the sub-models. All models are calculated in parallel
    and then aggregated together afterward.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e34968548748020d1df14d4d3a50d3a1.png)'
  prefs: []
  type: TYPE_IMG
- en: The calculation of the final ensemble aggregation uses either the simple average
    for regression problems or a simple majority vote for classification problems.
    For that, each model from each random sample produces a prediction for that given
    subset. For the average, those predictions are just summed up and divided by the
    number of created bags.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5ab048780de4ab27ebf06a3094d18e28.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: A simple majority voting works similarly but uses the predicted classes instead
    of numeric values. The algorithm identifies the class with the most predictions
    and assumes that the majority is the final aggregation. This is again very similar
    to our toy example, where two out of three algorithms predicted a picture to be
    a dog and the final aggregation was therefore a dog prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '**Random Forest**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A famous extension to the bagging method is the random forest algorithm, which
    uses the idea of bagging but uses also subsets of the features and not only subsets
    of the entries. Bagging, on the other hand, takes all given features into account.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Code example for bagging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the following, we will explore some useful python functions from the `sklearn.ensemble`library.
    The function called `BaggingClassifier`has a few parameters which can be looked
    up in the documentation, but the most important ones are *base_estimator*, *n_estimators*,
    and *max_samples*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*base_estimator:* You have to provide the underlying algorithm that should
    be used by the random subsets in the bagging procedure in the first parameter.
    This could be for example Logistic Regression, Support Vector Classification,
    Decision trees, or many more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*n_estimators:* The number of estimators defines the number of bags you would
    like to create here and the default value for that is 10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*max_samples:* The maximum number of samples defines how many samples should
    be drawn from X to train each base estimator. The default value here is one point
    zero which means that the total number of existing entries should be used. You
    could also say that you want only 80% of the entries by setting it to 0.8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After setting the scenes, this model object works like many other models and
    can be trained using the `fit()`procedure including X and y data from the training
    set. The corresponding predictions on test data can be done using `predict()`.
  prefs: []
  type: TYPE_NORMAL
- en: '#3: Boosting'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/03bf053342aca7b72583d4ffecf49a87.png)'
  prefs: []
  type: TYPE_IMG
- en: Boost your models! Image by [Unsplash](https://unsplash.com/)
  prefs: []
  type: TYPE_NORMAL
- en: Boosting is a little variation of the bagging algorithm and uses sequential
    processing instead of parallel calculations. While bagging aims to reduce the
    variance of the model, the boosting method tries aims to **reduce the bias** to
    avoid underfitting the data. With that idea in mind, boosting also uses a random
    subset of the data to create an average-performing model on that.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d313fbfc44511d468ea2155c21d9d67c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'For that, it uses the miss-classified entries of the weak model with some other
    random data to create a new model. Therefore, the different models are not randomly
    chosen but are mainly influenced by wrong classified entries of the previous model.
    The steps for this technique are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Train initial (weak) model**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You create a subset of the data and train a weak learning model which is assumed
    to be the final ensemble model at this stage. You then analyze the results on
    the given training data set and can identify those entries that were misclassified.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Update weights and train a new model** You create a new random subset of
    the original training data but weight those misclassified entries higher. This
    dataset is then used to train a new model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Aggregate the new model with the ensemble model**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next model should perform better on the more difficult entries and will
    be combined (aggregated) with the previous one into the new final ensemble model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Essentially, we can repeat this process multiple times and continuously update
    the ensemble model until our prediction power is good enough. The key idea here
    is clearly to create models that are also able to predict the more difficult data
    entries. This can then lead to a better fit of the model and reduces the bias.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2a3c0c3f3f39bc5f69d59934be3ac337.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: In comparison to Bagging, this technique uses weighted voting or weighted averaging
    based on the coefficients of the models that are considered together with their
    predictions. Therefore, this model can reduce underfitting, but might also tend
    to overfit sometimes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/62b660a8a0fbf2f39723e65cf44e651d.png)'
  prefs: []
  type: TYPE_IMG
- en: Code example for boosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the following, we will look at a similar code example but for boosting. Obviously,
    there exist multiple boosting algorithms. Besides the `GradientDescent` methodology,
    the `AdaBoost` is one of the most popular.
  prefs: []
  type: TYPE_NORMAL
- en: '*base_estimator:* Similar to Bagging, you need to define which underlying algorithm
    you would like to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*n_estimators:* The amount of estimators defines the maximum number of iterations
    at which the boosting is terminated. It is called the “maximum” number, because
    the algorithm will stop on its own, in case good performance is achieved earlier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*learning_rate*: Finally, the learning rate controls how much the new model
    is going to contribute to the previous one. Normally there is a trade-off between
    the number of iterations and the value of the learning rate. In other words: when
    taking smaller values of the learning rate, you should consider more estimators,
    so that your base model (the weak classifier) continues to improve.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `fit()`and `predict()`procedures work similarly to the previous bagging
    example. As you can see, it is easy to use such functions from existing libraries.
    But of course, you can also implement your own algorithms to build both techniques.
  prefs: []
  type: TYPE_NORMAL
- en: '#4: Conclusion: differences & similarities'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/3e703af4c2b3ab64c9aac40260fa66cb.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Unsplash](https://unsplash.com/)
  prefs: []
  type: TYPE_NORMAL
- en: Since we learned briefly how bagging and boosting work, I would like to put
    the focus now on comparing both methods against each other.
  prefs: []
  type: TYPE_NORMAL
- en: Similarities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Ensemble methods**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a general view, the similarities between both techniques start with the fact
    that both areensemble methods with the aim to use multiple learners over a single
    model to achieve better results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Multiple samples & aggregation**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To do that, both methods generate random samples and multiple training data
    sets. It is also similar that Bagging and Boosting both arrive at the end decision
    by aggregation of the underlying models: either by calculating average results
    or by taking a voting rank.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Purpose**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, it is reasonable that both aim to produce higher stability and better
    prediction for the data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Differences
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Data partition |** whole data vs. bias'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While bagging uses random bags out of the training data for all models independently,
    boosting puts higher importance on misclassified data of the upcoming models.
    Therefore, the data partition is different here.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Models |** independent vs. sequences'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bagging creates independent models that are aggregated together. However, boosting
    updates the existing model with the new ones in a sequence. Therefore, the models
    are affected by previous builds.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Goal |** variance vs. biasAnother difference is the fact that bagging aims
    to reduce the variance, but boosting tries to reduce the bias. Therefore, bagging
    can help to decrease overfitting, and boosting can reduce underfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Function |** weighted vs. non-weighted'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final function to predict the outcome uses equally weighted average or equally
    weighted voting aggregations within the bagging technique. Boosting uses weighted
    majority vote or weighted average functions with more weight to those with better
    performance on training data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Implications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It was shown that the main idea of both methods is to use multiple models together
    to achieve better predictions compared so single learning models. However, there
    is *no one-over-the-other statement* to choose between bagging and boosting since
    both have advantages and disadvantages.
  prefs: []
  type: TYPE_NORMAL
- en: While bagging decreases the variance and reduces overfitting, it will only rarely
    produce better bias. Boosting on the other hand side decreases the bias but might
    be more overfitted that bagged models.
  prefs: []
  type: TYPE_NORMAL
- en: Coming back to the variance-bias tradeoff figure, I tried to visualize the extreme
    cases when each method seems appropriate. However, this does not mean that they
    achieve the results without any drawbacks. The aim should always be to keep bias
    and variance in a reasonable balance.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging and boosting both uses all given features and select only the entries
    randomly. Random forest on the other side is an extension to bagging that creates
    also random subsets of the features. Therefore, random forest is used more often
    in practice than bagging.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@jonas_dieckmann?source=post_page-----23f9336d3cb0--------------------------------)
    [## Jonas Dieckmann - Medium'
  prefs: []
  type: TYPE_NORMAL
- en: Read writing from Jonas Dieckmann on Medium. analytics manager & product owner
    @ philips | passionate and writing about…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@jonas_dieckmann?source=post_page-----23f9336d3cb0--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: I hope you find it useful. Let me know your thoughts! And feel free to connect
    on LinkedIn at [https://www.linkedin.com/in/jonas-dieckmann/](https://www.linkedin.com/in/jonas-dieckmann/)
    and/or to follow me here on medium.
  prefs: []
  type: TYPE_NORMAL
- en: 'See also some of my other articles:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[](/introduction-to-ica-independent-component-analysis-b2c3c4720cd9?source=post_page-----23f9336d3cb0--------------------------------)
    [## Introduction to ICA: Independent Component Analysis'
  prefs: []
  type: TYPE_NORMAL
- en: Have you ever found yourself in a situation where you were trying to analyze
    a complex and highly correlated data set…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/introduction-to-ica-independent-component-analysis-b2c3c4720cd9?source=post_page-----23f9336d3cb0--------------------------------)
    [](/how-to-set-started-with-tensorflow-using-keras-api-and-google-colab-5421e5e4ef56?source=post_page-----23f9336d3cb0--------------------------------)
    [## How to get started with TensorFlow using Keras API and Google Colab
  prefs: []
  type: TYPE_NORMAL
- en: Step-by-step tutorial to analyze human activity with neuronal networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-to-set-started-with-tensorflow-using-keras-api-and-google-colab-5421e5e4ef56?source=post_page-----23f9336d3cb0--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1]: Bühlmann, Peter. (2012). Bagging, Boosting and Ensemble Methods. Handbook
    of Computational Statistics. 10.1007/978–3–642–21551–3_33.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2]: Machova, Kristina & Puszta, Miroslav & Barcák, Frantisek & Bednár, Peter.
    (2006). A comparison of the bagging and the boosting methods using the decision
    trees classifiers. Comput. Sci. Inf. Syst.. 3\. 57–72\. 10.2298/CSIS0602057M.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3]: Banerjee, Prashant. Bagging vs Boosting @kaggle: [https://www.kaggle.com/prashant111/bagging-vs-boosting](https://www.kaggle.com/prashant111/bagging-vs-boosting)'
  prefs: []
  type: TYPE_NORMAL
