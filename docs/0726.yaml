- en: 'Ensemble Learning: Bagging and Boosting'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成学习：袋装和提升
- en: 原文：[https://towardsdatascience.com/ensemble-learning-bagging-and-boosting-23f9336d3cb0?source=collection_archive---------0-----------------------#2023-02-23](https://towardsdatascience.com/ensemble-learning-bagging-and-boosting-23f9336d3cb0?source=collection_archive---------0-----------------------#2023-02-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/ensemble-learning-bagging-and-boosting-23f9336d3cb0?source=collection_archive---------0-----------------------#2023-02-23](https://towardsdatascience.com/ensemble-learning-bagging-and-boosting-23f9336d3cb0?source=collection_archive---------0-----------------------#2023-02-23)
- en: '[](https://medium.com/@jonas_dieckmann?source=post_page-----23f9336d3cb0--------------------------------)[![Jonas
    Dieckmann](../Images/e1f2d236e6bda6ec1e14fd5eaa9d205e.png)](https://medium.com/@jonas_dieckmann?source=post_page-----23f9336d3cb0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----23f9336d3cb0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----23f9336d3cb0--------------------------------)
    [Jonas Dieckmann](https://medium.com/@jonas_dieckmann?source=post_page-----23f9336d3cb0--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jonas_dieckmann?source=post_page-----23f9336d3cb0--------------------------------)[![Jonas
    Dieckmann](../Images/e1f2d236e6bda6ec1e14fd5eaa9d205e.png)](https://medium.com/@jonas_dieckmann?source=post_page-----23f9336d3cb0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----23f9336d3cb0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----23f9336d3cb0--------------------------------)
    [Jonas Dieckmann](https://medium.com/@jonas_dieckmann?source=post_page-----23f9336d3cb0--------------------------------)'
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1c8d1cf684f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-learning-bagging-and-boosting-23f9336d3cb0&user=Jonas+Dieckmann&userId=1c8d1cf684f2&source=post_page-1c8d1cf684f2----23f9336d3cb0---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----23f9336d3cb0--------------------------------)
    ·11 min read·Feb 23, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F23f9336d3cb0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-learning-bagging-and-boosting-23f9336d3cb0&user=Jonas+Dieckmann&userId=1c8d1cf684f2&source=-----23f9336d3cb0---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1c8d1cf684f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-learning-bagging-and-boosting-23f9336d3cb0&user=Jonas+Dieckmann&userId=1c8d1cf684f2&source=post_page-1c8d1cf684f2----23f9336d3cb0---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----23f9336d3cb0--------------------------------)
    ·11分钟阅读·2023年2月23日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F23f9336d3cb0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-learning-bagging-and-boosting-23f9336d3cb0&user=Jonas+Dieckmann&userId=1c8d1cf684f2&source=-----23f9336d3cb0---------------------clap_footer-----------)'
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F23f9336d3cb0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-learning-bagging-and-boosting-23f9336d3cb0&source=-----23f9336d3cb0---------------------bookmark_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F23f9336d3cb0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-learning-bagging-and-boosting-23f9336d3cb0&source=-----23f9336d3cb0---------------------bookmark_footer-----------)'
- en: Would you like to take your data science skills to the next level? Are you interested
    in improving the accuracy of your models and making more informed decisions based
    on your data? Then it’s time to explore the world of bagging and boosting. With
    these powerful techniques, you can improve the performance of your models, reduce
    errors and make more accurate predictions.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你想把你的数据科学技能提升到一个新水平吗？你是否希望提高模型的准确性，并基于数据做出更明智的决策？那么是时候探索袋装和提升的世界了。运用这些强大的技术，你可以提升模型的性能，减少错误，并做出更准确的预测。
- en: '![](../Images/4986fb00c9a65b94a926de13aad60077.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4986fb00c9a65b94a926de13aad60077.png)'
- en: Image by [Unsplash](https://unsplash.com/)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Unsplash](https://unsplash.com/) 提供
- en: 'Whether you are working on a classification problem, a regression analysis,
    or another data science project, bagging and boosting algorithms can play a crucial
    role. In this article, we #1 **summarize the main idea of ensemble learning,**
    introduce both,#2 **bagging** and #3 **boosting,** before we finally #4 **compare
    both methods** to highlight similarities and differences.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你是在处理分类问题、回归分析，还是其他数据科学项目，bagging和boosting算法都可以发挥关键作用。在本文中，我们#1 **总结了集成学习的主要思想**，介绍了#2
    **bagging**和#3 **boosting**，然后#4 **对比了这两种方法**，以突出相似性和差异。
- en: So let’s get ready for bagging and boosting to succeed!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们准备好bagging和boosting取得成功吧！
- en: '#1: Introduction and main idea: ensemble learning'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '#1：介绍和主要思想：集成学习'
- en: 'So when should we use it? Cleary, when we see overfitting or underfitting of
    our models. Let’s begin with the key concept of bagging and boosting, which both
    belong to the family of ensemble learning techniques:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 那我们什么时候应该使用它呢？显然，当我们看到模型的过拟合或欠拟合时。让我们从bagging和boosting的关键概念开始，这两者都属于集成学习技术的范畴：
- en: '![](../Images/62fe400c7e15d494a8434ff17c29e088.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62fe400c7e15d494a8434ff17c29e088.png)'
- en: Image by author
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: The main idea behind ensemble learning is the usage of multiple algorithms and
    models that are used together for the same task. While single models use only
    one algorithm to create prediction models, bagging and boosting methods aim to
    combine several of those to achieve better prediction with higher consistency
    compared to individual learnings.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习的主要思想是使用多个算法和模型一起完成相同的任务。虽然单一模型仅使用一个算法来创建预测模型，但bagging和boosting方法旨在结合多个算法，以实现比单独学习更高一致性的更好预测。
- en: '**Example: Image classification**'
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**示例：图像分类**'
- en: The essential concept is encapsulated by means of a didactic illustration involving
    image classification. Supposing a collection of images, each accompanied by a
    categorical label corresponding to the kind of animal, is available for the purpose
    of training a model. In a traditional modeling approach, we would try several
    techniques and calculate the accuracy to choose one over the other. Imagine we
    used logistic regression, decision tree, and support vector machines here that
    perform differently on the given data set.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本质概念通过涉及图像分类的教学插图来概括。假设有一组图像，每张图像都附有一个类别标签，表示动物的种类，用于训练模型。在传统建模方法中，我们会尝试几种技术并计算准确性，以选择一种优于其他的技术。假设我们在这里使用逻辑回归、决策树和支持向量机，这些模型在给定的数据集上表现不同。
- en: '![](../Images/3930d9d6beb52266303d80bf4d46440e.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3930d9d6beb52266303d80bf4d46440e.png)'
- en: Image by author
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: In the above example, it was observed that a specific record was predicted as
    a dog by the logistic regression and decision tree models, while a support vector
    machine identified it as a cat. As various models have their distinct advantages
    and disadvantages for particular records, it is the key idea of ensemble learning
    to combine all three models instead of selecting only one approach that showed
    the highest accuracy.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示例中，我们观察到逻辑回归和决策树模型将特定记录预测为狗，而支持向量机将其识别为猫。由于各种模型对特定记录有各自的优缺点，集成学习的关键思想是结合这三种模型，而不是选择表现出最高准确度的单一方法。
- en: The procedure is called *aggregation* or *voting* and combines the predictions
    of all underlying models, to come up with one prediction that is assumed to be
    more precise than any sub-model that would stay alone.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程称为*聚合*或*投票*，它结合了所有基础模型的预测，以得出一个被认为比任何单独模型更精确的预测。
- en: '**Bias-Variance tradeoff**'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**偏差-方差权衡**'
- en: The next chart might be familiar to some of you, but it represents quite well
    the relationship and the tradeoff between bias and variance on the test error
    rate.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个图表可能对你们中的一些人来说很熟悉，但它很好地表示了测试误差率上偏差和方差之间的关系和权衡。
- en: You might be familiar with the following concept, but I posit that it effectively
    illustrates the correlation and compromise between bias and variance with respect
    to the testing error rate.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能对以下概念很熟悉，但我认为它有效地说明了偏差和方差与测试误差率之间的相关性和权衡。
- en: '![](../Images/d665a9b49ef57d63a4d3e054d49e93b0.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d665a9b49ef57d63a4d3e054d49e93b0.png)'
- en: Image by author
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: The relationship between the variance and bias of a model is such that a reduction
    in variance results in an increase in bias, and vice versa. To achieve optimal
    performance, the model must be positioned at an equilibrium point, where the test
    error rate is minimized, and the variance and bias are appropriately balanced.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的方差与偏差之间的关系是方差的减少会导致偏差的增加，反之亦然。为了实现最佳性能，模型必须处于一个平衡点，在这个点上测试错误率最小化，同时方差和偏差得到适当平衡。
- en: Ensemble learning can help to balance both extreme cases to a more stable prediction.
    One method is called bagging and the other is called boosting.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习可以帮助将这两种极端情况平衡到更稳定的预测中。一种方法叫做袋装（bagging），另一种方法叫做提升（boosting）。
- en: '#2: Bagging (bootstrap aggregation)'
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '#2：袋装（自助聚合）'
- en: '![](../Images/3f8382bcb505d60bb288f60b84f52178.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f8382bcb505d60bb288f60b84f52178.png)'
- en: A random bag. Image by [Unsplash](https://unsplash.com/)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一个随机的袋子。图片来源于[Unsplash](https://unsplash.com/)
- en: Let us focus first on the Bagging technique called bootstrap aggregation. Bootstrap
    aggregation aims to solve the right extreme of the previous chart by **reducing
    the variance** of the model to avoid overfitting.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先关注一种叫做自助聚合的袋装技术。自助聚合旨在通过**减少模型的方差**来解决前一个图表中的右侧极端问题，以避免过拟合。
- en: '![](../Images/00958b27e492116b8530c18c220c752a.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/00958b27e492116b8530c18c220c752a.png)'
- en: Image by author
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: With this purpose, the idea is to have multiple models of the same learning
    algorithm that are trained by random subsets of the original training data. Those
    random subsets are called bags and can contain any combination of the data. Each
    of those datasets is then used to fit an individual model which produces individual
    predictions for the given data. Those predictions are then aggregated into one
    final classifier. The idea of this method is really close to our initial toy example
    with the cats and dogs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 目的在于拥有多个相同学习算法的模型，这些模型通过原始训练数据的随机子集进行训练。这些随机子集称为袋子，可以包含数据的任何组合。每个数据集都用于拟合一个独立的模型，该模型对给定数据产生独立的预测。然后将这些预测聚合成一个最终分类器。这种方法的想法实际上非常接近我们最初的玩具示例中的猫和狗。
- en: Using random subsets of data, the risk of overfitting is reduced and flattened
    by averaging the results of the sub-models. All models are calculated in parallel
    and then aggregated together afterward.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用随机数据子集，过拟合的风险被减少和通过对子模型结果进行平均来平滑。所有模型并行计算，然后在之后进行聚合。
- en: '![](../Images/e34968548748020d1df14d4d3a50d3a1.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e34968548748020d1df14d4d3a50d3a1.png)'
- en: The calculation of the final ensemble aggregation uses either the simple average
    for regression problems or a simple majority vote for classification problems.
    For that, each model from each random sample produces a prediction for that given
    subset. For the average, those predictions are just summed up and divided by the
    number of created bags.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的集成聚合计算使用简单平均（用于回归问题）或简单多数投票（用于分类问题）。为此，每个随机样本中的每个模型都会为该特定子集产生一个预测。对于平均值，这些预测只是相加然后除以创建的袋子数量。
- en: '![](../Images/5ab048780de4ab27ebf06a3094d18e28.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5ab048780de4ab27ebf06a3094d18e28.png)'
- en: Image by author
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: A simple majority voting works similarly but uses the predicted classes instead
    of numeric values. The algorithm identifies the class with the most predictions
    and assumes that the majority is the final aggregation. This is again very similar
    to our toy example, where two out of three algorithms predicted a picture to be
    a dog and the final aggregation was therefore a dog prediction.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 简单多数投票的工作原理类似，但使用预测的类别而不是数值。算法确定预测最多的类别，并假设多数就是最终的聚合结果。这再次非常类似于我们的玩具示例，其中三种算法中的两种预测图片为狗，因此最终聚合结果也是狗的预测。
- en: '**Random Forest**'
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**随机森林**'
- en: A famous extension to the bagging method is the random forest algorithm, which
    uses the idea of bagging but uses also subsets of the features and not only subsets
    of the entries. Bagging, on the other hand, takes all given features into account.
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对于袋装方法，一个著名的扩展是随机森林算法，它利用了袋装的思想，但也使用特征的子集，而不仅仅是条目的子集。另一方面，袋装方法会考虑所有给定的特征。
- en: Code example for bagging
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 袋装的代码示例
- en: In the following, we will explore some useful python functions from the `sklearn.ensemble`library.
    The function called `BaggingClassifier`has a few parameters which can be looked
    up in the documentation, but the most important ones are *base_estimator*, *n_estimators*,
    and *max_samples*.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨一些来自`sklearn.ensemble`库的有用python函数。名为`BaggingClassifier`的函数有一些参数，可以在文档中查阅，但最重要的参数是*base_estimator*、*n_estimators*和*max_samples*。
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*base_estimator:* You have to provide the underlying algorithm that should
    be used by the random subsets in the bagging procedure in the first parameter.
    This could be for example Logistic Regression, Support Vector Classification,
    Decision trees, or many more.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*base_estimator:* 你需要在第一个参数中提供随机子集在袋装过程中应该使用的基础算法。例如，这可以是逻辑回归、支持向量分类、决策树或更多其他算法。'
- en: '*n_estimators:* The number of estimators defines the number of bags you would
    like to create here and the default value for that is 10.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*n_estimators:* 估计器的数量定义了你希望在这里创建多少个袋，默认值为10。'
- en: '*max_samples:* The maximum number of samples defines how many samples should
    be drawn from X to train each base estimator. The default value here is one point
    zero which means that the total number of existing entries should be used. You
    could also say that you want only 80% of the entries by setting it to 0.8.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*max_samples:* 最大样本数定义了从X中抽取多少样本来训练每个基础估计器。默认值为1.0，意味着应该使用所有现有条目。你也可以通过将其设置为0.8来表示只使用80%的条目。'
- en: After setting the scenes, this model object works like many other models and
    can be trained using the `fit()`procedure including X and y data from the training
    set. The corresponding predictions on test data can be done using `predict()`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 设置场景后，该模型对象的工作方式类似于许多其他模型，可以使用`fit()`过程进行训练，包括来自训练集的X和y数据。对测试数据的相应预测可以使用`predict()`完成。
- en: '#3: Boosting'
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '#3: 提升'
- en: '![](../Images/03bf053342aca7b72583d4ffecf49a87.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03bf053342aca7b72583d4ffecf49a87.png)'
- en: Boost your models! Image by [Unsplash](https://unsplash.com/)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 提升你的模型！图片来自 [Unsplash](https://unsplash.com/)
- en: Boosting is a little variation of the bagging algorithm and uses sequential
    processing instead of parallel calculations. While bagging aims to reduce the
    variance of the model, the boosting method tries aims to **reduce the bias** to
    avoid underfitting the data. With that idea in mind, boosting also uses a random
    subset of the data to create an average-performing model on that.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 提升是袋装算法的一种小变体，使用序列处理而不是并行计算。袋装旨在减少模型的方差，而提升方法则旨在**减少偏差**以避免数据的欠拟合。基于这个理念，提升还使用数据的随机子集来创建一个平均性能的模型。
- en: '![](../Images/d313fbfc44511d468ea2155c21d9d67c.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d313fbfc44511d468ea2155c21d9d67c.png)'
- en: Image by author
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'For that, it uses the miss-classified entries of the weak model with some other
    random data to create a new model. Therefore, the different models are not randomly
    chosen but are mainly influenced by wrong classified entries of the previous model.
    The steps for this technique are the following:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，它使用弱模型的错误分类条目以及一些其他随机数据来创建一个新模型。因此，不同的模型不是随机选择的，而是主要受到前一个模型错误分类条目的影响。这种技术的步骤如下：
- en: '**Train initial (weak) model**'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练初始（弱）模型**'
- en: You create a subset of the data and train a weak learning model which is assumed
    to be the final ensemble model at this stage. You then analyze the results on
    the given training data set and can identify those entries that were misclassified.
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你创建一个数据子集并训练一个弱学习模型，此时假设它是最终集成模型。然后，你分析给定训练数据集上的结果，并可以识别那些被错误分类的条目。
- en: '**Update weights and train a new model** You create a new random subset of
    the original training data but weight those misclassified entries higher. This
    dataset is then used to train a new model.'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**更新权重并训练新模型** 你创建一个原始训练数据的新随机子集，但对那些错误分类的条目赋予更高的权重。然后使用这个数据集来训练新模型。'
- en: '**Aggregate the new model with the ensemble model**'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**将新模型与集成模型进行汇总**'
- en: The next model should perform better on the more difficult entries and will
    be combined (aggregated) with the previous one into the new final ensemble model.
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下一个模型应该在更难的条目上表现更好，并将与之前的模型合并（汇总）成新的最终集成模型。
- en: Essentially, we can repeat this process multiple times and continuously update
    the ensemble model until our prediction power is good enough. The key idea here
    is clearly to create models that are also able to predict the more difficult data
    entries. This can then lead to a better fit of the model and reduces the bias.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，我们可以多次重复这个过程，并不断更新集成模型，直到我们的预测能力足够好。这里的关键思想显然是创建能够预测更困难数据条目的模型。这可以导致模型更好地拟合数据，并减少偏差。
- en: '![](../Images/2a3c0c3f3f39bc5f69d59934be3ac337.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2a3c0c3f3f39bc5f69d59934be3ac337.png)'
- en: Image by author
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图片作者
- en: In comparison to Bagging, this technique uses weighted voting or weighted averaging
    based on the coefficients of the models that are considered together with their
    predictions. Therefore, this model can reduce underfitting, but might also tend
    to overfit sometimes.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 与Bagging相比，这种技术使用基于模型系数的加权投票或加权平均，这些模型与它们的预测一起考虑。因此，这种模型可以减少欠拟合，但有时也可能会过拟合。
- en: '![](../Images/62b660a8a0fbf2f39723e65cf44e651d.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62b660a8a0fbf2f39723e65cf44e651d.png)'
- en: Code example for boosting
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Boosting的代码示例
- en: In the following, we will look at a similar code example but for boosting. Obviously,
    there exist multiple boosting algorithms. Besides the `GradientDescent` methodology,
    the `AdaBoost` is one of the most popular.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将查看一个类似的代码示例，但用于boosting。显然，存在多种boosting算法。除了`GradientDescent`方法外，`AdaBoost`是最受欢迎的之一。
- en: '*base_estimator:* Similar to Bagging, you need to define which underlying algorithm
    you would like to use.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*base_estimator:* 类似于Bagging，你需要定义希望使用的基础算法。'
- en: '*n_estimators:* The amount of estimators defines the maximum number of iterations
    at which the boosting is terminated. It is called the “maximum” number, because
    the algorithm will stop on its own, in case good performance is achieved earlier.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*n_estimators:* 估计量的数量定义了Boosting终止的最大迭代次数。它被称为“最大”数量，因为如果算法在更早时达到良好性能，它会自行停止。'
- en: '*learning_rate*: Finally, the learning rate controls how much the new model
    is going to contribute to the previous one. Normally there is a trade-off between
    the number of iterations and the value of the learning rate. In other words: when
    taking smaller values of the learning rate, you should consider more estimators,
    so that your base model (the weak classifier) continues to improve.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*learning_rate:* 最后，学习率控制新模型对之前模型的贡献程度。通常，迭代次数与学习率的值之间存在权衡。换句话说，当学习率较小的时候，你应该考虑更多的估计量，以便基础模型（弱分类器）继续改进。'
- en: '[PRE1]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `fit()`and `predict()`procedures work similarly to the previous bagging
    example. As you can see, it is easy to use such functions from existing libraries.
    But of course, you can also implement your own algorithms to build both techniques.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit()`和`predict()`方法的工作方式与之前的bagging示例类似。正如你所见，使用现有库中的这些函数非常简单。不过，你也可以实现自己的算法来构建这两种技术。'
- en: '#4: Conclusion: differences & similarities'
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '#4: 结论：差异与相似性'
- en: '![](../Images/3e703af4c2b3ab64c9aac40260fa66cb.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e703af4c2b3ab64c9aac40260fa66cb.png)'
- en: Image by [Unsplash](https://unsplash.com/)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Unsplash](https://unsplash.com/)
- en: Since we learned briefly how bagging and boosting work, I would like to put
    the focus now on comparing both methods against each other.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们简要了解了bagging和boosting的工作原理，现在我想将重点放在对比这两种方法。
- en: Similarities
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相似性
- en: '**Ensemble methods**'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成方法**'
- en: In a general view, the similarities between both techniques start with the fact
    that both areensemble methods with the aim to use multiple learners over a single
    model to achieve better results.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从一般的角度来看，这两种技术的相似性始于它们都是集成方法，旨在通过多个学习器来改善单一模型的结果。
- en: '**Multiple samples & aggregation**'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多个样本与聚合**'
- en: 'To do that, both methods generate random samples and multiple training data
    sets. It is also similar that Bagging and Boosting both arrive at the end decision
    by aggregation of the underlying models: either by calculating average results
    or by taking a voting rank.'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了实现这一目标，这两种方法都会生成随机样本和多个训练数据集。Bagging和Boosting的相似之处在于，它们都通过对基础模型进行聚合来得出最终决策：要么通过计算平均结果，要么通过投票排名。
- en: '**Purpose**'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目的**'
- en: Finally, it is reasonable that both aim to produce higher stability and better
    prediction for the data.
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最终，两者的目标都是为了提高数据的稳定性和预测能力。
- en: Differences
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 差异
- en: '**Data partition |** whole data vs. bias'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据划分 |** 整体数据与偏差'
- en: While bagging uses random bags out of the training data for all models independently,
    boosting puts higher importance on misclassified data of the upcoming models.
    Therefore, the data partition is different here.
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于 bagging 在所有模型中独立地使用训练数据的随机包，而 boosting 对误分类数据给予更高的重要性。因此，这里的数据划分不同。
- en: '**Models |** independent vs. sequences'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型 |** 独立 vs. 序列'
- en: Bagging creates independent models that are aggregated together. However, boosting
    updates the existing model with the new ones in a sequence. Therefore, the models
    are affected by previous builds.
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Bagging 创建独立的模型并将其聚合在一起。然而，boosting 会用新的模型按顺序更新现有模型。因此，模型会受到之前构建的影响。
- en: '**Goal |** variance vs. biasAnother difference is the fact that bagging aims
    to reduce the variance, but boosting tries to reduce the bias. Therefore, bagging
    can help to decrease overfitting, and boosting can reduce underfitting.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标 |** 方差与偏差另一个区别是，bagging 旨在减少方差，而 boosting 尝试减少偏差。因此，bagging 有助于减少过拟合，而
    boosting 可以减少欠拟合。'
- en: '**Function |** weighted vs. non-weighted'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**函数 |** 加权与非加权'
- en: The final function to predict the outcome uses equally weighted average or equally
    weighted voting aggregations within the bagging technique. Boosting uses weighted
    majority vote or weighted average functions with more weight to those with better
    performance on training data.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最终预测结果的函数在 bagging 技术中使用等权重平均或等权重投票汇聚。Boosting 使用加权多数投票或加权平均函数，对训练数据表现更好的部分赋予更多权重。
- en: Implications
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 含义
- en: It was shown that the main idea of both methods is to use multiple models together
    to achieve better predictions compared so single learning models. However, there
    is *no one-over-the-other statement* to choose between bagging and boosting since
    both have advantages and disadvantages.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 研究表明，两种方法的主要思想是结合多个模型以实现比单一学习模型更好的预测。然而，并没有*绝对优于对方的说法*来选择 bagging 还是 boosting，因为两者都有优点和缺点。
- en: While bagging decreases the variance and reduces overfitting, it will only rarely
    produce better bias. Boosting on the other hand side decreases the bias but might
    be more overfitted that bagged models.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 bagging 减少了方差并减少了过拟合，但它很少能产生更好的偏差。另一方面，boosting 减少了偏差，但可能比 bagged 模型更过拟合。
- en: Coming back to the variance-bias tradeoff figure, I tried to visualize the extreme
    cases when each method seems appropriate. However, this does not mean that they
    achieve the results without any drawbacks. The aim should always be to keep bias
    and variance in a reasonable balance.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 回到方差-偏差权衡图，我尝试可视化每种方法看起来合适的极端情况。然而，这并不意味着它们能在没有任何缺点的情况下实现结果。目标应该始终是保持偏差和方差在合理的平衡中。
- en: Bagging and boosting both uses all given features and select only the entries
    randomly. Random forest on the other side is an extension to bagging that creates
    also random subsets of the features. Therefore, random forest is used more often
    in practice than bagging.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging 和 boosting 都使用所有给定特征，仅随机选择条目。另一方面，随机森林是对 bagging 的扩展，也创建特征的随机子集。因此，随机森林在实践中比
    bagging 使用得更频繁。
- en: '[](https://medium.com/@jonas_dieckmann?source=post_page-----23f9336d3cb0--------------------------------)
    [## Jonas Dieckmann - Medium'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jonas_dieckmann?source=post_page-----23f9336d3cb0--------------------------------)
    [## Jonas Dieckmann - Medium'
- en: Read writing from Jonas Dieckmann on Medium. analytics manager & product owner
    @ philips | passionate and writing about…
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阅读 Jonas Dieckmann 在 Medium 上的文章。分析经理 & 产品负责人 @ Philips | 热情并撰写关于……
- en: medium.com](https://medium.com/@jonas_dieckmann?source=post_page-----23f9336d3cb0--------------------------------)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@jonas_dieckmann?source=post_page-----23f9336d3cb0--------------------------------)
- en: I hope you find it useful. Let me know your thoughts! And feel free to connect
    on LinkedIn at [https://www.linkedin.com/in/jonas-dieckmann/](https://www.linkedin.com/in/jonas-dieckmann/)
    and/or to follow me here on medium.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你觉得这有用。请告诉我你的想法！也欢迎在 LinkedIn 上与我联系，链接为 [https://www.linkedin.com/in/jonas-dieckmann/](https://www.linkedin.com/in/jonas-dieckmann/)
    或在 medium 上关注我。
- en: 'See also some of my other articles:'
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参见我其他的一些文章：
- en: '[](/introduction-to-ica-independent-component-analysis-b2c3c4720cd9?source=post_page-----23f9336d3cb0--------------------------------)
    [## Introduction to ICA: Independent Component Analysis'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/introduction-to-ica-independent-component-analysis-b2c3c4720cd9?source=post_page-----23f9336d3cb0--------------------------------)
    [## ICA 介绍：独立成分分析'
- en: Have you ever found yourself in a situation where you were trying to analyze
    a complex and highly correlated data set…
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你是否曾遇到过尝试分析复杂且高度相关的数据集的情况……
- en: towardsdatascience.com](/introduction-to-ica-independent-component-analysis-b2c3c4720cd9?source=post_page-----23f9336d3cb0--------------------------------)
    [](/how-to-set-started-with-tensorflow-using-keras-api-and-google-colab-5421e5e4ef56?source=post_page-----23f9336d3cb0--------------------------------)
    [## How to get started with TensorFlow using Keras API and Google Colab
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/introduction-to-ica-independent-component-analysis-b2c3c4720cd9?source=post_page-----23f9336d3cb0--------------------------------)
    [](/how-to-set-started-with-tensorflow-using-keras-api-and-google-colab-5421e5e4ef56?source=post_page-----23f9336d3cb0--------------------------------)
    [## 如何使用Keras API和Google Colab开始使用TensorFlow
- en: Step-by-step tutorial to analyze human activity with neuronal networks
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分步教程分析人类活动与神经网络
- en: towardsdatascience.com](/how-to-set-started-with-tensorflow-using-keras-api-and-google-colab-5421e5e4ef56?source=post_page-----23f9336d3cb0--------------------------------)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/how-to-set-started-with-tensorflow-using-keras-api-and-google-colab-5421e5e4ef56?source=post_page-----23f9336d3cb0--------------------------------)
- en: References
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1]: Bühlmann, Peter. (2012). Bagging, Boosting and Ensemble Methods. Handbook
    of Computational Statistics. 10.1007/978–3–642–21551–3_33.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]: Bühlmann, Peter. (2012). Bagging、Boosting与集成方法。计算统计学手册. 10.1007/978–3–642–21551–3_33.'
- en: '[2]: Machova, Kristina & Puszta, Miroslav & Barcák, Frantisek & Bednár, Peter.
    (2006). A comparison of the bagging and the boosting methods using the decision
    trees classifiers. Comput. Sci. Inf. Syst.. 3\. 57–72\. 10.2298/CSIS0602057M.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[2]: Machova, Kristina & Puszta, Miroslav & Barcák, Frantisek & Bednár, Peter.
    (2006). 使用决策树分类器对Bagging和Boosting方法的比较。计算机科学与信息系统. 3. 57–72. 10.2298/CSIS0602057M.'
- en: '[3]: Banerjee, Prashant. Bagging vs Boosting @kaggle: [https://www.kaggle.com/prashant111/bagging-vs-boosting](https://www.kaggle.com/prashant111/bagging-vs-boosting)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[3]: Banerjee, Prashant. Bagging与Boosting @kaggle: [https://www.kaggle.com/prashant111/bagging-vs-boosting](https://www.kaggle.com/prashant111/bagging-vs-boosting)'
