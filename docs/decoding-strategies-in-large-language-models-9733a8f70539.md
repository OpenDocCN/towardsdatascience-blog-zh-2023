# å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è§£ç ç­–ç•¥

> åŸæ–‡ï¼š[`towardsdatascience.com/decoding-strategies-in-large-language-models-9733a8f70539`](https://towardsdatascience.com/decoding-strategies-in-large-language-models-9733a8f70539)

## ä»æŸæœç´¢åˆ°æ ¸é‡‡æ ·çš„æ–‡æœ¬ç”ŸæˆæŒ‡å—

[](https://medium.com/@mlabonne?source=post_page-----9733a8f70539--------------------------------)![Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----9733a8f70539--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9733a8f70539--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page-----9733a8f70539--------------------------------) [é©¬å…‹è¥¿å§†Â·æ‹‰åšè®·](https://medium.com/@mlabonne?source=post_page-----9733a8f70539--------------------------------)

Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9733a8f70539--------------------------------) Â·é˜…è¯»æ—¶é—´ 15 åˆ†é’ŸÂ·2023 å¹´ 6 æœˆ 4 æ—¥

--

![](img/8e3084bd4e009d7fb0c6ec5d7aef4aa6.png)

å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚

åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿·äººä¸–ç•Œä¸­ï¼Œå¾ˆå¤šå…³æ³¨éƒ½é›†ä¸­åœ¨æ¨¡å‹æ¶æ„ã€æ•°æ®å¤„ç†å’Œä¼˜åŒ–ä¸Šã€‚ç„¶è€Œï¼ŒåƒæŸæœç´¢è¿™æ ·çš„è§£ç ç­–ç•¥ï¼Œåœ¨æ–‡æœ¬ç”Ÿæˆä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œå´å¸¸å¸¸è¢«å¿½è§†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨ LLM å¦‚ä½•ç”Ÿæˆæ–‡æœ¬ï¼Œé€šè¿‡æ·±å…¥äº†è§£è´ªå©ªæœç´¢å’ŒæŸæœç´¢çš„æœºåˆ¶ï¼Œä»¥åŠä½¿ç”¨ top-k å’Œ nucleus é‡‡æ ·çš„é‡‡æ ·æŠ€æœ¯ã€‚

åˆ°æœ¬æ–‡ç»“æŸæ—¶ï¼Œä½ ä¸ä»…ä¼šå½»åº•ç†è§£è¿™äº›è§£ç ç­–ç•¥ï¼Œè¿˜ä¼šç†Ÿæ‚‰å¦‚ä½•å¤„ç†é‡è¦çš„è¶…å‚æ•°ï¼Œå¦‚æ¸©åº¦ã€num_beamsã€top_k å’Œ top_pã€‚

æœ¬æ–‡çš„ä»£ç å¯ä»¥åœ¨ [GitHub](https://github.com/mlabonne/llm-course/blob/main/Decoding_Strategies_in_Large_Language%C2%A0Models.ipynb) å’Œ [Google Colab](https://colab.research.google.com/drive/19CJlOS5lI29g-B3dziNn93Enez1yiHk2?usp=sharing) ä¸Šæ‰¾åˆ°ï¼Œä»¥ä¾›å‚è€ƒå’Œè¿›ä¸€æ­¥æ¢ç´¢ã€‚

# ğŸ“š èƒŒæ™¯

ä¸ºäº†å¼€å§‹ï¼Œè®©æˆ‘ä»¬ä»ä¸€ä¸ªä¾‹å­å¼€å§‹ã€‚æˆ‘ä»¬å°†â€œæˆ‘æœ‰ä¸€ä¸ªæ¢¦æƒ³â€è¿™ä¸ªæ–‡æœ¬è¾“å…¥åˆ° GPT-2 æ¨¡å‹ä¸­ï¼Œå¹¶è¦æ±‚å®ƒç”Ÿæˆæ¥ä¸‹æ¥çš„äº”ä¸ªæ ‡è®°ï¼ˆå•è¯æˆ–å­å•è¯ï¼‰ã€‚

```py
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model.eval()

text = "I have a dream"
input_ids = tokenizer.encode(text, return_tensors='pt').to(device)

outputs = model.generate(input_ids, max_length=len(input_ids.squeeze())+5)
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(f"Generated text: {generated_text}")
```

```py
Generated text: I have a dream of being a doctor.
```

å¥å­â€œæˆ‘æœ‰ä¸€ä¸ªæˆä¸ºåŒ»ç”Ÿçš„æ¢¦æƒ³â€ä¼¼ä¹æ˜¯ç”± GPT-2 ç”Ÿæˆçš„ã€‚ç„¶è€Œï¼ŒGPT-2 å¹¶æ²¡æœ‰*å‡†ç¡®*ç”Ÿæˆè¿™ä¸ªå¥å­ã€‚

æœ‰ä¸€ç§æ™®éçš„è¯¯è§£æ˜¯ï¼Œåƒ GPT-2 è¿™æ ·çš„ LLM **ç›´æ¥ç”Ÿæˆæ–‡æœ¬**ã€‚äº‹å®å¹¶éå¦‚æ­¤ã€‚ç›¸åï¼ŒLLM è®¡ç®— logitsï¼Œå³åˆ†é…ç»™å…¶è¯æ±‡è¡¨ä¸­æ¯ä¸ªå¯èƒ½æ ‡è®°çš„åˆ†æ•°ã€‚ä¸ºäº†ç®€åŒ–ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªè¯´æ˜æ€§è¿‡ç¨‹åˆ†è§£ï¼š

![](img/27050515483e3936d16fe97747ec1884.png)

å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚

åˆ†è¯å™¨ï¼Œ[å­—èŠ‚å¯¹ç¼–ç ](https://en.wikipedia.org/wiki/Byte_pair_encoding) åœ¨è¿™ä¸ªå®ä¾‹ä¸­ï¼Œå°†è¾“å…¥æ–‡æœ¬ä¸­çš„æ¯ä¸ªè¯ç¿»è¯‘æˆå¯¹åº”çš„è¯ IDã€‚ç„¶åï¼ŒGPT-2 ä½¿ç”¨è¿™äº›è¯ ID ä½œä¸ºè¾“å…¥å¹¶å°è¯•é¢„æµ‹ä¸‹ä¸€ä¸ªæœ€å¯èƒ½çš„è¯ã€‚æœ€åï¼Œæ¨¡å‹ç”Ÿæˆ logitsï¼Œè¿™äº› logits é€šè¿‡ softmax å‡½æ•°è½¬æ¢ä¸ºæ¦‚ç‡ã€‚

ä¾‹å¦‚ï¼Œè¯¥æ¨¡å‹ä¸ºâ€œofâ€ä½œä¸º â€œI have a dreamâ€ ä¹‹åçš„ä¸‹ä¸€ä¸ªè¯åˆ†é…äº† 17% çš„æ¦‚ç‡ã€‚è¿™ä¸ªè¾“å‡ºæœ¬è´¨ä¸Šè¡¨ç¤ºäº†æ½œåœ¨ä¸‹ä¸€ä¸ªè¯çš„æ’ååˆ—è¡¨ã€‚æ›´æ­£å¼åœ°ï¼Œæˆ‘ä»¬å°†è¿™ä¸ªæ¦‚ç‡è¡¨ç¤ºä¸º *P(of | I have a dream) = 17%*ã€‚

è‡ªå›å½’æ¨¡å‹å¦‚ GPT ä¼šåŸºäºå‰é¢çš„è¯é¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªè¯ã€‚è€ƒè™‘ä¸€ä¸ªè¯åºåˆ— *w = (w*â‚*, w*â‚‚*, â€¦, w*â‚œ*)*ã€‚è¿™ä¸ªåºåˆ—çš„è”åˆæ¦‚ç‡ *P(w)* å¯ä»¥è¢«åˆ†è§£ä¸ºï¼š

![](img/4b65c8dd18074c1330b34b2d9bbb8250.png)

å¯¹äºåºåˆ—ä¸­çš„æ¯ä¸ªè¯ *wáµ¢*ï¼Œ*P(wáµ¢ | wâ‚, wâ‚‚, â€¦, wáµ¢â‚‹â‚)* è¡¨ç¤ºåœ¨ç»™å®šæ‰€æœ‰å‰é¢çš„è¯ï¼ˆ*wâ‚, wâ‚‚, â€¦, wáµ¢â‚‹â‚*ï¼‰çš„æƒ…å†µä¸‹ *wáµ¢* çš„æ¡ä»¶æ¦‚ç‡ã€‚GPT-2 ä¸ºå…¶è¯æ±‡è¡¨ä¸­çš„æ¯ä¸€ä¸ª 50,257 ä¸ªè¯è®¡ç®—è¿™ä¸ªæ¡ä»¶æ¦‚ç‡ã€‚

è¿™å°±å¼•å‡ºäº†ä¸€ä¸ªé—®é¢˜ï¼šæˆ‘ä»¬å¦‚ä½•åˆ©ç”¨è¿™äº›æ¦‚ç‡ç”Ÿæˆæ–‡æœ¬ï¼Ÿè¿™å°±æ˜¯è§£ç ç­–ç•¥ï¼ˆå¦‚è´ªå©ªæœç´¢å’ŒæŸæœç´¢ï¼‰å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚

# ğŸƒâ€â™‚ï¸ è´ªå©ªæœç´¢

è´ªå©ªæœç´¢æ˜¯ä¸€ç§è§£ç æ–¹æ³•ï¼Œå®ƒåœ¨æ¯ä¸€æ­¥éƒ½é€‰æ‹©æœ€å¯èƒ½çš„è¯ä½œä¸ºåºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªè¯ã€‚ç®€å•æ¥è¯´ï¼Œå®ƒåªä¿ç•™æ¯ä¸ªé˜¶æ®µä¸­æœ€å¯èƒ½çš„è¯ï¼Œä¸¢å¼ƒæ‰€æœ‰å…¶ä»–æ½œåœ¨çš„é€‰é¡¹ã€‚ä»¥æˆ‘ä»¬çš„ä¾‹å­ä¸ºä¾‹ï¼š

+   **æ­¥éª¤ 1**ï¼šè¾“å…¥ï¼šâ€œI have a dreamâ€ â†’ æœ€å¯èƒ½çš„è¯ï¼š â€œ ofâ€

+   **æ­¥éª¤ 2**ï¼šè¾“å…¥ï¼šâ€œI have a dream ofâ€ â†’ æœ€å¯èƒ½çš„è¯ï¼š â€œ beingâ€

+   **æ­¥éª¤ 3**ï¼šè¾“å…¥ï¼šâ€œI have a dream of beingâ€ â†’ æœ€å¯èƒ½çš„è¯ï¼š â€œ aâ€

+   **æ­¥éª¤ 4**ï¼šè¾“å…¥ï¼šâ€œI have a dream of being aâ€ â†’ æœ€å¯èƒ½çš„è¯ï¼š â€œ doctorâ€

+   **æ­¥éª¤ 5**ï¼šè¾“å…¥ï¼šâ€œI have a dream of being a doctorâ€ â†’ æœ€å¯èƒ½çš„è¯ï¼š â€œ.â€

å°½ç®¡è¿™ç§æ–¹æ³•å¬èµ·æ¥ç›´è§‚ï¼Œä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè´ªå©ªæœç´¢å…·æœ‰çŸ­è§†æ€§ï¼šå®ƒåªè€ƒè™‘æ¯ä¸€æ­¥ä¸­æœ€å¯èƒ½çš„è¯ï¼Œè€Œä¸è€ƒè™‘å¯¹æ•´ä¸ªåºåˆ—çš„æ€»ä½“å½±å“ã€‚è¿™ä¸€ç‰¹æ€§ä½¿å¾—å®ƒå¿«é€Ÿä¸”é«˜æ•ˆï¼Œå› ä¸ºå®ƒä¸éœ€è¦è·Ÿè¸ªå¤šä¸ªåºåˆ—ï¼Œä½†ä¹Ÿæ„å‘³ç€å®ƒå¯èƒ½é”™è¿‡äº†é‚£äº›é€šè¿‡ç¨å¾®ä¸é‚£ä¹ˆå¯èƒ½çš„ä¸‹ä¸€ä¸ªè¯å¯èƒ½å‡ºç°çš„æ›´å¥½åºåˆ—ã€‚

æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨ graphviz å’Œ networkx æ¥è¯´æ˜è´ªå©ªæœç´¢çš„å®ç°ã€‚æˆ‘ä»¬é€‰æ‹©å¾—åˆ†æœ€é«˜çš„ IDï¼Œè®¡ç®—å…¶å¯¹æ•°æ¦‚ç‡ï¼ˆæˆ‘ä»¬å–å¯¹æ•°ä»¥ç®€åŒ–è®¡ç®—ï¼‰ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°æ ‘ä¸­ã€‚æˆ‘ä»¬å°†ä¸ºäº”ä¸ªè¯é‡å¤è¿™ä¸ªè¿‡ç¨‹ã€‚

```py
import matplotlib.pyplot as plt
import networkx as nx
import numpy as np
import time

def get_log_prob(logits, token_id):
    # Compute the softmax of the logits
    probabilities = torch.nn.functional.softmax(logits, dim=-1)
    log_probabilities = torch.log(probabilities)

    # Get the log probability of the token
    token_log_probability = log_probabilities[token_id].item()
    return token_log_probability

def greedy_search(input_ids, node, length=5):
    if length == 0:
        return input_ids

    outputs = model(input_ids)
    predictions = outputs.logits

    # Get the predicted next sub-word (here we use top-k search)
    logits = predictions[0, -1, :]
    token_id = torch.argmax(logits).unsqueeze(0)

    # Compute the score of the predicted token
    token_score = get_log_prob(logits, token_id)

    # Add the predicted token to the list of input ids
    new_input_ids = torch.cat([input_ids, token_id.unsqueeze(0)], dim=-1)

    # Add node and edge to graph
    next_token = tokenizer.decode(token_id, skip_special_tokens=True)
    current_node = list(graph.successors(node))[0]
    graph.nodes[current_node]['tokenscore'] = np.exp(token_score) * 100
    graph.nodes[current_node]['token'] = next_token + f"_{length}"

    # Recursive call
    input_ids = greedy_search(new_input_ids, current_node, length-1)

    return input_ids

# Parameters
length = 5
beams = 1

# Create a balanced tree with height 'length'
graph = nx.balanced_tree(1, length, create_using=nx.DiGraph())

# Add 'tokenscore', 'cumscore', and 'token' attributes to each node
for node in graph.nodes:
    graph.nodes[node]['tokenscore'] = 100
    graph.nodes[node]['token'] = text

# Start generating text
output_ids = greedy_search(input_ids, 0, length=length)
output = tokenizer.decode(output_ids.squeeze().tolist(), skip_special_tokens=True)
print(f"Generated text: {output}")
```

```py
Generated text: I have a dream of being a doctor.
```

æˆ‘ä»¬çš„è´ªå©ªæœç´¢ç”Ÿæˆçš„æ–‡æœ¬ä¸ transformers åº“ä¸­çš„æ–‡æœ¬ç›¸åŒï¼šâ€œI have a dream of being a doctorã€‚â€è®©æˆ‘ä»¬å¯è§†åŒ–æˆ‘ä»¬åˆ›å»ºçš„æ ‘ã€‚

```py
import matplotlib.pyplot as plt
import networkx as nx
import matplotlib.colors as mcolors
from matplotlib.colors import LinearSegmentedColormap

def plot_graph(graph, length, beams, score):
    fig, ax = plt.subplots(figsize=(3+1.2*beams**length, max(5, 2+length)), dpi=300, facecolor='white')

    # Create positions for each node
    pos = nx.nx_agraph.graphviz_layout(graph, prog="dot")

    # Normalize the colors along the range of token scores
    if score == 'token':
        scores = [data['tokenscore'] for _, data in graph.nodes(data=True) if data['token'] is not None]
    elif score == 'sequence':
        scores = [data['sequencescore'] for _, data in graph.nodes(data=True) if data['token'] is not None]
    vmin = min(scores)
    vmax = max(scores)
    norm = mcolors.Normalize(vmin=vmin, vmax=vmax)
    cmap = LinearSegmentedColormap.from_list('rg', ["r", "y", "g"], N=256) 

    # Draw the nodes
    nx.draw_networkx_nodes(graph, pos, node_size=2000, node_shape='o', alpha=1, linewidths=4, 
                          node_color=scores, cmap=cmap)

    # Draw the edges
    nx.draw_networkx_edges(graph, pos)

    # Draw the labels
    if score == 'token':
        labels = {node: data['token'].split('_')[0] + f"\n{data['tokenscore']:.2f}%" for node, data in graph.nodes(data=True) if data['token'] is not None}
    elif score == 'sequence':
        labels = {node: data['token'].split('_')[0] + f"\n{data['sequencescore']:.2f}" for node, data in graph.nodes(data=True) if data['token'] is not None}
    nx.draw_networkx_labels(graph, pos, labels=labels, font_size=10)
    plt.box(False)

    # Add a colorbar
    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)
    sm.set_array([])
    if score == 'token':
        fig.colorbar(sm, ax=ax, orientation='vertical', pad=0, label='Token probability (%)')
    elif score == 'sequence':
        fig.colorbar(sm, ax=ax, orientation='vertical', pad=0, label='Sequence score')
    plt.show()

# Plot graph
plot_graph(graph, length, 1.5, 'token')
```

![](img/dcb58f405a250824dbd57e3f478bfd51.png)

ä½œè€…æä¾›çš„å›¾ç‰‡ã€‚

åœ¨æ­¤å›¾ä¸­ï¼Œé¡¶çº§èŠ‚ç‚¹å­˜å‚¨è¾“å…¥ä»¤ç‰Œï¼ˆå› æ­¤æ¦‚ç‡ä¸º 100%ï¼‰ï¼Œè€Œæ‰€æœ‰å…¶ä»–èŠ‚ç‚¹è¡¨ç¤ºç”Ÿæˆçš„ä»¤ç‰Œã€‚è™½ç„¶è¿™ä¸ªåºåˆ—ä¸­çš„æ¯ä¸ªä»¤ç‰Œåœ¨é¢„æµ‹æ—¶éƒ½æ˜¯æœ€å¯èƒ½çš„ï¼Œä½†â€œbeingâ€å’Œâ€œdoctorâ€è¢«åˆ†é…äº†ç›¸å¯¹è¾ƒä½çš„æ¦‚ç‡ï¼Œåˆ†åˆ«ä¸º 9.68%å’Œ 2.86%ã€‚è¿™è¡¨æ˜â€œofâ€ï¼Œæˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªé¢„æµ‹ä»¤ç‰Œï¼Œå¯èƒ½ä¸æ˜¯æœ€åˆé€‚çš„é€‰æ‹©ï¼Œå› ä¸ºå®ƒå¯¼è‡´äº†â€œbeingâ€ï¼Œè¿™æ˜¯ç›¸å½“ä¸å¯èƒ½çš„ã€‚

åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨å…‰æŸæœç´¢å¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

# âš–ï¸ Beam Search

ä¸ä»…è€ƒè™‘ä¸‹ä¸€ä¸ªæœ€å¯èƒ½ä»¤ç‰Œçš„è´ªå©ªæœç´¢ä¸åŒï¼Œå…‰æŸæœç´¢è€ƒè™‘äº†*n*ä¸ªæœ€å¯èƒ½çš„ä»¤ç‰Œï¼Œå…¶ä¸­*n*ä»£è¡¨å…‰æŸçš„æ•°é‡ã€‚è¿™ä¸ªè¿‡ç¨‹ä¼šé‡å¤ï¼Œç›´åˆ°è¾¾åˆ°é¢„å®šä¹‰çš„æœ€å¤§é•¿åº¦æˆ–å‡ºç°åºåˆ—ç»“æŸä»¤ç‰Œã€‚æ­¤æ—¶ï¼Œå…·æœ‰æœ€é«˜æ€»ä½“åˆ†æ•°çš„åºåˆ—ï¼ˆæˆ–â€œå…‰æŸâ€ï¼‰è¢«é€‰ä¸ºè¾“å‡ºã€‚

æˆ‘ä»¬å¯ä»¥è°ƒæ•´ä¹‹å‰çš„å‡½æ•°ï¼Œä»¥è€ƒè™‘*n*ä¸ªæœ€å¯èƒ½çš„ä»¤ç‰Œï¼Œè€Œä¸ä»…ä»…æ˜¯ä¸€ä¸ªã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä¿æŒåºåˆ—åˆ†æ•°æ—¥å¿—*P(w)*ï¼Œå®ƒæ˜¯å…‰æŸä¸­æ¯ä¸ªä»¤ç‰Œå¯¹æ•°æ¦‚ç‡çš„ç´¯ç§¯å’Œã€‚æˆ‘ä»¬é€šè¿‡åºåˆ—é•¿åº¦æ¥å½’ä¸€åŒ–æ­¤åˆ†æ•°ï¼Œä»¥é˜²æ­¢å¯¹è¾ƒé•¿åºåˆ—çš„åå€šï¼ˆè¿™ä¸ªå› å­å¯ä»¥è°ƒæ•´ï¼‰ã€‚æˆ‘ä»¬å°†ç”Ÿæˆäº”ä¸ªé¢å¤–çš„ä»¤ç‰Œæ¥å®Œæˆå¥å­â€œI have a dream.â€

```py
from tqdm.notebook import tqdm

def greedy_sampling(logits, beams):
    return torch.topk(logits, beams).indices

def beam_search(input_ids, node, bar, length, beams, sampling, temperature=0.1):
    if length == 0:
        return None

    outputs = model(input_ids)
    predictions = outputs.logits

    # Get the predicted next sub-word (here we use top-k search)
    logits = predictions[0, -1, :]

    if sampling == 'greedy':
        top_token_ids = greedy_sampling(logits, beams)
    elif sampling == 'top_k':
        top_token_ids = top_k_sampling(logits, temperature, 20, beams)
    elif sampling == 'nucleus':
        top_token_ids = nucleus_sampling(logits, temperature, 0.5, beams)

    for j, token_id in enumerate(top_token_ids):
        bar.update(1)

        # Compute the score of the predicted token
        token_score = get_log_prob(logits, token_id)
        cumulative_score = graph.nodes[node]['cumscore'] + token_score

        # Add the predicted token to the list of input ids
        new_input_ids = torch.cat([input_ids, token_id.unsqueeze(0).unsqueeze(0)], dim=-1)

        # Add node and edge to graph
        token = tokenizer.decode(token_id, skip_special_tokens=True)
        current_node = list(graph.successors(node))[j]
        graph.nodes[current_node]['tokenscore'] = np.exp(token_score) * 100
        graph.nodes[current_node]['cumscore'] = cumulative_score
        graph.nodes[current_node]['sequencescore'] = 1/(len(new_input_ids.squeeze())) * cumulative_score
        graph.nodes[current_node]['token'] = token + f"_{length}_{j}"

        # Recursive call
        beam_search(new_input_ids, current_node, bar, length-1, beams, sampling, 1)

# Parameters
length = 5
beams = 2

# Create a balanced tree with height 'length' and branching factor 'k'
graph = nx.balanced_tree(beams, length, create_using=nx.DiGraph())
bar = tqdm(total=len(graph.nodes))

# Add 'tokenscore', 'cumscore', and 'token' attributes to each node
for node in graph.nodes:
    graph.nodes[node]['tokenscore'] = 100
    graph.nodes[node]['cumscore'] = 0
    graph.nodes[node]['sequencescore'] = 0
    graph.nodes[node]['token'] = text

# Start generating text
beam_search(input_ids, 0, bar, length, beams, 'greedy', 1)
```

è¯¥å‡½æ•°è®¡ç®— 63 ä¸ªä»¤ç‰Œçš„åˆ†æ•°å’Œ beams^length = 5Â² = 25 ä¸ªå¯èƒ½åºåˆ—ã€‚åœ¨æˆ‘ä»¬çš„å®ç°ä¸­ï¼Œæ‰€æœ‰ä¿¡æ¯éƒ½å­˜å‚¨åœ¨å›¾è¡¨ä¸­ã€‚æˆ‘ä»¬çš„ä¸‹ä¸€æ­¥æ˜¯æå–æœ€ä½³åºåˆ—ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬ç¡®å®šå…·æœ‰æœ€é«˜åºåˆ—åˆ†æ•°çš„å¶èŠ‚ç‚¹ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ‰¾åˆ°ä»æ ¹åˆ°è¿™ä¸ªå¶å­èŠ‚ç‚¹çš„æœ€çŸ­è·¯å¾„ã€‚æ²¿ç€è¿™æ¡è·¯å¾„çš„æ¯ä¸ªèŠ‚ç‚¹éƒ½åŒ…å«äº†æœ€ä¼˜åºåˆ—ä¸­çš„ä¸€ä¸ªä»¤ç‰Œã€‚ä»¥ä¸‹æ˜¯æˆ‘ä»¬å¦‚ä½•å®ç°å®ƒï¼š

```py
def get_best_sequence(G):
    # Create a list of leaf nodes
    leaf_nodes = [node for node in G.nodes() if G.out_degree(node)==0]

    # Get the leaf node with the highest cumscore
    max_score_node = None
    max_score = float('-inf')
    for node in leaf_nodes:
        if G.nodes[node]['sequencescore'] > max_score:
            max_score = G.nodes[node]['sequencescore']
            max_score_node = node

    # Retrieve the sequence of nodes from this leaf node to the root node in a list
    path = nx.shortest_path(G, source=0, target=max_score_node)

    # Return the string of token attributes of this sequence
    sequence = "".join([G.nodes[node]['token'].split('_')[0] for node in path])

    return sequence, max_score

sequence, max_score = get_best_sequence(graph)
print(f"Generated text: {sequence}")
```

```py
Generated text: I have a dream. I have a dream
```

æœ€ä½³åºåˆ—ä¼¼ä¹æ˜¯â€œI have a dream. I have a dreamâ€ï¼Œè¿™æ˜¯ GPT-2 çš„å¸¸è§å“åº”ï¼Œå°½ç®¡è¿™å¯èƒ½ä»¤äººæƒŠè®¶ã€‚ä¸ºäº†éªŒè¯è¿™ä¸€ç‚¹ï¼Œè®©æˆ‘ä»¬ç»˜åˆ¶å›¾è¡¨ã€‚

åœ¨è¿™ä¸ªå¯è§†åŒ–ä¸­ï¼Œæˆ‘ä»¬å°†æ˜¾ç¤ºæ¯ä¸ªèŠ‚ç‚¹çš„åºåˆ—åˆ†æ•°ï¼Œè¿™ä»£è¡¨äº†åˆ°è¯¥ç‚¹ä¸ºæ­¢çš„åºåˆ—åˆ†æ•°ã€‚å¦‚æœå‡½æ•° get_best_sequence()æ˜¯æ­£ç¡®çš„ï¼Œåˆ™åºåˆ—â€œI have a dream. I have a dreamâ€ä¸­çš„â€œdreamâ€èŠ‚ç‚¹åº”è¯¥åœ¨æ‰€æœ‰å¶èŠ‚ç‚¹ä¸­å…·æœ‰æœ€é«˜åˆ†æ•°ã€‚

```py
# Plot graph
plot_graph(graph, length, beams, 'sequence')
```

![](img/64c8bcbca63a9847c6a66c247c0f25ce.png)

ç¡®å®ï¼Œâ€œdreamâ€ä»¤ç‰Œå…·æœ‰**æœ€é«˜åºåˆ—åˆ†æ•°**ï¼Œå€¼ä¸º-0.69ã€‚ä»¤äººæ„Ÿå…´è¶£çš„æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å·¦ä¾§è´ªå©ªåºåˆ—â€œI have a dream of being a doctor.â€çš„åˆ†æ•°å€¼ä¸º-1.16ã€‚

æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œè´ªå©ªæœç´¢å¯¼è‡´äº†æ¬¡ä¼˜ç»“æœã€‚ä½†æ˜¯ï¼Œè€å®è¯´ï¼Œæˆ‘ä»¬çš„æ–°ç»“æœä¹Ÿæ²¡æœ‰ç‰¹åˆ«å¼•äººæ³¨ç›®ã€‚ä¸ºäº†ç”Ÿæˆæ›´å¤šæ ·åŒ–çš„åºåˆ—ï¼Œæˆ‘ä»¬å°†å®ç°ä¸¤ç§é‡‡æ ·ç®—æ³•ï¼štop-k å’Œ nucleusã€‚

# ğŸ² Top-k é‡‡æ ·

Top-k é‡‡æ ·æ˜¯ä¸€ç§åˆ©ç”¨è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æ¦‚ç‡åˆ†å¸ƒæ¥**ä»æœ€å¯èƒ½çš„*k*ä¸ªé€‰é¡¹ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªä»¤ç‰Œ**çš„æŠ€æœ¯ã€‚

ä¸¾ä¾‹æ¥è¯´ï¼Œå‡è®¾æˆ‘ä»¬æœ‰*k = 3*å’Œå››ä¸ª tokenï¼šAã€Bã€C å’Œ Dï¼Œåˆ†åˆ«çš„æ¦‚ç‡ä¸ºï¼š*P(A) = 30%*ã€*P(B) = 15%*ã€*P(C) = 5%*å’Œ*P(D) = 1%*ã€‚åœ¨ top-k é‡‡æ ·ä¸­ï¼Œtoken D è¢«å¿½ç•¥ï¼Œç®—æ³•ä¼šåœ¨ 60%çš„æ—¶é—´å†…è¾“å‡º Aï¼Œåœ¨ 30%çš„æ—¶é—´å†…è¾“å‡º Bï¼Œåœ¨ 10%çš„æ—¶é—´å†…è¾“å‡º Cã€‚è¿™ç§æ–¹æ³•ç¡®ä¿æˆ‘ä»¬ä¼˜å…ˆè€ƒè™‘æœ€å¯èƒ½çš„ tokenï¼ŒåŒæ—¶åœ¨é€‰æ‹©è¿‡ç¨‹ä¸­å¼•å…¥ä¸€å®šçš„éšæœºæ€§ã€‚

å¼•å…¥éšæœºæ€§çš„å¦ä¸€ç§æ–¹å¼æ˜¯æ¸©åº¦çš„æ¦‚å¿µã€‚æ¸©åº¦*T*æ˜¯ä¸€ä¸ªèŒƒå›´ä» 0 åˆ° 1 çš„å‚æ•°ï¼Œå®ƒå½±å“ softmax å‡½æ•°ç”Ÿæˆçš„æ¦‚ç‡ï¼Œä½¿æœ€å¯èƒ½çš„ token æ›´å…·å½±å“åŠ›ã€‚åœ¨å®é™…æ“ä½œä¸­ï¼Œå®ƒç®€å•åœ°åŒ…æ‹¬å°†è¾“å…¥ logits é™¤ä»¥æˆ‘ä»¬ç§°ä¹‹ä¸ºæ¸©åº¦çš„å€¼ï¼š

![](img/b3e030655ba992e2bc09d381d196c363.png)

è¿™é‡Œæ˜¯ä¸€ä¸ªå›¾è¡¨ï¼Œå±•ç¤ºäº†æ¸©åº¦å¯¹ç»™å®šè¾“å…¥ logits [1.5, -1.8, 0.9, -3.2] ç”Ÿæˆçš„æ¦‚ç‡çš„å½±å“ã€‚æˆ‘ä»¬ç»˜åˆ¶äº†ä¸‰ç§ä¸åŒçš„æ¸©åº¦å€¼æ¥è§‚å¯Ÿå·®å¼‚ã€‚

![](img/e34fa38ca09ebeaece4e1271c26964da.png)

æ¸©åº¦ä¸º 1.0 ç›¸å½“äºæ²¡æœ‰æ¸©åº¦çš„é»˜è®¤ softmaxã€‚å¦ä¸€æ–¹é¢ï¼Œä½æ¸©è®¾ç½®ï¼ˆ0.1ï¼‰ä¼šæ˜¾è‘—æ”¹å˜æ¦‚ç‡åˆ†å¸ƒã€‚è¿™åœ¨æ–‡æœ¬ç”Ÿæˆä¸­å¸¸ç”¨äºæ§åˆ¶ç”Ÿæˆè¾“å‡ºçš„â€œåˆ›é€ æ€§â€æ°´å¹³ã€‚é€šè¿‡è°ƒæ•´æ¸©åº¦ï¼Œæˆ‘ä»¬å¯ä»¥å½±å“æ¨¡å‹ç”Ÿæˆæ›´å…·å¤šæ ·æ€§æˆ–æ›´å¯é¢„æµ‹çš„å“åº”çš„ç¨‹åº¦ã€‚

ç°åœ¨è®©æˆ‘ä»¬å®ç° top-k é‡‡æ ·ç®—æ³•ã€‚æˆ‘ä»¬å°†åœ¨ beam_search()å‡½æ•°ä¸­ä½¿ç”¨â€œtop_kâ€å‚æ•°ã€‚ä¸ºäº†è¯´æ˜ç®—æ³•çš„å·¥ä½œåŸç†ï¼Œæˆ‘ä»¬è¿˜å°†ç»˜åˆ¶ top_k = 20 çš„æ¦‚ç‡åˆ†å¸ƒã€‚

```py
def plot_prob_distribution(probabilities, next_tokens, sampling, potential_nb, total_nb=50):
    # Get top k tokens
    top_k_prob, top_k_indices = torch.topk(probabilities, total_nb)
    top_k_tokens = [tokenizer.decode([idx]) for idx in top_k_indices.tolist()]

    # Get next tokens and their probabilities
    next_tokens_list = [tokenizer.decode([idx]) for idx in next_tokens.tolist()]
    next_token_prob = probabilities[next_tokens].tolist()

    # Create figure
    plt.figure(figsize=(0.4*total_nb, 5), dpi=300, facecolor='white')
    plt.rc('axes', axisbelow=True)
    plt.grid(axis='y', linestyle='-', alpha=0.5)
    if potential_nb < total_nb:
        plt.axvline(x=potential_nb-0.5, ls=':', color='grey', label='Sampled tokens')
    plt.bar(top_k_tokens, top_k_prob.tolist(), color='blue')
    plt.bar(next_tokens_list, next_token_prob, color='red', label='Selected tokens')
    plt.xticks(rotation=45, ha='right', va='top')
    plt.gca().spines['top'].set_visible(False)
    plt.gca().spines['right'].set_visible(False)
    if sampling == 'top_k':
        plt.title('Probability distribution of predicted tokens with top-k sampling')
    elif sampling == 'nucleus':
        plt.title('Probability distribution of predicted tokens with nucleus sampling')
    plt.legend()
    plt.savefig(f'{sampling}_{time.time()}.png', dpi=300)
    plt.close()

def top_k_sampling(logits, temperature, top_k, beams, plot=True):
    assert top_k >= 1
    assert beams <= top_k

    indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]
    new_logits = torch.clone(logits)
    new_logits[indices_to_remove] = float('-inf')

    # Convert logits to probabilities
    probabilities = torch.nn.functional.softmax(new_logits / temperature, dim=-1)

    # Sample n tokens from the resulting distribution
    next_tokens = torch.multinomial(probabilities, beams)

    # Plot distribution
    if plot:
        total_prob = torch.nn.functional.softmax(logits / temperature, dim=-1)
        plot_prob_distribution(total_prob, next_tokens, 'top_k', top_k)

    return next_tokens

# Start generating text
beam_search(input_ids, 0, bar, length, beams, 'top_k', 1)
```

![](img/12300ff8d47224b762e26e6519b528f2.png)

å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚

è¿™äº›å›¾ç»™å‡ºäº† top-k é‡‡æ ·å¦‚ä½•å·¥ä½œçš„è‰¯å¥½ç›´è§‚å°è±¡ï¼Œæ‰€æœ‰å¯èƒ½é€‰æ‹©çš„ token åœ¨æ°´å¹³æ¡çš„å·¦ä¾§ã€‚è™½ç„¶æœ€å¯èƒ½çš„ tokenï¼ˆçº¢è‰²ï¼‰å¤§å¤šæ•°æ—¶é—´è¢«é€‰æ‹©ï¼Œä½†å®ƒä¹Ÿå…è®¸é€‰æ‹©å¯èƒ½æ€§è¾ƒä½çš„ tokenã€‚è¿™æä¾›äº†ä¸€ä¸ªæœ‰è¶£çš„æƒè¡¡ï¼Œå¯ä»¥ä½¿åºåˆ—è¶‹å‘äºä¸€ä¸ªæ›´ä¸å¯é¢„æµ‹ä½†æ›´è‡ªç„¶çš„å¥å­ã€‚ç°åœ¨è®©æˆ‘ä»¬æ‰“å°å®ƒç”Ÿæˆçš„æ–‡æœ¬ã€‚

```py
sequence, max_score = get_best_sequence(graph)
print(f"Generated text: {sequence}")
```

```py
Generated text: I have a dream job and I want to
```

top-k é‡‡æ ·æ‰¾åˆ°äº†ä¸€ä¸ªæ–°åºåˆ—ï¼šâ€œæˆ‘æœ‰ä¸€ä¸ªæ¢¦æƒ³å·¥ä½œï¼Œæˆ‘æƒ³è¦â€ï¼Œè¿™æ¯”â€œæˆ‘æœ‰ä¸€ä¸ªæ¢¦æƒ³ã€‚æˆ‘æœ‰ä¸€ä¸ªæ¢¦æƒ³â€æ˜¾å¾—è‡ªç„¶å¾—å¤šã€‚æˆ‘ä»¬åœ¨å–å¾—è¿›å±•ï¼

è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªå†³ç­–æ ‘ä¸ä¹‹å‰çš„æœ‰ä½•ä¸åŒã€‚

```py
# Plot graph
plot_graph(graph, length, beams, 'sequence')
```

![](img/29cff567ceb5d77709230d8c84e85d04.png)

ä½ å¯ä»¥çœ‹åˆ°èŠ‚ç‚¹ä¸ä¹‹å‰çš„è¿­ä»£ç›¸æ¯”æœ‰æ˜¾è‘—ä¸åŒï¼Œåšå‡ºäº†æ›´å¤šæ ·çš„é€‰æ‹©ã€‚è™½ç„¶è¿™ä¸€æ–°ç»“æœçš„åºåˆ—åˆ†æ•°å¯èƒ½ä¸æ˜¯æœ€é«˜çš„ï¼ˆ-1.01ï¼Œè€Œä¹‹å‰ä¸º-0.69ï¼‰ï¼Œä½†è¦è®°ä½ï¼Œæ›´é«˜çš„åˆ†æ•°å¹¶ä¸æ€»æ˜¯èƒ½å¯¼è‡´æ›´ç°å®æˆ–æœ‰æ„ä¹‰çš„åºåˆ—ã€‚

ç°åœ¨æˆ‘ä»¬å¼•å…¥äº† top-k é‡‡æ ·ï¼Œæˆ‘ä»¬å¿…é¡»ä»‹ç»å¦ä¸€ç§æœ€å—æ¬¢è¿çš„é‡‡æ ·æŠ€æœ¯ï¼šæ ¸å¿ƒé‡‡æ ·ã€‚

# ğŸ”¬ æ ¸å¿ƒé‡‡æ ·

æ ¸é‡‡æ ·ï¼Œä¹Ÿç§°ä¸º top-p é‡‡æ ·ï¼Œä¸ top-k é‡‡æ ·é‡‡å–äº†ä¸åŒçš„æ–¹æ³•ã€‚æ ¸é‡‡æ ·ä¸æ˜¯é€‰æ‹©å‰*k*ä¸ªæœ€å¯èƒ½çš„æ ‡è®°ï¼Œè€Œæ˜¯é€‰æ‹©ä¸€ä¸ªæˆªæ­¢å€¼*p*ï¼Œä½¿å¾—**é€‰æ‹©çš„æ ‡è®°çš„æ¦‚ç‡æ€»å’Œè¶…è¿‡*p***ã€‚è¿™å½¢æˆäº†ä¸€ä¸ªä»ä¸­éšæœºé€‰æ‹©ä¸‹ä¸€ä¸ªæ ‡è®°çš„â€œæ ¸å¿ƒâ€ã€‚

æ¢å¥è¯è¯´ï¼Œæ¨¡å‹æŒ‰æ¦‚ç‡ä»é«˜åˆ°ä½æ£€æŸ¥å…¶æœ€å¯èƒ½çš„æ ‡è®°ï¼Œå¹¶ä¸æ–­å°†å®ƒä»¬æ·»åŠ åˆ°åˆ—è¡¨ä¸­ï¼Œç›´åˆ°æ€»æ¦‚ç‡è¶…è¿‡é˜ˆå€¼*p*ã€‚ä¸ top-k é‡‡æ ·ä¸åŒï¼Œæ ¸ä¸­åŒ…å«çš„æ ‡è®°æ•°é‡å¯èƒ½åœ¨æ¯ä¸€æ­¥å˜åŒ–ã€‚è¿™ç§å˜åŒ–é€šå¸¸å¯¼è‡´æ›´å…·å¤šæ ·æ€§å’Œåˆ›é€ æ€§çš„è¾“å‡ºï¼Œä½¿å¾—æ ¸é‡‡æ ·åœ¨æ–‡æœ¬ç”Ÿæˆç­‰ä»»åŠ¡ä¸­é¢‡å—æ¬¢è¿ã€‚

ä¸ºäº†å®ç°æ ¸é‡‡æ ·æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨`beam_search()`å‡½æ•°ä¸­ä½¿ç”¨â€œnucleusâ€å‚æ•°ã€‚åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†*p*çš„å€¼è®¾ç½®ä¸º 0.5ã€‚ä¸ºäº†ç®€åŒ–æ“ä½œï¼Œæˆ‘ä»¬å°†åŒ…æ‹¬ä¸€ä¸ªæœ€å°çš„æ ‡è®°æ•°ï¼Œç­‰äºå…‰æŸçš„æ•°é‡ã€‚æˆ‘ä»¬è¿˜å°†è€ƒè™‘ç´¯è®¡æ¦‚ç‡ä½äº*p*çš„æ ‡è®°ï¼Œè€Œä¸æ˜¯é«˜äºçš„æ ‡è®°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè™½ç„¶ç»†èŠ‚å¯èƒ½æœ‰æ‰€ä¸åŒï¼Œä½†æ ¸é‡‡æ ·çš„æ ¸å¿ƒæ€æƒ³ä¿æŒä¸å˜ã€‚

```py
def nucleus_sampling(logits, temperature, p, beams, plot=True):
    assert p > 0
    assert p <= 1

    # Sort the probabilities in descending order and compute cumulative probabilities
    sorted_logits, sorted_indices = torch.sort(logits, descending=True)
    probabilities = torch.nn.functional.softmax(sorted_logits / temperature, dim=-1)
    cumulative_probabilities = torch.cumsum(probabilities, dim=-1)

    # Create a mask for probabilities that are in the top-p
    mask = cumulative_probabilities < p

    # If there's not n index where cumulative_probabilities < p, we use the top n tokens instead
    if mask.sum() > beams:
        top_p_index_to_keep = torch.where(mask)[0][-1].detach().cpu().tolist()
    else:
        top_p_index_to_keep = beams

    # Only keep top-p indices
    indices_to_remove = sorted_indices[top_p_index_to_keep:]
    sorted_logits[indices_to_remove] = float('-inf')

    # Sample n tokens from the resulting distribution
    probabilities = torch.nn.functional.softmax(sorted_logits / temperature, dim=-1)
    next_tokens = torch.multinomial(probabilities, beams)

    # Plot distribution
    if plot:
        total_prob = torch.nn.functional.softmax(logits / temperature, dim=-1)
        plot_prob_distribution(total_prob, next_tokens, 'nucleus', top_p_index_to_keep)

    return next_tokens

# Start generating text
beam_search(input_ids, 0, bar, length, beams, 'nucleus', 1)
```

![](img/85778461ebdf2816a857f2d25e65eea7.png)

ä½œè€…æä¾›çš„å›¾ç‰‡ã€‚

åœ¨è¿™ä¸ªå›¾ä¸­ï¼Œä½ å¯ä»¥çœ‹åˆ°æ ¸ä¸­çš„æ ‡è®°æ•°é‡ï¼ˆå‚ç›´æ¡çš„å·¦ä¾§ï¼‰æ³¢åŠ¨å¾ˆå¤§ã€‚ç”Ÿæˆçš„æ¦‚ç‡åˆ†å¸ƒå˜åŒ–å¾ˆå¤§ï¼Œå¯¼è‡´é€‰æ‹©çš„æ ‡è®°ä¸æ€»æ˜¯æœ€å¯èƒ½çš„ã€‚è¿™ä¸ºç”Ÿæˆç‹¬ç‰¹è€Œå¤šæ ·åŒ–çš„åºåˆ—æ‰“å¼€äº†å¤§é—¨ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬è§‚å¯Ÿå®ƒç”Ÿæˆçš„æ–‡æœ¬ã€‚

```py
sequence, max_score = get_best_sequence(graph)
print(f"Generated text: {sequence}")
```

```py
Generated text: I have a dream. I'm going to
```

æ ¸é‡‡æ ·ç®—æ³•ç”Ÿæˆçš„åºåˆ—æ˜¯ï¼šâ€œI have a dream. Iâ€™m going toâ€ï¼Œä¸è´ªå©ªé‡‡æ ·ç›¸æ¯”ï¼Œæ˜¾ç¤ºäº†è¯­ä¹‰ä¸€è‡´æ€§çš„æ˜¾è‘—æå‡ã€‚

ä¸ºäº†æ¯”è¾ƒå†³ç­–è·¯å¾„ï¼Œè®©æˆ‘ä»¬å¯è§†åŒ–æ–°ç”Ÿæˆçš„æ ‘æ ¸é‡‡æ ·ã€‚

```py
# Plot graph
plot_graph(graph, length, beams, 'sequence')
```

![](img/23ad20ec7404c08df5cf975e2642a24b.png)

ä¸ top-k é‡‡æ ·ä¸€æ ·ï¼Œè¿™æ£µæ ‘ä¸è´ªå©ªé‡‡æ ·ç”Ÿæˆçš„æ ‘éå¸¸ä¸åŒï¼Œå±•ç¤ºäº†æ›´å¤šçš„å¤šæ ·æ€§ã€‚top-k å’Œæ ¸é‡‡æ ·åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶éƒ½æä¾›äº†ç‹¬ç‰¹çš„ä¼˜åŠ¿ï¼Œå¢å¼ºäº†å¤šæ ·æ€§ï¼Œå¹¶åœ¨è¾“å‡ºä¸­å¼•å…¥äº†åˆ›é€ åŠ›ã€‚ä½ å¯¹è¿™ä¸¤ç§æ–¹æ³•ï¼ˆç”šè‡³æ˜¯è´ªå©ªæœç´¢ï¼‰çš„é€‰æ‹©å°†å–å†³äºä½ é¡¹ç›®çš„å…·ä½“è¦æ±‚å’Œé™åˆ¶ã€‚

# ç»“è®º

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥æ¢è®¨äº† LLMï¼ˆå°¤å…¶æ˜¯ GPT-2ï¼‰ä½¿ç”¨çš„å„ç§è§£ç æ–¹æ³•ã€‚æˆ‘ä»¬ä»ç®€å•çš„**è´ªå©ªæœç´¢**å¼€å§‹ï¼Œè¿™ç§æ–¹æ³•ä¼šç«‹å³ï¼ˆä½†å¾€å¾€ä¸æ˜¯æœ€ä½³çš„ï¼‰é€‰æ‹©æœ€å¯èƒ½çš„ä¸‹ä¸€ä¸ªæ ‡è®°ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä»‹ç»äº†**å…‰æŸæœç´¢**æŠ€æœ¯ï¼Œè¿™ç§æ–¹æ³•åœ¨æ¯ä¸€æ­¥è€ƒè™‘å‡ ä¸ªæœ€å¯èƒ½çš„æ ‡è®°ã€‚å°½ç®¡å®ƒæä¾›äº†æ›´ä¸ºç»†è‡´çš„ç»“æœï¼Œä½†å…‰æŸæœç´¢æœ‰æ—¶åœ¨ç”Ÿæˆå¤šæ ·åŒ–å’Œå¯Œæœ‰åˆ›é€ æ€§çš„åºåˆ—æ–¹é¢è¡¨ç°ä¸ä½³ã€‚

ä¸ºäº†å¼•å…¥æ›´å¤šçš„å˜å¼‚æ€§ï¼Œæˆ‘ä»¬æ¥ç€ä½¿ç”¨äº†**top-k é‡‡æ ·**å’Œ**nucleus é‡‡æ ·**ã€‚Top-k é‡‡æ ·é€šè¿‡åœ¨*æœ€å¯èƒ½çš„ k ä¸ª*æ ‡è®°ä¸­éšæœºé€‰æ‹©æ¥å¤šæ ·åŒ–æ–‡æœ¬ç”Ÿæˆï¼Œè€Œ nucleus é‡‡æ ·åˆ™é€šè¿‡åŸºäºç´¯è®¡æ¦‚ç‡åŠ¨æ€åœ°å½¢æˆä¸€ä¸ªæ ‡è®°çš„æ ¸å¿ƒæ¥é‡‡å–ä¸åŒçš„æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•å„è‡ªå…·æœ‰ç‹¬ç‰¹çš„ä¼˜ç‚¹å’Œæ½œåœ¨çš„ç¼ºç‚¹ï¼Œå…·ä½“çš„é€‰æ‹©å°†ä¸»è¦å–å†³äºä½ é¡¹ç›®çš„è¦æ±‚ã€‚

æœ€ç»ˆï¼Œç†è§£è¿™äº›æŠ€æœ¯åŠå…¶æƒè¡¡å°†å¸®åŠ©ä½ æ›´å¥½åœ°å¼•å¯¼ LLMs ç”Ÿæˆè¶Šæ¥è¶ŠçœŸå®ã€ç»†è‡´å’Œå¼•äººå…¥èƒœçš„æ–‡æœ¬è¾“å‡ºã€‚

å¦‚æœä½ å¯¹æ›´å¤šå…³äº LLMs çš„æŠ€æœ¯å†…å®¹æ„Ÿå…´è¶£ï¼Œå¯ä»¥åœ¨ Twitter ä¸Šå…³æ³¨æˆ‘ [@maximelabonne](https://twitter.com/maximelabonne)ã€‚
