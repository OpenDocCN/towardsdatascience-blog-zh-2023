- en: 'Beyond the Basics: Reinforcement Learning with Jax — Part II: Developing an
    Exploitative Alternative to A/B Testing'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越基础知识：使用 Jax 的强化学习 —— 第二部分：开发一种替代 A/B 测试的利用性方法
- en: 原文：[https://towardsdatascience.com/beyond-the-basics-reinforcement-learning-with-jax-part-ii-developing-an-exploitative-9423cb6b2fa5?source=collection_archive---------4-----------------------#2023-06-02](https://towardsdatascience.com/beyond-the-basics-reinforcement-learning-with-jax-part-ii-developing-an-exploitative-9423cb6b2fa5?source=collection_archive---------4-----------------------#2023-06-02)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/beyond-the-basics-reinforcement-learning-with-jax-part-ii-developing-an-exploitative-9423cb6b2fa5?source=collection_archive---------4-----------------------#2023-06-02](https://towardsdatascience.com/beyond-the-basics-reinforcement-learning-with-jax-part-ii-developing-an-exploitative-9423cb6b2fa5?source=collection_archive---------4-----------------------#2023-06-02)
- en: A practical introduction to multi-arm bandits and the exploration-exploitation
    dilemma
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一种关于多臂赌博机和探索-利用困境的实用介绍
- en: '[](https://medium.com/@Lando-L?source=post_page-----9423cb6b2fa5--------------------------------)[![Lando
    L](../Images/9675667c061daf1d9ac527b49b7a8a9f.png)](https://medium.com/@Lando-L?source=post_page-----9423cb6b2fa5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9423cb6b2fa5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9423cb6b2fa5--------------------------------)
    [Lando L](https://medium.com/@Lando-L?source=post_page-----9423cb6b2fa5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@Lando-L?source=post_page-----9423cb6b2fa5--------------------------------)[![Lando
    L](../Images/9675667c061daf1d9ac527b49b7a8a9f.png)](https://medium.com/@Lando-L?source=post_page-----9423cb6b2fa5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9423cb6b2fa5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9423cb6b2fa5--------------------------------)
    [Lando L](https://medium.com/@Lando-L?source=post_page-----9423cb6b2fa5--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F395c5e41bd1e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-the-basics-reinforcement-learning-with-jax-part-ii-developing-an-exploitative-9423cb6b2fa5&user=Lando+L&userId=395c5e41bd1e&source=post_page-395c5e41bd1e----9423cb6b2fa5---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9423cb6b2fa5--------------------------------)
    ·19 min read·Jun 2, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9423cb6b2fa5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-the-basics-reinforcement-learning-with-jax-part-ii-developing-an-exploitative-9423cb6b2fa5&user=Lando+L&userId=395c5e41bd1e&source=-----9423cb6b2fa5---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F395c5e41bd1e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-the-basics-reinforcement-learning-with-jax-part-ii-developing-an-exploitative-9423cb6b2fa5&user=Lando+L&userId=395c5e41bd1e&source=post_page-395c5e41bd1e----9423cb6b2fa5---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9423cb6b2fa5--------------------------------)
    ·19 分钟阅读·2023年6月2日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9423cb6b2fa5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-the-basics-reinforcement-learning-with-jax-part-ii-developing-an-exploitative-9423cb6b2fa5&user=Lando+L&userId=395c5e41bd1e&source=-----9423cb6b2fa5---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9423cb6b2fa5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-the-basics-reinforcement-learning-with-jax-part-ii-developing-an-exploitative-9423cb6b2fa5&source=-----9423cb6b2fa5---------------------bookmark_footer-----------)![](../Images/696f574091a931a758adfa8f5a628807.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9423cb6b2fa5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-the-basics-reinforcement-learning-with-jax-part-ii-developing-an-exploitative-9423cb6b2fa5&source=-----9423cb6b2fa5---------------------bookmark_footer-----------)![](../Images/696f574091a931a758adfa8f5a628807.png)'
- en: Photo by [Carl Raw](https://unsplash.com/@carltraw?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Carl Raw](https://unsplash.com/@carltraw?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In our last blog post, we explored the Reinforcement Learning paradigm, delving
    into its core concepts of finite Markov Decision Processes, Policies, and Value
    Functions. Now, we are ready to apply our newfound knowledge and discover an alternative
    approach to traditional A/B testing via multi-arm bandits.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们上一篇博客文章中，我们探讨了强化学习范式，*深入研究*了其有限马尔可夫决策过程、策略和价值函数的核心概念。现在，我们准备应用新获得的知识，通过多臂老虎机发现传统
    A/B 测试的替代方法。
- en: Course links
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 课程链接
- en: '[Part I: Introduction and Core Concepts](https://medium.com/@Lando-L/beyond-the-basics-reinforcement-learning-with-jax-part-i-introduction-and-core-concepts-4f85f3478f5)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第一部分：介绍和核心概念](https://medium.com/@Lando-L/beyond-the-basics-reinforcement-learning-with-jax-part-i-introduction-and-core-concepts-4f85f3478f5)'
- en: '**Part II: Developing an exploitative alternative to A/B testing**'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第二部分：开发一种替代 A/B 测试的利用性方法**'
- en: Is A/B testing significantly overrated?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: A/B 测试是否被过分高估了？
- en: Think back to the last time you scrolled through your video streaming service
    looking for a new movie to watch. If you did not already know what to look for,
    you may have been affected by *the paradox of choice*¹. Having such an array of
    potentially good movies can make it difficult to make informed decisions. Instead,
    we often rely on simpler decisions that can be made almost instantly. As a result,
    we tend to compare videos based on their thumbnails and titles. Knowing of this
    effect, video production and streaming companies strive to optimise thumbnails
    and titles in order to increase their movies’ click rates. Without prior knowledge
    of the audience’s preferences, the process of creating engaging thumbnails and
    titles becomes one of *trial-and-error*. (I hope this rings a bell by now!)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下你上次浏览视频流媒体服务寻找新电影的情景。如果你之前不知道该找什么，你可能会受到*选择悖论*¹的影响。拥有如此多的潜在好电影可能会使做出明智的决定变得困难。相反，我们常常依赖于几乎可以立即做出的简单决定。因此，我们往往根据视频的缩略图和标题进行比较。了解到这一效果，视频制作和流媒体公司努力优化缩略图和标题，以提高电影的点击率。如果没有观众偏好的先前知识，制作吸引人的缩略图和标题的过程变成了*试错*。
    （我希望这现在让你有所领悟！）
- en: The traditional approach
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 传统方法
- en: Let us imagine ourselves in the shoes of the decision-maker tasked with choosing
    the right thumbnail and title for a newly-published movie on the streaming website.
    In addition to the original movie thumbnail and title, we are given a set of proposals
    consisting of two thumbnails and two titles. The traditional data-driven approach
    in this scenario would be to run an A/B test and compare the click rates of the
    original version with that of the proposed versions. Although the advantage of
    A/B tests is that, when given enough samples, it can identify whether or not the
    measured difference in click rate is statistically significant, it limits us to
    compare only two variants simultaneously and often requires extensive amounts
    of samples to determine its result. Furthermore, in the case one of the variants
    is performing drastically better than the other, we are still forced to serve
    the worse performing variant to half our customers until the end of the experiment,
    potentially leading to money losses.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设想自己是决策者，负责为流媒体网站上新发布的电影选择合适的缩略图和标题。除了原始的电影缩略图和标题外，我们还获得了一组建议，包括两个缩略图和两个标题。在这种情况下，传统的数据驱动方法是进行
    A/B 测试，将原始版本的点击率与建议版本的点击率进行比较。虽然 A/B 测试的优点在于，当样本量足够大时，可以确定点击率的差异是否具有统计显著性，但它限制我们只能同时比较两个变体，并且通常需要大量样本来确定结果。此外，如果其中一个变体的表现远远优于另一个，我们仍然被迫将表现较差的变体展示给一半的客户，直到实验结束，这可能导致金钱损失。
- en: The Reinforcement Learning approach
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习方法
- en: Alternatively, we can set up a *multi-arm bandit* (MAB) experiment. MABs are
    a simplification of Reinforcement Learning (RL) problems, as they can be defined
    entirely by a set of actions and a reward function. This makes them effectively
    like finite *Markov-Decision-Processes* (MDPs) with only one state. Unlike MDPs,
    the actions in MABs are independent of each other, meaning that at each time step,
    the same set of actions will provide the same reward distribution. Therefore,
    if we know the exact reward distribution of the available actions, we can simply
    use a greedy approach to choose the action with the highest payout. In contrast,
    MDPs require us to sometimes take ‘suboptimal’ actions to reach a highly rewarding
    state. However, the problems that can be modelled with MABs are fewer than those
    that can be modelled with MDPs. For example, in the music practice environment
    from the [last post](https://medium.com/@Lando-L/beyond-the-basics-reinforcement-learning-with-jax-part-i-introduction-and-core-concepts-4f85f3478f5),
    it was necessary to introduce states to model our friend’s mood, something which
    cannot be done with MABs.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以设置一个*多臂赌博机*（MAB）实验。MABs是强化学习（RL）问题的简化，因为它们可以完全由一组动作和一个奖励函数定义。这使得它们实际上类似于只有一个状态的有限*马尔可夫决策过程*（MDP）。与MDP不同的是，MABs中的动作是相互独立的，这意味着在每个时间步骤，相同的动作集会提供相同的奖励分布。因此，如果我们知道可用动作的确切奖励分布，我们可以简单地使用贪婪方法选择奖励最高的动作。相反，MDP有时需要我们采取‘次优’动作以达到高奖励状态。然而，MABs可以建模的问题比MDP少。例如，在[上一篇文章](https://medium.com/@Lando-L/beyond-the-basics-reinforcement-learning-with-jax-part-i-introduction-and-core-concepts-4f85f3478f5)中的音乐练习环境中，需要引入状态来建模我们朋友的情绪，而这在MABs中是做不到的。
- en: 'We can model the challenge from our video streaming platform example by defining
    the actions and reward function as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过定义如下的动作和奖励函数来建模我们视频流平台的挑战：
- en: Every time a user visits the website, we must choose which version to display.
    Our options consist of showing the original movie version, or one of the four
    variations created by combining the two thumbnails and two titles.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次用户访问网站时，我们必须选择显示哪个版本。我们的选择包括展示原版电影，或是由两个缩略图和两个标题组合而成的四种变体之一。
- en: Once we selected the variant to display, the user has the option to watch or
    not watch our film. Consequently, our reward function is binary, yielding a reward
    of zero if the user choses not to view the movie and a reward of one if they decide
    to watch the movie.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦我们选择了要展示的变体，用户可以选择观看或不观看我们的电影。因此，我们的奖励函数是二元的，如果用户选择不观看电影则奖励为零，如果他们决定观看电影则奖励为一。
- en: The advantages of the MAB approach compared with traditional A/B testing are
    numerous; it allows for an unlimited number of variants to be tested simultaneously,
    dynamically lowers the frequency of poorer performing variants, and converges
    with fewer samples — all of which lead to cost savings. The downside is that it
    does not provide the statistical significance of a traditional A/B test; however,
    when the implications of the experiment are not affecting people’s wellbeing (as
    is the case in most instances), statistical significance is not strictly necessary.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的A/B测试相比，MAB方法有许多优势；它允许同时测试无限数量的变体，动态降低表现较差的变体的频率，并且需要更少的样本量来收敛——这些都能带来成本节省。缺点是它不提供传统A/B测试的统计显著性；然而，当实验的影响不涉及人们的福祉（如大多数情况下），统计显著性并不是严格必要的。
- en: Let me see some code
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让我看看一些代码
- en: Finally, we will start writing code. For the code examples of this course, we
    will be using Python as our programming language, and will largely use [Jax](https://jax.readthedocs.io/en/latest/index.html)
    as our Machine Learning (ML) framework. Developed by Google, Jax is a Python library
    that is specifically designed for ML research. Unlike Tensorflow and PyTorch,
    it is built with the functional programming (FP) paradigm, making it highly composable
    and promoting the concept of pure functions that have no side effects. This means
    that all state changes, such as parameter updates or splitting of random generators,
    must be done explicitly. Although this may require more lines of code than their
    object-oriented programming (OOP) equivalent, it gives developers full control
    over state changes, leading to increased understanding of the code and fewer surprises.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将开始编写代码。对于本课程的代码示例，我们将使用 Python 作为我们的编程语言，并且主要使用 [Jax](https://jax.readthedocs.io/en/latest/index.html)
    作为我们的机器学习（ML）框架。Jax 是由 Google 开发的 Python 库，专门为 ML 研究设计。与 Tensorflow 和 PyTorch
    不同，它采用了函数式编程（FP）范式，使其高度可组合，并推动了纯函数的概念，即没有副作用。这意味着所有的状态变化，如参数更新或随机生成器的拆分，必须显式地进行。虽然这可能比它们的面向对象编程（OOP）等效方法需要更多的代码行，但它使开发人员对状态变化有了完全的控制，从而增加了对代码的理解，减少了意外情况。
- en: (The full code is available as a Jupyter notebook on [GitHub](https://github.com/Lando-L/reinforcement-learning-with-jax/blob/91404625fd22fe62cb3e644c0ee8d51dd6bf2f24/2-developing-an-exploitative-alternative-to-a-b-testing.ipynb)
    and [Kaggle](https://www.kaggle.com/code/landol/exploitative-alternative-to-a-b-testing).)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: （完整的代码可以在 [GitHub](https://github.com/Lando-L/reinforcement-learning-with-jax/blob/91404625fd22fe62cb3e644c0ee8d51dd6bf2f24/2-developing-an-exploitative-alternative-to-a-b-testing.ipynb)
    和 [Kaggle](https://www.kaggle.com/code/landol/exploitative-alternative-to-a-b-testing)
    上以 Jupyter notebook 形式获得。）
- en: Implementing the environment
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现环境
- en: As the first step, we implement the environment for our video streaming platform
    challenge. At a high-level, whenever someone visits the platform to scroll through
    the available movie options, the environment needs to ask the agent which thumbnails
    and titles to display, and then communicate to the agent which movie the visitor
    chose. The complexity of this task will depend on the maturity of the platform’s
    architecture, which could range from simply changing a few lines of code to developing
    an entirely new service.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步，我们实现了视频流平台挑战的环境。从高层次看，每当有人访问平台以浏览可用的电影选项时，环境需要询问代理显示哪些缩略图和标题，然后通知代理访客选择了哪部电影。这项任务的复杂性将取决于平台架构的成熟度，可能从简单地更改几行代码到开发一个全新的服务。
- en: 'For the purpose of this course, we will keep the environment simple by implementing
    it as a three-step process:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本课程的目的是通过实现一个三步过程来保持环境简单：
- en: Asking the agent which of the five variants it wishes to display.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 询问代理希望展示五个变体中的哪一个。
- en: Simulating the visitor’s choice.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模拟访客的选择。
- en: Communicating the agent its reward for the decision.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通知代理其决策的奖励。
- en: 'Since we are using Jax as our main ML framework, we need to import Jax and
    the three modules *numpy,* *lax* and *random*²:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用 Jax 作为主要 ML 框架，我们需要导入 Jax 和三个模块 *numpy,* *lax* 和 *random*²：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next, we set the constants of our environment, consisting of a random seed to
    ensure replicability, the number of visitors we want to simulate, and the expected
    click rates. It is important to note that, in the real world, the click rates
    are considered unknown. Since we are not running an actual experiment, we must
    simulate the visitors’ click behaviour. To do this, we define different click
    rates for the five variants to imitate the visitors’ preference, with the original
    variant having a click rate of 4.2%, and the four variants having click rates
    of 3%, 3.5%, 3.8%, and 4.5%, respectively.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们设置环境的常量，包括随机种子以确保可重复性、我们想要模拟的访客数量和预期的点击率。需要注意的是，在现实世界中，点击率被认为是未知的。由于我们没有进行实际实验，我们必须模拟访客的点击行为。为此，我们为五个变体定义不同的点击率，以模仿访客的偏好，其中原始变体的点击率为
    4.2%，四个变体的点击率分别为 3%、3.5%、3.8% 和 4.5%。
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Finally, we define a generic function that simulates a user visit, or a single
    step in our environment. This function comprises three steps and is close to the
    high-level implementation we set out earlier:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义了一个通用函数，用于模拟用户访问或环境中的单一步骤。该函数包括三个步骤，接近我们之前设定的高层次实现：
- en: Executing the agent’s policy to determine which variant to display to the user.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行代理的策略以确定显示给用户的变体。
- en: Randomly simulating whether the user clicked on the movie or not.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机模拟用户是否点击了电影。
- en: Updating the agent’s parameters based on the variant shown and the associated
    reward.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据显示的变体和相关的奖励更新代理的参数。
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Before we continue, let us discuss the function signature. If we fix the parameters
    *click_rates*, *policy_fn*, and *update_fn*, the *visit* function takes the environment
    state and the current timestep as its parameters and returns a tuple containing
    the next environment state and a boolean value that encodes the binary reward.
    In Haskell notation the function signature would looks like this:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们讨论一下函数签名。如果我们固定参数*click_rates*、*policy_fn*和*update_fn*，*visit*函数接受环境状态和当前时间步作为其参数，并返回一个包含下一个环境状态和一个编码二元奖励的布尔值的元组。在Haskell表示法中，函数签名看起来像这样：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Hence, to simulate the *n-th* step in our environment we pass the function the
    *n-th* state and *n-th* timestep*,* and we receive the *(n+1)-th* state and the
    *n-th* reward. For the *(n+1)-th* step, we use the same function, by passing it
    the *(n+1)-th* state returned by the last function call and the time step *n+1*.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了在我们的环境中模拟第*n*步，我们将函数传递给第*n*状态和第*n*时间步，*并接收第*(n+1)*状态和第*n*奖励。对于第*(n+1)*步，我们使用相同的函数，传递给它上一个函数调用返回的第*(n+1)*状态和时间步*n+1*。
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Having to pass a state and timestep parameter to every call of the *visit* function
    may seem cumbersome to people accustomed to OOP. However, in this example, using
    a pure function implementation for the environment has several advantages over
    an object-oriented approach. Firstly, it explicitly states which parameters the
    environment depends on, thus eliminating any hidden global variables that may
    influence the outcome. Secondly, it makes it easier to test the environment with
    various states and timesteps without having to set and read the environment’s
    internal state. Lastly, we will discover a useful function from the Jax library,
    which offers state management, thus drastically reducing the code needed at the
    call-side.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于习惯于面向对象编程（OOP）的人来说，每次调用*visit*函数时都必须传递状态和时间步参数可能会显得繁琐。然而，在这个例子中，使用纯函数实现环境相比于面向对象的方法有几个优势。首先，它明确了环境依赖于哪些参数，从而消除了可能影响结果的隐藏全局变量。其次，它使得测试环境与各种状态和时间步变得更加容易，而不必设置和读取环境的内部状态。最后，我们将发现来自Jax库的一个有用函数，它提供了状态管理，从而大幅减少了调用端所需的代码。
- en: Implementing the policies
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现策略
- en: With the environment in place, we can now implement our decision-making process
    or policies. We have already identified an optimal policy for MABs. Given the
    action-value distribution, the best action to take is the one with the highest
    expected payout, which is referred to as *exploitation*. However, since the actual
    action-value distribution is unknown to us, we must try out all available options
    at least once in order to estimate the distribution, a process often referred
    to as *exploration*. The delicate balance between exploration and exploitation
    is a recurrent theme in RL and will be discussed in more detail throughout the
    course.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 环境就绪后，我们现在可以实现我们的决策过程或策略。我们已经确定了MABs的最佳策略。考虑到动作-价值分布，采取的最佳行动是预期收益最高的那个，这被称为*exploitation*。然而，由于我们不知道实际的动作-价值分布，我们必须尝试所有可用的选项至少一次，以估计分布，这个过程通常被称为*exploration*。探索与利用之间的微妙平衡是强化学习中的一个反复出现的主题，并将在整个课程中详细讨论。
- en: The three policies we will cover in this blog post are the *epsilon-greedy*
    policy, the *Boltzmann* policy, and the *upper-confidence-bound* policy. All of
    which are *action-value methods*, meaning they explicitly estimate the values
    of all actions and base their decision making on these estimates. At the end we
    will cover a bonus policy, which is based on the *Thompson sampling* heuristic
    and is considered a *bayesian method*.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这篇博客文章中介绍的三种策略是*epsilon-greedy*策略、*Boltzmann*策略和*upper-confidence-bound*策略。它们都是*action-value
    methods*，即它们明确估计所有动作的值，并基于这些估计做出决策。最后，我们将介绍一种基于*Thompson sampling*启发式方法的奖励策略，它被认为是一种*bayesian
    method*。
- en: Action-value methods
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动作-价值方法
- en: The simplest way for an agent to estimate action-values is to average the rewards
    it has received for each variant so far. Here, *Q* denotes the agent’s action-value
    estimate of variant *a* at timestep *t.*
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 代理估计动作价值的最简单方法是平均它迄今为止收到的每个变体的奖励。在这里，*Q* 表示代理在时间步 *t* 对变体 *a* 的动作价值估计。
- en: '![](../Images/408d5f320d66872fc42a31e65dd673e4.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/408d5f320d66872fc42a31e65dd673e4.png)'
- en: Rather than re-calculating the average every round, we can implement action-value
    estimation in an incremental fashion. Where *Q* stores the current action-value
    estimate of each variant *a, N* counts how often *a* was shown, and *R* denotes
    the reward received at timestep *t*.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 与其每轮都重新计算平均值，不如以增量方式实现动作价值估计。在这里，*Q* 存储每个变体 *a* 当前的动作价值估计，*N* 计数 *a* 出现的频次，*R*
    表示在时间步 *t* 收到的奖励。
- en: '![](../Images/d129c762cd14f25aa335c1c61ac7dfc5.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d129c762cd14f25aa335c1c61ac7dfc5.png)'
- en: Let us implement two functions to work with action-value estimates. The first
    function initialises the lookup tables *Q* and *N* for each variant *a*, setting
    the estimates of all variants to an optimistic initial value of 1 (or a click
    rate of 100%). The second function updates *Q* and *N* according to the incremental
    update definition described above.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现两个函数来处理动作价值估计。第一个函数初始化每个变体 *a* 的查找表 *Q* 和 *N*，将所有变体的估计值设置为乐观的初始值 1（或点击率
    100%）。第二个函数根据上面描述的增量更新定义更新 *Q* 和 *N*。
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We chose to implement the estimate action-value initialisation and update using
    a functional approach, similar to the environment function implementation. Jax
    arrays, unlike numpy arrays, are immutable and therefore cannot be updated in-place;
    instead, each update returns a new copy with the applied changes, while the original
    array remains unchanged.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择使用函数式方法来实现动作价值估计的初始化和更新，这类似于环境函数的实现。与 numpy 数组不同，Jax 数组是不可变的，因此不能就地更新；相反，每次更新会返回一个包含所做更改的新副本，而原始数组保持不变。
- en: Epsilon-greedy policy
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Epsilon-greedy 策略
- en: The epsilon-greedy policy defines a stochastic approach to balancing the exploration
    and exploitation trade-off. Guided by the hyperparameter *ε*, it randomly decided
    between selecting the variant *a* with the highest action-value *Q* or selecting
    a uniformly random variant.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: epsilon-greedy 策略定义了一种随机方法来平衡探索和利用的权衡。根据超参数 *ε*，它随机决定是选择具有最高动作价值 *Q* 的变体 *a*，还是选择一个均匀随机的变体。
- en: '![](../Images/ba8de89e9dd50665f3b42e008be29b3e.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba8de89e9dd50665f3b42e008be29b3e.png)'
- en: In Jax we can define conditional policies using the *cond* function. It takes
    a predicate, two functions, and a variable amount of arguments. Depending on the
    result of the predicate *cond* employs one of the two functions passing it the
    given arguments.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Jax 中，我们可以使用 *cond* 函数定义条件策略。它接受一个谓词、两个函数和一个可变数量的参数。根据谓词的结果，*cond* 采用两个函数中的一个，并传递给它给定的参数。
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Boltzmann policy
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Boltzmann 策略
- en: 'The Boltzmann or softmax policy is similar to the epsilon-greedy policy in
    that it is a stochastic policy based on action-value estimates. This approach
    randomly samples a variant *a* from the probability distribution that results
    from applying the softmax function to the action-value estimates *Q*. The exploration-exploitation
    trade-off can be controlled through the temperature hyperparameter *τ*, where
    lower temperatures favour exploitation and higher temperatures promote exploration.
    The probability *P* of each variant to be selected is defined by:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Boltzmann 或 softmax 策略类似于 epsilon-greedy 策略，因为它是一种基于动作价值估计的随机策略。这种方法从应用 softmax
    函数到动作价值估计 *Q* 后得到的概率分布中随机抽取一个变体 *a*。探索与利用的权衡可以通过温度超参数 *τ* 来控制，其中较低的温度有利于利用，较高的温度促进探索。每个变体被选择的概率
    *P* 定义如下：
- en: '![](../Images/f61eeaa02bfa7055380975cc6a880f67.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f61eeaa02bfa7055380975cc6a880f67.png)'
- en: In Jax, this can be easily implemented by utilizing the *choice* function from
    the random module, parameterised by the softmax function applied to the action-value
    estimates.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Jax 中，这可以通过利用来自随机模块的 *choice* 函数来轻松实现，该函数以应用于动作价值估计的 softmax 函数作为参数。
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Upper-Confidence-Bound policy
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Upper-Confidence-Bound 策略
- en: We will now discuss a policy with a deterministic approach to balancing exploration
    and exploitation. Like the formerly discussed policies, it encourages exploitation
    by prioritising variants with high action-value estimates. However, instead of
    relying on stochasticity for exploration, it leverages a heuristic that encourages
    the selection of variants with low selection counts.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将讨论一种具有确定性方法的策略来平衡探索与利用。与前面讨论的策略类似，它通过优先选择具有高动作值估计的变体来鼓励利用。然而，它并不依赖随机性来进行探索，而是利用一种启发式方法来鼓励选择选择次数较少的变体。
- en: The heuristic accomplishes this by *remaining optimistic in the face of uncertainty*.
    Meaning, every variant is given the benefit of the doubt of being better than
    our current action-value estimate. During the experiment, each time a variant
    is selected and a real reward is observed, we get more confident in our action-value
    estimate and decrease the benefit of the doubt for that variant.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这个启发式方法通过*在面对不确定性时保持乐观*来实现。这意味着，每个变体都被赋予了比我们当前的动作值估计更好的怀疑余地。在实验过程中，每次选择一个变体并观察到实际奖励时，我们对我们的动作值估计会变得更有信心，并减少该变体的怀疑余地。
- en: Formally, we define this optimistic guess as the variants’ *upper-confidence-bound*
    (UCB) which is scaled by the confidence hyperparameter *c* and added it to the
    current action-value estimate. Finally, we select the variant with the highest
    sum.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 从形式上讲，我们将这种乐观猜测定义为变体的*上置信界*（UCB），它由置信超参数*c*缩放，并加到当前的动作值估计上。最后，我们选择总和最高的变体。
- en: '![](../Images/c3220146cde79e66ec374fcf4aadc4e0.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3220146cde79e66ec374fcf4aadc4e0.png)'
- en: 'The UCB policy is the first we discovered that rewards exploration as well
    as exploitation:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: UCB策略是我们发现的第一个同时奖励探索和利用的策略：
- en: Given two variants with the same action-value estimate *Q*, we will select the
    variant with a lower selection count *N*.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于两个具有相同动作值估计*Q*的变体，我们将选择选择次数较少的变体*N*。
- en: Given two variants with the same selection count *N*, we will select the variant
    with a higher action-value estimate *Q*.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于两个具有相同选择次数*N*的变体，我们将选择具有更高动作值估计*Q*的变体。
- en: To ensure a consistent function definition for all policies, the UCB policy
    takes a random number generator parameter, even though it is a deterministic algorithm.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保所有策略的函数定义一致，UCB策略需要一个随机数生成器参数，即使它是一个确定性算法。
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Bayesian methods
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 贝叶斯方法
- en: The discussed action-value methods make point estimates for the unknown click
    rates of our five variants. However, we now adopt a more Bayesian approach and
    treat the variants’ click rates as a set of independent random variables. Specifically,
    we define our current belief of a variants click rate *C* by modelling it as a
    random variable following a *beta* distribution².
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论的动作值方法对我们五个变体的未知点击率进行点估计。然而，我们现在采用一种更贝叶斯的方法，将变体的点击率视为一组独立的随机变量。具体而言，我们通过将其建模为遵循*贝塔*分布的随机变量²来定义我们对变体点击率*C*的当前信念。
- en: '![](../Images/5bdde6a5e07ca711a8a6786940822d5e.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5bdde6a5e07ca711a8a6786940822d5e.png)'
- en: 'The beta distribution is characterized by two parameters, *a* and *b*, which
    can be interpreted as the number of times variant *i* was clicked versus the number
    of times it was not clicked when shown. When comparing bayesian methods with action-value
    methods, we can use the expected value *E* of the random variable *C* to define
    our best guess, which can be determined by dividing the number of times a variant
    was clicked by the number of times it was shown:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 贝塔分布由两个参数*a*和*b*特征化，这些参数可以解释为变体*i*在展示时被点击的次数与未被点击的次数。比较贝叶斯方法与动作值方法时，我们可以使用随机变量*C*的期望值*E*来定义我们的最佳猜测，这可以通过将变体被点击的次数除以展示的次数来确定：
- en: '![](../Images/4fe83c7070c7a2e2397fd35be18f8c5b.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4fe83c7070c7a2e2397fd35be18f8c5b.png)'
- en: We define two functions to work with beta distributions, analogous to the action-value
    methods. The first initialises a uniform beta prior for each variant, while the
    second function calculates the posterior beta distribution by incrementing either
    the *a* or the *b* parameter by one.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了两个函数来处理贝塔分布，类似于动作值方法。第一个函数初始化每个变体的均匀贝塔先验，而第二个函数通过将*a*或*b*参数增加一来计算后验贝塔分布。
- en: '[PRE9]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Thompson sampling policy
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 汤普森采样策略
- en: The TS policy is based on a two-step heuristic, which works by drawing random
    click rate samples from our beta distribution, and then selecting the variant
    with the highest sample click rate. The feedback we receive is then instantaneously
    incorporated into the beta distribution of that variant, narrowing the distribution
    closer to the actual click rate.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: TS策略基于两步启发式方法，首先从我们的贝塔分布中抽取随机点击率样本，然后选择点击率样本最高的变体。我们收到的反馈会立即被纳入该变体的贝塔分布，从而使分布更接近实际点击率。
- en: 'Like the UCB policy, this approach rewards both exploration and exploitation:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 与UCB策略类似，这种方法同时奖励探索和利用：
- en: Given two variants with the same mean, the variant with a higher variance has
    a higher chance of being selected, since it has a broader distribution and will
    lead more often to higher action-values when sampled.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定两个均值相同的变体，方差更大的变体有更高的被选择几率，因为它具有更广泛的分布，采样时更容易得到更高的行动值。
- en: Given two variants with the same variance, the variant with the higher mean
    is selected more often, as it is more likely to sample a greater action-value.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定两个方差相同的变体，均值更高的变体更常被选择，因为它更有可能采样到更高的行动值。
- en: For the implementation of the TS policy we use Jax’s *random* module to sample
    random click rates based on the variants’ beta distribution and then select the
    variant with the highest sample.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于TS策略的实现，我们使用Jax的`*random*`模块来根据变体的贝塔分布采样随机点击率，然后选择点击率样本最高的变体。
- en: '[PRE10]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Implementing the evaluation
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现评估
- en: 'With the environment and policies in place, we can finally conduct our experiment
    and compare the results. Before we continue, I want to highlight that this experiment
    is intended to demonstrate how the algorithms work, not to empirically evaluate
    their performance. In order to execute the following implementations, we need
    to import the *partial* function from Python’s *functools* library and *pyplot*
    from *matplotlib*:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 环境和策略准备好后，我们终于可以进行实验并比较结果。在继续之前，我想强调，这个实验旨在演示算法的工作原理，而不是实证评估它们的性能。为了执行以下实现，我们需要从Python的`*functools*`库中导入`*partial*`函数，并从`*matplotlib*`中导入`*pyplot*`：
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The *evaluate* function is responsible for executing the *visit* function, as
    guided by the set of constants, the parameter initialisation and update functions,
    and the policy. The output of the evaluation is the final environment state, which
    includes the policy’s final parameters and final random number generator state,
    as well as the click history. We leverage Jax’s *scan* function to ensure that
    the experiment state is carried over and user clicks are accumulated. Moreover,
    *just-in-time* (JIT) compilation is employed to optimise performance, while *partial*
    is utilised to fix the *click_rate*, *policy_fn*, and *update_fn* parameters of
    the *visit* function so it matches the expected signature.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`*evaluate*`函数负责执行`*visit*`函数，这一过程由常量集合、参数初始化和更新函数以及策略指导。评估的输出是最终环境状态，包括策略的最终参数和最终随机数生成器状态，以及点击历史。我们利用Jax的`*scan*`函数来确保实验状态得以传递，用户点击数被累计。此外，`*just-in-time*`（JIT）编译用于优化性能，而`*partial*`则用于固定`*click_rate*`、`*policy_fn*`和`*update_fn*`参数，以确保它们与预期签名匹配。'
- en: '[PRE12]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The *regret* function is the last component of our evaluation. In RL lingo,
    regret is defined as the amount of reward we miss out on by taking a suboptimal
    action, and can only be calculated when the optimal action is known. Given the
    click history, our *regret* function calculates the regret of the action taken
    for every step of the environment.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`*regret*`函数是我们评估的最后一个组件。在强化学习术语中，`regret`定义为我们因采取次优行动而错失的奖励量，只有在知道最佳行动的情况下才能计算。根据点击历史，我们的`*regret*`函数计算环境中每一步采取的行动的遗憾值。'
- en: '[PRE13]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Next we run the evaluation for all four policies and visualise the regret. Note
    that the hyperparameters for the policies have not been fine-tuned, instead they
    are set to generic default values that are suitable for a wide variety of MAB
    problems.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们对所有四种策略进行评估并可视化遗憾值。请注意，策略的超参数尚未进行微调，而是设置为适合各种MAB问题的通用默认值。
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The resulting graph is shown below, plotting the regret of our policies over
    the number of visits. We can clearly see that all policies outperform a hypothetical
    A/B testing scenario in terms of regret. The epsilon-greedy and TS policies appear
    to perform slightly better than the boltzmann and UCB policies in this particular
    scenario.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图如下面所示，绘制了我们的策略在访问次数上的遗憾。我们可以清楚地看到，所有策略在遗憾方面都优于假设的 A/B 测试场景。在这个特定场景中，epsilon-贪婪和
    TS 策略的表现似乎略优于 Boltzmann 和 UCB 策略。
- en: '![](../Images/b991caf84e166570ab27028e50317295.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b991caf84e166570ab27028e50317295.png)'
- en: Visualisation created by the author.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 作者创建的可视化图。
- en: Beyond the regret
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越遗憾
- en: When conducting machine learning experiments, it is generally advised to define
    a set of metrics that measure qualitative performance aside from the objective
    function. For RL experiments, however, it is often not so straightforward to identify
    suitable metrics. In our case, I chose we dig deeper into the final parameters
    of our policies and compare the action-value estimation to the actual click rates
    and how often each variant was selected under the different policies.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行机器学习实验时，通常建议定义一组指标来衡量除目标函数外的定性性能。然而，对于强化学习实验，确定合适的指标往往并不那么简单。在我们的案例中，我选择深入研究我们策略的最终参数，并将动作值估计与实际点击率以及不同策略下每个变体的选择频率进行比较。
- en: Action-value estimates
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动作值估计
- en: We compare the accuracy of the policies’ action-value estimates by calculating
    the *root mean squared error* (RMSE) against the ground truth click rates, as
    seen in the table below.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过计算与实际点击率的 *均方根误差*（RMSE）来比较策略的动作值估计的准确性，如下表所示。
- en: '[PRE15]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Surprisingly, despite performing well overall, it turns out that the epsilon-greedy
    policy has prematurely honed in on exploiting the second-best variant, while the
    other policies have correctly identified the last variant as the optimal one.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 出乎意料的是，尽管整体表现良好，但事实证明 epsilon-贪婪策略过早地专注于剥削第二优的变体，而其他策略正确地识别了最后一个变体为最佳变体。
- en: The Boltzmann policy performed the best in terms of predicting the variants’
    click rates.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boltzmann 策略在预测变体的点击率方面表现最佳。
- en: The UCB policy and TS policy performed comparably well. While UCB appears to
    overestimate the value of V4 and underestimate V2, TS seems to overestimate V1
    and underestimate the original version.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UCB 策略和 TS 策略表现相当。虽然 UCB 似乎高估了 V4 的价值并低估了 V2，但 TS 似乎高估了 V1 并低估了原始版本。
- en: Variant counter
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变体计数器
- en: The second characteristic I want to discuss is how often each variant was selected
    under the different policies. Here, we simply compare the absolute variant counts
    of our experiment with 10,000 visitors.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我想讨论的第二个特征是不同策略下每个变体的选择频率。在这里，我们仅仅比较了实验中 10,000 名访客的绝对变体计数。
- en: '[PRE16]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The epsilon greedy algorithm was found to exploit too early, as it selected
    the original version over 83% of the time.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现 epsilon-贪婪算法过早地进行剥削，因为它选择原始版本的频率超过了 83%。
- en: The Boltzmann policy was remarkably accurate at predicting the variants’ click
    rates, likely due to its uniform exploration of all variants.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boltzmann 策略在预测变体的点击率方面表现出色，这可能是因为它对所有变体进行了均匀的探索。
- en: The TS policy was found to have a lower regret value than the UCB policy, likely
    due to its more extensive exploitation of the optimal variant.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TS 策略的遗憾值低于 UCB 策略，这可能是因为它更广泛地剥削了最佳变体。
- en: Conclusion
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: Upon further examination of our observations, we have identified several areas
    for improvement in our current experiment setup.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在进一步检查我们的观察结果后，我们发现当前实验设置中有几个改进的地方。
- en: The epsilon-greedy policy appears to be overly exploiting; as a result, we should
    increase the epsilon hyperparameter to encourage a more comprehensive exploration.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: epsilon-贪婪策略似乎过于依赖剥削，因此我们应该增加 epsilon 超参数，以鼓励更全面的探索。
- en: The Boltzmann policy appears to have explored much more than it has exploited,
    resulting in accurate predictions of the click rates, albeit missing out on rewards.
    This may be an indication that its temperature hyperparameter should be increased.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boltzmann 策略似乎进行了更多的探索而非剥削，导致准确的点击率预测，但错过了一些奖励。这可能表明其温度超参数应该增加。
- en: The UCB and TS policies performed well, but there is still room for improvement.
    We can fine-tune the confidence hyperparameter and the initial beta prior respectively
    to further optimize their performance.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UCB 和 TS 策略表现良好，但仍有改进的空间。我们可以分别调整置信度超参数和初始 beta 先验，以进一步优化它们的性能。
- en: 'In addition to the suggested hyperparameter changes, I encourage the interested
    reader to explore more sophisticated improvements as an exercise:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 除了建议的超参数更改外，我鼓励感兴趣的读者作为练习探索更复杂的改进：
- en: Utilizing a variety of random seeds and averaging the outcomes to reduce bias
    in the experiment results.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用多种随机种子并平均结果，以减少实验结果中的偏差。
- en: Implementing a dynamic scheduler to gradually reduce exploration over time,
    such as decreasing the epsilon or confidence parameter in epsilon-greedy and UCB
    policies, and increasing the temperature parameter in the Boltzmann policy.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施动态调度器，逐渐减少探索行为，例如在 epsilon-贪婪和 UCB 策略中降低 epsilon 或置信度参数，在 Boltzmann 策略中增加温度参数。
- en: '**What’s next?**'
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**接下来是什么？**'
- en: MAB are an incredibly versatile tool for solving a wide range of problems. In
    this post, we discussed four MAB policies that could be used to solve the challenge
    of the video streaming platform scenario. Other MAB use cases include ad selection
    for online advertisement, prioritisation of query results for search engines,
    and allocation of resources for different projects.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: MAB 是一种极其多用途的工具，用于解决各种问题。在本文中，我们讨论了四种 MAB 策略，这些策略可以用来解决视频流媒体平台场景中的挑战。其他 MAB
    使用案例包括在线广告的广告选择、搜索引擎查询结果的优先级排序，以及不同项目的资源分配。
- en: Moreover, MABs can be further extended to incorporate additional information.
    In our example, the decision-making process could be improved if we had access
    to more contextual data, such as individual user preferences and feature vectors
    for each variant. Such problems are referred to as contextual bandit problems,
    which we will explore further in later posts focusing on approximate solution
    methods.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，MAB 还可以进一步扩展以包含额外的信息。在我们的例子中，如果我们能够获取更多的上下文数据，例如个体用户的偏好和每个变体的特征向量，那么决策过程可能会得到改善。这类问题被称为上下文赌博问题，我们将在后续的文章中进一步探讨近似解决方法。
- en: '[1] Schwartz, Barry, and Barry Schwartz. “The paradox of choice: Why more is
    less.” New York: Ecco, 2004.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Schwartz, Barry, and Barry Schwartz. “选择的悖论：为什么更多就是更少。” 纽约：Ecco，2004。'
- en: '[2] We chose the Beta distribution to model the click rates, as it is the conjugate
    prior of the Bernoulli distribution which is our likelihood function of the visitor
    clicking or not clicking the shown variant. For a better understanding of probability
    distributions and Bayesian inference, I recommend consulting Christopher M. Bishop
    and Nasser M. Nasrabadi’s book “Pattern Recognition and Machine Learning”.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] 我们选择 Beta 分布来建模点击率，因为它是伯努利分布的共轭先验，而伯努利分布是我们用来描述访客点击或不点击展示变体的似然函数。为了更好地理解概率分布和贝叶斯推断，我推荐参考
    Christopher M. Bishop 和 Nasser M. Nasrabadi 的《模式识别与机器学习》一书。'
