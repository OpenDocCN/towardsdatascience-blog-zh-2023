- en: Deploying LLMs On Amazon SageMaker With DJL Serving
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/deploying-llms-on-amazon-sagemaker-with-djl-serving-8220e3cfad0c?source=collection_archive---------8-----------------------#2023-06-07](https://towardsdatascience.com/deploying-llms-on-amazon-sagemaker-with-djl-serving-8220e3cfad0c?source=collection_archive---------8-----------------------#2023-06-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Deploy BART on Amazon SageMaker Real-Time Inference
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ram-vegiraju.medium.com/?source=post_page-----8220e3cfad0c--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page-----8220e3cfad0c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8220e3cfad0c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8220e3cfad0c--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page-----8220e3cfad0c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6e49569edd2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeploying-llms-on-amazon-sagemaker-with-djl-serving-8220e3cfad0c&user=Ram+Vegiraju&userId=6e49569edd2b&source=post_page-6e49569edd2b----8220e3cfad0c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8220e3cfad0c--------------------------------)
    ·8 min read·Jun 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8220e3cfad0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeploying-llms-on-amazon-sagemaker-with-djl-serving-8220e3cfad0c&user=Ram+Vegiraju&userId=6e49569edd2b&source=-----8220e3cfad0c---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8220e3cfad0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeploying-llms-on-amazon-sagemaker-with-djl-serving-8220e3cfad0c&source=-----8220e3cfad0c---------------------bookmark_footer-----------)![](../Images/27d3d2701b592cc5346c24eb2c67afe6.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Image from [Unsplash](https://unsplash.com/photos/CejqWHRRXUQ)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) and Generative AI continue to take over the Machine
    Learning and general tech space in 2023\. With the LLM expansion has come an influx
    of new models that continue to improve at a stunning rate.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: While the accuracy and performance of these models are incredible, they have
    their own set of challenges in terms of hosting these models. Without model hosting,
    it is hard to recognize the value that these LLMs provide in real-world applications.
    What are the specific challenges with LLM hosting and performance tuning?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: How can we load these larger models that are scaling up to past 100s of GBs
    in size?
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何加载这些扩展到超过100 GB的大型模型？
- en: How can we properly apply model partitioning techniques to efficiently utilize
    hardware while not compromising on model accuracy?
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何正确地应用模型划分技术以高效利用硬件，同时不妥协模型准确性？
- en: How can we fit these models on a singular GPU or multiple?
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何将这些模型适配到单个GPU或多个GPU上？
- en: 'These are all challenging questions that are addressed and abstracted out through
    a model server known as [DJL Serving](http://djl.ai/serving/). DJL Serving is
    a high performance universal solution that integrates directly with various model
    partitioning frameworks such as the following: [HuggingFace Accelerate](https://huggingface.co/docs/accelerate/index),
    [DeepSpeed](https://github.com/microsoft/DeepSpeed), and [FasterTransformers](https://github.com/NVIDIA/FasterTransformer).
    With DJL Serving you…'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是通过被称为[DJL Serving](http://djl.ai/serving/)的模型服务器解决并抽象出来的挑战性问题。DJL Serving是一个高性能的通用解决方案，直接集成了各种模型划分框架，如[HuggingFace
    Accelerate](https://huggingface.co/docs/accelerate/index)、[DeepSpeed](https://github.com/microsoft/DeepSpeed)和[FasterTransformers](https://github.com/NVIDIA/FasterTransformer)。通过DJL
    Serving，你可以…
