- en: Deploying LLMs On Amazon SageMaker With DJL Serving
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/deploying-llms-on-amazon-sagemaker-with-djl-serving-8220e3cfad0c?source=collection_archive---------8-----------------------#2023-06-07](https://towardsdatascience.com/deploying-llms-on-amazon-sagemaker-with-djl-serving-8220e3cfad0c?source=collection_archive---------8-----------------------#2023-06-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Deploy BART on Amazon SageMaker Real-Time Inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ram-vegiraju.medium.com/?source=post_page-----8220e3cfad0c--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page-----8220e3cfad0c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8220e3cfad0c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8220e3cfad0c--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page-----8220e3cfad0c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6e49569edd2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeploying-llms-on-amazon-sagemaker-with-djl-serving-8220e3cfad0c&user=Ram+Vegiraju&userId=6e49569edd2b&source=post_page-6e49569edd2b----8220e3cfad0c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8220e3cfad0c--------------------------------)
    ·8 min read·Jun 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8220e3cfad0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeploying-llms-on-amazon-sagemaker-with-djl-serving-8220e3cfad0c&user=Ram+Vegiraju&userId=6e49569edd2b&source=-----8220e3cfad0c---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8220e3cfad0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeploying-llms-on-amazon-sagemaker-with-djl-serving-8220e3cfad0c&source=-----8220e3cfad0c---------------------bookmark_footer-----------)![](../Images/27d3d2701b592cc5346c24eb2c67afe6.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image from [Unsplash](https://unsplash.com/photos/CejqWHRRXUQ)
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) and Generative AI continue to take over the Machine
    Learning and general tech space in 2023\. With the LLM expansion has come an influx
    of new models that continue to improve at a stunning rate.
  prefs: []
  type: TYPE_NORMAL
- en: While the accuracy and performance of these models are incredible, they have
    their own set of challenges in terms of hosting these models. Without model hosting,
    it is hard to recognize the value that these LLMs provide in real-world applications.
    What are the specific challenges with LLM hosting and performance tuning?
  prefs: []
  type: TYPE_NORMAL
- en: How can we load these larger models that are scaling up to past 100s of GBs
    in size?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can we properly apply model partitioning techniques to efficiently utilize
    hardware while not compromising on model accuracy?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can we fit these models on a singular GPU or multiple?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These are all challenging questions that are addressed and abstracted out through
    a model server known as [DJL Serving](http://djl.ai/serving/). DJL Serving is
    a high performance universal solution that integrates directly with various model
    partitioning frameworks such as the following: [HuggingFace Accelerate](https://huggingface.co/docs/accelerate/index),
    [DeepSpeed](https://github.com/microsoft/DeepSpeed), and [FasterTransformers](https://github.com/NVIDIA/FasterTransformer).
    With DJL Serving you…'
  prefs: []
  type: TYPE_NORMAL
