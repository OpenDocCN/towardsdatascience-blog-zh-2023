- en: Tree of Thoughts Prompting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/tree-of-thoughts-prompting-65a3e51f9ac4?source=collection_archive---------7-----------------------#2023-12-22](https://towardsdatascience.com/tree-of-thoughts-prompting-65a3e51f9ac4?source=collection_archive---------7-----------------------#2023-12-22)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Solving multi-step problems with LLMs via deliberate planning and exploration…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wolfecameron.medium.com/?source=post_page-----65a3e51f9ac4--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----65a3e51f9ac4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----65a3e51f9ac4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----65a3e51f9ac4--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----65a3e51f9ac4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftree-of-thoughts-prompting-65a3e51f9ac4&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----65a3e51f9ac4---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----65a3e51f9ac4--------------------------------)
    ·20 min read·Dec 22, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F65a3e51f9ac4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftree-of-thoughts-prompting-65a3e51f9ac4&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----65a3e51f9ac4---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F65a3e51f9ac4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftree-of-thoughts-prompting-65a3e51f9ac4&source=-----65a3e51f9ac4---------------------bookmark_footer-----------)![](../Images/3a1d8bce28681223f010bacd921267e1.png)'
  prefs: []
  type: TYPE_NORMAL
- en: (Photo by [Johann Siemens](https://unsplash.com/@emben?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/green-tree-on-grassland-during-daytime-EPy0gBJzzZU?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
  prefs: []
  type: TYPE_NORMAL
- en: As large language models (LLMs) first started to gain in popularity, they were
    criticized for their shortcomings in solving complex, reasoning-based problems.
    Although scaling up these models (i.e., more parameters and more data) provided
    a near-uniform performance improvement across tasks, we saw virtually no boost
    in performance on reasoning-based tasks with modern LLMs. This changed with the
    proposal of advanced prompting techniques, such as chain of thought prompting
    [2] and self-consistency [3]. Such methods showed us that LLMs are more than capable
    of reasoning and solving complex, multi-step problems. They just have to be properly
    prompted to fully leverage these abilities.
  prefs: []
  type: TYPE_NORMAL
- en: “It is perhaps surprising that underlying all this progress is still the original
    autoregressive mechanism for generating text, which makes token-level decisions
    one by one and in a left-to-right fashion.” *— from [1]*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Even if proper prompting can enable LLMs to solve complex problems, these techniques
    are lacking. Namely, we typically *i)* provide a prompt to the LLM and *ii)* expect
    the model to use next token prediction to generate a full solution. Certain approaches
    may generate solutions in a step-by-step fashion (e.g., least-to-most…
  prefs: []
  type: TYPE_NORMAL
