- en: Build low-latency and scalable ML model prediction pipelines using Spark Structured
    Streaming and MLflow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark结构化流处理和MLflow构建低延迟且可扩展的ML模型预测管道
- en: 原文：[https://towardsdatascience.com/build-low-latency-and-scalable-ml-model-prediction-pipelines-using-spark-structured-streaming-and-535ae5244877?source=collection_archive---------23-----------------------#2023-01-10](https://towardsdatascience.com/build-low-latency-and-scalable-ml-model-prediction-pipelines-using-spark-structured-streaming-and-535ae5244877?source=collection_archive---------23-----------------------#2023-01-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/build-low-latency-and-scalable-ml-model-prediction-pipelines-using-spark-structured-streaming-and-535ae5244877?source=collection_archive---------23-----------------------#2023-01-10](https://towardsdatascience.com/build-low-latency-and-scalable-ml-model-prediction-pipelines-using-spark-structured-streaming-and-535ae5244877?source=collection_archive---------23-----------------------#2023-01-10)
- en: MLOps in practice series — sharing design and implementation patterns of critical
    MLOps component. The focus of today’s article is on building model prediction
    pipelines.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实践中的MLOps系列——分享关键MLOps组件的设计和实施模式。今天文章的重点是构建模型预测管道。
- en: '[](https://medium.com/@weiyunna91?source=post_page-----535ae5244877--------------------------------)[![YUNNA
    WEI](../Images/ffd0dd5c697dd2b4640ade49274d2bf9.png)](https://medium.com/@weiyunna91?source=post_page-----535ae5244877--------------------------------)[](https://towardsdatascience.com/?source=post_page-----535ae5244877--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----535ae5244877--------------------------------)
    [YUNNA WEI](https://medium.com/@weiyunna91?source=post_page-----535ae5244877--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@weiyunna91?source=post_page-----535ae5244877--------------------------------)[![YUNNA
    WEI](../Images/ffd0dd5c697dd2b4640ade49274d2bf9.png)](https://medium.com/@weiyunna91?source=post_page-----535ae5244877--------------------------------)[](https://towardsdatascience.com/?source=post_page-----535ae5244877--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----535ae5244877--------------------------------)
    [YUNNA WEI](https://medium.com/@weiyunna91?source=post_page-----535ae5244877--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4b47aa84fc4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-low-latency-and-scalable-ml-model-prediction-pipelines-using-spark-structured-streaming-and-535ae5244877&user=YUNNA+WEI&userId=4b47aa84fc4&source=post_page-4b47aa84fc4----535ae5244877---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----535ae5244877--------------------------------)
    ·8 min read·Jan 10, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F535ae5244877&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-low-latency-and-scalable-ml-model-prediction-pipelines-using-spark-structured-streaming-and-535ae5244877&user=YUNNA+WEI&userId=4b47aa84fc4&source=-----535ae5244877---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4b47aa84fc4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-low-latency-and-scalable-ml-model-prediction-pipelines-using-spark-structured-streaming-and-535ae5244877&user=YUNNA+WEI&userId=4b47aa84fc4&source=post_page-4b47aa84fc4----535ae5244877---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----535ae5244877--------------------------------)
    · 8分钟阅读·2023年1月10日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F535ae5244877&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-low-latency-and-scalable-ml-model-prediction-pipelines-using-spark-structured-streaming-and-535ae5244877&user=YUNNA+WEI&userId=4b47aa84fc4&source=-----535ae5244877---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F535ae5244877&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-low-latency-and-scalable-ml-model-prediction-pipelines-using-spark-structured-streaming-and-535ae5244877&source=-----535ae5244877---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F535ae5244877&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-low-latency-and-scalable-ml-model-prediction-pipelines-using-spark-structured-streaming-and-535ae5244877&source=-----535ae5244877---------------------bookmark_footer-----------)'
- en: 'To make ML models work in a real production environment, one of the most critical
    steps is to deploy the trained models for predictions. Model deployment (release)
    is a process that enables you to integrate trained ML models into production to
    make decisions on real-world data. When it comes to model deployment, there are
    generally two types:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 使机器学习模型在实际生产环境中工作，最关键的步骤之一是部署经过训练的模型进行预测。模型部署（发布）是一个过程，它使你能够将训练好的机器学习模型集成到生产环境中，以对实际数据做出决策。谈到模型部署，一般有两种类型。
- en: One is batch prediction where the trained models are called and fed with a batch
    of data at a certain interval (such as once per day or once per week depending
    on how the models are used in certain business contexts), to periodically generate
    predictions for use.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种是批量预测，其中经过训练的模型在某个间隔（如每天一次或每周一次，根据模型在特定业务环境中的使用情况）调用，并处理一批数据，以定期生成用于预测的结果。
- en: The other is online prediction where a trained model is packaged as a REST API
    or a containerized microservice, and the model returns prediction results (generally
    in JSON format) by responding to an API request. With online prediction, the model
    makes predictions in real-time, meaning, as soon as the API is called, a model
    prediction result will be returned. Additionally the model REST API is generally
    integrated as part of a web application for end users or downstream applications
    to interact with.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种是在线预测，其中经过训练的模型被打包成REST API或容器化微服务，模型通过响应API请求返回预测结果（通常为JSON格式）。使用在线预测时，模型实时进行预测，即一旦调用API，就会返回模型预测结果。此外，模型REST
    API通常集成在Web应用程序中，以便最终用户或下游应用程序进行交互。
