- en: Enhanced Large Language Models as Reasoning Engines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 增强的大型语言模型作为推理引擎
- en: 原文：[https://towardsdatascience.com/enhanced-large-language-models-as-reasoning-engines-582bff782113?source=collection_archive---------3-----------------------#2023-12-23](https://towardsdatascience.com/enhanced-large-language-models-as-reasoning-engines-582bff782113?source=collection_archive---------3-----------------------#2023-12-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/enhanced-large-language-models-as-reasoning-engines-582bff782113?source=collection_archive---------3-----------------------#2023-12-23](https://towardsdatascience.com/enhanced-large-language-models-as-reasoning-engines-582bff782113?source=collection_archive---------3-----------------------#2023-12-23)
- en: '[](https://medium.com/@alcarazanthony1?source=post_page-----582bff782113--------------------------------)[![Anthony
    Alcaraz](../Images/6a71a1752677bd07c384246fb0c7f7e8.png)](https://medium.com/@alcarazanthony1?source=post_page-----582bff782113--------------------------------)[](https://towardsdatascience.com/?source=post_page-----582bff782113--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----582bff782113--------------------------------)
    [Anthony Alcaraz](https://medium.com/@alcarazanthony1?source=post_page-----582bff782113--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@alcarazanthony1?source=post_page-----582bff782113--------------------------------)[![Anthony
    Alcaraz](../Images/6a71a1752677bd07c384246fb0c7f7e8.png)](https://medium.com/@alcarazanthony1?source=post_page-----582bff782113--------------------------------)[](https://towardsdatascience.com/?source=post_page-----582bff782113--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----582bff782113--------------------------------)
    [Anthony Alcaraz](https://medium.com/@alcarazanthony1?source=post_page-----582bff782113--------------------------------)'
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F30bc9ffd2f4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fenhanced-large-language-models-as-reasoning-engines-582bff782113&user=Anthony+Alcaraz&userId=30bc9ffd2f4b&source=post_page-30bc9ffd2f4b----582bff782113---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----582bff782113--------------------------------)
    ·12 min read·Dec 23, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F582bff782113&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fenhanced-large-language-models-as-reasoning-engines-582bff782113&user=Anthony+Alcaraz&userId=30bc9ffd2f4b&source=-----582bff782113---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F30bc9ffd2f4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fenhanced-large-language-models-as-reasoning-engines-582bff782113&user=Anthony+Alcaraz&userId=30bc9ffd2f4b&source=post_page-30bc9ffd2f4b----582bff782113---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----582bff782113--------------------------------)
    ·12 min read·2023年12月23日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F582bff782113&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fenhanced-large-language-models-as-reasoning-engines-582bff782113&user=Anthony+Alcaraz&userId=30bc9ffd2f4b&source=-----582bff782113---------------------clap_footer-----------)'
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F582bff782113&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fenhanced-large-language-models-as-reasoning-engines-582bff782113&source=-----582bff782113---------------------bookmark_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F582bff782113&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fenhanced-large-language-models-as-reasoning-engines-582bff782113&source=-----582bff782113---------------------bookmark_footer-----------)'
- en: '*Artificial intelligence software was used to enhance the grammar, flow, and
    readability of this article’s text.*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*人工智能软件被用来增强本文的语法、流畅性和可读性。*'
- en: The recent exponential advances in natural language processing capabilities
    from large language models (LLMs) have stirred tremendous excitement about their
    potential to achieve human-level intelligence. Their ability to produce remarkably
    coherent text and engage in dialogue after exposure to vast datasets seems to
    point towards flexible, general purpose reasoning skills.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，大型语言模型（LLMs）在自然语言处理能力上的指数级进展引发了对其实现人类水平智能潜力的巨大兴奋。它们在接触到大量数据集后，能够生成非常连贯的文本并进行对话，这似乎指向了灵活的、通用的推理能力。
- en: However, a growing chorus of voices urges caution against unchecked optimism
    by highlighting fundamental blindspots that limit neural approaches. LLMs still
    frequently make basic logical and mathematical mistakes that reveal a lack of
    systematicity behind their responses. Their knowledge remains intrinsically statistical
    without deeper semantic structures.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: More complex reasoning tasks further expose these limitations. LLMs struggle
    with causal, counterfactual, and compositional reasoning challenges that require
    going beyond surface pattern recognition. Unlike humans who learn abstract schemas
    to flexibly recombine modular concepts, neural networks memorize correlations
    between co-occurring terms. This results in brittle generalization outside narrow
    training distributions.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: The chasm underscores how human cognition employs structured symbolic representations
    to enable systematic composability and causal models for conceptualizing dynamics.
    We reason by manipulating…
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
