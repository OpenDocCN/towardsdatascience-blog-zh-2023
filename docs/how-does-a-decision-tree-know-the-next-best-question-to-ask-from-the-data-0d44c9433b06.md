# å†³ç­–æ ‘å¦‚ä½•çŸ¥é“ä»æ•°æ®ä¸­è¯¢é—®ä¸‹ä¸€ä¸ªæœ€ä½³é—®é¢˜ï¼Ÿ

> åŸæ–‡ï¼š[`towardsdatascience.com/how-does-a-decision-tree-know-the-next-best-question-to-ask-from-the-data-0d44c9433b06`](https://towardsdatascience.com/how-does-a-decision-tree-know-the-next-best-question-to-ask-from-the-data-0d44c9433b06)

## ä»é›¶å¼€å§‹æ„å»ºä½ è‡ªå·±çš„å†³ç­–æ ‘åˆ†ç±»å™¨ï¼ˆä½¿ç”¨ Pythonï¼‰ï¼Œå¹¶ç†è§£å®ƒå¦‚ä½•ä½¿ç”¨ç†µæ¥åˆ†è£‚èŠ‚ç‚¹

[](https://medium.com/@gurjinderkaur95?source=post_page-----0d44c9433b06--------------------------------)![Gurjinder Kaur](https://medium.com/@gurjinderkaur95?source=post_page-----0d44c9433b06--------------------------------)[](https://towardsdatascience.com/?source=post_page-----0d44c9433b06--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page-----0d44c9433b06--------------------------------) [Gurjinder Kaur](https://medium.com/@gurjinderkaur95?source=post_page-----0d44c9433b06--------------------------------)

Â·å‘å¸ƒäº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----0d44c9433b06--------------------------------) Â·14 åˆ†é’Ÿé˜…è¯»Â·2023 å¹´ 11 æœˆ 17 æ—¥

--

![](img/4f9f5b9174a45c2bfe56d92a8282fcbc.png)

å›¾ç‰‡ç”±[Daniele Levis Pelusi](https://unsplash.com/@yogidan2012?utm_source=medium&utm_medium=referral)æ‹æ‘„ï¼Œæ¥è‡ª[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

## ä»‹ç»

å†³ç­–æ ‘æ˜¯å¤šç”¨é€”çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œèƒ½å¤Ÿæ‰§è¡Œåˆ†ç±»å’Œå›å½’ä»»åŠ¡ã€‚å®ƒä»¬é€šè¿‡è¯¢é—®æœ‰å…³æ•°æ®çš„ç‰¹å¾çš„é—®é¢˜ï¼Œä½¿ç”¨ IF-ELSE ç»“æ„æ¥è·Ÿéšè·¯å¾„ï¼Œæœ€ç»ˆå¾—å‡ºé¢„æµ‹ç»“æœã€‚æŒ‘æˆ˜åœ¨äºç¡®å®šåœ¨æ¯ä¸€æ­¥å†³ç­–è¿‡ç¨‹ä¸­éœ€è¦è¯¢é—®ä»€ä¹ˆé—®é¢˜ï¼Œè¿™ä¹Ÿç­‰åŒäºç¡®å®šåœ¨æ¯ä¸ªå†³ç­–èŠ‚ç‚¹ä¸Šæœ€ä½³åˆ†è£‚çš„æ–¹å¼ã€‚

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†å°è¯•æ„å»ºä¸€ä¸ªç”¨äºç®€å•äºŒåˆ†ç±»ä»»åŠ¡çš„å†³ç­–æ ‘ã€‚æœ¬æ–‡çš„ç›®çš„æ˜¯ç†è§£åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šå¦‚ä½•ä½¿ç”¨ä¸çº¯åº¦åº¦é‡ï¼ˆ*ä¾‹å¦‚ç†µ*ï¼‰æ¥ç¡®å®šæœ€ä½³åˆ†è£‚ï¼Œæœ€ç»ˆæ„å»ºä¸€ä¸ªåŸºäºè§„åˆ™çš„æ ‘çŠ¶ç»“æ„ä»¥è·å¾—æœ€ç»ˆé¢„æµ‹ç»“æœã€‚

ä¸ºäº†è·å¾—å…³äºç†µå’ŒåŸºå°¼ä¸çº¯åº¦ï¼ˆç”¨äºæµ‹é‡éšæœºæ€§å¹¶ç¡®å®šå†³ç­–æ ‘ä¸­åˆ†è£‚è´¨é‡çš„å¦ä¸€ç§åº¦é‡ï¼‰çš„ç›´è§‰ï¼Œå¯ä»¥å¿«é€ŸæŸ¥çœ‹è¿™ç¯‡[æ–‡ç« ](https://medium.com/@gurjinderkaur95/entropy-and-gini-index-c04b7452efbe)ã€‚

## é—®é¢˜å®šä¹‰å’Œæ•°æ®

> **é—®é¢˜ï¼š** æ ¹æ®é±¼çš„é•¿åº¦å’Œé‡é‡é¢„æµ‹å®ƒæ˜¯é‡‘æªé±¼è¿˜æ˜¯é²‘é±¼ã€‚

æŒ‘æˆ˜åœ¨äºæ ¹æ®é±¼çš„*é‡é‡*å’Œ*é•¿åº¦*é¢„æµ‹é±¼çš„*ç±»å‹*ï¼ˆç›®æ ‡å˜é‡ï¼‰ã€‚è¿™æ˜¯ä¸€ä¸ªäºŒåˆ†ç±»ä»»åŠ¡çš„ç¤ºä¾‹ï¼Œå› ä¸ºæˆ‘ä»¬çš„ç›®æ ‡å˜é‡*ç±»å‹*æœ‰ä¸¤ä¸ªå¯èƒ½çš„å€¼ï¼Œå³*tuna*å’Œ*salmon*ã€‚

ä½ å¯ä»¥ä»[è¿™é‡Œ](https://github.com/gurjinderbassi/Machine-Learning/blob/main/fish.csv)ä¸‹è½½æ•°æ®é›†ã€‚

å¼ºçƒˆå»ºè®®ä½ åœ¨é˜…è¯»æœ¬æ–‡æ—¶è¿›è¡Œç¼–ç ï¼Œä»¥è·å¾—æœ€å¤§ç†è§£ :)

## ä»£ç è·Ÿéšå‰ææ¡ä»¶

è®©æˆ‘ä»¬ç¡®ä¿ä½ æœ‰ä¸€åˆ‡å‡†å¤‡å¥½å¼€å§‹ï¼ˆæˆ‘æ•¢æ‰“èµŒä½ å·²ç»å‡†å¤‡å¥½äº†ï¼Œä½†ä»¥é˜²ä¸‡ä¸€ï¼‰ã€‚

+   [***Python***](https://www.python.org/downloads/)

+   ä»»ä½•å…è®¸ä½ ä½¿ç”¨ Python (.ipynb æ‰©å±•å) ç¬”è®°æœ¬çš„***ä»£ç ç¼–è¾‘å™¨***ï¼Œä¾‹å¦‚ [Visual Studio Code](https://code.visualstudio.com/)ã€[Jupyter Notebook](https://jupyter.org/install) å’Œ [Google Colab](https://colab.research.google.com/?utm_source=scs-index) ç­‰ç­‰ã€‚

+   ***åº“*:** [pandas](https://pandas.pydata.org/docs/getting_started/install.html) å’Œ [numpy](https://numpy.org/install/) ç”¨äºæ•°æ®å¤„ç†ï¼›[plotly](https://plotly.com/python/getting-started/) ç”¨äºå¯è§†åŒ–ã€‚ï¼ˆå¦‚æœä½ æ„¿æ„ï¼Œå¯ä»¥ä½¿ç”¨ä»»ä½•å…¶ä»–çš„æ•°æ®å¯è§†åŒ–åº“ã€‚ï¼‰

+   [***æ•°æ®***](https://github.com/gurjinderbassi/Machine-Learning/blob/main/fish.csv)ï¼Œå½“ç„¶ã€‚

è¿™å°±æ˜¯æˆ‘ä»¬éœ€è¦çš„ä¸€åˆ‡ï¼Œå¾ˆå¯èƒ½ä½ å·²ç»å‡†å¤‡å¥½äº†ã€‚ç°åœ¨è®©æˆ‘ä»¬å¼€å§‹ç¼–ç å§ï¼

## ä¸€æ­¥æ­¥è§£å†³æ–¹æ¡ˆ

**åˆ›å»ºä¸€ä¸ªæ–°çš„ .ipynb æ–‡ä»¶å¹¶é¦–å…ˆå¯¼å…¥åº“ã€‚**

```py
import pandas as pd
import numpy as np
import plotly.graph_objects as go
```

**å°†æ•°æ®è¯»å…¥ pandas æ•°æ®æ¡†ã€‚**

```py
# read the csv file
df = pd.read_csv("fish.csv") 

# how many rows and columns?
print(df.shape)

# print column names
print(df.columns)

# print class distribution
print(df["type"].value_counts())
```

`å•å…ƒè¾“å‡ºï¼š`

![](img/0e8d1c7256af0d8151f6de75bdcf5596.png)

æˆ‘ä»¬çš„æ•°æ®æ¡†ä¸­æœ‰ 1000 è¡Œå’Œ 3 åˆ—ã€‚â€˜lengthâ€™ å’Œ â€˜weightâ€™ æ˜¯ç‰¹å¾ï¼Œâ€˜typeâ€™ æ˜¯ç›®æ ‡å˜é‡ã€‚ç”±äºâ€˜typeâ€™ åˆ—æœ‰å€¼â€”â€”â€˜tunaâ€™ å’Œ â€˜salmonâ€™ï¼Œè®©æˆ‘ä»¬å¯¹å®ƒä»¬è¿›è¡Œæ ‡ç­¾ç¼–ç ã€‚

**å¯¹ç›®æ ‡åˆ—è¿›è¡Œæ ‡ç­¾ç¼–ç ï¼š**

```py
df["type"] = df["type"].apply(lambda x: 1 if x=="tuna" else 0)
```

æˆ‘ä»¬å°†ç±»åˆ«æ ‡è®°ä¸ºï¼š`{salmon: 0 å’Œ tuna: 1}`

ç°åœ¨ï¼Œè®©æˆ‘ä»¬**ç»˜åˆ¶ä¸€ä¸ªæ•£ç‚¹å›¾æ¥å¯è§†åŒ–æˆ‘ä»¬çš„ç±»åˆ«ã€‚**

```py
# create a Figure
fig = go.Figure()

# specify custom colors for the plot
color_map = {
    0: "red",
    1: "blue",
}

# apply the color map to 'type' column
colors = df["type"].map(color_map)

# add a scatter trace to the figure
fig.add_trace(go.Scatter(x=df["length"], 
            y=df["weight"],
            mode="markers",
            marker=dict(color=colors, size=8)))

# add x-label, y-label and title
fig.update_layout(
    width=800,
    height=600,  
    title_text="Scatter Plot of Data",
    xaxis=dict(title="length",
               tickvals=[i for i in range(10)]),
    yaxis=dict(title="weight",
               tickvals=[i for i in range(10)])
)
```

`å•å…ƒè¾“å‡ºï¼š`

![](img/de8ddf4c7f4f68e80ce4cfa22beea6a4.png)

å›¾ç‰‡ç”±ä½œè€…æä¾›

æˆ‘ä»¬ç°åœ¨å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°ç”¨çº¢è‰²å’Œè“è‰²æ ‡è®°çš„ä¸¤ä¸ªç±»åˆ«ã€‚è¿™ä¸€åˆ‡éƒ½æ˜¯å…³äºæ•°æ®å‡†å¤‡çš„ï¼Œè®©æˆ‘ä»¬è¿›å…¥å†³ç­–æ ‘çš„éƒ¨åˆ†ã€‚å°†ç‰¹å¾åˆ—åç§°åˆ†é…ç»™ä¸€ä¸ªåˆ—è¡¨ï¼Œæˆ‘ä»¬ç¨åä¼šä½¿ç”¨åˆ°ã€‚

```py
features = ["length", "weight"]
```

**æ‰¾åˆ°æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªåˆ’åˆ†**

> åœ¨å†³ç­–æ ‘ä¸­çš„**åˆ’åˆ†**æŒ‡çš„æ˜¯å°†æ•°æ®åˆ†ä¸ºä¸¤ä¸ªï¼ˆæˆ–æ›´å¤šï¼‰åˆ†åŒºçš„ (ç‰¹å¾, å€¼) å¯¹ã€‚
> 
> åœ¨æ•°å€¼ç‰¹å¾çš„æƒ…å†µä¸‹ï¼Œåˆ’åˆ†å°†å¯¼è‡´æ•°æ®çš„ä¸¤ä¸ªåˆ†åŒºâ€”â€”ä¸€ä¸ªæ˜¯**ç‰¹å¾ â‰¤ å€¼**ï¼Œå¦ä¸€ä¸ªæ˜¯**ç‰¹å¾ > å€¼**ã€‚
> 
> åœ¨åˆ†ç±»ç‰¹å¾çš„æƒ…å†µä¸‹ï¼Œåˆ’åˆ†å°†å¯¼è‡´æ•°æ®çš„ä¸¤ä¸ªåˆ†åŒºâ€”â€”ä¸€ä¸ªæ˜¯**ç‰¹å¾=å€¼**ï¼Œå¦ä¸€ä¸ªæ˜¯**ç‰¹å¾ä¸ç­‰äºå€¼**ã€‚

```py
# Finding the first split:

# initialize best_params which is a dictionary that will keep track
# of best feature and split value at each node.
best_params = {"feature": None, "impurity": np.inf, "split_value": None}

# for each feature in features, do the following:
### for val in all feature values 
### (starting from the min possible value of the feature until max possible value, 
### incrementing by 'step_size'), check the following:
###### if impurity at this feature val is less than previously recorded impurity, 
###### then update best_params

# Following is the code for above interpretation
for feature in features:
    curr_val = df[feature].min()
    step_size = 0.1
    while curr_val <= df[feature].max():
        curr_feature_split_impurity = compute_impurity(df, feature, curr_val)
        if curr_feature_split_impurity < best_params["impurity"]:
            best_params["impurity"] = curr_feature_split_impurity
            best_params["feature"] = feature
            best_params["split_value"] = curr_val
        curr_val += step_size
```

è¿è¡Œæ­¤å•å…ƒå°†äº§ç”Ÿé”™è¯¯ï¼Œå› ä¸ºæˆ‘ä»¬è¿˜æ²¡æœ‰å®šä¹‰å‡½æ•° `compute_impurity`ã€‚æˆ‘ä»¬éœ€è¦è¿™ä¸ªå‡½æ•°æ¥æ¯”è¾ƒåœ¨åˆ’åˆ†ä¹‹å‰å’Œä¹‹åæ•°æ®çš„æ‚è´¨ã€‚ç»“æœåœ¨åˆ’åˆ†åæ‚è´¨æœ€ä½çš„ (ç‰¹å¾, å€¼) å¯¹å°†è¢«é€‰ä¸ºå½“å‰èŠ‚ç‚¹çš„æœ€ä½³åˆ’åˆ†ï¼Œæˆ‘ä»¬å°†ç›¸åº”åœ°æ›´æ–°*best_params*ã€‚

å®šä¹‰å‡½æ•°å¦‚ä¸‹ï¼š

```py
def compute_impurity(df, feature, val, criterion):
    """
    Inputs:
    df: dataframe before splitting
    feature: colname to test for best split
    val: value of 'feature' to test for best split

    Output: float
    Returns the entropy after split
    """

    # Make the split at (feature, val)
    left = df[df[feature]<=val]["type"]
    right = df[df[feature]>val]["type"]

    # calculate impurity of both partitions

    if criterion=="entropy":
        left_impurity = compute_entropy(left)
        right_impurity = compute_entropy(right)
    else:
        left_impurity = compute_gini(left)
        right_impurity = compute_gini(right)

    # return weighted entropy
    n = len(df) # total number of data points
    left_n = len(left) # number of data points in left partition
    right_n = len(right) # number of data points in right partition

    return (left_n/n)*left_impurity + (right_n/n)*right_impurity
```

æ­¤å‡½æ•°ä½¿ç”¨å¦ä¸€ä¸ªè¾…åŠ©å‡½æ•° `compute_entropy`ï¼Œå®ƒä¸ºç»™å®šç±»åˆ«åˆ—è¡¨æä¾›ç†µã€‚è®©æˆ‘ä»¬ä¹Ÿå®šä¹‰è¿™ä¸ªå‡½æ•°ï¼š

```py
def compute_entropy(vals):
    """
    Input:
    vals: list of 0s and 1s corresponding to two classes

    Output:
    entropy: float"""

    # probability of class labeled as 1 
    # will be equal to the average of vals
    p1 = np.mean(vals)
    p0 = (1-p1)

    # it means data is homoegeneous
    # entropy is 0 in this case
    if p1==0 or p0==0: 
        return 0

    return - (p0*np.log2(p0) + p1*np.log2(p1)) # formula of entropy for two classes
```

ç°åœ¨æˆ‘ä»¬å·²ç»å®šä¹‰äº†ä¸¤ä¸ªè¾…åŠ©å‡½æ•°ï¼Œå†æ¬¡è¿è¡Œä¹‹å‰å¯¼è‡´é”™è¯¯çš„å•å…ƒæ ¼ï¼Œç°åœ¨åº”è¯¥å¯ä»¥æˆåŠŸè¿è¡Œäº†ã€‚æ‰“å° *best_params* å­—å…¸ä»¥æ£€æŸ¥å®ƒæ˜¯å¦å·²æ›´æ–°ã€‚

```py
print(best_params)
```

`å•å…ƒæ ¼è¾“å‡ºï¼š`

![](img/989ee647c08e6b66d676165d687d6ea0.png)

å®Œç¾ï¼æˆ‘ä»¬åœ¨ `length = 3` å¤„å¾—åˆ°äº†ç¬¬ä¸€ä¸ªæœ€ä½³åˆ†å‰²ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬ç°åœ¨å°†æ•°æ®åˆ†ä¸ºä¸¤ä¸ªåˆ†åŒºâ€”â€”`data['length']<=3` å’Œ `data['length']>3`ã€‚

***æ³¨æ„ï¼š*** *åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬é€‰æ‹©ç»“æœç†µæœ€å°çš„åˆ†å‰²ã€‚å†³ç­–æ ‘å¯ä»¥åœ¨æ­¤åˆ†å‰²æ ‡å‡†ä¸Šæœ‰æ‰€ä¸åŒï¼Œä¾‹å¦‚ï¼Œ* ***ID3 ä½¿ç”¨******ä¿¡æ¯å¢ç›Š*** *(å³åˆ†å‰²å‰æ•°æ®çš„ç†µä¸åˆ†å‰²ååˆ†æ”¯ç†µåŠ æƒå’Œçš„å·®å€¼)ï¼Œè€Œ* ***CART ä½¿ç”¨åŸºå°¼æŒ‡æ•°*** *ä½œä¸ºå®ƒä»¬å„è‡ªçš„åˆ†å‰²æ ‡å‡†ã€‚*

ä¸‹é¢æ˜¯æˆ‘ä»¬å½“å‰å†³ç­–æ ‘çš„æ ·å­ã€‚ç”±äºå·¦ä¾§åˆ†æ”¯çš„æ•°æ®åªåŒ…å«ä¸€ä¸ªç±»åˆ«ï¼Œæˆ‘ä»¬å°†å…¶è®¾ä¸ºå¶èŠ‚ç‚¹ï¼›å› æ­¤ï¼Œä»»ä½• `length<=3` çš„æ•°æ®ç‚¹éƒ½å°†è¢«é¢„æµ‹ä¸º *é‡‘æªé±¼*ã€‚

(***æé†’ï¼š*** æ ¹æ®æˆ‘ä»¬çš„ color_mapï¼Œæˆ‘ä»¬å°†è“è‰²åˆ†é…ç»™é‡‘æªé±¼ï¼Œå°†çº¢è‰²åˆ†é…ç»™é²‘é±¼)ï¼š

```py
color_map = {
 0: "red", # salmon
 1: "blue", # tuna
}
```

å¯¹äºå³ä¾§åˆ†æ”¯çš„æ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥é€’å½’åœ°éµå¾ªç›¸åŒçš„è¿‡ç¨‹ï¼Œæ‰¾åˆ°æœ€ä½³åˆ†å‰²ï¼Œå¹¶å¯¹åç»­åˆ†æ”¯é‡å¤æ­¤è¿‡ç¨‹ï¼Œç›´åˆ°è¾¾åˆ°æœ€å¤§æ·±åº¦æˆ–ä¸å†å¯èƒ½è¿›è¡Œè¿›ä¸€æ­¥åˆ†å‰²ã€‚

![](img/358412cd61839e3cf75d0089b273a6d1.png)

åœ¨ç¬¬ä¸€æ¬¡åˆ†å‰²åå¯è§†åŒ–å†³ç­–æ ‘ï¼ˆå›¾åƒä½œè€…æä¾›ï¼‰

è¿™ä¸ªè¿‡ç¨‹ä¼šå¯¹æ¯ä¸ªåˆ†åŒºé‡å¤ï¼Œç›´åˆ°æ»¡è¶³åœæ­¢æ¡ä»¶ï¼ˆä¾‹å¦‚ï¼Œæ ‘çš„æœ€å¤§æ·±åº¦è¾¾åˆ°ï¼Œæˆ–è€…å¶èŠ‚ç‚¹ä¸­çš„æ ·æœ¬æ•°é‡ä½äºé˜ˆå€¼ç­‰ï¼‰ã€‚è¿™å°±æ˜¯å½“æˆ‘ä»¬ä½¿ç”¨å¦‚ scikit-learn ç­‰åŒ…ä¸­çš„åˆ†ç±»å™¨æˆ–å›å½’å™¨æ—¶ï¼Œ*è¶…å‚æ•°* å…è®¸æˆ‘ä»¬å®šä¹‰çš„å†…å®¹ã€‚

**ä½¿ç”¨é€’å½’æ¥æ³›åŒ–ä»£ç **

æˆ‘ä»¬å°†æŠŠä¸Šè¿°ä»£ç å°è£…åˆ°ä¸€ä¸ªå‡½æ•° `build_tree` ä¸­ï¼Œè¯¥å‡½æ•°å¯ä»¥é€’å½’è°ƒç”¨æ¥æ„å»ºå†³ç­–æ ‘ã€‚

**è¾…åŠ©å‡½æ•°ï¼š**

`compute_entropy:` è¿”å›å…·æœ‰ä¸¤ä¸ªç±»åˆ«çš„æ•°æ®é›†çš„ç†µã€‚ä¸Šè¿°å®šä¹‰ã€‚

`compute_gini:` è¿”å›å…·æœ‰ä¸¤ä¸ªç±»åˆ«çš„æ•°æ®é›†çš„åŸºå°¼æŒ‡æ•°ã€‚æˆ‘ä»¬å¯ä»¥é€‰æ‹©ä½¿ç”¨ç†µæˆ–åŸºå°¼æŒ‡æ•°ä½œä¸ºæˆ‘ä»¬çš„ impurity è¡¡é‡æ ‡å‡†ã€‚

```py
def compute_gini(vals):
    """
    Input: vals is a list of 0s and 1s
    Output:
    gini: float
    """

    # probability of '1' will be equal to the average of vals
    p1 = np.mean(vals)
    p0 = (1-p1) # since there are just two classes and p0+p1 = 1

    if p1==0 or p0==0:
        return 0

    return 1 - p1**2 - p0**2
```

`compute_impurity:` æ˜¯ä¸Šè¿° compute_impurity å‡½æ•°çš„æ‰©å±•ï¼Œå®ƒè¿”å›æ•°æ®é›†çš„ *impurity*ã€‚å®ƒä½¿ç”¨ `compute_entropy` å’Œ `compute_gini` å‡½æ•°æ ¹æ®æŒ‡å®šçš„æ ‡å‡†è®¡ç®—ç»™å®šåˆ†å‰²ç‚¹çš„ç†µå’ŒåŸºå°¼æŒ‡æ•°ã€‚

```py
def compute_impurity(df, feature, val, criterion):
    """
    Inputs:
    df: dataframe before splitting
    feature: colname to test for best split
    val: value of 'feature' to test for best split

    Output: float
    Returns the entropy after split
    """

    # Make the split at (feature, val)
    left = df[df[feature]<=val]["type"]
    right = df[df[feature]>val]["type"]

    # calculate impurity of both partitions

    if criterion=="entropy":
        left_impurity = compute_entropy(left)
        right_impurity = compute_entropy(right)
    else:
        left_impurity = compute_gini(left)
        right_impurity = compute_gini(right)

    # return weighted entropy
    n = len(df) # total number of data points
    left_n = len(left) # number of data points in left partition
    right_n = len(right) # number of data points in right partition

    return (left_n/n)*left_impurity + (right_n/n)*right_impurity
```

`get_best_params:` è¿”å›åŒ…å«å½“å‰èŠ‚ç‚¹åˆ†å‰²æ‰€ç”¨ç‰¹å¾å’Œå€¼çš„ *best_params* å­—å…¸ã€‚

```py
def get_best_params(df, features, criterion):
    """
    A function to determine the best split at a node

    Input:
    df: dataframe before split
    features: list of features
    criterion: impurity measure to use (gini or entropy)

    Output: 
    best_params: dict
    """

    # Initialize best_params
    best_params = {"feature": None, "val": None, "impurity": np.inf}

    # iterate for all features
    for feature in features:
        curr_val = df[feature].min()
        step_size = 0.1
        # iterate for all values for a feature (according to step_size)
        while curr_val<=df[feature].max():
            # calculate impurity (gini or entropy) for the current value of feature
            impurity = compute_impurity(df, feature, curr_val, criterion)

            # update best_params if impurity is less than previous impurity
            if impurity <= best_params["impurity"]:
                best_params["feature"] = feature
                best_params["val"] = curr_val
                best_params["impurity"] = impurity
            curr_val += step_size

    best_params["val"] = np.round(best_params["val"], 2)
    best_params["impurity"] = np.round(best_params["impurity"], 2)

    return best_params
```

**ä¸»å‡½æ•°**

`build_tree:` è¿™æ˜¯ä¸»è¦çš„é©±åŠ¨å‡½æ•°ï¼Œå®ƒåˆ©ç”¨è¾…åŠ©å‡½æ•°é€’å½’åœ°ä¸ºç»™å®šçš„æ•°æ®æ„å»ºå†³ç­–æ ‘ã€‚æˆ‘è¿˜æ·»åŠ äº†ä¸€äº›é¢å¤–çš„è¯­å¥ï¼Œè¯•å›¾åœ¨åˆ›å»ºè¿‡ç¨‹ä¸­ä»¥å¯è§£é‡Šçš„æ ¼å¼æ‰“å°å†³ç­–æ ‘ã€‚

```py
def build_tree(data, features, curr_depth=0, max_depth=3, criterion="entropy"):
    """A function to buil the decision tree recursively.

    Input:
    data: dataframe with columns length, weight, type
    features: ['length', 'weight']
    curr_depth: Keep track of depth at current node
    max_depth: Decision tree will stop growing if max_depth reached
    criterion: "gini" or "entropy" 

    """

    # Base case: max depth reached 
    if curr_depth >= max_depth:
        classes, counts = np.unique(data['type'].values, return_counts=True)
        predicted_class = classes[np.argmax(counts)]
        print(("--" * curr_depth) + f"Predict: {predicted_class}")
        return

    # Get the best feature and value to split the data
    best_params = get_best_params(data, features, criterion)

    # Base case: pure node, single class case
    if best_params["impurity"] == 0:
        predicted_class = data['type'].iloc[0]
        print(("--" * curr_depth) + f"Predict: {predicted_class}")
        return

    # If there's no feature that can improve the purity (not possible to split)
    if best_params["feature"] is None:
        predicted_class = data['type'].iloc[0]
        print(("--" * curr_depth) + f"Predict: {predicted_class}")
        return

    # Print the current question (decision rule)
    best_feature = best_params["feature"]
    best_split_val = best_params["val"]
    question = f"Is {best_feature} <= {best_split_val}?"
    print(("--" * (curr_depth*2)) + ">" + f"{question}")

    # Split the dataset
    left_df = data[data[best_feature] <= best_split_val]
    right_df = data[data[best_feature] > best_split_val]

    # Recursive calls for left and right subtrees
    if not left_df.empty:
        print(("--" * curr_depth) + f"Yes ->")
        build_tree(left_df, features, curr_depth + 1, max_depth, criterion)

    if not right_df.empty:
        print(("--" * curr_depth) + f"No ->")
        build_tree(right_df, features, curr_depth + 1, max_depth, criterion)
```

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ä¼ é€’æˆ‘ä»¬çš„é±¼æ•°æ®é›†å¹¶æµ‹è¯•ä»£ç çš„è¾“å‡ºï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚

```py
build_tree(df, ["length", "weight"], max_depth=4, criterion="entropy")
```

`å•å…ƒæ ¼è¾“å‡ºï¼š`

![](img/1a6cdb5c5b8311c3319f4849f90e4075.png)

ä»¥ä¸‹æ˜¯æˆ‘ä»¬æœ€ç»ˆå†³ç­–æ ‘çš„æ ·å­ï¼š

![](img/7f2c327a26b137d97f75628fa60f6327.png)

å›¾ç‰‡æ¥æºï¼šä½œè€…

è¿™å¯¹åº”äºä»¥ä¸‹å†³ç­–è¾¹ç•Œï¼š

![](img/868c6d0d1b4dbae53e9e139286993b98.png)

å›¾ç‰‡æ¥æºï¼šä½œè€…

**æ³¨æ„ï¼š** å¦‚æœå¶èŠ‚ç‚¹ä¸æ˜¯çº¯å‡€çš„ï¼Œå³å¶èŠ‚ç‚¹åˆ†åŒºä¸­æœ‰å¤šä¸ªç±»åˆ«ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ*åªéœ€é€‰æ‹©å¤šæ•°ç±»åˆ«ã€‚*

## ä»£ç é“¾æ¥

ä½ å¯ä»¥ä»[è¿™é‡Œ](https://github.com/gurjinderbassi/Machine-Learning/blob/main/Decision%20Tree%20-%20Fish%20dataset.ipynb)è·å–æœ€ç»ˆçš„ä»£ç ç¬”è®°æœ¬ã€‚

# ä¸»è¦æ”¶è·

å¦‚æœä½ å·²ç»è¯»åˆ°è¿™é‡Œï¼Œä½ ä¼šå¯¹è®¸å¤šä¸å†³ç­–æ ‘ç›¸å…³çš„é‡è¦é—®é¢˜æœ‰æ›´æ¸…æ™°çš„è®¤è¯†ï¼Œæ¯”å¦‚*å¯è§£é‡Šæ€§*ä½œä¸ºä¼˜ç‚¹ï¼Œä»¥åŠ*è¿‡æ‹Ÿåˆ*ä½œä¸ºä¸»è¦ç¼ºç‚¹â€”â€”ä½ å¯èƒ½åœ¨ä¹‹å‰æ¥è§¦å†³ç­–æ ‘æ—¶å·²ç»é‡åˆ°è¿‡ã€‚æˆ‘ä»¬ä¸èƒ½åœ¨è¿™é‡Œå¿½ç•¥è¿™äº›è¯é¢˜ï¼Œæ‰€ä»¥ç®€å•æåŠä¸€ä¸‹ã€‚

è®©æˆ‘ä»¬å…ˆæ¥çœ‹çœ‹ä¼˜ç‚¹ã€‚è¿˜æœ‰æ›´å¤šï¼Œä½†æˆ‘ä»¬åªåˆ—å‡ºæœ€é‡è¦çš„å‡ ä¸ªã€‚

## å†³ç­–æ ‘çš„ï¼ˆä¸»è¦ï¼‰ä¼˜ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ

+   ***å¯è§£é‡Šæ€§ï¼š*** å†³ç­–æ ‘çš„é¢„æµ‹ç›¸è¾ƒäºå…¶ä»–æœºå™¨å­¦ä¹ æ¨¡å‹æ›´æ˜“äºè§£é‡Šï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥ç›´è§‚åœ°æŸ¥çœ‹è¾¾åˆ°æœ€ç»ˆé¢„æµ‹æ‰€éµå¾ªçš„è·¯å¾„ã€‚

> å†³ç­–æ ‘ç›´è§‚ä¸”æ˜“äºè§£é‡Šï¼Œå³ä½¿å¯¹éæŠ€æœ¯äººå‘˜ä¹Ÿæ˜¯å¦‚æ­¤ã€‚

ä¾‹å¦‚ï¼Œå‡è®¾ä¸€å®¶é“¶è¡Œä½¿ç”¨å†³ç­–æ ‘é¢„æµ‹æ˜¯å¦åº”æ ¹æ®å®¢æˆ·çš„å±æ€§ï¼ˆå¦‚æ”¶å…¥ã€é“¶è¡Œä½™é¢ã€å¹´é¾„ã€èŒä¸šç­‰ï¼‰å‘å®¢æˆ·æˆäºˆè´·æ¬¾ã€‚å¦‚æœåˆ†ç±»ç³»ç»Ÿå»ºè®®é“¶è¡Œä¸åº”å‘å®¢æˆ·æä¾›è´·æ¬¾ï¼Œé‚£ä¹ˆé“¶è¡Œéœ€è¦åˆ¶å®šé€‚å½“çš„å›åº”ï¼Œè¯´æ˜æ‹’ç»çš„ç†ç”±ã€‚å¦åˆ™ï¼Œè¿™å¯èƒ½ä¼šæŸå®³ä»–ä»¬çš„ä¸šåŠ¡å’Œå£°èª‰ã€‚

+   ***æ— éœ€é‡åº¦é¢„å¤„ç†ï¼š*** ä¸å…¶ä»–ä¸€äº›æœºå™¨å­¦ä¹ æ¨¡å‹ä¸åŒï¼Œå†³ç­–æ ‘ä¸è¦æ±‚æ•°æ®è¿›è¡Œå½’ä¸€åŒ–ï¼ˆæˆ–æ ‡å‡†åŒ–ï¼‰ã€‚

> ä½ åªéœ€è¿›è¡Œæœ€å°‘çš„æ•°æ®é¢„å¤„ç†ï¼Œå†³ç­–æ ‘ä¹Ÿä¸ä¼šå¤ªåœ¨æ„ã€‚

æ­¤å¤–ï¼Œå®ƒå¯ä»¥è‡ªç„¶åœ°å¤„ç†åˆ†ç±»ç‰¹å¾ï¼Œæˆ‘ä»¬ä¸éœ€è¦æ‹…å¿ƒç‹¬çƒ­ç¼–ç ï¼ˆæˆ–å…¶ä»–è§£å†³æ–¹æ¡ˆï¼‰ï¼Œå› ä¸ºä¸åŒçš„ç±»åˆ«åœ¨åˆ†è£‚æ—¶ä¼šè¢«è§†ä¸ºä¸åŒçš„åˆ†æ”¯ã€‚

## ä¸»è¦ç¼ºç‚¹â€”â€”> è¿‡æ‹Ÿåˆ

> è¿‡æ‹Ÿåˆæ˜¯æŒ‡æˆ‘ä»¬çš„æ¨¡å‹å¥½å¾—ä¸çœŸå®ï¼Œå³æ¨¡å‹å¯¹è®­ç»ƒæ•°æ®çš„é€‚åº”è¿‡äºç´§å¯†ï¼Œä»¥è‡³äºä¸§å¤±äº†æ³›åŒ–èƒ½åŠ›ï¼Œæ— æ³•åœ¨é¢å¯¹æµ‹è¯•æ•°æ®æ—¶è¡¨ç°å‡ºç±»ä¼¼çš„å‡†ç¡®åº¦ã€‚

**å†³ç­–æ ‘å¦‚æœæ§åˆ¶ä¸å½“ï¼Œéå¸¸å®¹æ˜“è¿‡æ‹Ÿåˆã€‚** æ³¨æ„åœ¨æˆ‘ä»¬ä¸Šè¿°çš„ä¾‹å­ä¸­ï¼Œå¦‚æœæˆ‘ä»¬ä¸æ–­åœ°åˆ†è£‚è®­ç»ƒæ•°æ®è€Œä¸è®¾å®šä»»ä½•é™åˆ¶ï¼Œé‚£ä¹ˆå†³ç­–æ ‘å°†ä¸æ–­åˆ›å»ºæ›´å¤šçš„å†³ç­–è¾¹ç•Œï¼Œå­¦ä¹ è®­ç»ƒæ•°æ®ä¸­çš„å™ªå£°ã€‚

ä¸ºäº†ä¿æŒæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé‡‡å–æªæ–½é¿å…è¿‡æ‹Ÿåˆæ˜¯éå¸¸é‡è¦çš„ã€‚åœ¨å†³ç­–æ ‘çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡å–ä»¥ä¸‹**é˜²æ­¢è¿‡æ‹Ÿåˆçš„æ­¥éª¤ï¼š**

1.  *é¢„å‰ªæï¼š* å‰ªææŒ‡çš„æ˜¯é˜²æ­¢å†³ç­–æ ‘è¾¾åˆ°å…¶æœ€å¤§å®¹é‡ã€‚å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¸»åŠ¨è¿›è¡Œï¼š

+   è®¾ç½®*max_depth:* ä¸å…è®¸æ ‘çš„æ·±åº¦è¶…è¿‡é¢„å®šä¹‰çš„æ·±åº¦ã€‚

+   è®¾ç½®*min_samples_split:* å¦‚æœå†³ç­–èŠ‚ç‚¹å¤„çš„æ ·æœ¬æ•°é‡ä½äºæ­¤å€¼ï¼Œåˆ™ä¸å…è®¸åˆ†è£‚å‘ç”Ÿã€‚

+   è®¾ç½®*min_samples_leaf:* å¦‚æœä»»ä½•ç»“æœå¶èŠ‚ç‚¹å¤„çš„æ ·æœ¬æ•°é‡ä½äºæ­¤å€¼ï¼Œåˆ™ä¸å…è®¸åˆ†è£‚å‘ç”Ÿã€‚

è¿™äº›ï¼ˆä»¥åŠè®¸å¤šå…¶ä»–çš„ï¼‰æ˜¯æˆ‘ä»¬åœ¨é€šè¿‡è¯¸å¦‚ scikit-learn è¿™æ ·çš„åŒ…å®ç°å†³ç­–æ ‘æ—¶å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´çš„***è¶…å‚æ•°***ã€‚ä½ å¯ä»¥åœ¨è¿™ä¸ª[æ–‡æ¡£](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)ä¸­æ‰¾åˆ°æ‰€æœ‰è¶…å‚æ•°åŠå…¶å®šä¹‰ã€‚

*2\. åå‰ªæï¼š* æŒ‡çš„æ˜¯è®©å†³ç­–æ ‘åœ¨å…¶æœ€å¤§å®¹é‡ä¸‹ç”Ÿé•¿ï¼Œç„¶åä¸¢å¼ƒä¸€äº›çœ‹èµ·æ¥ä¸å¿…è¦æˆ–å¯¼è‡´é«˜æ–¹å·®çš„éƒ¨åˆ†/åˆ†æ”¯ã€‚

3\. å¦ä¸€ç§å¯èƒ½çš„è§£å†³æ–¹æ¡ˆæ˜¯ï¼š*ä¸è¦ä½¿ç”¨å†³ç­–æ ‘ï¼* è€Œæ˜¯é€‰æ‹©å®ƒä»¬çš„é«˜çº§ç‰ˆæœ¬â€”â€”æ¯”å¦‚*éšæœºæ£®æ—æˆ–æ¢¯åº¦æå‡æ ‘ :)* è¿™ä»ç„¶éœ€è¦ä½ å¯¹å‰è€…æœ‰åŸºæœ¬äº†è§£ï¼Œå› æ­¤é˜…è¯»è¿™ç¯‡æ–‡ç« ç»å¯¹ä¸ä¼šæµªè´¹æ—¶é—´ï¼

## å¥–åŠ±ç§¯åˆ†

è¿˜æœ‰ä¸€äº›å†³ç­–æ ‘çš„å…¶ä»–ç‰¹æ€§å€¼å¾—æ³¨æ„ï¼š

+   **éå‚æ•°**ï¼šå†³ç­–æ ‘æ˜¯éå‚æ•°æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œè¿™æ„å‘³ç€å®ƒä»¬å¯¹è®­ç»ƒæ•°æ®çš„åˆ†å¸ƒã€ç‰¹å¾çš„ç‹¬ç«‹æ€§ç­‰ä¸åšå‡è®¾ã€‚

+   **è´ªå¿ƒç®—æ³•**ï¼šå†³ç­–æ ‘éµå¾ªè´ªå¿ƒç®—æ³•ï¼Œè¿™æ„å‘³ç€å®ƒä»¬é€‰æ‹©åœ¨ç»™å®šèŠ‚ç‚¹å¤„è®¤ä¸ºæœ€å¥½çš„åˆ†è£‚ï¼ˆ*å³*ï¼Œå±€éƒ¨æœ€ä¼˜è§£ï¼‰ï¼Œä¸”æ— æ³•å›æº¯ï¼Œä»è€Œå¯èƒ½å¯¼è‡´æ¬¡ä¼˜è§£ã€‚

# ç»“è®º

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†å¦‚ä½•åœ¨æ²¡æœ‰ä½¿ç”¨ä»»ä½•åŒ…çš„æƒ…å†µä¸‹ä¸ºäºŒåˆ†ç±»ä»»åŠ¡æ„å»ºå†³ç­–æ ‘ï¼Œä»¥åœ¨æ¦‚å¿µå±‚é¢ä¸Šæ‰“ä¸‹åšå®çš„åŸºç¡€ã€‚æˆ‘ä»¬é€šè¿‡é€æ­¥è¿‡ç¨‹ç†è§£äº†å¦‚ä½•ä½¿ç”¨æ•°æ®ä¸çº¯åº¦åº¦é‡ï¼ˆå¦‚ç†µï¼‰åœ¨æ¯ä¸ªèŠ‚ç‚¹ç”Ÿæˆå†³ç­–è§„åˆ™ï¼Œç„¶ååœ¨ Python ä¸­å®ç°äº†é€’å½’ç®—æ³•ä»¥è¾“å‡ºæœ€ç»ˆçš„å†³ç­–æ ‘ã€‚è¿™ç¯‡æ–‡ç« çš„ç›®çš„æ˜¯é€šè¿‡æ·±å…¥äº†è§£å†³ç­–æ ‘çš„åŸºæœ¬åŸç†ã€‚

å®é™…ä¸Šï¼Œå½“å¤„ç†ç°å®ä¸–ç•Œçš„æ•°æ®å’Œé¢ä¸´æŒ‘æˆ˜æ—¶ï¼Œæˆ‘ä»¬é€šå¸¸ä¸éœ€è¦ä»é›¶å¼€å§‹åšè¿™äº›å·¥ä½œï¼Œå› ä¸ºæœ‰è®¸å¤šç°æˆçš„åŒ…å¯ä»¥ä½¿äº‹æƒ…å˜å¾—æ›´æ–¹ä¾¿å’Œç¨³å¥ã€‚ä½†æ‹¥æœ‰æ‰å®çš„åŸºç¡€ä¼šå¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°åˆ©ç”¨è¿™äº›åŒ…ï¼ŒåŒæ—¶åœ¨ä½¿ç”¨æ—¶ä¹Ÿä¼šæ›´æœ‰ä¿¡å¿ƒã€‚

å¸Œæœ›ä¸‹æ¬¡å½“æˆ‘ä»¬æ„å»ºä¸‹ä¸€ä¸ªå†³ç­–æ ‘æˆ–éšæœºæ£®æ—ï¼ˆè¿™æ˜¯ä¸€ç»„å†³ç­–æ ‘çš„é›†æˆï¼‰æ—¶ï¼Œæˆ‘ä»¬èƒ½åœ¨é…ç½®æ¨¡å‹æ—¶æ›´åŠ å‘¨åˆ°ï¼ˆå¹¶ä¸”æ›´å®¹æ˜“ç†è§£è¶…å‚æ•°çš„çœŸæ­£å«ä¹‰ ğŸ˜ºï¼‰ã€‚

å¸Œæœ›è¿™äº›å†…å®¹å¯¹ä½ æœ‰æ‰€å¸®åŠ©ã€‚æ¬¢è¿æä¾›ä»»ä½•åé¦ˆæˆ–å»ºè®®ã€‚

æˆ‘æƒ³æ„Ÿè°¢ [Ritvik Kharkar](https://medium.com/u/ddca178f703?source=post_page-----0d44c9433b06--------------------------------) çš„ç²¾å½© YouTube è§†é¢‘ï¼Œå¸®åŠ©æˆ‘æ›´å¥½åœ°ç†è§£äº†å†³ç­–æ ‘çš„æ¦‚å¿µã€‚æˆ‘ä»ä»–çš„å½•åƒä¸­è·å¾—äº†çµæ„Ÿï¼Œå†™äº†è¿™ç¯‡æ–‡ç« ï¼Œä½¿ç”¨äº†ä»–ç”¨çš„ç›¸åŒä¾‹å­ï¼Œå¹¶é€šè¿‡æ·»åŠ é€’å½’å®ç°å’Œæ‰“å°å†³ç­–æ ‘çš„é€»è¾‘å°†è§£å†³æ–¹æ¡ˆæ¨è¿›äº†ä¸€æ­¥ã€‚ä»–çš„è§†é¢‘é“¾æ¥åœ¨ä¸‹é¢çš„å‚è€ƒæ–‡çŒ®ä¸­ã€‚

# **ç›¸å…³é˜…è¯»**

+   **æƒ³è¦ç†è§£ impurity measures èƒŒåçš„ç›´è§‰ï¼Ÿ** è¯·æŸ¥çœ‹è¿™ç¯‡å…³äºç†µå’ŒåŸºå°¼æŒ‡æ•°çš„æ–‡ç« ï¼š

[](https://medium.com/@gurjinderkaur95/entropy-and-gini-index-c04b7452efbe?source=post_page-----0d44c9433b06--------------------------------) [## ç†µå’ŒåŸºå°¼æŒ‡æ•°

### ç†è§£è¿™äº›æŒ‡æ ‡å¦‚ä½•å¸®åŠ©æˆ‘ä»¬é‡åŒ–æ•°æ®é›†ä¸­çš„ä¸ç¡®å®šæ€§

medium.com](https://medium.com/@gurjinderkaur95/entropy-and-gini-index-c04b7452efbe?source=post_page-----0d44c9433b06--------------------------------)

+   **å¦‚ä½•è¯„ä¼°å†³ç­–æ ‘åˆ†ç±»å™¨ï¼Ÿ** è¯·æŸ¥çœ‹ä¸‹é¢çš„æ–‡ç« ï¼Œäº†è§£ä¸åŒçš„åˆ†ç±»æ¨¡å‹è¯„ä»·æŒ‡æ ‡ï¼š

[](https://medium.com/@gurjinderkaur95/evaluation-metrics-for-classification-beyond-accuracy-1e20d8c76ba0?source=post_page-----0d44c9433b06--------------------------------) [## åˆ†ç±»çš„è¯„ä»·æŒ‡æ ‡ - è¶…è¶Šå‡†ç¡®ç‡

### å±•å¼€æ··æ·†çŸ©é˜µã€ç²¾å‡†ç‡ã€å¬å›ç‡ã€F1 åˆ†æ•°å’Œ ROC æ›²çº¿

medium.com](https://medium.com/@gurjinderkaur95/evaluation-metrics-for-classification-beyond-accuracy-1e20d8c76ba0?source=post_page-----0d44c9433b06--------------------------------)

# å‚è€ƒæ–‡çŒ®

[1] AurÃ©lien GÃ©ron, (2019). *Hands-on machine learning with Scikit-Learn, Keras and TensorFlow: concepts, tools, and techniques to build intelligent systems* (ç¬¬ 2 ç‰ˆ). Oâ€™Reilly.

[2] ritvikmath çš„ YouTube [è§†é¢‘](https://youtu.be/dCez6oGZilY?si=slDWXQG5ZJgSv36W)

[3] [StatQuest](https://www.youtube.com/watch?v=_L39rN6gz7Y&ab_channel=StatQuestwithJoshStarmer)
