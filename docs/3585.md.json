["```py\nimport sys\nimport os\n!{sys.executable} -m pip install langchain==0.0.335 --no-warn-script-location > /dev/null\n!{sys.executable} -m pip install pygpt4all==1.1.0 --no-warn-script-location > /dev/null\n!{sys.executable} -m pip install gpt4all==1.0.12 --no-warn-script-location > /dev/null\n!{sys.executable} -m pip install transformers==4.35.1 --no-warn-script-location > /dev/null\n!{sys.executable} -m pip install datasets==2.14.6 --no-warn-script-location > /dev/null\n!{sys.executable} -m pip install tiktoken==0.4.0 --no-warn-script-location > /dev/null\n!{sys.executable} -m pip install chromadb==0.4.15 --no-warn-script-location > /dev/null\n!{sys.executable} -m pip install sentence_transformers==2.2.2 --no-warn-script-location > /dev/null\n```", "```py\ndef download_dataset(self, dataset):\n        \"\"\"\n        Downloads the specified dataset and saves it to the data path.\n\n        Parameters\n        ----------\n        dataset : str\n            The name of the dataset to be downloaded.\n        \"\"\"\n        self.data_path = dataset + '_dialogues.txt'\n\n        if not os.path.isfile(self.data_path):\n\n            datasets = {\"robot maintenance\": \"FunDialogues/customer-service-robot-support\", \n                        \"basketball coach\": \"FunDialogues/sports-basketball-coach\", \n                        \"physics professor\": \"FunDialogues/academia-physics-office-hours\",\n                        \"grocery cashier\" : \"FunDialogues/customer-service-grocery-cashier\"}\n\n            # Download the dialogue from hugging face\n            dataset = load_dataset(f\"{datasets[dataset]}\")\n            # Convert the dataset to a pandas dataframe\n            dialogues = dataset['train']\n            df = pd.DataFrame(dialogues, columns=['id', 'description', 'dialogue'])\n            # Print the first 5 rows of the dataframe\n            df.head()\n            # only keep the dialogue column\n            dialog_df = df['dialogue']\n\n            # save the data to txt file\n            dialog_df.to_csv(self.data_path, sep=' ', index=False)\n        else:\n            print('data already exists in path.')\n```", "```py\ndef load_model(self, n_threads, max_tokens, repeat_penalty, n_batch, top_k, temp):\n        \"\"\"\n        Loads the model with specified parameters for parallel processing.\n\n        Parameters\n        ----------\n        n_threads : int\n            The number of threads for parallel processing.\n        max_tokens : int\n            The maximum number of tokens for model prediction.\n        repeat_penalty : float\n            The penalty for repeated tokens in generation.\n        n_batch : int\n            The number of batches for processing.\n        top_k : int\n            The number of top k tokens to be considered in sampling.\n        \"\"\"\n        # Callbacks support token-wise streaming\n        callbacks = [StreamingStdOutCallbackHandler()]\n        # Verbose is required to pass to the callback manager\n\n        self.llm = GPT4All(model=self.model_path, callbacks=callbacks, verbose=False,\n                           n_threads=n_threads, n_predict=max_tokens, repeat_penalty=repeat_penalty, \n                           n_batch=n_batch, top_k=top_k, temp=temp)\n```", "```py\ndef build_vectordb(self, chunk_size, overlap):\n        \"\"\"\n        Builds a vector database from the dataset for retrieval purposes.\n\n        Parameters\n        ----------\n        chunk_size : int\n            The size of text chunks for vectorization.\n        overlap : int\n            The overlap size between chunks.\n        \"\"\"\n        loader = TextLoader(self.data_path)\n        # Text Splitter\n        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n        # Embed the document and store into chroma DB\n        self.index = VectorstoreIndexCreator(embedding= HuggingFaceEmbeddings(), text_splitter=text_splitter).from_loaders([loader])\n```", "```py\ndef retrieval_mechanism(self, user_input, top_k=1, context_verbosity = False, rag_off= False):\n        \"\"\"\n        Retrieves relevant document snippets based on the user's query.\n\n        Parameters\n        ----------\n        user_input : str\n            The user's input or query.\n        top_k : int, optional\n            The number of top results to return, by default 1.\n        context_verbosity : bool, optional\n            If True, additional context information is printed, by default False.\n        rag_off : bool, optional\n            If True, disables the retrieval-augmented generation, by default False.\n        \"\"\"\n\n        self.user_input = user_input\n        self.context_verbosity = context_verbosity\n\n        # perform a similarity search and retrieve the context from our documents\n        results = self.index.vectorstore.similarity_search(self.user_input, k=top_k)\n        # join all context information into one string \n        context = \"\\n\".join([document.page_content for document in results])\n        if self.context_verbosity:\n            print(f\"Retrieving information related to your question...\")\n            print(f\"Found this content which is most similar to your question: {context}\")\n\n        if rag_off:\n            template = \"\"\"Question: {question}\n            Answer: This is the response: \"\"\"\n            self.prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n        else:     \n            template = \"\"\" Don't just repeat the following context, use it in combination with your knowledge to improve your answer to the question:{context}\n\n            Question: {question}\n            \"\"\"\n            self.prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"]).partial(context=context)\n```", "```py\n def inference(self):\n        \"\"\"\n        Performs inference to generate a response based on the user's query.\n\n        Returns\n        -------\n        str\n            The generated response.\n        \"\"\"\n\n        if self.context_verbosity:\n            print(f\"Your Query: {self.prompt}\")\n\n        llm_chain = LLMChain(prompt=self.prompt, llm=self.llm)\n        print(\"Processing the information with gpt4all...\\n\")\n        response = llm_chain.run(self.user_input)\n\n        return  response \n```"]