- en: The Counter-Intuitive Nature of Probabilistic Relationships
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-counter-intuitive-nature-of-probabilistic-relationships-2589bdd2a763?source=collection_archive---------13-----------------------#2023-07-21](https://towardsdatascience.com/the-counter-intuitive-nature-of-probabilistic-relationships-2589bdd2a763?source=collection_archive---------13-----------------------#2023-07-21)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If *y* can be estimated as a linear function of x does not imply that x can
    also be estimated as a linear function of y
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://a-modirshanechi.medium.com/?source=post_page-----2589bdd2a763--------------------------------)[![Alireza
    Modirshanechi](../Images/400b5ff0fb743e32b7b057863352bb5a.png)](https://a-modirshanechi.medium.com/?source=post_page-----2589bdd2a763--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2589bdd2a763--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2589bdd2a763--------------------------------)
    [Alireza Modirshanechi](https://a-modirshanechi.medium.com/?source=post_page-----2589bdd2a763--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6a535debb95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-counter-intuitive-nature-of-probabilistic-relationships-2589bdd2a763&user=Alireza+Modirshanechi&userId=6a535debb95&source=post_page-6a535debb95----2589bdd2a763---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2589bdd2a763--------------------------------)
    ·10 min read·Jul 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2589bdd2a763&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-counter-intuitive-nature-of-probabilistic-relationships-2589bdd2a763&user=Alireza+Modirshanechi&userId=6a535debb95&source=-----2589bdd2a763---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2589bdd2a763&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-counter-intuitive-nature-of-probabilistic-relationships-2589bdd2a763&source=-----2589bdd2a763---------------------bookmark_footer-----------)![](../Images/037f83a963c741696c28433897b4b3ae.png)'
  prefs: []
  type: TYPE_NORMAL
- en: An example of probabilistic relationships (same visualization style as in Figure
    1A-B) — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Consider two real-valued variables *x* and *y*, for example, the height of a
    father and the height of his son. The central problem of [regression analyses](https://en.wikipedia.org/wiki/Regression_analysis)
    in statistics is to guess *y* by knowing *x*, e.g., to guess the height of the
    son based on the height of his father¹.
  prefs: []
  type: TYPE_NORMAL
- en: The idea in [*linear* regression](https://en.wikipedia.org/wiki/Linear_regression)
    is to use a linear function of *x* as a guess for *y.* Formally, this means to
    consider *ŷ(x) = α₁x + α₀* as our guess and find *α₀* and *α₁* by minimizing the
    [mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error) between
    *y* and *ŷ*. Now, let’s assume that we use a huge dataset and find the best possible
    values for *α₀* and *α₁*, so we know how to find the best estimate of *y* based
    on *x.* How can we use these best values for *α₀* and *α₁* to find a guess *x̂(y)*
    about *x* based on *y*? For example, if we always knew the best guess about the
    son’s height based on his father’s, then what would be our guess about the father’s
    height based on his son’s?
  prefs: []
  type: TYPE_NORMAL
- en: Such questions are special cases of “How can we use *ŷ(x)* to find *x̂(y)*?”
    Even though it may sound trivial, this question appears to be really difficult
    to address. In this article, I study the link between *ŷ(x)* and *x̂(y)* in both
    deterministic and probabilistic settings and show that our intuition for how *ŷ(x)*
    and *x̂(y)* relate to each other in deterministic settings cannot be generalized
    to probabilistic settings.
  prefs: []
  type: TYPE_NORMAL
- en: The formal statement of the problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deterministic settings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By deterministic settings, I mean situations where (i) there is no randomness
    and (ii) each value of *x* corresponds always to the same value of *y*. Formally,
    in these settings, I write *y = f(x)* for some function *f: R → R*. In such cases
    where *x* determines *y* with complete certainty (i.e., no randomness or noise),
    the best choice of *ŷ(x)* is *f(x)* itself. For example, if the height of a son
    is always 1.05 times his father’s height (let’s ignore the impossibility of the
    example for now!), then our best guess about the son’s height is to multiply the
    father’s height by 1.05.'
  prefs: []
  type: TYPE_NORMAL
- en: If *f* is an invertible function, then the best choice of *x̂(y)* is equal to
    the inverse of *f*. In the example above, this means that the best guess about
    the height of a father is always the height of his son divided by 1.05*.* Hence,
    the link between *ŷ(x)* and *x̂(y)* in deterministic cases is straightforward
    and can be reduced to finding the function *f* and its inverse.
  prefs: []
  type: TYPE_NORMAL
- en: Probabilistic settings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In probabilistic settings, *x* and *y* are samples of random variables *X* and
    *Y.* In such cases where a single value of *x* can correspond to several values
    of *y,* the best choice for *ŷ(x)* (in order to minimize the mean squared error)is
    the conditional expectation *E[Y|X=x]* — see footnote². In application-friendly
    words, this means that if you train a very expressive [neural network](https://en.wikipedia.org/wiki/Artificial_neural_network)
    to predict *y* given *x* (with a sufficiently big dataset), then your network
    would converge to *E[Y|X=x]*.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the best choice for *x̂(y)* is *E[X|Y=y] —* if you train your very
    expressive network to predict *x* given *y*, then it converges, in principle,
    to *E[X|Y=y].* Hence, the question of how *ŷ(x)* relates to *x̂(y)* in probabilistic
    settings can be rephrased as how the conditional expectations *E[Y|X=x]* and *E[X|Y=y]*
    relate to each other.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this article
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To simplify the problem, I focus on **linear** relationships, i.e., cases where
    *ŷ(x)* is linear in *x.* A linear deterministic relationship has a linear inverse,
    meaning that *y = αx* (for some *α≠0*) implies that *x = βy* with *β = 1/α* —
    see footnote³. The probabilistic linear relationship analogous to the deterministic
    relationship *y = αx* is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bddbef966917254d036924f531619976.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Equation 1**'
  prefs: []
  type: TYPE_NORMAL
- en: where *Z* is an additional random variable, often called ‘noise’ or ‘error term’,
    whose conditional average is assumed to be zero, i.e., *E[Z|X=x] = 0* for all
    *x*; note that we do not always assume that *Z* is independent of *X*. Using **Equation
    1**, the conditional expectation of *Y* given *X=x* is (see footnote⁴)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2eca59a4449f784e1f91b0c6ad2b1230.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Equation 2**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Equation 2** states that the conditional expectation *ŷ(x)* is linear in
    *x*, so it can be seen as the probabilistic twin of the linear deterministic relationship
    *y = αx.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the rest of this article, I would ask two questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Does **Equation 2** imply that *x̂(y) := E[X|Y=y] = βy* for some *β≠0*? In other
    words, does the linear relationship in **Equation 2** have a linear inverse?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If it is indeed the case that *x̂(y) = βy*, then can we write *β = 1/α* as in
    the deterministic case?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I use two counter examples and show that, as counter-intuitive as it may sound,
    the answer to both questions is negative!
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 1: When β is not the inverse of α'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As the first example, let me consider the most typical setup of linear regression
    problems, summarized in the following three assumptions (in addition to **Equation
    1**; see **Figure 1A** for visualization):'
  prefs: []
  type: TYPE_NORMAL
- en: Error term *Z* is independent of *X*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*X* has a Gaussian distribution with mean zero and variance 1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Z* has a Gaussian distribution with mean zero and variance *σ²*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/404b45a95710e593b002a446aafad1b0.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 1\. Visualizing example 1 and example 2.** Panels **A** and **B**
    visualize the conditional distribution of *Y* given X for example 1 (A; α = 0.5
    with fixed σ² = 3/4) and example 2 (B; α = 0.5 with σ² dependent on x). Given
    a value x for the random variable X, the random variable Y follows a Gaussian
    distribution in both examples: Black lines show the conditional expectation E[Y|X=x],
    and the shaded areas show the standard deviation of the Gaussian distributions.
    Points show 500 samples of the joint distribution of (X, Y). Panel **C** shows
    the marginal distribution of Y (with X having a standard normal distribution)
    for example 1 (blue) and example 2 (red): The marginal distribution of Y in example
    1 is Gaussian with mean zero and variance *α² + σ², but we can only numerically
    evaluate the marginal distribution of Y in example 2.*'
  prefs: []
  type: TYPE_NORMAL
- en: It is straightforward to show, after a few lines of algebra, that these assumptions
    imply that *Y* has a Gaussian distribution with mean zero and variance *α² + σ²*.
    Moreover, the assumptions imply that *X* and *Y* are jointly Gaussian with mean
    zero and covariance matrix equal to
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2d43e43c2bc38844f2111e7fd575eb82.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since we have the full joint distribution of *X* and *Y*, we can derive their
    conditional expectations (see footnote⁵):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/28807289a5a41e70fa2391d0e6d2a4c2.png)'
  prefs: []
  type: TYPE_IMG
- en: Hence, given the assumptions of our first example, **Equation 2** has a linear
    inverse of the form *x̂(y) = βy*, but *β* is not equal to its deterministic twin
    *1/α* — unless we have *σ = 0* which ***is*** equivalent to the deterministic
    case!
  prefs: []
  type: TYPE_NORMAL
- en: 'This result shows that our intuitions about deterministic linear relationships
    cannot be generalized to probabilistic linear relationships. To more clearly see
    the true insanity of what this result implies, let us first consider *α = 0.5*
    in a deterministic setting (*σ = 0*; blue curves in **Figure 2A and 2B**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/09a89ffb33570d1db5921cd0ccd18814.png)'
  prefs: []
  type: TYPE_IMG
- en: This means that, given a value of *x*, the value of *y* is half of *x*, and,
    given a value of *y*, the value of *x* is twice *y*, which appears to be intuitive.
    Importantly, we always have *x < y*. Now, let us again consider *α = 0.5* but
    this time with *σ² = 3/4* (red curves in **Figure 2A and 2B**). This choice of
    noise variance implies that *β = α = 0.5*, resulting in
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ee452bb34c001d37b883799145b03290.png)'
  prefs: []
  type: TYPE_IMG
- en: This means that, given a value of *x*, our estimation of *y* is half of *x*,
    yet, given a value of *y*, our estimation of *x* is also half of *y*! Strangely,
    we always have *x̂(y) < y* **and** *ŷ(x) < x* — which would be impossible if the
    variables were deterministic. What appears to be counter-intuitive is that **Equation
    1** can be rewritten as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a7d1bad07330772fece111870f475b47.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Equation 3**'
  prefs: []
  type: TYPE_NORMAL
- en: However, this can only imply that (as opposed to **Equation 2**)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8a809fbe6320d65f4b69c38e1d7a4feb.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Equation 4**'
  prefs: []
  type: TYPE_NORMAL
- en: The twist is that, while we have *E[Z|X=x]=0* by design, we cannot say anything
    about *E[Z|Y=y]* and its dependence on *y*! In other words, what makes *x̂(y)*
    different from *y/α* is that observation *y* has also information about error
    *Z*, e.g., if we observe a very large value of *y*, then it means that, with high
    probability, the error *Z* has also a large value, which should be taken into
    account when estimating *X*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/34c547a6bf1cd34829bd1f912a04df60.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 2\. Linear relationships and their inverses in examples 1 and 2.**
    Panel **A** shows the linear relationship between *ŷ(x)* and x in the probabilistic
    settings of examples 1 and 2 (red; α = 0.5) and the equivalent deterministic relationship
    between y and x (blue); not that *ŷ(x)* as a function x is the same in both examples.
    Panels **B** and **C** show the inverse relationships between *x̂(y)* and y in
    the probabilistic settings of example 1 (red in **B**; fixed σ² = 3/4) and example
    2 (red in **C**; σ² dependent on x). The blue line shows the inverse of the equivalent
    deterministic relationship for the reference. In all panels, the dashed black
    shows the y=x line.'
  prefs: []
  type: TYPE_NORMAL
- en: This is the simple explanation for seemingly contradictory statements like ‘tall
    fathers have sons who are (on average) tall but not as tall as themselves, and,
    at the same time, tall sons have fathers who are (on average) tall but not as
    tall as their sons’!
  prefs: []
  type: TYPE_NORMAL
- en: '**To conclude**, our example 1 shows that even if the probabilistic linear
    relationship *ŷ(x) = αx* has a linear inverse of the form *x̂(y) = βy*, the slope
    *β* is ***not*** necessarily equal to its deterministic twin *1/α*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 2: When *x̂(y) is* nonlinear'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having an inverse of the form *x̂(y) = βy* is only possible if *E[Z|Y=y]* in
    **Equation 4** is also a linear function of *y.* In the second example, I make
    a small modification to example 1 in order to break this condition!
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, I assume that the variance of the error term *Z* depends on
    the random variable *X* — as opposed to assumption 1 in example 1\. Formally,
    I assume (in addition to **Equation 1**; see **Figure 1B** for visualization):'
  prefs: []
  type: TYPE_NORMAL
- en: '*X* has a Gaussian distribution with mean zero and variance 1 (same as assumption
    2 in example 1).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Given *X=x*, the error *Z* has a Gaussian distribution with mean zero and variance
    *σ² = 0.01 + 1/(1 + 2x²).*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These assumptions effectively mean that, given *X=x*, the random variable *Y*
    has a Gaussian distribution with mean *αx* and variance *0.01 + 1/(1 + 2x²)* (see
    **Figure 1B**). As opposed to example 1 where the joint distribution of *X* and
    *Y* was a Gaussian distribution, the joint distribution of *X* and *Y* in example
    2 does not have an elegant form (see **Figure 1C**). However, we can still use
    the Bayes rule and find the relatively ugly conditional density of *X=x* given
    *Y=y* (see **Figure 3** for some examples evaluated numerically):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a53a46a03c5a40729cbcdded53d0dcf9.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Equation 5**'
  prefs: []
  type: TYPE_NORMAL
- en: where curly *N* denotes the probability density of the Gaussian distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e4dd217c37dbbab5a31d5518d0a43c33.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 3\. Conditional distribution of X given Y=y in example 2.** Prior
    distribution p(x) (blue curves), likelihood p(y|x) (orange curves), and the posterior
    distribution p(x|y) (black curves; evaluated numerically using **Equation 5**)
    for y = 0.5, 1.5, and 2, from left to right (assuming α = 0.5 in all cases).'
  prefs: []
  type: TYPE_NORMAL
- en: We can then use numerical methods and evaluate the conditional expectation
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/97a673f9587469d6d6dae6a462e17738.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Equation 6**'
  prefs: []
  type: TYPE_NORMAL
- en: for a given *y* and *α*. **Figure 2C** shows *x̂(y)* as a function of *y* for
    *α = 0.5*. As counter-intuitive as it may sound, the inverse relationship is highly
    nonlinear — as a result of the *x*-dependent error variance shown in **Figure
    1B**. This shows that the fact that *y* can be estimated well as a linear function
    of *x* does not imply that *x* can also be estimated well as a linear function
    of *y*. This is because *E[Z|Y=y]* in **Equation 4** can have any strange functional
    dependence on *y* when we go beyond standard assumptions similar to those in example
    1.
  prefs: []
  type: TYPE_NORMAL
- en: '**To conclude**, our example 2 shows that the probabilistic linear relationship
    *ŷ(x) = αx* does ***not*** necessarily have a linear inverse of the form *x̂(y)
    = βy.* Importantly, the inverse relationship between *x̂(y)* and *y* is dependent
    on the characteristics of the error term *Z.*'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout our education, most of us have built an enriched intuition about
    deterministic relationships — based on all the cool results we have seen in calculus,
    analysis, etc. However, it is crucial to be aware of the limitations of this intuition
    and that it must **not** be trusted when we think about probabilistic relationships.
    In particular, examples 1 and 2 show that even extremely simple probabilistic
    relationships can behave against our intuition.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I am grateful to [Johanni Brea](https://scholar.google.ch/citations?user=nZ0m0xUAAAAJ&hl=de),
    Mohammad Tinati, Martin Barry, [Guillaume Bellec](http://guillaume.bellec.eu/),
    [Flavio Martinelli](https://scholar.google.com/citations?user=DabSKBgAAAAJ&hl=en),
    and Ariane Delrocq for useful discussions and valuable feedback on the content
    of this article.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All code (in [Julia](https://julialang.org/) language) for the analyses can
    be found [here](https://github.com/modirshanechi/medium_notes/blob/master/MediumRandomNotes/notebooks/Probabilistic%20Linear%20Relation.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: 'Footnotes:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ¹ Interested readers can see [“How the father’s height influences the son’s
    height”](/how-the-fathers-height-influences-the-son-s-height-62ea0339638d) in
    Towards Data Science for an accessible treatment of this problem.
  prefs: []
  type: TYPE_NORMAL
- en: ² See the “[Minimum mean square error](https://en.wikipedia.org/wiki/Minimum_mean_square_error)”
    page on Wikipedia for more details.
  prefs: []
  type: TYPE_NORMAL
- en: ³ Without loss of generality, we always assume that both *x* and *y* have zero
    average. Hence, in the example of the heights of fathers and their sons, *x* and
    *y* denote the **difference** between their heights and the average heights of
    fathers and sons, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: ⁴ The relationship between **Equations 1** and **2** is reversible, i.e., if
    **Equation 2** is the only constraint on *X* and *Y*, then we can always write
    *Y* as in **Equation 1** with a random variable *Z* that satisfies *E[Z|X=x] =
    0*.
  prefs: []
  type: TYPE_NORMAL
- en: ⁵ See the section ‘Bivariate conditional expectation’ in the [‘Multivariate
    normal distribution’](https://en.wikipedia.org/wiki/Multivariate_normal_distribution)
    page on Wikipedia.
  prefs: []
  type: TYPE_NORMAL
