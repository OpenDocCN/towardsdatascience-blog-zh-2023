- en: 'Efficient Image Segmentation Using PyTorch: Part 4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/efficient-image-segmentation-using-pytorch-part-4-6c86da083432?source=collection_archive---------2-----------------------#2023-06-27](https://towardsdatascience.com/efficient-image-segmentation-using-pytorch-part-4-6c86da083432?source=collection_archive---------2-----------------------#2023-06-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Vision Transformer-based model
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@dhruvbird?source=post_page-----6c86da083432--------------------------------)[![Dhruv
    Matani](../Images/d63bf7776c28a29c02b985b1f64abdd3.png)](https://medium.com/@dhruvbird?source=post_page-----6c86da083432--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6c86da083432--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6c86da083432--------------------------------)
    [Dhruv Matani](https://medium.com/@dhruvbird?source=post_page-----6c86da083432--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F63f5d5495279&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficient-image-segmentation-using-pytorch-part-4-6c86da083432&user=Dhruv+Matani&userId=63f5d5495279&source=post_page-63f5d5495279----6c86da083432---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6c86da083432--------------------------------)
    ·14 min read·Jun 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6c86da083432&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficient-image-segmentation-using-pytorch-part-4-6c86da083432&user=Dhruv+Matani&userId=63f5d5495279&source=-----6c86da083432---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c86da083432&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fefficient-image-segmentation-using-pytorch-part-4-6c86da083432&source=-----6c86da083432---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: In this 4-part series, we’ll implement image segmentation step by step from
    scratch using deep learning techniques in PyTorch. This part will focus on implementing
    a Vision Transformer based model for image segmentation.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Co-authored with [Naresh Singh](https://medium.com/@brocolishbroxoli)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b093c6a5813a75152f341c2d075f686.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Result of running image segmentation using a vision transformer model
    architecture. From top to bottom, input images, ground truth segmentation masks,
    and predicted segmentation masks. Source: Author(s)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Article outline
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we will visit the [Transformer architecture](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)
    which has taken the world of deep learning by storm. The transformer is a multimodal
    architecture which can model different modalities such as language, vision, and
    audio.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将探讨 [变换器架构](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)，它在深度学习领域引起了轰动。变换器是一种多模态架构，可以建模语言、视觉和音频等不同模态。
- en: In this article, we will
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将
- en: Learn about the transformer architecture and the key concepts involved
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 了解变换器架构和涉及的关键概念
- en: Understand the vision transformer architecture
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 了解视觉变换器架构
- en: Introduce a vision transformer model that is written from scratch so that you
    can appreciate all the building blocks and moving parts
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 介绍一个从头开始编写的视觉变换器模型，以便你可以欣赏所有的构建块和活动部分
- en: Follow an input tensor fed into this model and inspect how it changes shape
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 跟踪输入张量进入此模型并检查其形状如何变化
- en: Use this model to perform image segmentation on the Oxford IIIT Pet dataset
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用此模型对牛津IIIT宠物数据集进行图像分割
- en: Observe the results of this segmentation task
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察此分割任务的结果
- en: Briefly introduce the SegFormer, a state of the art vision transformer for semantic
    segmentation
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 简要介绍SegFormer，一个用于语义分割的最新视觉变换器
- en: Throughout this article, we will reference code and results from this [notebook](https://github.com/dhruvbird/ml-notebooks/blob/main/pets_segmentation/vision-transformer-for-oxford-iiit-pet-segmentatio.ipynb)
    for model training. If you wish to reproduce the results, you’ll need a GPU to
    ensure that the first notebook completes running in a reasonable amount of time.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将参考此 [笔记本](https://github.com/dhruvbird/ml-notebooks/blob/main/pets_segmentation/vision-transformer-for-oxford-iiit-pet-segmentatio.ipynb)中的代码和结果进行模型训练。如果你希望重现结果，你需要一个GPU以确保第一个笔记本能在合理的时间内完成运行。
- en: Articles in this series
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本系列文章
- en: 'This series is for readers at all experience levels with deep learning. If
    you want to learn about the practice of deep learning and vision AI along with
    some solid theory and hands-on experience, you’ve come to the right place! This
    is expected to be a 4-part series with the following articles:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本系列适合所有经验水平的深度学习读者。如果你想了解深度学习和视觉AI的实践，同时获得一些扎实的理论和实际经验，你来对地方了！预计这是一个包含四部分的系列，以下是各篇文章：
- en: '[Concepts and Ideas](https://medium.com/p/89e8297a0923/)'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[概念与想法](https://medium.com/p/89e8297a0923/)'
- en: '[A CNN-based model](https://medium.com/p/bed68cadd7c7/)'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[基于CNN的模型](https://medium.com/p/bed68cadd7c7/)'
- en: '[Depthwise separable convolutions](https://medium.com/p/3534cf04fb89/)'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[深度可分离卷积](https://medium.com/p/3534cf04fb89/)'
- en: '**A Vision Transformer-based model (this article)**'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**基于视觉变换器的模型（本文）**'
- en: Let’s start our journey into vision transformers with an introduction and intuitive
    understanding of the transformer architecture.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从对变换器架构的介绍和直观理解开始我们的视觉变换器之旅。
- en: The Transformer Architecture
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变换器架构
- en: We can think of the transformer architecture as a composition of interleaving
    layers of *communication* and *computation*. This idea is depicted visually in
    figure 2\. The transformer has N processing units (N is 3 in Figure 2), each of
    which is responsible for processing a 1/N fraction of the input. For those processing
    units to produce meaningful results, each of them need to have a *global view*
    of the input. Hence, the system repeatedly communicates information about the
    data in every processing unit to every other processing unit; shown using the
    red, green, and blue arrows going from every processing unit to every other processing
    unit. This is followed by some computation based on this information. After sufficient
    repetitions of this process, the model is able to produce desired results.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将变换器架构看作是交替的*通信*和*计算*层的组合。这个概念在图2中以视觉形式展示。变换器有N个处理单元（图2中的N为3），每个处理单元负责处理输入的1/N部分。为了使这些处理单元产生有意义的结果，每个处理单元需要对输入有一个*全局视图*。因此，系统会在每个处理单元与其他所有处理单元之间反复传递数据的信息；这通过从每个处理单元到其他所有处理单元的红色、绿色和蓝色箭头表示。随后基于这些信息进行一些计算。经过足够多次的重复这个过程，模型能够产生期望的结果。
- en: '![](../Images/122173087073c92391689258759c200b.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/122173087073c92391689258759c200b.png)'
- en: 'Figure 2: Interleaved communication and computation in transformers. The image
    shows just 2 layers of communication and computation. In practice, there are many
    more such layers. Source: Author(s).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting most online resources typically discuss both the encoder and
    the decoder of the transformer as presented in the paper titled “[Attention is
    all you need](https://arxiv.org/abs/1706.03762).” However, in this article, we
    will describe just the encoder part of the transformer.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a closer look at what constitutes communication and computation in
    transformers.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'Communication in transformers: Attention'
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In transformers, communication is implemented by a layer known as the attention
    layer. In PyTorch, this is called [MultiHeadAttention](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html).
    We’ll get to the reason for that name in a bit.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'The documentation says:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '*“Allows the model to jointly attend to information from different representation
    subspaces as described in the paper:* [*Attention is all you need*](https://arxiv.org/abs/1706.03762)*.”*'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: The attention mechanism consumes an input tensor *x* of shape (Batch, Length,
    Features), and it produces a similarly shaped tensor *y* such that the features
    for each input are updated based on which other inputs in the same instance the
    tensor is paying attention to. Hence, the features of each tensor of length “Features”
    in the instance of size “Length” are updated based on every other tensor. This
    is where the quadratic cost of the attention mechanism comes in.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/88cc66f9ba95b1dbc9058469c80bcc5e.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Attention of the word “it ” shown relative to the other words in
    the sentence. We can see that “it “ is paying attention to the words “animal “,
    “too “, and “tire(d) ” in the same sentence. Source: Generated using [this colab](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: In the context of a vision transformer, the input to the transformer is an image.
    Let’s assume this to be a 128 x 128 (width, height) image. We chunk it into multiple
    smaller patches of size (16 x 16). For a 128 x 128 image, we get 64 patches (Length),
    8 patches in each row, and 8 rows of patches.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Each one of these 64 patches of size 16 x 16 pixels is considered to be a separate
    input to the transformer model. Without getting too deep into the details, it
    should be sufficient to think of this process as being driven by 64 different
    processing units, each of which is processing a single 16x16 image patch.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: In each round, the attention mechanism in each processing unit is responsible
    for looking at the image patch it is responsible for and querying each one of
    the other remaining 63 processing units to ask them for any information that may
    be relevant and useful to help it effectively process its own image patch.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: The communication step via attention is followed by computation, which we will
    look at next.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'Computation in transformers: Multi Layer Perceptron'
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变换器中的计算：多层感知器
- en: Computation in transformers is nothing but a MultiLayerPerceptron (MLP) unit.
    This unit is composed of 2 Linear layers, with a GeLU non-linearity in between.
    One can consider using other non-linearities as well. This unit first projects
    the input to 4x the size and reprojects it back to 1x, which is the same as the
    input size.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器中的计算实际上就是一个多层感知器（MLP）单元。该单元由2个线性层组成，中间有一个GeLU非线性激活函数。也可以考虑使用其他非线性激活函数。该单元首先将输入投影到4倍大小，然后再将其投影回1倍，即与输入大小相同。
- en: In the code we’ll see in our notebook, this class is called MultiLayerPerceptron.
    The code is shown below.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们笔记本中的代码中，这个类叫做MultiLayerPerceptron。代码如下。
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now that we understand the high level working of the transformer architecture,
    let’s focus our attention on the vision transformer since we’re going to be performing
    image segmentation.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了变换器架构的高层次工作原理，让我们将注意力集中在视觉变换器上，因为我们将进行图像分割。
- en: The Vision Transformer
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视觉变换器
- en: 'The vision transformer was first introduced by the paper titled “[An Image
    is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)”.
    The paper discusses how the authors apply the vanilla transformer architecture
    to the problem of image classification. This is done by splitting the image into
    patches of size 16x16, and treating each patch as an input token to the model.
    The transformer encoder model is fed these input tokens, and is asked to predict
    a class for the input image.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '视觉变换器首次在题为“[An Image is Worth 16x16 Words: Transformers for Image Recognition
    at Scale](https://arxiv.org/abs/2010.11929)”的论文中介绍。该论文讨论了作者如何将原始变换器架构应用于图像分类问题。这是通过将图像拆分为16x16的补丁，并将每个补丁视为模型的输入令牌来完成的。变换器编码器模型接收这些输入令牌，并被要求预测输入图像的类别。'
- en: '![](../Images/c8755cf63d577b7c3845764405c19dc4.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c8755cf63d577b7c3845764405c19dc4.png)'
- en: 'Figure 4: Source: [Transformers for image recognition at scale](https://arxiv.org/pdf/2010.11929.pdf).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：来源：[Transformers for image recognition at scale](https://arxiv.org/pdf/2010.11929.pdf)。
- en: In our case, we are interested in image segmentation. We can consider it to
    be a pixel-level classification task because we intend to predict a target class
    per pixel..
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们对图像分割感兴趣。我们可以将其视为像素级分类任务，因为我们打算预测每个像素的目标类别。
- en: We make a small but important change to the vanilla vision transformer and replace
    the MLP head for classification by an MLP head for pixel level classification.
    We have a single linear layer in the output that is shared by every patch whose
    segmentation mask is predicted by the vision transformer. This shared linear layer
    predicts a segmentation mask for every patch that was sent as input to the model.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对原始视觉变换器进行了一个小但重要的更改，将用于分类的MLP头替换为用于像素级分类的MLP头。我们在输出中有一个线性层，所有的补丁共享这个线性层，其分割掩模由视觉变换器预测。这个共享的线性层为每个输入到模型中的补丁预测一个分割掩模。
- en: In the case of the vision transformer, a patch of size 16x16 is considered to
    be equivalent to a single input token at a specific time step.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在视觉变换器的情况下，16x16的补丁被视为在特定时间步长的单个输入令牌。
- en: '![](../Images/760284762b3ddbd5e9805eb9e8046104.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/760284762b3ddbd5e9805eb9e8046104.png)'
- en: 'Figure 5: The end to end working of the vision transformer for image segmentation.
    Image generated using this [notebook](https://github.com/dhruvbird/ml-notebooks/blob/main/pets_segmentation/Vision%20Transformer%20Article%20Images.ipynb).
    Source: Author(s).'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：视觉变换器在图像分割中的端到端工作。图像使用这个[notebook](https://github.com/dhruvbird/ml-notebooks/blob/main/pets_segmentation/Vision%20Transformer%20Article%20Images.ipynb)生成。来源：作者。
- en: Building an intuition for tensor dimensions in vision transformers
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为视觉变换器中的张量维度建立直觉
- en: 'When working with deep CNNs, the tensor dimensions we used for the most part
    was (N, C H, W), where the letters stand for the following:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理深度CNN时，我们大多数使用的张量维度是（N, C H, W），其中字母代表以下含义：
- en: 'N: Batch size'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'N: 批次大小'
- en: 'C: Number of channels'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'C: 通道数'
- en: 'H: Height'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'H: 高度'
- en: 'W: Width'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'W: 宽度'
- en: You can see that this format is geared toward 2d image processing, since it
    smells of features that are very specific to images.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这个格式是针对2D图像处理的，因为它包含了非常特定于图像的特征。
- en: With transformers on the other hand, things become a lot more generic and domain
    agnostic. What we’ll see below applies to vision, text, NLP, audio or other problems
    where input data can be represented as a sequence. It is worth noting that there’s
    little vision specific bias in the representation of tensors as they flow through
    our vision transformer.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，使用变换器时，事情变得更加通用和与领域无关。以下所述适用于视觉、文本、NLP、音频或其他可以表示为序列的输入数据的问题。值得注意的是，在张量通过视觉变换器流动时，表示方式几乎没有特定于视觉的偏差。
- en: 'When working with transformers and attention in general, we expect the tensors
    to have the following shape: (B, T, C), where the letters stand for the following:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理变换器和注意力机制时，我们期望张量具有以下形状：（B, T, C），其中字母代表以下含义：
- en: 'B: Batch size (same as that for CNNs)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: B：批量大小（与 CNN 相同）
- en: 'T: Time dimension or sequence length. This dimension is also sometimes called
    L. In the case of vision transformers, each image patch corresponds to this dimension.
    If we have 16 image patches, then the value of the T dimension will be 16'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: T：时间维度或序列长度。这个维度有时也被称为 L。在视觉变换器的情况下，每个图像块对应于此维度。如果我们有 16 个图像块，那么 T 维度的值将是 16。
- en: 'C: The channel or embedding size dimension. This dimension is also sometimes
    called E. When processing images, each patch of size 3x16x16 (Channel, Width,
    Height) is mapped via a patch embedding layer to an embedding of size C. We’ll
    see how this is done later.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: C：通道或嵌入大小维度。这个维度有时也被称为 E。在处理图像时，每个 3x16x16（通道，宽度，高度）大小的图像块通过嵌入层映射到大小为 C 的嵌入。我们稍后将看到这是如何完成的。
- en: Let’s dive into how the input image tensor gets mutated and processed along
    its way to predicting the segmentation mask.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨输入图像张量如何在预测分割掩膜的过程中发生变化和处理。
- en: The journey of a tensor in a vision transformer
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视觉变换器中张量的传递过程
- en: In deep CNNs, the journey of a tensor looks something like this (in a UNet,
    SegNet, or other CNN based architecture).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度卷积神经网络中，张量的传递过程大致如下（在 UNet、SegNet 或其他基于 CNN 的架构中）。
- en: The input tensor is typically of shape (1, 3, 128, 128). This tensor goes through
    a series of convolution and max-pooling operations where its spatial dimensions
    are reduced and channel dimensions are increased, typically by a factor of 2 each.
    This is called the feature encoder. After this, we do the reverse operation where
    we increase the spatial dimensions and reduce the channel dimensions. This is
    called the feature decoder. After the decoding process, we get a tensor of shape
    (1, 64, 128, 128). This is then projected into the number of output channels C
    that we desire as (1, C, 128, 128) using a 1x1 pointwise convolution without bias.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 输入张量的形状通常为 (1, 3, 128, 128)。该张量经过一系列卷积和最大池化操作，其中空间维度被缩小，通道维度则通常增加 2 倍。这称为特征编码器。之后，我们进行反向操作，增加空间维度并减少通道维度。这称为特征解码器。解码过程后，我们得到形状为
    (1, 64, 128, 128) 的张量。然后，使用无偏置的 1x1 点卷积将其投影到我们期望的输出通道数量 C，即 (1, C, 128, 128)。
- en: '![](../Images/f40e95ce2f25b6e3d9ca56e9bd8d13f7.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f40e95ce2f25b6e3d9ca56e9bd8d13f7.png)'
- en: 'Figure 6: Typical progression of tensor shapes through a deep CNN used for
    image segmentation. Source: Author(s).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：用于图像分割的深度 CNN 中张量形状的典型演变过程。来源：作者。
- en: With vision transformers, the flow is much more complex. Let’s take a look at
    an image below and then try to understand how the tensor transforms shapes at
    every step along the way.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在视觉变换器中，流程要复杂得多。让我们先看下面的图像，然后尝试理解张量在每一步的形状如何变化。
- en: '![](../Images/65181e90e6a0365aaff6cf61ee98f58c.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/65181e90e6a0365aaff6cf61ee98f58c.png)'
- en: 'Figure 7: Typical progression of tensor shapes through a vision transformer
    for image segmentation. Source: Author(s).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：用于图像分割的视觉变换器中张量形状的典型演变过程。来源：作者。
- en: Let’s look at each step in more detail and see how it updates the shape of the
    tensor flowing through the vision transformer. To understand this better, let’s
    take concrete values for our tensor dimensions.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细查看每一步，并观察它如何更新通过视觉变换器的张量形状。为了更好地理解这一点，我们将张量的维度设置为具体的值。
- en: '**Batch Normalization:** The input and output tensors have shape (1, 3, 128,
    128). The shape is unchanged, but the values are normalized to zero mean and unit
    variance.'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**批量归一化：** 输入和输出张量的形状为 (1, 3, 128, 128)。形状保持不变，但值被标准化为零均值和单位方差。'
- en: '**Image to patches:** The input tensor of shape (1, 3, 128, 128) is converted
    into a stacked patch of 16x16 images. The output tensor has shape (1, 64, 768).'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**图像到补丁：** 形状为（1, 3, 128, 128）的输入张量被转换成堆叠的16x16图像补丁。输出张量的形状为（1, 64, 768）。'
- en: '**Patch embedding:** The patch embedding layer maps the 768 input channels
    to 512 embedding channels (for this example). The output tensor is of shape (1,
    64, 512). The patch embedding layer is basically just an nn.Linear layer in PyTorch.'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**补丁嵌入：** 补丁嵌入层将768个输入通道映射到512个嵌入通道（以此示例为例）。输出张量的形状为（1, 64, 512）。补丁嵌入层基本上只是PyTorch中的一个nn.Linear层。'
- en: '**Position embedding:** The position embedding layer doesn’t have an input
    tensor, but effectively contributes a learnable parameter (trainable tensor in
    PyTorch) o f the same shape as the patch embedding. This is of shape (1, 64, 512).'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**位置嵌入：** 位置嵌入层没有输入张量，但有效地贡献了一个可学习的参数（PyTorch中的可训练张量），其形状与补丁嵌入相同。形状为（1, 64,
    512）。'
- en: '**Add:** The patch and position embeddings are added together piecewise to
    produce the input to our vision transformer encoder. This tensor is of shape (1,
    64, 512). You’ll notice that the main workhorse of the vision transformer, i.e.
    the encoder basically leaves this tensor shape unchanged.'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**添加：** 补丁和位置嵌入逐个相加以产生视觉变换器编码器的输入。这个张量的形状为（1, 64, 512）。你会注意到，视觉变换器的主要工作单元，即编码器，基本上保持了这个张量形状不变。'
- en: '**Transformer encoder:** The input tensor of shape (1, 64, 512) flows through
    multiple transformer encoder blocks, each of which have multiple attention heads
    (communication) followed by an MLP layer (computation). The tensor shape remains
    unchanged as (1, 64, 512).'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**变换器编码器：** 形状为（1, 64, 512）的输入张量流经多个变换器编码器块，每个块具有多个注意力头（通信）后跟一个MLP层（计算）。张量的形状保持不变为（1,
    64, 512）。'
- en: '**Linear output projection:** If we assume that we want to segment each image
    into 10 classes, then we will need each patch of size 16x16 to have 10 channels.
    The nn.Linear layer for output projection will now convert the 512 embedding channels
    to 16x16x10 = 2560 output channels, and this tensor will look like (1, 64, 2560).
    In the diagram above C’ = 10\. Ideally, this would be a multi-layer perceptron,
    since *“*[*MLPs are universal function approximators*](https://en.wikipedia.org/wiki/Multilayer_perceptron)*”*,
    but we use a single linear layer since this is an educational exercise'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**线性输出投影：** 如果我们假设我们想将每个图像分割成10个类别，那么我们需要每个16x16大小的补丁具有10个通道。输出投影的nn.Linear层现在将512个嵌入通道转换为16x16x10
    = 2560个输出通道，这个张量的形状将是（1, 64, 2560）。在上面的图中，C’ = 10。理想情况下，这将是一个多层感知机，因为*“*[*MLPs
    are universal function approximators*](https://en.wikipedia.org/wiki/Multilayer_perceptron)*”*，但由于这是一个教育练习，我们使用了一个单一的线性层。'
- en: '**Patch to image:** This layer converts the 64 patches encoded as a (1, 64,
    2560) tensor back into something that looks like a segmentation mask. This can
    be 10 single channel images, or in this case a single 10 channel image, with each
    channel being the segmentation mask for one of the 10 classes. The output tensor
    is of shape (1, 10, 128, 128).'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**补丁到图像：** 这一层将编码为（1, 64, 2560）张量的64个补丁转换回看起来像分割掩码的东西。这可以是10个单通道图像，或者在这种情况下是一个单一的10通道图像，每个通道都是10个类别中的一个类别的分割掩码。输出张量的形状为（1,
    10, 128, 128）。'
- en: That’s it — we’ve successfully segmented an input image using a vision transformer!
    Next, let’s take a look at an experiment along with some results.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样——我们已经成功地使用视觉变换器对输入图像进行了分割！接下来，让我们看一下实验及其结果。
- en: Vision transformers in action
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视觉变换器的实际应用
- en: This [notebook](https://github.com/dhruvbird/ml-notebooks/blob/main/pets_segmentation/vision-transformer-for-oxford-iiit-pet-segmentatio.ipynb)
    contains all the code for this section.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这个[笔记本](https://github.com/dhruvbird/ml-notebooks/blob/main/pets_segmentation/vision-transformer-for-oxford-iiit-pet-segmentatio.ipynb)包含了本节的所有代码。
- en: As far as the code and class structure is concerned, it closely mimics the block
    diagram above. Most of the concepts mentioned above have a 1:1 correspondence
    to class names in this [notebook](https://github.com/dhruvbird/ml-notebooks/blob/main/pets_segmentation/vision-transformer-for-oxford-iiit-pet-segmentatio.ipynb).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 就代码和类结构而言，它紧密地模拟了上面的框图。上述提到的大部分概念与这个[笔记本](https://github.com/dhruvbird/ml-notebooks/blob/main/pets_segmentation/vision-transformer-for-oxford-iiit-pet-segmentatio.ipynb)中的类名有1:1的对应关系。
- en: There are some concepts related to the attention layers that are critical hyperparameters
    for our model. We didn’t mention anything about the details of the multi-head
    attention earlier since we mentioned that it’s out of scope for the purposes of
    this article. We highly recommend reading the reference material mentioned above
    before proceeding if you don’t have a basic understanding of the attention mechanism
    in transformers.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些与注意力层相关的概念，这些概念是我们模型的关键超参数。我们没有提及多头注意力的详细信息，因为我们提到它超出了本文的范围。如果你对 Transformers
    中的注意力机制没有基本了解，我们强烈建议在继续之前阅读上述参考材料。
- en: We used the following model parameters for the vision transformer for segmentation.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为视觉 Transformer 分割使用了以下模型参数。
- en: 768 embedding dimensions for the PatchEmbedding layer
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PatchEmbedding 层的嵌入维度为 768
- en: 12 Transformer encoder blocks
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 12 个 Transformer 编码器块
- en: 8 attention heads in each transformer encoder block
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个 Transformer 编码器块中有 8 个注意力头
- en: 20% dropout in multi-head attention and MLP
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在多头注意力和 MLP 中使用 20% 的 dropout
- en: This configuration can be seen in the VisionTransformerArgs Python dataclass.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配置可以在 VisionTransformerArgs Python 数据类中看到。
- en: '[PRE1]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: A similar configuration as [before](https://medium.com/p/bed68cadd7c7/) was
    used during model training and validation. The configuration is specified below.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型训练和验证过程中使用了与 [之前](https://medium.com/p/bed68cadd7c7/) 类似的配置。配置详见下文。
- en: The *random horizontal flip* and *colour jitter* data augmentations are applied
    to the training set to prevent overfitting
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对训练集应用 *随机水平翻转* 和 *颜色抖动* 数据增强以防止过拟合
- en: The images are resized to 128x128 pixels in a non-aspect preserving resize operation
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图像在非保持宽高比的调整操作中被调整为 128x128 像素
- en: No input normalization is applied to the images — instead a [batch normalization
    layer is used as the first layer of the model](/replace-manual-normalization-with-batch-normalization-in-vision-ai-models-e7782e82193c)
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图像未应用输入归一化 —— 而是使用了 [作为模型第一层的批归一化层](/replace-manual-normalization-with-batch-normalization-in-vision-ai-models-e7782e82193c)
- en: The model is trained for 50 epochs using the Adam optimizer with a LR of 0.0004
    and a StepLR scheduler that decays the learning rate by 0.8x every 12 epochs
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型使用 Adam 优化器进行 50 次训练周期，学习率为 0.0004，并使用 StepLR 调度器，每 12 个周期将学习率衰减 0.8 倍
- en: The cross-entropy loss function is used to classify a pixel as belonging to
    a pet, the background, or a pet border
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 交叉熵损失函数用于将像素分类为宠物、背景或宠物边界
- en: The model has 86.28M parameters and achieved a validation accuracy of 85.89%
    after 50 training epochs. This is less than the 88.28% accuracy achieved by deep
    CNN model after 20 training epochs. This could be due to a few factors that need
    to be validated experimentally.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 模型有 8628 万个参数，并在 50 次训练周期后达到了 85.89% 的验证准确率。这低于深度 CNN 模型在 20 次训练周期后达到的 88.28%
    的准确率。这可能由于一些需要实验验证的因素。
- en: The last output projection layer is a single nn.Linear and not a multi-layer
    perceptron
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后的输出投影层是单个 nn.Linear，而不是多层感知机。
- en: The 16x16 patch size is too large to capture more fine grained detail
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 16x16 的补丁大小太大，无法捕捉更多细粒度的细节
- en: Not enough training epochs
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练周期不足
- en: Not enough training data — it’s known that transformer models need a lot more
    data to train effectively compared to deep CNN models
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练数据不足 —— 众所周知，Transformer 模型相比于深度 CNN 模型需要更多的数据才能有效训练
- en: The learning rate is too low
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 学习率过低
- en: We plotted a gif showing how the model is learning to predict the segmentation
    masks for 21 images in the validation set.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们绘制了一个 gif，展示了模型如何学习预测验证集中的 21 张图像的分割掩码。
- en: '![](../Images/d8bbbd44bf1a44104ca6eacf763cecc4.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d8bbbd44bf1a44104ca6eacf763cecc4.png)'
- en: 'Figure 8: A gif showing the progression of segmentation masks predicted by
    the vision transformer for image segmentation model. Source: Author(s).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：一个 gif 展示了视觉 Transformer 对图像分割模型预测的分割掩码的进展。来源：作者。
- en: We notice something interesting in the early training epochs. The predicted
    segmentation masks have some strange blocking artifacts. The only reason we could
    think of for this is because we’re breaking down the image into patches of size
    16x16 and after very few training epochs, the model hasn’t learned anything useful
    beyond some very coarse grained information regarding whether this 16x16 patch
    is generally covered by a pet or by background pixels.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在早期训练轮次中注意到了一些有趣的现象。预测的分割掩码出现了一些奇怪的块状伪影。我们能想到的唯一原因是因为我们将图像分解为大小为 16x16 的图块，而在训练了很少的轮次后，模型还没有学到除了关于这个
    16x16 图块是否通常被宠物或背景像素覆盖的非常粗糙的信息之外的任何有用知识。
- en: '![](../Images/582f840408166227940298e106d6b43e.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/582f840408166227940298e106d6b43e.png)'
- en: 'Figure 9: The blocking artifacts seen in the predicted segmentation masks when
    using the vision transformer for image segmentation. Source: Author(s).'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：使用视觉变换器进行图像分割时，预测的分割掩码中看到的块状伪影。来源：作者。
- en: Now that we have seen a basic vision transformer in action, let’s turn our attention
    to a state of the art vision transformer for segmentation tasks.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到一个基本的视觉变换器在实际应用中，让我们将注意力转向用于分割任务的最先进视觉变换器。
- en: 'SegFormer: Semantic segmentation with transformers'
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SegFormer：基于变换器的语义分割
- en: The SegFormer architecture was proposed in [this paper](https://arxiv.org/pdf/2105.15203.pdf)
    in 2021\. The transformer we saw above is a simpler version of the SegFormer architecture.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: SegFormer 架构在 [这篇论文](https://arxiv.org/pdf/2105.15203.pdf) 中于 2021 年提出。我们看到的变换器是
    SegFormer 架构的简化版本。
- en: '![](../Images/e31f0f0d7960592cf42c623cfede6aeb.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e31f0f0d7960592cf42c623cfede6aeb.png)'
- en: 'Figure 10: The SegFormer architecture. Source: [SegFormer paper (2021)](https://arxiv.org/pdf/2105.15203.pdf).'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：SegFormer 架构。来源：[SegFormer 论文 (2021)](https://arxiv.org/pdf/2105.15203.pdf)。
- en: 'Most notably, the SegFormer:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 最值得注意的是，SegFormer：
- en: Generates 4 sets of images with patches of size 4x4, 8x8, 16x16, and 32x32 instead
    of a single patched image with patches of size 16x16
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成 4 组图像，图块大小分别为 4x4、8x8、16x16 和 32x32，而不是生成单一图块大小为 16x16 的图像。
- en: Uses 4 transformer encoder blocks instead of just 1\. This feels like a model
    ensemble
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用了 4 个变换器编码块，而不仅仅是 1 个。这感觉像是模型集成。
- en: Uses convolutions in the pre and post phases of self-attention
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在自注意力的前期和后期阶段使用卷积。
- en: Doesn’t use positional embeddings
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不使用位置嵌入
- en: Each transformer block processes images at spatial resolution H/4 x W/4, H/8
    x W/8, H/16 x W/16, and H/32, W/32
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个变换器块处理图像的空间分辨率为 H/4 x W/4、H/8 x W/8、H/16 x W/16 和 H/32 x W/32。
- en: Similarly, the channels increase when the spatial dimensions reduce. This feels
    similar to deep CNNs
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似地，当空间维度减少时，通道数会增加。这与深度 CNN 类似。
- en: Predictions at multiple spatial dimensions are upsampled and then merged together
    in the decoder
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在解码器中，将多个空间维度的预测结果上采样并合并在一起。
- en: An MLP combines all these predictions to provide a final prediction
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MLP 将所有这些预测结合起来，提供最终预测。
- en: The final prediction is at spatial dimension H/4, W/4 and not at H, W.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终预测位于空间维度 H/4，W/4，而不是 H，W。
- en: Conclusion
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In part-4 of this series, we were introduced to the transformer architecture
    and vision transformers in particular. We developed an intuitive understanding
    of how vision transformers work, and the basic building block involved in the
    communication and computation phases of vision transformers. We saw the unique
    patch based approach adopted by vision transformers for predicting segmentation
    masks and then combining the predictions together.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在本系列的第 4 部分中，我们介绍了变换器架构，特别是视觉变换器。我们对视觉变换器如何工作有了直观的理解，并了解了视觉变换器通信和计算阶段的基本构建块。我们看到了视觉变换器采用的独特图块方法，用于预测分割掩码，然后将预测结果结合在一起。
- en: We reviewed an experiment that shows vision transformers in action, and were
    able to compare results with deep CNN approaches. While our vision transformer
    is not state of the art, it was able to achieve pretty decent results. We provided
    a glimpse into state-of-the-art approaches such as SegFormer.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们回顾了一项实验，展示了视觉变换器的实际应用，并能够将结果与深度 CNN 方法进行比较。虽然我们的视觉变换器不是最先进的，但它能够取得相当不错的结果。我们简要介绍了如
    SegFormer 等最先进的方法。
- en: It should be clear by now that transformers have a lot more moving parts and
    are more complex compared to deep CNN-based approaches. From a raw FLOPs point
    of view, transformers hold the promise of being more efficient. In transformers,
    the only real layer that is computationally heavy is nn.Linear. This is implemented
    using optimized matrix multiplication on most architectures. Due to this architectural
    simplicity, transformers hold the promise of being easier to optimize and speed
    up compared to deep CNN-based approaches.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在应该很清楚，相比于基于深度 CNN 的方法，transformers 具有更多的活动部分且更复杂。从原始的 FLOPs 角度来看，transformers
    有可能更高效。在 transformers 中，唯一真正计算密集的层是 nn.Linear。大多数架构使用优化的矩阵乘法来实现这一点。由于这种架构上的简单性，transformers
    有可能比基于深度 CNN 的方法更容易优化和加速。
- en: Congratulations on making it this far! We’re glad you enjoyed reading this series
    on efficient image segmentation in PyTorch. If you have questions or comments,
    please feel free to leave them in the comments section.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你读到这里！我们很高兴你喜欢阅读这一系列关于 PyTorch 高效图像分割的文章。如果你有问题或评论，请随时在评论区留言。
- en: Further reading
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: The details of the attention mechanism are out of scope for this article. Besides,
    there are numerous high quality resources that you can refer to to understand
    the attention mechanism in great detail. Here are some that we highly recommend.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制的详细信息超出了本文的范围。此外，您可以参考许多高质量的资源以深入了解注意力机制。以下是我们强烈推荐的一些资源。
- en: '[The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[插图版 Transformer](http://jalammar.github.io/illustrated-transformer/)'
- en: '[NanoGPT from scratch using PyTorch](https://www.youtube.com/watch?v=kCc8FmEb1nY)'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[从零开始使用 PyTorch 实现 NanoGPT](https://www.youtube.com/watch?v=kCc8FmEb1nY)'
- en: We’ll provide links to articles that provide more details on vision transformers
    below.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下面提供更多关于 vision transformers 的文章链接。
- en: '[Implementing Vision Transformer (ViT) in PyTorch](/implementing-visualttransformer-in-pytorch-184f9f16f632):
    This article details the implementation of a vision transformer for image classification
    in PyTorch. Notably, their implementation uses [einops](https://einops.rocks/),
    which we avoid, since this is an education-focused exercise (we recommend learning
    and using einops for code readability though). We instead use native PyTorch operators
    for permuting and rearranging tensor dimensions. Additionally, there are a few
    places where the author uses Conv2d instead of Linear layers. We wanted to build
    an implementation of vision transformers without the use of convolutional layers
    entirely.'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[在 PyTorch 中实现 Vision Transformer (ViT)](/implementing-visualttransformer-in-pytorch-184f9f16f632)：这篇文章详细介绍了在
    PyTorch 中实现用于图像分类的 vision transformer。值得注意的是，他们的实现使用了 [einops](https://einops.rocks/)，但我们避免使用，因为这是一个以教育为重点的练习（不过我们建议您学习和使用
    einops 以提高代码的可读性）。我们则使用原生 PyTorch 操作符来排列和重排张量维度。此外，作者在一些地方使用了 Conv2d 替代 Linear
    层。我们希望在完全不使用卷积层的情况下构建 vision transformers 的实现。'
- en: '[Vision Transformer: AI Summer](https://theaisummer.com/vision-transformer/)'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Vision Transformer: AI Summer](https://theaisummer.com/vision-transformer/)'
- en: '[Implementing SegFormer in PyTorch](/implementing-segformer-in-pytorch-8f4705e2ed0e)'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[在 PyTorch 中实现 SegFormer](/implementing-segformer-in-pytorch-8f4705e2ed0e)'
