- en: 4-bit Quantization with GPTQ
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4ä½é‡åŒ–ä¸GPTQ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/4-bit-quantization-with-gptq-36b0f4f02c34?source=collection_archive---------0-----------------------#2023-07-31](https://towardsdatascience.com/4-bit-quantization-with-gptq-36b0f4f02c34?source=collection_archive---------0-----------------------#2023-07-31)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/4-bit-quantization-with-gptq-36b0f4f02c34?source=collection_archive---------0-----------------------#2023-07-31](https://towardsdatascience.com/4-bit-quantization-with-gptq-36b0f4f02c34?source=collection_archive---------0-----------------------#2023-07-31)
- en: Quantize your own LLMs using AutoGPTQ
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨AutoGPTQé‡åŒ–ä½ è‡ªå·±çš„LLM
- en: '[](https://medium.com/@mlabonne?source=post_page-----36b0f4f02c34--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----36b0f4f02c34--------------------------------)[](https://towardsdatascience.com/?source=post_page-----36b0f4f02c34--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----36b0f4f02c34--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----36b0f4f02c34--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne?source=post_page-----36b0f4f02c34--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----36b0f4f02c34--------------------------------)[](https://towardsdatascience.com/?source=post_page-----36b0f4f02c34--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----36b0f4f02c34--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----36b0f4f02c34--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc89da634938&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-bit-quantization-with-gptq-36b0f4f02c34&user=Maxime+Labonne&userId=dc89da634938&source=post_page-dc89da634938----36b0f4f02c34---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----36b0f4f02c34--------------------------------)
    Â·10 min readÂ·Jul 31, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F36b0f4f02c34&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-bit-quantization-with-gptq-36b0f4f02c34&user=Maxime+Labonne&userId=dc89da634938&source=-----36b0f4f02c34---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc89da634938&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-bit-quantization-with-gptq-36b0f4f02c34&user=Maxime+Labonne&userId=dc89da634938&source=post_page-dc89da634938----36b0f4f02c34---------------------post_header-----------)
    å‘è¡¨åœ¨[Towards Data Science](https://towardsdatascience.com/?source=post_page-----36b0f4f02c34--------------------------------)
    Â· 10åˆ†é’Ÿé˜…è¯» Â· 2023å¹´7æœˆ31æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F36b0f4f02c34&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-bit-quantization-with-gptq-36b0f4f02c34&user=Maxime+Labonne&userId=dc89da634938&source=-----36b0f4f02c34---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F36b0f4f02c34&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-bit-quantization-with-gptq-36b0f4f02c34&source=-----36b0f4f02c34---------------------bookmark_footer-----------)![](../Images/c14c3bcded8e89b92f862c1114cba7ff.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F36b0f4f02c34&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-bit-quantization-with-gptq-36b0f4f02c34&source=-----36b0f4f02c34---------------------bookmark_footer-----------)![](../Images/c14c3bcded8e89b92f862c1114cba7ff.png)'
- en: Image by author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: Recent advancements in weight quantization allow us to run massive large language
    models on consumer hardware, like a LLaMA-30B model on an RTX 3090 GPU. This is
    possible thanks to novel 4-bit quantization techniques with minimal performance
    degradation, like [GPTQ](https://arxiv.org/abs/2210.17323), [GGML](https://github.com/ggerganov/ggml),
    and [NF4](https://huggingface.co/blog/4bit-transformers-bitsandbytes).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘çš„æƒé‡é‡åŒ–è¿›å±•ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨æ¶ˆè´¹è€…ç¡¬ä»¶ä¸Šè¿è¡Œå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼Œä¾‹å¦‚åœ¨RTX 3090 GPUä¸Šè¿è¡ŒLLaMA-30Bæ¨¡å‹ã€‚è¿™å¾—ç›Šäºæ–°é¢–çš„4ä½é‡åŒ–æŠ€æœ¯ï¼Œæ€§èƒ½é™çº§æœ€å°ï¼Œå¦‚[GPTQ](https://arxiv.org/abs/2210.17323)ã€[GGML](https://github.com/ggerganov/ggml)å’Œ[NF4](https://huggingface.co/blog/4bit-transformers-bitsandbytes)ã€‚
- en: In the [previous article](https://medium.com/towards-data-science/introduction-to-weight-quantization-2494701b9c0c),
    we introduced naÃ¯ve 8-bit quantization techniques and the excellent LLM.int8().
    In this article, we will explore the popular **GPTQ algorithm** to understand
    how it works and implement it using the [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)
    library.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[ä¸Šä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/introduction-to-weight-quantization-2494701b9c0c)ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†æœ´ç´ çš„8ä½é‡åŒ–æŠ€æœ¯å’Œä¼˜ç§€çš„LLM.int8()ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨æµè¡Œçš„**GPTQç®—æ³•**ï¼Œäº†è§£å…¶å·¥ä½œåŸç†ï¼Œå¹¶ä½¿ç”¨[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)åº“è¿›è¡Œå®ç°ã€‚
- en: You can find the code on [Google Colab](https://colab.research.google.com/drive/1lSvVDaRgqQp_mWK_jC9gydz6_-y6Aq4A?usp=sharing)
    and [GitHub](https://github.com/mlabonne/llm-course/tree/main).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨ [Google Colab](https://colab.research.google.com/drive/1lSvVDaRgqQp_mWK_jC9gydz6_-y6Aq4A?usp=sharing)
    å’Œ [GitHub](https://github.com/mlabonne/llm-course/tree/main) ä¸Šæ‰¾åˆ°ä»£ç ã€‚
- en: ğŸ§  Optimal Brain Quantization
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ§  æœ€ä¼˜è„‘é‡åŒ–
- en: 'Letâ€™s start by introducing the problem weâ€™re trying to solve. For every layer
    â„“ in the network, we want to find a quantized version **Å´â‚—** *of the original
    weights* **Wâ‚—**. This is called the **layer-wise compression problem**. More specifically,
    to minimize performance degradation, we want the outputs (**Å´**áµ¨**X**áµ¨) of these
    new weights to be as close as possible to the original ones (**W**áµ¨**X**áµ¨). In
    other words, we want to find:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆä»‹ç»æˆ‘ä»¬è¦è§£å†³çš„é—®é¢˜ã€‚å¯¹äºç½‘ç»œä¸­çš„æ¯ä¸€å±‚ â„“ï¼Œæˆ‘ä»¬å¸Œæœ›æ‰¾åˆ°åŸå§‹æƒé‡ **Wâ‚—** çš„é‡åŒ–ç‰ˆæœ¬ **Å´â‚—**ã€‚è¿™ç§°ä¸º **é€å±‚å‹ç¼©é—®é¢˜**ã€‚æ›´å…·ä½“åœ°ï¼Œä¸ºäº†æœ€å°åŒ–æ€§èƒ½é™çº§ï¼Œæˆ‘ä»¬å¸Œæœ›è¿™äº›æ–°æƒé‡çš„è¾“å‡º
    (**Å´**áµ¨**X**áµ¨) å°½å¯èƒ½æ¥è¿‘åŸå§‹æƒé‡çš„è¾“å‡º (**W**áµ¨**X**áµ¨)ã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬å¸Œæœ›æ‰¾åˆ°ï¼š
- en: '![](../Images/bea35e882148e389c0c3d23f956d46d2.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bea35e882148e389c0c3d23f956d46d2.png)'
- en: Different approaches have been proposed to solve this problem, but weâ€™re interested
    in the [**Optimal Brain Quantizer**](https://arxiv.org/abs/2208.11580) (OBQ) framework
    here.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å·²æå‡ºäº†ä¸åŒçš„æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†æˆ‘ä»¬åœ¨è¿™é‡Œå…³æ³¨çš„æ˜¯ [**æœ€ä¼˜è„‘é‡åŒ–å™¨**](https://arxiv.org/abs/2208.11580)ï¼ˆOBQï¼‰æ¡†æ¶ã€‚
- en: 'This method is inspired by a **pruning technique** to carefully remove weights
    from a fully trained dense neural network (Optimal Brain Surgeon). It uses an
    approximation technique and provides explicit formulas for the best single weight
    *wğ¥* to remove and optimal update *Î´*êŸ³ to adjust the set of remaining non-quantized
    weights *F* to make up for the removal:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ–¹æ³•çš„çµæ„Ÿæ¥æºäºä¸€ç§ **å‰ªææŠ€æœ¯**ï¼Œç”¨äºä»å®Œå…¨è®­ç»ƒçš„å¯†é›†ç¥ç»ç½‘ç»œä¸­å°å¿ƒåœ°å»é™¤æƒé‡ï¼ˆæœ€ä¼˜è„‘å¤–ç§‘åŒ»ç”Ÿï¼‰ã€‚å®ƒä½¿ç”¨äº†ä¸€ç§è¿‘ä¼¼æŠ€æœ¯ï¼Œå¹¶æä¾›äº†æœ€ä½³å•ä¸ªæƒé‡ *wğ¥*
    çš„æ˜¾å¼å…¬å¼ä»¥å»é™¤ï¼Œå¹¶é€šè¿‡æœ€ä½³æ›´æ–° *Î´*êŸ³ æ¥è°ƒæ•´å‰©ä½™æœªé‡åŒ–æƒé‡ *F*ï¼Œä»¥å¼¥è¡¥å»é™¤çš„å½±å“ï¼š
- en: '![](../Images/c28f4ed29652a4fba9e7411172489f2a.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c28f4ed29652a4fba9e7411172489f2a.png)'
- en: where quant(*w*) is the weight rounding given by the quantization and **H**êŸ³
    is the Hessian.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ quant(*w*) æ˜¯ç”±é‡åŒ–ç»™å‡ºçš„æƒé‡èˆå…¥ï¼Œ**H**êŸ³ æ˜¯ Hessianã€‚
- en: Using OBQ, we can quantize the easiest weight first and then adjust all remaining
    non-quantized weights to **compensate for this precision loss**. Then we pick
    the next weight to quantize, and so on.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ OBQï¼Œæˆ‘ä»¬å¯ä»¥å…ˆé‡åŒ–æœ€ç®€å•çš„æƒé‡ï¼Œç„¶åè°ƒæ•´æ‰€æœ‰å‰©ä½™çš„æœªé‡åŒ–æƒé‡ä»¥ **è¡¥å¿è¿™ç§ç²¾åº¦æŸå¤±**ã€‚ç„¶åæˆ‘ä»¬é€‰æ‹©ä¸‹ä¸€ä¸ªè¦é‡åŒ–çš„æƒé‡ï¼Œä¾æ­¤ç±»æ¨ã€‚
- en: 'A potential issue with this approach is when there are outlier weights, which
    can result in high **quantization error**. Usually, these outliers would be quantized
    last, when there are few non-quantized weights left that could be adjusted to
    compensate for the large error. This effect can worsen when some weights are pushed
    further outside the grid by intermediate updates. A simple heuristic is applied
    to prevent this: outliers are quantized as soon as they appear.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•çš„ä¸€ä¸ªæ½œåœ¨é—®é¢˜æ˜¯ï¼Œå½“å­˜åœ¨å¼‚å¸¸æƒé‡æ—¶ï¼Œå¯èƒ½å¯¼è‡´é«˜ **é‡åŒ–è¯¯å·®**ã€‚é€šå¸¸ï¼Œè¿™äº›å¼‚å¸¸å€¼ä¼šåœ¨é‡åŒ–æœ€åå¤„ç†ï¼Œå½“å‰©ä¸‹çš„æœªé‡åŒ–æƒé‡è¾ƒå°‘æ—¶ï¼Œå¯ä»¥è°ƒæ•´ä»¥å¼¥è¡¥å¤§è¯¯å·®ã€‚å½“ä¸€äº›æƒé‡é€šè¿‡ä¸­é—´æ›´æ–°è¢«æ¨åˆ°ç½‘æ ¼ä¹‹å¤–æ—¶ï¼Œè¿™ç§æ•ˆåº”å¯èƒ½ä¼šåŠ å‰§ã€‚ä¸€ä¸ªç®€å•çš„å¯å‘å¼æ–¹æ³•æ˜¯å°½å¿«é‡åŒ–å‡ºç°çš„å¼‚å¸¸å€¼ã€‚
- en: 'This process could be computationally heavy, especially for LLMs. To deal with
    this, the OBQ method uses a trick that avoids redoing the entire computation each
    time a weight is simplified. After quantizing a weight, it adjusts the matrix
    used in calculations (the Hessian) by **removing the row and column** associated
    with that weight (using Gaussian elimination):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¿‡ç¨‹å¯èƒ½è®¡ç®—é‡å¾ˆå¤§ï¼Œç‰¹åˆ«æ˜¯å¯¹äº LLMã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒOBQ æ–¹æ³•ä½¿ç”¨äº†ä¸€ç§æŠ€å·§ï¼Œé¿å…äº†æ¯æ¬¡ç®€åŒ–æƒé‡æ—¶é‡æ–°è¿›è¡Œæ•´ä¸ªè®¡ç®—ã€‚é‡åŒ–ä¸€ä¸ªæƒé‡åï¼Œå®ƒé€šè¿‡
    **å»é™¤ä¸è¯¥æƒé‡ç›¸å…³çš„è¡Œå’Œåˆ—**ï¼ˆä½¿ç”¨é«˜æ–¯æ¶ˆå…ƒæ³•ï¼‰æ¥è°ƒæ•´è®¡ç®—ä¸­ä½¿ç”¨çš„çŸ©é˜µï¼ˆHessianï¼‰ï¼š
- en: '![](../Images/04c24c3d6a72907b1b3508009c8a72d8.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04c24c3d6a72907b1b3508009c8a72d8.png)'
- en: The method also employs vectorization to process multiple rows of the weight
    matrix at once. Despite its efficiency, the OBQâ€™s computation time increases significantly
    as the size of the weight matrix increases. This cubic growth makes it difficult
    to use OBQ on very large models with billions of parameters.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ–¹æ³•è¿˜é‡‡ç”¨äº†å‘é‡åŒ–å¤„ç†ä¸€æ¬¡å¤„ç†å¤šä¸ªæƒé‡çŸ©é˜µçš„è¡Œã€‚å°½ç®¡æ•ˆç‡å¾ˆé«˜ï¼Œä½†éšç€æƒé‡çŸ©é˜µå¤§å°çš„å¢åŠ ï¼ŒOBQ çš„è®¡ç®—æ—¶é—´æ˜¾è‘—å¢åŠ ã€‚è¿™ç§ç«‹æ–¹å¢é•¿ä½¿å¾—åœ¨å…·æœ‰æ•°åäº¿å‚æ•°çš„å¤§å‹æ¨¡å‹ä¸Šä½¿ç”¨
    OBQ å˜å¾—å›°éš¾ã€‚
- en: ğŸ§® The GPTQ Algorithm
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ§® GPTQ ç®—æ³•
- en: Introduced by Frantar et al. (2023), the [GPTQ algorithm](https://arxiv.org/abs/2210.17323)
    takes inspiration from the OBQ method, but with significant improvements to scale
    it for (very) large language models.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±Frantarç­‰äººï¼ˆ2023å¹´ï¼‰æå‡ºçš„[GPTQç®—æ³•](https://arxiv.org/abs/2210.17323)ä»OBQæ–¹æ³•ä¸­è·å¾—çµæ„Ÿï¼Œä½†å¯¹å…¶è¿›è¡Œäº†é‡å¤§æ”¹è¿›ï¼Œä»¥ä½¿å…¶é€‚ç”¨äºï¼ˆéå¸¸ï¼‰å¤§çš„è¯­è¨€æ¨¡å‹ã€‚
- en: 'Step 1: Arbitrary Order Insight'
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ­¥ï¼šä»»æ„é¡ºåºæ´å¯Ÿ
- en: The OBQ method selects weights (parameters in a model) for quantization in a
    certain order, determined by which will **add the least additional error**. However,
    GPTQ observes that for large models, quantizing weights in any fixed order can
    perform just as well. This is because even though some weights might introduce
    more error individually, they are quantized later in the process when there are
    few other weights left that could increase the error. So the order doesnâ€™t matter
    as much as we thought.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: OBQæ–¹æ³•æŒ‰ä¸€å®šé¡ºåºé€‰æ‹©è¦é‡åŒ–çš„æƒé‡ï¼ˆæ¨¡å‹ä¸­çš„å‚æ•°ï¼‰ï¼Œè¯¥é¡ºåºç”±**å¢åŠ æœ€å°‘é¢å¤–è¯¯å·®**çš„æƒé‡å†³å®šã€‚ç„¶è€Œï¼ŒGPTQè§‚å¯Ÿåˆ°ï¼Œå¯¹äºå¤§å‹æ¨¡å‹ï¼ŒæŒ‰ä»»ä½•å›ºå®šé¡ºåºé‡åŒ–æƒé‡çš„æ•ˆæœéƒ½ä¸€æ ·ã€‚è¿™æ˜¯å› ä¸ºå³ä½¿æŸäº›æƒé‡å¯èƒ½å•ç‹¬å¼•å…¥æ›´å¤šè¯¯å·®ï¼Œå®ƒä»¬ä¼šåœ¨å¤„ç†è¿‡ç¨‹ä¸­è¾ƒæ™šçš„é˜¶æ®µè¿›è¡Œé‡åŒ–ï¼Œæ­¤æ—¶å¯èƒ½æ²¡æœ‰å…¶ä»–æƒé‡å¯ä»¥å¢åŠ è¯¯å·®ã€‚å› æ­¤ï¼Œé¡ºåºå¹¶ä¸åƒæˆ‘ä»¬æƒ³è±¡çš„é‚£æ ·é‡è¦ã€‚
- en: Based on this insight, GPTQ aims to quantize all weights in the **same order
    for all rows** of a matrix. This makes the process faster because certain computations
    have to be done only once for each column, rather than once for each weight.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºè¿™ä¸€æ´å¯Ÿï¼ŒGPTQæ—¨åœ¨å¯¹çŸ©é˜µçš„**æ‰€æœ‰è¡Œä½¿ç”¨ç›¸åŒçš„é¡ºåºé‡åŒ–æ‰€æœ‰æƒé‡**ã€‚è¿™ä½¿å¾—è¿‡ç¨‹æ›´å¿«ï¼Œå› ä¸ºæŸäº›è®¡ç®—åªéœ€è¦å¯¹æ¯ä¸€åˆ—è¿›è¡Œä¸€æ¬¡ï¼Œè€Œä¸æ˜¯å¯¹æ¯ä¸ªæƒé‡è¿›è¡Œä¸€æ¬¡ã€‚
- en: '![](../Images/dd7b329f93ed489449b8f138e55b63b5.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd7b329f93ed489449b8f138e55b63b5.png)'
- en: Image by author
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: 'Step 2: Lazy Batch-Updates'
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬äºŒæ­¥ï¼šæ‡’æƒ°æ‰¹é‡æ›´æ–°
- en: This scheme wonâ€™t be fast because it requires updating a **huge matrix** with
    very few computations for each entry. This type of operation canâ€™t utilize the
    full compute capabilities of GPUs and will be slowed down by memory limitations
    (memory throughput bottleneck).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ–¹æ¡ˆä¸ä¼šå¾ˆå¿«ï¼Œå› ä¸ºå®ƒéœ€è¦æ›´æ–°ä¸€ä¸ª**å·¨å¤§çš„çŸ©é˜µ**ï¼Œæ¯ä¸ªæ¡ç›®çš„è®¡ç®—é‡éå¸¸å°‘ã€‚è¿™ç§æ“ä½œæ— æ³•åˆ©ç”¨GPUçš„å…¨éƒ¨è®¡ç®—èƒ½åŠ›ï¼Œå¹¶ä¸”ä¼šå—åˆ°å†…å­˜é™åˆ¶ï¼ˆå†…å­˜ååç“¶é¢ˆï¼‰çš„å½±å“ã€‚
- en: To resolve this, GPTQ introduces â€œlazy batchâ€ updates. It turns out that the
    final rounding decisions for a given column are only affected by updates performed
    on that column, not on later columns. Therefore, GPTQ can apply the algorithm
    to a **batch of columns at a time** (like 128 columns), updating only those columns
    and a corresponding block of the matrix. After a block is fully processed, the
    algorithm performs global updates on the entire matrix.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼ŒGPTQå¼•å…¥äº†â€œæ‡’æƒ°æ‰¹é‡â€æ›´æ–°ã€‚ç»“æœå‘ç°ï¼Œç»™å®šåˆ—çš„æœ€ç»ˆèˆå…¥å†³ç­–åªå—è¯¥åˆ—ä¸Šçš„æ›´æ–°å½±å“ï¼Œè€Œä¸å—åç»­åˆ—çš„å½±å“ã€‚å› æ­¤ï¼ŒGPTQå¯ä»¥å°†ç®—æ³•åº”ç”¨äº**ä¸€æ¬¡å¤„ç†ä¸€æ‰¹åˆ—**ï¼ˆå¦‚128åˆ—ï¼‰ï¼Œä»…æ›´æ–°è¿™äº›åˆ—åŠçŸ©é˜µä¸­çš„ç›¸åº”å—ã€‚ä¸€ä¸ªå—å®Œå…¨å¤„ç†åï¼Œç®—æ³•å¯¹æ•´ä¸ªçŸ©é˜µè¿›è¡Œå…¨å±€æ›´æ–°ã€‚
- en: '![](../Images/8fe33b573159cfae9d1247d3073088cf.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8fe33b573159cfae9d1247d3073088cf.png)'
- en: 'Step 3: Cholesky Reformulation'
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰æ­¥ï¼šCholeskyé‡æ„
- en: However, thereâ€™s one more issue to address. When the algorithm scales up to
    very large models, numerical inaccuracies can become a problem. Specifically,
    repeated applications of a certain operation can **accumulate numerical errors**.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¿‡ï¼Œè¿˜æœ‰ä¸€ä¸ªé—®é¢˜éœ€è¦è§£å†³ã€‚å½“ç®—æ³•æ‰©å±•åˆ°éå¸¸å¤§çš„æ¨¡å‹æ—¶ï¼Œæ•°å€¼ä¸å‡†ç¡®å¯èƒ½æˆä¸ºé—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼ŒæŸäº›æ“ä½œçš„é‡å¤åº”ç”¨å¯èƒ½ä¼š**ç§¯ç´¯æ•°å€¼è¯¯å·®**ã€‚
- en: To tackle this, GPTQ uses a [Cholesky decomposition](https://en.wikipedia.org/wiki/Cholesky_decomposition),
    a numerically stable method for solving certain mathematical problems. It involves
    precomputing some required information from the matrix using the Cholesky method.
    This approach, combined with a slight â€œdampeningâ€ (adding a small constant to
    diagonal elements of the matrix), helps the algorithm to avoid numerical issues.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒGPTQä½¿ç”¨äº†[Choleskyåˆ†è§£](https://en.wikipedia.org/wiki/Cholesky_decomposition)ï¼Œè¿™æ˜¯ä¸€ç§æ•°å€¼ç¨³å®šçš„æ•°å­¦é—®é¢˜è§£å†³æ–¹æ³•ã€‚å®ƒæ¶‰åŠä½¿ç”¨Choleskyæ–¹æ³•ä»çŸ©é˜µä¸­é¢„è®¡ç®—ä¸€äº›æ‰€éœ€çš„ä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•ï¼ŒåŠ ä¸Šè½»å¾®çš„â€œå‡éœ‡â€ï¼ˆå‘çŸ©é˜µå¯¹è§’çº¿å…ƒç´ ä¸­æ·»åŠ ä¸€ä¸ªå°å¸¸æ•°ï¼‰ï¼Œæœ‰åŠ©äºç®—æ³•é¿å…æ•°å€¼é—®é¢˜ã€‚
- en: 'The full algorithm can be summarized in a few steps:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæ•´ç®—æ³•å¯ä»¥æ€»ç»“ä¸ºå‡ ä¸ªæ­¥éª¤ï¼š
- en: The GPTQ algorithm begins with a Cholesky decomposition of the Hessian inverse
    (a matrix that helps decide how to adjust the weights)
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GPTQç®—æ³•ä»¥Hessiané€†çŸ©é˜µçš„Choleskyåˆ†è§£å¼€å§‹ï¼ˆè¯¥çŸ©é˜µæœ‰åŠ©äºå†³å®šå¦‚ä½•è°ƒæ•´æƒé‡ï¼‰ã€‚
- en: It then runs in loops, handling batches of columns at a time.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç„¶åå®ƒä»¥å¾ªç¯çš„æ–¹å¼è¿è¡Œï¼Œæ¯æ¬¡å¤„ç†ä¸€æ‰¹åˆ—ã€‚
- en: For each column in a batch, it quantizes the weights, calculates the error,
    and updates the weights in the block accordingly.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªæ‰¹æ¬¡ä¸­çš„æ¯ä¸€åˆ—ï¼Œå®ƒå¯¹æƒé‡è¿›è¡Œé‡åŒ–ï¼Œè®¡ç®—è¯¯å·®ï¼Œå¹¶ç›¸åº”åœ°æ›´æ–°å—ä¸­çš„æƒé‡ã€‚
- en: After processing the batch, it updates all remaining weights based on the blockâ€™s
    errors.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¤„ç†æ‰¹æ¬¡åï¼Œå®ƒä¼šæ ¹æ®å—çš„é”™è¯¯æ›´æ–°æ‰€æœ‰å‰©ä½™çš„æƒé‡ã€‚
- en: The GPTQ algorithm was tested on various language generation tasks. It was compared
    with other quantization methods, like rounding all weights to the nearest quantized
    value (RTN). GPTQ was used with the BLOOM (176B parameters) and OPT (175B parameters)
    model families, and models were quantized using a **single NVIDIA A100 GPU**.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: GPTQç®—æ³•åœ¨å„ç§è¯­è¨€ç”Ÿæˆä»»åŠ¡ä¸­è¿›è¡Œäº†æµ‹è¯•ã€‚å®ƒä¸å…¶ä»–é‡åŒ–æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒï¼Œæ¯”å¦‚å°†æ‰€æœ‰æƒé‡èˆå…¥åˆ°æœ€è¿‘çš„é‡åŒ–å€¼ï¼ˆRTNï¼‰ã€‚GPTQä¸BLOOMï¼ˆ176Bå‚æ•°ï¼‰å’ŒOPTï¼ˆ175Bå‚æ•°ï¼‰æ¨¡å‹ç³»åˆ—ä¸€èµ·ä½¿ç”¨ï¼Œå¹¶ä¸”æ¨¡å‹æ˜¯ä½¿ç”¨**å•ä¸ªNVIDIA
    A100 GPU**è¿›è¡Œé‡åŒ–çš„ã€‚
- en: ğŸ’» Quantize an LLM with AutoGPTQ
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ’» ä½¿ç”¨AutoGPTQå¯¹LLMè¿›è¡Œé‡åŒ–
- en: GPTQ has been very popular to create models in 4-bit precision that can efficiently
    run on GPUs. You can find many examples on the Hugging Face Hub, especially from
    [TheBloke](https://huggingface.co/TheBloke). If youâ€™re looking for an approach
    that is more CPU-friendly, [GGML](https://github.com/ggerganov/ggml) is currently
    your best option. Finally, the `transformers` library with `bitsandbytes` allows
    you to quantize a model when it's loaded using the `load_in_4bit=true` argument,
    which requires downloading full models and storing them in your RAM.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: GPTQåœ¨åˆ›å»ºå¯ä»¥é«˜æ•ˆè¿è¡ŒäºGPUä¸Šçš„4ä½ç²¾åº¦æ¨¡å‹æ–¹é¢éå¸¸å—æ¬¢è¿ã€‚ä½ å¯ä»¥åœ¨Hugging Face Hubä¸Šæ‰¾åˆ°è®¸å¤šç¤ºä¾‹ï¼Œç‰¹åˆ«æ˜¯æ¥è‡ª[TheBloke](https://huggingface.co/TheBloke)ã€‚å¦‚æœä½ åœ¨å¯»æ‰¾æ›´é€‚åˆCPUçš„æ–¹æ¡ˆï¼Œ[GGML](https://github.com/ggerganov/ggml)ç›®å‰æ˜¯ä½ çš„æœ€ä½³é€‰æ‹©ã€‚æœ€åï¼Œ`transformers`åº“ä¸`bitsandbytes`å…è®¸ä½ åœ¨åŠ è½½æ¨¡å‹æ—¶é€šè¿‡`load_in_4bit=true`å‚æ•°è¿›è¡Œé‡åŒ–ï¼Œè¿™éœ€è¦ä¸‹è½½å®Œæ•´çš„æ¨¡å‹å¹¶å°†å…¶å­˜å‚¨åœ¨ä½ çš„RAMä¸­ã€‚
- en: Letâ€™s implement the GPTQ algorithm using the AutoGPTQ library and quantize a
    GPT-2 model. This requires a GPU, but a free T4 on Google Colab will do. We start
    by loading the libraries and defining the model we want to quantize (in this case,
    GPT-2).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä½¿ç”¨AutoGPTQåº“å®ç°GPTQç®—æ³•ï¼Œå¹¶å¯¹GPT-2æ¨¡å‹è¿›è¡Œé‡åŒ–ã€‚è¿™éœ€è¦ä¸€ä¸ªGPUï¼Œä½†Google Colabä¸Šçš„å…è´¹T4å°±è¶³å¤Ÿäº†ã€‚æˆ‘ä»¬ä»åŠ è½½åº“å’Œå®šä¹‰è¦é‡åŒ–çš„æ¨¡å‹ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹æ˜¯GPT-2ï¼‰å¼€å§‹ã€‚
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We now want to load the model and the tokenizer. The tokenizer is loaded using
    the classic `AutoTokenizer` class from the `transformers` library. On the other
    hand, we need to pass a specific configuration (`BaseQuantizeConfig`) to load
    the model.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨è¦åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚åˆ†è¯å™¨ä½¿ç”¨`transformers`åº“ä¸­çš„ç»å…¸`AutoTokenizer`ç±»åŠ è½½ã€‚å¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬éœ€è¦ä¼ é€’ä¸€ä¸ªç‰¹å®šçš„é…ç½®ï¼ˆ`BaseQuantizeConfig`ï¼‰æ¥åŠ è½½æ¨¡å‹ã€‚
- en: 'In this configuration, we can specify the number of bits to quantize (here,
    `bits=4`) and the group size (size of the lazy batch). Note that this group size
    is optional: we could also use **one set of parameters** for the entire weight
    matrix. In practice, these groups generally improve the quality of the quantization
    at a very low cost (especially with `group_size=1024`). The `damp_percent` value
    is here to help the Cholesky reformulation and should not be changed.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ­¤é…ç½®ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‡å®šè¦é‡åŒ–çš„ä½æ•°ï¼ˆè¿™é‡Œæ˜¯`bits=4`ï¼‰å’Œç»„å¤§å°ï¼ˆæ‡’æ‰¹æ¬¡çš„å¤§å°ï¼‰ã€‚è¯·æ³¨æ„ï¼Œè¿™ä¸ªç»„å¤§å°æ˜¯å¯é€‰çš„ï¼šæˆ‘ä»¬ä¹Ÿå¯ä»¥ä¸ºæ•´ä¸ªæƒé‡çŸ©é˜µä½¿ç”¨**ä¸€ç»„å‚æ•°**ã€‚å®é™…ä¸Šï¼Œè¿™äº›ç»„é€šå¸¸èƒ½ä»¥éå¸¸ä½çš„æˆæœ¬æé«˜é‡åŒ–çš„è´¨é‡ï¼ˆç‰¹åˆ«æ˜¯ä½¿ç”¨`group_size=1024`æ—¶ï¼‰ã€‚`damp_percent`å€¼ç”¨äºå¸®åŠ©Choleskyé‡æ•´åŒ–ï¼Œä¸åº”æ›´æ”¹ã€‚
- en: Finally, the `desc_act` (also called act order) is a tricky parameter. It allows
    you to **process rows based on decreasing activation**, meaning the most important
    or impactful rows (determined by sampled inputs and outputs) are processed first.
    This method aims to place most of the quantization error (inevitably introduced
    during quantization) on less significant weights. This approach improves the overall
    accuracy of the quantization process by ensuring the most significant weights
    are processed with greater precision. However, when used alongside group size,
    `desc_act` can lead to performance slowdowns due to the need to frequently reload
    quantization parameters. For this reason, we won't use it here (it will probably
    be fixed in the future, however).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œ`desc_act`ï¼ˆä¹Ÿç§°ä¸ºact orderï¼‰æ˜¯ä¸€ä¸ªæ£˜æ‰‹çš„å‚æ•°ã€‚å®ƒå…è®¸ä½ **æ ¹æ®é€’å‡çš„æ¿€æ´»å¤„ç†è¡Œ**ï¼Œæ„å‘³ç€æœ€é‡è¦æˆ–å½±å“æœ€å¤§çš„è¡Œï¼ˆç”±é‡‡æ ·çš„è¾“å…¥å’Œè¾“å‡ºå†³å®šï¼‰é¦–å…ˆå¤„ç†ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨å°†å¤§éƒ¨åˆ†é‡åŒ–è¯¯å·®ï¼ˆä¸å¯é¿å…åœ°åœ¨é‡åŒ–è¿‡ç¨‹ä¸­å¼•å…¥ï¼‰æ”¾åœ¨ä¸å¤ªé‡è¦çš„æƒé‡ä¸Šã€‚é€šè¿‡ç¡®ä¿æœ€é‡è¦çš„æƒé‡ä»¥æ›´é«˜çš„ç²¾åº¦å¤„ç†ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥æé«˜é‡åŒ–è¿‡ç¨‹çš„æ•´ä½“å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œå½“ä¸ç»„å¤§å°ä¸€èµ·ä½¿ç”¨æ—¶ï¼Œ`desc_act`å¯èƒ½ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œå› ä¸ºéœ€è¦é¢‘ç¹é‡æ–°åŠ è½½é‡åŒ–å‚æ•°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œä¸ä¼šä½¿ç”¨å®ƒï¼ˆä¸è¿‡å®ƒå¯èƒ½ä¼šåœ¨æœªæ¥å¾—åˆ°ä¿®å¤ï¼‰ã€‚
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The quantization process **relies heavily on samples** to evaluate and enhance
    the quality of the quantization. They provide a means of comparison between the
    outputs produced by the origina and the newly quantized model. The larger the
    number of samples provided, the greater the potential for more accurate and effective
    comparisons, leading to improved quantization quality.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: é‡åŒ–è¿‡ç¨‹**ä¸¥é‡ä¾èµ–æ ·æœ¬**æ¥è¯„ä¼°å’Œæå‡é‡åŒ–è´¨é‡ã€‚æ ·æœ¬æä¾›äº†ä¸€ä¸ªæ¯”è¾ƒåŸå§‹æ¨¡å‹å’Œæ–°é‡åŒ–æ¨¡å‹è¾“å‡ºçš„æ–¹æ³•ã€‚æä¾›çš„æ ·æœ¬æ•°é‡è¶Šå¤šï¼Œæ¯”è¾ƒçš„æ½œåŠ›å°±è¶Šå¤§ï¼Œä»è€Œæé«˜é‡åŒ–è´¨é‡ã€‚
- en: In the context of this article, we utilize the [**C4 (Colossal Clean Crawled
    Corpus) dataset**](https://huggingface.co/datasets/c4) to generate our samples.
    The C4 dataset is a large-scale, multilingual collection of web text gathered
    from the Common Crawl project. This expansive dataset has been cleaned and prepared
    specifically for training large-scale language models, making it a great resource
    for tasks such as this. The WikiText dataset is another popular option.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡çš„èƒŒæ™¯ä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†[**C4 (Colossal Clean Crawled Corpus) æ•°æ®é›†**](https://huggingface.co/datasets/c4)æ¥ç”Ÿæˆæ ·æœ¬ã€‚C4æ•°æ®é›†æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€å¤šè¯­è¨€çš„ç½‘é¡µæ–‡æœ¬é›†åˆï¼Œæ¥è‡ªCommon
    Crawlé¡¹ç›®ã€‚è¿™ä¸ªåºå¤§çš„æ•°æ®é›†å·²ç»è¢«æ¸…ç†å’Œå‡†å¤‡å¥½ï¼Œä¸“é—¨ç”¨äºè®­ç»ƒå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼Œä½¿å…¶æˆä¸ºç±»ä¼¼ä»»åŠ¡çš„æä½³èµ„æºã€‚WikiTextæ•°æ®é›†æ˜¯å¦ä¸€ä¸ªå—æ¬¢è¿çš„é€‰æ‹©ã€‚
- en: In the following code block, we load 1024 samples from the C4 dataset, tokenize
    them, and format them.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»¥ä¸‹ä»£ç å—ä¸­ï¼Œæˆ‘ä»¬ä»C4æ•°æ®é›†ä¸­åŠ è½½1024ä¸ªæ ·æœ¬ï¼Œå¯¹å…¶è¿›è¡Œåˆ†è¯å’Œæ ¼å¼åŒ–ã€‚
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now that dataset is ready, we can start the quantization process with a batch
    size of 1\. Optionally, we also use [OpenAI Triton](https://github.com/openai/triton),
    a CUDA alternative, to communicate with the GPU. Once this is done, we save the
    tokenizer and the model in a safetensors format.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ•°æ®é›†å·²ç»å‡†å¤‡å¥½ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹é‡åŒ–è¿‡ç¨‹ï¼Œæ‰¹é‡å¤§å°ä¸º1ã€‚å¯é€‰åœ°ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨äº†[OpenAI Triton](https://github.com/openai/triton)ï¼Œè¿™æ˜¯CUDAçš„æ›¿ä»£æ–¹æ¡ˆï¼Œç”¨äºä¸GPUè¿›è¡Œé€šä¿¡ã€‚ä¸€æ—¦å®Œæˆï¼Œæˆ‘ä»¬å°†åˆ†è¯å™¨å’Œæ¨¡å‹ä¿å­˜ä¸ºsafetensorsæ ¼å¼ã€‚
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As per usual, the model and tokenizer can then be loaded from the output directory
    using the `AutoGPTQForCausalLM` and `AutoTokenizer` classes.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: åƒå¾€å¸¸ä¸€æ ·ï¼Œæ¨¡å‹å’Œåˆ†è¯å™¨å¯ä»¥ä»è¾“å‡ºç›®å½•ä¸­åŠ è½½ï¼Œä½¿ç”¨`AutoGPTQForCausalLM`å’Œ`AutoTokenizer`ç±»ã€‚
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Letâ€™s check that the model is working correctly. The AutoGPTQ model (mostly)
    works as a normal `transformers` model, which makes it compatible with inference
    pipelines, as shown in the following example:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚AutoGPTQæ¨¡å‹ï¼ˆå¤§å¤šæ•°æƒ…å†µä¸‹ï¼‰ä½œä¸ºä¸€ä¸ªæ™®é€šçš„`transformers`æ¨¡å‹å·¥ä½œï¼Œè¿™ä½¿å¾—å®ƒä¸æ¨ç†ç®¡é“å…¼å®¹ï¼Œå¦‚ä»¥ä¸‹ç¤ºä¾‹æ‰€ç¤ºï¼š
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We managed to get a convincing completion from our quantized GPT-2 model. A
    more in-depth evaluation would require **measuring the perplexity** of the quantized
    model versus the original one. However, we will leave it out of the scope of this
    article.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æˆåŠŸåœ°ä»é‡åŒ–åçš„GPT-2æ¨¡å‹ä¸­è·å¾—äº†ä»¤äººä¿¡æœçš„ç»“æœã€‚æ›´æ·±å…¥çš„è¯„ä¼°å°†éœ€è¦**æµ‹é‡é‡åŒ–æ¨¡å‹ä¸åŸå§‹æ¨¡å‹çš„å›°æƒ‘åº¦**ã€‚ä¸è¿‡ï¼Œè¿™è¶…å‡ºäº†æœ¬æ–‡çš„èŒƒå›´ã€‚
- en: Conclusion
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: In this article, we introduced the GPTQ algorithm, a state-of-the-art quantization
    technique to run LLMs on consumer-grade hardware. We showed how it addresses the
    layer-wise compression problem, based on an improved OBS technique with arbitrary
    order insight, lazy batch updates, and Cholesky reformulation. This novel approach
    **significantly reduces memory and computation requirements**, making LLMs accessible
    to a broader audience.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†GPTQç®—æ³•ï¼Œè¿™æ˜¯ä¸€ç§æœ€å…ˆè¿›çš„é‡åŒ–æŠ€æœ¯ï¼Œç”¨äºåœ¨æ¶ˆè´¹è€…çº§ç¡¬ä»¶ä¸Šè¿è¡ŒLLMsã€‚æˆ‘ä»¬å±•ç¤ºäº†å®ƒå¦‚ä½•è§£å†³åŸºäºæ”¹è¿›çš„OBSæŠ€æœ¯çš„å±‚çº§å‹ç¼©é—®é¢˜ï¼Œç»“åˆäº†ä»»æ„é¡ºåºæ´å¯Ÿã€æƒ°æ€§æ‰¹é‡æ›´æ–°å’ŒCholeskyé‡æ„ã€‚è¿™ç§æ–°é¢–çš„æ–¹æ³•**æ˜¾è‘—å‡å°‘äº†å†…å­˜å’Œè®¡ç®—éœ€æ±‚**ï¼Œä½¿LLMså¯ä»¥è¢«æ›´å¹¿æ³›çš„å—ä¼—ä½¿ç”¨ã€‚
- en: 'In addition, we **quantized our own LLM model** on a free T4 GPU and ran it
    to generate text. You can push your own version of a GPTQ 4-bit quantized model
    on the Hugging Face Hub. As mentioned in the introduction, GPTQ is not the only
    4-bit quantization algorithm: [GGML](https://github.com/ggerganov/ggml) and [NF4](https://huggingface.co/blog/4bit-transformers-bitsandbytes)
    are excellent alternatives with slightly different scopes. I encourage you to
    learn more about them and give them a shot!'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨å…è´¹çš„T4 GPUä¸Š**é‡åŒ–äº†æˆ‘ä»¬è‡ªå·±çš„LLMæ¨¡å‹**å¹¶è¿è¡Œå®ƒä»¥ç”Ÿæˆæ–‡æœ¬ã€‚ä½ å¯ä»¥åœ¨Hugging Face Hubä¸Šå‘å¸ƒä½ è‡ªå·±çš„GPTQ 4-bité‡åŒ–æ¨¡å‹ã€‚æ­£å¦‚å¼•è¨€ä¸­æåˆ°çš„ï¼ŒGPTQå¹¶ä¸æ˜¯å”¯ä¸€çš„4-bité‡åŒ–ç®—æ³•ï¼š[GGML](https://github.com/ggerganov/ggml)å’Œ[NF4](https://huggingface.co/blog/4bit-transformers-bitsandbytes)æ˜¯å…·æœ‰ç¨å¾®ä¸åŒèŒƒå›´çš„ä¼˜ç§€æ›¿ä»£æ–¹æ¡ˆã€‚æˆ‘é¼“åŠ±ä½ äº†è§£æ›´å¤šï¼Œå¹¶å°è¯•ä¸€ä¸‹ï¼
- en: If youâ€™re interested in more technical content around LLMs, follow me on Twitter
    [@maximelabonne](https://twitter.com/maximelabonne).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å¯¹æ›´å¤šå…³äºLLMsçš„æŠ€æœ¯å†…å®¹æ„Ÿå…´è¶£ï¼Œå¯ä»¥åœ¨Twitterä¸Šå…³æ³¨æˆ‘ [@maximelabonne](https://twitter.com/maximelabonne)ã€‚
- en: References
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: 'B. Hassibi, D. G. Stork and G. J. Wolff, [â€œOptimal Brain Surgeon and general
    network pruning,â€](https://ieeexplore.ieee.org/document/298572) IEEE International
    Conference on Neural Networks, San Francisco, CA, USA, 1993, pp. 293â€“299 vol.1,
    doi: 10.1109/ICNN.1993.298572.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'B. Hassibi, D. G. Stork å’Œ G. J. Wolff, [**â€œä¼˜åŒ–è„‘å¤–ç§‘åŒ»ç”Ÿå’Œé€šç”¨ç½‘ç»œä¿®å‰ªâ€**](https://ieeexplore.ieee.org/document/298572)ï¼ŒIEEE
    å›½é™…ç¥ç»ç½‘ç»œå¤§ä¼šï¼Œæ—§é‡‘å±±ï¼Œç¾å›½ï¼Œ1993å¹´ï¼Œé¡µç  293â€“299ï¼Œdoi: 10.1109/ICNN.1993.298572ã€‚'
- en: 'Elias Frantar, Sidak Pal Singh, & Dan Alistarh. (2023). [Optimal Brain Compression:
    A Framework for Accurate Post-Training Quantization and Pruning](https://arxiv.org/abs/2208.11580).'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elias Frantar, Sidak Pal Singh å’Œ Dan Alistarh. (2023). [**ä¼˜åŒ–è„‘å‹ç¼©ï¼šå‡†ç¡®çš„è®­ç»ƒåé‡åŒ–å’Œä¿®å‰ªæ¡†æ¶**](https://arxiv.org/abs/2208.11580)ã€‚
- en: 'Elias Frantar, Saleh Ashkboos, Torsten Hoefler, & Dan Alistarh. (2023). [GPTQ:
    Accurate Post-Training Quantization for Generative Pre-trained Transformers](https://arxiv.org/abs/2210.17323).'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elias Frantar, Saleh Ashkboos, Torsten Hoefler å’Œ Dan Alistarh. (2023). [**GPTQï¼šç”Ÿæˆé¢„è®­ç»ƒå˜æ¢å™¨çš„å‡†ç¡®è®­ç»ƒåé‡åŒ–**](https://arxiv.org/abs/2210.17323)ã€‚
- en: Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
    Matena, Yanqi Zhou, Wei Li, & Peter J. Liu. (2020). [Exploring the Limits of Transfer
    Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683v3).
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
    Matena, Yanqi Zhou, Wei Li å’Œ Peter J. Liu. (2020). [**æ¢ç´¢ç»Ÿä¸€çš„æ–‡æœ¬åˆ°æ–‡æœ¬å˜æ¢å™¨çš„è¿ç§»å­¦ä¹ æé™**](https://arxiv.org/abs/1910.10683v3)ã€‚
- en: Related articles
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç›¸å…³æ–‡ç« 
- en: '[](/introduction-to-weight-quantization-2494701b9c0c?source=post_page-----36b0f4f02c34--------------------------------)
    [## Introduction to Weight Quantization'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/introduction-to-weight-quantization-2494701b9c0c?source=post_page-----36b0f4f02c34--------------------------------)
    [## **æƒé‡é‡åŒ–ç®€ä»‹**](https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c?source=post_page-----36b0f4f02c34--------------------------------)'
- en: Reducing the size of Large Language Models with 8-bit quantization
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ 8 ä½é‡åŒ–å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤§å°
- en: towardsdatascience.com](/introduction-to-weight-quantization-2494701b9c0c?source=post_page-----36b0f4f02c34--------------------------------)
    [](/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32?source=post_page-----36b0f4f02c34--------------------------------)
    [## Fine-Tune Your Own Llama 2 Model in a Colab Notebook
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/introduction-to-weight-quantization-2494701b9c0c?source=post_page-----36b0f4f02c34--------------------------------)
    [](/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32?source=post_page-----36b0f4f02c34--------------------------------)
    [## **åœ¨ Colab Notebook ä¸­å¾®è°ƒä½ çš„ Llama 2 æ¨¡å‹**](https://towardsdatascience.com/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32?source=post_page-----36b0f4f02c34--------------------------------)'
- en: A practical introduction to LLM fine-tuning
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å…³äº LLM å¾®è°ƒçš„å®ç”¨ä»‹ç»
- en: towardsdatascience.com](/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32?source=post_page-----36b0f4f02c34--------------------------------)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32?source=post_page-----36b0f4f02c34--------------------------------)'
- en: '*Learn more about machine learning and support my work with one click â€” become
    a Medium member here:*'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*äº†è§£æ›´å¤šå…³äºæœºå™¨å­¦ä¹ çš„å†…å®¹ï¼Œå¹¶é€šè¿‡ä¸€é”®æ”¯æŒæˆ‘çš„å·¥ä½œ â€” ç°åœ¨æˆä¸º Medium ä¼šå‘˜ï¼š*'
- en: '[](https://medium.com/@mlabonne/membership?source=post_page-----36b0f4f02c34--------------------------------)
    [## Join Medium with my referral link â€” Maxime Labonne'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne/membership?source=post_page-----36b0f4f02c34--------------------------------)
    [## **é€šè¿‡æˆ‘çš„æ¨èé“¾æ¥åŠ å…¥ Medium â€” Maxime Labonne**](https://medium.com/@mlabonne/membership?source=post_page-----36b0f4f02c34--------------------------------)'
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every storyâ€¦
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½œä¸º Medium ä¼šå‘˜ï¼Œä½ çš„ä¼šå‘˜è´¹ç”¨çš„ä¸€éƒ¨åˆ†å°†ä¼šåˆ†é…ç»™ä½ é˜…è¯»çš„ä½œè€…ï¼Œå¹¶ä¸”ä½ å¯ä»¥å…¨é¢è®¿é—®æ¯ä¸ªæ•…äº‹â€¦â€¦
- en: medium.com](https://medium.com/@mlabonne/membership?source=post_page-----36b0f4f02c34--------------------------------)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/@mlabonne/membership?source=post_page-----36b0f4f02c34--------------------------------)'
- en: '*If youâ€™re already a member, you can* [*follow me on Medium*](https://medium.com/@mlabonne)*.*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¦‚æœä½ å·²ç»æ˜¯ä¼šå‘˜ï¼Œä½ å¯ä»¥* [*åœ¨ Medium ä¸Šå…³æ³¨æˆ‘*](https://medium.com/@mlabonne)*ã€‚*'
