- en: What does Entropy Measure? An Intuitive Explanation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/what-does-entropy-measure-an-intuitive-explanation-a7f7e5d16421?source=collection_archive---------1-----------------------#2023-01-04](https://towardsdatascience.com/what-does-entropy-measure-an-intuitive-explanation-a7f7e5d16421?source=collection_archive---------1-----------------------#2023-01-04)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://tim-lou.medium.com/?source=post_page-----a7f7e5d16421--------------------------------)[![Tim
    Lou, PhD](../Images/e4931bb6d59e27730529ceaf00a23822.png)](https://tim-lou.medium.com/?source=post_page-----a7f7e5d16421--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a7f7e5d16421--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a7f7e5d16421--------------------------------)
    [Tim Lou, PhD](https://tim-lou.medium.com/?source=post_page-----a7f7e5d16421--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8d41b438feef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-does-entropy-measure-an-intuitive-explanation-a7f7e5d16421&user=Tim+Lou%2C+PhD&userId=8d41b438feef&source=post_page-8d41b438feef----a7f7e5d16421---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a7f7e5d16421--------------------------------)
    ·11 min read·Jan 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa7f7e5d16421&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-does-entropy-measure-an-intuitive-explanation-a7f7e5d16421&user=Tim+Lou%2C+PhD&userId=8d41b438feef&source=-----a7f7e5d16421---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa7f7e5d16421&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-does-entropy-measure-an-intuitive-explanation-a7f7e5d16421&source=-----a7f7e5d16421---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Entropy may seem abstract, but it has an intuitive side: as the probability
    of seeing certain patterns in data. Here’s how it works.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4daae7bdad8d3ebc360bb1ef70760dd8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Background Credit: Joe Maldonado [@unsplash](https://unsplash.com/@joesracingteam)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In data science, there are many concepts linked to the notion of entropy. The
    most basic one is Shannon’s information entropy, defined for any distribution,
    *P*(*x*), through the formula:'
  prefs: []
  type: TYPE_NORMAL
- en: Where the sum is over all the possible categories in *C*.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are other related concepts that have similarly looking formulae:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Kullback–Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence):
    for comparing two distributions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Mutual information](https://en.wikipedia.org/wiki/Mutual_information): for
    capturing general relationships between two variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Cross entropy](https://en.wikipedia.org/wiki/Cross_entropy): for training
    classification models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Despite the ubiquity of entropy-like formulae, there are rarely discussions
    on the intuitions behind the formula: Why is the logarithm involved? Why are we
    multiplying *P*(*x*) and log *P*(*x*)? While many articles mention terms like
    “information”, “expected surprise”, the intuitions behind them are missing.'
  prefs: []
  type: TYPE_NORMAL
- en: It turns out, just like probabilities, entropy can be understood through a counting
    exercise, and it can be linked to a sort of log-likelihood for distributions.
    Furthermore, this counting can be linked to the literal number of bytes in a computer.
    These…
  prefs: []
  type: TYPE_NORMAL
