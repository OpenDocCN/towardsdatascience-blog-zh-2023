["```py\n# image_encoder - ResNet or Vision Transformer\n# text_encoder - CBOW or Text Transformer\n# I[n, h, w, c] - minibatch of aligned images\n# T[n, l] - minibatch of aligned texts\n# W_i[d_i, d_e] - learned proj of image to embed\n# W_t[d_t, d_e] - learned proj of text to embed\n# t - learned temperature parameter\n# extract feature representations of each modality\nI_f = image_encoder(I) #[n, d_i]\nT_f = text_encoder(T) #[n, d_t]\n# joint multimodal embedding [n, d_e]\nI_e = l2_normalize(np.dot(I_f, W_i), axis=1)\nT_e = l2_normalize(np.dot(T_f, W_t), axis=1)\n# scaled pairwise cosine similarities [n, n]\nlogits = np.dot(I_e, T_e.T) * np.exp(t)\n# symmetric loss function\nlabels = np.arange(n)\nloss_i = cross_entropy_loss(logits, labels, axis=0)\nloss_t = cross_entropy_loss(logits, labels, axis=1)\nloss = (loss_i + loss_t)/2\n```", "```py\nI_f = models.resnet34(pretrained=True)      # for encoding images\nT_f= AutoModel.from_pretrained(\"distilbert-base-multilingual-cased\") # for encoding captions\n```", "```py\nclass Projection(nn.Module):\n    def __init__(self, d_in: int, d_out: int, p: float=0.5) -> None:\n        super().__init__()\n        self.linear1 = nn.Linear(d_in, d_out, bias=False)\n        self.linear2 = nn.Linear(d_out, d_out, bias=False)\n        self.layer_norm = nn.LayerNorm(d_out)\n        self.drop = nn.Dropout(p)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        embed1 = self.linear1(x)\n        embed2 = self.drop(self.linear2(F.gelu(embed1)))\n        embeds = self.layer_norm(embed1 + embed2)\n        return embeds\n```", "```py\nclass VisionEncoder(nn.Module):\n    def __init__(self, d_out: int) -> None:\n        super().__init__()\n        base = models.resnet34(pretrained=True)\n        d_in = base.fc.in_features\n        base.fc = nn.Identity()\n        self.base = base\n        self.projection = Projection(d_in, d_out)\n        for p in self.base.parameters():\n            p.requires_grad = False\n\n    def forward(self, x):\n        projected_vec = self.projection(self.base(x))\n        projection_len = torch.norm(projected_vec, dim=-1, keepdim=True)\n        return projected_vec / projection_len\n\nclass TextEncoder(nn.Module):\n    def __init__(self, d_out: int) -> None:\n        super().__init__()\n        self.base = AutoModel.from_pretrained(Config.text_model)\n        self.projection = Projection(Config.transformer_embed_dim, d_out)\n        for p in self.base.parameters():\n            p.requires_grad = False\n\n    def forward(self, x):\n        out = self.base(x)[0]\n        out = out[:, 0, :]  # get CLS token output\n        projected_vec = self.projection(out)\n        projection_len = torch.norm(projected_vec, dim=-1, keepdim=True)\n        return projected_vec / projection_len\n\nvision_encoder = VisionEncoder(Config.embed_dim)\nI_e = vision_encoder(images)\ncaption_encoder = TextEncoder(Config.embed_dim)        \nT_e = caption_encoder(text[\"input_ids\"])\n```", "```py\nlogits = I_e @ T_e.T\n```", "```py\ndef CLIP_loss(logits: torch.Tensor) -> torch.Tensor:\n    n = logits.shape[1]      # number of samples\n    labels = torch.arange(n) # Create labels tensor\n    # Calculate cross entropy losses along axis 0 and 1\n    loss_i = F.cross_entropy(logits.transpose(0, 1), labels, reduction=\"mean\")\n    loss_t = F.cross_entropy(logits, labels, reduction=\"mean\")\n    # Calculate the final loss\n    loss = (loss_i + loss_t) / 2\n\n    return loss\n```", "```py\nclass CustomModel(nn.Module):\n    def __init__(self, lr: float = 1e-3) -> None:\n        super().__init__()\n        self.vision_encoder = VisionEncoder(Config.embed_dim)\n        self.caption_encoder = TextEncoder(Config.embed_dim)\n        self.tokenizer = Tokenizer(AutoTokenizer.from_pretrained(Config.text_model))\n        self.lr = lr\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    def forward(self, images, text):\n        text = self.tokenizer(text).to(self.device)\n\n        image_embed = self.vision_encoder(images)\n        caption_embed = self.caption_encoder(text[\"input_ids\"])\n        similarity = caption_embed @ image_embed.T\n\n        loss = CLIP_loss(similarity)\n        img_acc, cap_acc = metrics(similarity)\n        return loss, img_acc, cap_acc\n```", "```py\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom torchvision import transforms\nfrom PIL import Image\nimport torch\nfrom torchvision import transforms\nfrom PIL import Image\n# Define a custom dataset class for Flickr30k\nclass Flickr30kDataset(torch.utils.data.Dataset):\n    def __init__(self):\n        self.dataset = load_dataset(\"nlphuji/flickr30k\", cache_dir=\"./huggingface_data\")\n        self.transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n        ])\n        self.cap_per_image = 2\n\n    def __len__(self):\n        return self.dataset.num_rows[\"test\"] * self.cap_per_image\n\n    def __getitem__(self, idx):\n        original_idx = idx // self.cap_per_image\n        image = self.dataset[\"test\"][original_idx][\"image\"].convert(\"RGB\")\n        image = self.transform(image)\n\n        # labels\n        caption = self.dataset[\"test\"][original_idx][\"caption\"][idx % self.cap_per_image]\n\n        return {\"image\": image, \"caption\": caption}\n\n# Create an instance of the custom dataset\nflickr30k_custom_dataset = Flickr30kDataset()\n```", "```py\nfrom dataclasses import dataclass\n\n@dataclass\nclass Config:\n    \"\"\"\n    Configuration class for the CLIP training script.\n    \"\"\"\n\n    embed_dim: int = 512  # Embedding dimension\n    transformer_embed_dim: int = 768  # Transformer embedding dimension\n    max_len: int = 32  # Maximum text length\n    text_model: str = \"distilbert-base-multilingual-cased\"  # Text model name\n    epochs: int = 3 # Number of training epochs\n    batch_size: int = 128 # Batch size\n```", "```py\n# Create the DataLoader\nclip_dataloader = DataLoader(flickr30k_custom_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\n# Create an iterator from the dataloader\ndata_iter = iter(clip_dataloader)\n\n# Get one batch\nbatch = next(data_iter)\n\nimage = batch[\"image\"][0]  # get one image from the batch\ncaption = batch[\"caption\"][0]  # get one text from the batch\n\n# Convert the image tensor to a NumPy array and permute dimensions\nimage_np = np.transpose(image.numpy(), (1, 2, 0))\n\n# Display the image and caption\nplt.imshow(image_np)\nplt.title(f\"Caption: {caption}\")\nplt.show()\n```", "```py\n# Create an instance of your model\nmodel = CustomModel().to(device)\n\n# Define optimizer\noptimizer = torch.optim.Adam([\n    {'params': model.vision_encoder.parameters()},\n    {'params': model.caption_encoder.parameters()}\n], lr=model.lr)\n```", "```py\nbatch_zero = True\nfor epoch in range(start_epoch, num_epochs):\n    model.train()\n    for batch in clip_dataloader:\n        image = batch[\"image\"].to(device)\n        text = batch[\"caption\"]\n        # images, text = batch\n        loss, img_acc, cap_acc = model.common_step((image, text))\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch_zero:\n          print(f\"Epoch [{0}/{num_epochs}], Batch Loss: {loss.item()}\")\n          batch_zero = False\n\n    # Print training statistics\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Batch Loss: {loss.item()}\")\n\nprint(\"Training complete.\")\n```", "```py\nEpoch [0/3], Batch Loss: 4.854558944702148\nEpoch [1/3], Batch Loss: 3.187166690826416\nEpoch [2/3], Batch Loss: 3.0981950759887695\nEpoch [3/3], Batch Loss: 3.164858818054199\nTraining complete.\n```", "```py\nEpoch [0/3], Batch Loss: 4.852224349975586\nEpoch [1/3], Batch Loss: 2.7819151878356934\nEpoch [2/3], Batch Loss: 2.727229118347168\nEpoch [3/3], Batch Loss: 2.717097759246826\nTraining complete.\n```"]