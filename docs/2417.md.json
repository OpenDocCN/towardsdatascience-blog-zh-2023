["```py\nimport torch.nn as nn\n\nvocabulary_size = 2\nnum_dimensions_per_word = 2\n\nembds = nn.Embedding(vocabulary_size, num_dimensions_per_word)\n\nprint(embds.weight)\n---------------------\noutput:\nParameter containing:\ntensor([[-1.5218, -2.5683],\n        [-0.6769, -0.7848]], requires_grad=True)\n```", "```py\nvocabulary_size = 10_000\nnum_dimensions_per_word = 512\n\nembds = nn.Embedding(vocabulary_size, num_dimensions_per_word)\n\nprint(embds)\n---------------------\noutput:\nEmbedding(10000, 512)\n```", "```py\nfrom math import sin, cos\nmax_seq_len = 50 \nnumber_of_model_dimensions = 512\n\npositions_vector = np.zeros((max_seq_len, number_of_model_dimensions))\n\nfor position in range(max_seq_len):\n    for index in range(number_of_model_dimensions//2):\n        theta = position / (10000 ** ((2*index)/number_of_model_dimensions))\n        positions_vector[position, 2*index ] = sin(theta)\n        positions_vector[position, 2*index + 1] = cos(theta)\n\nprint(positions_vector)\n---------------------\noutput:\n(50, 512)\n```", "```py\nprint(positions_vector[0][:10])\n---------------------\noutput:\narray([0., 1., 0., 1., 0., 1., 0., 1., 0., 1.])\n```", "```py\nprint(positions_vector[1][:10])\n---------------------\noutput:\narray([0.84147098, 0.54030231, 0.82185619, 0.56969501, 0.8019618 ,\n       0.59737533, 0.78188711, 0.62342004, 0.76172041, 0.64790587])\n```"]