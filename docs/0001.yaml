- en: 'Graph ML in 2023: The State of Affairs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/graph-ml-in-2023-the-state-of-affairs-1ba920cb9232?source=collection_archive---------0-----------------------#2023-01-01](https://towardsdatascience.com/graph-ml-in-2023-the-state-of-affairs-1ba920cb9232?source=collection_archive---------0-----------------------#2023-01-01)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: STATE OF THE ART DIGEST
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hot trends and major advancements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://mgalkin.medium.com/?source=post_page-----1ba920cb9232--------------------------------)[![Michael
    Galkin](../Images/c5eb13334712ca0462d8a5df4a268ad0.png)](https://mgalkin.medium.com/?source=post_page-----1ba920cb9232--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1ba920cb9232--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1ba920cb9232--------------------------------)
    [Michael Galkin](https://mgalkin.medium.com/?source=post_page-----1ba920cb9232--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d4f8ddd1e68&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-ml-in-2023-the-state-of-affairs-1ba920cb9232&user=Michael+Galkin&userId=4d4f8ddd1e68&source=post_page-4d4f8ddd1e68----1ba920cb9232---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1ba920cb9232--------------------------------)
    ¬∑26 min read¬∑Jan 1, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1ba920cb9232&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-ml-in-2023-the-state-of-affairs-1ba920cb9232&user=Michael+Galkin&userId=4d4f8ddd1e68&source=-----1ba920cb9232---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1ba920cb9232&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-ml-in-2023-the-state-of-affairs-1ba920cb9232&source=-----1ba920cb9232---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: 2022 comes to an end and it is about time to sit down and reflect upon the achievements
    made in Graph ML as well as to hypothesize about possible breakthroughs in 2023\.
    Tune in üéÑ‚òï
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f8888b219ca9e135881455e3596c98d7.png)'
  prefs: []
  type: TYPE_IMG
- en: Background image generated by [DALL-E 2](https://openai.com/dall-e-2/), text
    added by Author.
  prefs: []
  type: TYPE_NORMAL
- en: '*The article is written together with* [*Hongyu Ren*](http://hyren.me/) *(Stanford
    University),* [*Zhaocheng Zhu*](https://kiddozhu.github.io/) *(Mila & University
    of Montreal). We thank* [*Christopher Morris*](https://chrsmrrs.github.io/) *and*
    [*Johannes Brandstetter*](https://www.microsoft.com/en-us/research/people/johannesb/)
    *for the feedback and helping with the Theory and PDE sections, respectively.
    Follow* [*Michael*](https://twitter.com/michael_galkin)*,* [*Hongyu*](https://twitter.com/ren_hongyu)*,*
    [*Zhaocheng*](https://twitter.com/zhu_zhaocheng), [*Christopher*](https://twitter.com/chrsmrrs)*,
    and* [*Johannes*](https://twitter.com/jo_brandstetter) *here on Medium and Twitter
    for more graph ml-related discussions.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Table of Contents:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Generative Models: Denoising Diffusion for Molecules and Proteins](#48f6)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[DFTs, ML Force Fields, Materials, and Weather Simulations](#d2e8)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Geometry & Topology & PDEs](#6d20)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Graph Transformers](#8e6c)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[BIG Graphs](#ca19)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[GNN Theory: Weisfeiler and Leman Go Places, Subgraph GNNs](#7986)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Knowledge Graphs: Inductive Reasoning Takes Over](#e5e6)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Algorithmic Reasoning and Alignment](#b2f5)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Cool GNN Applications](#0de4)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Hardware: IPUs and Graphcore win OGB LSC 2022](#b813)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[New Conferences: LoG and Molecular ML](#9b59)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Courses and Educational Materials](#41dc)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[New Datasets, Benchmarks, and Challenges](#3e6d)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Software Libraries and Open Source](#463f)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Join the Community](#1b30)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[The Meme of 2022](#7593)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Generative Models: Denoising Diffusion for Molecules and Proteins'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative diffusion models in the vision-language domain were the headline
    topic in the Deep Learning world in 2022\. While generating images and videos
    is definitely a cool playground to try out different models and sampling techniques,
    we‚Äôd argue that
  prefs: []
  type: TYPE_NORMAL
- en: the most *useful* applications of diffusion models in 2022 were actually created
    in the Geometric Deep Learning area focusing on molecules and proteins
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In our recent article, we were pondering whether [‚ÄúDenoising Diffusion Is All
    You Need?‚Äù](/denoising-diffusion-generative-models-in-graph-ml-c496af5811c5).
  prefs: []
  type: TYPE_NORMAL
- en: '[](/denoising-diffusion-generative-models-in-graph-ml-c496af5811c5?source=post_page-----1ba920cb9232--------------------------------)
    [## Denoising Diffusion Generative Models in Graph ML'
  prefs: []
  type: TYPE_NORMAL
- en: Is Denoising Diffusion all you need?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/denoising-diffusion-generative-models-in-graph-ml-c496af5811c5?source=post_page-----1ba920cb9232--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'There, we reviewed newest generative models for *graph generation* (DiGress),
    *molecular conformer generation* (EDM, GeoDiff, Torsional Diffusion), *molecular
    docking* (DiffDock), *molecular linking* (DiffLinker), and *ligand generation*
    (DiffSBDD). As soon as the post went public, several amazing protein generation
    models were released:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Chroma**](https://www.generatebiomedicines.com/chroma) from Generate Biomedicines
    allows to impose functional and geometric constraints, and even use natural language
    queries like ‚ÄúGenerate a protein with CHAD domain‚Äù thanks to a small GPT-Neo trained
    on protein captioning;'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7e24c508e8b597a6205035e92bd02e6d.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Chroma protein generation. Source:* [*Generate Biomedicines*](https://www.generatebiomedicines.com/chroma)'
  prefs: []
  type: TYPE_NORMAL
- en: '[**RoseTTaFold Diffusion**](https://www.bakerlab.org/2022/11/30/diffusion-model-for-protein-design/)
    (RF Diffusion) from the Baker Lab and MIT is packed with the similar functionality
    also allowing for text prompts like ‚ÄúGenerate a protein that binds to X‚Äù as well
    as being capable of functional motif scaffolding, scaffolding enzyme active sites,
    and *de novo* protein design. Strong point: 1000 designs generated with RF Diffusion
    were experimentally [synthesized and tested](https://twitter.com/DaveJuergens/status/1601675072175239170)
    in the lab!'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d6ce018a9c6d17df43d48a15da42dce6.png)'
  prefs: []
  type: TYPE_IMG
- en: '*RF Diffusion. Source:* [*Watson et al.*](https://www.biorxiv.org/content/10.1101/2022.12.09.519842v1)
    *BakerLab*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Meta AI FAIR team made amazing progress in protein design purely with language
    models: mid-2022, [**ESM-2**](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1)
    was released, a protein LM trained solely on protein sequences that outperforms
    ESM-1 and other baselines by a huge margin. Moreover, it was then shown that encoded
    LM representations are a very good starting point for obtaining the actual geometric
    configuration of a protein without the need for Multiple Sequence Alignments (MSAs)
    ‚Äî this is done via [**ESMFold**](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1).
    A big shoutout to Meta AI and FAIR for publishing the model and the weights: it
    is available in the [official GitHub repo](https://github.com/facebookresearch/esm)
    and [on HuggingFace](https://huggingface.co/models?other=esm) as well!'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6d7f2f02320b2b61008ce8bb1973205d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Scaling ESM-2 leads to better folding prediction. Source: [Lin, Akin, Rao,
    Hie et al](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1)'
  prefs: []
  type: TYPE_NORMAL
- en: 'üç≠ Later on, even more goodies arrived from the ESM team: [Verkuil et al.](https://www.biorxiv.org/content/10.1101/2022.12.21.521521v1)
    find that ESM-2 can generate *de novo* protein sequences that can actually be
    synthesized in the lab and, more importantly, do not have any match among known
    natural proteins. [Hie et al.](https://www.biorxiv.org/content/10.1101/2022.12.21.521526v1)
    propose pretty much a new programming language for protein designers (think of
    it as a query language for ESMFold) ‚Äî production rules organized in a syntax tree
    with constraint functions. Then, each program is ‚Äúcompiled‚Äù into an energy function
    that governs the generative process. Meta AI also released the biggest [Metagenomic
    Atlas](https://esmatlas.com/), but more on that in the **Datasets** section of
    this article.'
  prefs: []
  type: TYPE_NORMAL
- en: In the antibody design area, a similar LM-based approach is taken by **IgLM**
    by [Shuai, Ruffolo, and Gray](https://www.biorxiv.org/content/10.1101/2021.12.13.472419v2).
    IGLM generates antibody sequences conditioned on chain and species id tags.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we‚Äôd highlight a few works from Jian Tang‚Äôs lab at Mila. **MoleculeSTM**
    by [Liu et al.](https://arxiv.org/abs/2212.10789) is a CLIP-like text-to-molecule
    model (plus a new large pre-training dataset). MoleculeSTM can do 2 impressive
    things: (1) retrieve molecules by text description like ‚Äútriazole derivatives‚Äù
    and retrieve text description from a given molecule in SMILES, (2) molecule editing
    from text prompts like ‚Äúmake the molecule soluble in water with low permeability‚Äù
    ‚Äî and the model edits the molecular graph according to the description, mindblowing
    ü§Ø'
  prefs: []
  type: TYPE_NORMAL
- en: Then, **ProtSEED** by [Shi et al.](https://arxiv.org/abs/2210.08761) is a generative
    model for protein sequence *and* structure simultaneously (for example, most existing
    diffusion models for proteins can do only one of those at a time). ProtSEED can
    be conditioned on residue features or pairs of residues. Model-wise, it is an
    equivariant iterative model with improved triangular attention. ProtSEED was evaluated
    on Antibody CDR co-design, Protein sequence-structure co-design, and Fixed backbone
    sequence design.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e54ef61a84ca2aeca7a1b707387881e2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Molecule editing from text inputs. Source: [Liu et al.](https://arxiv.org/abs/2212.10789)'
  prefs: []
  type: TYPE_NORMAL
- en: Besides generating the protein structures, there are also some works for generating
    protein sequences from structures, known as inverse folding. Don‚Äôt forget to check
    out the [ESM-IF1](https://www.biorxiv.org/content/10.1101/2022.04.10.487779v2)
    from Meta and the [ProteinMPNN](https://www.science.org/doi/full/10.1126/science.add2187)
    from the Baker Lab.
  prefs: []
  type: TYPE_NORMAL
- en: '**What to expect in 2023**: (1) performance improvements of diffusion models
    such as faster sampling and more efficient solvers; (2) more powerful conditional
    protein generation models; (3) more successful applications of [Generative Flow
    Networks](https://arxiv.org/abs/2111.09266) (GFlowNets, check out the [tutorial](https://milayb.notion.site/The-GFlowNet-Tutorial-95434ef0e2d94c24aab90e69b30be9b3))
    to molecules and proteins.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**DFTs, ML Force Fields, Materials, and Weather Simulations**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI4Science becomes the frontier of equivariant GNN research and its applications.
    Pairing GNNs with PDEs, we can now tackle much more complex prediction tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In 2022, this frontier expanded to ML-based **Density Functional Theory** (DFT)
    and **Force fields** approximations used for **molecular dynamics** and **material
    discovery.** The other growing field is **Weather simulations**.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We would recommend the [talk](https://www.youtube.com/watch?v=t7q_ZNrBghY) by
    Max Welling for a broader overview of AI4Science and what is now enabled by using
    Deep Learning in science.
  prefs: []
  type: TYPE_NORMAL
- en: Starting with models, 2022 has seen a surge in equivariant GNNs for molecular
    dynamics and simulations, e.g., building upon [NequIP](https://arxiv.org/abs/2101.03164),
    **Allegro** by [Musaelian, Batzner, et al.](https://arxiv.org/abs/2204.05249)
    or **MACE** by [Batatia et al.](https://arxiv.org/abs/2206.07697) The design space
    for such models is very large, so refer to the recent survey by [Batatia, Batzner,
    et al.](https://arxiv.org/abs/2205.06643) for an overview. A crucial component
    for most of them is the [**e3nn**](https://github.com/e3nn/e3nn) library (paper
    by [Geiger and Smidt](https://arxiv.org/abs/2207.09453)) and the notion of tensor
    product. We highly recommend a great [new course](https://uvagedl.github.io/)
    by Erik Bekkers on Group Equivariant Deep Learning to understand the mathematical
    foundations and catch up with the recent papers.
  prefs: []
  type: TYPE_NORMAL
- en: ‚öõÔ∏è **Density Functional Theory** (DFT) calculations are one of the main workhorses
    of molecular dynamics (and account for a great deal of computing time in big clusters).
    DFT is O(n¬≥) to the input size though, so can ML help here? In *Learned Force
    Fields Are Ready For Ground State Catalyst Discovery,* [Schaarschmidt et al.](https://arxiv.org/abs/2209.12466)
    present the experimental study of models of learned potentials ‚Äî turns out GNNs
    can do a very good job in linear O(n) time! The **Easy Potentials** approach (trained
    on Open Catalyst data) turns out to be quite a good predictor especially when
    paired with a postprocessing step. Model-wise, it is an MPNN with the [Noisy Nodes](https://arxiv.org/abs/2106.07971)
    self-supervised objective.
  prefs: []
  type: TYPE_NORMAL
- en: In **Forces are not Enough**, [Fu et al.](https://arxiv.org/abs/2210.07237)
    introduce a new benchmark for molecular dynamics ‚Äî in addition to MD17, the authors
    add datasets on modeling liquids (Water), peptides (Alanine dipeptide), and solid-state
    materials (LiPS). More importantly, the authors consider a wide range of physical
    properties like stability of simulations, diffusivity, and radial distribution
    functions. Most SOTA molecular dynamics models were probed including SchNet, ForceNet,
    DimeNet, GemNet (-T and -dT), and NequIP.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b1ffbec28f6b6c061b8f5f66ff7fdc9e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [Fu et al.](https://arxiv.org/abs/2210.07237)'
  prefs: []
  type: TYPE_NORMAL
- en: In crystal structure modeling, we‚Äôd highlight **Equivariant Crystal Networks**
    by [Kaba and Ravanbakhsh](https://openreview.net/forum?id=0Dh8dz4snu) ‚Äî a neat
    way to build representations of periodic structures with crystalline symmetries.
    Crystals can be described with *lattices* and *unit cells* with basis vectors
    that are subject to group transformations. Conceptually, ECN creates edge index
    masks corresponding to symmetry groups, performs message passing over this masked
    index, and aggregates the results of many symmetry groups.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e6422b43252d3ad7ca97e55de20b9931.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [Kaba and Ravanbakhsh](https://openreview.net/forum?id=0Dh8dz4snu)'
  prefs: []
  type: TYPE_NORMAL
- en: Even more news on material discovery can found in the proceedings of the recent
    [AI4Mat NeurIPS workshop](https://sites.google.com/view/ai4mat)!
  prefs: []
  type: TYPE_NORMAL
- en: ‚òÇÔ∏è ML-based weather forecasting made a huge progress as well. In particular,
    [**GraphCast**](https://arxiv.org/abs/2212.12794) by DeepMind and [**Pangu-Weather**](https://arxiv.org/abs/2211.02556)
    by Huawei demonstrated exceptionally good results outperforming traditional models
    by a large margin. While Pangu-Weather leverages 3D/visual inputs and Visual Transformers,
    GraphCast employs a mesh MPNN where Earth is split into several hierarchy levels
    of meshes. The deepest level has about 40K nodes with 474 input features and the
    model outputs 227 predicted variables. The MPNN follows the ‚Äúencoder-processor-decoder‚Äù
    and has 16 layers. GraphCast is autoregressive model w.r.t. the next timestep
    prediction, that is, it takes previous two states and predicts the next one. GraphCast
    can build a 10-day forecast in <60 seconds on a single TPUv4 and is much more
    accurate than non-ML forecasting models. üëè
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/642385acd4eeb71b13131b45b29d0697.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Encoder-Processor-Decoder mesh MPNN in GraphCast. Source: [Lam, Sanchez-Gonzalez,
    Willson, Wirnsberger, Fortunato, Pritzel, et al.](https://arxiv.org/abs/2212.12794)'
  prefs: []
  type: TYPE_NORMAL
- en: '**What to expect in 2023**: We expect to see a lot more focus on computational
    efficiency and scalability of GNNs. Current GNN-based force-fields are obtaining
    remarkable accuracy, but are still 2‚Äì3 orders of magnitude slower than classical
    force-fields and are typically only deployed on a few hundred atoms. For GNNs
    to truly have a transformative impact on materials science and drug discovery,
    we will see many folks tackling this issue, be it through architectural advances
    or smarter sampling.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Geometry & Topology & PDEs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 2022, 1Ô∏è‚É£ we got a better understanding of oversmoothing and oversquashing
    phenomena in GNNs and their connections to algebraic topology; 2Ô∏è‚É£ using GNNs
    for PDE modeling is now mainstream.
  prefs: []
  type: TYPE_NORMAL
- en: 1Ô∏è‚É£ Michael Bronstein‚Äôs lab made huge contributions to this problem ‚Äî check
    those excellent posts on Neural Sheaf Diffusion and framing GNNs as gradient flows
  prefs: []
  type: TYPE_NORMAL
- en: '[](/neural-sheaf-diffusion-for-deep-learning-on-graphs-bfa200e6afa6?source=post_page-----1ba920cb9232--------------------------------)
    [## Neural Sheaf Diffusion for deep learning on graphs'
  prefs: []
  type: TYPE_NORMAL
- en: Cellular sheaf theory, a branch of algebraic topology, provides new insights
    into how Graph Neural Networks work and‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/neural-sheaf-diffusion-for-deep-learning-on-graphs-bfa200e6afa6?source=post_page-----1ba920cb9232--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'And on GNNs as gradient flows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/graph-neural-networks-as-gradient-flows-4dae41fb2e8a?source=post_page-----1ba920cb9232--------------------------------)
    [## Graph Neural Networks as gradient flows'
  prefs: []
  type: TYPE_NORMAL
- en: GNNs derived as gradient flows minimising a learnable energy that describes
    attractive and repulsive forces between‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/graph-neural-networks-as-gradient-flows-4dae41fb2e8a?source=post_page-----1ba920cb9232--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 2Ô∏è‚É£ Using GNNs for PDE modeling became a mainstream topic. Some papers require
    the ü§Ø **math alert** ü§Ø warning, but if you are familiar with the basics of ODEs
    and PDEs it should be much easier.
  prefs: []
  type: TYPE_NORMAL
- en: '*Message Passing Neural PDE Solvers* by [Brandstetter, Worrall, and Welling](https://openreview.net/forum?id=vSix3HPYKSU)
    describe how message passing can help solving PDEs, generalize better, and get
    rid of manual heuristics. Furthermore, MP-PDEs representationally contain classic
    solvers like finite differences.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4e9be60b522337c75d2f485d5e19ed8d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [Brandstetter, Worrall, and Welling](https://openreview.net/forum?id=vSix3HPYKSU)'
  prefs: []
  type: TYPE_NORMAL
- en: The topic was developed further by many recent works including continuous forecasting
    with implicit neural representations ([Yin et al.](https://arxiv.org/abs/2209.14855)),
    supporting mixed boundary conditions ([Horie and Mitsume](https://openreview.net/forum?id=B3TOg-YCtzo)),
    or latent evolution of PDEs ([Wu et al.](https://arxiv.org/abs/2206.07681))
  prefs: []
  type: TYPE_NORMAL
- en: '**What to expect in 2023**: Neural PDEs and their applications are likely to
    expand to more physics-related AI4Science subfields, where especially computational
    fluid dynamics (CFD) will potentially be influenced by GNN based surrogates in
    the coming months. Classical CFD is applied to a wide range of research and engineering
    problems in many fields of study, including aerodynamics, hypersonic and environmental
    engineering, fluid flows, visual effects in video games, or weather simulations
    as discussed above. GNN based surrogates might augment/replace traditional well-tried
    techniques such as finite element methods ([Lienen et al.](https://arxiv.org/abs/2203.08852)),
    remeshing algorithms ([Song et al.](https://arxiv.org/abs/2204.11188)), boundary
    value problems ([Loetsch et al.](https://arxiv.org/abs/2206.14092)), or interactions
    with triangularized boundary geometries ([Mayr et al.](https://arxiv.org/abs/2106.11299)).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The neural PDE community is starting to build strong and commonly used baselines
    and frameworks, which will in return help to accelerate the progress, e.g. **PDEBench**
    ([Takamoto et al.](https://arxiv.org/abs/2210.07182)) or **PDEArena** ([Gupta
    et al.](https://arxiv.org/abs/2209.15616))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Graph Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Definitely one of the main community drivers in 2022, **graph transformers**
    (GTs) evolved a lot towards higher effectiveness and better scalability. Several
    outstanding models published in 2022:'
  prefs: []
  type: TYPE_NORMAL
- en: '**üëë GraphGPS** by [Ramp√°≈°ek et al.](https://arxiv.org/abs/2205.12454) takes
    the title of **‚ÄúGT of 2022‚Äù** thanks to combining local message passing, global
    attention (optionally, linear for higher efficiency), and positional encodings
    that led to setting a new SOTA on ZINC and many other benchmarks. Check out a
    dedicated article on GraphGPS'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/graphgps-navigating-graph-transformers-c2cc223a051c?source=post_page-----1ba920cb9232--------------------------------)
    [## GraphGPS: Navigating Graph Transformers'
  prefs: []
  type: TYPE_NORMAL
- en: Recipes for cooking the best graph transformers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/graphgps-navigating-graph-transformers-c2cc223a051c?source=post_page-----1ba920cb9232--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: GraphGPS served as a backbone of **GPS++,** the [winning](https://ogb.stanford.edu/neurips2022/results/#winners_pcqm4mv2)
    OGB Large Scale Challenge 2022 model on PCQM4M v2 (graph regression). **GPS++**,
    [created by](https://arxiv.org/abs/2212.02229) Graphcore, Valence Discovery, and
    Mila, incorporates more features including 3D coordinates and leverages sparse-optimized
    IPU hardware (more on that in the following section). GPS++ weights are already
    [available](https://github.com/graphcore/ogb-lsc-pcqm4mv2) on GitHub!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/030d6d10617c7e1782ce0d078c915c21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'GraphGPS intuition. Source: [Ramp√°≈°ek et al](https://arxiv.org/abs/2205.12454)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transformer-M** by [Luo et al.](https://arxiv.org/abs/2210.01765) inspired
    many top OGB LSC models as well. Transformer-M adds 3D coordinates to the neat
    mix of joint 2D-3D pre-training. At inference time, when 3D info is not known,
    the model would infer a glimpse of 3D knowledge which improves the performance
    on PCQM4Mv2 by a good margin. Code is [available](https://github.com/lsj2408/Transformer-M)
    either.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b5d5841cfa0a2d34817e2a1d077b999.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Transformer-M joint 2D-3D pre-training scheme. Source:* [*Luo et al.*](https://arxiv.org/abs/2210.01765)'
  prefs: []
  type: TYPE_NORMAL
- en: '**TokenGT** by [Kim et al](https://arxiv.org/abs/2207.02505) goes even more
    explicit and adds all edges of the input graph (in addition to all nodes) to the
    sequence fed to the Transformer. With those inputs, encoder needs additional token
    types to distinguish nodes from edges. The authors prove several nice theoretical
    properties (although at the cost of higher computational complexity O((V+E)¬≤)
    that can get to the 4th power in the worst case of a fully-connected graph). Code
    is [available](https://github.com/jw9730/tokengt).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/362b38baa863661789419ac3384b6568.png)'
  prefs: []
  type: TYPE_IMG
- en: 'TokenGT adds both nodes and edges to the input sequence. Source: [Kim et al](https://arxiv.org/abs/2207.02505)'
  prefs: []
  type: TYPE_NORMAL
- en: '**What to expect in 2023**: for the coming year, we‚Äôd expect 1Ô∏è‚É£ GTs to scale
    up along the axes of both data and model parameters, from molecules of <50 nodes
    to graphs of millions of nodes, in order to witness the emergent behavior as in
    text & vision foundation models 2Ô∏è‚É£ similar to [BLOOM](https://huggingface.co/bigscience/bloom)
    by the BigScience Initiative, a big open-source pre-trained equivariant GT for
    molecular data, perhaps within the [Open Drug Discovery](https://m2d2.io/opendrugdiscovery/)
    project.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: BIG Graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: üî• One of our favorites in 2022 is *‚ÄúGraph Neural Networks for Link Prediction
    with Subgraph Sketching*‚Äù by [Chamberlain, Shirobokov et al.](https://arxiv.org/abs/2209.15486)
    ‚Äî this is a neat combination of algorithms + ML techniques. It is known that [SEAL](https://arxiv.org/pdf/2010.16103.pdf)-like
    labeling tricks dramatically improve link prediction performance compared to standard
    GNN encoders but suffer from big computation/memory overhead. In this work, the
    authors find that obtaining distances from two nodes of a query edge can be efficiently
    done with hashing ([MinHashing](https://en.wikipedia.org/wiki/MinHash)) and cardinality
    estimation ([HyperLogLog](https://en.wikipedia.org/wiki/HyperLogLog)) algorithms.
    Essentially, message passing is done over *minhashing* and *hyperloglog* initial
    sketches of single nodes (*min* aggregation for minhash, *max* for hyperloglog
    sketches) ‚Äî this is the core of the **ELPH** link prediction model (with a simple
    MLP decoder). The authors then design a more scalable **BUDDY** model where k-hop
    hash propagation can be precomputed before training. Experimentally, ELPH and
    BUDDY scale to large graphs that were previously way too large or resource hungry
    for labeling trick approaches. Great work and definitely a solid baseline for
    all future link prediction models! üëè
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0a1e1985708d039dbfd156a1aef9ac06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The motivation behind computing subgraph hashes to estimate cardinalities of
    neighborhoods and intersections. Source: [Chamberlain, Shirobokov et al.](https://arxiv.org/abs/2209.15486)'
  prefs: []
  type: TYPE_NORMAL
- en: On the graph sampling and minibatching side, [Gasteiger, Qian, and G√ºnnemann](https://openreview.net/forum?id=b9g0vxzYa_)
    design [**Influence-based Mini-Batching (IBMB)**](https://github.com/tum-daml/ibmb),
    a good example how Personalized PageRank (PPR) can solve even graph batching!
    IBMB aims at creating the smallest minibatches whose nodes have the maximum influence
    on the node classification task. In fact, the influence score is equivalent to
    PPR. Practically, given a set of target nodes, IBMB (1) partitions the graph into
    permanent clusters, (2) runs PPR within each batch to select top-PPR nodes that
    would form a final subgraph minibatch. The resulting minibatches can be sent to
    any GNN encoder. IBMB is pretty much **constant** O(1) to the graph size where
    partitioning and PPRs can be precomputed at the pre-processing stage.
  prefs: []
  type: TYPE_NORMAL
- en: Although the resulting batches are fixed and do not change over training (not
    stochastic enough), the authors design momentum-like optimization terms to mitigate
    this non-stochasticity. IBMB can be used both in training and inference with massive
    speedups ‚Äî up to 17x and 130x, respectively üöÄ
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/79a47281579302493940b6b733e1b1c3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Influence-based mini-batching. Source: [Gasteiger, Qian, and G√ºnnemann](https://openreview.net/forum?id=b9g0vxzYa_)'
  prefs: []
  type: TYPE_NORMAL
- en: The subtitle of this subsection could be ‚Äú*brought to you by Google*‚Äù since
    the majority of the papers have authors from Google ;)
  prefs: []
  type: TYPE_NORMAL
- en: '[Carey et al.](https://openreview.net/pdf?id=q5h7Ywx-sS) created ***Stars***,
    a method for building sparse similarity graphs at the scale of **tens of trillions**
    of edges ü§Ø. Pairwise N¬≤ comparisons would obviously not work here ‚Äî Stars employs
    two-hop [spanner graphs](https://en.wikipedia.org/wiki/Geometric_spanner) (those
    are the graphs where similar points are connected with at most two hops) and [SortingLSH](http://infolab.stanford.edu/~bawa/Pub/similarity.pdf)
    that together enable almost linear time complexity and high sparsity.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Dhulipala et al.](https://openreview.net/pdf?id=LpgG0C6Y75) created **ParHAC**,
    an approximate (1+ùùê) parallel algorithm for hierarchical agglomerative clustering
    (HAC) on very large graphs and extensive theoretical foundations of the algorithm.
    ParHAC has O(V+E) complexity and poly-log depth and runs up to 60x faster than
    baselines on graphs with **hundreds of billions** of edges (here it is the Hyperlink
    graph with 1.7B nodes and 125B edges).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Devvrit et al.](https://openreview.net/pdf?id=ldl2V3vLZ5) created **S¬≥GC**,
    a scalable self-supervised graph clustering algorithm with one-layer GNN and constrastive
    training objective. S¬≥GC uses both graph structure and node features and scales
    to graphs of up to 1.6B edges.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, [Epasto et al.](https://openreview.net/forum?id=Fhty8PgFkDo) created
    a differentially-private modification of PageRank!
  prefs: []
  type: TYPE_NORMAL
- en: 'LoG 2022 featured two tutorials on large-scale GNNs: [Scaling GNNs in Production](https://www.youtube.com/watch?v=HRC4hZKiUWU)
    by Da Zheng, Vassilis N. Ioannidis, and Soji Adeshina and [Parallel and Distributed
    GNNs](https://www.youtube.com/watch?v=e2jJU7u7si0) by Torsten Hoefler and Maciej
    Besta.'
  prefs: []
  type: TYPE_NORMAL
- en: '**What to expect in 2023**: further reduction in compute costs and inference
    time for very large graphs. Perhaps models for OGB LSC graphs could run on commodity
    machines instead of huge clusters?'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'GNN Theory: Weisfeiler and Leman Go Places, Subgraph GNNs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/b7121cd3156f09a0fadc730188d2124d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Tourists of the year! Source of the original portraits: [Towards Geometric
    Deep Learning IV: Chemical Precursors of GNNs](/towards-geometric-deep-learning-iv-chemical-precursors-of-gnns-11273d74125)
    by Michael Bronstein'
  prefs: []
  type: TYPE_NORMAL
- en: 'üèñ üåÑ Weisfeiler and Leman, grandfathers of Graph ML and GNN theory, had a very
    prolific traveling year! After visiting [Neural](https://ojs.aaai.org/index.php/AAAI/article/view/4384),
    [Sparse](https://proceedings.neurips.cc/paper/2020/file/f81dee42585b3814de199b2e88757f5c-Paper.pdf),
    [Topological](http://proceedings.mlr.press/v139/bodnar21a/bodnar21a.pdf), and
    [Cellular](https://proceedings.neurips.cc/paper/2021/file/157792e4abb490f99dbd738483e0d2d4-Paper.pdf)
    places in previous years, in 2022 we have seen them in several new places:'
  prefs: []
  type: TYPE_NORMAL
- en: WL Go **Machine Learning** ‚Äî a comprehensive survey by [Morris et al](https://arxiv.org/abs/2112.09992)
    on the basics of the WL test, terminology, and various applications;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WL Go **Relational** ‚Äî the first attempt by [Barcelo et al](https://arxiv.org/abs/2211.17113)
    to study expressiveness of relational GNNs used in multi-relational graphs and
    KGs. Turns out R-GCN and CompGCN are equally expressive and are bounded by the
    Relational 1-WL test, and the most expressive message function (aggregating entity-relation
    representations) is a Hadamard product;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[WL Go Walking by Niels M. Kriege](https://arxiv.org/abs/2205.10914) studies
    expressiveness of random walk kernels and finds that the RW kernel (with a small
    modification) is as expressive as a WL subtree kernel;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'WL Go **Geometric**: [Joshi, Bodnar et al](https://openreview.net/forum?id=kXe4Y0c4VqT)
    propose Geometric WL test (GWL) to study expressiveness of equivariant and invariant
    GNNs (to ceratin symmetries: translation, rotation, reflection, permutation).
    Turns out, equivariant GNNs (such as [E-GNN](https://arxiv.org/abs/2102.09844),
    [NequIP](https://arxiv.org/abs/2101.03164) or [MACE](https://arxiv.org/abs/2206.07697))
    are provably more powerful than invariant GNNs (such as [SchNet](https://proceedings.neurips.cc/paper/2017/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf)
    or [DimeNet](https://arxiv.org/abs/2011.14115));'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'WL Go **Temporal**: [Souza et al](https://openreview.net/pdf?id=MwSXgQSxL5s)
    propose Temporal WL test to study expressiveness of temporal GNNs. The authors
    then propose a novel injective aggregation function (and the PINT model) that
    should be most expressive;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'WL Go **Gradual**: [Bause and Kriege](https://openreview.net/forum?id=fe1DEN1nds)
    propose to modify the original WL color refinement with a non-injective function
    where different multi-sets *might* get assigned the same color (under certain
    conditions). It thus enables more gradual color refinement and slower convergence
    to stable coloring that eventually retains expressiveness of 1-WL but gets a few
    distinguishing properties on the way.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'WL Go **Infinite**: [Feldman et al](https://arxiv.org/abs/2201.13410) propose
    to change the initial node coloring with spectral features derived from the heat
    kernel of the Laplacian or with k-smallest eigenvectors of the Laplacian (for
    large graphs) which is quite close to Laplacian Positional Encodings (LPEs).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'WL Go **Hyperbolic**: [Nikolentzos et al](https://arxiv.org/abs/2211.02501)
    note that the color refinement procedure of the WL test produces a tree hierarchy
    of colors. In order to preserve relative distances of nodes encoded by those colors,
    the authors propose to map output states of each layer/iteration into a hyperbolic
    space and update it after each next layer. The final embeddings are supposed to
    retain the notion of node distances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'üìà In the realm of more expressive (than 1-WL) architectures, subgraph GNNs
    are the biggest trend. Among those, three approaches stand out: 1Ô∏è‚É£ **Subgraph
    Union Networks** (SUN) by [Frasca, Bevilacqua, et al.](https://arxiv.org/abs/2206.11140)
    that provide a comprehensive analysis of subgraph GNNs design space and expressiveness
    showing they are bounded by 3-WL; 2Ô∏è‚É£ **Ordered Subgraph Aggregation Networks**
    (OSAN) by [Qian, Rattan, et al](https://arxiv.org/abs/2206.11168) devise a hierarchy
    of subgraph-enhanced GNNs (k-OSAN) and find that k-OSAN are incomparable to k-WL
    but are strictly limited by (k+1)-WL. One particularly cool part of OSAN is using
    [Implicit MLE](https://arxiv.org/abs/2106.01798) (NeurIPS‚Äô21), a differentiable
    discrete sampling technique, for sampling ordered subgraphs. **Ô∏è3Ô∏è‚É£ SpeqNets**
    by [Morris et al.](https://arxiv.org/abs/2203.13913) devise a permutation-equivariant
    hierarchy of graph networks that balances between scalability and expressivity.
    4Ô∏è‚É£ **GraphSNN** by [Wijesinghe and Wang](https://openreview.net/pdf?id=uxgg9o7bI_3)
    derives expressive models based on the overlap of *subgraph* isomorphisms and
    *subtree* isomorpishms.'
  prefs: []
  type: TYPE_NORMAL
- en: ü§î A few works rethink the WL framework as a general means for GNN expressiveness.
    [Geerts and Reutter](https://openreview.net/pdf?id=wIzUeM3TAU) define **k-order
    MPNNs** that can be characterized with Tensor Languages (with a mapping between
    WL and **Tensor Languages**). A new [anonymous ICLR‚Äô23 submission](https://openreview.net/forum?id=r9hNv76KoT3)
    proposes to leverage [graph biconnectivity](https://en.wikipedia.org/wiki/Biconnected_component)
    and defines a **Generalized Distance WL** algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: If you‚Äôd like to study the topic even deeper, check out a wonderful [LOG 2022
    tutorial](https://www.youtube.com/watch?v=ASQYjbUBYzs&list=PL2iNJC54likoqgKwpFnbBik8Im1sZ27Hm&index=7)
    by Fabrizio Frasca, Beatrice Bevilacqua, and Haggai Maron with practical examples!
  prefs: []
  type: TYPE_NORMAL
- en: '**What to expect in 2023**: *1Ô∏è‚É£* More efforts on creating time- and memory-efficient
    subgraph GNNs. *2Ô∏è‚É£* Better understanding of generalization of GNNs. *3Ô∏è‚É£* Weisfeiler
    and Leman visit 10 new places!'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Knowledge Graphs: Inductive Reasoning Takes Over'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Last year, we observed a major shift in KG representation learning: transductive-only
    approaches are being actively retired in favor of inductive models that can build
    meaningful representation for new, unseen nodes and perform node classification
    and link prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In 2022, the field was expanding along two main axes: 1Ô∏è‚É£ inductive link prediction
    (LP) 2Ô∏è‚É£ and inductive (multi-hop) query answering that extends link prediction
    to much more complex prediction tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 1Ô∏è‚É£ In link prediction, the majority of inductive models (like [**NBFNet**](https://arxiv.org/abs/2106.06935)
    or [**NodePiece**](https://arxiv.org/abs/2106.12144)) transfer to unseen nodes
    at inference by assuming that the set of relation types is fixed during training
    and does not change over time so they can learn relation embeddings. What happens
    when the set of relations changes as well? In the hardest case, we‚Äôd want to transfer
    to KGs with completely different nodes **and** relation types.
  prefs: []
  type: TYPE_NORMAL
- en: So far, all such models supporting unseen relations resort to meta-learning
    which is slow and resource-hungry. In 2022, for the first time, [Huang, Ren, and
    Leskovec](https://openreview.net/forum?id=LvW71lgly25) proposed the Connected
    Subgraph Reasoner (**CSR**) framework that is inductive along **both** entities
    and relation types **and** does not need any meta-learning! üëÄ Generally, for new
    relations at inference, models see at least *k* example triples with this relation
    (hence, a k-shot learning scenario). Conceptually, CSR extracts subgraphs around
    each example trying to learn common relational patterns (i.e., optimizing edge
    masks) and then apply the mask to the query subgraph (with the missing target
    link to predict).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/495777bb4fc9ea6eae90cb434332ab5a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Inductive CSR that supports KGs with unseen entities and relation types. Source:
    [Huang, Ren, and Leskovec](https://openreview.net/forum?id=LvW71lgly25)'
  prefs: []
  type: TYPE_NORMAL
- en: '**ReFactor GNNs** by [Chen et al.](https://openreview.net/forum?id=81LQV4k7a7X)
    is another insightful work on inductive qualities of shallow KG embedding models
    ‚Äî particularly, the authors find that shallow factorization models like DistMult
    resemble infinitely deep GNNs when looking through the lens of backpropagation
    and how nodes update their representations from neighboring and non-neighboring
    nodes. Turns out that, theoretically, any factorization model can be turned into
    an inductive model!'
  prefs: []
  type: TYPE_NORMAL
- en: 2Ô∏è‚É£ Inductive representation learning arrived in the area of complex logical
    query answering as well. (shameless plug) In fact, it was one of the focuses of
    our team this year üòä First, in [Zhu et al.](https://arxiv.org/abs/2205.10128),
    we found that Neural Bellman-Ford nets generalize well from simple link prediction
    to complex query answering tasks in a new [**GNN Query Executor**](https://github.com/DeepGraphLearning/GNN-QE)
    (GNN-QE) model where a GNN based on NBF-Net performs relation projections while
    other logical operators are performed via fuzzy logic [t-norms](https://en.wikipedia.org/wiki/T-norm).
    Then, in [Inductive Logical Query Answering in Knowledge Graphs](https://openreview.net/forum?id=-vXEN5rIABY)
    we studied ‚öóÔ∏è *the essence of inductiveness* ‚öóÔ∏è and proposed two ways to answer
    logical queries over unseen entities at inference time, that is, via (1) inductive
    node representations obtained with NodePiece encoder paired with the inference-only
    decoder (less performant but scalable) or via (2) inductive relational structure
    representations akin to the one in GNN-QE (better quality but more resource-hungry
    and hard to scale). Overall we are able to scale to an inductive query setting
    on graphs **with millions of nodes and 500k unseen nodes and 5m unseen edges**
    during inference.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/063be3295d889aafc4b3b1c292e3e88b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Inductive logical query answering approaches: via node representations (NodePiece-QE)
    and relational structure representations (GNN-QE). Source: [Galkin et al.](https://arxiv.org/abs/2210.08008)'
  prefs: []
  type: TYPE_NORMAL
- en: The other cool work in the area is [**SMORE**](https://github.com/google-research/smore)by
    [Ren, Dai, et al.](https://arxiv.org/abs/2110.14890) ‚Äî it is a large-scale (transductive-only
    yet) system for complex query answering over very large graphs scaling up to the
    full Freebase with about 90M nodes and 300M edges üëÄ. In addition to CUDA, training,
    and pipeline optimizations, SMORE implements a bidirectional query sampler such
    that training queries can be generated on-the-fly right in the data loader instead
    of creating and storing huge datasets. Don‚Äôt forget to check out a [fresh hands-on
    tutorial](https://www.youtube.com/watch?v=kzWV57qJmiA&list=PL2iNJC54likoqgKwpFnbBik8Im1sZ27Hm&index=1)
    on large-scale graph reasoning from LOG 2022!
  prefs: []
  type: TYPE_NORMAL
- en: Last but not the least, [Yang, Lin and Zhang](https://arxiv.org/pdf/2209.08858.pdf)
    brought up an interesting paper rethinking the evaluation of knowledge graph completion.
    They point out knowledge graphs tend to be open-world (i.e., there are facts not
    encoded by the knowledge graph) rather close-world assumed by most works. As a
    result, metrics observed under the close-world assumption exhibit a log trend
    w.r.t. the true metric ‚Äî this means if you get 0.4 MRR for your model, chances
    are that the test knowledge graph is incomplete and your model has already done
    a good jobüëç. Maybe we can design some new dataset and evaluation to mitigate such
    an issue?
  prefs: []
  type: TYPE_NORMAL
- en: '**What to expect in 2023**: an inductive model fully transferable to different
    KGs with new sets of entities and relations, e.g., training on Wikidata, and running
    inference on DBpedia or Freebase.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Algorithmic Reasoning and Alignment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 2022 was a year of major breakthroughs and milestones for algorithmic reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: 1Ô∏è‚É£ First, the [**CLRS benchmark**](https://github.com/deepmind/clrs) by [Veliƒçkoviƒá
    et al.](https://arxiv.org/abs/2205.15659) is now available as the main playground
    to design and benchmark algorithmic reasoning models and tasks. CLRS already includes
    30 tasks (such as classical sorting algorithms, string algorithms, and graph algorithms)
    but still allows you to bring your own formulations or modify existing ones.
  prefs: []
  type: TYPE_NORMAL
- en: 2Ô∏è‚É£ Then, a **Generalist Neural Algorithmic Learner** by [Ibarz et al.](https://openreview.net/forum?id=FebadKZf6Gd)
    and DeepMind has shown that it is possible to train a *single* processor network
    that can be trained in the multi-task mode on different algorithms ‚Äî previously,
    you‚Äôd train a single model for a single task repeating that for all 30 CLRS problems.
    The paper also describes several modifications and tricks to the model architecture
    side and training procedure to let the model generalize better and prevent forgetting,
    e.g., triplet reasoning similar to triangular attention (common for molecular
    models) and [edge transformers](https://arxiv.org/abs/2112.00578). Overall, a
    new model brings a massive 25% absolute gain over baselines and solves 24 out
    of 30 CLRS tasks with 60%+ micro-F1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b4f7d36cd5ab8040d258a815d1dca4dc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [Ibarz et al.](https://openreview.net/forum?id=FebadKZf6Gd)'
  prefs: []
  type: TYPE_NORMAL
- en: 3Ô∏è‚É£ Last year, we [discussed](/graph-ml-in-2022-where-are-we-now-f7f8242599e0#72d1)
    the works on algorithmic alignment and saw the signs that GNNs can probably align
    well with dynamic programming. In 2022, [Dudzik and Veliƒçkoviƒá](https://openreview.net/forum?id=wu1Za9dY1GY)
    prove that **GNNs are Dynamic Programmers** using category theory, abstract algebra,
    and notion of *pushforward* and *pullback* operations. This is a wonderful example
    of applying category theory that many people consider ‚Äúabstract nonsense‚Äù üòâ. Category
    theory is likely to have more impact in GNN theory and Graph ML in general, so
    check out a fresh course [Cats4AI](https://cats.for.ai/) for a gentle introduction
    to the field.
  prefs: []
  type: TYPE_NORMAL
- en: 4Ô∏è‚É£ Finally, the work of [Beurer-Kellner et al.](https://openreview.net/forum?id=AiY6XvomZV4)
    is one of the first practical application of the neural algorithmic reasoning
    framework, here it is applied to configuring computer networks, i.e., routing
    protocols like BGP that are at the core of the internet. There, the authors show
    that representing a routing config as a graph allows to frame the routing problem
    as node property prediction. This approach brings whopping üëÄ **490x** üëÄ speedups
    compared to traditional rule-based routing methods and stil maintain 90+% specification
    consistency.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9045a1b7fb0c1e4f9b6f05c353eaaea9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [Beurer-Kellner et al.](https://openreview.net/forum?id=AiY6XvomZV4)'
  prefs: []
  type: TYPE_NORMAL
- en: If you want to follow algorithmic reasoning more closely, don‚Äôt miss a fresh
    [LoG 2022 tutorial](https://algo-reasoning.github.io/) by ‚Äã‚ÄãPetar Veliƒçkoviƒá,
    Andreea Deac and Andrew Dudzik.
  prefs: []
  type: TYPE_NORMAL
- en: '**What to expect in 2023: *1Ô∏è‚É£*** Algorithmic reasoning tasks are likely to
    scale to graphs of thousands of nodes and practical applications like in code
    analysis or databases, *2Ô∏è‚É£* even more algorithms in the benchmark, *3Ô∏è‚É£* most
    unlikely ‚Äî there will appear a model capable of solving quickselect *üòÖ*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Cool GNN Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: üëÉ **Learning to Smell with GNNs.** Back in 2019, Google AI started a [project](https://ai.googleblog.com/2019/10/learning-to-smell-using-deep-learning.html)
    on learning representations of smells. From basic chemistry we know that aromaticity
    depends on the molecular structure, e.g., cyclic compounds. In fact, the whole
    group of ‚Äùaromatic hydrocarbons‚Äù was named *aromatic* because they actually has
    some smell (compared to many non-organic molecules). If we have a molecular structure,
    we can employ a GNN on top of it and learn some representations!
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, Google AI released [a new blogpost](https://ai.googleblog.com/2022/09/digitizing-smell-using-molecular-maps.html)
    and paper by [Qian et al.](https://www.biorxiv.org/content/10.1101/2022.07.21.500995v3)
    describing the next phase of the project ‚Äî the **Principal Odor Map** that is
    able to group molecules in ‚Äúodor clusters‚Äù. The authors conducted 3 cool experiments:
    classifying 400 new molecules never smelled before and comparison to the averaged
    rating of a group of human panelists; linking odor quality to fundamental biology;
    and probing aromatic molecules on their mosquito repelling qualities. The GNN-based
    model shows very good results ‚Äî now we can finally claim that GNNs can smell!
    Looking forward for GNNs transforming the perfume industry.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/676191b7f09085ed9d141bae32bae385.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Embedding of odors. Source: [Google AI blog](https://ai.googleblog.com/2022/09/digitizing-smell-using-molecular-maps.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '‚öΩ **GNNs + Football.** If you thought that sophisticated GNNs for modelling
    trajectories are only used for molecular dynamics and arcane quantum simulations,
    fear not! Here is a cool practical application with a very high potential outreach:
    **Graph Imputer** by [Omidshafiei et al.](https://www.nature.com/articles/s41598-022-12547-0.epdf?sharing_token=HmyoHCAtNdoDfjlObtCiltRgN0jAjWel9jnR3ZoTv0NzQifNnvllGA8o7uZB3n1gdCaC-3jfBQwxpTCJNR7isTeW2uWhYUL8hz8MmWvyYQLogAFNcVp5ZZuTr_O-slFsi4f4-5pz3J2Th9rSxCJV-s63f-q5fojV0FBGNWKYlRQ%3D),
    DeepMind, and FC Liverpool predicts trajectories of football players (and the
    ball). Each game graph consists of 23 nodes, gets updated with a standard message
    passing encoder and a special time-dependent LSTM. The dataset is quite novel,
    too ‚Äî it consists of 105 English Premier League matches (avg 90 min each), all
    players and the ball were tracked at 25 fps, and the resulting training trajectory
    sequences encode about 9.6 seconds of gameplay.'
  prefs: []
  type: TYPE_NORMAL
- en: The paper is easy to read and has numerous football illustrations, check it
    out! Sports tech is actively growing those days, and football analysts now could
    go even deeper in studying their competitors. Will EPL clubs compete for GNN researchers
    in the upcoming transfer windows? Time to create transfermarkt for GNN researchers
    üòâ
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4636afc466b88dba1d5146bab2df8221.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Football match simulation is like molecular dynamics simulation! Source: [DeepMind](https://twitter.com/deepmind/status/1529444212864843777?lang=en)'
  prefs: []
  type: TYPE_NORMAL
- en: 'ü™ê **Galaxies and Astrophysics.** For astrophysics aficionados: **Mangrove**
    by [Jespersen et al.](https://arxiv.org/abs/2210.13473) applies GraphSAGE to merger
    trees of dark matter to predict a variety of galactic properties like stellar
    mass, cold gas mass, star formation rate, and even black hole mass. The paper
    is a bit heavy on the terminology of astrophysics but pretty easy in terms of
    GNN parameterization and training. Mangrove works 4‚Äì9 orders of magnitude faster
    than standard models. Experimental charts are pieces of art that you can hang
    on a wall üñºÔ∏è.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/77efdaa485947c23151de37ec221a2cb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Mangrove approach to present dark matter halos as merger trees and graphs.
    Source: [Jespersen et al.](https://arxiv.org/abs/2210.13473)'
  prefs: []
  type: TYPE_NORMAL
- en: 'ü§ñ **GNNs for code**. Code generation models like AlphaCode and Codex have mindblowing
    capabilities. Although LLMs are at the core of those models, GNNs do help in a
    few neat ways: **Instruction Pointer Attention GNNs** (IPA-GNNs) first proposed
    by [Bieber et al](https://arxiv.org/abs/2010.12621) have been used to [predict
    runtime errors](https://arxiv.org/abs/2203.03771) in competitive programming tasks
    ‚Äî so it is almost like a virtual code interpreter! **CodeTrek** by [Pashakhanloo
    et al.](https://openreview.net/forum?id=WQc075jmBmf) proposes to model a program
    as a relational graph and embed it via random walks and Transformer encoder. Downstream
    applications include variable misuse, prediction exceptions, predicting shadowed
    variables.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ab5c9d787662778c2d57153876512efe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [Pashakhanloo et al.](https://openreview.net/forum?id=WQc075jmBmf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hardware: IPUs and Graphcore Win OGB Large-Scale Challenge 2022'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ü•á 2022 brought a huge success to [Graphcore](https://www.graphcore.ai/) and
    [IPUs](https://www.graphcore.ai/bow-processors) ‚Äî the hardware optimized for sparse
    operations that are so needed when working with graphs. The first success story
    was optimizing Temporal Graph Nets (TGN) for IPUs with massive performance gains
    (check the [article](/accelerating-and-scaling-temporal-graph-networks-on-the-graphcore-ipu-c15ac309b765)
    in Michael Bronstein‚Äôs blog).
  prefs: []
  type: TYPE_NORMAL
- en: '[](/accelerating-and-scaling-temporal-graph-networks-on-the-graphcore-ipu-c15ac309b765?source=post_page-----1ba920cb9232--------------------------------)
    [## Accelerating and scaling Temporal Graph Networks on the Graphcore IPU'
  prefs: []
  type: TYPE_NORMAL
- en: Is GPU the best hardware choice for GNNs? Together with Graphcore, we explore
    the advantages of the new IPU‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/accelerating-and-scaling-temporal-graph-networks-on-the-graphcore-ipu-c15ac309b765?source=post_page-----1ba920cb9232--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Later on, Graphcore [stormed the leaderboards](https://www.graphcore.ai/posts/graphcore-claims-double-win-in-open-graph-benchmark-challenge)
    of OGB LSC‚Äô22 by winning 2 out of 3 tracks: link prediction on the **WikiKG90M
    v2** knowledge graph and graph regression on the **PCQM4M v2** molecular dataset.
    In addition to the sheer compute power, the authors took several clever model
    decisions: for link prediction it was [Balanced Entity Sampling and Sharing (BESS)](https://arxiv.org/abs/2211.12281)
    for training an ensemble of shallow LP models (check the [blog post](/large-scale-knowledge-graph-completion-on-ipu-4cf386dfa826)
    by Daniel Justus for more details), and GPS++ for the graph regression task (we
    covered GPS++ above in the GT section). You can [try out](https://ipu.dev/3FwVoLD)
    the pre-trained models using IPUs-powered virtual machines on Paperspace. Congratulations
    to Graphcore and their team! üëè'
  prefs: []
  type: TYPE_NORMAL
- en: PyG partnered with NVIDIA ([post](https://pyg.org/ns-newsarticle-accelerating-pyg-on-nvidia-gpus))
    and Intel ([post](https://pyg.org/news/accelerating-pyg-on-intel-cpus)) to increase
    the performance of core operations on GPUs and CPUs, respectively. Similarly,
    DGL [incorporated](https://www.dgl.ai/release/2022/07/25/release.html) new GPU
    optimizations in the recent 0.9 version. Massive gains for sparse matmuls and
    sampling procedures, so we‚Äôd encourage you to update your environments with the
    most recent versions!.
  prefs: []
  type: TYPE_NORMAL
- en: '**What to expect in 2023**: major GNN libraries are likely to increase the
    breadth of supported hardware backends such as IPUs or upcoming Intel Max Series
    GPUs.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'New Conferences: Learning of Graphs (LoG) and Molecular ML (MoML)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This year we witnessed the inauguration of two graph and geometric ML conferences:
    the [Learning on Graphs Conference (LoG)](https://logconference.org/#hero) and
    the [Molecular ML Conference](https://www.moml22.mit.edu/) (MoML).'
  prefs: []
  type: TYPE_NORMAL
- en: LoG is a more general all-around GraphML venue (held virtually this year) while
    MoML (held at MIT) has a broader mission and influence over the AI4Science community
    where graphs and geometry still plays a major role. Both conferences were received
    extremely well. MoML attracted 7 top speakers and 38 posters, LoG had ~3000 registrations,
    266 submissions, 71 posters, 12 orals, and 7 awesome tutorials (all recordings
    of oral talks and tutorials are [already on YouTube](https://www.youtube.com/@learningongraphs)).
    Besides, LoG introduced a great monetary incentive for reviewers, resulting in
    a well-recognized improvement of the review quality! From our point of view, quality
    of LoG reviews was often better than those at NeurIPS or ICML.
  prefs: []
  type: TYPE_NORMAL
- en: This is a huge win and carnival for the graph ML community, and congrats to
    everyone working in the field of graph and geometric machine learning with a new
    ‚Äúhome‚Äù venue!
  prefs: []
  type: TYPE_NORMAL
- en: '**What to expect in 2023:** LOG and MoML become main Graph ML venues to include
    into your submission calendar along with ICLR / NeurIPS / ICML'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Courses and Educational Materials
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Geometric Deep Learning Course ‚Äî [Second Edition](https://www.youtube.com/playlist?list=PLn2-dEmQeTfSLXW8yXP4q_Ii58wFdxb3C)
    (2022) is already on YouTube. The main entry point to the field.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[An Introduction to Group Equivariant Deep Learning](https://uvagedl.github.io/)
    by Erik Bekkers ‚Äî one of the best new courses about equivariance and equivariant
    models!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Cats4AI](https://cats.for.ai/) ‚Äî a new course by Andrew Dudzik, Bruno Gavranoviƒá,
    Jo√£o Guilherme Ara√∫jo, Petar Veliƒçkoviƒá, and Pim de Haan is the best place to
    learn about category theory and its connections to Geometric DL.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Summer School proceedings: [Italian Summer School on Geometric DL](https://www.sci.unich.it/geodeep2022/#home),
    London Geometry and Machine Learning ([LOGML](https://www.logml.ai/home-2022))
    Summer School, [BIRS Workshop on Topological Representation Learning](https://www.birs.ca/events/2022/5-day-workshops/22w5125).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stanford Graph Learning Workshop 2022](https://snap.stanford.edu/graphlearning-workshop-2022/)
    ‚Äî latest news from PyG developers and partners and Stanford researchers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New Datasets, Benchmarks, and Challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[OGB Large-Scale Challenge 2022](https://ogb.stanford.edu/neurips2022/): The
    second large scale challenge held at NeurIPS2022 with large and realistic graph
    ML tasks covering node-, edge-, graph-level predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Open Catalyst 2022 Challenge](https://opencatalystproject.org/challenge.html):
    the second edition of the challenge held at NeurIPS2022 with the task to design
    new machine learning models to predict the outcome of catalyst simulations used
    to understand activity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[CASP 15](https://predictioncenter.org/casp15/index.cgi): the protein structure
    prediction challenge disrupted by AlphaFold a few years ago at CASP 14\. Detailed
    analysis is yet to come, but it seems that MSAs strike back and best performing
    models still rely on MSAs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Long Range Graph Benchmark](https://arxiv.org/abs/2206.08164): for measuring
    GNNs and GTs capabilities of capturing long range interactions in graphs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Taxonomy of Graph Benchmarks](https://arxiv.org/abs/2206.07729), [Graph Learning
    Indexer](https://github.com/Graph-Learning-Benchmarks/gli): deeper studies of
    the dataset landscape in Graph ML outlining open challenges in benchmarking and
    trustworthiness of results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[GraphWorld](https://ai.googleblog.com/2022/05/graphworld-advances-in-graph.html):
    a framework for analyzing the performance of GNN architectures on millions of
    synthetic benchmark datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chartalist](https://openreview.net/forum?id=10iA3OowAV3) ‚Äî a collection of
    blockchain graph datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PEER protein learning benchmark](https://github.com/DeepGraphLearning/PEER_Benchmark):
    a multi-task benchmark for protein sequence understanding with 17 tasks of protein
    understanding lying in 5 task categories.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ESM Metagenomic Atlas](https://esmatlas.com/): acomprehensive database of
    over 600 million predicted protein structures with nice visualizations and search
    UI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Software Libraries and Open Source
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Mainstream graph ML libraries: [PyG 2.2](https://www.pyg.org/) (PyTorch), [DGL
    0.9](https://www.dgl.ai/) (PyTorch, TensorFlow, MXNet), [TF GNN](https://github.com/tensorflow/gnn)
    (TensorFlow) and [Jraph](https://github.com/deepmind/jraph) (Jax)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TorchDrug](https://torchdrug.ai/) and [TorchProtein](https://torchprotein.ai/):
    machine learning library for drug discovery and protein science'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PyKEEN](https://github.com/pykeen/pykeen): the best platform for training
    and evaluating knowledge graph embeddings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Graphein](https://graphein.ai/): a package that provides a number of types
    of graph-based representations of proteins'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[GRAPE](https://github.com/AnacletoLAB/grape) and [Marius](https://marius-project.org/):
    scalable graph processing and embedding libraries over billion-scale graphs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[MatSci ML Toolkit](https://github.com/IntelLabs/matsciml): a flexible framework
    for deep learning on the opencatalyst dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[E3nn](https://github.com/e3nn/e3nn): the go-to library for E(3) equivariant
    neural networks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join the Community
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Reading Groups: [Learning on Graphs and Geometry](https://m2d2.io/talks/log2/about/)
    (LOG2) reading group, [Molecular Modeling & Drug Discovery](https://m2d2.io/talks/m2d2/about/)
    (M2D2) reading group, and their Slack communities'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning of Graphs (LoG) [Slack community](https://logconference.org/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Michael Bronstein‚Äôs blog on Medium](https://michael-bronstein.medium.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PyG medium](https://medium.com/@pytorch_geometric), [blog posts](https://pyg.org/blogs-and-tutorials),
    and newsletter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[GraphML Telegram channel](https://t.me/graphML)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Meme of 2022 ü™ì
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/8b5892b801b9e71ad5914d29b86184f8.png)'
  prefs: []
  type: TYPE_IMG
- en: Created by Michael Galkin and Michael Bronstein
  prefs: []
  type: TYPE_NORMAL
