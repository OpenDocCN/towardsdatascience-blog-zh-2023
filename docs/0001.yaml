- en: 'Graph ML in 2023: The State of Affairs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å›¾å½¢æœºå™¨å­¦ä¹ åœ¨2023å¹´çš„ç°çŠ¶
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/graph-ml-in-2023-the-state-of-affairs-1ba920cb9232?source=collection_archive---------0-----------------------#2023-01-01](https://towardsdatascience.com/graph-ml-in-2023-the-state-of-affairs-1ba920cb9232?source=collection_archive---------0-----------------------#2023-01-01)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/graph-ml-in-2023-the-state-of-affairs-1ba920cb9232?source=collection_archive---------0-----------------------#2023-01-01](https://towardsdatascience.com/graph-ml-in-2023-the-state-of-affairs-1ba920cb9232?source=collection_archive---------0-----------------------#2023-01-01)
- en: STATE OF THE ART DIGEST
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æœ€å‰æ²¿åŠ¨æ€
- en: Hot trends and major advancements
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: çƒ­ç‚¹è¶‹åŠ¿å’Œé‡å¤§è¿›å±•
- en: '[](https://mgalkin.medium.com/?source=post_page-----1ba920cb9232--------------------------------)[![Michael
    Galkin](../Images/c5eb13334712ca0462d8a5df4a268ad0.png)](https://mgalkin.medium.com/?source=post_page-----1ba920cb9232--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1ba920cb9232--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1ba920cb9232--------------------------------)
    [Michael Galkin](https://mgalkin.medium.com/?source=post_page-----1ba920cb9232--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mgalkin.medium.com/?source=post_page-----1ba920cb9232--------------------------------)[![Michael
    Galkin](../Images/c5eb13334712ca0462d8a5df4a268ad0.png)](https://mgalkin.medium.com/?source=post_page-----1ba920cb9232--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1ba920cb9232--------------------------------)[![æ•°æ®ç§‘å­¦å‰æ²¿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1ba920cb9232--------------------------------)
    [Michael Galkin](https://mgalkin.medium.com/?source=post_page-----1ba920cb9232--------------------------------)'
- en: Â·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d4f8ddd1e68&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-ml-in-2023-the-state-of-affairs-1ba920cb9232&user=Michael+Galkin&userId=4d4f8ddd1e68&source=post_page-4d4f8ddd1e68----1ba920cb9232---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1ba920cb9232--------------------------------)
    Â·26 min readÂ·Jan 1, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1ba920cb9232&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-ml-in-2023-the-state-of-affairs-1ba920cb9232&user=Michael+Galkin&userId=4d4f8ddd1e68&source=-----1ba920cb9232---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d4f8ddd1e68&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-ml-in-2023-the-state-of-affairs-1ba920cb9232&user=Michael+Galkin&userId=4d4f8ddd1e68&source=post_page-4d4f8ddd1e68----1ba920cb9232---------------------post_header-----------)
    å‘å¸ƒåœ¨ [æ•°æ®ç§‘å­¦å‰æ²¿](https://towardsdatascience.com/?source=post_page-----1ba920cb9232--------------------------------)
    Â· 26åˆ†é’Ÿé˜…è¯» Â· 2023å¹´1æœˆ1æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1ba920cb9232&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-ml-in-2023-the-state-of-affairs-1ba920cb9232&user=Michael+Galkin&userId=4d4f8ddd1e68&source=-----1ba920cb9232---------------------clap_footer-----------)'
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1ba920cb9232&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-ml-in-2023-the-state-of-affairs-1ba920cb9232&source=-----1ba920cb9232---------------------bookmark_footer-----------)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1ba920cb9232&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-ml-in-2023-the-state-of-affairs-1ba920cb9232&source=-----1ba920cb9232---------------------bookmark_footer-----------)'
- en: 2022 comes to an end and it is about time to sit down and reflect upon the achievements
    made in Graph ML as well as to hypothesize about possible breakthroughs in 2023\.
    Tune in ğŸ„â˜•
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 2022å¹´å·²ç»ç»“æŸï¼Œæ˜¯æ—¶å€™åä¸‹æ¥å›é¡¾ä¸€ä¸‹åœ¨å›¾å½¢æœºå™¨å­¦ä¹ ï¼ˆGraph MLï¼‰æ–¹é¢å–å¾—çš„æˆå°±ï¼Œå¹¶å¯¹2023å¹´çš„å¯èƒ½çªç ´è¿›è¡Œå‡è®¾äº†ã€‚æ•¬è¯·å…³æ³¨ ğŸ„â˜•
- en: '![](../Images/f8888b219ca9e135881455e3596c98d7.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8888b219ca9e135881455e3596c98d7.png)'
- en: Background image generated by [DALL-E 2](https://openai.com/dall-e-2/), text
    added by Author.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: èƒŒæ™¯å›¾åƒç”± [DALL-E 2](https://openai.com/dall-e-2/) ç”Ÿæˆï¼Œæ–‡æœ¬ç”±ä½œè€…æ·»åŠ ã€‚
- en: '*The article is written together with* [*Hongyu Ren*](http://hyren.me/) *(Stanford
    University),* [*Zhaocheng Zhu*](https://kiddozhu.github.io/) *(Mila & University
    of Montreal). We thank* [*Christopher Morris*](https://chrsmrrs.github.io/) *and*
    [*Johannes Brandstetter*](https://www.microsoft.com/en-us/research/people/johannesb/)
    *for the feedback and helping with the Theory and PDE sections, respectively.
    Follow* [*Michael*](https://twitter.com/michael_galkin)*,* [*Hongyu*](https://twitter.com/ren_hongyu)*,*
    [*Zhaocheng*](https://twitter.com/zhu_zhaocheng), [*Christopher*](https://twitter.com/chrsmrrs)*,
    and* [*Johannes*](https://twitter.com/jo_brandstetter) *here on Medium and Twitter
    for more graph ml-related discussions.*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿™ç¯‡æ–‡ç« ç”±* [*Hongyu Ren*](http://hyren.me/) *(æ–¯å¦ç¦å¤§å­¦)ã€* [*Zhaocheng Zhu*](https://kiddozhu.github.io/)
    *(Mila å’Œè’™ç‰¹åˆ©å°”å¤§å­¦)å…±åŒæ’°å†™ã€‚æˆ‘ä»¬æ„Ÿè°¢* [*Christopher Morris*](https://chrsmrrs.github.io/)
    *å’Œ* [*Johannes Brandstetter*](https://www.microsoft.com/en-us/research/people/johannesb/)
    *åœ¨ç†è®ºå’Œåå¾®åˆ†æ–¹ç¨‹éƒ¨åˆ†çš„åé¦ˆå’Œå¸®åŠ©ã€‚è¯·å…³æ³¨* [*Michael*](https://twitter.com/michael_galkin)*ã€* [*Hongyu*](https://twitter.com/ren_hongyu)*ã€*
    [*Zhaocheng*](https://twitter.com/zhu_zhaocheng)ã€* [*Christopher*](https://twitter.com/chrsmrrs)*å’Œ*
    [*Johannes*](https://twitter.com/jo_brandstetter) *åœ¨Mediumå’ŒTwitterä¸Šï¼Œä»¥è·å–æ›´å¤šä¸å›¾æœºå™¨å­¦ä¹ ç›¸å…³çš„è®¨è®ºã€‚*'
- en: '**Table of Contents:**'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç›®å½•ï¼š**'
- en: '[Generative Models: Denoising Diffusion for Molecules and Proteins](#48f6)'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ç”Ÿæˆæ¨¡å‹ï¼šåˆ†å­å’Œè›‹ç™½è´¨çš„å»å™ªæ‰©æ•£æ¨¡å‹](#48f6)'
- en: '[DFTs, ML Force Fields, Materials, and Weather Simulations](#d2e8)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[DFTsã€æœºå™¨å­¦ä¹ åŠ›åœºã€ææ–™å’Œå¤©æ°”æ¨¡æ‹Ÿ](#d2e8)'
- en: '[Geometry & Topology & PDEs](#6d20)'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[å‡ ä½•å­¦ä¸æ‹“æ‰‘å­¦ä¸åå¾®åˆ†æ–¹ç¨‹](#6d20)'
- en: '[Graph Transformers](#8e6c)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[å›¾å˜æ¢å™¨](#8e6c)'
- en: '[BIG Graphs](#ca19)'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[å¤§å‹å›¾](#ca19)'
- en: '[GNN Theory: Weisfeiler and Leman Go Places, Subgraph GNNs](#7986)'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[GNNç†è®ºï¼šWeisfeilerå’ŒLemançš„å‰æ™¯ï¼Œå­å›¾GNN](#7986)'
- en: '[Knowledge Graphs: Inductive Reasoning Takes Over](#e5e6)'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[çŸ¥è¯†å›¾è°±ï¼šå½’çº³æ¨ç†æ¥ç®¡](#e5e6)'
- en: '[Algorithmic Reasoning and Alignment](#b2f5)'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ç®—æ³•æ¨ç†å’Œå¯¹é½](#b2f5)'
- en: '[Cool GNN Applications](#0de4)'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[é…·ç‚«çš„GNNåº”ç”¨](#0de4)'
- en: '[Hardware: IPUs and Graphcore win OGB LSC 2022](#b813)'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ç¡¬ä»¶ï¼šIPUå’ŒGraphcoreèµ¢å¾—OGB LSC 2022](#b813)'
- en: '[New Conferences: LoG and Molecular ML](#9b59)'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[æ–°çš„ä¼šè®®ï¼šLoG å’Œåˆ†å­æœºå™¨å­¦ä¹ ](#9b59)'
- en: '[Courses and Educational Materials](#41dc)'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[è¯¾ç¨‹å’Œæ•™è‚²ææ–™](#41dc)'
- en: '[New Datasets, Benchmarks, and Challenges](#3e6d)'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[æ–°çš„æ•°æ®é›†ã€åŸºå‡†å’ŒæŒ‘æˆ˜](#3e6d)'
- en: '[Software Libraries and Open Source](#463f)'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[è½¯ä»¶åº“å’Œå¼€æº](#463f)'
- en: '[Join the Community](#1b30)'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[åŠ å…¥ç¤¾åŒº](#1b30)'
- en: '[The Meme of 2022](#7593)'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2022å¹´çš„ç½‘ç»œè¿·å› ](#7593)'
- en: 'Generative Models: Denoising Diffusion for Molecules and Proteins'
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç”Ÿæˆæ¨¡å‹ï¼šåˆ†å­å’Œè›‹ç™½è´¨çš„å»å™ªæ‰©æ•£æ¨¡å‹
- en: Generative diffusion models in the vision-language domain were the headline
    topic in the Deep Learning world in 2022\. While generating images and videos
    is definitely a cool playground to try out different models and sampling techniques,
    weâ€™d argue that
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆæ‰©æ•£æ¨¡å‹åœ¨è§†è§‰è¯­è¨€é¢†åŸŸæ˜¯2022å¹´æ·±åº¦å­¦ä¹ ä¸–ç•Œçš„å¤´æ¡è¯é¢˜ã€‚å°½ç®¡ç”Ÿæˆå›¾åƒå’Œè§†é¢‘æ— ç–‘æ˜¯å°è¯•ä¸åŒæ¨¡å‹å’Œé‡‡æ ·æŠ€æœ¯çš„é…·ç‚«é¢†åŸŸï¼Œæˆ‘ä»¬è®¤ä¸º
- en: the most *useful* applications of diffusion models in 2022 were actually created
    in the Geometric Deep Learning area focusing on molecules and proteins
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åœ¨2022å¹´ï¼Œæ‰©æ•£æ¨¡å‹æœ€*æœ‰ç”¨*çš„åº”ç”¨å®é™…ä¸Šæ˜¯åœ¨å‡ ä½•æ·±åº¦å­¦ä¹ é¢†åŸŸä¸­åˆ›å»ºçš„ï¼Œé‡ç‚¹å…³æ³¨åˆ†å­å’Œè›‹ç™½è´¨
- en: In our recent article, we were pondering whether [â€œDenoising Diffusion Is All
    You Need?â€](/denoising-diffusion-generative-models-in-graph-ml-c496af5811c5).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬æœ€è¿‘çš„æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬åœ¨æ€è€ƒæ˜¯å¦ [â€œå»å™ªæ‰©æ•£æ¨¡å‹å°±æ˜¯ä½ æ‰€éœ€çš„ä¸€åˆ‡ï¼Ÿâ€](/denoising-diffusion-generative-models-in-graph-ml-c496af5811c5)ã€‚
- en: '[](/denoising-diffusion-generative-models-in-graph-ml-c496af5811c5?source=post_page-----1ba920cb9232--------------------------------)
    [## Denoising Diffusion Generative Models in Graph ML'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/denoising-diffusion-generative-models-in-graph-ml-c496af5811c5?source=post_page-----1ba920cb9232--------------------------------)
    [## å»å™ªæ‰©æ•£ç”Ÿæˆæ¨¡å‹åœ¨å›¾æœºå™¨å­¦ä¹ ä¸­çš„åº”ç”¨'
- en: Is Denoising Diffusion all you need?
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å»å™ªæ‰©æ•£æ¨¡å‹å°±æ˜¯ä½ æ‰€éœ€çš„ä¸€åˆ‡å—ï¼Ÿ
- en: towardsdatascience.com](/denoising-diffusion-generative-models-in-graph-ml-c496af5811c5?source=post_page-----1ba920cb9232--------------------------------)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/denoising-diffusion-generative-models-in-graph-ml-c496af5811c5?source=post_page-----1ba920cb9232--------------------------------)
- en: 'There, we reviewed newest generative models for *graph generation* (DiGress),
    *molecular conformer generation* (EDM, GeoDiff, Torsional Diffusion), *molecular
    docking* (DiffDock), *molecular linking* (DiffLinker), and *ligand generation*
    (DiffSBDD). As soon as the post went public, several amazing protein generation
    models were released:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é‚£é‡Œï¼Œæˆ‘ä»¬å›é¡¾äº†æœ€æ–°çš„ç”Ÿæˆæ¨¡å‹ç”¨äº*å›¾ç”Ÿæˆ*ï¼ˆDiGressï¼‰ã€*åˆ†å­æ„è±¡ç”Ÿæˆ*ï¼ˆEDMã€GeoDiffã€Torsional Diffusionï¼‰ã€*åˆ†å­å¯¹æ¥*ï¼ˆDiffDockï¼‰ã€*åˆ†å­è¿æ¥*ï¼ˆDiffLinkerï¼‰å’Œ*é…ä½“ç”Ÿæˆ*ï¼ˆDiffSBDDï¼‰ã€‚ä¸€æ—¦å¸–å­å…¬å¼€ï¼Œå‡ ç§ä»¤äººæƒŠå¹çš„è›‹ç™½è´¨ç”Ÿæˆæ¨¡å‹ä¹Ÿéšä¹‹å‘å¸ƒï¼š
- en: '[**Chroma**](https://www.generatebiomedicines.com/chroma) from Generate Biomedicines
    allows to impose functional and geometric constraints, and even use natural language
    queries like â€œGenerate a protein with CHAD domainâ€ thanks to a small GPT-Neo trained
    on protein captioning;'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[**Chroma**](https://www.generatebiomedicines.com/chroma)æ¥è‡ªGenerate Biomedicinesï¼Œå…è®¸æ–½åŠ åŠŸèƒ½æ€§å’Œå‡ ä½•çº¦æŸï¼Œç”šè‡³å¯ä»¥ä½¿ç”¨è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œæ¯”å¦‚â€œç”Ÿæˆä¸€ä¸ªå…·æœ‰CHADç»“æ„åŸŸçš„è›‹ç™½è´¨â€ï¼Œè¿™è¦å½’åŠŸäºä¸€ä¸ªå°å‹çš„GPT-Neoï¼Œç»è¿‡è›‹ç™½è´¨æ ‡æ³¨çš„è®­ç»ƒï¼›'
- en: '![](../Images/7e24c508e8b597a6205035e92bd02e6d.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e24c508e8b597a6205035e92bd02e6d.png)'
- en: '*Chroma protein generation. Source:* [*Generate Biomedicines*](https://www.generatebiomedicines.com/chroma)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*Chromaè›‹ç™½è´¨ç”Ÿæˆã€‚æ¥æºï¼š* [*Generate Biomedicines*](https://www.generatebiomedicines.com/chroma)'
- en: '[**RoseTTaFold Diffusion**](https://www.bakerlab.org/2022/11/30/diffusion-model-for-protein-design/)
    (RF Diffusion) from the Baker Lab and MIT is packed with the similar functionality
    also allowing for text prompts like â€œGenerate a protein that binds to Xâ€ as well
    as being capable of functional motif scaffolding, scaffolding enzyme active sites,
    and *de novo* protein design. Strong point: 1000 designs generated with RF Diffusion
    were experimentally [synthesized and tested](https://twitter.com/DaveJuergens/status/1601675072175239170)
    in the lab!'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[**RoseTTaFold Diffusion**](https://www.bakerlab.org/2022/11/30/diffusion-model-for-protein-design/)ï¼ˆRF
    Diffusionï¼‰æ¥è‡ªBaker Labå’ŒMITï¼Œå…·æœ‰ç±»ä¼¼åŠŸèƒ½ï¼Œè¿˜æ”¯æŒæ–‡æœ¬æç¤ºï¼Œå¦‚â€œç”Ÿæˆä¸€ä¸ªèƒ½å¤Ÿç»“åˆXçš„è›‹ç™½è´¨â€ï¼Œå¹¶ä¸”èƒ½å¤Ÿè¿›è¡ŒåŠŸèƒ½æ€§åŸºåºæ”¯æ¶ã€é…¶æ´»æ€§ä½ç‚¹æ”¯æ¶å’Œ*de
    novo*è›‹ç™½è´¨è®¾è®¡ã€‚å¼ºé¡¹ï¼šç”¨RF Diffusionç”Ÿæˆçš„1000ç§è®¾è®¡åœ¨å®éªŒå®¤ä¸­[è¢«åˆæˆå’Œæµ‹è¯•](https://twitter.com/DaveJuergens/status/1601675072175239170)ï¼'
- en: '![](../Images/d6ce018a9c6d17df43d48a15da42dce6.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6ce018a9c6d17df43d48a15da42dce6.png)'
- en: '*RF Diffusion. Source:* [*Watson et al.*](https://www.biorxiv.org/content/10.1101/2022.12.09.519842v1)
    *BakerLab*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*RF Diffusionã€‚æ¥æºï¼š* [*Watsonç­‰*](https://www.biorxiv.org/content/10.1101/2022.12.09.519842v1)
    *BakerLab*'
- en: 'The Meta AI FAIR team made amazing progress in protein design purely with language
    models: mid-2022, [**ESM-2**](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1)
    was released, a protein LM trained solely on protein sequences that outperforms
    ESM-1 and other baselines by a huge margin. Moreover, it was then shown that encoded
    LM representations are a very good starting point for obtaining the actual geometric
    configuration of a protein without the need for Multiple Sequence Alignments (MSAs)
    â€” this is done via [**ESMFold**](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1).
    A big shoutout to Meta AI and FAIR for publishing the model and the weights: it
    is available in the [official GitHub repo](https://github.com/facebookresearch/esm)
    and [on HuggingFace](https://huggingface.co/models?other=esm) as well!'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Meta AI FAIRå›¢é˜Ÿåœ¨è›‹ç™½è´¨è®¾è®¡é¢†åŸŸé€šè¿‡è¯­è¨€æ¨¡å‹å–å¾—äº†æƒŠäººçš„è¿›å±•ï¼š2022å¹´ä¸­ï¼Œ[**ESM-2**](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1)å‘å¸ƒäº†ï¼Œè¿™æ˜¯ä¸€ç§ä»…åœ¨è›‹ç™½è´¨åºåˆ—ä¸Šè®­ç»ƒçš„è›‹ç™½è´¨è¯­è¨€æ¨¡å‹ï¼Œè¿œè¶…ESM-1åŠå…¶ä»–åŸºå‡†æ¨¡å‹ã€‚è€Œä¸”ï¼Œåæ¥æ˜¾ç¤ºç¼–ç çš„è¯­è¨€æ¨¡å‹è¡¨ç¤ºæ˜¯è·å¾—è›‹ç™½è´¨å®é™…å‡ ä½•ç»“æ„çš„éå¸¸å¥½çš„èµ·ç‚¹ï¼Œè€Œæ— éœ€å¤šé‡åºåˆ—æ¯”å¯¹ï¼ˆMSAsï¼‰â€”â€”è¿™å¯ä»¥é€šè¿‡[**ESMFold**](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1)å®ç°ã€‚ç‰¹åˆ«æ„Ÿè°¢Meta
    AIå’ŒFAIRå‘å¸ƒäº†è¯¥æ¨¡å‹åŠå…¶æƒé‡ï¼šå®ƒåœ¨[å®˜æ–¹GitHubä»“åº“](https://github.com/facebookresearch/esm)å’Œ[HuggingFace](https://huggingface.co/models?other=esm)ä¸Šä¹Ÿå¯ä»¥æ‰¾åˆ°ï¼
- en: '![](../Images/6d7f2f02320b2b61008ce8bb1973205d.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d7f2f02320b2b61008ce8bb1973205d.png)'
- en: 'Scaling ESM-2 leads to better folding prediction. Source: [Lin, Akin, Rao,
    Hie et al](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰©å±•ESM-2å¯ä»¥è·å¾—æ›´å¥½çš„æŠ˜å é¢„æµ‹ã€‚æ¥æºï¼š[Lin, Akin, Rao, Hieç­‰](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1)
- en: 'ğŸ­ Later on, even more goodies arrived from the ESM team: [Verkuil et al.](https://www.biorxiv.org/content/10.1101/2022.12.21.521521v1)
    find that ESM-2 can generate *de novo* protein sequences that can actually be
    synthesized in the lab and, more importantly, do not have any match among known
    natural proteins. [Hie et al.](https://www.biorxiv.org/content/10.1101/2022.12.21.521526v1)
    propose pretty much a new programming language for protein designers (think of
    it as a query language for ESMFold) â€” production rules organized in a syntax tree
    with constraint functions. Then, each program is â€œcompiledâ€ into an energy function
    that governs the generative process. Meta AI also released the biggest [Metagenomic
    Atlas](https://esmatlas.com/), but more on that in the **Datasets** section of
    this article.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ­ éšåï¼Œæ¥è‡ªESMå›¢é˜Ÿçš„æ›´å¤šå¥½æ¶ˆæ¯ä¼ æ¥ï¼š[Verkuilç­‰](https://www.biorxiv.org/content/10.1101/2022.12.21.521521v1)å‘ç°ESM-2å¯ä»¥ç”Ÿæˆ*de
    novo*è›‹ç™½è´¨åºåˆ—ï¼Œè¿™äº›åºåˆ—å®é™…ä¸Šå¯ä»¥åœ¨å®éªŒå®¤ä¸­åˆæˆï¼Œå¹¶ä¸”æ›´é‡è¦çš„æ˜¯ï¼Œå®ƒä»¬åœ¨å·²çŸ¥çš„è‡ªç„¶è›‹ç™½è´¨ä¸­æ²¡æœ‰ä»»ä½•åŒ¹é…ã€‚[Hieç­‰](https://www.biorxiv.org/content/10.1101/2022.12.21.521526v1)æå‡ºäº†ä¸€ç§å…¨æ–°çš„è›‹ç™½è´¨è®¾è®¡ç¼–ç¨‹è¯­è¨€ï¼ˆå¯ä»¥æŠŠå®ƒçœ‹ä½œæ˜¯ESMFoldçš„æŸ¥è¯¢è¯­è¨€ï¼‰â€”â€”ç”Ÿäº§è§„åˆ™ä»¥çº¦æŸå‡½æ•°çš„è¯­æ³•æ ‘å½¢å¼ç»„ç»‡ã€‚ç„¶åï¼Œæ¯ä¸ªç¨‹åºè¢«â€œç¼–è¯‘â€ä¸ºä¸€ä¸ªæ§åˆ¶ç”Ÿæˆè¿‡ç¨‹çš„èƒ½é‡å‡½æ•°ã€‚Meta
    AIè¿˜å‘å¸ƒäº†æœ€å¤§çš„[Metagenomic Atlas](https://esmatlas.com/)ï¼Œä½†æ›´å¤šå†…å®¹è¯·å‚è§æœ¬æ–‡çš„**æ•°æ®é›†**éƒ¨åˆ†ã€‚
- en: In the antibody design area, a similar LM-based approach is taken by **IgLM**
    by [Shuai, Ruffolo, and Gray](https://www.biorxiv.org/content/10.1101/2021.12.13.472419v2).
    IGLM generates antibody sequences conditioned on chain and species id tags.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æŠ—ä½“è®¾è®¡é¢†åŸŸï¼Œ**IgLM** é‡‡ç”¨äº†ç±»ä¼¼çš„åŸºäº LM çš„æ–¹æ³•ï¼Œå¦‚ [Shuaiã€Ruffolo å’Œ Gray](https://www.biorxiv.org/content/10.1101/2021.12.13.472419v2)
    æ‰€è¿°ã€‚IGLM ç”ŸæˆåŸºäºé“¾å’Œç‰©ç§ ID æ ‡ç­¾çš„æŠ—ä½“åºåˆ—ã€‚
- en: 'Finally, weâ€™d highlight a few works from Jian Tangâ€™s lab at Mila. **MoleculeSTM**
    by [Liu et al.](https://arxiv.org/abs/2212.10789) is a CLIP-like text-to-molecule
    model (plus a new large pre-training dataset). MoleculeSTM can do 2 impressive
    things: (1) retrieve molecules by text description like â€œtriazole derivativesâ€
    and retrieve text description from a given molecule in SMILES, (2) molecule editing
    from text prompts like â€œmake the molecule soluble in water with low permeabilityâ€
    â€” and the model edits the molecular graph according to the description, mindblowing
    ğŸ¤¯'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬è¦ç‰¹åˆ«æåˆ° Jian Tang å®éªŒå®¤åœ¨ Mila çš„ä¸€äº›å·¥ä½œã€‚ç”± [Liu ç­‰äºº](https://arxiv.org/abs/2212.10789)
    æå‡ºçš„ **MoleculeSTM** æ˜¯ä¸€ä¸ªç±»ä¼¼ CLIP çš„æ–‡æœ¬åˆ°åˆ†å­æ¨¡å‹ï¼ˆä»¥åŠä¸€ä¸ªæ–°çš„å¤§å‹é¢„è®­ç»ƒæ•°æ®é›†ï¼‰ã€‚MoleculeSTM èƒ½åšåˆ°ä¸¤ä»¶ä»¤äººå°è±¡æ·±åˆ»çš„äº‹æƒ…ï¼šï¼ˆ1ï¼‰é€šè¿‡æ–‡æœ¬æè¿°å¦‚â€œå™»å”‘è¡ç”Ÿç‰©â€æ£€ç´¢åˆ†å­ï¼Œå¹¶ä»ç»™å®šçš„
    SMILES åˆ†å­ä¸­æ£€ç´¢æ–‡æœ¬æè¿°ï¼›ï¼ˆ2ï¼‰æ ¹æ®æ–‡æœ¬æç¤ºè¿›è¡Œåˆ†å­ç¼–è¾‘ï¼Œå¦‚â€œä½¿åˆ†å­åœ¨æ°´ä¸­æº¶è§£ä¸”æ¸—é€æ€§ä½â€â€”â€”æ¨¡å‹æ ¹æ®æè¿°ç¼–è¾‘åˆ†å­å›¾è°±ï¼Œä»¤äººç ç›®ç»“èˆŒ ğŸ¤¯
- en: Then, **ProtSEED** by [Shi et al.](https://arxiv.org/abs/2210.08761) is a generative
    model for protein sequence *and* structure simultaneously (for example, most existing
    diffusion models for proteins can do only one of those at a time). ProtSEED can
    be conditioned on residue features or pairs of residues. Model-wise, it is an
    equivariant iterative model with improved triangular attention. ProtSEED was evaluated
    on Antibody CDR co-design, Protein sequence-structure co-design, and Fixed backbone
    sequence design.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œ[Shi ç­‰äºº](https://arxiv.org/abs/2210.08761) æå‡ºçš„ **ProtSEED** æ˜¯ä¸€ä¸ªåŒæ—¶ç”Ÿæˆè›‹ç™½è´¨åºåˆ—
    *å’Œ* ç»“æ„çš„æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼Œå¤§å¤šæ•°ç°æœ‰çš„è›‹ç™½è´¨æ‰©æ•£æ¨¡å‹ä¸€æ¬¡åªèƒ½å¤„ç†å…¶ä¸­ä¹‹ä¸€ï¼‰ã€‚ProtSEED å¯ä»¥åŸºäºæ®‹åŸºç‰¹å¾æˆ–æ®‹åŸºå¯¹è¿›è¡Œæ¡ä»¶è®¾ç½®ã€‚ä»æ¨¡å‹çš„è§’åº¦çœ‹ï¼Œå®ƒæ˜¯ä¸€ä¸ªå…·æœ‰æ”¹è¿›çš„ä¸‰è§’æ³¨æ„åŠ›çš„å¯¹ç§°è¿­ä»£æ¨¡å‹ã€‚ProtSEED
    åœ¨æŠ—ä½“ CDR å…±åŒè®¾è®¡ã€è›‹ç™½è´¨åºåˆ—-ç»“æ„å…±åŒè®¾è®¡å’Œå›ºå®šéª¨æ¶åºåˆ—è®¾è®¡ä¸­è¿›è¡Œäº†è¯„ä¼°ã€‚
- en: '![](../Images/e54ef61a84ca2aeca7a1b707387881e2.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e54ef61a84ca2aeca7a1b707387881e2.png)'
- en: 'Molecule editing from text inputs. Source: [Liu et al.](https://arxiv.org/abs/2212.10789)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ–‡æœ¬è¾“å…¥ä¸­è¿›è¡Œåˆ†å­ç¼–è¾‘ã€‚æ¥æºï¼š[Liu ç­‰äºº](https://arxiv.org/abs/2212.10789)
- en: Besides generating the protein structures, there are also some works for generating
    protein sequences from structures, known as inverse folding. Donâ€™t forget to check
    out the [ESM-IF1](https://www.biorxiv.org/content/10.1101/2022.04.10.487779v2)
    from Meta and the [ProteinMPNN](https://www.science.org/doi/full/10.1126/science.add2187)
    from the Baker Lab.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†ç”Ÿæˆè›‹ç™½è´¨ç»“æ„å¤–ï¼Œè¿˜æœ‰ä¸€äº›å·¥ä½œè‡´åŠ›äºä»ç»“æ„ç”Ÿæˆè›‹ç™½è´¨åºåˆ—ï¼Œè¿™è¢«ç§°ä¸ºé€†æŠ˜å ã€‚ä¸è¦å¿˜è®°æŸ¥çœ‹ Meta çš„ [ESM-IF1](https://www.biorxiv.org/content/10.1101/2022.04.10.487779v2)
    å’Œ Baker å®éªŒå®¤çš„ [ProteinMPNN](https://www.science.org/doi/full/10.1126/science.add2187)ã€‚
- en: '**What to expect in 2023**: (1) performance improvements of diffusion models
    such as faster sampling and more efficient solvers; (2) more powerful conditional
    protein generation models; (3) more successful applications of [Generative Flow
    Networks](https://arxiv.org/abs/2111.09266) (GFlowNets, check out the [tutorial](https://milayb.notion.site/The-GFlowNet-Tutorial-95434ef0e2d94c24aab90e69b30be9b3))
    to molecules and proteins.'
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2023 å¹´çš„é¢„æœŸ**ï¼šï¼ˆ1ï¼‰æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½æ”¹è¿›ï¼Œä¾‹å¦‚æ›´å¿«çš„é‡‡æ ·å’Œæ›´é«˜æ•ˆçš„æ±‚è§£å™¨ï¼›ï¼ˆ2ï¼‰æ›´å¼ºå¤§çš„æ¡ä»¶è›‹ç™½è´¨ç”Ÿæˆæ¨¡å‹ï¼›ï¼ˆ3ï¼‰å¯¹ [ç”Ÿæˆæµç½‘ç»œ](https://arxiv.org/abs/2111.09266)ï¼ˆGFlowNetsï¼ŒæŸ¥çœ‹
    [æ•™ç¨‹](https://milayb.notion.site/The-GFlowNet-Tutorial-95434ef0e2d94c24aab90e69b30be9b3)ï¼‰åœ¨åˆ†å­å’Œè›‹ç™½è´¨ä¸­çš„æ›´æˆåŠŸåº”ç”¨ã€‚'
- en: '**DFTs, ML Force Fields, Materials, and Weather Simulations**'
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**DFTsã€ML åŠ›åœºã€ææ–™å’Œå¤©æ°”æ¨¡æ‹Ÿ**'
- en: AI4Science becomes the frontier of equivariant GNN research and its applications.
    Pairing GNNs with PDEs, we can now tackle much more complex prediction tasks.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: AI4Science æˆä¸ºå¯¹ç§° GNN ç ”ç©¶åŠå…¶åº”ç”¨çš„å‰æ²¿ã€‚é€šè¿‡å°† GNN ä¸ PDEs é…å¯¹ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥å¤„ç†æ›´å¤æ‚çš„é¢„æµ‹ä»»åŠ¡ã€‚
- en: In 2022, this frontier expanded to ML-based **Density Functional Theory** (DFT)
    and **Force fields** approximations used for **molecular dynamics** and **material
    discovery.** The other growing field is **Weather simulations**.
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 2022 å¹´ï¼Œè¿™ä¸€å‰æ²¿é¢†åŸŸæ‰©å±•åˆ°äº†ç”¨äº **åˆ†å­åŠ¨åŠ›å­¦** å’Œ **ææ–™å‘ç°** çš„åŸºäº ML çš„ **å¯†åº¦æ³›å‡½ç†è®º**ï¼ˆDFTï¼‰å’Œ **åŠ›åœº** è¿‘ä¼¼ã€‚å¦ä¸€ä¸ªä¸æ–­å¢é•¿çš„é¢†åŸŸæ˜¯
    **å¤©æ°”æ¨¡æ‹Ÿ**ã€‚
- en: We would recommend the [talk](https://www.youtube.com/watch?v=t7q_ZNrBghY) by
    Max Welling for a broader overview of AI4Science and what is now enabled by using
    Deep Learning in science.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ¨è Max Welling çš„ [è®²åº§](https://www.youtube.com/watch?v=t7q_ZNrBghY)ï¼Œä»¥è·å¾—å…³äº AI4Science
    çš„æ›´å¹¿æ³›æ¦‚è¿°ï¼Œä»¥åŠæ·±åº¦å­¦ä¹ åœ¨ç§‘å­¦ä¸­çš„åº”ç”¨ç°çŠ¶ã€‚
- en: Starting with models, 2022 has seen a surge in equivariant GNNs for molecular
    dynamics and simulations, e.g., building upon [NequIP](https://arxiv.org/abs/2101.03164),
    **Allegro** by [Musaelian, Batzner, et al.](https://arxiv.org/abs/2204.05249)
    or **MACE** by [Batatia et al.](https://arxiv.org/abs/2206.07697) The design space
    for such models is very large, so refer to the recent survey by [Batatia, Batzner,
    et al.](https://arxiv.org/abs/2205.06643) for an overview. A crucial component
    for most of them is the [**e3nn**](https://github.com/e3nn/e3nn) library (paper
    by [Geiger and Smidt](https://arxiv.org/abs/2207.09453)) and the notion of tensor
    product. We highly recommend a great [new course](https://uvagedl.github.io/)
    by Erik Bekkers on Group Equivariant Deep Learning to understand the mathematical
    foundations and catch up with the recent papers.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ¨¡å‹å¼€å§‹ï¼Œ2022å¹´è§è¯äº†ç­‰å˜GNNåœ¨åˆ†å­åŠ¨åŠ›å­¦å’Œæ¨¡æ‹Ÿä¸­çš„æ¿€å¢ï¼Œä¾‹å¦‚ï¼ŒåŸºäº[NequIP](https://arxiv.org/abs/2101.03164)ã€ç”±[Musaelianã€Batznerç­‰äºº](https://arxiv.org/abs/2204.05249)æå‡ºçš„**Allegro**æˆ–ç”±[Batatia
    et al.](https://arxiv.org/abs/2206.07697)æå‡ºçš„**MACE**ã€‚è¿™äº›æ¨¡å‹çš„è®¾è®¡ç©ºé—´éå¸¸å¤§ï¼Œå› æ­¤å¯ä»¥å‚è€ƒ[Batatiaã€Batznerç­‰äºº](https://arxiv.org/abs/2205.06643)çš„æœ€æ–°ç»¼è¿°ä»¥è·å¾—æ¦‚è¿°ã€‚å¯¹äºå¤§å¤šæ•°æ¨¡å‹æ¥è¯´ï¼Œä¸€ä¸ªå…³é”®ç»„ä»¶æ˜¯[**e3nn**](https://github.com/e3nn/e3nn)åº“ï¼ˆ[Geigerå’ŒSmidt](https://arxiv.org/abs/2207.09453)çš„è®ºæ–‡ï¼‰å’Œå¼ é‡ç§¯çš„æ¦‚å¿µã€‚æˆ‘ä»¬å¼ºçƒˆæ¨èErik
    Bekkersæä¾›çš„ä¸€ä¸ªå‡ºè‰²çš„[æ–°è¯¾ç¨‹](https://uvagedl.github.io/)ï¼Œä»¥ç†è§£æ•°å­¦åŸºç¡€å¹¶è·Ÿä¸Šæœ€æ–°è®ºæ–‡ã€‚
- en: âš›ï¸ **Density Functional Theory** (DFT) calculations are one of the main workhorses
    of molecular dynamics (and account for a great deal of computing time in big clusters).
    DFT is O(nÂ³) to the input size though, so can ML help here? In *Learned Force
    Fields Are Ready For Ground State Catalyst Discovery,* [Schaarschmidt et al.](https://arxiv.org/abs/2209.12466)
    present the experimental study of models of learned potentials â€” turns out GNNs
    can do a very good job in linear O(n) time! The **Easy Potentials** approach (trained
    on Open Catalyst data) turns out to be quite a good predictor especially when
    paired with a postprocessing step. Model-wise, it is an MPNN with the [Noisy Nodes](https://arxiv.org/abs/2106.07971)
    self-supervised objective.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: âš›ï¸ **å¯†åº¦æ³›å‡½ç†è®º**ï¼ˆDFTï¼‰è®¡ç®—æ˜¯åˆ†å­åŠ¨åŠ›å­¦çš„ä¸»è¦å·¥å…·ä¹‹ä¸€ï¼ˆå¹¶ä¸”åœ¨å¤§å‹é›†ç¾¤ä¸­å æ®äº†å¤§é‡è®¡ç®—æ—¶é—´ï¼‰ã€‚ç„¶è€Œï¼ŒDFTçš„æ—¶é—´å¤æ‚åº¦æ˜¯O(nÂ³)ï¼Œé‚£ä¹ˆæœºå™¨å­¦ä¹ èƒ½å¦æœ‰æ‰€å¸®åŠ©å‘¢ï¼Ÿåœ¨*å­¦ä¹ åˆ°çš„åŠ›åœºå·²å‡†å¤‡å¥½ç”¨äºåŸºæ€å‚¬åŒ–å‰‚å‘ç°*ä¸­ï¼Œ[Schaarschmidt
    et al.](https://arxiv.org/abs/2209.12466)å±•ç¤ºäº†å­¦ä¹ åŠ¿èƒ½æ¨¡å‹çš„å®éªŒç ”ç©¶â€”â€”ç»“æœè¡¨æ˜GNNsåœ¨O(n)çº¿æ€§æ—¶é—´å†…å¯ä»¥è¡¨ç°å¾—éå¸¸å¥½ï¼**Easy
    Potentials**æ–¹æ³•ï¼ˆåŸºäºOpen Catalystæ•°æ®è®­ç»ƒï¼‰è¯æ˜æ˜¯ä¸€ä¸ªéå¸¸å¥½çš„é¢„æµ‹å™¨ï¼Œç‰¹åˆ«æ˜¯ä¸åå¤„ç†æ­¥éª¤é…åˆæ—¶ã€‚æ¨¡å‹æ–¹é¢ï¼Œå®ƒæ˜¯ä¸€ä¸ªå¸¦æœ‰[Noisy
    Nodes](https://arxiv.org/abs/2106.07971)è‡ªç›‘ç£ç›®æ ‡çš„MPNNã€‚
- en: In **Forces are not Enough**, [Fu et al.](https://arxiv.org/abs/2210.07237)
    introduce a new benchmark for molecular dynamics â€” in addition to MD17, the authors
    add datasets on modeling liquids (Water), peptides (Alanine dipeptide), and solid-state
    materials (LiPS). More importantly, the authors consider a wide range of physical
    properties like stability of simulations, diffusivity, and radial distribution
    functions. Most SOTA molecular dynamics models were probed including SchNet, ForceNet,
    DimeNet, GemNet (-T and -dT), and NequIP.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨**ã€ŠåŠ›é‡ä¸è¶³ã€‹**ä¸­ï¼Œ[Fu et al.](https://arxiv.org/abs/2210.07237)æå‡ºäº†ä¸€ä¸ªæ–°çš„åˆ†å­åŠ¨åŠ›å­¦åŸºå‡†â€”â€”é™¤äº†MD17ï¼Œä½œè€…ä»¬è¿˜æ·»åŠ äº†æ¶²ä½“ï¼ˆæ°´ï¼‰ã€è‚½ï¼ˆä¸™æ°¨é…¸äºŒè‚½ï¼‰å’Œå›ºæ€ææ–™ï¼ˆLiPSï¼‰çš„æ•°æ®é›†ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œä½œè€…ä»¬è€ƒè™‘äº†å¹¿æ³›çš„ç‰©ç†å±æ€§ï¼Œå¦‚æ¨¡æ‹Ÿçš„ç¨³å®šæ€§ã€æ‰©æ•£æ€§å’Œå¾„å‘åˆ†å¸ƒå‡½æ•°ã€‚åŒ…æ‹¬SchNetã€ForceNetã€DimeNetã€GemNetï¼ˆ-Tå’Œ-dTï¼‰ä»¥åŠNequIPåœ¨å†…çš„å¤§å¤šæ•°SOTAåˆ†å­åŠ¨åŠ›å­¦æ¨¡å‹éƒ½è¢«æ¢è®¨äº†ã€‚
- en: '![](../Images/b1ffbec28f6b6c061b8f5f66ff7fdc9e.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b1ffbec28f6b6c061b8f5f66ff7fdc9e.png)'
- en: 'Source: [Fu et al.](https://arxiv.org/abs/2210.07237)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ¥æº: [Fu et al.](https://arxiv.org/abs/2210.07237)'
- en: In crystal structure modeling, weâ€™d highlight **Equivariant Crystal Networks**
    by [Kaba and Ravanbakhsh](https://openreview.net/forum?id=0Dh8dz4snu) â€” a neat
    way to build representations of periodic structures with crystalline symmetries.
    Crystals can be described with *lattices* and *unit cells* with basis vectors
    that are subject to group transformations. Conceptually, ECN creates edge index
    masks corresponding to symmetry groups, performs message passing over this masked
    index, and aggregates the results of many symmetry groups.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ™¶ä½“ç»“æ„å»ºæ¨¡ä¸­ï¼Œæˆ‘ä»¬è¦ç‰¹åˆ«æåˆ°ç”±[Kabaå’ŒRavanbakhsh](https://openreview.net/forum?id=0Dh8dz4snu)æå‡ºçš„**ç­‰å˜æ™¶ä½“ç½‘ç»œ**â€”â€”ä¸€ç§ç”¨æ¥æ„å»ºå…·æœ‰æ™¶ä½“å¯¹ç§°æ€§çš„å‘¨æœŸæ€§ç»“æ„è¡¨ç¤ºçš„å·§å¦™æ–¹æ³•ã€‚æ™¶ä½“å¯ä»¥ç”¨*æ™¶æ ¼*å’Œ*å•ä½èƒ*æ¥æè¿°ï¼ŒåŸºå‘é‡å¯ä»¥è¿›è¡Œç¾¤å˜æ¢ã€‚æ¦‚å¿µä¸Šï¼ŒECNåˆ›å»ºäº†ä¸å¯¹ç§°ç¾¤å¯¹åº”çš„è¾¹ç´¢å¼•æ©ç ï¼Œå¯¹è¿™äº›æ©ç ç´¢å¼•è¿›è¡Œæ¶ˆæ¯ä¼ é€’ï¼Œå¹¶èšåˆå¤šä¸ªå¯¹ç§°ç¾¤çš„ç»“æœã€‚
- en: '![](../Images/e6422b43252d3ad7ca97e55de20b9931.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e6422b43252d3ad7ca97e55de20b9931.png)'
- en: 'Source: [Kaba and Ravanbakhsh](https://openreview.net/forum?id=0Dh8dz4snu)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ¥æº: [Kabaå’ŒRavanbakhsh](https://openreview.net/forum?id=0Dh8dz4snu)'
- en: Even more news on material discovery can found in the proceedings of the recent
    [AI4Mat NeurIPS workshop](https://sites.google.com/view/ai4mat)!
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºææ–™å‘ç°çš„æ›´å¤šæ¶ˆæ¯å¯ä»¥åœ¨æœ€è¿‘çš„ [AI4Mat NeurIPS workshop](https://sites.google.com/view/ai4mat)
    ä¼šè®®è®°å½•ä¸­æ‰¾åˆ°ï¼
- en: â˜‚ï¸ ML-based weather forecasting made a huge progress as well. In particular,
    [**GraphCast**](https://arxiv.org/abs/2212.12794) by DeepMind and [**Pangu-Weather**](https://arxiv.org/abs/2211.02556)
    by Huawei demonstrated exceptionally good results outperforming traditional models
    by a large margin. While Pangu-Weather leverages 3D/visual inputs and Visual Transformers,
    GraphCast employs a mesh MPNN where Earth is split into several hierarchy levels
    of meshes. The deepest level has about 40K nodes with 474 input features and the
    model outputs 227 predicted variables. The MPNN follows the â€œencoder-processor-decoderâ€
    and has 16 layers. GraphCast is autoregressive model w.r.t. the next timestep
    prediction, that is, it takes previous two states and predicts the next one. GraphCast
    can build a 10-day forecast in <60 seconds on a single TPUv4 and is much more
    accurate than non-ML forecasting models. ğŸ‘
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: â˜‚ï¸ åŸºäºæœºå™¨å­¦ä¹ çš„å¤©æ°”é¢„æŠ¥ä¹Ÿå–å¾—äº†å·¨å¤§çš„è¿›å±•ã€‚ç‰¹åˆ«æ˜¯ï¼ŒDeepMind çš„ [**GraphCast**](https://arxiv.org/abs/2212.12794)
    å’Œåä¸ºçš„ [**Pangu-Weather**](https://arxiv.org/abs/2211.02556) å±•ç¤ºäº†å‡ºè‰²çš„ç»“æœï¼Œå¤§å¤§è¶…è¶Šäº†ä¼ ç»Ÿæ¨¡å‹ã€‚è™½ç„¶
    Pangu-Weather åˆ©ç”¨ 3D/è§†è§‰è¾“å…¥å’Œè§†è§‰å˜æ¢å™¨ï¼ŒGraphCast åˆ™é‡‡ç”¨äº†ç½‘æ ¼ MPNNï¼Œå…¶ä¸­åœ°çƒè¢«åˆ†å‰²ä¸ºå¤šä¸ªå±‚çº§çš„ç½‘æ ¼ã€‚æœ€æ·±å±‚æœ‰å¤§çº¦ 40K
    èŠ‚ç‚¹ï¼Œå…·æœ‰ 474 ä¸ªè¾“å…¥ç‰¹å¾ï¼Œæ¨¡å‹è¾“å‡º 227 ä¸ªé¢„æµ‹å˜é‡ã€‚MPNN éµå¾ªâ€œç¼–ç å™¨-å¤„ç†å™¨-è§£ç å™¨â€ç»“æ„ï¼Œå¹¶å…·æœ‰ 16 å±‚ã€‚GraphCast æ˜¯ä¸€ä¸ªè‡ªå›å½’æ¨¡å‹ï¼Œç›¸å¯¹äºä¸‹ä¸€ä¸ªæ—¶é—´æ­¥é¢„æµ‹ï¼Œå³å®ƒåˆ©ç”¨å‰ä¸¤ä¸ªçŠ¶æ€é¢„æµ‹ä¸‹ä¸€ä¸ªçŠ¶æ€ã€‚GraphCast
    å¯ä»¥åœ¨å•ä¸ª TPUv4 ä¸Šåœ¨ <60 ç§’å†…å»ºç«‹ 10 å¤©çš„é¢„æµ‹ï¼Œå¹¶ä¸”æ¯”éæœºå™¨å­¦ä¹ é¢„æµ‹æ¨¡å‹å‡†ç¡®å¾—å¤šã€‚ğŸ‘
- en: '![](../Images/642385acd4eeb71b13131b45b29d0697.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/642385acd4eeb71b13131b45b29d0697.png)'
- en: 'Encoder-Processor-Decoder mesh MPNN in GraphCast. Source: [Lam, Sanchez-Gonzalez,
    Willson, Wirnsberger, Fortunato, Pritzel, et al.](https://arxiv.org/abs/2212.12794)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: GraphCast ä¸­çš„ç¼–ç å™¨-å¤„ç†å™¨-è§£ç å™¨ç½‘æ ¼ MPNNã€‚æ¥æºï¼š[Lam, Sanchez-Gonzalez, Willson, Wirnsberger,
    Fortunato, Pritzel ç­‰](https://arxiv.org/abs/2212.12794)
- en: '**What to expect in 2023**: We expect to see a lot more focus on computational
    efficiency and scalability of GNNs. Current GNN-based force-fields are obtaining
    remarkable accuracy, but are still 2â€“3 orders of magnitude slower than classical
    force-fields and are typically only deployed on a few hundred atoms. For GNNs
    to truly have a transformative impact on materials science and drug discovery,
    we will see many folks tackling this issue, be it through architectural advances
    or smarter sampling.'
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2023å¹´çš„æœŸå¾…**ï¼šæˆ‘ä»¬æœŸæœ›çœ‹åˆ°æ›´å¤šå…³æ³¨ GNN çš„è®¡ç®—æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚ç›®å‰åŸºäº GNN çš„åŠ›åœºå–å¾—äº†æ˜¾è‘—çš„å‡†ç¡®æ€§ï¼Œä½†ä»æ¯”ç»å…¸åŠ›åœºæ…¢ 2-3 ä¸ªæ•°é‡çº§ï¼Œå¹¶ä¸”é€šå¸¸åªéƒ¨ç½²åœ¨å‡ ç™¾ä¸ªåŸå­ä¸Šã€‚ä¸ºäº†ä½¿
    GNN åœ¨ææ–™ç§‘å­¦å’Œè¯ç‰©å‘ç°ä¸­çœŸæ­£å‘æŒ¥å˜é©æ€§å½±å“ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°è®¸å¤šäººè‡´åŠ›äºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ— è®ºæ˜¯é€šè¿‡æ¶æ„è¿›æ­¥è¿˜æ˜¯æ›´æ™ºèƒ½çš„é‡‡æ ·æ–¹æ³•ã€‚'
- en: Geometry & Topology & PDEs
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‡ ä½•å­¦ & æ‹“æ‰‘å­¦ & PDEs
- en: In 2022, 1ï¸âƒ£ we got a better understanding of oversmoothing and oversquashing
    phenomena in GNNs and their connections to algebraic topology; 2ï¸âƒ£ using GNNs
    for PDE modeling is now mainstream.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ 2022 å¹´ï¼Œ1ï¸âƒ£ æˆ‘ä»¬å¯¹ GNN ä¸­çš„è¿‡åº¦å¹³æ»‘å’Œè¿‡åº¦å‹ç¼©ç°è±¡åŠå…¶ä¸ä»£æ•°æ‹“æ‰‘çš„å…³ç³»æœ‰äº†æ›´å¥½çš„ç†è§£ï¼›2ï¸âƒ£ ä½¿ç”¨ GNN è¿›è¡Œ PDE å»ºæ¨¡ç°åœ¨å·²ç»æˆä¸ºä¸»æµã€‚
- en: 1ï¸âƒ£ Michael Bronsteinâ€™s lab made huge contributions to this problem â€” check
    those excellent posts on Neural Sheaf Diffusion and framing GNNs as gradient flows
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ Michael Bronstein çš„å®éªŒå®¤å¯¹è¿™ä¸ªé—®é¢˜åšå‡ºäº†å·¨å¤§è´¡çŒ® â€”â€” æŸ¥çœ‹é‚£äº›å…³äºç¥ç»åˆ‡ç‰‡æ‰©æ•£å’Œå°† GNN è§†ä¸ºæ¢¯åº¦æµçš„ä¼˜ç§€å¸–å­ã€‚
- en: '[](/neural-sheaf-diffusion-for-deep-learning-on-graphs-bfa200e6afa6?source=post_page-----1ba920cb9232--------------------------------)
    [## Neural Sheaf Diffusion for deep learning on graphs'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/neural-sheaf-diffusion-for-deep-learning-on-graphs-bfa200e6afa6?source=post_page-----1ba920cb9232--------------------------------)
    [## ç¥ç»åˆ‡ç‰‡æ‰©æ•£åœ¨å›¾ä¸Šçš„æ·±åº¦å­¦ä¹ '
- en: Cellular sheaf theory, a branch of algebraic topology, provides new insights
    into how Graph Neural Networks work andâ€¦
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç»†èƒåˆ‡ç‰‡ç†è®ºï¼Œä½œä¸ºä»£æ•°æ‹“æ‰‘çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä¸ºå›¾ç¥ç»ç½‘ç»œçš„å·¥ä½œåŸç†æä¾›äº†æ–°çš„è§è§£â€¦
- en: towardsdatascience.com](/neural-sheaf-diffusion-for-deep-learning-on-graphs-bfa200e6afa6?source=post_page-----1ba920cb9232--------------------------------)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/neural-sheaf-diffusion-for-deep-learning-on-graphs-bfa200e6afa6?source=post_page-----1ba920cb9232--------------------------------)'
- en: 'And on GNNs as gradient flows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥åŠå…³äº GNN ä½œä¸ºæ¢¯åº¦æµçš„å†…å®¹ï¼š
- en: '[](/graph-neural-networks-as-gradient-flows-4dae41fb2e8a?source=post_page-----1ba920cb9232--------------------------------)
    [## Graph Neural Networks as gradient flows'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/graph-neural-networks-as-gradient-flows-4dae41fb2e8a?source=post_page-----1ba920cb9232--------------------------------)
    [## å›¾ç¥ç»ç½‘ç»œä½œä¸ºæ¢¯åº¦æµ'
- en: GNNs derived as gradient flows minimising a learnable energy that describes
    attractive and repulsive forces betweenâ€¦
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GNN ä½œä¸ºæ¢¯åº¦æµçš„è¡ç”Ÿç‰©ï¼Œé€šè¿‡æœ€å°åŒ–æè¿°å¸å¼•åŠ›å’Œæ’æ–¥åŠ›çš„å¯å­¦ä¹ èƒ½é‡æ¥ä¼˜åŒ–â€¦
- en: towardsdatascience.com](/graph-neural-networks-as-gradient-flows-4dae41fb2e8a?source=post_page-----1ba920cb9232--------------------------------)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/graph-neural-networks-as-gradient-flows-4dae41fb2e8a?source=post_page-----1ba920cb9232--------------------------------)'
- en: 2ï¸âƒ£ Using GNNs for PDE modeling became a mainstream topic. Some papers require
    the ğŸ¤¯ **math alert** ğŸ¤¯ warning, but if you are familiar with the basics of ODEs
    and PDEs it should be much easier.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ ä½¿ç”¨ GNNs è¿›è¡Œ PDE å»ºæ¨¡å·²æˆä¸ºä¸»æµè¯é¢˜ã€‚ä¸€äº›è®ºæ–‡éœ€è¦ ğŸ¤¯ **æ•°å­¦è­¦æŠ¥** ğŸ¤¯ æç¤ºï¼Œä½†å¦‚æœä½ å¯¹ ODEs å’Œ PDEs çš„åŸºç¡€çŸ¥è¯†å¾ˆç†Ÿæ‚‰ï¼Œè¿™åº”è¯¥ä¼šæ›´å®¹æ˜“ã€‚
- en: '*Message Passing Neural PDE Solvers* by [Brandstetter, Worrall, and Welling](https://openreview.net/forum?id=vSix3HPYKSU)
    describe how message passing can help solving PDEs, generalize better, and get
    rid of manual heuristics. Furthermore, MP-PDEs representationally contain classic
    solvers like finite differences.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ¶ˆæ¯ä¼ é€’ç¥ç» PDE æ±‚è§£å™¨* ç”± [Brandstetter, Worrall, å’Œ Welling](https://openreview.net/forum?id=vSix3HPYKSU)
    æè¿°äº†æ¶ˆæ¯ä¼ é€’å¦‚ä½•å¸®åŠ©è§£å†³ PDEï¼Œå…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶æ‘†è„±æ‰‹åŠ¨å¯å‘å¼ã€‚æ­¤å¤–ï¼ŒMP-PDEs åœ¨è¡¨ç¤ºä¸ŠåŒ…å«ç»å…¸æ±‚è§£å™¨ï¼Œå¦‚æœ‰é™å·®åˆ†ã€‚'
- en: '![](../Images/4e9be60b522337c75d2f485d5e19ed8d.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4e9be60b522337c75d2f485d5e19ed8d.png)'
- en: 'Source: [Brandstetter, Worrall, and Welling](https://openreview.net/forum?id=vSix3HPYKSU)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ¥æº: [Brandstetter, Worrall, å’Œ Welling](https://openreview.net/forum?id=vSix3HPYKSU)'
- en: The topic was developed further by many recent works including continuous forecasting
    with implicit neural representations ([Yin et al.](https://arxiv.org/abs/2209.14855)),
    supporting mixed boundary conditions ([Horie and Mitsume](https://openreview.net/forum?id=B3TOg-YCtzo)),
    or latent evolution of PDEs ([Wu et al.](https://arxiv.org/abs/2206.07681))
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥ä¸»é¢˜é€šè¿‡è®¸å¤šè¿‘æœŸå·¥ä½œå¾—åˆ°äº†è¿›ä¸€æ­¥å‘å±•ï¼ŒåŒ…æ‹¬ä½¿ç”¨éšå¼ç¥ç»è¡¨ç¤ºè¿›è¡Œè¿ç»­é¢„æµ‹ ([Yin et al.](https://arxiv.org/abs/2209.14855))ã€æ”¯æŒæ··åˆè¾¹ç•Œæ¡ä»¶
    ([Horie and Mitsume](https://openreview.net/forum?id=B3TOg-YCtzo))ï¼Œæˆ– PDE çš„æ½œåœ¨æ¼”å˜
    ([Wu et al.](https://arxiv.org/abs/2206.07681))
- en: '**What to expect in 2023**: Neural PDEs and their applications are likely to
    expand to more physics-related AI4Science subfields, where especially computational
    fluid dynamics (CFD) will potentially be influenced by GNN based surrogates in
    the coming months. Classical CFD is applied to a wide range of research and engineering
    problems in many fields of study, including aerodynamics, hypersonic and environmental
    engineering, fluid flows, visual effects in video games, or weather simulations
    as discussed above. GNN based surrogates might augment/replace traditional well-tried
    techniques such as finite element methods ([Lienen et al.](https://arxiv.org/abs/2203.08852)),
    remeshing algorithms ([Song et al.](https://arxiv.org/abs/2204.11188)), boundary
    value problems ([Loetsch et al.](https://arxiv.org/abs/2206.14092)), or interactions
    with triangularized boundary geometries ([Mayr et al.](https://arxiv.org/abs/2106.11299)).'
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2023 å¹´çš„æœŸå¾…**ï¼šç¥ç» PDE åŠå…¶åº”ç”¨æœ‰å¯èƒ½æ‰©å±•åˆ°æ›´å¤šä¸ç‰©ç†ç›¸å…³çš„ AI4Science å­é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯è®¡ç®—æµä½“åŠ¨åŠ›å­¦ï¼ˆCFDï¼‰åœ¨æœªæ¥å‡ ä¸ªæœˆå¯èƒ½ä¼šå—åˆ°åŸºäº
    GNN çš„æ›¿ä»£å“çš„å½±å“ã€‚ç»å…¸çš„ CFD è¢«å¹¿æ³›åº”ç”¨äºå¤šä¸ªé¢†åŸŸçš„ç ”ç©¶å’Œå·¥ç¨‹é—®é¢˜ï¼ŒåŒ…æ‹¬ç©ºæ°”åŠ¨åŠ›å­¦ã€é«˜è¶…å£°é€Ÿå’Œç¯å¢ƒå·¥ç¨‹ã€æµä½“æµåŠ¨ã€è§†é¢‘æ¸¸æˆä¸­çš„è§†è§‰æ•ˆæœï¼Œæˆ–ä¸Šè¿°å¤©æ°”æ¨¡æ‹Ÿã€‚åŸºäº
    GNN çš„æ›¿ä»£å“å¯èƒ½ä¼šå¢å¼º/æ›¿ä»£ä¼ ç»Ÿçš„ç»è¿‡éªŒè¯çš„æŠ€æœ¯ï¼Œå¦‚æœ‰é™å…ƒæ–¹æ³• ([Lienen et al.](https://arxiv.org/abs/2203.08852))ã€é‡ç½‘æ ¼ç®—æ³•
    ([Song et al.](https://arxiv.org/abs/2204.11188))ã€è¾¹å€¼é—®é¢˜ ([Loetsch et al.](https://arxiv.org/abs/2206.14092))ï¼Œæˆ–ä¸ä¸‰è§’åŒ–è¾¹ç•Œå‡ ä½•çš„äº¤äº’
    ([Mayr et al.](https://arxiv.org/abs/2106.11299))ã€‚'
- en: ''
  id: totrans-88
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The neural PDE community is starting to build strong and commonly used baselines
    and frameworks, which will in return help to accelerate the progress, e.g. **PDEBench**
    ([Takamoto et al.](https://arxiv.org/abs/2210.07182)) or **PDEArena** ([Gupta
    et al.](https://arxiv.org/abs/2209.15616))
  id: totrans-89
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ç¥ç» PDE ç¤¾åŒºæ­£åœ¨å¼€å§‹å»ºç«‹å¼ºå¤§ä¸”å¸¸ç”¨çš„åŸºå‡†å’Œæ¡†æ¶ï¼Œè¿™å°†åè¿‡æ¥å¸®åŠ©åŠ é€Ÿè¿›å±•ï¼Œä¾‹å¦‚ **PDEBench** ([Takamoto et al.](https://arxiv.org/abs/2210.07182))
    æˆ– **PDEArena** ([Gupta et al.](https://arxiv.org/abs/2209.15616))
- en: Graph Transformers
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å›¾ç¥ç»ç½‘ç»œå˜æ¢å™¨
- en: 'Definitely one of the main community drivers in 2022, **graph transformers**
    (GTs) evolved a lot towards higher effectiveness and better scalability. Several
    outstanding models published in 2022:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ç»å¯¹æ˜¯ 2022 å¹´çš„ä¸»è¦ç¤¾åŒºæ¨åŠ¨åŠ›ä¹‹ä¸€ï¼Œ**å›¾ç¥ç»ç½‘ç»œå˜æ¢å™¨**ï¼ˆGTsï¼‰åœ¨æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§æ–¹é¢æœ‰äº†å¾ˆå¤§è¿›å±•ã€‚2022 å¹´å‘å¸ƒäº†å‡ ä¸ªæ°å‡ºçš„æ¨¡å‹ï¼š
- en: '**ğŸ‘‘ GraphGPS** by [RampÃ¡Å¡ek et al.](https://arxiv.org/abs/2205.12454) takes
    the title of **â€œGT of 2022â€** thanks to combining local message passing, global
    attention (optionally, linear for higher efficiency), and positional encodings
    that led to setting a new SOTA on ZINC and many other benchmarks. Check out a
    dedicated article on GraphGPS'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸ‘‘ GraphGPS** ç”± [RampÃ¡Å¡ek et al.](https://arxiv.org/abs/2205.12454) æå‡ºçš„ï¼Œå› å…¶ç»“åˆäº†å±€éƒ¨æ¶ˆæ¯ä¼ é€’ã€å…¨å±€æ³¨æ„åŠ›ï¼ˆå¯é€‰åœ°ï¼Œçº¿æ€§ä»¥æé«˜æ•ˆç‡ï¼‰å’Œä½ç½®ç¼–ç ï¼Œæœ€ç»ˆåœ¨
    ZINC å’Œè®¸å¤šå…¶ä»–åŸºå‡†ä¸Šè®¾ç«‹äº†æ–°çš„ SOTA æ ‡æ†ï¼Œè¢«èª‰ä¸º **â€œ2022 å¹´çš„ GTâ€**ã€‚æŸ¥çœ‹æœ‰å…³ GraphGPS çš„ä¸“é—¨æ–‡ç« '
- en: '[](/graphgps-navigating-graph-transformers-c2cc223a051c?source=post_page-----1ba920cb9232--------------------------------)
    [## GraphGPS: Navigating Graph Transformers'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/graphgps-navigating-graph-transformers-c2cc223a051c?source=post_page-----1ba920cb9232--------------------------------)
    [## GraphGPS: å¯¼èˆªå›¾å½¢å˜æ¢å™¨'
- en: Recipes for cooking the best graph transformers
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æœ€ä½³å›¾å½¢å˜æ¢å™¨çš„çƒ¹é¥ªç§˜è¯€
- en: towardsdatascience.com](/graphgps-navigating-graph-transformers-c2cc223a051c?source=post_page-----1ba920cb9232--------------------------------)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/graphgps-navigating-graph-transformers-c2cc223a051c?source=post_page-----1ba920cb9232--------------------------------)
- en: GraphGPS served as a backbone of **GPS++,** the [winning](https://ogb.stanford.edu/neurips2022/results/#winners_pcqm4mv2)
    OGB Large Scale Challenge 2022 model on PCQM4M v2 (graph regression). **GPS++**,
    [created by](https://arxiv.org/abs/2212.02229) Graphcore, Valence Discovery, and
    Mila, incorporates more features including 3D coordinates and leverages sparse-optimized
    IPU hardware (more on that in the following section). GPS++ weights are already
    [available](https://github.com/graphcore/ogb-lsc-pcqm4mv2) on GitHub!
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: GraphGPS ä½œä¸º **GPS++** çš„æ ¸å¿ƒï¼Œ[è·èƒœ](https://ogb.stanford.edu/neurips2022/results/#winners_pcqm4mv2)
    OGB å¤§è§„æ¨¡æŒ‘æˆ˜èµ› 2022 æ¨¡å‹åœ¨ PCQM4M v2ï¼ˆå›¾å›å½’ï¼‰ä¸Šè¡¨ç°çªå‡ºã€‚**GPS++**ï¼Œ[ç”±](https://arxiv.org/abs/2212.02229)
    Graphcoreã€Valence Discovery å’Œ Mila åˆ›å»ºï¼Œæ•´åˆäº†æ›´å¤šç‰¹æ€§ï¼ŒåŒ…æ‹¬ 3D åæ ‡ï¼Œå¹¶åˆ©ç”¨äº†ç¨€ç–ä¼˜åŒ–çš„ IPU ç¡¬ä»¶ï¼ˆæ›´å¤šä¿¡æ¯è§ä¸‹ä¸€èŠ‚ï¼‰ã€‚GPS++
    æƒé‡å·²åœ¨ GitHub ä¸Š[å¯ç”¨](https://github.com/graphcore/ogb-lsc-pcqm4mv2)ï¼
- en: '![](../Images/030d6d10617c7e1782ce0d078c915c21.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/030d6d10617c7e1782ce0d078c915c21.png)'
- en: 'GraphGPS intuition. Source: [RampÃ¡Å¡ek et al](https://arxiv.org/abs/2205.12454)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: GraphGPS ç›´è§‚ç†è§£ã€‚æ¥æºï¼š[RampÃ¡Å¡ek et al](https://arxiv.org/abs/2205.12454)
- en: '**Transformer-M** by [Luo et al.](https://arxiv.org/abs/2210.01765) inspired
    many top OGB LSC models as well. Transformer-M adds 3D coordinates to the neat
    mix of joint 2D-3D pre-training. At inference time, when 3D info is not known,
    the model would infer a glimpse of 3D knowledge which improves the performance
    on PCQM4Mv2 by a good margin. Code is [available](https://github.com/lsj2408/Transformer-M)
    either.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**Transformer-M** ç”± [Luo et al.](https://arxiv.org/abs/2210.01765) æå‡ºçš„æ–¹æ³•ä¹Ÿå¯å‘äº†è®¸å¤šé¡¶çº§
    OGB LSC æ¨¡å‹ã€‚Transformer-M å°† 3D åæ ‡æ·»åŠ åˆ°æ•´æ´çš„ 2D-3D é¢„è®­ç»ƒæ··åˆä¸­ã€‚åœ¨æ¨ç†æ—¶ï¼Œå½“ 3D ä¿¡æ¯æœªçŸ¥æ—¶ï¼Œè¯¥æ¨¡å‹å°†æ¨æ–­å‡ºä¸€éƒ¨åˆ†
    3D çŸ¥è¯†ï¼Œä»è€Œæ˜¾è‘—æé«˜ PCQM4Mv2 çš„æ€§èƒ½ã€‚ä»£ç [å¯ç”¨](https://github.com/lsj2408/Transformer-M)ã€‚'
- en: '![](../Images/2b5d5841cfa0a2d34817e2a1d077b999.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b5d5841cfa0a2d34817e2a1d077b999.png)'
- en: '*Transformer-M joint 2D-3D pre-training scheme. Source:* [*Luo et al.*](https://arxiv.org/abs/2210.01765)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '*Transformer-M 2D-3D é¢„è®­ç»ƒæ–¹æ¡ˆã€‚æ¥æºï¼š* [*Luo et al.*](https://arxiv.org/abs/2210.01765)'
- en: '**TokenGT** by [Kim et al](https://arxiv.org/abs/2207.02505) goes even more
    explicit and adds all edges of the input graph (in addition to all nodes) to the
    sequence fed to the Transformer. With those inputs, encoder needs additional token
    types to distinguish nodes from edges. The authors prove several nice theoretical
    properties (although at the cost of higher computational complexity O((V+E)Â²)
    that can get to the 4th power in the worst case of a fully-connected graph). Code
    is [available](https://github.com/jw9730/tokengt).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**TokenGT** ç”± [Kim et al](https://arxiv.org/abs/2207.02505) æå‡ºçš„æ–¹æ³•æ›´åŠ æ˜ç¡®ï¼Œå¹¶å°†è¾“å…¥å›¾çš„æ‰€æœ‰è¾¹ï¼ˆé™¤äº†æ‰€æœ‰èŠ‚ç‚¹ï¼‰æ·»åŠ åˆ°å–‚ç»™
    Transformer çš„åºåˆ—ä¸­ã€‚ä½¿ç”¨è¿™äº›è¾“å…¥ï¼Œç¼–ç å™¨éœ€è¦é¢å¤–çš„æ ‡è®°ç±»å‹æ¥åŒºåˆ†èŠ‚ç‚¹å’Œè¾¹ã€‚ä½œè€…è¯æ˜äº†å‡ ä¸ªå¾ˆå¥½çš„ç†è®ºæ€§è´¨ï¼ˆå°½ç®¡ä»£ä»·æ˜¯è¾ƒé«˜çš„è®¡ç®—å¤æ‚åº¦ O((V+E)Â²)ï¼Œåœ¨å®Œå…¨è¿æ¥å›¾çš„æœ€åæƒ…å†µä¸‹å¯èƒ½è¾¾åˆ°å››æ¬¡æ–¹ï¼‰ã€‚ä»£ç [å¯ç”¨](https://github.com/jw9730/tokengt)ã€‚'
- en: '![](../Images/362b38baa863661789419ac3384b6568.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/362b38baa863661789419ac3384b6568.png)'
- en: 'TokenGT adds both nodes and edges to the input sequence. Source: [Kim et al](https://arxiv.org/abs/2207.02505)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: TokenGT å°†èŠ‚ç‚¹å’Œè¾¹éƒ½æ·»åŠ åˆ°è¾“å…¥åºåˆ—ä¸­ã€‚æ¥æºï¼š[Kim et al](https://arxiv.org/abs/2207.02505)
- en: '**What to expect in 2023**: for the coming year, weâ€™d expect 1ï¸âƒ£ GTs to scale
    up along the axes of both data and model parameters, from molecules of <50 nodes
    to graphs of millions of nodes, in order to witness the emergent behavior as in
    text & vision foundation models 2ï¸âƒ£ similar to [BLOOM](https://huggingface.co/bigscience/bloom)
    by the BigScience Initiative, a big open-source pre-trained equivariant GT for
    molecular data, perhaps within the [Open Drug Discovery](https://m2d2.io/opendrugdiscovery/)
    project.'
  id: totrans-105
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2023 å¹´çš„é¢„æœŸ**ï¼šåœ¨æ¥å¹´ï¼Œæˆ‘ä»¬é¢„è®¡ 1ï¸âƒ£ GTs åœ¨æ•°æ®å’Œæ¨¡å‹å‚æ•°ä¸¤ä¸ªç»´åº¦ä¸Šæ‰©å±•ï¼Œä»å°äº 50 èŠ‚ç‚¹çš„åˆ†å­åˆ°æ•°ç™¾ä¸‡èŠ‚ç‚¹çš„å›¾ï¼Œä»¥è§è¯å¦‚æ–‡æœ¬å’Œè§†è§‰åŸºç¡€æ¨¡å‹ä¸­çš„çªç°è¡Œä¸º
    2ï¸âƒ£ ç±»ä¼¼äº [BLOOM](https://huggingface.co/bigscience/bloom) ç”± BigScience Initiative
    æä¾›çš„ç”¨äºåˆ†å­æ•°æ®çš„å¤§å‹å¼€æºé¢„è®­ç»ƒç­‰å˜ GTï¼Œä¹Ÿè®¸åœ¨ [Open Drug Discovery](https://m2d2.io/opendrugdiscovery/)
    é¡¹ç›®ä¸­ã€‚'
- en: BIG Graphs
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤§å›¾
- en: ğŸ”¥ One of our favorites in 2022 is *â€œGraph Neural Networks for Link Prediction
    with Subgraph Sketching*â€ by [Chamberlain, Shirobokov et al.](https://arxiv.org/abs/2209.15486)
    â€” this is a neat combination of algorithms + ML techniques. It is known that [SEAL](https://arxiv.org/pdf/2010.16103.pdf)-like
    labeling tricks dramatically improve link prediction performance compared to standard
    GNN encoders but suffer from big computation/memory overhead. In this work, the
    authors find that obtaining distances from two nodes of a query edge can be efficiently
    done with hashing ([MinHashing](https://en.wikipedia.org/wiki/MinHash)) and cardinality
    estimation ([HyperLogLog](https://en.wikipedia.org/wiki/HyperLogLog)) algorithms.
    Essentially, message passing is done over *minhashing* and *hyperloglog* initial
    sketches of single nodes (*min* aggregation for minhash, *max* for hyperloglog
    sketches) â€” this is the core of the **ELPH** link prediction model (with a simple
    MLP decoder). The authors then design a more scalable **BUDDY** model where k-hop
    hash propagation can be precomputed before training. Experimentally, ELPH and
    BUDDY scale to large graphs that were previously way too large or resource hungry
    for labeling trick approaches. Great work and definitely a solid baseline for
    all future link prediction models! ğŸ‘
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”¥ æˆ‘ä»¬åœ¨ 2022 å¹´æœ€å–œæ¬¢çš„ä¸€ç¯‡æ˜¯[Chamberlain, Shirobokov ç­‰](https://arxiv.org/abs/2209.15486)çš„
    *â€œåŸºäºå­å›¾è‰å›¾çš„é“¾æ¥é¢„æµ‹å›¾ç¥ç»ç½‘ç»œ*â€â€”â€”è¿™æ˜¯ç®—æ³•ä¸ ML æŠ€æœ¯çš„å·§å¦™ç»“åˆã€‚ä¼—æ‰€å‘¨çŸ¥ï¼Œ[SEAL](https://arxiv.org/pdf/2010.16103.pdf)
    ç±»çš„æ ‡è®°æŠ€å·§ç›¸æ¯”äºæ ‡å‡† GNN ç¼–ç å™¨å¯ä»¥æ˜¾è‘—æå‡é“¾æ¥é¢„æµ‹æ€§èƒ½ï¼Œä½†å´é¢ä¸´å·¨å¤§çš„è®¡ç®—/å†…å­˜å¼€é”€ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œä½œè€…å‘ç°åˆ©ç”¨å“ˆå¸Œ ([MinHashing](https://en.wikipedia.org/wiki/MinHash))
    å’ŒåŸºæ•°ä¼°è®¡ ([HyperLogLog](https://en.wikipedia.org/wiki/HyperLogLog)) ç®—æ³•ï¼Œå¯ä»¥é«˜æ•ˆåœ°è·å–æŸ¥è¯¢è¾¹ä¸¤ä¸ªèŠ‚ç‚¹ä¹‹é—´çš„è·ç¦»ã€‚æœ¬è´¨ä¸Šï¼Œæ¶ˆæ¯ä¼ é€’æ˜¯åœ¨
    *minhashing* å’Œ *hyperloglog* å•èŠ‚ç‚¹çš„åˆæ­¥è‰å›¾ï¼ˆ*min* èšåˆç”¨äº minhashï¼Œ*max* ç”¨äº hyperloglog
    è‰å›¾ï¼‰ä¸Šè¿›è¡Œçš„â€”â€”è¿™å°±æ˜¯**ELPH** é“¾æ¥é¢„æµ‹æ¨¡å‹çš„æ ¸å¿ƒï¼ˆå¸¦æœ‰ç®€å•çš„ MLP è§£ç å™¨ï¼‰ã€‚ç„¶åï¼Œä½œè€…è®¾è®¡äº†ä¸€ä¸ªæ›´å…·å¯æ‰©å±•æ€§çš„ **BUDDY** æ¨¡å‹ï¼Œå…¶ä¸­
    k-hop å“ˆå¸Œä¼ æ’­å¯ä»¥åœ¨è®­ç»ƒå‰é¢„è®¡ç®—ã€‚å®éªŒè¡¨æ˜ï¼ŒELPH å’Œ BUDDY å¯ä»¥æ‰©å±•åˆ°ä»¥å‰è¿‡äºåºå¤§æˆ–èµ„æºæ¶ˆè€—è¿‡å¤§çš„å›¾ä¸­ã€‚å‡ºè‰²çš„å·¥ä½œï¼Œç»å¯¹æ˜¯æœªæ¥æ‰€æœ‰é“¾æ¥é¢„æµ‹æ¨¡å‹çš„åšå®åŸºçº¿ï¼ğŸ‘
- en: '![](../Images/0a1e1985708d039dbfd156a1aef9ac06.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a1e1985708d039dbfd156a1aef9ac06.png)'
- en: 'The motivation behind computing subgraph hashes to estimate cardinalities of
    neighborhoods and intersections. Source: [Chamberlain, Shirobokov et al.](https://arxiv.org/abs/2209.15486)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—å­å›¾å“ˆå¸Œä»¥ä¼°è®¡é‚»åŸŸå’Œäº¤é›†åŸºæ•°çš„åŠ¨æœºã€‚æ¥æºï¼š[Chamberlain, Shirobokov ç­‰](https://arxiv.org/abs/2209.15486)
- en: On the graph sampling and minibatching side, [Gasteiger, Qian, and GÃ¼nnemann](https://openreview.net/forum?id=b9g0vxzYa_)
    design [**Influence-based Mini-Batching (IBMB)**](https://github.com/tum-daml/ibmb),
    a good example how Personalized PageRank (PPR) can solve even graph batching!
    IBMB aims at creating the smallest minibatches whose nodes have the maximum influence
    on the node classification task. In fact, the influence score is equivalent to
    PPR. Practically, given a set of target nodes, IBMB (1) partitions the graph into
    permanent clusters, (2) runs PPR within each batch to select top-PPR nodes that
    would form a final subgraph minibatch. The resulting minibatches can be sent to
    any GNN encoder. IBMB is pretty much **constant** O(1) to the graph size where
    partitioning and PPRs can be precomputed at the pre-processing stage.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å›¾é‡‡æ ·å’Œè¿·ä½ æ‰¹å¤„ç†æ–¹é¢ï¼Œ[Gasteiger, Qian, å’Œ GÃ¼nnemann](https://openreview.net/forum?id=b9g0vxzYa_)
    è®¾è®¡äº†[**åŸºäºå½±å“åŠ›çš„è¿·ä½ æ‰¹å¤„ç† (IBMB)**](https://github.com/tum-daml/ibmb)ï¼Œè¿™æ˜¯ä¸ªå¾ˆå¥½çš„ä¾‹å­ï¼Œè¯´æ˜ä¸ªæ€§åŒ–PageRank
    (PPR) å¦‚ä½•è§£å†³å›¾æ‰¹å¤„ç†é—®é¢˜ï¼IBMB æ—¨åœ¨åˆ›å»ºå¯¹èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡å½±å“æœ€å¤§çš„æœ€å°è¿·ä½ æ‰¹æ¬¡ã€‚å®é™…ä¸Šï¼Œå½±å“åŠ›åˆ†æ•°ç­‰åŒäº PPRã€‚å®é™…ä¸Šï¼Œç»™å®šä¸€ç»„ç›®æ ‡èŠ‚ç‚¹ï¼ŒIBMB
    (1) å°†å›¾åˆ’åˆ†ä¸ºæ°¸ä¹…é›†ç¾¤ï¼Œ(2) åœ¨æ¯ä¸ªæ‰¹æ¬¡å†…è¿è¡Œ PPRï¼Œé€‰æ‹© top-PPR èŠ‚ç‚¹ä»¥å½¢æˆæœ€ç»ˆçš„å­å›¾è¿·ä½ æ‰¹æ¬¡ã€‚ç”Ÿæˆçš„è¿·ä½ æ‰¹æ¬¡å¯ä»¥å‘é€åˆ°ä»»ä½• GNN ç¼–ç å™¨ã€‚IBMB
    å¯¹å›¾çš„å¤§å°å‡ ä¹æ˜¯**å¸¸æ•°** O(1)ï¼Œå› ä¸ºåˆ†åŒºå’Œ PPR å¯ä»¥åœ¨é¢„å¤„ç†é˜¶æ®µé¢„è®¡ç®—ã€‚
- en: Although the resulting batches are fixed and do not change over training (not
    stochastic enough), the authors design momentum-like optimization terms to mitigate
    this non-stochasticity. IBMB can be used both in training and inference with massive
    speedups â€” up to 17x and 130x, respectively ğŸš€
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ç”Ÿæˆçš„æ‰¹æ¬¡æ˜¯å›ºå®šçš„ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸ä¼šæ”¹å˜ï¼ˆä¸å¤Ÿéšæœºï¼‰ï¼Œä½†ä½œè€…è®¾è®¡äº†ç±»ä¼¼åŠ¨é‡çš„ä¼˜åŒ–é¡¹æ¥ç¼“è§£è¿™ç§ééšæœºæ€§ã€‚IBMB å¯ä»¥åœ¨è®­ç»ƒå’Œæ¨ç†ä¸­ä½¿ç”¨ï¼Œé€Ÿåº¦æå‡å¯è¾¾
    17 å€å’Œ 130 å€ ğŸš€
- en: '![](../Images/79a47281579302493940b6b733e1b1c3.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/79a47281579302493940b6b733e1b1c3.png)'
- en: 'Influence-based mini-batching. Source: [Gasteiger, Qian, and GÃ¼nnemann](https://openreview.net/forum?id=b9g0vxzYa_)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºå½±å“åŠ›çš„è¿·ä½ æ‰¹å¤„ç†ã€‚æ¥æºï¼š[Gasteiger, Qian, å’Œ GÃ¼nnemann](https://openreview.net/forum?id=b9g0vxzYa_)
- en: The subtitle of this subsection could be â€œ*brought to you by Google*â€ since
    the majority of the papers have authors from Google ;)
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬å°èŠ‚çš„å‰¯æ ‡é¢˜å¯ä»¥æ˜¯â€œ*ç”±è°·æ­Œæä¾›*â€ï¼Œå› ä¸ºå¤§å¤šæ•°è®ºæ–‡çš„ä½œè€…éƒ½æ¥è‡ªè°·æ­Œ ;)
- en: '[Carey et al.](https://openreview.net/pdf?id=q5h7Ywx-sS) created ***Stars***,
    a method for building sparse similarity graphs at the scale of **tens of trillions**
    of edges ğŸ¤¯. Pairwise NÂ² comparisons would obviously not work here â€” Stars employs
    two-hop [spanner graphs](https://en.wikipedia.org/wiki/Geometric_spanner) (those
    are the graphs where similar points are connected with at most two hops) and [SortingLSH](http://infolab.stanford.edu/~bawa/Pub/similarity.pdf)
    that together enable almost linear time complexity and high sparsity.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[Careyç­‰](https://openreview.net/pdf?id=q5h7Ywx-sS) åˆ›å»ºäº†***Stars***ï¼Œä¸€ç§åœ¨**æ•°åä¸‡äº¿**è¾¹çš„è§„æ¨¡ä¸‹æ„å»ºç¨€ç–ç›¸ä¼¼æ€§å›¾çš„æ–¹æ³•ğŸ¤¯ã€‚æˆå¯¹çš„NÂ²æ¯”è¾ƒæ˜¾ç„¶ä¸å¯è¡Œâ€”â€”Starsä½¿ç”¨äº†ä¸¤ä¸ªè·³æ•°çš„[spannerå›¾](https://en.wikipedia.org/wiki/Geometric_spanner)ï¼ˆè¿™äº›å›¾ä¸­ç›¸ä¼¼çš„ç‚¹é€šè¿‡æœ€å¤šä¸¤ä¸ªè·³æ•°è¿æ¥ï¼‰å’Œ[SortingLSH](http://infolab.stanford.edu/~bawa/Pub/similarity.pdf)ï¼Œä¸¤è€…ç»“åˆä½¿å¾—æ¥è¿‘çº¿æ€§çš„æ—¶é—´å¤æ‚åº¦å’Œé«˜ç¨€ç–æ€§æˆä¸ºå¯èƒ½ã€‚'
- en: '[Dhulipala et al.](https://openreview.net/pdf?id=LpgG0C6Y75) created **ParHAC**,
    an approximate (1+ğ) parallel algorithm for hierarchical agglomerative clustering
    (HAC) on very large graphs and extensive theoretical foundations of the algorithm.
    ParHAC has O(V+E) complexity and poly-log depth and runs up to 60x faster than
    baselines on graphs with **hundreds of billions** of edges (here it is the Hyperlink
    graph with 1.7B nodes and 125B edges).'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[Dhulipalaç­‰](https://openreview.net/pdf?id=LpgG0C6Y75) åˆ›å»ºäº†**ParHAC**ï¼Œä¸€ç§ç”¨äºéå¸¸å¤§å›¾çš„è¿‘ä¼¼ï¼ˆ1+ğï¼‰å¹¶è¡Œå±‚æ¬¡èšç±»ï¼ˆHACï¼‰ç®—æ³•åŠå…¶å¹¿æ³›çš„ç†è®ºåŸºç¡€ã€‚ParHACå…·æœ‰O(V+E)å¤æ‚åº¦å’Œå¤šå¯¹æ•°æ·±åº¦ï¼Œåœ¨å…·æœ‰**æ•°ç™¾äº¿**è¾¹çš„å›¾ä¸Šè¿è¡Œé€Ÿåº¦æ¯”åŸºçº¿å¿«é«˜è¾¾60å€ï¼ˆè¿™é‡Œæ˜¯å…·æœ‰1.7BèŠ‚ç‚¹å’Œ125Bè¾¹çš„è¶…é“¾æ¥å›¾ï¼‰ã€‚'
- en: '[Devvrit et al.](https://openreview.net/pdf?id=ldl2V3vLZ5) created **SÂ³GC**,
    a scalable self-supervised graph clustering algorithm with one-layer GNN and constrastive
    training objective. SÂ³GC uses both graph structure and node features and scales
    to graphs of up to 1.6B edges.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[Devvritç­‰](https://openreview.net/pdf?id=ldl2V3vLZ5) åˆ›å»ºäº†**SÂ³GC**ï¼Œä¸€ç§å¯æ‰©å±•çš„è‡ªç›‘ç£å›¾èšç±»ç®—æ³•ï¼Œä½¿ç”¨å•å±‚GNNå’Œå¯¹æ¯”è®­ç»ƒç›®æ ‡ã€‚SÂ³GCä½¿ç”¨å›¾ç»“æ„å’ŒèŠ‚ç‚¹ç‰¹å¾ï¼Œèƒ½å¤Ÿæ‰©å±•åˆ°é«˜è¾¾1.6Bè¾¹çš„å›¾ã€‚'
- en: Finally, [Epasto et al.](https://openreview.net/forum?id=Fhty8PgFkDo) created
    a differentially-private modification of PageRank!
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œ[Epastoç­‰](https://openreview.net/forum?id=Fhty8PgFkDo) åˆ›å»ºäº†PageRankçš„å·®åˆ†éšç§ä¿®æ”¹ç‰ˆæœ¬ï¼
- en: 'LoG 2022 featured two tutorials on large-scale GNNs: [Scaling GNNs in Production](https://www.youtube.com/watch?v=HRC4hZKiUWU)
    by Da Zheng, Vassilis N. Ioannidis, and Soji Adeshina and [Parallel and Distributed
    GNNs](https://www.youtube.com/watch?v=e2jJU7u7si0) by Torsten Hoefler and Maciej
    Besta.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: LoG 2022åŒ…å«äº†ä¸¤ä¸ªå…³äºå¤§è§„æ¨¡GNNçš„æ•™ç¨‹ï¼š[ç”Ÿäº§ä¸­GNNçš„æ‰©å±•](https://www.youtube.com/watch?v=HRC4hZKiUWU)ç”±Da
    Zhengã€Vassilis N. Ioannidiså’ŒSoji Adeshinaä¸»è®²ï¼Œä»¥åŠ[å¹¶è¡Œå’Œåˆ†å¸ƒå¼GNN](https://www.youtube.com/watch?v=e2jJU7u7si0)ç”±Torsten
    Hoeflerå’ŒMaciej Bestaä¸»è®²ã€‚
- en: '**What to expect in 2023**: further reduction in compute costs and inference
    time for very large graphs. Perhaps models for OGB LSC graphs could run on commodity
    machines instead of huge clusters?'
  id: totrans-120
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2023å¹´å€¼å¾—æœŸå¾…**ï¼šè¿›ä¸€æ­¥é™ä½è®¡ç®—æˆæœ¬å’Œæ¨ç†æ—¶é—´ï¼Œä»¥åº”å¯¹éå¸¸å¤§çš„å›¾ã€‚ä¹Ÿè®¸OGB LSCå›¾çš„æ¨¡å‹å¯ä»¥åœ¨æ™®é€šæœºå™¨ä¸Šè¿è¡Œï¼Œè€Œä¸æ˜¯åœ¨å¤§å‹é›†ç¾¤ä¸Šï¼Ÿ'
- en: 'GNN Theory: Weisfeiler and Leman Go Places, Subgraph GNNs'
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GNNç†è®ºï¼šWeisfeilerå’ŒLemançš„è¶³è¿¹ï¼Œå­å›¾GNN
- en: '![](../Images/b7121cd3156f09a0fadc730188d2124d.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7121cd3156f09a0fadc730188d2124d.png)'
- en: 'Tourists of the year! Source of the original portraits: [Towards Geometric
    Deep Learning IV: Chemical Precursors of GNNs](/towards-geometric-deep-learning-iv-chemical-precursors-of-gnns-11273d74125)
    by Michael Bronstein'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: å¹´åº¦æ¸¸å®¢ï¼åŸå§‹è‚–åƒæ¥æºï¼š[å‡ ä½•æ·±åº¦å­¦ä¹ IVï¼šGNNçš„åŒ–å­¦å‰ä½“](/towards-geometric-deep-learning-iv-chemical-precursors-of-gnns-11273d74125)
    ç”±Michael Bronsteinä¸»è®²
- en: 'ğŸ– ğŸŒ„ Weisfeiler and Leman, grandfathers of Graph ML and GNN theory, had a very
    prolific traveling year! After visiting [Neural](https://ojs.aaai.org/index.php/AAAI/article/view/4384),
    [Sparse](https://proceedings.neurips.cc/paper/2020/file/f81dee42585b3814de199b2e88757f5c-Paper.pdf),
    [Topological](http://proceedings.mlr.press/v139/bodnar21a/bodnar21a.pdf), and
    [Cellular](https://proceedings.neurips.cc/paper/2021/file/157792e4abb490f99dbd738483e0d2d4-Paper.pdf)
    places in previous years, in 2022 we have seen them in several new places:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ– ğŸŒ„ Weisfeilerå’ŒLemanï¼Œå›¾MLå’ŒGNNç†è®ºçš„å¥ åŸºäººï¼Œåº¦è¿‡äº†éå¸¸å¤šäº§çš„ä¸€å¹´ï¼ç»§ä¹‹å‰è®¿é—®è¿‡[Neural](https://ojs.aaai.org/index.php/AAAI/article/view/4384)ã€[Sparse](https://proceedings.neurips.cc/paper/2020/file/f81dee42585b3814de199b2e88757f5c-Paper.pdf)ã€[Topological](http://proceedings.mlr.press/v139/bodnar21a/bodnar21a.pdf)å’Œ[Cellular](https://proceedings.neurips.cc/paper/2021/file/157792e4abb490f99dbd738483e0d2d4-Paper.pdf)åœºæ‰€åï¼Œ2022å¹´æˆ‘ä»¬åœ¨å‡ ä¸ªæ–°åœ°æ–¹è§åˆ°äº†ä»–ä»¬ï¼š
- en: WL Go **Machine Learning** â€” a comprehensive survey by [Morris et al](https://arxiv.org/abs/2112.09992)
    on the basics of the WL test, terminology, and various applications;
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WL Go **æœºå™¨å­¦ä¹ ** â€” [Morrisç­‰](https://arxiv.org/abs/2112.09992) å¯¹WLæµ‹è¯•çš„åŸºç¡€çŸ¥è¯†ã€æœ¯è¯­åŠå„ç§åº”ç”¨çš„ç»¼åˆè°ƒæŸ¥ï¼›
- en: WL Go **Relational** â€” the first attempt by [Barcelo et al](https://arxiv.org/abs/2211.17113)
    to study expressiveness of relational GNNs used in multi-relational graphs and
    KGs. Turns out R-GCN and CompGCN are equally expressive and are bounded by the
    Relational 1-WL test, and the most expressive message function (aggregating entity-relation
    representations) is a Hadamard product;
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WL Go **å…³ç³»** â€”â€” [Barcelo et al](https://arxiv.org/abs/2211.17113) é¦–æ¬¡å°è¯•ç ”ç©¶åœ¨å¤šå…³ç³»å›¾å’ŒçŸ¥è¯†å›¾ä¸­ä½¿ç”¨çš„å…³ç³»
    GNNs çš„è¡¨ç°åŠ›ã€‚ç»“æœè¡¨æ˜ï¼ŒR-GCN å’Œ CompGCN çš„è¡¨ç°åŠ›ç›¸åŒï¼Œä¸”å—é™äºå…³ç³» 1-WL æµ‹è¯•ï¼Œæœ€å…·è¡¨ç°åŠ›çš„æ¶ˆæ¯å‡½æ•°ï¼ˆèšåˆå®ä½“-å…³ç³»è¡¨ç¤ºï¼‰æ˜¯ Hadamard
    ä¹˜ç§¯ï¼›
- en: '[WL Go Walking by Niels M. Kriege](https://arxiv.org/abs/2205.10914) studies
    expressiveness of random walk kernels and finds that the RW kernel (with a small
    modification) is as expressive as a WL subtree kernel;'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[WL Go Walking by Niels M. Kriege](https://arxiv.org/abs/2205.10914) ç ”ç©¶äº†éšæœºæ¸¸èµ°æ ¸çš„è¡¨ç°åŠ›ï¼Œå‘ç°
    RW æ ¸ï¼ˆç»è¿‡å°çš„ä¿®æ”¹ï¼‰ä¸ WL å­æ ‘æ ¸ä¸€æ ·å…·è¡¨ç°åŠ›ã€‚'
- en: 'WL Go **Geometric**: [Joshi, Bodnar et al](https://openreview.net/forum?id=kXe4Y0c4VqT)
    propose Geometric WL test (GWL) to study expressiveness of equivariant and invariant
    GNNs (to ceratin symmetries: translation, rotation, reflection, permutation).
    Turns out, equivariant GNNs (such as [E-GNN](https://arxiv.org/abs/2102.09844),
    [NequIP](https://arxiv.org/abs/2101.03164) or [MACE](https://arxiv.org/abs/2206.07697))
    are provably more powerful than invariant GNNs (such as [SchNet](https://proceedings.neurips.cc/paper/2017/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf)
    or [DimeNet](https://arxiv.org/abs/2011.14115));'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WL Go **å‡ ä½•**ï¼š[Joshi, Bodnar et al](https://openreview.net/forum?id=kXe4Y0c4VqT)
    æå‡ºäº†å‡ ä½• WL æµ‹è¯•ï¼ˆGWLï¼‰æ¥ç ”ç©¶ç­‰å˜å’Œä¸å˜ GNNsï¼ˆå¯¹äºæŸäº›å¯¹ç§°æ€§ï¼šå¹³ç§»ã€æ—‹è½¬ã€åå°„ã€æ’åˆ—ï¼‰çš„è¡¨ç°åŠ›ã€‚ç»“æœè¡¨æ˜ï¼Œç­‰å˜ GNNsï¼ˆä¾‹å¦‚ [E-GNN](https://arxiv.org/abs/2102.09844)ã€[NequIP](https://arxiv.org/abs/2101.03164)
    æˆ– [MACE](https://arxiv.org/abs/2206.07697)ï¼‰åœ¨ç†è®ºä¸Šæ¯”ä¸å˜ GNNsï¼ˆä¾‹å¦‚ [SchNet](https://proceedings.neurips.cc/paper/2017/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf)
    æˆ– [DimeNet](https://arxiv.org/abs/2011.14115)ï¼‰æ›´å¼ºå¤§ã€‚
- en: 'WL Go **Temporal**: [Souza et al](https://openreview.net/pdf?id=MwSXgQSxL5s)
    propose Temporal WL test to study expressiveness of temporal GNNs. The authors
    then propose a novel injective aggregation function (and the PINT model) that
    should be most expressive;'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WL Go **æ—¶é—´**ï¼š[Souza et al](https://openreview.net/pdf?id=MwSXgQSxL5s) æå‡ºäº†æ—¶é—´
    WL æµ‹è¯•æ¥ç ”ç©¶æ—¶é—´ GNNs çš„è¡¨ç°åŠ›ã€‚ä½œè€…éšåæå‡ºäº†ä¸€ç§æ–°å‹çš„å•å°„èšåˆå‡½æ•°ï¼ˆä»¥åŠ PINT æ¨¡å‹ï¼‰ï¼Œè¿™åº”è¯¥æ˜¯æœ€å…·è¡¨ç°åŠ›çš„ï¼›
- en: 'WL Go **Gradual**: [Bause and Kriege](https://openreview.net/forum?id=fe1DEN1nds)
    propose to modify the original WL color refinement with a non-injective function
    where different multi-sets *might* get assigned the same color (under certain
    conditions). It thus enables more gradual color refinement and slower convergence
    to stable coloring that eventually retains expressiveness of 1-WL but gets a few
    distinguishing properties on the way.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WL Go **æ¸è¿›**ï¼š[Bause å’Œ Kriege](https://openreview.net/forum?id=fe1DEN1nds) æè®®ç”¨éå•å°„å‡½æ•°ä¿®æ”¹åŸå§‹
    WL é¢œè‰²ç»†åŒ–ï¼Œå…¶ä¸­ä¸åŒçš„å¤šé‡é›†åˆ*å¯èƒ½*è¢«åˆ†é…ç›¸åŒçš„é¢œè‰²ï¼ˆåœ¨æŸäº›æ¡ä»¶ä¸‹ï¼‰ã€‚å› æ­¤ï¼Œå®ƒä½¿å¾—é¢œè‰²ç»†åŒ–è¿‡ç¨‹æ›´åŠ æ¸è¿›ï¼Œå¹¶ä¸”æ”¶æ•›åˆ°ç¨³å®šç€è‰²çš„é€Ÿåº¦æ›´æ…¢ï¼Œæœ€ç»ˆä¿ç•™äº† 1-WL
    çš„è¡¨ç°åŠ›ï¼Œä½†åœ¨è¿‡ç¨‹ä¸­è·å¾—äº†ä¸€äº›åŒºåˆ†ç‰¹æ€§ã€‚
- en: 'WL Go **Infinite**: [Feldman et al](https://arxiv.org/abs/2201.13410) propose
    to change the initial node coloring with spectral features derived from the heat
    kernel of the Laplacian or with k-smallest eigenvectors of the Laplacian (for
    large graphs) which is quite close to Laplacian Positional Encodings (LPEs).'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WL Go **æ— é™**ï¼š[Feldman et al](https://arxiv.org/abs/2201.13410) æè®®ç”¨ä»æ‹‰æ™®æ‹‰æ–¯çš„çƒ­æ ¸è¡ç”Ÿçš„è°±ç‰¹å¾æˆ–æ‹‰æ™®æ‹‰æ–¯çš„
    k æœ€å°ç‰¹å¾å‘é‡ï¼ˆå¯¹äºå¤§å›¾ï¼‰æ¥æ”¹å˜åˆå§‹èŠ‚ç‚¹ç€è‰²ï¼Œè¿™ä¸æ‹‰æ™®æ‹‰æ–¯ä½ç½®ç¼–ç ï¼ˆLPEsï¼‰éå¸¸æ¥è¿‘ã€‚
- en: 'WL Go **Hyperbolic**: [Nikolentzos et al](https://arxiv.org/abs/2211.02501)
    note that the color refinement procedure of the WL test produces a tree hierarchy
    of colors. In order to preserve relative distances of nodes encoded by those colors,
    the authors propose to map output states of each layer/iteration into a hyperbolic
    space and update it after each next layer. The final embeddings are supposed to
    retain the notion of node distances.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WL Go **åŒæ›²**ï¼š[Nikolentzos et al](https://arxiv.org/abs/2211.02501) æŒ‡å‡º WL æµ‹è¯•çš„é¢œè‰²ç»†åŒ–è¿‡ç¨‹ä¼šäº§ç”Ÿé¢œè‰²çš„æ ‘çŠ¶å±‚æ¬¡ç»“æ„ã€‚ä¸ºäº†ä¿æŒè¿™äº›é¢œè‰²æ‰€ç¼–ç çš„èŠ‚ç‚¹ç›¸å¯¹è·ç¦»ï¼Œä½œè€…å»ºè®®å°†æ¯å±‚/è¿­ä»£çš„è¾“å‡ºçŠ¶æ€æ˜ å°„åˆ°åŒæ›²ç©ºé—´ï¼Œå¹¶åœ¨æ¯æ¬¡æ›´æ–°åè°ƒæ•´ã€‚æœ€ç»ˆçš„åµŒå…¥åº”è¯¥ä¿ç•™èŠ‚ç‚¹è·ç¦»çš„æ¦‚å¿µã€‚
- en: 'ğŸ“ˆ In the realm of more expressive (than 1-WL) architectures, subgraph GNNs
    are the biggest trend. Among those, three approaches stand out: 1ï¸âƒ£ **Subgraph
    Union Networks** (SUN) by [Frasca, Bevilacqua, et al.](https://arxiv.org/abs/2206.11140)
    that provide a comprehensive analysis of subgraph GNNs design space and expressiveness
    showing they are bounded by 3-WL; 2ï¸âƒ£ **Ordered Subgraph Aggregation Networks**
    (OSAN) by [Qian, Rattan, et al](https://arxiv.org/abs/2206.11168) devise a hierarchy
    of subgraph-enhanced GNNs (k-OSAN) and find that k-OSAN are incomparable to k-WL
    but are strictly limited by (k+1)-WL. One particularly cool part of OSAN is using
    [Implicit MLE](https://arxiv.org/abs/2106.01798) (NeurIPSâ€™21), a differentiable
    discrete sampling technique, for sampling ordered subgraphs. **ï¸3ï¸âƒ£ SpeqNets**
    by [Morris et al.](https://arxiv.org/abs/2203.13913) devise a permutation-equivariant
    hierarchy of graph networks that balances between scalability and expressivity.
    4ï¸âƒ£ **GraphSNN** by [Wijesinghe and Wang](https://openreview.net/pdf?id=uxgg9o7bI_3)
    derives expressive models based on the overlap of *subgraph* isomorphisms and
    *subtree* isomorpishms.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ“ˆ åœ¨æ¯”1-WLæ›´å…·è¡¨è¾¾åŠ›çš„æ¶æ„é¢†åŸŸä¸­ï¼Œå­å›¾GNNsæ˜¯æœ€å¤§çš„è¶‹åŠ¿ã€‚å…¶ä¸­æœ‰ä¸‰ç§æ–¹æ³•è„±é¢–è€Œå‡ºï¼š1ï¸âƒ£ **å­å›¾è”åˆç½‘ç»œ**ï¼ˆSUNï¼‰ï¼Œç”±[Frasca, Bevilacquaç­‰](https://arxiv.org/abs/2206.11140)æå‡ºï¼Œæä¾›äº†å­å›¾GNNsè®¾è®¡ç©ºé—´å’Œè¡¨è¾¾åŠ›çš„å…¨é¢åˆ†æï¼Œæ˜¾ç¤ºå®ƒä»¬å—é™äº3-WLï¼›2ï¸âƒ£
    **æœ‰åºå­å›¾èšåˆç½‘ç»œ**ï¼ˆOSANï¼‰ï¼Œç”±[Qian, Rattanç­‰](https://arxiv.org/abs/2206.11168)æå‡ºï¼Œè®¾è®¡äº†ä¸€ç§å­å›¾å¢å¼ºGNNsï¼ˆk-OSANï¼‰çš„å±‚æ¬¡ç»“æ„ï¼Œå¹¶å‘ç°k-OSANä¸k-WLä¸å¯æ¯”ï¼Œä½†ä¸¥æ ¼å—é™äºï¼ˆk+1ï¼‰-WLã€‚OSANçš„ä¸€ä¸ªç‰¹åˆ«é…·çš„éƒ¨åˆ†æ˜¯ä½¿ç”¨[éšå¼MLE](https://arxiv.org/abs/2106.01798)ï¼ˆNeurIPSâ€™21ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¯å¾®åˆ†çš„ç¦»æ•£é‡‡æ ·æŠ€æœ¯ï¼Œç”¨äºé‡‡æ ·æœ‰åºå­å›¾ã€‚**ï¸3ï¸âƒ£
    SpeqNets** ç”±[Morrisç­‰](https://arxiv.org/abs/2203.13913)æå‡ºï¼Œè®¾è®¡äº†ä¸€ç§å›¾ç½‘ç»œçš„ç½®æ¢ç­‰å˜å±‚æ¬¡ç»“æ„ï¼Œå¹³è¡¡äº†å¯æ‰©å±•æ€§å’Œè¡¨è¾¾åŠ›ã€‚4ï¸âƒ£
    **GraphSNN** ç”±[Wijesingheå’ŒWang](https://openreview.net/pdf?id=uxgg9o7bI_3)æå‡ºï¼ŒåŸºäº*å­å›¾*åŒæ„å’Œ*å­æ ‘*åŒæ„çš„é‡å æ¨å¯¼å‡ºè¡¨è¾¾æ€§æ¨¡å‹ã€‚
- en: ğŸ¤” A few works rethink the WL framework as a general means for GNN expressiveness.
    [Geerts and Reutter](https://openreview.net/pdf?id=wIzUeM3TAU) define **k-order
    MPNNs** that can be characterized with Tensor Languages (with a mapping between
    WL and **Tensor Languages**). A new [anonymous ICLRâ€™23 submission](https://openreview.net/forum?id=r9hNv76KoT3)
    proposes to leverage [graph biconnectivity](https://en.wikipedia.org/wiki/Biconnected_component)
    and defines a **Generalized Distance WL** algorithm.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤” ä¸€äº›ç ”ç©¶é‡æ–°å®¡è§†WLæ¡†æ¶ä½œä¸ºGNNè¡¨è¾¾åŠ›çš„ä¸€ç§é€šç”¨æ‰‹æ®µã€‚[Geertså’ŒReutter](https://openreview.net/pdf?id=wIzUeM3TAU)å®šä¹‰äº†**ké˜¶MPNNs**ï¼Œå¯ä»¥é€šè¿‡å¼ é‡è¯­è¨€ï¼ˆWLä¸**å¼ é‡è¯­è¨€**ä¹‹é—´çš„æ˜ å°„ï¼‰è¿›è¡Œè¡¨å¾ã€‚ä¸€ä¸ªæ–°çš„[åŒ¿åICLRâ€™23æäº¤](https://openreview.net/forum?id=r9hNv76KoT3)æå‡ºåˆ©ç”¨[å›¾çš„åŒè¿é€šæ€§](https://en.wikipedia.org/wiki/Biconnected_component)å¹¶å®šä¹‰äº†**å¹¿ä¹‰è·ç¦»WL**ç®—æ³•ã€‚
- en: If youâ€™d like to study the topic even deeper, check out a wonderful [LOG 2022
    tutorial](https://www.youtube.com/watch?v=ASQYjbUBYzs&list=PL2iNJC54likoqgKwpFnbBik8Im1sZ27Hm&index=7)
    by Fabrizio Frasca, Beatrice Bevilacqua, and Haggai Maron with practical examples!
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³æ›´æ·±å…¥åœ°ç ”ç©¶è¿™ä¸ªä¸»é¢˜ï¼ŒæŸ¥çœ‹Fabrizio Frascaã€Beatrice Bevilacquaå’ŒHaggai Maronçš„ç²¾å½©[LOG 2022æ•™ç¨‹](https://www.youtube.com/watch?v=ASQYjbUBYzs&list=PL2iNJC54likoqgKwpFnbBik8Im1sZ27Hm&index=7)åŠå…¶å®é™…ä¾‹å­å§ï¼
- en: '**What to expect in 2023**: *1ï¸âƒ£* More efforts on creating time- and memory-efficient
    subgraph GNNs. *2ï¸âƒ£* Better understanding of generalization of GNNs. *3ï¸âƒ£* Weisfeiler
    and Leman visit 10 new places!'
  id: totrans-136
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2023å¹´çš„æœŸå¾…**ï¼š*1ï¸âƒ£* æ›´å¤šè‡´åŠ›äºåˆ›å»ºæ—¶é—´å’Œå†…å­˜é«˜æ•ˆçš„å­å›¾GNNsã€‚*2ï¸âƒ£* æ›´å¥½åœ°ç†è§£GNNsçš„æ³›åŒ–èƒ½åŠ›ã€‚*3ï¸âƒ£* Weisfeilerå’ŒLemanè®¿é—®10ä¸ªæ–°åœ°æ–¹ï¼'
- en: 'Knowledge Graphs: Inductive Reasoning Takes Over'
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: çŸ¥è¯†å›¾è°±ï¼šå½’çº³æ¨ç†å æ®ä¸»å¯¼åœ°ä½
- en: 'Last year, we observed a major shift in KG representation learning: transductive-only
    approaches are being actively retired in favor of inductive models that can build
    meaningful representation for new, unseen nodes and perform node classification
    and link prediction.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: å»å¹´ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°äº†KGè¡¨ç¤ºå­¦ä¹ çš„é‡å¤§è½¬å˜ï¼šä¼ å¯¼æ€§æ–¹æ³•æ­£è¢«ç§¯ææ·˜æ±°ï¼Œå–è€Œä»£ä¹‹çš„æ˜¯å½’çº³æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥ä¸ºæ–°çš„ã€æœªè§è¿‡çš„èŠ‚ç‚¹æ„å»ºæœ‰æ„ä¹‰çš„è¡¨ç¤ºï¼Œå¹¶æ‰§è¡ŒèŠ‚ç‚¹åˆ†ç±»å’Œé“¾æ¥é¢„æµ‹ã€‚
- en: 'In 2022, the field was expanding along two main axes: 1ï¸âƒ£ inductive link prediction
    (LP) 2ï¸âƒ£ and inductive (multi-hop) query answering that extends link prediction
    to much more complex prediction tasks.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨2022å¹´ï¼Œè¯¥é¢†åŸŸæ²¿ç€ä¸¤ä¸ªä¸»è¦è½´çº¿æ‰©å±•ï¼š1ï¸âƒ£ å½’çº³é“¾æ¥é¢„æµ‹ï¼ˆLPï¼‰2ï¸âƒ£ å’Œå½’çº³ï¼ˆå¤šè·³ï¼‰æŸ¥è¯¢å›ç­”ï¼Œå°†é“¾æ¥é¢„æµ‹æ‰©å±•åˆ°æ›´å¤æ‚çš„é¢„æµ‹ä»»åŠ¡ã€‚
- en: 1ï¸âƒ£ In link prediction, the majority of inductive models (like [**NBFNet**](https://arxiv.org/abs/2106.06935)
    or [**NodePiece**](https://arxiv.org/abs/2106.12144)) transfer to unseen nodes
    at inference by assuming that the set of relation types is fixed during training
    and does not change over time so they can learn relation embeddings. What happens
    when the set of relations changes as well? In the hardest case, weâ€™d want to transfer
    to KGs with completely different nodes **and** relation types.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ åœ¨é“¾è·¯é¢„æµ‹ä¸­ï¼Œå¤§å¤šæ•°å½’çº³æ¨¡å‹ï¼ˆå¦‚[**NBFNet**](https://arxiv.org/abs/2106.06935)æˆ–[**NodePiece**](https://arxiv.org/abs/2106.12144)ï¼‰é€šè¿‡å‡è®¾å…³ç³»ç±»å‹çš„é›†åˆåœ¨è®­ç»ƒæœŸé—´æ˜¯å›ºå®šçš„ä¸”ä¸ä¼šéšæ—¶é—´å˜åŒ–ï¼Œä»è€Œè½¬ç§»åˆ°æœªè§èŠ‚ç‚¹ï¼Œå¹¶å­¦ä¹ å…³ç³»åµŒå…¥ã€‚å½“å…³ç³»é›†åˆä¹Ÿå‘ç”Ÿå˜åŒ–æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿåœ¨æœ€å›°éš¾çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿè½¬ç§»åˆ°å…·æœ‰å®Œå…¨ä¸åŒèŠ‚ç‚¹**å’Œ**å…³ç³»ç±»å‹çš„çŸ¥è¯†å›¾è°±ã€‚
- en: So far, all such models supporting unseen relations resort to meta-learning
    which is slow and resource-hungry. In 2022, for the first time, [Huang, Ren, and
    Leskovec](https://openreview.net/forum?id=LvW71lgly25) proposed the Connected
    Subgraph Reasoner (**CSR**) framework that is inductive along **both** entities
    and relation types **and** does not need any meta-learning! ğŸ‘€ Generally, for new
    relations at inference, models see at least *k* example triples with this relation
    (hence, a k-shot learning scenario). Conceptually, CSR extracts subgraphs around
    each example trying to learn common relational patterns (i.e., optimizing edge
    masks) and then apply the mask to the query subgraph (with the missing target
    link to predict).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæ‰€æœ‰æ”¯æŒæœªè§å…³ç³»çš„æ¨¡å‹éƒ½ä¾èµ–äºå…ƒå­¦ä¹ ï¼Œè¿™ç§æ–¹æ³•æ—¢æ…¢åˆæ¶ˆè€—èµ„æºã€‚åœ¨2022å¹´ï¼Œ[é»„ï¼Œä»»å’Œè±æ–¯ç§‘ç»´å¥‡](https://openreview.net/forum?id=LvW71lgly25)é¦–æ¬¡æå‡ºäº†**CSR**ï¼ˆè¿æ¥å­å›¾æ¨ç†å™¨ï¼‰æ¡†æ¶ï¼Œå®ƒåœ¨**å®ä½“**å’Œ**å…³ç³»ç±»å‹**ä¸Šéƒ½æ˜¯å½’çº³çš„**ä¸”**ä¸éœ€è¦ä»»ä½•å…ƒå­¦ä¹ ï¼ğŸ‘€
    é€šå¸¸ï¼Œåœ¨æ¨ç†æ—¶ï¼Œå¯¹äºæ–°å…³ç³»ï¼Œæ¨¡å‹è‡³å°‘çœ‹åˆ°*k*ä¸ªç¤ºä¾‹ä¸‰å…ƒç»„ï¼ˆå› æ­¤æ˜¯k-shotå­¦ä¹ åœºæ™¯ï¼‰ã€‚ä»æ¦‚å¿µä¸Šè®²ï¼ŒCSRæå–æ¯ä¸ªç¤ºä¾‹å‘¨å›´çš„å­å›¾ï¼Œè¯•å›¾å­¦ä¹ å…±åŒçš„å…³ç³»æ¨¡å¼ï¼ˆå³ä¼˜åŒ–è¾¹ç¼˜æ©ç ï¼‰ï¼Œç„¶åå°†æ©ç åº”ç”¨äºæŸ¥è¯¢å­å›¾ï¼ˆé¢„æµ‹ç¼ºå¤±çš„ç›®æ ‡é“¾è·¯ï¼‰ã€‚
- en: '![](../Images/495777bb4fc9ea6eae90cb434332ab5a.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/495777bb4fc9ea6eae90cb434332ab5a.png)'
- en: 'Inductive CSR that supports KGs with unseen entities and relation types. Source:
    [Huang, Ren, and Leskovec](https://openreview.net/forum?id=LvW71lgly25)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¯æŒå…·æœ‰æœªè§å®ä½“å’Œå…³ç³»ç±»å‹çš„çŸ¥è¯†å›¾è°±çš„**å½’çº³CSR**ã€‚æ¥æºï¼š[é»„ï¼Œä»»å’Œè±æ–¯ç§‘ç»´å¥‡](https://openreview.net/forum?id=LvW71lgly25)
- en: '**ReFactor GNNs** by [Chen et al.](https://openreview.net/forum?id=81LQV4k7a7X)
    is another insightful work on inductive qualities of shallow KG embedding models
    â€” particularly, the authors find that shallow factorization models like DistMult
    resemble infinitely deep GNNs when looking through the lens of backpropagation
    and how nodes update their representations from neighboring and non-neighboring
    nodes. Turns out that, theoretically, any factorization model can be turned into
    an inductive model!'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[Chenç­‰äºº](https://openreview.net/forum?id=81LQV4k7a7X)çš„**ReFactor GNNs**æ˜¯å¦ä¸€é¡¹æœ‰å…³æµ…å±‚KGåµŒå…¥æ¨¡å‹å½’çº³ç‰¹æ€§çš„æœ‰æ´å¯ŸåŠ›çš„å·¥ä½œâ€”â€”ç‰¹åˆ«æ˜¯ï¼Œä½œè€…å‘ç°æµ…å±‚å› å­åˆ†è§£æ¨¡å‹å¦‚DistMultåœ¨åå‘ä¼ æ’­çš„è§†è§’ä¸‹ï¼Œä¸æ— é™æ·±åº¦çš„GNNsç±»ä¼¼ï¼ŒèŠ‚ç‚¹å¦‚ä½•ä»é‚»è¿‘èŠ‚ç‚¹å’Œéé‚»è¿‘èŠ‚ç‚¹æ›´æ–°å…¶è¡¨ç¤ºã€‚ç†è®ºä¸Šï¼Œä»»ä½•å› å­åˆ†è§£æ¨¡å‹éƒ½å¯ä»¥è½¬å˜ä¸ºå½’çº³æ¨¡å‹ï¼'
- en: 2ï¸âƒ£ Inductive representation learning arrived in the area of complex logical
    query answering as well. (shameless plug) In fact, it was one of the focuses of
    our team this year ğŸ˜Š First, in [Zhu et al.](https://arxiv.org/abs/2205.10128),
    we found that Neural Bellman-Ford nets generalize well from simple link prediction
    to complex query answering tasks in a new [**GNN Query Executor**](https://github.com/DeepGraphLearning/GNN-QE)
    (GNN-QE) model where a GNN based on NBF-Net performs relation projections while
    other logical operators are performed via fuzzy logic [t-norms](https://en.wikipedia.org/wiki/T-norm).
    Then, in [Inductive Logical Query Answering in Knowledge Graphs](https://openreview.net/forum?id=-vXEN5rIABY)
    we studied âš—ï¸ *the essence of inductiveness* âš—ï¸ and proposed two ways to answer
    logical queries over unseen entities at inference time, that is, via (1) inductive
    node representations obtained with NodePiece encoder paired with the inference-only
    decoder (less performant but scalable) or via (2) inductive relational structure
    representations akin to the one in GNN-QE (better quality but more resource-hungry
    and hard to scale). Overall we are able to scale to an inductive query setting
    on graphs **with millions of nodes and 500k unseen nodes and 5m unseen edges**
    during inference.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ ä»å½’çº³è¡¨å¾å­¦ä¹ ä¹Ÿå¼€å§‹æ¶‰åŠåˆ°å¤æ‚çš„é€»è¾‘æŸ¥è¯¢å›ç­”ã€‚ ï¼ˆåšé¢œæ— è€»çš„æ’å…¥å¹¿å‘Šï¼‰äº‹å®ä¸Šï¼Œè¿™æ˜¯æˆ‘ä»¬å›¢é˜Ÿä»Šå¹´çš„é‡ç‚¹ä¹‹ä¸€ ğŸ˜Š é¦–å…ˆï¼Œåœ¨[Zhuç­‰äºº](https://arxiv.org/abs/2205.10128)çš„ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å‘ç°ç¥ç»è´å°”æ›¼-ç¦ç‰¹ç½‘ç»œä»ç®€å•çš„é“¾æ¥é¢„æµ‹æˆåŠŸåœ°æ¨å¹¿åˆ°äº†å¤æ‚çš„æŸ¥è¯¢ä»»åŠ¡ï¼Œè¿™æ˜¯åœ¨ä¸€ä¸ªæ–°çš„[**GNNæŸ¥è¯¢æ‰§è¡Œå™¨**](https://github.com/DeepGraphLearning/GNN-QE)ï¼ˆGNN-QEï¼‰æ¨¡å‹ä¸­ï¼Œå…¶ä¸­ä¸€ä¸ªåŸºäºNBF-Netçš„GNNè¿›è¡Œäº†å…³ç³»æŠ•å½±ï¼Œè€Œå…¶ä»–é€»è¾‘è¿ç®—é€šè¿‡æ¨¡ç³Šé€»è¾‘[t-èŒƒæ•°](https://en.wikipedia.org/wiki/T-norms)æ¥å®ç°ã€‚
    ç„¶åï¼Œåœ¨[çŸ¥è¯†å›¾ä¸­çš„å½’çº³é€»è¾‘æŸ¥è¯¢å›ç­”](https://openreview.net/forum?id=-vXEN5rIABY)ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†âš—ï¸ *å½’çº³æ€§çš„æœ¬è´¨*
    âš—ï¸å¹¶æå‡ºäº†ä¸¤ç§è§£å†³æ–¹æ¡ˆï¼Œä»¥åœ¨æ¨ç†æ—¶å›ç­”å…³äºæœªçŸ¥å®ä½“çš„é€»è¾‘æŸ¥è¯¢ï¼Œå³é€šè¿‡ï¼ˆ1ï¼‰ä¸ä»…ç”¨äºæ¨ç†çš„è§£ç å™¨é…å¯¹çš„å½’çº³èŠ‚ç‚¹è¡¨ç¤ºï¼ˆæ€§èƒ½è¾ƒå·®ä½†å¯æ‰©å±•ï¼‰ï¼Œæˆ–é€šè¿‡ï¼ˆ2ï¼‰ç±»ä¼¼äºGNN-QEä¸­çš„å½’çº³å…³ç³»ç»“æ„è¡¨ç¤ºï¼ˆè´¨é‡æ›´é«˜ä½†éœ€è¦æ›´å¤šèµ„æºå¹¶ä¸”éš¾ä»¥æ‰©å±•ï¼‰ã€‚
    æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨æ¨ç†æ—¶å¤„ç†**æ‹¥æœ‰æ•°ç™¾ä¸‡èŠ‚ç‚¹å’Œ500kä¸ªæœªè§èŠ‚ç‚¹ä»¥åŠ5mæœªè§è¾¹ç¼˜**çš„å›¾ä¸­è¿›è¡Œå½’çº³æŸ¥è¯¢è®¾ç½®ã€‚
- en: '![](../Images/063be3295d889aafc4b3b1c292e3e88b.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/063be3295d889aafc4b3b1c292e3e88b.png)'
- en: 'Inductive logical query answering approaches: via node representations (NodePiece-QE)
    and relational structure representations (GNN-QE). Source: [Galkin et al.](https://arxiv.org/abs/2210.08008)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: é€è¿‡èŠ‚ç‚¹è¡¨ç¤ºï¼ˆNodePiece-QEï¼‰å’Œå…³ç³»ç»“æ„è¡¨ç¤ºï¼ˆGNN-QEï¼‰è¿›è¡Œå½’çº³é€»è¾‘æŸ¥è¯¢å›ç­”ã€‚ æ¥æºï¼š[Galkinç­‰äºº](https://arxiv.org/abs/2210.08008)
- en: The other cool work in the area is [**SMORE**](https://github.com/google-research/smore)by
    [Ren, Dai, et al.](https://arxiv.org/abs/2110.14890) â€” it is a large-scale (transductive-only
    yet) system for complex query answering over very large graphs scaling up to the
    full Freebase with about 90M nodes and 300M edges ğŸ‘€. In addition to CUDA, training,
    and pipeline optimizations, SMORE implements a bidirectional query sampler such
    that training queries can be generated on-the-fly right in the data loader instead
    of creating and storing huge datasets. Donâ€™t forget to check out a [fresh hands-on
    tutorial](https://www.youtube.com/watch?v=kzWV57qJmiA&list=PL2iNJC54likoqgKwpFnbBik8Im1sZ27Hm&index=1)
    on large-scale graph reasoning from LOG 2022!
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªé¢†åŸŸä¸­çš„å¦ä¸€é¡¹é‡è¦å·¥ä½œæ˜¯[**SMORE**](https://github.com/google-research/smore)æ¥è‡ª[ä»»ã€æˆ´ç­‰äºº](https://arxiv.org/abs/2110.14890)
    â€” è¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ï¼ˆä»…é™ä¼ å¯¼ï¼‰çš„ç³»ç»Ÿï¼Œå¯å¯¹å¤§çº¦90Mä¸ªèŠ‚ç‚¹å’Œ300Mæ¡è¾¹ç¼˜çš„å®Œæ•´Freebaseè¿›è¡Œå¤æ‚æŸ¥è¯¢å›ç­”ğŸ‘€ã€‚ é™¤äº†CUDAã€è®­ç»ƒå’Œç®¡é“ä¼˜åŒ–ï¼ŒSMOREè¿˜å®ç°äº†ä¸€ä¸ªåŒå‘æŸ¥è¯¢é‡‡æ ·å™¨ï¼Œä½¿å¾—è®­ç»ƒæŸ¥è¯¢å¯ä»¥åœ¨æ•°æ®åŠ è½½å™¨ä¸­å³æ—¶ç”Ÿæˆï¼Œè€Œæ— éœ€åˆ›å»ºå’Œå­˜å‚¨å¤§é‡æ•°æ®é›†ã€‚
    ä¸è¦å¿˜è®°ä»LOG 2022ä¸­æŸ¥çœ‹å…³äºå¤§è§„æ¨¡å›¾æ¨ç†çš„[å…¨æ–°å®è·µæ•™ç¨‹](https://www.youtube.com/watch?v=kzWV57qJmiA&list=PL2iNJC54likoqgKwpFnbBik8Im1sZ27Hm&index=1)!
- en: Last but not the least, [Yang, Lin and Zhang](https://arxiv.org/pdf/2209.08858.pdf)
    brought up an interesting paper rethinking the evaluation of knowledge graph completion.
    They point out knowledge graphs tend to be open-world (i.e., there are facts not
    encoded by the knowledge graph) rather close-world assumed by most works. As a
    result, metrics observed under the close-world assumption exhibit a log trend
    w.r.t. the true metric â€” this means if you get 0.4 MRR for your model, chances
    are that the test knowledge graph is incomplete and your model has already done
    a good jobğŸ‘. Maybe we can design some new dataset and evaluation to mitigate such
    an issue?
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åä½†å¹¶éæœ€ä¸é‡è¦ï¼Œ[æ¨ã€æ—å’Œå¼ ](https://arxiv.org/pdf/2209.08858.pdf)æå‡ºäº†ä¸€ç¯‡æœ‰è¶£çš„è®ºæ–‡ï¼Œé‡æ–°æ€è€ƒäº†çŸ¥è¯†å›¾å®Œæˆçš„è¯„ä¼°ã€‚
    ä»–ä»¬æŒ‡å‡ºçŸ¥è¯†å›¾å€¾å‘äºæ˜¯å¼€æ”¾ä¸–ç•Œçš„ï¼ˆå³ï¼Œæœ‰äº›äº‹å®å¹¶æœªè¢«çŸ¥è¯†å›¾ç¼–ç ï¼‰ï¼Œè€Œä¸æ˜¯å¤§éƒ¨åˆ†ä½œå“æ‰€å‡è®¾çš„é—­ä¸–ç•Œã€‚ å› æ­¤ï¼Œåœ¨é—­ä¸–ç•Œå‡è®¾ä¸‹è§‚å¯Ÿåˆ°çš„æŒ‡æ ‡å¯¹çœŸå®æŒ‡æ ‡å‘ˆç°å‡ºäº†å¯¹æ•°è¶‹åŠ¿â€”â€”è¿™æ„å‘³ç€å¦‚æœä½ çš„æ¨¡å‹å¾—åˆ°äº†0.4çš„MRRï¼Œé‚£ä¹ˆæµ‹è¯•çŸ¥è¯†å›¾å¾ˆå¯èƒ½æ˜¯ä¸å®Œæ•´çš„ï¼Œè€Œä½ çš„æ¨¡å‹å·²ç»åšå¾—ç›¸å½“ä¸é”™ğŸ‘ã€‚ä¹Ÿè®¸æˆ‘ä»¬å¯ä»¥è®¾è®¡ä¸€äº›æ–°çš„æ•°æ®é›†å’Œè¯„ä¼°æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Ÿ
- en: '**What to expect in 2023**: an inductive model fully transferable to different
    KGs with new sets of entities and relations, e.g., training on Wikidata, and running
    inference on DBpedia or Freebase.'
  id: totrans-150
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2023 å¹´çš„é¢„æœŸ**ï¼šä¸€ä¸ªå¯ä»¥å®Œå…¨è¿ç§»åˆ°ä¸åŒçŸ¥è¯†å›¾è°±çš„æ–°é›†åˆçš„å½’çº³æ¨¡å‹ï¼Œä¾‹å¦‚åœ¨ Wikidata ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨ DBpedia æˆ– Freebase
    ä¸Šè¿è¡Œæ¨ç†ã€‚'
- en: Algorithmic Reasoning and Alignment
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç®—æ³•æ¨ç†ä¸å¯¹é½
- en: 2022 was a year of major breakthroughs and milestones for algorithmic reasoning.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 2022 å¹´æ˜¯ç®—æ³•æ¨ç†é¢†åŸŸå–å¾—é‡å¤§çªç ´å’Œé‡Œç¨‹ç¢‘çš„ä¸€å¹´ã€‚
- en: 1ï¸âƒ£ First, the [**CLRS benchmark**](https://github.com/deepmind/clrs) by [VeliÄkoviÄ‡
    et al.](https://arxiv.org/abs/2205.15659) is now available as the main playground
    to design and benchmark algorithmic reasoning models and tasks. CLRS already includes
    30 tasks (such as classical sorting algorithms, string algorithms, and graph algorithms)
    but still allows you to bring your own formulations or modify existing ones.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ é¦–å…ˆï¼Œ[**CLRS åŸºå‡†æµ‹è¯•**](https://github.com/deepmind/clrs)ç”± [VeliÄkoviÄ‡ ç­‰äºº](https://arxiv.org/abs/2205.15659)
    æä¾›ï¼Œç°åœ¨å¯ä»¥ä½œä¸ºè®¾è®¡å’Œè¯„ä¼°ç®—æ³•æ¨ç†æ¨¡å‹å’Œä»»åŠ¡çš„ä¸»è¦å¹³å°ã€‚CLRS å·²ç»åŒ…æ‹¬äº† 30 ä¸ªä»»åŠ¡ï¼ˆå¦‚ç»å…¸æ’åºç®—æ³•ã€å­—ç¬¦ä¸²ç®—æ³•å’Œå›¾ç®—æ³•ï¼‰ï¼Œä½†ä»ç„¶å…è®¸ä½ å¸¦å…¥è‡ªå·±çš„å…¬å¼æˆ–ä¿®æ”¹ç°æœ‰å…¬å¼ã€‚
- en: 2ï¸âƒ£ Then, a **Generalist Neural Algorithmic Learner** by [Ibarz et al.](https://openreview.net/forum?id=FebadKZf6Gd)
    and DeepMind has shown that it is possible to train a *single* processor network
    that can be trained in the multi-task mode on different algorithms â€” previously,
    youâ€™d train a single model for a single task repeating that for all 30 CLRS problems.
    The paper also describes several modifications and tricks to the model architecture
    side and training procedure to let the model generalize better and prevent forgetting,
    e.g., triplet reasoning similar to triangular attention (common for molecular
    models) and [edge transformers](https://arxiv.org/abs/2112.00578). Overall, a
    new model brings a massive 25% absolute gain over baselines and solves 24 out
    of 30 CLRS tasks with 60%+ micro-F1.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ ç„¶åï¼Œ[Ibarz ç­‰äºº](https://openreview.net/forum?id=FebadKZf6Gd) å’Œ DeepMind çš„**é€šç”¨ç¥ç»ç®—æ³•å­¦ä¹ å™¨**è¡¨æ˜ï¼Œå¯ä»¥è®­ç»ƒä¸€ä¸ª*å•ä¸€*çš„å¤„ç†å™¨ç½‘ç»œï¼Œåœ¨ä¸åŒç®—æ³•ä¸Šä»¥å¤šä»»åŠ¡æ¨¡å¼è¿›è¡Œè®­ç»ƒâ€”â€”ä»¥å‰ï¼Œä½ éœ€è¦ä¸ºæ¯ä¸ª
    CLRS é—®é¢˜è®­ç»ƒä¸€ä¸ªå•ç‹¬çš„æ¨¡å‹ã€‚è®ºæ–‡è¿˜æè¿°äº†æ¨¡å‹æ¶æ„å’Œè®­ç»ƒè¿‡ç¨‹ä¸­çš„è‹¥å¹²ä¿®æ”¹å’ŒæŠ€å·§ï¼Œä»¥ä½¿æ¨¡å‹æ›´å¥½åœ°è¿›è¡Œæ³›åŒ–å¹¶é˜²æ­¢é—å¿˜ï¼Œä¾‹å¦‚ï¼Œç±»ä¼¼äºä¸‰è§’æ³¨æ„åŠ›çš„ä¸‰å…ƒç»„æ¨ç†ï¼ˆåœ¨åˆ†å­æ¨¡å‹ä¸­å¸¸è§ï¼‰å’Œ
    [è¾¹å˜æ¢å™¨](https://arxiv.org/abs/2112.00578)ã€‚æ€»ä½“è€Œè¨€ï¼Œæ–°æ¨¡å‹å¸¦æ¥äº†æ¯”åŸºçº¿é«˜å‡º 25% çš„ç»å¯¹å¢ç›Šï¼Œå¹¶ä¸”ä»¥ 60%+ çš„å¾®è§‚
    F1 åˆ†æ•°è§£å†³äº† 30 ä¸ª CLRS ä»»åŠ¡ä¸­çš„ 24 ä¸ªã€‚
- en: '![](../Images/b4f7d36cd5ab8040d258a815d1dca4dc.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4f7d36cd5ab8040d258a815d1dca4dc.png)'
- en: 'Source: [Ibarz et al.](https://openreview.net/forum?id=FebadKZf6Gd)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[Ibarz ç­‰äºº](https://openreview.net/forum?id=FebadKZf6Gd)
- en: 3ï¸âƒ£ Last year, we [discussed](/graph-ml-in-2022-where-are-we-now-f7f8242599e0#72d1)
    the works on algorithmic alignment and saw the signs that GNNs can probably align
    well with dynamic programming. In 2022, [Dudzik and VeliÄkoviÄ‡](https://openreview.net/forum?id=wu1Za9dY1GY)
    prove that **GNNs are Dynamic Programmers** using category theory, abstract algebra,
    and notion of *pushforward* and *pullback* operations. This is a wonderful example
    of applying category theory that many people consider â€œabstract nonsenseâ€ ğŸ˜‰. Category
    theory is likely to have more impact in GNN theory and Graph ML in general, so
    check out a fresh course [Cats4AI](https://cats.for.ai/) for a gentle introduction
    to the field.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 3ï¸âƒ£ å»å¹´ï¼Œæˆ‘ä»¬[è®¨è®ºäº†](https://graph-ml-in-2022-where-are-we-now-f7f8242599e0#72d1)å…³äºç®—æ³•å¯¹é½çš„å·¥ä½œï¼Œå¹¶çœ‹åˆ°
    GNNs å¯èƒ½ä¸åŠ¨æ€è§„åˆ’è‰¯å¥½å¯¹é½çš„è¿¹è±¡ã€‚åœ¨ 2022 å¹´ï¼Œ[Dudzik å’Œ VeliÄkoviÄ‡](https://openreview.net/forum?id=wu1Za9dY1GY)
    é€šè¿‡èŒƒç•´è®ºã€æŠ½è±¡ä»£æ•°ä»¥åŠ*æ¨å‰*å’Œ*æ‹‰å›*æ“ä½œçš„æ¦‚å¿µè¯æ˜äº†**GNNs æ˜¯åŠ¨æ€è§„åˆ’å™¨**ã€‚è¿™æ˜¯åº”ç”¨èŒƒç•´è®ºçš„ä¸€ä¸ªç»å¦™ä¾‹å­ï¼Œè®¸å¤šäººè®¤ä¸ºè¿™æ˜¯â€œæŠ½è±¡æ— èŠâ€çš„ğŸ˜‰ã€‚èŒƒç•´è®ºå¯èƒ½ä¼šåœ¨
    GNN ç†è®ºå’Œå›¾ ML ä¸­äº§ç”Ÿæ›´å¤§çš„å½±å“ï¼Œå› æ­¤å¯ä»¥æŸ¥çœ‹æ–°çš„è¯¾ç¨‹ [Cats4AI](https://cats.for.ai/) ä»¥è·å¾—å¯¹è¯¥é¢†åŸŸçš„æ¸©å’Œä»‹ç»ã€‚
- en: 4ï¸âƒ£ Finally, the work of [Beurer-Kellner et al.](https://openreview.net/forum?id=AiY6XvomZV4)
    is one of the first practical application of the neural algorithmic reasoning
    framework, here it is applied to configuring computer networks, i.e., routing
    protocols like BGP that are at the core of the internet. There, the authors show
    that representing a routing config as a graph allows to frame the routing problem
    as node property prediction. This approach brings whopping ğŸ‘€ **490x** ğŸ‘€ speedups
    compared to traditional rule-based routing methods and stil maintain 90+% specification
    consistency.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 4ï¸âƒ£ æœ€åï¼Œ[Beurer-Kellner ç­‰äºº](https://openreview.net/forum?id=AiY6XvomZV4) çš„å·¥ä½œæ˜¯ç¥ç»ç®—æ³•æ¨ç†æ¡†æ¶çš„é¦–æ‰¹å®é™…åº”ç”¨ä¹‹ä¸€ï¼Œè¿™é‡Œå°†å…¶åº”ç”¨äºé…ç½®è®¡ç®—æœºç½‘ç»œï¼Œå³äº’è”ç½‘æ ¸å¿ƒçš„è·¯ç”±åè®®å¦‚
    BGPã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œä½œè€…å±•ç¤ºäº†å°†è·¯ç”±é…ç½®è¡¨ç¤ºä¸ºå›¾å½¢ï¼Œå¯ä»¥å°†è·¯ç”±é—®é¢˜æ¡†æ¶åŒ–ä¸ºèŠ‚ç‚¹å±æ€§é¢„æµ‹ã€‚è¿™ç§æ–¹æ³•ç›¸æ¯”ä¼ ç»Ÿçš„åŸºäºè§„åˆ™çš„è·¯ç”±æ–¹æ³•å¸¦æ¥äº†æƒŠäººçš„ğŸ‘€ **490x**
    ğŸ‘€åŠ é€Ÿï¼Œå¹¶ä¸”ä»ç„¶ä¿æŒ90%ä»¥ä¸Šçš„è§„èŒƒä¸€è‡´æ€§ã€‚
- en: '![](../Images/9045a1b7fb0c1e4f9b6f05c353eaaea9.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9045a1b7fb0c1e4f9b6f05c353eaaea9.png)'
- en: 'Source: [Beurer-Kellner et al.](https://openreview.net/forum?id=AiY6XvomZV4)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[Beurer-Kellner ç­‰](https://openreview.net/forum?id=AiY6XvomZV4)
- en: If you want to follow algorithmic reasoning more closely, donâ€™t miss a fresh
    [LoG 2022 tutorial](https://algo-reasoning.github.io/) by â€‹â€‹Petar VeliÄkoviÄ‡,
    Andreea Deac and Andrew Dudzik.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³æ›´æ·±å…¥åœ°äº†è§£ç®—æ³•æ¨ç†ï¼Œä¸è¦é”™è¿‡ Petar VeliÄkoviÄ‡ã€Andreea Deac å’Œ Andrew Dudzik æœ€æ–°çš„ [LoG 2022
    æ•™ç¨‹](https://algo-reasoning.github.io/)ã€‚
- en: '**What to expect in 2023: *1ï¸âƒ£*** Algorithmic reasoning tasks are likely to
    scale to graphs of thousands of nodes and practical applications like in code
    analysis or databases, *2ï¸âƒ£* even more algorithms in the benchmark, *3ï¸âƒ£* most
    unlikely â€” there will appear a model capable of solving quickselect *ğŸ˜…*'
  id: totrans-162
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2023 å¹´çš„å±•æœ›ï¼š *1ï¸âƒ£*** ç®—æ³•æ¨ç†ä»»åŠ¡å¯èƒ½ä¼šæ‰©å±•åˆ°æˆåƒä¸Šä¸‡ä¸ªèŠ‚ç‚¹çš„å›¾ï¼Œä»¥åŠä»£ç åˆ†ææˆ–æ•°æ®åº“ç­‰å®é™…åº”ç”¨ï¼Œ *2ï¸âƒ£* æ›´å¤šçš„åŸºå‡†ç®—æ³•ï¼Œ
    *3ï¸âƒ£* æœ€ä¸å¯èƒ½â€”â€”ä¼šå‡ºç°ä¸€ä¸ªèƒ½å¤Ÿè§£å†³ quickselect çš„æ¨¡å‹ *ğŸ˜…*'
- en: Cool GNN Applications
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é…·ç‚«çš„ GNN åº”ç”¨
- en: ğŸ‘ƒ **Learning to Smell with GNNs.** Back in 2019, Google AI started a [project](https://ai.googleblog.com/2019/10/learning-to-smell-using-deep-learning.html)
    on learning representations of smells. From basic chemistry we know that aromaticity
    depends on the molecular structure, e.g., cyclic compounds. In fact, the whole
    group of â€aromatic hydrocarbonsâ€ was named *aromatic* because they actually has
    some smell (compared to many non-organic molecules). If we have a molecular structure,
    we can employ a GNN on top of it and learn some representations!
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ‘ƒ **ä½¿ç”¨ GNN å­¦ä¹ å—…è§‰ã€‚** æ—©åœ¨2019å¹´ï¼ŒGoogle AI å¼€å§‹äº†ä¸€ä¸ªå…³äºå—…è§‰è¡¨ç¤ºå­¦ä¹ çš„ [é¡¹ç›®](https://ai.googleblog.com/2019/10/learning-to-smell-using-deep-learning.html)ã€‚ä»åŸºç¡€åŒ–å­¦çŸ¥è¯†æˆ‘ä»¬çŸ¥é“ï¼ŒèŠ³é¦™æ€§å–å†³äºåˆ†å­ç»“æ„ï¼Œä¾‹å¦‚ç¯çŠ¶åŒ–åˆç‰©ã€‚äº‹å®ä¸Šï¼Œæ•´ä¸ªâ€œèŠ³é¦™çƒƒâ€ç»„è¢«å‘½åä¸º
    *èŠ³é¦™çš„* æ˜¯å› ä¸ºå®ƒä»¬å®é™…ä¸Šæœ‰ä¸€äº›æ°”å‘³ï¼ˆä¸è®¸å¤šæ— æœºåˆ†å­ç›¸æ¯”ï¼‰ã€‚å¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ªåˆ†å­ç»“æ„ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å…¶åŸºç¡€ä¸Šä½¿ç”¨ GNN æ¥å­¦ä¹ ä¸€äº›è¡¨ç¤ºï¼
- en: 'Recently, Google AI released [a new blogpost](https://ai.googleblog.com/2022/09/digitizing-smell-using-molecular-maps.html)
    and paper by [Qian et al.](https://www.biorxiv.org/content/10.1101/2022.07.21.500995v3)
    describing the next phase of the project â€” the **Principal Odor Map** that is
    able to group molecules in â€œodor clustersâ€. The authors conducted 3 cool experiments:
    classifying 400 new molecules never smelled before and comparison to the averaged
    rating of a group of human panelists; linking odor quality to fundamental biology;
    and probing aromatic molecules on their mosquito repelling qualities. The GNN-based
    model shows very good results â€” now we can finally claim that GNNs can smell!
    Looking forward for GNNs transforming the perfume industry.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘ï¼ŒGoogle AI å‘å¸ƒäº† [ä¸€ç¯‡æ–°åšå®¢æ–‡ç« ](https://ai.googleblog.com/2022/09/digitizing-smell-using-molecular-maps.html)
    å’Œ [Qian ç­‰äºº](https://www.biorxiv.org/content/10.1101/2022.07.21.500995v3) çš„è®ºæ–‡ï¼Œæè¿°äº†é¡¹ç›®çš„ä¸‹ä¸€é˜¶æ®µâ€”â€”**ä¸»è¦æ°”å‘³åœ°å›¾**ï¼Œèƒ½å¤Ÿå°†åˆ†å­åˆ†ç»„ä¸ºâ€œæ°”å‘³ç°‡â€ã€‚ä½œè€…è¿›è¡Œäº†ä¸‰é¡¹æœ‰è¶£çš„å®éªŒï¼šå¯¹400ç§ä¹‹å‰ä»æœªé—»è¿‡çš„æ–°åˆ†å­è¿›è¡Œåˆ†ç±»ï¼Œå¹¶ä¸ä¸€ç»„äººç±»è¯„å®¡å‘˜çš„å¹³å‡è¯„åˆ†è¿›è¡Œæ¯”è¾ƒï¼›å°†æ°”å‘³è´¨é‡ä¸åŸºç¡€ç”Ÿç‰©å­¦å…³è”ï¼›ä»¥åŠæ¢æµ‹èŠ³é¦™åˆ†å­çš„é©±èšŠç‰¹æ€§ã€‚åŸºäº
    GNN çš„æ¨¡å‹è¡¨ç°éå¸¸å‡ºè‰²â€”â€”ç°åœ¨æˆ‘ä»¬å¯ä»¥è‡ªä¿¡åœ°è¯´ GNN å¯ä»¥å—…è§‰ï¼æœŸå¾… GNN åœ¨é¦™æ°´è¡Œä¸šçš„å˜é©ã€‚
- en: '![](../Images/676191b7f09085ed9d141bae32bae385.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/676191b7f09085ed9d141bae32bae385.png)'
- en: 'Embedding of odors. Source: [Google AI blog](https://ai.googleblog.com/2022/09/digitizing-smell-using-molecular-maps.html)'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: æ°”å‘³çš„åµŒå…¥ã€‚æ¥æºï¼š[Google AI åšå®¢](https://ai.googleblog.com/2022/09/digitizing-smell-using-molecular-maps.html)
- en: 'âš½ **GNNs + Football.** If you thought that sophisticated GNNs for modelling
    trajectories are only used for molecular dynamics and arcane quantum simulations,
    fear not! Here is a cool practical application with a very high potential outreach:
    **Graph Imputer** by [Omidshafiei et al.](https://www.nature.com/articles/s41598-022-12547-0.epdf?sharing_token=HmyoHCAtNdoDfjlObtCiltRgN0jAjWel9jnR3ZoTv0NzQifNnvllGA8o7uZB3n1gdCaC-3jfBQwxpTCJNR7isTeW2uWhYUL8hz8MmWvyYQLogAFNcVp5ZZuTr_O-slFsi4f4-5pz3J2Th9rSxCJV-s63f-q5fojV0FBGNWKYlRQ%3D),
    DeepMind, and FC Liverpool predicts trajectories of football players (and the
    ball). Each game graph consists of 23 nodes, gets updated with a standard message
    passing encoder and a special time-dependent LSTM. The dataset is quite novel,
    too â€” it consists of 105 English Premier League matches (avg 90 min each), all
    players and the ball were tracked at 25 fps, and the resulting training trajectory
    sequences encode about 9.6 seconds of gameplay.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: âš½ **GNNs + è¶³çƒã€‚** å¦‚æœä½ è®¤ä¸ºç”¨äºå»ºæ¨¡è½¨è¿¹çš„å¤æ‚GNNä»…ç”¨äºåˆ†å­åŠ¨åŠ›å­¦å’Œæ·±å¥¥çš„é‡å­æ¨¡æ‹Ÿï¼Œé‚£å°±ä¸è¦æ‹…å¿ƒï¼è¿™é‡Œæœ‰ä¸€ä¸ªéå¸¸æœ‰æ½œåŠ›çš„å®é™…åº”ç”¨ï¼š**Graph
    Imputer** ç”± [Omidshafiei ç­‰](https://www.nature.com/articles/s41598-022-12547-0.epdf?sharing_token=HmyoHCAtNdoDfjlObtCiltRgN0jAjWel9jnR3ZoTv0NzQifNnvllGA8o7uZB3n1gdCaC-3jfBQwxpTCJNR7isTeW2uWhYUL8hz8MmWvyYQLogAFNcVp5ZZuTr_O-slFsi4f4-5pz3J2Th9rSxCJV-s63f-q5fojV0FBGNWKYlRQ%3D)
    æå‡ºçš„ï¼ŒDeepMind å’Œåˆ©ç‰©æµ¦è¶³çƒä¿±ä¹éƒ¨é¢„æµ‹è¶³çƒè¿åŠ¨å‘˜ï¼ˆä»¥åŠè¶³çƒï¼‰çš„è½¨è¿¹ã€‚æ¯åœºæ¯”èµ›å›¾åŒ…å«23ä¸ªèŠ‚ç‚¹ï¼Œé€šè¿‡æ ‡å‡†æ¶ˆæ¯ä¼ é€’ç¼–ç å™¨å’Œç‰¹æ®Šçš„æ—¶é—´ä¾èµ–LSTMè¿›è¡Œæ›´æ–°ã€‚æ•°æ®é›†ä¹Ÿéå¸¸æ–°é¢–â€”â€”å®ƒåŒ…å«äº†105åœºè‹±è¶…æ¯”èµ›ï¼ˆæ¯åœºæ¯”èµ›å¹³å‡90åˆ†é’Ÿï¼‰ï¼Œæ‰€æœ‰çƒå‘˜å’Œè¶³çƒçš„è¿åŠ¨è¢«ä»¥25å¸§æ¯ç§’çš„é€Ÿåº¦è·Ÿè¸ªï¼Œå¾—åˆ°çš„è®­ç»ƒè½¨è¿¹åºåˆ—ç¼–ç äº†å¤§çº¦9.6ç§’çš„æ¯”èµ›è¿‡ç¨‹ã€‚
- en: The paper is easy to read and has numerous football illustrations, check it
    out! Sports tech is actively growing those days, and football analysts now could
    go even deeper in studying their competitors. Will EPL clubs compete for GNN researchers
    in the upcoming transfer windows? Time to create transfermarkt for GNN researchers
    ğŸ˜‰
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç¯‡è®ºæ–‡æ˜“äºé˜…è¯»ï¼Œå¹¶æœ‰å¤§é‡çš„è¶³çƒæ’å›¾ï¼Œå¿«å»çœ‹çœ‹å§ï¼ä½“è‚²æŠ€æœ¯åœ¨è¿™äº›å¹´é‡Œè¿…é€Ÿå‘å±•ï¼Œè¶³çƒåˆ†æå¸ˆç°åœ¨å¯ä»¥æ›´æ·±å…¥åœ°ç ”ç©¶ä»–ä»¬çš„å¯¹æ‰‹ã€‚è‹±è¶…ä¿±ä¹éƒ¨ä¼šåœ¨å³å°†åˆ°æ¥çš„è½¬ä¼šçª—å£ä¸­ç«äº‰GNNç ”ç©¶äººå‘˜å—ï¼Ÿæ˜¯æ—¶å€™ä¸ºGNNç ”ç©¶äººå‘˜åˆ›å»ºä¸€ä¸ªtransfermarktäº†ğŸ˜‰
- en: '![](../Images/4636afc466b88dba1d5146bab2df8221.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4636afc466b88dba1d5146bab2df8221.png)'
- en: 'Football match simulation is like molecular dynamics simulation! Source: [DeepMind](https://twitter.com/deepmind/status/1529444212864843777?lang=en)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: è¶³çƒæ¯”èµ›æ¨¡æ‹Ÿå°±åƒåˆ†å­åŠ¨åŠ›å­¦æ¨¡æ‹Ÿä¸€æ ·ï¼æ¥æºï¼š[DeepMind](https://twitter.com/deepmind/status/1529444212864843777?lang=en)
- en: 'ğŸª **Galaxies and Astrophysics.** For astrophysics aficionados: **Mangrove**
    by [Jespersen et al.](https://arxiv.org/abs/2210.13473) applies GraphSAGE to merger
    trees of dark matter to predict a variety of galactic properties like stellar
    mass, cold gas mass, star formation rate, and even black hole mass. The paper
    is a bit heavy on the terminology of astrophysics but pretty easy in terms of
    GNN parameterization and training. Mangrove works 4â€“9 orders of magnitude faster
    than standard models. Experimental charts are pieces of art that you can hang
    on a wall ğŸ–¼ï¸.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸª **é“¶æ²³ç³»ä¸å¤©ä½“ç‰©ç†å­¦ã€‚** å¯¹å¤©ä½“ç‰©ç†å­¦çˆ±å¥½è€…è€Œè¨€ï¼š**Mangrove** ç”± [Jespersen ç­‰](https://arxiv.org/abs/2210.13473)
    æå‡ºçš„åº”ç”¨GraphSAGEäºæš—ç‰©è´¨çš„åˆå¹¶æ ‘ï¼Œä»¥é¢„æµ‹å„ç§é“¶æ²³å±æ€§ï¼Œå¦‚æ’æ˜Ÿè´¨é‡ã€å†·æ°”ä½“è´¨é‡ã€æ˜Ÿå½¢æˆç‡ï¼Œç”šè‡³é»‘æ´è´¨é‡ã€‚å°½ç®¡è¿™ç¯‡è®ºæ–‡åœ¨å¤©ä½“ç‰©ç†å­¦æœ¯è¯­ä¸Šæœ‰äº›å¤æ‚ï¼Œä½†åœ¨GNNå‚æ•°åŒ–å’Œè®­ç»ƒæ–¹é¢ç›¸å¯¹ç®€å•ã€‚Mangroveçš„é€Ÿåº¦æ¯”æ ‡å‡†æ¨¡å‹å¿«4åˆ°9ä¸ªæ•°é‡çº§ã€‚å®éªŒå›¾è¡¨å°±åƒè‰ºæœ¯å“ä¸€æ ·ï¼Œå¯ä»¥æŒ‚åœ¨å¢™ä¸Š
    ğŸ–¼ï¸ã€‚
- en: '![](../Images/77efdaa485947c23151de37ec221a2cb.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77efdaa485947c23151de37ec221a2cb.png)'
- en: 'Mangrove approach to present dark matter halos as merger trees and graphs.
    Source: [Jespersen et al.](https://arxiv.org/abs/2210.13473)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Mangroveå°†æš—ç‰©è´¨æ™•å‘ˆç°ä¸ºåˆå¹¶æ ‘å’Œå›¾ã€‚æ¥æºï¼š[Jespersen ç­‰](https://arxiv.org/abs/2210.13473)
- en: 'ğŸ¤– **GNNs for code**. Code generation models like AlphaCode and Codex have mindblowing
    capabilities. Although LLMs are at the core of those models, GNNs do help in a
    few neat ways: **Instruction Pointer Attention GNNs** (IPA-GNNs) first proposed
    by [Bieber et al](https://arxiv.org/abs/2010.12621) have been used to [predict
    runtime errors](https://arxiv.org/abs/2203.03771) in competitive programming tasks
    â€” so it is almost like a virtual code interpreter! **CodeTrek** by [Pashakhanloo
    et al.](https://openreview.net/forum?id=WQc075jmBmf) proposes to model a program
    as a relational graph and embed it via random walks and Transformer encoder. Downstream
    applications include variable misuse, prediction exceptions, predicting shadowed
    variables.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤– **GNNsç”¨äºä»£ç **ã€‚åƒAlphaCodeå’ŒCodexè¿™æ ·çš„ä»£ç ç”Ÿæˆæ¨¡å‹å…·æœ‰ä»¤äººæƒŠå¹çš„èƒ½åŠ›ã€‚è™½ç„¶LLMsæ˜¯è¿™äº›æ¨¡å‹çš„æ ¸å¿ƒï¼Œä½†GNNsåœ¨å‡ ä¸ªå·§å¦™çš„æ–¹é¢ç¡®å®æœ‰å¸®åŠ©ï¼š**æŒ‡ä»¤æŒ‡é’ˆæ³¨æ„åŠ›GNNs**ï¼ˆIPA-GNNsï¼‰é¦–æ¬¡ç”±[Bieber
    et al](https://arxiv.org/abs/2010.12621)æå‡ºï¼Œç”¨äº[é¢„æµ‹è¿è¡Œæ—¶é”™è¯¯](https://arxiv.org/abs/2203.03771)åœ¨ç«èµ›ç¼–ç¨‹ä»»åŠ¡ä¸­â€”â€”è¿™å‡ ä¹å°±åƒä¸€ä¸ªè™šæ‹Ÿçš„ä»£ç è§£é‡Šå™¨ï¼ç”±[Pashakhanloo
    et al.](https://openreview.net/forum?id=WQc075jmBmf)æå‡ºçš„**CodeTrek**å»ºè®®å°†ç¨‹åºå»ºæ¨¡ä¸ºå…³ç³»å›¾ï¼Œå¹¶é€šè¿‡éšæœºæ¸¸èµ°å’ŒTransformerç¼–ç å™¨è¿›è¡ŒåµŒå…¥ã€‚ä¸‹æ¸¸åº”ç”¨åŒ…æ‹¬å˜é‡è¯¯ç”¨ã€é¢„æµ‹å¼‚å¸¸å’Œé¢„æµ‹è¢«é®è”½çš„å˜é‡ã€‚
- en: '![](../Images/ab5c9d787662778c2d57153876512efe.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ab5c9d787662778c2d57153876512efe.png)'
- en: 'Source: [Pashakhanloo et al.](https://openreview.net/forum?id=WQc075jmBmf)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[Pashakhanloo et al.](https://openreview.net/forum?id=WQc075jmBmf)
- en: 'Hardware: IPUs and Graphcore Win OGB Large-Scale Challenge 2022'
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¡¬ä»¶ï¼šIPUså’ŒGraphcoreèµ¢å¾—OGBå¤§è§„æ¨¡æŒ‘æˆ˜èµ›2022
- en: ğŸ¥‡ 2022 brought a huge success to [Graphcore](https://www.graphcore.ai/) and
    [IPUs](https://www.graphcore.ai/bow-processors) â€” the hardware optimized for sparse
    operations that are so needed when working with graphs. The first success story
    was optimizing Temporal Graph Nets (TGN) for IPUs with massive performance gains
    (check the [article](/accelerating-and-scaling-temporal-graph-networks-on-the-graphcore-ipu-c15ac309b765)
    in Michael Bronsteinâ€™s blog).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¥‡ 2022å¹´ä¸º[Graphcore](https://www.graphcore.ai/)å’Œ[IPUs](https://www.graphcore.ai/bow-processors)å¸¦æ¥äº†å·¨å¤§çš„æˆåŠŸâ€”â€”è¿™äº›ç¡¬ä»¶ä¸“é—¨ä¼˜åŒ–äº†å¤„ç†å›¾å½¢æ—¶éå¸¸éœ€è¦çš„ç¨€ç–æ“ä½œã€‚ç¬¬ä¸€ä¸ªæˆåŠŸæ•…äº‹æ˜¯ä¼˜åŒ–äº†IPUsä¸Šçš„Temporal
    Graph Nets (TGN)ï¼Œå–å¾—äº†å·¨å¤§çš„æ€§èƒ½æå‡ï¼ˆæŸ¥çœ‹Michael Bronsteinåšå®¢ä¸­çš„[æ–‡ç« ](/accelerating-and-scaling-temporal-graph-networks-on-the-graphcore-ipu-c15ac309b765)ï¼‰ã€‚
- en: '[](/accelerating-and-scaling-temporal-graph-networks-on-the-graphcore-ipu-c15ac309b765?source=post_page-----1ba920cb9232--------------------------------)
    [## Accelerating and scaling Temporal Graph Networks on the Graphcore IPU'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[## åŠ é€Ÿå’Œæ‰©å±•åœ¨Graphcore IPUä¸Šçš„Temporal Graph Networks](/accelerating-and-scaling-temporal-graph-networks-on-the-graphcore-ipu-c15ac309b765?source=post_page-----1ba920cb9232--------------------------------)'
- en: Is GPU the best hardware choice for GNNs? Together with Graphcore, we explore
    the advantages of the new IPUâ€¦
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GPUæ˜¯GNNsçš„æœ€ä½³ç¡¬ä»¶é€‰æ‹©å—ï¼Ÿä¸Graphcoreä¸€èµ·ï¼Œæˆ‘ä»¬æ¢è®¨äº†æ–°IPUçš„ä¼˜åŠ¿â€¦â€¦
- en: towardsdatascience.com](/accelerating-and-scaling-temporal-graph-networks-on-the-graphcore-ipu-c15ac309b765?source=post_page-----1ba920cb9232--------------------------------)
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/accelerating-and-scaling-temporal-graph-networks-on-the-graphcore-ipu-c15ac309b765?source=post_page-----1ba920cb9232--------------------------------)'
- en: 'Later on, Graphcore [stormed the leaderboards](https://www.graphcore.ai/posts/graphcore-claims-double-win-in-open-graph-benchmark-challenge)
    of OGB LSCâ€™22 by winning 2 out of 3 tracks: link prediction on the **WikiKG90M
    v2** knowledge graph and graph regression on the **PCQM4M v2** molecular dataset.
    In addition to the sheer compute power, the authors took several clever model
    decisions: for link prediction it was [Balanced Entity Sampling and Sharing (BESS)](https://arxiv.org/abs/2211.12281)
    for training an ensemble of shallow LP models (check the [blog post](/large-scale-knowledge-graph-completion-on-ipu-4cf386dfa826)
    by Daniel Justus for more details), and GPS++ for the graph regression task (we
    covered GPS++ above in the GT section). You can [try out](https://ipu.dev/3FwVoLD)
    the pre-trained models using IPUs-powered virtual machines on Paperspace. Congratulations
    to Graphcore and their team! ğŸ‘'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤åï¼ŒGraphcoreåœ¨OGB LSCâ€™22çš„æ’è¡Œæ¦œä¸Š[å¤§æ”¾å¼‚å½©](https://www.graphcore.ai/posts/graphcore-claims-double-win-in-open-graph-benchmark-challenge)ï¼Œåœ¨3ä¸ªèµ›é“ä¸­èµ¢å¾—äº†2ä¸ªï¼š**WikiKG90M
    v2**çŸ¥è¯†å›¾è°±ä¸Šçš„é“¾æ¥é¢„æµ‹å’Œ**PCQM4M v2**åˆ†å­æ•°æ®é›†ä¸Šçš„å›¾å›å½’ã€‚é™¤äº†å¼ºå¤§çš„è®¡ç®—èƒ½åŠ›å¤–ï¼Œä½œè€…è¿˜åšå‡ºäº†ä¸€äº›å·§å¦™çš„æ¨¡å‹å†³ç­–ï¼šå¯¹äºé“¾æ¥é¢„æµ‹ï¼Œé‡‡ç”¨äº†[å¹³è¡¡å®ä½“é‡‡æ ·å’Œå…±äº«
    (BESS)](https://arxiv.org/abs/2211.12281)æ¥è®­ç»ƒä¸€ä¸ªæµ…å±‚LPæ¨¡å‹çš„é›†åˆï¼ˆæŸ¥çœ‹Daniel Justusçš„[åšå®¢æ–‡ç« ](/large-scale-knowledge-graph-completion-on-ipu-4cf386dfa826)äº†è§£æ›´å¤šç»†èŠ‚ï¼‰ï¼Œå¯¹äºå›¾å›å½’ä»»åŠ¡é‡‡ç”¨äº†GPS++ï¼ˆæˆ‘ä»¬åœ¨GTéƒ¨åˆ†ä¸­è®¨è®ºäº†GPS++ï¼‰ã€‚ä½ å¯ä»¥[å°è¯•](https://ipu.dev/3FwVoLD)åœ¨Paperspaceä¸Šä½¿ç”¨IPUsæ”¯æŒçš„è™šæ‹Ÿæœºæ¥ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ã€‚ç¥è´ºGraphcoreåŠå…¶å›¢é˜Ÿï¼ğŸ‘
- en: PyG partnered with NVIDIA ([post](https://pyg.org/ns-newsarticle-accelerating-pyg-on-nvidia-gpus))
    and Intel ([post](https://pyg.org/news/accelerating-pyg-on-intel-cpus)) to increase
    the performance of core operations on GPUs and CPUs, respectively. Similarly,
    DGL [incorporated](https://www.dgl.ai/release/2022/07/25/release.html) new GPU
    optimizations in the recent 0.9 version. Massive gains for sparse matmuls and
    sampling procedures, so weâ€™d encourage you to update your environments with the
    most recent versions!.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: PyG ä¸ NVIDIA ([post](https://pyg.org/ns-newsarticle-accelerating-pyg-on-nvidia-gpus))
    å’Œ Intel ([post](https://pyg.org/news/accelerating-pyg-on-intel-cpus)) åˆä½œï¼Œåˆ†åˆ«æé«˜äº†
    GPU å’Œ CPU ä¸Šæ ¸å¿ƒæ“ä½œçš„æ€§èƒ½ã€‚ç±»ä¼¼åœ°ï¼ŒDGL [çº³å…¥](https://www.dgl.ai/release/2022/07/25/release.html)
    äº†æœ€è¿‘ 0.9 ç‰ˆæœ¬ä¸­çš„æ–° GPU ä¼˜åŒ–ã€‚å¯¹äºç¨€ç–çŸ©é˜µä¹˜æ³•å’Œé‡‡æ ·ç¨‹åºéƒ½æœ‰å¤§å¹…æå‡ï¼Œå› æ­¤æˆ‘ä»¬å»ºè®®ä½ æ›´æ–°åˆ°æœ€æ–°ç‰ˆæœ¬çš„ç¯å¢ƒï¼
- en: '**What to expect in 2023**: major GNN libraries are likely to increase the
    breadth of supported hardware backends such as IPUs or upcoming Intel Max Series
    GPUs.'
  id: totrans-185
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2023 å¹´çš„æœŸæœ›ï¼š** ä¸»è¦ GNN åº“å¯èƒ½ä¼šæ‰©å±•å¯¹ç¡¬ä»¶åç«¯çš„æ”¯æŒï¼Œå¦‚ IPU æˆ–å³å°†æ¨å‡ºçš„ Intel Max ç³»åˆ— GPUã€‚'
- en: 'New Conferences: Learning of Graphs (LoG) and Molecular ML (MoML)'
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ–°ä¼šè®®ï¼šå­¦ä¹ å›¾å½¢ï¼ˆLoGï¼‰å’Œåˆ†å­ MLï¼ˆMoMLï¼‰
- en: 'This year we witnessed the inauguration of two graph and geometric ML conferences:
    the [Learning on Graphs Conference (LoG)](https://logconference.org/#hero) and
    the [Molecular ML Conference](https://www.moml22.mit.edu/) (MoML).'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ä»Šå¹´æˆ‘ä»¬è§è¯äº†ä¸¤ä¸ªå›¾å½¢å’Œå‡ ä½• ML ä¼šè®®çš„å¯åŠ¨ï¼š[å­¦ä¹ å›¾å½¢ä¼šè®®ï¼ˆLoGï¼‰](https://logconference.org/#hero) å’Œ [åˆ†å­
    ML ä¼šè®®](https://www.moml22.mit.edu/)ï¼ˆMoMLï¼‰ã€‚
- en: LoG is a more general all-around GraphML venue (held virtually this year) while
    MoML (held at MIT) has a broader mission and influence over the AI4Science community
    where graphs and geometry still plays a major role. Both conferences were received
    extremely well. MoML attracted 7 top speakers and 38 posters, LoG had ~3000 registrations,
    266 submissions, 71 posters, 12 orals, and 7 awesome tutorials (all recordings
    of oral talks and tutorials are [already on YouTube](https://www.youtube.com/@learningongraphs)).
    Besides, LoG introduced a great monetary incentive for reviewers, resulting in
    a well-recognized improvement of the review quality! From our point of view, quality
    of LoG reviews was often better than those at NeurIPS or ICML.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: LoG æ˜¯ä¸€ä¸ªæ›´é€šç”¨çš„å…¨æ–¹ä½ GraphML ä¼šè®®ï¼ˆä»Šå¹´ä»¥è™šæ‹Ÿå½¢å¼ä¸¾è¡Œï¼‰ï¼Œè€Œ MoMLï¼ˆåœ¨ MIT ä¸¾åŠï¼‰å…·æœ‰æ›´å¹¿æ³›çš„ä½¿å‘½å’Œå¯¹ AI4Science ç¤¾åŒºçš„å½±å“ï¼Œå…¶ä¸­å›¾å’Œå‡ ä½•ä»ç„¶æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚è¿™ä¸¤ä¸ªä¼šè®®éƒ½å¾—åˆ°äº†æå¥½çš„åå“ã€‚MoML
    å¸å¼•äº† 7 ä½é¡¶çº§è®²è€…å’Œ 38 ä¸ªæµ·æŠ¥ï¼ŒLoG æœ‰å¤§çº¦ 3000 ä¸ªæ³¨å†Œã€266 ä¸ªæŠ•ç¨¿ã€71 ä¸ªæµ·æŠ¥ã€12 ä¸ªå£å¤´æŠ¥å‘Šå’Œ 7 ä¸ªç²¾å½©çš„æ•™ç¨‹ï¼ˆæ‰€æœ‰å£å¤´æŠ¥å‘Šå’Œæ•™ç¨‹çš„å½•éŸ³[å·²åœ¨
    YouTube](https://www.youtube.com/@learningongraphs) ä¸Šï¼‰ã€‚æ­¤å¤–ï¼ŒLoG ä¸ºè¯„å®¡å¼•å…¥äº†ä¸°åšçš„å¥–é‡‘æ¿€åŠ±ï¼Œæ˜¾è‘—æé«˜äº†è¯„å®¡è´¨é‡ï¼åœ¨æˆ‘ä»¬çœ‹æ¥ï¼ŒLoG
    çš„è¯„å®¡è´¨é‡é€šå¸¸ä¼˜äº NeurIPS æˆ– ICML çš„è¯„å®¡ã€‚
- en: This is a huge win and carnival for the graph ML community, and congrats to
    everyone working in the field of graph and geometric machine learning with a new
    â€œhomeâ€ venue!
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å›¾å½¢ ML ç¤¾åŒºçš„ä¸€æ¬¡å·¨å¤§èƒœåˆ©å’Œåº†å…¸ï¼Œç¥è´ºæ‰€æœ‰åœ¨å›¾å½¢å’Œå‡ ä½•æœºå™¨å­¦ä¹ é¢†åŸŸå·¥ä½œçš„äººå‘˜ï¼Œæ‹¥æœ‰äº†ä¸€ä¸ªæ–°çš„â€œå®¶â€ä¼šè®®ï¼
- en: '**What to expect in 2023:** LOG and MoML become main Graph ML venues to include
    into your submission calendar along with ICLR / NeurIPS / ICML'
  id: totrans-190
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2023 å¹´çš„æœŸæœ›ï¼š** LOG å’Œ MoML å°†æˆä¸ºåŒ…æ‹¬åœ¨æäº¤æ—¥å†ä¸­çš„ä¸»è¦ Graph ML ä¼šè®®ï¼Œæ­¤å¤–è¿˜æœ‰ ICLR / NeurIPS /
    ICMLã€‚'
- en: Courses and Educational Materials
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¯¾ç¨‹å’Œæ•™è‚²ææ–™
- en: Geometric Deep Learning Course â€” [Second Edition](https://www.youtube.com/playlist?list=PLn2-dEmQeTfSLXW8yXP4q_Ii58wFdxb3C)
    (2022) is already on YouTube. The main entry point to the field.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‡ ä½•æ·±åº¦å­¦ä¹ è¯¾ç¨‹ â€” [ç¬¬äºŒç‰ˆ](https://www.youtube.com/playlist?list=PLn2-dEmQeTfSLXW8yXP4q_Ii58wFdxb3C)ï¼ˆ2022
    å¹´ï¼‰å·²åœ¨ YouTube ä¸Šã€‚è¯¥é¢†åŸŸçš„ä¸»è¦å…¥é—¨ç‚¹ã€‚
- en: '[An Introduction to Group Equivariant Deep Learning](https://uvagedl.github.io/)
    by Erik Bekkers â€” one of the best new courses about equivariance and equivariant
    models!'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ç¾¤ä½“ç­‰å˜æ·±åº¦å­¦ä¹ ä»‹ç»](https://uvagedl.github.io/) ç”± Erik Bekkers æä¾› â€” è¿™æ˜¯å…³äºç­‰å˜æ€§å’Œç­‰å˜æ¨¡å‹çš„æœ€ä½³æ–°è¯¾ç¨‹ä¹‹ä¸€ï¼'
- en: '[Cats4AI](https://cats.for.ai/) â€” a new course by Andrew Dudzik, Bruno GavranoviÄ‡,
    JoÃ£o Guilherme AraÃºjo, Petar VeliÄkoviÄ‡, and Pim de Haan is the best place to
    learn about category theory and its connections to Geometric DL.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Cats4AI](https://cats.for.ai/) â€” ç”± Andrew Dudzikã€Bruno GavranoviÄ‡ã€JoÃ£o Guilherme
    AraÃºjoã€Petar VeliÄkoviÄ‡ å’Œ Pim de Haan å¼€è®¾çš„æ–°è¯¾ç¨‹ï¼Œæ˜¯äº†è§£èŒƒç•´ç†è®ºåŠå…¶ä¸å‡ ä½•æ·±åº¦å­¦ä¹ è¿æ¥çš„æœ€ä½³åœºæ‰€ã€‚'
- en: 'Summer School proceedings: [Italian Summer School on Geometric DL](https://www.sci.unich.it/geodeep2022/#home),
    London Geometry and Machine Learning ([LOGML](https://www.logml.ai/home-2022))
    Summer School, [BIRS Workshop on Topological Representation Learning](https://www.birs.ca/events/2022/5-day-workshops/22w5125).'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¤å­£å­¦æ ¡æˆæœï¼š[æ„å¤§åˆ©å‡ ä½•æ·±åº¦å­¦ä¹ å¤å­£å­¦æ ¡](https://www.sci.unich.it/geodeep2022/#home)ã€ä¼¦æ•¦å‡ ä½•ä¸æœºå™¨å­¦ä¹ ï¼ˆ[LOGML](https://www.logml.ai/home-2022)ï¼‰å¤å­£å­¦æ ¡ã€[BIRS
    é¡¶ç‚¹è¡¨ç¤ºå­¦ä¹ ç ”è®¨ä¼š](https://www.birs.ca/events/2022/5-day-workshops/22w5125)ã€‚
- en: '[Stanford Graph Learning Workshop 2022](https://snap.stanford.edu/graphlearning-workshop-2022/)
    â€” latest news from PyG developers and partners and Stanford researchers.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ–¯å¦ç¦å›¾å­¦ä¹ ç ”è®¨ä¼š 2022](https://snap.stanford.edu/graphlearning-workshop-2022/) â€”
    PyGå¼€å‘è€…ã€åˆä½œä¼™ä¼´å’Œæ–¯å¦ç¦ç ”ç©¶äººå‘˜çš„æœ€æ–°æ¶ˆæ¯ã€‚'
- en: New Datasets, Benchmarks, and Challenges
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ–°æ•°æ®é›†ã€åŸºå‡†å’ŒæŒ‘æˆ˜
- en: '[OGB Large-Scale Challenge 2022](https://ogb.stanford.edu/neurips2022/): The
    second large scale challenge held at NeurIPS2022 with large and realistic graph
    ML tasks covering node-, edge-, graph-level predictions.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OGBå¤§è§„æ¨¡æŒ‘æˆ˜ 2022](https://ogb.stanford.edu/neurips2022/): ç¬¬äºŒå±Šå¤§è§„æ¨¡æŒ‘æˆ˜ï¼Œåœ¨NeurIPS2022ä¸Šä¸¾åŠï¼Œæ¶µç›–èŠ‚ç‚¹ã€è¾¹ã€å›¾çº§é¢„æµ‹çš„å¤§è§„æ¨¡å’Œç°å®å›¾æœºå™¨å­¦ä¹ ä»»åŠ¡ã€‚'
- en: '[Open Catalyst 2022 Challenge](https://opencatalystproject.org/challenge.html):
    the second edition of the challenge held at NeurIPS2022 with the task to design
    new machine learning models to predict the outcome of catalyst simulations used
    to understand activity'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å¼€æ”¾å‚¬åŒ–å‰‚ 2022 æŒ‘æˆ˜](https://opencatalystproject.org/challenge.html): ç¬¬äºŒå±ŠæŒ‘æˆ˜ï¼Œåœ¨NeurIPS2022ä¸Šä¸¾åŠï¼Œä»»åŠ¡æ˜¯è®¾è®¡æ–°çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œä»¥é¢„æµ‹å‚¬åŒ–å‰‚æ¨¡æ‹Ÿçš„ç»“æœï¼Œä»¥äº†è§£å…¶æ´»æ€§ã€‚'
- en: '[CASP 15](https://predictioncenter.org/casp15/index.cgi): the protein structure
    prediction challenge disrupted by AlphaFold a few years ago at CASP 14\. Detailed
    analysis is yet to come, but it seems that MSAs strike back and best performing
    models still rely on MSAs.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CASP 15](https://predictioncenter.org/casp15/index.cgi): ç”±AlphaFoldåœ¨CASP 14ä¸­å¼•å‘çš„è›‹ç™½è´¨ç»“æ„é¢„æµ‹æŒ‘æˆ˜ã€‚è¯¦ç»†åˆ†æå°šå¾…å…¬å¸ƒï¼Œä½†ä¼¼ä¹MSAså·åœŸé‡æ¥ï¼Œè¡¨ç°æœ€å¥½çš„æ¨¡å‹ä»ç„¶ä¾èµ–äºMSAsã€‚'
- en: '[Long Range Graph Benchmark](https://arxiv.org/abs/2206.08164): for measuring
    GNNs and GTs capabilities of capturing long range interactions in graphs.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[é•¿è·ç¦»å›¾åŸºå‡†](https://arxiv.org/abs/2206.08164): ç”¨äºæµ‹é‡GNNså’ŒGTsåœ¨å›¾ä¸­æ•æ‰é•¿è·ç¦»äº¤äº’çš„èƒ½åŠ›ã€‚'
- en: '[Taxonomy of Graph Benchmarks](https://arxiv.org/abs/2206.07729), [Graph Learning
    Indexer](https://github.com/Graph-Learning-Benchmarks/gli): deeper studies of
    the dataset landscape in Graph ML outlining open challenges in benchmarking and
    trustworthiness of results.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å›¾åŸºå‡†åˆ†ç±»](https://arxiv.org/abs/2206.07729)ï¼Œ[å›¾å­¦ä¹ ç´¢å¼•å™¨](https://github.com/Graph-Learning-Benchmarks/gli):
    å¯¹å›¾æœºå™¨å­¦ä¹ æ•°æ®é›†æ™¯è§‚çš„æ·±å…¥ç ”ç©¶ï¼Œæ¦‚è¿°äº†åŸºå‡†æµ‹è¯•å’Œç»“æœå¯ä¿¡åº¦ä¸­çš„å¼€æ”¾æŒ‘æˆ˜ã€‚'
- en: '[GraphWorld](https://ai.googleblog.com/2022/05/graphworld-advances-in-graph.html):
    a framework for analyzing the performance of GNN architectures on millions of
    synthetic benchmark datasets'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GraphWorld](https://ai.googleblog.com/2022/05/graphworld-advances-in-graph.html):
    ä¸€ä¸ªæ¡†æ¶ï¼Œç”¨äºåˆ†æGNNæ¶æ„åœ¨æ•°ç™¾ä¸‡ä¸ªåˆæˆåŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚'
- en: '[Chartalist](https://openreview.net/forum?id=10iA3OowAV3) â€” a collection of
    blockchain graph datasets'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Chartalist](https://openreview.net/forum?id=10iA3OowAV3) â€” ä¸€ç³»åˆ—åŒºå—é“¾å›¾æ•°æ®é›†ã€‚'
- en: '[PEER protein learning benchmark](https://github.com/DeepGraphLearning/PEER_Benchmark):
    a multi-task benchmark for protein sequence understanding with 17 tasks of protein
    understanding lying in 5 task categories.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PEERè›‹ç™½è´¨å­¦ä¹ åŸºå‡†](https://github.com/DeepGraphLearning/PEER_Benchmark): ä¸€ä¸ªå¤šä»»åŠ¡è›‹ç™½è´¨åºåˆ—ç†è§£åŸºå‡†ï¼ŒåŒ…æ‹¬17ä¸ªè›‹ç™½è´¨ç†è§£ä»»åŠ¡ï¼Œåˆ†ä¸º5ä¸ªä»»åŠ¡ç±»åˆ«ã€‚'
- en: '[ESM Metagenomic Atlas](https://esmatlas.com/): acomprehensive database of
    over 600 million predicted protein structures with nice visualizations and search
    UI.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ESMå®åŸºå› ç»„å›¾è°±](https://esmatlas.com/): ä¸€ä¸ªç»¼åˆæ€§æ•°æ®åº“ï¼ŒåŒ…å«è¶…è¿‡6äº¿ä¸ªé¢„æµ‹è›‹ç™½è´¨ç»“æ„ï¼Œæä¾›äº†æ¼‚äº®çš„å¯è§†åŒ–å’Œæœç´¢ç•Œé¢ã€‚'
- en: Software Libraries and Open Source
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è½¯ä»¶åº“å’Œå¼€æº
- en: 'Mainstream graph ML libraries: [PyG 2.2](https://www.pyg.org/) (PyTorch), [DGL
    0.9](https://www.dgl.ai/) (PyTorch, TensorFlow, MXNet), [TF GNN](https://github.com/tensorflow/gnn)
    (TensorFlow) and [Jraph](https://github.com/deepmind/jraph) (Jax)'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸»æµå›¾æœºå™¨å­¦ä¹ åº“ï¼š[PyG 2.2](https://www.pyg.org/)ï¼ˆPyTorchï¼‰ï¼Œ[DGL 0.9](https://www.dgl.ai/)ï¼ˆPyTorchã€TensorFlowã€MXNetï¼‰ï¼Œ[TF
    GNN](https://github.com/tensorflow/gnn)ï¼ˆTensorFlowï¼‰å’Œ [Jraph](https://github.com/deepmind/jraph)ï¼ˆJaxï¼‰
- en: '[TorchDrug](https://torchdrug.ai/) and [TorchProtein](https://torchprotein.ai/):
    machine learning library for drug discovery and protein science'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TorchDrug](https://torchdrug.ai/) å’Œ [TorchProtein](https://torchprotein.ai/):
    ç”¨äºè¯ç‰©å‘ç°å’Œè›‹ç™½è´¨ç§‘å­¦çš„æœºå™¨å­¦ä¹ åº“ã€‚'
- en: '[PyKEEN](https://github.com/pykeen/pykeen): the best platform for training
    and evaluating knowledge graph embeddings'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyKEEN](https://github.com/pykeen/pykeen): ç”¨äºè®­ç»ƒå’Œè¯„ä¼°çŸ¥è¯†å›¾è°±åµŒå…¥çš„æœ€ä½³å¹³å°ã€‚'
- en: '[Graphein](https://graphein.ai/): a package that provides a number of types
    of graph-based representations of proteins'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Graphein](https://graphein.ai/): æä¾›å¤šç§åŸºäºå›¾çš„è›‹ç™½è´¨è¡¨ç¤ºçš„åŒ…ã€‚'
- en: '[GRAPE](https://github.com/AnacletoLAB/grape) and [Marius](https://marius-project.org/):
    scalable graph processing and embedding libraries over billion-scale graphs'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GRAPE](https://github.com/AnacletoLAB/grape) å’Œ [Marius](https://marius-project.org/):
    å¯æ‰©å±•çš„å›¾å¤„ç†å’ŒåµŒå…¥åº“ï¼Œå¤„ç†è¶…å¤§è§„æ¨¡å›¾ã€‚'
- en: '[MatSci ML Toolkit](https://github.com/IntelLabs/matsciml): a flexible framework
    for deep learning on the opencatalyst dataset'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MatSci MLå·¥å…·åŒ…](https://github.com/IntelLabs/matsciml): ç”¨äºåœ¨opencatalystæ•°æ®é›†ä¸Šè¿›è¡Œæ·±åº¦å­¦ä¹ çš„çµæ´»æ¡†æ¶ã€‚'
- en: '[E3nn](https://github.com/e3nn/e3nn): the go-to library for E(3) equivariant
    neural networks'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[E3nn](https://github.com/e3nn/e3nn)ï¼šç”¨äº E(3) ç­‰å˜ç¥ç»ç½‘ç»œçš„é¦–é€‰åº“'
- en: Join the Community
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŠ å…¥ç¤¾åŒº
- en: 'Reading Groups: [Learning on Graphs and Geometry](https://m2d2.io/talks/log2/about/)
    (LOG2) reading group, [Molecular Modeling & Drug Discovery](https://m2d2.io/talks/m2d2/about/)
    (M2D2) reading group, and their Slack communities'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é˜…è¯»å°ç»„ï¼š[å›¾å­¦ä¹ ä¸å‡ ä½•](https://m2d2.io/talks/log2/about/)ï¼ˆLOG2ï¼‰é˜…è¯»å°ç»„ï¼Œ[åˆ†å­å»ºæ¨¡ä¸è¯ç‰©å‘ç°](https://m2d2.io/talks/m2d2/about/)ï¼ˆM2D2ï¼‰é˜…è¯»å°ç»„åŠå…¶
    Slack ç¤¾åŒº
- en: Learning of Graphs (LoG) [Slack community](https://logconference.org/)
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å›¾å­¦ä¹ ï¼ˆLoGï¼‰[Slack ç¤¾åŒº](https://logconference.org/)
- en: '[Michael Bronsteinâ€™s blog on Medium](https://michael-bronstein.medium.com/)'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Michael Bronstein çš„ Medium åšå®¢](https://michael-bronstein.medium.com/)'
- en: '[PyG medium](https://medium.com/@pytorch_geometric), [blog posts](https://pyg.org/blogs-and-tutorials),
    and newsletter'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyG medium](https://medium.com/@pytorch_geometric)ã€[åšå®¢æ–‡ç« ](https://pyg.org/blogs-and-tutorials)å’Œé€šè®¯'
- en: '[GraphML Telegram channel](https://t.me/graphML)'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GraphML Telegram é¢‘é“](https://t.me/graphML)'
- en: The Meme of 2022 ğŸª“
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2022 å¹´çš„æµè¡Œæ–‡åŒ–ç°è±¡ ğŸª“
- en: '![](../Images/8b5892b801b9e71ad5914d29b86184f8.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b5892b801b9e71ad5914d29b86184f8.png)'
- en: Created by Michael Galkin and Michael Bronstein
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ç”± Michael Galkin å’Œ Michael Bronstein åˆ›å»º
