- en: 'GPT and Beyond: The Technical Foundations of LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/gpt-and-beyond-the-technical-foundations-of-llms-2e2c89fc6c7a?source=collection_archive---------4-----------------------#2023-08-03](https://towardsdatascience.com/gpt-and-beyond-the-technical-foundations-of-llms-2e2c89fc6c7a?source=collection_archive---------4-----------------------#2023-08-03)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://towardsdatascience.medium.com/?source=post_page-----2e2c89fc6c7a--------------------------------)[![TDS
    Editors](../Images/4b2d1beaf4f6dcf024ffa6535de3b794.png)](https://towardsdatascience.medium.com/?source=post_page-----2e2c89fc6c7a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2e2c89fc6c7a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2e2c89fc6c7a--------------------------------)
    [TDS Editors](https://towardsdatascience.medium.com/?source=post_page-----2e2c89fc6c7a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7e12c71dfa81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-and-beyond-the-technical-foundations-of-llms-2e2c89fc6c7a&user=TDS+Editors&userId=7e12c71dfa81&source=post_page-7e12c71dfa81----2e2c89fc6c7a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2e2c89fc6c7a--------------------------------)
    ·Sent as a [Newsletter](/newsletter?source=post_page-----2e2c89fc6c7a--------------------------------)
    ·3 min read·Aug 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2e2c89fc6c7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-and-beyond-the-technical-foundations-of-llms-2e2c89fc6c7a&user=TDS+Editors&userId=7e12c71dfa81&source=-----2e2c89fc6c7a---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2e2c89fc6c7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-and-beyond-the-technical-foundations-of-llms-2e2c89fc6c7a&source=-----2e2c89fc6c7a---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: In just a few short months, large language models moved from the realm of specialized
    researchers into the everyday workflows of data and ML teams all over the world.
    Here at TDS, we’ve seen how, along with this transition, much of the focus has
    shifted into practical applications and hands-on solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Jumping straight into tinkering mode can make a lot of sense for data professionals
    working in industry—time is precious, after all. Still, it’s always a good idea
    to establish a solid grasp of the inner workings of the technology we use and
    work on, and that’s precisely what our weekly highlights address.
  prefs: []
  type: TYPE_NORMAL
- en: Our recommended reads looks both at the theoretical foundations of LLMs—specifically,
    the GPT family—and at the high-level questions their arrival raises. Even if you’re
    just a casual user of these models, we think you’ll enjoy these thoughtful explorations.
  prefs: []
  type: TYPE_NORMAL
- en: The transformers architecture is the groundbreaking innovation that made GPT
    models possible in the first place. As [Beatriz Stollnitz](https://medium.com/u/1c8863892480?source=post_page-----2e2c89fc6c7a--------------------------------)
    makes clear, “understanding the details of how they work is an important skill
    for every AI practitioner,” and [**you’ll leave her thorough explainer with a
    crystal-clear idea**](/the-transformer-architecture-of-gpt-models-b8695b48728b)
    of transformers’ power.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Lily Hughes-Robinson](https://medium.com/u/5389e25ca1bb?source=post_page-----2e2c89fc6c7a--------------------------------)
    offers [**a different approach for learning about transformers**](/learning-transformers-code-first-part-2-gpt-up-close-and-personal-1635b52ae0d7)**:**
    one that focuses on the source code so you can build your knowledge intuitively
    from the ground up.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How important is size when it comes to LLMs’ performance? [Gadi Singer](https://medium.com/u/51de1f48d0b?source=post_page-----2e2c89fc6c7a--------------------------------)
    dives into this question in great detail as he [**surveys the latest crop of compact
    generative AI models**](/survival-of-the-fittest-compact-generative-ai-models-are-the-future-for-cost-effective-ai-at-scale-6bbdc138f618)**.**
    These contenders aim to compete with GPT-4 in accuracy, but at a lower cost and
    with a greater potential to achieve scalability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/2c84298995d84249504a2861bc494d4e.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [K8](https://unsplash.com/@_k8_?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'Of all the heated debates surrounding ChatGPT and similar tools, perhaps none
    has been more contentious than the question around LLMs’ supposed intelligence.
    [Lan Chu](https://medium.com/u/3916743f0e10?source=post_page-----2e2c89fc6c7a--------------------------------)
    tackles this topic head on, and [**brings a refreshingly measured and pragmatic
    perspective to the conversation**](/is-chatgpt-actually-intelligent-42d07462fe59).
    (Spoiler alert: no, AI isn’t conscious; yes, it’s complicated.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“So, how can we move beyond perceiving LLMs like ChatGPT as magical black boxes?
    Physics may provide an answer.” [Tim Lou, PhD](https://medium.com/u/8d41b438feef?source=post_page-----2e2c89fc6c7a--------------------------------)’s
    latest article proposes a thought-provoking idea: that [**the equations that make
    language models tick are analogous to the laws of physics**](/understanding-large-language-models-the-physics-of-chat-gpt-and-bert-ea512bcc6a64)
    and the way they govern particles and forces.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We published *so* many fantastic articles on other topics in recent weeks; here
    are just a few we absolutely had to highlight.
  prefs: []
  type: TYPE_NORMAL
- en: Who says summer reading has to be lightweight fluff? Our August Edition [brings
    together an impressive collection](/august-edition-summer-reads-for-data-scientists-52d5ad64835b)
    of engaging, enlightening, and heat-proof posts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The missing ingredient in your marketing strategy might just be machine learning](/leveraging-machine-learning-for-effective-marketing-strategy-development-99b1b887f2f5),
    says [Elena K.](https://medium.com/u/1a63363b910a?source=post_page-----2e2c89fc6c7a--------------------------------),
    whose debut TDS story is full actionable tips and tricks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you’re in the mood for another business-focused topic, you’re in luck: [Matteo
    Courthoud](https://medium.com/u/666130fb420f?source=post_page-----2e2c89fc6c7a--------------------------------)
    is back with a new contribution that focuses on the interaction of churn and revenue.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Turning back to the more practical side of working with LLMs, [Felipe de Pontes
    Adachi](https://medium.com/u/a038269245d5?source=post_page-----2e2c89fc6c7a--------------------------------)
    [outlines seven tactics for monitoring their behavior](/7-ways-to-monitor-large-language-model-behavior-25c267d58f06)
    to ensure consistent performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Anna Via](https://medium.com/u/c1a8933ed8b?source=post_page-----2e2c89fc6c7a--------------------------------)’s
    new post encourages industry data practitioners to take a step back before launching
    a ML-centered project and to [ask if a machine learning model is even necessary](/to-use-or-not-to-use-machine-learning-d28185382c14)
    for the problem at hand.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thank you for supporting our authors! If you enjoy the articles you read on
    TDS, consider [becoming a Medium member](https://bit.ly/tds-membership) — it unlocks
    our entire archive (and every other post on Medium, too).
  prefs: []
  type: TYPE_NORMAL
- en: We hope many of you are also [planning to attend Medium Day](https://blog.medium.com/youre-invited-to-medium-day-526193e251b2#:~:text=On%20August%2012th%2C%202023%2C%20we,share%20their%20lives%20on%20Medium.)
    on August 12 to celebrate the community and the stories that make it special —
    [registration (which is free) is now open](https://hopin.com/events/medium-day-2023/registration).
  prefs: []
  type: TYPE_NORMAL
- en: Until the next Variable,
  prefs: []
  type: TYPE_NORMAL
- en: TDS Editors
  prefs: []
  type: TYPE_NORMAL
