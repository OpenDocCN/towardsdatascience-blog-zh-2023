- en: 'GPT and Beyond: The Technical Foundations of LLMs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT 与超越：大型语言模型的技术基础
- en: 原文：[https://towardsdatascience.com/gpt-and-beyond-the-technical-foundations-of-llms-2e2c89fc6c7a?source=collection_archive---------4-----------------------#2023-08-03](https://towardsdatascience.com/gpt-and-beyond-the-technical-foundations-of-llms-2e2c89fc6c7a?source=collection_archive---------4-----------------------#2023-08-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/gpt-and-beyond-the-technical-foundations-of-llms-2e2c89fc6c7a?source=collection_archive---------4-----------------------#2023-08-03](https://towardsdatascience.com/gpt-and-beyond-the-technical-foundations-of-llms-2e2c89fc6c7a?source=collection_archive---------4-----------------------#2023-08-03)
- en: '[](https://towardsdatascience.medium.com/?source=post_page-----2e2c89fc6c7a--------------------------------)[![TDS
    Editors](../Images/4b2d1beaf4f6dcf024ffa6535de3b794.png)](https://towardsdatascience.medium.com/?source=post_page-----2e2c89fc6c7a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2e2c89fc6c7a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2e2c89fc6c7a--------------------------------)
    [TDS Editors](https://towardsdatascience.medium.com/?source=post_page-----2e2c89fc6c7a--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://towardsdatascience.medium.com/?source=post_page-----2e2c89fc6c7a--------------------------------)[![TDS
    编辑](../Images/4b2d1beaf4f6dcf024ffa6535de3b794.png)](https://towardsdatascience.medium.com/?source=post_page-----2e2c89fc6c7a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2e2c89fc6c7a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2e2c89fc6c7a--------------------------------)
    [TDS 编辑](https://towardsdatascience.medium.com/?source=post_page-----2e2c89fc6c7a--------------------------------)'
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7e12c71dfa81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-and-beyond-the-technical-foundations-of-llms-2e2c89fc6c7a&user=TDS+Editors&userId=7e12c71dfa81&source=post_page-7e12c71dfa81----2e2c89fc6c7a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2e2c89fc6c7a--------------------------------)
    ·Sent as a [Newsletter](/newsletter?source=post_page-----2e2c89fc6c7a--------------------------------)
    ·3 min read·Aug 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2e2c89fc6c7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-and-beyond-the-technical-foundations-of-llms-2e2c89fc6c7a&user=TDS+Editors&userId=7e12c71dfa81&source=-----2e2c89fc6c7a---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7e12c71dfa81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-and-beyond-the-technical-foundations-of-llms-2e2c89fc6c7a&user=TDS+Editors&userId=7e12c71dfa81&source=post_page-7e12c71dfa81----2e2c89fc6c7a---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2e2c89fc6c7a--------------------------------)
    ·发送至 [新闻通讯](/newsletter?source=post_page-----2e2c89fc6c7a--------------------------------)
    ·3分钟阅读·2023年8月3日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2e2c89fc6c7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-and-beyond-the-technical-foundations-of-llms-2e2c89fc6c7a&user=TDS+Editors&userId=7e12c71dfa81&source=-----2e2c89fc6c7a---------------------clap_footer-----------)'
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2e2c89fc6c7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-and-beyond-the-technical-foundations-of-llms-2e2c89fc6c7a&source=-----2e2c89fc6c7a---------------------bookmark_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2e2c89fc6c7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-and-beyond-the-technical-foundations-of-llms-2e2c89fc6c7a&source=-----2e2c89fc6c7a---------------------bookmark_footer-----------)'
- en: In just a few short months, large language models moved from the realm of specialized
    researchers into the everyday workflows of data and ML teams all over the world.
    Here at TDS, we’ve seen how, along with this transition, much of the focus has
    shifted into practical applications and hands-on solutions.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅几个月时间，大型语言模型就从专业研究人员的领域进入了全球数据和机器学习团队的日常工作流。在 TDS，我们见证了随着这一过渡，焦点也转向了实际应用和动手解决方案。
- en: Jumping straight into tinkering mode can make a lot of sense for data professionals
    working in industry—time is precious, after all. Still, it’s always a good idea
    to establish a solid grasp of the inner workings of the technology we use and
    work on, and that’s precisely what our weekly highlights address.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 直接进入调整模式对于在行业中工作的数据专业人员来说是非常合理的——毕竟时间是宝贵的。然而，总是很有必要建立对我们使用和工作的技术的深入理解，这正是我们每周亮点所关注的。
- en: Our recommended reads looks both at the theoretical foundations of LLMs—specifically,
    the GPT family—and at the high-level questions their arrival raises. Even if you’re
    just a casual user of these models, we think you’ll enjoy these thoughtful explorations.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: The transformers architecture is the groundbreaking innovation that made GPT
    models possible in the first place. As [Beatriz Stollnitz](https://medium.com/u/1c8863892480?source=post_page-----2e2c89fc6c7a--------------------------------)
    makes clear, “understanding the details of how they work is an important skill
    for every AI practitioner,” and [**you’ll leave her thorough explainer with a
    crystal-clear idea**](/the-transformer-architecture-of-gpt-models-b8695b48728b)
    of transformers’ power.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Lily Hughes-Robinson](https://medium.com/u/5389e25ca1bb?source=post_page-----2e2c89fc6c7a--------------------------------)
    offers [**a different approach for learning about transformers**](/learning-transformers-code-first-part-2-gpt-up-close-and-personal-1635b52ae0d7)**:**
    one that focuses on the source code so you can build your knowledge intuitively
    from the ground up.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How important is size when it comes to LLMs’ performance? [Gadi Singer](https://medium.com/u/51de1f48d0b?source=post_page-----2e2c89fc6c7a--------------------------------)
    dives into this question in great detail as he [**surveys the latest crop of compact
    generative AI models**](/survival-of-the-fittest-compact-generative-ai-models-are-the-future-for-cost-effective-ai-at-scale-6bbdc138f618)**.**
    These contenders aim to compete with GPT-4 in accuracy, but at a lower cost and
    with a greater potential to achieve scalability.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/2c84298995d84249504a2861bc494d4e.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: Photo by [K8](https://unsplash.com/@_k8_?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'Of all the heated debates surrounding ChatGPT and similar tools, perhaps none
    has been more contentious than the question around LLMs’ supposed intelligence.
    [Lan Chu](https://medium.com/u/3916743f0e10?source=post_page-----2e2c89fc6c7a--------------------------------)
    tackles this topic head on, and [**brings a refreshingly measured and pragmatic
    perspective to the conversation**](/is-chatgpt-actually-intelligent-42d07462fe59).
    (Spoiler alert: no, AI isn’t conscious; yes, it’s complicated.)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“So, how can we move beyond perceiving LLMs like ChatGPT as magical black boxes?
    Physics may provide an answer.” [Tim Lou, PhD](https://medium.com/u/8d41b438feef?source=post_page-----2e2c89fc6c7a--------------------------------)’s
    latest article proposes a thought-provoking idea: that [**the equations that make
    language models tick are analogous to the laws of physics**](/understanding-large-language-models-the-physics-of-chat-gpt-and-bert-ea512bcc6a64)
    and the way they govern particles and forces.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We published *so* many fantastic articles on other topics in recent weeks; here
    are just a few we absolutely had to highlight.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Who says summer reading has to be lightweight fluff? Our August Edition [brings
    together an impressive collection](/august-edition-summer-reads-for-data-scientists-52d5ad64835b)
    of engaging, enlightening, and heat-proof posts.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谁说夏季阅读一定要轻松无聊？我们的八月版 [汇集了一系列令人印象深刻的文章](/august-edition-summer-reads-for-data-scientists-52d5ad64835b)，这些文章引人入胜，启发思考，并且耐高温。
- en: '[The missing ingredient in your marketing strategy might just be machine learning](/leveraging-machine-learning-for-effective-marketing-strategy-development-99b1b887f2f5),
    says [Elena K.](https://medium.com/u/1a63363b910a?source=post_page-----2e2c89fc6c7a--------------------------------),
    whose debut TDS story is full actionable tips and tricks.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[你的营销策略中可能缺少的成分就是机器学习](/leveraging-machine-learning-for-effective-marketing-strategy-development-99b1b887f2f5)，[Elena
    K.](https://medium.com/u/1a63363b910a?source=post_page-----2e2c89fc6c7a--------------------------------)说，她的首篇TDS故事充满了可操作的技巧和窍门。'
- en: 'If you’re in the mood for another business-focused topic, you’re in luck: [Matteo
    Courthoud](https://medium.com/u/666130fb420f?source=post_page-----2e2c89fc6c7a--------------------------------)
    is back with a new contribution that focuses on the interaction of churn and revenue.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你对另一个商业主题感兴趣，你很幸运：[Matteo Courthoud](https://medium.com/u/666130fb420f?source=post_page-----2e2c89fc6c7a--------------------------------)带来了一个新的贡献，专注于流失率和收入的互动。
- en: Turning back to the more practical side of working with LLMs, [Felipe de Pontes
    Adachi](https://medium.com/u/a038269245d5?source=post_page-----2e2c89fc6c7a--------------------------------)
    [outlines seven tactics for monitoring their behavior](/7-ways-to-monitor-large-language-model-behavior-25c267d58f06)
    to ensure consistent performance.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回到与LLM工作更实际的一面，[Felipe de Pontes Adachi](https://medium.com/u/a038269245d5?source=post_page-----2e2c89fc6c7a--------------------------------)
    [概述了监控其行为的七种策略](/7-ways-to-monitor-large-language-model-behavior-25c267d58f06)，以确保性能一致。
- en: '[Anna Via](https://medium.com/u/c1a8933ed8b?source=post_page-----2e2c89fc6c7a--------------------------------)’s
    new post encourages industry data practitioners to take a step back before launching
    a ML-centered project and to [ask if a machine learning model is even necessary](/to-use-or-not-to-use-machine-learning-d28185382c14)
    for the problem at hand.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Anna Via](https://medium.com/u/c1a8933ed8b?source=post_page-----2e2c89fc6c7a--------------------------------)的新帖子鼓励行业数据从业者在启动以ML为中心的项目之前，退后一步，[询问是否真的需要机器学习模型](/to-use-or-not-to-use-machine-learning-d28185382c14)来解决当前的问题。'
- en: Thank you for supporting our authors! If you enjoy the articles you read on
    TDS, consider [becoming a Medium member](https://bit.ly/tds-membership) — it unlocks
    our entire archive (and every other post on Medium, too).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你对我们作者的支持！如果你喜欢在TDS上阅读的文章，考虑 [成为 Medium 会员](https://bit.ly/tds-membership)
    —— 这将解锁我们的整个档案（还有Medium上的所有其他文章）。
- en: We hope many of you are also [planning to attend Medium Day](https://blog.medium.com/youre-invited-to-medium-day-526193e251b2#:~:text=On%20August%2012th%2C%202023%2C%20we,share%20their%20lives%20on%20Medium.)
    on August 12 to celebrate the community and the stories that make it special —
    [registration (which is free) is now open](https://hopin.com/events/medium-day-2023/registration).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望你们中的许多人也在 [计划参加 Medium Day](https://blog.medium.com/youre-invited-to-medium-day-526193e251b2#:~:text=On%20August%2012th%2C%202023%2C%20we,share%20their%20lives%20on%20Medium.)，于8月12日庆祝社区以及让它特别的故事
    —— [注册（免费）现已开放](https://hopin.com/events/medium-day-2023/registration)。
- en: Until the next Variable,
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 直到下一个Variable，
- en: TDS Editors
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: TDS编辑团队
