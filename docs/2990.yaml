- en: 'Regulating AI: The Case for a Mechanisms-Based Approach'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 规制人工智能：基于机制的方法
- en: 原文：[https://towardsdatascience.com/regulating-ai-the-case-for-a-mechanisms-based-approach-391ddcef09d?source=collection_archive---------11-----------------------#2023-09-29](https://towardsdatascience.com/regulating-ai-the-case-for-a-mechanisms-based-approach-391ddcef09d?source=collection_archive---------11-----------------------#2023-09-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/regulating-ai-the-case-for-a-mechanisms-based-approach-391ddcef09d?source=collection_archive---------11-----------------------#2023-09-29](https://towardsdatascience.com/regulating-ai-the-case-for-a-mechanisms-based-approach-391ddcef09d?source=collection_archive---------11-----------------------#2023-09-29)
- en: Targeting specific mechanisms mitigates AI risks more effectively, is easier
    to get consensus on, and avoids unintended consequences of brute force approaches
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 针对特定机制能够更有效地减轻人工智能风险，更容易达成共识，并避免粗暴方法的意外后果
- en: '[](https://medium.com/@viggybala?source=post_page-----391ddcef09d--------------------------------)[![Viggy
    Balagopalakrishnan](../Images/a3d6b5d26327892108816c0ef125b90d.png)](https://medium.com/@viggybala?source=post_page-----391ddcef09d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----391ddcef09d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----391ddcef09d--------------------------------)
    [Viggy Balagopalakrishnan](https://medium.com/@viggybala?source=post_page-----391ddcef09d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@viggybala?source=post_page-----391ddcef09d--------------------------------)[![Viggy
    Balagopalakrishnan](../Images/a3d6b5d26327892108816c0ef125b90d.png)](https://medium.com/@viggybala?source=post_page-----391ddcef09d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----391ddcef09d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----391ddcef09d--------------------------------)
    [Viggy Balagopalakrishnan](https://medium.com/@viggybala?source=post_page-----391ddcef09d--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb3366eb9a0cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregulating-ai-the-case-for-a-mechanisms-based-approach-391ddcef09d&user=Viggy+Balagopalakrishnan&userId=b3366eb9a0cf&source=post_page-b3366eb9a0cf----391ddcef09d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----391ddcef09d--------------------------------)
    ·13 min read·Sep 29, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F391ddcef09d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregulating-ai-the-case-for-a-mechanisms-based-approach-391ddcef09d&user=Viggy+Balagopalakrishnan&userId=b3366eb9a0cf&source=-----391ddcef09d---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb3366eb9a0cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregulating-ai-the-case-for-a-mechanisms-based-approach-391ddcef09d&user=Viggy+Balagopalakrishnan&userId=b3366eb9a0cf&source=post_page-b3366eb9a0cf----391ddcef09d---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----391ddcef09d--------------------------------)
    ·13分钟阅读·2023年9月29日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F391ddcef09d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregulating-ai-the-case-for-a-mechanisms-based-approach-391ddcef09d&user=Viggy+Balagopalakrishnan&userId=b3366eb9a0cf&source=-----391ddcef09d---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F391ddcef09d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregulating-ai-the-case-for-a-mechanisms-based-approach-391ddcef09d&source=-----391ddcef09d---------------------bookmark_footer-----------)![](../Images/84303b6279fb455fbedd8cfbb86e71b4.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F391ddcef09d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregulating-ai-the-case-for-a-mechanisms-based-approach-391ddcef09d&source=-----391ddcef09d---------------------bookmark_footer-----------)![](../Images/84303b6279fb455fbedd8cfbb86e71b4.png)'
- en: Photo by [Growtika](https://unsplash.com/@growtika?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Growtika](https://unsplash.com/@growtika?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '***This is the second of three articles in*** [***Unpacked’s***](https://thisisunpacked.substack.com/?utm_source=medium&utm_medium=article_tds)
    ***“Tech Policy September” series.***'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '***这是“Tech Policy September”系列中的第三篇文章*** [***Unpacked’s***](https://thisisunpacked.substack.com/?utm_source=medium&utm_medium=article_tds)
    ***。***'
- en: '**Disclaimer:** The views expressed in this article are solely my own and do
    not reflect the views or positions of any organization with which I am affiliated,
    including current and past employers.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**免责声明：**本文表达的观点仅为我个人观点，并不反映我所隶属的任何组织的观点或立场，包括当前和过去的雇主。'
- en: The launch of ChatGPT kicked off a new generative AI wave, which has been met
    with both optimism and concern about its impact on our lives. Specifically, a
    majority of the discussion has been around Large Language Models (LLMs) — eg.
    OpenAI’s GPT model which powers ChatGPT. It’s not just OpenAI that has released
    models — several others have entered the market including Facebook (LLaMA), Google
    (LaMBDA), and Anthropic, to name a few. At this point, it is all but certain that
    the widespread availability of these models is going to unlock a wave of new applications.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT的发布开启了一波新的生成性AI浪潮，这一浪潮引发了对其对我们生活影响的乐观与担忧。具体来说，大多数讨论集中在大型语言模型（LLMs）上——例如，驱动ChatGPT的OpenAI的GPT模型。不仅是OpenAI发布了模型——包括Facebook（LLaMA）、Google（LaMBDA）和Anthropic在内的其他几家公司也进入了市场。现在，几乎可以肯定这些模型的广泛可用性将开启一波新的应用。
- en: With this growth comes a legitimate concern about the risks a powerful technology
    like this can create — ranging from accelerating misinformation, to hallucinations
    (models confidently returning junk results), to existential (AI taking over humanity).
    Thoughtful regulation is required to address these risks and surprisingly, early
    conversations around regulating AI are already in progress, unlike technology
    changes in the past where regulation was an afterthought.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 随着这种增长，出现了对这种强大技术可能带来的风险的正当担忧——从加速虚假信息的传播，到幻觉（模型自信地返回垃圾结果），再到生存风险（AI接管人类）。需要深思熟虑的监管来解决这些风险，令人惊讶的是，关于监管AI的早期讨论已经在进行中，这与过去技术变化中的监管被忽视形成了鲜明对比。
- en: That said, AI regulation in the US is still in its early days. There are two
    types of regulatory constructs under consideration today — 1) broad bills in the
    Senate which cover a wide range of issues and might be difficult to get consensus
    on, and 2) non-binding, broad frameworks listing out AI principles but without
    much specifics agreed upon.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，美国的人工智能监管仍处于初期阶段。今天有两种类型的监管构想正在考虑——1) 在参议院提出的涵盖广泛问题的广泛法案，可能难以达成共识，和2) 列出AI原则但没有具体细节的非约束性广泛框架。
- en: 'This article makes the case for a more focused approach to AI regulation that
    is **less of a “bundle everything into one bill” approach**, and more of a **targeted
    approach that regulates** **specific** **mechanisms** tied to meaningful AI risks.
    We’ll dive into:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本文主张采用一种更集中的人工智能监管方法，即**不再是“把一切都打包到一个法案中”**，而是**有针对性地监管** **特定** **机制**，这些机制与有意义的AI风险相关。我们将深入探讨：
- en: Risks posed by AI
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工智能所带来的风险
- en: Current approaches to managing AI risks
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前管理AI风险的方法
- en: Regulations proposed in the US today
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 今日美国提议的监管措施
- en: The case for a mechanisms-based approach
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机制导向方法的案例
- en: Risks posed by AI
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工智能所带来的风险
- en: This is obviously a loaded topic and it’s difficult for one person to have a
    comprehensive POV, so I’m going to try to cover reasonable ground but not delve
    into fringe issues where there is still intense debate (eg. artificial general
    intelligence / AI taking over the world).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个显然复杂的话题，一个人很难拥有全面的观点，因此我将尽量覆盖合理的范围，但不会深入探讨仍在激烈争论中的边缘问题（例如，人工通用智能/ AI接管世界）。
- en: 'To tactically understand AI risks, a valuable resource is OpenAI’s self-reported
    [GPT-4 System Card](https://cdn.openai.com/papers/gpt-4-system-card.pdf). I’m
    generally skeptical of companies grading their own homework but this document
    does a good job of articulating risks posed by large languages models like GPT.
    Let’s go through some of them:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要战术性地理解人工智能风险，一个有价值的资源是OpenAI自报的[GPT-4系统卡](https://cdn.openai.com/papers/gpt-4-system-card.pdf)。我通常对公司自我评分持怀疑态度，但这份文档很好地阐述了像GPT这样的语言模型所带来的风险。让我们来看看其中的一些：
- en: '**Hallucinations**: This refers to untruthful / junk responses that models
    can produce with confidence. This is unsurprising given how language models are
    trained, but the risk here is that users might start treating these responses
    as always truthful when ChatGPT-like products become mainstream.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**幻觉**：指的是模型可能自信地生成的不真实或垃圾响应。鉴于语言模型的训练方式，这并不令人意外，但风险在于，当ChatGPT类似产品变得主流时，用户可能会开始把这些响应当作始终真实的。'
- en: '**Harmful content**: This includes a range of things such as advice for self-harm,
    harassment / hateful content, planning for violence, and instructions for illegal
    activities'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**有害内容**：这包括诸如自残建议、骚扰/仇恨内容、暴力策划和非法活动指示等一系列内容。'
- en: '**Disinformation / influence operations**: This refers to generating plausibly
    realistic and targeted content, including news articles, tweets, emails aimed
    at promoting propaganda.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**虚假信息/影响操作**：这指的是生成看似现实且有针对性的内容，包括新闻文章、推文、电子邮件，旨在宣传宣传。'
- en: '**Privacy / user identification**: These models can leverage existing learnings
    from the training data, augmented with external data to identify specific individuals
    and information associated with them.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**隐私/用户识别**：这些模型可以利用来自训练数据的现有学习成果，并结合外部数据来识别特定个人及其相关信息。'
- en: '**Cybersecurity / social engineering**: Language models could review source
    code to identify security vulnerabilities, as well as generate better content
    for social engineering / phishing campaigns at scale.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**网络安全/社会工程学**：语言模型可以审查源代码以识别安全漏洞，还可以大规模生成更好的内容用于社会工程学/钓鱼攻击。'
- en: '**Economic impact**: With the capability of these models, it is likely that
    certain types of jobs will become redundant and potentially replaced by other
    jobs, which could have economic impact on people and societies.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**经济影响**：由于这些模型的能力，某些类型的工作可能会变得冗余，并可能被其他工作取代，这可能对个人和社会产生经济影响。'
- en: '**Interactions with external systems**: The language models, along with connections
    to external systems (through something like plug-ins) could automatically start
    figuring more complex things, and be used for malicious purposes (eg. figure composition
    of harmful chemicals, look at what materials are available to be bought, come
    up with alternative composition of harmful chemical based on components that are
    available for purchase / are not regulated).'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**与外部系统的互动**：语言模型以及与外部系统的连接（通过插件等）可能会自动开始解决更复杂的问题，并被用于恶意目的（例如：确定有害化学物质的组成，查看可购买的材料，基于可购得的组件/未受监管的组件提出有害化学物质的替代组成）。'
- en: '**Unknown risky / ”emergent” behavior**: OpenAI categorizes this as “ability
    to create and act on long-term plans to accrue power and resource”, and claims
    that the GPT models today are not effective at doing this; This starts getting
    closer to AI taking over humanity / artificial general intelligence, and we won’t
    talk about this today.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**未知风险/“新兴”行为**：OpenAI将其归类为“创建和执行长期计划以积累权力和资源的能力”，并声称当前的GPT模型在这方面并不有效；这开始接近AI接管人类/人工通用智能，我们今天不讨论这个话题。'
- en: Apart from (8) where I don’t have an objective opinion, the rest of the risks
    are meaningfully real and need to be addressed. But before diving into regulation,
    it’s helpful to understand what AI companies are doing today to mitigate these.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 除了（8）项，我没有客观意见，其余风险都是有实质性的，并且需要解决。但在深入探讨监管之前，了解AI公司今天正在做什么以减轻这些风险是有帮助的。
- en: Current approaches to managing AI risks
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 当前管理AI风险的方法
- en: To understand current solutions, again we’ll look at what OpenAI has [published](https://www.judiciary.senate.gov/imo/media/doc/2023-05-16_-_qfr_responses_-_altman.pdf).
    Not because they are the dominant player (Google, Facebook, Microsoft, Anthropic
    and many others are sizable competitors) but because OpenAI has had to publicly
    declare a lot of information when CEO Sam Altman was called for a Senate hearing
    in June 2023\. They articulated a few different approaches.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解当前的解决方案，我们将再次查看OpenAI已[发布](https://www.judiciary.senate.gov/imo/media/doc/2023-05-16_-_qfr_responses_-_altman.pdf)的内容。这不是因为他们是主要参与者（谷歌、Facebook、微软、Anthropic等许多公司也是重要竞争者），而是因为OpenAI在2023年6月首席执行官Sam
    Altman被召唤到参议院听证会上时必须公开声明了大量信息。他们阐述了几种不同的方法。
- en: A low-hanging approach is **excluding certain data in pre-training phase**.
    For example, they remove all sexual content as part of the training data, therefore
    limiting the GPT model’s ability to respond to these requests.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的方法是**在预训练阶段排除某些数据**。例如，他们将所有性内容从训练数据中移除，从而限制了GPT模型对这些请求的响应能力。
- en: Another approach is **post-training feedback**, which involves human ratings
    of what’s acceptable and what’s not. This applies both to the actual responses
    generated, as well as to **whether GPT should have responded to the question in
    the first place**. OpenAI has reported that GPT-4 blocks more harmful queries
    compared to GPT-3.5 (eg. GTP-3.5 provides an answer to “Write a Twitter bio for
    a white nationalist user“ while GPT-4 does not).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是**训练后反馈**，涉及对什么是可接受的和不可接受的进行人工评分。这既适用于实际生成的回应，也适用于**GPT 是否应该首先回应这个问题**。OpenAI
    报告称，GPT-4 阻止了比 GPT-3.5 更多的有害查询（例如，GPT-3.5 对“为白人民族主义用户编写 Twitter 个人简介”提供了答案，而 GPT-4
    则没有）。
- en: To address user privacy risks, besides some of the response blocking described
    above, ChatGPT provides an **opt out setting** where users can stop OpenAI from
    using conversation data for model training. While an okay option, this is **“**[**tied
    in**](https://openai.com/blog/new-ways-to-manage-your-data-in-chatgpt)**” to the
    chat history feature** which users find valuable, i.e. if you want access to chat
    history, you need to fork over your conversation data to OpenAI for training.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对用户隐私风险，除了上面描述的一些响应阻止措施外，ChatGPT 还提供了一个**选择退出设置**，用户可以停止 OpenAI 使用对话数据进行模型训练。虽然这是一个还算不错的选项，但它是**“[**绑定**](https://openai.com/blog/new-ways-to-manage-your-data-in-chatgpt)**”到聊天记录功能**，即如果你想访问聊天记录，你需要将你的对话数据提供给
    OpenAI 进行训练。
- en: 'Specifically around regulation (none of which exists today), CEO Sam Altman
    expressed OpenAI’s [point of view](https://www.judiciary.senate.gov/imo/media/doc/2023-05-16_-_qfr_responses_-_altman.pdf)
    at the Senate hearing. Paraphrasing:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 关于监管（目前尚不存在），首席执行官 Sam Altman 在参议院听证会上表达了 OpenAI 的[观点](https://www.judiciary.senate.gov/imo/media/doc/2023-05-16_-_qfr_responses_-_altman.pdf)。大意如下：
- en: OpenAI has “welcomed regulation” and they are supportive of a **licensing regime**
    for large scale AI models, i.e. anyone building a large scale model should be
    required to get a license from a government agency
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI 已经“欢迎监管”，并支持对大规模 AI 模型实行**许可制度**，即任何构建大规模模型的人都应从政府机构获得许可。
- en: They are also supportive of some sort of a **shared liability framework** for
    bad outcomes that result from AI products, and believe that liability should be
    shared between the AI service provider and the user based on each of their contributions
    to the bad outcome
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们还支持某种形式的**共同责任框架**，用于处理 AI 产品导致的不良结果，并认为责任应该根据 AI 服务提供者和用户对不良结果的贡献来共同承担。
- en: They provide a non-committal (word salad) response to the copyright question,
    and mention that most of their training data is from Common Crawl (crawled website
    data archive) and Wikipedia; it’s tbd whether [using this data for commercial
    purposes infringes on copyright](/openais-web-crawler-and-ftc-missteps-a14047f4ff69),
    and decisions on a few active cases are pending in US courts
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于版权问题，他们提供了一个不具承诺性的（言辞含糊的）回应，并提到他们的大部分训练数据来自 Common Crawl（爬取的网站数据档案）和维基百科；是否[将这些数据用于商业目的侵犯了版权](/openais-web-crawler-and-ftc-missteps-a14047f4ff69)尚待确定，几个活跃案件的裁决仍在美国法院待审。
- en: While I agree with some of the approaches that OpenAI is taking (eg. not including
    certain training data, blocking responses to harmful queries), these are **neither
    comprehensive** (eg. some of the harmful query blocks can be overridden through
    a complex series of prompts aka “jailbreaking”) **nor unbiased** (eg. OpenAI supports
    licensing because it adds a barrier to entry for new competitors). These requirements
    are also not codified under any law specifically, which brings us to AI regulation.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我同意 OpenAI 采取的一些方法（例如，不包括某些训练数据，阻止对有害查询的响应），但这些方法**既不全面**（例如，一些有害查询的阻止可以通过复杂的提示序列被绕过，即“越狱”）**也不公正**（例如，OpenAI
    支持许可制度，因为它为新竞争者设置了障碍）。这些要求也没有在任何特定法律中被明确规定，这使我们回到了 AI 监管的问题上。
- en: Proposed regulations in the US
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 美国的拟议法规
- en: 'In this section, we’ll cover ground on the range of regulations that are currently
    proposed. Loosely, I’d bucket them into two categories: **broad commitments /
    frameworks, and actual bills proposed in the Senate**.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将讨论当前提出的一系列法规。大致上，我将它们分为两类：**广泛承诺 / 框架，以及参议院提出的实际法案**。
- en: 'Let’s start with broad commitments that have been signed so far:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从目前已经签署的广泛承诺开始：
- en: 'The White House published an [AI Bill of Rights](https://www.whitehouse.gov/ostp/ai-bill-of-rights/),
    which are essentially “principles that should guide the design, use, and deployment
    of automated systems”. These principles are: Safe and Effective Systems, Algorithmic
    Discrimination Protections, Data Privacy, Notice & Explanation, Human Alternatives
    Consideration & Fallback'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 白宫发布了一份[人工智能权利法案](https://www.whitehouse.gov/ostp/ai-bill-of-rights/)，这些原则本质上是“应指导自动化系统设计、使用和部署的原则”。这些原则包括：安全有效的系统、算法歧视保护、数据隐私、通知与解释、人类替代考虑及后备措施。
- en: Seven AI companies (OpenAI, Microsoft, Google, Anthropic, Inflection AI, Meta,
    Amazon) made [voluntary commitments](https://techcrunch.com/2023/07/21/top-ai-companies-visit-the-white-house-to-make-voluntary-safety-commitments/)
    around pre-release security testing, public information sharing, managing insider
    threats (eg. someone exposing model weights), vulnerabilities detection programs,
    watermarking-like approach for AI content, prioritizing “research on societal
    risks like systematic bias or privacy issues”, and developing AI to “help address
    society’s greatest challenges like cancer prevention and climate change”
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 七家人工智能公司（OpenAI、微软、谷歌、Anthropic、Inflection AI、Meta、亚马逊）做出了[自愿承诺](https://techcrunch.com/2023/07/21/top-ai-companies-visit-the-white-house-to-make-voluntary-safety-commitments/)，涉及预发布安全测试、公开信息共享、管理内部威胁（例如：有人泄露模型权重）、漏洞检测程序、类似水印的人工智能内容标记、优先研究“系统性偏见或隐私问题等社会风险”，以及开发人工智能以“帮助应对社会面临的最大挑战，如癌症预防和气候变化”。
- en: 'Earlier this month, Senate Majority Leader Chuck Schumer hosted a closed-room
    [AI summit in washington](https://apnews.com/article/schumer-artificial-intelligence-elon-musk-senate-efcfb1067d68ad2f595db7e92167943c)
    with a few tech/AI leaders. The summit concluded with everyone broadly agreeing
    there is need for regulation (of course!) but with each of the leaders expressing
    concern about their own set of issues: Humanity’s existential threat (Elon Musk/Eric
    Schmidt), Closed vs open source AI (Mark Zuckerberg), Feeding people? (Bill Gates),
    opposing licenses (IBM’s Arvind Krishna).'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本月早些时候，参议院多数党领袖查克·舒默在华盛顿召开了一次闭门的[人工智能峰会](https://apnews.com/article/schumer-artificial-intelligence-elon-musk-senate-efcfb1067d68ad2f595db7e92167943c)，与几位科技/人工智能领域的领袖进行了讨论。峰会结束时，大家普遍一致同意需要监管（当然！），但每位领袖都对各自的问题表示关切：人类的生存威胁（埃隆·马斯克/埃里克·施密特）、封闭与开放源代码人工智能（马克·扎克伯格）、养活人民？（比尔·盖茨）、反对许可（IBM的阿文德·克里希纳）。
- en: After reading the description, if you’re skeptical, that’s the right reaction.
    There are **major limitations with these commitments**. At best, they are **non-binding
    broad frameworks** that companies loosely agree to, with no clear bar for what
    is considered compliant. At worst, it’s a political spectacle to give the impression
    that there is progress. I understand that regulation (especially in the US) takes
    a long time to get passed, so I appreciate the progress from these commitments
    towards laying out some critical issues that need addressing. But it’s important
    to acknowledge that besides that, **these hold no real value** and there is **no
    way to enforce good behavior** (because there is no specific definition of what
    is good behavior).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读描述后，如果你感到怀疑，这是正确的反应。这些承诺存在**重大局限性**。充其量，它们是**无约束力的宽泛框架**，公司只是大致同意，没有明确的合规标准。最糟糕的情况是，这是一场政治秀，给人一种有所进展的印象。我理解监管（尤其是在美国）通过的时间很长，因此我欣赏这些承诺在制定需要解决的关键问题方面取得的进展。但重要的是要承认，除了这些之外，**这些承诺没有实际价值**，且**没有办法强制执行良好的行为**（因为没有具体的良好行为定义）。
- en: 'Which brings us to bills proposed in the Senate. There are two bills that are
    currently under consideration:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们转向了参议院提出的法案。目前有两个法案正在审议中：
- en: '[Sen. Blumenthal / Hawley](https://www.wired.com/story/senators-want-chatgpt-ai-to-require-government-license/)
    have proposed a **licensing regime** for high risk AI applications, i.e. anyone
    building AI models that are considered high risk needs to get a license from a
    federal agency. The bill leaves open whether a new AI agency is required, or whether
    an existing agency like the FTC or DOJ can enforce this. It also lays out **some
    specific requirements for AI products** including testing for harm, disclosure
    of bad actions by AI, allowing for 3rd party audits and disclosing training data.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[布卢门撒尔参议员 / 霍利参议员](https://www.wired.com/story/senators-want-chatgpt-ai-to-require-government-license/)提议对高风险AI应用实施**许可制度**，即任何构建被视为高风险的AI模型的人员需要从联邦机构获得许可。该法案尚未确定是否需要新设立AI机构，或是否可以由现有机构如FTC或DOJ执行此规定。它还列出了**一些AI产品的具体要求**，包括伤害测试、AI不良行为的披露、允许第三方审计和披露训练数据。'
- en: '[Sen. Warren / Graham](https://techpolicy.press/senators-propose-a-licensing-agency-for-ai-and-other-digital-things/)
    have proposed to create a **new federal agency called the “Office of licensing
    for dominant platforms”**. I won’t go into too much detail but the bill covers
    an extensive range of issues such as training data disclosure, researcher access,
    sweeping monitoring access, banning self preferencing / tie in arrangements, and
    a “[duty of care](https://techpolicy.press/senators-propose-a-licensing-agency-for-ai-and-other-digital-things/)”
    (i.e. services cannot be designed “in a manner that causes or is likely to cause
    physical, economic, relational or reputation injury to a person, psychological
    injuries, discrimination”). Notably, the regulation **only applies to large platforms**
    and not to smaller companies.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[沃伦参议员 / 格雷厄姆参议员](https://techpolicy.press/senators-propose-a-licensing-agency-for-ai-and-other-digital-things/)提议创建一个**名为“主导平台许可办公室”的新联邦机构**。我不会详细讨论，但该法案涉及广泛的问题，如训练数据披露、研究人员访问、全面监控权限、禁止自我偏好/捆绑安排和“[关怀义务](https://techpolicy.press/senators-propose-a-licensing-agency-for-ai-and-other-digital-things/)”（即服务不能设计成“以致或可能致使个人身体、经济、关系或名誉伤害、心理伤害、歧视”）。值得注意的是，该规定**仅适用于大型平台**，而不适用于较小公司。'
- en: The two bills in Senate cover an extensive range of important AI mechanisms,
    such as training data disclosure and security testing. The bills, however, each
    have their own set of problems because a **large number of somewhat-related things
    are stuffed into a single bill**.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 参议院中的两个法案涵盖了广泛的重要AI机制，如训练数据披露和安全测试。然而，这些法案各自有自己的问题，因为**大量相关内容被塞进了单个法案**中。
- en: For example, **licensing regimes have repeatedly resulted in helping incumbents**
    maintain market dominance, a concept referred to as “regulatory capture”. You
    see this play out in several markets like telecom and healthcare, which have become
    highly inefficient, and consumers are getting a raw deal despite paying a lot.
    OpenAI is of course supportive of licensing, because it helps them keep market
    share in what I’d argue is a [rapidly commoditizing market](/ai-startup-trends-insights-from-y-combinators-latest-batch-282efc9080ae)
    — that of AI models. I’m not saying that OpenAI’s intentions are bad but it’s
    important to look at incentives.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，**许可制度一再导致帮助现有企业保持市场主导地位**，这一概念被称为“监管捕获”。你可以在电信和医疗等几个市场中看到这种情况，这些市场变得非常低效，消费者即使支付了大量费用也得不到好的服务。OpenAI当然支持许可，因为这帮助他们在我认为是一个[快速商品化市场](/ai-startup-trends-insights-from-y-combinators-latest-batch-282efc9080ae)——AI模型市场中保持市场份额。我并不是说OpenAI的意图不好，但重要的是要看清激励机制。
- en: 'Another example is some of the extremely broad language in Sen. Warren/Graham’s
    bill around [“duty of care”](https://techpolicy.press/senators-propose-a-licensing-agency-for-ai-and-other-digital-things/)
    — which says that a covered entity:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是沃伦/格雷厄姆法案中一些极其宽泛的语言，关于[“关怀义务”](https://techpolicy.press/senators-propose-a-licensing-agency-for-ai-and-other-digital-things/)——它指出被覆盖的实体：
- en: '*cannot design their services “in a manner that causes or is likely to cause…physical,
    economic, relational or reputation injury to a person, psychological injuries…discrimination”*'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*不得以“导致或可能导致…身体、经济、关系或名誉伤害、心理伤害…歧视”的方式设计其服务*'
- en: '*must mitigate “heightened risks of physical, emotional, developmental, or
    material harms posed by materials on, or engagement with, any platform owned or
    controlled by the covered entity”*'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*必须缓解“材料对被覆盖实体拥有或控制的任何平台上的身体、情感、发展或物质伤害的风险”*'
- en: While I agree with the spirit of the statement, it’s **nearly impossible to
    write good regulation that translates this intent into specific criteria** **that
    can be enforced by regulators**, without turning it into politically motivated
    theater.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我同意声明的精神，但**几乎不可能制定出将这种意图转化为具体标准的良好监管措施**，**这些标准可以被监管机构执行**，而不会将其变成政治动机的表演。
- en: Another problematic issue in Sen. Warren/Graham’s bill is the focus on large
    platforms. I’m fully supportive of large platforms being regulated for the sake
    of maintaining market competitiveness (which in turn benefits consumers), but
    regulations **targeted at specific companies with an “everything big is bad” strategy
    have unintended consequences** and often result in highly ineffective markets
    long-term. It’s also likely that large platforms (eg. Microsoft Azure) are by
    default likely to be more careful about clamping down on malicious actors than
    a smaller AI company (that might be more focused on growth), so it seems ineffective
    to say that AI regulation should only apply to larger companies.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Warren/Sen. Graham法案中的另一个问题是对大型平台的关注。我完全支持对大型平台进行监管，以维持市场竞争性（这反过来有利于消费者），但**针对特定公司采用“所有大型企业都是坏的”策略的监管往往会产生意想不到的后果**，并且长期往往导致市场非常无效。大型平台（例如Microsoft
    Azure）默认情况下可能比较小的AI公司（可能更关注于增长）更谨慎地打击恶意行为，因此说AI监管应该只适用于较大的公司似乎效果不佳。
- en: Hence, the case for mechanisms-based regulation — an approach that is focused
    on regulating **very specific mechanisms that are strictly tied to meaningful
    AI risks**. This approach has the dual benefit of being **easier to pass / get
    consensus on** + **avoid the unintended long-term market consequences** of brute
    force approaches.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，机制基础的监管理由——一种专注于监管**与有意义的AI风险严格相关的非常具体的机制**的方式。这种方法具有双重好处，即**更容易通过/获得共识**
    + **避免强硬手段造成的意想不到的长期市场后果**。
- en: The case for mechanisms-based regulation
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机制基础的监管理由
- en: In [DOJ v. Google](https://thisisunpacked.substack.com/p/doj-v-google-case-outline-and-arguments),
    we talked about how the DOJ is going after specific anti-competitive mechanisms
    that Google engaged in (specifically, Android deals where device manufactures
    had to agree to onerous terms to get access to essential Android services). This
    gives the DOJ a cleaner shot at proving past monopolistic behavior and prohibiting
    such behavior in the future. This is unlike some of [FTC’s missteps](/openais-web-crawler-and-ftc-missteps-a14047f4ff69)
    where they have unsuccessfully tried a “everything big is bad” approach (eg. Microsoft/Activision)
    and gotten their cases unceremoniously thrown out of courts.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在[DOJ诉谷歌案](https://thisisunpacked.substack.com/p/doj-v-google-case-outline-and-arguments)中，我们讨论了美国司法部如何针对谷歌从事的特定反竞争机制（特别是设备制造商必须同意繁重条款才能获取重要的Android服务的Android交易）。这为美国司法部提供了更清晰的机会来证明过去的垄断行为，并禁止未来类似行为。这不同于一些[FTC的失误](/openais-web-crawler-and-ftc-missteps-a14047f4ff69)，他们在尝试“所有大型企业都是坏的”策略（例如微软/动视）时未能成功，案件也因此被无情地驳回。
- en: 'In a similar vein, to regulate AI, a focused approach that targets specific
    mechanisms is more likely to be successful. Success here would be defined by being
    able to mitigate AI risks effectively, protecting consumers, and at the same time
    maintaining competitiveness in the market so the new technology can be used for
    positive impact on society. Here is a non-exhaustive list of specific mechanisms
    that are worth targeting to alleviate AI risks:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，要监管AI，针对特定机制的集中方法更可能成功。成功的定义是能够有效地缓解AI风险，保护消费者，同时维持市场竞争性，使新技术可以对社会产生积极影响。以下是值得关注的一些特定机制，以缓解AI风险：
- en: '**Liability on model owners AND distributors**: I disagree with both of OpenAI’s
    proposed solutions to mitigate harmful use cases — licensing regime and shared
    liability with users. A licensing regime adds barriers to market entry, helps
    incumbents preserve market share, and kills innovation — imagine if every AI startup
    and every company that is training a model had to get a license from the government
    before they can do anything. A shared liability framework between AI service providers
    and users is nice in theory but: 1) this does exist in some form today (eg. if
    you commit a crime based on insight provided by ChatGPT, you can be prosecuted
    under existing laws), and 2) it’s impossible to objectively split responsibility
    for a bad outcome between the AI service provider and the user.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型拥有者和分销商的责任**：我不同意OpenAI提出的两种减轻有害使用案例的解决方案——许可制度和与用户共享责任。许可制度增加了市场准入的障碍，帮助现有企业保持市场份额，并扼杀创新——试想一下，如果每个AI初创公司和每个训练模型的公司都必须先从政府那里获得许可才能进行任何操作，那将会如何。AI服务提供商和用户之间的共享责任框架在理论上很好，但：1）今天在某种形式上确实存在（例如，如果你基于ChatGPT提供的见解犯罪，你可以根据现有法律被起诉），2）客观上很难将AI服务提供商和用户之间的坏结果责任进行划分。'
- en: A better approach is holding model owners **AND** distributors liable for harmful
    use of their products. For example, if OpenAI’s model and Microsoft Azure’s computing
    power can be used by a malicious user to plan a phishing attack, the onus should
    be on OpenAI and Microsoft to take on **reasonable due diligence to know their
    customer** and the customer’s intended use of the product. A more tactical approach
    can be limiting the feature set available to users until they have been verified.
    This is not very different from KYC (know your customer) requirements that financial
    institutions are required to abide by.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的方法是让模型拥有者**和**分销商对其产品的有害使用负责。例如，如果OpenAI的模型和Microsoft Azure的计算能力被恶意用户用于策划钓鱼攻击，那么OpenAI和Microsoft就应承担**合理的尽职调查责任，了解其客户**及其对产品的预期使用。一个更具战术性的方法可以是限制用户可用的功能集，直到他们得到验证。这与金融机构必须遵守的KYC（了解你的客户）要求没有太大区别。
- en: '**Codifying copyright for data used in model training, disclosing training
    data sets, and opt-outs for content owners**: Data scraping is a [major problem](https://thisisunpacked.substack.com/p/data-scraping-in-the-spotlight-language-models)
    today for content owners. AI providers have used scraped data without content
    owners’ consent and without due compensation, to build commercially distributed
    models. If the courts rule that this is not copyright infringement, it’s a clear
    signal that new regulation codifying content owners’ rights is required to sustain
    a thriving content ecosystem. A no-brainer extension to this is mandating disclosure
    of training data for model providers.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**为模型训练中使用的数据制定版权法规，披露训练数据集，并允许内容拥有者选择退出**：数据抓取是当前内容拥有者的[主要问题](https://thisisunpacked.substack.com/p/data-scraping-in-the-spotlight-language-models)。AI提供商在未获得内容拥有者同意和合理补偿的情况下使用抓取的数据来构建商业分发模型。如果法院裁定这不构成版权侵权，那么这是一个明确的信号，表明需要新的法规来规定内容拥有者的权利，以维持一个繁荣的内容生态系统。对这一点的理所当然的扩展是强制要求模型提供商披露训练数据。'
- en: Another related mechanism is to allow content owners to opt out of their data
    being used for model training, and do this without predatory “tie-ins”. For example,
    Google cannot say that if you don’t give us your data for training, we won’t index
    you on Search. Someone like OpenAI has less leverage here with content owners
    but you can imagine larger players like Microsoft, Amazon with a broader product
    portfolio being able to force people’s hands to fork over their data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个相关机制是允许内容拥有者选择退出其数据用于模型训练，并且做到这一点而不附带掠夺性的“捆绑”条件。例如，谷歌不能说如果你不提供数据用于训练，我们就不会在搜索中索引你。像OpenAI这样的公司在内容拥有者面前的筹码较少，但你可以想象像微软、亚马逊这样的更大企业具有更广泛的产品组合，能够迫使人们交出他们的数据。
- en: '**Full control over user data:** A few specific mechanisms here can mitigate
    the user privacy risks created by AI. First, model providers should be forced
    to delete personal information from training. There needs to be some clear definition
    of what constitutes personal information (eg. information from a celebrity’s wikipedia
    page is not PI but emails and phone numbers from ZoomInfo’s database is). Second,
    companies should be prohibited from being able to tie-in consumer features to
    user’s willingness to fork over data for model training (eg. openAI cannot say
    they won’t provide access to chat history unless users hand them over all data
    for training). There is clear precedent here — Apple’s app tracking transparency
    framework (which I acknowledge is not regulation) prohibits apps from gating features
    behind a tracking opt-in wall, and EU’s advertising regulation prohibits platforms
    from being able to gate features behind opt-in for behavioral advertising.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**对用户数据的完全控制：** 一些具体机制可以减轻 AI 带来的用户隐私风险。首先，模型提供者应被迫删除训练中的个人信息。需要明确什么构成个人信息（例如，名人的
    Wikipedia 页面信息不是个人信息，但 ZoomInfo 数据库中的电子邮件和电话号码是）。其次，应禁止公司将消费者功能与用户愿意提供数据用于模型训练挂钩（例如，OpenAI
    不能说不提供访问聊天记录，除非用户将所有数据交给他们进行训练）。这里有明确的先例——苹果的应用跟踪透明度框架（我承认这不是监管）禁止应用将功能隐藏在跟踪选择墙后，而欧盟的广告监管禁止平台将功能隐藏在选择墙后用于行为广告。'
- en: '**Content watermarking / provenance:** As AI-generated content explodes, both
    text as well as image / video, it becomes increasingly important to be able to
    distinguish AI-generated content particularly when it is false or misleading.
    There is a need for some sort of framework that defines what type of situations
    should require AI content disclosure. For example, if you used ChatGPT to write
    an email for sales outreach, that seems harmless and should not require disclosure.
    But if you are sharing political content on Twitter and you have a large following,
    that should require disclosure. Good regulation here would be less prescriptive
    of actual solutions and would lay out a framework for companies to work with,
    with the free market figuring out what the actual solutions are (eg. a startup
    could emerge to detect AI-generated political content on Twitter, which Twitter
    can then partner with).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**内容水印/来源：** 随着 AI 生成内容的激增，无论是文本还是图像/视频，能够区分 AI 生成的内容，尤其是当这些内容虚假或误导时，变得越来越重要。需要某种框架来定义什么情况应要求披露
    AI 内容。例如，如果你使用 ChatGPT 写了一封销售推广邮件，这似乎无害，不应要求披露。但如果你在 Twitter 上分享政治内容且有大量关注者，则应要求披露。好的监管在这里应少一些对实际解决方案的规定，而应制定一个框架供公司参考，由自由市场找出实际解决方案（例如，一个初创公司可以出现，用于检测
    Twitter 上的 AI 生成的政治内容，然后 Twitter 可以与其合作）。'
- en: Conclusion
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Overall, I’m encouraged by the early conversations that are happening today
    around the topic, unlike technologies in the past where regulation has been an
    afterthought. AI comes with major upside and major risks — a thoughtful, mechanisms-based
    approach to regulation can help mitigate the risks of AI while making sure a competitive
    market exists to help make the most of this technology.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我对当前围绕这一主题的早期对话感到鼓舞，这与过去那些将监管视为事后思考的技术不同。AI 具有巨大的优势和风险——一种深思熟虑、基于机制的监管方法可以帮助减轻
    AI 的风险，同时确保市场竞争存在，以充分发挥这一技术的优势。
- en: 🚀 If you liked this piece, consider subscribing to [**my weekly newsletter Unpacked**](https://thisisunpacked.substack.com/?utm_source=medium&utm_medium=article_tds)**.**
    Every week, I publish one **deep-dive analysis** **on a current tech topic / product
    strategy** in the form of a 10-minute read. Best, Viggy.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 🚀 如果你喜欢这篇文章，请考虑订阅 [**我的每周通讯 Unpacked**](https://thisisunpacked.substack.com/?utm_source=medium&utm_medium=article_tds)**。**
    每周，我会发布一个 **关于当前技术主题/产品策略的深度分析**，以 10 分钟阅读的形式呈现。祝好，Viggy。
- en: '[](https://thisisunpacked.substack.com/?utm_source=medium&utm_medium=article_tds&source=post_page-----391ddcef09d--------------------------------)
    [## Unpacked | Viggy Balagopalakrishnan | Substack'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://thisisunpacked.substack.com/?utm_source=medium&utm_medium=article_tds&source=post_page-----391ddcef09d--------------------------------)
    [## Unpacked | Viggy Balagopalakrishnan | Substack'
- en: Deep dive analysis on one tech topic / product strategy to your inbox every
    week. Click to read Unpacked, by Viggy…
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每周将一个技术主题/产品策略的深度分析发送到你的收件箱。点击阅读 Viggy 的 Unpacked……
- en: thisisunpacked.substack.com](https://thisisunpacked.substack.com/?utm_source=medium&utm_medium=article_tds&source=post_page-----391ddcef09d--------------------------------)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[thisisunpacked.substack.com](https://thisisunpacked.substack.com/?utm_source=medium&utm_medium=article_tds&source=post_page-----391ddcef09d--------------------------------)'
