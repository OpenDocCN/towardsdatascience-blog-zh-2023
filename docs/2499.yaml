- en: Latest in CNN Kernels for Large Image Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/latest-in-cnn-kernels-for-large-image-models-b48b27e75638?source=collection_archive---------3-----------------------#2023-08-04](https://towardsdatascience.com/latest-in-cnn-kernels-for-large-image-models-b48b27e75638?source=collection_archive---------3-----------------------#2023-08-04)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A high-level overview of the latest convolutional kernel structures in Deformable
    Convolutional Networks, DCNv2, DCNv3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@1996hwm?source=post_page-----b48b27e75638--------------------------------)[![Wanming
    Huang](../Images/05051c8e4fe9c6ffc1d28a9578bf6537.png)](https://medium.com/@1996hwm?source=post_page-----b48b27e75638--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b48b27e75638--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b48b27e75638--------------------------------)
    [Wanming Huang](https://medium.com/@1996hwm?source=post_page-----b48b27e75638--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1cad9f97731c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-in-cnn-kernels-for-large-image-models-b48b27e75638&user=Wanming+Huang&userId=1cad9f97731c&source=post_page-1cad9f97731c----b48b27e75638---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b48b27e75638--------------------------------)
    ·7 min read·Aug 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb48b27e75638&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-in-cnn-kernels-for-large-image-models-b48b27e75638&user=Wanming+Huang&userId=1cad9f97731c&source=-----b48b27e75638---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb48b27e75638&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-in-cnn-kernels-for-large-image-models-b48b27e75638&source=-----b48b27e75638---------------------bookmark_footer-----------)![](../Images/53825b3549061898e455c5ff37ab15ed.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Cape Byron Lighthouse, Australia | photo by author
  prefs: []
  type: TYPE_NORMAL
- en: As the remarkable success of OpenAI’s ChatGPT has sparked the boom of large
    language models, many people foresee the next breakthrough in large image models.
    In this domain, vision models can be prompted to analyze and even generate images
    and videos in a similar manner to how we currently prompt ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: 'The latest deep learning approaches for large image models have branched into
    two main directions: those based on convolutional neural networks (CNNs) and those
    based on transformers. This article will focus on the CNN side and provide a high-level
    overview of those improved CNN kernel structures.'
  prefs: []
  type: TYPE_NORMAL
- en: Table of Content
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[DCN](#00f3)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[DCNv2](#1709)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[DCNv3](#22a3)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1\. Deformable Convolutional Networks (DCN)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditionally, CNN kernels have been applied to fixed locations in each layer,
    resulting in all activation units having the same receptive field.
  prefs: []
  type: TYPE_NORMAL
- en: As in the figure below, to perform convolution on an input feature map ***x***,
    the value at each output location ***p****0* is calculated as an element-wise
    multiplication and summation between kernel weight ***w*** and a sliding window
    on ***x.*** The sliding window is defined by a grid ***R***, which is also the
    receptive field for ***p****0****.*** The size of ***R*** remains the same across
    all locations within the same layer of *y*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cbb617318368ccf22a3c93a1182e37b6.png)'
  prefs: []
  type: TYPE_IMG
- en: Regular convolution operation with 3x3 kernel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each output value is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4060a93bc9894d787e43933e6adbef99.png)'
  prefs: []
  type: TYPE_IMG
- en: Regular convolution operation function from [paper](https://arxiv.org/pdf/1703.06211v3.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: where ***p****n* enumerates locations in the sliding window (grid ***R***).
  prefs: []
  type: TYPE_NORMAL
- en: 'The RoI (region of interest) pooling operation, too, operates on bins with
    a fixed size in each layer. For (*i, j*)-th bin containing *nij* pixels, its pooling
    outcome is computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7eb8c29a407505e0889f7429f7b574bb.png)'
  prefs: []
  type: TYPE_IMG
- en: Regular average RoI pooling function from [paper](https://arxiv.org/pdf/1703.06211v3.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Again shape and size of bins are the same in each layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6bc09b7c45eef4cf0b3a7ead1634614e.png)'
  prefs: []
  type: TYPE_IMG
- en: Regular average RoI pooling operation with 3x3 bin.
  prefs: []
  type: TYPE_NORMAL
- en: Both operations thus become particularly problematic for high-level layers that
    encode semantics, e.g., objects with varying scales.
  prefs: []
  type: TYPE_NORMAL
- en: '[DCN](https://arxiv.org/pdf/1703.06211v3.pdf) proposes deformable convolution
    and deformable pooling that are more flexible to model those geometric structures.
    Both operate on the 2D spatial domain, i.e., the operation remains the same across
    the channel dimension.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deformable convolution**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/ac617c6bd395fa1da6bc84592033e74a.png)'
  prefs: []
  type: TYPE_IMG
- en: Deformable convolution operation with 3x3 kernel.
  prefs: []
  type: TYPE_NORMAL
- en: Given input feature map **x**, for each location ***p****0* in the output feature
    map **y**, DCN adds 2D offsets △***p****n* when enumerating each location ***p****n*
    in a regular grid ***R***.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/252f21b12aa8ce2e3daa7fbb8152e2ae.png)'
  prefs: []
  type: TYPE_IMG
- en: Deformable convolution function from [paper](https://arxiv.org/pdf/1703.06211v3.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: These offsets are learned from preceding feature maps, obtained via an additional
    conv layer over the feature map. As these offsets are typically fractional, they
    are implemented via bilinear interpolation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Deformable RoI pooling**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to the convolution operation, pooling offsets △***p****ij* are added
    to the original binning positions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/959d5a7c4599ec6b37fe78d6f2b673a9.png)'
  prefs: []
  type: TYPE_IMG
- en: Deformable RoI pooling function from [paper](https://arxiv.org/pdf/1703.06211v3.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: As in the figure below, these offsets are learned through a fully connected
    (FC) layer after the original pooling result.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0069fe84c6c266e72c62cba93337ff30.png)'
  prefs: []
  type: TYPE_IMG
- en: Deformable average RoI pooling operation with 3x3 bin.
  prefs: []
  type: TYPE_NORMAL
- en: '**Deformable Position-Sentitive (PS) RoI pooling**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When applying deformable operations to PS RoI pooling ([Dai et al., n.d.](https://arxiv.org/pdf/1605.06409v2.pdf)),
    as illustrated in the figure below, offsets are applied to each score map instead
    of the input feature map. These offsets are learned through a conv layer instead
    of an FC layer.
  prefs: []
  type: TYPE_NORMAL
- en: '*Position-Sensitive RoI pooling* ([Dai et al., n.d.](https://arxiv.org/pdf/1605.06409v2.pdf))*:
    Traditional RoI pooling loses information regarding which object part each region
    represents. PS RoI pooling is proposed to retain this information by converting
    input feature maps to k² score maps for each object class, where each score map
    represents a specific spatial part. So for C object classes, there are total k²
    (C+1) score maps.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2ce91cae1a55879f64a3a8c6b7f74cce.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration of 3x3 deformable PS RoI pooling | source from [paper](https://arxiv.org/pdf/1703.06211v3.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 2\. DCNv2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although DCN allows for more flexible modelling of the receptive field, it
    assumes pixels within each receptive field contribute equally to the response,
    which is often not the case. To better understand the contribution behaviour,
    authors use three methods to visualize the spatial support:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Effective receptive fields: gradient of the node response with respect to intensity
    perturbations of each image pixel'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Effective sampling/bin locations: gradient of the network node with respect
    to the sampling/bin locations'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Error-bounded saliency regions: progressively masking the parts of the image
    to find the smallest image region that produces the same response as the entire
    image'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To assign learnable feature amplitude to locations within the receptive field,
    [DCNv2](https://arxiv.org/pdf/1811.11168.pdf) introduces modulated deformable
    modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/27af2f6ab5d795a12b0e24917a48cab3.png)'
  prefs: []
  type: TYPE_IMG
- en: DCNv2 convolution function from [paper](https://arxiv.org/pdf/1811.11168.pdf),
    notations revised to match ones in DCN paper.
  prefs: []
  type: TYPE_NORMAL
- en: For location ***p****0*, the offset △***pn*** and its amplitude △***m****n*
    are learnable through separate conv layers applied to the same input feature map.
  prefs: []
  type: TYPE_NORMAL
- en: DCNv2 revised deformable RoI pooling similarly by adding a learnable amplitude
    △***m****ij* for each (i,j)-th bin.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3717e68140640eabaf86a43988b688a3.png)'
  prefs: []
  type: TYPE_IMG
- en: DCNv2 pooling function from [paper](https://arxiv.org/pdf/1811.11168.pdf), notations
    revised to match ones in DCN paper.
  prefs: []
  type: TYPE_NORMAL
- en: DCNv2 also expands the use of deformable conv layers to replace regular conv
    layers in conv3 to conv5 stages in ResNet-50.
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. DCNv3**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To reduce the parameter size and memory complexity from DCNv2, [DCNv3](https://arxiv.org/pdf/2211.05778.pdf)
    makes the following adjustments to the kernel structure.
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by [depthwise separable convolution (Chollet, 2017)](https://arxiv.org/pdf/1610.02357.pdf)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Depthwise separable convolution decouples traditional convolution into: 1\.
    depth-wise convolution: each channel of the input feature is convolved separately
    with a filter; 2\. point-wise convolution: a 1x1 convolution applied across channels.*'
  prefs: []
  type: TYPE_NORMAL
- en: The authors propose to let the feature amplitude *m* be the depth-wise part,
    and the projection weight *w* shared among locations in the grid as the point-wise
    part.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Inspired by [group convolution](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
    (Krizhevsky, Sutskever and Hinton, 2012)
  prefs: []
  type: TYPE_NORMAL
- en: '*Group convolution: Split input channels and output channels into groups and
    apply separate convolution to each group.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[DCNv3](https://arxiv.org/pdf/2211.05778.pdf) (Wang et al., 2023) propose splitting
    the convolution into G groups, each having separate offset △***p****gn* and feature
    amplitude △***m****gn*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'DCNv3 is hence formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cb0312cbf8c926d9e223e8254ec7fda7.png)'
  prefs: []
  type: TYPE_IMG
- en: DCNv3 convolution function from [paper,](https://arxiv.org/pdf/2211.05778.pdf)
    notations revised to match ones in DCN paper.
  prefs: []
  type: TYPE_NORMAL
- en: where *G* is the total number of convolution groups, ***w****g* is location
    irrelevant, △***m****gn* is normalized by the softmax function so that the sum
    over grid ***R*** is 1.
  prefs: []
  type: TYPE_NORMAL
- en: Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far DCNv3 based InternImage has demonstrated superior performance in multiple
    downstream tasks such as detection and segmentation, as shown in the table below,
    as well as the leaderboard on papers with code. Refer to the original paper for
    more detailed comparisons.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e0b2852403fca3c51778ab822abfde1a.png)'
  prefs: []
  type: TYPE_IMG
- en: Object detection and instance segmentation performance on COCO val2017\. The
    FLOPs are measured with 1280×800 inputs. AP’ and AP’ represent box AP and mask
    AP, respectively. “MS” means multi-scale training. Source from [paper](https://arxiv.org/pdf/2211.05778.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7bdd9ad06e7904179df50b42de38d53f.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot of the leaderboard for object detection from [paperswithcode.com](https://paperswithcode.com/task/object-detection).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a582193e0ed5faab979a487ba6e3a102.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot of the leaderboard for semantic segmentation from [paperswithcode.com](https://paperswithcode.com/task/semantic-segmentation).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this article, we have reviewed kernel structures for regular convolutional
    networks, along with their latest improvements, including deformable convolutional
    networks (DCN) and two newer versions: DCNv2 and DCNv3\. We discussed the limitations
    of traditional structures and highlighted the advancements in innovation built
    upon previous versions. For a deeper understanding of these models, please refer
    to the papers in the References section.'
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Special thanks to [Kenneth Leung](https://medium.com/u/dcd08e36f2d0?source=post_page-----b48b27e75638--------------------------------),
    who inspired me to create this piece and shared amazing ideas. A huge thank you
    to Kenneth, [Melissa Han](https://medium.com/u/2c1cee38ff69?source=post_page-----b48b27e75638--------------------------------),
    and Annie Liao, who contributed to improving this piece. Your insightful suggestions
    and constructive feedback have significantly impacted the quality and depth of
    the content.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Dai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H. and Wei, Y. (n.d.). *Deformable
    Convolutional Networks*. [online] Available at: [https://arxiv.org/pdf/1703.06211v3.pdf.](https://arxiv.org/pdf/1703.06211v3.pdf.)'
  prefs: []
  type: TYPE_NORMAL
- en: '‌Zhu, X., Hu, H., Lin, S. and Dai, J. (n.d.). *Deformable ConvNets v2: More
    Deformable, Better Results*. [online] Available at: [https://arxiv.org/pdf/1811.11168.pdf.](https://arxiv.org/pdf/1811.11168.pdf.)'
  prefs: []
  type: TYPE_NORMAL
- en: '‌Wang, W., Dai, J., Chen, Z., Huang, Z., Li, Z., Zhu, X., Hu, X., Lu, T., Lu,
    L., Li, H., Wang, X. and Qiao, Y. (n.d.). *InternImage: Exploring Large-Scale
    Vision Foundation Models with Deformable Convolutions*. [online] Available at:
    [https://arxiv.org/pdf/2211.05778.pdf](https://arxiv.org/pdf/2211.05778.pdf) [Accessed
    31 Jul. 2023].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chollet, F. (n.d.). *Xception: Deep Learning with Depthwise Separable Convolutions*.
    [online] Available at: [https://arxiv.org/pdf/1610.02357.pdf.](https://arxiv.org/pdf/1610.02357.pdf.)'
  prefs: []
  type: TYPE_NORMAL
- en: ‌Krizhevsky, A., Sutskever, I. and Hinton, G.E. (2012). ImageNet classification
    with deep convolutional neural networks. *Communications of the ACM*, 60(6), pp.84–90\.
    doi:https://doi.org/10.1145/3065386.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dai, J., Li, Y., He, K. and Sun, J. (n.d.). *R-FCN: Object Detection via Region-based
    Fully Convolutional Networks*. [online] Available at: [https://arxiv.org/pdf/1605.06409v2.pdf.](https://arxiv.org/pdf/1605.06409v2.pdf.)'
  prefs: []
  type: TYPE_NORMAL
- en: ‌‌
  prefs: []
  type: TYPE_NORMAL
- en: ‌
  prefs: []
  type: TYPE_NORMAL
