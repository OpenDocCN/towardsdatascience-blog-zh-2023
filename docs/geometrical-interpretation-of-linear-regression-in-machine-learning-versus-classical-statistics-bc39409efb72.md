# 机器学习与经典统计中的线性回归几何解释

> 原文：[https://towardsdatascience.com/geometrical-interpretation-of-linear-regression-in-machine-learning-versus-classical-statistics-bc39409efb72?source=collection_archive---------4-----------------------#2023-12-12](https://towardsdatascience.com/geometrical-interpretation-of-linear-regression-in-machine-learning-versus-classical-statistics-bc39409efb72?source=collection_archive---------4-----------------------#2023-12-12)

## **通过视觉和分析来揭示线性回归的困惑**

[](https://medium.com/@rishabh29288?source=post_page-----bc39409efb72--------------------------------)[![Rishabh Raman](../Images/041c1c6dd8713bbd2fab5408d5236213.png)](https://medium.com/@rishabh29288?source=post_page-----bc39409efb72--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bc39409efb72--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bc39409efb72--------------------------------) [Rishabh Raman](https://medium.com/@rishabh29288?source=post_page-----bc39409efb72--------------------------------)

·

[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F12323e77e1e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgeometrical-interpretation-of-linear-regression-in-machine-learning-versus-classical-statistics-bc39409efb72&user=Rishabh+Raman&userId=12323e77e1e2&source=post_page-12323e77e1e2----bc39409efb72---------------------post_header-----------) 发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bc39409efb72--------------------------------) ·9 分钟阅读·2023年12月12日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbc39409efb72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgeometrical-interpretation-of-linear-regression-in-machine-learning-versus-classical-statistics-bc39409efb72&user=Rishabh+Raman&userId=12323e77e1e2&source=-----bc39409efb72---------------------clap_footer-----------)

--

[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbc39409efb72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgeometrical-interpretation-of-linear-regression-in-machine-learning-versus-classical-statistics-bc39409efb72&source=-----bc39409efb72---------------------bookmark_footer-----------)![](../Images/facef23d7c152cc1e5e168e448bdcba6.png)

图片：线性回归插图，作者 Stpasha，通过维基共享资源（公共领域）。原始图片链接：[https://upload.wikimedia.org/wikipedia/commons/8/87/OLS_geometric_interpretation.svg](https://upload.wikimedia.org/wikipedia/commons/8/87/OLS_geometric_interpretation.svg)

上述图像表示了普通最小二乘法（OLS）或线性回归的几何解释（在经典统计中这两个词可以互换使用）。让我们以直观的方式解析我们所看到的内容：

+   **变量（X1和X2）：** 想象你有两个变量X1和X2。这些变量可以代表任何东西，比如你学习的小时数和你参加的练习考试次数。

+   **数据点（y）：** 现在，你有了你的结果或你试图预测的内容，我们称之为‘y’。在我们的例子中，这可能是你在实际考试中的成绩。

+   **平面（colX）：** 平面代表通过组合不同数量的变量X1和X2可以得到的所有可能预测值。在我们的例子中，它可能代表基于不同的学习时间和练习考试次数你可能预测的所有可能考试成绩。

+   **估计系数（Beta1和Beta2）：** 这些是OLS方法对每个变量对你的成绩影响程度的最佳猜测。因此，Beta1可能告诉你每多学一个小时成绩预计会增加多少，而Beta2可能告诉你每增加一个练习考试成绩会增加多少。

+   **预测点（XB^）：** 这是基于估计系数你得到的预测成绩。它位于平面上，因为它是使用OLS估计的变量X1和X2的组合。

+   **实际点（y）：** 这是你的实际考试成绩。

+   **误差（ε）：** 这是你实际成绩和预测成绩之间的差异。换句话说，就是预测与现实的偏差。

# **那么，OLS如何运作呢？**

OLS尝试找到Beta1和Beta2的值，使得当你使用X1和X2（学习小时数和练习考试次数）来预测‘y’（考试成绩）时，所有数据点的误差（ε）尽可能小。在图形中，这就像是调整平面，直到垂直虚线（表示误差）总长度尽可能短。实际数据点（y）到平面（colX）的最短距离总是垂直于平面的直线。OLS找到一个特定的平面，使这些垂直距离对于所有点都达到最小。

换句话说，OLS试图“拟合”平面，使其尽可能接近你的实际成绩，同时认识到它通常不会通过所有实际点，因为现实生活中很少有那么完美的情况。

**这就像把最好的纸张贴在一群铅笔点下面，使纸张尽可能接近所有点。**

让我们回顾OLS的主要假设，并将其与上述视觉图像联系起来：

**1\. 线性性**

**假设：** 自变量（X1，X2）与因变量（y）之间的关系是线性的。

**视觉解释：** 在图像中，这就是我们使用平面（colX）来表示 X1 和 X2 组合的原因。如果关系不是线性的，我们无法用平面来表示，它会是曲线或其他形状。

**2\. 独立性**

**假设：** 观察值彼此独立。

**视觉解释：** 每个数据点（表示一个观察值）是独立绘制的。如果存在依赖性，我们会在误差（ε）中看到系统的模式，比如它们都集中在平面的一侧，这表明一个数据点的位置可能预测另一个，违反了这一假设。

**3\. 同方差性**

**假设：** 误差项（ε）的方差在所有自变量水平上保持不变。

**视觉解释：** 理想情况下，实际数据点（y）到预测平面（colX）的垂直距离应该均匀分布。这些距离不应有漏斗形状或模式；它们应显得随机。如果误差随着 X1 或 X2 的增加而增大或减小，这将违反同方差性。

**4\. 无完美多重共线性**

**假设：** 自变量之间没有完全相关性。

**视觉解释：** 在图示中，X1 和 X2 由两个方向不同的箭头表示。如果它们完全相关，它们会指向完全相同的方向，我们将不会有平面而是直线。这将使我们无法估计 X1 和 X2 对 y 的独特影响。

**5\. 无自相关性**

**假设：** 误差项之间不相关。

**视觉解释：** 这个假设涉及误差项，虽然在图像中没有明确显示，但我们推断每个误差项（ε）是随机的，不受前一个或下一个误差项的影响。如果存在模式（例如，一个误差总是跟随另一个相似大小的误差），我们会怀疑自相关性。

**6\. 外生性**

**假设：** 误差项的期望值为零。

**视觉解释：** 这意味着平面应当被定位，使得误差在平均值上互相抵消。一些数据点会在平面之上，一些会在平面之下，但没有系统性偏差使它们都在平面之上或之下。

**7\. 误差的正态性（通常是假设检验的假设）**

**假设：** 误差项呈正态分布。

**视觉解释：** 虽然正态性假设在数据和模型的三维图中无法直观呈现，但如果我们查看误差项的直方图，我们期望看到正态分布的熟悉钟形曲线。

# 线性回归在机器学习宇宙中如何不同于经典统计学中的普通最小二乘法线性回归？

在经典统计学中，普通最小二乘法（OLS）可以通过最大似然估计（MLE）的视角来接近。MLE和OLS都旨在找到模型的最佳参数，但它们来自不同的理论，并使用不同的方法来实现这一目标。

**最大似然估计 (MLE) 方法：**

MLE基于概率。它提出的问题是：“给定一组数据点，什么是最可能的模型参数，这些参数能够生成这些数据？”MLE假设误差具有某种概率分布（通常是正态分布），然后找到最大化观察到实际数据的可能性的参数值。在几何视觉中，这类似于调整平面（colX）的角度和位置，使得看到实际数据点（y）的概率最高。似然不仅考虑点到平面（误差）的距离，还考虑误差分布的形状。

**机器学习 (ML) 中目标函数的最小化：**

另一方面，ML方法通常将回归框架作为优化问题。目标是找到那些使某个目标函数（通常是平方误差总和（SSE））最小的参数。这是一种比MLE更直接的方法，因为它对误差的底层概率分布假设较少。它只是试图在平方意义上将数据点到预测平面的距离尽可能小，以更严厉地惩罚较大的误差。几何解释是你在倾斜和移动平面（colX），以最小化从实际点（y）到平面的垂直距离的平方和（虚线）。

**两者比较：**

尽管程序有所不同——一种是基于概率的方法，另一种是优化技术——但在OLS的情况下，它们往往会产生相同的结果。这是因为当误差服从正态分布时，线性模型系数的MLE会得出与最小化平方误差总和相同的方程。从视觉上看，两种方法实际上都是在尝试在变量X1和X2的空间中定位同一个平面，以便它最好地表示与y的关系。

主要区别在于解释和潜在的泛化。MLE的框架允许在建模误差结构时有更多灵活性，并且可以扩展到误差不被假设为正态分布的模型中。ML方法通常更直接，专注于平方差的总和，而不关注底层概率分布。

总之，虽然最大似然估计 (MLE) 和最小化方法可能会得到相同的最小二乘回归系数，但它们在概念上是不同的。MLE 是概率性的，根植于在给定模型下观察数据的可能性，而最小化方法是算法性的，专注于直接减少误差。几何视觉对两者来说是相同的，但平面位置的原理不同。

# **附加：当你在上述解释中引入正则化时会发生什么？**

正则化是一种用于防止模型过拟合的技术，过拟合可能发生在模型过于复杂，开始捕捉数据中的噪声而不仅仅是数据的真实底层模式。正则化有不同的类型，但最常见的两种是：

+   **套索回归 (L1 正则化)：** 这增加了一个等于系数绝对值的惩罚项。它可以将一些系数减少为零，有效地进行特征选择。

+   **岭回归 (L2 正则化)：** 这增加了一个等于系数平方的惩罚项。所有系数都被相同的因子缩小，没有一个被归零。

让我们用将被子（代表我们的回归模型）铺在床上（代表我们的数据）的例子来说明。在没有正则化的最小二乘法中，我们试图将被子铺得尽可能多地接触床上的点（数据点），最小化被子与床面之间的距离（误差）。

现在，想象一下床非常不平坦，而我们的被子非常柔韧。如果没有正则化，我们可能会把被子贴得非常紧，以至于它适应了每一个小凸起和凹陷，甚至是由于有人没有铺平床单的小问题——这就是过拟合。

# 引入正则化：

+   **使用套索回归 (L1)：** 就像是说：“我希望被子合适，但也希望它尽量平整，褶皱尽可能少。”每一个褶皱代表模型中的一个特征，而 L1 正则化尝试最小化褶皱的数量。最后，你会有一个合适的被子，但可能不会进入每一个缝隙，尤其是那些仅仅是噪声的缝隙。在几何视觉中，套索回归会试图让平面（colX）很好地贴合点，但可能会在不重要的变量方向上变得平坦（将系数缩小为零）。

+   **使用岭回归 (L2)：** 这就像是想要被子贴合得很好，但同时也希望被子均匀展开，没有任何部分离床太远。因此，即使被子仍然紧贴床面，它也不会过度扭曲以适应细微的凸起。在几何视觉中，岭回归添加了一个惩罚项，约束了系数，使它们缩小到接近零但不完全为零。这保持了平面接近点，但防止它倾斜得太厉害以适应特定点，从而保持了一定的距离（偏差）以防止对噪声的过拟合。

# 带有正则化的视觉解释：

当将正则化添加到几何解释中时：

+   平面（colX）可能不会像之前那样靠近每个单独的数据点（y）。正则化故意引入了一些偏差。

+   平面将趋向于更加稳定，较少倾斜于任何单个异常值点，因为对于大系数的惩罚意味着模型不能过于敏感于任何单一的数据点或特征。

+   向量（Beta1X1 和 Beta2X2）的长度可能会更短，反映了每个变量对预测的影响被故意限制的事实。

从本质上讲，正则化在让模型的训练数据拟合能力稍微打折的同时，换取了改进的模型泛化能力，这意味着模型在未见过的数据上表现更好，而不仅仅是在训练数据上。就像选择一个稍微宽松的毯子尺寸，它在所有实际用途上都足够好，而不是一个完美贴合每一个轮廓但可能不切实际或过于特定于一个床的尺寸。

# 结论

总结来说，线性回归的几何解释弥合了经典统计学与机器学习之间的差距，提供了对这一基本技术的直观理解。虽然经典统计学通过普通最小二乘法（Ordinary Least Squares）来处理线性回归，而机器学习则通常使用最大似然估计（Maximum Likelihood Estimation）或目标函数最小化，但这两种方法最终都旨在以视觉上易于理解的方式最小化预测误差。

引入像Lasso和Ridge这样的正则化技术进一步丰富了这种解释，突出了模型准确性和泛化能力之间的平衡。这些方法防止了过拟合，确保模型对新的、未见过的数据保持稳健和有效。

总体而言，这种几何视角不仅揭示了线性回归的奥秘，还强调了在不断发展的数据分析和机器学习领域中基础概念的重要性。这是一个强有力的提醒，表明复杂的算法可以根植于简单而深刻的几何原理中。
