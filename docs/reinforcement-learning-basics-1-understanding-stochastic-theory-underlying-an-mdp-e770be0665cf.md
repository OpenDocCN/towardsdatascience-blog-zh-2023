# 强化学习基础：理解马尔科夫决策过程背后的随机理论

> 原文：[https://towardsdatascience.com/reinforcement-learning-basics-1-understanding-stochastic-theory-underlying-an-mdp-e770be0665cf?source=collection_archive---------5-----------------------#2023-02-16](https://towardsdatascience.com/reinforcement-learning-basics-1-understanding-stochastic-theory-underlying-an-mdp-e770be0665cf?source=collection_archive---------5-----------------------#2023-02-16)

## 第一部分：马尔科夫决策模型的理论基础，这为强化学习问题提供了基础

[](https://medium.com/@shaileydash?source=post_page-----e770be0665cf--------------------------------)[![Shailey Dash](../Images/b5a44aafce4c310f60398e714d515b76.png)](https://medium.com/@shaileydash?source=post_page-----e770be0665cf--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e770be0665cf--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e770be0665cf--------------------------------) [Shailey Dash](https://medium.com/@shaileydash?source=post_page-----e770be0665cf--------------------------------)

·

[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc4fe44a8e55d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-basics-1-understanding-stochastic-theory-underlying-an-mdp-e770be0665cf&user=Shailey+Dash&userId=c4fe44a8e55d&source=post_page-c4fe44a8e55d----e770be0665cf---------------------post_header-----------) 发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e770be0665cf--------------------------------) ·28分钟阅读·2023年2月16日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe770be0665cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-basics-1-understanding-stochastic-theory-underlying-an-mdp-e770be0665cf&user=Shailey+Dash&userId=c4fe44a8e55d&source=-----e770be0665cf---------------------clap_footer-----------)

--

[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe770be0665cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-basics-1-understanding-stochastic-theory-underlying-an-mdp-e770be0665cf&source=-----e770be0665cf---------------------bookmark_footer-----------)![](../Images/7ffb68ec9c76b1f9339fd860f362d1a1.png)

一个简单的MDP示例，包括三个状态（绿色圆圈）和两个动作（橙色圆圈），以及两个奖励（图片来源：[维基百科](https://en.wikipedia.org/wiki/Markov_decision_process#/media/File:Markov_Decision_Process.svg)）

强化学习（RL）是一种机器学习类型，使代理能够通过采取行动在不确定的环境中学习实现目标。强化学习的一个重要方面是它评估所采取的行动，而不是通过提供正确的行动来指导。每个行动都有一个相关的奖励，向代理发出该行动在朝着目标前进中成功的信号。代理在环境中重复导航，学习如何优化以达到其目标。

直到最近，大多数强化学习的成功与游戏代理相关，如 Alpha Go。然而，强化学习越来越多地应用于现实世界中的应用，如自动驾驶汽车和机器人自动化。最近推出的 ChatGPT 也在其架构中包含了一个用于答案微调的 RL 组件。鉴于此，理解 RL 变得重要，因为它被认为具有陡峭的学习曲线，既包括理论方面，也包括实际应用。

本文重点讨论了马尔可夫决策模型（MDP），它通常用于形式化强化学习代理的问题。然而，这种对强化学习问题的概念化一个被忽视的方面是 MDP 框架是随机理论的重要应用。

基本的马尔可夫决策过程方程背后有相当复杂的随机和概率理论。没有理解这些方程及其推导过程，就很难在 RL 领域取得进展。本文旨在澄清 MDP 背后的随机和概率概念，重点理解 Sutton（2017）第3章中呈现的方程。

基本的 MDP 框架如 Sutton 和 Barto（2017），第3章中所呈现的，实际上看起来相当简单和直观——这是一个大师级的巧妙手法！然而，这个框架背后有大量的统计理论。我们不需要了解随机过程的深度，因为，相信我，真的很多！但了解一些 MDP 中使用的概念背后的细微之处是很重要的。因此，理解什么是随机过程很重要，因为这些理论是 MDP 的核心。

本文是一个系列文章中的第一篇，旨在使强化学习（RL）的基础知识在理论和实际应用层面上更加清晰。

**目标受众是谁？**

-   当然，这看起来像是为初学者准备的，但我认为这篇文章实际上对那些有一定 RL 知识的人更有用，他们可以更直观地理解 Sutton (2017) 第 3 章中呈现的方程。这篇文章是我学习 RL 过程的结果。我最初很高兴地开始阅读 Sutton 和 Barto (2017)，并认为我已经理解了第 3 章中大部分关于 MDP 框架的概念。然而，当我尝试实际推导第 3 章中呈现的一些方程时，我发现自己无法弄明白。因此，我意识到可能有更多的内容超出了我的认识范围。我找不到真正的资源将 MDP 关联到其基础统计学支撑上。我不得不使用各种材料，从 Wikipedia 到各大学级的随机理论课程。鉴于这些基础材料的分散，我决定在这篇文章中记录下来。

**方法 — 解码 Sutton 和 Barto (2017)**

-   另一件事是这篇文章的格式。Sutton (2017) 代表了 RL 理论的圣经。目前的版本是在线提供的更新版。然而，大部分理论材料实际上是在 20 年前的书籍第一版中发表的。此外，书中第 3 章所采用的方法是互联网中常见的方法。鉴于此，我在这篇文章中的方法有点像一个“钥匙”，我呈现了书中的材料，然后通过随机理论的视角进行解释。

-   稍微岔开一下，为“钥匙”辩护。大量学生，尤其是在学校，广泛使用它们；而教师实际上对这种“填鸭式教学”表示不满。然而，当你在网上学习，通常是独自一人时，详细的指南变得重要，因为你不能像在课堂上那样举手提问。

-   够多的前言！让我们转到实际话题….

**MDP 的作用**

-   理解强化学习问题在理论 MDP 环境中如何工作，对于掌握 RL 的基本原理至关重要。许多重要的 RL 算法要么基于马尔可夫方程或模型，要么代表了对马尔可夫模型的一些关键偏离。MDP 是强化学习过程的数学理想化形式，并提供了一个理想化的强化学习问题的理论框架。因此，无法回避：理解 MDP 对理解 RL 问题及其解决方法是基础性的。

-   注意，随机理论是概率理论的一个特别复杂的分支，有时所使用的符号可能非常令人畏惧。我将尝试更明确地呈现 MDP 背后的随机概念和思想。希望最终的结果能突显出马尔可夫模型背后的复杂性。

-   在这篇文章中，我将内容分为 3 个部分：

**第 1 节：MDP 的关键组件**

- RL 问题

-   状态、代理、环境、奖励等。

**第2节：MDP中使用的概率和随机概念**

- 随机变量、状态或样本空间、随机过程、实现或轨迹

- 概率分布、联合分布、边际分布、条件分布

- 推导马尔可夫性质

- 概率转移矩阵

**第3节：使用概率和随机概念理解Sutton Ch3方程**

- MDP模型

- MDP模型的形式化表示

- Sutton (2017) Ch3中的方程3.2, 3.3, 3.4, 3.5, 3.6

这可能只是MDP的介绍部分。你可能会问：“那价值函数、策略、Q值函数、贝尔曼方程呢？”这些内容将在下一篇文章中介绍，因为一旦我讲解了上述内容，你会同意这些内容本身就已经很多了。

**第1节：MDP的关键组成部分**

首先，为了设置我们即将解释的内容的背景，我将简要总结[Sutton (2017), Ch3](http://www.incompleteideas.net/book/RLbook2020.pdf)使用的基本MDP模型和定义。

***为什么选择MDP？***

MDP框架受欢迎的主要原因是由于马尔可夫性质，其中未来状态仅依赖于当前状态，而不依赖于状态的历史。换句话说，当前状态封装了做出关于未来状态决策所需的所有相关信息。顺便说一句，这种看似无害的假设实际上背后有丰富的概率理论，并且也有相应的影响。

由于这个特性，MDP代表了一种可处理的方式来形式化顺序决策。它提供了一个概率框架，用于建模在结果部分随机和部分由决策者控制的情况下的决策。在这种情况下，行动不仅在奖励方面有直接的回报，还会影响后续时间段或状态，从而影响未来的奖励。当然，强化学习在实践中的方式与MDP有所不同。

***一些定义以开始***

**代理：**这是学习者或决策者，在环境中做出顺序决策以实现最终目标。

**环境：**这被定义为代理控制范围之外的所有事物。因此，环境可以是机器人的外部环境，可以是自驾车的道路和行人等。在视频游戏中，环境就是游戏。环境的以下几点是重要的：

+   观察是代理接收到的输入——例如，自驾车只能接收来自其周围环境的输入，而不会意识到前方很远处的道路障碍。

+   只有在知道环境模型的情况下，代理才能完全了解环境，例如掌握游戏规则的结构，这些规则决定了环境。

+   环境会因代理的行动而改变（突然刹车会造成小堵塞），但也可能自行变化（道路改道）。

***什么是 RL 问题？***

典型的 RL 问题涉及一个学习代理在一段时间内与其环境互动以实现目标。为此，学习代理必须能够在一定程度上感知环境的状态，并且必须能够采取影响状态的行动。这个基本问题在下面的图中展示。

![](../Images/6586539f97d6dff700ae8f57dbde2125.png)

在马尔可夫决策过程中，代理和环境的交互（图像来源：作者创作，灵感来源于 Sutton (2017) 第三章）

***时间的测量***

时间通常以离散步骤来测量：t=0,1,2,..T。其中，T 代表最终或终端状态。因此，这是一个*有限离散时间*问题。还有连续时间问题以及无限视界问题，但这些在数学上处理起来更为复杂。因此，重点在于有限离散时间模型。

***状态***

这代表了环境的一个快照，包含了所有与环境相关的信息。这包括奖励信息、环境如何根据行动发生变化等。以自动驾驶汽车为例，状态可能代表汽车周围的一个区域。类似地，以前进中的机器人为例，它将代表在采取如前进等行动后环境的快照。

对于马尔可夫目的，代理和环境在一系列离散时间步骤中交互：t=0,1,2…T。其中，T 代表最终或终端状态。在每个时间步骤 t，代理接收环境的某种表示，封装为 Sₜ ∈ **S**，其中 **S** 是所有状态的集合。在 MDP 的背景下，状态是一个离散随机变量。

***奖励***

代理在下一时间周期从环境中接收到一个信号或奖励 Rₜ₊₁，这与行动在实现整体目标方面的表现相关。奖励通常定义为一个标量，其中 Rₜ ∈ ***R*** ⊂ℝ 只是实数集合的一个例子（复杂的符号表示简单的东西！但在统计世界中就是这样😊）。

奖励可以是确定性的，也可以是随机变量。Sutton（2017）将 Rₜ 视为随机变量，尽管大多数示例采用确定性奖励。

现在，Rₜ 依赖于世界的当前状态、刚刚采取的行动和世界的下一个状态，即 Rₜ = R(Sₜ, Aₜ, Sₜ₊₁)。然而，它通常被简化为仅依赖于当前状态和行动对，即 Rₜ = R(Sₜ, Aₜ)。

现在我们已经有了 MDP 的基本组成部分，即状态、奖励、代理、环境，我们需要更好地理解在概率和随机理论背景下，MDP 变量状态、奖励、代理和环境的性质。

现在，让我们跳到概率和随机理论……

**第2节：概率与随机基础**

要定义随机过程，我们首先需要理解什么是随机变量、概率分布、联合分布、边际分布、条件分布和概率链规则。

现在让我们花些时间理解上述概念。一旦理解这些，我们将回到MDP并利用这些概念来理解其工作原理。

由于统计和概率非常广泛，我希望专注于这些概念在MDP中的适用性。因此，我将以*斜体*展示与MDP概念相关的统计学概念，然后解释其在MDP中的相关性。希望这能使MDP概念更加清晰。

***随机变量***

*在概率论中，随机变量是捕捉变量可以取多个值或发生的方式。它表示一个映射或函数，捕捉变量可以映射到多个值的事实。例如，掷硬币的结果有：{H,T}，我们可以将其映射到介于0和1之间的数字，表示概率。请参阅这个* [*维基百科*](https://en.wikipedia.org/wiki/Random_variable) *页面以获取更正式的定义。顺便说一句，我发现维基百科上的概率和统计页面是一个非常好的资源，尽管稍微有些高级。*

*当随机变量的范围是可数的时，称为离散变量，其分布是离散概率质量函数（这个概念我稍后会定义，因为它将被频繁使用）。如果变量是连续的，那么它的分布可以由概率分布函数定义。*

*随机变量的定义通常为实值情况定义，记作* **R***。这个定义也可以扩展到任何可测集合E，其中包含随机布尔值或分类值、向量、矩阵或函数。*

这点很重要，因为在MDP中，我们有实际值的随机变量以及分类类型。同时，随机过程（稍后会定义）也是时间的随机函数。

在上述定义的MDP背景下，我们有状态和奖励作为随机变量。这两个变量都是离散的，并且有一个有限的范围。

***状态空间或样本空间：***

*当我们谈论随机数时，随机变量可以取的有限状态数称为样本状态或在MDP情况下的状态空间。样本空间的最简单例子是掷硬币。它可以取两个值之一：正面或反面。*

在Sutton第3章中，这由集合***S***表示，包含不同的状态，每个状态可以代表不同的情况。例如，我们可以有一个用于去办公室的随机过程。***S***可能包含的状态列表如下：

1\. s’: 遇到通勤去办公室的交通堵塞

2\. s: 遇到去办公室的清晰道路

3\. s”: 遇到因事故造成的道路阻塞

这些状态构成了样本空间或随机变量的范围，即它可以取的值。

***随机过程***：

*随机过程是由变量 t（通常代表时间）索引的随机变量的集合或集成。明确地说，随机过程包括在多个时间点上的相同随机变量。随机过程通常由时间变量索引（它也可以是向量空间，但为了避免混淆，我们暂且不提）。因此，每个索引或时间点都与特定的随机变量相关联。了解更多关于随机过程的信息，请访问这个* [*维基百科*](https://en.wikipedia.org/wiki/Stochastic_process) *页面。*

*这意味着在每个时间点，随机变量可以实现* ***S****.**中的一个值。随机过程也可以写作：S(t,i)，t*∈T。*其中状态是时间（由 t 索引）和代表状态空间***S***的特定状态 i 的函数。*

状态空间是通过反映随机过程可以取的不同值的元素来定义的。因此，在 MDP 的情况下，状态 Sₜ 是一个随时间变化的随机变量，由 t 索引，其中对于每个 t，Sₜ 可以取值于有限集合 ***S =***{s,s’,s’’’}。

关于随机过程的困惑在于，通常跳过第二个索引以避免混乱的展示。然而，没有它的话，实际有 2 件事情正在发生的清晰度会丧失：

1\. 随着变量 t 的演变，随时间推移的运动

2\. 在每个时间点上，从状态集合中选择一个状态

让我们继续以道路为例，以说明随机过程随时间演变和不同状态空间之间的区别。在某个时间点上，随机变量可以取有限数量的状态空间。例如，当我们前往办公室时，在前 15 分钟内，我们可能会遇到清晰的道路或堵车。接下来的 15 分钟，情况可能会发生变化。假设到达办公室需要 45 分钟，那么我们可以将其分为 3 个 15 分钟的时间段。因此，我们的时间段可以是 t=1,2,3。

在前 15 分钟内，道路清晰的概率为 p，遇到堵车的概率为 q，遇到道路封闭的概率为 r。在接下来的 15 分钟内，这三种结果中的一种可能再次出现，最后 15 分钟也是如此。这就是随时间演变。这意味着，在每个时间点上，存在获得某个结果的特定概率。可以通过以下方式进行说明。

![](../Images/e72ab0a6813c0777f742c6e210871e7a.png)

办公室驱动 MDP（图像来源：作者）

图像显示了一个样本轨迹。在时间t=1，即前15分钟，道路可能是畅通的、拥堵的或封闭的，各有相应的概率。类似地，对于t=2或第二个15分钟，以此类推。在某一时刻，比如前15分钟，我们可以看到道路状态可以随机变化于3种可能状态。这代表了在某一时刻状态的随机性。然而，如果我们看时间t，那么我们就有了一个轨迹。所以，如果我们有3个时间点，我们将有3个随机变量，每个时间点对应一个。

我们还可以在这种情况下理解奖励集。奖励集**R**在这种情况下就是在特定时间覆盖特定道路段所需的时间。每种状态都有一个定义的时间分布（为了简化我们可以以离散分钟为单位）。这意味着什么？基本上，道路的每种状态都与时间分布相关。就像有时我们在15分钟内完成清晰道路 (s)，有时由于一些随机变化在17分钟，有时在18分钟。因此，我们可以定义R(s)={15,16,17,18}。对于清晰道路条件下完成路段所需时间有一个概率分布。这是奖励也可能是随机的一个直观示例——这是Sutton (2017)，Ch3中的大多数示例没有涵盖的内容。道路的每种状态——例如，拥堵状态——将有其自身的ETA分布。

***随机过程的实现或轨迹***

*随机过程可以有许多* [*结果*](https://en.wikipedia.org/wiki/Outcome_(probability))*, 由于其随机性。随机过程的单个结果被称为* [***实现***](https://en.wikipedia.org/wiki/Stochastic_process)***、情节或轨迹****。它是通过在随机过程的每个时间点上取随机变量的一个可能值来形成的。*

所以，继续我们的交通示例：图像中的箭头显示了一个可能的轨迹。在前15分钟，司机遇到了一条畅通的道路，在接下来的15分钟遇到了拥堵，而在最后15分钟遇到了封闭的道路。所以，一个潜在的样本路径可以是：(s,s’,s’’)。这只是一个可能的实现。司机每天都经历这条路径，并且很可能每天会遇到略微不同的状态集；另一天可能是 (s’’,s’,s)。

*随机过程被广泛应用于对系统和现象的概率表示，这些系统和现象表现为随机变化。简单来说，随机过程或模型用于估算随时间变化的随机变量的各种结果的概率。例子包括* [*细菌*](https://en.wikipedia.org/wiki/Bacteria) *群体的增长，或* [*气体*](https://en.wikipedia.org/wiki/Gas)[*分子*](https://en.wikipedia.org/wiki/Molecule)*的运动。它们在金融分析中也被广泛使用，其中随机模型可用于估算涉及不确定性的情况，如投资回报、市场波动或通货膨胀率*

*此外，还有许多不同类型的随机过程，其中 Markov 过程是一种。其他类型包括随机游走、高斯过程等。每种随机过程都有其特定的假设和特征，这些特征很重要。*

***概率分布***

现在我们转向如何获取不同状态的概率？回顾上面的例子，我们已经定义了遇到拥堵和清晰道路的概率分别为 q 和 p。现在，让我们稍微详细说明一下这在概率分布中的含义以及它们是如何产生的。

随机结果（道路拥堵）的概率被评估为在非常长的重复序列中该结果发生的比例。因此，这就是我们如何通过记录多个天数中的道路状态变量在特定时间间隔的状态来评估 p、q、r。

*记录随机变量的所有这些输出概率*，Sₜ，*给我们提供了随机变量的概率分布。在离散情况下，这被称为概率质量分布。* *这是一个提供离散随机变量等于特定值的概率的函数。有关更多信息，请参阅* [*这里*](https://en.wikipedia.org/wiki/Probability_distribution)*。*

继续我们办公室通勤的例子：状态变量因此可以以特定的概率取这些值。例如，如果我们为一条道路定义的状态集包含 3 种可能的状态：**S**= { s = 清晰, s’= 拥堵, s’’= 阻塞}。

在特定时间点（比如早晨），道路可以有这三种状态中的任何一种，且具有相应的概率。例如，它可以是自由 =0.5，拥堵 =0.4，阻塞 =0.1（阻塞相对较少见！）。这些概率的集合被称为该变量的概率分布，即道路的状态。

***联合分布 vs 边际分布 vs 条件分布***

*在随机过程的情况下，由于随机变量相同但在不同时间点测量，因此状态在 t,* Sₜ , *可能与之前的状态有关，即* Sₜ₋₁, Sₜ₋₂。*在这种情况下，如果我们想了解* Sₜ *的概率，则相关的是联合概率质量分布 (pmf)。联合 pmf 允许我们计算涉及多个随机变量的事件的概率，同时考虑变量之间的关系。*

*给定两个* [*随机变量*](https://en.wikipedia.org/wiki/Random_variable) *定义在相同的* [*概率空间*](https://en.wikipedia.org/wiki/Probability_space) *上，* [***联合概率分布***](https://en.wikipedia.org/wiki/Joint_probability_distribution) *是所有可能的输出对的相应* [*概率分布*](https://en.wikipedia.org/wiki/Probability_distribution) *。*

*这可以更一般地在马尔可夫背景下定义为：*

*假设我们考虑一个离散时间 2 时间段随机过程，* Sₙ *, 其中 n=1,2。*所以，* S₁ 和 S₂ *是两个离散随机变量，可以从集合 i* ∈ S = {1,2} 中取值。* Sₙ 的联合分布给出每个 n，即时间段和有限状态序列的：(i₁,i₂) 为：

![](../Images/3ee11bfd7563334600b682307bfac7e8.png)

让我们看看我们的简单道路示例：

为简单起见，假设只有 2 个时间段，即 Sₜ 和 Sₜ₋₁。状态空间仍然是 3：{clear(C)，jammed(J)，blocked(B)}。

联合概率表如下：

![](../Images/b744db17148155938ea05c72804862fe.png)

2 时间段办公室驾驶联合概率分布（来源：作者）

联合概率是两个事件同时发生的概率。上表提供了两个事件的不同实例组合。注意，由于这是一个随机过程，时间 t-1 总是先于 t。

***边际概率分布***

*变量的* [*边际分布*](https://en.wikipedia.org/wiki/Joint_probability_distribution) *给出了在子集中各种值的概率，而不参考其他变量的值。Sᵢ 的边际质量函数为：*

![](../Images/7ed0b4f566946e349449eeb2156a2566.png)

边际概率是单个事件发生的概率。本质上，边际分布是按列的概率总和。这个方程要求你对 j 列进行求和。任何组合的联合概率由单元格概率给出。在我们示例的上下文中，它是 Sₜ 中遇到交通堵塞的概率，而不考虑 Sₜ₋₁ 中的路况，即我们对 J 列的概率进行求和。

***条件概率***

*一个* [*条件概率*](https://en.wikipedia.org/wiki/Conditional_probability_distribution) *是指在另一个特定事件已经发生的条件下，某一特定事件发生的概率。这意味着在* Sₜ *的情况下，给定道路在* Sₜ₋₁ *时是畅通的情况下发生堵塞的概率。*

现在在我们的例子中，由于 Sₜ 是一个随机变量，并且我们考虑的是一个两个时期的例子，所以联合概率和条件概率是相同的。

*在数学上，给定另一个变量的条件下的变量分布是两个变量的联合分布除以另一个变量的边际分布。*

*一般而言，定义为：*

![](../Images/dde8178697faf0a7231e62d4bf85bd21.png)

*将其推广到我们有 n 个随机变量的情况，* X₁,.., Xₙ。

*这 n 个变量的联合概率分布是：*

![](../Images/2d293c3740f6f14ecbe1a7a3ee91fb29.png)

*这可以写成条件函数乘以边际函数。这是基于* [*概率链式法则*](https://en.wikipedia.org/wiki/Chain_rule_(probability)) *的。在符号表示上，* Pₓ *（*x₁）= P(X=x₁)。* 对于两个随机变量* X₁, X₂ *，其联合分布可以表示为：*

![](../Images/831a66b22750e3ca7f6ad95e7465229c.png)

*用言语表述：两个变量* X₁, X₂ *取特定值* x₁, x₂ *的联合概率分布可以表示为：* X₂=x₂ *在给定* X₁=x₁ *的条件下的条件概率，乘以* X₁=x₁ *的边际概率。*

***我们为什么需要这些分布——推导Markov性质的需求***

联合、条件和边际概率分布定义在MDP（马尔可夫决策过程）背景下变得重要，当我们想确定状态变量在不同时间段过渡时取特定值的概率时。我们的例子很简单，由三个时间段组成。但是，如果时间段是100，我们想了解状态在第100个时间段取特定值的概率怎么办？这就是潜在的概率问题。让我们看看一般情况如何解决，然后了解Markov性质。

*对于更一般的情况，要获得* X₁,.., Xₙ *随机变量取特定值* x₁,.., xₙ *的联合概率，可以通过* Xₙ *给定之前所有随机变量取特定值的条件概率，乘以前 n-1 次分布的联合概率分布来找到。*

![](../Images/32c29c24137d4886665587e377268d89.png)

*我们可以将 P(*Xₙ₋₁=xₙ₋₁…,X₁=x₁)* 的联合概率表达式再次展开为* Xₙ₋₁ *给定（*Xₙ₋₂,…,X₁*）的条件概率和（*X₁,..,Xₙ₋₂*）的联合概率。*

*这可以进一步向后展开。最终，我们将得到条件概率序列和第一个变量 P(*X₁=x₁) 的边际概率的表达式。*

![](../Images/91af8fb4cd7f3e847a7d8ded58ae7481.png)

现在让我们将其翻译到MDP的背景中。到目前为止，这已经以任何随机变量序列的形式呈现。然而，这个结果很容易转化为随机变量和马尔可夫过程。我们只需将Xᵢ替换为Sₜ，现在索引将以时间的形式出现，即t=1,2,..n，而不是Xᵢ = xᵢ。

我为什么要重复这些明显的内容？因为有很多材料使用不同的变量和索引来描述马尔可夫过程和其他随机变量序列，这可能会非常混乱。我尝试为自己解开这些复杂的概念，因此写下这些内容，以帮助那些试图弄清楚马尔可夫结果背后的概率理论的非统计学家。

在分析MDP时，t=1,..,n，状态xᵢ ∈ S。为了明确，xₙ是时间段t=n时的一个特定状态值。

让我们写出状态变量在t=1..n的演变过程中，获得S₁,..,Sₙ的联合概率。代入上述讨论的一般情况，我们得到的联合概率是条件概率的序列和S₁的边际分布。

![](../Images/e219768fe3b276680ffb69ef9a7f5725.png)

用语言描述就是：获得状态S₁,..,Sₙ的概率等于在时间t=n时状态的值为xₙ的条件概率，乘以前n-1期的联合分布。这展开为一系列条件概率和状态S₁的边际或初始分布。

现在这只是一个随机过程，其中状态在时间段n的值依赖于过去状态值的历史。这显然是难以计算的，因此需要马尔可夫性质。

***马尔可夫性质:***

马尔可夫性质使得状态Sₙ只依赖于紧接前一个状态Sₙ₋₁，而不是整个历史状态的积累。在这种情况下，我们的状态Sₙ的条件概率减少为：

![](../Images/fa4686a7d953c456ec5748e0a61e7e2e.png)

然而，我们仍然需要估计n-1个一步条件概率以及初始概率分布，*P(*X₁=x₁)。这仍然比较复杂，因为需要计算大量的条件概率。为了简化并使计算更可行，进一步的假设是过渡概率的时间同质性。

***过渡概率的时间同质性***

*这基本上说的是：从时间段n的状态i过渡到时间段n+1的状态j的概率是相同的，不受时间段的影响：*

![](../Images/7252b0a97eedde66e56d3017a4a6eaa1.png)

*这意味着从一个状态过渡到下一个状态的概率是固定的，不受时间的影响。*

放到我们的道路示例中，从清晰道路到拥堵道路的概率是固定的，无论我们在前 2 个时间段还是最后 2 个时间段遇到这 2 个状态。因此，转移概率简化为以下 9 种情况，与时间无关。其中 C=清晰，j=拥堵，B=封堵。

![](../Images/607a0cf49440b45aa8727f0ec37ec842.png)

如你所见，如果状态的数量少于时间周期，通常这种情况会显著减少需要计算的概率。

***转移概率***

*状态* [*转移概率矩阵*](https://en.wikipedia.org/wiki/Discrete-time_Markov_chain) *给出了在单一步骤中从一个状态转移到另一个状态的概率。*

*对于 t=1,2 状态情况，这将把问题的维度简化为*

![](../Images/17300716c49a49e0bad004b4fc071bb3.png)

这分解为以下几个组成部分：

*条件概率：*

![](../Images/8174f8ebfb168c1ee0e42311baec22df.png)

*初始分布或边际分布：P(*S₁=x₁)*

在我们的道路示例中，这些是 3 个状态的初始概率：p、q、r。

所以，我们所需的仅仅是一步条件概率和状态的初始分布，以获得转移到某个状态的联合概率。

*转移矩阵可以表示如下：*

![](../Images/a184600677e45bc59986b9fad4c3c4bc.png)

来源：[维基百科 随机矩阵](https://en.wikipedia.org/wiki/Stochastic_matrix#:~:text=In%20mathematics%2C%20a%20stochastic%20matrix,substitution%20matrix%2C%20or%20Markov%20matrix.)

*在转移矩阵中，行代表当前状态，列代表未来状态。因此* p₁₂ *是从时间 t 的状态 1 转移到时间 t+1 的状态 2 的概率。*

*另外，请注意从一个状态出发的所有分支概率之和必须为 1。这是逻辑上的，因为你必须从一个状态转移到另一个状态。数学上可以写作：*

![](../Images/a8e31f02b2165753041bbf20c42822b1.png)

*为什么我们对‘j’进行求和？因为这代表了你可以从‘i’转移到的可能状态。*

***马尔可夫链动态***

对于马尔可夫链，我们使用“动态”一词来描述在给定初始状态下，状态在短时间间隔内的变化。马尔可夫链的演变是一个随机过程，因此我们不能确切地说初始状态之后会跟随什么状态序列。在强化学习中，我们希望根据当前状态或初始状态预测未来状态。一旦我们有了概率转移矩阵和初始分布，可以很容易地计算出给定 S₁ 的未来状态 Sₙ 的预测。n 步转移概率矩阵可以通过将单步概率矩阵自乘 n 次得到。

![](../Images/8f6bd37c2c6ba38902a8477428014508.png)

这就是马尔可夫系统的美妙之处，以及它为何在强化学习中被使用。一旦我们拥有一个可处理的转移概率集，我们就可以计算从任何初始状态到状态j在n步内的转移概率。

现在，在对MDP背后的随机和概率进行如此长时间的离题讨论之后，我将简要回顾Sutton（2017）第3章中的MDP方程。

**第3节：使用概率和随机概念理解Sutton第3章的MDP方程**

***MDP模型***

在这一节中，我简要定义了MDP，给出本质上是教科书中的定义。在下一节中，我们将探讨MDP的统计理论。

> *MDP框架是一个关于目标导向学习从交互中问题的显著抽象。它提出，无论感知、记忆和控制装置的细节如何，以及目标是什么，任何目标导向行为的学习问题都可以归结为在代理和环境之间来回传递的三个信号：一个信号表示代理所做的选择（行动），一个信号表示选择的依据（状态），一个信号定义代理的目标（奖励）。这个框架可能不足以有效地表示所有决策学习问题，但它被证明在广泛的应用中非常有用。*

*Sutton第2章，第50页*

实际上，大部分MDP框架可以在没有详细的概率和随机理论知识的情况下高层次地理解。然而，现在我们已经接触到这些概念，我们可以真正欣赏到其中的复杂性。让我们现在看看Sutton（2017）第3章中与马尔可夫过程相关的一些方程。

***马尔可夫性质***

这本质上是未来在给定现在的情况下与过去无关的观点。它在概率上被概念化为：

![](../Images/e8419e256fd14c81e1b587212d392bd8.png)

这已经在上文中详细讨论过，使我们能够理解基本的马尔可夫问题如何变得更容易处理。满足马尔可夫性质的随机过程通常比将历史过程纳入模型中更简单分析。在实践中，考虑马尔可夫性质是否成立是很重要的。例如，对于汽车的位置来说，汽车的当前所在位置将决定它未来的状态。但在某些情况下，历史可能很重要：例如，交易代理可能需要考虑股票的过去趋势以及当前价格，以决定是否买入或卖出。

马尔可夫过程是具有以下性质的随机过程：

(a.) 可能的结果或状态的数量是有限的

+   这是为了分析的便利。无限过程也可以通过一些小的操作来处理。然而，对于时间依赖的过程，我们假设时间是有限的 t=0,1,2,…,T。

(b.) 任何阶段的结果仅依赖于前一阶段的结果 — 由上述讨论的马尔可夫性质给出。

(c.) 转移概率随时间保持不变。

(d.) 系统是平稳的 — 这实际上没有明确说明，但属性 c) 实际上隐含了这一点。

***MDP 的正式表示***

正式地，MDP 可以表示如下。鉴于马尔可夫性质，MDP 定义如下：一个通常包含 5 个元素的元组：

![](../Images/060e9627a0855438bb93867e50909331.png)

**S**：环境中的状态集合：*这实际上是一个随机过程*

**A**：每个状态下可能的动作集合

**R:** 对动作的奖励函数：*这通常是确定性的，但有时也可以是随机的*

**p :** 从一个状态转移到下一个状态的概率矩阵 — 这是 MDP 的关键部分。它由状态变量的联合和条件分布定义。

**γ :** 适用于远期奖励的折扣因子

MDP 和智能体共同产生一个序列或轨迹，如下所示：

![](../Images/c3dfa5301b2d5039c76ef0268da018a2.png)

这个过程是一个随机过程且是有限的，即有 T 个时间步骤，并且有一个明确的终止状态。

***理解方程***

*方程 3.2*

![](../Images/dda8c3e2e3878d545cc3ee992cfd1ba7.png)

该方程告诉我们 Rₜ 和 Sₜ 是具有离散概率质量分布的随机变量，这些分布依赖于先前的状态和动作。方程 3.2 定义了动态函数 p。这是一个包含 4 个参数的函数。它还为每种状态和动作选择指定了条件分布。对于特定的状态 s’ 和奖励 r，我们可以估计在时间 t 这些值发生的概率，给定在 t-1 时刻的前状态和动作值。

*方程 3.3*

![](../Images/35cffa20d2bc48f2491305f3080fb469.png)

方程 3.3 基本上告诉我们，对于每个“s”和“a”的值，都有一个状态和奖励的联合分布。这基本上是一个联合条件分布，因此它的和必须为 1。联合分布将具有不同组合的 s’ 和 r 的概率。所有组合的概率之和必须为 1（[参见维基百科](https://en.wikipedia.org/wiki/Joint_probability_distribution)）。

让我们回到之前的道路示例。在道路的每个状态下，我们有两个动作：驾驶或停止。如果我们处于“拥堵”状态并选择驾驶，那么在这两者的条件下，将有一个下一状态的状态和奖励的联合分布。

*状态转移概率 — 方程 3.4*

![](../Images/f20656f25b96ae18ef0a61e75f836266.png)

状态转移概率通过对奖励概率进行求和来给出（请注意，对于概率挑战：类似于状态的边际概率分布 - [参见维基百科](https://en.wikipedia.org/wiki/Joint_probability_distribution)）。

*期望奖励 — 方程3.5*

![](../Images/3d16279d0fa3436d03788c23e817da7c.png)

在这里，我们再次处理方程3.2。这次我们对状态的概率进行求和（对于概率不太清楚的人：类似于奖励的边际概率分布 — [参见Wikipedia](https://en.wikipedia.org/wiki/Joint_probability_distribution)）。这给出了奖励分布的边际概率（3.5的极端右侧表达式）。然后我们将这个概率乘以实际奖励r，以获得期望奖励。

*期望奖励的状态-动作-下一状态三元组 — 方程3.6*

![](../Images/a79bbafe62dcd3eaceb9c8f08eea3eb3.png)

这个有点棘手。奖励和状态的联合概率条件在‘s’和‘a’下除以状态转移概率。这是联合分布链式法则的一个应用（参见 [Wikipedia](https://en.wikipedia.org/wiki/Chain_rule_(probability))）。

![](../Images/403725b86d34581c1cfbaf9ef844b799.png)

让我们代入我们的术语。

![](../Images/c03c5419d93ff42848065fc7eff84bb1.png)![](../Images/8accbbba3b261f8518cac930ab112523.png)

现在我们可以对Rₜ取期望值

![](../Images/01fa099b84e858904ef3bb2a6430d147.png)

所以，这完成了MDP设置的基本方程的推导。理解3.2到3.6每一个方程都很重要，因为Sutton（2017年）提到这些方程都是表示MDP的一种方式，并且会在后续章节中再次使用。

**总结..**

好朋友们，这一段关于概率和随机理论的旅程已经相当长了。然而，我希望你们意识到的是基础Markov系统的复杂性和美丽。如果不理解这些内容，很难弄清楚基本MDP方程的底层方程。如果不理解这些方程及其推导方式，当涉及到策略、价值函数和Q值时，很可能会完全迷失。下一部分教程中会涉及到这些内容。

希望你喜欢我的写作。请考虑关注我，以便获取更多类似的文章，我会继续撰写关于强化学习的内容：[https://medium.com/@shaileydash](https://medium.com/@shaileydash)

也请在评论中告诉我你的意见和建议，这将帮助我修改内容。

你可以在Linkedin上关注我，获取更多关于AI和数据科学的企业导向文章：[www.linkedin.com/in/shailey-dash-phd-8b4a286](http://www.linkedin.com/in/shailey-dash-phd-8b4a286)

**参考文献**

我列出了一份相当长的统计学和概率理论资源列表。这些资源都非常优秀，但也相当困难。

Richard S. Sutton 和 Andrew G. Barto. [《强化学习：导论；第2版》](http://incompleteideas.net/book/bookdraft2017nov5.pdf)。2017年。

[https://en.wikipedia.org/wiki/Markov_decision_process](https://en.wikipedia.org/wiki/Markov_decision_process)

[https://en.wikipedia.org/wiki/Stochastic_process](https://en.wikipedia.org/wiki/Stochastic_process)

[https://en.wikipedia.org/wiki/Joint_probability_distribution](https://en.wikipedia.org/wiki/Joint_probability_distribution)

[https://en.wikipedia.org/wiki/Conditional_probability_distribution](https://en.wikipedia.org/wiki/Conditional_probability_distribution)

[https://www.kent.ac.uk/smsas/personal/lb209/files/notes1.pdf](https://www.kent.ac.uk/smsas/personal/lb209/files/notes1.pdf)

Lawler, G.F. (2006). 随机过程导论（第2版）。Chapman and Hall/CRC。 [https://doi.org/10.1201/9781315273600](https://doi.org/10.1201/9781315273600)

Yates, R.D. 和 Goodman, D.J., 2014\. *概率与随机过程：电气与计算机工程师的友好介绍*。John Wiley & Sons.

Unnikrishna Pillai, 概率与随机过程，第8讲，NYU Tandon 工程学院，[https://www.youtube.com/watch?v=nFF6eRQT2-c&t=455s](https://www.youtube.com/watch?v=nFF6eRQT2-c&t=455s)
