- en: LLM Monitoring and Observability — A Summary of Techniques and Approaches for
    Responsible AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM 监控与可观测性——负责任 AI 的技术和方法总结
- en: 原文：[https://towardsdatascience.com/llm-monitoring-and-observability-c28121e75c2f?source=collection_archive---------0-----------------------#2023-09-15](https://towardsdatascience.com/llm-monitoring-and-observability-c28121e75c2f?source=collection_archive---------0-----------------------#2023-09-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/llm-monitoring-and-observability-c28121e75c2f?source=collection_archive---------0-----------------------#2023-09-15](https://towardsdatascience.com/llm-monitoring-and-observability-c28121e75c2f?source=collection_archive---------0-----------------------#2023-09-15)
- en: '[](https://medium.com/@joshpoduska?source=post_page-----c28121e75c2f--------------------------------)[![Josh
    Poduska](../Images/89ee323bac41d31083206a37df1dd0a3.png)](https://medium.com/@joshpoduska?source=post_page-----c28121e75c2f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c28121e75c2f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c28121e75c2f--------------------------------)
    [Josh Poduska](https://medium.com/@joshpoduska?source=post_page-----c28121e75c2f--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@joshpoduska?source=post_page-----c28121e75c2f--------------------------------)[![Josh
    Poduska](../Images/89ee323bac41d31083206a37df1dd0a3.png)](https://medium.com/@joshpoduska?source=post_page-----c28121e75c2f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c28121e75c2f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c28121e75c2f--------------------------------)
    [Josh Poduska](https://medium.com/@joshpoduska?source=post_page-----c28121e75c2f--------------------------------)'
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb6dae10267e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-monitoring-and-observability-c28121e75c2f&user=Josh+Poduska&userId=b6dae10267e5&source=post_page-b6dae10267e5----c28121e75c2f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c28121e75c2f--------------------------------)
    ·10 min read·Sep 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc28121e75c2f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-monitoring-and-observability-c28121e75c2f&user=Josh+Poduska&userId=b6dae10267e5&source=-----c28121e75c2f---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb6dae10267e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-monitoring-and-observability-c28121e75c2f&user=Josh+Poduska&userId=b6dae10267e5&source=post_page-b6dae10267e5----c28121e75c2f---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c28121e75c2f--------------------------------)
    ·10分钟阅读·2023年9月15日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc28121e75c2f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-monitoring-and-observability-c28121e75c2f&user=Josh+Poduska&userId=b6dae10267e5&source=-----c28121e75c2f---------------------clap_footer-----------)'
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc28121e75c2f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-monitoring-and-observability-c28121e75c2f&source=-----c28121e75c2f---------------------bookmark_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc28121e75c2f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-monitoring-and-observability-c28121e75c2f&source=-----c28121e75c2f---------------------bookmark_footer-----------)'
- en: '***Intended Audience****:* ***Practitioners*** *who want to learn what approaches
    are available and how to get started implementing them, and* ***leaders*** *seeking
    to understand the art of the possible as they build governance frameworks and
    technical roadmaps.*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '***目标受众***: ***实践者*** *希望了解可用的方法以及如何开始实施这些方法的情况，和* ***领导者*** *希望在构建治理框架和技术路线图时理解可能性的艺术。*'
- en: Seemingly overnight every CEO to-do list, job posting, and resume includes generative
    AI (genAI). And rightfully so. Applications based on foundation models have already
    changed the way millions work, learn, write, design, code, travel, and shop. Most,
    including me, feel this is just the tip of the iceberg.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎一夜之间，每个首席执行官的待办事项、招聘广告和简历上都出现了生成 AI（genAI）。这完全是应有的。基于基础模型的应用程序已经改变了数百万人的工作、学习、写作、设计、编程、旅行和购物方式。大多数人，包括我在内，都认为这只是冰山一角。
- en: In this article, I summarize research conducted on existing methods for large
    language model (LLM) monitoring. I spent many hours reading documentation, watching
    videos, and reading blogs from software vendors and open-source libraries specializing
    in LLM monitoring and observability. The result is a practical taxonomy for monitoring
    and observing LLMs. I hope you find it useful. In the near future, I plan to conduct
    a literature search of academic papers to add a forward-looking perspective.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '***Software researched*****:* [*Aporia*](https://www.aporia.com/)*,* [*Arize*](https://arize.com/)*,*
    [*Arthur*](https://www.arthur.ai/)*,* [*Censius*](https://censius.ai/)*,* [*Databricks/MLFlow*](https://www.databricks.com/blog/announcing-mlflow-24-llmops-tools-robust-model-evaluation)*,*
    [*Datadog*](https://www.datadoghq.com/)*,* [*DeepChecks*](https://deepchecks.com/)*,*
    [*Evidently*](https://www.evidentlyai.com/)*,* [*Fiddler*](https://www.fiddler.ai/)*,*
    [*Galileo*](https://www.rungalileo.io/)*,* [*Giskard*](https://www.giskard.ai/)*,*
    [*Honeycomb*](https://www.honeycomb.io/)*,* [*Hugging Face*](https://huggingface.co/)*,*
    [*LangSmith*](https://www.langchain.com/langsmith)*,* [*New Relic*](https://newrelic.com/)*,*
    [*OpenAI*](https://openai.com/)*,* [*Parea*](https://www.parea.ai/)*,* [*Trubrics*](https://trubrics.com/)*,*
    [*Truera*](https://truera.com/)*,* [*Weights & Biases*](https://wandb.ai/site)*,*
    [*Why Labs*](https://whylabs.ai/)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '*****This article presents a cumulative taxonomy without grading or comparing
    software options. [Reach out to me](https://www.linkedin.com/in/joshpoduska/)
    if you’d like to discuss a particular software covered in my research.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Article Outline**'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Evaluating LLMs** — How are LLMs evaluated and deemed ready for production?'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tracking LLMs** — What does it mean to track an LLM and what components need
    to be included?'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring LLMs** — How are LLMs monitored once they are in production?'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The LLM Lifecycle**'
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/54f47b4b6f031b0a07869bb6629c7276.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: Photo created by Josh Poduska — [Link to Interactive View](https://miro.com/app/board/uXjVMkjWJq4=/?share_link_id=719484162544)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '**The race is on to incorporate LLMs in production workflows, but the technical
    community is scrambling to develop best practices to ensure these powerful models
    behave as expected over time.**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '**Evaluating LLMs**'
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Evaluating a traditional machine learning (ML) model involves checking the accuracy
    of its output or predictions. This is usually measured by well-known metrics such
    as Accuracy, RMSE, AUC, Precision, Recall, and so forth. Evaluating LLMs is a
    lot more complicated. Several methods are used today by data scientists.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: (1) Classification and Regression Metrics
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: LLMs can produce numeric predictions or classification labels, in which case
    evaluation is easy. It’s the same as with traditional ML models. While this is
    helpful in some cases, we are usually concerned with evaluating LLMs that produce
    text.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: (2) Standalone text-based Metrics
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: These metrics are useful for evaluating text output from an LLM when you do
    not have a source of ground truth. It is up to you to determine what is acceptable
    based on past experience, academic suggestions, or comparing scores of other models.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当你没有可靠的基准数据源时，这些指标对于评估 LLM 的文本输出非常有用。你可以根据过去的经验、学术建议或其他模型的评分来决定什么是可接受的。
- en: '[Perplexity](https://huggingface.co/spaces/evaluate-metric/perplexity) is one
    example. It measures the likelihood the model would generate an input text sequence
    and can be thought of as evaluating how well the model learned the text it was
    trained on. Other examples include [Reading Level](https://docs.whylabs.ai/docs/langkit-modules#text-statistics)
    and [Non-letter Characters](https://www.evidentlyai.com/blog/unstructured-data-monitoring).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[困惑度](https://huggingface.co/spaces/evaluate-metric/perplexity)是一个例子。它衡量模型生成输入文本序列的可能性，可以看作是评估模型对其训练文本的学习效果的方式。其他例子包括[阅读水平](https://docs.whylabs.ai/docs/langkit-modules#text-statistics)和[非字母字符](https://www.evidentlyai.com/blog/unstructured-data-monitoring)。'
- en: A more sophisticated standalone approach involves extracting embeddings from
    model output and analyzing those embeddings for unusual patterns. This can be
    done manually by inspecting a graph of your embeddings in a 3D plot. Coloring
    or comparing by key fields like gender, predicted class, or perplexity score can
    reveal lurking problems with your LLM application and provide a measure of bias
    and explainability. Several software tools exist that allow you to visualize embeddings
    in this way. They cluster the embeddings and map them into 3 dimensions. This
    is usually done with [HDBSCAN and UMAP](https://arize.com/resource/understanding-umap-and-hdbscan/),
    but some leverage a [K-means-based approach](https://www.fiddler.ai/blog/monitoring-natural-language-processing-and-computer-vision-models-part-1).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 更复杂的独立方法包括从模型输出中提取嵌入，并分析这些嵌入以寻找不寻常的模式。这可以通过检查 3D 图中的嵌入图来手动完成。通过颜色编码或按关键字段（如性别、预测类别或困惑度评分）比较，可以揭示你的
    LLM 应用中潜在的问题，并提供偏见和解释性的衡量。存在一些软件工具可以以这种方式可视化嵌入。它们将嵌入聚类并映射到 3 维空间。这通常通过[HDBSCAN
    和 UMAP](https://arize.com/resource/understanding-umap-and-hdbscan/)完成，但有些工具采用[基于
    K-means 的方法](https://www.fiddler.ai/blog/monitoring-natural-language-processing-and-computer-vision-models-part-1)。
- en: In addition to visual assessment, an anomaly detection algorithm can be run
    across the embeddings to look for outliers.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 除了视觉评估外，还可以运行异常检测算法来检查嵌入中是否存在异常值。
- en: (3) Evaluation Datasets
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 评估数据集
- en: A dataset with ground truth labels allows for the comparison of textual output
    to a baseline of approved responses.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有真实标签的数据集可以将文本输出与批准的响应基准进行比较。
- en: A well-known example is the [ROUGE](https://huggingface.co/spaces/evaluate-metric/rouge)
    metric. In the case of language translation tasks, ROUGE relies on a reference
    dataset whose answers are compared against the LLM being evaluated. Relevance,
    accuracy, and a host of other metrics can be calculated against a reference dataset.
    Embeddings play a key role. Standard [distance metrics](https://docs.aporia.com/api-reference/metrics-glossary#statistical-distances)
    like as J-S Distance, Hellinger Distance, KS Distance, and PSI compare your LLM
    output embeddings to the ground truth embeddings.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一个著名的例子是[ROUGE](https://huggingface.co/spaces/evaluate-metric/rouge)指标。在语言翻译任务中，ROUGE
    依赖于一个参考数据集，其答案与正在评估的 LLM 进行比较。可以根据参考数据集计算相关性、准确性和其他一系列指标。嵌入在其中起着关键作用。标准的[距离指标](https://docs.aporia.com/api-reference/metrics-glossary#statistical-distances)如
    J-S 距离、赫灵距离、KS 距离和 PSI，将你的 LLM 输出嵌入与真实标签嵌入进行比较。
- en: Lastly, there are a number of widely accepted benchmark tests for LLMs. Stanford’s
    [HELM page](https://crfm.stanford.edu/helm/latest/?group=core_scenarios) is a
    great place to learn about them.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，存在一些广泛接受的 LLM 基准测试。斯坦福的[HELM 页面](https://crfm.stanford.edu/helm/latest/?group=core_scenarios)是了解这些测试的好地方。
- en: (4) Evaluator LLMs
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 评估 LLM
- en: At first glance, you may think it’s cheating the system to use an LLM to evaluate
    an LLM, but many feel that this is the best path forward and [studies](https://arxiv.org/abs/2212.08073)
    have shown promise. It is highly likely that using what I call Evaluator LLMs
    will be the predominant method for LLM evaluation in the near future.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，你可能会认为使用 LLM 来评估 LLM 是作弊，但许多人认为这是前进的最佳途径，[研究](https://arxiv.org/abs/2212.08073)也显示了希望。使用我所说的评估
    LLM 很可能会成为不久的将来 LLM 评估的主要方法。
- en: One widely accepted example is the [Toxicity](https://huggingface.co/spaces/evaluate-measurement/toxicity)
    metric. It relies on an Evaluator LLM (Hugging Face recommends [roberta-hate-speech-dynabench-r4](https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target))
    to determine if your model’s output is Toxic. All the metrics above under Evaluation
    Datasets apply here as we treat the output of the Evaluator LLM as the reference.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一个广泛接受的例子是[Toxicity](https://huggingface.co/spaces/evaluate-measurement/toxicity)度量。它依赖于一个评估器
    LLM（Hugging Face 推荐的[roberta-hate-speech-dynabench-r4](https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target)）来确定你的模型输出是否有毒。上述所有关于评估数据集的度量在这里也适用，因为我们将评估器
    LLM 的输出视为参考。
- en: According to researchers at Arize, Evaluator LLMs should be configured to provide
    binary classification labels for the metrics they test. Numeric scores and ranking,
    [they explain](https://docs.arize.com/phoenix/concepts/llm-evals#binary-performance-evaluation),
    need more work and are not as performant as binary labeling.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 Arize 的研究人员，评估器 LLM 应配置为提供二元分类标签以用于它们测试的度量。[他们解释说](https://docs.arize.com/phoenix/concepts/llm-evals#binary-performance-evaluation)，数值评分和排名需要更多工作，并且不如二元标记表现良好。
- en: (5) Human Feedback
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: (5) 人类反馈
- en: With all the emphasis on measurable metrics in this post, software documentation,
    and marketing material, you should not forget about manual human-based feedback.
    This is usually considered by data scientists and engineers in the early stages
    of building an LLM application. LLM observability software usually has an interface
    to aid in this task. In addition to early development feedback, it is a best practice
    to include human feedback in the final evaluation process as well (and ongoing
    monitoring). Grabbing 50 to 100 input prompts and manually analyzing the output
    can teach you a lot about your final product.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这篇文章、软件文档和营销材料强调了可测量的度量，但你不应忘记手动基于人类的反馈。这通常在构建 LLM 应用的早期阶段由数据科学家和工程师考虑。LLM
    可观察性软件通常有一个接口来协助这个任务。除了早期开发反馈之外，将人类反馈纳入最终评估过程（以及持续监控）也是一种最佳实践。获取 50 到 100 个输入提示并手动分析输出可以让你对最终产品有很大的了解。
- en: '**Tracking LLMs**'
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**跟踪 LLMs**'
- en: Tracking is the precursor to monitoring. In my research, I found enough nuance
    in the details of tracking LLMs to warrant its own section. The low-hanging fruit
    of tracking involves capturing the number of requests, response time, token usage,
    costs, and error rates. Standard system monitoring tools play a role here alongside
    the more LLM-specific options (and those traditional monitoring companies have
    marketing teams that are quick to claim LLM Observability and Monitoring based
    on simple functional metric tracking).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪是监控的前提。在我的研究中，我发现跟踪 LLMs 的细节中有足够的细微差别，需要单独成章。跟踪的低悬果包括捕获请求数量、响应时间、令牌使用情况、成本和错误率。标准系统监控工具在这里发挥作用，同时还有更多针对
    LLM 的选项（传统监控公司也有营销团队迅速宣称基于简单功能度量跟踪的 LLM 可观察性和监控）。
- en: Deep insights are gained from capturing input prompts and output responses for
    future analysis. This sounds simple on the surface, but it’s not. The complexity
    comes from something I’ve glossed over so far (and most data scientists do the
    same when talking or writing about LLMs). We are not evaluating, tracking, and
    monitoring an LLM. We are dealing with an application; a conglomerate of one or
    more LLMs, pre-set instruction prompts, and agents that work together to produce
    the output. Some LLM applications are not that complex, but many are, and the
    trend is toward more sophistication. In even slightly sophisticated LLM applications
    it can be difficult to nail down the final prompt call. If we are debugging, we’ll
    need to know the state of the call at each step along the way and the sequence
    of those steps. Practitioners will want to leverage software that helps with unpacking
    these complexities.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 从捕获输入提示和输出响应中获得深刻见解以进行未来分析。这听起来很简单，但实际上并非如此。复杂性来自于我迄今为止略过的某些内容（大多数数据科学家在谈论或写作关于
    LLM 时也一样）。我们并不是在评估、跟踪和监控一个 LLM。我们处理的是一个应用程序；一个或多个 LLM、预设的指令提示和协作生成输出的代理的集合。一些
    LLM 应用并不复杂，但许多是的，并且趋势是向更复杂的方向发展。在即使是稍微复杂的 LLM 应用中，确定最终的提示调用可能是困难的。如果我们在调试，我们需要知道每一步的调用状态及这些步骤的顺序。实践者会希望利用帮助解开这些复杂性的工具。
- en: '**Monitoring LLMs**'
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**监控 LLMs**'
- en: While most LLMs and LLM applications undergo at least some form of evaluation,
    too few have implemented continuous monitoring. We’ll break down the components
    of monitoring to help you build a monitoring program that protects your users
    and brand.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大多数 LLM 和 LLM 应用程序至少经历过某种形式的评估，但实施持续监控的却很少。我们将拆解监控的组成部分，以帮助你建立一个保护用户和品牌的监控程序。
- en: (1) Functional Monitoring
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 功能监控
- en: To start, the low-hanging fruit mentioned in the Tracking section above should
    be monitored on a continuous basis. This includes the number of requests, response
    time, token usage, costs, and error rates.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，上述跟踪部分提到的简单任务应持续监控。这包括请求数量、响应时间、令牌使用、成本和错误率。
- en: (2) Monitoring Prompts
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 监控提示
- en: Next on your list should be monitoring user-supplied prompts or inputs. Standalone
    metrics like Readability could be informative. Evaluator LLMs should be utilized
    to check for Toxicity and the like. Embedding distances from the reference prompts
    are smart metrics to include. Even if your application can handle prompts that
    are substantially different than what you anticipated, you will want to know if
    your customers’ interaction with your application is new or changes over time.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你的列表中应该包括监控用户提供的提示或输入。像可读性这样的独立指标可能会很有帮助。评估 LLM 应该用于检查毒性等问题。将嵌入距离与参考提示进行比较是明智的指标。即使你的应用程序能够处理与预期大相径庭的提示，你仍然希望了解客户与应用程序的互动是否是新的或随时间变化的。
- en: 'At this point, we need to introduce a new evaluation class: adversarial attempts
    or malicious prompt injections. This is not always accounted for in the initial
    evaluation. Comparing against reference sets of known adversarial prompts can
    flag bad actors. Evaluator LLMs can also classify prompts as malicious or not.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们需要引入一个新的评估类别：对抗性尝试或恶意提示注入。这在初步评估中并不总是被考虑到。与已知对抗性提示的参考集进行比较可以标记出恶意行为者。评估
    LLM 也可以将提示分类为恶意或非恶意。
- en: (3) Monitoring Responses
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 监控响应
- en: There are a variety of useful checks to implement when comparing what your LLM
    application is spitting out to what you expect. Consider relevance. Is your LLM
    responding with relevant content or is it off in the weeds (hallucination)? Are
    you seeing a divergence from your anticipated topics? How about sentiment? Is
    your LLM responding in the right tone and is this changing over time?
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在将你的 LLM 应用程序产生的内容与预期结果进行比较时，有许多有用的检查可以实施。考虑相关性。你的 LLM 是否在响应相关内容，还是在胡言乱语（幻想）？你是否看到与预期主题的偏离？情感如何？你的
    LLM 是否以正确的语调回应，且这种情况是否随着时间变化？
- en: You probably don’t need to monitor all these metrics on a daily basis. Monthly
    or quarterly will be sufficient for some. On the other hand, Toxicity and harmful
    output are always top on the worry list when deploying LLMs. These are examples
    of metrics that you will want to track on a more regular basis. Remember that
    the embedding visualization techniques discussed earlier may help with root cause
    analysis.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能不需要每天监控所有这些指标。每月或每季度一次对于某些指标来说就足够了。另一方面，毒性和有害输出始终是部署 LLM 时最令人担忧的问题。这些是你需要更频繁跟踪的指标。记住，之前讨论的嵌入可视化技术可能有助于根本原因分析。
- en: '[Prompt leakage](https://learnprompting.org/docs/prompt_hacking/leaking) is
    an adversarial approach we haven’t introduced yet. Prompt leakage occurs when
    someone tricks your application into divulging your stored prompts. You likely
    spent a lot of time figuring out which pre-set prompt instructions gave the best
    results. This is sensitive IP. Prompt leakage can be discovered by monitoring
    responses and comparing them to your database of prompt instructions. Embedding
    distance metrics work well.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[提示泄露](https://learnprompting.org/docs/prompt_hacking/leaking)是一种我们尚未介绍的对抗性方法。提示泄露发生在某人欺骗你的应用程序泄露你存储的提示时。你可能花了很多时间找出哪个预设提示指令效果最好。这是敏感的知识产权。提示泄露可以通过监控响应并将其与提示指令数据库进行比较来发现。嵌入距离指标效果很好。'
- en: If you have evaluation or reference datasets, you may want to periodically test
    your LLM application against these and compare the results of previous tests.
    This can give you a sense of accuracy over time and can alert you to drift. If
    you discover issues, some tools that manage embeddings allow you to export datasets
    of underperforming output so you can fine-tune your LLM on these classes of troublesome
    prompts.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有评估或参考数据集，你可能需要定期测试你的LLM应用，并比较之前测试的结果。这可以给你一个随时间变化的准确度感知，并能提醒你漂移。如果发现问题，一些管理嵌入的工具允许你导出表现不佳的输出数据集，以便你可以在这些有问题的提示类别上微调你的LLM。
- en: (4) Alerting and Thresholds
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 警报与阈值
- en: Care should be taken to ensure that your thresholds and alerts do not cause
    too many false alarms. Multivariate drift detection and alerting can help. I have
    thoughts on how to do this but will save those for another article. Incidentally,
    I didn’t see one mention of false alarm rates or best practices for thresholds
    in any of my research for this article. That’s a shame.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 应注意确保你的阈值和警报不会引发过多的虚假警报。多变量漂移检测和警报可以有所帮助。我有一些关于如何做到这一点的想法，但会留到另一篇文章中。顺便说一下，我在研究本文时没有看到关于虚假警报率或阈值最佳实践的任何提及。这真是个遗憾。
- en: There are several nice features related to alerts that you may want to include
    on your must-have list. Many monitoring systems provide integration with information
    feeds like Slack and Pager Duty. Some monitoring systems allow automatic response
    blocking if the input prompt triggers an alert. The same feature can apply to
    screening the response for PII leakage, Toxicity, and other quality metrics before
    sending it to the user.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个与警报相关的不错功能，你可能想将它们列入必备清单。许多监控系统提供与信息馈送（如 Slack 和 Pager Duty）的集成。一些监控系统允许如果输入提示触发警报，则自动响应拦截。相同的功能可以应用于在将响应发送给用户之前筛查个人信息泄露、毒性以及其他质量指标。
- en: I’ll add one more observation here as I didn’t know where else to put it. Custom
    metrics can be very important to your monitoring scheme. Your LLM application
    may be unique, or perhaps a sharp data scientist on your team thought of a metric
    that will add significant value to your approach. There will likely be advances
    in this space. You will want the flexibility of custom metrics.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我会在这里添加一个观察点，因为我不知道把它放在哪里。自定义指标对你的监控方案可能非常重要。你的LLM应用可能是独特的，或者你团队中的一位优秀数据科学家想出了一个将显著提升你方法的指标。这个领域可能会有进展。你会希望有自定义指标的灵活性。
- en: (5) The Monitoring UI
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: (5) 监控用户界面
- en: If a system has a monitoring capability, it will have a UI that shows time-series
    graphs of metrics. That’s pretty standard. UIs start to differentiate when they
    allow for drilling down into alert trends in a manner that points to some level
    of root cause analysis. Others facilitate visualization of the embedding space
    based on clusters and projections (I’d like to see, or conduct, a study on the
    usefulness of these embedding visualizations in the wild).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果系统具备监控功能，它将有一个用户界面（UI），显示度量的时间序列图。这是相当标准的。用户界面开始有所区别，当它们允许深入挖掘警报趋势，并指向某种程度的根本原因分析时。其他界面则通过基于集群和投影的嵌入空间可视化（我希望能看到或进行一项关于这些嵌入可视化在实际应用中有效性的研究）。
- en: More mature offerings will group monitoring by users, projects, and teams. They
    will have RBAC and work off the assumption that all users are on a need-to-know
    basis. Too often anyone in the tool can see everyone’s data, and that won’t fly
    at many of today’s organizations.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 更成熟的产品将按用户、项目和团队对监控进行分组。它们将具有RBAC，并基于所有用户都是需要知道的原则。过于频繁地，工具中的任何人都能看到每个人的数据，这在许多当今的组织中是不可接受的。
- en: One cause of the problem I highlighted regarding the tendency for alerts to
    yield an unacceptable false alarm rate is that the UI does not facilitate a proper
    analysis of alerts. It is rare for software systems to attempt any kind of optimization
    in this respect, but some do. Again, there is much more to say on this topic at
    a later point.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我所强调的警报产生不可接受虚假警报率的一个原因是用户界面未能促进对警报的适当分析。软件系统很少尝试在这方面进行任何优化，但有些系统会。再次提到，这个话题还有很多需要讨论的内容，稍后会再提及。
- en: '**Final Thoughts for Practitioners and Leaders**'
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**对从业者和领导者的最终思考**'
- en: Leaders, there is too much at stake to not place LLM monitoring and observability
    near the top of your organizational initiatives. I don’t say this only to prevent
    causing harm to users or losing brand reputation. Those are obviously on your
    radar. What you might not appreciate is that your company’s quick and sustainable
    adoption of AI could mean the difference between success and failure, and a mature
    [responsible AI framework](https://www.salesforceairesearch.com/trusted-ai) with
    a detailed technical roadmap for monitoring and observing LLM applications will
    provide a foundation to enable you to scale faster, better, and safer than the
    competition.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Practitioners, the concepts introduced in this article provide a list of tools,
    techniques, and metrics that should be included in the implementation of LLM observability
    and monitoring. You can use this as a guide to ensure that your monitoring system
    is up to the task. And you can use this as a basis for deeper study into each
    concept we discussed.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: This is an exciting new field. Leaders and practitioners who become well-versed
    in it will be positioned to help their teams and companies succeed in the age
    of AI.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '***About the author:***'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[*Josh Poduska*](https://www.linkedin.com/in/joshpoduska/) *is an AI Leader,
    Strategist, and Advisor with over 20 years of experience. He is the former Chief
    Field Data Scientist at Domino Data Lab and has managed teams and led data science
    strategy at multiple companies. Josh has built and implemented data science solutions
    across multiple domains. He has a Bachelor’s in Mathematics from UC Irvine and
    a Master’s in Applied Statistics from Cornell University.*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
