- en: A Pythonista’s Intro to Semantic Kernel
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《Pythonista的语义内核入门》
- en: 原文：[https://towardsdatascience.com/a-pythonistas-intro-to-semantic-kernel-af5a1a39564d?source=collection_archive---------0-----------------------#2023-09-02](https://towardsdatascience.com/a-pythonistas-intro-to-semantic-kernel-af5a1a39564d?source=collection_archive---------0-----------------------#2023-09-02)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-pythonistas-intro-to-semantic-kernel-af5a1a39564d?source=collection_archive---------0-----------------------#2023-09-02](https://towardsdatascience.com/a-pythonistas-intro-to-semantic-kernel-af5a1a39564d?source=collection_archive---------0-----------------------#2023-09-02)
- en: '[](https://medium.com/@chris.p.hughes10?source=post_page-----af5a1a39564d--------------------------------)[![Chris
    Hughes](../Images/87b16cd8677739b12294380fb00fde85.png)](https://medium.com/@chris.p.hughes10?source=post_page-----af5a1a39564d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----af5a1a39564d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----af5a1a39564d--------------------------------)
    [Chris Hughes](https://medium.com/@chris.p.hughes10?source=post_page-----af5a1a39564d--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@chris.p.hughes10?source=post_page-----af5a1a39564d--------------------------------)[![Chris
    Hughes](../Images/87b16cd8677739b12294380fb00fde85.png)](https://medium.com/@chris.p.hughes10?source=post_page-----af5a1a39564d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----af5a1a39564d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----af5a1a39564d--------------------------------)
    [Chris Hughes](https://medium.com/@chris.p.hughes10?source=post_page-----af5a1a39564d--------------------------------)'
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff13df9df155e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pythonistas-intro-to-semantic-kernel-af5a1a39564d&user=Chris+Hughes&userId=f13df9df155e&source=post_page-f13df9df155e----af5a1a39564d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----af5a1a39564d--------------------------------)
    ·30 min read·Sep 2, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Faf5a1a39564d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pythonistas-intro-to-semantic-kernel-af5a1a39564d&user=Chris+Hughes&userId=f13df9df155e&source=-----af5a1a39564d---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff13df9df155e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pythonistas-intro-to-semantic-kernel-af5a1a39564d&user=Chris+Hughes&userId=f13df9df155e&source=post_page-f13df9df155e----af5a1a39564d---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----af5a1a39564d--------------------------------)
    ·30分钟阅读·2023年9月2日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Faf5a1a39564d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pythonistas-intro-to-semantic-kernel-af5a1a39564d&user=Chris+Hughes&userId=f13df9df155e&source=-----af5a1a39564d---------------------clap_footer-----------)'
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faf5a1a39564d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pythonistas-intro-to-semantic-kernel-af5a1a39564d&source=-----af5a1a39564d---------------------bookmark_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faf5a1a39564d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pythonistas-intro-to-semantic-kernel-af5a1a39564d&source=-----af5a1a39564d---------------------bookmark_footer-----------)'
- en: Since the release of [ChatGPT](https://openai.com/blog/chatgpt), Large language
    models (LLMs) have received a huge amount of attention in both industry and the
    media; resulting in an unprecedented demand to try and leverage LLMs in almost
    every conceivable context.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 自从发布了[ChatGPT](https://openai.com/blog/chatgpt)以来，大型语言模型（LLMs）在行业和媒体中受到了极大的关注；导致了几乎在所有可以想到的背景下利用LLMs的前所未有的需求。
- en: '[Semantic Kernel](https://github.com/microsoft/semantic-kernel) is an open-source
    SDK originally developed by Microsoft to power products such as Microsoft 365
    Copilot and Bing, designed to make it easy to integrate LLMs into applications.
    It enables users to leverage LLMs to orchestrate workflows based on natural language
    queries and commands by making it possible to connect these models with external
    services that provide additional functionality the model can use to complete tasks.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[语义内核](https://github.com/microsoft/semantic-kernel)是一个开源SDK，最初由微软开发，用于支持Microsoft
    365 Copilot和Bing等产品，旨在简化将LLM集成到应用程序中的过程。它使用户能够利用LLM根据自然语言查询和命令来编排工作流程，通过将这些模型与提供附加功能的外部服务连接，使模型能够利用这些服务完成任务。'
- en: As it was created with the Microsoft ecosystem in mind, many of the complex
    examples currently available are written in C#, with fewer resources focusing
    on the Python SDK. In this blog post, I shall demonstrate how to get started with
    Semantic Kernel using Python, introducing the key components and exploring how
    these can be used to perform various tasks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它是针对 Microsoft 生态系统创建的，因此目前可用的许多复杂示例都是用 C# 编写的，关注 Python SDK 的资源较少。在这篇博客文章中，我将展示如何使用
    Python 入门 Semantic Kernel，介绍关键组件，并探索如何利用这些组件执行各种任务。
- en: 'In this article, what we shall cover includes the following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将涵盖以下内容：
- en: '[The Kernel](#05b4)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[内核](#05b4)'
- en: '[Connectors](#e4fc)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[连接器](#e4fc)'
- en: '[Prompt Functions](#8f79)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[提示函数](#8f79)'
- en: '- [Creating a custom connector](#e102)'
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- [创建自定义连接器](#e102)'
- en: '[Using a Chat Service](#224f)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用聊天服务](#224f)'
- en: '- [Making a simple chatbot](#2fc5)'
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- [制作一个简单的聊天机器人](#2fc5)'
- en: '[Memory](#e201)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[内存](#e201)'
- en: '- [Using a text embedding service](#0c36)'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- [使用文本嵌入服务](#0c36)'
- en: '- [Integrating memory into context](#0a3b)'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- [将内存集成到上下文中](#0a3b)'
- en: '[Plugins](#360e)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[插件](#360e)'
- en: '- [Using out-of-the-box plugins](#920c)'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- [使用现成插件](#920c)'
- en: '- [Creating custom plugins](#28e8)'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- [创建自定义插件](#28e8)'
- en: '- [Chaining multiple plugins](#6f42)'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- [链式调用多个插件](#6f42)'
- en: '[Orchestrating workflows with a planner](#d6ab)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用规划器编排工作流](#d6ab)'
- en: '**Disclaimer:** Semantic Kernel, like everything related to related to LLMs,
    is moving incredibly fast. As such, interfaces may change slightly over time;
    I will try to keep this post updated where I can.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**免责声明：** 由于 Semantic Kernel 和所有与 LLM 相关的内容一样，发展非常迅速。因此，接口可能会随着时间的推移稍有变化；我会尽量保持这篇文章的更新。'
- en: Whilst I work for Microsoft, I am not asked to, or compensated for, promoting
    Semantic Kernel in any way. In [Industry Solutions Engineering (ISE)](https://playbook.microsoft.com/code-with-engineering/ISE/),
    we pride ourselves on using what we feel are the best tools for the job depending
    on the situation and the customer that we are working with. In cases that we choose
    not to use Microsoft products, we provide detailed feedback to the product teams
    on the reasons why, and the areas where we feel things are missing or could be
    improved; this feedback loop usually results in Microsoft products being well
    suited for our needs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我在 Microsoft 工作，但我并没有被要求或获得任何补偿来推广 Semantic Kernel。在 [行业解决方案工程 (ISE)](https://playbook.microsoft.com/code-with-engineering/ISE/)
    中，我们以根据情况和所服务的客户选择我们认为最佳的工具为荣。在我们选择不使用 Microsoft 产品的情况下，我们会向产品团队提供详细的反馈，说明原因以及我们认为缺失或可以改进的地方；这种反馈循环通常会导致
    Microsoft 产品更好地满足我们的需求。
- en: Here, I am choosing to promote Semantic Kernel because, despite a few rough
    edges here and there, I believe that it shows great promise, and I prefer the
    design choices made by Semantic Kernel compared to some of the other solutions
    I’ve explored.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我选择推广 Semantic Kernel，因为尽管有一些小瑕疵，我相信它显示了很大的潜力，并且我更喜欢 Semantic Kernel 在设计选择上的表现，相比于我所探索的其他一些解决方案。
- en: 'The packages used at the time of writing were:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 撰写时使用的软件包包括：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '***Tl;dr:*** *If you just want to see some working code that you can use directly,
    all of the code required to replicate this post is available as a notebook* [*here.*](https://gist.github.com/Chris-hughes10/6dacd205f1da3cc3aec4fc45e57fb0b6)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '***简而言之：*** *如果你只是想看看一些可以直接使用的工作代码，所有复制此文章所需的代码都可以在这里的笔记本中找到* [*这里。*](https://gist.github.com/Chris-hughes10/6dacd205f1da3cc3aec4fc45e57fb0b6)'
- en: Acknowledgements
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢
- en: I’d like to thank my colleague [Karol Zak](https://medium.com/u/c92a3c46e12c?source=post_page-----af5a1a39564d--------------------------------),
    for collaborating with me on exploring how to get the most out of Semantic Kernel
    for our use cases, and providing code which inspired some of the examples in this
    post!
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我想感谢我的同事 [Karol Zak](https://medium.com/u/c92a3c46e12c?source=post_page-----af5a1a39564d--------------------------------)，感谢他与我合作探索如何在我们的用例中充分利用
    Semantic Kernel，并提供了一些启发本文中示例的代码！
- en: '![](../Images/e28a03d56de945f92b52f6c062791732.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e28a03d56de945f92b52f6c062791732.png)'
- en: 'An overview of Semantic Kernel’s components. Image from: [https://learn.microsoft.com/en-us/semantic-kernel/media/kernel-flow.png](https://learn.microsoft.com/en-us/semantic-kernel/media/kernel-flow.png)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Semantic Kernel 组件概述。图片来源：[https://learn.microsoft.com/en-us/semantic-kernel/media/kernel-flow.png](https://learn.microsoft.com/en-us/semantic-kernel/media/kernel-flow.png)
- en: Now, let’s begin with the central component of the library.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们从库的核心组件开始。
- en: The Kernel
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内核
- en: '*Kernel: “The core, center, or essence of an object or system.” —* [*Wiktionary*](https://en.wiktionary.org/wiki/kernel)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*核心： “对象或系统的核心、中心或本质。” —* [*维基词典*](https://en.wiktionary.org/wiki/kernel)'
- en: One of the key concepts in Semantic Kernel is the kernel itself, which is the
    main object that we will use to orchestrate our LLM based workflows. Initially,
    the kernel has very limited functionality; all of its features are largely powered
    by external components that we will connect to. The kernel then acts as a processing
    engine that fulfils a request by invoking appropriate components to complete the
    given task.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 语义内核中的一个关键概念就是内核本身，它是我们用来协调基于LLM的工作流的主要对象。最初，内核的功能非常有限；它的所有功能主要由我们将要连接的外部组件提供。然后，内核作为一个处理引擎，通过调用适当的组件来完成给定的任务。
- en: 'We can create a kernel as demonstrated below:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按照下面的示例创建一个内核：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Connectors
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连接器
- en: To make our kernel useful, we need to connect one or more AI models, which enable
    us to use our kernel to understand and generate natural language; this is done
    using a *connector*. Semantic Kernel provides out-of-the-box connectors that make
    it easy to add AI models from different sources, such as [OpenAI](https://platform.openai.com/overview),
    [Azure OpenAI](https://azure.microsoft.com/en-us/products/ai-services/openai-service),
    and [Hugging Face](https://huggingface.co/docs/hub/index). These models are then
    used to provide a *service* to the kernel.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们的内核有用，我们需要连接一个或多个AI模型，这使我们能够利用内核来理解和生成自然语言；这通过*连接器*来完成。语义内核提供了开箱即用的连接器，使得从不同来源添加AI模型变得简单，例如
    [OpenAI](https://platform.openai.com/overview)、[Azure OpenAI](https://azure.microsoft.com/en-us/products/ai-services/openai-service)
    和 [Hugging Face](https://huggingface.co/docs/hub/index)。这些模型随后用于向内核提供*服务*。
- en: 'At the time of writing, the following services are supported:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，支持以下服务：
- en: '**text completion service**: used to generate natural language'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本生成服务**：用于生成自然语言'
- en: '**chat service**: used to create a conversational experience'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聊天服务**：用于创建对话体验'
- en: '**text embedding generation service**: used to encode natural language into
    embeddings'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本嵌入生成服务**：用于将自然语言编码为嵌入'
- en: Each type of service can support multiple models from different sources at the
    same time, making it possible to switch between different models, depending on
    the task and the preference of the user. If no specific service or model is specified,
    the kernel will default to the first service and model that was defined.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 每种类型的服务可以同时支持来自不同来源的多个模型，这使得可以根据任务和用户的偏好在不同模型之间切换。如果没有指定特定的服务或模型，内核将默认为定义的第一个服务和模型。
- en: 'We can see all of the currently registered services using the following attribute:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下属性查看所有当前注册的服务：
- en: '![](../Images/aab6d6be7e5607c8e2dd48122a6d7279.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aab6d6be7e5607c8e2dd48122a6d7279.png)'
- en: As expected, we don’t currently have any connected services! Let’s change that.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，我们当前没有任何连接的服务！让我们来改变这一点。
- en: Here, I will start by accessing a [GPT3.5-turbo model](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#gpt-35)
    which I deployed using the [Azure OpenAI service](http://127.0.0.1:8800/files/notebooks/(https%3A/azure.microsoft.com/en-us/products/ai-services/openai-service)?_xsrf=2%7Cdf00fee6%7Cc6c752f68f76d91ba394976c9852cc67%7C1693063310)
    in my [Azure subscription](https://azure.microsoft.com/en-gb/free/search/?ef_id=_k_2411806e795914439019e49fb1bde4ba_k_&OCID=AIDcmm3bvqzxp1_SEM__k_2411806e795914439019e49fb1bde4ba_k_&msclkid=2411806e795914439019e49fb1bde4ba).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我将开始访问一个 [GPT3.5-turbo模型](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#gpt-35)，这是我通过
    [Azure OpenAI服务](http://127.0.0.1:8800/files/notebooks/(https%3A/azure.microsoft.com/en-us/products/ai-services/openai-service)?_xsrf=2%7Cdf00fee6%7Cc6c752f68f76d91ba394976c9852cc67%7C1693063310)
    在我的 [Azure订阅](https://azure.microsoft.com/en-gb/free/search/?ef_id=_k_2411806e795914439019e49fb1bde4ba_k_&OCID=AIDcmm3bvqzxp1_SEM__k_2411806e795914439019e49fb1bde4ba_k_&msclkid=2411806e795914439019e49fb1bde4ba)
    中部署的。
- en: As this model can be used for both text completion and chat, I will register
    using both services.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个模型可以用于文本生成和聊天，我将同时注册这两项服务。
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We can now see that the chat service has been registered as both a text completion
    and a chat completion service.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以看到，聊天服务已注册为文本生成和聊天生成服务。
- en: '![](../Images/b5994699626278686d707033f96a639e.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5994699626278686d707033f96a639e.png)'
- en: To use the non-Azure OpenAI API, the only change we would have to make is to
    use the `OpenAITextCompletion` and `OpenAIChatCompletion`connectors instead of
    our Azure classes. Don't worry if you don't have access to OpenAI models, we will
    look at how to connect to open source models a little later; the choice of model
    won't affect any of the next steps.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用非Azure OpenAI API，我们唯一需要更改的是使用`OpenAITextCompletion`和`OpenAIChatCompletion`连接器，而不是我们的Azure类。如果你没有访问OpenAI模型的权限，也不用担心，我们稍后会看看如何连接到开源模型；模型的选择不会影响后续步骤。
- en: To retrieve a service after we have registered it, we can use the following
    methods on the kernel.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 要在注册服务后检索服务，我们可以使用内核上的以下方法。
- en: '![](../Images/de5652a97fad9c676dc68815e59c43de.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de5652a97fad9c676dc68815e59c43de.png)'
- en: Now that we have registered some services, let’s explore how we can interact
    with them!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经注册了一些服务，让我们探索一下如何与它们交互！
- en: Prompt functions
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示函数
- en: The way to interact with a LLM through Semantic Kernel is to create a *Prompt
    Function*. A prompt function expects a natural language input and uses an LLM
    to interpret what is being asked, then act accordingly to return an appropriate
    response. For example, a prompt function could be used for tasks such as text
    generation, summarization, sentiment analysis, and question answering.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 通过语义内核与LLM交互的方式是创建一个*提示函数*。提示函数期望自然语言输入，并使用LLM来解释所要求的内容，然后采取相应的行动以返回合适的响应。例如，提示函数可用于文本生成、摘要、情感分析和问答等任务。
- en: 'In Semantic Kernel, a semantic function is composed of two components:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在语义内核中，语义功能由两个组件组成：
- en: '**Prompt Template**: the natural language query or command that will be sent
    to the LLM'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示模板**：将发送给LLM的自然语言查询或命令'
- en: '**Execution config**: contains the settings and options for the prompt function,
    such as the service that it should use, the parameters it should expect, and the
    description of what the function does.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**执行配置**：包含提示功能的设置和选项，例如应该使用的服务、期望的参数以及功能描述。'
- en: The simplest way to get started is by using the kernel’s `create_function_from_prompt`
    method, which accepts a prompt and execution config, as well as some identifiers
    to help keep track of the function in the kernel.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的入门方法是使用内核的`create_function_from_prompt`方法，该方法接受提示和执行配置，以及一些标识符以帮助跟踪内核中的函数。
- en: 'To illustrate this, let’s create a simple prompt:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，让我们创建一个简单的提示：
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, we have used the `{{$}}` syntax to represent an argument that will be
    injected into our prompt. Whilst we shall see many more examples of this throughout
    this post, a comprehensive guide to templating syntax can be found [in the documentation](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/prompt-template-syntax).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了`{{$}}`语法来表示将注入到提示中的参数。虽然我们将在整个帖子中看到更多的示例，但有关模板语法的全面指南可以在[文档中](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/prompt-template-syntax)找到。
- en: Next, we need to create an execution config. If we know the type of service
    that we want to use to execute our function, we can import the corresponding config
    class and create an instance of this, as demonstrated below.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要创建一个执行配置。如果我们知道要使用哪种服务来执行我们的函数，可以导入相应的配置类并创建其实例，如下所示。
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Whilst this works, it does couple our function to a certain type of service,
    which limits our flexibility. An alternative approach is to retrieve the corresponding
    configuration class directly from the service we intend to use, as demonstrated
    below.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这样做有效，但它将我们的函数与某种类型的服务耦合，限制了我们的灵活性。另一种方法是直接从我们打算使用的服务中检索相应的配置类，如下所示。
- en: '![](../Images/94340339cbd93685463d88e00fc7e2b5.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94340339cbd93685463d88e00fc7e2b5.png)'
- en: This way, we can select the service we wish to use at runtime, and automatically
    load an appropriate config object. Let’s us this approach to create our execution
    config.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们可以在运行时选择我们希望使用的服务，并自动加载合适的配置对象。让我们使用这种方法来创建我们的执行配置。
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now, we can create our function!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以创建我们的函数了！
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now, we can use our function using the kernel’s `invoke` method. As many of
    our connected services are likely to be calling external APIs, `invoke` is an
    asyncronous method, based on [Asyncio](https://docs.python.org/3/library/asyncio.html).
    This enables us to execute multiple calls to external services simultaneously,
    without waiting for a response for each one.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用内核的`invoke`方法来调用我们的函数。由于我们连接的许多服务可能会调用外部API，`invoke`是一个异步方法，基于[Asyncio](https://docs.python.org/3/library/asyncio.html)。这使我们能够同时执行多个外部服务调用，而无需为每个调用等待响应。
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The response object contains valuable information about our function call, such
    as the parameters that were used; provided everything worked as expected, we can
    access our result using the `str` constructor on the object.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 响应对象包含有关我们的函数调用的有价值信息，例如所使用的参数；如果一切按预期工作，我们可以使用对象上的`str`构造函数来访问结果。
- en: '![](../Images/a53588cf283ee939231eaf251f4f921e.png)![](../Images/24397236996fd8bd8efe17ad066bed69.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a53588cf283ee939231eaf251f4f921e.png)![](../Images/24397236996fd8bd8efe17ad066bed69.png)'
- en: Here, we can see that our function has worked!
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到我们的函数已经成功工作！
- en: Using Local Models
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用本地模型
- en: In addition to using models behind APIs, we can also use the kernel to orchestrate
    calls to local models. To illustrate this, let’s register another text completion
    service, and create a config which enables us to specify that we would like to
    use our new service. For our second completion service, let’s use a model from
    the [Hugging Face transformers library](https://huggingface.co/docs/transformers/index).
    To do this, we use the `HuggingFaceTextCompletion` connector.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用API背后的模型外，我们还可以使用内核来协调对本地模型的调用。为了说明这一点，让我们注册另一个文本完成服务，并创建一个配置，使我们能够指定我们希望使用新的服务。对于我们的第二个完成服务，我们使用来自[Hugging
    Face transformers library](https://huggingface.co/docs/transformers/index)的模型。为此，我们使用`HuggingFaceTextCompletion`连接器。
- en: Here, as we will be running the model locally, I have selected [OPT-350m](https://huggingface.co/facebook/opt-350m),
    a older model aiming to roughly match the performance of GPT-3, which should be
    able to run quickly easily on most hardware.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，由于我们将本地运行模型，我选择了[OPT-350m](https://huggingface.co/facebook/opt-350m)，这是一个较旧的模型，旨在大致匹配GPT-3的性能，应该能够在大多数硬件上快速轻松地运行。
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now, let’s create our config object. We can do this in a similar way before,
    but this time passing the `service_id` associated with our Hugging Face service.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建我们的配置对象。我们可以以类似的方式进行，但这次需要传递与我们Hugging Face服务相关的`service_id`。
- en: '[PRE9]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We can now create and execute our function as we saw earlier.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以创建并执行我们的函数，就像我们之前看到的那样。
- en: '[PRE10]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/123471b0361323dd4b855faa403f1c0b.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/123471b0361323dd4b855faa403f1c0b.png)'
- en: Well, the generation seems to have worked, but it is arguably not as good as
    the response provided by GPT3.5\. This is not unexpected, as this is an older
    model! Interestingly, we can see that, before it reached its max token limit,
    it started answering a similar pattern about Berlin; this behaviour is not unexpected
    when dealing with text completion models.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，生成似乎已经成功，但可以说效果不如GPT-3.5所提供的响应。这并不意外，因为这是一个较旧的模型！有趣的是，我们可以看到，在达到最大令牌限制之前，它开始以类似的模式回答关于柏林的问题；这种行为在处理文本完成模型时并不意外。
- en: Creating a custom connector
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建自定义连接器
- en: Now that we have seen how to create a semantic function and specify which service
    we would like our function to use. However, until this point, all of the services
    we have used have relied on out-of-the-box connectors. In some cases, we may wish
    to use a model from a different library to those currently supported, for which
    we will need a custom connector. Let’s look at how we can do this.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到如何创建语义函数并指定我们希望我们的函数使用哪个服务。然而，直到这一点为止，我们使用的所有服务都依赖于现成的连接器。在某些情况下，我们可能希望使用来自不同库的模型，而不是当前支持的模型，这时我们需要一个自定义连接器。让我们来看看如何实现这一点。
- en: As an example, let’s use a transformer model from the [curated transformers](https://github.com/explosion/curated-transformers)
    library.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以使用来自[curated transformers](https://github.com/explosion/curated-transformers)库的变换器模型。
- en: To create a custom connector, we need to subclass `TextCompletionClientBase`,
    which acts as a thin wrapper around our model. A simple example of how to do this
    is provided below.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建自定义连接器，我们需要继承`TextCompletionClientBase`，它作为我们模型的一个轻量级封装。下面提供了一个简单的示例。
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now, we can register our connector and create a semantic function as demonstrated
    before. Here, I am using the [Falcon-7B model](https://huggingface.co/tiiuae/falcon-7b),
    which will require a GPU to run in a reasonable amount of time. Here, I used a
    [Nvidia A100](https://learn.microsoft.com/en-us/azure/virtual-machines/nda100-v4-series)
    on an Azure virtual machine, as running it locally was too slow.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以注册我们的连接器并创建一个语义函数，如前所示。这里，我使用了[Falcon-7B 模型](https://huggingface.co/tiiuae/falcon-7b)，这需要一个
    GPU 才能在合理的时间内运行。在这里，我使用了[英伟达 A100](https://learn.microsoft.com/en-us/azure/virtual-machines/nda100-v4-series)在
    Azure 虚拟机上运行，因为在本地运行太慢了。
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/1ab8f519e295a3ddd66f265930d493ea.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ab8f519e295a3ddd66f265930d493ea.png)'
- en: Once again, we can see that the generation has worked, but it quickly descends
    into repetition after it has answered our question.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们可以看到生成的内容有效，但在回答了我们的问题后很快陷入了重复。
- en: A likely reason for this is the model that we have selected. Commonly, autoregressive
    transformer models are trained to predict the next word on a large corpus of text;
    essentially making them powerful autocomplete machines! Here, it appears that
    it has tried to ‘complete’ our question, which has resulted in it continuing to
    generate text, which isn’t helpful for us.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这很可能是由于我们选择的模型。通常，自回归变换器模型被训练以预测大量文本中的下一个单词；从本质上讲，使其成为强大的自动补全机器！在这里，它似乎试图‘完成’我们的问题，这导致它继续生成文本，这对我们并没有帮助。
- en: Using a Chat Service
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用聊天服务
- en: Some LLM models have undergone additional training, to make them more useful
    to interact with. An example of this process is detailed in OpenAI’s [InstructGPT](https://arxiv.org/abs/2203.02155)
    paper.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一些 LLM 模型经过了额外的训练，使其在交互中更有用。OpenAI 的[InstructGPT](https://arxiv.org/abs/2203.02155)论文详细介绍了这一过程的一个例子。
- en: At a high level, this usually involves adding one or more supervised finetuning
    steps where, instead of random unstructured text, the model is trained on curated
    examples of tasks such as question answering and summarisation; these models are
    usually known as *instruction-tuned* or *chat* models.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，这通常涉及添加一个或多个监督微调步骤，在这些步骤中，模型不是在随机的非结构化文本上训练，而是在策划的任务示例上进行训练，例如问答和总结；这些模型通常被称为*指令调优*或*聊天*模型。
- en: As we already observed how base LLMs can generate more text than we need, let’s
    investigate whether a chat model will perform differently. To use our chat model,
    we need to update our config to specify an appropriate service and create a new
    function; we shall use `azure_gpt35_chat_completion` in our case.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经观察到基础 LLM 可以生成比我们需要的更多的文本，因此让我们调查聊天模型是否表现不同。要使用我们的聊天模型，我们需要更新配置以指定适当的服务并创建一个新函数；在我们的情况下，我们将使用`azure_gpt35_chat_completion`。
- en: '![](../Images/eb8e7f97e4af91fddf18fc2c4412a3d2.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb8e7f97e4af91fddf18fc2c4412a3d2.png)'
- en: '[PRE13]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/c35c56b3d16c3a36404774a81543bf26.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c35c56b3d16c3a36404774a81543bf26.png)'
- en: Excellent, we can see that the chat model has given us a much more concise answer!
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，我们可以看到聊天模型给出了一个更简洁的回答！
- en: Previously, as we were using text completion models, we had formatted our prompt
    as a sentence for the model to complete. However, the instruction tuned models
    *should* be able to understand a question, so we may be able to change our prompt
    to make it a little more flexible. Let’s see how we can adjust our prompt with
    the aim of interacting with the model as if it was a chatbot designed to provide
    us information about places that we may like to visit.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，由于我们使用的是文本补全模型，我们将提示格式化为模型要完成的句子。然而，经过指令调优的模型*应该*能够理解问题，因此我们可能能够调整提示，使其更加灵活。让我们看看如何调整我们的提示，以便与模型进行互动，就像它是一个设计来提供有关我们可能喜欢访问的地方的信息的聊天机器人。
- en: First, let’s adjust our function config to make our prompt more generic.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们调整我们的函数配置，使提示更加通用。
- en: '[PRE14]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here, we can see that we are only passing in the user input, so we must phrase
    our input as a question. Let’s try this.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到我们仅传递了用户输入，因此我们必须将输入表达为问题。让我们尝试一下。
- en: '[PRE15]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](../Images/a398bf72d90a1e98ee11afc2e6f4968f.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a398bf72d90a1e98ee11afc2e6f4968f.png)'
- en: Great, that seems like it has worked. Let’s try asking a follow up question.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，这似乎有效。让我们尝试提出一个后续问题。
- en: '![](../Images/ace2e9eef5a2f4b8636a1f5efde9aa59.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ace2e9eef5a2f4b8636a1f5efde9aa59.png)'
- en: We can see that the model has provided a very generic answer, which doesn’t
    take into account our previous question at all. This is expected, as the prompt
    that the model recieved was `"What are some interesting things to do there?"`,
    we didn't provide any context on where 'there' is!
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '我们可以看到模型提供了一个非常通用的回答，这完全没有考虑到我们之前的问题。这是可以预期的，因为模型收到的提示是`"What are some interesting
    things to do there?"`，我们没有提供“那里”指的是哪里！ '
- en: Let’s see how we can extend our approach to make a simple chatbot in the following
    section.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在接下来的部分中扩展我们的方法，制作一个简单的聊天机器人。
- en: Making a simple Chatbot
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 制作一个简单的聊天机器人
- en: Now that we have seen how we can use a chat service, let’s explore how we can
    create a simple chatbot.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何使用聊天服务，让我们探索如何创建一个简单的聊天机器人。
- en: 'Our chatbot should be able to do three things:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的聊天机器人应该能够做三件事：
- en: Know it’s purpose and inform us of this
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 知道它的目的并告知我们
- en: Understand the current conversation context
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解当前对话的上下文
- en: Reply to our questions
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回复我们的提问
- en: Let’s adjust our prompt to reflect this.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们调整我们的提示以反映这一点。
- en: '[PRE16]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Notice that we have added the variable `history` which will be used to provide
    previous context to the chatbot. Whilst this is quite a naive approach, as long
    conversations will quickly cause the prompt to reach the model's maximum context
    length, it should work for our purposes.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们添加了变量`history`，它将用于向聊天机器人提供以前的上下文。虽然这是一种相当幼稚的方法，因为长时间的对话会很快导致提示达到模型的最大上下文长度，但对于我们的目的来说应该有效。
- en: So far, we have only used prompts which use a single variable. To use multiple
    variables we need adjust our config, as demonstrated below, by creating a `PromptTemplateConfig`;
    which defines which inputs we are expecting.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只使用了使用单个变量的提示。要使用多个变量，我们需要调整我们的配置，如下所示，通过创建一个`PromptTemplateConfig`；它定义了我们期望的输入。
- en: '[PRE17]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now, let’s use this updated config and prompt to create our chatbot
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用这个更新后的配置和提示来创建我们的聊天机器人
- en: '[PRE18]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: To keep track of the history to include in our prompt, we can use a `ChatHistory`
    object. Let's create a new instance of this.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟踪要包含在提示中的历史记录，我们可以使用`ChatHistory`对象。让我们创建一个新的实例。
- en: '[PRE19]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Additionally, to pass multiple arguments to our prompt, we can use the `KernelArguments`,
    so that we are only passing a single parameter to `invoke`; which contains all
    arguments.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了将多个参数传递给我们的提示，我们可以使用`KernelArguments`，这样我们只需将一个参数传递给`invoke`；这个参数包含所有的参数。
- en: We can see how to do this by creating a simple chat function, to update our
    history after each interaction.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过创建一个简单的聊天功能来实现这一点，以在每次互动后更新我们的历史记录。
- en: '[PRE20]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Let’s try it out!
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试看！
- en: '![](../Images/792c0f9fcef0e077c9628a9b23b6f4c7.png)![](../Images/34145621cec35d2f67386bfb0313eafc.png)![](../Images/61f2e36ac018128cd7a049b1555c57ce.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/792c0f9fcef0e077c9628a9b23b6f4c7.png)![](../Images/34145621cec35d2f67386bfb0313eafc.png)![](../Images/61f2e36ac018128cd7a049b1555c57ce.png)'
- en: Here, we can see that this has fulfilled our requirements quite well!
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到这满足了我们的要求！
- en: Inspecting our prompt, we can see that our history is being rendered into a
    format which has the option of including additional metadata. Whilst this may
    be a useful implementation detail, it is likely that we don’t want our prompt
    formatted this way!
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 检查我们的提示，我们可以看到历史记录被渲染成一个可以包含额外元数据的格式。虽然这可能是一个有用的实现细节，但我们很可能不希望我们的提示以这种方式格式化！
- en: When using a library such as semantic kernel, it is important to be able to
    verify exactly what is being passed into the model, as the way that a prompt is
    written and formatted can have a big impact on the result.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用如语义内核这样的库时，能够准确验证传递给模型的内容非常重要，因为提示的书写和格式化方式对结果有很大影响。
- en: Most language models, such as the OpenAI APIs, do not take a single prompt as
    an input, but prefer inputs formatted as a list of messages; alternating between
    the user and the model. We can inspect how our prompt will be broken down into
    messages below.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数语言模型，例如OpenAI API，不接受单个提示作为输入，而是更喜欢格式化为消息列表的输入；在用户和模型之间交替。我们可以检查我们的提示将如何被拆分成消息。
- en: '![](../Images/1dedd9d10b61fa6508b4dd463bc76e50.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1dedd9d10b61fa6508b4dd463bc76e50.png)'
- en: Here, we can see that all of the formatting associated with the chat history
    has been removed, and the messages appear how we would expect.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到与聊天历史相关的所有格式都已被移除，消息看起来符合我们的预期。
- en: Memory
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 记忆
- en: When interacting with our chatbot, one of the key aspects that made the experience
    feel like a useful interaction was that the chatbot was able to retain the context
    of our previous questions. We did this by giving the chatbot access to *memory*,
    leveraging `ChatHistory` to handle this for us.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在与我们的聊天机器人互动时，使体验感觉像有用互动的关键因素之一是聊天机器人能够保留我们之前问题的上下文。我们通过让聊天机器人访问*记忆*，利用`ChatHistory`来处理这一点。
- en: Whilst this worked well enough for our simple use case, all of our conversation
    history was stored in our system’s RAM and not persisted anywhere; once we shut
    down our system, this is gone forever. For more intelligent applications, it can
    be useful to be able to build and persist both short and long term memory for
    our models to access.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这对于我们简单的用例效果很好，但我们所有的对话历史都存储在系统的RAM中，并未持久化；一旦我们关闭系统，这些数据将永远消失。对于更智能的应用程序，能够构建和持久化短期和长期记忆以供模型访问是很有用的。
- en: Additionally, in our example, we were feeding *all* of our previous interactions
    into our prompt. As models usually have a fixed size context window — which determines
    how long our prompts can be — this will quickly break down if we start to have
    lengthy conversations. One way to avoid this is to store our memory as separate
    ‘chunks’ and only load information that we think may be relevant into our prompt.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在我们的示例中，我们将*所有*之前的互动内容都输入到我们的提示中。由于模型通常具有固定大小的上下文窗口——这决定了我们的提示可以有多长——如果我们开始进行长时间的对话，这将很快崩溃。避免这种情况的一种方法是将记忆存储为独立的“块”，并仅将我们认为可能相关的信息加载到提示中。
- en: Semantic Kernel offers some functionality around how we can incorporate memory
    into our applications, so let’s explore how we can leverage these.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 语义内核提供了一些关于如何将记忆融入应用程序的功能，所以让我们探索一下如何利用这些功能。
- en: As an example, let’s extend our chatbot so that it has access to some information
    that is stored in memory.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们扩展我们的聊天机器人，使其能够访问存储在记忆中的一些信息。
- en: First, we need some information that may be relevant to our chatbot. Whilst
    we could manually resarch and curate relevant information, it is quicker to have
    the model generate some for us! Let’s get the model to generate some facts about
    the city of London. We can do this as follows.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要一些可能与我们的聊天机器人相关的信息。虽然我们可以手动研究和整理相关信息，但让模型为我们生成信息会更快！让我们让模型生成一些关于伦敦市的事实。我们可以按如下方式进行：
- en: '[PRE21]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](../Images/fca819fa55e89cf5a9027549c79aebba.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fca819fa55e89cf5a9027549c79aebba.png)'
- en: 'Now that we have some text, so that the model can access only the parts that
    it needs, let’s divide this into chunks. Semantic kernel offers some functionality
    to do this in it’s `text_chunker` module. We can use this as demonstrated below:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一些文本，为了让模型只访问它需要的部分，让我们将其划分成几个块。语义内核提供了一些功能来实现这一点，在它的`text_chunker`模块中。我们可以按如下方式使用它：
- en: '[PRE22]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](../Images/13e57bb83384a7163ce0341532935a1e.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13e57bb83384a7163ce0341532935a1e.png)'
- en: We can see that the text has been split into 8 chunks. Depending on the text,
    we will have to adjust the maximum number of tokens specified for each chunk.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到文本被分割成了8个块。根据文本内容，我们需要调整每个块的最大标记数。
- en: Using a Text Embedding Service
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用文本嵌入服务
- en: Now that we have chunked our data, we need to create a representation of each
    chunk that enables us to calculate relevance between text; we can do this by representing
    our text as embeddings.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对数据进行了块化处理，我们需要创建每个块的表示，以便能够计算文本之间的相关性；我们可以通过将文本表示为嵌入来实现这一点。
- en: To generate embeddings, we need to add a text embedding service to our kernel.
    Similarly to before, there are various connectors that can be used, depending
    on the source of the underlying model.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成嵌入，我们需要将文本嵌入服务添加到我们的内核中。类似于之前的情况，根据基础模型的来源，有各种连接器可以使用。
- en: First, let’s use a `[text-embedding-ada-002](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#embeddings-models)`
    [model](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#embeddings-models)
    deployed in the Azure OpenAI service. This model was trained by OpenAI, and more
    information about this model can be found in their [launch blog post](https://openai.com/blog/new-and-improved-embedding-model).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们使用一个`[text-embedding-ada-002](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#embeddings-models)`
    [模型](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#embeddings-models)，该模型在Azure
    OpenAI服务中部署。这个模型由OpenAI训练，更多关于这个模型的信息可以在他们的[发布博客文章](https://openai.com/blog/new-and-improved-embedding-model)中找到。
- en: '[PRE23]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now that we have access to a model that can generate embeddings, we need somewhere
    to store these. Semantic Kernel provides the concept of a MemoryStore, which is
    an interface to various persistence providers.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以访问生成嵌入的模型，我们需要一个地方来存储这些嵌入。Semantic Kernel 提供了一个 MemoryStore 的概念，它是各种持久性提供程序的接口。
- en: For production systems, we would probably want to use a database for our persistence,
    to keep things simple for our example, we shall use in-memory storage. Let’s create
    an instance of an in-memory memory store.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生产系统，我们可能会希望使用数据库进行持久化。为了简化我们的示例，我们将使用内存存储。让我们创建一个内存中的记忆存储实例。
- en: '[PRE24]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Whilst we have used an in-memory memory store to keep things simple for our
    example, we would probably want to use a database for our persistence when building
    more complex systems. Semantic Kernel offers connectors to popular storage solutions
    such as [CosmosDB](https://learn.microsoft.com/en-us/azure/cosmos-db/introduction),
    Redis, [Postgres](https://www.postgresql.org/) and many others. As memory stores
    have a common interface, the only change that would be required would be modifying
    the connector used, which makes it easy to switch between providers.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们使用了内存中的记忆存储来简化我们的示例，但在构建更复杂的系统时，我们可能会希望使用数据库进行持久化。Semantic Kernel 提供了与流行存储解决方案的连接器，如[CosmosDB](https://learn.microsoft.com/en-us/azure/cosmos-db/introduction)、Redis、[Postgres](https://www.postgresql.org/)等。由于记忆存储具有通用接口，因此唯一需要更改的只是修改使用的连接器，这使得在提供程序之间切换变得容易。
- en: Now that we have defined our memory store, we need to generate our embeddings.
    Semantic Kernel provides *Semantic memory* data sctuctures to help with this;
    which associate a memory store with a service that can generate embeddings. Here,
    we are going to use `SemanticTextMemory`, which will enable us to embed and retrieve
    our document chunks.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了我们的记忆存储，我们需要生成嵌入。Semantic Kernel 提供了*语义记忆*数据结构来帮助实现这一点；它将记忆存储与可以生成嵌入的服务关联起来。在这里，我们将使用`SemanticTextMemory`，它将使我们能够嵌入和检索我们的文档片段。
- en: '[PRE25]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We can now save information to our memory store as follows.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以按照如下方式将信息保存到我们的记忆存储中。
- en: '[PRE26]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Here, we have created a new collection, to group similar documents.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了一个新的集合，用于分组相似的文档。
- en: 'We can now query this collection in the following way:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以以以下方式查询此集合：
- en: '[PRE27]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![](../Images/9b0a34d7f1e6ebcbaa52a0417d0a44b0.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b0a34d7f1e6ebcbaa52a0417d0a44b0.png)'
- en: Looking at the results, we can see that relevant information has been returned;
    which is reflected by the high relevance scores.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 查看结果，我们可以看到返回了相关信息；这反映在高相关性评分上。
- en: However, this was quite easy, as we have information direcly relating to what
    we being asked, using very similar language. Let’s try a more subtle query.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这其实很简单，因为我们有直接相关的信息，使用了非常相似的语言。让我们尝试一个更微妙的查询。
- en: '[PRE28]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![](../Images/4e4b62175a68c308f862a2f2d830f10c.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4e4b62175a68c308f862a2f2d830f10c.png)'
- en: Here, we can see that we have received exactly the same results. However, as
    our second result explicitly mentions ‘food from around the world’, I feel that
    this is a better match. This highlights some of the potential limitations of a
    semantic search approach.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到我们得到了完全相同的结果。然而，由于我们的第二个结果明确提到了‘来自世界各地的食物’，我认为这是更好的匹配。这突显了语义搜索方法的一些潜在局限性。
- en: Using an Open Source model
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用开源模型
- en: 'Out of interest, let’s see how an open source model compares with our OpenAI
    service in this context. We can register a [Hugging Face sentence transformer
    model](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) for this
    purpose, as demonstrated below:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 出于兴趣，让我们看看开源模型在这个上下文中与我们的 OpenAI 服务的比较。我们可以注册一个[Hugging Face 句子变换模型](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)来实现这一点，如下所示：
- en: '[PRE29]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We can now query these in the same way as before.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以以与之前相同的方式进行查询。
- en: '[PRE30]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![](../Images/cf172189efbb0a30b9b1ebc4c55663a5.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf172189efbb0a30b9b1ebc4c55663a5.png)'
- en: '[PRE31]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '![](../Images/74d509ae00eb7311f6b7bfaa22c84a27.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/74d509ae00eb7311f6b7bfaa22c84a27.png)'
- en: We can see that we have returned the same chunks, but our relevance scores are
    different. We can also observe the difference in the dimensions of the embeddings
    generated by the different models.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，我们返回了相同的片段，但相关性评分不同。我们还可以观察到不同模型生成的嵌入的维度差异。
- en: '![](../Images/6fe7aeae1fb97200b29bb8eefd1044e3.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6fe7aeae1fb97200b29bb8eefd1044e3.png)'
- en: Integrating memory into context
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将记忆集成到上下文中
- en: In our previous example, we saw that whilst we could identify broadly relevant
    information based on an embedding search, for more subtle queries we didn’t receive
    the most relevant result. Let’s explore whether we can improve upon this.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的示例中，我们看到虽然可以基于嵌入搜索识别大致相关的信息，但对于更细微的查询，我们没有得到最相关的结果。让我们探索是否可以改进这一点。
- en: One way that we could approach this is to provide the relevant information to
    our chatbot, and then let the model decide which parts are the most relevant.
    Let’s create a prompt which instructs the model to answer the question based on
    the context provided, and register a prompt function.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以采取的一种方法是将相关信息提供给我们的聊天机器人，然后让模型决定哪些部分最相关。让我们创建一个提示，指示模型根据提供的上下文回答问题，并注册一个提示功能。
- en: '[PRE32]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Now, we can use this function to answer our more subtle question. First, we
    create a context object, and add our question to this.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用这个功能来回答我们更细微的问题。首先，我们创建一个上下文对象，并将问题添加到其中。
- en: '[PRE33]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Next, we can manually perform our embedding search, and add the retrieved information
    to our context.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以手动执行嵌入搜索，并将检索到的信息添加到我们的上下文中。
- en: '[PRE34]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We create a context object, and add our question to this.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个上下文对象，并将问题添加到其中。
- en: '[PRE35]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Finally, we can execute our function.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以执行我们的功能。
- en: '[PRE36]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '![](../Images/9e1624ff5d7fe229469aaa2990655f2b.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e1624ff5d7fe229469aaa2990655f2b.png)'
- en: This time, we see that our answer has referenced the information that we are
    looking for and provided a better answer!
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，我们看到我们的答案引用了我们寻找的信息，并提供了更好的答案！
- en: Plugins
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 插件
- en: A plugin in Semantic Kernel is a group of functions that can be loaded into
    the kernel to be exposed to AI apps and services. The functions within plugins
    can then be orchestrated by the kernel to accomplish tasks.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在语义内核中，插件是一组可以加载到内核中以暴露给 AI 应用程序和服务的功能。插件中的功能可以由内核协调以完成任务。
- en: The [documentation](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/plugins)
    describes plugins as the “building blocks” of Semantic Kernel, which can be chained
    together to create complex workflows; as plugins follow the OpenAI plugin specification,
    plugins created for OpenAI services, Bing, and Microsoft 365 can be used with
    Semantic Kernel.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '[文档](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/plugins)将插件描述为语义内核的“构建块”，可以将它们链接在一起以创建复杂的工作流程；由于插件遵循
    OpenAI 插件规范，因此为 OpenAI 服务、必应和 Microsoft 365 创建的插件可以与语义内核一起使用。'
- en: 'Semantic Kernel [provides several plugins out-of-the-box](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/out-of-the-box-plugins?tabs=python),
    which include:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 语义内核 [提供了几种开箱即用的插件](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/out-of-the-box-plugins?tabs=python)，包括：
- en: '**ConversationSummaryPlugin**: To summarize a conversation'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ConversationSummaryPlugin**：用于总结对话'
- en: '**HttpPlugin**: To call APIs'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HttpPlugin**：用于调用 API'
- en: '**TextMemoryPlugin**: To store and retrieve text in memory'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TextMemoryPlugin**：用于在内存中存储和检索文本'
- en: '**TimePlugin**: To acquire the time of day and any other temporal information'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TimePlugin**：用于获取时间和任何其他时间信息'
- en: Let’s start by exploring how we can use a pre-defined plugin, before moving
    on to investigate how we can create custom plugins.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先探索如何使用预定义的插件，然后再调查如何创建自定义插件。
- en: Using an out-of-the-box plugin
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用开箱即用的插件
- en: One of the plugins included in Semantic Kernel is `TextMemoryPlugin`, which
    provides functionality to save and recall information from memory. Let's see how
    we can use this to simplify our previous example of populating our prompt context
    from memory.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 语义内核中包含的一个插件是`TextMemoryPlugin`，它提供了从内存中保存和检索信息的功能。让我们看看如何使用它来简化我们之前的示例，即从内存中填充提示上下文。
- en: First, we must import our plugin, as demonstrated below.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须导入我们的插件，如下所示。
- en: '![](../Images/f968c5e002ddf81ab69811fb823cd5a1.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f968c5e002ddf81ab69811fb823cd5a1.png)'
- en: Here, we can see that this plugin contains two semantic functions, `recall`
    and `save`.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到这个插件包含两个语义功能，`recall` 和 `save`。
- en: 'Now, let’s modify our prompt:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们修改我们的提示：
- en: '[PRE37]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: We can see that, to use the `recall` function, we can reference this in our
    prompt. Now, let's create a config and register a function.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，要使用`recall`功能，我们可以在提示中引用它。现在，让我们创建一个配置并注册一个功能。
- en: '[PRE38]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In our manual example, we were able to control aspects such as the number of
    results returned and the collection to search. When using `TextMemoryPlugin`,
    we can set these by adding them to `KernelArguments`. Let's try out our function.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的手动示例中，我们能够控制返回结果的数量和搜索的集合。使用`TextMemoryPlugin`时，我们可以通过将这些添加到`KernelArguments`来设置它们。让我们尝试一下我们的函数。
- en: '[PRE39]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '![](../Images/36ff67413f9cf314b5c000dca6c273b8.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36ff67413f9cf314b5c000dca6c273b8.png)'
- en: We can see that this is equivalent to our manual approach.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到这与我们的手动方法是等效的。
- en: Creating Custom Plugins
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建自定义插件
- en: Now that we understand how to create semantic functions, and how to use plugins,
    we have everything we need to start making our own plugins!
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了如何创建语义函数以及如何使用插件，我们拥有了一切所需来开始制作自己的插件！
- en: 'Plugins can contain two types of functions:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 插件可以包含两种类型的函数：
- en: '**Prompt functions**: use natural language to perform actions'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示函数**：使用自然语言执行操作'
- en: '**Native functions**: use Python code to perform actions'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本地函数**：使用 Python 代码执行操作'
- en: which can be combined within a single plugin.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这些可以在一个插件中结合使用。
- en: The choice of whether to use a prompt vs native function depends on the task
    that you are performing. For tasks involving understanding or generating language,
    prompt functions are the obvious choice. However, for more deterministic tasks,
    such as performing mathematical operations, downloading data or accessing the
    time, native functions are better suited.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 是否使用提示函数或本地函数取决于你所执行的任务。对于涉及理解或生成语言的任务，提示函数显然是首选。然而，对于更确定性的任务，如执行数学运算、下载数据或访问时间，本地函数更为合适。
- en: Let’s explore how we can create each type. First, let’s create a folder to store
    our plugins.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨如何创建每种类型。首先，让我们创建一个文件夹来存储我们的插件。
- en: '[PRE40]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '**Creating a Poem generator plugin**'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**创建一个诗歌生成器插件**'
- en: For our example, let’s create a plugin which generates poems; for this, using
    a prompt function seems a natural choice. We can create a folder for this plugin
    in our directory.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例，让我们创建一个生成诗歌的插件；为此，使用提示函数似乎是自然的选择。我们可以在目录中为此插件创建一个文件夹。
- en: '[PRE41]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Recalling that plugins are just a collection of functions, and we are creating
    a semantic function, the next part should be quite familiar. The key difference
    is that, instead of defining our prompt and config inline, we will create individual
    files for these; to make it easier to load.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，插件只是函数的集合，而我们正在创建一个语义函数，下一部分应该很熟悉。关键的不同点在于，我们将不再在线定义提示和配置，而是为这些创建单独的文件，以便更容易加载。
- en: Let’s create a folder for our semantic function, which we shall call `write_poem`.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为我们的语义函数创建一个文件夹，命名为`write_poem`。
- en: '[PRE42]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Next, we create our prompt, saving it as `skprompt.txt`.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建我们的提示，保存为`skprompt.txt`。
- en: '![](../Images/8f42f25e22886c6b01197b54db47e664.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f42f25e22886c6b01197b54db47e664.png)'
- en: Now, let’s create our config and store this in a json file.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建我们的配置并将其存储在 json 文件中。
- en: Whilst it is always good practice to set meaningful descriptions in our config,
    this becomes more important when we are defining plugins; plugins should provide
    clear descriptions that describe how they behave, what their inputs and outputs
    are, and what their side effects are. The reason for this is that this is the
    interface that is presented by our kernel and, if we want to be able to use an
    LLM to orchestrate tasks, it needs to be able to understand the plugin’s functionality
    and how to call it so that it can select appropriate functions.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在配置中设置有意义的描述总是一个好习惯，但当我们定义插件时，这变得更加重要；插件应提供清晰的描述，说明它们的行为、输入输出以及副作用。原因在于，这是由我们的内核呈现的接口，如果我们希望使用
    LLM 来协调任务，它需要能够理解插件的功能及如何调用它，以便选择适当的函数。
- en: '[PRE43]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now, we are able to import our plugin:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以导入我们的插件：
- en: '[PRE45]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Inspecting our plugin, we can see that it exposes our `write_poem` semantic
    function.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 检查我们的插件，我们可以看到它暴露了我们的`write_poem`语义函数。
- en: '![](../Images/afe6ac1b359eaf572171a98b54ad10a5.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/afe6ac1b359eaf572171a98b54ad10a5.png)'
- en: We can call our function using the kernel, as we have seen before.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像以前一样使用内核调用我们的函数。
- en: '[PRE46]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '![](../Images/9352eab944764efc3a05a9f3dc5ad8ab.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9352eab944764efc3a05a9f3dc5ad8ab.png)'
- en: 'or, we can use it in another semantic function:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以在另一个语义函数中使用它：
- en: '[PRE47]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '![](../Images/ce64b97153dd1ba8bf2b4f833b801fe8.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce64b97153dd1ba8bf2b4f833b801fe8.png)'
- en: '**Creating an Image Classifier plugin**'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '**创建一个图像分类器插件**'
- en: Now that we have seen how to use a prompt function in a plugin, let’s take a
    look at how we can use a native function.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了如何在插件中使用提示函数，让我们来看看如何使用原生函数。
- en: Here, let’s create a plugin that takes an image url, then downloads and classifies
    the image. Once again, let’s create a folder for our new plugin.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个插件，它接受一个图片网址，然后下载并分类图片。再一次，让我们为我们的新插件创建一个文件夹。
- en: '[PRE48]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Now, we can create our Python module. Inside the module, we can be quite flexible.
    Here, we have created a class with two methods, the key step is to use the `kernel_function`
    decorator to specify which methods should be exposed as part of the plugin.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以创建我们的Python模块。在模块内部，我们可以非常灵活。在这里，我们创建了一个具有两个方法的类，关键步骤是使用`kernel_function`装饰器来指定哪些方法应该作为插件的一部分被暴露。
- en: For our inputs, we have used the `Annotated` type hint to provide a description
    of what our argument does. More information can be found [in the documentation](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/native-functions?tabs=python).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的输入，我们使用了`Annotated`类型提示来提供我们参数的描述。更多信息可以在[文档中](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/native-functions?tabs=python)找到。
- en: '[PRE49]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: For this example, I have used the excellent [Pytorch Image Models](https://github.com/huggingface/pytorch-image-models)
    library to provide our classifier. For more information on how this library works,
    check out this [blog post](/getting-started-with-pytorch-image-models-timm-a-practitioners-guide-4e77b4bf9055).
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我使用了出色的[Pytorch Image Models](https://github.com/huggingface/pytorch-image-models)库来提供我们的分类器。有关该库如何工作的更多信息，请查看这篇[博客文章](/getting-started-with-pytorch-image-models-timm-a-practitioners-guide-4e77b4bf9055)。
- en: Now, we can simply import our plugin as seen below.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以如下面所示简单地导入我们的插件。
- en: '[PRE50]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Inspecting our plugin, we can see that only our decorated function is exposed.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 检查我们的插件，我们可以看到只有我们装饰过的函数被暴露出来。
- en: '![](../Images/da9f13185d2693a44d1a13045d64dde8.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da9f13185d2693a44d1a13045d64dde8.png)'
- en: We can verify that our plugin works using an [image of a cat from Pixabay](https://pixabay.com/photos/cat-kitten-pet-striped-young-1192026/).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用[来自Pixabay的猫的图片](https://pixabay.com/photos/cat-kitten-pet-striped-young-1192026/)来验证我们的插件是否有效。
- en: '![](../Images/e665bbc0cb413f97f5ca010e6dbf0983.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e665bbc0cb413f97f5ca010e6dbf0983.png)'
- en: '[PRE51]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '![](../Images/b594aff9671760a0edeebd60819063c5.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b594aff9671760a0edeebd60819063c5.png)'
- en: Manually calling our function, we can see that our image has been classified
    correctly! In the same way as before, we could also reference this function directly
    from a prompt. However, as we have already demonstrated this, let’s try something
    slightly different in the following section.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 通过手动调用我们的函数，我们可以看到我们的图片已被正确分类！和之前一样，我们也可以直接从提示中引用这个函数。然而，既然我们已经演示过了，让我们在下一部分尝试一些稍微不同的东西。
- en: Chaining multiple plugins
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 链接多个插件
- en: Now that we have defined a variety of functions, both inline and as plugins,
    let’s see how we can orchestrate a workflow that calls more than one function.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了各种函数，包括内联函数和插件，让我们看看如何协调一个调用多个函数的工作流程。
- en: If we would like to execute multiple functions independently, this is straightforward;
    we can simply pass a list of functions to `invoke`, as demonstrated below.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望独立执行多个函数，这很简单；我们只需将函数列表传递给`invoke`，如下所示。
- en: '[PRE52]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '![](../Images/3cb07fd67a06aef978b9e4bf0ed538ac.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3cb07fd67a06aef978b9e4bf0ed538ac.png)'
- en: Here, we can see that the same input has been used for each function. We could
    have defined different named parameter in our `KernelArguments`, but if multiple
    functions have arguments with the same name, this becomes difficult. As an aside,
    our poem generator seemed to do a great job given that it was only provided with
    a url!
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到相同的输入被用于每个函数。我们本可以在`KernelArguments`中定义不同命名的参数，但如果多个函数的参数具有相同的名称，这会变得困难。顺便提一下，我们的诗歌生成器似乎做得很棒，考虑到它只提供了一个网址！
- en: A more interesting case is when we would like to use the output from one function
    as the input to another, so let’s explore that.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 更有趣的情况是，当我们希望将一个函数的输出用作另一个函数的输入时，让我们来深入探讨一下。
- en: 'To provide more finegrained control over how functions are invoked, the kernel
    enables us to define handlers, where we can inject custom behaviour:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供对函数调用方式的更精细控制，内核使我们能够定义处理程序，在其中我们可以注入自定义行为：
- en: '`add_function_invoking_handler`: used to register handlers that are called
    before a function is called'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_function_invoking_handler`：用于注册在函数调用之前调用的处理程序'
- en: '`add_function_invoked_handler`: used to register handlers that are called after
    a function is called'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_function_invoked_handler`：用于注册在函数调用后被调用的处理程序'
- en: As we would like to update our input to the next function with the previous
    function’s output, we can define a short function to do this, and register this
    so that it is called after each function has been invoked. Let’s see how we can
    do this.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们希望将输入更新为下一个函数的输出，可以定义一个简短的函数来完成此操作，并注册它，以便在每次函数调用后调用。让我们看看如何做到这一点。
- en: First, we need to define a function which takes the kernel and an instance of
    `FunctionInvokedEventArgs`, and updates our arguments.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要定义一个函数，该函数接受内核和`FunctionInvokedEventArgs`实例，并更新我们的参数。
- en: '[PRE53]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Next, we can register this with our kernel.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以将其注册到我们的内核中。
- en: '[PRE54]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Now, we can invoke our functions as before.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以像之前一样调用我们的函数。
- en: '[PRE55]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '![](../Images/2b19673d4d16226d06cdcac56c51f830.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b19673d4d16226d06cdcac56c51f830.png)'
- en: We can see that, using both plugins sequentially, we have classified the image
    and wrote a poem about it!
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，通过顺序使用两个插件，我们已经对图像进行了分类并为其写了一首诗！
- en: Orchestrating workflows with a Planner
  id: totrans-294
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用规划器协调工作流
- en: At this point, we have thoroughly explored semantic functions, understand how
    functions can be grouped and used as part of a plugin, and have seen how we can
    chain plugins together manually. Now, let’s explore how we can create and orchestrate
    workflows using LLMs. To do this, Semantic Kernel provides *Planner* objects,
    which can dynamically create chains of functions to try and achieve a goal.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经深入探讨了语义函数，理解了函数如何被分组并用作插件的一部分，并且已经看到如何手动将插件链在一起。现在，让我们探索如何使用LLMs创建和协调工作流。为此，Semantic
    Kernel提供了*Planner*对象，它可以动态创建函数链以尝试实现目标。
- en: A planner is a class that takes a user prompt and a kernel, and uses the kernel’s
    services to create a plan of how to perform the task, using the functions and
    plugins that have been made available to the kernel. As the plugins are the main
    building blocks of these plans, the planner relies heavily on the descriptions
    provided; if plugins and functions don’t have clear descriptions, the planner
    will not be able to use them correctly. Additionally, as a planner can combine
    functions in various different ways, it is important to ensure that we only expose
    functions that we are happy for the planner to use.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 规划器是一个类，它接受用户提示和内核，并利用内核的服务创建执行任务的计划，使用内核中可用的函数和插件。由于插件是这些计划的主要构建块，因此规划器在很大程度上依赖于提供的描述；如果插件和函数没有明确的描述，规划器将无法正确使用它们。此外，由于规划器可以以各种不同的方式组合函数，因此确保仅暴露我们希望规划器使用的函数是很重要的。
- en: As the planner relies on a model to generate a plan, there can be errors introduced;
    these usually arise when the planner doesn’t properly understand how to use the
    function. In these cases, I have found that providing explicit instructions —
    such as describing the inputs and outputs, and stating whether inputs are required
    — in the descriptions can lead to better results. Additionally, I have had better
    results using instruction tuned models than base models; base text completion
    models tend to hallucinate functions that don’t exist or create multiple plans.
    Despite these limitations, when everything works correctly, planners can be incredibly
    powerful!
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 由于规划器依赖于模型来生成计划，因此可能会引入错误；这些错误通常发生在规划器未能正确理解如何使用函数时。在这些情况下，我发现提供明确的指令——如描述输入和输出，并说明输入是否为必需——可以取得更好的结果。此外，使用指令调优模型比使用基础模型效果更佳；基础文本补全模型往往会虚构不存在的函数或生成多个计划。尽管存在这些限制，当一切正常工作时，规划器可以非常强大！
- en: Let’s explore how we can do this by exploring if we can create a plan to write
    a poem about an image, based on its url; using the plugins we created earlier.
    As we have defined lots of functions that we no longer need, let’s create a new
    kernel, so we can control which functions are exposed.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过探讨是否可以创建一个基于图像url的诗歌计划，使用我们之前创建的插件，来探索如何做到这一点。由于我们定义了许多不再需要的函数，让我们创建一个新的内核，以便控制暴露的函数。
- en: '[PRE56]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: To create our plan, let’s use our OpenAI chat service.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建我们的计划，让我们使用我们的OpenAI聊天服务。
- en: '[PRE57]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Now, let’s import our plugins.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们导入我们的插件。
- en: '[PRE58]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: We can see which functions our kernel has access to as demonstrated below.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们的内核可以访问哪些函数，如下所示。
- en: '![](../Images/c38c0113d01700a4cbb9684a12fd50e6.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c38c0113d01700a4cbb9684a12fd50e6.png)'
- en: Now, let’s import our planner object.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们导入我们的规划器对象。
- en: '[PRE59]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: To use our planner, all we need is a prompt. Often, we will need to tweak this
    depending on the plans that are generated. Here, I have tried to be as explicit
    as possible about the input that is required.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用我们的规划工具，我们只需要一个提示。通常，我们需要根据生成的计划进行调整。在这里，我尽可能明确所需的输入。
- en: '[PRE60]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Next, we can use our planner to create a plan for how it will solve the task.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用我们的规划工具来制定解决任务的计划。
- en: '[PRE61]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '![](../Images/5d418bcf66576207199f693bfc3ea199.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d418bcf66576207199f693bfc3ea199.png)'
- en: Inspecting our plan, we can see that the model has correctly identified out
    input, and the correct functions to use!
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 检查我们的计划，我们可以看到模型已经正确识别了输入，以及正确的函数使用方法！
- en: Finally, all that is left to do is to execute our plan.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，只剩下执行我们的计划了。
- en: '[PRE62]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '![](../Images/c0959b816490d6da186bda29626396a9.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c0959b816490d6da186bda29626396a9.png)'
- en: Wow, it worked! For a model trained to predict the next word, that is pretty
    powerful!
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，成功了！对于一个被训练来预测下一个词的模型来说，这真是相当强大的！
- en: As a word of warning, I was quite lucky when making this example that the generated
    plan worked first time. However, we are relying on a model to correctly interpret
    our instructions, as well as understanding the tools available; not to mention
    that LLMs can hallucinate and potentially dream up new functions that don’t exist!
    For me personally, in a production system, I would feel much more comfortable
    manually creating the workflow to execute, rather than leaving it to the LLM!
    As the technology continues to improve, especially at the current rate, hopefully
    this recommendation will become outdated!
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个警告，在制作这个示例时，我很幸运地第一次生成的计划就有效。然而，我们依赖于模型正确解读我们的指令，并理解可用的工具；更不用说LLM可能会出现虚假信息，甚至可能会梦到不存在的新功能！对我个人来说，在生产系统中，我会觉得手动创建工作流程执行会更舒服，而不是依赖LLM！随着技术的不断进步，尤其是当前的速度，希望这个建议会变得过时！
- en: Conclusion
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Hopefully this has provided a good introduction to Semantic Kernel and has inspired
    you to explore using it for your own use cases.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这为您提供了对 Semantic Kernel 的良好介绍，并激励您探索将其用于自己的用例。
- en: '*All of the code required to replicate this post is available as a notebook*
    [*here*](https://gist.github.com/Chris-hughes10/6dacd205f1da3cc3aec4fc45e57fb0b6)*.*'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '*重复此帖子所需的所有代码可以在[这里](https://gist.github.com/Chris-hughes10/6dacd205f1da3cc3aec4fc45e57fb0b6)*找到。*'
- en: '[*Chris Hughes*](https://www.linkedin.com/in/chris-hughes1/) *is on LinkedIn*'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '[*Chris Hughes*](https://www.linkedin.com/in/chris-hughes1/) *在LinkedIn上*'
- en: References
  id: totrans-323
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: '[Introducing ChatGPT (openai.com)](https://openai.com/blog/chatgpt)'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[介绍 ChatGPT (openai.com)](https://openai.com/blog/chatgpt)'
- en: '[microsoft/semantic-kernel: Integrate cutting-edge LLM technology quickly and
    easily into your apps (github.com)](https://github.com/microsoft/semantic-kernel)'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[microsoft/semantic-kernel: 将最前沿的LLM技术快速、轻松地集成到您的应用程序中 (github.com)](https://github.com/microsoft/semantic-kernel)'
- en: '[Who We Are — Microsoft Solutions Playbook](https://playbook.microsoft.com/code-with-engineering/ISE/)'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[我们是谁 — Microsoft 解决方案手册](https://playbook.microsoft.com/code-with-engineering/ISE/)'
- en: '[kernel — Wiktionary, the free dictionary](https://en.wiktionary.org/wiki/kernel)'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[内核 — 维基词典，免费的词典](https://en.wiktionary.org/wiki/kernel)'
- en: '[Overview — OpenAI API](https://platform.openai.com/overview)'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[概述 — OpenAI API](https://platform.openai.com/overview)'
- en: '[Azure OpenAI Service — Advanced Language Models | Microsoft Azure](https://azure.microsoft.com/en-us/products/ai-services/openai-service)'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Azure OpenAI 服务 — 高级语言模型 | Microsoft Azure](https://azure.microsoft.com/en-us/products/ai-services/openai-service)'
- en: '[Hugging Face Hub documentation](https://huggingface.co/docs/hub/index)'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hugging Face Hub 文档](https://huggingface.co/docs/hub/index)'
- en: '[Azure OpenAI Service models — Azure OpenAI | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#gpt-35)'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Azure OpenAI 服务模型 — Azure OpenAI | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#gpt-35)'
- en: '[Create Your Azure Free Account Today | Microsoft Azure](https://azure.microsoft.com/en-gb/free/search/?ef_id=_k_2411806e795914439019e49fb1bde4ba_k_&OCID=AIDcmm3bvqzxp1_SEM__k_2411806e795914439019e49fb1bde4ba_k_&msclkid=2411806e795914439019e49fb1bde4ba)'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[今天创建您的 Azure 免费帐户 | Microsoft Azure](https://azure.microsoft.com/en-gb/free/search/?ef_id=_k_2411806e795914439019e49fb1bde4ba_k_&OCID=AIDcmm3bvqzxp1_SEM__k_2411806e795914439019e49fb1bde4ba_k_&msclkid=2411806e795914439019e49fb1bde4ba)'
- en: '[How to use prompt template language in Semantic Kernel | Microsoft Learn](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/prompt-template-syntax)'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何在 Semantic Kernel 中使用提示模板语言 | Microsoft Learn](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/prompt-template-syntax)'
- en: '[asyncio — Asynchronous I/O — Python 3.11.5 documentation](https://docs.python.org/3/library/asyncio.html)'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[asyncio — 异步 I/O — Python 3.11.5 文档](https://docs.python.org/3/library/asyncio.html)'
- en: '[🤗 Transformers (huggingface.co)](https://huggingface.co/docs/transformers/index)'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[🤗 Transformers (huggingface.co)](https://huggingface.co/docs/transformers/index)'
- en: '[gpt2 · Hugging Face](https://huggingface.co/gpt2)'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[gpt2 · Hugging Face](https://huggingface.co/gpt2)'
- en: '[explosion/curated-transformers: 🤖 A PyTorch library of curated Transformer
    models and their composable components (github.com)](https://github.com/explosion/curated-transformers)'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[explosion/curated-transformers: 🤖 PyTorch 精选 Transformer 模型及其可组合组件 (github.com)](https://github.com/explosion/curated-transformers)'
- en: '[tiiuae/falcon-7b · Hugging Face](https://huggingface.co/tiiuae/falcon-7b)'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[tiiuae/falcon-7b · Hugging Face](https://huggingface.co/tiiuae/falcon-7b)'
- en: '[ND A100 v4-series — Azure Virtual Machines | Microsoft Learn](https://learn.microsoft.com/en-us/azure/virtual-machines/nda100-v4-series)'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ND A100 v4 系列 — Azure 虚拟机 | Microsoft Learn](https://learn.microsoft.com/en-us/azure/virtual-machines/nda100-v4-series)'
- en: '[[2203.02155] Training language models to follow instructions with human feedback
    (arxiv.org)](https://arxiv.org/abs/2203.02155)'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[[2203.02155] 训练语言模型以根据人类反馈执行指令 (arxiv.org)](https://arxiv.org/abs/2203.02155)'
- en: '[Azure OpenAI Service models — Azure OpenAI | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#embeddings-models)'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Azure OpenAI 服务模型 — Azure OpenAI | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#embeddings-models)'
- en: '[New and improved embedding model (openai.com)](https://openai.com/blog/new-and-improved-embedding-model)'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[新改进的嵌入模型 (openai.com)](https://openai.com/blog/new-and-improved-embedding-model)'
- en: '[Introduction — Azure Cosmos DB | Microsoft Learn](https://learn.microsoft.com/en-us/azure/cosmos-db/introduction)'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[介绍 — Azure Cosmos DB | Microsoft Learn](https://learn.microsoft.com/en-us/azure/cosmos-db/introduction)'
- en: '[PostgreSQL: The world’s most advanced open source database](https://www.postgresql.org/)'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PostgreSQL：世界上最先进的开源数据库](https://www.postgresql.org/)'
- en: '[sentence-transformers/all-MiniLM-L6-v2 · Hugging Face](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[sentence-transformers/all-MiniLM-L6-v2 · Hugging Face](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)'
- en: '[Understanding AI plugins in Semantic Kernel and beyond | Microsoft Learn](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/plugins?tabs=Csharp)'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[理解 Semantic Kernel 及其他 AI 插件 | Microsoft Learn](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/plugins?tabs=Csharp)'
- en: '[Out-of-the-box plugins available in Semantic Kernel | Microsoft Learn](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/out-of-the-box-plugins?tabs=python)'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Semantic Kernel 中的现成插件 | Microsoft Learn](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/out-of-the-box-plugins?tabs=python)'
- en: '[How to add native code to your AI apps with Semantic Kernel | Microsoft Learn](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/native-functions?tabs=python)'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何将本地代码添加到您的 AI 应用中与 Semantic Kernel | Microsoft Learn](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/native-functions?tabs=python)'
- en: '[huggingface/pytorch-image-models: PyTorch image models, scripts, pretrained
    weights — ResNet, ResNeXT, EfficientNet, NFNet, Vision Transformer (ViT), MobileNet-V3/V2,
    RegNet, DPN, CSPNet, Swin Transformer, MaxViT, CoAtNet, ConvNeXt, and more (github.com)](https://github.com/huggingface/pytorch-image-models)'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[huggingface/pytorch-image-models: PyTorch 图像模型、脚本、预训练权重 — ResNet, ResNeXT,
    EfficientNet, NFNet, Vision Transformer (ViT), MobileNet-V3/V2, RegNet, DPN, CSPNet,
    Swin Transformer, MaxViT, CoAtNet, ConvNeXt 等 (github.com)](https://github.com/huggingface/pytorch-image-models)'
- en: '[Getting Started with PyTorch Image Models (timm): A Practitioner’s Guide |
    by Chris Hughes | Towards Data Science](/getting-started-with-pytorch-image-models-timm-a-practitioners-guide-4e77b4bf9055)'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 PyTorch 图像模型 (timm) 入门：实用指南 | by Chris Hughes | Towards Data Science](/getting-started-with-pytorch-image-models-timm-a-practitioners-guide-4e77b4bf9055)'
