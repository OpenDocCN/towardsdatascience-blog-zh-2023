- en: A Pythonista’s Intro to Semantic Kernel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-pythonistas-intro-to-semantic-kernel-af5a1a39564d?source=collection_archive---------0-----------------------#2023-09-02](https://towardsdatascience.com/a-pythonistas-intro-to-semantic-kernel-af5a1a39564d?source=collection_archive---------0-----------------------#2023-09-02)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@chris.p.hughes10?source=post_page-----af5a1a39564d--------------------------------)[![Chris
    Hughes](../Images/87b16cd8677739b12294380fb00fde85.png)](https://medium.com/@chris.p.hughes10?source=post_page-----af5a1a39564d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----af5a1a39564d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----af5a1a39564d--------------------------------)
    [Chris Hughes](https://medium.com/@chris.p.hughes10?source=post_page-----af5a1a39564d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff13df9df155e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pythonistas-intro-to-semantic-kernel-af5a1a39564d&user=Chris+Hughes&userId=f13df9df155e&source=post_page-f13df9df155e----af5a1a39564d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----af5a1a39564d--------------------------------)
    ·30 min read·Sep 2, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Faf5a1a39564d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pythonistas-intro-to-semantic-kernel-af5a1a39564d&user=Chris+Hughes&userId=f13df9df155e&source=-----af5a1a39564d---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faf5a1a39564d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pythonistas-intro-to-semantic-kernel-af5a1a39564d&source=-----af5a1a39564d---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: Since the release of [ChatGPT](https://openai.com/blog/chatgpt), Large language
    models (LLMs) have received a huge amount of attention in both industry and the
    media; resulting in an unprecedented demand to try and leverage LLMs in almost
    every conceivable context.
  prefs: []
  type: TYPE_NORMAL
- en: '[Semantic Kernel](https://github.com/microsoft/semantic-kernel) is an open-source
    SDK originally developed by Microsoft to power products such as Microsoft 365
    Copilot and Bing, designed to make it easy to integrate LLMs into applications.
    It enables users to leverage LLMs to orchestrate workflows based on natural language
    queries and commands by making it possible to connect these models with external
    services that provide additional functionality the model can use to complete tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: As it was created with the Microsoft ecosystem in mind, many of the complex
    examples currently available are written in C#, with fewer resources focusing
    on the Python SDK. In this blog post, I shall demonstrate how to get started with
    Semantic Kernel using Python, introducing the key components and exploring how
    these can be used to perform various tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, what we shall cover includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[The Kernel](#05b4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Connectors](#e4fc)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Prompt Functions](#8f79)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '- [Creating a custom connector](#e102)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Using a Chat Service](#224f)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '- [Making a simple chatbot](#2fc5)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Memory](#e201)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '- [Using a text embedding service](#0c36)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- [Integrating memory into context](#0a3b)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Plugins](#360e)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '- [Using out-of-the-box plugins](#920c)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- [Creating custom plugins](#28e8)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- [Chaining multiple plugins](#6f42)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Orchestrating workflows with a planner](#d6ab)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disclaimer:** Semantic Kernel, like everything related to related to LLMs,
    is moving incredibly fast. As such, interfaces may change slightly over time;
    I will try to keep this post updated where I can.'
  prefs: []
  type: TYPE_NORMAL
- en: Whilst I work for Microsoft, I am not asked to, or compensated for, promoting
    Semantic Kernel in any way. In [Industry Solutions Engineering (ISE)](https://playbook.microsoft.com/code-with-engineering/ISE/),
    we pride ourselves on using what we feel are the best tools for the job depending
    on the situation and the customer that we are working with. In cases that we choose
    not to use Microsoft products, we provide detailed feedback to the product teams
    on the reasons why, and the areas where we feel things are missing or could be
    improved; this feedback loop usually results in Microsoft products being well
    suited for our needs.
  prefs: []
  type: TYPE_NORMAL
- en: Here, I am choosing to promote Semantic Kernel because, despite a few rough
    edges here and there, I believe that it shows great promise, and I prefer the
    design choices made by Semantic Kernel compared to some of the other solutions
    I’ve explored.
  prefs: []
  type: TYPE_NORMAL
- en: 'The packages used at the time of writing were:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '***Tl;dr:*** *If you just want to see some working code that you can use directly,
    all of the code required to replicate this post is available as a notebook* [*here.*](https://gist.github.com/Chris-hughes10/6dacd205f1da3cc3aec4fc45e57fb0b6)'
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I’d like to thank my colleague [Karol Zak](https://medium.com/u/c92a3c46e12c?source=post_page-----af5a1a39564d--------------------------------),
    for collaborating with me on exploring how to get the most out of Semantic Kernel
    for our use cases, and providing code which inspired some of the examples in this
    post!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e28a03d56de945f92b52f6c062791732.png)'
  prefs: []
  type: TYPE_IMG
- en: 'An overview of Semantic Kernel’s components. Image from: [https://learn.microsoft.com/en-us/semantic-kernel/media/kernel-flow.png](https://learn.microsoft.com/en-us/semantic-kernel/media/kernel-flow.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s begin with the central component of the library.
  prefs: []
  type: TYPE_NORMAL
- en: The Kernel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Kernel: “The core, center, or essence of an object or system.” —* [*Wiktionary*](https://en.wiktionary.org/wiki/kernel)'
  prefs: []
  type: TYPE_NORMAL
- en: One of the key concepts in Semantic Kernel is the kernel itself, which is the
    main object that we will use to orchestrate our LLM based workflows. Initially,
    the kernel has very limited functionality; all of its features are largely powered
    by external components that we will connect to. The kernel then acts as a processing
    engine that fulfils a request by invoking appropriate components to complete the
    given task.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a kernel as demonstrated below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Connectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To make our kernel useful, we need to connect one or more AI models, which enable
    us to use our kernel to understand and generate natural language; this is done
    using a *connector*. Semantic Kernel provides out-of-the-box connectors that make
    it easy to add AI models from different sources, such as [OpenAI](https://platform.openai.com/overview),
    [Azure OpenAI](https://azure.microsoft.com/en-us/products/ai-services/openai-service),
    and [Hugging Face](https://huggingface.co/docs/hub/index). These models are then
    used to provide a *service* to the kernel.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of writing, the following services are supported:'
  prefs: []
  type: TYPE_NORMAL
- en: '**text completion service**: used to generate natural language'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**chat service**: used to create a conversational experience'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**text embedding generation service**: used to encode natural language into
    embeddings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each type of service can support multiple models from different sources at the
    same time, making it possible to switch between different models, depending on
    the task and the preference of the user. If no specific service or model is specified,
    the kernel will default to the first service and model that was defined.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see all of the currently registered services using the following attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aab6d6be7e5607c8e2dd48122a6d7279.png)'
  prefs: []
  type: TYPE_IMG
- en: As expected, we don’t currently have any connected services! Let’s change that.
  prefs: []
  type: TYPE_NORMAL
- en: Here, I will start by accessing a [GPT3.5-turbo model](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#gpt-35)
    which I deployed using the [Azure OpenAI service](http://127.0.0.1:8800/files/notebooks/(https%3A/azure.microsoft.com/en-us/products/ai-services/openai-service)?_xsrf=2%7Cdf00fee6%7Cc6c752f68f76d91ba394976c9852cc67%7C1693063310)
    in my [Azure subscription](https://azure.microsoft.com/en-gb/free/search/?ef_id=_k_2411806e795914439019e49fb1bde4ba_k_&OCID=AIDcmm3bvqzxp1_SEM__k_2411806e795914439019e49fb1bde4ba_k_&msclkid=2411806e795914439019e49fb1bde4ba).
  prefs: []
  type: TYPE_NORMAL
- en: As this model can be used for both text completion and chat, I will register
    using both services.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We can now see that the chat service has been registered as both a text completion
    and a chat completion service.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b5994699626278686d707033f96a639e.png)'
  prefs: []
  type: TYPE_IMG
- en: To use the non-Azure OpenAI API, the only change we would have to make is to
    use the `OpenAITextCompletion` and `OpenAIChatCompletion`connectors instead of
    our Azure classes. Don't worry if you don't have access to OpenAI models, we will
    look at how to connect to open source models a little later; the choice of model
    won't affect any of the next steps.
  prefs: []
  type: TYPE_NORMAL
- en: To retrieve a service after we have registered it, we can use the following
    methods on the kernel.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/de5652a97fad9c676dc68815e59c43de.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have registered some services, let’s explore how we can interact
    with them!
  prefs: []
  type: TYPE_NORMAL
- en: Prompt functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The way to interact with a LLM through Semantic Kernel is to create a *Prompt
    Function*. A prompt function expects a natural language input and uses an LLM
    to interpret what is being asked, then act accordingly to return an appropriate
    response. For example, a prompt function could be used for tasks such as text
    generation, summarization, sentiment analysis, and question answering.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Semantic Kernel, a semantic function is composed of two components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt Template**: the natural language query or command that will be sent
    to the LLM'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Execution config**: contains the settings and options for the prompt function,
    such as the service that it should use, the parameters it should expect, and the
    description of what the function does.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The simplest way to get started is by using the kernel’s `create_function_from_prompt`
    method, which accepts a prompt and execution config, as well as some identifiers
    to help keep track of the function in the kernel.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate this, let’s create a simple prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have used the `{{$}}` syntax to represent an argument that will be
    injected into our prompt. Whilst we shall see many more examples of this throughout
    this post, a comprehensive guide to templating syntax can be found [in the documentation](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/prompt-template-syntax).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need to create an execution config. If we know the type of service
    that we want to use to execute our function, we can import the corresponding config
    class and create an instance of this, as demonstrated below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Whilst this works, it does couple our function to a certain type of service,
    which limits our flexibility. An alternative approach is to retrieve the corresponding
    configuration class directly from the service we intend to use, as demonstrated
    below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/94340339cbd93685463d88e00fc7e2b5.png)'
  prefs: []
  type: TYPE_IMG
- en: This way, we can select the service we wish to use at runtime, and automatically
    load an appropriate config object. Let’s us this approach to create our execution
    config.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can create our function!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can use our function using the kernel’s `invoke` method. As many of
    our connected services are likely to be calling external APIs, `invoke` is an
    asyncronous method, based on [Asyncio](https://docs.python.org/3/library/asyncio.html).
    This enables us to execute multiple calls to external services simultaneously,
    without waiting for a response for each one.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The response object contains valuable information about our function call, such
    as the parameters that were used; provided everything worked as expected, we can
    access our result using the `str` constructor on the object.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a53588cf283ee939231eaf251f4f921e.png)![](../Images/24397236996fd8bd8efe17ad066bed69.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can see that our function has worked!
  prefs: []
  type: TYPE_NORMAL
- en: Using Local Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to using models behind APIs, we can also use the kernel to orchestrate
    calls to local models. To illustrate this, let’s register another text completion
    service, and create a config which enables us to specify that we would like to
    use our new service. For our second completion service, let’s use a model from
    the [Hugging Face transformers library](https://huggingface.co/docs/transformers/index).
    To do this, we use the `HuggingFaceTextCompletion` connector.
  prefs: []
  type: TYPE_NORMAL
- en: Here, as we will be running the model locally, I have selected [OPT-350m](https://huggingface.co/facebook/opt-350m),
    a older model aiming to roughly match the performance of GPT-3, which should be
    able to run quickly easily on most hardware.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s create our config object. We can do this in a similar way before,
    but this time passing the `service_id` associated with our Hugging Face service.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We can now create and execute our function as we saw earlier.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/123471b0361323dd4b855faa403f1c0b.png)'
  prefs: []
  type: TYPE_IMG
- en: Well, the generation seems to have worked, but it is arguably not as good as
    the response provided by GPT3.5\. This is not unexpected, as this is an older
    model! Interestingly, we can see that, before it reached its max token limit,
    it started answering a similar pattern about Berlin; this behaviour is not unexpected
    when dealing with text completion models.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a custom connector
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have seen how to create a semantic function and specify which service
    we would like our function to use. However, until this point, all of the services
    we have used have relied on out-of-the-box connectors. In some cases, we may wish
    to use a model from a different library to those currently supported, for which
    we will need a custom connector. Let’s look at how we can do this.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, let’s use a transformer model from the [curated transformers](https://github.com/explosion/curated-transformers)
    library.
  prefs: []
  type: TYPE_NORMAL
- en: To create a custom connector, we need to subclass `TextCompletionClientBase`,
    which acts as a thin wrapper around our model. A simple example of how to do this
    is provided below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can register our connector and create a semantic function as demonstrated
    before. Here, I am using the [Falcon-7B model](https://huggingface.co/tiiuae/falcon-7b),
    which will require a GPU to run in a reasonable amount of time. Here, I used a
    [Nvidia A100](https://learn.microsoft.com/en-us/azure/virtual-machines/nda100-v4-series)
    on an Azure virtual machine, as running it locally was too slow.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/1ab8f519e295a3ddd66f265930d493ea.png)'
  prefs: []
  type: TYPE_IMG
- en: Once again, we can see that the generation has worked, but it quickly descends
    into repetition after it has answered our question.
  prefs: []
  type: TYPE_NORMAL
- en: A likely reason for this is the model that we have selected. Commonly, autoregressive
    transformer models are trained to predict the next word on a large corpus of text;
    essentially making them powerful autocomplete machines! Here, it appears that
    it has tried to ‘complete’ our question, which has resulted in it continuing to
    generate text, which isn’t helpful for us.
  prefs: []
  type: TYPE_NORMAL
- en: Using a Chat Service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some LLM models have undergone additional training, to make them more useful
    to interact with. An example of this process is detailed in OpenAI’s [InstructGPT](https://arxiv.org/abs/2203.02155)
    paper.
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, this usually involves adding one or more supervised finetuning
    steps where, instead of random unstructured text, the model is trained on curated
    examples of tasks such as question answering and summarisation; these models are
    usually known as *instruction-tuned* or *chat* models.
  prefs: []
  type: TYPE_NORMAL
- en: As we already observed how base LLMs can generate more text than we need, let’s
    investigate whether a chat model will perform differently. To use our chat model,
    we need to update our config to specify an appropriate service and create a new
    function; we shall use `azure_gpt35_chat_completion` in our case.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eb8e7f97e4af91fddf18fc2c4412a3d2.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c35c56b3d16c3a36404774a81543bf26.png)'
  prefs: []
  type: TYPE_IMG
- en: Excellent, we can see that the chat model has given us a much more concise answer!
  prefs: []
  type: TYPE_NORMAL
- en: Previously, as we were using text completion models, we had formatted our prompt
    as a sentence for the model to complete. However, the instruction tuned models
    *should* be able to understand a question, so we may be able to change our prompt
    to make it a little more flexible. Let’s see how we can adjust our prompt with
    the aim of interacting with the model as if it was a chatbot designed to provide
    us information about places that we may like to visit.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s adjust our function config to make our prompt more generic.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that we are only passing in the user input, so we must phrase
    our input as a question. Let’s try this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a398bf72d90a1e98ee11afc2e6f4968f.png)'
  prefs: []
  type: TYPE_IMG
- en: Great, that seems like it has worked. Let’s try asking a follow up question.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ace2e9eef5a2f4b8636a1f5efde9aa59.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the model has provided a very generic answer, which doesn’t
    take into account our previous question at all. This is expected, as the prompt
    that the model recieved was `"What are some interesting things to do there?"`,
    we didn't provide any context on where 'there' is!
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how we can extend our approach to make a simple chatbot in the following
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Making a simple Chatbot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have seen how we can use a chat service, let’s explore how we can
    create a simple chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our chatbot should be able to do three things:'
  prefs: []
  type: TYPE_NORMAL
- en: Know it’s purpose and inform us of this
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand the current conversation context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reply to our questions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s adjust our prompt to reflect this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we have added the variable `history` which will be used to provide
    previous context to the chatbot. Whilst this is quite a naive approach, as long
    conversations will quickly cause the prompt to reach the model's maximum context
    length, it should work for our purposes.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have only used prompts which use a single variable. To use multiple
    variables we need adjust our config, as demonstrated below, by creating a `PromptTemplateConfig`;
    which defines which inputs we are expecting.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s use this updated config and prompt to create our chatbot
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: To keep track of the history to include in our prompt, we can use a `ChatHistory`
    object. Let's create a new instance of this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Additionally, to pass multiple arguments to our prompt, we can use the `KernelArguments`,
    so that we are only passing a single parameter to `invoke`; which contains all
    arguments.
  prefs: []
  type: TYPE_NORMAL
- en: We can see how to do this by creating a simple chat function, to update our
    history after each interaction.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Let’s try it out!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/792c0f9fcef0e077c9628a9b23b6f4c7.png)![](../Images/34145621cec35d2f67386bfb0313eafc.png)![](../Images/61f2e36ac018128cd7a049b1555c57ce.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can see that this has fulfilled our requirements quite well!
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting our prompt, we can see that our history is being rendered into a
    format which has the option of including additional metadata. Whilst this may
    be a useful implementation detail, it is likely that we don’t want our prompt
    formatted this way!
  prefs: []
  type: TYPE_NORMAL
- en: When using a library such as semantic kernel, it is important to be able to
    verify exactly what is being passed into the model, as the way that a prompt is
    written and formatted can have a big impact on the result.
  prefs: []
  type: TYPE_NORMAL
- en: Most language models, such as the OpenAI APIs, do not take a single prompt as
    an input, but prefer inputs formatted as a list of messages; alternating between
    the user and the model. We can inspect how our prompt will be broken down into
    messages below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1dedd9d10b61fa6508b4dd463bc76e50.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can see that all of the formatting associated with the chat history
    has been removed, and the messages appear how we would expect.
  prefs: []
  type: TYPE_NORMAL
- en: Memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When interacting with our chatbot, one of the key aspects that made the experience
    feel like a useful interaction was that the chatbot was able to retain the context
    of our previous questions. We did this by giving the chatbot access to *memory*,
    leveraging `ChatHistory` to handle this for us.
  prefs: []
  type: TYPE_NORMAL
- en: Whilst this worked well enough for our simple use case, all of our conversation
    history was stored in our system’s RAM and not persisted anywhere; once we shut
    down our system, this is gone forever. For more intelligent applications, it can
    be useful to be able to build and persist both short and long term memory for
    our models to access.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, in our example, we were feeding *all* of our previous interactions
    into our prompt. As models usually have a fixed size context window — which determines
    how long our prompts can be — this will quickly break down if we start to have
    lengthy conversations. One way to avoid this is to store our memory as separate
    ‘chunks’ and only load information that we think may be relevant into our prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic Kernel offers some functionality around how we can incorporate memory
    into our applications, so let’s explore how we can leverage these.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, let’s extend our chatbot so that it has access to some information
    that is stored in memory.
  prefs: []
  type: TYPE_NORMAL
- en: First, we need some information that may be relevant to our chatbot. Whilst
    we could manually resarch and curate relevant information, it is quicker to have
    the model generate some for us! Let’s get the model to generate some facts about
    the city of London. We can do this as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/fca819fa55e89cf5a9027549c79aebba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we have some text, so that the model can access only the parts that
    it needs, let’s divide this into chunks. Semantic kernel offers some functionality
    to do this in it’s `text_chunker` module. We can use this as demonstrated below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/13e57bb83384a7163ce0341532935a1e.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the text has been split into 8 chunks. Depending on the text,
    we will have to adjust the maximum number of tokens specified for each chunk.
  prefs: []
  type: TYPE_NORMAL
- en: Using a Text Embedding Service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have chunked our data, we need to create a representation of each
    chunk that enables us to calculate relevance between text; we can do this by representing
    our text as embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: To generate embeddings, we need to add a text embedding service to our kernel.
    Similarly to before, there are various connectors that can be used, depending
    on the source of the underlying model.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s use a `[text-embedding-ada-002](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#embeddings-models)`
    [model](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#embeddings-models)
    deployed in the Azure OpenAI service. This model was trained by OpenAI, and more
    information about this model can be found in their [launch blog post](https://openai.com/blog/new-and-improved-embedding-model).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have access to a model that can generate embeddings, we need somewhere
    to store these. Semantic Kernel provides the concept of a MemoryStore, which is
    an interface to various persistence providers.
  prefs: []
  type: TYPE_NORMAL
- en: For production systems, we would probably want to use a database for our persistence,
    to keep things simple for our example, we shall use in-memory storage. Let’s create
    an instance of an in-memory memory store.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Whilst we have used an in-memory memory store to keep things simple for our
    example, we would probably want to use a database for our persistence when building
    more complex systems. Semantic Kernel offers connectors to popular storage solutions
    such as [CosmosDB](https://learn.microsoft.com/en-us/azure/cosmos-db/introduction),
    Redis, [Postgres](https://www.postgresql.org/) and many others. As memory stores
    have a common interface, the only change that would be required would be modifying
    the connector used, which makes it easy to switch between providers.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have defined our memory store, we need to generate our embeddings.
    Semantic Kernel provides *Semantic memory* data sctuctures to help with this;
    which associate a memory store with a service that can generate embeddings. Here,
    we are going to use `SemanticTextMemory`, which will enable us to embed and retrieve
    our document chunks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We can now save information to our memory store as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have created a new collection, to group similar documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now query this collection in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9b0a34d7f1e6ebcbaa52a0417d0a44b0.png)'
  prefs: []
  type: TYPE_IMG
- en: Looking at the results, we can see that relevant information has been returned;
    which is reflected by the high relevance scores.
  prefs: []
  type: TYPE_NORMAL
- en: However, this was quite easy, as we have information direcly relating to what
    we being asked, using very similar language. Let’s try a more subtle query.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/4e4b62175a68c308f862a2f2d830f10c.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can see that we have received exactly the same results. However, as
    our second result explicitly mentions ‘food from around the world’, I feel that
    this is a better match. This highlights some of the potential limitations of a
    semantic search approach.
  prefs: []
  type: TYPE_NORMAL
- en: Using an Open Source model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Out of interest, let’s see how an open source model compares with our OpenAI
    service in this context. We can register a [Hugging Face sentence transformer
    model](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) for this
    purpose, as demonstrated below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: We can now query these in the same way as before.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/cf172189efbb0a30b9b1ebc4c55663a5.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/74d509ae00eb7311f6b7bfaa22c84a27.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that we have returned the same chunks, but our relevance scores are
    different. We can also observe the difference in the dimensions of the embeddings
    generated by the different models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6fe7aeae1fb97200b29bb8eefd1044e3.png)'
  prefs: []
  type: TYPE_IMG
- en: Integrating memory into context
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our previous example, we saw that whilst we could identify broadly relevant
    information based on an embedding search, for more subtle queries we didn’t receive
    the most relevant result. Let’s explore whether we can improve upon this.
  prefs: []
  type: TYPE_NORMAL
- en: One way that we could approach this is to provide the relevant information to
    our chatbot, and then let the model decide which parts are the most relevant.
    Let’s create a prompt which instructs the model to answer the question based on
    the context provided, and register a prompt function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can use this function to answer our more subtle question. First, we
    create a context object, and add our question to this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Next, we can manually perform our embedding search, and add the retrieved information
    to our context.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: We create a context object, and add our question to this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we can execute our function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9e1624ff5d7fe229469aaa2990655f2b.png)'
  prefs: []
  type: TYPE_IMG
- en: This time, we see that our answer has referenced the information that we are
    looking for and provided a better answer!
  prefs: []
  type: TYPE_NORMAL
- en: Plugins
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A plugin in Semantic Kernel is a group of functions that can be loaded into
    the kernel to be exposed to AI apps and services. The functions within plugins
    can then be orchestrated by the kernel to accomplish tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The [documentation](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/plugins)
    describes plugins as the “building blocks” of Semantic Kernel, which can be chained
    together to create complex workflows; as plugins follow the OpenAI plugin specification,
    plugins created for OpenAI services, Bing, and Microsoft 365 can be used with
    Semantic Kernel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Semantic Kernel [provides several plugins out-of-the-box](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/out-of-the-box-plugins?tabs=python),
    which include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ConversationSummaryPlugin**: To summarize a conversation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**HttpPlugin**: To call APIs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TextMemoryPlugin**: To store and retrieve text in memory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TimePlugin**: To acquire the time of day and any other temporal information'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start by exploring how we can use a pre-defined plugin, before moving
    on to investigate how we can create custom plugins.
  prefs: []
  type: TYPE_NORMAL
- en: Using an out-of-the-box plugin
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the plugins included in Semantic Kernel is `TextMemoryPlugin`, which
    provides functionality to save and recall information from memory. Let's see how
    we can use this to simplify our previous example of populating our prompt context
    from memory.
  prefs: []
  type: TYPE_NORMAL
- en: First, we must import our plugin, as demonstrated below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f968c5e002ddf81ab69811fb823cd5a1.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can see that this plugin contains two semantic functions, `recall`
    and `save`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s modify our prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: We can see that, to use the `recall` function, we can reference this in our
    prompt. Now, let's create a config and register a function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: In our manual example, we were able to control aspects such as the number of
    results returned and the collection to search. When using `TextMemoryPlugin`,
    we can set these by adding them to `KernelArguments`. Let's try out our function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/36ff67413f9cf314b5c000dca6c273b8.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that this is equivalent to our manual approach.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Custom Plugins
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we understand how to create semantic functions, and how to use plugins,
    we have everything we need to start making our own plugins!
  prefs: []
  type: TYPE_NORMAL
- en: 'Plugins can contain two types of functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt functions**: use natural language to perform actions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Native functions**: use Python code to perform actions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: which can be combined within a single plugin.
  prefs: []
  type: TYPE_NORMAL
- en: The choice of whether to use a prompt vs native function depends on the task
    that you are performing. For tasks involving understanding or generating language,
    prompt functions are the obvious choice. However, for more deterministic tasks,
    such as performing mathematical operations, downloading data or accessing the
    time, native functions are better suited.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore how we can create each type. First, let’s create a folder to store
    our plugins.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '**Creating a Poem generator plugin**'
  prefs: []
  type: TYPE_NORMAL
- en: For our example, let’s create a plugin which generates poems; for this, using
    a prompt function seems a natural choice. We can create a folder for this plugin
    in our directory.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Recalling that plugins are just a collection of functions, and we are creating
    a semantic function, the next part should be quite familiar. The key difference
    is that, instead of defining our prompt and config inline, we will create individual
    files for these; to make it easier to load.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s create a folder for our semantic function, which we shall call `write_poem`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Next, we create our prompt, saving it as `skprompt.txt`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8f42f25e22886c6b01197b54db47e664.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let’s create our config and store this in a json file.
  prefs: []
  type: TYPE_NORMAL
- en: Whilst it is always good practice to set meaningful descriptions in our config,
    this becomes more important when we are defining plugins; plugins should provide
    clear descriptions that describe how they behave, what their inputs and outputs
    are, and what their side effects are. The reason for this is that this is the
    interface that is presented by our kernel and, if we want to be able to use an
    LLM to orchestrate tasks, it needs to be able to understand the plugin’s functionality
    and how to call it so that it can select appropriate functions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are able to import our plugin:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Inspecting our plugin, we can see that it exposes our `write_poem` semantic
    function.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/afe6ac1b359eaf572171a98b54ad10a5.png)'
  prefs: []
  type: TYPE_IMG
- en: We can call our function using the kernel, as we have seen before.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9352eab944764efc3a05a9f3dc5ad8ab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'or, we can use it in another semantic function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ce64b97153dd1ba8bf2b4f833b801fe8.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Creating an Image Classifier plugin**'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen how to use a prompt function in a plugin, let’s take a
    look at how we can use a native function.
  prefs: []
  type: TYPE_NORMAL
- en: Here, let’s create a plugin that takes an image url, then downloads and classifies
    the image. Once again, let’s create a folder for our new plugin.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can create our Python module. Inside the module, we can be quite flexible.
    Here, we have created a class with two methods, the key step is to use the `kernel_function`
    decorator to specify which methods should be exposed as part of the plugin.
  prefs: []
  type: TYPE_NORMAL
- en: For our inputs, we have used the `Annotated` type hint to provide a description
    of what our argument does. More information can be found [in the documentation](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/native-functions?tabs=python).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: For this example, I have used the excellent [Pytorch Image Models](https://github.com/huggingface/pytorch-image-models)
    library to provide our classifier. For more information on how this library works,
    check out this [blog post](/getting-started-with-pytorch-image-models-timm-a-practitioners-guide-4e77b4bf9055).
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can simply import our plugin as seen below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Inspecting our plugin, we can see that only our decorated function is exposed.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/da9f13185d2693a44d1a13045d64dde8.png)'
  prefs: []
  type: TYPE_IMG
- en: We can verify that our plugin works using an [image of a cat from Pixabay](https://pixabay.com/photos/cat-kitten-pet-striped-young-1192026/).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e665bbc0cb413f97f5ca010e6dbf0983.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/b594aff9671760a0edeebd60819063c5.png)'
  prefs: []
  type: TYPE_IMG
- en: Manually calling our function, we can see that our image has been classified
    correctly! In the same way as before, we could also reference this function directly
    from a prompt. However, as we have already demonstrated this, let’s try something
    slightly different in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Chaining multiple plugins
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have defined a variety of functions, both inline and as plugins,
    let’s see how we can orchestrate a workflow that calls more than one function.
  prefs: []
  type: TYPE_NORMAL
- en: If we would like to execute multiple functions independently, this is straightforward;
    we can simply pass a list of functions to `invoke`, as demonstrated below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3cb07fd67a06aef978b9e4bf0ed538ac.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can see that the same input has been used for each function. We could
    have defined different named parameter in our `KernelArguments`, but if multiple
    functions have arguments with the same name, this becomes difficult. As an aside,
    our poem generator seemed to do a great job given that it was only provided with
    a url!
  prefs: []
  type: TYPE_NORMAL
- en: A more interesting case is when we would like to use the output from one function
    as the input to another, so let’s explore that.
  prefs: []
  type: TYPE_NORMAL
- en: 'To provide more finegrained control over how functions are invoked, the kernel
    enables us to define handlers, where we can inject custom behaviour:'
  prefs: []
  type: TYPE_NORMAL
- en: '`add_function_invoking_handler`: used to register handlers that are called
    before a function is called'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`add_function_invoked_handler`: used to register handlers that are called after
    a function is called'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we would like to update our input to the next function with the previous
    function’s output, we can define a short function to do this, and register this
    so that it is called after each function has been invoked. Let’s see how we can
    do this.
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to define a function which takes the kernel and an instance of
    `FunctionInvokedEventArgs`, and updates our arguments.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Next, we can register this with our kernel.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can invoke our functions as before.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2b19673d4d16226d06cdcac56c51f830.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that, using both plugins sequentially, we have classified the image
    and wrote a poem about it!
  prefs: []
  type: TYPE_NORMAL
- en: Orchestrating workflows with a Planner
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, we have thoroughly explored semantic functions, understand how
    functions can be grouped and used as part of a plugin, and have seen how we can
    chain plugins together manually. Now, let’s explore how we can create and orchestrate
    workflows using LLMs. To do this, Semantic Kernel provides *Planner* objects,
    which can dynamically create chains of functions to try and achieve a goal.
  prefs: []
  type: TYPE_NORMAL
- en: A planner is a class that takes a user prompt and a kernel, and uses the kernel’s
    services to create a plan of how to perform the task, using the functions and
    plugins that have been made available to the kernel. As the plugins are the main
    building blocks of these plans, the planner relies heavily on the descriptions
    provided; if plugins and functions don’t have clear descriptions, the planner
    will not be able to use them correctly. Additionally, as a planner can combine
    functions in various different ways, it is important to ensure that we only expose
    functions that we are happy for the planner to use.
  prefs: []
  type: TYPE_NORMAL
- en: As the planner relies on a model to generate a plan, there can be errors introduced;
    these usually arise when the planner doesn’t properly understand how to use the
    function. In these cases, I have found that providing explicit instructions —
    such as describing the inputs and outputs, and stating whether inputs are required
    — in the descriptions can lead to better results. Additionally, I have had better
    results using instruction tuned models than base models; base text completion
    models tend to hallucinate functions that don’t exist or create multiple plans.
    Despite these limitations, when everything works correctly, planners can be incredibly
    powerful!
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore how we can do this by exploring if we can create a plan to write
    a poem about an image, based on its url; using the plugins we created earlier.
    As we have defined lots of functions that we no longer need, let’s create a new
    kernel, so we can control which functions are exposed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: To create our plan, let’s use our OpenAI chat service.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s import our plugins.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: We can see which functions our kernel has access to as demonstrated below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c38c0113d01700a4cbb9684a12fd50e6.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let’s import our planner object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: To use our planner, all we need is a prompt. Often, we will need to tweak this
    depending on the plans that are generated. Here, I have tried to be as explicit
    as possible about the input that is required.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Next, we can use our planner to create a plan for how it will solve the task.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5d418bcf66576207199f693bfc3ea199.png)'
  prefs: []
  type: TYPE_IMG
- en: Inspecting our plan, we can see that the model has correctly identified out
    input, and the correct functions to use!
  prefs: []
  type: TYPE_NORMAL
- en: Finally, all that is left to do is to execute our plan.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c0959b816490d6da186bda29626396a9.png)'
  prefs: []
  type: TYPE_IMG
- en: Wow, it worked! For a model trained to predict the next word, that is pretty
    powerful!
  prefs: []
  type: TYPE_NORMAL
- en: As a word of warning, I was quite lucky when making this example that the generated
    plan worked first time. However, we are relying on a model to correctly interpret
    our instructions, as well as understanding the tools available; not to mention
    that LLMs can hallucinate and potentially dream up new functions that don’t exist!
    For me personally, in a production system, I would feel much more comfortable
    manually creating the workflow to execute, rather than leaving it to the LLM!
    As the technology continues to improve, especially at the current rate, hopefully
    this recommendation will become outdated!
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hopefully this has provided a good introduction to Semantic Kernel and has inspired
    you to explore using it for your own use cases.
  prefs: []
  type: TYPE_NORMAL
- en: '*All of the code required to replicate this post is available as a notebook*
    [*here*](https://gist.github.com/Chris-hughes10/6dacd205f1da3cc3aec4fc45e57fb0b6)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chris Hughes*](https://www.linkedin.com/in/chris-hughes1/) *is on LinkedIn*'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Introducing ChatGPT (openai.com)](https://openai.com/blog/chatgpt)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[microsoft/semantic-kernel: Integrate cutting-edge LLM technology quickly and
    easily into your apps (github.com)](https://github.com/microsoft/semantic-kernel)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Who We Are — Microsoft Solutions Playbook](https://playbook.microsoft.com/code-with-engineering/ISE/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[kernel — Wiktionary, the free dictionary](https://en.wiktionary.org/wiki/kernel)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Overview — OpenAI API](https://platform.openai.com/overview)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Azure OpenAI Service — Advanced Language Models | Microsoft Azure](https://azure.microsoft.com/en-us/products/ai-services/openai-service)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hugging Face Hub documentation](https://huggingface.co/docs/hub/index)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Azure OpenAI Service models — Azure OpenAI | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#gpt-35)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Create Your Azure Free Account Today | Microsoft Azure](https://azure.microsoft.com/en-gb/free/search/?ef_id=_k_2411806e795914439019e49fb1bde4ba_k_&OCID=AIDcmm3bvqzxp1_SEM__k_2411806e795914439019e49fb1bde4ba_k_&msclkid=2411806e795914439019e49fb1bde4ba)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to use prompt template language in Semantic Kernel | Microsoft Learn](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/prompt-template-syntax)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[asyncio — Asynchronous I/O — Python 3.11.5 documentation](https://docs.python.org/3/library/asyncio.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[🤗 Transformers (huggingface.co)](https://huggingface.co/docs/transformers/index)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[gpt2 · Hugging Face](https://huggingface.co/gpt2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[explosion/curated-transformers: 🤖 A PyTorch library of curated Transformer
    models and their composable components (github.com)](https://github.com/explosion/curated-transformers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[tiiuae/falcon-7b · Hugging Face](https://huggingface.co/tiiuae/falcon-7b)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ND A100 v4-series — Azure Virtual Machines | Microsoft Learn](https://learn.microsoft.com/en-us/azure/virtual-machines/nda100-v4-series)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[2203.02155] Training language models to follow instructions with human feedback
    (arxiv.org)](https://arxiv.org/abs/2203.02155)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Azure OpenAI Service models — Azure OpenAI | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#embeddings-models)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[New and improved embedding model (openai.com)](https://openai.com/blog/new-and-improved-embedding-model)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introduction — Azure Cosmos DB | Microsoft Learn](https://learn.microsoft.com/en-us/azure/cosmos-db/introduction)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PostgreSQL: The world’s most advanced open source database](https://www.postgresql.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[sentence-transformers/all-MiniLM-L6-v2 · Hugging Face](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Understanding AI plugins in Semantic Kernel and beyond | Microsoft Learn](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/plugins?tabs=Csharp)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Out-of-the-box plugins available in Semantic Kernel | Microsoft Learn](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/out-of-the-box-plugins?tabs=python)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to add native code to your AI apps with Semantic Kernel | Microsoft Learn](https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/native-functions?tabs=python)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[huggingface/pytorch-image-models: PyTorch image models, scripts, pretrained
    weights — ResNet, ResNeXT, EfficientNet, NFNet, Vision Transformer (ViT), MobileNet-V3/V2,
    RegNet, DPN, CSPNet, Swin Transformer, MaxViT, CoAtNet, ConvNeXt, and more (github.com)](https://github.com/huggingface/pytorch-image-models)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Getting Started with PyTorch Image Models (timm): A Practitioner’s Guide |
    by Chris Hughes | Towards Data Science](/getting-started-with-pytorch-image-models-timm-a-practitioners-guide-4e77b4bf9055)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
