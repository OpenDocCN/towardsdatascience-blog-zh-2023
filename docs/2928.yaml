- en: 'The Olympics of AI: Benchmarking Machine Learning Systems'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-olympics-of-ai-benchmarking-machine-learning-systems-c4b2051fbd2b?source=collection_archive---------2-----------------------#2023-09-22](https://towardsdatascience.com/the-olympics-of-ai-benchmarking-machine-learning-systems-c4b2051fbd2b?source=collection_archive---------2-----------------------#2023-09-22)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How do benchmarks birth breakthroughs?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@matthew_stewart?source=post_page-----c4b2051fbd2b--------------------------------)[![Matthew
    Stewart, PhD](../Images/67da9fb3c0d2516ece297f2729aa0ce8.png)](https://medium.com/@matthew_stewart?source=post_page-----c4b2051fbd2b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c4b2051fbd2b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c4b2051fbd2b--------------------------------)
    [Matthew Stewart, PhD](https://medium.com/@matthew_stewart?source=post_page-----c4b2051fbd2b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb89dbc0712c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-olympics-of-ai-benchmarking-machine-learning-systems-c4b2051fbd2b&user=Matthew+Stewart%2C+PhD&userId=b89dbc0712c4&source=post_page-b89dbc0712c4----c4b2051fbd2b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c4b2051fbd2b--------------------------------)
    ·13 min read·Sep 22, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc4b2051fbd2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-olympics-of-ai-benchmarking-machine-learning-systems-c4b2051fbd2b&user=Matthew+Stewart%2C+PhD&userId=b89dbc0712c4&source=-----c4b2051fbd2b---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc4b2051fbd2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-olympics-of-ai-benchmarking-machine-learning-systems-c4b2051fbd2b&source=-----c4b2051fbd2b---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: You can’t improve what you don’t measure. — ***Peter Drucker***
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/c81835eea9541f2e21572c7a05d610e6.png)'
  prefs: []
  type: TYPE_IMG
- en: The Olympic rings. Image created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Four-Minute Mile: The Benchmark that Redefined Running'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For years, running a mile in under four minutes was considered not just a daunting
    challenge, but by many an impossible feat. It was a psychological and physical
    benchmark that many thought was unattainable. Doctors and sports experts theorized
    that the human body was not capable of running that fast for that long. This belief
    was so ingrained that some even suggested attempting to do so could be fatal.
  prefs: []
  type: TYPE_NORMAL
- en: '[Sir Roger Bannister](https://en.wikipedia.org/wiki/Roger_Bannister), a British
    middle-distance runner and medical student, thought differently. While he recognized
    the challenge, he believed that the barrier was more psychological than physiological.
    Bannister took a scientific approach to his training, breaking down the mile into
    sections and rigorously timing each one. He also employed a rigorous training
    regimen based on interval training and set smaller benchmarks for himself in the
    lead-up to his record attempt.'
  prefs: []
  type: TYPE_NORMAL
- en: On May 6, 1954, at a track in Oxford, England, with the help of his friends
    Chris Brasher and Chris Chataway as pacemakers, Bannister made his attempt to
    break the four-minute barrier. He completed the mile in 3 minutes 59.4 seconds,
    shattering the threshold and making history.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0b0f7dde05850ea47428dd61ad729d7e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Roger Bannister running during a race. Image Source: [Norske Leksikon](https://media.snl.no/media/234992/standard_compressed_NL-HaNA_2.24.01.09_0_902-9145-groot.jpg)
    (CC-BY 4.0).'
  prefs: []
  type: TYPE_NORMAL
- en: The aftermath of Bannister’s achievement was highly unexpected. [Gunder Hägg](https://en.wikipedia.org/wiki/Gunder_H%C3%A4gg)’s
    1945 record (4 minutes 1.4 seconds) had stood for almost a decade before Bannister
    came along. However, once the four-minute mile benchmark was broken, others soon
    followed. Just 46 days after Bannister’s run, [John Landy](https://en.wikipedia.org/wiki/John_Landy)
    finished a mile in 3 minutes 57.9 seconds. Over the next ten years, the record
    was beaten another 5 times. The current record, set by [Hicham El Guerrouj](https://en.wikipedia.org/wiki/Hicham_El_Guerrouj),
    stands at 3 minutes 43.1 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: World record mile times for the period 1900–2000\. Notice the gap between 1945
    and 1954 before Roger Bannister beat the four-minute mile benchmark—otherwise,
    the downward trend is almost linear. Figure created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Bannister’s achievement illustrates the power of benchmarks, not just as measures
    of performance but as motivators for change. Once the four-minute “benchmark”
    was broken, it redefined what athletes believed was possible. The barrier was
    as much in the mind as it was on the track.
  prefs: []
  type: TYPE_NORMAL
- en: The four-minute mile embodies the transformative power of benchmarks across
    disciplines. Benchmarks provide a way to quantify performance improvements for
    particular tasks, giving us a way to compare ourselves to others. This is the
    entire basis for sporting events such as the Olympics. However, benchmarks are
    only useful if the community they involve can decide on a common goal to be pursued.
  prefs: []
  type: TYPE_NORMAL
- en: In the realm of machine learning and computer science, **benchmarks serve as
    the communal Olympics** — a grand arena where algorithms, systems, and methodologies
    compete, not for medals, but for the pride of advancement and the drive for innovation.
    Just as athletes train for years to shave milliseconds off their time in pursuit
    of Olympic gold, developers and researchers optimize their models and systems
    to improve performance, striving to outperform on established benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: The art and science of benchmarking lie in the establishment of that *common
    goal*. It is not merely about setting a task, but ensuring it captures the essence
    of real-world challenges, pushing the boundaries of what is possible while remaining
    relevant and applicable. Poorly chosen benchmarks can lead researchers astray,
    optimizing for tasks that do not translate to improvement in real-world applications.
    A well-designed benchmark can guide a whole community toward breakthroughs that
    redefine a field.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, while benchmarks are tools for comparison and competition, their true
    value lies in their ability to unite a community around a shared vision. Much
    like Bannister’s run didn’t just break a record but redefined athletic potential,
    a well-conceptualized benchmark can elevate an entire discipline, shifting paradigms
    and ushering in new eras of innovation.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will explore the crucial role of benchmarking in advancing
    computer science and machine learning by journeying through its history, discussing
    the latest trends in benchmarking machine learning systems, and seeing how it
    spurs innovation in the hardware sector.
  prefs: []
  type: TYPE_NORMAL
- en: 'Benchmarking Computing Systems: SPEC'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the 1980s, as the personal computer revolution was taking off, there was
    a growing need for standardized metrics to compare the performance of different
    computer systems: **a** [**benchmark**](https://en.wikipedia.org/wiki/Benchmark_(computing)).
    Before standardized benchmarks, manufacturers often developed and used their own
    custom benchmarks. These benchmarks tended to highlight their machines’ strengths
    while downplaying their weaknesses. It became clear that a neutral, universally
    accepted benchmark was necessary for comparison.'
  prefs: []
  type: TYPE_NORMAL
- en: To address this challenge, the [System Performance Evaluation Cooperative](https://www.spec.org/)
    (SPEC) was developed. The members of this organization were hardware vendors,
    researchers, and other stakeholders interested in creating a universal standard
    for benchmarking [central processing units](https://en.wikipedia.org/wiki/Central_processing_unit)
    (CPUs), also commonly referred to as ‘chips’.
  prefs: []
  type: TYPE_NORMAL
- en: SPEC’s first major contribution was the [SPEC89](https://www.spec.org/cpu89/)
    benchmark suite, which was groundbreaking in that it was one of the first attempts
    at an industry-standard CPU benchmark. SPEC’s benchmarks focused on real-world
    applications and computing tasks, aiming to provide metrics that mattered to end-users
    rather than esoteric or niche measurements.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, as the benchmark evolved, an intriguing phenomenon emerged: the so-called
    “**benchmark effect**.” As the SPEC benchmarks became the gold standard for measuring
    CPU performance, CPU designers started optimizing their designs for SPEC’s benchmarks.
    In essence, because the industry had come to value SPEC benchmarks as a measure
    of overall performance, there was a strong incentive for manufacturers to ensure
    their CPUs performed exceptionally well on these tests — even if it meant potentially
    sacrificing performance in non-SPEC tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: This wasn’t necessarily SPEC’s intention, and it led to a spirited debate within
    the computer science community. Were the benchmarks genuinely representative of
    real-world performance? Or were they driving a form of tunnel vision, where the
    benchmarks became an end unto themselves rather than a means to an end?
  prefs: []
  type: TYPE_NORMAL
- en: Recognizing these challenges, SPEC continually updated its benchmarks over the
    years to stay ahead of the curve and prevent undue optimization. Their benchmark
    suites expanded to cover different domains, from integer and floating-point computation
    to more domain-specific tasks in graphics, file systems, and more.
  prefs: []
  type: TYPE_NORMAL
- en: The story of SPEC and its benchmarks underscores the profound impact that benchmarking
    can have on an entire industry’s direction. The benchmarks didn’t merely measure
    performance — **they influenced it**. It’s a testament to the power of standardization,
    but also a cautionary tale about the unintended consequences that can emerge when
    a single metric becomes the focal point of optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Today, SPEC benchmarks, along with other benchmarks, continue to play a vital
    role in shaping the computer hardware industry and guiding consumers and enterprises
    in their purchasing decisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Benchmarking Deep Learning: ImageNet'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the late 2000s, [computer vision](https://en.wikipedia.org/wiki/Computer_vision),
    a subfield of AI focused on enabling machines to interpret and make decisions
    based on visual data, was struggling to make progress. Traditional techniques
    had made progress, but they were hitting a performance plateau on many tasks.
    The methods available at the time relied heavily on hand-crafted features, requiring
    experts to meticulously design and select specific features for each task. It
    was a tedious process with many limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Then [ImageNet](https://en.wikipedia.org/wiki/ImageNet) was released, a massive
    visual database initiated by [Dr. Fei-Fei Li](https://en.wikipedia.org/wiki/Fei-Fei_Li)
    and her team. ImageNet provided millions of labeled images spanning thousands
    of categories. The sheer volume of this dataset was unprecedented, and only enabled
    by the ability to crowdsource data labeling through cloud-based approaches like
    [Amazon Mechanical Turk](https://www.mturk.com/). ImageNet was one of the first
    dataset benchmarks — since its release, the ImageNet [paper](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
    has been cited over 50,000 times.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9ac69c468dec86f3bdcfd6971e360b5e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A visual compilation of ImageNet images. Image source: [Gluon](https://cv.gluon.ai/build/examples_datasets/imagenet.html)
    (CC-BY 4.0).'
  prefs: []
  type: TYPE_NORMAL
- en: 'But collecting the dataset was just the beginning. In 2010, the [ImageNet Large
    Scale Visual Recognition Challenge](https://www.image-net.org/challenges/LSVRC/)
    (ILSVRC) was launched. The challenge was simple in its objective but daunting
    in its scale: automatically classify an image into one of 1,000 categories. This
    benchmark challenge would provide an objective measure of progress in computer
    vision, on a scale far beyond previous attempts.'
  prefs: []
  type: TYPE_NORMAL
- en: The initial years saw incremental improvements over traditional methods. However,
    the 2012 challenge witnessed a transformative shift. A team from the University
    of Toronto, led by [Alex Krizhevsky](https://en.wikipedia.org/wiki/Alex_Krizhevsky),
    [Ilya Sutskever](https://en.wikipedia.org/wiki/Ilya_Sutskever), and [Geoffrey
    Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton), introduced a deep convolutional
    neural network (CNN) called “[AlexNet](https://en.wikipedia.org/wiki/AlexNet).”
    Their model achieved an error rate of 15.3%, slashing the previous year’s error
    by nearly half!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4d8cdf3f2733a03984fb86feed41e4dd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Error rates on the ImageNet Large-Scale Visual Recognition Challenge. Accuracy
    dramatically improved with the introduction of deep learning in 2012 and continued
    to improve thereafter. Humans perform with an error rate of approximately 5%.
    Image source: [2018 NIH/RSNA/ACR/The Academy Workshop](https://www.researchgate.net/publication/332452649_A_Roadmap_for_Foundational_Research_on_Artificial_Intelligence_in_Medical_Imaging_From_the_2018_NIHRSNAACRThe_Academy_Workshop).
    This image has been reproduced in accordance with the Creative Commons Attribution
    4.0 International License (CC BY 4.0).'
  prefs: []
  type: TYPE_NORMAL
- en: What made this possible? Deep learning, and particularly CNNs, had the capability
    to learn features directly from raw pixels, eliminating the need for manual feature
    crafting. Given enough data and computational power, these networks could uncover
    intricate patterns that were far beyond what traditional methods could manage.
  prefs: []
  type: TYPE_NORMAL
- en: 'The success of AlexNet was a watershed moment in the development of AI. The
    years following 2012 saw deep learning methods dominating the ImageNet challenge,
    driving error rates lower and lower. The clear message from the benchmarks was
    undeniable: deep learning, once a niche area in machine learning, was set to revolutionize
    computer vision.'
  prefs: []
  type: TYPE_NORMAL
- en: And it did more than that. The success in the ILSVRC acted as a catalyst, propelling
    deep learning to the forefront of not just computer vision, but numerous areas
    in AI, from natural language processing to game playing. The challenge underscored
    the potential of deep learning, attracting researchers, funding, and focus to
    the area.
  prefs: []
  type: TYPE_NORMAL
- en: By setting a clear, challenging benchmark, the ImageNet challenge played a pivotal
    role in redirecting the trajectory of AI research, leading to the current deep
    learning-driven AI renaissance we witness today.
  prefs: []
  type: TYPE_NORMAL
- en: 'Benchmarking Machine Learning Systems: MLPerf'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The transformative impact of benchmarks like SPEC and ImageNet naturally prompts
    the question: What’s next? As deep learning models became increasingly complex,
    so did their computational demands. This shifted attention to another critical
    component — the hardware that powered these models. Enter [MLPerf](https://www.nvidia.com/en-us/data-center/resources/mlperf-benchmarks/).'
  prefs: []
  type: TYPE_NORMAL
- en: MLPerf emerged as a collaborative effort involving industry giants and academic
    institutions, with the mission of creating a standard set of benchmarks to measure
    the performance of machine learning hardware, software, and cloud platforms. As
    the name suggests, MLPerf focuses explicitly on machine learning, capturing a
    broad spectrum of tasks ranging from image classification to reinforcement learning.
    The objective was clear — to provide clarity in a field where “best performance”
    claims were becoming commonplace, yet were often based on inconsistent criteria
    or cherry-picked metrics.
  prefs: []
  type: TYPE_NORMAL
- en: The introduction of MLPerf presented the tech industry with a much-needed unified
    yardstick. For academia, it provided a clear performance target, fostering an
    environment where innovation in algorithms could be easily measured and compared.
    For industry, especially hardware manufacturers, it posed both a challenge and
    an opportunity. No longer could a new chip be launched with vague assertions about
    its machine learning performance — there was now a universally accepted benchmark
    that would put any such claims to the test.
  prefs: []
  type: TYPE_NORMAL
- en: And just like SPEC influenced CPU design, MLPerf began shaping the direction
    of AI hardware. Companies started optimizing their designs with MLPerf benchmarks
    in mind, and it was not just about raw performance. The benchmarks also incorporated
    efficiency metrics, encouraging innovations that delivered not just speed but
    also energy efficiency — a pressing concern in the age of colossal transformer
    models and environmental consciousness. These benchmarks are used routinely by
    big tech companies, such as Nvidia and AMD, to showcase their new hardware.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/463b8524dab68c052df67ae5871ed966.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Nvidia H100 normalized performance on MLPerf Inference v3.0 Datacenter vs.
    the previous Nvidia A100 system. As can be seen, the H100 has a 4x speed-up on
    the full-sized large language model BERT compared with the previous generation
    of chip. Image source: [MLCommons](https://mlcommons.org/en/inference-datacenter-30/)
    and [Nvidia Blogs](https://blogs.nvidia.com/blog/2023/04/05/inference-mlperf-ai/).
    Image reproduced with permission from MLCommons.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Today, there are dozens of MLPerf-like benchmarks that are managed by [MLCommons](https://mlcommons.org/en/),
    including:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**MLPerf Training**](https://mlcommons.org/en/training-normal-30/)**.** For
    benchmarking system performance while training a machine learning model (more
    relevant to researchers).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLPerf Inference.** For benchmarking system performance while performing
    inference of a machine learning model (more relevant to companies hosting models
    through the cloud). There are multiple versions of MLPerf Inference focusing on
    [datacenters](https://mlcommons.org/en/inference-datacenter-31/), [mobile devices](https://mlcommons.org/en/inference-mobile-30/),
    [edge devices](https://mlcommons.org/en/inference-edge-31/), and [tiny machine
    learning devices](https://mlcommons.org/en/inference-tiny-11/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**MLPerf Training HPC**](https://mlcommons.org/en/training-hpc-20/)**.** For
    benchmarking workloads relevant to high-performance computing systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**MLPerf Storage**](https://mlcommons.org/en/storage-results-05/)**.** For
    benchmarking worloads relevant to storage systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But MLPerf isn’t without its critics. As with any benchmark that gains prominence,
    there are concerns about “overfitting” to benchmarks, where designs excessively
    optimize for the benchmark tests at the potential cost of real-world applicability.
    Moreover, there’s the ever-present challenge of ensuring that benchmarks remain
    relevant, updating them to reflect the rapid advancements in the ML field.
  prefs: []
  type: TYPE_NORMAL
- en: 'Still, the story of MLPerf, much like its predecessors, underscores a fundamental
    truth: **benchmarks catalyze progress**. They don’t just measure the state of
    the art; they shape it. By setting clear, challenging targets, they focus collective
    energies, driving industries and research communities to break new grounds. And,
    in a world where AI continues to redefine what is possible, having a compass to
    navigate its complexities becomes not just desirable, but essential.'
  prefs: []
  type: TYPE_NORMAL
- en: The Challenge of Benchmarking Generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Other than AI hardware, [large language models](https://en.wikipedia.org/wiki/Large_language_model),
    a form of generative AI, are a key focus of benchmarking efforts. More generally
    referred to as [**foundational models**](https://en.wikipedia.org/wiki/Foundation_models),
    these are more difficult to benchmark than hardware or even many other types of
    machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: This is because the success of a language model doesn’t hinge solely on raw
    computational speed or accuracy in narrowly defined tasks. Instead, it rests on
    the model’s ability to generate coherent, contextually relevant, and informative
    responses across a wide variety of prompts and contexts. Furthermore, evaluating
    the “quality” of a response is inherently subjective and can vary based on the
    application or the biases of the evaluator. Given the complexities, benchmarks
    for language models like [GPT-3](https://en.wikipedia.org/wiki/GPT-3) or [BERT](https://en.wikipedia.org/wiki/BERT_(language_model))
    must be more diverse and multifaceted than traditional benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most well-known benchmarks for language models is the [General Language
    Understanding Evaluation (GLUE)](https://gluebenchmark.com/) benchmark, developed
    in 2018\. GLUE wasn’t just a single task; it was a collection of nine diverse
    language tasks, ranging from sentiment analysis to textual entailment. The idea
    was to provide a comprehensive evaluation, ensuring models were not just excelling
    at one task but were genuinely capable of understanding language across various
    challenges.
  prefs: []
  type: TYPE_NORMAL
- en: The impact of GLUE was immediate and profound. For the first time, there was
    a clear, consistent benchmark against which language models could be evaluated.
    Soon, tech giants and academia alike were participating, each vying for the top
    spot on the GLUE leaderboard.
  prefs: []
  type: TYPE_NORMAL
- en: When [GPT-2](https://en.wikipedia.org/wiki/GPT-2) was first evaluated against
    the GLUE benchmark, it secured a then-astounding score that surpassed many models.
    This wasn’t just a testament to GPT-2’s prowess but underscored the value of GLUE
    in providing a clear measurement stick. The ability to claim “*state-of-the-art
    on GLUE*” became a coveted recognition in the community.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, GLUE’s success was a double-edged sword. By late 2019, many models
    had begun to saturate GLUE’s leaderboard, with scores nearing the human baseline.
    This saturation highlighted another critical aspect of benchmarking: **the need
    for benchmarks to evolve with the field**. To address this, the same team introduced
    [SuperGLUE](https://super.gluebenchmark.com/), a tougher benchmark designed to
    push the boundaries further.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Benchmarks like [GLUE](https://gluebenchmark.com/), [SuperGLUE](https://super.gluebenchmark.com/),
    and [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) are used to evaluate
    models on specific tasks such as sentiment analysis and question answering. But
    these benchmarks only scratch the surface of what foundational models aim to achieve.
    Beyond task-specific accuracy, other dimensions have emerged to assess these models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Robustness.** How well does the model handle edge cases or adversarial inputs?
    Robustness benchmarks challenge models with inputs designed to confuse or mislead
    them, gauging their resilience against malicious actors or unexpected scenarios.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Generalization and Transfer Learning.** Foundational models are expected
    to perform well on tasks they haven’t been explicitly trained for. Evaluating
    a model’s zero-shot or few-shot learning capabilities, where it is given tasks
    with minimal to no prior examples, is crucial to understand its flexibility and
    adaptability.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Interactivity and Coherence.** For applications like chatbots or virtual
    assistants, how consistently and coherently a model responds over extended interactions
    is vital. Benchmarks in this space might involve long dialogues or maintaining
    context over multiple exchanges.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Safety and Controllability.** With increasing model sizes, these benchmarks
    ensure that models do not produce harmful, inappropriate, or nonsensical outputs
    is essential.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Customizability.** As foundational models become more widespread, there’s
    a growing need for them to be tailored to specific domains or applications. Benchmarks
    in this area might evaluate how well a model can be fine-tuned on a new dataset
    or adapt to a particular industry’s jargon and nuances.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An interesting development is that as the performance of language models moves
    closer to human performance, tests that have historically been used to assess
    human performance are now being used as benchmarks for language models. For instance,
    GPT-4 was tested on exams like the SAT, LSAT, and medical boards. On the SAT,
    it scored 1410, ranking in the top 6% nationally. GPT-4 was even able to pass
    all versions of the medical board exams, with a mean score of 80.7%. However,
    for the LSAT, it scored lower with 148 and 157, placing it in the 37th and 70th
    percentiles.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e5309b9f92dc3ff7ddeb907911c67478.png)'
  prefs: []
  type: TYPE_IMG
- en: 'GPT performance on academic and professional exams. Figure from the “[*GPT-4
    Technical Report*](https://arxiv.org/abs/2303.08774)”. Image source: [OpenAI](https://arxiv.org/abs/2303.08774)
    (CC-BY 4.0).'
  prefs: []
  type: TYPE_NORMAL
- en: It will be interesting to see how benchmarking approaches continue to develop
    for language models as they begin to rival and exceed human performance in many
    areas.
  prefs: []
  type: TYPE_NORMAL
- en: The Future of Benchmarking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The future of benchmarking is evolving rapidly, diversifying to address the
    broad spectrum of emerging technologies and applications. Here are some examples
    of emerging areas where benchmarking is being implemented:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**RobotPerf**](https://accelerationrobotics.com/pdf/robotperf.pdf): As robotics
    becomes more integrated into our daily lives, benchmarks like RobotPerf are being
    crafted to specifically measure and accelerate robotics applications, ensuring
    that machines meet both efficiency and safety standards.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**NeuroBench**](https://arxiv.org/abs/2304.04640): In the realm of brain-inspired
    computing, NeuroBench is pioneering the way in assessing neuromorphic systems,
    offering insights into how closely these architectures mimic neural processes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**XRBench**](https://arxiv.org/abs/2211.08675): The virtual and augmented
    reality sectors have seen a resurgence with Meta and Apple entering the space
    with new hardware. To this end, XRBench was developed to focus on Extended Reality
    (XR) applications, vital for an immersive and seamless user experience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**MAVBench**](https://arxiv.org/abs/1905.06388): As drones become more and
    more commercially relevant through advances in multi-agent systems and battery
    technology, benchmarks like MAVbench will play an important role in optimizing
    the performance of these systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The computer science and machine learning communities are well aware of the
    importance of benchmarking for driving progress in their fields. Now, even [NeurIPS](https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems),
    one of the flagship AI conferences, has dedicated a [track](https://nips.cc/Conferences/2023/CallForDatasetsBenchmarks)
    solely for datasets and benchmarks. Now in its third year, this track is gaining
    immense momentum, reflected in the staggering number of close to 1,000 submissions
    this year alone. This trend underscores that, as technology continues its relentless
    march, benchmarks will continue to guide and shape its trajectory in real time,
    as it has done before.
  prefs: []
  type: TYPE_NORMAL
- en: Concluding Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The role of benchmarks in shaping progress, whether in athletics or AI, cannot
    be overstated. They act both as mirrors, reflecting the current state of affairs,
    and windows, offering a glimpse into future potentials. As AI continues to influence
    diverse applications and industries, from healthcare to finance, having robust
    benchmarks becomes crucial. They ensure that progress is not just rapid but also
    meaningful, steering efforts towards challenges that matter. As Sir Roger Bannister
    showed us with his four-minute mile, sometimes the most daunting benchmarks, once
    conquered, can unleash waves of innovation and inspiration for years to come.
    In the world of machine learning and computing, the race is far from over.
  prefs: []
  type: TYPE_NORMAL
