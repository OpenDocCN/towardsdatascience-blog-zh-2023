- en: 'The Olympics of AI: Benchmarking Machine Learning Systems'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-olympics-of-ai-benchmarking-machine-learning-systems-c4b2051fbd2b?source=collection_archive---------2-----------------------#2023-09-22](https://towardsdatascience.com/the-olympics-of-ai-benchmarking-machine-learning-systems-c4b2051fbd2b?source=collection_archive---------2-----------------------#2023-09-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How do benchmarks birth breakthroughs?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@matthew_stewart?source=post_page-----c4b2051fbd2b--------------------------------)[![Matthew
    Stewart, PhD](../Images/67da9fb3c0d2516ece297f2729aa0ce8.png)](https://medium.com/@matthew_stewart?source=post_page-----c4b2051fbd2b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c4b2051fbd2b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c4b2051fbd2b--------------------------------)
    [Matthew Stewart, PhD](https://medium.com/@matthew_stewart?source=post_page-----c4b2051fbd2b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb89dbc0712c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-olympics-of-ai-benchmarking-machine-learning-systems-c4b2051fbd2b&user=Matthew+Stewart%2C+PhD&userId=b89dbc0712c4&source=post_page-b89dbc0712c4----c4b2051fbd2b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c4b2051fbd2b--------------------------------)
    ·13 min read·Sep 22, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc4b2051fbd2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-olympics-of-ai-benchmarking-machine-learning-systems-c4b2051fbd2b&user=Matthew+Stewart%2C+PhD&userId=b89dbc0712c4&source=-----c4b2051fbd2b---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc4b2051fbd2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-olympics-of-ai-benchmarking-machine-learning-systems-c4b2051fbd2b&source=-----c4b2051fbd2b---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: You can’t improve what you don’t measure. — ***Peter Drucker***
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/c81835eea9541f2e21572c7a05d610e6.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
- en: The Olympic rings. Image created by the author.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'The Four-Minute Mile: The Benchmark that Redefined Running'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For years, running a mile in under four minutes was considered not just a daunting
    challenge, but by many an impossible feat. It was a psychological and physical
    benchmark that many thought was unattainable. Doctors and sports experts theorized
    that the human body was not capable of running that fast for that long. This belief
    was so ingrained that some even suggested attempting to do so could be fatal.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '[Sir Roger Bannister](https://en.wikipedia.org/wiki/Roger_Bannister), a British
    middle-distance runner and medical student, thought differently. While he recognized
    the challenge, he believed that the barrier was more psychological than physiological.
    Bannister took a scientific approach to his training, breaking down the mile into
    sections and rigorously timing each one. He also employed a rigorous training
    regimen based on interval training and set smaller benchmarks for himself in the
    lead-up to his record attempt.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[罗杰·班尼斯特爵士](https://en.wikipedia.org/wiki/Roger_Bannister)，一位英国中长跑运动员和医学生，持不同观点。他虽然认识到挑战，但认为障碍更多的是心理上的而非生理上的。班尼斯特采取了科学的方法进行训练，将一英里分成几个部分，并严格计时。他还采用了基于间歇训练的严格训练计划，并在尝试创纪录之前为自己设定了较小的基准。'
- en: On May 6, 1954, at a track in Oxford, England, with the help of his friends
    Chris Brasher and Chris Chataway as pacemakers, Bannister made his attempt to
    break the four-minute barrier. He completed the mile in 3 minutes 59.4 seconds,
    shattering the threshold and making history.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 1954年5月6日，在英格兰牛津的一条跑道上，在朋友克里斯·布拉舍尔和克里斯·查塔维作为领跑者的帮助下，班尼斯特尝试突破四分钟障碍。他以3分钟59.4秒完成了一英里，打破了这一门槛，创造了历史。
- en: '![](../Images/0b0f7dde05850ea47428dd61ad729d7e.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0b0f7dde05850ea47428dd61ad729d7e.png)'
- en: 'Roger Bannister running during a race. Image Source: [Norske Leksikon](https://media.snl.no/media/234992/standard_compressed_NL-HaNA_2.24.01.09_0_902-9145-groot.jpg)
    (CC-BY 4.0).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 罗杰·班尼斯特在比赛中奔跑。图片来源：[挪威百科全书](https://media.snl.no/media/234992/standard_compressed_NL-HaNA_2.24.01.09_0_902-9145-groot.jpg)
    (CC-BY 4.0)。
- en: The aftermath of Bannister’s achievement was highly unexpected. [Gunder Hägg](https://en.wikipedia.org/wiki/Gunder_H%C3%A4gg)’s
    1945 record (4 minutes 1.4 seconds) had stood for almost a decade before Bannister
    came along. However, once the four-minute mile benchmark was broken, others soon
    followed. Just 46 days after Bannister’s run, [John Landy](https://en.wikipedia.org/wiki/John_Landy)
    finished a mile in 3 minutes 57.9 seconds. Over the next ten years, the record
    was beaten another 5 times. The current record, set by [Hicham El Guerrouj](https://en.wikipedia.org/wiki/Hicham_El_Guerrouj),
    stands at 3 minutes 43.1 seconds.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 班尼斯特成就的后果是高度意外的。[贡德·海格](https://en.wikipedia.org/wiki/Gunder_H%C3%A4gg)的1945年纪录（4分钟1.4秒）保持了近十年才被班尼斯特打破。然而，一旦四分钟英里基准被突破，其他人很快跟随其后。在班尼斯特跑步后的46天，[约翰·兰迪](https://en.wikipedia.org/wiki/John_Landy)完成了一英里，时间为3分钟57.9秒。在接下来的十年里，这一纪录又被打破了5次。目前的纪录由[希沙姆·埃尔·盖鲁吉](https://en.wikipedia.org/wiki/Hicham_El_Guerrouj)创造，时间为3分钟43.1秒。
- en: World record mile times for the period 1900–2000\. Notice the gap between 1945
    and 1954 before Roger Bannister beat the four-minute mile benchmark—otherwise,
    the downward trend is almost linear. Figure created by the author.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 1900年至2000年期间的世界纪录英里时间。注意1945年到1954年之间的间隔，直到罗杰·班尼斯特打破四分钟英里基准——否则，下降趋势几乎是线性的。图由作者创建。
- en: Bannister’s achievement illustrates the power of benchmarks, not just as measures
    of performance but as motivators for change. Once the four-minute “benchmark”
    was broken, it redefined what athletes believed was possible. The barrier was
    as much in the mind as it was on the track.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 班尼斯特的成就展示了基准的力量，不仅作为性能的衡量标准，也作为变革的激励。一旦四分钟的“基准”被打破，它重新定义了运动员认为可能的极限。这个障碍既存在于思想中，也存在于跑道上。
- en: The four-minute mile embodies the transformative power of benchmarks across
    disciplines. Benchmarks provide a way to quantify performance improvements for
    particular tasks, giving us a way to compare ourselves to others. This is the
    entire basis for sporting events such as the Olympics. However, benchmarks are
    only useful if the community they involve can decide on a common goal to be pursued.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 四分钟英里体现了基准在各学科中的变革力量。基准为特定任务的性能改进提供了量化的方法，让我们可以与他人进行比较。这是奥运会等体育赛事的全部基础。然而，基准只有在参与的社区能够确定共同目标时才有用。
- en: In the realm of machine learning and computer science, **benchmarks serve as
    the communal Olympics** — a grand arena where algorithms, systems, and methodologies
    compete, not for medals, but for the pride of advancement and the drive for innovation.
    Just as athletes train for years to shave milliseconds off their time in pursuit
    of Olympic gold, developers and researchers optimize their models and systems
    to improve performance, striving to outperform on established benchmarks.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习和计算机科学领域，**基准测试是社区的奥林匹克**——一个宏大的竞技场，在这里，算法、系统和方法论竞争的不是奖牌，而是进步的自豪感和创新的动力。正如运动员为了奥林匹克金牌而训练多年、争取毫秒级的进步一样，开发者和研究人员优化他们的模型和系统，以提高性能，力求在既定基准测试中超越对手。
- en: The art and science of benchmarking lie in the establishment of that *common
    goal*. It is not merely about setting a task, but ensuring it captures the essence
    of real-world challenges, pushing the boundaries of what is possible while remaining
    relevant and applicable. Poorly chosen benchmarks can lead researchers astray,
    optimizing for tasks that do not translate to improvement in real-world applications.
    A well-designed benchmark can guide a whole community toward breakthroughs that
    redefine a field.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试的艺术与科学在于建立那个*共同目标*。这不仅仅是设定一个任务，而是确保它能够捕捉到现实世界挑战的本质，推动可能性的边界，同时保持相关性和适用性。选择不当的基准测试可能会使研究人员走入歧途，优化的任务无法在现实世界应用中带来改进。一个设计良好的基准测试可以引导整个社区向突破性进展迈进，重新定义一个领域。
- en: Hence, while benchmarks are tools for comparison and competition, their true
    value lies in their ability to unite a community around a shared vision. Much
    like Bannister’s run didn’t just break a record but redefined athletic potential,
    a well-conceptualized benchmark can elevate an entire discipline, shifting paradigms
    and ushering in new eras of innovation.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，虽然基准测试是用于比较和竞争的工具，其真正的价值在于它们能够团结一个共同愿景的社区。就像班尼斯特的跑步不仅打破了纪录，还重新定义了运动潜力一样，一个构思良好的基准测试可以提升整个学科，改变范式并引领创新新时代。
- en: In this article, we will explore the crucial role of benchmarking in advancing
    computer science and machine learning by journeying through its history, discussing
    the latest trends in benchmarking machine learning systems, and seeing how it
    spurs innovation in the hardware sector.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将探讨基准测试在推进计算机科学和机器学习中的关键作用，通过回顾其历史，讨论基准测试机器学习系统的最新趋势，了解它如何在硬件领域推动创新。
- en: 'Benchmarking Computing Systems: SPEC'
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基准测试计算系统：SPEC
- en: 'In the 1980s, as the personal computer revolution was taking off, there was
    a growing need for standardized metrics to compare the performance of different
    computer systems: **a** [**benchmark**](https://en.wikipedia.org/wiki/Benchmark_(computing)).
    Before standardized benchmarks, manufacturers often developed and used their own
    custom benchmarks. These benchmarks tended to highlight their machines’ strengths
    while downplaying their weaknesses. It became clear that a neutral, universally
    accepted benchmark was necessary for comparison.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在1980年代，随着个人计算机革命的兴起，对标准化指标以比较不同计算机系统性能的需求日益增长：**一个** [**基准测试**](https://en.wikipedia.org/wiki/Benchmark_(computing))。在标准化基准测试出现之前，制造商通常会开发并使用他们自己定制的基准测试。这些基准测试往往突出了他们机器的优点，同时淡化了其缺点。显然，需要一个中立且被广泛接受的基准测试来进行比较。
- en: To address this challenge, the [System Performance Evaluation Cooperative](https://www.spec.org/)
    (SPEC) was developed. The members of this organization were hardware vendors,
    researchers, and other stakeholders interested in creating a universal standard
    for benchmarking [central processing units](https://en.wikipedia.org/wiki/Central_processing_unit)
    (CPUs), also commonly referred to as ‘chips’.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这一挑战，[系统性能评估合作组织](https://www.spec.org/)（SPEC）应运而生。该组织的成员包括硬件供应商、研究人员以及其他致力于创建通用基准标准的利益相关者，主要用于对[中央处理单元](https://en.wikipedia.org/wiki/Central_processing_unit)（CPUs），也称为‘芯片’进行基准测试。
- en: SPEC’s first major contribution was the [SPEC89](https://www.spec.org/cpu89/)
    benchmark suite, which was groundbreaking in that it was one of the first attempts
    at an industry-standard CPU benchmark. SPEC’s benchmarks focused on real-world
    applications and computing tasks, aiming to provide metrics that mattered to end-users
    rather than esoteric or niche measurements.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: SPEC的首个重大贡献是[SPEC89](https://www.spec.org/cpu89/)基准测试套件，它在行业标准CPU基准测试中开创了先河。SPEC的基准测试聚焦于实际应用和计算任务，旨在提供对最终用户有意义的指标，而不是那些深奥或小众的测量。
- en: 'However, as the benchmark evolved, an intriguing phenomenon emerged: the so-called
    “**benchmark effect**.” As the SPEC benchmarks became the gold standard for measuring
    CPU performance, CPU designers started optimizing their designs for SPEC’s benchmarks.
    In essence, because the industry had come to value SPEC benchmarks as a measure
    of overall performance, there was a strong incentive for manufacturers to ensure
    their CPUs performed exceptionally well on these tests — even if it meant potentially
    sacrificing performance in non-SPEC tasks.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着基准测试的发展，出现了一个有趣的现象：所谓的“**基准效应**”。随着SPEC基准测试成为测量CPU性能的黄金标准，CPU设计师开始针对SPEC的基准测试优化他们的设计。实质上，由于行业已经将SPEC基准测试视为整体性能的衡量标准，制造商有强烈的动力确保他们的CPU在这些测试中表现出色——即使这可能意味着在非SPEC任务中牺牲性能。
- en: This wasn’t necessarily SPEC’s intention, and it led to a spirited debate within
    the computer science community. Were the benchmarks genuinely representative of
    real-world performance? Or were they driving a form of tunnel vision, where the
    benchmarks became an end unto themselves rather than a means to an end?
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这并非SPEC的初衷，导致了计算机科学社区内部的激烈辩论。这些基准测试是否真正代表了现实世界的性能？还是它们驱动了一种狭隘的视角，使得基准测试成为目的本身而不是实现目标的手段？
- en: Recognizing these challenges, SPEC continually updated its benchmarks over the
    years to stay ahead of the curve and prevent undue optimization. Their benchmark
    suites expanded to cover different domains, from integer and floating-point computation
    to more domain-specific tasks in graphics, file systems, and more.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 认识到这些挑战后，SPEC多年来不断更新其基准测试，以保持领先并防止过度优化。他们的基准测试套件扩展到涵盖不同领域，从整数和浮点计算到更具体领域的任务，如图形、文件系统等。
- en: The story of SPEC and its benchmarks underscores the profound impact that benchmarking
    can have on an entire industry’s direction. The benchmarks didn’t merely measure
    performance — **they influenced it**. It’s a testament to the power of standardization,
    but also a cautionary tale about the unintended consequences that can emerge when
    a single metric becomes the focal point of optimization.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: SPEC及其基准测试的故事强调了基准测试对整个行业方向的深远影响。这些基准测试不仅仅是衡量性能——**它们还影响了性能**。这证明了标准化的力量，但也提醒我们，当一个单一的指标成为优化的焦点时，可能会出现意想不到的后果。
- en: Today, SPEC benchmarks, along with other benchmarks, continue to play a vital
    role in shaping the computer hardware industry and guiding consumers and enterprises
    in their purchasing decisions.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，SPEC基准测试以及其他基准测试继续在塑造计算机硬件行业和指导消费者及企业的购买决策中发挥至关重要的作用。
- en: 'Benchmarking Deep Learning: ImageNet'
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习基准测试：ImageNet
- en: In the late 2000s, [computer vision](https://en.wikipedia.org/wiki/Computer_vision),
    a subfield of AI focused on enabling machines to interpret and make decisions
    based on visual data, was struggling to make progress. Traditional techniques
    had made progress, but they were hitting a performance plateau on many tasks.
    The methods available at the time relied heavily on hand-crafted features, requiring
    experts to meticulously design and select specific features for each task. It
    was a tedious process with many limitations.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在2000年代末，[计算机视觉](https://en.wikipedia.org/wiki/Computer_vision)，一个专注于使机器能够解读和基于视觉数据做出决策的AI子领域，正面临进展缓慢的问题。传统技术有所进展，但在许多任务上已经达到性能平台期。当时的方法严重依赖于手工制作的特征，要求专家精心设计和选择每个任务的特定特征。这是一个繁琐且有许多限制的过程。
- en: Then [ImageNet](https://en.wikipedia.org/wiki/ImageNet) was released, a massive
    visual database initiated by [Dr. Fei-Fei Li](https://en.wikipedia.org/wiki/Fei-Fei_Li)
    and her team. ImageNet provided millions of labeled images spanning thousands
    of categories. The sheer volume of this dataset was unprecedented, and only enabled
    by the ability to crowdsource data labeling through cloud-based approaches like
    [Amazon Mechanical Turk](https://www.mturk.com/). ImageNet was one of the first
    dataset benchmarks — since its release, the ImageNet [paper](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
    has been cited over 50,000 times.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 随后发布了[ImageNet](https://en.wikipedia.org/wiki/ImageNet)，这是由[李飞飞博士](https://en.wikipedia.org/wiki/Fei-Fei_Li)及其团队发起的一个庞大的视觉数据库。ImageNet提供了数百万张标记图像，涵盖了数千个类别。这个数据集的庞大规模前所未有，仅通过[Amazon
    Mechanical Turk](https://www.mturk.com/)等云端方法进行众包数据标记才得以实现。ImageNet是最早的数据集基准之一——自发布以来，ImageNet的[论文](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)已被引用超过50,000次。
- en: '![](../Images/9ac69c468dec86f3bdcfd6971e360b5e.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ac69c468dec86f3bdcfd6971e360b5e.png)'
- en: 'A visual compilation of ImageNet images. Image source: [Gluon](https://cv.gluon.ai/build/examples_datasets/imagenet.html)
    (CC-BY 4.0).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图像Net图像的视觉编译。图像来源：[Gluon](https://cv.gluon.ai/build/examples_datasets/imagenet.html)（CC-BY
    4.0）。
- en: 'But collecting the dataset was just the beginning. In 2010, the [ImageNet Large
    Scale Visual Recognition Challenge](https://www.image-net.org/challenges/LSVRC/)
    (ILSVRC) was launched. The challenge was simple in its objective but daunting
    in its scale: automatically classify an image into one of 1,000 categories. This
    benchmark challenge would provide an objective measure of progress in computer
    vision, on a scale far beyond previous attempts.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 但收集数据集只是开始。2010年，[ImageNet大规模视觉识别挑战](https://www.image-net.org/challenges/LSVRC/)（ILSVRC）启动。挑战的目标简单，但规模庞大：将图像自动分类到1,000个类别中的一个。这个基准挑战将提供计算机视觉进步的客观衡量，超越了以往的尝试。
- en: The initial years saw incremental improvements over traditional methods. However,
    the 2012 challenge witnessed a transformative shift. A team from the University
    of Toronto, led by [Alex Krizhevsky](https://en.wikipedia.org/wiki/Alex_Krizhevsky),
    [Ilya Sutskever](https://en.wikipedia.org/wiki/Ilya_Sutskever), and [Geoffrey
    Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton), introduced a deep convolutional
    neural network (CNN) called “[AlexNet](https://en.wikipedia.org/wiki/AlexNet).”
    Their model achieved an error rate of 15.3%, slashing the previous year’s error
    by nearly half!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 初期几年在传统方法上取得了渐进式的改进。然而，2012年的挑战带来了变革性的转变。由[亚历克斯·克里兹赫夫斯基](https://en.wikipedia.org/wiki/Alex_Krizhevsky)、[伊利亚·苏茨克弗](https://en.wikipedia.org/wiki/Ilya_Sutskever)和[杰弗里·辛顿](https://en.wikipedia.org/wiki/Geoffrey_Hinton)领导的多伦多大学团队推出了一种深度卷积神经网络（CNN），称为“[AlexNet](https://en.wikipedia.org/wiki/AlexNet)”。他们的模型实现了15.3%的错误率，将前一年的错误率几乎削减了一半！
- en: '![](../Images/4d8cdf3f2733a03984fb86feed41e4dd.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d8cdf3f2733a03984fb86feed41e4dd.png)'
- en: 'Error rates on the ImageNet Large-Scale Visual Recognition Challenge. Accuracy
    dramatically improved with the introduction of deep learning in 2012 and continued
    to improve thereafter. Humans perform with an error rate of approximately 5%.
    Image source: [2018 NIH/RSNA/ACR/The Academy Workshop](https://www.researchgate.net/publication/332452649_A_Roadmap_for_Foundational_Research_on_Artificial_Intelligence_in_Medical_Imaging_From_the_2018_NIHRSNAACRThe_Academy_Workshop).
    This image has been reproduced in accordance with the Creative Commons Attribution
    4.0 International License (CC BY 4.0).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ImageNet大规模视觉识别挑战的错误率。自2012年深度学习引入以来，准确率显著提高，并持续改善。人类的错误率约为5%。图像来源：[2018 NIH/RSNA/ACR/The
    Academy Workshop](https://www.researchgate.net/publication/332452649_A_Roadmap_for_Foundational_Research_on_Artificial_Intelligence_in_Medical_Imaging_From_the_2018_NIHRSNAACRThe_Academy_Workshop)。此图像依据知识共享署名4.0国际许可协议（CC
    BY 4.0）进行复制。
- en: What made this possible? Deep learning, and particularly CNNs, had the capability
    to learn features directly from raw pixels, eliminating the need for manual feature
    crafting. Given enough data and computational power, these networks could uncover
    intricate patterns that were far beyond what traditional methods could manage.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 什么使这一切成为可能？深度学习，特别是卷积神经网络（CNN），能够直接从原始像素中学习特征，省去了手动特征设计的需求。只要有足够的数据和计算能力，这些网络可以揭示出传统方法无法处理的复杂模式。
- en: 'The success of AlexNet was a watershed moment in the development of AI. The
    years following 2012 saw deep learning methods dominating the ImageNet challenge,
    driving error rates lower and lower. The clear message from the benchmarks was
    undeniable: deep learning, once a niche area in machine learning, was set to revolutionize
    computer vision.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet的成功是人工智能发展中的一个分水岭。2012年后，深度学习方法主导了ImageNet挑战，驱动错误率不断降低。从基准测试中传达的明确信息是不容忽视的：深度学习，曾经是机器学习中的一个小众领域，现在正准备彻底改变计算机视觉。
- en: And it did more than that. The success in the ILSVRC acted as a catalyst, propelling
    deep learning to the forefront of not just computer vision, but numerous areas
    in AI, from natural language processing to game playing. The challenge underscored
    the potential of deep learning, attracting researchers, funding, and focus to
    the area.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 而且它不仅仅是这样。ILSVRC中的成功作为催化剂，将深度学习推到了计算机视觉以及从自然语言处理到游戏玩耍等众多AI领域的前沿。挑战突显了深度学习的潜力，吸引了研究人员、资金和关注。
- en: By setting a clear, challenging benchmark, the ImageNet challenge played a pivotal
    role in redirecting the trajectory of AI research, leading to the current deep
    learning-driven AI renaissance we witness today.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设定一个明确且具有挑战性的基准，ImageNet挑战在重新调整AI研究轨迹方面发挥了关键作用，促成了我们今天见证的深度学习驱动的AI复兴。
- en: 'Benchmarking Machine Learning Systems: MLPerf'
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习系统的基准测试：MLPerf
- en: 'The transformative impact of benchmarks like SPEC and ImageNet naturally prompts
    the question: What’s next? As deep learning models became increasingly complex,
    so did their computational demands. This shifted attention to another critical
    component — the hardware that powered these models. Enter [MLPerf](https://www.nvidia.com/en-us/data-center/resources/mlperf-benchmarks/).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 像SPEC和ImageNet这样的基准的变革性影响自然引发了一个问题：接下来是什么？随着深度学习模型变得越来越复杂，它们的计算需求也在增加。这将注意力转向了另一个关键组成部分——支撑这些模型的硬件。进入[MLPerf](https://www.nvidia.com/en-us/data-center/resources/mlperf-benchmarks/)。
- en: MLPerf emerged as a collaborative effort involving industry giants and academic
    institutions, with the mission of creating a standard set of benchmarks to measure
    the performance of machine learning hardware, software, and cloud platforms. As
    the name suggests, MLPerf focuses explicitly on machine learning, capturing a
    broad spectrum of tasks ranging from image classification to reinforcement learning.
    The objective was clear — to provide clarity in a field where “best performance”
    claims were becoming commonplace, yet were often based on inconsistent criteria
    or cherry-picked metrics.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: MLPerf作为一个涉及行业巨头和学术机构的合作努力出现，旨在创建一套标准的基准来测量机器学习硬件、软件和云平台的性能。正如名字所示，MLPerf明确专注于机器学习，涵盖从图像分类到强化学习的广泛任务。目标明确——在一个“最佳性能”声明变得司空见惯但往往基于不一致标准或挑选指标的领域中提供清晰度。
- en: The introduction of MLPerf presented the tech industry with a much-needed unified
    yardstick. For academia, it provided a clear performance target, fostering an
    environment where innovation in algorithms could be easily measured and compared.
    For industry, especially hardware manufacturers, it posed both a challenge and
    an opportunity. No longer could a new chip be launched with vague assertions about
    its machine learning performance — there was now a universally accepted benchmark
    that would put any such claims to the test.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: MLPerf的引入为科技行业提供了一个迫切需要的统一标准。对于学术界，它提供了明确的性能目标，促成了一个可以轻松测量和比较算法创新的环境。对于行业，尤其是硬件制造商，它既是挑战也是机会。新芯片不能再以模糊的机器学习性能声明推出——现在有了一个被普遍接受的基准，任何此类声明都将接受考验。
- en: And just like SPEC influenced CPU design, MLPerf began shaping the direction
    of AI hardware. Companies started optimizing their designs with MLPerf benchmarks
    in mind, and it was not just about raw performance. The benchmarks also incorporated
    efficiency metrics, encouraging innovations that delivered not just speed but
    also energy efficiency — a pressing concern in the age of colossal transformer
    models and environmental consciousness. These benchmarks are used routinely by
    big tech companies, such as Nvidia and AMD, to showcase their new hardware.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 就像SPEC影响了CPU设计一样，MLPerf开始塑造AI硬件的方向。公司们开始根据MLPerf基准优化他们的设计，这不仅仅关乎原始性能。基准测试还融入了效率指标，鼓励那些不仅提供速度而且具备能源效率的创新——在巨大的变换模型和环保意识的时代，这是一个迫切关注的问题。这些基准测试被像Nvidia和AMD这样的科技公司日常使用，以展示他们的新硬件。
- en: '![](../Images/463b8524dab68c052df67ae5871ed966.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/463b8524dab68c052df67ae5871ed966.png)'
- en: 'Nvidia H100 normalized performance on MLPerf Inference v3.0 Datacenter vs.
    the previous Nvidia A100 system. As can be seen, the H100 has a 4x speed-up on
    the full-sized large language model BERT compared with the previous generation
    of chip. Image source: [MLCommons](https://mlcommons.org/en/inference-datacenter-30/)
    and [Nvidia Blogs](https://blogs.nvidia.com/blog/2023/04/05/inference-mlperf-ai/).
    Image reproduced with permission from MLCommons.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Nvidia H100在MLPerf Inference v3.0数据中心的标准化性能对比之前的Nvidia A100系统。如图所示，与前一代芯片相比，H100在全尺寸大型语言模型BERT上的速度提高了4倍。图片来源：[MLCommons](https://mlcommons.org/en/inference-datacenter-30/)和[Nvidia
    Blogs](https://blogs.nvidia.com/blog/2023/04/05/inference-mlperf-ai/)。图片经MLCommons许可转载。
- en: 'Today, there are dozens of MLPerf-like benchmarks that are managed by [MLCommons](https://mlcommons.org/en/),
    including:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，有数十种类似MLPerf的基准测试，由[MLCommons](https://mlcommons.org/en/)管理，包括：
- en: '[**MLPerf Training**](https://mlcommons.org/en/training-normal-30/)**.** For
    benchmarking system performance while training a machine learning model (more
    relevant to researchers).'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**MLPerf Training**](https://mlcommons.org/en/training-normal-30/)**.** 用于在训练机器学习模型时评估系统性能（与研究人员相关）。'
- en: '**MLPerf Inference.** For benchmarking system performance while performing
    inference of a machine learning model (more relevant to companies hosting models
    through the cloud). There are multiple versions of MLPerf Inference focusing on
    [datacenters](https://mlcommons.org/en/inference-datacenter-31/), [mobile devices](https://mlcommons.org/en/inference-mobile-30/),
    [edge devices](https://mlcommons.org/en/inference-edge-31/), and [tiny machine
    learning devices](https://mlcommons.org/en/inference-tiny-11/).'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLPerf Inference.** 用于在执行机器学习模型推理时评估系统性能（与通过云托管模型的公司相关）。MLPerf Inference有多个版本，关注[数据中心](https://mlcommons.org/en/inference-datacenter-31/)、[移动设备](https://mlcommons.org/en/inference-mobile-30/)、[边缘设备](https://mlcommons.org/en/inference-edge-31/)和[微型机器学习设备](https://mlcommons.org/en/inference-tiny-11/)。'
- en: '[**MLPerf Training HPC**](https://mlcommons.org/en/training-hpc-20/)**.** For
    benchmarking workloads relevant to high-performance computing systems.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**MLPerf Training HPC**](https://mlcommons.org/en/training-hpc-20/)**.** 用于基准测试与高性能计算系统相关的工作负载。'
- en: '[**MLPerf Storage**](https://mlcommons.org/en/storage-results-05/)**.** For
    benchmarking worloads relevant to storage systems.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**MLPerf Storage**](https://mlcommons.org/en/storage-results-05/)**.** 用于基准测试与存储系统相关的工作负载。'
- en: But MLPerf isn’t without its critics. As with any benchmark that gains prominence,
    there are concerns about “overfitting” to benchmarks, where designs excessively
    optimize for the benchmark tests at the potential cost of real-world applicability.
    Moreover, there’s the ever-present challenge of ensuring that benchmarks remain
    relevant, updating them to reflect the rapid advancements in the ML field.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 但MLPerf并非没有批评者。正如任何获得显著关注的基准测试一样，存在对“过拟合”基准测试的担忧，即设计过度优化基准测试，可能以牺牲实际应用性为代价。此外，还面临着确保基准测试保持相关性的挑战，及时更新以反映机器学习领域的快速进展。
- en: 'Still, the story of MLPerf, much like its predecessors, underscores a fundamental
    truth: **benchmarks catalyze progress**. They don’t just measure the state of
    the art; they shape it. By setting clear, challenging targets, they focus collective
    energies, driving industries and research communities to break new grounds. And,
    in a world where AI continues to redefine what is possible, having a compass to
    navigate its complexities becomes not just desirable, but essential.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，MLPerf的故事，与其前辈一样，强调了一个基本真理：**基准测试催化进步**。它们不仅仅衡量最先进的技术；它们塑造它。通过设定明确而具有挑战性的目标，它们集中集体的精力，推动行业和研究社区开拓新领域。在一个人工智能不断重新定义可能性的世界里，拥有一个指引其复杂性的指南针不仅仅是可取的，而是必不可少的。
- en: The Challenge of Benchmarking Generative AI
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成性AI的基准测试挑战
- en: Other than AI hardware, [large language models](https://en.wikipedia.org/wiki/Large_language_model),
    a form of generative AI, are a key focus of benchmarking efforts. More generally
    referred to as [**foundational models**](https://en.wikipedia.org/wiki/Foundation_models),
    these are more difficult to benchmark than hardware or even many other types of
    machine learning models.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 除了AI硬件，[大型语言模型](https://en.wikipedia.org/wiki/Large_language_model)，一种生成性AI，是基准测试工作的关键重点。更普遍地称为[**基础模型**](https://en.wikipedia.org/wiki/Foundation_models)，这些比硬件或其他类型的机器学习模型更难以基准测试。
- en: This is because the success of a language model doesn’t hinge solely on raw
    computational speed or accuracy in narrowly defined tasks. Instead, it rests on
    the model’s ability to generate coherent, contextually relevant, and informative
    responses across a wide variety of prompts and contexts. Furthermore, evaluating
    the “quality” of a response is inherently subjective and can vary based on the
    application or the biases of the evaluator. Given the complexities, benchmarks
    for language models like [GPT-3](https://en.wikipedia.org/wiki/GPT-3) or [BERT](https://en.wikipedia.org/wiki/BERT_(language_model))
    must be more diverse and multifaceted than traditional benchmarks.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为语言模型的成功不仅仅依赖于原始计算速度或在狭义任务中的准确性。相反，它取决于模型在各种提示和上下文中生成连贯、具有上下文相关性和信息性的回应的能力。此外，评估回应的“质量”本质上是主观的，并可能根据应用程序或评估者的偏见而有所不同。鉴于复杂性，像[GPT-3](https://en.wikipedia.org/wiki/GPT-3)或[BERT](https://en.wikipedia.org/wiki/BERT_(language_model))这样的语言模型的基准必须比传统基准更加多样和多方面。
- en: One of the most well-known benchmarks for language models is the [General Language
    Understanding Evaluation (GLUE)](https://gluebenchmark.com/) benchmark, developed
    in 2018\. GLUE wasn’t just a single task; it was a collection of nine diverse
    language tasks, ranging from sentiment analysis to textual entailment. The idea
    was to provide a comprehensive evaluation, ensuring models were not just excelling
    at one task but were genuinely capable of understanding language across various
    challenges.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最著名的语言模型基准之一是[通用语言理解评估（GLUE）](https://gluebenchmark.com/)，该基准于2018年开发。GLUE不仅仅是一个单一任务；它是一个包含九种多样化语言任务的集合，从情感分析到文本蕴含。其目的是提供全面的评估，确保模型不仅仅在一个任务上表现出色，而是真正能够在各种挑战中理解语言。
- en: The impact of GLUE was immediate and profound. For the first time, there was
    a clear, consistent benchmark against which language models could be evaluated.
    Soon, tech giants and academia alike were participating, each vying for the top
    spot on the GLUE leaderboard.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: GLUE的影响立竿见影。首次出现了一个明确、一致的基准，用于评估语言模型。不久之后，科技巨头和学术界也纷纷参与，每个参与者争夺GLUE排行榜的首位。
- en: When [GPT-2](https://en.wikipedia.org/wiki/GPT-2) was first evaluated against
    the GLUE benchmark, it secured a then-astounding score that surpassed many models.
    This wasn’t just a testament to GPT-2’s prowess but underscored the value of GLUE
    in providing a clear measurement stick. The ability to claim “*state-of-the-art
    on GLUE*” became a coveted recognition in the community.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当[GPT-2](https://en.wikipedia.org/wiki/GPT-2)首次在GLUE基准上进行评估时，它取得了当时令人惊叹的成绩，超过了许多模型。这不仅证明了GPT-2的强大能力，还强调了GLUE在提供明确测量标准方面的价值。能够声称“*在GLUE上达到最先进水平*”成为了社区中备受追捧的认可。
- en: 'However, GLUE’s success was a double-edged sword. By late 2019, many models
    had begun to saturate GLUE’s leaderboard, with scores nearing the human baseline.
    This saturation highlighted another critical aspect of benchmarking: **the need
    for benchmarks to evolve with the field**. To address this, the same team introduced
    [SuperGLUE](https://super.gluebenchmark.com/), a tougher benchmark designed to
    push the boundaries further.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，GLUE的成功是把双刃剑。到2019年底，许多模型开始在GLUE的排行榜上达到饱和，分数接近人类基线。这种饱和突出了基准测试的另一个关键方面：**基准需要随着领域的发展而进化**。为了解决这个问题，同一团队推出了[SuperGLUE](https://super.gluebenchmark.com/)，这是一个更为严格的基准，旨在进一步推动边界。
- en: 'Benchmarks like [GLUE](https://gluebenchmark.com/), [SuperGLUE](https://super.gluebenchmark.com/),
    and [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) are used to evaluate
    models on specific tasks such as sentiment analysis and question answering. But
    these benchmarks only scratch the surface of what foundational models aim to achieve.
    Beyond task-specific accuracy, other dimensions have emerged to assess these models:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 像[GLUE](https://gluebenchmark.com/)、[SuperGLUE](https://super.gluebenchmark.com/)和[SQuAD](https://rajpurkar.github.io/SQuAD-explorer/)这样的基准被用来评估模型在情感分析和问答等特定任务上的表现。但这些基准只是触及了基础模型旨在实现的目标的表面。除了任务特定的准确性，出现了其他维度来评估这些模型：
- en: '**Robustness.** How well does the model handle edge cases or adversarial inputs?
    Robustness benchmarks challenge models with inputs designed to confuse or mislead
    them, gauging their resilience against malicious actors or unexpected scenarios.'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**鲁棒性。** 模型如何处理边缘情况或对抗性输入？鲁棒性基准测试通过设计旨在困惑或误导模型的输入来挑战模型，评估它们对恶意行为者或意外情况的抵御能力。'
- en: '**Generalization and Transfer Learning.** Foundational models are expected
    to perform well on tasks they haven’t been explicitly trained for. Evaluating
    a model’s zero-shot or few-shot learning capabilities, where it is given tasks
    with minimal to no prior examples, is crucial to understand its flexibility and
    adaptability.'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**泛化能力和迁移学习。** 基础模型应能在未被明确训练的任务上表现良好。评估模型的零样本或少样本学习能力，即在几乎没有先前示例的情况下完成任务，对于了解其灵活性和适应性至关重要。'
- en: '**Interactivity and Coherence.** For applications like chatbots or virtual
    assistants, how consistently and coherently a model responds over extended interactions
    is vital. Benchmarks in this space might involve long dialogues or maintaining
    context over multiple exchanges.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**交互性和连贯性。** 对于聊天机器人或虚拟助手等应用，模型在长时间互动中的一致性和连贯性至关重要。该领域的基准测试可能涉及长对话或在多个交流中保持上下文。'
- en: '**Safety and Controllability.** With increasing model sizes, these benchmarks
    ensure that models do not produce harmful, inappropriate, or nonsensical outputs
    is essential.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**安全性和可控性。** 随着模型规模的增加，这些基准测试确保模型不会产生有害、不适当或无意义的输出是至关重要的。'
- en: '**Customizability.** As foundational models become more widespread, there’s
    a growing need for them to be tailored to specific domains or applications. Benchmarks
    in this area might evaluate how well a model can be fine-tuned on a new dataset
    or adapt to a particular industry’s jargon and nuances.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**可定制性。** 随着基础模型的广泛应用，对其进行特定领域或应用的定制需求日益增加。该领域的基准测试可能会评估模型在新数据集上的微调能力或适应特定行业术语和细微差别的效果。'
- en: An interesting development is that as the performance of language models moves
    closer to human performance, tests that have historically been used to assess
    human performance are now being used as benchmarks for language models. For instance,
    GPT-4 was tested on exams like the SAT, LSAT, and medical boards. On the SAT,
    it scored 1410, ranking in the top 6% nationally. GPT-4 was even able to pass
    all versions of the medical board exams, with a mean score of 80.7%. However,
    for the LSAT, it scored lower with 148 and 157, placing it in the 37th and 70th
    percentiles.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的发展是，随着语言模型的表现越来越接近人类表现，历史上用于评估人类表现的测试现在也被用作语言模型的基准测试。例如，GPT-4在SAT、LSAT和医学考试等考试中进行了测试。在SAT中，它得分1410，排名全国前6%。GPT-4甚至能够通过所有版本的医学考试，平均得分为80.7%。然而，在LSAT中，它的得分较低，分别为148和157，位于第37和第70百分位。
- en: '![](../Images/e5309b9f92dc3ff7ddeb907911c67478.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e5309b9f92dc3ff7ddeb907911c67478.png)'
- en: 'GPT performance on academic and professional exams. Figure from the “[*GPT-4
    Technical Report*](https://arxiv.org/abs/2303.08774)”. Image source: [OpenAI](https://arxiv.org/abs/2303.08774)
    (CC-BY 4.0).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: GPT在学术和专业考试中的表现。图源自“[*GPT-4技术报告*](https://arxiv.org/abs/2303.08774)” 。图像来源：[OpenAI](https://arxiv.org/abs/2303.08774)
    (CC-BY 4.0)。
- en: It will be interesting to see how benchmarking approaches continue to develop
    for language models as they begin to rival and exceed human performance in many
    areas.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 观察基准测试方法如何继续发展以应对语言模型在许多领域逐渐与人类表现相媲美甚至超越人类，将会很有趣。
- en: The Future of Benchmarking
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基准测试的未来
- en: 'The future of benchmarking is evolving rapidly, diversifying to address the
    broad spectrum of emerging technologies and applications. Here are some examples
    of emerging areas where benchmarking is being implemented:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试的未来正在快速发展，日益多样化，以应对新兴技术和应用的广泛范围。以下是一些正在实施基准测试的新兴领域示例：
- en: '[**RobotPerf**](https://accelerationrobotics.com/pdf/robotperf.pdf): As robotics
    becomes more integrated into our daily lives, benchmarks like RobotPerf are being
    crafted to specifically measure and accelerate robotics applications, ensuring
    that machines meet both efficiency and safety standards.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**RobotPerf**](https://accelerationrobotics.com/pdf/robotperf.pdf)：随着机器人技术在我们日常生活中越来越多地集成，像RobotPerf这样的基准测试正在被设计用来专门衡量和加速机器人应用，确保机器在效率和安全标准上达到要求。'
- en: '[**NeuroBench**](https://arxiv.org/abs/2304.04640): In the realm of brain-inspired
    computing, NeuroBench is pioneering the way in assessing neuromorphic systems,
    offering insights into how closely these architectures mimic neural processes.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**NeuroBench**](https://arxiv.org/abs/2304.04640)：在脑启发计算领域，NeuroBench在评估类脑系统方面开创了先河，提供了这些架构在多大程度上模拟神经过程的见解。'
- en: '[**XRBench**](https://arxiv.org/abs/2211.08675): The virtual and augmented
    reality sectors have seen a resurgence with Meta and Apple entering the space
    with new hardware. To this end, XRBench was developed to focus on Extended Reality
    (XR) applications, vital for an immersive and seamless user experience.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**XRBench**](https://arxiv.org/abs/2211.08675)：虚拟现实和增强现实领域随着 Meta 和 Apple
    进入这一领域并推出新硬件而复苏。为此，XRBench 被开发以专注于扩展现实（XR）应用，这对于提供沉浸式和无缝的用户体验至关重要。'
- en: '[**MAVBench**](https://arxiv.org/abs/1905.06388): As drones become more and
    more commercially relevant through advances in multi-agent systems and battery
    technology, benchmarks like MAVbench will play an important role in optimizing
    the performance of these systems.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**MAVBench**](https://arxiv.org/abs/1905.06388)：随着无人机在多智能体系统和电池技术进步下变得越来越具有商业相关性，像
    MAVBench 这样的基准测试将在优化这些系统的性能方面发挥重要作用。'
- en: The computer science and machine learning communities are well aware of the
    importance of benchmarking for driving progress in their fields. Now, even [NeurIPS](https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems),
    one of the flagship AI conferences, has dedicated a [track](https://nips.cc/Conferences/2023/CallForDatasetsBenchmarks)
    solely for datasets and benchmarks. Now in its third year, this track is gaining
    immense momentum, reflected in the staggering number of close to 1,000 submissions
    this year alone. This trend underscores that, as technology continues its relentless
    march, benchmarks will continue to guide and shape its trajectory in real time,
    as it has done before.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学和机器学习社区对基准测试在推动领域进步中的重要性深有认识。现在，连 [NeurIPS](https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems)，作为旗舰
    AI 会议之一，也专门设置了一个 [track](https://nips.cc/Conferences/2023/CallForDatasetsBenchmarks)，专注于数据集和基准测试。现在已进入第三年，这个
    track 正在获得巨大的动力，今年单是提交的数量就接近 1,000 个。这一趋势突显出，随着技术不断前进，基准测试将继续在实时中指导和塑造其轨迹，正如过去所做的那样。
- en: Concluding Thoughts
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结语
- en: The role of benchmarks in shaping progress, whether in athletics or AI, cannot
    be overstated. They act both as mirrors, reflecting the current state of affairs,
    and windows, offering a glimpse into future potentials. As AI continues to influence
    diverse applications and industries, from healthcare to finance, having robust
    benchmarks becomes crucial. They ensure that progress is not just rapid but also
    meaningful, steering efforts towards challenges that matter. As Sir Roger Bannister
    showed us with his four-minute mile, sometimes the most daunting benchmarks, once
    conquered, can unleash waves of innovation and inspiration for years to come.
    In the world of machine learning and computing, the race is far from over.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试在塑造进步方面的作用，无论是在体育还是人工智能领域，都不可低估。它们既是镜子，反映当前的状况，也是窗口，提供未来潜力的瞥见。随着人工智能继续影响从医疗保健到金融等各种应用和行业，拥有强大的基准测试变得至关重要。它们确保进展不仅迅速而且有意义，将努力引向真正重要的挑战。正如
    Sir Roger Bannister 通过他的四分钟一英里向我们展示的那样，有时最令人畏惧的基准，一旦被征服，能够在未来几年释放出创新和灵感的波澜。在机器学习和计算的世界里，这场竞赛远未结束。
