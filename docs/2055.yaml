- en: 4 Ways to Encode Categorical Features with High Cardinality — with Python Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13?source=collection_archive---------0-----------------------#2023-06-26](https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13?source=collection_archive---------0-----------------------#2023-06-26)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn to apply target encoding, count encoding, feature hashing and Embedding
    using scikit-learn and TensorFlow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@aichabokbot?source=post_page-----1bc6d8fd7b13--------------------------------)[![Aicha
    Bokbot](../Images/1aa9ba6ae6296d8be3350b14dba97dd2.png)](https://medium.com/@aichabokbot?source=post_page-----1bc6d8fd7b13--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1bc6d8fd7b13--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1bc6d8fd7b13--------------------------------)
    [Aicha Bokbot](https://medium.com/@aichabokbot?source=post_page-----1bc6d8fd7b13--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F50566ce7e21&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13&user=Aicha+Bokbot&userId=50566ce7e21&source=post_page-50566ce7e21----1bc6d8fd7b13---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1bc6d8fd7b13--------------------------------)
    ·9 min read·Jun 26, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1bc6d8fd7b13&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13&user=Aicha+Bokbot&userId=50566ce7e21&source=-----1bc6d8fd7b13---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1bc6d8fd7b13&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13&source=-----1bc6d8fd7b13---------------------bookmark_footer-----------)![](../Images/c1d754d660f384bf0e8be0017f196bbb.png)'
  prefs: []
  type: TYPE_NORMAL
- en: “Click” — Photo by [Cleo Vermij](https://unsplash.com/@cleovermij?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we will go through 4 popular methods to encode categorical
    variables with high cardinality: **(1) Target encoding, (2) Count encoding, (3)
    Feature hashing** and **(4) Embedding**.'
  prefs: []
  type: TYPE_NORMAL
- en: We will explain how each method works, discuss its pros and cons and observe
    its impact on the performance of a classification task.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table of content**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: — [Introducing categorical features](#8744)
  prefs: []
  type: TYPE_NORMAL
- en: '*(1)* [*Why do we need to encode categorical features?*](#2429) *(2)* [*Why
    one-hot encoding is not suited to high cardinality?*](#b13b)'
  prefs: []
  type: TYPE_NORMAL
- en: — [Application on an AdTech dataset](#706a)
  prefs: []
  type: TYPE_NORMAL
- en: — [Overview of each encoding method](#a959)
  prefs: []
  type: TYPE_NORMAL
- en: '*(1)* [*Target encoding*](#ffbc) *(2)* [*Count encoding*](#fbd1) *(3)* [*Feature
    hashing*](#e278) *(4)* [*Embedding*](#99d8)'
  prefs: []
  type: TYPE_NORMAL
- en: — [Benchmarking the performance to predict CTR](#892c)
  prefs: []
  type: TYPE_NORMAL
- en: — [Conclusion](#033c)
  prefs: []
  type: TYPE_NORMAL
- en: — [To go further](#3bd1)
  prefs: []
  type: TYPE_NORMAL
- en: '**Introducing categorical features**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Categorical features** are a type of variables that describe categories or
    groups (e.g. gender, color, country), as opposed to **numerical features** that
    measure a quantity (e.g. age, height, temperature).'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of categorical data: **ordinal features** which categories
    can be ranked and sorted (e.g. sizes of T-shirt or restaurant ratings from 1 to
    5 star) and **nominal features** which categories don’t imply any meaningful order
    (e.g. name of a person, of a city).'
  prefs: []
  type: TYPE_NORMAL
- en: Why do we need to encode categorical features?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Encoding a categorical variable means finding a mapping that converts a category
    to a numerical value.
  prefs: []
  type: TYPE_NORMAL
- en: While some algorithms can work with categorical data directly (like decision
    trees), **most machine learning models cannot handle categorical features** and
    were designed to operate with numerical data only. Encoding categorical variables
    is a necessary step.
  prefs: []
  type: TYPE_NORMAL
- en: Besides, some machine learning libraries require all data to be numerical. This
    is the case of scikit-learn for example.
  prefs: []
  type: TYPE_NORMAL
- en: Why one-hot encoding is not suited to high cardinality?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A common approach to encoding categorical features is to apply **one-hot encoding**.
    This method encodes categorical variables by adding one binary variable for each
    unique category.
  prefs: []
  type: TYPE_NORMAL
- en: If a feature describing colors has three categories [red, blue, green], a one-hot
    encoder would transform it into three binary variables, one for each category.
  prefs: []
  type: TYPE_NORMAL
- en: 'If a categorical feature has hundreds or thousands of categories, applying
    one-hot encoding would add hundreds or thousands of binary variables to the features
    vector. Models struggles with large sparse data as they face the the [curse of
    dimensionality](https://www.quora.com/What-is-the-curse-of-dimensionality): it
    is more difficult to search in a solution space with more dimensions, easier to
    overfit, computational time is increased, as well as space complexity.'
  prefs: []
  type: TYPE_NORMAL
- en: So how to to encode highly cardinal categorical features without increasing
    the dimensionality of feature vectors?
  prefs: []
  type: TYPE_NORMAL
- en: Application on an AdTech dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will answer that question by applying four encoding techniques on the dataset
    of [**Criteo’s Display Advertising Challenge**](https://www.kaggle.com/competitions/criteo-display-ad-challenge/data)
    to predict click-through rates on display ads.
  prefs: []
  type: TYPE_NORMAL
- en: It is a famous Kaggle challenge launched in 2014 by **Criteo**, a French online
    advertising company specialized in programmatic advertising and real time bidding.
    **Click through rate (CTR)** of an ad is the number of times it was clicked divided
    by the number of times it was displayed on a page.
  prefs: []
  type: TYPE_NORMAL
- en: '**Datasets in AdTech usually contain ID variables with high cardinality**,
    such as *site_id* (ID of the website on which an ad is displayed), *advertiser_id*
    (ID of the brand behind the ad), *os_id* (ID of operating system of the user for
    whom the ad is displayed).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Criteo dataset consists of 1 million rows, 39 anonymized columns**: 13
    numerical variables and 26 categorical variables. Their cardinality is in the
    table below. We see that **many features have very high cardinalities (above 10k)**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d673f17f1b6da26bcc979fd333875bc1.png)'
  prefs: []
  type: TYPE_IMG
- en: Cardinaliy of categorical features in the Criteo dataset
  prefs: []
  type: TYPE_NORMAL
- en: The dataset contains 241,338 categories overall. **Applying one hot encoding
    would mean transforming the feature space from 39 dimensions to 241,351 dimensions.**
    It is clear that applying computations to a sparse matrix of over 241k columns
    is very costly and inefficient.
  prefs: []
  type: TYPE_NORMAL
- en: Let us split the dataset into training and testing set and explore the encoding
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Overview of each encoding method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: (1) Target encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We use the target encoder of the library [category_encoder](https://contrib.scikit-learn.org/category_encoders/targetencoder.html),
    which is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Features are replaced with a blend of the expected value of the target given
    particular categorical value and the expected value of the target over all the
    training data.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note that we only fit the encoder on the training dataset and then use the fitted
    encoder to transform both training and testing set. As we do not have access to
    y_test in real life, it would be cheating to use it to fit the encoder.
  prefs: []
  type: TYPE_NORMAL
- en: '**DIMENSION OF ENCODED FEATURES SPACE:** 39 columns, X_train_encoded and X_test_encoded
    have the same shape as x_train and y_train.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PROS:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: -** parameter free
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- no increase in feature space'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**CONS:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: -** risk of *target leakage* (target leakage means using some information from
    target to predict the target itself)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- When categories have few samples, the target encoder would replace them by
    values very close to the target which makes the model prone to overfitting the
    training set'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- does not accept new values in testing set'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2) Count encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With count encoding, **also called Frequency encoding**, categories are replaced
    by their frequency in the dataset. If the ID *3f4ec687* appears 957 times in the
    column C7, then we would replace the *3f4ec687* by 957.
  prefs: []
  type: TYPE_NORMAL
- en: 'If two categories appear the same amount of times in the dataset, such a method
    encodes them with the same value although they do not hold the same information.
    This **creates what we call a collision**: two distinct categories are encoded
    with the same value.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**DIMENSION OF ENCODED FEATURES SPACE:** 39 columns, X_train_encoded and X_test_encoded
    have the same shape as x_train and y_train.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PROS:**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '- easy to understand and implement'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- parameter free'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- no increase in feature space'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**CONS:**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '- risk of information loss when collision happens'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- can be too simplistic (the only information we keep from the categorical
    features is their frequency)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- does not accept new values in testing set'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (3) Feature hashing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Feature hashing **projects categorical features into a feature vector of fixed
    dimension** that is smaller than the original feature space. The dimension of
    that feature vector needs to be defined beforehand. This is done by using the
    **hashing trick which applies a hash function to the features** and uses their
    hash values as indices in the feature vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are **two ways to implement feature hashing**:'
  prefs: []
  type: TYPE_NORMAL
- en: either we **apply hashing feature by feature** (there one feature space per
    feature and we therefore need to choose one dimension parameter per feature)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: or **we hash all features together** (there one single feature space for all
    features, one parameter to choose, but collisions can happen between features).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This method is not parameter free, we need to choose the size of the hashing
    space. We follow the advice of the great article [Don’t get tricked by the hashing
    trick](https://booking.ai/dont-be-tricked-by-the-hashing-trick-192a6aae3087) and
    choose the hashing size = 20**k* where *k* is the number of categorical features
    (in our case *k*=26).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**DIMENSION OF ENCODED FEATURES SPACE:** 533 columns'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PROS:**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '- limited increase of feature space (as compared to one hot encoding)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- does not grow in size and accepts new values during inference as it does
    not maintain a dictionary of observed categories'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- captures interactions between features when feature hashing is applied on
    all categorical features combined to create a single hash'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**CONS:**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '- need to tune the parameter of hashing space dimension'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- risk of collision when the dimension of hashing space is not big enough'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (4) Embedding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Embeddings is a popular encoding technique from Deep Learning and natural language
    processing (NLP). It consists on **building a trainable lookup table that maps
    categories to a a fixed-length vector representation**. During the training, the
    weights in the table are updated to better describe the similarities between categories.
  prefs: []
  type: TYPE_NORMAL
- en: We will follow this [Keras tutorial](https://keras.io/examples/structured_data/classification_with_tfdf/#experiment-3-decision-forests-with-trained-embeddings)
    to “**build an encoder model that codes the categorical features to embeddings**,
    where the size of the embedding for a given categorical feature is the square
    root to the size of its vocabulary. We train these embeddings in a simple NN model
    through back-propagation.”
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**DIMENSION OF ENCODED FEATURES SPACE:** 64 columns'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PROS:**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '- limited increase of feature space (as compared to one hot encoding)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- accepts new values during inference'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- captures interactions between features and learns the similarities between
    categories'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**CONS:**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '- need to tune the parameter of embedding size'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- the embeddings and a logistic regression model cannot be trained synergically
    in one phase, since logistic regression do not train with backpropagation. Rather,
    embeddings has to be trained in an initial phase, and then used as static inputs
    to the decision forest model.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Benchmarking the performance to predict CTR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We fit a simple logistic regression to predict the CTR and generate predictions
    with each encoding method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We compute the log loss, AUC, recall and average of the predict CTR. The results
    are in the table below. **The** **average CTR in the dataset is 22.6%.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b04ea3a2befc95538b580375fb0ffdc1.png)'
  prefs: []
  type: TYPE_IMG
- en: We first notice that the AUC is pretty low for all encoding methods, which is
    mostly due to the fact that we used a very simple model, a more complex model
    than Logistic Regression would be more suited for this task.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first three methods (target, count and hashing encoding) **did not allow
    the model to capture enough signals to predict the CTR**: the average predicted
    CTR is very low compared to the true average, the recall is also very close to
    zero and the AUC close 0.5 informs us that the model is almost random. The **model
    with embedding shows the highest AUC and recall**, and an average predicted CTR
    close to the target.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We’ve explored four techniques to encode categorical data with high cardinality:
    target encoding, count encoding, feature hashing and embedding. Specifically,
    we learned:'
  prefs: []
  type: TYPE_NORMAL
- en: The challenge of working with high-cardinality categorical data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The advantages and limitations of each of the four techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to implement each technique as part of a classification task to predict
    Click Trough Rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To go further
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Notebook with the code in this article: [Encoding high cardinality categorical
    features](https://www.kaggle.com/aichabokbot/encoding-high-cardinality-categorical-features)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheat-sheet on how to choose the most relevant encoding technique: [Encode
    Smarter: How to Easily Integrate Categorical Encoding into Your Machine Learning
    Pipeline](https://innovation.alteryx.com/encode-smarter/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More on the concept of collision: [Don’t be tricked by the Hashing Trick](https://booking.ai/dont-be-tricked-by-the-hashing-trick-192a6aae3087)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More on categorical embedding: [Dense NN with Categorical Embeddings](https://www.kaggle.com/code/dustyturner/dense-nn-with-categorical-embeddings)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
