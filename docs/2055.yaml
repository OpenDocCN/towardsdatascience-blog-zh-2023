- en: 4 Ways to Encode Categorical Features with High Cardinality — with Python Implementation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4种编码具有高基数的分类特征的方法——带Python实现
- en: 原文：[https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13?source=collection_archive---------0-----------------------#2023-06-26](https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13?source=collection_archive---------0-----------------------#2023-06-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13?source=collection_archive---------0-----------------------#2023-06-26](https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13?source=collection_archive---------0-----------------------#2023-06-26)
- en: Learn to apply target encoding, count encoding, feature hashing and Embedding
    using scikit-learn and TensorFlow
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习如何使用scikit-learn和TensorFlow应用目标编码、计数编码、特征哈希和嵌入
- en: '[](https://medium.com/@aichabokbot?source=post_page-----1bc6d8fd7b13--------------------------------)[![Aicha
    Bokbot](../Images/1aa9ba6ae6296d8be3350b14dba97dd2.png)](https://medium.com/@aichabokbot?source=post_page-----1bc6d8fd7b13--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1bc6d8fd7b13--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1bc6d8fd7b13--------------------------------)
    [Aicha Bokbot](https://medium.com/@aichabokbot?source=post_page-----1bc6d8fd7b13--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@aichabokbot?source=post_page-----1bc6d8fd7b13--------------------------------)[![Aicha
    Bokbot](../Images/1aa9ba6ae6296d8be3350b14dba97dd2.png)](https://medium.com/@aichabokbot?source=post_page-----1bc6d8fd7b13--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1bc6d8fd7b13--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1bc6d8fd7b13--------------------------------)
    [Aicha Bokbot](https://medium.com/@aichabokbot?source=post_page-----1bc6d8fd7b13--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F50566ce7e21&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13&user=Aicha+Bokbot&userId=50566ce7e21&source=post_page-50566ce7e21----1bc6d8fd7b13---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1bc6d8fd7b13--------------------------------)
    ·9 min read·Jun 26, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1bc6d8fd7b13&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13&user=Aicha+Bokbot&userId=50566ce7e21&source=-----1bc6d8fd7b13---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F50566ce7e21&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13&user=Aicha+Bokbot&userId=50566ce7e21&source=post_page-50566ce7e21----1bc6d8fd7b13---------------------post_header-----------)
    发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----1bc6d8fd7b13--------------------------------)
    · 9分钟阅读·2023年6月26日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1bc6d8fd7b13&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13&user=Aicha+Bokbot&userId=50566ce7e21&source=-----1bc6d8fd7b13---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1bc6d8fd7b13&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13&source=-----1bc6d8fd7b13---------------------bookmark_footer-----------)![](../Images/c1d754d660f384bf0e8be0017f196bbb.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1bc6d8fd7b13&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13&source=-----1bc6d8fd7b13---------------------bookmark_footer-----------)![](../Images/c1d754d660f384bf0e8be0017f196bbb.png)'
- en: “Click” — Photo by [Cleo Vermij](https://unsplash.com/@cleovermij?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: “点击” — 照片由[Cleo Vermij](https://unsplash.com/@cleovermij?utm_source=medium&utm_medium=referral)提供，发布在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'In this article, we will go through 4 popular methods to encode categorical
    variables with high cardinality: **(1) Target encoding, (2) Count encoding, (3)
    Feature hashing** and **(4) Embedding**.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将深入探讨4种流行的方法来编码具有高基数的分类变量：**（1）目标编码，（2）计数编码，（3）特征哈希** 和 **（4）嵌入**。
- en: We will explain how each method works, discuss its pros and cons and observe
    its impact on the performance of a classification task.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将解释每种方法的工作原理，讨论其优缺点，并观察其对分类任务性能的影响。
- en: '**Table of content**'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**目录**'
- en: — [Introducing categorical features](#8744)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: — [引入类别特征](#8744)
- en: '*(1)* [*Why do we need to encode categorical features?*](#2429) *(2)* [*Why
    one-hot encoding is not suited to high cardinality?*](#b13b)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*(1)* [*为什么我们需要编码类别特征？*](#2429) *(2)* [*为什么一热编码不适用于高基数？*](#b13b)'
- en: — [Application on an AdTech dataset](#706a)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: — [在AdTech数据集上的应用](#706a)
- en: — [Overview of each encoding method](#a959)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: — [每种编码方法概述](#a959)
- en: '*(1)* [*Target encoding*](#ffbc) *(2)* [*Count encoding*](#fbd1) *(3)* [*Feature
    hashing*](#e278) *(4)* [*Embedding*](#99d8)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*(1)* [*目标编码*](#ffbc) *(2)* [*计数编码*](#fbd1) *(3)* [*特征哈希*](#e278) *(4)* [*嵌入*](#99d8)'
- en: — [Benchmarking the performance to predict CTR](#892c)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: — [预测CTR的性能基准测试](#892c)
- en: — [Conclusion](#033c)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: — [结论](#033c)
- en: — [To go further](#3bd1)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: — [进一步阅读](#3bd1)
- en: '**Introducing categorical features**'
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**引入类别特征**'
- en: '**Categorical features** are a type of variables that describe categories or
    groups (e.g. gender, color, country), as opposed to **numerical features** that
    measure a quantity (e.g. age, height, temperature).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**类别特征**是一种描述类别或组的变量（如性别、颜色、国家），而**数值特征**则测量数量（如年龄、身高、温度）。'
- en: 'There are two types of categorical data: **ordinal features** which categories
    can be ranked and sorted (e.g. sizes of T-shirt or restaurant ratings from 1 to
    5 star) and **nominal features** which categories don’t imply any meaningful order
    (e.g. name of a person, of a city).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 类别数据有两种类型：**有序特征**，其类别可以排序（如T恤尺码或餐厅评分从1星到5星），和**名义特征**，其类别不具有任何有意义的顺序（如人的名字、城市名）。
- en: Why do we need to encode categorical features?
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们为什么需要编码类别特征？
- en: Encoding a categorical variable means finding a mapping that converts a category
    to a numerical value.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 编码类别变量意味着找到一种映射，将类别转换为数值。
- en: While some algorithms can work with categorical data directly (like decision
    trees), **most machine learning models cannot handle categorical features** and
    were designed to operate with numerical data only. Encoding categorical variables
    is a necessary step.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然一些算法可以直接处理类别数据（如决策树），**但大多数机器学习模型无法处理类别特征**，它们设计时只处理数值数据。对类别变量进行编码是一个必要的步骤。
- en: Besides, some machine learning libraries require all data to be numerical. This
    is the case of scikit-learn for example.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些机器学习库要求所有数据必须是数值型的。例如，scikit-learn 就是这种情况。
- en: Why one-hot encoding is not suited to high cardinality?
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么一热编码不适用于高基数？
- en: A common approach to encoding categorical features is to apply **one-hot encoding**.
    This method encodes categorical variables by adding one binary variable for each
    unique category.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 编码类别特征的常见方法是应用**一热编码**。这种方法通过为每个唯一类别添加一个二元变量来对类别变量进行编码。
- en: If a feature describing colors has three categories [red, blue, green], a one-hot
    encoder would transform it into three binary variables, one for each category.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个描述颜色的特征有三个类别[红色、蓝色、绿色]，一热编码器将其转换为三个二元变量，每个类别一个。
- en: 'If a categorical feature has hundreds or thousands of categories, applying
    one-hot encoding would add hundreds or thousands of binary variables to the features
    vector. Models struggles with large sparse data as they face the the [curse of
    dimensionality](https://www.quora.com/What-is-the-curse-of-dimensionality): it
    is more difficult to search in a solution space with more dimensions, easier to
    overfit, computational time is increased, as well as space complexity.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个类别特征有数百或数千个类别，应用一热编码将会向特征向量中添加数百或数千个二元变量。模型在面对大规模稀疏数据时会遇到[维度灾难](https://www.quora.com/What-is-the-curse-of-dimensionality)的问题：在更多维度的解空间中搜索更困难，容易过拟合，计算时间增加，以及空间复杂度上升。
- en: So how to to encode highly cardinal categorical features without increasing
    the dimensionality of feature vectors?
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 那么如何在不增加特征向量维度的情况下编码高基数的类别特征呢？
- en: Application on an AdTech dataset
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在AdTech数据集上的应用
- en: We will answer that question by applying four encoding techniques on the dataset
    of [**Criteo’s Display Advertising Challenge**](https://www.kaggle.com/competitions/criteo-display-ad-challenge/data)
    to predict click-through rates on display ads.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过在[**Criteo展示广告挑战赛**](https://www.kaggle.com/competitions/criteo-display-ad-challenge/data)的数据集上应用四种编码技术来预测展示广告的点击率，来回答这个问题。
- en: It is a famous Kaggle challenge launched in 2014 by **Criteo**, a French online
    advertising company specialized in programmatic advertising and real time bidding.
    **Click through rate (CTR)** of an ad is the number of times it was clicked divided
    by the number of times it was displayed on a page.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '**Datasets in AdTech usually contain ID variables with high cardinality**,
    such as *site_id* (ID of the website on which an ad is displayed), *advertiser_id*
    (ID of the brand behind the ad), *os_id* (ID of operating system of the user for
    whom the ad is displayed).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Criteo dataset consists of 1 million rows, 39 anonymized columns**: 13
    numerical variables and 26 categorical variables. Their cardinality is in the
    table below. We see that **many features have very high cardinalities (above 10k)**.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d673f17f1b6da26bcc979fd333875bc1.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: Cardinaliy of categorical features in the Criteo dataset
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: The dataset contains 241,338 categories overall. **Applying one hot encoding
    would mean transforming the feature space from 39 dimensions to 241,351 dimensions.**
    It is clear that applying computations to a sparse matrix of over 241k columns
    is very costly and inefficient.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Let us split the dataset into training and testing set and explore the encoding
    methods.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Overview of each encoding method
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: (1) Target encoding
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We use the target encoder of the library [category_encoder](https://contrib.scikit-learn.org/category_encoders/targetencoder.html),
    which is defined as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Features are replaced with a blend of the expected value of the target given
    particular categorical value and the expected value of the target over all the
    training data.
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that we only fit the encoder on the training dataset and then use the fitted
    encoder to transform both training and testing set. As we do not have access to
    y_test in real life, it would be cheating to use it to fit the encoder.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '**DIMENSION OF ENCODED FEATURES SPACE:** 39 columns, X_train_encoded and X_test_encoded
    have the same shape as x_train and y_train.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PROS:'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: -** parameter free
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- no increase in feature space'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**CONS:'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: -** risk of *target leakage* (target leakage means using some information from
    target to predict the target itself)
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- When categories have few samples, the target encoder would replace them by
    values very close to the target which makes the model prone to overfitting the
    training set'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- does not accept new values in testing set'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2) Count encoding
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With count encoding, **also called Frequency encoding**, categories are replaced
    by their frequency in the dataset. If the ID *3f4ec687* appears 957 times in the
    column C7, then we would replace the *3f4ec687* by 957.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'If two categories appear the same amount of times in the dataset, such a method
    encodes them with the same value although they do not hold the same information.
    This **creates what we call a collision**: two distinct categories are encoded
    with the same value.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**DIMENSION OF ENCODED FEATURES SPACE:** 39 columns, X_train_encoded and X_test_encoded
    have the same shape as x_train and y_train.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码特征空间的维度：** 39 列，X_train_encoded 和 X_test_encoded 的形状与 x_train 和 y_train
    相同。'
- en: '**PROS:**'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优点：**'
- en: '- easy to understand and implement'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 易于理解和实现'
- en: '- parameter free'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 无参数'
- en: '- no increase in feature space'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 特征空间没有增加'
- en: '**CONS:**'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺点：**'
- en: '- risk of information loss when collision happens'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 当发生碰撞时存在信息丢失的风险'
- en: '- can be too simplistic (the only information we keep from the categorical
    features is their frequency)'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 可能过于简化（我们从分类特征中保留的唯一信息是它们的频率）'
- en: '- does not accept new values in testing set'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 在测试集中不接受新值'
- en: (3) Feature hashing
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: (3) 特征哈希
- en: Feature hashing **projects categorical features into a feature vector of fixed
    dimension** that is smaller than the original feature space. The dimension of
    that feature vector needs to be defined beforehand. This is done by using the
    **hashing trick which applies a hash function to the features** and uses their
    hash values as indices in the feature vector.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 特征哈希**将分类特征投影到一个固定维度的特征向量中**，该维度比原始特征空间要小。这个特征向量的维度需要事先定义。这是通过使用**哈希技巧，将哈希函数应用于特征**，并将其哈希值作为特征向量中的索引来完成的。
- en: 'There are **two ways to implement feature hashing**:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 有**两种实现特征哈希的方法**：
- en: either we **apply hashing feature by feature** (there one feature space per
    feature and we therefore need to choose one dimension parameter per feature)
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者**我们逐个特征应用哈希**（每个特征一个特征空间，因此我们需要为每个特征选择一个维度参数）
- en: or **we hash all features together** (there one single feature space for all
    features, one parameter to choose, but collisions can happen between features).
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或**我们将所有特征一起哈希**（所有特征共享一个特征空间，选择一个参数，但特征之间可能会发生碰撞）。
- en: This method is not parameter free, we need to choose the size of the hashing
    space. We follow the advice of the great article [Don’t get tricked by the hashing
    trick](https://booking.ai/dont-be-tricked-by-the-hashing-trick-192a6aae3087) and
    choose the hashing size = 20**k* where *k* is the number of categorical features
    (in our case *k*=26).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法不是无参数的，我们需要选择哈希空间的大小。我们遵循伟大文章 [不要被哈希技巧欺骗](https://booking.ai/dont-be-tricked-by-the-hashing-trick-192a6aae3087)
    的建议，选择哈希大小 = 20**k*，其中 *k* 是分类特征的数量（在我们的例子中 *k*=26）。
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**DIMENSION OF ENCODED FEATURES SPACE:** 533 columns'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码特征空间的维度：** 533 列'
- en: '**PROS:**'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优点：**'
- en: '- limited increase of feature space (as compared to one hot encoding)'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 特征空间的增加有限（与独热编码相比）'
- en: '- does not grow in size and accepts new values during inference as it does
    not maintain a dictionary of observed categories'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 由于不维护观察到的类别的字典，因此在推断过程中不会增长大小并接受新值'
- en: '- captures interactions between features when feature hashing is applied on
    all categorical features combined to create a single hash'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 当特征哈希应用于所有分类特征以创建单个哈希时，捕捉特征之间的交互'
- en: '**CONS:**'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺点：**'
- en: '- need to tune the parameter of hashing space dimension'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 需要调整哈希空间维度的参数'
- en: '- risk of collision when the dimension of hashing space is not big enough'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 当哈希空间的维度不足时存在碰撞风险'
- en: (4) Embedding
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: (4) 嵌入
- en: Embeddings is a popular encoding technique from Deep Learning and natural language
    processing (NLP). It consists on **building a trainable lookup table that maps
    categories to a a fixed-length vector representation**. During the training, the
    weights in the table are updated to better describe the similarities between categories.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是一种流行的编码技术，来自深度学习和自然语言处理（NLP）。它的核心是**构建一个可训练的查找表，将类别映射到固定长度的向量表示**。在训练过程中，查找表中的权重会被更新，以更好地描述类别之间的相似性。
- en: We will follow this [Keras tutorial](https://keras.io/examples/structured_data/classification_with_tfdf/#experiment-3-decision-forests-with-trained-embeddings)
    to “**build an encoder model that codes the categorical features to embeddings**,
    where the size of the embedding for a given categorical feature is the square
    root to the size of its vocabulary. We train these embeddings in a simple NN model
    through back-propagation.”
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遵循这个 [Keras 教程](https://keras.io/examples/structured_data/classification_with_tfdf/#experiment-3-decision-forests-with-trained-embeddings)
    来“**构建一个编码器模型，将分类特征编码为嵌入向量**，其中给定分类特征的嵌入大小是其词汇表大小的平方根。我们通过反向传播在一个简单的神经网络模型中训练这些嵌入向量。”
- en: '[PRE4]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**DIMENSION OF ENCODED FEATURES SPACE:** 64 columns'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码特征空间的维度：** 64 列'
- en: '**PROS:**'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优点：**'
- en: '- limited increase of feature space (as compared to one hot encoding)'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 特征空间的增加有限（与独热编码相比）'
- en: '- accepts new values during inference'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 在推断过程中接受新值'
- en: '- captures interactions between features and learns the similarities between
    categories'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 捕捉特征之间的交互并学习类别之间的相似性'
- en: '**CONS:**'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺点：**'
- en: '- need to tune the parameter of embedding size'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 需要调整嵌入大小的参数'
- en: '- the embeddings and a logistic regression model cannot be trained synergically
    in one phase, since logistic regression do not train with backpropagation. Rather,
    embeddings has to be trained in an initial phase, and then used as static inputs
    to the decision forest model.'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 嵌入和逻辑回归模型不能在一个阶段中协同训练，因为逻辑回归不使用反向传播进行训练。相反，嵌入需要在初始阶段进行训练，然后作为静态输入用于决策森林模型。'
- en: Benchmarking the performance to predict CTR
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测CTR的性能基准
- en: We fit a simple logistic regression to predict the CTR and generate predictions
    with each encoding method.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们拟合了一个简单的逻辑回归模型来预测CTR，并使用每种编码方法生成预测。
- en: '[PRE5]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We compute the log loss, AUC, recall and average of the predict CTR. The results
    are in the table below. **The** **average CTR in the dataset is 22.6%.**
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算了对数损失、AUC、召回率和预测CTR的平均值。结果见下表。**数据集中的平均CTR为22.6%。**
- en: '![](../Images/b04ea3a2befc95538b580375fb0ffdc1.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b04ea3a2befc95538b580375fb0ffdc1.png)'
- en: We first notice that the AUC is pretty low for all encoding methods, which is
    mostly due to the fact that we used a very simple model, a more complex model
    than Logistic Regression would be more suited for this task.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先注意到所有编码方法的AUC都相当低，这主要是因为我们使用了一个非常简单的模型，比逻辑回归更复杂的模型更适合这个任务。
- en: 'The first three methods (target, count and hashing encoding) **did not allow
    the model to capture enough signals to predict the CTR**: the average predicted
    CTR is very low compared to the true average, the recall is also very close to
    zero and the AUC close 0.5 informs us that the model is almost random. The **model
    with embedding shows the highest AUC and recall**, and an average predicted CTR
    close to the target.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 前三种方法（目标编码、计数编码和哈希编码）**未能让模型捕捉到足够的信号以预测CTR**：与真实平均值相比，平均预测CTR非常低，召回率也接近于零，AUC接近0.5则表明模型几乎是随机的。**使用嵌入的模型显示出最高的AUC和召回率**，以及接近目标的平均预测CTR。
- en: Conclusion
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'We’ve explored four techniques to encode categorical data with high cardinality:
    target encoding, count encoding, feature hashing and embedding. Specifically,
    we learned:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了四种编码高基数分类数据的技术：目标编码、计数编码、特征哈希和嵌入。具体来说，我们学到了：
- en: The challenge of working with high-cardinality categorical data
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高基数分类数据处理的挑战
- en: The advantages and limitations of each of the four techniques
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 四种技术的优缺点
- en: How to implement each technique as part of a classification task to predict
    Click Trough Rate
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将每种技术应用于分类任务中以预测点击率
- en: To go further
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Notebook with the code in this article: [Encoding high cardinality categorical
    features](https://www.kaggle.com/aichabokbot/encoding-high-cardinality-categorical-features)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本文代码的笔记本：[编码高基数分类特征](https://www.kaggle.com/aichabokbot/encoding-high-cardinality-categorical-features)
- en: 'Cheat-sheet on how to choose the most relevant encoding technique: [Encode
    Smarter: How to Easily Integrate Categorical Encoding into Your Machine Learning
    Pipeline](https://innovation.alteryx.com/encode-smarter/)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何选择最相关的编码技术的备忘单：[更聪明地编码：如何轻松将分类编码集成到您的机器学习流程中](https://innovation.alteryx.com/encode-smarter/)
- en: 'More on the concept of collision: [Don’t be tricked by the Hashing Trick](https://booking.ai/dont-be-tricked-by-the-hashing-trick-192a6aae3087)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于碰撞的更多信息：[不要被哈希技巧欺骗](https://booking.ai/dont-be-tricked-by-the-hashing-trick-192a6aae3087)
- en: 'More on categorical embedding: [Dense NN with Categorical Embeddings](https://www.kaggle.com/code/dustyturner/dense-nn-with-categorical-embeddings)'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于分类嵌入的更多信息：[带有分类嵌入的密集神经网络](https://www.kaggle.com/code/dustyturner/dense-nn-with-categorical-embeddings)
