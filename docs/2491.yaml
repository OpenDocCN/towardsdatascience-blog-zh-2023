- en: Creating 3D Videos from RGB Videos
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从RGB视频创建3D视频
- en: 原文：[https://towardsdatascience.com/creating-3d-videos-from-rgb-videos-491a09fa1e79?source=collection_archive---------3-----------------------#2023-08-03](https://towardsdatascience.com/creating-3d-videos-from-rgb-videos-491a09fa1e79?source=collection_archive---------3-----------------------#2023-08-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/creating-3d-videos-from-rgb-videos-491a09fa1e79?source=collection_archive---------3-----------------------#2023-08-03](https://towardsdatascience.com/creating-3d-videos-from-rgb-videos-491a09fa1e79?source=collection_archive---------3-----------------------#2023-08-03)
- en: Instructions for generating consistent depthmap and point cloud videos from
    any RGB videos
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成一致的深度图和点云视频的指南
- en: '[](https://medium.com/@berkanzorlubas?source=post_page-----491a09fa1e79--------------------------------)[![Berkan
    Zorlubas](../Images/6d13c115064dfa1bf3918ef009a30797.png)](https://medium.com/@berkanzorlubas?source=post_page-----491a09fa1e79--------------------------------)[](https://towardsdatascience.com/?source=post_page-----491a09fa1e79--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----491a09fa1e79--------------------------------)
    [Berkan Zorlubas](https://medium.com/@berkanzorlubas?source=post_page-----491a09fa1e79--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@berkanzorlubas?source=post_page-----491a09fa1e79--------------------------------)[![Berkan
    Zorlubas](../Images/6d13c115064dfa1bf3918ef009a30797.png)](https://medium.com/@berkanzorlubas?source=post_page-----491a09fa1e79--------------------------------)[](https://towardsdatascience.com/?source=post_page-----491a09fa1e79--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----491a09fa1e79--------------------------------)
    [Berkan Zorlubas](https://medium.com/@berkanzorlubas?source=post_page-----491a09fa1e79--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7d74427941be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-3d-videos-from-rgb-videos-491a09fa1e79&user=Berkan+Zorlubas&userId=7d74427941be&source=post_page-7d74427941be----491a09fa1e79---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----491a09fa1e79--------------------------------)
    ·8 min read·Aug 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F491a09fa1e79&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-3d-videos-from-rgb-videos-491a09fa1e79&user=Berkan+Zorlubas&userId=7d74427941be&source=-----491a09fa1e79---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7d74427941be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-3d-videos-from-rgb-videos-491a09fa1e79&user=Berkan+Zorlubas&userId=7d74427941be&source=post_page-7d74427941be----491a09fa1e79---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----491a09fa1e79--------------------------------)
    ·8 min read·2023年8月3日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F491a09fa1e79&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-3d-videos-from-rgb-videos-491a09fa1e79&user=Berkan+Zorlubas&userId=7d74427941be&source=-----491a09fa1e79---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F491a09fa1e79&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-3d-videos-from-rgb-videos-491a09fa1e79&source=-----491a09fa1e79---------------------bookmark_footer-----------)![](../Images/c89b7361a6263a9248708884ab00f1d8.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F491a09fa1e79&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-3d-videos-from-rgb-videos-491a09fa1e79&source=-----491a09fa1e79---------------------bookmark_footer-----------)![](../Images/c89b7361a6263a9248708884ab00f1d8.png)'
- en: Image by the author (edited version of the video frame from [stock footage](https://www.videvo.net/video/businessman-and-businesswoman-finishing-meeting-with-handshake/1112997/)
    provided by Videvo, downloaded from [www.videvo.net](http://www.videvo.net/))
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片（视频帧的编辑版本，来自 [库存镜头](https://www.videvo.net/video/businessman-and-businesswoman-finishing-meeting-with-handshake/1112997/)
    提供者Videvo，下载自 [www.videvo.net](http://www.videvo.net/)）
- en: I’ve always had discontentment with the fact that we archive our digital memories
    in 2D formats — photographs and videos that, despite their clarity, lack the depth
    and immersion of the experiences they capture. It’s a limitation that seems arbitrary
    in an era where machine learning models can be powerful enough to understand the
    3Dness of photos and videos.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我一直对我们将数字记忆以2D格式存档这一事实感到不满——尽管照片和视频很清晰，却缺乏它们所捕捉的经历的深度和沉浸感。在机器学习模型足够强大到理解照片和视频的3D感的时代，这似乎是一个任意的限制。
- en: 3D data from images or videos not only lets us experience our memories more
    vividly and interactively but also offers new possibilities for editing and post-processing.
    Imagine being able to effortlessly remove objects from a scene, switch out backgrounds,
    or even shift the perspective to see a moment from a new viewpoint. Depth-aware
    processing also gives machine learning algorithms a richer context to understand
    and manipulate visual data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 来自图像或视频的3D数据不仅使我们能够更生动、互动地体验记忆，还提供了编辑和后处理的新可能性。试想能够轻松地从场景中移除对象、更换背景，甚至改变视角以从新的视点查看一个瞬间。深度感知处理也为机器学习算法提供了更丰富的上下文来理解和处理视觉数据。
- en: 'While searching for methods to generate consistent depth of videos, I found
    a [research paper](https://github.com/google/dynamic-video-depth) that suggested
    a nice approach. This approach involves training two neural networks together
    using the entire input video: a convolutional neural network (CNN) to predict
    depth, and an MLP to predict the motion in the scene, or “scene flow”. This flow
    prediction network is used in a special way where it’s applied repeatedly over
    different periods of time. This allows it to figure out both small and large changes
    in the scene. The small changes help to ensure that the motion from one moment
    to the next is smooth in 3D, while the larger changes help to make sure that the
    entire video is consistent when seen from different perspectives. This way, we
    can create 3D videos that are both locally and globally accurate.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在寻找生成一致深度视频的方法时，我发现了一篇[研究论文](https://github.com/google/dynamic-video-depth)，它建议了一种很好的方法。这种方法涉及使用整个输入视频训练两个神经网络：一个卷积神经网络（CNN）来预测深度，一个MLP来预测场景中的运动或“场景流”。这个流预测网络以特殊的方式应用，在不同时间段内重复应用。这使它能够识别场景中的小变化和大变化。小变化有助于确保3D中的运动从一个时刻到下一个时刻是平滑的，而大变化有助于确保整个视频在不同视角下是一致的。这样，我们可以创建既局部又全球准确的3D视频。
- en: The [code repo](https://github.com/google/dynamic-video-depth) of the paper
    is publicly available, however, the pipeline for processing arbitrary videos is
    not entirely explained, and at least for me, it was unclear how to process any
    video with the proposed pipeline. In this blog post, I will try to fill that gap
    and give a step-by-step tour of how to use the pipeline on your videos.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文的[代码库](https://github.com/google/dynamic-video-depth)是公开的，但处理任意视频的管道并没有完全解释，至少对我来说，如何使用所提议的管道处理任何视频仍然不清楚。在这篇博客文章中，我将尝试填补这个空白，并逐步介绍如何在你的视频上使用这个管道。
- en: You can check my version of the code on my [GitHub](https://github.com/berkanz/dynamic-video-depth)
    page, which I will be referring to.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以查看我在[GitHub](https://github.com/berkanz/dynamic-video-depth)页面上的代码版本，我将会参考这个版本。
- en: 'Step 1: Extract frames from the video'
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1步：从视频中提取帧
- en: 'The first thing we do in the pipeline is to extract frames from the chosen
    video. I’ve added a script for this purpose, which you can find at `scripts/preprocess/custom/extract_frames_from_video.py`.
    To run the code, simply use the following command in your terminal:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在管道中的第一步是从选择的视频中提取帧。我添加了一个脚本用于这个目的，你可以在`scripts/preprocess/custom/extract_frames_from_video.py`中找到。要运行代码，只需在终端中使用以下命令：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Using the resize_factor argument, you can downsample or upsample your frames.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`resize_factor`参数，你可以对帧进行下采样或上采样。
- en: I have selected [this](https://medium.com/r?url=https%3A%2F%2Fwww.videvo.net%2Fvideo%2Fbusinessman-and-businesswoman-finishing-meeting-with-handshake%2F1112997%2F)
    video for my tests. Initially, it had a resolution of 1280x720, but to speed up
    processing in subsequent steps, I downsized it to 640x360 using a resize_factor
    of 0.5.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我选择了[这个](https://medium.com/r?url=https%3A%2F%2Fwww.videvo.net%2Fvideo%2Fbusinessman-and-businesswoman-finishing-meeting-with-handshake%2F1112997%2F)视频进行测试。最初，它的分辨率是1280x720，但为了加快后续步骤的处理速度，我将其缩小到640x360，使用了0.5的`resize_factor`。
- en: 'Step 2: Segment a foreground object in the video'
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2步：在视频中分割前景对象
- en: The next step in our process requires segmenting or isolating one of the main
    foreground objects in the video, which is crucial for estimating the camera’s
    position and angle within the video. The reason? Objects closer to the camera
    significantly influence pose estimation more than those farther away. To illustrate,
    imagine an object one meter away moving 10 centimeters — this would translate
    to a sizeable change in the image, perhaps dozens of pixels. But, if the same
    object were 10 meters away and moved the same distance, the image change would
    be far less noticeable. Consequently, we’re generating a ‘mask’ video to focus
    on the relevant areas for pose estimation, simplifying our calculations.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们过程中的下一步需要对视频中的一个主要前景物体进行分割或隔离，这对估计相机在视频中的位置和角度至关重要。原因是？离相机较近的物体对姿态估计的影响比离相机较远的物体要大。举个例子，想象一个距离1米远的物体移动10厘米——这将导致图像发生较大的变化，可能是几十个像素。但如果同样的物体距离10米远且移动相同的距离，图像变化则不那么明显。因此，我们生成了一个‘遮罩’视频，以便关注对姿态估计相关的区域，简化我们的计算。
- en: I preferred [Mask-RCNN](https://github.com/matterport/Mask_RCNN) for segmenting
    the frames. You can use other segmentation models of your preference as well.
    For my video, I decided to segment the person on the right as he stays in the
    frame during the entire video and seems close enough to the camera.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我偏好使用[Mask-RCNN](https://github.com/matterport/Mask_RCNN)来分割帧。你也可以使用其他你喜欢的分割模型。对于我的视频，我决定对右侧人物进行分割，因为他在整个视频中都出现在画面中，并且看起来离相机足够近。
- en: 'To generate the mask video, some manual adjustments specific to your video
    are necessary. Since my video contains two individuals, I began by segmenting
    the masks for both persons. Afterward, I extracted the mask for the person on
    the right through hardcoding. Your approach may vary depending on the foreground
    object you’ve selected and its position within the video. The script responsible
    for creating the mask can be found at `./render_mask_video.py`. The section of
    the script where I specify the mask selection process is as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成遮罩视频，需要对你的视频进行一些特定的手动调整。由于我的视频中包含两个人物，我首先对这两个人物进行了遮罩分割。之后，我通过硬编码提取了右侧人物的遮罩。根据你选择的前景物体及其在视频中的位置，你的方法可能会有所不同。负责创建遮罩的脚本可以在`./render_mask_video.py`中找到。我指定遮罩选择过程的脚本部分如下：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The original video and the mask video are visualized side-by-side in the following
    animation:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 原始视频和遮罩视频在以下动画中并排显示：
- en: '![](../Images/5415b89d0edd7d56e8c8dfe2a6510436.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5415b89d0edd7d56e8c8dfe2a6510436.png)'
- en: '**(Left)** Stock footage provided by Videvo, downloaded from [www.videvo.net](http://www.videvo.net/)
    | **(Right)** Masked video created by the author'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**（左）** 由Videvo提供的库存视频，从[www.videvo.net](http://www.videvo.net/)下载 | **（右）**
    作者创建的遮罩视频'
- en: 'Step 3: Estimate camera pose and intrinsic parameters'
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '步骤 3: 估计相机姿态和内部参数'
- en: After creating the mask frames, we now proceed with the computation of camera
    pose and intrinsic estimation. For this, we use a tool called [Colmap](https://colmap.github.io/).
    It is a multi-view stereovision tool that creates meshes from multiple images
    and also estimates camera movements and intrinsic. It has a GUI as well as a command
    line interface. You can download the tool from [this link](https://colmap.github.io/install.html).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 创建遮罩帧后，我们现在开始计算相机姿态和内部估计。为此，我们使用一个叫做[Colmap](https://colmap.github.io/)的工具。它是一个多视图立体视觉工具，可以从多个图像创建网格，并估计相机的移动和内部参数。它既有图形用户界面，也有命令行界面。你可以从[这个链接](https://colmap.github.io/install.html)下载该工具。
- en: Once you start the tool, you will press “Reconstruction” on the top bar (see
    figure below), then “Automatic reconstruction”. In the pop-up window,
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 启动工具后，你需要点击顶部栏上的“重建”（见下图），然后选择“自动重建”。在弹出窗口中，
- en: enter `./datafiles/custom/triangulation` to “Workspace folder”
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进入`./datafiles/custom/triangulation`到“工作空间文件夹”
- en: enter `./datafiles/custom/JPEGImages/640p/custom` to “Image folder”
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进入`./datafiles/custom/JPEGImages/640p/custom`到“图像文件夹”
- en: enter `./datafiles/custom/JPEGImages/640p/custom` to “Image folder”
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进入`./datafiles/custom/JPEGImages/640p/custom`到“图像文件夹”
- en: enter `./datafiles/custom/Annotations/640p/custom` to “Mask folder”
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进入`./datafiles/custom/Annotations/640p/custom`到“遮罩文件夹”
- en: tick “Shared intrinsics” option
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 勾选“共享内部参数”选项
- en: click “Run”.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点击“运行”。
- en: The computation can take a while depending on how many images you have and the
    resolution of the images. Once the computation is over, click “Export model as
    text” under “File” and save the output files in `./datafiles/custom/triangulation`.
    It will create two text and one mesh (.ply) file.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 计算可能需要一些时间，具体取决于你有多少图像以及图像的分辨率。计算完成后，点击“文件”下的“将模型导出为文本”并将输出文件保存在`./datafiles/custom/triangulation`中。这将创建两个文本文件和一个网格文件（.ply）。
- en: '![](../Images/2e8c3c0eadcfec9d044a8fea1d67cb91.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e8c3c0eadcfec9d044a8fea1d67cb91.png)'
- en: Instructions for Colmap — Image by the author
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Colmap 的说明 — 图片由作者提供
- en: 'This step is not over yet, we have to process the outputs of Colmap. I’ve written
    a script to automate it. Simply, run the following command in the terminal:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步还没有结束，我们需要处理Colmap的输出。我编写了一个脚本来自动化这一过程。只需在终端中运行以下命令：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: It will create “custom.intrinsics.txt”, “custom.matrices.txt” and “custom.obj”.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 它将创建“custom.intrinsics.txt”、“custom.matrices.txt”和“custom.obj”。
- en: '![](../Images/42b639e07be8b2bbd996a698e0c04fe0.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/42b639e07be8b2bbd996a698e0c04fe0.png)'
- en: Output files of Colmap — Image by the author
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Colmap的输出文件 — 图片由作者提供
- en: Now we are ready to proceed with the dataset generation for the training.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好进行训练的数据集生成。
- en: 'Step 4: Prepare the dataset for training'
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 4：为训练准备数据集
- en: 'The training requires a dataset that consists of depth estimations of each
    frame by [MiDas](https://github.com/isl-org/MiDaS), corresponding cross-frame
    flow estimations, and depth sequences. The scripts for creating these were provided
    in the original repo, I only changed the input and output directories in them.
    By running the command below, all the required files will be created and placed
    in the appropriate directories:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 训练需要一个数据集，该数据集包括每帧的深度估计由[MiDas](https://github.com/isl-org/MiDaS)提供，相应的跨帧光流估计和深度序列。创建这些的脚本在原始仓库中提供，我只是更改了其中的输入和输出目录。通过运行下面的命令，将创建所有所需的文件并放置在适当的目录中：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Before training, please check if you have .npz and .pt files in `datafiles/custom_processed/frames_midas/custom`
    , `datafiles/custom_processed/flow_pairs/custom` and `datafiles/custom_processed/sequences_select_pairs_midas/custom`.
    After the verification, we can proceed with the training.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练之前，请检查`datafiles/custom_processed/frames_midas/custom`、`datafiles/custom_processed/flow_pairs/custom`和`datafiles/custom_processed/sequences_select_pairs_midas/custom`中是否存在
    .npz 和 .pt 文件。验证后，我们可以继续进行训练。
- en: 'Step 5: Training'
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 5：训练
- en: 'The training part is straightforward. To train the neural network with your
    custom dataset, simply run the following command in the terminal:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 训练部分很简单。要用自定义数据集训练神经网络，只需在终端中运行以下命令：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'After training the neural network for 10 epochs, I observed that the loss began
    to saturate, and as a result, I decided not to continue training for additional
    epochs. Below is the loss curve graph of my training:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 经过10次迭代的神经网络训练后，我观察到损失开始饱和，因此决定不再继续训练更多的迭代。以下是我的训练损失曲线图：
- en: '![](../Images/3d9745795ce3b24a84d736bc6567cb44.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d9745795ce3b24a84d736bc6567cb44.png)'
- en: Loss vs. epoch curve — Image by the author
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 损失 vs. 迭代曲线 — 图片由作者提供
- en: Throughout the training, all checkpoints are stored in the directory `./logdir/nets/`.
    Furthermore, after every epoch, the training script generates test visualizations
    in the directory `./logdir/visualize`. These visualizations can be particularly
    helpful in identifying any potential issues that might have occurred during training,
    in addition to monitoring the loss.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，所有检查点都保存在目录`./logdir/nets/`中。此外，每次迭代后，训练脚本会在目录`./logdir/visualize`中生成测试可视化。这些可视化可以特别有助于识别训练过程中可能出现的任何潜在问题，除了监控损失外。
- en: 'Step 6: Create depthmaps of each frame using the trained model'
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 6：使用训练好的模型创建每帧的深度图
- en: 'Using the latest checkpoint, we now generate the depth map of each frame with
    `test.py` script. Simply run the following command in the terminal:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用最新的检查点，我们现在用`test.py`脚本生成每帧的深度图。只需在终端中运行以下命令：
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This will generate one .npz file for each frame(a dictionary file consisting
    of RGB frame, depth, camera pose, flow to next image, and so on), and three depth
    renders (ground truth, MiDaS, and trained network’s estimation) for each frame.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为每一帧生成一个 .npz 文件（一个包含RGB帧、深度、相机姿态、流向下一张图像等的字典文件），以及每一帧的三个深度渲染（真实值、MiDaS和训练网络的估计）。
- en: 'Step 7: Create point cloud videos'
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 7：创建点云视频
- en: In the last step, we load the batched .npz files frame-by-frame and create colored
    point clouds by using the depth and RGB information. I use the [open3d](http://www.open3d.org/docs/0.9.0/index.html)
    library to create and render point clouds in Python. It is a powerful tool with
    which you can create virtual cameras in 3D space and take captures of your point
    clouds with them. You can also edit/manipulate your point clouds; I applied open3d’s
    built-in outlier removal functions to remove flickery and noisy points.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步，我们逐帧加载批处理的 .npz 文件，并利用深度和 RGB 信息创建彩色点云。我使用 [open3d](http://www.open3d.org/docs/0.9.0/index.html)
    库在 Python 中创建和渲染点云。这是一个强大的工具，你可以在 3D 空间中创建虚拟相机并用它们捕捉点云。你还可以编辑/操作点云；我应用了 open3d
    内置的异常点移除功能来去除闪烁和噪声点。
- en: While I won’t delve into the specific details of my open3d usage to keep this
    blog post succinct, I have included the script, `render_pointcloud_video.py`which
    should be self-explanatory. If you have any questions or require further clarification,
    please don’t hesitate to ask.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我不会详细讨论我使用 open3d 的具体细节，以保持本文的简洁性，但我已包含了脚本 `render_pointcloud_video.py`，该脚本应当易于理解。如果你有任何问题或需要进一步澄清，请随时问我。
- en: Here is what the point cloud and depth map videos look like for the video I
    processed.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我处理的视频的点云和深度图像的视频效果。
- en: '![](../Images/9fa90df27e34da041b4929750695d159.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9fa90df27e34da041b4929750695d159.png)'
- en: '**(Left)** Stock footage provided by Videvo, downloaded from [www.videvo.net](http://www.videvo.net/)
    | **(Right)** Depthmap video created by the author | **(Bottom)** Colorized point
    cloud video created by the author'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**（左）** 视频素材由 Videvo 提供，下载自 [www.videvo.net](http://www.videvo.net/) | **（右）**
    作者制作的深度图视频 | **（下）** 作者制作的彩色点云视频'
- en: '*A higher-resolution version of this animation is uploaded to* [*YouTube*](https://youtu.be/0AOiY2Sn-4E)*.*'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*此动画的高分辨率版本已上传至* [*YouTube*](https://youtu.be/0AOiY2Sn-4E)*。*'
- en: Well, depth maps and point clouds are cool, but you might be wondering what
    you can do with them. Depth-aware effects can be remarkably potent when compared
    to traditional methods of adding effects. For instance, depth-aware processing
    enables the creation of various cinematic effects that would otherwise be challenging
    to achieve. With the estimated depth of a video, you can seamlessly incorporate
    synthetic camera focus and defocus, resulting in a realistic and consistent bokeh
    effect.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，深度图和点云很酷，但你可能想知道你可以用它们做什么。与传统的效果添加方法相比，深度感知效果可以非常强大。例如，深度感知处理可以创建各种电影效果，否则很难实现。通过视频的估计深度，你可以无缝地加入合成的相机对焦和虚焦，产生真实且一致的散景效果。
- en: Moreover, depth-aware techniques offer the possibility of implementing dynamic
    effects like the “dolly zoom”. By manipulating the position and intrinsics of
    the virtual camera, this effect can be applied to generate stunning visual sequences.
    Additionally, depth-aware object insertion ensures that virtual objects are realistically
    fixed within videos, maintaining consistent positions throughout the scenes.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，深度感知技术提供了实现动态效果如“推镜变焦”的可能性。通过调整虚拟相机的位置和内参，这种效果可以生成惊艳的视觉序列。此外，深度感知的对象插入确保虚拟对象在视频中真实固定，保持整个场景中的一致位置。
- en: The combination of depth maps and point clouds unleashes a world of possibilities
    for captivating storytelling and imaginative visual effects, propelling the creative
    potential of filmmakers and artists to new heights.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 深度图和点云的结合为引人入胜的叙事和富有创意的视觉效果开辟了无限可能，推动了电影制作人和艺术家的创意潜力达到新的高度。
- en: Right after clicking the “publish” button of this article, I will roll up my
    sleeves and work on making such effects.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 点击本文的“发布”按钮后，我将挽起袖子开始制作这些效果。
- en: Have a great day ahead!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 祝你有美好的一天！
