["```py\npip install torch torchvision\n```", "```py\npip install fiftyone\n```", "```py\npip install git+https://github.com/facebookresearch/segment-anything.git\n```", "```py\nimport numpy as np\nimport PIL\nimport torch\n\nimport fiftyone as fo\nimport fiftyone.zoo as foz # for loading/downloading datasets\n\nfrom segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n```", "```py\nsam_checkpoint = \"path/to/ckpt.pth\"\nmodel_type = \"vit_h\"\ndevice = \"cuda\"\n\nsam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\nsam.to(device)\n```", "```py\ndataset = foz.load_zoo_dataset(\n    \"open-images-v7\", \n    split=\"validation\", \n    max_samples=100,\n    label_types=[\"detections\", \"points\"],\n    shuffle=True,\n)\n```", "```py\ndataset.name = \"openimages_sam\"\ndataset.persistent = True\ndataset.compute_metadata()\n\n## visualize the dataset\nsession = fo.launch_app(dataset)\n```", "```py\nmask_generator = SamAutomaticMaskGenerator(\n    model=sam,\n    points_per_side=32,\n    pred_iou_thresh=0.9,\n    stability_score_thresh=0.92,\n    crop_n_layers=1,\n    crop_n_points_downscale_factor=2,\n    min_mask_region_area=400\n)\n```", "```py\nimage = np.array(PIL.Image.open(sample.filepath))\nmasks = mask_generator.generate(image)\n```", "```py\ndef add_SAM_auto_segmentation(sample):\n    image  = np.array(PIL.Image.open(sample.filepath))\n    masks = mask_generator.generate(image)\n\n    full_mask = np.zeros_like(masks[0][\"segmentation\"]).astype(int)\n    for i in range(len(masks)):\n        x, y = np.where(masks[i]['segmentation'])\n        full_mask[x,y] = i + 1\n\n    sample[\"auto_SAM\"] = fo.Segmentation(mask=full_mask.astype(np.uint8))\n```", "```py\ndef add_SAM_auto_segmentations(dataset):\n    for sample in dataset.iter_samples(autosave=True, progress=True):\n        add_SAM_auto_segmentation(sample)\n```", "```py\ndataset.first().points.keypoints[0]\n```", "```py\n<Keypoint: {\n    'id': '644c260d753fe20b7f60f9de',\n    'attributes': {},\n    'tags': [],\n    'label': 'Rope',\n    'points': [[0.11230469, 0.7114094]],\n    'confidence': None,\n    'index': None,\n    'estimated_yes_no': 'no',\n    'source': 'ih',\n    'yes_votes': 0,\n    'no_votes': 3,\n    'unsure_votes': 0,\n}>\n```", "```py\npredictor = SamPredictor(sam)\n```", "```py\ndef generate_sam_points(keypoints, label, w, h):\n    def scale_keypoint(p):\n        return [p[0] * w, p[1] * h]\n\n    sam_points, sam_labels = [], []\n    for kp in keypoints:\n        if kp.label == label and kp.estimated_yes_no != \"unsure\":\n            sam_points.append(scale_keypoint(kp.points[0]))\n            sam_labels.append(bool(kp.estimated_yes_no == \"yes\"))\n\n    return np.array(sam_points), np.array(sam_labels)\n```", "```py\ndef add_SAM_semantic_segmentation(sample, n2i):\n    image = np.array(PIL.Image.open(sample.filepath))\n    predictor.set_image(image)\n\n    if sample.points is None:\n        return\n\n    points = sample.points.keypoints\n    labels = list(set([point.label for point in points]))\n\n    w, h = sample.metadata.width, sample.metadata.height\n    semantic_mask = np.zeros((h, w))\n    for label in labels:\n        sam_points, sam_labels = generate_sam_points(points, label, w, h)\n        if not np.any(sam_labels):\n            continue\n\n        masks, scores, _ = predictor.predict(\n            point_coords=sam_points,\n            point_labels=sam_labels,\n            multimask_output=True,\n        )\n        mask = masks[np.argmax(scores)].astype(int) ## get best guess\n\n        semantic_mask *= (1 - mask)\n        semantic_mask += mask * n2i[label]\n\n    sample[\"semantic_SAM\"] = fo.Segmentation(\n        mask=semantic_mask.astype(np.uint8)\n    )\n```", "```py\ndef add_SAM_semantic_segmentations(dataset):\n    point_classes = dataset.distinct(\"points.keypoints.label\")\n    dataset.default_mask_targets = {i+1:n for i, n in enumerate(point_classes)}\n    dataset.default_mask_targets[0] = \"other\"  # reserve 0 for background\n    NAME_TO_INT = {n:i+1 for i, n in enumerate(point_classes)}\n    dataset.save()\n\n    for sample in dataset.iter_samples(autosave=True, progress=True):\n        add_SAM_semantic_segmentation(sample, NAME_TO_INT)\n```", "```py\ndef fo_to_sam(box, img_width, img_height):\n    new_box = np.copy(np.array(box))\n    new_box[0] *= img_width\n    new_box[2] *= img_width\n    new_box[1] *= img_height\n    new_box[3] *= img_height\n    new_box[2] += new_box[0]\n    new_box[3] += new_box[1]\n    return np.round(new_box).astype(int)\n```", "```py\ndef add_SAM_mask_to_detection(detection, mask, img_width, img_height):\n    y0, x0, y1, x1 = fo_to_sam(detection.bounding_box, img_width, img_height)    \n    mask_trimmed = mask[x0:x1+1, y0:y1+1]\n    detection[\"mask\"] = np.array(mask_trimmed)\n    return detection\n```", "```py\ndef add_SAM_instance_segmentation(sample):\n    w, h = sample.metadata.width, sample.metadata.height\n    image = np.array(PIL.Image.open(sample.filepath))\n    predictor.set_image(image)\n\n    if sample.detections is None:\n        return\n\n    dets = sample.detections.detections\n    boxes = [d.bounding_box for d in dets]\n    sam_boxes = np.array([fo_to_sam(box, w, h) for box in boxes])\n\n    input_boxes = torch.tensor(sam_boxes, device=predictor.device)\n    transformed_boxes = predictor.transform.apply_boxes_torch(input_boxes, image.shape[:2])\n\n    masks, _, _ = predictor.predict_torch(\n            point_coords=None,\n            point_labels=None,\n            boxes=transformed_boxes,\n            multimask_output=False,\n        )\n\n    new_dets = []\n    for i, det in enumerate(dets):\n        mask = masks[i, 0]\n        new_dets.append(add_SAM_mask_to_detection(det, mask, w, h))\n\n    sample.detections = fo.Detections(detections = new_dets)\n```", "```py\ndef add_SAM_instance_segmentations(dataset):\n    for sample in dataset.iter_samples(autosave=True, progress=True):\n        add_SAM_instance_segmentation(sample)\n```"]