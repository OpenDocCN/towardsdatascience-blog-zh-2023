- en: How to Store Historical Data Much More Efficiently
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-store-historical-data-much-more-efficiently-78b0f2c8c811?source=collection_archive---------2-----------------------#2023-09-10](https://towardsdatascience.com/how-to-store-historical-data-much-more-efficiently-78b0f2c8c811?source=collection_archive---------2-----------------------#2023-09-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A hands-on tutorial using PySpark to store up to only 0.01% of a DataFrame’s
    rows without losing any information.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@tomergabay?source=post_page-----78b0f2c8c811--------------------------------)[![Tomer
    Gabay](../Images/1fb1d408bc89415918c1aa6733df44e1.png)](https://medium.com/@tomergabay?source=post_page-----78b0f2c8c811--------------------------------)[](https://towardsdatascience.com/?source=post_page-----78b0f2c8c811--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----78b0f2c8c811--------------------------------)
    [Tomer Gabay](https://medium.com/@tomergabay?source=post_page-----78b0f2c8c811--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc9c352dba00a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-store-historical-data-much-more-efficiently-78b0f2c8c811&user=Tomer+Gabay&userId=c9c352dba00a&source=post_page-c9c352dba00a----78b0f2c8c811---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----78b0f2c8c811--------------------------------)
    ·10 min read·Sep 10, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F78b0f2c8c811&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-store-historical-data-much-more-efficiently-78b0f2c8c811&user=Tomer+Gabay&userId=c9c352dba00a&source=-----78b0f2c8c811---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F78b0f2c8c811&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-store-historical-data-much-more-efficiently-78b0f2c8c811&source=-----78b0f2c8c811---------------------bookmark_footer-----------)![](../Images/e6a3ccf160e7ca3d2ab0eba9ba6199b7.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Supratik Deshmukh](https://unsplash.com/@supratikdeshmukh?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 'In an era where companies and organizations are collecting more data than ever
    before, datasets tend to accumulate millions of unnecessary rows that don’t contain
    any new or valuable information. In this article, we’ll focus on a critical aspect
    of data management: deleting rows in a dataset if they provide no added value,
    using PySpark*.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '*[PySpark](https://spark.apache.org/docs/latest/api/python/index.html#:~:text=PySpark%20is%20the%20Python%20API,for%20interactively%20analyzing%20your%20data.)
    is used over pandas when dealing with very large datasets because it can process
    data across multiple computers, making it faster and more scalable. Pandas works
    well for smaller datasets that can fit in memory on a single machine but may become
    slow or even impractical for big data.'
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*[PySpark](https://spark.apache.org/docs/latest/api/python/index.html#:~:text=PySpark%20is%20the%20Python%20API,for%20interactively%20analyzing%20your%20data.)
    被用于处理非常大的数据集，因为它可以跨多个计算机处理数据，从而使处理速度更快，扩展性更强。Pandas 适用于可以放在单台计算机内存中的较小数据集，但对于大数据可能变得缓慢甚至不切实际。'
- en: 'Let’s imagine the following situation: you work as a data engineer/scientist
    in the maintenance department of a real estate company. For the past ten years,
    your company has done a full load of all maintenance data from an external database
    containing the conditions of your buildings and stored it in the company’s cloud
    storage. The data could e.g. look like this:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设想以下情况：你在一家房地产公司的维护部门担任数据工程师/科学家。过去十年里，你的公司从一个外部数据库中提取了所有维护数据，包含建筑物的状况，并将其存储在公司的云存储中。这些数据可能看起来像这样：
- en: 'Three columns are present in this dataset:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中包含三列：
- en: '`id` -> for the ID of the building.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`id` -> 用于建筑物的 ID。'
