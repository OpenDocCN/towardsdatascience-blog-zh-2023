["```py\nimport tensorflow_datasets as tfds\n\nDATASET_NAME = 'rock_paper_scissors'\n(dataset_train_raw, dataset_test_raw), dataset_info = tfds.load(\n    name=DATASET_NAME,\n    data_dir='tmp',\n    with_info=True,\n    as_supervised=True,\n    split=[tfds.Split.TRAIN, tfds.Split.TEST],\n)\n\n# plot samples from the dataset\nfig = tfds.show_examples(dataset_train_raw, dataset_info)\n```", "```py\ndef lr_function(epoch):\n    # set start, min and max value for learning rate\n    start_lr = 1e-3; min_lr = 1e-3; max_lr = 2e-3\n\n    # define the number of epochs to increase \n    # LR lineary and then the decay factor\n    rampup_epochs = 6; sustain_epochs = 0; exp_decay = .5\n\n    # method to update the LR value based on the current epoch\n    def lr(epoch, start_lr, min_lr, max_lr, rampup_epochs,\n           sustain_epochs, exp_decay):\n        if epoch < rampup_epochs:\n            lr = ((max_lr - start_lr) / rampup_epochs\n                        * epoch + start_lr)\n        elif epoch < rampup_epochs + sustain_epochs:\n            lr = max_lr\n        else:\n            lr = ((max_lr - min_lr) *\n                      exp_decay**(epoch - rampup_epochs -\n                                    sustain_epochs) + min_lr)\n        return lr\n\n    return lr(epoch, start_lr, min_lr, max_lr,\n              rampup_epochs, sustain_epochs, exp_decay)\n```", "```py\n# Set Image Shape \nINPUT_IMG_SHAPE= (128, 128, 3)\n\n# Get Pretrained MobileNetV2\nbase_model = tf.keras.applications.MobileNetV2(\n  input_shape=INPUT_IMG_SHAPE,\n  include_top=False,\n  weights='imagenet',\n  pooling='avg'\n)\n\n# Attach a classification head\nmodel_lr = tf.keras.models.Sequential()\nmodel_lr.add(base_model)\nmodel_lr.add(tf.keras.layers.Dropout(0.5))\nmodel_lr.add(tf.keras.layers.Dense(\n    units=NUM_CLASSES,\n    activation=tf.keras.activations.softmax,\n    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n))\n\n# compile the model\nmodel_lr.compile(\n    optimizer=tf.keras.optimizers.Adam(),\n    loss=tf.keras.losses.sparse_categorical_crossentropy,\n    metrics=['accuracy']\n)\n\n# set number of epochs\ninitial_epochs = 24\n\n# Set the model for training\n# The LearningRateScheduler callback is where we\n# plug our custom 1-cycle rate function\ntraining_history_lr = model_lr.fit(\n    x=dataset_train_augmented_shuffled.repeat(),\n    validation_data=dataset_test_shuffled.repeat(),\n    epochs=initial_epochs,\n    steps_per_epoch=steps_per_epoch,\n    validation_steps=validation_steps,\n    callbacks=[\n        tf.keras.callbacks.LearningRateScheduler(lambda epoch: \\\n                                             lr_function(epoch),\n                                             verbose=True)\n    ],\n    verbose=1\n)\n```", "```py\n# Simple CNN\nTest loss:  0.7511898279190063\nTest accuracy:  0.7768816947937012\n\n# MobileNetV2\nTest loss:  0.24527719616889954\nTest accuracy:  0.9220430254936218\n\n# MobileNetV2_LR\nTest loss:  0.27864792943000793\nTest accuracy:  0.9166666865348816\n```"]