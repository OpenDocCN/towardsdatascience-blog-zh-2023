- en: Creating a Data Pipeline with Spark, Google Cloud Storage and Big Query
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Spark、Google Cloud Storage 和 Big Query 创建数据管道
- en: 原文：[https://towardsdatascience.com/creating-a-data-pipeline-with-spark-google-cloud-storage-and-big-query-a72ede294f4c?source=collection_archive---------7-----------------------#2023-03-06](https://towardsdatascience.com/creating-a-data-pipeline-with-spark-google-cloud-storage-and-big-query-a72ede294f4c?source=collection_archive---------7-----------------------#2023-03-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/creating-a-data-pipeline-with-spark-google-cloud-storage-and-big-query-a72ede294f4c?source=collection_archive---------7-----------------------#2023-03-06](https://towardsdatascience.com/creating-a-data-pipeline-with-spark-google-cloud-storage-and-big-query-a72ede294f4c?source=collection_archive---------7-----------------------#2023-03-06)
- en: On-premise and cloud working together to deliver a data product
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本地和云端协同工作以交付数据产品
- en: '[](https://joaopedro214.medium.com/?source=post_page-----a72ede294f4c--------------------------------)[![João
    Pedro](../Images/64a0e14527be213e5fde0a02439fbfa7.png)](https://joaopedro214.medium.com/?source=post_page-----a72ede294f4c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a72ede294f4c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a72ede294f4c--------------------------------)
    [João Pedro](https://joaopedro214.medium.com/?source=post_page-----a72ede294f4c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://joaopedro214.medium.com/?source=post_page-----a72ede294f4c--------------------------------)[![João
    Pedro](../Images/64a0e14527be213e5fde0a02439fbfa7.png)](https://joaopedro214.medium.com/?source=post_page-----a72ede294f4c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a72ede294f4c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a72ede294f4c--------------------------------)
    [João Pedro](https://joaopedro214.medium.com/?source=post_page-----a72ede294f4c--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb111eee95c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-a-data-pipeline-with-spark-google-cloud-storage-and-big-query-a72ede294f4c&user=Jo%C3%A3o+Pedro&userId=b111eee95c&source=post_page-b111eee95c----a72ede294f4c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a72ede294f4c--------------------------------)
    ·10 min read·Mar 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa72ede294f4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-a-data-pipeline-with-spark-google-cloud-storage-and-big-query-a72ede294f4c&user=Jo%C3%A3o+Pedro&userId=b111eee95c&source=-----a72ede294f4c---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb111eee95c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-a-data-pipeline-with-spark-google-cloud-storage-and-big-query-a72ede294f4c&user=Jo%C3%A3o+Pedro&userId=b111eee95c&source=post_page-b111eee95c----a72ede294f4c---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a72ede294f4c--------------------------------)
    · 10 分钟阅读 · 2023年3月6日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa72ede294f4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-a-data-pipeline-with-spark-google-cloud-storage-and-big-query-a72ede294f4c&user=Jo%C3%A3o+Pedro&userId=b111eee95c&source=-----a72ede294f4c---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa72ede294f4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-a-data-pipeline-with-spark-google-cloud-storage-and-big-query-a72ede294f4c&source=-----a72ede294f4c---------------------bookmark_footer-----------)![](../Images/43f50bf401c4c19245d3fab8b7acb67f.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa72ede294f4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-a-data-pipeline-with-spark-google-cloud-storage-and-big-query-a72ede294f4c&source=-----a72ede294f4c---------------------bookmark_footer-----------)![](../Images/43f50bf401c4c19245d3fab8b7acb67f.png)'
- en: Photo by [Toro Tseleng](https://unsplash.com/@crayon__artworks?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Toro Tseleng](https://unsplash.com/@crayon__artworks?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Developing a data pipeline is somewhat similar to playing with lego, you mentalize
    what needs to be achieved (the data requirements), choose the pieces (software,
    tools, platforms), and fit them together. And, like in lego, the complexity of
    the building process is determined by the complexity of the final goal.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 开发数据管道在某种程度上类似于玩乐高，你需要构思实现目标（数据需求），选择合适的零件（软件、工具、平台），然后将它们组装在一起。就像在乐高中一样，构建过程的复杂性取决于最终目标的复杂性。
- en: It’s possible to go from simple ETL pipelines built with python to move data
    between two databases to very complex structures, using Kafka to stream real-time
    messages between all sorts of cloud structures to serve multiple end applications.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 从使用 Python 构建的简单 ETL 管道在两个数据库之间移动数据，到使用 Kafka 在各种云结构之间流式传输实时消息以服务多个最终应用程序，非常复杂的结构都是可能的。
- en: But, the reality is that the current data scenario is more like those fancy
    expensive professional lego sets, with all sorts of pieces to solve specific needs
    with new ones popping up on every corner. You probably already saw [Matt Turck’s
    2021 Machine Learning, AI and Data (MAD) Landscape](https://mattturck.com/data2021/).
    And the bad part — the instructions manual is not included.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 但现实是，当前的数据场景更像那些昂贵的专业乐高套件，拥有各种解决特定需求的零件，并且新的零件在每个角落不断出现。你可能已经看过 [Matt Turck
    的 2021 年机器学习、人工智能和数据 (MAD) 领域](https://mattturck.com/data2021/)。而糟糕的部分——说明书没有包含在内。
- en: Many open-source data-related tools have been developed in the last decade,
    like Spark, Hadoop, and Kafka, without mention all the tooling available in the
    Python libraries. And that’s are the tools I like to cover in my posts, they’re
    free, they have well-maintained docker images and I can develop self-contained
    apps with docker that anyone can run anywhere.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 过去十年中，许多开源数据相关工具如 Spark、Hadoop 和 Kafka 已经被开发出来，更不用说 Python 库中所有可用的工具了。这些就是我喜欢在文章中覆盖的工具，它们是免费的，维护良好的
    Docker 镜像，并且我可以使用 Docker 开发自包含的应用程序，任何人都可以在任何地方运行。
- en: But, as the data landscape matures, all the arrows seem to point in the same
    direction — Cloud computing. And this is, by no means, a surprise. Companies targeting
    specifically data applications like Databricks, DBT, and Snowflake are exploding
    in popularity while the classic players (AWS, Azure, and GCP) are also investing
    heavily in their data products.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，随着数据领域的成熟，所有的箭头似乎都指向同一个方向——云计算。这一点毫不令人惊讶。专注于数据应用的公司如 Databricks、DBT 和 Snowflake
    正在迅速流行，而传统玩家（AWS、Azure 和 GCP）也在大力投资他们的数据产品。
- en: And that’s the target of today’s post — We’ll be developing a data pipeline
    using Apache Spark, Google Cloud Storage, and Google Big Query (using the free
    tier)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是今天文章的目标——我们将使用 Apache Spark、Google Cloud Storage 和 Google Big Query（使用免费层）来开发一个数据管道。
- en: not sponsored.
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 未经赞助。
- en: The tools
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工具
- en: '**Spark** is an all-purpose distributed memory-based data processing framework
    geared towards processing extremely large amounts of data. I covered Spark in
    many other posts.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**Spark** 是一个面向处理极大量数据的通用分布式内存数据处理框架。我在许多其他文章中介绍过 Spark。'
- en: '**Google Cloud Storage (GCS)**is Google’s blob storage. The idea is simple:
    create a bucket and store files in it. Read them later using their “path”. Folders
    are a lie and the objects are immutable.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**Google Cloud Storage (GCS)** 是 Google 的对象存储。概念很简单：创建一个桶并将文件存储在其中。稍后使用它们的“路径”读取。文件夹是虚假的，对象是不可变的。'
- en: '**Google Big Query (GBQ)** is Google’s cloud data warehouse solution. An OLAP-focused
    database with a serverless SQL query execution capable of processing large amounts
    of data.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**Google Big Query (GBQ)** 是 Google 的云数据仓库解决方案。一个以 OLAP 为重点的数据库，具有无服务器 SQL
    查询执行能力，能够处理大量数据。'
- en: The data
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据
- en: 'We’re going to build a data pipeline to process and store data from the Brazilian
    “higher education” (literal translation) census. This census yearly collects many
    statistics about Brazilian higher education institutions (basically the universities)
    from many perspectives: institutional, social & demographic, geographic, and so
    on.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个数据管道来处理和存储来自巴西“高等教育”（字面翻译）普查的数据。该普查每年收集有关巴西高等教育机构（主要是大学）从不同角度的许多统计数据：机构的、社会和人口统计的、地理的等等。
- en: We’ll be dealing with the *courses report*, which contains statistics about
    every Brazilian superior course (graduation, post-graduation, doctorate, etc).
    This data is publically available [CC BY-ND 3.0] in CSV files (one per year).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将处理 *课程报告*，其中包含有关每个巴西高等教育课程（本科、研究生、博士等）的统计数据。这些数据公开可用 [CC BY-ND 3.0]，以 CSV
    文件形式提供（每年一个）。
- en: The implementation
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现
- en: The pipeline idea is simple, download the CSV files to the local machine, convert
    them into a Delta-Lake table stored in a GCS bucket, do the transformations needed
    over this delta table, and save the results in a Big Query Table that can be easily
    consumed by other downstream tasks.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 管道的想法很简单，将 CSV 文件下载到本地机器，转换为存储在 GCS 桶中的 Delta-Lake 表，对这个 delta 表进行所需的转换，并将结果保存到一个
    Big Query 表中，以便其他下游任务可以轻松使用。
- en: '![](../Images/11b2045ad52ab8651da0058becf8be49.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11b2045ad52ab8651da0058becf8be49.png)'
- en: The proposed pipeline. Image by the author.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 提议的管道。作者提供的图像。
- en: The bucket will function as a raw file storage that aggregates all the reports
    in a single place. The BigQuery table will store our service-ready data, already
    filtered, aggregated, and containing only the useful columns.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 存储桶将作为原始文件存储，聚合所有报告到一个地方。BigQuery 表将存储我们准备好的数据，已经过滤、聚合，并且只包含有用的列。
- en: As mentioned before, the census collects a lot of statistics from all the higher
    education institutes which includes, but is not limited to, universities. To simulate
    a “real situation”, let’s suppose we need to create a table to answer various
    social/demographic questions about the new students that ingress universities
    every year.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，普查收集了来自所有高等教育机构的大量统计数据，包括但不限于大学。为了模拟“真实情况”，假设我们需要创建一个表格来回答关于每年进入大学的新生的各种社会/人口统计问题。
- en: 0\. Setting up the environment
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 0\. 设置环境
- en: All the code is available on this [GitHub repository](https://github.com/jaumpedro214/posts).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 所有代码都可以在这个 [GitHub 仓库](https://github.com/jaumpedro214/posts)中找到。
- en: You will need docker installed on your local machine to create the Spark cluster
    and python to download the files.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要在本地计算机上安装 docker 以创建 Spark 集群，并且需要 python 来下载文件。
- en: 'The docker-compose file:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: docker-compose 文件：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The spark Dockerfile:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: spark Dockerfile：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The docker images are already configured to automatically create a new environment
    from the scratch, so we can focus more on the implementation rather than on the
    configuration.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: docker 镜像已经配置为从头自动创建一个新环境，因此我们可以更多地关注实现而不是配置。
- en: Of course, you’ll need to create a Google Cloud Platform account. Even though
    we are going to use only the free quota, your credit card information is needed.
    GCP states that it will not charge you unless you explicitly end your free-trial
    period, but be careful.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你需要创建一个 Google Cloud Platform 账户。尽管我们只会使用免费的配额，但仍然需要你的信用卡信息。GCP 表示除非你明确结束免费试用期，否则不会收费，但请小心。
- en: 'Once you’ve created the account follow the steps:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 创建账户后，按照以下步骤操作：
- en: '**1.** Access the GCP console and **create a new project**. I’ve named my “BigQueryFirstSteps”'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**1.** 访问 GCP 控制台并**创建一个新项目**。我将我的项目命名为“BigQueryFirstSteps”'
- en: '![](../Images/340a6e94c035fa53ebee52c1e841bf79.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/340a6e94c035fa53ebee52c1e841bf79.png)'
- en: '**2\.** Authorize the APIs for Google Cloud Storage and BigQuery in the API
    & Services tab.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\.** 在 API & Services 标签中授权 Google Cloud Storage 和 BigQuery 的 API。'
- en: '**3.** Create a new bucket in the Google Cloud Storage named **censo-ensino-superior**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.** 在 Google Cloud Storage 中创建一个名为**censo-ensino-superior**的新存储桶'
- en: '![](../Images/3e4a3538dcef2ce5700fafe7d899a1da.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e4a3538dcef2ce5700fafe7d899a1da.png)'
- en: '**4\.** Create a new dataset in Google Big Query named **censo-ensino-superior**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**4\.** 在 Google Big Query 中创建一个名为**censo-ensino-superior**的新数据集'
- en: '![](../Images/f597a457047d711e3130624f7ea308b7.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f597a457047d711e3130624f7ea308b7.png)'
- en: '**5\.** Create a new service account in the *service account* item inside IAM
    & Administrator tab with the appropriate roles to read, write and create GCP buckets
    and GBQ tables (I’ve used *BigQuery administrator* and *Storage administrator*)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**5\.** 在 IAM & Administrator 标签页中的*服务账户*项目下创建一个新的服务账户，并分配适当的角色以读取、写入和创建 GCP
    存储桶和 GBQ 表（我使用了*BigQuery 管理员*和*存储管理员*角色）'
- en: '**6\.** Still on this page, generate a new access key (JSON) to the newly created
    account. The key will be downloaded to your local machine.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**6\.** 在此页面上，生成一个新的访问密钥（JSON）用于新创建的账户。密钥将下载到你的本地计算机。'
- en: Back to the local environment, execute the *prepare_env.sh* script.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 返回本地环境，执行*prepare_env.sh* 脚本。
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: It creates a few folders with specific authorizations (so the spark containers
    can read and write from them) and downloads the GCS connector for spark.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 它创建了几个具有特定授权的文件夹（以便 spark 容器可以从中读取和写入）并下载了 GCS 连接器以供 spark 使用。
- en: Now, rename your JSON credentials file to **gcp-credentials.json** and put it
    inside the ./src/credentials folder (along with the bucket_name.txt file).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，将你的 JSON 凭证文件重命名为**gcp-credentials.json**并放置在 ./src/credentials 文件夹中（与 bucket_name.txt
    文件一起）。
- en: Finally, start the containers with
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，启动容器：
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 1\. Downloading data
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 下载数据
- en: 'Just run the script:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 只需运行脚本：
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: And the CSV files will be downloaded to the ./data folder.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: CSV 文件将下载到 ./data 文件夹中。
- en: '![](../Images/ab9e4c5614c78d5b9c0878bef020a285.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ab9e4c5614c78d5b9c0878bef020a285.png)'
- en: 2\. Converting CSV to Delta Lake in GCS
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 将 CSV 转换为 GCS 中的 Delta Lake
- en: The first thing to do is instantiate a Spark Session and configure it with the
    Delta-Lake dependencies.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 首先需要做的是实例化一个 Spark 会话，并配置 Delta-Lake 依赖项。
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Reading the downloaded CSV files is quite easy, just specify the options and
    give the path.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 读取下载的 CSV 文件非常简单，只需指定选项并给出路径。
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To write fewer data in the GCP bucket I’ve also selected just the columns that
    are useful to the final table: year of the census; course’s identification; area
    of knowledge; location and type; the number of new students by gender, age and
    skin color.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 GCP 桶中写入更少的数据，我还仅选择了对最终表有用的列：普查年份；课程标识；知识领域；地点和类型；按性别、年龄和肤色统计的新生人数。
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Writing data to a GCP Bucket is just like writing to the file system but, instead
    of passing a local path, we need to specify the bucket path with its own syntax:
    “gs://<bucket_name>/<filepath_to_be_create>”.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据写入 GCP 桶就像写入文件系统一样，但我们需要用自己的语法指定桶路径：“gs://<bucket_name>/<filepath_to_be_create>”。
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The code above creates a new Delta Table inside the **censo-ensino-superior**
    bucket named **censo_cursos.**
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码在 **censo-ensino-superior** 桶内创建了一个名为 **censo_cursos** 的新 Delta 表。
- en: We don’t need to handle authentication in the code because the credentials were
    properly configured during the docker build phase.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要在代码中处理身份验证，因为凭据在 Docker 构建阶段已经正确配置。
- en: 'To execute this script, access the spark container’s terminal (master or worker)
    and execute:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此脚本，请访问 Spark 容器的终端（主节点或工作节点）并执行：
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: After a minute or so, the script will finish and the data will be available
    in your GCS bucket.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一分钟左右，脚本将完成，数据将可用在你的 GCS 桶中。
- en: '![](../Images/c8fdf35dee751871fa131a082a03d7e1.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c8fdf35dee751871fa131a082a03d7e1.png)'
- en: 3\. Processing Delta Table from GCS to GBQ
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 从 GCS 处理 Delta 表到 GBQ
- en: Again, the first thing to do is instantiate a Spark Session, the same way did
    before.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 首先需要做的是实例化一个 Spark 会话，与之前所做的相同。
- en: Reading data from a bucket also follows the same logic as writing.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 从桶中读取数据也遵循与写入相同的逻辑。
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: With the data in hand, we can make some transformations as usual.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备好后，我们可以像往常一样进行一些转换。
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The query above first filters only the bachelor and degree courses (using their
    codes) and groups the result by year, detailed area, and state, summing up the
    number of new students (“QT_ING_XYZ”) of each category.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 上述查询首先筛选出仅包含学士和学位课程（使用它们的代码），并按年份、详细领域和州对结果进行分组，汇总每个类别的新生人数（“QT_ING_XYZ”）。
- en: To write data to a BigQuery table we need to use the format “bigquery” along
    with a few options. Obviously, we need to pass the table and database that we’re
    writing to. As this table still does not exist, the option “createDisposition”
    needs to be set to“CREATE_IF_NEEDED”.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 要将数据写入 BigQuery 表，我们需要使用格式 “bigquery” 以及一些选项。显然，我们需要传递要写入的表和数据库。由于这个表尚不存在，因此需要将选项“createDisposition”设置为“CREATE_IF_NEEDED”。
- en: The standard [GBQ-Spark connector](https://cloud.google.com/dataproc/docs/tutorials/bigquery-connector-spark-example)
    uses a GCS bucket as an intermediate buffer to move the data from/to GBQ. So,
    we need to pass a “temporaryGcsBucket” option with a bucket name. For simplicity,
    I’ve used the same bucket created previously.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的 [GBQ-Spark 连接器](https://cloud.google.com/dataproc/docs/tutorials/bigquery-connector-spark-example)使用
    GCS 桶作为一个中间缓冲区，用于从 GBQ 传输数据。因此，我们需要传递一个“temporaryGcsBucket”选项，并指定桶名称。为了简单起见，我使用了之前创建的相同桶。
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Also, be aware that this writing is running on mode=“overwrite”, it will erase
    any previous data if the table already exists. If you only want to add new rows,
    use the “append” mode.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，请注意这次写入是以 mode=“overwrite” 模式进行的，如果表已经存在，它将覆盖任何以前的数据。如果你只想添加新行，请使用“append”模式。
- en: And that’s all.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些了。
- en: 'To run this job, just type:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行这个任务，只需输入：
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'And the table will be created and populated, let’s have a look:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 表将会被创建并填充，让我们来看看：
- en: '![](../Images/1b1928a2eb05013d0322edeadf2be961.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b1928a2eb05013d0322edeadf2be961.png)'
- en: Just to exemplify, let’s run a query.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了举例说明，让我们运行一个查询。
- en: The query below calculates the percentages of men and women in each area of
    knowledge.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的查询计算了每个知识领域男性和女性的百分比。
- en: '[PRE14]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The results:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：
- en: '![](../Images/921969adee7c397d481691898cd1a103.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/921969adee7c397d481691898cd1a103.png)'
- en: Query results with commented translations. Image by the author.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 带有注释翻译的查询结果。图片由作者提供。
- en: Conclusion
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this post, we learned how to develop a data pipeline using Apache Spark (on-premise)
    as the data processing tool and the pair Google Cloud Storage (for raw file storage)
    and Google Big Query (to serve the processed data to analytical queries) as the
    storage solution.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们学习了如何使用 Apache Spark（本地部署）作为数据处理工具，并将 Google Cloud Storage（用于原始文件存储）和
    Google Big Query（用于为分析查询提供处理后的数据）作为存储解决方案来开发数据管道。
- en: 'Interacting with data from the cloud using spark is not so special. The harder
    part is configuring the environment: finding the right connectors, putting them
    in the correct place, and using the correct formats. Confession: I struggle a
    lot to learn how to properly configure the docker images with the credentials.
    But once this process is mastered, querying and manipulating data flows just as
    usual.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Spark 与云中的数据进行交互并没有那么特别。更困难的部分是配置环境：找到合适的连接器，将其放置在正确的位置，并使用正确的格式。坦白说，我在学习如何正确配置
    Docker 镜像和凭据时遇到了很多困难。但一旦掌握了这个过程，查询和操作数据就会像平常一样。
- en: And that’s one of the things I most like about Apache Spark —It separates the
    processing logic from the connection logic. If, for example, we change our blob
    storage solution from GCS to Amazon’s S3, all that we need to do is reconfigure
    the environment with new AWS credentials and change the read and write commands.
    All the query/transformation logic remains the same.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是我最喜欢 Apache Spark 的其中一个原因——它将处理逻辑与连接逻辑分开。例如，如果我们将 blob 存储解决方案从 GCS 更改为 Amazon
    的 S3，我们需要做的就是用新的 AWS 凭据重新配置环境并更改读写命令。所有的查询/转换逻辑保持不变。
- en: But, besides not being “so special”, learning how to interact with the cloud
    storage components is an extremely important skill, and I hope that this post
    helped you in having a better understanding of this process.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 但除了“没有那么特别”之外，学习如何与云存储组件交互是一个极其重要的技能，我希望这篇文章能帮助你更好地理解这个过程。
- en: As always, I’m not an expert in any of the subjects addressed in the post, and
    I strongly recommend further reading, see the references below.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我不是帖子中涉及的任何主题的专家，我强烈建议进一步阅读，见下方参考文献。
- en: Thank you for reading! ;)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！;)
- en: References
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*All the code is available in* [*this GitHub repository*](https://github.com/jaumpedro214/posts)*.'
  id: totrans-104
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*所有代码都可以在* [*这个 GitHub 仓库*](https://github.com/jaumpedro214/posts)*中找到*。'
- en: Data used —* [*Microdados do Censo da Educação Superior*](https://www.gov.br/inep/pt-br/acesso-a-informacao/dados-abertos/microdados/censo-da-educacao-superior)*,
    [CC BY-ND 3.0], INEP-Brazilian Gov.*
  id: totrans-105
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用的数据 —* [*Microdados do Censo da Educação Superior*](https://www.gov.br/inep/pt-br/acesso-a-informacao/dados-abertos/microdados/censo-da-educacao-superior)*，[CC
    BY-ND 3.0]，INEP-巴西政府*
- en: '[1] Chambers, B., & Zaharia, M. (2018). Spark: The definitive guide: Big data
    processing made simple. “ O’Reilly Media, Inc.”.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Chambers, B., & Zaharia, M. (2018). Spark: The definitive guide: Big data
    processing made simple. “ O’Reilly Media, Inc.”.'
- en: '[2] What is BigQuery? (n.d.). *Google Cloud*. [Link](https://cloud.google.com/bigquery/docs/introduction).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] 什么是 BigQuery？（无日期）。*Google Cloud*。 [链接](https://cloud.google.com/bigquery/docs/introduction)。'
- en: '[3] *Delta Lake Official Page*. (n.d.). Delta Lake. [https://delta.io/](https://delta.io/)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] *Delta Lake 官方页面*。（无日期）。Delta Lake。 [https://delta.io/](https://delta.io/)'
- en: '[4] Databricks. (2020c, September 15). *Making Apache SparkTM Better with Delta
    Lake* [[Video](https://www.youtube.com/watch?v=LJtShrQqYZY)]. YouTube.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Databricks. (2020年9月15日)。*利用 Delta Lake 改善 Apache SparkTM* [[视频](https://www.youtube.com/watch?v=LJtShrQqYZY)]。YouTube。'
- en: '[5]*Use the BigQuery connector with Spark*. (n.d.). Google Cloud. [Link](https://cloud.google.com/dataproc/docs/tutorials/bigquery-connector-spark-example).'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[5]*使用 BigQuery 连接器与 Spark*。（无日期）。Google Cloud。 [链接](https://cloud.google.com/dataproc/docs/tutorials/bigquery-connector-spark-example)。'
- en: '[6] Sohail, K. (2021, December 15). *Read files from Google Cloud Storage Bucket
    using local PySpark and Jupyter Notebooks*. Medium. [Link](https://kashif-sohail.medium.com/read-files-from-google-cloud-storage-bucket-using-local-pyspark-and-jupyter-notebooks-f8bd43f4b42e).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Sohail, K. (2021年12月15日)。*使用本地 PySpark 和 Jupyter Notebooks 从 Google Cloud
    Storage Bucket 读取文件*。Medium。 [链接](https://kashif-sohail.medium.com/read-files-from-google-cloud-storage-bucket-using-local-pyspark-and-jupyter-notebooks-f8bd43f4b42e)。'
