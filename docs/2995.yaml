- en: 'Orca: Properly Imitating Proprietary LLMs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Orca: 正确模仿专有LLMs'
- en: 原文：[https://towardsdatascience.com/orca-properly-imitating-proprietary-llms-44ffa0293adb?source=collection_archive---------3-----------------------#2023-09-30](https://towardsdatascience.com/orca-properly-imitating-proprietary-llms-44ffa0293adb?source=collection_archive---------3-----------------------#2023-09-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/orca-properly-imitating-proprietary-llms-44ffa0293adb?source=collection_archive---------3-----------------------#2023-09-30](https://towardsdatascience.com/orca-properly-imitating-proprietary-llms-44ffa0293adb?source=collection_archive---------3-----------------------#2023-09-30)
- en: Leveraging imitation to create high-quality, open-source LLMs…
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用模仿创建高质量、开源的LLMs……
- en: '[](https://wolfecameron.medium.com/?source=post_page-----44ffa0293adb--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----44ffa0293adb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----44ffa0293adb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----44ffa0293adb--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----44ffa0293adb--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page-----44ffa0293adb--------------------------------)[![卡梅隆·R·沃尔夫博士](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----44ffa0293adb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----44ffa0293adb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----44ffa0293adb--------------------------------)
    [卡梅隆·R·沃尔夫博士](https://wolfecameron.medium.com/?source=post_page-----44ffa0293adb--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Forca-properly-imitating-proprietary-llms-44ffa0293adb&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----44ffa0293adb---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----44ffa0293adb--------------------------------)
    ·16 min read·Sep 30, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F44ffa0293adb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Forca-properly-imitating-proprietary-llms-44ffa0293adb&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----44ffa0293adb---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Forca-properly-imitating-proprietary-llms-44ffa0293adb&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----44ffa0293adb---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----44ffa0293adb--------------------------------)
    ·16 min read·2023年9月30日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F44ffa0293adb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Forca-properly-imitating-proprietary-llms-44ffa0293adb&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----44ffa0293adb---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F44ffa0293adb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Forca-properly-imitating-proprietary-llms-44ffa0293adb&source=-----44ffa0293adb---------------------bookmark_footer-----------)![](../Images/aa2e365a5a760e33e8968360e016c5b1.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F44ffa0293adb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Forca-properly-imitating-proprietary-llms-44ffa0293adb&source=-----44ffa0293adb---------------------bookmark_footer-----------)![](../Images/aa2e365a5a760e33e8968360e016c5b1.png)'
- en: (Photo by [Thomas Lipke](https://unsplash.com/@t_lipke?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/p5nDU-d3Y0s?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: （照片由 [Thomas Lipke](https://unsplash.com/@t_lipke?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    在 [Unsplash](https://unsplash.com/photos/p5nDU-d3Y0s?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供）
- en: As research progresses on large language models (LLMs), one key question that
    remains unanswered is whether an existing, high-quality LLM can be used to effectively
    train another LLM. Currently, there is a lot of debate and contention around this
    topic. The recent explosion of open-source imitation models initially indicated
    that proprietary LLMs like ChatGPT could be easily replicated at a low cost. However,
    subsequent research concluded that the evaluation of such models was incomplete
    and misleading, finding that these models actually have large gaps in their comprehension.
    In this overview, we will study work [1] that aims to solve the limitations of
    open-source replicas of proprietary LLMs via a more robust approach. In particular,
    we will see that imitation learning can be made more effective by curating a larger
    dataset with more detailed information.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对大型语言模型（LLMs）的研究进展，一个尚未解决的关键问题是，现有的高质量LLM是否可以用来有效地训练另一个LLM。目前，围绕这个话题存在很多争议和讨论。近期开源模仿模型的爆炸性增长最初表明，像ChatGPT这样的专有LLM可以以低成本轻松复制。然而，随后的研究得出结论，认为这些模型的评估是不完整和具有误导性的，发现这些模型实际上在理解上存在很大的差距。在这篇概述中，我们将研究旨在通过更强有力的方法解决专有LLM开源副本局限性的工作[1]。特别是，我们将看到，通过策划一个包含更多详细信息的大型数据集，可以使模仿学习变得更有效。
- en: '“As these models continue to evolve and become more powerful, an intriguing
    question arises: Can we use the model itself to supervise its own behavior or
    that of other AI models?” *— from [1]*'
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “随着这些模型的不断演变和变得更加强大，一个引人入胜的问题出现了：我们是否可以利用模型本身来监督其自身行为或其他人工智能模型的行为？” *— 来自 [1]*
