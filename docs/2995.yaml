- en: 'Orca: Properly Imitating Proprietary LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/orca-properly-imitating-proprietary-llms-44ffa0293adb?source=collection_archive---------3-----------------------#2023-09-30](https://towardsdatascience.com/orca-properly-imitating-proprietary-llms-44ffa0293adb?source=collection_archive---------3-----------------------#2023-09-30)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Leveraging imitation to create high-quality, open-source LLMs…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wolfecameron.medium.com/?source=post_page-----44ffa0293adb--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----44ffa0293adb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----44ffa0293adb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----44ffa0293adb--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----44ffa0293adb--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Forca-properly-imitating-proprietary-llms-44ffa0293adb&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----44ffa0293adb---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----44ffa0293adb--------------------------------)
    ·16 min read·Sep 30, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F44ffa0293adb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Forca-properly-imitating-proprietary-llms-44ffa0293adb&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----44ffa0293adb---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F44ffa0293adb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Forca-properly-imitating-proprietary-llms-44ffa0293adb&source=-----44ffa0293adb---------------------bookmark_footer-----------)![](../Images/aa2e365a5a760e33e8968360e016c5b1.png)'
  prefs: []
  type: TYPE_NORMAL
- en: (Photo by [Thomas Lipke](https://unsplash.com/@t_lipke?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/p5nDU-d3Y0s?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  prefs: []
  type: TYPE_NORMAL
- en: As research progresses on large language models (LLMs), one key question that
    remains unanswered is whether an existing, high-quality LLM can be used to effectively
    train another LLM. Currently, there is a lot of debate and contention around this
    topic. The recent explosion of open-source imitation models initially indicated
    that proprietary LLMs like ChatGPT could be easily replicated at a low cost. However,
    subsequent research concluded that the evaluation of such models was incomplete
    and misleading, finding that these models actually have large gaps in their comprehension.
    In this overview, we will study work [1] that aims to solve the limitations of
    open-source replicas of proprietary LLMs via a more robust approach. In particular,
    we will see that imitation learning can be made more effective by curating a larger
    dataset with more detailed information.
  prefs: []
  type: TYPE_NORMAL
- en: '“As these models continue to evolve and become more powerful, an intriguing
    question arises: Can we use the model itself to supervise its own behavior or
    that of other AI models?” *— from [1]*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
