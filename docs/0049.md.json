["```py\ndocker-compose up\n```", "```py\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\n# SparkSession\nURL_SPARK = \"spark://spark:7077\"\n\nspark = (\n    SparkSession.builder\n    .appName(\"spark-ml\")\n    .config(\"executor.memory\", \"4g\")\n    .master(URL_SPARK)\n    .getOrCreate()\n)\n```", "```py\ndf_avocado = spark.read.csv(\n  \"/data/avocado.csv\", \n  header=True, \n  inferSchema=True\n)\n\n# cache data\ndf_avocado.cache()\ndf_avocado.show(4)\n```", "```py\ndf_avocado_train, df_avocado_test = df_avocado.randomSplit([0.75, 0.25], seed=214)\n```", "```py\nCOLUMNS = ['AveragePrice', 'type']\nCOLUMNS = [f\"`{col}`\" for col in COLUMNS]\n\nLOG_COLUMNS =  ['4225', '4770', 'Small Bags', 'Large Bags', 'XLarge Bags']\nLOG_COLUMNS = [f\"LOG(`{col}`+1) AS `LOG {col}`\" for col in LOG_COLUMNS]\n\nsql_trans = SQLTransformer(\n    statement=f\"\"\"\n\n    SELECT\n    {', '.join(COLUMNS)}\n    , {', '.join(LOG_COLUMNS)}\n    ,YEAR(__THIS__.Date)-2000 AS year\n    ,MONTH(__THIS__.Date) AS month\n\n    FROM __THIS__\n\n    \"\"\"\n)\n\n# Visualize the data\nsql_trans.transform(df_avocado_train).show(4)\n```", "```py\nfrom pyspark.ml.feature import MinMaxScaler\n\n# Creating a Month vector column\nmonth_vec_ass = VectorAssembler(inputCols=['month'], outputCol='month_vec')\n\ndf_avocado_month_ass = month_vec_ass.transform(sql_trans.transform(df_avocado_train))\n\n# Scaling the month column\nmonth_scaler = MinMaxScaler(inputCol='month_vec', outputCol='month_scaled')\nmonth_scaler = month_scaler.fit(df_avocado_month_ass)\n\nmonth_scaler\\\n  .transform(df_avocado_month_ass)\\\n  .select( ['month', 'month_vec', 'month_scaled'] )\\\n  .show(10)\n```", "```py\nstr_indexer = StringIndexer(inputCol=\"type\", outputCol=\"type_index\")\n\nstr_indexer = str_indexer.fit(df_avocado_train)\n\nstr_indexer\\\n  .transform(df_avocado_train)\\\n  .select( [\"type\", \"type_index\"] )\\\n  .show(4)\n```", "```py\n# Apply transformations\n## SQL transformer\ndf_avocado_train_transformed = sql_trans.transform(df_avocado_train)\n\n## String indexer\ndf_avocado_train_transformed = str_indexer.transform(df_avocado_train_transformed)\n\n## Month scaler (vector assembler + minmax scaler)\ndf_avocado_train_transformed = month_vec_ass.transform(df_avocado_train_transformed)\ndf_avocado_train_transformed = month_scaler.transform(df_avocado_train_transformed)\n\n# Join all features into a single vector\nnumerical_vec_ass = VectorAssembler(\n    inputCols=[\n      'year', 'month_scaled', 'LOG 4225', \n      'LOG 4770', 'LOG Small Bags', \n      'LOG Large Bags', 'LOG XLarge Bags'\n    ],\n    outputCol='features_num'\n)\ndf_avocado_train_transformed = numerical_vec_ass.transform(df_avocado_train_transformed)\n\n# Join all categorical features into a single vector\ncategorical_vec_ass = VectorAssembler(\n    inputCols=['type_index'],\n    outputCol='features_cat'\n)\ndf_avocado_train_transformed = categorical_vec_ass.transform(df_avocado_train_transformed)\n\n# See the result\ndf_avocado_train_transformed.select(['features_cat', 'features_num', 'AveragePrice']).show(4, False)\n```", "```py\n# Scaling the numerical features using a StandardScaler\nstd_scaler = StandardScaler(\n    inputCol=\"features_num\",\n    outputCol=\"features_scaled\",\n    withStd=True,\n    withMean=True\n)\n\nstd_scaler = std_scaler.fit(df_avocado_train_transformed)\nstd_scaler.transform(df_avocado_train_transformed).select(['features_scaled']).show(5, False)\n```", "```py\n# Machine learning pipeline\nfrom pyspark.ml import Pipeline\n\n# Create a preprocessing pipeline\nprepro_pipe = Pipeline(stages=[\n    sql_trans,\n    str_indexer,\n    month_vec_ass,\n    month_scaler,\n    numerical_vec_ass,\n    categorical_vec_ass,\n    std_scaler,\n\n    # Join all features into a single vector\n    VectorAssembler(\n        inputCols=['features_scaled', 'features_cat'],\n        outputCol='features'\n    ),\n])\n\n# Fit the pipeline\npipeline_model = prepro_pipe.fit(df_avocado_train)\n\n# Transform the data\ndf_avocado_train_transformed = pipeline_model.transform(df_avocado_train)\n\n# See the result\ndf_avocado_train_transformed.select(['features', 'AveragePrice']).show(4, False)\n```", "```py\n+--------------------------------------------------------------------------------------------------------------------------------------------+------------+\n|features                                                                                                                                    |AveragePrice|\n+--------------------------------------------------------------------------------------------------------------------------------------------+------------+\n|[-1.2177154955881637,1.6482225355667333,0.9527463109714546,1.0269649008115518,0.5657377199959452,0.8334134211814762,-0.6436162273445295,0.0]|0.49        |\n|[-1.2177154955881637,1.6482225355667333,0.7058305701685025,1.0954357394643428,0.7803295242390127,0.8574417380503548,2.012648481596976,0.0]  |0.71        |\n|[-1.2177154955881637,1.6482225355667333,0.9399552148956506,1.5037797059140563,0.8203168521795554,0.6002078289352569,2.1083545825302594,0.0] |0.8         |\n|[-1.2177154955881637,1.6482225355667333,1.1142436751287843,1.5073956355774096,1.4653967110976907,1.0678725104034048,2.0181300922626053,0.0] |0.8         |\n+--------------------------------------------------------------------------------------------------------------------------------------------+------------+\nonly showing top 4 rows\n```", "```py\nfrom pyspark.ml.regression import LinearRegression\n\n# Create a linear regression model\nlin_reg = LinearRegression(\n    featuresCol='features',\n    labelCol='AveragePrice',\n    predictionCol='prediction',\n\n    # Hyperaparameters\n    maxIter=1000,\n    regParam=0.3,       # Regularization\n    elasticNetParam=0.8 # Regularization mixing parameter. 1 for L1, 0 for L2.\n)\n```", "```py\n# Fit the model\nlin_reg_model = lin_reg.fit(df_avocado_train_transformed)\n\n# See the output\ndf_avocado_train_pred = lin_reg_model.transform(df_avocado_train_transformed)\ndf_avocado_train_pred.select(\n  ['features', 'AveragePrice', 'prediction']\n).show(4, False)\n```", "```py\n+--------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------+\n|features                                                                                                                                    |AveragePrice|prediction        |\n+--------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------+\n|[-1.2177154955881637,1.6482225355667333,0.9527463109714546,1.0269649008115518,0.5657377199959452,0.8334134211814762,-0.6436162273445295,0.0]|0.49        |1.4003505112793717|\n|[-1.2177154955881637,1.6482225355667333,0.7058305701685025,1.0954357394643428,0.7803295242390127,0.8574417380503548,2.012648481596976,0.0]  |0.71        |1.4003505112793717|\n|[-1.2177154955881637,1.6482225355667333,0.9399552148956506,1.5037797059140563,0.8203168521795554,0.6002078289352569,2.1083545825302594,0.0] |0.8         |1.4003505112793717|\n|[-1.2177154955881637,1.6482225355667333,1.1142436751287843,1.5073956355774096,1.4653967110976907,1.0678725104034048,2.0181300922626053,0.0] |0.8         |1.4003505112793717|\n+--------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------+\nonly showing top 4 rows\n```", "```py\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nreg_eval = RegressionEvaluator(\n    labelCol='AveragePrice',\n    predictionCol='prediction',\n    metricName='rmse' # Root mean squared error\n)\n\n# Evaluate the model\nreg_eval.evaluate(df_avocado_train_pred)\n\n# Output >> 0.3978489578943717\n```", "```py\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\nml_pipeline = Pipeline(stages=[\n    prepro_pipe, # Preprocessing pipeline\n    lin_reg      # Linear regression model\n])\n\nparam_grid = ParamGridBuilder() \\\n    .addGrid(lin_reg.regParam, [0.0, 0.1, 0.3, 0.5]) \\\n    .addGrid(lin_reg.elasticNetParam, [0.0, 0.5, 1.0]) \\\n    .build()\n```", "```py\nreg_eval = RegressionEvaluator(\n    labelCol='AveragePrice',\n    predictionCol='prediction',\n    metricName='rmse' # Root mean squared error\n)\n\n# Join everything together using a CrossValidator object.\ncrossval_ml = CrossValidator(\n    estimator=ml_pipeline, \n    estimatorParamMaps=param_grid, \n    evaluator=reg_eval, \n    numFolds=4\n)\n```", "```py\ncrossval_ml_model = crossval_ml.fit(df_avocado_train)\n```", "```py\nbest_model = crossval_ml_model.bestModel\nbest_score = crossval_ml_model.avgMetrics[0]\n\nprint(\"Best model: \", best_model)\nprint(\"Best score: \", best_score)\n\n# Output >>>\n# > Best model:  PipelineModel_dc90de555ac1\n# > Best score:  0.2833541578138277\n```", "```py\n# The last stage in the pipeline is the Linear Regression\nbest_lin_reg_params = best_model.stages[-1].extractParamMap()\n\nprint(\"Best score (RMSE):\", best_score, end=\"\\n\\n\")\nfor parameter, value in best_lin_reg_params.items():\n    print(f\"{str(parameter):50s}, {value}\")\n```", "```py\nBest score (RMSE): 0.2833541578138277\n\nLinearRegression_eeaa1d8bf6ea__aggregationDepth   , 2\nLinearRegression_eeaa1d8bf6ea__elasticNetParam    , 0.0\nLinearRegression_eeaa1d8bf6ea__epsilon            , 1.35\nLinearRegression_eeaa1d8bf6ea__featuresCol        , features\nLinearRegression_eeaa1d8bf6ea__fitIntercept       , True\nLinearRegression_eeaa1d8bf6ea__labelCol           , AveragePrice\nLinearRegression_eeaa1d8bf6ea__loss               , squaredError\nLinearRegression_eeaa1d8bf6ea__maxBlockSizeInMB   , 0.0\nLinearRegression_eeaa1d8bf6ea__maxIter            , 1000\nLinearRegression_eeaa1d8bf6ea__predictionCol      , prediction\nLinearRegression_eeaa1d8bf6ea__regParam           , 0.0\nLinearRegression_eeaa1d8bf6ea__solver             , auto\nLinearRegression_eeaa1d8bf6ea__standardization    , True\nLinearRegression_eeaa1d8bf6ea__tol                , 1e-06\n```", "```py\ndf_avocado_test_pred = best_model.transform(df_avocado_test)\n\n# show scores\nprint(reg_eval.evaluate(df_avocado_test_pred))\n\n# Output\n# > 0.28368085199676235\n```"]